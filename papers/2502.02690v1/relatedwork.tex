\section{Related Works}
\vspace{-0.1cm}
\subsection{Controllale Video Generation}
Recent advances in controllable video generation have led to significant progress, with text-to-video (T2V) models \citep{yang2024cogvideox,singer2022make,ho2022video,zhou2022magicvideo,opensora} achieving impressive results in generating videos from textual descriptions. However,
% text prompts alone are insufficient to accurately describe a video, as they lack the dimensionality needed to capture its full complexity compared to the dynamic nature of video. The
effectiveness of the control is highly dependent on the quality of the input prompt, making it difficult to achieve fine-grained control over the generated content. An alternative method for control involves leveraging side information such as pose \citep{tu2024stableanimator, zhu2025champ}, camera motion \citep{yang2024direct, wang2024motionctrl}, depth \citep{liu2024stablev2v, xing2024make} and so on. While this approach allows for more precise control, it requires paired data, which can be challenging to collect. 
Besides, most of the aforementioned alignment-based techniques share a common issue: the control signals are directly aligned with the entire video. This issue not only reduces efficiency but also complicates the task of achieving independent control over different aspects of the video, which further motivates us to propose a framework to find the disentanglement representation for conditional generation.

% As a result, when you generate a video but wish to modify a very small detail within it, you would have to rerun the generation model from scratch, as even minor changes to the prompt can affect the entire video's latent encoding. This issue not only reduces efficiency but also complicates the task of achieving independent control over different aspects of the video. The reliance on this direct alignment makes it difficult to isolate and manipulate specific components of the video without unintentionally altering others.

\subsection{Nonlinear Independent Component Analysis}
Nonlinear independent component analysis offers a potential approach to uncover latent causal variables in time series data. These methods typically utilize auxiliary information, such as class labels or domain-specific indices, and impose independence constraints to enhance the identifiability of latent variables. Time-contrastive learning (TCL) \citep{hyvarinen2016unsupervised} builds on the assumption of independent sources and takes advantage of the variability in variance across different segments of data. Similar Permutation-based contrastive learning (PCL) \citep{hyvarinen2017nonlinear} introduces a learning framework that tell true independent sources from their permuted counterparts. Additionally, i-VAE \citep{khemakhem2020variational} employs deep neural networks and Variational Autoencoders (VAEs) to closely approximate the joint distribution of observed data and auxiliary non-stationary regimes. Recently, \cite{yao2021learning,yao2022temporally} extends the identifiability to linear and nonlinear non-Gaussian cases without auxiliary variables, respectively. CaRiNG \citep{chen2024caring} further tackles the case when the mixing process is non-invertible. Additionally, CITRIS \citep{lippe2022citris, lippe2023causal} emphasizes the use of intervention target data, and IDOL \citep{li2024identification} incorporates sparsity into the latent transition process to identify latent variables, even in the presence of instantaneous effects.

\vspace{-0.3cm}