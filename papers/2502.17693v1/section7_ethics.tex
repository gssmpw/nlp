\section{Ethics considerations}
\label{sec:ethics}

We have assessed the value of publishing this work against potentially adverse consequences due to its methodology and/or data practices. Specifically, we considered risks related to \emph{user harm}, \emph{equitable selection}, \emph{user consent}, and \emph{user data handling}~\cite{irb}.

\myparagraph{User harm.}
We acknowledge that fighting abuse on online social networks is a task fraught with risk: some users are harmed by the abuse itself, while others are harmed by over-enforcement when defenses become too aggressive. This entire work is devoted to systematically balancing reduction in both of these harms. Our system's ``explore-exploit'' strategy aims to reduce harm over the entire platform while taking into account that not all local decisions can be perfect. However, our experiments demonstrate that the PRO system offers significant benefit to users in terms of overall harm reduction.

We also considered the risk of our experiments over-enforcing on benign users due to system and/or model deficiencies. To mitigate this risk we set up our experiments to include only users that our binary classifiers predicted to be abusive with high confidence. The precision of the binary classifiers was confirmed to be greater than 90\% at the time of the experiments. Given the above considerations, we believe that our experiment exposes users to risks that are ``reasonable in relation to anticipated benefits''~\cite{irb}. 

\myparagraph{Equitable selection.} Our subjects are necessarily chosen equitably since a user's participation in the experiment is determined by a random number generator whose output is independent of any user properties.

\myparagraph{User consent.}
Partridge and Allman~\cite{partridge-allman} observe that ``direct
consent is not possible in most Internet measurements,'' and our study is a good example.
Since we cannot predict in advance who our classifiers will determine to be abusive, we would have to obtain consent either from the entire \osn population or from the specific users acted on by PRO at the moment action is taken. Building a bespoke consent flow for this study would be a large engineering task and would risk both information bias (users might act differently knowing they were in a security study) and selection bias (both benign and abusive users who choose to participate in a security study may not reflect the general population). Either type of bias could render our statistical analyses invalid and thus handicap our ability to measurably improve abuse detection.

Partridge and Allman suggest that ``proxy consent'' is the \emph{de facto} standard in large-scale internet measurement studies, giving the example of ``network measurements taken on a university campus typically seek consent from the university.'' In our case institutional consent was rendered via the \osns' agreements to let us conduct and publish this research. In particular, the reviewers approving the research noted that all users in our study have accepted the terms of service of \ig or \fb (as applicable), which address use of data in the context of investigating suspicious activity and addressing policy violations.
The reviewers therefore concluded that the \osn terms of service provide users a sufficient level of transparency.

We note that explicit consideration of user consent is not historically an element of large-scale internet security studies. Bilogrevic \emph{et al.}~\cite{bilogrevic-google} use a proxy approach similar to ours, deriving their user consent from the fact that users opted in to a setting to ``Make searches and browsing better.'' However, recent work studying millions of users on Reddit~\cite{kumar-reddit}, Facebook~\cite{golla-facebook,onaolapo-facebook}, and Google Chrome~\cite{thomas-stuffing} do not address user consent at all in their ethics discussions. We discuss open questions in this area in Section~\ref{sec:conclusion}.


\myparagraph{Data handling.}
Before developing and testing our system,
we assessed how data would be used and protected and ensured that technical systems and/or manual processes were in place to mitigate any identified risks prior to the launching the experiment\ifanon\else~\cite{meta-privacy-review}\fi. For this project, we mitigated risks by:
\begin{itemize}
    \squeezelist
    \item Limiting data collection to a set of user features identified as being relevant for abuse detection;
    \item Specifically excluding any sensitive data from collection;
    \item Restricting access to both collected and inferred data;
    \item Deleting all user-identifying data within 90 days of collection;
    \item Using technical safeguards to ensure that the data are only used for safety, integrity, and security use cases.
\end{itemize}
