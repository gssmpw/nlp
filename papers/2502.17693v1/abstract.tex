\begin{abstract}

Detecting phishing, spam, fake accounts, data scraping, and other malicious
activity in online social networks (\osns) is a problem that has been studied
for well over a decade, with a number of important results. Nearly all
existing works on abuse detection have as their goal producing the best
possible binary classifier; i.e., one that labels unseen examples as
``benign'' or ``malicious'' with high precision and recall.
However, no prior published work considers what comes
next: what does the service actually {\em do} after it detects abuse?

In this paper, we argue that {\em detection} as described in previous work
{\em is not the goal} of those who are fighting \osn abuse. Rather, we
believe the goal to be {\em selecting actions} (e.g., ban the user, block the
request, show a CAPTCHA, or ``collect more evidence'') that {\em optimize a
tradeoff} between harm caused by abuse and impact on benign users.  With this
framing, we see that enlarging the set of possible actions allows us to move
the Pareto frontier in a way that is unattainable by simply tuning the
threshold of a binary classifier.

To demonstrate the potential of our approach, we present {\em Predictive
Response Optimization (PRO),} a system based on reinforcement learning that
utilizes available contextual information to predict future abuse and
user-experience metrics conditioned on each possible action, and select
actions that optimize a multi-dimensional tradeoff between abuse/harm and
impact on user experience.

We deployed versions of PRO targeted at stopping automated activity on \fbig{}. 
In both cases our experiments showed that PRO outperforms a baseline
classification system, reducing abuse volume by \igreduction
and \fbreduction (respectively) with no negative impact to users. We also
present several case studies that demonstrate how PRO can quickly and
automatically adapt to changes in business constraints, system behavior,
and/or adversarial tactics.
\end{abstract}
