\section{Abuse minimization as a constrained optimization problem}\label{sec:opt}

In this section we reframe the goal of anti-abuse systems in online social networks: rather than existing to {\em classify} content or behavior, such systems exist to {\em optimize the tradeoff} between abuse reduction (e.g., removing spam) and impact on user experience (e.g., too much benign content removed). Specifically, our position is that {\bf anti-abuse systems are solving the {\em constrained optimization problem} of minimizing abuse volume within a ``budget'' of operational costs.} We conjecture that this reframing will enable us to devise a system that blocks more abuse than a classification system, without adversely impacting user experience.

\subsection{Abuse minimization is a tradeoff problem} 
\label{sec:business_constraints}

Anti-abuse systems seek to achieve the dual aims of both reducing abuse prevalence and minimizing costs that arise as a result of practical considerations. Some of these quantifiable business considerations are:

\begin{itemize}
\setlength\itemsep{-2pt}
    \item Size of the operational team that processes appeals of incorrect content deletions or account disables.
    \item Users choosing to leave the platform because of enforcement actions against their content.
    \item Negative impact to usability/engagement metrics on the \osn due to increased user friction (e.g., challenges and verifications).
    \item Computer and network hardware needed to run the anti-abuse system itself.
\end{itemize}

Conceptually, the abuse-cost tradeoff can be visualized as a Pareto frontier~\cite{Wilkinson_2006} if we simplify and reduce operational cost considerations to a single ``cost'' dimension (Figure~\ref{fig:pareto}).
As illustrated, abuse can be fully eradicated by any method if the cost axis is not constrained --- after all, if we block {\em all} users from using the platform, there will be no abuse left! However, for any anti-abuse method to work in a real-world setting, the system must be tuned to operate under some cost constraints (``budget'' in Figure~\ref{fig:pareto}).
The abuse/cost tradeoff space is also often multi-dimensional; i.e., multiple business-metric constraints can exist simultaneously. 

\begin{figure}[htbp]
\centering
\captionsetup{justification=centering,margin=1cm}
\includegraphics[width=8cm]{images/pareto.png}
\caption{Two conceptual anti-abuse methods tuned under constraints.}\label{fig:pareto}
\end{figure}

\subsection{Enforcement actions really matter}\label{sec:enforcement_actions}

On its surface, the ``what to do'' problem seems fairly straightforward to solve: if an entity is abusive, remove it from the \osn! However, if we take content moderation as an example of a typical use case, we see that a policy of ``delete all content classified as abusive'' quickly runs into issues such as those mentioned in the Introduction.

To address these issues while remaining effective at stopping abuse, anti-abuse systems typically employ multiple enforcement actions that are designed to hinder or discourage abusive behavior to varying degrees. At the same time, these actions affect benign (i.e., non-violating) users to different extents, ranging from mild annoyance to total loss of access to their account and the value associated with it.
As an example, in the case of content moderation, a possible set of enforcement actions could include one or more of the following (ranging from least to most intrusive):
\begin{itemize}
\vspace{-2pt}
\squeezelist
    \item No action,
    \item Incrementing a ``strike counter'' for the author~\cite{youtube},
    \item Showing a warning notification to the author,
    \item Down-ranking/reducing visibility of the content,
    \item Asking the author to perform an interactive challenge (e.g., CAPTCHA, MFA) before posting content~\cite{von2003captcha,searles2023empirical},
    \item Deleting the content,
    \item Temporarily banning the author from sharing content,
    \item Temporarily banning the author from the service,
    \item Permanently revoking the authorâ€™s access to the service.
\end{itemize}

In ``traditional'' systems, action selection is implemented using a set of hand-written logical rules that incorporate information such as classifier confidence, past enforcement/strike counts, and other features related to the entity. The focus of prior research on the classification problem, while leaving enforcement selection to rule- or strike-based logic, results in three major pitfalls when applied in practical settings:
\begin{itemize}
\squeezelist
    \item Abuse prevalence could remain high due to ineffective enforcement; e.g., as a result of poor optimization against practical constraints.
    \item Fixed action-selection rule logic does not adapt to changing adversarial behavior; e.g., learning to bypass certain enforcement actions.
   \item Introducing new actions requires writing new routing rules and could cause constraint metrics to shift in the wrong direction.
\end{itemize}

Instead of relying on hand-written rules, we propose formalizing the enforcement-selection problem as a constrained optimization problem and then develop a reinforcement-learning algorithm to solve it.

\subsection{Formalizing the optimization problem}\label{sec:problem_math}

Anti-abuse systems evaluate and execute actions over a potentially very large set $\mathcal{E}$ of entities (\osn accounts, pieces of content, IP addresses, and so on). If the system takes action on entity $e \in \mathcal{E}$ at time $t_0$, we can measure the impact of this action on entity $e$ over the subsequent time period $\tau = [t_0, t_1]$ using a set of ``abuse metrics'' $Abuse_{j}$ ($j  = 1, \dots, N_A$) and ``cost metrics'' $Cost_{j}$ ($j = 1,  \dots, N_C$). 

To make our discussion more concrete, we will use the running example of \osn accounts posting spam, with abuse metric $Abuse^*$ being ``number of spam posts during the time period $\tau$'' and cost metric $Cost^*$ being ``number of non-spam posts blocked during the time period $\tau$.'' 

Let $\mathcal{A}$ denote the (finite) set of all possible actions. We define the vector of all ``next actions'' for all entities as  
\begin{equation}\label{eq:action_set}
\mathbf{a} \in \AAA = \left\{ \left(a_e\right)_{ e\in\mathcal{E}}:  a_e \in \mathcal{A} \right\} .
\end{equation}
The set $\mathcal{A}$ contains the special action $A_0$ denoting ``no action,'' plus other actions such as those described in Section~\ref{sec:enforcement_actions}. In our spam example we will let $\mathcal{A}$ be a set of three actions: $A_0 = $ ``no action''; $A_1 = $ ``show a CAPTCHA''; $A_2 = $ ``disable the account.''

At optimization time $t_0$, our goal is to determine the action vector $\mathbf{a}$ that minimizes the total abuse over the period $\tau$
\begin{equation}\label{eq:objective} 
\min_\mathbf{a} \sum_{e \in \mathcal{E}} \sum_{j=1}^{N_A} Abuse_{j}(e \mid \mathbf{a})
\end{equation}
while constraining the cost over the same period
\begin{equation}
	\label{eq:constraints}
	\sum_{e \in \mathcal{E}} Cost_{j}(e \mid \mathbf{a}) \leq B_{j}, \qquad j = 1, \ldots, N_C
\end{equation}
where $B_j \geq 0$ is a global ``budget'' for the $j$th metric. In our concrete example, we want to minimize total spam posts while keeping the total number of blocked non-spam posts below a certain fixed number $B^*$ determined by the business.

We assume that all abuse and cost metrics are normalized in such a way that 
\begin{equation}
    \label{eq:normalization}
Abuse_{j}(e \mid \mathbf{a}^0) = Cost_{j}(e \mid \mathbf{a}^0) = 0,
\end{equation}
where the baseline action vector $\mathbf{a}^0 = \left(A_0, \ldots, A_0\right)$ corresponds to applying ``no action'' to all entities, effectively ``turning off'' the anti-abuse system. Furthermore, we assume that the metrics are signed such that smaller values are ``better'' (i.e., we want to {\em minimize} both abuse and cost). This normalization ensures that $\mathbf{a}^0$ always satisfies the constraints, making the optimization problem feasible (i.e., a solution always exists). 

Let us consider the effect of this normalization on our spam example. An account $e_{spam}$ that posts only spam will (presumably) have 
$Abuse^*(e_{spam} \mid \mathbf{a}) \le 0$
for all $\mathbf{a}$, since (presumably) any nontrivial action will reduce the amount of spam posted by that account. The account will also have $Cost^*(e_{spam} \mid \mathbf{a}) = 0$ for all $\mathbf{a}$ since there are no non-spam posts to block. On the other hand, an account $e_{benign}$ that posts no spam will have $Abuse^*(e_{benign} \mid \mathbf{a}) = 0$ for all $\mathbf{a}$ and 
$Cost^*(e_{benign} \mid \mathbf{a}) \ge 0 $
for all $\mathbf{a}$ since (presumably) any nontrivial action will only decrease the number of non-spam posts; i.e., contribute a non-negative number of blocked posts.\footnote{Note that ``blocked posts'' in this example includes not only non-spam posts blocked directly but also those ``prevented'' relative to the no-action baseline. For example, if the action is to disable the account then the $Cost^*$ metric attempts to estimate how many non-spam posts the user ``would have made'' had they not been blocked.}

In practice, at optimization time we don't have access to any of the abuse and cost metrics since they refer to a future time period and will only be available after a time delay.  Moreover,  the constrained optimization problem (\ref{eq:objective})--(\ref{eq:constraints}) couples together all the entities, which makes it unfeasible to solve at high frequency. Therefore, we also consider an unconstrained relaxation consisting of maximizing the following ``reward'' function with respect to $\mathbf{a}$:
\begin{equation}
\label{eq:reward}
r(\mathbf{x},\mathbf{a}) = -\sum_{e \in \mathcal{E}} r_e(\mathbf{x},\mathbf{a}), \quad r_e(\mathbf{x},\mathbf{a}) = \sum_{j=1}^N w_j \cdot m_{j}(e \mid \mathbf{x}, \mathbf{a})
\end{equation}
\noindent where we have combined together all the metric functions
\[
(m_{1}, \ldots, m_{N})= (Abuse_{1}, \ldots, Abuse_{N_A}, Cost_{1}, \ldots, Cost_{N_C}),
\]
(setting $N = N_A + N_C$) and introduced multipliers $w_j\geq0$ that determine the relative weighting of each $Abuse$ and $Cost$ metric. The $w_j$ also implicitly convert all metrics into a common unit; for example an abuse metric might be in units of spam posts while a cost metric might be in units of benign users blocked.  The quantity 
\begin{equation}\label{eq:state}
\mathbf{x} = \left({x}_e \in \mathcal{X}\right)_{e\in\mathcal{E}}
\end{equation}
represents the state information (features) available at optimization time for all entities, where $\mathcal{X}$ is the ``feature space'' used to describe the state for a particular entity. This information allows us to leverage predictive models obtained via machine learning to approximate a solution to the optimization problem. Applying the notation in \eqref{eq:reward} to our spam example gives us the per-entity reward function
\begin{equation}
    \label{eq:example_reward}
 r_e(\mathbf{x},\mathbf{a}) = Abuse^*(e \mid \mathbf{x},\mathbf{a}) + w \cdot Cost^*(e \mid \mathbf{x}, \mathbf{a}).
 \end{equation}
Here we can interpret $w$ as the ``relative weight'' of the two harms being traded off: blocking one non-spam post is equivalent to allowing $w$ spam posts.

In general, maximizing (\ref{eq:reward})  is not equivalent to solving (\ref{eq:objective})--(\ref{eq:constraints}), though in some cases there exist Lagrange multipliers $w_j$ that make the two problems exactly equivalent. Nevertheless, the multipliers $w_j$ can be adjusted periodically to ensure that the optimal solution of (\ref{eq:reward}) tracks the optimal solution of (\ref{eq:objective})--(\ref{eq:constraints}) as closely as possible.  We show in the next section that the unconstrained relaxation (\ref{eq:reward}), combined with suitable modeling assumptions, enables high-frequency decision making by decoupling the optimization across entities.
