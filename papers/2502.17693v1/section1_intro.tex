\section{Introduction}\label{sec:intro}

Online Social Networks (\osns) connect billions of people around the world, allowing them to share content, engage in discussions, learn about events and issues, find employment, buy and sell goods, and undertake any number of other useful activities. However, the very utility and popularity of \osns attracts malicious actors who want to take advantage of the \osn's users for their own (usually monetary) gain. As a result, spam~\cite{ceas2010}, phishing~\cite{jagatic2007social}, fake engagement~\cite{dekoven2018following}, abusive accounts~\cite{xu2021deep}, and data scraping~\cite{musa} have become important problems for both academia and the \ifanon \osns \else social networks \fi themselves.

The traditional approach to fighting \osn abuse has been to build {\em binary classifiers} that distinguish ``benign'' content, entities, or actions from ``malicious'' ones. The \osn then runs these classifiers either synchronously or asynchronously and blocks or removes the detected abuse. A great deal of research has gone into designing better and better classifiers (see Section~\ref{sec:related_abuse} for a survey), but even the best known classifier will make thousands to millions of mistakes when run on {\em every} account or action in a large \osn, leading to degraded experiences for the users who encounter these mistakes.

Most binary classifiers produce classifications by comparing a numerical score against a threshold. For instance, scores above the threshold will result in classifying a piece of content as ``abusive.'' Tuning the threshold allows the operator to control the tradeoff between abuse and user experience in a rudimentary way. The \osn can set a ``budget'' on the model's precision (e.g., choose the lowest threshold such that at least $X\%$ of spam is classified correctly) or the false positive rate (e.g., choose the lowest threshold such that at most $Y\%$ of benign content is classified incorrectly). The modeling goal then becomes improving the classifier's recall within these constraints.  However, producing only classification results is insufficient to prevent or mitigate abuse --- eventually, the \osn must take actions with real-world consequences.

Even in a simple setting such as spam detection, where the obvious action is to simply remove the content, there are many factors to consider when taking action:
\begin{itemize}
\squeezelist
    \item Classifiers are never 100\% precise, so users may complain that their benign content is being taken down.
    \item Some users may not be aware of the site's content policy; these users have the potential to be educated and thus improve the quality of their content.
    \item Classifiers have some latency, so malicious users may post abusive content faster than it can be taken down, leading to more spam on the \osn.
\end{itemize}
In the binary-classification approach described above, there is only one parameter we can adjust to balance these competing considerations. It follows that if we want to improve user experience without degrading our ability to remove abusive content, our only avenue is to improve the model, which is a difficult and time-consuming effort.

A first direction of improvement is to {\bf expand the set of possible enforcement actions} beyond the binary ``hard block'' or ``allow.'' For instance, we can add a third, ``soft'' enforcement action: an action that introduces some friction but does not completely block a user. Such actions are less disruptive to benign users while (possibly) being less effective at stopping abuse. We can now segment our classification results into three groups instead of two: the ``worst'' content gets blocked as before, the ``best'' is let through, and the ``suspicious'' is routed through the ``soft'' action. By adjusting the relative sizes of the three groups, we gain an extra degree of freedom to control the tradeoff between amount of abuse blocked and negative impact to user experience.

As a real-world example, almost every website that has a login page chooses to block some logins, let some through, and send the rest through additional checks such as a CAPTCHA or SMS code verification. CAPTCHA and SMS code verification are examples of ``soft'' enforcement actions that lower the cost of false positives, and thus allow the website to send ``suspicious'' users through some non-zero level of enforcement, which reduces false negatives. However, a multi-class classification approach would still run into the problem of defining ``worst,'' ``best,'' and ``suspicious'' in a way that is sufficiently precise to generate objective and low-noise labels.

This additional complexity compels us to consider the action-selection problem from an entirely new perspective: that of {\em adaptive control theory}~\cite{aastrom2008adaptive} and {\em reinforcement learning}~\cite{sutton2018reinforcement}, which are frameworks that focus on decision-making with causal consequences. Reinforcement learning (RL) uses observations from previous actions to choose actions that optimize a reward (in our example, a quantity that captures the amounts of abusive content posted and benign content blocked) while also implementing a data-collection strategy that yields adaptation to non-stationary conditions. Continuous exploration is especially important in an adversarial environment, as future abusive behavior will change in response to our actions on past examples. The full technical details of our formalization appear in Section~\ref{sec:opt}.

In Section~\ref{sec:algo} we present a reinforcement-learning system for fighting abuse on \osns, which we call {\em Predictive Response Optimization} (PRO). We describe the system in terms that apply to {\em any} type of abuse, as long as it can be measured in some way. The system revolves around models that predict the cost and benefit of each action (the {\em contextual multi-armed bandit} setting~\cite{Li_2010}), overlaid with a {\em model predictive control} framework to adjust tradeoffs amongst various abuse and user-experience metrics.

In Section~\ref{sec:system_in_practice} we describe our application of PRO to the problem of reducing scraping activity in \osns. We defined metrics, implemented the system on \fbig, and collected data for two weeks on each. Our experiments showed that in both cases PRO stops significantly more abuse than a baseline system that determines actions by applying a set of hand-coded rules to classifier outputs. Specifically, we {\bf reduced abuse volume by \igreduction on \ig and \fbreduction on fb, with no negative impact on ``benign usage'' metrics}.\footnote{The difference of an order of magnitude between the two results is due to the relative maturity of the experimental and control groups on the two \osns. For details see Section~\ref{ss:results}.}

In Section~\ref{sec:system_in_practice} we also detail five case studies illustrating how PRO can quickly and automatically adapt to changes in business considerations, system behavior, and/or adversarial tactics. Specifically, our experiments show that:
\begin{itemize}
\squeezelist
	\item Adding a new user metric led to an \smsreduction reduction in SMS expenditures with no increase in abuse volume.
	\item After we saw signs of over-enforcement on \surface, adding a new user metric led to a \mwebdaulossreduction decrease in user churn.
	\item Introduction of a new enforcement action reduced abuse volume by \newresponsereduction without any manual intervention.
	\item When a bug changed the behavior of a particular enforcement action, the system adjusted automatically to stop selecting the action.
	\item When adversaries began to evade a particular enforcement action, the system adjusted automatically to select that action less frequently.
\end{itemize}

\myparagraph{In summary, our contributions are:}
\begin{itemize}
\squeezelist
    \item We introduce the perspective that {\em selection of enforcement actions,} rather than binary classification, is the true goal of abuse-fighting systems in online social networks.
    \item We formalize action selection as a {\em reinforcement learning problem} that attempts to balance abuse volume against cost of blocking benign users or content.
    \item We design {\em Predictive Response Optimization} (PRO), a large-scale reinforcement-learning system for action selection and the first application of RL to abuse reduction.
    \item We implement the PRO system on \fbig and observe that it significantly reduces scraping activity with no negative impact to users.
    \item We describe a number of case studies that demonstrate the ability of PRO to adapt to changing conditions with minimal manual intervention.
\end{itemize}
