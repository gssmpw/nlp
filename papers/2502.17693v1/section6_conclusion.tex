\section{Directions for Future Work}
\label{sec:conclusion}

\myparagraph{Optimizing long-term reward}. PRO selects actions to maximize reward over a fixed time horizon. However, we may wish to select actions to reduce abuse and cost in the long term. We could view the RL problem as a ``continuing'' task, or as an infinite-horizon task instead of a finite-horizon one~\cite{sutton2018reinforcement}, and optimize using RL algorithms such as Q-learning \cite{watkins1992q,mnih2015human,kalweit2020deep}. For each metric, we could train on the sequence of actions taken on each entity and leverage Q-learning's ability to learn cumulative long-term discounted rewards.

\myparagraph{Evaluating exploration strategies}. Unlike supervised learning, RL involves an explore-exploit tradeoff. If PRO always issues the same action to an entity, it can never learn whether that action was actually the best choice. Having this feedback loop is even more important in an adversarial setting. However, comparing exploration strategies can be challenging. Simply A/B testing the same RL system with two exploration implementations will not work if the models share training data, as the Test model will ``free-ride'' on the Control model's exploration, or vice versa. Developing approaches to compare exploration methods would allow us to test other exploration strategies such as Upper Confidence Bound~\cite{carpentier2011upper} or Quantile Regression-based sampling~\cite{quantileregression}.

\myparagraph{Addressing over- and under-enforcement.} Case Study 1 describes one set of users on which the system made locally sub-optimal decisions. Other such populations include:
\begin{itemize}
    \squeezelist
    \item ``Power users,'' defined as non-malicious users who use the platform in such a way that their activity appears automated. The population of power users is so small that over-enforcement on this subset doesn't meaningfully impact the global cost metrics.
    \item ``Repeat offenders,'' defined as users sent through the PRO system multiple times. If the system doesn't update features quickly enough then it risks repeating a response that was either too harsh for a benign user or not effective on a malicious user.
    \item ``Low-information'' users, who may use unauthorized third-party tools or otherwise inadvertently breach the \osn terms of service.
\end{itemize}
For each of these cases, we believe that some combination of new cost metrics (as in Case Study 1) and/or new enforcement actions (as in Case Study 3) can improve the model's performance.

\myparagraph{Generalizing our solution to other anti-abuse use cases.} Our experiments provide empirical evidence about our system yielding measurable improvements in reducing scraping of \osns. Assessing how our solution can impact other abuse problems remains an open area of research.

\myparagraph{Understanding consent in adversarial studies.} In our discussion of user consent we asserted that ``users might act differently knowing they were in a security study'' and that ``both benign and abusive users who choose to participate in a security study may not reflect the general population.'' These assertions have never been tested rigorously; a study that tested hypotheses on user consent in adversarial environments would provide crucial scientific input to the ethical standards for all future studies  involving real-world adversaries.

\ifanon
\else

\newpage

\subsection*{Authors' Contributions}

\begin{itemize}
    \squeezelist
    \item Garrett Wilson developed model enhancements and observability infrastructure, drafted the technical portions of this paper, and coordinated the paper-revision process.
    \item Geoffrey Goh designed the experimentation process and developed core components including metrics, enforcement actions, serving infrastructure, and the model predictive controller.
    \item Yan Jiang extended PRO to \fb and ran related experiments for this paper.
    \item Ajay Gupta and Jiaxuan Wang provided engineering support.
    \item David Freeman consulted in the initial design phase of PRO and coordinated the writing process.
    \item Francesco Dinuzzo created this project and led the team that built and maintained the PRO system. He designed the reinforcement learning approach to abuse mitigation, built the initial production versions of the system, and guided its development over multiple iterations.
\end{itemize}

\subsection*{Acknowledgments}

The authors thank Katriel Cohn-Gordon, Feargus Pendlebury, Francesco Logozzo, Chris Palow, and Yiannis Papagiannis for helpful feedback on earlier drafts of this paper. We thank Sandeep Hebbani, Emile Litvak, and Gemma Silvers for encouraging publication of this paper. We thank the five anonymous reviewers for their feedback, and in particular the shepherd who helped us craft the paper into its current form.

\fi
