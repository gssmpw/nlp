\section{Related Work}
\label{sec:related-work}

The seminal \textit{Attention is All You Need}~\citep{vaswani_attention_2017} paper laid the foundations
for LLMs such as GPT-4~\citep{openai_gpt-4_2024}, Gemini~\citep{team_gemini_2024},
or Llama~\citep{grattafiori_llama_2024}.
%
Surprisingly, LLMs perform decent on reasoning tasks,
especially if prompted via a Chain-of-Thought (CoT) approach~\citep{wei_chain--thought_2022}.
This behavior is part of an emergent property of LLMs named \textit{in-context-learning}
or \textit{few-shot-learning}~\citep{shanahan_talking_2024}.
Although CoT achieves astonishing results on reasoning benchmarks,
it is not faithful\footnote{
\textit{Faithful} means that the reasoning chain corresponds to how the model arrives at the answer~\citep{lyu_faithful_2023}.}
\citep{lyu_faithful_2023}.
Further, it is argued that not only is the reasoning not faithful
but also, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
and, therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks''~\citep{panas_can_2024}.
Related results show that LLMs do not
acquire systematic problem solving skills~\citep{dziri_faith_2023}.
%
%Earlier language-models were constructed by recurrent neural networks~\citep{kombrink_recurrent_2011}.
%Later work already used the attention mechanisms~\citep{luong_effective_2015},
%however the seminal \textit{Attention Is All You Need}~\citep{vaswani_attention_2017} paper
%laid the foundations for the Large Language Models (LLMs) as we know of today.
%An overview is given in~\citep{wang_history_2024},
%
%On a societal side, the public release of GPT-3~\citep{brown_language_2020} demonstrated LLMs capability to an audience outside of the AI community.
%GPT-4~\citep{openai_gpt-4_2024} improved upon GPT-3 on several metrics, but also has a significant larger size.
%Other popular LLMs include
%Llama~\citep{grattafiori_llama_2024},
%Gemini~\citep{team_gemini_2024}, or
%Mistral~\citep{jiang_mistral_2023}.
%
%Truly surprising for LLMs is that they perform decent in reasoning tasks,
%especially if prompted via a Chain-of-Thought (CoT) approach~\citep{wei_chain--thought_2024}.
%CoT is an example of an emergent property of LLMs, 
%which is commonly referred to as \textit{in-context-learning}, 
%or \textit{few-shot-learning}~\citep{shanahan_talking_2024}.
%Although CoT achieves astonishing results on reasoning benchmarks,
%it is not faithful
%\footnote{\textit{Faithful} means that the reasoning chain corresponds to how the model arrives at the answer~\citep{lyu_faithful_2023}.}
%\citep{lyu_faithful_2023}.
%Further it is argued that not only is the reasoning not faithful,
%but further, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
%and therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks.''~\citep{panas_can_2024}.
%

The logical reasoning capability of LLMs is measured with datasets such as
the ProntoQA~\citep{saparov_language_2023},  
the ProofWriter~\citep{tafjord_proofwriter_2021}, or
the \textit{FOLIO}~\citep{han_folio_2024} dataset.
%
%For measuring the logical reasoning capabilities of LLMs multiple datasets have been proposed.
%The \textit{ProntoQA}
%\footnote{Proof and Ontology-Generated Question-Answering}
%is a recent synthetically generated dataset~\citep{saparov_language_2023}.
%Alternative datasets include the \textit{ProofWriter}~\citep{tafjord_proofwriter_2021},
%\textit{AR-LSAT}~\citep{zhong_analytical_2022},
%and the \textit{LogicalDeduction}~\citep{srivastava_beyond_2023}
%dataset.
%\citep{lampinen_language_2024} introduced Syllogism, Wason, and NLI datasets for LLMs.
%
Improving LLM's reasoning capability was approached by different angles.
\citet{geva_injecting_2020} try to improve numerical capabilities
by injecting additional numerical data in the pre-training phase
and further fine-tune the model.
Other approaches focus on fine-tuning~\citep{yang_generating_2022}.
However, it was argued that these approaches fail to address the inherent inability of LLMs to reason 
mathematically~\citep{panas_can_2024}.

Neurosymbolic AI~\citep{garcez_neurosymbolic_2023} approaches offer an alternative to the pure sub-symbolic approaches.
Examples include differentiable logic~\citep{badreddine_logic_2022},
designing neural networks that act as Turing machines~\citep{siegelmann_computational_1995},
or visual question answering with logic-programming and
deep learning~\citep{eiter_logic-based_2023}.
%or combining learning with logic-programming~\citep{yang_neurasp_2020}.
For LLM logical reasoning tasks, 
\textit{Logic-LM}~\citep{pan_logic-lm_2023} is a neurosymbolic method that
combines LLMs with symbolic solvers.
%by combining LLMs with symbolic solvers.
The studied solvers include a Prolog~\citep{korner_fifty_2022}, First-Order-Logic (FOL)~\citep{enderton_mathematical_1972}, Constraint-Satisfaction-Problems~\citep{kumar_algorithms_1992}, and a Satisfiability-Problem~\citep{cook_complexity_1971} solver.
Implementation-wise, Logic-LM uses Python libraries for these solvers.
For Prolog they use \textit{Pyke}~\citep{frederiksen_applying_2008},
for SMT solving (SAT) they use \textit{Z3}~\citep{de_moura_z3_2008},
for FOL they use \textit{Prover9}~\citep{mccune_prover9_2010},
and for constraint solving they use the \textit{Python-constraint}~\citep{niemeyer_python-constraint_2024} library.
Logic-LM++~\citep{kirtania_logic-lm_2024} claims to improve on Logic-LM
by adding an improved self-refinement module that
takes more solver information into account.
\citet{lam_closer_2024} acknowledge performance differences between solvers
but fail to identify that these stem from the chosen intermediate language.
For knowledge based systems previous research shows that different query languages have an
impact on LLM understanding~\citep{liu_how_2024}.

Differing from these approaches,
we study the impact of the used syntax inherent to the intermediate language of 
neurosymbolic logical reasoners.
%symbolic formalisms for logical reasoning tasks.
%This paper discusses the effect of the used syntax inherent in different symbolic formalisms.
In particular, this paper contrasts the syntax used by Logic-LM, to Pyke's
and Answer Set Programming's (ASP) syntax.
Answer Set Programming (ASP)~\citep{gelfond_logic_2002} is a declarative problem-solving paradigm.
As our ASP solver we use Clingo~\citep{kaminski_how_2023} due to its
readily available Python support.
%