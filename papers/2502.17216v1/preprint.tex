
\documentclass{article} % For LaTeX2e
\usepackage{preprint,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}
%
\usepackage{hyperref}
\usepackage{url}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{booktabs}       % professional-quality tables
\usepackage{multirow}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - Define LST-LISTINGS
% lstlisting definition
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\definecolor{backcolor}{rgb}{0.95,0.95,0.95}
%\definecolor{backcolor}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolor},
    breaklines=true,
    numbers=left,
    numbersep=5pt,
    basicstyle=\ttfamily\footnotesize,
    captionpos=b
}
\lstset{style=mystyle}





\title{Making LLMs Reason?\\
The Intermediate Language Problem in\\
Neurosymbolic Approaches
}
% Keywords:
% Logical Reasoning, Neurosymbolic AI, Language Model, Symbolic Language, Answer Set Programming, Neurosymbolic LLMs

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy
\author{Alexander G. Beiser, David penz\\
TUWien, Vienna, Austria \\
\texttt{\{alexander.beiser,david.penz\}@tuwien.ac.at} 
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Logical reasoning tasks manifest themselves as a challenge to Large Language Models (LLMs).
Neurosymbolic approaches use LLMs to translate logical reasoning problems formulated in natural language
into a formal intermediate language. Subsequently, the usage of symbolic reasoners yields reliable solving thereof.
However, LLMs often fail in translation due to poorly chosen intermediate languages.
%deterministic symbolic reasoners solve this problem reliably.
%Although they perform better than humans on some benchmarks,
%they are still far away from performing logically sound reasoning.

We introduce the intermediate language problem,
which is the problem of choosing a suitable formal language representation for neurosymbolic approaches.
Theoretically, we argue that its origins lie 
%It stems from
in the inability of LLMs to distinguish syntax from semantics
and the relative independence of the problem from its representation.
We showcase its existence experimentally by contrasting two
intermediate languages, 
Answer Set Programming and the Python Knowledge Engine.
In addition, we demonstrate the effects of varying degrees
of supplementary context information.
%, with varying degree of additional context information.
Our results show a maximum difference in overall-accuracy of $53.20\%$
and $49.26\%$ in execution-accuracy.
%Additionally, we demonstrate by using \textit{in-context-learning}, that providing more supplementary
%information generally helps LLMs in translation.
When using the GPT4o-mini LLM we beat the state-of-the-art in
overall-accuracy on 
the ProntoQA dataset by $21.20\%$ and by $50.50\%$ on the ProofWriter dataset.

\end{abstract}

%
\section{Introduction}
%
Large Language Models (LLMs) perform surprisingly well on logical reasoning tasks~\citep{saparov_language_2023}.
Actually, they perform better than humans on some datasets
while still falling prey to the same fallacies as humans do~\citep{lampinen_language_2024}.
Take, for example, the following two syllogism chains and determine whether they are correct or false:
(1) \textit{All cats are mammals}, \textit{all mammals are animals}, \textit{all cats are animals},
and (2) \textit{all tumpus are wumpus}, \textit{all wumpus are vumpus}, \textit{all tumpus are vumpus}.
Research indicates that both humans and LLMs perform better on reasoning chains of type (1) (real ontologies)
than on type (2) (fictional ones)~\citep{lampinen_language_2024}.
This is interesting, 
as in logic, there is no distinction between (1) and (2) (both are correct),
as there is a separation of semantics from its representation (syntax).

Improving performance of LLMs on logical reasoning tasks (datasets)
must involve getting
LLMs to reason more abstractly, thereby better separating semantics from syntax.
%so to separate semantics from representation in a better way. 
One such attempt is Chain of Thought (CoT)~\citep{wei_chain--thought_2022} prompting.
%It nudges LLMs to reason according to reasoning chains.
However, LLM's reasoning chains are non-faithful in general~\citep{lyu_faithful_2023}.
%This may yield incorrect results, even if the intermediate steps are correct.
%
Faithful reasoning chains are obtained by Neurosymbolic AI
by using LLMs to translate a logical reasoning problem (posed in a natural language) % henceforth assumed to be English),
into a formal (symbolic) language.
This translation is subsequently (faithfully) solved by a symbolic reasoner.
Finally, its output is translated back into natural language.
One such approach is Logic-LM~\citep{pan_logic-lm_2023}.
%which performs better than the previous state-of-the-art (SOTA) on various logical reasoning benchmarks.

However, these neurosymbolic approaches fall short of
acknowledging the impact of the \textit{intermediate language}\footnote{
The formal language used between the LLM and the symbolic solver.}
on translation.
This paper investigates the current state-of-the-art (SOTA) approach Logic-LM
and demonstrates that the choice of representation language matters.
For this task, we compare the representation languages of the original Logic-LM paper
to Answer Set Programming and the Python Knowledge Engine.
%for two datasets where logic programming can be used.
We show differences of up to $49.26\%$ in in execution-accuracy
when different levels of additional information 
are given to the LLM.
Further, we beat the current SOTA by $50.50\%$ on the ProofWriter dataset.

We start with a discussion of related work (Section~\ref{sec:related-work}),
followed by preliminary information (Section~\ref{sec:background}).
%a detailed review of Logic-LM (Section~\ref{sec:logic-lm}),
In Section~\ref{sec:the-intermediate-language-problem} we introduce 
the \textit{intermediate language problem},
and in Section~\ref{section:experiments:experiments} we show our experiments.
We discuss our findings in Section~\ref{sec:discussion},
and close with a discussion of our limitations (Section~\ref{sec:limitations}).
%
\section{Related Work}
\label{sec:related-work}

The seminal \textit{Attention is All You Need}~\citep{vaswani_attention_2017} paper laid the foundations
for LLMs such as GPT-4~\citep{openai_gpt-4_2024}, Gemini~\citep{team_gemini_2024},
or Llama~\citep{grattafiori_llama_2024}.
%
Surprisingly, LLMs perform decent on reasoning tasks,
especially if prompted via a Chain-of-Thought (CoT) approach~\citep{wei_chain--thought_2022}.
This behavior is part of an emergent property of LLMs named \textit{in-context-learning}
or \textit{few-shot-learning}~\citep{shanahan_talking_2024}.
Although CoT achieves astonishing results on reasoning benchmarks,
it is not faithful\footnote{
\textit{Faithful} means that the reasoning chain corresponds to how the model arrives at the answer~\citep{lyu_faithful_2023}.}
\citep{lyu_faithful_2023}.
Further, it is argued that not only is the reasoning not faithful
but also, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
and, therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks''~\citep{panas_can_2024}.
Related results show that LLMs do not
acquire systematic problem solving skills~\citep{dziri_faith_2023}.
%
%Earlier language-models were constructed by recurrent neural networks~\citep{kombrink_recurrent_2011}.
%Later work already used the attention mechanisms~\citep{luong_effective_2015},
%however the seminal \textit{Attention Is All You Need}~\citep{vaswani_attention_2017} paper
%laid the foundations for the Large Language Models (LLMs) as we know of today.
%An overview is given in~\citep{wang_history_2024},
%
%On a societal side, the public release of GPT-3~\citep{brown_language_2020} demonstrated LLMs capability to an audience outside of the AI community.
%GPT-4~\citep{openai_gpt-4_2024} improved upon GPT-3 on several metrics, but also has a significant larger size.
%Other popular LLMs include
%Llama~\citep{grattafiori_llama_2024},
%Gemini~\citep{team_gemini_2024}, or
%Mistral~\citep{jiang_mistral_2023}.
%
%Truly surprising for LLMs is that they perform decent in reasoning tasks,
%especially if prompted via a Chain-of-Thought (CoT) approach~\citep{wei_chain--thought_2024}.
%CoT is an example of an emergent property of LLMs, 
%which is commonly referred to as \textit{in-context-learning}, 
%or \textit{few-shot-learning}~\citep{shanahan_talking_2024}.
%Although CoT achieves astonishing results on reasoning benchmarks,
%it is not faithful
%\footnote{\textit{Faithful} means that the reasoning chain corresponds to how the model arrives at the answer~\citep{lyu_faithful_2023}.}
%\citep{lyu_faithful_2023}.
%Further it is argued that not only is the reasoning not faithful,
%but further, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
%and therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks.''~\citep{panas_can_2024}.
%

The logical reasoning capability of LLMs is measured with datasets such as
the ProntoQA~\citep{saparov_language_2023},  
the ProofWriter~\citep{tafjord_proofwriter_2021}, or
the \textit{FOLIO}~\citep{han_folio_2024} dataset.
%
%For measuring the logical reasoning capabilities of LLMs multiple datasets have been proposed.
%The \textit{ProntoQA}
%\footnote{Proof and Ontology-Generated Question-Answering}
%is a recent synthetically generated dataset~\citep{saparov_language_2023}.
%Alternative datasets include the \textit{ProofWriter}~\citep{tafjord_proofwriter_2021},
%\textit{AR-LSAT}~\citep{zhong_analytical_2022},
%and the \textit{LogicalDeduction}~\citep{srivastava_beyond_2023}
%dataset.
%\citep{lampinen_language_2024} introduced Syllogism, Wason, and NLI datasets for LLMs.
%
Improving LLM's reasoning capability was approached by different angles.
\citet{geva_injecting_2020} try to improve numerical capabilities
by injecting additional numerical data in the pre-training phase
and further fine-tune the model.
Other approaches focus on fine-tuning~\citep{yang_generating_2022}.
However, it was argued that these approaches fail to address the inherent inability of LLMs to reason 
mathematically~\citep{panas_can_2024}.

Neurosymbolic AI~\citep{garcez_neurosymbolic_2023} approaches offer an alternative to the pure sub-symbolic approaches.
Examples include differentiable logic~\citep{badreddine_logic_2022},
designing neural networks that act as Turing machines~\citep{siegelmann_computational_1995},
or visual question answering with logic-programming and
deep learning~\citep{eiter_logic-based_2023}.
%or combining learning with logic-programming~\citep{yang_neurasp_2020}.
For LLM logical reasoning tasks, 
\textit{Logic-LM}~\citep{pan_logic-lm_2023} is a neurosymbolic method that
combines LLMs with symbolic solvers.
%by combining LLMs with symbolic solvers.
The studied solvers include a Prolog~\citep{korner_fifty_2022}, First-Order-Logic (FOL)~\citep{enderton_mathematical_1972}, Constraint-Satisfaction-Problems~\citep{kumar_algorithms_1992}, and a Satisfiability-Problem~\citep{cook_complexity_1971} solver.
Implementation-wise, Logic-LM uses Python libraries for these solvers.
For Prolog they use \textit{Pyke}~\citep{frederiksen_applying_2008},
for SMT solving (SAT) they use \textit{Z3}~\citep{de_moura_z3_2008},
for FOL they use \textit{Prover9}~\citep{mccune_prover9_2010},
and for constraint solving they use the \textit{Python-constraint}~\citep{niemeyer_python-constraint_2024} library.
Logic-LM++~\citep{kirtania_logic-lm_2024} claims to improve on Logic-LM
by adding an improved self-refinement module that
takes more solver information into account.
\citet{lam_closer_2024} acknowledge performance differences between solvers
but fail to identify that these stem from the chosen intermediate language.
For knowledge based systems previous research shows that different query languages have an
impact on LLM understanding~\citep{liu_how_2024}.

Differing from these approaches,
we study the impact of the used syntax inherent to the intermediate language of 
neurosymbolic logical reasoners.
%symbolic formalisms for logical reasoning tasks.
%This paper discusses the effect of the used syntax inherent in different symbolic formalisms.
In particular, this paper contrasts the syntax used by Logic-LM, to Pyke's
and Answer Set Programming's (ASP) syntax.
Answer Set Programming (ASP)~\citep{gelfond_logic_2002} is a declarative problem-solving paradigm.
As our ASP solver we use Clingo~\citep{kaminski_how_2023} due to its
readily available Python support.
%
\section{Preliminaries}
\label{sec:background}
%
We consider LLMs as black-box next-token predictor machines.
This means that given the token vector (array) $\vec{t}$, 
they select the token $t$ in token-space $\mathcal{T}$ with the maximum predicted value:
\begin{align*}
    f(\vec{t}) = \argmax_{t \in \mathcal{T}} p(t|\vec{t})
\end{align*}

Take for example the token-space $\mathcal{T}_c = \{\text{dead}, \text{alive}\}$ (reduced for this example),
and the tokens $\vec{t} = \left(\text{Schr√∂dinger's},\text{cat},\text{is} \right)$.
Then, provided the LLM has the following $p$ values
%\footnote{We chose alive with a higher percentage, as in our nonstatistically relevant experiments,
%GPT-4o (tested on 16.12.2024) chose \textit{alive} in 10 out of 10 trials.}
:
$\left( p(\text{alive})=0.51, p(\text{dead})=0.49 \right)$,
we obtain $f(\vec{t}) = \text{alive}$.

\subsection{Chain-of-Thought (CoT) prompting}

Chain-of-Thought (CoT) prompting is an \textit{in-context-learning}
technique that has applications ranging from helping 
LLMs to express their uncertainty~\citep{xiong_can_2024},
to improving the reasoning capabilities of LLMs on reasoning datasets~\citep{wei_chain--thought_2022}.
CoT nudges the LLM to mimic a human reasoning chain,
where we show an example adapted from the ProntoQA dataset~\citep{saparov_language_2023} as used for Logic-LM~\citep{pan_logic-lm_2023}:
%
%CoT is then enabled by adding the following listing as a pre-prefix to the prompt.
%
\begin{lstlisting}
The following example showcases the line of reasoning you have to follow:
---- Question ----
Each cat is a carnivore. Fae is a cat.
True or false: Fae is a carnivore
---- Reasoning ----
Fae is a cat. Each cat is a carnivore. So Fae is a carnivore.
\end{lstlisting}
%
We show a full CoT example in the Appendix.
%in Section~\ref{sec-appendix:CoT-Full-Prompt}.
Reasoning chains are \textit{faithful}
whenever the result follows from the individual steps in the reasoning chain.
%The examples shown so far display \textit{faithful} reasoning chains.
%Meaning that the answer is in correspondence with the reasoning chain.
However, LLM's reasoning chains are \textit{non-faithful} in general~\citep{lyu_faithful_2023}.

\subsection{Logic Programming}

As intermediate languages between LLMs and symbolic reasoning, we consider Logic Programming languages.
In more detail, we consider Answer Set Programming (ASP)
and the intermediate language used for the Python knowledge engine (Pyke).
We start with introducing ASP, and continue to introduce Pyke.

ASP is a declarative rule-based paradigm
commonly used for modeling and solving complex planning or scheduling problems~\citep{abels_train_2021}.
We provide a brief summary of the main concepts of ASP.
For details, we refer to~\citep{eiter_answer_2009}.
%Detailed accounts are given in~\citep{dantsin_complexity_2001,eiter_complexity_2007}.
An ASP program $\Pi$ consists of rules $r \in \Pi$ of the form
%\footnote{Syntactically, ``$\leftarrow$'' is replaced with ``:-'',
%and a rule is terminated with a ``.'' when $r$ is used in an implementation.
%So
%``$\textit{cat}(X) \leftarrow \textit{carnivor}(X)$''
%becomes
%``$\textit{cat}(X) \text{ :- } \textit{carnivor}(X).$''
%}
:
\[
    p_1(\boldsymbol{X}_1) \lor \ldots \lor p_l(\boldsymbol{X}_l) \;\; \text{:-} \;\; p_{l+1}(\boldsymbol{X}_{l+1}), \ldots, p_m(\boldsymbol{X}_m), \neg p_{m+1}(\boldsymbol{X}_{m+1}), \ldots, \neg p_n(\boldsymbol{X}_n)
\]
We call $p_i$ a literal, and $\boldsymbol{X}_i$ its term vector.
Both stand for abstract concepts with an arbitrary naming.
For the exact syntax definition, see~\citep{calimeri_asp-core-2_2020}.
For our purposes, we consider term vectors consisting of variables, integers, and lower- and upper-case letters.
Let $p_i(\boldsymbol{X}_i) \text{ for } 1 \leq i \leq l$ be the rule's head $H_r$,
$p_i(\boldsymbol{X}_i) \text{ for } {l+1} \leq i \leq m$ be the positive body $B_r^+$,
and $p_i(\boldsymbol{X}_i) \text{ for } {m+1} \leq i \leq n$ be the negative body $B_r^{-}$.
Semantically, a rule fires (meaning (one $h \in H_r$) $H_r$ holds), whenever its body is true.
A body is true whenever all literals of $B_r^+$ hold, but no literal of $B_r^{-}$ holds.
%Semantics is defined in the Appendix in Section~\ref{sec:appendix:subsec:asp}.
\textit{Grounding} replaces variables by their concrete domain values.
Grounding is computationally very expensive and a topic of current research~\citep{beiser_bypassing_2024}.
The solutions to a grounded program are called answer sets.

\textit{Pyke} defines rules and facts.
Rules in Pyke express \textit{if}-\textit{then} statements.
Their syntax is Python like.
Semantically Pyke works with forward, or backward chaining algorithms.
%
%
\subsection{Neurosymbolic LLM Reasoning}
\label{sec:logic-lm}
%\begin{minipage}{0.49\textwidth}
%\end{minipage}
%
\begin{figure}[t]
    %
    %\includegraphics[width=14.5cm]{imgs/neurosymbolic_schematics.pdf}
    \includegraphics[width=14.5cm]{imgs/schematics-in-context-learning-02.pdf}
    %
    \caption{
        The Neurosymbolic approach to solving logical reasoning problems.
        Provided a natural language reasoning \textit{problem formulation},
        an LLM translates this into a \textit{formal intermediate language}.
        A symbolic reasoner subsequently computes a solution to the problem.
        Finally, the LLM re-translates the symbolic solution into a human readable format (\textit{output}),
        or if the solution is already human readable (dotted-line), it is directly returned.
        \textit{In-context-learning} pre-prompts the LLM with suitable \textit{instructions} and a 
        \textit{correctly parsed example} to increase translation performance.
    }
    %
    \label{sec:logic_lm:neurosymbolic_schematics}
    %
\end{figure}
%
%
Logic-LM~\citep{pan_logic-lm_2023}
%\footnote{
%    We show the details of Logic-LM in the Appendix in %Section~\ref{sec:appendix:subsec:logic-lm}, and
%    in Figures~\ref{fig:logic-lm-overview} and~\ref{fig:logic-lm-framework}.
%}
is a neurosymbolic approach that integrates symbolic reasoners into the reasoning steps of LLMs.
It translates a natural language posed problem into its formal symbolic reasoner
acceptable formulation.
Subsequently, the symbolic reasoner solves the problem by obtaining a solution,
which can be either re-translated into natural language or directly outputted.
Logic-LM consists of three main modules, the \textit{problem formulator},
the \textit{symbolic reasoner}, and the \textit{result interpreter}.
%
%\paragraph{Problem Formulator}
The \textit{Problem Formulator}
takes the problem in natural language and translates it to a formal formulation.
%This effectively corresponds to the translation step of Figure~\ref{sec:logic_lm:neurosymbolic_schematics}.
%where they use the LLMs: GPT-3.5 turbo, GPT-3.5 text-davinci-003, and GPT-4.
Technically, they prompt the LLM by utilizing the in-context learning technique.
%by providing a full example translation.
%
%\paragraph{Symbolic Reasoner}
The \textit{Symbolic Reasoner}
uses the formal language of the problem formulator as an input to obtain a solution.
Logic-LM uses four different symbolic solvers: Pyke, Prover9, Python-constraint, and Z3.
The \textit{result interpreter} is called whenever a solution is produced by Logic-LM.
%On the other hand, if the formal language specification is syntactically correct,
%the \textit{Result Interpreter} is called.
It re-translates the symbolic output back into human-readable language.
%
%The result interpreter translates the symbolic result back into the output format specified by the problem formulation.
They implemented the result interpreter as a rule-based program.
However, they note that also an LLM can be used.
%For most tested symbolic reasoners Logic-LM uses specialized rule-based translations for this task,
%however, for outputs in the format from CSP solvers, they also test the usage of LLMs.
%Logic-LM uses the five different logical reasoning datasets for benchmarking their system
%introduced in Section~\ref{sec:related-work}.
A further sub-module is the \textit{self-refiner}, which takes solver syntax error
messages into account.

We adapt this setup by changing the solver accepted input language (or encoding)
to a (largely) arbitrary formal intermediate language. 
By doing that, we can adapt the syntax of the intermediate language to tweak LLM reasoning performance.
%
Figure~\ref{sec:logic_lm:neurosymbolic_schematics} depicts the high-level schematics of this approach.


\section{The Intermediate Language Problem}
\label{sec:the-intermediate-language-problem}
%
We introduce the intermediate language problem, which is the problem
of choosing a suitable \textit{intermediate language} for Neurosymbolic reasoning tasks,
using LLMs as a translation tool between natural and formal (intermediate) languages.
% for neurosymbolic reasoning tasks.
%It arises in the context of the neurosymbolic approach for solving formal reasoning tasks with LLMs.
%where LLMs
%are used as a translation procedure between natural and formal languages.
Intuitively, the intermediate language problem stems from two observations:
(i) LLMs are unable to separate syntax from semantics,
and (ii) the actual formal intermediate language is (largely) irrelevant for formal problems.
%We note that current neurosymbolic research, such as Logic-LM~\citep{pan_logic-lm_2023},
%Logic-LM++~\citep{kirtania_logic-lm_2024}, or~\citet{lam_closer_2024},
%seem to ignore these facts, while focusing on solver comparisons, or 
%additional self-refinement steps to improve performance.
We start by justifying our claim for the intertwined syntax and semantics of LLMs,
which we follow by an argument for the irrelevancy of the choice of the formal intermediate language.

\subsection{LLMs cannot separate Syntax from Semantics}

We base our claim \textit{LLMs cannot separate syntax from semantics} on both, experimental observations and theoretical considerations.
First observe that in logics, the representation (syntax) does not have an effect on semantics.
Take for example the syllogism chain in Equation~(\ref{sec:logic-lm:eq:syntax-semantics}).
%
\begin{align}
    \label{sec:logic-lm:eq:syntax-semantics}& \left( \forall x: A(x) \rightarrow B(X) \land \forall x: B(X) \rightarrow C(X) \right) \rightarrow \forall x: A(X) \rightarrow C(X)
\end{align}
%
This statement is a valid syllogism chain, where $A$, $B$, and $C$ stand for abstract concepts.
Therefore, logically, it does not matter whether $A$ is a \textit{cat} or a \textit{wumpus}.
This leads us to the following observation:
\textit{If LLMs are capable of separating syntax from semantics, then changing the representation in a logical reasoning task does not affect performance (accuracy)}.
%
In the following, we argue by contraposition\footnote{
    By contraposition we follow, that \textit{if changing the representation has an effect on performance (accuracy), then LLMs are not able to separate syntax from semantics}.}
that LLMs are not able to separate syntax from semantics.


In~\citet{saparov_language_2023} they showed experiments that compare realistic to fictional ontologies on LLMs.
Take for example, the two ontologies \textit{all cats are carnivores} (realistic),
and \textit{all tumpus are wumpus} (fictional).
The results show that LLMs achieve better results on realistic ontologies than on fictional ontologies.
This is in line with the research conducted in~\citet{lampinen_language_2024},
which discusses LLMs and human reasoning abilities.
Their conclusion is that as humans, LLMs are biased by the semantic context,
and as humans, LLMs perform better in realistic settings, than in fictional settings.
To demonstrate this, we show an example which adheres to the \textit{law of syllogism} (transitive reasoning, Equation~(\ref{sec:logic-lm:eq:syntax-semantics})),
which is similar to the research shown in~\citep{lampinen_language_2024}.
%
%To demonstrate this take the following (true) syllogism chains:
%
(1) Provided \textit{all cats are mammals}, and \textit{all mammals are animals}, then \textit{all cats are animals},
and (2) assumed \textit{all tumpus are wumpus}, and \textit{all wumpus are vumpus}, then \textit{all tumpus are vumpus}.
Syllogisms of type~(1) are more often considered as true
than syllogisms of type~(2) by both humans and LLMs.
However, logically both syllogism chains are the same (valid).
Further, our results from Section~\ref{section:experiments:experiments} are in line with these results.
These observations lead us to confirm that \textit{LLMs} are unable to separate syntax from semantics.
%To see this observe Equation~\ref{sec:logic-lm:eq:syntax-semantics}.
%
%
%From a black-box perspective, we base our claim on the already conducted research,
%as if LLMs would be capable of separating syntax from semantic,
%there would be no difference in accuracy between datasets containing fictious and real scenarios,
%with the same underlying syllogisms.

The possible underlying reasons for this problem are briefly discussed:
By taking a more mechanistic interpretability like perspective,
we speculate that the issue can be traced back to the current form of (word) embeddings.
It is known from research that an embedding vector has an
assigned meaning~\citep{mikolov_linguistic_2013}. % which is dependent on the representation.
Observe that due to the architecture of LLMs this holds true in LLMs as well~\citep{bricken_towards_2023}.
Conversely, we speculate that any embedding approach that respects a separation of syntax and semantics
should be able to detach meaning from the representation.

\subsection{Intermediate language is independent of the solver}
%\subsection{Formal Problems are (largely) independent of their Representation Language}

We take a computational complexity theory point of view, for arguing that 
%\textit{formal problems are (largely) independent of their representation language}.
we can effectively choose an arbitrary intermediate language.
%\textit{choosing (largely)}
Therefore, we briefly introduce some necessary concepts of complexity theory,
where a detailed account of this field is given in~\citep{papadimitriou_computational_1994}.
A first crucial observation to our discussion is the distinction between a formal \textit{problem},
and a \textit{solver}.
A formal problem $\mathcal{P}$ is an instance $\mathcal{I}$, with an attached question $\mathcal{Q}$.
On the other hand, a solver takes a problem $\mathcal{P}$,
encoded in a solver specific intermediate (representation) language,
and produces a solution $\mathcal{S}$.
%
Take for example the famous \textit{boolean satisfiability problem} (SAT).
Its instance consists of a propositional ($\approx$ boolean) formula $\mathcal{I}$,
such as $\left(a \land (b \lor c) \land \neg a \right)$,
and its attached question $\mathcal{Q}$ is:
Does there exist a satisfying assignment for $\mathcal{I}$\footnote{
    For our example the answer is no, due to $a \land \ldots \land \neg a$.
}?
Solvers on the other hand take the instance $\mathcal{I}$ encoded in a specific way, be it the 
\textit{DIMACS} format\footnote{See the Appendix for an example.},
in a logic programming representation,
or hypothetically in the unicode representation from above.
This is our first observation for our argument.
Note that for SAT highly efficient solvers are available, such as Z3~\citep{de_moura_z3_2008}.

%But how can we solve SAT automatically?
%For this we can use \textit{suitable} solvers, where for SAT
%specialized solvers are readily available that are highly efficient.
Taking a complexity theoretic perspective we note that SAT is in \textsf{NP} (Actually \textsf{NP}-complete~\citep{cook_complexity_1971}).
Remind yourself that \textsf{NP} means (intuitively) that the problem is decidable by a non-deterministic
Turing machine in polynomial time.
This definition gives rise to the concept of a complexity class $\mathcal{C}$,
which intuitively encodes a set of problems
that all can be solved by a certain type of turing machine, under certain resource constraints.
The last crucial concept we need to introduce for our argument is the concept of a \textit{reduction}\footnote{
Polynomial time, many-one reduction.
}.
Intuitively, a reduction takes a problem $\mathcal{P}$, and efficiently \textit{translates} it into another 
problem $\mathcal{P}'$, s.t. the solutions exactly match.

Such a reduction can be of high practical use.
Assume that a problem $\mathcal{P}$ has no efficient specialized solver,
but there exists an efficient reduction to SAT.
Then by reducing $\mathcal{P}$ to SAT, $\mathcal{P}$ can be solved efficiently.
Therefore, every solver suitable for solving SAT problems,
can solve problems that can be reduced to SAT. 
%
So, coming back to our hypothesis that for many formal problems the choice of formal intermediate language 
is irrelevant, 
the concept of the reduction is our second argument.

We summarize that for a formal problem,
one efficient solver for a complexity class\footnote{In more detail for the $\mathcal{C}$-complete problems. We refer to the Appendix for this definition.
%For the definition of complete problems see Appendix Section~\ref{section:appendix:hardeness-and-completeness}.
}
is sufficient (second argument),
and this solver's input (representation) is implementation specific.
%
%So given a formal problem expressed in a representation language, we can translate it (efficiently)
%to another language while keeping semantics.
%
For LLMs we follow that for a logical reasoning task,
we can pick an intermediate language that is beneficial for LLM translation.
In more detail, we are only required to pick a language that is strong enough to represent 
the complexity theoretic problem class of our reasoning problem.
%observe that we are not (necessarily) restricted to translate our input problem to a particular solver's representation language.
%Which implies that we should use a representation language that is beneficial for 
%LLM translation tasks.
%from an LLM's perspective,
%as we can subsequently reduce the formal problem into a problem where a fast solver is available.

\subsection{Choosing a suitable intermediate language}

From this discussion we follow two crucial observations:
First, one should pick a representation language that is inherently beneficial for LLM translation tasks.
So, as an example, we think it is more suitable to encode the problem:
``If it rains, then the street is wet.'' into a propositional logic formulation
($\textit{Rains} \rightarrow \textit{WetStreet}$)
than into a bit-representation.
%, which is subsequently given to a Turing machine.
Second, the actual encoding in the representation language should provide supplementary information to the 
LLM.
%, which is beneficial for translation.
Going back to the example, an encoding such as $\textit{Rains} \rightarrow \textit{WetStreet}$, 
provides more supplementary information to the LLM than $\textit{p1} \rightarrow \textit{p2}$.
In our experimental evaluation we demonstrate that both observations hold.
%
%In our experimental evaluation, we check both the inherent suitability of an
%intermediate language and the impact of the encoding in the language
%o the performance.

\section{Experiments}
\label{section:experiments:experiments}

We performed a set of experiments to empirically verify our \textit{intermediate language problem}
hypothesis,
i.e., that the intermediate language has an effect on solving performance.
In the following, we will briefly go over our benchmark scenarios of different intermediate languages 
and our benchmark setup
before coming to our results.


\subsection{Intermediate Languages and Additional Context}
\label{sec:experiment-scenarios-description}


We conducted experiments on $11$ scenarios.
The scenarios differ by the used \textit{in-context-learning} techniques, mainly in which intermediate language 
is used and what additional context is provided.
Our baselines are the scenarios used in Logic-LM.
Namely the \textit{standard} scenario, which is direct prompting of the LLM,
the \textit{CoT} scenario, which is the chain-of-thought prompting of the LLM,
and the \textit{Logic-LM} scenario without self-refiner,
which uses a self-defined intermediate language to communicate with the Pyke solver.
Our other $8$ scenarios can be split into two groups:
Those with Pyke ($4$ scenarios), and those with ASP ($4$ scenarios).
The differences in the scenarios of each group manifest themselves by different levels of additional
supplementary information provided to the LLM.

To showcase how the neurosymbolic scenarios differ, we show snippets of example prompts.
Note that we used the \textit{in-context-learning} approach for these scenarios.
All prompts include high-level information about what the LLM should do,
which is followed by an example problem translation into the desired solver language.
Consider for this the following example (snippet) from the ProntoQA,
which we use as our running example to demonstrate the different example translations:
\begin{lstlisting}
Problem: Wren is a tumpus. Each tumpus is a wumpus.
Wumpuses are brown. Each wumpus is not metallic.
Question:
Is the following statement true or false? Wren is not metallic.
\end{lstlisting}
%
When translating the above-stated
%Doing the translation of the above stated
problem by hand to ASP, one has to define suitable predicates,
terms, rules, and a query.
Suitable predicates are \textit{wumpus(X)},  \textit{brown(X)}, and \textit{metallic(X)}.
Terms are the variable $X$ and \textit{wren},
and rules encode the relationships, such as \textit{Each tumpus is a wumpus}
by \textit{wumpus(X) :- tumpus(X)} in ASP.
Therefore, for the ASP scenario \textit{Text} we use the following in-context-learning example
translation:%\footnote{The full example is shown in the appendix.}:
\begin{lstlisting}
Facts:
tumpus(wren).
Rules:
wumpus(X):-tumpus(X). brown(X):-wumpus(X). -metallic(X):-wumpus(X).
Query:
-metallic(wren).
\end{lstlisting}
%
Observe that the \textit{Text} scenario keeps the names of the predicates,
but does not explicitly state that parts are code.
Scenario \textit{Prog.} (Programming) wraps the code snippets into markdown code snippets ($```$).
Scenario \textit{No-C.} (No-Context) changes \textit{Text}, by changing the predicate names
to \textit{p1} (\textit{wumpus}), \textit{p2} (\textit{brown}), and \textit{p3} (\textit{metallic}).
On the other hand, \textit{Comm.} (Comment) changes \textit{Prog.}, by additionally adding 
comments with \textit{supplementary context information} from the problem formulation.
We show in Figure~\ref{sec:logic-lm:fig:supplementary-information} examples for these described scenarios.
 
\begin{figure}[t]
    \includegraphics[width=14.5cm]{imgs/schematics-supplementary-information.pdf}
    \caption{
        Experiment scenarios with increasing level of supplementary context
        information for in-context-learning.
        \textit{No-Context} obfuscates the predicate names,
        \textit{text} is a direct-translation of the problem,
        \textit{program} encapsulates the code in markdown code syntax, and
        \textit{comment} adds the relevant information from the problem formulation.
        Examples shown in ASP syntax.
    }
    \label{sec:logic-lm:fig:supplementary-information}
\end{figure}

In the translation to Pyke, the general steps are the same as for the translation to ASP.
However, as Pyke's format is lengthier than ASP's, we show in the following listing the 
translation of \textit{Each tumpus is a wumpus} to Pyke's format,
while we show example encodings for each scenario in the Appendix.
The next listing depicts the \textit{Text} scenario:
%In more detail, the listing depicts the \textit{Text} scenario.
%
\begin{lstlisting}
fact1
	foreach
		facts.Tumpus($x, True)
	assert
		facts.Wumpus($x, True)
\end{lstlisting}
%
As before, the \textit{Prog.} scenario encapsulates the rules, facts and queries in markdown code snippets,
the \textit{No-C.} scenario removes associations by enumerating the predicate names,
and the \textit{Comm.} scenario adds additional supplementary comments to the problem formulation.


\subsection{Benchmark Setup}

%We demonstrate the significance of choosing a suitable representation language by contrasting 
%the languages of two symbolic solvers, which are both in the logic programming paradigm: Pyke and ASP (with the Clingo solver).
%We experiment on different encodings for both Pyke and ASP, with varying degree of supplementary information
%provided to the LLM.
%
We used the ProntoQA and ProofWriter datasets, as their problems contain fragments of
first-order logic that are solvable by ASP.
The datasets were used in the Logic-LM configuration,
where the ProntoQA dataset was used in the fictional characters variant in the 5 hops version with 500 samples and 
the ProofWriter dataset with 600 samples and 5 hops reasoning depth.
Our experiments were conducted on an adapted Logic-LM implementation
that features an ASP symbolic solver and result interpreter based on Clingo,
a new\footnote{The original Logic-LM implementation uses a Pyke solver and interpreter with a self-defined intermediate language. We changed it to the actual Pyke syntax, while we use the original one as the Logic-LM baseline.}
Pyke solver and interpreter, and
different levels of additional context according to the previous section.
As we do not compare the understandability of the
syntax errors messages of different solvers, we disabled the self-refinement module.
For the LLM we used GPT-4o-mini without any modifications and temperature $0$. 
%
We measured the number of correct syntactical- ($\#\text{EXEC}$)
and the number of correctly solved instances ($\#\text{TRUE}$).
An instance is syntactically correct, whenever the produced output of the LLM adheres to the solver input language.
Similarly, we consider a problem as correctly solved whenever the solver input is syntactically correct,
and the solver produces the correct answer\footnote{
    Our measurement is stricter than that of Logic-LM, as Logic-LM
    performs a random guess on not syntactically correct inputs.}.
From these we compute the \textit{execution-rate}, which is the fraction of correct syntactical outputs (Exec-Rate, $\frac{\#\text{EXEC}}{\#D}$),
\textit{execution-accuracy}, which is the fraction of correctly solved instances of all syntactically correct ones (Exec-Acc, $\frac{\#\text{TRUE}}{\#\text{EXEC}}$),
and \textit{overall-accuracy}, which is the fraction of correctly solved instances over the entire dataset (Overall-Acc, $\frac{\#\text{TRUE}}{\#\text{\#D}}$).

\newpage
\subsection{Results}
%
\begin{table}[t]
    \resizebox{14cm}{!}{
    \centering
    \begin{tabular}{ll|ccc|ccc}
    \toprule
        \multicolumn{2}{c}{Method} & \multicolumn{3}{c}{ProntoQA} & \multicolumn{3}{c}{ProofWriter} \\

        \cmidrule(lr){3-5} \cmidrule(lr){6-8}           &  & Overall-Acc & Exec-Rate & Exec-Acc & Overall-Acc & Exec-Rate & Exec-Acc\\
    \midrule
        \multirow{4}{*}{Baseline} & Standard        & 58.60 & / & 58.60 & 19.33 & / & 19.33 \\
        &CoT             & 72.80 & / & 72.80 & 19.00 & / & 19.00 \\
        &Logic-LM & 74.40 & \textbf{100.00} & 74.40 & 00.00 & 00.00 & 00.00 \\ \midrule
        %
        \multirow{4}{*}{Pyke} & No-C.    & 41.60 & 82.00 & 50.73 & 42.67 & 81.50 & 52.35 \\
        &Text     & 75.40 & 99.00 & 76.16 & 47.33 & 65.17 & 72.63 \\
        &Prog.    & 93.80 & 99.60 & 94.12 & 55.17 & 78.67 & 70.13 \\
        &Comm.    & 91.00 & 99.80 & 91.18 & 58.00 & 78.33 & 74.04 \\ \midrule
        %
        \multirow{4}{*}{ASP} & No-C.      & 42.40 & 86.00 & 49.30 & 49.17 & \textbf{95.33} & 51.57 \\
        &Text      & 70.40 & 90.40  & 77.88 & 54.83 & 82.17 & 66.73 \\
        &Prog.      & 81.20 & 91.80  & 88.45 & 53.83 & 79.33 & 67.86 \\
        &Comm.      & \textbf{95.60} & 97.00  & \textbf{98.56} & \textbf{69.83} & 90.17 & \textbf{77.45} \\
    \bottomrule
    \end{tabular}
        }
        \caption{
            Performance is dependent on the intermediate language.
            Additional context information in the intermediate language
            yields performance improvements.
            %and the additional information provided
            %in \textit{in-context-learning}.
            Detailed results of the experiments, contrasting our baselines, to Pyke and ASP intermediate languages as described in Section~\ref{sec:experiment-scenarios-description}.
            %Further, we compare different levels of supplementary information
            %in \textit{in-context-learning}, ranging from 
            %\textit{No-C.} (No-Context), \textit{Text} (default),
            %\textit{Prog.} (code marked by markdown syntax), and
            %\textit{Comm.} (comments for additional information).
            All experiments were conducted on GPT-4o-mini with temperature 0.
            We measure overall-accuracy, execution-rate, and execution-accuracy
            on the ProntoQA and ProofWriter datasets.
            All values are shown in $\%$.
        }
        \label{tbl:asp-table-our-results}
\end{table}
%
%

We show in Table~\ref{tbl:asp-table-our-results} the results of our experiments\footnote{
Supplementary material available under: \\
\url{https://osf.io/xnr49/?view_only=f24c3db4f48545fab49daedb07a683e5}}.
As discussed above, the main aim of our experiments is to empirically verify our 
\textit{intermediate language problem} hypothesis from Section~\ref{sec:the-intermediate-language-problem}.
Secondary, we analyze which techniques help in obtaining better results.
To verify our hypothesis,
observe the pairs of Pyke and ASP for 
$\{\text{No-C.}, \text{Text}, \text{Prog.}, \text{Comm.}\}$.
Observe the difference in execution-rate
%for $\{\text{Text}, \text{Prog.}, \text{Comm.}\}$ 
for ProntoQA:
which is $\{82.00,99.00,99.60,99.80\}$ for Pyke, and $\{86.0,90.40,91.80,97.00\}$ for ASP.
However, not only the execution-rate differs, but also the execution-accuracy, which is 
$\{50.73,76.16, 94.12, 91.18\}$ for Pyke, and $\{49.30,77.88, 88.45, 98.56\}$ for ASP.
Further, observe the differences for ProofWriter in execution-rate,
which is $\{81.50,65.17,78.67,78.33\}$ for Pyke and
$\{95.33,82.17,79.33,90.17\}$ for ASP
and the difference in execution-accuracy for Pyke, which is
$\{52.35,72.63,70.13,74.04\}$ and $\{51.57,66.73,67.86,77.45\}$ for ASP.
Also the overall-accuracy measures differ.

Next we show the impact of the increase in additional context (Figure~\ref{sec:logic-lm:fig:supplementary-information}).
%generally speaking, leads to an increase in performance (overall-accuracy and execution-accuracy).
We observe a monotone increase in overall-accuracy for the ProntoQA dataset when using 
ASP from $42.40\%$ (\text{No-C.}) to $95.60\%$ (Comm.), which is a difference of $53.20\%$.
The execution-rate difference between No-C. and Comm. is $49.26\%$.
For Pyke the difference is still striking, as it goes from $41.60\%$ (No-C.) to $91.00\%$ (Comm.), however it is not monotone.
The ProofWriter dataset shows a smaller increase from $49.17\%$ (No-C.) to $69.83\%$ for ASP
and $42.67\%$ to $58.00\%$ for Pyke.

Comparing the baseline measures to the other experiments we observe an increase in 
overall-accuracy for both the ProntoQA and ProofWriter datasets.
For ProntoQA the increase from Baseline (Logic-LM) to ASP (Comm.)
is $21.20\%$ in overall-accuracy and $24.16\%$ in execution-accuracy,
whereas for ProofWriter the increase from Baseline (Standard) to ASP (Comm.) is
$50.50\%$ in overall-accuracy and $58.12\%$ execution-accuracy.

\subsection{Interpretation}

We can confirm our intermediate language hypothesis, as we were able
to show differences in overall- and execution-accuracy between different
intermediate languages.
In addition to that, our results suggest that an increase in context information 
leads to an increase in accuracy.
This can be seen by the increase in accuracy when going from the no-context
scenario to the comments scenario.
The execution-rate remains relatively unaffected by different intermediate languages.

%
%
\section{Discussion}
\label{sec:discussion}

In this paper we discussed the impact of the representation language for 
solving logical reasoning problems in the neurosymbolic approach for LLMs.
Using the neurosymbolic approach one uses LLMs for translating a natural-language-posed problem
into an intermediate formal language, which is subsequently solved by a symbolic solver.
This is enabled by the \textit{in-context-learning} capability of LLMs.
Previous results showed that with this approach the logical reasoning capability of LLMs
can be greatly enhanced~\citep{pan_logic-lm_2023}.

Although these approaches discuss the impact of different solvers, they fail to recognize that
the intermediate formal language is the main factor that impacts performance in terms of overall- and execution-accuracy.
This stems from two facts:
(i) LLMs suffer from concept entanglement, as do humans~\citep{lampinen_language_2024}.
Intuitively, this means that when logical reasoning problems are posed in a counter-intuitive way,
humans (and LLMs) perform worse (on average).
(ii) Formal problems (like a logical reasoning task) are independent of 
their solver, and consequently of their representation language.

From this analysis we introduced the \textit{intermediate language} problem,
which is the problem of choosing a suitable formal language for the 
symbolic solver.
We demonstrated its existence through experiments and additionally showed
that additional context in the in-context learning tends to yield better results.
By using our highest additional context configuration, we were able to 
beat the current state-of-the-art.

%Lastly, we note that we were able to beat the state-of-the-art by improving overall-accuracy 
%for both ProntoQA ($18.20\%$) and ProofWriter ($49.84\%$),
%for GPT-4o-mini.
%
%Combining (i) and (ii) we hypothesize in the \textit{intermediate language problem}
%that (1) there are representation languages that are more, or less appropriate for a neurosymbolic
%approach,
%and that (2) the encoding (for example choosing appropriate names for predicates) is important.
%
%We verify our hypothesis in our experiments, by contrasting two logic programming intermediate languages:
%the Python Knowledge Engine (Pyke) and Answer Set Programming (ASP).
%We observe both differences in execution-rate (up to $16.6\%$ for the ProofWriter dataset)
%and execution-accuracy (up to $9.34\%$ for the ProofWriter dataset).
%However, more striking is the difference in the used encoding,
%where we observe both execution-rate (up to $15.60\%$ for the ProntoQA dataset),
%and execution-accuracy (up to $52.32\%$ for the ProntoQA dataset) differences.
%
%Secondary, we observe that for improving the execution-accuracy it is beneficial to 
%include more appropriate information in the encoding.
%We observe an accuracy improvement of up to $52.32\%$ between ASP (No-Context),
%and ASP (Comments).
%
%We think that the investigation of a suitable intermediate language is a promising future direction
%for improving the logical reasoning capability of LLMs.
%In the long term, we want to conduct research on a further understanding of the concept entanglement
%problem in LLMs.
%In line with this, we would like to investigate other approaches of separating semantic
%and syntax in LLMs, and machine learning in general.
%
%
\section{Limitations}
\label{sec:limitations}
%
Although we are confident in our observations about the intermediate language
problem and the general tendency that adding more context information to the 
in-context learning technique will improve logical reasoning capabilities,
we are also aware of our experimental and inherent limitations.

Experiment-wise, we conducted experiments on the GPT-4o-mini model for economic and
environmental reasons.
We expect a significant increase in all scores when other,
more advanced or bigger, models are used,
such as GPT-4o, GPT-o1, GPT-o3, or Gemini 2.
The increase in performance can render the accuracy differences on the ProntoQA and
ProofWriter datasets insignificant.
We are still confident that when moving to harder datasets, such as FOLIO, or when using different intermediate languages, the effects can be reproduced.

%
%
Inherent to the neurosymbolic approach is the inclusion of a separate symbolic
reasoning system.
However, in an ideal integration of a symbolic solver into an LLM,
the symbolic solver's details are hidden from the end-user.
We mean by that the hiding of the symbolic solver from the end-user in the
chat and in the API and the automatic decision of the LLM, when the usage of the
symbolic solver is possible and beneficial.
Nonetheless, the symbiosis of the LLM with the symbolic solver
into one coherent system that
\textit{automatically detects when the usage of the symbolic solver is beneficial},
might pose a major challenge.
% and we see it as future work.
%
%
%We think that with the introduction of the intermediate language problem
%we have set the stage for future neurosymbolic LLM tasks.
%Further, we are confident that our observation of adding suitable problem context
%in the in-context learning step will improve the reasoning capability.
%Still, we are aware that 
%Still, we are aware that more research has to be conducted 
%However, we are aware that 
%However, more research has to be conducted towards finding a suitable
%intermediate language.
%
% confident that adding more context will generally speaking help
% more research for better intermediate language
% 
% Add GPT-4o-mini considerations for baseline comparisons
% Include self-refinement baselines
% Other LLMs
% Also other reasoning improvements!
%
% Add in preliminaries:
%   - Intuitive definition for intermediate language
%   - Formal language
%   - And how we use it








\clearpage

%\subsubsection*{Author Contributions}
%If you'd like to, you may include  a section for author contributions as is done
%in many journals. This is optional and at the discretion of the authors.

%\subsubsection*{Acknowledgments}
%Use unnumbered third level headings for the acknowledgments. All
%acknowledgments, including those to funding agencies, go at the end of the paper.


\bibliography{preprint}
\bibliographystyle{preprint}

\clearpage
\appendix
\section{Appendix}

\subsection{Additional Experimental Details}

%\url{https://www.dropbox.com/scl/fo/85apgzyycy2m5bdoj7pu5/AJV27j7Viq7X8bafRCPpwlU?rlkey=vskvldivlke6i7xmqpno7fs1p&st=h9p08wvr&dl=0}.
We show in Figures~\ref{section:appendix:fig:error-histogram}--\ref{sec:asp_logic_lm:fig:proofwriter_plot}
our analysis of the distribution of errors.
No distribution is apparent.
There is no example, where all methods fail.
However, there are some where all methods succeed.

\begin{figure}[t]
    \includegraphics[width=14.5cm]{imgs/prontoQA_histogram.pdf}
    \caption{
    There is no apparent pattern which observations are inherently hard, or easy.
    No datapoint fails for all methods, but there are some datapoints that work for all methods.
    Error histogram of the ProntoQA dataset.
    X-axis represents the observation number of the dataset,
    the y-axis the number of errors across all experiment methods (11 methods, maximum of 11).
    Red represents a syntax error in the neurosymbolic approach,
    while blue represents a reasoning error.
    %
    }
    \label{section:appendix:fig:error-histogram}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=14.5cm]{imgs/prontoqa_plot.pdf}
    \caption{
        There is no apparent pattern where the methods struggle, except for the general observations already discussed in the main-part.
    Error plot for the ProntoQA dataset.
    X-axis represents the observation number of the dataset.
    The Y-axis represents the 11 different methods.
    A blue tick (line) represents a failed instance due to a reasoning error,
    a red tick a syntax error.
        .}
    \label{sec:asp_logic_lm:fig:prontoqa_plot}
\end{figure}
%
%

\begin{figure}[t]
    \includegraphics[width=14.5cm]{imgs/proofWriter_histogram.pdf}
    \caption{
    There is no apparent pattern which observations are inherently hard, or easy.
    Some datapoints fail for all methods, and there are some datapoints that work for all methods.
    Error histogram of the ProofWriter dataset.
    X-axis represents the observation number of the dataset,
    the y-axis the number of errors across all experiment methods (11 methods, maximum of 11).
    Red represents a syntax error in the neurosymbolic approach,
    while blue represents a reasoning error.
    %
    }
    \label{section:appendix:fig:error-histogram-proof-writer}
\end{figure}

\begin{figure}[t]
    \includegraphics[width=14.5cm]{imgs/proofWriter_plot.pdf}
    \caption{
        There is no apparent pattern where the methods struggle, except for the general observations already discussed in the main-part.
    Error plot for the ProofWriter dataset.
    X-axis represents the observation number of the dataset.
    The Y-axis represents the 11 different methods.
    A blue tick (line) represents a failed instance due to a reasoning error,
    a red tick a syntax error.
        .}
    \label{sec:asp_logic_lm:fig:proofwriter_plot}
\end{figure}


\subsubsection{Why do Parsing Errors Occur?}

A qualitative analysis of the errors results in an observation that the (strong) negation in our ASP encoding
seems to be hard.
Take for example ProntoQA example number 94 (ASP with Comments).
There ChatGPT produces the erroneous results:
%
\begin{lstlisting}
"Facts:\n```\n% Max is [..] -not(spicy(X)) :- vumpus(X). [..]
\end{lstlisting}
%
In ASP a statement like \textit{-not} is not allowed, as it mixes default with strong negation.

For Pyke, we take example ProntoQA example 197 (Prog.).
There ChatGPT translated the fact to a rule:
\begin{lstlisting}
"Facts:\n```\nJompus($x, True)\nassert\n    Kind($x, True)\n\nfact1\nforeach [..]
\end{lstlisting}



\subsection{Further Details on Logic-LM}
\label{sec:appendix:subsec:logic-lm}

In addition to the already discussed concepts in the main part, we now present the self-refiner:
If the input contains syntax errors, the symbolic reasoner produces an error with a suitable error message.
Whenever \textit{Self-Refinement} is activated, the system tries to correct the syntax error,
by re-prompting the problem formulator with the error message from the symbolic reasoner.



\subsubsection{Running Example}

Logic-LM uses five different datasets to compare their results, of four different symbolic reasoners.
We explain the workings of Logic-LM along the ProntoQA dataset, with their logic programming\footnote{Logic-LM introduces rules in Prolog as clauses,
which they define as: $F_1 \land \ldots \land F_m \rightarrow F_{m+1} \land \ldots \land F_n$.
%However, this is \textit{not} correct in a strong sense.
In clause form this rule is $\left(\neg F_1 \lor \ldots \lor \neg F_m \lor (F_{m+1} \land \ldots \land F_n) \right)$,
so unless $(F_{m+1} \land \ldots \land F_n)$ is treated as a single literal,
their definition is not a clause.
}
formulation using the Pyke solver.
The first part of the example shows the problem, the second one the question.
The goal is to decide the question, which is a true or false question, based on the inferences and facts in the problem:
\begin{lstlisting}
Problem: Wren is a tumpus. Each tumpus is a wumpus.
Wumpuses are brown. Each wumpus is not metallic.
Question:
Is the following statement true or false? Wren is not metallic.
\end{lstlisting}

This example needs two inference steps, (i) deduce that Wren is a wumpus,
(ii) that wren is not metallic.
The number of necessary inference steps for ProntoQA can be set as a parameter in the dataset generation phase.
Further, the ProntoQA dataset includes misleading statements, such as \textit{Wumpuses are brown},
which do not help in the inference.

\subsubsection{Problem Formulator}


For translating the natural language problem into the formal language, Logic-LM leverages on the few-shot learning technique.
Thereby, in addition to the problem specification,
detailed instruction on how to convert the natural language to the formal language are given.
Furthermore, they provide one example to the LLM,
including problem specification, formal language translation, and result.

We show an example for the in-context learning for the ProntoQA~\cite{saparov_language_2023} dataset prompt.
The LLM translates a single ProntoQA problem into a formal language that can be parsed with a rule based parser
to the Pyke~\cite{frederiksen_applying_2008} Prolog style.
The ProntoQA data consists of a general task description, followed by an example problem,
including an example translation.
We start with the general task description:
%
\begin{lstlisting}[basicstyle=\small]
Task Description: You are given a problem description and a question.
The task is to:  1) define all the predicates in the problem
2) parse the problem into logic rules based on the defined predicates
3) write all the facts mentioned in the problem
4) parse the question into the logic form
\end{lstlisting}
%
This is followed by an example problem.
In the following we show a snippet of the actual example problem.
%
\begin{lstlisting}[basicstyle=\small]
Problem [Snippet]: Alex is a tumpus. Tumpuses are vumpuses. Each vumpus is a
yumpus. Yumpuses are numpuses. Each numpus is a dumpus. Every dumpus is not shy.
Question: True or false: Alex is not shy.
\end{lstlisting}
%
Finally, Logic-LM provides an example translation into their \textit{intermediate language format}.
First the predicate specification is shown, followed by the facts,
and finally by the rules.
%Note the divergence from standard Prolog syntax.
%
\begin{lstlisting}[basicstyle=\small]
Predicates:
Tumpuses($x, bool) ::: Does x belong to Tumpuses?
Vumpuses($x, bool) ::: Does x belong to Vumpuses?
[...]
Facts:
Tumpuses(Alex, True)
Rules:
Tumpuses($x, True) >>> Vumpuses($x, True)
[...]
Query:
Shy(Alex, False)
\end{lstlisting}
%
The question query is added after these prefix-prompts.
%After this Logic-LM adds the actual problem we want to solve (our running example),
%and queries the LM.
The full prompt can be found in the Appendix in Section~\ref{sec-appendix:logic-lm-example}.
The output of the LLM is then passed on to the symbolic reasoner.

\subsubsection{Symbolic Reasoner, Interpreter and Self-Refiner}

Logic-LM takes the output of the LLM, parses it into Pyke's format, and then calls and interprets Pyke.
The parsing is done entirely in Python,
which splits the output into a fact, and a rule knowledge base.
From this knowledge base, Pyke uses its backward and forward chaining mechanisms to obtain an answer.
For ProntoQA, Logic-LM uses a rule-based mechanism to interpret the answer.

Below, we show how the first rule ``\textit{Each tumpus is a wumpus}''
of our running example (which we assume is correctly translated)
gets parsed into Pyke's format.
First we show the translated rule:
%
\begin{lstlisting}
Tumpuses($x, True) >>> Wumpus($x, True)
\end{lstlisting}
%
The symbolic reasoner parses the translated rule to the following Pyke rule:
%
\begin{lstlisting}
foreach
    facts.Tumpus($x, True)
assert
    facts.Wumpus($x, True)
\end{lstlisting}

If the symbolic reasoner is unable to execute the program (for example, due to a syntax error),
then the (optional) self-refiner takes the error message of the symbolic solver into account.
Ideally, the then-generated is correct.



\subsubsection{Discussion of Logic-LM}

Logic-LM claims that their method shows an improvement of $39.20\%$ over standard prompting,
and $18.40\%$ over CoT prompting for GPT-3.5.
However, as they are using two different GPT-3.5 versions (gpt-3.5-turbo and text-davinci-003),
and further not show how they compute these improvement values,
we were not able to verify their claim.
Although we acknowledge that both, the relative improvement for gpt-3.5-turbo, and for text-davinci-003,
is approximately in this range (see Appendix Section~\ref{sec-appendix:comp-improvement}).

Although the idea of self-refinement is promising, the current self-refinement mode
is less significant than choosing a better representation language.
Their maximum increase in execution-accuracy is $2.50\%$ points (ours $49.82\%$ points), 
while their maximum execution-rate increase is $17.60\%$ points (ours $17.80\%$ points).
%at best w.r.t. the execution-accuracy
%(See Table~\ref{tbl:logic-lm-self-refinement}).
%Logic-LM explains this as the difference between a syntactically correct program (``\textit{valid}'' programs),
%and semantically correct programs (``\textit{correct}'' programs).
Lastly, as they are randomly guessing solutions when they encounter a syntax error,
their results for overall-accuracy is shifted slightly in a positive way.

\subsubsection{Checking Improvement Values}
\label{sec-appendix:comp-improvement}

Let the relative improvement be $\frac{\textit{New-Acc}}{\textit{Old-Acc}}$,
and let $\mathcal{D}$ be the set of the experiments (ProntoQA, ProofWriter, FOLIO, LogicalDeduction, and AR-LSAT).
Then we define the average\footnote{
We acknowledge that a weighted average, by the number of samples per dataset, is another feasible option.
}
improvement to be $\frac{1}{5} \Sigma_{d \in \mathcal{D}} \frac{\textit{New-Acc}_d}{\textit{Old-Acc}_d}$.

Comparing standard vs. Logic-LM,
Logic-LM achieves a relative improvement of $45.23\%$ for gpt-3.5-turbo,
of $47.47\%$ for text-davinci-003,
and of $24.98\%$ for gpt-4.
Regarding CoT vs. Logic-LM,
Logic-LM achieves a relative improvement
of $25.14\%$ for gpt-3.5-turbo,
of $19.59\%$ for text-davinci-003,
and of $10.44\%$ for gpt-4.

Therefore, although we could not verify their claims of an improvement of $39.2\%$ (standard vs. Logic-LM),
or $18.4\%$ (CoT vs. Logic-LM),
we conclude that their reported numbers are in the range of our computed improvements,
and therefore likely due to a different weighting of the average.



\subsection{Chain-of-Thought (CoT) Prompt}
\label{sec-appendix:CoT-Full-Prompt}

We show a full example of a CoT prompt for the question:
%
\begin{lstlisting}
Tumpuses are rompuses. Rompuses are not luminous. Stella is a tumpus.
True or false: Stella is not luminous.
\end{lstlisting}

The prompt is shown in the listing below:
%
\begin{lstlisting}
Given a problem statement, the goal is to answer a true/false question.
The following example showcases the line of reasoning you have to follow:
---- Question ----
Each cat is a carnivore. Every carnivore is not herbivorous. Fae is a cat.
True or false: Fae is not herbivorous.
---- Reasoning ----
Fae is a cat. Each cat is a carnivore. So Fae is a carnivore.
Every carnivore is not herbivorous. So Fae is not herbivorous.
---- Answer ----
True
----
Now consider the following question:
---- Question ----
Tumpuses are rompuses. Rompuses are not luminous. Stella is a tumpus.
True or false: Stella is not luminous.
---- Reasoning ----
\end{lstlisting}

If the LLM answers correctly (and faithfully), we get:

\begin{lstlisting}
Stella is a tumpus. Tumpuses are rompuses. So Stella is a rompus.
Rompuses are not luminous. So Stella is not luminous.
---- Answer ----
True
\end{lstlisting}

\subsection{Logic-LM: Example Prompt}
\label{sec-appendix:logic-lm-example}

The following prompt is the full prompt for the example shown in the main part of the paper,
for the Logic-LM Pyke reasoner (Baseline (Logic-LM)).

\begin{lstlisting}
Task Description: You are given a problem description and a question. The task is to: 
1) define all the predicates in the problem
2) parse the problem into logic rules based on the defined predicates
3) write all the facts mentioned in the problem
4) parse the question into the logic form
------
Problem:
Alex is a tumpus. Tumpuses are vumpuses. Each vumpus is a
yumpus. Yumpuses are numpuses. Each numpus is a dumpus. Every dumpus is not shy.
Question:
True or false: Alex is not shy.
###
Predicates:
Tumpuses($x, bool) ::: Does x belong to Tumpuses?
Vumpuses($x, bool) ::: Does x belong to Vumpuses?
Yumpus($x, bool) ::: Does x belong to Yumpus?
Numpus($x, bool) ::: Does x belong to Numpus?
Dumpus($x, bool) ::: Does x belong to Dumpus?
Shy($x, bool) ::: Is x shy?
Liquid($x, bool) ::: Is x liquid?
Zumpus($x, bool) ::: Does x belong to Zumpus?
Facts:
Tumpuses(Alex, True)
Rules:
Tumpuses($x, True) >>> Vumpuses($x, True)
Vumpuses($x, True) >>> Yumpus($x, True)
Yumpus($x, True) >>> Numpus($x, True)
Numpus($x, True) >>> Dumpus($x, True)
Dumpus($x, True) >>> Shy($x, False)
Query:
Shy(Alex, False)
------
Problem: 
Wren is a tumpus. Each tumpus is a wumpus.
Wumpuses are brown. Each wumpus is not metallic.
Question:
Is the following statement true or false? Wren is not metallic.
###
\end{lstlisting}

\subsection{Pyke (Text) Prompt}

The following shows our full in-context learning prompt for Pyke (Text).

\begin{lstlisting}
Task Description: You are given a problem description and a question.
In general, the task is to parse the problem description and question into a a Pyke (Python Knowledge Engine) readable format.
In more detail:
1) Define the facts.
2) Define the rules.
3) Define the "query". The query has to be defined according to the following example: Given the question: "True or false: Alex is not shy".
Then you should define this as "Shy(alex, false)".
The program must by syntactically correct. A correctly parsed example is given below. The output should be given in a Pyke readable format. Therefore, be sure not to use any "bullet points", or "numberings" when printing the output. Further, no special characters like ("#") must occur.
------
Problem:
Each jompus is fruity. Every jompus is a wumpus. Every wumpus is not transparent. Wumpuses are tumpuses. Tumpuses are mean. Tumpuses are vumpuses. Every vumpus is cold. Each vumpus is a yumpus. Yumpuses are orange. Yumpuses are numpuses.
Numpuses are dull. Each numpus is a dumpus. Every dumpus is not shy. Impuses are shy. Dumpuses are rompuses. Each rompus is liquid. Rompuses are zumpuses. Alex is a tumpus.
Question:
True or false: Alex is not shy.
###
Facts:
Tumpus(Alex, True)
Rules:
fact1
	foreach
		facts.Jompus($x, True)
	assert
		facts.Fruity($x, True)

fact2
	foreach
		facts.Jompus($x, True)
	assert
		facts.Wumpus($x, True)
[...]
Query:
Shy(Alex,False)
[...]
\end{lstlisting}

\subsection{Pyke (No-C.) Prompt}

The following shows our full in-context learning prompt for Pyke (No-C.).

\begin{lstlisting}
[...]
###
Facts:
P1(Alex, True)
Rules:
fact1
	foreach
		facts.P2($x, True)
	assert
		facts.P3($x, True)

fact2
	foreach
		facts.P2($x, True)
	assert
		facts.P4($x, True)
[...]
Query:
P13(Alex,False)
[...]
\end{lstlisting}

\subsection{Pyke (Prog.) Prompt}

The following shows our full in-context learning prompt for Pyke (Prog).

\begin{lstlisting}
[...]
###
Facts:
```
Tumpus(Alex, True)
```
Rules:
```
fact1
	foreach
		facts.Jompus($x, True)
	assert
		facts.Fruity($x, True)

fact2
	foreach
		facts.Jompus($x, True)
	assert
		facts.Wumpus($x, True)
[...]
```
Query:
```
Shy(Alex,False)
```
[...]
\end{lstlisting}


\subsection{Pyke (Comm.) Prompt}

The following shows our full in-context learning prompt for Pyke (Comm.).

\begin{lstlisting}
[...]
###
Facts:
```
# Alex is a tumpus.
Tumpus(Alex, True)
```
Rules:
```
# Each jompus is fruity.
fact1
	foreach
		facts.Jompus($x, True)
	assert
		facts.Fruity($x, True)
# Every jompus is a wumpus.
fact2
	foreach
		facts.Jompus($x, True)
	assert
		facts.Wumpus($x, True)
[...]
```
Query:
```
# True or false: Alex is not shy.
Shy(Alex,False)
```
[...]
\end{lstlisting}

\subsection{ASP (Text) Prompt}

The following shows our full in-context learning prompt for ASP (Text).

\begin{lstlisting}
[...]
###
Facts:
tumpus(alex).
Rules:
fruity(X) :- jompus(X).
wumpus(X) :- jompus(X).
[...]
Query:
-shy(alex).
[...]
\end{lstlisting}

\subsection{ASP (No-C.) Prompt}

The following shows our full in-context learning prompt for ASP (No-C.).

\begin{lstlisting}
[...]
Facts:
p1(alex).
Rules:
p2(X) :- p3(X).
p4(X) :- p3(X).
[...]
Query:
-p15(alex).
[...]
\end{lstlisting}

\subsection{ASP (Prog.) Prompt}

The following shows our full in-context learning prompt for ASP (Prog.).

\begin{lstlisting}
[...]
Facts:
```
tumpus(alex).
```
Rules:
```
fruity(X) :- jompus(X).
wumpus(X) :- jompus(X).
[...]
```
Query:
```
-shy(alex).
```
[...]
\end{lstlisting}

\subsection{ASP (Comm.) Prompt}

The following shows our full in-context learning prompt for ASP (Comm.).

\begin{lstlisting}
[...]
Facts:
```
% Alex is a tumpus.
tumpus(alex).
```
Rules:
```
% Each jompus is fruity.
fruity(X) :- jompus(X).
% Every jompus is a wumpus.
wumpus(X) :- jompus(X).
[...]
```
Query:
```
% True or false: Alex is not shy.
-shy(alex).
```
[...]
\end{lstlisting}


\subsection{Hardness and Completeness}
\label{section:appendix:hardeness-and-completeness}

For a formal problem $\mathcal{P}$, and a complexity class $\mathcal{C}$,
if all problems $\mathcal{P}'$ can be reduced to $\mathcal{P}$, then $\mathcal{P}$ is $\mathcal{C}$-hard.
If additionally, $\mathcal{P} \in \mathcal{C}$, then it is said that $\mathcal{P}$ is $\mathcal{C}$-complete.

\subsection{DIMACS Example}
\label{sec:appendix:dimacs-example}

The DIMACS format is a popular format for representing SAT problems, encoded as a conjunctive-normal-form.
Our example $\left(a \land (b \lor c) \land \neg a \right)$ translates to:
\begin{lstlisting}
p cnf 3 3
1
2 3
-1
\end{lstlisting}

\subsection{Answer Set Programming Semantics}
\label{sec:appendix:subsec:asp}

Let $\mathcal{I}$ be a set of atoms (an interpretation).
Then $\mathcal{I}$ is an answer set, iff all rules $r \in \Pi$ are satisfied,
and all atoms $a \in \mathcal{I}$ are founded.
    $r \in \Pi$ is satisfied iff $\left( H_r \cup B_r^{-} \right) \cap \mathcal{I} \not = \emptyset$, or
    $B_r^+ \setminus \mathcal{I} \not = \emptyset$.
    An atom $a \in \mathcal{I}$ is founded if there exists $r \in \Pi$, s.t.
    $a \in H_r$,
    $B_r^+ \subseteq \mathcal{I}$,
    $\mathcal{I} \cap \left( B_r^{-} \cup (H_r \setminus \{a\}) \right) = \emptyset$, and 
    for each $b \in B_r^+: \psi(b) < \psi(a)$, where $\psi$ is a level-mapping function.
    Another (popular) characterization of answer sets is the Gelfond-Lifschitz-Reduct.

Intuitively this means that for each rule it must hold that
whenever the positive body can be derived,
and no literal of the negative body holds,
then (one literal of) the head must hold as well.
An excellent overview about ASP is provided by~\cite{eiter_answer_2009}.

Syntactically, Prolog and ASP are similar.
In terms of expressiveness however, Prolog is in general Turing-complete,
which does not hold for ASP.
Prolog programs are evaluated by using backtracking,
and SLD-Resolution~\cite{wolf_sld-resolution_1997},
for a given query.
However, \textit{Pyke}'s~\cite{frederiksen_applying_2008} syntax
significantly diverges from the standard (ISO) Prolog.
In Prolog the statement \textit{Each cat is a carnivore} can be expressed with
``$\textit{cat}(X) \text{ :- } \textit{carnivore}(X).$'' which is in contrast to Pyke's syntax:
``$\textit{foreach facts.cat}(\$x, \textit{True}) \textit{ assert facts.carnivore }(\$x, \textit{True})$''


\end{document}
