
@inproceedings{pan_logic-lm_2023,
	title = {Logic-{LM}: {Empowering} {Large} {Language} {Models} with {Symbolic} {Solvers} for {Faithful} {Logical} {Reasoning}},
	shorttitle = {Logic-{LM}},
	doi = {10.18653/v1/2023.findings-emnlp.248},
	urldate = {2024-10-12},
	booktitle = {{EMNLP23}},
	author = {Pan, Liangming and Albalak, Alon and Wang, Xinyi and Wang, William},
	year = {2023},
	pages = {3806--3824},
	file = {Pan et al. - 2023 - Logic-LM Empowering Large Language Models with Sy.pdf:/home/thinklex/Zotero/storage/598EYBLH/Pan et al. - 2023 - Logic-LM Empowering Large Language Models with Sy.pdf:application/pdf},
}

@incollection{panas_can_2024,
	address = {Cham},
	title = {Can {Large} {Language} {Models} {Put} 2 and 2 {Together}? {Probing} for {Entailed} {Arithmetical} {Relationships}},
	shorttitle = {Can {Large} {Language} {Models} {Put} 2 and 2 {Together}?},
	abstract = {Two major areas of interest in the era of Large Language Models regard questions of what do LLMs know, and if and how they may be able to reason, or rather, approximately reason. Since to date these lines of work progressed largely in parallel, we are interested in investigating the intersection: probing for reasoning about the implicitlyheld knowledge. Suspecting the performance to be lacking, we use a very simple set-up of comparisons between cardinalities associated with elements of various subjects (e.g. the number of legs of a bird vs. the number of wheels on a tricycle). We empirically demonstrate that although LLMs make steady progress in knowledge acquisition and (pseudo)reasoning with each new GPT release, they remain limited in their capabilities to performing probabilistic retrieval. We argue that pure statistical learning can not cope with the combinatorial explosion inherent in many commonsense reasoning tasks. Further, we emphasise that bigger is not always better and chasing purely statistical improvements is ﬂawed at the core, since it only exacerbates the dangerous conﬂation of the production of correct answers with genuine reasoning ability.},
	urldate = {2024-10-12},
	booktitle = {NeSy24},
	author = {Panas, Dagmara and Seth, Sohan and Belle, Vaishak},
	year = {2024},
	doi = {10.1007/978-3-031-71170-1_21},
	pages = {258--276},
	file = {Panas et al. - 2024 - Can Large Language Models Put 2 and 2 Together Pr.pdf:/home/thinklex/Zotero/storage/JR4ERNTX/Panas et al. - 2024 - Can Large Language Models Put 2 and 2 Together Pr.pdf:application/pdf},
}

@inproceedings{kirtania_logic-lm_2024,
	title = {{LOGIC}-{LM}++: {Multi}-{Step} {Refinement} for {Symbolic} {Formulations}},
	shorttitle = {{LOGIC}-{LM}++},
	doi = {10.18653/v1/2024.nlrse-1.6},
	abstract = {In this paper we examine the limitations of Large Language Models (LLMs) for complex reasoning tasks. Although recent works have started to employ formal languages as an intermediate representation for reasoning tasks, they often face challenges in accurately generating and refining these formal specifications to ensure correctness. To address these issues, this paper proposes Logic-LM++, an improvement on Logic-LM (Pan et al., 2023). It uses the ability of LLMs to do pairwise comparisons, allowing the evaluation of the refinements suggested by the LLM. The paper demonstrates that Logic-LM++ outperforms Logic-LM and other contemporary techniques across natural language reasoning tasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average improvement of 18.5\% on standard prompting, 12.3\% on chain of thought prompting and 5\% on Logic-LM.},
	urldate = {2024-10-24},
	booktitle = {{ACL24}},
	publisher = {Association for Computational Linguistics},
	author = {Kirtania, Shashank and Gupta, Priyanshu and Radhakrishna, Arjun},
	year = {2024},
	pages = {56--63},
	annote = {Place: Bangkok, Thailand},
	file = {Kirtania et al. - 2024 - LOGIC-LM++ Multi-Step Refinement for Symbolic For.pdf:/home/thinklex/Zotero/storage/XPC993IM/Kirtania et al. - 2024 - LOGIC-LM++ Multi-Step Refinement for Symbolic For.pdf:application/pdf},
}

@inproceedings{lyu_faithful_2023,
	title = {Faithful {Chain}-of-{Thought} {Reasoning}},
	doi = {10.18653/v1/2023.ijcnlp-main.20},
	urldate = {2024-12-13},
	booktitle = {{IJCNLP23}},
	author = {Lyu, Qing and Havaldar, Shreya and Stein, Adam and Zhang, Li and Rao, Delip and Wong, Eric and Apidianaki, Marianna and Callison-Burch, Chris},
	year = {2023},
	pages = {305--329},
	annote = {Place: Nusa Dua, Bali},
	file = {Lyu et al. - 2023 - Faithful Chain-of-Thought Reasoning.pdf:/home/thinklex/Zotero/storage/EM6WVTL6/Lyu et al. - 2023 - Faithful Chain-of-Thought Reasoning.pdf:application/pdf},
}

@article{shanahan_talking_2024,
	title = {Talking about {Large} {Language} {Models}},
	volume = {67},
	doi = {10.1145/3624724},
	abstract = {Interacting with a contemporary LLM-based conversational agent can create an illusion of being in the presence of a thinking creature. Yet, in their very nature, such systems are fundamentally not like us.},
	number = {2},
	urldate = {2024-12-13},
	journal = {Com. ACM},
	author = {Shanahan, Murray},
	year = {2024},
	pages = {68--79},
	file = {Shanahan - 2024 - Talking about Large Language Models.pdf:/home/thinklex/Zotero/storage/4KTZZW8J/Shanahan - 2024 - Talking about Large Language Models.pdf:application/pdf},
}

@article{saparov_language_2023,
	title = {Language {Models} {Are} {Greedy} {Reasoners}: {A} {Systematic} {Formal} {Analysis} of {Chain}-of-{Thought}},
	abstract = {Large language models (LLMs) have shown remarkable reasoning capabilities given chain-of-thought prompts (examples with intermediate reasoning steps). Existing benchmarks measure reasoning ability indirectly, by evaluating accuracy on downstream tasks such as mathematical reasoning. However, it is unclear how these models obtain the answers and whether they rely on simple heuristics rather than the generated chain-of-thought. To enable systematic exploration of the reasoning ability of LLMs, we present a new synthetic question-answering dataset called PrOntoQA, where each example is generated from a synthetic world model represented in first-order logic. This allows us to parse the generated chain-of-thought into symbolic proofs for formal analysis. Our analysis on InstructGPT and GPT-3 shows that LLMs are quite capable of making correct individual deduction steps, and so are generally capable of reasoning, even in fictional contexts. However, they have difficulty with proof planning: When multiple valid deduction steps are available, they are not able to systematically explore the different options.},
	journal = {ICLR23},
	author = {Saparov, Abulhair and He, He},
	year = {2023},
	file = {5954_language_models_are_greedy_rea.pdf:/home/thinklex/Zotero/storage/XE55NCSZ/5954_language_models_are_greedy_rea.pdf:application/pdf},
}

@inproceedings{tafjord_proofwriter_2021,
	title = {{ProofWriter}: {Generating} {Implications}, {Proofs}, and {Abductive} {Statements} over {Natural} {Language}},
	shorttitle = {{ProofWriter}},
	doi = {10.18653/v1/2021.findings-acl.317},
	urldate = {2024-12-13},
	booktitle = {{IJCNLP}21},
	author = {Tafjord, Oyvind and Dalvi, Bhavana and Clark, Peter},
	year = {2021},
	pages = {3621--3634},
	annote = {Place: Online},
	file = {Tafjord et al. - 2021 - ProofWriter Generating Implications, Proofs, and .pdf:/home/thinklex/Zotero/storage/74JL4VCU/Tafjord et al. - 2021 - ProofWriter Generating Implications, Proofs, and .pdf:application/pdf},
}

@misc{liu_understanding_2024,
	title = {Understanding {LLMs}: {A} {Comprehensive} {Overview} from {Training} to {Inference}},
	shorttitle = {Understanding {LLMs}},
	url = {http://arxiv.org/abs/2401.02038},
	abstract = {The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There’s an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model finetuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs’ utilization and provides insights into their future development.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
	year = {2024},
	doi = {10.48550/arXiv.2401.02038},
	keywords = {Computer Science - Computation and Language},
	annote = {Issue: arXiv:2401.02038 arXiv: 2401.02038 [cs]},
	file = {Liu et al. - 2024 - Understanding LLMs A Comprehensive Overview from .pdf:/home/thinklex/Zotero/storage/67YCVR5Z/Liu et al. - 2024 - Understanding LLMs A Comprehensive Overview from .pdf:application/pdf},
}

@article{yang_harnessing_2024,
	title = {Harnessing the {Power} of {LLMs} in {Practice}: {A} {Survey} on {ChatGPT} and {Beyond}},
	volume = {18},
	issn = {1556-4681, 1556-472X},
	shorttitle = {Harnessing the {Power} of {LLMs} in {Practice}},
	url = {https://dl.acm.org/doi/10.1145/3649506},
	doi = {10.1145/3649506},
	abstract = {This article presents a comprehensive and practical guide for practitioners and end-users working with Large Language Models (LLMs) in their downstream Natural Language Processing (NLP) tasks. We provide discussions and insights into the usage of LLMs from the perspectives of models, data, and downstream tasks. First, we offer an introduction and brief summary of current language models. Then, we discuss the influence of pre-training data, training data, and test data. Most importantly, we provide a detailed discussion about the use and non-use cases of large language models for various natural language processing tasks, such as knowledge-intensive tasks, traditional natural language understanding tasks, generation tasks, emergent abilities, and considerations for specific tasks. We present various use cases and non-use cases to illustrate the practical applications and limitations of LLMs in real-world scenarios. We also try to understand the importance of data and the specific challenges associated with each NLP task. Furthermore, we explore the impact of spurious biases on LLMs and delve into other essential considerations, such as efficiency, cost, and latency, to ensure a comprehensive understanding of deploying LLMs in practice. This comprehensive guide aims to provide researchers and practitioners with valuable insights and best practices for working with LLMs, thereby enabling the successful implementation of these models in a wide range of NLP tasks. A curated list of practical guide resources of LLMs, regularly updated, can be found at https://github.com/Mooler0410/LLMsPracticalGuide . An LLMs evolutionary tree, editable yet regularly updated, can be found at llmtree.ai .},
	number = {6},
	urldate = {2024-11-26},
	journal = {ACM Trans. Knowl. Discov. Data},
	author = {Yang, Jingfeng and Jin, Hongye and Tang, Ruixiang and Han, Xiaotian and Feng, Qizhang and Jiang, Haoming and Zhong, Shaochen and Yin, Bing and Hu, Xia},
	year = {2024},
	pages = {1--32},
	file = {Yang et al. - 2024 - Harnessing the Power of LLMs in Practice A Survey.pdf:/home/thinklex/Zotero/storage/UD4DBQW2/Yang et al. - 2024 - Harnessing the Power of LLMs in Practice A Survey.pdf:application/pdf},
}

@article{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring signiﬁcantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	journal = {NeurIPS17},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	year = {2017},
	file = {Vaswani et al. - Attention is All you Need.pdf:/home/thinklex/Zotero/storage/7APH92S6/Vaswani et al. - Attention is All you Need.pdf:application/pdf},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4’s performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and al.,et.},
	year = {2024},
	doi = {10.48550/arXiv.2303.08774},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Issue: arXiv:2303.08774 arXiv: 2303.08774 [cs]},
	file = {OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:/home/thinklex/Zotero/storage/DR9IECPH/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf},
}

@inproceedings{luong_effective_2015,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/D15-1166},
	doi = {10.18653/v1/D15-1166},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	urldate = {2024-11-26},
	booktitle = {{EMNLP15}},
	publisher = {Association for Computational Linguistics},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	keywords = {Computer Science - Computation and Language},
	pages = {1412--1421},
	annote = {Place: Lisbon, Portugal arXiv: 1508.04025 [cs]},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/home/thinklex/Zotero/storage/4V6VB47Y/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf},
}

@misc{luong_effective_2015-1,
	title = {Effective {Approaches} to {Attention}-based {Neural} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1508.04025},
	abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
	urldate = {2024-11-26},
	publisher = {arXiv},
	author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
	year = {2015},
	doi = {10.48550/arXiv.1508.04025},
	keywords = {Computer Science - Computation and Language},
	annote = {Issue: arXiv:1508.04025 arXiv: 1508.04025 [cs]},
	file = {Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:/home/thinklex/Zotero/storage/7SUYIE9L/Luong et al. - 2015 - Effective Approaches to Attention-based Neural Mac.pdf:application/pdf},
}

@inproceedings{wei_chain--thought_2022,
	title = {Chain-of-{Thought} {Prompting} {Elicits} {Reasoning} in {Large} {Language} {Models}},
	abstract = {We explore how generating a chain of thought—a series of intermediate reasoning steps—signiﬁcantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufﬁciently large language models via a simple method called chain-ofthought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain-of-thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a PaLM 540B with just eight chain-of-thought exemplars achieves state-of-the-art accuracy on the GSM8K benchmark of math word problems, surpassing even ﬁnetuned GPT-3 with a veriﬁer.},
	urldate = {2024-11-26},
	booktitle = {{NeurIPS22}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	year = {2022},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {24824--24837},
	annote = {arXiv: 2201.11903 [cs]},
	file = {Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:/home/thinklex/Zotero/storage/9399HQ92/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf},
}

@incollection{eiter_answer_2009,
	title = {Answer {Set} {Programming}: {A} {Primer}},
	volume = {5689},
	shorttitle = {Answer {Set} {Programming}},
	abstract = {Answer Set Programming (ASP) is a declarative problem solving paradigm, rooted in Logic Programming and Nonmonotonic Reasoning, which has been gaining increasing attention during the last years. This article is a gentle introduction to the subject; it starts with motivation and follows the historical development of the challenge of deﬁning a semantics for logic programs with negation. It looks into positive programs over stratiﬁed programs to arbitrary programs, and then proceeds to extensions with two kinds of negation (named weak and strong negation), and disjunction in rule heads. The second part then considers the ASP paradigm itself, and describes the basic idea. It shows some programming techniques and brieﬂy overviews Answer Set solvers. The third part is devoted to ASP in the context of the Semantic Web, presenting some formalisms and mentioning some applications in this area. The article concludes with issues of current and future ASP research.},
	urldate = {2024-08-18},
	booktitle = {LNCS},
	author = {Eiter, Thomas and Ianni, Giovambattista and Krennwallner, Thomas},
	year = {2009},
	doi = {10.1007/978-3-642-03754-2_2},
	pages = {40--110},
	file = {Eiter et al. - 2009 - Answer Set Programming A Primer.pdf:/home/thinklex/Zotero/storage/33UIIXK6/Eiter et al. - 2009 - Answer Set Programming A Primer.pdf:application/pdf},
}

@article{eiter_complexity_2007,
	title = {Complexity results for answer set programming with bounded predicate arities and implications},
	volume = {51},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-008-9086-5},
	doi = {10.1007/s10472-008-9086-5},
	number = {2},
	urldate = {2024-08-18},
	journal = {Ann. Math. Artif. Intell.},
	author = {Eiter, Thomas and Faber, Wolfgang and Fink, Michael and Woltran, Stefan},
	year = {2007},
	pages = {123--165},
	file = {Eiter et al. - 2007 - Complexity results for answer set programming with.pdf:/home/thinklex/Zotero/storage/GUYQCIPM/Eiter et al. - 2007 - Complexity results for answer set programming with.pdf:application/pdf},
}

@article{calimeri_asp-core-2_2020,
	title = {{ASP}-{Core}-2 {Input} {Language} {Format}},
	volume = {20},
	doi = {10.1017/S1471068419000450},
	abstract = {Standardization of solver input languages has been a main driver for the growth of several areas within knowledge representation and reasoning, fostering the exploitation in actual applications. In this document, we present the ASP-Core-2 standard input language for Answer Set Programming, which has been adopted in ASP Competition events since 2013.},
	number = {2},
	journal = {TPLP},
	author = {Calimeri, Francesco and Faber, Wolfgang and Gebser, Martin and Ianni, Giovambattista and Kaminski, Roland and Krennwallner, Thomas and Leone, Nicola and Maratea, Marco and Ricca, Francesco and Schaub, Torsten},
	year = {2020},
	pages = {294--309},
	file = {Calimeri et al. - 2020 - ASP-Core-2 Input Language Format.pdf:/home/thinklex/Zotero/storage/58PXBLDF/Calimeri et al. - 2020 - ASP-Core-2 Input Language Format.pdf:application/pdf},
}

@article{kaminski_how_2023,
	title = {How to {Build} {Your} {Own} {ASP}-based {System}?},
	volume = {23},
	issn = {1471-0684, 1475-3081},
	doi = {10.1017/S1471068421000508},
	abstract = {Answer Set Programming, or ASP for short, has become a popular and sophisticated approach to declarative problem solving. Its popularity is due to its attractive modeling-grounding-solving workﬂow that provides an easy approach to problem solving, even for laypersons outside computer science. However, in contrast to ASP’s ease of use, the high degree of sophistication of the underlying technology makes it even hard for ASP experts to put ideas into practice whenever this involves modifying ASP’s machinery. For addressing this issue, this tutorial aims at enabling users to build their own ASP-based systems. More precisely, we show how the ASP system clingo can be used for extending ASP and for implementing customized special-purpose systems. To this end, we propose two alternatives. We begin with a traditional AI technique and show how metaprogramming can be used for extending ASP. This is a rather light approach that relies on clingo’s reiﬁcation feature to use ASP itself for expressing new functionalities. The second part of this tutorial uses traditional programming (in Python) for manipulating clingo via its application programming interface. This approach allows for changing and controlling the entire model-ground-solve workﬂow of ASP. Central to this is clingo’s new Application class that allows us to draw on clingo’s infrastructure by customizing processes similar to the one in clingo. For instance, we may apply manipulations to programs’ abstract syntax trees, control various forms of multi-shot solving, and set up theory propagators for foreign inferences. A cross-sectional structure, spanning meta as well as application programming, is clingo’s intermediate format, aspif, that speciﬁes the interface among the underlying grounder and solver. We illustrate the aforementioned concepts and techniques throughout this tutorial by means of examples and several nontrivial case studies. In particular, we show how clingo can be extended by diﬀerence constraints and how guess-and-check programming can be implemented with both meta and application programming.},
	number = {1},
	urldate = {2024-08-19},
	journal = {TPLP},
	author = {Kaminski, Roland and Romero, Javier and Schaub, Torsten and Wanko, Philipp},
	year = {2023},
	pages = {299--361},
	file = {Kaminski et al. - 2023 - How to Build Your Own ASP-based System!.pdf:/home/thinklex/Zotero/storage/TR33DEUP/Kaminski et al. - 2023 - How to Build Your Own ASP-based System!.pdf:application/pdf},
}

@misc{jiang_mistral_2023,
	title = {Mistral {7B}},
	url = {http://arxiv.org/abs/2310.06825},
	abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B – Instruct, that surpasses the Llama 2 13B – Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, Lélio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	year = {2023},
	doi = {10.48550/arXiv.2310.06825},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Issue: arXiv:2310.06825 arXiv: 2310.06825 [cs]},
	file = {Jiang et al. - 2023 - Mistral 7B.pdf:/home/thinklex/Zotero/storage/MK2G769T/Jiang et al. - 2023 - Mistral 7B.pdf:application/pdf},
}

@article{wang_history_2024,
	title = {History, development, and principles of large language models: an introductory survey},
	issn = {2730-5953, 2730-5961},
	shorttitle = {History, development, and principles of large language models},
	url = {https://link.springer.com/10.1007/s43681-024-00583-7},
	doi = {10.1007/s43681-024-00583-7},
	abstract = {Language models serve as a cornerstone in natural language processing, utilizing mathematical methods to generalize language laws and knowledge for prediction and generation. Over extensive research spanning decades, language modeling has progressed from initial statistical language models to the contemporary landscape of large language models (LLMs). Notably, the swift evolution of LLMs has reached the ability to process, understand, and generate human-level text. Nevertheless, despite the significant advantages that LLMs offer in improving both work and personal lives, the limited understanding among general practitioners about the background and principles of these models hampers their full potential. Notably, most LLM reviews focus on specific aspects and utilize specialized language, posing a challenge for practitioners lacking relevant background knowledge. In light of this, this survey aims to present a comprehensible overview of LLMs to assist a broader audience. It strives to facilitate a comprehensive understanding by exploring the historical background of language models and tracing their evolution over time. The survey further investigates the factors influencing the development of LLMs, emphasizing key contributions. Additionally, it concentrates on elucidating the underlying principles of LLMs, equipping audiences with essential theoretical knowledge. The survey also highlights the limitations of existing work and points out promising future directions.},
	urldate = {2024-12-15},
	journal = {AI Ethics},
	author = {Wang, Zichong and Chu, Zhibo and Doan, Thang Viet and Ni, Shiwen and Yang, Min and Zhang, Wenbin},
	year = {2024},
	file = {Wang et al. - 2024 - History, development, and principles of large lang.pdf:/home/thinklex/Zotero/storage/MZPIEPZD/Wang et al. - 2024 - History, development, and principles of large lang.pdf:application/pdf},
}

@article{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	abstract = {We demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even becoming competitive with prior state-ofthe-art ﬁne-tuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous nonsparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks. We also identify some datasets where GPT3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora.},
	journal = {NeurIPS20},
	author = {Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom},
	year = {2020},
	pages = {1877--1901},
	file = {Brown et al. - Language Models are Few-Shot Learners.pdf:/home/thinklex/Zotero/storage/7L2IP7NM/Brown et al. - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	abstract = {In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textbackslash}textbackslashtextgreater99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and al.,et.},
	year = {2024},
	doi = {10.48550/arXiv.2403.05530},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Issue: arXiv:2403.05530 arXiv: 2403.05530 [cs]},
	file = {Team et al. - 2024 - Gemini 1.5 Unlocking multimodal understanding acr.pdf:/home/thinklex/Zotero/storage/PVTQYA72/Team et al. - 2024 - Gemini 1.5 Unlocking multimodal understanding acr.pdf:application/pdf},
}

@misc{grattafiori_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-12-15},
	publisher = {arXiv},
	author = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and al.,et.},
	year = {2024},
	doi = {10.48550/arXiv.2407.21783},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Issue: arXiv:2407.21783 arXiv: 2407.21783 [cs]},
	file = {Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:/home/thinklex/Zotero/storage/42IWFJNA/Grattafiori et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf},
}

@inproceedings{kombrink_recurrent_2011,
	title = {Recurrent neural network based language modeling in meeting recognition},
	url = {https://www.isca-archive.org/interspeech_2011/kombrink11_interspeech.html},
	doi = {10.21437/Interspeech.2011-720},
	abstract = {We use recurrent neural network (RNN) based language models to improve the BUT English meeting recognizer. On the baseline setup using the original language models we decrease word error rate (WER) more than 1\% absolute by n-best list rescoring and language model adaptation. When n-gram language models are trained on the same moderately sized data set as the RNN models, improvements are higher yielding a system which performs comparable to the baseline. A noticeable improvement was observed with unsupervised adaptation of RNN models. Furthermore, we examine the inﬂuence of word history on WER and show how to speed-up rescoring by caching common preﬁx strings.},
	urldate = {2024-12-15},
	booktitle = {Interspeech 2011},
	publisher = {ISCA},
	author = {Kombrink, Stefan and Mikolov, Tomáš and Karafiát, Martin and Burget, Lukáš},
	year = {2011},
	pages = {2877--2880},
	file = {Kombrink et al. - 2011 - Recurrent neural network based language modeling i.pdf:/home/thinklex/Zotero/storage/H5I3PPJJ/Kombrink et al. - 2011 - Recurrent neural network based language modeling i.pdf:application/pdf},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	number = {8},
	journal = {Neural computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	year = {1997},
	pages = {1735--1780},
	file = {lstm.pdf:/home/thinklex/Zotero/storage/ZK2APKAY/lstm.pdf:application/pdf},
}

@inproceedings{han_folio_2024,
	title = {{FOLIO}: {Natural} {Language} {Reasoning} with {First}-{Order} {Logic}},
	shorttitle = {{FOLIO}},
	doi = {10.18653/v1/2024.emnlp-main.1229},
	abstract = {Large language models (LLMs) have achieved remarkable performance on a variety of natural language understanding tasks. However, existing benchmarks are inadequate in measuring the complex logical reasoning capabilities of a model. We present FOLIO, a human-annotated, logically complex and diverse dataset for reasoning in natural language (NL), equipped with first-order logic (FOL) annotations. FOLIO consists of 1,430 examples (unique conclusions), each paired with one of 487 sets of premises used to deductively reason for the validity of each conclusion. The logical correctness of the premises and conclusions is ensured by their FOL annotations, which are automatically verified by an FOL inference engine. In addition to the main NL reasoning task, NLFOL pairs in FOLIO constitute a new NL-FOL translation dataset. Our experiments on FOLIO systematically evaluate the FOL reasoning ability of supervised fine-tuning on medium-sized language models. For both NL reasoning and NL-FOL translation, we benchmark multiple state-of-the-art language models. Our results show that a subset of FOLIO presents a challenge for one of the most capable Large Language Model (LLM) publicly available, GPT-4.},
	urldate = {2024-12-15},
	booktitle = {{EMNLP24}},
	author = {Han, Simeng and Schoelkopf, Hailey and Zhao, Yilun and Qi, Zhenting and Riddell, Martin and Zhou, Wenfei and Coady, James and Peng, David and Qiao, Yujie and Benson, Luke and al.,et.},
	year = {2024},
	pages = {22017--22031},
	annote = {Place: Miami, Florida, USA},
	file = {Han et al. - 2024 - FOLIO Natural Language Reasoning with First-Order.pdf:/home/thinklex/Zotero/storage/D8WMG2IP/Han et al. - 2024 - FOLIO Natural Language Reasoning with First-Order.pdf:application/pdf},
}

@inproceedings{zhong_analytical_2022,
	title = {Analytical {Reasoning} of {Text}},
	url = {https://aclanthology.org/2022.findings-naacl.177},
	doi = {10.18653/v1/2022.findings-naacl.177},
	urldate = {2024-12-15},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {NAACL} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Zhong, Wanjun and Wang, Siyuan and Tang, Duyu and Xu, Zenan and Guo, Daya and Chen, Yining and Wang, Jiahai and Yin, Jian and Zhou, Ming and Duan, Nan},
	year = {2022},
	pages = {2306--2319},
	annote = {Place: Seattle, United States},
	file = {Zhong et al. - 2022 - Analytical Reasoning of Text.pdf:/home/thinklex/Zotero/storage/RKW763T3/Zhong et al. - 2022 - Analytical Reasoning of Text.pdf:application/pdf},
}

@article{srivastava_beyond_2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=uyTL5Bvosj},
	journal = {Transactions on Machine Learning Research},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek and Shoeb, Abu Awal Md and Abid, Abubakar and Fisch, Adam and Brown, Adam R. and Santoro, Adam and Gupta, Aditya and Garriga-Alonso, Adrià and Kluska, Agnieszka and Lewkowycz, Aitor and Agarwal, Akshat and Power, Alethea and Ray, Alex and Warstadt, Alex and Kocurek, Alexander W. and Safaya, Ali and Tazarv, Ali and Xiang, Alice and Parrish, Alicia and Nie, Allen and Hussain, Aman and Askell, Amanda and Dsouza, Amanda and Slone, Ambrose and Rahane, Ameet and Iyer, Anantharaman S. and Andreassen, Anders Johan and Madotto, Andrea and Santilli, Andrea and Stuhlmüller, Andreas and Dai, Andrew M. and La, Andrew and Lampinen, Andrew Kyle and Zou, Andy and Jiang, Angela and Chen, Angelica and Vuong, Anh and Gupta, Animesh and Gottardi, Anna and Norelli, Antonio and Venkatesh, Anu and Gholamidavoodi, Arash and Tabassum, Arfa and Menezes, Arul and Kirubarajan, Arun and Mullokandov, Asher and Sabharwal, Ashish and Herrick, Austin and Efrat, Avia and Erdem, Aykut and Karakaş, Ayla and Roberts, B. Ryan and Loe, Bao Sheng and Zoph, Barret and Bojanowski, Bartłomiej and Özyurt, Batuhan and Hedayatnia, Behnam and Neyshabur, Behnam and Inden, Benjamin and Stein, Benno and Ekmekci, Berk and Lin, Bill Yuchen and Howald, Blake and Orinion, Bryan and Diao, Cameron and Dour, Cameron and Stinson, Catherine and Argueta, Cedrick and Ferri, Cesar and Singh, Chandan and Rathkopf, Charles and Meng, Chenlin and Baral, Chitta and Wu, Chiyu and Callison-Burch, Chris and Waites, Christopher and Voigt, Christian and Manning, Christopher D. and Potts, Christopher and Ramirez, Cindy and Rivera, Clara E. and Siro, Clemencia and Raffel, Colin and Ashcraft, Courtney and Garbacea, Cristina and Sileo, Damien and Garrette, Dan and Hendrycks, Dan and Kilman, Dan and Roth, Dan and Freeman, C. Daniel and Khashabi, Daniel and Levy, Daniel and González, Daniel Moseguí and Perszyk, Danielle and Hernandez, Danny and Chen, Danqi and Ippolito, Daphne and Gilboa, Dar and Dohan, David and Drakard, David and Jurgens, David and Datta, Debajyoti and Ganguli, Deep and Emelin, Denis and Kleyko, Denis and Yuret, Deniz and Chen, Derek and Tam, Derek and Hupkes, Dieuwke and Misra, Diganta and Buzan, Dilyar and Mollo, Dimitri Coelho and Yang, Diyi and Lee, Dong-Ho and Schrader, Dylan and Shutova, Ekaterina and Cubuk, Ekin Dogus and Segal, Elad and Hagerman, Eleanor and Barnes, Elizabeth and Donoway, Elizabeth and Pavlick, Ellie and Rodolà, Emanuele and Lam, Emma and Chu, Eric and Tang, Eric and Erdem, Erkut and Chang, Ernie and Chi, Ethan A. and Dyer, Ethan and Jerzak, Ethan and Kim, Ethan and Manyasi, Eunice Engefu and Zheltonozhskii, Evgenii and Xia, Fanyue and Siar, Fatemeh and Martínez-Plumed, Fernando and Happé, Francesca and Chollet, Francois and Rong, Frieda and Mishra, Gaurav and Winata, Genta Indra and Melo, Gerard de and Kruszewski, Germàn and Parascandolo, Giambattista and Mariani, Giorgio and Wang, Gloria Xinyue and Jaimovitch-Lopez, Gonzalo and Betz, Gregor and Gur-Ari, Guy and Galijasevic, Hana and Kim, Hannah and Rashkin, Hannah and Hajishirzi, Hannaneh and Mehta, Harsh and Bogar, Hayden and Shevlin, Henry Francis Anthony and Schuetze, Hinrich and Yakura, Hiromu and Zhang, Hongming and Wong, Hugh Mee and Ng, Ian and Noble, Isaac and Jumelet, Jaap and Geissinger, Jack and Kernion, Jackson and Hilton, Jacob and Lee, Jaehoon and Fisac, Jaime Fernández and Simon, James B. and Koppel, James and Zheng, James and Zou, James and Kocon, Jan and Thompson, Jana and Wingfield, Janelle and Kaplan, Jared and Radom, Jarema and Sohl-Dickstein, Jascha and Phang, Jason and Wei, Jason and Yosinski, Jason and Novikova, Jekaterina and Bosscher, Jelle and Marsh, Jennifer and Kim, Jeremy and Taal, Jeroen and Engel, Jesse and Alabi, Jesujoba and Xu, Jiacheng and Song, Jiaming and Tang, Jillian and Waweru, Joan and Burden, John and Miller, John and Balis, John U. and Batchelder, Jonathan and Berant, Jonathan and Frohberg, Jörg and Rozen, Jos and Hernandez-Orallo, Jose and Boudeman, Joseph and Guerr, Joseph and Jones, Joseph and Tenenbaum, Joshua B. and Rule, Joshua S. and Chua, Joyce and Kanclerz, Kamil and Livescu, Karen and Krauth, Karl and Gopalakrishnan, Karthik and Ignatyeva, Katerina and Markert, Katja and Dhole, Kaustubh and Gimpel, Kevin and Omondi, Kevin and Mathewson, Kory Wallace and Chiafullo, Kristen and Shkaruta, Ksenia and Shridhar, Kumar and McDonell, Kyle and Richardson, Kyle and Reynolds, Laria and Gao, Leo and Zhang, Li and Dugan, Liam and Qin, Lianhui and Contreras-Ochando, Lidia and Morency, Louis-Philippe and Moschella, Luca and Lam, Lucas and Noble, Lucy and Schmidt, Ludwig and He, Luheng and Oliveros-Colón, Luis and Metz, Luke and Senel, Lütfi Kerem and Bosma, Maarten and Sap, Maarten and Hoeve, Maartje Ter and Farooqi, Maheen and Faruqui, Manaal and Mazeika, Mantas and Baturan, Marco and Marelli, Marco and Maru, Marco and Ramirez-Quintana, Maria Jose and Tolkiehn, Marie and Giulianelli, Mario and Lewis, Martha and Potthast, Martin and Leavitt, Matthew L. and Hagen, Matthias and Schubert, Mátyás and Baitemirova, Medina Orduna and Arnaud, Melody and McElrath, Melvin and Yee, Michael Andrew and Cohen, Michael and Gu, Michael and Ivanitskiy, Michael and Starritt, Michael and Strube, Michael and Swędrowski, Michał and Bevilacqua, Michele and Yasunaga, Michihiro and Kale, Mihir and Cain, Mike and Xu, Mimee and Suzgun, Mirac and Walker, Mitch and Tiwari, Mo and Bansal, Mohit and Aminnaseri, Moin and Geva, Mor and Gheini, Mozhdeh and T, Mukund Varma and Peng, Nanyun and Chi, Nathan Andrew and Lee, Nayeon and Krakover, Neta Gur-Ari and Cameron, Nicholas and Roberts, Nicholas and Doiron, Nick and Martinez, Nicole and Nangia, Nikita and Deckers, Niklas and Muennighoff, Niklas and Keskar, Nitish Shirish and Iyer, Niveditha S. and Constant, Noah and Fiedel, Noah and Wen, Nuan and Zhang, Oliver and Agha, Omar and Elbaghdadi, Omar and Levy, Omer and Evans, Owain and Casares, Pablo Antonio Moreno and Doshi, Parth and Fung, Pascale and Liang, Paul Pu and Vicol, Paul and Alipoormolabashi, Pegah and Liao, Peiyuan and Liang, Percy and Chang, Peter W. and Eckersley, Peter and Htut, Phu Mon and Hwang, Pinyu and Miłkowski, Piotr and Patil, Piyush and Pezeshkpour, Pouya and Oli, Priti and Mei, Qiaozhu and Lyu, Qing and Chen, Qinlang and Banjade, Rabin and Rudolph, Rachel Etta and Gabriel, Raefer and Habacker, Rahel and Risco, Ramon and Millière, Raphaël and Garg, Rhythm and Barnes, Richard and Saurous, Rif A. and Arakawa, Riku and Raymaekers, Robbe and Frank, Robert and Sikand, Rohan and Novak, Roman and Sitelew, Roman and Bras, Ronan Le and Liu, Rosanne and Jacobs, Rowan and Zhang, Rui and Salakhutdinov, Russ and Chi, Ryan Andrew and Lee, Seungjae Ryan and Stovall, Ryan and Teehan, Ryan and Yang, Rylan and Singh, Sahib and Mohammad, Saif M. and Anand, Sajant and Dillavou, Sam and Shleifer, Sam and Wiseman, Sam and Gruetter, Samuel and Bowman, Samuel R. and Schoenholz, Samuel Stern and Han, Sanghyun and Kwatra, Sanjeev and Rous, Sarah A. and Ghazarian, Sarik and Ghosh, Sayan and Casey, Sean and Bischoff, Sebastian and Gehrmann, Sebastian and Schuster, Sebastian and Sadeghi, Sepideh and Hamdan, Shadi and Zhou, Sharon and Srivastava, Shashank and Shi, Sherry and Singh, Shikhar and Asaadi, Shima and Gu, Shixiang Shane and Pachchigar, Shubh and Toshniwal, Shubham and Upadhyay, Shyam and Debnath, Shyamolima Shammie and Shakeri, Siamak and Thormeyer, Simon and Melzi, Simone and Reddy, Siva and Makini, Sneha Priscilla and Lee, Soo-Hwan and Torene, Spencer and Hatwar, Sriharsha and Dehaene, Stanislas and Divic, Stefan and Ermon, Stefano and Biderman, Stella and Lin, Stephanie and Prasad, Stephen and Piantadosi, Steven and Shieber, Stuart and Misherghi, Summer and Kiritchenko, Svetlana and Mishra, Swaroop and Linzen, Tal and Schuster, Tal and Li, Tao and Yu, Tao and Ali, Tariq and Hashimoto, Tatsunori and Wu, Te-Lin and Desbordes, Théo and Rothschild, Theodore and Phan, Thomas and Wang, Tianle and Nkinyili, Tiberius and Schick, Timo and Kornev, Timofei and Tunduny, Titus and Gerstenberg, Tobias and Chang, Trenton and Neeraj, Trishala and Khot, Tushar and Shultz, Tyler and Shaham, Uri and Misra, Vedant and Demberg, Vera and Nyamai, Victoria and Raunak, Vikas and Ramasesh, Vinay Venkatesh and prabhu, vinay uday and Padmakumar, Vishakh and Srikumar, Vivek and Fedus, William and Saunders, William and Zhang, William and Vossen, Wout and Ren, Xiang and Tong, Xiaoyu and Zhao, Xinran and Wu, Xinyi and Shen, Xudong and Yaghoobzadeh, Yadollah and Lakretz, Yair and Song, Yangqiu and Bahri, Yasaman and Choi, Yejin and Yang, Yichi and Hao, Sophie and Chen, Yifu and Belinkov, Yonatan and Hou, Yu and Hou, Yufang and Bai, Yuntao and Seid, Zachary and Zhao, Zhuoye and Wang, Zijian and Wang, Zijie J. and Wang, Zirui and Wu, Ziyi},
	year = {2023},
	file = {Beyond the Imitation Game Benchmark.pdf:/home/thinklex/Zotero/storage/XL6FSWJJ/Beyond the Imitation Game Benchmark.pdf:application/pdf},
}

@inproceedings{geva_injecting_2020,
	title = {Injecting {Numerical} {Reasoning} {Skills} into {Language} {Models}},
	doi = {10.18653/v1/2020.acl-main.89},
	abstract = {Large pre-trained language models (LMs) are known to encode substantial amounts of linguistic information. However, high-level reasoning skills, such as numerical reasoning, are difﬁcult to learn from a language-modeling objective only. Consequently, existing models for numerical reasoning have used specialized architectures with limited ﬂexibility. In this work, we show that numerical reasoning is amenable to automatic data generation, and thus one can inject this skill into pre-trained LMs, by generating large amounts of data, and training in a multi-task setup. We show that pre-training our model, GENBERT, on this data, dramatically improves performance on DROP (49.3 → 72.3 F1), reaching performance that matches state-of-the-art models of comparable size, while using a simple and general-purpose encoder-decoder architecture. Moreover, GENBERT generalizes well to math word problem datasets, while maintaining high performance on standard RC tasks. Our approach provides a general recipe for injecting skills into large pre-trained LMs, whenever the skill is amenable to automatic data augmentation.},
	urldate = {2024-12-15},
	booktitle = {{ACL20}},
	author = {Geva, Mor and Gupta, Ankit and Berant, Jonathan},
	year = {2020},
	pages = {946--958},
	annote = {Place: Online},
	file = {Geva et al. - 2020 - Injecting Numerical Reasoning Skills into Language.pdf:/home/thinklex/Zotero/storage/7AAPNDWP/Geva et al. - 2020 - Injecting Numerical Reasoning Skills into Language.pdf:application/pdf},
}

@inproceedings{yang_generating_2022,
	title = {Generating {Natural} {Language} {Proofs} with {Verifier}-{Guided} {Search}},
	doi = {10.18653/v1/2022.emnlp-main.7},
	urldate = {2024-12-15},
	booktitle = {{EMNLP22}},
	author = {Yang, Kaiyu and Deng, Jia and Chen, Danqi},
	year = {2022},
	pages = {89--105},
	annote = {Place: Abu Dhabi, United Arab Emirates},
	file = {Yang et al. - 2022 - Generating Natural Language Proofs with Verifier-G.pdf:/home/thinklex/Zotero/storage/ZUB6586L/Yang et al. - 2022 - Generating Natural Language Proofs with Verifier-G.pdf:application/pdf},
}

@article{badreddine_logic_2022,
	title = {Logic {Tensor} {Networks}},
	volume = {303},
	doi = {10.1016/j.artint.2021.103649},
	urldate = {2024-12-15},
	journal = {AI},
	author = {Badreddine, Samy and d'Avila Garcez, Artur and Serafini, Luciano and Spranger, Michael},
	year = {2022},
	pages = {103649},
	file = {Badreddine et al. - 2022 - Logic Tensor Networks.pdf:/home/thinklex/Zotero/storage/HM9FSGC5/Badreddine et al. - 2022 - Logic Tensor Networks.pdf:application/pdf},
}

@article{teso_structured_2017,
	title = {Structured learning modulo theories},
	volume = {244},
	issn = {00043702},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0004370215000648},
	doi = {10.1016/j.artint.2015.04.002},
	urldate = {2024-12-15},
	journal = {Artificial Intelligence},
	author = {Teso, Stefano and Sebastiani, Roberto and Passerini, Andrea},
	year = {2017},
	pages = {166--187},
	file = {Teso et al. - 2017 - Structured learning modulo theories.pdf:/home/thinklex/Zotero/storage/Q7CU6Q67/Teso et al. - 2017 - Structured learning modulo theories.pdf:application/pdf},
}

@article{garcez_neurosymbolic_2023,
	title = {Neurosymbolic {AI}: the 3rd wave},
	volume = {56},
	shorttitle = {Neurosymbolic {AI}},
	doi = {10.1007/s10462-023-10448-w},
	abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning have achieved unprecedented impact across research communities and industry. Nevertheless, concerns around trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neurosymbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability by offering symbolic representations for neural models. In this paper, we relate recent and early research in neurosymbolic AI with the objective of identifying the most important ingredients of neurosymbolic AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. Finally, this review identifies promising directions and challenges for the next decade of AI research from the perspective of neurosymbolic computing, commonsense reasoning and causal explanation.},
	number = {11},
	urldate = {2024-11-18},
	journal = {Artif Intell Rev},
	author = {Garcez, Artur d’Avila and Lamb, Luís C.},
	year = {2023},
	pages = {12387--12406},
	file = {Garcez and Lamb - 2023 - Neurosymbolic AI the 3rd wave.pdf:/home/thinklex/Zotero/storage/XRMTRTP3/Garcez and Lamb - 2023 - Neurosymbolic AI the 3rd wave.pdf:application/pdf},
}

@article{siegelmann_computational_1995,
	title = {On the {Computational} {Power} of {Neural} {Nets}},
	abstract = {This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a "sigmoidal" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets.},
	journal = {JCSS},
	author = {Siegelmann, Hava T and Sontag, Eduardo D},
	year = {1995},
	file = {1-s2.0-S0022000085710136-main.pdf:/home/thinklex/Zotero/storage/3R88SZWU/1-s2.0-S0022000085710136-main.pdf:application/pdf},
}

@inproceedings{yang_neurasp_2020,
	title = {{NeurASP}: {Embracing} {Neural} {Networks} into {Answer} {Set} {Programming}},
	isbn = {978-0-9992411-6-5},
	shorttitle = {{NeurASP}},
	doi = {10.24963/ijcai.2020/243},
	abstract = {We present NeurASP, a simple extension of answer set programs by embracing neural networks. By treating the neural network output as the probability distribution over atomic facts in answer set programs, NeurASP provides a simple and effective way to integrate sub-symbolic and symbolic computation. We demonstrate how NeurASP can make use of a pre-trained neural network in symbolic computation and how it can improve the neural network’s perception result by applying symbolic reasoning in answer set programming. Also, NeurASP can make use of ASP rules to train a neural network better so that a neural network not only learns from implicit correlations from the data but also from the explicit complex semantic constraints expressed by the rules.},
	urldate = {2024-10-06},
	author = {Yang, Zhun and Ishay, Adam and Lee, Joohyung},
	year = {2020},
	pages = {1755--1762},
	annote = {Place: Yokohama, Japan},
}

@article{korner_fifty_2022,
	title = {Fifty {Years} of {Prolog} and {Beyond}},
	volume = {22},
	doi = {10.1017/S1471068422000102},
	abstract = {Abstract Both logic programming in general and Prolog in particular have a long and fascinating history, intermingled with that of many disciplines they inherited from or catalyzed. A large body of research has been gathered over the last 50 years, supported by many Prolog implementations. Many implementations are still actively developed, while new ones keep appearing. Often, the features added by different systems were motivated by the interdisciplinary needs of programmers and implementors, yielding systems that, while sharing the “classic” core language, in particular, the main aspects of the ISO-Prolog standard, also depart from each other in other aspects. This obviously poses challenges for code portability. The field has also inspired many related, but quite different languages that have created their own communities. This article aims at integrating and applying the main lessons learned in the process of evolution of Prolog. It is structured into three major parts. First, we overview the evolution of Prolog systems and the community approximately up to the ISO standard, considering both the main historic developments and the motivations behind several Prolog implementations, as well as other logic programming languages influenced by Prolog. Then, we discuss the Prolog implementations that are most active after the appearance of the standard: their visions, goals, commonalities, and incompatibilities. Finally, we perform a SWOT analysis in order to better identify the potential of Prolog and propose future directions along with which Prolog might continue to add useful features, interfaces, libraries, and tools, while at the same time improving compatibility between implementations.},
	number = {6},
	urldate = {2024-12-15},
	journal = {TPLP},
	author = {Körner, Philipp and Leuschel, Michael and Barbosa, João and Costa, Vítor Santos and Dahl, Verónica and Hermenegildo, Manuel V. and Morales, Jose F. and Wielemaker, Jan and Diaz, Daniel and Abreu, Salvador and Ciatto, Giovanni},
	year = {2022},
	pages = {776--858},
	file = {Körner et al. - 2022 - Fifty Years of Prolog and Beyond.pdf:/home/thinklex/Zotero/storage/NDUZZSMR/Körner et al. - 2022 - Fifty Years of Prolog and Beyond.pdf:application/pdf},
}

@article{kumar_algorithms_1992,
	title = {Algorithms for {Constraint}-{Satisfaction} {Problems}: {A} {Survey}},
	volume = {13},
	doi = {10.1609/aimag.v13i1.976},
	number = {1},
	journal = {AI Mag.},
	author = {Kumar, Vipin},
	year = {1992},
	pages = {32},
}

@book{enderton_mathematical_1972,
	address = {New York,},
	title = {A {Mathematical} {Introduction} to {Logic}},
	publisher = {Academic Press},
	author = {Enderton, Herbert},
	year = {1972},
}

@inproceedings{cook_complexity_1971,
	title = {The complexity of theorem-proving procedures},
	doi = {10.1145/800157.805047},
	abstract = {It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed.},
	booktitle = {{STOC}71},
	author = {Cook, Stephen A.},
	year = {1971},
	pages = {151--158},
	annote = {Place: New York, NY, USA},
}

@misc{frederiksen_applying_2008,
	title = {Applying {Expert} {System} {Technology} to {Code} {Reuse} with {Pyke}},
	url = {https://pyke.sourceforge.net/PyCon2008-paper.html},
	abstract = {This paper explores a new approach to code reuse using a backward-chaining rule-based system, similar to prolog, to generate a function call graph before the functions are called. This is compared with current solutions which build the call graph as the functions are called. This approach is introduced through an open source project called Pyke (Python Knowledge Engine). Finally, the initial results show that the utility of this approach far exceeds expectations; leading to something more akin to automatic programming rather than adaptable libraries. A call for help is given to explore the capabilities of this approach across different domains.},
	publisher = {PyCon},
	author = {Frederiksen, Bruce},
	year = {2008},
}

@misc{mccune_prover9_2010,
	title = {Prover9 and {Mace4}},
	url = {http://www.cs.unm.edu/~mccune/Prover9},
	author = {McCune, William},
	year = {2010},
}

@article{de_moura_z3_2008,
	title = {Z3: {An} {Efficient} {SMT} {Solver}},
	doi = {doi.org/10.1007/978-3-540-78800-3_2},
	abstract = {Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications.},
	journal = {TACAS08},
	author = {de Moura, Leonardo and Bjorner, Nikolaj},
	year = {2008},
	pages = {337--340},
	file = {de Moura and Bjorner - 2008 - Z3 An Efficient SMT Solver.pdf:/home/thinklex/Zotero/storage/8ZR38P7D/de Moura and Bjorner - 2008 - Z3 An Efficient SMT Solver.pdf:application/pdf},
}

@misc{niemeyer_python-constraint_2024,
	title = {python-constraint},
	url = {https://github.com/python-constraint/python-constraint},
	author = {Niemeyer, Gustavo and Celles, Sebastien and Willemsen, Floris-Jan},
	year = {2024},
}

@article{dantsin_complexity_2001,
	title = {Complexity and expressive power of logic programming},
	volume = {33},
	doi = {10.1145/502807.502810},
	abstract = {This article surveys various complexity and expressiveness results on different forms of logic programming. The main focus is on decidable forms of logic programming, in particular, propositional logic programming and datalog, but we also mention general logic programming with function symbols. Next to classical results on plain logic programming (pure Horn clause programs), more recent results on various important extensions of logic programming are surveyed. These include logic programming with different forms of negation, disjunctive logic programming, logic programming with equality, and constraint logic programming.},
	number = {3},
	journal = {ACM Comput. Surv.},
	author = {Dantsin, Evgeny and Eiter, Thomas and Gottlob, Georg},
	year = {2001},
	pages = {374--425},
	file = {Dantsin et al. - Complexity and expressive power of logic programmi.pdf:/home/thinklex/Zotero/storage/J29JLSEQ/Dantsin et al. - Complexity and expressive power of logic programmi.pdf:application/pdf},
}

@incollection{wolf_sld-resolution_1997,
	title = {{SLD}-resolution},
	isbn = {978-3-540-69049-8},
	booktitle = {Foundations of {Inductive} {Logic} {Programming}},
	author = {Wolf, Roland and Nienhuys-Cheng, Shan-Hwei},
	year = {1997},
	doi = {10.1007/3-540-62927-0_7},
	pages = {105--126},
	file = {1997 - SLD-resolution.pdf:/home/thinklex/Zotero/storage/Q2B5GNJC/1997 - SLD-resolution.pdf:application/pdf},
}

@book{kahneman_thinking_2011,
	address = {New York, NY, US},
	series = {Thinking, fast and slow.},
	title = {Thinking, fast and slow.},
	isbn = {0-374-27563-7 (Hardcover); 1-4299-6935-0 (PDF); 978-0-374-27563-1 (Hardcover); 978-1-4299-6935-2 (PDF)},
	abstract = {In the highly anticipated Thinking, Fast and Slow, Kahneman takes us on a groundbreaking tour of the mind and explains the two systems that drive the way we think. System 1 is fast, intuitive, and emotional; System 2 is slower, more deliberative, and more logical. Kahneman exposes the extraordinary capabilities—and also the faults and biases—of fast thinking, and reveals the pervasive influence of intuitive impressions on our thoughts and behavior. The impact of loss aversion and overconfidence on corporate strategies, the difficulties of predicting what will make us happy in the future, the challenges of properly framing risks at work and at home, the profound effect of cognitive biases on everything from playing the stock market to planning the next vacation—each of these can be understood only by knowing how the two systems shape our judgments and decisions. Engaging the reader in a lively conversation about how we think, Kahneman reveals where we can and cannot trust our intuitions and how we can tap into the benefits of slow thinking. He offers practical and enlightening insights into how choices are made in both our business and our personal lives—and how we can use different techniques to guard against the mental glitches that often get us into trouble. Thinking, Fast and Slow will transform the way you think about thinking. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {Farrar, Straus and Giroux},
	author = {Kahneman, Daniel},
	year = {2011},
	keywords = {*Cognitive Processes, *Mind, *Thinking, Choice Behavior, Decision Making, Intuition, Judgment},
}

@article{lampinen_language_2024,
	title = {Language models, like humans, show content effects on reasoning tasks},
	volume = {3},
	doi = {10.1093/pnasnexus/pgae233},
	abstract = {Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.},
	number = {7},
	urldate = {2024-12-17},
	journal = {PNAS Nexus},
	author = {Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie C Y and Sheahan, Hannah R and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
	editor = {Abbott, Derek},
	year = {2024},
	pages = {pgae233},
	file = {2207.07051v4.pdf:/home/thinklex/Zotero/storage/QQSPF6LZ/2207.07051v4.pdf:application/pdf;Lampinen et al. - 2024 - Language models, like humans, show content effects.pdf:/home/thinklex/Zotero/storage/678XYB4G/Lampinen et al. - 2024 - Language models, like humans, show content effects.pdf:application/pdf},
}

@article{bricken_towards_2023,
	title = {Towards monosemanticity: {Decomposing} language models with dictionary learning},
	volume = {2},
	journal = {Transformer Circuits Thread},
	author = {Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and {others}},
	year = {2023},
        url = {https://transformer-circuits.pub/2023/monosemantic-features},
}

@inproceedings{mikolov_linguistic_2013,
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	booktitle = {{NAACL13}},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	year = {2013},
	pages = {746--751},
	annote = {Place: Atlanta, Georgia},
	file = {Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:/home/thinklex/Zotero/storage/GPEDMPDS/Mikolov et al. - 2013 - Linguistic Regularities in Continuous Space Word R.pdf:application/pdf},
}

@misc{lam_closer_2024,
	title = {A {Closer} {Look} at {Logical} {Reasoning} with {LLMs}: {The} {Choice} of {Tool} {Matters}},
	shorttitle = {A {Closer} {Look} at {Logical} {Reasoning} with {LLMs}},
	url = {http://arxiv.org/abs/2406.00284},
	abstract = {The emergence of Large Language Models (LLMs) has demonstrated promising progress in solving logical reasoning tasks effectively. Several recent approaches have proposed to change the role of the LLM from the reasoner into a translator between natural language statements and symbolic representations which are then sent to external symbolic solvers to resolve. This paradigm has established the current state-of-the-art result in logical reasoning (i.e., deductive reasoning). However, it remains unclear whether the variance in performance of these approaches stems from the methodologies employed or the specific symbolic solvers utilized. There is a lack of consistent comparison between symbolic solvers and how they influence the overall reported performance. This is important, as each symbolic solver also has its own input symbolic language, presenting varying degrees of challenge in the translation process. To address this gap, we perform experiments on 3 deductive reasoning benchmarks with LLMs augmented with widely used symbolic solvers: Z3, Pyke, and Prover9. The tool-executable rates of symbolic translation generated by different LLMs exhibit a near 50\% performance variation. This highlights a significant difference in performance rooted in very basic choices of tools. The almost linear correlation between the executable rate of translations and the accuracy of the outcomes from Prover9 highlight a strong alignment between LLMs ability to translate into Prover9 symbolic language, and the correctness of those translations.},
	urldate = {2024-12-16},
	publisher = {arXiv},
	author = {Lam, Long Hei Matthew and Thatikonda, Ramya Keerthy and Shareghi, Ehsan},
	year = {2024},
	doi = {10.48550/arXiv.2406.00284},
	keywords = {Computer Science - Computation and Language},
	annote = {Issue: arXiv:2406.00284 arXiv: 2406.00284 [cs]},
	file = {Lam et al. - 2024 - A Closer Look at Logical Reasoning with LLMs The .pdf:/home/thinklex/Zotero/storage/IXF95JM2/Lam et al. - 2024 - A Closer Look at Logical Reasoning with LLMs The .pdf:application/pdf},
}

@book{papadimitriou_computational_1994,
	title = {Computational complexity},
	publisher = {Addison-Wesley},
	author = {Papadimitriou, Christos H.},
	year = {1994},
	file = {Papadimitriou - 1994 - Computational complexity.pdf:/home/thinklex/Zotero/storage/NQZLLCJA/Papadimitriou - 1994 - Computational complexity.pdf:application/pdf},
}

@inproceedings{cook_complexity_1971-1,
	series = {{STOC}71},
	title = {The complexity of theorem-proving procedures},
	doi = {10.1145/800157.805047},
	booktitle = {{ACM} {STOC71}},
	publisher = {Association for Computing Machinery},
	author = {Cook, Stephen A.},
	year = {1971},
	pages = {151--158},
	annote = {Place: New York, NY, USA},
	file = {800157.805047.pdf:/home/thinklex/Zotero/storage/ZR2TSLDD/800157.805047.pdf:application/pdf},
}

@inproceedings{beiser_bypassing_2024,
	title = {Bypassing the {ASP} {Bottleneck}: {Hybrid} {Grounding} by {Splitting} and {Rewriting}},
	shorttitle = {Bypassing the {ASP} {Bottleneck}},
	doi = {10.24963/ijcai.2024/360},
	abstract = {Answer Set Programming (ASP) is a key paradigm for problems in artificial intelligence and industrial contexts. In ASP, problems are modeled via a set of rules. Over the time this paradigm grew into a rich language, enabling complex rule types like aggregate expressions. Most practical ASP systems follow a ground-and-solve pattern, where rule schemes are grounded and resulting rules are solved. There, the so-called grounding bottleneck may prevent from solving, due to sheer grounding sizes. Recently body-decoupled grounding (BDG) demonstrated how to reduce grounding sizes by delegating effort to solving. However, BDG provides limited interoperability with traditional grounders and only covers simple rule types. In this work, we establish hybrid grounding — based on a novel splitting theorem that allows us to freely combine BDG with traditional grounders. To mitigate huge groundings in practice, we define rewriting procedures for efficiently deferring grounding effort of aggregates to solving. Our experimental results indicate that this approach is competitive, especially for instances, where traditional grounding fails.},
	urldate = {2024-11-25},
	booktitle = {{IJCAI24}},
	author = {Beiser, Alexander G. and Hecher, Markus and Unalan, Kaan and Woltran, Stefan},
	year = {2024},
	pages = {3250--3258},
	annote = {Place: Jeju, South Korea},
	file = {Beiser et al. - 2024 - Bypassing the ASP Bottleneck Hybrid Grounding by .pdf:/home/thinklex/Zotero/storage/IT7578BH/Beiser et al. - 2024 - Bypassing the ASP Bottleneck Hybrid Grounding by .pdf:application/pdf},
}

@inproceedings{liu_how_2024,
	title = {How {Proficient} {Are} {Large} {Language} {Models} in {Formal} {Languages}? {An} {In}-{Depth} {Insight} for {Knowledge} {Base} {Question} {Answering}},
	booktitle = {ACL24},
	author = {Liu, Jinxi and Cao, Shulin and Shi, Jiaxin and Zhang, Tingjian and Hou, Lei and Li, Juanzi},
	year = {2024},
}

@inproceedings{dziri_faith_2023,
	title = {Faith and {Fate}: {Limits} of {Transformers} on {Compositionality}},
	booktitle = {NeurIPS23},
        pages = {70293--70332},
	author = {Dziri, Nouha and Lu, Ximing and Sclar, Melanie and Li, Xiang Lorraine and Jiang, Liwei and Lin, Bill Yuchen and Welleck, Sean and West, Peter and Bhagavatula, Chandra and Bras, Ronan Le and Hwang, Jena D. and Sanyal, Soumya and Ren, Xiang and Ettinger, Allyson and Harchaoui, Zaid and Choi, Yejin},
	year = {2023},
}

@inproceedings{xiong_can_2024,
	title = {Can {LLMs} {Express} {Their} {Uncertainty}? {An} {Empirical} {Evaluation} of {Confidence} {Elicitation} in {LLMs}},
	booktitle = {ICLR24},
	author = {Xiong, Miao and Hu, Zhiyuan and Lu, Xinyang and LI, YIFEI and Fu, Jie and He, Junxian and Hooi, Bryan},
	year = {2024},
}

@inproceedings{eiter_logic-based_2023,
	title = {A logic-based approach to contrastive explainability for neurosymbolic visual question answering},
	doi = {10.24963/ijcai.2023/408},
	abstract = {Visual Question Answering (VQA) is a well-known problem for which deep-learning is key. This poses a challenge for explaining answers to questions, the more if advanced notions like contrastive explanations (CEs) should be provided. The latter explain why an answer has been reached in contrast to a different one and are attractive as they focus on reasons necessary to flip a query answer. We present a CE framework for VQA that uses a neurosymbolic VQA architecture which disentangles perception from reasoning. Once the reasoning part is provided as logical theory, we use answer-set programming, in which CE generation can be framed as an abduction problem. We validate our approach on the CLEVR dataset, which we extend by more sophisticated questions to further demonstrate the robustness of the modular architecture. While we achieve top performance compared to related approaches, we can also produce CEs for explanation, model debugging, and validation tasks, showing the versatility of the declarative approach to reasoning.},
	booktitle = {IJCAI23},
	author = {Eiter, Thomas and Geibinger, Tobias and Higuera, Nelson and Oetsch, Johannes},
	year = {2023},
}

@article{gelfond_logic_2002,
	title = {Logic programming and knowledge representation—{The} {A}-{Prolog} perspective},
	volume = {138},
	doi = {10.1016/S0004-3702(02)00207-2},
	abstract = {In this paper we give a short introduction to logic programming approach to knowledge representation and reasoning. The intention is to help the reader to develop a ‘feel’ for the ﬁeld’s history and some of its recent developments. The discussion is mainly limited to logic programs under the answer set semantics. For understanding of approaches to logic programming built on well-founded semantics, general theories of argumentation, abductive reasoning, etc., the reader is referred to other publications.  2002 Elsevier Science B.V. All rights reserved.},
	number = {1},
	urldate = {2024-08-18},
	journal = {AI},
	author = {Gelfond, Michael and Leone, Nicola},
	year = {2002},
	pages = {3--38},
	file = {Gelfond and Leone - 2002 - Logic programming and knowledge representation—The.pdf:/home/thinklex/Zotero/storage/LXT9ZG83/Gelfond and Leone - 2002 - Logic programming and knowledge representation—The.pdf:application/pdf},
}

@article{abels_train_2021,
	title = {Train {Scheduling} with {Hybrid} {Answer} {Set} {Programming}},
	volume = {21},
	doi = {10.1017/S1471068420000046},
	abstract = {We present a solution to real-world train scheduling problems, involving routing, scheduling, and optimization, based on Answer Set Programming (ASP). To this end, we pursue a hybrid approach that extends ASP with diﬀerence constraints to account for a ﬁne-grained timing. More precisely, we exemplarily show how the hybrid ASP system clingo[DL] can be used to tackle demanding planning and scheduling problems. In particular, we investigate how to boost performance by combining distinct ASP solving techniques, such as approximations and heuristics, with preprocessing and encoding techniques for tackling large-scale, real-world train-scheduling instances.},
	number = {3},
	urldate = {2024-12-22},
	journal = {TPLP},
	author = {Abels, Dirk and Jordi, Julian and Ostrowski, Max and Schaub, Torsten and Toletti, Ambra and Wanko, Philipp},
	year = {2021},
	pages = {317--347},
	file = {Abels et al. - 2021 - Train Scheduling with Hybrid Answer Set Programmin.pdf:/home/thinklex/Zotero/storage/TGFFD9VK/Abels et al. - 2021 - Train Scheduling with Hybrid Answer Set Programmin.pdf:application/pdf},
}

@article{beiser_asp-driven_2024,
	title = {{ASP}-driven {User}-interaction with {Clinguin}},
	abstract = {We present clinguin, a system for ASP-driven user interface design. Clinguin streamlines the development of user interfaces for ASP developers by letting them build interactive prototypes directly in ASP, eliminating the need for separate frontend languages. To this end, clinguin uses a few dedicated predicates to define user interfaces and the treatment of user-triggered events. This simple design greatly facilitates the specification of user interactions with an ASP system, in our case clingo.},
	journal = {ICLP24},
	author = {Beiser, Alexander G. and Hahn, Susana and Schaub, Torsten},
	year = {2024},
	file = {Beiser et al. - ASP-driven User-interaction with Clinguin.pdf:/home/thinklex/Zotero/storage/TJLYHDIW/Beiser et al. - ASP-driven User-interaction with Clinguin.pdf:application/pdf},
}