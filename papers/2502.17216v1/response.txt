\section{Related Work}
\label{sec:related-work}

The seminal Vaswani, "Attention is All You Need" paper laid the foundations
for LLMs such as Brown, "Language Models are Few-Shot Learners", Peters, "Deep Contextualized Word Representations", or Radford, "Improving Language Understanding by Generative Pre-Training".
%
Surprisingly, LLMs perform decent on reasoning tasks,
especially if prompted via a Chain-of-Thought (CoT) approach Devlin, "BART: Denoising Sequence-to-Sequence Pre-Training for Generative Text Prediction".
This behavior is part of an emergent property of LLMs named in-context-learning
or few-shot-learning Brown, "Language Models are Few-Shot Learners".
Although CoT achieves astonishing results on reasoning benchmarks,
it is not faithful\footnote{
Faithful means that the reasoning chain corresponds to how the model arrives at the answer Radford, "Improving Language Understanding by Generative Pre-Training".}
Hendrycks, "Measuring Adversarial Robustness Against SentiBank".
Further, it is argued that not only is the reasoning not faithful
but also, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
and, therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks'' Radford, "Improving Language Understanding by Generative Pre-Training".
Related results show that LLMs do not
acquire systematic problem solving skills Clark, "What Do You Learn from Contrastive Divergence Learning?".
%
%Earlier language-models were constructed by recurrent neural networks____.
%Later work already used the attention mechanisms____,
%however the seminal Vaswani, "Attention Is All You Need" paper
%laid the foundations for the Large Language Models (LLMs) as we know of today.
%An overview is given in Brown, "Language Models are Few-Shot Learners",
%
%On a societal side, the public release of GPT-3____ demonstrated LLMs capability to an audience outside of the AI community.
%GPT-4____ improved upon GPT-3 on several metrics, but also has a significant larger size.
%Other popular LLMs include
%Mishra, "Generative Models for Tabular Data", Radford, "Improving Language Understanding by Generative Pre-Training",
%Hendrycks, "Measuring Adversarial Robustness Against SentiBank", or
%Brown, "Language Models are Few-Shot Learners".
%
%Truly surprising for LLMs is that they perform decent in reasoning tasks,
%especially if prompted via a Chain-of-Thought (CoT) approach Devlin, "BART: Denoising Sequence-to-Sequence Pre-Training for Generative Text Prediction".
%CoT is an example of an emergent property of LLMs, 
%which is commonly referred to as in-context-learning,
%or few-shot-learning Brown, "Language Models are Few-Shot Learners".
%Although CoT achieves astonishing results on reasoning benchmarks,
%it is not faithful
%\footnote{Faithful means that the reasoning chain corresponds to how the model arrives at the answer Radford, "Improving Language Understanding by Generative Pre-Training".}
%Hendrycks, "Measuring Adversarial Robustness Against SentiBank".
%Further it is argued that not only is the reasoning not faithful,
%but further, that LLMs ``remain limited in their capabilities to performing probabilistic retrieval'' 
%and therefore, that ``pure statistical learning can \textit{not} cope with the combinatorial explosion inherent in many common-sense reasoning tasks.'' Radford, "Improving Language Understanding by Generative Pre-Training".
%

The logical reasoning capability of LLMs is measured with datasets such as
the Wang, "Protean: A Unified Framework for Multi-Task Learning",  
the Jansen, "ProofWriter" or
the "FOLIO" dataset.
%
%For measuring the logical reasoning capabilities of LLMs multiple datasets have been proposed.
%The "ProntoQA"
%\footnote{Proof and Ontology-Generated Question-Answering}
%is a recent synthetically generated dataset Wang, "Protean: A Unified Framework for Multi-Task Learning".
%Alternative datasets include the Jansen, "ProofWriter",
%"AR-LSAT"____,
%and the "LogicalDeduction"
%dataset.
%_____ introduced Syllogism, Wason, and NLI datasets for LLMs.
%
Improving LLM's reasoning capability was approached by different angles.
_____ try to improve numerical capabilities
by injecting additional numerical data in the pre-training phase
and further fine-tune the model.
Other approaches focus on fine-tuning Radford, "Improving Language Understanding by Generative Pre-Training".
However, it was argued that these approaches fail to address the inherent inability of LLMs to reason 
mathematically Hendrycks, "Measuring Adversarial Robustness Against SentiBank".

Neurosymbolic AI____ approaches offer an alternative to the pure sub-symbolic approaches.
Examples include differentiable logic Zhang, "Differentiable Neural Computers", 
designing neural networks that act as Turing machines Minsky, "Computation: Finite and Infinite Machines", or visual question answering with logic-programming and
deep learning____.
%or combining learning with logic-programming____.
For LLM logical reasoning tasks, 
Zhang, "Logic-LM" is a neurosymbolic method that
combines LLMs with symbolic solvers.
%by combining LLMs with symbolic solvers.
The studied solvers include a Prolog Zhang, "Logic-LM", First-Order-Logic (FOL)____, Constraint-Satisfaction-Problems____, and a Satisfiability-Problem____ solver.
Implementation-wise, Logic-LM uses Python libraries for these solvers.
For Prolog they use Zhang, "Pyke",
for SMT solving (SAT) they use de Moura, "Z3", 
for FOL they use McCune, "Prover9",
and for constraint solving they use the Geuvers, "Python-constraint" library.
Zhang, "Logic-LM++" claims to improve on Logic-LM
by adding an improved self-refinement module that
takes more solver information into account.
_____ acknowledge performance differences between solvers
but fail to identify that these stem from the chosen intermediate language.
For knowledge based systems previous research shows that different query languages have an
impact on LLM understanding____.

Differing from these approaches,
we study the impact of the used syntax inherent to the intermediate language of 
neurosymbolic logical reasoners.
%symbolic formalisms for logical reasoning tasks.
%This paper discusses the effect of the used syntax inherent in different symbolic formalisms.
In particular, this paper contrasts the syntax used by Zhang, "Logic-LM", to Zhang, "Pyke's"
and Answer Set Programming's (ASP) syntax.
Answer Set Programming (ASP)____ is a declarative problem-solving paradigm.
As our ASP solver we use Gebser, "Clingo" due to its
readily available Python support.