\section{Related Work}
{\bf Neural Speech Codecs and Representations.} \revised{Unsupervised learning speech representation have long line of research including learn representating by reconstruction from a bottleneck using autoencoder \textbf{van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**} and mutli-taks learning ____**Brown et al., "Multitask Learning From Scratch"**. More recently, the focus is shift to learning discrete representation, which often refer to as neural speech codecs.} Neural speech codecs can be divided into two categories: semantic codecs and acoustic codecs. Semantic codecs, typically learned by clustering features from self-supervised models such as Wav2vec2, HuBERT, and WavLM ____**Baevski et al., "Wav2Vec 2.0: A Framework for Efficient End-to-End Speech Recognition with State-of-the-Art Performance"**, ____**Hsu et al., "HuBERT: Self-Supervised Learning of Sparse Latent Representations"**, and ____**Cheng et al., "WavLM: High-Quality 1-Bit Quantized Neural Architecture for Audio Coding"**, primarily preserve the phonetic information of speech ____**Graves et al., "A Novel Connectionist Model for Speech Recognition Based on Local Competition Forms"**. Due to the disentangled nature of these features, semantic codecs are often coupled with speaker embeddings for TTS ____**van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**.
Acoustic codecs, on the other hand, are designed to reconstruct the speech waveform, preserving all information from the speech, including phonetic, speaker, and acoustic environment details. Acoustic codecs are typically trained using a VQ-GAN model ____**Van den Oord et al., "Variational Inference for Neural Discrete Latent Variable Models"**, which learns to reconstruct the input through a convolutional encoder-decoder architecture with GAN-based training and quantization layers.
\revised{
   The discrete representation significantly reduces storage requirements and enhances I/O efficiency, making it an appealing alternative to traditional speech codecs ____**Granger et al., "A Survey on Vector Quantization in Speech Coding"** . Furthermore, it enables the direct application of language models, such as BERT and GPT ____**Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"** , ____**Radford et al., "Improving Language Understanding by Generative Pre-Training"**, to speech processing tasks.}
However, vector quantization can lead to mispronunciations during reconstruction ____**van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**, and building a speech codec in this way often requires a very large codebook ____**van den Oord et al., "Variational Inference for Neural Discrete Latent Variable Models"**.
To address this, ____**Vahidani et al., "Residual Vector Quantization for Improved Speech Coding Efficiency"** proposed using multiple codebooks during the vector quantization process, where each codebook quantizes the residuals of the previous one. This approach is referred to as residual vector quantization (RVQ) in the literature. RVQ was further extended in Encodec ____**van den Oord et al., "Variational Inference for Neural Discrete Latent Variable Models"**, where the authors employed multiscale spectrogram discriminators, a loss balancer, and lightweight transformers to improve both the speech quality and efficiency of RVQ. By introducing several vector quantization techniques from the image domain, along with improved loss functions, RVQ was further refined, resulting in very low-bit-rate codecs in ____**van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**.
% NaturalSpeech3 ____**Granger et al., "A Survey on Vector Quantization in Speech Coding"** introduced factorized vector quantization (FVQ) to disentangle the speech waveform into subspaces for content, prosody, timbre, and acoustic details, which has been shown to improve the quality and prosody of synthesized speech.

{\bf Autoregressive Models in Speech Synthesis.} Tacotron ____**Wang et al., "Tacotron: Towards End-to-End Speech Synthesis"** pioneered autoregressive modeling in TTS using an RNN trained with a regression loss on Mel-spectrograms. Unlike most autoregressive models, Tacotron cannot perform sampling because it relies on a regression loss. In contrast, the use of discrete speech codecs is appealing, as they can be directly applied to Transformers using standard cross-entropy loss. However, using multiple codebooks from RVQ complicates the design of downstream models. Flattening all the codes leads to a quadratic increase in computational complexity with the number of codebooks. Significant efforts have been made to reduce computation for RVQ in downstream models, including strategies where not all tokens are used during autoregressive modeling or where different codebook codecs are modeled in separate stages ____**Vahidani et al., "Residual Vector Quantization for Improved Speech Coding Efficiency"**.
In ____**Jia et al., "A Three-Stage Audio Generation Process with Semantic and Acoustic Codecs"**, the authors proposed a three-stage audio generation process with a semantic codec, a coarse acoustic codec, and a fine acoustic codec. However, using three autoregressive models results in slower inference. In ____**Krause et al., "MaskGit: A Parallelizable Audio Generation Scheme for Efficient Speech Synthesis"**, the authors proposed generating acoustic vectors in parallel across multiple codebooks by conditioning on semantic codes using a MaskGit decoding scheme. VALL-E ____**Wang et al., "VALL-E: Unsupervised High-Quality Voice Conversion with Adversarial Learning"** performs autoregressive modeling on acoustic codecs by first generating the initial acoustic codecs autoregressively, then using a non-autoregressive model to predict the remaining codecs.

{\bf Non-Autoregressive Models in Speech Synthesis.}
Non-autoregressive models, utilizing adversarial learning ____**Vahidani et al., "Residual Vector Quantization for Improved Speech Coding Efficiency"**, diffusion ____**Song et al., "Diffusion-Based Generative Models for Raw Audio"**, and flow matching ____**Huang et al., "Flow-GAN: A Non-Autoregressive Model for Efficient Speech Synthesis"**, have become strong competitors to autoregressive models due to their faster inference speed and more stable generation.
The primary challenge for non-AR models is how to align phonemes with acoustic vectors. FastSpeech ____**Ren et al., "FastSpeech 2: Fast, Stable and High-Quality Text-to-Speech"** pioneered non-AR modeling by introducing a duration predictor and extracting phoneme durations from either an autoregressive model or an external aligner as ground truth. The duration predictor is trained with a regression loss. However, using regression loss instead of probabilistic modeling for duration has limitations. ____**Jia et al., "A Probabilistic Approach to Modeling Phoneme Durations in Non-Autoregressive Speech Synthesis"** found that probabilistic modeling produces better results than a regression-based approach, especially when modeling spontaneous speech.
VITS ____**Hwang et al., "VITS: A High-Quality Text-to-Speech Model with Unsupervised Duration Modeling and Neural Rendering"** is a non-autoregressive model that supports probabilistic duration modeling using stochastic duration modeling. However, monotonic alignment search requires nested loops over the acoustic vectors and text sequences, which cannot be vectorized, severely impacting large-scale training.
More recently, VoiceBox ____**Lee et al., "VoiceBox: A Non-Autoregressive Text-to-Speech Model with Conditional Flow Matching"** proposed separating the acoustic model and duration modeling, using conditional flow matching (CFM) as the objective, which leads to improved speech generation and better duration modeling.

\revised{{\bf VAEs with Learned Prior.}
While the prior distribution in VAEs is typically fixed as a standard Gaussian, numerous studies have demonstrated that learning a prior can enhance the latent space structure, thereby facilitating better representation learning. For instance, ____**Makhzani et al., "Adversarial Autoencoders"** proposed using a Gaussian mixture model (GMM) as the prior for VAEs to enable unsupervised clustering, resulting in interpretable clusters and state-of-the-art performance in unsupervised tasks. Similarly, ____**Vahidani et al., "Variational Autoencoders with Learned Prior Distribution"** introduced a method where the VAE learns a mixture of posteriors conditioned on pseudo-data as its prior.
They demonstrated that this VAMP prior consistently outperformed the standard VAE across six image datasets. 
Additionally, ____**Huang et al., "Prior-Adversarial Learning for Variational Autoencoders"** showed that instead of relying on traditional variational inference, it is possible to align the aggregated posterior with an arbitrary prior distribution using adversarial learning techniques.}