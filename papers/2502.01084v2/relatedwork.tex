\section{Related Work}
{\bf Neural Speech Codecs and Representations.} \revised{Unsupervised learning speech representation have long line of research including learn representating by reconstruction from a bottleneck using autoencoder \citep{chorowski2019unsupervised,qian2019autovc} and mutli-taks learning \citep{ravanelli2020multi}. More recently, the focus is shift to learning discrete representation, which often refer to as neural speech codecs.} Neural speech codecs can be divided into two categories: semantic codecs and acoustic codecs. Semantic codecs, typically learned by clustering features from self-supervised models such as Wav2vec2, HuBERT, and WavLM \citep{wav2vec2,hubert,wavlm}, primarily preserve the phonetic information of speech \citep{choi2024self}. Due to the disentangled nature of these features, semantic codecs are often coupled with speaker embeddings for TTS \citep{polyak2021speech,controlvc}.
Acoustic codecs, on the other hand, are designed to reconstruct the speech waveform, preserving all information from the speech, including phonetic, speaker, and acoustic environment details. Acoustic codecs are typically trained using a VQ-GAN model \citep{vqvae,esser2021taming}, which learns to reconstruct the input through a convolutional encoder-decoder architecture with GAN-based training and quantization layers.
\revised{
   The discrete representation significantly reduces storage requirements and enhances I/O efficiency, making it an appealing alternative to traditional speech codecs \citep{zeghidour2021soundstream,defossez2022high} . Furthermore, it enables the direct application of language models, such as BERT and GPT \citep{devlin2018bert,brown2020language}, to speech processing tasks.}
However, vector quantization can lead to mispronunciations during reconstruction \citep{casanova2024xtts,betker2023better}, and building a speech codec in this way often requires a very large codebook \citep{zeghidour2021soundstream}.
To address this, \citep{zeghidour2021soundstream} proposed using multiple codebooks during the vector quantization process, where each codebook quantizes the residuals of the previous one. This approach is referred to as residual vector quantization (RVQ) in the literature. RVQ was further extended in Encodec \citep{defossez2022high}, where the authors employed multiscale spectrogram discriminators, a loss balancer, and lightweight transformers to improve both the speech quality and efficiency of RVQ. By introducing several vector quantization techniques from the image domain, along with improved loss functions, RVQ was further refined, resulting in very low-bit-rate codecs in \citep{kumar2024high}.
% NaturalSpeech3 \citep{ju2024naturalspeech} introduced factorized vector quantization (FVQ) to disentangle the speech waveform into subspaces for content, prosody, timbre, and acoustic details, which has been shown to improve the quality and prosody of synthesized speech.

{\bf Autoregressive Models in Speech Synthesis.} Tacotron \citep{tacotron,wang2017tacotron} pioneered autoregressive modeling in TTS using an RNN trained with a regression loss on Mel-spectrograms. Unlike most autoregressive models, Tacotron cannot perform sampling because it relies on a regression loss. In contrast, the use of discrete speech codecs is appealing, as they can be directly applied to Transformers using standard cross-entropy loss. However, using multiple codebooks from RVQ complicates the design of downstream models. Flattening all the codes leads to a quadratic increase in computational complexity with the number of codebooks. Significant efforts have been made to reduce computation for RVQ in downstream models, including strategies where not all tokens are used during autoregressive modeling or where different codebook codecs are modeled in separate stages \citep{copet2024simple,valle}.
In \citep{borsos2023audiolm}, the authors proposed a three-stage audio generation process with a semantic codec, a coarse acoustic codec, and a fine acoustic codec. However, using three autoregressive models results in slower inference. In \citep{borsos2023soundstorm}, the authors proposed generating acoustic vectors in parallel across multiple codebooks by conditioning on semantic codes using a MaskGit decoding scheme. VALL-E \citep{valle} performs autoregressive modeling on acoustic codecs by first generating the initial acoustic codecs autoregressively, then using a non-autoregressive model to predict the remaining codecs.

{\bf Non-Autoregressive Models in Speech Synthesis.}
Non-autoregressive models, utilizing adversarial learning \citep{hifigan,vits,lim2022jets}, diffusion \citep{jeong2021diff,popov2021grad,huang2022prodiff,liu2022diffgan,li2024styletts}, and flow matching \citep{voicebox,mehta2024matcha}, have become strong competitors to autoregressive models due to their faster inference speed and more stable generation.
The primary challenge for non-AR models is how to align phonemes with acoustic vectors. FastSpeech \citep{fastspeech,fastspeech2} pioneered non-AR modeling by introducing a duration predictor and extracting phoneme durations from either an autoregressive model or an external aligner as ground truth. The duration predictor is trained with a regression loss. However, using regression loss instead of probabilistic modeling for duration has limitations. \citep{mehta2024should} found that probabilistic modeling produces better results than a regression-based approach, especially when modeling spontaneous speech.
VITS \citep{vits} is a non-autoregressive model that supports probabilistic duration modeling using stochastic duration modeling. However, monotonic alignment search requires nested loops over the acoustic vectors and text sequences, which cannot be vectorized, severely impacting large-scale training.
More recently, VoiceBox \citep{voicebox} proposed separating the acoustic model and duration modeling, using conditional flow matching (CFM) as the objective, which leads to improved speech generation and better duration modeling.

\revised{{\bf VAEs with Learned Prior.}
While the prior distribution in VAEs is typically fixed as a standard Gaussian, numerous studies have demonstrated that learning a prior can enhance the latent space structure, thereby facilitating better representation learning. For instance, \citep{dilokthanakul2016deep} proposed using a Gaussian mixture model (GMM) as the prior for VAEs to enable unsupervised clustering, resulting in interpretable clusters and state-of-the-art performance in unsupervised tasks. Similarly, \citep{tomczak2018vae} introduced a method where the VAE learns a mixture of posteriors conditioned on pseudo-data as its prior. 
They demonstrated that this VAMP prior consistently outperformed the standard VAE across six image datasets. 
Additionally, \citep{makhzani2015adversarial} showed that instead of relying on traditional variational inference, it is possible to align the aggregated posterior with an arbitrary prior distribution using adversarial learning techniques.
}