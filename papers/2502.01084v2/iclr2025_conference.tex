
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{algorithmicx}
\usepackage{xcolor}
\newcommand{\revised}[1]{{#1}}
\algnewcommand{\LineComment}[1]{\Statex \(\triangleright\) #1}
\title{Continuous Autoregressive Modeling with Stochastic Monotonic Alignment for Speech Synthesis}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\author{Weiwei Lin \\
Department of Electrical and Electronic Engineering \\
The Hong Kong Polytechnic University\\
Hong Kong SAR, China \\
\texttt{weiwei.lin@connect.polyu.hk} \\
\And
Chenghan He  \\
Department of Computing \\
The Hong Kong Polytechnic University \\
Hong Kong SAR, China \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\renewcommand{\ve}[1]{\boldsymbol{\mathbf{#1}}}
\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
   We propose a novel autoregressive modeling approach for speech synthesis, combining a variational autoencoder (VAE) with a multi-modal latent space and an autoregressive model that uses Gaussian Mixture Models (GMM) as the conditional probability distribution. Unlike previous methods that rely on residual vector quantization, our model leverages continuous speech representations from the VAE's latent space, greatly simplifying the training and inference pipelines. We also introduce a stochastic monotonic alignment mechanism to enforce strict monotonic alignments. Our approach significantly outperforms the state-of-the-art autoregressive model VALL-E in both subjective and objective evaluations, achieving these results with only 10.3\% of VALL-E's parameters. This demonstrates the potential of continuous speech language models as a more efficient alternative to existing quantization-based speech language models. Sample audio can be found at \url{https://tinyurl.com/gmm-lm-tts}.
\end{abstract}
\section{Introduction}

Transformers trained with autoregressive (AR) objectives have become the dominant approach in natural language processing (NLP) \citep{radford2019language,brown2020language,vaswani2017attention}. The vast successes in NLP have inspired researchers to apply Transformers and autoregressive objectives to image and speech domains as well \citep{ramesh2021zero,valle,betker2023better}. Since speech and images are continuous signals, discretization is a critical first step before applying discrete autoregressive training. As a result, autoregressive modeling of images and audio typically involves two stages of training.
In the first stage, a VQ-VAE \citep{vqvae} or a variant of VQ-VAE \citep{zeghidour2021soundstream,defossez2022high,kumar2024high} is trained to encode input data into discrete latent representations using a vector quantization bottleneck. After training the VQ-VAE, an autoregressive model is trained on the discrete latent codes produced by the encoder \citep{ramesh2021zero}. The AR model captures the sequential dependencies in the latent space, learning to predict the next latent code based on previous ones, which enables high-fidelity generation.
One of the most attractive features of AR modeling in speech is its ability to model both the distribution of acoustic vectors and the duration simultaneously. This is particularly beneficial compared to non-autoregressive models, which often require separate duration modules and external alignments \citep{fastspeech2,fastspeech,styletts}. Another advantage of AR models is their capability for in-context learning of speaker style and voice through prompting, which has proven to be more powerful than traditional speaker embedding-based voice cloning \citep{valle,casanova2024xtts}.

It has been found that to faithfully reconstruct audio signals, the required codebook size can be prohibitively large \citep{zeghidour2021soundstream}, and the use of discrete codes often introduces artifacts and mispronunciation during reconstruction \citep{casanova2024xtts,betker2023better}. Additionally, codebooks are often under-utilized, and training stability can be compromised due to the use of the Straight-Through (ST) estimator \citep{lancucki2020robust,mentzer2023finite}. While using multiple codebooks with residual vector quantization (RVQ) mitigates some of these issues \citep{zeghidour2021soundstream,defossez2022high}, it comes at the cost of requiring a specialized second-stage model to handle the additional codebooks \citep{borsos2023soundstorm,valle}. To address the computational overhead introduced by these extra codebooks, some approaches avoid using all tokens in autoregressive modeling \citep{valle,lyth2024natural,copet2024simple}.

All of the aforementioned issues stem from the use of quantization during VAE training. In this paper, we demonstrate that vector quantization is not a necessary prerequisite for learning an autoregressive model. By constraining the latent representation to be a continuous multimodal distribution during VAE training and recovering it in the second stage with a continuous autoregressive model, we are able to build an autoregressive model that retains all the advantages of AR modeling (prompting, integrated duration modeling, and sampling) without many of the challenges associated with VQ. Table~\ref{tab:method_comparison} compares our proposed continuous autoregressive model with VALL-E \citep{valle}, a state-of-the-art autoregressive text-to-speech (TTS) model. Without the need for multiple codebooks from residual vector quantization, we are able to use a single Conformer model to perform autoregressive modeling on all tokens, cutting theoretical computation in half compared to VALL-E's AR and NAR mixed approach. Furthermore, the removal of quantization allows us to build an ultra-compact TTS model with only 51.5M parameters, which is 10.3\% of VALL-E's size. Despite its smaller size, our model achieves lower WER and higher MOS than VALL-E, thanks to the continuous autoregressive modeling approach.

\begin{table}
   \centering
   \caption{Comparison of VALL-E and Our Method.}
   \label{tab:method_comparison}
   \begin{tabular}{lll}
      \toprule
      Method               &                    VALL-E &     Ours  \\ 
      \midrule
      Codec Model          &                   RVQ &  GMM-VAE \\
      TTS Model            &  Decoder-only-Transformers &   GMM-LM-Mini \\
      Codec Related Param. &                       16.7M &        0 \\
      TTS Model Param.         &                     496.5M &      51.5M \\
      Dominant Computation &                     $2DL^2$ &   $DL^2$  \\
      All Tokens AR       &                        No &      Yes \\
      Strict Mono. Alignment &                        No &      Yes \\
      \midrule
      WER           $\downarrow$        &      6.04                   &     {\bf 2.83}\\
     Q-MOS          $\uparrow$        &           3.54               &    {\bf  4.11}\\
      \bottomrule
   \end{tabular}
\end{table}
   
\section{Related Work}
{\bf Neural Speech Codecs and Representations.} \revised{Unsupervised learning speech representation have long line of research including learn representating by reconstruction from a bottleneck using autoencoder \citep{chorowski2019unsupervised,qian2019autovc} and mutli-taks learning \citep{ravanelli2020multi}. More recently, the focus is shift to learning discrete representation, which often refer to as neural speech codecs.} Neural speech codecs can be divided into two categories: semantic codecs and acoustic codecs. Semantic codecs, typically learned by clustering features from self-supervised models such as Wav2vec2, HuBERT, and WavLM \citep{wav2vec2,hubert,wavlm}, primarily preserve the phonetic information of speech \citep{choi2024self}. Due to the disentangled nature of these features, semantic codecs are often coupled with speaker embeddings for TTS \citep{polyak2021speech,controlvc}.
Acoustic codecs, on the other hand, are designed to reconstruct the speech waveform, preserving all information from the speech, including phonetic, speaker, and acoustic environment details. Acoustic codecs are typically trained using a VQ-GAN model \citep{vqvae,esser2021taming}, which learns to reconstruct the input through a convolutional encoder-decoder architecture with GAN-based training and quantization layers.
\revised{
   The discrete representation significantly reduces storage requirements and enhances I/O efficiency, making it an appealing alternative to traditional speech codecs \citep{zeghidour2021soundstream,defossez2022high} . Furthermore, it enables the direct application of language models, such as BERT and GPT \citep{devlin2018bert,brown2020language}, to speech processing tasks.}
However, vector quantization can lead to mispronunciations during reconstruction \citep{casanova2024xtts,betker2023better}, and building a speech codec in this way often requires a very large codebook \citep{zeghidour2021soundstream}.
To address this, \citep{zeghidour2021soundstream} proposed using multiple codebooks during the vector quantization process, where each codebook quantizes the residuals of the previous one. This approach is referred to as residual vector quantization (RVQ) in the literature. RVQ was further extended in Encodec \citep{defossez2022high}, where the authors employed multiscale spectrogram discriminators, a loss balancer, and lightweight transformers to improve both the speech quality and efficiency of RVQ. By introducing several vector quantization techniques from the image domain, along with improved loss functions, RVQ was further refined, resulting in very low-bit-rate codecs in \citep{kumar2024high}.
% NaturalSpeech3 \citep{ju2024naturalspeech} introduced factorized vector quantization (FVQ) to disentangle the speech waveform into subspaces for content, prosody, timbre, and acoustic details, which has been shown to improve the quality and prosody of synthesized speech.

{\bf Autoregressive Models in Speech Synthesis.} Tacotron \citep{tacotron,wang2017tacotron} pioneered autoregressive modeling in TTS using an RNN trained with a regression loss on Mel-spectrograms. Unlike most autoregressive models, Tacotron cannot perform sampling because it relies on a regression loss. In contrast, the use of discrete speech codecs is appealing, as they can be directly applied to Transformers using standard cross-entropy loss. However, using multiple codebooks from RVQ complicates the design of downstream models. Flattening all the codes leads to a quadratic increase in computational complexity with the number of codebooks. Significant efforts have been made to reduce computation for RVQ in downstream models, including strategies where not all tokens are used during autoregressive modeling or where different codebook codecs are modeled in separate stages \citep{copet2024simple,valle}.
In \citep{borsos2023audiolm}, the authors proposed a three-stage audio generation process with a semantic codec, a coarse acoustic codec, and a fine acoustic codec. However, using three autoregressive models results in slower inference. In \citep{borsos2023soundstorm}, the authors proposed generating acoustic vectors in parallel across multiple codebooks by conditioning on semantic codes using a MaskGit decoding scheme. VALL-E \citep{valle} performs autoregressive modeling on acoustic codecs by first generating the initial acoustic codecs autoregressively, then using a non-autoregressive model to predict the remaining codecs.

{\bf Non-Autoregressive Models in Speech Synthesis.}
Non-autoregressive models, utilizing adversarial learning \citep{hifigan,vits,lim2022jets}, diffusion \citep{jeong2021diff,popov2021grad,huang2022prodiff,liu2022diffgan,li2024styletts}, and flow matching \citep{voicebox,mehta2024matcha}, have become strong competitors to autoregressive models due to their faster inference speed and more stable generation.
The primary challenge for non-AR models is how to align phonemes with acoustic vectors. FastSpeech \citep{fastspeech,fastspeech2} pioneered non-AR modeling by introducing a duration predictor and extracting phoneme durations from either an autoregressive model or an external aligner as ground truth. The duration predictor is trained with a regression loss. However, using regression loss instead of probabilistic modeling for duration has limitations. \citep{mehta2024should} found that probabilistic modeling produces better results than a regression-based approach, especially when modeling spontaneous speech.
VITS \citep{vits} is a non-autoregressive model that supports probabilistic duration modeling using stochastic duration modeling. However, monotonic alignment search requires nested loops over the acoustic vectors and text sequences, which cannot be vectorized, severely impacting large-scale training.
More recently, VoiceBox \citep{voicebox} proposed separating the acoustic model and duration modeling, using conditional flow matching (CFM) as the objective, which leads to improved speech generation and better duration modeling.

\revised{{\bf VAEs with Learned Prior.}
While the prior distribution in VAEs is typically fixed as a standard Gaussian, numerous studies have demonstrated that learning a prior can enhance the latent space structure, thereby facilitating better representation learning. For instance, \citep{dilokthanakul2016deep} proposed using a Gaussian mixture model (GMM) as the prior for VAEs to enable unsupervised clustering, resulting in interpretable clusters and state-of-the-art performance in unsupervised tasks. Similarly, \citep{tomczak2018vae} introduced a method where the VAE learns a mixture of posteriors conditioned on pseudo-data as its prior. 
They demonstrated that this VAMP prior consistently outperformed the standard VAE across six image datasets. 
Additionally, \citep{makhzani2015adversarial} showed that instead of relying on traditional variational inference, it is possible to align the aggregated posterior with an arbitrary prior distribution using adversarial learning techniques.
}
\section{Autoregressive Modeling with Continuous Neural Speech Codec}
In this section, we introduce the three major components of our model:
(1) A Gaussian Mixture Models VAE (GMM-VAE) that compresses speech with a multi-modal latent constraint.
(2) A Gaussian Mixture Models language model (GMM-LM) that autoregressively models the compressed acoustic vectors conditioned on text.
(3) An attention mechanism that aligns the encoder and decoder in a strictly monotonic fashion.
\subsection{Learning Continuous Speech Codecs with Gaussian Mixture Models VAE}
Learning compact representations of high-dimensional signals has been a long-standing area of research. \citep{hinton2006reducing,vincent2008extracting,vae,vqvae}. Variational autoencoders (VAEs) \citep{vae} provide a general framework for learning generative models and performing data compression. Let $\ve{x}$ denote the input to the VAE and $\ve{z}$ the latent code. The objective is to maximize the Evidence Lower Bound (ELBO):
\begin{equation}
   {\cal L} = \mathbb{E}_{q(\ve{z}|\ve{x})}[\log p(\ve{x}|\ve{z})] - \text{KL}(q(\ve{z}|\ve{x}) || {\cal N}(\ve{0}, \ve{I}))
\end{equation}
where $\mathbb{E}_{q(\ve{z}|\ve{x})}[\log p(\ve{x}|\ve{z})]$ is the log-likelihood of the reconstructed data, and \( \text{KL}(q(\ve{z}|\ve{x}) || {\cal N}(\ve{0}, \ve{I})) \) is the KL divergence term that encourages the latent distribution to be close to a standard Gaussian. The KL divergence acts as a critical regularization mechanism to ensure a meaningful latent space.
In VQ-VAE \citep{vqvae}, the continuous latent variable \( \ve{z} \) is replaced by a discrete latent code \( z \), corresponding to an index in a learned codebook with \( K \) possible entries. Each codebook entry is a \( D \)-dimensional vector $\ve{v}$. Instead of sampling from a latent distribution during decoding, VQ-VAE quantizes the input, mapping the continuous encoder output to the nearest discrete code. Denote the output of the VAE encoder as $\ve{h} = E(\ve{x})$, and the posterior is defined as:
\begin{equation}
   q(z=k \mid \ve{x}) = \begin{cases}1, & \text {if } k = \underset{b}{\arg \min }\left\|\ve{h}-\ve{v}_b\right\|_2 \\ 0, & \text{otherwise}\end{cases},
   \label{eq:quant}
\end{equation}
where $E(.)$ denotes the encoder, typically composed of several convolutional layers. Since quantization is a non-differentiable operation that prevents gradient backpropagation, a straight-through estimator is used to pass gradients from the decoder to the encoder \citep{vqvae}. A vector quantization objective (codebook loss) and a commitment loss are added:
\begin{gather}
   \mathcal{L} = \log p\left(\ve{x} \mid z_q\right) + \beta\left\|z_e - \operatorname{sg}\left(z_q\right)\right\|_2^2 + \left\|\operatorname{sg}\left(z_e\right) - z_q\right\|_2^2, \\
   { \text{where} } \quad z_q(\ve{x}) = \ve{v}_k, \quad k = \operatorname{argmin}_b\left\|E(\ve{x})-\ve{v}_b\right\|_2 \nonumber
\end{gather}
where $\operatorname{sg}$ represents the stop-gradient operator. Unlike traditional VAEs, the prior $p(z)$ is assumed to be uniform, so the KL divergence is constant and has not effect on learning. Nonetheless, regularization is achieved through vector quantization. Quantization also implicitly defines a categorical distribution during training, allowing for the second stage where an autoregressive model is trained to recover the prior/latent distribution \citep{vqvae}.
However, it has been observed that the codebook size can be prohibitively large for compact discrete audio codes \citep{zeghidour2021soundstream}, and mispronunciations can occur during decoding \citep{casanova2024xtts,betker2023better}. 
% Additionally, codebooks are often under-utilized, and training stability is compromised due to the presence of the straight-through estimator \citep{lancucki2020robust,mentzer2023finite}.
\begin{figure}[t]
   \centering
   % \includegraphics[width=\textwidth]{fig2.pdf}  % Adjust the path if needed
   \includegraphics[width=\textwidth]{codec_comp.pdf}  % Adjust the path if needed
   \caption{Training procedure and architecture difference of typical residual vector quantitation codec model (left) and our proposed GMM-VAE speech codec model (right).}
   \label{fig:codec}
\end{figure}
\begin{figure}[t]
   \centering
   \includegraphics[width=0.8\textwidth]{downstream_model.pdf}  % Adjust the path if needed
   \caption{Training procedure and architecture difference of VALL-E with  residual vector quantitation codec model (left) and our proposed GMM-LM with GMM-VAE speech codec model (right).}
   \label{fig:downstream}
\end{figure}

We propose a VAE compression model that supports a multi-modal latent space, but without relying on quantization. {\bf Our key finding is that as long as the VAE stage defines a valid latent distribution that can be recovered in the second stage, quantization is not the only option.} Since our goal is not to build a generative model in the VAE stage, we use a deterministic mapping but skip the quantization step in Eq.~\ref{eq:quant}. The posterior can be written as $q(\ve{h}|\ve{x}) = \delta(\ve{h} - E(\ve{x}))$,
where $\delta$ is the Dirac delta function.
The appeal of the categorical distribution defined by VQ is its ability to model multi-modal distributions, which is essential for speech. To achieve a similar effect, we use a learned mixture of Gaussians prior with parameters $\left\{\pi_l, \boldsymbol{\mu}_l, \boldsymbol{\sigma}_l\right\}_{l=1}^L$, allowing the continuous latent distribution to be multi-modal. The ELBO for this setup can be written as:
\begin{equation}
   {\cal L} = \mathbb{E}_{q(\ve{h}|\ve{x})}[\log p(\ve{x}|\ve{h})] - \lambda \text{KL}(q(\ve{h}|\ve{x}) || \sum_{l=1}^L \pi_l \mathcal{N}\left( \boldsymbol{\mu}_l, \mathbf{\Sigma}_l\right)),
\label{eq:gmm-obj}
\end{equation}
where $\lambda$ controls the strength of the regularization, $l$ is the mixture index, and $L$ is the total number of mixtures.
Since our posterior is a deterministic function and the prior is a mixture of Gaussians, we no longer have an analytical solution for the KL divergence. Instead, we use Monte Carlo estimation to compute the KL divergence. Standard deep learning libraries, such as PyTorch, already support this estimation, and backpropagation through mixture sampling is derived in \citep{graves2016stochastic}. We refer to a VAE trained with the proposed GMM constraint as GMM-VAE. 
When training GMM-VAE for speech signals, we replace the reconstruction term with Mel-spectrogram reconstruction loss and add discriminators, as done in RVQ, Encodec, and DAC \citep{zeghidour2021soundstream,defossez2022high,kumar2024high}. Figure~\ref{fig:codec} illustrates the differences in training procedures and model architecture between the residual vector quantization model and the proposed GMM-VAE.

\subsection{Autoregressive Speech Modeling with GMM-LM}
Autoregressive models are typically associated with Transformer models that use discrete inputs and outputs \citep{vaswani2017attention,radford2019language}. The conditional probabilities at time step $t$ can be expressed as:
\begin{equation}
   P\left(z_t \mid z_{t-1}, z_{t-2}, \ldots, z_1, Y\right) = \operatorname{Categorical}\bigg(f(z_{t-1}, z_{t-2}, \ldots, z_1, Y)\bigg)
\end{equation}
where \( f() \) is a neural network that takes in the previous predictions \( z_{t-1}, z_{t-2}, \ldots, z_1 \) and the conditioning information \( Y \), to predict the next token. The network is typically trained with standard cross-entropy loss. However, the conditional probability of an autoregressive model does not necessarily need to follow a categorical distribution, as long as it is conditioned on the previous token and produces a valid distribution \citep{salimans2017pixelcnn++}. 

To this end, we define an autoregressive model for the continuous random variable \( \ve{h}_t \in \mathbb{R}^D \), where the conditional probability is represented as a mixture of Gaussians \citep{tschannen2024givt}:
\begin{equation}
   p\left(\ve{h}_t \mid \ve{h}_{t-1}, \ldots, \ve{h}_1, Y\right) = \sum_{n=1}^N {\omega}_n^{t} \mathcal{N}\left(\ve{h}_t ; \ve{\nu}_n^{t},\left(\ve{\tau}_n^{t}\right)^2\right)
\end{equation}
where \( \omega_n^t \), \( \ve{\nu}_n^t \), and \( \ve{\tau}_n^t \) represent the $n$-th mixture's weight, mean, and diagonal variances for frame $t$. \( N \) is the number of Gaussian components. The mixture parameters are produced by a neural network $f()$ that takes in the previous inputs and conditioning information:
\begin{gather}
   \left[ \omega_1^{t}, \ldots, \omega_{N}^{t}, \ve{\nu}_1^{t}, \ldots, \ve{\nu}_{N}^{t},  \ve{\tau}_1^{t}, \ldots, \ve{\tau}_{N}^{t} \right] = f(\ve{h}_{t-1}, \ldots, \ve{h}_1, Y).
\end{gather}
To ensure the mixture weights and variances are valid, we apply softmax to normalize the \( \omega_n^{t} \) values and softplus to the \( \ve{\tau}_{n}^{t} \) values in the network output.
In the case of Gaussian mixture autoregressive modeling, we no longer need an embedding layer or a softmax layer. 
Since there is no stop token in the continuous case, the model requires an additional linear classifier to predict the stop token, as in \citep{tacotron}. The autoregressive model is implemented using a Conformer encoder-decoder architecture. In this paper, we refer to the proposed Gaussian Mixture Models based autoregressive model as GMM-LM.
Figure~\ref{fig:downstream} illustrates the differences in training procedures and architecture between VALL-E with a residual vector quantization codec model (left) and our proposed GMM-LM with a GMM-VAE speech codec model (right). 
\subsection{Stochastic Hard Monotonic Alignment Learning}  
\label{sec:mono_att}
\revised{In speech synthesis, monotonic alignments often result in lower word error rates and improved naturalness in  generated speech\citep{chen2023vector}.}  Our monotonic alignment mechanism is based on the work of \citep{raffel2017online}. Given the energy \( e_{ij} \) between encoder state \( j \) and decoder state \( i \), we represent the alignment probability using a sigmoid function:
\begin{equation}
   p_{i, j}=\operatorname{Sigmoid}\left(e_{i, j}\right)
\end{equation}
If we use dot product to compute the energy, \( p_{i, j} \) can be computed in parallel but does not enforce a monotonic constraint. To enforce monotonic attention, an iterative process \citep{raffel2017online,he2019robust} is used to compute the attention score from  \( p_{i, j} \):
\begin{equation}
   \alpha_{i,j}=\alpha_{i-1, j-1}\left(1-p_{i, j-1}\right) + \alpha_{i-1, j} p_{i j}
   \label{eq:mono_att}
\end{equation}
Here, \( \alpha_{i,j} \) represents the attention weight used to sum up encoder states. However, while \( \alpha_{i,j} \) is monotonic in expectation, it does not guarantee that every alignment step will strictly follow the monotonic constraint for each sample.
In \citep{raffel2017online}, the authors propose adding Gaussian noise to the energy term to encourage binary \( p_{i, j} \) during training. However, this approach introduces bias during optimization, and discrepancy between training and test time can lead to performance issues \citep{chiu2017monotonic}.
We propose replacing the soft \( p_{i,j} \) with a binary value \( u_{i, j}^{\text{forward}} \), sampled from a Bernoulli distribution during the forward pass:
\begin{equation}
   u_{i, j}^{\text{forward}} \sim \operatorname{Bernoulli}\left(p_{i, j}\right)
   \label{eq:gumforward}
\end{equation}
This ensures that attention weights are always binary. To enable backpropagation, we approximate the gradient using a Gumbel-Softmax relaxation of the Bernoulli distribution \citep{jang2016categorical}:
\begin{equation}
   u_{i, j}^{\text{backward}}=\frac{\exp \left( (\log \left(p_{i, j}\right) + g_1) / s \right)}{\exp \left( (\log \left(p_{i, j}\right) + g_1) / s \right) + \exp \left((\log \left(1 - p_{i, j}\right) + g_2) / s \right)}
   \label{eq:gumbackward}
\end{equation}
where \( g_1, g_2 \sim \text{Gumbel}(0, 1) \), and \( s \) is a temperature parameter that controls the degree of relaxation. We schedule \( s \) to decrease gradually toward the end of training. \revised{
   Algorithm~\ref{alg:alg1} provides a detailed explanation of how encoder and decoder features are monotonically aligned and fed into the decoder to compute the negative log-likelihood.
}

\section{Experiment Setup}
\label{sec:exp-setup}
{\bf Training Procedure} All models were trained on the LibriLight corpus \citep{kahn2020libri}, which contains 60k hours of unlabeled 16kHz audiobook speech. We transcribed the audio using Whisper V2 \citep{whisper}. Since the number of utterances per speaker vary significantly, we used a class-balanced sampler to ensure each speaker was sampled the same number of times during training.
For GMM-VAE, we used the same model architecture as described in the DAC paper \citep{kumar2024high}, with modifications that removed all quantization-related layers.
Although it is possible to learn all the mixture parameters for GMM-VAE, in this paper, we only learned the means of each mixture. We assumed the mixture weights were equal, and the covariance matrices were set to identity.
For GMM-LM, we used the Conformer encoder and decoder from SpeechBrain \citep{speechbrain} with monotonic attention modification proposed in Section~\ref{sec:mono_att}. We trained models with 51.5M (Mini), 101M, 170M, and 315M (Large) parameters. The details of the GMM-LM configurations can be found in the Table~\ref{tab:model_architecture} in Appendix.
For GMM-VAE training, we used 8 GPUs, with a total batch size of 680, and each segment length was 8960. We used scheduler-free AdamW \citep{defazio2024road} with a learning rate of 0.001, training for a total of 1,000k steps. For GMM-LM training, we used 8 GPUs with a total batch size of 240. We used the same scheduler-free AdamW with a learning rate of 0.01 and a gradient norm clip of 0.005, training for a total of 200k steps. \revised{We did not observe any impact of random seeds on the runs.}

{\bf Evaluation Protocol} We used the LibriSpeech test set for evaluation \citep{librispeech}. For zero-shot TTS, we used standard objective measures such as word error rate (WER) and speaker similarity (SIM) to assess the content and speaker fidelity of the generated speech. We used wav2vec2-large \citep{wav2vec2}, fine-tuned on LibriSpeech 960h, to transcribe the generated speech and then computed the WER against the ground truth. For SIM, we used the model released by WeSpeaker \citep{wang2023wespeaker}, trained on the VoxCeleb datasets \citep{nagrani2017voxceleb,chung2018voxceleb2}. The objective metrics were computed over the entire test set.
For subjective metrics, we conducted mean opinion score (MOS) evaluations on the generated audio to assess the naturalness and speaker cloning fidelity. For each model, 40 samples were generated and evaluated by 20 listeners. The listeners were asked to evaluate the speech quality and naturalness (Q-MOS) and the speaker similarity between the prompt speech and the generated speech (S-MOS).
In addition to zero-shot TTS, we were also interested in the quality of GMM-VAE and GMM-LM reconstructions. We used standard reconstruction metrics such as Mel Distance, ViSQOL, and SI-SNR, as in \citep{kumar2024high}. For GMM-LM, we used teacher forcing to generate the reconstructed codec, which was then used by the GMM-VAE decoder to reconstruct the waveforms.

{\bf Baseline Models.} We evaluated our model against state-of-the-art TTS models such as VALL-E \citep{valle}, HierSpeech++ \citep{lee2023hierspeech++} \revised{(97M Parameters)}, and StyleTTS2 \citep{li2024styletts} \revised{(142M Parameters)}. For HierSpeech++ and StyleTTS2, we used the official implementations to train the models.
\begin{table}[ht]
   \centering
   \begin{tabular}{lcccccc}
   \toprule
   \textbf{Prompt Duration} & \textbf{Model} & \textbf{WER(\%)}$\downarrow$ & \textbf{SIM}$\uparrow$ & \textbf{S-MOS}$\uparrow$ & \textbf{Q-MOS} $\uparrow$\\
   \midrule
   & GroundTruth  & 2.26  &  - &  4.24\tiny{ $\pm$ 0.06} &  4.24\tiny{ $\pm$ 0.10} \\
   \midrule
   \multirow{4}{*}{3 Seconds}  
     & StyleTTS-2     & 3.32 & 0.70 & 3.44 \tiny{ $\pm$ 0.10} & 3.82\tiny{ $\pm$ 0.06} \\
     & HierSpeech++   & 3.45 & 0.71 & 3.52\tiny{ $\pm$ 0.13} & 3.71\tiny{ $\pm$ 0.07} \\
     & VALL-E       & 6.04 & 0.73 & 3.65\tiny{ $\pm$ 0.11} & 3.54\tiny{ $\pm$ 0.08} \\
     & Ours-Mini         & 2.83 & 0.80 & 3.84\tiny{ $\pm$ 0.13} &{\bf 4.11\tiny{ $\pm$ 0.08}} \\
     & Ours-Large         & {\bf 2.81} & {\bf 0.85} & {\bf 4.04 \tiny{ $\pm$ 0.07}} & 4.06\tiny{ $\pm$ 0.11} \\
   \midrule
   \multirow{4}{*}{8 Seconds}  
     & StyleTTS-2   & 3.34 & 0.77 & 3.57\tiny{ $\pm$ 0.07} & 3.92\tiny{ $\pm$ 0.10} \\
     & HierSpeech++ & 3.22 & 0.75 & 3.66\tiny{ $\pm$ 0.10} & 3.80\tiny{ $\pm$ 0.08} \\
     & VALL-E       & 7.54 & 0.73 & 3.71\tiny{ $\pm$ 0.11} & 3.58\tiny{ $\pm$ 0.08} \\
     & Ours-Mini         & 3.02 & 0.81 & 3.82\tiny{ $\pm$ 0.09} & 3.87\tiny{ $\pm$ 0.12} \\
     & Ours-Large         & {\bf 2.75} & {\bf 0.88} & {\bf 3.92\tiny{ $\pm$ 0.13}} & {\bf 4.02\tiny{ $\pm$ 0.10}} \\
   \midrule
   \multirow{4}{*}{15 Seconds} 
     & StyleTTS-2   & 3.02 & 0.76 & 3.52\tiny{ $\pm$ 0.13} & 4.02\tiny{ $\pm$ 0.08} \\
     & HierSpeech++ & 3.12 & 0.77 & 3.73\tiny{ $\pm$ 0.11} & 3.84\tiny{ $\pm$ 0.07} \\
     & VALL-E       & 9.68 & 0.63 & 3.62\tiny{ $\pm$ 0.12} & 3.32\tiny{ $\pm$ 0.12} \\
     & Ours-Mini         & 2.92 & 0.80 & 3.78\tiny{ $\pm$ 0.08} &{\bf  4.16\tiny{ $\pm$ 0.09}} \\
     & Ours-Large         & {\bf 2.72} & {\bf 0.91} & {\bf 4.17\tiny{ $\pm$ 0.07}} & 3.92\tiny{ $\pm$ 0.06}\\
   \bottomrule
   \end{tabular}
   \caption{Comparison of models' zero-shot TTS performance on WER, SIM, S-MOS, and P-MOS across different prompt durations.}
   \label{tab:zero-shot-tts}
\end{table}
\section{Experiment Results}
\subsection{Zero-shot Text-to-Speech}
We evaluated our model's zero-shot text-to-speech ability using prompts against both state-of-the-art autoregressive model VALL-E, and non-autoregressive models, StyleTTS-2 and HierSpeech++. 
To study the effect of prompt length on speech synthesis, we divided the prompts into three groups: 3 seconds, 8 seconds, and 15 seconds. If an utterance exceeded the specified duration, we truncated it; otherwise, we sampled another utterance from the same speaker and concatenated it until reaching the desired duration.
We used two versions of our model for evaluation: a mini version with 51.2M parameters and a large version with 315M parameters, both of which output 6 Gaussian mixture parameters with diagonal covariances. Both models were trained on encoder features from a GMM-VAE model trained with 3 mixture components, constrained by $\lambda=50$.
The results are shown in Table~\ref{tab:zero-shot-tts}. The first observation from Table~\ref{tab:zero-shot-tts} is that non-AR models such as StyleTTS2 and HierSpeech++ achieved much better WER than VALL-E. We found that VALL-E was prone to skipping words or failing in the middle of sentences, suggesting that simple cross-attention mechanisms may not be sufficient for autoregressive models. Contrary to our expectations, VALL-E's WER worsened as the prompt length increased.
Another observation is that non-AR models do not significantly benefit from longer prompts, as there was no performance improvement from 8-second to 15-second prompts in SIM and S-MOS for HierSpeech++ and StyleTTS2.
By contrast, our proposed GMM-LM model consistently outperformed both VALL-E and the state-of-the-art non-AR models in content fidelity and speaker cloning, demonstrating a clear advantage in speaker-related metrics with longer prompts. We also found that scaling our model from mini to large primarily benefited speaker-related metrics (SIM and S-MOS), while WER and general speech quality did not improve as much.
\begin{figure}[h!]
   \centering
   \includegraphics[width=0.9\textwidth]{comp.pdf}  % Adjust the path if needed
   \caption{Comparison on GMM-LM models' zero-shot TTS performance trained on Mel-Spectrogram features and GMM-VAE features.}
   \label{fig:model-size}
\end{figure}
\subsection{Mel-Spectrogram vs. GMM-VAE Features}
In this section, we evaluate GMM-LM models trained on Mel-spectrograms against GMM-LM models trained on GMM-VAE encoder features. 
We trained models with 51.5M, 101M, 170M, and 315M parameters, each outputting 6 Gaussian mixture parameters with diagonal covariances. For models using GMM-VAE features, the GMM-VAE model was trained with 3 mixture components, constrained by $\lambda=50$. For the Mel-spectrogram model, we used a 120-dimensional Mel-spectrogram with a hop length of 240.
We evaluated the models' zero-shot TTS performance using objective metrics on the LibriSpeech test set. The results are presented in Figure~\ref{fig:model-size}.
From Figure~\ref{fig:model-size}, it can be seen that GMM-LM models trained on GMM-VAE features significantly outperform those trained on Mel-spectrogram features. The discrepancy is especially pronounced with smaller models. Although increasing model size reduces the performance gap for Mel-based models, this comes at the cost of increased computation and model size. 
Thus, we conclude that GMM-LM with GMM-VAE features is a more efficient approach that delivers better performance for speech synthesis. Additionally, we found that GMM-LM models using Mel-spectrograms take much longer to achieve proper alignment compared to GMM-LM models using GMM-VAE features. We hypothesize that the GMM-VAE step helps extract semantic information, which greatly aids the alignment steps.
\begin{table}[th]
   \centering
   \begin{tabular}{lcccc}
   \toprule
   \textbf{Method} & \textbf{Model} & \textbf{Mel Distance $\downarrow$} & \textbf{ViSQOL $\uparrow$} & \textbf{SI-SDR $\uparrow$} \\ 
   \midrule
   % \multirow{2}{*}{Encode-Decode} & SoundStream      &  &  &  \\ 
   \multirow{2}{*}{Encoding-Decoding}                   & Encodec     & 1.47 & 3.41  & 5.66 \\ 
                                 %  & DAC               & 1.17 & 3.95 &  \\ 
                                  & GMM-VAE           &  {\bf 1.13}& {\bf 3.96}  & {\bf 8.89} \\ 
   \midrule
   \multirow{2}{*}{Teacher Forcing}       & VALL-E         &2.26   & 2.25  &  -0.15  \\ 
         & GMM-LM     &  {\bf 1.82}   &{\bf 2.86}  & {\bf 4.45} \\ 
   \bottomrule
   \end{tabular}
   \caption{Comparison of reconstruction quality for codec models (Encodec and GMM-VAE) and downstream models with teacher forcing (VALL-E and GMM-LM).}
   \label{tab:codec_recon}
\end{table}
\subsection{Reconstruction Quality of Discrete and Continuous Codecs}
In this section, we evaluate the reconstruction quality of our GMM-VAE model in comparison with the RVQ codec model, Encodec. The GMM-VAE model was trained with 3 mixture components, constrained by $\lambda=50$.
Our primary focus is on how well these models can reconstruct the input, as this greatly impacts tasks like zero-shot TTS.
Even if codec models can perfectly reconstruct the input, if the downstream models are unable to predict the correct codec, the effort is futile. Therefore, we also evaluate the reconstruction quality with downstream models such as VALL-E and our GMM-LM. Specifically, we encode the input speech to generate speech codecs, then use VALL-E or GMM-LM in teacher forcing mode to predict the codec. Finally, we reconstruct the speech by decoding the teacher-forced predicted codec.
We use common speech reconstruction quality metrics, including Mel Distance, ViSQOL \citep{hines2015visqol}, and SI-SDR. The results are presented in Table~\ref{tab:codec_recon}. It is clear from the table that our GMM-VAE significantly outperforms Encodec in speech reconstruction. The performance gap is even larger when the downstream models are included.
\begin{table}[t]
   \centering
   \begin{tabular}{@{}c c *{5}{S[table-format=1.2]}@{}}
   \toprule
   \textbf{No. Gaussian} & \textbf{Mode} & \multicolumn{5}{c}{\boldmath$\lambda$ } \\ 
   \cmidrule(lr){3-7}
   & & {\boldmath$0.1$} & \textbf{1} & \textbf{10} & \textbf{50} & \textbf{100} \\
   \midrule
   1 & GMM-VAE Recon & 0.76 & 0.89 & 1.14 & 1.67 & 2.06 \\
   1 & GMM-LM Teacher Forcing     & {--} & 4.62 & 3.82 & 3.16 & 4.48 \\
   \midrule
   3 & GMM-VAE Recon. & 0.71 & 0.77 & 0.98 & {\bf 1.13} & 1.77 \\
   3 & GMM-LM Teacher Forcing      & 6.72 & 3.04 & 1.87 & {\bf 1.82} & 4.27 \\
   \midrule
   6 & GMM-VAE Recon. & 0.73 & 0.86 & 0.95 & 1.37 & 1.83 \\
   6 & GMM-LM Teacher Forcing      & 4.63 & 4.07 & 2.54 & 2.44 & 5.26 \\
   \bottomrule
   \end{tabular}
   \caption{Reconstruction Quality (Mel Distance $\downarrow$) of GMM-VAE and GMM-LM (with teacher forcing) across different number of Gaussians and $\lambda$ values.}
   \label{tab:lambda-effect}
   \end{table}   
\subsection{Ablation Study}

{\bf Divergence constraint.} Since we no longer perform quantization when training the speech codec, the divergence constraint in Eq.~\ref{eq:gmm-obj} becomes the only factor that directly influences the latent distribution, aside from the reconstruction loss. Because our goal is to use the codec for downstream models rather than compress speech per se, we evaluate both the reconstruction quality of GMM-VAE and the downstream GMM-LM reconstruction with teacher forcing, using Mel-spectrogram distance. The GMM-LM model used is the large version with 6 Gaussian mixtures and diagonal covariances.
Specifically, we investigate how the value of $\lambda$ and the number of Gaussian means in the divergence constraint affect reconstruction. The results are presented in Table~\ref{tab:lambda-effect}. It is important to note that without any divergence constraint, we were unable to successfully train the downstream GMM-LM models.
As expected, the VAE reconstruction Mel distance increases with larger $\lambda$. However, the effect of $\lambda$ on downstream model reconstruction is less straightforward. With $\lambda=0.1$, although we observed very low Mel distance, we were unable to train downstream models (the model could not produce intelligible speech conditioned on text). When increasing $\lambda$ from 1 to 50, although the GMM-VAE reconstruction quality decreases, the downstream model’s teacher forcing reconstruction quality improves.
In terms of the number of Gaussians in the constraint term, we found that using three Gaussians yields better results than using a single Gaussian, with both GMM-VAE and GMM-LM showing smaller Mel distances. However, increasing the number of Gaussians to 6 led to a general decrease in reconstruction quality. Since our focus is on improving the GMM-LM generation quality, we selected three Gaussians with $\lambda=50$ as the optimal hyper-parameters.
 
{\bf GMM-LM Parameterization.} 
The GMM parameterization in GMM-LM directly affects the output distribution. We aim to investigate how the number of Gaussian mixtures and the choice between full or diagonal covariance matrices impact model performance.
We assess the generated speech using objective metrics such as WER and SIM. Furthermore, since GMM parameterization also influences the sampling process, we measure the diversity of the generated samples under different parameterization schemes. Specifically, given the target text and speech prompt, we randomly sample from the parameterized GMM to generate speech three times. We ask human evaluators to rate the diversity of the three samples on a scale from 0 to 5. These results are compared with VALL-E, StyleTTS2, and HierSpeech++, as presented in Table~\ref{tab:mixture_param_comparison}.
For comparison, we also included Tacotron’s approach, shown in the first row of Table~\ref{tab:mixture_param_comparison}, which trains an autoregressive model using only the mean and a regression loss. This approach does not allow for sampling. We found that using a single Gaussian with an L1 loss led to weaker performance compared to models with multiple mixtures. As we increased the mixture count to 3, and then to 6, we observed improved diversity in both diagonal and full covariance models.
However, the WER improvement was mixed: generally the full covariance model worsened with more mixtures, while the diagonal covariance model maintained approximately the same WER. Finally, we did not observe better results with 10 mixtures. In conclusion, there is no clear advantage in using full covariance matrices or increasing the number of mixtures beyond 6.
When comparing the speech diversity of GMM-LM with VALL-E, StyleTTS2, and HierSpeech++, our model consistently produced significantly more diverse samples. For StyleTTS2 and HierSpeech++, the multiple generated samples showed little variation. For VALL-E, while some degree of diversity were observed, the model frequently failed during random sampling. When excluding failed samples, the diversity remained limited. In contrast, our model consistently generated samples with diverse speaking styles.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
   \begin{table}[th]
      \centering
      \begin{tabular}{cc|cccc}
      \toprule
      \textbf{No. Mixture} & \textbf{Variance Type} & \textbf{Loss} & \textbf{WER $\downarrow$} & \textbf{SIM $\uparrow$} & \textbf{Diversity $\uparrow$} \\ 
      \midrule
      \multirow{3}{*}{1}  & None     & L1 Mel-Dist & 4.29 & 0.75 & - \\ 
                          & Diagonal & NLL        & 3.34 & 0.81  & 2.17 \\ 
                          & Full     & NLL        & 3.45 & 0.82 &  2.32\\ 
      \midrule
      \multirow{2}{*}{3}  & Diagonal & NLL        & 2.89 & 0.85 & 3.14 \\ 
                          & Full     & NLL        & 3.74 & 0.72 &  2.85\\ 
      \midrule
      \multirow{2}{*}{6}  & Diagonal & NLL        & {\bf 2.72} & {\bf 0.91}  & {\bf 3.42} \\ 
                          & Full     & NLL        & 4.82 & 0.69 & 3.01 \\ 
      \midrule
      10                  & Diagonal & NLL        & 5.21 & 0.71 & 3.08 \\ 
      \midrule
      \multicolumn{2}{c|}{\textbf{VALL-E}}& - & 9.68 & 0.63 &1.47  \\ 
      \multicolumn{2}{c|}{\textbf{StyleTTS2}}& - & 3.02& 0.76 &0.6  \\ 
      \multicolumn{2}{c|}{\textbf{HierSpeech++}}& - & 3.12 & 0.77 &0.3  \\ 
      \bottomrule
      \end{tabular}
      \caption{Zero-shot TTS performance of GMM-LM with different mixture counts and variance types, evaluated on WER, SIM, and sample diversity.}
      \label{tab:mixture_param_comparison}
   \end{table}
\section{Conclusion}
   We presented a new autoregressive modeling approach for speech synthesis, utilizing a Gaussian Mixture VAE (GMM-VAE) for compression and a Gaussian Mixture Model language model (GMM-LM) for autoregressive generation. By eliminating the need for quantization and introducing stochastic monotonic alignment, our method ensures stable, efficient, and high-quality speech synthesis.
   Our experiments showed that this approach outperforms traditional VQ-based methods, achieving lower WER and higher MOS scores with reduced model complexity. These results demonstrate the potential of continuous latent spaces for autoregressive modeling in speech synthesis, offering a simpler and more effective alternative to VQ models.
\section{Acknowledgment}
This work was supported by the RGC of Hong Kong SAR, Grant No. PolyU 15228223. We would like to thank Prof. Man-Wai Mak for the leadership in this project.
\section{Reproducibility Statement}
   We have provided detailed descriptions of the hyperparameters and training procedures in Section~\ref{sec:exp-setup}. Our implementations are based on publicly available codebases (\href{https://github.com/descriptinc/descript-audio-codec}{descript-audio-codec} and \href{https://speechbrain.readthedocs.io/en/latest/API/speechbrain.lobes.models.transformer.Conformer.html}{SpeechBrain}) and datasets. With the detailed explanations of the training process and hyperparameters, readers should be able to easily reproduce our models. Additionally, we plan to release the code and pre-trained models soon.
\bibliography{example_paper}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
\begin{algorithm}[H]
   \caption{GMM-LM Monotonic Alignment and Decoder Training Procedure} \label{algo:stage2}
   \begin{algorithmic}[1] 
      \Require Acoustic features $\{\ve{h}_{i}\}_{i=1}^{I}$ extracted from the GMM-VAE encoder. Phoneme features \(\ve{Y} = \{\ve{y}_{j}\}_{j=1}^{J}\) from the GMM-LM text encoder, and the GMM-LM Decoder \(D\).
      
      \State Compute energy terms in parallel for all $i,j$ using the dot product:
      \[
      e_{i, j} = \ve{h}_i^{\top} \ve{y}_{j}
      \]
      
      \State Sample initial alignments for all $i,j$ from a Bernoulli distribution:
      \[
      u_{i, j}^{\text{forward}} \sim \operatorname{Bernoulli}(\text{sigmoid}(e_{i, j}))
      \]
      \State Save the Gumbel-Softmax relaxation $u_{i, j}^{\text{backward}}$ (Eq.~\ref{eq:gumbackward}) for gradient computation.
      
      \State Initialize \(\boldsymbol{\alpha}_0 = [1, 0, 0, \ldots, 0] \in \mathbb{R}^J\), $\mathcal{L}_{\mathrm{NLL}}=0$, and \(\ve{C}_0 = []\) (an empty sequence for accumulating contexts).
      
      \For{each time step \(i = 1\) to \(I-1\)}
        \State Let $\ve{u}_i = [u_{i,1}, u_{i,2}, \ldots, u_{i,J}]$.
        
        \State Refine alignments to enforce monotonicity:
        \[
        \boldsymbol{\alpha}_i = \boldsymbol{\alpha}_{i-1} \cdot \ve{u}_i + \operatorname{shift}\left(\boldsymbol{\alpha}_{i-1} \cdot (1 - \ve{u}_i), 1\right)
        \]
        \Comment{\(\text{shift}(\cdot, 1)\) shifts the vector by one position to the right, filling the leftmost entry with zero.}
        
        \State Compute the attention context using the refined alignments:
        \[
        \ve{c}_i = \ve{Y}^{\top} \boldsymbol{\alpha}_i + \ve{h}_i
        \]
        
        \State Update the accumulated attention contexts:
        \[
        \ve{C}_i = \operatorname{concat}(\ve{C}_{i-1}, \ve{c}_i)
        \]
      %   \Comment{\(\operatorname{concat}(\ve{C}_{i-1}, \ve{c}_i)\) appends \(\ve{c}_i\) to \(\ve{C}_{i-1}\).}
        
        \State Pass the accumulated context \(\ve{C}_i\) to the decoder network \(D\) to obtain $N$ mixtures parameters:
        \[
        D(\ve{C}_i) = \left[ \omega_1^{t+1}, \ldots, \omega_{N}^{t+1}, \ve{\nu}_1^{t+1}, \ldots, \ve{\nu}_{N}^{t+1},  \ve{\tau}_1^{t+1}, \ldots, \ve{\tau}_{N}^{t+1} \right] 
        \]
        \Comment{\(\omega^{i+1}_{n}\) are normalized using the softmax and \(\boldsymbol{\tau}^{i+1}_n\) are passed through the softplus to ensure they are positive.}
        
        \State Compute the negative log-likelihood for the next frame:
        \[
        \mathcal{L}_{\mathrm{NLL}} \mathrel{+}= -\log p\left(\ve{h}_{i+1} \mid \{\omega^{i+1}_{n}, \boldsymbol{\nu}^{i+1}_{n}, \boldsymbol{\tau}^{i+1}_{n}\}_{n=1}^{N}\right)
        \]
        
      \EndFor
   \end{algorithmic}
   \label{alg:alg1}
\end{algorithm}
\subsection{Ablation Study on Alignment Module}
Even with powerful neural codecs, we found that monotonic alignment forcing remains crucial for the stability of autoregressive models. We compared our monotonic alignment approach with using direct cross-attention, location-sensitive attention \citep{tacotron}, and various monotonic attention variants. It is worth noting that location-sensitive attention is not strictly monotonic, meaning it can still result in word repetition and lookahead errors. We also found that it was slow to train because energy terms are computed iteratively.
Since the alignment module primarily affects phonetic content, we used WER to measure the performance of different alignment methods. For monotonic attention, we explored the use of pre-sigmoid noise, Gumbel Softmax, and the proposed Straight-Through (ST) Gumbel in Eq.~\ref{eq:gumforward} and Eq.~\ref{eq:gumbackward}. The results are presented in Table~\ref{table:alignment_methods_comparison}. As seen from the table, using cross-attention alone led to poor results—some samples even failed to produce intelligible speech. Location-sensitive attention performed better, but it still suffered from frequent word repetition errors, and both training and inference were slow. Our proposed monotonic alignment with ST Gumbel outperformed all other methods, including the original monotonic attention with pre-sigmoid noise.
\begin{table}[ht]
   \centering
   \begin{tabular}{lccccc}
   \toprule
   \textbf{Alignment} & \textbf{Cross Att.} & \textbf{Loc. Att.} & \multicolumn{3}{c}{\textbf{Monotonic Att.}} \\
   \cmidrule(lr){4-6}
    & & & \textbf{w. Noise} & \textbf{w. Gumbel} & \textbf{w. ST Gumbel} \\
   \midrule
   \textbf{WER(\%)} &6.6 &  6.02 & 3.77 &3.34 & {\bf 2.72}\\
   \bottomrule
   \end{tabular}
   \caption{Word Error Rate (WER) comparison of various alignment methods}
   \label{table:alignment_methods_comparison}
   \end{table}   
\subsection{Details of Model Architecture}

For GMM-LM, we used the Conformer encoder and decoder from SpeechBrain \citep{speechbrain} with monotonic attention modification proposed in Section~\ref{sec:mono_att}. We trained models with 51.5M (Mini), 101M, 170M, and 315M (Large) parameters. The details of the GMM-LM configurations can be found in the Table~\ref{tab:model_architecture}.
\begin{table}[h!]
   \centering
   \begin{tabular}{lcccc}
   \hline
   \textbf{Name} & \textbf{Mini} & \textbf{101M} & \textbf{170M} & \textbf{Large} \\
   \hline
   \multicolumn{5}{l}{\textbf{Encoder}} \\
   \hline
   \textbf{No. Layers} & 3 & 3 & 3 & 3 \\
   \textbf{Encoder Dim.}   & 512 & 512 & 512 & 512 \\
   \textbf{Feedforward Dim.}     & 1024 & 1024 & 1024 & 1024 \\
   \hline
   \multicolumn{5}{l}{\textbf{Decoder}} \\
   \hline
   \textbf{No. Layers} & 6 & 6 & 6 & 12 \\
   \textbf{Decoder Dim.}   & 512 & 768 & 1024 & 1024 \\
   \textbf{Feedforward Dim.}     & 2048 & 3144 & 4096 & 4096 \\
   \hline
   \textbf{Total Params. (M)} & 51.5 & 101 & 170 & 315 \\
   \hline
   \end{tabular}
   \label{tab:model_architecture}
   \caption{Model Architecture and Parameters}
   \end{table}

\subsection{Reconstruction Quality on Training Set}
\revised{
   Table~\ref{tab:lambda-effect} shows that the GMM-VAE with 3 mixtures and $\lambda =50$ achieves the best reconstruction quality on the evaluation set. To further investigate the underperformance of the 6-mixture model, we selected a subset of the training data (randomly sampled 200 speakers with 5 utterances each) and computed the reconstruction loss. The results, along with the evaluation set reconstruction loss, are presented in Table~\ref{tab:lambda-effect-training}.
   From Table~\ref{tab:lambda-effect-training}, we observed that the reconstruction loss on the training set is consistently lower than that on the evaluation set across all configurations. This discrepancy may be attributed to a slight domain difference between the LibriSpeech evaluation set and the LibriSpeech-Light training set, with the latter containing noisier recordings. Another key observation is that the 6-mixture model achieves a lower reconstruction loss on the training set compared to the 3-mixture model, yet it performs slightly worse in terms of reconstruction quality on the evaluation set. This suggests that the 6-mixture model may suffer from overfitting.
}

\subsection{Zero-shot Voice Clone Noise Robustness}
\revised{
In this section, we evaluate the robustness of TTS zero-shot voice cloning against noise. Specifically, we added varying levels of noise to the LibriSpeech clean test set and used the noisy audio as prompt input for the TTS models. The generated speech was assessed using WER and SIM metrics under different noise levels, with the results presented in Table~\ref{tab:wer_sim_comparison}.
From the table, we observe that, with the exception of VALL-E, most models demonstrate robustness to noisy prompts in terms of WER. However, speaker similarity (SIM) significantly decreases for models like StyleTTS-2 and HierSpeech+++. In contrast, the proposed method shows strong robustness to noise for both WER and SIM, particularly in its large version, where WER and SIM degrade only slightly across different noise levels.
}
\begin{table}[h!]
   \centering
   \begin{tabular}{lcccccc}
       \toprule
       \multirow{2}{*}{\textbf{Model}} & \multicolumn{3}{c}{\textbf{WER (\%)}} & \multicolumn{3}{c}{\textbf{SIM}} \\
       \cmidrule(lr){2-4} \cmidrule(lr){5-7}
        & \textbf{-20 dB} & \textbf{-15 dB} & \textbf{-10 dB} & \textbf{-20 dB} & \textbf{-15 dB} & \textbf{-10 dB} \\
       \midrule
       StyleTTS-2      & 3.45 & 3.49 & 3.54 & 0.72 & 0.71 & 0.68 \\
       HierSpeech+++   & 3.61 & 3.72 & 3.91 & 0.69 & 0.64 & 0.58 \\
       VALL-E          & 9.84 & 10.86 & 12.61 & 0.61 & 0.54 & 0.51 \\
       Ours-Mini       & 2.85 & 3.01 & 3.04 & 0.77 & 0.74 & 0.74 \\
       Ours-Large      & {\bf 2.77} & {\bf 2.82} & {\bf 2.97} & {\bf 0.88} & {\bf 0.87} & {\bf 0.85} \\
       \bottomrule
   \end{tabular}
   \caption{WER and SIM comparison across models under different prompt noise levels. Lower WER and higher SIM indicate better performance.}
   \label{tab:wer_sim_comparison}
\end{table}
   \begin{table}[t]
      \centering
      \begin{tabular}{@{}c c *{5}{S[table-format=1.2]}@{}}
      \toprule
      \textbf{No. Gaussian} & \textbf{Mode} & \multicolumn{5}{c}{\boldmath$\lambda$ } \\ 
      \cmidrule(lr){3-7}
      & & {\boldmath$0.1$} & \textbf{1} & \textbf{10} & \textbf{50} & \textbf{100} \\
      \midrule
      1 & Training Set  & 0.72 & 0.78 & 0.98 & 1.54 & 1.85 \\
      1 & Evaluation Set & 0.76 & 0.89 & 1.14 & 1.67 & 2.06 \\
      \midrule
      3 & Training Set & 0.68 & 0.70 & 0.93 &  1.11 & 1.54 \\
      3 & Evaluation Set & 0.71 & 0.77 & 0.98 & 1.13 & 1.77 \\
      \midrule
      6 & Training Set & 0.67 & 0.73 & 0.86 & 1.02 & 1.32 \\
      6 & Evaluation Set & 0.73 & 0.86 & 0.95 & 1.37 & 1.83 \\
      \bottomrule
      \end{tabular}
      \caption{Reconstruction Quality (Mel Distance $\downarrow$) of GMM-VAE and GMM-LM (with teacher forcing) across different number of Gaussians and $\lambda$ values.}
      \label{tab:lambda-effect-training}
      \end{table}   

      \begin{table}[h!]
         \centering
         \begin{tabular}{lcccc}
             \toprule
             \textbf{Model} & \textbf{Output} & \textbf{Codec Model} & \textbf{Cross Att.} & \textbf{Mono. Align.} \\
             \midrule
             Discrete AR & Logits Group          & VQ-VAE   & 8.02 & 5.35 \\
             Discrete AR with Delay Pred. & Multiple Logits Groups & DAC      & 7.83 & 5.87 \\
             GMM-LM & GMM Parameters         & GMM-VAE  & 6.60 & 2.72 \\
             \bottomrule
         \end{tabular}
         \caption{WER (\%) of the models using different codec models, with and without the proposed monotonic alignment on LibriSpeech-test-clean.}
         \label{tab:model_comparison_mono}
     \end{table}
\subsection{GMM-VAE Model Details}   
\revised{
The GMM-VAE architecture is inspired by the DAC model \cite{kumar2024high} and comprises a convolutional encoder and decoder. Both the encoder and decoder feature a convolutional layer, which performs upsampling or downsampling based on the stride, followed by a residual block. Each residual block combines convolutional layers with non-linear Snake activations. The encoder progressively downsamples the input audio waveform with strides of [2, 4, 8, 8], while the decoder mirrors this by upsampling at corresponding rates of [8, 8, 4, 2]. The dimensionality of the encoder and decoder is configured to 64 and 1536, respectively. The GMM-VAE contains a total of 76.5M parameters, distributed as 22.4M in the encoder and 54.1M in the decoder.}
\subsection{Comparison of GMM-LM and VQ-based LM with Monotonic Alignment}
\revised{As shown in Table~\ref{table:alignment_methods_comparison}, the proposed monotonic alignment significantly improves the WER of the GMM-LM model. This raises a natural question: can discrete AR models benefit from the proposed monotonic alignment in a similar way? 
To investigate this, we conducted a comparison using discrete autoregressive (AR) models with the proposed monotonic alignment method, implemented using the same architecture as the GMM-LM large version (315m), except for the discrete codec embedding layer and softmax layers.
Specifically, we implemented two discrete AR encoder-decoder models:}
\revised{
\begin{itemize}
   \item {\bf Discrete AR}. The first model used discrete codes extracted from a VQ-VAE codec model, implemented with a single codebook containing 8192 entries and using the same architecture as the DAC model \citep{kumar2024high}. This model was trained to predict the next tokens using standard cross-entropy with a single softmax layer.
   \item {\bf Discrete AR with Delay Pred}. The second model used discrete codes extracted from the DAC model with 8 codebooks, each containing 1024 entries. The model adopted delayed codebook prediction with multiple softmax layers, as proposed in MusicGen \citep{copet2024simple} and \citep{lyth2024natural}.
\end{itemize}}
\revised{
The results of these experiments are presented in Table 10 of the revised manuscript and summarized below. The results clearly show that monotonic alignment contributes to WER  improvement. However, even with monotonic alignment, the discrete models do not achieve the same performance as the proposed GMM-based version. We believe this performance gap can be attributed to two factors:
\begin{enumerate}
   \item Quantization and discrete representations introduce limitations: Discrete representations can result in mispronunciations and artifacts. This is why some researchers use embeddings before the AR model's softmax layer as input to waveform decoders as a workaround \citep{casanova2024xtts,betker2023better}.
   \item Challenges of applying monotonic alignment to RVQ-based models. RVQ models increase the complexity of TTS systems, as they require multiple softmax heads to predict codes from different codebooks. While it is possible to predict all codebooks in parallel at each timestep, this approach ignores dependencies between codebooks and yields suboptimal results. A better approach, as used in VALL-E and the delayed prediction model in Table~\ref{tab:model_comparison_mono}, is to predict "coarse" codes first, followed by "fine" codes. However, this approach complicates monotonic alignment because alignment must be performed using only the coarse codes at each timestep. This limitation likely contributes to the higher WER observed with RVQ-based models.
\end{enumerate}}
\end{document}
