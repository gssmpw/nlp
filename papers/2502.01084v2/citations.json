[
  {
    "index": 0,
    "papers": [
      {
        "key": "chorowski2019unsupervised",
        "author": "Chorowski, Jan and Weiss, Ron J and Bengio, Samy and Van Den Oord, A{\\\"a}ron",
        "title": "Unsupervised speech representation learning using wavenet autoencoders"
      },
      {
        "key": "qian2019autovc",
        "author": "Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and Hasegawa-Johnson, Mark",
        "title": "Autovc: Zero-shot voice style transfer with only autoencoder loss"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "ravanelli2020multi",
        "author": "Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua",
        "title": "Multi-task self-supervised learning for robust speech recognition"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "wav2vec2",
        "author": "Alexei Baevski and\nYuhao Zhou and\nAbdelrahman Mohamed and\nMichael Auli",
        "title": "wav2vec 2.0: {A} Framework for Self-Supervised Learning of Speech\nRepresentations"
      },
      {
        "key": "hubert",
        "author": "Wei{-}Ning Hsu and\nBenjamin Bolte and\nYao{-}Hung Hubert Tsai and\nKushal Lakhotia and\nRuslan Salakhutdinov and\nAbdelrahman Mohamed",
        "title": "HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction\nof Hidden Units"
      },
      {
        "key": "wavlm",
        "author": "Sanyuan Chen and\nChengyi Wang and\nZhengyang Chen and\nYu Wu and\nShujie Liu and\nZhuo Chen and\nJinyu Li and\nNaoyuki Kanda and\nTakuya Yoshioka and\nXiong Xiao and\nJian Wu and\nLong Zhou and\nShuo Ren and\nYanmin Qian and\nYao Qian and\nJian Wu and\nMichael Zeng and\nXiangzhan Yu and\nFuru Wei",
        "title": "{WavLM}: Large-Scale Self-Supervised Pre-Training for Full Stack Speech\nProcessing"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "choi2024self",
        "author": "Choi, Kwanghee and Pasad, Ankita and Nakamura, Tomohiko and Fukayama, Satoru and Livescu, Karen and Watanabe, Shinji",
        "title": "Self-Supervised Speech Representations are More Phonetic than Semantic"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "polyak2021speech",
        "author": "Polyak, Adam and Adi, Yossi and Copet, Jade and Kharitonov, Eugene and Lakhotia, Kushal and Hsu, Wei-Ning and Mohamed, Abdelrahman and Dupoux, Emmanuel",
        "title": "Speech resynthesis from discrete disentangled self-supervised representations"
      },
      {
        "key": "controlvc",
        "author": "Chen, Meiying and Duan, Zhiyao",
        "title": "ControlVC: Zero-Shot Voice Conversion with Time-Varying Controls on Pitch and Rhythm"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "vqvae",
        "author": "Van Den Oord, Aaron and Vinyals, Oriol and others",
        "title": "Neural Discrete Representation Learning"
      },
      {
        "key": "esser2021taming",
        "author": "Esser, Patrick and Rombach, Robin and Ommer, Bjorn",
        "title": "Taming transformers for high-resolution image synthesis"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zeghidour2021soundstream",
        "author": "Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco",
        "title": "Soundstream: An end-to-end neural audio codec"
      },
      {
        "key": "defossez2022high",
        "author": "D{\\'e}fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi",
        "title": "High fidelity neural audio compression"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "devlin2018bert",
        "author": "Devlin, Jacob",
        "title": "Bert: Pre-training of deep bidirectional transformers for language understanding"
      },
      {
        "key": "brown2020language",
        "author": "Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "casanova2024xtts",
        "author": "Casanova, Edresson and Davis, Kelly and G{\\\"o}lge, Eren and G{\\\"o}knar, G{\\\"o}rkem and Gulea, Iulian and Hart, Logan and Aljafari, Aya and Meyer, Joshua and Morais, Reuben and Olayemi, Samuel and others",
        "title": "XTTS: a Massively Multilingual Zero-Shot Text-to-Speech Model"
      },
      {
        "key": "betker2023better",
        "author": "Betker, James",
        "title": "Better speech synthesis through scaling"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zeghidour2021soundstream",
        "author": "Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco",
        "title": "Soundstream: An end-to-end neural audio codec"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "zeghidour2021soundstream",
        "author": "Zeghidour, Neil and Luebs, Alejandro and Omran, Ahmed and Skoglund, Jan and Tagliasacchi, Marco",
        "title": "Soundstream: An end-to-end neural audio codec"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "defossez2022high",
        "author": "D{\\'e}fossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi",
        "title": "High fidelity neural audio compression"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kumar2024high",
        "author": "Kumar, Rithesh and Seetharaman, Prem and Luebs, Alejandro and Kumar, Ishaan and Kumar, Kundan",
        "title": "High-fidelity audio compression with improved rvqgan"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "ju2024naturalspeech",
        "author": "Ju, Zeqian and Wang, Yuancheng and Shen, Kai and Tan, Xu and Xin, Detai and Yang, Dongchao and Liu, Yanqing and Leng, Yichong and Song, Kaitao and Tang, Siliang and others",
        "title": "Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "tacotron",
        "author": "Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others",
        "title": "Natural {TTS} synthesis by conditioning {WaveNet} on mel spectrogram predictions"
      },
      {
        "key": "wang2017tacotron",
        "author": "Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and others",
        "title": "Tacotron: Towards end-to-end speech synthesis"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "copet2024simple",
        "author": "Copet, Jade and Kreuk, Felix and Gat, Itai and Remez, Tal and Kant, David and Synnaeve, Gabriel and Adi, Yossi and D{\\'e}fossez, Alexandre",
        "title": "Simple and controllable music generation"
      },
      {
        "key": "valle",
        "author": "Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others",
        "title": "Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers, 2023"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "borsos2023audiolm",
        "author": "Borsos, Zal{\\'a}n and Marinier, Rapha{\\\"e}l and Vincent, Damien and Kharitonov, Eugene and Pietquin, Olivier and Sharifi, Matt and Roblek, Dominik and Teboul, Olivier and Grangier, David and Tagliasacchi, Marco and others",
        "title": "Audiolm: a language modeling approach to audio generation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "borsos2023soundstorm",
        "author": "Borsos, Zal{\\'a}n and Sharifi, Matt and Vincent, Damien and Kharitonov, Eugene and Zeghidour, Neil and Tagliasacchi, Marco",
        "title": "SoundStorm: Efficient Parallel Audio Generation"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "valle",
        "author": "Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others",
        "title": "Neural Codec Language Models Are Zero-Shot Text to Speech Synthesizers, 2023"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "hifigan",
        "author": "Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung",
        "title": "Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis"
      },
      {
        "key": "vits",
        "author": "Kim, Jaehyeon and Kong, Jungil and Son, Juhee",
        "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech"
      },
      {
        "key": "lim2022jets",
        "author": "Lim, Dan and Jung, Sunghee and Kim, Eesung",
        "title": "JETS: Jointly training FastSpeech2 and HiFi-GAN for end to end text to speech"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "jeong2021diff",
        "author": "Jeong, Myeonghun and Kim, Hyeongju and Cheon, Sung Jun and Choi, Byoung Jin and Kim, Nam Soo",
        "title": "Diff-tts: A denoising diffusion model for text-to-speech"
      },
      {
        "key": "popov2021grad",
        "author": "Popov, Vadim and Vovk, Ivan and Gogoryan, Vladimir and Sadekova, Tasnima and Kudinov, Mikhail",
        "title": "Grad-tts: A diffusion probabilistic model for text-to-speech"
      },
      {
        "key": "huang2022prodiff",
        "author": "Huang, Rongjie and Zhao, Zhou and Liu, Huadai and Liu, Jinglin and Cui, Chenye and Ren, Yi",
        "title": "Prodiff: Progressive fast diffusion model for high-quality text-to-speech"
      },
      {
        "key": "liu2022diffgan",
        "author": "Liu, Songxiang and Su, Dan and Yu, Dong",
        "title": "Diffgan-tts: High-fidelity and efficient text-to-speech with denoising diffusion gans"
      },
      {
        "key": "li2024styletts",
        "author": "Li, Yinghao Aaron and Han, Cong and Raghavan, Vinay and Mischler, Gavin and Mesgarani, Nima",
        "title": "Styletts 2: Towards human-level text-to-speech through style diffusion and adversarial training with large speech language models"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "voicebox",
        "author": "Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others",
        "title": "Voicebox: Text-guided multilingual universal speech generation at scale"
      },
      {
        "key": "mehta2024matcha",
        "author": "Mehta, Shivam and Tu, Ruibo and Beskow, Jonas and Sz{\\'e}kely, {\\'E}va and Henter, Gustav Eje",
        "title": "Matcha-TTS: A fast TTS architecture with conditional flow matching"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "fastspeech",
        "author": "Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan",
        "title": "Fastspeech: Fast, robust and controllable text to speech"
      },
      {
        "key": "fastspeech2",
        "author": "Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan",
        "title": "Fastspeech 2: Fast and high-quality end-to-end text to speech"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "mehta2024should",
        "author": "Mehta, Shivam and Lameris, Harm and Punmiya, Rajiv and Beskow, Jonas and Sz{\\'e}kely, {\\'E}va and Henter, Gustav Eje",
        "title": "Should you use a probabilistic duration model in TTS? Probably! Especially for spontaneous speech"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "vits",
        "author": "Kim, Jaehyeon and Kong, Jungil and Son, Juhee",
        "title": "Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "voicebox",
        "author": "Le, Matthew and Vyas, Apoorv and Shi, Bowen and Karrer, Brian and Sari, Leda and Moritz, Rashel and Williamson, Mary and Manohar, Vimal and Adi, Yossi and Mahadeokar, Jay and others",
        "title": "Voicebox: Text-guided multilingual universal speech generation at scale"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "dilokthanakul2016deep",
        "author": "Dilokthanakul, Nat and Mediano, Pedro AM and Garnelo, Marta and Lee, Matthew CH and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray",
        "title": "Deep unsupervised clustering with gaussian mixture variational autoencoders"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "tomczak2018vae",
        "author": "Tomczak, Jakub and Welling, Max",
        "title": "VAE with a VampPrior"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "makhzani2015adversarial",
        "author": "Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan",
        "title": "Adversarial autoencoders"
      }
    ]
  }
]