\section{Experiment}
We conduct experiments to demonstrate the effectiveness of the proposed FairDual.

%The codes and data are released on Supplementary Material.




\subsection{Experimental settings}\label{sec:exp_settings}

\textbf{Datasets}. The experiments are conducted on the commonly used two widely used and publicly available recommendation datasets, including MIND~\citep{wu2020mind}\footnote{\url{https://microsoftnews.msn.com}},
Amazon-Book and Amazon-Electronic~\citep{he2016ups}\footnote{\url{http://jmcauley.ucsd.edu/data/amazon/}}. Their detailed statistical information is in Appendix~\ref{app:exp_settings}.


\textbf{Evaluation}. We arrange all interactions in the dataset chronologically by their timestamps and employ the first 80\% interactions as training data. The remaining 20\% of interactions are divided equally, with each 10\% segment used for validation and testing, respectively, during evaluation.


Regarding the metric, following the practice in~\cite{dai23Uncover}, we utilize Normalized Discounted Cumulative Gain (NDCG) and mean Reciprocal Rank (MRR) to measure the accuracy:
$
    \text{NDCG@K} = \frac{1}{|\mathcal{U}|}\sum_{u=1}^{|\mathcal{U}|}\frac{\sum_{i\in L_K(u)} (2^{c_{u,i}}-1)/(\log_2(j+1))}{(2^{\text{rank}_i}-1)/(\log_2(\text{rank}_i+1))}, \quad \text{MRR@K} = \frac{1}{|\mathcal{U}|}\sum_{u=1}^{|\mathcal{U}|}\frac{1}{\text{rank}_i},
$
where $\text{rank}_i$ is the rank of the first correct answer. Meanwhile, we employ MMF@K to gauge the degree of fairness, which quantifies the aggregated ranking score of the 20\% worst-off groups~\citep{nips21welf, xu2023p}.

\textbf{Backbones and baselines}. For the backbone, we first select three large-scale recommender models: \textbf{NRMS}~\citep{wu-etal-2019-neural-news},  \textbf{RecFormer}~\citep{Recformer} and \textbf{BigRec}~\citep{bao2023bi}. Note that BigRec only utilizes 1024 samples to train due to large computational cost. 



For the baselines, we choose several fair-aware re-weight baselines that aim to improve group MMF: \textbf{UNI} (without considering fairness), \textbf{DRO}~\citep{hashimoto2018fairness}, \textbf{S-DRO}~\citep{wen2022distributionally}, \textbf{Prop}~\citep{hu2023adaptive}, \textbf{IFairLRS}~\citep{jiang2024itemside} and \textbf{Maxmin Sample}~\citep{abernethy2022active}. 



The detailed descriptions of the backbones and baselines are in Appendix~\ref{app:exp_settings}.

\textbf{Implemtation details.} We provide our detailed running environment, all hyper-parameter settings, utilized LLMs settings, and used the toolkit in Appendix~\ref{app:exp_settings}. 


%We also include all the baseline and dataset processing details in the Fairness-aware RS benchmark


%As for the hyper-parameters, the learning rate $\eta\in [1e^{-2},1e^{-4}]$, $\gamma\in [128, 1280]$ and trade-off factor $\lambda\in [0, 5]$. We set the $m_g$ as the group size $m_g=|\mathcal{I}_g|$. We implement FairDual with the cvxpy~\citep{cvxpy} for optimization. The gradient descent package used Pytorch~\citep{pytorch} to apply the auto-gradient. The experiments were conducted under a server with a single NVIDIA A5000.





% Please add the following required packages to your document preamble:
% \usepackage{booktabs}



\begin{table*}[t]
\caption{Performance comparisons between ours and the baselines on three datasets under best-performing BigRec backbones. The $*$ means the improvements are statistically significant (t-tests and $p$-value $< 0.05$). The bold number indicates that the accuracy value exceeds that of all the baselines.} \label{exp:main}
\centering
\resizebox{0.99\linewidth}{!}{
    % \renewcommand\arraystretch{1.4}
    \centering
    %\setlength{\tabcolsep}{8mm}
\begin{tabular}{@{}clrrrrrrrrr@{}}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Models/Metrics}}} & \multicolumn{3}{c}{$K=5$} & \multicolumn{3}{c}{$K=10$} & \multicolumn{3}{c}{$K=20$} \\ \cmidrule(l){3-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
\multicolumn{2}{c}{} & NDCG (\%) & MRR (\%) & MMF (\%) & NDCG (\%) & MRR (\%) & MMF (\%) & NDCG (\%) & MRR (\%) & MMF (\%) \\ \midrule
\multirow{8}{*}{\textbf{MIND}} & UNI & 1.02 & 0.79 & 1.63 & 1.50 & 0.98 & 2.33 & 2.16 & 1.16 & 2.94 \\ 
 & DRO & 0.90 & 0.67 & 1.81 & 1.37 & 0.87 & 2.51 & 1.94 & 1.02 & 3.21 \\ 
 & Prop & 1.11 & 0.88 & 1.97 & 1.62 & 1.09 & 2.53 & 2.14 & 1.23 & 3.05 \\ 
 & S-DRO & 0.91 & 0.70 & 1.87 & 1.42 & 0.91 & 2.41 & 1.93 & 1.04 & 3.02 \\ 
 & IFairLRS & 0.87 & 0.66 & 2.21 & 1.27 & 0.83 & 2.91 & 1.78 & 0.97 & 2.86 \\  
 & Maxmin sample & 0.98 & 0.75 & 2.25 & 1.49 & 0.96 & 1.71 & 2.19 & 1.15 & 3.13 \\
 \cmidrule(l){2-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
 & \textbf{Ours} & \textbf{1.15}$^*$ & \textbf{0.88} & \textbf{2.82}$^*$ & \textbf{1.69}$^*$ & \textbf{1.11} & \textbf{2.99}$^*$ & \textbf{2.28}$^*$ & \textbf{1.27}$^*$ & \textbf{3.39}$^*$  \\ 
 & $\;\;$improv.(\%) & 3.60 & 0.00 & 25.33 & 4.32 & 1.83 & 2.75 & 4.10 & 3.25 & 5.61 \\
 \hline
 \multirow{8}{*}{\textbf{Book}} & UNI & 2.99 & 2.79 & 8.44 & 3.19 & 2.87 & 8.32 & 3.44 & 2.94 & 8.15 \\ 
 & DRO & 2.94 & 2.72 & 8.39 & 3.15 & 2.81 & 8.29 & 3.37 & 2.87 & 8.10 \\ 
 & Prop & 2.64 & 2.45 & 8.68 & 2.83 & 2.53 & 8.30 & 3.05 & 2.59 & 8.01 \\ 
 & S-DRO & 2.61 & 2.44 & 8.37 & 2.80 & 2.52 & 8.21 & 3.06 & 2.59 & 8.07 \\ 
 & IFairLRS & 2.30 & 2.16 & 8.46 & 2.51 & 2.25 & 8.20 & 2.76 & 2.32 & 8.17 \\  
 & Maxmin sample & 2.49 & 2.31 & 6.80 & 2.72 & 2.43 & 6.80 & 2.97 & 2.74 & 7.50 \\
 \cmidrule(l){2-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
 & \textbf{Ours} & \textbf{3.11}$^*$ & \textbf{2.88} & \textbf{8.90}$^*$ & \textbf{3.31}$^*$ & \textbf{2.96} & \textbf{9.00}$^*$  & \textbf{3.60}$^*$ & \textbf{3.04} & \textbf{8.89}$^*$  \\ 
 & $\;\;$improv.(\%) & 4.01 & 3.23 & 2.53 & 3.76 & 3.14 & 8.17 & 4.65 & 3.40 & 8.81 \\
 \hline
  \multirow{8}{*}{\textbf{Electronic}} & UNI & 4.61& 4.30& 0.26& 4.93& 4.43& 0.25& 5.30 & 4.53 & 0.21 \\ 
 & DRO & 4.65 & 4.34& 0.24& 4.96& 4.46& 0.24& 5.33& 4.57& 0.21 \\ 
 & Prop & 4.63& 4.33& 0.26& 4.96& 4.47& 0.25& 5.33& 4.57& 0.21 \\ 
 & S-DRO &  4.60 & 4.29& 0.25& 4.92& 4.42& 0.24& 5.29& 4.52& 0.20 \\ 
 & IFairLRS & 2.21& 2.06& 0.19& 2.46& 2.16& 0.17& 2.69& 2.22& 0.12 \\  
 & Maxmin sample & 4.60& 4.31& 0.27& 4.92& 4.44& 0.25& 5.31& 4.55& 0.21\\
 \cmidrule(l){2-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
 & \textbf{Ours} & \textbf{5.08}$^*$ & \textbf{4.78} & \textbf{0.31}$^*$ & \textbf{5.43}$^*$ & \textbf{4.92} & \textbf{0.30}$^*$  & \textbf{5.84}$^*$ & \textbf{5.03} & \textbf{0.26}$^*$  \\ 
 & $\;\;$improv.(\%) & 9.24 & 10.1 & 14.8 & 9.47 & 10.0 & 19.9 & 0.95 & 10.0 & 23.8\\
 \bottomrule
 
\end{tabular}

}
\end{table*}


\begin{table*}[t]
\caption{Performance comparisons between ours under other backbones on MIND dataset. The $*$ means the improvements are statistically significant (t-tests and $p$-value $< 0.05$). The bold number indicates that the accuracy value exceeds that of all the baselines.}\label{exp:backbones}
\centering
\resizebox{0.99\linewidth}{!}{
     %   \renewcommand\arraystretch{1.1}
    \centering
    %\setlength{\tabcolsep}{5mm}
\begin{tabular}{@{}clrrrrrrrrr@{}}
\toprule
\multicolumn{2}{c}{\multirow{2}{*}{\textbf{Models/Metrics}}} & \multicolumn{3}{c}{top-5} & \multicolumn{3}{c}{top-10} & \multicolumn{3}{c}{top-20} \\ \cmidrule(l){3-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
\multicolumn{2}{c}{} & NDCG (\%) & MRR (\%) & MMF (\%) & NDCG (\%) & MRR (\%) & MMF (\%) & NDCG (\%) & MRR (\%) & MMF (\%) \\ \midrule
\multirow{8}{*}{\textbf{NRMS}} & DRO & 0.44 & 0.32 & 0.12 & 0.66 & 0.42 & 3.60 & 1.06 & 0.50 & 9.94 \\ 
 & Prop & 0.44 & 0.32 & 0.12 & 0.66 & 0.42 & 3.49 & 1.06 & 0.52 & 9.94 \\ 
 & S-DRO & 0.52 & 0.34 & 0.10 & 0.76 & 0.40 & 2.05 & 1.20 & 0.52 & 8.74 \\ 
 & IFairLRS & 0.40 & 0.28 & 0.69 & 0.62 & 0.36 & 4.20 & 0.96 & 0.44 & 10.58 \\ 
 & Maxmin sample & 0.38  & 0.31  & 0.20  & 0.45  & 0.34  & 4.00  & 0.67  & 0.422  & 9.99 \\
 \cmidrule(l){2-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
 & \textbf{Ours} & \textbf{0.60}$^*$ & \textbf{0.40}$^*$ & \textbf{1.07}$^*$ & \textbf{0.84}$^*$ & \textbf{0.46}$^*$ & \textbf{4.93}$^*$ & \textbf{1.28}$^*$ & \textbf{0.60}$^*$ & \textbf{11.35}$^*$ \\ 
 \bottomrule
\multirow{8}{*}{\textbf{RecFormer}} & DRO & 0.57 & 0.45 & 1.08 & 0.89 & 0.59 & 1.08 & 1.41 & 0.73 & 1.52 \\ 
 & Prop & 0.57 & 0.45 & 1.08 & 0.89 & 0.58 & 1.08 & 1.41 & 0.72 & 1.52 \\ 
 & S-DRO & 0.57 & 0.45 & 1.20 & 0.91 & 0.60 & 1.15 & 1.46 & 0.73 & 1.62 \\ 
 & IFairLRS & 0.46 & 0.37 & 1.68 & 0.76 & 0.49 & 1.70 & 1.29 & 0.63 & 2.12 \\ 
 & Maxmin sample & 0.51 & 0.41 & 0.94 & 0.85 & 0.55 & 1.50 & 1.37 & 0.69 & 2.48 \\
 \cmidrule(l){2-5} \cmidrule(l){6-8} \cmidrule(l){9-11}
 & \textbf{Ours} & \textbf{0.59}$^*$ & \textbf{0.45} & \textbf{1.88}$^*$ & \textbf{0.99}$^*$ & \textbf{0.60} & \textbf{1.94}$^*$ & \textbf{1.55}$^*$ & \textbf{0.75} & \textbf{2.58}$^*$  \\
  \bottomrule
\end{tabular}
}
\end{table*}



\subsection{Experimental Results on Full Datasets}\label{sec:main_exp}
Firstly, we conduct experiments to show the performance of FairDual and other baselines across all large-scale recommendation backbones.
Table~\ref{exp:main} presents the experimental outcomes for our FairDual model and the baseline methods across all datasets, respectively. Table~\ref{exp:backbones} presents the experimental outcomes for our FairDual model and the baseline methods across other different backbones on the MIND dataset. 
To make fair comparisons, all the baselines were tuned to their hyperparameters to obtain the best trade-off accuracy and fairness performance under our settings.


From the experiments, it is evident that FairDual consistently outperforms the baseline methods across all datasets and various base models, spanning different top-K ranking sizes. This is reflected in accuracy metrics such as NDCG and MRR, as well as the fairness metric MMF. The results conclusively demonstrate that FairDual effectively ensures the model reaches a better convergence point in terms of both accuracy and fairness by leveraging dual gradient descent.

We also conduct experiments on traditional RS backbones, as detailed in Appendix~\ref{app:sec:non_llms}. The results also consistently confirm the effectiveness of our model FairDual.


\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/analysis.pdf}
    \caption{Sub-figure (a) conducts a simulation to show Jensen gap $J(B)$ changes \wrt batch size $B$ and group size $|\mathcal{G}|$ for all baselines and FairDual. Sub-figure(b,c) conducts on MIND dataset under BigRec. Sub-figure (b) describes the NDCG and MMF changes \wrt accuracy-fairness trade-off co-efficient $\lambda$. Sub-figure (c) conducts the case study on the advantage group News and worst-off group Sports. We show their weight $\bm{s}_g$, group score $\bm{w}_g$, and t-SNE embeddings of UNI and FairDual. }
    \label{fig:analysis}
    \vspace{-0.3cm}
\end{figure}

\subsection{Experimental Analysis}
In this section, we first replicate the simulation settings outlined in Section~\ref{sec:emp_analysis} to investigate how the Jensen gap changes. Then we conduct analysis on MIND dataset under BigRec base models.

\textbf{Jensen gap.} Firstly, we investigate the variations in the Jensen gap concerning batch size $B$ and group size $|\mathcal{G}|$ across both baseline methods and our proposed FairDual model. As shown in Figure~\ref{fig:analysis} (a), we can see that FairDual has a lower Jensen gap than other online models across different batch sizes and group sizes. Furthermore, it's evident that the Jensen gap exhibited by FairDual remains consistently stable across various batch sizes, with only a marginal increase observed as the group size expands. This indicates that FairDual can consistently maintain a low Jensen gap level.


\textbf{Influence on co-efficient $\lambda$.} Then, we will investigate the impacts of trade-off co-efficient $\lambda$. Figure~\ref{fig:analysis} (b) illustrates that the fairness degree (MMF) increases proportionally with the rise in $\lambda$, aligning with our expectations. However, we also observe that the accuracy increases as $\lambda$ changes from $0$ to $5$ and then decreases. This phenomenon occurs due to the presence of popularity bias in recommendation datasets~\citep{jiang2024itemside}. A relatively higher fairness degree helps mitigate this bias, leading to increased accuracy. However, when $\lambda$ becomes too large, it inevitably enlarges the Jensen gap, which hurts the model's performance. We also conduct an experiment to analyze the effect of the popularity bias in Appendix~\ref{app:pop_bias}.


\textbf{Case study.} Finally, we conduct a case study on the head group \textit{News}, which consistently exhibited superior exposure compared to other groups, in contrast to the tail group \textit{Sports}, which typically had lower exposure levels. Firstly, from the two figures at the top of Figure~\ref{fig:analysis} (c), we observe that as the training progresses, the tail group Sports gradually gains more weight ($\bm{s}_g$), while the head group News consistently receives relatively low weight. Consequently, this leads to the group scores $\bm{w}$ of the two groups being close to each other. 

At the same time, we visualize the item embeddings using T-SNE~\citep{van2008visualizing} for both the baseline UNI and our model FairDual, as shown in the bottom two sub-figures of Figure~\ref{fig:analysis} (c). From the figure, we compute the embedding KL divergence of two different groups between UNI (0.113) and our method FairDual (0.083). This shows that UNI establishes a clear classification line to distinguish between different groups. However, FairDual tends to bring the embeddings of the tail group closer to those of the head group, ultimately increasing the fairness.

\textbf{Other experimental analysis.} For analysis of other parameters dual learning rate $\eta$, updating gap $\beta$, user history length $H$, the sample size $Q$, and the impact of the hidden layer numbers, please see the Appendix~\ref{app:para_analysis}. For the computational and storage costs analysis can be seen in Appendix~\ref{app:computational}. We also test the performance of other fairness metric in Appendix~\ref{app:GINI}.


%\textbf{Convergence speed.} In Table~\ref{tab:convergence}, we measured the convergence time (performance stabilizing within 50 steps) of our method compared to other baselines under BigRec backbones on MIND dataset. From the results, we can observe that our method shows the fastest convergence time compared to other fair-aware RS baselines nearly 30\%, which is very important for industrial applications. These results will also confirm our good convergence ability (Theorem~\ref{theo:Jensen_Gap}) compared to other baselines.