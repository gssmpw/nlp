\section{Problem Analysis}\label{sec:analsysis}
In real-world scenarios, the number of users $|\mathcal{U}|$ is often large, and a mini-batch sampling strategy with a batch size of B is often necessary due to the large computational costs. Each batch only contains a subset of users. However, we show that the non-linear additivity property of the MMF-based objective will introduce the Jensen gap between the modelâ€™s convergence point and the optimal point when employing mini-batch sampling strategies.

In this section, we analyze why the Jensen gap exists and how it will influence the model's convergence in both theoretical and empirical ways.


\subsection{Theoretical Analysis}

Firstly, we will re-write the optimization objective using the following theorem:
\begin{theorem}\label{theo:alpha_fair}
    For a vector $\bm{x}\in\mathbb{R}^n$, $\bm{x}^i$ denotes the element of the vector raised to the power of $i$. Similarly, $\log(\bm{x})$ denotes the element of the vector reduced as $\log(\bm{x}_i)$. 
    Let $\bm{A}\in\mathbb{R}^{|\mathcal{I}|\times|\mathcal{G}|}$ is the item-group adjacent matrix, and $\bm{A}_{ig} = 1$ indicates item $i\in \mathcal{I}_g$, and 0 otherwise. Let $\bm{w}\in\mathbb{R}^{|\mathcal{I}|} = [-\sum_{u\in\mathcal{U}}c_{u,i}\log(\hat{c}_{u,i})]_{i\in\mathcal{I}}$ and its feasible region is $\mathcal{W}=\{\bm{w}|\sum_{i\in\mathcal{I}} c_{u,i} \leq K, \forall u\in\mathcal{U}, c_{u,i}\in [0,1]\}$. Then there exist $t \in [0, \infty)$ (value of $t$ relates to the value of $M$) and a weight vector $\bm{b}\in\mathbb{R}^{|\mathcal{G}|}\ge 0$, s.t. Equation~(\ref{eq:obj}) can be optimized as:
    \begin{equation}\label{eq:t_fair}
     %     \mathcal{L} = \min    \begin{cases} 
     %   \bm{1}^{\top}(\hat{\bm{A}}^{\top}\bm{w})^{1-t} & \text{if } t \ge 0, t \neq 1 \\
     %   \bm{1}^{\top}\log(\hat{\bm{A}}^{\top}\bm{w}) & \text{if } t=1, \\
     % \end{cases}\\
      \mathcal{L} = \min_{w\in\mathcal{W}}   
       \bm{b}^{\top}(\hat{\bm{A}}^{\top}\bm{w})^{1+t}
    \end{equation}
    where $\hat{\bm{A}}$ is the row-normalized matrix for $\bm{A}$: $\hat{\bm{A}}=\text{diag}(\bm{A}\bm{1})^{-1}\bm{A}$. $\text{diag}(\bm{x})$ denotes to construct a diagonal matrix based on vector $\bm{x}$.
\end{theorem}


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{img/simulation.pdf}
    \caption{Loss converges simulation with 1000 users and 1000 items. Sub-figure (a) and (b) illustrate the distance away from the optimal point (\ie Jensen gap) \wrt mini-batch and group size, respectively. Figure (a) was conducted with the same group size (G=7) under different batch sizes, while Figure (b) was conducted with the same batch size (B=32) under different group sizes. Sub-figure (c) describes the converged trace under different batch sizes.}
    \label{fig:intro}
    %\vspace{-0.5cm}
\end{figure}

The detailed proof of Theorem~\ref{theo:alpha_fair} can be seen in Appendix~\ref{app:prof_alpha_fair}. When transforming the original optimization process in Equation~(\ref{eq:obj}) into Equation~(\ref{eq:t_fair}),  we can easily observe that the loss function does not adhere to linear additivity. Then the
Jensen gap will arise under mini-batch sample strategy by formulating it using the following theorem:
%, ultimately preventing convergence to the global optimal point. Formally, we can

\begin{theorem}\label{theo:error}
    Under mini-batch sample strategies, we partition the user set $\mathcal{U}$ into $|\mathcal{U}|/B$ subsets and perform optimization on each subset. Let $\bm{e}^j\in\mathbb{R}^{|\mathcal{G}|}$ be the group accumulated utility under $j$-th partition, where each element 
    $\bm{e}_{j,g} = -\sum_{u\in\mathcal{U}_j}\sum_{i\in\mathcal{I}_g}c_{u,i}\log (\hat{c}_{u,i})$.
    Let $f(\bm{x})=x^{t+1}$. We can write the mini-batch optimizing loss objective $\mathcal{L}^B$ as:
        $
            \mathcal{L}^B = \min \sum_{j=1}^{|\mathcal{U}|/B} \bm{b}^{\top}f(\bm{e}_j),
        $
        where $\mathcal{U}_j$ is the $j$-th partition of the user set $\mathcal{U}$. Then, Jensen gap~\citep{gao2017bounds, ullah2021determination} is defined as:
        \begin{equation}\label{eq:Jensen_gap}
            J(B) = |\mathcal{L}^B - \mathcal{L}|=|\mathcal{L}^B - \min \bm{b}^{\top}f(\sum_{j=1}^{|\mathcal{U}|/B} \bm{e}_j)| \neq 0.
        \end{equation}

   
   When optimizing Equation~(\ref{eq:t_fair}) under the mini-batch sampling style, the mini-batch size $B$ becomes smaller and group size $\mathcal{G}$ becomes larger, the  Jensen gap $J(B)$ will become larger. Moreover, when the mini-batch size becomes smaller, we are more likely to underestimate the original loss, \ie $\mathcal{L}^B \leq \mathcal{L}$. The loss underestimation will result in the Jensen gap.
   
   %when $t>1$, we are more likely to overestimate the original loss function with the mini-batch size becoming smaller, \ie $\mathcal{L}^B \ge \mathcal{L}$.
\end{theorem}

% when the fairness degree is small $0 < t \leq 1$,  and when the fairness degree is high $t > 1$, the combination of different batches forms a convex function
The detailed proof of Theorem~\ref{theo:error} can be seen in Appendix~\ref{app:prof_error}. The intuitive reason behind the Jensen gap raised by group MMF is that the accuracy-fairness trade-off problem does not adhere to linear additive attributes. Essentially, the combination of different batches forms a concave function. Mini-batch size and group size both measure the degree of data partitioning, where smaller batch sizes and larger group sizes lead to fewer data partitions. As a result, due to the non-linear and intricate function form of a neural network~\citep{sun2019optimization}, these errors in estimating the loss function impede the model from converging to the optimal point, thus diminishing the performance. Next, we will give an empirical analysis to prove this.


\subsection{Empirical analysis}\label{sec:emp_analysis}
In this section,  we illustrate a simulation (Figure~\ref{fig:intro}) conducted under the assumption of knowing every user-item true preference score to validate the correctness of our theoretical analysis. We use the simple recommendation model: Matrix Factorization~\citep{singh2008unified} since we can have a closed-form expression on parameter updating. Then we apply a common mini-batch training strategy to optimize accuracy-fairness objective based on the parameters outlined in~\cite{xu2023p, fairrec}, with the accuracy-fairness coefficient of 0.5.

%across different mini-batch and group sizes.

As shown in Figure~\ref{fig:intro} (a) and (b), we uncover that the Jensen gap (distance away from the optimal point) will deviate with smaller mini-batch sizes and larger group sizes. Figure (c) describes the converge trace under different batch sizes by mapping the top-K simplex space of three groups of recommendation ranking to a 2-dimensional space through a topological homeomorphic transformation~\citep{kozlov2008combinatorial}.
Figure~\ref{fig:intro} (c) also indicates that different batch sizes result in different gradient optimization directions, with smaller batch sizes leading to larger shifts in the error of the optimization direction.
These empirical results confirm the correctness of our theoretical analysis.


For other types of fairness, such as the power-mean welfare family~\citep{cousins2021axiomatic} and the Gini welfare function~\citep{do2022optimizing}, also exhibit non-linear properties, leading to analogous Jensen gap phenomena. We discuss them in the Appendix~\ref{app:generalize} and Appendix~\ref{app:GINI}.

%Other fairness optimization objectives, such as the power-mean welfare family~\citep{cousins2021axiomatic} and the Gini welfare function~\citep{do2022optimizing}, also exhibit non-linear properties, leading to analogous Jensen gap phenomena. This paper mainly uses max-min fairness as an example to analyze and propose solutions. Our paper mainly calls on communities to pay attention to the bias caused by the Jensen gap when optimizing the objective constrained by fairness requirements. 
%\subsection{Discussion}


%This work will provide inspiration for the community of fairness and large model optimization.



\section{Method}
In this section, we will introduce our method FairDual.

%which effectively and efficiently bridges the Jensen gap on large-scale datasets within large recommender models (\eg large language models).

%there exist a $L>0$, 

\subsection{Optimizing Max-min Fairness as Group-weighted Objective}
In this section, in order to tackle this problem, we show that the MMF-constrained objective can be regarded as the group-weighted optimization problem using the following theorem:

\begin{theorem}\label{theo:reweight}
    By introducing the dual variable $\bm{\mu}$, the dual form of the Equation~(\ref{eq:obj}) is
    \begin{equation}\label{eq:reweight}
        \mathcal{L}' = \min_{\hat{c}_{u,i}} \quad -\sum_{u\in\mathcal{U}}\sum_{g\in\mathcal{G}}\bm{s}_g\sum_{i\in\mathcal{I}_g}c_{u,i}\log(\hat{c}_{u,i}),
    \end{equation}
    where $\bm{s}_g = 1-\bm{\mu}_g$ and 
    $
    \bm{\mu} =  \argmin_{\bm{\mu}\in\mathcal{M}} \left(\max \sum_{u\in\mathcal{U}}\sum_{g\in\mathcal{G}}\bm{s}_g\sum_{i\in\mathcal{I}_g}c_{u,i}\log(\hat{c}_{u,i}) + \lambda r^*(\bm{\mu})\right),
    $
    where $r^*(\mu) = \max_{\bm{w}_g\leq m_g} \left(\min_{g\in\mathcal{G}} m_g(\hat{\bm{A}}\bm{w})_g+\hat{\bm{A}}^{\top}\bm{w}\bm{\mu}/\lambda\right)=\sum_{g}m_g\bm{\mu}_g/\lambda+1$, $\mathcal{M}=\left\{\bm{\mu} ~\left|~ \sum_{g\in\mathcal{S}} \bm{\mu}_gm_g \ge -\lambda, \forall \mathcal{S}\in\mathcal{G}_s\right.\right\},$ where $\mathcal{G}_s$ is the set of all subsets of $\mathcal{G}$ (\ie power set).
\end{theorem}

The detailed proof of Theorem~\ref{theo:reweight} can be seen in Appendix~\ref{app:prof_reweight}. From Theorem~\ref{theo:reweight}, we observe that the recommendation task constrained by max-min fairness can be viewed as a re-weighting approach across different groups on the original loss function solely optimized for accuracy. 


% Furthermore, the fairness weight $\bm{\mu}_g$ can be likened to the shadow price in economics~\citep{dreze1990policy}, reflecting how changes in fairness constraints affect the objective function. Specifically, a high shadow price indicates that this constraint is the primary factor constraining accuracy optimization. Conversely, a low or zero shadow price suggests that the fairness constraint currently imposes little restriction on accuracy. Therefore, the problem will become how to choose the weight $s_g$ for each group $g$ to make the loss converge to the optimal point.

Intuitively, $s_g=1-\mu_g$ is the negative shadow price~\citep{dreze1990policy}. The high shadow price $\mu_g$ indicates that this constraint is the primary factor constraining accuracy optimization. Conversely, a low or zero shadow price suggests that the fairness constraint currently imposes little restriction on accuracy. Specifically, a high $s_g$ signifies that this constraint is the primary factor limiting fairness optimization for group $g$, whereas a low or zero $s_g$ implies that the accuracy constraint for group $g$ currently has little impact on the overall optimization.

%with decreasing it leading to the most substantial positive impact on the objective function

\begin{algorithm}[t]
    \caption{FairDual}
	\label{alg:fairdual}
	\begin{algorithmic}[1]
	\REQUIRE Dataset $\mathcal{D} = \{u,i,c_{u,i}\}$, item-group adjacent matrix $\bm{A}$, dual learning rate $\eta$, trade-off coefficient $\lambda$, $m^i_{\text{freeze}}(\cdot)$ updating step $\beta$, batch size $B$ and sample item number $Q$ and the weight $m_g$ for each group $g$. $\hat{\bm{A}}=\text{diag}(\bm{A}\bm{1})^{-1}\bm{A}$.
	\ENSURE The model parameters of $m^i(\cdot), m^u(\cdot)$.
        %\STATE Initialize model parameters of $m^i(\cdot), m^u(\cdot)$.
        % \STATE Copy parameters from $m^i(\cdot)$ to $m^i_{\text{freeze}}(\cdot)$ and get all item embedding $\bm{E}$
	\FOR{$n=1,\cdots,N$}
	       \STATE Set $\bm{\gamma}_{1,g}=m_g, \forall g\in\mathcal{G}$
	       \FOR{$j=1,\cdots, |\mathcal{U}|/B$}
                 \IF{$(n*|\mathcal{N}|/B + j) \% \beta = 0$}
                 \STATE Copy parameters from $m^i(\cdot)$ to $m^i_{\text{freeze}}(\cdot)$ and get all item embedding $\bm{E}$
                 \STATE Initialize dual solution $\bm{\mu} = 0$, and momentum gradient $\bm{g} = 0$ and $t=0$.
                 \ENDIF
    	    \STATE Get sub-dataset $\{u,i,c_{u,i}\}_{b=1}^B$ and user feature $\bm{e}_u$ and item feature $\bm{e}_i$
    	    \STATE $\mathcal{L}^j = [-c_{u, i}\log(\hat{c}_{u,i})]_{b=1}^B, \quad \bm{s}^j = \bm{1} - \hat{\bm{A}}^j\bm{\mu}$
                \STATE Apply gradient descent based on loss $(\bm{s}^j)^{\top}\mathcal{L}^j$
                \STATE $\widetilde{\bm{w}}_b = \sum_{k=1}^K (\bm{e}_{u_b}^{\top}\bm{E}^b)_{[k]}, \forall b$
    	    \STATE $\widetilde{\bm{g}}^j = -(\hat{\bm{A}}^j)^{\top} \widetilde{\bm{w}}+ \bm{\gamma}_j\quad$, $\bm{g}^j=\alpha \widetilde{\bm{g}}^j + (1-\alpha)\mathbf{g}, \quad \bm{g}=\bm{g}^j$
                \STATE $\bm{\gamma}_j=\bm{\gamma}_{j-1}-(\hat{\bm{A}}^j)^{\top} \widetilde{\bm{w}},  \bm{\mu} =  \argmin_{\bm{\mu}_0\in\mathcal{M}} \left[(\bm{g}^j)^{\top}\bm{\mu}_0 + \eta \|\bm{\mu}_0-\bm{\mu}\|_2^2\right]$
                % \STATE 
                % $
                %     \bm{\mu} =  \argmin_{\bm{\mu}_0\in\mathcal{M}} \left[(\bm{g}^j)^{\top}\bm{\mu}_0 + \eta \|\bm{\mu}_0-\bm{\mu}\|_2^2\right]
                % $
                %\STATE $t = t+1$ 
	       \ENDFOR
        \ENDFOR
	
	\end{algorithmic}
 
\end{algorithm}

% In recommendation, the user-item pair will be sampled from all dataset $\mathcal{D}$, therefore, a
%\vspace{-0.3cm}



\subsection{FairDual}

We then will introduce our method FairDual under random shuffling mini-batch training strategies. The overall workflow of FairDual under every two batches $j$ and $j+1$ can be seen in Figure~\ref{fig:framework}. According to analysis in Theorem~\ref{theo:reweight}, under each epoch, the overall optimization process will become:
\begin{equation}\label{eq:dual_loss}
     \mathcal{L}^{'B} = \min \quad \sum_{j=1}^{|\mathcal{U}|/B} (\bm{s}^j)^{\top}\bm{l}^j,
\end{equation}
where $\bm{l}^j\in\mathbb{R}^B, \bm{s}^j\in\mathbb{R}^B$ is loss and its weight under $j$-th batch.
Next, we will explain how $\bm{l}^j$ and $\bm{s}^j$ update on each batch $j$. Detailed algorithm workflow can be seen in Algorithm~\ref{alg:fairdual}. Note that, following the practice in~\cite{bao2023bi}, we utilize the user's historical behaviors to represent each user, thereby treating each sample as a unique user.


%even a large language model, such as NRMS~\cite{wu-etal-2019-neural-news}
\subsubsection{Accuracy Loss Constructing}\label{sec:backbone}
In the mainstream recommender architectures, the primary objective is to make the predicted score close to the true user preference. That is, at each batch $j$, there are $B$ user-item pair $[(u, i)]_{b=1}^B$ arrives. Then the loss vector $\mathcal{L}^j$ is computed as:
\begin{equation}\label{eq:acc_loss}
    \bm{l}^j = [-c_{u, i}\log(\hat{c}_{u,i})]_{b=1}^B,
\end{equation}
where
$\hat{c}_{u,i} = -d(\bm{e}_{u}, \bm{e}_{i})\leq 1$,
where $d(\cdot)$ is the normalized distance between embedding $\bm{e}_{u}, \bm{e}_{i}$. The commonly used distance metric is the dot-product, i.e.
$
    d(\bm{e}_u, \bm{e}_i) = -\bm{e}_u^{\top}\bm{e}_i,
$
and the $\bm{e}_{u}$ and $\bm{e}_{i}$ are calculated by a complex model, i.e. 
$
    \bm{e}_{u} = m^u(u), \bm{e}_{i} = m^i(i),
$
where $m^u(\cdot)$ and $ m^i(\cdot)$ are two embedding extraction networks. Typically, the user $u$ is represented by the item-clicked history sequences before clicking the item $i$: $[i^1, i^2, \cdots, i^n]$, where $n$ is the fixed item sequence length. 

Note that in text-based recommendation models such as BigRec~\citep{bao2023bi} and Recformer~\citep{Recformer}, each item $i$ is represented as a sequence of words in natural language: $i = [w^1, w^2, \cdots, w^l]$, where $l$ is the length of the sentence and user behaviors are also represented in prompt form~\citep{bao2023bi}. In such cases, Equation~\eqref{eq:acc_loss} is extended to $\log {\hat{c}_{u,i}} = \sum_{i=1}^l\log(P(w^i))$, where $P(w)$ refers to the predicted probability of the word $w$ generated by the LLMs, while other equations remain unchanged.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{img/framework.pdf}
    \caption{Overall workflow of FairDual under every two batches $j$ and $j+1$. }
    \label{fig:framework}
    \vspace{-0.3cm}
\end{figure}

\subsubsection{Mirror Gradient Decent for Group Weight}

For each batch $j$, the model needs to decide the weight $\bm{s}^j$ for each sample. The weight $\bm{s}^j$ is computed utilizing mirror-gradient descent~\citep{balseiro2021regularized} technique. Specifically, 
\begin{equation}\label{eq:weight}
    \bm{s}^j = \bm{1} - \hat{\bm{A}}^j\bm{\mu}^j,
\end{equation}
where $\hat{\bm{A}}^j\in\mathbb{R}^{B\times|\mathcal{G}|}$ represents the row-normalized item-group adjacency matrix for $\bm{A}$ in batch $j$ (see details in Equation~\eqref{eq:t_fair}), with $\bm{A}_{i_b, g} = 1$ indicating that the $b$-th item in batch $i_b\in \mathcal{I}_g$ belongs to group $g$, and 0 otherwise. And $\bm{\mu}^j$ is the dual variable at $j$-th batch, which updates as:
\begin{equation}\label{eq:dual_update}
\begin{aligned}
      \bm{\mu}^j =  \argmin_{\bm{\mu}} \left[(\bm{g}^j)^{\top}\bm{\mu} + \eta \|\bm{\mu}-\bm{\mu}^{j-1}\|\right], \quad \textrm{s.t.} \sum_{j=1}^{g} \bm{\mu}_jm_j + \lambda \ge 0,~ \forall g = 1, 2 , \ldots, |\mathcal{G}|, 
\end{aligned}
\end{equation}
where $\eta$ is the learning rate, $\bm{g}^j$ is the sub-gradient of the Equation~\eqref{eq:reweight} \wrt the dual variable $\bm{\mu}^j\in\mathbb{R}^{|\mathcal{G}|}$. The projection step can be efficiently solved using convex optimization solvers~\citep{balseiro2021regularized} since $\mathcal{D}$ is coordinate-wisely symmetric. 

Specifically, to ensure smoothness and make user of historical information, we utilize the momentum gradient descent to update $\bm{g}^j$:
\begin{equation}
    \bm{g}^j=\alpha \widetilde{\bm{g}}^j + (1-\alpha)\mathbf{g}^{j-1}, \quad \widetilde{\bm{g}}^j=\partial (\bm{s}^j\mathcal{L}^j + \lambda r^*(\bm{\mu}^j)) = -(\mathbf{A}^j)^{\top} \widetilde{\bm{w}}+ \bm{\gamma}_j,
\end{equation}
where $\bm{\gamma}_j\in\mathbb{R}^{|\mathcal{G}|}$ is the vector, whose element of index $g$ denotes the remaining required loss (\ie reward) for the group $g$ at batch step $j$, $\widetilde{\bm{w}}\in\mathbb{R}^{B}$ represents the estimated ranking score that each user query will receive. However, given the vast size of the item corpus in recommendation systems, conducting a full ranking on all items is impractical. Therefore, we randomly sample $Q$ items to approximate the ranking scores across all items. 
The $Q$ items' embeddings are denoted as $\bm{E}^j\in\mathbb{R}^{Q\times d}$. Formally, for the $b$-th element $\widetilde{\bm{w}}_b$, we can write:
$
    \widetilde{\bm{w}}_b = \sum_{k=1}^K (\bm{E}^j\bm{e}_{u_b})_{[k]},
$
where $\bm{x}_{[k]}$ denote the $k$-th largest element in vector $\bm{x}$ and $K$ is the ranking size. 


Note that $\bm{E}^b$ is sampled from the pre-stored item embedding $\bm{E}\in\mathbb{R}^{|\mathcal{I}|\times d}$, which is pre-calculated using the freezer network $m_{\text{freeze}}^i(\cdot)$. This is done to mitigate the significant fluctuations in $\widetilde{\bm{w}}$ caused by unstable training~\citep{fan2020theoretical}. To achieve this, we freeze the item feature extractor $m^i(\cdot)$ as $m_{\text{freeze}}^i(\cdot)$ and transfer the parameters from $m^i(\cdot)$ to $m_{\text{freeze}}^i(\cdot)$ every $\beta$ batches. For the first batch process, we initialize $\bm{g}^1$ as $\bm{0}$, which will not make an effect on the first batch.


\subsubsection{Bound on Jensen Gap}\label{sec:bound}
We will provide the Jensen gap converge analysis of FairDual in the following theorem.

%, which shows that FairDual can achieve superior converge rates even under small batch sizes and large group sizes.

\begin{theorem}[Bound on Jensen Gap]\label{theo:Jensen_Gap}
There exists $H>0$ such that $\|\bm{\mu}^j-\bm{\mu}\|_2^2\leq H$ and function $\|\cdot\|_2^2$ is $\sigma-$strongly convex. 
Then, there exists $L>0$, the Jensen gap of FairDual can be bounded as:
\begin{equation}
    J(B) \leq \frac{H}{\eta} + \frac{|\mathcal{U}|L|\mathcal{G}|^2}{B(1-\alpha)\sigma}\eta + \frac{L|\mathcal{G}|^2}{2(1-\alpha)^2\sigma\eta}.
\end{equation}
When setting learning rate $\eta=O(B^{-1/2})$, the bound of Jensen gap is comparable with $O(B^{-1/2})$.
\end{theorem}
The detailed proof can be seen in Appendix~\ref{app:prof_Jensen_Gap}. From Theorem~\ref{theo:Jensen_Gap}, it is apparent that the Jensen gap will widen as the batch size $B$ decreases and the group size $|\mathcal{G}|$, as well as the max-min fairness degree $\lambda$, increase. However, FairDual demonstrates a sub-linear convergence rate concerning the batch size $B$, and it maintains strong performance even with small batch sizes and large group sizes across various fairness degrees.



