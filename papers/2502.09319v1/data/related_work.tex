\section{Related Work}
\textbf{Fairness concept in RS.} One common categorization is based on the involvement of different stakeholders~\citep{abdollahpouri2020multistakeholder, abdollahpouri2019multi}, divides fairness into individual fairness~\citep{marras2022equality, li2021towards}, which aims to ensure equitable treatment for individual users, and group fairness, which classifies items into various groups~\citep{xu2023p, xu2024fairsync, fairrec, cpfair, wu2021tfrom}. Various approaches have been proposed to optimize fairness utilizing different fairness objectives. For instance, ~\cite{fairrec} proposed using the Shapley value, while ~\cite{do2022optimizing} suggested optimizing the Gini Index. On the other hand, works such as ~\cite{xu2023p, nips21welf, xu2024fairsync} advocate for optimizing MMF, which requires every group should receive a ``minimum wage''. Typically, we mainly focus on the group MMF, which is closer to the industrial scenarios since certain studies propose to ensure minimum item exposures for attracting providers to join or enhancing the visibility of specific item categories~\citep{fairrec, xu2024fairsync, zhu2020measuring}.
%In this paper, we mainly focus is on the group MMF, as it is commonly employed in real-world scenarios.

%while others like ~\citep{jiang2021generalized, Tang23FairBias, jiang2024itemside} recommend using the distance between different item utilities. 

\textbf{Optimizing fairness in RS.} When optimizing fairness, previous research often categorizes methods into three categories based on recommendation phases, including pre-processing~\citep{Calmon17, xiong2024fairwasp, xu-etal-2024-study}, post-processing~\citep{xu2023p, fairrec, wu2021tfrom, TaxRank} and in-processing~\citep{narasimhan2020pairwise, Tang23FairBias}. 
In this paper, we theoretically demonstrate that the in-processing method constrained by group MMF can be essentially reformulated as a re-weighting approach. Prior research employed static or dynamic group weights to achieve fairness. For static weights, ~\cite{jiang2024itemside, xiong2024fairwasp} proposes to set item weight according to its popularity and the Wasserstein distance of two groups, respectively.  For dynamic weighting, some work~\citep{chen2023fairly, chai2022fairness, wen2022distributionally} propose to design weights based on the training state, while \cite{hu2023adaptive} employs a dynamic re-weighting strategy to mitigate distribution shifts between training and test data. ~\cite{roh2020fairbatch} also proposes to set different batch sizes to optimize fairness. However, these methods are either designed for simple cases involving only two groups, or they lack theoretical guarantees when applied to group MMF settings.

\textbf{Optimizing fairness in ML.} In machine learning (ML), previous work aims to optimize different fairness functions to achieve various social welfare objectives. For example, the power-mean welfare family seeks to balance accuracy and fairness objectives by applying the exponential form~\citep{cousins2021axiomatic, cousins2023revisiting} and max-min fairness~\citep{abernethy2022active, agarwal2018reductions} aims to support the worst-off groups. When optimizing fairness, we commonly try to optimize a nonlinear fairness function. When adopting optimization methods such as stochastic gradient descent (SGD), an unavoidable bias will exist~\citep{demidovich2023guide, hu2020biased}. To bridge this bias, previous ML methods have employed sampling strategies~\citep{abernethy2022active, cousins2022uncertainty} to obtain unbiased samples, while some methods have utilized debiasing SGD~\citep{demidovich2023guide, agarwal2018reductions} to mitigate the bias. However, these works cannot be applied to large-scale industrial RS since they often require a convex optimization process that is impractical for RS tasked with serving millions of users and hundreds of groups. To efficiently bridge the Jensen gap, our method improves debiasing SGD by developing a large-scale friendly mirror SGD learning algorithm. 

%It can be efficiently applied to real industrial systems.




%In pre-processing methods, a straightforward approach involves adjusting data before the learning process. For example, ~\citep{Calmon17} adopt a transformation function to original data, and ~\citep{xiong2024fairwasp} propose to utilize the Wasserstein optimizing process to sample different group samples. In post-processing methods, previous work often models the fair-aware re-ranking tasks as integral linear programming~\citep{xu2023p, fairrec, wu2021tfrom, nips21welf}. When it comes to in-processing methods, researchers~\citep {narasimhan2020pairwise, Tang23FairBias} frequently integrate the fairness loss function with the original recommendation loss function to achieve fairness. In this paper, our primary focus is on in-processing methods for MMF.

%, as they offer a more effective and adaptable solution by balancing accuracy and fairness in an end-to-end manner~\citep{li2022fairness, caton2020fairness}.

