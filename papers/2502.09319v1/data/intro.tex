\section{Introduction}

% Recently, group max-min fairness (MMF) has emerged as a critical concern in industrial recommender systems (RS), aiming to provide support for marginalized groups such as providers and item categories within a period of time~\citep{xu2023p, fairrec}. 
% In such a way, new providers can be attracted to join and certain item categories should be amplified during particular festivals or in response to platform development requirements~\citep{zhu2020measuring, xu2024fairsync}.  
%small sellers in Amazon have difficulty facing the challenges of ``lower profitability''


% Group max-min fairness (MMF) has gained significant attention in industrial recommender systems (RS), as it seeks to provide support for weak item groups, including those categorized by item types or providers~\citep{xu2023p, fairrec, xu2024fairsync}. 
Group max-min fairness (MMF) has gained significant attention in industrial recommender systems (RS), as it seeks to provide support for marginalized item groups, where item groups are usually divided by item categories or providers~\citep{xu2023p, fairrec, xu2024fairsync}.  
For example, optimizing MMF can alleviate the low exposure problem of small sellers in the Amazon RS platform~\citep{fairrec}.  
As illustrated in European competition law~\citep{jones2014eu}, protecting these weak supplier groups is essential for preventing large platforms from engaging in unfair strategies, thereby ensuring fair competition~\citep{matten2008implicit}. 


%广：一个更具体的例子，结合MMF的特性, 深：unfairness会带来concern，符合某些价值，法律


%When integrating group MMF constraint into the optimization objective in RS, it often necessitates that each item group receives a minimum specified reward, akin to a ``minimum wage policy''~\cite{fairrec, xu2024fairsync, nips21welf}.
%Meanwhile, such MMF constraints often necessitate calculating the fairness loss accumulated across all samples rather than computing the loss on each sample and aggregating them. 
%Due to the limited ranking slots, changing one sample will inevitably impact the ranking results of other samples. 
%Therefore, When introducing the MMF constraint, the loss function violates a crucial assumption: the independence of samples, resulting in the group MMF constraint causing the loss function to deviate from linear additivity (See Section~\ref{sec:analsysis}). 


Formally, the group MMF optimization objective entails calculating the overall utility of each group and maximizing the utility of the least advantaged group. Typically, group utility computation often necessitates aggregating the overall recommendation ranking list over a specified period~\citep{xu2023p, nips21welf, xu2024fairsync}, for example, the group utility could be the exposures of action movies within a day. 
Due to the limited ranking slots, adjusting the exposure of items to improve the utility of one item group will inevitably affect the ranking outcomes of other item groups. 
Therefore, the loss function with the MMF constraint for RS violates a crucial assumption: the independence of samples, resulting in the MMF loss of different item groups not adhering to linear additivity (see theoretical proof in Section~\ref{sec:analsysis}). 



We theoretically and empirically show that the non-linear additivity property of the MMF-constrained objective will introduce a Jensen gap~\citep{gao2017bounds, ullah2021determination} between the model's convergence point and the optimal point when if mini-batch sampling is applied for optimization (see Section~\ref{sec:analsysis}). 
Meanwhile, it is proved that as the mini-batch size decreases and the group size increases, the Jensen gap will become more pronounced in the optimization process, significantly harming the model's performance. 
However, mini-batch sampling strategies are essential for accelerating the model training process, especially as data and model sizes in RS continue to grow, such as the development of large language models (LLMs) based RS~\citep{bao2023bi, lin2024bridging, lin2024data}.






Some previous approaches have the potential to bridge the Jensen gap for RS. 
One type of heuristic approach can be applied to bridge the Jensen gap, such as sample re-weighting strategies~\citep{chen2023fairly, wen2022distributionally}, which dynamically assigns a higher weight to the weaker group across different batches.
However, the effectiveness of this research line is limited due to the lack of theoretical guarantees. 
Another type of work utilizes machine learning techniques that can help to optimize the non-linear additive loss functions. 
For example, \cite{abernethy2022active, cousins2022uncertainty} have proposed sampling strategies to obtain unbiased samples, while some methods utilize debiasing gradient descent~\citep{demidovich2023guide, agarwal2018reductions} to introduce a bias correction term in the gradient. However, these methods cannot be applied to existing large-scale industrial RS, as they often require a convex optimization process that is impractical for RS that typically serves millions of users and hundreds of groups.


%which need to serve millions of users and hundreds of groups,
 
%In the machine learning (ML) field, some work~\citep{demidovich2023guide, agarwal2018reductions} utilized the debiasing gradient method to mitigate the local bias when optimizing the non-linear additive loss functions, however, these works cannot be applied to existing large-scale industrial RS, since recommendation task requires to serve millions of users and hundreds of groups. 


To overcome the challenges for bridging the Jensen gap, in this paper, we firstly theoretically demonstrate that the optimization objectives when incorporating group MMF constraint can be essentially reformulated as a group-weighted accuracy optimization objective on different groups.
Then, we introduce a large-scale friendly, and effective algorithm called FairDual
to optimize the group-weighted objective for minimizing the Jensen gap. 
Specifically, we formulate the fairness-constraint problem as its dual, where the dual variable (referred to as the shadow price in economics~\citep{dreze1990policy}) can be interpreted as the sample weight assigned to each sample in the mini-batch optimization process.
Then, FairDual leverages dual-optimization techniques to optimize the weight of different group losses utilizing dual mirror gradient techniques efficiently.

%group MMF constraint under random shuffle mini-batch optimization strategy, even when confronted with small mini-batch sizes and large group sizes. 



Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal point under a random shuffling mini-batch training style. Moreover, the Jensen gap can be well bounded (See Section~\ref{sec:bound}) even when confronted with small mini-batch sizes and large group sizes. Extensive experiments using six large-scale RS backbone models on three publicly available datasets show that FairDual consistently reduces the Jensen gap and outperforms all baselines with a large margin in terms of both accuracy and fairness while achieving better efficiency. 



%Specifically, we begin by formulating the fairness-constraint problem as its dual, where the dual variable (referred to as the shadow price in economics~\citep{dreze1990policy}) can be interpreted as the sample weight assigned to each sample in the mini-batch optimization process. We then propose an efficient algorithm to optimize the sample weight utilizing dual mirror gradient techniques efficiently. 

