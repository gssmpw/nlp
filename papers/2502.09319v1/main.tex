\documentclass{article}



% ready for submission
%\PassOptionsToPackage{numbers, compress}{natbib}

\usepackage{iclr2025,times}




\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage[boxed,ruled,lined]{algorithm2e}
\usepackage{algorithmic}

% ------new command-------
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{wrapfig}
\usepackage{subfigure}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{cleveref}
\usepackage{caption}
\usepackage{threeparttable}
\usepackage{comment}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{definition}{Definition}
\usepackage{setspace}
\usepackage{adjustbox}
\usepackage{bbding}
\usepackage{bm}
%\usepackage[numbers]{nat}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}

\definecolor{Instruction}{RGB}{237, 84, 102}
\definecolor{Example}{RGB}{81, 140, 192}
\definecolor{Question}{RGB}{0, 0, 0}
\definecolor{Answer}{RGB}{140, 192, 81}

\newcommand{\ie}{\emph{i.e., }}
\newcommand{\eg}{\emph{e.g., }}
\newcommand{\etal}{\emph{et al. }}
\newcommand{\st}{\emph{s.t. }}
\newcommand{\etc}{\emph{etc.}}
\newcommand{\wrt}{\emph{w.r.t. }}
\newcommand{\cf}{\emph{cf. }}
\newcommand{\aka}{\emph{a.k.a. }}
\newcommand{\re}[1]{{\color{blue}#1}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\title{Towards Fairness in Global Optimization: Mini-batch Sample Strategies for Recommendation Systems}
\title{FairDual: A Mini-Batch Sample Strategy for Multi-Group Max-min Fairness in Recommendation}
\title{FairDual: Optimizing Non-Finite-Sum Objectives Under Amortized Max-Min Fairness Constraints}
\title{Bridging Jensen Gap for Max-Min Group Fairness Optimization in Recommendation}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\input{preamble/authors}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.

\iclrfinalcopy
\begin{document}


\maketitle


\begin{abstract}


% In fair-aware recommender systems (RS), group max-min fairness (MMF) is regarded as a common optimization objective as industrial platforms frequently establish minimum exposure thresholds for diverse groups to attract new providers and boost the visibility of specific categories.
% However, our theoretical analysis indicates that integrating such MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces a gap (known as the Jensen gap) between the model's convergence point and the optimal solution when employing mini-batch sampling strategies. Both our theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, exacerbated by increasing computational costs and model sizes (such as large language models), the Jensen gap will become more pronounced.
% To bridge the Jensen gap, we first theoretically demonstrate that the optimization constrained by MMF can be essentially reformulated as a re-weighting approach. Previous studies often propose heuristic re-weighting approaches or debiasing gradient descent methods to enhance global fairness, but they either lack rigorous theoretical guarantees or face challenges in being directly applicable to large-scale industrial RS. In this paper, we present a model named FairDual, which utilizes dual-optimization to develop a large-scale friendly, ranking-specific model. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution. We show that the Jensen gap can be well bounded under a random shuffling mini-batch training style. Extensive experiments conducted on three large-scale RS backbone models using two publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness.

Group max-min fairness (MMF) is commonly used in fairness-aware recommender systems (RS) as an optimization objective, as it aims to protect marginalized item groups and ensures a fair competition platform. However, our theoretical analysis indicates that integrating MMF constraint violates the assumption of sample independence during optimization, causing the loss function to deviate from linear additivity. Such nonlinearity property introduces the Jensen gap between the model's convergence point and the optimal point if mini-batch sampling is applied. Both theoretical and empirical studies show that as the mini-batch size decreases and the group size increases, the Jensen gap will widen accordingly. 
Some methods using heuristic re-weighting or debiasing strategies have the potential to bridge the Jensen gap. However, they either lack theoretical guarantees or suffer from heavy computational costs. To overcome these limitations, we first theoretically demonstrate that the MMF-constrained objective can be essentially reformulated as a group-weighted optimization objective. Then we present an efficient and effective algorithm named FairDual, which utilizes a dual optimization technique to minimize Jensen gap. Our theoretical analysis demonstrates that FairDual can achieve a sub-linear convergence rate to the globally optimal solution and the Jensen gap can be well bounded under a mini-batch sampling strategy with random shuffle. Extensive experiments conducted using six large-scale RS backbone models on three publicly available datasets demonstrate that FairDual outperforms all baselines in terms of both accuracy and fairness. Our data and codes are shared at~\url{https://github.com/XuChen0427/FairDual}.



%, exacerbated by increasing computational costs and model sizes (such as large language models)



\end{abstract}



\input{data/intro}

\input{data/related_work}

\input{data/formulation}

\input{data/method}

\input{data/experiment}



\section{Conclusion}\label{sec:limitation}

In this paper, we theoretically demonstrate that adapting mini-batch training with the objective constrained by group MMF inevitably leads to the Jensen gap, thereby impairing the performance of the RS model. We theoretically and empirically analyze the origins of the Jensen gap by demonstrating that the integration of the MMF constraint disrupts the assumption of sample independence during optimization, leading to a deviation of the loss function from linear additivity.
Then, to efficiently bridge the Jensen gap, we develop a large-scale friendly algorithm named FairDual, which employs dual-optimization techniques to minimize the Jensen gap at a sub-linear rate.   Extensive experiments conducted on three large-scale recommendation system backbone models using two publicly available datasets show that FairDual consistently outperforms all baseline methods. 

%However, our methods still face challenges in terms of convergence, requiring large datasets and computational resources, particularly when dealing with large group sizes. It will inspire the community to design a more effective and efficient method to bridge the Jensen gap and applies to other non-linear optimization objectives.

\input{data/ack}
% xxx

\bibliographystyle{abbrvnat}
\bibliography{mybib}

\input{data/appendix}
%\input{data/checklist}

\end{document}