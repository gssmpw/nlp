
%% bare_conf.tex
%% V1.4b
%% 2015/08/26
%% by Michael Shell
%% See:
%% http://www.michaelshell.org/
%% for current contact information.
%%
%% This is a skeleton file demonstrating the use of IEEEtran.cls
%% (requires IEEEtran.cls version 1.8b or later) with an IEEE
%% conference paper.
%%
%% Support sites:
%% http://www.michaelshell.org/tex/ieeetran/
%% http://www.ctan.org/pkg/ieeetran
%% and
%% http://www.ieee.org/

%%*************************************************************************
%% Legal Notice:
%% This code is offered as-is without any warranty either expressed or
%% implied; without even the implied warranty of MERCHANTABILITY or
%% FITNESS FOR A PARTICULAR PURPOSE! 
%% User assumes all risk.
%% In no event shall the IEEE or any contributor to this code be liable for
%% any damages or losses, including, but not limited to, incidental,
%% consequential, or any other damages, resulting from the use or misuse
%% of any information contained here.
%%
%% All comments are the opinions of their respective authors and are not
%% necessarily endorsed by the IEEE.
%%
%% This work is distributed under the LaTeX Project Public License (LPPL)
%% ( http://www.latex-project.org/ ) version 1.3, and may be freely used,
%% distributed and modified. A copy of the LPPL, version 1.3, is included
%% in the base LaTeX documentation of all distributions of LaTeX released
%% 2003/12/01 or later.
%% Retain all contribution notices and credits.
%% ** Modified files should be clearly indicated as such, including  **
%% ** renaming them and changing author support contact information. **
%%*************************************************************************


% *** Authors should verify (and, if needed, correct) their LaTeX system  ***
% *** with the testflow diagnostic prior to trusting their LaTeX platform ***
% *** with production work. The IEEE's font choices and paper sizes can   ***
% *** trigger bugs that do not appear when using other class files.       ***                          ***
% The testflow support page is at:
% http://www.michaelshell.org/tex/testflow/



\documentclass[conference]{IEEEtran}
\usepackage{array}
\usepackage{booktabs} % For formal tables
\usepackage{multirow}
\usepackage{lipsum}
\usepackage{algorithm}
\usepackage{color, soul}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{mdframed}
\usepackage{url}
\usepackage{cite}
%\usepackage{appendix}
\usepackage[noend]{algpseudocode}
\usepackage[utf8]{inputenc}
\usepackage{comment}
\usepackage{hyperref}

% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}
\newcolumntype{P}[1]{>{\centering\arraybackslash}m{#1}}
\newcommand{\specialcell}[2][c]{%
  \begin{tabular}[#1]{@{}c@{}}#2\end{tabular}}

% Some Computer Society conferences also require the compsoc mode option,
% but others use the standard conference format.
%
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[conference]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
% %
% % paper title
% % Titles are generally capitalized except for words such as a, an, and, as,
% % at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% % not capitalized unless they are the first or last word of the title.
% % Linebreaks \\ can be used within to get better formatting as desired.
% % Do not put math or special symbols in the title.
\title{Can Machine Learning Support the Selection of Studies for Systematic Literature Review Updates?}

\author{

\IEEEauthorblockN{Marcelo Costalonga}
\IEEEauthorblockA{\textit{PUC-Rio} \\
Rio de Janeiro, Brazil \\
mcardoso@inf.puc-rio.br}
\and
\IEEEauthorblockN{Bianca Minetto Napole\~ao}
\IEEEauthorblockA{\textit{Université du Québec à Chicoutimi} \\
Chicoutimi, Canada \\
bianca.minetto-napoleao1@uqac.ca}
\and
\IEEEauthorblockN{Maria Teresa Baldassarre}
\IEEEauthorblockA{\textit{University of Bari} \\
Bari, Italy \\
mariateresa.baldassarre@uniba.it}
\and
\IEEEauthorblockN{Katia Romero Felizardo}
\IEEEauthorblockA{\textit{Universidade Tecnológica Federal do Paraná} \\
Cornélio Procópio, Brazil \\
katiascannavino@utfpr.edu.br}
\and
\IEEEauthorblockN{Igor Steinmacher}
\IEEEauthorblockA{\textit{Northern Arizona University} \\
Flagstaff, USA \\
igor.steinmacher@nau.edu}
\and
\IEEEauthorblockN{Marcos Kalinowski}
\IEEEauthorblockA{\textit{PUC-Rio} \\
Rio de Janeiro, Brazil \\
kalinowski@inf.puc-rio.br}
}
% // TODO: ajeitar emails
%\and
%\IEEEauthorblockN{Daniel Méndez Fernández}
%\IEEEauthorblockA{Software \& Systems Engineering\\
%Technical University of Munich\\
%Garching, Germany\\
%Email: daniel.mendez@tum.de}










% % conference papers do not typically use \thanks and this command
% % is locked out in conference mode. If really needed, such as for
% % the acknowledgment of grants, issue a \IEEEoverridecommandlockouts
% % after \documentclass

% % for over three affiliations, or if they all won't fit within the width
% % of the page, use this alternative format:
% % 
% %\author{\IEEEauthorblockN{Michael Shell\IEEEauthorrefmark{1},
% %Homer Simpson\IEEEauthorrefmark{2},
% %James Kirk\IEEEauthorrefmark{3}, 
% %Montgomery Scott\IEEEauthorrefmark{3} and
% %Eldon Tyrell\IEEEauthorrefmark{4}}
% %\IEEEauthorblockA{\IEEEauthorrefmark{1}School of Electrical and Computer Engineering\\
% %Georgia Institute of Technology,
% %Atlanta, Georgia 30332--0250\\ Email: see http://www.michaelshell.org/contact.html}
% %\IEEEauthorblockA{\IEEEauthorrefmark{2}Twentieth Century Fox, Springfield, USA\\
% %Email: homer@thesimpsons.com}
% %\IEEEauthorblockA{\IEEEauthorrefmark{3}Starfleet Academy, San Francisco, California 96678-2391\\
% %Telephone: (800) 555--1212, Fax: (888) 555--1212}
% %\IEEEauthorblockA{\IEEEauthorrefmark{4}Tyrell Inc., 123 Replicant Street, Los Angeles, California 90210--4321}}




% % use for special paper notices
% %\IEEEspecialpapernotice{(Invited Paper)}


% make the title area
\maketitle

% % no keywords
% \begin{IEEEkeywords}
% requirements engineering, machine learning, systematic mapping study
% \end{IEEEkeywords}



% % For peer review papers, you can put extra information on the cover
% % page as needed:
% % \ifCLASSOPTIONpeerreview
% % \begin{center} \bfseries EDICS Category: 3-BBND \end{center}
% % \fi
% %
% % For peerreview papers, this IEEEtran command inserts a page break and
% % creates the second title. It will be ignored for other modes.
% % \IEEEpeerreviewmaketitle

% \input{sections/01-introduction.tex}
% \input{sections/02-background.tex}
% \input{sections/03-protocol.tex}
% \input{sections/04-results.tex}
% \input{sections/05-conclusions.tex}
% \input{sections/06-threats.tex}
% \input{sections/07-remarks.tex}

% \bibliographystyle{IEEEtranS}
% \bibliography{bibTex/sigproc} 

% \input{Apendix/apendix.tex}
% \end{document}


% % As a general rule, do not put math, special symbols or citations
% % in the abstract
\begin{abstract}
[Background] Systematic literature reviews (SLRs) are essential for synthesizing evidence in Software Engineering (SE), but keeping them up-to-date requires substantial effort. Study selection, one of the most labor-intensive steps, involves reviewing numerous studies and requires multiple reviewers to minimize bias and avoid loss of evidence.
[Objective] This study aims to evaluate if Machine Learning (ML) text classification models can support reviewers in the study selection for SLR updates.
[Method] We reproduce the study selection of an SLR update performed by three SE researchers. We trained two supervised ML models (Random Forest and Support Vector Machines) with different configurations using data from the original SLR. We calculated the study selection effectiveness of the ML models for the SLR update in terms of precision, recall, and F-measure. We also compared the performance of human-ML pairs with human-only pairs when selecting studies.
[Results] The ML models achieved a modest F-score of 0.33, which is insufficient for reliable automation. However, we found that such models can reduce the study selection effort by 33.9\% without loss of evidence (keeping a 100\% recall). Our analysis also showed that the initial screening by pairs of human reviewers produces results that are much better aligned with the final SLR update result. [Conclusion] Based on our results, we conclude that although ML models can help reduce the effort involved in SLR updates, achieving rigorous and reliable outcomes still requires the expertise of experienced human reviewers for the initial screening phase.


\end{abstract}

% no keywords
\begin{IEEEkeywords}
Systematic Review Automation, Selection of Studies, Machine Learning, Systematic Literature Review Update
\end{IEEEkeywords}
% \keywords{Systematic Review Automation, Selection of Studies, Machine Learning}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

\input{sections/01-introduction.tex}
\input{sections/02-background.tex}
\input{sections/03-goals-and-rqs.tex}
\input{sections/04-methodology.tex}
\input{sections/05-results.tex}
\input{sections/06-discussion.tex}
\input{sections/07-threats.tex}
\input{sections/08-conclusions.tex}

\section*{Acknowledgment}

We express our gratitude to CNPq (Grant 312275/2023-4), FAPERJ (Grant E-26/204.256/2024), and Stone Co. for their generous support.

\bibliographystyle{IEEEtranS}
\bibliography{bibTex/sigproc} 

\end{document}

% \maketitle



% %--------------------------------------------------------
% \section{Background and Related Work}
% \label{sec:relatedwork}

% %SLR update definition
% An SLR update is a more recent (updated) version of an SLR that includes new evidence (primary studies) \cite{Mendes2020}. For the inclusion of new and relevant evidence, one of the initial steps is to conduct the study selection activity which consists of analyzing the retrieved studies from the search process to evaluate the need and to perform the SLR update. 

% %related work
% As mentioned before, there are several initiatives in SE towards improvements for SLR update (e.g. \cite{felizardo16, Garces17, Mendes2020, Wohlin2020}). However, considering the focus of our study on automation to select studies for SLR updates, we highlighted three main related works \cite{Watanabe20, Felizardo14, Napoleao2021} described in the following.

% The work of Watanabe \textit{et al.} \cite{Watanabe20} also evaluated the use of text classification (text mining combined with ML Models) to support the study selection activity for SLR updates in SE. They performed an evaluation with 8 SLRs from different research domains in a cross-validation procedure using Decision Tree (DT) and SVM as ML classification algorithms. The results achieved on average a \textit{F-score} of 0.92, \textit{Recall} of 0.93 and \textit{Precision} of 0.92. Unlike the approach proposed in \cite{Watanabe20}, our study evaluates the ML Models SVM and RF using a detailed database of a solid ongoing SLR update conducted by renowned researchers in the field of EBSE. Furthermore, we compare the agreement level of the adopted ML Models with the expert reviewers through \textit{Kappa} analysis \cite{Cohen10, Kitchenham15}.

% %While the contributions of Watanabe et al. \cite{Watanabe20} laid a valuable foundation by showing the potential of supervised ML Models in assisting the study selection process for SLR updates, our study takes a different approach to enhance the applicability of such methodologies. Our study used a detailed database of a solid ongoing SLR update conducted by renowned researchers in the field of EBSE, we  also used a distinct approach to retrieve the non-selected studies from the original SLR, and we explored different Feature Selection (FS) strategies, ML Models and techniques to train our classifiers. Furthermore, our study's contribution goes beyond evaluating how much effort could be reduced by the ML classifiers during the selection of studies process, we also  compared in detail the agreement level of the adopted ML classifiers with the expert reviewers through \textit{kappa} analysis \cite{Kitchenham15}.

% Felizardo \textit{et al.} \cite{Felizardo14} also proposes an automated alternative to support the selection of studies for SLR updates. The authors proposes a tool called Revis which links new evidence with the original SLR's evidence using the K-Nearest Neighbor (KNN) Edges Connection technique. The tool output is presented in two distinct visualizations, a content map and an Edge Bundles diagram. The results showed an increase in the number of studies correctly included compared to the traditional manual approach.

% Napoleão et al. \cite{Napoleao2021} performed a cross-domain Systematic Mapping (SM) on existing automated support for searching and selecting studies for SLRs and SMs in SE and Medicine. The authors indicated potential ML Models that can be adopted to support the study selection activities. They also indicated the most adopted methods (cross-validation and experiment) and metrics (\textit{Recall}, \textit{Precision} and \textit{F-measure}) to assess text classification approaches. The choice of the ML Models and the assessment metrics and methods for this study are considered finds of this work.


% \section{Goal and Research Questions}
% \label{sec:researchissues}

% Our goal is to evaluate the adoption of ML Models to support the selection of studies for SLR updates. We translated our goal into three different research questions (RQs).

%     \textbf{RQ1:} \textit{How effective are ML Models in selecting studies for SLR updates?}

%     We represent the effectiveness of the ML models in supporting the selection of studies activity using metrics such as \textit{Recall}, \textit{Precision} and \textit{F-measure} \cite{Napoleao2021, Watanabe20}. Our ML automated analysis considers only title and abstract of the studies. Our ML models results are compared with the included studies selected manually for the SLR update under evaluation. 
    
%     %Our ML automated analysis considers only title and abstract of the studies and the metrics are calculated at first considering the results from the expert reviewers analysis only on the title, abstract and keywords and next, considering also their results from the full-text analysis.

%     \textbf{RQ2:} \textit{How much effort can ML Models reduce during the study selection activity for SLR updates?}

%     We calculate the effort reduction by the relation of the number of studies that will need to have their title, abstract and keywords manually analyzed without the support of ML Models versus the number of studies to be analyzed after the use of the ML solution.
   
%     % Justificar o recall diferente de 100 (relatar o caso 100 e os casos 99)
%     % Mostrar um gráfico exibindo as variações de Falso Negativo / Positivo variando o Recall

%     \textbf{RQ3:} \textit{Can Machine Learning replace a reviewer in the selection of studies for SLRs?}
%     % We compared the agreement level of the ML Model with the highest \textit{F-score} value, supporting a single reviewer with the agreement level of each pair of reviewers, by calculating their Cohen's \textit{Kappa} coefficient \cite{Cohen10, Kitchenham15}.

%     We compared the levels of agreement and similarity of the ML Model with the highest \textit{F-score} value by performing two different analysis. For the agreement analysis, we used the Cohen's Kappa coefficient to measure the level of concordance between the ML Model and reviewers \cite{Cohen10, Kitchenham15}. For the similarity analysis, we used the Euclidean Distance to measure the distance between the ML Model and reviewers from different perspectives \cite{SERRA2014305}, to verify if the ML model had a higher similarity with any reviewer in comparison to the similarity within the assessment team, if the ML model had a higher similarity with the final than any reviewers, if the ML model decreased the distance from the final results when combining its answers with a single reviewer in comparison to other pairs of reviewers and if the ML model decreased the distance from the final results when combining its answers with two reviewers in comparison to the assessment team. The last evaluations intended to simulate the ML model working together with other reviewers during the agreement criteria from the selection of studies.  

%     % \end{itemize}

%     %---- Here I did not defined Kappa. I think we can defined it in the methodology section.

%     %Kappa analysis ->  Euclidean Distance
%     %agremeent level do algoritmo com os revisores - titulo, abstract and keywords
%     % future assessement + Hipótese :  Machine learning can replace a reviewer in a SLR update?

% %--------------------------------------------------------
% \section {Study Design}
% \label{sec:methodology}

% %Bianca: Aqui estava faltando definir qual o research metodo que a gente utilizou para testar o que foi desenvolvido. Eu utilizei a ideia do small-scale evaluation, visto que segundo o trabalho do Wohlin o que fizemos nao é um estudo de caso, nem um experimento. 

% % 5 steps Runeson (referencia/exemplo)
% % We follow the five main steps for conducting case studies
% % proposed by [21]: Design, preparation, collecting data, analysis
% % and reporting.

% % TODO: Explicar o Precison/Recall nessa seção e não nos Resultados -- %Bianca: Eu coloquei uma versao de definicao, veja o que acha. 

% In this Section, we present the key aspects of the study design. In order to evaluate our proposition, we performed a small-scale evaluation \cite{Wohlin2022cs}. According to the smell indicator proposed by Wohlin \& Rainer \cite{Wohlin2022cs}, the correct label for our evaluation is small-scale evaluation instead of a case study. In order to guide and report our study design, we divided our study design into two main parts: (i) Data Collection and (ii) Design \& Execution. 

% In Section \ref{subsec:data} we describe the  data collection process used in our small-scale evaluation to train and test our ML models. In Section \ref{subsec:studydesing} we detail our proposed solution developed to train and configure the investigated ML models. 

% \subsection{Data Collection}
% \label{subsec:data}

% \begin{figure*} [ht]
%     \centering
%     \includegraphics[width=400pt]{pictures/latest/fig03-data-acquisition-v2.pdf}
%     \caption{Data collection process}
%     \label{fig:fig-data-selection}
% \end{figure*}

% We used as instrument of our small-scale evaluation an ongoing SLR update of \cite{Wohlin2022} conducted by the same authors of this replication (team assessment). We chose this ongoing SLR update since the inclusion and exclusion of new studies were conducted based on individual assessments and the consensus of three experienced SLR researchers by analysing title, abstract, keywords and then full-text of the studies manually, allowing us to have confidence in this data for building reliable training and testing sets.

% The team assessment provided us all the studies they analyzed during the SLR update (.bib files), a total of 591 references, of which 39 were included and 552 were excluded for the update. We used these studies to form our testing set for our ML models, we filtered the studies to consider only first studies in English with a valid abstract. At the end, we used 551 studies in our testing set, of which 38 were included by the team assessment and 513 were excluded for the update.

% To train our ML models, we used a training set with 128 studies, of which 45 studies were included and 83 were excluded. The 45 studies used to train our models with what should be included were the same studies included in the original SLR. Since the team assessment did not list the studies that were excluded during the study selection phase of the original SLR, we performed a backward snowballing on the original references to obtain the 83 studies used to train our models with what should be excluded. Figure \ref{fig:fig-data-selection} summarizes this process.

% \subsection{Design \& Execution}
% \label{subsec:studydesing}

% We developed a pipeline with the following steps to automate the study selection process of an SLR update by using ML and answer our research questions. Our pipeline is illustrated in Figure \ref{fig:fig-study-design}. 

% \begin{figure*} [hb]
%     \centering
%     \includegraphics[width=1.0\linewidth]{pictures/latest/fig-pipeline-details-v2.pdf}
%     \caption{Study design pipeline}
%     \label{fig:fig-study-design}
% \end{figure*}

% In summary, our pipeline process a set of .bib files containing the list of studies to train the ML models and the list of studies to be analyzed. After completing its execution, it returns a report file in .xlsx format informing which studies should be included and excluded, as well as metrics about the predictions made by the ML model and the configuration that was used to run its execution.
% The pipeline must receive four different .bib files as input, one file containing the list of studies that should be excluded and one file containing the list of studies that should be included for each set (training and testing). In case there are any errors in the input files, the pipeline will stop its execution and will inform which entry was associated to each error as well as the type of error. Currently, our pipeline doesn't support automatic error resolution for invalid .bib files, they need to be manually fixed by the user.

% As shown in Figure~\ref{fig:fig-study-design} we firstly validated the .bib files of our testing and training sets to ensure completeness of the set avoiding duplicated entries or missing keys. Each study entry must have a title, the year of publication, an abstract text and a list of authors. 
% Secondly, we to applied text filtering techniques with Natural Language Processing (NLP) \cite{NLTK}, such as Lemmatization and Tokenization, to remove irrelevant characters from the texts. Thirdly, we applied Text Vectorization on the filtered texts using  Term-frequency/Inverse-Document-Frequency (TF/IDF), a technique that transforms text data into a numerical matrix of features. Fourthly, we used statistical methods to compute and select the most relevant features. In the fifth step, we trained and tuned our ML Models using our training set. Finally, in the last step, we used our ML Models to predict which studies of our testing set should be included and excluded and compared the results of each one and the agreement level in comparison with the team assessment.

% Additionally, an optional .env file can be passed as input to our pipeline, this file allows some steps in our pipeline to use a specific configuration, such as choosing the configuration of the Feature Selection (FS) method to compute the features, as well as the number of features to be selected in step four, and choosing the configuration for the ML models regarding which algorithm to be used, or which metric should be targeted when tuning the model as well as the type of cross validation to be performed, in step five. All parameters that can be configured are also illustrated in Figure~\ref{fig:fig-study-design}.

% % Based on the work of Napoleão \textit{et al.} \cite{Napoleao2021} which indicates the most used ML classifiers for assisting the selection of studies of SLRs and on the contributions of Pintas \textit{et al.} \cite{pintas2021feature} that evaluated the most adopted ML classifiers and Feature Selection (FS) techniques for text classification, we chose to evaluate two of them: Support Vector Machines (SVM) and Random Forest (RF).

% Based on the promising results achieved using SVM in the work of Watanabe \textit{et al.} \cite{Watanabe20} and the work of Napoleão \textit{et al.} \cite{Napoleao2021}, which highlighted SVM as one of the most used ML classifiers for assisting the selection of studies in SLRs, we decided to evaluate SVM in our work. Additionally, considering the work of Pintas \textit{et al.} \cite{pintas2021feature}, which evaluated the most adopted ML classifiers and Feature Selection (FS) techniques for text classification and concluded that the five most used classifiers are SVM, NB, KNN, DT, and RF, we performed initial tests using these classifiers. Our first tests showed that SVM and RF were achieving better results than the others. Therefore, we decided to focus our evaluation on these two classifiers: Support Vector Machines (SVM) and Random Forest (RF).

% % We experimented multiple configurations of our pipeline and evaluated different configurations for Feature Selection and for training and tuning of our ML classifiers. To select the best features, we tested different statistical methods such as Chi-squared (Chi2) \cite{Chi2}, Pearson Correlation \cite{pearson_r} and Analysis of Variance (Anova-F) \cite{ANOVA}. We tested different techniques to tune our ML classifiers such as K-fold cross-validation, Times-Series cross-validations and hyperparameter tuning with GirdSearch \cite{GridSearch}.

% We experimented multiple configurations of our pipeline and evaluated different configurations for FS and for training and tuning of our ML classifiers. During step four to compute the best features, we tested different statistical methods such as Chi-squared (Chi2) \cite{Chi2}, Pearson Correlation \cite{pearson_r} and Analysis of Variance (Anova-F) \cite{ANOVA} as well as a different range of features. We also tested different techniques to tune our ML classifiers such as K-fold cross-validation, Time-Series cross-validations and hyperparameter tuning with GirdSearch \cite{GridSearch}.

% For each evaluation, we executed the pipeline from start to finish in a clean environment using one statistical method at a time. To avoid introducing bias, the feature selection step was conducted solely based on the training set texts. Once the best features were identified in the training set, the same feature set was applied to the testing set to ensure consistency.
% To prevent overfitting, our machine learning classifiers were trained using a single type of cross-validation in each evaluation. Specifically, when utilizing GridSearch for parameter tuning, we didn't perform any other cross-validation technique, as GridSearch inherently includes cross-validation for measuring the most efficient parameter configuration. We chose this approach to maintain evaluation coherence and rigor.


% %--------------------------------------------------------
% \section{Results}
% \label{sec:results}

% In this section, we present the results of our study, following the research questions presented earlier in Section \ref{sec:researchissues}. To answer each question, we analyzed the results of the ML Models and then compared them with the results obtained manually by the SLR update authors under evaluation. 

% To answer questions RQ1 and RQ2, we reproduced the same evaluation performed by SLR update authors during the selection of studies for the SLR update by using our classifiers to predict  which studies should be included and excluded. For RQ1 we configured our model maximizing F-score and for RQ2 we configured our model maximizing Recall. Then we performed the agreement and similarity analysis using the predictions made by our model used in RQ1.

% Precision indicates how accurate the positive predictions made by the model are, while Recall indicates how well the model captures all the actual positive instances. For Precision, values closer to 1 indicate a lower number of False Positives (FP) results and values closer to 0 indicate a higher number of FP. And for Recall, values closer to 1 indicate a lower number of False Negatives (FN) and values closer to 0 indicate a higher number of FN.
% % Jogar para dentro da secao das RQs se for util

% We conducted our evaluation by varying the number of best features to be considered in each execution. After applying Text Filtering and Text Vectorization techniques, presented in steps three and four of our pipeline, our training set comprised a total of 23630 features, in contrast to our testing set, which comprised 119560 features. Given that the number of features in our testing set was more than five times greater than our training set, maximizing the number of best features in our training set was crucial to the performance of our ML models.

% We identified the range with the most relevant features in our training set as 900 to 1500 features, which was the range used in most of our evaluations. Notably, the best results, both in terms of F-score (RQ1) and Recall (RQ2), were consistently achieved with experiments that selected the 1200 best features.
% % Mover para RQ3

% % In the original table provided by the team assessment, besides what studies were included and excluded for the update, it also had the opinion of each reviewer about each study during the study selection process they performed. The reviewers could express their opinions about each study in three different level of certainty: 0 -- certain that the study should be excluded, 1 -- uncertain if the study should be excluded or included and 2 -- certain that the study should be included. We used this information to answer RQ3 and perform the \textit{Kappa} analysis.
% The document provided by the team assessment contained the final result of each study (if they were included or excluded), and also had the opinion of each reviewer about each study during the study selection process they performed. The reviewers could express their opinions in three different levels of certainty: 0 – certain that the study should be excluded, 1 – uncertain if the study should be excluded or included and 2 – certain that the study should be included. We used this information to answer RQ3 and perform the agreement and similarity analysis. Table~\ref{tab:slr-update-results-sample} illustrates the format of this document. 
% % TODO: Add table

% \begin{table}[!h]
% \caption{Example of the document format provided by the assessment team.}
% \tiny
% \begin{center}
% \begin{tabular}{ m{1.8cm} m{0.95cm} m{0.95cm} m{0.95cm} m{0.95cm} } 
%  \hline 
%   \textbf{Study} & \textbf{Final Result} & \textbf{R1} & \textbf{R2} & \textbf{R3} \\
%  First Study & 1 & 2 & 1 & 2 \\ 
%  Second Study & 0 & 2 & 0 & 0 \\ 
%  Third Study & 1 & 2 & 1 & 0 \\ 
%  Fourth Study & 0 & 0 & 2 & 1 \\ 
%  Fifth Study & 0 & 0 & 0 & 0 \\
%  \end{tabular}
% \end{center}
%  \label{tab:slr-update-results-sample}
% \end{table}


% We displayed the complete information of our results for the best configurations we found for RQ1 and RQ2, as well as all the other tests executions in an \textit{Appendix document}\footnote{https://zenodo.org/records/11021614} available online.

% \subsection{RQ1: \textit{How effective are ML Models in selecting studies for SLR updates?}}
% \label{results:RQ1}

% % TODO: Descrever os algoritmos de ML usados previamente na seção IV (Methodology)
% % To answer this question, during the ML classifiers tuning step, we trained our classifiers with GridSearch focusing on maximizing the F-score. Our best result was obtained by RF with a Precision of 0.22, Recall of 0.63 and F-score of 0.33 using the Anova-F statistical method. Figure \ref{fig:fig-rf-distribution} illustrates the distribution of the predictions' probabilities made by RF with this configuration. As we can see, its distribution is closer to the behavior of the selection studies task of SLRs and SLR updates, where most of the studies are excluded and not included.
% % % Colocar tabela com todos os resultados (é possível ver que o melhor resultado foi obtido com RF usando Anova-F...)
% To answer this question, during the ML models tuning step, we trained our classifiers with GridSearch focusing on maximizing the F-score. Our best result was obtained by RF with a Precision of 0.22, Recall of 0.63 and F-score of 0.33 using the Anova-F statistical method, with 1200 features. We used a default threshold of 0.5 to consider which studies should be included and excluded by our ML models. The parameters tested and selected by GridSearch for this configuration can be found in this document\footnote{https://zenodo.org/api/records/11021614/draft/files/RQ1-RQ3-best-configuration-RF.csv/content}. 
% And the table containing all the predictions made by our ML model for each study for this question can be seen in this document\footnote{https://zenodo.org/api/records/11019279/draft/files/RQ1-RF-predictions.csv/content}.

% % TODO: Add tables with config for rq1 and rq2

% Figure \ref{fig:fig-rf-distribution} illustrates the distribution of the predictions' scores made by RF with this configuration. In order to calculate each of these metrics, we compared our ML models' predictions with the final results only, obtained after the agreement criteria was applied by the team assessment, which is illustrated by the first column of Table~\ref{tab:slr-update-results-sample}.

% \begin{figure} [!h]
%     \centering
%     \includegraphics[width=1\linewidth]{pictures/latest/RF_test_v4-gridsearch_pearson_fs-k1200_bins10_v1.pdf}
%     \caption{RF Predictions Distribution}
%     \label{fig:fig-rf-distribution}
% \end{figure}


% % < Aumentar fontes das legendas e valores dos eixos X e Y de todos os graficos >


% \subsection{RQ2: \textit{How much effort can ML Models reduce during
% the study selection activity for SLR updates?}}
% \label{results:RQ2}

% % To answer this question, we tuned the ML Models with  the intention of maximizing the Recall. Since the purpose of this question was to evaluate how much human effort could be reduced by the use of ML Models during the selection of studies, we wanted to mitigate the chances of a false negative (FN) result, so the reviewers could simply ignore the studies excluded by the ML Model without worrying about losing a relevant study.

% % As demonstrated in Figure \ref{fig:fig-svm-distribution}, our best result was obtained by the SVM algorithm with a Precision of 0.10, Recall of 1.0 and F-score of 0.19 using the Pearson Correlation statistical method, with 1200 features. We used a default threshold of 0.5 to consider which studies should be included and excluded by our algorithms. 

% % According to Table \ref{tab:effort_reduction}, by maximizing the Recall, SVM was able to exclude a total of 187 studies, which represents 33.9\% of the total amount of studies in our testing set. By increasing the threshold, we are able to see that we can reduce the human effort even more at the risk of having more FN results. For a threshold range greater than 0.5 until 0.75, only one false negative was found. Notably, this FN result was one of the few cases where the team assessment had a lot of disparity. Initially, considering only the analysis of the reviewers individually (before they discussed it with each other), the reviewer R1 voted 2, R2 voted 1, and R3 voted 0.

% To answer this question, we tuned the ML models with the intention of maximizing the Recall. Since the purpose of this question was to evaluate how much human effort could be reduced by the use of ML Models during the selection of studies, we wanted to mitigate the chances of a false negative (FN) result, so the reviewers could simply ignore the studies excluded by the ML model without worrying about losing a relevant study.

% Our best result was obtained by using the SVM algorithm with a Precision of 0.10, Recall of 1.0 and F-score of 0.19 using the Pearson Correlation statistical method, with 1200 features. We used a default threshold of 0.5 to consider which studies should be included and excluded by our ML models. The parameters tested and selected by GridSearch for this configuration can be found in this document\footnote{https://zenodo.org/api/records/11021614/draft/files/RQ2-best-configuration-SVM.csv/content}. The table containing all the predictions made by our ML model for each study for this question can be seen in this document\footnote{https://zenodo.org/api/records/11019279/draft/files/RQ2-SVM-predictions.csv/content}.  

% \begin{figure} [!h]
%     \centering
%     \includegraphics[width=1\linewidth]{pictures/latest/fig-rq2-SVM_test_v4-gridsearch_pearson_fs-k1200_bins10.pdf}
%     \caption{SVM Predictions Distribution}
%     \label{fig:fig-svm-distribution}
% \end{figure}


% According to Table \ref{tab:effort_reduction}, by maximizing the Recall, SVM was able to exclude a total of 187 studies, which represents 33.9\% of the total amount of studies in our testing set. By increasing the threshold, we are able to see that we can reduce the human effort even more at the risk of having more FN results. For a threshold range greater than 0.5 until 0.75, only one false negative was found, while the number of TN increased by 87. Notably, this FN result was one of the few cases where the team assessment had a lot of disparity. Considering only the initial analysis of the reviewers individually (before they discussed it with each other), the reviewer R1 voted 2, R2 voted 1, and R3 voted 0. 
% For a threshold range greater than 0.75 until 0.80, when compared to the previous threshold range, the number of FN results increased by 1, while the number of TN increased by 18. 
% Finally, for a threshold range greater than 0.80 until 0.85, when compared to the previous threshold range, the number of FN results increased by 2, while the number of TN increased by 24. 

% As well as RQ1, to answer this question, we compared our ML models' predictions with the final results only, obtained after the agreement criteria was applied by the team assessment, which is illustrated by the first column of Table~\ref{tab:slr-update-results-sample}.


% \begin{table}
% \centering
% \begin{tabular}{|c|c|c|c|c|c|c|}
% \hline
% \textbf{Threshold(\%)} & \textbf{RECALL (\%)} & \textbf{TN} & \textbf{TP} & \textbf{FN} & \textbf{FP} & \textbf{Reduced (\%)} \\
% \hline
% 0.50\% & 100.00\% & 187 & 38 & 0 & 326 & 33.9\% \\ 
% 0.75\% & 97.37\% & 265 & 37 & 1 & 248 & 48.3\% \\  
% 0.80\% & 94.74\% & 283 & 36 & 2 & 267 & 51.7\% \\  
% 0.85\% & 89.49\% & 307 & 34 & 4 & 206 & 56.4\% \\ 
% \hline
% \end{tabular}
% \caption{Tradeoff between effort reduction and number of FN.}
% \label{tab:effort_reduction}
% \end{table}

% \subsection{RQ3: \textit{Can Machine Learning replace a reviewer in the selection of studies for SLRs?}}
% \label{results:RQ3}

% To answer this question, we conducted a two-fold analysis to evaluate both aspects of agreement and similarity, considering not only the comparison between our ML model and single reviewer results but also the final results and the average answer between multiple reviewers.

% The agreement analysis indicates how two or more raters make the same classifications, it measures the concordance of results, we used the Cohen's Kappa coefficient for this. While, the similarity analysis indicates the resemblance between the classifications of two or more raters, it measures how close the results are, even if they are not exactly the same, we used the Euclidean Distance for this. 

% Firstly, we looked at the agreement and similarity levels between the reviewers among the team assessment, and analyzed the information provided by the team assessment regarding each reviewer's vote before applying the agreement criteria. Table~\ref{tab:reviewers-agreement-table} shows the Kappa values between reviewers, and Table~\ref{tab:reviewers-similarity-table} shows the Euclidean Distance between reviewers.

% \begin{table}
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{} & \textbf{R1} & \textbf{R2} & \textbf{R3} \\
% \hline
% \textbf{R1} & 1 & 0.47 & 0.35 \\
% \textbf{R2} & 0.47 & 1 & 0.43 \\
% \textbf{R3} & 0.35 & 0.43 & 1 \\
% \hline
% \end{tabular}
% \caption{Agreement Among the Assessment Team.}
% \label{tab:reviewers-agreement-table}
% \end{table}

% \begin{table}
% \centering
% \begin{tabular}{|c|c|c|c|}
% \hline
% \textbf{} & \textbf{R1} & \textbf{R2} & \textbf{R3} \\
% \hline
% \textbf{R1} & 0 & 13.0 & 14.04 \\
% \textbf{R2} & 13.0 & 0 & 11.22 \\
% \textbf{R3} & 14.04 & 11.22 & 0 \\
% \hline
% \end{tabular}
% \caption{Similarity Among the Assessment Team.}
% \label{tab:reviewers-similarity-table}
% \end{table}

% We considered the following rage of Cohen's Kappa coefficient values as the following agreement levels \cite{carletta1996assessing}:
% \begin{itemize}
%     \item from 0.00 to 0.20: Poor Agreement
%     \item from 0.21 to 0.40: Fair Agreement
%     \item from 0.41 to 0.60: Moderate Agreement
%     \item from 0.61 to 0.80: Substantial Agreement
%     \item from 0.81 to 1.00: Almost Perfect Agreement
% \end{itemize}

% The highest agreement was achieved between reviewer 1 (R1) and reviewer 2 (R2) with 0.47 of agreement, which can be considered a Moderate Agreement. Whilst, the lowest agreement was between the reviewer 3 (R3) and R1 with 0.35 of agreement, which can be considered a Fair Agreement. Since the similarity is inversely proportional to the Euclidean Distance, we can notice that R2 and R3 had the most similar opinions with the smallest distance of 11.22 in comparison to R1 and R3 with the highest distance of 14.04.

% To compare our ML models with the reviewers, we used the same RF Model used to answer RQ1, since it had the better f-score, it made more realistic predictions in contrast to our SVM model used to answer RQ2, which was configured to maximize recall. 
% We normalized our RF results in three ranges to represent the same three categories used by the reviewers during the step to apply the inclusion and exclusion criteria. Figure~\ref{fig:fig-reviewers-votes-distribution} shows the distribution of votes by each reviewer in each category. We decided the threshold for the exclusion of studies should be greater than the rest, since the frequency of occurrence of votes "0" was clearly the highest for the assessment team. And we decided the range of "uncertainty" should be the smallest, since it was the vote less frequent. Considering that, we decided to use the following range of predictions score to normalize our RF model. 
% \begin{itemize}
%     \item from 0.00 to 0.50: should be excluded
%     \item from 0.51 to 0.60: uncertain
%     \item from 0.61 to 1.00: should be included
% \end{itemize}

% Figure~\ref{fig:fig-rf-normalized-distribution} illustrates the normalized distribution of our ML model used in RQ1.

% \begin{figure} [!h]
%     \centering
%     \includegraphics[width=1\linewidth]{pictures/latest/fig-rq3-reviewers-votes-distribution.pdf}
%     \caption{Reviewers votes distribution.}
%     \label{fig:fig-reviewers-votes-distribution}
% \end{figure}

% \begin{figure} [!h]
%     \centering
%     \includegraphics[width=1\linewidth]{pictures/latest/rf_test_v4_normlized_agreement.pdf}
%     \caption{RQ3: RF Predictions Distribution considering "uncertain" range.}
%     \label{fig:fig-rf-normalized-distribution}
% \end{figure}



% We used the normalized results of our RF model to evaluate its agreement and similarity with the reviewers. Table~\ref{tab:rf-kappa-and-euclidean-table} illustrates the Cohen's Kappa coefficient and the Euclidean Distance between our RF model and each reviewer.

% % After we converted the probabilities given by our RF model used in RQ1 into the three categories, we compared the normalized results of the RF model with the results of each reviewer and calculated the \textit{Cohen's Kappa Coefficient} and the Euclidean Distance between them. The agreement level between the RF model and each reviewer was: RF | R1 = 0.27, RF | R2 = 0.37, RF | R3 = 0.30. The Euclidean Distance between the RF model and each reviewer was: RF | R1 = 17.89, RF | R2 = 16.76, RF | R3 = 17.52. Table~\ref{tab:rf-kappa-and-euclidean-table} illustrates both results.

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
% \textbf{Comparison} & \textbf{Kappa} & \textbf{Euclidean Distance} \\ \hline
% R1 vs RF & 0.27 & 17.89 \\ \hline
% R2 vs RF & 0.37 & 16.76 \\ \hline
% R3 vs RF & 0.30 & 17.52 \\ \hline
% \end{tabular}
% \caption{Cohen's Kappa coefficient and Euclidean Distance between RF model and Reviewers}
% \label{tab:rf-kappa-and-euclidean-table}
% \end{table}

% Then, we used the Euclidean Distance to evaluate the similarity between the Final Results (FR) and our RF model and reviewers, when compared individually and collectively. In order to evaluate the answers from the FR accurately, we normalize the binary answers in FR to considered that the included studies a value of 2 and excluded studies had a value 0 in FR. So if a reviewer voted to include a study with certainty and the study was indeed included, the distance between these two points would be zero.

% We evaluated the Euclidean Distance (ED) by three different perspectives, as follows:
% \begin{itemize}
%     \item Similarity between single answers and FR: 
%     \[
%     \textit{ED}(i, FR) \text{ where } i \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
%     \]
%     \item Similarity between pairs and FR:
%     \[
%     \textit{ED}\left(\frac{i + j}{2}, \text{FR}\right) \text{ where } i \neq j \text{ and } i, j \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
%     \]
%     \item Similarity between groups and FR:
%     \[
%     \textit{ED}\left(\frac{i + j + k}{3}, \text{FR}\right) \text{ where } i \neq j \neq k \text{ and } i, j, k \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
%     \]
% \end{itemize}

% Table~\ref{tab:similarity-distance-FR-table} shows the ED measured in each case. As we can see, the smallest distance in comparison to the FR in each case was given by: R2 with ED = 9.95, pair(R2,R3) with ED = 8.86 and team assessment with ED = 8.17.  

% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|}
% \hline
% \textbf{Comparison} & \textbf{Euclidean Distance} \\ \hline
% R1 vs FR & 12.00 \\ \hline
% R2 vs FR & \textbf{9.95} \\ \hline
% R3 vs FR & 11.00 \\ \hline
% RF vs FR & 17.94 \\ \hline
% \hline
% avg(R1,R2) vs FR & 8.90 \\ \hline
% avg(R1,R3) vs FR & 9.12 \\ \hline
% avg(R2,R3) vs FR & \textbf{8.86} \\ \hline
% avg(R1,RF) vs FR & 12.37 \\ \hline
% avg(R2,RF) vs FR & 11.84 \\ \hline
% avg(R3,RF) vs FR & 12.03 \\ \hline
% \hline
% avg(R1,R2,R3) vs FR & \textbf{8.17 }\\ \hline
% avg(RF,R2,R3) vs FR & 10.06 \\ \hline
% avg(R1,RF,R3) vs FR & 10.20 \\ \hline
% avg(R1,R2,RF) vs FR & 10.15 \\ \hline
% \end{tabular}
% \caption{Euclidean Distance Analysis considering FR}
% \label{tab:similarity-distance-FR-table}
% \end{table}

% %--------------------------------------------------------
% \section{Discussion}
% \label{sec:discussion}

% \subsection{\textit{Research Question 1}}
% \label{discussion:RQ1}
% Regarding RQ1 - “How effective are ML models in selecting studies for SLR updates?”, based on our results, we concluded that the best configuration to maximize the F-score of our ML models was using RF with Anova-F. Not only that, but in almost all of our tests configurations, RF outperformed SVM in terms of F-score with exception for one test, where we selected only 900 best features and used the Pearson Correlation method to compute the best features. We also noticed that for most cases, the Anova-F improved the F-score rating at the cost of lowering its Recall. 
% However, even noticing that the RF was more successful in this task than SVM and considering its best result, its F-score was still not good enough to be considered to automate the process of selecting studies for the SLR update.


% % TODO: O watanabe coloca tabelas com o resultados dos dois algoritmos DT e SVM variando o numero de features e na discussion compara o seus desempenhos...

% \subsection{\textit{Research Question 2}}
% \label{discussion:RQ2}
% Regarding RQ2 - “How much effort can ML models reduce during the study selection activity for SLR updates?”, our results showed that the best configuration to maximize the Recall of our ML models was using SVM with Pearson Correlation. On one hand, it's possible to see that in all of our tests, SVM had higher Recall marks than RF. Particularly, when used with Pearson Correlation to select the best features, it had the highest marks for Recall in most cases (although even in executions using Anova-F it still reached a high Recall of 0.97 for some tests). On the other hand, its F-score and Precision marks were very low. However, even if a big number of FP studies remained after that, we believe that our SVM model showed potential for reducing human effort during the study selection task of SLR updates, by automatically excluding part of the irrelevant studies.

% \subsection{\textit{Research Question 3}}
% \label{discussion:RQ3}
% Regarding RQ3 - “Can Machine Learning Replace a Reviewer in the Selection of Studies for Systematic Literature Reviews?” our evaluation showed that the best result was achieved by the RF classifier with the same configuration used for RQ1. As we can see in Table~\ref{tab:reviewers-agreement-table}, the strongest level of agreement was between R1 and R2 with a score of 0.47 for Cohen's Kappa Coefficient, followed by an agreement of 0.43 between R2 and R3, and followed by the weakest agreement between R1 and R3 with a score of 0.35. In this case, two pairs of reviewers had a moderate level of agreement and one pair had only a fair level of agreement. 

% On the other hand, when compared with our RF classifier, it had its strongest agreement of 0.37 with R2, followed by an agreement of 0.30 with R3 and an agreement of 0.27 with R1, all considered as fair level of agreement. Even though the pair RF classifier and R2 had a stronger agreement than the pair R1 and R3, both still had the same agreement level (fair agreement), also the R2 had a Moderate Agreement with both reviewers, while it had only a fair agreement with the RF classifier. Also, when comparing the agreement of RF with R3 and its respective pairs (R3|R1 and R3|R2), it had a weaker agreement than both, the same was true when comparing RF with R2 and its respective pairs. 

% Looking at Table~\ref{tab:reviewers-similarity-table}, we can see that R2 and R3 had the most similar answers, with an ED of 11.22 between them. As showed in Table~\ref{tab:similarity-distance-FR-table} R2 also had the smallest distance from FR when compared individually (with an ED of 9.95), as expected, the pair R2 and R3 had the strongest similarity with FR (with an ED of 8.86) in contrast to the other pairs. Finally, the closest distance was given by the team assessment with an ED of 8.17, which is expected since the FR was generated from their answers. It's possible to see that our RF model had the highest distances in all comparisons and even though looking at the distance to FR obtained when working together with R1 didn't cause much negative impact (ED(R1,RF) = 12.37 vs ED(R1) = 12.00) as the rest, it shows that our ML model didn't help any of the reviewers to get closer to the FR. 

% Therefore, we concluded that our supervised ML Models are not ready to replace a reviewer during the selection of studies for SLR updates.

% It is worth mentioning that the similarity and agreement levels between our classifier and the team assessment could increase or decrease depending on how we configure the thresholds to normalize the probabilities given by our ML classifier into the three categories used by the team assessment. We noticed that reducing the range to consider a vote as uncertain would increase the level of agreement with all members of the assessment team. But in most cases, our classifier still had only a fair level of agreement, so our conclusion was the same.


% \section{Threats to Validity}
% \label{sec:threats}

% In the following, we enumerate the main threats to the validity of our study.

% \textbf{Construct Validity.}
% Our evaluation results might have been affected by the choice of ML algorithms. Other algorithms could have been explored in our study and can be considered as part of future work.

% \textbf{Internal Validity.}
% Our training dataset comprised only studies including in the SLR replication \cite{Wohlin2022} (training included) and those obtained through backward snowballing (training excluded). We deliberately excluded studies not in English or those categorized as Ph.D. dissertations or book chapters from our testing set, same approach adopted by the SLR replication. This focused approach aimed to provide our models with only relevant and essential data. Another potential threat is that during manual process performed by the team assessment, the authors could end up reading other sections of the studies besides the title and abstracts when they are not completely sure if the study should be included or excluded just by reading its abstract. This is a possible advantage manual process could have over our models that consider only content from title and abstract. 

% \textbf{External Validity.} The dataset used in our analysis might not represent the diversity of SLR Updates in SE. Similar analyses could have been conducted based on other SLRs to improve the generalizability of our results. However, replicating our results on other SLRs to strengthen external validity would require significant effort. Moreover, it is challenging to acquire a reliable and detailed SLR dataset for SLRs updates that could be considered in our evaluation. 

% \textbf{Reliability.} One limitation of our study is associated with the dataset used in our evaluation and the possibility of sample bias. The data used in our evaluation was acquired from the same authors who performed the SLR replication, also through a rigorous analysis process. In addition, to improve the reliability of our results, our ML models and the small-scale evaluation dataset are openly available. 

% %--------------------------------------------------------
% \section{Conclusion}
% \label{sec:conclusion}
% % In this paper we have presented an evaluation using our ML models to replicate the process of an ongoing SLR update performed by three experienced researchers, to evaluate it as a supporting tool for the studies’ selection task.  We compared the results of our ML models with the members of the team assessment. 

% % We concluded that our ML models are not ready to automatically select studies for the study selection task, and may also not be used to replace an additional reviewer. However, there is potential for reducing human effort during the study selection task of SLR updates, by automatically excluding part of the irrelevant studies.

% % Our next steps is to investigate even further the use of ML models with different configurations and text classification techniques (variar algoritmos, métodos estatísticos de FS, técnicas de TF-IDF) with the same goals. In addition, we intend to explore ML models with different datasets to evaluate their performance. We also believe we could achieve promising results by adapting our pipeline to have multiple iterations, such as: first we could only exclude irrelevant studies using a ML model with a configuration maximized by the recall, than we could try to predict the most relevant in the remaining results using another ML model maximized by the f-score.

% This study advances the application of supervised ML models as a supporting tool for researchers during SLRs updates, by developing and testing a comprehensive supervised ML-based pipeline to automate the study selection process. We have demonstrated through our tests, using realistic data to build our datasets, that while our ML models, have shown promise in reducing human effort to perform the study selection activity by pre-filtering irrelevant studies, they currently lack the precision required to completely automate the selection of studies for SLR updates. We also concluded that they did not achieve a level of similarity and agreement strong enough to be considered sufficient to replace a human reviewer during an SLR update in the study selection phase. Our work also highlights different configurations used for our ML models that correlates to their recall and F-score, providing results that can be useful for further exploration in this area.

% Our next step is to investigate even further the use of ML models with different configurations and text classification techniques with the same goals. In addition, we intend to explore our pipeline using datasets with a different structure to evaluate its performance. Additionally, we intend to perform new tests with the objective of improving our pipeline's automation, for instance, by experimenting to use the ML model configured to maximize the recall to perform an initial filter on the studies and then use another ML model configured to maximize the f-score to make predicts based on the results previously filtered. Also, we believe we could leverage from the work of Pintas \textit{et al.} \cite{pintas2021feature} by performing a deeper analysis of our dataset structure and use the contributions of their work to decide which FS strategies should work the best. Lastly, considering the increase number of studies using LLMs to support SLRs and SLR updates \cite{bolanos2024artificial}, we intend to evaluate the use of LLMs in this context as well and compare with our models.

% \balance
% \bibliographystyle{ACM-Reference-Format}
% \bibliography{acmart}

% % \bibliographystyle{ACM-Reference-Format}
% %\bibliography{bib/base}
% %\printbibliography

% \end{document}
% \endinput

