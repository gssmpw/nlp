\section {Study Design}
\label{sec:methodology}

%Bianca: Aqui estava faltando definir qual o research metodo que a gente utilizou para testar o que foi desenvolvido. Eu utilizei a ideia do small-scale evaluation, visto que segundo o trabalho do Wohlin o que fizemos nao é um estudo de caso, nem um experimento. 

% 5 steps Runeson (referencia/exemplo)
% We follow the five main steps for conducting case studies
% proposed by [21]: Design, preparation, collecting data, analysis
% and reporting.

% TODO: Explicar o Precison/Recall nessa seção e não nos Resultados -- %Bianca: Eu coloquei uma versao de definicao, veja o que acha. 

In this section, we present the study design. To evaluate our research questions, we performed a small-scale evaluation \cite{Wohlin2022cs}, following two main steps: (i) Data Collection and (ii) Design \& Execution. We describe the data collection to train and test our ML models in Section \ref{subsec:data}. In Section \ref{subsec:studydesing}, we detail how we designed and executed our strategy to train and configure the ML models. 

\subsection{Data Collection}
\label{subsec:data}

\begin{figure*} [ht]
    \centering
    \includegraphics[width=380pt]{figures/fig03-data-acquisition-v2.pdf}
    \caption{Data collection process}
    \label{fig:fig-data-selection}
\end{figure*}

We used an ongoing SLR update conducted by the same first three authors of this replication \cite{Wohlin2022} as the instrument of our study. We chose this ongoing SLR update since the inclusion and exclusion of new studies were rigorously conducted based on individual assessments and the consensus of three experienced SLR researchers. First, each researcher screened all the papers, analyzing the title, abstract, and keywords and registering his individual assessment (\textit{i.e.}, we had three assessments for each paper). Then, the full texts of the studies were analyzed, and discussions were held to reach a final consensus on the list of included and excluded papers. Hence, we had the results of the initial screening by each researcher and also the final list of papers to be included and excluded.

We had access to all the studies the team analyzed during the SLR update (.bib files): a total of 591 papers were analyzed for the SLR update, of which 39 were included, and 552 were excluded. We filtered the studies to consider only the studies in English and containing an abstract. In the end, we used 551 studies in our testing set, of which 38 were included by the team assessment and 513 were excluded. We used these studies to form the testing set for our ML models. 

To train our ML models, we used a training set with 128 studies, of which 45 were included and 83 were excluded. The 45 studies used to train our models with what should be included were the same studies included in the original SLR \cite{Wohlin2022}. Since we did not have access to the list of excluded studies during the study selection phase of the original SLR, we performed a backward snowballing \cite{Wohlin14} on the original references to obtain the 83 studies used to train our models with what should be excluded. Figure \ref{fig:fig-data-selection} summarizes this process. The bib files for the included and excluded studies of the training and testing sets are available in our open science repository~\cite{zenodoOpenScience}.

\subsection{Design \& Execution}
\label{subsec:studydesing}

We developed a pipeline with the following steps to automate the study selection process of an SLR update by using ML and answering our research questions. Our pipeline is illustrated in Figure \ref{fig:fig-study-design}. 

\begin{figure*} [ht]
    \centering
    \includegraphics[width=380pt]{figures/fig-pipeline-details-v4.pdf}
    \caption{Study design pipeline}
    \label{fig:fig-study-design}
\end{figure*}

In summary, our pipeline processes a set of .bib files containing the list of studies to train the ML models and the list of studies to be analyzed. After its execution, it returns a report file in .xlsx format with the ML model predictions, informing which studies should be included and excluded, as well as metrics about the ML model predictions and the configuration used.

The pipeline receives four different .bib files as input, one containing the list of studies that should be excluded and one containing the list of studies that should be included for each set (training and testing). In case there are any errors in the input files, the pipeline will stop its execution and will inform which entry was associated with each error as well as the type of error. 

As shown in Figure~\ref{fig:fig-study-design}, we first validated the .bib files of our testing and training sets to ensure the completeness of the set, avoiding duplicated entries or missing keys. Each study entry must have a title, the year of publication, an abstract text, and an author list. 
Secondly, we applied text filtering techniques with Natural Language Processing (NLP) \cite{NLTK}, such as Lemmatization and Tokenization, to remove irrelevant characters. Thirdly, we applied Text Vectorization on the filtered texts using  Term-frequency/Inverse-Document-Frequency (TF/IDF), a technique that transforms text data into a numerical matrix of features. Fourthly, we used statistical methods to compute and select the most relevant features. In the fifth step, we trained and tuned our ML models using our training set. Finally, in the last step, we used our ML models to predict which studies of our testing set should be included and excluded and compared the results with the final list of included and excluded studies.

Additionally, an optional .env file can be passed as input to our pipeline; this file allows some steps in our pipeline to use a specific configuration, such as choosing the configuration of the Feature Selection (FS) method to compute the features, as well as the number of features to be selected in step four, and choosing the configuration for the ML models regarding which algorithm to be used, or which metric should be targeted when tuning the model as well as the type of cross-validation to be performed, in step five. All parameters that can be configured are also shown in Figure~\ref{fig:fig-study-design}.

Building on the work by Napoleão \textit{et al.} \cite{Napoleao2021}, which highlighted Support Vector Machines (SVM) as one of the most commonly used machine learning classifiers for assisting study selection in systematic literature reviews (SLRs), we chose to evaluate SVM in our study. Additionally, inspired by the findings of Pintas \textit{et al.} \cite{pintas2021feature}, who analyzed the most widely adopted ML classifiers and feature selection techniques for text classification, identifying SVM, Naive Bayes, k-Nearest Neighbors, Decision Trees, and Random Forest (RF) as the top five classifiers, we conducted initial tests with these classifiers. Our preliminary results showed that SVM and RF outperformed the other classifiers. Consequently, we focused our evaluation on SVM and RF.

% We experimented multiple configurations of our pipeline and evaluated different configurations for Feature Selection and for training and tuning of our ML classifiers. To select the best features, we tested different statistical methods such as Chi-squared (Chi2) \cite{Chi2}, Pearson Correlation \cite{pearson_r} and Analysis of Variance (Anova-F) \cite{ANOVA}. We tested different techniques to tune our ML classifiers such as K-fold cross-validxation, Times-Series cross-validations and hyperparameter tuning with GirdSearch \cite{GridSearch}.

We experimented with multiple pipeline configurations and evaluated different configurations for FS and training and tuning of our ML classifiers. During step four, to compute the best features, we tested different statistical methods such as Chi-squared (Chi2) \cite{Chi2}, Pearson Correlation \cite{pearson_r} and Analysis of Variance (Anova-F) \cite{ANOVA} as well as different ranges of features. After applying Text Filtering and Text Vectorization techniques, presented in steps three and four of our pipeline, our training set comprised 23,630 features. We identified the range with the most relevant features in our training set as 900 to 1,500 features. Notably, the best results, both in terms of F-score and Recall, were consistently achieved with experiments that selected the 1200 best features.

For each evaluation, we executed the pipeline from start to finish in a clean environment using one statistical method at a time. To avoid data leakage and bias, feature selection was conducted solely based on the training set. Furthermore, we used GridSearch for parameter tuning when creating the model, which inherently includes k-fold cross-validation for measuring the most efficient parameter configuration when developing the model based on the training set. Finally, the trained model was applied to predict the inclusion or exclusion of the unseen (holdout) papers of the testing set. Hence, the hereafter reported results refer not to cross-validations conducted during model creation but to evaluating the tuned model based on the holdout testing set for which three experienced SE researchers had manually and rigorously crafted the information on inclusion and exclusion.

The complete Python code that automates our pipeline is available in our open science repository~\cite{zenodoOpenScience}.