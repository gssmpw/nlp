\section{Background and Related Work}
\label{sec:relatedwork}

An SLR update is a more recent (updated) version of an SLR \cite{Mendes2020}. For the inclusion of new and relevant evidence, one of the initial steps is to conduct the study selection activity which consists of analyzing the studies retrieved from applying the search strategy.

%related work
There have been several efforts in SE towards improvements for SLR updates (e.g. \cite{felizardo16, Garces17, Mendes2020, Wohlin2020}). However, considering the focus of our study on supporting study selection for SLR updates, we highlighted three main related works \cite{Watanabe20, Felizardo14, Napoleao2021}, described in the following.

The work of Watanabe \textit{et al.} \cite{Watanabe20} also evaluated the use of text classification (text mining combined with ML models) to support the study selection activity for SLR updates in SE. They evaluated eight SLRs from different research domains in a cross-validation procedure using Decision Tree (DT) and SVM as ML classification algorithms. The results achieved on average a \textit{F-score} of 0.92, \textit{Recall} of 0.93, and \textit{Precision} of 0.92. These results significantly outperform those of almost any expert human reviewer. However, we could not replicate their results and identified potential issues in the code. For example, feature selection was applied to the entire dataset before splitting it into training and testing sets. This means information from the SLR update may have influenced the model in ways that would not be possible before an actual SLR update. Additionally, the evaluation used time-series cross-validation without keeping the SLR update papers as a fully independent holdout set for testing purposes. While cross-validation is valuable during model development, a robust model evaluation relies on a separate test dataset to avoid data leakage, identify overfitting, and allow a fair assessment. Nevertheless, their work contributes valuable documentation of procedures and artifacts, which can help to enhance our understanding.

%Unlike the approach proposed in \cite{Watanabe20}, our study achieved the best results with SVM and RF ML models using a detailed database of a solid ongoing SLR update conducted by renowned researchers in the field of EBSE. Furthermore, we compare the agreement level of the adopted ML Models with the expert reviewers through \textit{Kappa} analysis \cite{Cohen10, Kitchenham15}.

%While the contributions of Watanabe et al. \cite{Watanabe20} laid a valuable foundation by showing the potential of supervised ML Models in assisting the study selection process for SLR updates, our study takes a different approach to enhance the applicability of such methodologies. Our study used a detailed database of a solid ongoing SLR update conducted by renowned researchers in the field of EBSE, we also used a distinct approach to retrieve the non-selected studies from the original SLR, and we explored different Feature Selection (FS) strategies, ML Models and techniques to train our classifiers. Furthermore, our study's contribution goes beyond evaluating how much effort could be reduced by the ML classifiers during the selection of studies process, we also compared in detail the agreement level of the adopted ML classifiers with the expert reviewers through \textit{kappa} analysis \cite{Kitchenham15}.

Felizardo \textit{et al.} \cite{Felizardo14} propose an automated alternative to support the selection of studies for SLR updates. They propose a tool called \textit{Revis}, which links new evidence with the original SLR's evidence using the K-Nearest Neighbor (KNN) Edges Connection technique. The results showed an increase in the number of studies correctly included compared to the traditional manual approach.

Napoleão \textit{et al.} \cite{minetto2024emerging} presented an automated tool prototype aimed at supporting the search and selection of studies for SLR updates using Machine Learning (ML) algorithms. Their evaluation demonstrated the feasibility of automating snowballing-based search strategies with minimal losses and that machine learning can help reduce the number of papers to be manually analyzed. Their work reinforces the potential of ML-based automation to reduce manual effort in SLR updates while highlighting the importance of conservative thresholds to minimize the risk of missing relevant studies.

More recently, Felizardo \textit{et al.}~\cite{felizardoEtAl2024} evaluated the accuracy (\textit{i.e.}, studies correctly classified) of using ChatGPT–4.0 to support SLR study selection based on the title, abstract, and keywords. Their analysis indicated that it is not advisable to outsource the selection to ChatGPT. However, they mention that it could be valuable as a support tool, aiding researchers to avoid missing evidence when properly used. However, their study did not consider SLR updates or training based on data from the original SLR.

Napoleão \textit{et al.} \cite{Napoleao2021} performed a cross-domain Systematic Mapping (SM) on existing automated support for searching and selecting studies for SLRs and SMs in SE and Medicine. The authors indicated potential ML models that can be adopted to support the study selection activities. They also indicated the most adopted methods (cross-validation during model creation and experimentation for model evaluation) and metrics (\textit{Recall}, \textit{Precision} and \textit{F-measure}) to assess text classification approaches. The choice of the ML models and the assessment metrics and methods for this study considered their findings.


