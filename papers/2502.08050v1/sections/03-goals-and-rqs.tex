\section{Goal and Research Questions}
\label{sec:researchissues}

The goal of this study is to evaluate the adoption of ML models to support the selection of studies for SLR updates. We translated our goal into three different Research Questions (RQs).

    \textbf{RQ1:} \textit{How effective are ML models in selecting studies for SLR updates?}

    To answer this research question, we represent the effectiveness of the ML models in supporting study selection using metrics such as \textit{Recall}, \textit{Precision} and \textit{F-measure} \cite{Napoleao2021, Watanabe20}. Our ML automated analysis considers only the title, abstract, and keywords of the studies. Our ML models were trained with data from the original SLR and asked to select studies for the SLR update. The results were compared with the final results of the included and excluded studies (according to the consensus discussion of the three experienced SE researchers) for the SLR update.
    
    %Our ML automated analysis considers only title and abstract of the studies and the metrics are calculated at first considering the results from the expert reviewers analysis only on the title, abstract and keywords and next, considering also their results from the full-text analysis.

    \textbf{RQ2:} \textit{How much effort can ML models reduce during the study selection activity of SLR updates?}

    For this research question, we calculate the effort reduction by the relation of the number of studies that need to have their title, abstract, and keywords manually analyzed without the support of ML models versus the number of studies to be analyzed after discarding studies that would have a low probability of being included according to the ML model. \textit{I.e.}, we analyzed the percentage of studies that could be safely discarded based on their inclusion probability while keeping a 100\% recall of the included studies. 
   
    \textbf{RQ3:} \textit{How does the support of ML in the selection of studies compare to the support of an additional human reviewer?}
    % We compared the agreement level of the ML Model with the highest \textit{F-score} value, supporting a single reviewer with the agreement level of each pair of reviewers, by calculating their Cohen's \textit{Kappa} coefficient \cite{Cohen10, Kitchenham15}.

    In this research question, we assessed how a pair of a human and an ``ML model reviewer" would compare against pairs of human reviewers in determining the list of studies to be included. Therefore, to improve the chances of providing good support, we used the ML model with the highest \textit{F-score}. In the initial screening, each of the three SE researchers had assessed each paper on a scale from zero to two (0 - exclude, 1 - unsure, 2 - include). We adjusted the outcome of the ML model (probabilities for inclusion) to that same scale of integers. Thereafter, we calculated the aggregated outcome for the list of papers for each possible pair of reviewers using the average score of the pair members. By using the average, we fairly hypothesize that, when working in pairs, each member of the pair would equally influence inclusion or exclusion. Finally, we compared the aggregated outcome of each pair with the final results using the Euclidean distance to understand how far each pair was from the oracle.

    % \end{itemize}

    %---- Here I did not defined Kappa. I think we can defined it in the methodology section.

    %Kappa analysis ->  Euclidean Distance
    %agremeent level do algoritmo com os revisores - titulo, abstract and keywords
    % future assessement + Hip√≥tese :  Machine learning can replace a reviewer in a SLR update?