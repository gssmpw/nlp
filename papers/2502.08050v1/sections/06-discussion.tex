\section{Discussion}
\label{sec:discussion}

The results provide valuable insights into the limitations of machine learning (ML) models to support systematic literature review (SLR) updates. In this discussion, we interpret these results in light of the research questions, contextualize their implications, and outline the trade-offs associated with applying ML models in this domain.

\subsection{Effectiveness of ML Models for SLR Study Selection (RQ1)}

The results for RQ1 indicate that our best-performing model, Random Forest (RF), achieved a modest balance between precision and recall with an F-score of 0.33 at the default threshold of 0.5. This result suggests that while the ML model was able to identify some relevant studies, its overall ability to precisely distinguish between relevant and irrelevant studies was limited. Adjusting the threshold improved the F-score to 0.41, highlighting the sensitivity of the model’s performance to the chosen threshold. However, this improvement came at the cost of increasing false negatives (FNs), potentially missing valuable studies. We interpret the RF model’s performance as indicating that ML may assist in informally identifying a subset of relevant studies but is not yet reliable for the selection of studies for SLR updates.

\subsection{Effort Reduction through ML Models (RQ2)}

In answering RQ2, we focused on maximizing recall to avoid FNs. In our investigations, the SVM model was more suitable for focusing on achieving a high recall and demonstrating some potential for reducing human screening efforts. Results demonstrated that with a recall of 100\%, the SVM model could exclude 33.9\% of studies from the review process without missing any relevant studies. This reduction represents a significant decrease in the manual workload, suggesting ML’s potential to assist researchers with the initial screening stage. However, to achieve this high recall, the model produced a high rate of false positives (FPs), still requiring significant human review effort to discard many non-relevant studies.

As shown in Table \ref{tab:effort_reduction}, gradually increasing the inclusion probability threshold reduced the number of FPs at the cost of a minor drop in recall. For instance, at a threshold of 0.75, the model achieved a recall of 97.37\%, with a reduction of 48.3\% in the number of studies needing review. We interpret this result as indicating that, while ML can reduce screening efforts, care must be taken when applying thresholds to avoid introducing a risk of overlooking critical studies.

\subsection{Supporting Human Reviewers (RQ3)}

For RQ3, we evaluated the support ML could provide compared to that of an additional human reviewer. When we treated the RF model as an additional reviewer and calculated Euclidean Distance (ED) to assess alignment with the final inclusion decision, individual human reviewers outperformed the RF model. Furthermore, pairs of human reviewers clearly outperformed human-ML pairs, suggesting that human-only review teams achieve more accurate results.

This finding reinforces the challenges ML models face in fully replicating the nuanced judgment of human reviewers. Hence, ML can not replace additional human reviewers, and ML assistance is not a valid argument for quality in the selection process. Pairs of human reviewers are still highly recommended for selecting studies in SLR updates.