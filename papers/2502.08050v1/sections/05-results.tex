\section{Results}
\label{sec:results}

To calculate the machine learning model results and address our research questions, we used two primary data sources: the final inclusion or exclusion decision for each paper in the SLR update (serving as the oracle) to answer RQ1 and RQ2, and the initial screening results from each of the three researchers to answer RQ3. Table \ref{tab:slr-update-results-sample} provides an overview of the data provided by the assessment team for 551 papers (38 included and 513 excluded). In this table, a ``Final Result" value of 1 indicates inclusion, while 0 indicates exclusion. Additionally, each reviewer recorded their initial screening assessment on a scale from zero to two (0 - exclude, 1 - unsure, 2 - include).

\begin{table}[!ht]
\scriptsize
\caption{Overview of the data provided by the assessment team.}
\begin{center}
\begin{tabular}{ m{1.8cm} m{1.5cm} m{1cm} m{1cm} m{1cm} } 
 \hline 
  \textbf{Study} & \textbf{Final Result} & \textbf{R1} & \textbf{R2} & \textbf{R3} \\
 First Study & 1 & 2 & 1 & 2 \\ 
 Second Study & 0 & 2 & 0 & 0 \\ 
 Third Study & 1 & 2 & 1 & 0 \\ 
 Fourth Study & 0 & 0 & 2 & 1 \\ 
 Fifth Study & 0 & 0 & 0 & 0 \\
 \end{tabular}
\end{center}
 \label{tab:slr-update-results-sample}
\end{table}

The complete information of our results for the best configurations we found for RQ1 and RQ2, as well as all the other test executions, are available in our open science repository~\cite{zenodoOpenScience}. 

\subsection{RQ1: \textit{How effective are ML models in selecting studies for SLR updates?}}
\label{results:RQ1}

To answer this question, during the ML models tuning step, we trained our classifiers with GridSearch, focusing on maximizing the F-score. We got the best result using RF with a Precision of 0.22, Recall of 0.63, and F-score of 0.33 using the Anova-F statistical method, with 1,200 features. We used a default inclusion probability threshold of 0.5 to consider which studies should be included and excluded by our ML models. Figure \ref{fig:fig-rf-distribution} illustrates the distribution of the ML predictions' inclusion probabilities made by RF with this configuration. The parameters tested and selected by GridSearch, as well as the predictions made by our ML model using these parameters, can be found in our online repository.



\begin{figure}[ht]
    \centering
    \includegraphics[width=6cm]{figures/RF_test_v4-gridsearch_pearson_fs-k1200_bins10_v1.pdf}
    \caption{RF inclusion probability distribution}
    \label{fig:fig-rf-distribution}
\end{figure}

% NOTE: TO REDUCE IMAGE SIZE
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=0.35\textwidth]{figures/RF_test_v4-gridsearch_pearson_fs-k1200_bins10_v1.pdf}
%     \caption{RF Predictions Distribution}
%     \label{fig:fig-rf-distribution}
% \end{figure}


Table \ref{tab:rq1-threshold-analysis} shows that by increasing the inclusion probability threshold, our RF model was able to achieve more accurate results with an F-score of 0.41 at the cost of increasing the number of FN (\textit{i.e.}, losing evidence).

\begin{table}[ht]
\centering
\scriptsize
\caption{Tradeoff between F-score and number of FN.}
\label{tab:rq1-threshold-analysis}
\begin{tabular}{|c|c|c|c|c|c|}
\hline
\textbf{Threshold(\%)} & \textbf{F-score} & \textbf{TN} & \textbf{TP} & \textbf{FN} & \textbf{FP} \\
\hline
0.50\% & 0.33 & 430 & 24 & 14 & 83 \\ 
0.60\% & 0.38 & 468 & 20 & 18 & 45 \\  
0.65\% & 0.39 & 485 & 16 & 22 & 28 \\  
0.70\% & 0.41 & 498 & 14 & 24 & 15 \\ 
\hline
\end{tabular}
\end{table}

\subsection{RQ2: \textit{How much effort can ML models reduce during the study selection activity of SLR updates?}}
\label{results:RQ2}

% To answer this question, we tuned the ML Models with  the intention of maximizing the Recall. Since the purpose of this question was to evaluate how much human effort could be reduced by the use of ML Models during the selection of studies, we wanted to mitigate the chances of a false negative (FN) result, so the reviewers could simply ignore the studies excluded by the ML Model without worrying about losing a relevant study.

% As demonstrated in Figure \ref{fig:fig-svm-distribution}, our best result was obtained by the SVM algorithm with a Precision of 0.10, Recall of 1.0 and F-score of 0.19 using the Pearson Correlation statistical method, with 1200 features. We used a default threshold of 0.5 to consider which studies should be included and excluded by our algorithms. 

% According to Table \ref{tab:effort_reduction}, by maximizing the Recall, SVM was able to exclude a total of 187 studies, which represents 33.9\% of the total amount of studies in our testing set. By increasing the threshold, we are able to see that we can reduce the human effort even more at the risk of having more FN results. For a threshold range greater than 0.5 until 0.75, only one false negative was found. Notably, this FN result was one of the few cases where the team assessment had a lot of disparity. Initially, considering only the analysis of the reviewers individually (before they discussed it with each other), the reviewer R1 voted 2, R2 voted 1, and R3 voted 0.

To answer this question, we tuned the ML models to maximize the Recall. Since the purpose of this question was to evaluate how much human effort could be reduced by using ML models during the selection of studies, we wanted to mitigate the chances of false negatives (FN) so the reviewers could simply ignore the studies excluded by the ML model without worrying about loss of evidence.

Our best result was obtained by using the SVM algorithm with a Precision of 0.10, Recall of 1.0, and F-score of 0.19 using the Pearson Correlation statistical method, with 1,200 features. We used a default inclusion probability threshold of 0.5 to consider which studies should be included and excluded by our ML models. The first line of Table \ref{tab:effort_reduction} shows these results. It is possible to observe that while maximizing the Recall to 100\%, SVM could exclude 187 studies, which represents 33.9\% of the total amount of studies in our testing set. The parameters tested and selected by GridSearch for this configuration and a table with all the predictions made by our ML model for each study analyzed to answer RQ2 are available online.  

\begin{table}[ht]
\centering
\scriptsize
\caption{Tradeoff between effort reduction and number of FN.}
\label{tab:effort_reduction}
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
\textbf{Threshold(\%)} & \textbf{RECALL (\%)} & \textbf{TN} & \textbf{TP} & \textbf{FN} & \textbf{FP} & \textbf{Reduced (\%)} \\
\hline
0.50\% & 100.00\% & 187 & 38 & 0 & 326 & 33.9\% \\ 
0.75\% & 97.37\% & 265 & 37 & 1 & 248 & 48.3\% \\  
0.80\% & 94.74\% & 283 & 36 & 2 & 267 & 51.7\% \\  
0.85\% & 89.49\% & 307 & 34 & 4 & 206 & 56.4\% \\ 
\hline
\end{tabular}
\end{table}

% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=1\linewidth]{figures/fig-rq2-SVM_test_v4-gridsearch_pearson_fs-k1200_bins10.pdf}
%     \caption{SVM Predictions Distribution}
%     \label{fig:fig-svm-distribution}
% \end{figure}

It is noteworthy that by increasing the inclusion probability threshold (\textit{i.e.}, including only studies for which the ML model considered a higher inclusion probability), we can reduce the human effort even more at the risk of having more FN results and losing evidence. It is possible to observe, for instance, that increasing the threshold to 75\% results in only one FN while the number of TN increases by 87. \textit{I.e.}, one study that should be included would be lost, but 87 less would have to be analyzed. Notably, this particular FN was a complicated case in which the initial screening by the human reviewers also had a lot of disparity. Reviewer R1 voted 2 (include), R2 voted 1 (unsure), and R3 voted 0 (exclude). 

% "so it couldn't reduce much effort for the reviewers" -> Quantificar melhor
% "De acordo com a tabela X conseguimos reduzir em Y%..."


% We achieved some results with 100\%, but in these cases the number of FP studies were still very high, so it couldn't reduce much effort for the reviewers. Whereas, results with a lower Recall percentage indicates a big reduction in the number of FP in exchange for a little number of relevant studies. So we could argue that the tradeoff can be … \textbf{\textit{<worth it or not>}}? - Reescrever detalhando os valores encontrados nesses casos


\subsection{RQ3: \textit{How does the support of ML in the selection of studies compare to the support of an additional human reviewer?}}
\label{results:RQ3}

In this research question, we explored how well a human and an ``ML model reviewer" pair would perform in selecting studies for inclusion compared to pairs of human reviewers. To maximize support, we used the ML model with the highest \textit{F-score} (the same RF model used to answer RQ1). After all, a higher \textit{F-score} indicates that this model made more realistic predictions. 

To prepare the data, we mapped the RF model’s inclusion probabilities to match the scale used by reviewers in the initial screening (0 - exclude, 1 - unsure, 2 - include). Since our dataset was unbalanced, as shown by the reviewers' assessment distributions in Figure~\ref{fig:fig-reviewers-votes-distribution}, we adjusted thresholds to achieve a similar distribution, mapping probabilities as follows:
\begin{itemize}
    \item 0.00 to 0.50: 0 - exclude
    \item 0.51 to 0.60: 1 - unsure
    \item 0.61 to 1.00: 2 - include
\end{itemize}

Figure~\ref{fig:fig-rf-normalized-distribution} shows the RF model's probability distribution with these thresholds applied. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=6cm]{figures/fig-rq3-reviewers-votes-distribution.pdf}
    \caption{Reviewers assessment distribution.}
    \label{fig:fig-reviewers-votes-distribution}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=6cm]{figures/rf_test_v4_normlized_agreement-v10.pdf}
    \caption{Predictions Distribution considering "uncertain" range.}
    \label{fig:fig-rf-normalized-distribution}
\end{figure}

By mapping the RF model’s outcomes to the same scale used by the three human reviewers, we were able to treat the model as an additional reviewer in our analyses. \textit{I.e.}, we now had initial screenings in similar scales for R1, R2, R3, and RF. The final step of data preparation involved aligning the Final Result (FR) with the same scale, assigning a value of 2 to included studies and 0 to excluded studies. 

After data preparation, our analysis involved using the Euclidean Distance (ED) to evaluate how close different combinations of reviewers were to the FR (oracle). We evaluated the ED to the FR for single reviewers, pairs, and groups of three reviewers, as follows:
\begin{itemize}
    \item Similarity between single answers and FR: 
    \[
    \textit{ED}\left(i, \text{FR}\right) \text{ where } i \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
    \]
    \item Similarity between pairs and FR:
    \[
    \textit{ED}\left(\frac{i + j}{2}, \text{FR}\right) \text{ where } i \neq j \text{ and } i, j \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
    \]
    \item Similarity between groups and FR:
    \begin{multline}
    \textit{ED}\left(\frac{i + j + k}{3}, \text{FR}\right) \\ 
    \text{ where } i \neq j \neq k \text{ and } i, j, k \in \{\text{R1}, \text{R2}, \text{R3}, \text{RF}\}
    \end{multline}
\end{itemize}

Table~\ref{tab:similarity-distance-FR-table} shows the ED measured in each case. As we can see, the smallest distance compared to the FR in each case was given by: R2 with ED = 9.95, pair (R2, R3) with ED = 8.86, and Group (R1, R2, R3) with ED = 8.17. It is possible to observe that (i) individual human reviewers outperformed the RF model, (ii) pairs of human reviewers alone outperformed pairs of a human reviewer and the RF ML model, and (iii) replacing any human reviewer of the assessment team with the RF model would lead to poorer results.

\begin{table}[!ht]
\centering
\scriptsize
\caption{Euclidean Distance Analysis considering FR}
\label{tab:similarity-distance-FR-table}
\begin{tabular}{|c|c|}
\hline
\textbf{Comparison} & \textbf{Euclidean Distance} \\ \hline
R1 vs FR & 12.00 \\ \hline
R2 vs FR & \textbf{9.95} \\ \hline
R3 vs FR & 11.18 \\ \hline
RF vs FR & 16.67 \\ \hline
\hline
Pair (R1,R2) vs FR & 8.90 \\ \hline
Pair (R1,R3) vs FR & 9.23 \\ \hline
Pair (R2,R3) vs FR & \textbf{8.97} \\ \hline
Pair (R1,RF) vs FR & 11.58 \\ \hline
Pair (R2,RF) vs FR & 11.48 \\ \hline
Pair (R3,RF) vs FR & 11.80 \\ \hline
\hline
Group (R1,R2,R3) vs FR & \textbf{8.25}\\ \hline
Group (RF,R2,R3) vs FR & 10.02 \\ \hline
Group (R1,RF,R3) vs FR & 9.93 \\ \hline
Group (R1,R2,RF) vs FR & 9.77 \\ \hline
\end{tabular}
\end{table}