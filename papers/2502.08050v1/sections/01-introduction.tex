\section{Introduction}
\label{sec:introduction}

In the context of Evidence-Based Software Engineering (EBSE), Systematic Literature Reviews (SLR) are the main instrument to identify, synthesize, and summarize current evidence on a research topic or phenomenon of interest \cite{Kitchenham15}. Since the introduction of SLR in the Software Engineering (SE) field in 2004 \cite{Kitchenham04}, especially over the recent years, the number of SLRs has increased substantially \cite{Mendes2020,Napoleao2021S}. As stated by Mendes \textit{et al.} \cite{Mendes2020}, several SLRs in SE need to be updated. Only 20 SLRs were updated between 2006 and 2018 in a scenario of over 400 published SLRs in SE \cite{Mendes2020}. 

Outdated SLRs could lead researchers to make obsolete decisions or conclusions about a research topic \cite{Watanabe20}. Despite several initiatives in SE to keep SLRs updated (e.g., processes \cite{Dieste08a, Mendes2020}; guidelines \cite{Wohlin2020, felizardo16}; and experience reports \cite{Garces17, Felizardo20}) there is a lack of investigation on automation tools to support the SLR updates.

Performing an SLR update demands significant effort and time for reasons such as (i) the rapid growth of available evidence \cite{Zhang18, Stol15}, which hampers and slows down the identification of relevant sources; and (ii) the lack of detailed protocol documentation and data availability \cite{Ampatzoglou2019, Zhou2015}, which makes the SLR update process even more difficult, especially when different researchers carry out the update, as most of the tacit knowledge from the original SLR is lost \cite{Felizardo20, Fabbri13}. These reasons directly impact the selection of new studies during SLR updates since they are crucial to determining whether or not new evidence should be taken into account.

%The selection of studies activity for SLR updates follows the same process of performing the selection of studies for SLRs \cite{Kitchenham15}. The work of Napole\~ao et al. \cite{Napoleao2021} indicates that automated approaches addressing several Machine Learning (ML) algorithms demonstrated significant support in reducing the time and effort during the study selection activity for SLRs. In addition, the work of Watanabe et al. \cite{Watanabe20} showed the potential of supervised ML Models in reducing the effort required to select studies for SLR updates. %Both works highlighted the need for further investigation on the adoption of ML Models to support the study selection activity for SLR updates.

Machine learning (ML) has been effectively used for classification tasks in several domains.
Considering the potential benefits of exploring Machine Learning (ML) algorithms in the SLR and SLR updates context \cite{Napoleao2021, Watanabe20}, we present an empirical investigation with the \textbf{goal} of evaluating the adoption of ML models to support the selection of studies for SLR updates with respect to three perspectives: (i) effectiveness of the ML models in selecting relevant studies; (ii) effort reduction in terms of numbers of studies to be analyzed; and (iii) support in assisting reviewers in accurately selecting studies, when compared to the assistance of an additional human reviewer.

To achieve our study goal, we investigate the potential of two supervised ML models, Support Vector Machines (SVM) and Random Forest (RF). These models were chosen considering they are among the five most used text classifiers \cite{pintas2021feature} and performed better than the other three (Naive Bayes, k-Nearest Neighbors, and Decision Trees) in our initial tests. We compare their results with a rigorous manual selection process of an SLR update conducted by three experienced SE researchers. During this process, initially, each human reviewer assessed the papers based on title, abstract, and keywords using the following scale: 0 - exclude, 1 - unsure, 2 - include. Thereafter, they discussed differences and conducted the inclusion by also analyzing the full texts of the papers until reaching a consensus, producing a final curated list of included and excluded studies that we consider as our oracle. The initial assessments made by the reviewers based on the title, abstract, and keywords were naturally error-prone, similar to the ML models, which also relied on the title, abstract, and keywords for their classification. 

%Besides the positive results obtained with the adoption of supervised ML Models in \cite{Watanabe20}, we opted to use them to take advantage of the knowledge from the list of included studies in the original SLR to perform a new study selection for its update.

The main \textbf{contributions} of our study include (i) an empirical analysis of the effectiveness of ML models to support the study selection of SLR updates; (ii) the potential to reduce human effort during study selection; and (iii) an analysis on whether the ML models could assist individual reviewers in accurately selecting studies to be included.

Our findings indicated that we cannot rely on ML models for study selection in SLR updates, as their current effectiveness (f-score of 0.33) is clearly insufficient. However, there is potential for reducing the study selection effort for SLR updates by about 34\% without loss of evidence (recall 100\%), discarding studies with a low probability of being included according to the ML models. Furthermore, we found that pairing human reviewers with ML models when selecting studies is also not a good option, as their aggregated results do not improve their individual results. In fact, pairs of human reviewers performed much better than pairs of a human and an ML model.

% Update this organization when the paper is done.
The remainder of this study is organized as follows. Section \ref{sec:relatedwork} presents the background and related work. Section \ref{sec:researchissues} defines our goal and research questions. Section \ref{sec:methodology} describes our study design and data acquisition. Section \ref{sec:results} presents the results of each research question. Section \ref{sec:discussion} discusses our results before presenting the study's threats to validity in Section \ref{sec:threats}. Finally, conclusions are drawn (Section \ref{sec:conclusion}).