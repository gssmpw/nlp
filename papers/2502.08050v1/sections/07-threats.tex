\section{Threats to Validity}
\label{sec:threats}

In the following, we enumerate the main threats to the validity of our study, using the categories suggested by~\cite{Runeson12}.

\textbf{Construct Validity.}
Our evaluation results might have been affected by the choice of the SLR update and of the ML algorithms. Regarding the chosen SLR update, it is very difficult to get access to details such as individual assessments by reviewers during the initial screening process, which we needed for our analyses. Our SLR update dataset had such detailed information for 551 studies and is available online~\cite{zenodoOpenScience}. Regarding the algorithms, we analyzed the most used ones for text classification~\cite{pintas2021feature} and dug deeper into the two that showed the most prominent initial evaluation results on our dataset. 

\textbf{Internal Validity.}
Our training dataset comprised only studies included in the SLR replication \cite{Wohlin2022} (training included) and those obtained through backward snowballing (training excluded). We deliberately excluded studies not in English or those categorized as Ph.D. dissertations or book chapters from our testing set, the same criteria adopted by the SLR. A potential threat to internal validity that could have favored human reviewers is that, during the manual initial screening process, while this was not part of the procedure, the human reviewers could have ended up reading other sections of the studies besides the title, abstracts, and keywords.

\textbf{External Validity.} The dataset used in our analysis might not represent the diversity of SLR updates in SE. However, we did not find other SLRs with available data on the individual assessments applied during the initial screening. Replicating the investigations on other SLR updates to strengthen external validity would require significant effort for which we would have to involve the wider community. While not claiming external validity, we believe that sharing our initial evaluation results can already provide some valuable insights.

\textbf{Reliability.} The data used in our evaluation, including the individual initial screening assessments and the final list of papers to be included in the SLR update, was generated by the same (first three) authors who performed the SLR replication~\cite{Wohlin2022}. In addition, to improve the reliability of our results, our ML models and the evaluation datasets are openly available and auditable. 