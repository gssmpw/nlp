\section{Conclusion}
\label{sec:conclusion}

This study investigated the application of supervised ML models as a supporting tool for researchers during study selection in SLR updates. Therefore, we developed a supervised ML pipeline for the study selection process. The focus was on investigating the effectiveness of ML models, the potential to reduce human effort, and the ability to provide support to individual human reviewers. We employed two ML models, Random Forest (RF) and Support Vector Machine (SVM), and assessed them on a dataset derived from a carefully manually curated SLR update. During this investigation process, our work also highlighted different configurations used for our ML models that correlate to their recall and F-score, providing results that can be useful for further exploration in this area.

Our results indicate that while ML can assist in preliminary study selection by reducing the volume of studies requiring manual review, it is not yet effective enough to automate this process or to directly assist a single human reviewer to produce more accurate selection results. Specifically, RF, our best model for study selection effectiveness, achieved a modest F-score of 0.33, with limited precision and recall, which is clearly insufficient for study selection. Meanwhile, SVM demonstrated potential in reducing effort by excluding up to 33.9\% of irrelevant studies without sacrificing recall. The comparison between human-only reviewer pairs and human-ML reviewer pairs for the initial screening showed that pairs of human reviewers produce results that are much better aligned with the final curated result of the SLR update. 

Considering our findings, we put forward that serious SLR update efforts should still rely on (at least two) experienced human researchers for the initial screening of papers to be included. Hence, this study contributes to understanding the practical limitations of ML in study selection and highlights the need for careful human involvement in this process to ensure the quality and rigor of SLR outcomes. 

Future research could focus on refining ML configurations, investigating adaptive thresholds to improve model performance in SLR update contexts, and exploring hybrid approaches (\textit{e.g.}, humans assisted by ML to reduce the overall screening effort by discarding studies with low probability of being included). We also recommend further investigating large language models (LLMs) within the SLR update context. 

