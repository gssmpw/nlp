%
\begin{figure*}[!t]
\includegraphics[width=1.0\linewidth]{figures/corruption_image.pdf}
%\vskip-2ex
\caption{\textbf{Overview of degradation modes in the SemanticKITTI dataset.} 
The degradation modes are illustrated, where clean image (a) serves as the baseline. Fog (b), brightness (c), darkness (d), and motion blur (f) represent degradations influenced by environmental factors or the interaction between the environment and sensors. Shot noise (e), primarily caused by sensor limitations or camera malfunctions, is included to evaluate the ability of event cameras to compensate for the shortcomings of traditional cameras in adverse scenarios.
}
\label{fig:corruption}
%\vskip-3ex
\end{figure*}
\input{tables/Kitti_SSC}



\section{Experiment}
\label{sec:exp}
%
\begin{figure*}[t]
\centering
\includegraphics[width=1\linewidth]
{figures/DSEC-SSCvisual.pdf}
% \vskip-2ex
\caption{\textbf{Qualitative results of EvSSC and baseline on DSEC-SSC.} EvSSC better captures scene layouts in low-light scenes.}
\label{fig:dsecquality}
% \vskip-2ex
\end{figure*}


\subsection{Datasets}
\noindent\textbf{DSEC}, introduced by Gehrig~\textit{et al.}~\cite{gehrig2021dsec}, consists of $24$ sequences captured in real-world outdoor driving scenes using event cameras, RGB cameras, and LiDAR. 
The dataset includes $7,800$ training samples and $2,100$ testing samples, captured at a size of $640{\times}480$. It covers both daytime and nighttime scenarios, with small and large motions. As the dataset does not provide official occupancy ground truth, we generated semantic occupancy labels by utilizing the released 2D semantic segmentation annotations and depth data, followed by manual refinement, resulting in the derived DSEC-SSC dataset (see Sec.~\ref{sec:benchmark} for details). 
LiDAR scans were voxelized into a $128{\times}128{\times}16$ grid with voxel sizes of $0.4m$, labeled into $14$ common categories (see Tab.~\ref{table:dsec_ssc}), consistent with semantic labels provided by DSEC.

\noindent \textbf{SemanticKITTI}~\cite{behley2019semantickitti} consists of outdoor LiDAR scans voxelized into a $256{\times}256{\times}32$ grid with $0.2m$ voxels, labeled into $21$ classes ($19$ \emph{semantic}, $1$ \emph{free}, $1$ \emph{unknown}). 
We use RGB images of size $370{\times}1,220$. We adopt the official $3834/815$ train/val splits and consistently evaluate at full scale (\textit{i.e.}, $1{:}1$). 
To assess the impact of the event modality on occupancy prediction for this dataset, we generate event sequences for SemanticKITTI using the official DVS-Voltmeter codebase~\cite{lin2022dvs}, resulting in the derived SemanticKITTI-E dataset. 
Additionally, to evaluate SSC's robustness against camera corruption, we simulate five common real-world corruptions including \emph{motion blur}, \emph{fog}, \emph{brightness}, \emph{darkness}, and \emph{shot noise}.

\subsection{Quantitative Comparison}
\noindent \textbf{Analyses on the DSEC-SSC benchmark.} 
We first benchmark popular camera occupancy prediction methods~\cite{cao2022monoscene,zhang2023occformer,li2023voxformer,mei2024sgn} on the newly proposed DSEC-SSC dataset. 
For a fair comparison, we report training results using both 2D images ($x^{rgb}$) and event data ($x^{event}$) as inputs. 
Note that we did not modify the baseline model structures. 
As a framework approach, EvSSC integrates both image and event modalities, making it applicable to various baselines. 
As shown in Tab.~\ref{table:dsec_ssc}, using only the event modality decreases mIoU and IoU across all camera methods. 
For example, the event modality version of MonoScene~\cite{cao2022monoscene} shows a drop in mIoU compared to the RGB modality version by $1.86$ ($15.86$ \textit{vs.} $17.72$), which is expected as the event modality lacks the detailed textures of RGB. 
EvSSC proves effective across different RGB SSC baselines. 
EvSSC (VoxFormer) improves mIoU by $0.72$ compared to RGB-only VoxFormer ($26.34$ \textit{vs.} $25.62$) and by $10.26$ compared to its event modality counterpart ($26.34$ \textit{vs.} $16.08$). Similarly, EvSSC (SGN) boots mIoU by $0.31$ compared to its RGB baseline ($29.37$ \textit{vs.} $29.06$) and by $6.84$ compared to its event modality counterpart ($29.37$ \textit{vs.} $22.53$).
%

\noindent \textbf{Analyses on the SemanticKITTI-E benchmark.}
%
As shown in Tab.~\ref{table:kitti_e_ssc}, we compare EvSSC, which fuses event and RGB modalities, with RGB-only occupancy prediction methods on SemanticKITTI-E. 
Despite the simulated nature of event data on this dataset, EvSSC effectively shows the complementary strengths of event and RGB modalities. 
EvSSC (VoxFormer) improves mIoU by $5.8\%$ over VoxFormer~\cite{li2023voxformer} ($13.61$ \textit{vs.} $12.86$). 
Using SGN-S~\cite{mei2024sgn} as the baseline, EvSSC (SGN) achieves the highest accuracy among visual occupancy models, reaching mIoU of $15.15$, surpassing 
%
Symphonies~\cite{jiang2024symphonize} ($15.15$ \textit{vs.} $14.89$). 
This further demonstrates the effectiveness of incorporating event modality to aid RGB in occupancy prediction, which comes with only a slight increase in terms of latency.
Evaluated on a single NVIDIA RTX 3090 GPU, the inference latencies of VoxFormer and EvSSC are $0.996s$ and $1.005s$ per frame, respectively.

%

\subsection{Enhancing Robustness with Event Modality}
%

Traditional visual occupancy models are highly susceptible to image degradation challenges, such as low-light conditions and motion blur. 
As shown in Tab.~\ref{table:dsec_ssc}, DSEC-SSC includes numerous low-light scenes, where we quantitatively demonstrate the robustness that the event modality brings to occupancy prediction in real-world settings. 
To further evaluate the impact of event modality under various degradation conditions, we generated the SemanticKITTI-C dataset, featuring five common degradation scenarios: \emph{motion blur}, \emph{fog}, \emph{brightness}, \emph{darkness}, and \emph{shot noise} from image sensors. 
Specifically, we consider two experimental setups, reporting occupancy mIoU for each.

\input{tables/Kitti_corrupted_combined}

1) \textbf{Out-Of-Domain (OOD):} 
In this setting, models are trained on clean data and tested directly on degraded scenes without prior exposure to degradation. 
As shown in Tab.~\ref{table:kitti_c_ssc}, both VoxFormer~\cite{li2023voxformer} and SGN~\cite{mei2024sgn} show large performance drops across all degradation types, especially in motion blur ($12.86{\rightarrow}8.54$), darkness ($12.86{\rightarrow}9.53$), and shot noise ($12.86{\rightarrow}8.10$). 
Although EvSSC is also trained only on clean data, the inclusion of the event modality effectively enhances the robustness of both baselines across all degradations. 
For example, with motion blur, EvSSC improves mIoU by $6.8\%$ ($9.12$ \textit{vs.} $8.54$) and $8.0\%$ ($8.91$ \textit{vs.} $8.25$) over the baselines, demonstrating the value of temporal cues in events; for darkness, EvSSC improves by $3.6\%$ ($9.87$ \textit{vs.} $9.53$) and $6.8\%$ ($9.78$ \textit{vs.} $9.16$), showing complementary strengths of the high dynamic range in event sensors.
\input{tables/Ablations}
2) \textbf{In-Domain (ID):} Here, models are pre-trained on degraded data. 
As shown in Tab.~\ref{table:kitti_c_ssc}, while overall occupancy prediction improves under ID training compared to OOD, accuracy remains lower than on clean data. 
Including events significantly enhances the in-domain performance of both baselines across all degradations. 
%
Especially in shot noise, EvSSC further improves by $52.5\%$ ($12.64$ \textit{vs.} $8.29$) and $5.1\%$ ($14.32$ \textit{vs.} $13.62$), confirming the robustness of event-RGB fusion when the image sensor partially fails.
%

%
\begin{figure*}[t!]
\centering
%\vskip-2ex
\begin{minipage}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/KITTI_visualization1.pdf}
    %\vskip-2ex
\end{minipage}

%\vskip-0.3ex

\begin{minipage}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/KITTI_visualization2.pdf}
    %\vskip-2ex
\end{minipage}
%\vskip-1ex
\caption{\textbf{Qualitative results of EvSSC and baseline on SemanticKITTI-C.} More visual results of 3D occupancy on SemanticKITTI-C, showing predictions from VoxFormer-S~\cite{li2023voxformer} and EvSSC (VoxFormer). The first four rows correspond to the degradation mode of fog, while the last four rows correspond to the degradation mode of motion blur.}
\label{fig:corruption_v}
%\vskip-4ex
\end{figure*}
\subsection{Qualitative Comparison}
\noindent \textbf{Analyses on the DSEC-SSC benchmark.}
Based on the visualization results of EvSSC and the baseline SGN~\cite{mei2024sgn} on the DSEC-SSC validation set, as shown in Fig.~\ref{fig:dsecquality}, it can be observed that SGN often fails to fully reconstruct or detect traffic signs and cars in low-light conditions. 
In contrast, with the assistance of event data, EvSSC is capable of achieving complete reconstruction. Notably, in the fifth column, for vehicles with incomplete appearances in the image that SGN fails to detect, the integration of event data enables successful detection. 
Furthermore, as seen in the first and sixth columns, the incorporation of event data significantly improves the detection of lane markings and facilitates the reconstruction of detailed scene elements at the traffic intersections on low-contrast road surfaces.

\noindent \textbf{Analyses on the SemanticKITTI-C benchmark.}
As illustrated in Fig.~\ref{fig:corruption_v}, we visualize the prediction results of EvSSC and the baseline in scenarios with motion blur. It can be observed that due to the loss of texture information in the image, VoxFormer produces a significant number of mispredicted voxels. In contrast, with the assistance of event data, EvSSC achieves more accurate predictions, resulting in a cleaner and more refined visualization.


\subsection{Ablation Studies}
%

%
As shown in Tab.~\ref{tab:ablations}, we conduct ablations to validate design choices of EvSSC, reporting occupancy IoU and mIoU for various configurations.
(a) We examine the impact of three event-image fusion paradigms for 2D-to-3D occupancy models. 
To avoid bias introduced by complex fusion modules, we use simple additive fusion. 
Results show ``Fusion-Based Lifting'', which introduces event modality during the 2D-to-3D lifting process, provides the most significant mIoU gain ($13.10$ \textit{vs.} $12.86$).
Thus, EvSSC integrates the ELM module into the lifting stage.
(b) We explore the effects of different event representations on RGB-Event occupancy prediction.
The rasterized event representation used in this work encodes multiple statistics across channels, including counts and timestamps for positive and negative events. 
This approach effectively captures motion while aligning with RGB guidance, resulting in the highest mIoU gain over naive 2D event frames ($13.61$ \textit{vs.} $13.40$) and several representation methods~\cite{delbruck2008frame,vonmarcard2018recovering}.
(c) Building on (a), we investigate the impact of various attentional and RGB-X fusion methods~\cite{woo2018cbam,zhang2023cmx,sun2022event_deblurring} within a transformer-based lifting paradigm using VoxFormer~\cite{li2023voxformer}. 
Results show the dual-branch self-attention design in ELM provides the largest boost compared to simple addition ($13.61$ \textit{vs.} $13.10$).
(d) To assess the generalizability of ELM across architectures, we test ELM on LSS-style~\cite{philion2020lift} SGN~\cite{mei2024sgn}. 
Results confirm the effectiveness of dual-branch self-attention in this setup as well, with mIoU boosting from $14.58$ to $15.15$.

\subsection{Efficiency Analysis}
We further perform efficiency evaluations on the DSEC-SSC, SemanticKITTI-E, and SemanticKITTI-C datasets using a single NVIDIA RTX 3090 GPU. 
As shown in Tab.~\ref{table:efficiency}, our EvSSC consistently achieves great accuracy gains on all datasets and a remarkable $52.5\%$ improvement of mIoU in degraded scenarios while requiring only an additional $0.91$GB of GPU memory and a $0.9\%$ increase in latency.
Furthermore, on the non-degraded SemanticKITTI-E dataset, a mere millisecond-level latency is required to achieve a mIoU improvement of $0.75$. Efficiency analysis shows that minimal additional deployment costs can significantly improve prediction robustness, enhancing autonomous driving safety and practical application.

\input{tables/efficiency}
