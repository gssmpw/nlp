\section{Related Work}


\begin{figure*}[!t]
\centering
\includegraphics[width=1.0\linewidth]{figures/dsec_ssc_pipeline.pdf}
\caption{\textbf{Overview of the occupancy label generation pipeline.} The pipeline consists of three main steps: Semantic-Maps-Guided Dynamic Object Segmentation (Sec.~\ref{sec:semantic_guided_dynamic_object_segmentation}), Dynamic Object 4D Reconstruction (Sec.~\ref{sec:dynamic_objects_4d_reconstruction}), and Probability-Guided Voxel Refinement (Sec.~\ref{sec:probability_guided_voxel_refinement}).  
}
\label{fig:dsec_ssc_pipeline}
\end{figure*}

\subsection{Semantic Scene Completion}
Existing Semantic Scene Completion (SSC) approaches can be mainly classified into LiDAR-based, camera-based, and modality-fusion methods.
Although LiDAR-based~\cite{xia2023scpnet,cheng2021s3cnet,yan2021js3c_net,roldao2020lmscnet,jang2024talos,cao2024slcf,peng2022mass} and modality-fusion methods~\cite{pan2024co_occ,ming2024occfusion,lu2024_ipvoxelnet,xue2024bi_ssc,xue2024bi_ssc,liu20242d_guided,ding2024radarocc,zhang2024radocc,cui2024loma} can deliver relatively strong performance, camera-based methods are often preferred for practical deployment due to their lower economic costs and superior real-time capabilities.
%
MonoScene~\cite{cao2022monoscene} introduces the first approach to infer 3D SSC from a single monocular image. 
Recent advancements in SSC have developed from dual-path transformer model~\cite{zhang2023occformer} to two-stage architectures with masked autoencoders~\cite{li2023voxformer}, followed by the introduction of context-aware instance queries~\cite{jiang2024symphonize} and hybrid guidance to improve feature separation~\cite{mei2024sgn,shi2024panossc}.
Further research expands into several aspects with efficiency optimization~\cite{wang2024not,hou2024fastocc,zhao2024lowrankocc,wang2024reliocc,yang2024adaptiveocc}, 
%
offboard perception~\cite{shi2024occfiner,ma2024zopp,zheng2024occworld,ouyang2024linkocc},
%
3D representation~\cite{huang2024gaussianformer,huang2023tpvformer,shi2024occupancy_set_points,xiao2024instance_aware,wang2024opus}, 
sparse feature processing~\cite{tang2024sparseocc,liu2024fully_sparse,wei2023surroundocc}, 
open-vocabulary recognition~\cite{vobecky2024pop_3d,zheng2024veon}, 
self-supervision~\cite{huang2024selfocc,hayler2024s4c}, 
%
and context enhancement~\cite{yu2024cgformer,zhao2024hybridocc,ma2024cotr,li2024htcl,li2024viewformer},
%




While conventional cameras are cost-effective, their perception capabilities are highly susceptible to poor weather and illumination conditions.
Event cameras provide robust, high-frequency information in complex environments. 
Therefore, we introduce a real-world 3D occupancy prediction dataset, \emph{DSEC-SSC}, incorporating event modality and for the first time accomplishing event-aided SSC.

%



\subsection{Event-driven Semantic Dense Understanding}
Recently, event cameras have gained prominence in semantic dense understanding due to their high dynamic range, low latency, and low bandwidth. 
RGB-Event fusion~\cite{xia2023cmda,zhang2023cmx,zhen2025event_guided,xie2024eisnet} significantly benefits the performance of vision tasks. 
Many studies have explored image-event fusion, fully leveraging texture features of RGB and high-frequency characteristics of events.
Event-based semantic segmentation, first introduced in Ev-SegNet~\cite{alonso2019ev_segnet}, capitalizes on asynchronous events to achieve significant advances.
Subsequent works focus on scene-adaptive rendering~\cite{low2021superevents}, unsupervised adaptation~\cite{sun2022ess}, event-specific attention~\cite{jia2023event_posterior}, bidirectional fusion~\cite{zhang2021issafe,zhang2021exploring}, and multi-branch feature extraction~\cite{zhang2024spikingedn,long2024spike_brgnet}, advancing the robustness and flexibility of event representation. 
%
Event cameras have also supported advancements in object detection~\cite{cao2024embracing,liu2024enhancing_traffic,li2023sodformer}, deblurring~\cite{sun2022event_deblurring,kim2024frequency,yang2024latency}, flow estimation~\cite{wan2023rpeflow,ye2023towards_event}, and tracking~\cite{zhang2023frame_event_tracking,wang2023visevent,wang2024towards_robust_tracking}. 
Yet, event-aided SSC remains largely underexplored. 
We propose \emph{EvSSC}, containing our custom-designed \emph{Event-aided Lifting Module (ELM)} for 2D-to-3D view transformation, tailored for the SSC task, enabling effective cross-modal fusion.

