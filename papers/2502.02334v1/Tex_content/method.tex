
%
\section{EvSSC: Proposed Fusion Framework}
%

%
Current visual occupancy perception methods~\cite{li2023voxformer,mei2024sgn,cao2022monoscene} rely heavily on traditional cameras, limiting their robustness in dynamic scenes. 
To enable RGB-Event fusion-driven SSC, we develop \emph{EvSSC}, the first framework for event-aided semantic scene completion, introducing three fusion paradigms and proposing the \emph{Event-aided Lifting Module (ELM)}, designed for efficient integration of image and event features. ELM is flexible across various network architectures, making it adaptable to different SSC models.

%

%
\subsection{Fusion Paradigms}
%

%
The standard SSC network includes a camera encoder, 2D-to-3D transformation, 3D backbone, and completion head. The encoder extracts features through an image backbone and neck, which provide the foundation for constructing 3D volume features during the transformation stage, using either transformer-based~\cite{li2023voxformer,huang2023tpvformer} or LSS-based~\cite{mei2024sgn,philion2020lift,zhang2023occformer} methods. The 2D-to-3D lifting process is crucial in this pipeline, as it aligns 2D image features with the 3D spatial structure, enabling precise 3D occupancy predictions that are essential for autonomous perception. Accurate lifting ensures that spatial cues from 2D inputs are effectively translated into coherent 3D representations, directly impacting the model’s depth and spatial reasoning capabilities. To enable the effective fusion of image and event features within this framework, we propose three fusion paradigms, as shown in Fig.~\ref{fig:fusion&framework}.

%
\noindent\textbf{Fusion then lifting.} 
%
In this paradigm, image features \( \mathbf{F}^{\text{2D}}_{\text{img}} {\in} \mathbb{R}^{b \times c \times d} \) and event features \( \mathbf{F}^{\text{2D}}_{\text{event}} {\in} \mathbb{R}^{b \times c \times d} \) are fused at the 2D stage, where \( b {\times} c \) is the spatial resolution, and \( d \) is the feature dimension. $\oplus$ denotes the chosen fusion method:
\begin{align}
&\mathbf{F}^{\text{2D}}_{\text{fusion}} = \mathbf{F}^{\text{2D}}_{\text{img}} \oplus \mathbf{F}^{\text{2D}}_{\text{event}}.
\end{align}
This early fusion enhances feature alignment but may result in spatial inconsistencies in complex scenes.

\noindent\textbf{Fusion-based lifting.} 
Here, features for the image modality are represented as \( \mathbf{F}^{k}_{\text{img}} {\in} \mathbb{R}^{N_p \times d} \) and \( \mathbf{F}^{v}_{\text{img}} {\in} \mathbb{R}^{N_p \times d} \) for the key and value features, respectively. 
Similarly, the features for the event modality are represented as \( \mathbf{F}^{k}_{\text{event}} {\in} \mathbb{R}^{N_p \times d} \) and \( \mathbf{F}^{v}_{\text{event}} {\in} \mathbb{R}^{N_p \times d} \), where \( N_p \) is the number of query proposals, and \( d \) is the feature dimension.
The fused multi-modal features for key and value are then given by:
\begin{align}
&\mathbf{F}^{k}_{\text{fusion}} = \mathbf{F}^{k}_{\text{img}} \oplus \mathbf{F}^{k}_{\text{event}}, 
&\mathbf{F}^{v}_{\text{fusion}} = \mathbf{F}^{v}_{\text{img}} \oplus \mathbf{F}^{v}_{\text{event}}.
\end{align}
This approach combines the complementary features of both modalities, yielding robust fused representations for 3D volume construction.

\input{tables/Dsec_SSC}

\noindent\textbf{Decode then fusion.} 
In this paradigm, the fusion occurs after 2D-to-3D transformation. Features for the image modality are represented as \( \mathbf{F}^{\text{3D}}_{\text{img}} {\in} \mathbb{R}^{h \times w \times c \times d} \), and the features for the event modality are represented as \( \mathbf{F}^{\text{3D}}_{\text{event}} {\in} \mathbb{R}^{h \times w \times c \times d} \), where \( h {\times} w {\times} c \) denotes the 3D spatial resolution, and \( d \) is the feature dimension.
The fused multi-modal 3D features are then given by:
\begin{align}
&\mathbf{F}^{\text{3D}}_{\text{fusion}} = \mathbf{F}^{\text{3D}}_{\text{img}} \oplus \mathbf{F}^{\text{3D}}_{\text{event}}.
\end{align}
%
While this method preserves the independence of each modality’s features, it may delay cross-modal interaction, limiting early-stage fusion benefits.



%



\subsection{Event-aided Lifting Module}
%

% 
The 2D-to-3D lifting process is pivotal in aligning 2D image features with 3D spatial structures, which is crucial for precise 3D occupancy predictions. 
%
Accurate lifting ensures effective translation of spatial cues from 2D inputs into coherent 3D representations, significantly impacting the model’s depth and spatial reasoning capabilities~\cite{philion2020lift,li2023bevdepth}. 
Fusion-Based Lifting offers potential benefits,
%
as it effectively combines complementary features from both modalities in the view transformation and enhances the robustness of 3D volume construction. 
%
Following this rationale, we design the \emph{Event-aided Lifting Module (ELM)} for adaptive 2D/3D feature fusion. 
In our ELM design, we combine key and value features from both image and event modalities in a self-attention framework. 
This integration allows the module to fuse spatial and temporal information adaptively.
%

Let the key and value features from the image encoder be represented as \( \mathbf{F}^{k}_{\text{img}} {\in} \mathbb{R}^{N \times d} \) and \( \mathbf{F}^{v}_{\text{img}} {\in} \mathbb{R}^{N \times d} \), respectively, and the corresponding features from the event encoder as \( \mathbf{F}^{k}_{\text{event}} {\in} \mathbb{R}^{N \times d} \) and \( \mathbf{F}^{v}_{\text{event}} {\in} \mathbb{R}^{N \times d} \), where \( N \) represents the number of spatial locations, and \( d \) is the feature dimension.
The image and event features are first added:
\begin{align}
   \mathbf{F}^{k}_{\text{add}} = \mathbf{F}^{k}_{\text{img}} \oplus \mathbf{F}^{k}_{\text{event}}, \quad \mathbf{F}^{v}_{\text{add}} = \mathbf{F}^{v}_{\text{img}} \oplus \mathbf{F}^{v}_{\text{event}}.
\end{align}
Within ELM, self-attention is applied to the aggregated features to generate the fused multi-modal representation. The attention scores are computed using the concatenated key and value features:
\vspace{-1em} 
\begin{equation}
   \text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax} \left( \frac{\mathbf{Q} \cdot \mathbf{K}^T}{\sqrt{d}} \right) \cdot \mathbf{V},
\end{equation}

\begin{equation}
   w = \sigma\left( \text{G-Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) + \text{L-Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) \right),
\end{equation}

\begin{equation}
   \mathbf{F}^{k}_{\text{fusion}} = (1-w) \cdot \mathbf{F}^{k}_{\text{img}} + w \cdot \mathbf{F}^{k}_{\text{event}},
\end{equation}

\begin{equation}
   \mathbf{F}^{v}_{\text{fusion}} = (1-w) \cdot \mathbf{F}^{v}_{\text{img}} + w \cdot \mathbf{F}^{v}_{\text{event}},
\end{equation}
where \( \mathbf{Q} {=} \mathbf{F}^{k}_{\text{add}} \), \( \mathbf{K} {=} \mathbf{F}^{k}_{\text{add}} \), and \( \mathbf{V} {=} \mathbf{F}^{v}_{\text{add}} \).
After obtaining the fused multi-modal features from the self-attention module, deformable attention is applied for voxel querying, allowing the network to adaptively focus on relevant spatial information. Let \( \mathbf{Q}_{\text{voxel}} \) represent the voxel query features, and the deformable attention output is:
\begin{align}
   \mathbf{F}_{\text{voxel}} = \text{DeformableAttention}(\mathbf{Q}_{\text{voxel}}, \mathbf{F}^{v}_{\text{fusion}}).
\end{align}
The output \( \mathbf{F}_{\text{voxel}} \) represents the fused 3D voxel features, which are passed to the segmentation head for the final SSC.


%







