\section{Introduction}
%
\IEEEPARstart{S}{emantic} Scene Completion (SSC), or 3D semantic occupancy prediction, is critical in autonomous driving and robotics, where understanding complex environments is essential for safe navigation~\cite{zhang2024vision3D}. 
The task involves generating a dense scene representation encompassing both geometric and semantic details~\cite{xu2023survey, xu2025survey}.

\begin{figure}[t]
\centering
\includegraphics[width=1.0\linewidth]{figures/core_concepts.pdf}
%
\caption{(a) \textbf{Role of event data in enhancing semantic scene completion:} Under challenging lighting conditions, RGB-based methods struggle to detect low-contrast objects, whereas event data enhances visibility and improves 3D occupancy predictions. 
(b) \textbf{Performance comparison on the corrupted SemanticKITTI-C:} mIoU results across Out-of-Domain and In-Domain scenarios, with and without event data integration, showing scores under various conditions, including Motion Blur (MB), Fog (F), Brightness (B), Darkness (D), and Shot Noise (SN).}
\label{fig:core_concepts}
\end{figure}

%

However, traditional camera-based SSC methods often struggle in challenging lighting and weather conditions due to issues such as motion blur, low-light scenarios~\cite{Gehrig2024LowLatency}, and adverse weather effects~\cite{Kong2023Robo3D}. Event cameras offer a promising visual solution, providing high-temporal-resolution data with advantages like robustness to motion blur and low-latency response~\cite{gallego2020eventbased}. As shown in Fig.~\ref{fig:core_concepts}(a), in low-light scenarios, event data captures low-contrast objects, such as vehicles, that RGB fails to distinguish. These features make event cameras highly suitable for tasks requiring consistent and reliable scene understanding, particularly in degraded visual environments. 
%
The complementary nature of event and RGB data allows event cameras to capture rapid motion dynamics that are often missed by standard cameras, whereas RGB images contribute spatial detail. This bidirectional complementarity enhances 3D occupancy prediction, making autonomous driving safer and more responsive.

\input{tables/Existing_Datasets}

Despite recent advancements and the establishment of benchmarks~\cite{behley2019semantickitti,pan2020semanticposs,tian2024occ3d,wang2023openoccupancy,tong2023scene_as_occupancy},
%
event-based semantic scene completion remains underexplored. There is still a lack of event-modality SSC datasets due to the challenges in creating accurate labels. 
To address this dearth, we introduce \emph{DSEC-SSC} (see Tab.~\ref{table:dataset}), the first real-world event-camera dataset for event-aided semantic scene completion, featuring dedicated processing to ensure reliable ground truth for dynamic objects. 
We also propose a semi-automatic label generation pipeline, which effectively bridges the domain gap between datasets to handle dynamic objects, even in the absence of 2D and 3D ground truth annotations for object detection during the establishment of our DSEC-SSC. 
The semi-automated annotation pipeline (Fig.~\ref{fig:dsec_ssc_pipeline}) simplifies complex point cloud labeling into efficient 2D image-based annotation and enables precise 4D reconstruction of dynamic objects, producing high-accuracy spatiotemporal ground truth. The method is sensor-agnostic, and compatible with various LiDAR brands and models, offering a practical approach to generating accurate 3D/4D occupancy data. 
Moreover, we benchmark a range of classic and recent SSC models on the DSEC-SSC dataset.

%

Further, we propose \emph{EvSSC}, an event-aided framework for Semantic Scene Completion (SSC) that enhances 3D occupancy prediction by integrating event data. 
Central to EvSSC is the \emph{Event-aided Lifting Module (ELM)}, which performs the crucial lifting process, transforming 2D features into a coherent 3D space. 
This transformation is essential in 2D-to-3D occupancy prediction, as it aligns 2D features with the 3D spatial structure of the scene, directly impacting accuracy and stability.
To effectively fuse image and event data, we explore three paradigms (Fig.~\ref{fig:fusion&framework}(a)): fusion-then-lifting, decode-then-fusion, and our preferred fusion-based lifting. Fusion-then-lifting enhances early feature alignment but risks spatial inconsistencies, while decode-then-fusion maintains modality independence but delays interaction. 
Fusion-based lifting integrates event data directly in the lifting stage, preserving spatial fidelity and capturing temporal dynamics.
The ELM in EvSSC uses multi-scale attention during lifting to adaptively merge 2D event and RGB features into 3D space, resulting in robust SSC predictions in dynamic environments.

To verify the effectiveness of our approach, we benchmark EvSSC on the newly introduced DSEC-SSC and the simulated SemanticKITTI-E datasets. 
EvSSC consistently outperforms baseline models, demonstrating significant gains across both transformer-based~\cite{li2023voxformer} and LSS-based~\cite{mei2024sgn} SSC architectures. 
These results underscore the advantages of our fusion-based lifting method in leveraging event data for more accurate 3D occupancy predictions.
Furthermore, to evaluate EvSSCâ€™s robustness in challenging conditions, we conducted tests across five common degradation scenarios in autonomous driving: motion blur, fog, low and high brightness, and noise. 
%
As shown in Fig.~\ref{fig:core_concepts}(b), EvSSC achieves up to a $52.5\%$ relative improvement in mIoU on the corrupted SemanticKITTI-C dataset, confirming its resilience in degraded environments.

In summary, we deliver the following contributions:
\begin{itemize}
    \item We establish \emph{DSEC-SSC}, the first real-world event camera dataset tailored for semantic scene completion, and benchmark a variety of classic and recent SSC models.
    \item We propose \emph{EvSSC}, with \emph{Event-aided Lifting Module (ELM)}, achieving high accuracy in semantic scene completion, as validated through extensive benchmarks on the \emph{DSEC-SSC} and \emph{SemanticKITTI-E} datasets.
    \item We perform comprehensive experiments across diverse corruption scenarios, demonstrating the robustness and superiority of our event-based approach in more challenging conditions.
\end{itemize}

%