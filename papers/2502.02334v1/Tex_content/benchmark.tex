%
\section{DSEC-SSC: an Event-based SSC Dataset}
\label{sec:benchmark}

To address the lack of event-based 3D occupancy prediction datasets, we present DSEC-SSC, the first SSC dataset enhanced with real-world event modality, utilizing a streamlined 4D labeling pipeline that bypasses traditional reliance on 3D object detection or point cloud semantic annotations. 
Our approach includes three main stages: Semantic-guided Object Mapping, Spatiotemporal Purification of Dynamics, and Probability-guided Voxel Refinement, as illustrated in Fig.~\ref{fig:dsec_ssc_pipeline}.
DSEC-SSC includes $6$ sequences for training and $6$ for validation, consisting of $3,488$ frames with $14$ semantic classes, as shown in Fig.~\ref{fig:frequency}, which are grouped into five main categories: vehicle, human, ground, object, and structure. 
Each frame spans a region of $[{-}25.6m,{-}25.6m,{-}3m,25.6m,25.6m,3.4m]$ in 3D space with a voxel resolution of $0.4m$. Our proposed pipeline allows researchers to create their own 3D occupancy datasets with minimal manual effort, even with limited resources like Velodyne-16.

\begin{figure}[!t]
\includegraphics[width=1.0\linewidth]{figures/frequency.pdf}
%\vskip-2ex
\caption{\textbf{Overview of label distribution in the DSEC-SSC dataset.} The label distribution in the DSEC-SSC dataset is presented with the y-axis plotted on a logarithmic scale.
}
\label{fig:frequency}
%\vskip-2ex
\end{figure}

%
\subsection{Semantic-guided Object Mapping}
\label{sec:semantic_guided_dynamic_object_segmentation}
A primary challenge with DSEC~\cite{gehrig2021dsec} is the absence of point cloud semantic annotations, making it difficult to separate dynamic objects. Our 2D semantic-guided approach projects point clouds onto a 2D semantic map $\mathcal{I}_{\text{label}}$ provided by DSEC, enabling segmentation of static ($\mathcal{P}_{\text{Static}}$) and dynamic ($\mathcal{P}_{\text{Dynamic}}$) elements. This reduces manual annotation requirements, allowing efficient object isolation with only targeted human intervention.
After separating the dynamic and static point clouds, we obtain poses through a LiDAR-SLAM~\cite{xu2022fastlio2}, constructing both static maps $\mathcal{M}_{\text{S}}$ and dynamic maps $\mathcal{M}_{\text{D}}$.

%
\subsection{Spatiotemporal Purification of Dynamics}
\label{sec:dynamic_objects_4d_reconstruction}
Accurate and consistent labels are essential for 3D occupancy prediction. To obtain 4D ground truth for semantic scene completion and prevent dynamic objects from suddenly flickering between consecutive frames and trailing artifacts. Handling voxels for dynamic objects is extremely crucial,
which essentially arises from a lack of precise spatiotemporal positioning. 

Therefore, our approach to handling dynamic objects is based on two premises. First, DSEC uses the Velodyne-16 for mapping, resulting in very sparse captures of dynamic objects, making it difficult to map them completely and sometimes missing them altogether in certain frames. 
%
Second, we observed a ``pipeline'' effect in the 3D semantic map~\cite{roldao20223d}, which naturally retains spatiotemporal information. Leveraging this effect, we extract dynamic objects' positions and orientations from the pipeline map, then place reconstructed models of these dynamic objects at corresponding locations to achieve temporally continuous voxel representations.
%
To ensure dynamic objects are not lost when they are missed by LiDAR in certain frames, we incorporate manual supervision. First, we manually annotate ``pipeline'' instances in Birdâ€™s-Eye-View (BEV). Next, we apply linear interpolation to estimate the position and orientation of any missing dynamic objects, providing reliable spatiotemporal information for dynamic objects $\mathcal{M}_{\text{D}}$. As shown in Fig.~\ref{fig:car}, this process effectively removes trailing artifacts from dynamic objects.

\subsection{Probability-guided Voxel Refinement}
\label{sec:probability_guided_voxel_refinement}
Labels obtained in the 2D semantic map are coarse. 
To acquire more accurate labels, the temporal information is leveraged. We propose a method based on Probability-guided Voxel Refinement, which can be divided into two steps: purification of the static scene through point cloud clustering and further refinement based on a voting mechanism.

The semantic labeling errors in the static point cloud stem from two main sources: one is the low accuracy of certain frames in the 2D semantic maps, and the other is occlusions in the 2D semantic map. Therefore, further purification in 3D is also required.
After performing plane fitting to remove the ground $\mathcal{M}_{\text{g}}$, we apply point cloud clustering to the non-ground points $\mathcal{M}_{\text{non-g}}$. 
For each cluster, we assign the semantic label with the highest occurrence probability as the semantic label for that cluster.
% \vspace{-1em}
\begin{align}
&\mathcal{M}=\mathcal{M}_{\text{D}}+\mathcal{M}_{\text{S}},\quad\mathcal{M}_{\text{S}}=\mathcal{M}_{\text{g}}+\mathcal{M}_{\text{non-g}}, \label{eq:static} \\
&\mathcal{M}_{\text{g}}=\arg\max\sum_{i=1}^N \mathbf{1}(d_i<\epsilon),\ d_i=\operatorname{dist}\left({M}_{\text{S}},\mathcal{M}_{\text{g}}\right), \label{eq:ground} \\
&\mathcal{M}_{\text{non-g}}=\left\{\operatorname{K-means}\left(M_{\text{non-g},t}\right)\mid  t=1,2,\dots,N\right\}, \label{eq:non-ground} 
\end{align}
Here, \( \mathbf{1} \) is an indicator function, which counts a point as an inlier if the distance \( d_i \) is less than threshold \( \epsilon \), and \(M_{\text{S}}{\in}\mathcal{M}_{\text{S}}\).

Next, dynamic objects and static backgrounds are aggregated based on temporal information. During this process, we apply voxel-based voting within a bounding box defined by $[{-}25.6m,{-}25.6m,{-}3m,25.6m,25.6m,3.4m]$.
%
\begin{equation}
L_i = \arg\max_{l} \sum_{j} \delta(L_{i,j}, l).
\label{eq:Li}
\end{equation}
Here, \( V_i \) represents the \( i \){-}th voxel, and \( P_{i,j} \) denotes the \( j \){-}th point within \( V_i \), with each point \( P_{i,j} \) associated with a semantic label \( L_{i,j} \). 
The function \( \delta(L_{i,j}, l) \) is the Kronecker delta, which equals $1$ if \( L_{i,j} {=} l \) and $0$ otherwise. 
After the refinement operation, high-quality voxels are obtained.

\begin{figure}[t!]
    \centering
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/withoutDynamicObjectProcess.pdf}
        \caption{ }
        \label{fig:withoutDynamicObjectProcess}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/withDynamicObjectProcess.pdf}
        \caption{ }
        \label{fig:withDynamicObjectProcess}
    \end{subfigure}
    \caption{Comparison of point clouds (a) without and (b) with dynamic object processing.}
    \label{fig:car}
\end{figure}

\begin{figure*}[t!]
\centering
\begin{minipage}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/fusion.pdf}
    %
\end{minipage}

%\vskip0.1ex

\begin{minipage}{1\linewidth}
    \centering
    \includegraphics[width=1\linewidth]{figures/network.pdf}
\end{minipage}
\caption{\textbf{(a) Event-image 2D-to-3D fusion paradigms:} a spectrum of paradigms for feature fusion: (Left) performs 2D feature fusion after encoding, (Right) performs voxel fusion prior to the segmentation head. (Middle) ELM: fuse 2D or 3D features during the lifting process. \textbf{(b) EvSSC}: Given events and RGB images, 2D features are extracted. In ELM, by incorporating camera and level embeddings, the image \(k\)\&\(v\) and event \(k\)\&\(v\) are fused through self-attention to obtain the fusion \(k\)\&\(v\). Mask tokens and voxel queries are added to complete voxel features through deformable attention. 3D occupancy prediction is obtained through the segmentation head.}
\label{fig:fusion&framework}
\end{figure*}

%