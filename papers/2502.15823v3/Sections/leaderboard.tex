\section{Leaderboard based on InductionBench}
To facilitate straightforward comparisons among different LLMs, we introduce a two-part benchmark leaderboard: a \emph{standard leaderboard} and an \emph{exploration leaderboard}. The \emph{standard leaderboard} is based completely on the three function classes we talked about, and this leaderboard simply presents an aggregated score to directly reflect LLM's performance. The \emph{exploration leaderboard} includes a slightly new design of function class and we will present the motivation and details below. 

\subsection{Standard Leaderboard}
The standard leaderboard consists of 1,080 questions spanning three classes of deterministic regular functions: \emph{ISL}, \emph{L-OSL}, and \emph{R-OSL} in equal proportion. Specifically, it includes: \begin{itemize}[leftmargin=*, itemsep=1pt] \item 360 ISL questions, \item 360 L-OSL questions, \item 360 R-OSL questions. \end{itemize}

Within each function class, we have settings for $k\in\{2, 3, 4\}, |\Sigma|\in\{5, 6, 7, 8\}$, and number of rules $\in\{3, 4, 5\}$. Each unique parameter combination has 10 data points, totaling 360 points per function class. The performance metrics \emph{recall}, \emph{precision}, and \emph{compatibility} are computed on a per-setting basis. We then form an overall \emph{weighted average} to account for variations in function-space size: 

\begin{definition}
For a given setting characterized by $(k, |\Sigma|, r)$, the weight $w$ is defined as $\frac{|\Sigma|^k}{\sum\limits_{k'=2}^{k'=4}\sum\limits_{s=5}^{s=8}s^{k'}}$, where $k$ is the Markov window, $|\Sigma|$ is the alphabet size, and $r$ is the minimal rule count.
\end{definition}

For each function class (ISL, L-OSL, R-OSL), we compute a weighted recall, precision, and compatibility according to the above scheme and then take the average of these three scores to produce the final leaderboard score for that class. The overall score across all three classes is the average of those class-wise scores.

Table~\ref{tab:leaderboard} summarizes current leaderboard results for several representative models. Notably, even o3-mini achieves only a 
5.69\% compatibility score, largely because none of the models succeed on tasks where $k=4$. Since those high-complexity settings receive substantially larger weights than cases where $k\in\{2, 3\}$, they disproportionately reduce the overall average.

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccc}
    \toprule
    model & average recall & average precision & average compatibility \\
    \midrule
    Llama-3.1 8b & 0.00&	0.00&	0.00\\
    Qwen2.5-Coder-32B-Instruct	&7.26	&0.66	&0.03\\
    Llama-3.3-70b	&6.55	&5.76&	0.12\\
    DeepSeek-R1-Distill-Llama-70B	&3.84	&5.14	&0.78\\
    o3-mini	&28.93	&43.12	&5.69\\
    \bottomrule
    \end{tabular}
    \caption{Leaderboard Result}
    \label{tab:leaderboard}
\end{table}

To balance the influence of different complexity settings, we additionally report an alternative evaluation metric that replaces each original weight with its logarithm. This approach dampens the dominance of $k=4$ scenarios, yielding a more even distribution of weights across the benchmark’s parameter space.

\begin{table}[!ht]
    \centering
    \begin{tabular}{lccc}
    \toprule
    model & average recall & average precision & average compatibility \\
    \midrule
    Llama-3.1 8b&	0.00	&0.00	&0.00\\
    Qwen2.5-Coder-32B-Instruct	&7.48	&6.60	&0.48\\
    Llama-3.3-70b	&8.71	&7.50	&0.87\\
    DeepSeek-R1-Distill-Llama-70B	&23.17&	24.66&	8.63\\
    o3-mini	&57.58	&63.89&33.93\\
    \bottomrule
    \end{tabular}
    \caption{Leaderboard Result with Log Weight}
    \label{tab:leaderboard_log}
\end{table}


\subsection{Exploration Leaderboard}
A key concern in using subregular function classes (\emph{e.g.}, ISL, L-OSL, R-OSL) is that polynomial-time learning algorithms already exist for these classes, potentially allowing a trivial ``hack'' to achieve artificially high performance. Though we advocate not using the provbly correct algorithm for task solving so that we can genuinely evaluate LLM's inductive reasoning ability, to make sure, we introduce an \emph{exploration leaderboard} that focuses on \emph{Input-Output Strictly Local (IOSL)} functions: a more speculative class for which no known algorithm can reliably learn the entire function from finite data in finite time.

\paragraph{Rationale.} Since IOSL lacks a proven polynomial-time learning procedure, successful performance here would more credibly reflect genuine inductive reasoning rather than the application of a known ``shortcut'' algorithm. Furthermore, IOSL functions have not been deeply studied in the literature, offering an opportunity to see whether LLMs can advance this open research area.


This is the definition of IOSL:
\begin{definition}[IOSL]
A function f is IOSL if there is a $k$ such that for all $u_1, u_2\in\Sigma^*$, if $\textsc{Suff}^{k-1}(u_1) = \textsc{Suff}^{k-1}(u_2)$ and $\textsc{Suff}^{k-1}(f(u_1)) = \textsc{Suff}^{k-1}(f(u_2))$, then $\textsc{tails}_f(u_1) = \textsc{tails}_f(u_2)$.
\end{definition}

In essence, this condition requires the model to distinguish between input-based and output-based Markovian triggers, making the learned transformation highly non-trivial if no pre-existing algorithm is used.

\paragraph{Leaderboard Setup.} The IOSL-based leaderboard contains 1{,}080 datapoints, mirroring the standard leaderboard in overall structure: $k\in\{2, 3, 4\}, |\Sigma|\in\{5, 6, 7, 8\}$, number of rules $\in\{3, 4, 5\}$. For each setting, there are 30 datapoints per setting (for equivalence to the standard leaderboard’s size).

Since IOSL is not known to admit a finite-characteristic sample or minimal representation in the same sense as the deterministic classes, we introduce two adaptations for evaluation:

\begin{enumerate}[leftmargin=*, itemsep=1pt] \item \textbf{Sample Size.} We arbitrarily fix the sample size at $2*|\Sigma|^k$, as no characteristic sample is theoretically guaranteed.

\item \textbf{Evaluation Metrics.} We focus primarily on \emph{compatibility}, as recall and precision hinge on the assumption of a unique minimal-length description, which may not exist for IOSL. If a model's generated rule set is compatible with the data, we then check whether its description length is shorter, identical, or longer than our function's reference length. A longer description indicates a definite failure to produce a minimal representation; shorter or equal does not guarantee minimality, but it at least suggests the model avoids obvious redundancy. \end{enumerate}

By presenting both a standard leaderboard (subregular classes with known learnability) and an exploration leaderboard (IOSL with no established finite-data algorithm), we offer a balanced view: models can demonstrate success in theoretically well-understood tasks while also exploring novel, under-constrained function classes—thereby reducing the concern that high performance might merely reflect an existing ``hack.''


