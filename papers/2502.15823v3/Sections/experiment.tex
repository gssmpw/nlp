\section{Main Experiment}
\paragraph{Experiment Setting} We evaluate using zero-shot chain-of-thought prompting on six SOTA LLMs, including Llama-3.3-70b \citep{dubey2024llama} with FP8 quantization, Llama-3.1-405b with FP8 quantization, GPT-4o \citep{hurst2024gpt}, DeepSeek-V3 \citep{liu2024deepseek}, o1-mini\citep{jaech2024openai}, and o3-mini. For all models, we evaluate with all settings including $k\in\{2, 3, 4\}$, $|\Sigma|\in\{2, 3, 4\}$, number of rules $\in\{2, 3, 4\}$, and sample set size to be 1, 2, 3, 4 times larger than the characteristic sample. For each setting, we randomly generate 10 functions $f$ and corresponding input-output sample $\mathcal{D}$ to calculate the result. As o1-mini and o3-mini perform much better than other models, in addition, we evaluate on two more complex settings with $k\in\{4, 5\}, |\Sigma| = 5$.

\paragraph{Evaluation Metrics}
For each experiment setting, we leverage three metrics to evaluate performance: Precision, Recall, Compatibility. Let $R$ be the unique ground-truth rule set of minimal length for function $f$, $P$ be the predicted rule set generated by LLM, $\mathcal{D}$ be the provided sample set in the context on which we evaluate the correctness of $P$. \\
\textbf{Precision} measures how many of the predicted rules are correct relative to all rules the model generated: $\frac{|R\cap P|}{|P|}$. captures the proportion of the model's rules that align exactly with the ground-truth rules. A higher precision indicates fewer unnecessary/redundant rules.\\
\textbf{Recall} measures what fraction of the ground-truth rules the model successfully recovered: $\frac{|R\cap P|}{|R|}$. A higher recall reflects the model's ability to cover all aspects of the correct transformation.\\
\textbf{Compatibility} measures whether applying the predicted rule set $P$ to each input in the sample set $\mathcal{D}$ yields the correct output:
\begin{align*} 
\text{Compatibility}(P, \mathcal{D}) = 
\begin{cases} 
1 & \text{if } \forall (x_i, y_i)\in \mathcal{D}, P(x_i) = y_i\\
0 & \text{otherwise}
\end{cases} 
\end{align*}

Compatibility is the most fundamental measure, as it verifies whether the generated function accurately reproduces all observed inputâ€“output pairs in $\mathcal{D}$. A trivial way to achieve perfect compatibility is to include every pair $(x_i, y_i[-1])\in \mathcal{D}$ as an independent rule; however, doing so results in very low precision, indicating a failure to capture the underlying generalizable structure of the function. Note that all results presented are expressed as percentages.

\subsection{Model Performance Comparison}
\begin{table*}[!ht]
    \centering
    \renewcommand{\arraystretch}{1.1}
    \resizebox{14.5cm}{!}{
    \begin{tabular}{l ccc ccc ccc}
        \toprule
           \multirow{2.5}{*}{\bf Models} & \multicolumn{3}{c}{\bf ISL} & \multicolumn{3}{c}{\bf L-OSL} & \multicolumn{3}{c}{\bf R-OSL}\\
          \cmidrule(lr){2-4} \cmidrule(lr){5-7}  \cmidrule(lr){8-10}
           & recall  & precision & compatibility & recall  & precision & compatibility & recall  & precision & compatibility \\
         \midrule
           Llama-3.3 70B & 10.00 & 5.32 & 0.00 & 10.00 & 6.33 & 0.00 & 10.00 & 10.83 & 0.00\\
           Llama-3.1 405B & 10.00 & 3.75 & 0.00 & 6.67 & 1.10 & 0.00 & 13.33 & 1.85 & 0.00\\
           GPT-4o & 10.00 & 2.67 & 0.00 & 13.33 & 3.82 & 0.00 & 16.67 & 6.73 & 0.00\\
           DeepSeek-V3 & 13.33 & 2.46 & 0.00 & 23.33 & 2.73 & 0.00 & 3.33 & 0.25 & 0.00\\
           o1-mini & 36.67 & 22.09 & 0.00 & 43.33 & 32.12 & 0.00 & 26.67 & 17.58 & 0.00\\
           o3-mini & \textbf{73.33} & \textbf{59.58} & \textbf{10.00} & \textbf{66.67} & \textbf{69.17} & \textbf{10.00} & \textbf{63.33} & \textbf{62.00} & \textbf{30.00}\\
           %o1-preview & 40.00 & 56.67 & 0.00 & 60.00 & 82.67 & 20.00 & 13.20 & 15.00 & 0.00\\
           \bottomrule
    \end{tabular}
    }
    \caption{Zero-shot CoT benchmark result with $k = 4$, $|\Sigma|$ = 4, number of rules = 3, sample size = 2}
    \vspace{-10pt}
    \label{tab:main}
\end{table*}
Table~\ref{tab:main} showcases model performance under particularly challenging conditions: $k=4, |\Sigma| = 4$ with 3 rules and sample size twice that of the minimal size of characteristic sample (detailed analysis on the impact of sample size is presented in Section 5.2). As seen in the table, compatibility scores collapse to 0 for all models except o3-mini, which also achieves relatively modest compatibility overall. This pattern highlights the difficulty that current LLMs face when required to track slightly broader contexts window even under very limited vocabulary size = 4. Full results are presented in Tables \ref{tab:ISL_main}, \ref{tab:LOSL_main}, \ref{tab:ROSL_main} in Appendix. Table~\ref{tab:harder} further reports the performance of o1-mini and o3-mini under slightly more challenging settings. Although both models generally exhibit non-trivial recall and precision, their compatibility scores consistently remain at or near zero. It is important to note that ISL, L-OSL, and R-OSL are the simplest function classes within the subregular hierarchy of string-to-string mappings. Thus, despite the strong performance of state-of-the-art models on benchmarks in coding \citep{jain2024livecodebench}, mathematics \citep{mirzadeh2024gsm}, and knowledge-intensive tasks \citep{wang2024mmlu}, they falter on this elementary inductive reasoning task. 

\begin{table}[!ht]
    \centering
    \begin{tabular}{l c ccc ccc}
        \toprule
          \multirow{2.5}{*}{\bf Models}& \multirow{2.5}{*}{\bf Settings}& \multicolumn{3}{c}{\bf k = 4} & \multicolumn{3}{c}{\bf k = 5}\\
          \cmidrule(lr){3-5} \cmidrule(lr){6-8} 
          & & R  & P & C & R  & P & C\\
            \midrule \multicolumn{8}{c}{\textbf{ISL}} \\
            \midrule
           \multirow{2}{*}{o1-mini} & rules = 4 & 25.00 & 12.21 & 0.00 & 10.00 & 9.10 & 0.00\\
           & rules = 5 & 36.00 & 40.41 & 0.00 & 10.00 & 3.14 & 0.00\\
           \hdashline
           \multirow{2}{*}{o3-mini} & rules = 4 & 37.50 & 49.83 & 0.00 & 27.50 & 30.75 & 0.00\\
           & rules = 5 & 42.00 & 58.67 & 0.00 & 20.00 & 38.33 & 0.00\\
           \midrule \multicolumn{8}{c}{\textbf{L-OSL}} \\
            \midrule
           \multirow{2}{*}{o1-mini} & rules = 4 & 32.50 & 37.34 & 0.00 & 15.00 & 29.33 & 10.00\\
           & rules = 5 & 28.00 & 23.17 & 0.00 & 8.00 & 4.61 & 0.00\\
           \hdashline
           \multirow{2}{*}{o3-mini} & rules = 4 & 57.50 & 58.93 & 0.00 & 22.50 & 39.26 & 0.00\\
           & rules = 5 & 48.00 & 71.38 & 0.00 & 10.00 & 23.67 & 0.00\\
           \midrule \multicolumn{8}{c}{\textbf{R-OSL}} \\
            \midrule
           \multirow{2}{*}{o1-mini} & rules = 4 & 17.50 & 22.33 & 0.00 & 12.50 & 7.63 & 0.00\\
           & rules = 5 & 16.00 & 15.42 & 0.00 & 18.00 & 22.82 & 0.00\\
           \hdashline
           \multirow{2}{*}{o3-mini} & rules = 4 & 45.00 & 43.76 & 10.00 & 20.00 & 50.00 & 0.00\\
           & rules = 5 & 38.00 & 55.17 & 0.00 & 14.00 & 36.17 & 0.00\\
           \bottomrule
    \end{tabular}
    \caption{o1-mini and o3-mini results (R = Recall, P = Precision, C = Compatibility) on harder setting with $k\in\{4, 5\}, |\Sigma| = 5$, sample size = 2}
    \vspace{-10pt}
    \label{tab:harder}
\end{table}



\subsection{Impact of Different Factors}
Figure~\ref{fig:ISL_figure} shows how five models (Llama3.3-70B, Llama3.1-405B, GPT-4o, DeepSeek-V3, and o1-mini) perform on various ISL tasks, organized by three key parameters: the context window size $k$, the vocabulary size $|\Sigma|$, and the minimal number of rules required to describe the function. The top row of panels presents recall, the middle row presents precision, and the bottom row presents compatibility. We did not include o3-mini here because its performance is way stronger than all other five models and thereby in order to show the impact of various factors, we omit this model for better visual clarity. Based on these figures, the impact of $k$, $|\Sigma|$, and number of rules become very clear: (1) increasing $k$ markedly reduces recall, precision, and compatibility (2) $|\Sigma|$ does not impact the performance much (3) the number of minimal rules can substantially affect compatibility with large $k$ and $|\Sigma|$. 

\paragraph{Impact of $k$.} Across all models, moving from $k=2$ to $k=4$ markedly reduces recall, precision, and compatibility. This trend underscores how increasing the context window increases the complexity of the underlying ISL functions and making it more challenging for current LLMs to learn the correct transformations. Longer look-ahead requires the model to track additional input context, which can overload its capacity to induce reliable rules.

\paragraph{Impact of $|\Sigma|$.} In contrast, enlarging the vocabulary from $|\Sigma| = 2$ to $|\Sigma| = 4$ does not consistently degrade performance to the same degree as increasing $k$. While some models exhibit slight declines in recall or precision with a larger alphabet, these effects are neither as uniform nor as pronounced as those induced by a bigger Markov window. This finding suggests that the breadth of symbol variation matters less than the depth of sequential dependencies.


\paragraph{Impact of the Number of Rules.} Notably, the number of minimal rules can substantially affect compatibility. When $k=2$ and $|\Sigma| = 2$, a comparatively small search space, changing the number of rules does not drastically alter compatibility. However, under more demanding scenarios where $k\in\{3, 4\}$, the data indicate that adding rules can cause compatibility to plummet. In many cases, having just one rule still yields nontrivial compatibility, whereas introducing a second or third rule often overwhelms the models, resulting in compatibility scores near 0. 

\input{Sections/tables/ISL_figure}


\paragraph{Impact of Examples} We further examined whether few-shot prompting could enhance model performance. In our experiments with Llama-3.3 70B, we varied the number of in-context examples (1-shot, 2-shot, and 3-shot) to determine their effect on the model's ability to induce the correct function representation. The results indicate that when both the vocabulary size and the context window $k$ are small, adding more examples improves performance across the evaluated metrics. However, as the complexity increasesâ€”with larger values of $k$ and vocabulary sizesâ€”the benefits of additional few-shot examples become negligible. These findings suggest that while few-shot learning is beneficial for simpler settings, its efficacy diminishes in more complex inductive tasks. Experiment results are presented in Tables \ref{tab:few_shot_ISL}, \ref{tab:few_shot_LOSL}, and \ref{tab:few_shot_ROSL}.


\input{Fig/shot_k}


\paragraph{Robustness} We assess the stability of inductive reasoning by varying the number of inputâ€“output pairs provided to the model. The x-axis represent $\frac{|\mathcal{D}|}{|S|}$ where $S$ is the minimal set of examples needed to guarantee learnability of the underlying function. The hypothesis is that if the model were performing genuine logical or inductive reasoning, we would expect performance to remain stable or even improve as more data points become available, since these points should further clarify the underlying function. Figure~\ref{fig:sample_size_impact} illustrates how average compatibility decreases steeply as the number of provided inputâ€“output examples increases. This drop suggests that the LLM's reasoning process is not robustly inductive: rather than refining its hypothesis with additional data, the model appears to become confused or overwhelmed, leading to poorer overall performance. Consequently, these findings highlight the limited robustness of current LLMs' inductive reasoning, particularly in scenarios where increasing the available data should theoretically facilitate, rather than hinder, function inference.

\input{Fig/sample_size_multiple}

\input{Fig/context_length}

Moreover, to isolate the influence of context length from the effect of adding genuinely new data, we conduct an additional experiment in which we simply extending the context size by \emph{repeating} the minimal characteristic sample without introducing novel inputâ€“output pairs. Comparing Figures~\ref{fig:sample_size_impact} and \ref{fig:context_length_impact} reveals that while compatibility does diminish with increased context length (\emph{e.g.}, at a multiple of 2), the decline is relatively small when scaling further to multiples of 3, 4, or 5. By contrast, when truly new datapoints are added (and not just repeated), compatibility plummets nearly to zero for multiples of 4 and 5. These results confirm that the primary driver of performance degradation is the inclusion of additional, distinct datapoints rather than simply lengthening the context.

\paragraph{Error Type Analysis} We further examined the specific types of errors made by LLMs when their predicted functions failed to match the ground-truth dataset. At a high level, we distinguish between \emph{missing rules} (leading to low recall) and \emph{wrong rules} (leading to low precision).

\textbf{Missing Rules:}
These refer to ground-truth rules that do not appear in the model's predicted rule set. We classify missing rules into three subtypes: 
\begin{enumerate} \item \emph{Too General.} Although a certain ground-truth rule $r:c\circ u\to v$ was missed, there exists a corresponding predicted rule $r': c'\circ u'\to v'$ that over-generalizes. Specifically, the condition $c'$ is a \emph{proper suffix} of $c$, causing $r'$ to apply more broadly than intended.
\item \emph{Too Specific.} The opposite of the above: a predicted rule condition $c'$ is a proper \emph{extension} of $c$, thus applying too narrowly and failing to match some instances that should have been captured by the ground-truth rule.
\item \emph{Completely Missed.} No predicted rule over-generalizes or under-generalizes the ground-truth rule; in other words, this pattern is simply absent from the predicted rule set altogether. \end{enumerate}

\textbf{Wrong Rules:}  
These refer to rules present in the model's predicted set that do not exist in the ground truth. We categorize such rules into four types:
\begin{enumerate}
    \item \emph{Too General.} The rule \(r': c' \circ u' \to v'\) is overly broad, applying in contexts where the ground truth does not. This typically arises when \(c'\) is a proper suffix of some genuine condition \(c\) and thus fails to capture necessary constraints.  
    \item \emph{Too Specific.} The rule narrowly addresses only a subset of the intended patterns (e.g., by employing a condition \(c'\) that is an extension of the legitimate condition \(c\)), thereby missing broader contexts that should have matched.  
    \item \emph{Correct Condition but Wrong Transformation.} Here, the predicted rule accurately identifies the correct condition \(c'\) and target input character \(u'\), but the transformation \(v'\) is incorrect.  
    \item \emph{Completely Wrong.} None of the above criteria apply: the rule's condition and transformation are both inconsistent with the ground truth, indicating a fundamental misunderstanding.
\end{enumerate}


\begin{figure*}[!ht]
    \centering
    \includegraphics[scale=0.25]{Fig/error_analysis.pdf}
    \caption{Error Type Analysis}
    \label{fig:error_analysis}
\end{figure*}

We present a breakdown of error types for three models in Figure\ref{fig:error_analysis}: Llama3.3-70B, o1-mini, and o3-mini. Among \emph{missing rules}, the most common issue is \textbf{completely missed}, where the model fails to identify the relevant pattern at all. The second most frequent error is \textbf{too general}, suggesting that the predicted condition is shorter than needed, thereby overgeneralizing the intended behavior. In contrast, \textbf{too specific} errors in this category are relatively rare. Among \emph{wrong rules}, the majority are \textbf{completely wrong}, followed by a notable fraction of \textbf{too general}. Although there is also a non-negligible number of \textbf{too specific} errors, these tend to occur when a single ground-truth rule (e.g., $ab\circ c\to b$ is replaced by multiple subcases (\emph{e.g.}, $aab\circ c\to b, bab\circ c\to b, cab\circ c\to b$, indicating the model has uncovered individual instances but failed to unify them into a concise representation. Finally, \textbf{correct condition but wrong transformation} occurs relatively infrequently, implying that once the model infers the correct condition pattern, it typically produces the correct transformation.

\subsection{Summary of findings}
Overall, our experiments reveal four main insights into the inductive reasoning performance of current LLMs:
\begin{itemize}
    \item Context window size $k$ dominates complexity: Increasing $k$ from 2 to 4 significantly degrades recall, precision, and compatibility, underscoring how longer look-ahead windows intensify the complexity of ISL functions. 
    \item Number of Rules increases difficulty under large hypothesis space: The number of minimal rules required can drastically lower compatibility in more challenging settings with large $k$ adn $|\Sigma|$, indicating that managing multiple interacting rules overwhelms many models.
    \item Few-shot examples do not help much: Few-shot examples help in simpler configurations but yield diminishing returns as $k$ adn $|\Sigma|$ growsâ€”suggesting that, past a certain complexity threshold, additional examples do not compensate for the model's limited inductive capacity.
    \item Current LLMs are very unrobust: Providing more novel data should theoretically clarify function patterns, yet performance often plummets, reflecting a fragility in inductive reasoning.
    \item Error analysis shows that missing rules are most frequently ``completely missed'' or ``too general,'' while wrong rules often end up ``completely wrong'' or again ``too general.'' Only a small fraction are ``too specific'' or feature a ``correct condition but wrong transformation,'' indicating that once models identify the right condition, they typically produce the correct transformation.
\end{itemize}

Taken together, these findings highlight fundamental limits in current LLMs' inductive reasoning. Even state-of-the-art models often fail as complexity grows, or when confronted with more data than their inductive mechanisms appear able to systematically absorb.



