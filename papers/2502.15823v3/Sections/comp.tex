\section{Computational Complexity in Inductive Reasoning}
\label{sec:com}
\textbf{InductionBench} uses string-to-string transformation/functions as a proxy to study inductive reasoning, which has established computational complexity hierarchy\citep{roche1997finite, engelfriet2001mso}. We focus on the subregular hierarchy, the hierarchy under regular functions. Though with limited expressive power, our experiments show that these classes already present substantial challenges for LLMs.

Specifically, we limit our attention to three classes of deterministic regular functions—\emph{Left Output-Strictly-Local} (L-OSL), \emph{Right Output-Strictly-Local} (R-OSL), and \emph{Input-Strictly-Local} (ISL), whose positions in the subregular hierarchy are illustrated in Figure~\ref{fig:subregular} \citep{heinz2018computational}. These classes represent
the lowest-complexity tier for string-to-string mappings within the subregular hierarchy. They are proper subclasses of subsequential function class and, more broadly, of weakly-deterministic class and non-deterministic class, which are themselves subsets of the regular function classes. Although we do not elaborate on the complete regular function hierarchy here, it is important to note that the ISL, L-OSL, and R-OSL classes are among the simplest in this framework.

Strictly local functions can be seen as operating with a fixed amount of look-ahead, similar to Markov processes. They are \emph{provably learnable in polynomial time from polynomially sized samples} \citep{chandlee2014learning, de1997characteristic, chandlee2015output, jardine2014very}. Moreover, prior work has shown that an algorithm exists to learn the unique (up to isomorphism) smallest subsequential finite-state transducer that represents such ISL, L-OSL, R-OSL functions \citep{satta1997string, arasu2009learning}. This property allows us to evaluate not only whether LLMs can discover the correct patterns but also whether they can identify the simplest or most concise representation consistent with the data. %Our experiments thus aim to determine if current LLMs can effectively detect and generalize the underlying ISL, L-OSL, and R-OSL functions from finite data, as well as whether they can pinpoint the minimal transformations required to match observed input–output behavior.

%Within the broader category of regular functions, we focus on the deterministic function classes: Left Output-Strictly-Local ($L-OSL$), Right Output-Strictly-Local (R-OSL), and Input-Strictly-Local ($ISL$) functions \citep{heinz2018computational}, whose function classes hierarchical relationship in the subregular hierarchy are presented in Figure~\ref{fig:subregular}. 
%one can distinguish between non-deterministic and deterministic classes \citep{heinz2018computational}. Non-deterministic regular functions are a strict superset of \emph{weakly-deterministic} regular functions. The deterministic classes, which lie inside this weakly-deterministic region (as shown in Figure~\ref{fig:subregular}), include left-subsequential, right-subsequential, Left Output-Strictly-Local ($L-OSL$), Right Output-Strictly-Local (R-OSL), and Input-Strictly-Local ($ISL$) functions. Notably, $ISL$, $L-OSL$, and R-OSL overlap without subsuming one another; $L-OSL$ is a subclass of left-subsequential, and R-OSL is a subclass of right-subsequential. Both left-subsequential and right-subsequential classes, in turn, belong to the weakly-deterministic subset of regular functions.
%We specifically target these deterministic classes because they are provably learnable in polynomial time using polynomially sized samples \citep{chandlee2014learning, de1997characteristic}. Also, they are among the simplest classes of functions in string-to-string mappings. Intuitively, these classes can be viewed as transformations with finite look-ahead, akin to Markov processes, thus making them theoretically tractable for inductive learning. In this work, we limit our scope to $ISL$, $L-OSL$, and R-OSL functions, the simplest computational classes within the subregular hierarchy, and investigate how effectively LLMs can handle these inductive reasoning tasks.

\subsection{Preliminary}
Before providing the definitions of the three function classes, we first introduce the fundamental mathematical notations and formal definitions underpinning our discussion of string-to-string transformations and their properties.

Let $\Sigma$ be a finite alphabet. We denote by $\Sigma^*$ the set of all finite strings over $\Sigma$, and by $\Sigma^{\leq n}$ the set of all strings over $\Sigma$ of length at most $n$. The empty string is denoted as $\lambda$. The set of prefixes of a string $w$ is denoted as \textsc{Pref}($w$), defined as $\{p\in \Sigma^*\mid \exists s\in \Sigma^* s.t. w = ps\},$ and the set of suffixes of $w$ denoted as \textsc{Suff}($w$), defined as $\{s\in \Sigma^*\mid \exists p\in \Sigma^* s.t. w = ps\}.$ The longest common prefix of a set of strings $S$ is denoted as \textsc{lcp}($S$), defined as 
\begin{equation}
    p\in\cap_{w\in S}\textsc{Pref}(w) \text{ such as } \forall p'\in\cap_{w\in S}\textsc{Pref}(w), |p'| < |p|.
\end{equation}
For any function $f:\Sigma^*\to\Gamma^*$ and $w\in\Sigma^*$, let the tails of $w$ with respect to $f$ be defined as
\begin{equation}
    \textsc{tails}_f(w) = \{(y, v)\mid f(wy) = uv \text{ and }
    u = \textsc{lcp}(f(w\Sigma^*))\}.
\end{equation}
Intuitively, \textsc{tails}$_f(w)$ collects all possible continuations $(y, v)$ by appending $y$ to $w$. It summarizes how $f$ might extend beyond the partial input $w$. The total number of distinct tails across all strings in $\Sigma^*$ provides a measure of how many different non-trivial local transformation $f$ encodes.


\subsection{Function Class Definition}
Based on the concepts outlined above, we define the three function classes.
\begin{definition}[ISL]
A function f is ISL if there is a $k$ such that for all $u_1, u_2\in\Sigma^*$, if $\textsc{Suff}^{k-1}(u_1) = \textsc{Suff}^{k-1}(u_2)$, then $\textsc{tails}_f(u_1) = \textsc{tails}_f(u_2)$.
\end{definition}

In simpler terms, this means that the output at each position in the string depends only on the preceding $k-1$ characters of the \emph{input}, making the transformation \emph{Markovian} with respect to the input. Below is a simple example:
\begin{exmp}
Suppose a function $f:\{a, b\}^*\to\{a, b\}^*$ rewrites each $b$ to $a$ \emph{only} if it appears after the input substring $ba$. In this scenario, we have $k=3$, and there are two distinct \emph{tails}:
\begin{equation*}
    \textsc{tails}_f(w) = \{(\lambda, \lambda),  (b, a), (bb, ab), (ab, ab) \dots\}, \quad 
    \forall w\in\Sigma^*\text{ such that } ba\in\textsc{suff}(w) 
\end{equation*}
and
\begin{equation*}
\textsc{tails}_f(w') = \{(\lambda, \lambda),  (a, a), (bb, bb), (ab, ab) \dots\}, \quad \forall w'\in\Sigma^*\text{ such that } ba\notin \textsc{suff}(w')
\end{equation*}
\end{exmp}

These tails indicate how the function's behavior shifts depending on whether the immediate context ends in $ba$. Such context-dependent tails also highlights that ISL functions can be effectively characterized or represented by local input constraints.

\begin{definition}[L-OSL]
A function f is L-OSL if there is a $k$ such that for all $u_1, u_2\in\Sigma^*$, if $\textsc{Suff}^{k-1}(f(u_1)) = \textsc{Suff}^{k-1}(f(u_2))$, then $\textsc{tails}_f(u_1) = \textsc{tails}_f(u_2)$.
\end{definition}

In other words, the output at each position in the transformed string depends only on the preceding $k-1$ characters of the \emph{output} itself, rather than on the input. This property can be understood as a form of Markovian process \emph{on the output}. Below is a simple example:
\begin{exmp}
Suppose a function $f$ rewrites each $b$ to $\lambda$ \emph{only} if it appears after the output substring $ba$. In this scenario, we have $k=3$, and there are two distinct \emph{tails}:
\begin{multline*}
    \textsc{tails}_f(w) = \{(\lambda, \lambda), (a, a), (b, \lambda),
    (bb, \lambda), (ab, ab), (ba, a), \dots\} \\
    \forall w\in\Sigma^*\text{ such that } ba\in\textsc{suff}(f(w))
\end{multline*}
and
\begin{multline*}
    \textsc{tails}_f(w) = \{(\lambda, \lambda), (a, a), (b, b),
    (bb, bb), (ab, ab), (ba, ba) \dots\} \\
    \forall w\in\Sigma^*\text{ such that } ba\notin\textsc{suff}(f(w))
\end{multline*}
\end{exmp}

While L-OSL depends preceding output symbols to the ``left'', R-OSL functions depends on a limited number of \emph{future} output symbols to the ``right''. Conceptually, one can view R-OSL as analogous to L-OSL, except that the input is processed in reverse order. Although both belong to the broader OSL paradigm, they are \emph{incomparable} classes: each can express transformations the other cannot. The formal definition of R-OSL follows:

\begin{definition}[R-OSL]
A function f is R-OSL if there is a $k$ such that for all $u_1, u_2\in\Sigma^*$, if $\textsc{Suff}^{k-1}(f(u_1^{-1})) = \textsc{Suff}^{k-1}(f(u_2^{-1}))$, then $\textsc{tails}_f(u_1^{-1}) = \textsc{tails}_f(u_2^{-1})$.
\end{definition}

Intuitively, this class of functions can be viewed as a \emph{rightward} Markovian process on the output. Each output symbol is determined not by the preceding symbols as in L-OSL but by the next $k-1$ symbols that will appear in the output.

The three classes, ISL, L-OSL, and R-OSL, are each deterministic and exhibit Markovian behavior, yet remain pairwise incomparable within the broader subregular hierarchy. In this work, we further restrict our attention to functions that involve \emph{substitution} which replaces one character with another and \emph{deletion} which maps a character to the empty string $\lambda$.

\subsection{Learnability}
The three function classes are \emph{identifiable in polynomial time using a polynomially sized characteristic sample} \citep{chandlee2014learning, chandlee2015output}. In other words, there exists a polynomial-time algorithm that, given sufficient data for a target function $f$, can produce a representation $\tau$ that satisfies $f(w) = \tau(w)$ for every $w\in\Sigma^*$. In other words, once sufficient data is presented, one can reliably recover a function equivalent to $f$ on all possible inputs. This learnability property underpins the value of these classes as testbeds for inductive reasoning, since the data requirement remains polynomial and successful inference is theoretically guaranteed.

We formalize ``sufficient data'' as the minimal set of input–output pairs needed to learn a $k$-strictly local function $f$, which is known as characteristic sample. Adapting the original definition\footnote{simplified from original definition} for clarity \citep{chandlee2014learning, chandlee2015output}, we define:
\begin{definition}[Characteristic Sample]
For a given $k$-ISL $f$, the characteristic sample $S$ is defined as $\{(w, w')\mid w\in \Sigma^{\leq k}\land f(w) = w'\}$. For a given $k$-OSL $f$, the characteristic sample $S$ is defined as $\{(w, w')\mid w'\in \Sigma^{\leq k}\land f(w) = w'\}$.
\end{definition}

If a provided dataset contains such characteristic sample, a learning algorithm can reconstruct a representation of $f$ that matches its behavior on every string in $\Sigma$. Accordingly, in the context of LLMs, we expect that providing this dataset as in-context examples should enable the model to induce the underlying string-to-string mapping. 

\subsection{Unique Function Representation}
Beyond verifying that a model can accurately discover a function from data, we also investigate how succinctly the model describes its inferred rules. This aspect is of both theoretical and practical interest: a minimal or \emph{most concise} representation not only offers interpretability advantages but can also reflect the model's capacity for truly generalizable, rather than merely enumerative, learning. 

One function can be represented or written in a non-unique way. For instance, consider an ISL function $f_1$ with $k=2$ over $\Sigma = \{a, b\}$ that maps the input character $a$ to $b$ when it comes after $b$, that rewrites each $a$ to $b$ only if the preceding character is $b$, while leaving other substrings unchanged. One concise description is:
\begin{equation}
% \hspace{-3mm}\begin{aligned}
\hspace{-3mm} f_1(w) {=}\,
  \begin{cases}
 f_1(w_1)ba^{-1}f_1(aw_2), \\
 \quad\quad \parbox[t]{0.6\linewidth}{
        if $w_1$ ends with $b$ and 
        $w {=}w_1$ $aw_2$ for some $w_1, w_2 {\in} \Sigma^*$
      }\\
 w, \,\,\,\,\, \text{otherwise}
  \end{cases}
% \end{aligned}
\hspace{-10pt} 
\end{equation}

An alternative yet more verbose description of the same function might redundantly enumerate multiple cases:
\begin{equation} 
\hspace{-3mm} f_1'(w) {=}
\!\!\begin{cases} 
f_1'(w_1)ba^{-1}f_1'(aw_2), \\
 \quad\quad \parbox[t]{0.6\linewidth}{
 if $w_1$ ends with $ab$ and $w = w_1aw_2$ for some $w_1, w_2$ $\in\Sigma^*$ } \\
f_1'(w_1)ba^{-1}f_1'(aw_2) \\
\quad\quad \parbox[t]{0.6\linewidth}{ 
 if $w_1$ ends with $bb$ and $w = w_1aw_2$ for some $w_1, w_2$ $\in\Sigma^*$ }\\
w,\,\,\,\,\, \text{otherwise}
\end{cases} 
\hspace{-10pt}
\end{equation}

Although these two representations encode the same function, the second contains repetitive conditions and fails to emphasize that the output of $f_1$ depends solely on the single preceding character instead of the penultimate character. 

Because these functions admit a \emph{unique} minimal representation (up to isomorphism) \citep{chandlee2014learning, oncina1991inductive}, we can directly compare the function produced by an LLM to the ground-truth minimal form. In doing so, we evaluate whether the model not only \emph{discovers} the correct transformation but also \emph{simplifies} it to the most parsimonious description possible—an essential indicator of robust inductive reasoning.

\subsection{Rule-based Representation}
To streamline the generation and parsing of function representations, we employ a simplified notation wherein each transformation is written as ``condition $\circ$ target character $\to$ output of the target character'' \citep{bird1994one}.  In this notation, the \emph{condition} represents the minimal substring needed to trigger a transformation, while any input substring not matching this condition remains unchanged. For instance, in the earlier example, this approach permits a concise notation $b\circ a\to b$, indicating that the input $a$ is mapped to $b$ when it comes after $b$; otherwise, the input string remains unaltered. This concise, rule-based format simplifies both the model's output generation (by reducing complex functional descriptions) and our subsequent evaluation, as the applicable transformations can be easily parsible and verified.

To demonstrate the simplicity of rule-based representation: given an ISL function $f_2$ with $k=2$, the input $a$ becomes $b$ when it comes after $b$ and two consecutive $a$s will be reduced to one single $a$. The minimal function representation is as below:
\begin{equation}
\hspace{-3mm} f_2(w) {=}\,
  \begin{cases}
 f_2(w_1)ba^{-1}f_2(aw_2), \\
 \quad\quad \parbox[t]{0.6\linewidth}{
        if $w_1$ ends with $b$ and 
        $w {=}w_1aw_2$ for some $w_1, w_2 {\in} \Sigma^*$
      }\\
 f_2(w_1)a^{-1}f_2(aw_2), \\
 \quad\quad \parbox[t]{0.6\linewidth}{
        if $w_1$ ends with $a$ and 
        $w {=}w_1aw_2$ for some $w_1, w_2 {\in} \Sigma^*$
      }\\
 w, \,\,\,\,\, \text{otherwise}
  \end{cases}
\hspace{-10pt} 
\end{equation}

In the simplified rule-based format, it can be written as:$a\circ a\to\lambda\text{, }b\circ a\to b$. In summary, $f_1$ can be minimally expressed with a single rule, $f_2$ requires two rules.

