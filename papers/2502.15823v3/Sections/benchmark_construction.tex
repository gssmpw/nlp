\section{Benchmark Construction} 
In this section, we detail how our benchmark is constructed from the previously defined function classes. Each datapoint $(\mathcal{D}, f)$ in the benchmark is a pair of dataset $\mathcal{D}$ and function $f$ where $\mathcal{D}$ is a set of input-output pairs generated by $f$.

Each of ISL, L-OSL, and R-OSL classes can be further subdivided into incremental levels of complexity, determined by three key parameters: (1) the context window size $k$ (2) the vocabulary size $|\Sigma|$ (3) the minimal representation length of the function, \emph{i.e.} the minimal set of rules corresponding to the function. Given $k$ and $|\Sigma|$, the search space is $2^{|\Sigma|^k}$; given the number of rules $n$ additionally, the search space is $|\Sigma|^k \choose n$. To rigorously evaluate LLMs' inductive capabilities, we systematically vary these parameters across ISL, L-OSL, and R-OSL function classes.

In addition, we examine how performance changes with different numbers of inputâ€“output pairs in the prompt. Although having the characteristic sample present should theoretically guarantee recoverability of the underlying function, our empirical results indicate that the overall number of examples strongly affects performance. While extra data can provide richer information, it also increases context length considerably and heightens processing demands \citep{li2024long}. By varying the number of provided datapoints, we further investigate the extent to which the model engages in genuine reasoning and how robust its inductive abilities remain under changing input sizes.

\paragraph{Function Generation}
To systematically create benchmark instances, we first \emph{randomly generate} functions $f$ based on the three parameters: $k$, $|\Sigma|$, and the number of minimal rules describing $f$ by generating the set of rules that can describe $f$. While multiple representations of varying length can describe the same function, each function has a \emph{unique minimal representation} (up to isomorphism). During function generation, we therefore ensure that each function is expressed by a minimal, non-redundant rule set. Formally, if a $f$ is represented by a set of rules $R_f = \{r_1, r_2, ..., r_n\}$ where each $r_i$ has the form of $c_i\circ u_i\to v_i$ (with $c_i$ as the condition substring, $u_i$ the target character, and $v_i$ the transformed output for $u_i$), there are several constraints may be applied to functions belonging to the three classes. 

\begin{definition}[General Consistency]
Given $f$ represented by a set of rules $R_f: \forall r_i, r_j\in R_f, c_i\circ u_i\notin\textsc{Suff}(c_j\circ u_j)$ and $c_j\circ u_j\notin\textsc{Suff}(c_i\circ u_i)$.
\end{definition}

General Consistency ensures that the rules do not contradict one another or become redundant when conditions overlap. For instance, a function whose rule-based representation of $r_1: a\circ b\to a$ and $r_2: aa\circ b\to a$ is redundant, as the scenarios where $r_1$ is applied is a superset of the scenarios where $r_2$ is applied. For another instance, there does not exist a deterministic function that can be described by $r_1: a\circ b\to a$ and $r_2: aa\circ b\to \lambda$. Generating rule-based representations for ISL functions needs only satisfy this constraint.

\begin{definition}[OSL Non-Redundancy Guarantee]
Given $f$ represented by a set of rules $R:\forall r_i\in $ $R_f, \lnot\exists s_i'$ ${\in}\{s_i|s_i {\in} c_i\}$ such that $ \exists r_j\in R_f$ such that $ s_i' = c_j\circ u_j, \text{ unless }\exists r_k\in R$ such that $c_k\circ v_k = s_i'$.
\end{definition}

Constraint 2 is specific to the two OSL function classes because we need to make sure that all output conditions in the rule actually surface somewhere in the outputs of some datapoints. If the output condition $c$ never actually surface as the output, the rule will never be put into effect. Thereby the above rule basically requires that condition part of all rules can surface, either because it will never be modified by some other rule, or it emerges on the surface because of the application of other rule. For instance, a function represented by rules $r_1: aa\circ b\to a, r_2: a\circ a\to c$ is redundant because $r_1$ will never be applied because the string $aa$ will never surface as output and thus it will never be put into effect; For another instance, a function represented by $r_1: aa\circ b\to a, r_2: a\circ a\to c, r_3: a\circ d\to a$ is non redundant because even though into $aa$ string will be modified into $ac$, but $aa$ will surface in some datapoint because $ad$ will be modified into $aa$ and thus $r_1$ will be able to be applied.

Generating the functions following the two constraints, we ensure that the generated function representation is minimal, non-reducible guarantees a clear measure of complexity. One additional requirement is imposed to ensure each function indeed requires a look-ahead of size $k$. Specifically:

\begin{definition}[$k$-Complexity Guarantee]
Given $f$ whose designated context window $k = k_1$, $\exists r'\in R \text{ such that } c'\circ u'\to v'$ such that $|c'\circ u'| = k_1$.
\end{definition}

This condition guarantees that the function is genuinely $k$-strictly local (for ISL or OSL), rather than being representable with a smaller window size. Consequently, the functions we generate faithfully reflect the intended complexity level.

After generating the function $f$, we generate the characteristic sample of input-output pairs. For instance, given a function $f$ with $k = 2$ and $\Sigma = \{a, b\}$, the characteristic sample is $\{(a, f(a)),(b, f(b)), (ab, f(ab)), (aa, f(aa)), (bb, f(bb)), (ba, f(ba))\}$, a small set whose size is 6. By expanding this sample set, we can explore whether providing more than the minimal necessary examples aids or hinders the model's performance to infer the underlying function.

To evaluate how effectively an LLM can induce the underlying function, we include in the prompt (1) the function class, (2) context window $k$, (3) the alphabet $\Sigma$ which are information that guarantee learnability of the function. Then given the sample dataset, we request LLMs to produce a minimal rule-based description that reproduces the provided sample set, revealing whether it can \emph{discover} and \emph{optimally represent} the underlying transformation.

