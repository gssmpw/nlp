
\section{Related Work}

%A wide range of benchmarks have been introduced to assess the reasoning capabilities of large language models (LLMs). These benchmarks often target deductive, abductive, or mathematical reasoning and vary in complexity and scope.

\paragraph{Deductive Reasoning.} One major branch of reasoning benchmarks centers on deductive inference, where models apply established premises to derive specific conclusions. Notable examples include ReClor \citep{yu2020reclor}, which evaluates the ability to solve logical reasoning questions resembling those found in standardized tests, and various logic-based benchmarks of increasing complexity, from propositional logic to first-order logic \citep{han2022folio, parmar2024logicbench, zhu2023dyval, hua2024disentangling}. These tasks typically require handling structured logical relationships with minimal ambiguity in how premises lead to conclusions.

Another type of reasoning benchmarks is mathematical problem solving, including elementary arithmetic to advanced competition-level questions.  \citet{hendrycks2021measuring} test both computational skills and the sequential reasoning steps involved in mathematics. \citet{cobbe2021training} covers a broad spectrum of topics, including geometry and higher-level problem solving. However, most standard mathematics problem-solving tasks can be framed as deductive reasoning, as they involve applying established axioms, definitions, and theorems in a logically valid sequence to derive a conclusion. 
%For instance, when solving geometry or algebra problems, one typically starts with known principles such as the Pythagorean theorem and the quadratic formula and then methodically deduces the final answer. 

\paragraph{Inductive Reasoning.} Despite the diversity of existing benchmarks, inductive reasoning, where models hypothesize and generalize patterns from examples without pre-specified rules, remains comparatively underexplored. Current evaluations of inductive skills have largely been limited to small-scale symbolic regression, artificial language translation, and concept learning \citep{liu2024incomplete, lake2019human, qiu2023phenomenal}, which, although important in real-world scenarios, often lack three key elements: (1) an explicit analysis of the inherent difficulty of the task (2) a guarantee that the provided inputâ€“output dataset can identify the target function (3) a mechanism to evaluate whether models can identify the ``best possible hypothesis'' under Occam's Razor \citep{blumer1987occam, baker2007occam} principle, \emph{i.e.}, a description with minimal length \citep{hansen2001model, grunwald2007minimum}.

\paragraph{Our Contribution.} To address these shortcomings, we introduce a new benchmark targeting on inductive reasoning skills. Building on subregular hierarchy and corresponding polynomial time and data learnability guarantees, our benchmark, \textbf{InductionBench}, tests how effectively LLMs infer underlying transformation functions from finite datapoints. We also measure the degree to which models produce minimal, non-redundant hypotheses, providing a lens into their ability of generalization. Through a fine-grained, gradually increasing level of complexity, our evaluations reveal how current LLMs cope with the growing search space. There are several advantages of our benchmark:
\begin{enumerate}
    \item \textbf{Automated Evaluation}: Because the data is derived from well-defined functions, one can directly compare the model's output with the known ground-truth function, eliminating the need for expensive human annotations. 
    \item \textbf{Dynamic Data Generation}: The dataset is produced randomly based on specific function classes, allowing periodic ``refreshes'' to prevent models from relying on memorized examples.
    \item \textbf{Rigorous Assessment of Hypothesis Space}: As the function is well-defined, one can control the size of the hypothesis space with precision. This control enables a rigorous and systematic evaluation of LLM performance from a theoretically grounded perspective.
\end{enumerate}