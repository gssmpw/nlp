\pdfoutput=1

\documentclass[table, x11names]{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{mathtools}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{xcolor}
\usepackage{microtype}
\newtheorem{definition}{Definition}
\newtheorem{exmp}{Example}[section]
\usepackage{latexsym}
\usepackage{multicol, multirow}
\usepackage{booktabs}
\usepackage{arydshln}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{color}
\usepackage{verbatimbox}
\usepackage{listings}

\usepackage{tikz}
\usetikzlibrary{intersections}
\usetikzlibrary{positioning}
\usepackage{wrapfig}


%New colors defined below
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\setlist[itemize]{leftmargin=*}
\setlist[enumerate]{leftmargin=*}
\graphicspath{ {./images/} }

%Code listing style named "mystyle"
\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour}, commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2
}

%"mystyle" code listing set
\lstset{style=mystyle}

\usepackage[strict]{changepage}
\usepackage{framed}
\definecolor{demonstrationshade}{rgb}{0.95,0.95,1}
\definecolor{promptshade}{rgb}{0.95,0.95,1}
\newenvironment{demonstration}{%
  \def\FrameCommand{%
    \hspace{1pt}%
    {\color{black}\vrule width 2pt}%
    {\color{demonstrationshade}\vrule width 4pt}%
    \colorbox{demonstrationshade}%
  }%
  \MakeFramed{\advance\hsize-\width\FrameRestore}%
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{1pt}{7pt}%
  \vspace{2pt}\vspace{2pt}%
}
{%
  \vspace{2pt}\end{adjustwidth}\endMakeFramed%
}

\input{color.tex}
% \usepackage{inconsolata}

\usepackage{pgfplots}
\usepgfplotslibrary{groupplots}
%\pgfplotsset{compat=1.14}
\usetikzlibrary{decorations.pathreplacing}
\pgfplotsset{
axis background/.style={fill=gallery},
grid=both,
  xtick pos=left,
  ytick pos=left,
  tick style={
    major grid style={style=white,line width=1pt},
    minor grid style=gallery,
    draw=none,
  },
  minor tick num=1,
}



\title{InductionBench: LLMs Fail in the Simplest\\Complexity Class}


\author{
  Wenyue Hua$^{1}$ Tyler Wong$^1$ Sun Fei\\
  Liangming Pan$^2$ Adam Jardine$^3$ William Yang Wang$^1$\footnote{Corresponding authors: wenyuehua@ucsb.edu, william@cs.ucsb.edu. I'm very grateful for extensive discussion with Wenda Xu, Xinyi Wang at UCSB.} \\
  \\
  $^1$University of California, Santa Barbara,\\
  $^2$University of Arizona,
  $^3$Rutgers University, New Brunswick
}



\begin{document}
\maketitle

\input{Sections/abstract}

\input{Sections/intro}

\input{Sections/related}

\input{Sections/comp}

\input{Sections/benchmark_construction}

\input{Sections/experiment}

\input{Sections/leaderboard}


\section{Conclusion} 

In this work, we introduced a systematic benchmark for assessing the inductive reasoning capabilities of LLMs, leveraging both well-studied subregular function classes (ISL, L-OSL, and R-OSL) and a more exploratory class (IOSL) for which no known polynomial-time learning algorithm exists. By controlling parameters such as the Markov window size $k$, the vocabulary size $|\Sigma|$, and the minimal number of rules, we offered precise yet flexible tasks capable of probing a model’s capacity to infer general transformations from limited data. Our findings revealed several significant challenges for current LLMs—especially when required to track deeper dependencies or manage larger search spaces—and underscored the fragility of their inductive reasoning under increased context or novel data.

Through experiments measuring recall, precision, and compatibility, we demonstrated that factors like the Markov window size $k$ and the number of rules more profoundly degrade performance than an expanded alphabet. Moreover, while few-shot prompting showed promise in simpler scenarios, its benefits quickly plateaued in more complex contexts. An error analysis further highlighted how many rules go completely missing or become overgeneralized under stringent settings, indicating that LLMs often fail to synthesize key patterns comprehensively.

We also proposed an exploration leaderboard targeting IOSL functions, a class beyond established theoretical learnability, to address concerns that performance gains might stem from known polynomial-time algorithms rather than genuine inductive reasoning. This complementary evaluation opens avenues for research on less tractable classes and poses a more authentic test of generalization and adaptability.

Overall, our results highlight the need for more robust inductive reasoning strategies within current LLM architectures. We hope that our benchmark will help catalyze progress in both theoretical understanding and practical innovations around LLMs' inductive capabilities.

\section*{Limitations}

While our benchmark offers a rigorous, theoretically grounded approach to evaluating inductive reasoning in LLMs, current paper is subject to two notable constraints:

\paragraph{Synthetic Rather Than Real-World Data.} All tasks and evaluations rely on functions generated from carefully controlled parameters rather than naturally occurring texts or real-world datasets. Although this design enables precise measurement of inductive capabilities, it may not fully capture the complexity of practical language use, where ambiguous contexts, noisy inputs, and domain-specific factors can further challenge inference.

\paragraph{Restricted Access to the o1 Model.} Our investigation into the o1 family of models is hindered by limited availability and computational resources. As a result, certain aspects of o1’s inductive behavior may remain unexamined, and a more exhaustive exploration of variations or fine-tuning strategies for o1 could further illuminate its performance.

% Entries for the entire Anthology, followed by custom entries
\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}


\appendix
\section{Appendix}
Full results on ISL, OSL, and few-shot experiments are presented here.

\clearpage
\input{ISL_table}

\input{L_OSL_table}

\input{R_OSL_table}

\input{few_shot_ISL}

\input{few_shot_LOSL}

\input{few_shot_ROSL}

\end{document}
