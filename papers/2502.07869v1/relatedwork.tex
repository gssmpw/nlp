\section{Related Work}
\label{sec:related_work} 
We next review related methods for egocentric 3D human pose estimation and event-based 3D reconstruction. 

\subsection{Egocentric 3D Human Pose Estimation}
3D human pose estimation from egocentric monocular or stereo RGB views has been actively studied during the last decade. 
While the earliest approaches were optimisation-based \citep{rhodin2016egocap}, the field promptly adopted neural architectures following the state of the art in human pose estimation. 
Thus, follow-up methods used a two-stream CNN architecture \citep{xu2019mo2cap2} and auto-encoders for monocular \citep{tome2019xr, Tom2023SelfPose3E} and stereo inputs \citep{zhao2021egoglass, hakada2022unrealego,hakada2024unrealego2,Kang2023Ego3DPose}. 
Another work focused on the automatic calibration of fisheye cameras widely used in the egocentric setting \citep{zhang2021automatic}. 
Recent papers leverage human motion priors and temporal constraints for predictions in the global coordinate frame \citep{wang2021estimating}; reinforcement learning for improved physical plausibility of the estimated motions \citep{yuan2019ego, luo2021dynamics}; semi-supervised GAN-based human pose enhancement with external views \citep{wang2022estimating} and depth estimation \citep{wang2023scene}; and scene-conditioned denoising diffusion probabilistic models~\citep{Zhang_2023_ICCV}. 
%
\citet{Khirodkar_2023_ICCV} address a slightly different setting and use a multi-stream transformer to capture multiple humans in front-facing egocentric views. 
%
Meanwhile, \citet{wang2024egocentric} focus on egocentric whole-body motion capture with a single fisheye camera, utilising FisheyeViT for feature extraction, specialised networks for hand tracking, and a diffusion-based model for refining motion estimates.


All these works demonstrated promising results and pushed the field forward. 
They, however, were designed for synchronously operating RGB cameras and, hence---as every RGB-based method---suffer from inherent limitations of these sensors (detailed in Sec.~\ref{sec:intro}). 
Thus, only a few of them support real-time frame rates \citep{xu2019mo2cap2, tome2019xr}. 
Moreover, it is unreasonable to expect that RGB-based approaches can be easily adapted for event streams. 
In contrast, we propose an approach 
that (for the first time) accounts for the new data type in the context of egocentric 3D vision (events) and estimates 3D human poses at high 3D pose update rates. 

Last but not least, none of the existing datasets for the training and evaluation of egocentric 3D human pose estimation techniques and related problems \citep{rhodin2016egocap, xu2019mo2cap2, tome2019xr, wang2021estimating, Zhang_ECCV_2022, wang2023scene, Pan_2023_ICCV, Khirodkar_2023_ICCV, wang2022estimating, wang2023egowholebody} provide event streams or frames at framerate sufficient to generate events with event steam simulators \citep{rebecq18a_esim}. 
To evaluate and train our approach, we synthesise and record the necessary datasets (\textit{i.e.}, synthetic, real, and background augmentation) required to investigate event-based 3D human pose estimation on HMDs. 









\subsection{Event-based Methods for 3D Reconstruction} 

Substantial discrepancies between RGB frames and asynchronous event data have spurred the development of specialised 3D pose estimation methods, ranging from purely event-based approaches~\citep{rudnev2021eventhands, Nehvi2021, Zou2021eventhpe, Wang2022EvAC3D, xue2022event, chen2022efficient, rudnev2023eventnerf, Millerdurai_3DV2024} to RGB-event hybrid methods~\citep{EventCap2020, Zou2021eventhpe, park20243d, jiang2024complementing}. 
%
Although hybrid solutions can offer complementary information, they also significantly increase bandwidth usage, power consumption, and computational overhead---factors that become especially problematic for battery-powered head-mounted displays. For a comparison of bandwidth usage and power consumption between RGB and event cameras, please see App.~\ref{appen:event_cam_effi}. 
%
Consequently, our work adopts a purely event-based paradigm.


Within the event-based domain, \citet{Nehvi2021} track non-rigid 3D objects (polygonal meshes or parametric 3D models) with 
a differentiable event stream simulator. 
\citet{rudnev2021eventhands} synthesise a 
dataset with human hands 
to train a neural 3D hand pose tracker with a Kalman filter. 
They introduce a lightweight LNES representation of events for learning as an improvement upon event frames. 
Next, \citet{xue2022event} optimise the parameters of a 3D hand model by associating events with mesh faces using the expectation-maximisation framework assuming that events are predominantly triggered by hand contours. 
Some works represent events as spatiotemporal points in space and encode them either as point clouds 
\citep{chen2022efficient, Millerdurai_3DV2024}. 
Consequently, most of these approaches are slow (due to different reasons such as iterative optimisation or computationally expensive operations on 3D point clouds), with the notable exception of EventHands \citep{rudnev2021eventhands} 
achieving up to $1$kHz hand pose update rates. 


In our work, we leverage LNES \citep{rudnev2021eventhands} because it operates independently of the input event count, facilitates real-time inference, and can be efficiently processed using neural components (\eg~CNN layers). 
%
Unlike the previously discussed approaches, our method is specifically designed for the egocentric setting and achieves the highest accuracy among all the methods compared.
%
In particular, we incorporate 
a novel residual %
mechanism that propagates events (event history) from the previous frame to the current one, prioritising events triggered around the human. 
This is also helpful 
when only a few events are triggered due to the lack of motion. 


\subsection{Alternate Sensors for 3D Human Pose Estimation} 
%
Inertial measurement units (IMUs) have been widely used for 3D human pose estimation, often relying on multiple sensors---typically up to six---strategically placed on the head, arms, pelvis, and legs to track body movements~\citep{von2017sparse,huang2018deep,yi2021transpose,jiang2022transformer,yi2022physical}.
%
While these systems can deliver reasonable accuracy, they tend to be cumbersome and inflexible due to the large number of sensors required and the associated calibration demands. 
%
Recent advancements have reduced the reliance on multiple sensors, with some systems using as few as three IMUs~\citep{aliakbarian2022flag,winkler2022questsim,jiang2022avatarposer,lee2023questenvsim,jiang2023egoposer,zheng2023realistic,jiang2025manikin}, typically mounted on the head and hands, making them more practical for applications such as virtual reality (VR). 
%
However, even with fewer sensors, these systems remain prone to issues like sensor drift and frequent recalibration during rapid motion, limiting their effectiveness in high-dynamic scenarios.


Another line of research fuses IMUs with additional modalities such as RGB data~\citep{gilbert2019fusing,von2016human,malleson2017real,guzov2021human,yi2023egolocate,dai2024hmd} or depth maps~\citep{helten2013real}, offering improved global positioning or fine-grained pose estimates. 
%
Yet, vision-based methods remain sensitive to low-light environments, occlusions, and motion blur, particularly when subjects move rapidly or operate in challenging lighting. 
%
Although diffusion-based approaches~\citep{du2023agrol,li2023ego,guzov2024hmd} have yielded smoother poses, most rely on future frames to achieve robust predictions, making them unsuitable for real-time usage. 
%

In contrast, we propose a purely event-camera-based approach, which operates at high frame rates (\ie $140$~fps) and exhibits robustness to challenging conditions like low light and fast motion. 
%
By mounting a single event camera on a head-mounted display (HMD), we eliminate the need for additional body-worn sensors, thus simplifying the setup and avoiding drift issues. 
%
This setup not only handles large lighting variations but also naturally accommodates rapid head and body movements, making it especially well-suited for real-time, egocentric 3D human pose estimation.




\begin{figure*}[t]
\begin{center}
   \includegraphics[width=1.0\textwidth]{EE3D_IJCV-Architecture.pdf}
\end{center}
    \caption{
    \textbf{Overview of our EventEgo3D++ approach}. 
    The HMD captures an egocentric event stream, which is then converted to a series of 2D LNES frames \citep{rudnev2021eventhands} as inputs to our neural architecture to estimate the 3D poses of the HMD user.
    The residual event propagation module (REPM) emphasises events triggered around the human by considering the temporal context of observations (realised with a frame buffer with event decay based on event confidence). 
    REPM, hence, helps the encoder-decoder (from LNES to heatmaps) and the heatmap lifting module to estimate accurate 3D human poses. 
    The method is supervised with ground-truth human body masks, heatmaps and 3D human poses.
}
\label{fig:eventego}
\end{figure*}