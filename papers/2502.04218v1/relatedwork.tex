\section{Related Work}
\subsection{Zero-Shot Learning}
Language models have increasingly been used for tasks that they were not explicitly trained on, beginning with models like GPT-2 \cite{radford2019language}. LLMs can effectively be used in zero-shot settings because they learn significant \textit{world knowledge} in addition to \textit{linguistic knowledge} from their training data. This world knowledge is particularly useful in tasks like question answering (QA).%


\subsection{Bias in Large Language Models}
Work on demographic bias in word representations goes back to the mid-2010s, with \citet{bolukbasi2016man} and \citet{caliskan2017semantics}'s work on gender bias in static word embeddings. This led to work (e.g., \citet{zhao-etal-2018-learning}) on methods to debias word embeddings, which have had mixed success \cite{gonen-goldberg-2019-lipstick}. As generative models have become more prevalent, researchers have used prompt-based strategies to quantify bias in LLMs \cite{sheng-etal-2019-woman,lucy-bamman-2021-gender}. Beyond gender, harmful biases have been observed against Muslims \cite{abid2021persistentantimuslim} and the LGBTQ+ community \cite{felkner-etal-2023-winoqueer}. These biases have been a major source of critique of LLMs, and their uncovering has led to both specific methods to address bias \cite{liang2021towards} and more general methods like RLHF \cite{ouyang2022training} that promise among other goals to combat bias. Our work is distinct from prior work in that it focuses on gender bias when LLMs are prompted to generate factual information.