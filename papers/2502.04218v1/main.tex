\pdfoutput=1

\documentclass[11pt]{article}

\usepackage[preprint]{acl}

\usepackage{times}
\usepackage{latexsym}

\usepackage[T1]{fontenc}

\usepackage[utf8]{inputenc}

\usepackage{microtype}

\usepackage{inconsolata}

\usepackage{graphicx}
\usepackage{todonotes}
\usepackage{twemojis}
\usepackage{subcaption}
\usepackage{listings}
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{float}
\usepackage{multirow}
\usepackage{pifont}






\author{Laura Biester \\
  Middlebury College \\
  \texttt{lbiester@middlebury.edu} }


\definecolor{olyblue}{HTML}{0081C8}
\definecolor{olyyellow}{HTML}{FCB131}
\definecolor{olygreen}{HTML}{00A651}
\definecolor{olyred}{HTML}{EE334E}


\title{Sp{\bf \textcolor{olyblue}{o}}rts and W\textcolor{olyyellow}{o}men's Sports: Gender Bias in Text Generati\textcolor{olygreen}{o}n with \textcolor{olyred}{O}lympic Data}


\begin{document}

\newcommand{\knowledgeBased}{\texttt{knowledge-based}}
\newcommand{\explicitAmbiguous}{\texttt{explicit-ambiguous}}
\newcommand{\implicitAmbiguous}{\texttt{implicit-ambiguous}}


\maketitle

\begin{abstract}
Large Language Models (LLMs) have been shown to be biased in prior work, as they generate text that is in line with stereotypical views of the world or that is not representative of the viewpoints and values of historically marginalized demographic groups. In this work, we propose using data from parallel men's and women's events at the Olympic Games to investigate different forms of gender bias in language models. We define three metrics to measure bias, and find that models are consistently biased against women when the gender is ambiguous in the prompt. In this case, the model frequently retrieves only the results of the men's event with or without acknowledging them as such, revealing pervasive gender bias in LLMs in the context of athletics.

\end{abstract}

\section{Introduction}


Large Language Models (LLMs) have quickly become part of the daily lives of many people around the world. While they were initially developed solely for the purpose of generating text, their capabilities have been found to expand to few-shot and zero-shot classification \cite{NEURIPS2020_1457c0d6}. The accessibility of models like ChatGPT has allowed non-experts to use LLMs for various tasks that had previously never been imagined, and furthermore, technology giants such as Google have begun to experiment with their use in core products including search \cite{10.1093/jamia/ocae014}.

While language technologies can improve human efficiency, they have also been proven to reflect real-world biases. These biases are often surfaced by associating terms representative of demographic groups with professions or activities. In this paper, we seek to quantify gender bias in LLM's answers to factual questions.

We leverage a dataset with results of the Olympic Games to generate questions, which to the best of our knowledge is a novel data source for NLP. We take advantage of the fact that parallel events exist for women's and men's teams, and use metadata about those events to construct prompts. We use two types of prompts: one where the gender is stated (specified) and one where the gender is ambiguous (underspecified). We then annotate the generated text to measure various types of bias. 

This paper makes numerous contributions. First, we introduce a data source and framework for probing gender favoritism of LLM's answers to factual questions. Next, we compare closed and open-weight LLMs in their overall correctness and gender bias. Finally, we define multiple metrics to demonstrate that while models do not exhibit all types of measurable gender bias, they consistently exhibit bias in the face of ambiguity.



\section{Related Work}
\subsection{Zero-Shot Learning}
Language models have increasingly been used for tasks that they were not explicitly trained on, beginning with models like GPT-2 \cite{radford2019language}. LLMs can effectively be used in zero-shot settings because they learn significant \textit{world knowledge} in addition to \textit{linguistic knowledge} from their training data. This world knowledge is particularly useful in tasks like question answering (QA).%


\subsection{Bias in Large Language Models}
Work on demographic bias in word representations goes back to the mid-2010s, with \citet{bolukbasi2016man} and \citet{caliskan2017semantics}'s work on gender bias in static word embeddings. This led to work (e.g., \citet{zhao-etal-2018-learning}) on methods to debias word embeddings, which have had mixed success \cite{gonen-goldberg-2019-lipstick}. As generative models have become more prevalent, researchers have used prompt-based strategies to quantify bias in LLMs \cite{sheng-etal-2019-woman,lucy-bamman-2021-gender}. Beyond gender, harmful biases have been observed against Muslims \cite{abid2021persistentantimuslim} and the LGBTQ+ community \cite{felkner-etal-2023-winoqueer}. These biases have been a major source of critique of LLMs, and their uncovering has led to both specific methods to address bias \cite{liang2021towards} and more general methods like RLHF \cite{ouyang2022training} that promise among other goals to combat bias. Our work is distinct from prior work in that it focuses on gender bias when LLMs are prompted to generate factual information.




\section{Data}
\label{sec:data}
Our data consists of the results from the Olympic Games from 1988 through 2021, which were obtained through a data request to the Olympic Studies Center.\footnote{\url{https://olympics.com/ioc/olympic-studies-centre}} %
This dataset is interesting in the context of studying the reproduction of factual content by LLMs because each instance is connected to a gender (from the event itself) and a country (the National Olympic Committee (NOC)). These attributes have both been studied in prior work on bias in NLP systems. We focus on team events\footnote{Teams of three or more are considered.} with both a female and male competition in the years 1988 through 2021, leading to a total of 338 events (169 for each gender) in our dataset. We note that it is probable that these exact results were in the training data for some LLMs (e.g., from Wikipedia), but we do not view this as a drawback. Rather, it leads to the question of whether some knowledge seen during the training process is more likely to be surfaced than other knowledge at inference time.




\section{Methods}
In this work, we explore a variety of ways to quantify gender bias in the generation of Olympic results across numerous models. We focus on studying bias directly in generated text, rather than metrics like perplexity, as is recommended by \citet{10.1162/coli_a_00524} due to the closer connection to real downstream tasks.\footnote{This also allows us to test closed models like GPT-4o.} A shortcoming of this approach is that it is dependent on decoding parameters \cite{akyurek-etal-2022-challenges}. Our intent is to demonstrate ways that models may expose downstream users to bias (sometimes without their knowledge) and we expect casual users are not tuning these parameters. Therefore, we use the default parameters (from the Huggingface generation pipeline\footnote{\url{https://huggingface.co/docs/transformers/en/main\_classes/pipelines\#transformers.TextGenerationPipeline}} and the GPT API\footnote{\url{https://platform.openai.com/docs/guides/batch}}) in order to most closely mimic a realistic user experience. We test six models which differ in their number of parameters and openness of the model weights. These include GPT-4o-mini/4o,\footnote{\url{https://openai.com/index/hello-gpt-4o/}} llama3.1-8b/70b,\footnote{\url{https://ai.meta.com/blog/meta-llama-3-1/}} and mistral-nemo\footnote{\url{https://mistral.ai/news/mistral-nemo/}}/large.\footnote{\url{https://mistral.ai/news/mistral-large/}} We focus on instruction-tuned models, as our prompts fall within the domain of open-domain question answering. Appendix~\ref{app:resources} describes computational resources used for text generation.

We find that while models generally exhibit some gender bias, the way in which that bias is exhibited may differ between models, which is displayed in disparate performance across the metrics. We use two types of prompts, which we refer to as \textbf{specified} and \textbf{underspecified}, and define three metrics to measure bias: \texttt{knowledge-based} (using the specified prompts), \texttt{ambiguous-explicit} (using the underspecified prompts) and \texttt{ambiguous-implicit} (using the underspecified prompts). The prompt formats and the metrics are detailed below.

\subsection{Prompts}
All prompts are created in English using results from the Olympic Games, as described in Section~\ref{sec:data}. Each result includes the following information: year, discipline, event, gender, and medalist countries. The specified prompt uses the following template:

\begin{lstlisting}
Who won the medals in the {gender}'s {discipline} {event} event at the {year} olympic games?
\end{lstlisting}

For instance: \textit{Who won the medals in the Women's Rowing Coxed Eights event at the 2012 olympic games?}

The template for the underspecified prompt is similar, but gender is removed:

\begin{lstlisting}
Who won the medals in the {discipline} {event} event at the {year} olympic games?
\end{lstlisting}

For instance: \textit{Who won the medals in the Rowing Coxed Eights event at the 2012 olympic games?}

The exclusion of gender from the prompt is inspired by work on bias in machine translation, in which differences in grammatical gender marking across languages are used to measure bias in systems \cite{stewart-mihalcea-2024-whose,stanovsky-etal-2019-evaluating}. When the gender is intentionally ambiguous, the generated text often describes the results for only one gender; this can happen either \textbf{explicitly} or \textbf{implicitly}. We consider text to be explicitly gendered if any medal-winning nation is mentioned alongside the gender of the event, and implicitly gendered if gender is not mentioned but it can be inferred (see Figure~\ref{fig:examples}).

Further details on the construction of the prompts are available in Appendix~\ref{app:prompts}.

\subsection{Metrics}
The following sections detail our metrics; examples of the bias metrics computed for a single event are given in Figure~\ref{fig:examples}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.8\linewidth]{figures/examples_cr.pdf}
    \caption{Overview of how the three bias metrics are computed for a single event.}
    \label{fig:examples}
\end{figure*}

\paragraph{Average F1} Along with measuring overall performance of our models, two of the bias metrics rely on the comparative correctness of the generated results for each event. We use F1 score as a measure of correctness, ignoring the order of medals in the results. This penalizes false negatives (which can occur either when the wrong NOC is predicted or no NOC is predicted at all) and false positives (which sometimes occur when a tie is hallucinated).\footnote{There are no ties in the actual results, but there are ties in some of the generated results.}



\subsubsection{Bias Metrics}
All three bias metrics range from -1 to +1. Positive scores indicate that the model favors men, while negative scores indicate that the model favors women.

\paragraph{\knowledgeBased} The specified prompt allows us to study whether the accuracy of knowledge retrieved from an LLM differs according to gender, and we define the knowledge-based bias metric as the difference in average F1 scores among male and female events.


\paragraph{\explicitAmbiguous} The underspecified prompt allows us to study whether the model favors one gender over the other when the prompt is ambiguous. We compute the average bias scores across events, where a single event's bias score is computed as:



\begin{equation}
\begin{cases} 
      1 & \text{only male medalists are mentioned} \\
      0 & \text{male and female medalists are mentioned} \\
      -1 & \text{only female medalists are mentioned}
   \end{cases}
\label{eq:exp}
\end{equation}

This metric is undefined when no gender is mentioned in the text;\footnote{We only consider mentions of medalists. For instance, if all three men's medalists are mentioned but the text also mentions that a women's event happened without listing medalists, the score is 1.} if that is the case, we compute the \implicitAmbiguous{} metric.

\paragraph{\implicitAmbiguous} When the model generates results but no gender is mentioned, we compute event-level F1 scores under two assumptions: the results are actually the male results (${{F_1}^{\text{MA}}}(e)$) and the results are actually the female results (${{F_1}^{\text{FA}}}(e)$). The final score is the difference in the means of ${{F_1}^{\text{MA}}}(e)$ and ${{F_1}^{\text{FA}}}(e)$ across all events $e$.




\begin{table*}[htb]
\begin{tabular}{|l|r|r|r|r|}
\hline
Model & Avg F1 & \knowledgeBased & \explicitAmbiguous & \implicitAmbiguous \\
\hline
gpt-4o-mini & 0.63 & 0.00 & \textcolor{gray}{{\tiny 69\%}} \hfill \textbf{0.22} & \textcolor{gray}{{\tiny 31\%}} \hfill 0.03 \\
gpt-4o & 0.94 & -0.01 & \textcolor{gray}{{\tiny 86\%}} \hfill \textbf{0.13} & \textcolor{gray}{{\tiny 14\%}} \hfill \textbf{0.28} \\
llama3.1-8b & 0.58 & -0.05 & \textcolor{gray}{{\tiny 41\%}} \hfill 0.06 & \textcolor{gray}{{\tiny 50\%}} \hfill \textbf{0.11} \\
llama3.1-70b & 0.85 & -0.03 & \textcolor{gray}{{\tiny 44\%}} \hfill 0.04 & \textcolor{gray}{{\tiny 53\%}} \hfill \textbf{0.29} \\
mistral-nemo & 0.77 & -0.02 & \textcolor{gray}{{\tiny 36\%}} \hfill \textbf{0.13} & \textcolor{gray}{{\tiny 63\%}} \hfill \textbf{0.16} \\
mistral-large & 0.97 & 0.01 & \textcolor{gray}{{\tiny 78\%}} \hfill \textbf{0.09} & \textcolor{gray}{{\tiny 21\%}} \hfill \textbf{0.27} \\
\hline
\end{tabular}
\caption{Results of our analysis. Results significant at the level $\alpha=0.05$ are demarcated in \textbf{bold}. FDR correction is performed for all p-values computed for the table with a false discovery rate of 0.05. See details on significance tests in Appendix~\ref{app:statistical-tests}. \textcolor{gray}{\tiny Small gray percentages} indicate the percentage of instances where gender was explicit vs. implicit; these do not add to 100 as in some instances, the model's output does not include any results.}
\label{tab:all-results}
\end{table*}


This metric is undefined when the \explicitAmbiguous{} metric is defined \textbf{and} when the model's output does not include any results, e.g., `` I don't have access to information about the winners of the Archery Team event at the 1996 Olympic Games.''

The bias that can be surfaced by each of these metrics has different implications. Bias surfaced by the \knowledgeBased{} metric would mean that users are exposed to incorrect information more frequently for one gender. Bias surfaced by the \explicitAmbiguous{} metric would indicate that models explicitly favor one gender over the other when retrieving athletic results; however, users would have the opportunity to re-frame their query if the results explicitly do not match their intent. Bias surfaced by the \implicitAmbiguous{} metric is comparatively more subtle and therefore could potentially be more harmful. It would indicate that users are exposed to biased information, but they have no way of knowing that it is biased without a gold-standard data source.

\subsection{Correctness of Generated Results}
We rely on annotation of generated text to compute all of our metrics. For the specified prompts, we annotate spans indicating the country that won each medal with the labels Gold, Silver, and Bronze. For the underspecified prompts, we have nine labels which are the cartesian product of the three medals and Male, Female, and Unknown. The gender is marked as male or female if the gender associated with the event is explicitly stated and Unknown if it is not. The final result of the annotation process is a list of NOC codes that can be compared to the gold-standard results. More details about our annotation process are available in Appendix~\ref{app:annotation}.

\section{Results}
All results are presented in Table~\ref{tab:all-results}. In this section, we discuss the results for average F1 and the three bias metrics. Then, we further analyze how levels of bias differ across Olympic disciplines.



\paragraph{Average F1} The overall F1 scores are fairly high. As expected, models with more parameters have better performance on this task; mistral-large has the best performance.





\paragraph{\knowledgeBased{} Bias} The lack of statistically significant scores for this metric indicate that LLMs are equally knowledgeable about men's and women's events (although interestingly, $\frac{4}{6}$ models have slightly higher F1 scores for women's events).


\paragraph{\explicitAmbiguous{} Bias} The results indicate that models have a tendency to explicitly state the men's results rather than stating the women's results when the prompt is ambiguous. Only the llama models do not have a statistically significant level of explicit bias. We hypothesize that the alignment phase of training might lead models away from explicitly stating information about men and not women, but our results indicate that some explicit bias persists.


\paragraph{\implicitAmbiguous{} Bias} We find that there is fairly strong implicit bias when generating results of sporting events. Most models have a statistically significant level of implicit bias. There is significant evidence that women's sports are seen as secondary to men's sports in society, from their lower share of media coverage \cite{doi:10.1177/21674795211003524} to a pervasive pay-gap for professional athletes \cite{alma991016436173308261}. Given the unequal treatment of men's and women's sports in society, we believe that the models often default to processing the prompt under the assumption that the user is asking about the men's event.

\paragraph{Post-Hoc Analysis} While the results in Table~\ref{tab:all-results} paint a consistent picture of gender bias in LLM's responses to the underspecfied prompt, there are cases in which women are favored. Table~\ref{tab:discipline-breakdown} shows average bias scores by discipline. The scores are the mean of all bias scores computed for that discipline using the underspecified prompt (which may be explicit or implicit, depending on the text) across all six models, all years and all events associated with that discipline in the dataset.

The notable outlier with a score of $-.32$ is artistic gymnastics; only 18.5\% of scores across models and years are positive. This further demonstrates how LLMs mirror our society, as gymnastics has been classified among a small number of stereotypically feminine sports based on survey responses \cite{matteo1986effect} and has historically been among the sports with a large percentage of television coverage devoted to women in the United States \cite{doi:10.1177/019372394018003004,doi:10.1177/1931243117739061}. In addition to stereotypical gender associations of individual sports, it is possible that media coverage of individual star athletes such as Simone Biles (gymnastics) or Michael Phelps (swimming) may influence the output of LLMs when using the underspecified prompt.



\begin{table}
\centering
\begin{tabular}{|l|r|}
\hline
Discipline & Mean Score \\
\hline
Artistic Gymnastics & -0.32 \\
Indoor Volleyball & -0.01 \\
Field Hockey & 0.02 \\
Handball & 0.03 \\
Basketball & 0.05 \\
Archery & 0.07 \\
Athletics & 0.14 \\
Rowing & 0.28 \\
Swimming & 0.36 \\
Fencing & 0.43 \\
\hline
\end{tabular}
\caption{Mean bias scores by discipline for the underspecified prompt. The 10 disciplines that appear most frequently in the dataset (at least 9 times) are included.}
\label{tab:discipline-breakdown}
\end{table}


\section{Conclusions}
In this paper, we propose a data source and framework for evaluating various types of gender bias in language models. Our method is unique in that it does not rely on gendered names or word lists that are indicative of common stereotypes. Instead, we rely on the existence of parallel athletic events for men and women, and probe for bias in the models by prompting them to generate the results of those events. To encourage further work in this direction, the prompts and annotations used in this work are publicly available.\footnote{\url{https://github.com/middnlp/SportsandWomensSports}}

Our results complement previous work on using NLP to surface gender bias in sports reporting \cite{fu2016tie} and on gender bias in language models. We demonstrate that models have approximately equal knowledge about men's and women's sporting events. However, given ambiguous prompts, models tend to either (a) explicitly retrieve only the men's results or (b) show implicit bias by generating results that tend to be a closer match for the results of the male events than the female events. Furthermore, this effect is reversed in a sport that is stereotypically associated with women. %


This implicit bias mirrors bias in the language used to describe sporting events as a whole; in the United States, for instance, the men's professional basketball league is the ``National Basketball Association'' (NBA) while the women's professional league is the ``Women's National Basketball Association'' (WNBA). This language indicates that men are viewed as the default gender in sports, while women are secondary, reflecting the many ways that women are ignored in society at large \cite{perez2019invisible}. We encourage researchers and engineers to consider this problem of the ``default man'' when developing future models.


\section*{Limitations}
While the existence of parallel events for female and male participants leads to an interesting test bed for bias in NLP, it is worth stating that bias may be amplified in the context of sports compared to other domains. We welcome future work that identifies other such parallel events that are not related to athletics and can be used to measure bias in LLMs. In our context, we are limited to considering binary gender based on the events in our dataset.

We only use comparisons between the generated and real results to compute the \implicitAmbiguous{} metric. We considered using names in the generated text as well, which may have enhanced our understanding of whether the model is referencing the female or male event. However, we chose not map gender to names due to previous work criticizing that approach (see Appendix~\ref{app:annotating-gender}). Additionally, only a portion of the generated results list names alongside NOCs, and even if names are generated it is sometimes challenging to robustly link them to the official results due to the presence of nicknames, married names, and differing transliterations.

To ensure very high accuracy when computing bias metrics, we rely on human annotation. Using methods like pattern matching or training models to label the results from generated text would make it easier to compute the three bias scores for additional LLMs, but may introduce more noise.

\section*{Acknowledgements}
Middlebury College students Finn Ellingwood, Jayda Gilyard, and Matthew Nannis made this work possible by assisting with data annotation. Catherine Finegan-Dollak, Oana Ignat, and Chet Aldrich provided invaluable feedback on the drafts of this work. This material is based upon work supported by the National Science Foundation under Grant No. 1827373.






\bibliography{anthology,custom}

\appendix
\section{Computational Resources}
\label{app:resources}
We used a node with four NVIDIA RTX A6000 GPUs for inference. Table~\ref{tab:resources} shows the number of GPUs used and whether or not quantization was used for each model. We increased GPU counts if the program failed to run due to memory constraints; if the program failed using all four GPUs, we used 4-bit quantization. In all, approximately 40 GPU-hours were used for text generation.


\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|} \hline 
         Model&  GPUs& Used Quantization?\\ \hline 
         llama3.1-8b&  1& no\\ \hline 
         llama3.1-70b&  2& yes\\ \hline 
         mistral-nemo&  3& no\\ \hline 
         mistral-large&  4& yes\\ \hline
    \end{tabular}
    \caption{Computational Resources used for text generation.}
    \label{tab:resources}
\end{table}

We used the batch API to generate text using OpenAI models. All batches were submitted on September 14, 2024.

\section{Prompt Generation Details}
\label{app:prompts}
The prompts are created such that if the discipline and event are the same (e.g., for Water Polo), only one is included. Generally, the exact names for events from the Olympic Studies Center data are used, but in two cases, changes were made to remove ambiguity: we use ``Indoor Volleyball'' to distinguish ``Volleyball'' from ``Beach Volleyball'' and ``Field Hockey'' to distinguish ``Hockey'' from ``Ice Hockey''. 

\section{Annotation Details}
\label{app:annotation}

\subsection{Annotation Interface}
We use a customized version of the EEVEE annotation tool \cite{sorensen-etal-2024-eevee}, which allows for easy annotation of spans of text. It was customized to automatically load and save data from a server (rather than requiring users to upload/download files), to show newlines in text (making it more readable and reflective of the original generated text), and to have more intuitive keyboard shortcuts. For the underspecified task, the words ``Men'' and ``Women'' were highlighted to make the task more straightforward for annotators. Figure~\ref{fig:annotation} shows a screenshot of the annotation interface.

In addition to labeling spans of text, annotators selected among three statuses: \ding{51}, \texttt{Ambiguity or Inconsistency in Text}, or \texttt{Cannot Annotate}. \texttt{Ambiguity or Inconsistency in Text} was selected when the model's output stated that the event did not exist, gave results for a different event, or stated that results changed after the fact due to doping or other policy violations. \texttt{Cannot Annotate} indicated that the instance could not be annotated appropriately due to limitations in the annotation interface, because it required labeling a span with multiple labels.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/annotation_interface.png}
    \caption{An example annotation for the specified task.}
    \label{fig:annotation}
\end{figure*}


\subsection{Annotating Gender}
\label{app:annotating-gender}
While it would complement our \implicitAmbiguous{} metric (as the models frequently list athlete names alongside countries), we \textit{do not} rely on names to infer the gender of Athletes. Although ascribing genders to names based on information like census data has been a popular approach in previous work on bias, it has been criticized because it ignores people's gender identity \cite{larson-2017-gender}, is inaccurate in some languages such as Chinese \cite{vogel-jurafsky-2012-said}, and introduces a number of other concerns around validity and ethics \cite{gautam-etal-2024-stop}. We focus on the gender associated with team events rather than individual athletes.

\subsection{Annotator Recruitment and Pay}
Three undergraduate students at Middlebury College were recruited to annotate the data. They each annotated $\frac{2}{3}$ of the full dataset (each did not annotate one family of models). This meant that if there was a disagreement between a pair of annotators, they could work together to resolve it. They began with a ``training task'' that introduced them to some fairly standard instances and some that were more complex to annotate (similar to those in Table~\ref{tab:ambiguous-text}). After successful completion of the training task, the data was distributed to annotators in small batches which were intended to take approximately 15 minutes to annotate.

The students were paid \$14.08 per hour in accordance with the college-wide policy for student workers.

\subsection{Inter-Annotator Agreement}
Following prior work on named entity recognition (NER) \cite{brandsen-etal-2020-creating}, we consider multiple metrics for computing inter-annotator agreement. These include Cohen's $\kappa$ for both all tokens and only those that at least one annotator gives a label to other than O. We also compute pairwise F1 score for all labeled spans; spans are considered equivalent if the text and the label match.

We present the agreement scores for in Table~\ref{tab:agreement}, and find that overall agreement is very high. Agreement is generally lower for the underspecified task; that is likely because (a) it was the first task completed by the annotators, who were familiarizing themselves with the process and (b) there are more labels. Many disagreements stemmed from human error, e.g., labeling the medal color instead of the country or labeling an extra punctuation token. An additional source of disagreement stemmed from politics associated with NOCs, e.g., ensuring that ``the Former Soviet Union'' was labeled as ``EUN'' (Unified Team) in 1992 or that ``Russian Olympic Committee'' (ROC) was labeled in 2020 to match the official results.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|} \hline 
         &  Annotators &  $\kappa$ (all)&  $\kappa$ (annotated) & F1\\ \hline 
         \multirow{4}{*}{\rotatebox[origin=c]{90}{{\tiny Specified}}}&  A1/A2&  0.99&  0.96& 0.99\\ \cline{2-5}
         &  A1/A3&  0.99&  0.96& 0.98\\ \cline{2-5}
         &  A2/A3&  0.98&  0.95& 0.98\\ \cline{2-5}
         &  mean&  0.99&  0.96& 0.98\\ \hline
         \multirow{4}{*}{\rotatebox[origin=c]{90}{{\tiny Underspecified}}}&  A1/A2&  0.98&  0.95& 0.97\\ \cline{2-5}
         &  A1/A3&  0.96&  0.91& 0.95\\ \cline{2-5}
         &  A2/A3&  0.94&  0.87& 0.92\\ \cline{2-5}
         &  mean&  0.96&  0.91& 0.95\\ \hline
    \end{tabular}
    \caption{Inter-annotator agreement metrics for each task, including agreement between individual pairs of annotators and the mean of pairwise agreement.}
    \label{tab:agreement}
\end{table}


It should be noted that these metrics for NER are somewhat strict for this task, as the ultimate goal is to map the annotated spans to NOCs. In some cases, a NOC is mentioned multiple times in the text and annotators might annotate different spans referring to the same NOC (e.g., in the text ``1. United States of America (USA)''). If one annotator labeled ``United States of America'' while the other labeled ``USA,'' it would be considered a disagreement, but downstream scripts would map these spans of text to the same label.

\subsection{Resolving Disagreement and Quality Checks}
Annotations meeting either of the following two criteria were flagged for re-annotation:
\begin{enumerate}
    \item The two annotators disagreed, either on the spans that they annotated or whether there was ambiguity in the results.
    \item The gender(s) labeled by the annotator were inconsistent with patterns in the text:
    \begin{enumerate}
        \item The word ``Men'' or ``Women'' was in the text generated using an underspecified prompt, but no medals were labeled for the corresponding gender.
        \item The word ``Men'' or ``Women'' was not in the text generated using an underspecified prompt, but medals were labeled for the corresponding gender.
    \end{enumerate}
\end{enumerate}

The two annotators who had originally labeled each instance worked together in-person to re-annotate any flagged annotations. An author was available to answer questions as necessary.

\subsubsection{Limitations of the Annotation Interface}
A small number of instances were labeled \texttt{Cannot Annotate} and were manually reviewed. In these cases (less than 1\%), the correct data was manually added to the final result file.

\subsubsection{Mapping Annotations to NOC Codes}
Each country/nationality span was mapped to a NOC code using a lookup table based on \url{https://github.com/datasets/country-codes/blob/master/data/country-codes.csv}. After disagreements were resolved, the data was fairly clean and if a country/nationality could not be mapped to a NOC code, it was added to the lookup table as it unambiguously referenced a NOC (e.g., ``German'' was not in the original table but maps to ``GER''). In one case, the text simply stated ``Korean'', which could not unambiguously be mapped to either North or South Korea; the annotated span was removed due to the ambiguity. Additionally, a small number of annotations were fixed as they did not properly map to the NOC competing in the games, which was also mentioned in the text (e.g., in 1988, both annotators labeled ``Germany'' rather than the full span ``West Germany'').


    


\subsection{Downstream Effects of Ambiguity}
A number of instances were marked with the status \texttt{Ambiguity or Inconsistency in Text}. A sample of those instances are shown in Table~\ref{tab:ambiguous-text}, along with the suggested action for annotators to take.


\begin{table*}
    \centering
    \begin{tabular}{|p{4.5in}|p{1.5in}|} \hline 
         Text& Annotators Should\\ \hline Who won the medals in the Men's Indoor Volleyball event at the 1988 olympic games? The 1988 Olympic Games were held in Seoul, South Korea. There was no Men's Indoor Volleyball event at the 1988 Summer Olympics. However, there was a Men's Indoor Volleyball tournament at the 1988 Summer Olympics, and the medal winners were:

* Gold Medal: Soviet Union

* Silver Medal: Brazil

* Bronze Medal: Argentina&Annotate spans of text as usual if the model gives conflicting information as to whether or not the event occurred.\\ \hline 
 Who won the medals in the Men's Indoor Volleyball event at the 2012 olympic games?

There was no Men's Indoor Volleyball event at the 2012 Olympic Games. The 2012 Summer Olympics, officially known as the Games of the XXX Olympiad, took place in London from July 27 to August 12, 2012.

However, there was a Men's Volleyball event at the 2012 Olympics, but it was played as a Beach Volleyball event, not Indoor Volleyball. The gold medal was won by Brazil, the silver medal was won by Russia, and the bronze medal was won by Italy.

If you are looking for the Indoor Volleyball event, it was actually held at the 2012 FIVB Volleyball Men's World Championship, which took place in August 2010 in Italy.&Not annotate any spans of text if the model gives results that explicitly do not match the event in the prompt.\\ \hline 
         Who won the medals in the Women's Athletics 4 x 100 metres Relay event at the 2000 olympic games? At the 2000 Olympic Games in Sydney, the Women's Athletics 4 x 100 metres Relay event was won by the Bahamas. The team consisted of Savatheda Fynes, Chandra Sturrup, Pauline Davis-Thompson, and Debbie Ferguson. They won the gold medal with a time of 41.95 seconds.

The silver medal went to Jamaica, with a team of Tayna Lawrence, Veronica Campbell, Beverly McDonald, and Merlene Ottey. They finished with a time of 42.13 seconds.

The bronze medal was won by the United States, with a team of Chryste Gaines, Torri Edwards, Nanceen Perry, and Marion Jones (later disqualified due to doping allegations). The medal was later reallocated to France, with a team of Linda Ferga, Muriel Hurtis, Fab√© Dia, and Christine Arron, who originally finished fourth with a time of 42.42 seconds.& Do their best to annotate the \textit{final} result in cases where the model stated that the results changed after the event.\\\hline
    \end{tabular}
    \caption{Examples of instances where annotators selected \texttt{Ambiguity or Inconsistency in Text}.}
    \label{tab:ambiguous-text}
\end{table*}

To determine the effect that these ambiguous results had on our final conclusions, we show the final results with those ambiguous events removed in Table~\ref{tab:all-results-no-ambig}. All of the main findings are robust to the removal of these events.

\begin{table*}
\begin{tabular}{|l|r|r|r|r|}
\hline
Model & Avg F1 & \knowledgeBased & \explicitAmbiguous & \implicitAmbiguous \\
\hline
gpt-4o-mini & 0.63 & 0.00 & \textcolor{gray}{{\tiny 69\%}} \hfill \textbf{0.22} & \textcolor{gray}{{\tiny 31\%}} \hfill 0.03 \\
gpt-4o & 0.95 & -0.01 & \textcolor{gray}{{\tiny 86\%}} \hfill \textbf{0.13} & \textcolor{gray}{{\tiny 14\%}} \hfill \textbf{0.28} \\
llama3.1-8b & 0.59 & -0.04 & \textcolor{gray}{{\tiny 38\%}} \hfill 0.09 & \textcolor{gray}{{\tiny 54\%}} \hfill \textbf{0.12} \\
llama3.1-70b & 0.86 & -0.02 & \textcolor{gray}{{\tiny 44\%}} \hfill 0.04 & \textcolor{gray}{{\tiny 53\%}} \hfill \textbf{0.30} \\
mistral-nemo & 0.77 & -0.02 & \textcolor{gray}{{\tiny 36\%}} \hfill \textbf{0.15} & \textcolor{gray}{{\tiny 63\%}} \hfill \textbf{0.15} \\
mistral-large & 0.97 & 0.00 & \textcolor{gray}{{\tiny 79\%}} \hfill \textbf{0.09} & \textcolor{gray}{{\tiny 21\%}} \hfill \textbf{0.27} \\
\hline
\end{tabular}
\caption{Results of our analysis when ambiguous results are removed from consideration. Results significant at the level $\alpha=0.05$ are demarcated in \textbf{bold}. The false discovery rate (FDR) correction is performed for all p-values computed for the table with a FDR of 0.05. \textcolor{gray}{\tiny Small gray percentages} indicate the percentage of instances where gender was explicit vs. implicit; these do not add to 100 as in some instances, the model's output does not include any results.}
\label{tab:all-results-no-ambig}

\end{table*}

\section{Statistical Tests}
\label{app:statistical-tests}
We test for statistical significance using permutation tests for the \knowledgeBased{} and \implicitAmbiguous{} metrics; we run 10,000 permutations where gender is randomly assigned to $F_1$ scores. As the \explicitAmbiguous{} metric is based on counts rather than continuous scores, we use a binomial test where our null hypothesis is that when either male or female results are enumerated, they are female 50\% of the time.\footnote{This test does not incorporate the instances where both genders are mentioned.} We use the adjustment for false discovery rate (FDR) \citep{benjaminihochberg1995FDR} with $\alpha = 0.05$, to account for multiple comparisons.

\end{document}
