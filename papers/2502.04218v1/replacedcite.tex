\section{Related Work}
\subsection{Zero-Shot Learning}
Language models have increasingly been used for tasks that they were not explicitly trained on, beginning with models like GPT-2 ____. LLMs can effectively be used in zero-shot settings because they learn significant \textit{world knowledge} in addition to \textit{linguistic knowledge} from their training data. This world knowledge is particularly useful in tasks like question answering (QA).%


\subsection{Bias in Large Language Models}
Work on demographic bias in word representations goes back to the mid-2010s, with ____ and ____'s work on gender bias in static word embeddings. This led to work (e.g., ____) on methods to debias word embeddings, which have had mixed success ____. As generative models have become more prevalent, researchers have used prompt-based strategies to quantify bias in LLMs ____. Beyond gender, harmful biases have been observed against Muslims ____ and the LGBTQ+ community ____. These biases have been a major source of critique of LLMs, and their uncovering has led to both specific methods to address bias ____ and more general methods like RLHF ____ that promise among other goals to combat bias. Our work is distinct from prior work in that it focuses on gender bias when LLMs are prompted to generate factual information.