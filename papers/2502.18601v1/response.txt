\section{Related Work}
\label{sec:related_work}

This section presents a comprehensive review of the definition of the convex hull, followed by an exploration of the algorithms commonly utilized for its numerical computation. Subsequently, an in-depth examination of the current state-of-the-art anomaly detection (AD) algorithms is provided, focusing on their methodologies, applications, and performance characteristics.

\subsection{Convex hull}
\label{sec:rw_convex_hull}

The convex hull holds significant importance across various research domains, including computer graphics **Gibbs, "Convex Hulls"**, computational geometry **Boissonnat, "Geometric Fitting of Implicit Surfaces"**,**Faugère, "Computing the convex hull of a set of points in three dimensions"**,**Lee, "On the complexity of polyhedra in 3-space"** and numerous other fields.



The convex hull is the smallest convex set that completely encloses a given set of points, minimizing the hyper-volume enclosed by the set. Formally, for a set of points \( \{\boldsymbol{x}_i\}_{i=1}^n \subset \mathbb{R}^d \), the convex hull is the minimal convex polytope containing all points, and it is expressed as **Garey, "Computers and Intractability: A Guide to the Theory of NP-Completeness"**:

\begin{equation}
CH(S) = \left\{ \sum_{i=1}^{n} \alpha_i \boldsymbol{x}_i \mid \alpha_i \geq 0 \, \wedge \, \sum_{i=1}^{n} \alpha_i = 1 \right\},
\end{equation}

where \( \alpha_i \) represents the non-negative scalar coefficients associated with each point \( \boldsymbol{x}_i \) in the set \( S \).


\subsubsection{Computing convex hull}
Various methods have been proposed to compute the convex hull of an arbitrary set of points, leveraging the convex hull's inherent convexity properties. Among these, three widely utilized algorithms are noteworthy: the Gift Wrap algorithm **Graham, "Convex bodies and algebraic geometry"**,**Chan, "Optimal output-sensitive convex hull algorithms in two and three dimensions"**, the Graham Scan **Graham, "An efficient algorithm for planar convex hulls"** , and the Quickhull algorithm **Berg, "Output-sensitive algorithms for computing visibility polygons"**.


The Gift Wrap algorithm **Graham, "Convex bodies and algebraic geometry"** also known as Jarvis March, constructs the convex hull incrementally by identifying boundary points one at a time. Starting with the leftmost point, which is guaranteed to be part of the convex hull, the algorithm iteratively selects the point that forms the smallest counterclockwise angle with the line segment formed by the current hull boundary. This selection ensures that the boundary progresses in a \say{wrapping} motion, ultimately enclosing all points. The hull is complete once the algorithm loops back to the starting point. The simplicity of the Gift Wrap algorithm makes it a natural choice for small datasets or cases where the number of points on the hull is small. However, its time complexity, \(\mathcal{O}(nh)\), where \(n\) is the total number of points and \(h\) is the number of points on the hull, can become prohibitive for larger datasets with dense point distributions.

Graham’s Scan algorithm **Graham, "An efficient algorithm for planar convex hulls"** begins by identifying a reference point, typically the point with the smallest \(y\)-coordinate (or the leftmost point in case of ties). The algorithm then sorts all points by their polar angle relative to this reference point, ensuring a natural order for constructing the hull. Using a stack, Graham’s Scan processes these points sequentially, adding points to the stack while ensuring that each addition maintains the convexity of the boundary. If a new point causes the boundary to form a concave angle, points are removed from the stack until convexity is restored. The reliance on sorting gives Graham’s Scan a time complexity of \(\mathcal{O}(n \log n)\), making it particularly well-suited for static datasets where sorting overhead can be amortized.



The Quickhull algorithm employs a divide-and-conquer approach to compute the convex hull of a given set of points. The process begins by identifying the points with minimum and maximum \(x\)-coordinates defining the initial boundary segment. The algorithm then determines the point farthest from this segment, effectively partitioning the dataset into two subsets: points located to the left and right of the segment. This procedure is applied recursively to each subset, iteratively identifying the points that constitute the convex hull. Although Quickhull is well-suited for datasets with non-uniform spatial distributions, its worst-case computational complexity is \(\mathcal{O}(n^2)\), which occurs when the point distribution leads to excessive recursion.



These algorithms form the basis for efficient convex hull computation and have been extended to handle dynamic datasets. For instance, updating the convex hull after removing a single point can often be achieved in \(\mathcal{O}(n + k)\), where \(k\) is the number of points on the hull boundary, avoiding the need for a complete recomputation. Their adaptability and efficiency ensure these methods remain central to computational geometry and its applications in higher dimensions.

Chan's algorithm **Chan, "Optimal output-sensitive convex hull algorithms in two and three dimensions"** computes the convex hull with a time complexity of \( O(n \log n + n^{\lfloor d/2 \rfloor}) \), making it computationally efficient for large datasets. An alternative method is described by Nielsen and Nock **Nielsen, "The Convex Hull of a Set of Points"**, which integrates the efficiency of divide-and-conquer strategies with the precision of geometric methods. This ensures that the convex hull can be computed in optimal time for various applications.

\subsubsection{Convex hull for anomaly detection}
Identifying outliers using the convex hull is based on the observation that anomalous points necessitate the expansion of the convex hull's boundaries. From an energy perspective, this approach can be interpreted as a trade-off analysis, wherein the inclusion of additional data points results in a transition from a set with a minimal convex hull to an expanded configuration encompassing these new points. This shift highlights the balance between maintaining a compact representation and accommodating potential outliers within the dataset.
Using a support vector machine for constructing boundaries hyperplane was done by Zhang and Gu **Zhang, "Support Vector Machines"**,**Wand, "Multivariate Analysis of Shape Variation in Morphometric Studies"**. Using the convex hull to improve the calculation of the mean and variance to detect anomalies was done by Costa et at **Costa, "Convex Hulls for Anomaly Detection"**. Blaise et al **Blaise, "Voroi Diagram and Convex Hull for Anomaly Detection"** suggest a method to detect a group of anomalies using the Voroi diagram and the convex hull. 

\subsection{Anomaly Detection}
\label{sec:rw_anomaly_detection}
Anomaly detection plays a critical role in database projects. However, due to the predominantly unsupervised nature of anomaly detection (AD), there is often no definitive solution to AD tasks in many real-world applications **Chandola, "Anomaly detection in distributed systems"**. As a result, a variety of methods have been developed over the years, each employing distinct strategies to identify anomalies based on specific assumptions regarding the properties of the data, the anomalies, or both. Among these methods, several algorithms have gained widespread adoption:

Isolation Forest **Liu, "Isolation Forests"** isolates individual data points through recursive partitioning of the data space, identifying outliers based on the speed at which they can be separated. It constructs an ensemble of isolation trees, where data points that are isolated with shorter average path lengths are flagged as anomalies. \textit{Isolation Forest} is particularly effective when anomalies are sparse and well-separated from normal data points, especially in high-dimensional spaces. However, its performance may be compromised when anomalies are densely clustered or exhibit intricate patterns, as random splits may fail to capture these underlying structures. Moreover, this method relies solely on the ordinal ranking of each variable, disregarding distances and inter-variable relationships. As a result, its effectiveness may be diminished in situations where these factors are critical. The method operates based on the topology of the data, meaning that geometrical transformations, such as stretching or shrinking, do not affect the results. Nonetheless, in many practical applications, the geometry of the data plays an important role, and omitting this consideration may lead to false discoveries of abnormalities.

\textit{Single-Class SVM} **Schölkopf, "Estimating the Support of a High-Dimensional Distribution"** identifies anomalies as points that are farthest from the decision boundary. This method is particularly effective in high-dimensional spaces where traditional clustering algorithms fail. However, it requires careful tuning of parameters to avoid overfitting.

\textit{Local Outlier Factor (LOF)} **Breunig, "LOF: Identifying Density-Based Local Outliers"** identifies anomalies as points that have a significantly lower density than their neighbors. This method is particularly effective in datasets with varying densities and scales. However, it can be computationally expensive for large datasets.

\textit{One-class SVM} **Schölkopf, "Estimating the Support of a High-Dimensional Distribution"** identifies anomalies as points that are farthest from the decision boundary. This method is particularly effective in high-dimensional spaces where traditional clustering algorithms fail. However, it requires careful tuning of parameters to avoid overfitting.

\textit{K-means} **MacQueen, "Some methods for classification and analysis of multivariate observations"** identifies anomalies as points that are farthest from the cluster centroid. This method is particularly effective in datasets with well-separated clusters. However, it can be sensitive to initial conditions and may not perform well in high-dimensional spaces.

\textit{Hierarchical clustering} **Johnson, "Hierarchical Clustering for Anomaly Detection"** identifies anomalies as points that are farthest from the cluster centroid. This method is particularly effective in datasets with hierarchical structures. However, it can be computationally expensive for large datasets and may not perform well in high-dimensional spaces.

The primary limitation of the clustering process is its inherent instability, which is highly sensitive to the initial conditions.