\section{Related Work}
\label{sec:related_work}

This section presents a comprehensive review of the definition of the convex hull, followed by an exploration of the algorithms commonly utilized for its numerical computation. Subsequently, an in-depth examination of the current state-of-the-art anomaly detection (AD) algorithms is provided, focusing on their methodologies, applications, and performance characteristics.

\subsection{Convex hull}
\label{sec:rw_convex_hull}

The convex hull holds significant importance across various research domains, including computer graphics \cite{jayaram2016convex}, computational geometry \cite{lee1984computational}, optimization \cite{lachand2005minimizing}, and numerous other fields.



The convex hull is the smallest convex set that completely encloses a given set of points, minimizing the hyper-volume enclosed by the set. Formally, for a set of points \( \{\boldsymbol{x}_i\}_{i=1}^n \subset \mathbb{R}^d \), the convex hull is the minimal convex polytope containing all points, and it is expressed as \cite{cristescu2013non}:

\begin{equation}
CH(S) = \left\{ \sum_{i=1}^{n} \alpha_i \boldsymbol{x}_i \mid \alpha_i \geq 0 \, \wedge \, \sum_{i=1}^{n} \alpha_i = 1 \right\},
\end{equation}

where \( \alpha_i \) represents the non-negative scalar coefficients associated with each point \( \boldsymbol{x}_i \) in the set \( S \).


\subsubsection{Computing convex hull}
Various methods have been proposed to compute the convex hull of an arbitrary set of points, leveraging the convex hull's inherent convexity properties. Among these, three widely utilized algorithms are noteworthy: the Gift Wrap algorithm \cite{chan1996optimal}, the Graham Scan \cite{xu2010concave}, and the Quickhull algorithm \cite{barber1996quickhull}.


The Gift Wrap algorithm \cite{chan1996optimal}, also known as Jarvis March, constructs the convex hull incrementally by identifying boundary points one at a time. Starting with the leftmost point, which is guaranteed to be part of the convex hull, the algorithm iteratively selects the point that forms the smallest counterclockwise angle with the line segment formed by the current hull boundary. This selection ensures that the boundary progresses in a \say{wrapping} motion, ultimately enclosing all points. The hull is complete once the algorithm loops back to the starting point. The simplicity of the Gift Wrap algorithm makes it a natural choice for small datasets or cases where the number of points on the hull is small. However, its time complexity, \(\mathcal{O}(nh)\), where \(n\) is the total number of points and \(h\) is the number of points on the hull, can become prohibitive for larger datasets with dense point distributions.

Graham’s Scan algorithm \cite{xu2010concave} begins by identifying a reference point, typically the point with the smallest \(y\)-coordinate (or the leftmost point in case of ties). The algorithm then sorts all points by their polar angle relative to this reference point, ensuring a natural order for constructing the hull. Using a stack, Graham’s Scan processes these points sequentially, adding points to the stack while ensuring that each addition maintains the convexity of the boundary. If a new point causes the boundary to form a concave angle, points are removed from the stack until convexity is restored. The reliance on sorting gives Graham’s Scan a time complexity of \(\mathcal{O}(n \log n)\), making it particularly well-suited for static datasets where sorting overhead can be amortized.



The Quickhull algorithm employs a divide-and-conquer approach to compute the convex hull of a given set of points. The process begins by identifying the points with minimum and maximum \(x\)-coordinates defining the initial boundary segment. The algorithm then determines the point farthest from this segment, effectively partitioning the dataset into two subsets: points located to the left and right of the segment. This procedure is applied recursively to each subset, iteratively identifying the points that constitute the convex hull. Although Quickhull is well-suited for datasets with non-uniform spatial distributions, its worst-case computational complexity is \(\mathcal{O}(n^2)\), which occurs when the point distribution leads to excessive recursion.



These algorithms form the basis for efficient convex hull computation and have been extended to handle dynamic datasets. For instance, updating the convex hull after removing a single point can often be achieved in \(\mathcal{O}(n + k)\), where \(k\) is the number of points on the hull boundary, avoiding the need for a complete recomputation. Their adaptability and efficiency ensure these methods remain central to computational geometry and its applications in higher dimensions.

Chan's algorithm \cite{chan1996optimal} computes the convex hull with a time complexity of \( O(n \log n + n^{\lfloor d/2 \rfloor}) \), making it computationally efficient for large datasets. An alternative method is described by Nielsen and Nock \cite{nielsen2009sided}, which integrates the efficiency of divide-and-conquer strategies with the precision of geometric methods. This ensures that the convex hull can be computed in optimal time for various applications.

\subsubsection{Convex hull for anomaly detection}
Identifying outliers using the convex hull is based on the observation that anomalous points necessitate the expansion of the convex hull's boundaries. From an energy perspective, this approach can be interpreted as a trade-off analysis, wherein the inclusion of additional data points results in a transition from a set with a minimal convex hull to an expanded configuration encompassing these new points. This shift highlights the balance between maintaining a compact representation and accommodating potential outliers within the dataset.
Using a support vector machine for constructing boundaries hyperplane was done by Zhang and Gu~\cite{zhang2007ch} and Wand et al~\cite{wang2022anomaly}. Using the convex hull to improve the calculation of the mean and variance to detect anomalies was done by Costa et at\cite{costa2013partially}. Blaise et al \cite{blaise2022group} suggest a method to detect a group of anomalies using the Voroi diagram and the convex hull. 

\subsection{Anomaly Detection}
\label{sec:rw_anomaly_detection}
Anomaly detection plays a critical role in database projects. However, due to the predominantly unsupervised nature of anomaly detection (AD), there is often no definitive solution to AD tasks in many real-world applications \cite{ghahramani2003unsupervised}. As a result, a variety of methods have been developed over the years, each employing distinct strategies to identify anomalies based on specific assumptions regarding the properties of the data, the anomalies, or both. Among these methods, several algorithms have gained widespread adoption:

Isolation Forest \cite{cheng2019outlier} isolates individual data points through recursive partitioning of the data space, identifying outliers based on the speed at which they can be separated. It constructs an ensemble of isolation trees, where data points that are isolated with shorter average path lengths are flagged as anomalies. \textit{Isolation Forest} is particularly effective when anomalies are sparse and well-separated from normal data points, especially in high-dimensional spaces. However, its performance may be compromised when anomalies are densely clustered or exhibit intricate patterns, as random splits may fail to capture these underlying structures. Moreover, this method relies solely on the ordinal ranking of each variable, disregarding distances and inter-variable relationships. As a result, its effectiveness may be diminished in situations where these factors are critical. The method operates based on the topology of the data, meaning that geometrical transformations, such as stretching or shrinking, do not affect the results. Nonetheless, in many practical applications, the geometry of the data plays an important role, and omitting this consideration may lead to false discoveries of abnormalities.

\textit{Single-Class SVM} \cite{shin2005one,oza2018one} detect anomalies by learning a boundary around normal data. Trained on data from a single class, these methods assume points outside the learned boundary are anomalous. \textit{Single-Class SVMs} construct a hyperplane that maximally separates normal data from the origin.

Gaussian Mixture Models (GMM) \cite{li2016anomaly} detect anomalies by modeling data as a mixture of Gaussian distributions, identifying points with low likelihood under the model as anomalies. \textit{GMM} is suitable for data clusters that approximate Gaussian shapes but may struggle with non-Gaussian clusters or complex distributions. This can be a generalization of the Z-score. Instead of a single bell in the Gaussian distribution, there exist a few. In real-life data, this is not common. 

Local Outlier Factor (LOF) \cite{alghushairy2020review} measures the local density deviation of a data point relative to its neighbors, identifying outliers in areas of significantly lower density. \textit{LOF} is useful for data with local clusters or varying densities but may struggle with uniform-density data or when clear neighborhood structures are absent. This method detects abnormality without considering the global structure of the data. Nonetheless, detecting abnormalities globally is very important in many cases. 

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) \cite{schubert2017dbscan} groups data into dense regions, identifying points in sparse regions as outliers. \textit{DBSCAN} is effective for datasets with varying densities and distinct clusters but may perform poorly on data with uniform density or overlapping clusters.

K-means \cite{munz2007traffic} partitions data into a set number of clusters, flagging points with high distances from the nearest cluster center as anomalies. This method assumes spherical clusters and works best when clusters are compact and anomalies are far from cluster centers but are limited by varying cluster shapes or densities.

Mean Shift \cite{yang2021mean} iteratively shifts data points toward high-density regions, identifying clusters as density peaks and labeling points outside these clusters as outliers. \textit{Mean Shift} is effective when data has distinct density peaks but may be less effective in uniformly distributed data or data lacking prominent clusters.

The primary limitation of the clustering process is its inherent instability, which is highly sensitive to the initial conditions.