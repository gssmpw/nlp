\documentclass[12pt, double]{article}
\usepackage{times}
\usepackage{wrapfig}
\usepackage{verbatim}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{srcltx}
\usepackage{color}
\usepackage{lineno}
\usepackage{dirtytalk}
\usepackage{amsthm}
\usepackage{authblk}
\usepackage{listings}
\usepackage{tikz}
\usepackage{epsfig,amsfonts,amssymb}
\usepackage{adjustbox}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
% Page layout adjustments
\pagestyle{empty}
\setlength{\topmargin}{.1in}
\addtolength{\textwidth}{1.5in}
\addtolength{\oddsidemargin}{-0.75in}
\addtolength{\evensidemargin}{-0.75in}
\addtolength{\marginparwidth}{-0.5in}
\addtolength{\textheight}{1in}
\renewcommand{\floatpagefraction}{0.8}


\begin{document}

% Title page
\title{\Large Tighten The Lasso: A Convex Hull Volume-based Anomaly Detection Method}
\author[1,*,x]{Uri Itai}
\author[2]{Asael Bar Ilan}
\author[3,4,x]{Teddy Lazebnik}

\affil[1]{Department of Mathematics, The Guangdong Technion-Israel Institute of Technology, China}
\affil[2]{Department of Computer Science, Open University of Israel, Israel}
\affil[3]{Department of Mathematics, Ariel University, UK}
\affil[4]{Department of Cancer Biology, Cancer Institute, University College London, London, UK}
\affil[*]{Corresponding author: \texttt{uri.itai@gmail.com}}
\affil[x]{These authors contributed equally.}
\date{\today}

\maketitle

\thispagestyle{empty}

\begin{abstract}
The rapid advancements in data-driven methodologies have underscored the critical importance of ensuring data quality. Consequently, detecting out-of-distribution (OOD) data has emerged as an essential task to maintain the reliability and robustness of data-driven models, in general, and machine and deep learning models, in particular. In this study, we leveraged the convex hull property of a dataset and the fact that anomalies highly contribute to the increase of the CH's volume to propose a novel anomaly detection algorithm. Our algorithm computes the CH's volume as an increasing number of data points are removed from the dataset to define a decision line between OOD and in-distribution data points. We compared the proposed algorithm to seven widely used anomaly detection algorithms over ten datasets, showing comparable results for state-of-the-art (SOTA) algorithms. Moreover, we show that with a computationally cheap and simple check, one can detect datasets that are well-suited for the proposed algorithm which outperforms the SOTA anomaly detection algorithms.  \\ \\ \noindent
\textbf{Keywords}: Outlier Detection, Convex Hull, Out-of-Distribution.
\end{abstract}

% Reset page numbering and add header
\pagestyle{myheadings}
\markboth{Draft: \today}{Draft: \today}
\setcounter{page}{1}

\section{Introduction}
\label{sec:introduction}

Anomaly Detection (AD) is a critical component in machine learning, data analysis, statistics, and nearly all disciplines dealing with quantitative data. It involves identifying patterns, observations, or events that deviate significantly from expected norms \cite{chandola2009anomaly}, often referred to as anomalies or outliers. These anomalies frequently indicate rare but crucial occurrences \cite{pang2021deep,xu2019recent} and have diverse applications, including the early detection of mechanical issues \cite{chandola2009anomaly}, financial fraud detection \cite{ahmed2016survey}, cybersecurity \cite{ten2011anomaly}, enhanced decision-making processes \cite{prasad2010anomaly}, and real-time clinical alert systems \cite{habeeb2019real}.

In a data-driven context, AD is commonly regarded as a subfield of machine learning (ML) and can be mathematically framed as a binary classification problem, where data is categorized as either normal or anomalous \cite{jia2019anomaly,gornitz2013toward}. However, due to the inherent rarity of anomalies, conventional \say{normal} classification models often underperform in this task, necessitating the development of specialized algorithms tailored for anomaly detection \cite{nassif2021machine}.

Indeed, various groups of anomaly detection (AD) algorithms have been developed, each tailored to different types of data and problem contexts \cite{ten2011anomaly,fernando2021deep,himeur2021artificial}. These algorithms can be broadly categorized into three primary types: statistical-based methods, machine learning (ML)-based methods, and distance-based methods. 

The traditional method of determining anomalies by taking $k$ standard deviations away from the mean is extended using the Z-score (T statistics), in which the confidence interval is represented as an ellipsoid. This advancement paves the way for more refined, statistically-based techniques, including Gaussian mixture models \cite{li2016anomaly}, Mahalanobis distance \cite{barnard2000detecting}, and hypothesis testing \cite{cohen2015active}.
This method concerning the convex hull was done in \cite{costa2013partially}.
These methods rely on the presumption that the data adheres to a specific statistical distribution, thereby identifying any points that significantly diverge from this distribution as anomalies.

ML-based methods can be classified into supervised, semi-supervised, and unsupervised approaches \cite{alpaydin2020introduction}. Supervised methods, such as neural networks or support vector machines, require labeled datasets containing normal and abnormal instances. However, obtaining these labels is often impractical in real-world scenarios due to the high cost and difficulty associated with labeling data \cite{wang2014new}. In contrast, semi-supervised methods, such as  One-class SVM \cite{manevitz2001one}, One-class deep neural networks \cite{oza2018one}, and auto-encoders \cite{zhou2017anomaly}, as well as local outlier factor \cite{cheng2019outlier}, are trained exclusively on normal data and detect anomalies by identifying instances that do not conform to the learned patterns \cite{boukerche2020outlier}. Additional methods include contracting a gain function \cite{novello2024improving} or constructing a distribution \cite{costa2013partially}.

Another approach involves the use of Voronoi diagrams \cite{blaise2022group}. In this method, the data is partitioned into bins, and points that do not fit within these bins are flagged as outliers. The computation of each cell in the diagram is based on the convex hull.


Distance-based methods, such as K-nearest neighbors (KNN) \cite{peterson2009k} and clustering techniques like DBSCAN \cite{schubert2017dbscan}, involve measuring the distance or similarity between data points. Data points far from clusters or with few neighboring points are considered anomalies. These methods are typically limited in scalability, as they struggle to handle large datasets due to their computational complexity \cite{mandhare2017comparative}. Moreover, distance-based methods tend to perform poorly in non-uniform data distributions, as they assume anomalies are far from most data points. 

An alternative approach to AD utilizes the concept of a convex hull, defined as the smallest convex set that contains a given set of points in a metric space \cite{avis1995how}. Simply put, a convex hull is a set of outermost nails on a board stretching a rubber band around a set of nails when released. Computing the convex hull plays an important role in many fields of computer science, such as computer graphics \cite{barber1996quickhull} and computer-aided design \cite{barnhill2014computer}. This geometric framework leads to convex hull-based algorithms for AD, which offers an efficient way to identify outliers by leveraging geometric properties \cite{wang2022anomaly}. A convex hull, representing the boundary of the normal data distribution, delineates the smallest convex shape enclosing a set of points in multi-dimensional space \cite{wang2022anomaly}. Any data point outside this convex boundary is considered anomalous \cite{costa2013partially}. These algorithms are particularly powerful because they provide a clear, interpretable geometric definition of normal behavior, making it easy to visualize and understand the separation between normal and anomalous data. Moreover, convex hull-based methods are adaptable to high-dimensional and complex datasets as they require only a well-defined distance metric, which is relatively straightforward in most real-world applications \cite{blaise2022group,li2016improving,olteanu2023meta}. Several studies already used convex-hull-based AD algorithms, showing promising results \cite{ch_g_1,ch_g_2,ch_g_3}. For instance, Liu at al. \cite{liu2009novel} proposed adding a discount factor to the convex hull to avoid overfitting on the training dataset. Casale et al. \cite{casale2014approximate} reduce the computation time of the convex hull as an AD algorithm using the approximate polytope ensemble technique. In an applied context, He et al. \cite{he2020kernel} propose a novel classifier, the Kernel Flexible and Displaceable Convex Hull-based Tensor Machine, designed for gearbox fault diagnosis using multi-source signals. This classifier uses a convex hull approach in tensor feature space to classify feature tensors, demonstrating improved robustness and effectiveness in identifying gearbox faults with small sample sizes.

Although efficient, existing algorithms lack a definitive mechanism for determining the optimal convex hull shape for a given dataset without introducing additional assumptions. Furthermore, their practical applications remain ambiguous within the broader context of state-of-the-art anomaly detection (AD) algorithms.

In this study, we propose a novel parameter-free AD algorithm grounded in the convex hull family of methods. This algorithm balances two configurations: the convex hull encompassing the entire dataset and the convex hull computed for a subset of the data, excluding the most "distant" samples. By leveraging this approach, we introduce a topology-agnostic AD algorithm that maintains computational efficiency and resource requirements comparable to other state-of-the-art AD algorithms while exhibiting robustness to complex data geometries.


The proposed method not only identifies outliers but also quantifies the extent to which a given instance deviates from the norm. This metric is instrumental in distinguishing between tail events and out-of-distribution instances, a distinction of particular importance in time series analysis \cite{blazquez2021review} and online data processing \cite{subramaniam2006online}. While a detailed examination of this distinction lies beyond the scope of the present paper, it will be addressed in future research.

We conducted extensive experiments using ten real-world datasets spanning various domains to evaluate the proposed method. We benchmarked the performance of our model against seven state-of-the-art AD algorithms representing all three principal categories of AD methods. The results demonstrate. The Convex Hull algorithm demonstrates strong performance, surpassing all competing methods except for Isolation Forest and Local Outlier Factor. However, Isolation Forest does not account for distances and angles and remains invariant under shearing transformations, which may not always be desirable. In contrast, Local Outlier Factor is a local algorithm, meaning it primarily identifies local anomalies rather than global ones. As a result, it may fail to detect global outliers. Consequently, the Convex Hull-based outlier detection algorithm proves to be effective in scenarios that require a global geometric perspective.

The rest of the paper is organized as follows. Section \ref{sec:related_work} reviews the state-of-the-art AD algorithms as well as the convex hull family of algorithms and provides examples of geometry-based AD algorithms for one and two dimensions. Section \ref{sec:algorithm} formally introduces the proposed convex-hull-based AD algorithm with an analytical analysis of the algorithm. Section \ref{sec:experiments} outlines the experiments on real-world data comparing the proposed algorithms with current state-of-the-art AD algorithms and also the sensitivity analysis of the proposed algorithm. Finally, Section \ref{sec:conclusions} discusses the applicative outcomes of this study and suggests possible future research. 

\section{Related Work}
\label{sec:related_work}

This section presents a comprehensive review of the definition of the convex hull, followed by an exploration of the algorithms commonly utilized for its numerical computation. Subsequently, an in-depth examination of the current state-of-the-art anomaly detection (AD) algorithms is provided, focusing on their methodologies, applications, and performance characteristics.

\subsection{Convex hull}
\label{sec:rw_convex_hull}

The convex hull holds significant importance across various research domains, including computer graphics \cite{jayaram2016convex}, computational geometry \cite{lee1984computational}, optimization \cite{lachand2005minimizing}, and numerous other fields.



The convex hull is the smallest convex set that completely encloses a given set of points, minimizing the hyper-volume enclosed by the set. Formally, for a set of points \( \{\boldsymbol{x}_i\}_{i=1}^n \subset \mathbb{R}^d \), the convex hull is the minimal convex polytope containing all points, and it is expressed as \cite{cristescu2013non}:

\begin{equation}
CH(S) = \left\{ \sum_{i=1}^{n} \alpha_i \boldsymbol{x}_i \mid \alpha_i \geq 0 \, \wedge \, \sum_{i=1}^{n} \alpha_i = 1 \right\},
\end{equation}

where \( \alpha_i \) represents the non-negative scalar coefficients associated with each point \( \boldsymbol{x}_i \) in the set \( S \).


\subsubsection{Computing convex hull}
Various methods have been proposed to compute the convex hull of an arbitrary set of points, leveraging the convex hull's inherent convexity properties. Among these, three widely utilized algorithms are noteworthy: the Gift Wrap algorithm \cite{chan1996optimal}, the Graham Scan \cite{xu2010concave}, and the Quickhull algorithm \cite{barber1996quickhull}.


The Gift Wrap algorithm \cite{chan1996optimal}, also known as Jarvis March, constructs the convex hull incrementally by identifying boundary points one at a time. Starting with the leftmost point, which is guaranteed to be part of the convex hull, the algorithm iteratively selects the point that forms the smallest counterclockwise angle with the line segment formed by the current hull boundary. This selection ensures that the boundary progresses in a \say{wrapping} motion, ultimately enclosing all points. The hull is complete once the algorithm loops back to the starting point. The simplicity of the Gift Wrap algorithm makes it a natural choice for small datasets or cases where the number of points on the hull is small. However, its time complexity, \(\mathcal{O}(nh)\), where \(n\) is the total number of points and \(h\) is the number of points on the hull, can become prohibitive for larger datasets with dense point distributions.

Graham’s Scan algorithm \cite{xu2010concave} begins by identifying a reference point, typically the point with the smallest \(y\)-coordinate (or the leftmost point in case of ties). The algorithm then sorts all points by their polar angle relative to this reference point, ensuring a natural order for constructing the hull. Using a stack, Graham’s Scan processes these points sequentially, adding points to the stack while ensuring that each addition maintains the convexity of the boundary. If a new point causes the boundary to form a concave angle, points are removed from the stack until convexity is restored. The reliance on sorting gives Graham’s Scan a time complexity of \(\mathcal{O}(n \log n)\), making it particularly well-suited for static datasets where sorting overhead can be amortized.



The Quickhull algorithm employs a divide-and-conquer approach to compute the convex hull of a given set of points. The process begins by identifying the points with minimum and maximum \(x\)-coordinates defining the initial boundary segment. The algorithm then determines the point farthest from this segment, effectively partitioning the dataset into two subsets: points located to the left and right of the segment. This procedure is applied recursively to each subset, iteratively identifying the points that constitute the convex hull. Although Quickhull is well-suited for datasets with non-uniform spatial distributions, its worst-case computational complexity is \(\mathcal{O}(n^2)\), which occurs when the point distribution leads to excessive recursion.



These algorithms form the basis for efficient convex hull computation and have been extended to handle dynamic datasets. For instance, updating the convex hull after removing a single point can often be achieved in \(\mathcal{O}(n + k)\), where \(k\) is the number of points on the hull boundary, avoiding the need for a complete recomputation. Their adaptability and efficiency ensure these methods remain central to computational geometry and its applications in higher dimensions.

Chan's algorithm \cite{chan1996optimal} computes the convex hull with a time complexity of \( O(n \log n + n^{\lfloor d/2 \rfloor}) \), making it computationally efficient for large datasets. An alternative method is described by Nielsen and Nock \cite{nielsen2009sided}, which integrates the efficiency of divide-and-conquer strategies with the precision of geometric methods. This ensures that the convex hull can be computed in optimal time for various applications.

\subsubsection{Convex hull for anomaly detection}
Identifying outliers using the convex hull is based on the observation that anomalous points necessitate the expansion of the convex hull's boundaries. From an energy perspective, this approach can be interpreted as a trade-off analysis, wherein the inclusion of additional data points results in a transition from a set with a minimal convex hull to an expanded configuration encompassing these new points. This shift highlights the balance between maintaining a compact representation and accommodating potential outliers within the dataset.
Using a support vector machine for constructing boundaries hyperplane was done by Zhang and Gu~\cite{zhang2007ch} and Wand et al~\cite{wang2022anomaly}. Using the convex hull to improve the calculation of the mean and variance to detect anomalies was done by Costa et at\cite{costa2013partially}. Blaise et al \cite{blaise2022group} suggest a method to detect a group of anomalies using the Voroi diagram and the convex hull. 

\subsection{Anomaly Detection}
\label{sec:rw_anomaly_detection}
Anomaly detection plays a critical role in database projects. However, due to the predominantly unsupervised nature of anomaly detection (AD), there is often no definitive solution to AD tasks in many real-world applications \cite{ghahramani2003unsupervised}. As a result, a variety of methods have been developed over the years, each employing distinct strategies to identify anomalies based on specific assumptions regarding the properties of the data, the anomalies, or both. Among these methods, several algorithms have gained widespread adoption:

Isolation Forest \cite{cheng2019outlier} isolates individual data points through recursive partitioning of the data space, identifying outliers based on the speed at which they can be separated. It constructs an ensemble of isolation trees, where data points that are isolated with shorter average path lengths are flagged as anomalies. \textit{Isolation Forest} is particularly effective when anomalies are sparse and well-separated from normal data points, especially in high-dimensional spaces. However, its performance may be compromised when anomalies are densely clustered or exhibit intricate patterns, as random splits may fail to capture these underlying structures. Moreover, this method relies solely on the ordinal ranking of each variable, disregarding distances and inter-variable relationships. As a result, its effectiveness may be diminished in situations where these factors are critical. The method operates based on the topology of the data, meaning that geometrical transformations, such as stretching or shrinking, do not affect the results. Nonetheless, in many practical applications, the geometry of the data plays an important role, and omitting this consideration may lead to false discoveries of abnormalities.

\textit{Single-Class SVM} \cite{shin2005one,oza2018one} detect anomalies by learning a boundary around normal data. Trained on data from a single class, these methods assume points outside the learned boundary are anomalous. \textit{Single-Class SVMs} construct a hyperplane that maximally separates normal data from the origin.

Gaussian Mixture Models (GMM) \cite{li2016anomaly} detect anomalies by modeling data as a mixture of Gaussian distributions, identifying points with low likelihood under the model as anomalies. \textit{GMM} is suitable for data clusters that approximate Gaussian shapes but may struggle with non-Gaussian clusters or complex distributions. This can be a generalization of the Z-score. Instead of a single bell in the Gaussian distribution, there exist a few. In real-life data, this is not common. 

Local Outlier Factor (LOF) \cite{alghushairy2020review} measures the local density deviation of a data point relative to its neighbors, identifying outliers in areas of significantly lower density. \textit{LOF} is useful for data with local clusters or varying densities but may struggle with uniform-density data or when clear neighborhood structures are absent. This method detects abnormality without considering the global structure of the data. Nonetheless, detecting abnormalities globally is very important in many cases. 

Density-Based Spatial Clustering of Applications with Noise (DBSCAN) \cite{schubert2017dbscan} groups data into dense regions, identifying points in sparse regions as outliers. \textit{DBSCAN} is effective for datasets with varying densities and distinct clusters but may perform poorly on data with uniform density or overlapping clusters.

K-means \cite{munz2007traffic} partitions data into a set number of clusters, flagging points with high distances from the nearest cluster center as anomalies. This method assumes spherical clusters and works best when clusters are compact and anomalies are far from cluster centers but are limited by varying cluster shapes or densities.

Mean Shift \cite{yang2021mean} iteratively shifts data points toward high-density regions, identifying clusters as density peaks and labeling points outside these clusters as outliers. \textit{Mean Shift} is effective when data has distinct density peaks but may be less effective in uniformly distributed data or data lacking prominent clusters.

The primary limitation of the clustering process is its inherent instability, which is highly sensitive to the initial conditions.

\section{Convex Hull Method for Anomaly Detection}
\label{sec:algorithm}
A volume-based convex hull method provides a robust framework for anomaly detection by leveraging the geometric properties of the data distribution to identify points that deviate significantly from the majority. The convex hull's volume measures the dataset's spatial extent within \( n \)-dimensional space. Anomalies, or outliers, are often located at the periphery of the data distribution, where their separation from the dense data core disproportionately inflates the convex hull's volume.

The proposed method identifies compact and densely distributed subsets of data by minimizing the convex hull volume while maximizing the number of enclosed points. This approach effectively excludes anomalous points. However, balancing these conflicting objectives—maximizing data inclusion while minimizing the convex hull volume—poses significant challenges, akin to the sorites paradox \cite{hyde2011sorites} when considered in extreme scenarios. 

To address these challenges, a well-defined stopping condition must be established for the iterative removal or addition of points. Alternatively, appropriate weights can be assigned to these competing objectives within an optimization framework to achieve a balance between the two goals.

Formally, to define an algorithm based on the above motivation, we first define \( S_p \) as the subset of \( S \), a set \( S \subseteq \mathbb{R}^n \), that maximizes the volume of its convex hull. Based on this definition, the volume-based convex hull takes the following form:
\begin{equation}\label{Eq:alt_form}
f(p) = |S_p| - \lambda \, \text{vol}(CH(S_p)), \quad \lambda > 0,
\end{equation}
where \( CH(S_p) \) represents the convex hull of the subset \( S_p \), and \( \text{vol}(CH(S_p)) \) denotes its volume. The function \( f(p) \) quantifies the balance between the size of the subset \( |S_p| \) and the volume of its convex hull, emphasizing subsets that are both compact and densely packed. \( \lambda \) acts as a sensitivity parameter such that larger values of \( \lambda \) prioritize minimizing convex hull volume, effectively isolating outliers, while smaller values of \( \lambda \) tolerate broader variations in the data distribution.

Algorithm \ref{Alg:Ab_CH_Mod} shows a pseudo-code of the proposed algorithm which accepts a dataset (\(S\)), stopping criteria (\(SC\)), and an algorithm to compute the convex hull \(CH\); and returns the subset of samples in the dataset which are not anomaly-free.   

\begin{algorithm}[H]
\caption{Modified Volume-Based Convex Hull Anomaly Detection}\label{Alg:Ab_CH_Mod}
\begin{algorithmic}[1]
\Require Dataset \( S \subseteq \mathbb{R}^n \), stopping criteria \( SC \), convex hull method \( \text{CH} \)
\State Initialize \( S_p \gets S, r \gets \emptyset \)
    \State \( S_h \gets \text{CH}(S_p) \)
    \State \( vol_c \gets volume(S_h) \)
    \State \( vol_m \gets vol_c \)
    \State \( S_h^{new} \gets S_h \)
    \State \( r_n \gets \emptyset \)
\Repeat
    \For{\( p \in S_h \)}
        \State \( S_p^n \gets S_p \setminus \{p\} \)
        \State \( S_h^n \gets \text{CH}(S_p^n) \)
        \State \( vol_n \gets volume(S_h^n) \)
        \If{\( vol_n \leq vol_m\)}
            \State \( S_h^{new} \gets S_h^n \)
            \State \( vol_m \gets vol_n \)
            \State \( r_n \gets p \)
        \EndIf
    \EndFor
    \State \( S_h \gets S_h^{new} \) 
    \State \( r \gets r \cup r_n \) 
    \If{\( SC \vee |r| = |S|\)}
        \State \Return \( S \setminus r \)
    \EndIf
\Until{True}
\end{algorithmic}
\end{algorithm}

Practically, one can use multiple stopping conditions, including the Elbow Point Method \cite{thorndike1953belongs} which locates the inflection point in a cost function curve, and the Akaike Information Criterion (AIC) 
\cite{akaike2011akaike}.

The computational complexity of the proposed algorithm is determined by the iterative nature of its optimization process and the operations performed within each iteration. The algorithm comprises an outer loop that iterates until the stopping condition \( SC \) is satisfied or all points are removed from the dataset (\( |r| = |S| \)). Let \( T \) denote the total number of iterations. 

Within each iteration, the algorithm computes the convex hull \( S_h \) of the remaining dataset \( S_p \) and evaluates all points within the current convex hull. For a dataset of size \( n \), computing the convex hull requires \( O(|S_p|^2) \) operations in the worst case, where \( |S_p| \leq n \). The algorithm evaluates each point \( p \) in the convex hull \( S_h \) by temporarily removing it to calculate the volume of the resulting convex hull \( S_h^n \). Each convex hull computation for \( S_p^n = S_p \setminus \{p\} \) also has a complexity of \( O(|S_p|^2) \). 

Given that \( |S_h| \), the number of points in the convex hull, can be as large as \( n \), evaluating all points in \( S_h \) requires \( O(|S_h| \cdot |S_p|^ 2) \) operations per iteration. In the worst-case scenario where \( |S_h| = n \) and \( |S_p| = n \), this results in \( O(n \cdot n^2) = O(n^3) \) operations for one iteration. The total number of iterations \( T \) is bounded by \( n \) in the worst case, as at least one point is removed from \( S_p \) in each iteration. Thus, the overall time complexity of the algorithm becomes \( O(T \cdot n^3) = O(n^4) \).  



The memory requirements of the algorithm are primarily determined by the need to store the input dataset, the convex hull \( S_h \), and intermediate results. The input dataset \( S \subseteq \mathbb{R}^d \), consisting of \( n \) points in \( d \)-dimensional space, requires \( O(n \cdot d) \) memory. 

The convex hull \( S_h \) requires storage proportional to the number of vertices in the hull. In the worst-case scenario, where all \( n \) points are part of \( S_h \), this also necessitates \( O(n \cdot d) \) memory. Temporary storage is required for subsets such as \( S_p^n = S_p \setminus \{p\} \), which similarly demands \( O(n \cdot d) \) memory. 

The computation of the volume introduces a negligible constant memory overhead. Therefore, the memory complexity is primarily driven by the storage requirements for the dataset and the convex hull, resulting in an overall memory complexity of \( O(n \cdot d) \).

\section{Experiments}
\label{sec:experiments}
In this section, we present the conducted experiments to evaluate the proposed CH anomaly detection algorithm. 

\subsection{Datasets}
Table \ref{table:datasets} presents the datasets used in this study with their number of rows, cols, and portion of tagged anomalies. The datasets range from small ones with as few as 148 rows and up to medium ones with over 60 thousand rows. The number of cols also ranges from 7 to 41 cols, representing a relatively wide range of configurations. In addition, we also present the datasets that are considered appropriate to the CH algorithms, denoted by \textit{Yes} in the last column, as these show a large reduction in the CH's volume over the first 10 steps of the algorithm. 

\begin{table}[h!]
\centering
\begin{tabular}{l l c c c c}
\hline \hline
\textbf{Name} & \textbf{Description} & \textbf{\# Samples} & \textbf{\# Cols} & \textbf{Portion of Anomalies} & \textbf{CH-friendly} \\ \hline\hline
Glass  & Glass identification data & 214 & 7 & 4.2\% & No \\
Ionosphere  & Radar data for ionosphere detection & 351 & 32 & 35.9\% & No\\
Lymphography  & Medical lymphography data & 148 & 19 & 4.1\% & No\\
PenDigits  & Handwritten digit recognition data & 9,868 & 16 & 0.2\%& No \\
Shuttle  & NASA shuttle anomaly data & 1,013 & 9 & 1.3\% & No \\
WBC  & Wisconsin breast cancer data & 454 & 9 & 2.2\%& No \\
Waveform  & Synthetic waveform data & 3,443 & 21 & 2.9\%  & Yes \\
KDDCup99  & Network intrusion detection dataset & 60,632 & 41 & 0.4\%& Yes \\
WDBC  & Wisconsin diagnostic breast cancer & 367 & 30 & 2.7\% & Yes \\
WPBC  & Wisconsin prognostic breast cancer & 198 & 33 & 23.7\% & Yes \\ \hline \hline
\end{tabular}
\caption{An overview of the datasets used in this study.}
\label{table:datasets}
\end{table}

\subsection{Convex hull anomaly detection implementation}
In order to explore different implementations of the proposed CH anomaly detection algorithm, we consider two properties - a dimension reduction algorithm and a stop condition. For the dimension reduction, we adopted the popular principal component analysis (PCA) \cite{roweis1997algorithms} and the t-distributed stochastic neighbor embedding (t-SNE) \cite{wattenberg2016use} algorithms. As a default, we assume both reduce to a two-dimensional space. We used this step to obtain a feasible computational time for the proposed algorithm. For the stop condition, we adopted three strategies: Naive, Elbow, and Optimal. For the Naive strategy, we stopped the process once the change in the CH's volume was less than 1\% of the original change (i.e., the change in the CH's volume between the original dataset and after removing the first data point). For the Elbow strategy, we performed the entire CH's computation, up to three data points, and took the elbow point of the CH's volume profile \cite{bholowalia2014ebk}. Finally, the Optimal strategy is an unrealistic one and used only to explore the model's performance, in which the number of anomaly points is known in advance and the CH stops after the same number of steps.

\subsection{Performance}
Table \ref{table:p1} presents the performance of all six CH configurations (for two-dimensionality reduction methods over three stooping strategies) and seven baseline anomaly detection algorithms in terms of their accuracy, \(F_1\) score, recall, and precision. Notably, the proposed Convex Hull algorithm performs comparably to the Isolation Forest model, which is considered the current state-of-the-art and obtains the highest \(F_1\) score of 0.2619. More precisely, the Convex Hull algorithm with the optimal stopping strategy and t-SNE obtained the highest precision (and accuracy) followed by the other versions of the Convex Hull and only then by the Isolation forest algorithm. The DBSCAN algorithm obtains an almost perfect recall of 0.9968 but overall produces extremely poor results with \(F_1\) score of 0.0528. Expectedly, the optimal, elbow, and naive stop strategies result in a monotonic decrease in performance in terms of all metrics. In addition, the t-SNE produces better results in terms of precision and \(F_1\) score but worse in terms of recall compared to the PCA method. 

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \hline \hline
        \textbf{Algorithm} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} \\
        \hline \hline
        Convex Hull, Naive + PCA       & 0.8739 & 0.2181 & 0.2702 & 0.2112 \\
        Convex Hull, Naive + t-SNE     & 0.9150 & 0.2337 & 0.2244 & 0.2240 \\
        Convex Hull, Elbow + PCA       & 0.8739 & 0.2181 & 0.2702 & 0.2112 \\
        Convex Hull, Elbow + t-SNE     & 0.9163 & 0.2335 & 0.2203 & 0.2261 \\
        Convex Hull, Optimal + PCA     & 0.8739 & 0.2181 & 0.2702 & 0.2112 \\
        Convex Hull, Optimal + t-SNE   & \textbf{0.9154} & 0.2343 & 0.2229 & \textbf{0.2278} \\
        \hline
        Isolation Forest              & 0.9077 & \textbf{0.2619} & 0.6681 & 0.1812 \\
        Local Outlier Factor          & 0.9003 & 0.2110 & 0.6135 & 0.1438 \\
        One-Class SVM                 & 0.7665 & 0.1703 & 0.5824 & 0.1173 \\
        Gaussian Mixture Models       & 0.4131 & 0.0894 & 0.5203 & 0.0612 \\
        K-means                       & 0.3727 & 0.0736 & 0.5086 & 0.0480 \\
        DBSCAN                        & 0.0346 & 0.0528 & \textbf{0.9968} & 0.0283 \\
        Mean Shift                    & 0.0622 & 0.0497 & 0.9090 & 0.0267 \\
        \hline \hline
    \end{tabular}
    \caption{Performance of the models with comparison to baseline algorithms. The best outcome for each metric is highlighted in bold.}
    \label{table:p1}
\end{table}

Table \ref{table:p2} presents a similar analysis to Table \ref{table:p1} but focuses only on the four datasets that are deemed Convex Hull friendly (see Table \ref{table:datasets}). In such settings, the Convex Hull with an optimal stooping strategy with the t-SNE obtains the highest \(F_1\) score and precision of 0.1698 and 0.1491, respectively. Even the more realistic configuration of the algorithm with the elbow point stooping condition outperforms all other algorithms in terms of \(F_1\) score and precision of 0.1657 and 0.1457, respectively. Like before, optimal, elbow, and naive stooping strategies result in a monotonic decrease in performance in terms of all metrics. In addition, the t-SNE produces better results in terms of precision and \(F_1\) score but worse in terms of recall compared to the PCA method. 

\begin{table}[h!]
    \centering
    \begin{tabular}{lcccc}
        \hline \hline
        \textbf{Algorithm} & \textbf{Accuracy} & \textbf{F1 Score} & \textbf{Recall} & \textbf{Precision} \\
        \hline \hline
        Convex Hull, Naive + PCA     & \textbf{0.9485} & 0.1452 & 0.1577 & 0.1460 \\
        Convex Hull, Naive + t-SNE   & 0.9462 & 0.1661 & 0.2022 & 0.1461 \\
        Convex Hull, Elbow + PCA     & 0.9507 & 0.1134 & 0.1194 & 0.1470 \\ 
        Convex Hull, Elbow + t-SNE   & 0.9462 & 0.1657 & 0.2015 & 0.1457 \\
        Convex Hull, Optimal + PCA   & \textbf{0.9485} & 0.1452 & 0.1577 & 0.1460 \\
        Convex Hull, Optimal + t-SNE & 0.9464 & \textbf{0.1698} & 0.2067 & \textbf{0.1491} \\ \hline
        Isolation Forest            & 0.8856 & 0.0899 & 0.4992 & 0.0579 \\
        Local Outlier Factor        & 0.8886 & 0.1097 & 0.3523 & 0.0726 \\
        One-Class SVM               & 0.7750 & 0.0569 & 0.4106 & 0.0349 \\
        Mean Shift                  & 0.0760 & 0.0484 & \textbf{0.9995} & 0.0261 \\
        Gaussian Mixture Models     & 0.4577 & 0.0449 & 0.5820 & 0.0249 \\
        K-means                     & 0.4572 & 0.0459 & 0.6448 & 0.0253 \\
        DBSCAN                      & 0.0344 & 0.0478 & 0.9927 & 0.0259 \\
        \hline \hline
    \end{tabular}
    \caption{Performance of the models with comparison to baseline algorithms for the four datasets detected as Convex Hull friendly. The best outcome for each metric is highlighted in bold.}
    \label{table:p2}
\end{table}

\subsection{Sensitivity analysis}
We computed the one-dimensional sensitivity analysis \cite{peter2010numerical} of each of the epidemiological parameters on the decline in fertility. Table \ref{table:sensitivity} outlines the results of the analysis. presenting mean change obtained from a linear regression \cite{seber2012linear} model fitted on all the datasets ranging between 50\% and 100\% of the data with the smallest step size for each feature from the original value presented in Table \ref{table:datasets}. We obtained a coefficient of determination of \(R^2 = 0.906\) indicating the values of the linear regression well capture the sensitivity of the model's performance, in terms of F1 score to each of the model's parameters. Peculiarly, only the number of samples (rows), dimensionality reduction dimension, and stopping condition are statistically significant with \(p < 0.05\).  

\begin{table}[h]
    \centering
    \begin{tabular}{lcc}
        \hline \hline
        \textbf{Parameter} & \textbf{Value} & \textbf{p-value}  \\
        \hline \hline
        \# of samples & \(0.032\) & \(4.2 \cdot 10^{-5}\) \\
        \# of features & \(-0.063\) & \(7.0 \cdot 10^{-3}\) \\
        Portion of anomalies & \(0.008\) & \(0.128\) \\
        CH-friendly & \(0.049\) & \(0.038\) \\
        Stopping condition - Naive & \(-0.005\) & \(0.046\) \\
        Stopping condition - elbow & \(0.001\) & \(0.033\) \\
        Dimensionality reduction - PCA & \(-0.082\) & \(0.083\) \\
        Dimensionality reduction dimension & \(-0.004\) & \(0.001\) \\
        \hline \hline
    \end{tabular}
    \caption{Sensitivity analysis of the dataset and proposed model parameters. The table provides parameter values and their p-values from a linear regression model predicting the model's average F1 score across the datasets in Table \ref{table:datasets}.}
    \label{table:sensitivity}
\end{table}


\subsection{Special cases}In Figure~\ref{subfig:s3}, the convex hull provides an accurate representation of the geometric structure of the data. Consequently, the convex hull method performs effectively in this case. 

In contrast, Figs.~\ref{subfig:s1} and \ref{subfig:s2} illustrate cases involving a \textit{torus} and a \textit{circle} with noise, respectively. These cases are synthetically designed to highlight edge cases where the proposed Convex Hull model exhibits suboptimal performance. The primary reason for this limitation is that the majority of points in these sets lie on the convex hull's boundary. As a result, processing these boundary points demands substantial computational resources. However, it is important to note that the cases are relatively rare in most real-world datasets.

\begin{figure}[!ht]
    \centering
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{prefer_edges.pdf}
        \caption{Unnormalized dimensions.}
        \label{subfig:s3}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{donat.pdf}
        \caption{Torus.}
        \label{fig:subfig:s1}
    \end{subfigure}
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{circle.pdf}
        \caption{Circle with noise.}
        \label{fig:subfig:s2}
    \end{subfigure}
    \caption{Special cases where the proposed Convex Hull algorithm produces poor results. The black dots indicate the in-distribution while the red ones indicate anomalies.}
    \label{fig:subfigures}
\end{figure}

Table \ref{table:special} presents the \(F_1\) score of the Convex Hull algorithm with the optimal stopping strategy compared to the Isolation Forest algorithm for the three edge cases shown in Figs. \ref{subfig:s1}, \ref{subfig:s2}, and \ref{subfig:s3}. For the torus case the Convex Hull receives \(F_1\) score of 0 while the Isolation Forest archives \(F_1\) score of 1. The latter two cases show less dramatic differences with around 0.18 and 0.1 differences in the \(F_1\) score between the Isolation Forest and the Convex Hull algorithms. 

\begin{table}[h!]
    \centering
    \begin{tabular}{llc}
        \hline \hline
        \textbf{Algorithm} & \textbf{Case} & \textbf{F1 Score}  \\
        \hline \hline
        Convex Hull, Optimal & Torus & 0.0000  \\
        Convex Hull, Optimal & Circle with noise & 0.2393 \\
        Convex Hull, Optimal & Unnormalized dimensions & 0.0823 \\ \hline
        Isolation Forest & Torus & 1.0000 \\
        Isolation Forest & Circle with noise & 0.4170 \\
        Isolation Forest & Unnormalized dimensions & 0.1874  \\
        \hline \hline
        \label{table:p2}
    \end{tabular}
    \caption{\(F_1\) score of the Convex Hull algorithm with the optimal stopping strategy compared to the Isolation Forest algorithm for the three edge cases shown in Figs. \ref{subfig:s1}, \ref{subfig:s2}, and \ref{subfig:s3}.}
    \label{table:special}
\end{table}

\section{Conclusions}
\label{sec:conclusions}
In this study, we introduce a volume-based Convex Hull method for anomaly detection. The approach effectively identifies anomalies by leveraging the geometric properties of data distribution, prioritizing compact and dense subsets while excluding outliers. This dual-objective strategy ensures that anomalies, which disproportionately increase the convex hull's volume, are systematically identified. However, balancing the competing objectives of minimizing volume and maximizing data inclusion is a non-trivial challenge that requires careful calibration of parameters and stopping conditions.

The computational complexity of the method is notable, with a worst-case time complexity of \(O(N^4)\) (where \(N\) is the number of data points in the dataset). This significant cost arises from the iterative nature of the algorithm and the repeated computation of convex hulls. Nonetheless, using dimension reduction methods, such as PCA and t-SNE, explored in the experiments, proves to be crucial in mitigating computational demands while obtaining comparable and sometimes superior results to current state-of-the-art animal detection algorithms. That said, these methods introduce trade-offs by potentially altering the original data structure, which may impact the performance of anomaly detection. Moreover, the role of stopping conditions in the overall effectiveness of the method is central to the algorithm's performance. Namely, the naive, elbow, and optimal stopping strategies reveal varying levels of precision and computational efficiency, while the latter was not explicitly evaluated. Notably, while the naive strategy offers simplicity, its reliance on fixed thresholds can result in premature termination or excessive computation. The elbow method strikes a balance by dynamically adapting to the change in the convex hull's volume, whereas the optimal method, despite being impractical for real-world applications, serves as a valuable benchmark for evaluating the algorithm's potential.

The proposed algorithm considers the geometric properties of the set, particularly the importance of distances and angles. It remains invariant under geometric transformations such as translation, rotation, and reflection. Additionally, it is invariant under isotropic scaling. However, transformations that do not preserve angles, such as shearing, can alter the algorithm's outcome. It is important to note that in applications where angle preservation is critical, this behavior is acceptable.

Interestingly, the proposed algorithm produces poor results for \textit{inside} out of distribution cases \cite{lazebnik2024introducing}, as indicated by the \textit{torus}-shaped data with several anomaly data points in the center of the torus, unlike other algorithms such as the Isolation Forest which takes into account only the topology, not the distance and the angles. as the latter portions the feature space locally rather than globally like the proposed convex hull algorithm.  

This study is not without limitations. First, the choice of stopping conditions remains a critical yet subjective aspect of the method. While dynamic strategies like the elbow method offer some flexibility, there is no universal approach to determining optimal stopping criteria across diverse datasets, leaving a promising avenue for future investigation. Second, we do not explore the impact of noise or overlapping anomalies in detail, which could further challenge the method's robustness. Finally, the experiments focus primarily on small to moderately sized real-world datasets, leaving the scalability and generalizability of the method to large and highly complex datasets largely untested. This limitation is rooted in the lack of quality-tagged anomalies in large-size datasets. Future research should remedy this limitation given such datasets became available. 

Taken jointly, the findings of this study highlight the potential of the convex hull-based approach as a robust and interpretable method for anomaly detection across diverse datasets. By leveraging geometric principles, the method effectively identifies outliers while offering insights into the underlying structure of the data. Despite its computational intensity and sensitivity to certain data distributions, the proposed framework demonstrates a promising balance between accuracy and interpretability, particularly when combined with dimensionality reduction techniques. 

\section*{Declarations}
\subsection*{Funding}
This study received no external funding.

\subsection*{Conflicts of Interest/Competing Interests}
The authors declare no conflict of interest.

\subsection*{Data and code availability}
The data is freely available in the following GitHub repository: \url{https://github.com/asaelbarilan/Anomaly_Detection_Using_Convex_Hull}.

\subsection*{Author contribution}
Uri Itai: Conceptualization, Formal analysis, Writing - Original Draft. \\
Asael Bar Ilan: Software, Formal analysis, Data Curation, Writing - Review \& Editing, Visualization. \\
Teddy Lazebnik: Methodology, Validation, Formal analysis, Investigation, Data Curation, Writing - Original Draft, Writing - Review \& Editing, Supervision, Project administration. 

\bibliography{biblio}
\bibliographystyle{unsrt}

\end{document}

