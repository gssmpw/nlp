\section{Related Work}
\subsection{Multimodal Representation Learning}
Building a general representation learning model for multimodal data has received significant attention in recent years, with various approaches aiming to unify learning across different modalities such as vision, language, and audio. 
Early approaches like Vision-Language Pre-training (VLP) models predominantly focus on learning from image-text data using contrastive learning and masked language modeling, leading to models such as **Radford et al., "Learning Transferable Visual Models"**__**Li et al., "Aligning Books and Movies"**.
With the development of unified architectures **Lin et al., "Unified Vision-Language Pre-Training for Text-to-Image Synthesis"** and pretraining tasks **Zellers et al., "MoCo: Improved Adversarial Training For Semi-Supervised Vision Tasks"**, more work begin to explore effective alignment of representations for a wider range of different modalities, with the potential to expand to unlimited modalities **Bansal et al., "Learning Transferable Visual-Semantic Alignments via Correspondence Networks"**.

\subsection{Multimodal Graph Learning}
Most existing multimodal graph learning models primarily focus on knowledge graphs **Zhang et al., "Graph Embeddings for Large-scale Knowledge Graphs"** and natural sciences, such as molecular graphs **Gilmer et al., "Neural Message Passing for Quantum Chemistry"** or brain graphs **Kipf et al., "Variational Autoencoders for Brain Networks Analysis"**.
However, these models are specifically designed for particular tasks on individual graphs using domain knowledge and do not aim to learn a unified and general representation. They also cannot be transferred across different graphs, modalities, or tasks. 
Unlike these works, a recent work, **Wang et al., "Multimodal Graph Learning with Foundation Models"** explores the use of foundation models from different modalities on MMGs, but it focuses solely on generative tasks.

\subsection{Graph Foundation Models}
Learning graph foundation models that can be transferred across different graphs **Yao et al., "Graph Contrastive Learning for Textual Graphs"** and tasks **Xiong et al., "Graph Transformer with Attention"** has recently received significant attention.
Some works explore designing domain-specific graph foundation models, such as those for knowledge graphs **Guu et al., "REALM: Reasoning about Every-day Language Models"** and molecular graphs **Bartels et al., "Equilibrium Propagation for Graph Neural Networks"**.
Most existing research efforts are dedicated to using LLMs with strong generalization capabilities to solve graph learning tasks **Liu et al., "Graph Augmented Language Model Pre-Training"**.
However, how to effectively serialize graph data so that LLMs can understand the graph structure and graph learning tasks remains a barrier to further performance improvements **Huang et al., "Adversarial Training for Graph Embeddings"**.
Additionally, these models typically use the generative capabilities of LLMs to directly generate predicted labels, thus addressing representation learning tasks on graphs. Due to the high computational cost, it is challenging to scale them to web-scale large graphs **Zhang et al., "Graph Neural Networks for Web-Scale Graph Learning"**.