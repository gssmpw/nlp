\section{Related Work}
\subsection{Multimodal Representation Learning}
Building a general representation learning model for multimodal data has received significant attention in recent years, with various approaches aiming to unify learning across different modalities such as vision, language, and audio. 
Early approaches like Vision-Language Pre-training (VLP) models predominantly focus on learning from image-text data using contrastive learning and masked language modeling, leading to models such as CLIP~\cite{radford2021learning} and ALIGN~\cite{jia2021scaling}.
With the development of unified architectures~\cite{vaswani2017attention,dosovitskiy2020image,jaegleperceiver} and pretraining tasks~\cite{he2022masked,baobeit,kenton2019bert,radford2018improving}, more work begin to explore effective alignment of representations for a wider range of different modalities, with the potential to expand to unlimited modalities~\cite{girdhar2023imagebind,wang2023one}.

\subsection{Multimodal Graph Learning}
Most existing multimodal graph learning models primarily focus on knowledge graphs~\cite{chen2022hybrid,zeng2023multi} and natural sciences, such as molecular graphs~\cite{jinlearning} or brain graphs~\cite{wang2023hypergraph}.
However, these models are specifically designed for particular tasks on individual graphs using domain knowledge and do not aim to learn a unified and general representation. They also cannot be transferred across different graphs, modalities, or tasks. 
Unlike these works, a recent work, MMGL~\cite{yoon2023multimodal} explores the use of foundation models from different modalities on MMGs, but it focuses solely on generative tasks.


\subsection{Graph Foundation Models}
Learning graph foundation models that can be transferred across different graphs~\cite{he2024unigraphlearningunifiedcrossdomain,he2024generalizing,qiu2020gcc,hou2024graphalign} and tasks~\cite{hou2023graphmae2,liu2023one,he2024generalizing} has recently received significant attention.
Some works explore designing domain-specific graph foundation models, such as those for knowledge graphs~\cite{galkin2023towards} and molecular graphs~\cite{xiamole}.
Most existing research efforts are dedicated to using LLMs with strong generalization capabilities to solve graph learning tasks~\cite{wang2024can,he2024g,sui2024fidelis,liu2023one}.
However, how to effectively serialize graph data so that LLMs can understand the graph structure and graph learning tasks remains a barrier to further performance improvements~\cite{zhang2024can}.
Additionally, these models typically use the generative capabilities of LLMs to directly generate predicted labels, thus addressing representation learning tasks on graphs. Due to the high computational cost, it is challenging to scale them to web-scale large graphs~\cite{wang2024can,he2024g}.

% In addition, there are some works exploring domain-specific graph foundation models, such as knowledge graphs and molecular graphs.