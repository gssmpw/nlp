[
  {
    "index": 0,
    "papers": [
      {
        "key": "radford2021learning",
        "author": "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others",
        "title": "Learning transferable visual models from natural language supervision"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "jia2021scaling",
        "author": "Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom",
        "title": "Scaling up visual and vision-language representation learning with noisy text supervision"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "vaswani2017attention",
        "author": "Vaswani, A",
        "title": "Attention is all you need"
      },
      {
        "key": "dosovitskiy2020image",
        "author": "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others",
        "title": "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"
      },
      {
        "key": "jaegleperceiver",
        "author": "Jaegle, Andrew and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Doersch, Carl and Ionescu, Catalin and Ding, David and Koppula, Skanda and Zoran, Daniel and Brock, Andrew and Shelhamer, Evan and others",
        "title": "Perceiver IO: A General Architecture for Structured Inputs \\& Outputs"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "he2022masked",
        "author": "He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\\'a}r, Piotr and Girshick, Ross",
        "title": "Masked autoencoders are scalable vision learners"
      },
      {
        "key": "baobeit",
        "author": "Bao, Hangbo and Dong, Li and Piao, Songhao and Wei, Furu",
        "title": "BEiT: BERT Pre-Training of Image Transformers"
      },
      {
        "key": "kenton2019bert",
        "author": "Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina",
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
      },
      {
        "key": "radford2018improving",
        "author": "Radford, Alec",
        "title": "Improving language understanding by generative pre-training"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "girdhar2023imagebind",
        "author": "Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan",
        "title": "Imagebind: One embedding space to bind them all"
      },
      {
        "key": "wang2023one",
        "author": "Wang, Peng and Wang, Shijie and Lin, Junyang and Bai, Shuai and Zhou, Xiaohuan and Zhou, Jingren and Wang, Xinggang and Zhou, Chang",
        "title": "One-peace: Exploring one general representation model toward unlimited modalities"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2022hybrid",
        "author": "Chen, Xiang and Zhang, Ningyu and Li, Lei and Deng, Shumin and Tan, Chuanqi and Xu, Changliang and Huang, Fei and Si, Luo and Chen, Huajun",
        "title": "Hybrid transformer with multi-level fusion for multimodal knowledge graph completion"
      },
      {
        "key": "zeng2023multi",
        "author": "Zeng, Yawen and Jin, Qin and Bao, Tengfei and Li, Wenfeng",
        "title": "Multi-modal knowledge hypergraph for diverse image retrieval"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "jinlearning",
        "author": "Jin, Wengong and Yang, Kevin and Barzilay, Regina and Jaakkola, Tommi",
        "title": "Learning Multimodal Graph-to-Graph Translation for Molecule Optimization"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2023hypergraph",
        "author": "Wang, Meiling and Shao, Wei and Huang, Shuo and Zhang, Daoqiang",
        "title": "Hypergraph-regularized multimodal learning by graph diffusion for imaging genetics based alzheimer\u2019s disease diagnosis"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "yoon2023multimodal",
        "author": "Yoon, Minji and Koh, Jing Yu and Hooi, Bryan and Salakhutdinov, Ruslan",
        "title": "Multimodal graph learning for generative tasks"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "he2024unigraphlearningunifiedcrossdomain",
        "author": "Yufei He and Yuan Sui and Xiaoxin He and Bryan Hooi",
        "title": "UniGraph: Learning a Unified Cross-Domain Foundation Model for Text-Attributed Graphs"
      },
      {
        "key": "he2024generalizing",
        "author": "He, Yufei and Hou, Zhenyu and Cen, Yukuo and He, Feng and Cheng, Xu and Hooi, Bryan",
        "title": "Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-Training on Industrial-Scale Data"
      },
      {
        "key": "qiu2020gcc",
        "author": "Qiu, Jiezhong and Chen, Qibin and Dong, Yuxiao and Zhang, Jing and Yang, Hongxia and Ding, Ming and Wang, Kuansan and Tang, Jie",
        "title": "Gcc: Graph contrastive coding for graph neural network pre-training"
      },
      {
        "key": "hou2024graphalign",
        "author": "Hou, Zhenyu and Li, Haozhan and Cen, Yukuo and Tang, Jie and Dong, Yuxiao",
        "title": "GraphAlign: Pretraining One Graph Neural Network on Multiple Graphs via Feature Alignment"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hou2023graphmae2",
        "author": "Hou, Zhenyu and He, Yufei and Cen, Yukuo and Liu, Xiao and Dong, Yuxiao and Kharlamov, Evgeny and Tang, Jie",
        "title": "Graphmae2: A decoding-enhanced masked self-supervised graph learner"
      },
      {
        "key": "liu2023one",
        "author": "Liu, Hao and Feng, Jiarui and Kong, Lecheng and Liang, Ningyue and Tao, Dacheng and Chen, Yixin and Zhang, Muhan",
        "title": "One for all: Towards training one graph model for all classification tasks"
      },
      {
        "key": "he2024generalizing",
        "author": "He, Yufei and Hou, Zhenyu and Cen, Yukuo and He, Feng and Cheng, Xu and Hooi, Bryan",
        "title": "Generalizing Graph Transformers Across Diverse Graphs and Tasks via Pre-Training on Industrial-Scale Data"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "galkin2023towards",
        "author": "Galkin, Mikhail and Yuan, Xinyu and Mostafa, Hesham and Tang, Jian and Zhu, Zhaocheng",
        "title": "Towards foundation models for knowledge graph reasoning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "xiamole",
        "author": "Xia, Jun and Zhao, Chengshuai and Hu, Bozhen and Gao, Zhangyang and Tan, Cheng and Liu, Yue and Li, Siyuan and Li, Stan Z",
        "title": "Mole-BERT: Rethinking Pre-training Graph Neural Networks for Molecules"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "wang2024can",
        "author": "Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia",
        "title": "Can language models solve graph problems in natural language?"
      },
      {
        "key": "he2024g",
        "author": "He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan",
        "title": "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"
      },
      {
        "key": "sui2024fidelis",
        "author": "Sui, Yuan and He, Yufei and Liu, Nian and He, Xiaoxin and Wang, Kun and Hooi, Bryan",
        "title": "FiDeLiS: Faithful Reasoning in Large Language Model for Knowledge Graph Question Answering"
      },
      {
        "key": "liu2023one",
        "author": "Liu, Hao and Feng, Jiarui and Kong, Lecheng and Liang, Ningyue and Tao, Dacheng and Chen, Yixin and Zhang, Muhan",
        "title": "One for all: Towards training one graph model for all classification tasks"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2024can",
        "author": "Zhang, Yizhuo and Wang, Heng and Feng, Shangbin and Tan, Zhaoxuan and Han, Xiaochuang and He, Tianxing and Tsvetkov, Yulia",
        "title": "Can LLM Graph Reasoning Generalize beyond Pattern Memorization?"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "wang2024can",
        "author": "Wang, Heng and Feng, Shangbin and He, Tianxing and Tan, Zhaoxuan and Han, Xiaochuang and Tsvetkov, Yulia",
        "title": "Can language models solve graph problems in natural language?"
      },
      {
        "key": "he2024g",
        "author": "He, Xiaoxin and Tian, Yijun and Sun, Yifei and Chawla, Nitesh V and Laurent, Thomas and LeCun, Yann and Bresson, Xavier and Hooi, Bryan",
        "title": "G-retriever: Retrieval-augmented generation for textual graph understanding and question answering"
      }
    ]
  }
]