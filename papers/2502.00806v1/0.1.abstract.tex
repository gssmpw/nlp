\begin{abstract}

% Graph foundation models aim xxx,

% However, they primarily focus on TAGs while can not handle MMG,

% To this end, we propose UniGraph2, by xxx

% Concretely, 

% Experiments,

Existing foundation models, such as CLIP, aim to learn a unified embedding space for multimodal data, enabling a wide range of downstream web-based applications like search, recommendation, and content classification. However, these models often overlook the inherent graph structures in multimodal datasets, where entities and their relationships are crucial. %For example, in social networks, users are connected through friendships, follows, or interactions, and share content in various modalities like text and images. 
Multimodal graphs (MMGs) represent such graphs where each node is associated with features from different modalities, while the edges capture the relationships between these entities.
% such data by combining diverse modalities with graph structures that capture the relationships between entities.
% in e-commerce platforms, products are linked based on co-purchase patterns, user reviews, and shared attributes, encompassing multiple data types.
% \todo{introduce MMG, and real applications}However, none of these models consider the graph structure inherent in many multimodal datasets, where entities and their relationships are critical. 
On the other hand, existing graph foundation models primarily focus on text-attributed graphs (TAGs) and are not designed to handle the complexities of MMGs. To address these limitations, we propose \model\footnote[1]{The code is available at \url{https://github.com/yf-he/UniGraph2}}, a novel cross-domain graph foundation model that enables general representation learning on MMGs, providing a unified embedding space. \model employs modality-specific encoders alongside a graph neural network (GNN) to learn a unified low-dimensional embedding space that captures both the multimodal information and the underlying graph structure. We propose a new cross-domain multi-graph pre-training algorithm at scale to ensure effective transfer learning across diverse graph domains and modalities. Additionally, we adopt a Mixture of Experts (MoE) component to align features from different domains and modalities, ensuring coherent and robust embeddings that unify the information across modalities. Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as representation learning, transfer learning, and multimodal generative tasks, offering a scalable and flexible solution for learning on MMGs.



% Graph foundation models (GFMs), which aim to learn knowledge from graphs in different domains and transfer it to various tasks, have become a promising direction for web mining. However, existing GFMs, e.g., UniGraph, primarily focus on text-attributed graphs (TAGs) and fail to handle the data in various modalities, e.g., text, image, and video on the Web, limiting the applicability. To this end, we propose a new GFM method, termed \model, to achieve the general ability on multi-modal graphs (MMGs), by refactoring UniGraph from three key aspects, including architecture, pre-training, and alignment. Concretely, for the architecture, we first design the GNN-based modulation-specific encoders to learn a unified representation that captures both modality information and structure information. Then, for the pre-training, we propose a large-scale cross-domain pre-training algorithm to learn knowledge across diverse graph domains and modalities. In addition, for the alignment, we introduce a Mixture-of-Expert (MoE) component to align features from different domains, ensuring coherent and robust embeddings that unify the information across modalities. By these designs, \model is powered by the strong multimodal representation learning ability on graph data, improving the applicability and performance in web mining. Extensive experiments on xx multimodal tasks demonstrate the superiority of our proposed method. Remarkablely, xxx

% Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as node classification, retrieval, and transfer learning, offering a scalable and flexible solution for learning on multimodal graphs.


\end{abstract}

