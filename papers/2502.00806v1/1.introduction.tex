\section{Introduction}


Real-world web applications increasingly rely on multimodal data, where information is derived from a variety of sources such as text, images, and audio~\cite{ngiam2011multimodal,baltruvsaitis2018multimodal}. 
% Effectively integrating these diverse data modalities has become essential for a wide range of tasks, including recommendation systems, information retrieval, and semantic understanding. 
Recent foundation models have focused on learning a unified embedding space across different modalities that allows for the seamless integration of multimodal data, thereby enabling effective cross-modal interactions and supporting downstream applications~\cite{radford2021learning,girdhar2023imagebind}. 
% Despite the growing importance of such models, most existing approaches overlook the graph structure of multimodal data, which inherently represents entities and their relationships in many domains, from social networks to e-commerce networks~\cite{yoon2023multimodal,zhu2024multimodal,ektefaie2023multimodal}.

Models such as CLIP~\cite{radford2021learning} have demonstrated the power of learning from multimodal data by mapping text and images into a shared embedding space. 
However, CLIP and similar models are fundamentally limited by their reliance on a 1-to-1 mapping between paired modalities, such as text-to-image alignment, ignoring more complex structures where nodes can be connected through many-to-many relationships and involve multiple modalities. 
These models fail to account for the graph structure present in numerous web domains, from social networks to e-commerce networks~\cite{yoon2023multimodal,zhu2024multimodal,ektefaie2023multimodal}, where entities and their interactions are crucial to understanding the underlying relationships.
For example, in e-commerce platforms, recommendation systems rely on complex networks of products, users, and their interactions~\cite{schafer2001commerce}. 
Each node represents a a user or a product, and edges represent interactions like purchases, views, or reviews. 
Additionally, both users and products are associated with rich multimodal data: product descriptions (text), images (visual), and user reviews (text), and demonstration videos (audio and visual). 
Integrating these diverse data types within the graph structure is essential for accurate recommendations and personalized user experiences~\cite{gao2022graph}.
% \todo{more specific examples for real-world usage, like recommendation, and define multimodal graphs.}
To address these challenges, Multimodal Graphs (MMGs) have been introduced as a framework that combines graph structures with multimodal data~\cite{zhu2024multimodal,ektefaie2023multimodal}. 
On MMGs, nodes are enriched with information from multiple modalities, allowing for a more comprehensive representation of entities and their relationships. 
However, existing MMGs learning methods can only train models individually for a specific graph and task~\cite{yoon2023multimodal,chen2022hybrid,zeng2023multi}, and cannot achieve cross-graph and cross-task transfer like foundation models do without retraining or fine-tuning.
% Therefore, there is a growing need for a unified embedding space that can account for both the multimodal nature of data and the graph structures that capture entity relationships.

% Graph-structured data is critical in capturing relational information, where nodes represent entities and edges capture the interactions or dependencies between them. 

% While there has been considerable progress in learning on text-attributed graphs (TAGs)~\cite{he2024unigraphlearningunifiedcrossdomain,heharnessing,chiennode}, where each node is enriched with textual feature, the extension to more complex multimodal graphs (MMGs), where nodes may consist of diverse modalities, remains underexplored. 
% Current graph foundation models are largely limited to TAGs~\cite{he2024unigraphlearningunifiedcrossdomain} and are unable to generalize to MMGs, which often feature incomplete or partial modality data, posing additional challenges in real-world settings. 
% This gap highlights the need for a unified model capable of learning from MMGs while preserving both the structural and multimodal information.

% To address this gap, UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain}, a recent graph foundation model, successfully introduced a unified embedding space for text-attributed graphs (TAGs). This was an important step forward, as it demonstrated the power of learning representations from graph structures combined with textual features.

Recently, there has been considerable progress in learning foundation models for text-attributed graphs (TAGs)~\cite{he2024unigraphlearningunifiedcrossdomain,he2024g,heharnessing,chiennode}, which can be viewed as a special case of MMGs where the node features are are exclusively in the text modality.
% , the extension to more complex multimodal graphs (MMGs), where nodes consist of diverse modalities, remains underexplored. 
One prominent effort in this direction is UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain}, which introduces a unified embedding space that combines graph structure and node-level textual information for all TAGs. UniGraph employs a masked prediction framework~\cite{he2022masked,kenton2019bert,radford2018improving}, inspired by the success of masked language models (MLMs)~\cite{kenton2019bert}. In this framework, UniGraph performs self-supervised pre-training by masking node-level text attributes and learning to predict the missing information based on the graph context. 
% This approach allows the model to capture both the structural relationships in the graph and the semantic information encoded in the textual attributes.
% Specifically, UniGraph trains a graph neural network (GNN) alongside a language model (LM) to process both the graph topology and the text attributed to each node. 
% The GNN learns the structural dependencies between nodes, while the LM captures the semantic meaning of the text attributes. 
% By jointly optimizing these two components, UniGraph learns a unified low-dimensional representation for any TAGs, which can then be used for a variety of downstream tasks, such as node classification or link prediction. 
% Despite its effectiveness on TAGs, UniGraph is limited in its ability to generalize to MMGs, where nodes may contain features from diverse modalities such as images, audio, or video, in addition to text. \todo{mention cross-domain pre-training as second challenge}
Despite its effectiveness on TAGs, UniGraph faces two significant limitations when extended to more complex settings. First, it is limited in its ability to generalize to MMGs, where nodes may contain features from diverse modalities such as images, in addition to text. 
Second, UniGraph focuses on pre-training on a single graph from one domain, which restricts its capacity to leverage knowledge across multiple domains. 
% In real-world applications, data often comes from various sources with different graph structures and domain-specific features, necessitating a cross-domain multi-graph pre-training approach.
In training a foundation model, it is essential to employ more diverse pre-training data from different domains to enhance the model's generalization~\cite{radford2021learning,achiam2023gpt,girdhar2023imagebind}.

\vpara{Presented Work.} 
In this work, we propose \model, a graph foundation model for MMGs that provides a unified embedding space across graph domains and modalities, as shown in Figure ~\ref{fig:fig1}.
In \model, nodes are not restricted to textual attributes; instead, they can incorporate features from any combination of modalities. 
Similar to UniGraph, \model adopts a masked prediction framework, but generalizes the masked prediction task to accommodate multimodal data. In this setup, the model is tasked with predicting missing node attributes, which could be text, image features, or any other modality, based on the graph structure and the available multimodal information. 
This allows the model to learn rich, unified representations that capture both the multimodal features of each node and the relationships encoded in the graph.

Furthermore, while UniGraph focuses on pre-training within a single graph domain, \model introduces a more robust multi-graph pre-training strategy. In real-world applications, data often comes from multiple sources, each with different graph structures and node modalities. To handle this, \model proposes a cross-domain multi-graph pre-training framework, which enables the model to learn compact and transferable knowledge across a diverse set of graph datasets with varying modality and domain distributions. 
% By learning from multiple graphs, \model generalizes better to unseen data and supports more complex multimodal graph structures in downstream tasks.
A key component of this framework is the Mixture of Experts (MoE)~\cite{shazeer2017outrageously,hou2024graphalign}, which is specifically designed to align node features from different domains and modalities. The MoE dynamically selects the most appropriate experts for each input data, ensuring that the diverse multimodal features are coherently integrated into the unified embedding space.

In summary, our key contributions in \model are:
\begin{itemize}
    \item We generalize the masked prediction framework used in UniGraph to support multimodal graphs, allowing nodes to include a variety of modalities such as text and images.
    \item We introduce a cross-domain multi-graph pre-training strategy, enabling \model to learn unified and transferable representations across different graph domains and modalities.
    % \item We incorporate an MoE component to align multimodal features from different graph domains, ensuring that the model dynamically selects and integrates the most compact and relevant information.
    \item We demonstrate through extensive experimentation that \model outperforms state-of-the-art models in various multimodal graph learning tasks, including representation learning, transfer learning, and multimodal generative tasks, particularly when data is drawn from multiple graph domains.
\end{itemize}