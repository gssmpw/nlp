\section{Related Work}
\subsection{Multimodal Representation Learning}
Building a general representation learning model for multimodal data has received significant attention in recent years, with various approaches aiming to unify learning across different modalities such as vision, language, and audio. 
Early approaches like Vision-Language Pre-training (VLP) models predominantly focus on learning from image-text data using contrastive learning and masked language modeling, leading to models such as CLIP____ and ALIGN____.
With the development of unified architectures____ and pretraining tasks____, more work begin to explore effective alignment of representations for a wider range of different modalities, with the potential to expand to unlimited modalities____.

\subsection{Multimodal Graph Learning}
Most existing multimodal graph learning models primarily focus on knowledge graphs____ and natural sciences, such as molecular graphs____ or brain graphs____.
However, these models are specifically designed for particular tasks on individual graphs using domain knowledge and do not aim to learn a unified and general representation. They also cannot be transferred across different graphs, modalities, or tasks. 
Unlike these works, a recent work, MMGL____ explores the use of foundation models from different modalities on MMGs, but it focuses solely on generative tasks.


\subsection{Graph Foundation Models}
Learning graph foundation models that can be transferred across different graphs____ and tasks____ has recently received significant attention.
Some works explore designing domain-specific graph foundation models, such as those for knowledge graphs____ and molecular graphs____.
Most existing research efforts are dedicated to using LLMs with strong generalization capabilities to solve graph learning tasks____.
However, how to effectively serialize graph data so that LLMs can understand the graph structure and graph learning tasks remains a barrier to further performance improvements____.
Additionally, these models typically use the generative capabilities of LLMs to directly generate predicted labels, thus addressing representation learning tasks on graphs. Due to the high computational cost, it is challenging to scale them to web-scale large graphs____.

% In addition, there are some works exploring domain-specific graph foundation models, such as knowledge graphs and molecular graphs.