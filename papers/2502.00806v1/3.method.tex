\section{Preliminaries}
\subsection{Multimodal Graphs (MMGs)}
\begin{definition}[Multimodal Graphs]
\label{def:multimodal_graphs}
A Multimodal Graph (MMG) is defined as a graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{M}, \Omega) \), where \( \mathcal{V} \) represents the set of nodes and \( \mathcal{E} \) represents the set of edges. The function \( \mathcal{M} : \mathcal{V} \rightarrow 2^{\Omega} \) maps each node \( v \in \mathcal{V} \) to a subset of modalities \( \Omega_v \subseteq \Omega \), where \( \Omega \) denotes the set of all possible modalities, such as text, images, or other data types. Each node \( v \) in \( \mathcal{V} \) can possess multiple features from different modalities, but not all nodes are required to have features from every modality. 
% This structure allows for the integration of diverse data types in a single graph representation, facilitating complex interactions and analyses across multiple modalities.
\end{definition}
For a Text-Attributed Graph \( \mathcal{G}_{\text{TAG}} = (\mathcal{V}, \mathcal{E}, \mathcal{M}, \{\text{text}\}) \), where each node has an associated text \( t_v \in \mathcal{T}_\mathcal{V} \), we define the mapping function for MMGs as follows:
% \begin{equation}
%     \mathcal{M}(v) = \{ t_v \},\ \text{where} \ t_v \in \mathcal{T}_\mathcal{V}.
% \end{equation}
% Here, \( \Omega = \{\text{text}\} \) is the set of possible modalities, limited to textual data in this context.
\begin{equation} 
\mathcal{M}(v) = \{\text{text}\}, \ \text{for all}\ v \in \mathcal{V}.
\end{equation}
Here, \( \Omega = \{\text{text}\} \) is the set of possible modalities, limited to textual data in this context.




% For a Multimodal Graph \( \mathcal{G}_{\text{MMG}} = (\mathcal{V}, \mathcal{E}, \mathcal{M}) \), each node \( v \in \mathcal{V} \) maps to a set of features from potentially multiple modalities:
% \[
% \mathcal{M}(v) = \Omega_v \subseteq \Omega
% \]

% In the specific case of a TAG being represented as an MMG, the function \( \mathcal{M} \) simplifies such that each node \( v \) has exactly one modality, text, thus \( \Omega_v = \{\text{text}\} \) for all \( v \in \mathcal{V} \). 

% \subsection{General Representation Learning on TAGs}
% % \begin{problem}[General Representation Learning on TAGs]
% % Consider a Text-Attributed Graph (TAG) defined as \( \mathcal{G}_{\text{TAG}} = (\mathcal{V}, \mathcal{E}, \mathcal{T}_\mathcal{V}) \), where each node \( v_i \in \mathcal{V} \) is associated with a textual feature \( t_{v_i} \). The challenge in self-supervised learning (SSL) on TAGs involves deriving a low-dimensional representation for each node \( v_i \) in a unified embedding space. The objective is to learn a function \( f: \mathcal{V} \rightarrow \mathbb{R}^d \), where \( d \ll |\mathcal{V}| \).
% % \end{problem}
% \begin{problem}[General Representation Learning on TAGs]
% Consider a collection of Text-Attributed Graphs (TAGs) in the pre-training set \( \mathcal{D}_{\text{pretrain}} \), where each graph \( \mathcal{G}_{\text{TAG},k} = (\mathcal{V}_k, \mathcal{E}_k, \mathcal{T}_{\mathcal{V}_k}) \) contains nodes \( v_{ik} \in \mathcal{V}_k \) each associated with a textual feature \( t_{v_{ik}} \). The challenge in general representation learning on TAGs involves self-supervised pre-training a function \( f: \mathcal{V}_k \rightarrow \mathbb{R}^d \) across this diverse dataset. The objective is to develop a model that generalizes well to any new, unseen graph, enabling effective inference across various TAGs.
% For inference, the pre-trained model \( f \) is applied to a new, unseen graph \( \mathcal{G}^{\text{inf}}_{\text{TAG}} = (\mathcal{V}^{\text{inf}}, \mathcal{E}^{\text{inf}}, \mathcal{T}_{\mathcal{V}}^{\text{inf}}) \) to generate embeddings for its nodes, thereby facilitating downstream tasks on \( \mathcal{G}^{\text{inf}}_{\text{TAG}} \) without further training.
% \end{problem}

\subsection{General Representation Learning on MMGs}
General representation learning~\cite{muennighoff2022mteb,radford2021learning,girdhar2023imagebind,wang2023one} on MMGs aims to learn a self-supervised pre-trained model 
%that generalizes well to unseen graphs, enabling effective transfer learning. The goal is to design a model 
that can infer meaningful representations for any new MMG, facilitating downstream tasks without the need for additional training or fine-tuning on new data.


\begin{problem}[General Representation Learning on MMGs]
Consider a collection of Multimodal Graphs (MMGs) in the pre-training set \( \mathcal{D}_{\text{pretrain}} \), where each graph \( \mathcal{G}_{k} = (\mathcal{V}_k, \mathcal{E}_k, \mathcal{M}_k) \) contains nodes \( v_{ik} \in \mathcal{V}_k \) each associated with a set of modalities \( \Omega_{v_{ik}} \subseteq \Omega \), encompassing various data types such as text, images, and other feature modalities. The challenge in general representation learning on MMGs involves self-supervised pre-training a function \( f: \mathcal{V}_k \rightarrow \mathbb{R}^d \) across this diverse dataset. The objective is to develop a model that generalizes well to any new, unseen graph, enabling effective inference across various MMGs. For inference, the pre-trained model \( f \) is applied to a new, unseen graph \( \mathcal{G}^{\text{inf}} = (\mathcal{V}^{\text{inf}}, \mathcal{E}^{\text{inf}}, \mathcal{M}^{\text{inf}}) \) to generate embeddings for its nodes, thereby facilitating downstream tasks on \( \mathcal{G}^{\text{inf}} \) without further training.
\end{problem}



\vpara{UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain}.} 
TAGs are a subset of MMGs where each node is associated with textual features. 
As a general representation learning model on TAGs, 
% UniGraph extends prior methodologies that utilized distinct text encoders and GNNs for SSL on TAGs, 
UniGraph unifies the learning process by integrating LM and GNN into a single encoder.
% \todo{be more specific about pre-training, masking kind of thing.}
% In pre-training, the joint optimization in UniGraph for solving SSL on TAGs can be formulated as follows:

% \begin{equation}
% \theta_1^*, \theta_2^*, \theta_3^* = \arg\min_{\theta_1, \theta_2, \theta_3} \mathcal{L}_{SSL} \left( f_{\theta_1}^{GNN}, f_{\theta_2}^{LM}, f_{\theta_3}^{Decoder}, \mathcal{G}_{\text{TAG}} \right),    
% \end{equation}

% where \( f_{\theta_1}^{GNN} \) and \( f_{\theta_2}^{LM} \) are components of a unified encoder, and \( f_{\theta_3}^{Decoder} \) acts as the decoder for pre-training task. 
In UniGraph's pre-training, the masked prediction process can be mathematically formulated in two key steps:
\begin{enumerate}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{Masked Encoding:} For each node \( v \in \mathcal{V} \) has its textual feature \( t_v \) partially masked and encoded by an LM \( f_{\theta_1}^{\text{LM}} \), producing hidden representations \( \mE_v = f_{\theta_1}^{\text{LM}}(\text{Mask}(t_v)) \). The GNN \( f_{\theta_2}^{\text{GNN}} \) propagates node embeddings across the graph, where the final node embedding is:
    \begin{equation}
        \mE_{\text{CLS}}' = f_{\theta_2}^{\text{GNN}}(\mathcal{G}_{\text{TAG}}, \mE_{\text{CLS}}),
    \end{equation}
    with \( \mE_{\text{CLS}} \) representing the embeddings of all nodes' \([\text{CLS}]\) tokens from \( f_{\theta_1}^{LM} \).
    
    \item \textbf{Decoding:} The MLP decoder \( f_{\theta_3}^{\text{Decoder}} \) combines the masked textual embeddings \( \mE_v \) and the graph embeddings \( \mE_{\text{CLS}}' \) to reconstruct the masked tokens. The predicted probability distribution \( P_v \) over the vocabulary is obtained via:
    \begin{equation}
        P_v = f_{\theta_3}^{\text{Decoder}}(\text{concat}(\mE_v, \mE_{\text{CLS}}')),
    \end{equation}
and the model minimizes the masked language modeling loss \( \mathcal{L}_\text{{MLM}} \), formulated as:
\begin{equation}
    \mathcal{L}_\text{{MLM}} = -\frac{1}{|\mathcal{V}|} \sum_{v \in \mathcal{V}} \sum_{i} I(v, i) \log P_v[i, T_i],
\end{equation}
% where \( I(v, i) \) is an indicator function that selects masked tokens for each node \( v \), and \( T_i \) is the true token at position \( i \). The overall optimization problem is:
where \( I(v, i) \) indicates masked positions and \( T_i \) are the true tokens. The optimal parameters are obtained by:
\begin{equation}
    \theta_1^*, \theta_2^*, \theta_3^* = \arg\min_{\theta_1, \theta_2, \theta_3} \mathcal{L}_\text{{MLM}}.
\end{equation}
\end{enumerate}



In inference, the pre-trained model is used to generate embeddings for any unseen TAG \( \mathcal{G}^{\text{inf}}_{\text{TAG}} = (\mathcal{V}^{\text{inf}}, \mathcal{E}^{\text{inf}}, \mathcal{T}^{\text{inf}}_\mathcal{V})\) by processing the graph structure and node texts through the same encoder:
\begin{equation}
\mH^{\text{inf}} = f_{\theta_2^*}^{\text{GNN}} \left( \mathcal{G}^{\text{inf}}_{\text{TAG}}, \mX^{\text{inf}} \right), \text{ where } \mX^{\text{inf}} = f_{\theta_1^*}^{\text{LM}}(\mathcal{T}^{\text{inf}}_\mathcal{V}).    
\end{equation}
This process allows the model to generalize to new data, capturing both structural and textual graph attributes.

% \subsection{General Representation Learning on MMGs}
% % \begin{problem}[Self-supervised Learning on MMGs]
% % Consider a Multimodal Graph (MMG) defined as \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{M}) \), where each node \( v_i \in \mathcal{V} \) is associated with a set of modalities \( \Omega_{v_i} \subseteq \Omega \), encompassing various data types such as text, images, and other feature modalities. The challenge in self-supervised learning (SSL) on MMGs lies in effectively integrating these diverse modalities to derive a coherent, low-dimensional representation for each node \( v_i \) in a unified embedding space. The objective is to learn a function \( f: \mathcal{V} \rightarrow \mathbb{R}^d \), where \( d \ll |\mathcal{V}| \), that encapsulates the complex multimodal relationships inherent within the graph structure.
% % \end{problem}

% \begin{problem}[General Representation Learning on MMGs]
% Consider a collection of Multimodal Graphs (MMGs) in the pre-training set \( \mathcal{D}_{\text{pretrain}} \), where each graph \( \mathcal{G}_{k} = (\mathcal{V}_k, \mathcal{E}_k, \mathcal{M}_{\mathcal{V}_k}) \) contains nodes \( v_{ik} \in \mathcal{V}_k \) each associated with a set of modalities \( \Omega_{v_{ik}} \subseteq \Omega \), encompassing various data types such as text, images, and other feature modalities. The challenge in general representation learning on MMGs involves self-supervised pre-training a function \( f: \mathcal{V}_k \rightarrow \mathbb{R}^d \) across this diverse dataset. The objective is to develop a model that generalizes well to any new, unseen graph, enabling effective inference across various MMGs. For inference, the pre-trained model \( f \) is applied to a new, unseen graph \( \mathcal{G}^{\text{inf}} = (\mathcal{V}^{\text{inf}}, \mathcal{E}^{\text{inf}}, \mathcal{M}_{\mathcal{V}}^{\text{inf}}) \) to generate embeddings for its nodes, thereby facilitating downstream tasks on \( \mathcal{G}^{\text{inf}} \) without further training.
% \end{problem}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1\linewidth]{figs/arch.pdf}
    % \vskip -0.1in
    \caption{\textbf{Overview of the \model framework.} In pre-training, 
    1) UniGraph2 uses frozen Modality-Specific Encoders to encode raw multimodal data (e.g., text, images) into vector node features. Then, a portion of these node features is randomly masked. 
    2) Considering the diversity of node features across different modalities and graph domains, a Mixture of Experts (MoE) network is used to align the different node features, allowing the model to assign each node to one or more experts based on its domain and modality. 
    3) The aligned node features are fed into a GNN for learning and projected into a unified embedding space.
    4) The decoding involves two objectives: a. Each graph domain corresponds to a specific decoder for reconstructing the node features. b. A shared shortest path distance decoder is used to reconstruct the graph structures.
    }
    \vspace{-3mm}
    \label{fig:arch}
\end{figure*}

\section{The \model Framework}
The overall framework of \model is illustrated in Figure~\ref{fig:arch}.
The \model framework introduces a unified approach to learning representations of multimodal graphs (MMGs), which consist of nodes with diverse modal features (such as text and images) and edges representing relationships between these entities. 
% \model generalizes the task of representation learning by leveraging a combination of modality-specific encoders and a graph neural network (GNN) to integrate and align multimodal information into a shared embedding space.
The framework comprises three key modules: the multimodal feature encoders, which process multimodal features through modality-specific encoders; the Mixture of Experts (MoE) module, which selects specialized MLP to align features across domains and modalities; and the decoders, which map the unified embeddings back into domain-specific inputs. 
The GNN operates as the central component that propagates node embeddings based on both their multimodal features and the underlying graph structure.

% To further enhance the model’s generalization capabilities, \model employs a cross-domain multi-graph pre-training strategy, allowing it to handle diverse multimodal graphs from different domains. This results in robust, transferable embeddings that support various downstream tasks, such as node classification, link prediction, and multimodal retrieval.

\subsection{Multimodal Masking Strategies}
In UniGraph2, masking strategies play a crucial role in the self-supervised learning framework for MMGs. The objective is to mask a portion of the node features and require the model to reconstruct them, thereby encouraging the model to effectively capture both the structural and multimodal information.

\vpara{Modality-Specific Encoding.}
Before applying the masking process, modality-specific encoders are used to map raw data from different modalities into feature vectors. In the context of a multimodal graph \( \mathcal{G} = (\mathcal{V}, \mathcal{E}, \mathcal{M}), \Omega\), where each node \( v \in \mathcal{V} \) can have features from a subset of modalities \( \Omega_v \subseteq \Omega \), the raw features are transformed using encoders specific to each modality (e.g., a language model for text, and a Vision Transformer for images).

Let \( E_{\omega} \) represent the encoder for a modality \( \omega \in \Omega \), and let \( \vx_i^{(\omega)} \in \mathbb{R}^{d_{\text{in}}} \) denote the feature vector for node \( v_i \) obtained from modality \( \omega \). The modality-specific encoding can be expressed as:
\begin{equation}
    \vx_i^{(\omega)} = E_{\omega}(v_i^{(\omega)}).
\end{equation}

The features \( \vx_i \in \mathbb{R}^{d_{\text{in}}} \) for node \( v_i \) are then obtained by averaging the features from all modalities \( \Omega_v \) associated with the node:
\begin{equation}
    \vx_i = \frac{1}{|\Omega_v|} \sum_{\omega \in \Omega_v} \vx_i^{(\omega)}.
\end{equation}


\vpara{Masking Node Features.} Once the features of each node are encoded, a masking strategy is applied. A subset of nodes \( \tilde{\mathcal{V}} \subseteq \mathcal{V} \) is selected uniformly without replacement, and their features are replaced with a mask token \( \vx_{[M]} \), a learnable vector \( \vx_{[M]} \in \mathbb{R}^{d_{\text{in}}} \). This process is applied to approximately 75\% of the nodes to encourage robust learning by focusing on the graph context and unmasked nodes.
% \vpara{Masking Node Features.}
% Once the features of each node are encoded, a masking strategy is applied. A subset of nodes \( \tilde{\mathcal{V}} \subseteq \mathcal{V} \) uniformly without replacement and replace their features with a mask token \( [\vx_{[M]}] \), a learnable vector \( \vx_{[M]} \in \mathbb{R}^{d_{\text{in}}} \). This process is applied to approximately 75\% of the nodes to encourage robust learning by focusing on the graph context and unmasked nodes.
For each node \( v_i \in \mathcal{V} \), the masked feature \( \tilde{\vx} \) is defined as:
\begin{equation}
    \tilde{\vx}_i = 
\begin{cases}
\vx_{[M]} & \text{if } v_i \in \tilde{\mathcal{V}}, \\
\vx_i & \text{if } v_i \notin \tilde{\mathcal{V}}.
\end{cases}
\end{equation}

This masked feature \( \tilde{\vx} \) serves as the input to the MoE, which aligns the features from different graph domains and modalities. 
% Masking encourages the model to infer the missing node attributes by leveraging neighborhood information within the graph, thus improving the generalization of the learned representations.

\subsection{Mixture of Experts (MoE) Alignment}

Inspired by and adopted from GraphAlign~\cite{hou2024graphalign}, the MoE module~\cite{shazeer2017outrageously} in UniGraph2 is designed to achieve cross-domain and cross-modality alignment by dynamically selecting specialized experts for different types of data. 
In MMGs, nodes may come from various domains (e.g., social networks, product networks) and have features from different modalities (e.g., text, images). 
A single expert network might struggle to learn appropriate representations for such diverse data. However, with the MoE architecture, the model can assign each node to one or more experts based on its domain and modality. 
This enables the model to adaptively align and fuse heterogeneous node features by leveraging specialized experts for specific data types. 
The result is a flexible and powerful model that can learn and generalize across diverse graph structures and modalities, even when there are significant differences in feature types and distributions across domains.

% Nodes \( v_i \in \mathcal{V} \) from different graphs and domains may have input features \( \tilde{\vx}_i \) of varying dimensions. To ensure compatibility across domains, we first transform the input features of each node into a common feature space using a domain-specific Multi-Layer Perceptron (MLP). For a node \( v_i \) from domain \( d \), the transformed feature vector \( \tilde{\vx}_i \) is computed as follows:
% \begin{equation}
%     \tilde{\vx}_i = \text{MLP}_d(\tilde{\vx}_i)
% \end{equation}
% where \( \text{MLP}_d \) is the MLP associated with domain \( d \). This transformation ensures that nodes from different domains, with potentially different feature dimensions, are mapped to a common space for further processing by the MoE module.

Each node \( v_i \) is assigned to one or more experts through a gating mechanism. Each expert \( E_k \) is an MLP that processes the feature vector \( \tilde{\vx}_i \). The final node embedding \( \ve_i \) is computed as a weighted combination of the outputs from the selected experts:
\begin{equation}
    \ve_i = \sum_{k=1}^{K} \alpha_{i,k} E_k(\tilde{\vx}_i).
\end{equation}
Here, \( E_k(\tilde{\vx}_i) \) denotes the output of expert \( k \) for the node's feature vector \( \tilde{\vx}_i \), and \( \alpha_{i,k} \) represents the weight assigned to the \( k \)-th expert for node \( v_i \). The weights \( \alpha_{i,k} \) are computed using a softmax gating function, which assigns higher weights to the experts that are more relevant for the node based on its transformed features:
\begin{equation}
    \alpha_{i,k} = \frac{\exp(g_k(\tilde{\vx}_i))}{\sum_{k=1}^{K}, \exp(g_k(\tilde{\vx}_i))}, 
\end{equation}
where \( g_k(\cdot) \) is the gating function that scores the relevance of expert \( E_k \) for node \( v_i \). The gating function \( g_k \) is also an MLP that computes a scalar relevance score for each expert based on the input \( \tilde{\vx}_i \):
\begin{equation}
    g_k(\tilde{\vx}_i) = \text{MLP}_g(\tilde{\vx}_i)_k.
\end{equation}
% Here, \( \text{MLP}_g(\tilde{\vx}_i) \) represents the output of the single gating MLP for node \( v_i \)'s transformed features \( \tilde{\vx}_i \), and 
Here, the subscript \( k \) denotes the \( k \)-th component of the gating MLP output, corresponding to the relevance score for expert \( E_k \).

Thus, the MoE module adaptively routes each node’s features to the most relevant experts, allowing for effective cross-domain and multimodal alignment. The experts, being specialized MLPs, capture domain-specific or modality-specific knowledge, enabling UniGraph2 to generalize well across diverse data distributions.


\vpara{GNN Encoding.}
Once the aligned node embeddings \( \ve_i \) are obtained through the MoE module, they are passed through a GNN, denoted as \( f_{\text{GNN}} \), to further refine the node representations by incorporating the structural information of the graph \( \mathcal{G} \). The GNN takes \( \ve_i \) as input and propagates messages between neighboring nodes to learn the final node embeddings \( \vh_i \):
\begin{equation}
    \vh_i = f_{\text{GNN}}(\ve_i, \mathcal{G}).
\end{equation}
Here, \( f_{\text{GNN}}(\cdot) \) represents the GNN, which updates the embedding of each node by aggregating information from its neighbors. 
% This step allows the model to integrate both the multimodal feature information from the MoE and the topological structure of the graph, resulting in a final embedding \( \vh_i \) that captures both the multimodal and structural characteristics of the data. 

\vpara{Scaling to Web-Scale Graphs.}
To ensure the scalability of \model on web-scale graphs, we use the Personalized PageRank (PPR) algorithm for subgraph sampling. By using PPR as the sampling strategy, we can generate the most structurally significant local subgraphs~\cite{bianchini2005inside,gasteiger2018predict}. Unlike other sampling methods, such as neighbor sampling or k-hop neighbors, PPR can identify key nodes and structures that hold importance in a wider context, making them more broadly applicable~\cite{lofgren2016personalized,he2024unigraphlearningunifiedcrossdomain}.

\begin{table*}[t]\footnotesize
    \centering
    \renewcommand\tabcolsep{2.7pt}
    \caption{\textbf{Experiment results in self-supervised representation learning.} We report accuracy (\%) for node/edge classification tasks and MRR (\%) for link prediction tasks. \model and other self-supervised baselines (rows in white) are jointly pre-trained on Products, Papers100M, Goodreads-LP and Amazon-Cloth, and then evaluated on the individual target dataset. \textit{"In-distribution"} refers to pre-training on multiple datasets and evaluating on the same datasets. \textit{"In-domain Generalization"} involves testing on target datasets from the same domain as one of the pre-training datasets. \textit{"Out-of-domain Generalization"} evaluates on datasets from domains not seen during pre-training. The performance of methods that are directly pre-trained on the individual target dataset, is marked in \colorbox{Gray}{gray}. The methods highlighted in \textbf{bold} are the best-performing ones among the "rows in white" methods, while those marked in \textcolor{red}{red} are the best-performing methods among all methods, including those in the \colorbox{Gray}{gray} rows.
    }
    \vskip -0.10in
    \label{tab:ssrl}
    \begin{tabular}{lccccccccccc}
    \toprule[1.1pt]
    & \multicolumn{4}{c}{\textbf{In-distribution}} & \multicolumn{4}{c}{\textbf{In-domain Generalization}}& \multicolumn{3}{c}{\textbf{Out-of-domain Generalization}}\\
    \cmidrule(lr){2-5}\cmidrule(lr){6-9}\cmidrule(lr){10-12}
         & Products & Papers100M & Goodreads-LP & Amazon-Cloth & Arxiv & Amazon-Sports & Goodreads-NC & Ele-fashion & Wiki-CS & FB15K237 & WN18RR \\
    \midrule
    \multicolumn{5}{l}{\textbf{Use CLIP to encode raw multimodal data as input features.}} \\ 
    % Linear & 65.28{\footnotesize$\pm$0.12} & 50.21{\footnotesize$\pm$0.09} & 66.48{\footnotesize$\pm$0.11} & 18.24{\footnotesize$\pm$0.21} & 61.56{\footnotesize$\pm$0.02} & 25.91{\footnotesize$\pm$0.08} & 9.24{\footnotesize$\pm$0.01} & 82.18{\footnotesize$\pm$0.03} & 67.53{\footnotesize$\pm$0.05} & 88.65{\footnotesize$\pm$0.13} & 72.68{\footnotesize$\pm$0.14}\\
    NoPretrain & 68.01{\footnotesize$\pm$0.15} & 54.99{\footnotesize$\pm$0.04} & 9.61{\footnotesize$\pm$0.21}& 19.01{\footnotesize$\pm$0.04} & 62.01{\footnotesize$\pm$0.14} & 26.01{\footnotesize$\pm$0.14} & 68.12{\footnotesize$\pm$0.13} & 75.11{\footnotesize$\pm$0.12} & 68.12{\footnotesize$\pm$0.06} & 89.42{\footnotesize$\pm$0.20} & 74.00{\footnotesize$\pm$0.02}\\
    BGRL & 70.11{\footnotesize$\pm$0.14} & 57.12{\footnotesize$\pm$0.05} & 20.53{\footnotesize$\pm$0.02} & 19.11{\footnotesize$\pm$0.01} & 65.25{\footnotesize$\pm$0.05} & 27.35{\footnotesize$\pm$0.05} & 72.97{\footnotesize$\pm$0.08} & 76.53{\footnotesize$\pm$0.02} & 70.11{\footnotesize$\pm$0.14} & 88.11{\footnotesize$\pm$0.12} & 73.24{\footnotesize$\pm$0.11}\\
    \rowcolor{Gray} BGRL & 75.86{\footnotesize$\pm$0.11} & 60.35{\footnotesize$\pm$0.11} & 26.42{\footnotesize$\pm$0.15}& 20.11{\footnotesize$\pm$0.45} & 70.15{\footnotesize$\pm$0.14} & 30.11{\footnotesize$\pm$0.12} & 80.53{\footnotesize$\pm$0.35} & 81.94{\footnotesize$\pm$0.10} & 73.11{\footnotesize$\pm$0.09} & 92.22{\footnotesize$\pm$0.14} & 76.15{\footnotesize$\pm$0.16} \\
    GraphMAE2 & 72.25{\footnotesize$\pm$0.16} & 60.25{\footnotesize$\pm$0.01} & 24.11{\footnotesize$\pm$0.14} & 19.55{\footnotesize$\pm$0.22} & 69.18{\footnotesize$\pm$0.02} & 28.94{\footnotesize$\pm$0.02} & 76.18{\footnotesize$\pm$0.05} & 77.04{\footnotesize$\pm$0.05} & 72.15{\footnotesize$\pm$0.14} & 90.54{\footnotesize$\pm$0.04} & 74.11{\footnotesize$\pm$0.13}\\
    \rowcolor{Gray} GraphMAE2 & 77.34{\footnotesize$\pm$0.15} & 61.97{\footnotesize$\pm$0.10} & 26.89{\footnotesize$\pm$0.14}& 19.87{\footnotesize$\pm$0.21} & 70.46{\footnotesize$\pm$0.07} & 30.83{\footnotesize$\pm$0.11} & 80.24{\footnotesize$\pm$0.14} & 82.11{\footnotesize$\pm$0.01} & 76.01{\footnotesize$\pm$0.24} & 92.96{\footnotesize$\pm$0.14} & 76.97{\footnotesize$\pm$0.14} \\
    GCOPE & 78.01{\footnotesize$\pm$0.13} & 62.34{\footnotesize$\pm$0.11} & 23.11{\footnotesize$\pm$0.13}& 18.72{\footnotesize$\pm$0.25} & 70.24{\footnotesize$\pm$0.11} & 26.18{\footnotesize$\pm$0.12} & 79.11{\footnotesize$\pm$0.14} & 78.97{\footnotesize$\pm$0.10} & 73.57{\footnotesize$\pm$0.12} & 91.25{\footnotesize$\pm$0.15} & 75.68{\footnotesize$\pm$0.10} \\
    \midrule
    \multicolumn{5}{l}{\textbf{Use raw text as input features.}} \\
    GIANT-XRT  &  72.56{\footnotesize$\pm$0.10} & 64.53{\footnotesize$\pm$0.11} & 8.11{\footnotesize$\pm$0.05} & 16.78{\footnotesize$\pm$0.25} & 70.89{\footnotesize$\pm$0.11} & 22.01{\footnotesize$\pm$0.04} & 58.14{\footnotesize$\pm$0.10} & 67.01{\footnotesize$\pm$0.05} & 74.01{\footnotesize$\pm$0.03} & 90.14{\footnotesize$\pm$0.14} & 75.01{\footnotesize$\pm$0.13}\\
    % +GraphMAE2 &  \\
    UniGraph & 80.11{\footnotesize$\pm$0.21} & 65.23{\footnotesize$\pm$0.20} & 19.19{\footnotesize$\pm$0.10}& 16.38{\footnotesize$\pm$0.08} & 72.15{\footnotesize$\pm$0.18} & 25.89{\footnotesize$\pm$0.12} & 73.26{\footnotesize$\pm$0.12} & 75.11{\footnotesize$\pm$0.06} & 76.35{\footnotesize$\pm$0.20} & 93.11{\footnotesize$\pm$0.09} & 84.06{\footnotesize$\pm$0.24} \\
    \rowcolor{Gray} UniGraph  & 82.24{\footnotesize$\pm$0.24} & 67.89{\footnotesize$\pm$0.21} & 22.31{\footnotesize$\pm$0.05}& 18.01{\footnotesize$\pm$0.03} & \textcolor{red}{73.97{\footnotesize$\pm$0.22}} & 27.11{\footnotesize$\pm$0.10} & 78.14{\footnotesize$\pm$0.11} & 81.05{\footnotesize$\pm$0.08} & 81.22{\footnotesize$\pm$0.24} & 95.24{\footnotesize$\pm$0.23} & 87.21{\footnotesize$\pm$0.76} \\
    \midrule
    \multicolumn{5}{l}{\textbf{Use raw multimodal data as input features.}} \\
    CLIP & 65.28{\footnotesize$\pm$0.12} & 50.21{\footnotesize$\pm$0.09} & 9.24{\footnotesize$\pm$0.01} & 18.24{\footnotesize$\pm$0.21} & 61.56{\footnotesize$\pm$0.02} & 25.91{\footnotesize$\pm$0.08} & 66.48{\footnotesize$\pm$0.11} & 82.18{\footnotesize$\pm$0.03} & 67.53{\footnotesize$\pm$0.05} & 88.65{\footnotesize$\pm$0.13} & 72.68{\footnotesize$\pm$0.14}\\
    ImageBind & 45.11{\footnotesize$\pm$0.02} & 42.53{\footnotesize$\pm$0.11} & 6.89{\footnotesize$\pm$0.04} & 19.10{\footnotesize$\pm$0.10} & 42.11{\footnotesize$\pm$0.03} & 27.11{\footnotesize$\pm$0.04} &  55.71{\footnotesize$\pm$0.04}& 83.14{\footnotesize$\pm$0.06} & 49.28{\footnotesize$\pm$0.03} & 68.20{\footnotesize$\pm$0.10} & 64.38{\footnotesize$\pm$0.12} \\
    \hdashline
    NoPretrain & 68.34{\footnotesize$\pm$0.14} & 55.15{\footnotesize$\pm$0.10} & 9.62{\footnotesize$\pm$0.02}& 19.25{\footnotesize$\pm$0.04} & 63.76{\footnotesize$\pm$0.11} & 25.03{\footnotesize$\pm$0.15} & 68.01{\footnotesize$\pm$0.15} & 83.96{\footnotesize$\pm$0.10} & 68.45{\footnotesize$\pm$0.10} & 89.14{\footnotesize$\pm$0.19} & 74.01{\footnotesize$\pm$0.15}\\
    \model & \textcolor{red}{\textbf{82.79{\footnotesize$\pm$0.02}}} & \textcolor{red}{\textbf{67.95{\footnotesize$\pm$0.11}}} & \textcolor{red}{\textbf{28.98{\footnotesize$\pm$0.11}}}& \textcolor{red}{\textbf{24.64{\footnotesize$\pm$0.09}}} & \textbf{72.56{\footnotesize$\pm$0.15}} & \textbf{30.95{\footnotesize$\pm$0.11}} & \textbf{81.15{\footnotesize$\pm$0.12}} & \textbf{85.71{\footnotesize$\pm$0.11}} & \textbf{78.15{\footnotesize$\pm$0.09}} & \textbf{94.38{\footnotesize$\pm$0.05}} & \textbf{85.47{\footnotesize$\pm$0.11}} \\
    \rowcolor{Gray} \model  & 82.36{\footnotesize$\pm$0.21}  & 67.67{\footnotesize$\pm$0.18} & 28.76{\footnotesize$\pm$0.08}& 24.06{\footnotesize$\pm$0.06} & 73.46{\footnotesize$\pm$0.17} & \textcolor{red}{31.61{\footnotesize$\pm$0.14}} & \textcolor{red}{81.97{\footnotesize$\pm$0.10}} & \textcolor{red}{87.91{\footnotesize$\pm$0.09}} & \textcolor{red}{82.86{\footnotesize$\pm$0.07}} & \textcolor{red}{95.29{\footnotesize$\pm$0.04}} & \textcolor{red}{87.86{\footnotesize$\pm$0.06}} \\
    \bottomrule[1.1pt]
    \end{tabular}
    \vspace{-3mm}
\end{table*}


\subsection{Multiple Decoders}
Graphs from diverse domains exhibit distinct structural and feature characteristics. 
A single, generic decoder would struggle to capture the specific nuances and patterns of each domain, as different types of graphs often require specialized approaches for feature reconstruction. 
By incorporating multiple decoders, each tailored to a specific graph domain, UniGraph2 is able to accurately reconstruct features while preserving domain-specific details.

\vpara{Feature Reconstruction.} Each decoder is responsible for reconstructing the original node features \( \vx_i \) from the embeddings \( \vz_i \) generated by the GNN encoder. Formally, for a domain-specific GNN decoder \( f_D \), the reconstructed feature \( \vz_i \) is obtained as:
\begin{equation}
    \vz_i = f_D(\vh_i, \mathcal{G}).
\end{equation}
To measure the reconstruction quality, UniGraph2 uses a cosine similarity loss~\cite{hou2023graphmae2,hou2022graphmae}, which is defined as follows:
\begin{equation}
    \mathcal{L}_{\text{feat}} = \frac{1}{|\tilde{\mathcal{V}}|} \sum_{v_i \in \tilde{\mathcal{V}}} \left( 1 - \frac{\vx_i^T \vz_i}{\|\vx_i\| \cdot \|\vz_i\|} \right)^\gamma, \quad \gamma \geq 1,
\end{equation}
where \( \vx_i \) represents the original feature for node \( v_i \), \( \vz_i \) is the reconstructed feature, and \( \gamma \) is a hyperparameter that controls the sharpness of the loss. This loss ensures that the reconstructed features \( \vz_i \) maintain the same directional similarity as the original features \( \vx_i \), encouraging accurate feature reconstruction.

% Each domain-specific decoder focuses on reconstructing the node features for the corresponding domain, allowing UniGraph2 to specialize its decoding process based on the unique properties of each graph type.

\vpara{Structural Reconstruction.} In addition to reconstructing node features, UniGraph2 incorporates a shared decoder across all domains to capture structural information. Specifically, the model performs an edge-level reconstruction task to predict the shortest path distance (SPD) between node pairs, which encodes global proximity and connectivity within the graph.

The shortest path distance \( \text{SPD}_{i,j} \) between nodes \( v_i \) and \( v_j \) is pre-computed using Dijkstra's algorithm. The loss function for shortest path distance regression is defined as:
% \begin{equation}
%     \mathcal{L}_{\text{SPD}} = \frac{1}{|\mathcal{E}|} \sum_{(i,j) \in \mathcal{E}} \left\| f_{SPD} (\vh_i \parallel \vh_j) - \text{SPD}_{i,j} \right\|^2,
% \end{equation}
\begin{equation} 
    \mathcal{L}_{\text{SPD}} = \frac{1}{|\mathcal{V}|^2} \sum_{(i,j) \in \mathcal{V} \times \mathcal{V}} \left\| f_{SPD} (\vh_i \parallel \vh_j) - \text{SPD}_{i,j} \right\|^2, 
    \end{equation}
where \( \vh_i \) and \( \vh_j \) are the final GNN embeddings for nodes \( v_i \) and \( v_j \), respectively, \( \parallel \) denotes concatenation, and \( f_{SPD} \) is a task-specific head that predicts the shortest path distance between the two nodes.
By regressing the SPD, the model learns to reconstruct the underlying structure of the graph, allowing it to capture the global connectivity between nodes, which is essential for tasks that depend on the graph's topology.

Then overall loss is obtained by combining the two losses with a mixing coefficient $\lambda$.

\vspace{-1mm}
\subsection{Inference}
In the inference phase, the pre-trained \model model is deployed to generate node embeddings for any unseen multimodal graph \( \mathcal{G}^{\text{inf}} = (\mathcal{V}^{\text{inf}}, \mathcal{E}^{\text{inf}}, \mathcal{M}^{\text{inf}}) \). The inference process follows a streamlined version of the training pipeline, leveraging the Modality-Specific Encoders, the MoE module, and the GNN to produce high-quality embeddings for downstream tasks such as classification, transfer learning, or generative tasks.

\vpara{Modality-Specific Encoding.}
For each node \( v_i \in \mathcal{V}^{\text{inf}} \), the raw features from various modalities are first processed through the respective modality-specific encoders. Let \( \Omega_{v_i}^{\text{inf}} \subseteq \Omega \) represent the set of modalities associated with node \( v_i \) in the inference graph. The modality-specific features are transformed as follows:
\(
\vx_i^{(\omega)} = E_{\omega}(v_i^{(\omega)}), \quad \forall \omega \in \Omega_{v_i}^{\text{inf}}.
\)
The node feature vector \( \vx_i^{\text{inf}} \) is obtained by averaging the features from all available modalities:
\(
    \vx_i = \frac{1}{|\Omega_v|} \sum_{\omega \in \Omega_v} \vx_i^{(\omega)}.
\)

\vpara{Feature Alignment.}
The modality-specific feature vectors are passed through the MoE module to align and fuse information across modalities and domains. The same gating mechanism used during training is applied to select the relevant experts for each node.
For each node \( v_i \), the final fused embedding \( \ve_i^{\text{inf}} \) is computed as a weighted sum of the selected experts:
\(
\ve_i^{\text{inf}} = \sum_{k=1}^{K} \alpha_{i,k}^{\text{inf}} E_k(\vx_i^{\text{inf}}),
\)
where \( \vx_i^{\text{inf}} \) is the transformed feature of node \( v_i \), and \( \alpha_{i,k}^{\text{inf}} \) represents the weight assigned to expert \( E_k \) for the given node, computed using the softmax gating function.

\vpara{GNN Encoding.}
Once the aligned node features are obtained, they are passed through the GNN module to incorporate the structural information of the inference graph \( \mathcal{G}^{\text{inf}} \). The GNN refines node embeddings by propagating messages between neighboring nodes. The output node embeddings \( \vh_i^{\text{inf}} \) are computed as:
\(
\vh_i^{\text{inf}} = f_{\text{GNN}}(\ve_i^{\text{inf}}, \mathcal{G}^{\text{inf}}),
\)
where \( f_{\text{GNN}} \) is the pre-trained GNN.
