\vspace{-1mm}
\section{Experiments}
In this section, we evaluate our \model framework on three distinct research problems: 1) Self-Supervised Representation Learning, 2) Few-Shot Transfer, and 3) Multimodal Generative Tasks. 
Table~\ref{tab:dataset} lists all 14 datasets used in the experiments.
\vspace{-1mm}
\subsection{Self-Supervised Representation Learning}
\label{sec:lp}
\vpara{Setup.}
We adopt the widely used linear probing protocol to evaluate the representation learning capability of self-supervised pre-trained models on unseen datasets. Specifically, we train a linear classifier on top of the embeddings generated by a frozen pre-trained model. Our model, along with all self-supervised learning baselines, is first jointly pre-trained on ogbn-Product, ogbn-Papers100M, Goodreads-LP, and Amazon-Cloth. We then evaluate the pre-trained models on each individual dataset. Detailed settings and hyperparameters are provided in Appendix~\ref{appendix:imple}.

For the baselines, we compare \model with state-of-the-art generative graph self-supervised learning methods, GraphMAE2~\cite{hou2023graphmae2}, and contrastive methods, BGRL~\cite{thakoor2021bootstrapped}. As these methods are not inherently designed for cross-domain tasks, we leverage CLIP~\cite{radford2021learning} to unify the input node features across different graphs. We also include a comparison with a multi-graph pre-training method, GCOPE~\cite{zhao2024all}. \model and all baseline methods utilize GAT~\cite{velivckovic2018graph} as the backbone GNN. 
For baselines that use TAGs as input, we select GIANT-XRT~\cite{zhaolearning} and UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain}. Since these methods cannot process image data, they rely solely on text from MMG as node features, ignoring image inputs. For baseline approaches that accept multimodal data, we choose widely used multimodal models, CLIP~\cite{radford2021learning} and ImageBind~\cite{girdhar2023imagebind}. To maintain consistency with the baselines, \model also uses CLIP's pre-trained vision and text encoders as Modality-Specific Encoders.


Our objective is to develop a general embedding model capable of generating high-quality representations for any MMG. To assess this, we evaluate the performance of \model and the baselines in three different settings: (1) \textit{In-distribution}, where models are pre-trained on multiple datasets and evaluated on each corresponding dataset individually; (2) \textit{In-domain Generalization}, which tests pre-trained models on target datasets from the same domain as one of the pre-training datasets; and (3) \textit{Out-of-domain Generalization}, where models are evaluated on datasets from domains unseen during pre-training.

\vpara{Research Questions.} In this subsection, we aim to answer the following research questions: 
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{RQ1: Negative Transfer in Multi-Graph Pre-Training.} How do existing graph pre-training methods, which are primarily designed for single-graph pre-training, perform when applied to multi-graph pre-training, and how do they compare to our proposed \model?
    \item \textbf{RQ2: Comparison to Other Foundation Models.} How does \model, which takes both multimodal data and graph structures as input, perform compared to methods that consider only multimodal data (CLIP, ImageBind) or only TAGs (UniGraph)?
    \item \textbf{RQ3: Generalization Capability.} How does \model, designed as a foundation model, perform in terms of generalizing to unseen graphs, and how does it compare to methods trained directly on the target graphs?
\end{itemize}

\begin{table*}[t]\footnotesize
    \centering
    \renewcommand\tabcolsep{3.5pt}
    \caption{\textbf{Experiment results in few-shot transfer.} We report accuracy (\%) for node/edge classification tasks. \model and other self-supervised baselines (rows in white) are jointly pre-trained on Product, Papers100M, Goodreads-NC and Amazon-Cloth, and then evaluated on the individual target dataset. \textit{"In-domain Generalization"} tests on target datasets from the same domain as one of the pre-training datasets. \textit{"Out-of-domain Generalization"} evaluates on datasets from domains not seen during pre-training. The performance of methods that are direcly pre-trained on the individual target dataset, is marked in \colorbox{Gray}{gray}. 
    }
    \vskip -0.10in
    \label{tab:fwt}
    \begin{tabular}{lcccccccccccccccccc}
    \toprule[1.1pt]
    & \multicolumn{12}{c}{\textbf{In-domain Generalization}}& \multicolumn{6}{c}{\textbf{Out-of-domain Generalization}}\\
   \cmidrule(lr){2-13}\cmidrule(lr){14-19}
        & \multicolumn{2}{c}{Cora-5-way} & \multicolumn{2}{c}{PubMed-2-way} & \multicolumn{2}{c}{Arxiv-5-way} & \multicolumn{3}{c}{Goodreads-NC-5-way} & \multicolumn{3}{c}{Ele-fashion-5-way} & \multicolumn{2}{c}{Wiki-CS-5-way} & \multicolumn{2}{c}{FB15K237-20-way} & \multicolumn{2}{c}{WN18RR-5-way} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}\cmidrule(lr){8-10}\cmidrule(lr){11-13}\cmidrule(lr){14-15}\cmidrule(lr){12-13}\cmidrule(lr){14-15}\cmidrule(lr){16-17}\cmidrule(lr){18-19}
    &5-shot & 1-shot  & 5-shot & 1-shot  &5-shot & 1-shot  &5-shot& 3-shot & 1-shot  &5-shot & 3-shot& 1-shot  & 5-shot & 1-shot  &5-shot & 1-shot  & 5-shot & 1-shot \\
    \midrule
    \multicolumn{10}{l}{\textbf{Use CLIP to encode raw multimodal data as input features.}} \\ 
    NoPretrain & 41.09 & 27.05 & 59.81 & 55.28 & 63.78 & 41.10 & 41.64 & 40.01 & 31.04 & 63.96 & 58.32 & 47.48 & 52.29 & 32.94 & 72.97 & 47.01 & 50.75 & 30.11  \\
    BGRL & 52.01 & 35.18 & 66.04 & 59.04 & 60.12 & 46.67 & 47.01 & 44.22 & 30.35 & 64.72 & 60.16 & 46.49 & 52.10 & 32.85 & 75.39 & 45.15 & 47.42 & 34.57 \\
    % \rowcolor{Gray} BGRL \\
    GraphMAE2 & 52.89 & 36.25 & 66.89 & 59.95 & 60.91 & 47.29 & 47.84 & 44.80 & 30.93 & 65.52 & 60.92 & 47.24 & 52.83 & 33.41 & 75.95 & 45.81 & 48.14 & 35.21 \\
    Prodigy & 53.01 & 39.59 & 69.11 & 60.42 & 63.53 & \underline{51.33} & \underline{50.01} & \underline{46.39} & 34.98 & 67.35 & 63.87 & 50.79 & 55.94 & 36.35 & 78.01 & 51.39 & 54.94 & 38.73 \\
    \rowcolor{Gray} OFA & 53.11 & 40.04 & 69.45 & \underline{60.38} & 63.11 & 50.25 & 49.61 & 46.24 & \underline{35.14} & \underline{67.94} & \underline{64.18} & \underline{51.35} & \underline{56.01} & \underline{37.02} & \underline{78.33} & 52.02 & 55.05 & 39.11 \\
    % \rowcolor{Gray} GraphMAE2 \\
    GCOPE & 51.98 & 36.14 & 66.25 & 59.16 & 60.29 & 47.19 & 48.52 & 44.89 & 31.20 & 65.10 & 61.33 & 48.51 & 53.74 & 34.19 & 76.10 & 48.93 & 50.19 & 35.05 \\
    \midrule
    \multicolumn{10}{l}{\textbf{Use raw text as input features.}} \\
    GIANT-XRT   & 50.11 & 37.85 & 68.19 & 58.78 & 62.01 & 49.01 & 46.01 & 43.86 & 30.01 & 62.97 & 61.21 & 47.76 & 54.01 & 35.04 & 76.09 & 50.25 & 53.01 & 35.19\\
    % +GraphMAE2 &  \\
    UniGraph & \underline{54.23} & \underline{40.45} & \underline{70.21} & 60.19 & \underline{64.76} & 50.63 & 46.19 & 44.01 & 33.53 & 66.21 & 62.04 & 50.17 & 56.16 & 37.19 & 78.21 & \underline{52.19} & \underline{55.18} & \underline{39.18}\\
    % \rowcolor{Gray} UniGraph  &  \\
    \midrule
    \multicolumn{10}{l}{\textbf{Use raw multimodal data as input features.}} \\
    CLIP & 41.23 & 28.41 & 61.67 & 55.71 & 63.46 & 40.14 & 41.24 & 40.11 & 30.97 & 62.51 & 58.23 & 46.15 & 51.69 & 31.61 & 72.31 & 47.14 & 50.83 & 31.35 \\
    ImageBind & 32.19 & 23.90 & 58.20 & 54.24 & 62.48 & 38.17 & 29.10 & 28.14 & 21.42 & 51.25 & 48.05 & 44.93 & 48.14 & 30.28 & 69.12 & 41.80 & 41.24 & 26.91 \\
    \hdashline
    NoPretrain & 42.41 & 28.39 & 60.78 & 55.90 & 64.29 & 41.98 & 42.21 & 41.20 & 31.14 & 64.15 & 58.91 & 47.90 & 52.90 & 33.14 & 74.10 & 48.11 & 51.92 & 31.84  \\
    \model & \textbf{56.01} & \textbf{42.98} & \textbf{72.19} & \textbf{61.24} & \textbf{66.24} & \textbf{51.98} & \textbf{51.73} & \textbf{47.42} & \textbf{37.01} & \textbf{69.29} & \textbf{65.29} & \textbf{53.85} & \textbf{57.28} & \textbf{38.47} & \textbf{79.34} & \textbf{52.19} & \textbf{55.59} & \textbf{39.93}\\
    % \rowcolor{Gray} \model  &  \\
    \bottomrule[1.1pt]
    \end{tabular}
    \vspace{-4.6mm}
\end{table*}




\vpara{Results.}
Table~\ref{tab:ssrl} presents the results.
We interpret these results by answering three research questions:
\begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]
    \item \textbf{RQ1: Negative Transfer in Multi-Graph Pre-Training.} Existing graph pre-training methods exhibit negative transfer when applied to multi-graph pre-training, whereas \model shows improvements in this context. The results in the \textit{In-distribution} setting demonstrate that both BGRL and GraphMAE2 experience a significant performance drop when pre-trained on multi-graphs (rows in white), compared to pre-training on single graph only (rows in gray). This suggests that pre-training on other datasets negatively affects performance on the target dataset. However, UniGraph2 shows improvement under multi-graph pre-training, indicating that it successfully addresses the shortcomings of existing graph pre-training algorithms struggling with multi-graphs.
    \item \textbf{RQ2: Comparison to Other Foundation Models.} UniGraph2 outperforms methods that consider only multimodal data (CLIP, ImageBind) or only TAGs (UniGraph). We observe that without considering the graph structure, the performance of the acknowledged powerful multimodal foundation models like CLIP is not comparable to UniGraph2. Meanwhile, UniGraph, which cannot process image data, also shows less ideal results due to the lack of information. This further highlights the necessity of designing foundation models specifically for multimodal graphs.
    \item \textbf{RQ3: Generalization Capability.} Compared to baseline methods, UniGraph2 demonstrates strong generalization capabilities. The results in the \textit{In-domain Generalization} and \textit{Out-of-domain Generalization} settings show that UniGraph2 effectively transfers knowledge from pre-training to unseen graphs. Compared to the NoPretrain method, UniGraph2 shows significant improvements. The consistent performance gains indicate that UniGraph2 can extract meaningful patterns during pre-training, which are beneficial for tackling graph learning tasks. Furthermore, UniGraph2 is comparable to methods trained directly on the target datasets, achieving similar accuracy while benefiting from greater efficiency without requiring exhaustive task-specific training.
\end{itemize}










\vspace{-2.8mm}
\subsection{Few-Shot Transfer}
\vpara{Setup.}
In this part, we evaluate the ability of the pre-trained models to perform few-shot in-context transfer without updating the model parameters. 
For baseline methods, in addition to the pre-trained models mentioned in Section~\ref{sec:lp}, we also compare two recent graph in-context learning methods: the self-supervised pre-training method Prodigy~\cite{huang2024prodigy} and the supervised pre-training method OFA~\cite{liuone}.


For evaluation, we strictly follow the setting of Prodigy~\cite{huang2024prodigy}. 
For an N-way K-shot task, we adopt the original train/validation/test splits in each downstream classification dataset, and construct a $K$-shot prompt for test nodes (or edges) from the test split by randomly selecting $K$ examples per way from the train split. By default in all experiments, we sample 500 test tasks.

We adopt the few-shot classification strategy in UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain} for \model. The model computes average embeddings for each class and assigns a query sample to the class with the highest similarity to its embedding.

% \vpara{Research Questions.}
% In this subsection, we aim to answer the following research questions: 
% \begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]
%     \item \textbf{RQ1:} How does \model, which takes both multimodal data and graph structures as input, perform in terms of few-shot transfer capabilities compared to foundation models that consider only multimodal data (CLIP, ImageBind) or only TAGs (UniGraph)?
%     \item \textbf{RQ2:} How does \model perform compared to other graph few-shot learning methods?
% \end{itemize}
\vpara{Results.}
In Table~\ref{tab:fwt}, our \model model consistently outperforms all the baselines. This further demonstrates the powerful generalization capabilities of UniGraph2 as a foundation model.
In particular, compared to other graph few-shot learning methods such as Prodigy, OFA, and GCOPE, UniGraph2 does not rely on complex prompt graph designs, and its simple few-shot strategy is both efficient and effective.


\begin{table*}[t]
\centering
 \renewcommand\tabcolsep{4.3pt}
\caption{Experiment results in multimodal generative tasks. We strictly follow the setting in MMGL~\cite{yoon2023multimodal}. The task is to generate a single sentence that summarizing the content of a particular section. The summary is generated based on all images and (non-summary) text present in the target and context sections. We provide different information of MMGs to the base LM: (1) section all (text + image), (2) page text, and (3) page all (all texts and images). We encode multiple multimodal neighbor information using three different neighbor encodings methods: \textit{Self-Attention with Text+Embeddings (SA-TE)}, \textit{Self-Attention with Embeddings (SA-E)}, and \textit{Cross-Attention with Embeddings (CA-E)}.}
\vskip -0.10in
\label{tab:gen}
\begin{tabular}{llcccccccccccc}
\toprule[1.1pt]
& & \multicolumn{4}{c}{BLEU-4} & \multicolumn{4}{c}{ROUGE-L} & \multicolumn{4}{c}{CIDEr} \\
\cmidrule(lr){3-6}\cmidrule(lr){7-10}\cmidrule(lr){11-14}
Input Type & Method & SA-TE & SA-E & CA-E  & Avg. gain & SA-TE & SA-E & CA-E  & Avg. gain & SA-TE & SA-E & CA-E & Avg. gain\\
\midrule
\multirow{2}{*}{Section all} & MMGL & 8.03 & 7.56 & 8.35 & - & 40.41 & 39.89 & 39.98 & - & 77.45 & 74.33 & 75.12 & - \\
& +\model & \textbf{9.24} & \textbf{9.01} & \textbf{9.39} & 15.57\% & \textbf{43.01} & \textbf{43.24} & \textbf{42.98} & 7.44\% & \textbf{81.15} & \textbf{80.39} & \textbf{81.91} & 7.32\% \\
\midrule
\multirow{2}{*}{Page text} & MMGL & 9.81 & 8.37 & 8.47 & - & 42.94 & 40.92 & 41.00 & & 92.71 & 80.14 & 80.72 & - \\
& +\model & \textbf{10.31} & \textbf{10.10} & \textbf{9.98} & 14.53\% & \textbf{43.19} & \textbf{43.08} & \textbf{42.75} &3.38\% & \textbf{93.19} & \textbf{90.41} & \textbf{93.11} & 9.56\% \\
\midrule
\multirow{2}{*}{Page all} & MMGL & 9.96 & 8.58 & 8.51 & - & 43.32 & 41.01 & 41.55 & - & 96.01 & 82.28 & 80.31 & - \\
& +\model & \textbf{10.12} & \textbf{10.05} & \textbf{10.33} & 13.38\% & \textbf{44.10} & \textbf{42.08} & \textbf{42.44} & 2.18\% & \textbf{96.32} & \textbf{91.24} & \textbf{94.15} & 9.49\% \\
% \midrule
% Max input length &  \\
    \bottomrule[1.1pt]
\end{tabular}
\vspace{-3mm}
\end{table*}


\vspace{-5mm}
\subsection{Multimodal Generative Tasks}
\vpara{Setup.}
\model is designed as a general representation learning model. The embeddings it generates can be utilized by various generative foundation models, such as LLMs, to empower downstream generative tasks. 
% \model is a general embedding model designed to generate embeddings that can be used by various generative foundation models, such as LLMs, to enhance downstream generative tasks. 
To further demonstrate this, we select the section summarization task on the WikiWeb2M dataset for our experiments.
The WikiWeb2M dataset~\cite{burns2023suite} is designed for multimodal content understanding, using many-to-many text and image relationships from Wikipedia. It includes page titles, section titles, section text, images, and indices for each section.
In this work, we focus on section summarization, where the task is to generate a summary sentence from section content using both text and images.

% \todo{how mmgl do}
For the experiments, we follow the MMGL~\cite{yoon2023multimodal} setup, using four types of information: section text, section images, context text, and page-level text/images. 
Consistent with MMGL, we fine-tune Open Pre-trained Transformer (OPT-125m)~\cite{zhang2022opt} to read the input section text/images and generate a summary. Multimodal neighbors are first encoded using frozen vision/text encoders and then aligned to the text-only LM space using 1-layer MLP mapper.
In MMGL, CLIP~\cite{radford2021learning} encoders are used for text and image encoding, remaining frozen during fine-tuning. In our experiments, we replace CLIP embeddings with our \model embeddings.

% \vpara{Research Questions.}
% In this subsection, we aim to answer the following research question: 
% \begin{itemize}[leftmargin=*,itemsep=0pt,parsep=0.2em,topsep=0.3em,partopsep=0.3em]
%     \item \textbf{RQ1:} How do the embeddings generated by \model perform on generative tasks compared to multimodal foundation models like CLIP?
% \end{itemize}


\vpara{Results.}
Table~\ref{tab:gen} shows that under different input types and different neighbor encoding strategies, the embeddings generated by UniGraph2 bring significant improvements compared to MMGL's default CLIP embeddings. 
We also observe that UniGraph2's embeddings are more robust to different neighbor encoding strategies compared to CLIP and do not rely on a specific strategy.



\begin{table}[t]%\small
\centering
\renewcommand\tabcolsep{1.6pt}
\caption{\textbf{Ablation studies on \model key components.}}
\vskip -0.1in
\label{tab:kc}
\begin{tabular}{lcccc}
\toprule[1.1pt]
    & Products & Amazon-Cloth & Goodreads-NC  & WN18RR \\
\midrule
    \model & \textbf{82.79{\tiny$\pm$0.02}} & \textbf{24.64{\tiny$\pm$0.09}} & \textbf{81.15{\tiny$\pm$0.12}} & \textbf{85.47{\tiny$\pm$0.11}}\\
    w/o MoE & 81.01{\tiny$\pm$0.10} & 21.33{\tiny$\pm$0.04} & 80.10{\tiny$\pm$0.04} & 83.99{\tiny$\pm$0.21}\\
    w/o feat loss& 69.12{\tiny$\pm$0.09} & 18.43{\tiny$\pm$0.24} & 68.12{\tiny$\pm$0.01} & 74.11{\tiny$\pm$0.03}\\
    w/o SPD loss& 82.42{\tiny$\pm$0.11} & 23.39{\tiny$\pm$0.05} & 80.24{\tiny$\pm$0.02} & 85.24{\tiny$\pm$0.11}\\
\bottomrule[1.1pt]
\end{tabular}
\vspace{-3.3mm}
\end{table}

\begin{table}[t]%\small
\centering
\renewcommand\tabcolsep{2.4pt}
\caption{\textbf{Ablation studies on Modality-Specific Encoders.}}
\vskip -0.1in
\label{tab:enc}
\begin{tabular}{lcccc}
\toprule[1.1pt]
    & Products & Amazon-Cloth & Goodreads-NC  & WN18RR \\
\midrule
    CLIP & 82.79{\tiny$\pm$0.02} & 24.64{\tiny$\pm$0.09} & 81.15{\tiny$\pm$0.12} & \textbf{85.47{\tiny$\pm$0.11}}\\
    ImageBind & 82.32{\tiny$\pm$0.05} & \textbf{25.01{\tiny$\pm$0.11}} & 80.33{\tiny$\pm$0.22} & 84.29{\tiny$\pm$0.07}\\
    T5+ViT& \textbf{82.99{\tiny$\pm$0.04}} & 24.38{\tiny$\pm$0.28} & \textbf{81.28{\tiny$\pm$0.11}} & 84.16{\tiny$\pm$0.04}\\
\bottomrule[1.1pt]
\end{tabular}
\vspace{-4.8mm}
\end{table}

\subsection{Model Analysis}
We select four datasets from different domains to conduct more in-depth studies. We adopt self-supervised representation learning for evaluation.

\vpara{Ablation on Key Components.}
Table~\ref{tab:kc} shows the performance of the \model framework after removing some key designs. "W/o MoE" represents that we use simple MLP instead MoE to align node features. 
"W/o feat loss" represents that we only use the SPD loss for pre-training, while "w/o SPD loss" refers to the opposite.
The overall results confirm that all key designs contribute positively to the performance of \model.

\vpara{Ablation on Modality-Specific Encoders}
In Table~\ref{tab:enc}, we study the influence of different Modality-Specific Encoders on the performance of encoding raw multimodal data. CLIP and ImageBind are feature encoders that map features from various modalities to a shared embedding space, whereas T5+ViT employs SOTA embedding methods for each modality independently, without specific alignment. The results show that all methods achieve comparable performance, indicating that \model effectively aligns features regardless of whether they have been pre-aligned or not.

\begin{table}[t] \scriptsize
\centering
\renewcommand\tabcolsep{3.5pt}
\caption{\textbf{Comparison of GPU hours and performance on ogbn-Arxiv and ogbn-Papers100M.}}
\vskip -0.1in
\label{tab:ccp}
\begin{tabular}{ccccc}
\toprule[1.1pt]
Method & Pre-training & Downstream Training & Downstream Inference & Test Accuracy \\
\midrule
\multicolumn{5}{l}{\textbf{ogbn-Arxiv (169,343 nodes)}} \\ 
% \multirow{3}{*}{\shortstack{ogbn-Arxiv \\ (169,343 nodes)}} 
  GAT        & -    & 0.39 h & 5.5 mins  & 70.89 $\pm$ 0.43 \\
  GraphMAE2  & -    & 5.1 h     & 5.4 mins  & 70.46 $\pm$ 0.07 \\
  UniGraph   & 28.1 h & -      & 9.8 mins & 72.15 $\pm$ 0.18 \\
  UniGraph2  & 5.2 h & - & 5.7 mins &    \textbf{72.56 $\pm$ 0.15}  \\
\midrule
\multicolumn{5}{l}{\textbf{ogbn-Papers100M (111,059,956 nodes)}} \\
  GAT        & -    & 6.8 h     & 23.1 mins & 65.98 $\pm$ 0.23 \\
  GraphMAE2  & -    & 23.2 h    & 23.0 mins & 61.97 $\pm$ 0.24 \\
  UniGraph   & 28.1 h & -      & 40.1 mins & 67.89 $\pm$ 0.21 \\
  UniGraph2 & 5.2 h & - & 24.8 mins &  \textbf{67.95 $\pm$ 0.11} \\
\bottomrule[1.1pt]
\end{tabular}
\vspace{-4.5mm}
\end{table}

\vpara{Efficiency Analysis.}
\model, designed as a foundation model, incurs significant computational costs primarily during the pre-training phase. 
However, it offers the advantage of applicability to new datasets in the inference phase without requiring retraining. 
We compare of the training and inference costs of our model with other models. GAT~\cite{velivckovic2018graph} is a supervised trained GNN. 
GraphMAE2~\cite{hou2023graphmae2} is a self-supervised learning method with GAT as the backbone network. 
UniGraph~\cite{he2024unigraphlearningunifiedcrossdomain} is a graph foundation model for TAGs.
We select ogbn-Arxiv and ogbn-Papers100M, two datasets of different scales for experiments. 
From the results in the Table~\ref{tab:ccp}, we observe that although UniGraph2 has a long pre-training time, its inference time on downstream datasets is comparable or shorter than the combined training and inference time of GNN-based methods. This advantage further increases with the size and potential quantity of downstream datasets.
% The same conclusion also applies to space complexity. Although LM has a larger number of parameters, since we only need to perform inference on the downstream dataset, we avoid the additional space occupation in the backward propagation during training. 