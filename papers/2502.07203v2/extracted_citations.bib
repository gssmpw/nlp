@article{bigioi2024speech,
  title={Speech driven video editing via an audio-conditioned diffusion model},
  author={Bigioi, Dan and Basak, Shubhajit and Stypu{\l}kowski, Micha{\l} and Zieba, Maciej and Jordan, Hugh and McDonnell, Rachel and Corcoran, Peter},
  journal={Image and Vision Computing},
  volume={142},
  pages={104911},
  year={2024},
  publisher={Elsevier}
}

@article{cao2024joyvasa,
  title={JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation},
  author={Cao, Xuyang and Shi, Sheng and Zhao, Jun and Yao, Yang and Fei, Jintao and Gao, Minyu and Wang, Guoxin},
  journal={arXiv preprint arXiv:2411.09209},
  year={2024}
}

@article{chen2024echomimic,
  title={Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions},
  author={Chen, Zhiyuan and Cao, Jiajiong and Chen, Zhiquan and Li, Yuming and Ma, Chenguang},
  journal={arXiv preprint arXiv:2407.08136},
  year={2024}
}

@inproceedings{cheng2022videoretalking,
  title={Videoretalking: Audio-based lip synchronization for talking head video editing in the wild},
  author={Cheng, Kun and Cun, Xiaodong and Zhang, Yong and Xia, Menghan and Yin, Fei and Zhu, Mingrui and Wang, Xuan and Wang, Jue and Wang, Nannan},
  booktitle={SIGGRAPH Asia 2022 Conference Papers},
  pages={1--9},
  year={2022}
}

@inproceedings{gan2023efficient,
  title={Efficient emotional adaptation for audio-driven talking-head generation},
  author={Gan, Yuan and Yang, Zongxin and Yue, Xihang and Sun, Lingyun and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22634--22645},
  year={2023}
}

@inproceedings{ji2022eamm,
  title={Eamm: One-shot emotional talking face via audio-based emotion-aware motion model},
  author={Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun},
  booktitle={ACM SIGGRAPH 2022 Conference Proceedings},
  pages={1--10},
  year={2022}
}

@article{jiang2024loopy,
  title={Loopy: Taming audio-driven portrait avatar with long-term motion dependency},
  author={Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo},
  journal={arXiv preprint arXiv:2409.02634},
  year={2024}
}

@article{li2017learning,
  title={Learning a model of facial shape and expression from 4D scans.},
  author={Li, Tianye and Bolkart, Timo and Black, Michael J and Li, Hao and Romero, Javier},
  journal={ACM Trans. Graph.},
  volume={36},
  number={6},
  pages={194--1},
  year={2017}
}

@article{lin2024takin,
  title={Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical and Landmark Loss Optimization},
  author={Lin, Bin and Yu, Yanzhen and Ye, Jianhao and Lv, Ruitao and Yang, Yuguang and Xie, Ruoye and Yu, Pan and Zhou, Hongbin},
  journal={arXiv preprint arXiv:2410.14283},
  year={2024}
}

@article{ma2023dreamtalk,
  title={Dreamtalk: When expressive talking head generation meets diffusion probabilistic models},
  author={Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong},
  journal={arXiv e-prints},
  pages={arXiv--2312},
  year={2023}
}

@inproceedings{mukhopadhyay2024diff2lip,
  title={Diff2lip: Audio conditioned diffusion models for lip-synchronization},
  author={Mukhopadhyay, Soumik and Suri, Saksham and Gadde, Ravi Teja and Shrivastava, Abhinav},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5292--5302},
  year={2024}
}

@inproceedings{peng2024synctalk,
  title={Synctalk: The devil is in the synchronization for talking head synthesis},
  author={Peng, Ziqiao and Hu, Wentao and Shi, Yue and Zhu, Xiangyu and Zhang, Xiaomei and Zhao, Hao and He, Jun and Liu, Hongyan and Fan, Zhaoxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={666--676},
  year={2024}
}

@inproceedings{prajwal2020lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={484--492},
  year={2020}
}

@inproceedings{pumarola2018ganimation,
  title={Ganimation: Anatomically-aware facial animation from a single image},
  author={Pumarola, Albert and Agudo, Antonio and Martinez, Aleix M and Sanfeliu, Alberto and Moreno-Noguer, Francesc},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={818--833},
  year={2018}
}

@inproceedings{ren2021pirenderer,
  title={Pirenderer: Controllable portrait image generation via semantic neural rendering},
  author={Ren, Yurui and Li, Ge and Chen, Yuanqi and Li, Thomas H and Liu, Shan},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={13759--13768},
  year={2021}
}

@inproceedings{shen2023difftalk,
  title={Difftalk: Crafting diffusion models for generalized audio-driven portraits animation},
  author={Shen, Shuai and Zhao, Wenliang and Meng, Zibin and Li, Wanhua and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1982--1991},
  year={2023}
}

@article{siarohin2019first,
  title={First order motion model for image animation},
  author={Siarohin, Aliaksandr and Lathuili{\`e}re, St{\'e}phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@inproceedings{tian2025emo,
  title={EMO: Emote Portrait Alive Generating Expressive Portrait Videos with Audio2Video Diffusion Model Under Weak Conditions},
  author={Tian, Linrui and Wang, Qi and Zhang, Bang and Bo, Liefeng},
  booktitle={European Conference on Computer Vision},
  pages={244--260},
  year={2025},
  organization={Springer}
}

@article{xu2024hallo,
  title={Hallo: Hierarchical audio-driven visual synthesis for portrait image animation},
  author={Xu, Mingwang and Li, Hui and Su, Qingkun and Shang, Hanlin and Zhang, Liwei and Liu, Ce and Wang, Jingdong and Yao, Yao and Zhu, Siyu},
  journal={arXiv preprint arXiv:2406.08801},
  year={2024}
}

@article{xu2024vasa,
  title={Vasa-1: Lifelike audio-driven talking faces generated in real time},
  author={Xu, Sicheng and Chen, Guojun and Guo, Yu-Xiao and Yang, Jiaolong and Li, Chong and Zang, Zhenyu and Zhang, Yizhong and Tong, Xin and Guo, Baining},
  journal={arXiv preprint arXiv:2404.10667},
  year={2024}
}

@inproceedings{zhang2023dinet,
  title={Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video},
  author={Zhang, Zhimeng and Hu, Zhipeng and Deng, Wenjin and Fan, Changjie and Lv, Tangjie and Ding, Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={3},
  pages={3543--3551},
  year={2023}
}

@inproceedings{zhang2023sadtalker,
  title={Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8652--8661},
  year={2023}
}

@article{zheng2024memo,
  title={MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation},
  author={Zheng, Longtao and Zhang, Yifan and Guo, Hanzhong and Pan, Jiachun and Tan, Zhenxiong and Lu, Jiahao and Tang, Chuanxin and An, Bo and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2412.04448},
  year={2024}
}

