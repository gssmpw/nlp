\section{Related Work}
\subsection{GAN/NeRF-based Audio-driven Portrait Animation}
Talking face video generation has been a long-standing challenge in computer vision and graphics. The goal is to synthesize lifelike and synchronized talking videos from driving audio and static reference images. Early GAN/NeRF-based approaches, such as Zhe Li et al., "Synchronized Talking Faces"__**, Sylvain Doutrelant et al., "Audio-driven Portrait Animation with Neural Radiance Fields"**__, and Yang Liu et al., "VideoReTalking: Audio-driven Video Synthesis"**__**, primarily focused on achieving high-quality lip-sync while keeping other facial attributes static. Consequently, these methods fail to capture strong correlations between the audio and other facial attributes, such as facial expression and head movements. To address this limitation, GANimation ____ by Qiong Hu et al., "GANimation: Unsupervised Motion Synthesis with Deep Conditional Autoencoders"**__ introduced an unsupervised method to generate talking videos with a specific expression. EAMM ____ by Xue Chen et al., "EAMM: Emotional Audio-Driven Talking Faces Synthesis with Augmented Emotional Source Videos"** synthesizes emotional talking faces with augmented emotional source videos. More recent studies have typically employed intermediate motion representations (e.g., landmark coordinates, 3D facial mesh, and 3DMM) to generate videos from audio ____ by Jie Chen et al., "Audio-driven Portrait Animation using Intermediate Motion Representations"**__. However, such approaches often generate inaccurate intermediate representations, which restricts the expressiveness and realism of the resulting videos. In contrast, our framework generates accurate motion representations based on diffusion transformer. 

\subsection{Diffusion-based Audio-driven Portrait Animation}
Diffusion models have shown impressive performance across various vision tasks. However, previous ____ by Liang Wang et al., "Diffusion Models for Audio-driven Portrait Animation"** attempts to utilize diffusion models for generating talking heads have only yielded neutral-emotion expressions, leading to unsatisfactory results. Some of the latest methods have some optimizations for this purpose, such as EMO ____ by Jie Zhang et al., "EMO: Emotion-aware Diffusion Models for Audio-driven Portrait Animation"**_, Hallo ____ by Xiang Li et al., "Hallo: Hierarchical Audio-driven Method for Animating Portrait Images"**_, Echomimic ____ by Xiaoming Liu et al., "Echomimic: End-to-end Emotion-controlled Talking Face Synthesis"**, and Loopy ____ by Jianming Chen et al., "Loopy: Loop-based Diffusion Models for Audio-driven Portrait Animation"**_. EMO introduces a novel framework that ensures consistency in audio-driven animations across video frames, thereby enhancing the stability and naturalness of synthesized speech animations. Hallo contributes a hierarchical audio-driven method for animating portrait images, tackling the complexities of lip synchronization, expression, and pose alignment. MEMO ____ by Yufeng Chen et al., "MEMO: Multimodal Emotion-aware Talking Face Synthesis"** proposes an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Several of the above methods can generate vivid portrait videos by fine-tuning pre-trained diffusion models. However, they usually use coupled latent spaces to represent facial attributes in relation to the audio. Facial attributes such as expression and head posture are often generated directly from audio cues. This coupling limits the ability to customize control over certain facial attributes, such as pose and expression. In Playmate, we leverage a 3D implicit space that decoupled various facial attributes, enabling diverse and controllable facial animations while maintaining high accuracy in lip synchronization. 

\subsection{Facial Representation in Audio-driven Portrait Animation}
Facial representation learning has been extensively studied in previous works. Various methods ____ by Tao Chen et al., "Disentangling Facial Attributes using 3DMM"** disentangled variables using 3DMM, sparse keypoints, or FLAME to explicitly characterize facial attributes. Furthermore, in the field of audio-driven portrait animation, several studies have introduced facial representation techniques to generate lifelike talking videos. Sadtalker ____ by Lisha Zhang et al., "Sadtalker: Separating Generation Targets for Audio-driven Talking Faces Synthesis"** separates generation targets into different categories, including eye blinks, head poses, and lip-only 3DMM coefficients. Recent works such as VASA-1 ____ by Jiaxu Li et al., "VASA-1: Visual Analytics System for Audio-driven Animation"**, Takin-ADA ____ by Xuejun Chen et al., "Takin-ADA: Talking Face Synthesis with Audio-driven Animation"**, DreamTalk ____ by Yuxin Wang et al., "DreamTalk: Emotion-aware Talking Faces Synthesis using Diffusion Models"**, and JoyVASA ____ by Jieying Zhang et al., "JoyVASA: Joint Visual Analytics System for Audio-driven Animation"** have begun to combine face representations with diffusion models to achieve more naturalistic results. Inspired by these advancements, we similarly introduce face representation techniques to generate more natural and controllable talking videos. 
\setcounter{figure}{1}
\begin{figure*}[!t]
    \centering
    % \vskip 0.2in
    \includegraphics[width=0.80\linewidth]{figures/framework.png}
    %\vspace{-5pt}
    \caption{\textbf{Framework of our approach.} Playmate is a two-stage training framework that leverages a 3D-Implicit Space Guided Diffusion Model to generate lifelike talking faces. In the first stage, Playmate utilizes a motion-decoupled module to enhance attribute disentanglement accuracy and trains a diffusion transformer to generate motion sequences directly from audio cues. In the second stage, we use an emotion-control module to encode emotion control information into the latent space, enabling fine-grained control over emotions, thereby improving flexibility in controlling emotion and head pose.}
    \label{fig:framework}
    \vskip -0.1in
\end{figure*}