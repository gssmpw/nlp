\section{Related Work}
\subsection{GAN/NeRF-based Audio-driven Portrait Animation}
Talking face video generation has been a long-standing challenge in computer vision and graphics. The goal is to synthesize lifelike and synchronized talking videos from driving audio and static reference images. Early GAN/NeRF-based approaches, such as Wav2Lip ____, Dinet ____, and VideoReTalking ____, primarily focused on achieving high-quality lip-sync while keeping other facial attributes static. Consequently, these methods fail to capture strong correlations between the audio and other facial attributes, such as facial expression and head movements. To address this limitation, GANimation ____ introduced an unsupervised method to generate talking videos with a specific expression. EAMM ____ synthesizes emotional talking faces with augmented emotional source videos. More recent studies have typically employed intermediate motion representations (e.g., landmark coordinates, 3D facial mesh, and 3DMM) to generate videos from audio ____. However, such approaches often generate inaccurate intermediate representations, which restricts the expressiveness and realism of the resulting videos. In contrast, our framework generates accurate motion representations based on diffusion transformer. 

\subsection{Diffusion-based Audio-driven Portrait Animation}
Diffusion models have shown impressive performance across various vision tasks. However, previous ____ attempts to utilize diffusion models for generating talking heads have only yielded neutral-emotion expressions, leading to unsatisfactory results. Some of the latest methods have some optimizations for this purpose, such as EMO ____, Hallo ____, Echomimic ____, and Loopy ____. EMO introduces a novel framework that ensures consistency in audio-driven animations across video frames, thereby enhancing the stability and naturalness of synthesized speech animations. Hallo contributes a hierarchical audio-driven method for animating portrait images, tackling the complexities of lip synchronization, expression, and pose alignment. MEMO ____ proposes an end-to-end audio-driven portrait animation approach to generate identity-consistent and expressive talking videos. Several of the above methods can generate vivid portrait videos by fine-tuning pre-trained diffusion models. However, they usually use coupled latent spaces to represent facial attributes in relation to the audio. Facial attributes such as expression and head posture are often generated directly from audio cues. This coupling limits the ability to customize control over certain facial attributes, such as pose and expression. In Playmate, we leverage a 3D implicit space that decoupled various facial attributes, enabling diverse and controllable facial animations while maintaining high accuracy in lip synchronization. 

\subsection{Facial Representation in Audio-driven Portrait Animation}
Facial representation learning has been extensively studied in previous works. Various methods ____ disentangled variables using 3DMM, sparse keypoints, or FLAME to explicitly characterize facial attributes. Furthermore, in the field of audio-driven portrait animation, several studies have introduced facial representation techniques to generate lifelike talking videos. Sadtalker ____ separates generation targets into different categories, including eye blinks, head poses, and lip-only 3DMM coefficients. Recent works such as VASA-1 ____, Takin-ADA ____, DreamTalk ____, and JoyVASA ____ have begun to combine face representations with diffusion models to achieve more naturalistic results. Inspired by these advancements, we similarly introduce face representation techniques to generate more natural and controllable talking videos. 
\setcounter{figure}{1}
\begin{figure*}[!t]
    \centering
    % \vskip 0.2in
    \includegraphics[width=0.80\linewidth]{figures/framework.png}
    %\vspace{-5pt}
    \caption{\textbf{Framework of our approach.} Playmate is a two-stage training framework that leverages a 3D-Implicit Space Guided Diffusion Model to generate lifelike talking faces. In the first stage, Playmate utilizes a motion-decoupled module to enhance attribute disentanglement accuracy and trains a diffusion transformer to generate motion sequences directly from audio cues. In the second stage, we use an emotion-control module to encode emotion control information into the latent space, enabling fine-grained control over emotions, thereby improving flexibility in controlling emotion and head pose.}
    \label{fig:framework}
    \vskip -0.1in
\end{figure*}