[
  {
    "index": 0,
    "papers": [
      {
        "key": "prajwal2020lip",
        "author": "Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV",
        "title": "A lip sync expert is all you need for speech to lip generation in the wild"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2023dinet",
        "author": "Zhang, Zhimeng and Hu, Zhipeng and Deng, Wenjin and Fan, Changjie and Lv, Tangjie and Ding, Yu",
        "title": "Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "cheng2022videoretalking",
        "author": "Cheng, Kun and Cun, Xiaodong and Zhang, Yong and Xia, Menghan and Yin, Fei and Zhu, Mingrui and Wang, Xuan and Wang, Jue and Wang, Nannan",
        "title": "Videoretalking: Audio-based lip synchronization for talking head video editing in the wild"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "pumarola2018ganimation",
        "author": "Pumarola, Albert and Agudo, Antonio and Martinez, Aleix M and Sanfeliu, Alberto and Moreno-Noguer, Francesc",
        "title": "Ganimation: Anatomically-aware facial animation from a single image"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ji2022eamm",
        "author": "Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun",
        "title": "Eamm: One-shot emotional talking face via audio-based emotion-aware motion model"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "zhang2023sadtalker",
        "author": "Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei",
        "title": "Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation"
      },
      {
        "key": "gan2023efficient",
        "author": "Gan, Yuan and Yang, Zongxin and Yue, Xihang and Sun, Lingyun and Yang, Yi",
        "title": "Efficient emotional adaptation for audio-driven talking-head generation"
      },
      {
        "key": "peng2024synctalk",
        "author": "Peng, Ziqiao and Hu, Wentao and Shi, Yue and Zhu, Xiangyu and Zhang, Xiaomei and Zhao, Hao and He, Jun and Liu, Hongyan and Fan, Zhaoxin",
        "title": "Synctalk: The devil is in the synchronization for talking head synthesis"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "bigioi2024speech",
        "author": "Bigioi, Dan and Basak, Shubhajit and Stypu{\\l}kowski, Micha{\\l} and Zieba, Maciej and Jordan, Hugh and McDonnell, Rachel and Corcoran, Peter",
        "title": "Speech driven video editing via an audio-conditioned diffusion model"
      },
      {
        "key": "mukhopadhyay2024diff2lip",
        "author": "Mukhopadhyay, Soumik and Suri, Saksham and Gadde, Ravi Teja and Shrivastava, Abhinav",
        "title": "Diff2lip: Audio conditioned diffusion models for lip-synchronization"
      },
      {
        "key": "shen2023difftalk",
        "author": "Shen, Shuai and Zhao, Wenliang and Meng, Zibin and Li, Wanhua and Zhu, Zheng and Zhou, Jie and Lu, Jiwen",
        "title": "Difftalk: Crafting diffusion models for generalized audio-driven portraits animation"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "tian2025emo",
        "author": "Tian, Linrui and Wang, Qi and Zhang, Bang and Bo, Liefeng",
        "title": "EMO: Emote Portrait Alive Generating Expressive Portrait Videos with Audio2Video Diffusion Model Under Weak Conditions"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu2024hallo",
        "author": "Xu, Mingwang and Li, Hui and Su, Qingkun and Shang, Hanlin and Zhang, Liwei and Liu, Ce and Wang, Jingdong and Yao, Yao and Zhu, Siyu",
        "title": "Hallo: Hierarchical audio-driven visual synthesis for portrait image animation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2024echomimic",
        "author": "Chen, Zhiyuan and Cao, Jiajiong and Chen, Zhiquan and Li, Yuming and Ma, Chenguang",
        "title": "Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "jiang2024loopy",
        "author": "Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo",
        "title": "Loopy: Taming audio-driven portrait avatar with long-term motion dependency"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zheng2024memo",
        "author": "Zheng, Longtao and Zhang, Yifan and Guo, Hanzhong and Pan, Jiachun and Tan, Zhenxiong and Lu, Jiahao and Tang, Chuanxin and An, Bo and Yan, Shuicheng",
        "title": "MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "siarohin2019first",
        "author": "Siarohin, Aliaksandr and Lathuili{\\`e}re, St{\\'e}phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu",
        "title": "First order motion model for image animation"
      },
      {
        "key": "ren2021pirenderer",
        "author": "Ren, Yurui and Li, Ge and Chen, Yuanqi and Li, Thomas H and Liu, Shan",
        "title": "Pirenderer: Controllable portrait image generation via semantic neural rendering"
      },
      {
        "key": "li2017learning",
        "author": "Li, Tianye and Bolkart, Timo and Black, Michael J and Li, Hao and Romero, Javier",
        "title": "Learning a model of facial shape and expression from 4D scans."
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhang2023sadtalker",
        "author": "Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei",
        "title": "Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "xu2024vasa",
        "author": "Xu, Sicheng and Chen, Guojun and Guo, Yu-Xiao and Yang, Jiaolong and Li, Chong and Zang, Zhenyu and Zhang, Yizhong and Tong, Xin and Guo, Baining",
        "title": "Vasa-1: Lifelike audio-driven talking faces generated in real time"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "lin2024takin",
        "author": "Lin, Bin and Yu, Yanzhen and Ye, Jianhao and Lv, Ruitao and Yang, Yuguang and Xie, Ruoye and Yu, Pan and Zhou, Hongbin",
        "title": "Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical and Landmark Loss Optimization"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ma2023dreamtalk",
        "author": "Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong",
        "title": "Dreamtalk: When expressive talking head generation meets diffusion probabilistic models"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "cao2024joyvasa",
        "author": "Cao, Xuyang and Shi, Sheng and Zhao, Jun and Yao, Yang and Fei, Jintao and Gao, Minyu and Wang, Guoxin",
        "title": "JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation"
      }
    ]
  }
]