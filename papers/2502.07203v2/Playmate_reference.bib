%% survey
@article{xue2024human,
  title={Human motion video generation: A survey},
  author={Xue, Haiwei and Luo, Xiangyang and Hu, Zhanghao and Zhang, Xin and Xiang, Xunzhi and Dai, Yuqin and Liu, Jianzhuang and Zhang, Zhensong and Li, Minglei and Yang, Jian and others},
  journal={Authorea Preprints},
  year={2024},
  publisher={Authorea}
}

@article{jiang2024audio,
  title={Audio-Driven Facial Animation with Deep Learning: A Survey},
  author={Jiang, Diqiong and Chang, Jian and You, Lihua and Bian, Shaojun and Kosk, Robert and Maguire, Greg},
  journal={Information},
  volume={15},
  number={11},
  pages={675},
  year={2024},
  publisher={MDPI}
}


%% diffusion model
@article{ho2020denoising,
  title={Denoising diffusion probabilistic models},
  author={Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={6840--6851},
  year={2020}
}  % ddpm

@article{song2020denoising,
  title={Denoising diffusion implicit models},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  journal={arXiv preprint arXiv:2010.02502},
  year={2020}
}  % ddim

@article{Dhariwal_Nichol_2021,  
  title={Diffusion Models Beat GANs on Image Synthesis}, 
  journal={Neural Information Processing Systems,Neural Information Processing Systems}, 
  author={Dhariwal, Prafulla and Nichol, AlexanderQuinn}, 
  year={2021}, 
  month={Dec}, 
  language={en-US} 
}  % ADM

@inproceedings{rombach2022high,
  title={High-resolution image synthesis with latent diffusion models},
  author={Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10684--10695},
  year={2022}
}  % stable diffusion

@inproceedings{peebles2023scalable,
  title={Scalable diffusion models with transformers},
  author={Peebles, William and Xie, Saining},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={4195--4205},
  year={2023}
}  % DiT


% infrastructure model
@article{gulati2020conformer,
  title={Conformer: Convolution-augmented transformer for speech recognition},
  author={Gulati, Anmol and Qin, James and Chiu, Chung-Cheng and Parmar, Niki and Zhang, Yu and Yu, Jiahui and Han, Wei and Wang, Shibo and Zhang, Zhengdong and Wu, Yonghui and others},
  journal={arXiv preprint arXiv:2005.08100},
  year={2020}
}  % Conformer


%% speech representation
@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}  % wav2vec2

@article{hsu2021hubert,
  title={Hubert: Self-supervised speech representation learning by masked prediction of hidden units},
  author={Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
  journal={IEEE/ACM transactions on audio, speech, and language processing},
  volume={29},
  pages={3451--3460},
  year={2021},
  publisher={IEEE}
}  % hubert


%% cfg
@article{ho2022classifier,
  title={Classifier-free diffusion guidance},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint arXiv:2207.12598},
  year={2022}
}  # cfg

@inproceedings{brooks2023instructpix2pix,
  title={Instructpix2pix: Learning to follow image editing instructions},
  author={Brooks, Tim and Holynski, Aleksander and Efros, Alexei A},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18392--18402},
  year={2023}
}

%% GAN-base audio-driven face animation
@inproceedings{chen2018lip,
  title={Lip movements generation at a glance},
  author={Chen, Lele and Li, Zhiheng and Maddox, Ross K and Duan, Zhiyao and Xu, Chenliang},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={520--535},
  year={2018}
}  

@inproceedings{prajwal2020lip,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={484--492},
  year={2020}
}  % wav2lip

@article{lu2021live,
  title={Live speech portraits: real-time photorealistic talking-head animation},
  author={Lu, Yuanxun and Chai, Jinxiang and Cao, Xun},
  journal={ACM Transactions on Graphics (ToG)},
  volume={40},
  number={6},
  pages={1--17},
  year={2021},
  publisher={ACM New York, NY, USA}
}  % lsp

@inproceedings{zhang2023dinet,
  title={Dinet: Deformation inpainting network for realistic face visually dubbing on high resolution video},
  author={Zhang, Zhimeng and Hu, Zhipeng and Deng, Wenjin and Fan, Changjie and Lv, Tangjie and Ding, Yu},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={3},
  pages={3543--3551},
  year={2023}
}  % Dinet

@inproceedings{zhang2023sadtalker,
  title={Sadtalker: Learning realistic 3d motion coefficients for stylized audio-driven single image talking face animation},
  author={Zhang, Wenxuan and Cun, Xiaodong and Wang, Xuan and Zhang, Yong and Shen, Xi and Guo, Yu and Shan, Ying and Wang, Fei},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={8652--8661},
  year={2023}
}  % sadtalker

@inproceedings{ma2023styletalk,
  title={Styletalk: One-shot talking head generation with controllable speaking styles},
  author={Ma, Yifeng and Wang, Suzhen and Hu, Zhipeng and Fan, Changjie and Lv, Tangjie and Ding, Yu and Deng, Zhidong and Yu, Xin},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={2},
  pages={1896--1904},
  year={2023}
}  % styletalk

@inproceedings{tan2024flowvqtalker,
  title={FlowVQTalker: High-Quality Emotional Talking Face Generation through Normalizing Flow and Quantization},
  author={Tan, Shuai and Ji, Bin and Pan, Ye},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={26317--26327},
  year={2024}
}  % FlowVQTalker

@inproceedings{ki2023stylelipsync,
  title={StyleLipSync: Style-based personalized lip-sync video generation},
  author={Ki, Taekyung and Min, Dongchan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22841--22850},
  year={2023}
}  % StyleLipSync

@inproceedings{cheng2022videoretalking,
  title={Videoretalking: Audio-based lip synchronization for talking head video editing in the wild},
  author={Cheng, Kun and Cun, Xiaodong and Zhang, Yong and Xia, Menghan and Yin, Fei and Zhu, Mingrui and Wang, Xuan and Wang, Jue and Wang, Nannan},
  booktitle={SIGGRAPH Asia 2022 Conference Papers},
  pages={1--9},
  year={2022}
}  % videoretalking

@inproceedings{pumarola2018ganimation,
  title={Ganimation: Anatomically-aware facial animation from a single image},
  author={Pumarola, Albert and Agudo, Antonio and Martinez, Aleix M and Sanfeliu, Alberto and Moreno-Noguer, Francesc},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={818--833},
  year={2018}
}  % Ganimation

@inproceedings{ji2022eamm,
  title={Eamm: One-shot emotional talking face via audio-based emotion-aware motion model},
  author={Ji, Xinya and Zhou, Hang and Wang, Kaisiyuan and Wu, Qianyi and Wu, Wayne and Xu, Feng and Cao, Xun},
  booktitle={ACM SIGGRAPH 2022 Conference Proceedings},
  pages={1--10},
  year={2022}
}  % EAMM

@inproceedings{gan2023efficient,
  title={Efficient emotional adaptation for audio-driven talking-head generation},
  author={Gan, Yuan and Yang, Zongxin and Yue, Xihang and Sun, Lingyun and Yang, Yi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={22634--22645},
  year={2023}
}  % EAT

@inproceedings{peng2024synctalk,
  title={Synctalk: The devil is in the synchronization for talking head synthesis},
  author={Peng, Ziqiao and Hu, Wentao and Shi, Yue and Zhu, Xiangyu and Zhang, Xiaomei and Zhao, Hao and He, Jun and Liu, Hongyan and Fan, Zhaoxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={666--676},
  year={2024}
}  % Synctalk


%% Nerf-based audio-driven face animation
@inproceedings{guo2021ad,
  title={Ad-nerf: Audio driven neural radiance fields for talking head synthesis},
  author={Guo, Yudong and Chen, Keyu and Liang, Sen and Liu, Yong-Jin and Bao, Hujun and Zhang, Juyong},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={5784--5794},
  year={2021}
}  % AD-Nerf

@article{wang2024expression,
  title={Expression-aware neural radiance fields for high-fidelity talking portrait synthesis},
  author={Wang, Xueping and Ruan, Tao and Xu, Jun and Guo, Xueni and Li, Jiahe and Yan, Feihu and Zhao, Guangzhe and Wang, Caiyong},
  journal={Image and Vision Computing},
  volume={147},
  pages={105075},
  year={2024},
  publisher={Elsevier}
}  % ER-Nerf

@article{ye2023geneface++,
  title={Geneface++: Generalized and stable real-time audio-driven 3d talking face generation},
  author={Ye, Zhenhui and He, Jinzheng and Jiang, Ziyue and Huang, Rongjie and Huang, Jiawei and Liu, Jinglin and Ren, Yi and Yin, Xiang and Ma, Zejun and Zhao, Zhou},
  journal={arXiv preprint arXiv:2305.00787},
  year={2023}
}  % Geneface++


%% diffusion-based audio-driven portrait animation
@article{bigioi2024speech,
  title={Speech driven video editing via an audio-conditioned diffusion model},
  author={Bigioi, Dan and Basak, Shubhajit and Stypu{\l}kowski, Micha{\l} and Zieba, Maciej and Jordan, Hugh and McDonnell, Rachel and Corcoran, Peter},
  journal={Image and Vision Computing},
  volume={142},
  pages={104911},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{mukhopadhyay2024diff2lip,
  title={Diff2lip: Audio conditioned diffusion models for lip-synchronization},
  author={Mukhopadhyay, Soumik and Suri, Saksham and Gadde, Ravi Teja and Shrivastava, Abhinav},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={5292--5302},
  year={2024}
}  % Diff2lip

@inproceedings{shen2023difftalk,
  title={Difftalk: Crafting diffusion models for generalized audio-driven portraits animation},
  author={Shen, Shuai and Zhao, Wenliang and Meng, Zibin and Li, Wanhua and Zhu, Zheng and Zhou, Jie and Lu, Jiwen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1982--1991},
  year={2023}
}  % Difftalk

@article{sun2023vividtalk,
  title={Vividtalk: One-shot audio-driven talking head generation based on 3d hybrid prior},
  author={Sun, Xusen and Zhang, Longhao and Zhu, Hao and Zhang, Peng and Zhang, Bang and Ji, Xinya and Zhou, Kangneng and Gao, Daiheng and Bo, Liefeng and Cao, Xun},
  journal={arXiv preprint arXiv:2312.01841},
  year={2023}
}  % Vidvidtalk

@inproceedings{tian2025emo,
  title={EMO: Emote Portrait Alive Generating Expressive Portrait Videos with Audio2Video Diffusion Model Under Weak Conditions},
  author={Tian, Linrui and Wang, Qi and Zhang, Bang and Bo, Liefeng},
  booktitle={European Conference on Computer Vision},
  pages={244--260},
  year={2025},
  organization={Springer}
}  % EMO

@article{he2023gaia,
  title={Gaia: Zero-shot talking avatar generation},
  author={He, Tianyu and Guo, Junliang and Yu, Runyi and Wang, Yuchi and Zhu, Jialiang and An, Kaikai and Li, Leyi and Tan, Xu and Wang, Chunyu and Hu, Han and others},
  journal={arXiv preprint arXiv:2311.15230},
  year={2023}
}  % Gaia

@article{xu2024vasa,
  title={Vasa-1: Lifelike audio-driven talking faces generated in real time},
  author={Xu, Sicheng and Chen, Guojun and Guo, Yu-Xiao and Yang, Jiaolong and Li, Chong and Zang, Zhenyu and Zhang, Yizhong and Tong, Xin and Guo, Baining},
  journal={arXiv preprint arXiv:2404.10667},
  year={2024}
}  Vasa-1

@article{xu2024hallo,
  title={Hallo: Hierarchical audio-driven visual synthesis for portrait image animation},
  author={Xu, Mingwang and Li, Hui and Su, Qingkun and Shang, Hanlin and Zhang, Liwei and Liu, Ce and Wang, Jingdong and Yao, Yao and Zhu, Siyu},
  journal={arXiv preprint arXiv:2406.08801},
  year={2024}
}  % Hallo

@article{cui2024hallo2,
  title={Hallo2: Long-duration and high-resolution audio-driven portrait image animation},
  author={Cui, Jiahao and Li, Hui and Yao, Yao and Zhu, Hao and Shang, Hanlin and Cheng, Kaihui and Zhou, Hang and Zhu, Siyu and Wang, Jingdong},
  journal={arXiv preprint arXiv:2410.07718},
  year={2024}
}  % Hallo2

@article{zheng2024memo,
  title={MEMO: Memory-Guided Diffusion for Expressive Talking Video Generation},
  author={Zheng, Longtao and Zhang, Yifan and Guo, Hanzhong and Pan, Jiachun and Tan, Zhenxiong and Lu, Jiahao and Tang, Chuanxin and An, Bo and Yan, Shuicheng},
  journal={arXiv preprint arXiv:2412.04448},
  year={2024}
}  % MEMO

@article{cao2024joyvasa,
  title={JoyVASA: Portrait and Animal Image Animation with Diffusion-Based Audio-Driven Facial Dynamics and Head Motion Generation},
  author={Cao, Xuyang and Shi, Sheng and Zhao, Jun and Yao, Yang and Fei, Jintao and Gao, Minyu and Wang, Guoxin},
  journal={arXiv preprint arXiv:2411.09209},
  year={2024}
}  % JoyVASA

@article{ji2024sonic,
  title={Sonic: Shifting Focus to Global Audio Perception in Portrait Animation},
  author={Ji, Xiaozhong and Hu, Xiaobin and Xu, Zhihong and Zhu, Junwei and Lin, Chuming and He, Qingdong and Zhang, Jiangning and Luo, Donghao and Chen, Yi and Lin, Qin and others},
  journal={arXiv preprint arXiv:2411.16331},
  year={2024}
}  % sonic

@article{ma2023dreamtalk,
  title={Dreamtalk: When expressive talking head generation meets diffusion probabilistic models},
  author={Ma, Yifeng and Zhang, Shiwei and Wang, Jiayu and Wang, Xiang and Zhang, Yingya and Deng, Zhidong},
  journal={arXiv e-prints},
  pages={arXiv--2312},
  year={2023}
}  % dreamtalk

@article{chen2024echomimic,
  title={Echomimic: Lifelike audio-driven portrait animations through editable landmark conditions},
  author={Chen, Zhiyuan and Cao, Jiajiong and Chen, Zhiquan and Li, Yuming and Ma, Chenguang},
  journal={arXiv preprint arXiv:2407.08136},
  year={2024}
}  % Echomimic

@article{lin2024takin,
  title={Takin-ADA: Emotion Controllable Audio-Driven Animation with Canonical and Landmark Loss Optimization},
  author={Lin, Bin and Yu, Yanzhen and Ye, Jianhao and Lv, Ruitao and Yang, Yuguang and Xie, Ruoye and Yu, Pan and Zhou, Hongbin},
  journal={arXiv preprint arXiv:2410.14283},
  year={2024}
}  % Takin-ADA

@inproceedings{liu2024anitalker,
  title={Anitalker: animate vivid and diverse talking faces through identity-decoupled facial motion encoding},
  author={Liu, Tao and Chen, Feilong and Fan, Shuai and Du, Chenpeng and Chen, Qi and Chen, Xie and Yu, Kai},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={6696--6705},
  year={2024}
}  % Anitalker

@article{jiang2024loopy,
  title={Loopy: Taming audio-driven portrait avatar with long-term motion dependency},
  author={Jiang, Jianwen and Liang, Chao and Yang, Jiaqi and Lin, Gaojie and Zhong, Tianyun and Zheng, Yanbo},
  journal={arXiv preprint arXiv:2409.02634},
  year={2024}
}  % Loopy


%% diffusion-based audio-driven 3d head animation
@article{sun2024diffposetalk,
  title={Diffposetalk: Speech-driven stylistic 3d facial animation and head pose generation via diffusion models},
  author={Sun, Zhiyao and Lv, Tian and Ye, Sheng and Lin, Matthieu and Sheng, Jenny and Wen, Yu-Hui and Yu, Minjing and Liu, Yong-jin},
  journal={ACM Transactions on Graphics (TOG)},
  volume={43},
  number={4},
  pages={1--9},
  year={2024},
  publisher={ACM New York, NY, USA}
}  % Diffposetalk


%% image generation
@article{podell2023sdxl,
  title={Sdxl: Improving latent diffusion models for high-resolution image synthesis},
  author={Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and M{\"u}ller, Jonas and Penna, Joe and Rombach, Robin},
  journal={arXiv preprint arXiv:2307.01952},
  year={2023}
}  % Sdxl

@inproceedings{esser2024scaling,
  title={Scaling rectified flow transformers for high-resolution image synthesis},
  author={Esser, Patrick and Kulal, Sumith and Blattmann, Andreas and Entezari, Rahim and M{\"u}ller, Jonas and Saini, Harry and Levi, Yam and Lorenz, Dominik and Sauer, Axel and Boesel, Frederic and others},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024}
}  % sd3.0


%% video generation
@article{liu2024sora,
  title={Sora: A review on background, technology, limitations, and opportunities of large vision models},
  author={Liu, Yixin and Zhang, Kai and Li, Yuan and Yan, Zhiling and Gao, Chujie and Chen, Ruoxi and Yuan, Zhengqing and Huang, Yue and Sun, Hanchi and Gao, Jianfeng and others},
  journal={arXiv preprint arXiv:2402.17177},
  year={2024}
}  % sora

@article{lin2024open,
  title={Open-sora plan: Open-source large video generation model},
  author={Lin, Bin and Ge, Yunyang and Cheng, Xinhua and Li, Zongjian and Zhu, Bin and Wang, Shaodong and He, Xianyi and Ye, Yang and Yuan, Shenghai and Chen, Liuhan and others},
  journal={arXiv preprint arXiv:2412.00131},
  year={2024}
}  % Open-sora

@article{kong2024hunyuanvideo,
  title={HunyuanVideo: A Systematic Framework For Large Video Generative Models},
  author={Kong, Weijie and Tian, Qi and Zhang, Zijian and Min, Rox and Dai, Zuozhuo and Zhou, Jin and Xiong, Jiangfeng and Li, Xin and Wu, Bo and Zhang, Jianwei and others},
  journal={arXiv preprint arXiv:2412.03603},
  year={2024}
}  % HunyuanVideo

@article{hong2022cogvideo,
  title={Cogvideo: Large-scale pretraining for text-to-video generation via transformers},
  author={Hong, Wenyi and Ding, Ming and Zheng, Wendi and Liu, Xinghan and Tang, Jie},
  journal={arXiv preprint arXiv:2205.15868},
  year={2022}
}  % Cogvideo

@article{yang2024cogvideox,
  title={Cogvideox: Text-to-video diffusion models with an expert transformer},
  author={Yang, Zhuoyi and Teng, Jiayan and Zheng, Wendi and Ding, Ming and Huang, Shiyu and Xu, Jiazheng and Yang, Yuanming and Hong, Wenyi and Zhang, Xiaohan and Feng, Guanyu and others},
  journal={arXiv preprint arXiv:2408.06072},
  year={2024}
}  % Cogvideox


%% portrait animation
@article{li2017learning,
  title={Learning a model of facial shape and expression from 4D scans.},
  author={Li, Tianye and Bolkart, Timo and Black, Michael J and Li, Hao and Romero, Javier},
  journal={ACM Trans. Graph.},
  volume={36},
  number={6},
  pages={194--1},
  year={2017}
}  % flame

@article{siarohin2019first,
  title={First order motion model for image animation},
  author={Siarohin, Aliaksandr and Lathuili{\`e}re, St{\'e}phane and Tulyakov, Sergey and Ricci, Elisa and Sebe, Nicu},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}  % fomm

@inproceedings{ren2021pirenderer,
  title={Pirenderer: Controllable portrait image generation via semantic neural rendering},
  author={Ren, Yurui and Li, Ge and Chen, Yuanqi and Li, Thomas H and Liu, Shan},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={13759--13768},
  year={2021}
}  % Pirenderer

@inproceedings{wang2021one,
  title={One-shot free-view neural talking-head synthesis for video conferencing},
  author={Wang, Ting-Chun and Mallya, Arun and Liu, Ming-Yu},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={10039--10049},
  year={2021}
}  % face vid2vid

@article{guo2024liveportrait,
  title={Liveportrait: Efficient portrait animation with stitching and retargeting control},
  author={Guo, Jianzhu and Zhang, Dingyun and Liu, Xiaoqiang and Zhong, Zhizhou and Zhang, Yuan and Wan, Pengfei and Zhang, Di},
  journal={arXiv preprint arXiv:2407.03168},
  year={2024}
}  % liveportrait

@inproceedings{drobyshev2022megaportraits,
  title={Megaportraits: One-shot megapixel neural head avatars},
  author={Drobyshev, Nikita and Chelishev, Jenya and Khakhulin, Taras and Ivakhnenko, Aleksei and Lempitsky, Victor and Zakharov, Egor},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={2663--2671},
  year={2022}
}


%% dataset
@article{ephrat2018looking,
  title={Looking to listen at the cocktail party: A speaker-independent audio-visual model for speech separation},
  author={Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T and Rubinstein, Michael},
  journal={arXiv preprint arXiv:1804.03619},
  year={2018}
}  % avspeech
`
@inproceedings{wang2020mead,
  title={Mead: A large-scale audio-visual dataset for emotional talking-face generation},
  author={Wang, Kaisiyuan and Wu, Qianyi and Song, Linsen and Yang, Zhuoqian and Wu, Wayne and Qian, Chen and He, Ran and Qiao, Yu and Loy, Chen Change},
  booktitle={European Conference on Computer Vision},
  pages={700--717},
  year={2020},
  organization={Springer}
}  % MEAD

@inproceedings{zhang2021flow,
  title={Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset},
  author={Zhang, Zhimeng and Li, Lincheng and Ding, Yu and Fan, Changjie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3661--3670},
  year={2021}
}  % HDTF

@inproceedings{xie2022vfhq,
  title={Vfhq: A high-quality dataset and benchmark for video face super-resolution},
  author={Xie, Liangbin and Wang, Xintao and Zhang, Honglun and Dong, Chao and Shan, Ying},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={657--666},
  year={2022}
}  % VFHQ

@inproceedings{yu2023celebv,
  title={Celebv-text: A large-scale facial text-video dataset},
  author={Yu, Jianhui and Zhu, Hao and Jiang, Liming and Loy, Chen Change and Cai, Weidong and Wu, Wayne},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14805--14814},
  year={2023}
}  % Celebv-text

@article{montesinos2021cappella,
  title={A cappella: Audio-visual singing voice separation},
  author={Montesinos, Juan F and Kadandale, Venkatesh S and Haro, Gloria},
  journal={arXiv preprint arXiv:2104.09946},
  year={2021}
}  % Acappella

@inproceedings{liu2022mafw,
  title={Mafw: A large-scale, multi-modal, compound affective database for dynamic facial expression recognition in the wild},
  author={Liu, Yuanyuan and Dai, Wei and Feng, Chuanxu and Wang, Wenbin and Yin, Guanghao and Zeng, Jiabei and Shan, Shiguang},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={24--32},
  year={2022}
}  % MAFW


%% Evaluation metrics
@inproceedings{chung2017out,
  title={Out of time: automated lip sync in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Computer Vision--ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13},
  pages={251--263},
  year={2017},
  organization={Springer}
}  % Sync-C Sync-D

@article{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}  % FID

@article{unterthiner2019fvd,
  title={FVD: A new metric for video generation},
  author={Unterthiner, Thomas and van Steenkiste, Sjoerd and Kurach, Karol and Marinier, Rapha{\"e}l and Michalski, Marcin and Gelly, Sylvain},
  year={2019}
}  % FVD

@inproceedings{zhang2018unreasonable,
  title={The unreasonable effectiveness of deep features as a perceptual metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={586--595},
  year={2018}
}  % LPIPS


%% fundamental loss
@article{simonyan2014very,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}  % vgg-19

@inproceedings{johnson2016perceptual,
  title={Perceptual losses for real-time style transfer and super-resolution},
  author={Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
  booktitle={Computer Vision--ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14},
  pages={694--711},
  year={2016},
  organization={Springer}
}  % Perceptual loss


%% insightface
@inproceedings{deng2020retinaface,
  title={Retinaface: Single-shot multi-level face localisation in the wild},
  author={Deng, Jiankang and Guo, Jia and Ververas, Evangelos and Kotsia, Irene and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={5203--5212},
  year={2020}
}  % Retinaface

@article{guo2021sample,
  title={Sample and computation redistribution for efficient face detection},
  author={Guo, Jia and Deng, Jiankang and Lattas, Alexandros and Zafeiriou, Stefanos},
  journal={arXiv preprint arXiv:2105.04714},
  year={2021}
}

@inproceedings{deng2019arcface,
  title={Arcface: Additive angular margin loss for deep face recognition},
  author={Deng, Jiankang and Guo, Jia and Xue, Niannan and Zafeiriou, Stefanos},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4690--4699},
  year={2019}
}  % arcface


%% others
@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}  % adam

@inproceedings{savchenko2023facial,
  title={Facial Expression Recognition with Adaptive Frame Rate based on Multiple Testing Correction},
  author={Savchenko, Andrey},
  booktitle={Proceedings of the 40th International Conference on Machine Learning (ICML)},
  pages={30119--30129},
  year={2023},
  editor={Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume={202},
  series={Proceedings of Machine Learning Research},
  month={23--29 Jul},
  publisher={PMLR},
  url={https://proceedings.mlr.press/v202/savchenko23a.html}
}  % emo-A