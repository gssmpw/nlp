\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
\usepackage[accepted]{icml2025}
% \everypar{\looseness=-1}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
% \usepackage[preprint]{neurips_2024}
\everypar{\looseness=-1}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage{amsmath}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{cleveref}
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{comment}
\input{macros.tex}
\usepackage{enumitem}
\setenumerate{partopsep=0cm, topsep=0cm}
\usepackage{enumitem}
\usepackage{subcaption}
\usepackage{graphicx}

\usepackage{amsthm}
\newtheorem{assumption}{Assumption}
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary
}{Corollary
}
\newtheorem{definition}{Definition}
\newtheorem{remark}{Remark}

\newcommand{\alex}[1]{{\scriptsize\textbf{\color{red} AK: #1}}}
\newcommand{\petr}[1]{{\scriptsize\textbf{\color{magenta} PM: #1}}}

\usepackage{xcolor}

\newcommand{\new}[1]{{\color{orange} #1}}
\newcommand{\newpetr}[1]{{\color{magenta} #1}}



% \title{High Probability Bounds




%\author{Nikita Kornilov\\
%  MIPT, Skoltech\\
%  \texttt{kornilov.nm@phystech.edu} 
%}


\begin{document}

\twocolumn[
\icmltitle{Sign Operator for Coping with Heavy-Tailed Noise: High Probability Convergence Bounds with Extensions to Distributed Optimization and Comparison Oracle}

\begin{icmlauthorlist}
\icmlauthor{Nikita Kornilov}{MIPT,Skoltech}
\icmlauthor{Philip Zmushko}{Yandex,MIPT}
\icmlauthor{Andrei Semenov}{EPFL}
\icmlauthor{Alexander Gasnikov}{MIPT,Skoltech,IU}
\icmlauthor{Alexander Beznosikov}{MIPT}

%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{MIPT}{Moscow Institute of Physics and Technology}
\icmlaffiliation{Skoltech}{Skolkovo Institute of Science and Technology}
\icmlaffiliation{EPFL}{Machine Learning and Optimization Laboratory (MLO),
EPFL, Lausanne, Switzerland}
\icmlaffiliation{IU}{Innopolis University}
\icmlaffiliation{Yandex}{Yandex}

\icmlcorrespondingauthor{Nikita Kornilov}{kornilov.nm@phystech.edu}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

\vskip 0.3in ]
\printAffiliationsAndNotice{}

\begin{abstract}
The growing popularity of AI optimization problems involving severely corrupted data has increased the demand for methods capable of handling heavy-tailed noise, i.e., noise with bounded $\kappa$-th moment, $\kappa \in (1,2]$. For the widely used clipping technique, effectiveness heavily depends on the careful tuning of clipping levels throughout training. In this paper, we demonstrate that using only the sign of the input, without introducing additional hyperparameters, is sufficient to cope with heavy-tailed noise effectively. For smooth non-convex functions,  we prove that \algname{SignSGD} achieves optimal sample complexity $\tilde{O}\left(\varepsilon^{-\frac{3\kappa - 2}{\kappa - 1}}\right)$ with high probability for attaining an average gradient norm accuracy of $\varepsilon$. Under the assumption of symmetric noise, we use \algname{SignSGD} with Majority Voting to extend this bound to the distributed optimization or reduce the sample complexity to $\tilde{O}(\varepsilon^{-4})$ in the case of a single worker with arbitrary parameters.
Furthermore, we explore the application of the sign operator in zeroth-order optimization with an oracle that can only compare function values at two different points. We propose a novel method, \algname{MajorityVote-CompsSGD}, and provide the first-known high-probability bound $\tilde{O}(\varepsilon^{-6})$ for the number of comparisons under symmetric noise assumption. Our theoretical findings are supported by the superior performance of sign-based methods in training Large Language Models.
\end{abstract}
\section{Introduction}
\paragraph{Problem statement.} Consider the stochastic optimization problem of a smooth non-convex function $f:\R^d \to \R$:
\begin{eqnarray}
    \min\limits_{x \in \R^d} f(x) := \EE_{\xi \sim \mathcal{S}} [f(x, \xi)],\label{eq: min problem}
\end{eqnarray}
where random variable $\xi$ can only be sampled from an unknown distribution $\mathcal{S}.$ The gradient oracle gives unbiased gradient estimate $\nabla f (x, \xi) \in \R^d$. For example, in machine learning, $f(x, \xi)$ can be interpreted as a loss function on a sample $\xi$ \cite{shalev2014understanding}.

The most popular approach for solving \eqref{eq: min problem} is Stochastic Gradient Descent (\algname{SGD}) 
 \cite{robbins1951stochastic}:
\begin{equation}
    x^{k+1} = x^k - \gamma_k \cdot  g^k, \quad g^k := \nabla f (x^k, \xi^k). \notag \label{eq: sgd intro}
\end{equation}
For non-convex functions, the main goal of stochastic optimization is to find a point with small gradient norm.

Huge success of stochastic first-order methods in rapidly developing neural networks field \cite{bottou2012stochastic, kingma2014adam} has sparked numerous works studying \algname{SGD} under various assumptions on corrupting noise induced by the randomness $\xi$. The first bounds in expectation for the sample complexity were derived for sub-Gaussian noise \cite{nemirovski2009robust} and for noise with bounded variance (BV) \cite{ghadimi2013stochastic}. 

Due to expensive training  of large deep learning models \cite{davis2021low}, more informative \textit{high probability (HP)} bounds have gained even more attention than bounds in expectation describing methods behavior over several runs. HP bounds provide convergence guarantees which hold true with probability at least $1 - \delta, \delta \in (0,1).$ The bound in expectation can be reduced to the HP bound using the Markov's inequality, however, it leads to a dominant $\nicefrac{1}{\delta}$ factor. HP bounds for \algname{SGD} under Gaussian noise were obtained in \cite{li2020high} and had logarithmic $\log\nicefrac{1}{\delta}$ dependency. However, already under the BV noise, vanilla \algname{SGD} achieves only $\nicefrac{1}{\sqrt{\delta}}$ rate \cite{sadiev2023high}. Moreover, it was shown that the BV assumption can not describe loss functions in modern deep learning problems. In Transformer models, stochasticity tends to have a rather \textit{heavy-tailed (HT)} distribution \cite{simsekli2019tail, zhang2020adaptivegood, gurbuzbalaban2021heavy}. This means that the noise has bounded $\kappa$-th moment for some $\kappa \in (1,2]$, that is, $\EE_\xi[\| \nabla f (x, \xi) - \nabla f(x)\|_2^\kappa] \leq \sigma^\kappa$. The desire to obtain a better $\delta$-dependency and to consider HT noise in the HP bounds motivated the development of more robust methods. 

\subsection{Related works}
\paragraph{Clipping.} The idea of clipping the norm of gradient estimate before \algname{SGD} step demonstrates great empirical results \cite{pascanu2013difficulty, goodfellow2016deep} and helps achieve $\log \nicefrac{1}{\delta}$ dependency under BV assumption \cite{nazin2019algorithms, gorbunov2020stochastic}. 

The clipping operator is defined as $\clip(g^k, \lambda_k) := \min\{1, \nicefrac{\lambda_k}{\|g^k\|_2}\} \cdot g^k$ and can be applied not only 
non-convex minimization problems, but also to convex optimization, variational inequalities \cite{sadiev2023high}, non-smooth optimization \cite{zhang2020adaptivegood}, zeroth-order optimization \cite{kornilov2024accelerated}, robust aggregation \cite{karimireddy2021learning}, distributed optimization \cite{liu2022communication, qin2025high} and ensuring differential privacy \cite{andrew2021differentially}. %The first modification of \algname{SGD} which helps improve convergence and obtain high probability bounds under BV and heavy-tailed noise is \algname{ClipSGD} \cite{sadiev2023high, nguyen2023high, nguyen2023improved}. 

Let us list the latest results on the HP convergence of \algname{SGD} with clipping, called \algname{ClipSGD}, under the HT noise assumption. First, for non-convex functions, the authors of \cite{zhang2020adaptivegood} proved lower bounds for sample complexity in expectation. As shown in \cite{sadiev2023high}, \algname{ClipSGD} with proper clipping levels and stepsizes achieves this lower bound with extra logarithmic factors on $\delta$ and accuracy. In a number of works, the authors relax the HT assumption and consider only symmetric noise. This relaxation allows them to eliminate the dependency on $\kappa$ and break the actual lower bounds. For example, in \cite{puchkin2024breaking},  the authors used the coordinate-wise median operator, which requires only a few noise samples to lighten its distribution so it becomes BV. \cite{puchkin2024breaking} proved convergence of \algname{ClipSGD} combined with the median operator for (strongly) convex functions as if $\kappa = 2$. %For close-to-symmetric noises, the authors use median of means and obtain the same results. 
%Similar suboptimal bound  $\tilde{\Omega} \left( \varepsilon^{-\frac{\kappa}{\kappa - 1} }\right)$  for strongly functions is attained as well. 

Despite clipping's effectiveness, it requires careful tuning of clipping levels whose optimal values depend on the iteration number and all characteristics of objective function and HT noise \citep[Theorem $3.1$]{sadiev2023high}. Hence, the community has proposed other robust modifications of \algname{SGD}.   
 \vspace{-20pt}
\paragraph{Normalization.}
A natural relaxation of clipping with a profound level schedule is the permanent normalization of gradient estimate, i.e., $\text{norm}(g^k) := \frac{g^k}{\|g^k\|_2}.$ \algname{SGD} with additional normalization is called \algname{NSGD} \cite{hazan2015beyond}.


In early works devoted to \algname{NSGD}, only BV noise and bounds in expectation were considered \cite{barakat2023reinforcement, yang2024two}. In \cite{liu2023breaking, cutkosky2021high}, normalization was combined with clipping, which helped to cope with HT noise and obtain HP bounds.

Recently, the HP convergence of vanilla \algname{NSGD} was proved under HT noise in \cite{hubler2024gradient}. The authors showed that the complexity of \algname{NSGD} exactly matches the lower bound before mentioned from \cite{zhang2020adaptivegood} without logarithmic accuracy factors and only with mild $\log\nicefrac{1}{\delta}$ dependency. Moreover, in experiments with sequence labeling via LSTM Language Models \cite{merity2017regularizing}, normalization demonstrated better results than tuned clipping.  

Unlike \algname{ClipSGD}, \algname{NSGD} requires large batch sizes for robust convergence. It can be fixed by replacing batching with momentum techniques, which keeps the same sample complexity \cite{cutkosky2020momentum}. However, for methods with momentum, the convergence bounds are usually proved in expectation, and the HP bounds with logarithmic $\delta$ dependency have not yet been obtained. In experiments with VB noise, one can observe super-linear dependency on $\log \frac{1}{\delta}$ \cite{hubler2024gradient}.

\paragraph{Sign operator.} There is one more promising modification of \algname{SGD} which behavior under heavy-tailed noise has not yet been studied. Originally proposed in \cite{bernstein2018signsgd} for distributed optimization, \algname{SignSGD} takes only a sign of each coordinate of gradient estimate $$x^{k+1} = x^k - \gamma_k \cdot \sign(g^k).$$ There is one peculiarity in bounds for sign-based methods: they are proved w.r.t. the $\ell_1$-norm instead of smaller $\ell_2$-norm. As a consequence, additional $d$ dependent factors appear.  
Under BV noise, \algname{SignSGD} achieves optimal sample complexity in expectation (up to $d$ factors). Similar to \algname{NSGD}, \algname{SignSGD} requires aggressive batching which can be substituted with momentum \cite{sun2023momentum}. The alternative solution is to add error feedback mechanism that additionally fixes the biased nature of sign operator \cite{seide20141, karimireddy2019error}.

The main motivation for the original \algname{SignSGD} was communication effectiveness and empirical robustness in distributed optimization \cite{bernstein2018majorityvote}, since sending sign vector costs $O(d)$ operations. In theory, the effectiveness was proved only under additional assumptions on noise, e.g., symmetry and unimodality. Other applications and expansions of \algname{SignSGD} are as follows: 
\cite{safaryan2021stochastic} proposed an updated theory for a wider class of noises in the distributed setup, \cite{liu2019signsgd} generalized \algname{SignSGD} to zeroth-order oracle, \cite{jin2020stochastic} studied federated learning with compression.

For all the previously mentioned works, the results were obtained \textit{in expectation and only for BV noise}. The HP bounds were obtained only in \cite{armacki2023high, armacki2024large}, where the authors proposed a unified framework for the theoretical analysis of online non-linear \algname{SGD}. It includes a wide range of non-linear gradient estimation transformations such as \textit{clipping, normalization, and sign operator.} However, in these works, the HT assumption was relaxed to symmetric noises only. The authors proved HP bounds which are arbitrarily close to the optimal ones for BV noise. 
  

 

\subsection{Contributions.}
In our paper, we demonstrate that sign-based methods can handle heavy-tailed noise in zeroth- and first-order smooth non-convex optimization more effectively than clipping or normalization:
\begin{itemize}
    \item We prove \textit{the first high probability optimal bounds $\tilde{O} \left((\sqrt{d}/\varepsilon)^\frac{3\kappa - 2}{\kappa  -1}\right)$ under heavy-tailed noise} $\kappa \in (1,2]$  for \algname{SignSGD}  with mini-batching (Th. \ref{thm:minibatch SignSGD}). For \algname{SignSGD} with momentum, we generalize this bound in expectation (Th. \ref{thm:momentum SignSGD}). See Sections \ref{sec:minibatch sign sgd} and \ref{sec: MsignSGD}.  

    \item For symmetric heavy-tailed noise $\kappa \in (1,2]$, we combine \algname{SignSGD} with majority voting and achieve an exact high-probability bound $\tilde{O} \left((\sqrt{d}/\varepsilon)^4\right)$ for the sample complexity (Th. \ref{thm:com-sign conv}). See Section \ref{sec: signsggd with majority}.
%\item  and majority voting (Th.\ref{thm:com-sign conv})

    \item For the zeroth-order oracle which can only compare function values at two points, we propose a novel and simple \algname{MajorityVote-CompSGD} method. We prove \textit{the first high probability bound $\tilde{O}((d/\varepsilon^2)^3)$ for the comparison number under symmetric heavy-tailed noise} (Th. \ref{thm: MajorVote-SignSTP}). For \textit{sum-type functions}, we prove the bound $\tilde{O} \left((\sqrt{d}/\varepsilon)^\frac{4\kappa - 2}{\kappa  - 1}\right)$ for the number of function calls under \textit{any heavy-tailed noise}. See Section \ref{sec: compsgd majority}.

    %\item We expand our high probability results to strongly convex functions using restarts technique (Th. \ref{thm: restarts}).    

    \item To validate our findings in real-world scenarios with heavy-tailed noise, we evaluate the sign-based methods on Transformer models, demonstrating their effectiveness in both pre-training LLaMA 130M~\citep{llama} on C4~\citep{c4} dataset and zeroth-order  RoBERTa~\citep{roberta} fine-tuning on multiple NLP classification tasks. See Section \ref{sec:experiments}.
\end{itemize}
\paragraph{Notations.} The notation $\overline{1,n}$ is the set of natural numbers $\{1, 2, \dots, n\}$.   For a vector $x \in \R^d$, index $i \in \overline{1,d}$ returns its $i$-th coordinate $x = (x_1, \dots, x_d)$.  We define $\ell_p$-norm $p \in [1,+\infty]$ as $(\|x\|_p)^p := \sum_{i=1}^d |x_i|^p, x \in \R^d$.  The notation $\la x, y \ra := \sum_{i=1}^{d} x_i y_i$ stands for the standard scalar product for $x,y \in \R^d$. 

The sign operator $\sign(\cdot)$ returns the sign of a scalar input and can also be applied element-wise to a vector. The notation $\widetilde\cO$ hides the logarithmic factors in the probability of failure $\delta$.

 
%\paragraph{Paper oraganization.}

\section{High probability bounds for sign-based methods under heavy-tailed noise}\label{sec: sec_2}

In this section, we present our novel convergence guarantees with high probability for existing sign-based methods for non-convex functions with heavy-tailed noise in gradient estimates. For each algorithm, we provide an explicit optimal tuning for the parameters. If function's smoothness constant and noise's characteristics are not given, we state the rates for arbitrary tuning. All proofs are located in Appendix \ref{app: proofs}.

\subsection{Assumptions}
%We use standard assumptions from non-convex optimization and heavy-tailed noise.  
\begin{assumption}[Lower bound]\label{as: bounded}
    The objective function $f$ is lower bounded by $f^* > -\infty$, i.e., $f(x) \geq f^*, \forall x \in \R^d.$
\end{assumption}
\begin{assumption}[Smoothness]\label{as: smooth}
    The objective function $f$ is differentiable and $L$-smooth, i.e., for the positive constant $L$
    $$\|\nabla f(x) - \nabla f(y)\|_2 \leq L \|x-y\|_2, \quad \forall x, y \in \R^d. $$
\end{assumption}
\begin{assumption}[Heavy-tailed noise in gradient estimates]\label{as: pBCM}
    The unbiased estimate $\nabla f (x, \xi)$  has bounded $\kappa$-th moment $\kappa \in (1,2]$ for each coordinate, i.e., $\forall x \in \R^d$: 
    \begin{itemize}
        \item $\EE_\xi [\nabla f (x, \xi)] = \nabla f(x),$
        \item $\EE_\xi [|\nabla f (x, \xi)_i - \nabla f(x)_i|^\kappa] \leq \sigma_i^\kappa, i \in \overline{1,d},$
    \end{itemize}
    where $\Vec{\sigma} = [\sigma_1, \dots, \sigma_d]$ are non-negative constants.
    If $\kappa = 2$, then the noise is called a bounded variance. 
\end{assumption}




\subsection{\algname{SignSGD} and its HP convergence properties} 
We begin our analysis with the simplest of sign-based methods, namely \algname{SignSGD} (Alg. \ref{alg: signSGD}) and prove a general lemma on its convergence with high probability.
\begin{algorithm}[ht!]
\caption{\algname{SignSGD} }
\label{alg: signSGD}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample $\xi^k$ and compute estimate $g^k = \nabla f(x^k, \xi^k)$;
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot \sign(g^k)$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^T\}$ . 
\end{algorithmic}
\end{algorithm}
\begin{lemma}[\textbf{\algname{SignSGD} Convergence Lemma}] \label{lem: signsgd T update}
Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}) and HT gradient estimates (As. \ref{as: function noise xi}). Then Alg. \ref{alg: signSGD} after $T$ iterations with constant stepsizes $\gamma_k \equiv \gamma$ achieves with probability at least $1 - \delta$ starting with $\Delta_1 = f(x^1) - f^*$:
\begin{eqnarray}
\frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 &\leq& \frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta})  + 4 \|\Vec{\sigma}\|_1 \notag \\
       &+& 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}). \label{eq: signsgd convergence lemma}
\end{eqnarray}
\end{lemma}

From the bound \eqref{eq: signsgd convergence lemma}, one can derive the sample complexity for arbitrary parameters or calculate the optimal ones. In order to achieve accuracy $\varepsilon$, the noise $\|\Vec{\sigma}\|_1$ have not to exceed $\varepsilon$.  The first way to lower the noise is to use batching. 
\subsection{\algname{SignSGD} with batching}\label{sec:minibatch sign sgd}

\begin{algorithm}[ht!]
\caption{\algname{minibatch-SignSGD} }
\label{alg:minibatch-signSGD}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, batchsizes $\{B_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample $\{\xi^k_i\}_{i=1}^{B_k}$ and compute gradient estimate \\ $g^k = \sum_{i=1}^{B_k} \nicefrac{\nabla f(x^k, \xi^k_i)}{B_k}$;
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot \sign(g^k)$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\textbf{HP complexity for \algname{minibatch-SignSGD}}]\label{thm:minibatch SignSGD}
Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}) and HT gradient estimates (As. \ref{as: pBCM}). Then Alg. \ref{alg:minibatch-signSGD} requires the sample complexity $N$  to achieve $\frac{1}{T} \sum_{k=1}^{T}  \|\nabla f(x^k)\|_1 \leq \varepsilon$ with probability at least $1-\delta$ for:

\textbf{arbitrary tuning:}  $T, \gamma_k \equiv \frac{\gamma_0}{\sqrt{T}}, B_k \equiv \max \{1, B_0T\}$:
\begin{equation}
    N = O\left(\frac{B_0(\nicefrac{\Delta_1}{\gamma_0} + Ld \gamma_0)^4} {\varepsilon^4} + \frac{1}{B_0}\left(\frac{\|\Vec{\sigma}\|_1}{\varepsilon}\right)^\frac{2\kappa}{\kappa-1}\right), \label{eq: sign SGD arb}
\end{equation} 
\textbf{optimal tuning:} $T = O\left(\frac{\Delta_1L_\delta d }{\varepsilon^2}\right), \gamma_k \equiv \sqrt{\frac{\Delta_1}{8 L_\delta dT}} , B_k \equiv \max \left\{1, \left(\frac{16\|\Vec{\sigma}\|_1}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right\}:$ 
\begin{equation}
    N = O\left(\frac{\Delta_1L_\delta d}{\varepsilon^2} + \frac{\Delta_1 L_\delta d} {\varepsilon^2} 
 \left(\frac{\|\Vec{\sigma}\|_1}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right), \label{eq: sign SGD optimal}
\end{equation}
where $\Delta_1 = f(x^1) - f^*, L_\delta = L \log(\nicefrac{1}{\delta}).$
\end{theorem}

\subsection{\algname{SignSGD} with majority voting}\label{sec: signsggd with majority}
The second approach to noise reduction inherent to sign-based methods is majority voting.

\paragraph{Majority voting.} As mentioned above, the original motivation of \algname{SignSGD} was fast communication in distributed optimization \cite{bernstein2018majorityvote,jin2020stochastic}. Consider one server and $M$ workers, each of which computes its own gradient estimate. The server receives signs of all estimates, aggregates them, and sends the updated estimate back to the workers. In related works, various types of aggregation were studied, but the most effective one turned out to be majority voting. For sign vectors $\sign(g^{k}_i), i \in \overline{1,M}$, each coordinate of the resulting update vector is the majority of the received signs:
\begin{equation}
    g^k = \sign\left(\sum\limits_{i=1}^M \sign(g^{k}_i)\right).
\end{equation}
To be effective, majority voting must decrease the noise of the aggregated update vector. This is achieved via showing that probability
$\mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign\left[\sum\limits_{i=1}^M\sign(g^k_i)_j\right]\right] $  decreases with the growth of $M.$
However, for arbitrary noise distributions, it does not hold true. Choosing the most frequent value from the sign sequence $\{\sign(g^k_i)\}_{i=1}^M$ is actually $M$ Bernoulli trials. In these trials,  the probability of choosing a correct answer grows only if the probability of failure of a single worker is less than $\frac{1}{2}$, i.e.:
\begin{equation}\label{eq:prob leq 1/2}
    \mathbb{P}\left[\sign(\nabla f (x^k)) \neq \sign(g^k_i) \right] < \frac12, \forall i \in \overline{1,M}.
\end{equation}
For example, the condition \eqref{eq:prob leq 1/2} is satisfied if the noise of the gradient estimate for each coordinate is \textit{unimodal and symmetric about its mean}. We use this assumption in our paper, but other assumptions \cite{safaryan2021stochastic} leading to \eqref{eq:prob leq 1/2} are valid as well.
\begin{algorithm}[ht!]
\caption{\algname{MajorityVote-SignSGD} }
\label{alg:majorityvotesignSGDsingle}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^0 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, batchsizes $\{M_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample $\{\xi^k_i\}_{i=1}^{B_k}$ and  compute gradient estimate \\ $g^k = \sum_{i=1}^{M_k} \sign(\nabla f(x^k, \xi^k_i))$;

\STATE Set $x^{k+1} = x^k - \gamma_k \cdot\sign\left(g^k\right)$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}

%In this case, we can break lower bounds achieved by \algname{minibatch-SignSGD} and observe the rates as if noise is BV. For symmetric noise, majority voting is more effective for variance reduction than averaging. Moreover, majority voting allows working with oracles which give only signs of gradient estimates.


\begin{theorem}[\textbf{HP complexity for \algname{MajorityVote-SignSGD}}]\label{thm:com-sign conv}
  Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}) and the gradient estimates corrupted by \textbf{unimodal and
symmetric} HT noise (As. \ref{as: pBCM}). Then Alg. \ref{alg:majorityvotesignSGDsingle} requires the sample complexity $N$ to achieve $\frac{1}{T} \sum_{k=1}^{T}  \|\nabla f(x^k)\|_1 \leq \varepsilon$ with probability at least $1-\delta$ for:

\textbf{arbitrary tuning:}  $T, \gamma_k \equiv \frac{\gamma_0}{\sqrt{T}}, M_k \equiv \max\{1, M_0T\}$: 
\begin{eqnarray}
    N = O\left(\frac{M_0(\nicefrac{\Delta_1}{\gamma_0} + L_\delta d \gamma_0  + \nicefrac{a_\kappa \|\Vec{\sigma}\|_1}{\sqrt{M_0}})^4 }{\varepsilon^4}\right), \label{eq: majority signsgd arb}
\end{eqnarray}
\textbf{optimal tuning:} $T = O\left(\frac{\Delta_1L_\delta d }{\varepsilon^2}\right), \gamma_k \equiv \sqrt{\frac{\Delta_1}{8L_\delta d T }}, M_k \equiv \max\left\{1,   \left( 8a_\kappa\frac{\|\Vec{\sigma}\|_1}{\varepsilon}\right)^2\right\}:$ 
\begin{eqnarray}
    N = O\left(\frac{ \Delta_1 L_\delta d}{\varepsilon^2} +   \frac{ \Delta_1L_\delta d} { \varepsilon^2} 
 \left(\frac{a_\kappa\|\Vec{\sigma}\|_1}{ \varepsilon}\right)^2\right),\label{eq: majority signSGD optimal}
\end{eqnarray}
where $\Delta_1\!=\!f(x^1) - f^*, L_\delta\!=\!L \log \nicefrac{1}{\delta}, (a_\kappa)^\kappa := \frac{\kappa+1}{\kappa - 1}$.
\end{theorem}
%The second part of Theorem \ref{thm:com-sign conv} states that, for symmetric heavy-tailed noises, majority voting is more effective than batching in the undistributed setup. Indeed, instead of batching with batchsize $B \sim \nicefrac{1}{\varepsilon^\frac{\kappa}{\kappa-1}}$, one can look at the majority voting with $M \sim \nicefrac{1}{\varepsilon^2}$ voters as an alternative which achieves accuracy $\varepsilon$ in $T \sim \nicefrac{1}{\varepsilon^2}$ iterations and sample complexity $\nicefrac{1}{\varepsilon^4}$.
The bound \eqref{eq: majority signSGD optimal} matches \algname{minibatch-SignSGD} bound \eqref{eq: sign SGD optimal} if $\kappa = 2$. The dependency on $\kappa$ is expressed in slowing degenerating multiplicative factor $\alpha_\kappa$ instead of $\varepsilon^{-\frac{\kappa}{\kappa - 1}}$.  
\begin{remark}
    In Appendix \ref{sec: distributed}, we provide a method built on top of \algname{minibatch-SignSGD} algorithm and majority voting for the distributed setup with the fixed number of workers.
\end{remark}

\subsection{\algname{SignSGD} with momentum}\label{sec: MsignSGD}

Instead of variance reduction, one can use the momentum technique with the same sample complexity.

\begin{algorithm}[ht!]
\caption{\algname{M-SignSGD} }
\label{alg:SignSGD-M}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $K$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, momentums $\{\beta_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample $\xi^k$ and compute estimate $g^k = \nabla f(x^k, \xi^k)$;
\STATE Compute  $m^k = \beta_k m^{k-1} + (1-\beta_k) g^k$;
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot \text{sign}(m^k)$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}
%In \algname{M-SignSGD}, the sign operator is applied to the momentum vector instead of the gradient estimate. The following theorem states in expectation convergence rates and parameters for \algname{M-SignSGD}.    
\begin{theorem}[\textbf{Complexity for \algname{M-SignSGD} in expectation}]\label{thm:momentum SignSGD}
Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}) and HT gradient estimates (As. \ref{as: pBCM}). Then Alg. \ref{alg:SignSGD-M} requires $T$ iterations  to achieve  $\frac{1}{T} \sum_{k=1}^{T}  \EE \left[ \|\nabla f(x^k)\|_1 \right]  \leq \varepsilon$ for:

\textbf{arbitrary tuning:}  $T, \gamma_k \equiv \gamma_0  T^{-\frac34}, \beta_k \equiv 1 - \nicefrac{1}{\sqrt{T}}$:
$$ T = O\left(\frac{(\nicefrac{\Delta_1}{\gamma_0} + Ld \gamma_0)^4} {\varepsilon^4} + \left(\frac{\sqrt{d}\|\Vec{\sigma}\|_\kappa}{\varepsilon}\right)^\frac{2\kappa}{\kappa-1}\right),$$

\textbf{optimal tuning:} $\gamma_k \equiv \sqrt{\frac{\Delta_1(1-\beta_k)}{4LdT}}, \beta_k \equiv 1 - \min \left\{1, \frac{1}{\|\Vec{\sigma}\|_\kappa^2} \cdot \left(\frac{\Delta_1 L}{ T}\right)^\frac{\kappa}{3\kappa-2}\right\}: $ 
\begin{equation}\label{eq:N sign-SGD-M optimal}
    T = O\left(\frac{\Delta_1Ld} {\varepsilon^2} + \frac{\Delta_1Ld} {\varepsilon^2} 
 \left(\frac{\sqrt{d}\|\Vec{\sigma}\|_\kappa}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right),
\end{equation}
where $\Delta_1 = f(x^1) - f^*$.
\end{theorem}
In comparison with \eqref{eq: sign SGD optimal} for \algname{minibatch-SignSGD}, in expectation bound \eqref{eq:N sign-SGD-M optimal} has a larger $\sqrt{d}\|\Vec{\sigma}\|_\kappa$ factor instead of $\|\Vec{\sigma}\|_1$, but they are still close due to the norm relation \eqref{eq: norm relation main text}.


\subsection{Discussion and comparison with related works}
\paragraph{Optimality.} First, we compare theoretical complexities. In \cite{zhang2020adaptivegood}, the authors provided lower bound in expectation for the sample complexity $\Omega \left( \frac{\Delta_1 L}{\varepsilon^2} + \frac{\Delta_1 L}{\varepsilon^2}\left(\frac{\|\Vec{\sigma}\|_\kappa}{\varepsilon} \right)^{\frac{\kappa}{\kappa - 1} }\right)$ w.r.t. the $\ell_2$-norm for non-convex functions. Our bounds are the first-known bounds with HP for any HT noise: \algname{minibatch-SignSGD} attains bound $O\left( \frac{\Delta_1 L d \log\nicefrac{1}{\delta}} {\varepsilon^2}  + \frac{\Delta_1 L d \log\nicefrac{1}{\delta}} {\varepsilon^2} 
 \left(\frac{\|\Vec{\sigma}\|_1}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right)$ w.r.t. the $\ell_1$-norm with linear $\log\nicefrac{1}{\delta}$ dependency. However, there are $d$ factors and $\|\Vec{\sigma}\|_{1}$ instead of smaller $\|\Vec{\sigma}\|_{\kappa}$. Since 
 \begin{equation}
     \|x\|_2 \leq \|x\|_1 \leq \sqrt{d}\|x\|_2, \forall x \in \R^d, \label{eq: norm relation main text} 
 \end{equation}
 in order to achieve $\varepsilon$ accuracy in the $\ell_2$-norm, accuracy $\varepsilon'$ in the $\ell_1$-norm has to be $\varepsilon' =   \varepsilon\cdot\sqrt{d}$. Thus, the total number of samples and the  optimality remain. A good analysis of the relation between convergence w.r.t. the different norms is given in \cite{bernstein2018signsgd}. For \algname{M-SignSGD}, \eqref{eq:N sign-SGD-M optimal} exactly matches the optimal bound with the same remarks.
\vspace{-10pt}
\paragraph{Clipping.} According to the HP analysis  of \algname{ClipSGD} from \citep[Theorem 3.1]{sadiev2023high}, it achieves before mentioned lower bound with extra $\log\nicefrac{1}{\varepsilon}$ and $\log\nicefrac{1}{\delta}$ factors. Moreover, the constants concealed behind $O$ notation have $10^3$ magnitudes. To compare, sign-based methods have smaller constants without extra accuracy factors. From the practical point of view, clipping levels depend on the iteration number and affect the final accuracy without a good tuning \citep[Theorem 3.1]{sadiev2023high}. The sign-based methods work well with constant, arbitrary parameters.   
\vspace{-5pt}
\paragraph{Normalized SGD.} In \cite{hubler2024gradient}, the authors analyze HP convergence of normalization-based methods under HT noise. These methods use normalization $\nicefrac{g^k}{\|g^k\|_2}$ instead of $\sign(g^k)$. Namely, for non-convex functions, \algname{minibatch-NSGD} with the same optimal batchsizes has sample complexity w.r.t. to the $\ell_2$-norm:
\begin{equation}
    O\left(\frac{\Delta_1L_\delta} {\varepsilon^2} + \frac{\Delta_1L_\delta} {\varepsilon^2} 
 \left(\frac{\|\Vec{\sigma}\|_\kappa}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right). \label{eq: NSGD optimal}
\end{equation}
The only difference of \eqref{eq: NSGD optimal} from \eqref{eq: sign SGD optimal} is the absence of $d$ factors, which can be explained by different norm for required accuracy. The same comparison is valid for the momentum methods.  From the practical point of view, sign-based methods can be applied to distributed optimization (Appendix \ref{sec: distributed}) where normalization does not fit. Besides, one can use majority voting as a more powerful alternative to batching.  
\vspace{-5pt}
\paragraph{Symmetric HT noise.} 
For \algname{MajorityVote-SignSGD}, the bounds \eqref{eq: majority signsgd arb}, \eqref{eq: majority signSGD optimal} match optimal bounds in expectation for first-order non-convex methods with \textit{BV noise} \cite{ arjevani2023lower} with mild $\log\nicefrac{1}{\delta}$ dependency. In \cite{armacki2023high, armacki2024large}, the authors considered  only symmetric noise and proved bounds \textit{arbitrary close} to $O(\varepsilon^{-4})$ in online paradigm. On the contrary, our bounds are tight.

\vspace{-1pt}
\section{High probability bounds for comparison oracle under heavy-tailed noise}\label{sec: sec_3}

In this section, we switch from the first-order optimization and the gradient oracle to the zeroth-order optimization and an oracle which can only compare two corrupted function values at two different points. For non-convex functions, we propose our novel \algname{MajorityVote-CompSGD} method and prove its HP bounds under symmetric HT noise. All proofs are located in Appendix \ref{app: proofs}. 

\subsection{Comparison oracle}
For any two points $x, y \in \R^D$, the \textbf{stochastic comparison oracle} $\phi(x,y, \xi = \{\xi_x, \xi_y\})$ determines which noisy function value, $f(x, \xi_x)$ or $f(y, \xi_y)$, is larger (the realizations $\xi_x$ and $\xi_y$ may be dependent): 
$$\phi(x,y, \xi) = \sign(f(x, \xi_x) - f(y, \xi_y)).$$
This oracle concept is natural for describing human decision-making \cite{lobanov2024acceleration}. Given a choice between two options, it is usually much easier to choose which option is better rather than estimate quantative  difference. The stochasticity $\xi$ describes the variety of division makers and their random states. For example, this oracle is extensively used in Reinforcement Learning (RL) and training Large Language Models via RL with human feedback \cite{ouyang2022training, wang2023rlhf, tang2023zeroth}. 
\subsection{Related works} %Zeroth-order methods can be divided into two main groups according to working principles.  
%The first group utilizes explicit function values to approximate gradient by a finite difference which is then plugged into a first-order method \cite{shamir2017optimal, gasnikov2022power, kornilov2024accelerated}. For non-convex functions under BV noise and convex functions under HT noise, optimal rates differ from optimal rates for first-order methods only by $d$ dependent factors \cite{danilova2022recent, kornilov2024accelerated}.     


%The second group of methods uses comparisons to find the minimal point among the observed ones from random directions. 
%With comparison oracle, the existing methods use comparisons to find the minimal point among the observed ones from random directions. 
The most common instance of methods using comparisons is Stochastic Three Points (\algname{STP}) \cite{bergou2020stochastic, gorbunov2022acceleratedstp}. It takes a random direction and goes along it where the function value is smaller. Initially \algname{STP} was analyzed for non-convex functions without any noise. In \cite{boucherouite2024minibatch}, the authors worked with sum-type functions and stochastic mini-batches. They proved convergence of \algname{STP} in expectation under BV noise, but with the obligatory condition on huge batch sizes.

In \cite{saha2021dueling}, the authors considered a noisy comparison oracle where noise was introduced as a fixed probability of receiving a wrong sign during the comparison. They restated \algname{STP} via the sign operator and at each iteration repeated Bernoulli trials with comparisons to ensure the sign correctness with high confidence. The authors obtained HP bounds, but only for convex and strongly-convex functions. 

There are other approaches to incorporate comparison oracle. In \cite{lobanov2024acceleration}, the authors used Coordinate Gradient Descent (\algname{CGD}) with the search for the steepest stepsizes using the golden ration method. In the deterministic case with adversarial (non-stochastic) noise, this approach achieved better parameter dependencies and practical results. Especially for strongly convex functions, for which the authors used accelerated \algname{CGD}. The authors also proposed an algorithm for the stochastic oracle and proved its asymptotic convergence. In \cite{tang2023zeroth}, a comparison oracle was used to build a ranking-based gradient estimate over random directions, which was then plugged into \algname{GD}.  

\subsection{Assumptions}
\textbf{Heavy-tailed noise.} We consider the following corrupting heavy-tailed noises induced by variable $\xi$:
\begin{assumption}[Heavy-tailed noise in function estimates]\label{as: function noise xi}
    The function estimate $f(x, \xi)$ is unbiased and has a bounded $\kappa$-th moment $\kappa \in (1,2]$ with $\sigma > 0$, i.e.,
    
    \textbf{1)} $\EE_\xi [f(x, \xi)] = f(x), \quad \forall x \in \R^d,$
    
    \textbf{2)} $\EE_\xi [|f(x, \xi) - f(x)|^\kappa] \leq \sigma^\kappa,\quad \forall x \in \R^d.$
    
    For $\kappa = 2$, the noise is called bounded variance.
\end{assumption}
For example, the estimate $f(x, \xi)$ can be corrupted at each point by independent heavy-tailed noise $\xi$ with bounded $\kappa$-th moment: $f(x, \xi) := f(x) + \xi$.

Another example of such estimate is when we optimize a sum-type function $f(x) = \frac{1}{K}\sum_{i=1}^K f_i(x)$, and $\xi$ denotes a random batch $I$ of fixed size $|I|$ from $\{1, \dots, K\},$ that is, $f(x,\xi) = \frac{1}{|I|}\sum_{i \in I} f_i(x).$ For two points, oracle gives the same $\xi$ realization (batch). This estimate satisfies the discrete BV noise assumption \cite{boucherouite2024minibatch}.

\textbf{Random directions.} We use the following assumption on the random directions' distribution  $\mathcal{D}$:
\begin{assumption}[Random directions]\label{as: D dist}
    The distribution $\mathcal{D}$ on $\R^d$ has the following properties:
    
    \textbf{1)} There exist a norm $\|\cdot\|_p, p \in [1, 2]$ and a constant $\alpha_p > 0$, such that for all $ g \in \R^d$:
    $$\EE_{\mathbf{e} \in \mathcal{D}} |\la g, \mathbf{e} \ra | \geq \alpha_p \| g \|_p.$$
    \textbf{2)} For all $\mathbf{e} \in \mathcal{D}$, the norms $\|\mathbf{e}\|_2 \leq 1,  \|\mathbf{e}\|_q \leq 1, \frac{1}{p} + \frac{1}{q} = 1$.
\end{assumption}
We use the following instances of $\mathcal{D}$ with explicit constants and norms \citep[Lemma $3.4$]{bergou2020stochastic}:

\textbf{1)} Uniform distribution on Euclidean sphere $S^d_2 := \{\mathbf{e}| \quad\|\mathbf{e}\|_2  = 1 \}, p = 2, \alpha_p = \frac{1}{\sqrt{2\pi d}}.$ 

\textbf{2)} Uniform distribution on standard basic vectors $\{ e_1, \dots, e_d\},  p = 1, \alpha_p = \frac{1}{d}.$


\subsection{\algname{CompSignSGD} and its convergence properties}
In \cite{lobanov2024acceleration}, the authors propose a nameless procedure for stochastic oracle:
\begin{equation}
    x^{k+1} = x^k - \gamma_k \cdot \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi_-)) \cdot \mathbf{e}^k.\notag
\end{equation}
If value $f(x^k - \gamma_k \mathbf{e}^k, \xi_-)$ is smaller than $f(x^k + \gamma_k \mathbf{e}^k, \xi_+)$, then sign equals to $1$ and $x^{k+1} = x^k - \gamma_k \mathbf{e}^k$. Otherwise, the point $x^{k+1} = x^k - \gamma_k \mathbf{e}^k$ is chosen.
We name it \algname{CompSGD} (Alg. \ref{alg:comp-sinSGD})  and prove the following convergence Lemma. 
\begin{algorithm}[ht!]
\caption{\algname{CompSGD} }
\label{alg:comp-sinSGD}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$;

\FOR{$k=1,\ldots, T$}
\STATE Sample direction $\mathbf{e}^k$ and $\xi^k$ ;
\STATE $\phi^k := \sign\left[ f(x^k\!+\! \gamma_k\mathbf{e}^k, \xi_+^k)\!-\!f(x^k\!-\!\gamma_k\mathbf{e}^k, \xi_-^k)\right]$;
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot \phi^k \cdot \mathbf{e}^k$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ ; 
\end{algorithmic}
\end{algorithm}

\begin{lemma}[\textbf{\algname{CompSGD} Convergence Lemma}]
    \label{lem: compsgd T update}
Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}), random directions with $\alpha_p$ (As. \ref{as: D dist}) and HT function estimates (As. \ref{as: function noise xi}). Then Alg. \ref{alg:comp-sinSGD} after $T$ iteration with constant stepsizes $\gamma_k \equiv \gamma$ achieves with probability at least $1 - \delta$ starting with $\Delta_1 = f(x^1) - f^*$:
\begin{eqnarray}
\frac{1}{T} \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_p \!\!&\leq&\!\!\frac{2\Delta_1}{T\alpha_p \gamma} + \frac{12d^{\frac1p} \|\nabla f (x^0)\|_2}{T\sqrt{d}\alpha_p} \log(\nicefrac{1}{\delta})\notag \\
&+& 24\frac{Ld^{\frac1p}\gamma}{\sqrt{d}\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{8\sigma}{\alpha_p\gamma}. \notag %+ \frac{12\sqrt{d}}{T\alpha^2_p} \|\nabla f (x^0)\|_p  \log(\nicefrac{1}{\delta}). \notag  
\end{eqnarray}
\end{lemma}
As a consequence, in order to achieve accuracy $\varepsilon$, the noise $\sigma$ must not exceed $\sigma \sim  \alpha_p\varepsilon^2$. 

\subsection{Our \algname{MajorityVote-CompSGD}}\label{sec: compsgd majority}
At this point, we propose our novel \algname{MajorityVote-CompSGD} which  can reduce noise  via the majority voting over comparison signs:
$$x^{k+1}\!=\!x^k\!-\!\gamma_k \sign\!\!\left[\sum_{i=1}^M \!\phi(x^k + \gamma_k \mathbf{e}^k, x^k - \gamma_k \mathbf{e}^k, \xi^k_i)\!\right]\!\!\mathbf{e}^k.$$\vspace{-5pt}
\begin{algorithm}[ht!]
\caption{\algname{MajorityVote-CompSignSGD} }
\label{alg:majorvoting-STP}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, batchsizes $\{M_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample direction $\mathbf{e}^k$ and $\{\xi_i^k\}_{i=1}^{M_k}$;
\STATE $\phi^k_i = \sign\left[ f(x^k\!+\! \gamma_k\mathbf{e}^k, \xi^k_{i,+})\!-\!f(x^k\!-\!\gamma_k\mathbf{e}^k, \xi^k_{i,-})\right];$
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot \sign\left(\sum_{i=1}^{M_k} \phi^k_i \right) \cdot \mathbf{e}^k$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}

\vspace{-7pt}
Similar to \algname{MajorityVote-SignSGD}, we require additional assumption of unimodality and symmetry of HT noise $f(x,\xi) - f(x), \forall x \in \R^d.$ 
\begin{theorem}[\textbf{HP complexity for \algname{MajorityVote-CompSGD}}] \label{thm: MajorVote-SignSTP}
Consider the lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}), random directions with $\alpha_p$ (As. \ref{as: D dist}) and HT \textbf{unimodal and
symmetric} function estimates (As. \ref{as: function noise xi}). Then Alg. \ref{alg:majorvoting-STP} requires comparison number $N$ to achieve  $\frac{1}{T} \sum\limits_{k=1}^{T}   \|\nabla f(x_k)\|_p  \leq \varepsilon$ with probability at least $1 - \delta$ for:

\textbf{arbitrary tuning:} $T, \gamma_k \equiv \nicefrac{\gamma_0}{\sqrt{T}}, M_k \equiv \max\{1, M_0T^2\}$:
\begin{equation}
    N = M_0T^3 = O\left(\frac{M_0\cdot(\nicefrac{(\Delta_1 +  \nicefrac{a_\kappa\sigma}{\sqrt{M_0}})}{\gamma_0} + L_{\delta,p}\gamma_0)^6}{(\alpha_p\varepsilon)^6}\right), \notag
\end{equation}
\textbf{optimal tuning:} $T = O\left(\frac{\Delta_1 L_{\delta,p}}{\alpha^2_p \varepsilon^2 }\right), \gamma_k \equiv \sqrt{\frac{\Delta_1}{12L_{\delta,p} T}}$ and $M_k \equiv \max \left\{1,  \left(\frac{32 a_\kappa\sigma}{\alpha_p\varepsilon\gamma}\right)^2\right\}$:
\begin{equation}
    N = O\left(\frac{\Delta_1 L_{\delta,p}}{\alpha^2_p\varepsilon^2 } + \frac{\Delta_1 L_{\delta,p}}{\alpha^2_p\varepsilon^2 }  \left( \frac{a_\kappa \sigma L_{\delta,p}}{\alpha_p^2\varepsilon^2} \right)^2\right), \label{eq: majorityvote-compsgd bound}
\end{equation}
where $\Delta_1\!\!=\!\!f(x^1)\!-\!\!f^*, L_{\delta,p}\!=\!L \log( \frac{1}{\delta}) d^{\frac1p - \frac12}, (a_\kappa)^{\kappa} = \frac{\kappa+1}{\kappa - 1}.$
\end{theorem}

\begin{remark}[\textbf{\algname{CompSGD} with function batching}]
    If one can directly compare the batched function values at two points, e.g. with the sum-type objective functions, then batch averaging can be applied under any HT noise. Similarly to \algname{minibatch-SignSGD}, we substitute the step $3$ in Alg. \ref{alg:majorvoting-STP} with  $g^k = \sign\left[ \sum\limits_{i=1}^{B_k} f(x^k + \gamma_k\mathbf{e}^k, \xi_{i,+}^k) - \sum\limits_{i=1}^{B_k} f(x^k - \gamma_k\mathbf{e}^k, \xi_{i,-}^k) \right]$ and build a new \algname{minibatch-CompSGD} method (See Appendix \ref{sec: compsggd and batching}).  It achieves the number of function calls $N$ with HP with the optimal parameters $T = O\left(\frac{\Delta_1 L_{\delta,p} }{\alpha_p^2\varepsilon^2 }\right), \gamma_k \equiv \sqrt{\frac{\Delta_1 }{TL_{\delta,p} }}$ and $B_k \equiv \max\left\{1, \left( \frac{\sigma T}{\Delta_1} \right)^\frac{\kappa}{\kappa-1}\right\}$ :
\begin{equation}\label{eq: minibatch stp bound}
    N = O\left(\frac{\Delta_1 L_{\delta,p}}{\alpha_p^2\varepsilon^2 } + \frac{\Delta_1 L_{\delta,p}}{\alpha_p^2\varepsilon^2 } \cdot  \left( \frac{\sigma L_{\delta,p} }{\alpha_p^2\varepsilon^2} \right)^\frac{\kappa}{\kappa-1}\right).
\end{equation}
\end{remark}
\subsection{Discussion and comparison with related works}
%For two considered distributions $\mathcal{D}$, we estimate the value $\alpha_p \varepsilon$.  Euclidean sphere's value $\alpha_p$ is large by a factor $\sqrt{d}$. However, sphere itself induces $\ell_2$ norm in the final estimate \eqref{eq: minibatch stp bound}. Due to norm inequality
%$$\|\nabla f(x)\|_2 \leq \|\nabla f(x)\|_1 \leq \sqrt{d}\|\nabla f(x)\|_2,$$
%in order to achieve the same $\varepsilon$ bound for $\ell_2$ norm with the standard basis, the actual $\ell_1$ accuracy $\varepsilon'$ must $\varepsilon' =   \varepsilon\cdot\sqrt{d}$. Hence, the value $\alpha_p \varepsilon$ for both setups is the same.
\textbf{Optimality.} When $\mathcal{D}$ is a Euclidean sphere, the first term $\tilde{O}(\nicefrac{dL}{\varepsilon^{2}})$ from \eqref{eq: majorityvote-compsgd bound}, \eqref{eq: minibatch stp bound}  matches the bounds for noiseless methods from previous works \cite{bergou2020stochastic,tang2023zeroth, lobanov2024acceleration} and the optimal bound for the deterministic zeroth-order optimization \cite{nemirovskij1983problem}. Moreover, our HP threshold for noise $\sigma \sim \varepsilon^2/\sqrt{d}$ is the same as the threshold for adversarial noise from \cite{lobanov2024acceleration} or for batched variance from \cite{boucherouite2024minibatch}. This threshold is optimal w.r.t. $\varepsilon$ and $d$ \cite{Lobanov_2023}. Hence, our bounds are tight.

\textbf{Comparison.} Although \algname{CompSGD} was proposed in \cite{lobanov2024acceleration}, the authors only proved its \textit{ asymptotic convergence with parameters depending on the solution}. We prove its explicit formulas with HP (Lemma \ref{lem: compsgd T update}) and propose a novel modification (Alg. \ref{alg:majorvoting-STP}) which converges non-asymptotically (Th. \ref{thm: MajorVote-SignSTP}).

The noisy comparison oracle from \cite{saha2021dueling} is similar to ours. The authors used a non-trivial assumption: \begin{equation}
\textstyle
    \mathbb{P}_\xi\left[\phi(x,y, \xi) \neq \sign(f(x) - f(y))\right] \leq \nicefrac{1}{2} - \nu,  \forall x,y \in \R^d, \label{eq: saha assumption}
\end{equation} for some constant $\nu \in (0,\nicefrac{1}{2})$. First, all results from \cite{saha2021dueling} are proved for the convex functions, and we prove it for the non-convex ones. Next, we highlight that \textit{our Assumption \ref{as: function noise xi} is much weaker and general}, since \eqref{eq: saha assumption} can fail even under BV noise. In proofs, we show that
\begin{equation*}
\textstyle
  \mathbb{P}_\xi\left[\phi(x,y, \xi) \!\neq\! \sign(f(x)\! -\! f(y))\right] \leq  \nicefrac{\sigma}{|f(x)\!-\!f(y)|}.  
\end{equation*}
Thus, in the vicinity of the stationary point where the changes of the function are small or under large $\sigma$, \eqref{eq: saha assumption} cannot hold. 

\input{experiments}

%\section*{Impact Statements}

%This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{refs}
\bibliographystyle{icml2025}




\onecolumn
\appendix
\section{\algname{SignSGD} for distributed optimization}\label{sec: distributed}
Consider distributed optimization with one server and $M$ workers, each of which calculates its own gradient estimate. The server receives all estimates, aggregates them, and sends back the updated solution to the workers. Sign-based methods are so effective in terms of communication \cite{bernstein2018majorityvote,jin2020stochastic}, as sending a sign vector costs only $O(d)$ operations. We use aggregation based on the majority voting. 
\begin{algorithm}[ht!]
\caption{\algname{Distributed-MajorityVote-SignSGD} }
\label{alg:majorityvotesignSGD}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, batchsizes $\{B_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample $\{\xi^{k,j}_i\}_{i=1}^{B_k}$ and compute gradient estimate $g^{k,j} = \sum_{i = 1}^{B_k} \nabla \nicefrac{f(x^k, \xi^{k,j}_i )}{B_k}$ for each worker $j \in \overline{1,M}$;
\STATE Send signs $\sign(g^{k,j})$ to server for each worker $j \in \overline{1,M}$;
\STATE Compute on server $g^k =  \sign\left(\sum_{j=1}^M \sign(g^{k,j})\right);$
\STATE Send point $x^{k+1} = x^k - \gamma_k \cdot g^k$ to each worker;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\textbf{HP complexity for \algname{Distributed-MajorityVote-SignSGD}}]\label{thm:dist signsgd conv}
  Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}) and the gradient estimates corrupted by \textbf{unimodal and
symmetric} HT noise (As. \ref{as: pBCM}). Then Alg. \ref{alg:majorityvotesignSGD} with $M$ workers requires the sample complexity $N_M$ per worker to achieve $\frac{1}{T} \sum_{k=1}^T  \|\nabla f(x^k)\|_1 \leq \varepsilon$ with probability at least $1-\delta$ for:

\textbf{arbitrary tuning:}  $T, \gamma_k \equiv \frac{\gamma_0}{\sqrt{T}}, B_k \equiv \max \{1, B_0T\}$: 
$$N_M  = O\left(\frac{B_0(\nicefrac{\Delta_1}{\gamma_0} + L_\delta d \gamma_0)^4} {\varepsilon^4} + \frac{ 1}{B_0}\left(\frac{a_\kappa \|\Vec{\sigma}\|_1}{\sqrt{M}\varepsilon}\right)^\frac{2\kappa}{\kappa-1}\right), $$
 \textbf{optimal tuning:} $T = O\left(\frac{\Delta_1L_\delta d }{\varepsilon^2}\right), \gamma_k \equiv \sqrt{\frac{\Delta_1}{8dL_\delta T}}, B_k \equiv \max \left\{1,   \left(\frac{16a_\kappa\|\Vec{\sigma}\|_1}{\sqrt{M} \varepsilon}\right)^\frac{\kappa}{\kappa-1}\right\}:$ 
$$N_M = O\left(\frac{ \Delta_1L_\delta d}{\varepsilon^2} +   \frac{ \Delta_1L_\delta d} {\varepsilon^2} 
 \left(\frac{a_\kappa \|\Vec{\sigma}\|_1}{\sqrt{M} \varepsilon}\right)^\frac{\kappa}{\kappa-1}\right),$$

where  $ \Delta_1 = f(x^1) - f^*, L_\delta = L \log \nicefrac{1}{\delta}, (a_\kappa)^\kappa := \left(\frac{\kappa+1}{\kappa - 1}\right).$

 
\end{theorem}
The proof of Theorem \ref{thm:dist signsgd conv} is located in Appendix \ref{subsec: majority vote sign proofs}.

\begin{remark}[\textbf{\algname{SignSGD} with median clipping}]
    For the symmetric HT noise with a mild condition on probability density function, there exists a complement to the batch averaging, namely, coordinate-wise median operator \cite{puchkin2024breaking}. For all $\kappa \in (1,2]$, it requires only $9$ samples to build an unbiased BV gradient estimate. Then it can be combined with \algname{minibatch-SignSGD} as if $\kappa = 2$.  In this case, the $\kappa$ dependency from Theorems \ref{thm:com-sign conv} and \ref{thm:dist signsgd conv} can be completely removed.  
\end{remark}

\section{\algname{CompSGD} with function batching} \label{sec: compsggd and batching}
If one can batch function values at two points before its direct comparison (e.g. with sum-type objective function), then \algname{CompSGD} combined with the batching achieves the following bounds.

\begin{algorithm}[ht!]
\caption{\algname{minibatch-CompSGD} }
\label{alg:minibatch-STP}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^1 \in \R^d$, number of iterations $T$, stepsizes  $\{\gamma_k\}_{k=1}^{T}$, batchsizes $\{B_k\}_{k=1}^{T}$.

\FOR{$k=1,\ldots, T$}
\STATE Sample direction $\mathbf{e}^k$ and $\{\xi_i^k\}_{i=1}^{B_k}$;
\STATE Compare  $g^k = \sign\left( \sum\limits_{i=1}^{B_k} f(x^k + \gamma_k\mathbf{e}^k, \xi_i^k) - \sum\limits_{i=1}^{B_k}f(x^k - \gamma_k\mathbf{e}^k, \xi_i^k) \right)$;
\STATE Set $x^{k+1} = x^k - \gamma_k \cdot g^k  \cdot \mathbf{e}^k$;
\ENDFOR
\ENSURE uniformly random point from $\{x^1, \dots, x^{T}\}$ . 
\end{algorithmic}
\end{algorithm}

\begin{theorem}[\textbf{HP complexity for \algname{minibatch-CompSGD}}] \label{thm: minibatch stp}
Consider lower-bounded $L$-smooth function $f$ (As. \ref{as: bounded}, \ref{as: smooth}), random directions with $\alpha_p$ (As. \ref{as: D dist}) and HT function estimates with $\sigma, \kappa \in (1,2]$ (As. \ref{as: function noise xi}). Then Alg. \ref{alg:minibatch-STP} requires  $N$ function calls to achieve  $\frac{1}{T} \sum_{k=1}^T   \|\nabla f(x_k)\|_p  \leq \varepsilon$ with probability at least $1 - \delta$ for:

\textbf{optimal tuning:} $T = O\left(\frac{\Delta_1 L_{\delta,p} }{\alpha^2_p \varepsilon^2 }\right), \gamma_k =\equiv \sqrt{\frac{\Delta_1 }{TL_{\delta,p}}}$ and $B_k \equiv \max\left\{1, \left( \frac{\sigma T }{\Delta_1} \right)^\frac{\kappa}{\kappa-1}\right\}$ :
\begin{equation}\label{eq: minibatch stp bound app}
    N = O\left(\frac{\Delta_1 L_{\delta,p} }{\alpha^2_p \varepsilon^2   } + \frac{\Delta_1 L_{\delta,p}}{\alpha^2_p \varepsilon^2 } \cdot  \left( \frac{\sigma L_{\delta,p} }{\alpha^2_p \varepsilon^2} \right)^\frac{\kappa}{\kappa-1}\right),
\end{equation}
where $\Delta_1 = f(x^1) - f^*, L_{\delta,p} = d^{\frac{1}{p} - \frac12} L \log \nicefrac{1}{\delta}.$
\end{theorem}
The proof of Theorem \ref{thm: minibatch stp} is located in Appendix \ref{subsec: minibatch-comp proof}. For two considered distributions $\mathcal{D}$, the Euclidean sphere and the standard basis, we estimate the value $\alpha_p \varepsilon$.  Euclidean sphere's value $\alpha_p$ is large by a factor $\sqrt{d}$. However, the sphere itself induces the $\ell_2$-norm in the final estimate \eqref{eq: minibatch stp bound}. Due to the inequality 
$$\|\nabla f(x)\|_2 \leq \|\nabla f(x)\|_1 \leq \sqrt{d}\|\nabla f(x)\|_2,$$
in order to achieve the same $\varepsilon$ bound for the $\ell_2$-norm with the standard basis, the actual $\ell_1$-accuracy $\varepsilon'$ must $\varepsilon' =   \varepsilon*\sqrt{d}$. Hence, the value $\alpha_p \varepsilon$ for both setups is the same.  

%In case of corrupting noise $\xi$, we use the same assumption of unimodality and symmetry of $\xi$ for effective majority voting and variance reduction.


\section{Proofs}\label{app: proofs}
\subsection{Technical lemmas and propositions}
We use the following facts from the linear algebra and convex analysis \cite{boyd2004convex}:
\begin{proposition}[Smoothness inequality]\label{lem: L smooth ineq}
For $L$-smooth function $f$ (As. \ref{as: smooth}), the following inequality holds true    \begin{equation}
    f(y) - f(x) - \la \nabla f(x), y -x \ra \leq \frac{L}{2}\|x-y\|_2^2, \quad \forall x, y \in \R^d. 
\end{equation}
\end{proposition}
\begin{proposition}[Norm Relation]
    For two norms $ \ell_p$ and $\ell_q$ with $\ 1 \leq p \leq q \leq 2$, the following relation holds true:
    \begin{eqnarray}
        \|x\|_q \leq \|x\|_p \leq d^{\frac{1}{p} - \frac{1}{q}}\|x\|_q, \quad \forall x \in \R^d. \label{eq: norm relation}
    \end{eqnarray}
\end{proposition}
\begin{proposition}[Jensen's Inequality]
    For scalar random variable $\xi$ with bounded $\kappa$-th moment $\kappa \in (1,2]$,  the following inequality holds true:
    \begin{equation}
        \EE [|\xi|] \leq \left(\EE[|\xi|^\kappa] \right)^\frac{1}{\kappa}. \label{eq: Jensen}
    \end{equation}
\end{proposition}
\begin{proposition}[Markov's Inequality]
    For scalar random variable $\xi$ with bounded first moment, the following inequality holds true for any $a > 0$:
    \begin{equation}
        \mathbb{P}(|\xi - \EE[\xi]]| \geq a) \leq \frac{\EE[|\xi|]}{a}. \label{eq: Markov}
    \end{equation}
\end{proposition}
To prove the HP bounds with the logarithmic dependency, we use the following measure concentration result (see, for example, \citep[Lemma $1$]{li2020high}.
\begin{lemma}[Measure Concentration Lemma]\label{lem: bernstein ineq}
    Let  $\{D_k\}_{k = 1}^T$ be a martingale difference sequence (MDS), i.e., $\EE[D_k| D_{k-1}, \dots, D_1] = 0$ for all $k \in \overline{1,T}$. Furthermore, for each $k \in \overline{1,T}$, there exists positive $\sigma_k \in \R$, s.t. $\EE\left[\exp\left(\frac{D_k^2}{\sigma_k^2}\right)| k\right] \leq e.$ Then the following probability bound holds true:
    \begin{equation} \label{eq: bernstein ineq}
        \forall \lambda > 0, \delta \in (0,1): \quad \mathbb{P}\left(\sum\limits_{k=1}^T D_k \leq \frac{3}{4}\lambda \sum \limits_{k=1}^T \sigma_k^2  + \frac1\lambda \log(\nicefrac{1}{\delta})\right) \geq 1  - \delta.
    \end{equation}
\end{lemma}
To control error reduction during batching, we use the following batching lemma for HT variables. Its modern proof for $d = 1$ was proposed in \citep[Lemma $4.2$]{cherapanamjeri2022optimal} and then generalized for the multidimensional case  in \cite{kornilov2024accelerated, hubler2024gradient}.
\begin{lemma}[HT Batching Lemma]\label{lem: batching p}
    Let $\kappa \in (1,2]$, and $X_1, \dots, X_B \in \R^d$ be a martingale  difference sequence (MDS), i.e., $\EE[X_i| X_{i-1}, \dots, X_1] = 0$ for all $i \in \overline{1,B}$.  If all variables $X_i$ have bounded $\kappa-$th moment, i.e., $\EE[\|X_i\|_2^\kappa] < +\infty,$ then the following bound holds true
    \begin{eqnarray}
        \EE\left[\left|\left|\frac{1}{B}\sum_{i=1}^B X_i \right|\right|_2^\kappa\right]\leq \frac{2}{B^\kappa} \sum_{i=1}^B \EE[ \|X_i\|^\kappa_2]. \label{eq: HT batching}
    \end{eqnarray}
\end{lemma}
We need the following lemma about changes after one update step of sign-based methods from \citep[Lemma $1$]{sun2023momentum}.
\begin{lemma}[Sign Update Step Lemma] \label{lem: single update step} Let $x, m \in \R^d$ be arbitrary vectors, $A = \text{diag}(a_1, \dots, a_d)$ be diagonal matrix and $f$ be $L$-smooth function (As.~\ref{as: smooth}). Then for the update step  
$$x' = x - \gamma \cdot  A \cdot  \sign(m)$$
with $\epsilon := m - \nabla f(x)$, the following inequality holds true
\begin{equation}
    f(x') - f(x) \leq - \gamma \|A\nabla f(x)\|_1 + 2\gamma\|A\|_F\|\epsilon\|_2 + \frac{L\gamma^2\|A\|^2_F}{2}. \label{eq: single sign update}
\end{equation}
\end{lemma}

\subsection{Proof of \algname{SignSGD} General Convergence Lemma \ref{lem: signsgd T update}} \label{subsec: signsgd conv}
For beginning, we prove general lemma about \algname{SignSGD} convergence with HT unbiased gradient estimates $g^k$ with $\Vec{\sigma}, \kappa \in (1,2].$ This proof considerably relies on proof techniques for \algname{NSGD} from \cite{hubler2024gradient}.  
\begin{proof}
    Consider the $k$-th step of \algname{SignSGD}. We use smoothness of function $f$ (Lemma \ref{lem: L smooth ineq}) to estimate:
    \begin{eqnarray}
        f(x^{k+1}) - f(x^k) &\leq& \la \nabla f (x^k), x^{k+1} - x^k \ra + \frac{L}{2}\|x^{k+1} - x^k\|_2^2 \notag \\
        &=& - \gamma_k \la \nabla f (x^k), \sign(g^k) \ra + \frac{L\|\sign(g^k)\|^2_2}{2}\gamma_k ^2 \notag \\
        &=& - \gamma_k  \frac{\la \nabla f (x^k), \sign(g^k) \ra}{\|\nabla f (x^k)\|_1} \cdot \|\nabla f (x^k)\|_1 + \frac{Ld}{2}\gamma_k ^2.  \notag
    \end{eqnarray}
    Consequently, after summing all $T$ steps, we obtain:
    \begin{eqnarray}
        \sum \limits_{k=1}^{T} \gamma_k \frac{\la \nabla f (x^k), \sign(g^k) \ra}{\|\nabla f (x^k)\|_1} \cdot \|\nabla f (x^k)\|_1  \leq \underset{ = \Delta_1}{\underbrace{f(x^1) - f(x^*)}} + \frac{Ld}{2}\sum \limits_{k=1}^T \gamma_k^2.
    \end{eqnarray}
    We introduce the following terms  $\phi_k := \frac{\la \nabla f (x^k), \sign(g^k) \ra}{\|\nabla f (x^k)\|_1} \in [-1,1]$, $\psi_k := \EE[\phi_k| x^{k }]$ and $D_k := - \gamma_k (\phi_k - \psi_k)\|\nabla f (x^k)\|_1$. We note that $D_k$ is a martingale difference sequence ($\EE[D_k|D_{k-1}, \dots, D_k] = 0$) and satisfies 
    $$\exp\left( \frac{D_k^2}{4\gamma_k^2\|\nabla f (x^k)\|_1^2}\right) = \exp \left(\frac{(\phi_k - \psi_k)^2}{4}\right) \leq e.$$
    Applying Measure Concentration Lemma \ref{lem: bernstein ineq} to MSD $D_k$ with $\sigma^2_k = 4 \gamma_k^2 \|\nabla f (x^k)\|_1^2$, we derive the bound for all $\lambda > 0$ with probability at least $1 - \delta$:
    $$\sum\limits_{k=1}^{T} \gamma_k(\psi_k - 3 \lambda \gamma_k \|\nabla f (x^k)\|_1 ) \|\nabla f (x^k)\|_1 \leq \Delta_1 + \frac{Ld}{2}\sum\limits_{k=0}^{T-1} \gamma_k^2 + \frac{1}{\lambda} \log(\nicefrac{1}{\delta}).$$
     We use norm relation \eqref{eq: norm relation} and $L$-smoothness (As.\ref{as: smooth}) to estimate maximum gradient norm for all $k \in \overline{2,T+1}:$
    \begin{eqnarray}
        \|\nabla f (x^k)\|_1 &\leq& \sqrt{d}\|\nabla f (x^k)\|_2 \leq \sqrt{d}\|\nabla f (x^k) - \nabla f (x^{k-1}) + \nabla f (x^{k-1}) \|_2  \notag \\
    &\leq& \sqrt{d}\|\nabla f (x^k) - \nabla f (x^{k-1})\|_2 + \sqrt{d} \|\nabla f (x^{k-1}) \|_2 \leq \sqrt{d} L \|x^k - x^{k-1}\|_2  + \sqrt{d}\|\nabla f (x^{k-1})\|_2  \notag\\
    &\leq& \sqrt{d} L  \gamma_{k-1} \sqrt{d} + \sqrt{d}\|\nabla f (x^{k-1})\|_2 \leq \sqrt{d}\|\nabla f (x^1)\|_1 +  Ld\sum_{\tau=1}^{k-1}\gamma_\tau.
    \end{eqnarray}
    Hence, the choice $\lambda := \frac{1}{6d(\gamma^{max} \|\nabla f (x^1)\|_1  + C_TL)}$ where $C_T := \max\limits_{k \in \overline{1,T}} \gamma_k \cdot  \sum\limits_{\tau=1}^{k-1}\gamma_\tau$ and $\gamma^{max} := \max\limits_{k \in \overline{1,T}} \gamma_k  $ yields with probability at least $1 - \delta$:
    \begin{equation}
        \sum\limits_{k=1}^T \gamma_k\left(\psi_k - \frac{1}{2}\right)\|\nabla f (x^k)\|_1 \leq \Delta_1 + \frac{Ld}{2}\sum_{k=1}^T\gamma_k^2 + 6d(\gamma^{max} \|\nabla f (x^1)\|_1  + C_TL) \log(\nicefrac{1}{\delta}), \label{eq: sign sgd before prob}
    \end{equation}
    Next, we estimate each term $\psi_k \|\nabla f (x^k)\|_1$ in the previous sum:
\begin{eqnarray}
\psi_k \|\nabla f (x^k)\|_1 &=& \EE \left[ \la \nabla f (x^k), \sign(g^k) \ra| x^k \right] \notag \\
&=& \|\nabla f (x^k)\|_1 - \sum_{i=1}^d 2 |\nabla f (x^k)|_i \cdot \mathbb{P}(\sign(\nabla f (x^k))_i\neq \sign(g^k)_i | x^k).\label{eq: line with prob sign}
\end{eqnarray}
For each coordinate, we have a bound derived from Markov's inequality \eqref{eq: Markov}  followed by Jensens inequality \eqref{eq: Jensen}:
\begin{eqnarray}
    \mathbb{P}(\sign(\nabla f (x^k))_i \neq \sign(g^k)_i | x^k) &\leq& \mathbb{P}(|\nabla f (x^k)_i -  g^k_i| \geq |\nabla f (x^k)_i| |  x^k)  
    \leq \frac{\EE_{\xi^k}[|\nabla f (x^k)_i - g^k_i| ]}{|\nabla f (x^k)_i|} \notag \\ &\leq& \frac{(\EE_{\xi^k}[|\nabla f (x^k)_i - g^k_i|^\kappa ])^\frac{1}{\kappa}}{|\nabla f (x^k)_i|}  \leq \frac{\sigma_i }{|\nabla f (x^k)_i|}. \label{eq:Prob sign not eq simple}
\end{eqnarray}
Hence, the whole sum can be bounded as 
\begin{eqnarray}
    \sum_{i=1}^d 2 |\nabla f (x^k)|_i \cdot \mathbb{P}(\sign(\nabla f (x^k))_i\neq \sign(g^k)_i | x^k)
    &\leq& 2 \|\Vec{\sigma}\|_1. \notag \notag 
\end{eqnarray}
Finally, we put this bound in \eqref{eq: sign sgd before prob} and obtain:
   \begin{eqnarray}
       \frac12 \sum\limits_{k=1}^T \gamma_k\|\nabla f (x^k)\|_1 &\leq& f(x^1) - f(x^*) + \frac{Ld}{2}\sum_{k=1}^T\gamma_k^2 + 2\sum_{k=1}^T\gamma_k \|\Vec{\sigma}\|_1 \notag \\
       &+& 6d(\gamma^{max} \|\nabla f (x^1)\|_1  + C_TL) \log(\nicefrac{1}{\delta}).
   \end{eqnarray}
Plugging in constant stepsizes $\gamma_k \equiv \gamma$ implies $C_T = T\gamma^2, \gamma^{max} = \gamma$ and the required inequality \eqref{eq: signsgd convergence lemma}:
$$\frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta})  + 4 \|\Vec{\sigma}\|_1 + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}).$$
\end{proof}
\subsection{Proof of \algname{minibatch-SignSGD} Complexity Theorem \ref{thm:minibatch SignSGD}}
\begin{proof}
According to Lemma \ref{lem: signsgd T update}, \algname{minibatch-SignSGD} with batched gradient estimates of batchsize $B$ corrupted by HT noise with $\Vec{\sigma_B}$ convergence as follows:  
$$\frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta})  + 4 \|\Vec{\sigma_B}\|_1 + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}).$$
Due to Batching Lemma \ref{lem: batching p}, we can estimate the $\kappa-$th moment of the batched estimate as
   $\|\Vec{\sigma_B}\|_1 \leq \frac{2\|\Vec{\sigma}\|_1}{B^{\frac{\kappa-1}{\kappa}}}$  and  derive:
\begin{equation}\label{eq: minibatch signsgd bound before params}
    \frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta})  + 8 \frac{\|\Vec{\sigma}\|_1}{B^{\frac{\kappa-1}{\kappa}}} + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}).
\end{equation}
We can omit the last term since its dependency on $T$ has the largest power.

\textbf{1) For arbitrary tuning}, we use parameters $T, \gamma_k = \frac{\gamma_0}{\sqrt{T}}, B_k = \max \{1, B_0T\}$ to get:
$$\frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{\sqrt{T}\gamma_0} + 16 \frac{Ld\gamma_0}{\sqrt{T}} \log(\nicefrac{1}{\delta})  + 8 \frac{\|\Vec{\sigma}\|_1}{B_0^{\frac{\kappa-1}{\kappa}} T^{\frac{\kappa-1}{\kappa}}} + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}).$$
 Setting such $T$ that the first two terms become less than $\varepsilon$, we obtain the final complexity $N = T \cdot B_0T.$

\textbf{2) For optimal tuning}, we first choose large enough $B$ to bound the term $8\frac{\|\Vec{\sigma}\|_1}{B^{\frac{\kappa-1}{\kappa}}} \leq \varepsilon/2 \Rightarrow B_k \equiv \max \left\{1,  \left(\frac{16\|\Vec{\sigma}\|_1}{\varepsilon}\right)^\frac{\kappa}{\kappa-1}\right\}$. Then we choose optimal $\gamma = \sqrt{\frac{\Delta_1}{8L_\delta dT}}$ minimizing $ \min_\gamma \left\{\frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta}) \right\}= \sqrt{\frac{128 \Delta_1 Ld \log(\nicefrac{1}{\delta} )}{T}}.$ Finally, $T$ is set to bound $ \sqrt{\frac{128 \Delta_1 Ld \log(\nicefrac{1}{\delta} )}{T}} \leq \varepsilon/2 \Rightarrow T = O\left(\frac{\Delta_1L\log(\nicefrac{1}{\delta} ) d }{\varepsilon^2}\right).$ 
\end{proof}



\subsection{Proof of \algname{M-SignSGD} Complexity Theorem \ref{thm:momentum SignSGD}}
In this proof, we generalize Theorem $1$ from \cite{sun2023momentum} for HT noise. 
\begin{proof}
Since we set constant steps sizes and momentum, we denote them as $\gamma \equiv \gamma_k$ and $\beta \equiv \beta_k$, respectively. We use notations $\epsilon^k := m^k - \nabla f(x^k)$ and $\theta^k := g^k - \nabla f(x^k)$. Therefore, we have at $k$-th step values:
\begin{eqnarray}
    m^k &=& \beta m^{k-1} + (1-\beta) g^k= \gamma (\epsilon^{k-1} + \nabla f(x^{k-1})) + (1-\gamma)(\theta^k + \nabla f(x^k)),\notag \\
    \epsilon^k &=& m^k - \nabla f(x^k) = \beta \epsilon^{k-1} + \beta(\underset{=:s^k}{\underbrace{\nabla f (x^{k-1}) - \nabla f (x^k)}} ) + (1- \beta)\theta^k, \notag\\
    \epsilon^k &=& m^k - \nabla f(x^k) = \beta \epsilon^{k-1} + \beta s^k + (1- \beta)\theta^k.\notag 
\end{eqnarray}
Unrolling the recursion, we obtain an explicit formula (upper index of $\beta$ is its power):
\begin{eqnarray}
\epsilon^{k} &=& \beta^{k-1}\epsilon^1 + \sum_{i=2}^{k} \beta^{k-i} s^i + (1-\beta) \sum_{i=2}^{k} \beta^{k-i} \theta^i. \label{eq: unrolling m-signsgd}
\end{eqnarray}
From smoothness of $f$ (As. \ref{as: smooth}) follows the bound:
$$\|s^k\|_2 \leq L\|x^{k-1}-x^k\|_2 \leq L \sqrt{d} \gamma.$$
Hence, the norm of \eqref{eq: unrolling m-signsgd} can be bounded as:
$$\|\epsilon^k\|_2 \leq \beta^{k-1}\|\epsilon^1\|_2 + L\sqrt{d}\gamma \cdot \sum_{i=2}^k \beta^{k-i}  + (1-\beta) \|\sum_{i=2}^k \beta^{k-i} \theta^i\|_2.$$
We notice that variables $\{\theta_i\}$ are martingale difference sequence from Lemma \ref{lem: batching p} which we plan to use. Due to the formal definition of $\theta^i = g^i - \nabla f(x^i) = \nabla f(x^i, \xi_i) - \nabla f(x^i)$ and \algname{M-SinSGD} step, the conditioning on $ \theta^{i-1}, \dots, \theta^1$  with randomness $\xi_1, \dots, \xi_{i-1} $ is equivalent to the conditioning on point s $x^{i},\dots ,x^{2}$. Hence, we show by definition of martingale difference sequence that  $$\EE[\theta^i| \theta^{i-1}, \dots, \theta^1 ] = \EE[\theta^i|x^{i},\dots ,x^{2}] = \EE[\nabla f(x^i, \xi_i) - \nabla f(x^i)|x^{i},\dots ,x^{2}] = 0.$$

To take math expectation from both sides, we first take it from the term
\begin{eqnarray}
    \EE \left[ \|\sum_{i=2}^k \beta^{k-i} \theta^i\|_2 \right]\leq \left(\EE \left[\|\sum_{i=2}^k \beta^{k-i} \theta^i\|_2^\kappa\right]\right)^\frac1\kappa \overset{\text{Lem. }\ref{lem: batching p} }{\leq} \left(\sum_{i=2}^k 2 \EE \left[\| \beta^{(k-i)}\theta^i\|_2^\kappa\right]\right)^\frac1\kappa \leq  \left(\sum_{i=2}^k 2 \beta^{\kappa(k-i)}\EE \left[\| \theta^i\|_2^\kappa\right]\right)^\frac1\kappa. \label{eq: m-signsgd 1} %\leq \frac{2\|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa}.
\end{eqnarray}
For each $i \in \overline{2,T}$, we estimate $\EE \left[\| \theta^i\|_2^\kappa\right]$ as 
\begin{eqnarray}
    \EE \left[\| \theta^i\|_2^\kappa\right] \overset{\eqref{eq: norm relation}}{\leq } \EE \left[\| \theta^i\|_\kappa^\kappa\right] = \EE \left[\sum_{j=1}^d| g^k_j - \nabla f(x^k)_j|^\kappa\right] \overset{As. \ref{as: pBCM}}{\leq }  \sum_{j=1}^d \sigma^\kappa_j = \|\Vec{\sigma}\|_\kappa^\kappa. 
\end{eqnarray}
We continue bounding \eqref{eq: m-signsgd 1} with
\begin{eqnarray}
    \eqref{eq: m-signsgd 1} \leq \left(\sum_{i=2}^k 2 \beta^{\kappa(k-i)}\|\Vec{\sigma}\|_\kappa^\kappa\right)^\frac1\kappa \leq \frac{2\|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa}. \notag
\end{eqnarray}
Therefore, the final math expectation can be calculated as:
\begin{eqnarray}
     \EE\|\epsilon^k\|_2 &\leq& \beta^{k-1} \EE \|\epsilon^1\|_2 + \frac{L\sqrt{d}\gamma}{1 - \beta}   + \frac{2(1-\beta) \|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa}.
\end{eqnarray}
Now, we can use update step Lemma \ref{lem: single update step} and then take math expectation:
\begin{eqnarray}
    f(x^{k+1}) - f(x^k) &\leq& - \gamma \|\nabla f(x^k)\|_1 + 2\gamma \sqrt{d} \|\epsilon^k\|_2 + \frac{L\gamma^2d}{2}, \notag \\
   \EE[f(x^{k+1})] - \EE[f(x^k)] &\leq& - \gamma \EE[\|\nabla f(x^k)\|_1] + 2\gamma \sqrt{d} \beta^{k-1} \EE \|\epsilon^1\|_2 + \frac{2Ld\gamma^2}{1 - \beta}   + \frac{4\gamma \sqrt{d}(1-\beta) \|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa} + \frac{L\gamma^2d}{2}. \notag
\end{eqnarray}
Summing it over $k$ and dividing by $T\gamma$, we derive
\begin{equation}
    \frac{1}{T} \sum_{k=1}^T \EE\|\nabla f(x^k)\|_1 \leq \frac{f(x^1) - f_*}{\gamma T} + \frac{4Ld\gamma}{1 - \beta} + \frac{4\sqrt{d}(1-\beta) \|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa} + 2\sqrt{d} \sum_{k=1}^T\beta^{k-1}\|\epsilon^0\|_2/T. \label{eq: m-signsgd 2}
\end{equation}
We omit the last term since its dependency on $T$ is much weaker.

\textbf{1) For arbitrary tuning}, we set $1 - \beta = \frac{1}{\sqrt{T}}, \gamma = \gamma_0 T^{-\frac{3}{4}}$ and obtain
$$\frac{1}{T} \sum_{k=1}^T \EE\|\nabla f(x^k)\|_1 \leq \frac{\Delta_1}{\gamma_0 T^\frac14} + \frac{4Ld\gamma_0}{T^\frac14} + \frac{4\sqrt{d} \|\Vec{\sigma}\|_\kappa}{T^\frac{\kappa - 1}{2\kappa}} + \frac{2\sqrt{d} \|\epsilon^0\|_2}{T^\frac12}.$$
Next, we choose $T$ to limit $\frac{\Delta_1/\gamma_0 + Ld\gamma_0}{ T^\frac14} \leq \frac{\varepsilon}{2}$ and $\frac{4\sqrt{d} \|\Vec{\sigma}\|_1}{T^\frac{\kappa - 1}{2\kappa}} \leq \frac{\varepsilon}{2}.$

\textbf{2) For optimal tuning}, we first choose optimal  $\gamma = \sqrt{\frac{\Delta_1(1 - \beta)}{4LdT}}$  via minimizing $\min_\gamma \left\{\frac{\Delta_1}{\gamma T} + \frac{4Ld\gamma}{1 - \beta}\right\}$. Then we find optimal  $\beta = 1 - \min \left\{1, \left(\frac{\Delta_1 L}{\|\Vec{\sigma}\|_\kappa^2 T}\right)^\frac{\kappa}{3\kappa-2}\right\}$ via minimizing remaining
$$\min_\beta \left\{\sqrt{\frac{16\Delta_0Ld}{T(1-\beta)}} +  \frac{4\sqrt{d}(1-\beta) \|\Vec{\sigma}\|_\kappa}{(1 - \beta^\kappa)^\frac1\kappa} \right\}.$$
Finally, we select $T$ according to required accuracy $\varepsilon$. 
\end{proof}

\subsection{Proofs of \algname{MajorityVote-SignSGD} Complexity Theorems \ref{thm:com-sign conv} and \ref{thm:dist signsgd conv}}\label{subsec: majority vote sign proofs}
\begin{proof}[Proof of Theorem \ref{thm:com-sign conv}]
    The beginning of this proof exactly copies the proof of \algname{SignSGD} Convergence Lemma (Appendix \ref{subsec: signsgd conv}) until equality \eqref{eq: line with prob sign}. We have to estimate the probability of failure of majority voting for each coordinate $j$ conditioned on $x^k$, namely,
   \begin{eqnarray}
     \mathbb{P}\left(\sign(\nabla f (x^k))_j\neq \sign\left[\sum_{i=1}^{M}\sign(g^k_i) \right]_j\right), \quad g^k_i = \nabla f(x^k, \xi^k_i). \notag 
\end{eqnarray}
%%For the sake of convenience, we omit coordinate index $j$. 
We use the generalized Gauss's Inequality about distribution of unimodal symmetric random variables \citep[Theorem 1]{dharmadhikari1986gauss}. 
\begin{lemma}[Gauss's Inequality]\label{lem: gauss ineq}
Let a random variable $\xi$ be unimodal symmetric with mode $\nu$ and bounded $\kappa$-th moment, $\kappa \in (1,2]$. Then the following bound holds:
$$\mathbb{P}\left[|\xi - \nu| \geq \tau\right] \leq \begin{cases} \left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{\EE[|\xi - \nu|]^\kappa}{\tau^\kappa}, & \quad \tau^\kappa \geq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} \cdot \EE[|\xi - \nu|^\kappa], \\
1 - \left[\frac{\tau^\kappa}{(\kappa+1) \EE[|\xi - \nu|]^\kappa}\right]^\frac1\kappa, &\quad \tau^\kappa \leq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} \cdot \EE[|\xi - \nu|^\kappa].
\end{cases}$$    
\end{lemma}
We use Gauss's Inequality for each variable $g^k_{i,j} = \nabla f(x^k, \xi^k_i)_j$ satisfying the symmetry requirement from the theorem's statement. We denote  $S_j := \frac{|\nabla f (x^k)_j|}{\sigma_j}$ and bound
\begin{eqnarray}
    \mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign(g^k_{i,j})\right] &=& \mathbb{P}\left[g^k_{i,j} - \nabla f (x^k)_j \geq |\nabla f (x^k)_j|\right] \notag \\
    &=& \frac12 \mathbb{P}\left[|g^k_{i,j} - \nabla f (x^k)_j| \geq |\nabla f (x^k)_j|\right] \notag \\
    &\leq&  \begin{cases} \frac12\left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{\sigma_j^\kappa}{|\nabla f (x^k)_j|^\kappa}, & \quad |\nabla f (x^k)_j|^\kappa \geq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} \cdot \sigma_j^\kappa, \\
\frac12 - \frac12\left[\frac{|\nabla f (x^k)_j|^\kappa}{(\kappa+1) \sigma_j^\kappa}\right]^\frac1\kappa, &\quad |\nabla f (x^k)_j|^\kappa \leq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} \cdot \sigma_j^\kappa,
\end{cases} \notag \\
    &\leq&  \begin{cases} \frac12\left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}, & \quad S_j^\kappa \geq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} , \\
\frac12 - \frac12\frac{S_j}{(\kappa+1)^\frac{1}{\kappa} }, &\quad S_j^\kappa \leq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}},
\end{cases}\notag
\end{eqnarray}

We denote probability of failure of a single estimate by 
\begin{eqnarray}
q_j &:=& \mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign(g^k_{i,j})\right] \notag \\
&\leq&  \begin{cases} \frac12\left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}, & \quad S_j^\kappa \geq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} , \\
\frac12 - \frac12\frac{S_j}{(\kappa+1)^\frac{1}{\kappa} }, &\quad S_j^\kappa \leq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}},\notag 
\end{cases} \\
&=:& \tilde{q}_j(S_j). \label{eq: gauss inequality}
\end{eqnarray}
Moreover, this probability  $q_j \leq \tilde{q}_j(S_j) < \frac12$, and the deviation of $q_j$ from $\frac12$ can be bounded by 
$$\varepsilon_j := \frac{1}{2} - q_j \leq \frac{1}{2} - \tilde{q}_j(S_j) =: \tilde{\varepsilon}_j(S_j).$$
The probability of getting the wrong sign can be restated as the probability of failing half out of $M$  Bernoulli trials  with fail probability $q_j$:
\begin{eqnarray}
    \mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign\left[\sum\limits_{i=1}^M\sign(g^k_{i,j})\right]\right]  \leq \frac{1}{1 + \frac{M}{\frac{1}{4\varepsilon_j^2} - 1}} < \frac{1}{1 + \frac{M}{\frac{1}{4\tilde{\varepsilon}_j^2(S_j)} - 1}}. \label{eq: prob not equal after bernoulli}
\end{eqnarray}
First, we consider the case $S_j \geq \frac{\kappa}{(\kappa+1)^{\frac{\kappa-1}{\kappa}}}$:
\begin{eqnarray}
    \frac{1}{4\tilde{\varepsilon}_j^2(S_j)} - 1 &=& \frac{1}{4\left(\frac12 - \frac12 \left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}\right)^2} - 1 = \frac{1}{1 + \left(\frac{\kappa}{\kappa+1}\right)^{2\kappa} \frac{1}{S_j^{2\kappa}} -2 \left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}} - 1\notag\\
    &=& \frac{1}{S_j^\kappa} \frac{2 \left(\frac{\kappa}{\kappa+1}\right)^\kappa -  \left(\frac{\kappa}{\kappa+1}\right)^{\kappa} \frac{1}{S_j^{\kappa}}}{1 + \left(\frac{\kappa}{\kappa+1}\right)^{\kappa} \frac{1}{S_j^{\kappa}} -2 \left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}} \notag\\
    &\leq& \frac{1}{S_j^\kappa} \frac{2 \left(\frac{\kappa}{\kappa+1}\right)^\kappa }{1 - 2 \left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S_j^\kappa}} \leq \frac{1}{S_j^\kappa} \frac{\kappa+1}{\kappa-1}.
\end{eqnarray}
We use the inequality $\frac{1}{1 + x^\kappa} \leq \frac{1}{x}, x > 0$ on \eqref{eq: prob not equal after bernoulli}:
\begin{eqnarray}
\eqref{eq: prob not equal after bernoulli} &\leq& \frac{\left(\frac{1}{4\tilde{\varepsilon}_j^2(S_j)} - 1\right)^\frac1\kappa}{M^\frac1\kappa} \leq \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{1}{M^\frac1p} \cdot \frac{1}{S_j}.\label{eq: S geq bound}   
\end{eqnarray}
For the case $S_j < \frac{\kappa}{(\kappa+1)^{\frac{\kappa-1}{\kappa}}}$, we derive the bound:
\begin{eqnarray}
    \frac{1}{4\tilde{\varepsilon}_j^2(S_j)} - 1 &=& \frac{(\kappa+1)^\frac2\kappa}{S_j^2} - 1 \leq \frac{4}{S_j^2}.
\end{eqnarray}
And we use the inequality $\frac{1}{1 + x^2} \leq \frac{1}{2x}, x > 0$ on \eqref{eq: prob not equal after bernoulli}:
\begin{eqnarray}
\eqref{eq: prob not equal after bernoulli} &\leq& \frac{\sqrt{\frac{1}{4\tilde{\varepsilon}_j^2(S_j)} - 1}}{2\sqrt{M}} \leq \frac{1}{\sqrt{M}} \cdot \frac{1}{S_j}.\label{eq: S leq bound}    
\end{eqnarray}
Combining \eqref{eq: S geq bound} and \eqref{eq: S leq bound} together, we obtain the bound for each coordinate:
 \begin{eqnarray}
     \mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign\left[\sum\limits_{i=1}^M\sign(g^k_{i,j})\right]\right]  \leq \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{1}{\sqrt{M}} \cdot \frac{1}{S_j} = \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{1}{\sqrt{M}} \frac{\sigma_j}{|\nabla f (x^k)_j|}. \label{eq: majority proof P_M bound}
 \end{eqnarray}
The rest of this proof is copying the proof of \algname{SignSGD} Convergence Lemma (Appendix \ref{subsec: signsgd conv}) until the equality \eqref{eq: line with prob sign}. There we replace probability of single estimate with the majority voting and obtain:
$$ \sum_{j=1}^d  |\nabla f (x^k)|_j \cdot \mathbb{P}\left[\sign(\nabla f (x^k)_j) \neq \sign\left[\sum\limits_{i=1}^M\sign(g^k_{i,j})\right]\right]
   \leq  \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{\|\Vec{\sigma}\|_1}{\sqrt{M}} $$ instead of
$$ \sum_{j=1}^d  |\nabla f (x^k)|_j \cdot \mathbb{P}(\sign([\nabla f (x^k))]_j \neq [\sign(g^k)]_j  )
   \leq  \|\Vec{\sigma}\|_1. $$
Hence, the final bound on  sum of $\ell_1$-norm of gradients with probability at least $1 - \delta$ is 
\begin{eqnarray}
    \frac12 \sum\limits_{k=1}^T \gamma_k\|\nabla f (x^k)\|_1 &\leq& f(x^1) - f(x^*) + \frac{Ld}{2}\sum_{k=1}^T\gamma_k^2 + 2\left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \sum_{k=1}^T\gamma_k  \cdot \frac{\|\Vec{\sigma}\|_1}{\sqrt{M}}  \notag \\
    &+& 6d(\gamma^{max} \|\nabla f (x^1)\|_1  + C_TL) \log(\nicefrac{1}{\delta}). \notag 
\end{eqnarray}
Plugging in constant stepsizes $\gamma_k \equiv \gamma$ implies $C_T = T\gamma^2, \gamma^{max} = \gamma$ and denoting $a_\kappa := \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa}$, we have :
\begin{equation}
    \frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta})  + 4 a_\kappa \|\Vec{\sigma}\|_1/\sqrt{M} + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}). \label{eq: majorvote convergence}
\end{equation}
\textbf{1) For arbitrary tuning}, we use parameters $T, \gamma_k = \frac{\gamma_0}{\sqrt{T}}, M_k = \max \{1, M_0T\}$ to get:
$$\frac1T \sum\limits_{k=1}^{T} \|\nabla f (x^k)\|_1 \leq \frac{2\Delta_1}{\sqrt{T}\gamma_0} + 16 \frac{Ld\gamma_0}{\sqrt{T}} \log(\nicefrac{1}{\delta})  + 4 a_\kappa \frac{\|\Vec{\sigma}\|_1}{\sqrt{M_0 T}} + 12\frac{d\|\nabla f (x^1)\|_1}{T}  \log(\nicefrac{1}{\delta}).$$
 Setting such $T$ that the first three terms become less than $\varepsilon$, we obtain the final complexity $N = T \cdot M_0T.$

\textbf{2) For optimal tuning}, we first choose large enough $M$ to bound the term $4a_\kappa \frac{\|\Vec{\sigma}\|_1}{\sqrt{M_k}} \leq \varepsilon/2 \Rightarrow M_k \equiv \max \left\{1,  \left(\frac{8 a_k\|\Vec{\sigma}\|_1}{\varepsilon}\right)^2\right\}$. Then we choose optimal $\gamma = \sqrt{\frac{\Delta_1}{8L_\delta dT}}$ minimizing $ \min_\gamma \left\{\frac{2\Delta_1}{T\gamma} + 16 Ld\gamma \log(\nicefrac{1}{\delta}) \right\}= \sqrt{\frac{128 \Delta_1 Ld \log(\nicefrac{1}{\delta} )}{T}}.$ Finally, $T$ is set to bound $ \sqrt{\frac{128 \Delta_1 Ld \log(\nicefrac{1}{\delta} )}{T}} \leq \varepsilon/2 \Rightarrow T = O\left(\frac{\Delta_1L\log(\nicefrac{1}{\delta} ) d }{\varepsilon^2}\right).$ 
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:dist signsgd conv}]
    This proof completely copies Proof of \algname{minibatch-SignSGD} Complexity Theorem starting with line \eqref{eq: majorvote convergence} and substituting $\|\Vec{\sigma}\|_1$ with $\frac{a_\kappa \|\Vec{\sigma}\|_1}{\sqrt{M}}$.
\end{proof}


\subsection{Proof of \algname{CompSGD} Convergence Lemma \ref{lem: compsgd T update}}\label{subsec: compsgd proof}
\begin{proof}
    Consider the $k$-th step of \algname{CompSGD}. We use smoothness of function $f$ (Lemma \ref{lem: L smooth ineq}) to estimate:
    \begin{eqnarray}
        f(x^{k+1}) - f(x^k) &\leq& \la \nabla f (x^k), x^{k+1} - x^k \ra + \frac{L}{2}\|x^{k+1} - x^k\|_2^2 \notag \\
        &=& - \gamma_k \cdot \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot  \la \nabla f (x^k), \mathbf{e}^k \ra + \frac{L}{2}\gamma_k ^2 \|\mathbf{e}^k\|_2^2 \notag \\
            &\overset{As. \ref{as: D dist}}{\leq}& - \gamma_k  \frac{ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra}{\|\nabla f (x^k)\|_p} \cdot \|\nabla f (x^k)\|_p + \frac{L}{2}\gamma_k ^2.  \notag
    \end{eqnarray}
    Consequently, after summing $T$ steps, we obtain
    \begin{eqnarray}
        \sum \limits_{k=1}^T \gamma_k \frac{\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra}{\|\nabla f (x^k)\|_p} \cdot \|\nabla f (x^k)\|_p  \leq \underset{= \Delta_1}{\underbrace{f(x^1) - f(x^*)}} + \frac{L}{2}\sum \limits_{k=1}^T \gamma_k^2.
    \end{eqnarray}
    Next, we deal with terms  $\phi_k := \frac{\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra}{\|\nabla f (x^k)\|_p}$, $\psi_k := \EE[\phi_k| x^{k }]$ and $D_k := - \gamma_k (\phi_k - \psi_k)\|\nabla f (x^k)\|_p/\alpha_p,$ where $\alpha_p$ is taken from lemma's statement. The terms $\phi_k$ are bounded with $|\phi_k| \leq 1$ due to CauchySchwarz inequality :
    $$|\phi_k| = \frac{|\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra|}{\|\nabla f (x^k)\|_p} \leq \frac{|\la \nabla f (x^k), \mathbf{e}^k \ra|}{\|\nabla f (x^k)\|_p} \leq \|\mathbf{e}^k\|_q \overset{As. \ref{as: D dist}}{\leq} 1.$$
    We note that $D_k$ is a martingale difference sequence ($\EE[D_k|D_{k-1}, \dots, D_1] = 0$) satisfying the inequality
    $$\exp\left( \frac{D_k^2}{4\gamma_k^2\|\nabla f (x^k)\|_p^2/\alpha^2_p}\right) = \exp \left(\frac{(\phi_k - \psi_k)^2}{4}\right) \leq e.$$
    Applying Measure Concentration Lemma \ref{lem: bernstein ineq} to MSD $D_k$ with $\sigma^2_k = 4 \gamma_k^2 \|\nabla f (x^k)\|_p^2/\alpha_p$, we derive the bound for all $\lambda > 0$ with probability at least $1 - \delta:$
    $$\sum\limits_{k=1}^T \gamma_k(\psi_k - 3 \lambda \gamma_k \|\nabla f (x^k)\|_p ) \frac{\|\nabla f (x^k)\|_p}{\alpha_p} \leq \frac{\Delta_1}{\alpha_p} + \frac{L}{2\alpha_p}\sum\limits_{k=1}^T \gamma_k^2 + \frac{1}{\lambda} \log(\nicefrac{1}{\delta})$$
    Next, we use norm relation \eqref{eq: norm relation}, $L$-smoothness (As. \ref{as: smooth})  and update step of \algname{CompSGD} to estimate maximal norm achieved for $k \in \overline{2,T+1}$: 
   \begin{eqnarray}
        \|\nabla f (x^k)\|_p &\leq& d^{\frac1p - \frac12}\|\nabla f (x^k)\|_2 \leq d^{\frac1p - \frac12}\|\nabla f (x^k) - \nabla f (x^{k-1}) + \nabla f (x^{k-1}) \|_2  \notag \\
    &\leq& d^{\frac1p - \frac12}\|\nabla f (x^k) - \nabla f (x^{k-1})\|_2 + d^{\frac1p - \frac12} \|\nabla f (x^{k-1}) \|_2 \leq d^{\frac1p - \frac12} L \|x^k - x^{k-1}\|_2  + d^{\frac1p - \frac12}\|\nabla f (x^{k-1})\|_2  \notag\\
    &\leq& d^{\frac1p - \frac12} L   \gamma_{k-1}  + d^{\frac1p - \frac12}\|\nabla f (x^{k-1})\|_2 \leq d^{\frac1p - \frac12}\|\nabla f (x^1)\|_2 +  d^{\frac1p - \frac12}L\sum_{\tau=1}^{k-1}\gamma_\tau.
    \end{eqnarray}
 Hence, the choice $\lambda := \frac{\alpha_p}{6d^{\frac1p - \frac12}(\gamma^{max} \|\nabla f (x^1)\|_2  + C_TL)}$ yields with probability at least $1 - \delta$:
    \begin{eqnarray}
        \sum\limits_{k=1}^T \gamma_k\left(\frac{\psi_k}{\alpha_p} - \frac{1}{2}\right)\|\nabla f (x^k)\|_p \leq \frac{\Delta_1}{\alpha_p} + \frac{L}{2\alpha_p}\sum_{k=1}^T\gamma_k^2 + \frac{6d^{\frac1p - \frac12}}{\alpha_p}(\gamma^{max} \|\nabla f (x^1)\|_2  + C_TL) \log(\nicefrac{1}{\delta}), \label{eq: comp proof eq prob}
    \end{eqnarray}
where $C_T := \max\limits_{k \in \overline{1,T}} \gamma_k \cdot  \sum\limits_{\tau=1}^{k-1}\gamma_\tau. $
    Finally, we estimate the term $\psi_k \|\nabla f (x^k)\|_p$:
\begin{eqnarray}
&& \EE_{\xi, \mathbf{e}^k} \left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra\right] = \EE_{\mathbf{e}^k} |\la \nabla f (x^k), \mathbf{e}^k \ra| \notag \\
 &-& \EE_{\mathbf{e}^k} \left[2 \cdot \mathbb{P}_\xi\left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra) \right] \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right].\notag
\end{eqnarray}

Next, we consider two cases to deal with probability over $\xi$: $|\la \nabla f (x^k), \mathbf{e}^k \ra| \geq 2\gamma_kL$ and $|\la \nabla f (x^k), \mathbf{e}^k \ra| \leq 2\gamma_kL$.

\textbf{Case $|\la \nabla f (x^k), \mathbf{e}^k \ra| \leq 2\gamma_k L$:} 
\begin{eqnarray}
    \EE_{\xi, \mathbf{e}^k} \left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra\right] &\geq& -\EE_{\mathbf{e}^k} [|\la \nabla f (x^k), \mathbf{e}^k \ra|] \notag  \\
    &\geq& \EE_{\mathbf{e}^k}[|\la \nabla f (x^k), \mathbf{e}^k \ra|] - 4\gamma_k L \notag \\
    &\overset{\text{As. \ref{as: D dist}}}{\geq}& \alpha_p \|\nabla f (x^k)\|_p - 4\gamma_k L.\notag
\end{eqnarray}


\textbf{Case $|\la \nabla f (x^k), \mathbf{e}^k \ra| \geq 2\gamma_kL$:} 

We change sign operators to equivalent ones denoting $\theta^k_+ := f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k + \gamma_k \mathbf{e}^k)$ and $\theta^k_- := f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-) - f(x^k - \gamma_k \mathbf{e}^k)$:
\begin{eqnarray}
    \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) &\neq& \sign(\la \nabla f (x^k), \mathbf{e}^k \ra) \notag  \\
    &\Updownarrow&\notag\\
    \sign(f(x^k + \gamma_k \mathbf{e}^k) - f(x^k - \gamma_k \mathbf{e}^k) + \theta^k_+ - \theta^k_-) &\neq& \sign(2\gamma_k\cdot \la \nabla f (x^k), \mathbf{e}^k \ra). \notag 
\end{eqnarray}
Further, we can bound probability by considering bigger number of cases: 
\begin{eqnarray}
&&\mathbb{P}_\xi\left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra) \right]\label{eq: comp proof EE}\\
    &=&\mathbb{P}_\xi\left[ \sign(f(x^k + \gamma_k \mathbf{e}^k) - f(x^k - \gamma_k \mathbf{e}^k) + \theta_+^k - \theta_-^k) \neq \sign(2\gamma_k \cdot \la \nabla f (x^k), \mathbf{e}^k \ra) \right] \notag \\
    &\leq& \mathbb{P}_\xi \left[ |f(x^k + \gamma_k \mathbf{e}^k) - f(x^k - \gamma_k \mathbf{e}^k) + \theta_+^k - \theta_-^k - 2\gamma_k \cdot \la \nabla f (x^k), \mathbf{e}^k \ra| \geq 2\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right]  \notag \\
    &\leq& \mathbb{P}_\xi \left[ |f(x^k + \gamma_k \mathbf{e}^k) - f(x^k - \gamma_k \mathbf{e}^k) - 2\gamma_k \cdot \la \nabla f (x^k), \mathbf{e}^k \ra|  + |\theta_+^k - \theta_-^k| \geq 2\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right]  \notag \\
    &\leq& \mathbb{P}_\xi \left[ 2L^2\gamma_k^2  + |\theta_+^k - \theta_-^k| \geq 2\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right]. \label{eq: before gamma}
\end{eqnarray}
Since we consider the case $|\la \nabla f (x^k), \mathbf{e}^k \ra| \geq 2\gamma_kL$, then we bound
\begin{eqnarray}
    \eqref{eq: before gamma} &\leq& \mathbb{P}_\xi \left[ \gamma_k\cdot|\la \nabla f (x^k), \mathbf{e}^k \ra|  + |\theta_+^k - \theta_-^k| \geq 2\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right] \notag \\
   & \leq&   \mathbb{P}_\xi\left[  |\theta_+^k - \theta_-^k| \geq \gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right] \notag \\
   &\overset{\text{Markov ineq.} \eqref{eq: Markov}:}{\leq}& \frac{\EE_{\xi}[|\theta_+^k - \theta_-^k| ]}{\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra|} \leq \frac{2 \sigma}{\gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra|} \label{eq: comp proof final p bound}.
\end{eqnarray}
Finally, we have can obtain the bound
\begin{eqnarray}
    \EE_{\xi, \mathbf{e}^k} \left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_+) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_-)) \cdot \la \nabla f (x^k), \mathbf{e}^k \ra\right] &\geq& 
    \EE_{\mathbf{e}^k} |\la \nabla f (x^k), \mathbf{e}^k \ra| - \frac{4\sigma}{\gamma_k}\notag \\
    &\overset{\text{As. \ref{as: D dist}}}{\geq}& \alpha_p \|\nabla f (x^k)\|_p - \frac{4\sigma}{\gamma_k}.\notag 
\end{eqnarray}
Combining two cases together, we get that $\psi_k \|\nabla f (x^k)\|_p \geq \alpha_p \|\nabla f (x^k)\|_p - 4\gamma_k L - \frac{4\sigma}{\gamma_k} $, and the bound follows from \eqref{eq: comp proof eq prob}
   \begin{eqnarray}
       \frac{1}{2} \sum\limits_{k=1}^T \gamma_k\|\nabla f (x^k)\|_p &\leq& \frac{\Delta_1}{\alpha_p} + \frac{L}{2\alpha_p}\sum_{k=1}^T\gamma_k^2 +  \sum_{k=1}^T\gamma_k\cdot \frac{4L\gamma_k}{\alpha_p} +  4\sum_{k=1}^T\frac{\sigma}{\alpha_p} \notag \\
       &+&  \frac{6d^{\frac1p - \frac12}}{\alpha_p}(\gamma^{max} \|\nabla f (x^0)\|_2  + C_TL) \log(\nicefrac{1}{\delta}) \notag 
    \end{eqnarray}
Plugging in constant stepsizes $\gamma_k \equiv \gamma$, $C_T = T\gamma^2, \gamma^{max} = \gamma$  and dividing both sides by $\frac{T \gamma}{2}$ yields the required result:
\begin{eqnarray}
       \frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p &\leq& \frac{2\Delta_1}{T\alpha_p \gamma} + 24d^{\frac1p - \frac12}\frac{L\gamma}{\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{8\sigma}{\alpha_p\gamma}  + \frac{12d^{\frac1p - \frac12} \|\nabla f (x^1)\|_2}{T\alpha_p} \log(\nicefrac{1}{\delta}). \label{eq: compsgd convergence proof}
   \end{eqnarray} \end{proof}
\subsection{Proof  of \algname{minibatch-CompSGD} Complexity Theorem \ref{thm: minibatch stp}}\label{subsec: minibatch-comp proof}
\begin{proof}
We start with \algname{CompSGD} Convergence Lemma \ref{lem: compsgd T update} and constant batchsizes $B$, stepsizes $\gamma$ \eqref{eq: compsgd convergence proof}:
\begin{eqnarray}
       \frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p &\leq& \frac{2\Delta_1}{T\alpha_p \gamma} + 24d^{\frac1p - \frac12}\frac{L\gamma}{\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{8\sigma_B}{\alpha_p\gamma}  + \frac{12d^{\frac1p - \frac12} \|\nabla f (x^1)\|_2}{T\alpha_p} \log(\nicefrac{1}{\delta}). \notag
   \end{eqnarray}
   Due to Batching Lemma \ref{lem: batching p}, we can estimate the $\kappa-$th moment of the batched function as:
   $$\sigma_B \leq \frac{2\sigma}{B^{\frac{\kappa-1}{\kappa}}}.$$
   Hence, we have
\begin{eqnarray}
       \frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p &\leq& \frac{2\Delta_1}{T\alpha_p \gamma} + 24d^{\frac1p - \frac12}\frac{L\gamma}{\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{16\sigma}{B^{\frac{\kappa-1}{\kappa}}\alpha_p\gamma}  + \frac{12d^{\frac1p - \frac12} \|\nabla f (x^1)\|_2}{T\alpha_p} \log(\nicefrac{1}{\delta}). \notag
   \end{eqnarray}
\textbf{Optimal tuning:} 
  $T$ dependency in the first three terms is dominating  in comparison with the last term, hence, we neglect it. Choosing $B$ such that $\frac{\sigma }{\Delta_1 B^\frac{\kappa-1}{\kappa}} \leq \frac{1}{T}$, we have
\begin{eqnarray}
       \frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p &\leq& \frac{18\Delta_1}{T\alpha_p \gamma} + 24d^{\frac1p - \frac12}\frac{L\gamma}{\alpha_p} \log(\nicefrac{1}{\delta}). \notag
   \end{eqnarray}
   With  $\gamma = \sqrt{\frac{\Delta_1}{TL d^{\frac1p - \frac12}}}$, we obtain the required number of iterations $T$ to achieve $\frac{1}{T} \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p \leq \varepsilon$ equals to $\tilde{O}\left(\frac{\Delta_1 L}{\varepsilon^2 \alpha_p}\right).$
\end{proof}

\subsection{Proof of \algname{MajorityVote-CompSGD} Complexity Theorem \ref{thm: MajorVote-SignSTP}}
\begin{proof}
    The beginning of the proof copies the proof of \algname{CompSGD} Convergence Lemma \ref{subsec: compsgd proof} until the line \eqref{eq: comp proof EE} where we instead estimate probability
    $$\mathbb{P}_\xi\left[ \sign\left[\sum_{i=1}^M\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_{i,+}) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_{i,-}))\right] \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra) \right].
    $$
    Each comparison $\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_{i,+}) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_{i,-})) \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra)$ is a Bernoulli trial with failure probability \eqref{eq: comp proof final p bound}:
    $$\mathbb{P}_\xi\left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_{i,+}) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_{i,-})) \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra\right]\leq  \mathbb{P}_\xi\left[  |\theta_{i,+}^k - \theta_{i,-}^k| \geq \gamma_k \cdot |\la \nabla f (x^k), \mathbf{e}^k \ra| \right]. $$
    The right probability can be estimated using Gauss inequality (see Lemma \ref{lem: gauss ineq} in proof \ref{subsec: majority vote sign proofs}) for unimodal symmetric noise $\theta_{i,+}^k - \theta_{i,-}^k$ by \eqref{eq: gauss inequality} with $S = \frac{|\la \nabla f (x^k), \mathbf{e}^k \ra|}{2\sigma}$:
    $$\mathbb{P}_\xi\left[ \sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_{i,+}) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_{i,-})) \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra\right]\leq  \begin{cases} \frac12\left(\frac{\kappa}{\kappa+1}\right)^\kappa \frac{1}{S^\kappa}, & \quad S^\kappa \geq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}} , \\
\frac12 - \frac12\frac{S}{(\kappa+1)^\frac{1}{\kappa} }, &\quad S^\kappa \leq \frac{\kappa^\kappa}{(\kappa+1)^{\kappa-1}}.\notag \end{cases}$$

For the probabilities with upper bounds like this, the resulting probability after $M$ Bernoulli trials can be bounded by  (See proof \ref{subsec: majority vote sign proofs} from \eqref{eq: gauss inequality} until \eqref{eq: majority proof P_M bound}):
\begin{eqnarray}
    \mathbb{P}_\xi\left[ \sign\left[\sum_{i=1}^M\sign(f(x^k + \gamma_k \mathbf{e}^k, \xi^k_{i,+}) - f(x^k - \gamma_k \mathbf{e}^k, \xi^k_{i,-}))\right] \neq \sign(\la \nabla f (x^k), \mathbf{e}^k \ra) \right] &\leq& \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{1}{\sqrt{M}} \cdot \frac{1}{S_j} \notag \\
    &=& \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa} \cdot \frac{1}{\sqrt{M}} \frac{2\sigma}{|\la \nabla f (x^k), \mathbf{e}^k \ra|}, \notag 
\end{eqnarray}
where we denote $a_\kappa := \left(\frac{\kappa+1}{\kappa - 1}\right)^\frac{1}{\kappa}.$ The rest of the proof copies the proof of \algname{CompSGD} Convergence Lemma \ref{subsec: compsgd proof} with substitution $\sigma \to \frac{a_\kappa\sigma}{\sqrt{M}}$, and we obtain the bound:
\begin{eqnarray}
       \frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p &\leq& \frac{2\Delta_1}{T\alpha_p \gamma} + 24d^{\frac1p - \frac12}\frac{L\gamma}{\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{16a_\kappa \sigma}{\sqrt{M}\alpha_p\gamma}  + \frac{12d^{\frac1p - \frac12} \|\nabla f (x^1)\|_2}{T\alpha_p} \log(\nicefrac{1}{\delta}). \notag
   \end{eqnarray}
The last term converges much faster than other, hence, we neglect it. We also use notation $L_{\delta,p} = d^{\frac1p - \frac12} L \log(\nicefrac{1}{\delta}).$ 

\textbf{1) For arbitrary tuning}, we use parameters $T, \gamma_k = \frac{\gamma_0}{\sqrt{T}}, M_k = \max \{1, M_0T^2\}$ to get:
$$\frac1T \sum\limits_{k=1}^T \|\nabla f (x^k)\|_p \leq \frac{2\Delta_1}{\sqrt{T}\alpha_p \gamma_0} + 24d^{\frac1p - \frac12}\frac{L\gamma_0}{\sqrt{T}\alpha_p} \log(\nicefrac{1}{\delta}) +   \frac{16a_\kappa \sigma}{\sqrt{M_0}\alpha_p\gamma_0\sqrt{T}}  + \frac{12d^{\frac1p - \frac12} \|\nabla f (x^1)\|_2}{T\alpha_p} \log(\nicefrac{1}{\delta}).$$
 Setting such $T$ that the first three terms become less than $\varepsilon$, we obtain the final complexity $N = T \cdot M_0T^2.$

\textbf{2) For optimal tuning}, we first choose large enough $M$ to bound the term $ \frac{16a_\kappa\sigma}{\sqrt{M_k}\alpha_p \gamma} \leq \varepsilon/2 \Rightarrow M_k \equiv \max \left\{1,  \left(\frac{32 a_\kappa\sigma}{\alpha_p\varepsilon\gamma}\right)^2\right\}$. Then we choose optimal $\gamma = \sqrt{\frac{\Delta_1}{12L_{\delta,p} T}}$ minimizing $ \min_\gamma \frac{1}{\alpha_p}\left\{\frac{2\Delta_1}{T\gamma} + 24 L_{\delta,p}\gamma  \right\}= \frac{1}{\alpha_p}\sqrt{\frac{48 \Delta_1 L_{\delta,p}}{T}}.$ Finally, $T$ is set to bound $ \frac{1}{\alpha_p}\sqrt{\frac{48 \Delta_1 L_{\delta,p}}{T}} \leq \varepsilon/2 \Rightarrow T = O\left(\frac{\Delta_1L_{\delta,p} }{\alpha_p^2\varepsilon^2}\right).$
\end{proof}




\section{Experimental details}
\subsection{LLaMA 130M pre-training on C4}\label{app:pre-training}

We adopted a LLaMA-based architecture~\citep{llama} with RMSNorm~\citep{rmsnorm} and SwiGLU~\citep{shazeer2020glu} activations on the C4 dataset~\citep{c4}. 
Following~\citet{relora}, we used a batch size of 512 sequences and a sequence length of 256. 
We used a T5 tokenizer, since it was also trained on C4 with dictionary size equal to 32k.
We trained the model for 100k steps.

For all experiments, while the main model parameters use the respective optimization method, the LM head layer is optimized with AdamW~\citep{loshchilov2017decoupled}. 
This follows prior work~\citet{anything_but_sgd} which demonstrated that the LM head layer requires more fine-grained effective learning rate adaptation across different tokens for optimal performance.
We used the Nesterov acceleration scheme with a momentum value of 0.9 for all methods except AdamW.
For AdamW, we used standard hyperparameters: $\beta_1 = 0.9, \beta_2 = 0.999, \varepsilon=$1e-8. 

We selected the learning rate through a grid search with multiplicative step of $10^{\frac{1}{4}}$ (LM head layer optimized with AdamW and learning rate equal to 1e-3).
We used a cosine learning rate schedule with a warmup of 10\% of the total number of steps and decay of the final learning rate down to 10\% of the peak learning rate.
% For all methods except \algname{M-NSGD} we used gradient clipping with a threshold of 1.0.
In addition, we selected the best weight decay value between [0, 0.01, 0.1].

The final best hyperparameters are shown in~\Cref{tab:hyperparams}.

\renewcommand{\arraystretch}{1.1}

\begin{table}[t!]
    \caption{LLaMA 130m pre-raining hyperparameters.}
    \label{tab:hyperparams}
    % \vspace{-3mm}
    \begin{center}
        \begin{tabular}{|l|c|c|c|c|}
        \hline
        \textbf{Method} & \algname{M-ClippedSGD} & \algname{M-NSGD} & \algname{M-SignSGD} & \algname{AdamW} \\
        \hline
        \textbf{Learning rate} & $10^{1.5}$ & $10^{0}$ & $10^{-2.75}$ & $10^{-3}$\\
        \hline
        \textbf{Gradient clipping} & $0.03125$ & - & -  & 1.0 \\
        \hline
        \textbf{Weight decay} & $0$ & $0$ & $0.01$ & $0.01$\\
        \hline
        \end{tabular}
    \end{center}
    % \vspace{-1.em}
\end{table}


\subsection{RoBERTa large fine-tuning}\label{app:fine-tuning}

For these experiments, we follow~\citet{setup_finetuning} for the prompt-based fine-tuning paradigm for masked language models and reuse training hyperparameters from~\citet{mezo}. Please refer to the original papers for more details. We compare methods in few-shot scenario with $k=16$ examples.

For \algname{CompSGD}~\Cref{alg:comp-sinSGD}, we sampled $\mathbf{e}^k$ from scaled Euclidian sphere, i.e. $\alpha\cdot S_2^d = \{\mathbf{e}| \|\mathbf{e}\|_2 = \alpha\}.$ 
We set $\alpha$ equal to $17$ for all datasets and selected the learning rate in [0.3, 1.0, 3.0] based on validation score.

\end{document}


\section{Restarted methods for strongly convex functions}

In this section, we use restart technique to obtain better convergence bounds for strongly convex functions.

\begin{assumption}[Strong Convexity]\label{as: strong convex}
    The objective smooth function $f$ is $\mu$-strongly convex, i.e.,
    $$f(y) - f(x) - \la \nabla f(x), y - x \ra \geq \frac{\mu}{2}\|y - x\|_2^2, \quad \forall x, y \in \R^d. $$
    If $\mu = 0$, the function is called simply "convex."
\end{assumption}
 Restart technique runs several iterations of a base algorithm and then restarts it using previous output as an initial point. Thus, restarts sustain faster convergence rate inherent to the beginning of an optimization procedure and avoids huge batches with fewer iterations. Strong convexity  allows to use an average point instead of random one, and then  estimate function accuracy instead of gradient norm. 

\begin{algorithm}[ht!]
\caption{\algname{Restarted-$\mathcal{A}$} }
\label{alg:restart}   
\begin{algorithmic}[1]
\REQUIRE Starting point $x^0 \in \R^d$, base algorithm $\mathcal{A}$, number of restarts $R$, number of iterations $\{T_r\}_{r=1}^R$, base algorithm parameters $\{\theta_r\}_{r=1}^R$.

\FOR{$r=1,\ldots, R$}
\STATE Run $T_r$ iterations of base algorithm $\mathcal{A}$ with initial point $x^{r-1}$ and parameters $\theta_r$.
\STATE Average interim points from the previous run $x^{r} = \frac{1}{T_r} \sum\limits_{i=1}^{T_r} x_r^i$.
\ENDFOR
\ENSURE $x^R$. 
\end{algorithmic}
\end{algorithm}

\begin{theorem}\label{thm: restarts}
    For bounded $L$-smooth $\mu$-strongly convex function $f$ (As. \ref{as: bounded}, \ref{as: smooth}, \ref{as: strong convex}) with heavy-tailed noise (As. \ref{as: pBCM} or \ref{as: function noise xi}), \algname{Restarted-$\mathcal{A}$} requires sample complexity $N$ to achieve $f(x^R) - f(x^*) \leq \varepsilon$ with probability at least $1-\delta$ starting with $\Delta_1 = f(x^0) - f^*$:

    \algname{minibatch-SignSGD}: $R = \log_2 \frac{\Delta_1}{\varepsilon}, T_r = \tilde{O} \left(\frac{L^2d^2}{\mu^2}\right), \gamma_r = \sqrt{\nicefrac{\Delta_1}{(2^{r-1}LdT_r)}}, B_r = \max\{1, \left(\nicefrac{2^r \sigma}{(\Delta_1\mu)}\right)^\frac{\kappa}{\kappa - 1} \}$:
    $$N = \tilde{O} \left(\frac{Ld}{\varepsilon\mu^2} + \frac{Ld^2}{ \varepsilon\mu^2}\cdot \left( \frac{\sigma}{\varepsilon\mu}\right)^\frac{\kappa}{\kappa - 1 } \right).$$
    %\algname{M-SignSGD} (in expectation): $R = \log_2 \frac{\Delta_1}{\varepsilon}, T_r = \tilde{O} \left(\frac{2^rL}{\Delta_1 \mu^2 } +  \frac{2^rL}{\Delta_1 \mu^2 }\left(\frac{2^r\sigma\sqrt{d}}{\Delta_1\mu} \right)^\frac{\kappa}{\kappa - 1}\right), \beta_r = 1 - \min\left\{1, \frac{1}{\sigma^2} \left( \frac{\Delta_1 L}{2^{r-1 }T_r}\right)^\frac{\kappa}{3\kappa -2 }\right\},\gamma_r = \sqrt{\frac{\Delta_1 (1 - \beta_r)}{2^{r-1}T_rLd}}$:
%$$N = \tilde{O} \left(\frac{L}{ \varepsilon \mu^2} +  \frac{L}{\varepsilon \mu^2 }\left(\frac{\sigma\sqrt{d}}{\varepsilon\mu} \right)^\frac{\kappa}{\kappa - 1}\right).$$
    \algname{minibatch-SignSTP}: $R = \log_2 \frac{\Delta_1}{\varepsilon}, T_r = \tilde{O} \left(\frac{2^rL\beta_2^2 }{\mu^2 \alpha_p^2 \Delta_1}\right), \gamma_r = \sqrt{\frac{\Delta_1}{2^{r-1}T_rL\beta_2^2}}, B_r = \max\{1, \left(\nicefrac{2^{2r} \sigma L\beta_2^2 }{(\alpha_p\Delta_1\mu)^2}\right)^\frac{\kappa}{\kappa - 1} \}$:
    $$N = \tilde{O} \left(\frac{L\beta_2^2 }{\alpha_p^2\varepsilon\mu^2} + \frac{L\beta_2^2 }{\alpha_p^2\varepsilon\mu^2} \cdot \left( \frac{\sigma L \beta_2^2 }{\alpha_p^2 \varepsilon^2 \mu^2}\right)^\frac{\kappa}{\kappa - 1}\right).$$
\end{theorem}

