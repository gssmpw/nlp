@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17455--17466},
  year={2021}
}

@article{armacki2023high,
  title={High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise},
  author={Armacki, Aleksandar and Sharma, Pranay and Joshi, Gauri and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.18784},
  year={2023}
}

@article{armacki2024large,
  title={Large Deviations and Improved Mean-squared Error Rates of Nonlinear SGD: Heavy-tailed Noise and Power of Symmetry},
  author={Armacki, Aleksandar and Yu, Shuhua and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2410.15637},
  year={2024}
}

@inproceedings{barakat2023reinforcement,
  title={Reinforcement learning with general utilities: Simpler variance reduction and large state-action space},
  author={Barakat, Anas and Fatkhullin, Ilyas and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={1753--1800},
  year={2023},
  organization={PMLR}
}

@article{bernstein2018majorityvote,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}

@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International conference on machine learning},
  pages={2260--2268},
  year={2020},
  organization={PMLR}
}

@article{cutkosky2021high,
  title={High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}

@inproceedings{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1594--1602},
  year={2015}
}

@article{hubler2024gradient,
  title={From Gradient Clipping to Normalization for Heavy Tailed SGD},
  author={H{\"u}bler, Florian and Fatkhullin, Ilyas and He, Niao},
  journal={arXiv preprint arXiv:2410.13849},
  year={2024}
}

@article{jin2020stochastic,
  title={Stochastic-sign SGD for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}

@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for byzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={5311--5319},
  year={2021},
  organization={PMLR}
}

@article{kornilov2024accelerated,
  title={Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite variance},
  author={Kornilov, Nikita and Shamir, Ohad and Lobanov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander and Shibaev, Innokentiy and Gorbunov, Eduard and Horv{\'a}th, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{liu2019signsgd,
  title={signSGD via zeroth-order oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26204--26217},
  year={2022}
}

@inproceedings{liu2023breaking,
  title={Breaking the lower bound with (little) structure: Acceleration in non-convex stochastic optimization with heavy-tailed noise},
  author={Liu, Zijian and Zhang, Jiawei and Zhou, Zhengyuan},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2266--2290},
  year={2023},
  organization={PMLR}
}

@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}

@article{nazin2019algorithms,
  title={Algorithms of robust stochastic optimization based on mirror descent method},
  author={Nazin, Aleksandr Viktorovich and Nemirovsky, AS and Tsybakov, Aleksandr Borisovich and Juditsky, AB},
  journal={Automation and Remote Control},
  volume={80},
  number={9},
  pages={1607--1627},
  year={2019},
  publisher={Springer}
}

@article{nguyen2023high,
  title={High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise},
  author={Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy Le},
  journal={arXiv preprint arXiv:2302.05437},
  year={2023}
}

@article{nguyen2023improved,
  title={Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tails},
  author={Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2304.01119},
  year={2023}
}

@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}

@inproceedings{puchkin2024breaking,
  title={Breaking the heavy-tailed noise barrier in stochastic optimization problems},
  author={Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={856--864},
  year={2024},
  organization={PMLR}
}

@article{qin2025high,
  title={High Probability Convergence of Clipped Distributed Dual Averaging With Heavy-Tailed Noises},
  author={Qin, Yanfu and Lu, Kaihong and Xu, Hang and Chen, Xiangyong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  year={2025},
  publisher={IEEE}
}

@article{sadiev2023high,
  title={High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2302.00999},
  year={2023}
}

@inproceedings{safaryan2021stochastic,
  title={Stochastic sign descent methods: New algorithms and better theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={9224--9234},
  year={2021},
  organization={PMLR}
}

@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  volume={2014},
  pages={1058--1062},
  year={2014},
  organization={Singapore}
}

@inproceedings{sun2023momentum,
  title={Momentum ensures convergence of signsgd under weaker assumptions},
  author={Sun, Tao and Wang, Qingsong and Li, Dongsheng and Wang, Bao},
  booktitle={International Conference on Machine Learning},
  pages={33077--33099},
  year={2023},
  organization={PMLR}
}

@article{yang2024two,
  title={Two sides of one coin: the limits of untuned SGD and the power of adaptive methods},
  author={Yang, Junchi and Li, Xiang and Fatkhullin, Ilyas and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{zhang2020adaptivegood,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}

