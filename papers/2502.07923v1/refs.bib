@book{rachev2003handbook,
  title={Handbook of heavy tailed distributions in finance: Handbooks in finance, Book 1},
  author={Rachev, Svetlozar Todorov},
  year={2003},
  publisher={Elsevier}
}

@inproceedings{foster2019complexity,
  title={The complexity of making the gradient small in stochastic convex optimization},
  author={Foster, Dylan J and Sekhari, Ayush and Shamir, Ohad and Srebro, Nathan and Sridharan, Karthik and Woodworth, Blake},
  booktitle={Conference on Learning Theory},
  pages={1319--1345},
  year={2019},
  organization={PMLR}
}

@article{dharmadhikari1986gauss,
  title={The Gauss--Tchebyshev inequality for unimodal distributions},
  author={Dharmadhikari, Sudhakar W and Joag-Dev, Kumar},
  journal={Theory of Probability \& Its Applications},
  volume={30},
  number={4},
  pages={867--871},
  year={1986},
  publisher={SIAM}
}

@inproceedings{cherapanamjeri2022optimal,
  title={Optimal mean estimation without a variance},
  author={Cherapanamjeri, Yeshwanth and Tripuraneni, Nilesh and Bartlett, Peter and Jordan, Michael},
  booktitle={Conference on Learning Theory},
  pages={356--357},
  year={2022},
  organization={PMLR}
}
@article{amari1993backpropagation,
  title={Backpropagation and stochastic gradient descent method},
  author={Amari, Shun-ichi},
  journal={Neurocomputing},
  volume={5},
  number={4-5},
  pages={185--196},
  year={1993},
  publisher={Elsevier}
}

@article{hubler2024gradient,
  title={From Gradient Clipping to Normalization for Heavy Tailed SGD},
  author={H{\"u}bler, Florian and Fatkhullin, Ilyas and He, Niao},
  journal={arXiv preprint arXiv:2410.13849},
  year={2024}
}
@inproceedings{puchkin2024breaking,
  title={Breaking the heavy-tailed noise barrier in stochastic optimization problems},
  author={Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov, Alexander},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={856--864},
  year={2024},
  organization={PMLR}
}
@article{bernstein2018majorityvote,
  title={signSGD with majority vote is communication efficient and fault tolerant},
  author={Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima},
  journal={arXiv preprint arXiv:1810.05291},
  year={2018}
}

@inproceedings{bernstein2018signsgd,
  title={signSGD: Compressed optimisation for non-convex problems},
  author={Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree},
  booktitle={International Conference on Machine Learning},
  pages={560--569},
  year={2018},
  organization={PMLR}
}
@article{jin2020stochastic,
  title={Stochastic-sign SGD for federated learning with theoretical guarantees},
  author={Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu},
  journal={arXiv preprint arXiv:2002.10940},
  year={2020}
}
@inproceedings{safaryan2021stochastic,
  title={Stochastic sign descent methods: New algorithms and better theory},
  author={Safaryan, Mher and Richt{\'a}rik, Peter},
  booktitle={International Conference on Machine Learning},
  pages={9224--9234},
  year={2021},
  organization={PMLR}
}
















@article{jakovetic2023nonlinear,
  title={Nonlinear gradient mappings and stochastic optimization: A general framework with applications to heavy-tail noise},
  author={Jakovetic, Dusan and Bajovic, Dragana and Sahu, Anit Kumar and Kar, Soummya and Milosevich, Nemanja and Stamenkovic, Dusan},
  journal={SIAM Journal on Optimization},
  volume={33},
  number={2},
  pages={394--423},
  year={2023},
  publisher={SIAM}
}

@article{kornilov2023gradient,
  title={Gradient-free methods for non-smooth convex stochastic optimization with heavy-tailed noise on convex compact},
  author={Kornilov, Nikita and Gasnikov, Alexander and Dvurechensky, Pavel and Dvinskikh, Darina},
  journal={Computational Management Science},
  volume={20},
  number={1},
  pages={37},
  year={2023},
  publisher={Springer}
}

@article{gorbunov2022clipped,
  title={Clipped stochastic methods for variational inequalities with heavy-tailed noise},
  author={Gorbunov, Eduard and Danilova, Marina and Dobre, David and Dvurechenskii, Pavel and Gasnikov, Alexander and Gidel, Gauthier},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={31319--31332},
  year={2022}
}

@article{gorbunov2023high,
  title={High-Probability Convergence for Composite and Distributed Stochastic Minimization and Variational Inequalities with Heavy-Tailed Noise},
  author={Gorbunov, Eduard and Sadiev, Abdurakhmon and Danilova, Marina and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2310.01860},
  year={2023}
}

@misc{ermoliev1976stochastic,
  title={Stochastic programming methods},
  author={Ermoliev, Yu},
  year={1976},
  publisher={Nauka}
}
@article{pasechnyuk2023upper,
  title={Upper bounds on maximum admissible noise in zeroth-order optimisation},
  author={Pasechnyuk, Dmitry A and Lobanov, Aleksandr and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2306.16371},
  year={2023}
}
@article{risteski2016algorithms,
  title={Algorithms and matching lower bounds for approximately-convex optimization},
  author={Risteski, Andrej and Li, Yuanzhi},
  journal={Advances in Neural Information Processing Systems},
  volume={29},
  year={2016}
}

@article{nesterov2017random,
  title={Random gradient-free minimization of convex functions},
  author={Nesterov, Yurii and Spokoiny, Vladimir},
  journal={Foundations of Computational Mathematics},
  volume={17},
  pages={527--566},
  year={2017},
  publisher={Springer}
}

@inproceedings{gurbuzbalaban2021fractional,
  title={Fractional moment-preserving initialization schemes for training deep neural networks},
  author={Gurbuzbalaban, Mert and Hu, Yuanhan},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2233--2241},
  year={2021},
  organization={PMLR}
}


@article{dorn2023implicitly,
  title={Implicitly normalized forecaster with clipping for linear and non-linear heavy-tailed multi-armed bandits},
  author={Dorn, Yuriy and Nikita, Kornilov and Kutuzov, Nikolay and Nazin, Alexander and Gorbunov, Eduard and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2305.06743},
  year={2023}
}

@article{flaxman2004online,
  title={Online convex optimization in the bandit setting: gradient descent without a gradient},
  author={Flaxman, Abraham D and Kalai, Adam Tauman and McMahan, H Brendan},
  journal={arXiv preprint cs/0408007},
  year={2004}
}



@inproceedings{simsekli2019tail,
  title={A tail-index analysis of stochastic gradient noise in deep neural networks},
  author={Simsekli, Umut and Sagun, Levent and Gurbuzbalaban, Mert},
  booktitle={International Conference on Machine Learning},
  pages={5827--5837},
  year={2019},
  organization={PMLR}
}

@inproceedings{bartlett2008high,
  title={High-probability regret bounds for bandit online linear optimization},
  author={Bartlett, Peter and Dani, Varsha and Hayes, Thomas and Kakade, Sham and Rakhlin, Alexander and Tewari, Ambuj},
  booktitle={Proceedings of the 21st Annual Conference on Learning Theory-COLT 2008},
  pages={335--342},
  year={2008},
  organization={Omnipress}
}
@article{bubeck2012regret,
author = {S\'ebastien Bubeck and Nicol\'o Cesa-Bianchi},
title = {Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems},
url = {http://dx.doi.org/10.1561/2200000024},
year = {2012},
volume = {5},
journal = {Foundations and TrendsÂ® in Machine Learning},
doi = {10.1561/2200000024},
issn = {1935-8237},
number = {1},
pages = {1-122}
}
@article{wang2021convergence,
  title={Convergence rates of stochastic gradient descent under infinite noise variance},
  author={Wang, Hongjian and Gurbuzbalaban, Mert and Zhu, Lingjiong and Simsekli, Umut and Erdogdu, Murat A},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={18866--18877},
  year={2021}
}

@article{larson2019derivative,
  title={Derivative-free optimization methods},
  author={Larson, Jeffrey and Menickelly, Matt and Wild, Stefan M},
  journal={Acta Numerica},
  volume={28},
  pages={287--404},
  year={2019},
  publisher={Cambridge University Press}
}

@article{shamir2017optimal,
  title={An optimal algorithm for bandit and zero-order convex optimization with two-point feedback},
  author={Shamir, Ohad},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={1703--1713},
  year={2017},
  publisher={JMLR. org}
}


@article{duchi2015optimal,
  title={Optimal rates for zero-order convex optimization: The power of two function evaluations},
  author={Duchi, John C and Jordan, Michael I and Wainwright, Martin J and Wibisono, Andre},
  journal={IEEE Transactions on Information Theory},
  volume={61},
  number={5},
  pages={2788--2806},
  year={2015},
  publisher={IEEE}
}

@article{bubeck2019complexity,
  title={Complexity of highly parallel non-smooth convex optimization},
  author={Bubeck, S{\'e}bastien and Jiang, Qijia and Lee, Yin-Tat and Li, Yuanzhi and Sidford, Aaron},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}


@article{bayandina2018gradient,
  title={Gradient-free two-point methods for solving stochastic nonsmooth convex optimization problems with small non-random noises},
  author={Bayandina, Anastasia Sergeevna and Gasnikov, Alexander V and Lagunovskaya, Anastasia A},
  journal={Automation and Remote Control},
  volume={79},
  pages={1399--1408},
  year={2018},
  publisher={Springer}
}

@article{nguyen2023high,
  title={High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise},
  author={Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy Le},
  journal={arXiv preprint arXiv:2302.05437},
  year={2023}
}

@article{nguyen2023improved,
  title={Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tails},
  author={Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy L},
  journal={arXiv preprint arXiv:2304.01119},
  year={2023}
}


@article{Gorbunov2020StochasticOW,
  title={Stochastic Optimization with Heavy-Tailed Noise via Accelerated Gradient Clipping},
  author={Eduard A. Gorbunov and Marina Danilova and Alexander V. Gasnikov},
  journal={ArXiv},
  year={2020},
  volume={abs/2005.10785}
}

@book{spall2005introduction,
  title={Introduction to stochastic search and optimization: estimation, simulation, and control},
  author={Spall, James C},
  year={2005},
  publisher={John Wiley \& Sons}
}
@article{gorbunov2019upper,
  title={On the Upper Bound for the Expectation of the Norm of a Vector Uniformly Distributed on the Sphere and the Phenomenon of Concentration of Uniform Measure on the Sphere.},
  author={Gorbunov, EA and Vorontsova, Evgeniya Alekseevna and Gasnikov, Alexander Vladimirovich},
  journal={Mathematical Notes},
  volume={106},
  year={2019}
}
@article{akhavan2022gradient,
  title={A gradient estimator via L1-randomization for online zero-order optimization with two point feedback},
  author={Akhavan, Arya and Chzhen, Evgenii and Pontil, Massimiliano and Tsybakov, Alexandre B},
  journal={arXiv preprint arXiv:2205.13910},
  year={2022}
}
@article{ledoux2005concentration,
  title={The Concentration of Measure Phenomenon. Ed. by Peter Landweber et al. Vol. 89},
  author={Ledoux, Michel},
  journal={Mathematical Surveys and Monographs. Providence, Rhode Island: American Mathematical Society},
  pages={181},
  year={2005}
}
@article{nemirovsky1979problem,
  title={Problem complexity and optimization method efficiency},
  author={Nemirovsky, AS and Yudin, DB},
  journal={M.: Nauka},
  year={1979}
}

@article{gasnikov2022randomized,
  title={Randomized gradient-free methods in convex optimization},
  author={Gasnikov, Alexander and Dvinskikh, Darina and Dvurechensky, Pavel and Gorbunov, Eduard and Beznosikov, Aleksander and Lobanov, Alexander},
  journal={arXiv preprint arXiv:2211.13566},
  year={2022}
}


@article{Lobanov_2022,
  title={Gradient-free federated learning methods with l\_1 and l\_2-randomization for non-smooth convex stochastic optimization problems},
  author={Alashkar, BA and Gasnikov, Alexander Vladimirovich and Dvinskikh, Darina Mikhailovna and Lobanov, Aleksandr Vladimirovich},
  journal={Zhurnal Vychislitel'noi Matematiki i Matematicheskoi Fiziki},
  volume={63},
  number={9},
  pages={1458--1512},
  year={2023},
  publisher={Russian Academy of Sciences, Branch of Mathematical Sciences}
}


@article{zhang2022parameter,
  title={Parameter-free regret in high probability with heavy tails},
  author={Zhang, Jiujia and Cutkosky, Ashok},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={8000--8012},
  year={2022}
}

@article{gasnikov2018universal,
  title={Universal method for stochastic composite optimization problems},
  author={Gasnikov, Alexander Vladimirovich and Nesterov, Yu E},
  journal={Computational Mathematics and Mathematical Physics},
  volume={58},
  pages={48--64},
  year={2018},
  publisher={Springer}
}
@article{lan2012validation,
  title={Validation analysis of mirror descent stochastic approximation method},
  author={Lan, Guanghui and Nemirovski, Arkadi and Shapiro, Alexander},
  journal={Mathematical programming},
  volume={134},
  number={2},
  pages={425--458},
  year={2012},
  publisher={Springer}
}
@inproceedings{vural2022mirror,
  title={Mirror descent strikes again: Optimal stochastic convex optimization under infinite noise variance},
  author={Vural, Nuri Mert and Yu, Lu and Balasubramanian, Krishna and Volgushev, Stanislav and Erdogdu, Murat A},
  booktitle={Conference on Learning Theory},
  pages={65--102},
  year={2022},
  organization={PMLR}
}
@article{sadiev2023high,
  title={High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance},
  author={Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2302.00999},
  year={2023}
}
@article{polyak1980optimal,
  title={Optimal pseudogradient adaptation algorithms},
  author={Polyak, Boris Teodorovich and Tsypkin, Yakov Zalmanovich},
  journal={Avtomatika i Telemekhanika},
  number={8},
  pages={74--84},
  year={1980},
  publisher={Russian Academy of Sciences, Branch of Power Industry, Machine Building~â¦}
}

@article{jakovetic2022nonlinear,
  title={Nonlinear gradient mappings and stochastic optimization: A general framework with applications to heavy-tail noise},
  author={Jakovetic, Dusan and Bajovic, Dragana and Sahu, Anit Kumar and Kar, Soummya and Milosevic, Nemanja and Stamenkovic, Dusan},
  journal={arXiv preprint arXiv:2204.02593},
  year={2022}
}



@inproceedings{gasnikov2022power,
  title={The power of first-order smooth optimization for black-box non-smooth problems},
  author={Gasnikov, Alexander and Novitskii, Anton and Novitskii, Vasilii and Abdukhakimov, Farshed and Kamzolov, Dmitry and Beznosikov, Aleksandr and Takac, Martin and Dvurechensky, Pavel and Gu, Bin},
  booktitle={International Conference on Machine Learning},
  pages={7241--7265},
  year={2022},
  organization={PMLR}
}

@article{zhang2022lower,
  title={On lower iteration complexity bounds for the convex concave saddle point problems},
  author={Zhang, Junyu and Hong, Mingyi and Zhang, Shuzhong},
  journal={Mathematical Programming},
  volume={194},
  number={1-2},
  pages={901--935},
  year={2022},
  publisher={Springer}
}

@article{ouyang2021lower,
  title={Lower complexity bounds of first-order methods for convex-concave bilinear saddle-point problems},
  author={Ouyang, Yuyuan and Xu, Yangyang},
  journal={Mathematical Programming},
  volume={185},
  number={1},
  pages={1--35},
  year={2021},
  publisher={Springer}
}

@article{nemirovskij1983problem,
  title={Problem complexity and method efficiency in optimization},
  author={Nemirovskij, Arkadij Semenovi{\v{c}} and Yudin, David Borisovich},
  year={1983},
journal={M.: Nauka},
  publisher={Wiley-Interscience}
}

@article{yue2022lower,
  title={On the Lower Bound of Minimizing {P}olyak-{L}ojasiewicz functions},
  author={Yue, Pengyun and Fang, Cong and Lin, Zhouchen},
  journal={arXiv preprint arXiv:2212.13551},
  year={2022}
}

@inproceedings{karimi2016linear,
  title={Linear convergence of gradient and proximal-gradient methods under the {P}olyak-{L}ojasiewicz condition},
  author={Karimi, Hamed and Nutini, Julie and Schmidt, Mark},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={795--811},
  year={2016},
  organization={Springer}
}

@article{khaled2020better,
  title={Better theory for SGD in the nonconvex world},
  author={Khaled, Ahmed and Richt{\'a}rik, Peter},
  journal={arXiv preprint arXiv:2002.03329},
  year={2020}
}

@inproceedings{abadi2016deep,
  title={Deep learning with differential privacy},
  author={Abadi, Martin and Chu, Andy and Goodfellow, Ian and McMahan, H Brendan and Mironov, Ilya and Talwar, Kunal and Zhang, Li},
  booktitle={Proceedings of the 2016 ACM SIGSAC conference on computer and communications security},
  pages={308--318},
  year={2016}
}

@inproceedings{karimireddy2021learning,
  title={Learning from history for byzantine robust optimization},
  author={Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={5311--5319},
  year={2021},
  organization={PMLR}
}

@article{arjevani2022lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  pages={1--50},
  year={2022},
  publisher={Springer}
}

@article{li2020high,
  title={A high probability analysis of adaptive SGD with momentum},
  author={Li, Xiaoyu and Orabona, Francesco},
  journal={arXiv preprint arXiv:2007.14294},
  year={2020}
}

@article{necoara2019linear,
  title={Linear convergence of first order methods for non-strongly convex optimization},
  author={Necoara, Ion and Nesterov, Yu and Glineur, Francois},
  journal={Mathematical Programming},
  volume={175},
  number={1},
  pages={69--107},
  year={2019},
  publisher={Springer}
}

@article{liu2023stochastic,
  title={Stochastic Nonsmooth Convex Optimization with Heavy-Tailed Noises},
  author={Liu, Zijian and Zhou, Zhengyuan},
  journal={arXiv preprint arXiv:2303.12277},
  year={2023}
}

@article{liu2022loss,
  title={Loss landscapes and optimization in over-parameterized non-linear systems and neural networks},
  author={Liu, Chaoyue and Zhu, Libin and Belkin, Mikhail},
  journal={Applied and Computational Harmonic Analysis},
  volume={59},
  pages={85--116},
  year={2022},
  publisher={Elsevier}
}

@article{lojasiewicz1963topological,
  title={A topological property of real analytic subsets},
  author={Lojasiewicz, Stanislaw},
  journal={Coll. du CNRS, Les {\'e}quations aux d{\'e}riv{\'e}es partielles},
  volume={117},
  number={87-89},
  pages={2},
  year={1963}
}

@article{polyak1963gradient,
  title={Gradient methods for the minimisation of functionals},
  author={Polyak, Boris T},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={3},
  number={4},
  pages={864--878},
  year={1963},
  publisher={Elsevier}
}

@article{dvurechenskii2018decentralize,
  title={Decentralize and randomize: Faster algorithm for Wasserstein barycenters},
  author={Dvurechenskii, Pavel and Dvinskikh, Darina and Gasnikov, Alexander and Uribe, Cesar and Nedich, Angelia},
  journal={Advances in Neural Information Processing Systems},
  volume={31},
  year={2018}
}

@article{gasnikov2016universal,
  title={Universal fast gradient method for stochastic composit optimization problems},
  author={Gasnikov, Alexander and Nesterov, Yurii},
  journal={arXiv preprint arXiv:1604.05275},
  year={2016}
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press}
}


@inproceedings{loizou2020stochastic,
  title={Stochastic hamiltonian gradient methods for smooth games},
  author={Loizou, Nicolas and Berard, Hugo and Jolicoeur-Martineau, Alexia and Vincent, Pascal and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={International Conference on Machine Learning},
  pages={6370--6381},
  year={2020},
  organization={PMLR}
}

@article{abernethy2019last,
  title={Last-iterate convergence rates for min-max optimization},
  author={Abernethy, Jacob and Lai, Kevin A and Wibisono, Andre},
  journal={arXiv preprint arXiv:1906.02027},
  year={2019}
}

@article{bohm2022solving,
  title={Solving Nonconvex-Nonconcave Min-Max Problems exhibiting Weak Minty Solutions},
  author={B{\"o}hm, Axel},
  journal={arXiv preprint arXiv:2201.12247},
  year={2022}
}

@article{lee2021fast,
  title={Fast extra gradient methods for smooth structured nonconvex-nonconcave minimax problems},
  author={Lee, Sucheol and Kim, Donghwan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{mai2021stability,
  title={Stability and Convergence of Stochastic Gradient Clipping: Beyond Lipschitz Continuity and Smoothness},
  author={Mai, Vien V and Johansson, Mikael},
  journal={arXiv preprint arXiv:2102.06489},
  year={2021}
}

@inproceedings{zhang2020gradient,
title={Why Gradient Clipping Accelerates Training: A Theoretical Justification for Adaptivity},
author={Jingzhao Zhang and Tianxing He and Suvrit Sra and Ali Jadbabaie},
booktitle={International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=BJgnXpVYwS}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@inproceedings{hazan2015beyond,
  title={Beyond convexity: Stochastic quasi-convex optimization},
  author={Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai},
  booktitle={Advances in Neural Information Processing Systems},
  pages={1594--1602},
  year={2015}
}

@article{korpelevich1976extragradient,
  title={The extragradient method for finding saddle points and other problems},
  author={Korpelevich, Galina M},
  journal={Matecon},
  volume={12},
  pages={747--756},
  year={1976}
}
@inproceedings{gidel2019negative,
  title={Negative momentum for improved game dynamics},
  author={Gidel, Gauthier and Hemmat, Reyhane Askari and Pezeshki, Mohammad and Le Priol, R{\'e}mi and Huang, Gabriel and Lacoste-Julien, Simon and Mitliagkas, Ioannis},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  year={2019},
  organization={PMLR}
}
@book{goodfellow2016deep,
  title={Deep learning},
  author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year={2016},
  publisher={MIT press}
}

@article{beznosikov2022stochastic,
  title={Stochastic Gradient Descent-Ascent: Unified Theory and New Efficient Methods},
  author={Beznosikov, Aleksandr and Gorbunov, Eduard and Berard, Hugo and Loizou, Nicolas},
  journal={arXiv preprint arXiv:2202.07262},
  year={2022}
}

@article{beznosikov2020distributed,
  title={Distributed Saddle-Point Problems: Lower Bounds, Optimal and Robust Algorithms},
  author={Beznosikov, Aleksandr and Samokhin, Valentin and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2010.13112},
  year={2020}
}

@article{davis2021low,
  title={From low probability to high confidence in stochastic convex optimization},
  author={Davis, Damek and Drusvyatskiy, Dmitriy and Xiao, Lin and Zhang, Junyu},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={49},
  pages={1--38},
  year={2021}
}
@inproceedings{heusel2017gans,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  booktitle={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{miyato2018spectral,
  title={Spectral normalization for generative adversarial networks},
  author={Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  booktitle={ICLR},
  year={2018}
}

@article{tran2019self,
  title={Self-supervised gan: Analysis and improvement with multi-class minimax game},
  author={Tran, Ngoc-Trung and Tran, Viet-Hung and Nguyen, Bao-Ngoc and Yang, Linxiao and Cheung, Ngai-Man Man},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@inproceedings{
brock2019large,
title={Large Scale {GAN} Training for High Fidelity Natural Image Synthesis},
author={Andrew Brock and Jeff Donahue and Karen Simonyan},
booktitle={International Conference on Learning Representations},
year={2019},
url={https://openreview.net/forum?id=B1xsqj09Fm},
}
@misc{
jelassi2022adam,
title={Adam is no better than normalized {SGD}:  Dissecting how adaptivity improves {GAN} performance},
author={Samy Jelassi and Arthur Mensch and Gauthier Gidel and Yuanzhi Li},
year={2022},
url={https://openreview.net/forum?id=D9SuLzhgK9}
}

@InProceedings{Sauer2021ARXIV,
  author    = {Axel Sauer and Katja Schwarz and Andreas Geiger},
  title     = {StyleGAN-XL: Scaling StyleGAN to Large Diverse Datasets},
  journal   = {arXiv.org},
  volume    = {abs/2201.00273},
  year      = {2022},
  url       = {https://arxiv.org/abs/2201.00273},
}

@book{conn2009introduction,
  title={Introduction to derivative-free optimization},
  author={Conn, Andrew R and Scheinberg, Katya and Vicente, Luis N},
  year={2009},
  publisher={SIAM}
}

@inproceedings{gorbunov2022extragradient,
  title={Extragradient method: ${\cO}(\nicefrac{1}{K})$ last-iterate convergence for monotone variational inequalities and connections with cocoercivity},
  author={Gorbunov, Eduard and Loizou, Nicolas and Gidel, Gauthier},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={366--402},
  year={2022},
  organization={PMLR}
}

@article{mertikopoulos2019learning,
  title={Learning in games with continuous action sets and unknown payoff functions},
  author={Mertikopoulos, Panayotis and Zhou, Zhengyuan},
  journal={Mathematical Programming},
  volume={173},
  number={1},
  pages={465--507},
  year={2019},
  publisher={Springer}
}

@article{song2020optimistic,
  title={Optimistic dual extrapolation for coherent non-monotone variational inequalities},
  author={Song, Chaobing and Zhou, Zhengyuan and Zhou, Yichao and Jiang, Yong and Ma, Yi},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={14303--14314},
  year={2020}
}

@article{bauschke2021generalized,
  title={Generalized monotone operators and their averaged resolvents},
  author={Bauschke, Heinz H and Moursi, Walaa M and Wang, Xianfu},
  journal={Mathematical Programming},
  volume={189},
  number={1},
  pages={55--74},
  year={2021},
  publisher={Springer}
}

@inproceedings{daskalakis2021complexity,
  title={The complexity of constrained min-max optimization},
  author={Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  booktitle={Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing},
  pages={1466--1478},
  year={2021}
}

@inproceedings{diakonikolas2021efficient,
  title={Efficient methods for structured nonconvex-nonconcave min-max optimization},
  author={Diakonikolas, Jelena and Daskalakis, Constantinos and Jordan, Michael},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={2746--2754},
  year={2021},
  organization={PMLR}
}

@article{juditsky2011first,
  title={First order methods for nonsmooth convex large-scale optimization, i: general purpose methods},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and others},
  journal={Optimization for Machine Learning},
  pages={121--148},
  year={2011},
  publisher={Boston: MIT Press}
}

@article{ghadimi2012optimal,
  title={Optimal stochastic approximation algorithms for strongly convex stochastic composite optimization i: A generic algorithmic framework},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={22},
  number={4},
  pages={1469--1492},
  year={2012},
  publisher={SIAM}
}

@article{ghadimi2013stochastic,
  title={Stochastic first-and zeroth-order methods for nonconvex stochastic programming},
  author={Ghadimi, Saeed and Lan, Guanghui},
  journal={SIAM Journal on Optimization},
  volume={23},
  number={4},
  pages={2341--2368},
  year={2013},
  publisher={SIAM}
}

@article{nesterov2007dual,
  title={Dual extrapolation and its applications to solving variational inequalities and related problems},
  author={Nesterov, Yurii},
  journal={Mathematical Programming},
  volume={109},
  number={2},
  pages={319--344},
  year={2007},
  publisher={Springer}
}

@book{lan2020first,
  title={First-order and stochastic optimization methods for machine learning},
  author={Lan, Guanghui},
  publisher={Springer},
  year={2020}
}

@book{nesterov2018lectures,
  title={Lectures on convex optimization},
  author={Nesterov, Yurii and others},
  volume={137},
  year={2018},
  publisher={Springer}
}

@article{kingma2014adam,
  title={Adam: A method for stochastic optimization},
  author={Kingma, Diederik P and Ba, Jimmy},
  journal={arXiv preprint arXiv:1412.6980},
  year={2014}
}

@article{cutkosky2021high,
  title={High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails},
  author={Cutkosky, Ashok and Mehta, Harsh},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@article{nazin2019algorithms,
  title={Algorithms of robust stochastic optimization based on mirror descent method},
  author={Nazin, Aleksandr Viktorovich and Nemirovsky, AS and Tsybakov, Aleksandr Borisovich and Juditsky, AB},
  journal={Automation and Remote Control},
  volume={80},
  number={9},
  pages={1607--1627},
  year={2019},
  publisher={Springer}
}
@article{arjevani2023lower,
  title={Lower bounds for non-convex stochastic optimization},
  author={Arjevani, Yossi and Carmon, Yair and Duchi, John C and Foster, Dylan J and Srebro, Nathan and Woodworth, Blake},
  journal={Mathematical Programming},
  volume={199},
  number={1},
  pages={165--214},
  year={2023},
  publisher={Springer}
}
@inproceedings{pascanu2013difficulty,
  title={On the difficulty of training recurrent neural networks},
  author={Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  booktitle={International conference on machine learning},
  pages={1310--1318},
  year={2013}
}

@article{robbins1951stochastic,
  title={A stochastic approximation method},
  author={Robbins, Herbert and Monro, Sutton},
  journal={The annals of mathematical statistics},
  pages={400--407},
  year={1951},
  publisher={JSTOR}
}

@article{gorbunov2021near,
  title={Near-Optimal High Probability Complexity Bounds for Non-Smooth Stochastic Optimization with Heavy-Tailed Noise},
  author={Gorbunov, Eduard and Danilova, Marina and Shibaev, Innokentiy and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={arXiv preprint arXiv:2106.05958},
  year={2021}
}
@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@inproceedings{boucherouite2024minibatch,
  title={Minibatch stochastic three points method for unconstrained smooth minimization},
  author={Boucherouite, Soumia and Malinovsky, Grigory and Richt{\'a}rik, Peter and Bergou, El Houcine},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={18},
  pages={20344--20352},
  year={2024}
}

@inproceedings{lobanov2024acceleration,
  title={Acceleration exists! optimization problems when oracle can only compare objective function values},
  author={Lobanov, Aleksandr and Gasnikov, Alexander and Krasnov, Andrey},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024}
}

@inproceedings{gorbunov2022stochastic,
  title={Stochastic extragradient: General analysis and improved rates},
  author={Gorbunov, Eduard and Berard, Hugo and Gidel, Gauthier and Loizou, Nicolas},
  booktitle={International Conference on Artificial Intelligence and Statistics},
  pages={7865--7901},
  year={2022},
  organization={PMLR}
}

@article{zhang2020improved,
  title={Improved analysis of clipping algorithms for non-convex optimization},
  author={Zhang, Bohang and Jin, Jikai and Fang, Cong and Wang, Liwei},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15511--15521},
  year={2020}
}

@article{wang2023rlhf,
  title={Is RLHF More Difficult than Standard RL?},
  author={Wang, Yuanhao and Liu, Qinghua and Jin, Chi},
  journal={arXiv preprint arXiv:2306.14111},
  year={2023}
}
@inproceedings{Dvinskikh_2022,
  title={Noisy Zeroth-Order Optimization for Non-smooth Saddle Point Problems},
  author={Dvinskikh, Darina and Tominin, Vladislav and Tominin, Iaroslav and Gasnikov, Alexander},
  booktitle={Mathematical Optimization Theory and Operations Research: 21st International Conference, MOTOR 2022, Petrozavodsk, Russia, July 2--6, 2022, Proceedings},
  pages={18--33},
  year={2022},
  organization={Springer}
}

@article{Lobanov_2023,
  title={Stochastic Adversarial Noise in the" Black Box" Optimization Problem},
  author={Lobanov, Aleksandr},
  journal={arXiv preprint arXiv:2304.07861},
  year={2023}
}

@article{loizou2021stochastic,
  title={Stochastic gradient descent-ascent and consensus optimization for smooth games: Convergence analysis under expected co-coercivity},
  author={Loizou, Nicolas and Berard, Hugo and Gidel, Gauthier and Mitliagkas, Ioannis and Lacoste-Julien, Simon},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  year={2021}
}

@inproceedings{mishchenko2020revisiting,
  title = 	 {Revisiting Stochastic Extragradient},
  author =       {Mishchenko, Konstantin and Kovalev, Dmitry and Shulgin, Egor and Richtarik, Peter and Malitsky, Yura},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {4573--4582},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  publisher =    {PMLR},
  abstract = 	 {We fix a fundamental issue in the stochastic extragradient method by providing a new sampling strategy that is motivated by approximating implicit updates. Since the existing stochastic extragradient algorithm, called Mirror-Prox, of (Juditsky, 2011) diverges on a simple bilinear problem when the domain is not bounded, we prove guarantees for solving variational inequality that go beyond existing settings. Furthermore, we illustrate numerically that the proposed variant converges faster than many other methods on several convex-concave saddle-point problems. We also discuss how extragradient can be applied to training Generative Adversarial Networks (GANs) and how it compares to other methods. Our experiments on GANs demonstrate that the introduced approach may make the training faster in terms of data passes, while its higher iteration complexity makes the advantage smaller.}
}

@inproceedings{hsieh2019convergence,
  author = {Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J\'{e}r\^{o}me and Mertikopoulos, Panayotis},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {On the convergence of single-call stochastic extra-gradient methods},
 volume = {32},
 year = {2019}
}

@article{hsieh2020explore,
  title={Explore Aggressively, Update Conservatively: Stochastic Extragradient Methods with Variable Stepsize Scaling},
  author={Hsieh, Yu-Guan and Iutzeler, Franck and Malick, J{\'e}r{\^o}me and Mertikopoulos, Panayotis},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  year={2020}
}

@article{nemirovski2009robust,
  title={Robust stochastic approximation approach to stochastic programming},
  author={Nemirovski, Arkadi and Juditsky, Anatoli and Lan, Guanghui and Shapiro, Alexander},
  journal={SIAM Journal on optimization},
  volume={19},
  number={4},
  pages={1574--1609},
  year={2009},
  publisher={SIAM}
}

@article{dem1972numerical,
  title={Numerical methods for finding saddle points},
  author={Dem'yanov, Vladimir Fedorovich and Pevnyi, Aleksandr Borisovich},
  journal={USSR Computational Mathematics and Mathematical Physics},
  volume={12},
  number={5},
  pages={11--52},
  year={1972},
  publisher={Elsevier}
}

@article{gidel2019variational,
  title={A variational inequality perspective on generative adversarial networks},
  author={Gidel, Gauthier and Berard, Hugo and Vignoud, Ga{\"e}tan and Vincent, Pascal and Lacoste-Julien, Simon},
  journal={International Conference on Learning Representations},
  year={2019}
}

@misc{ryu2021large,
  title={Large-scale convex optimization via monotone operators},
  author={Ryu, Ernest K and Yin, Wotao},
  year={2021},
  publisher={Draft}
}

@article{harker1990finite,
  title={Finite-dimensional variational inequality and nonlinear complementarity problems: a survey of theory, algorithms and applications},
  author={Harker, Patrick T and Pang, Jong-Shi},
  journal={Mathematical programming},
  volume={48},
  number={1},
  pages={161--220},
  year={1990},
  publisher={Springer}
}

@inproceedings{goodfellow2014generative,
    author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {Z. Ghahramani and M. Welling and C. Cortes and N. Lawrence and K. Q. Weinberger},
    pages = {},
    publisher = {Curran Associates, Inc.},
    title = {Generative Adversarial Nets},
    volume = {27},
    year = {2014}
}

@article{wayne2014hierarchical,
  title={Hierarchical control using networks trained with higher-level forward models},
  author={Wayne, Greg and Abbott, LF},
  journal={Neural computation},
  volume={26},
  number={10},
  pages={2163--2193},
  year={2014},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â¦}
}


@inproceedings{vezhnevets2017feudal,
  title={Feudal networks for hierarchical reinforcement learning},
  author={Vezhnevets, Alexander Sasha and Osindero, Simon and Schaul, Tom and Heess, Nicolas and Jaderberg, Max and Silver, David and Kavukcuoglu, Koray},
  booktitle={International Conference on Machine Learning},
  pages={3540--3549},
  year={2017},
  organization={PMLR}
}

@inproceedings{bose2020adversarial,
  author = {Bose, Joey and Gidel, Gauthier and Berard, Hugo and Cianflone, Andre and Vincent, Pascal and Lacoste-Julien, Simon and Hamilton, Will},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Larochelle and M. Ranzato and R. Hadsell and M. F. Balcan and H. Lin},
 pages = {8921--8934},
 publisher = {Curran Associates, Inc.},
 title = {Adversarial Example Games},
 volume = {33},
 year = {2020}
}

@article{juditsky2011solving,
  title={Solving variational inequalities with stochastic mirror-prox algorithm},
  author={Juditsky, Anatoli and Nemirovski, Arkadi and Tauvel, Claire},
  journal={Stochastic Systems},
  volume={1},
  number={1},
  pages={17--58},
  year={2011},
  publisher={INFORMS}
}

@article{dzhaparidze2001bernstein,
  title={On Bernstein-type inequalities for martingales},
  author={Dzhaparidze, Kacha and Van Zanten, JH},
  journal={Stochastic processes and their applications},
  volume={93},
  number={1},
  pages={109--117},
  year={2001},
  publisher={Elsevier}
}

@article{freedman1975tail,
  title={On tail probabilities for martingales},
  author={Freedman, David A and others},
  journal={the Annals of Probability},
  volume={3},
  number={1},
  pages={100--118},
  year={1975},
  publisher={Institute of Mathematical Statistics}
}

@article{bennett1962probability,
  title={Probability inequalities for the sum of independent random variables},
  author={Bennett, George},
  journal={Journal of the American Statistical Association},
  volume={57},
  number={297},
  pages={33--45},
  year={1962},
  publisher={Taylor \& Francis Group}
}

@article{gorbunov2020stochastic,
  title={Stochastic optimization with heavy-tailed noise via accelerated gradient clipping},
  author={Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15042--15053},
  year={2020}
}

@article{zhang2020adaptive,
  title={Why are Adaptive Methods Good for Attention Models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank J and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  pages={},
  volume={33},
  year={2020}
}

@article{gulrajani2017improved,
  title={Improved training of wasserstein gans},
  author={Gulrajani, Ishaan and Ahmed, Faruk and Arjovsky, Martin and Dumoulin, Vincent and Courville, Aaron C},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{krizhevsky2009cifar10,
  title={Learning multiple layers of features from tiny images},
  author={Krizhevsky, Alex and Hinton, Geoffrey and others},
  year={2009},
  publisher={Citeseer}
}

@inproceedings{karras2020stylegan2,
  title={Analyzing and improving the image quality of stylegan},
  author={Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8110--8119},
  year={2020}
}

@inproceedings{karras2019stylegan1,
  title={A style-based generator architecture for generative adversarial networks},
  author={Karras, Tero and Laine, Samuli and Aila, Timo},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4401--4410},
  year={2019}
}

@article{heusel2017fid,
  title={Gans trained by a two time-scale update rule converge to a local nash equilibrium},
  author={Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@inproceedings{jordanova2017measuringheavy,
  title={Measuring heavy-tailedness of distributions},
  author={Jordanova, Pavlina K and Petkova, Monika P},
  booktitle={AIP Conference Proceedings},
  volume={1910},
  number={1},
  pages={060002},
  year={2017},
  organization={AIP Publishing LLC}
}


@article{Bogolubsky_2016,
  title={Learning supervised pagerank with gradient-based and gradient-free optimization methods},
  author={Bogolubsky, Lev and Dvurechenskii, Pavel and Gasnikov, Alexander and Gusev, Gleb and Nesterov, Yurii and Raigorodskii, Andrei M and Tikhonov, Aleksey and Zhukovskii, Maksim},
  journal={Advances in neural information processing systems},
  volume={29},
  year={2016}
}

@article{bergou2020stochastic,
  title={Stochastic three points method for unconstrained smooth minimization},
  author={Bergou, El Houcine and Gorbunov, Eduard and Richtarik, Peter},
  journal={SIAM Journal on Optimization},
  volume={30},
  number={4},
  pages={2726--2749},
  year={2020},
  publisher={SIAM}
}


@article{qin2025high,
  title={High Probability Convergence of Clipped Distributed Dual Averaging With Heavy-Tailed Noises},
  author={Qin, Yanfu and Lu, Kaihong and Xu, Hang and Chen, Xiangyong},
  journal={IEEE Transactions on Systems, Man, and Cybernetics: Systems},
  year={2025},
  publisher={IEEE}
}

@article{kornilov2024zeroth,
  title={Zeroth-order Median Clipping for Non-Smooth Convex Optimization Problems with Heavy-tailed Symmetric Noise},
  author={Kornilov, Nikita and Dorn, Yuriy and Lobanov, Aleksandr and Kutuzov, Nikolay and Shibaev, Innokentiy and Gorbunov, Eduard and Gasnikov, Alexander and Nazin, Alexander},
  journal={arXiv preprint arXiv:2402.02461},
  year={2024}
}

@article{merity2017regularizing,
  title={Regularizing and optimizing LSTM language models},
  author={Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
  journal={arXiv preprint arXiv:1708.02182},
  year={2017}
}
@inproceedings{barakat2023reinforcement,
  title={Reinforcement learning with general utilities: Simpler variance reduction and large state-action space},
  author={Barakat, Anas and Fatkhullin, Ilyas and He, Niao},
  booktitle={International Conference on Machine Learning},
  pages={1753--1800},
  year={2023},
  organization={PMLR}
}

@article{liu2022communication,
  title={A communication-efficient distributed gradient clipping algorithm for training deep neural networks},
  author={Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={26204--26217},
  year={2022}
}

@article{yang2024two,
  title={Two sides of one coin: the limits of untuned SGD and the power of adaptive methods},
  author={Yang, Junchi and Li, Xiang and Fatkhullin, Ilyas and He, Niao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{boyd2004convex,
  title={Convex optimization},
  author={Boyd, Stephen},
  journal={Cambridge UP},
  year={2004}
}
@inproceedings{gurbuzbalaban2021heavy,
  title={The heavy-tail phenomenon in SGD},
  author={Gurbuzbalaban, Mert and Simsekli, Umut and Zhu, Lingjiong},
  booktitle={International Conference on Machine Learning},
  pages={3964--3975},
  year={2021},
  organization={PMLR}
}

@incollection{bottou2012stochastic,
  title={Stochastic gradient descent tricks},
  author={Bottou, L{\'e}on},
  booktitle={Neural Networks: Tricks of the Trade: Second Edition},
  pages={421--436},
  year={2012},
  publisher={Springer}
}
@inproceedings{liu2023breaking,
  title={Breaking the lower bound with (little) structure: Acceleration in non-convex stochastic optimization with heavy-tailed noise},
  author={Liu, Zijian and Zhang, Jiawei and Zhou, Zhengyuan},
  booktitle={The Thirty Sixth Annual Conference on Learning Theory},
  pages={2266--2290},
  year={2023},
  organization={PMLR}
}


@inproceedings{liu2019signsgd,
  title={signSGD via zeroth-order oracle},
  author={Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@inproceedings{saha2021dueling,
  title={Dueling convex optimization},
  author={Saha, Aadirupa and Koren, Tomer and Mansour, Yishay},
  booktitle={International Conference on Machine Learning},
  pages={9245--9254},
  year={2021},
  organization={PMLR}
}
@inproceedings{tang2023zeroth,
  title={Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles},
  author={Tang, Zhiwei and Rybin, Dmitry and Chang, Tsung-Hui},
  booktitle={ICML 2023 Workshop The Many Facets of Preference-Based Learning},
year={2023},
}
@incollection{danilova2022recent,
  title={Recent theoretical advances in non-convex optimization},
  author={Danilova, Marina and Dvurechensky, Pavel and Gasnikov, Alexander and Gorbunov, Eduard and Guminov, Sergey and Kamzolov, Dmitry and Shibaev, Innokentiy},
  booktitle={High-Dimensional Optimization and Probability: With a View Towards Data Science},
  pages={79--163},
  year={2022},
  publisher={Springer}
}

@inproceedings{sun2023momentum,
  title={Momentum ensures convergence of signsgd under weaker assumptions},
  author={Sun, Tao and Wang, Qingsong and Li, Dongsheng and Wang, Bao},
  booktitle={International Conference on Machine Learning},
  pages={33077--33099},
  year={2023},
  organization={PMLR}
}
@article{pichapati2019adaclip,
  title={Adaclip: Adaptive clipping for private sgd},
  author={Pichapati, Venkatadheeraj and Suresh, Ananda Theertha and Yu, Felix X and Reddi, Sashank J and Kumar, Sanjiv},
  journal={arXiv preprint arXiv:1908.07643},
  year={2019}
}

@article{kornilov2024accelerated,
  title={Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite variance},
  author={Kornilov, Nikita and Shamir, Ohad and Lobanov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander and Shibaev, Innokentiy and Gorbunov, Eduard and Horv{\'a}th, Samuel},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{andrew2021differentially,
  title={Differentially private learning with adaptive clipping},
  author={Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={17455--17466},
  year={2021}
}


@article{armacki2024large,
  title={Large Deviations and Improved Mean-squared Error Rates of Nonlinear SGD: Heavy-tailed Noise and Power of Symmetry},
  author={Armacki, Aleksandar and Yu, Shuhua and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2410.15637},
  year={2024}
}



@article{moulines2011non,
  title={Non-asymptotic analysis of stochastic approximation algorithms for machine learning},
  author={Moulines, Eric and Bach, Francis},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

@article{zhang2020adaptivegood,
  title={Why are adaptive methods good for attention models?},
  author={Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={15383--15393},
  year={2020}
}
@inproceedings{cutkosky2020momentum,
  title={Momentum improves normalized sgd},
  author={Cutkosky, Ashok and Mehta, Harsh},
  booktitle={International conference on machine learning},
  pages={2260--2268},
  year={2020},
  organization={PMLR}
}

@inproceedings{bottou2010large,
  title={Large-scale machine learning with stochastic gradient descent},
  author={Bottou, L{\'e}on},
  booktitle={Proceedings of COMPSTAT'2010: 19th International Conference on Computational StatisticsParis France, August 22-27, 2010 Keynote, Invited and Contributed Papers},
  pages={177--186},
  year={2010},
  organization={Springer}
}
@article{armacki2023high,
  title={High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise},
  author={Armacki, Aleksandar and Sharma, Pranay and Joshi, Gauri and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya},
  journal={arXiv preprint arXiv:2310.18784},
  year={2023}
}


@inproceedings{seide20141,
  title={1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs.},
  author={Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong},
  booktitle={Interspeech},
  volume={2014},
  pages={1058--1062},
  year={2014},
  organization={Singapore}
}
@inproceedings{karimireddy2019error,
  title={Error feedback fixes signsgd and other gradient compression schemes},
  author={Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin},
  booktitle={International Conference on Machine Learning},
  pages={3252--3261},
  year={2019},
  organization={PMLR}
}

@article{gorbunov2022acceleratedstp,
  title={An accelerated method for derivative-free smooth stochastic convex optimization},
  author={Gorbunov, Eduard and Dvurechensky, Pavel and Gasnikov, Alexander},
  journal={SIAM Journal on Optimization},
  volume={32},
  number={2},
  pages={1210--1238},
  year={2022},
  publisher={SIAM}
}



@inproceedings{relora,
  title={Relora: High-rank training through low-rank updates},
  author={Lialin, Vladislav and Muckatira, Sherin and Shivagunde, Namrata and Rumshisky, Anna},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

@article{llama,
  title={Llama: Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={arXiv preprint arXiv:2302.13971},
  year={2023}
}

@article{c4,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{mnli,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{sst2,
    title = "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
    author = "Socher, Richard  and
      Perelygin, Alex  and
      Wu, Jean  and
      Chuang, Jason  and
      Manning, Christopher D.  and
      Ng, Andrew  and
      Potts, Christopher",
    booktitle = "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing",
    month = oct,
    year = "2013",
    address = "Seattle, Washington, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D13-1170",
    pages = "1631--1642",
}

@article{wang2018glue,
  title={Glue: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex},
  journal={arXiv preprint arXiv:1804.07461},
  year={2018}
}

@inproceedings{finetune_setup_1,
  title={A kernel-based view of language model fine-tuning},
  author={Malladi, Sadhika and Wettig, Alexander and Yu, Dingli and Chen, Danqi and Arora, Sanjeev},
  booktitle={International Conference on Machine Learning},
  pages={23610--23641},
  year={2023},
  organization={PMLR}
}

@article{finetune_setup_2,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@article{mezo,
  title={Fine-tuning language models with just forward passes},
  author={Malladi, Sadhika and Gao, Tianyu and Nichani, Eshaan and Damian, Alex and Lee, Jason D and Chen, Danqi and Arora, Sanjeev},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53038--53075},
  year={2023}
}

@inproceedings{trec,
  title={Building a question answering test collection},
  author={Voorhees, Ellen M and Tice, Dawn M},
  booktitle={Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={200--207},
  year={2000}
}

@article{loshchilov2017decoupled,
  title={Decoupled weight decay regularization},
  author={Loshchilov, I},
  journal={arXiv preprint arXiv:1711.05101},
  year={2017}
}

@article{anything_but_sgd,
  title={Deconstructing what makes a good optimizer for language models},
  author={Zhao, Rosie and Morwani, Depen and Brandfonbrener, David and Vyas, Nikhil and Kakade, Sham},
  journal={arXiv preprint arXiv:2407.07972},
  year={2024}
}

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}

@article{roberta,
  title={RoBERTa: A Robustly Optimized BERT Pretraining Approach},
  author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
  journal={ArXiv},
  year={2019},
  volume={abs/1907.11692}
}

@article{shazeer2020glu,
  title={Glu variants improve transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@article{lobanov2024order,
  title={The Order Oracle: a New Concept in The Black Box Optimization Problems},
  author={Lobanov, Aleksandr and Gasnikov, Alexander and Krasnov, Andrei},
  journal={arXiv preprint arXiv:2402.09014},
  year={2024}
}

@article{setup_finetuning,
  title={Making pre-trained language models better few-shot learners},
  author={Gao, Tianyu and Fisch, Adam and Chen, Danqi},
  journal={arXiv preprint arXiv:2012.15723},
  year={2020}
}

@article{rmsnorm,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}