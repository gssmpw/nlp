\section{Related works}
\paragraph{Clipping.} The idea of clipping the norm of gradient estimate before \algname{SGD} step demonstrates great empirical results ____ and helps achieve $\log \nicefrac{1}{\delta}$ dependency under BV assumption ____. 

The clipping operator is defined as $\clip(g^k, \lambda_k) := \min\{1, \nicefrac{\lambda_k}{\|g^k\|_2}\} \cdot g^k$ and can be applied not only 
non-convex minimization problems, but also to convex optimization, variational inequalities ____, non-smooth optimization ____, zeroth-order optimization ____, robust aggregation ____, distributed optimization ____ and ensuring differential privacy ____. %The first modification of \algname{SGD} which helps improve convergence and obtain high probability bounds under BV and heavy-tailed noise is \algname{ClipSGD} ____. 

Let us list the latest results on the HP convergence of \algname{SGD} with clipping, called \algname{ClipSGD}, under the HT noise assumption. First, for non-convex functions, the authors of ____ proved lower bounds for sample complexity in expectation. As shown in ____, \algname{ClipSGD} with proper clipping levels and stepsizes achieves this lower bound with extra logarithmic factors on $\delta$ and accuracy. In a number of works, the authors relax the HT assumption and consider only symmetric noise. This relaxation allows them to eliminate the dependency on $\kappa$ and break the actual lower bounds. For example, in ____,  the authors used the coordinate-wise median operator, which requires only a few noise samples to lighten its distribution so it becomes BV. ____ proved convergence of \algname{ClipSGD} combined with the median operator for (strongly) convex functions as if $\kappa = 2$. %For close-to-symmetric noises, the authors use median of means and obtain the same results. 
%Similar suboptimal bound  $\tilde{\Omega} \left( \varepsilon^{-\frac{\kappa}{\kappa - 1} }\right)$  for strongly functions is attained as well. 

Despite clipping's effectiveness, it requires careful tuning of clipping levels whose optimal values depend on the iteration number and all characteristics of objective function and HT noise ____. Hence, the community has proposed other robust modifications of \algname{SGD}.   
 \vspace{-20pt}
\paragraph{Normalization.}
A natural relaxation of clipping with a profound level schedule is the permanent normalization of gradient estimate, i.e., $\text{norm}(g^k) := \frac{g^k}{\|g^k\|_2}.$ \algname{SGD} with additional normalization is called \algname{NSGD} ____.


In early works devoted to \algname{NSGD}, only BV noise and bounds in expectation were considered ____. In ____, normalization was combined with clipping, which helped to cope with HT noise and obtain HP bounds.

Recently, the HP convergence of vanilla \algname{NSGD} was proved under HT noise in ____. The authors showed that the complexity of \algname{NSGD} exactly matches the lower bound before mentioned from ____ without logarithmic accuracy factors and only with mild $\log\nicefrac{1}{\delta}$ dependency. Moreover, in experiments with sequence labeling via LSTM Language Models ____, normalization demonstrated better results than tuned clipping.  

Unlike \algname{ClipSGD}, \algname{NSGD} requires large batch sizes for robust convergence. It can be fixed by replacing batching with momentum techniques, which keeps the same sample complexity ____. However, for methods with momentum, the convergence bounds are usually proved in expectation, and the HP bounds with logarithmic $\delta$ dependency have not yet been obtained. In experiments with VB noise, one can observe super-linear dependency on $\log \frac{1}{\delta}$ ____.

\paragraph{Sign operator.} There is one more promising modification of \algname{SGD} which behavior under heavy-tailed noise has not yet been studied. Originally proposed in ____ for distributed optimization, \algname{SignSGD} takes only a sign of each coordinate of gradient estimate $$x^{k+1} = x^k - \gamma_k \cdot \sign(g^k).$$ There is one peculiarity in bounds for sign-based methods: they are proved w.r.t. the $\ell_1$-norm instead of smaller $\ell_2$-norm. As a consequence, additional $d$ dependent factors appear.  
Under BV noise, \algname{SignSGD} achieves optimal sample complexity in expectation (up to $d$ factors). Similar to \algname{NSGD}, \algname{SignSGD} requires aggressive batching which can be substituted with momentum ____. The alternative solution is to add error feedback mechanism that additionally fixes the biased nature of sign operator ____.

The main motivation for the original \algname{SignSGD} was communication effectiveness and empirical robustness in distributed optimization ____, since sending sign vector costs $O(d)$ operations. In theory, the effectiveness was proved only under additional assumptions on noise, e.g., symmetry and unimodality. Other applications and expansions of \algname{SignSGD} are as follows: 
____ proposed an updated theory for a wider class of noises in the distributed setup, ____ generalized \algname{SignSGD} to zeroth-order oracle, ____ studied federated learning with compression.

For all the previously mentioned works, the results were obtained \textit{in expectation and only for BV noise}. The HP bounds were obtained only in ____, where the authors proposed a unified framework for the theoretical analysis of online non-linear \algname{SGD}. It includes a wide range of non-linear gradient estimation transformations such as \textit{clipping, normalization, and sign operator.} However, in these works, the HT assumption was relaxed to symmetric noises only. The authors proved HP bounds which are arbitrarily close to the optimal ones for BV noise.