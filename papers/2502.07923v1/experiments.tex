\vspace{-8pt}
\section{Experiments}\label{sec:experiments}
In this section, we present experimental results for both first-order and zero-order methods described in~\Cref{sec: sec_2,sec: sec_3}, respectively. 
To demonstrate the superiority of sign-based methods for problems with heavy-tailed noise, we focus on language model training tasks. 
This choice is motivated by two factors: first, these tasks are known to exhibit heavy-tailed noise characteristics~\citep{zhang2020adaptive}, and second, they represent an important real-world application domain.
\subsection{\algname{M-SignSGD} on LLaMA pre-training}
First, we evaluate the performance of \algname{M-SignSGD}~(\Cref{alg:SignSGD-M}) on the language model pre-training task. 
We adopt the established experimental setup from~\cite{relora}, training a 130M parameter LLaMA-like model~\citep{llama} on the Colossal Clean Crawled Corpus (C4) dataset~\citep{c4}. 
The C4 dataset represents a colossal, cleaned version of Common Crawl's web corpus, specifically designed for pre-training language models and word representations.

For our comparison, we focus on two key techniques for handling heavy-tailed noise: gradient clipping with momentum and gradient normalization with momentum. As representative methods, we choose \algname{M-ClippedSGD} \cite{zhang2020improved} and \algname{M-NSGD} \cite{cutkosky2020momentum}, respectively.
We also compare to AdamW~\cite{loshchilov2017decoupled}, as a de facto method for the first-order optimization algorithm for deep learning.
To ensure a fair comparison, we conduct an extensive grid search over key hyperparameters, including learning rate, weight decay, and clipping level. 
Detailed information on the final hyperparameter values and complete experimental setup is provided in~\Cref{app:pre-training}.

\Cref{tab:pre-training} presents final validation perplexity 
% (exponent of validation loss) 
for each method. 
\algname{M-SignSGD} demonstrates superior performance over the baselines, aligning with our theoretical results.

\vspace{-5pt}
\subsection{\algname{CompSGD} on RoBERTa fine-tuning}
Second, we consider a zeroth-order setting. Following \algname{MeZO}~\citep{mezo}, we evaluate our method on classification fine-tuning tasks, specifically SST-2~\citep{sst2}, MNLI~\citep{mnli}, TREC~\citep{trec}, on the RoBERTa-large model \cite{roberta}. 
We employ the established few-shot prediction setting~\citep{finetune_setup_1,finetune_setup_2}. See details in~\Cref{app:fine-tuning}.

We compare \algname{CompSGD}~\Cref{alg:comp-sinSGD} with the pre-trained model without fine-tuning (Zero-shot) and the original \algname{MeZO} version. As demonstrated in~\Cref{tab:fine-tuning}, the sign-based method again outperforms its non-sign counterpart.
\vspace{-5pt}
\subsection{\algname{CompSGD} for accuracy maximization}
Third, we simulate the zeroth-order environment with comparison oracles as follows. We take the prediction accuracy of the linear model on the training dataset as the objective:
\vspace{-2mm}
\begin{equation*}
\textstyle
    f(x) = \left(\!1 - \text{Acc}\!\left(\mathbf{y}_{\text{train}}, \text{sign}\!\left(\!\frac{2}{1 + \exp (- \mathbf{X}_{\text{train}} x)}\!\right) - 1\!\right)\!\right).    
\end{equation*}
As training data, we consider classification tasks from LibSVM \cite{chang2011libsvm}: \texttt{mushrooms}, \texttt{phishing}, \texttt{a6a}. 
In Figure \ref{fig:1}, we give the dynamics of accuracy on the test sample for our method and for another method working with the comparison oracle \algname{OrderRCD} \cite{lobanov2024order}. Here, we also outperformed the competitor. 

% \vspace{-3mm}
\begin{table}[t!]
    \caption{Perplexity of LLaMA-130M model pre-trained on C4 for 100k steps. Lower is better.}
    \label{tab:pre-training}
    \vspace{-3mm}
    \begin{center}
    \begin{tabular}{c|c}
    \toprule
    \textbf{Method} & \textbf{Perplexity $\downarrow$} \\
    \midrule
    \algname{M-SignSGD} & \textbf{18.37}\\
    \hline
    \algname{M-NSGD} &  19.28 \\
    \algname{M-ClippedSGD} & 18.95 \\
    \algname{AdamW} & 18.67 \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-1.em}
\end{table}
\vspace{-1mm}
\begin{table}[t!]
    \caption{Accuracy of RoBERTa-large (350M parameters) fine-tuned on different tasks. Higher is better.}
    \label{tab:fine-tuning}
    \vspace{-3mm}
    \begin{center}
    \begin{tabular}{c|ccc}
    \toprule
    \textbf{Method} & \textbf{SST-2}& \textbf{MNLI} & \textbf{TREC} \\
    \midrule
    \algname{CompSGD} & \textbf{91.9} & \textbf{63.8} & \textbf{77.2} \\
    \hline
    \algname{MeZO} & 91.7 & \textcolor{black}{58.7} & \textcolor{black}{76.9} \\
    \hline
    Zero-shot & 79.0 & 48.8 & 32.0 \\
    \bottomrule
    \end{tabular}
    \end{center}
    \vspace{-2.em}
\end{table}

\begin{figure}[ht]
    \centering
        \includegraphics[width=0.15\textwidth]{1.pdf}
        \includegraphics[width=0.15\textwidth]{2.pdf}
        \includegraphics[width=0.15\textwidth]{3.pdf}
    \vspace{-3mm}
    \caption{Performance of zeroth-order methods with comparison oracle.}
    \label{fig:1}
\end{figure}