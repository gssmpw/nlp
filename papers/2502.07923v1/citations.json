[
  {
    "index": 0,
    "papers": [
      {
        "key": "pascanu2013difficulty",
        "author": "Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua",
        "title": "On the difficulty of training recurrent neural networks"
      },
      {
        "key": "goodfellow2016deep",
        "author": "Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron",
        "title": "Deep learning"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "nazin2019algorithms",
        "author": "Nazin, Aleksandr Viktorovich and Nemirovsky, AS and Tsybakov, Aleksandr Borisovich and Juditsky, AB",
        "title": "Algorithms of robust stochastic optimization based on mirror descent method"
      },
      {
        "key": "gorbunov2020stochastic",
        "author": "Gorbunov, Eduard and Danilova, Marina and Gasnikov, Alexander",
        "title": "Stochastic optimization with heavy-tailed noise via accelerated gradient clipping"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "sadiev2023high",
        "author": "Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\\'a}rik, Peter",
        "title": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "zhang2020adaptivegood",
        "author": "Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit",
        "title": "Why are adaptive methods good for attention models?"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "kornilov2024accelerated",
        "author": "Kornilov, Nikita and Shamir, Ohad and Lobanov, Aleksandr and Dvinskikh, Darina and Gasnikov, Alexander and Shibaev, Innokentiy and Gorbunov, Eduard and Horv{\\'a}th, Samuel",
        "title": "Accelerated zeroth-order method for non-smooth stochastic convex optimization problem with infinite variance"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "karimireddy2021learning",
        "author": "Karimireddy, Sai Praneeth and He, Lie and Jaggi, Martin",
        "title": "Learning from history for byzantine robust optimization"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "liu2022communication",
        "author": "Liu, Mingrui and Zhuang, Zhenxun and Lei, Yunwen and Liao, Chunyang",
        "title": "A communication-efficient distributed gradient clipping algorithm for training deep neural networks"
      },
      {
        "key": "qin2025high",
        "author": "Qin, Yanfu and Lu, Kaihong and Xu, Hang and Chen, Xiangyong",
        "title": "High Probability Convergence of Clipped Distributed Dual Averaging With Heavy-Tailed Noises"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "andrew2021differentially",
        "author": "Andrew, Galen and Thakkar, Om and McMahan, Brendan and Ramaswamy, Swaroop",
        "title": "Differentially private learning with adaptive clipping"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "sadiev2023high",
        "author": "Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\\'a}rik, Peter",
        "title": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
      },
      {
        "key": "nguyen2023high",
        "author": "Nguyen, Ta Duy and Nguyen, Thien Hang and Ene, Alina and Nguyen, Huy Le",
        "title": "High Probability Convergence of Clipped-SGD Under Heavy-tailed Noise"
      },
      {
        "key": "nguyen2023improved",
        "author": "Nguyen, Ta Duy and Ene, Alina and Nguyen, Huy L",
        "title": "Improved Convergence in High Probability of Clipped Gradient Methods with Heavy Tails"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhang2020adaptivegood",
        "author": "Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit",
        "title": "Why are adaptive methods good for attention models?"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "sadiev2023high",
        "author": "Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\\'a}rik, Peter",
        "title": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "puchkin2024breaking",
        "author": "Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov, Alexander",
        "title": "Breaking the heavy-tailed noise barrier in stochastic optimization problems"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "puchkin2024breaking",
        "author": "Puchkin, Nikita and Gorbunov, Eduard and Kutuzov, Nickolay and Gasnikov, Alexander",
        "title": "Breaking the heavy-tailed noise barrier in stochastic optimization problems"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "sadiev2023high",
        "author": "Sadiev, Abdurakhmon and Danilova, Marina and Gorbunov, Eduard and Horv{\\'a}th, Samuel and Gidel, Gauthier and Dvurechensky, Pavel and Gasnikov, Alexander and Richt{\\'a}rik, Peter",
        "title": "High-Probability Bounds for Stochastic Optimization and Variational Inequalities: the Case of Unbounded Variance"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "hazan2015beyond",
        "author": "Hazan, Elad and Levy, Kfir and Shalev-Shwartz, Shai",
        "title": "Beyond convexity: Stochastic quasi-convex optimization"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "barakat2023reinforcement",
        "author": "Barakat, Anas and Fatkhullin, Ilyas and He, Niao",
        "title": "Reinforcement learning with general utilities: Simpler variance reduction and large state-action space"
      },
      {
        "key": "yang2024two",
        "author": "Yang, Junchi and Li, Xiang and Fatkhullin, Ilyas and He, Niao",
        "title": "Two sides of one coin: the limits of untuned SGD and the power of adaptive methods"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "liu2023breaking",
        "author": "Liu, Zijian and Zhang, Jiawei and Zhou, Zhengyuan",
        "title": "Breaking the lower bound with (little) structure: Acceleration in non-convex stochastic optimization with heavy-tailed noise"
      },
      {
        "key": "cutkosky2021high",
        "author": "Cutkosky, Ashok and Mehta, Harsh",
        "title": "High-probability Bounds for Non-Convex Stochastic Optimization with Heavy Tails"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "hubler2024gradient",
        "author": "H{\\\"u}bler, Florian and Fatkhullin, Ilyas and He, Niao",
        "title": "From Gradient Clipping to Normalization for Heavy Tailed SGD"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "zhang2020adaptivegood",
        "author": "Zhang, Jingzhao and Karimireddy, Sai Praneeth and Veit, Andreas and Kim, Seungyeon and Reddi, Sashank and Kumar, Sanjiv and Sra, Suvrit",
        "title": "Why are adaptive methods good for attention models?"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "merity2017regularizing",
        "author": "Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard",
        "title": "Regularizing and optimizing LSTM language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "cutkosky2020momentum",
        "author": "Cutkosky, Ashok and Mehta, Harsh",
        "title": "Momentum improves normalized sgd"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "hubler2024gradient",
        "author": "H{\\\"u}bler, Florian and Fatkhullin, Ilyas and He, Niao",
        "title": "From Gradient Clipping to Normalization for Heavy Tailed SGD"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "bernstein2018signsgd",
        "author": "Bernstein, Jeremy and Wang, Yu-Xiang and Azizzadenesheli, Kamyar and Anandkumar, Animashree",
        "title": "signSGD: Compressed optimisation for non-convex problems"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "sun2023momentum",
        "author": "Sun, Tao and Wang, Qingsong and Li, Dongsheng and Wang, Bao",
        "title": "Momentum ensures convergence of signsgd under weaker assumptions"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "seide20141",
        "author": "Seide, Frank and Fu, Hao and Droppo, Jasha and Li, Gang and Yu, Dong",
        "title": "1-bit stochastic gradient descent and its application to data-parallel distributed training of speech DNNs."
      },
      {
        "key": "karimireddy2019error",
        "author": "Karimireddy, Sai Praneeth and Rebjock, Quentin and Stich, Sebastian and Jaggi, Martin",
        "title": "Error feedback fixes signsgd and other gradient compression schemes"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "bernstein2018majorityvote",
        "author": "Bernstein, Jeremy and Zhao, Jiawei and Azizzadenesheli, Kamyar and Anandkumar, Anima",
        "title": "signSGD with majority vote is communication efficient and fault tolerant"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "safaryan2021stochastic",
        "author": "Safaryan, Mher and Richt{\\'a}rik, Peter",
        "title": "Stochastic sign descent methods: New algorithms and better theory"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "liu2019signsgd",
        "author": "Liu, Sijia and Chen, Pin-Yu and Chen, Xiangyi and Hong, Mingyi",
        "title": "signSGD via zeroth-order oracle"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "jin2020stochastic",
        "author": "Jin, Richeng and Huang, Yufan and He, Xiaofan and Dai, Huaiyu and Wu, Tianfu",
        "title": "Stochastic-sign SGD for federated learning with theoretical guarantees"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "armacki2023high",
        "author": "Armacki, Aleksandar and Sharma, Pranay and Joshi, Gauri and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya",
        "title": "High-probability Convergence Bounds for Nonlinear Stochastic Gradient Descent Under Heavy-tailed Noise"
      },
      {
        "key": "armacki2024large",
        "author": "Armacki, Aleksandar and Yu, Shuhua and Bajovic, Dragana and Jakovetic, Dusan and Kar, Soummya",
        "title": "Large Deviations and Improved Mean-squared Error Rates of Nonlinear SGD: Heavy-tailed Noise and Power of Symmetry"
      }
    ]
  }
]