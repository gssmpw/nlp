\section{Related Works}
\subsubsection{Structured Pruning}
\label{sec:structured}
MPF~\cite{He2019FilterPB} considers the geometric distance between filters and neighboring filters to guide the pruning process. CPMC~\cite{Yan2020ChannelPV} leverages relationships between filters in the current layer and subsequent layers for pruning decisions. HRank~\cite{Lin2020HRankFP} compresses models by constructing low-rank matrices using information from the same batch. Scop~\cite{Tang2020SCOPSC} reduces filters based on their deviation from the expected network behavior. 
GDP~\cite{Guo2021GDPSN} employs gates with differentiable polarization, learning whether gates are zero during training for pruning purposes. EEMC~\cite{Zhang2021ExplorationAE} uses a multivariate Bernoulli distribution along with stochastic gradient Hamiltonian Monte Carlo for pruning. SRR-GR~\cite{Wang2021ConvolutionalNN} identifies redundant structures within CNNs. FTWT~\cite{Elkerdawy2021FireTW} predicts pruning strategies using a self-supervised mask. DECORE~\cite{Alwani2021DECOREDC} assigns an agent to each filter and uses lightweight learning to decide whether to keep or discard each filter. EPruner~\cite{Lin2021NetworkPU} employs Affinity Propagation for efficient pruning. DPFPS~\cite{Ruan2021DPFPSDA} directly learns the network with structural sparsity for pruning. CC~\cite{Li2021TowardsCC} combines tensor decomposition for filter pruning. NPPM~\cite{Gao2021NetworkPV} trains the network to predict the performance of the pruned model, guiding the pruning process. LRF-60~\cite{Joo2021LinearlyRF} uses Linearly Replaceable Filters to aid pruning decisions. AutoBot~\cite{Castells2021AutomaticNN} tests each filter one-by-one to ensure pruning precision. 
PGMPF~\cite{Cai2022PriorGM} utilizes prior masks as the basis for pruning masks. DLRFC~\cite{He2022FilterPV} uses Receptive Field Criterion to measure filter importance. FSM~\cite{Duan2022NetworkPV} aggregates feature and filter information to evaluate their relevance for shifting. Rrandom~\cite{Li2022RevisitingRC} employs random search strategies for pruning.  The study \cite{Hussien2024SmallCS} employs activation statistics, foundations in information theory, and statistical analysis with designed regularizations for pruning. CSD~\cite{Xie2024AdaptivePO} uses channel spatial dependability metric and leverages feature characteristics for pruning. However, the statistical learning information considered by these methods may not be precise enough.



\subsubsection{Information Bottleneck and Lasso Principle}

The Information Bottleneck (IB)~\cite{Tishby2000TheIB} method extracts the most relevant output information by considering the input. Aside from the previous mentioned methods, the works~\cite{Tishby2015DeepLA,Dai2018CompressingNN} were the first to use IB for compressing CNNs. FPGM~\cite{He2018FilterPV} employs the geometric median for pruning. L1~\cite{Li2016PruningFF} illustrates the correspondence between Lasso and IB for pruning. The approach in~\cite{He2017ChannelPF} utilizes a two-step optimization process incorporating Lasso for pruning. Papers~\cite{Scardapane2016GroupSR,Scardapane2016GroupSR} leverage the $l_2$ lasso (e.g., group lasso) for pruning, with analyzed complexity~\cite{Rajaraman2023GreedyPW}. APIB~\cite{guo2023automatic} integrates IB with the Hilbert-Schmidt Independence Criterion Lasso for exploration. The paper~\cite{Sakamoto2024EndtoEndTI} also shows through analysis that layer-level pruning, based on the Hilbert-Schmidt independence criterion, is preferable to end-to-end pruning.

However, none of these methods account for class-wise information. To address this, from the information bottleneck perspective, we employ state-of-the-art techniques like graph-structured lasso~\cite{Liu2024NovelTG} and tree-guided lasso~\cite{Liu2024SparseVS} to aggregate class-wise information effectively to propose our novel methods and prune filters inside CNNs.
% Dorador2024TheoreticalAE foreset