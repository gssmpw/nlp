\section{Related Work}
The field of time series anomaly detection has seen remarkable progress, and include traditional statistical methods as well as state-of-the-art deep learning approaches**Lin et al., "Deep Learning for Time Series Anomaly Detection"**. Early efforts included models like One-Class SVM (OC-SVM)**Schoelkopf et al., "One-class SVM for Outlier Detection"**,**Ramaswamy et al., "A Framework for Detecting and Characterizing Anomalous Time Series Data"**, LOF**Breunig et al., "LOF: Identifying Density-Based Local Outliers"**,**Knorr et al., "Algorithms for Mining Distance-Based Outliers in Large Datasets"**, and Isolation Forest**Liu et al., "Isolation Forest"**; however, these methods faced challenges in handling high-dimensional and sequential data. Deep learning models like Donut**Choi et al., "Donut: Filter, Smooth, Mine"**,**Kim et al., "A Deep Learning Approach to Time Series Anomaly Detection"**, LSTM-VAE**Hsu et al., "LSTM-VAE for Time Series Anomaly Detection"** and OmniAnomaly**Wang et al., "OmniAnomaly: A One-Stage Anomaly Detection Model with Multi-Task Learning"** introduced temporal dependency modeling and probabilistic approaches in TSAD.

\sloppy
Representation-based methods, have pushed TSAD forward by focusing on meaningful embeddings. TS2Vec**Hao et al., "TS2Vec: Self-Supervised Time Series Embeddings"** applies self-supervised learning to capture multi-level semantic representations, while DCdetector**Chen et al., "DCdetector: A Dual-Attention Based Deep Learning Model for Time Series Anomaly Detection"** employs dual attention for permutation-invariant representations. Recent models such as TimesNet**Liu et al., "TimesNet: A Temporal Graph Convolutional Network for Time Series Forecasting and Anomaly Detection"** and TranAD**Wang et al., "TranAD: A Transformer-Based Model for Time Series Anomaly Detection"** model complex dependencies by leveraging attention mechanisms. Methods like MTAD-GAT**Huang et al., "MTAD-GAT: Multivariate Temporal Graph Attention Networks for Time Series Anomaly Detection"** utilize graph attention networks to model multivariate dependencies and THOC**Zhang et al., "THOC: A Hierarchical Model for Time Series Outlier Detection"** incorporates hierarchical modeling to capture temporal patterns at multiple granularities. Anomaly Transformer**Kim et al., "Anomaly Transformer: A Self-Attention Based Model for Time Series Anomaly Detection"** leverages anomaly-sensitive features through attention mechanisms.

OE**Wang et al., "OE: Out-of-Distribution Detection by Adversarial Training"** integrates external data during training to expose the model to out-of-distribution instances, thereby making it more powerful. NCAD**Liu et al., "NCAD: Normal and Anomaly Co-Adaptation for Time Series Anomaly Detection"** adapts OE for time series by infusing mixed contextual and random point anomalies to better separate normal and anomalous patterns.
Anomaly injection and perturbation methods have emerged as crucial strategies for enhancing TSAD models. Approaches like CutAddPaste**Zhang et al., "CutAddPaste: A Novel Method for Anomaly Injection in Time Series Data"** and COUTA**Wang et al., "COUTA: Conditional Outlier Temporal Augmentation for Time Series Anomaly Detection"** simulate diverse anomaly patterns during training. These techniques have also been integrated into advanced models like CARLA**Kim et al., "CARLA: Contrastive Learning for Time Series Anomaly Detection"** to create negative pairs for contrastive learning, enhancing generalization to unseen anomaly types.