%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf, nonacm]{acmart}
% \documentclass[sigconf,anonymous,review]{acmart}

\usepackage{amsfonts}
\usepackage{amsmath}
% \usepackage{amssymb}

\usepackage{booktabs} % For better tables
\usepackage{longtable} % For long tables spanning multiple pages
\usepackage{lscape} % For landscape mode (if required)
\usepackage{multirow}
\usepackage{placeins} 
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{subcaption}

% Define theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma} % Lemma shares numbering with Theorem
\newtheorem{proposition}{Proposition}

\usepackage{threeparttable}
\usepackage{adjustbox}

%%


%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2025}
% \acmYear{2025}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[KDD '25]{}{August 03--07, 2025}{Toronto, Canada}
% \acmConference[arxiv]{2025}{February}{2025}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{GenIAS: Generator for Instantiating Anomalies in time Series}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Zahra Zamanzadeh Darban}
\email{zahra.zamanzadeh@monash.edu}
\orcid{}
\authornotemark[1]
\affiliation{%
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}
}

\author{Qizhou Wang}
\email{mike.wang@unimelb.edu.au}
\affiliation{%
  \institution{The University of Melbourne}
  \city{Parkville}
  \state{Victoria}
  \country{Australia}
}

\author{Geoffrey~I.~Webb}
\email{geoff.webb@monash.edu}
\affiliation{%
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}
}

\author{Shirui Pan}
\email{s.pan@griffith.edu.au}
\affiliation{%
\institution{Griffith University}
    \city{Gold Coast}
    \state{Queensland}
    \country{Australia}}

\author{Charu C.~Aggarwal}
\email{charu@us.ibm.com}
\affiliation{%
\institution{IBM T. J. Watson Research Center}
    \city{Yorktown Heights}
    \state{NY}
    \country{USA}}

\author{Mahsa Salehi}
\email{mahsa.salehi@monash.edu}
\affiliation{%
  \institution{Monash University}
  \city{Melbourne}
  \state{Victoria}
  \country{Australia}}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Darban et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
A recent and promising approach for building time series anomaly detection (TSAD) models is to inject synthetic samples of anomalies within real data sets. The existing injection mechanisms have significant limitations --- most of them rely on ad~hoc, hand-crafted strategies which fail to capture the natural diversity of anomalous patterns, or are restricted to univariate time series settings. To address these challenges, we design a generative model for TSAD using a variational autoencoder, which is referred to as a \underline{Gen}erator for \underline{I}nstantiating \underline{A}nomalies in Time \underline{S}eries ({\bf GenIAS}). GenIAS is designed to produce diverse and realistic synthetic anomalies for TSAD tasks. By employing a novel learned perturbation mechanism in the latent space and injecting the perturbed patterns in different segments of time series, GenIAS can generate anomalies with greater diversity and varying scales. Further, guided by a new triplet loss function, which uses a min-max margin and a new variance-scaling approach to further enforce the learning of compact normal patterns, GenIAS ensures that anomalies are distinct from normal samples while remaining realistic. 
%GenIAS utilizes a new variance-scaling approach to further enforce the learning of compact normal patterns, making the generated anomalies more distinguishable from normal samples. 
The approach is effective for both univariate and multivariate time series. We demonstrate the diversity and realism of the generated anomalies.
Our extensive experiments demonstrate that GenIAS --- when integrated into a TSAD task --- consistently outperforms seventeen traditional and deep anomaly detection models, thereby highlighting the potential of generative models for time series anomaly generation. 
% The source code is available on GitHub.\footnote{\url{https://anonymous.4open.science/r/genias_public}}.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Mathematics of computing~Time series analysis}
% \ccsdesc[500]{Computing methodologies~Anomaly detection}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Anomaly generator, Time series, Anomaly detection}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Unsupervised methods for Time Series Anomaly Detection (TSAD) often learn a model for boundary of normal data without access to labeled anomalies. This reliance on normal data makes models vulnerable to unseen or diverse anomaly patterns, which are common in time series settings. This is particularly true in complex multivariate scenarios, where anomalous patterns may vary significantly across different dimensions or even be correlated.

%Recent direction in TSAD have increasingly involved anomaly injection techniques to address the limitations of relying solely on normal-boundary learning. 
Anomaly injection techniques are an increasingly popular approach to address the limitations of solely relying on normal-boundary learning. Models like CARLA~\cite{DARBAN2025carla}, CutAddPaste~\cite{wang2024cutaddpaste}, NCAD~\cite{ncad2022}, and COUTA~\cite{Calibrated} have demonstrated the potential of synthetic anomaly injection in improving detection capabilities. However, these models face notable limitations. CARLA, CutAddPaste, and COUTA are restricted to predefined anomaly types, such as point, seasonal, trend, and shapelet anomalies. CutAddPaste further employs a rigid augmentation strategy (of ``cut-add-paste''), which may not align well with realistic patterns in complex applications. NCAD, on the other hand, relies on prior knowledge about anomalies to generate realistic anomalies. However, it is limited by the anomalies present in the datasets and hence may not be able to generate diverse types of anomalies. %Moreover, it is less versatile in scenarios with unlabeled data. %These methods are also susceptible because there is wide variability both in the types of time series data and the types of anomalies present in them (across different domains).
Finally, all the existing anomaly injection models are susceptible to slight variations in the time series, as they generate anomalies in the raw time series space. 

We address these gaps by introducing the Generator for Instantiating Anomalies in Time Series (GenIAS), a novel generative model designed to produce diverse and realistic synthetic anomalies for TSAD. In contrast to previous approaches, GenIAS operates in the latent space rather than the raw time series space. This makes it less susceptible to minor variations in the raw time series and allows it to learn more generalized normal patterns. While generative models mainly used to generate normal data, we are able to leverage the power of variational neural models to generate anomalies that align well with the temporal properties of time series, and with correlations between variables if the input time series is multivariate. GenIAS builds upon a hybrid Temporal Convolutional Network-Variational Autoencoder (TCN-VAE) architecture to move beyond rigid, handcrafted augmentation strategies and maps the complex properties of time series data into a multidimensional latent space. By employing a learned perturbation mechanism applied to the latent space variance and injecting the perturbed patterns into different segments of the time series using a patching mechanism, GenIAS generates anomalies with varying scales and enhanced diversity, resulting in more realistic anomaly patterns. We use the term \textit{realism} to refer to realistic anomaly patterns in this paper. Additionally, we employ an adjusted triplet loss function, guided by a min-max margin to guarantee that the generated anomalies are both distinct from normal data and realistic at the same time. Further, we introduce a novel variance-scaling approach, which enforces the compactness of latent representations, making the generated anomalies more distinguishable from normal samples. We demonstrate the effectiveness of GenIAS when extended as a TSAD model. 
Our key contributions are as follows:
\begin{itemize}
\item We propose GenIAS, which is a novel anomaly generator designed to seamlessly handle both univariate time series (UTS) and multivariate time series (MTS). By employing deep generative techniques, GenIAS produces anomalies that are both diverse and realistic. %, enabling enhanced anomaly detection capabilities.
% \item To improve the separation between normal and anomalous data, we introduce a novel loss function that rewards compact representations in the latent space in VAE-based anomaly generators by defining a tailored prior variance. This approach imposes tighter control over the latent space, enhancing the separation between normal and generated anomalous data both in the latent and raw time series spaces (see Figure~\ref{fig:genias_idea}). As a result, the compact representation facilitates the generation of diverse, realistic, and meaningful anomalies, thereby improving their utility for anomaly detection tasks. This approach has been supported by a theoretical statistical analysis that proves its effectiveness.
\item We propose a novel loss function for our VAE-based anomaly generator, enforcing compact latent representations via a tailored prior variance. This improves the separation between normal and anomalous data in latent space, enhancing anomaly generation (see Figure~\ref{fig:genias_idea}). A theoretical analysis supports its effectiveness.

\item We demonstrate GenIAS's ability to generate diverse and realistic anomalies through two newly introduced metrics. The evaluation is conducted on four state-of-the-art TSAD models leveraging anomaly injection, across nine widely used MTS and UTS datasets.
% \item We demonstrate the broad applicability of GenIAS by integrating it into a state-of-the-art TSAD model and compare the integrated model with seventeen baseline TSAD models. We conduct extensive experiments on nine widely used UTS and MTS datasets from diverse domains to validate the broad applicability of GenIAS. Our evaluation shows performance improvements that are directly attributable to the quality of GenIAS's anomaly generation. 
\item We demonstrate GenIAS's broad applicability by integrating it into a state-of-the-art TSAD model and comparing it with 17 baselines. Extensive experiments on nine diverse UTS and MTS datasets confirm its effectiveness, with performance gains directly linked to the quality of GenIAS's anomaly generation.

\end{itemize}
\begin{figure}[t]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/genias_idea.pdf}
    \vspace{-5pt}
    \caption{An illustration of our key idea: Variance-scaling enforces latent space compactness in the latent space, strengthening the separation between normal and generated anomalous samples by GenIAS, in latent space. The bottom figure, with $\sigma_{\text{prior}}$ = 0.5, visually shows this effect.}
    \vspace{-20pt}
    \label{fig:genias_idea}
\end{figure}

\section{Related Work}
The field of time series anomaly detection has seen remarkable progress, and include  traditional statistical methods as well as state-of-the-art deep learning approaches~\cite{schmidl2022anomaly,audibert2022deep,darban2024deep}. Early efforts included models like One-Class SVM (OC-SVM)~\cite{scholkopf1999support}, LOF~\cite{breunig2000lof}, and Isolation Forest~\cite{liu2008isolation}; however, these methods faced challenges in handling high-dimensional and sequential data. Deep learning models like Donut~\cite{xu2018donut}, LSTM-VAE~\cite{park2018lstmvae} and OmniAnomaly~\cite{su2019robust} introduced temporal dependency modeling and probabilistic approaches in TSAD.

\sloppy
Representation-based methods, have pushed TSAD forward by focusing on meaningful embeddings. TS2Vec~\cite{yue2022ts2vec} applies self-supervised learning to capture multi-level semantic representations, while DCdetector~\cite{yang2023dcdetector} employs dual attention for permutation-invariant representations. Recent models such as TimesNet~\cite{wu2023timesnet} and TranAD~\cite{tuli2022tranad} model complex dependencies by leveraging attention mechanisms. Methods like MTAD-GAT~\cite{zhao2020mtad} utilize graph attention networks to model multivariate dependencies and THOC~\cite{shen2020thoc} incorporates hierarchical modeling to capture temporal patterns at multiple granularities. Anomaly Transformer~\cite{xu2021anomalytran} leverages anomaly-sensitive features through attention mechanisms.

OE~\cite{hendrycks2018OE} integrates external data during training to expose the model to out-of-distribution instances, thereby making it more powerful. NCAD\cite{ncad2022} adapts OE for time series by infusing mixed contextual and random point anomalies to better separate normal and anomalous patterns.
Anomaly injection and perturbation methods have emerged as crucial strategies for enhancing TSAD models. Approaches like CutAddPaste~\cite{wang2024cutaddpaste} and COUTA~\cite{Calibrated} simulate diverse anomaly patterns during training. These techniques have also been integrated into advanced models like CARLA~\cite{DARBAN2025carla} to create negative pairs for contrastive learning, enhancing generalization to unseen anomaly types.

\section{GenIAS Method}
%This section addresses the problem statement, the generative mechanism, and refinement via patching. Finally, we demonstrate the generative mechanism in a TSAD case study.

\subsection{Problem Statement}
Let $\mathcal{D}$ represent a time series dataset consisting of $N$ time series windows, where each window \(\mathbf{X} \in \mathbb{R}^{T \times D}\) is a time series window of length \(T\) with \(D\) dimension(s). Assume that samples $X$ in \(\mathcal{D}\) is drawn from an unknown data distribution \(p(\mathbf{X})\). In TSAD datasets, the training data is typically unlabeled, and it is assumed normal since anomalies are rare and have negligible impact. The objective is to use a generative mapping \(\Gamma: \mathbf{X} \rightarrow \widetilde{\mathbf{X}}\) to transform a normal sample \(\mathbf{X}\) into an \textit{anomalous} sample \(\widetilde{\mathbf{X}} \in \mathbb{R}^{T \times D}\), such that the anomalous distribution \(p(\widetilde{\mathbf{X}})\) satisfies \(p(\widetilde{\mathbf{X}}) \neq p(\mathbf{X})\). The anomalies must preserve essential temporal properties in the normal data while exhibiting deviations that make them statistically separated.

The first challenge is to define a perturbation mechanism on the latent representation  \(\mathbf{z} \in \mathbb{R}^{L}\) with \(L\) dimensions, parameterized by \(p(\mathbf{z} \mid \mathbf{X})\), to produce a shifted latent distribution \(p(\widetilde{\mathbf{z}})\). This ensures that the anomalies are distinguishable, satisfying \(p(\widetilde{\mathbf{z}}) \neq p(\mathbf{z})\) while retaining plausibility by remaining within the data manifold. Furthermore, by perturbing the latent representations, our model is less susceptible to the slight deviations (e.g., noise) in the raw time series space. The second challenge is to densify the normal latent distribution \(p(\mathbf{z})\), enabling the mapping \(\Gamma(\mathbf{X}) = \hat{\mathbf{X}}\) to accurately reconstruct normal samples and establish a clear boundary for generating anomalies. Together, addressing these challenges guarantees that the generated anomalies achieve a balance between \textit{diversity} and \textit{realism}. %distinctiveness
This approach is designed to handle univariate (\( D=1 \)) and multivariate (\( D > 1 \)) time series.%, enabling the generation of labeled anomalous datasets that can be used to enhance the training and evaluation of TSAD models in various domains.

\subsection{Anomaly Generation}\label{sec:anom_generation}
GenIAS is a generative model designed to learn compact representations of normal time series data and generate realistic anomalies by strategically perturbing the latent space. The model comprises three main components: an encoder, a latent space representation, and a decoder. It is trained end-to-end with an objective function that balances reconstruction accuracy, latent space regularization, and anomaly generation quality.

\subsubsection{Model Workflow and Perturbation Mechanism}\label{sec:workflow}

The input to the model is a time series window \( \mathbf{X} \) of shape \( T \times D \). The input \( \mathbf{X} \) is normalized to ensure consistency across features, improving model stability during training.
The model architecture combines a TCN and a VAE to combine the strengths of both approaches. The VAE learns a latent representation of normal data and perturbs it in the latent space to enhance robustness to noisy time series and improve the learning of normal patterns. The TCN captures temporal dependencies within the time series more effectively. The \textbf{encoder} maps the input sample \( \mathbf{X} \) into a latent representation \( \mathbf{z} \) by capturing temporal dependencies through stacked convolutional layers. These layers, interleaved with activation functions and dropout, effectively model complex temporal patterns while reducing overfitting. The encoder outputs the parameters \( \mu \) (mean) and \( \sigma^2 \) (variance), which define a Gaussian distribution \( \mathcal{N}(\mu, \sigma^2) \) in the latent space, where $\mu$ and $\sigma$ are vectors of size $L$, the same as the size of latent vector $\mathbf{z}$. 

The \textbf{latent space representation} forms the core of the model. For normal samples, latent vectors \( \mathbf{z} \) are sampled as: $\mathbf{z} \sim \mathcal{N}(\mu, \sigma^2)$.
To generate anomalous representations \( \tilde{\mathbf{z}} \), the standard deviation \(\sigma\) is perturbed while keeping \(\mu\) unchanged:
\begin{equation}
\tilde{\mathbf{z}} = \mu + \psi \cdot (\sigma \odot \epsilon),
\label{eq:z_perturbed}
\end{equation}
where \( \psi \) is the learned perturbation scale, and \( \epsilon \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) \) is a noise vector. Perturbing \(\sigma\) instead of \(\mu\) ensures that the anomalies deviate in a way that preserves the central tendency of the normal data, thereby maintaining realism. Altering \(\mu\) could disrupt this central tendency, leading to unrealistic deviations that do not align with the underlying data manifold.

The \textbf{decoder} reconstructs the input sample from \( \mathbf{z} \) and \( \tilde{\mathbf{z}} \). Using transpose convolutional layers upscales the latent representations back to the original input dimensions while preserving the temporal structure. The decoder outputs the following:
\begin{itemize}
    \item \( \hat{\mathbf{X}} \): The reconstructed normal window, derived from \( \mathbf{z} \).
    \item \( \tilde{\mathbf{X}} \): The generated anomalous window, derived from \( \tilde{\mathbf{z}} \).
\end{itemize}

Figure \ref{fig:genias_arc} illustrates the overall workflow of the model, including the TCN-VAE architecture and the perturbation mechanism. The training process optimizes a combination of losses to ensure accurate reconstruction, realistic anomaly generation, and a compact latent space learning (see Algorithm \ref{alg:genias} for details).

\begin{figure*}[t]
    \centering
\includegraphics[width=1\linewidth]{figs/genias_arc.pdf}
    \vspace{-25pt}
    \caption{The overall architecture of GenIAS: The model comprises an encoder, a structured latent space, and a decoder. The encoder, built with a TCN-VAE, maps input time series windows into a latent representation by learning temporal dependencies. The latent space regularization ensures compact normal representations, while anomalies are generated by perturbing the variance of the learned distribution without altering the mean, preserving realistic deviations. The decoder reconstructs both normal and perturbed windows using transpose convolutional layers.}
    \vspace{-10pt}
    \label{fig:genias_arc}
\end{figure*}

% This part details the loss components and their integration into the total loss function of the model. The input dataset is denoted by \( \mathbf{X} \), reconstructed outputs by \( \hat{\mathbf{X}} \), and anomalous         outputs by \( \tilde{\mathbf{X}} \). The dataset size is represented as \( |\mathbf{X}| \), corresponding to the total number of samples.

\subsubsection{Reconstruction Loss}
\label{sec:rec_loss}

This portion of the overall loss is designed to accurately reconstruct normal input samples. It is defined using the Mean Squared Error (MSE) between an input sample \( \mathbf{X} \) and its reconstruction \( \hat{\mathbf{X}} \):
\begin{equation}
\mathcal{L}_{\text{recon}} = \frac{1}{|\mathcal{D}|} \sum_{\mathbf{X} \in \mathcal{D}} \frac{1}{T \cdot D} \sum_{t=1}^{T} \sum_{d=1}^{D} \big( \mathbf{X}[t, d] - \hat{\mathbf{X}}[t, d] \big)^2,
\label{eq:reconstruction_loss}
\end{equation}
%where \( \mathcal{D} \) is the dataset, \( \mathbf{X} \) is an input sample, \( \hat{\mathbf{X}} \) is the reconstructed output, \( T \) is the number of time steps, and \( D \) is the number of input dimensions. 
This loss penalizes the discrepancy between \( \mathbf{X} \) and \( \hat{\mathbf{X}} \), ensuring that the model captures the key temporal and structural features of normal samples.

\subsubsection{Perturbation Loss}
\label{sec:pert_loss}

The perturbation loss ensures that the generated anomalies \( \tilde{\mathbf{X}} \) are both distinct from normal samples \( \mathbf{X} \) and exhibit realistic deviations. This is achieved through two complementary terms: a \textbf{triplet loss} to enforce distinctiveness and a \textbf{regularization term} to maintain realism.

The first term, the \textbf{triplet loss}, encourages the original time series sample \( {\mathbf{X}} \) to be closer to the reconstructed sample \( \hat{\mathbf{X}} \) than the generated anomalous sample \( \tilde{\mathbf{X}} \), ensuring that the anomalies are sufficiently distinct from original time series. By maximizing the margin between \( d(\mathbf{X}, \hat{\mathbf{X}}) \) and \( d(\mathbf{X}, \tilde{\mathbf{X}}) \), where \( d(x, y) \) denotes the MSE distance, this term pushes the anomalous samples further away from the normal samples in the raw time series space. We also ensure that this margin is at least \(\delta_{\text{min}}\), thus the generated anomalies be more distinct.

The second term (the \textbf{regularization term}) prevents the anomalous samples \( \tilde{\mathbf{X}} \) from deviating excessively from the original samples, thereby preserving realism. It penalizes deviations where distance \( d(\mathbf{X}, \tilde{\mathbf{X}}) \) exceeds a maximum allowable threshold \(\delta_{\text{max}}\). This ensures that the generated anomalies remain plausible and do not degenerate into unstructured or unrealistic noise.

The perturbation loss is formulated as:
\begin{equation}
\begin{split}
\mathcal{L}_{\text{perturb}} = & \frac{1}{|\mathcal{D}|} \sum_{\mathbf{X} \in \mathcal{D}} \Big[ \max\big( d(\mathbf{X}, \hat{\mathbf{X}}) - d(\mathbf{X}, \tilde{\mathbf{X}}) + \delta_{\text{min}}, 0 \big) \Big] \\
& + \frac{1}{|\mathcal{D}|} \sum_{\mathbf{X} \in \mathcal{D}} \max\big( d(\mathbf{X}, \tilde{\mathbf{X}}) - \delta_{\text{max}}, 0 \big),
\end{split}
\label{eq:perturbation_loss}
\end{equation}
%where \( \mathcal{D} \) is the dataset, \( \mathbf{X} \) is an input sample, \( \hat{\mathbf{X}} \) is its reconstruction, and \( \tilde{\mathbf{X}} \) is the corresponding generated anomaly. The parameters \(\delta_{\text{min}}\) and \(\delta_{\text{max}}\) define the minimum margin for distinctness and the maximum allowable deviation for realism, respectively. 
Together, these terms ensure that the generated anomalies achieve the dual objectives of being distinct yet realistic.

\subsubsection{Zero-Perturbation Loss}
\label{sec:zero_pert_loss}

The zero-perturbation loss focuses on dimensions in the input sample \( \mathbf{X} \) that are entirely zero. It ensures that anomalies \( \tilde{\mathbf{X}} \) introduce meaningful perturbations in these dimensions, encouraging deviations from the original sample where appropriate.
The loss is defined as:
\begin{equation}
\mathcal{L}_{\text{zero-perturb}} = \frac{1}{|\mathcal{D}|} \sum_{\mathbf{X} \in \mathcal{D}}(\frac{1}{T \cdot |\mathcal{Z}|} \sum_{t=1}^{T} \sum_{d \in \mathcal{Z}} \big( d(\mathbf{X}, \tilde{\mathbf{X}}) + 1)^{-1},
\label{eq:zero_perturbation_loss}
\end{equation}
where \( \mathcal{Z} \) is the set of zero-valued dimensions in \( \mathbf{X} \). By penalizing dimensions that deviate unrealistically in regions of zero value, this loss encourages the model to generate plausible anomalies while preserving the unique characteristics of zero-valued dimensions.
%where \( \mathcal{D} \) is the dataset, \( \mathbf{X} \) is an input sample, \( \tilde{\mathbf{X}} \) is the corresponding generated anomaly, and \( \mathcal{Z} \) is the set of zero-valued dimensions in \( \mathbf{X} \). By penalizing dimensions that deviate unrealistically in regions of zero value, this loss encourages the model to generate plausible anomalies while preserving the unique characteristics of zero-valued dimensions in \( \mathbf{X} \).

\subsubsection{Enhanced KL Divergence Loss}
\label{sec:en_kl_loss}
The standard KL divergence loss is typically used to regularize the latent space by aligning the approximate posterior distribution \( q(z|\mathbf{X}) \), parameterized by \( \mu \) and \(\sigma^2 \), with a standard Gaussian prior \( p(z) \), defined as \( \mathcal{N}(0, 1) \). While effective for many tasks, this standard approach may fail to create the tight and compact latent representations necessary for generating anomalies that are distinguishable from normal samples.

To address this, we propose an enhanced KL divergence loss by incorporating a tunable prior variance, \( \sigma_{\text{prior}}^2 \), where \( \sigma_{\text{prior}} < 1 \). This modification enforces tighter representations of normal samples in the latent space. The modified KL divergence loss is defined as:
\begin{equation}
\mathcal{L}_{\text{en-KL}} = -\frac{1}{2} \sum_{i=1}^{|\mathbf{\mathcal{D}}|} \sum_{j=1}^{L} \Big[ 1 + \log(\sigma^2_{ij}) - \mu_{ij}^2 - \frac{\sigma^2_{ij}}{\sigma_{\text{prior}}^2} + 2 \log(\sigma_{\text{prior}}) \Big],
\label{eq:kl_divergence_loss}
\end{equation}
Where \( \mu_{ij} \) and \(\sigma^2_{ij}\) are the mean and variance of the latent distribution for the \( j \)-th dimension of the \( i \)-th sample, and \( \sigma_{\text{prior}} \) is the prior's standard deviation.

\begin{theorem}[Enhanced Separation Between Normals and Anomalies in Latent Space] \label{theorem:1}
Let a VAE be trained on normal data with a latent prior \( \mathcal{N}(0, \sigma^2_{\text{prior}}) \), where \( \sigma_{\text{prior}} < 1 \). The encoder posterior for normal samples follows \( \mathcal{N}(\mu, \sigma^2_{\text{normal}}) \), with compactness enforced by KL regularization. If the encoder \( \phi \) applies a perturbation scale \( \psi \), inflating variance as  
\(
\sigma^2_{\text{anom}} = \psi \sigma^2_{\text{normal}}, \quad \text{with } \psi > 1,
\)
then the KL divergence between normal and anomalous latent distributions strictly increases compared to the case where \( \sigma_{\text{prior}} \geq 1 \). %Consequently, the separation of reconstruction errors between normal and generated anomalies in raw time series space, i.e., \(\Delta \text{MSE} = \text{MSE}_{\text{anom}} - \text{MSE}_{\text{normal}}\), becomes larger.

%If, for anomalous inputs, the encoder produces a posterior with inflated variance (i.e. $\sigma^2_{\text{anom}} = \psi \sigma^2_{\text{normal}}$ with a scaling factor $\psi > 1$), then the divergence between the latent distributions of normal and anomalous samples increases. Consequently, because the decoder is less accurate away from the normal latent region, the separation in reconstruction errors \(\Delta \text{MSE} = \text{MSE}_{\text{anom}} - \text{MSE}_{\text{normal}}\) becomes larger.
\end{theorem}
The proof is provided in Appendix~\ref{app:proof_t_1}. We also illustrate in Figure~\ref{fig:mse_hist} in the Appendix how our enhanced KL loss leads to the separation of reconstruction error in the raw space.

The benefits of the enhanced KL divergence loss are as follows:
\begin{itemize}
    \item \textit{Controlled spread}: \( \sigma_{\text{prior}} \) determines the spread of normal samples in the latent space. Smaller values enforce tighter clustering, leading to compact representations while allowing limited diversity for variability. (See Lemma \ref{lemma:compact})
    \item \textit{Regularization}: The KL loss with \( \sigma_{\text{prior}} \) ensures a well-regularized latent space, reducing overfitting and promoting smooth sampling.
    \item \textit{Improved normal sample representation}: By constraining the latent space, normal samples occupy a distinct, dense region, enhancing their separability from anomalies. (See Lemma \ref{lemma-recon-norm} and Theorem \ref{theorem:1})
    \item \textit{Enhanced interpretability}: The structured latent space shaped by \( \sigma_{\text{prior}} \) provides consistent and interpretable embeddings for both normal and anomalous data.
\end{itemize}

\subsubsection{Total Loss}
\label{sec:total_loss}
The total loss integrates all the above components, each weighted by a corresponding hyperparameter:
\begin{equation}
\mathcal{L}_{\text{total}} = \alpha \cdot \mathcal{L}_{\text{recon}} 
+ \beta \cdot \mathcal{L}_{\text{perturb}} 
+ \gamma \cdot \mathcal{L}_{\text{zero-perturb}} 
+ \zeta \cdot \mathcal{L}_{\text{en-KL}},
\label{eq:total_loss}
\end{equation}
Where \( \alpha \), \( \beta \), \( \gamma \), and \( \zeta \) are the weights for the reconstruction, perturbation, zero-perturbation, and KL divergence losses, respectively.

\subsection{Deviation Patching Approach} \label{sec:patching}
After training the GenIAS model in the first step, the anomaly injection process uses the pretrained model to perturb the latent space representation of time series windows (\( \mathbf{X} \)) and generate realistic anomalous samples (\( \tilde{\mathbf{X}} \)). This step consists of two phases: (1) anomaly injection via latent space perturbation and (2) patching the injected anomalies using a deviation-based approach, defined by a scaling factor \( \tau \), to ensure the generated samples are realistic and aligned with the domain requirements. The injection and patching process is applied independently for each dimension (\( d \)) of the MTS, and more details are provided in Algorithm~\ref{alg:patching}.
\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/smap-e2-d0.pdf}
        \label{fig:smap-e2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\textwidth}
        \includegraphics[width=\textwidth]{figs/msl-p15-d0.pdf}
        \label{fig:msl-p15}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\textwidth}
        \includegraphics[width=\textwidth]{figs/smd-m32-d2.pdf}
        \label{fig:smd-m32}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\textwidth}
        \includegraphics[width=\textwidth]{figs/yahoo-r13.pdf}
        \label{fig:yahoo-r13}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.19\textwidth}
        \includegraphics[width=\textwidth]{figs/kpi-e0747cad.pdf}
        \label{fig:kpi-e0747cad}
    \end{subfigure}
    \vspace{-20pt}
    \caption{Effect of deviation-based patching with different thresholds (0.4, 0.2, 0.05) on generated anomalous windows across various entities from the MSL, SMAP, SMD, Yahoo, and KPI datasets.}
    \vspace{-10pt}
    \label{fig:patching}
\end{figure*}

% \subsection{Case Study: GenIAS in TSAD}
% \subsection{Case Study: Integration with CARLA}
% To demonstrate the effectiveness of the proposed anomaly generator in TSAD, GenIAS, we integrate it with CARLA---a state-of-the-art contrastive learning model for TSAD. Specifically, we replace CARLA's original anomaly injection mechanism with GenIAS. This integration allows us to evaluate how GenIAS enhances the representation learning and anomaly detection capabilities in TSAD models like CARLA.
% \vspace{-21pt}
\section{Experiments}
% In this section, w
We conduct extensive evaluations of GenIAS's performance using a comprehensive selection of datasets and baselines. Our evaluation consists of two parts: (1) Generation Evaluation, where we compare GenIAS against other anomaly injection/generation models to assess its capability in generating realistic and diverse anomalies, and (2) Case Study – TSAD Evaluation, where we benchmark GenIAS against seventeen competing methods using nine real-world datasets covering both MTS and UTS. Specifically, to study the effectiveness of GenIAS's anomaly generation in TSAD, we integrate GenIAS with the anomaly detector architecture from CARLA to implement a GenIAS-enhanced TSAD model (i.e., substituting CARLA's anomaly generator with GenIAS while keeping the rest of the framework identical). We choose CARLA as our base detector due to its SOTA performance, which enables direct comparison with the strongest generation-based TSAD method. Through this comprehensive evaluation, we provide a thorough analysis of GenIAS's capabilities in both anomaly generation and TSAD tasks.
% This comprehensive evaluation ensures a thorough analysis of GenIAS’s capabilities in both anomaly generation and TSAD performance.


\setlength{\textfloatsep}{10pt}
\setlength{\tabcolsep}{3pt}
\begin{table}[t]
\centering
\caption{An overview of dataset statistics.}
\vspace{-10pt}
\label{tab:dset}
\begin{adjustbox}{width=0.8\linewidth}
\begin{tabular}{c|cccccc}
\hline
\textbf{Benchmark} & \textbf{\# datasets} & \textbf{\# dims} & \textbf{Train size} & \textbf{Test size} & \textbf{AR\%} \\ \hline
MSL~\citep{hundman2018detecting} & 27 & 55 & 58,317  & 73,729  & 10.72\% \\
SMAP~\citep{hundman2018detecting} & 55 & 25 & 140,825 & 444,035 & 13.13\% \\
SMD~\citep{su2019robust} & 28 & 38 & 708,405 & 708,420 & 4.16\%  \\
SWaT~\citep{mathur2016swat} & 1 & 51 & 496,800 & 449,919 & 12.33\%  \\
GECCO~\citep{moritz2018gecco} & 1 & 9 & 69,260 & 69,261 & 1.1\%  \\
SWAN~\citep{DVN/EBCFKM_2020} & 1 & 38 & 60,000 & 60,000 & 32.6\%  \\
UCR~\citep{wu2021current} & 205 & 1  & 2,238,349 & 6,143,541 & 0.6\%  \\
Yahoo-A1~\citep{yahoods} & 67 & 1  & 46,667  & 46,717  & 1.76\%  \\
KPI~\citep{aiops_challenge_2018} & 29 & 1  & 1,048,576 & 2,918,847 & 1.87\%  \\ \hline
\end{tabular}%
\end{adjustbox}
\end{table}

% \vspace{-4pt}
\subsection{Benchmark Datasets} \label{sec:datasets}
GenIAS is thoroughly evaluated on nine widely adopted real-world benchmark datasets for TSAD. Their key statistics are reported in Table \ref{tab:dset}. In particular, our selection includes six MTS datasets (MSL, SMAP, SMD, SWaT, GECCO, and SWAN) and three UTS datasets (UCR, Yahool-A1 and KPI), containing diverse anomaly types and a variety of anomaly ratios, thereby providing a comprehensive evaluation of the performance of our proposed methods and the baselines. Detailed dataset descriptions can be found in Appendix~\ref{app:dataset}.

\subsection{Baselines} \label{sec:baselines}
% The proposed approach is compared against seventeen relevant and top TSAD models, encompassing diverse categories of anomaly detection methods.
GenIAS is compared with seventeen relevant and competitive baseline methods that leverage a wide array of anomaly detection approaches, which can be categorised into four broad categories: classic anomaly detection methods, positive-unlabeled (P-U) methods, unsupervised deep TSAD methods that do not leverage anomaly generation  (US-WI), and unsupervised TSAD methods that employ anomaly generation (US-I).

Specifically, three classic general AD methods - OCSVM~\cite{scholkopf1999support}, LOF~\cite{breunig2000lof}, and Isolation Forest~\cite{liu2008isolation} - are included to establish a base performance for demonstrating the advantages of deep learning TSAD approaches. LSTM-VAE~\cite{park2018lstmvae}, a deep learning TSAD method that utilises labeled anomalies, is included to examine the effects of leveraging labelled anomalies 
for TSAD.
% Another particularly relevant category is US-WI, as they assume identical availability of anomaly prior knowledge as our setting. Our selection of US-WI methods includes various label-free TSAD supervision approaches. Donut~\cite{xu2018donut}, OmniAnomaly~\cite{su2019robust}, AnomalyTransformer~\cite{xu2021anomalytran}, and TranAD~\cite{Tuli2022TranADDT} use reconstruction error to detect TS anomalies. THOC~\cite{shen2020timeseries} and TimesNet~\cite{wu2023timesnet} treat TSAD as a forecasting problem, detecting anomalies based on deviations from predicted/expected patterns. MTAD-GAT~\cite{zhao2020mtad} integrates graph attention mechanisms to capture both temporal and multivariate dependencies. Representation learning methods like TS2Vec~\cite{yue2022ts2vec} and DCdetector~\cite{yang2023dcdetector} focus on establishing robust and anomaly-discriminative representation spaces to spot anomalies.
US-WI is another relevant category, which includes a variety of unsupervised TSAD baselines that do not leverage anomaly generation. Donut~\cite{xu2018donut}, OmniAnomaly~\cite{su2019robust}, AnomalyTransformer~\cite{xu2021anomalytran}, and TranAD~\cite{Tuli2022TranADDT} use reconstruction error to detect anomalies. THOC~\cite{shen2020timeseries} and TimesNet~\cite{wu2023timesnet} treat TSAD as a forecasting problem. MTAD-GAT~\cite{zhao2020mtad} integrates graph attention mechanisms to capture both temporal and multivariate dependencies. Representation learning methods like TS2Vec~\cite{yue2022ts2vec} and DCdetector~\cite{yang2023dcdetector} focus on establishing robust and anomaly-discriminative representations.
% US-I is the most relevant category, not only because of its unsupervised nature but also because it enables us to highlight the advantages of GenIAS's anomaly generation capabilities. 
US-I is the most relevant category, which allows us to compare GenIAS with existing generation methods for unsupervised TSAD. We include four recent unsupervised TSAD methods that leverage various anomaly generation approaches: NCAD~\cite{ncad2022}, CutAddPaste~\cite{wang2024cutaddpaste}, COUTA~\cite{Calibrated}, and CARLA~\cite{DARBAN2025carla}.
% NCAD~\cite{ncad2022}, CutAddPaste~\cite{wang2024cutaddpaste}, and COUTA~\cite{Calibrated} generate synthetic anomalies based on predefined anomaly patterns during training.  CARLA~\cite{DARBAN2025carla} exemplifies recent advances in self-supervised learning by incorporating anomaly injection within a contrastive learning framework. 
% 9

\subsection{Evaluation Metrics}
\label{sec:metrics}
We adopt seven evaluation metrics to assess GenIAS's performance in terms of generation quality and TSAD performance.

\noindent \textbf{Generation quality.} 
To quantitatively examine the quality of anomaly generation, we introduce two key metrics: \textit{Anomalous Representation Proximity (ARP)}, which assesses realism, and \textit{Entropy-Based Diversity Index (EDI)}, which characterizes the diversity of the generated anomalies, motivated by the idea in AnomalyDiffusion~\cite{hu2024anomalydiffusion}. Due to page limits, the definitions and discussions of these metrics are provided in the Appendix~\ref{app:eval_metrics}. In general, ARP is the primary metric as it measures the closeness of the generated anomalies to the actual anomalies, see Appendix~\ref{app:ARP}. EDI is complementary to ARP, as diversity should only be examined under the condition that the generated anomalies can illustrate the actual anomalies, see Appendix~\ref{app:EDI}.
% which quantifies diversity of generated anomalies. Together, these metrics provide a comprehensive assessment of anomaly generation.

\noindent\textbf{TSAD evaluation.} We consider five widely-used, complementary metrics: Best F1 score, AUPR (Area Under the Precision-Recall Curve), Affiliation F1 ~\cite{huet2022local}, PATE ~\cite{Pate2024}, and AUROC (Area Under the Receiver Operating Characteristic Curve) for TSAD performance evaluation. This combination of metrics provides a comprehensive, fair and robust TSAD evaluation. Detailed descriptions of evaluation metrics are provided in Appendix~\ref{app:tsadmetrics}.

\subsection{Implementation Details}
Our experiments are conducted on a server with an NVIDIA A40 GPU, 13 CPU cores, and 250GB RAM. We employ the unsupervised TSAD network architecture and training scheme from CARLA as our default TSAD model to perform GenIAS-enhanced detection. GenIAS is implemented in Python using PyTorch and trained using the Adam optimizer with a learning rate of $10^{-4}$ for 1000 epochs. The default window size $T$ is set to 200, and latent dimension size $L$ is set to 100 and 50, for MTS and UTS respectively. The hyperparameters for the total loss are $\alpha=1.0$, $\beta=0.1$, $\gamma=0.0/0.01$ (for UTS/MTS), and $\zeta=0.1$, which are fixed for all datasets. We adopt the baseline results that are produced using the original implementation and maintain their default hyperparameter settings. 
Due to the page limit, the detailed implementation details and hyperparameter settings are provided in Appendix~\ref{sec:implementation}.

\begin{table}[t]
\small
\caption{Anomaly generation quality of GenIAS and US-I baselines in ARP, with the best model in bold and the second-best model underlined, where CAP abbreviates CutAddPaste.}
\vspace{-10pt}
\label{table:arp}
\begin{adjustbox}{width=0.75\columnwidth}
\begin{tabular}{ccccccc}
\toprule
Metric                & Dataset & COUTA               & NCAD            & CAP             & CARLA           & GenIAS          \\ \midrule
\multirow{10}{*}{ARP} & MSL     & \textbf{0.3613}              & 0.3568          & 0.3406          & 0.3586          & \underline{ 0.3605}    \\
                      & SMAP    & 0.7573              & 0.8052          & \underline{ 0.8110}    & 0.7636          & \textbf{0.8113} \\
                      & SMD     & \underline{ 0.0951}        & 0.0810          & 0.0896          & \textbf{0.0958} & 0.0922          \\
                      & SWaT    & 0.9970              & 0.9964          & 0.9963          & \textbf{0.9980} & \underline{ 0.9974}    \\
                      & GECCO   & \underline{ 0.1296}        & 0.1145          & \textbf{0.1335} & 0.1163          & 0.1265          \\
                      & SWAN    & 0.2400              & 0.2362          & 0.2313          & \underline{ 0.2410}    & \textbf{0.2415} \\
                      & Yahoo   & 0.6541              & 0.6139          & 0.6476          & \underline{ 0.6632}    & \textbf{0.6756} \\
                      & KPI     & 0.6856              & \textbf{0.8427} & 0.7207          & 0.7582          & \underline{ 0.7907}    \\
                      & UCR     & 0.1998              & 0.2059          & 0.1936          & \underline{ 0.2086}    & \textbf{0.2161} \\ \cmidrule(lr){2-7}
                      & Rank    & 3.11                & 3.78            & 3.89            & \underline{ 2.44}      & \textbf{1.78}  \\  \cmidrule(lr){1-7}
\multirow{10}{*}{EDI} & MSL     & \underline{ 0.8251}        & 0.7311          & 0.7169          & 0.5822          & \textbf{0.8833} \\
                      & SMAP    & \underline{ 0.8557}        & 0.6699          & 0.6364          & 0.6335          & \textbf{0.8932} \\
                      & SMD     & \underline{ 0.8782}        & 0.7648          & 0.6404          & 0.8041          & \textbf{0.9262} \\
                      & SWaT    & 0.6425              & 0.6164          & \textbf{0.7967}          & 0.6005          & \underline{ 0.7259}    \\
                      & GECCO   & 0.7379              & 0.5571          & 0.7072          & \textbf{0.9327}          & \underline{ 0.8303}    \\
                      & SWAN    & \underline{ 0.9589}        & 0.9189          & 0.7428          & 0.9286          & \textbf{0.9840} \\
                      & Yahoo   & \underline{ 0.8627}        & 0.7202          & 0.7577          & 0.5948          & \textbf{0.9182} \\
                      & KPI     & \underline{ 0.7607}        & 0.6166          & 0.7505          & 0.6962          & \textbf{0.8442} \\
                      & UCR     & \underline{ 0.8166}        & 0.7321          & 0.7536          & 0.7483          & \textbf{0.8961} \\ \cmidrule(lr){2-7}
                      & Rank    & \underline{ 2.22} & 4.11            & 3.56            & 3.89            & \textbf{1.22}   \\  
                      \bottomrule
\end{tabular}
\end{adjustbox}
\vspace{-5pt}
\end{table}

% \begin{table}[t]
% \caption{TSAD performance of GenIAS and baseline models using EDI and ARP, with the best model in bold and the second-best model underlined.}
% \vspace{-10pt}
% \label{table:gen_results}
% \centering
% \begin{tabular}{llccccc} 
% \toprule
% Dataset & Metric & COUTA & NCAD & CAP\footnotemark[1] & CARLA & GenIAS \\ 
% \midrule
% \multirow{2}{*}{MSL} & EDI & \underline{0.8251} & 0.7311 & 0.7169 & 0.5822 & \textbf{0.8833} \\
%  & ARP & \textbf{0.3613} & 0.3568 & 0.3406 & 0.3586 & \underline{0.3605} \\
% \midrule
% \multirow{2}{*}{SMAP} & EDI & \underline{0.8557} & 0.6699 & 0.6364 & 0.6335 & \textbf{0.8932} \\
%  & ARP & 0.7573 & 0.8052 & \underline{0.8110} & 0.7636 & \textbf{0.8113} \\
% \midrule
% \multirow{2}{*}{SMD} & EDI & \underline{0.8782} & 0.7648 & 0.6404 & 0.8041 & \textbf{0.9262} \\
%  & ARP & \underline{0.0951} & 0.0810 & 0.0896 & \textbf{0.0958} & 0.0922 \\
% \midrule
% \multirow{2}{*}{SWaT} & EDI & 0.6425 & 0.6164 & \textbf{0.7967} & 0.6005 & \underline{0.7259} \\
%  & ARP & 0.9970 & 0.9964 & 0.9963 & \textbf{0.9980} & \underline{0.9974} \\
% \midrule
% \multirow{2}{*}{GECCO} & EDI & 0.7379 & 0.5571 & 0.7072 & \textbf{0.9327} & \underline{0.8303} \\
%  & ARP & \underline{0.1296} & 0.1145 & \textbf{0.1335} & 0.1163 & 0.1265 \\
% \midrule
% \multirow{2}{*}{SWAN} & EDI & \underline{0.9589} & 0.9189 & 0.7428 & 0.9286 & \textbf{0.9840} \\
%  & ARP & 0.2400 & 0.2362 & 0.2313 & \underline{0.2410} & \textbf{0.2415} \\
% \midrule
% \multirow{2}{*}{Yahoo} & EDI & \underline{0.8627} & 0.7202 & 0.7577 & 0.5948 & \textbf{0.9182} \\
%  & ARP & 0.6541 & 0.6139 & 0.6476 & \underline{0.6632} & \textbf{0.6756} \\
% \midrule
% \multirow{2}{*}{KPI} & EDI & \underline{0.7607} & 0.6166 & 0.7505 & 0.6962 & \textbf{0.8442} \\
%  & ARP & 0.6856 & \textbf{0.8427} & 0.7207 & 0.7582 & \underline{0.7907} \\
% \midrule
% \multirow{2}{*}{UCR} & EDI & \underline{0.8166} & 0.7321 & 0.7536 & 0.7483 & \textbf{0.8961} \\
%  & ARP & 0.1998 & 0.2059 & 0.1936 & \underline{0.2086} & \textbf{0.2161} \\
% \midrule
% \bottomrule
% \multicolumn{2}{l}{\footnotesize 1 CAP: CutAddPaste}
% \end{tabular}
% \end{table}

\subsection{Generation Performance}
% Before studying TSAD performance, 
We first examine the anomaly generation quality of GenIAS and the competing methods that employ anomaly injection (i.e., the US-I category, see section~\ref{sec:baselines}) in terms of ARP and EDI, as discussed in Section \ref{sec:metrics}. We report the results of ARP and EDI in Table \ref{table:arp}. GenIAS achieves the highest average ARP ranking, suggesting it generates overall the most realistic anomalies across all datasets. The realism of the anomaly distribution is essential for providing anomaly-informed supervision in the absence of labelled anomalies. Similarly for EDI, GenIAS also achieves the highest average ranking, indicating good diversity in its generated anomalies while sufficiently illustrating the actual anomalies. This diversity implies that the generated anomalies can also lie beyond the original anomaly distribution, which can be beneficial for learning a more stringent normal class boundary, further improving TSAD performance, provided that they do not interfere with the identification of the normal region. To achieve this, GenIAS treats creating informative but not excessive diversity as a key consideration through its novel perturbation loss.

% Table~\ref{table:gen_results} shows the generation results.
\begin{table*}[htbp]
\caption{TSAD performance of GenIAS and baseline methods using F1 score and AUPR, with the best model in bold and the second-best model underlined. U and M denote results unavailable for models limited to UTS or MTS, respectively.}
\label{table:main}
\vspace{-10pt}
\begin{adjustbox}{width=0.8\textwidth}
\begin{tabular}{ccccccccccccccc} \toprule
                       &                          &                    & \multicolumn{7}{c}{MTS}                      & \multicolumn{4}{c}{UTS} &       \\ \cmidrule(lr){4-10}  \cmidrule(lr){11-14}

Metric                 & Cat.                     & Methods            & MSL   & SMAP  & SMD   & SWAT  & GECCO & SWAN  & Avg-M & Yahoo  & KPI   & UCR   & Avg-U & Avg   \\ \midrule
\multirow{18}{*}{F1}   & \multirow{3}{*}{Classic} & OCSVM (1999)       & 0.308 & 0.411 & 0.319 & 0.743 & 0.297 & 0.557 & 0.439 & 0.608  & 0.148 & 0.011 & 0.256 & 0.378 \\
                       &                          & LOF (2000)         & 0.368 & 0.400 & 0.204 & 0.331 & 0.075 & 0.655 & 0.339 & 0.586  & 0.101 & 0.010 & 0.232 & 0.303 \\
                       &                          & IForest (2008)     & 0.278 & 0.353 & 0.306 & 0.745 & 0.255 & 0.717 & 0.442 & 0.553  & 0.236 & 0.010 & 0.266 & 0.384 \\ \cmidrule(lr){3-15}
                       & PU                      & LSTM-VAE (2018)    & 0.407 & 0.437 & 0.298 & 0.724 & 0.208 & 0.625 & 0.450 & M      & M     & M     & M     & M     \\ \cmidrule(lr){3-15}
                       & \multirow{9}{*}{US-WI}  & Donut (2018)       & U     & U     & U     & U     & U     & U     & U     & 0.489  & 0.126 & 0.009 & 0.208 & U     \\
                       &                          & Omni (2019)        & 0.243 & 0.325 & 0.459 & \textbf{0.763} & 0.166 & 0.541 & 0.416 & M      & M     & M     & M     & M     \\
                       &                          & THOC (2020)        & 0.309 & 0.327 & 0.168 & 0.638 & 0.146 & 0.506 & 0.349 & 0.253  & 0.233 & 0.010 & 0.165 & 0.288 \\
                       &                          & MTAD-GAT(2020)     & 0.473 & 0.519 & 0.347 & 0.242 & 0.313 & 0.599 & 0.416 & M      & M     & M     & M     & M     \\
                       &                          & AnomalyTran (2021) & 0.345 & 0.407 & 0.304 & 0.738 & 0.155 & 0.524 & 0.412 & M      & M     & M     & M     & M     \\
                       &                          & TranAD (2022)      & 0.428 & 0.472 & 0.361 & 0.310 & 0.280 & 0.527 & 0.396 & 0.566  & 0.287 & 0.010 & 0.288 & 0.360 \\
                       &                          & TS2Vec (2022)      & 0.299 & 0.371 & 0.173 & 0.261 & 0.063 & 0.502 & 0.278 & 0.484  & 0.204 & 0.007 & 0.232 & 0.263 \\
                       &                          & TimesNet (2023)    & 0.358 & 0.402 & 0.338 & 0.217 & 0.297 & 0.492 & 0.351 & 0.513  & 0.241 & 0.011 & 0.255 & 0.319 \\
                       &                          & Dcdetector (2023)  & 0.227 & 0.275 & 0.083 & 0.217 & 0.021 & 0.492 & 0.219 & 0.112  & 0.042 & 0.008 & 0.054 & 0.164 \\  \cmidrule(lr){3-15}
                       & \multirow{4}{*}{US-I}   & NCAD (2022)        & 0.266 & 0.358 & 0.183 & 0.217 & 0.296 & 0.492 & 0.302 & 0.134  & 0.168 & 0.020 & 0.107 & 0.237 \\
                       &                          & CutAddPaste (2024) & 0.363 & 0.458 & 0.192 & 0.700 & 0.280 & 0.896 & 0.482 & 0.321  & \textbf{0.620} & \underline{0.042} & 0.328 & 0.430 \\
                       &                          & COUTA (2024)       & 0.414 & 0.421 & 0.362 & 0.312 & \textbf{0.389} & 0.626 & 0.421 & 0.658  & 0.221 & 0.010 & 0.296 & 0.379 \\
                       &                          & CARLA (2025)       & \underline{0.523} &\underline{0.529} & \underline{0.511} & 0.721 & 0.293 & \underline{0.908} & \underline{0.581} & \underline{0.749}  & 0.308 & 0.039 & \underline{0.365} & \underline{0.509} \\ \cmidrule(lr){3-15}
                       & Ours                     & GenIAS             & \textbf{0.579} & \textbf{0.620} & \textbf{0.528} & \underline{0.755} & \underline{0.329} & \textbf{0.937} & \textbf{0.625} & \textbf{0.811}  & \underline{0.393} & \textbf{0.064} &\textbf{0.423} & \textbf{0.557} \\ \midrule\midrule
\multirow{18}{*}{AUPR} & \multirow{3}{*}{Classic} & OCSVM (1999)       & 0.202 & 0.256 & 0.327 & 0.696 & 0.163 & 0.608 & 0.375 & \underline{0.703}  & 0.300 & 0.028 & 0.344 & 0.365 \\ 
                       &                          & LOF (2000)         & 0.217 & 0.248 & 0.277 & 0.185 & 0.031 & 0.624 & 0.264 & 0.615  & 0.109 & 0.043 & 0.256 & 0.261 \\
                       &                          & IForest (2008)     & 0.182 & 0.190 & 0.278 &\textbf{0.739} & 0.162 & 0.739 & 0.382 & 0.614  & 0.288 & 0.021 & 0.308 & 0.357 \\ \cmidrule(lr){3-15}
                       & PU                      & LSTM-VAE (2018)    & 0.285 & 0.258 & 0.395 & 0.685 & 0.045 & 0.674 & 0.390 & M      & M     & M     & M     & M     \\ \cmidrule(lr){3-15}
                       & \multirow{9}{*}{US-WI}  & Donut (2018)       & U     & U     & U     & U     & U     & U     & U     & 0.264  & 0.078 & 0.015 & 0.119 & U    \\
                       &                          & Omni (2019)        & 0.149 & 0.115 & 0.365 & \underline{0.713} & 0.034 & 0.587 & 0.327 & M      & M     & M     & M     & M     \\
                       &                          & THOC (2020)        & 0.240 & 0.195 & 0.107 & 0.537 & 0.076 & 0.432 & 0.265 & 0.349  & 0.229 & 0.014 & 0.197 & 0.242 \\
                       &                          & MTAD-GAT(2020)     & 0.335 & 0.339 & 0.401 & 0.095 & 0.200 & 0.613 & 0.331 & M      & M     & M     & M     & M     \\
                       &                          & AnomalyTran (2021) & 0.236 & 0.264 & 0.273 & 0.680 & 0.059 & 0.568 & 0.347 & M      & M     & M     & M     & M     \\
                       &                          & TranAD (2022)      & 0.273 & 0.287 & 0.412 & 0.192 & 0.119 & 0.586 & 0.312 & 0.691  & 0.285 & 0.022 & 0.333 & 0.319 \\
                       &                          & TS2Vec (2022)      & 0.132 & 0.148 & 0.113 & 0.136 & 0.040 & 0.408 & 0.163 & 0.491  & 0.221 & 0.009 & 0.240 & 0.189 \\
                       &                          & TimesNet (2023)    & 0.283 & 0.208 & 0.385 & 0.082 & 0.189 & 0.508 & 0.276 & 0.671  & 0.237 & 0.032 & 0.313 & 0.288 \\
                       &                          & Dcdetector (2023)  & 0.129 & 0.124 & 0.043 & 0.126 & 0.011 & 0.326 & 0.127 & 0.041  & 0.018 & 0.009 & 0.023 & 0.092 \\ \cmidrule(lr){3-15}
                       & \multirow{4}{*}{US-I}                  & NCAD (2022)        & 0.146 & 0.156 & 0.096 & 0.106 & 0.135 & 0.407 & 0.174 & 0.067  & 0.089 & 0.103 & 0.086 & 0.145 \\ 
                       &                          & CutAddPaste (2024) & 0.242 & 0.322 & 0.153 & 0.582 & 0.159 & 0.808 & 0.378 & 0.116  & \textbf{0.602} & 0.188 & 0.302 & 0.352 \\
                       &                          & COUTA (2024)       & 0.247 & 0.248 & 0.400 & 0.159 & \textbf{0.264} & 0.685 & 0.334 & 0.555  & 0.282 & 0.049 & 0.295 & 0.321 \\
                       &                          & CARLA (2025)       & \underline{0.501} & \underline{0.448} & \underline{0.507} & 0.681 & 0.201 & \underline{0.814} & \underline{0.525} & 0.627  & 0.299 &\underline{0.247} & \underline{0.391} & \underline{0.481} \\ \cmidrule(lr){3-15}
                       & Ours                     & GenIAS             & \textbf{0.529} & \textbf{0.467} & \textbf{0.512} & 0.704 & \underline{0.236} & \textbf{0.882} & \textbf{0.555} & \textbf{0.794}  & \underline{0.357} & \textbf{0.280} & \textbf{0.477} & \textbf{0.529} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\end{table*}

\subsection{Visualization} \label{sec:visual}
% Figure~\ref{fig:tsne} visualizes the distribution of of real and generated/injected anomalies in the representation space using t-SNE plots. t-SNE is used to visualize how well generated anomalies align with real anomalies in the test datasets, helping assess both their similarity and diversity across different datasets, including Yahoo, KPI, UCR, SMAP, and SMD. 
% The results show that GenIAS produces a more diverse and well-distributed set of anomalies. 
\begin{figure*}[t]
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{figs/yahoo_tsne.pdf}
        \label{fig:yahoo_tsne}
        \vspace{-15pt}
        \caption{Yahoo - Entity real\_65}
        % \vspace{5pt}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{figs/smap_tsne.pdf}
        \label{fig:smap_tsne}
        \vspace{-15pt}
        \caption{SMAP - Entity E5}
        % \vspace{5pt}
    \end{subfigure}
    \vspace{-10pt}
    \caption{Each plot visualizes a different time series (TS) injection or generation method using t-SNE. Blue markers represent the original anomalous TS, while orange markers represent the injected or generated anomalous TS.}
    \vspace{-5pt}
    \label{fig:tsne}
\end{figure*}
To illustrate GenIAS's enhanced anomaly generation compared to baseline methods that also leverage anomaly generation (US-I), we present t-SNE visualizations comparing their generated and real anomalies in Figure~\ref{fig:tsne} on two selected datasets, Yahoo (UTS) and SMAP (MTS). Some additional visualizations are included in Figure \ref{fig:ext_tsne} in Appendix \ref{sec:apd:vis}.
% Due to the page limit, please refer to the Appendix for the complete visualization. 

Impressively, the distributions of the generated anomalies by GenIAS align with the actual anomalies distribution substantially closer compared to the baseline methods (i.e., actual anomaly regions are densely covered by the generated anomalies).
% , with few superficial anomalies located in normal (uncolored) regions). 
This is essential to provide anomaly-discriminative information for establishing more distinctive normal and anomaly representations in unsupervised TSAD training.
% , and a more selective reconstruction of VAEs. 
On the other hand, the baseline methods, to varying degrees, are unable to generate anomalies that sufficiently represent the actual anomaly distribution, which is evidenced by major blue areas (representing actual anomaly regions) that are not covered by orange areas (generated anomaly regions). 
% generate anomalies that may fall within normal regions or under-represent anomaly regions, which are common causes for false positive rates and diminished detection sensitivity. 
For example, CARLA, the second-best method for overall ARP and TSAD performance, also suffers from these issues: on Yahoo (a), generated anomalies largely miss the anomaly regions, while on SMAP (b), generated anomalies form two major clusters leaving a gap in between, which is an actual anomaly region failing to be represented. These observations further substantiate that relying on handcrafted, predefined anomaly patterns for anomaly generation is prone to distribution misalignment when the assumed patterns deviate from actual ones, and disconnected/scattered anomaly regions when the assumed patterns differ significantly. Whereas GenIAS, through controlled and regularized perturbation on the learned distribution, generates anomalies without pattern speculation, achieving both better alignment and smoothness.

\subsection{Main TSAD Performance}\label{sec:tsad}
We report the TSAD performance of GenIAS and the competing methods in terms of the best F1 score and AUPR in Table \ref{table:main}. Due to space limitations, please refer to the Appendix~\ref{app:othermetrics} for results on the other three previously mentioned metrics discussed in Section \ref{sec:metrics}. Impressively, GenIAS achieves significantly better overall TSAD performance compared to the baseline methods across all metrics, demonstrating its superior effectiveness in handling both MTS and UTS. Specifically, among methods that can operate on both types of time series, GenIAS achieves 9.4\% and 29.5\% performance improvements compared to the second and third best performing methods in F1, respectively. Similarly, it outperforms the second best model by 10.0\% and third best model by 44.9\% in terms of AUPR. Upon examining the more detailed TSAD performance with respect to factors such as method category and dataset type, we noted the following observations.  
\vspace{2pt}

\noindent\textbf{Consistent effectiveness for both MTS and UTS}. On each type of dataset, GenIAS yields similar levels of performance improvement as observed in overall performance. For example, compared to the second-best model for both types, GenIAS improves F1 by 7.6\% and AUPR by 8.4\% on MTS, and improves F1 by 15.9\% and AUPR by 22.0\% for UTS. This highlights its ability to generate informative and diverse anomalies that can mimic anomalous patterns within each dimension and the dependencies between dimensions. We also provide the average rank of all models in the form of critical difference diagrams \cite{demvsar2006statistical} for F1 and AUPR in both MTS and UTS (see Appendix~\ref{app:critical}). GenIAS achieved the highest rank in all four comparisons and demonstrated statistically significant improvements over all baselines in MTS for both AUPR and F1.


\vspace{2pt}
\noindent\textbf{Enhanced Anomaly Generation}. It is worth noting that even when compared to US-I baselines, GenIAS consistently outperforms them due to its ability to generate more realistic and diverse anomalous patterns. In particular, since GenIAS-enhanced TSAD model and CARLA only differ in their anomaly generation while sharing the same architecture and training methods, their performance gap underscores the effectiveness of GenIAS's approach. This is attributed to its TSAD tailored variational representation learning and novel perturbation strategies, which enforce more distinctive normal representations and are not confined to predefined, handcrafted anomaly prototypes.

\vspace{2pt}
\noindent\textbf {Importance of anomaly generation in TSAD}. It is notable that US-I injection methods are generally more effective than methods in other categories that do not leverage labeled anomalies. Remarkably, some of these methods, such as CutAddPaste and CARLA, can even outperform the supervised baseline LASM-VAE. This suggests that, with well-designed anomaly injection techniques, the generated pseudo anomalies can be sufficiently realistic to represent actual anomalies, including those not well captured by the labeled training data in supervised settings. Moreover, leveraging anomaly generation can generate both larger numbers and more diverse types of anomalies, which can improve TSAD representation learning. Therefore, anomaly injection should be incorporated as an essential module in TSAD models.

\subsection{Ablation Study} \label{sec:ablation}
To better understand the contributions of different components and configurations in our proposed model, we conducted an ablation study. Our analysis focuses on the following aspects: (i) \textbf{The impact of the patch selection},
(ii) \textbf{The effectiveness of the prior variance}, and
(iii) \textbf{The effectiveness of the min-max margin}.

\subsubsection{Impact of the Patch Selection}
First, in Figure~\ref{fig:patch}, we present our study of F1 and AUPR for two patching methods—Length-driven and Deviation patching—evaluated across varying portions of patch lengths and thresholds. The x-axis represents the portion of patch length (ranging from 0.2 to 1, in intervals of 0.2), while thresholds \( \tau \) are explored at 0.05, 0.1, 0.2, 0.4, and 0.6. The best results for each method, identified through this parameter exploration, are then compared in Table~\ref{tab:patch}. The results in Table~\ref{tab:patch} demonstrate that Deviation patching consistently outperforms the other approaches, achieving the highest F1 and AUPR scores across all datasets. For example, it achieves a 0.044 increase in F1 and a 0.063 improvement in AUPR for SMAP compared to the baseline without patching (wo Patch). \textbf{Length-driven} patching, which introduces anomalies based on predefined temporal lengths, shows moderate improvements over the baseline in most scenarios but slightly underperforms on more complex datasets, such as Yahoo, where it reduces F1 and AUPR scores. The strong performance of Deviation patching, as defined in Section~\ref{sec:patching}, underscores its ability to better align generated anomalies with true anomaly distributions, making it a robust and effective approach for anomaly generation and detection.

\begin{table}[t]
\caption{Performance metrics for different patching methods.}
\vspace{-10pt}
\begin{adjustbox}{width=0.8\columnwidth}
\centering
\begin{tabular}{@{}lc|ccc@{}}
\toprule
\textbf{Metric} & \textbf{Dataset} & \textbf{wo Patch} & \textbf{Length-driven} & \textbf{Deviation-based} \\ \midrule
\multirow{3}{*}{\textbf{F1}} & MSL  & 0.535 & 0.547 & \textbf{0.579} \\
                         & SMAP & 0.533 & 0.571 & \textbf{0.620} \\
                         & Yahoo & 0.755 & 0.741 & \textbf{0.811} \\ \midrule
\multirow{3}{*}{\textbf{AUPR}}    & MSL  & 0.466 & 0.488 & \textbf{0.529} \\
                         & SMAP & 0.425 & 0.429 & \textbf{0.467} \\
                         & Yahoo & 0.776 & 0.766 & \textbf{0.794} \\ \bottomrule
\end{tabular}
\end{adjustbox}
\label{tab:patch}
\end{table}

\begin{figure}[t]
    \centering
    \vspace{-10pt}
    \includegraphics[width=1\linewidth]{figs/combined_plots.pdf}
    \vspace{-20pt}
    \caption{F1 and AUPR for different patching methods.}
    \label{fig:patch}
\end{figure}

\subsubsection{Effectiveness of the Prior Variance}
Figure~\ref{fig:prior} analyzes the impact of prior variance on anomaly detection performance, showing consistent trends across datasets and indicating that the model is not highly sensitive to this parameter. For the best F1 scores, prior variances of 0.5 achieve optimal performance across all datasets (MSL: 0.579, SMAP: 0.62, Yahoo: 0.811), indicating a spot where the model balances bias and variance effectively. However, as the prior variance increases beyond 0.5, performance declines, suggesting that overly broad priors introduce excessive flexibility, reducing model specificity. Similarly, the AUPR results follow this trend, with 0.5 showing superior means across datasets and narrower standard deviation (std) bars, emphasizing the robustness of this setting. The increasing std with higher variances further supports that larger priors may lead to unstable and less reliable performance. Overall, a prior variance of 0.5 emerges as the most effective choice, yielding a balance between accuracy and stability for both F1 and AUPR.

\begin{figure}[t]
% \vspace{-5pt}
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/F1_prior.pdf}
        \label{fig:f1_prior}
        \vspace{-5pt}
        % \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.23\textwidth}
        \includegraphics[width=\textwidth]{figs/AUPR_prior.pdf}
        \label{fig:aupr_prior}
        \vspace{-5pt}
        % \caption{}
    \end{subfigure}
    \vspace{-20pt}
    \caption{F1 and AUPR for different prior variance ($\sigma_{\text{prior}}^2$)}
    \label{fig:prior}
\end{figure}

\subsubsection{Effectiveness of the Min-Max Margin}
\begin{figure}[t]
    \centering
    \vspace{-15pt}
    \includegraphics[width=0.9\linewidth]{figs/min-max.pdf}
    \vspace{-10pt}
    \caption{Heatmaps of F1 and AUPR scores across datasets illustrating the effectiveness of min-max margins.}
    \label{fig:min-max}
    \vspace{-10pt}
\end{figure}
The heatmaps in Figure~\ref{fig:min-max} illustrate the impact of min-max margins (\(\delta_{\text{min}}, \delta_{\text{max}}\)) on performance metrics, such as F1 and AUPR, across MSL, SMAP, and Yahoo datasets. Smaller max margins (\(\delta_{\text{max}} = 0.2\)) and moderate min margins (\(\delta_{\text{min}} = 0.1\)) consistently yield the best results, with peak F1 scores of 0.579, 0.62, and 0.811 for MSL, SMAP, and Yahoo, respectively. Similarly, the AUPR values align with this trend. 
These results validate the design rationale in Section~\ref{sec:pert_loss}, where \(\delta_{\text{min}}\) enforces distinctness, and \(\delta_{\text{max}}\) maintains realism. Larger margins (e.g., \(\delta_{\text{max}} = 1\)) reduce performance, as seen in the lower F1 scores of MSL and Yahoo. These findings emphasize the importance of balancing min-max margins.

\section{Conclusion and Future Work}
In this work, we introduced GenIAS, a generative anomaly injection framework designed to enhance time series anomaly detection by creating realistic and diverse anomalies. By perturbing latent representations of a generative model trained on normal data, GenIAS produces anomalies that are similar to real-world anomalies. When integrated into a TSAD model, it further enhances anomaly detection performance by adding high-quality synthetic anomalies to the training set.
For future work, we aim to refine adaptive perturbation techniques and an adaptive patching approach to better align with real-world domain-specific anomalies. Additionally, we plan to incorporate correlation between dimensions to guarantee the generation of intermetric anomalies, making the synthetic anomalies more representative of real-world multivariate dependencies. These enhancements will improve the realism of generated anomalies by GenIAS and its effectiveness in time series anomaly detection.

% %%
% %% The acknowledgements section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert for the bagels and for explaining CMYK and colour spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used and
%% the bibliography file.
% \newpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{manuscript}


%%
%% If your work has an appendix, this is the place to put it.
\newpage
\appendix

\section{Datasets Descriptions} \label{app:dataset}
Specifically, five popular TSAD datasets are included to evaluate multivariate TSAD. Among these, \textbf{NASA Datasets} — The \textbf{Mars Science Laboratory (MSL)} and \textbf{Soil Moisture Active Passive (SMAP)} datasets\footnote{\url{https://www.kaggle.com/datasets/patrickfleith/nasa-anomaly-detection-dataset-smap-msl}}~\citep{hundman2018detecting} are collected from NASA spacecraft. These datasets include anomaly information derived from incident reports for spacecraft monitoring systems. \textbf{Server Machine Dataset (SMD)}\footnote{\url{https://github.com/NetManAIOps/OmniAnomaly/tree/master/ServerMachineDataset}}~\citep{su2019robust} comprises data from 28 servers over 10 days. Normal data is observed during the first 5 days, while anomalies are injected into the remaining 5 days. \textbf{Secure Water Treatment (SWaT)}\footnote{\url{https://itrust.sutd.edu.sg/testbeds/secure-water-treatment-swat/}}~\citep{mathur2016swat} contains data from a water treatment platform monitored by 51 sensors over 11 days. A total of 41 anomalies were deliberately generated during the last 4 days using various attack scenarios. \textbf{SWAN}\footnote{\url{https://dataverse.harvard.edu/api/access/datafile/:persistentId?persistentId=doi:10.7910/DVN/EBCFKM/K9AOSI}}~\citep{DVN/EBCFKM_2020} is a publicly accessible, MTS benchmark derived from solar photospheric vector magnetograms in the Spaceweather HMI Active Region Patch series. \textbf{GECCO}\footnote{\url{http://www.spotseven.de/gecco/gecco-challenge/gecco-challenge-2018/}}~\citep{moritz2018gecco} is a dataset on drinking water quality adapted for applications of the `Internet of Things'. It was introduced during the 2018 Genetic and Evolutionary Computation Conference.

In addition to the above MTS datasets, we also evaluated UTS datasets: \textbf{UCR}\footnote{\url{https://www.cs.ucr.edu/~eamonn/time_series_data_2018/}}~\citep{wu2021current} was provided for the Multi-dataset Time Series Anomaly Detection Competition at KDD2021. This dataset includes 250 UTS subdatasets, each containing one anomaly within its subsequences.
\textbf{Yahoo}\footnote{\url{https://webscope.sandbox.yahoo.com/catalog.php?datatype=s&did=70}}~\citep{yahoods} consists of 367 hourly time series with labels. Our focus is on the A1 benchmark, which includes 67 UTS representing “real” production traffic data from Yahoo properties.
\textbf{Key Performance Indicators (KPI)}\footnote{\url{https://github.com/NetManAIOps/KPI-Anomaly-Detection}} are sourced from real-world Internet company scenarios. It contains service and machine KPIs, such as response time, page views, CPU usage, and memory usage.

For all datasets, we adopt their original train/test splits, with the training data being unlabeled, except for the Yahoo and GECCO datasets, where we adopt a 50/50 split for training and testing.

\section{List of Symbols and Notations}

You can find the list of symbols and notations, and their definitions in Table \ref{tab:symbols_notations}.

\begin{table}[htp]
    \centering
    \small
    \vspace{-10pt}
    \caption{List of Symbols and Notations}
     \vspace{-10pt}
    \begin{tabular}{ll}
        \toprule
        \textbf{Symbol} & \textbf{Definition} \\
        \midrule
        \( \mathcal{D} \) & Time series dataset \\
        \( T \) & Number of time steps in a time series window \\
        \( D \) & Number of time series dimensions \\
        \( \mathbf{X} \) & Time series window of shape \( T \times D \) \\
        \( \widetilde{\mathbf{X}} \) & Generated anomalous time series sample \\
        \( \hat{\mathbf{X}} \) & Reconstructed normal sample from \( \mathbf{z} \) \\  
        \( \Gamma \) & Generative mapping from normal to anomalous samples \\
        \( \phi \) & Parameters of the encoder \\
        \( \theta \) & Parameters of the decoder \\
        \( \mathbf{z} \) & Latent representation of a normal sample \\
        \( \widetilde{\mathbf{z}} \) & Perturbed latent representation of an anomaly \\
        \( L \) & Latent space dimensionality\\
        \( p(\mathbf{X}) \) & Normal data distribution \\
        \( p(\widetilde{\mathbf{X}}) \) & Anomalous data distribution \\  
        \( q(\mathbf{z} | \mathbf{X}) \) & Latent distribution of normal data \\
        \( p(\widetilde{\mathbf{z}}) \) & Shifted latent distribution for anomalies \\
        \( \mu \) & Mean of the latent distribution \\
        \( \sigma \) & Standard deviation of the latent distribution \\
        \( \sigma_{\text{prior}} \) & Prior standard deviation for KL divergence loss \\
        \( \psi \) & Learned perturbation scale \\
        \( \delta_{\text{min}} \) & Minimum margin for anomaly distinctness \\
        \( \delta_{\text{max}} \) & Maximum allowable deviation for realism \\ 
        \( \tau \) & Threshold for deviation-based patching \\
        \bottomrule
    \end{tabular}
    \label{tab:symbols_notations}
\end{table}

\section{Algorithms Overview} \label{app:code}
The pseudo-code for GenIAS is provided in Algorithm~\ref{alg:genias}, while the deviation-based patching approach is detailed in Algorithm~\ref{alg:patching}.


\begin{algorithm}[h]
\caption{End-to-End Training of GenIAS}
\label{alg:genias}
\KwIn{TS dataset \( \mathcal{D}\), prior standard deviation \(\sigma_{\text{prior}}\), loss coefficients \(\alpha\), \(\beta\), \(\gamma\), \(\zeta\).}
\KwOut{Trained encoder parameters \(\phi\), decoder parameters \(\theta\), perturbation scale \(\psi\).}

\nl \textbf{Initialize} encoder \(q_\phi\), decoder \(g_\theta\), latent space parameters \(\mu\), \(\sigma\), and perturbation scale \(\psi\)\;
\nl \While{training not converged}{
\nl     Sample a batch \(\{\mathbf{\text{X}}_i\}_{i=1}^{B} \subset \mathcal{D}\) of size \(B\)\;
\nl     Encode: Obtain latent parameters \(\mu_i\), \(\sigma_i\) by \(q_{\phi}(\mathbf{\text{X}}_i)\)\;
\nl     Sample latent vectors \(\mathbf{\text{z}}_i \sim \mathcal{N}({\mu}_i, \operatorname{diag}({\sigma}_i^2))\)\;
\nl     Generate perturbed latent vectors \(\widetilde{\mathbf{z}}_i\) \hfill $\triangleright$ Eq. (\ref{eq:z_perturbed})\\
\nl     Decode reconstructed samples: \(\hat{\mathbf{X}}_i = g_\theta(\mathbf{z}_i)\)\;
\nl     Decode anomalous samples: \(\hat{\mathbf{X}}_i\ = g_\theta(\widetilde{\mathbf{z}}_i)\)\;
\nl     Compute the reconstruction loss $\mathcal{L}_{\text{recon}}$; \hfill $\triangleright$ Eq. (\ref{eq:reconstruction_loss})\\
\nl     Compute the perturbation loss $\mathcal{L}_{\text{perturb}}$; \hfill $\triangleright$ Eq. (\ref{eq:perturbation_loss})\\
\nl     Compute the zero-perturb loss $\mathcal{L}_{\text{zero-perturb}}$; \hfill $\triangleright$ Eq. (\ref{eq:zero_perturbation_loss})\\
\nl     Compute the enhanced KL loss $\mathcal{L}_{\text{en-KL}}$; \hfill $\triangleright$ Eq. (\ref{eq:kl_divergence_loss})\\
\nl     $\mathcal{L}_{\text{total}} = \alpha \cdot \mathcal{L}_{\text{recon}} + \beta \cdot \mathcal{L}_{\text{perturb}} + \gamma \cdot \mathcal{L}_{\text{zero-perturb}} + \zeta \cdot \mathcal{L}_{\text{en-KL}}$;\\
\nl     Update the model parameters \(\phi\), \(\theta\) and \(\psi\)\  by minimizing \(\mathcal{L}_{\text{total}}\)\;
}
\nl \Return \(\phi\), \(\theta\), \(\psi\)\;
\end{algorithm}


\begin{algorithm}[h]
\caption{Patching with GenIAS}
\label{alg:patching}
\KwIn{Pretrained GenIAS model's encoder \(q_\phi\), decoder \(g_\theta\) and perturbation scales \(\psi\), time series window \( \mathbf{X} \), threshold scaling factor \( \tau \).}
\KwOut{Patched anomalous time series windows \( \tilde{\mathbf{X}}_{\text{patched}} \).}

\textbf{Phase 1: Anomaly Generation}\\
\nl Load encoder \(q_\phi\), decoder \(g_\theta\), and perturbation scales \(\psi\).

\nl     Encode: Obtain latent parameters \(\mu\), \(\sigma\) by \(q_{\phi}(\mathbf{\text{X}})\)\;

\nl     Generate perturbed latent vectors \(\widetilde{\mathbf{z}}\) \hfill $\triangleright$ Eq. (\ref{eq:z_perturbed})\\

\nl     Decode \( \widetilde{\mathbf{z}} \) to generate preliminary anomalies: \( \tilde{\mathbf{X}} = g_\theta(\tilde{\mathbf{z}}) \)\;
% }

\textbf{Phase 2: Deviation-Based Patching}\\
% \nl \textbf{For each dimension} \( d \) in \( \mathbf{X} \):
\nl \ForEach{dimension \( d \) \text{in} \( \mathbf{X} \)}{
\nl     \( \text{deviation}_d = \left| \mathbf{X}_d - \tilde{\mathbf{X}}_d \right|^2 \)\;
\nl     \( \text{amplitude}_d = \max(\mathbf{X}_d) - \min(\mathbf{X}_d) \)\;
% \nl     Compare deviations to threshold: \( \tau \cdot \text{amplitude}_d \)\;
\nl     Generate patched output for dimension \( d \):
    \[
    \tilde{\mathbf{X}}_{\text{patched}, d} = 
    \begin{cases} 
    \tilde{\mathbf{X}}_d & \text{if deviation}_d > \tau \cdot \text{amplitude}_d, \\
    \mathbf{X}_d & \text{otherwise}.
    \end{cases}
    \]
}

\nl \Return \( \tilde{\mathbf{X}}_{\text{patched}} \) as the patched anomalous TS windows.
\end{algorithm}


\section{Proofs of Theorem} \label{app:proofs}
\subsection{Notation and Setup}
Assume that the latent encoder produces a Gaussian posterior
\begin{equation}
q_{\phi}(z \mid {\mathbf{X}}) = \mathcal{N} \left( \mu, \operatorname{diag}(\sigma^2) \right),
\end{equation}
and that the prior is chosen as
\begin{equation}
p(z) = \mathcal{N} \left( 0, \sigma^2_{\text{prior}} I \right),
\end{equation}
where $\sigma^2_{\text{prior}}$ is a hyperparameter and $\mu$ is a vector $\mu=(\mu_1,...,\mu_L)$. In the following, we consider the effect of using a prior $\sigma^2_{\text{prior}} < 1$ in our Enhanced KL Loss (Equation~\ref{eq:kl_divergence_loss}).

\subsection{Prerequisites}
\begin{lemma}[Compact Latent Space in Enhanced KL Loss] \label{lemma:compact}
Consider the KL divergence between the estimated posterior and the prior distributions for a latent dimension $j$. If the enhanced KL loss -Equation~\ref{eq:kl_divergence_loss}- is minimized (or its gradient dominates locally) with a prior variance $\sigma^2_{\text{prior}} < 1$, then for each latent variable $z_j$ the optimal posterior variance satisfies $\sigma^2_j = \sigma^2_{\text{prior}}$. Hence, the variance of latent representations for normal samples are forced to be less than 1, i.e., more ``compact''.
\end{lemma}

\begin{proof}
For a one-dimensional Gaussian posterior $\mathcal{N}(\mu, \sigma^2)$ and a prior $\mathcal{N}(0, \sigma^2_{\text{prior}})$, the KL divergence is
\begin{equation}
D_{\text{KL}}(q(z \mid \mathbf{X}) \parallel p(z)) = \frac{1}{2} \left( \frac{\sigma^2}{\sigma^2_{\text{prior}}} + \frac{\mu^2}{\sigma^2_{\text{prior}}} - 1 - \log \frac{\sigma^2}{\sigma^2_{\text{prior}}} \right).
\end{equation}

If we minimize the KL term with respect to $\sigma^2$ (ignoring the contribution from the reconstruction term and assuming $\mu$ is fixed), we define a function $f$ that takes the part that depends on $\sigma^2$:
\begin{equation}
f(\sigma^2) = \frac{1}{2} \left( \frac{\sigma^2}{\sigma^2_{\text{prior}}} - \log \frac{\sigma^2}{\sigma^2_{\text{prior}}} \right).
\end{equation}

Taking the derivative,
\begin{equation}
\frac{d f}{d \sigma^2} = \frac{1}{2} \left( \frac{1}{\sigma^2_{\text{prior}}} - \frac{1}{\sigma^2} \right).
\end{equation}

Setting the derivative to zero yields
\begin{equation}
\frac{1}{\sigma^2_{\text{prior}}} - \frac{1}{\sigma^2} = 0 \quad \Longrightarrow \quad \sigma^2 = \sigma^2_{\text{prior}}.
\end{equation}

Thus, when $\sigma^2_{\text{prior}} < 1$, the optimal posterior variance (in the sense of minimizing the KL divergence) is less than 1, leading to a more compact latent representation.
\end{proof}

\begin{lemma}[Reduced Reconstruction Error for Normal Samples] \label{lemma-recon-norm}
Assume that the decoder $g_\theta(z)$ is trained on normal data. If the latent representations of normal samples are forced to be compact (i.e. the variance $\sigma^2(\mathbf{X})$ is reduced), then a first-order Taylor expansion argument shows that the additional error due to sampling variability is reduced, thereby lowering the expected reconstruction error.
\end{lemma}

\begin{proof}
For an input $\mathbf{X}$ with latent variable $z \sim q_\phi(z \mid \mathbf{X})$ and decoder $g_\theta(z)$, the reconstruction error is given by
\begin{equation}
\text{MSE}(\mathbf{X}) = \mathbb{E}_{z \sim q_\phi(z \mid \mathbf{X})} \left[ \| \mathbf{X} - g_\theta(z) \|^2 \right].
\label{eq:reconappendix}
\end{equation}

A first-order Taylor expansion of $g_\theta(z)$ around the mean $\mu(\mathbf{X})$ gives
\begin{equation}
g_\theta(z) \approx g_\theta(\mu(\mathbf{X})) + J_{g_\theta} (\mu(\mathbf{X})) (z - \mu(\mathbf{X})),
\label{eq:jacob}
\end{equation}
where $J_{g_\theta}$ is the Jacobian of $g_\theta$ at $\mu(\mathbf{X})$. We substitute Equation~\ref{eq:jacob} in Equation~\ref{eq:reconappendix}. While $\mu(\mathbf{X})$ stays the same in both Enhanced KL and KL cases, $z-\mu(\mathbf{X})$ varies in these two cases. Hence we focus on the variable term in this substitution, i.e., $\mathbb{E} \left[ \| J_{g_\theta} (z - \mu(\mathbf{X})) \|^2 \right]$. 
In a multi-dimensional latent space, if the posterior covariance is $\Sigma(\mathbf{X}) = \operatorname{diag}(\sigma_1^2(\mathbf{X}), \dots, \sigma_d^2(\mathbf{X}))$, then the second-order contribution to the reconstruction error is approximately
\begin{equation}
\mathbb{E} \left[ \| J_{g_\theta} (z - \mu(\mathbf{X})) \|^2 \right] = \operatorname{trace} \left( J_{g_\theta}^T J_{g_\theta} \Sigma(\mathbf{X}) \right).
\label{eq:msejacob}
\end{equation}

When the enhanced KL loss forces each $\sigma_j^2(\mathbf{X})$ to be small (namely $\sigma^2_{\text{prior}} < 1$), the contribution of the variability term
\begin{equation}
\operatorname{trace} \left( J_{g_\theta}^T J_{g_\theta} \Sigma(\mathbf{X}) \right)
\end{equation}
is reduced. Hence, the overall reconstruction error decreases. Figure \ref{fig:mse_hist} shows the impact of $\sigma^2_{\text{prior}}$ on the reconstruction MSE distribution for entity C-1 in MSL dataset.
\end{proof}

% \textbf{Remark.} Although the reconstruction error also depends on the decoder’s nonlinearity and the term $\| \mathbf{X} - g_\theta (\mu(\mathbf{X})) \|^2$, the second-order term (due to latent variability) is reduced by enforcing a compact latent space, which is beneficial when the decoder is sensitive to deviations from the mean.

\subsection{Proof of Theorem 1} \label{app:proof_t_1}
% \begin{theorem}[Enhanced Separation of Reconstruction Errors for Anomalies]
\textbf{Statement:} \textit{Let a VAE be trained on normal data with a latent prior \( \mathcal{N}(0, \sigma^2_{\text{prior}}) \), where \( \sigma_{\text{prior}} < 1 \). The encoder posterior for normal samples follows \( \mathcal{N}(\mu, \sigma^2_{\text{normal}}) \), with compactness enforced by KL regularization. If the encoder \( \phi \) applies a perturbation scale \( \psi \), inflating variance as  
\(
\sigma^2_{\text{anom}} = \psi \sigma^2_{\text{normal}}, \quad \text{with } \psi > 1,
\)
then the KL divergence between normal and anomalous latent distributions strictly increases compared to the case where \( \sigma_{\text{prior}} \geq 1 \).}  

%Consequently, the separation of reconstruction errors between normal and generated anomalies in raw time series space, i.e.,}
%\begin{equation}
%\Delta \text{MSE} = \text{MSE}_{\text{anom}} - \text{MSE}_{\text{normal}}
%\end{equation}
%\textit{becomes larger.}
% \end{theorem}

\begin{proof}
For generated anomalous samples, assume that the latent posterior is
\begin{equation}
q_{\text{normal}}(z) = \mathcal{N}(\mu_{\text{normal}}, \operatorname{diag}(\sigma^2_{\text{normal}})),
\end{equation}
with $\sigma^2_{\text{normal}} \approx \sigma^2_{\text{prior}}$ (by Lemma~\ref{lemma:compact}). For anomalous inputs, suppose the encoder yields an inflated variance so that
\begin{equation}
q_{\text{anom}}(z) = \mathcal{N}(\mu_{\text{anom}}, \operatorname{diag}(\psi \sigma^2_{\text{normal}})),
\end{equation}
with $\psi > 1$. We assume that the means are similar, i.e. $\mu_{\text{anom}} \approx \mu_{\text{normal}}$, so that the primary difference is in the variances.

We aim to show that the KL divergence between the latent distributions of normal and anomalous samples \textbf{strictly increases} when the VAE prior variance is \textbf{compact} (\(\sigma_{\text{prior}} < 1\)) compared to when no prior compactness is enforced (\(\sigma_{\text{prior}} \geq 1\)).

From the given equation the KL divergence between the anomalous and normal posterior distributions in one dimension is:

\begin{equation}
\begin{split}
D_{\text{KL}} \left( \mathcal{N}(\mu, \psi \sigma^2_{\text{normal}}) \Big\| \mathcal{N}(\mu, \sigma^2_{\text{prior}}) \right) = \\
\frac{1}{2} \left( \frac{\psi \sigma^2_{\text{normal}}}{\sigma^2_{\text{prior}}} - 1 - \log \frac{\psi \sigma^2_{\text{normal}}}{\sigma^2_{\text{prior}}} \right).
\end{split}
\end{equation}

We compare two scenarios:
\begin{itemize}
    \item \textbf{Scenario A (Compact Latent Space):} \( \sigma_{\text{prior}} < 1 \), let \( \sigma_{\text{prior}} = \alpha \), where \( 0 < \alpha < 1 \).
    \item \textbf{Scenario B (Non-Compact Latent Space):} \( \sigma_{\text{prior}} \geq 1 \), specifically, we consider \( \sigma_{\text{prior}} = 1 \).
\end{itemize}

For \textbf{Scenario A} (\(\sigma_{\text{prior}} = \alpha\)):

\begin{equation}
D_{\text{KL}, A} = \frac{1}{2} \left( \frac{\psi \sigma^2_{\text{normal}}}{\alpha^2} - 1 - \log \frac{\psi \sigma^2_{\text{normal}}}{\alpha^2} \right).
\end{equation}

For \textbf{Scenario B} (\(\sigma_{\text{prior}} = 1\)):

\begin{equation}
D_{\text{KL}, B} = \frac{1}{2} \left( \frac{\psi \sigma^2_{\text{normal}}}{1} - 1 - \log \frac{\psi \sigma^2_{\text{normal}}}{1} \right).
\end{equation}

Since \( 0 < \alpha < 1 \), we observe that the fraction \( \frac{\psi \sigma^2_{\text{normal}}}{\alpha^2} \) in \(D_{\text{KL}, A}\) is \textbf{strictly larger} than \( \frac{\psi \sigma^2_{\text{normal}}}{1} \) in \(D_{\text{KL}, B}\).

The function \( f(x) = x - 1 - \log x \) is \textbf{monotonically increasing} for \( x > 1 \), meaning:

\begin{equation}
f\left(\frac{\psi \sigma^2_{\text{normal}}}{\alpha^2}\right) > f\left(\frac{\psi \sigma^2_{\text{normal}}}{1}\right).
\end{equation}

Thus, we obtain:

\begin{equation}
D_{\text{KL}, A} > D_{\text{KL}, B}.
\end{equation}

Since \(D_{\text{KL}, A} > D_{\text{KL}, B}\), we conclude that the KL divergence between normal and anomalous latent distributions is strictly larger when the VAE is trained with a compact latent space (\(\sigma_{\text{prior}} < 1\)). 

This means that a compact latent space \textbf{amplifies the separation} between normal and anomalous samples under perturbation.


%Because $\sigma^2_{\text{normal}}$ is small (with $\sigma^2_{\text{normal}} \approx \sigma^2_{\text{prior}} < 1$), the term $\frac{\psi \sigma^2_{\text{normal}}}{\sigma^2_{\text{prior}}}$ is approximately $\psi$. Thus, the divergence is roughly proportional to $\psi$; that is, the further the anomalous variance is from the normal variance, the larger the divergence.

%From Lemma \ref{lemma-recon-norm}, the MSE depends on latent variance (Equation~\ref{eq:msejacob}) and a compact normal variance $\sigma_{\text{normal}}^2$ reduces $\text{MSE}_{\text{normal}}$. For anomalies with inflated variance $\psi \sigma_{\text{normal}}^2$, the second-order term in the Taylor expansion of $g_\theta$ becomes larger, hence $\Delta \text{MSE} > 0 \quad \text{and grows with } \psi$. Therefore, the gap $\Delta \text{MSE}$ increases, amplifying the distinction between normal and anomalous inputs.

%Regarding the impact of choosing $\sigma_{\text{prior}}^2 < 1$, if $\sigma_{\text{prior}}^2$ had been $\geq 1$, normal samples would not be forced into as narrow a posterior, shrinking the difference between $\sigma_{\text{normal}}^2$ and $\sigma_{\text{anom}}^2$. That would reduce the gap in reconstruction errors. Hence, enforcing a strictly smaller prior variance can magnify the separation by lowering the MSE for normal data while anomalies deviate more drastically in latent space.

% Let's assume that we assign variable $A$ to Equation~\ref{eq:KLlast}. In the original KL divergence loss term in VAE, the $\sigma^2_{\text{prior}}$ in this equation is replaced with $\sigma^2$. Let's assume that we assign variable $B$ to the equation resulting from this replacement. In order to prove that the divergence between the latent distributions of normal and anomalous samples increase, we should prove $A > B$. Our null hypothesis is $A \leq B$ and our alternative hypothesis is H1 is $A>B$ However, since $\sigma^2_{\text{prior}} \leq \sigma^2$, we reject the null hypothesis which means $A > B$. 


\end{proof}

\section{Details of Evaluation Metrics}\label{app:eval_metrics}
To support a consistent and meaningful evaluation of generated anomalies, we utilize a Deep SVDD \citep{pmlr-v80-ruff18a} representation with a fixed 128-dimensional embedding for all models. This representation provides a structured feature space. Additionally, we evaluate models using test datasets that contain labeled anomalies, allowing for a more reliable assessment of anomaly generation quality. Let \(\Omega\) denote the DeepSVDD model. The representation of real anomalous samples in the test dataset is given by \(V_{\text{real}} = \Omega(\text{X}_{\text{real anomalies}})\), while the representation of generated anomalous samples, produced by the anomaly generator model from the train dataset, is given by \(V_{\text{gen}} = \Omega(\tilde{{\text{X}}})\).

\subsection{Anomalous Representation Proximity (ARP)} \label{app:ARP}
ARP measures how well the generated anomalies cover the distribution of real anomalies in the representation space. For each real anomaly, we compute the distance to its closest generated anomaly. The inverse of the average of these minimum distances reflects how well the generated anomalies span the real anomaly space. Formally, if \(V_{\text{real}}\) and \(V_{\text{gen}}\) are the sets of real and generated anomaly representations, the ARP is defined as:
\begin{equation}
\text{ARP} = \frac{1}{1 + \frac{1}{|V_{\text{real}}|} \sum_{\mathbf{v}^r \in V_{\text{real}}} \min_{\mathbf{v}^g \in V_{\text{gen}}} d(\mathbf{v}^r, \mathbf{v}^g)}.
\end{equation}
where \(d(\cdot, \cdot)\) is the Euclidean distance metric. A higher ARP indicates that the generated anomalies are more similar to real anomalies in the representation space, meaning better realism.

\subsection{Entropy-Based Diversity Index (EDI)} \label{app:EDI}
EDI quantifies how well a model generates a diverse set of anomalies by evaluating their distribution across the representation space. First, we collect the representations of all generated anomalies from all models into a global set:
\begin{equation}
V_{\text{all}} = \bigcup_{m=1}^{M} V_{\text{gen}}^{(m)},
\end{equation}
where \( V_{\text{gen}}^{(m)} = \{\mathbf{v}_1^{(m)}, \mathbf{v}_2^{(m)}, \dots, \mathbf{v}_{N_m}^{(m)}\} \) represents the set of generated anomalies for model \( m \), and \( M \) is the number of models being evaluated. Next, we partition the overall representation space into \( K \) non-overlapping regions \( \{R_1, R_2, \dots, R_K\} \) based on \( V_{\text{all}} \), ensuring that each anomaly belongs to exactly one region. 

For a specific model \( m \), the proportion of its generated anomalies in region \( R_i \) is defined as:
\begin{equation}
p_i^{(m)} = \frac{|\{\mathbf{v}_j^{(m)} \in V_{\text{gen}}^{(m)} \mid \mathbf{v}_j^{(m)} \in R_i\}|}{N_m},
\end{equation}
where \( N_m = |V_{\text{gen}}^{(m)}| \) is the total number of generated anomalies for model \( m \), and \( |\cdot| \) denotes the cardinality of a set.

The EDI for model \( m \) is then computed using \textit{Shannon Entropy}~\cite{shannon1948mathematical} as:
\begin{equation}
\text{EDI}^{(m)} = - \sum_{i=1}^{K} p_i^{(m)} \log p_i^{(m)}.
\end{equation}

A higher EDI indicates that a model generates anomalies that are well-distributed across the space, reflecting greater diversity rather than being concentrated in a few regions.


\subsection{TSAD Evaluation Metrics} \label{app:tsadmetrics}
We use Best F1 score to measures the performance of TSAD models at the optimal threshold. AUPR and AUROC are holistic measures that assess model performance across various thresholds. All these metrics account for class imbalance, with AUPR placing greater emphasis on anomaly class performance, making it more reflective of anomaly detection capabilities.

In addition, the two recent metrics are chosen to evaluate TSAD performance at finer granularities: Affiliation F1 (derived from Affiliation-Precision and Affiliation-Recall) and PATE (Proximity-Aware Time series anomaly Evaluation that incorporates proximity-based weighting and buffer zones). Both metrics assess early or delayed detections. 

\section{Implementation Details} \label{sec:implementation}
All evaluations were performed on a system with an A40 GPU, 13 CPU cores, and 250 GB of RAM. We use the same hyperparameters for GenIAS across UTS and MTS datasets, as detailed in Table~\ref{tab:hyperparam}.
\begin{table}[h]
    \centering
    \small
    \caption{hyperparameter configurations used in GenIAS.}
    \vspace{-5pt}
    \begin{tabular}{ll}
        \toprule
        \textbf{Parameter} & \textbf{Value} \\
        \midrule
        \( T \) & 200 \\
        \( L \) & UTS = 50, MTS = 100 \\
        \( \sigma_{\text{prior}} \) & 0.5 \\
        \textit{lr} & initial=$10^{-4}$, with scheduled reduction\\
        \textit{epochs} & max=1000 \\
        \textit{kernel size} & 3\\
        \textit{dropout} & 0.1\\
        \textit{batch size} & 100 \\
        $\alpha$ & 1.0 \\
        $\beta$ & 0.1 \\
        $\gamma$ & UTS = 0.0, MTS = 0.01 \\
        $\zeta$ & 0.1 \\
        \bottomrule
    \end{tabular}
    \label{tab:hyperparam}
\end{table}

\section{Additional Results}
\subsection{Visualizing Enhanced KL Loss Effects}
Separation of reconstruction error in the input space is illustrated in Figure~\ref{fig:mse_hist}.
\begin{figure}[t]
    \centering
    \vspace{-5pt}
    \includegraphics[width=1.0\linewidth]{figs/mse_hist_c1.pdf}
    \vspace{-20pt}
    \caption{Separation of reconstruction error (anomaly score) in the input space with a prior of 0.5 (left) and without a prior (right) on entity C-1 from the MSL dataset. The y-axis is logarithmic.}
    \vspace{-5pt}
    \label{fig:mse_hist}
\end{figure}

\subsection{TSAD additional results}

\subsubsection{Critical Difference diagrams} \label{app:critical}
Statistical significance in critical difference digrams was determined using a Friedman test followed by a post-hoc Wilcoxon signed-rank test with Holm correction \cite{demvsar2006statistical}. The critical difference diagrams in Figure~\ref{fig:cdc} show that GenIAS consistently ranks first across both multivariate (MTS) and univariate (UTS) time series settings for AUPR and F1 Score. In the MTS setting, GenIAS demonstrates a statistically significant improvement over other baselines, indicating its superior performance in capturing anomalies.

\begin{figure}[t]
    \centering
    % First row
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figs/cdc-f1-mts.pdf}
        \label{fig:cdc-f1-mts}
        \vspace{-20pt}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{figs/cdc-pr-mts.pdf}
        \vspace{-20pt}
        \caption{}
        \label{fig:cdc-pr-mts}
    \end{subfigure}
    % Second row
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figs/cdc-f1-uts.pdf}
        \label{fig:cdc-f1-uts}
        \vspace{-20pt}
        \caption{}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{figs/cdc-pr-uts.pdf}
        \label{fig:cdc-pr-uts}
        \vspace{-20pt}
        \caption{}
    \end{subfigure}
    % \vspace{-10pt}
    \caption{Critical difference diagrams for F1 and AUPR scores: (a, b) represent multivariate models, while (c, d) showcase univariate models.}
    \label{fig:cdc}
\end{figure}


\subsubsection{TSAD performance in the other metrics.} \label{app:othermetrics}
In Table~\ref{table:tsad_app}, we present the TSAD performance of GenIAS and the baseline methods in terms of three additional metrics: Affiliation F1 (Aff), PATE, and AUROC. In general, similar observations can be drawn as in Section~\ref{sec:tsad} - GenIAS achieves superior performance compared to the baseline methods. Specifically, for Aff and PATE, GenIAS achieves the best overall performance across both MTS and UTS benchmark datasets. In terms of AUROC, GenIAS achieves the second-best overall performance and best overall performance among neural network-based methods, while Isolation Forest, a well-known legacy shallow AD method, yields surprisingly good performance.

% Please add the following required packages to your document preamble:
% \usepackage{multirow}

\begin{table*}[t]
\caption{TSAD performance of GenIAS and baseline methods in terms of Aff, PATE and AUROC, with the best model in bold and the
second-best model underlined. U and M denote results unavailable for models limited to UTS or MTS, respectively.}
\label{table:tsad_app}
\begin{adjustbox}{width=0.87\textwidth}
\begin{tabular}{ccccccccccccccc}
\toprule
\multicolumn{1}{l}{}      & \multicolumn{1}{l}{}     & \multicolumn{1}{l}{} & \multicolumn{7}{c}{Multivariate}                                                                                     & \multicolumn{4}{c}{Univariate}                                    & \multicolumn{1}{l}{} \\ \cmidrule(lr){4-10} \cmidrule(lr){11-14} 
Metric                    & Cat.                     & Methods              & MSL            & SMAP           & SMD            & SWAT           & GECCO          & SWAN           & Avg-M          & Yahoo          & KPI            & UCR            & Avg-U          & Avg                  \\ \midrule
\multirow{18}{*}{Aff}     & \multirow{3}{*}{Classic} & OCSVM (1999)         & 0.742          & 0.761          & 0.733          & 0.197          & 0.425          & 0.711          & 0.595          & \textbf{0.923} & 0.761          & 0.710          & 0.798          & 0.663                \\
                          &                          & LOF (2000)           & 0.775          & 0.762          & 0.666          & 0.613          & 0.227          & 0.745          & 0.631          & 0.906          & 0.674          & 0.718          & 0.766          & 0.676                \\
                          &                          & IForest (2008)       & 0.719          & 0.728          & 0.699          & 0.535          & 0.259          & 0.824          & 0.627          & 0.907          & 0.760          & 0.686          & 0.784          & 0.680                \\ \cmidrule(lr){3-15}
                          & PU                       & LSTM-VAE (2018)      & 0.824          & 0.782          & 0.630          & 0.324          & 0.312          & 0.726          & 0.600          & M              & M              & M              & M              & M                    \\
                          & \multirow{9}{*}{US-WI}   & Donut (2018)         & U              & U              & U              & U              & U              & U              & U              & 0.777          & 0.682          & 0.685          & 0.715          & -                    \\
                          &                          & Omni (2019)          & 0.680          & 0.779          & 0.706          & 0.156          & 0.230          & 0.608          & 0.527          & M              & M              & M              & M              & M                    \\
                          &                          & THOC (2020)          & 0.778          & 0.781          & 0.677          & 0.679          & 0.321          & 0.713          & 0.658          & 0.796          & 0.676          & 0.642          & 0.705          & 0.674                \\
                          &                          & MTAD-GAT(2020)       & \underline{ 0.833}    & \underline{ 0.848}    & 0.731          & 0.355          & 0.308          & 0.745          & 0.637          & M              & M              & M              & M              & M                    \\
                          &                          & AnomalyTran (2021)   & 0.774          & 0.766          & 0.664          & 0.482          & 0.551          & 0.537          & 0.629          & M              & M              & M              & M              & M                    \\
                          &                          & TranAD (2022)        & 0.771          & 0.751          & 0.719          & 0.606          & 0.233          & 0.661          & 0.624          & 0.917          & 0.779          & 0.698          & 0.798          & 0.682                \\
                          &                          & TS2Vec (2022)        & 0.694          & 0.695          & 0.750          & \underline{ 0.700}    & 0.520          & 0.728          & 0.681          & 0.867          & 0.818          & 0.671          & 0.785          & 0.716                \\
                          &                          & TimesNet (2023)      & 0.769          & 0.710          & \textbf{0.795} & 0.693          & \underline{ 0.829}    & 0.710          & 0.751          & 0.917          & \underline{ 0.844}    & 0.712          & 0.824          & 0.775                \\
                          &                          & Dcdetector (2023)    & 0.706          & 0.680          & 0.528          & 0.693          & 0.671          & 0.710          & 0.665          & 0.737          & 0.666          & 0.671          & 0.691          & 0.674                \\ \cmidrule(lr){3-15}
                          & \multirow{4}{*}{US-I}    & NCAD (2022)          & 0.685          & 0.664          & 0.732          & 0.693          & \textbf{0.887} & 0.710          & 0.729          & 0.645          & 0.753          & 0.747          & 0.715          & 0.724                \\
                          &                          & CutAddPaste (2024)   & 0.762          & 0.753          & 0.693          & \textbf{0.764} & 0.659          & 0.875          & 0.751          & 0.832          & 0.818          & \textbf{0.820} & 0.823          & 0.775                \\
                          &                          & COUTA (2024)         & 0.785          & 0.762          & \underline{ 0.764}    & 0.645          & 0.592          & 0.731          & 0.713          & 0.906          & \textbf{0.851} & 0.730          & \underline{ 0.829}    & 0.752                \\
                          &                          & CARLA (2025)         & 0.816          & 0.808          & 0.733          & 0.592          & 0.756          & \underline{ 0.932}    & \underline{ 0.773}    & 0.874          & 0.701          & 0.805          & 0.793          & \underline{ 0.780}          \\ \cmidrule(lr){3-15}
                          & Ours                     & GenIAS               & \textbf{0.851} & \textbf{0.856} & 0.731          & 0.596          & 0.763          & \textbf{0.935} & \textbf{0.789} & \underline{ 0.921}    & 0.788          & \underline{ 0.814}    & \textbf{0.841} & \textbf{0.806}       \\ \cmidrule(lr){1-15}
\multirow{18}{*}{PATE}    & \multirow{3}{*}{Classic} & OCSVM (1999)         & 0.267          & 0.270          & 0.363          & 0.698          & 0.191          & 0.704          & 0.416          & \underline{ 0.780}    & 0.343          & 0.033          & 0.385          & 0.405                \\
                          &                          & LOF (2000)           & 0.242          & 0.251          & 0.319          & 0.187          & 0.042          & 0.706          & 0.291          & 0.690          & 0.159          & 0.056          & 0.302          & 0.295                \\
                          &                          & IForest (2008)       & 0.195          & 0.197          & 0.306          & \textbf{0.743} & 0.191          & 0.799          & 0.405          & 0.735          & 0.325          & 0.027          & 0.362          & 0.391                \\ \cmidrule(lr){3-15}
                          & PU                       & LSTM-VAE (2018)      & 0.318          & 0.278          & 0.436          & 0.693          & 0.061          & 0.753          & 0.423          & M              & M              & M              & M              & M                    \\ \cmidrule(lr){3-15}
                          & \multirow{9}{*}{US-WI}   & Donut (2018)         & U              & U              & U              & U              & U              & U              & U              & 0.358          & 0.120          & 0.018          & 0.165          & -                    \\
                          &                          & Omni (2019)          & 0.143          & 0.162          & 0.422          & 0.702          & 0.076          & 0.709          & 0.369          & M              & M              & M              & M              & M                    \\
                          &                          & THOC (2020)          & 0.307          & 0.218          & 0.147          & 0.508          & 0.104          & 0.563          & 0.308          & 0.488          & 0.317          & 0.016          & 0.274          & 0.296                \\
                          &                          & MTAD-GAT(2020)       & 0.391          & 0.371          & 0.445          & 0.115          & 0.247          & 0.707          & 0.379          & M              & M              & M              & M              & M                    \\
                          &                          & AnomalyTran (2021)   & 0.267          & 0.283          & 0.332          & 0.683          & 0.112          & 0.685          & 0.394          & M              & M              & M              & M              & M                    \\
                          &                          & TranAD (2022)        & 0.238          & 0.279          & 0.458          & 0.192          & 0.141          & 0.688          & 0.333          & 0.777          & 0.408          & 0.025          & 0.403          & 0.356                \\
                          &                          & TS2Vec (2022)        & 0.143          & 0.150          & 0.137          & 0.147          & 0.055          & 0.530          & 0.194          & 0.636          & 0.247          & 0.144          & 0.342          & 0.243                \\
                          &                          & TimesNet (2023)      & 0.282          & 0.207          & 0.452          & 0.083          & 0.240          & 0.636          & 0.317          & 0.760          & 0.355          & 0.042          & 0.386          & 0.340                \\
                          &                          & Dcdetector (2023)    & 0.176          & 0.135          & 0.097          & 0.124          & 0.025          & 0.455          & 0.169          & 0.184          & 0.052          & 0.025          & 0.087          & 0.141                \\
                          & \multirow{4}{*}{US-I}    & NCAD (2022)          & 0.157          & 0.161          & 0.129          & 0.107          & 0.272          & 0.556          & 0.230          & 0.145          & 0.173          & 0.130          & 0.149          & 0.203                \\
                          &                          & CutAddPaste (2024)   & 0.269          & 0.347          & 0.200          & 0.613          & 0.252          & 0.842          & 0.421          & 0.237          & \textbf{0.741} & 0.262          & 0.413          & 0.418                \\
                          &                          & COUTA (2024)         & 0.270          & 0.244          & 0.447          & 0.159          & \textbf{0.285} & 0.756          & 0.360          & 0.684          & 0.335          & 0.054          & 0.358          & 0.359                \\
                          &                          & CARLA (2025)         & \underline{ 0.493}    & \underline{ 0.413}    & \underline{ 0.488}    & 0.655          & 0.193          & \underline{ 0.911}    & \underline{ 0.526}    & 0.695          & 0.378          & \underline{ 0.301}    & \underline{ 0.458}    & \underline{ 0.503}          \\ \cmidrule(lr){3-15}
                          & Ours                     & GenIAS               & \textbf{0.571} & \textbf{0.516} & \textbf{0.553} & \underline{ 0.718}    & \underline{ 0.274}    & \textbf{0.941} & \textbf{0.596} & \textbf{0.838} & \underline{ 0.456}    & \textbf{0.331} & \textbf{0.542} & \textbf{0.578}       \\ \cmidrule(lr){1-15}
\multirow{18}{*}{AUC-ROC} & \multirow{3}{*}{Classic} & OCSVM (1999)         & 0.605          & 0.634          & 0.762          & 0.799          & 0.446          & 0.714          & 0.660          & \textbf{0.918} & 0.728          & 0.567          & 0.738          & 0.686                \\
                          &                          & LOF (2000)           & 0.610          & 0.584          & 0.708          & 0.660          & 0.370          & 0.789          & 0.620          & 0.890          & 0.604          & 0.575          & 0.690          & 0.643                \\
                          &                          & IForest (2008)       & 0.571          & 0.579          & 0.768          & \textbf{0.842} & \textbf{0.927} & \textbf{0.862} & \textbf{0.758} & \underline{ 0.915}    & 0.749          & 0.570          & 0.745          & \textbf{0.754}       \\ \cmidrule(lr){3-15}
                          & PU                       & LSTM-VAE (2018)      & 0.618          & 0.650          & 0.771          & 0.806          & 0.506          & 0.763          & 0.686          & M              & M              & M              & M              & M                    \\ \cmidrule(lr){3-15}
                          & \multirow{9}{*}{US-WI}   & Donut (2018)         & U              & U              & U              & U              & U              & U              & U              & 0.676          & 0.640          & 0.498          & 0.605          & -                    \\
                          &                          & Omni (2019)          & 0.387          & 0.570          & 0.794          & 0.811          & 0.430          & 0.696          & 0.615          & M              & M              & M              & M              & M                    \\
                          &                          & THOC (2020)          & 0.654          & 0.604          & 0.678          & 0.318          & 0.395          & 0.578          & 0.538          & 0.873          & \underline{ 0.799}    & 0.558          & 0.743          & 0.606                \\
                          &                          & MTAD-GAT(2020)       & 0.674          & 0.641          & \underline{ 0.812}    & \underline{ 0.822}    & 0.590          & 0.749          & \underline{ 0.715}    & M              & M              & M              & M              & M                    \\
                          &                          & AnomalyTran (2021)   & 0.641          & 0.653          & 0.764          & 0.809          & 0.698          & 0.678          & 0.707          & M              & M              & M              & M              & M                    \\
                          &                          & TranAD (2022)        & 0.632          & 0.645          & 0.808          & 0.677          & 0.519          & 0.677          & 0.660          & 0.889          & 0.763          & 0.515          & 0.722          & 0.681                \\
                          &                          & TS2Vec (2022)        & 0.503          & 0.517          & 0.496          & 0.548          & 0.501          & 0.573          & 0.523          & 0.698          & 0.567          & 0.501          & 0.589          & 0.545                \\
                          &                          & TimesNet (2023)      & 0.664          & 0.578          & \textbf{0.837} & 0.243          & \underline{ 0.924}    & 0.598          & 0.641          & 0.892          & 0.750          & 0.597          & 0.746          & 0.676                \\
                          &                          & Dcdetector (2023)    & 0.535          & 0.519          & 0.579          & 0.504          & 0.504          & 0.499          & 0.523          & 0.603          & 0.504          & 0.509          & 0.539          & 0.528                \\
                          & \multirow{4}{*}{US-I}    & NCAD (2022)          & 0.491          & 0.493          & 0.572          & 0.365          & 0.692          & 0.592          & 0.534          & 0.567          & 0.671          & 0.609          & 0.616          & 0.561                \\
                          &                          & CutAddPaste (2024)   & 0.673          & 0.631          & 0.631          & 0.777          & 0.789          & 0.464          & 0.661          & 0.684          & \textbf{0.810} & 0.617          & 0.704          & 0.675                \\
                          &                          & COUTA (2024)         & 0.661          & 0.606          & 0.811          & 0.643          & 0.660          & \underline{ 0.802}    & 0.697          & 0.911          & 0.787          & 0.559          & \textbf{0.752} & 0.716                \\
                          &                          & CARLA (2025)         & \underline{ 0.694}    & \underline{ 0.677}    & 0.657          & 0.783          & 0.696          & 0.510          & 0.670          & 0.543          & 0.604          & \underline{ 0.735}    & 0.627          & 0.655                \\  \cmidrule(lr){3-15}
                          & Ours                     & GenIAS               & \textbf{0.745} & \textbf{0.718} & 0.633          & 0.786          & 0.811          & 0.536          & 0.705          & 0.778          & 0.691          & \textbf{0.778} & \underline{ 0.749}    & \underline{ 0.720}     \\ \bottomrule     
\end{tabular}
\end{adjustbox}
\end{table*}

\begin{figure*}[t]
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{figs/kpi_tsne.pdf}
        \label{fig:kpi_tsne}
        \vspace{-15pt}
        \caption{KPI - Entity c69a50cf-ee03-3bd7-831e-407d36c7ee91}
        \vspace{5pt}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{figs/smd_tsne.pdf}
        \label{fig:smd_tsne}
        \vspace{-15pt}
        \caption{SMD - Entity machine-2-3}
        \vspace{5pt}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.65\textwidth}
        \includegraphics[width=\textwidth]{figs/ucr_tsne.pdf}
        \label{fig:ucr_tsne}
        \vspace{-15pt}
        \caption{UCR - Entity 222\_UCR\_Anomaly\_mit14046longtermecg\_56123\_91200\_91700}
        \vspace{5pt}
    \end{subfigure}
    \vspace{-5pt}
    \caption{Each plot visualises a different time series (TS) injection or generation method using t-SNE. Blue markers represent the original anomalous TS, while orange markers represent the injected or generated anomalous TS.}
    \vspace{-5pt}
    \label{fig:ext_tsne}
\end{figure*}

\subsubsection{Additional Visualizations}
\label{sec:apd:vis}
To further illustrate the realistic anomaly generation of GenIAS, we present three additional visualizations comparing generated and real anomalies from both GenIAS and the US-i baselines in Figure \ref{fig:ext_tsne}. The observations are consistent with Section~\ref{sec:visual} in the main text, demonstrating that GenIAS produces comparatively more realistic anomalies with better smoothness in the latent space.

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
