\section{Related Work}
The field of time series anomaly detection has seen remarkable progress, and include  traditional statistical methods as well as state-of-the-art deep learning approaches~\cite{schmidl2022anomaly,audibert2022deep,darban2024deep}. Early efforts included models like One-Class SVM (OC-SVM)~\cite{scholkopf1999support}, LOF~\cite{breunig2000lof}, and Isolation Forest~\cite{liu2008isolation}; however, these methods faced challenges in handling high-dimensional and sequential data. Deep learning models like Donut~\cite{xu2018donut}, LSTM-VAE~\cite{park2018lstmvae} and OmniAnomaly~\cite{su2019robust} introduced temporal dependency modeling and probabilistic approaches in TSAD.

\sloppy
Representation-based methods, have pushed TSAD forward by focusing on meaningful embeddings. TS2Vec~\cite{yue2022ts2vec} applies self-supervised learning to capture multi-level semantic representations, while DCdetector~\cite{yang2023dcdetector} employs dual attention for permutation-invariant representations. Recent models such as TimesNet~\cite{wu2023timesnet} and TranAD~\cite{tuli2022tranad} model complex dependencies by leveraging attention mechanisms. Methods like MTAD-GAT~\cite{zhao2020mtad} utilize graph attention networks to model multivariate dependencies and THOC~\cite{shen2020thoc} incorporates hierarchical modeling to capture temporal patterns at multiple granularities. Anomaly Transformer~\cite{xu2021anomalytran} leverages anomaly-sensitive features through attention mechanisms.

OE~\cite{hendrycks2018OE} integrates external data during training to expose the model to out-of-distribution instances, thereby making it more powerful. NCAD\cite{ncad2022} adapts OE for time series by infusing mixed contextual and random point anomalies to better separate normal and anomalous patterns.
Anomaly injection and perturbation methods have emerged as crucial strategies for enhancing TSAD models. Approaches like CutAddPaste~\cite{wang2024cutaddpaste} and COUTA~\cite{Calibrated} simulate diverse anomaly patterns during training. These techniques have also been integrated into advanced models like CARLA~\cite{DARBAN2025carla} to create negative pairs for contrastive learning, enhancing generalization to unseen anomaly types.