\section{Related Work}
The field of time series anomaly detection has seen remarkable progress, and include  traditional statistical methods as well as state-of-the-art deep learning approaches____. Early efforts included models like One-Class SVM (OC-SVM)____, LOF____, and Isolation Forest____; however, these methods faced challenges in handling high-dimensional and sequential data. Deep learning models like Donut____, LSTM-VAE____ and OmniAnomaly____ introduced temporal dependency modeling and probabilistic approaches in TSAD.

\sloppy
Representation-based methods, have pushed TSAD forward by focusing on meaningful embeddings. TS2Vec____ applies self-supervised learning to capture multi-level semantic representations, while DCdetector____ employs dual attention for permutation-invariant representations. Recent models such as TimesNet____ and TranAD____ model complex dependencies by leveraging attention mechanisms. Methods like MTAD-GAT____ utilize graph attention networks to model multivariate dependencies and THOC____ incorporates hierarchical modeling to capture temporal patterns at multiple granularities. Anomaly Transformer____ leverages anomaly-sensitive features through attention mechanisms.

OE____ integrates external data during training to expose the model to out-of-distribution instances, thereby making it more powerful. NCAD____ adapts OE for time series by infusing mixed contextual and random point anomalies to better separate normal and anomalous patterns.
Anomaly injection and perturbation methods have emerged as crucial strategies for enhancing TSAD models. Approaches like CutAddPaste____ and COUTA____ simulate diverse anomaly patterns during training. These techniques have also been integrated into advanced models like CARLA____ to create negative pairs for contrastive learning, enhancing generalization to unseen anomaly types.