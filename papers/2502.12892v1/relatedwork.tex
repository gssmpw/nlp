\section{Related Work}
\label{sec:related_work}
\paragraph{Sparse Coding \& Dictionary Learning.}
Dictionary learning~\cite{tovsic2011dictionary,rubinstein2010dictionaries,elad2010sparse,mairal2014sparse,dumitrescu2018dictionary} emerged as a fundamental approach for uncovering latent factors of a data-generating process in signal processing and machine learning, building upon early work in sparse coding \cite{olshausen1996emergence, olshausen1997sparse, lee2006efficient,foldiak2008sparse,rentzeperis2023beyond}.
The primary objective of these methods is to find a \emph{sparse} representation of input data~\cite{hurley2009comparing,eamaz2022building}, such that each data sample can be accurately reconstructed using a linear combination of only a small subset of dictionary atoms.
The field gained momentum with compressed sensing theory \cite{donoho2006compressed, candes2006robust, candes2008introduction, lopes2013estimating, rencker2019sparse}, which established theoretical foundations for sparse signal recovery. 
Early dictionary learning methods evolved from vector quantization and K-means clustering \cite{gersho1991vector, lloyd1982least}, leading to more sophisticated approaches like Non-negative Matrix Factorization (NMF) and its variants \cite{lee1999learning,lee2001algorithms,gillis2020nonnegative, ding2008convex,kersting2010hierarchical,thurau2009convex,gillis2015exact}, Sparse PCA, \cite{aspremont2004sparse,zou2006sparse} and K-SVD \cite{aharon2006rm,elad2006image}. 
The field further expanded rapidly \cite{wright2010sparse,chen2021low,tasissa2023kds} with online methods \cite{mairal2009online,kasiviswanathan2012online,lu2013online} and structured sparsity \cite{jenatton2010structured, bach2012structured,sun2014learning}. 
Theoretical guarantees for dictionary learning emerged \cite{aharon2006uniqueness,spielman2012exact,hillar2015can,fu2018identifiability,barbier2022statistical,hu2023global}, alongside connections to deep learning \cite{baccouche2012spatio, tariyal2016deep,papyan2017convolutional,mahdizadehaghdam2019deep,tamkin2023codebook,yu2023white}. 
Parallel developments in archetypal analysis from \citet{cutler1994archetypal} provided complementary perspectives on dictionary learning by focusing on extreme points in a set of observations \cite{dubins1962extreme}.\vspace{-5pt}


\vspace{-2mm}
\paragraph{Vision Explainability.} Early work in the field of Explainable AI primarily revolved around attribution methods, which highlight the input regions that most influence a modelâ€™s prediction~\cite{simonyan2013deep,zeiler2014visualizing,bach2015pixel,springenberg2014striving,smilkov2017smoothgrad,sundararajan2017axiomatic,Selvaraju_2019,Fong_2017,fel2021sobol,novello2022making,muzellec2023gradient}---in other words, \emph{where} the network focuses its attention to produce its prediction. 
While valuable, attribution methods exhibit two core limitations: (\textit{\textbf{i}}) they provide limited information about the semantic organization of learned representations~\cite{hase2020evaluating,hsieh2020evaluations,nguyen2021effectiveness,fel2021cannot,kim2021hive,sixt2020explanations}, and 
(\textit{\textbf{ii}}) they can produce incorrect explanations~\cite{adebayo2018sanity, ghorbani2017interpretation,slack2021counterfactual, sturmfels2020visualizing,hsieh2020evaluations,hase2021out}.
In other words, just because the explanations make sense to humans, we cannot conclude that they accurately reflect what is actually happening within the model---as shown by ongoing efforts to develop robust evaluation metrics for explainability~\cite{petsiuk2018rise, aggregating2020,jacovi2020towards,hedstrom2022quantus,fel2022xplique,hsieh2020evaluations, boopathy2020proper, lin2019explanations,idrissi2021developments}.

To overcome the constraints above, concept-based interpretability~\cite{kim2018interpretability} has gained traction. Its central objective is to pinpoint semantically meaningful directions---revealing not just \emph{where} the model is looking, but also \emph{what} patterns or concepts it employs---and to link these systematically to latent activations~\cite{bau2017network,ghorbani2019towards,zhang2021invertible, fel2023craft,graziani2023concept,vielhaben2023multi,kowal2024understanding,kowal2024visual}. 
In line with this perspective, \citet{fel2023holistic}~demonstrate that popular concept-extraction methods : ACE~\cite{ghorbani2017interpretation}, ICE~\cite{zhang2021invertible}, CRAFT~\cite{fel2023craft}~and more recently SAEs~\cite{cunningham2023sparse, bricken2023monosemanticity, rajamanoharan2024jumping, gao2024scaling, surkov2024unpacking, gorton2024missing, bhalla2024interpreting} all address essentially the same dictionary learning task, albeit subject to distinct constraints (see Eq.~\ref{eq:dico_constraints}). 

Within this broader context, we note Sparse Autoencoders (SAEs) have emerged as a highy scalable special case of dictionary learning: unlike NMF, Sparse-PCA, or other optimization problem, SAEs can be trained with the same algorithms and architectures used in modern deep learning pipelines, making them especially well-suited for large-scale concept extraction. 
However, motivated by more ambitious use-cases of interpretability, e.g., to develop transparency and accountability~\cite{anwar2024foundational}, recent work has started to demonstrate limitations in existing SAE frameworks and propose improvements. Examples of such limitations include learning of overly specific or sensitive features~\cite{bricken2023monosemanticity, chanin2024absorption}, challenges in compositionality~\cite{wattenberg2024relational, lwcomposition}, and limited effects of latent interventions~\cite{bhalla2024towards, menon2024analyzing}.
In this paper, we aim to bring to attention an underappreciated challenge in SAEs' training --\textit{instability} -- wherein mere reruns of SAE training yield inconsistent interpretations (Figure~\ref{fig:toy_rasae}).