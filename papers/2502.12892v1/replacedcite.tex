\section{Related Work}
\label{sec:related_work}
\paragraph{Sparse Coding \& Dictionary Learning.}
Dictionary learning____ emerged as a fundamental approach for uncovering latent factors of a data-generating process in signal processing and machine learning, building upon early work in sparse coding ____.
The primary objective of these methods is to find a \emph{sparse} representation of input data____, such that each data sample can be accurately reconstructed using a linear combination of only a small subset of dictionary atoms.
The field gained momentum with compressed sensing theory ____, which established theoretical foundations for sparse signal recovery. 
Early dictionary learning methods evolved from vector quantization and K-means clustering ____, leading to more sophisticated approaches like Non-negative Matrix Factorization (NMF) and its variants ____, Sparse PCA, ____ and K-SVD ____. 
The field further expanded rapidly ____ with online methods ____ and structured sparsity ____. 
Theoretical guarantees for dictionary learning emerged ____, alongside connections to deep learning ____. 
Parallel developments in archetypal analysis from ____ provided complementary perspectives on dictionary learning by focusing on extreme points in a set of observations ____.\vspace{-5pt}


\vspace{-2mm}
\paragraph{Vision Explainability.} Early work in the field of Explainable AI primarily revolved around attribution methods, which highlight the input regions that most influence a modelâ€™s prediction____---in other words, \emph{where} the network focuses its attention to produce its prediction. 
While valuable, attribution methods exhibit two core limitations: (\textit{\textbf{i}}) they provide limited information about the semantic organization of learned representations____, and 
(\textit{\textbf{ii}}) they can produce incorrect explanations____.
In other words, just because the explanations make sense to humans, we cannot conclude that they accurately reflect what is actually happening within the model---as shown by ongoing efforts to develop robust evaluation metrics for explainability____.

To overcome the constraints above, concept-based interpretability____ has gained traction. Its central objective is to pinpoint semantically meaningful directions---revealing not just \emph{where} the model is looking, but also \emph{what} patterns or concepts it employs---and to link these systematically to latent activations____. 
In line with this perspective, ____~demonstrate that popular concept-extraction methods : ACE____, ICE____, CRAFT____~and more recently SAEs____ all address essentially the same dictionary learning task, albeit subject to distinct constraints (see Eq.~\ref{eq:dico_constraints}). 

Within this broader context, we note Sparse Autoencoders (SAEs) have emerged as a highy scalable special case of dictionary learning: unlike NMF, Sparse-PCA, or other optimization problem, SAEs can be trained with the same algorithms and architectures used in modern deep learning pipelines, making them especially well-suited for large-scale concept extraction. 
However, motivated by more ambitious use-cases of interpretability, e.g., to develop transparency and accountability____, recent work has started to demonstrate limitations in existing SAE frameworks and propose improvements. Examples of such limitations include learning of overly specific or sensitive features____, challenges in compositionality____, and limited effects of latent interventions____.
In this paper, we aim to bring to attention an underappreciated challenge in SAEs' training --\textit{instability} -- wherein mere reruns of SAE training yield inconsistent interpretations (Figure~\ref{fig:toy_rasae}).