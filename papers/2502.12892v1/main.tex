\documentclass{article}

\usepackage{macro}
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} 
\usepackage{multirow}
\usepackage[colorlinks=true,allcolors=indigo]{hyperref}

\newcommand{\theHalgorithm}{\arabic{algorithm}}

\usepackage[nohyperref,accepted]{icml2024}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{xurl}
\usepackage{code}
\usepackage[capitalize,noabbrev,nameinlink]{cleveref}
\usepackage{footmisc}
\usepackage{tabularx}

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

\newcommand{\fel}[1]{{\color{yellow}[Fel]: #1}}
\newcommand{\ekdeep}[1]{{\color{indigo}[Ekdeep]: #1}}
\newcommand{\talia}[1]{{\color{green}[Talia]: #1}}

\newcommand{\primary}[1]{{\color{anthracite}#1}}
\newcommand{\rasae}{{\primary{\textbf{RA-SAE}}}}
\newcommand{\asae}{{\primary{\textbf{A-SAE}}}}
\newcommand{\asaes}{{\primary{\textbf{A-SAE}}}s}

\newcommand{\best}[1]{{\color{green}\bf#1}}
\newcommand{\worst}[1]{{\color{red}\bf#1}}



\newcommand{\customfootnote}[2]{%
\renewcommand{\thefootnote}{}
    \begingroup
    \renewcommand\thefootnote{#1}%
    \footnotemark
    \endgroup
    \footnotetext{#2}%
\renewcommand{\thefootnote}{\arabic{footnote}}
}
\icmltitlerunning{Archetypal SAE}

\begin{document}

\newcommand{\aut}[1]{\rule[-0.5mm]{0mm}{1mm} \textbf{#1}}
\newcommand{\af}[1]{\small{#1} \rule[-0.5mm]{0mm}{1mm}}

\newcommand{\customTitleBlock}{%
  \icmltitle{Archetypal SAE: Adaptive and Stable Dictionary Learning for Concept Extraction in Large Vision Models}%
  \begin{center}
    \begin{tabular}{ccc}
      \shortstack{\aut{Thomas Fel}$^\star$ \\ \af{Kempner Institute} \\ \af{Harvard University}} &
      \shortstack{\aut{Ekdeep Singh Lubana}$^\star$ \\ \af{CBS-NTT Program in Physics of Intelligence} \\ \af{Harvard University}} &
      \shortstack{\aut{Jacob S. Prince} \\ \af{Dept. of Psychology} \\ \af{Harvard University}}
      \vspace{3mm}
    \end{tabular}
    \begin{tabular}{cccc}
      \shortstack{\aut{Matthew Kowal} \\ \af{FAR AI,} \\ \af{York University}} &
      \shortstack{\aut{Victor Boutin} \\ \af{CerCo} \\ \af{CNRS}} &
      \shortstack{\aut{Isabel Papadimitriou} \\ \af{Kempner Institute} \\ \af{Harvard University}} &
      \shortstack{\aut{Binxu Wang} \\ \af{Kempner Institute} \\ \af{Harvard University}} 
      \vspace{3mm}
    \end{tabular}
    \begin{tabular}{ccc}
      \shortstack{\aut{Martin Wattenberg}$^\dagger$ \\ \af{Harvard University,} \\ \af{Google DeepMind}} &
      \shortstack{\aut{Demba Ba} \\ \af{Kempner Institute} \\ \af{Harvard University}} &
      \shortstack{\aut{Talia Konkle} \\ \af{Dept. of Psychology \& Kempner Institute} \\ \af{Harvard University}}
    \end{tabular}
  \end{center}%
}

\twocolumn[\begin{minipage}{\textwidth}\customTitleBlock\end{minipage}]



\icmlcorrespondingauthor{Thomas Fel}{tfel@g.harvard.edu}
\icmlkeywords{Explainability, Interpretability, Dictionary Learning, Computer Vision, Archetypal Analysis}

\customfootnote{}{$^\star$ These authors contributed equally to this work.}
\customfootnote{}{$^\dagger$ work done at Harvard.}
\customfootnote{}{\\~Preprint. Under review.}

\begin{abstract} 
\vspace{2mm}
Sparse Autoencoders (SAEs) have emerged as a powerful framework for machine learning interpretability, enabling the unsupervised decomposition of model representations into a dictionary of abstract, human-interpretable concepts.  
However, we reveal a fundamental limitation: existing SAEs exhibit severe instability, as identical models trained on similar datasets can produce sharply different dictionaries, undermining their reliability as an interpretability tool.
To address this issue, we draw inspiration from the Archetypal Analysis framework introduced by \citet{cutler1994archetypal} and present Archetypal SAEs (\asae), wherein dictionary atoms are constrained to the convex hull of data. 
This geometric anchoring significantly enhances the stability of inferred dictionaries, and their mildly relaxed variants {\rasae}s further match state-of-the-art reconstruction abilities.
To rigorously assess dictionary quality learned by SAEs, we introduce two new benchmarks that test (\textbf{\textit{i}}) plausibility, if dictionaries recover ``true'' classification directions and (\textbf{\textit{ii}}) identifiability, if dictionaries disentangle synthetic concept mixtures. %
Across all evaluations, {\rasae}s consistently yield more structured representations while uncovering novel, semantically meaningful concepts in large-scale vision models.  

\end{abstract}

\begin{figure}
    \vspace{-16.0mm}
    \centering
    \makebox[\textwidth]{ %
        \hspace{-0.53\textwidth} %
        \includegraphics[width=0.55\textwidth]{assets/big_picture.jpg}
    }
    \vspace{-6mm}
    \caption{\textbf{A) Archetypal-SAE.} 
    Archetypal-SAEs constrain dictionary atoms (decoder directions) to the data’s convex hull, improving stability. A relaxed variant (\rasae) allows mild relaxation, matching standard SAEs in reconstruction while maintaining stability. Both integrate with any SAE variant (e.g., TopK, JumpReLU). 
    \textbf{B) Instability Problem.}
    Standard SAEs produce inconsistent dictionaries across runs, undermining interpretability. For example, in classical SAEs, the second most important concept for ``rabbit'' in one run has no counterpart in another run ($\cos = 0.58$). In contrast, Archetypal-SAEs maintain consistent concept correspondences across runs, ensuring stability.
    }
    \label{fig:toy_rasae}
    \vspace{-0mm}
\end{figure}

\clearpage

\section{Introduction}



Artificial Neural Networks (ANNs) have revolutionized computer vision, setting new benchmarks across a wide range of tasks. Despite these successes, the ``black-box'' nature of ANNs poses significant challenges, particularly in domains requiring transparency, accountability, and adherence to strict ethical and regulatory standards~\cite{tripicchio2020deep}. Beyond compliance, there is a growing curiosity within the scientific community to better understand these models' internal mechanisms, both to satisfy fundamental questions about their function, leverage insights for debugging~\cite{adebayo2020debugging} and improvement, and even explore parallels with neuroscience~\cite{goodwin2022toward,vilasposition}.
These motivations have spurred the rapid growth of explainable artificial intelligence (XAI), a field dedicated to enhancing the interpretability of ANNs, thereby bridging the gap between machine intelligence and human understanding~\cite{doshivelez2017rigorous}. 

Among XAI approaches, concept-based methods~\cite{kim2018interpretability} have emerged as a powerful framework for uncovering intelligible visual concepts embedded within the intricate activation patterns of ANNs~\cite{ ghorbani2019towards, zhang2021invertible, fel2023craft, graziani2023concept, vielhaben2023multi}. These methods excel in making the internal representations of ANNs more comprehensible by associating them with human-interpretable concepts.
Recently, concept extraction methods have been shown to be instances of dictionary learning~\cite{fel2023holistic}, where the goal is to map the activation space of an ANN into a (higher-dimensional), sparse ``concept space''. The resulting concept basis is often considered more interpretable. For such representations to be effective, they must be as sparse as possible while still enabling accurate reconstruction of the original activations from the learned basis using a dictionary of atoms---also called prototypes.
Historically, dictionary learning methods have included techniques such as Non-negative Matrix Factorization (NMF)~\cite{lee2001algorithms,kowal2024understanding,jourdan2023cockatiel} and K-Means~\cite{gersho1991vector,ghorbani2019towards}, while more recent approaches like Sparse Autoencoders (SAEs)~\cite{cunningham2023sparse, bricken2023monosemanticity, rajamanoharan2024jumping,gao2024scaling,thasarathan2025universal,poche2025consim} have emerged as a powerful alternative. SAEs achieve a good balance between sparsity and reconstruction quality, and their optimization frameworks scale well to large datasets. However, compared to traditional methods, SAEs suffer from a critical limitation: instability. As illustrated in Figure~\ref{fig:toy_rasae}, training two identical SAEs on the same data can yield significantly different dictionaries (concept bases), undermining their reliability and interpretability.

In this work, we address the instability of current SAEs by introducing two novel variants: Archetypal-SAE (\asae) and its relaxed counterpart~(\rasae). Building upon archetypal analysis \citep{cutler1994archetypal}, A-SAE constrains each dictionary atom to reside within the convex hull of the training data, ensuring a more stable and consistent set of basis elements across different training runs by virtue of this geometrical constraint. \rasae~further extend this framework by incorporating a small relaxation term, allowing for slight deviations from the convex hull to enhance modeling flexibility while maintaining stability.
Overall, our work makes the following contributions.





\begin{enumerate}
\item \textbf{Instability in SAEs.} We identify a critical limitation of current SAEs paradigms: two training runs on identical data can yield concept dictionaries that are largely distinct, hence compromising their reliability as an interpretability protocol. 

\item \textbf{\asae: Archetypal anchoring to overcome instability.} To address the challenge above, we take inspiration from \citet{cutler1994archetypal}'s Archetypal analysis of dictionary learning and propose \asae, an SAE paradigm wherein the dictionary atoms (decoder directions) are forced to lie in the convex hull of sample representations. As we show, this geometrical anchoring yields substantial stability across training runs. Moreover, we show a mild relaxation of this protocol, which we title \rasae, uncovers meaningful and interpretable concepts in large-scale vision models.

\item \textbf{Rigorous evaluations with novel metrics and benchmarks.} We introduce novel metrics to evaluate the quality of dictionaries learned using different SAE paradigms, while proposing two new benchmarks for testing SAEs' ability to recover classification directions and to disentangle synthetic image mixtures, inspired by identifiability theory~\citep{locatello2019challenging, locatello2020weakly, higgins2017beta}. Our results provide substantial evidence that \asaes~ find more structured and coherent concepts. Further, to enable reproduction, we open-source our extensive codebase for large-scale SAE training on modern vision models. 
\end{enumerate}


\section{Related Work}
\label{sec:related_work}
\paragraph{Sparse Coding \& Dictionary Learning.}
Dictionary learning~\cite{tovsic2011dictionary,rubinstein2010dictionaries,elad2010sparse,mairal2014sparse,dumitrescu2018dictionary} emerged as a fundamental approach for uncovering latent factors of a data-generating process in signal processing and machine learning, building upon early work in sparse coding \cite{olshausen1996emergence, olshausen1997sparse, lee2006efficient,foldiak2008sparse,rentzeperis2023beyond}.
The primary objective of these methods is to find a \emph{sparse} representation of input data~\cite{hurley2009comparing,eamaz2022building}, such that each data sample can be accurately reconstructed using a linear combination of only a small subset of dictionary atoms.
The field gained momentum with compressed sensing theory \cite{donoho2006compressed, candes2006robust, candes2008introduction, lopes2013estimating, rencker2019sparse}, which established theoretical foundations for sparse signal recovery. 
Early dictionary learning methods evolved from vector quantization and K-means clustering \cite{gersho1991vector, lloyd1982least}, leading to more sophisticated approaches like Non-negative Matrix Factorization (NMF) and its variants \cite{lee1999learning,lee2001algorithms,gillis2020nonnegative, ding2008convex,kersting2010hierarchical,thurau2009convex,gillis2015exact}, Sparse PCA, \cite{aspremont2004sparse,zou2006sparse} and K-SVD \cite{aharon2006rm,elad2006image}. 
The field further expanded rapidly \cite{wright2010sparse,chen2021low,tasissa2023kds} with online methods \cite{mairal2009online,kasiviswanathan2012online,lu2013online} and structured sparsity \cite{jenatton2010structured, bach2012structured,sun2014learning}. 
Theoretical guarantees for dictionary learning emerged \cite{aharon2006uniqueness,spielman2012exact,hillar2015can,fu2018identifiability,barbier2022statistical,hu2023global}, alongside connections to deep learning \cite{baccouche2012spatio, tariyal2016deep,papyan2017convolutional,mahdizadehaghdam2019deep,tamkin2023codebook,yu2023white}. 
Parallel developments in archetypal analysis from \citet{cutler1994archetypal} provided complementary perspectives on dictionary learning by focusing on extreme points in a set of observations \cite{dubins1962extreme}.\vspace{-5pt}


\vspace{-2mm}
\paragraph{Vision Explainability.} Early work in the field of Explainable AI primarily revolved around attribution methods, which highlight the input regions that most influence a model’s prediction~\cite{simonyan2013deep,zeiler2014visualizing,bach2015pixel,springenberg2014striving,smilkov2017smoothgrad,sundararajan2017axiomatic,Selvaraju_2019,Fong_2017,fel2021sobol,novello2022making,muzellec2023gradient}---in other words, \emph{where} the network focuses its attention to produce its prediction. 
While valuable, attribution methods exhibit two core limitations: (\textit{\textbf{i}}) they provide limited information about the semantic organization of learned representations~\cite{hase2020evaluating,hsieh2020evaluations,nguyen2021effectiveness,fel2021cannot,kim2021hive,sixt2020explanations}, and 
(\textit{\textbf{ii}}) they can produce incorrect explanations~\cite{adebayo2018sanity, ghorbani2017interpretation,slack2021counterfactual, sturmfels2020visualizing,hsieh2020evaluations,hase2021out}.
In other words, just because the explanations make sense to humans, we cannot conclude that they accurately reflect what is actually happening within the model---as shown by ongoing efforts to develop robust evaluation metrics for explainability~\cite{petsiuk2018rise, aggregating2020,jacovi2020towards,hedstrom2022quantus,fel2022xplique,hsieh2020evaluations, boopathy2020proper, lin2019explanations,idrissi2021developments}.

To overcome the constraints above, concept-based interpretability~\cite{kim2018interpretability} has gained traction. Its central objective is to pinpoint semantically meaningful directions---revealing not just \emph{where} the model is looking, but also \emph{what} patterns or concepts it employs---and to link these systematically to latent activations~\cite{bau2017network,ghorbani2019towards,zhang2021invertible, fel2023craft,graziani2023concept,vielhaben2023multi,kowal2024understanding,kowal2024visual}. 
In line with this perspective, \citet{fel2023holistic}~demonstrate that popular concept-extraction methods : ACE~\cite{ghorbani2017interpretation}, ICE~\cite{zhang2021invertible}, CRAFT~\cite{fel2023craft}~and more recently SAEs~\cite{cunningham2023sparse, bricken2023monosemanticity, rajamanoharan2024jumping, gao2024scaling, surkov2024unpacking, gorton2024missing, bhalla2024interpreting} all address essentially the same dictionary learning task, albeit subject to distinct constraints (see Eq.~\ref{eq:dico_constraints}). 

Within this broader context, we note Sparse Autoencoders (SAEs) have emerged as a highy scalable special case of dictionary learning: unlike NMF, Sparse-PCA, or other optimization problem, SAEs can be trained with the same algorithms and architectures used in modern deep learning pipelines, making them especially well-suited for large-scale concept extraction. 
However, motivated by more ambitious use-cases of interpretability, e.g., to develop transparency and accountability~\cite{anwar2024foundational}, recent work has started to demonstrate limitations in existing SAE frameworks and propose improvements. Examples of such limitations include learning of overly specific or sensitive features~\cite{bricken2023monosemanticity, chanin2024absorption}, challenges in compositionality~\cite{wattenberg2024relational, lwcomposition}, and limited effects of latent interventions~\cite{bhalla2024towards, menon2024analyzing}.
In this paper, we aim to bring to attention an underappreciated challenge in SAEs' training --\textit{instability} -- wherein mere reruns of SAE training yield inconsistent interpretations (Figure~\ref{fig:toy_rasae}).



\section{(In)Stability of SAEs}
\label{sec:stability}
\begin{figure*}
    \vspace{-2mm}
    \centering
    \includegraphics[width=0.99\textwidth]{assets/pareto_vision.jpg}
    \vspace{-1mm}
    \caption{\textbf{SAEs are a promising direction for scalable concept extraction in vision.} Comparison of reconstruction error ($\ell_2$ Loss) and sparsity across four large-scale vision models: ConvNext, DINO, SigLIP, and ViT. The figure compares the performance of various dictionary learning methods, including classical approaches (Convex-NMF, Semi-NMF) and modern Sparse Autoencoders (Vanilla SAE, Top-K SAE, JumpReLU SAE). Each SAE is trained up to 250 million tokens per epoch over 50 epochs, demonstrating the scalability of SAEs and their ability to achieve superior trade-offs between reconstruction fidelity and sparsity compared to traditional methods.}
    \label{fig:pareto}
    \vspace{-0mm}
\end{figure*}


We first establish the challenge of instability in SAEs' training. To this end, we start by defining notations and providing background on the SAEs analyzed in this work, then offering a formal measure of instability in SAE training.\vspace{-3pt}

\paragraph{Notation.}
Throughout this work, $||\cdot||_2$ and $||\cdot||_F$ represent the $\ell_2$ and Frobenius norms, respectively. We define $[n] = \{1, \dots, n\}$, and consider a general representation learning framework where a deep learning model $\pred : \SX \to \SA$ maps inputs from an input space $\SX$ to a representation space $\SA \subseteq \R^d$. The representation is captured as a set of $n$ points arranged in a matrix $\A \in \R^{n \times d}$, with $\A_i$ denoting its $i$-th row, where $i \in [n]$ and $\A_i \in \R^d$. For any matrix $\X$, $\X \geq 0$ denotes that all elements of $\X$ are non-negative, and $\mathcal{P}(n)$ is the set of $n \times n$ signed permutation matrices. Given $\A \in \R^{n \times d}$, we define $\mathrm{cone}(\A) = \{\x \mid \x = \A \bm{v}, \; \bm{v} \in \R^n, \; \bm{v} \geq 0\}, ~ \text{and} ~ \mathrm{conv}(\A) = \{\x \mid \x = \A \bm{v}, \; \bm{v} \in \R^n, \; \bm{v} \geq 0, \; \bm{1}^\tr \bm{v} = 1\}$ as the conical and convex hulls, respectively, generated by the columns of $\A$.
\paragraph{Concept Extraction as Dictionary Learning.}
Concept extraction can be naturally framed as a dictionary learning problem, wherein a set of $n$ activations, represented by the matrix $\A \in \R^{n \times d}$, is approximated using a structured decomposition. The goal is to learn a \textbf{Dictionary} $\D$---also referred to as atoms~\cite{serre2006learning}, prototypes, or a codebook~\cite{tamkin2023codebook}---such that activations can be reconstructed as sparse linear combinations of these learned directions. The corresponding coefficients, known as \textbf{Codes} $\Z$, capture the latent structure of the activations and enforce interpretability by promoting sparsity or non-negativity. This leads to the general optimization framework:
\vspace{-1mm}
\begin{equation}
\begin{aligned}
\nonumber
    & (\Z^\star, \D^\star) = \argmin_{\Z, \D} || \A - \Z \D^\tr ||^2_F, \\
    \text{s.t.} ~~ &\begin{cases}
        \forall i, \Z_i \in \{ \e_1, \ldots, \e_k \}, & \text{(\textbf{ACE} - K-Means)}, \\
        \D^\tr \D = \mathbf{I}, & \text{(\textbf{ICE} - PCA)}, \\
        \Z \geq 0, \D \geq 0, & \text{(\textbf{CRAFT} - NMF)}, \\
        \Z = \bm{\Psi}_{\theta}(\A), ||\Z||_0 \leq K, & \text{(SAEs)}.
    \end{cases}
\end{aligned}
\label{eq:dico_constraints}
\end{equation}
Here, $\e_i$ denotes the $i$-th canonical basis vector, $\mathbf{I}$ is the identity matrix, and $\encoder$ is a neural network parameterized by $\theta$ (typically a single feedforward layer with bias). Notably, in Sparse Autoencoder (SAE) literature, the dictionary $\D$ is often identified with the decoder weight matrix, denoted as $\mat{W}_{\text{dec}}$. This optimization framework unifies various classical methods for concept extraction, ranging from clustering-based approaches~\cite{ghorbani2019towards}, orthogonal factorization methods~\cite{zhang2021invertible,graziani2021sharpening}, and nonnegative matrix factorization~\cite{fel2023craft} to modern sparse autoencoding techniques (SAEs)~\cite{cunningham2023sparse,bricken2023monosemanticity}.

Despite similar formulations to SAEs, solving K-Means, PCA, or NMF typically does not rely on backpropagation and lacks inherent batch-learning capabilities. This make SAEs particularly appealing for large-scale applications. 
Additionally, optimization problems solved in multiple steps, such as NMF, Semi-NMF, or K-Means, can be interpreted as having a multi-layer nonlinear encoding (akin to a $\encoder$ with multiple layers)\footnote{A single optimization step in a NMF is akin to a linear model followed by a ReLU (projection onto the positive orthant).}. We note that the decoding process in these methods remains linear.

To compare these approaches, we generally evaluate the trade-off between two metrics: \textit{sparsity}, measured as the $\ell_0$ norm of $\Z$, and \textit{fidelity}, measured as the $\ell_2$ reconstruction error. 
As a starting point, we propose to study this pareto frontier using state-of-the-art SAEs, including Jump-ReLU~\cite{rajamanoharan2024jumping}, TopK~\cite{gao2024scaling}, and a vanilla SAE with $\ell_1$ regularization~\cite{bricken2023monosemanticity}, alongside classical sparse dictionary learning methods. Since PCA is non-sparse, K-Means sparsity is fixed, and NMF is applicable only to non-negative activations, we adopt modified versions of these methods that are broadly applicable for concept extraction~\cite{kowal2024understanding,parekh2024concept}, such as Convex-NMF and Semi-NMF.
For SAEs, we use the following standard formulation:
\vspace{-2mm}
\begin{equation}
\encoder(\A) = \bm{\sigma}(\A \mat{W}_{\bm{\theta}} + \bm{b}),
\vspace{-1mm}
\end{equation}
where $\A$ denotes a linear encoder layer and $\bm{\sigma}(\cdot)$ is an elementwise non-linearity that depends on the specific SAE architecture.
For all SAEs, the resulting codes $\Z = \encoder(\A) \geq 0$ holds. 
We employ a Silverman kernel \cite{silverman1984spline} for instantiating Jump-ReLU SAEs. 
For vanilla SAEs, an $\ell_1$ regularization on $\mathbf{Z}$ is applied until the desired sparsity is achieved. Top-K and Jump-ReLU SAEs directly control or optimize an $\ell_0$ constraint.


Results in Figure~\ref{fig:pareto} illustrate that the SAE methods discussed above outperform optimization based dictionary learning methods in terms of reconstruction fidelity for fixed sparsity levels. While this positions SAEs as a compelling solution for concept extraction, as we show in the following, a significant drawback of current methods lies in their instability: minor changes to the dataset can lead to substantial variations in the learned dictionary.

\begin{figure*}
    \vspace{-1mm}
    \centering
    \includegraphics[width=0.99\textwidth]{assets/instability.jpg}
    \vspace{0mm}
    \caption{\textbf{Stability-Reconstruction tradeoff} (optimal: top-left). We implement 5 dictionary learning methods on 4 models at 5 levels of sparsity each, as well as our A-SAE method. We show that SAEs exhibit instability (minor perturbations in the dataset can lead to significant changes in the learned dictionary), while traditional dictionary learning methods are more stable but worse at reconstructing the data. Archetypal-SAEs (ours) help mitigate this issue. We measure stability based on \Cref{eq:stability}: the optimal average cosine similarity between the dictionaries across 4 runs after finding the best alignment via the Hungarian algorithm. 
    Archetypal-SAEs improve stability without compromising reconstruction fidelity, performing better on the stability-reconstruction tradeoff than existing methods. 
    }
    \label{fig:instability}
\end{figure*}


\paragraph{Measuring Instability.} To formally define a notion of stability in SAEs' training, we seek inspiration from the dictionary learning literature and prior works~\cite{spielman2012exact}, yielding the following metric for two dictionaries $\D \in \R^{n \times d}$ and $\D'$:
\begin{equation}
\text{Stability}(\D, \D') = \max_{\bm{\Pi} \in \mathcal{P}(n)} \frac{1}{n} \text{Tr}(\D^{\tr} \bm{\Pi} \D'),
\label{eq:stability}
\end{equation}
where we assume that each atom lies on the unit $\ell_2$-norm ball, i.e., $||\D_i||_2 = 1$ for all $i \in [n]$. This metric essentially measures the optimal average cosine similarity between the dictionaries after finding the best alignment via the Hungarian algorithm\footnote{In practice, we use the \texttt{linear\_sum\_assignment} function from \texttt{scipy}~\cite{virtanen2020scipy}, which solves the linear sum assignment problem by finding the optimal matching in a bipartite graph using the Jonker-Volgenant algorithm.}.

We evaluate stability across various vision models by altering only the random seed of the algorithm, while keeping the dataset unchanged across 4 runs. A similar trend is observed when the dataset is perturbed by as little as $5\%$ or $10\%$, consistent with prior findings~\cite{fel2023holistic, braun2024identifying, paulo2025sparse}. Results in Figure~\ref{fig:instability} show that while SAEs outperform classical dictionary learning methods in terms of reconstruction error ($\ell_2$-loss), they exhibit lower stability, with cosine stability values around $0.5$ for TopK SAE trained on DinoV2 with over 250 million tokens. As a first approximation, this implies that re-running the same algorithm with a different seed can result in dictionaries where only half the concepts remain, while the other half are new orthogonal concepts.
By contrast, the proposed Archetypal SAE, introduced below, achieves stability comparable to classical dictionary learning methods without compromising reconstruction fidelity.





\section{Towards Archetypal SAEs} 
\label{sec:archetypal}

In their seminal work, \citet{cutler1994archetypal} proposed representing each data point as a convex combination of \textit{Archetypes}, which are themselves defined as convex combinations of data points. Concretely, the dictionary (the collection of Archetypes) can be constructed by multiplying the data by a row-stochastic matrix.
Drawing on these ideas, we propose a solution to the problem of instability in SAEs' training: the \textbf{Archetypal SAE} (\asae), which acts as a plug-and-play parameterization of the dictionary---i.e., the decoder matrix---and can be seamlessly integrated into any existing SAE, e.g., TopK~\cite{gao2024scaling} or Jump-ReLU~\cite{rajamanoharan2024jumping}. 

\paragraph{Formulation.} Let $\A \in \R^{n \times d}$ represent the data matrix (with $n$ data points in $\R^d$), and $\Delta^n = \{ \x \in \R^n \mid x_i \geq 0, \bm{1}^{\tr} \x = 1 \}$ denote the $(n\!-\!1)$-dimensional simplex in $\R^n$. A matrix $\W \in \R^{k \times n}$ is \emph{row-stochastic} if each row $\W_i$ belongs to $\Delta^n$. Define the set of row-stochastic matrices as
\begin{equation}
\Omega_{k,n} \triangleq \{ \W \in \R^{k \times n} \mid \W \geq 0, \W \bm{1}_n = \bm{1}_k \}.
\end{equation}
An archetypal dictionary $\D$ is then defined as follows.
\begin{equation}
\D = \W \A, \quad \text{where } \W \in \Omega_{k,n}.
\end{equation}
Hence, each row of $\D$ is a convex combination of the rows of $\A$, ensuring that each archetype originates from the data.\vspace{-2mm}


\paragraph{Geometric Interpretation.}
In standard SAEs (and most dictionary learning approaches), the dictionary $\D$ is \emph{free} in the sense that each atom $\D_i \in \R^d$ can be placed anywhere in the ambient space. From a geometric perspective, this flexibility allows the reconstructions $\Z\D$ to span regions that may exceed the convex hull of the data $\A$. 
While this unconstrained setting enables greater expressivity, it comes with significant drawbacks. Specifically, small perturbations in the data or random initializations can lead to unstable solutions, resulting in dictionaries that differ drastically across training runs.  
Moreover, if the dictionary atoms $\D_i$ point in directions unrelated to the data, probing these directions may fail to activate any meaningful mechanisms within the underlying model~\cite{makelov2023subspace}. This highlights the importance of ensuring that the dictionary aligns with ``real'' directions inherent to the data.  

In contrast to above, the \emph{Archetypal} dictionary imposes a crucial geometric restriction: every dictionary atom is constrained to lie within the convex hull of $\A$, i.e., each dictionary atom $\D_i$ is a convex combination of samples' representations. 
Thus, once multiplied by a nonnegative $\Z$, the reconstructions $\Z \D$ remain within the \emph{conic} hull of $\A$. 
This anchoring within the data manifold precludes the emergence of pathological or out‐of‐sample directions, yielding stability gains shown empirically in Figure~\ref{fig:instability} and formalized in Proposition~\ref{prop:stability}. 
Concretely, we always have:\vspace{-2mm}

\begin{equation*}
\D \in \mathrm{conv}(\A), ~~ \Z\D \in \mathrm{cone}(\A).    
\end{equation*}

Moreover, one can restrict $\D$ to be formed from a subset $\C \subseteq \R^{n' \times d}$ (rather than all of $\A$) without losing expressivity, provided $\C$ contains the extreme points of $\A$~\cite{dubins1962extreme}. Indeed, in that case $\mathrm{cone}(\C) = \mathrm{cone}(\A)$, ensuring the same representational power (see Proposition~\ref{prop:archetypal-convex-conic}). 

We must now ask the core question that makes SAEs exciting: does an SAE with an Archetypal dictionary, as defined above, scale? Indeed, optimizing the matrix $\W$ of size $k \times n$, where $n$ is the number of points and $k$ is the number of concepts, is often infeasible in practice (e.g., when the number of tokens $n > 10^8$). This limitation speaks to the importance of the matrix $\C$, which can either be a subset of $\A$ or elements within $\mathrm{conv}(\A)$, such as mixtures of points. Specifically, accessing extreme points is necessary to achieve perfect reconstruction~\cite{simon2011convexity}, but this is also intractable in practice for such high dimensions. %
However, we propose to address this with a relaxation.

\paragraph{Scaling Archetypal-SAE.}
\label{sec:scale}
To maintain the desirable properties of \asae~while addressing scalability, we propose using a reduced subset of points $\C$, with $n' \ll n$, chosen as centroids of $\A$ obtained via K-Means. We fix $\C$ and train only $\W$. 
We apply a ReLU activation and normalize each row of $\W$ at each step to ensure that $\W \in \Omega_{k,n}$. 
Experiments indicate that K-Means forms the most reliable method for distilling $\A$ into $\C$ (see Figure~\ref{fig:distillation}), compared to alternatives such as isolation forests~\cite{liu2008isolation}, convex hull computation in reduced dimensions, or outlier detection methods~\cite{scholkopf1999support,breunig2000lof} (details in Appendix \ref{ap:distillation}). 
Hereafter, we use \asae~to refer to this implementation where we directly optimize $\W$ to find the best convex combination of points that reconstruct data.
\begin{figure}
    \vspace{-1mm}
    \centering
    \includegraphics[width=1.01\linewidth]{assets/ablation_delta.jpg}
    \caption{\textbf{Impact of the Relaxation Parameter ($\delta$).} Enumerating extreme points is infeasible in practice; therefore, we introduce a small relaxation parameter $(\delta)$ that allows exploration beyond the convex hull of $\mathbf{C}$. The magnitude of this relaxation enables the Archetypal SAE to achieve performance comparable to the unconstrained TopK SAE denoted as Baseline (\textbf{left}) while maintaining excellent stability (\textbf{right}).\vspace{-1mm}}
    \label{fig:ablation_delta}
\end{figure}
To enable a controlled degree of flexibility beyond $\mathrm{conv}(\C)$, we introduce a mild relaxation term $\bm{\Lambda} \in \R^{k \times d}$, a matrix of the same dimensions as the dictionary, with a small norm constraint $||\bm{\Lambda}||_2^2 \leq \delta$. This leads to a relaxed formulation, which we call the \textbf{Relaxed Archetypal SAE} (\rasae). Unlike standard A-SAEs, \rasae~learns both the convex combination weights $\W$ and the relaxation term $\bm{\Lambda}$, while ensuring that deviations from the convex hull remain minimal. Formally, we define the dictionary as:
$$
\D = \W \C + \bm{\Lambda}, \quad s.t. \quad \W \in \Omega_{k,n} ~~ \text{and} ~~ ||\bm{\Lambda}||_2^2 \leq \delta.
$$
Here, only $\W$ and $\bm{\Lambda}$ are trainable parameters, as detailed in the pseudocode (Figure~\ref{code:archetypal}). This implementation ensures that $\W$ remains row-stochastic and that the deviation term $\bm{\Lambda}$ stays within the prescribed norm constraint. As shown in Figure~\ref{fig:ablation_delta}, \rasae~achieves reconstruction performance on par with conventional Top-K SAEs while maintaining the stability benefits of the archetypal constraint.

\begin{figure}[h]
\centering
\noindent\begin{minipage}{0.5\textwidth}
\begin{RoundedListing}
class ArchetypalDictionary(nn.Module):
  def __init__(self, C, k, $\delta$=1.0):
    super().__init__()
    n$'$, d = C.shape
    self.register_buffer("C", C)
    self.W = nn.Parameter(torch.eye(k, n$'$))
    self.$\Lambda$ = nn.Parameter(torch.zeros(k, d))
    self.$\delta$ = $\delta$

  def forward(self, Z):
    with torch.no_grad():
      W = torch.relu(self.W)
      W /= W.sum(dim=-1, keepdim=True)
      $\Lambda$ *= torch.clamp(
          self.$\delta$ / $\Lambda$.norm(dim=-1, keepdim=True),
          max=1
      )
    D = W @ self.C + $\Lambda$
    return Z @ D
\end{RoundedListing}
\end{minipage}
\caption{\textbf{Pseudocode for Relaxed Archetypal SAE (\rasae).} This implementation ensures that dictionary atoms remain close to convex hull of the data $\mathrm{conv}(\C)$ while allowing controlled deviations for better flexibility.}
\label{code:archetypal}
\end{figure}



\section{Experiments}
This section is organized into five parts. 
We begin by describing the experimental setup in detail. 
Then, we introduce a novel set of theoretical metrics designed to better understand the differences between optimization-based dictionary learning methods, standard SAEs, and the proposed Archetypal SAEs.
Following this, we present a new benchmark inspired by identifiability theory, which serves to evaluate the plausibility and uniqueness of the learned representations. 
We then propose a second, more practical benchmark to assess whether the models can retrieve ``true'' directions or concepts effectively utilized by the models.
Finally, we conclude with qualitative examples showcasing the concepts discovered by SAEs, particularly when applied to the DINOv2 model, providing insight into the interpretability and utility of the learned representations.

\vspace{-3mm}
\paragraph{Setup.} 
We evaluate five models: DINOv2~\cite{darcet2023vision,oquab2023dinov2}, SigLip~\cite{zhai2023sigmoid}, ViT~\cite{dosovitskiy2020image}, ConvNeXt~\cite{liu2022convnet}, and ResNet50~\cite{he2016deep}, sourced from the \texttt{timm} library~\cite{wightman2019pytorch}.
Unless specified, we trained overcomplete dictionaries with size $k = 5 \times$ the feature dimension (e.g., $768 \times 5$ for DINOv2 and $2048 \times 5$ for ConvNeXt). Models were trained on the ImageNet dataset ($\sim1.28$M images), resulting in over $60$M tokens per epoch for ConvNeXt ($7 \times 7$ tokens/image) and over $250$M tokens per epoch for DINOv2 ($14 \times 14$ patches/image) across 50 epochs.
Semi-NMF and Convex-NMF were trained using gradient descent with accumulation and $\ell_1$ regularization to control sparsity, while the \rasae~was applied atop a TopK SAE to maintain consistent sparsity. To compute $\C$, we applied K-Means clustering to the entire dataset, reducing it to $32,\!000$ centroids, which achieved reconstruction error comparable to the unconstrained SAE. The data matrix $\A$ was element-wise standardized.

\subsection{Dictionary Learning Metrics}
\label{sec:metrics}
\begin{table*}[h!]
\centering
\resizebox{0.75\linewidth}{!}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Metric} & \textbf{Van. SAE} & \textbf{TopK SAE} & \textbf{Jump SAE} & \textbf{SNMF} & \textbf{CNMF} & \rasae \\
\midrule
$R^2$ ($\uparrow$) & 83.94 & \underline{89.52} & \textbf{89.92} & 67.43 & 55.48 & 89.34 \\
Dead Codes ($\downarrow$) & \textbf{0.00} & \textbf{0.00} & \textbf{0.00} & 0.064 & 0.031 & 0.02 \\
\midrule
Stability ($\uparrow$) & 0.710 & 0.542 & 0.539 & 0.925 & \textbf{0.933} & \underline{0.927} \\
Max Cosine ($\uparrow$) & 0.997 & 0.993 & 0.994 & \textbf{0.999} & \textbf{0.999} & \textbf{0.999} \\
OOD Score ($\downarrow$) & 0.451 & 0.551 & 0.551 & 0.430 & \underline{0.087} & \textbf{0.060} \\
\midrule
Stable Rank ($\downarrow$) & 86.8 & 141.6 & 128.0 & \textbf{5.38} & 6.65 & \underline{5.89} \\
Eff. Rank ($\downarrow$) & 363 & 372 & 371 & \textbf{186} & \underline{289} & 310 \\
Coherence ($\downarrow$) & 0.838 & \underline{0.728} & \textbf{0.560} & 0.999 & 0.999 & 0.973 \\
\midrule
Connectivity ($\uparrow$) & 0.000 & 0.002 & 0.003 & \textbf{0.243} & 0.138 & \underline{0.159} \\
Neg. Inter. ($\downarrow$) & 39.99 & 135.7 & 243 & \underline{0.005} & \textbf{0.002} & 0.012 \\
\bottomrule
\end{tabular}
}
\caption{
\textbf{Quantitative comparison of the dictionary learning methods on DINOv2,} using a 90\% sparse, overcomplete dictionary with 2000 concepts. SAE methods achieve the highest reconstruction performance. C-NMF, S-NMF, and Archetypal methods excel in consistency, ensuring stability across runs and that found concepts are close to real data (OOD). Additionally, these methods demonstrate superior dictionary structure (Stable and Eff.\ Rank) and codes structure (Connectivity and Neg\ Inter.), indicating patterns in the inferred concepts and structured sparsity.
}
\label{table:sae_metrics}
\end{table*}

To improve SAEs, it is essential to deepen our understanding of the solutions they yield. 
We thus evaluate both standard and novel sets of metrics that evaluate dictionary learning methods across four key dimensions: (\textbf{\textit{i}}) sparse reconstruction, (\textbf{\textit{ii}}) consistency, (\textbf{\textit{iii}}) structure in the dictionary ($\D$), and (\textbf{\textit{iv}}) structure in the codes ($\Z$), all reported in Tab.~\ref{table:sae_metrics} (see App.~\ref{ap:metrics} for formal definitions). 
Overall, we find Archetypal SAEs achieve a strong balance between reconstruction performance, consistency, and identification of structure. 

\textbf{\textbf{\textit{i}}) Sparse Reconstruction.}
Prior work~\cite{bricken2023monosemanticity} commonly assesses the quality of the learned dictionary to reconstruct the original data under sparsity constraints via metrics such as $R^2$, sparsity ($\ell_0$ norm), and the effective usage of dictionary atoms (e.g., dead codes). We find that Archetypal SAEs perform on-par with existing SAEs and outperform NMF methods in reconstruction, achieving higher $R^2$ scores for comparable sparsity levels. 

\textbf{\textbf{\textit{ii}}) Consistency.}
Excelling in reconstruction does not guarantee that the learned solution aligns with the underlying data distribution or is stable across training runs.
To this end, we measure stability, which assesses the consistency of learned dictionaries across different initializations or perturbations in the data, and the OOD score, which quantifies how close the dictionary atoms are to real data points, i.e., whether learned concepts remain grounded and interpretable.
Our findings indicate that SAEs perform poorly in consistency, showing both low stability (as evidenced in Figure~\ref{fig:instability}) and suboptimal OOD scores. In contrast, Archetypal SAEs significantly enhance stability and OOD score without sacrificing reconstruction performance.
In Proposition~\ref{prop:ood}, we provide theoretical arguments showing that, under mild assumptions, the OOD score of Archetypal SAEs is inherently lower-bounded, ensuring that learned dictionary atoms remain well-anchored within the data.

\textbf{\textbf{\textit{iii}}) Structure in the Dictionary ($\D$).}
This dimension examines whether the learned dictionary exhibits meaningful patterns. A well-structured dictionary may reveal meta-concepts or decomposable higher-level abstractions. Metrics such as stable rank, effective rank, and coherence provide insights into the compactness and interpretability of the dictionary. 
In particular, among solutions with comparable reconstruction performance, a more structured dictionary is preferable due to its potential for higher interpretability and organization. Across runs, methods like CNMF, SNMF, and Archetypal dictionary learning consistently yield dictionaries with better structure, reflecting their capacity for capturing higher-level patterns within the data.
We also provide theoretical arguments in Proposition~\ref{prop:rank}, demonstrating that the rank of Archetypal dictionaries is inherently bounded by the rank of the data matrix.

\textbf{\textbf{\textit{iv}}) Structure in the Codes ($\Z$).}
The structure in the encoding space is equally important, as it determines how concepts are combined to reconstruct the data. Connectivity, measured as the $\ell_0$ norm of $\Z\Z^\tr$, reflects the combinatorial diversity of the concepts. High connectivity enables complex reconstructions, while low connectivity highlights structural sparsity and simpler patterns.
Additionally, negative interference (Neg. Inter.) quantifies the simultaneous activation of conflicting concepts, which can cancel each other out. Archetypal SAEs and optimization-based dictionary learning approaches reliably produce more structured codes with reduced interference, enhancing the coherence of their representations.






\subsection{Plausibility Benchmark} 
\vspace{0mm}

\begin{table}[h!]
\centering
\hspace{-2mm}
\resizebox{1.03\linewidth}{!}{%
\begin{tabular}{lccccccc}
\toprule
\textbf{Dict. size (k)} & \textbf{512} & \textbf{1k} & \textbf{2k} & \textbf{4k} & \textbf{8k} & \textbf{16k} & \textbf{32k} \\
\midrule
\multicolumn{8}{l}{\textbf{ConvNeXt}} \\
\midrule
Baseline (TopK SAE)& 0.1681 & 0.1686 & \worst{0.1668} & \worst{0.1671} & \worst{0.1684} & \worst{0.1692} & \worst{0.1684} \\
A-SAE ($\delta=0$) & \best{0.2172} & \best{0.3046} & \best{0.3597} & 0.3887 & 0.3957 & 0.3984 & 0.3999 \\
RA-SAE ($\delta=0.01$) & 0.1973 & 0.2887 & 0.3581 & \best{0.3900} & 0.4007 & 0.4038 & 0.4045 \\
RA-SAE ($\delta=0.1$) & 0.1270 & 0.1596 & 0.2106 & 0.2681 & 0.3280 & 0.3674 & 0.3845 \\
RA-SAE ($\delta=1.0$) & \worst{0.1172} & \worst{0.1475} & 0.2124 & 0.3116 & \best{0.4342} & \best{0.5203} & \best{0.5581} \\
\midrule
\multicolumn{8}{l}{\textbf{ResNet}} \\
\midrule
Baseline (TopK SAE)& \worst{0.2295} & \worst{0.2484} & \worst{0.3055} & \worst{0.3203} & \worst{0.3301} & \worst{0.3014} & \worst{0.3125} \\
A-SAE ($\delta=0$) & 0.5920 & 0.5985 & 0.5992 & 0.6013 & 0.6029 & 0.6105 & 0.6133 \\
RA-SAE ($\delta=0.01$) & 0.5905 & 0.5985 & 0.5991 & 0.6013 & 0.6039 & 0.6106 & 0.6136 \\
RA-SAE ($\delta=0.1$) & 0.5777 & 0.5932 & 0.6002 & 0.6039 & 0.6046 & 0.6067 & 0.6083 \\
RA-SAE ($\delta=1.0$) & \best{0.6151} & \best{0.6165} & \best{0.6173} & \best{0.6189} & \best{0.6200} & \best{0.6208} & \best{0.6213} \\
\midrule
\multicolumn{8}{l}{\textbf{ViT}} \\
\midrule
Baseline (TopK SAE) & \worst{0.1317} & \worst{0.1589} & \worst{0.1984} & \worst{0.2265} & \worst{0.2595} & \worst{0.2807} & \worst{0.2939} \\
A-SAE ($\delta=0$) & 0.2079 & 0.2459 & 0.2820 & 0.3096 & 0.3361 & 0.3581 & 0.3721 \\
RA-SAE ($\delta=0.01$) & 0.2102 & 0.2490 & 0.2861 & 0.3132 & 0.3200 & 0.3642 & 0.3821 \\
RA-SAE ($\delta=0.1$) & \best{0.2278} & \best{0.2747} & \best{0.3153} & 0.3410 & 0.3496 & 0.3912 & 0.4103 \\
RA-SAE ($\delta=1.0$) & 0.1497 & 0.2123 & 0.2936 & \best{0.3624} & \best{0.4277} & \best{0.4786} & \best{0.5014} \\
\bottomrule
\end{tabular}
}
\caption{\textbf{Plausibility Benchmark Results.} The Plausibility Score measures the alignment between the learned dictionary concepts and the classification head's directions. \rasae~achieves a significantly higher score compared to a TopK SAE (baseline). Best scores are wrapped in \best{green}, and worst scores are wrapped in \worst{red}.}
\label{table:plausibility_benchmark}
\end{table}

\begin{figure*}
    \centering
    \includegraphics[width=0.95\linewidth]{assets/identifiability.jpg}
    \vspace{-0mm}
    \caption{\textbf{Soft Identifiability benchmark.} This example uses the ``gems'' dataset, part of the 12 identifiability benchmarks we introduce. The goal is to evaluate whether SAEs (or any dictionary learning method under study) can disentangle and recover each object from the hidden ground truth generative process. By analyzing the model's ability to assign distinct codes to the underlying concepts, we test its capacity to reconstruct and interpret the true structure of the data.}
    \label{fig:identifiability}
    \vspace{-0mm}
\end{figure*}


We evaluate SAEs' ability to recover true classification directions by assessing whether the learned dictionary $\D$ aligns with the classifier's final layer weights $\{\bm{v}_1, \dots, \bm{v}_c\}$, where $c$ is the number of classes.
Specifically, for each class vector $\bm{v}_i$, we compute the most aligned dictionary atom $\D_j$ and average the alignment score: $\text{Plausibility} = \frac{1}{c} \sum_{i=1}^{c} \max_{j} \langle \bm{v}_i, \D_j \rangle$. 
A score of 1 indicates perfect alignment, while a score of 0 implies that all concepts lie in the classifier's null space, making probing ineffective. 
This metric could help us detect potential hallucinations if concepts diverge too much from true classification directions, and is broadly similar to \citet{karvonen2024measuring, mayne2024can}'s evaluation of whether SAEs infer known linear features in toy settings.
Results are reported in Table~\ref{table:plausibility_benchmark}. 
We find that classical SAEs, even with extremely large dictionaries, achieve limited alignment with the classification directions (inline with prior work). %
In contrast, \rasae~significantly enhances this alignment, recovering a substantial portion of the true classification directions. 
This demonstrates \rasae's efficacy in producing semantically meaningful dictionaries.
\subsection{Soft Identifiability Benchmark}
\label{subsec:identifiability_benchmark}
Recent work on disentangled representation learning often evaluates whether an autoencoder trained to reconstruct samples from a data-generating process learns to represent its underlying concepts~\citep{locatello2019challenging, locatello2020weakly, von2021self, gresele2020incomplete, khemakhem2020variational, schott2021visual, zimmermann2021contrastive, menon2024analyzing}. 
Identifiability theorems on the topic~\citep{locatello2019challenging, locatello2020weakly} have however shown that unless the autoencoding architecture possesses the right inductive biases that match the generative process, there is no guarantee concepts underlying the data will map onto the autoencoder's latents. Since these results do not make assumptions about the data modality, they remain valid for a standard SAE training setup, e.g., similar to our experiments above. Then, an intriguing experiment involves evaluating whether when trained on representations of samples from a toy data-generating process with predefined concepts, \asae, or any other SAE architecture, develops latents capturing said concepts---if it does, then that is strongly suggestive of the SAE possessing the right inductive biases, i.e., it captures the mechanism via which a model encodes concepts in its representations.

Motivated by the above, we next propose a Soft Identifiability Benchmark that assesses the capability of SAEs to disentangle synthetic image mixtures with known underlying concepts. 
Specifically, we construct twelve synthetic datasets, each comprising images formed by collaging four distinct objects (e.g., different types of gems) sampled from a pre-defined set. 
Each dataset is processed through a pre-trained vision model to obtain pooled activations, which serve as inputs for the SAE.
Ideally, when trained on these activations, the SAE is able to recover the original objects as distinct concepts within its dictionary. 
To evaluate the same, we assess performance by checking whether each object class has a corresponding concept that activates above a threshold $\lambda$ when an object $y_j$ appears in the image. 
Formally, for each image, we feed it into a model and then into the trained SAE to get a concept-label pair $(\vec{z}, \vec{y})$, where $\vec{z} \in \R^k$ represents the $k$ concept values and $\vec{y} \in \R^c$ denotes the $c$ class labels. We then define the accuracy for the class $j$ as: $\text{Accuracy}_j = \max_{\lambda \in \R, i \in [k]} ~~\mathbb{P}_{(\vec{z}, \vec{y})}((z_i > \lambda) = y_j)$.

\begin{figure}[h]
\centering
\vspace{-2mm}
\includegraphics[width=0.48\textwidth]{assets/qualitative_surprising.png}
\vspace{-5mm}
\caption{\textbf{Exotic Concepts in DinoV2,} among the $16,000$ concepts found by \rasae, \textbf{A)} Highlights tokens in shadows of dogs, suggesting shadow-based features, potentially used for depth reasoning. \textbf{B)} A ``barber'' concept exclusively active for barbers, not their clients. \textbf{C)} A fine-grained visual concept focusing on petal edges or contours.}
\label{fig:surprising_main}
\vspace{-4mm}
\end{figure}

\begin{table}[t]
\vspace{-1mm}
\centering
\resizebox{0.99\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{DINOv2} & \textbf{ResNet} & \textbf{SigLIP} & \textbf{ViT} \\
\midrule
KMeans       & 0.7678 & 0.7624 & 0.7684 & 0.7702 \\
ICA          & 0.8092 & 0.8370 & 0.8243 & 0.8267 \\
Sparse PCA   & 0.7981 & 0.8318 & 0.8069 & 0.8082 \\
SVD          & 0.7979 & 0.8291 & 0.8062 & 0.8075 \\
SemiNMF      & 0.8297 & 0.8327 & 0.8358 & 0.8423 \\
ConvexNMF    & 0.7645 & 0.7582 & 0.7639 & 0.7634 \\
PCA          & 0.7979 & 0.8291 & 0.8062 & 0.8075 \\
Vanilla      & 0.8047 & 0.8167 & 0.8126 & 0.8223 \\
TopK         & 0.8135 & 0.8150 & 0.8289 & 0.8328 \\
Jump         & 0.8010 & 0.7988 & 0.8131 & 0.8053 \\
A-SAE        & \textbf{0.9482} & \textbf{0.9631} & \textbf{0.9602} & \textbf{0.9615} \\
RA-SAE       & \underline{0.9447} & \underline{0.9602} & \underline{0.9585} & \underline{0.9586} \\
\bottomrule
\end{tabular}
}
\vspace{-0mm}
\caption{\textbf{Soft Identifiability benchmark Across Models and Methods.} This table presents the average accuracy scores for various methods evaluated across four different models: DINO, ResNet, SigLIP, and ViT. Best-performing scores for each model are in \textbf{bold} and second best are \underline{underlined}. Full results are available in Appendix \ref{ap:identifiability}.}
\label{table:summary_accuracy_scores}
\vspace{-0mm}
\end{table}

Intuitively, the evaluation above gauges whether each object is disentangled into its ``own box'', in that a threshold $\lambda$ enables one-hot classification for all objects.
In each of the 12 synthetic datasets, the size of the dictionary is set to the exact number of distinct objects (from 9 to 20 objects) and we generate datasets of 2,000 samples. An example of the dataset and the procedure is given in Figure~\ref{fig:identifiability}.

Results are shown in Table~\ref{table:summary_accuracy_scores}. Classical dictionary learning methods and standard SAEs exhibit comparable performance, struggling to accurately recover the true generative factors. In contrast, \rasae~consistently achieves higher accuracy in recovering the underlying object classes across all synthetic datasets. This indicates that \rasae~not only stabilizes the dictionary, but also enhances its ability to discern the true underlying structure of the data, thereby improving the disentanglement of mixed objects.

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.99\textwidth]{./assets/qualitative_rabbit.jpg}
    \caption{\textbf{Top-5 Concepts for the Rabbit Class in DinoV2.} The RA-SAE on top of TopK identifies distinct and fine-grained concepts, including rabbit ears, body, face, and paws. These concepts exhibit greater structure and granularity compared to those found by the unconstrained TopK SAE (baseline) method.}
    \label{fig:rabbit_top_concepts}
\end{figure*}

\subsection{Qualitative Results}
\label{subsec:qualitative_results}

Beyond quantitative evaluations on synthetic datasets, we also provide qualitative results to illustrate the meaningfulness of the concepts learned by \rasae~on DinoV2 with natural images. \rasae~uncovers unexpected concepts, such as shadow-based features (potentially linked to depth reasoning), a context-dependent ``barber'' concept (activating for barbers but not clients), and fine-grained edge detection in flower petals (Figure~\ref{fig:surprising_main}). It also learns more structured within-class distinctions (e.g., separating rabbit ears, faces, and paws) compared to TopK-SAEs (Figure~\ref{fig:rabbit_top_concepts}). Finally, its dictionary forms clear clusters, grouping semantically related features like fine-grained animal faces or spatial concepts such as ``underneath'' (Figure~\ref{fig:umap_qualitative}).

\section{Conclusion}
\label{sec:conclusion}
We identified a fundamental instability in Sparse Autoencoders, where identical training runs can yield divergent dictionaries, limiting their reliability for interpretability. To address this, we introduced Archetypal SAEs (\asae), which constrain dictionary atoms to the convex hull of the data, significantly enhancing stability while preserving expressivity. We further proposed a relaxed variant, \rasae, which balances reconstruction quality with meaningful concept discovery in large-scale vision models.  
To rigorously assess these models, we developed novel evaluation metrics and benchmarks inspired by identifiability theory, providing a systematic framework for measuring dictionary quality and disentanglement. Our findings suggest that archetypal constraints not only stabilize SAEs but also improve the structure and plausibility of learned representations.  
Beyond vision, our approach lays the groundwork for more reliable concept discovery in broader modalities, including large language models and other structured data domains. 


\section*{Acknowledgments}
This work has been made possible in part by a gift from the Chan Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and Artificial Intelligence at Harvard University. MW acknowledges support from a Superalignment Fast Grant from OpenAI, Effective Ventures Foundation, Effektiv Spenden Schweiz, and the Open Philanthropy Project.

\newpage
\bibliography{xai,dictionary_learning}
\bibliographystyle{icml2024}
\clearpage








\appendix
\onecolumn

\section*{Appendix}

\section{Qualitative Examples}  
\label{ap:qualitative}

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\textwidth]{./assets/ra-sae_dino_details.jpg}
    \caption{\textbf{Performance, Auto-Correlation, and Firing Rate of DinoV2 for Archetypal SAE.} 
    The Archetypal SAE analyzed here demonstrates a well-structured dictionary with low auto-correlation, indicating that the dictionary atoms are not collinear. The firing rate exhibits a long-tail distribution, and reconstruction error is uniformly distributed across tokens, except for the \textsc{CLS} token, which shows the highest reconstruction error.}
    \label{fig:detail_rasae_dino}
\end{figure*}

This section provides qualitative insights into the concepts learned by DinoV2~\cite{oquab2023dinov2,darcet2023vision}. We trained an Archetypal SAE on DinoV2-B with 4 registers and reported key performance metrics in Figure~\ref{fig:detail_rasae_dino}. 

To explore the dictionary learned by the model, we analyzed three clusters of concepts, as illustrated in Figure~\ref{fig:umap_qualitative}. Specifically:  
\textcolor{red}{$\bullet$} A cluster of concepts capturing complex hand positions, ranging from hands in pockets to hands resting on another person (e.g., on a shoulder).  
\textcolor{yellow}{$\bullet$} An abstract ``under'' concept cluster, linking entities such as birds, zebras, felines, and airplanes, while highlighting the lower regions of objects.  
\textcolor{blue}{$\bullet$} A cluster representing fine-grained facial concepts in animals, including ears, eyebrows, and cheeks.

\begin{figure*}[h]
\vspace{-5mm}
    \centering
    \includegraphics[width=0.85\textwidth]{assets/rasae_umap_3_cluster.jpg}
    \caption{\textbf{Examples of 3 Concept Clusters in DinoV2.} Each cluster contains 4 example concepts. \textcolor{red}{$\bullet$} Complex hand positions, ranging from hands in pockets to hands on another person. \textcolor{yellow}{$\bullet$} Abstract ``under'' concepts, linking animals and objects, such as birds, zebras, felines, and airplanes, while focusing on lower regions. \textcolor{blue}{$\bullet$} Fine-grained animal facial features, including ears, eyebrows, and cheeks.}
    \label{fig:umap_qualitative}
    \vspace{-4mm}
\end{figure*}

In addition, we identified surprising and specific concepts among the $16,000$ dictionary atoms, shown in Figure~\ref{fig:surprising}. These include:  
\textbf{A)} A concept highlighting tokens corresponding to shadows of dogs, suggesting that DinoV2 may use shadows as a feature, potentially contributing to its depth estimation or 3D reasoning capabilities. 
\textbf{B)} A ``barber'' concept that activates exclusively on tokens representing barbers but not on the individuals receiving haircuts or shaves.  
\textbf{C)} A fine-grained visual concept that activates along the contours or edges of flower petals.

Finally, we present the top-5 concepts associated with the rabbit class on DinoV2 in Figure~\ref{fig:rabbit_top_concepts}. The concepts learned by RA-SAE are distinct and exhibit greater structure compared to their TopK counterparts. For instance, RA-SAE successfully identifies separate concepts for rabbit ears, body, face, and paws, demonstrating its ability to disentangle fine-grained features within a class. These results suggest that RA-SAE provides a more organized and meaningful decomposition of concepts.


\clearpage

\twocolumn[]
\section{Formal Definitions of Metrics}  
\label{ap:metrics}  

To comprehensively evaluate SAEs and their archetypal variants, we have defined in Sec.\ref{sec:metrics} a set of metrics that assess four key dimensions of the SAEs: (\textbf{\textit{i}}) sparse reconstruction, (\textbf{\textit{ii}}) consistency, (\textbf{\textit{iii}}) structure in the dictionary ($\D$), and (\textbf{\textit{iv}}) structure in the codes ($\Z$).  

\subsection{Sparse Reconstruction}  
As explained in the main paper, these metrics evaluate the ability of the model to accurately reconstruct activations while enforcing sparsity constraints. We believe they are well understood and already used by the interpretability community. The \textbf{Reconstruction Error ($R^2$)}  measures the fidelity of the reconstruction by quantifying how well the learned dictionary approximates the input activations:   
\begin{equation}
R^2 = 1 - \frac{||\A - \hat{\A}||_F^2}{||\A - \bar{\A}||_F^2}, %
\end{equation}  
with $\hat{\A} = \Z\D$ the predicted activation and $\bar{\A}$ the mean activation matrix. Essentially, $R^2$ measures how much we improve on explaining variance upon the best possible predictor that uses only a single bias. 
The \textbf{Dead Codes} measure the fraction of dictionary atoms that remain unused across the dataset, highlighting inefficiencies in the learned representation, for a set of $n$ codes and $k$ concepts $\Z \in \R^{n \times k}$:
\begin{equation}  
\text{Dead Codes} = 1-\frac{1}{k} ||\sum_i^n \Z_i||_0  
\end{equation}  

\subsection{Consistency}  
The second category of metrics assesses how consistent and well-grounded the learned dictionary is. Specifically, we evaluate (\textbf{i}) the \textbf{Stability} of the learned solution across training runs and (\textbf{ii}) its proximity to real data, measured via the \textbf{Out-of-Distribution (OOD) Score}.  

\textbf{Stability}, introduced in Eq.~\ref{eq:stability}, quantifies how consistent the learned dictionary remains when training is repeated with different initializations. Given two independently trained dictionaries, $\D$ and $\D'$, stability is defined as:  
\begin{equation}
\text{Stability}(\D, \D') = \max_{\bm{\Pi} \in \mathcal{P}(n)} \frac{1}{n} \text{Tr}(\D^{\tr} \bm{\Pi} \D'),
\end{equation}
where $\mathcal{P}(n)$ is the set of $n \times n$ signed permutation matrices. A score of $1$ indicates perfect alignment—each concept in $\D$ has a direct equivalent in $\D'$, while a score of $0$ implies that all concepts are seed-specific and change arbitrarily across runs.  

A looser measure of stability is the \textbf{Max Cosine Similarity}, which only considers the best-matching concept between two training runs:  
\begin{equation}  
\text{Max Cosine} = \max_{i,j} \langle \D_i, \D_j' \rangle.
\end{equation}  
This metric provides an upper bound on alignment but does not enforce global consistency across the dictionary.  

Beyond stability, we assess how well the learned dictionary aligns with real data. The \textbf{Out-of-Distribution (OOD) Score} measures the deviation of dictionary atoms from real data points by computing the cosine similarity between each dictionary atom $\D_i$ and its closest real activation $\A_j$:  
\begin{equation}  
\text{OOD Score} = 1 - \frac{1}{k} \sum_{i=1}^{k} \max_{j \in [n]} \langle \D_i, \A_j \rangle.
\end{equation}  
A score of $0$ indicates that every dictionary atom $\D_i$ exactly matches an existing data point $\A_j$, meaning the model purely reconstructs real activations. Notably, methods like Separable-NMF~\cite{gillis2020nonnegative}, which enforce the ``pure-pixel'' assumption by explicitly selecting dictionary atoms from the dataset, naturally achieve an OOD score of $0$.  

Together, these metrics provide a comprehensive evaluation of how stable, interpretable, and grounded the learned dictionary remains across training runs and relative to real data.  

\subsection{Structure in the Dictionary ($\D$)}  
The third category assesses the internal organization of the dictionary, providing insights into its effective dimensionality, redundancy, and (we hope) overall interpretability. Unlike reconstruction or consistency metrics, which evaluate external properties of the learned dictionary, these metrics focus on how well-formed and structured the set of concepts is. Again, a dictionary with lower effective dimensionality suggests structure like compositionality and/or hierarchical concepts. \textbf{Stable Rank} is the first metric we propose that provides an effective measure of the intrinsic dimensionality of the dictionary:  
\begin{equation}  
\text{Stable Rank} = \frac{||\D||_F^2}{||\D||_2^2}.  
\end{equation}  
Unlike the traditional matrix rank, which is sensitive to numerical precision (all dictionaries are nearly full rank in practice because of numerical error), the stable rank remains well-behaved even in high-dimensional settings, serving as a smooth proxy for rank estimation. \textbf{Effective Rank} offers an alternative perspective by measuring the entropy of the singular value distribution of $\D$:  
\begin{equation}  
\text{Eff. Rank} = \exp\left(- \sum_{i=1}^{k} \sigma_i \log \sigma_i \right),  
\end{equation}  
where $\sigma_i$ are the \textbf{normalized} singular values of $\D$ (i.e., $\sum_i \sigma_i = 1$). An effective rank close to $k$ suggests that all dictionary atoms are equally important, whereas a low effective rank implies that only a few dominant concepts capture most of the variation in the data. A fully orthogonal dictionary (thus not overcomplete) would achieve an effective rank of $k$. \textbf{Coherence} is a common notion in dictionary learning and compress sensing, it quantifies redundancy between dictionary atoms by measuring the maximum pairwise cosine similarity:  
\begin{equation}  
\text{Coherence} = \max_{i \neq j} |\D_i^\tr \D_j|.  
\end{equation}  
We still admit that each $\D_i$ is on the $\ell_2$ ball. Lower coherence indicates that dictionary atoms are more diverse and span independent directions, which is desirable for disentangled representations. Conversely, high coherence suggests that multiple dictionary atoms encode nearly identical features, reducing the efficiency of the learned basis. Notably, coherence is closely related to the concept of \textit{mutual incoherence} in compressed sensing~\cite{donoho2006compressed}, where low-coherence bases are preferred for sparse signal recovery.  

\subsection{(\textbf{\textit{iv}}) Structure in the Codes ($\Z$)}  
The last category measures how much structure we have in the codes. While dictionary structure (\textbf{\textit{iii}}) focuses on the learned basis $\D$, the structure of the codes $\Z$ determines how these dictionary atoms are used to reconstruct activations. A well-structured encoding should exhibit meaningful combinations of concepts while avoiding redundancy and destructive interference. \textbf{Connectivity} measures the diversity of concept usage by counting the number of \emph{unique co-activations} within the code matrix:  
\begin{equation}  
\text{Connectivity} = 1 - (\frac{1}{d^2} ||\Z^\tr\Z||_0).  
\end{equation}  
This metric quantifies how many distinct pairs of concepts $(i,j)$ are activated together across samples. A high connectivity score suggests that a broad range of concepts can be meaningfully combined, leading to more complex representations. Conversely, low connectivity implies a highly structured representation, and in some sense a group-sparsity representation where only a subset of concepts can fire together. We note that connectivity in sparse coding has been linked to compositionality~\cite{olshausen1996emergence}. We believe that none of the SAEs currently studied achieve interesting performance on this metric. Finally, \textbf{Negative Interference} quantifies the extent to which co-activated concepts cancel each other out, reducing their effectiveness:  
\begin{equation}  
\text{Neg. Inter.} = ||\text{ReLU}(-(\Z^\tr \Z) \odot (\D \D^\tr))||_2.  
\end{equation}  
Where $\odot$ is the Hadamard product, this metric captures cases where two concepts $i$ and $j$ frequently co-activate (as measured by $\Z^\tr \Z$), yet their dictionary atoms are negatively correlated (indicated by a negative dot product in $\D \D^\tr$). The ReLU function ensures that only destructive interactions are counted, where activation of both concepts leads to mutual cancellation rather than constructive combination.  A high negative interference score suggests that the learned dictionary contains redundant or antagonistic concepts. In extreme cases, we could imagine a strong negative interference can lead to concept pairs that consistently suppress each other to comply with some sparsity constraint.


\clearpage

\section{Distilling $\A$ into $\C$}
\label{ap:distillation}

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{assets/distillation.jpg}
    \caption{\textbf{Comparison of Distillation Methods.} To scale up the Archetypal SAE, it is impractical to utilize the entire data matrix $\mathbf{A}$ for identifying archetypes. Instead, we first reduce the dataset to a smaller subset of points, denoted as $\mathbf{C}$, and construct the archetypes/dictionary elements from this reduced set. Among the distillation methods evaluated, K-Means proves to be the most effective approach, generating points within the convex hull of the data and achieving strong performance scores. These experiments were conducted on DINOv2 using an Archetypal SAE without relaxation.}
    \label{fig:distillation}
\end{figure}

As a recall, Archetypal SAEs construct dictionary atoms as convex combinations of data points,  
\begin{equation}
\D = \W \A, \quad \text{with} \quad \W \in \Omega_{k, n}, \quad \A \in \R^{n \times d},
\end{equation}  
requiring access to the full data matrix $\A$. However, the original problem is intractable, as the number of points $n$ requires storing and processing millions of activations at each gradient step, which is computationally prohibitive, particularly for large-scale datasets of tokens. To address this, we proposed in Sec.~\ref{sec:archetypal} a \textit{distillation} step, reducing $\A$ to a compact subset $\C$. The dictionary $\D$ is then formed using only $\C \in \R^{n' \times d}$, with $n' \ll n$, ensuring a tractable optimization while remaining representative of the original distribution.  

We investigated five different methods to distill $\A$ into $\C$, as illustrated in Fig.~\ref{fig:distillation}:  
\begin{itemize}  
    \item \textbf{K-Means}: Groups data into $m$ clusters and selects centroids as representatives. This ensures that $\C$ remains within the convex hull of $\A$ while capturing its most frequent patterns.  
    \item \textbf{Local Outlier Factor (LOF)}~\cite{breunig2000lof}: Identifies statistically atypical points compared to their neighbors, helping remove rare or extreme cases.  
    \item \textbf{Isolation Forest (Iso)}~\cite{liu2008isolation}: Uses recursive partitioning to isolate anomalies, providing an efficient method for detecting outliers.  
    \item \textbf{Convex Hull on Reduced Dimensions}: Computes the convex hull of $\A$ after projecting it onto its 10 principal components via PCA. This ensures that extreme points defining the overall shape of the distribution are retained while reducing computational complexity.  
    \item \textbf{One-Class SVM (OC-SVM)}~\cite{scholkopf1999support}: A support vector method that learns a boundary around high-density regions, effectively isolating representative points while filtering outliers.  
\end{itemize}  

Among these, \textbf{K-Means emerges as the most effective method}. Furthermore, while convex hull approaches theoretically guarantee coverage of extreme points, their computational cost scales poorly with high dimensions, making them impractical for large datasets. 


\section{Implementation Details}  
\label{ap:implementation}  

We provide an efficient PyTorch implementation of the relaxed dictionary learning module used in Archetypal SAEs. Given a distilled set of centroids $\C \in \mathbb{R}^{n' \times d}$, our goal is to construct a dictionary $\D$ as a convex combination of elements in $\C$, while allowing a controlled degree of relaxation via additive perturbations.  

\begin{itemize}[label=-, topsep=0pt, partopsep=0pt, itemsep=0pt, parsep=0pt]
\item The weight matrix $\W \in \mathbb{R}^{k \times n'}$ is constrained to the probability simplex, ensuring convex combinations of centroids. This is enforced via a \texttt{convex\_param} function that projects $\W$ onto the simplex using ReLU and row-wise normalization.  
\item The relaxation term $\bm{\delta} \in \mathbb{R}^{k \times d}$ allows mild deviations from strict convexity. To prevent excessive drift, $\bm{\delta}$ is regularized by adaptively scaling its norm to remain within a relaxation factor.  
\item Dictionary atoms are computed as $\D = \W \C + \bm{\delta}$.  
\item The model supports gradient updates for $\W$ and $\bm{\delta}$ while keeping $\C$ fixed.
\end{itemize}

\begin{figure*}[h]
\centering
\noindent\begin{minipage}{1.0\textwidth}
\begin{RoundedListing}
class ArchetypalDictionary(nn.Module):
  """Relaxed Archetypal SAE (RA-SAE) dictionary.

  Constructs a dictionary where each atom is a convex combination of data 
  points from C, with a small relaxation term $\Lambda$ constrained by $\delta$.
  """

  def __init__(self, C, k, delta=1.0):
    """
    Parameters
    ----------
    C : Tensor
        Candidate archetypes of shape (n', d).
    k : int
        Number of dictionary atoms.
    delta : float
        Upper bound on the norm of the relaxation term $\Lambda$.
    """
    super().__init__()
    n_prime, d = C.shape
    self.register_buffer("C", C)  # store C as a fixed buffer (non-trainable)
    self.W = nn.Parameter(torch.eye(k, n_prime))  # trainable param (row-stochastic)
    self.Lambda = nn.Parameter(torch.zeros(k, d))  # small relaxation term
    self.delta = delta  # constraint on the relaxation term norm

  def forward(self, Z):
    """
    Parameters
    ----------
    Z : Tensor
        Sparse codes of shape (n, k).

    Returns
    -------
    Tensor
        Reconstructed activations of shape (n, d).
    """
    with torch.no_grad():
      # ensure W remains row-stochastic (positive and row sum to one)
      W = torch.relu(self.W)  
      W /= W.sum(dim=-1, keepdim=True) 
      self.W.data = W

      # enforce the norm constraint on $\Lambda$ to limit deviation from conv(C)
      norm_Lambda = self.Lambda.norm(dim=-1, keepdim=True)  # norm per row
      scaling_factor = torch.clamp(self.delta / norm_Lambda, max=1)  # safe scaling factor
      self.Lambda *= scaling_factor  # scale $\Lambda$ to satisfy $||\Lambda|| \leq \delta$

    # compute the dictionary as a convex combination plus relaxation
    D = self.W @ self.C + self.Lambda

    return Z @ D
\end{RoundedListing}
\end{minipage}
\caption{\textbf{Detailed Pytorch code for Relaxexd Archetypal SAE (\rasae).}}
\label{code:archetypal_details}
\end{figure*}
\clearpage



\section{Theoretical properties of Archetypal Dictionary}

In this section, we provide theoretical insights into Archetypal Sparse Autoencoders (A-SAEs) by addressing three key aspects: (\textbf{\textit{i}}) why standard SAEs can produce dictionaries that drift away from the data manifold, (\textbf{\textit{ii}}) a geometric interpretation of the A-SAE solution and the conditions under which distillation is optimal, and (\textbf{\textit{iii}}) bounds on the stability, rank, and out-of-distribution (OOD) score of A-SAEs.

A simple explanation for why standard SAEs may drift away from the data manifold can be found by examining the gradient descent (GD) update rule for the dictionary $\D$. Given a dataset $\A \in \R^{n \times d}$, nonnegative codes $\Z \geq 0 \in \R^{n \times k}$, and a dictionary $\D \in \R^{k \times d}$ with unit-norm rows ($\|\D_j\|_2 = 1$), the standard SAE optimization problem is:
\begin{equation}
\min_{\Z,\D} \|\A - \Z \D^\tr\|_F^2 \quad \text{s.t.} \quad \Z \geq 0, \quad \|\D_j\|_2 = 1.
\end{equation}
The gradient of the reconstruction loss with respect to $\D$ is given by:
\begin{equation}
\nabla_{\D} \|\A - \Z \D^\tr\|_F^2 = 2(\Z^\tr \Z \D - \Z^\tr \A).
\end{equation}
This gradient can be decomposed into two components:
\begin{align*}
\Delta \D &= \underbrace{\Z^\tr \A}_{\text{data-anchored term}} - \underbrace{\Z^\tr \Z \D}_{\text{out-of-data term}}.
\end{align*}
The first term, $\Z^\tr \A$, pulls dictionary atoms toward a conic combination of data points, anchoring them to $\mathrm{cone}(\A)$. However, the second term, $\Z^\tr \Z \D$, introduces a drift effect that pushes the dictionary away from the data, as it depends on the correlations within $\Z$ and the original seed $\D$. 

Empirically, in high-dimensional settings where the codes $\Z$ exhibit sufficient variability could induce the second term to dominate. This could explain why classical SAEs have a relatively low OOD score, as shown in Sec.~\ref{table:sae_metrics} and why minor perturbations in initialization or the training set can potentially yield different dictionaries, as the second term is entirely dependent of the seed.

\subsection{Geometric interpretation of A-SAE}

\begin{proposition}[Archetypal Dictionary, Convex and Conic Hulls]
\label{prop:archetypal-convex-conic}
Given $\A \in \R^{n \times d}$ as a set of $n$ data points and $\W \in \Omega_{k,n}$ as any row-stochastic matrix, parameterizing $\D = \W \A$ ensures that each concept $\D_i$ lies within the convex hull of the data, i.e., $\D_i \in \mathrm{conv}(\A)$ for all $i \in [k]$. Moreover, for any nonnegative codes $\Z \geq 0$, the reconstruction $\Z \D$ lies within the conic hull of the data, i.e., $\Z \D \subseteq \mathrm{cone}(\A)$. More generally, let $\C \in \R^{n' \times d}$ such that $\mathrm{conv}(\C) \subseteq \mathrm{conv}(\A)$ be a subset of $\A$ or any set of points within $\mathrm{conv}(\A)$. Then $\D' = \W \C$ satisfies $\D'_i \in \mathrm{conv}(\C) \subseteq \mathrm{conv}(\A)$ and $\Z \D' \subseteq \mathrm{cone}(\A)$. Finally, if $\C$ includes the extreme points of $\A$, then no representational power is lost.
\end{proposition}

\begin{proof}
Since $\W$ is row-stochastic, each $\D_i = \W_i \A$ is a convex combination of the rows of $\A$, i.e., $\D_i \in \mathrm{conv}(\A)$. Furthermore, for nonnegative $\Z$, we have $\Z \D = (\Z \W) \A$, with $\Z \W \geq 0$, which implies that $\Z \D \subseteq \mathrm{cone}(\A)$. With $\D' = \W \C$ for $\mathrm{conv}(\C) \subseteq \mathrm{conv}(\A)$, each row $\D'_i$ lies within $\mathrm{conv}(\C) \subseteq \mathrm{conv}(\A)$. Lastly, if $\C$ contains the extreme points~\cite{boyd2004convex} of $\A$, then by simple application of the Krein-Milman theorem \cite{lax2014functional}, $\mathrm{conv}(\C) = \mathrm{conv}(\A)$ and $\mathrm{cone}(\Z \W \C) = \mathrm{cone}(\A)$, ensuring no loss in expressivity.
\end{proof}

The constraints imposed by \asae~on $\D$ lead to a straightforward yet crucial geometric property: each dictionary atom $\D_i$ remains within the convex hull of the data.
This result, while simple, has interesting implications for some of the metrics we study, notably stability and OOD.
In fact, we will now see that this implies a bounded OOD score, prevents the rank of the dictionary (and thus its structure) to higher than the rank of the data, and induces some loose form of algorithmic stability.

\subsection{Stability of Archetypal Dictionary}

\begin{proposition}[Geometric Stability of Archetypal Dictionaries]
\label{prop:stability}
Let $\A,\A' \in \mathbb{R}^{n \times d}$ be two data matrices such that 
$||\A - \A'||_{F} \le \varepsilon$. 
Suppose $\W,\W' \in \Omega_{k,n}$ are row-stochastic matrices (i.e.\ each row 
belongs to the probability simplex). 
Define the archetypal dictionaries $\D = \W \A$ and $\D' = \W' \A'$. 
Then,
\begin{align*}
||\D - \D'||_{F} \le \sqrt{k}\, \varepsilon + 2\sqrt{k}\, \min\left(||\A||_{F}, ||\A'||_{F}\right).
\end{align*}
\end{proposition}

\begin{proof}[Proof]
Using triangle inequality, we have
\begin{align*}
||\D - \D'||_F = ||\W\A - \W'\A'||_F \le &||\W\A - \W\A'||_F ~+~ \\ &||\W\A' - \W'\A'||_F.
\end{align*}
We bound each term separately, since $\D$ and $\D'$ are archetypal dictionaries, $\W$ is row-stochastic, hence $||\W||_{2} \le \sqrt{k}$. Therefore,
\begin{align*}
||\W\A - \W\A'||_F = ||\W(\A - \A')||_F& \le \\
&||\W||_{2}\, ||\A - \A'||_{F},
\end{align*}
and $||\W||_{2}\,||\A - \A'||_{F} \leq \sqrt{k}\, \varepsilon$.
 For the second term,
\begin{align*}
||\W\A' - \W'\A'||_F = ||(\W - \W')\,\A'&||_F \le \\ 
&||\W - \W'||_F\, ||\A'||_{F},
\end{align*}
with $||\W - \W'||_F \leq 2\sqrt{k}$ summing these yield:
$$
||\D - \D'||_F \le \sqrt{k}\, \varepsilon +  2\sqrt{k}\, ||\A'||_{F}.
$$
It is straightforward to repeat the above process with a $||\A'||_{F}$ factor in the right hand side. Taking the minimum over the two factors then completes the proof.
\end{proof}

The key observation is that a row-stochastic matrix $\W$ cannot 
stretch data arbitrarily, imposing a natural control on how $\D = \W\A$ 
changes if $\A$ is slightly perturbed. 
By contrast, in the unconstrained setting, where the dictionary $\D$ is free to move anywhere (subject only to norms or regularization), there is no comparably simple bound ensuring stability. Even small perturbations in $\A$ (or in the random seed, initialization, etc.) can shift the solution significantly: as there is no requirement that $\D$ stay close to \(\mathrm{conv}(\A)\), the learned atoms can drift to entirely different regions of the space.

\subsection{Controlled Rank and Stability in A-SAE Dictionaries}

We now show that the rank of the dictionary obtained using the Archetypal parametrization are inherently controlled. Specifically, the rank of the dictionary cannot exceed the rank of the data. This property is interesting as it prevents the dictionary from becoming arbitrarily complex, promoting solutions that are more structured and aligned with the data. Consequently, the dictionary is more likely to uncover meta-concepts or low-rank representations.

\begin{proposition}[Rank Bound of Archetypal Dictionaries]
\label{prop:rank}
Let $\A \in \mathbb{R}^{n \times d}$ be a data matrix with $\mathrm{rank}(\A) = r \leq d$, assuming $n \gg d$ and $k \gg d$. Let $\W \in \Omega_{k,n}$ be a row-stochastic matrix, and define the dictionary as $\D = \W \A$.
Then, the rank of $\D$ is bounded by the rank of the data:
\[
\mathrm{rank}(\D) \leq \min(\mathrm{rank}(\A), \mathrm{rank}(\W)) \leq d.
\]
\end{proposition}

\begin{proof}
Since $\D = \W \A$, the column space of $\D$ is contained in the column space of $\A$, implying
\[
\mathrm{rank}(\D) \leq \mathrm{rank}(\A) = r.
\]
Additionally, since $\W \in \mathbb{R}^{k \times n}$ is row-stochastic, its rank is at most \( \min(k, n) \), giving
\[
\mathrm{rank}(\W) \leq \min(k, n).
\]
it follows that
\[
\mathrm{rank}(\D) = \mathrm{rank}(\W \A) \leq \min(r, \mathrm{rank}(\W)).
\]
\end{proof}

\subsection{Bounding OOD score with Archetypal Constraints}

We now demonstrate that the OOD measure of dictionary atoms obtained under some assumption is inherently lower-bounded. Specifically, the measure is directly tied to the weights in the row-stochastic matrix, with a maximum value of 1 achieved when a dictionary atom perfectly aligns with a data point. This property is particularly interesting as it ensures that dictionary atoms remain well-grounded in the data. Furthermore, the sparsity of the weight matrix plays a crucial role in maintaining orthogonality, thereby preserving the plausibility of the assumption and the robustness of the bounds.

\begin{proposition}[OOD Measure with Non-Interfering Archetypes]
\label{prop:ood}
Let $\A \in \mathbb{R}^{n \times d}$ be our data matrix where each point $\A_i$ is normalized ($\|\A_i\|_2 = 1$ for all $i \in [n]$). Let our archetypal dictionary $\D = \W \A$, where $\W \in \mathbb{R}^{k \times n}$ is a row-stochastic matrix ($\W \in \Omega_{k,n}$). We assume non-interfering Archetypes, meaning two non-orthogonal archetypes cannot be active at the same time (but can exist in the bank of points $\A$). Formally, for each row $\W_i$, the active rows of $\A$ (those $\A_j$ with $\W_{ij} > 0$) are orthogonal, i.e.,
\[
\langle \A_j, \A_{j'} \rangle = 0 \quad \text{for } j \neq j' \text{ and } \W_{ij}, \W_{ij'} > 0.
\]
Then, the out-of-distribution (OOD) measure for each $\D_i$ admits the upper bound:
\[
\mathrm{OOD}(\D_i) \leq 1-\max_{j \in [n]} \W_{ij}.
\]
\end{proposition}

\begin{proof}
By definition, $\D_i = \sum_{j=1}^n \W_{ij} \A_j$, so
\[
\langle \D_i, \A_j \rangle = \left\langle \sum_{k=1}^n \W_{ik} \A_k, \A_j \right\rangle = \sum_{k=1}^n \W_{ik} \langle \A_k, \A_j \rangle.
\]
Under the orthogonality assumption $\langle \A_k, \A_j \rangle = 0$ for $k \neq j$, only the $k = j$ term remains:
\[
\langle \D_i, \A_j \rangle = \W_{ij} \langle \A_j, \A_j \rangle.
\]
Since $\|\A_j\|_2 = 1$, we have $\langle \A_j, \A_j \rangle = 1$, so:
\[
\langle \D_i, \A_j \rangle = \W_{ij}.
\]

By definition:
\[
\|\D_i\|_2^2 = \left\| \sum_{j=1}^n \W_{ij} \A_j \right\|_2^2.
\]
By the orthogonality of the active rows of $\A$, the contributions of different rows do not interfere, so:
\[
\|\D_i\|_2^2 = \sum_{j=1}^n \W_{ij}^2 \|\A_j\|_2^2.
\]
Since $\|\A_j\|_2 = 1$, this simplifies to:
\[
\|\D_i\|_2^2 = \sum_{j=1}^n \W_{ij}^2.
\]

Substituting these yields our bound:
\[
\mathrm{OOD}(\D_i) = 1 -\max_{j \in [n]} \frac{\langle \D_i, \A_j \rangle}{\|\D_i\|_2} \leq 1- \max_{j \in [n]} \W_{ij}.
\]
\end{proof}

As a notable special case, we observe that $\mathrm{OOD}(\D_i) = 0$ when $\W_{ij} = 1$ for some $j \in [n]$, as the dictionary atom aligns perfectly with a data point. In practice, the sparsity of $\W$ plays a crucial role: it limits interference between (non-orthogonal) components, ensuring that the orthogonality assumption remains plausible and that the derived bounds hold robustly.

\section{Kernel for JumpReLU}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{assets/jumprelu_kernels.png}
    \caption{Example of kernel functions for JumpReLU over the interval $[-3, 3]$ with a bandwidth of 1. The Silverman kernel produced more stable and positive results, leading to its selection for all experiments with a smaller bandwidth of $10^{-2}$.}
    \label{fig:jumprelu_kernels}
\end{figure}

JumpReLU~\cite{rajamanoharan2024jumping} is a recently introduced activation mechanism for SAEs designed to optimize $\ell_0$ sparsity by controlling the discontinuities of ReLU through a parameter $\theta$. Its optimization relies on a kernel for density estimation. To assess the effect of kernel choice, we evaluated several options, including Gaussian, Cauchy, and Silverman, on DinoV2. As shown in Figure~\ref{fig:jumprelu_kernels}, the Silverman kernel consistently yielded the most stable and accurate reconstruction results. For our experiments, we selected the Silverman kernel with a bandwidth of $10^{-2}$, although the choice of kernel appears to have only a modest impact on performance.


\section{Soft Identifiability Benchmark}
\label{ap:identifiability}

In this appendix, we provide additional details regarding the experimental setup and evaluation criteria used in the Soft Identifiability Benchmark. We recall that the goal is to assess the ability of SAEs to recover distinct concepts from synthetic image mixtures, where the underlying generative factors are known.

We generate twelve synthetic datasets using Midjourney API\footnote{www.midjourney.com}. For each of these datasets, we programmatically create 4,000 images. These images are constructed by collaging four distinct objects selected from the predefined set, such as different types of gems. Each dataset is generated from between 9 and 20 unique objects, with the dictionary size set exactly to the number of true generative factors (unique object number). An example of some datasets used in the benchmark is shown in Figure~\ref{ap:fig:identifiability}.

Each dataset is split into a training set of 2,000 images and a test set of 2,000 images. The images are processed through a pre-trained vision model, in our case DinoV2, ResNet50, SigLIP and ViT. The resulting pooled activations serve as the input representations for the SAE. 

\begin{figure*}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{assets/identifiablity_datasets.jpg}
    \caption{Examples of synthetic datasets used in the Identifiability Benchmark.}
    \label{ap:fig:identifiability}
\end{figure*}


\paragraph{Metrics.} To quantitatively evaluate identifiability, we define an accuracy metric that measures whether each object class in the dataset is correctly assigned a distinct concept in the SAE dictionary. Given an image, we pass it through the vision model and then through the trained SAE to obtain a concept-label pair $(\vec{z}, \vec{y})$, where $\vec{z} \in \mathbb{R}^k$ represents the $k$ learned concept activations, and $\vec{y} \in \mathbb{R}^c$ denotes the $c$ ground-truth class labels.

We define the accuracy for class $j$ as:
\begin{equation}
    \text{Accuracy}_j = \max_{\lambda \in \mathbb{R}, i \in [k]} \mathbb{P}_{(\vec{z}, \vec{y})}((z_i > \lambda) = y_j),
\end{equation}
where $\lambda$ is a threshold determining whether a concept is activated.

To find an appropriate $\lambda$, we use the empirical percentiles of the concept activations $\vec{Z}$, ranging from the 1st to the 100th percentile. This ensures that the threshold is adaptive to the distribution of activations, optimizing for the best classification accuracy.

\subsection{Complete Results}
The full set of results across all methods and datasets is provided in Table~\ref{table:accuracy_scores}. We also provide a comprehensive breakdown, including per-dataset accuracy scores and additional analysis.

\input{assets/identifiability_full}


\end{document}
