\begin{table*}[t]
    \small
    \centering
    \caption{Comparison with pre-training methods. All models are \textbf{pre-trained} on image-caption data and tested on VG150~\cite{krishna2017visual} test set directly. 
    Our models trained on COCO captions are used as pre-trained models for OvR-SGG and OvD+R-SGG settings.
    }    
    \setlength\tabcolsep{5.3pt}
    \scalebox{0.94}{
    \begin{tabular}{|rl||cc|ccc|}
    \thickhline
    \rowcolor{mygray}
    \multicolumn{2}{|c||}{SGG model} &  Backbone & Grounding & R@20  & R@50  & R@100  \\
    \hline
    \hline
    LSWS~\cite{yelinguistic} &$_\text{CVPR'21}$& -& - & - & 3.28 & 3.69 \\ 
    MOTIFS~\cite{zellers2018neural}&$_\text{CVPR'18}$& - & Li \etal~\cite{li2022integrating} & 5.02 & 6.40 & 7.33 \\
    Uniter~\cite{chen2020uniter}&$_\text{ECCV'20}$& - & SGNLS \cite{zhong2021learning} & - & 5.80 & 6.70 \\
    Uniter~\cite{chen2020uniter}&$_\text{ECCV'20}$ & - & Li \etal  \cite{li2022integrating} & 5.42 & 6.74 & 7.62 \\
    \hline
    $\text{VS}^3$~\cite{zhang2023learning} &$_\text{CVPR'23}$&  \multirow{3}[0]{*}{Swin-T} & GLIP-L \cite{li2022grounded}  & 5.59 & 7.30 & 8.62 \\
    OvSGTR~\cite{chen2024expanding}& $_\text{ECCV'24}$ & & Grounding DINO \cite{liu2023grounding} & {6.61} &{8.92} &{10.90} \\ 
     \textbf{INOVA} (\textbf{Ours}) & & & Grounding DINO \cite{liu2023grounding} & \textbf{7.86} &\textbf{10.81} &\textbf{13.31} \\ 
     \hline
    OvSGTR~\cite{chen2024expanding}& $_\text{ECCV'24}$ & \multirow{2}[0]{*}{Swin-B} & Grounding DINO \cite{liu2023grounding} & {6.88} & {9.30} & {11.48} \\
     \textbf{INOVA} (\textbf{Ours})& & & Grounding DINO \cite{liu2023grounding} & \textbf{8.28} & \textbf{11.61} &\textbf{14.33} \\
    \thickhline
    \end{tabular}
    }
    \vspace{-0.5em}
    \label{tab:pretrain}
\end{table*}