\section{Introduction}
Scene graph generation~\cite{xu2017scene} (SGG) aims to map an image into a structured semantic representation, where objects are expressed as nodes and their relationships are as edges within the graph. Recently, with the burgeoning of large-scale models, \eg, vision-language models (VLMs) and multimodal large language models (MLLMs), open vocabulary SGG~\cite{he2022towards,li2024pixels,chen2024expanding} (OVSGG) has emerged as a promising area. It pushes beyond predefined categories to support the recognition and generation of novel objects and relationships, holding great potential for real-world applications.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{figures/intro_v3.pdf}
    \vspace{-2em}
    \caption{ Overview of the OVSGG framework challenges.
1) VLM Pre-training, using solely entity categories for object detection causes ambiguity in associating object pairs (\eg, identifying the correct ``\texttt{man}-\texttt{surfboard}'' for the ``\texttt{hold}'').
2) SFT, bipartite graph matching misaligns non-interacting objects (\eg, ``\texttt{man}\includegraphics[scale=0.4]{figures/man1.png}'') with interacting target ``\texttt{man}'' in $\langle$\texttt{man}, \texttt{riding}, \texttt{horse}$\rangle$.}
    \label{fig:intro}
\vspace{-1em}
\end{figure}

Generally, an end-to-end VLM-based\footnote{We primarily discuss VLM-based models here due to the high resource demands of MLLM-based approaches.} OVSGG pipeline consists of two stages: \textbf{VLM Pre-training} and \textbf{Supervised Fine-Tuning (SFT)}. The \textbf{former} involves pre-training a VLM on large-scale datasets to realize a visual-concept alignment by comparing the given caption and visual regions. Specifically, due to the lack of region-level information (\eg, bounding box annotations), recent work~\cite{he2022towards, zhang2023learning, chen2024expanding} adopts a weakly-supervised strategy to generate $\langle$\texttt{subject}, \texttt{predicate}, \texttt{object}$\rangle$ triplets with bounding boxes as pseudo supervisions. As displayed in Figure~\ref{fig:intro}(a), this approach extracts semantic graphs from image captions using SGG parsers~\cite{schuster2015generating}, then grounds objects in the graphs with pre-trained object detectors (\eg, Faster R-CNN~\cite{ren2015faster}, GLIP~\cite{li2022grounded} and Grounding DINO~\cite{liu2023grounding}). The \textbf{latter} stage refines the model's performance on task-specific objectives by leveraging high-quality annotations. Concretely, it fine-tunes part of VLM's parameters~\cite{chen2024expanding} or adapts prompt-tuning~\cite{he2022towards} on SGG dataset with fully-supervised triplet annotations. Leveraging these bounding box annotations, a DETR-like structure~\cite{carion2020end} with bipartite graph matching is typically used to align predicted entities with ground-truth labels. This stage further enhances the model's capability to recognize and generate precise scene graphs (\cf Figure~\ref{fig:intro}(b)).

Despite impressive, existing OVSGG methods often treat all objects \textit{equally}, ignoring the distinct characteristics of the \textbf{interacting objects}. By \textit{equally}, we mean the lack of differentiation between instances within the same category. For example, the \texttt{man} involved in a holding action and the \texttt{man} without any action are represented in an indistinguishable manner. It can lead to \textbf{\emph{mismatches in relation pairs}} during both pre-training and SFT stages, which induces the following drawbacks: \hypertarget{Q1}{\ding{182}} \textit{Bringing noisy supervision in pre-training}. As illustrated in Figure~\ref{fig:intro}(a), relying solely on entity categories (\eg, \texttt{man} and \texttt{surfboard}) to detect objects generates a large number of candidate pairs. This ambiguity makes it hard to associate relation (\eg, ``\texttt{hold}'') to the proper object pair (\eg, ``\texttt{man}-\texttt{surfboard}''). Using mismatched triplets (\eg, \texttt{man} in red and \texttt{surfboard} in pink) further exacerbates the confusion, hindering the training of robust SGG models. \myhyperlink{Q2}{\ding{183}} \textit{Leading mismatched bipartite graph during SFT}. In Figure~\ref{fig:intro}(b), a non-interacting ``\texttt{man}\includegraphics[scale=0.4]{figures/man1.png}'' can be mistakenly associated with \texttt{man} in the triplet annotation $\langle$\texttt{man}, \texttt{riding}, \texttt{horse}$\rangle$. However, the real target is another ``\texttt{man}~\scalebox{-1}[1]{\includegraphics[scale=0.4]{figures/man2.png}}'' engaged in riding. This mismatch further complicates the relation classification task, making it harder to predict correct interactions.


In this paper, we take a closer look at interacting objects in each stage, and propose the \underline{\textbf{IN}}teraction-aware \underline{\textbf{O}}pen-\underline{\textbf{V}}oc\underline{\textbf{A}}bulary SGG framework (\textbf{INOVA}).  INOVA follows a dual-encoder-single-decoder architecture~\cite{liu2023grounding}, comprising three key components: the visual and text encoders, the cross-modality decoder, and the entity and relation classifiers. During the VLM pre-training stage, INOVA introduces an \textbf{interaction-aware target generation} strategy that employs bidirectional interaction prompts to guide the grounding of interacting object pairs. These prompts incorporate interaction tokens that capture contextual dependencies and relational semantics, enabling the model to distinguish interacting objects from non-interacting ones through the attention mechanism~\cite{vaswani2017attention}. For the SFT stage, we devise a two-step \textbf{interaction-guided query selection} mechanism to prioritize interacting objects and incorporate relational context into the query selection process. This mechanism mitigates the interference of inactive objects and reduces mismatches in bipartite graph matching, ensuring robust relation prediction. Additionally, to distinguish interacting objects (engaged in both seen and unseen triplets) from the background and address the challenge of catastrophic knowledge forgetting~\cite{chen2024expanding} during SFT, we adopt an \textbf{interaction-consistent knowledge distillation} (KD). It utilizes a teacher model pre-trained on image-caption data to guide the student model in preserving both point-wise semantic alignment and inter-pair relational consistency. By explicitly modeling the relative dependencies between interaction-based and non-interaction pairs, it enhances the model's robustness in handling novel triplet combinations and background. 

To evaluate INOVA, we conducted comprehensive experiments on benchmark Visual Genome (VG)~\citep{krishna2017visual} and GQA~\citep{hudson2019gqa} datasets to validate its effectiveness in addressing the key challenges of OVSGG. In summary, our contributions are threefold:
% \vspace{-0.5em}
\begin{itemize}[itemsep=0pt, parsep=0pt, topsep=0pt, partopsep=0pt]
\item We reveal key limitations in existing OVSGG frameworks, \ie, treating all objects \textit{equally}, which neglects the distinct characteristics of interacting objects and results in mismatched relation pairs.
\item We propose the INOVA framework that incorporates interaction-aware target generation, interaction-guided query selection, and interaction-consistent KD to pay attention to interacting objects, alleviating mismatched relation pairs and interference of irrelevant objects.
\item Extensive experiments on two prevalent SGG benchmarks demonstrate the effectiveness of INOVA.
\end{itemize}




