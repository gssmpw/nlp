\section{Related Work}

% \subsection{Scene Graph Generation (SGG)}

% \subsection{Open Vocabulary Scene Graph Generation}
\textbf{OVSGG.}
This task bridges the gap between closed-set SGG and real-world requirements by leveraging VLMs or MLLMs to generalize beyond predefined categories~\cite{radford2021learning, liu2023grounding}. Current approaches fall into two main categories:
1) \textit{VLM-based Methods.} These approaches primarily rely on contrastive pre-training to align visual and textual embeddings. By comparing visual features of unseen objects or relations and their semantic counterparts in common semantics spaces, these models (\eg, CLIP~\cite{radford2021learning} and Grounding DINO~\cite{liu2023grounding}) enables zero-shot generalization. Recent advancements, such as He \etal~\cite{he2022towards}, explore visual-relation pre-training and prompt-based fine-tuning for OVSGG. Yu \etal~\cite{yu2023visually} leverage CLIP to align relational semantics in multimodal spaces, while Chen \etal~\cite{chen2024expanding} use a student-teacher framework to improve open-set relation prediction. Besides, other methods integrate category descriptions~\cite{li2024zero} or scene-level descriptions~\cite{chen2024scene} to enrich the semantic context and improve the discrimination among different relationships.
2) \textit{MLLM-based Methods.} These tactics extend the capabilities of VLMs by incorporating auto-regressive language models, predicting objects and relations in an open-ended manner. Specifically, they utilize the sequential prediction capabilities of MLLMs, \eg, BLIP~\cite{li2023blip} and LLaVA~\cite{liu2024visual}, to model scene graphs as structured sequences. For example, PGSG~\cite{li2024pixels} and OpenPSG~\cite{zhou2025openpsg} employ auto-regressive modeling to iteratively predict objects and relations, providing fine-grained relational reasoning for open-set triplets. ASMv2~\cite{wang2025all} builds on LLaVA~\cite{liu2024visual} with instruction fine-tuning, unifying text generation, object localization, and relation comprehension. Despite their power, MLLM-based methods typically require huge computing resources. In this paper, we focus on VLM-based methods and propose an interaction-aware framework that explicitly models object interactions and enhances generalization to novel categories.


% \subsection{Weakly Supervised Scene Graph Generation} 
\textbf{Weakly Supervised SGG.} This task aims to train models using language descriptions instead of fully annotated scene graphs. Existing works usually extract entities and relations from captions using language parsers~\cite{schuster2015generating}, then ground corresponding regions. Grounding methods include contrastive learning-based graph matching~\cite{shi2021simple}, semantic matching rules~\cite{zhong2021learning}, knowledge distillation from pre-trained VLMs~\cite{li2022integrating}, and aligning regions and words for scene graph supervision~\cite{zhang2023learning}. Recent large language model (LLM)-based approaches, \eg, LLM4SGG~\cite{kim2024llm4sgg} uses LLMâ€™s reasoning capabilities to refine triplet extraction and alignment, mitigating semantic over-simplification. Similarly, GPT4SGG~\cite{chen2023gpt4sgg} synthesizes holistic and region-specific narratives, using the generative power of GPT-4~\cite{openai2023gpt} to capture both global context and local details. In this paper, we propose a simple and efficient method that only use LLM to generate counter-actions involved in bidirectional interaction prompts to improve interacting object detection accuracy.

% \subsection{Knowledge Distillation}
\textbf{Knowledge Distillation (KD).}
This strategy trains a smaller ``student'' model to replicate the outputs of a larger ``teacher'' model, commonly used in open-vocabulary learning to transfer knowledge from VLMs. It encourages the student to mimic the teacher's enriched hidden space, enabling generalization from base to novel concepts. Prior work~\cite{gu2021open,zang2022open} explores KD in open vocabulary object detection by using L1/MSE loss to align the student detector's features with the teacher VLM's regional visual features. 
% However, the feature space of models presents complex structures, and hard alignment -- by pulling the features of penultimate layers closer -- might be insufficient to capture the structured distribution.
However, this hard alignment may fail to capture complex feature structures. Later work~\cite{NEURIPS2022_dabf6125} aligns the similarity of inter-embeddings, aiding in the acquisition of structured knowledge. Recent work extends to multi-scale level~\cite{wang2023object} or bags-of-region level~\cite{wu2023aligning}, contrasting with InfoNCE loss. This paper adopts an interaction-consistent KD that combines point-to-point concept retention and structure-aware interaction retention distillation, preserving teacher's knowledge and identifying novel relationships beyond backgrounds.