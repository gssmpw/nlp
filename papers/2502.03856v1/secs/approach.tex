\section{Methodology}


\subsection{OVSGG Pipeline Review}
\textbf{Formulation.} Given an image $I$, SGG aims to construct a structured semantic graph $ \mathcal{G} = (\mathcal{V}, \mathcal{E}) $.  Each node $ v_i \in \mathcal{V} $ is defined by its bounding box (bbox) and category, while each edge $ e_{ij} \in \mathcal{E} $ represents the relationship between $ v_i $ and $ v_j $. In \textbf{open-vocabulary settings}, the label set $ \mathcal{C} $ for nodes and edges is divided into \textit{base classes} $ \mathcal{C}_B $ and \textit{novel classes} $ \mathcal{C}_N $, such that $ \mathcal{C}_B \cup \mathcal{C}_N = \mathcal{C} $ and $ \mathcal{C}_B \cap \mathcal{C}_N = \emptyset $. $ \mathcal{C}_B $ contains seen classes during training, while $ \mathcal{C}_N $ includes unseen classes that the model is expected to generalize to during inference.

\subsubsection{Architecture}
As illustrated in Figure~\ref{fig:framework}(b), an end-to-end OVSGG framework~\cite{chen2024expanding} typically follows a dual-encoder-single-decoder architecture~\cite{liu2023grounding}, involving three main components: the visual and text encoders, the cross-modality decoder, the entity and relation classifiers. 


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{figures/framework_v3.pdf}
    \put(-388, 68){\tiny{(Eq.~\ref{eq:step1})}}
    \put(-205, 68){\tiny{(Eq.~\ref{eq:step2})}}
    \vspace{-2em}
    \caption{Overview of INOVA for OVSGG. (a) VLM Pre-training: Interaction-aware target generation uses bidirectional interaction prompts and rule-based bounding box combinations to generate supervision, enriching object tokens with contextual interaction semantics. (b) SFT: A two-step interaction-guided query selection (IQS) prioritizes interacting objects and integrates relational context into object tokens, refining queries for the decoder. Bipartite graph matching aligns predictions with ground-truth for entity and relation classification.}
    \label{fig:framework}
\end{figure*}

\textbf{Visual and Text Encoders.}
The visual encoder (VE) extracts multi-scale visual features $\mathbf{V} \in \mathbb{R}^{N_v \times d}$ by the image backbone (\eg, Swin Transformer~\cite{liu2021swin}). For the text encoder (TE), input prompts are constructed by concatenating all predefined object and relation categories into a single sequence, \eg, ``\texttt{[CLS]} \texttt{man}. \texttt{horse}. \texttt{[SEP]} \texttt{riding}. \texttt{above}. \texttt{[PAD]}'', following~\cite{chen2024expanding}. Using this prompt, TE extracts object features $\mathbf{T}_o \in \mathbb{R}^{N_o \times d}$ and relation features $\mathbf{T}_r \in \mathbb{R}^{N_r \times d}$ using a pre-trained language model (\eg, BERT~\cite{kenton2019bert}). Here, $N_v$, $N_o$, and $N_r$ denote the numbers of image, object, and relation tokens, respectively. $d$ is the feature dimension.

\textbf{Cross-Modality (CM) Decoder.}  
It refines the representations of $K$ object queries $\{\mathbf{q}_i\}_{i=1}^K$ through a series of operations, including a self-attention layer, an image cross-attention layer for visual features, and a cross-attention layer for text features derived from prompts~\cite{liu2023grounding}. These refined queries are then passed through a feed-forward network (FFN) to predict object bbox coordinates. Following~\cite{chen2024expanding, shit2022relationformer}, a global relation query $\mathbf{q}_{rel}$ is introduced to capture spatial and semantic dependencies among objects in the image, complementing the local interactions represented by the object queries.

\textbf{Entity and Relation Classifiers.}
The entity/relation classifier compares node/edge features with text features of object/relation classes in a shared semantic space for open-vocabulary recognition. $\!$Concretely, node features $\{\mathbf{e}_o\}$ are from refined object queries and edge features $\{\mathbf{e}_{ij}\}$ are constructed by combining paired object features to capture subject-object interactions. $\!$To model interactions effectively, VS~\cite{zhang2023learning} constructs edge features by computing the differences and sums of object features. In contrast, we follow~\cite{chen2024expanding,shit2022relationformer} to concatenate a global relation embedding $\mathbf{e}_{rln}$ (refined representation of relation query) with pairwise object embeddings. The concatenated features are through a two-layer MLP to capture holistic dependencies and interactions for $\mathbf{e}_{ij}$.

\subsubsection{Training Process}
\label{sec:method_train}
\textbf{Bipartite Graph Matching.}
During training, it matches object queries with ground-truth (GT) annotations by minimizing a cost function based on semantic similarity and spatial alignment~\cite{carion2020end}. Matched queries are used for entity classification and linked to the matched GT's subordinate triplet, serving as input for edge representations.

\textbf{Training Objectives.}
Following~\cite{chen2024expanding}, there are three training losses: 1) \textit{Bbox Regression Loss}: Combines L1 $\mathcal{L}_{reg}$ and GIoU loss $\mathcal{L}_{giou}$~\cite{rezatofighi2019generalized} to ensure accurate object localization with precise positions and bounding box overlaps. 2) \textit{Entity Classification Loss}: Applies Focal Loss~\cite{lin2017focal} $\mathcal{L}_{obj}$ to alleviate imbalance-distribution issue by focusing on hard-to-classify and underrepresented object categories. 3) \textit{Relation Classification Loss}:
Uses binary cross-entropy (BCE) loss $\mathcal{L}_{rel}$ to align predicted relation scores with GT annotations.
% \footnote{Detailed formulations are left in Appendix.\label{fn:loss}}. 

\subsection{INOVA}
As illustrated in Figure~\ref{fig:framework}, INOVA follows a two-stage training process, incorporating interaction-aware target generation during pre-training and interaction-guided query selection in SFT to alleviate mismatches caused by uniform treatment of objects in each stage. Besides, an interaction-consistent KD further enhances the model's ability to distinguish interaction-based pairs from background noises.

\subsubsection{Interaction-Aware Target Generation}
\label{sec:itg}

To effectively identify interacting objects in weakly annotated data during pre-training, $\!$we devise an interaction-aware target generation tactic that uses bidirectional triplets rather than relying on a direct combination of all entity classes (\eg, ``\texttt{man}. \texttt{surfboard}.'') for object detection.

To be specific, after the semantic graph parsing process, we employ Grounding DINO~\cite{liu2023grounding} as the object detector and design \textbf{bidirectional interaction prompt} to guide the object localization. The bidirectional interaction prompt is constructed by combining two perspectives for each interaction triplet: one reflecting the action from the subject’s viewpoint (\eg, ``\texttt{man hold surfboard}'') and another from the object’s perspective (\eg, \texttt{surfboard held by man}''). The former is directly derived from the components of the interaction triplet, while the latter converse the subject and object with a \textit{counter-action} (\eg, ``\texttt{held by}'') generated by an LLM (\eg, Llama2~\cite{touvron2023llama})\footnote{The generation process of counter-action is in the Appendix C.\label{footnote:counterrel_gen}}. The dual-perspective construction process brings two key advantages: 1) \textit{Modeling Context Information}: Through the attention mechanism in the text encoder of Grounding DINO, the bidirectional interaction prompt integrates contextual interaction information into object tokens. As shown in Figure~\ref{fig:framework}(a), the attention mechanism enables the token ``\texttt{man}'' to absorb relevant interaction semantics, such as ``\texttt{hold surfboard}'', ensuring that the grounded object ``\texttt{man}'' is correctly aligned with its interaction context.
2) \textit{Enhancing Object Role Awareness}: By reversing operation, the object (\eg, ``\texttt{surfboard}'') of given triplet becomes the syntactic subject of the whole sentence (\eg, ``\texttt{surfboard held by man}''). As the central of the rephrased sentence, the syntactic subject receives heightened attention, improving its accuracy in localization. 

Furthermore, inspired by~\cite{li2022integrating,kim2024llm4sgg}, we adopt a \textit{rule-based combination} that combines overlapping subject and object bounding boxes to form reliable triplet supervision by Intersection over Union (IoU) score.

\subsubsection{Interaction-Guided Query Selection}
\label{sec:iqs}
During SFT, we introduce a two-step selection strategy for query initialization and refinement to prioritize interacting objects, mitigating the bipartite graph mismatched problem by reducing non-interacting candidates. 


\textbf{Step I.}  
This step aims to directly identify the most relevant visual tokens that are likely to participate in object interactions. Intuitively, the visual features of interacting objects should exhibit strong correlations with both object and relation semantics. To achieve this, for each visual token $\mathbf{v}_i \in \mathbf{V}_v$, a relevance score $s_i$ is computed by combining its maximum similarity with object and relation class tokens:
\begin{equation}
\small
\label{eq:step1}
    s_i = \big( \max(\mathbf{v}_i \mathbf{T}_{o}^\top) \big)^\gamma \cdot \big( \max(\mathbf{v}_i \mathbf{T}_{r}^\top) \big)^{1-\gamma},
\end{equation}
where $\max(\mathbf{v}_i \mathbf{T}_{o}^\top)$ computes the maximum similarity between the visual token $\mathbf{v}_i$ and all object class tokens in $\mathbf{T}_{o}$, while $\max(\mathbf{v}_i \mathbf{T}_{r}^\top)$ computes the maximum similarity between $\mathbf{v}_i$ and all relation class tokens in $\mathbf{T}_{r}$. The parameter $\gamma\in[0, 1]$ balances their contributions.


Based on the relevance scores, the top $K$ query indices, denoted as $\mathcal{I}_{K}$, are selected by the following procedure:
\begin{equation}
\small
    \mathcal{I}_{K} = \text{Top}_{K} ( \{ s_i \mid i = 1, 2, \dots, N_v \} ).
\end{equation}
The visual features and the position embedding~\cite{liu2023grounding} corresponding to the selected indices $\mathcal{I}_{K}$ are used to initialize queries for further decoding operations.

\textbf{Step II.}  
Nevertheless, the object and relation tokens are encoded individually in Step I, which limits capturing interaction semantics and distinguishing among objects. To this end, Step II explicitly models interaction semantics by integrating relational context into the object tokens. 
Specifically, after the initial forward pass, the model predicts a set of visual relation triplets $\langle$\texttt{subject}, \texttt{predicate}, \texttt{object}$\rangle$. These triplets are decomposed into interaction pairs $\langle$\texttt{subject}, \texttt{predicate}$\rangle$ and $\langle$\texttt{predicate}, \texttt{object}$\rangle$, which serve as \textbf{interaction prompts}. These prompts are encoded through the TE of VLM to get interaction tokens embeddings $\mathbf{T}_{in}$. The decomposition process has dual advantages: First, by leveraging interaction prompts, the TE's attention mechanism integrates interaction information into the object tokens, enabling the model to capture contextual dependencies and enhance its understanding of relationships. For instance, the token ``\texttt{man}'' can incorporate the semantic meaning of the interaction ``\texttt{riding}'' to obtain ``\texttt{man}~\scalebox{-1}[1]{\includegraphics[scale=0.4]{figures/man2.png}}'' in Figure~\ref{fig:framework}(b). Second, decomposing triplets into pairs avoids direct interference between object tokens, effectively preserving their unique characteristics. As illustrated in Figure~\ref{fig:framework}(b), ``\texttt{man}~\scalebox{-1}[1]{\includegraphics[scale=0.4]{figures/man2.png}}'' and ``\texttt{horse}~{\includegraphics[scale=0.04]{figures/horse.png}}'' are independently processed, preventing unnecessary dependencies across unrelated categories and maintaining the individual semantics of each object.

\textit{Interaction Query Selection.} For each visual token $\mathbf{v}_i$, the interaction relevance score $s_i^{in}$ is calculated by measuring the maximum similarity with interaction tokens:
\begin{equation}
\small
\label{eq:step2}
    s_i^{in} = \max (\mathbf{v}_i \mathbf{T}_{in}^\top).
\end{equation}
 The query indices set prioritizes the top $L$ tokens with the highest interaction relevance:
\begin{equation}
\small
    \mathcal{I}_L^{in} = \text{Top}_L ( \{ s_i^{in} \mid i = 1, 2, \dots, N_v \} ).
\end{equation}
\textit{Missing Query Selection.} However, relying solely on interaction relevance may fail to identify objects absent from the initially predicted triplets yet crucial for comprehensive scene understanding. To address this, the object relevance score $s_i^{o}$ is computed similarly, but using object tokens $\mathbf{T}_{o}$. The remaining $K-L$ query indices are selected based on object relevance, excluding those already chosen:
\begin{equation}
\small
    \mathcal{I}_{K-L}^{o} = \text{Top}_{K-L} (\{ s_i^o \mid i \notin \mathcal{I}_L^{in}, i = 1, 2, \dots, N_v \} ).
\end{equation}
The final query indices set combines these two subsets:
\begin{equation}
\mathcal{I}_K = \mathcal{I}_L^{in} \cup \mathcal{I}_{K-L}^{o}. \end{equation}
Combining Step I and Step II, the query selection achieves both interaction relevance and comprehensive integration of relational context. Step I identifies interaction-relevant tokens by balancing object and relation semantics, while Step II refines the representation by embedding relational context into object tokens through interaction prompts. This two-step strategy effectively reduces non-interacting candidates and mitigates mismatches in the bipartite graph. For ease of understanding, the pseudo-codes is left in Appendix D.

% By incorporating relational predictions and generating interaction-aware tokens, this step enhances the prioritization of interacting objects and filters out irrelevant queries. This improves the query selection process by capturing contextual dependencies and adapting the query set to the specific relational context of the image. The selected queries are used for the second forward pass, ensuring a more accurate and efficient modeling of object interactions.


\subsubsection{Interaction-Consistent KD}  
\label{sec:ickd}
\begin{figure}
    \centering
    \includegraphics[width=1.0\linewidth]{figures/kd.pdf}
    \put(-184, 53){\scalebox{0.7}{$\mathcal{L}_{VRD}$}}
    \put(-65, 52){\scalebox{0.7}{$\mathcal{L}_{RRD}$}}  
    \put(-232, 56){\scalebox{0.8}{$\mathbf{e}_{{\textit{\tiny{T}}}}$}}  
    \put(-135, 51){\scalebox{0.8}{$\mathbf{e}_{\text{\textit{\tiny{S}}}}$}} 
    \put(-108, 33){\scalebox{0.65}{$\mathbf{M}_{\text{\textit{\tiny{T}}}}^{ij}$}} 
    \vspace{-2em}
    \caption{Illustration of interaction-consistent KD.}
    \label{fig:kd}
    \vspace{-1em}
\end{figure}
Beyond the localization and classification objectives mentioned in Sec.~\ref{sec:method_train}, we adopt interaction-consistent knowledge distillation to enhance the model's ability to distinguish interacting pairs from background pairs and address catastrophic forgetting of learned relational semantics mentioned in~\cite{chen2024expanding}. Specifically, it leverages the VLM pre-trained in the first stage as the teacher model. The student network is designed as a pseudo-siamese structure of the teacher model, initialized with the teacher's parameters.  

Interaction-consistent KD combines visual-concept retention distillation and relative-interaction retention distillation to align the student model with the teacher's semantic space while maintaining inter-pair relational consistency. The entire loss function contains two complementary objectives:
% VRD focuses on preserving point-wise semantic alignment, while RRD enforces structural alignment of relational dependencies between interacting pairs, ensuring a comprehensive transfer of relational knowledge. The overall loss function integrates two complementary objectives:

\textit{Visual-concept Retention Distillation (VRD)}: As proposed in~\cite{chen2024expanding}, this objective ensures that the student's edge features remain point-wise consistent with the teacher's semantic space for negative samples, thereby preserving semantic alignment. The loss is defined as:
\begin{equation}
\small
\mathcal{L}_{{VRD}} = \frac{1}{|\mathcal{N}|} \sum_{\mathbf{e} \in \mathcal{N}} \| \mathbf{e}_{\text{\textit{\tiny{S}}}} - \mathbf{e}_{\text{\textit{\tiny{T}}}} \|_1,
\label{eq:visual_concept_kd}
\end{equation}
where $\mathbf{e}_{\text{\textit{\tiny{S}}}}$ and $\mathbf{e}_{\text{\textit{\tiny{T}}}}$ denote the edge features of the student and teacher models, and $\mathcal{N}$ is the set of negative samples.

\textit{Relative-interaction Retention Distillation (RRD)}: While VRD effectively preserves point-wise semantic consistency, it fails to ensure the relative relationships between triplets, \ie, distinguishing interaction pairs from backgrounds (\cf Figure~\ref{fig:kd}(a)). $\!$RRD explicitly models inter-pair relativity~\cite{NEURIPS2022_dabf6125} by aligning the structure similarity of triplet embeddings between the teacher and student models. The structure similarity matrices for the teacher and student models, $\mathbf{M}_{\text{\textit{\tiny{T}}}}$ and $\mathbf{M}_{\text{\textit{\tiny{S}}}}$, are normalized by L2 norm:
\begin{equation}
\small
\mathbf{M}_{\text{\textit{\tiny{T}}}}^{ij} = \frac{ \mathbf{e}_{\text{\textit{\tiny{T}}}}^i \cdot \mathbf{e}_{\text{\textit{\tiny{T}}}}^{j\top} }{\| \mathbf{e}_{\text{\textit{\tiny{T}}}}^i \cdot\mathbf{e}_{\text{\textit{\tiny{T}}}}^{j\top} \|_{2}}, \quad
\mathbf{M}_{\text{\textit{\tiny{S}}}}^{ij} = \frac{ \mathbf{e}_{\text{\textit{\tiny{S}}}}^i \cdot \mathbf{e}_{\text{\textit{\tiny{S}}}}^{j\top} }{\| \mathbf{e}_{\text{\textit{\tiny{S}}}}^i \cdot \mathbf{e}_{\text{\textit{\tiny{S}}}}^{j\top} \|_{2}}.
\label{eq:cosine_similarity}
\end{equation}
The RRD loss then aligns these similarity matrices by minimizing the Frobenius norm $\| \cdot \|_F$ between them:
\begin{equation}
\small
\mathcal{L}_{{RRD}} = \frac{1}{|\mathcal{N}|^2} \| \mathbf{M}_{\text{\textit{\tiny{S}}}} - \mathbf{M}_{\text{\textit{\tiny{T}}}} \|_F^2.
\label{eq:relative_interaction_kd}
\end{equation}
\textbf{Final Objectives:} Combine localization and classification losses with above complementary objectives to achieve point-wise semantic alignment and relational consistency:
\begin{equation}
\small
\mathcal{L} = \mathcal{L}_{reg} + \mathcal{L}_{giou}  + \mathcal{L}_{obj} + \mathcal{L}_{rel} + \beta_1 \mathcal{L}_{{VRD}} + \beta_2 \mathcal{L}_{{RRD}}.
\end{equation}
The weights $\beta_1$ and $\beta_2$ control the relative importance of semantic alignment and relational consistency.

