\section{Experiments}
\subsection{Experiment setup}
\textbf{Datasets.} We evaluated INOVA on two SGG benchmarks: 1) \textbf{VG}~\cite{krishna2017visual} contains annotations for 150 object categories and 50 relation categories across 108,777 images. Following standard setup~\cite{xu2017scene}, 70\% of the images are used for training, 5,000 for validation, and the remaining for testing. For a fair comparison, we excluded images overlapping with the pre-training dataset of Grounding DINO~\cite{liu2023grounding}, retaining 14,700 test images as in~\cite{zhang2023learning}. $\!$2) \textbf{GQA}~\cite{hudson2019gqa} uses the GQA200 split~\cite{dong2022stacked, sudhakaran2023vision}, including 200 object categories and 100 predicate categories. We randomly sampled 70\% of the object and predicate categories as the base, and more details can be found in the Appendix A.

\textbf{Metrics.} We conducted experiments under the challenging Scene Graph Detection (\textbf{SGDET}) protocol~\cite{xu2017scene,krishna2017visual}, which requires detecting objects and identifying relationships between object pairs without GT object labels or bounding boxes. We reported: 1) \textbf{Recall@K} (\textbf{R@K}): The proportion of ground-truth triplets correctly predicted within the top-K confident predictions.
2) \textbf{Mean R@K} (\textbf{mR@K}): The average R@K across all categories.
\input{tables/ovr}
\input{tables/ovdr}
\textbf{Implementation Details.}
Due to space constraints, detailed implementation is provided in the Appendix A.
\subsection{Comparison with State-of-the-Art Methods}
\textbf{Setting.} Following~\cite{chen2024expanding}, we compared our INOVA with existing SOTA methods, \ie, \textbf{VS}~\cite{zhang2023learning}, \textbf{OvSGTR}~\cite{chen2024expanding}, and \textbf{RAHP}~\cite{liu2025relation} under two OVSGG settings: 1) \textbf{OvR-SGG}: Evaluates generalization to unseen relations while retaining original object categories. Fifteen of 50 relation categories in VG150 are removed during training, with performance measured on ``Base+Novel (Relation)'' and ``Novel (Relation)''.
2) \textbf{OvD+R-SGG}: Assesses handling of unseen objects and relations simultaneously. Both novel objects and relations are excluded during training, evaluated on ``Joint Base+Novel'', ``Novel (Object)'', and ``Novel (Relation)''. 

\textbf{Results.}
We conducted quantitative experiments on the VG dataset~\cite{krishna2017visual} in both the OvR-SGG and OvD+R-SGG setups, with results presented in Table~\ref{tab:ovr} and Table~\ref{tab:ovdr}, respectively. Notably, INOVA consistently outperforms the latest state-of-the-art methods across all metrics. In the OvR-SGG setup, INOVA surpasses the RAHP (Swin-T) by \textbf{+1.78}\% R@100 within the novel relation categories, demonstrating superior generalization and reduced overfitting. With the Swin-B backbone, INOVA achieves R@100 over OvSGTR across both base and novel relations, and \textbf{+4.94}\% R@100 in novel relations alone, further emphasizing its robustness. In the more challenging OvD+R-SGG scenario, INOVA continues to outperform the competition. Specifically, on the joint base and novel classes, INOVA gains \textbf{+4.90}\% and \textbf{+2.16}\% R@100 over OvSGTR with the Swin-T and Swin-B backbones, respectively. These results validate INOVA's superior performance and robust generalization across both relation and object domains.

\subsection{Diagnostic Experiment}
\input{tables/abla}
To ensure a comprehensive evaluation, we performed a series of ablation studies on the VG dataset~\cite{krishna2017visual} in the challenging OvD+R-SGG scenario.


\textbf{Key Components Analysis.} 
The results are summarized in Table~\ref{tab:abla}, with the first row representing the baseline OVSGG pipeline with \textit{Visual-concept Retention Distillation} proposed in~\cite{chen2024expanding}. From this analysis, four key conclusions can be drawn: \textbf{First}, incorporating \textit{Interaction-aware Target Generation} (ITG) leads to consistent improvements across all metrics, including a \textbf{3.94}\% R@100 gain on the joint base and novel classes compared to the baseline. This demonstrates that ITG effectively improves performance by considering interaction contexts in supervision generation. \textbf{Second}, introducing \textit{Interaction-guided Query Selection} (IQS) further refines the query selection process. By prioritizing interacting objects and minimizing mismatched assignments, IQS achieves notable improvements, such as \textbf{3.00}\% R@100 gains, highlighting its ability to enhance precision by focusing on interacting object pairs. \textbf{Third}, leveraging \textit{Relative-interaction Retention Distillation} (RRD) ensures relational consistency during training, resulting in significant performance boosts. RRD contributes \textbf{2.83}\% R@100 gains, improving the model's ability to handle novel classes effectively. \textbf{Fourth}, the integration of all three components (\ie, ITG, IQS, and RRD) yields the best overall performance, with \textbf{1.92}\%$\sim$\textbf{8.28}\% improvements across all evaluation metrics. However, the improvement is less pronounced than expected, since each strategy prioritizes interacting objects, which may lead to diminishing returns by progressively reducing non-interacting objects. Despite this, the combined results still demonstrates enhanced relational understanding and serve as a valuable tool for improving performance in complex scenarios.


\begin{figure}[!t]
    \centering
    \includegraphics[width=1\linewidth]{figures/itg.pdf}
    \vspace{-2.0em}
    \caption{Interaction-aware target generation.}
    \label{fig:itg}
    \vspace{-1.5em}
\end{figure}

\input{tables/pretrain}

\begin{figure}[!t]
    \centering
    \vspace{-0.3em}
    \includegraphics[width=1\linewidth]{figures/query.pdf}
    \vspace{-2.3em}
    \caption{Interaction-guided query selection.}
    \label{fig:iqs}
    \vspace{-1.2em}
\end{figure}



\textbf{Supervision Analysis.}
We investigated ITG's impact in the pre-training process (\cf Table~\ref{tab:pretrain}). As seen, models pre-trained on COCO~\cite{chen2015microsoft} captions with INOVA variants consistently outperform others, achieving \textbf{13.31}\% R@100 with Swin-T and \textbf{14.22}\% R@100 with Swin-B. These results demonstrate the effectiveness of incorporating ITG in the VLM pre-training process.

In addition, we visualized the object detection results from ITG and the original methods that solely use object categories for detection. As displayed in Figure~\ref{fig:framework}, the original method produces redundant objects, complicating the identification of subject-object interactions. For instance, given the ``$\langle$\texttt{people}, \texttt{ride}, \texttt{bike}$\rangle$'' triplet, the baseline detects multiple instances of ``\texttt{people}'' and ``\texttt{bike}'', obscuring the interaction. In contrast, ITG leverages bidirectional interaction prompts and attention mechanisms to accurately localize the interaction-relevant objects. A similar enhancement is observed in the ``$\langle$\texttt{bikes}, \texttt{on}, \texttt{boat}$\rangle$'' triplet, where ITG focuses on interaction-relevant entities. 

\textbf{Query Visualization.} To demonstrate the effectiveness of IQS, we visualized the top-50 selected queries in Figure~\ref{fig:iqs}. As seen, the original approach makes no distinction between instances within the same category, such as ``\texttt{man}'' or ``\texttt{zebra}'', resulting in both interacting and non-interacting instances receiving a similar number of queries. This indiscriminate query generation increases the likelihood of incorrect matches during bipartite graph matching, as irrelevant regions compete with interaction-relevant instances. Conversely, IQS prioritizes queries for interacting instances (``man holding'' or ``zebra laying on'' in Figure~\ref{fig:iqs}), increasing discrimination among objects with the same categories. 
