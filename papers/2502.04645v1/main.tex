\documentclass[sigconf,natbib=true, nonacm]{acmart}

% \usepackage{graphicx}
% \usepackage{amsmath} 
% \usepackage{comment}
% \usepackage{tabularx}
% \usepackage{multicol}
% \usepackage{multirow}
% \usepackage{booktabs}
\usepackage{xcolor}
\usepackage{natbib}
\definecolor{darkgreen}{RGB}{0,100,0} 
% \usepackage{listings} % For code formatting
% \usepackage{enumitem} % Add this line to your preamble
% \usepackage{caption}
% \usepackage{float} % in your preamble if not already there
% \captionsetup{labelfont=normalfont,textfont=normalfont}
% \usepackage[table,xcdraw]{xcolor}
\usepackage{threeparttable}
% \newcommand{\yb}[1]{\textcolor{red}{[yubin: #1]}} 
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

% Rights management information.  This information is sent to you
% when you complete the rights form.  These commands have SAMPLE
% values in them; it is your responsibility as an author to replace
% the commands and values with those provided to you when you
% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2024}
\acmYear{}
\acmDOI{XXXXXXX.XXXXXXX}

% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[SIGIR '25]{}{July 13--18,
  2025}{Padua, Italy}


\newcommand{\todo}{\textcolor{red}}
\newcommand{\newchange}{\textcolor{blue}}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Cross-Encoder Rediscovers a Semantic Variant of BM25}

\author{Meng Lu}
\affiliation{%
  \institution{Brown University}
  \city{Providence}
  \state{Rhode Island}
  \country{USA}
}
\email{meng_lu@brown.edu}

% Second co-first author
\author{Catherine Chen}
\affiliation{%
  \institution{Brown University}
  \city{Providence}
  \state{Rhode Island}
  \country{USA}
}
\email{catherine_s_chen@brown.edu}

% Other author
\author{Carsten Eickhoff}
\affiliation{%
  \institution{University of Tübingen}
  \city{Tübingen}
  \country{Germany}
}
\email{carsten.eickhoff@uni-tuebingen.de}



\begin{abstract}
%Transformer-based neural ranking models (NRMs) have surpassed traditional ranking methods, such as BM25, and earlier neural models designed to emulate BM25 with explicit components for semantic TF and IDF computations. Despite their superior performance, NRMs remain challenging to interpret, particularly regarding their internal relevance computations. Using path patching, we traced the core components of the model's relevance computation path, which functions as a semantic variant of BM25. The path includes (1) components responsible for computing and transmitting BM25-like information— semantic TF, IDF, document length, and term saturation effects—and (2) attention heads that utilize this information to perform BM25-style relevance scoring. Using mechanistic interpretability methods, we localize the specific components responsible for each step of this process, which lays the groundwork for potential model editing to enhance model transparency, addressing safety concerns, and improving scalability in training and real-world applications.

%OLDER ABSTRACT
Neural Ranking Models (NRMs) have rapidly advanced state-of-the-art performance on information retrieval tasks. In this work, we investigate a Cross-Encoder variant of MiniLM to determine which relevance features it computes and where they are stored. We find that it employs a semantic variant of the traditional BM25 in an interpretable manner, featuring localized components: (1) Transformer attention heads that compute soft term frequency while controlling for term saturation and document length effects, and (2) a low-rank component of its embedding matrix that encodes inverse document frequency information for the vocabulary. This suggests that the Cross-Encoder uses the same fundamental mechanisms as BM25, but further leverages their capacity to capture semantics for improved retrieval performance. The granular understanding lays the groundwork for model editing to enhance model transparency, addressing safety concerns, and improving scalability in training and real-world applications.

\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10002951.10003317.10003338</concept_id>
       <concept_desc>Information systems~Retrieval models and ranking</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010187.10010192</concept_id>
       <concept_desc>Computing methodologies~Causal reasoning and diagnostics</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Information systems~Retrieval models and ranking}
\ccsdesc[300]{Computing methodologies~Causal reasoning and diagnostics}

%
% Keywords. The author(s) should pick words that accurately describe
% the work being presented. Separate the keywords with commas.
\keywords{explainability, neural retrieval models, IR axioms, mechanistic interpretability}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}
Information retrieval (IR) has long relied on foundational models like BM25, grounded in intuitive heuristics for relevance estimation. BM25 leverages term frequency (TF) and inverse document frequency (IDF) to rank documents effectively, achieving strong performance across various tasks. Its simplicity and interpretability have made it a cornerstone of traditional IR systems. Inspired by BM25’s success, earlier neural IR models (e.g., MatchPyramid \cite{pang2016text}, DRMM \cite{guo2016deep}) were purposefully designed to emulate BM25’s principles. These models incorporated explicit components for semantic TF and IDF computations, blending neural architectures with established IR heuristics to improve relevance estimation.

However, the advent of transformer-based models has revolutionized the field of IR. These models, trained end-to-end on raw data, excel at extracting context-dependent semantic signals for ranking tasks. By leveraging the multi-attention mechanism and vast parameter spaces, transformers~\cite{vaswani_attention} capture nuanced relationships between query and document terms that go beyond traditional heuristic-based approaches. Despite their superior performance, these models come with significant trade-offs: their complexity and lack of interpretability make it challenging to understand how they assess relevance. This raises fundamental questions: how do these models assess relevance? Do they rely on established IR principles such as TF and IDF, or do they operate as vast, unstructured statistical approximators?

In this work, we build upon previous correlational studies by employing mechanistic interpretability~\cite{nanda2022mechanistic,elhage2021mathematical,olsson_induction,meng_rome,wang_ioi,pearl} methods to address these questions. These techniques have further advanced our understanding of the Transformer-based  architectures~\cite{vaswani_attention} that underpin many modern NLP systems~\cite{olsson_induction,geiger,meng_rome,wang_ioi,Vig} and in IR, have been used to identify components responsible for encoding term frequency signals in bi-encoders~\cite{chen_axiomatic}. 

In this paper, we deeply analyze a cross-encoder and validate prior research suggesting the presence of a BM25-like signal by causally demonstrating how a semantic variant of BM25 is precisely implemented within the components of a transformer architecture.



Specifically, our main findings are:
\begin{itemize}
\item (\S\ref{relevance_scoring_heads}-\S\ref{matching_heads}) We use path patching to identify attention heads that compute and process BM25-like components, including a semantic version of TF (soft-TF), term saturation, and document length. These heads send this information to another group of heads (Relevance Scoring Heads), which we hypothesize compute a BM25-style function by aggregating all these relevance signals.
\item (\S\ref{IDF}): We find evidence that IDF information exists in a dominant low-rank vector of the embedding matrix and can be manipulated to control term importance.
\item (\S\ref{sec:circuit-function-approximatio}): We confirm that Relevance Scoring Heads implement a BM25-style computation by defining a linear approximation of the cross-encoder's relevance computation and evaluating its ability to reconstruct scores, confirming that our circuit understanding captures the core mechanism of relevance computation. 
% Since this linear model encapsulates both the final computation and the previously identified components contributing to it, we conclude that our circuit understanding captures the core mechanism of the cross-encoder's relevance computation.
\end{itemize}
\input{tex_figures_and_tables/axioms_table}
By leveraging path patching for a sequential tracing of subcomponents on the cross-encoder's term-matching path, we reverse-engineer a dominant portion of cross-encoder's pathway to compute relevance. Such granular understanding reveals potential for model-editing, potentially enabling model performance enhance, controllability and personalization, and mitigate biases for NRMs.


\begin{comment}
\section{Related Work} 

\subsubsection{Mechanistic Interpretability for IR framework} Previously, Probing was a widely used method to assess a model’s internal representations, aiming to identify specific concepts encoded by the network and localize the components responsible for such behaviors. This is typically achieved by training a lightweight classifier on top of the model's internal structures (e.g., embeddings or attention maps) to evaluate the information encoded in those layers [7, 10, 12, 13, 22, 30, 35, 37]. While probing reveals correlational information about what a model learns, it faces limitations when determining causal relationships or confirming whether these learned concepts are actively utilized during inference [1, 2]. 

In contrast, mechanistic interpretability methods go beyond correlation by offering a more detailed, causality-driven understanding of how internal components affect model behavior, which surpasses traditional explainable IR (XIR) techniques based solely on correlational insights. To provide a more granular understanding of internal model decision-making processes, Chen et al. propose the use of causal interventions to reverse engineer neural rankers, and demonstrate how mechanistic interpretability methods can be used to isolate components satisfying term-frequency axioms within a ranking model.

\subsubsection{Non-mechanistic Interpetablity for Bert-Base IR model internal mechanisms} Previous research has shown through probing that IDF is present in Bert-based IR model and query-document interaction is present in BERT-based IR models (Zhan et al. and Choi et al.). 

Choi adopts linear probing to investigate the internal mechanism of the model: by training a lightweight classifier on top of the model's internal structures (e.g., embeddings or attention maps) to evaluate the information encoded in those layers. Choi et al. showed that IDF information can be reliably extracted from BERT-based NRMs. However, probing experiments only established a correlation between certain internal signals and IDF but stopped short of confirming whether BERT-based NRMs explicitly compute or use IDF.  

Zhan et al. concludes that BERT aggregates document information to query token representations through their interactions in later layers, by showing that the removal of interactions from document to query results in those layers result in significant performance loss. However, Zhan et al. did not clearly define or explain “interaction” and “representation”, specifically what the document representation contains and how the model “aggregates doc information” because of the limits of probing and interaction ablation for analysis. Zhan et al. could only access by-layer importance but was unable to understand the interaction or representation construction through the attention mechanism or the feedforward network.

\begin{figure*}%[!ht]
    \centering
    \includegraphics[width=5in]{paperdiagrams/CircuitGeneral.pdf}
    \caption{}
    %\small\textsuperscript{3} We blur the face of the non-reading child in accordance with the participant consent form.
    \label{fig:your_label}
\end{figure*}
\subsubsection{Mechanistic Interpretability for IR framework and TF components} In contrast, mechanistic interpretability methods go beyond correlation by offering a more detailed, causality-driven understanding of how internal components affect model behavior, which surpasses traditional explainable IR (XIR) techniques based solely on correlational insights. 

To provide a more granular understanding of internal model decision-making processes, Chen et al. propose the use of causal interventions to reverse engineer neural rankers, and demonstrate how mechanistic interpretability methods can be used to isolate components satisfying term-frequency axioms within a ranking model. In particular, Chen et al. find Duplicate Token Heads that write the term frequency signal of the query token in the document to the residual stream for producing the ranking score. We adopt a similar methodology, extending it to investigate soft term frequency (soft-tf) in cross-encoders. Specifically, we analyze how soft-tf signals are generated and utilized in query-document interactions within cross-encoders to compute ranking scores.


While previous research has highlighted the presence of components analogous to those used in BM25, it remains unclear how these elements—such as traditional scoring features like TF and IDF, along with semantic matches—interact to compute relevance scores in neural ranking models (NRMs).

\end{comment}



\section{Related Work} \label{background}


\subsection{Interpreting Neural IR Models}

In contrast to early neural IR models (e.g., MatchPyramid \cite{pang2016text}, DRMM \cite{guo2016deep}) that were explicitly designed to be neural term matching models, recent pre-trained transformer-based ranking models achieve significant performance gains by capturing nuanced query-document interactions and forming rich contextual representations~\cite{intro_NRMS_Trabelsi_2021, intro_NRMS_guo}. Although not explicitly designed to encode relevance concepts, these models have been found to learn relevance concepts such as TF \cite{chen_axiomatic}, IDF \cite{choi_idf}, and even combinations of them such as BM25 \cite{wang2021bert, rau2022different, yates2021pretrained, macavaney2022abnirml}. To better understand how these relevance signals are represented and utilized, researchers have turned to  IR axioms \cite{camara2020diagnosing, rennings2019axiomatic, volske2021towards} and constructed axiom-specific diagnostic datasets to test how well models rank document pairs in accordance with axiomatic constraints~\cite{diagnostic_dataset}. Many studies also rely on correlational methods such as probing \cite{choi_idf, zhan, formal2021white, formal2022match, macavaney2022abnirml} to diagnose model behavior and assess their adherence to desired relevance properties. However, the underlying mechanisms of how these concepts are encoded remain unclear. This work aims to identify the specific components involved in this process and explain how they interact to determine relevance.

% \todo{Clean up:} Recent research has sought to understand how this mechanism unfold by examining the smaller components of BM25 such as TF or IDF \todo{add citations} and also look at how queries and documents interact to aggregate relevance information. Some turn to formalized notions of retrieval heuristics in the form of IR axioms, to diagnose model behavior and their adherence to these desired relevance properties. These studies, while they provide more insight than previously know about how relevance concepts are encoded in the model, it is unknown how the model identifies these signals and combines them to send relevance information from start to end. In this work, we seek to provide a more granular understanding about which specific components are involved in this process and describe how they interact to determine relevance.



% \todo{add more citations} Recent research has sought to analyze NRMs to assess how they compute relevance and whether traditional components like TF and IDF are effectively modeled within them~\cite{choi_idf,zhan}. For instance, Choi et al. demonstrated that IDF information can be extracted from BERT-based NRMs using lightweight classifiers, while Zhan et al. concluded that BERT aggregates document information into query token representations through interactions in later layers~\cite{choi_idf,zhan}. However, these studies rely on probing techniques, offering correlational rather than causal evidence. 


\subsection{Mechanistic Interpretability}

To better understand how information is encoded in models, mechanistic interpretability~\cite{nanda2022mechanistic,elhage2021mathematical,olsson_induction,meng_rome,wang_ioi,pearl} aims to reverse-engineer model behavior by analyzing the causal effects of components, providing stronger causal evidence than probing \cite{Vig, meng_rome}. Gaining traction in NLP, these methods have been used to uncover how LLMs perform tasks, such as indirect object identification \cite{meng_rome} and greater-than computation \cite{hanna2024does}, and have recently been applied to IR to identify attention heads encoding term frequency signals \cite{chen_axiomatic}.

Mechanistic interpretability methods rely on causal interventions, such as activation patching (or causal mediation analysis) where activations are swapped between input pairs to observe their effects on model behavior and localize specific mechanisms. Activation patching provides a solid foundation for initial analyses, allowing researchers to identify and isolate specific mechanisms within a model \cite{zhang2023towards, heimersheim2024use}. For example, \citet{chen_axiomatic} use activation patching with IR axioms to identify attention heads that encode a term frequency signal in a bi-encoder. However, activation patching has limitations as it only evaluates the indirect effect components have on the output. In contrast, path patching allows us to trace the computational path (circuit) from input to output, providing insights into how components sequentially interact \cite{wang_ioi, goldowskydill_ppatching}. In this work, we leverage path patching to reverse-engineer the computational path through which a NRM uses to compute relevance. 
% a circuit that captures the core model's relevance computation pathway. 
% This step-by-step process reveals key points for model editing, introducing the potential for creating more controllable NRMs.


\section{Methodology} \label{methods}


\input{tex_figures_and_tables/path_patching_overview}

% Our causal analysis employs diagnostic dataset construction, path patching, Singular Value Decomposition (SVD), and ablation studies. In this section, we explain how we adapt the first two to track how the model implements the composite BM25.
In this section, we explain how we construct diagnostic datasets and use path patching to track how the model implements a semantic version of BM25.

\subsection{Diagnostic Dataset Construction}
\label{sec:diagnostic-dataset}
\textit{\textbf{Mapping BM25 components to IR axioms.}} IR axioms are derived from the same retrieval heuristics BM25 is composed of: TF, IDF, term saturation, and document length \cite{fang2004formal}. Table \ref{tab:axioms_table} outlines the mapping from BM25 components to axioms and provides their formal definitions. Note that for TF, we extend this analysis of term matching to include semantic matches, where semantically similar terms contribute to the relevance score, based on the intuition about the semantic understanding capabilities of Transformer-based models~\cite{intro_NRMS_Trabelsi_2021}. We refer to this extension as \textbf{soft-TF} and utilize both TFC1 and STMC1 as the corresponding axioms. 

\textit{\textbf{Constructing minimal-pair-based diagnostic datasets.}} We construct a diagnostic dataset for each axiomatic component by perturbing documents to create input pairs. Our starting point is the base dataset from Chen et al.~\cite{chen_axiomatic}, who create a diagnostic dataset for analyzing TFC1 using MS-MARCO \cite{msmarco_dataset}, consisting of 10k query-document pairs. To create the diagnostic dataset for each axiom, we perturb each query-document pair in the base dataset in accordance with the formal axiomatic definition (Table \ref{tab:axioms_table}). Each perturbed sample differs from the original only slightly, with the perturbation introducing an additional signal corresponding to the relevance concept we aim to analyze. Our perturbation strategies and examples of input pairs are shown in Table \ref{tab:axioms_table}.
% These perturbations allow us to isolate and measure the effect of each individual component’s signal when running experiments.Specifically, for STMC1, we ask GPT-4o to generate 20 similar words and select the one with closest cross-ecoder's embedding similarity for perturbation as the injected similar token. For TFC2, we use GPT-4o to generate relevant response sentences for a given query as the baseline document. For LNC1, we pair random document sentences from a different query in the base dataset (the number of sentences depends on the perturbation number) to add non-relevant information and lengthen the document.

%This decomposition isolates the role of each component, allowing us to target them individually. Table\ref{tab:axioms_table} contains specific definitions of the 

%\textbf{\textit{Diagnostic Datasets.}}
%\todo{ still working on this part (1) need to explain diagnostic dataset and make a sighir 23' reference. (2) Need to talk about why we  1. Base dataset (These are all from msmarco,\#samples, msmarco,), all diagnostic datasets need to be related to the dataset expalin the reason why that needs to be 2. refer to table 1 and xplain why we chose some of the axioms 3. how each one is generated. + explain why we look at soft-TF instead of TF}
%\begin{enumerate}

%     \item \textbf{Axiom-Based Decomposition:}
%     We break down the target ranking function (e.g., BM25) into its constituent components, each associated with a specific axiom, as shown in Table 1. This decomposition isolates the role of each component, allowing us to target them individually.
%     \item \textbf{Diagnostic Dataset Construction:}\todo{ explain how is our original dataset = base dataset constructed? Or just say we use Chen et al. dataset as the base dataset.}
%     For each axiom-linked component, we construct axiom-based minimal pairs using the dataset proposed by Chen et al.~(2023). Each minimal pair consists of an original input document and a slightly modified version that differs only in the attribute governed by the relevant axiom, as shown in Table~\ref{tab:axioms_table}. Here, \textit{b} denotes the baseline, and \textit{p} denotes the perturbed samples. Specifically, for STMC1, we use GPT-4 to generate 30 semantically similar terms for a selected query term and then choose the term with the highest cross-encoder's embedding similarity as the replacement. For TFC2, we use GPT-4 to generate relevant response sentences.
    

% \end{enumerate}
\begin{comment}
\todo{Reword:} We describe all the datasets we use in our analysis here at the beginning to make explanation of component behavior clearer and more organized later.

\noindent\textbf{\textit{Term Frequency (TFC1 \todo{cite} and STMC1 \todo{cite})}}
\todo{explain definition, why we choose these specific axioms, and how we construct these diagnostic datasets. also why these are the basis for behavior we are trying to isolate - maybe TF is known to be a major player in NRMs?}
%define "soft-TF" in the background section and explain why we use TFC1 and STMC1 as the main datasets for path patching there? that way, we can use this section to just focus on describing what's going on with this head.
%In this section, we relevant work to our paper and provide a background of our methodology.


\noindent\textbf{\textit{Inverse Document Frequency (TDC \todo{cite})}}
\todo{explain definition, why we choose these specific axioms, and how we construct these diagnostic datasets}


\noindent\textbf{\textit{Term Frequency Saturation (TFC2 \todo{cite})}}
\todo{explain definition, why we choose these specific axioms, and how we construct these diagnostic datasets}


\noindent\textbf{\textit{Length Normalization (LNC1 \todo{cite})}}
\todo{explain definition, why we choose these specific axioms, and how we construct these diagnostic datasets}

\end{comment}

%By constructing diagnostic datasets based on individual axioms, we create controlled experiments where only the variable corresponding to the axiom is modified, while all other factors remain constant.  Then, through these ``controlled experiments'' we can track down whether and where each crucial BM25 component exists in the cross-encoder model.


\subsection{Path Patching}
\label{sec:path-patching}

%We adopt path patching~\cite{wang_ioi,goldowskydill_ppatching} that enables researchers to trace the flow of specific signals through the model’s architecture and measure their causal impact on the final output. 

% \todo{do we need to cite?}
The intuition of path patching relies on the Transformer~\cite{vaswani_attention} architecture, where stacks of multi-headed attention and multi-layer perceptron (MLP) layers are connected by residual connections. These residual connections are additive functions, which essentially allow components to ``read'' and ``write'' information to a common \textit{residual stream}. ~\cite{mathematical_transformer}. The goal of path patching is to isolate which components ``send'' and ``receive'' information through the residual stream, and what information is being transmitted.

%Then, path patching enables us to trace how specific signals, such as term frequency (TF), propagate through the model to influence predictions.
Path patching \cite{wang_ioi, goldowskydill_ppatching} involves using minimal input pairs (we denote as baseline \textit{b} and perturbed p), that differ only by a target behavioral signal to be evaluated (e.g., axiomatic property). Activations (e.g., residual stream output, attention head or layer output, MLP output) are iteratively copied from the perturbed input into the corresponding location in the baseline input to isolate the impact of that component on downstream computations.  In this work, we focus on analyzing the roles of attention heads in relevance computation and leave analysis of MLPs for future investigation.

% Path patching allows us to analyze not only its impact on final logits but also its effect on intermediate components. In this work, we focus on intermediate attention heads, and patch patching enables us to iteratively trace backward by identifying which attention heads are critical for transferring relevance signals to early heads and then patching to those early heads to determine which earlier components were most important for passing the information until all behaviors of interest in the study are explained.


The path patching algorithm is an iterative backward process that starts with identifying which upstream components send important information to the logits (Figure~\ref{fig:path-patching-overview}). Specifically, it involves four forward passes through the model to identify which upstream (sender) components send information to the target (receiver) component (i.e., logits): (1) Run on the baseline input \( x_b \) and cache activations. (2) Run on the perturbed input \( x_p \) and cache activations. (3) Select the sender set \( s \), the components whose activations are patched in, and the receiver set \( r \), the components where the effect of patching \( s \) is analyzed. Run the forward pass on \( x_b \), patch in s, freeze all other activations, and recompute \( r' \), which is the same as \( r\) from the \( x_b \) run except for the direct influence from s to r. Cache \( r' \). (4) Run the model on \( x_b \), and patch in \( r' \) values. Measure the difference in logits between \( x_b \) and \( x_p \) to quantify the effect of \( s \) on  \( r\) in terms of passing the additional signal.

The effect of a patch is measured by the difference in logits (which in the case of cross-encoders, is equivalent to the difference in relevance scores). This algorithm is then iteratively repeated for each important upstream component. For more information, we refer the reader to Wang et al.~\cite{wang_ioi} and Goldowsky-Dill et al.~\cite{goldowskydill_ppatching}.

We begin by applying path patching to track the path of term-matching, the most fundamental component of BM25. To carry this out, we use the TFC1 and STMC1 diagnostic datasets to identify the components responsible for encoding the relevance signal of an additional query term in a document, providing the foundation for our analysis of model behavior.
% The largest increase in logits occurs when the patched components effectively capture and transmit the additional signal, highlighting their importance in the circuit.

%\textit{\textbf{Residual Stream and Computation in Transformers.}} 
%To understand how relevance signals are transmitted through a transformer model, it is important to recap the flow of information in these architectures. Transformers consist of several stacks of multi-headed attention and multi-layer perceptron (MLP) layers, connected through residual streams. Each residual block includes an attention layer followed by an MLP layer, and both layers \textit{read} from the residual stream (via linear projection) and \textit{write} back to it (via another linear projection). %This structure allows intermediate layers to iteratively refine token representations.


%By replacing certain activations (e.g., residual stream at a certain layer for a token position, attention output, mlp output) from the residual stream of \( x(q, b) \) into the residual stream of \( x(q, p) \) into the residual stream of \( x(q, b) \) for the same component, we can isolate how much difference does patching in this one component bring to final relevance score, thereby determining its causal effect on downstream components and logits. 

%the corresponding minimal pairs \(x(q,c)\) (clean input) and \(x(q,p)\) (perturbed input) from Table~\ref{tab:axioms_table}. We intervene on the forward pass of the  \(x(q,c)\), replacing individual activations with activations from  \(x(q,p)\) to see how the replacement affect final relevance score. We will first describe activation patching, which is the basis for path patching as a preliminary for then describing path patching in more detail.


%MAYBE CAN MOVE SOME INTO THE DIAGRAM LATER:
%Given a vector \( v \) from the residual stream (\( \text{resid\_pre} \)), the attention and MLP layers update \( v \) by adding to it, e.g., \( v = v + \text{MLP}(v) \). If we modify some value \( h_p \) in a component, \( v \) will be altered, and downstream components using \( v \) will also be affected. To isolate the direct causal effect of \( h_p \) on specific downstream components, we must prevent indirect effects caused by changes to \( v \). This is achieved by patching every subsequent activation in the forward pass for \( x(q, p) \) unless those activations belong to the components we wish to analyze.

%By selectively patching activations from \( h_p \) into specific downstream components, we quantify how much \( h_p \) directly influences those components (e.g., other attention heads) and how effectively it transmits the additional relevance signal.

%\subsubsection{\textbf{Activation Patching}}
%Let \( h_p \) be some model component vector (resid\_pre or output from the attention heads or mlp) in the cache of activations from  \(x(q,p)\).  If we believe  \( h_p \) is central for encoding additional relevance score and causing increase to the relevance score, we can run a forward pass on \(x(q,c)\) and replace the output vector \( h_c \) with \( h_p \) and continue the forward pass as before. 
%We can then check the logits and quantify how much using \( h_p \) affects the logits. For example, when tracking the flow of TF information using the TFC1 diagnostic dataset, patching components that encode the relevance signal of an additional query term from \(x(q,p)\) will result in the largest increase in logits.

%However, by doing this, it is unclear if \( h_p \) had the effect of directly influencing the logits or influenced the logits through the effect it had on some other model component downstream of it. To separate out the direct effect, we use path patching.

%Path Patching
\begin{comment}
In path patching, we would like to isolate the effect of patching in some value \( h_p \) EVEN TO  some downstream component(s) even other attention heads or mlp layers (not only logits) , e.g. we want to answer the questions which heads are the most importnat for moving that specific relevance signal to other intermediate heads. 

Given a vector v from the resid\_pre, the attention and MLP layers update \( v \) by adding into it, e.g., \( v = v + \text{MLP}(v) \). If we change \( h_p \), \( v \) will obviously be changed as well. Since downstream components use \( v \) as input, if we affect the value of \( v \), the later components will be affected by that change and thus introduce indirect causal effects on the prediction. To avoid this, we have to patch in every activation from the cache for \(x(q,p)\)’s activations if that component would be affected by the change to \( v \) earlier on in the forward pass. If every subsequent component is unaffected by the change from \( h_p \), then only the final logits will be affected by that patch. We can selectively allow the change to \( v \) via \( h_p \) to affect some downstream components other than the logits (like other attention heads) to isolate \( h_p \)’s direct effect on those downstream component, which quantifies how much it encodes additional relevance signal. 

The general path patching algorithm requires four forward passes:

\begin{enumerate}
    \item Cache all of the activations for input \(x(q,c)\).
    \item Cache all of the activations for input \(x(q,p)\).
    \item Select some model component(s) that you want to patch from \(x(q,p)\) to \(x(q,c)\), call this the sender set \( s \). 
    Select some component(s) for which you want to quantify how changing \( s \) affects them (``How does changing \( s \) affect \( r \)?''). 
    Call them the receiver set \( r \). Run the forward pass on \(x(q,c)\), patching in \( s \) with the activations from \(x(q,p)\). 
    Patch in all downstream components in \(x(q,c)\)’s cache, except for those in \( r \). Components in \( r \) get recomputed and now account for the change made by patching \( s \). Let these recomputed values be \( r' \).
    \item Run the model on \(x(q,c)\) again, this time activation patching in the values for \( r' \). Measure the difference in logits between the answers for \(x(q,c)\) and \(x(q,p)\); this tells you how changing the path from \( s \) to \( r \) affects the logits, 
\end{enumerate}

See Wang et al.\cite{wang_ioi} and Goldowsky-Dill et al.\cite{goldowskydill_ppatching} for more details.
\end{comment}


\subsection{Model}
We analyze ms-marco-MiniLM-L-12-v2~\cite{minilm_huggingface} for our BERT-based cross-encoder, a model trained on the MS MARCO dataset, and chosen for its high evaluation performance the MS MARCO Passage Re-Ranking ~\cite{msmarco_dataset} and TREC Deep Learning 2019 datasets~\cite{trec_dataset}. The cross-encoder processes input x, consisting a query \(q\) and a document \(d\) in the format: \texttt{<CLS> q1, q2, ... <SEP> d1, d2, ... <SEP>}. After all transformer layers, the \texttt{<CLS>} token vector is passed to a classifier to produce the logits.

\section{Semantic Scoring Circuit} \label{sec:circuit_section}

% \input{tex_figures_and_tables/circuit_diagram}

Previous work in mechanistic interpretability has identified groups of attention heads whose behaviors generalize across tasks. For example, "induction heads," "previous token heads," and "duplicate token heads"~\cite{olsson_induction} have been shown to play a role in various NLP tasks by completing sub-tasks within larger task-specific circuits. Identifying these heads is therefore crucial. Following the convention of naming head groups based on their mechanisms, we assign names to key components according to their specialized roles in document relevance computation. Figure~\ref{fig:CircuitGeneral} illustrates the uncovered relevance circuit, with details provided below:

\begin{itemize}
    \item \textbf{Matching Heads} generate a \textit{Matching Score} by summing \textit{soft-TF} values for each query term, while accounting for term saturation and document length effects.
    \item \textbf{Contextual Query Representation Heads} distribute soft-TF information of the higher-IDF query tokens among all query tokens to strengthen the higher-IDF tokens' representations.
    \item \textbf{IDF} of vocabulary terms is stored in a dominant \textbf{low-rank vector of the model's Embedding Matrix}. %MLP layers use these embeddings to compute \textit{importance signals} for the BM-Scoring Heads.
    \item Previously listed components send the four BM25 signals to \textbf{Relevance Scoring Heads}, which we hypothesize are summing \textit{Matching Score} weighted by IDF in a BM25 manner.
\end{itemize}

\input{tex_figures_and_tables/circuit_diagram}

\begin{comment}

\begin{enumerate}
    \item \textbf{Matching Heads} generate a \textit{Matching Score} by summing \textit{soft-TF} (describing the extent of semantic similarity) values for each query term, while accounting for term saturation and document length effects.
    \item \textbf{Contextual Query Representation Heads} distribute soft-TF information of the higher-IDF query tokens among all query tokens to strengthen the higher-IDF tokens' representations.
    \item \textbf{IDF} of vocabulary terms is stored in the first \textbf{low-rank component of the model's Embedding Matrix} acquired through SVD. %MLP layers use these embeddings to compute \textit{importance signals} for the BM-Scoring Heads.
    \item \textbf{Relevance Scoring Heads} sum the \textit{Matching Score} for each query token weighted by its respective IDF values in a way analogous to a semantic variant of the BM25 computation.
\end{enumerate}

%Below we show how we discovered each of the previously discussed components, providing evidence that they behave as claimed. 
\end{comment}
As described in \S\ref{methods}, we analyze the critical soft-TF component using path patching, tracing heads from "back" to "end" to align with its iterative nature. We first identify attention heads directly influencing logits and hypothesize they perform BM25-like computations by summing soft-TF weighted by IDF. Tracing further, we uncover Contextual Query Representation Heads and Matching Heads where soft-TF is originally computed. Using other diagnostic datasets, we find term saturation and document length signals intertwine with soft-TF at the Matching Heads. After tracking soft-TF, we apply SVD and intervention and localize IDF values in the first low-rank vector of the Embedding. We localize all BM25-like components in this section and verify that Relevance Scoring Heads perform BM25-like computations in the next section.

%To maintain consistency with the iterative backward nature of path patching, we describe the components from the ``end'' to the ``beginning'' of the model. Specifically, first, we identify the attention heads that directly affect the logits and compute the overall relevance score using both soft-TF and IDF of the query representation (Relevance Scoring Heads). Next, we trace the information flow back to the intermediate components that construct a contextualized query representation (Contextual Query Representation Heads). Then, we demonstrate the sources of soft-TF during the initial query-dependent document representation phase (Matching Heads). Then, we describe the source of IDF residing the first low-rank component of the Embedding Matrix (Embedding Matrix). 

% We begin by hypothesizing the BM-Scoring Heads' function (§3.1.1). We then show how we discovered each of the intermediate components and provide evidence supporting their claimed behavior. In §3.5, we provide causal proof for BM-Scoring Heads hypothesis
%In §4 we further quantitatively confirm the overall computations of the BM-Scoring Heads and evaluate the circuit’s resemblance to BM25.


\subsection{Relevance Scoring Heads}
\label{relevance_scoring_heads}
\input{tex_figures_and_tables/patching_to_logits}

\textit{\textbf{Information Flow.}}
To identify components directly transmitting soft-TF signals to the relevance scores, we path patch to the logits using the TFC1 and STMC1 diagnostic datasets. 
%%To investigate the core mechanisms underlying the BM25 function, we start by analyzing the most critical componet \todo{add citation to support TF as most critical} soft-TF through path patching on the TFC1 and STMC1 diagnostic datasets, and discovered that 

Figure~\ref{fig:BMHeads_patching} highlights the most significant heads (i.e., those causing a $>30\%$ increase in ranking score) for the soft-TF-related axioms: 10.1 (Layer 10, Head 1), 10.4, 10.7, and 10.10. The high correlation in patching importance values (\textit{corr} = 0.9996, \textit{p} < 0.001) and the fact that the four critical heads are shared between TFC1 and STMC1 suggests that the model treats exact and semantic term matches similarly.
% To investigate the core mechanisms of the BM25 function, we begin by analyzing TF, which measures the occurrence of \textit{query term duplicates} in a document. Based on the intuition about the semantic understanding capabilities of transformer-based models~\cite{intro_NRMS_Trabelsi_2021}, we extend this analysis to include semantic matches, where semantically similar terms contribute to the relevance score. We refer to this extension as \textbf{soft-TF} and utilize the TFC1 and STMC1 diagnostic datasets \ref{tab:axioms_table} for Path Patching to the logits.

\input{tex_figures_and_tables/relevance_scoring_head_behavior}

%\subsubsection{\textbf{We hypothesize that BM-Scoring Heads sum the soft-TF of each query token weighted by its IDF to compute the relevance score}} 

% Figure \ref{fig:BMHeads_patching} reveals that (1) the importance of the heads for TFC1 and STMC1 is highly correlated: at the final layers, the TFC and STMC circuits rely on the same set of heads. (2) Path patching to the logits identifies heads 10.1, 10.10, 10.4, and 10.7 as most important in transmitting \textit{soft-TF} information to the final \texttt{[CLS]} token, thereby causing a significant increase in the ranking score. 

\textit{\textbf{Component Behavior.}}
Upon closer inspection of the attention patterns of these heads, we observe that the \texttt{[CLS]} token \textbf{selectively} attends to query tokens and retrieves \textit{soft-TF} based on each query token's IDF value (Figure~\ref{fig:BMHeads_IDF}). Specifically, head 10.1 focuses predominantly on the highest-idf query token: the more uncommon a query token (i.e., the higher its IDF), the more the \texttt{[CLS]} token attends to it to capture its \textit{soft-TF} information. Conversely, head 10.10 primarily attends to low-IDF tokens, while heads 10.7 and 10.4 focus on middle-to-high-IDF tokens. 

To quantify this observation, we first compute the Pearson correlation between each head's attention pattern and the query tokens' IDF values. The results show strong correlations for heads 10.1 (\( r = 0.878 \)), 10.4 (\( r = 0.780 \)), and 10.7 (\( r = 0.798 \)), with \( p < 0.001 \) for all, establishing a statistically significant relationship (threshold: \( p < 0.05 \)) between the attention values from \texttt{[CLS]} and the query tokens' IDF values for 10.1, 10.4 and 10.7. However, Head 10.10 exhibits a weaker correlation (\( r = 0.206 \)). Then, we calculate the average IDF value weighted by each head's attention distribution across query tokens to further validate this trend. Head 10.1 has the highest weighted average IDF (2.704), followed by heads 10.7 (1.983), 10.4 (2.171), and finally head 10.10 (1.457). This ranking quantitatively supports our observation and explains the weaker correlation between 10.10's attention value with IDF values due to its negative association with them. Thus, each head covers different parts of the query based on IDF ranges, and together they send soft-TF information for all the query tokens to [CLS] together using a computation consisting of both soft-TF and IDF. 


\textit{{\textbf{BM25-based Scoring Hypothesis.}}}  Considering that the mentioned computation resembles BM25, \textbf{we hypothesize that the Relevance Scoring Heads perform a BM25-like summation of soft-TF values weighted by IDF of the query tokens.} The model might attribute importance to the output of 10.1, 10.4 \& 10.7, 10.10 from highest to lowest (e.g. controlled by different output projection matrix values) such that if a query token's soft-TF is processed by 10.1, its soft-TF contributes the most to the relevance score. In this way, the query token's soft-TF contribution to the relevance score is proportional to its IDF value. We verify this hypothesis in \S\ref{sec:circuit-function-approximatio}. 
% If a query token has a high IDF, head 10.1 will process its soft-TF information, and model will assign the highest weight to 10.1's output, giving the token's soft-TF information the most significance in the final computation. If the query token has very low IDF, then head 10.10 will process its soft-TF information and contributes just a little to the computation. 


\begin{comment}
Following this linear summation understanding, the attention pattern here is IDF-correlated, and the v vector contains the soft-TF information, so it seems that W\_O (IDF*soft-TF) summed across all the heads. 

Increasing the IDF should lead to an increase in relevance score.

Notably, these heads appear to perform relevance aggregation through linear summation of IDF and soft-TF information. We expand further on this in Section~\ref{sec:circuit-function-approximatio}. 

As previously discussed in \ref{sec:path-patching}, each transformer layer updates the residual stream first by attention block that read from the residual stream via linear projection and write back to it via another linear projection


More specifically, each attention layer is composed of multiple heads, where each head performs two key computations: (1) QK (Query-Key) Circuit: Computes the \textbf{attention pattern}, which determines how much each token attends to every other token. (2) OV (Output-Value) Circuit: Produces the \textbf{value vector (v)}, encoding how each token contributes to the output when attended to. Each head computes the dot product of the attention pattern and \( v \), then multiplies it by \( W_O \) to produce the head's output. All heads' outputs are summed into the residual stream in parallel. 

Following this linear summation understanding, the attention pattern here is IDF-correlated, and the v vector contains the soft-TF information, so it seems that 

Considering that the W\_O at these heads have very similar norms 

the fact that there is a strong correlation between 
\end{comment}
%Thus, each head in the layer covers different parts of the query based on IDF ranges, collectively transmitting soft-TF information for all query tokens to the \texttt{[CLS]} token. Considering that in a transformer structure, the residual stream integrates head outputs (\( \text{Residual Stream} = \text{Heads}(\text{head output} + \text{residual stream}) \)), head output has the tokens' soft-TF information taking into account of their IDF values, and these heads' output is added linearly to the residual stream. These heads appear to perform relevance aggregation through linear summation of TF and soft-TF information. We expand further on this in Section~\ref{sec:circuit-function-approximatio}. closley, analyzing its similarity to BM25. 






%(if the idf is lower, 10.1 which is originally responsible for attending to the highest-idf and maybe [CLS] attribute a high weight to the query token's soft-TF is no longer attending to it. Now it is left to 10.10 (which attends a very low IDF) to process its soft-TF information and maybe [CLS] attribute a low weight to 10.10 so this query token's soft-TF is "less counted".)



\input{tex_figures_and_tables/MH_example}
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=\columnwidth]{paperdiagrams/MH_patching.png}
%      \caption{Path Patching from individual attention heads to BM-scoring heads reveals two groups of important heads for computing or moving soft-TF information. We define Group 1 as Matching Heads and Group 2 as Contextual Query Representation Heads.}
%     \label{fig:MH_patching}
% \end{figure}


\textit{\textbf{Important Upstream Heads.}} Continuing our ``backward pass,'' we path patch to the relevance scoring heads to identify upstream components transmitting soft-TF information. This patching results for TFC1 and STMC1 show high correlation of 1.0 ( \textit{p} <0.001) and reveals a shared set of attention heads, divided into two functional groups: (1) \textbf{Contextual Query Representation Heads} (\S\ref{cqr_heads}) in middle layers, which reorganize soft-TF information among query tokens to construct a contextualized query representation. (2) \textbf{ Matching Heads} (\S\ref{matching_heads}) in early layers, which compute Matching Scores (soft-TF weighted by term saturation and document length effects).

%We also path patch using the IDF Diagnostic Dataset to see which earlier components most strongly affect the BM-scoring heads’ attention on the first document token, we identify the components directly responsible for moving or computing the IDF information. The model identifies the embedding as very crucial, which means IDF exist in the embedding.
\subsection{Contextual Query Representation Heads} \label{cqr_heads}
The first group of heads that send information to Relevance Scoring Heads are termed \textit{contextual query representation heads}: 8.10 and 9.11. These heads aggregate soft-TF information of the higher-IDF query tokens and distribute this information among all query tokens, strengthening their representations for the Relevance Scoring Heads' final computation.

At the Relevance Scoring Heads, the \texttt{[CLS]} token retrieves soft-TF information from all query tokens. Thus, we analyze the attention patterns among query tokens in these two intermediary heads to understand how they modify the query token representation in the residual stream. The attention patterns reveal a distinct pattern: all query tokens in these heads consistently focus on one or two higher-IDF query tokens, with strong correlations to IDF values (9.11: $r=0.829$, 8.10: $r=0.781$ ) with both  \( p < 0.001 \). The evidence suggests that 8.10 and 9.11 act as Contextual Query Representation Heads: they distribute the soft-TF information of higher-IDF tokens across all query tokens. This ensures that when  \texttt{[CLS]} token retrieves soft-TF information at Relevance Scoring Heads, each query token retains a stronger representation of the higher-IDF tokens' soft-TF values.

\textit{\textbf{Important Upstream Components.}} Path patching to 8.10 and 9.11 confirms that these heads also receive soft-TF information from the Matching Heads.

\subsection{Matching Heads} \label{matching_heads}
%We hypothesize that these heads compute the \textit{soft-TF} value and transmit it to the BM-Scoring Heads. In this context, \textit{soft-TF} can be approximated by the sum of the attention patterns across each query token.
\textit{Matching Heads} (0.8, 1.7, 2.1, 3.1, 4.9, 5.7, 5.9, 6.3, 6.5, 7.9, 8.0, 8.1, 8.8) compute and transmit a \textit{Matching Score}, combining soft-TF, term saturation, and document length signals.

\textit{\textbf{Component Behavior.}} 
%Observing the attention pattern diagrams, many heads with patching importance $> 0$ attend to both exact duplicates and semantically similar (soft) matches to varying degrees
%As an example, 1.7 semantically similar (soft) matches to varying degrees shown in 
Patching results reveal that Matching Heads transmit soft-TF information to the Relevance Scoring Heads and Contextual Query Representation Heads. A key question, however, is whether these heads merely propagate soft-TF signals from earlier components or compute the soft-TF themselves. Analyzing their attention patterns indicates that they actively attend to semantic matches and compute the soft-TF. Figure~\ref{fig:MH_example} illustrates this behavior for head 1.7: all query and documents tokens are strongly attending duplicates and mildly attending to semantically-similar terms.

Qualitative examination of the 13 heads with positive patching importance reveals attention patterns similar to those in Figure~\ref{fig:MH_example}. We quantitatively check whether the query tokens are in fact matching semantically by measuring the correlation between the attention values (from any token to any document token) and the tokens' semantic similarity, approximated using cross-encoder's embeddings. The results show that Matching Heads exhibit a significantly higher average Pearson correlation (\textit{corr} = 0.500, p<0.001) compared to other heads (\textit{corr} = 0.132), confirming that these heads function as Matching Heads: at these heads, query tokens actively attend to semantically similar terms, with the attention value serving as an indicator of semantic similarity. Following this intuition, the sum of attention values from a query token to all document tokens can approximate the \textbf{soft-TF} of a query term.

To validate the proposed function of the Matching Heads, we mean-ablate these Matching Heads and observe that the TF perturbed samples’ average value drops from 5.146 to -4.394, and the STMC perturbed samples’ average value drops from -1.998 to -4.611. The drastic drop occurs because ablation deactivates all semantic-matching functions for all query terms. 
%In addition, the ablation results in Figure~\ref{fig:MH_ablation} provide further support for this hypothesis.
%We slightly modified the example shown in Table~\ref{tab:axioms_table} by using GPT4o to generate baseline document with five pronouns (instead of 3) and incrementally substituted these pronouns to create p1 to p5. 

%Since they effectively attend to semantically similar terms and capture these similarity signals in their attention values, we call them Matching Heads.

%According to our hypothesis, an increase in the number of duplicates should result in a monotonic increase in the Matching Score, which can be shown by a positive value of Monotonicity Score, defined as follows:

% introducing more metrics might make things overly confusing, plus it also takes up a lot of space. Let's focus more on the intuition:
%\begin{equation}
%\text{Monotonicity Score} = %\frac{\text{Number of Non-Decreasing %Transitions}}{\text{Total Transitions}}
%\end{equation}

%For example, we found that many 

%To quantify this behavior, we define the \textbf{Soft-TF Correlation Score} for a query token $q_i$ and a document token $d_i$ as the Pearson correlation between the attention values from $q_i$ to $d_i$ and their semantic similarity. \textbf{Matching Heads} are defined as those with patching importance $> 0$ and Soft-TF Correlation Scores in the top 20\% across all heads, which results in the following attention heads:  0.8, 1.7, 2.1, 3.1,  4.9, 5.7, 5.9,  6.3, 6.5, 7.9, 8.0, 8.1, 8.8. These heads exhibit an average Soft-TF Correlation Score of \textbf{0.500}, indicating a significantly higher correlation between attention values and semantic similarity comparing to baseline of \textbf{0.123}, the average across correlation among attention heads. This suggests that Matching Heads effectively capture semantic similarity signals, and the sum of attention across all document tokens for a query token may serve as soft-TF.
\begin{comment}
\begin{figure}[!ht]
    \centering
    % Upper plot
    \includegraphics[width=\columnwidth]{paperdiagrams/MH_furtherpatching_1.png}
    \vspace{0.5em} % Small vertical space between the plots
    % Lower plot
    \includegraphics[width=\columnwidth]{paperdiagrams/MH_furtherpatching_2.png}
    \caption{
        The upper plot shows the average ranking score increase when patching each Matching Head, revealing a characteristic TFC2 curve with a sharp increase as occurrences transition from 0 to 1, followed by a milder increase from 1 to 5. The lower plot zooms in on the 1–5 occurrence range, highlighting the slightly increasing trend for most Matching Heads and their respective monotonicity scores.
    }
    \label{fig:MH_furtherpatching}
\end{figure}
\end{comment}





\input{tex_figures_and_tables/MH_other_signals}

\textit{\textbf{Matching Score Contains Term Saturation and Document Length Signals.}}
The average Soft-TF Correlation Score of Matching Heads (0.500) indicates that their attention values from query tokens to document tokens encode not only semantic similarity but also additional signals. To examine whether these attention values capture term saturation or document length effects, we analyze their behavior under using two diagnostic datasets: (1) TFC2 (increasing occurrences of a query term), and (2) LNC1 dataset (increasing number of irrelevant sentences to simulate the isolated effect of longer document length).


%\begin{figure}[!ht]
%    \centering
%    \includegraphics[width=\columnwidth]%{paperdiagrams/MH_furtherpatching_2.png}
%    \caption{The average ranking score increase shows a drastic rise from 0 to 1 occurrence for each Matching Head, while the 1–5 occurrence range exhibits a slight increasing trend. To better analyze this, we zoom into the 1–5 range, highlighting the increasing trend for most Matching Heads.}
%    \label{fig:MH_furtherpatching}
%\end{figure}

We define \(x_{\text{selected}}\) as the selected query term, which is the duplicate term in TFC2 samples shown in Table \ref{tab:axioms_table}. Similarly, \(x_{\text{others}}\) refers to document tokens that are not duplicates of any query token. For each case, we calculate the normalized sum of attention values from all query tokens to \(x_{\text{selected}}\) and \(x_{\text{others}}\), representing the ``total semantic match'' of the matched token or the average across unmatched tokens, respectively.

As the occurrences of \(x_{\text{selected}}\) increase, its average attention value grows (Figure~\ref{fig:MH_other_signals_tfcs}). On the other hand, the attention for \(x_{\text{others}}\) remains relatively constant, which aligns with our soft-TF understanding: only document tokens that are semantically similar to a query token get nonzero attention values, proportional to the extent and frequency of semantic similarity. Furthermore, for most heads, the TFC2 trend is preserved in the attention values. For LNC1, injecting more irrelevant sentences leads to an increase in the attention value of all query tokens, regardless of whether they match specific query terms (Figure \ref{fig:Discussion_doclen_attn}). Notably, this \textit{increase} in attention occurs even as the overall relevance score \textit{decreases} (as expected from LNC1), suggesting that attention values encode mixed and composite signals of soft-TF, term saturation, and document length, which the model has learned to disentangle to produce appropriate relevance score changes. The varying degrees of influence on the heads' attention values shown in Figure~\ref{fig:MH_other_signals_tfcs} and~\ref{fig:Discussion_doclen_attn} suggest that some Matching Heads play a more significant role in regulating these effects, and they may collectively approximate the ideal term effects. Thus, we define this complex attention value as the \textbf{\textit{Matching Score}} to reflect that it encapsulates soft-TF, term saturation, and document length signals, three important signals of BM25.


\textit{\textbf{Important Upstream Components.}}
Due to time constraint, we further path patch to the three most important Matching Heads (7.9, 4.9, 2.1) to track how soft-TF flows into them. We find that the embedding exhibits the most direct influence, while previous attention and mlp layers exhibit very little influence. This completes our tracing of soft-TF path, confirming that the Matching Heads are the original ``generator'' of soft-TF as query token attends to soft matches to compute the Matching Score.

% Path Patching applied to the \textit{v}-vectors of BM-scoring heads on soft-TF diagnostic datasets reveals two other critical intermediaries—Contextual Query Representation Heads 9.11 and 8.10—that facilitate the flow of soft-TF information. 
%Experiments and attention diagram analysis show that these heads share key similarities with Relevance Scoring heads: (1) path patching confirms that heads 9.11 and 8.10 get soft-TF information also from the Matching Heads, and (2) they exhibit highly correlated attention patterns with Relevance Scoring heads 10.1 and 10.7 (Pearson\_corr(9.11, 10.1) = 0.8910, Pearson\_corr(8.10, 10.7) = 0.7082): in Relevance Scoring Heads, [CLS] attends to query tokens based on IDF. 


%As we show earlier that IDF influence which tokens the Relevance Scoring Heads process the soft-TF information for, the high correlation suggests that IDF influences which query tokens these heads prioritize. Unlike BM-scoring heads, 9.11 and 8.10 do not write directly to the [CLS] token. Instead, they refine and organize soft-TF signals from the Matching Heads before passing them to the BM-scoring heads for BM25-style computations. This additional step likely integrates contextual information, enhancing the relevance scoring process beyond traditional BM25-like mechanisms. 


%\input{tex_figures_and_tables/model_edit}


\subsection{IDF in the Embedding Matrix}
\label{IDF}
% Previously, we describe the flow of soft-TF information in the model. 
In addition to soft-TF, IDF is a critical signal for Relevance Scoring Heads. Here, we use SVD and low-rank intervention to track where IDF is stored or computed.
%\subsection{Embedding Matrix}
%or path patching experiment leads us to determine the embedding matrix is important, but explaining this path patching takes a lot of space

\textit{\textbf{IDF exists in the first low-rank component of the embedding matrix.}}  Previously, Choi et al. ~\cite{choi_idf} provide correlational evidence of IDF information existing in the embedding matrix. We decompose the embedding space into orthogonal components using Singular Value Decomposition (SVD). Recall that the embedding matrix \( W_E \) is structured such that each row corresponds to a word's representation in the vocabulary. The columns of \( W_E \) represent different dimensions of this vector space, capturing various latent features of the words.

SVD allows us to decompose a complex matrix into a sum of rank-1 matrices, ordered by their contribution to the overall structure of the matrix. A rank-1 matrix is the outer product of a column and row vector, capturing a single direction in the data. This decomposition enables us to represent \( W_E \) in orthogonal directions, allowing us to analyze the dominant directions in an easily interpretable manner due to their rank-1 structure. Mathematically, the SVD of \( W_E \) is expressed as:
\[
W_E = USV^T = \sum_{i=1}^{r} \sigma_i u_i v_i^T
\]
Here, \( \sigma_i \) are the singular values, which quantify the importance or strength of each rank-1 component; \( u_i \) and \( v_i \)  left and right singular vectors, respectively, forming orthonormal bases that capture patterns in the row (token embeddings) and column spaces (feature dimensions); and \( r \) is the rank of \( W_E \), representing the number of non-zero singular values and  independent directions in the matrix. By focusing on the largest singular values (\( \sigma_i \)) and their corresponding singular vectors (\( u_i, v_i \)), we can study  whether IDF is stored in dominant components of the Embedding. 

Notably, we find that the rank-1 vector with the highest singular value, we call \( U_0 \), has a large negative pearson correlation (\(-71.36\%\)) with the IDF value from the MS-MARCO train set~\cite{msmarco_dataset}: higher values in this low-rank component for a given vocabulary token correspond to lower IDF values for that token. This suggests that IDF information is encoded prominently within the most significant low-rank component of the embedding matrix.

To validate this correlation, we perform an intervention in the next section to demonstrate that the IDF values from \( U_0 \) have a causal effect on downstream components (i.e., relevance scoring heads) and thus, the overall relevance computation.


\input{tex_figures_and_tables/model_editing_method_diagram}
\textit{\textbf{IDF intervention shows causal evidence that the model uses IDF in \( U_0 \) for relevance computation.}}
Given previous understanding on \( U_0 \), we can interpret \( U_0 \) as a 1-D IDF dictionary, where \( \text{idx}(q_i) \) corresponds to the vocabulary index of \( q_i \), mapping to its respective IDF value. Modifying the value at \( \text{idx}(q_i) \) in \( U_0 \) allows us to adjust the importance of the Matching Score for \( q_i \) (Figure~\ref{fig:model_edit_method}). 

If the model uses the IDF values encoded in \( U_0 \), then modifying these values should result in corresponding changes to the ranking score. Specifically, according to the BM25-based Scoring Hypothesis in \S\ref{relevance_scoring_heads}, there is a linear relationship between the IDF of a query token \( q_i \) and the relevance score: increasing the IDF of \( q_i \) should increase the relevance score. We have shown that \( U_0 \) is rank-1 and negatively correlated with IDF. If the model indeed uses the IDF values encoded in \( U_0 \), we can increase \( \text{IDF}(q_i) \) by decreasing qi's value in U0, which would directly increase the relevance score. The converse should hold true for decreasing IDF. %Furthermore, we also take 10.1 and evaluate whether [CLS] attention on the selected query token is correspondingly affected.

One way to observe whether this is happening is to look at very controlled, minimal examples that we curate specifically to fully observe the effect. Using our base dataset, we create two documents, \texttt{doc1} and \texttt{doc2}, containing distinct repeated tokens: first token (\texttt{tok1}) and second token (\texttt{tok2}), respectively (e.g., query: “computer science department number”, \texttt{doc1}: “computer computer computer”, \texttt{doc2}: “science science science”). Then, we can edit the IDF of \texttt{tok1} and evaluate the IDF editing effect on the \texttt{doc1} and \texttt{doc2} relevance scores.
\input{tex_figures_and_tables/model_editing_result}

Figure~\ref{fig:model_edit_result} offers support for the direct causal relationship: across most scaling factors, larger increase (or decrease) in tok1's IDF result in larger score increase (or decrease, respectively) for \texttt{doc1}. The largely monotonic trend not only offers causal evidence for the model's use of \( U_0 \) to acquire IDF values but also shows preliminary evidence supporting that the model computes a sum of soft-TF weighted by IDF values (\S4.1).


Furthermore, the discovery of the low-rank nature of the IDF signals enables targeted interventions. Our intervention experiment demonstrates that modifying IDF values allows for precise control over the importance of specific terms within the cross-encoder.

%and  (b) head 10.1's attention on the edited query token (recall we previously hypothesize in §3.1.1 that 10.1 attends to the high-IDF query token and model attributes highest weight to its output).
%but also shows preliminary evidence that the Relevance Scoring Heads uses these IDF values to determine which query tokens they each attend to, and thus influencing the relevance score because the soft-TF information from the query token mostly attended by 10.1 will contribute the most.
%. Moreover, changes in relevance scores align precisely with changes in the attention scores of the BM-Scoring Head (10.1) on the edited query token: increasing/decreasing \( U_0[\text{idx}(q_i)] \) increases/decreases 10.1's attention on the query token correspondingly. 


%Using model-editing, we offer causal proof that (1) BM-Scoring Heads use \( U_0 \)'s IDF values 
%(2) Attention of 10.1 BM-Scoring Heads are attending to query tokens based on the IDF: the higher the IDF, the more it attends to 
%(2) BM-Scoring Hypothesis (§3.1.1):  the Multi-head attention mechanism BM-Scoring Heads sum the Matching Score for each query token weighted by its respective IDF values in a way analogous to BM25.

%\textbf{Model Editing shows BM-scoring Heads use IDF values from the low-rank vector in their linear relevance computation.}
%We further verify that the first low-rank component U0 indeed carries the causally important signals for model relevance using model-editing (Figure~\ref{fig:ModelEdit_method}). 

\section{Validation of BM25-like Computation}

\label{sec:circuit-function-approximatio}
After we established where all the BM25-like components were initially computed or stored and how they were passed to Relevance Scoring Heads, we now assess whether the Relevance Scoring Heads perform a BM25-style computation as hypothesized in \S4.1. To do this, we first formalize the hypothesized function of the heads and the information flowing into them as a BM25-style linear function. Next, we evaluate the linear model's ability to reconstruct the cross-encoder scores by examining how well the hypothesized linear model fits the data. 

%\input{tex_figures_and_tables/table_validation}

%Given the above understanding that the model takes in notions of TF, IDF, term saturation, and document length, we notice that the way the model computes relevance bears a strikingly similar resemblance to a well known traditional model of relevance, BM25 (cite). Specifically, the final components in the circuit, the relevance scoring heads, seem to aggregate these relevance features to determine the final relevance score. Similarly to BM25, the relevance scoring heads (talk through behavior to draw a high-level comparison to BM25). In this section, we analyze how well the model's relevance scores can be approximated by a BM25-like representation of the Semantic Scoring circuit.

%Given that the model leverages BM25 components in its relevance computation circuit, we now move onto quantitatively examining how closely the circuit aligns with a BM25-like scoring mechanism. 
% In Section 3, we have shown evidence of all important circuit components except the BM-Scoring Heads, which we earlier hypothesized that they sum the Matching Score for each query token weighted by its IDF values similar to BM25 (§3.1.1). In this section, we will validate its similarity to BM25 and the accuracy of our circuit understanding. 

%In this section, we further quantitatively confirm the overall computations of the BM-Scoring Heads and evaluate the circuit’s resemblance to BM25.

\subsection{Formalizing the Hypothesized Function}
Specifically, as proposed in the BM-based Scoring Hypothesis, the relevance scoring heads appear to compute a summation of Matching Scores weighted by IDF. The Matching Score incorporates soft-TF, along with term saturation and document length signals, all of which are components of the BM25 function. If this hypothesis holds—that these components interact in a BM25-like manner—then the semantic scoring circuit can be expressed as a linear function:

{\small
\[
\sum_{i=1}^{N} \text{linear\_combo}\big(-U_0(q_i), \text{MS}_{\text{total}}(q_i, d_j), 
[-U_0(q_i) \cdot \text{MS}_{\text{total}}(q_i, d_j)]\big)
\]

where the components of the linear combination are defined as follows:
\begin{enumerate}
    \item \( U_0(q_i) \): The value of \( q_i \) in the \( U_0 \) vector, representing the model's interpretation of \( q_i \)'s IDF.
    \item \( \text{MS}_{\text{total}}(q_i, d_j) \): The total Matching Score, computed as the sum of Matching Scores from individual heads (\( \text{MS}_{H_k} \)) weighted by learned weights \( \alpha_k \). Each Matching Score represents the sum of attention values from \( q_i \) to all document tokens:
    \[
    \text{MS}_{\text{total}}(q_i, d_j) = \sum_{k=1}^{13} \alpha_k \cdot \text{MS}_{H_k}
    \]
    \item \( -U_0(q_i) \cdot \text{MS}_{\text{total}}(q_i, d_j) \): The interaction term, analogous to the product of IDF and TF in BM25.
\end{enumerate}
}





\begin{comment}
{\small
\[
\sum_{i=1}^{N} \text{linear\_combo}\big(-U_0(qi),  \text{MS}_{\text{total}}(q_i, d_j), \, 
[-U_0 \cdot \text{MS}_{\text{total}}(q_i, d_j)]\big),
\]
\[
\text{MS}_{\text{total}}(q_i, d_j) = \sum_{k=1}^{13} \alpha_k \cdot \text{MS}_{H_k}
\]
}
The three parts within each linear combination consists (1) \( U_0 \) (qi) is the value of qi in  \( U_0 \) vector that stores IDF, (2) the total Matching Score \( \text{MS}_{\text{total}}(q_i, d_j) \), which is further decomposed into Matching Score (sum of attention values from \( q_i \) to all document tokens) from each Matching Head weighted by a learned weight \( \alpha_k \).  (3) the interaction between these \( U_0 \) (qi)  and \( \text{MS}_{\text{total}}(q_i, d_j) \) that BM25 function also supports. 
\end{comment}
Since the formalization contains not only the hypothesized computation function but also \( U_0 \) and \( MS \) from earlier subparts of the circuit, the linear model is an effective representation of the circuit. 

\subsection{\textbf{Assessing the Linear Model Fit}}
Given the linear representation, we can determine whether our hypothesis holds by comparing the linear model’s performance against the cross-encoder’s actual relevance scores. 

We first train a linear regression model using our base dataset. As the number of coefficients in our linear regression scales quickly with each additional included query term, we apply a query length cutoff to five tokens in this initial experiment to first establish feasibility of representing the circuit in this manner. When performing forward passes on the samples, we extract the following for each query token: (1) the \(\text{MS}\) (Matching Score), calculated as the sum of the query token's attention over document tokens from the 13 Matching Heads, and (2) the token's corresponding value in \(U_0\), obtained by applying SVD to the embedding matrix. These serve as the input features (``x'') for the linear regression model, while the cross-encoder's ranking scores are used as the target values (``y''). We then split the data into an 80/20 train-test set to evaluate how well the linear representation of the Semantic Scoring Circuit predicts the cross-encoder's relevance scores.

The linear regression model achieves a high pearson correaltion (\textbf{corr = 0.8157}, \textit{p} < 0.001) with ground-truth relevance scores, showing that it effectively captures the core components of the cross-encoder's scoring mechanism in a simplified and interpretable form. This correlation surpasses that of the traditional BM25 scoring function under optimized parameters (\(k=0.5, b=0.9\); corr = 0.4200, \textit{p} < 0.001), which demonstrates the discovered \( U_0 \) and \( MS \) components more effectively capture the signals that the cross-encoder utilizes for ranking. 


\subsection{\textbf{Generalization across IR datasets}}
We previously test our linear model on an MSMARCO-based dataset with a fixed number of query tokens as an initial proof of concept. Now, we ask the question, how well does this linear model represent the cross-encoder's relevance computation on datasets it was not trained on? In this section, we extend our analysis to 12 different IR datasets\footnote{Due to computational resources, we use a subset of BEIR datasets  ~\cite{beir} (ArguAna, Climate-FEVER, FEVER, FiQA-2018, HotpotQA, NFCorpus, NQ, Quora, SCIDOCS, SciFact, TREC-COVID, Touche-2020), but the chosen ones sufficiently cover a wide variety of domains.} and investigate various query lengths.
% To further validate the consistency, generalizability, and accuracy of the SemanticBM function, we extended our analysis across 12 BEIR datasets~\cite{beir} spanning diverse domains to show the SemanticBM consistently reflects the cross-encoder's ranking process for various query lengths.

\textit{\textbf{Dataset.}}
Since our linear model relies on fixed features based on query length, we stratify the dataset based on query length.  To ensure sufficient samples, we aggregate samples from 12 datasets and only retain groups with more than 500 samples. We incorporate groups with query lengths from 1 to 22, which correponds to the 99.95th percentile of MSMARCO's query length distribution to exclude outliers with excessively long queries that the model has rarely seen during training. This process results in a total of 19 groups, comprising 278194 samples, with an average group size of 14641.790 samples. While this approach may be unconventional for retrieval, our goal is to simply demonstrate consistency between the linear model's relevance score and the cross-encoder to confirm the hypothesized function of the Relevance Scoring Heads.

First, as cross-encoders are typically used for re-ranking tasks, we follow the classic re-ranking setup by first retrieving a candidate set of documents (top 10) with BM25 over all queries across the test collection of the 12 datasets. These candidate sets are then split into train-test groups using an 80/20 ratio.

\textit{\textbf{Experiments and Results.}}
We train 22 linear regression models, one for each query length (1-22) to predict the cross-encoder's relevance scores. We evaluate each model using: (1) Pearson Correlation: quantifies the correlation between predicted and actual relevance scores; (2) Spearman Rank Correlation: assesses the consistency of the ranked lists between predictions and cross-encoder outputs; (3) NDCG@10: measures ranking performance and ensures no significant performance discrepancies. 
% These metrics collectively provide a comprehensive assessment of the alignment between the linear model's predictions and the cross-encoder's outputs. 
We include BM25 and a randomized set of linear regression features as baselines.

% \input{tex_figures_and_tables/table_validation_test}

Table \ref{tab:table_validation} shows the results, and we report median values to account for observable skewness caused by outliers in the data. We observe a Pearson correlation of ranking scores with a median of \textbf{0.8401} (median \textit{p} < 0.001), a Spearman rank correlation of \textbf{0.7619} (median \textit{p} = 0.072), and an \textbf{88.4\% alignment} with cross-encoder performance in terms of NDCG@10. 

The experimental results confirm the hypothesized function of the Relevance Scoring Heads (\S\ref{relevance_scoring_heads}). Since this linear model summarizes the whole circuit as it is structured to incorporate both the computation and the necessary components, the high correlation with the cross-encoder's performance shows that our circuit understanding has captured the core part of the cross-encoder's relevance ranking mechanism.

\input{tex_figures_and_tables/table_validation_test}

% We used BM25 to retrieve the top 10 candidate documents per query and extracted the cross-encoder’s token-level features (\(U_0, MS\)). Samples were filtered to include query lengths between 1 and 22, grouped by length, and restricted to groups with at least 500 samples for statistical robustness. Within each group, we performed a 20/80 train-test split and trained a linear regression model to predict the cross-encoder’s \( \text{model\_score} \) from the feature vectors. We evaluated the following metrics: (1)\textbf{ Pearson correlation }between the cross-encoder’s ranking scores and SemanticBM’s predictions. (2) \textbf{Spearman rank correlation} computed on a per-query basis using the top-10 BM25 documents. (3) \textbf{NDCG@10}, calculated using a global \( qrels \) dictionary combining datasets, for both the cross-encoder’s original document scores and SemanticBM’s predicted scores.

% Figure 10 summarizes the results: we observe a Pearson correlation of ranking scores with a median of \textbf{0.8401}, a Spearman rank correlation of \textbf{0.7619}, and an \textbf{88.4\% alignment} with cross-encoder performance in terms of NDCG@10. This finding underscores the strong alignment between our Semantic-BM approximation and the cross-encoder's more complex, nonlinear computations, despite the inherent simplicity of the Semantic-BM model.

% \section{Discussion} \label{discussion}

% In this section, we present insights from our circuit analysis on how cross-encoders determine relevance and propose potential applications of these findings.

\section{Insights into Model Mechanisms}


% \textit{\textbf{\todo{Something about contextual representations?}}}

% \textit{\textbf{Title about IDF}}
% The way IDF information is encoded in BERT-based NRMs has significant implications for improving retrieval. \citet{choi_idf} provided correlational evidence of IDF encoding in BERT embeddings, and in this work, we build on their findings by offering causal evidence that this information resides in the first low-rank matrix from SVD decomposition. While \citet{choi_idf} hypothesized that using ground truth IDF values could directly improve the performance of BERT-based NRMs, they had limited success. Similarly, our initial experiments manipulating ground truth IDF values in the low-rank matrix produce comparable results (\todo{ref section}). However, we believe there may be alternative ways to leverage these insights and propose additional downstream applications in this section, which we propose in \todo{ref conclusion section}.


\textit{\textbf{A two-stage process for query-document interactions.}}
Our circuit analysis reveals that query-document interactions happen in two stages, bridging previous conflicting perspectives on how BERT cross-encoders form query representations. In the early to mid layers (0-8), matching heads attend to document terms that are exact or ``soft'' matches to query terms (\S\ref{matching_heads}). This first stage aligns with \citet{qiao2019understanding}, who find that BERT focuses on exact query term matches or close paraphrases in the document, suggesting query-dependent document representations. 

In mid to late layers (8-9), document representations are aggregated into query token representations by Contextual Query Representation heads (Section \ref{cqr_heads}), consistent with \citet{zhan} who argue that document tokens are largely query-independent. These two stages offer an explanation to previous conflicting perspectives on the role of query-document interactions: document representations initially have a query-specific relevance signal, but later, this signal is transferred to the query tokens. This dual-stage interaction process provides a more nuanced understanding of how query-document interactions evolve across cross-encoder layers.


\textit{\textbf{Is (semantic) BM25 the whole story?}}
While the Semantic Scoring circuit bears a strong resemblance to BM25, there also may be other factors influencing relevance computation that are out of the scope of investigation for this work. In this work, we compare it to BM25 because of its well-known status and the fact that previous research has shown that NRMs encode a BM25-like relevance signal \cite{wang2021bert, rau2022different, yates2021pretrained, macavaney2022abnirml}. However, there are other term-matching models (e.g., TF-IDF, QL, etc.) that also rely on similar retrieval heuristics, and it is possible that the model implements these as well. 

Additionally, we note that our BM25-like approximation of the Semantic Scoring circuit does not fully approximate the cross-encoder's true relevance scores. There are a couple potential explanations for this: (1) the nonlinearity of neural models, which our linear regression model does not capture, (2) potentially unexplored components such as additional attention heads that could be uncovered through a different choice of dataset or multi-layer perception (MLP) layers not covered in our analysis, and (3) the incorporation of signals beyond traditional relevance heuristics, such as real-world knowledge learned during pre-training \cite{macavaney2022abnirml}. We leave investigation of these avenues for future work.



\textit{\textbf{Towards a holistic approach in axiomatic analysis.}}
Previous research investigating NRMs' adherence to IR axioms shows that BERT does not adhere to most of them, leading to concerns about the limitations of current axiom definitions and the need for new ones \cite{camara2020diagnosing}. Our findings offer an explanation for these shortcomings, suggesting that the challenge in axiomatic analysis lies with the way it has been applied, rather than the axioms themselves. BERT's ``rediscovery'' of BM25 indicates that it leverages a combination of retrieval heuristics to compute relevance, whereas previous analysis treats axioms independently. Going forward, we may consider analyzing axioms in combinations to better reflect the multidimensional nature of relevance. 


% short version of conclusion
\section{Conclusion} \label{conclusion}

In this work, we mechanistically uncovered the core components of relevance scoring pathway of an NRM, revealing their ability to leverage contextual representations to perform a semantic-variant of BM25. Our granular analysis paves way for future directions. For example, future work could build on our IDF editing technique (\S\ref{IDF}) to adjust term importance, offering more control over model behavior and leading to improvements in model robustness against adversarial attacks, personalization, and bias mitigation. In addition, our evidence about the centrality of IDF information in the model's embedding matrix raises the question of how crucial non-embedding weights are for task generalization, and future work could investigate whether fine-tuning only the embedding matrix with domain-specific IDF values can achieve robust performance while being far more parameter efficient. Finally, in this work, we analyze the behavior of a single cross-encoder in order to deeply analyze its mechanisms, and future work should investigate their generalizability to other neural IR models. Overall, identifying universal components or architecture-specific mechanisms will advance interpretability and inform the design of controllable and efficient neural ranking systems for real-world applications.

% In this work, we analyze the mechanisms of a single cross-encoder in order to deeply analyze its mechanisms. Future work should investigate whether other neural IR models behave similarly. Identifying universal components or architecture-specific mechanisms will advance interpretability and inform the design of controllable and efficient neural ranking systems for real-world applications, as neural IR systems play an increasingly central role in information retrieval.
\begin{comment}
\textit{\textbf{Controllable Models.}}
Mechanistic interpretability research has shown that understanding and localizing model components can facilitate performance improvements, such as through model editing. For example, \citet{meng_rome} use causal interventions to identify critical neuron activations, enabling Rank-1 Model Editing to enhance factual recall in transformers. Future work could build on these insights to manipulate IDF values in the embedding matrix in IR models to adjust term importance, offering more control over model behavior and leading to improvements in model robustness against adversarial attacks, personalization, and bias mitigation.


\textit{\textbf{Parameter Efficient Fine-Tuning.}}
The centrality of IDF information in the model's embedding matrix raises the question of how crucial non-embedding weights are for task generalization. Previous research shows that the embedding matrix carries significant representational power such that models initialized with random weights can still perform reasonably well in various NLP tasks \cite{random_transformers}. Future work could investigate whether fine-tuning only the embedding matrix with domain-specific IDF values can achieve robust performance while being far more parameter efficient.
\end{comment}



% \subsection{Potential Downstream Applications}

% \todo{Brief intro about IDF insights from embedding matrix} In §3 and §4, we establish that IDF values for tokens are encoded in the first low-rank matrix \( U_0 \), and that IDF plays a central role in query term weighting within the Semantic Scoring Circuit.

% The way IDF information is encoded in BERT-based NRMs has significant implications for improving retrieval. \citet{choi_idf} provided correlational evidence of IDF encoding in BERT embeddings, and in this work, we build on their findings by offering causal evidence that this information resides in the first low-rank matrix from SVD decomposition. While \citet{choi_idf} hypothesized that using ground truth IDF values could directly improve the performance of BERT-based NRMs, they had limited success. Similarly, our initial experiments manipulating ground truth IDF values in the low-rank matrix produce comparable results. However, we believe there are alternative ways to leverage these insights and propose additional downstream applications in this section.


%As shown §3.4.2, \( U_0 \) is a 1-D IDF dictionary mapping to its respective IDF value. Modifying the value at \( \text{idx}(q_i) \) in \( U_0 \) allows us to adjust the importance of the Matching Score for \( q_i \). The full Model Editing method is presented in Figure~\ref{fig:ModelEdit_method}.

% \textit{\textbf{Controllable Models.}}
% Research in mechanistic interpretability has demonstrated that localizing and understanding circuit components for specific tasks can facilitate model editing to enhance task performance. For instance, \citet{meng_rome} employ causal interventions to identify neuron activations critical to factual predictions in autoregressive transformer language models and leverage this insight to develop Rank-1 Model Editing, improving zero-shot factual recall. Leveraging these insights, we propose how our findings can enable controllability and personalization in IR models. As shown in §3.4, the IDF ``dictionary'' in the first low-rank matrix \( U_0 \) can be directly manipulated. Specifically, IDF values can be adjusted to \textit{upscale} or \textit{downscale} the importance of specific terms, effectively introducing ``tuning dials'' for targeted control over model behavior. Preliminary experiments (beyond the scope of this paper) suggest that this approach may not only improve model robustness against adversarial attacks but also enable other applications, such as model personalization or bias mitigation.

% \textit{\textbf{Parameter Efficient Fine-Tuning.}}
% The centrality of IDF information within the circuit of the model and the fact that it exists in the first low-rank matrix of embedding raises an intriguing question: how critical are the non-embedding weights for task generalization, (e.g. finetuning NRMs for other domain-specific datasets)? Previous research has demonstrated that the embedding matrix itself carries significant representational power such that models initialized with random weights can still perform reasonably well in various NLP tasks \cite{random_transformers}. This insight has practical implications for efficient fine-tuning strategies: rather than updating the entire parameter space, fine-tuning the embedding matrix to better encode IDF values aligned with the retrieval domain may suffice. Our initial experiments provide promising evidence that this approach could achieve robust performance while being far more parameter-efficient.




\begin{comment}
… This provides new evidence corroborating that deep language models can represent the types of ranking features traditionally believed necessary for language processing, and moreover that they can model complex interactions between these based on the BM25 function.
Future work: 
\begin{enumerate}
    \item \textbf{Generalization Across NRMs:} While this study focuses on a single cross-encoder model, our findings suggest potential generalization to other models. Prior research has indicated that various neural architectures encode TF and IDF information, suggesting that the BM25-like circuit identified here may be a shared mechanism across models, albeit manifesting in different heads or layers.
    \item \textbf{Semantic Matching Mechanisms:} Beyond functional differences such as controlling for TFC2 and LNC1, further investigation is needed to understand how Matching Heads capture semantic matches and how they differ in representing these matches.
    \item \textbf{Nonlinearity Beyond BM25:} While BM25 provides a foundational structure, neural models likely incorporate additional non-linearities and components. For example, positional bias—a feature absent in BM25—may play a significant role. Future studies could leverage the weights learned by the SemanticBM linear approximator to decode how query token positions influence scores and identify components that modulate positional bias for more precise control.
    \item \textbf{Downstream Task Potential:} Investigating how BM25-inspired mechanisms uncovered in this study can be extended or adapted for downstream tasks could lead to more efficient and robust models for various applications.
\end{enumerate}
\end{comment}




\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

\begin{comment}

\appendix
\section{path patching full diagram?}
\input{tex_figures_and_tables/path_patching_alternative_diagram_appendix}

\section{Ablation Experiment Result}
% \begin{figure}[!ht]
%     \centering
%     \includegraphics[width=\columnwidth]{paperdiagrams/MH_ablation.png}
%     \caption{Mean-ablating the Matching Heads causes a substantial decrease in the ranking scores for both \( x_{\text{orig}} \) and \( x_{\text{perturbed}} \), further confirming the hypothesized function that the Matching Heads are writing soft-TF to the residual stream. The drastic drop occurs because ablation deactivates all semantic matching functions for all query terms. %Furthermore, the identified Matching Heads account for approximately \textbf{90\% of the total loss} induced by ablating all heads with similar matching patterns. This result reinforces the central role of the Matching Heads in capturing and sending semantic term frequency information to the model's final scoring mechanism.
%     }
%     \label{fig:MH_ablation}
% \end{figure}
\section{Detailed Path Patching Results}

\section{Further Patching Experiment to confirm Matching Heads are not just writing a Binary Signal of Existence}
The signals written by most Matching Heads increase as the number of duplicate tokens rises, rather than remaining constant, proving that these signals are discrete rather than binary. To demonstrate this, we designed an activation patching experiment (\S2.3) that patches only the Matching Heads using the TFC2 Diagnostic Dataset. Figure~\ref{fig:MH_other_signals_tfcs} shows that for most Matching Heads, increasing the number of duplicate tokens results in a monotonically increasing pattern in their output signals. This behavior further supports our conclusion that Matching Heads compute and encode soft-TF signals rather than merely relaying binary information. 

\section{Preliminary Experiments for Downstream Applications}
\subsection{IDF: Further patching experiment}confirms that the IDF-relevant low-rank direction uncovered by SVD is demonstrably causal, contributing to the final relevance computation via the model’s MLP layers.
\begin{figure*}%[!ht]
    \centering
    \includegraphics[width=7in]{paperdiagrams/IDF_patching.png}
    \caption{Given an original document, we create a perturbed document by swapping in a high-IDF token at the first position for activation patching. Given our earlier finding that BM-scoring heads’ attention strongly correlates with token IDF values, changing the first token’s IDF produces a sizable shift in attention at that position. By identifying which components most strongly affect the BM-scoring heads’ attention on the first document token, we identify the components directly responsible for moving or computing the IDF information. Results revealed: (1) Layer 0: The IDF signal is already present here. (2) MLP Layers 8, 9: These layers further amplify the IDF signal.}
    \label{fig:IDF_patching}
\end{figure*}
\subsection{SVD Model Editing on Perturbation Dataset} We also look at the effect of model-editing on \textbf{(b) natural TFC1 perturbation dataset}.  Unlike the synthetic setting, real documents include various other tokens. Thus, we hypothesized that decreasing the importance of \(q_{\text{selected}}\) causes the \([CLS]\) token to shift relative attention to \(q_{\text{unselected}}\) tokens. Thus, we do not expect a strictly monotonic shift in relevance scores as for (a), but  we should observe a narrowed relevance score gap between perturbed and original sample.
\subsection{Model Hypothesis Testing}
Total Groups: 19
Total Samples: 278194
Average Group Size: 14641.78947368421
Median Group Size: 12980.0
Min Group Size: 1139
Max Group Size: 30850
Standard Deviation: 9186.571096376054
\begin{figure}[!ht]
    \centering
    \includegraphics[width=0.4\textwidth]{paperdiagrams/model_hypo_group_size.png}
    \caption{Group size, grouped by query length} 
    \label{fig:BMHeads_patching}
    % \vspace{-1em}
\end{figure}
\subsection{SVD Model Editing}

In §3.4.2, we introduced a model editing technique to dynamically control the importance signal of query tokens in, and here we present encouraging evidence to use the model editing technique for group-editing the importance signals in downstream tasks.

%First, we provide preliminary evidence for its potential for unsafe document mitigation, addressing concerns about the vulnerability of over-parameterized transformer-based retrieval models to small adversarial errors \textbackslash{}
%cite\{guo\}. Second, we analyze how model editing can enable efficient fine-tuning, tackling the scalability. Specifically, fine-tuning 


\subsection{Mitigating Adversarial Attacks}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/ModelEdit_dangerous.png}
    \caption{Left: Decreasing the IDF of dangerous tokens to a larger scale shows an increasing trend in adversarial mitigation proportion. Right: Enlarging the decrease scale maintains a high NDCG score, demonstrating that the general ranking capability of the cross-encoder remains unaffected.
}
    \label{fig:ModelEdit_dangerous}
\end{figure}
First, we show the potential of the model-editing in mitigating unsafe document without disrupting cross-encoder’s normal ranking capability — a problem identified by methods relying on retraining or regularization often degrade model performance.

First, we construct a dataset using obscene and offensive words \todo{cite}, filtering out multi-word entries and injecting these unsafe tokens into the safe samples of our TFC datasets, and we inspect the subgroup of query and document where inserting an unsafe token in the document significantly increases a relevance score, thus matching the query to an unsafe document. Our approach is simple as “erase” the injected dangerous token by simply reducing its importance signal. Among 17,537 successful adversarial samples, our approach \textbf{80.396\% recovery rate} (unsafe document to be ranked lower than the safe document) when the Decrease Scale is set to -1200, while maintaining a high \textbf{nDCG of 0.9861}, indicating only a \textbf{1.39\% drop in ranking performance}. These results provides the preliminary evidence for the potential of localized editing in the IDF-storing low-rank matrix.


\subsection{Parameter Efficient Fine-Tuning}

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/ModelEdit_finetune.png}
    \caption{For random seed 15, aligned full fine-tuning achieves the best performance and aligned embedding only achieves performance comparative to full fine-tuning.}
    \label{fig:ModelEdit_finetune}
\end{figure}

Our SemanticBM analysis of the cross-encoder circuit reveals that the heads and MLP layers within the transformer layers function similarly to the BM25 scoring mechanism. This suggests that adapting a cross-encoder trained on MSMARCO to other domain-specific documents requires minimal changes to the transformer components, as they primarily re-perform their core function. Instead, the adaptation is likely dependent on the embedding matrix, which must relearn IDF values for unseen tokens and refine embeddings to improve semantic matches through the Matching Heads.

As a proof of concept, using nfcorpus from BEIR\cite{beir}, we train a few fine-tuning epochs to evaluate the performance across four conditions: (A1) full fine-tuning, (A2) IDF-aligned full fine-tuning (aligning token IDFs to the dataset using the model-editing method from S4), (B1) embedding-only fine-tuning, and (B2) IDF-aligned embedding-only fine-tuning. Results across three random seeds showed that IDF-aligned full fine-tuning (A2) achieved the highest NDCG scores, while IDF-aligned embedding-only fine-tuning (B2) outperformed unaligned embedding-only fine-tuning (B1). Additionally, full fine-tuning (A1) and embedding-only fine-tuning (B1) showed no significant difference, supporting the idea that adaptation primarily refines the embedding matrix rather than all model weights. These findings provide preliminary evidence that fine-tuning just on the embedding is sufficient to adapting cross-encoders to new domains, and reveals IDF-aligned initialization as a promising pathway for efficient adaptation.







\subsection{TFC2 and LNC1}


\subsection{Composite of Matching Score: Term Saturation Signal and Document Length Signal}

Section~X shows that using only the Matching Score (MS) and \( U_0 \) achieves an 80\% correlation with ground-truth cross-encoder model scores, even without explicitly including document length or \( k \) values for term saturation in the linear regression model. The absence of performance improvement when these factors are added suggests they are inherently encoded within the Matching Score. To explain our discussion in Section~X, we hypothesize that the Matching Score not only captures soft-TF information but also incorporates additional signals. Below, we outline our hypothesis on how these signals are embedded within the Matching Score.

\subsubsection{(3.1) Term Saturation Signal in the Matching Score}
Figure~8 in Section~(4.2.2) shows that individual Matching Heads exhibit a tfc2 curve, with sharp score increases from 0 to 1 occurrences, followed by milder increases from 1 to 5. Differences in tfc2 curves—quantified by varying monotonicity scores—indicate that some heads exert stronger control over tfc2.

We hypothesize that Matching Heads collectively approximate the ideal tfc2 curve by varying their degree of tfc2 adherence. Ablation experiments support this hypothesis: removing heads such as (1, 7), (5, 9), (7, 9), (3, 1), (4, 9), (8, 1), (8, 8), (6, 5), (6, 3), (0, 8) individually lowers the ranking score at Term Occurrence = 1 below the ``No Ablation'' baseline, amplifying the gap between 0 and 1 occurrences—a defining feature of the tfc2 curve. 

\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/Discussion_tfc2.png}
    \caption{}
    \label{fig:Discussion_tfc2}
\end{figure}


Ablating these heads together increases the term saturation factor \( k \) from 0.28 to 0.42, weakening tfc2 control and producing a near-linear Matching Score trend (Figure~15, red line). These results suggest that different Matching Heads contribute varying degrees of tfc2 adherence, with this subgroup playing a critical role in amplifying the tfc2 pattern.

\subsubsection{(3.2) Document Length Signal in the Matching Score}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/Discussion_doclen_attn.png}
    \caption{}
    \label{fig:Discussion_doclen_attn}
\end{figure}
As shown in Figure~16, minimal pairs derived from the lnc1 axiom reveal that most Matching Heads exhibit an increasing trend in attention score on any document token as document length grows, regardless of whether the document token is a duplicate of the query token. However, this correlation varies across heads. For instance, head (7, 9) exhibits a flat line, suggesting that this head primarily encodes term frequency (TF) and operates independently of document length.

To quantify these differences, we analyze the contributions of soft-TF and document length to the attention scores of various Matching Heads. For each head, we train a linear regression model with the following factors:
\begin{enumerate}
    \item \( x_1 \) (Soft-TF): The soft-TF value derived from baseline inputs, representing the ``actual'' term frequency unaffected by document length changes.
    \item \( x_2 \) (Document Length): The total length of the document, which increases as irrelevant sentences are added.
\end{enumerate}
Heads with high soft-TF/doc\_len ratios, such as (7, 9) and (4, 9), predominantly amplify the score based on term occurrences rather than document length, while other heads balance these factors more equally.
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/Discussion_doclen_RATIO.png}
    \caption{}
    \label{fig:Discussion_doclen_RATIO}
\end{figure}
\begin{figure}[!ht]
    \centering
    \includegraphics[width=\columnwidth]{paperdiagrams/Discussion_doclen_ablation.png}
    \caption{}
    \label{fig:Discussion_doclen_ablation}
\end{figure}

Based on our hypothesis, removing heads with high soft-TF/doc\_len ratios should result in a stronger influence of document length on the Matching Score, flattening the curve. Conversely, amplifying the contribution of these heads is expected to reduce the impact of document length. To test this, we ablated the two heads with the highest ratios, (7, 9) and (4, 9), and observed results consistent with our hypothesis: the Matching Score significantly decreased for shorter documents when these heads were ablated. Clamping their outputs to the largest positive values effectively minimized the document length effect. These findings highlight the critical role of specific heads in modulating document length influence within the scoring mechanism.

\end{comment}
\end{document}

