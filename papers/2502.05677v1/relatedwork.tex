\section{Related Work}
We discuss various approaches to determining the ``interestingness'' of a scenario, a concept that can vary depending on the use case. ``Interestingness'' can refer to the level of interactivity between agents, the safety-critical nature of the scenario, or the predictability of agent behavior. Identifying such ``interesting'' scenarios enables the curation of more challenging and informative datasets. We also provide a concise overview of trajectory prediction models, which are key components in analyzing the interestingness of a scenario.

\subsection{Surprisingness of an outcome}
The concept of surprise in the context of AV is introduced in \cite{dinparastdjadid2023measuring,engstrom2024modeling}. Although various options have been proposed to quantify surprise, it is generally understood as a comparison between the posterior distribution or outcome and the prior distribution. 
When the posterior or outcome significantly deviates from what the prior distribution describes, the scenario is considered surprising, as it was not adequately anticipated by the prior. A key insight from these studies is that there is no universal formula for measuring surprise. Instead, multiple mathematical approaches and interpretations can be employed. This paper builds upon the concept of surprise, aiming to investigate and evaluate a family of potential surprise measures and assess their effectiveness based on human preferences.

\subsection{Measuring interactivity using counterfactual analysis}
A scene can be considered interesting if the behavior of one agent directly influences the behavior of another, indicating that the agents are \textit{interacting}. However, close proximity does not necessarily imply interactivity. As such, a commonly used approach to measure interactivity involves counterfactual analysis to determine how differently an agent behaves if another agent acts differently. Using powerful trajectory prediction models, \cite{tolstaya2021identifying} measures interactivity by calculating the Kullbackâ€“Leibler (KL) divergence between a target agent's (i) future distribution conditioned on a query agent's future trajectory and (ii) its future marginal distribution. The KL divergence is not symmetric and other divergence metrics could be more applicable. Rather than conditioning on a query agent's future, \cite{hsu2023interpretable, schaefer2021leveraging} instead removes the query agent from the scene to determine whether the presence of one agent affects the behavior of another. Task relevance can also be incorporated by considering the distribution of rewards rather than trajectories, as discussed in \cite{stoler2024safeshift}. However, this approach requires knowledge of each agent's desired task, which may be challenging to obtain or infer.

\subsection{Dynamics-based identification of interesting scenarios}
There are various methods that reason about the dynamics of agents when determining whether a scenario is considered interesting. For instance, \cite{makansi2021exposing,feng2024unitraj} evaluate scenario interestingness, particularly in terms of difficulty, by analyzing the accuracy of a linear prediction model, such as a Kalman filter. If the Kalman prediction is poor, a non-linear prediction becomes necessary, implying the scenario is ``interesting.'' Alternatively, some methods employ basic extrapolation (e.g., constant velocity, braking, optimal control) to determine whether agents are likely to collide in the near future~\cite{sadat2021diverse,jiang2024interhub,topan2022interaction,westhofen2023criticality,junietz2018criticality}.
While these dynamics-based approaches are relatively straightforward and do not rely on large datasets, they involve simplifying assumptions that fail to capture non-trivial behaviors influenced by environmental factors such as road geometry and traffic rules. In contrast, rather than explicitly modeling dynamics, \cite{bronstein2023embedding} uses supervised learning with driving logs to estimate collision potential. This method predicts the likelihood of a collision in the near future based on previous trajectory segments and environmental context but requires access to extensive and diverse datasets.

\subsection{Motion Prediction Models}
AV trajectory predictors can be classified based on how agent trajectories are represented (agent-, scene-, or query-centric) and the network architecture employed (Transformer, diffusion model, or autoregression). In agent-centric representations (e.g.~\cite{zhao2021tnt,gu2021densetnt,shi2022motion}), the model generates marginal predictions for a single target agent, treating other agents as part of the environment. This approach facilitates linear runtime by making separate inferences for each agent, which complicates the derivation of joint predictions. Scene-centric representation, on the other hand, predicts the motion of all agents jointly using a single representation relative to a central reference agent, requiring only one inference step. However, it tends to sacrifice prediction accuracy for agents located farther from the ego vehicle due to inherent spatial biases. Query-centric models~\cite{zhou2023query,shi2024mtr++} combine aspects of agent- and scene-centric representations. Although requiring more memory, it strikes a balance by being more efficient than agent-centric models and more accurate than scene-centric ones. Since our surprise metric relies on the joint distribution of future trajectory, this work focuses on scene- and query-centric models.

From a network architecture standpoint, trajectory prediction models differ in how trajectory predictions are produced. 
Transformer-based models (e.g.,~\cite{ngiam2021scene,girgis2021latent}) typically encode input features and then decode the future trajectory in a single forward step. Diffusion-based models~\cite{ho2020denoising} are gaining popularity for their ability to model complex distributions, leveraging a guidance function to influence the denoising process~\cite{zhong2023guided,zhong2023language,huang2024versatile,jiang2023motiondiffuser}. Inspired by advances in natural language community, autoregressive models~\cite{zhang2024closed,philion2024trajeglish,wu2024smart,hu2025solving,zhao2024kigras,seff2023motionlm} tokenize trajectory data, effectively converting it into a discrete representation. However, this discretization complicates the measurement of distributional shifts. As such, this work focuses on diffusions and transformers, specifically those with single-step forward structures.