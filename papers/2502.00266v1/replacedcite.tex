\section{Related work}
\subsection{Masked image reconstruction}

Masked image modeling has emerged as a pivotal learning technique in computer vision ____. For instance, Masked Autoencoders ____ learn to reconstruct missing patches given only a small subset of visible patches reducing computational cost. Masked Diffusion Transformer ____ demonstrates enhanced training efficiency and generation results using a denoising diffusion objective on masked images. Cross-Attention Masked Autoencoders ____ used cross-attention in the decoder to query visible tokens for masked image patch reconstruction. The cross-attention component takes a weighted sum of the visible tokens across different input blocks to fuse the features for each decoder block, leveraging low-level information for reconstruction.

\subsection{Disentangled representation learning}

Disentangled concept learning ____ aims to uncover the underlying explanatory factors hidden within observed data. For example, methods such as $\beta$-VAE ____ and FactorVAE ____ search for directions in the latent space that correlate with distinct human-interpretable concepts. Moreover, Concept Tokenization ____ focuses on learning disentangled object representations and inspecting latent traversals for various factors. Additionally, Concept Bottleneck models ____ learn representations that correspond to specific human-understandable concepts. Energy-based methods ____ aim to compute energy functions of various concepts and combine their probability distributions achieving conjunctions, disjunctions, and negations of various concepts.
%However, identifying these diverse factors in the latent space can be tedious usually with extensive human effort. %In particular, supervised methods employ training data annotated with specific concept labels ____. %Although there are a number of annotated datasets such as CelebA ____, the binary attribute labels in these datasets are restricted to simpler concept prediction tasks and reconstruction tasks from cross-modal representations usually necessitate learning projection networks or other more complicated architecture. 

Conventional concept learning methods typically rely on fully observable images for training. While masking strategies have proven effective in reducing computational cost in natural language processing, their usage in concept learning tasks remains underexplored. This is primarily because masking a large portion of image patches greatly limits the information available for disentangling effective concepts. To address this challenge, we integrate learnable concept tokens at various granularity levels into the masked reconstruction process using the asymmetric Multi-layer Concept Map (MCM) architecture. This approach could not only reduce computational cost for learning effective concepts but also enhance model's concept prediction capability. Our goal is to advance the masked concept learning objective, paving the way for more efficient model architectures.