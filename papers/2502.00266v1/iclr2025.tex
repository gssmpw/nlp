
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}

\usepackage{amsmath,amssymb}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{bbm}
\usepackage{booktabs}



\title{MCM: Multi-layer Concept Map for Efficient Concept Learning from Masked Images}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\newcommand*\samethanks[1][\value{footnote}]{\footnotemark[#1]}
\author{\normalsize Yuwei Sun\textsuperscript{1,2}, Lu Mi\textsuperscript{3}, Ippei Fujisawa\textsuperscript{1}, Ryota Kanai\textsuperscript{1}\\
\normalsize\textsuperscript{1}Araya Research, \textsuperscript{2}RIKEN AIP, \textsuperscript{3}Georgia Institute of Technology\\
}
% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Masking strategies commonly employed in natural language processing are still underexplored in vision tasks such as concept learning, where conventional methods typically rely on full images. However, using masked images diversifies perceptual inputs, potentially offering significant advantages in concept learning with large-scale Transformer models. To this end, we propose Multi-layer Concept Map (MCM), the first work to devise an efficient concept learning method based on masked images. In particular, we introduce an asymmetric concept learning architecture by establishing correlations between different encoder and decoder layers, updating concept tokens using backward gradients from reconstruction tasks. The learned concept tokens at various levels of granularity help either reconstruct the masked image patches by filling in gaps or guide the reconstruction results in a direction that reflects specific concepts. Moreover, we present both quantitative and qualitative results across a wide range of metrics, demonstrating that MCM significantly reduces computational costs by training on fewer than 75\% of the total image patches while enhancing concept prediction performance. Additionally, editing specific concept tokens in the latent space enables targeted image generation from masked images, aligning both the visible contextual patches and the provided concepts. By further adjusting the testing time mask ratio, we could produce a range of reconstructions that blend the visible patches with the provided concepts, proportional to the chosen ratios.
\end{abstract}



\section{Introduction}

Humans often learn concepts through contextual understanding by recognizing relationships among features. Similarly, in a reconstruction task, masking a large portion of the input enables the model to leverage context from unmasked regions, thereby potentially enhancing the learning of dependencies that define concepts. By deprioritizing pixel-level details, the masking strategy encourages the focus on consistent features across instances, leading to better generalization. While masking strategies are well-studied in language tasks, they still remain underexplored in vision tasks, particularly in the context of concept learning, where existing studies typically focus on learning from full images. Consequently, we aim to investigate whether the masking objective diversifies perceptual inputs and could provide additional benefits for concept learning with large-scale Transformer models.

We propose the Multi-layer Concept Map (MCM) method to facilitate masked concept learning through vision reconstruction tasks. Specifically, we leverage cross-attention for learning concept tokens at various granularity levels from masked images. These concept tokens assist in reconstructing input images by filling gaps or guiding reconstruction results in a specific direction for effective concept manipulation. Our method employs an asymmetric concept learning architecture, establishing correlations between different encoder and decoder layers (Figure \ref{fig:scheme}). This architecture allows concept tokens to be updated using backward gradients from reconstruction tasks, enabling decoder layers to focus on distinct encoder layer outputs and enhancing reconstruction performance.

MCM is an efficient method for masked concept learning with significantly reduced computational cost, achieved by masking large portions of image patches and using an asymmetric model architecture. Nevertheless, extensive experimental results demonstrate that MCM could also enhance concept prediction performance compared to conventional methods. Furthermore, even with extremely limited input information, the model effectively learns a set of concepts that guide reconstruction results in a specific direction for concept manipulation. The reconstructed images align with the visible unmasked image tokens while reflecting the provided concept tokens. Consequently, MCM learns effective concept tokens by training on less than 75\% of the total image patches, achieving enhanced or competitive performance in both prediction and reconstruction tasks.

Overall, our main contributions are three-fold: 

(1) We propose the Multi-layer Concept Map (MCM) method to facilitate masked concept learning through vision reconstruction tasks, which involves the masked concept encoder and multi-layer concept mapping architecture (Section \ref{sec:mcm}).

(2) This study investigates two dedicated loss functions to enhance the model's ability in concept prediction, especially for training on unbalanced concept classes, i.e., the disentanglement loss and weighted concept loss (Section \ref{sec:imag}).

(3) The extensive quantitative and qualitative analysis involves concept prediction performance, reconstruction quality measured by Fréchet Inception Distance, computational cost, and diverse visualizations. MCM learns effective concept tokens using less than 75\% of image patches while achieving competitive performance (Section \ref{sec:quan}, \ref{sec:qual}).

The remainder of this paper is structured as follows. Section 2 reviews the most recent work on masked image reconstruction and disentangled representation learning. Section 3 demonstrates the essential definitions, assumptions, and technical underpinnings of the proposed method. Section 4 presents a thorough examination using a broad range of metrics to assess concept prediction and reconstruction performance. Section 5 concludes our findings and gives out future directions.



\section{Related work}

\subsection{Masked image reconstruction}

Masked image modeling has emerged as a pivotal learning technique in computer vision \cite{he2022masked,yue2023understanding,zheng2023fast,chen2023improving,fu2024rethinking}. For instance, Masked Autoencoders \cite{he2022masked} learn to reconstruct missing patches given only a small subset of visible patches reducing computational cost. Masked Diffusion Transformer \cite{zheng2023fast} demonstrates enhanced training efficiency and generation results using a denoising diffusion objective on masked images. Cross-Attention Masked Autoencoders \cite{fu2024rethinking} used cross-attention in the decoder to query visible tokens for masked image patch reconstruction. The cross-attention component takes a weighted sum of the visible tokens across different input blocks to fuse the features for each decoder block, leveraging low-level information for reconstruction.

\subsection{Disentangled representation learning}

Disentangled concept learning \cite{bengio2013representation,higgins2017beta,locatello2019challenging,harkonen2020ganspace,alias2021neural,yang2022visual,sun2023associative,ismail2023concept} aims to uncover the underlying explanatory factors hidden within observed data. For example, methods such as $\beta$-VAE \cite{higgins2017beta} and FactorVAE \cite{kim2018disentangling} search for directions in the latent space that correlate with distinct human-interpretable concepts. Moreover, Concept Tokenization \cite{yang2022visual} focuses on learning disentangled object representations and inspecting latent traversals for various factors. Additionally, Concept Bottleneck models \cite{ismail2023concept,yuksekgonul2022post,oikarinen2023label,yang2023language} learn representations that correspond to specific human-understandable concepts. Energy-based methods \cite{du2020compositional,li2022energy} aim to compute energy functions of various concepts and combine their probability distributions achieving conjunctions, disjunctions, and negations of various concepts.
%However, identifying these diverse factors in the latent space can be tedious usually with extensive human effort. %In particular, supervised methods employ training data annotated with specific concept labels \cite{koh2020concept}. %Although there are a number of annotated datasets such as CelebA \cite{liu2015faceattributes}, the binary attribute labels in these datasets are restricted to simpler concept prediction tasks and reconstruction tasks from cross-modal representations usually necessitate learning projection networks or other more complicated architecture. 

Conventional concept learning methods typically rely on fully observable images for training. While masking strategies have proven effective in reducing computational cost in natural language processing, their usage in concept learning tasks remains underexplored. This is primarily because masking a large portion of image patches greatly limits the information available for disentangling effective concepts. To address this challenge, we integrate learnable concept tokens at various granularity levels into the masked reconstruction process using the asymmetric Multi-layer Concept Map (MCM) architecture. This approach could not only reduce computational cost for learning effective concepts but also enhance model's concept prediction capability. Our goal is to advance the masked concept learning objective, paving the way for more efficient model architectures.



\section{Method}

In this section, we introduce the Multi-layer Concept Map (MCM) method, which involves the masked concept encoder and multi-layer concept decoder architecture. In addition to the reconstruction target, we devise two dedicated loss functions to enhance the model's concept prediction, especially for unbalanced concept classes, i.e., the disentanglement loss and weighted concept loss. 

%We then conduct a theoretical analysis of the proposed method's benefits and establish connections to the cognitive modeling of the shared workspace.

%\subsection{Motivation}

%Conventional concept learning methods rely heavily on training with full images. While masking strategies have been widely used in reconstruction tasks for reducing computational cost, their application in concept learning has not yet been explored. One of the major reasons is that masking images can reduce the available information from the input, limiting the concepts that can be learned. To address this challenge in using masking for concept learning, we employ an asymmetric Multi-layer Concept Map (MCM) architecture for learning concepts from masked images by establishing correlations between different encoder and decoder layers (Figure \ref{fig:scheme1}), allowing concepts to be updated using backward information from the reconstruction task. Consequently, the learnable concepts are updated based on both the limited unmasked patches and the reconstruction task target. This approach preserves the model's ability to learn a set of effective concept tokens while significantly reducing computational cost by using only a small portion of the image patches.

%Using masking in reconstruction tasks usually introduces inconsistent reconstruction results of the masked image patches. With a large masking ratio like 75\%, the masked patches are reconstructed with inconsistency showing reconstruction results that are largely different from the original image, e.g., in the Masked autoencoder method \cite{he2022masked}. Our method addresses this issue by incorporating the learnable concept tokens during training. These tokens guide the reconstruction process, allowing it to be influenced not only by the contexts information of the unmasked patch tokens but also by a set of concepts, which provide detailed information for reconstructing the masked patches. Using this architecture, we can reconstruct the patches of a masked image with a large masking ratio of 75\% or higher, ensuring that the reconstructed patches align with both the unmasked image contexts and the detailed concepts provided by humans. This enhances the interpretability and reduces the training cost for an image reconstruction model.


\subsection{Multi-layer concept map for masked concept learning}
\label{sec:mcm}

\begin{figure}[t]
    \centering
    % Subfigure (a)
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/scheme1.png}
        \caption{MCM randomly masks an image and a set of learnable concepts is learned at each encoder layer alongside the visible tokens. Learnable mask tokens, initialized with Gaussian noise, are added at the masked positions in the encoder output. The decoder then utilizes the mask tokens for reconstruction via cross-attention, leveraging concept tokens at various granularities. For computational efficiency, in the asymmetric architecture, concept tokens from every two encoder layers are used in cross-attention.}
    \end{subfigure}
    \hfill
    % Subfigure (b)
    \begin{subfigure}[b]{0.48\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/scheme2.png}
        \caption{In the encoder layer, self-attention and a feedforward network (FFN) process the input visible tokens, while cross-attention updates concept tokens using the input tokens, followed by an FFN. Skip connections and layer normalization are utilized throughout. In the decoder layer, cross-attention updates mask tokens using concept tokens learned from specific encoder layers as keys and values. The decoder layer also employs an FFN, skip connections, and layer normalization.}
    \end{subfigure}
    \caption{(a) Multi-layer Concept Map (MCM) model architecture. (b) Details of the encoder and decoder layers.}
    \vspace{-10pt}
    \label{fig:scheme}
\end{figure}


\subsubsection{Concept encoding from masked images}
MCM employs multiple encoder layers to encode visible tokens and learn concept tokens at various granularity levels. Then, decoder layers aim to reconstruct the masked patches using the concept tokens and contextual information from the visible tokens. In particular, MCM divides images into patches and processes them using attention mechanisms \cite{attention}. While model architectures such as convolution layers could also be used for encoding and decoding, the masking strategy is typically utilized in Transformer models.

Let $x \in \mathbb{R}^{H \times W \times C}$ be an input image, where $(H, W)$ represents the resolution of the image and $C$ is the number of channels. The image $x$ is partitioned into a sequence of patches $x_p \in \mathbb{R}^{N \times (P^2 \cdot C)}$, where $(P, P)$ denotes the resolution of each patch, and $N = \frac{HW}{P^2}$ is the total number of patches. These patches are then mapped to embeddings $v_p \in \mathbb{R}^{N \times E}$ via a linear projection $W^P \in \mathbb{R}^{(P^2 \cdot C)\times E}$. With a mask ratio $r$, we randomly remove $\lfloor rN \rfloor$ tokens from the input, leaving only $N - \lfloor rN \rfloor$ visible tokens as input $v_{\text{masked}} \in \mathbb{R}^{(N - \lfloor rN \rfloor) \times E}$ to the model. A higher mask ratio enhances computational efficiency but reduces contextual information for concept learning. Therefore, an optimal mask ratio likely exists, balancing both efficiency and performance.

An encoder model that consists of multiple attention layers takes the encoded visible tokens $v_{\text{masked}} \in \mathbb{R}^{(N - \lfloor rN \rfloor) \times E}$ and a set of learnable concept tokens $C_0 \in \mathbb{R}^{M \times E}$ as the input (Figure \ref{fig:scheme}). Notably, we initialize the concept tokens using Gaussian noise and share them \textit{across batch samples}. Then, for each encoder layer $l_\text{encoder}$, we employ multi-head cross-attention to update the concept tokens $C_{l_\text{encoder}} \in \mathbb{R}^{M \times E}$ using the visible patch tokens $v_{\text{masked}}^{l_\text{encoder}}$ as the key and value. As a result, the output of an attention head $i$ is a weighted sum of the values, i.e., $\text{softmax}\left( \frac{W^Q_i C_{l_\text{encoder}} (W^K_i v_{\text{masked}}^{l_\text{encoder}})^T}{\sqrt{E}} \right) W^V_i v_{\text{masked}}^{l_\text{encoder}}$, where $W^Q,\, W^K,\, \text{and}\, W^V$ are the projection weights for the query, key, and value, respectively. Finally, we use a feedforward network to obtain $C_{l_\text{encoder}+1}$. Note that we do not employ self-attention for the learned concept tokens, which enables individual concept updates thus diversifying concept tokens. Moreover, we process the visible patch tokens $v_{\text{masked}}^{l_\text{encoder}+1}$ for extracting high-level contextual information. In particular, we employ self-attention followed by a feedforward network to learn associations among visible tokens. For learning both concept and visible patch tokens, the skip connection and layer normalization is employed throughout. All the feedforward networks consist of two linear layers with a GELU activation \cite{hendrycks2016gaussian} in between. We stack multiple concept encoder layers $l_\text{encoder} \in \{1, 2, \dots, L_{\text{encoder}}\}$ in depth to obtain the latent representations of concept tokens $C_{L_{\text{encoder}}}$ and visible tokens $v_{\text{masked}}^{L_{\text{encoder}}}$ as the outputs of the encoder model.


\subsubsection{Image decoding with multi-layer concept mapping}
\label{sec:imag}

To reconstruct the masked patches, we add learnable mask tokens $v_{\text{init}} \in \mathbb{R}^{\lfloor N \times r \rfloor \times E}$ at the positions of masked patches in the encoder output. These mask tokens are initialized with values drawn from a Gaussian distribution. Notably, we concatenate and rearrange the visible tokens $v_{\text{masked}}^{L_{\text{encoder}}} \in \mathbb{R}^{(N - \lfloor N \times r \rfloor) \times E}$ and the mask tokens $v_{\text{init}} \in \mathbb{R}^{\lfloor N \times r \rfloor \times E}$ based on the mask indices $Z\in \mathbb{R}^{\lfloor N\times \gamma\rfloor}$, as the decoder input $v_{\text{full}}^0 \in \mathbb{R}^{N\times E}$. Moreover, the decoder model computes on the full $N$ image tokens that are much more than the $N-\lfloor N \times r \rfloor$ visible tokens processed by the encoder model. To alleviate the computational cost induced by the decoding process with the full patch length, we employ an asymmetric architecture for the decoder using half the number of layers as the encoder. %We would show that this asymmetric architecture is not only more efficient but also can achieve competitive performance.

A specific multi-layer concept mapping architecture is devised based on cross-attention components between paired encoder and decoder layers. This enables the reconstruction of mask tokens using the learned concept tokens from various encoder layers. With the asymmetric architecture, every two encoder layer's concept tokens are utilized for reconstructing the mask tokens of a specific decoder layer through cross-attention $\text{MHA}(\cdot)$, using the concept tokens as the key and value, i.e.,  $\hat{v}_{\text{full}}^{l_\text{decoder}} \leftarrow \text{MHA}(v_{\text{full}}^{l_\text{decoder}}, C_{L_\text{encoder} - 2\times l_\text{decoder}})$. Then, a feedforward network $\text{FF}(\cdot)$ computes the decoder layer output $v_{\text{full}}^{l_\text{decoder}+1}\leftarrow \text{FF}(\hat{v}_{\text{full}}^{l_\text{decoder}})$. Note that we refrain from using self-attention in the decoder model to prevent the model from overly focusing on contextual visible tokens, allowing the model to prioritize the concept token learning. Consequently, after stacking $L_{\text{decoder}}$ decoder layers, the full tokens $v_{\text{full}}^{L_{\text{decoder}}}$ are converted into a pixel-level image $\hat{x} \in \mathbb{R}^{N \times (P^2 \times 3)}$ as the reconstruction result.

\vspace{-5pt}
\paragraph{Masked reconstruction loss.}

MCM updates learnable concept tokens through a reconstruction objective. In particular, we compute the reconstruction loss between the decoder output $\hat{x}$ and the input image $x$ using the mean squared error loss: $\ell_{\text{MSE}}(X, C_0) = \frac{1}{B} \sum_{i=1}^{B} \left( \hat{x}_i - x_i \right)^2.$ To enhance computational efficiency, we specifically compute the loss for the mask tokens as follows:
$$\ell_{\text{re}}(X, C_0) = \frac{\sum_{j=1}^{N} \ell_{\text{MSE}}^j \cdot \mathbbm{1}[j \in Z]}{\lfloor N \times \gamma \rfloor},$$
where $Z$ is the indices of mask tokens, $\mathbbm{1}[j \in Z]$ is an indicator function that outputs 1 if the $j$th token is a masked position and 0 otherwise, and $\ell_{\text{MSE}}^j$ is the mean squared error loss for the $j$th token.

\vspace{-5pt}
\paragraph{Disentanglement loss.}

We aim to encourage the model to learn mutually exclusive representations for various concept tokens, thus enhancing its generalization to unseen test samples. We devise a disentanglement loss by randomly swapping a concept in the latent space with its antonym and identifying the modified concepts from reconstruction results. In particular, given an image $x$ and its predicted concepts $C_{L_\text{encoder}}$ in the latent space, we select a specific concept position $j \in \{1,2,...,M\}$ based on a random binary mask $U \in \{0, 1\}^{M}$, where exactly one position has a value of 1, indicating where the concept modification occurs. We then replace the concept $C^{j}_{L_\text{encoder}}$ with its antonym token $\mathcal{O}(C^{j}_{L_\text{encoder}})$, where $\mathcal{O}(\cdot)$ is a function that maps a concept to its antonym. The decoder $f_\text{decoder}$ then reconstructs an image $\tilde{x}$ with the modified concepts $\hat{C}_{L_{\text{encoder}}}$. Intuitively, if the model learns the differences among concepts, the predicted concepts in the reconstruction results would show modifications only in the selected one. To verify this, we input a reconstruction result $\tilde{x}$ into the encoder $f_\text{encoder}$ to obtain the predicted concepts $\tilde{C}_{L_\text{encoder}}$, which are expected to match the modified concepts $\hat{C}_{L_{\text{encoder}}}$. Consequently, we devise the disentanglement loss as follows:
$$
\hat{c}^i_{L_{\text{encoder}}} = \{U_j \cdot \mathcal{O}(c^{i,j}_{L_{\text{encoder}}}) + (1 - U_j) \cdot c^{i,j}_{L_{\text{encoder}}}\}_{j=1}^M,\,\,
\tilde{c}^i_{L_{\text{encoder}}} = f_\text{encoder}f_\text{decoder}(\hat{c}^i_{L_{\text{encoder}}}),
$$
$$\ell_{\text{disentangle}}(X,C_0,U) = \frac{1}{B} \sum_{i=1}^{B} \left( \hat{c}^i_{L_{\text{encoder}}} - \tilde{c}^i_{L_{\text{encoder}}} \right)^2.$$

\vspace{-10pt}
\paragraph{Weighted concept loss.}

Concepts involved in a dataset are often biased. To encourage the model to focus more on underrepresented concepts during training, we further propose the weighted concept loss to adjust the impact of each concept's prediction error using the concept's frequency in a batch. In the latent space, we utilize a set of concept embeddings $C_{\text{prototype}}$ learned through approaches such as self-supervised learning, where numerical concept labels are converted to semantic embeddings of specific concepts, each with a dimension $E$. Notably, given a batch of size \( B \) and the predicted concept tokens $c_{L_{\text{encoder}}}^{i,j}$, for each sample \( i \) and concept index \( j \), the loss is formulated as:
\[
\ell_{\text{concept}}(C_{L_{\text{encoder}}},C_{\text{prototype}}) = \frac{1}{B} \sum_{i=1}^{B} \sum_{j=1}^{M} w_{i,j} \cdot \left( c_{L_{\text{encoder}}}^{i,j} - c_{\text{prototype}}^{i,j} \right)^2,
\]
where \( w_{i,j} \) represents a weight assigned to the error of the concept \( j \) in sample \( i \). These weights are inversely proportional to the frequency values of the concepts in the batch:\(
w_{i,j} = \frac{S}{\text{freq}(c_{\text{prototype}}^{i,j}) + \epsilon},
\)
where \( \text{freq}(c_{\text{prototype}}^{i,j}) \) is the frequency of the concept \( c_{\text{prototype}}^{i,j}\) in the batch $\{\{c^{i,j}_{\text{prototype}}\}_{j=1}^M\}_{i=1}^B$, \( S \) is a scaling constant to control the magnitude of the weights, and \( \epsilon \) is a small constant added to avoid division by zero and to smooth the weights. The weighted concept loss gives higher importance to less frequent concepts by increasing their corresponding weights. Conversely, more frequent concepts have lower weights with reduced contribution. Additionally, we use coefficients $\alpha$ and $\beta$ to balance the various loss components, i.e., $\mathcal{L}(X, C_0, C_{\text{prototype}}, U) = \ell_{\text{re}} + \alpha \cdot \ell_{\text{disentangle}} + \beta \cdot \ell_{\text{concept}}.$ 
%The complete algorithm is demonstrated in Algorithm \ref{alg:main}.


\begin{comment}
\begin{algorithm}[!t]
\small 
\caption{Multi-layer Concept Map (MCM)}
\label{alg:main}
\begin{algorithmic}[1]
\STATE \textbf{Input:} image patches: $X\in\mathbb{R}^{B\times N\times (P^2\times C)}$; image tokens: $V_p\in\mathbb{R}^{B\times N\times E}$; learnable concept tokens: $C_0 \in \mathbb{R}^{B\times M\times E}$; concept embeddings: $C_{\text{prototype}} \in \mathbb{R}^{B\times M\times E}$.
\STATE $V_{\text{masked}}\in\mathbb{R}^{B\times (N - \lfloor N\times r\rfloor)\times E}\leftarrow$ Randomly mask input tokens with a ratio $r$ \\   $\triangleright$Mask indices: $Z\in \mathbb{R}^{\lfloor N\times r\rfloor}$.
\STATE \textbf{Encoder}
\FOR{layer $l_\text{encoder} = 1, 2,\dots,L_{\text{encoder}}$}
\STATE $\hat{C}_{l_\text{encoder}}\leftarrow \text{MHA}(C_{l_\text{encoder}},V_{\text{masked}}^{l_\text{encoder}},V_{\text{masked}}^{l_\text{encoder}})$ $\triangleright$ query, key, value.
\STATE $C_{{l_\text{encoder}}+1}\leftarrow \text{FF}(\hat{C}_{l_\text{encoder}})$
\IF{Every two encoder layers}
    \STATE Add $C_{{l_\text{encoder}}+1}$ to the list of learned concepts $C_\text{encoder}\{C_3, C_5, .., C_{{l_\text{encoder}}+1},..\}$
\ENDIF
\STATE $\hat{V}_{\text{masked}}^{l_\text{encoder}}\leftarrow \text{SA}(V_{\text{masked}}^{l_\text{encoder}}) $ 
\STATE $V_{\text{masked}}^{{l_\text{encoder}}+1}\leftarrow \text{FF}(\hat{V}_{\text{masked}}^{l_\text{encoder}})$
\ENDFOR
\STATE Compute the weighted concept loss: $\ell_{\text{concept}}(C_{L_{\text{encoder}}},C_{\text{prototype}}) = \frac{1}{B} \sum_{i=1}^{B} \sum_{j=1}^{M} w_{i,j} \cdot \left( c_{L_{\text{encoder}}}^{i,j} - c_{\text{prototype}}^{i,j} \right)^2.$
\STATE \textbf{Decoder}
\STATE Initialize learnable mask tokens $V_{\text{init}}\in \mathbb{R}^{B\times \lfloor N\times \gamma\rfloor \times E}$ with values drawn from a Gaussian distribution.
\STATE $V_{\text{full}}^0\leftarrow $ Concatenate $V_{\text{masked}}^{L_{\text{encoder}}}$ and $V_{\text{init}}$, and rearrange based on the mask indices $Z\in \mathbb{R}^{\lfloor N\times \gamma\rfloor}$.
\FOR{layer $l_\text{decoder} = 1, 2,\dots,L_{\text{decoder}}$}
\STATE Retrieve learned concepts from the encoder $C_{l_\text{decoder}} \leftarrow C_\text{encoder}[-l_\text{decoder}]$.
\STATE $\hat{V}_\text{full}^{l_\text{decoder}} \leftarrow \text{MHA}(V_\text{full}^{l_\text{decoder}},C_{l_\text{decoder}}: C_{L_\text{encoder}-2\times l_\text{decoder}}).$
\STATE $V_\text{full}^{l_\text{decoder}+1}\leftarrow \text{FF}(\hat{V}_\text{full}^{l_\text{decoder}})$
\ENDFOR
\STATE Convert $V_\text{full}^{L_{\text{decoder}}}$ into pixel level images $\hat{X}\in\mathbb{R}^{B\times N\times (P^2\times C)}$
\STATE Compute the masked reconstruction loss: $\ell_{\text{re}}(X, C_0) = \frac{\frac{1}{B} \sum_{j=1}^{N} \sum_{i=1}^{B} \left( \hat{x}_{i,j} - x_{i,j} \right)^2 \cdot \mathbbm{1}[j \in Z]}{\lfloor N \times \gamma \rfloor}.$
\STATE 
\STATE \textbf{Disentanglement Loss}
\STATE Select a specific concept position $j \in \{1,2,...,M\}$ based on a random binary mask $U \in \{0, 1\}^{M}$.
\STATE Replace the concept $C^{j}_{L_\text{encoder}}$ with its antonym token $\mathcal{O}(C^{j}_{L_\text{encoder}})$: $\hat{c}^i_{L_{\text{encoder}}} = \{U_j \cdot \mathcal{O}(c^{i,j}_{L_{\text{encoder}}}) + (1 - U_j) \cdot c^{i,j}_{L_{\text{encoder}}}\}_{j=1}^M.$
\STATE Reconstruct an image based on the modified concepts: $\tilde{x}_i \leftarrow f_\text{decoder}(\hat{c}^i_{L_{\text{encoder}}}).$
\STATE Predict the concepts in $\tilde{x}_i$ using the encoder: $\tilde{c}^i_{L_{\text{encoder}}} \leftarrow f_\text{encoder}(\tilde{x}_i).$
\STATE Compute the disentanglement loss: $\ell_{\text{disentangle}}(X,C_0,U) = \frac{1}{B} \sum_{i=1}^{B} \left( \hat{c}^i_{L_{\text{encoder}}} - \tilde{c}^i_{L_{\text{encoder}}} \right)^2.$
\STATE The final loss: $\mathcal{L}(X, C_0, C_{\text{prototype}}, U) = \ell_{\text{re}}(X, C_0) + \alpha \cdot \ell_{\text{disentangle}}(X, C_0, U) + \beta \cdot \ell_{\text{concept}}(X, C_0, C_{\text{prototype}})$
  $\;\;\triangleright \alpha, \beta$ : coefficients.
\end{algorithmic}
\end{algorithm}
\end{comment}


\vspace{-2pt}
\section{Experiments}

This section describes the detailed experimental settings. We present both quantitative and qualitative results on concept prediction performance, reconstruction quality, and computational cost, followed by extensive ablation studies. We demonstrate the method's image editing capability by showing how specific concept features could be disentangled exclusively from input images and how multiple concepts in the latent space could be combined. The results indicate that using an optimized mask ratio not only reduces computational cost but enhances model concept learning performance.

\vspace{-1pt}
\subsection{Settings}

\subsubsection{Datasets}

We employ the CelebA \cite{liu2015faceattributes} dataset, a face attributes dataset characterized factors of variation, where we selected 11 concepts with varying frequencies (please refer to Appendix \ref{sec:unba}), including \lq Bald,' \lq Bangs,' \lq Black Hair,' \lq Blond Hair,' \lq Chubby,' \lq Eyeglasses,' \lq Mustache,' \lq Wearing Hat,' \lq Male,' \lq Smiling,' and \lq Young.' Antonyms were generated by appending \lq Not' before each concept (e.g., \lq Smiling' vs. \lq Not Smiling'). 

\subsubsection{Models}

Models of varying sizes (Small, Base, and Large) were trained with hyperparameters tailored to their complexity. For the detailed architecture settings, please refer to Appendix \ref{sec:app}. To learn concept tokens that contribute not only to concept prediction but to masked image reconstruction tasks, we convert binary concept labels into 512-dimensional embeddings using a pretrained CLIP model \cite{radford2021learning}. These concept embeddings provide guidance in the latent space, facilitating the reconstruction of masked patches. All experiments were performed using four A100 GPUs.

\subsection{Quantitative results}
\label{sec:quan}

\begin{comment}
\subsubsection{Interpretability}

To obtain concept prediction results, we convert the learned latent-space concept embeddings into a concept token by comparing them with each of the prototype concept tokens and selecting the one with the minimum cosine distance. Then, based on the measured distances of the prediction result with other concepts, the learned latent space can be used to estimate concept probabilities enhancing model interpretability (Figure \ref{fig:prop}).

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/concept_lists.png}
    \caption{Examples of concept predictions for interpretability.}
    %Base seed 1 epochs 2000, 75% masking; prediction results with softmax to be updated.
    \label{fig:prop}
\end{figure}
\end{comment}


\subsubsection{Concept prediction and reconstruction performance}

We aim to study how various training mask ratios affect the model's performance in the concept prediction and reconstruction tasks. We measure numerical prediction performance based on accuracy, precision, recall, and F1 score, evaluating reconstruction performance based on the Fréchet Inception Distance (FID) score. Table \ref{tab:mask_combined} demonstrates the impact of varying mask ratios, showing an optimized mask ratio of 0.5 for the small-sized model and 0.25 for the base and large-sized models. For each entry, we trained the small and base-sized models for 500 epochs and the large-sized model for 100 epochs. In particular, for the base and large-sized models, as the mask ratio increases beyond 0.25, accuracy and F1-score start to decline, and FID increases substantially.

\begin{table}[t]
\centering
\small
\setlength{\tabcolsep}{5pt}
\caption{Performance metrics at different mask ratios for small, base, and large-sized models. Note that since higher input image resolutions generally yield higher FID scores for larger models, we typically compare FID scores given the same image complexity.}
\begin{tabular}{c|ccccccc}
\toprule
\textbf{Model Size} & \textbf{Mask Ratio} & \textbf{Accuracy$\uparrow$} & \textbf{Precision$\uparrow$} & \textbf{Recall$\uparrow$} & \textbf{F1-Score$\uparrow$} & \textbf{FID$\downarrow$} & \textbf{Training T (h)$\downarrow$}\\
\midrule
\multirow{6}{*}{Small} 
& 0.0 & 0.877 & 0.514 & 0.563 & 0.537 & \textbf{1.053} & 4.13\\
& 0.1 & 0.878 & 0.518 & 0.566 & 0.541 & 1.263 & 4.08\\
& 0.25 & 0.882 & 0.521 & 0.57 & 0.544 & 1.438 & 4.03\\
& \textbf{0.5} & \textbf{0.884} & \textbf{0.523} & \textbf{0.572} & \textbf{0.546} & 2.045 & 3.87\\
& 0.75 & 0.854 & 0.496 & 0.542 & 0.518 & 4.655 & 3.76\\
& 0.9 & 0.845 & 0.487 & 0.531 & 0.508 & 10.97 & \textbf{3.57}\\
\midrule
\multirow{6}{*}{Base} 
& 0.0 & 0.935 & 0.857 & 0.815 & 0.835 & 1.972 & 6.45\\
& 0.1 & 0.943 & 0.876 & 0.828 & 0.851 & 1.886 & 6.33\\
& \textbf{0.25} & \textbf{0.945} & 0.881 & \textbf{0.839} & \textbf{0.860} & \textbf{1.884} & 6.12\\
& 0.5 & 0.942 & 0.888 & 0.812 & 0.849 & 2.67 & 6.0\\
& 0.75 & 0.935 & \textbf{0.901} & 0.786 & 0.839 & 5.386 & 5.79\\
& 0.9 & 0.917 & 0.847 & 0.729 & 0.784 & 14.279 & \textbf{5.51}\\
\midrule
\multirow{6}{*}{Large} 
& 0.0 & 0.946 & 0.807 & 0.785 & 0.796 & 4.195 & 9.82\\
& 0.1 & 0.925 & 0.753 & 0.74 & 0.746 & 4.033 & 9.76\\
& \textbf{0.25} & \textbf{0.955} & \textbf{0.821} & \textbf{0.796} & \textbf{0.808} & \textbf{4.027} & 9.65\\
& 0.5 & 0.937 & 0.739 & 0.714 & 0.726 & 5.871 & 8.93\\
& 0.75 & 0.913 & 0.652 & 0.674 & 0.663 & 7.755 & 8.23\\
& 0.9 & 0.835 & 0.528 & 0.533 & 0.53 & 12.993 & \textbf{8.11}\\
\bottomrule
\end{tabular}
\vspace{-10pt}
\label{tab:mask_combined}
\end{table}


\begin{comment}
\begin{figure}[t]
    \centering
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/small_perf.png}
        \caption{Small-sized model.}
        \label{fig:masks}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mask_ratio.png}
        \caption{Base-sized model.}
        \label{fig:maskb}
    \end{subfigure}%
    \hfill
    \begin{subfigure}{0.33\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figures/large_perf.png}
        \caption{Large-sized model.}
        \label{fig:maskl}
    \end{subfigure}
    \caption{Accuracy, F1-Score, and FID vs. Mask Ratio for small, base, and large-sized models. Optimized concept prediction and reconstruction performance achieved with a mask ratio of 0.25, showcasing the benefits of the masking strategy in concept learning.}
    %large model figure 0.25 values updated
    
    \label{fig:mask_comparison}
\end{figure}
\end{comment}



\subsubsection{Ablation studies}

We conducted extensive ablation studies to evaluate the benefits of the various components in the proposed MCM method. In addition to the ablations for the proposed two types of losses, i.e., the weighted concept loss and disentanglement loss, we specifically consider the following ablations:

%\noindent (1) \textbf{W/ Weighted Loss:} Adjusts concept loss impact based on concept frequency, encouraging focus on underrepresented concepts.

%\noindent (2) \textbf{W/ Disentanglement Loss:} Introduces antonym swaps to distinguish modified concepts in reconstructions.

\noindent (1) \textbf{W/O Branches:} Instead of using learned concept tokens from different encoder layers with the cross-attention mechanism, self-attention is employed to process concept tokens sequentially at each decoder layer, similar to \cite{yang2022visual}.

\noindent (2) \textbf{W/O Learnable Latent Concepts:} Concept label embeddings query encoder layers, with the weighted output serving as input to specific decoder layers.

\noindent (3) \textbf{Repetitive Latent Concepts:} The learned concept tokens in the latent space are distributed across all decoder layers.

The weighted concept loss balances gradient assignments for imbalanced concept classes, thus improving recall with enhanced sensitivity to minority classes (\lq W/ Weighted Loss' ablation in Table \ref{tab:loss_metrics}). The disentanglement loss enhances the decoder's capability to reconstruct masked images from concept tokens, thereby enhancing reconstruction quality with significantly reduced FID scores. Consequently, incorporating both losses resulted in the best concept prediction performance, as evaluated by the test accuracy and F1-score, while maintaining a low FID score of 1.605, effectively balancing image quality. Moreover, the branches architecture plays a significant role in enhancing performance, as its absence led to a sizable decrease in F1-score and an increase in FID (\lq W/O Branches' ablation). The \lq W/O Learnable Latent Concepts' ablation and \lq Repetitive Latent Concepts' ablation highlight the efficacy of the proposed concept learning mechanism via cross-attention. While ablating these components was not as impactful as removing the branches themselves, it still resulted in degraded performance for both concept prediction and reconstruction tasks. Consequently, the complete model, with all components included, achieved the best performance.


\begin{table}[t]
\centering
\small
%\setlength{\tabcolsep}{2pt}
\caption{Ablation studies on the various components of MCM.}
\begin{tabular}{c|ccccc}
\toprule
\textbf{Method}& \textbf{Accuracy$\uparrow$} & \textbf{Precision$\uparrow$} & \textbf{Recall$\uparrow$} & \textbf{F1-Score$\uparrow$} & \textbf{FID$\downarrow$} \\
\midrule
Default MCM & 0.945& 0.881 & 0.839 & 0.860& 1.884\\
W/ Weighted Loss& \textbf{0.946}& 0.886& 0.852& 0.869& 1.734\\
W/ Disentanglement Loss& 0.945& \textbf{0.893} & 0.841& 0.866& \textbf{1.584}\\
W/ Weighted and Disentanglement Losses& \textbf{0.946}& 0.886& \textbf{0.859}& \textbf{0.872}& 1.605\\
\midrule
W/O Branches & 0.925 & 0.756 & 0.686 & 0.719 & 3.971\\
W/O Learnable Latent Concepts & 0.944 & 0.831 & 0.781 & 0.805 & 2.535\\
Repetitive Latent Concepts & 0.926 & 0.753 & 0.706 & 0.729 & 2.167 \\
\bottomrule
\end{tabular}
\vspace{-10pt}
\label{tab:loss_metrics}
\end{table}


\vspace{-10pt}
\paragraph{Comparison with Masked Autoencoder.}

Table \ref{tab:baseline} demonstrates the comparison with the Masked Autoencoder (MAE) \cite{he2022masked} on the concept learning tasks. We follow the architecture designs and hyperparameter settings of a base-sized MAE. For MAE, we added a linear layer in its latent space to predict concepts using the visible tokens, training the model from scratch. Additionally, the binary concept labels from the CelebA dataset were employed for each concept position. 

\begin{table}[t]
\centering
\small
%\setlength{\tabcolsep}{2pt}
%baseline using CLIP pretrained model for downstream concept prediction tasks
\caption{Performance comparison with the Masked Autoencoder (MAE) using base-sized models shows that, even with supervised guidance from binary labels, MAE struggles to achieve competitive concept learning at a 0.25 training mask ratio. While MCM significantly improves concept prediction, it trades off reconstruction quality. However, in the following experiments, we demonstrate image editing capabilities of MCM that MAE with a concept learning objective cannot achieve.}
\begin{tabular}{c|ccccc}
\toprule
\textbf{Method}& \textbf{Accuracy$\uparrow$} & \textbf{Precision$\uparrow$} & \textbf{Recall$\uparrow$} & \textbf{F1-Score$\uparrow$} & \textbf{FID$\downarrow$}  \\
\midrule
MCM & 0.945& 0.881 & 0.839 & 0.860 & 1.884 \\
W/ Weighted and Disentanglement Losses & \textbf{0.946} & \textbf{0.886} & \textbf{0.859} & \textbf{0.872} & 1.605\\
Masked Autoencoder & 0.537 & 0.809 & 0.75 & 0.778 & \textbf{1.031}\\
\bottomrule
\end{tabular}
\label{tab:baseline}
\end{table}


%\subsection{Downstream Classification Task}
%For datasets with class labels, we evaluate the downstream classification task performance using the encoded latent representation of the visible tokens. During the inference of the concepts involved in an image, we remove the mask. To ensure fairness for classifiers that rely solely on image representation, we refrain from using the concept prediction results. Instead, the model predicts the class of an image based solely on the patch representations. Table presents the classification accuracy.


\subsection{Qualitative results}
\label{sec:qual}

\vspace{-10pt}
\paragraph{Masked image reconstruction and editing.}

Masked image reconstruction involves predicting the original image from a partially masked input, where patches are randomly removed. We demonstrate the performance of our approach (training mask ratio 25\%) in reconstructing images with a significant portion of patches masked during testing (e.g., 75\%) in Figure \ref{fig:reconstruction}. Additionally, by providing specific text-based concept tokens, we can traverse the concept latent space and manipulate the reconstruction of the masked patches. The resulting image is reconstructed to align with both the contextual unmasked patches and the specified concepts. Notably, we can either activate each concept individually or combine multiple concepts, as illustrated in Figure \ref{fig:muledit}. We also compare editing results across various model sizes with different computational costs, highlighting that the model's ability to perform masked image editing and reconstruction improves progressively as it scales.


\begin{figure}[t]
    \centering
    \begin{subfigure}[t]{0.33\linewidth}
        \centering
        \includegraphics[width=0.95\linewidth]{figures/celeba_large.png}
        \caption{Image reconstruction results for the large-sized model.}
        \label{fig:reconstruction}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.65\linewidth}
        \raggedleft
        \includegraphics[width=0.87\linewidth]{figures/celeba_base_editing.png}
        \caption{Image editing results. For MCM-Small, from left to right: \lq Not Smiling', \lq Male', \lq Eyeglasses', and \lq Eyeglasses + Male + Bangs + Mustache'. For MCM-Base, left: original; right: \lq Smiling + Eyeglasses'. We demonstrate enhanced image editing quality when scaling up the model.}
        \label{fig:muledit}
    \end{subfigure}
    \vspace{-6pt}
    \caption{Image reconstruction and editing results with a testing mask ratio of 75\%.}
    \label{fig:combined}
\end{figure}



\subsubsection{Varying the test mask ratio}

Figure \ref{fig:testmaskvary} illustrates how the reconstruction results change with different mask ratios during testing, producing a range of reconstructed images that blend visible contextual patches with the provided concepts, proportional to the chosen ratio.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/re_ratio_new.png}
    \caption{We could employ masks of any arbitrary size during the test phase. A larger mask size (e.g., 95\%) provides a reconstruction that better represents the edited concepts, while a smaller mask size (e.g., 0\%) generates images that align more closely with the contexts.}
    \label{fig:testmaskvary}
\end{figure}



\section{Conclusions}
%first work enables control in mask image reconstruction

We introduced the Multi-layer Concept Map (MCM) for efficient concept learning from masked images. MCM employs a reconstruction target enhanced by the weighted concept and disentanglement losses, reducing computational cost while maintaining competitive performance in concept learning. MCM enables effective image editing, producing diverse blends of concepts that align with visible contextual patches for the reconstruction task. We hope this work contributes to more efficient concept learning and enhanced interpretability with large-scale Transformer models.

\vspace{-10pt}
\paragraph{Limitations.}

Unlike conventional concept learning methods that rely on binary labels, MCM utilizes concept embeddings derived through self-supervised learning with the pretrained CLIP model. However, in practice, collecting paired concept-image samples is still necessary for learning effective concept embeddings. Moreover, the concept latent space is not inherently designed for continual learning, where concept classes evolve over time. Future research on dynamically expanding and reusing learned concepts would be valuable for enhancing adaptability in such settings.




\bibliography{references}
\bibliographystyle{iclr2025_conference}



\newpage
\appendix
\section{Appendix}

\subsection{Model settings}
\label{sec:app}
Common parameters across all models included the AdamW optimizer with a learning rate of \(1 \times 10^{-3}\), a weight decay of 0.01, four self-attention heads, hidden layer size of 512, and a batch size of 1024. The Small model featured 2 encoder layers, 1 decoder layer, an MLP size of 128, and was trained for 500 epochs on 48x48 images with a patch size of 6. The Base model expanded to 6 encoder layers, 3 decoder layers, an MLP size of 512, and used 96x96 images with a patch size of 8. Finally, the Large model utilized 12 encoder layers, 6 decoder layers, an MLP size of 1024, and 192x192 images with the same patch size of 8 but was trained for only 100 epochs due to its increased complexity. The coefficients \(\alpha\) and \(\beta\) have a default value of one.
 
\begin{table}[!h]
\centering
\small
\renewcommand{\arraystretch}{1.2}
\caption{Hyperparameters for different model sizes.}
\begin{tabular}{p{6cm}p{4cm}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textbf{Common Parameters}} \\
\hline
Optimizer & AdamW \\
Weight decay & 0.01 \\
Learning rate & $1 \times 10^{-3}$ \\
Number of self-attention heads & 4 \\
Size of hidden layers & 512 \\
 Batch size&1024\\
\hline
\multicolumn{2}{l}{\textbf{Small Model Parameters}} \\
\hline
Number of encoder layers & 2 \\
Number of decoder layers & 1 \\
Size of MLP & 128 \\
Epochs & 500 \\
Image size (CelebA) & 48 \\
Patch size (CelebA) & 6 \\
\hline
\multicolumn{2}{l}{\textbf{Base Model Parameters}} \\
\hline
Number of encoder layers & 6 \\
Number of decoder layers & 3 \\
Size of MLP & 512 \\
Epochs & 500 \\
Image size (CelebA) & 96 \\
Patch size (CelebA) & 8 \\
\hline
\multicolumn{2}{l}{\textbf{Large Model Parameters}} \\
\hline
Number of encoder layers & 12 \\
Number of decoder layers & 6 \\
Size of MLP & 1024 \\
Epochs & 100 \\
Image size (CelebA) & 192 \\
Patch size (CelebA) & 8 \\
\bottomrule
\end{tabular}
\label{tab:hyperparameters}
\end{table}


\subsection{Unbalanced concept class distribution in CelebA}
\label{sec:unba}

The concepts in CelebA exhibit varying frequencies (see Figure \ref{fig:fre}); for instance, "Mustache" is a rare concept, making it particularly challenging to learn. Consequently, we devise the weighted concept loss to tackle the challenge posed by unbalanced concept classes effectively.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/feature_frequency.png}
    \caption{Unbalanced concept classes in the CelebA dataset.}  
    \label{fig:fre}
\end{figure}

\begin{figure}[!t]
    \centering
    \includegraphics[width=0.9\linewidth]{figures/re_mask.png}
    \caption{Image editing with different mask shapes based on the concept set \lq Male + Eyeglasses + Mustache + Smiling'. The random mask method reconstructs the masked patches while preserving consistency with the specific image contexts. In contrast, the square mask fails to produce diverse results across multiple contexts.}
    \label{fig:maskm}
\end{figure}


\subsection{Additional testing time results}

\begin{table}[H]
\centering
\small
\caption{Performance metrics at different training mask ratios (base-sized model). For all experiments, we employ \textit{a testing mask ratio of 0}, i.e., using the entire image patches for evaluation.}
\begin{tabular}{c|ccccc}
\toprule
\textbf{Mask Ratio} & \textbf{Accuracy$\uparrow$} & \textbf{Precision$\uparrow$} & \textbf{Recall$\uparrow$} & \textbf{F1-Score$\uparrow$} & \textbf{FID$\downarrow$} \\
\hline
0.0  & 0.940 & 0.854 & 0.812 & 0.832 & 1.520 \\
0.1  & 0.944 & 0.824 & 0.794 & 0.809 & 1.856 \\
0.25 & \textbf{0.947} & 0.877 & \textbf{0.824} & \textbf{0.850} & \textbf{1.843} \\
0.5  & 0.946 & 0.881 & 0.819 & 0.849 & 2.112 \\
0.75 & 0.946 & \textbf{0.890} & 0.812 & 0.849 & 3.790 \\
0.9  & 0.938 & 0.840 & 0.776 & 0.807 & 8.045 \\
\bottomrule
\end{tabular}
\label{tab:maskb}
\end{table}


\subsection{Design of the testing time mask}

We compare the editing results between the random mask and the square mask methods in Figure \ref{fig:maskm}. Using a random mask generated an image that aligned with both the visible contextual tokens and the provided concepts. However, a square mask produced unnatural and repetitive editing results across samples, which failed to fit the visible contextual tokens.


\end{document}
