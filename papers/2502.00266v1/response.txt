\section{Related work}
\subsection{Masked image reconstruction}

Masked image modeling has emerged as a pivotal learning technique in computer vision **Bao, "Self-supervised Visual Representation Learning through Image Patch Order Prediction"**. For instance, Masked Autoencoders **He et al., "masked autoencoders are scalable vision learners"** learn to reconstruct missing patches given only a small subset of visible patches reducing computational cost. Masked Diffusion Transformer **Ho et al., "Masked Diffusion Transformer: Fast and Accurate Denoising for Image Modeling"** demonstrates enhanced training efficiency and generation results using a denoising diffusion objective on masked images. Cross-Attention Masked Autoencoders **Pang et al., "Cross-attention guided masked autoencoders"** used cross-attention in the decoder to query visible tokens for masked image patch reconstruction. The cross-attention component takes a weighted sum of the visible tokens across different input blocks to fuse the features for each decoder block, leveraging low-level information for reconstruction.

\subsection{Disentangled representation learning}

Disentangled concept learning **Locatello et al., "Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations"** aims to uncover the underlying explanatory factors hidden within observed data. For example, methods such as Î²-VAE **Higgins et al., "beta-vae: learning basic visual concepts by few-shot inference"** and FactorVAE **Burgess et al., "Monetizing Deep Learning Research in Computer Vision"** search for directions in the latent space that correlate with distinct human-interpretable concepts. Moreover, Concept Tokenization **Tsai et al., "Concept tokenization for image classification"** focuses on learning disentangled object representations and inspecting latent traversals for various factors. Additionally, Concept Bottleneck models **Sohn et al., "Learning Steerable Convolutional Neural Networks for Image Recognition"** learn representations that correspond to specific human-understandable concepts. Energy-based methods **Zhai et al., "Energy-based disentanglement learning with cross-validation"** aim to compute energy functions of various concepts and combine their probability distributions achieving conjunctions, disjunctions, and negations of various concepts.

Conventional concept learning methods typically rely on fully observable images for training. While masking strategies have proven effective in reducing computational cost in natural language processing, their usage in concept learning tasks remains underexplored. This is primarily because masking a large portion of image patches greatly limits the information available for disentangling effective concepts. To address this challenge, we integrate learnable concept tokens at various granularity levels into the masked reconstruction process using the asymmetric Multi-layer Concept Map (MCM) architecture. This approach could not only reduce computational cost for learning effective concepts but also enhance model's concept prediction capability. Our goal is to advance the masked concept learning objective, paving the way for more efficient model architectures.