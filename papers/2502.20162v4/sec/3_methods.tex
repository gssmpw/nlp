\section{Methods}
\label{sec:methods}

\subsection{Preliminaries}
\label{sec:preliminaries}
Consider a classification problem with $K$ classes. During model training we
have access to a data set composed of distinct source data distributions (or
\textit{domains}), $\mathcal{S} = \left\{D_1, D_2,\dots,
D_{\card{S}}\right\}$. From each domain $D_i$, we observe $n_i$
labeled data points, such that $(\bm{x}_j^{(i)}, y_j^{(i)}) \sim D_i$, for
$j=1,\dotsc,n_i$. Similarly, the test dataset consists of $\mathcal{T} =
\left\{D^{T}_1, D^{T}_2, \dotsc, D^{T}_{\card{T}}\right\}$
\textit{unseen} target data distributions, from which the model cannot retrieve
any information during training. The goal is to learn a single labeling function
$h(\bm{x}; \bm{\theta})$, parameterized by $\bm{\theta}$, which correctly maps
input observations $\bm{x}_j^{(i)}$ to their labels ${y_j^{(i)}}$ for both the
seen source and the unseen target domains.

If $\mathcal{L}_i(\bm{\theta}) = \frac{1}{n_i} \sum_{j=1}^{n_i}
\ell(h({\bm{x}_j}^{(i)};\bm{\theta}), {y_j^{(i)}})$ represents the loss associated to the
$i$-th source data domain in the training set, we define the overall cost
function $\mathcal{L}(\bm{\theta}) = \frac{1}{\card{S}} \sum_{i=1}^{\card{S}}
\mathcal{L}_i(\bm{\theta})$, as the average loss over all available source domains.
The function $\ell(\cdot, \cdot)$ is a classification loss, in our
case cross-entropy, that measures the error between the predicted label
$\hat{y}$ of an input observation and its true label $y$. The standard way of model training is \emph{Empirical Risk Minimization} or \emph{ERM} which uses the following objective on the training domain data
\begin{equation}\label{erm-optimization-eq}
	\bm{\theta}^{*} = \arg_{\bm{\theta}}\min \frac{1}{\card{S}} \sum_{i=1}^{\card{S}} \mathcal{L}_i(\bm{\theta}) + \lambda R(\bm{\theta})
\end{equation}
where $R(\cdot)$ is a regularization term and $\lambda$ a hyperparameter responsible for controlling the contribution of regularization to the loss, leading to a parameter vector $\bm{\theta}^{*}$. In practice, networks trained via ERM have been shown to overfit the data distributions present in the training set. Previous works (such as \cite{shi2021gradient}) have observed that the directions of gradients for different domains during training play a significant role in model generalization. Given source domain losses $\mathcal{L}_i (\bm{\theta})$ and $\mathcal{L}_j(\bm{\theta})$ and their corresponding gradients $\bm{g}_i = \nabla_{\bm{\theta}} \mathcal{L}_i(\bm{\theta})$, $\bm{g}_j = \nabla_{\bm{\theta}} \mathcal{L}_j(\bm{\theta})$, gradient \textbf{conflicts} arise when the angle of the gradients grows e.g., above $\lvert \pi/2 \rvert$, or, equivalently, their cosine similarity $\frac{\bm{g}^{T}_i \cdot \bm{g}_j}{\|\bm{g}_i\| \|\bm{g}_j\|}$ becomes negative. In the following section, we further explore the role of gradient similarity as an indicator for domain overfitting.

%% Previous work has studied gradient alignment for domain generalization, but has been limited to the application of specfic operations on the conflicting gradients, such as projecting domain $i$'s gradient onto the normal vector of domain $j$'s gradient \cite{yu2020gradient}, or on either setting the conflicting gradients to zero or to random values drawn from a normal distribution. In this work, inspired by \textit{Simulated Annealing} optimization \cite{kirkpatrick1983optimization}, we propose an alternative optimization strategy and randomly search for a point in a neighborhood of the network's parameter space where gradient conflicts between domains is small.

\subsection{A Generative Model for Domain Generalization}

\label{sec:generative}
\begin{figure}
  \centering
  %\scalebox{.7}{
    \input{figures/generative}
  %}
    %
    \caption{Simplified generative model for multi-domain data.}
    \label{fig:generative}
\end{figure}

To assist with the development and understanding of the proposed method consider the generative model of Figure \ref{fig:generative}, summarized by the following process:
\begin{align*}
  y &\sim Multinoulli(p_1, \dotsc, p_K)\\
  d &\sim Multinoulli(q_1, \dotsc, q_L)\\
  \bm{z}_y &\sim p_y \\
  \bm{z}_d &\sim p_d \\
  \bm{e} &\sim p_e \\
  \bm{x} &= f(\bm{z}_y, \bm{z}_d, \bm{e})
\end{align*}
where $y$ is a variable corresponding to the class, $d$ is a variable corresponding to the domain (with $L=\card{S}+\card{T}$), $\bm{z}_y$ is a latent multivariate class-specific representation of class $y$, drawn from an unknown distribution $p_y$, and $\bm{z}_d$ is a latent multivariate domain-specific representation of the domain $d$, drawn from an unknown distribution $p_d$. Notice that these representations are disentangled, i.e., $\bm{z}_y$ does not depend on $d$. The observed sample $\bm{x}$, on the other hand, is derived using the function $f(\bm{z}_y, \bm{z}_d, \bm{e})$ that depends on both the class and domain representations, as well as independent nuisance variables, $\bm{e}$.

Without loss of generality, let's assume that we have two source domains ($\card{S} = 2$) and two classes ($K = 2$). Then, during model training we sample only from the source domains and use the loss $\mathcal{L} = \alpha_1\mathcal{L}_1 + \alpha_2\mathcal{L}_2$,
\begin{equation}
  \mathcal{L}_i = -\mathbb{E}_{d=i}\left[\log h(y | \bm{x};\bm{\theta})\right]
  \label{eq:loss}
\end{equation}
where $h$ is the probability estimated by our model for class $y$,  $\alpha_i$ is the percentage of samples of domain $i$, and $i = 1, 2$. The expectation is over samples that have been generated using the above process when $d=i$. Each gradient update step depends on the loss gradient
\begin{equation}
  \begin{split}
  \nabla_{\bm{\theta}}\mathcal{L} &= \alpha_1\nabla_{\bm{\theta}}\mathcal{L}_1 + \alpha_2\nabla_{\bm{\theta}}\mathcal{L}_2\\
  &= -\alpha_1\mathbb{E}_{d=1}\left[\nabla_{\bm{\theta}}\log h(y | \bm{x};\bm{\theta})\right] \\
  & -\alpha_2\mathbb{E}_{d=2}\left[\nabla_{\bm{\theta}}\log h(y | \bm{x};\bm{\theta})\right]
  \end{split}
  \label{eq:expgrad}
\end{equation}
where we have used the linearity of the expectation to obtain the expectation of
the derivatives. Notice that the gradient update step depends on
$\nabla_{\bm{\theta}}\log h(y | \bm{x};\bm{\theta}) = \nabla_{\bm{\theta}}\log h(y | f(\bm{z}_y, \bm{z}_d, \bm{e} ;\bm{\theta}))$.

For models that use an internal domain-invariant representation
(i.e., a representation that does not depend on $\bm{z}_d$) it will hold that
$h(y|\bm{x}) = h(y|f(\bm{z}_y, \bm{z}_1, \bm{e})) = h(y|f(\bm{z}_y,\bm{z}_2,
\bm{e}))$, and therefore $\mathbb{E}_{d=1}\left[u(h(y|\bm{x}))\right] =
\mathbb{E}_{d=2}\left[u(h(y|\bm{x}))\right]$ for any function $u$. Thus, both
the losses, $\mathcal{L}_i$ and their gradients,
$\nabla_{\bm{\theta}}\mathcal{L}_i$, should be equal, in expectation, for
different domains $i$. This observation provides a \emph{necessary} condition for
domain-invariance, which in turn provisions the main motivation for the development of our method, presented in the following section.

\subsection{Gradient-Guided Annealing for DG}
\label{sec:annealing}

\begin{algorithm}[t!]
	\caption{Implementing GGA in training}
	\begin{algorithmic}[1]
		\Require   Pretrained DNN $h$
		with parameters $\bm{\theta}_0$, training dataset with domains $\mathcal{S}=\{ D_{i}\}_1^{\card{S}}$. Loss gradients $\bm{g}$. Learning rate $\eta$, perturbation parameter $\rho$, optimization step to start the annealing process $A_s$, optimization step to end the annealing process $A_e$, number of annealing iterations per optimization step $n_a$. Total number of training iterations $n$.
		\item[]
		\For{$t \leftarrow 1$ \textbf{to} $n$}
		\State  Sample a mini-batch: $\mathcal{B} \leftarrow \mathcal{B}_{\mathcal{D}_1}+...+\mathcal{B}_{\mathcal{D}_{\card{S}}}$
		\If{$A_s \leq t \leq A_e$}
		\State \texttt{\#\small{Begin Gradient-Guided Annealing:}}
		\State Compute the mini-batch loss at starting params: \\\hspace{1.5cm} $\mathcal{L}_B \leftarrow \mathcal{L}(\mathcal{B}; \bm{\theta}_t)$
		\State Calculate minimum domain grad pair sim: \\\hspace{1cm} 
		$\text{sim} \leftarrow \min\big( \sum\limits_{\substack{i=1, j=1, \\
				i \neq j}}^{i \leq \card{S}, j \leq \card{S}} \frac{\bm{g}^{T}_i \cdot \bm{g}_j}{\|\bm{g}_i\| \|\bm{g}_j\|}\big)$ 
		\State $\bm{\theta}_{t} \leftarrow \bm{\theta}_{t-1}$
		\For{ step $a \leftarrow 1$ \textbf{to} $n_a$}
		\State $\bm{\theta}_a \leftarrow \bm{\theta}_t + \mathcal{U}(-\rho, \rho)$
		\State Calculate minimum domain grad pair sim: \\\hspace{1.5cm} $\text{sim}_a \leftarrow \min\big( \sum\limits_{\substack{i=1, j=1,\\ i \neq j}}^{i \leq \card{S}, j \leq \card{S}} \frac{\bm{g}^{T}_i \cdot \bm{g}_j}{\|\bm{g}_i\| \|\bm{g}_j\|}\big)$
		\State Compute the mini-batch loss at new params: \\\hspace{1.5cm} $\mathcal{L}_a \leftarrow \mathcal{L}(\mathcal{B}; \bm{\theta}_a)$
		\If{$\big(\text{sim}_a > \text{sim} \big) \wedge \big(\mathcal{L}_a - \mathcal{L}_{\mathcal{B}} < 0.1 \big) $}
		\State $\mathcal{L}_{\mathcal{B}} \leftarrow \mathcal{L}_a$, $\bm{\theta}_t \leftarrow \bm{\theta}_a$, $\text{sim} \leftarrow \text{sim}_a$
		\EndIf
		\EndFor
		\EndIf
		\State \texttt{\#\small{Update weights:}}
		\State Compute mini-batch loss: $\mathcal{L}_\mathcal{B} =\mathcal{L}(\mathcal{B};\bm{\theta}_t)$
		\State $\bm{\theta}_{t+1}=\bm{\theta}_{t} -\eta \cdot \nabla\mathcal{L}_\mathcal{B}(\mathcal{B};\bm{\theta}_t)$
		\State $t=t+1$
		\EndFor
	\end{algorithmic}
	\label{alg:GGA}
\end{algorithm}

The analysis presented in the previous section shows that we can use the
agreement of gradients $\nabla\mathcal{L}_i$ as an indicator of domain
invariance, since the less the model $h(\cdot|\bm{\theta})$ depends on the domain
$d$, the more similar the expected value of the gradient,
$\mathbb{E}[\nabla\mathcal{L}_d]$, will be (see Eq. \eqref{eq:expgrad}). Inspired
by Simulated Annealing \cite{kirkpatrick1983optimization}, we achieve this by
adding random noise to the parameters of the model in search for a point with
high domain gradient agreement, as measured by the increase of the minimum gradient similarity across any pair of domains,
\begin{equation}
	\text{grad\_sim} = \min \big( \sum\limits_{\substack{i=1, j=1, \\
			i \neq j}}^{i \leq \card{S}, j \leq \card{S}} \frac{\bm{g}^{T}_i \cdot \bm{g}_j}{\|\bm{g}_i\| \|\bm{g}_j\|}\big)
  \label{eq:gradsim}
\end{equation}

In more detail, as it is common in DG literature, we start with a pre-trained
model $f(\cdot, \bm{\theta}_0)$ and perform a small number of warmup training
steps. This ensures that the model approaches a region of the parameter space
that achieves a low loss for the target problem. We then calculate the minimum 
pairwise gradient similarity among source domains and begin searching the
neighborhood of the parameter space for points where both: (a) the domain 
gradients agree and (b) the loss are reduced\footnote{In our experiments, we relax the loss constraint and accept the new parameter set when the gradient similarity has increased and training loss has been reduced within a specific range (i.e $\mathcal{L} - \mathcal{L}' < 0.1$), where $\mathcal{L}'$ is the loss value at the updated set of parameters.}. This is implemented through 
iterative random perturbations $\bm{\theta}' \leftarrow \bm{\theta} + \mathcal{U}(-\rho, \rho)$, where $\mathcal{U}$ is the multivariate uniform 
distribution. Each new point is selected only if it simultaneously achieves 
higher gradient agreement and lower loss (thus improving the Pareto front). 
After a fixed number of iterations, the point that has achieved the highest 
gradient agreement and the lowest loss is selected. This process is repeated 
after every training iteration step. After a number of optimization steps, we 
allow the model to be trained without parameter perturbations, following a 
standard SGD-based procedure, as usual. The GGA algorithm is presented in 
\textbf{Algorithm} \ref{alg:GGA}.