\section{Conclusions}
\label{sec:conclusions}

In this work, we investigated gradient conflicts in the context of domain generalization, leading to the development of GGA; a simple yet effective algorithm that identifies points in the parameter space where domain gradients align early in training before continuing optimization, ultimately enhancing model generalization. Through a comprehensive comparative analysis we have demonstrated the efficacy of GGA, which is able to achieve superior generalization capabilities in standard DG benchmarks and outperform SoTA 
baselines. What's more, as GGA is both model and method agnostic it can also be utilized by other algorithms. Interestingly, its combination with previous methods proves beneficial, as their majority yields improved performance over all benchmarks. Finally, we validate the impact of our algorithm on gradient alignment through additional experiments, showing that, unlike the baseline, domain gradients align during training.

However, our method does not come without some key limitations. First of all,
the annealing steps and gradient similarity computation add computational overhead. Furthermore, GGA relies on domain labels, making it inapplicable to single-source DG, where domain labels are not available during training. Its effectiveness is also batch-size dependent, as larger batches provide further information regarding gradient alignment and can lead to improved results. Future work will further explore gradient alignment in multi-domain settings and refine GGA to make annealing steps adaptive rather than random.

\noindent\textbf{Acknowledgement.} This work was supported by the European Unionâ€™s Horizon 2020 research and innovation programme under Grant Agreement No. 965231, project REBECCA (REsearch on BrEast Cancer induced chronic conditions supported by Causal Analysis of multi-source
data).



