\begin{table*}[h]
	\centering
	\small
	\caption{{\bf Comparison of \textbf{GGA} with the ERM baseline}. The top out-of-domain accuracies on five domain generalization benchmarks averaged over three trials, are presented.}
	\label{table:baseline-results}
	% \vspace{-0.5em}
	%	\renewcommand{\arraystretch}{1.1}
	%	\setlength{\tabcolsep}{3pt}
	%	\setlength{\abovetopsep}{0.5em}
	\begin{tabular}{l|ccccc|c}
		\toprule
		Algorithm & PACS & VLCS & OfficeHome & {TerraInc} & {DomainNet} & {Avg.}  \\
		\midrule
		ERM  & %\cite{vapnik1998statistical}   & 
		85.5\scriptsize$\pm0.2$             & 
		77.3\scriptsize$\pm0.4$             & 
		66.5\scriptsize$\pm0.3$             & 
		46.1\scriptsize$\pm1.8$             & 
		43.8\scriptsize$\pm0.3$             & 63.9  \\
		\midrule
		\textbf{GGA} (ours)                 & 
		\textbf{87.3}\scriptsize{$\pm0.4$}           & 
		\textbf{79.9}\scriptsize{$\pm0.4$}           & 
		\textbf{68.5}\scriptsize{$\pm0.2$}           &  
		\textbf{50.6}\scriptsize{$\pm0.1$}           & 
		\textbf{45.2}\scriptsize{$\pm0.2$}           & \textbf{66.3}  \\
		%		\midrule
		
		
		\bottomrule
		
	\end{tabular}
	% \vspace{-0.5em}
\end{table*}


\begin{table*}[h]
	\centering
	\small
	\caption{\textbf{Comparison with state-of-the-art domain generalization methods.} Out-of-domain accuracies on five domain generalization benchmarks are shown. Top performing methods are highlighted in \textbf{bold} while second-best are \textit{underlined}.
		The results marked by $\dagger, \ddagger$ are copied from Gulrajani and Lopez-Paz \cite{gulrajani2020domainbed} and Wang et al. \cite{wang2023sharpness}, respectively. For fair comparison, the training of each algorithm combined with \textbf{GGA}, were run on the respective codebases. Average accuracies and standard errors are calculated from three trials for the combination of GGA with past algorithms and from 5 trials for GGA. In {\color{green} green} and {\color{red} red}, we highlight the performance boost and decrease of applying GGA on top of each algorithm respectively, averaged over three trials. Due to computational resources, for DomainNet we do not combine GGA with previous methods.}
	\label{table:total-results}
	% \vspace{-0.5em}
	\renewcommand{\arraystretch}{1.1}
	\setlength{\tabcolsep}{3pt}
	\setlength{\abovetopsep}{0.5em}
	\begin{tabular}{l|cccc|c||c|c}
		\toprule
		Algorithm & PACS          & VLCS          & OfficeHome    & {TerraInc}    & {Avg.} & DomainNet & Total \\
		\midrule
		
		Mixstyle$^\ddagger$ \cite{zhou2021mixstyle}     & 
		85.2\scriptsize{$\pm0.3$} 
		\small{({\color{green}$+0.3$})}         & 
		77.9\scriptsize{$\pm0.5$} \small{({\color{green}$+0.6$})}          & 60.4\scriptsize{$\pm0.3$} \small{({\color{green}$+0.5$})}          & 44.0\scriptsize{$\pm0.7$}
		\small{({\color{green}$+1.1$})}          & 
		%		
		66.9 \small{({\color{green}$+0.6$})}         &
		34.0\scriptsize{$\pm0.1$}           & 60.3 \\
		
		GroupDRO$^\ddagger$ \cite{Sagawa2020GroupDRO}    & 
		84.4\scriptsize{$\pm0.8$}          
		\small{({\color{green}$+1.4$})}   & 
		76.7\scriptsize{$\pm0.6$}          
		\small{({\color{green}$+0.6$})}   & 
		66.0\scriptsize{$\pm0.7$}             
		\small{({\color{green}$+2.2$})}   & 
		43.2\scriptsize{$\pm1.1$}              
		\small{({\color{green}$+1.5$})}   & 
		%		 
		67.6 \small{({\color{green}$+1.4$})}         &
		33.3\scriptsize{$\pm0.2$}          & 60.7 \\
		
		MMD$^\ddagger$ \cite{li2018mmd}                  & 
		84.7\scriptsize{$\pm0.5$}          
		\small{({\color{green}$+0.8$})} & 
		77.5\scriptsize{$\pm0.9$}            
		\small{({\color{green}$+1.3$})} & 
		66.3\scriptsize{$\pm0.1$}          
		\small{({\color{green}$+1.6$})} & 
		42.2\scriptsize{$\pm1.6$}            
		\small{({\color{green}$+6.3$})} & 
		%		 
		67.7 \small{({\color{green}$+2.5$})} &      
		23.4\scriptsize{$\pm9.5$}           &  58.8 \\
		
		AND-mask \cite{shahtalebi2021sand} & 
		84.4\scriptsize{$\pm0.9$}  
		\small{({\color{green}$+0.1$})}& 
		78.1\scriptsize{$\pm0.9$}  
		\small{({\color{green}$+0.3$})}& 
		65.6\scriptsize{$\pm0.4$}  
		\small{({\color{green}$+1.2$})} & 
		44.6\scriptsize{$\pm0.3$} 
		\small{({\color{red}$-0.4$})}
		& 68.2 \small{({\color{green}$+0.3$})}    &
		37.2\scriptsize{$\pm0.6$} & 62.0 \\
		
		ARM$^\ddagger$ \cite{zhang2020arm}               &
		85.1\scriptsize{$\pm0.4$}          
		\small{({\color{green}$+0.5$})}& 
		77.6\scriptsize{$\pm0.3$} 
		\small{({\color{green}$+0.9$})}           & 
		64.8\scriptsize{$\pm0.3$} 
		\small{({\color{green}$+2.0$})}           & 
		45.5\scriptsize{$\pm0.3$} 
		\small{({\color{green}$+0.8$})}           & 
		%		 
		68.3 \small{({\color{green}$+1.1$})}         &
		35.5\scriptsize{$\pm0.2$}          & 61.7 \\
		
		IRM$^\dagger$ \cite{arjovsky2019irm}            & 
		83.5\scriptsize{$\pm0.8$}          
		\small{({\color{red}$-1.5$})}& 
		78.5\scriptsize{$\pm0.5$}          
		\small{({\color{red}$-0.9$})} & 
		64.3\scriptsize{$\pm2.2$}          
		\small{({\color{red}$-2.1$})}& 
		47.6\scriptsize{$\pm0.8$}          
		\small{({\color{red}$-3.9$})} & 
		%		 
		68.5 \small{({\color{red}$-2.1$})}        &
		33.9\scriptsize{$\pm2.8$}          & 61.6 \\
		
		MTL$^\ddagger$ \cite{blanchard2021mtl_marginal_transfer_learning}    & 
		84.6\scriptsize{$\pm0.5$}           
		\small{({\color{green}$+0.9$})} & 
		77.2\scriptsize{$\pm0.4$}          
		\small{({\color{green}$+2.0$})} & 
		66.4\scriptsize{$\pm0.5$}          
		\small{({\color{green}$+0.4$})} & 
		45.6\scriptsize{$\pm1.2$}          
		\small{({\color{green}$+0.6$})}&  
		%		 
		68.5 \small{({\color{green}$+0.9$})}        &
		40.6\scriptsize{$\pm0.1$}          & 62.9 \\
		
		
		VREx$^\ddagger$ \cite{krueger2020vrex}           & 
		84.9\scriptsize{$\pm0.6$} 
		\small{({\color{green}$+0.6$})}           & 
		78.3\scriptsize{$\pm0.2$} 
		\small{({\color{green}$+0.1$})}           & 
		66.4\scriptsize{$\pm0.6$} 
		\small{({\color{green}$+1.3$})}           & 
		46.4\scriptsize{$\pm0.6$} 
		\small{({\color{green}$+2.0$})}           & 
		%		 
		69.0 \small{({\color{green}$+1.0$})}      &
		33.6\scriptsize{$\pm2.9$}          & 61.9   \\
		
		MLDG$^\dagger$ \cite{li2018learning}                & 
		84.9\scriptsize{$\pm1.0$}          
		\small{({\color{green}$+0.7$})} &
		77.2\scriptsize{$\pm0.4$}          
		\small{({\color{green}$+1.3$})} & 
		66.8\scriptsize{$\pm0.6$}          
		\small{({\color{green}$+1.2$})} &
		47.7\scriptsize{$\pm0.2$}          
		\small{({\color{green}$+1.1$})} & 
		%		 
		69.2 \small{({\color{green}$+1.2$})}    &
		41.2\scriptsize{$\pm0.1$}            & 63.6  \\
		
		Mixup$^\dagger$ \cite{xu2020interdomain_mixup_aaai}             & 
		84.6\scriptsize{$\pm0.6$}            
		\small{({\color{green}$+1.2$})} & 
		77.4\scriptsize{$\pm0.6$}          
		\small{({\color{green}$+1.8$})} & 
		68.1\scriptsize{$\pm0.3$}            
		\small{({\color{green}$+1.0$})} & 
		47.9\scriptsize{$\pm0.8$}            
		\small{({\color{green}$+2.0$})} & 
		%		
		69.5   \small{({\color{green}$+1.5$})}    &
		39.2\scriptsize{$\pm0.1$}            & 63.4    \\
		
		
		SagNet$^\dagger$ \cite{nam2019sagnet}           &             
		86.3\scriptsize{$\pm0.2$} 
		\small{({\color{red}$-1.0$})} & 
		77.8\scriptsize{$\pm0.5$}          
		\small{({\color{green}$+0.9$})} & 
		68.1\scriptsize{$\pm0.1$}          
		\small{({\color{green}$+0.3$})}& 
		48.6\scriptsize{$\pm1.0$}          
		\small{({\color{green}$+0.7$})}& 
		%		 
		70.2 \small{({\color{green}$+0.4$})}        &
		40.3\scriptsize{$\pm0.1$}          &  64.2 \\
		
		
		CORAL$^\dagger$ \cite{sun2016coral}             & 
		86.2\scriptsize{$\pm0.3$}          
		\small{({\color{green}$+0.7$})}& 78.8\scriptsize{$\pm0.6$}          
		\small{({\color{red}$-0.4$})} & 
		68.7\scriptsize{$\pm0.3$}          
		\small{({\color{green}$+0.2$})} & 
		47.6\scriptsize{$\pm1.0$}          
		\small{({\color{green}$+0.3$})}& 
		%		 
		70.3    \small{({\color{green}$+0.2$})}    &
		41.5\scriptsize{$\pm0.1$}          & 64.5   \\
		
		\midrule
		
		RSC$^\dagger$ \cite{huang2020rsc}               & 
		85.2\scriptsize{$\pm0.9$}          
		\small{({\color{green}$+0.1$})}& 77.1\scriptsize{$\pm0.5$}          
		\small{({\color{green}$+0.2$})}& 
		65.5\scriptsize{$\pm0.9$} \small{($+0.0$)}        & 
		46.6\scriptsize{$\pm1.0$}          
		\small{({\color{green}$+0.2$})}& 
		%		 
		68.6 \small{({\color{green}$+0.1$})}   &
		38.9\scriptsize{$\pm0.5$}          & 62.7 \\
		
		Fish $^\ddagger$ \cite{shi2021gradient}                     & 
		85.5\scriptsize{$\pm0.3$}          
		\small{({\color{green}$+0.1$})} & 
		77.8\scriptsize{$\pm0.3$}          
		\small{({\color{green}$+0.9$})} & 
		68.6\scriptsize{$\pm0.4$}            
		\small{({\color{red}$-0.6$})} & 
		45.1\scriptsize{$\pm1.3$}            
		\small{({\color{green}$+3.8$})} & 
		%		 
		69.3 \small{({\color{green}$+1.0$})}      &
		42.7\scriptsize{$\pm0.2$}            &   63.9    \\ 
		
		SAM $^\ddagger$ \cite{foret2020sharpness}  & 
		85.8\scriptsize$\pm0.2$             
		\small{({\color{green}$+0.6$})} & 
		79.4\scriptsize$\pm0.1$              
		\small{({\color{green}$+0.7$})} & 
		\underline{69.6}\scriptsize$\pm0.1$              
		\small{({\color{green}$+0.4$})} & 
		43.3\scriptsize$\pm0.7$              
		\small{({\color{green}$+2.6$})} & 
		%		 
		69.5 \small{({\color{green}$+1.1$})}   &
		44.3\scriptsize$\pm0.0$              &  64.5 \\
		
		GSAM $^\ddagger$ \cite{zhuang2022surrogate}  & 
		85.9\scriptsize$\pm0.1$           
		\small{({\color{green}$+0.4$})} & 
		79.1\scriptsize$\pm0.2$             
		\small{({\color{green}$+1.0$})} & 
		69.3\scriptsize$\pm0.0$             
		\small{({\color{green}$+0.3$})}& 
		47.0\scriptsize$\pm0.8$             
		\small{({\color{green}$+0.6$})}& 
		%		 
		70.3  \small{({\color{green}$+0.2$})}  &
		44.6\scriptsize$\pm0.2$             &   65.1 \\
		
		SAGM $^\ddagger$ \cite{wang2023sharpness}       & 
		\underline{86.6}\scriptsize{$\pm0.2$}           
		\small{({\color{green}$+0.2$})}& 
		\textbf{80.0}\scriptsize{$\pm0.3$}           
		\small{({\color{red}$-0.3$})}& 
		\textbf{70.1}\scriptsize{$\pm0.2$}           
		\small{({\color{red}$-0.6$})}& 
		\underline{48.8}\scriptsize{$\pm0.9$}           
		\small{({\color{red}$-0.1$})}& 
		%		 
		\underline{71.4} \small{({\color{red}$-0.2$})}  &
		45.0\scriptsize{$\pm0.2$}           &  66.1 \\
		
		%		\midrule
		\midrule
		\textbf{GGA} (ours)                 & 
		\textbf{87.3}\scriptsize{$\pm0.4$}           & 
		\underline{79.9}\scriptsize{$\pm0.4$}           & 
		68.5\scriptsize{$\pm0.2$}           &  
		\textbf{50.6}\scriptsize{$\pm0.1$}    &  \textbf{71.7} &
		\textbf{45.2}\scriptsize{$\pm0.2$} & \textbf{66.3}\\
		%		\midrule
		
		
		\bottomrule
		
	\end{tabular}
	% \vspace{-0.5em}
\end{table*}


\section{Experiments}
\label{sec:experiments}

\subsection{Experimental setup and implementation details}

In our experiments, we follow the protocol of DomainBed 
\cite{gulrajani2020domainbed} and exhaustively evaluate our algorithm against state-of-the-art algorithms on five DG benchmarks, using the same dataset splits and model selection. For the hyperparameter search space, we follow \cite{cha2021swad} in order to avoid the high computational burden of DomainBed. The datasets included in the benchmarks are, PACS \cite{li2017deeper} (9,991 images, 7 classes, and 4 domains), VLCS \cite{fang2013unbiased} (10,729 images, 5 classes, and 4 domains), OfficeHome \cite{venkateswara2017deep} (15,588 images, 65 classes, and 4 domains), TerraIncognita \cite{beery2018recognition} (24,788 images, 10 classes, and 4 domains) and DomainNet \cite{peng2019moment} (586,575 images, 345 classes,
and 6 domains).

In all experiments, the \textit{leave-one-domain-out} cross-validation protocol is followed. Specifically, in each run a single domain is left out 
as the target (test) domain, while the rest of the domains are used for 
training. The final performance of each algorithm is calculated by averaging the top-1 accuracy on the target domain, with different train-validation splits. For training, we utilize a ResNet-50 
\cite{he2016deep} pretrained on ImageNet \cite{russakovsky2015imagenet} for the backbone feature extractor and ADAM for the optimizer. Regarding GGA, 
during training we let each algorithm run for several training iterations depending on the dataset and then begin the parameter space 
search, as described in Section \ref{sec:methods}. For the neighborhood size
in the search process, we set $\rho$ to $0.0005$\footnote{The selection of $\rho$ was based on previous algorithms that implement weight perturbations in ResNet-50 networks \cite{foret2020sharpness, wang2023sharpness, le2024gradient}. The sensitivity analysis presented in subsection \ref*{sensitivity} also reveals that $\rho = 0.0005$ yields the best performance.} and search for a total of $A = 250$ steps before moving to the next mini-batch of 48 samples from each domain, in each dataset. In all experiments, we perform search iterations for $100$ different mini-batches during early training stages. The rest of the hyperparameters, such as learning rate, weight decay and dropout rate, are tuned according to \cite{cha2021swad} and are presented in Table \ref{table:hyperparameters}.
To account for the variability introduced in the random search of GGA, we 
repeat the experiments with 5 different seeds for each dataset. All models were trained on a cluster containing $4\times40$GB NVIDIA A$100$ GPU cards, split into $8$ $20$GB virtual MIG devices and $1\times24$GB NVIDIA RTX A$5000$ GPU card.

\begin{table}[h]
	\centering
	%\footnotesize
		\resizebox{\linewidth}{!}{
		\begin{tabular}{lrrrrr} 
			\toprule
			\textbf{Hyperparameter}& PACS & VLCS  & OH & TI & DN\\ \midrule
			Learning rate&{3e-5}&{1e-5}&{1e-5}& 1e-5 & 3e-5 \\ 
			Dropout & 0.5 & 0.5 & 0.5 & 0.5 & 0.5 \\ 
			Weight decay&1e-4 & 1e-4&1e-4 & 1e-4 & 1e-4 \\ 
			Training Steps & 5000 & 5000 & 5000 & 5000 & 15000 \\
			$\rho$ & {5e-5} & {5e-5} & {5e-5} & {5e-5} & 5e-5 \\
			GGA Start-End & 100-150, 1500-1550 & 100-200 & 100-200 & 500-600 & 100-200 \\ 
			\bottomrule
		\end{tabular}
				}
	\caption{Hyperparameters for DG experiments. OH, TI and DN stand for OfficeHome, TerraIncognita and DomainNet respectively. $\rho$ is the parameter space search used during the annealing steps in GGA, while the last row indicates the training iterations during which GGA occurs.}
	\label{table:hyperparameters}
\end{table}

\subsection{Comparative Evaluation}
\label{results}

The average OOD performances of the baseline vanilla ERM \cite{vapnik1998statistical} and state-of-the-art DG methods on a total of 5 DG benchmarks, are reported in Tables \ref{table:baseline-results} and \ref{table:total-results} respectively. The results for each separate domain in each benchmark are reported in the Appendix. In the experiments, we apply GGA on-top of the rest of the DG algorithms and show that its properties generalize to other methods as well, boosting their overall performance. It should also be noted that GGA can be adapted to training scenarios where distributions shifts are present in training data. 

Initially, to validate whether the application of GGA in the early stages of 
model training leads to models with increased generalizabilty, we compare the 
results with the vanilla ERM baseline. As presented in Table \ref{table:baseline-results}, GGA is able to boost the 
performance of the baseline model by an average of 2.4\% on all 
benchmarks, demonstrating the efficacy of the proposed algorithm. For further evaluation, we also compare GGA with state-of-the-art DG methods \cite{li2018mmd, zhou2021mixstyle, Sagawa2020GroupDRO, arjovsky2019irm, zhang2020arm, krueger2020vrex, shahtalebi2021sand, ganin2016dann, huang2020rsc, blanchard2021mtl_marginal_transfer_learning, xu2020interdomain_mixup_aaai, li2018learning, shi2021gradient, nam2019sagnet, sun2016coral, foret2020sharpness, zhuang2022surrogate, wang2023sharpness, vapnik1998statistical}, before applying GGA to them as well. We note that we only include previous works that have implemented a ResNet-50 as the backbone encoder and do not use additional components or ensembles. In Table \ref{table:total-results} we differentiate between the methods that operate 
on model gradients \cite{huang2020rsc, shi2021gradient, foret2020sharpness, zhuang2022surrogate, wang2023sharpness} and the rest of the algorithms. Even without its application to other methods, GGA is able to surpass most of the previously proposed algorithms, while also setting the state-of-the-art on PACS ($+0.7\%$) and on the challenging TerraIncognita ($+1.8$) dataset, while remaining competitive in the other two. What's more important is that the application of GGA in conjunction with the rest of the DG methods, proves beneficial and ultimately boosts their overall performance in almost each case by an average of around $1\%$ and in some cases up to even $6.3\%$. With regards to IRM, which seems to be significantly impacted negatively by the application of GGA, its learning objective emphasizes on simultaneously minimizing the training loss of each source domain and not necessarily on the pairwise agreement of gradients among domains. It is therefore not able to converge to a good solution after the initial model's weights have been perturbed during training.

From the experimental results, it is evident that the application of GGA 
and its search for parameter values where gradients align between domains is beneficial to model training. By introducing the proposed annealing step before the final stages of  training, the majority of the models exceed their previous performance and exhibit improved generalization capabilities.

\begin{figure*}[t]
	\centering
	\includegraphics[width=\textwidth]{vlcs-grads.png}
	\caption{Impact of GGA on gradient alignment during model training on the VLCS dataset. As illustrated, in the case of vanilla ERM, even though the training loss is minimized, the average cosine similarity among domain gradients remains low. In the case of GGA however, after the algorithm searches for points in the parameter space with increased gradient similarity, the gradients continue to agree during training, while the total training loss is also minimized.}
	\label{vlcs-grads}
\end{figure*}

\subsection{Evaluating the impact of GGA on gradient disagreement}
\label{gga-evaluation}

As discussed in Section \ref{sec:methods}, when a training dataset is composed
as a mixture of multiple domains, conflicting gradients between mini-batches drawn
from each domain lead to models that do not infer based on domain-invariant
features and which generalize to previously unseen data samples, but are hindered
by domain-specific, spurious correlations. This is evident in the case of
vanilla models trained via ERM where the average gradient similarity among
domains continues to remain low upon reaching a local minima. Our hypothesis is
that this behavior can be avoided by searching for a parameter set of common
agreement between domains before optimizing via gradient descent.

To demonstrate the operation of the proposed algorithm in practice against ERM, we calculate the average gradient cosine similarity between mini-batches from source domains during training for the VLCS dataset, along 
with the training loss in each iteration. As a result, each sub-figure in Figure
\ref{vlcs-grads} illustrates the progression of the training gradient alignment
between domains, against the total training loss. 

As expected, in the very initial iterations the gradients of the pretrained
model parameters point towards a common direction. However, in the case of ERM
as training progresses and the loss is minimized, the domain gradients begin to
disagree leading the model to converge to undesirable minima that do not
generalize across domains. On the other hand, when GGA is applied the model
searches for parameters such that gradients are aligned before continuing
training. This is illustrated by the spike in gradient similarity,
during iterations $100$ up to $200$. After GGA concludes, we observe that the
model continues training by descending into minima where gradients agree among
domains.

\subsection{Sensitivity Analysis}
\label{sensitivity}

\begin{figure}[t]
	\centering
%	% First figure
%	\begin{subfigure}{0.3\textwidth}
%		\centering
%		\includegraphics[width=\textwidth]{gradient-conflicts-oh.png}
%		\caption{Gradient conflicts - ERM}
%	\end{subfigure}%
%	\hfill
	% Second figure
	\begin{subfigure}{0.5\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{rho-ablation.png}
%		\caption{Sensitivity analysis of $\rho$}
	\end{subfigure}%
	\hfill
	% Third figure
	\begin{subfigure}{0.5\columnwidth}
		\centering
		\includegraphics[width=\textwidth]{init-ablation.png}
%		\caption{GGA initialization analysis}
	\end{subfigure}
\caption{Sensitivity analysis of the parameter space search magnitude $\rho$ and the training stage application of GGA. The analysis on PACS reveals smaller weight perturbations and gradient annealing during early training iterations lead to increased model performance.}
\label{sensitivity-fig}
\end{figure}

The two core parameters of GGA, are the size of the parameter search $\rho$ and
the moment of our methods implementation during training, i.e., during the
early, mid or late training stages. To justify their selection, we conduct a
sensitivity analysis (Fig. \ref{sensitivity-fig}) by varying one of the above
parameters while fixing the other at its optimal value.

Regarding the magnitude of weight perturbations during the application of GGA,
we found that the optimal value was $\rho = 5e-5$. As illustrated in Figure
\ref{sensitivity-fig}, a larger magnitude of perturbation led to decreased model
performance. Intuitively, the application of larger noise to the model
parameters leads to sets that are not close to the solution, making it
increasingly difficult for the model to converge. On the other hand, smaller
perturbations seem to have little to no effect on training, as the search is
limited to spaces near the current parameters, which is why the model
performance falls back close to that of ERM. With regards to the stage of
training during which GGA will be applied, we found that the models yielded
better performance when the search was initialized in earlier stages.We hypothesize that applying perturbations near the end of training displaces the model from a local optimum, requiring additional iterations to converge.
