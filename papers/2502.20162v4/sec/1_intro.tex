\section{Introduction}
\label{sec:intro}

The vast majority of neural networks today are trained via stochastic gradient
descent methods, such as SGD \cite{bottou1998online} or ADAM
\cite{kingma2014adam}, where the gradient direction guides the optimization
through the loss landscape, aiming to converge to a minimum. What is more, it
has been empirically shown that the astounding performance and generalization
capabilities of these over-parameterized models stems from the fact that loss
surfaces of large neural networks have multitudinous local minima
\cite{auer1995exponentially, safran2016quality}, most of which yield similar
performance upon model convergence \cite{choromanska2015loss,
  nguyen2018optimization}.

The above optimization process assumes that all available data samples are independent and identically
distributed (i.i.d.). When this assumption holds, the parameter values reached
during training is likely to generalize to the test distribution. However, in
several real-world scenarios the i.i.d. assumption is not met, and models are
evaluated on similar but distinct out-of-distribution (OOD) data resulting 
from domain shifts to the training distribution, leading to a domain
generalization (DG) \cite{zhou_domain_2023} problem. In this case, the 
training loss minimum may lead to a poor parameter configuration for the test data.

The disagreement of gradients across data from different domains during 
training, can provide an indication that the described problem occurs. This observation 
about gradient conflicts was initially made in the multi-task learning 
paradigm \cite{9392366}, where gradients of different tasks pointed to 
conflicting directions on the loss surface. To mitigate the effects of 
gradient conflicts, methods to balance the relative gradient magnitudes were 
proposed \cite{chen2018gradnorm, kendall2018multi}, along with algorithms that remove disagreeing gradient components among tasks \cite{sener2018multi,
  yu2020gradient}. Similarly, gradient conflicts were also addressed in the
context of DG in \cite{mansilla2021domain}, where gradients with different signs among domains were either muted or set to random values; as they are 
assumed to contain domain-specific information. With that being said, it has 
been empirically shown that even though the above issues arise, the correct 
selection and tuning of hyperparameters can be enough for vanilla network 
training to surpass even state-of-the-art algorithms \cite{gulrajani2020domainbed}. This evidence leads us to believe that there 
exist local minima on the vanilla loss surface that lead to more robust and 
generalizable models. Depending on the starting parameter configuration, even a 
model optimized by traditional Empirical Risk Minimization can reach a 
solution that is locally optimal for all distinct domains. These hypotheses, 
along with problems poised by gradient disagreement, are demonstrated in Section \ref{toy-example} (illustrated in Fig. \ref{fig:concept}) and are further explored in Section \ref{sec:generative}.

Starting from these ideas, we propose an alternative strategy for updating the
parameters of a neural network during the optimization process, in an attempt to
``\textit{set the model up for success}''. Specifically, inspired by
Simulated Annealing Optimization \cite{kirkpatrick1983optimization}, during the
early stages of model training we iteratively anneal, i.e. randomly perturb the
parameters of the model, and search for a set of parameter values where the
gradients between all training domains agree, before minimizing the total loss
across all training domains. We call this simple, yet effective, strategy
\textit{Gradient-Guided Annealing} or \textit{GGA}. When evaluated on extensive
and challenging DG benchmarks, GGA is able to boost the performance of the
baseline by significant margins, even yielding state-of-the-art results
without the application of additional data processing or augmentation
techniques. Additionally, since GGA can be considered a general strategy for
handling multi-domain datasets it can also be combined with previously
proposed algorithms. Experimental results of the combined methods,
demonstrate the efficacy of GGA, as it is able to enhance their
generalization capacity and overall accuracy, boosting their performance over the baseline benchmarks.

Our primary contributions are as follows:
\begin{itemize}
 \item We present Gradient-Guided Annealing (GGA), a DG method for training neural networks such that gradients align across domains.
 \item We validate the effectiveness of GGA on multiple, challenging Domain Generalization benchmarks both as a standalone algorithm and by combining it with previous state-of-the-art methods.
 \item We offer further evidence on the effectiveness of the proposed method by investigating the domain gradients during training and its sensitivity to the choice of hyperparameters.
\end{itemize}

%-------------------------------------------------------------------------

\subsection{Gradient Disagreement and Domain Overfitting}
\label{toy-example}


To demonstrate how the domain generalization problem manifests during
optimization in the source domains during training, we consider the following
simple synthetic binary classification problem. For training, data is sampled
from a mixture of two domains, while testing takes place on a different,
previously unseen, target domain. Each sample has two features, a class-specific
feature $x_1$ and a domain-specific feature $x_2$. Our goal is to learn a model
on the source domains, that can discriminate between classes effectively on the
unseen target domain.

Concretely, for the source domains, we draw 200 points from a 2-D Gaussian
distribution with an isotropic covariance matrix for each domain and class, i.e.,
\begin{equation}
	\bm{x}_{y}^{(d)} \sim \mathcal{N}(\bm{\mu}_{y}^{(d)}, \sigma \bm{I}_2)
\end{equation}
Where the subscript $y$ indicates the class and the exponent $d$ the domain,
while $\bm{\mu}_1^{(1)} = [-2.5, -2.5]$, $\bm{\mu}_1^{(2)} = [-2.5, 2.5]$, $\bm{\mu}_2^{(1)} =
[2.5, -2.5]$, $\bm{\mu}_2^{(2)} = [2.5, 2.5]$, $\sigma = 0.5$ and $\bm{I}_2$ is
the $2\times 2$ identity matrix. We also draw an additional 400 samples from a
held-out test domain with means at $\bm{\mu}_1^{(3)} = [-2.5, -7.5]$ and $\bm{\mu}_2^{(3)}
= [2.5, -7.5]$. The drawn samples are shown in the left column of Fig. \ref{fig:concept}. In this
example, the classes can be distinguished solely on $x_1$ (the
``class-specific'' feature), while domains differ in terms of feature $x_2$ (the
``domain-specific'' feature).

We train a 4\textsuperscript{th} degree polynomial logistic regression model
trained with Empirical Risk Minimization (ERM, \cite{vapnik1998statistical}) and
binary cross-entropy loss using the SGD optimizer. In the top-left example of Fig. \ref{fig:concept} the initial parameter conditions were such that 
training converged to a local minimum of the loss that leads the model to consider both $x_1$ and $x_2$ for its decisions. In this case, the model has clearly overfit
its source domains and will fail when presented with out-of-distribution data
from the held-out domain. An indicator of this was the fact that gradients of
the loss for samples of different domains were dissimilar during training. This
is in contrast to the bottom-left GGA model in Fig. \ref{fig:concept} which 
mostly relies on $x_1$ to discriminate between classes. This model was trained by using the
proposed gradient agreement strategy and although the resulting training loss is
slightly higher, the model successfully generalizes to new domains.

In the rest of the paper, we first discuss the most relevant works in the domain generalization literature (Section \ref{sec:related}) and then present 
the proposed methodology (Section \ref{sec:methods}). Followingly, we present the experimental setup and results (Section \ref{sec:experiments}), and finally conclude the paper with a
discussion on limitations of our method and directions for future research
(Section \ref{sec:conclusions}).





