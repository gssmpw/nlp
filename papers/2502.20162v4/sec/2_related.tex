\section{Related Work}
\label{sec:related}

\textbf{Domain Generalization (DG)} methods focus on learning a model from one
or multiple \textit{source} data sets, or domains, which can generalize to
previously unseen, out-of-distribution \textit{target} domains. Existing DG
methods in the literature can be categorized into two major groups; single-source and multi-source. In addition to not having any knowledge about the unseen data,
single-source algorithms do not leverage information regarding the presence of
disinct domains in the training set. On the other hand, multi-source methods utilize
domain labels and often take advantage of the statistical differences in the
sample distributions. Specifically, most popular algorithms include data
augmentation \cite{zhou2021mixstyle, carlucci2019domain} which proves beneficial
for regularizing over-parameterized neural networks and improving
generalization, meta-learning \cite{li2018learning, zhang2020arm,
  balaji2018metareg, dou2019domain}, which exposes models to domain shifts
during training, and disentangled representation learning \cite{peng2019domain, wang2020cross, zhang2022towards, 10472869}, where 
models most commonly include modules or architectures that focus on decomposing learned representations into
domain-specific and domain-invariant parts. Additionally, domain alignment
\cite{muandet2013domain, ganin2016domain, wang2021respecting} and causal
representation learning algorithms \cite{mahajan2021domain, lv2022causality}
have also been proposed in the literature towards producing robust models that
retain their generalization capabilities on unseen data. Finally, ensemble
learning methods \cite{zhou2012ensemble} have also been proposed for DG, where
techniques such as weight averaging \cite{izmailov2018averaging} lead to
improved generalization \cite{cha2021swad}.

\textbf{Gradient operations for DG.} Lately, there has been a surging interest
in addressing the DG problem from a gradient-aware perspective. The most relevant works to ours, leverage gradient information to learn generalized
representations from the source datasets. For example, \cite{huang2020rsc}
proposes a self-challenging learning scheme, by muting the feature
representations associated with the highest gradients and forcing the network to
learn via alternative routes. In another work, the authors of
\cite{shi2021gradient} propose Fish, a first-order algorithm that approximates
the optimization of the gradient inner product between domains. Inspired by gradient surgery in multi-task learning \cite{yu2020gradient},
\cite{mansilla2021domain} proposes aligning the gradient vectors among source
domains by retaining the individual same-sign gradients, and either setting the rest to zero or random values. Finally, there has also been great interest into researching the properties of Sharpness-Aware Minimization (SAM)
\cite{foret2020sharpness, zhuang2022surrogate, wang2023sharpness,
  le2024gradient}, as it has been hypothesized that flatter minima lead to
smaller DG gaps and improved generalization.

\textbf{Simulated Annealing for Deep Learning.} Although explored in past 
literature, simulated annealing (SA) has not been explicitly proposed for DG. 
To avoid being ``trapped'' in local minima during model optimization, 
\cite{cai2021sa} proposes SA-GD, a simulated annealing method for gradient 
descent. Similarly, \cite{RERE2015137} shows that by sacrificing computation time, simulated annealing optimization can also improve the results of
standard CNN architectures. In a more recent work, the authors of \cite{sarfi2023simulated} propose SEAL and apply simulated annealing in the 
early layers of networks to prohibit them from learning overly specific 
representations and improve model generalization. 
Furthermore, there has also been research regarding the combination of SA 
with reinforcement learning (RL) algorithms, where RL is used to optimize 
specific hyperparameters of the SA process \cite{pmlr-v206-correia23a}.

When compared to previously proposed methods which take into consideration
gradient behaviour, GGA has some key differences. In contrast to gradient
surgery algorithms \cite{yu2020gradient, mansilla2021domain} that either mute,
aggregate or set gradients to random values, and Fish \cite{shi2021gradient}
that approximately optimizes the inner-product between domain gradients, GGA
searches for existing parameter space points where gradients of different
domains have pairwise small angles. Furthermore, as GGA is applied in the early
stages of training and for a limited number of training iterations, the high
computational burden of traditional simulated annealing methods is avoided.