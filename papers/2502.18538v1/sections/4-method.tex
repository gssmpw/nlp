\section{Method}
\label{Method}
In this section, we first present our overall network architecture (\S  \ref{dnacnn}), followed by an introduction to the core component of this architecture - the Gated Convolution Block (\S \ref{gatecnn}). Subsequently, we delve into the selection of convolutional network methods, specifically whether to use dilation or down-sampling (\S  \ref{downsample}). Finally, we discuss our pretraining approach and downstream usage (\S \ref{MLM}).

\subsection{ConvNova} \label{dnacnn}
\begin{figure}[!]
    \centering
    \includegraphics[width=\textwidth]{imgs/main_new.png} 
    \caption{\textbf{ConvNova architecture.} The ConvNova model processes double stands as inputs. The sequences are initially subjected to one-hot encoding and subsequently pass through a convolution layer. The processed data then enters a series of Gated Convolution Blocks (GCBs), the specifics of which are elaborated in %section ยง 
    \S  
    \ref{gatecnn}. The output from the GCBs is then fed into an MLP. The final stage of the framework bifurcates into two distinct heads: the downstream head and the pretraining head.}\label{fig:dnacnn}
\end{figure}
Figure ~\ref{fig:dnacnn} shows the overall illustration of ConvNova. DNA sequences are mapped to the hidden space through a convolution operation, followed by $N$ gated convolution blocks (GCBs), and finally processed through a multilayer perceptron (MLP). Different output heads are used in pretraining and downstream tasks. Each GCB utilizes dilated convolution to increase the receptive field and aggregate the features through a dual-branch structure.

\subsection{Gated Convolution Block} \label{gatecnn}
\citet{yu2019free} introduce  gated convolution as a solution to a problem inherent to standard convolution, which indiscriminately treats all input pixels as valid. This method enhances partial convolution by offering a learnable dynamic feature selection mechanism for each channel at every spatial location throughout all layers. This mechanism allows for more nuanced and effective feature selection, improving the performance of convolutional neural networks.

With the motivation to effectively retain and forget information, we propose Gated Convolutional Blocks (GCBs). The dual-branch structure of these blocks is designed to facilitate independent feature extraction, thereby promoting complementary representation learning. Let's define $\mathbf{A} \in \mathbb{R}^{l\times d}$ as the feature extracted by the left branch and $\mathbf{B} \in \mathbb{R}^{l\times d}$ as the right branch feature.

We first process the left branch feature $\mathbf{A}$ through a LayerNorm layer and then pass it through a convolution layer, followed by the GELU activation function to obtain the intermediate features $\mathbf{h}$ of the current layer. In the other path, the right branch features $\mathbf{B}$ are processed through other LayerNorm and convolution layers that do not share weight with the left branch, followed by the sigmoid activation function to obtain the intermediate features $\mathbf{g}$.

Next, we update the left branch features $\mathbf{A}$ by multiplying the intermediate features $\mathbf{g}$ with the intermediate features $\mathbf{h}$. Simultaneously, the right branch features $\mathbf{g}$ are used to update the right branch features $\mathbf{B}$ themselves. Additionally, the kernel size for all convolutional layers is consistently set to 9. This entire process, which integrates both branches, is described by Eq.\ref{gateconvblock} and is illustrated in the Gated Convolution Block of Figure \ref{fig:dnacnn}:
\begin{equation}\label{gateconvblock}
\begin{aligned}
    \mathbf{h} &= \operatorname{GELU}(\operatorname{convolution}(\mathbf{A})) \\
    \mathbf{g} &= \operatorname{sigmoid}(\operatorname{convolution}(\operatorname(\mathbf{B})) \\
    \mathbf{A} &= \mathbf{A} + \mathbf{h} \odot \mathbf{g} \\
    \mathbf{B} &= \mathbf{B} + \mathbf{g}
\end{aligned}
\end{equation}

A stage is composed of several GCBs, with the dilation rate for each Block set to 
%$[\mathbf{1}$, $\mathbf{1}$, $\mathbf{d}$, $\mathbf{d^2}$, $\mathbf{d^3}, ...]$ 
$[{1}$, ${1}$, ${d}$, ${d^2}$, ${d^3}, ...]$. 
%
In different experiment, we use different stages. By default, 1 stage uses 5 GCBs. You can find the information in Table \ref{table:hp}.

% \begin{algorithm}
% \caption{FLIP}\label{alg:gen_comp}
% \begin{algorithmic}[h!]
% \Require  Input DNA sequence $seq \in R^{L}$
% \State $cseq$ = []

% \For {(each nucleotide $nc$ in $seq$)}
%     \If{$nc$ is "N"}
%         \State $cseq$.append($nc$)
%     \Else
%         \State $cseq$.append(3-$nc$) \Comment{0:A, 1:C, 2:G, 3:T, 4:N}
%     \EndIf
% \EndFor
% \State $cseq$ = $cseq$.reshape($seq$.shape)
% \State \Return $cseq$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
%     \caption{Conv\_dilation}\label{alg:conv_dilation}
%     \begin{algorithmic}[h!]
%     \Require Input $x \in R^{L \times D}$, layer num $i$ knernel size $k$, dilation $d$
%     \State $x$ = LayerNorm($x$)
%     \State \Return convolution($x$, kernel\_size=$k$, dilation=$d^{i-1}$, padding=$\frac{d^{i-1}\cdot (k-1)}{2}$)
%     \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}
% \caption{DCGCnn}\label{alg:dcgcnn}
% \begin{algorithmic}[h!]
% \Require  Input DNA sequence $seq \in R^{L}$, knernel size $k$, dilation $d$
% \State $seq$ := Onehot($seq$)
% \State $cseq$ := Onehot(FLIP($seq$))
% \State $feat$ := convolution($seq$) 
% \State $cfeat$ := convolution($cseq$) \Comment{$feat, cfeat \in R^{L \times D}$}
% \For {($i = 1,\dots, num_layer$)}
%     \State $h$ = GELU(Conv\_dilation($feat$, $i$, $k$, $d$))
%     \State $g$ = $\sigma$(Conv\_dilation($cfeat$, $i$, $k$, $d$)) \Comment{$h, g \in R^{L \times D}$}
%     \State $feat$ = $feat$ + $g \cdot h$
%     \State $cfeat$ = $cfeat$ + $g$
% \EndFor
% \State \Return MLP($feat$)
% \end{algorithmic}
% \end{algorithm}

\subsection{MLM Pretraining and Downstream Usage}
\paragraph{MLM Pretraining}\label{MLM}
We use the bidirectional masked language model (MLM) pretraining method and have observed significant performance improvement in downstream tasks. 

In this work, 10\% of the nucleotides in a primary DNA strand are masked, and the model is trained to predict the type of the masked tokens. Therefore no extra labels are required. The objective of this pretraining is to infer the type of nucleotides that have been masked by utilizing the surrounding nucleotides. The optimization objective is to minimize the cross-entropy loss. The pretraining data is HG38, same as HyenaDNA~\citep{nguyen2024hyenadna}.

\paragraph{Downstream Usage}
Upon completing the pretraining phase for 400 epochs, we proceed to finetune ConvNova on the downstream tasks, keeping the entire model adjustable. We employ the pooling method of HyenaDNA to get an embedding for each DNA sequence and utilize a linear mapping to the output label dimension to serve as the finetuned head. In cases where it's not significantly indicated, the setting for our downstream tasks remains consistent with that of HyenaDNA~\citep{nguyen2024hyenadna}.

