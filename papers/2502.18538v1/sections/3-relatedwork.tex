\section{Preliminaries and Related Work}
\label{related_work}
% \subsection{DNA in life science}
% Deoxyribonucleic acid, or DNA, is a polymer composed of two intertwined strands that form a ladder-like, double-helix structure. It consists of four nucleotide bases: adenine (A), cytosine (C), guanine (G), and thymine (T). The bonds between these nucleotide bases create the 'rungs' of the ladder, with A pairing with T and C pairing with G. DNA carries the genetic instructions for the creation of proteins. In complex organisms, DNA can span billions of nucleotide base pairs (bps), but these extensive strands coil tightly around proteins in the nucleus known as histones.

% In DNA, the two strands are complementary, meaning that the sequence of bases in one strand determines the sequence in the other. This is due to the specific base pairing rules: adenine (A) always pairs with thymine (T), and guanine (G) always pairs with cytosine (C). This is known as Watson-Crick base pairing. One strand is known as the "main strand", and the other is the "complementary strand". During DNA replication, the leading strand is synthesized continuously, while the complementary strand is synthesized in fragments, called Okazaki fragments, which are later joined together.
% This complementary base pairing ensure the accurate replication of DNA, as each strand can serve as a template for the formation of a new complementary strand. It also allows for error checking and repair mechanisms, contributing to the stability and integrity of the genetic information. Therefore, models that incorporate both strands can more intuitively grasp the intrinsic nature of DNA sequences.

\subsection{Transformer-based DNA models}
Transformers have become a dominant force across various fields in the deep learning community, and recent research has begun to explore their potential within the realm of genomics. DNABERT~\citep{ji2021dnabert} was the first to establish the transformer as a foundational model for DNA, utilizing k-mer tokenization. Nucleotide Transformer (NT)~\citep{dalla2023nucleotide} scaled up the transformer model (with sizes of 500M and 2.5B) and investigated the impact of different pretraining datasets (HG38\footnote
{\url{https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.26}} and Multispecies%
\footnote
{based on genomes %available on
at 
\url{https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.26}; %for 
more details can refer to NT's original paper.}) on 18 downstream tasks. Their experiments demonstrate that multispecies data and larger models generally yield superior performance. DNABERT-2~\citep{zhou2023dnabert}, also pretrained on multispecies data, tackled several problems encountered with DNABERT. 
% also pretrained on multispecies data, and addressed several challenges encountered with DNABERT. 
They further explored the Byte Pair Encoding (BPE) tokenization and ALiBi positional embedding as effective strategies for modeling DNA with transformer models. Additionally, they utilized FlashAttention to accelerate the processing. Meanwhile, BigBird~\citep{zaheer2020big}, despite not being specifically designed, had also been applied in genomics.

While adept at capturing long-range dependencies, transformer-based models in genomics face challenges compared to CNNs. CNNs possess an inductive bias towards local, translation-invariant features \citep{battaglia2018relational}, aligning well with many genomic patterns.
In contrast, transformers lack this inherent bias for local structure modeling. Additionally, the attention layer's O${(n^2)}$ complexity poses computational challenges, especially for long DNA sequences.

\subsection{SSM-inspired DNA models}
Another approach that has gained traction involves the use of State Space Models (SSMs). HyenaDNA ~\citep{nguyen2024hyenadna} is a decoder-only, sequence-to-sequence model that incorporates a modified convolution operator (Hyena~\citep{poli2023hyena}). This innovative approach enables the expansion of input sequences of the model to 1M-size nucleotides and significantly outperforms transformer-based models in a variety of tasks with a model size that is 300 times smaller. Additionally, Mamba~\citep{gu2023mamba}, a state-space-model inspired architecture, has demonstrated promising, albeit preliminary, experiments on DNA sequences. Caduceus~\citep{schiff2024caduceus} refines the Mamba Block into an RC-equivariant architecture. By taking into account the reverse complementary strand, Caduceus surpasses both HyenaDNA and transformer-based models in downstream tasks, marking a significant advancement in the field. 

\subsection{CNN-based DNA models}

Convolutional Neural Network (CNN)--based DNA models are powerful deep learning tools designed to analyze and interpret DNA sequences by automatically extracting significant motifs and patterns. Early models like DeepBind~\citep{alipanahi2015predicting} predicted the sequence specificities of DNA and RNA-binding proteins, while DeepSEA~\citep{zhou2015predicting} used CNNs to predict the effects of noncoding variants. Basset~\citep{kelley2016basset} advanced the field by learning the regulatory code of the accessible genome using deep CNNs.

Recent advancements have further enhanced the capabilities of CNN-based genomic models. Enformer~\citep{avsec2021effective} integrates convolutional layers with attention mechanisms to predict gene expression from DNA sequences, effectively capturing short- and long-range regulatory interactions. Borzoi~\citep{linder2023borzoi} builds upon the Basenji~\citep{kelley2018sequential} family by incorporating dilated convolutions and skip connections, improving performance on various genomic prediction tasks. LegNet~\citep{penzar2023legnet} employs a convolutional architecture for predicting gene expression and single-nucleotide variant effects in regulatory regions, achieving first place in the DREAM 2022 challenge.

These models demonstrate the CNNs' strengths, capturing local sequence features and motifs crucial for understanding genetic information. However, despite their success, they have not been incorporated into the broader consideration of DNA foundation models.