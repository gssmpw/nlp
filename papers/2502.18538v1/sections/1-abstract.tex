\begin{abstract}
In recent years, a variety of methods based on Transformer and state space model (SSM) architectures have been proposed, advancing foundational DNA language models. 
% However, there is a lack of comparison between these recent approaches and the widely used convolutional networks (CNNs) on foundation model benchmarks.
However, there is a lack of comparison between these recent approaches and the classical architecture---convolutional networks (CNNs)---on foundation model benchmarks.
This raises the question: \textit{ Are CNNs truly being surpassed by these recent approaches based on transformer and SSM architectures?} In this paper, we develop a simple but well-designed CNN-based method, termed \textbf{ConvNova}. ConvNova identifies and proposes three effective designs: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branch framework for gating mechanisms. 
Through extensive empirical experiments, we demonstrate that ConvNova significantly outperforms recent methods on more than half of the tasks across several foundation model benchmarks. For example, in histone-related tasks, ConvNova exceeds the second-best method by an average of 5.8\%, while generally utilizing fewer parameters and allowing faster computation.  
% Additionally, our exploration of the receptive field in histone-related tasks yields further insights. 
% Additionally, the experiments observed that the appropriate receptive field size for CNNs may be related to the biological characteristics of the task.
In addition, the experiments observed findings that may be related to biological characteristics.
This indicates that CNNs are still a strong competitor compared to Transformers and SSMs. We anticipate that this work will spark renewed interest in CNN-based methods for DNA foundation models. Code is available at: \url{https://github.com/aim-uofa/ConvNova}
\end{abstract}