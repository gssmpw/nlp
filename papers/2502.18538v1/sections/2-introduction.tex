
% 为啥用cnn就看怎么说了 1. dna有一堆pattern 2. dna大部分其实是冗余信息，没用 3. 两者都有
% 结果查下来是2


% \section{Introduction}
% \begin{figure}[!b]
%     \centering
%     \includegraphics[width=\linewidth, height=0.2\textheight]{imgs/Fig1.png} 
%     \caption{Overview of transcription and translation process. Coding region only accounts for 2\% approximately in human genome, exons for 1.2\%. In non coding regions, except for functional regions like enhancer, promoter, terminator, other regions remain mainly(91.8\%) redundant or understudied.}\label{fig:dna}
% \end{figure}
% \begin{wrapfigure}[!b]{r}{0.58\textwidth}
% \begin{figure}[!b]
% \resizebox{0.58\textwidth}{!}{
    
%     \includegraphics[width=0.5\linewidth, height=0.2\textheight]{imgs/modelsizev2.png}  %这个是新的
% }
%     \caption{Overview of transcription and translation process. Coding region only accounts for 2\% approximately in human genome, exons for 1.2\%. In non coding regions, except for functional regions like enhancer, promoter, terminator, other regions remain mainly(91.8\%) redundant or understudied.}\label{fig:dna}

% \end{figure}
% \end{wrapfigure}
% \begin{wrapfigure}{r}{0.5\textwidth}
% {
%   \centering
%   \includegraphics[width=0.5\textwidth]{imgs/modelsizev2.png}
%   \caption{Model Size and mean AUC Score. ConvNova outperforms HyenaDNA, DNABERT, DNABERT-2, and Nucleotide Transformer v2 on DeepSEA dataset}\label{fig:dna}
% }
% \end{wrapfigure}

% \begin{figure}[htbp]
%  \centering
%     \label{fig:topresults}
%      \centering
%   \subfigure{\includegraphics[width=0.45\textwidth]{imgs/model_size_vs_accuracy.png}}
%   \subfigure{\includegraphics[width=0.45\textwidth]{imgs/runtime_vs_seqlen.png}}
%     \caption{
%     A) The trade-off between model size and accuracy (AUC score). ConvNova achieves the current SoTA performance. B) The trade-off between input sequence length and runtime. ConvNova exhibits clear superiority over HyenaDNA and Caduceus. All models are around 7M, batch size is 1, tested on A100 80GB. }

% \end{figure}
\begin{figure}[h]
    \vspace{-0.5em}
    \centering
    \includegraphics[width=\textwidth]{imgs/FirstFig.png}
    \caption{A) The trade-off between model size and accuracy (AUC score) of 
    %different 
    various 
    methods. ConvNova achieves the current SoTA performance. B) The trade-off between input sequence length and runtime of different methods. ConvNova exhibits clear superiority over HyenaDNA and Caduceus. All models are around 7M parameters, tested on A100 80GB with batch size 1.}
    \label{fig:topresults}
\end{figure}
\section{Introduction}
%DNA, being consistently at the heart of scientific research, has brought about significant advancements in various fields, 
%
% CS:
Consistently at the forefront of scientific research, DNA has catalyzed significant advancements across a multitude of fields,
including aging research, synthetic biology, and disease treatment \citep{chucair2024age, moe2013preparing}.
% Conversely, 
In contrast, 
unlike NLP, comprehending DNA poses a formidable challenge, owing to the myriad undiscovered biological principles underlying its structure and function.
% Unlike sequence modeling in natural language processing (NLP)~\citep{achiam2023gpt}, protein sequence~\citep{nijkamp2023progen2, wang2024diffusion}, the understanding of DNA extends far beyond what is currently known, with numerous enigmas awaiting discovery. 
Fortunately, the rapid progress of genome language modeling has showcased its dominance in numerous subsequent applications. These include the prediction of promoters~\citep{zhang2022ipro}, gene expression~\citep{avsec2021effective}, DNA methylation~\citep{jin2022idna}, analysis of chromatin state~\citep{lee2022learning}, prediction of promoter-enhancer interactions~\citep{chen2022capturing}, TF-DNA binding prediction~\citep{wang2022towards}, variant effect prediction~\citep{rozowsky2023tex}, and gene network prediction~\citep{theodoris2023transfer}.

Versatile sequence models have been established as foundational models for genetics, typically falling into two categories: Transformer-based methods (\textit{e.g.}, DNABERT~\citep{ji2021dnabert}, DNABERT-2~\citep{zhou2023dnabert}, NucleotideTransformer~\citep{dalla2023nucleotide}) and SSM-inspired methods (\textit{e.g.}, HyenaDNA~\citep{nguyen2024hyenadna}, Caduceus~\citep{schiff2024caduceus}). These approaches have made significant progress in genomic language modeling tasks and are now widely adopted. Prior to these developments, however, convolutional neural networks (CNNs) were the dominant technique for DNA modeling in bioinformatics, giving rise to notable models such as Basset~\citep{kelley2016basset}, Bassenji~\citep{kelley2018sequential}, and LegNet~\citep{penzar2023legnet}. Despite their %past 
success, CNN architectures have not been systematically compared to the newer DNA foundation models in recent research. Comparisons between CNNs and modern approaches have been limited to specific domains, such as NTv2~\citep{dalla2023nucleotide} and Caduceus~\citep{schiff2024caduceus}. In these cases, only a few domain-specific CNN models, like Spliceator~\citep{scalzitti2021spliceator} and some benchmark baselines, have been evaluated.



In this landscape, we rethink and re-evaluate \textit{whether CNN methods are indeed less effective than the current leading paradigms, including Transformer-based methods and SSM-inspired methods}, which have been the primary focus of recent research in DNA foundation models. The motivation behind this is straightforward, as CNNs still hold advantages over existing methods: 1) The input lengths for downstream tasks on DNA vary greatly (ranging from tens to thousands). Transformers are not robust to sequence length variations ~\citep{press2021train}, and their performance can be affected. 2) Transformers have a computational complexity of $\mathit{O}(n^2)$, higher than CNN methods. Additionally, to avoid overly long sequences, transformers also require tokenizers, which affect translational invariance ~\citep{zhou2023dnabert}. 3) Mamba \citep{gu2023mamba} is better suited for tasks involving long sequences with autoregressive properties \citep{yu2024mambaout}, while DNA sequences typically do not exhibit autoregressive characteristics. 4) CNNs possess a local inductive bias, which can provide better data utilization efficiency and are more friendly towards tasks with small training data volumes.
% The motivation behind this reevaluation is straightforward: 1) DNA often interacts with enzymes to perform its functions, and these interactions are typically localized, akin to a sliding window scanning through DNA segments~\citep{givaty2009protein, kuriyan1993sliding, polach1995mechanism}. This mechanism aligns with the principles of CNNs and their locality inductive bias, suggesting that CNNs might be more suitable for DNA modeling. 2) The computational complexity of CNNs is linear, $\mathit{O}(n$), which is more efficient than the quadratic complexity of Transformers, $\mathit{O}(n^2$), and the $\mathit{O}(n\operatorname{log}(n)$) complexity of Hyena. This efficiency offers a significant advantage for modeling long DNA sequences.
% Based on this motivation, we conduct extensive experiments to verify our proposition. The experiments demonstrate that, with proper implementation, CNNs can \textit{significantly outperform current methods.} To achieve this, we introduce ConvNova, a novel CNN-based architecture specifically designed for DNA sequence modeling.
% Compared to previous CNN methods, the three key designs in ConvNova are: 

Based on this motivation, we propose a simple yet well-designed CNN method, named ConvNova. Extensive empirical experiments demonstrate that ConvNova outperforms recent Transformer %or
and 
SSM-inspired methods on more than half of the evaluated tasks, indicating that CNNs continue to demonstrate superiority. Through an analysis of the design of ConvNova, we identify and propose three key designs that enable CNNs to achieve superior performance:
1) \textit{dilated convolution } \citep{yu2015multi}, 2) \textit{gated convolution} \citep{yu2019free}, and 3) \textit{a dual-branch framework for the gated convolution}. Specifically, we found that for DNA tasks, increasing the receptive field of the CNN through downsampling (U-Net \citep{ronneberger2015u} style) 
can severely degrade CNN performance.
%
This stands in contrast to observations in other fields, such as computer vision.
%
However, dilated convolution can circumvent this issue by enlarging the receptive field without the need for downsampling. Additionally, gated convolution can significantly enhance CNN performance. We hypothesize that this is because DNA may contain a substantial amount of irrelevant segments, and the gating mechanism can suppress this information. Lastly, we discovered that splitting the network pathways into two branches, with one branch exclusively providing gating signals to the other, can improve network performance. For detailed designs, please refer to 
%Section
\S \ref{Method} and %Section
\S \ref{experiments}.
% NucleotideTransformer(NT) and DNABERT-2 have demonstrated that the performance of downstream tasks can be enhanced through pretraining on multispecies data. Moreover, DNABERT-2 has unveiled that Byte Pair Encoding (BPE)~\cite{sennrich2015neural} and Attention with Linear Biases (ALiBi)~\cite{press2021train} can also contribute to improved performance. However, transformer-based models continue to grapple with challenges posed by the ${O(N^2)}$ complexity attention layer and the necessity for a large number of parameters to achieve competitive performance.

% HyenaDNA introduced an modified convolution network (CNN) that is trained using the next token prediction (NTP) objective during the pretraining phase. Caduceus, on the other hand, transformed the original causal mamba block into a bidirectional version, pioneering a reverse complements (RC) equivariant model that more accurately reflects the two-strand nature of DNA. 

% However, one aspect that was not considered in previous work is that DNA is special sequence with locally sparse properties. First, it has been confirmed that in human DNA, only 1.2\% is exon ~\citep{encode2012integrated}, which will be transcribed and expressed as proteins, while the vast majority of the remaining regions are non-coding regions. Although non-coding regions can have important regulatory functions, approximately 8.2\% of the human genome is nonfunctional (No regulatory functions nor transcript of any product) ~\citep{rands20148}. Second, even within a gene, the expression level (information content) of nucleotides at different positions varies ~\citep{yun2012systematic}. This is depicted in Figure ~\ref{fig:dna}. 

% To tackle the challenge brought by the aforementioned traits, we roll back to the CNN model, which is both time-efficient(${O{N}}$) and suitable for sparse data(considering its initial usage in image). Moreover, modeling complements of each DNA pair can significantly improve performance ~\citep{zhou2022towards} and regulatory regions can function bidirectionally (See enhancers in Figure ~\ref{fig:dna}). Hence, we put forward the \textbf{D}NA \textbf{C}omplements \textbf{G}ated \textbf{CNN} model (\textbf{ConvNova}). This model is not only tailored to accommodate the bidirectional and dual-strand characteristics of DNA, but it also surpasses the state-of-the-art by ??? on xxxx tasks. To take advantage of the opposite strand and to alleviate model overfitting, we employ a Gated-Convolution approach, as detailed by ~\citep{yu2019free}. This acts as a semi-self-soft-mask, capitalizing on the one-to-one correspondence property of nucleotides. Further details are elaborated in Section~\ref{Method}.



% We then apply our pretrained ConvNova models to 29 diverse downstream
% genomic tasks to showcase both its long-range and short-range ability  . On the NT benchmarks ~\citep{dalla2023nucleotide}, ConvNova achieves state-of-the-art (SoTA) on 12 of 18 datasets while using a model aligned with the current smallest model(HyenaDNA 1.7M) and soley pretrained on human genome. On the GenomicBenchmarks ~\citep{grevsova2023genomic}, ConvNova surpasses SoTA on xx datasets on average by +10 accuracy points, and by as much as +20 accuracy points on enhancer function identification. We also experiment the scaling-up law on the DeepSEA datasets ~\citep{zhou2015predicting}, which consists of 919 binary classification tasks. We find that with sufficient data, ConvNova can have the potential to discover the mysterious understudied DNA regions. Finally, we analyze its long-range ability on variant effect prediction (VEP) in Caduceus by surpassing all the current models.

Comprehensive experiments demonstrate the superiority of ConvNova.  On the NT benchmarks, ConvNova achieves SoTA performance on 12 out of 18 datasets. Notably, in the H3K4me3 task, it outperforms the second-best method by 10.5\%, and in the histone-related tasks, it surpasses the second-best method by an average of 5.8\%. In the gene finding benchmark (long sequences modeling), ConvNova significantly surpasses HyenaDNA~\citep{nguyen2024hyenadna} (55\% vs. 35\%). Compared to the transformer model with much larger parameters (7.4M vs. 336M), ConvNova still achieves better performance (55\% vs. 52\%).

% We also observe that adjusting the receptive field size of ConvNova, specifically through controlling the dilation stride, reveals task-specific performance variations. For certain tasks like H3K4me2, H3K4me3, and H3K14ac, smaller receptive fields surprisingly improve performance, suggesting these tasks may focus on localized DNA regions. For detailed findings and analysis, please refer to Section \ref{discussion}.

% By adjusting the receptive field size of ConvNova, specifically through controlling the dilation stride, we reveal that the performance of the vast majority of tasks improves with the increase of the receptive field of the CNN. However, we find that some specific tasks, including H3K4me2, H3K4me3, and H3K14ac, exhibit pronounced localized characteristics; therefore, we speculate that this phenomenon is caused by the biological characteristics. For detailed findings and analysis, please refer to Section \ref{discussion}.

By adjusting the receptive field size of ConvNova, specifically through controlling the dilation stride, we reveal that the performance of the vast majority of tasks improves with the increase of the receptive field of the CNN. However, we find that some specific tasks, including H3K4me2, H3K4me3, and H3K14ac, exhibit the opposite phenomenon. Existing wet lab experimental data indicate that some of these tasks exhibit pronounced localized characteristics; therefore, this phenomenon may be caused by biological characteristics. For detailed findings and analysis, please refer to %Section 
\S \ref{discussion}.


% In brief, 
In summary, 
the main contributions of our work 
%can be summarized 
are 
as follows:
\begin{itemize}

\item We reexamine the convolutional paradigm for DNA foundation models. Analyses and extensive experimentation demonstrate the continued competitiveness of convolutional neural networks (CNNs) for downstream tasks. This may prompt the community to reconsider CNNs for such tasks.

\item We propose ConvNova and identify three key designs for genomic language modeling: dilated convolution, gated convolution, and a dual-branch design. With these designs, ConvNova achieves superior performance on numerous benchmarks while maintaining fewer parameters.

\item ConvNova outperforms recent methods on over half of the tasks across various DNA foundation model benchmarks, typically using fewer parameters and achieving faster performance.

% \item We investigate the varying demands for receptive field sizes across different tasks. However, the phenomenon of H3K4me2 requiring a small receptive field lacks known biological explanations, while others have related biological phenomena. This may imply the presence of some unknown biological phenomena underlying H3K4me2.
% \item We compare traditional supervised models with genomic foundation models, revealing that while supervised models can achieve strong performance on certain tasks, they are not as versatile and often struggle with specific tasks. In contrast, genomic foundation models, like ConvNova, demonstrate more comprehensive and robust performance across a wider range of genomics applications.
    % \item We introduce ConvNova, a parameter and time efficient pretrained cnn model with ????????time ?????.
    % \item We prove the complementary strand could provide significant extra information with respect to DNA foundation model.
    % \item Our experiments demonstrate that ConvNova have comprehensive ability in tackling DNA challenges ranging from short sequence modeling, long sequence modeling, generalize to multi-species tasks.
    % \item We show the potential to scale up ConvNova with sufficient data.
\end{itemize}
% \begin{figure*}[t]
    % \centering
    % \begin{subfigure}[b]{\linewidth}
    %     \includegraphics[width=\linewidth]{imgs/Fig1a.png} 
    %     \caption{Your caption for Fig1a} 
    %     \label{fig:1a}
    % \end{subfigure}
    
    % \begin{subfigure}[b]{\linewidth}
    %     \includegraphics[width=\linewidth]{imgs/Fig1b.png} 
    %     \caption{Your caption for Fig1b} 
    %     \label{fig:1b}
    % \end{subfigure}
    % \caption{Your general caption for Fig1}
    % \label{fig:1}
% \end{figure*}\label{fig:model}
