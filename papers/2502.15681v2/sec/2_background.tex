\section{Background}

\subsection{Diffusion models}
% \av{We should introduce the method name (\method) in abstract and intro as well.}
% \av{Yang Song also had an identity similar to ours in Theorem 2 of \href{https://arxiv.org/pdf/2101.09258v1}{this paper}. Should we discuss this somewhere?} \yx{okay! I can add some discussions after our theorem 1.}
The goal of \methodtext is to accelerate the generation of pre-trained (continuous-time) DMs~\cite{song2020score, ho2020ddpm}. In this paper, we follow the popular EDM framework~\cite{Karras2022ElucidatingTD} for the notations and forward/backward processes. DMs perturb the clean data {$\rvx_0 \sim p_{\textrm{data}}$} in a fixed forward process using $\sigma^2(t)$-variance Gaussian noise, where $\rvx_0 \in \R^d$ and $t$ denotes the time along the diffusion process. The resulting intermediate distribution is denoted as $p_t(\rvx_t)$ with $\rvx_t \in \R^d$. For notation simplicity, we will use $\rvx$ to replace $\rvx_t$, unless stated otherwise, throughout the paper.
For sufficiently large $\sigma_\textrm{max}$, this distribution is almost identical to pure random Gaussian noise. DMs leverage this observation to sample the initial noise $\epsilon_\text{max} \sim \mathcal{N}( \mathbf{0},\sigma_\textrm{max}^2\mI)$, and then iteratively denoise the sample by solving the following backward ODE/SDE, which guarantees that if $\sigma(0)=0$, the final $\rvx$ follows the data distribution $p_{\textrm{data}}$:
{\small
\begin{equation}\label{eq:generation_sde}
\begin{split}
    & d\rvx = \underbrace{-\dot{\sigma}(t)\sigma(t)\boldsymbol{\nabla}_\rvx \log p_t(\rvx) dt}_{\textrm{Probability Flow ODE}} \\
    &\underbrace{- \beta(t)\sigma^2(t) \boldsymbol{\nabla}_\rvx \log p_t(\rvx) dt + \sqrt{2\beta(t)}\sigma(t)d\boldsymbol{\omega}_t}_{\textrm{Langevin Diffusion SDE}},
\end{split}
\end{equation}
}%
where $\boldsymbol{\omega}_t$ is a standard Wiener process and $\boldsymbol{\nabla}_\rvx \log p_t(\rvx)$ is the \textit{score function} of the intermediate distribution $p_t(\rvx)$. The score function is learned by a neural network $s_\phi(\rvx; \sigma(t))$ trained with the denoising score matching objective~\cite{vincent2011,song2019generative}. In \Cref{eq:generation_sde}, the first term is the Probability Flow ODE,  which guides samples from high to low noise levels. The second term is a Langevin Diffusion SDE, which acts as an equilibrium sampler across different noise levels $\sigma(t)$~\cite{Karras2022ElucidatingTD, Xu2023RestartSF}. This component can be scaled by the time-dependent parameter $\beta(t)$ with $\beta(t)=0$ leading to ODE-based synthesis. %However, solving the diffusion ODE and SDE typically involves a considerable number of iterations (often tens or hundreds), posing a significant challenge to the practical deployment of diffusion models. 
Although different kinds of accelerated samplers for diffusion ODE~\cite{Song2020DenoisingDI, lu2022dpm, Karras2022ElucidatingTD} and SDE~\cite{JolicoeurMartineau2021GottaGF, Karras2022ElucidatingTD, Xu2023RestartSF} have been proposed, they usually still require $>20$ sampling steps in practice to produce decent samples. 


\subsection{Variational score distillation}

A recent line of works~\cite{yin2024one, yin2024improved} aim to distill the teacher diffusion models $s_\phi$ into a single step generator $G_\theta$, through \textit{variational score distillation (VSD)}, which is originally introduced for test-time optimization of 3D objects~\cite{Wang2023ProlificDreamerHA}. The goal is to enable a student model $G_\theta$ to directly map the noise $\rvz$ from the prior distribution $p(\rvz) = \mathcal{N}(\rvz;  \mathbf{0}, \mI)$ to the clean sample $\rvx_0$ at $\sigma=0$ using $\rvx_0 = G_\theta(\rvz)$, effectively bypassing the iterative sampling process. Let $p_\phi$ denote the distribution obtained by plugging in pre-trained diffusion models $s_\phi(\rvx; \sigma(t))$ in \Cref{eq:generation_sde}, and let $q_\theta$ denote the output distribution by the one-step generator $G_\theta$~(in the following text, we drop the subscript in $p_\phi$ and $q_\theta$ for notation simplicity). Then, the gradient update for the generator can be formulated as follows:
\begin{align*}
     \E_{t, \rvz, \epsilon} \left[ \left(s_\phi(\rvx; \sigma(t)) - \nabla_\rvx \log q_\theta(\rvx; \sigma(t))\right) \nabla_\theta G_\theta(\rvz)\right]
     \numberthis \label{eq:vsd-obj}
\end{align*}
where $\rvx = G_\theta (\rvz) + \sigma(t) \epsilon $ and $\epsilon \sim \mathcal{N}(  \mathbf{0}, \mI)$.
Intuitively, the gradient encourages the generator to produce samples that lie within high-density regions of the data distribution. This is achieved through the teacher score term, $s_\phi(\rvx; \sigma(t))$, which guides the generated samples towards areas where the teacher model assigns high probability. To prevent mode collapse, the gradient also incorporates a term that discourages the generator from simply concentrating on a single high-density point in the teacher's distribution. This is done by subtracting the score of the student distribution, $\nabla_\rvx \log q_\theta(\rvx; \sigma(t))$. The gradient update is shown to perform distribution matching by minimizing the reverse-KL divergence between the teacher and student distributions~\cite{poole2023dreamfusion, yin2024one}. 

To estimate the score of the student distribution, previous works~\cite{Wang2023ProlificDreamerHA, yin2024one} have employed another \textit{fake score} network $s_\psi (\rvx, \sigma(t))$ to approximate  $\nabla_\rvx \log q_\theta(\rvx; \sigma(t))$. The fake score network $s_\psi (\rvx, \sigma(t))$ is dynamically updated with the standard denoising score matching loss, where the ``clean'' samples come from the generator $G_\theta$ during training. 
Thus, the VSD training alternates between the generator update and the fake score update, with a two time-scale update rule for stabilized training~\citep{yin2024improved}.
Additionally, to further close the gap between the one-step generator and the multi-step teacher diffusion model, a GAN loss is applied to the VSD training pipeline~\citep{yin2024improved}, where a lightweight GAN classifier takes as input the middle features from the fake score network.

% \yx{TODO: add a sentence, saying in practice the matching is performed across all $t$. We may also save it in the method section.}

% \yx{TODO: say something about why it's better than GAN here. }

\subsection{$f$-divergence}

In probability theory, an $f$-divergence~\cite{Rnyi1961OnMO} quantifies the difference between two probability density functions, $p$ and $q$.  Specifically, when $p$ is absolutely continuous with respect to $q$, the $f$-divergence is defined as:
\begin{align*}
D_f(p||q) = \int q(\rvx) f\Big(\frac{p(\rvx)}{q(\rvx)}\Big) d\rvx
\end{align*}
where $f$ is a convex function on $(0,+\infty)$ satisfying $f(1)=0$. This divergence satisfies several important properties, including non-negativity and the data processing inequality. Many commonly used divergences can be expressed as special cases of the $f$-divergence by choosing an appropriate function $f$. These include the forward-KL divergence, reverse-KL divergence, Hellinger distance, and Jensen-Shannon (JS) divergence, as shown in Table~\ref{tab:f-div}. In generative learning, $f$-divergence has been widely applied to popular generative models, such as GANs~\citep{nowozin2016f}, VAEs~\citep{wan2020fvi}, energy-based models~\citep{yu2020training} and diffusion models~\citep{tang2024fine}.

% \yx{TODO: highlight the importance of f-div. say something about its application in ML community, such as f-GAN, f-VAE and some Info-Max representation learning by optimizing its dual lower bound.}
% \wn{I added a sentence but I think we can leave the detailed discussion to the related work section.}