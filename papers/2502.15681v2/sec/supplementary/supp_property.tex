\section{Properties of $f$-divergence}

\label{app:property}

\subsection{Mode-seeking behavior in $f$-divergence}

\subsubsection{Classification by mode-seeking}
Mode-seeking, as described in Section 10.1.2 in \cite{Bishop2006PatternRA}, refers to the tendency of fitted generative models to capture only a subset of the dominant modes in the data distribution. This occurs during the minimization of the $f$-divergence $\min_q D_f(p||q)$ between the true data distribution ($p$) and the learned generative distribution ($q$).  An $f$-divergence is considered ``mode-seeking" if its minimization leads to this mode-seeking behavior in the corresponding generative model. The mode-seeking behavior in generative models translates into a lack of diversity in practice. Most of the previous classifications of mode-seeking divergences are mainly based on empirical observations. For example, reverse-KL is widely considered mode-seeking, and forward-KL aims for the opposite~(\ie mode-coverage)~\cite{Poole2016ImprovedGO}. Here, we applied the criteria proposed in \cite{pmlr-v206-ting-li23a}~(see Definition 4.1 in the paper) to roughly classify the $f$-divergence based on mode-seeking. Intuitively, a smaller limit indicates a higher tolerance of the corresponding $f$-divergence for large density ratios ($r=p/q$). This allows the generative distribution $q$ to assign less probability mass to regions where the true distribution $p$ has high density without incurring a significant penalty. Consequently, this behavior can lead to mode-seeking, where the model focuses on capturing only the dominant modes of the data distribution. Hence, we use the rate of the limit to classify divergence in the mode-seeking column in Table~\ref{tab:f-div}.

\subsubsection{Relation to the weighting function $h$}
Another paper~\cite{Shannon2020NonsaturatingGT} classifies the mode-seeking divergence based on the increasing rate of the limits $ \lim_{r\to \infty}f''(r)$ and $ \lim_{r\to 0}f''(r)$, through a concept of ``tail weight". The tail weight associated with $ \lim_{r\to \infty}f''(r)$~(right tail weight) / $ \lim_{r\to 0}f''(r)$~(left tail weight) describes how strongly the mode-seeking / mode-coverage behavior is penalized. A larger rate of limit can be translated into a higher penalty on mode-seeking. Table~\ref{tab:tail-weight-rate} demonstrates tail weights for different divergences. In general, less mode-seeking divergence will have a larger right tail weight~(rate of $\lim_{r\to \infty}f''(r)$) and smaller left tail weight~(rate of $\lim_{r\to 0}f''(r)$). As a result, when using these canonical $f$-divergences, the weighting function $h(r)$ would be an increasing function if the divergence is less mode-seeking since $h(r)= f''(r)r^2$. For example, $h$ in JS and forward-KL is an increasing function, while in reverse-KL, $h$ stays constant. An increasing h tends to downweight regions with lower density in the true data distribution $p$. This corresponds to regions where the teacher score is less reliable. Fig.~\ref{fig:teasdr-2} illustrates the idea.

\begin{table}[h]
    \centering
    \begin{tabular}{l c c c c c c}
    \toprule
     & reverse-KL & softened RKL & JS & squared Hellinger & forward-KL & Jefferys  \\
    \midrule
    Rate of $ \lim_{r\to \infty}f''(r)$ & $\gO(r^{-2})$& $\gO(r^{-3})$ & $\gO(r^{-2})$ & $\gO(r^{-\frac{3}{2}})$ & $\gO(r^{-1})$ & $\gO(r^{-1})$\\
    Rate of $ \lim_{r\to 0}f''(r)$& $\gO(r^{-2})$& $\gO(r^{-2})$ & $\gO(r^{-1})$ & $\gO(r^{-\frac{3}{2}})$ & $\gO(r^{-1})$ & $\gO(r^{-2})$\\
    \bottomrule
    \end{tabular}
    \caption{Right / left weight for different $f$-divergences. We shift the tail weight in \cite{Shannon2020NonsaturatingGT} by a constant for clarity.}
    \label{tab:tail-weight-rate}
\end{table}


 
\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{img/teaser_4.png}
    \caption{Illustration of how the weighting function $h$~(\textcolor{myred}{red dotted line}) in less mode-seeking divergence~(forward-KL, $h = p/q$) helps to learn the true data distribution $ p$, compared to more mode-seeking divergence~(reverse-KL, $h \equiv 1$). We illustrate how using a less mode-seeking divergence can better capture different modes, from a skewed initial generative distribution $q$, with the help of the weighting function.}
    \label{fig:teasdr-2}
\end{figure}

\subsection{$f$-divergence and Fisher divergence}


A line of work focuses on achieving distributional matching by minimizing the Fisher divergence or its variants~\cite{zhou2024score, luo2024one}. While $f$-divergence-based distillation methods match the probability density functions $p_t$~(teacher distribution) and $q_t$~(student distribution), Fisher divergence-based distillation aims to match the distributions by minimizing the distance between their score functions. This equates to matching the gradients of the log probability density functions, $\nabla_\rvx \log p_t(\rvx)$ and $\nabla_\rvx \log q_t(\rvx)$:
\begin{align*}
    &\textbf{($f$-divergence):} \quad \min_{q_t} \int q_t(\rvx) f\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right) d\rvx \\
    &\textbf{(General Fisher divergence):} \quad \min_{q_t} \int q_t(\rvx) \rvd(\nabla_\rvx \log p_t(\rvx), \nabla_\rvx \log q_t(\rvx)) d\rvx \numberthis \label{eq:fisher}
\end{align*}
where $\rvd$ is a scalar-valued proper distance function satisfying $\rvd(\rvx) \ge 0$ and $\rvd(\rvx)=0$ if and only if $\rvx=0$. When $\rvd$ is squared $\ell_2$ distance, \Eqref{eq:fisher} reduces to Fisher divergence. 

In practice, directly minimizing the Fisher divergence in Equation \ref{eq:fisher} is challenging. Existing works often rely on certain assumptions and approximations to make this optimization tractable. For example, \cite{luo2024one} imposes the stop gradient operation on the sampling distribution $q_t$~(first term in the integral in \Eqref{eq:fisher}). In addition, they typically use a Monte-Carlo sampler~\cite{zhou2024score}, derived from Tweedieâ€™s Formula, to estimate an intractable term in the gradient.