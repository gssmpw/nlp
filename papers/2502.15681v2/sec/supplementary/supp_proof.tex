
\section{Proofs}
\label{app:proof}

In this section, we provide proofs for Theorem~\ref{thm-main} and Proposition~\ref{prop:general} in the main text. We will start with Lemma~\ref{lemma:cov} before proving Theorem~\ref{thm-main}.

\setcounter{theorem}{0}
\setcounter{proposition}{1}

\begin{lemma}
\label{lemma:cov}
Assuming that sampling from $\rvx \sim  q_t(\rvx)$ can be parameterized to $\rvx = G_\theta(\rvz)+\sigma(t)\epsilon$ for $\rvz \sim  p(\rvz), \epsilon \sim \mathcal{N}(  \mathbf{0}, \mI)$ and $G_\theta,g $ are differentiable mappings. In addition, $g$ is constant with respect to $\theta$. Then $\int \nabla_\theta  q_t(\rvx) g(\rvx) d\rvx = \int\int p(\epsilon)  p(\rvz) \nabla_\rvx g(\rvx) \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz$.
\end{lemma}
\begin{proof}
    As $q_t$ and $g$ are both continuous functions, we can interchange integration and differentiation:
    \begin{align*}
        \int \nabla_\theta  q_t(\rvx) g(\rvx) d\rvx  &= \nabla_\theta \int   q_t(\rvx) g(\rvx) d\rvx \\
        &= \nabla_\theta \int  \int p(\epsilon)  p(\rvz)  g(G_\theta(\rvz)+\sigma(t)\epsilon)  d\epsilon d\rvz \\
        &=  \int  \int p(\epsilon)  p(\rvz)  \nabla_\theta g(G_\theta(\rvz)+\sigma(t)\epsilon)  d\epsilon d\rvz  \numberthis\label{eq:lemma1-inter}\\
        &=  \int  \int p(\epsilon)  p(\rvz)  \nabla_{\rvx} g(G_\theta(\rvz)+\sigma(t)\epsilon) \nabla_\theta G_\theta(\rvz)  d\epsilon d\rvz \\
        &= \int\int p(\epsilon)  p(\rvz) \nabla_\rvx g(\rvx) \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz
    \end{align*}
where $\rvx = G_\theta(\rvz)+\sigma(t)\epsilon$. We can interchange integration and differentiation again in \Eqref{eq:lemma1-inter} as $g$ is a differentiable function.
\end{proof}
\begin{theorem}

Let $p$ be the teacher's generative distribution, and let $q$ be a distribution induced by transforming a prior distribution $ p(\rvz)$ through the differentiable mapping $G_\theta$. Assuming $f$ is twice continuously differentiable, then the gradient of $f$-divergence between the two intermediate distribution $p_t$ and $q_t$ w.r.t $\theta$ is:
{
\begin{align*} 
    &\nabla_\theta D_f(p_t||q_t) = \E_{\substack{\rvz,  \epsilon}}-\left[f''\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)^2\left(\underbrace{\nabla_\rvx \log p_t(\rvx)}_{\textrm{teacher score}} - \underbrace{\nabla_\rvx \log q_t(\rvx)}_{\textrm{fake score}} \right)  \nabla_\theta G_\theta(\rvz)\right]
    \numberthis \label{eq:time-0-loss-2}
\end{align*}
}%
where $\rvz \sim  p(\rvz), \epsilon \sim \mathcal{N}(  \mathbf{0}, \mI)$ and $ \rvx = G_\theta(\rvz)+\sigma(t)\epsilon $
\end{theorem}


\begin{proof}

Note that both the intermediate student distribution $q_t$ and the sample $\rvx$ have a dependency on the generator parameter $\theta$. In the proof, we simplify the expression $\int (\nabla_\theta  q_t(\rvx)) g(\rvx) d\rvx$ as $\int \nabla_\theta  q_t(\rvx) g(\rvx) d\rvx$ for clarity. The total derivative of $f$-divergence between teacher's and student's intermediate distribution is as follows: 


\begin{align*}
&\nabla_\theta D_f(p_t(\rvx)||q_t(\rvx)) =   \nabla_\theta \int q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx \\
&=   \int \nabla_\theta q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx + \int  q_t(\rvx) \nabla_\theta f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx \\
&=  \int \nabla_\theta q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx - \int  q_t(\rvx)  f'(\frac{p_t(\rvx)}{q_t(\rvx)})\frac{p_t(\rvx)}{q^2_t(\rvx)} \nabla_\theta q_t(\rvx)d\rvx\\
% &=\nabla_\theta \E_{\rvx \sim q_t}\left[f(\frac{p_t(\rvx)}{q_t(\rvx)})  \right]\\
% &=\nabla_\theta \E_{\rvz \sim  p(\rvz), \epsilon \sim \mathcal{N}(  \mathbf{0}, \mI), \rvx = G_\theta(\rvz)+\sigma(t)\epsilon}\left[f(\frac{p_t(\rvx)}{q_t(\rvx)})  \right]\\
% &=\int p(\epsilon)  \nabla_\theta \int p(\rvz) f(\frac{p_t(G_\theta(\rvz)+\sigma(t)\epsilon)}{q_t(G_\theta(\rvz)+\sigma(t)\epsilon)}) d\rvz d\epsilon \\
%     &=  \int  p(\epsilon) \left(\underbrace{\int  p(\rvz) \frac{\partial f(\frac{p_t(\rvx)}{q_t(\rvx)})}{\partial \rvx}\Big|_{\rvx=G_\theta(\rvz)+\sigma(t)\epsilon} \nabla_\theta G_\theta(\rvz) d\rvz}_{\frac{\partial f}{\partial \rvx} \frac{ d \rvx }{ d \theta}} \right.\\
%     & \qquad \qquad \qquad + \left.\underbrace{\int  p(\rvz) \left(f'(\frac{p_t(\rvx)}{q_t(\rvx)})(-\frac{p_t(\rvx)}{q_t^2(\rvx)}) \nabla_\theta q_t(\rvx_{\textrm{sg}})\right)_{\rvx=G_\theta(\rvz)+\sigma(t)\epsilon} d\rvz }_{\frac{\partial f}{\partial q_t} \frac{ d q_t }{ d \theta}}\right)d\epsilon \\
%     &=   \underbrace{\int \nabla_\theta q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx}_{\frac{\partial f}{\partial \rvx} \frac{ d \rvx }{ d \theta}} - \E_{\epsilon \sim \mathcal{N}(  \mathbf{0}, \mI)}\underbrace{\E_{\rvz \sim  p(\rvz), \rvx = G_\theta(\rvz)+\sigma(t)\epsilon}\left[f'(\frac{p_t(\rvx)}{q_t(\rvx)})(-\frac{p_t(\rvx)}{q_t^2(\rvx)}) \nabla_\theta q_t(\rvx))  \right]}_{\frac{\partial f}{\partial q_t} \frac{ d q_t }{ d \theta}}\numberthis \label{eq:lemma-use-1}\\ 
%     &=   \underbrace{\int \nabla_\theta q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx}_{\frac{\partial f}{\partial \rvx} \frac{ d \rvx }{ d \theta}} - \E_{\rvx \sim q_t}\left[f'(\frac{p_t(\rvx)}{q_t(\rvx)})(-\frac{p_t(\rvx)}{q_t^2(\rvx)}) \nabla_\theta q_t(\rvx))  \right]\\
    &=  \underbrace{\int \nabla_\theta q_t(\rvx) f(\frac{p_t(\rvx)}{q_t(\rvx)}) d\rvx }_{I} - \underbrace{\int \nabla_\theta q_t(\rvx) f'(\frac{p_t(\rvx)}{q_t(\rvx)}) \frac{p_t(\rvx)}{q_t(\rvx)}  d\rvx}_{II} \numberthis \label{eq:combined}
\end{align*}

    
Note that by notation $\nabla_\theta  q_t(\rvx)$, we mean that only the first $q$ inisde each integral has gradient w.r.t $\theta$. (I) / (II) is the term associated with the partial derivative of $f$ with respect to $\rvx$ / $q$, respectively. Above we see that both partial derivatives (I) and (II) are in the form $\int \nabla_\theta  q_t(\rvx) g(\rvx) d\rvx$ where $g$ is a differentiable function that is constant with respect to $\theta$. Using the identity in Lemma~\ref{lemma:cov} again, we can simplify (I) and (II) to:
{
\begin{align*}
I &= \int \int p(\epsilon)  p(\rvz) f'(\frac{ p_t(\rvx)}{ q_t(\rvx)}) \nabla_\rvx \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz \\
II &= \int \int p(\epsilon) p(\rvz) f''(\frac{ p_t(\rvx)}{ q_t(\rvx)}) \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\rvx \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\theta G_\theta(\rvz)d\epsilon d\rvz + \int \int p(\epsilon) p(\rvz) f'(\frac{ p_t(\rvx)}{ q_t(\rvx)}) \nabla_\rvx \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz 
\end{align*}
}%
Putting (I) and (II) in \Eqref{eq:combined}, we have:
{
\begin{align*}
    \nabla_\theta D_f(p_t(\rvx)||q_t(\rvx)) = &-\int  \int p(\epsilon) p(\rvz) f''(\frac{ p_t(\rvx)}{ q_t(\rvx)}) \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\rvx \frac{ p_t(\rvx)}{ q_t(\rvx)} \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz \\
= &-\int  \int p(\epsilon) p(\rvz) f''\left(\frac{ p_t(\rvx)}{ q_t(\rvx)}\right)\left(\frac{ p_t(\rvx)}{ q_t(\rvx)}\right)^2\left[\nabla_\rvx \log  p_t(\rvx) - \nabla_\rvx \log  q_t(\rvx)\right] \nabla_\theta G_\theta(\rvz) d\epsilon d\rvz \\
= &\E_{\substack{\rvz,  \epsilon}}-\left[f''\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)^2\left(\underbrace{\nabla_\rvx \log p_t(\rvx)}_{\textrm{teacher score}} - \underbrace{\nabla_\rvx \log q_t(\rvx)}_{\textrm{fake score}} \right)  \nabla_\theta G_\theta(\rvz)\right]
\end{align*}
}%
where the last identity is from the log derivative trick, \ie $\nabla_\rvx \frac{p_t(\rvx)}{q_t(\rvx)} = \frac{p_t(\rvx)}{q_t(\rvx)}\left[\nabla_\rvx \log  p_t(\rvx) - \nabla_\rvx \log  q_t(\rvx)\right] $. 

\end{proof}


\mainprop*

\begin{proof}
    To constitute a valid $f$-divergence, the requirement for $f $ is that  $f$ is a convex function on $(0,+\infty)$ satisfying $f(1)=0$. For any function $h$ that is continuous and non-negative function on $(0, +\infty)$, the function $g(r) = h(r)/r^2$ is also a continuous and non-negative function on $(0, +\infty)$. By the fundamental theorem of calculus, we know that there exists a continuous function $m(r)$ whose second derivative equals to $g(r)$, \ie $m''(r) = g(r)$. Let $f(r) = m(r) - m(1)$, it is straightforward to see that $f(1)=0$ and $f''(r)=h(r)/r^2$. In addition, $f$ is a convex function on $(0, +\infty)$ as its second derivative is non-negative in this domain. Let $ \rvx = G_\theta(\rvz)+\sigma(t)\epsilon $, we can re-express the expectation as follows:
    \begin{align*}
        &\E_{\substack{\rvz,  \epsilon}}-\left[h\left(r_t(\rvx)\right)\right.\left({\nabla_\rvx \log p_t(\rvx)} - {\nabla_\rvx \log q_t(\rvx)} \right)  \nabla_\theta G_\theta(\rvz)] \\
        &= \E_{\substack{\rvz,  \epsilon}}-\left[f''\left(r_t(\rvx)\right)r^2_t(\rvx)\right.\left({\nabla_\rvx \log p_t(\rvx)} - {\nabla_\rvx \log q_t(\rvx)} \right)  \nabla_\theta G_\theta(\rvz)] \\
        &= \nabla_\theta D_f(p_t(\rvx)||q_t(\rvx)) 
    \end{align*}
    where the last equation is by Theorem~\ref{thm-main}.
\end{proof}