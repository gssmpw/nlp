\section{Training details}


\label{app:details}

In this section, we provide training details for \methodtext on CIFAR-10, ImageNet-64, COYO-700M~(w/ SD v1.5 model) and COYO-700M-Aesthetic-6.5~(w/ SDXL model). The COYO-700M-Aesthetic-6.5 is a subset of COYO-700M by filtering out images with aesthetic score lower than 6.5. Table~\ref{tab:training} shows the values of common training hyper-parameters on different datasets. For most hyper-parameters, we directly borrow the value from \cite{yin2024improved}, which is a special case in the \methodtext framework. Inspired by the three-stage training in  \cite{yin2024improved}, we also divide the ImageNet-64 training process into two stages with different learning rates. In the first stage, we train the model with a learning rate of 2e-6 for 200k iterations, then fine-tune it with a learning rate of 5e-7 for 180k iterations. We apply TTUR~\cite{yin2024improved} for all the models. We further provide an algorithm box in Alg~\ref{alg:f-distill} for clarity. For hyper-parameters, we use a batch size of 2048 / 512 / 1024 / 384 for CIFAR-10 / ImageNet-64 / COYO (SD v1.5) / COYO (SDXL).

\cite{yin2024improved} uses the online fake score network as the feature extractor for the GAN discriminator. This complicates the training process, as there is an additional hyper-parameter balancing the denoising score-matching loss and GAN loss for updating the fake score network. To simplify the use of GAN in our framework, we use the fixed teacher network as the feature extractor, similar to LADD~\cite{sauer2024fast}. Unlike \cite{yin2024improved}, including the fake score network as part of the learnable parameter in the GAN discriminator, the learnable parameter in the new setup is a small classification head whose input is the feature from the teacher network. We empirically observe that the modification leads to better performance on CIFAR-10, as shown in Fig.~\ref{fig:feature-a}. We also include an ablation study~(green line in Fig.~\ref{fig:feature-b}), which uses the fake score as a feature extractor but does not update it in the GAN loss. The model behaves poorly in this case, as the fake score is constantly getting updated with denoising score-matching loss, validating the benefits of using the fixed teacher score as feature extractor.

For evaluation, we report the FID / Recall score on 50K samples on CIFAR-10 and ImageNet-64, and 30K samples on zero-shot MS-COCO dataset. We report the CLIP score on 30K samples using MS-COCO datasets. 






\begin{table}[t]
\footnotesize
    \centering
    \begin{tabular}{l c c c c}
    \toprule
     & CIFAR-10 & ImageNet-64 & COYO-700M & COYO-700M-Aesthetic-6.5  \\
    \midrule
    Batch size & 2048 & 512 & 1024 & 384 \\
    Fake score update frequency & 5 & 5 &  10 & 5\\
    GAN loss weight & 1e-3 & 3e-3 & 1e-3& 5e-3\\
    GAN discriminator input resolution & (32, 16, 8) & (8) & (8) & (16)\\
    Total iteration & 60K & 380K & 60K & 25K\\
    Teacher & EDM~\cite{Karras2022ElucidatingTD} & EDM~\cite{Karras2022ElucidatingTD} & Stable Diffusion v1.5~\cite{rombach2022high} & SDXL~\cite{podell2024sdxl}\\
    CFG weight & 1 & 1 & 1.75/5 & 8 \\
        & \\[-1.9ex]
    \cdashline{1-5}
    & \\[-1.9ex]
    \textbf{Adam optimizer} \\
    Learning rate & 1e-4 & 2e-6 &  1e-5&  5e-7\\
    Learning rate for fine-tuning & - & 5e-7 & - & - \\
    Weight decay & 1e-2 & 1e-2 & 1e-2 & 1e-2  \\
    $\gamma$ in R1 regularization & 1 & 0 & 1 & 1\\
    \bottomrule
    \end{tabular}
    \caption{Training configuration for \methodtext on different datasets.}
    \label{tab:training}
\end{table}

\begin{figure*}[t]
    \centering
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{img/cifar_fid_gan_ablate_2.pdf}
        \vspace{-3pt}
    \caption{}
        \label{fig:feature-a}
  \end{subfigure}
 \hspace{3pt}
  \begin{subfigure}[b]{0.35\textwidth}
    \includegraphics[width=\textwidth]{img/cifar_fid_gan_ablate.pdf}
    \vspace{-3pt}
    \caption{}
    \label{fig:feature-b}
  \end{subfigure}
    \caption{FID score versus training iteration on CIFAR-10. \textit{Fake score feature}: fake score as the extractor, updating both the fake score and classification head in the GAN discriminator loss. \textit{Teacher score feature}: teacher score as the extractor, updating classification head in the GAN discriminator loss. \textit{Teacher score feature, updating cls head}: fake score as the extractor, updating classification head in the GAN discriminator loss. (a) is the zoomed-in visualization of (b).
    %\av{(a) is the zoomed-in visualization of (b).}
    }
    \label{fig:feature}
\end{figure*}



\begin{algorithm}[t]
\small
    \caption{$f$-distill Training}
    \label{alg:f-distill}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Teacher diffusion model $s_\phi$, fake score $s_\psi$, one-step student $G_\theta$, discriminator $D_\lambda$, total iteration $M$, batch size $B$, fake score update frequency $\tau$, weighting function $h$, GAN loss coefficient $w_{\textrm{GAN}}$, minimun/maximum value for ratio $r_{\textrm{min}}$/$r_{\textrm{max}}$.
        \For{iteration in $0 \dots M$}
                    \State $\rvx_1,...,\rvx_B \sim p_{\text{data}}$, $\epsilon_1,...,\epsilon_B \sim \mathcal{N}(  \mathbf{0}, \mI)$,
            $t_{1},...,t_B \sim \gU\{1, \dots, T\}$
            \State Generate the data by student: $\rvy_i = G_\theta(\epsilon_i)$
        \If{iteration$\% \tau = 0$}  \Comment{Update student}
            \State Compute and clip the density ratio: $r_i = \texttt{clip}(\exp(D_\lambda(\rvy_i)),r_{\textrm{min}},r_{\textrm{max}}) $
               \State Normalize the weighting coefficient: $\bar{r}_i = r_i / \sum_i r_i$ \Comment{First-stage normalization}
            \State Compute the weighting coefficient: $h_i = h(\bar{r}_i)$
            \State Normalize the weighting coefficient: $\bar{h}_i = h_i / \frac{1}{B}\sum_i h_i$ \Comment{Second-stage normalization}
            \State Compute the empirical $f$-distill loss $\tilde{\gL}_{\textrm{\method}}(\theta)$ based on \Eqref{eq:obj-final} with $(\rvy_i, t_i, \bar{h}_i)_{i=1}^B$ and fake score $s_\psi$
            \State Compute the empirical GAN loss $\tilde{\gL}_{\textrm{GAN}}(\theta)$ for generator with $(\rvy_i, t_i)_{i=1}^B$ 
            \State Update the student parameter $\theta$ by $\tilde{\gL}_{\textrm{\method}}(\theta)+w_{\textrm{GAN}}\tilde{\gL}_{\textrm{GAN}}(\theta)$.
        \Else \Comment{Update fake score and discriminator}
            \State Update the fake score with denoising score-matching loss using $(\rvy_i, t_i)_{i=1}^B$.
            \State Update the discriminator with empirical GAN loss $\tilde{\gL}_{\textrm{GAN}}(\lambda)$ for discriminator with $(\rvx_i, \rvy_i, t_i)_{i=1}^B$ 
        \EndIf
        
        \EndFor
    \end{algorithmic}
    \end{algorithm}