\section{Additional Results}
\label{app:results}
\subsection{CIFAR-10 result with more baselines}


Due to space limits, we do not include baselines in the table for CIFAR-10~(Table~\ref{tab:cifar10}) in the main text. We provide a more comprehensive comparison in Table~\ref{tab:cifar10-more}. Please note that we mainly use this dataset for analyzing the difference of variants under the \methodtext family. Unlike previous works using pre-trained feature extractor~\cite{Kim2023ConsistencyTM, zheng2024diffusion}, we use a simple classification head as a discriminator on top of the teacher's features, and do not tune hyper-parameter on this dataset. Note that our approach simply employs the teacher model as feature extractor unlike GDD-I and CTM, as we primarily use this dataset to analyze the relative performance of different $f$s\looseness=-1.


\begin{table}[htbp]
\footnotesize
    \centering
    \begin{tabular}{l c c c}
    \toprule
    & FID $\downarrow$ & Recall $\uparrow$ & NFE\\
        \textbf{Multi-step diffusion models}\\
        \midrule
           DDPM~\citep{ho2020ddpm} & 3.17 & & 1000  \\    
    LSGM~\citep{vahdat2021score} & 2.10 & & 138 \\
  EDM~\cite{Karras2022ElucidatingTD}~(teacher) & 1.79 & & 35 \\
    PFGM++~\citep{Xu2023PFGMUT} & 1.74 & & 35 \\
    \midrule
    \textbf{GANs}\\
    BigGAN~\cite{Brock2018LargeSG} & 14.73& & 1\\ 
    StyleSAN-XL~\cite{Sauer2022StyleGANXLSS} & 1.36 & & 1\\
    \midrule
    \textbf{Diffusion distillation}\\
    % iCT~\cite{song2023improved} & 2.83 \\
    % TCM~\cite{lee2024truncated} & 2.20 \\
    Adversarial distillation & 2.60 & &1\\
    SiD~($\alpha=1$)~\cite{zhou2024score} & 1.93  & &1\\
    SiD~($\alpha=1.2$)~\cite{zhou2024score} & 1.71  & &1\\
    CTM~\cite{Kim2023ConsistencyTM} & 1.73  & &1\\ 
    GDD~\cite{zheng2024diffusion} & 1.66 & &1\\
    GDD-I~\cite{zheng2024diffusion} & 1.44 & &1\\
    \midrule
    \textbf{\method}\\
    reverse-KL~(\textcolor{red}{\cmark}, DMD2 \cite{yin2024improved}) & 2.13 & 0.60 &1\\
    softened RKL~(\textcolor{red}{\cmark}) & 2.21 & 0.60 &1\\
    squared Hellinger~(\textcolor{orange}{--}) & 1.99 & 0.63   &1\\
    JS~(\textcolor{orange}{--}) & 2.00 & 0.62 &1\\
    Jeffreys~(\textcolor{green}{\xmark}) & 2.05 &0.62 &1\\
    forward-KL~(\textcolor{green}{\xmark}) & 1.92 & 0.62 &1\\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{FID and Recall scores on CIFAR-10. \textcolor{red}{\cmark}/\textcolor{orange}{--}/\textcolor{green}{\xmark} stand for high/medium/low mode-seeking tendency for $f$-divergence. }
    \label{tab:cifar10-more}
\end{table}

\subsection{Diversity evaluation based on in-batch similarity}
\label{app:diversity}
\begin{table}[t]
\footnotesize
    \centering
    \begin{tabular}{l c}
    \toprule
    &  In-batch-sim~$\downarrow$ \\
    \midrule
    
    Teacher (NFE=50, CFG=3, ODE)  & 0.55 / 0.70 \\
      Teacher (NFE=50, CFG=8, ODE)  & 0.62 / 0.72\\
      & \\[-1.9ex]
    \cdashline{1-2}
    & \\[-1.9ex]
    \textit{CFG=1.75} \\
    reverse-KL (DMD2)  & 0.50 / 0.42 \\
    JS~(\textit{ours})  & 0.49 / 0.41 \\
                & \\[-1.9ex]
    \cdashline{1-2}
    & \\[-1.9ex]
    \textit{CFG=5} \\
    reverse-KL (DMD2)  & 0.67 / 0.60\\
    JS~(\textit{ours})  & 0.65 / 0.58 \\
    \bottomrule
    \end{tabular}
    \caption{In-batch similarity on MS COCO 2014 / Parti-Prompt. We did not use Recall for text-to-image evaluation because it requires generating numerous samples per prompt. Furthermore, we found the Diversity score in \cite{yin2024improved} to be unreliable; higher CFG values in the teacher model, which are known to reduce diversity, will result in better Diversity scores.}
    \label{tab:sd-diversity}
\end{table}


It is important to understand the diversity of generated samples given a text prompt. We did not use Recall because it is unsuitable for measuring diversity in text-to-image generation, as it requires generating numerous samples per prompt. Furthermore, we found the Diversity score in \cite{yin2024improved} unreliable: higher CFG values in the teacher model, known to reduce diversity, will result in better Diversity scores. Hence, we opt for the in-batch similarity score. As a result, we use the in-batch similarity~\cite{corso2023particle} to measure the diversity. In-batch similarity~\cite{corso2023particle} calculates the average pairwise cosine similarity of features within an image batch, with DINO~\cite{caron2021dino} as the feature extractor. 

Table~\ref{tab:sd-diversity} reports the in-batch similarity score to measure the diversity of text-to-image tasks. \textit{We did not use Recall for text-to-image evaluation because it requires generating numerous samples per prompt. Furthermore, we found the Diversity score in \cite{yin2024improved} to be unreliable; higher CFG values in the teacher model, which are known to reduce diversity, will result in better Diversity scores.} Our main finding is that JS outperforms reverse-KL in in-batch similarity across datasets and CFGs (two numbers are MS-COCO/Parti-Prompt subset~\cite{yin2024improved} evaluations). JS shows a larger diversity gain on higher CFG, suggesting it preserves more modes. 


\subsection{Image quality evaluation based on HPSv2}
\label{app:hpsv2}
\begin{table}[t]
\footnotesize
    \centering
    \begin{tabular}{l c c c c}
    \toprule
    & Anime & Photo & Concept Art & Painting\\
    \midrule
    SDv1.5 (CFG=3) & 26.30& 27.56 & 25.86 &  26.08\\
    SDv1.5 (CFG=8) & 27.53& 28.46 & 26.94 &  26.83\\
            & \\[-1.9ex]
    \cdashline{1-5}
    & \\[-1.9ex]
    InstaFlow~\cite{liu2023instaflow} & 25.98 & 26.32 & 25.79 & 25.93\\
    SwiftBrush~\cite{nguyen2024swiftbrush} & 26.91 & 27.21 & 26.32 & 26.37\\
    SwiftBrush v2~\cite{dao2025swiftbrush} & 27.25 & 27.62 & 26.86 & 26.77\\
        & \\[-1.9ex]
    \cdashline{1-5}
    & \\[-1.9ex]
        \textit{CFG=1.75} \\
    reverse-KL (DMD2~\cite{yin2024improved}) & 26.20  & 27.33 & 25.82 & 25.68  \\
    JS~(\textit{ours}) &  26.32\textcolor{green}{$\uparrow$} & 27.71\textcolor{green}{$\uparrow$} & 25.79\textcolor{red}{$\downarrow$} & 25.81\textcolor{green}{$\uparrow$} \\
    & \\[-1.9ex]
    \cdashline{1-5}
    & \\[-1.9ex]
        \textit{CFG=5} \\
    reverse-KL (DMD2~\cite{yin2024improved}) & 26.52  &27.86 &26.25  & 26.10  \\
    JS~(\textit{ours}) & 26.85\textcolor{green}{$\uparrow$}  & 27.98\textcolor{green}{$\uparrow$} & 26.37\textcolor{green}{$\uparrow$} & 26.34\textcolor{green}{$\uparrow$} \\
    \bottomrule
    \end{tabular}
    \caption{HPSv2 score}
    \label{tab:hpsv2}
    \vspace{-10pt}
\end{table}

We evaluate the HPSv2 score~(higher is better) following the protocol in \cite{dao2025swiftbrush} to assess the image quality. In Table~\ref{tab:hpsv2}, we observe that JS consistently outperforms reverse-KL on almost all prompt categories. When CFG=5, JS performs competitively to 50-step teacher SDv.15 (CFG=8) and SwiftBrush v2. 

\subsection{Training stability}

In section~\ref{sec:properties} and section~\ref{app:property}, we show that more mode-coverage divergence tends to have a more rapidly increasing $h$, resulting in a higher-variance gradient. Although we propose a double normalization scheme in section~\ref{sec:properties}, we show that it is insufficient on larger-scale COYO-700M when using the SD v.1.5 model. As shown in Fig.~\ref{fig:training-loss}, the loss of forward-KL has a significantly larger fluctuation than the one in JS. In addition, the forward-KL achieves a much worse FID~(8.70) compared to JS~(7.45). We hypothesize that the inaccurate estimation of the density ratio $r$ by the discriminator on this dataset contributes to this phenomenon. We will leave the stabilization techniques for further work.


\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{img/loss_variance_sd.pdf}
    \caption{Training dynamics of JS and forward-KL on COYO-700M.}
    \label{fig:training-loss}
\end{figure*}

\subsection{Higher classifier-free guidance}

In this section, we experiment with applying higher classifier-free guidance~(CFG) to \methodtext, by replacing the teacher score in the gradient~(\Eqref{eq:time-0-loss}) with the corresponding CFG version. In Fig.~\ref{fig:vis-cfg-5}, Fig.~\ref{fig:vis-cfg-5-2} and Fig.~\ref{fig:vis-cfg-5-3}, we compare the generated samples by one-step JS with CFG=5, and by SD v1.5~(NFE=50) with CFG=5 / CFG=8. The first three prompts in these figures are randomly chosen from COYO-700M, and the next three prompts are from \cite{yin2024one}. We observe that, in general, the one-step student matches, or even outperforms, the teacher in most cases. In addition, JS produces more diverse samples compared to reverse-KL~(RKL).

We observe that both JS and revesre-KL~(DMD2~\cite{yin2024improved}) diverges when using CFG=8. We first hypothesize that this is because the generated samples and the real data~(COYO-700M) have a larger domain gap, exacerbating the training instability in GAN. DMD~\cite{yin2024one, yin2024improved} uses LAION-Aesthetic 5.5+ as the training set, which is considered more saturated and has a smaller domain gap with data generated by high CFG. However, we find that removing GAN loss does not resolve this issue.The training instability with high CFG might be linked to the weak teacher model~(SD1.5), as our preliminary experiments with clipped teacher score prevented divergence.

\subsection{Ablation study on GAN loss}

\begin{table}[t]
\footnotesize
    \centering
    \begin{tabular}{l c c}
    \toprule
    &  w/o GAN loss & w/ GAN loss \\
    \midrule
   \textit{CIFAR-10}\\
    reverse-KL (DMD2)  & 4.07 & 2.13\\
    JS  & 3.98 & 2.00 \\
    squared Hellinger &3.81 & 1.99 \\
    forward-KL &\bf{3.76} & \bf{1.92}\\
    \midrule
    \textit{MS-COCO-30k}\\
    reverse-KL (DMD2)  & 9.54 & 8.17\\
    JS  & \bf{9.10} & \bf{7.42} \\
    \bottomrule
    \end{tabular}
    \caption{FID score for ablation study on GAN loss}
    \label{tab:gan-loss-ab}
\end{table}


The final training framework in the experimental section contains two losses: the $f$-distill objective~(\Eqref{eq:obj-final}) and GAN loss. In this section, we conduct ablation studies on the GAN loss on CIFAR-10 and MS-COCO 2014 in $f$-distill.As shown in Table~\ref{tab:gan-loss-ab}, the relative ranking of FID scores by different f-divergence remains the same with or without GAN loss across datasets. 



\begin{figure*}
\centering
    \begin{subfigure}[b]{0.87\textwidth}
    \includegraphics[width=\textwidth]{img/cp1.png}
  \end{subfigure}
  \vfill
  \begin{subfigure}[b]{0.87\textwidth}
    \includegraphics[width=\textwidth]{img/cp2.png}
  \end{subfigure}
  \vspace{-12pt}
    \caption{SD v1.5: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.}
    \label{fig:vis-cfg-5}
\end{figure*}

\begin{figure*}
\centering
    \begin{subfigure}[b]{0.88\textwidth}
\includegraphics[width=\textwidth]{img/cp3.png}
  \end{subfigure}
\vfill
  \begin{subfigure}[b]{0.87\textwidth}
  \includegraphics[width=\textwidth]{img/cp4.png}
  \end{subfigure}
  \vspace{-12pt}
    \caption{SD v1.5: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.}
    \label{fig:vis-cfg-5-2}
\end{figure*}

\begin{figure*}
\centering
  \begin{subfigure}[b]{0.875\textwidth}
    \includegraphics[width=\textwidth]{img/cp5.png}
  \end{subfigure}\vfill
    \begin{subfigure}[b]{0.87\textwidth}
    \includegraphics[width=\textwidth]{img/cp6.png}
  \end{subfigure}
  \vspace{-12pt}
    \caption{SD v1.5: Generated samples from multi-step teachers and single-step students, using the same prompts and random seeds. The real data used for GAN objective are from COYO-700M.}
    \label{fig:vis-cfg-5-3}
\end{figure*}