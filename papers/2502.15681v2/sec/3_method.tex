\section{Method: general $f$-divergence minimization}

\begin{table*}[t]
\vspace{-0.7cm}
\small
    \centering
    \begin{tabular}{l c c c c c}
 \toprule
  & $f(r)$ & $h(r)$ & Mode-seeking? & Saturation? & Variance \\
  \midrule
  reverse-KL & $-\log r$ & $1$ & Yes & No & - \\
    softened RKL & $(r+1) \log{(\frac12+\frac{1}{2r})}$ & $\frac{1}{r+1}$ & Yes  & No & Low \\
Jensen-Shannon & $r\log r - (r+1) \log \frac{r+1}{2}$ & $\frac{r}{r+1}$ & Medium & Yes & Low \\
  squared Hellinger &
     $1-\sqrt{r}$ & $\frac14 r^{\frac12}$
 & Medium & Yes & Low \\ 
  forward-KL & $r\log r$ & $r$ & No & No & High \\
 Jeffreys & ${(r-1)\log(r)}$ & $r+1$ & No & No & High \\
    \bottomrule
    \end{tabular}
        \caption{Comparison of different $f$-divergences as a function of the likelihood ratio $r :=p(\rvx)/q(\rvx)$}
    \label{tab:f-div}
    \vspace{-12pt}
\end{table*}

In this section, we introduce a general distillation framework, termed \method, based on minimizing the $f$-divergence between the teacher and student distributions. Since the student distribution $q$ is the push-forward measure induced by the one-step generator $G_\theta$, it implicitly depends on the generator's parameters $\theta$. Due to this implicit dependency, directly calculating the gradient of $f$-divergence, $D_f(p||q)$, w.r.t $\theta$ presents a challenge. However, the following theorem establishes the analytical expression for this gradient, revealing that it can be formulated as a weighted version of the gradient employed in variational score distillation. Notably, these weights are determined by the density ratio of the generated samples. We state the theorem more generally by providing the gradient for $p_t$ and $q_t$, where $p_t$ is the perturbed distribution through the diffusion forward process for the teacher's distribution $p$, \textit{i.e.,} $p_t = p_0 * \gN( \mathbf{0},\sigma^2(t)\mI)$~(same for the student distribution $q$). \looseness=-1
\vspace{-2pt}
\begin{restatable}{theorem}{mainthm}
\label{thm-main}
Let $p$ be the teacher's generative distribution, and let $q$ be a distribution induced by transforming a prior distribution $p(\rvz)$ through the differentiable mapping $G_\theta$. Assuming $f$ is twice continuously differentiable, then the gradient of $f$-divergence between the two intermediate distribution $p_t$ and $q_t$ w.r.t $\theta$ is:
{
\small
\begin{align*} 
    &\nabla_\theta D_f(p_t||q_t) = \E_{\substack{\rvz,  \epsilon}}-\Big[f''\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)\left(\frac{p_t(\rvx)}{q_t(\rvx)}\right)^2\\
    & \quad  \quad \quad \quad \quad \ \ \Big(\underbrace{\nabla_\rvx \log p_t(\rvx)}_{\textrm{teacher score}} - \underbrace{\nabla_\rvx \log q_t(\rvx)}_{\textrm{fake score}} \Big)  \nabla_\theta G_\theta(\rvz)\Big]
    \numberthis \label{eq:time-0-loss}
\end{align*}
}%
where $\rvz \sim p(\rvz), \epsilon \sim \mathcal{N}(  \mathbf{0}, \mI)$ and $ \rvx = G_\theta(\rvz)+\sigma(t)\epsilon $

\end{restatable}
\begin{proofs} For simplicity, we prove the $t=0$ case in the main text. Similar proof applies for any $t>0$.
{\small
\begin{align*}
&\nabla_\theta D_f(p(\rvx)||q(\rvx)) =   \nabla_\theta \int q(\rvx) f(\frac{p(\rvx)}{q(\rvx)}) d\rvx \\
    &=  \underbrace{\int \nabla_\theta q(\rvx) f(\frac{p(\rvx)}{q(\rvx)}) d\rvx}_{I} - \underbrace{\int \nabla_\theta q(\rvx) f'(\frac{p(\rvx)}{q(\rvx)}) \frac{p(\rvx)}{q(\rvx)}  d\rvx}_{II}
\end{align*}
}%
    \iffalse
\begin{align*}
&\nabla_\theta D_f(p(\rvx)||q(\rvx)) =   \nabla_\theta \int q(\rvx) f(\frac{p(\rvx)}{q(\rvx)}) d\rvx \\
    &\overset{\rvx=G_\theta(\rvz)}{=}  \nabla_\theta \int p_{\textrm{prior}}(\rvz) f(\frac{p(G_\theta(\rvz))}{q(G_\theta(\rvz))}) d\rvz \\
    &=  \int p_{\textrm{prior}}(\rvz) f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\nabla_{\rvx} \frac{p(\rvx)}{q_{\textrm{sg}}(\rvx)}\nabla_\theta G_\theta(\rvz) d\rvz  \\
    &\qquad -   \int p_{\textrm{prior}}(\rvz) f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\frac{p(\rvx)}{q^2(\rvx)}\nabla_\theta q(G_{\theta_{\textrm{sg}}}(\rvz)) d\rvz
    \numberthis \label{eq:full-grad}
    \end{align*}
\fi
It can be shown that (I) / (II) arises from the term associated with the partial derivative of $f$ with respect to $\rvx$ / $q$, respectively. Above we see that both partial derivatives (I) and (II) are in the form $\int \nabla_\theta q(\rvx) g(\rvx) d\rvx$ where $g$ is a differentiable function that is constant with respect to $\theta$. Assuming that sampling from $\rvx \sim q(\rvx)$ can be parameterized to $\rvx = G_\theta(\rvz)$ for $\rvz \sim p(\rvz)$, we can use the identity $\int \nabla_\theta q(\rvx) g(\rvx) d\rvx = \int p(\rvz) \nabla_\rvx g(\rvx) \nabla_\theta G_\theta(\rvz) d\rvz$. The proof for the identity is provided in the Appendix. Using the identity we can simplify (I) and (II) to:
{
\begin{align*}
I &= \int p(\rvz) f'(\frac{p(\rvx)}{q(\rvx)}) \nabla_\rvx \frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz \\
II &= \int p(\rvz) f''(\frac{p(\rvx)}{q(\rvx)}) \frac{p(\rvx)}{q(\rvx)} \nabla_\rvx \frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz \\
& + \int p(\rvz) f'(\frac{p(\rvx)}{q(\rvx)}) \nabla_\rvx \frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz 
\end{align*}
}%
Putting (I) and (II) in \Eqref{eq:time-0-loss}, we have:
{
\begin{align*}
    \nabla_\theta D_f = &-\int p(\rvz) f''(\frac{p(\rvx)}{q(\rvx)}) \frac{p(\rvx)}{q(\rvx)} \nabla_\rvx \frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz \\
= &-\int p(\rvz) f''\left(\frac{p(\rvx)}{q(\rvx)}\right)\left(\frac{p(\rvx)}{q(\rvx)}\right)^2\\
    & \quad \quad \left[\nabla_\rvx \log p(\rvx) - \nabla_\rvx \log q(\rvx)\right] \nabla_\theta G_\theta(\rvz) d\rvz 
\end{align*}
}%
where the last identity is from the log derivative trick. 
\end{proofs}
\iffalse
The second term in \Eqref{eq:full-grad} can be further simplified into:
    \begin{align*}
 &\int p_{\textrm{prior}}(\rvz) f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\frac{p(\rvx)}{q^2(\rvx)} \nabla_\theta q(G_{\theta_{\textrm{sg}}}(\rvz)) dz \\
 &\overset{\rvx=G_\theta(\rvz)}{=} \int q(\rvx) f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\frac{p(\rvx)}{q^2(\rvx)} \nabla_\theta q(\rvx) dx \\
&=\nabla_\theta \int f'\left(\frac{p(\rvx)}{q_{\textrm{sg}}(\rvx)}\right)\frac{p(\rvx)}{q_{\textrm{sg}}(\rvx)}  q(\rvx) dx \\
    &\overset{\rvx=G_\theta(\rvz)}{=} \nabla_\theta \int f'\left(\frac{p(G_\theta(\rvz))}{q(G_\theta(\rvz))}\right)\frac{p(G_\theta(\rvz))}{q(G_\theta(\rvz))} p_{\textrm{prior}}(\rvz) d\rvz \\
&=  \int p_{\textrm{prior}}(\rvz) f''\left(\frac{p(\rvx)}{q(\rvx)}\right)\nabla_\rvx\frac{p(\rvx)}{q(\rvx)}\frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz \\
    &\qquad + \int p_{\textrm{prior}}(\rvz) f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\nabla_\rvx\frac{p(\rvx)}{q(\rvx)} \nabla_\theta G_\theta(\rvz) d\rvz\\
    &=  \int p_{\textrm{prior}}(\rvz) f''\left(\frac{p(\rvx)}{q(\rvx)}\right)(\frac{p(\rvx)}{q(\rvx)})^2\\
    & \qquad  [\nabla_\rvx \log p(\rvx) - \nabla_\rvx \log q(\rvx)] \nabla_\theta G_\theta(\rvz) d\rvz \\
    &\qquad + \int p_{\textrm{prior}}(\rvz)  \left(f'\left(\frac{p(\rvx)}{q(\rvx)}\right)\nabla_\rvx\frac{p(\rvx)}{q(\rvx)}\right) \nabla_\theta G_\theta(\rvz) d\rvz
    \numberthis\label{eq:full-2}
\end{align*}
where ${\textrm{sg}}$ stands for stop gradient. We conclude the theorem by plugging \Eqref{eq:full-2} into \Eqref{eq:full-grad}.
\fi



We defer the completed proofs to App~\ref{app:proof} in the supplementary material. Although the student's generative distribution $q$ depends on the parameter $\theta$, Theorem~\ref{thm-main} provides an analytical expression for the gradient of $f$-divergences between the teachers' and students' generative distributions. This gradient is expressed as the score difference between the teacher's and student's distributions, weighted by a time-dependent factor $f''\left({p_t(\rvx_t)}/{q_t(\rvx_t)}\right)\left({p_t(\rvx_t)}/{q_t(\rvx_t)}\right)^2$ determined by both the chosen $f$-divergence and the density ratio. Crucially, every term in the theorem is tractable
%\av{tractable?}
, enabling the optimization of distributional matching through general $f$-divergence minimization. For notation convenience, let $h(r) := f''(r)r^2$ denote the weighting function, and $r_t(\rvx) := p_t(\rvx)/q_t(\rvx)$ denote the density-ratio at time $t$. It is worth noting that the gradient of the variational score distillation~(\Eqref{eq:vsd-obj}) can be recovered as a special case of our framework by setting $h(r)\equiv 1$ %\av{$h(r)\equiv -1$?} 
in \Eqref{eq:time-0-loss}, which corresponds to minimizing the reverse-KL divergence~($f(r)=-\log r $). 

\cite{song2021maximum} also shows a connection between f-divergence and score difference, expressing the former as a time integral of the squared score difference in their Theorem 2. However, our formulation differs in two key aspects: (1) Their objective's gradient necessitates computing a Jacobian-vector product, which can be computationally expensive. 
(2) While their $f$-divergence is expressed as an integral of the score difference over time, our $f$-divergence, $D_f(p_t||q_t)$, depends only on the weighting and score at time $t$. In the following proposition, we further show that if the weighting function $h$ is continuous and non-negative on $(0, +\infty)$, then its product with score difference is the gradient of certain $f$-divergence:
\vspace{-5pt}
\begin{restatable}{proposition}{mainprop}\label{prop:general}
For any function $h$ that is continuous and non-negative on $(0, +\infty)$, the expectation 
$\E_{\substack{\rvz,  \epsilon}}-\left[h\left(r_t(\rvx)\right)\right.\left({\nabla_\rvx \log p_t(\rvx)} - {\nabla_\rvx \log q_t(\rvx)} \right)  \nabla_\theta G_\theta(\rvz)]
$
corresponds to the gradient of an $f$-divergence.
\end{restatable}
% \av{Although we limit our study in this paper to canonical forms of $f$-divergence, Proposition~\ref{prop:general} allows us to use any continuous and non-negative scalar function as $h$.}
Although we limit our study in this paper to canonical forms of $f$-divergence, Proposition~\ref{prop:general} allows us to use any continuous and non-negative scalar function as $h$.

In practice, \cite{yin2024one} suggests performing distributional matching all the time along the diffusion process, as teacher and student will have high discrepancy at smaller times, leading to optimization difficulties. We follow this setup and minimize the $f$-divergence along the whole time range, \ie $\gL(\theta) = \int_0^T w_tD_f(p_t||q_t)dt$, where $w_t$ is a time-dependent weight for equalizing the gradient magnitudes across times. The final objective function for \methodtext is as follows:
\iffalse
\begin{align*}
\small 
    &\gL_{\textrm{\method}}(\theta) = \frac12 \E_{t, \rvx_t \sim q_t}[(\rvx-\\
    & \texttt{sg}(\rvx + w_t{h(r_t(\rvx_t))}(\underbrace{\nabla_\rvx \log p_t(\rvx_t)}_{\textrm{teacher score}} - \underbrace{\nabla_\rvx \log q_{t}(\rvx_t)}_{\textrm{fake score}}))^2]
    \numberthis \label{eq:obj-final}
\end{align*}
\fi
{
\begin{align*}
    &\gL_{\textrm{\method}}(\theta) = \E_{t, \rvx}\Big[ \numberthis \label{eq:obj-final} \\
    & \texttt{\color{red}sg\big(} w_t h(r_t(\rvx))({\nabla_\rvx \log p_t(\rvx)} - {\nabla_\rvx \log q_{t}(\rvx)}){\color{red}\big)}^T \rvx\Big]
\end{align*}
}%
% \av{Do we still need to emphasize on \texttt{sg} in $\nabla_\rvx \log q_{t, \texttt{sg}}(\rvx_t)$? I am in favor of dropping it.}
where $\rvx {=} G_\theta (\rvz) {+} \sigma(t) \epsilon , \rvz {\sim} p(\rvz), \epsilon {\sim} \mathcal{N}(  \mathbf{0}, \mI)$, and $\texttt{sg}$ stands for stop gradient. The gradient of \Eqref{eq:obj-final} equals the time integral of the gradient of $f$-divergence in Theorem 1~(\Eqref{eq:time-0-loss}). In practice, the score of student distribution $\nabla_\rvx \log q_{t}(\rvx_t)$ is approximated by an online diffusion model $s_\psi (\rvx, \sigma(t))$.  \cite{yin2024improved} augments the variational score distillation loss with a GAN objective to further enhance performance. This is motivated by the fact that variational score distillation relies solely on the teacher's score function and is therefore limited by the teacher's capabilities. Incorporating a GAN objective allows the student generator $G_\theta$ to surpass the teacher's limitations by leveraging real data to train a discriminator $D_{\lambda}$: 
% \av{why isn't the real data diffused below? Shouldn't D take $t$ or $\sigma(t)$ as input, since it is time dependent?}\yx{I use $p_{\textrm{data}, t}$ to denote the diffused distribution}
\begin{align*}
    &\gL_{\textrm{GAN}}(\lambda) = \E_{t, \rvx \sim p_{\textrm{data}}, \epsilon_1}[\log D_\lambda(\rvx+ \sigma(t) \epsilon_1)] + \\
    &\qquad\qquad \quad\quad \E_{t, \rvz , \epsilon_2}[\log (1-D_\lambda(G_\theta (\rvz) + \sigma(t) \epsilon_2))]
\end{align*}
where $\rvz {\sim} p(\rvz), \epsilon_1, \epsilon_2 \sim \mathcal{N}(\mathbf{0}, \mI) $. We incorporate the auxiliary GAN objective as in prior work, which offers the additional advantage of providing a readily available estimate of the density ratio $r(\rvx_t)$ required by the weighting function in \Eqref{eq:obj-final}. The density ratio is approximated as follows: $r(\rvx_t)= p_t(\rvx)/q_t(\rvx) \approx p_{\textrm{data}, t}/q_t(\rvx) = D_{\lambda}(\rvx_t, t)/(1-D_{\lambda}(\rvx_t, t))$.  In essence, the GAN discriminator $D_{\lambda}$ provides a direct estimate of the density ratio, facilitating the computation of the weighting function. 

% \subsection{Practical considerations}
% \label{subsec:practice}
% \paragraph{Variance-reduction techniques for \method} The weighting functions

% % \begin{enumerate}
% %     \item ratio normalization 
% %     \item total normalization
% %     \item experiments
% % \end{enumerate}

% \paragraph{Teacher encoder as feature extractor}

% \yx{TODO: CIFAR-10 experiments}

