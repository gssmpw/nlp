\vspace{-2pt}
\section{Conclusions}
\vspace{-2pt}
We have proposed \method, a novel and general framework for distributional matching distillation based on $f$-divergence minimization. We derive a gradient update rule comprising the product of a weighting function and the score difference between the teacher and student distributions. \methodtext encompasses previous variational score distillation objectives while allowing less mode-seeking divergences. By leveraging the weighting function, \methodtext naturally downweights regions with larger score estimation errors.  Experiments on various image generation tasks demonstrate the strong one-step generation capabilities of \method. 
