
\section{Experiment}
\label{sec:experiment}




\subsection{Image generation}


We evaluate \methodtext on CIFAR-10~\citep{krizhevsky2009learning} and ImageNet-64~\citep{deng2009imagenet} for class-conditioned image generation, and on zero-shot MS COCO 2014~\cite{Lin2014MicrosoftCC} for text-to-image generation. We use COYO-700M~\cite{kakaobrain2022coyo-700m} as the training set for text-to-image generation. We use pre-trained models in EDM~\cite{Karras2022ElucidatingTD} as teachers for CIFAR-10 / ImageNet-64, and Stable Diffusion~(SD) v1.5~\cite{rombach2021highresolution} / SDXL~\cite{podell2024sdxl} for text-to-image. As DMD2~\cite{yin2024improved} is a special case~(reverse-KL) under the \methodtext framework, we use the learning rates, CFG guidance weight, update frequency for fake score and discriminator, and the coefficient for GAN loss in generator update in DMD2. 
% We posit that hyperparameters tuned for reverse-KL should generalize effectively to other divergences. 
In the text-to-image experiment, we observe that the estimation of density ratio~(and thus the weighting function $h$) by the discriminator is inaccurate at the early stage of training. To address this, we ``warm up" the discriminator by initializing the model with a pre-trained reverse-KL model, which has a constant $h$. We defer the training details to App~\ref{app:details}. % in the supplementary material. 

Our baseline comparisons include multi-step diffusion models and existing diffusion distillation techniques. We also re-implemented DMD2~\cite{yin2024improved} within our codebase.  Furthermore, to isolate the impact of our proposed \methodtext objective, we conducted an ablation study by removing it and training solely with the GAN objective (denoted as ``Adversarial distillation" in the tables).

\begin{figure*}[t]
    \vspace{-0.7cm}
    \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim=0.2cm 0.cm 0.cm 0.2cm, clip]{img/CIFAR_forward_KL_h_normalized.pdf}
        \vspace{-12pt}
    \caption{}
  \end{subfigure}
 \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim=0.2cm 0.cm 0.cm 0.2cm, clip]{img/ImageNet_JS_h_normalized.pdf}
    \vspace{-12pt}
    \caption{}
  \end{subfigure}\hfill
    \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth, trim=0.2cm 0.cm 0.cm 0.2cm, clip]{img/SD_normalized.pdf}
    \vspace{-12pt}
    \caption{}
  \end{subfigure}
  \vspace{-6pt}
    \caption{Normalized weighting function $h$ and score difference versus the index of 6.4K generated samples, sorted by the score difference, on \textbf{(a)} CIFAR-10 with forward-KL; \textbf{(b)} ImageNet-64 with JS; \textbf{(c)} SDv1.5 with JS.}
    \label{fig:h-scorediff}
    \vspace{-6pt}
\end{figure*}

\begin{table}[t]
\scriptsize
    \centering
    \begin{tabular}{l c c c}
    \toprule
    & FID $\downarrow$ & CLIP score $\uparrow$& Latency $\downarrow$ \\
    \midrule
    \textbf{Multi-step diffusion models}\\
    LDM~\cite{rombach2021highresolution} & 12.63 && 3.7s \\
    DALLÂ·E 2~\cite{ramesh2022dalle2} & 10.39 && 27s \\
Imagen~\cite{Saharia2022PhotorealisticTD} & 7.27 &0.28*& 9.1s \\
    eDiff-I~\cite{balaji2022ediffi} & \bf{6.95} &0.29*& 32.0s \\
    UniPC~\cite{zhao2024unipc} & 19.57 &&  0.26s \\
    Restart~\cite{Xu2023RestartSF} & 13.16 & 0.299 &3.40s\\
    & \\[-1.9ex]
    \cdashline{1-4}
    & \\[-1.9ex]
    \textbf{Teacher}\\
    SDv1.5 (NFE=100, CFG=3, ODE) & 8.59 & 0.308&  2.59s\\
    SDv1.5 (NFE=200, CFG=2, SDE) & 7.21 & 0.301 &10.25s \\
        \midrule
    \textbf{GANs}\\
    StyleGAN-T~\cite{sauer2023stylegan} & 13.90 &0.29*&  0.10s\\
    GigaGAN~\cite{kang2023scaling} & 9.09 &  &0.13s\\
    \midrule
    \textbf{Diffusion distillation}\\
    SwiftBrush~\cite{nguyen2024swiftbrush} & 16.67 &0.29* & 0.09s\\
    SwiftBrush v2~\cite{dao2025swiftbrush} & 8.14 & 0.32* & 0.06s\\
    HiPA~\cite{zhang2023hipa} & 13.91 && 0.09s\\
    InstaFlow-0.9B~\cite{liu2023instaflow} & 13.10 && 0.09s\\
    UFOGen~\cite{xu2024ufogen} & 12.78 && 0.09s\\
    DMD~\cite{yin2024one} & 11.49 && 0.09s \\
    EMD~\cite{xie2024distillation} & 9.66 && 0.09s \\
        & \\[-1.9ex]
    \cdashline{1-4}
    & \\[-1.9ex]
    \textit{CFG=1.75} \\
    reverse-KL (DMD2~\cite{yin2024improved}) & 8.17 & 0.287 & 0.09s\\
    JS~(\textit{ours}) & \bf{7.42} & 0.292 & 0.09s\\
        & \\[-1.9ex]
    \cdashline{1-4}
    & \\[-1.9ex]
    \textit{CFG=5} \\
    reverse-KL (DMD2~\cite{yin2024improved}) & 15.23 & 0.309 & 0.09s\\
    JS~(\textit{ours}) & 14.25 & 0.311 & 0.09s\\
    \bottomrule
    \end{tabular}
    \caption{FID score together with inference latency on text-to-image generation, zero-shot MS COCO-30k 512$\times $ 512. * denotes that the value is taken from the corresponding paper.}
    \label{tab:sd}
    \vspace{-5pt}
\end{table}

% \begin{table}[t]
% \vspace{-2pt}
% \scriptsize
%     \centering
%     \begin{tabular}{l c c c}
%     \toprule
%     & ImageReward & CLIP score & Aesthetic score  \\
%     \midrule    
%     SDXL (NFE=100, CFG=8) & & \\
%     reverse-KL~(DMD2~\cite{yin2024improved}) & 1.19 & 5.38  & 0.294\\
%     JS & 1.21 & 5.43& 0.296 \\
%     \bottomrule
%     \end{tabular}
%     \vspace{-5pt}
%     \caption{Quality and text-image alignment evaluation on SDXL.}
%     \label{tab:sdxl}
%     \vspace{-6pt}
% \end{table}


% \begin{table}[t]
% \footnotesize
%     \centering
%     \begin{tabular}{l c}
%     \toprule
%     &  In-batch-sim~$\downarrow$ \\
%     \midrule
    
%     SDv1.5 (NFE=50, CFG=3, ODE)  & 0.696 \\
%       SDv1.5 (NFE=50, CFG=8, ODE)  & 0.717 \\
%       & \\[-1.9ex]
%     \cdashline{1-2}
%     & \\[-1.9ex]
%     \textit{CFG=1.75} \\
%     reverse-KL (DMD2~\cite{yin2024improved})  & 0.417\\
%     JS~(\textit{ours})  & 0.410 \\
%                 & \\[-1.9ex]
%     \cdashline{1-2}
%     & \\[-1.9ex]
%     \textit{CFG=5} \\
%     reverse-KL (DMD2~\cite{yin2024improved})  & 0.598\\
%     JS~(\textit{ours})  & 0.576 \\
%     \bottomrule
%     \end{tabular}
%     \caption{In-batch similarity on Parti-Prompt}
%     \label{tab:sd-diversity}
%     \vspace{-10pt}
% \end{table}



\vspace{-10pt}
\paragraph{Evaluations.} We measure sample quality with Fr\'{e}chet Inception Distance (FID)~\citep{heusel2017gans}. For diversity, we use the Recall score~\cite{Kynknniemi2019ImprovedPA}. For image-caption alignment, we report the CLIP score. We defer more evaluations to App~\ref{app:diversity} and ~\ref{app:hpsv2} on other metrics, such as in-batch similarity~\cite{corso2023particle} and HPSv2 score~\cite{dao2025swiftbrush} for diversity and image quality assessment.


\vspace{-10pt}
\paragraph{Results.} We first experiment with various $f$-divergences on CIFAR-10, to analyze the relative performance of different $f$s. Table~\ref{tab:cifar10} shows that all the variants under \methodtext outperform the adversarial distillation baseline, validating the effectiveness of \methodtext in addition to GAN objective. In particular, $f$-divergences with milder mode-seeking behavior, such as forward-KL and Jeffreys divergences, generally yield lower FID scores and higher Recall scores. 



\begin{figure*}[t]
\centering
\includegraphics[width=0.91\textwidth]{img/vis_2.png}
\vspace{-11pt}
\caption{\textbf{(a)} Uncurated generated samples by the multi-step teacher diffusion models (top), and one-step student in \methodtext (bottom), using the same random seed. The teacher diffusion models use 35 and 50 steps on ImageNet-64 and Stable Diffusion v1.5, respectively. \textbf{(b)} Generated samples by reverse-KL and JS,  using a prompt in COYO: ``\texttt{a blue and white passenger train coming to a stop}".\looseness=-1}
\label{fig:vis}
\vspace{-10pt}
\end{figure*}



Table~\ref{tab:imagenet-64} and Table~\ref{tab:sd} report FID, Recall and CLIP score in two more challenging datasets. We report the inference latency on a single NVIDIA A100 GPU on text-to-image generation, as in \cite{yin2024improved}. Our main findings are: (1) \textbf{\methodtext with JS divergence achieves the current state-of-the-art one-step FID score on both ImageNet-64 and zero-shot MS COCO.} Concretely, JS achieves FID scores of 1.16 on ImageNet-64, outperforming previous best-performing diffusion models, GANs, and distillation methods, except for GDD-I~\cite{zheng2024diffusion}. GDD-I solely applies the GAN objective, which is known to be unstable in large-scale settings. It is reflected in the worse Recall score of GDD-I. Furthermore, JS obtains an FID score of {\bf{7.42}} on MS COCO, when using a CFG=1.75, significantly outperforming previous distillation methods and approaching the performance of leading diffusion models like eDiff-I~\cite{balaji2022ediffi}~(FID of 6.95). (2) \textbf{Forward-KL and JS get better FID scores than reverse-KL}. The two variants with less mode-seeking behavior continue to outperform the reverse-KL~(DMD2~\cite{yin2024improved}). 
% In addition, for CLIP score, we observed that the JS outperforms reverse-KL with varying CFG weight. 
% In particular, JS still achieves a decent CLIP score (0.292) while achieving state-of-the-art one-step FID score (7.42). 
When using a higher CFG value 5  in distillation, the JS outperforms most of all the baselines except SwiftBruch v2~\cite{dao2025swiftbrush}, which uses a more advanced SD2.1 as teacher model and additional CLIP loss during training.
(3) \textbf{JS is more stable than forward-KL}. While forward-KL can be less mode-seeking, it suffers from higher variance.  Our experiments on ImageNet-64 confirm that JS exhibits a significantly more stable training process~(App~\ref{app:results}), resulting in a better FID score. We further scale JS to larger models SDXL and observed competitive results to teacher, as shown in Fig.~\ref{fig:vis-main}. \looseness=-1



We further provide a visual comparison of generated samples by teacher and student in Fig.~\ref{fig:vis}. We observe that the generated images by one-step students generally have richer details and more aligned with the text prompts. We provide detailed prompts and extended samples in App~\ref{app:extended} in the supplementary material. 







\subsection{Behavior of weighting function $h$}

As discussed in Section~\ref{sec:properties}, $f$-divergence with less mode-seeking tendancy has a faster increasing second derivative $f''$, resulting in an increasing weighting function $h(r)=f''(r)r^2$. This means that generated samples in the low-density regions of the data distribution $p$ will be down-weighted accordingly, and the teacher models are prone to inaccurate score estimation in these regions~\cite{karras2024guiding}.


To further understand the behavior of $h$, we study its relation with the score difference between a teacher and fake score, \ie $||s_\phi(\rvx; \sigma(t)) - s_\psi(\rvx, \sigma(t))||_2$ on real datasets. Recall that the teacher $s_\phi$ approximates the true score, \ie $s_\phi(\rvx; \sigma(t)) \approx \nabla_\rvx \log p_t(\rvx)$, and the online fake score approximate the generated distribution, \ie $s_\psi (\rvx, \sigma(t)) \approx \nabla_\rvx \log q_t(\rvx)$. We compute both $h$ and the score difference for 6.4K generated samples and sort them in ascending order of their score difference. Fig.~\ref{fig:h-scorediff} shows that the sample's weighting $h$ generally goes in the opposite direction with its score difference when using less mode-seeking divergences. This observation suggests that when using non-mode-seeking divergences~(\textit{e.g.,} forward-KL, JS), \methodtext effectively downweights samples in regions where the teacher and fake scores exhibit substantial discrepancies, which typically correspond to low-density regions of the true data distribution.
