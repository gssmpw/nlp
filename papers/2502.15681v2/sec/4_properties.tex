\section{Comparing properties of $f$-divergence}

\label{sec:properties}

In this section, we compare the properties across different distance measures in the $f$-divergence family, in the context of diffusion distillation. We will inspect their three properties: mode-seeking, saturation, and variance during training. We summarize the comparison of different $f$s, and their corresponding weighting function $h$, in Table~\ref{tab:f-div}.

\vspace{-10pt}
\paragraph{Mode-seeking.} Mode-seeking divergences~\cite{Bishop2006PatternRA, pmlr-v206-ting-li23a}, such as reverse-KL, encourage the generative distribution $q$ only to capture a subset of the modes of data distribution. This behavior, however, is undesirable for generative models as it can lead to dropped modes and a loss of diversity in generated samples. This phenomenon is observed in the variational score distillation loss~\cite{lu2024simplifying} used in DMD~\cite{yin2024one, yin2024improved}, which corresponds to minimizing the reverse-KL divergence in \methodtext. One way to characterize mode-seeking behavior is by examining the limit $\lim_{r\to \infty}f(r)/r$~\cite{pmlr-v206-ting-li23a}. A lower growth rate of the limit indicates more mode-seeking~(we defer detailed discussions to Sec~\ref{app:property}). Both reverse-KL and JS divergences exhibit this finite limit, with JS having a higher growth rate (and thus, less mode-seeking behavior). In contrast, forward-KL has an infinite limit, echoing its well-known mode-covering property. 

It is noteworthy that in Table~\ref{tab:f-div}, divergences with a stronger tendency towards mode-seeking also exhibit a slower rate of increase in their weighting function $h(r)$ as $r{\to} \infty$. This behavior stems from the fact that $f''(r)=h(r)/r^2$ also increases more slowly, thus tolerating larger density ratios $p/q$ (\ie allowing $q$ to disregard some modes in $p$), ultimately leading to mode-seeking behavior~\cite{Shannon2020NonsaturatingGT}.
% This observation aligns with the concept of ``tail weight of right-mismatching" introduced in \cite{Shannon2020NonsaturatingGT}, which provides a more rigorous characterization based on the increasing rate of $ \lim_{r\to \infty}f''(r)$. 
For example, $h$ in JS and forward-KL is an increasing function, while in reverse-KL $h$ stays constant. As a result, the weighting function in less mode-seeking divergence will tend to downweight samples in low-density regions of teacher distribution.  \looseness=-1

\vspace{-10pt}

\paragraph{Saturation.} A challenge encountered by prior generative models, such as GANs~\cite{goodfellow2014generative}, when utilizing $f$-divergence is the issue of saturation.  In the early stages of training, the generative and data distribution are often poorly aligned, resulting in samples from $q$ having very low probability under $p$, and vice versa. Consequently, the density ratio $p/q$ tends to be either extremely large or near zero. This poses optimization issues when divergences have small gradients at both extremes. From Fig.~\ref{fig:f}, we can see that squared Hellinger and JS divergences have smaller gradients at the extremes. Nevertheless, in diffusion distillation literature~\cite{yin2024one, yin2024improved}, the saturation issue is mitigated by initializing the weights of the student model with the pre-trained diffusion models.

\begin{figure}[t]
    \centering
    \vspace{-0.6cm}
  \begin{subfigure}[b]{0.234\textwidth}
    \includegraphics[width=\textwidth, trim=0.cm 0.cm 0.6cm 1.cm, clip]{img/f.pdf}
        \vspace{-9pt}
    \caption{}
        \label{fig:f}
  \end{subfigure}
 \hfill
  \begin{subfigure}[b]{0.234\textwidth}
    \includegraphics[width=\textwidth, trim=0.cm 0.cm 0.6cm 1.cm, clip]{img/h.pdf}
    \vspace{-9pt}
    \caption{}
        \label{fig:h}
  \end{subfigure}
  \vspace{-6pt}
    \caption{The absolute value of $f'$~\textbf{(a)} and weighting function $h(r)$~\textbf{(b)} in different $f$-divergences.}
    \vspace{-15pt}
\end{figure}

\begin{table}[b]
\vspace{-8pt}
\footnotesize
    \centering
    \begin{tabular}{l c c c}
    \toprule
    & FID $\downarrow$ & Recall $\uparrow$ \\
    \midrule    EDM~\cite{Karras2022ElucidatingTD}~(NFE=35) & 1.79 & 0.63 \\
    % \midrule
    % \textbf{Diffusion distillation}\\
    % iCT~\cite{song2023improved} & 2.83 \\
    % TCM~\cite{lee2024truncated} & 2.20 \\
    Adversarial distillation & 2.60 \\
    % SiD~\cite{zhou2024score} & 1.71 \\
    % CTM~\cite{Kim2023ConsistencyTM} & 1.73\\ 
    % GDD-I~\cite{zheng2024diffusion} & 1.44 \\
    \midrule
    \textbf{\method}\\
    reverse-KL~(\textcolor{red}{\cmark}, DMD2 \cite{yin2024improved}) & 2.13 & 0.60 \\
    softened RKL~(\textcolor{red}{\cmark}) & 2.21 & 0.60 \\
    squared Hellinger~(\textcolor{orange}{--}) & 1.99 & 0.63   \\
    JS~(\textcolor{orange}{--}) & 2.00 & 0.62\\
    Jeffreys~(\textcolor{green}{\xmark}) & 2.05 &0.62 \\
    forward-KL~(\textcolor{green}{\xmark}) & 1.92 & 0.62 \\
    \bottomrule
    \end{tabular}
    \vspace{-5pt}
    \caption{FID and Recall scores on CIFAR-10. \textcolor{red}{\cmark}/\textcolor{orange}{--}/\textcolor{green}{\xmark} stand for high/medium/low mode-seeking tendency for $f$-divergence. }
    \label{tab:cifar10}
    \vspace{-6pt}
\end{table}




\begin{figure}
    \centering
    %\vspace{-0.5cm}
  \begin{subfigure}[b]{0.234\textwidth}
    \includegraphics[width=\textwidth, trim=0.2cm 0.cm 0.5cm 0.5cm, clip]{img/variance.pdf}
    %\vspace{-5pt}
    \caption{}
     \label{fig:var}
  \end{subfigure}
    \begin{subfigure}[b]{0.234\textwidth}
    \includegraphics[width=\textwidth, trim=0.2cm 0.cm 0.5cm 0.5cm, clip]{img/loss_variance.pdf}
    %\vspace{-9pt}
    \caption{}
      \label{fig:var-loss}
  \end{subfigure}
   %\vspace{-6pt}
    \caption{\textbf{(a)} Normalized variance versus the mean difference between two Gaussians. \textbf{(b)} Training losses of forward-KL w/ and w/o normalizations. }
    %\vspace{-15pt}
\end{figure}


\begin{table}[t]
\footnotesize
\vspace{-0.5cm}
    \centering
    \begin{tabular}{l c c c}
    \toprule
    & FID $\downarrow$  & Recall $\uparrow$ & NFE\\
    \midrule
    \textbf{Multi-step diffusion models}\\
    EDM~(Teacher)~\cite{Karras2022ElucidatingTD} & 2.35 & 0.68 & 79 \\
    RIN~\cite{Jabri2022ScalableAC} & 1.23 & & 1000\\
    DisCo-Diff~\cite{xu2024disco} & 1.22 & & 623\\
    \midrule
    \textbf{GANs}\\
    BigGAN-deep~\cite{Brock2018LargeSG} & 4.06  & 0.48 & 1 \\
    StyleGAN-XL~\cite{Sauer2022StyleGANXLSS} & 1.52 &  & 1 \\
    \midrule
    \textbf{Diffusion distillation}\\
    DSNO~\cite{zheng2022fast} & 7.83  & 0.61 & 1\\
    % Diff-Instruct~\cite{luo2024diff} & 5.57 & & 1 \\
    % iCT~\cite{song2023improved} & 4.02  & 0.63  & 1\\
    iCT-deep~\cite{song2023improved} & 3.25 &0.63 & 1 \\
    Moment Matching~\cite{salimans2024multistep} & 3.00 & & 1 \\
    DMD~\cite{yin2024one} & 2.62 & & 1 \\
    ECM~\cite{geng2024consistency} & 2.49 & &1 \\
    % ECM~\cite{geng2024consistency} & 1.67 & &2 \\
    TCM~\cite{lee2024truncated} & 2.20 & &1 \\
    % TCM~\cite{lee2024truncated} & 1.62 & & 2 \\
    EMD~\cite{xie2024distillation} & 2.20 & 0.59 & 1 \\
    CTM~\cite{kim2024consistency} & 1.92  & 0.57 & 1\\
    Adversarial distillation & 1.88 & & 1\\
    SiD~\cite{zhou2024score} & 1.52  & 0.63 & 1 \\
    % GDD~\cite{zheng2024diffusion} & 1.42  & 0.59 & 1\\
    GDD-I~\cite{zheng2024diffusion} & 1.16  & 0.60 & 1\\
    & \\[-1.9ex]
    \cdashline{1-4}
    & \\[-1.9ex]
    \textbf{\method}\\
    reverse-KL~(DMD2~\cite{yin2024improved}) & 1.27  & 0.65 & 1\\
    forward-KL~(\textit{ours}) & 1.21 & 0.65  & 1\\
    JS~(\textit{ours}) & \bf{1.16}  & \bf{0.66} & 1\\
    \bottomrule
    \end{tabular}
    \caption{FID score, Recall and NFE on ImageNet-64.}
    \label{tab:imagenet-64}
    \vspace{-15pt}
\end{table}
\vspace{-10pt}
\paragraph{Variance.}
The variance of the weighting function $h$ in the final objective~(\Eqref{eq:obj-final}) is essential to training stability. We use the normalized variance $\Var_q\left(h(p/q)/\E_q[h(p/q)]\right)$ to characterize the variance of different $f$s, ensuring scale-invariant comparison. Fig.~\ref{fig:var} illustrates the normalized variance as a function of the mean difference between two 1D unit-variance Gaussians. The variance of the forward-KL divergence and the Jefferys 
%\av{did you mean Jefferys?} 
increases significantly as the distance between the Gaussians grows. In contrast, the Jensen-Shannon divergence and the squared Hellinger distance remain relatively stable. This stability contributes to the superior empirical performance of the low-variance Jensen-Shannon divergence in the experimental section.\looseness=-1

To address the high variance often observed in weighting functions for less mode-seeking divergences (see Fig.~\ref{fig:h}), we propose a two-stage normalization scheme. The first stage normalizes the time-dependent density ratio $r_t$, leveraging the fact the expectation of $r_t$ is $1$, \ie $\E_{q_t}[r_t]=1$. We discretize the time range into bins and normalize the values of $r_t$ within each bin by their average. The second stage directly normalizes the weighting function $h$ by its average value within each mini-batch. 
% This normalization is crucial because the training process involves both the \methodtext objective and the GAN objective. 
% It ensures scale-invariance for the weightings, as the scale can vary significantly for different $f$s. 
This maintains the relative importance of the \methodtext objective w.r.t the GAN objective, as the scale of weighting function can vary significantly for different $f$s.. Fig.~\ref{fig:var-loss} demonstrates that the loss exhibits a much smaller variance after the above normalization techniques on ImageNet-64 using the forward-KL divergence.  We provide an algorithm box in Alg~\ref{alg:f-distill}. \looseness=-1