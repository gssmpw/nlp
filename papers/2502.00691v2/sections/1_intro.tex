
\section{Introduction}\label{sec1}

Large Language Models (LLMs) have demonstrated remarkable performance across various domains~\citep{app, gpt4, llama3, gemini, yang2024qwen2}. Yet, solving complex mathematical problems still remains challenging, as the task requires hybrid skills in abstract reasoning, symbolic manipulation, and precise numerical computation~\citep{pal, mammoth, tora, htl}. 
Current approaches adopt two complementary paradigms: (1) \emph{chain-of-thought (CoT) reasoning}, which decomposes problems into intermediate reasoning steps~\citep{cot, metamath}, and (2) \emph{external tool integration}, where models generate code snippets to offload computations to interpreters or symbolic solvers~\citep{openmath, mammoth}. While CoT reasoning excels at semantic parsing and stepwise logic, its reliance on token-level autoregressive generation often propagates numerical errors. Conversely, tool-based approaches ensure computational precision but suffer from a semantic-to-symbolic translation gap, where even minor syntactic errors or contextual misinterpretations disrupt execution~\citep{htl}.

\input{figs/fig_autocode_analysis}
Recent hybrid frameworks like Mammoth~\citep{mammoth}, Deepseek-Math~\citep{tora, dsmath}, and Qwen-2.5-Math~\citep{qwen25} attempt to combine these paradigms through interleaved CoT-code reasoning. However, as our empirical analysis reveals (Fig.~\ref{fig_analysis}), current methods exhibit a critical rigidity: they either default to CoT reasoning unless explicitly prompted for code generation or adhere to static templates for tool invocation.  We trace this limitation to prevailing supervised fine-tuning (SFT) paradigms that condition models to (1) passively follow user instructions (e.g., "Let’s write a Python program"~\citep{mammoth}), (2) replicate fixed code-integration patterns from curated datasets~\citep{qwen25}, or (3) imitate teacher-forced tool-use trajectories~\citep{tora, dsmath}.  Consequently, LLMs lack \emph{metacognitive awareness} -- the capacity to dynamically evaluate their intrinsic capabilities against problem contexts and autonomously determine when and how to integrate tools. This deficiency motivates our central research question:

\vspace{0.1in}
\emph{How can mathematical LLMs learn autonomous code integration (AutoCode) that optimally complements their inherent reasoning capabilities?}
\vspace{0.1in}

Reinforcement learning (RL) offers a promising pathway by optimizing policies through self-generated trajectories, as evidenced by recent successes like DeepSeek R1~\citep{dsr1}. However, we empirically observe that standard RL methods is inefficient in learning autonomous code integration (\textit{AutoCode}) strategies (see Sec.~\ref{sec_ablation}). This stems from RL's tendency to exploit local policy neighborhoods, thereby insufficiently exploring the vast combinatorial space of potential CoT-code interleaving patterns. Such myopic exploration constrains the discovery of high-reward reasoning paths that judiciously blend both modalities, particularly as the model’s reasoning capabilities evolve during training.

To address this challenge, we propose a novel Expectation-Maximization (EM) framework that synergizes guided exploration with policy optimization. Our key innovation lies in formulating code-integration decisions as latent variables within an EM paradigm, creating a self-reinforcing cycle: the E-step identifies high-potential code-integration decisions through guided exploration, while the M-step optimizes policy parameters for joint metacognitive tool-usage and reasoning. 

This dual mechanism enables models to adapt tool-use strategies as their capabilities evolve during training. Practically, we achieve efficiency through two design choices: (1) an offline data curation step (E-step) that prioritizes high-return code invocation decisions through guided exploration, and (2) an off-policy RL optimization step (M-step) that jointly improves tool-usage and reasoning. This approach offers enhanced control and efficiency compared to standard RL, which is particularly beneficial for resource-constrained companies or researchers.

Extensive experiments demonstrate that our method (a) preserves higher training efficiency while achieving better performance, and (b) learns intelligent code integration strategies that achieves higher accuracy than either CoT or code prompted in isolation. Notably, our show consistent improvements across different benchmarks, raising MATH500 from 60.4\% to 71.4\%. 

Our contribution is summarized as follows:
(1) We diagnose a critical gap in mathematical LLM -- the inability to autonomously integrate tools based on metacognitive awareness -- and demonstrate standard RL’s inefficiency in addressing it.
(2) We propose a novel EM-based framework that jointly adapts the tool-usage strategies with evolving reasoning abilities, with a simple yet efficient implementation. 
(3) We demonstrate superior results in both training efficiency and accuracy on challenging benchmarks.

% We distill this objective into a central goal: developing \emph{autonomous code integration (AutoCode)} for math LLMs, as illustrated in Fig.~\ref{fig_introduction}. Existing tool-integrated math LLMs generally lack the AutoCode capability, because these models are trained to follow an externally imposed methodology that do not adapt to their unique strengths. Therefore, we ask: 

% \emph{How to enable LLMs to learn their own strategies to select the methodology for math queries, such that it complements the model's inherent capabilities?}

% Recent advancements focus on synergizing these paradigms by developing \emph{tool-integrated reasoning} frameworks, such as TORA~\citep{tora, dsmath}, HTL~\citep{htl} and DotaMath~\citep{dotamath}, which merge CoT reasoning with code generation to provide more fluid interactions between abstract reasoning and precise computation.  However, despite the potential benefits of incorporating external tools, their successful integration presupposes that the model is both adept at using the tool and capable of reasoning effectively about the problem with the code. 

% This assumption raises concerns about LLMs' capability boundaries of using tools, underscoring the need for more intelligent code integration strategies. Rather than a reactive decision, code integration should be a deliberate decision informed by the model's capabilities, mirroring human-like methodology-selection for problem-solving.

% We distill this objective into a central goal: developing \emph{autonomous code integration (AutoCode)} for math LLMs, as illustrated in Fig.~\ref{fig_introduction}. Existing tool-integrated math LLMs generally lack the AutoCode capability, because these models are trained to follow an externally imposed methodology that do not adapt to their unique strengths. Therefore, we ask: 

% \emph{How to enable LLMs to learn their own strategies to select the methodology for math queries, such that it complements the model's inherent capabilities?}

% To address this challenge, we introduce a novel Expectation-Maximization (EM) formulation that enables a self-teaching mechanism without relying on external dictations. Specifically, the Expectation step (E-step) computes a reference strategy through self-exploration, representing the belief over its capabilities. The maximization step (M-step) updates the LLM based on the data induced by the new belief. Furthermore, we propose an efficient joint training scheme for this framework, which features a novel data synthesis strategy for offline data curation and efficient off-policy reinforcement learning (RL). 

