
\section{Appendix}
% \subsection{Performance Table}

\subsection{Derivation of the EM}\label{app_em}
We first model RL as a maximum likelihood problem. Specifically, we consider `maximizing expected return' as `maximizing the likelihood of observing a correct response`, this is equivalent to maximizing the following log-likelihood,
\begin{align*}
    &\max_\theta \log P(r=1 | x_q;\theta) \\
    &= \max_\theta \log \sum_{c} p_\theta(c | x_q)\sum_{y_a}p_\theta(y_a | x_q,c)\\&\cdot P(r=1 | y_a,c,x_q) \\
    &\doteq \mathcal{J}(\theta),
    % \\
    % &= \max_\theta \log \sum_{c} p_\theta(c | x_q)\sum_{y_a}p_\theta(y_a | x_q,c)\cdot P(r=1 | y_a,x_q).
\end{align*}
where we factorize the language model as \( p_\theta(y_a | x_q) = \sum_c p_\theta(c | x_q) p_\theta (y_a | x_q, c) \).

% Here we define \( P(r=1 | y_a,x_q) \propto R(x_q, y_a) \), which intuitively says that responses with higher reward has higher likelihood of being correct. 

% We aim to empower the model to decide on its own a methodology \( c \) before generating the solution, so we factorize the language model as \( p_\theta(y_a | x_q) = \sum_c p_\theta(c | x_q) p_\theta (y_a | x_q, c) \). 
Since the variable \( c \) is unobservable for lack of reliable supervision, we resort to the EM framework. Specifically,  we treat \( c \) as a hidden variable, and introduce a proposal distribution \(s(c | x_q) \), which represents the belief of \( c \) given the fixed LLM \( \mathcal{M}_\theta \) and the data evidence. We have the following derivations, \( \mathcal{J}(\theta) 
 =\)
\begin{align*}
    % &= \log \sum_{c} p_\theta(c | x_q)\sum_{y_a}p_\theta(y_a | x_q,c)\cdot P(r=1 | y_a,c,x_q)\\
    &\log \sum_c s(c | x_q)\cdot \frac{p_\theta(\cdot | x_q)}{s(\cdot | x_q)}\\&\cdot
    % P(r=1 | x_q,c;\theta)
    \sum_{y_a}p_\theta(y_a | x_q,c)\cdot P(r=1 | y_a,c,x_q) 
    \\
    % &=\log \E_{s(c | x_q)}  \frac{p_\theta(\cdot | x_q)}{c(\cdot | x_q)}\cdot\E_{p_\theta(y_a | x_q,c)}[P(r=1 | y_a,x_q)]
    &\ge \sum_c s(c | x_q) \\&\left[ \log  
    \frac{p_\theta(c | x_q)
    \sum_{y_a}p_\theta(y_a | x_q,c) P(r=1 | y_a,c,x_q) 
    }{s(c | x_q)}
    \right]\\
    &=\E_{s(c | x_q)}\left[
    \log \frac{p_\theta(c | x_q)\cdot P(r=1 | c,x_q;\theta)}{s(c | x_q)}
    \right]\\
    &\doteq \mathcal{J}_{\mathrm{ELBO}}(s,\theta),
    %\\
    % &=-\mathrm{D_{KL}}\left(s(c | x_q)\ |  p_\theta(c | x_q)\right)+\E_{s(c | x_q)}\left[
    % \log P(r=1 | x_q,c;\theta)
    % \right]
\end{align*}
where \(P(r=1 | c,x_q;\theta)\) denotes the likelihood of generating correct responses given \((x_q,c)\) following the solution-generation policy \( p_\theta(y_a | x_q,c) \). 

In the E-step, we are essentially minimizing a KL divergence,
\begin{align}
    \max_s \mathcal{J}_{\mathrm{ELBO}}(s,\theta)
    = \min_s \mathrm{D_{KL}}\left(s(c | x_q)\ | s^\ast(c | x_q)\right),
\end{align}
where the minimizer is \(s^\ast(c | x_q) \propto p_\theta(c | x_q)\cdot
P(r=1 | c,x_q;\theta)\). Intuitively, the belief over the methodology \( c \) following a posterior update: it is based on the prior of the current model \( p_\theta(c | x_q) \) and the likelihood of data \( P(r=1 | c,x_q;\theta) \). The optimal methodology-selection strategy assigns higher probability to method \( c \) if following the current LLM it has higher likelihood to generate correct responses or higher prior over it. 

To compute the optimal strategy, we define \( P(r=1 | c,x_q;\theta) = \frac{\exp(\alpha\cdot p_\theta(c|x_q) Q^\theta(x_q,c))}{Z(x_q)}\) as an energy-based distribution, where the negative energy function evaluates the expected return induced by the current solution-generation policy: \( Q(x_q,c;\theta) =  \E_{ p_\theta(y_a | x_q,c)}[R(x_q,y_a)]\), \(\alpha>0\) is a temperature controlling the sharpness of the distribution.  Then the minimizer can be computed by enumerating over \( c \). 


In the M-step, we optimize \( \max_\theta \mathcal{J}_{\mathrm{ELBO}}(s,\theta) = \)
\begin{align*}
    & \max_\theta \E_{s(c | x_q)}[\log P(r=1 | c,x_q;\theta)]\\&  -\mathrm{D_{KL}}\left(s(c | x_q)\ | p_\theta(c | x_q)\right)\\
    &= \max_\theta \E_{s(c | x_q)}[Q(x_q,c;\theta)]-\mathrm{D_{KL}}\left(s(c | x_q)\ | p_\theta(c | x_q)\right)
\end{align*}
which maximizes the expected return while imitating the optimal strategy.

% \subsection{Standard EM}
% \begin{align}
%     &\log p(O=1|x) \\
%     &= \log \sum_y p_\theta(y|x)  p(O=1|x,y) \\
%     &=\log \sum_y q(y | x)\cdot \frac{p_\theta(y | x)}{q(y | x)}\cdot
%     p(O=1|x,y)
%     \\
%     &\ge \sum_y q(y | x) \left[ \log  
%     \frac{p_\theta(y | x)\cdot
%      p(O=1 | x,y) 
%     }{q(y | x)}
%     \right]\quad \text{(Jensen's Inequality)}\\
%     % &=\E_{s(c | x_q)}\left[
%     % \log \frac{p_\theta(c | x_q)\cdot P(r=1 | c,x_q;\theta)}{s(c | x_q)}
%     % \right]\\
%     &\doteq \mathcal{J}_{\mathrm{ELBO}}(q,\theta)\\
%     &=-\mathrm{D_{KL}}\left(~q(y|x)~\|~p_\theta(y|x) p(O=1|x,y)~\right)\\
%     &=\E_{q(y|x)}[\log p(O=1|x,y) ] - \mathrm{D_{KL}}\left(~q(y|x)~\|~p_\theta(y|x) ~\right)
% \end{align}
% \begin{align}
%     &\max_q \mathcal{J}_{\mathrm{ELBO}}(q,\theta)\\
%     &=\max_q \sum_y q(y | x) \left[ \log  
%     \frac{p_\theta(y | x)\cdot
%      p(O=1 | x,y) 
%     }{q(y | x)}
%     \right]\\
%     &=\min_q \mathrm{D_{KL}}\left(~q(y|x)~\|~p_\theta(y|x) p(O=1|x,y)~\right)\\
%     &\Rightarrow{} q(y|x)\propto p_\theta(y|x) p(O=1|x,y)
% \end{align}

% \begin{align}
%     &\max_\theta \mathcal{J}_{\mathrm{ELBO}}(q,\theta)\\
%     &=\max_\theta \sum_y q(y | x) \left[ \log  
%     \frac{p_\theta(y | x)\cdot
%      p(O=1 | x,y) 
%     }{q(y | x)}
%     \right]\\
%     &=\max_\theta \sum_y q(y | x)\log p_\theta (y|x)\\
%     &=\max_\theta \sum_y \left[p_{\bar{\theta}}(y|x) R(x,y)\right] \log p_\theta (y|x) 
% \end{align}
% \section{The EM Algorithm Diagram}\label{sec_alg}
% \begin{algorithm}[h]
%     \caption{AutoCode4Math EM Learning}
%     \begin{algorithmic}[1]
%         \State \textbf{Input:} Query set \( \mathcal{Q}  \)
%         \State \textbf{Output:} Parameters \( \theta \)
        
%         \State Initialize parameters from base model \( \theta^{(0)} \)
%         \State \( t \gets 0 \)

%         \Repeat
%             \State // \textbf{E-step: Find the reference strategy \( s^\ast(c|x_q) \)} 
%             \State Perform \( K \) random rollouts for each query to collect the query-methodology-responses dataset \( \ds_{\mathrm{Dictated}} \) and \( \ds_{\mathrm{Native}} \). Store the outcome \( r \), and the log-likelihoods. 
%             \State Using \( \ds_{\mathrm{Dictated}} \), compute the Q-values for  \( x_q \in \mathcal{Q}, c\in\{0,1\} \) according to \eqref{eq_qvalue}.
%             \State Take hard-max or sample from the soft-max of the Q-values to obtain \( c^\ast \sim s(c|x_q) \).
%             \State Synthesize complete trajectories to obtain \(\ds\), using self-exploratory and self-reflective synthesis, based on the reference selection \(c^\ast\) for each query, \( \ds_{\mathrm{Dictated}} \) and \( \ds_{\mathrm{Native}} \). 
            
%             \State // \textbf{M-step: Update the LLM \(p_\theta \)} 
%             \State Optimize the LLM according to \eqref{eq_finalm}, using the dataset \(\ds \).

%             \State \( t \gets t + 1 \)
%         \Until convergence 

%         \Return \( \theta^{(t)} \)
%     \end{algorithmic}
% \end{algorithm}



% \section{Additional Experiments}
% We aim to answer the following research questions:

% \emph{Q4: Effectiveness in Autonomous Code Integration. We evaluate how our models and baselines perform when required to perform autonomous code integration.  Specifically, we investigate the effectiveness of their methodology-selection and its relation to the final performance.}

% % \subsection{Analysis on the Effectiveness in Autonomous Code Integration}
% \textbf{Baselines.} We compare our approach with several models that
% natively support both code and Chain-of-Thought (CoT) responses for math
% queries: GPT-4, Mammoth-70B trained using Hybrid Instruction Tuning, and
% DeepseekMath-Instruct-7B trained using tool-integrated reasoning
% annotations.

% \textbf{Evaluation Metrics.}

% \begin{itemize}[leftmargin=0.5cm]
% \item
%   \textbf{Final Pass@1 Accuracy of the Complete Response}

%   \begin{itemize}[leftmargin=0.5cm]
  
%   \item
%     ``Autocode'': The LLM autonomously decides the methodology. For
%     baselines without AutoCode Training, a four-shot prompt template
%     (Appendix D.3) is used.
%   \item
%     ``Code'': The LLM is explicitly prompted to generate a code
%     response. For GPT-4, a four-shot template is applied. For other
%     baselines, we use their native zero-shot templates.
%   \item
%     ``CoT'': The LLM is explicitly prompted to use CoT reasoning with
%     native templates.
%   \end{itemize}
% \item
%   \textbf{Autocode CodeRate and Improvement Over Best Dictation}

%   \begin{itemize}[leftmargin=0.5cm]
%   \item
%     \textbf{CodeRate} reflects the reduction in code usage compared to
%     dictated code prompting.
%   \item
%     \textbf{Accuracy Improvement over best performance} of either CoT or
%     code dictation reflects the improvement of AutoCode over the native
%     dictated inference.
%   \end{itemize}
% \item
%   \textbf{Methodology-Selection Accuracy and Its Connection to Final
%   Accuracy}

%   \begin{itemize}
  
%   \item
%     Ground-Truth Labels: Methodology-selection is treated as a binary
%     classification task. The classification label is derived by
%     performing 10 Monte Carlo rollouts per query with controlled
%     methodologies. The optimal methodology is chosen based on higher
%     expected correctness.
%   \item
%     Imbalanced Classification: Since model capabilities differ,
%     reference decisions are imbalanced (e.g., GPT-4 strongly prefers
%     CoT, with only 7.45\% of queries requiring coding). We report
%     \textbf{mean accuracy across CoT-preferred and code-preferred
%     queries} (Selection mAcc), along with per-class accuracy (in the
%     last 3 and 4 columns).
%   \item
%     Connection to Final Accuracy: Correct methodology-selection does not
%     always guarantee a correct response due to prompt context mismatches
%     and greedy decoding. We report \textbf{Pass@1 accuracy within
%     correct selections} to evaluate how proper methodology-selection
%     directly contributes to correct responses.
%   \end{itemize}
% \end{itemize}
% \input{tables/analysis_autocode}

% As shown in Tab.~\ref{table_autocode_analysis}:

% \textbf{Effectiveness in Methodology-Selection.} AutoCode Training
%   significantly improves methodology-selection accuracy, outperforming
%   baseline models by over 20\% (compare row 3 with row 5). In
%   particular, GPT-4 exhibits low mean accuracy (50.51\%) due to its
%   strong bias for CoT. If GPT-4 intelligently selected methodologies, it
%   could achieve an additional 7.62\% gain in accuracy for queries
%   requiring code responses.
  
% \textbf{Accuracy Improvements and Code Reduction.} AutoCode Training
%   enables our Deepseek-based model to achieve a 5.04\% accuracy
%   improvement compared to the best dictated inference (code prompting)
%   while reducing code usage by up to 66\%. Similar trends are observed
%   with the Qwen2Math base model. In contrast, baseline models experience
%   substantial accuracy drops when attempting autonomous
%   methodology-selection. For example, DeepseekMath loses 11.54\%
%   accuracy when self-selecting methodologies (row 3).
  
% \textbf{Connection Between Methodology-Selection and Final Accuracy.}
%   Baseline models surpass random selection (50\%) in
%   methodology-selection accuracy but fail to improve final accuracy over
%   best dictation. This is accredited to \textbf{the gap between
%   methodology-selection and final correctness}\emph{: better
%   methodology-selection is not always accompanied with correct final
%   response, due to the greedy decoding and mismatch in prompt context.
%   However, }\textbf{our proposed EM-based joint training significantly
%   bridge this gap}: it improves Pass@1 accuracy within correct
%   selections, achieving rates as high as 95\% (rows 4 and 5). This
%   success is due to training the LLM with complete responses, optimizing
%   both methodology-selection and final correctness jointly.
  


\section{Statistics and Analysis}
% We analyze the AutoCode capabilities of the existing tool-integrated LLMs, GPT-4o and DeepseekMath in Tab.~\ref{tab_autocode}. We prompt AutoCode capbilities using the instruction shown in Sec.~\ref{list_autocode_prompt}. Both models show clear preference toward using CoT reasoning, with less than 10\% code executions. It is also counter-intuitive at first sight why using very small portion of code executions can result in significant performance degradation -- in the case of GPT-4o, only 1\% code executions result in 6\% decrease in accuracy. We conjecture that this owes to the sensitivity to the AutoCode few-shot prompting. Rarely seeing such demonstrations make the models prone generate wrong responses, compared to standard clean few-shot prompting. 

We list the collected public SFT data in Tab.~\ref{tab_sft_data}. 

% We show the distribution of multi-round responses of the model AutoCode4Math-Deepseek in Tab.~\ref{tab_multiround}.


% \begin{table*}[ht]
% \caption{\small \textbf{Analysis of existing tool-integrated math LLMs.} We find that two state-of-the-art tool-integrated system, GPT-4o and DeepseekMath, struggle with AutoCode. When given few-shot demonstrations for choosing between CoT and code to solve given math queries, they show clear preference of using CoT (less than 10\% code executions). This reveals that these model cannot decide on their own what methodologies to choose for a given math query. In addition, we observe that there is significant performance degradations when prompted for AutoCode: only 1\% code executions lead to 6\% accuracy decrease in the case of GPT-4o. This reflects that their performance are sensitive to the given few-shot prompting. }
% \centering
% \tiny
% \resizebox{\linewidth}{!}{%
% \begin{tabular}{ccccc}
% \toprule
% Model                &  CoT (Acc/Code Rate)     & Code (Acc/Code Rate)  & AutoCode-4shots (Acc/Code Rate)             \\
% \midrule
% GPT-4o &  77.04 / 0 & 57.34 / 97.58 & 71.54 / 1.24 \\
% Deepseek-Math-Instruct & 46.92 / 0 & 46.26 / 99.92 & 41.46 / 5.18 \\
% Mammoth-Mistral-7B & 19.18 / 0 & 40.2 / 99.38 & 37.58 / 95.5 \\
% Mammoth-7B & 10.7 / 0 & 29.1 / 99.96 & 23.86 / 95.4 \\
% Mammoth-13B & 12.7 / 0 & 31.4 / 98.96 & 22.54 / 60.68 \\
% Mammoth-70B & 20.6 / 0.02 & 40.58 / 99.62 & 33.4 / 83.66 \\
% \hline
% \end{tabular}
% }

% \label{tab_autocode}
% \end{table*}


\begin{table*}[ht]
\caption{\small \textbf{Public SFT Data Used in this Work.} We collect public query set for AutoCode Training. After Deduplication, the total amount of query used is \(119\)K. If the base model is not trained to write code for math problems, we use the SFT annotations associated with the above queries. }
\centering
\tiny
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccccc}
\toprule
\textbf{Dataset}                                     & \textbf{Size}  & \textbf{Unique Queries}     & \textbf{CoT Solutions}     & \textbf{Code Solutions}          \\
\midrule
Openmath~\cite{openmath}                      & 129917	&70002 & 25116	 & 104801                \\
Math-Instruct~\citep{mammoth}              &  237781   & 219607  & 188644     &  49137             \\
Metamath~\cite{mammoth}                  & 285000    & 161337   &285000       &    0                 \\ 
MMOS~\citep{mmos}                      & 134610    &69007    &   0     & 134610 \\
\hline
\end{tabular}
}

\label{tab_sft_data}
\end{table*}

\begin{table*}[ht]
\caption{\small \textbf{Distribution of Multi-Round Responses.}}
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{cccccccc}
\toprule
Model                & Dataset  & Queries      & Round1     & Round2 & Round3      \\
\midrule
Deepseek-math-AutoCode    
% & Math  &5000  & CoT Trigger       & 4992       & 0         & 7  & 1               \\
%                          &  &5000 & PoT Prompt       & 0         & 4971         & 0  & 29  \\
                         & MATH &5000       & 1252                & 3719  & 29  \\
                         % & Gsm8k 
                         % & 1319 & CoT Trigger       & 1318         & 0         & 0  & 1               \\
                         % &  & 1319 & PoT Prompt       & 0         & 1309         & 0  & 10  \\
                         & GSM8k &1319       & 1013                & 304  & 2  \\
                         
\hline
\end{tabular}
}\label{tab_multiround}

\end{table*}



\section{Examples}
\label{sec:appendix:examples}

\subsection{Success Cases}
\label{sec:appendix:examples:success}

\lstinputlisting[caption={Success case for multi-round.}]{case/correct.md}
\vspace{10pt}
\noindent\rule{\textwidth}{0.4pt} 
\subsection{Failure Cases}
\label{sec:appendix:examples:fail}

\lstinputlisting[caption={Failure case for multi-round.}]{case/error.md}

\vspace{10pt}
\noindent\rule{\textwidth}{0.4pt} 
\subsection{AutoCode 4shots}\label{list_autocode_prompt}
\lstinputlisting[caption={Autocode 4shots example.}]{case/4shots.md}