\section{Background}
% \begin{tcolorbox}[simplebox]
% We first formally define the problem and highlight its challenge. 
% Then we present an EM approach to address this challenge. 
% \end{tcolorbox}
% \vspace{-0.3cm}
% \subsection{Problem Statement }\label{sec_ps}

% Here’s a polished and enriched version of your problem formulation section, with improved clarity, precision, and academic tone:

% ---
\input{figs/fig_example}
\textbf{Problem Statement.} Modern tool-augmented language models address mathematical problems \( x_q \in \mathcal{X}_Q \) by generating step-by-step solutions that interleave natural language reasoning with executable Python code (Fig.~\ref{fig_example}). Formally, given a problem \( x_q \), a model \( \mathcal{M}_\theta \) iteratively constructs a solution \( y_a = \{y_1, \dots, y_T\} \) by sampling components \( y_t \sim p(y_t | y_{<t}, x_q) \), where \( y_{<t} \) encompasses both prior reasoning steps, code snippets and execution results \( \mathbf{e}_t \) from a Python interpreter. The process terminates upon generating an end token, and the solution is evaluated via a binary reward \( r(y_a,x_q) = \mathbb{I}(y_a \equiv y^*) \) indicating equivalence to the ground truth \( y^* \). The learning objective is formulated as:
\[
\max_{\theta} \mathbb{E}_{x_q \sim \mathcal{X}_Q} \left[r(y_a, x_q) \right]
\]

\noindent\textbf{Challenge and Motivation.} Developing autonomous code integration (AutoCode) strategies poses unique challenges, as optimal tool-usage behaviors must dynamically adapt to a model's intrinsic capabilities and problem-solving contexts. While traditional supervised fine-tuning (SFT) relies on imitation learning from expert demonstrations, this paradigm fundamentally limits the emergence of self-directed tool-usage strategies. Unfortunately, current math LLMs predominantly employ SFT to orchestrate tool integration~\citep{mammoth, tora, dsmath, htl}, their rigid adherence to predefined reasoning templates therefore struggles with the dynamic interplay between a model’s evolving problem-solving competencies and the adaptive tool-usage strategies required for diverse mathematical contexts.

Reinforcement learning (RL) offers a promising alternative by enabling trial-and-error discovery of autonomous behaviors. Recent work like DeepSeek-R1~\citep{dsr1} demonstrates RL's potential to enhance reasoning without expert demonstrations. However, we observe that standard RL methods (e.g., PPO~\cite{ppo}) suffer from a critical inefficiency (see Sec.~\ref{sec_ablation}): Their tendency to exploit local policy neighborhoods leads to insufficient exploration of the vast combinatorial space of code-integrated reasoning paths, especially when only given a terminal reward in mathematical problem-solving.

To bridge this gap, we draw inspiration from human metacognition -- the iterative process where learners refine tool-use strategies through deliberate exploration, outcome analysis, and belief updates. A novice might initially attempt manual root-finding via algebraic methods, observe computational bottlenecks or inaccuracies, and therefore prompting the usage of calculators. Through systematic reflection on these experiences, they internalize the contextual efficacy of external tools, gradually forming stable heuristics that balance reasoning with judicious tool invocation. 


To this end, \emph{our focus diverges from standard agentic tool-use frameworks~\citep{agentr}}, which merely prioritize successful tool execution. Instead, \emph{we aim to instill \emph{human-like metacognition} in LLMs, enabling them to (1) determine tool-usage based on their own capability boundaries (see the analysis in Sec.~\ref{sec_ablation}), and (2) dynamically adapt tool-usage strategies as their reasoning abilities evolve (via our EM framework).}
% For instance, while an LLM might solve a combinatorics problem via CoT alone, it should autonomously invoke code for eigenvalue calculations in linear algebra where symbolic computations are error-prone. Achieving this requires models to \emph{jointly optimize} their reasoning and tool-integration policies in a mutually reinforcing manner.


% Mirroring this metacognitive cycle, we propose an Expectation-Maximization (EM) framework that allows LLMs to develop AutoCode strategies via guided exploration (the E-step) and self-refinement (the M-step).


% \vspace{-0.3cm}
\section{Methodology}

Inspired by human metacognitive processes, we introduce an Expectation-Maximization (EM) framework that trains LLMs for autonomous code integration (AutoCode) through alternations (Fig.~\ref{fig_overview}):

\begin{enumerate}[leftmargin=0.5cm,topsep=1pt,itemsep=0pt,parsep=0pt]
    \item \emph{Guided Exploration (E-step):} Identifies high-potential code-integrated solutions by systematically probing the model's inherent capabilities.
\item \emph{Self-Refinement (M-step):} Optimizes the model's tool-usage strategy and chain-of-thought reasoning using curated trajectories from the E-step.
\end{enumerate}

\input{figs/fig_method_overview}

\subsection{The EM Framework for AutoCode}

A central challenge in AutoCode lies in the code triggering decisions, represented by the binary decision \(c \in \{0, 1\}\).  While supervised fine-tuning (SFT) suffers from missing ground truth for these decisions, standard reinforcement learning (RL) struggles with the combinatorial explosion of code-integrated reasoning paths. Our innovation bridges these approaches through systematic exploration of both code-enabled (\(c=1\)) and non-code (\(c=0\)) solution paths, constructing reference decisions for policy optimization.

We formalize this idea within a maximum likelihood estimation (MLE) framework. Let \( P (r=1 | x_q;\theta\) denote the probability of generating a correct response to query \( x_q \) under model \(\mathcal{M}_\theta\). Our objective becomes:
\begin{align}
    \mathcal{J}_{\mathrm{MLE}}(\theta) \doteq \log P(r=1 | x_q; \theta) \label{eq_mle}
\end{align}
This likelihood depends on two latent factors: (1) the code triggering decision \(\pi_\theta(c | x_q)\) and (2) the solution generation process \(\pi_\theta(y_a | x_q, c)\). Here, for notation-wise clarity, we consider  code-triggering decision at a solution's beginning (\( c\) following \(x_q\) immediately). We show generalization to mid-reasoning code integration in Sec.~\ref{sec_impl}.

The EM framework provides a principled way to optimize this MLE objective in the presence of latent variables~\cite{prml}. We derive the evidence lower bound (ELBO): \( \mathcal{J}_{\mathrm{ELBO}}(s, \theta) \doteq \)
\begin{align}
    % \mathcal{J}_{\mathrm{MLE}}(\theta) &
    % \ge 
    \mathbb{E}_{s(c | x_q)}\left[\log \frac{\pi_\theta(c | x_q) \cdot P(r=1 | c, x_q; \theta)}{s(c | x_q)}\right] 
    % \\
     \label{eq_elbo}
\end{align}
where \(s(c | x_q)\) serves as a surrogate distribution approximating optimal code triggering strategies. It is also considered as the reference decisions for code integration. 

\noindent\textbf{E-step: Guided Exploration}  computes the reference strategy \(s(c | x_q)\) by maximizing the ELBO, equivalent to minimizing the KL-divergence: \( \max_s \mathcal{J}_{\mathrm{ELBO}}(s, \theta) = \)
\begin{align}
     - \mathrm{D_{KL}}\left(s(c | x_q) \| P(r=1, c | x_q; \theta)\right) \label{eq_estep}
\end{align}

The reference strategy \(s(c | x_q)\) thus approximates the posterior distribution over code-triggering decisions \(c\) that maximize correctness, i.e., \(P(r=1, c | x_q; \theta)\).  Intuitively, it guides exploration by prioritizing decisions with high potential: if decision \(c\) is more likely to lead to correct solutions, the reference strategy assigns higher probability mass to it, providing guidance for the subsequent RL procedure.

\noindent\textbf{M-step: Self-Refinement } updates the model parameters \(\theta\) through a composite objective:
\begin{multline}
\max_\theta \mathcal{J}_{\mathrm{ELBO}}(s, \theta) =\mathbb{E}_{\substack{c \sim s(c|x_q) \\ y_a \sim \pi_\theta(y_a|x_q, c)}} \Big[ r(x_q, y_a) \Big] \\- \mathcal{CE}\Big(s(c|x_q) \,\|\, \pi_\theta(c|x_q)\Big)\label{eq_mstep}
\end{multline}
The first term implements reward-maximizing policy gradient updates for solution generation, while while the second aligns native code triggering with reference strategies through cross-entropy minimization (see Fig.~\ref{fig_overview} for an illustration of the optimization). This dual optimization jointly enhances both tool-usage policies and reasoning capabilities.



\subsection{Practical Implementation}\label{sec_impl}
In the above EM framework, we alternate between finding a reference strategy \( s \) for code-triggering decisions  in the E-step, and perform reinforcement learning under the guidance from \( s \) in the M-step. We implement this framework through an iterative process of offline data curation and off-policy RL.

\noindent\textbf{Offline Data Curation.} We implement the E-step through Monte Carlo rollouts and subsampling. For each problem \(x_q\), we estimate the reference strategy as an energy distribution: 
\begin{equation}
    s^\ast(c | x_q)  = \frac{\exp\left(\alpha\cdot \pi_\theta(c | x_q) Q(x_q,c;\theta)\right)}{Z(x_q)}.\label{eq_sampling}
\end{equation}
where \( Q(x_q,c;\theta)\) estimates the expected value through \( K \) rollouts per decision, \(\pi_\theta(c|x_q) \) represents the model's current prior and the \( Z(x_q) \) is the partition function to ensure normalization. Intuitively, the strategy will assign higher probability mass to the decision \( c \) that has higher expected value \( Q(x_q,c;\theta)\) meanwhile balancing its intrinsic preference \( \pi_\theta(c|x_q)\). 

Our curation pipeline proceeds through: 
\begin{itemize}[leftmargin=0.5cm,topsep=1pt,itemsep=0pt,parsep=0pt]
\item Generate \(K\) rollouts for \(c=0\) (pure reasoning) and \(c=1\) (code integration), creating candidate dataset \(\mathcal{D}\).  
\item Compute \(Q(x_q,c)\) as the expected success rate across rollouts for each pair \((x_q,c)\).  
\item Subsample \(\mathcal{D}_{\text{train}}\) from \(\mathcal{D}\) using importance weights according to Eq.~\ref{eq_sampling}.  
\end{itemize}

To explicitly probe code-integrated solutions, we employ prefix-guided generation -- e.g., prepending prompts like \texttt{``Let’s first analyze the problem, then consider if python code could help''} -- to bias generations toward free-form code-reasoning patterns.

 This pipeline enables guided exploration by focusing on high-potential code-integrated trajectories identified by the reference strategy, contrasting with standard RL’s reliance on local policy neighborhoods. As demonstrated in Sec.~\ref{sec_ablation}, this strategic data curation significantly improves training efficiency by shaping the exploration space.





\noindent\textbf{Off-Policy RL.}
To mitigate distributional shifts caused by mismatches between offline data and the policy, we optimize a clipped off-policy RL objective. The refined M-step (Eq.~\ref{eq_mstep}) becomes:
\begin{multline}
    % \max_\theta 
    \underset{(x_q,y_a)}{\mathbb{E}}\left[
\text{clip}\left(\frac{\pi_\theta(y_a|x_q)}{\pi_{\text{ref}}(y_a|x_q)},1-\epsilon,1+\epsilon\right)\cdot A\right]
\\-\mathbb{E}_{(x_q,c)}\Big[\log \pi_\theta(c|x_q) \Big]\label{eq_finalm}
\end{multline}
where  \( (x_q, c, y_a) \) is sampled from the dataset \( \mathcal{D}_{\text{train}} \). The importance weight \(\frac{\pi_\theta(y_a|x_q)}{\pi_{\text{ref}}(y_a|x_q)}\) accounts for off-policy correction with PPO-like clipping. The advantage function \(A(x_q,y_a)\) is computed via query-wise reward normalization~\cite{ppo}. 

\noindent\textbf{Generalizing to Mid-Reasoning Code Integration.} Our method extends to mid-reasoning code integration by initiating Monte Carlo rollouts from partial solutions \((x_q, y_{<t})\). Notably, we observe emergence of mid-reasoning code triggers after initial warm-up with prefix-probed solutions. Thus, our implementation requires only two initial probing strategies: explicit prefix prompting for code integration and vanilla generation for pure reasoning, which jointly seed diverse mid-reasoning code usage in later iterations.
