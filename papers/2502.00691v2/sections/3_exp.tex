
\section{Experiments}\label{sec_exp}
\vspace{-0.2cm}
Our experiments investigate three key research questions:

\noindent\emph{Q1: Method Effectiveness.} How does our approach enhance performance across both in-domain and out-of-domain mathematical benchmarks compared to existing math LLMs?

\noindent\emph{Q2: Baseline Comparisons.} How does our method compare to standard RL and SFT baselines in terms of training efficiency and exploration patterns?

\noindent\emph{Q3: AutoCode Analysis.} What strategies does the model learn for code integration, and how do these strategies contribute to performance gains?

\noindent\textbf{Datasets and Benchmarks.} Our method only requires a query set for training. We collect public available queries from MATH~\citep{math} and Numina~\cite{numina}, and sample \(7K\) queries based on difficulties. We upload the collected data to the annonymous repo. For evaluation, we employ: GSM8k~\citep{gsm8k}, MATH500~\citep{math}, GaokaoMath2023~\citep{mario}, OlympiadBench~\citep{olympiad}, the American Invitational Mathematics Examination (AIME24), and the American
Mathematics Competitions (AMC23). This benchmark suite spans elementary to Olympiad-level mathematics. We adopt Pass@1 accuracy~\citep{pass1, dsr1} as our primary metric, using evaluation scripts from DeepseekMath~\citep{dsmath} and Qwen2Math~\citep{yang2024qwen2}. For competition-level benchmarks (AIME/AMC), we use 64 samples with temperature 0.6 following Deepseek R1 protocols.

\input{tables/main_results}
\noindent\textbf{Baselines and Implementation.} 
We compare against three model categories: \begin{itemize}[leftmargin=0.5cm,itemsep=0pt,parsep=0pt]
\item Proprietary models: o1~\cite{o1}, GPT-4~\citep{gpt4} and Claude~\citep{claude}
\item Recent math-specialized LMs: NuminaMath~\citep{numina}, Mathstral~\citep{mathstral}, Mammoth~\citep{mammoth}, ToRA~\citep{tora}, DartMath~\cite{tong2024dartmath}. We do not compare with models that rely on test-time scaling, such as MCTS or long CoT. 
\item Foundation models enhanced with our method: Qwen2Math~\citep{yang2024qwen2}, DeepseekMath~\citep{dsmath} and Qwen-2.5~\cite{qwen25}.
\end{itemize}

Our implementation uses \( K = 8 \) rollouts per query (temperature=1.0, top-p=0.9). Training completes in about 10 hours on \(8\times\) A100 (80GB) GPUs across three epochs of 7K queries. We release code, models and data via an \href{https://anonymous.4open.science/r/AnnonySubmission-0C62}{anonymous repository}.

% \vspace{-0.1cm}
\subsection{Main Results}\label{sec_main}
Notably, we observe a minimum performance gain of 11\% on the MATH500 benchmark, escalating to an impressive 9.4\% absolute improvement on the highly challenging AIME benchmark.  Across in-domain benchmarks, our method yields an average improvement of 8.9\%, and for out-of-domain benchmarks, we achieve a substantial average gain of 6.98\%. These results  validate the effectiveness of our approach across model families and problem difficulty levels.  

\subsection{Ablation Study}\label{sec_ablation}
We conduct three primary analyses: (a) comparison with standard RL and SFT baselines to validate our method's effectiveness in facilitating exploration, (b) visualization of exploration patterns to reveal limitations in the standard RL paradigam, and (c) behavioral analysis of code integration strategies. These analyses collectively demonstrate our method's benefits in facilitating guided exploration and explains how it improves performance.

\input{figs/fig_training_efficiency}
\noindent\textbf{Training Efficiency.} We evaluated the learning dynamics of our approach in direct comparison to three established training paradigms:
\begin{itemize}[leftmargin=0.5cm,itemsep=0pt,parsep=0pt]
\item \emph{Base+RL}:  On-policy Reinforcement Learning (RL) initialized from a base model without Supervised Fine-Tuning (SFT). This follows the methodology of DeepSeek R1, designed to isolate and assess the pure effects of RL training.
\item \emph{SFT}: Supervised Fine-Tuning, the prevailing training paradigm widely adopted in current tool-integrated math Language Models (LMs).
\item \emph{SFT+RL}: Standard RL applied after SFT, serving as a conventional baseline for evaluating our EM-based RL method.
\end{itemize}

From the figure, we make the following key observations: 


\begin{itemize}[leftmargin=0.5cm,itemsep=0pt,parsep=0pt]
   \item  While Reinforcement Learning directly from the base model (\emph{Base+RL}) exhibits consistent performance improvement, its training efficiency is lower than training paradigms incorporating SFT.  In addition, the model rarely explores code-integrated solutions, with the code invocation rate below 5\%. This strongly suggest that \emph{reinforcement learning tool-usage behavior from scratch is inherently inefficient}.
    \item SFT effectively provides a strong initialization point, but \emph{SFT alone exhibits limited asymptotic performance}. This suggests that SFT lacks the capacity to adapt and optimize beyond the scope of the expert demonstrations, thereby limiting further improvement. 
    \item Standard RL applied after SFT shows initial further improvement but subsequently plateaus, \emph{even after an extended training stage}.  This suggests \emph{the exploration-exploitation dilemma when applying RL for LLM post-training}: standard RL with vanilla rollout exploration tends to exploit local optima and insufficiently explores the combinatorial code-integrated trajectories.
\end{itemize}

To further substantiate the exploration limitations inherent in the conventional \emph{SFT+RL} paradigm, we present a visualization of the exploration patterns. We partitioned the model-generated responses during self-exploration into three distinct training phases and analyzed the statistical distribution of code invocation rates across queries as the model's policy evolved throughout training. As depicted in Figure~\ref{fig_visualize_explore}, the distribution of code invocation progressively concentrates towards the extremes – either minimal or maximal code use – indicating the model's growing tendency to exploit its local policy neighborhood. This exploitation manifests as a focus on refining established code-triggering decisions, rather than engaging in broader exploration of alternative approaches.


\input{figs/fig_visualize_explore}
These empirical observations lend strong support to our assertion that standard RL methods are susceptible to premature exploitation of the local policy space when learning AutoCode strategies. In sharp contrast, our proposed EM method facilitates a more guided exploration by sub-sampling trajectories according to the reference strategy (Sec.~\ref{sec_impl}). This enables continuous performance (evidenced in Sec.~\ref{sec_main}) and mitigating the risk of converging to suboptimal local optima (Fig.~\ref{fig_training_efficiency}).


\input{figs/fig_learned_behavior}
\noindent\textbf{Analysis on Code Integration Behaviors.}
We investigated the properties of the learned code integration strategies to gain deeper insights into the mechanisms behind our method's performance gains. Our central hypothesis posits that optimal code integration unlocks synergistic performance benefits by effectively combining the strengths of CoT and code executions.  This synergy presents a "free lunch" scenario: a well-learned metacognitive tool-usage strategy can elevate overall performance, provided the model demonstrates competence in solving \emph{distinct} subsets of queries using either CoT or code execution.

To empirically validate this "free lunch" principle and demonstrate the superiority of our approach in realizing it, we benchmarked our model against baselines that inherently support both code execution and Chain-of-Thought (CoT) reasoning: GPT-4, Mammoth-70B, and DeepseekMath-Instruct-7B. Our analysis evaluated the model's autonomous decision to invoke code when not explicitly instructed on which strategy to employ. We compared this "AutoCode" performance against scenarios where models were explicitly prompted to utilize either code or CoT reasoning. We also considered the theoretical "free lunch" upper bound – the accuracy achieved by combining the successful predictions from either strategy (i.e., taking the union of queries solved by CoT or code).

As visually presented in Figure~\ref{fig_learned_behavior}, existing baseline models exhibit inferior performance in AutoCode mode compared to scenarios where code invocation is explicitly prompted, e.g., DeepseekMath-Instruct-7B shows a degradation of 11.54\% in AutoCode mode. This suggests that their AutoCode strategies are often suboptimal, performing closer to random selection between CoT and code (selection accuracy near 50\%), resulting in AutoCode falling between the performance of explicitly triggered CoT and code. In contrast, our models learn more effective code integration strategies.  AutoCode4Math-Qwen2.5, for example, improves upon explicitly code-triggered performance by 7\%, indicating a true synergistic integration of reasoning and code execution.


To quantify the effectiveness of these learned "AutoCode" strategies, we calculated the CoT/code selection accuracy. We used the outcome of explicit instruction (i.e., performance when explicitly prompted for CoT or code) as a proxy for the ground-truth optimal method selection.  Our model achieves a selection accuracy of 89.53\%, showcasing the high efficacy of the learned code integration strategy.