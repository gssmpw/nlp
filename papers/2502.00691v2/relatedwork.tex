\section{Related Work and Discussion}
\textbf{Tool-Integrated Math LLMs.} 
Math language models adopted two major paradigms: Chain-of-Thought (CoT) reasoning and the use of external tools, such as Python programs~\citep{metamath, mammoth, openmath}. Each paradigm offers unique benefits, and recent hybrid frameworks~\citep{mammoth, tora, htl, dsmath, qwen25} increasingly seek to combine them for synergy. However, current models exhibit critical rigidity, motivating our work to realize the true metacognitive capacity that enjoys synergistic benefits of CoT and code. 

\noindent\textbf{EM for RL.} Expectation-Maximization (EM) has proven effective for maximum likelihood problems involving hidden variables, such as Expert Iteration~\citep{expertiter}, Iterative Maximum Likelihood~\citep{iml, iml1}, Meta-Reinforcement Learning~\citep{varibad, vem}, and Adversarial Games~\citep{acb}. In the context of math LLMs, the most relevant works are \citep{restem} and \citep{iml2}, which apply EM-style iterative self-training to math problem-solving. Unlike these approaches, we leverage the EM framework for guided exploration during reinforcement learning of language models.

\noindent\textbf{Conclusion.} Existing tool-integrated math language models lack the metacognitive capacity to effectively determine code integration, hindering their ability to fully realize the synergistic benefits of tool integration and CoT.  To address this critical gap, we propose a novel EM-based framework that combines guided exploration with policy optimization.  Our experiments demonstrate the limitations of standard SFT and RL in efficiently exploring the combinatorial space of code-integrated trajectories and highlight the superior training efficiency and performance of our approach.

\clearpage

\noindent\textbf{Limitations.} 
The scope of our work is primarily focused on mathematical problem-solving.  While we observe promising results on challenging benchmarks like MATH500, the generalizability of our approach to other domains requiring the metacognitive capacity of tool integration and CoT, such as scientific reasoning or code generation for general-purpose tasks, remains to be explored.  Future work should investigate the effectiveness of our framework across a wider range of tasks and domains.

% The study of tool-integrated Language models is motivated by the belief that transformers are prone to cumulative errors due to the autoregressive probabilistic decoding and hence tool integration show promise to ensure the computational precision. However, such inaccuracies seem to be largely mitigated by performing test-time scaling techniques, thus reducing the significance of tool-integrated reasoning systems.   
% This paper investigates autonomous code integration for math LLMs. To address the challenge of unreliable external supervision, we propose to factorize out the hidden methodology-selection from response generation, and develop a novel EM formulation. The EM framework alternates between computing a reference strategy for methodology-selection through self-exploration and updating language model based on the reference guidance. This approach supports an efficient joint training scheme that allows for holistic offline data collection coupled with RL training. Our extensive experiments demonstrate the effectiveness of the proposed method, and our ablation studies further elucidate the properties of the learned model.

% However, there are several limitations and areas for future work regarding AutoCode4Math. First, the generalization of methodology-selection depends significantly on the quality of the collected query set. Further research is needed to understand what characteristics of queries contribute to effective generalization. Second, we did not extensively explore the influence of hyperparameters related to RL iterations, such as dataset size and the number of iterations, in the current version. We are actively working on this. Third, as this is a preliminary work in autonomous code integration, we have not yet investigated alternative approaches for decision routing, such as using Mixture-of-Experts (MoEs)~\citep{moe}, and we not yet fully understand the fundamental reason why EM outperforms RL. These areas present important directions for advancing AutoCode capabilities in math LLMs.

\bibliography{custom}
% \bibliographystyle{acl_natbib}
\clearpage
% \appendix
% \input{sections/10_appendix}