% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{pifont}
\usepackage{colortbl}
\usepackage{amsmath, amssymb}
\usepackage{enumitem}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{tcolorbox}

\tcbset{
  myaddedbox/.style={
    colback=blue!5,
    colframe=blue!75,
    fonttitle=\bfseries,
    boxrule=0.5mm,
    arc=2mm,
    left=2mm,
    right=2mm,
    top=2mm,
    bottom=2mm
  }
}
\tcbset{
  simplebox/.style={
    colback=white,     % Background color (white for no fill)
    colframe=blue!75,    % Border color
    boxrule=0.4pt,     % Thin border
    sharp corners,     % No rounded corners
    left=1mm,          % Minimal padding
    right=1mm,
    top=1mm,
    bottom=1mm
  }
}
\usepackage{listings}
\newcommand{\dcold}{\mathrm{D_{cold}}}
\newcommand{\dwarm}{\mathrm{D_{warm}}}
\newcommand{\ds}{\mathrm{D}}
\lstdefinelanguage{Markdown}{
  keywords={},
  sensitive=false,
}

\lstset{
    language=Markdown,
    basicstyle=\ttfamily,
    breaklines=true,
    columns=fullflexible
}

\title{Learning Autonomous Code Integration for Math Language Models}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{Haozhe Wang$^{\dagger\ddagger}$, Long Li$^\dagger$, Chao Qu$^\dagger$, Fengming Zhu$^\ddagger$, Weidi Xu$^\dagger$, Wei Chu$^\dagger$, Fangzhen Lin$^\ddagger$\\
\texttt{jasper.whz@outlook.com}\\
INF Technology$^\dagger$, Hong Kong University of Science and Technology$^\ddagger$
}
% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging
% funding agencies.  Funding acknowledgements go at the end of the paper.} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.
\definecolor{LightCyan}{rgb}{0.88,1,1}
\newcommand{\xmark}{\ding{55}} % Define xmark as a cross symbol
\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\up}{$\uparrow$}
\newcommand{\down}{$\downarrow$}


\begin{document}


\maketitle
\input{sections/0_abstract}

\input{sections/1_intro}

\input{sections/2_method}

\input{sections/3_exp}
\section{Related Work and Discussion}
\textbf{Tool-Integrated Math LLMs.} 
Math language models adopted two major paradigms: Chain-of-Thought (CoT) reasoning and the use of external tools, such as Python programs~\citep{metamath, mammoth, openmath}. Each paradigm offers unique benefits, and recent hybrid frameworks~\citep{mammoth, tora, htl, dsmath, qwen25} increasingly seek to combine them for synergy. However, current models exhibit critical rigidity, motivating our work to realize the true metacognitive capacity that enjoys synergistic benefits of CoT and code. 

\noindent\textbf{EM for RL.} Expectation-Maximization (EM) has proven effective for maximum likelihood problems involving hidden variables, such as Expert Iteration~\citep{expertiter}, Iterative Maximum Likelihood~\citep{iml, iml1}, Meta-Reinforcement Learning~\citep{varibad, vem}, and Adversarial Games~\citep{acb}. In the context of math LLMs, the most relevant works are \citep{restem} and \citep{iml2}, which apply EM-style iterative self-training to math problem-solving. Unlike these approaches, we leverage the EM framework for guided exploration during reinforcement learning of language models.

\noindent\textbf{Conclusion.} Existing tool-integrated math language models lack the metacognitive capacity to effectively determine code integration, hindering their ability to fully realize the synergistic benefits of tool integration and CoT.  To address this critical gap, we propose a novel EM-based framework that combines guided exploration with policy optimization.  Our experiments demonstrate the limitations of standard SFT and RL in efficiently exploring the combinatorial space of code-integrated trajectories and highlight the superior training efficiency and performance of our approach.

\clearpage

\noindent\textbf{Limitations.} 
The scope of our work is primarily focused on mathematical problem-solving.  While we observe promising results on challenging benchmarks like MATH500, the generalizability of our approach to other domains requiring the metacognitive capacity of tool integration and CoT, such as scientific reasoning or code generation for general-purpose tasks, remains to be explored.  Future work should investigate the effectiveness of our framework across a wider range of tasks and domains.

% The study of tool-integrated Language models is motivated by the belief that transformers are prone to cumulative errors due to the autoregressive probabilistic decoding and hence tool integration show promise to ensure the computational precision. However, such inaccuracies seem to be largely mitigated by performing test-time scaling techniques, thus reducing the significance of tool-integrated reasoning systems.   
% This paper investigates autonomous code integration for math LLMs. To address the challenge of unreliable external supervision, we propose to factorize out the hidden methodology-selection from response generation, and develop a novel EM formulation. The EM framework alternates between computing a reference strategy for methodology-selection through self-exploration and updating language model based on the reference guidance. This approach supports an efficient joint training scheme that allows for holistic offline data collection coupled with RL training. Our extensive experiments demonstrate the effectiveness of the proposed method, and our ablation studies further elucidate the properties of the learned model.

% However, there are several limitations and areas for future work regarding AutoCode4Math. First, the generalization of methodology-selection depends significantly on the quality of the collected query set. Further research is needed to understand what characteristics of queries contribute to effective generalization. Second, we did not extensively explore the influence of hyperparameters related to RL iterations, such as dataset size and the number of iterations, in the current version. We are actively working on this. Third, as this is a preliminary work in autonomous code integration, we have not yet investigated alternative approaches for decision routing, such as using Mixture-of-Experts (MoEs)~\citep{moe}, and we not yet fully understand the fundamental reason why EM outperforms RL. These areas present important directions for advancing AutoCode capabilities in math LLMs.

\bibliography{custom}
% \bibliographystyle{acl_natbib}
\clearpage
% \appendix
% \input{sections/10_appendix}

\end{document}

