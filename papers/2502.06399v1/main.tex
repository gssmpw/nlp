\documentclass{article}
\usepackage[utf8]{inputenc}

%\usepackage{amsaddr}
\usepackage{subcaption}
\usepackage{physics}
\usepackage{amsmath,graphicx}
\usepackage{mathtools}
\mathtoolsset{showonlyrefs=true}
%\usepackage{times}
%\usepackage{comment}
\usepackage{float}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}  
\usepackage{xfrac}
\usepackage[ruled,vlined]{algorithm2e}
%\usepackage{hyperref}
% \hypersetup{
%     colorlinks=true,
%     linkcolor=blue,
%     filecolor=blue,
%     citecolor = blue,      
%     urlcolor=blue,
%     }
\usepackage{authblk}

\usepackage[backref=page]{hyperref}
%%%% these patches ensure that the backrefs point to the actual occurrences of the citations in the text, not just the page or section in which they appeared
%%%% https://tex.stackexchange.com/questions/54541/precise-back-reference-target-with-hyperref-and-backref
%%%% BEGIN BACKREF DIRECT PATCH, apply these AFTER loading hyperref package with appropriate backref option
% The following options are provided for the patch, currently with a poor interface!
% * If there are multiple cites on the same (page|section) (depending on backref mode),
%   should we show only the first one or should we show them all?
\newif\ifbackrefshowonlyfirst
\backrefshowonlyfirstfalse
%\backrefshowonlyfirsttrue
%%%% end of options
%
% hyperref is essential for this patch to make any sense, so it is not unreasonable to request it be loaded before applying the patch
\makeatletter
% 1. insert a phantomsection before every cite, so hyperref has something to target
%    * in case natbib is loaded. hyperref provides an appropriate hook so this should be safe, and we don't even need to check if natbib is loaded!
\let\BR@direct@old@hyper@natlinkstart\hyper@natlinkstart
\renewcommand*{\hyper@natlinkstart}{\phantomsection\BR@direct@old@hyper@natlinkstart}% note that the anchor will appear after any brackets at the start of the citation, but that's not really a big issue?
%    * if natbib isn't used, backref lets \@citex to \BR@citex during \AtBeginDocument
%      so just patch \BR@citex
\let\BR@direct@oldBR@citex\BR@citex
\renewcommand*{\BR@citex}{\phantomsection\BR@direct@oldBR@citex}%

% 2. if using page numbers, show the page number but still hyperlink to the phantomsection instead of just the page!
\long\def\hyper@page@BR@direct@ref#1#2#3{p. \hyperlink{#3}{#1}}

% check which package option the user loaded (pages (hyperpageref) or sections (hyperref)?)
\ifx\backrefxxx\hyper@page@backref
    % they wanted pages! make sure they get our re-definition
    \let\backrefxxx\hyper@page@BR@direct@ref
    \ifbackrefshowonlyfirst
        %\let\backrefxxxdupe\hyper@page@backref% test only the page number
        \newcommand*{\backrefxxxdupe}[3]{#1}% test only the page number
    \fi
\else
    \ifbackrefshowonlyfirst
        \newcommand*{\backrefxxxdupe}[3]{#2}% test only the section name
    \fi
\fi

% 3. now make sure that even if there is no numbered section, the hyperref's still work instead of going to the start of the document!
\RequirePackage{etoolbox}
\patchcmd{\Hy@backout}{Doc-Start}{\@currentHref}{}{\errmessage{I can't seem to patch backref}}
\makeatother
%%%% END BACKREF PATCHES

\usepackage[numbers]{natbib}
\bibliographystyle{abbrvnat}

\usepackage{blindtext} % for dummy text

\makeatletter
    \setlength\@fptop{0\p@}
\makeatother

\usepackage{fourier}

\newtheorem{theorem}{Theorem}[section]

\newtheorem{definition}{Definition}[section]

\newtheorem{assumption}{Assumption}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}{Proposition}[section]
\newtheorem{remark}{Remark}[section]

\newcommand{\inp}[0]{{z}}
\newcommand{\inpS}[0]{{\mathcal{Z}}}
%\newcommand{\inner}[2]{ \left\langle #1 \; , \; #2 \right\rangle }
% by Yen-Huan
\usepackage{interval}
\intervalconfig{soft open fences}
\usepackage{xcolor}
\usepackage{braket}
\usepackage{changes}
% \newcommand{\norm}[1]{\Vert #1 \Vert}
\DeclareMathOperator{\inte}{Int}
\DeclareMathOperator{\dom}{dom}
\DeclareMathOperator{\supp}{supp}
\DeclareMathOperator*{\argmin}{ \arg \min }

\renewcommand{\intercal}{\mathsf{\scriptscriptstyle{T}}}

\title{A Linearly Convergent Algorithm for Computing the Petz-Augustin Information}

\author[1]{Chun-Neng~Chu}
\author[2]{Wei-Fu~Tseng}
\author[1,2,3]{Yen-Huan Li}

\affil[1]{Department of Computer Science and Information Engineering,\protect\\National Taiwan University}
\affil[2]{Department of Mathematics, National Taiwan University}
\affil[3]{Center for Quantum Science and Engineering, \protect\\ National Taiwan University}

\date{}
\begin{document}

\maketitle

\begin{abstract}
We propose an iterative algorithm for computing the Petz-Augustin information of order $\alpha\in(1/2,1)\cup(1,\infty)$. 
The optimization error is guaranteed to converge at a rate of $O\left(\abs{1-1/\alpha}^T\right)$, where $T$ is the number of iterations. 
Let $n$ denote the cardinality of the input alphabet of the classical-quantum channel, and $d$ the dimension of the quantum states. 
The algorithm has an initialization time complexity of $O\left(n d^{3}\right)$ 
%. 
%, 
%where $n$ is the cardinality of the input alphabet of the classical-quantum channel, and $d$ is the dimension of the quantum state. 
% The per-iteration time complexity of the algorithm is $O\left(n d^{2}+d^3\right)$.  
and a per-iteration time complexity of $O\left(n d^{2}+d^3\right)$. 
To the best of our knowledge, this is the first algorithm for computing the Petz-Augustin information with a non-asymptotic convergence guarantee. 
\end{abstract}

%\section{Introduction}
%
%Let $\alpha \in ( 0, 1 ) \cup ( 1, \infty )$. 
%We consider the problem of computing the order-$\alpha$ \emph{Petz-Augustin information}, given by
%\begin{equation}
%f_\star = \min_{\sigma \in \mathcal{D}_d} f ( \sigma ), \quad f ( \sigma ) \coloneqq \mathsf{E}_\omega \left[ D_\alpha ( \omega \Vert \sigma )  \right] . \label{eq_problom} 
%\end{equation}
%Here, $\mathcal{D}_d$ denotes the set of quantum density matrices--Hermitian positive semi-definite matrices of unit traces---in $\mathbb{C}^{d \times d}$; 
%$\omega$ is a random variable taking values in a finite set $\Omega \subset \mathcal{D}_d$; 
%for any two density matrices $\rho$ and $\sigma$, the notation $D_\alpha ( \rho \Vert \sigma )$ denotes the order-$\alpha$ Petz-R\'enyi divergence \citep{Petz1986}: 
%\[
%D_\alpha ( \rho \Vert \sigma ) \coloneqq \frac{1}{1 - \alpha} \log \tr ( \rho^\alpha \sigma^{1 - \alpha} ) , 
%\]
%where $\sigma$ is allowed to have trace not equal to $1$. 
%We assume that the union of $\supp \omega$ equals $\mathbb{C}^d$; 
%otherwise, one can consider a lower-dimensional subspace of $\mathbb{C}^d$. 
%The Petz-Augustin information is crucial in, for example, characterizing the error exponents for quantum source and channel coding as well as privacy amplification \cite{Cheng2024a,Cheng2022,Cheng2022a,Mosonyi2017,Shen2023}. 
%% Remarkably, the Petz-Augustin information converges to the quantum relative entropy when $\alpha \to 1$. 
%
%We will focus on the case where $\alpha \in ( 0, 1 ) \cup ( 1, 2 )$, as then the optimization problem \eqref{eq_problom} is convex \cite{Mosonyi2017}. 
%However, despite the convexity, \citet{You2022} have shown that neither the objective function $f$ nor its gradient is Lipschitz continuous, violating standard assumptions in analyzing first-order optimization algorithms. 
%As a result, even the convergence of gradient descent, arguably the most basic first-order algorithm, for the optimization problem \eqref{eq_problom} has been unclear.  
%Although optimization algorithms exploiting higher-order information may work \citep{Nesterov2018a}, their computational complexities grow fast with increasing dimension $d$, making them prohibitive for quantum information applications.  
%
%In this paper, we propose a simple first-order algorithm for solving the optimization problem \eqref{eq_problom} with $\alpha \in ( 1, 2 ]$. 
%
%We prove that the algorithm outputs an $\varepsilon$-approximate solution in $O ( c_\alpha \log ( 1 / \varepsilon ) )$ iterations, where $c_\alpha \coloneqq \alpha / ( \alpha - 1 )$. 
%% Its per-iteration time complexity is $O ( \abs{ \Omega } d^2 + d^3 )$, where $\abs{ \Omega }$ denotes the cardinality of the set $\Omega$.
%To the best of our knowledge, this is the first algorithm for computing the Petz-Augustin information that possesses a non-asymptotic error guarantee. 
%
%\section{Related Work}
%
%We recover the classical Augustin information when all matrices in the optimization problem \eqref{eq_problom} share the same eigenbasis. 
%The Augustin information was proposed by \citet{Augustin1969}.
%\citet{Augustin1978} also proposed a fixed point iteration, which was later proved to asymptotically converge 

\section{Introduction}
Several fundamental quantities in quantum information theory, including the quantum channel capacity, the relative entropy of entanglement, and the quantum R\'{e}nyi information, are formulated as optimization problems and lack closed-form expressions \cite{Hayashi2017,Wilde2017}. 
%These two are added by Prof. Li.
%p383 (8.77) entanglement
%p393  Further, Donald and Horodecki [33] showed Condition E3 for entanglement of relative entropy Er,S(ρ)(They call it relative entropy of entanglement)
%p159 (4.9) cq channel capacity
While their operational meanings and mathematical properties have been explored in depth, much less is known %regarding 
about
how to efficiently compute these quantities. 
Recent works 
% have focused 
have been primarily focusing 
on 
the computation of quantities involving 
the quantum relative entropy. 
For instance, the self-concordance property of the quantum relative entropy has been exploited to facilitate the use of interior-point methods \citep{Fawzi2023,Faybusovich2017,He2024b}. 
Various 
representations of the matrix logarithm and quantum relative entropy
have been established, enabling numerous optimization tasks to be solved by semidefinite programming \cite{Brown2024,Chandrasekaran2017,Fawzi2022,Fawzi2017a,Fawzi2019,Frenkel2023,He2024b,Huang2024,Jencova2024,Kossmann2024a,Kossmann2024b}. 
The classical Blahut-Arimoto algorithm has been generalized and quite well studied for computing quantum channel capacities \cite{Hayashi2024,He2024a,Li2019b,Nagaoka1998,Ramakrishnan2021}. 

A natural generalization of the quantum relative entropy is the quantum R\'{e}nyi divergence. 
Due to the non-commutative nature of the quantum setup, there are %several 
multiple notions of quantum R\'{e}nyi divergences, 
such as the Petz-R{\'e}nyi divergence \cite{Petz1986}, the sandwiched R{\'e}nyi divergence \cite{Muller2013,Wilde2014}, the geometric R\'{e}nyi divergence \cite{Fang2021,Matsumoto2018}, and the \#-R{\'e}nyi divergence \cite{Fawzi2021}.
Unfortunately, research on optimization involving quantum R{\'e}nyi divergences remains limited. 
\citet{Fawzi2021}, 
%p2 Using the joint concavity of the matrix geometric mean, the optimization program in (5) is convex and for rational values of α it can be expressed as a semidefinite program [14, 33
%p20 Using the fact that Deregα (N kM) ≤ D#α (N kM) and the fact that the set of channels VΘ is representable by a semidefinite program, we obtain efficiently computable bound
as well as 
\citet{Fang2021}, 
%p4 any optimization minM∈V Dbα(N kM) can be computed as a semidefinite program if V is a set of channels characterized by semidefinite conditions.
formulated computational tasks involving the \#-R{\'e}nyi divergence and the geometric R{\'e}nyi divergence as semidefinite programs.
\citet{Liu2024}, by utilizing a specific approximation of the matrix geometric mean,  proposed a new class of algorithms that can be used to compute the geometric R\'{e}nyi divergence. 
%p5 "We introduce quantum algorithms for learning the metric in machine learning, by phrasing this as an optimisation problem using a geometric perspective."
%p6 the first quantum algorithm for computing the geometric α-R´enyi relative entropy,
%p1 For example, we show how to use them in the estimation of geometric ´enyi relative entropies
% \citet{You2022} proposed entropic mirror descent with the Polyak step size to compute various forms of the quantum R\'{e}nyi information and the quantum Augustin information. 
% Notably, the algorithm proposed by \citet{You2022} is a general-purpose convex optimization algorithm that does not exploit the specific structures of quantum R\'{e}nyi divergences and lacks a non-asymptotic convergence guarantee.
\citet{You2022} proposed computing various forms of the quantum R\'{e}nyi information and the quantum Augustin information via mirror descent with a Polyak-type step size. 
Notably, their algorithm guarantees only asymptotic convergence and lacks a complexity characterization. 

We are particularly interested in computing the Petz-Augustin information of order $\alpha$, which is defined as the minimum of an expected Petz-R\'{e}nyi divergence of order 
$\alpha$ over the set of quantum density matrices \eqref{eq:Petz_Augustin}.
The Petz-Augustin information is a generalization of the quantum mutual information and has an application in characterizing the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}.
Unlike the quantum mutual information, the Petz-Augustin information does not have a closed-form expression.
The optimization problem defining the Petz-Augustin information is convex for orders $\alpha \in (0, 1) \cup (1, 2]$ \cite{Mosonyi2017}. 
% , and we focus on the regime $\alpha\in\left(1,2\right]$.
However, the gradients and Hessians of R{\'e}nyi divergences are unbounded \cite[Propositions 3.1 and 3.2]{You2022}, 
violating standard assumptions in convex optimization literature. 
As a result, standard first-order optimization algorithms and their theoretical guarantees do not directly apply.
To the best of our knowledge, no existing first-order optimization method for computing the Petz-Augustin information of any order 
has a non-asymptotic convergence guarantee.  
Although second-order methods \cite{Nesterov2018a} may be applicable, their per-iteration time complexities grow rapidly with the dimension of the quantum state, and hence do not scale well with the number of qubits.

We propose a simple iteration rule in Section \ref{eq:CohenIteration}, tailored for computing the Petz-Augustin information.
We prove that the optimization error of the proposed iteration rule converges at a rate of $O\left(\abs{1-1/\alpha}^T\right)$ for all $\alpha \in (1/2,1)\cup(1,\infty)$, where $T$ denotes the number of iterations (Theorem \ref{ineq:FvalConv}).
This result is achieved by establishing the contractive property of the iterates with respect to the Thompson metric (Lemma \ref{ineq:Contract}).
Notably, our algorithm is computationally cheaper than standard first-order methods, as
the time complexity of computing a gradient is $O\left(nd^2+d^4\right)$ \cite{You2022}, whereas the per-iteration time complexity of our algorithm is only $O\left(n d^2+d^3\right)$, where $n$ is the cardinality of the input alphabet of the classical-quantum channel, and $d$ is the dimension of the quantum state.

%We shall note that our iteration rule is inspired by an efficient algorithm proposed by \citet{Cohen2015} for computing the \emph{$\ell_p$-Lewis weights}.
%The $\ell_p$-Lewis weights, along with their variants, have applications in $\ell_1$-regression \cite{Parulekar2021,Durfee2018} and linear programming \cite{Lee2020}.
%For a comprehensive treatment of the $\ell_p$-Lewis weights, we refer readers to \citet{Cohen2015}.
%In Section \ref{subsec:RelationWithCP}, we discuss the relation between the $\ell_p$-Lewis weights and the Petz-Augustin information.
%Although there are similarities, we clarify in Section \ref{subsec:RelationWithCP} that our results cannot be trivially derived from those of \citet{Cohen2015}.
%In Section \ref{subsec:Conv}, we address the difficulties encountered in adapting the algorithm proposed by \citet{Cohen2015} to compute the Petz-Augustin information, with the hope that further studies on more general optimization problems involving quantum R\'{e}nyi divergences will be inspired by our analysis.

%Finally, we note that our algorithm does not cover the case where $\alpha\in(0,1)$, which appears in characterizing the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}. % See (22) in Cheng2019 and (3) in Dalai2014
%However, Lemma \ref{ineq:TraceBoundViolation} indicates that our main theorem cannot be directly extended to orders less than $1$. 
%Generalizing our algorithm for orders less than $1$ remains a direction for future research.




%Although the Petz-Augustin information of order $\alpha \in (1,2]$ currently lacks an operational meaning, it still offers valuable implications. 
%Specifically, there exists another type of quantum Augustin information, known as the \emph{sandwiched Augustin information}, which is upper-bounded by the Petz-Augustin information for any shared order $\alpha > 1$ \cite{Mosonyi2017}. 
%The sandwiched Augustin information of order $\alpha \in (1,2]$ is crucial for characterizing achievability bounds in quantum soft covering and privacy amplification \cite{Cheng2022b,Shen2023}, suggesting that the Petz-Augustin information provides a looser bound.

 
%in
%of %https://link.springer.com/article/10.1007/s00220-017-2928-4
%classical-quantum channel coding. 
%Looking ahead, the Petz-Augustin information has broader applications for orders less than $1$ than for those in $\left(1,2\right]$.
%For example, it is used in characterizing the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}. % See (22) in Cheng2019 and (3) in Dalai2014
%Looking ahead, the Petz-Augustin information of order $\alpha\in(0,1)$ is used to characterize the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}, % See (22) in Cheng2019 and (3) in Dalai2014
%whereas for $\alpha\in\left(1,2\right]$, it currently lacks an operational meaning.
%Our algorithm does not cover the case where $\alpha\in(0,1)$, which appears in characterizing the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}. % See (22) in Cheng2019 and (3) in Dalai2014
%However, Lemma \ref{ineq:TraceBoundViolation} indicates that our main theorem cannot be directly extended to orders less than $1$. 
%Generalizing our algorithm for orders less than $1$ remains a direction for future research.


\section{Related Work}
\label{sec:related_work}
\subsection{Computing Classical Augustin information}
The classical Augustin information \cite{Augustin1978,Csiszar1995} can be viewed as a special case of the Petz-Augustin information 
where all the matrices commute. 
Augustin \cite{Augustin1978} proposed a fixed-point iteration, which we 
refer to as the Augustin iteration, for computing the Augustin information. 
The Augustin iteration
is proved to converge asymptotically for $\alpha\in(0,1)$ by 
\citet{Karakos2008} and 
\citet{Nakiboglu2019}. 
Recently, \citet{Tsai2024} proved that the Augustin iteration converges 
%linearly 
at a linear rate
with respect to the Hilbert projective metric for $\alpha\in(1/2,1)\cup(1,3/2)$.

Our proposed method can be viewed as a generalization of the Augustin iteration for the classical Augustin information.
Denote the function to be minimized in computing the Petz-Augustin information as $f_{\alpha}(\sigma_B)$, which is given in Section \ref{Sec:PetzAug}.
The Augustin iteration can be written as\footnote{Although originally proposed for the classical case, where matrices reduce to vectors, we present it in matrix form, as our algorithm is designed for the quantum case.}
\begin{align}
    \sigma_B^{(t+1)}=\sigma_B^{(t)} \left(-\nabla f_{\alpha}\left(\sigma_B^{(t)}\right)\right),
\end{align}
where $\sigma_B^{(t)}$ denotes the $t^{\text{th}}$ iterate of the algorithm.
On the other hand, our method can be expressed as 
\begin{align}
    \sigma_B^{(t+1)}=\sigma_B^{(t)} \left(-\nabla f_{\alpha}\left(\sigma_B^{(t)}\right)\right)^{1/\alpha},
\end{align}
if all the matrices commute. 
Thus, our method can be viewed as a generalization of the Augustin iteration with an additional parameter $1/\alpha$ in the exponent, which is analogous to the step size in first-order optimization methods. 

For algorithms different from the Augustin iteration, an alternating minimization method \cite{Kamatsuka2024} converges at a rate of $O(1/T)$ for $\alpha\in(1,\infty)$ \cite{Tsai2024}, where $T$ denotes the number of iterations. 
Riemannian gradient descent with respect to the Poincar{\'e} metric %\cite{Wang2024} 
also converges at a rate of $O(1/T)$ for all $\alpha\in(0,1)\cup(1,\infty)$ \cite{Wang2024}.

\subsection{Computing Petz-Augustin information}
The optimization problem defining the Petz-Augustin information of order $\alpha$ is known to be convex for $\alpha\in(0,1)\cup\left(1,2\right]$ \cite{Mosonyi2017}.
Since the objective function has a locally bounded gradient, entropic mirror descent with Armijo line search \cite{Li2019a} or with the Polyak step size \cite{You2022} is applicable for $\alpha\in(0,1)\cup\left(1,2\right]$. 
However, these two algorithms only guarantee asymptotic convergence. 
To the best of our knowledge, there is currently no 
% algorithm 
first-order method
that guarantees a non-asymptotic convergence rate for any $\alpha\in(0,1)\cup(1,\infty)$.

Our algorithm (Equation \eqref{eq:DiscreteCP}) is inspired by an algorithm proposed by \citet{Cohen2015} for computing the $\ell_p$-Lewis weights, which has been proven to converge linearly with respect to the Thompson metric.
The $\ell_p$-Lewis weights, along with their variants, have applications in $\ell_1$-regression \cite{Durfee2018,Parulekar2021} and linear programming \cite{Lee2020}.
%For a comprehensive discussion of the $\ell_p$-Lewis weights, we refer readers to the paper of \citet{Cohen2015}.
%Note that our results 
%% cannot be trivially derived 
%do not immediately follow
%from those of \citet{Cohen2015}.
%See Section \ref{subsec:RelationWithCP} for a detailed discussion.
Despite the conceptual connection, our results do not immediately follow from those of \citet{Cohen2015}, as discussed in Section \ref{subsec:RelationWithCP}. 

After completing this work, we noticed a recent study by \citet{Cheng2024}, which also
considers an iterative algorithm that coincides with ours.
However, their derivation and analysis adopt a different perspective and only guarantee asymptotic convergence for $\alpha\in(1,\infty)$.
In contrast, we prove that %our proposed 
the 
algorithm converges at a rate of $O\left(\left|1-1/\alpha\right|^T\right)$ for $\alpha\in(1/2,1)\cup(1,\infty)$, 
not only extending the range of $\alpha$ but also providing a non-asymptotic guarantee. 
%, where $T$ denotes the number of iterations.

%For related works that did not focus on the computation, \citet{Cheng2019}, \citet{Cheng2024}, as well as \citet{Mosonyi2021}, proposed several operators and analyzed their fixed-point properties at the minimizers of the convex optimization problem that defines the Petz-Augustin information.
%In particular, our proposed iteration rule in Section \ref{subsec:CohenIteration} involves repeatedly applying an operator  $T_{f_{\alpha}}$ to the iterates.
%The operator $T_{f_{\alpha}}$ coincides with the one proposed by \citet[Equation (32)]{Cheng2024}, where it is referred to as the \emph{(adjusted) power mean}.
%However, they did not design algorithms based on their proposed operators.

%To design an efficient algorithm for computing the Petz-Augustin information, we observe that computing the classical Augustin information of order $\alpha$ is similar to the task of computing the $\ell_p$-Lewis weights.
%Specifically, this connection holds when $\alpha=2/p$. 
%For a comprehensive treatment of computing the $\ell_p$-Lewis weights, we refer readers to \citet{Cohen2015} as well as \citet{Lee2020}. 
%\citet{Cohen2015} also proposed a fixed-point iteration, which we refer to as the \textit{CP-iteration}. They proved the linear convergence of the CP-iteration by leveraging the contractive property of the iteration mapping.
%The relationship between the computation of the Augustin information and the $\ell_p$-Lewis weights is briefly discussed in Section \ref{subsec:CohenIteration}. 
%Our proposed algorithm can be seen as a quantum generalization of the CP-iteration. 
%We also prove a linear convergence rate for the optimization error, which was not considered in \citet{Cohen2015}.


\section{Preliminaries}
\label{sec:problem}
\subsection{Notations}
We denote the sets of vectors in $\mathbb{R}^d$ with nonnegative entries and
strictly 
positive entries by $\mathbb{R}_{+}^d$ and $\mathbb{R}_{++}^d$, respectively.
We denote $\mathbb{C}^d$ by $\mathcal{H}_B$. 
We denote the set of all Hermitian matrices in $\mathbb{C}^{d\times d}$ by $\mathcal{B}\left(\mathcal{H}_B\right)$.
%We denote the Hilbert-Schmidt inner product by $\braket{\cdot , \cdot}_{\text{H}}$. 

For any $\sigma_B \in \mathcal{B}(\mathcal{H}_B)$, we denote its support by 
\begin{align}
    \supp(\sigma_B)\coloneqq \Set{ u \in \mathcal{H}_B \mid \sigma_B u \neq 0 }.
\end{align}

We define $A_i$ to be the $i^{\text{th}}$ row of a matrix $A$ and $v[i]$ to be the $i^{\text{th}}$ entry of a vector $v$. 
For any vector $v$, we denote by $\mathrm{Diag}(v)$ the diagonal matrix whose $i$-th diagonal element is $v[i]$. 
For any $v\in\mathbb{C}^d$, we denote its conjugate transpose by $v^*$.
For any function \( f: \mathbb{R} \to \mathbb{R} \) and 
% a 
vector \( v \in \mathbb{R}^d \), we define \( f(v) \) 
% to be 
as 
% a 
the 
\( d \)-dimensional vector where \( f(v)[i] = f(v[i]) \). 
Similarly, for such a function \( f \) and \( \sigma \in \mathcal{B}(\mathcal{H}_B) \), we define \( f(\sigma) \) as \( \sum_{i=1}^d f(\lambda_i)  u_i u_i^* \), where \( \sigma = \sum_{i=1}^d \lambda_i   u_i u_i^* \) is the eigendecomposition of \( \sigma \). 


We denote the probability simplex in $\mathbb{R}^d$ by $\Delta_{d-1}$, i.e.,
\begin{align}
    \Delta_{d-1}\coloneqq % \left\{ v \in \mathbb{R}_{+}^d \middle\vert\ \sum_{i=1}^d v[i] = 1 \right\}.
    \Set{ v \in \mathbb{R}_{+}^d | \sum_{i=1}^d v[i] = 1 }.
\end{align}

For any \( \sigma_1, \sigma_2 \in \mathcal{B}(\mathcal{H}_B) \), we write \( \sigma_1 \leq \sigma_2 \) if and only if \( \sigma_2 - \sigma_1 \) is positive semi-definite. Similarly, we write \( \sigma_1 < \sigma_2 \) if and only if \( \sigma_2 - \sigma_1 \) is positive definite.
We define $\mathcal{B}\left(\mathcal{H}_B\right)_{+}$ and $\mathcal{B}\left( \mathcal{H}_B\right)_{++}$ as the nonnegative cone and the positive cone in \( \mathcal{B}(\mathcal{H}_B) \), respectively. 
Specifically,
\begin{equation} 
\mathcal{B}\left(\mathcal{H}_B\right)_{+} \coloneqq \set{ \sigma_B \in \mathcal{B}(\mathcal{H}_B) | \sigma_B \geq 0},
\end{equation} 
and
\begin{equation} 
\mathcal{B}\left(\mathcal{H}_B\right)_{++} \coloneqq \set{ \sigma_B \in \mathcal{B}(\mathcal{H}_B) | \sigma_B > 0}.
\end{equation}
We denote the set of quantum density matrices in $\mathcal{B}\left(\mathcal{H}_B\right)_{+}$ by $\mathcal{D}(\mathcal{H}_B)$, i.e.,
\begin{equation} \label{eq:set_B}
\mathcal{D}(\mathcal{H}_B) \coloneqq \set{ \sigma_B \in \mathcal{B}\left(\mathcal{H}_B\right)_{+} | \operatorname{Tr}[\sigma_B] = 1}.
\end{equation} 
For any $\sigma\in\mathcal{B}\left(\mathcal{H}_B\right)_{+}$, we denote its $i^{\text{th}}$ eigenvalue, ordered in decreasing order, by $\lambda_i(\sigma)$. 


%The idea of considering the Thompson metric is inspired by the work of \citet{Tsai2024}, in which they proved that the iterates of \emph{Augustin's iteration} for computing the Petz-Augustin information, under the condition that all matrices commute, converges linearly with respect to \emph{Hilbert's projective metric}.
%Notably, the algorithm we propose in Section \ref{subsec:CohenIteration} iteratively applies an operator  $T_{f_{\alpha}}$ to update the iterates.
%In Section \ref{subsec:Conv}, we analyze the convergence of our proposed iteration rule by proving the contractive property of the iterates with respect to the Thompson metric.  
%Then, we show in Lemma \ref{ineq:SmallThomp2SmallOptError} and Theorem \ref{ineq:FvalConv} that the convergence guarantee of the iterates with respect to the Thompson metric can be translated into that of the function values.


\subsection{Thompson metric}
The Thompson metric is a useful tool to study the behavior of dynamical systems \citep{Krause2015,Lemmens2012,Nussbaum1988,Thompson1963}. 
% The Thompson metric 
It 
can be defined on the interiors of any normal cone in real Banach spaces, such as $\mathbb{R}_{++}^d$ and $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ . 
% \cite{Nussbaum1988}.
%\citep{Krause2015,Lemmens2012,Nussbaum1988,Thompson1963}. 
%Nussbaum1988,p17
%cone imply close and convex, p9
Here, we are only interested in the following definition of the Thompson metric specialized for $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$.
\begin{definition}[{\cite{Thompson1963}}]
    \label{eq:Thompson}
    The Thompson metric between
	any     
    $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ % from \citet{Thompson1963} is equivalently defined as
    is given by 
    \begin{equation}
        d_{\mathrm{T}}(V,U)\coloneqq \inf %\left\{r\geq0|\exp(-r) V\leq U \leq \exp(r)V\right\}.
        \Set{ r\geq0 | \exp(-r) V\leq U \leq \exp(r)V }.
    \end{equation}
\end{definition}

%The Thompson metric is commonly used to study the existence and uniqueness of fixed points of nonlinear operators via 
%% a 
%contraction arguments \cite{Lim2009,Montrucchio1998}.

We
will 
analyze the convergence of our proposed iteration rule by proving a contractive property of the iterates with respect to the Thompson metric (see Lemma \ref{ineq:Contract} and Theorem \ref{ineq:FvalConv}). 
This analysis relies on the following lemmas concerning the Thompson metric, specialized for $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$.

\begin{lemma}[{\cite[Lemma 3]{Thompson1963}}]
    \label{eq:ThompIsMetric}
    The Thompson metric is a well-defined metric on $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, and $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ is complete with respect to this metric.
\end{lemma}
\begin{lemma}[{\cite[Proposition 1.5]{Nussbaum1988}}]% page 19
    \label{ineq:NegCurveProp}
    For any $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, 
    we have
    \begin{align}
        d_{\mathrm{T}}(U^r,V^r)\leq |r|d_{\mathrm{T}}(U,V),\quad\forall r\in[-1,1].
    \end{align}
\end{lemma}
%\begin{lemma}[{\cite{Thompson1963}}]
%    \label{eq:Brouwer}
%    Let $T:\mathcal{B}\left(\mathcal{H}_B\right)_{++}\mapsto\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ be an operator.
%    Suppose there exists a scalar $r$ with $0\leq r<1$ such that
%    \begin{align}
%        d_{\mathrm{T}}(T(U),T(V))
%        \leq r d_{\mathrm{T}}(U,V),\quad\forall U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}.
%    \end{align}
%    Then, there exists a unique fixed point $\sigma\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ of the operator $T$.
    %Moreover, let $\sigma_1\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, and define the sequence  $\sigma_{t+1}=T(\sigma_{t})$ for all $t\in\mathbb{N}$.
    %Then, the sequence $\left\{\sigma_t\right\}_t$ converges in norm to $\sigma$.
%\end{lemma}

\begin{lemma}
    \label{ineq:ThompsonLogHomo}
    For any $U,V\in\mathcal{B}\left( \mathcal{H}_B\right)_{++}$ and $r>0$, we have
    \begin{align}
        d_{\mathrm{T}}\left(U,rV\right)
        \leq d_{\mathrm{T}}\left(U,V\right)+\abs{\log(r)}.
    \end{align}
\end{lemma}

\begin{proof}
    By Definition \ref{eq:Thompson}, we write
    \begin{align}
        \exp\left(-d_{\mathrm{T}}\left(V,U\right)\right)V
        \leq U
        \leq \exp\left(d_{\mathrm{T}}\left(V,U\right)\right)V.
    \end{align}
    It follows that
    \begin{align}
        \exp\left(-d_{\mathrm{T}}\left(V,U\right)-\abs{\log(r)}\right)V
        &\leq\exp\left(-d_{\mathrm{T}}\left(V,U\right)+\log(r)\right)V\nonumber\\
        &\leq rU\nonumber\\
        &\leq \exp\left(d_{\mathrm{T}}\left(V,U\right)+\log(r)\right)V\nonumber\\
        &\leq \exp\left(d_{\mathrm{T}}\left(V,U\right)+\abs{\log(r)}\right)V.\nonumber
    \end{align}
    By Definition \ref{eq:Thompson}, the inequalities above imply that 
    \begin{align}
        d_{\mathrm{T}}\left(U,rV\right)
        \leq d_{\mathrm{T}}\left(V,U\right)+\abs{\log(r)}.
    \end{align}
    This concludes the proof.
\end{proof}
 
\subsection{Petz-Augustin Information}
\label{Sec:PetzAug}
Let 
$\mathcal{X}=
%\{1,2,...,n\}$ 
\Set{1,2,\dots,n}$
denote the input alphabet, and let  $P_{X}$ denote the probability distribution of 
a
%the 
random variable $X$ defined on $\mathcal{X}$. 
Let $\mathcal{W}:\mathcal{X}\mapsto\mathcal{D}(\mathcal{H}_B)$ be 
a
%the 
classical-quantum channel that maps an element $x\in \mathcal{X}$ to a density matrix $\rho_B^x$.
We assume that $\supp\left(\sum_{x:P_X(x)>0}\rho_B^x\right) = \mathcal{H}_B \setminus \set{0}$.
If the assumption does not hold, we may project all the matrices to a lower-dimensional space.
%Let $\rho_{{X}B} = \sum_{x\in \mathcal{X}}P_{X}(x)E_x\otimes\rho_B^x$ be the classical-quantum state, where $E_x$ is the $n$-by-$n$ diagonal matrix with the $x^{\text{th}}$ diagonal element equal to $1$ and all other diagonal elements equal to 0.


Given \(\rho \in \mathcal{D}(\mathcal{H}_B)\) and \(\sigma \in \mathcal{B}\left(\mathcal{H}_B\right)_{+}\), the Petz-R\'{e}nyi divergence of order \(\alpha\) is given by \cite{Petz1986}  
\[
D_{\alpha}(\rho \| \sigma) \coloneqq  
\begin{cases}  
\frac{1}{\alpha - 1} \log \operatorname{Tr}\left[\rho^{\alpha} \sigma^{1-\alpha}\right], & \text{if } \left(\supp(\rho) \subseteq \supp(\sigma)\right)\\
&\text{ or }\left(\alpha\in(0,1)
\text{ and }\supp(\rho)\cap\supp(\sigma)\neq \emptyset\right), \\  
\infty, &\text{otherwise },%& \text{if } \supp(\rho) \not\subseteq \supp(\sigma)\text{ and }\alpha\in(1,\infty),  
\end{cases}   
\]
for \(\alpha \in (0, 1) \cup (1, \infty)\), where we allow \(\sigma\) to have a trace not equal to 1.
%The function above is just the \emph{Petz-R\'enyi divergence} \cite{Petz1986}, where we allow $\sigma$ to have a trace not equal to $1$.
% Under the assumption that $\cup_{x:P_X(x)>0} \supp\left(\rho_B^x\right) = \mathcal{H}_B$,
The Petz-Augustin information of order $\alpha$ 
%for $\rho_{{X}B}$ 
is given by 
\begin{equation} \label{eq:Petz_Augustin}
    \min_{\sigma_B \in \mathcal{D}(\mathcal{H}_B)} f_{\alpha}(\sigma_B), \quad f_{\alpha} ( \sigma_B ) \coloneqq \mathbb{E}_{P_{X}}\left[{D_{\alpha}\left(\rho_B^{X} \vert\vert\sigma_B\right)}\right].
\end{equation}
Under the assumption that $\supp\left(\sum_{x:P_X(x)>0}\rho_B^x\right) = \mathcal{H}_B\setminus\set{0}$, the minimizer of this minimization problem exists \cite[Lemma IV.8]{Mosonyi2021} and is 
% a full-rank density matrix 
full-rank \cite[Lemma IV.11]{Mosonyi2021}.



%The reason why we consider the  Thompson metric instead of Hilbert's projective metric is that the convergence of the function values is more important than that of the iterates.
%Specifically, We show in Lemma \ref{ineq:SmallThomp2SmallOptError} and Theorem \ref{ineq:FvalConv} that the convergence guarantee of the iterates with respect to the Thompson metric can be translated into that of the function values.

%We only focus on the Thompson metric defined on the positive definite matrices.




\section{A Simple Iteration Rule and Its Convergence}
\label{sec:alg}
\subsection{A Simple Iteration Rule}
\label{subsec:CohenIteration}


Let $\alpha\in(0,1)\cup(1,\infty)$. 
Given $f_{\alpha}$, we define the operator
\begin{equation}
    \label{eq:CohenIteration}
    T_{f_{\alpha}} \colon \mathcal{B}\left(\mathcal{H}_B\right)_{++}\mapsto\mathcal{B}\left(\mathcal{H}_B\right)_{++}:U\mapsto\left(\mathbb{E}_{P_X}\left[\frac{ \left(\rho_B^X\right)^\alpha}{ \Tr\left[ \left(\rho_B^X\right)^\alpha U \right] }\right]\right)^{(1-\alpha)/\alpha}.
\end{equation}
We propose the following simple iteration rule for solving the optimization problem \eqref{eq:Petz_Augustin}:  
\begin{itemize}
\item Let $\sigma_B^{(1)}$ be a full-rank density matrix. 
\item For every $t \in \mathbb{N}$, compute $\sigma_B^{(t + 1)} = T_{f_{\alpha}} \left(\left( \sigma_B^{(t)}\right)^{1-\alpha} \right)^{1/(1-\alpha)}$, and output $\frac{\sigma_B^{(t + 1)}}{\Tr\left[\sigma_B^{(t + 1)}\right]}$. 
\end{itemize} 

To verify the well-definedness of the operator $T_{f_{\alpha}}$, we first observe that
for each $x\in\mathcal{X}$, since $\rho_B^x\neq 0$, the denominator $\Tr\left[ \left(\rho_B^x\right)^\alpha U \right]$ is finite and positive 
% , i.e.,
% $\Tr\left[ \left(\rho_B^x\right)^\alpha U \right]\in(0,\infty)$,
for any $U\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$.
Consequently, we have 
\begin{align}
    \supp\left(\frac{\left(\rho_B^x\right)^{\alpha}}{\Tr\left[ \left(\rho_B^x\right)^\alpha U \right]}\right)
    =\supp\left(\rho_B^x\right).
\end{align}
Given the assumption $\supp\left(\sum_{x:P_X(x)>0}\rho_B^x\right) = \mathcal{H}_B \setminus \set{0}$, it follows that the support of $T_{f_{\alpha}}(U)$ is equal to $\mathcal{H}_B\setminus\set{0}$. 
This ensures that the operator $T_{f_{\alpha}}$ is well-defined. 
To understand why we restrict the domain of the operator $T_{f_{\alpha}}$ to $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, suppose $U\in\mathcal{B}\left(\mathcal{H}_B\right)_{+}$ is not full rank.
In this case, the denominator $\Tr\left[ \left(\rho_B^x\right)^\alpha U \right]$ may become zero, rendering the definition of the operator $T_{f_{\alpha}}$ ill-defined.


% Recall that we are given $d$-by-$d$ density matrices $\rho_B^x$ for $x=1,2,\dots n$.
To evaluate the time complexity of the proposed iteration rule, we 
% write the iteration rule 
express it 
explicitly as follows:
\begin{align}
    \label{eq:DiscreteCP}
    \sigma_B^{(t + 1)} = \left(\sum_{x=1}^n P_X(x)\frac{ \left(\rho_B^x\right)^\alpha}{ \Tr\left[ \left(\rho_B^x\right)^\alpha \left(\sigma_B^{(t)}\right)^{1-\alpha} \right] }\right)^{1/\alpha}.
\end{align}
% For each $x\in\{1,2,\cdots,n\}$, t
The matrix powers $\left(\rho_B^x\right)^\alpha$ can be computed and stored before the first iteration begins.
%Thus, the per-iteration time complexity is dominated by the computation of the matrix power $(\sigma_B^{(t)})^{1-\alpha}$, 
%$n$ traces of matrix multiplications $\Tr\left[ (\rho_B^x)^\alpha (\sigma_B^{(t)})^{1-\alpha} \right]$, and the other matrix power $\left(\cdot\right)^{1/\alpha}$ taken outside the summation.
% For any $\sigma,\rho\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ and $r\in\mathbb{R}$, t
Given $\left(\rho_B^x\right)^\alpha$ for all $x \in \mathcal{X}$, each $\Tr\left[ \left(\rho_B^x\right)^\alpha \left(\sigma_B^{(t)}\right)^{1-\alpha} \right]$ can be computed in $O ( d^2 )$ time. 
Computing $\left(\sigma_B^{(t)}\right)^{1-\alpha}$ and raising a matrix to the power $(1 / \alpha)$ each require $O ( d^3 )$ time. 
%The computational complexities of computing 
%a matrix power 
%% the matrix power $\rho^r$ 
%and the trace of
%a 
%matrix multiplication 
%% $\Tr\left[\rho\sigma\right]$ 
%are $O(d^3)$ and $O(d^2)$, respectively.
%%Consequently, the initialization and per-iteration time complexities are $O\left(n d^{3}\right)$ and $O\left(d^{3}+n d^{2}\right)$, respectively.
Consequently, the initialization time complexity is $O\left(n d^{3}\right)$, and the per-iteration time complexity is $O\left(d^{3} + n d^{2}\right)$.

\subsection{Convergence Analysis}
\label{subsec:Conv}
%We note that, for $\alpha>1$, to the best of our knowledge, it is currently unknown whether the \emph{Augustin mean} is unique.
%Therefore, to 

Below, we present our main theorem.
\begin{theorem}
    \label{ineq:FvalConv}
    For any $\alpha\in(1/2,1)\cup(1,\infty)$, let $\Set{\sigma_B^{(t)}}_{t\in\mathbb{N}}$ be the sequence of iterates generated by our proposed iteration rule.
    Then, we have 
    \begin{align}
        d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\frac{\sigma_B^{(T+1)}}{\Tr\left[\sigma_B^{(T+1)}\right]}\right)^{1-\alpha}\right)
        \leq 2\left|1-\frac{1}{\alpha}\right|^{T} d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right),
    \end{align}
    and
    \begin{align}
        f_{\alpha}\left(\frac{\sigma_B^{(T+1)}}{\Tr\left[\sigma_B^{(T+1)}\right]}\right)-f_{\alpha}\left(\sigma_B^{\star}\right)
        \leq \left|\frac{2}{\alpha-1}\right|\cdot\left|1-\frac{1}{\alpha}\right|^{T} d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right),
    \end{align}
    where $\sigma_B^{\star}$ is the minimizer of the optimization problem \eqref{eq:Petz_Augustin}.
    Moreover, for $\alpha > 1$, the function values are non-increasing, i.e.,
    \begin{align}
        f_{\alpha}\left(\frac{\sigma_B^{(t+1)}}{\Tr\left[\sigma_B^{(t+1)}\right]}\right)
        \leq
        f_{\alpha}\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right),\quad\forall t\in\mathbb{N}.
    \end{align}
    Furthermore, the quantity $d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right)$ is bounded above.
\end{theorem}

% The proof of Theorem \ref{ineq:FvalConv} in Section \ref{subsec:PfMainThm} is based on the following key observations:
The proof of Theorem \ref{ineq:FvalConv}, which we defer to Section \ref{subsec:PfMainThm}, relies on the following observations.
\begin{itemize}
    \item % The operator $T_{f_{\alpha}}$ is contractive with respect to the Thompson metric (Section \ref{subsec:Contract}). 
    The operator $T_{f_{\alpha}}$ is contractive with a ratio of $| 1 - 1 / \alpha |$ in the Thompson metric (Section \ref{subsec:Contract}). 
    As a result, it has a unique fixed point, and the iterates $\sigma_B^{(t)}$ converge to this fixed point at a rate of $O\left(|1-1/\alpha|^T\right)$
    in the Thompson metric. 
    \item The unique fixed point of $T_{f_{\alpha}}$ 
    % is also 
    coincides with 
    the minimizer of the optimization problem \eqref{eq:Petz_Augustin} (see Section \ref{subsec:FixPt}).
    \item % The iterates $\sigma_B^{(t)}$ may fall outside the constraint set $\mathcal{D}\left(\mathcal{H}_B\right)$ of the optimization problem \eqref{eq:Petz_Augustin}.
%    Fortunately, we show that Thompson metric is stable under
%    trace 
%    normalization (Section \ref{subsec:NormalStable}).
	The iterates $\sigma_B^{(t)}$ may not be ``physical,'' in the sense that they may not have unit traces.
	Fortunately, we show that the Thompson metrics between the iterates and the minimizer are preserved under trace normalization, up to a multiplicative constant. 
    Therefore, the trace-normalized iterates still converge 
    to the minimizer
    at a rate of $O\left(|1-1/\alpha|^T\right)$.
    \item % The variation in the function values can be upper-bounded by the Thompson metric (Section \ref{subsec:ControlFval}); hence, the convergence guarantee of the iterates $\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]$ with respect to the Thompson metric can be translated into the convergence guarantee of the function values $f_{\alpha}\left(\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]\right)$.
    % The optimization error in function value can be upper-bounded by the Thompson metric between the iterates and the minimizer (see Section \ref{subsec:ControlFval}). 
    The variation in function values can be upper-bounded by the Thompson metric between the iterates and the minimizer (see Section \ref{subsec:ControlFval}). 
    Consequently, the above error bound in the Thompson metric translates into an error bound in function value.
\end{itemize}
\subsubsection{Contractivity of $T_{f_{\alpha}}$}
\label{subsec:Contract}
%We first prove that the operator $T_{f_{\alpha}}$ is contractive.
%\begin{lemma}(Fej\'{e}r Monotonicity)
\begin{lemma}[{Contractive Property}]
    \label{ineq:Contract}
    Let $\alpha\in(1/2,1)\cup(1,\infty)$. 
    For any $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, we have
    \begin{align}
        d_{\mathrm{T}}\left(T_{f_{\alpha}}(V),T_{f_{\alpha}}(U)\right)
        \leq \left|1-\frac{1}{\alpha}\right| d_{\mathrm{T}}\left(V,U\right).
    \end{align}
\end{lemma}
\begin{proof}
     By Definition \ref{eq:Thompson}, we have
    \begin{align}
        \exp\left(-d_{\mathrm{T}}\left(V,U\right)\right)V
        \leq U
        \leq \exp\left(d_{\mathrm{T}}\left(V,U\right)\right)V.\nonumber
    \end{align}
    Since $\rho_{B}^{X}\in\mathcal{D}\left(\mathcal{H}_B\right)$, we write
    \begin{align}
        T_{f_{\alpha}}\left(U\right)^{\alpha/(1-\alpha)}
        &=\mathbb{E}_{P_X}\left[\frac{ \left(\rho_B^X\right)^{\alpha}}{\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha}U\right]}\right]\nonumber\\
        &\geq\exp\left(- d_{\mathrm{T}}\left(V,U\right)\right)\mathbb{E}_{P_X}\left[\frac{ \left(\rho_B^X\right)^{\alpha}}{\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha}V\right]}\right]\nonumber\\
        &=\exp\left(-d_{\mathrm{T}}\left(V,U\right)\right)T_{f_{\alpha}}\left(V\right)^{\alpha/(1-\alpha)}.\nonumber
    \end{align}
    Similarly, we write
    \begin{align}
        T_{f_{\alpha}}\left(U\right)^{\alpha/(1-\alpha)}
        &\leq\exp\left(d_{\mathrm{T}}\left(V,U\right)\right)\mathbb{E}_{P_X}\left[\frac{ \left(\rho_B^X\right)^{\alpha}}{\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha}V\right]}\right]\nonumber\\
        &=\exp\left(d_{\mathrm{T}}\left(V,U\right)\right)T_{f_{\alpha}}\left(V\right)^{\alpha/(1-\alpha)}.\nonumber
    \end{align}
    By Definition \ref{eq:Thompson}, the two inequalities above imply that 
    \begin{align}
        d_{\mathrm{T}}\left(T_{f_{\alpha}}\left(V\right)^{\alpha/(1-\alpha)},T_{f_{\alpha}}\left(U\right)^{\alpha/(1-\alpha)}\right) \leq d_{\mathrm{T}} ( V, U ).
    \end{align}
    Then, by Lemma \ref{ineq:NegCurveProp}, we have
    \begin{align}
        d_{\mathrm{T}}\left(T_{f_{\alpha}}\left(V\right),T_{f_{\alpha}}\left(U\right)\right)
        &=d_{\mathrm{T}}\left(\left(T_{f_{\alpha}}\left(V\right)^{\alpha/(1-\alpha)}\right)^{(1-\alpha)/\alpha},\left(T_{f_{\alpha}}\left(U\right)^{\alpha/(1-\alpha)}\right)^{(1-\alpha)/\alpha}\right)\nonumber\\
        &\leq\left|\frac{1-\alpha}{\alpha}\right|d_{\mathrm{T}}\left(T_{f_{\alpha}}\left(V\right)^{\alpha/(1-\alpha)},T_{f_{\alpha}}\left(U\right)^{\alpha/(1-\alpha)}\right)\nonumber\\
        &\leq\left|1-\frac{1}{\alpha}\right|d_{\mathrm{T}}\left(V,U\right).\nonumber
    \end{align}
\end{proof}
\subsubsection{Fixed-Point Property of $T_{f_{\alpha}}$}
\label{subsec:FixPt}
\begin{lemma}
    \label{eq:FixPtIsMin}
    For any $\alpha  \in (0, 1) \cup (1, \infty)$, there exists a unique minimizer $\sigma_B^{\star}$ of the optimization problem \eqref{eq:Petz_Augustin}.
    Moreover, for the same $\sigma_B^{\star}$, $\left(\sigma_B^{\star}\right)^{1-\alpha}$ is the unique fixed point of the operator $T_{f_{\alpha}}$ for $\alpha\in(0,1)\cup(1,\infty)$.
\end{lemma}
%\begin{corollary}
%    \label{ineq:Fejer}
%    Consider $\alpha\in(1/2,1)\cup(1,\infty)$.
%    Let $\sigma_B^{\star}$ and $\sigma_B^{(t)}$ be as defined in Theorem \ref{ineq:FvalConv}.
%    Then, we have
%    \begin{align}
%        d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(T+1)}\right)^{1-\alpha}\right)
%        \leq\left|1-\frac{1}{\alpha}\right|^{T} d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{1}\right)^{1-\alpha}\right).
%    \end{align}
%\end{corollary}
The remainder of this section is devoted to the proof of Lemma \ref{eq:FixPtIsMin}.
For $\alpha\in(0,1)$, Lemma \ref{eq:FixPtIsMin} has been 
% proved 
proven 
by \citet[Proposition 2(b)]{Cheng2019}.
For $\alpha\in(1,\infty)$, the proof of Lemma \ref{eq:FixPtIsMin} relies on the following observations:
\begin{itemize}
    \item The traces of the iterates are always less than or equal to $1$ (Lemma \ref{ineq:TraceBound}).
    \item The function values are non-increasing (Lemma \ref{ineq:FvalDecrease}).
\end{itemize}
%\begin{lemma}[{\cite[Proposition 2]{Cheng2019}}]
%    \label{eq:ChengFix}
%    Let $\alpha\in(0,1)$.
%    There exists a unique minimizer $\sigma_B^{\star}$ of the optimization problem \eqref{eq:Petz_Augustin}.
%    Moreover, the following equation holds:
%    \begin{align}
%        \sigma_B^{\star}=\left(\mathbb{E}_{P_X}\left[\frac{\left(\rho_B^{X}\right)^{\alpha}}{\Tr\left[\left(\rho_B^{X}\right)^{\alpha}\left(\sigma_B^{\star}\right)^{1-\alpha}\right]}\right]\right)^{1/\alpha}.
%    \end{align}
%\end{lemma}

We will use Lemma \ref{ineq:Araki} and Lemma \ref{ineq:HolderEq} to prove Lemma \ref{ineq:TraceBound}.
% , both of which were established in earlier research. 
% Subsequently, using Lemma \ref{ineq:TraceBound}, we can establish Lemma \ref{ineq:FvalDecrease}.
\begin{lemma}[{Araki-Lieb-Thirring Inequality \cite{Araki1990}}]
\label{ineq:Araki}
For any $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$, we have 
\begin{align}
    \Tr\left[\left(V^{1/2}UV^{1/2}\right)^{sr}\right]\leq\Tr\left[\left(V^{r/2} U^r V^{r/2}\right)^{s}\right],
\end{align}
for all $s>0$ and $r\geq 1$.
\end{lemma}
\begin{lemma}[{H\"older Inequality \cite{Larotonda2018}}]
    \label{ineq:HolderEq}
    For any $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ and $p>1$, we have
    \begin{align}
        \Tr[UV]\leq\Tr[U^{p}]^{1/p}\Tr[V^{p/(p-1)}]^{1-1/p}.
    \end{align}
    Moreover, equality holds if and only if 
    \begin{align}
        \frac{U^p}{\Tr[U^p]}=\frac{V^{p/(p-1)}}{\Tr[V^{p/(p-1)}]}.
    \end{align}
\end{lemma}
% \begin{lemma}[{Bound of the Trace}]
\begin{lemma}[{Bound of Trace}]
\label{ineq:TraceBound}
For any $\alpha\in(1,\infty)$ and $\sigma_B\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $\Tr\left[\sigma_B\right]\leq 1$, we have 
\begin{align}
    \Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right]\leq 1.
\end{align}
Moreover, equality holds if and only if 
$\sigma_B$ is a fixed point of $T_{f_\alpha}\left((\cdot)^{1-\alpha}\right)^{1/(1-\alpha)}$ on $\mathcal{D} ( \mathcal{H}_B )$.
\end{lemma}

\begin{proof}
% Let $\sigma_B$ be as defined in Lemma \ref{ineq:TraceBound}.
Let $\sigma_B \in \mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $\Tr\left[\sigma_B\right]\leq 1$. 
Let $U = T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{\alpha/(1-\alpha)}$ and $V = \sigma_B^{1-\alpha}$.
Then, both $U$ and $V$ are positive definite, and we have 
% .
% We have 
\begin{align}
    \Tr \left[ UV \right]
    = \Tr\left[ \mathbb{E}_{P_X}\left[\frac{ (\rho_B^X)^\alpha}{ \Tr\left[ (\rho_B^X)^\alpha \sigma_B^{1-\alpha} \right] }\right] \sigma_B^{1-\alpha} \right]
    =1.\nonumber
\end{align}
Then, we write 
    \begin{align}
        \Tr\left[\left(T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)\right)^{1/(1-\alpha)}\right]
        &= \Tr\left[ U^{ 1/\alpha }\right]\\
        &= \Tr\left[ \left( V^{ 1/(2\alpha)} U^{ 1/\alpha } V^{ 1/(2\alpha) }\right) V^{ - 1/\alpha } \right]\\
        &\leq\Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha }  \Tr\left[ \left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} \right]^{ 1- 1/\alpha } \\
        &=\Tr\left[ \left( V^{  1/(2\alpha) } U^{1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha }  \Tr\left[ \sigma_B \right]^{ 1- 1/\alpha }\\
        &\leq \Tr\left[ \left( V^{ 1/(2\alpha)} U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha },
    \end{align}
    where the first inequality follows from the H\"older inequality (Lemma \ref{ineq:HolderEq}), 
    the third equality follows from the definition of $V$,
    and the last inequality follows from the assumption that $\Tr[\sigma_B] \leq 1$.

    Then, by the Araki-Lieb-Thirring inequality (Lemma \ref{ineq:Araki}), we have 
    \begin{align}
        \Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right] 
        \leq \Tr\left[ V^{1/2 } U V^{ 1/2 }\right]
        = \Tr\left[UV\right]= 1.
    \end{align} 
    Therefore, we conclude that 
    \begin{align}
        \Tr\left[\left(T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)\right)^{1/(1-\alpha)}\right]\leq 1.
    \end{align}
    
    We proceed to prove the if and only if condition. 
    Note that the ``if'' direction holds trivially. 
    It remains to prove the ``only if'' direction. 
%    To prove that the equality
%    holds if and only if
%    $\sigma_B$ is a fixed point of $T_{f_\alpha}\left((\cdot)^{1-\alpha}\right)^{1/(1-\alpha)}$ on $\mathcal{D} ( \mathcal{H}_B )$,
%    it suffices to verify the ``only if'' direction.
    Suppose that $\Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right]=1$.
    Let $U$ and $V$ be defined as above. 
    Recall that we have proved
    \begin{align}
        \Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right]
        &= \Tr\left[ \left( V^{ 1/(2\alpha)} U^{ 1/\alpha } V^{ 1/(2\alpha) }\right) V^{ - 1/\alpha } \right]\\
        &\leq\Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha }  \Tr\left[ \left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} \right]^{ 1- 1/\alpha } \\
        &=\Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha }  \Tr\left[ \sigma_B \right]^{ 1- 1/\alpha },
    \end{align}
    and
    \begin{align}
        \Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]\leq 1.
    \end{align}
    Since we have assumed that $\Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right]=1$ and $\Tr\left[ \sigma_B \right]\leq 1$,
    it must be the case that
    \begin{align}
        \Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]
        =\Tr\left[ \left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} \right]
        =\Tr[\sigma_B]
        =1,
    \end{align}
    and
    \begin{align}
        &\Tr\left[ \left( V^{ 1/(2\alpha)} U^{ 1/\alpha } V^{ 1/(2\alpha) }\right) V^{ - 1/\alpha } \right]\\
        &\quad=\Tr\left[ \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha \right]^{ 1/\alpha }  \Tr\left[ \left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} \right]^{ 1- 1/\alpha }.
    \end{align}    
    %It remains to verify that $T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}=\sigma_B$.
    Using the equality condition of the H\"{o}lder inequality (Lemma \ref{ineq:HolderEq}), the above equality implies
    \begin{align}
        \frac{\left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha}{\Tr\left[\left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha\right]}
        =\frac{\left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)}}{\Tr\left[ \left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} \right]},
    \end{align}
    where the denominators on both sides, as concluded above, are equal to $1$.
    Therefore, we have  
    \[
    \left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha=\left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)} . 
    \]
    Plugging in the definitions of $U$ and $V$, we get 
    \[
    T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}=\sigma_B . 
    \]
%    Moreover, by the definitions of $U$ and $V$, the following conditions are equivalent:
%    \begin{itemize}
%        \item $\left( V^{ 1/(2\alpha) } U^{ 1/\alpha } V^{ 1/(2\alpha) }\right)^\alpha=\left( V^{ -1/\alpha } \right)^{\alpha/(\alpha-1)}$
%        \item $T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}=\sigma_B$
%    \end{itemize}
    This completes the proof. 
    % Hence, we conclude that $T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}=\sigma_B$, provided that $\Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right]=1$.
    %Thus, we conclude that 
    %\begin{align}
    %    \Tr\left[T_{f_{\alpha}}\left(\sigma_B\right)\right]\leq 1.
    %\end{align}
\end{proof}


%Second, we prove that the function values $f_{\alpha}\left(\sigma_B^{(t)}\right)$ are non-increasing.
Next, using Lemma \ref{ineq:TraceBound}, we prove that the function values are non-increasing , as stated in Lemma \ref{ineq:FvalDecrease}.
\begin{lemma}[{Monotonicity of the Function Value}]
    \label{ineq:FvalDecrease}
    For any $\alpha\in(1,\infty)$ and $\sigma_B\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $\Tr\left[\sigma_B\right]\leq 1$, we have  
    \begin{align}
        f_{\alpha}\left(T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right)
        \leq f_{\alpha}(\sigma_B).
    \end{align}
    Moreover, equality holds if and only if $\sigma_B^{1 - \alpha}$ is a fixed point of $T_{f_\alpha}$.
\end{lemma}
\begin{proof}
    % Suppose that
    % $\sigma_B^{1 - \alpha}$ is not a fixed point of $T_{f_\alpha}$. 
    Let $\sigma_B \in \mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $\Tr\left[\sigma_B\right]\leq 1$. 
    We write
    \begin{align}
        &f_{\alpha}\left(T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{1/(1-\alpha)}\right)- f_{\alpha}(\sigma_B)\nonumber\\
        &\quad =\frac{1}{\alpha-1}\mathbb{E}_{P_X}\left[\log\left(\frac{ \Tr\left[ \left(\rho_B^X\right)^{\alpha}T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)\right]}{\Tr\left[ \left(\rho_B^X\right)^{\alpha}\sigma_B^{1-\alpha}\right]}\right)\right]\nonumber\\
        &\quad\leq\frac{1}{\alpha-1}\mathbb{E}_{P_X}\left[\frac{ \Tr\left[ \left(\rho_B^X\right)^{\alpha}T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)\right]}{\Tr\left[ \left(\rho_B^X\right)^{\alpha}\sigma_B^{1-\alpha}\right]}-1\right]\nonumber\\
        &\quad =\frac{1}{\alpha-1}\left(\Tr\left[T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)^{\alpha/(1-\alpha)}T_{f_{\alpha}}\left(\sigma_B^{1-\alpha}\right)\right]-1\right),\nonumber %\\
        % &\quad< 0,
    \end{align}
    where the first inequality exploits the fact that $\log x \leq x - 1$, 
    and 
    the second equality follows from the definition of the operator $T_{f_{\alpha}}$.
    % , and the second inequality follows from Lemma \ref{ineq:TraceBound}.
    % This concludes the proof.
    The lemma then follows from Lemma \ref{ineq:TraceBound}. 
\end{proof}

%Third, we prove that the optimal set $\mathcal{P}_{f_{\alpha}}$ is closed under the operator $T_{f_{\alpha}}$.
%\begin{lemma}[{Closedness of the Optimal Set}]
%    \label{eq:CloseOptSet}
%    Consider $\alpha\in(1,\infty)$.
%    For all $\sigma_B^{\star}$ in the optimal set $\mathcal{P}_{f_{\alpha}}$, we have
%    \begin{align}
%        T_{f_{\alpha}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\in\mathcal{P}_{f_{\alpha}}.
%    \end{align}
%\end{lemma}
Finally, we prove Lemma \ref{eq:FixPtIsMin} by showing that the unique fixed point of the operator $T_{f_{\alpha}}$ is also the minimizer of the optimization problem \eqref{eq:Petz_Augustin}.
\begin{proof}(Lemma \ref{eq:FixPtIsMin})
    By Lemma \ref{eq:ThompIsMetric},  Lemma \ref{ineq:Contract}, and the Banach fixed point theorem, there exists a unique $\sigma_B^{\star}\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $\left(\sigma_B^{\star}\right)^{1-\alpha}$ is the fixed point of the operator $T_{f_{\alpha}}$ for $\alpha\in(1/2,1)\cup(1,\infty)$.
    %By Lemma \ref{eq:ChengFix} and the definition of the operator $T_{f_{\alpha}}$, $\sigma_B^{\star}$ is also the unique minimizer of the optimization problem \eqref{eq:Petz_Augustin} for $\alpha\in(1/2,1)$.
    We recall that for $\alpha\in(0,1)$, Lemma \ref{eq:FixPtIsMin} has already been proved by \citet[Proposition 2(b)]{Cheng2019}.
    For $\alpha\in(1,\infty)$, let $\tilde{\sigma}_B^{\star}$ be the minimizer of the optimization problem \eqref{eq:Petz_Augustin}.
    Suppose that $\left(\tilde{\sigma}_B^{\star}\right)^{1-\alpha}$ is not the fixed point of $T_{f_{\alpha}}$.
    Then, the equality conditions in Lemma \ref{ineq:TraceBound} and Lemma \ref{ineq:FvalDecrease} do not hold, and we have 
    \begin{align}
        &f_{\alpha}\left(\frac{T_{f_{\alpha}}\left(\left(\tilde{\sigma}_B^{\star}\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\tilde{\sigma}_B^{\star}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}\right)\\
        &\quad=f_{\alpha}\left(T_{f_{\alpha}}\left(\left(\tilde{\sigma}_B^{\star}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right)+\log\left(\Tr\left[T_{f_{\alpha}}\left(\left(\tilde{\sigma}_B^{\star}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]\right)\\
        &\quad<f_{\alpha}\left(\tilde{\sigma}_B^{\star}\right).
    \end{align}
    %where the inequality follows from Lemma \ref{ineq:FvalDecrease} and Lemma \ref{ineq:TraceBound}.
    This inequality contradicts the optimality of $\tilde{\sigma}_B^{\star}$.
    Therefore, we conclude that $\sigma_B^{\star}=\tilde{\sigma}_B^{\star}$.
\end{proof}
\begin{remark}
    After completing our work, we happened to find that the fixed-point property proven by \citet[Proposition 4(c)]{Cheng2018}\footnote{Note that this result does not appear in the journal version \cite{Cheng2022}.} can also lead to the conclusion in Lemma \ref{eq:FixPtIsMin}.
    However, our proof strategy differs from theirs.
    Furthermore, for $\alpha > 1$, our proof strategy yields an additional useful property for implementing the proposed iteration rule: the function values are non-increasing along the iteration path (Lemma \ref{ineq:FvalDecrease}).
\end{remark}


%Second, we show that the unique fixed point of the operator $T_{\alpha}$ is the minimizer of the optimization problem \eqref{eq:Petz_Augustin}.
%\begin{lemma}[{\citet[Remark IV.15]{Mosonyi2021}}]
%    (Suspected! In \cite[Theorem IV.14]{Mosonyi2021}, they only prove it for $\alpha\in\left(0,2\right]$. They claim that hao-chung proved it in 2019. However, \citet{Cheng2024} claim that it was proven by \citet{Mosonyi2021}. Anyhow, I first trust them.)
%    Consider $\alpha\in(0,1)\cup(1,\infty)$.
%    Then, any $\sigma_B^{\star}$ is a minimizer of the optimization problem \eqref{eq:Petz_Augustin} if and only if it satisfies the following equation:
%    \begin{align}
%        \sigma_B^{\star}=\mathbb{E}_{P_X}\left[\frac{\left(\sigma_B^{\star}\right)^{(1-\alpha)/2}\left(\rho_B^{X}\right)^{\alpha}\left(\sigma_B^{\star}\right)^{(1-\alpha)/2}}{\Tr\left[\left(\rho_B^{X}\right)^{\alpha}\left(\sigma_B^{\star}\right)^{1-\alpha}\right]}\right].
%    \end{align}
%\end{lemma}

%\begin{corollary}(Fixed-Point Property)
%    \label{eq:FixedPt}
%    Consider $\alpha\in(1/2,1)\cup(1,\infty)$.
%    Then, minimizer $\sigma_B^{\star}$ of the optimization problem \eqref{eq:Petz_Augustin} uniquely exists, and satisfies the following equation:
%    \begin{align}
%        \left(\sigma_B^{\star}\right)^{1-\alpha}=T_{\alpha}\left(\left(\sigma_B^{\star}\right)^{1-\alpha}\right).
%    \end{align}
%\end{corollary}
% \subsubsection{Stability of Thompson Metric under Normalization}
\subsubsection{Preservation of Thompson Metric under Trace-Normalization}
\label{subsec:NormalStable}
The constraint set of the optimization problem \eqref{eq:Petz_Augustin} is the set of density matrices $\mathcal{D}\left(\mathcal{H}_B\right)$, whereas the traces of the iterates $\sigma_B^{(t)}$ may not equal $1$.
To address this, we show in Lemma \ref{ineq:SmallNormalThompson} that the Thompson metric is 
% stable 
preserved
under 
% normalization.
trace-normalization, up to a multiplicative constant of $2$. 

\begin{lemma}
    \label{ineq:SmallNormalThompson}
    Let $\alpha\in(0,1)\cup(1,\infty)$.
    For any $U,V\in\mathcal{B}\left( \mathcal{H}_B\right)_{++}$ such that $\Tr[V]=1$, we have
    \begin{align}
        d_{\mathrm{T}}\left(V^{1-\alpha},\left(\frac{U}{\Tr[U]}\right)^{1-\alpha}\right)
        \leq 2 d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right).
    \end{align}
\end{lemma}

% To prove Lemma \ref{ineq:SmallNormalThompson}, we rely on Lemma \ref{ineq:Lidskii}, which was established in earlier research.
We will use 
% Lemma \ref{ineq:ThompsonLogHomo}  and 
Lemma \ref{ineq:Lidskii} to prove Lemma \ref{ineq:SmallNormalThompson}. 

\begin{lemma}[{\cite[Corollary 7.7.4(c)]{Horn2013}}]
    \label{ineq:Lidskii}
    For any $U,V\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ such that $U\leq V$, we have
    \begin{align}
        \lambda_i(U)\leq\lambda_i(V),\quad\forall i\in\set{1,2,\dots,d}.
    \end{align}
\end{lemma}
\begin{proof}(Lemma \ref{ineq:SmallNormalThompson})
    Let $U,V$ be as defined in Lemma \ref{ineq:SmallNormalThompson}.
    By Lemma \ref{ineq:ThompsonLogHomo}, we write 
    \begin{align}
        d_{\mathrm{T}}\left(V^{1-\alpha},\left(\frac{U}{\Tr[U]}\right)^{1-\alpha}\right)\leq d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right)+|\alpha-1|\cdot\left|\log\left(\Tr[U]\right)\right|.
    \end{align}
    It remains to bound the quantity $\Tr[U]$, which can be written as follows:
    \begin{align}
        \Tr[U]
        =\sum_{i=1}^{d}\left(\lambda_i(U)^{1-\alpha}\right)^{1/(1-\alpha)}
        =\sum_{i=1}^{d}\left(\lambda_i(U^{1-\alpha})\right)^{1/(1-\alpha)}.
    \end{align}
    On the other hand, by Lemma \ref{ineq:Lidskii}, for each $i\in\set{1,2,\dots,d}$, we have
    \begin{align}
        \exp\left(- d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)\right)\lambda_i\left(V^{1-\alpha}\right)
        \leq \lambda_i\left(U^{1-\alpha}\right)
        \leq\exp\left( d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)\right)\lambda_i\left(V^{1-\alpha}\right).
    \end{align}
    Consequently, we obtain
    \begin{align}
        \exp\left( \frac{-d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)}{|1-\alpha|}\right)\Tr[V]
        \leq\Tr[U]
        \leq \exp\left( \frac{d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)}{|1-\alpha|}\right)\Tr[V].
    \end{align}
    Since we assume that $\Tr[V]=1$, it follows that
    \begin{align}
        %\left|\log\left(\Tr[U]\right)\right|
        \abs{\log\left(\Tr[U]\right)}
        \leq\left|\frac{1}{\alpha-1}\right|d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right).
    \end{align}
    This concludes the proof.
\end{proof}



% \subsubsection{Optimization Error in Function Value}
\subsubsection{Bounding Variation in Function Values}
\label{subsec:ControlFval}
Finally, it remains to translate the convergence guarantee of the iterates into that of the function values.
We prove that the difference between the function values is bounded above by the Thompson metric.
\begin{lemma}
    \label{ineq:SmallThomp2SmallOptError}
    Let $\alpha\in(0,1)\cup(1,\infty)$.
    For any $U,V\in\mathcal{B}\left( \mathcal{H}_B\right)_{++}$, we have
    \begin{align}
        f_{\alpha}\left(U\right)-f_{\alpha}\left(V\right)\leq \left|\frac{1}{\alpha-1}\right|d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right).
    \end{align}
\end{lemma}
\begin{proof}(Lemma \ref{ineq:SmallThomp2SmallOptError})
    %We first assume $\alpha>1$.
    Let $U,V$ be as defined in Lemma \ref{ineq:SmallThomp2SmallOptError}.
    By Definition \ref{eq:Thompson}, we have
    \begin{align}
        \exp\left(- d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)\right)V^{1-\alpha}
        \leq U^{1-\alpha}
        \leq\exp\left( d_{\mathrm{T}}
        \left(V^{1-\alpha},U^{1-\alpha}\right)\right)V^{1-\alpha}.
    \end{align}
    Therefore, we write
    \begin{align}
        f_{\alpha}\left(U\right)
        &=\mathbb{E}_{P_X}\left[\frac{1}{\alpha-1}\log\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha} U^{1-\alpha}\right]\right]\nonumber\\
        &\leq\mathbb{E}_{P_X}\left[\frac{1}{\alpha-1}\log\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha}\left(\exp\left( d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right)\right)V^{1-\alpha}\right)\right]\right]\nonumber\\
        &=f_{\alpha}\left(V\right)+\frac{1}{\alpha-1}d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right)\nonumber
    \end{align}
    for $\alpha>1$, and
    \begin{align}
        f_{\alpha}\left(U\right)
        &\leq\mathbb{E}_{P_X}\left[\frac{1}{\alpha-1}\log\operatorname{Tr}\left[ \left(\rho_B^X\right)^{\alpha}\left(\exp\left( -d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right)\right)V^{1-\alpha}\right)\right]\right]\nonumber\\
        &=f_{\alpha}\left(V\right)+\frac{1}{1-\alpha} d_{\mathrm{T}}\left(V^{1-\alpha},U^{1-\alpha}\right)\nonumber
    \end{align}
    for $\alpha<1$.
    %where the last equality follows from the log-homogeneity of the function $f_{\alpha}$.
    This concludes the proof.
    %for $\alpha>1$.
    
    %By the same argument, the inequality also holds for $\alpha\in(0,1)$.
\end{proof}

\subsubsection{Proof of the Main Theorem}
\label{subsec:PfMainThm}
\begin{proof}(Theorem \ref{ineq:FvalConv})
    Let $\sigma_B^{\star}$ and $\sigma_B^{(t)}$ be defined as in Theorem \ref{ineq:FvalConv}.
    By Lemma \ref{ineq:Contract}, Lemma \ref{eq:FixPtIsMin}, and induction, we write
    \begin{align}
        d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(T+1)}\right)^{1-\alpha}\right) 
        \leq\left|1-\frac{1}{\alpha}\right|^T d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right).
    \end{align}
    Consequently, by Lemma \ref{ineq:SmallNormalThompson}, we obtain
    \begin{align}
        d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\frac{\sigma_B^{(T+1)}}{\Tr\left[\sigma_B^{(T+1)}\right]}\right)^{1-\alpha}\right) 
        \leq 2\left|1-\frac{1}{\alpha}\right|^T d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right).
    \end{align}
    By Lemma \ref{ineq:SmallThomp2SmallOptError}, it follows that
    \begin{align}
        f_{\alpha}\left(\frac{\sigma_B^{(T+1)}}{\Tr\left[\sigma_B^{(T+1)}\right]}\right)-f_{\alpha}\left(\sigma_B^{\star}\right)
        \leq \left|\frac{2}{\alpha-1}\right|\cdot \left|1-\frac{1}{\alpha}\right|^T d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right).
    \end{align}
    It remains to prove that
    \begin{align}
        f_{\alpha}\left(\frac{\sigma_B^{(t+1)}}{\Tr\left[\sigma_B^{(t+1)}\right]}
        \right)
        \leq f_{\alpha}\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right),\quad\forall\alpha>1.
    \end{align} 
    By the monotonicity of the function value (Lemma \ref{ineq:FvalDecrease}) and the bound of the trace (Lemma \ref{ineq:TraceBound}), we write
    \begin{align}
        &f_{\alpha}\left(\frac{T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}\right)\\
        &\quad=f_{\alpha}\left(T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right)+\log\left(\Tr\left[T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]\right)\\
        &\quad\leq f_{\alpha}\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)+0,\quad\forall\alpha>1.
    \end{align}
    It remains to prove that
    \begin{align}
        \frac{\sigma_B^{(t+1)}}{\Tr\left[\sigma_B^{(t+1)}\right]}
        = \frac{T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}.
    \end{align}
    Note that for any $\sigma\in\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ and $\gamma>0$, we have
    \begin{align}
        T_{f_{\alpha}}\left(\left(\gamma\sigma\right)^{1-\alpha}\right)^{1/(1-\alpha)}
        =\gamma^{(\alpha-1)/\alpha} T_{f_{\alpha}}\left(\sigma^{1-\alpha}\right)^{1/(1-\alpha)},
    \end{align}
    and thus,
    \begin{align}
        \frac{T_{f_{\alpha}}\left(\left(\gamma\sigma\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\gamma\sigma\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}
        =\frac{T_{f_{\alpha}}\left(\sigma^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\sigma^{1-\alpha}\right)^{1/(1-\alpha)}\right]}.
    \end{align}
    This implies that, by taking $\sigma=\sigma_B^{(t)}$ and $\gamma=1/\Tr\left[\sigma_B^{(t)}\right]$,
    \begin{align}
        \frac{\sigma_B^{(t+1)}}{\Tr\left[\sigma_B^{(t+1)}\right]}
        =\frac{T_{f_{\alpha}}\left(\left(\sigma_B^{(t)}\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\sigma_B^{(t)}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}
        = \frac{T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}}{\Tr\left[T_{f_{\alpha}}\left(\left(\frac{\sigma_B^{(t)}}{\Tr\left[\sigma_B^{(t)}\right]}\right)^{1-\alpha}\right)^{1/(1-\alpha)}\right]}.
    \end{align}
    
    Finally, since $\left(\sigma_B^{(1)}\right)^{1-\alpha}$ and $\left(\sigma_B^{\star}\right)^{1-\alpha}$ are full-rank density matrices, and the Thompson metric is a metric on $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ (Lemma \ref{eq:ThompIsMetric}), the quantity $d_{\mathrm{T}}\left(\left(\sigma_B^{\star}\right)^{1-\alpha},\left(\sigma_B^{(1)}\right)^{1-\alpha}\right)$ is finite. 
    This concludes the proof.
\end{proof}

\section{Interesting Connection with $\ell_p$-Lewis Weights}
\label{subsec:RelationWithCP}
Our iteration rule, introduced in Section \ref{subsec:CohenIteration}, is inspired by the iteration rule 
%due to 
proposed by
\citet{Cohen2015} for computing the $\ell_p$-Lewis weights. 
% the computation of $\ell_p$-Lewis weight in \cite{Cohen2015}. % which involves computing
To this end, we introduce the $\ell_p$-Lewis weights and discuss their connection to our work.
%The following paragraphs in this section are organized as follows:
%\begin{itemize}
%    \item First, we introduce the optimization problem defining the $\ell_p$-Lewis weights (Problem \eqref{eq:LewisDef}), along with the iteration rule proposed by \citet{Cohen2015} for solving it (Equation \eqref{eq:CPIterDef}).
%    \item Second, to demonstrate the similarity between the $\ell_p$-Lewis weights and the Petz-Augustin information, we reformulate a special case of the optimization problem defining the $\ell_p$-Lewis weights (Problem \eqref{eq:ReWrittenLewis}) and the corresponding iteration rule (Equation \eqref{eq:RawCohenIteration}).
%    \item Finally, we highlight the challenges in adapting the algorithm proposed by \citet{Cohen2015} to compute the Petz-Augustin information.
%\end{itemize}

The $\ell_p$-Lewis weights are given by \cite{Lee2020}
\begin{equation}
    \label{eq:LewisDef}
    w^{\star}\in\argmin_{w\in m \Delta_{d-1}}\frac{-1}{1-\frac{2}{p}}\log\det\left(A^{\intercal}\mathrm{Diag}\left(w^{1-2/p}\right)A\right),
\end{equation}
for $p\in(0,\infty)$, where $A$ is a $d$-by-$m$ real matrix and $A^\intercal$ denotes the transpose of $A$.
\citet{Cohen2015} proposed the following iteration rule: 
\begin{equation}
    \label{eq:CPIterDef}
    w^{(t+1)}=T_{\mathrm{CP}}\left(w^{(t)}\right),
\end{equation}
where
\begin{align}
    T_{\mathrm{CP}}\left(w^{(t)}\right)[i]\coloneqq\left(A_i^{\intercal}\left(A^{\intercal}\mathrm{Diag}\left(\left(w^{(t)}\right)^{1-2/p}\right)A\right)^{-1}A_i\right)^{p/2},
\end{align}
for each $i\in\set{1,2,\dots,d}$.
They proved that 
% their 
the 
iterates 
% $\mathrm{Diag}\left(w_t\right)$ 
$w^{(t)}$
converge linearly to 
% $\mathrm{Diag}\left(w_{\star}\right)$ 
$w^{\star}$
with respect to the Thompson metric for $p\in(0,4)$.

To demonstrate the similarity between their proposed iteration rule and ours (Equation \eqref{eq:DiscreteCP}), we note that, when $m=1$, the $\ell_p$-Lewis weights can be written as 
\begin{equation}
    \label{eq:ReWrittenLewis}
    w^{\star}\in\argmin_{w\in \Delta_{d-1}}\frac{-1}{1-\frac{2}{p}}\log\left(\Tr\left[\mathrm{Diag}\left(a\right)^{2/p}\mathrm{Diag}\left(w\right)^{1-2/p}\right]\right),
\end{equation}
where $a$ is a $d$-dimensional vector with $a[i]=|A_i|^p$ for each $i\in\set{1,2,\dots,d}$.
Let $\alpha=2/p$. 
Then, the problem corresponds to a special case of the optimization problem \eqref{eq:Petz_Augustin} where all 
% the 
matrices commute.
%, and the cardinality of the input alphabet is $1$. 
In this case, 
% their algorithm 
the iteration rule proposed by \citet{Cohen2015}
can be expressed as
\begin{equation}
    \label{eq:RawCohenIteration}
    \mathrm{Diag}\left(w^{(t+1)}\right)=\left(\frac{\mathrm{Diag}\left(a\right)^{\alpha}}{\Tr\left[\mathrm{Diag}\left(a\right)^{\alpha}\mathrm{Diag}\left(w^{(t)}\right)^{1-\alpha}\right]}\right)^{1/\alpha} , 
\end{equation}
where we deliberately write the iterates as matrices to illustrate the similarity with our iteration rule \eqref{eq:DiscreteCP}. 
%Therefore, our algorithm can be viewed as a generalization of the algorithm proposed by \citet{Cohen2015}.


Despite the similarity in algorithms, our results cannot be trivially derived from the work of \citet{Cohen2015}.
Specifically, there are two main challenges in adapting their proposed algorithm to compute the Petz–Augustin information:
\begin{itemize}
    \item The optimization problem \eqref{eq:Petz_Augustin} defining the Petz–Augustin information includes an additional expectation term, $\mathbb{E}_{P_X}\left[\cdot\right]$.
    %, taken over the input alphabet $X$.
    Consequently, even when all the matrices commute, the computation of the Petz–Augustin information cannot be reduced to that of the $\ell_p$-Lewis weights.
    \item \citet{Cohen2015} established the contractive property of their proposed algorithm with respect to the Thompson metric by proving
    \begin{align}
    \log\left(\max\Set{\frac{T_{\mathrm{CP}}(u)[i]}{T_{\mathrm{CP}}(v)[i]}, \frac{T_{\mathrm{CP}}(v)[i]}{T_{\mathrm{CP}}(u)[i]}}\right)\leq\left|1-\frac{p}{2}\right|\log\left(\max\Set{\frac{u[i]}{v[i]}, \frac{v[i]}{u[i]}}\right),
    \end{align}
    for each $1 \leq i \leq d$.
    This can be viewed as comparing the eigenvalues of the commuting matrices $\mathrm{Diag}\left(u\right)$ and $\mathrm{Diag}\left(v\right)$.
    However, in our case, the iterates $\sigma_B^{(t)}$ and the minimizer $\sigma_B^{\star}$, as defined in Theorem \ref{ineq:FvalConv}, may not commute, making it infeasible to compare their eigenvalues within matched eigenspaces.
    %\item In our case, we aim to analyze the convergence of the function values $f_{\alpha}(\sigma_B^{(t)})$, but it is unclear whether this can be achieved based on the convergence guarantee of the iterates $\sigma_B^{(t)}$.
\end{itemize}

For the first challenge, we identify an appropriate generalization of the algorithm proposed by \citet{Cohen2015}, leading to a new algorithm introduced in Section \ref{subsec:CohenIteration} for computing the Petz-Augustin information.

For the second challenge, we leverage the properties of the Thompson metric specialized for $\mathcal{B}\left(\mathcal{H}_B\right)_{++}$ (Lemma \ref{eq:ThompIsMetric}, \ref{ineq:NegCurveProp} and \ref{ineq:ThompsonLogHomo}).
Specifically, in Lemma \ref{ineq:Contract}, we show that our proposed algorithm is a contraction with respect to the Thompson metric for $\alpha\in(1/2,1)\cup(1,\infty)$.

%For the third challenge, we prove in Lemma \ref{ineq:SmallThomp2SmallOptError} and Theorem \ref{ineq:FvalConv} that the convergence guarantee of the iterates $\sigma_B^{(t)}$ with respect to the Thompson metric can be translated into the convergence guarantee of the function values $f_{\alpha}(\sigma_B^{(t)})$.


\section{Numerical Results} \label{sec:numerics}

% \begin{figure}[H]
\begin{figure}[ht]
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_0.8.png}
      \caption{$\alpha=0.8$}
      \label{fig:sfig1}
    \end{subfigure}%
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_1.5.png}
      \caption{$\alpha=1.5$}
      \label{fig:sfig2}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_3.png}
      \caption{$\alpha=3$}
      \label{fig:sfig3}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_5.png}
      \caption{$\alpha=5$}
      \label{fig:sfig4}
    \end{subfigure}
    \caption{Approximate optimization error versus the number of iterations for $\alpha>0.5$}
    \label{fig1}
\end{figure}

% \begin{figure}[H]
\begin{figure}[ht]
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_0.2.png}
      \caption{$\alpha=0.2$}
      \label{fig:sfig5}
    \end{subfigure}
    \begin{subfigure}{.5\textwidth}
      \centering
      \includegraphics[width=.8\linewidth]{figure_alpha_0.4.png}
      \caption{$\alpha=0.4$}
      \label{fig:sfig6}
    \end{subfigure}
    \caption{Approximate optimization error versus the number of iterations for $\alpha\leq 0.5$}
    \label{fig2}
\end{figure}
We implement our proposed 
% algorithm 
iteration rule 
described in Section \ref{subsec:CohenIteration} to compute the Petz-Augustin information for $\alpha$ in $\set{0.2,0.4,0.8,1.5,3,5}$. 
The source code is available on GitHub\footnote{\url{https://github.com/chunnengchu/PetzAugustin/}}.
Throughout the experiments for $\alpha>0.5$, we set the cardinality of $\mathcal{X}$ to $2^{5}$ and the dimension of $\mathcal{H}_B$ to $2^{7}$. 
The quantum states $ \rho_B^x $ are generated using the 
% $\href{https://qutip.org/docs/3.1.0/guide/guide-random.html}{rand\_dm}$ 
\texttt{rand\_dm}
function from the 
% PYTHON 
Python 
package QuTiP \citep{Johansson2012}.
Since the exact solution of the optimization problem \eqref{eq:Petz_Augustin} is unavailable, we compute an approximate optimization error instead. 
The approximate optimization error is defined as $f_{\alpha}\left(\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]\right)-\hat{f}^{\star}$, where $\hat{f}^\star$ denotes the function value of the last iterate after $30$ iterations of our proposed algorithm.
%The approximate optimization error at the $t^{\text{th}}$ iteration is computed as 
%% $f_{\alpha}\left(\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]\right)-f^{\star}$, 
%$f_{\alpha}\left(\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]\right)-\hat{f}_{\star}$, 
%where 
%% $f^{\star}$ 
%$\hat{f}_\star$
%denotes the function value of the last iterate after $30$ iterations of our proposed algorithm. 

% For $\alpha\leq 0.5$, 
For experiments with $\alpha \leq 0.5$, we manually design a challenging instance of the optimization problem \eqref{eq:Petz_Augustin}, 
% which is described in detail in \texttt{README.md} in the 
detailed in the \texttt{README.md} file in the 
% same 
aforementioned 
GitHub repository. 
The approximate optimization error is defined similarly as above. 
%Similarly, the approximate optimization error at the $t^{\text{th}}$ iteration is computed as $f_{\alpha}\left(\sigma_B^{(t)}/\Tr\left[\sigma_B^{(t)}\right]\right)-f^{\star}$.
However, our proposed algorithm is not guaranteed to converge for $\alpha\leq 0.5$.
Hence, for $\alpha\leq 0.5$, we 
% denote $f^{\star}$ as 
replace $\hat{f}^{\star}$ with 
the function value of the best iterate over $30$ iterations of entropic mirror descent with the Polyak step size \cite{You2022}, as this method is guaranteed to converge asymptotically and is known to converge quickly in practice.
% And the 


In Figure \ref{fig1}, we observe linear convergence rates for the optimization error when $\alpha>0.5$.
Notably, Theorem \ref{ineq:FvalConv} establishes that the exponent of the linear convergence rate is bounded above by $\left|1 - \frac{1}{\alpha}\right|$.
Consistent with this result, Figure \ref{fig1} demonstrates a similar relationship between $\alpha$ and the
empirical 
convergence rate.

%Our convergence guarantee in Theorem \ref{ineq:FvalConv} does not cover the case where $\alpha\in(0,1)$, which appears in characterizing the sphere-packing exponent of classical-quantum channel coding \cite{Dalai2014, Cheng2019}.
Since Lemma \ref{eq:FixPtIsMin} implies that our algorithm functions as a fixed-point iteration for $\alpha \in (0, 1)\cup(1,\infty)$, despite the fact that our convergence guarantee in Theorem \ref{ineq:FvalConv} does not cover the case where $\alpha \leq 0.5$, we present in Figure \ref{fig2} the experimental results for $\alpha \in \set{0.2, 0.4}$. 
Numerical experiments suggest that our proposed algorithm seems to diverge for $\alpha\in\set{0.2,0.4}$ on the aforementioned synthetic instance of the optimization problem \eqref{eq:Petz_Augustin}.
%Additionally, we observe that the algorithm generates iterates with extremely large or small entries after a few iterations. 
%This behavior causes the function values to exceed Python's representable numerical range.
Since no existing algorithm for computing the Petz-Augustin information of order $\alpha\in\left(0,1/2\right]$ has a non-asymptotic convergence guarantee, developing a rigorous algorithm for this purpose remains an open direction for future research.
%Additionally, the valleys in the middle of Figures \ref{fig:sfig5} and \ref{fig:sfig6} indicate that the smallest function values are encountered in the first few iterations, respectively.

\section{Acknowledgements}

This work is supported by the Young Scholar Fellowship (Einstein Program) of the National Science and Technology Council (NSTC) of Taiwan under grant number NSTC 112-2636-E-002-003; the 2030 Cross-Generation Young Scholars Program (Excellent Young Scholars) of the NSTC under grant number NSTC 112-2628-E-002-019-MY3; the research project “Geometry of Quantum Learning and Optimization” of National Taiwan University under grant number NTU-CC-114L895006; and the Academic Career Development Research Program (Laurel Research Project) of National Taiwan University under grant number NTU-CDP-114L7744.

%\bibliographystyle{alpha}
\bibliography{refs}
%\input{main.bbl}

%\appendix


\end{document}