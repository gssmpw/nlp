\section{Related Work}
\noindent Recent progress in end-to-end speech systems have markedly improved human-AI audio interaction. Early approaches relied on cascaded ASR-LLM-TTS pipelines~\citep{huang2023audiogptunderstandinggeneratingspeech}, where distinct modules for speech recognition, language modeling, and speech synthesis are sequentially connected. However, these systems suffered from latency buildup, error propagation, and disjointed optimization. Later approaches sought to enhance integration by directly linking speech encoders to LLMs through trainable adapters~\citep{kong2020pannslargescalepretrainedaudio, chu2024qwen2audiotechnicalreport, das2024speechverselargescalegeneralizableaudio}, though they still required separate TTS modules for audio output. 

\bigskip
\noindent The emergence of fully end-to-end systems marked a paradigm shift. Architectures like Llama-Omni~\citep{fang2024llamaomniseamlessspeechinteraction} integrated non-autoregressive~(NAR) TTS modules with language models, using connectionist temporal classification (CTC) loss. Freeze-Omni~\citep{wang2024freezeomnismartlowlatency} uses a combination of autoregressive and NAR speech decoders. These systems demonstrated improved latency but exhibited limitations in handling emotional nuance and natural conversational flow. MinMo~\citep{chen2025minmomultimodallargelanguage} introduced autoregressive speech token prediction through the CosyVoice2~\citep{du2024cosyvoice2scalablestreaming} decoder, while interleaved modeling approaches~\citep{zeng2024glm4voiceintelligenthumanlikeendtoend, nguyen2024spiritlminterleavedspoken} alternated between text and speech token generation at the sequence level.

\bigskip
\noindent Parallel decoding architectures like Moshi~\citep{2024moshispeechtextfoundationmodel} and Mini-Omni~\citep{xie2024miniomnilanguagemodelshear} represented a significant leap by generating text and multiple speech codebook tokens simultaneously. These systems achieved lower latency through compressed speech token sequences but faced challenges in preserving linguistic capabilities when scaling speech token bandwidth. Current systems generally specialized in specific aspects: GLM-4-Voice~\citep{zeng2024glm4voiceintelligenthumanlikeendtoend} prioritized latency reduction, while Moshi emphasized speech quality, but none holistically addressed emotion awareness, conversational naturalness, and real-time knowledge integration.

\bigskip
\noindent Recent methodological advances have systematically investigated emotion-aware interaction paradigms, though their integration with multi-modal frameworks remains nascent. While some systems~\citep{wang2024freezeomnismartlowlatency} incorporated basic sentiment analysis, they lacked bidirectional emotional resonance-neither detecting paralinguistic cues in user speech nor generating contextually appropriate emotional responses. The naturalness gap persisted due to LLMs' tendency toward verbose, text-optimized outputs~\citep{fang2024llamaomniseamlessspeechinteraction}, ill-suited for spoken dialogue. Recent work has introduced task-specific optimizations: LUCY~\citep{gao2025lucylinguisticunderstandingcontrol} adopted the architectural framework of Mini-Omni~\citep{xie2024miniomnilanguagemodelshear}, augmented with specialized fine-tuning on conversational datasets for emotion control and function-calling.

%********************************%
%***********SECTION 2************%
%********************************%