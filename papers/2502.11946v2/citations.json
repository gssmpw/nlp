[
  {
    "index": 0,
    "papers": [
      {
        "key": "huang2023audiogptunderstandinggeneratingspeech",
        "author": "Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others",
        "title": "Audiogpt: Understanding and generating speech, music, sound, and talking head"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kong2020pannslargescalepretrainedaudio",
        "author": "Kong, Qiuqiang and Cao, Yin and Iqbal, Turab and Wang, Yuxuan and Wang, Wenwu and Plumbley, Mark D",
        "title": "Panns: Large-scale pretrained audio neural networks for audio pattern recognition"
      },
      {
        "key": "chu2024qwen2audiotechnicalreport",
        "author": "Chu, Yunfei and Xu, Jin and Yang, Qian and Wei, Haojie and Wei, Xipin and Guo, Zhifang and Leng, Yichong and Lv, Yuanjun and He, Jinzheng and Lin, Junyang and others",
        "title": "Qwen2-audio technical report"
      },
      {
        "key": "das2024speechverselargescalegeneralizableaudio",
        "author": "Das, Nilaksh and Dingliwal, Saket and Ronanki, Srikanth and Paturi, Rohit and Huang, Zhaocheng and Mathur, Prashant and Yuan, Jie and Bekal, Dhanush and Niu, Xing and Jayanthi, Sai Muralidhar and others",
        "title": "Speechverse: A large-scale generalizable audio language model"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "fang2024llamaomniseamlessspeechinteraction",
        "author": "Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang",
        "title": "Llama-omni: Seamless speech interaction with large language models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wang2024freezeomnismartlowlatency",
        "author": "Wang, Xiong and Li, Yangze and Fu, Chaoyou and Shen, Yunhang and Xie, Lei and Li, Ke and Sun, Xing and Ma, Long",
        "title": "Freeze-omni: A smart and low latency speech-to-speech dialogue model with frozen llm"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "chen2025minmomultimodallargelanguage",
        "author": "Chen, Qian and Chen, Yafeng and Chen, Yanni and Chen, Mengzhe and Chen, Yingda and Deng, Chong and Du, Zhihao and Gao, Ruize and Gao, Changfeng and Gao, Zhifu and others",
        "title": "Minmo: A multimodal large language model for seamless voice interaction"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "du2024cosyvoice2scalablestreaming",
        "author": "Du, Zhihao and Wang, Yuxuan and Chen, Qian and Shi, Xian and Lv, Xiang and Zhao, Tianyu and Gao, Zhifu and Yang, Yexin and Gao, Changfeng and Wang, Hui and others",
        "title": "Cosyvoice 2: Scalable streaming speech synthesis with large language models"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "zeng2024glm4voiceintelligenthumanlikeendtoend",
        "author": "Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie",
        "title": "Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot"
      },
      {
        "key": "nguyen2024spiritlminterleavedspoken",
        "author": "Nguyen, Tu Anh and Muller, Benjamin and Yu, Bokai and Costa-Jussa, Marta R and Elbayad, Maha and Popuri, Sravya and Duquenne, Paul-Ambroise and Algayres, Robin and Mavlyutov, Ruslan and Gat, Itai and others",
        "title": "Spirit-lm: Interleaved spoken and written language model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "2024moshispeechtextfoundationmodel",
        "author": "D{\\'e}fossez, Alexandre and Mazar{\\'e}, Laurent and Orsini, Manu and Royer, Am{\\'e}lie and P{\\'e}rez, Patrick and J{\\'e}gou, Herv{\\'e} and Grave, Edouard and Zeghidour, Neil",
        "title": "Moshi: a speech-text foundation model for real-time dialogue"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xie2024miniomnilanguagemodelshear",
        "author": "Xie, Zhifei and Wu, Changqiao",
        "title": "Mini-omni: Language models can hear, talk while thinking in streaming"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zeng2024glm4voiceintelligenthumanlikeendtoend",
        "author": "Zeng, Aohan and Du, Zhengxiao and Liu, Mingdao and Wang, Kedong and Jiang, Shengmin and Zhao, Lei and Dong, Yuxiao and Tang, Jie",
        "title": "Glm-4-voice: Towards intelligent and human-like end-to-end spoken chatbot"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2024freezeomnismartlowlatency",
        "author": "Wang, Xiong and Li, Yangze and Fu, Chaoyou and Shen, Yunhang and Xie, Lei and Li, Ke and Sun, Xing and Ma, Long",
        "title": "Freeze-omni: A smart and low latency speech-to-speech dialogue model with frozen llm"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "fang2024llamaomniseamlessspeechinteraction",
        "author": "Fang, Qingkai and Guo, Shoutao and Zhou, Yan and Ma, Zhengrui and Zhang, Shaolei and Feng, Yang",
        "title": "Llama-omni: Seamless speech interaction with large language models"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "gao2025lucylinguisticunderstandingcontrol",
        "author": "Gao, Heting and Shao, Hang and Wang, Xiong and Qiu, Chaofan and Shen, Yunhang and Cai, Siqi and Shi, Yuchen and Xu, Zihan and Long, Zuwei and Zhang, Yike and others",
        "title": "LUCY: Linguistic Understanding and Control Yielding Early Stage of Her"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "xie2024miniomnilanguagemodelshear",
        "author": "Xie, Zhifei and Wu, Changqiao",
        "title": "Mini-omni: Language models can hear, talk while thinking in streaming"
      }
    ]
  }
]