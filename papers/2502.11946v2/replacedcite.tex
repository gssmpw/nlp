\section{Related Work}
\noindent Recent progress in end-to-end speech systems have markedly improved human-AI audio interaction. Early approaches relied on cascaded ASR-LLM-TTS pipelines____, where distinct modules for speech recognition, language modeling, and speech synthesis are sequentially connected. However, these systems suffered from latency buildup, error propagation, and disjointed optimization. Later approaches sought to enhance integration by directly linking speech encoders to LLMs through trainable adapters____, though they still required separate TTS modules for audio output. 

\bigskip
\noindent The emergence of fully end-to-end systems marked a paradigm shift. Architectures like Llama-Omni____ integrated non-autoregressive~(NAR) TTS modules with language models, using connectionist temporal classification (CTC) loss. Freeze-Omni____ uses a combination of autoregressive and NAR speech decoders. These systems demonstrated improved latency but exhibited limitations in handling emotional nuance and natural conversational flow. MinMo____ introduced autoregressive speech token prediction through the CosyVoice2____ decoder, while interleaved modeling approaches____ alternated between text and speech token generation at the sequence level.

\bigskip
\noindent Parallel decoding architectures like Moshi____ and Mini-Omni____ represented a significant leap by generating text and multiple speech codebook tokens simultaneously. These systems achieved lower latency through compressed speech token sequences but faced challenges in preserving linguistic capabilities when scaling speech token bandwidth. Current systems generally specialized in specific aspects: GLM-4-Voice____ prioritized latency reduction, while Moshi emphasized speech quality, but none holistically addressed emotion awareness, conversational naturalness, and real-time knowledge integration.

\bigskip
\noindent Recent methodological advances have systematically investigated emotion-aware interaction paradigms, though their integration with multi-modal frameworks remains nascent. While some systems____ incorporated basic sentiment analysis, they lacked bidirectional emotional resonance-neither detecting paralinguistic cues in user speech nor generating contextually appropriate emotional responses. The naturalness gap persisted due to LLMs' tendency toward verbose, text-optimized outputs____, ill-suited for spoken dialogue. Recent work has introduced task-specific optimizations: LUCY____ adopted the architectural framework of Mini-Omni____, augmented with specialized fine-tuning on conversational datasets for emotion control and function-calling.

%********************************%
%***********SECTION 2************%
%********************************%