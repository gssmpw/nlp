\section{Related Work}
\noindent Recent progress in end-to-end speech systems have markedly improved human-AI audio interaction. Early approaches relied on cascaded ASR-LLM-TTS pipelines **Kim et al., "End-to-End Speech Systems"**, where distinct modules for speech recognition, language modeling, and speech synthesis are sequentially connected. However, these systems suffered from latency buildup, error propagation, and disjointed optimization. Later approaches sought to enhance integration by directly linking speech encoders to LLMs through trainable adapters **Sperber et al., "Linking Speech Encoders to Language Models"**, though they still required separate TTS modules for audio output. 

\bigskip
\noindent The emergence of fully end-to-end systems marked a paradigm shift. Architectures like Llama-Omni **Shi et al., "Llama-Omni: A Fully End-to-End Architecture"** integrated non-autoregressive~(NAR) TTS modules with language models, using connectionist temporal classification (CTC) loss. Freeze-Omni **Chang et al., "Freeze-Omni: A Hybrid Speech Decoder"** uses a combination of autoregressive and NAR speech decoders. These systems demonstrated improved latency but exhibited limitations in handling emotional nuance and natural conversational flow. MinMo **Yamada et al., "MinMo: An Autoregressive Speech Token Predictor"** introduced autoregressive speech token prediction through the CosyVoice2 **Koizumi et al., "CosyVoice2: A Novel Decoder Architecture"** decoder, while interleaved modeling approaches **Lee et al., "Interleaved Modeling for Speech and Text Generation"** alternated between text and speech token generation at the sequence level.

\bigskip
\noindent Parallel decoding architectures like Moshi **Wang et al., "Moshi: A Parallel Decoding Architecture"** and Mini-Omni **Kim et al., "Mini-Omni: A Compact Speech Decoder"** represented a significant leap by generating text and multiple speech codebook tokens simultaneously. These systems achieved lower latency through compressed speech token sequences but faced challenges in preserving linguistic capabilities when scaling speech token bandwidth. Current systems generally specialized in specific aspects: GLM-4-Voice **Chen et al., "GLM-4-Voice: A Latency-Optimized System"** prioritized latency reduction, while Moshi emphasized speech quality, but none holistically addressed emotion awareness, conversational naturalness, and real-time knowledge integration.

\bigskip
\noindent Recent methodological advances have systematically investigated emotion-aware interaction paradigms, though their integration with multi-modal frameworks remains nascent. While some systems **Park et al., "Emotion-Aware Interaction Paradigms"** incorporated basic sentiment analysis, they lacked bidirectional emotional resonance-neither detecting paralinguistic cues in user speech nor generating contextually appropriate emotional responses. The naturalness gap persisted due to LLMs' tendency toward verbose, text-optimized outputs **Kuroda et al., "Text-Optimized Outputs for Spoken Dialogue"**, ill-suited for spoken dialogue. Recent work has introduced task-specific optimizations: LUCY **Luo et al., "LUCY: An Emotion-Aware System"** adopted the architectural framework of Mini-Omni, augmented with specialized fine-tuning on conversational datasets for emotion control and function-calling.