\subsection{A Dual Algorithm}\label{sec:dual_algorithm}

The gradient descent method is a classical method to solve unconstrained convex optimization problems. To use this method to solve \eqref{eq:dual_problem}, we need to establish conditions under which $D(\lambda)$ is differentiable and also obtain an expression for the gradient of $D(\lambda)$. Lemma \ref{lem:dual_subgradient} gives us an expression for the subdifferential of $D(\lambda)$. Because $D(\lambda)$ is a convex function, the subdifferential set is nonempty at all points. $D(\lambda)$ is differentiable precisely at those points where the subdifferential set has a unique element. The lemma follows immediately from Danskin's theorem, also known as the envelope theorem. (See Appendix B of \cite{bertsekas1999nonlinear} for the precise statement and proof of Danskin's theorem).

\begin{lemma}\label{lem:dual_subgradient} 
Let $D(\lambda)$ be the function as defined in \eqref{eq:dual_function}. The subdifferential set of $D(\lambda)$ is given by
\[\partial D(\lambda) = \left\{ \nabla_\lambda L(f, \lambda): f \in F(\lambda) \right\} = \left\{ -Rf: f \in F(\lambda) \right\}\]
where $F(\lambda) \triangleq \arg \max_{f \in A} L(f, \lambda)$ is the set of all flow vectors that maximize the Lagrangian, given $\lambda$.
\end{lemma}

In Section \ref{sec:convergence}, we show that as long as the regularizer coefficient $\eta_{i,j}$ is strictly positive for all $(i,j) \in \mathcal{N}$, $F(\lambda)$ has a unique value for all $\lambda \in \mathbb{R}^E$. By Lemma \ref{lem:dual_subgradient}, $D(\lambda)$ is differentiable everywhere whenever this condition holds.


The gradient descent algorithm to solve the dual problem is presented below. Initialize the algorithm by setting $\lambda[0]$ to be the zero vector. For every $t \in \mathbb{N}$, set
\begin{equation}\label{eq:algorithm}\tag{$\mathbf{A}$}
\begin{split}
    f[t] &= \arg \max_{f \in A} L(f, \lambda[t]) \\
    \lambda[t+1] &= \lambda[t] + \gamma Rf[t]
\end{split}
\end{equation}
Here, $\gamma$ is a strictly positive stepsize parameter in the algorithm that remains constant for all time. 
In each iteration $t$, the flows $f[t]$ are set so as to maximize the Lagrangian, given the current values of $\lambda [t]$, while the Lagrange multipliers $\lambda[t+1]$ are updated in a direction opposite to the gradient of $D(\lambda[t])$. In case there is more than one value of $f$ that maximizes $L(f, \lambda[t])$, we can set $f[t]$ to any such value. In this case, algorithm \eqref{eq:algorithm} is equivalent to the subgradient method applied to $D(\lambda)$. Under appropriate conditions, we expect $\lambda[t]$ to approach a solution of \eqref{eq:dual_problem}, and consequently, $f[t]$ to approach a solution of \eqref{eq:primal_problem}. We defer the convergence analysis of \eqref{eq:algorithm} to Section \ref{sec:convergence}. For now, we explain how \eqref{eq:algorithm} can be implemented as a network protocol in a PCN.
