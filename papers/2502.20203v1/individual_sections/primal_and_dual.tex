\subsection{A Network Utility Maximization Problem}
%The DEBT control protocol's objective is to converge to a detailed-balanced flow $f$ that maximizes the sum of utilities of all pairs of transacting nodes, subject to the constraint that the flows don't exceed the demands. Here, we extend the notion of utilities from individual transactions (introduced in Section \ref{sec:transaction_requests} to flow rates). 
To formulate the protocol's objective in mathematical terms, we introduce some notation. Let $\mathcal{N}$ denote the set of transacting node-pairs, \textit{i.e.}, $\mathcal{N} = \{(i,j): a_{i,j} > 0\}$.
Let $U(f) = \sum_{(i,j) \in \mathcal{N}} U_{i,j}(\totalflow_{i,j})$ denote the total utility of all transacting node-pairs as a function of a stationary flow $f$. 
Define a \textit{feasible flow} to be any flow that meets both the demand constraints and the detailed balance constraints (\textit{i.e.}, the condition $Rf = 0$).

{As we will see later, it will be important in our proofs that the flows in the network respond smoothly to the prices. For this reason,} we add a quadratic regularizer term, $H(f)$, to the utility function, where
\begin{equation}\label{eq:entropy_regularizer}
    H(f) \triangleq -\sum\nolimits_{(i,j) \in \mathcal{N}}\eta_{i,j} \sum\nolimits_{k=1}^{|P_{i,j}|} (f_{i,j,k})^2.
\end{equation}
Here, $\eta_{i,j}$ is a non-negative scalar term that controls the weight of the regularizer.
One interpretation of the regularizer is that it is an incentive for splitting the flow among different competitive paths (see Section \ref{sec:prices} for details). Define $U(f) + H(f)$ to be the \textit{net utility} of a particular flow $f$.

The protocol's goal is to find a feasible stationary flow that maximizes the net utility of the payment channel network. In mathematical terms, this can be expressed as obtaining a solution to the following optimization problem:
% \begin{itemize}
%     \item The total flow from $i$ to $j$, $f_{i,j}$, is less than or equal to $a_{i,j}$. We call this the demand constraint.
%     \item The net flow along any channel in the network is zero, i.e., $(Rf)_{u,v} = 0$ for all channels $(u,v)$. We call this the detailed balance constraint.
% \end{itemize}
% Recall that $f = (f_{i,j,k})$ is the vector of flows on all paths between all transacting nodes while $f_{i,j} = \sum_{k}f_{i,j,k}$ is the total flow from $i$ to $j$. With the above notation, the problem of our interest can be succinctly written as:
% \begin{align}\label{eq:primal_problem}
%     \max_{f \geq 0} \quad &\sum_{(i,j) \in \mathcal{N}} U_{i,j}\left(f_{i,j}\right) \\ 
%     \text{s.t.} \quad &g = 0 \label{eq:detailed_balance_constraints} \\
%     & f_{i,j} \leq a_{i,j} \quad \forall \ (i,j) \in V \times V \label{eq:demand_constraints}
% \end{align}
\begin{equation}\label{eq:primal_problem}
\tag{$\mathbf{P}$}
\begin{split}
    \max_{f \in A} \quad & U(f) + H(f)\\ 
    \text{s.t.} \quad &Rf = 0 
\end{split}
\end{equation}
The symbol $\eqref{eq:primal_problem}$ denotes that the optimization problem presented above is the \textit{primal} (or original) problem. Let $f^*$ denote any solution to this problem.

Observe that the set of feasible flows is a compact, convex set. Moreover, it is nonempty for any problem parameters, since the empty flow ($f = 0$) is a feasible flow. Therefore, a solution to $\eqref{eq:primal_problem}$ always exists. Also note that \eqref{eq:primal_problem} is a convex optimization problem, since both $U(f)$ and $H(f)$ are concave and the constraint set is convex. Lastly, if all $\eta_{i,j}$ are strictly positive, then the objective function is strongly concave. This ensures that $f^*$ is unique.

\subsection{The Dual Problem}\label{sec:dual_problem}
% We now present an %algorithm to solve the optimization problem $\eqref{eq:primal_problem}$. The first step in deriving this algorithm is to consider 
% alternate, equivalent form of the primal problem that does not explicitly have the detailed balance constraint. The technique of relaxing constraints using \textit{Lagrange multipliers}, which we now present, is a standard technique in the field of optimization (see, e.g., \cite{bertsekas1999nonlinear}). 
The primal problem \eqref{eq:primal_problem} is hard to solve because of the detailed balance constraints. With the aid of Lagrange multipliers, we derive its \textit{dual problem} that does not explicitly have these constraints. (See \cite{bertsekas1999nonlinear} for an exposition on Lagrange multipliers and duality).
Let $\lambda_{u,v}$ denote the Lagrange multiplier for the constraint $(Rf)_{u,v} = 0$; let $\lambda \in \mathbb{R}^E$ denote the vector of all such terms. Define the Lagrangian of the problem $\eqref{eq:primal_problem}$ by
\begin{align}\label{eq:lagrangian}
    L(f, \lambda) \triangleq U(f) + H(f) - \lambda^TRf.
\end{align}

Using the Lagrangian, we can formulate an equivalent form of the problem \eqref{eq:primal_problem} as follows:
\begin{align}\label{eq:lagrangian_max_min}
    &\max_{f \in A} \  \inf_{\lambda \in \mathbb{R}^E}\quad L(f, \lambda) \nonumber \\ 
    = &\max_{f \in A} \  \inf_{\lambda \in \mathbb{R}^E}\quad  U(f) + H(f) - \lambda^TRf
\end{align}
% Let us denote the inner optimization problem by \eqref{eq:primal_inner_problem}:
% \begin{equation}\label{eq:primal_inner_problem}\tag{$\mathbf{P^*}(f)$}
%     \inf_{\lambda \in \mathbb{R}^E}\quad  U(f) + H(f) - \lambda^TRf.
% \end{equation}
Observe that \eqref{eq:lagrangian_max_min} is equivalent to \eqref{eq:primal_problem} because 
%consider the problem \eqref{eq:primal_inner_problem}. Unless $Rf = 0$, the value of \eqref{eq:primal_inner_problem} is $-\infty$. Therefore, to solve \eqref{eq:lagrangian_max_min}, 
$f$ must be chosen such that $Rf = 0$ if \eqref{eq:lagrangian_max_min} is to have a finite value. 
%However, under this constraint, the value of \eqref{eq:primal_inner_problem} is equal to the objective function in \eqref{eq:primal_problem}.

The dual of the optimization problem \eqref{eq:primal_problem} %(or equivalently, of \eqref{eq:lagrangian_max_min}) 
is obtained by changing the order of minimization and maximization in \eqref{eq:lagrangian_max_min}:
\begin{align}\label{eq:dual_problem}
    &\inf_{\lambda \in \mathbb{R}^E} \  \max_{f \in A}\quad L(f, \lambda) 
    = \inf_{\lambda \in \mathbb{R}^E} D(\lambda), \tag{$\mathbf{D}$}
\end{align}
%The advantage of the formulation in \eqref{eq:lagrangian_max_min} is that since the detailed balance constraints are not explicit, we can consider a gradient-based algorithm to solve the optimization problem in \eqref{eq:lagrangian_max_min}. To elaborate, we can begin with some arbitrary initial flows (say $f[0]$), solve \eqref{eq:primal_inner_problem} with $f = f[0]$ to obtain $\lambda[0]$, and then take a step in the direction of $\nabla_f L(f[0], \lambda[0])$ to obtain $f[1]$. The algorithm iterates until convergence. In the literature, such an algorithm is called a \textit{primal algorithm} \cite{kelly1998rate, srikant2013communication}. In some cases, as in our problem, the solution of \eqref{eq:primal_inner_problem} for arbitrary $f$ may take values $+\infty$ or $-\infty$. To avoid these extremities, one can introduce a regularizer for $\lambda$. By carefully updating the regularizer along with the variables, the algorithm is guaranteed to converge. Such algorithms are termed as the \textit{method of multipliers} (see \cite{bertsekas1996constrained}).
%
%In this work, we adopt a different strategy. We invoke the notion of the \textit{dual} of the primal optimization problem. The dual function is defined as follows:
where $D(\lambda)$, called the \textit{dual function}, is defined as follows:
\begin{align}\label{eq:dual_function}
    D(\lambda) \triangleq \max_{f \in A} U(f) + H(f) - \lambda^TRf \ \  \forall \, \lambda \, \in \, \mathbb{R}^{E}.
\end{align}
For any $\lambda$, $L(f, \lambda)$ is finite for all $f \in A$, because $A$ is a bounded set. Therefore, $D(\lambda)$ is well-defined for all $\lambda \in \mathbb{R}^{E}$. 

Observe that the dual function is a convex function of $\lambda$ \cite{bertsekas1999nonlinear} and that the dual problem has no constraints. This implies that the dual problem is easy to solve. In Section \ref{sec:convergence}, we show how a solution of \eqref{eq:dual_problem} yields a solution of \eqref{eq:primal_problem}.


%Moreover, $D(\lambda)$ is a convex function of $\lambda$. %Moreover, the dual problem has a finite minimum value. \textcolor{red}{Later, we show that the dual problem always has a finite solution.}

% Note that \eqref{eq:primal_problem} is a convex optimization problem with linear equality constraints and a bounded, polyhedral domain (composed of linear inequality constraints). Thus, there is a strong relation between the primal problem \eqref{eq:primal_problem} and its dual \eqref{eq:dual_problem} (see \cite{bertsekas1989parallel}, Appendix C). In particular, we note the following theorem, which will be useful later in relating the optimal solutions of \eqref{eq:dual_problem} and \eqref{eq:primal_problem}.

% \begin{theorem}[Saddle Point Theorem (adapted from \cite{bertsekas1989parallel})]\label{thm:saddle_point}
% For $f^*$ to be an optimal solution of \eqref{eq:primal_problem} and $\lambda^*$ to be an optimal solution of \eqref{eq:dual_problem}, it is necessary and sufficient that $f^* \in A$ and 
% \begin{align} \label{eq:optimality_conditions_2}\tag{$\mathbf{S}$}
%     L(f^*, \lambda) \geq L(f^*, \lambda^*) \geq L(f, \lambda^*) \ \forall \ \lambda \in \mathbb{R}^{E}, \ \forall \ f \in A.
% \end{align}
% \end{theorem}

% Equation \eqref{eq:optimality_conditions_2} is the definition of a saddle-point of $L(f, \lambda)$. Thus, the above statement says that  $f^*$ is primal optimal and $\lambda^*$ is dual optimal if and only if $(f^*, \lambda^*)$ is a saddle-point for the Lagrangian.


% \begin{itemize}
%     \item $f^*$ be primal feasible (satisfies balance constraints and demand constraints) and 
%     \begin{align} \label{eq:optimality_conditions_1}
%         \sum_{(i,j) \in \mathcal{N}} U_{i,j}\left(f^*_{i,j}\right) &= \max_{f \in A} L(f, \lambda^*) \\
%         &= \max_{f \in A} \sum_{(i,j) \in \mathcal{N}} U_{i,j}\left(f_{i,j}\right) - {\lambda^*}^TRf
%     \end{align}
%     Equivalently,
%     \item $f^* \in A$ and 
%     \begin{align} \label{eq:optimality_conditions_2}
%         L(f^*, \lambda) \geq L(f^*, \lambda^*) \geq L(f, \lambda^*) \ \forall \ \lambda \in \mathbb{R}^{E}, \ \forall \ f \in A
%     \end{align}
% \end{itemize}
