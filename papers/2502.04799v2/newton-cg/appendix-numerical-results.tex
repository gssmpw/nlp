
\section{Additional numerical results} \label{sec:appendix/numerical-results}


This section provides a detailed description of the experimental setup and additional results to supplement \Cref{sec:main/numerical}.
We implement our algorithm in MATLAB R2023a 
and denote the variant using the first regularizer in \theoremref{thm:newton-local-rate-boosted} as \algname{ARNCG}$_g$,
and the variant using the second regularizer as \algname{ARNCG}$_\epsilon$. 
We use the official Julia implementation provided by \citet{hamad2024simple} for their \algname{CAT}.\footnote{See \url{https://github.com/fadihamad94/CAT-Journal}.}
As the code for \algname{AN2CER} is not publicly available, we investigate several ways to implement it in MATLAB and report the best results, as detailed in \Cref{sec:appendix/implementation-details}.

Our experimental settings follow those described by \citet{hamad2024simple}, we conduct all experiments in a single-threaded environment on a machine running Ubuntu Server 22.04, equipped with dual-socket Intel(R) Xeon(R) Silver 4210 CPUs 
and 192 GB of RAM.
Each socket is installed with three 32 GB RAM modules, running at 2400 MHz.
The algorithm is considered successful if it terminates when $\epsilon_k \leq  \epsilon =  10^{-5}$ such that $k \leq 10^5$. If the algorithm fails to terminate within 5 hours, it is also recorded as a failure.

We evaluate these algorithms using the standard CUTEst benchmark for nonlinear optimization~\citep{gould2015cutest}.
Specifically, we consider all unconstrained problems with more than 100 variables that are commonly available through the Julia and MATLAB interfaces%
\footnote{See \url{https://github.com/JuliaSmoothOptimizers/CUTEst.jl} for the Julia interface, and \url{https://github.com/matcutest/matcutest} for the MATLAB interface.}
of this benchmark,
comprising a total of 124 problems.
The dimensions of these problems range from 100 to 123200.

\subsection{Implementation details} \label{sec:appendix/implementation-details}
\paragraph{\algname{ARNCG}}
The initial point for each problem is provided by the benchmark itself.
Other parameters of \Cref{alg:adap-newton-cg} are set as follows:
\begin{align*}
\mu = 0.3,
\beta = 0.5,
\tau_- = 0.3, 
\tau = \tau_+ = 1.0,
\gamma = 5,
M_0 = 1 
\text{ and } 
\eta = 0.01.
\end{align*}
We consider two choices for $m_{\mathrm{max}}$:
\begin{enumerate}
    \item Setting $m_{\mathrm{max}} = 1$ so that at most 4 function evaluations per each iteration.
    \item Setting $m_{\mathrm{max}} = \lfloor \log_\beta 10^{-8} \rfloor$ to be the smallest integer such that $\beta^{m_{\mathrm{max}}+1} > 10^{-8}$.
\end{enumerate}
In our experiments, we find that $m_{\mathrm{max}} = 1$ works well, 
and the algorithm is not sensitive to the above parameters, so we do not perform further fine-tuning.
In the implementation of \texttt{CappedCG}, we do not keep the historical iterations to save memory.
Instead, we evaluate \eqref{eqn:capped-cg-slow-decay-condition} by regenerating the iterations. 
In practice, we observe that step \eqref{eqn:capped-cg-slow-decay-condition} is triggered very infrequently, resulting in minimal computational overhead.
The \texttt{TERM} state is primarily designed to ensure theoretical guarantees for Hessian-vector products in \theoremref{sec:appendix/oracle-complexity-proof}, 
and we find it is not triggered in practice. 
Since the termination condition of \texttt{CappedCG} using the error $\|r_k\| \leq \hat \xi \|r_0\|$ may not be appropriate for a large $\|r_0\|$, 
we instead require it to satisfy  $\|r_k\| \leq \min(\hat \xi \|r_0\|, 0.01)$.

The fallback step in the main loop of \Cref{alg:adap-newton-cg} is mainly designed for theoretical considerations, as described in \lemmaref{lem:main/transition-between-subsequences-give-valid-regularizer}.
It ensures that an abrupt increase in the gradient norm followed by a sudden drop does not compromise the validity of this lemma
but results in a wasted iteration.
However, we note that this condition can be relaxed to the following to enhance practical performance:
\begin{align}
    \label{eqn:appendix/fallback-relaxed}
    \lambda g_{k + \frac{1}{2}} >  g_k
    \text{ and }
    g_k \leq \lambda g_{k-1}, \text{ for } \lambda \in (0, 1].
\end{align}
When $\lambda = 1$,
this condition reduces to the original one. In our experiments, we explore the choices of 
$\lambda = 1$, $\lambda = 0.01$, and the impact of removing the fallback step (i.e., $\lambda = 0$).
Moreover, we note that when $\theta = 0$, the fallback step and the trial step are identical so the choices of $\lambda$ do not affect the results.
In practice, we suggest setting a small $\lambda$ or removing the fallback step. 

We also terminate the algorithm and \emph{mark it as a failure} if both the function value and gradient norm remain unchanged for 20 iterations 
or if the current search direction satisfies $\| d_k \| \leq 2 \times 10^{-16}$, 
or if the Lipschitz constant estimation satisfies $M_k \geq 10^{40}$,
as these scenarios may indicate numerical issues.
\figureref{fig:main-algoperf} in the main text is generated under the above settings with $\lambda = 0$ and $m_{\mathrm{max}} = 1$.

For the Hessian evaluations, we only access it through the Hessian-vector products, 
and count the evaluation number as the number of iterations minus the number of the linesearch failures.
Since when a linesearch failure occurs, the next point is the same as the current point and does not increase the oracle complexity of Hessian evaluations.

\paragraph{\algname{AN2CER}}
Our implementation follows the algorithm described in \citet[Section~2]{gratton2024yet}, with parameters adopted from their suggested values.  
The algorithm first attempts to solve the regularized Newton equation using the regularizer $\sqrt{\kappa_a M_k g_k}$.
If this attempt fails, the minimal eigenvalue $\lambda_{\mathrm{min}}(\nabla^2\varphi(x_k))$ is computed.  
The algorithm then switches to the regularizer $\sqrt{M_k g_k} + [-\lambda_{\mathrm{min}}(\nabla^2\varphi(x_k))]_+$ when $\lambda_{\mathrm{min}}(\nabla^2\varphi(x_k)) > \kappa_C \sqrt{M_k g_k}$, and directly uses the corresponding eigenvector otherwise.

In AN2CER, the authors suggest using Cholesky factorization to solve the Newton equation and invoking the full eigendecomposition (i.e., the \texttt{eig} function in MATLAB) to find the minimal eigenvalue when the factorization fails.  
We observe that, in the current benchmark, it is more efficient to use \texttt{CappedCG} as the equation solver and compute the minimal eigenvalue using MATLAB's \texttt{eigs} function when \texttt{NC} is returned.  
This modification preserves the success rate and oracle evaluations of the original implementation while significantly reducing computational cost.  
We also note that there are several variants of AN2CER in \citet{gratton2024yet}, and we find that the current version yields the best results among them.




\subsection{Results on the CUTEst benchmark}



Following \citet{hamad2024simple}, we report the shifted geometric mean%
\footnote{For a dataset $\{ a_i \}_{i\in[k]}$, the shifted geometric mean is defined as  $\exp\left( \frac{1}{k} \sum_{i=1}^k \log (a_i + 1)  \right)$, which accounts for cases where $a_i = 0$.} 
of Hessian, gradient and function evaluations, as well as the elapsed time in \tableref{tab:appendix-comparision-fallback,tab:appendix-comparision-theta}.
In our algorithm, we define normalized Hessian-vector products as the original products divided by the problem dimension $n$,
which can be interpreted as the fraction of information about the Hessian that is revealed to the algorithm;
the linesearch failure rate is the fraction of iterations that exceed the maximum allowed steps $m_{\mathrm{max}}$; 
and the second linesearch rate measures the fraction of times the linesearch rule \eqref{eqn:newton-cg-sol-decay-smaller-stepsize} is invoked.
The medians of these metrics are provided in \tableref{tab:appendix-comparision-fallback-median,tab:appendix-comparision-theta-median}.
The success rate as a function of oracle evaluations is plotted in \figureref{fig:appendix-comparision-fallback,fig:appendix-comparision-theta}.
When an algorithm fails, the elapsed time is recorded as twice the time limit (i.e., 10 hours), and the oracle evaluations are recorded as twice the iteration limit (i.e., $2 \times 10^5$).
We note that the choices for handling failure cases in the reported metrics of these tables may affect the relative comparison of results with different success rates,
although they follow the convention from previous works.
Therefore, we suggest that readers also focus on the figures for a detailed analysis of each algorithm's behavior.


\paragraph{The fallback parameter}
From \tableref{tab:appendix-comparision-fallback,tab:appendix-comparision-fallback-median} and \figureref{fig:appendix-comparision-fallback}, 
we observe that the choice of the fallback parameter $\lambda$ in \eqref{eqn:appendix/fallback-relaxed} 
does not significantly affect the success rate, 
and the overall performance remains similar across different values of $\lambda$.  
For larger $\lambda$, the fallback step is generally triggered more frequently (as indicated by the ``fallback rate''), leading to increased computational time and oracle evaluations.
Interestingly, ARNCG$_\epsilon$ with $m_{\mathrm{max}} = 1$ seems an exception that $\lambda = 1$ is beneficial for specific problems and gives a slightly higher success rate.

\paragraph{The regularization coefficients}
\tableref{tab:appendix-comparision-theta,tab:appendix-comparision-theta-median} and \figureref{fig:appendix-comparision-theta} present comparisons for different values of $\theta$.  
As $\theta$ increases, the performance initially improves but then declines.
Larger $\theta$ imposes stricter tolerance requirements on \texttt{CappedCG} (as indicated by the number of Hessian-vector products in these tables), 
and increases computational costs,
while smaller $\theta$ may lead to a slower local convergence.
Thus, we recommend choosing $\theta \in [0.5, 1]$ to balance computational efficiency and local behavior.

We also note that this tolerance requirement is designed for local convergence and is not necessary for global complexity,
so there may be room for improvement.
For example, we can use a fixed tolerance $\eta$ when the current gradient norm is larger than a threshold, and switch to the current choice $\min( \eta, \sqrt{M_k} \omega_k)$ otherwise.
We leave this for future exploration.

Although ARNCG$_g$ has a slightly higher worst-case complexity (by a double-logarithmic factor) than ARNCG$_\epsilon$, 
they exhibit similar empirical performance, and in some cases, ARNCG$_g$ even performs better.  

A potential failure case \emph{in practice} for ARNCG$_\epsilon$ occurs when the iteration enters a neighborhood with a small gradient norm and then escapes via a negative curvature direction.  
Consequently, $\epsilon_k$ stays small while $g_k$ may grow large, making the method resemble the fixed $\epsilon$ scenario.  
Interestingly, 
this same condition is also what introduces the logarithmic factor in
ARNCG$_g$ \emph{theoretically}.

\paragraph{The linesearch parameter}
Since our algorithm relies on a linesearch step, it requires more function evaluations than CAT for large $m_{\mathrm{max}}$.  
If evaluating the target function is expensive, we may need to set a small $m_{\mathrm{max}}$, or even $m_{\mathrm{max}} = 0$.  
Under the latter case, at most two tests of the line search criteria are performed, and the parameter $M_k$ is increased when these tests fail.  
Our theory guarantees that $M_k = O(L_H)$, so this choice remains valid.  
In practice, we observe that using a relatively small $m_{\mathrm{max}}$ gives better results.




\paragraph{Case studies for local behavior}
We present two benchmark problems that exhibit superlinear local convergence behavior.  
As illustrated in \figureref{fig:appendix-comparision-local}, a larger $\theta$ gives faster local convergence.
We only show the algorithm using the second regularizer in this figure, and note that the two regularizers have a similar behavior since in the local regime they reduce to $g_k^{\frac{1}{2} + \theta} g_{k-1}^{-\theta}$, as shown in the last paragraph of the proof of \propositionref{prop:mixed-newton-nonconvex-phase-local-rates}.
Generally, it is hard to identify when the algorithm enters the neighborhood for superlinear convergence.
For \texttt{HIMMELBG}, the algorithm appears to be initialized near the local regime. 
For \texttt{ROSENBR}, the algorithm enters the local regime after approximately 20 iterations.

\begin{figure}[!tbp]
    \centering
    \includegraphics{figures/single_HIMMELBG.pdf} \hfill
    \includegraphics{figures/single_ROSENBR.pdf} \\
    \caption{
        Illustration of the local behavior of our method on the \texttt{HIMMELBG} (left plot) and \texttt{ROSENBR} (right plot) problems from the CUTEst benchmark for $\lambda=0$ and $m_{\mathrm{max}} = 1$.
        All methods converge to the same point. 
        }
    \label{fig:appendix-comparision-local}
\end{figure}

\begin{figure}[!tbp]
    \centering
    \includegraphics{figures/fallback-ss1-gg1.0-time.pdf} \hfill
    \includegraphics{figures/fallback-ss1-gg1.0-hesseval.pdf} \\
    \vspace{1em}
    \includegraphics{figures/fallback-ss1-gg1.0-gradeval.pdf} \hfill
    \includegraphics{figures/fallback-ss1-gg1.0-funceval.pdf} 
    \caption{
        Comparison of success rates as functions of elapsed time, Hessian evaluations, gradient evaluations and function evaluations for solving problems in the CUTEst benchmark.
        The fallback parameter $\lambda$ in \eqref{eqn:appendix/fallback-relaxed} varies, and $m_{\mathrm{max}} = 1$.
        }
    \label{fig:appendix-comparision-fallback}
\end{figure}

\begin{table}[!tbp]
    \caption{
        Shifted geometric mean of the relevant metrics for different methods in the CUTEst benchmark.
        The fallback, second linesearch and linesearch failure rates are reported as mean values.
        The fallback parameter $\lambda$ in \eqref{eqn:appendix/fallback-relaxed} varies.
        }
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cccccccccc}  \toprule
       \input{table/results-ss1.with.fallback.tex}%
       \input{table/results.with.fallback.tex}%
    \end{tabular}%
    }
    \label{tab:appendix-comparision-fallback}
\end{table}
\begin{table}[!tbp]
    \caption{
        Median of the relevant metrics for different methods in the CUTEst benchmark.
        The fallback parameter $\lambda$ in \eqref{eqn:appendix/fallback-relaxed} varies.
        }
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{cccccccccc}  \toprule%
       \input{table/results-ss1.with.fallback.medium.tex}%
       \input{table/results.with.fallback.medium.tex}%
    \end{tabular}%
    }
    \label{tab:appendix-comparision-fallback-median}
\end{table}


\begin{figure}[!tbp]
    \centering
    \includegraphics{figures/theta-ss1-gg-time.pdf} \hfill
    \includegraphics{figures/theta-ss1-gg-hesseval.pdf} \\
    \vspace{1em}
    \includegraphics{figures/theta-ss1-gg-gradeval.pdf} \hfill
    \includegraphics{figures/theta-ss1-gg-funceval.pdf}
    \caption{
        Comparison of success rates as functions of elapsed time, Hessian evaluations, gradient evaluations and function evaluations for solving problems in the CUTEst benchmark.
        The parameter $\theta$ in \theoremref{thm:newton-local-rate-boosted} varies, and the fallback step is removed, i.e., $\lambda = 0$ in \eqref{eqn:appendix/fallback-relaxed}, and $m_{\mathrm{max}} = 1$.
        }
    \label{fig:appendix-comparision-theta}
\end{figure}



\begin{table}[!tbp]
    \caption{
        Shifted geometric mean of the relevant metrics for different methods in the CUTEst benchmark.
        The linesearch failure rate is reported as mean values.
        The parameter $\theta$ in \theoremref{thm:newton-local-rate-boosted} and the linesearch parameter $m_{\mathrm{max}}$ vary, and $\lambda = 0$.
        }
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccccc}  \toprule%
       \input{table/results-ss1.without.fallback.tex}%
       \input{table/results.without.fallback.tex}%
    \end{tabular}%
    }
    \label{tab:appendix-comparision-theta}
\end{table}
\begin{table}[!tbp]
    \caption{
        Median of the relevant metrics for different methods in the CUTEst benchmark.
        The parameter $\theta$ in \theoremref{thm:newton-local-rate-boosted} and the linesearch parameter $m_{\mathrm{max}}$ vary, and $\lambda = 0$.
        }
    \resizebox{\columnwidth}{!}{%
    \begin{tabular}{ccccccccc}  \toprule%
       \input{table/results-ss1.without.fallback.medium.tex}%
       \input{table/results.without.fallback.medium.tex}%
    \end{tabular}%
    }
    \label{tab:appendix-comparision-theta-median}
\end{table}
