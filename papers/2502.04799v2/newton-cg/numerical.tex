\section{Preliminary numerical results} 
\label{sec:main/numerical}

\begin{figure}[tbp]
    \centering
    \includegraphics{figures/main-ss1-time.pdf} \hfill
    \includegraphics{figures/main-ss1-hesseval.pdf}
    \caption{
        Comparison of success rates as functions of elapsed time and Hessian evaluations for CUTEst benchmark problems.  
        \algname{ARNCG$_g$}, \algname{ARNCG$_\epsilon$}, and ``Fixed'' correspond to \Cref{alg:adap-newton-cg} with the first and second regularizers from \theoremref{thm:newton-local-rate-boosted}, and a fixed $\omega_k \equiv \sqrt{\epsilon}$, respectively.  
        For Hessian evaluations, 
        since our algorithm accesses this information only via Hessian-vector products, 
        we count multiple products involving $\nabla^2\varphi(x)$ at the same point $x$ as a single evaluation.
        }
    \label{fig:main-algoperf}
\end{figure}

In this section, we present some preliminary numerical results.\footnote{Our code is available at \url{https://github.com/miskcoo/ARNCG}.} %
Our primary goal is to provide an overall sense of our algorithm's performance and the effects of its components.
Detailed results are deferred to \Cref{sec:appendix/numerical-results}.

Since the recently proposed trust-region-type method \algname{CAT} has an optimal rate and shows competitiveness with state-of-the-art solvers~\citep{hamad2024simple}, we adopt their experimental setup and compare with it, as well as the regularized Newton-type method \algname{AN2CER} proposed by \citet{gratton2024yet}.
The experiments are conducted on the 124 unconstrained problems with more than 100 variables from the widely used CUTEst benchmark for nonlinear optimization~\citep{gould2015cutest}.
The algorithm is considered successful if it terminates with $\epsilon_k \leq \epsilon = 10^{-5}$ such that $k \leq 10^5$. If the algorithm fails to terminate within 5 hours, it is also recorded as a failure.

In \Cref{sec:appendix/numerical-results}, 
we observe that the fallback step has insignificant impact on  performance yet increases computational cost, suggesting it can be relaxed or removed.
Furthermore, $\theta \in [0.5, 1]$ balances computational efficiency and local behavior 
and a small $m_{\mathrm{max}}$ is preferable. 
Finally, the second linesearch step \eqref{eqn:smooth-line-search-sol-smaller-stepsize} and the \texttt{TERM} state of \texttt{CappedCG} are rarely taken in practice.

\figureref{fig:main-algoperf} shows our method without the fallback step (see \Cref{sec:appendix/numerical-results} for details). 
It is slightly faster than CAT and AN2CER, 
as each iteration uses only a few Hessian-vector products, 
whereas CAT relies on multiple Cholesky factorizations and AN2CER involves minimal eigenvalue computations. 
Meanwhile, our method requires a similar number of Hessian evaluations as CAT, and slightly fewer than AN2CER.
We also note that using a fixed $\omega_k = \sqrt{\epsilon}$ in \Cref{alg:adap-newton-cg}
may lead to failures when $g_k \gg \epsilon$, resulting in deteriorated performance.
Additionally, our method requires significantly less memory ($\sim$6GB) compared to CAT ($\sim$74GB) for the largest problem in the benchmark with 123200 variables, as it avoids  constructing the full Hessian.
