\section{Main results for global rates}
\label{sec:appendix/global-rate-under-local-boosting}


Throughout this section, we follow the partition \eqref{eqn:proof/newton-partition} defined in \Cref{sec:main/sqrt-global-rate}, 
and provide detailed proofs for the global rates in \theoremref{thm:newton-local-rate-boosted} and corresponding lemmas described in \Cref{sec:main/sqrt-global-rate}.
For the sake of readability, we restate the lemmas mentioned in \Cref{sec:main/sqrt-global-rate}.

\subsection{Details in \Cref{sec:main/sqrt-global-rate}}

\begin{lemma}[Restatement of \lemmaref{lem:main/transition-between-subsequences-give-valid-regularizer}]
    \label{lem:proof/transition-between-subsequences-give-valid-regularizer}
    Under the regularizer choices of \theoremref{thm:newton-local-rate-boosted}, 
    we have $\omega_{\ell_{j}-1} = \omega_{\ell_{j}-1}^{\supfallback}$ for each %
    $j \ge 2$,
    and 
    \begin{equation}
        \label{eqn:inexact-mixed-newton-boundary-bound}
        \varphi(x_{\ell_{j}})
        - \varphi(x_{\ell_{j}-1}) 
        \leq 
        -\tilde C_1 M_{\ell_j-1}^{-\frac{1}{2}} \bone_{\{\ell_{j}-1 \notin \cJ^{1}\}} (\omega_{\ell_{j}-1}^{\supfallback})^3,
    \end{equation}
    where $\tilde C_1, \tilde C_4$ are defined in \lemmaref{lem:lipschitz-constant-estimation}.
    Moreover, if $M_{\ell_j-1} > \tilde C_4 L_H$, then $\ell_j -1 \in\cJ^{-1}$.
\end{lemma}
\begin{proof}
    Let $k = \ell_j - 1$.
    If the fallback step is taken, then $\omega_k = \omega_k^{\supfallback}$ holds.
    We consider the case where the trial step at $k$-th iteration is accepted, 
    then we know $g_{k + \frac{1}{2}} = g_{k+1} 
    > g_k$ by the partition rule \eqref{eqn:proof/newton-partition}.
    However, the acceptance rule of the trial step in \Cref{alg:adap-newton-cg} 
    gives that $g_k > g_{k-1}$, 
    and hence $\min(1, g_k^\theta g_{k-1}^{-\theta}) = 1$.
    Moreover, we have $g_{k-1} \geq \epsilon_{k-1}$ and then 
    \begin{align*}
    \epsilon_k 
    = \min(\epsilon_{k-1}, g_k) 
    \geq \min(\epsilon_{k-1}, g_{k-1}) 
    = \epsilon_{k-1} \geq \epsilon_k.
    \end{align*}
    Therefore, $\epsilon_k^\theta \epsilon_{k-1}^{-\theta} = 1$.
    Combining these discussions, we know $\omega_k = \omega_k^{\supfallback}$ for the two choices of regularizers.

    It remains to show that $D_k \geq(\omega_k^{\supfallback})^3$ for $D_k$ defined in \lemmaref{lem:lipschitz-constant-estimation},
    which holds since we know
    $g_{k+1} > g_k$
    by the partition rule \eqref{eqn:proof/newton-partition}, and $g_k \geq (\omega_k^{\supfallback})^2$ by the choice of regularizers, and therefore,
    \begin{align}
    D_k \overset{\eqref{eqn:summarized-descent-amount}}{\geq}
    \min((\omega_k^{\supfallback})^3, g_{k+1}^2(\omega_k^{\supfallback})^{-1}) 
    \geq \min((\omega_k^{\supfallback})^3, g_{k}^2(\omega_k^{\supfallback})^{-1}) 
    \geq (\omega_k^{\supfallback})^3.
    \label{eqn:proof/transition-descent-amount}
    \end{align}


Finally, when $M_k > \tilde C_4 L_H$, we use \corollaryref{cor:appendix/decreasing-Mk-condition} to show that $k \in \cJ^{-1}$.
For the first case in that corollary,
since $\tau_- < 1$, then  $\omega_{k} = \omega_{k}^{\supfallback} > \tau_- \omega_{k}^{\supfallback}$, 
then the corollary gives $k \in \cJ^{-1}$.
For the second case, the results follows from \eqref{eqn:proof/transition-descent-amount} and 
$\min(\omega_k^3, g_{k+1}^2\omega_k^{-1}) \geq (\omega_k^{\supfallback})^3 > \tau_- (\omega_k^{\supfallback})^3$.
\end{proof}

\begin{lemma}[Restatement of \lemmaref{lem:main/iteration-in-a-subsequence}]
    \label{lem:proof/iteration-in-a-subsequence}
    Under the regularizer choices of \theoremref{thm:newton-local-rate-boosted}, 
    we have $(\omega_k^{\supfallback})^{1 + 2\theta}(\omega^{\supfallback}_{k-1})^{-2\theta}\leq \omega_k \leq \omega^{\supfallback}_k$ for each %
    $k \ge 1$.
    Moreover, for $j \geq 1$ and $\ell_j  < k < \ell_{j+1}$,
    \begin{align}
        \varphi(x_{k})
        - \varphi(x_{\ell_{j}})
        \leq
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap \cJ^{-1} |
            + 
            \max\left ( 0, | I_{\ell_j,k} \cap \cJ^0 | - T_{\ell_j,k} - 5 \right ) 
        \right ) (\omega_k^{\supfallback})^3
         ,
         \label{eqn:inexact-mixed-newton-inner-bound}
    \end{align}
    where $C_{i,j} = \tilde C_1 %
    \min_{i \leq l < j} M_l^{-\frac{1}{2}}$,
    $T_{i,j}=2\log\log\big (3 (\omega_i^{\supfallback})^2 (\omega_j^{\supfallback})^{-2}\big)$, 
    and $\tilde C_1$ is defined in \lemmaref{lem:lipschitz-constant-estimation}.
\end{lemma}
\begin{proof}
    Under the regularizers choices, we know for each $k \in \N$, $D_k$ defined in \eqref{eqn:summarized-descent-amount} satisfies that 
    \begin{align}
        \nonumber
    D_k 
    &\geq \min\left( (\omega_k^{\supfallback})^3, g_{k+1}^2\omega_k^{-1}, \omega_k^3 \right)
    = \min\left( g_{k+1}^2\omega_k^{-1}, \omega_k^3 \right) \\
    &\geq 
    \min\left( g_{k+1}^2(\omega_k^{\supfallback})^{-1}, (\omega_k^{\supfallback})^{3 + 6\theta}(\omega_{k-1}^{\supfallback})^{-6\theta} \right).
    \label{eqn:proof/lower-bound-of-descent-amount-general}
    \end{align}
    \paragraph{Case 1}
    For the first choice of regularizers, 
    we have $\omega_i^{\supfallback} = \sqrt{g_i}$ and
    $T_{i,j} = 2\log\log \frac{3g_i}{g_j}$, and
    \begin{align*}
        \varphi(x_{i+1}) - \varphi(x_i)
        \overset{\eqref{eqn:summarized-descent-inequality}}{\leq} 
        \begin{cases}
        -C_i\min\left( g_{i+1}^2 g_i^{-\frac{1}{2}}, g_i^{\frac{3}{2} + 3\theta} g_{i-1}^{-3\theta} \right),
        & \text{ if } i \notin \cJ^{-1}, \\
        -C_i g_i^{\frac{3}{2}}, 
        & \text{ if } i \in \cJ^{-1},
        \end{cases}
    \end{align*}
    where $C_i := \tilde C_1 M_i^{-\frac{1}{2}}$.
    
    When $\theta > 0$, for any $\ell_j < k \leq \ell_{j+1} - 1$, using \lemmaref{lem:accumulated-mixed-descent-lower-bound} with 
    \begin{equation}
     (p_1, q_1, p_2, q_2, a, A, K, S) = \left (2, \frac{1}{2}, \frac{3}{2} + 3\theta, 3\theta, g_{k}, g_{\ell_j}, k - \ell_j - 1, I_{\ell_{j},k} \cap \cJ^0 \right ), 
     \label{eqn:proof/parameter-choices-of-the-descent-lowerbound-lemma}
    \end{equation}
    we see that 
    \begin{align}
        \nonumber
        \varphi(x_{k})
        - \varphi(x_{\ell_{j}})
        \overset{\eqref{eqn:summarized-descent-inequality}}&{\leq}
         - \tilde C_1 \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^{-1}}} 
         M_i^{-\frac{1}{2}} g_i^{\frac{3}{2}}  
         - \tilde C_1 
         \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^0}} 
         M_i^{-\frac{1}{2}} 
         \min\left( g_{i+1}^2 g_i^{-\frac{1}{2}}, g_i^{\frac{3}{2} + 3\theta} g_{i-1}^{-3\theta} \right)  \\
         \nonumber
         &\leq 
         - C_{\ell_j,k}
         \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^{-1}}} 
        g_i^{\frac{3}{2}} 
         - C_{\ell_j,k}
         \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^{0}}} 
         \min\left( g_{i+1}^2 g_i^{-\frac{1}{2}}, g_i^{\frac{3}{2} + 3\theta} g_{i-1}^{-3\theta} \right)
         \\
        \overset{\eqref{eqn:accumulated-mixed-descent-lower-bound}}&{\leq} 
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap \cJ^{-1} |
            + 
            \max\left ( 0, | I_{\ell_j,k} \cap \cJ^0 | - T_{\ell_j,k} - 5 \right ) 
        \right ) g_{k}^{\frac{3}{2}}
         .
         \label{eqn:proof/inexact-mixed-newton-inter-bound}
    \end{align}

    On the other hand, when $\theta = 0$, we know $\varphi(x_{i+1}) - \varphi(x_i) \leq -C_i g_{i+1}^2 g_i^{-\frac{1}{2}}$ for $i \notin \cJ^{-1}$, and \eqref{eqn:proof/inexact-mixed-newton-inter-bound} also holds by applying \lemmaref{lem:accumulated-descent-lower-bound} with
    \begin{equation*}
     (p, q, a, A, K, S) = \left (2, \frac{1}{2}, g_{k}, g_{\ell_j}, k - \ell_j - 1, I_{\ell_{j},k} \cap \cJ^0 \right ).
    \end{equation*}

    \paragraph{Case 2}
    For the second choice of the regularizers, we have $\omega_i^{\supfallback} = \sqrt{\epsilon_i}$
    and $T_{i,j} = 2\log\log \frac{3\epsilon_i}{\epsilon_j}$.

    Since $\epsilon_k$ is non-increasing and $\omega_k \leq \sqrt{\epsilon_k}$ for each $k \in \N$,  
    then for a fixed $i$ such that $\ell_j \leq i < \ell_{j+1} - 1$, we know $g_i \geq g_{i+1}$ and have the following two cases.
    \begin{enumerate}
        \item If $g_{i+1} \geq \epsilon_{i-1}$, we know $\epsilon_{i} 
        = \min( \epsilon_{i-1}, g_i) 
        \geq \min( \epsilon_{i-1}, g_{i+1}) 
        = \epsilon_{i-1} \geq \epsilon_i$. 
        Then, 
        \begin{align*}
        D_i 
        \overset{\eqref{eqn:proof/lower-bound-of-descent-amount-general}}{\geq} 
        \min \big (g_{i+1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \big )
        \overset{(g_{i+1} \geq \epsilon_{i-1})}{\geq}
        \min \big (\epsilon_{i-1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \big )
        \overset{(\epsilon_i = \epsilon_{i-1})}{=}
        \epsilon_i^{\frac{3}{2}}
        .
        \end{align*}
        \item If $g_{i+1} < \epsilon_{i-1}$, 
        then using $g_{i+1} \geq \min(g_{i+1}, \epsilon_i) =  \epsilon_{i+1}$,
        we have
        \begin{align*}
        D_i 
        \overset{\eqref{eqn:proof/lower-bound-of-descent-amount-general}}{\geq} 
        \min \big (g_{i+1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \big )
        \overset{(g_{i+1} \geq \epsilon_{i+1})}{\geq}
        \min \big (\epsilon_{i+1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \big )
        .
        \end{align*}
    \end{enumerate}
    Thus, from \lemmaref{lem:lipschitz-constant-estimation}, we know for $\ell_j \leq i < \ell_{j+1} - 1$, it holds that
    \begin{align*}
        \varphi(x_{i+1}) - \varphi(x_i)
        \overset{\eqref{eqn:summarized-descent-inequality}}{\leq} 
        \begin{cases}
        -C_i \min \left (\epsilon_{i+1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \right ),
        & \text{ if } i \notin \cJ^{-1} \text{ and } g_{i+1} < \epsilon_{i-1}, \\
        -C_i \epsilon_i^{\frac{3}{2}}, 
        & \text{ if } i \in \cJ^{-1} \text{ or } g_{i+1} \geq \epsilon_{i-1}.
        \end{cases}
    \end{align*}

    Define $\cJ^0_+ = \cJ^0 \cap \{ i : g_{i+1} \geq \epsilon_{i-1} \}$ and $\cJ^0_- = \cJ^0 \setminus \cJ_+^0$.
    For any $\ell_j < k \leq \ell_{j+1} - 1$ and $\theta > 0$, we can apply \lemmaref{lem:accumulated-mixed-descent-lower-bound},
    with the parameters $a, A$, and $\{g_i\}_{0\le i \le K+1}$ therein chosen as $\epsilon_k, \epsilon_{\ell_j}$, and $\{\epsilon_i\}_{\ell_j \le i\le k}$, respectively, 
    and other parameter choices remain the same as \eqref{eqn:proof/parameter-choices-of-the-descent-lowerbound-lemma}.
    Then, we know
    \begin{align}
        \nonumber
        \varphi(x_{k})
        - \varphi(x_{\ell_{j}}) 
        \overset{\eqref{eqn:summarized-descent-inequality}}&{\leq}
         - C_{\ell_j,k}
         \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^{-1} \cup \cJ^0_+}} \epsilon_i^{\frac{3}{2}}  
         - C_{\ell_j,k}
         \sum_{\substack{\ell_j \leq i < k\\ i\in \cJ^0_-}} 
            \min \left (\epsilon_{i+1}^2\epsilon_i^{-\frac{1}{2}}, \epsilon_i^{\frac{3}{2} + 3\theta}\epsilon_{i-1}^{-3\theta} \right ) 
            \\
         \nonumber
        \overset{\eqref{eqn:accumulated-descent-lower-bound}}&{\leq} 
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap (\cJ^{-1} \cup \cJ^0_+) | + 
            \max\left ( 0, | I_{\ell_j,k} \cap \cJ^0_- | - T_{\ell_j, k} - 5 \right ) 
        \right ) \epsilon_{k}^{\frac{3}{2}} \\
        \nonumber
        &= 
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap \cJ^{-1} | + 
            \max\left ( | I_{\ell_j,k} \cap \cJ^0_+ |, | I_{\ell_j,k} \cap \cJ^0 | - T_{\ell_j,k} - 5 \right ) 
        \right ) \epsilon_{k}^{\frac{3}{2}} \\
        &\leq 
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap \cJ^{-1} | + 
            \max\left ( 0, | I_{\ell_j,k} \cap \cJ^0 | - T_{\ell_j,k} - 5 \right ) 
        \right ) \epsilon_{k}^{\frac{3}{2}}
        .
        \label{eqn:proof/inexact-newton-inter-bound-loglog-removed}
    \end{align}
    Similarly, when $\theta = 0$ we can invoke \lemmaref{lem:accumulated-descent-lower-bound} to obtain the same result.
\end{proof}

\begin{proposition}[Restatement of \propositionref{prop:main/accumulated-descent}]
    \label{prop:proof/accumulated-descent}
    Under the regularizer choices of \theoremref{thm:newton-local-rate-boosted}, 
    for each $k \geq 0$, we have
    \begin{align}
        \varphi(x_{k})
        - \varphi(x_0)
        \leq
         - C_{0,k}
        \Big (\underbrace{
            |I_{0,k} 
            \cap \cJ^{-1}|
            + \max\left( |S_k \cap \cJ^0|, |I_{0,k}
             \cap \cJ^0| - V_k - 5J_k \right)
            }_{\Sigma_k} \Big )
         \epsilon_k^{\frac{3}{2}}
         ,
        \label{eqn:newton-global-final-inequality}
    \end{align}
    where $C_{0,k}$ is defined in \lemmaref{lem:proof/iteration-in-a-subsequence},
    and $V_k = \sum_{j=1}^{J_k-1} T_{\ell_j,\ell_{j+1}} + T_{\ell_{J_k},k}$,
    and $S_k = \bigcup_{j=1}^{J_k-1}\{\ell_{j+1}-1\}$,
    and $J_k = \max\{ j : \ell_j \leq k \}$.
\end{proposition}
\begin{proof}
    For each $j \geq 0$ such that $\ell_{j+1} - \ell_j \geq 2$, 
    using \eqref{eqn:inexact-mixed-newton-inner-bound} with $k = \ell_{j+1} - 1$ and \eqref{eqn:inexact-mixed-newton-boundary-bound}, and $\bone_{\{k\notin \cJ^1\}} = \bone_{\{k\in \cJ^{-1}\}} + \bone_{\{k\in \cJ^0\}}$, we find 
    \begin{align*}
        &\peq \varphi(x_{\ell_{j+1}})
        - \varphi(x_{\ell_{j}})  
        = 
        \left (\varphi(x_{\ell_{j+1}}) - \varphi(x_{\ell_{j+1}-1})  \right )
        + \left ( \varphi(x_{\ell_{j+1}-1}) - \varphi(x_{\ell_{j}+1}) \right ) 
        \\
        &\leq 
        - C_{\ell_j,\ell_{j+1}}
        \left ( 
            |I_{\ell_j,\ell_{j+1}} \cap \cJ^{-1} |
            +
            \max\left ( \bone_{\{\ell_{j+1}-1 \in \cJ^{0}\}}, | I_{\ell_j,\ell_{j+1}} \cap \cJ^0 | - T_{j} - 5 \right ) 
        \right )
        (\omega^{\supfallback}_{{\ell_{j+1}-1}})^3,
    \end{align*}
    where $T_j := T_{\ell_j,\ell_{j+1}}$ and $I_{i,j}, T_{i,j}, C_{i,j}$ are defined in \lemmaref{lem:proof/iteration-in-a-subsequence}.
    On the other hand,
    when $\ell_{j+1} - \ell_j = 1$, then the above inequality also holds since it reduces to \eqref{eqn:inexact-mixed-newton-boundary-bound}.

    Define $J_k = \max\left\{ j : \ell_j \leq k \right\}$,
     then $\ell_{J_k} \leq k < \ell_{J_{k}+1}$, and the following inequality holds by noticing that for each $i \in \N$, either $\omega^{\supfallback}_i = \sqrt{\epsilon_i}$ or $\omega^{\supfallback}_i = \sqrt{g_i} \geq \sqrt{\epsilon_i}$.
    \begin{align}
        \nonumber
        &\peq \varphi(x_k) - \varphi(x_0) 
        = 
        \varphi(x_k) - \varphi(x_{\ell_{J_k}})
        + \sum_{j = 1}^{J_k-1} 
        \left ( \varphi(x_{\ell_{j+1}}) - \varphi(x_{\ell_{j}}) \right ) \\
        \nonumber
        &\leq
        - C_{\ell_{J_k},k}
        \left ( 
            |I_{\ell_{J_k},k} \cap \cJ^{-1} |
            + 
            \max\left ( 0, | I_{\ell_{J_k},k} \cap \cJ^0 | - T_{\ell_{J_k},k} - 5 \right ) 
        \right ) \epsilon_{k}^{\frac{3}{2}} \\
        \nonumber
        &\peq 
        -
        \sum_{j=1}^{J_k-1}
        C_{\ell_j,\ell_{j+1}}
        \left ( 
            |I_{\ell_j,\ell_{j+1}} \cap \cJ^{-1} |
            +
            \max\left ( \bone_{\{\ell_{j+1}-1 \in \cJ^{0}\}}, | I_{\ell_j,\ell_{j+1}} \cap \cJ^0 | - T_{j} - 5 \right ) 
        \right )
        \epsilon_{{\ell_{j+1}-1}}^{\frac{3}{2}}
        \\
        &\leq -C_{0,k} \epsilon_k^{\frac{3}{2}}
        \left (
            |I_{0,k} \cap \cJ^{-1}|
            + \max\left( 
               |S_k \cap \cJ^0|,
                |I_{0,k} \cap \cJ^0| - V_k - 5J_k
                \right)
        \right ),%
    \end{align}
    where $V_k = \sum_{j=1}^{J_k-1} T_j + T_{\ell_{J_k},k}$, $S_k = \bigcup_{j=1}^{J_k-1}\{\ell_{j+1}-1\}$ 
    and the last inequality follows from $\max(a, b) + \max(c, d) \geq \max(a + c, b + d)$.
\end{proof}

\begin{proposition}[Restatement of \propositionref{prop:main/initial-phase-decreasing-Mk}]
    \label{prop:proof/initial-phase-decreasing-Mk}
    Let $k_{\mathrm{init}} = \min\{ j : M_j \leq \tilde C_4 L_H \}$ if $M_0 > \tilde C_4 L_H$ and $k_{\mathrm{init}} = 0$ otherwise, then 
    for the first choice of regularizers in \theoremref{thm:newton-local-rate-boosted}, we have
    \begin{equation}
        k_{\mathrm{init}}  
        \leq 
        \left[ \log_\gamma \frac{\gamma M_0}{\tilde C_4 L_H} \right]_+
        \left ( \tilde C_3 \log \frac{U_\varphi}{\epsilon_{k_{\mathrm{init}}}} + 3 \right )
        + 2
        ,
    \end{equation}
    where $\tilde C_3^{-1} = \frac{1}{2\max(2, 3\theta)} \log\frac{1}{\tau_-} > 0$ and $\tilde C_4$ is defined in \lemmaref{lem:lipschitz-constant-estimation},
    and $[x]_+$ denotes $\max(0, x)$.
    For the second choice of regularizers, we have
    \begin{equation}
        k_{\mathrm{init}} 
        \leq 
        \left[ \log_\gamma \frac{M_0}{\tilde C_4 L_H} \right]_+
        + \tilde C_3 \log \frac{U_\varphi}{\epsilon_{k_{\mathrm{init}}}}
        + 2
        .
    \end{equation}
\end{proposition}
\begin{proof}
    Using \lemmaref{lem:lipschitz-constant-estimation}
    and observing that the constants therein satisfy $\tilde C_4 \geq \tilde C_5$,
    then we know 
    $M_k$ is non-increasing for $k < k_{\mathrm{init}}$.
    Hence, $\tilde C_4 L_H < M_k = M_0 \gamma^{-|I_{0,k} \cap \cJ^{-1}|}$, and equivalently,
    \begin{equation}
        \label{eqn:proof/lipschitz-decay-count}
       \log_\gamma (\tilde C_4L_H) < \log_\gamma M_k = \log_\gamma M_0 - |I_{0,k} \cap \cJ^{-1}|.
    \end{equation}

    By definition of $\delta_k$ in \theoremref{thm:newton-local-rate-boosted}, we know $\delta_k^\theta = \omega_k^{\supsucc} (\omega_k^{\supfallback})^{-1} \leq 1$.
    Let %
    $\cI_{i,j} = \{ l \in I_{i,j} : \delta_l^\alpha < \tau_-\}$,
    and $\cI_{i,j}^+ = \{ l \in I_{i,j} : \delta_{l+1}^\alpha < \tau_- \}$. 
    From \lemmaref{lem:lipschitz-constant-estimation}, 
    when $M_k > \tilde C_4 L_H$ and $\tau_- \leq \min\big ( \delta_k^\alpha, \delta_{k+1}^\alpha \big )$, 
    we have $k \in \cJ^{-1}$.
    Equivalently, we have $(I_{i,j} \setminus \cI_{i,j}) \cap (I_{i,j} \setminus \cI_{i,j}^+) \subseteq I_{i,j} \cap \cJ^{-1}$ for $i < j < k_{\mathrm{init}}$.
    Then, 
    \begin{align}
        \nonumber
        |I_{i,j} \cap \cJ^{-1} |
        &\geq 
        |(I_{i,j} \setminus \cI_{i,j}) \cap (I_{i,j} \setminus \cI_{i,j}^+)|
        = 
        |I_{i,j} \setminus 
        ( \cI_{i,j}
        \cup  \cI^+_{i,j})| \\
        &\geq 
        |I_{i,j}| -  ( |\cI_{i,j}| +  |\cI^+_{i,j}|) 
        \geq 
        |I_{i,j}| -  2|\cI^+_{i-1,j}|
        ,
        \label{eqn:proof/initial-phase-newton-gradient-decay-raw}
    \end{align}
    where the last inequality follows from $\cI_{i,j} = \cI_{i-1,j-1}^+ \subseteq \cI_{i-1,j}^+$.
    Reformulating \eqref{eqn:proof/initial-phase-newton-gradient-decay-raw} obtains
    \begin{equation}
        |\cI^+_{i,j+1}|\geq \frac{1}{2}\left( |I_{i+1,j+1}| - |I_{i+1,j+1} \cap \cJ^{-1}| \right),  \forall\, 0\leq i < j < k_{\mathrm{init}} - 1.
        \label{eqn:proof/initial-phase-newton-gradient-decay}
    \end{equation}

    \paragraph{Case 1}
    We consider the first choice of regularizers, i.e., $\delta_k = \min(1, g_kg_{k-1}^{-1})$.
    Following the partition \eqref{eqn:proof/newton-partition},
    for any $\ell_j \leq l < \ell_{j+1}-1$ and $l < k_{\mathrm{init}} - 1$, 
    we know $g_{l+1} \leq g_l$ and $\delta_{l+1} = g_{l+1}g_l^{-1}$.
    Therefore,
    since $\log \delta_{l+1} \leq 0$ and $\log \tau_- < 0$, it holds that 
    \begin{align}
        \nonumber
        \log \frac{g_{l+1}}{g_{\ell_j}}
        &= \sum_{\ell_j \leq i \leq l} \log \delta_{i+1}
        \leq \sum_{i\in \cI^+_{\ell_j,l+1}} \log \delta_{i+1} \\
        &<
        \frac{\log\tau_-}{\alpha} | \cI^+_{\ell_j,l+1} |
        \overset{\eqref{eqn:proof/initial-phase-newton-gradient-decay}}{\leq}
        -A ( |I_{\ell_j+1,l+1}| - |I_{\ell_j+1,l+1} \cap \cJ^{-1}| ),
        \label{eqn:proof/initial-phase-newton-gradient-decay-inner-subsequence}
    \end{align}
    where $A = \frac{1}{2\alpha} \log \frac{1}{\tau_-} > 0$.
    Let $k < k_{\mathrm{init}} - 1$ and 
    $\hat J_k = \max\left\{ j : \ell_j \leq k + 1 \right\}$,
    then
    \begin{align}
        \nonumber
        \hat J_k \log \frac{\epsilon_{k+1}}{U_\varphi}
        &\leq \sum_{j=1}^{\hat J_k-1} \log \frac{g_{\ell_{j+1}-1}}{g_{\ell_j}}
        + \log\frac{g_{k+1}}{g_{\ell_{\hat J_k}}}  \\
        \nonumber
        \overset{\eqref{eqn:proof/initial-phase-newton-gradient-decay-inner-subsequence}}&{\leq}
        -A \sum_{j=1}^{\hat J_k-1} (
            |I_{\ell_j+1,\ell_{j+1}-1}|
            - |I_{\ell_j+1,\ell_{j+1}-1}\cap\cJ^{-1}|
            )  
            \\
        \nonumber
            &\peq 
        -A  (
            |I_{\ell_{\hat J_k}+1,k+1}|
           - |I_{\ell_{\hat J_k}+1,k+1}\cap \cJ^{-1}|
           )
        \\
        &\leq -A ( |I_{1,k+1}| - 2\hat J_k - |I_{1,k+1} \cap \cJ^{-1}|),
        \label{eqn:proof/initial-phase-newton-gradient-decay-summarized}
    \end{align}
    where the last inequality follows from 
    $|I_{\ell_j+1,\ell_{j+1}-1}| = |I_{\ell_j+1,\ell_{j+1}+1}| - 2$ and 
    $I_{\ell_j+1,\ell_{j+1}-1} \cap \cJ^{-1} \subseteq I_{\ell_j+1,\ell_{j+1}+1} \cap \cJ^{-1}$.

    For $1 \leq j \leq \hat J_k$, we have $\ell_j - 1 \leq k < k_{\mathrm{init}} - 1$, then \lemmaref{lem:proof/transition-between-subsequences-give-valid-regularizer}
    gives $\ell_j - 1 \in \cJ^{-1}$,
    Therefore, $|I_{0,k+1}\cap\cJ^{-1}|\geq \hat J_k$ and \eqref{eqn:proof/lipschitz-decay-count} yields 
    $\log_\gamma (\tilde C_4 L_H) < \log_\gamma M_0 - \hat J_k$.
    That is, $\hat J_k \leq \log_\gamma \frac{\gamma M_0}{\tilde C_4L_H}$.
    From \eqref{eqn:proof/lipschitz-decay-count}, we know
    \begin{align*}
         k = |I_{1,k+1}| 
         \overset{\eqref{eqn:proof/initial-phase-newton-gradient-decay-summarized}}&{\leq}
        J_k \left ( A^{-1}\log \frac{U_\varphi}{\epsilon_{k+1}} + 2 \right )
        + |I_{1,k+1} \cap \cJ^{-1}|  \\
        \overset{\eqref{eqn:proof/lipschitz-decay-count}}&{\leq}
        J_k \left ( A^{-1}\log \frac{U_\varphi}{\epsilon_{k+1}} + 2 \right )
        +  \log_\gamma \frac{M_0}{\tilde C_4L_H} 
        \leq 
        \log_\gamma \frac{\gamma M_0}{\tilde C_4 L_H} \left ( A^{-1}\log \frac{U_\varphi}{\epsilon_{k+1}} + 3 \right ).
    \end{align*}

    \paragraph{Case 2}
    When $\delta_k = \epsilon_k \epsilon_{k-1}^{-1}$ for each $k \in \N$. 
    For any $k < k_{\mathrm{init}}-1$, 
    we know a similar version of \eqref{eqn:proof/initial-phase-newton-gradient-decay-inner-subsequence} holds since $\log \delta_{i+1} \leq 0$:
    \begin{align*}
        \log\frac{\epsilon_{k+1}}{\epsilon_0}
        &= \sum_{i \in I_{0,k+1}} \log \delta_{i+1}
        \leq \sum_{i \in \cI^+_{0,k+1}} \log \delta_{i+1} \\
        &< -2A |\cI_{0,k+1}^+|
        \overset{\eqref{eqn:proof/initial-phase-newton-gradient-decay}}{\leq}
        -A ( |I_{1,k+1}| - | I_{1,k+1} \cap \cJ^{-1} |).
    \end{align*}
    Therefore, we have
    \begin{align*}
         k  = |I_{1,k+1}|
         &\leq 
         A^{-1} \log \frac{\epsilon_0}{\epsilon_{k+1}}
        + |I_{1,k+1} \cap \cJ^{-1}|  
        \overset{\eqref{eqn:proof/lipschitz-decay-count}}{\leq}
         A^{-1} \log \frac{\epsilon_0}{\epsilon_{k+1}}
         +
         \log_\gamma \frac{\gamma M_0}{\tilde C_4 L_H}.
    \end{align*}

    Finally, the proof is completed by setting $k = k_{\mathrm{init}} - 2$, and noticing that the conclusion automatically holds when $M_0 \leq \tilde C_4 L_H$.
\end{proof}



\subsection{Proof of the global rates in Theorem~\ref{thm:newton-local-rate-boosted}} \label{sec:appendix/global-rate-proof}

The following theorem provides a precise version of the global rates in \theoremref{thm:newton-local-rate-boosted}.  
It can be translated into \theoremref{thm:newton-local-rate-boosted} by using the identity $[\log L_H]_+ + [\log L_H^{-1}]_+ = |\log L_H|$.  

Since the right-hand sides of the following bounds are non-decreasing as $\epsilon_k$ decreases,  
whenever an $\epsilon$-stationary point is encountered such that $\epsilon_k \leq g_k \leq \epsilon$,  
the two inequalities below hold with $\epsilon_k$ replaced by $\epsilon$.  
Hence, the iteration bounds in \theoremref{thm:newton-local-rate-boosted} are valid.


\begin{theorem}[Precise statement of the global rates in \theoremref{thm:newton-local-rate-boosted}]
    \label{thm:appendix/global-newton-complexity}
    Let $\{ x_k \}_{k \ge 1}$ be generated by \Cref{alg:adap-newton-cg} with $\theta \geq 0$. 
    Under Assumption~\ref{assumption:liphess} and let $C = \max(\tilde C_4, \gamma \tilde C_5)^{\frac{1}{2}} \tilde C_1^{-1}$ 
    with the constants $\tilde C_1, \tilde C_4, \tilde C_5$ defined in \lemmaref{lem:lipschitz-constant-estimation},
    and let $\tilde C_3, k_{\mathrm{init}}$ be defined in \propositionref{prop:proof/initial-phase-decreasing-Mk},
    we have
    \begin{enumerate}
        \item
        If $\omega_k^{\supfallback} = \sqrt{g_k}$, and $\omega_k^{\supsucc} = \omega_k^{\supfallback} \min ( 1, g_k^\theta g_{k-1}^{-\theta} )$, 
        then 
        \begin{align*}
        k
        &\leq 
        \left[ \log_\gamma \frac{\gamma M_0}{\tilde C_4 L_H} \right]_+
        \left ( \tilde C_3 \log \frac{U_\varphi}{\epsilon_{k}} + 3 \right ) \\
        &\peq + 
        5\left ( C\Delta_\varphi L_H^{\frac{1}{2}} 
        \epsilon_k^{-\frac{3}{2}}
        + \left[ \log_\gamma \frac{\tilde C_5 L_H}{M_0} \right]_+ + 2 \right )
        \left ( \log\log \frac{U_\varphi}{\epsilon_k} + 7 \right )
        + 2
        .
        \end{align*}
        \item
        If $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$, 
        and $\omega_k^{\supsucc} = \omega_k^{\supfallback} \epsilon_k^\theta \epsilon_{k-1}^{-\theta}$, 
        then 
        \begin{align*}
        k
        &\leq 
        40 \left ( C\Delta_\varphi L_H^{\frac{1}{2}} 
        \epsilon_k^{-\frac{3}{2}}
        + \left[ \log_\gamma \frac{\tilde C_5 L_H}{M_0} \right]_+ + 2 \right ) \\
        &\peq  \peq
        + \left[ \log_\gamma \frac{M_0}{\tilde C_4L_H} \right]_+
        + (24 + \tilde C_3)\log \frac{U_\varphi}{\epsilon_k}
        + 2
        .
        \end{align*}
    \end{enumerate}
    Moreover, there exists a subsequence $\{ x_{k_j} \}_{j \geq 0}$ such that $\lim_{j \to \infty} x_{k_j} = x^*$ with $\nabla \varphi(x^*) = 0$.
\end{theorem}
\begin{proof}
    Let $k_{\mathrm{init}}$ be defined in \propositionref{prop:proof/initial-phase-decreasing-Mk}, 
    without loss of generality,
    we can drop the iterations $\{ x_j \}_{j \leq k_{\mathrm{init}}}$
    and assume $M_0 \leq \tilde C_4 L_H$, where $\tilde C_4$ is defined in \lemmaref{lem:lipschitz-constant-estimation}.
    By \lemmaref{lem:lipschitz-constant-estimation}, 
    we know $k \in \cJ^1$ implies $M_k \leq \tilde C_5 L_H$, and hence 
    $\sup_{j \geq 0} M_j \leq \max(\tilde C_4, \gamma \tilde C_5) L_H$.

    By applying \propositionref{prop:proof/accumulated-descent}, we have
    \begin{align*}
        -\Delta_\varphi \leq \varphi(x_{k}) - 
        \varphi(x_{0})
        \overset{\eqref{eqn:newton-global-final-inequality}}&{\leq}
        -C_{0,k} \Sigma_k \epsilon_k^{\frac{3}{2}}
        \leq 
        -\tilde C_1 (\max(\tilde C_4, \gamma \tilde C_5) L_H)^{-\frac{1}{2}} \Sigma_k \epsilon_k^{\frac{3}{2}},
    \end{align*}
    which implies that $\Sigma_k \leq C L_H^{\frac{1}{2}} \Delta_\varphi \epsilon_k^{-\frac{3}{2}}$ with $C = \max(\tilde C_4, \gamma \tilde C_5)^{\frac{1}{2}} \tilde C_1^{-1}$, 
    and the theorem can be proved by find a lower bound over $\Sigma_k$.
    \paragraph{Case 1}
    For the first choice of regularizers, \lemmaref{lem:main/lower-bound-of-Vk} shows that $V_k \leq J_k \log\log\frac{U_\varphi}{\epsilon_k}$, and hence,
    \begin{align*}
       \Sigma_k &\geq 
       |I_{0,k}\cap \cJ^{-1}|
       + 
       \max \left ( 
        |S_k \cap \cJ^{-1}|,  
        |I_{0,k}\cap \cJ^0| - 
        J_k \left( \log\log \frac{U_\varphi}{\epsilon_k} + 5 \right)
        \right ) \\ 
    \overset{\eqref{eqn:basic-counting-lemma-loss-descent}}&{\geq}
      \frac{k}{5\left( \log\log \frac{U_\varphi}{\epsilon_k} + 7 \right)}
      - \left[ \log_\gamma \frac{\tilde C_5 L_H}{M_0} \right]_+ - 2,
    \end{align*}
    where \lemmaref{lem:basic-counting-lemma} is invoked with $W_k = 0$ and $U_k = \log\log\frac{U_\varphi}{\epsilon_k} + 5$.
    Reorganizing the above inequality and incorporating the initial phase in \propositionref{prop:main/initial-phase-decreasing-Mk} yields
        \begin{align*}
        k
        &\leq 
        k_{\mathrm{init}}
        + 
        5\left ( C\Delta_\varphi L_H^{\frac{1}{2}} 
        \epsilon_k^{-\frac{3}{2}}
        + \left[ \log_\gamma \frac{\tilde C_5 L_H}{M_0} \right]_+ + 2 \right )
        \left ( \log\log \frac{U_\varphi}{\epsilon_k} + 7 \right )
        .
        \end{align*}
    \paragraph{Case 2}
    For the second choice of regularizers,
    \lemmaref{lem:main/lower-bound-of-Vk} shows that $V_k \leq \log\frac{U_\varphi}{\epsilon_k} + J_k$, and 
    \begin{align*}
       \Sigma_k &\geq 
       |I_{0,k}\cap \cJ^{-1}|
       + 
       \max \left ( 
        |S_k \cap \cJ^{-1}|,  
        |I_{0,k}
        \cap \cJ^0| 
        - \log\frac{U_\varphi}{\epsilon_k}
        - 6J_k
        \right ).
    \end{align*}
    Using \lemmaref{lem:basic-counting-lemma} with $U_k = 6$ and $W_k = \log \frac{U_\varphi}{\epsilon_k}$, we know either $\log \frac{U_\varphi}{\epsilon_k} \geq k / 24$, or 
    \begin{align*}
        \Sigma_k & \geq 
        \frac{k}{40}
      - \left[ \log_\gamma \frac{\tilde C_5 L_H}{M_0} \right]_+ - 2.
    \end{align*}
    By incorporating the case $k \leq 24 \log\frac{U_\varphi}{\epsilon_k}$ and the initial phase in \propositionref{prop:main/initial-phase-decreasing-Mk}, the proof is completed.

    \paragraph{The subsequence convergence}
    From the global complexity we know $\lim_{k \to \infty}\epsilon_k = 0$.
    Since $\epsilon_k = \min(\epsilon_{k-1}, g_k)$, we can construct a subsequence $\{ x_{k_j} \}_{j \geq 0}$ such that $g_{k_j} = \epsilon_{k_j}$.
    Note $\varphi(x_{k_j}) \leq \varphi(x_0)$ and the compactness of the sublevel set $L_\varphi(x_0)$ in Assumption~\ref{assumption:liphess}, we know there is a further subsequence of $\{ x_{k_j} \}$ converging to some point $x^*$.
    Since $\nabla\varphi$ is a continuous map, we know $\nabla\varphi(x^*) = 0$.
\end{proof}



\subsection{Proof of Theorem~\ref{thm:newton-local-rate-boosted-oracle-complexity}}\label{sec:appendix/oracle-complexity-proof}

\begin{proof}
The two gradient evaluations come from $\nabla \varphi(x_k)$ and $\nabla \varphi(x_k + d_k)$.
The number of function value evaluations in a linesearch criterion
is upper bounded by $m_{\mathrm{max}} + 1$, 
In the \texttt{SOL} case, at most two criteria are tested, in the \texttt{NC} case one criterion is tested.
Thus, the total number of function evaluations is bounded by $2m_{\mathrm{max}} + 2$.
The number of Hessian-vector product evaluations can be bounded using \lemmaref{lem:capped-cg}.
\end{proof}
