\section{Technical lemmas for global rates} \label{sec:appendix/global-rate-technical-lemmas}

\subsection{Descent lemmas and their proofs} \label{sec:proof-descent-lemma}

In this section we provide the descent lemmas for the \texttt{NC} case (\lemmaref{lem:newton-cg-nc}) and the \texttt{SOL} case (\lemmaref{lem:newton-cg-sol}).
The lemma for the \texttt{NC} case is the same as \citet[Lemma 6.3]{he2023newton}, and we include the proof for completeness.
However, %
the linesearch rules for the \texttt{SOL} case are different, so we need a complete proof.

\begin{lemma}
    \label{lem:newton-cg-nc}%
    Suppose $\text{d\_type}, d, \tilde d, m$ be the those in 
    the subroutine \texttt{NewtonStep} of \Cref{alg:adap-newton-cg}, and $x, \omega, M$ be its inputs.
    Suppose $\text{d\_type} = \texttt{NC}$ and let 
    $m_*$ be the smallest integer such that \eqref{eqn:smooth-line-search-nc} holds.
    If $0 < m_* \leq m_{\mathrm{max}}$, we have
    \begin{align}
        \label{eqn:newton-cg-nc-stepsize}
        \beta^{m_* - 1} &> \frac{3M(1 - 2\mu)}{L_H}, \\
        \label{eqn:newton-cg-nc-decay}
        \varphi(x + \beta^{m_*}d) - \varphi(x) & <
        -\frac{9\beta^2(1 - 2\mu)^2\mu}{L_H^2} M^{\frac{3}{2}} \omega^3
         .
    \end{align}
    When $m_* = 0$, the linesearch rule gives
    \begin{align}
        \label{eqn:newton-cg-nc-decay-ls0}
        \varphi(x + d) - \varphi(x) &\leq 
        -\mu M^{-\frac{1}{2}} \omega^3
         .
    \end{align}
    Finally, when $m_* > m_{\mathrm{max}}$, we have $M \leq (3 - 6\mu)^{-1} L_H$.
\end{lemma}
\begin{proof}
    Let $H = \nabla^2\varphi(x)$, 
    from \eqref{eqn:smooth-line-search-nc-direction} we can verify that
    $\|d \| = L(\bar d) = M^{-1} \|d\|^{-2} |d^\top H d|$, where $\bar d = \|\tilde d\|^{-1} \tilde d$ and $\tilde d$ is the direction satisfying \lemmaref{lem:capped-cg}. 
    Then, $d^\top Hd=-M\|d\|^3$ and $d^\top\nabla\varphi(x)\leq 0$.
    When $m_* \geq 1$, let $0 \leq j \leq m_* - 1$, then \eqref{eqn:smooth-line-search-nc} fails to hold with $m = j$, and 
    \begin{align}
        \nonumber
        -\mu\beta^{2j} M \| d \|^3
        < 
        \varphi(x + \beta^j d) - \varphi(x)
        \overset{\eqref{eqn:hessian-lip-value-inequ}}&{\leq} 
        \beta^j \nabla \varphi(x)^\top d + \frac{\beta^{2j}}{2} d^\top H d + \frac{L_H}{6} \beta^{3j} \| d \|^3 \\
        &\leq \frac{\beta^{2j}}{2} d^\top H d + \frac{L_H}{6} \beta^{3j} \| d \|^3 \\
        &= - \frac{\beta^{2j}}{2} M \|d\|^3 + \frac{L_H}{6} \beta^{3j} \| d \|^3.
        \label{eqn:proof/nc-descent-lemma-ls-nonzero}
    \end{align}
    Dividing both sides by $\beta^{2j} \| d \|^3$ we have
    \begin{align}
        \label{eqn:proof/linesearch-nc-failure}
        -M\mu
        < 
        - \frac{M}{2} + \frac{L_H}{6} \beta^{j}.
    \end{align}
    Therefore, rearranging the above inequality gives \eqref{eqn:newton-cg-nc-stepsize}.

    From \eqref{eqn:capped-cg-nc-direction-inequ} and \eqref{eqn:smooth-line-search-nc-direction},
    we know $\tilde d^\top H \tilde d \leq -\sqrt M \omega \| \tilde d \|^2$ and hence $\| d \| = M^{-1} \frac{|\tilde d^\top H \tilde d|}{\|\tilde d\|^2} \geq M^{-\frac{1}{2}} \omega$.
    By the linesearch rule \eqref{eqn:smooth-line-search-nc}, we have
    \begin{align*}
        \varphi(x + \beta^{m_*}d) - \varphi(x) 
        \leq - \mu\beta^{2m_*} M\| d \|^3
        \leq
        - \mu \beta^{2m_*} M^{-\frac{1}{2}} \omega^3
        \overset{\eqref{eqn:newton-cg-nc-stepsize}}{<} 
        -  \frac{9\beta^2(1 - 2\mu)^2\mu}{L_H^2} M^{\frac{3}{2}}\omega^3.
    \end{align*}
    When $m_* = 0$, \eqref{eqn:newton-cg-nc-decay-ls0} can be also proven using the above argument.

    Finally, when $m_* > m_{\mathrm{max}} \geq 0$, 
    we know \eqref{eqn:smooth-line-search-nc} fails to holds with $m = 0$, and then \eqref{eqn:proof/linesearch-nc-failure} holds with $j = 0$.
    Therefore, we have $M < (3 - 6\mu)^{-1}L_H$.

    \end{proof}

The following lemma summarizes the properties of \texttt{NewtonStep} for \texttt{SOL} case. 
Its first item is the necessary condition that the linesearch \eqref{eqn:smooth-line-search-sol} or \eqref{eqn:smooth-line-search-sol-smaller-stepsize} fails,
which will be used by subsequent items.
\begin{lemma}
    \label{lem:newton-cg-sol}%
    Suppose $\text{d\_type}, d, m, \hat m, \alpha$ be the those in the subroutine \texttt{NewtonStep} of \Cref{alg:adap-newton-cg}, and $x, \omega, M$ be its inputs.
    Suppose $\text{d\_type} = \texttt{SOL}$, and 
    let $m_* \geq 0$ be the smallest integer such that $\eqref{eqn:smooth-line-search-sol}$ holds, 
    and $\hat m_* \geq 0$ be the smallest integer such that $\eqref{eqn:smooth-line-search-sol-smaller-stepsize}$ holds,
    then we have
    \begin{enumerate}
        \item 
        Suppose $\mu \tau \beta^j d^\top \nabla \varphi(x) < \varphi(x + \tau \beta^j d) - \varphi(x)$ for some $\tau \in (0, 1]$ and $j \geq 0$, then 
    \begin{align}
        \label{eqn:newton-cg-sol-stepsize-when-linesearch-violated}
        \beta^{j} &
        > \sqrt{\frac{6(1 - \mu)M^{\frac{1}{2}}\omega}{L_H\tau^2\|d\|} }
        = \frac{\sqrt 2 C_M \omega^{\frac{1}{2}}}{\tau M^{\frac{1}{4}}\|d \|^{\frac{1}{2}}}
        ,
    \end{align}
        where $C_{M} := \sqrt{\frac{3(1 - \mu)M}{L_H} } \geq \sqrt{\frac{M}{L_H}}$.
        \item 
    If $m_{\mathrm{max}} \geq m_* > 0$, then $\alpha = \beta^{m_*}$ and 
    \begin{align}
        \label{eqn:newton-cg-sol-stepsize}
        \beta^{m_* - 1} &
        > \max\left ( \beta^{m_{\mathrm{max}} - 1}, C_{M} \| \nabla \varphi(x) \|^{-\frac{1}{2}}\omega \right )
        , \\
        \label{eqn:newton-cg-sol-decay}
        \varphi(x + \alpha d) - \varphi(x)
        & <
        - \frac{36\beta\mu(1 - \mu)^2}{L_H^2} M^{\frac{3}{2}} \omega^3
        .
    \end{align}
    \item If $m_* > m_{\mathrm{max}}$ but $m_{\mathrm{max}} \geq \hat m_* > 0$, then
        $\beta^{\hat m_* - 1}
        > \sqrt 2 C_{M}$.
    \item If $m_* > m_{\mathrm{max}}$ but $m_{\mathrm{max}} \geq \hat m_* \geq 0$, then $\alpha = \hat \alpha \beta^{\hat m_*}$ with $\hat \alpha = \min(1, \omega^{\frac{1}{2}}M^{-\frac{1}{4}}\|d\|^{-\frac{1}{2}})$, and 
    \begin{align}
        \label{eqn:newton-cg-sol-decay-smaller-stepsize}
        \varphi(x + \alpha d) - \varphi(x)
        &< %
    -\mu\beta^{\hat m_*} C_{M}^3 \min\left( C_{M}, 1  \right) M^{-\frac{1}{2}} \omega^3.
    \end{align}
    \item If both $m_* > m_{\mathrm{max}}$ and $\hat m_* > m_{\mathrm{max}}$, then $M \leq \frac{L_H}{2}$.
    \item If $m_* = 0$ (i.e., the stepsize $\alpha = 1$), then
    \begin{align}
        \label{eqn:newton-cg-sol-decay-ls0}
        \varphi(x + d) - \varphi(x)
        &\leq 
        -\frac{4\mu M^{-\frac{1}{2}}}{25 + 8L_HM^{-1}} \min \left( \| \nabla \varphi(x + d) \|^2 \omega^{-1}, \omega^3 \right).
    \end{align}
    \end{enumerate}
\end{lemma}
\begin{proof}
    Let $H = \nabla^2\varphi(x)$.
    We note that in the \texttt{SOL} setting, the direction $d$ is the same as $\tilde d$ returned by \texttt{CappedCG}, so \lemmaref{lem:capped-cg} holds for $d$.

    (1). By the assumption we have
    we have
\begin{align*}
    \mu \tau \beta^jd^\top \nabla \varphi(x)
    < 
    \varphi(x + \tau \beta^j d) - \varphi(x)
    \overset{\eqref{eqn:hessian-lip-value-inequ}}{\leq} \tau \beta^j d^\top \nabla \varphi(x)
    + \frac{\tau^2 \beta^{2j}}{2} d^\top H d + \frac{L_H}{6} \tau^3\beta^{3j} \| d \|^3,
\end{align*}
Rearranging the above inequality and dividing both sides by $\tau \beta^j$, we have
\begin{align}
    -(1 - \mu) d^\top \nabla \varphi(x)
    <
    \frac{\tau\beta^{j}}{2} d^\top H d + \frac{L_H}{6} \tau^2\beta^{2j} \| d \|^3.
    \label{eqn:newton-line-search-sol-rearrange}
\end{align}
From \lemmaref{lem:capped-cg}, we know that  
$d^\top \nabla \varphi(x) = - d^\top Hd - 2\sqrt M \omega \|d\|^2$, then since $\mu \in (0, 1/2)$, $j \geq 0$ and $\beta \in (0, 1)$, $\tau \in (0, 1]$, we have $1 - \mu > 1/2 \geq \beta^j / 2 \geq \tau \beta^j / 2$ and 
\begin{align*}
     \frac{L_H}{6}\tau^2 \beta^{2j} \| d \|^3 
     \overset{\eqref{eqn:newton-line-search-sol-rearrange}}&{>} 
    \left (1 - \mu - \frac{\tau\beta^j}{2} \right ) d^\top H d + 2\sqrt M\omega (1 - \mu) \|d\|^2 \\
    \overset{\eqref{eqn:capped-cg-hessian-lowerbound-H}}&{>} 
    -\sqrt M\omega \left (1 - \mu - \frac{\tau\beta^j}{2} \right ) \| d\|^2  + 2\sqrt M\omega (1 - \mu) \|d\|^2 \\
    &=
     \sqrt M\omega \left (1 - \mu + \frac{\tau\beta^j}{2} \right ) \|d\|^2.
\end{align*}
Therefore, we have
\begin{equation}
    \beta^{2j} 
    > \frac{6\sqrt M\omega (1 - \mu + \tau\beta^j / 2)}{L_H \tau^2 \| d\|}
    \geq \frac{6\sqrt M\omega (1 - \mu)}{L_H  \tau^2\| d\|}
    ,
    \label{eqn:capped-cg-sol-d-geq-omega}
\end{equation}
which proves \eqref{eqn:newton-cg-sol-stepsize-when-linesearch-violated}.

(2). In particular, when $m_* > 0$, we know \eqref{eqn:smooth-line-search-sol} is violated for $m = 0$, then \eqref{eqn:newton-cg-sol-stepsize-when-linesearch-violated} with $\tau = 1$ and $j = 0$ gives a lower bound of $d$:
\begin{equation}
    \label{eqn:capped-cg-sol-d-lower-bound}
    \| d \| 
    > 
    \frac{6\sqrt M\omega(1 - \mu)}{L_H}
    \geq 
     C_{M}^2 M^{-\frac{1}{2}}\omega
    .
\end{equation}
Note that \eqref{eqn:smooth-line-search-sol} is also violated for $m_* - 1$, then \eqref{eqn:newton-cg-sol-stepsize-when-linesearch-violated} holds with $(j, \tau) = (m_* - 1, 1)$, and we have
\begin{equation}
    \beta^{m_*-1} 
    \overset{\eqref{eqn:newton-cg-sol-stepsize-when-linesearch-violated}}{\geq} 
    \sqrt{\frac{6\sqrt M\omega (1 - \mu)}{L_H  \| d\|}}
    \overset{\eqref{eqn:capped-cg-io-diff}}{\geq} 
    \sqrt{\frac{3(1 - \mu)}{L_H } \frac{M\omega^2}{\| \nabla \varphi(x) \|}}
    = C_M\|\nabla\varphi(x) \|^{-\frac{1}{2}} \omega
    ,
    \label{eqn:capped-cg-sol-d-geq-omega-sol-normal-ls-case}
\end{equation}
which yields \eqref{eqn:newton-cg-sol-stepsize}.
Moreover, the descent of the function value can be bounded as follows:
\begin{align}
    \nonumber
    \varphi(x + \beta^{m_*}d) - \varphi(x)
    \overset{\eqref{eqn:smooth-line-search-sol}}&{\leq}
    \mu \beta^{m_*} d^\top\nabla\varphi(x) \\
    \nonumber
    \overset{\eqref{eqn:capped-cg-descent-direction}}&{=}
    -\mu \beta^{m_*} d^\top (H + 2\sqrt M\omega \Id)d 
    \overset{\eqref{eqn:capped-cg-hessian-lowerbound}}{\leq}
    -\mu \sqrt M\omega \beta^{m_*} \| d \|^2 \\
    \nonumber
    \overset{\eqref{eqn:capped-cg-sol-d-geq-omega-sol-normal-ls-case}}&{<}
    -\mu \beta \sqrt M\omega \| d \|^2
            \sqrt{\frac{6\sqrt M\omega(1 - \mu)}{L_H \|d \|}}
    = -\mu \beta (\sqrt M\omega \| d \|)^{\frac{3}{2}}
            \sqrt{\frac{6(1 - \mu)}{L_H}} \\
            \label{eqn:proof-sol-loss-descent}
    \overset{\eqref{eqn:capped-cg-sol-d-lower-bound}}&{<}
    - \frac{36\beta\mu(1 - \mu)^2}{L_H^2} M^{\frac{3}{2}} \omega^3.
\end{align}

(3).
The linesearch rule \eqref{eqn:smooth-line-search-sol-smaller-stepsize} can be regarded as using the rule in \eqref{eqn:smooth-line-search-sol} with a new direction $\hat \alpha d$, where $\hat \alpha = \min(1, \omega^{\frac{1}{2}} M^{-\frac{1}{4}} \|d \|^{-\frac{1}{2}})$.
Since $\hat m_* > 0$, then \eqref{eqn:smooth-line-search-sol-smaller-stepsize} is violated for $0 \leq j < \hat m_*$, and \eqref{eqn:newton-cg-sol-stepsize-when-linesearch-violated} with $\tau = \hat \alpha$ gives
\begin{align}
    \label{eqn:proof/sol-linesearch-failure-smaller-stepsize}
    \beta^{2j} 
    > \frac{6\sqrt M \omega (1 - \mu)}{L_H  \hat \alpha^2\| d \|}
    \geq \frac{6M (1 - \mu)}{L_H} = 2C_{M}^2
    .
\end{align}
Thus, the result follows from setting $j = \hat m_* - 1$.

(4).
Since $m_* > m_{\mathrm{max}} \geq 0$, 
then the linesearch rule \eqref{eqn:smooth-line-search-sol} is violated for $m = 0$ 
such that \eqref{eqn:capped-cg-sol-d-lower-bound} holds.
Hence, following the first two lines of the proof of \eqref{eqn:proof-sol-loss-descent}, we have
\begin{align*}
    \varphi(x + \hat \alpha\beta^{\hat m_*} d) - \varphi(x)
    &\leq 
    -\mu\beta^{\hat m_*}  M^{\frac{1}{2}} \omega \hat \alpha\|d\|^2  \\
    &=
    -\mu\beta^{\hat m_*} M^{\frac{1}{2}} \omega \min\left( \|d\|^2, \omega^{\frac{1}{2}} M^{-\frac{1}{4}} \|d\|^{\frac{3}{2}} \right) \\
    \overset{\eqref{eqn:capped-cg-sol-d-lower-bound}}&{\leq} 
    -\mu\beta^{\hat m_*}  M^{\frac{1}{2}} \omega \min\left( C_{M}^4M^{-1}  \omega^2, C_{M}^3 M^{-1}\omega^2  \right) \\
    &=
    -\mu\beta^{\hat m_*} C_{M}^3 \min\left( C_{M}, 1  \right) M^{-\frac{1}{2}} \omega^3.
\end{align*}


(5). Since $\hat m_* > m_{\mathrm{max}} \geq 0$, 
then \eqref{eqn:proof/sol-linesearch-failure-smaller-stepsize} holds with $j = 0$, which implies that $1 > 2 C_M^2$, i.e., $2M \leq L_H$.

(6). When $m_* = 0$, 
by the linesearch rule and \lemmaref{lem:capped-cg} we have
\begin{align}
    \label{eqn:newton-cg-sol-proof-ls0}
    \varphi(x + d) - \varphi(x)
    \leq \mu d^\top\nabla \varphi(x)
    \leq -\mu \sqrt M \omega \|d\|^2.
\end{align}
It remains to give a lower bound of $\|d\|$ as in \eqref{eqn:capped-cg-sol-d-lower-bound}, which is similar to the proof of \citet[Lemma 6.2]{he2023newton} with their $\epsilon_H$ and $\zeta$ replaced with our $\sqrt M\omega$ and $\tilde \eta$. 
Since special care must be taken with respect to $M$, we present the proof below.
Note that 
\begin{align*}
    \| \nabla \varphi(x + d) \|
    &\leq 
    \| \nabla \varphi(x + d) - \nabla \varphi(x) - \nabla^2\varphi(x) d \| \\
    &\peq 
    + \| \nabla \varphi(x) + (\nabla^2\varphi(x) + 2\sqrt M\omega \Id) d \|
      + 2 \sqrt M\omega \| d \| \\
    \overset{\eqref{eqn:capped-cg-hessian-upperbound}}&{\leq}
    \frac{L_H}{2} \| d \|^2
    + \sqrt M \left ( \frac{1}{2} \omega \tilde \eta
    + 2\omega \right ) \| d \|.
\end{align*}
Then, by the property of quadratic functions, we know 
\begin{align*}
    \| d \| 
    &\geq \frac{-(\tilde \eta + 4)+ \sqrt{(\tilde \eta + 4)^2 + 8L_H (\sqrt M\omega)^{-2}\| \nabla \varphi(x + d) \|}}{2L_H} \sqrt M\omega \\
    &\geq c_0 \sqrt M\omega \min\left( \omega^{-2}\| \nabla \varphi(x + d)\|, 1  \right),
\end{align*}
where 
$c_0 := \frac{4M^{-1}}{4 + \tilde \eta + \sqrt{(4 + \tilde \eta)^2 + 8M^{-1}L_H}} 
\geq \frac{2M^{-1}}{\sqrt{(4 + \tilde \eta)^2 + 8M^{-1}L_H}} 
\geq \frac{2M^{-1}}{\sqrt{25 + 8M^{-1}L_H}}$, 
and we have used the inequality $-a+\sqrt{a^2+bs}\geq(-a+\sqrt{a^2+b})\min(s,1)$ from \citet[Lemma 17]{royer2018complexity},
with $a=\tilde\eta+4 \leq 5$, $b=8L_H{M^{-1}}$ and $s = \omega^{-2}\|\nabla\varphi(x+d)\|$.
Combining with \eqref{eqn:newton-cg-sol-proof-ls0}, we get \eqref{eqn:newton-cg-sol-decay-ls0}. 
\end{proof}

\subsection{Proof of Lemma~\ref{lem:lipschitz-constant-estimation}} \label{sec:appendix/summarized-descent}

In this section, we provide the proof of \lemmaref{lem:lipschitz-constant-estimation}. 
It is highly technical but mostly based on the descent lemmas (\lemmaref{lem:newton-cg-nc,lem:newton-cg-sol}) and the choices of regularizers in \theoremref{thm:newton-local-rate-boosted}.

First, we give an auxiliary lemma for the claim about $k \in \cJ^{-1}$ in \lemmaref{lem:lipschitz-constant-estimation}. 
\begin{lemma}
    \label{lem:appendix/decreasing-Mk-condition}
    Suppose the following two properties are true:
    \begin{enumerate}
        \item Suppose $\text{d\_type}_k \neq \texttt{SOL}$ or $m_k > 0$. %
        If $M_k > \tilde C_4 L_H$ and $\omega_k \geq \tau_-\omega_k^{\supfallback}$, then $k \in \cJ^{-1}$;
        \item Suppose $\text{d\_type}_k = \texttt{SOL}$ and $m_k = 0$. 
        If $M_k > L_H$ and 
        $\min\big ( \omega^3_k, g_{k+1}^2 \omega_k^{-1} \big )
        \geq \tau_-(\omega_k^{\supfallback})^3$, then $k \in \cJ^{-1}$,
    \end{enumerate}
    where $\delta_k^\theta = \omega_k^{\supsucc} (\omega_k^{\supfallback})^{-1}$ is defined in \theoremref{thm:newton-local-rate-boosted}.
    Then, if $M_k > \tilde C_4 L_H$ and $\tau_- \leq \min\big ( \delta_k^\alpha, \delta_{k+1}^\alpha \big )$, 
    we know $k \in \cJ^{-1}$.
\end{lemma}
\begin{proof}
Let $\alpha = \max(2, 3\theta)$.
    We consider the following two cases: %
    \begin{enumerate}
        \item Note that $\tau_- < 1$. If $\omega_k < \tau_- \omega_k^{\supfallback}$, then we know the trial step is accepted since $\omega_k \neq \omega_k^{\supfallback}$, and hence, $\omega_k = \omega_k^{\supsucc}$ and $\tau_- > \delta_k^\theta \geq \delta_k^\alpha$ since $\delta_k \in (0, 1]$ and $\theta \leq \alpha$.
        \item  If $\min\big ( g_{k+1}^2 \omega_k^{-1}, \omega_k^3 \big ) < \tau_- (\omega_k^{\supfallback})^3$,
        we use the choice $\omega_k^{\supfallback} = \sqrt{g_k}$ as an example, the case for $\omega_k^{\supfallback} \sqrt{\epsilon_k}$ is similar and follows from $g_{k+1} \geq \epsilon_{k+1}$.
        In this case, we have $\delta_k = \min(1, g_kg_{k-1}^{-1})$.
        When the fallback step is taken, we have $\omega_k = \omega_k^{\supfallback}$, and
        \begin{align*}
            \tau_- > g_k^{-\frac{3}{2}}\min\big( g_{k+1}^2 g_k^{-\frac{1}{2}}, g_k^{\frac{3}{2}} \big) =  \delta^2_k.
        \end{align*}
        Since $\delta_k \in (0, 1]$ and $2 \leq \alpha$, we have $\tau_- > \delta_k^\alpha$.
    On the other hand, when the trial step is taken, we have $\omega_k = \omega_k^{\supsucc} = \sqrt{g_k} \delta_k^\theta$ and 
    \begin{align*}
    \tau_- 
    &> g_k^{-\frac{3}{2}}\min\big( g_{k+1}^2 g_k^{-\frac{1}{2}} \delta_k^{-\theta}, g_k^{\frac{3}{2}} \delta_k^{3\theta} \big)  
    \overset{(\delta_k \leq 1)}{\geq} 
    g_k^{-\frac{3}{2}}\min\big( g_{k+1}^2 g_k^{-\frac{1}{2}}, g_k^{\frac{3}{2}} \delta_k^{3\theta} \big)  \\
    &= \min\big( g_{k+1}^2 g_k^{-2}, \delta_k^{3\theta} \big)  
    \geq \min\big ( \delta_{k+1}^2, \delta_k^{3\theta}  \big ) 
    \geq \min\big ( \delta_{k+1}^\alpha, \delta_k^\alpha \big ).
    \end{align*}
    \end{enumerate}
    Conversely, we find when $\tau_- \leq \min\big ( \delta_k^\alpha, \delta_{k+1}^\alpha \big )$, 
    the assumptions of this lemma give that $k \in \cJ^{-1}$.
\end{proof}

We will also show that the two properties listed in \lemmaref{lem:appendix/decreasing-Mk-condition} hold in the proof of \lemmaref{lem:lipschitz-constant-estimation} below, and leave this fact as a corollary for our subsequent usage.
\begin{corollary}
    \label{cor:appendix/decreasing-Mk-condition}
    Under the regularizers in \theoremref{thm:newton-local-rate-boosted}, 
    the two properties in \lemmaref{lem:appendix/decreasing-Mk-condition} hold.
\end{corollary}

\begin{proof}[Proof of \lemmaref{lem:lipschitz-constant-estimation}]
    Define $\Delta_k = \varphi(x_k) - \varphi(x_{k+1})$.
    We denote $\omega_k = \omega_k^{\supsucc}$ if the trial step is taken, and $\omega_k = \omega_k^{\supfallback}$ otherwise.

    \paragraph{Case 1}
    When $\text{d\_type}_k = \texttt{SOL}$ and $m_k = 0$, i.e., $x_{k+1} = x_k + d_k$, 
    we define $E_k := \min \left(  g_{k+1}^2\omega_k^{-1}, \omega_k^3 \right)$.
    \begin{enumerate}
        \item When $k \in \cJ^1$, i.e., $M_{k + 1} = \gamma M_k$, we have
        \begin{align*}
            \frac{4\mu}{33} \tau_+ M_k^{-\frac{1}{2}} E_k
            \geq \Delta_k
            \overset{\eqref{eqn:newton-cg-sol-decay-ls0}}{\geq}
        \frac{4\mu M_k^{-\frac{1}{2}}}{25 + 8L_HM_k^{-1}} E_k,
        \end{align*}
        where the first inequality follows from the condition for increasing $M_k$ in \Cref{alg:adap-newton-cg}.
        The above display implies $25 + 8L_HM_k^{-1} \geq 33\tau_+^{-1} \geq 33$ as $\tau_+ \leq 1$, and hence, $M_k \leq L_H$.
        \item When $E_k \geq \tau_-(\omega_k^{\supfallback})^3$ 
        and $M_k > L_H$, 
        we have $k\in \cJ^{-1}$ since
        \begin{align*}
            \Delta_k
            \overset{\eqref{eqn:newton-cg-sol-decay-ls0}}{\geq}
            \frac{4\mu M_k^{-\frac{1}{2}} E_k}{25 + 8L_HM_k^{-1}}
            > \frac{4\mu M_k^{-\frac{1}{2}}\tau_- (\omega_k^{\supfallback})^3}{25 + 8}
            = \frac{4}{33}\mu\tau_-  M_k^{-\frac{1}{2}}(\omega_k^{\supfallback})^3,
        \end{align*}
        which satisfies the condition in \Cref{alg:adap-newton-cg} for decreasing $M_k$ since $\bar \omega$ therein is $\omega_k^{\supfallback}$.
        Thus, the second property of \lemmaref{lem:appendix/decreasing-Mk-condition} is true.
    \end{enumerate}

    \paragraph{Case 2}
    When $\text{d\_type}_k = \texttt{SOL}$, 
    and let $m_*$ and $\hat m_*$ be the smallest integer such that \eqref{eqn:smooth-line-search-sol} and \eqref{eqn:smooth-line-search-sol-smaller-stepsize} hold, respectively, as defined in \lemmaref{lem:newton-cg-sol}.
    We also recall that $C_{M_k}^2 = \frac{3(1-\mu)M_k}{L_H} \geq \frac{M_k}{L_H}$.

    Since the previous case addresses $m_* = 0$, we assume $m_* > 0$ here.
    Then, the condition for increasing $M_k$ in \Cref{alg:adap-newton-cg} is
    \begin{equation}
        \label{eqn:proof/cond-inc-Mk-SOL}
        \Delta_k \leq \tau_+ \beta \mu M_k^{-\frac{1}{2}} \omega_k^3.
    \end{equation}
    The condition for decreasing $M_k$ is
    \begin{equation}
        \label{eqn:proof/cond-dec-Mk-SOL}
        \Delta_k \geq \mu \tau_- M_k^{-\frac{1}{2}} (\omega_k^{\supfallback})^3.
    \end{equation}

    \begin{enumerate}
        \item When $k \in \cJ^1$ and $m_{\mathrm{max}} \geq m_* > 0$,
        i.e., $m_k = m_*$ and $x_{k+1} = x_k = \beta^{m_k} d_k$,
         we have
    \begin{align*}
         \tau_+ \beta \mu M_k^{-\frac{1}{2}} \omega_k^3
         \overset{\eqref{eqn:proof/cond-inc-Mk-SOL}}{\geq}
        \Delta_k
         \overset{\eqref{eqn:newton-cg-sol-decay}}{\geq}
         \frac{36\beta\mu(1 - \mu)^2}{L_H^2} M_k^{\frac{3}{2}} \omega_k^3
         \geq \frac{9\beta\mu}{L_H^2} M_k^{\frac{3}{2}} \omega_k^3,
    \end{align*}
    Since $\tau_+ \leq 1$, then we know $M_k \leq \tau_+^{\frac{1}{2}} L_H / 3 \leq L_H / 3$.
        \item When $m_{\mathrm{max}} \geq m_*  > 0$ and $M_{k} \geq \tau_-^{-1}(9\beta)^{-\frac{1}{2}}L_H$ 
        and $\omega_k \geq \tau_- \omega_k^{\supfallback}$,
    then 
    \begin{align*}
        \Delta_k
         \overset{\eqref{eqn:newton-cg-sol-decay}}{\geq}
        \frac{9\beta\mu}{L_H^2} M_k^{\frac{3}{2}}  \omega_k^3
        = \left (\frac{9\beta\mu}{L_H^2} M_k^{2} \right ) M_k^{-\frac{1}{2}} \omega_k^3
        \geq 
        \mu \tau_-^{-2} M_k^{-\frac{1}{2}} (\tau_-^{3}(\omega_k^{\supfallback})^3)
        =
        \mu \tau_- M_k^{-\frac{1}{2}} (\omega_k^{\supfallback})^3,
    \end{align*}
        which satisfies \eqref{eqn:proof/cond-dec-Mk-SOL}, and hence $k \in \cJ^{-1}$.

        \item When $k \in \cJ^1$ and $m_* > m_{\mathrm{max}}$ and $m_{\mathrm{max}} \geq \hat m_* \geq 0$, then we know 
        \begin{align*}
        \tau_+ \beta \mu M_k^{-\frac{1}{2}} \omega_k^3 
         \overset{\eqref{eqn:proof/cond-inc-Mk-SOL}}{\geq}
        \Delta_k
        \overset{\eqref{eqn:newton-cg-sol-decay-smaller-stepsize}}{\geq} 
        \mu \beta^{\hat m_*} C_{M_k}^3 \min\left( C_{M_k}, 1 \right) M_k^{-\frac{1}{2}} \omega_k^3
        ,
        \end{align*}
        which implies $\beta \geq \beta \tau_+ \geq  \beta^{\hat m_*} C_{M_k}^3 \min\left( C_{M_k}, 1 \right)$.
        If $C_{M_k} \leq 1$, then its definition implies that $M_k \leq 2L_H / 3$.
        Otherwise, we have 
    $\beta \geq \beta^{\hat m_*} C_{M_k}^3$.
    When $\hat m_* = 0$, we know $C_{M_k}^3 \leq \beta \leq 1$ and hence $M_k \leq 2L_H / 3$;
    when $\hat m_* > 0$, \lemmaref{lem:newton-cg-sol} shows $\beta^{\hat m_* - 1} > \sqrt 2 C_{M_k} > C_{M_k}$, 
    and hence $C_{M_k}^4 \leq 1$, leading to $M_k \leq 2L_H / 3$.
        \item When $m_* > m_{\mathrm{max}}$ and $m_{\mathrm{max}} \geq \hat m_* \geq 0$, and  $M_k \geq L_H$, we have $C_{M_k} \geq 1$ 
        and by \lemmaref{lem:newton-cg-sol}, $\hat m_* = 0$, 
        since otherwise we have $1 \geq \beta^{\hat m_* - 1} > \sqrt{2} C_{M_k} > 1$, leading to a contradiction.
    Then, \eqref{eqn:newton-cg-sol-decay-smaller-stepsize} gives  
    $\Delta_k \geq \mu M_k^{-\frac{1}{2}} \omega_k^3$, 
    and therefore $k \in \cJ^{-1}$ as long as $\omega_k \geq \tau_- \omega_k^{\supfallback}$.
    \item When $m_* > m_{\mathrm{max}}$ and $\hat m_* > m_{\mathrm{max}}$, then \lemmaref{lem:newton-cg-sol} shows that $M_k \leq L_H / 2$,
    and the algorithm directly increases $M_k$ so that $k \in \cJ^1$.
    \end{enumerate}
    The above arguments show that when $k \in \cJ^1$, we have $M_k \leq L_H \leq \tilde C_5 L_H$,
    and when $\omega_k \geq \tau_- \omega_k^{\supfallback}$ and $M_k > \tilde C_4 L_H  \geq  \max(1, \tau_-^{-1}(9\beta)^{-\frac{1}{2}}) L_H$, we have $k \in \cJ^{-1}$, i.e., the first property of \lemmaref{lem:appendix/decreasing-Mk-condition} is true for \texttt{SOL} case.

    \paragraph{Case 3}
     When $\text{d\_type}_k = \texttt{NC}$, let $m_*$ be the smallest integer such that \eqref{eqn:smooth-line-search-nc} holds, as defined in \lemmaref{lem:newton-cg-nc}.
     In this case, the condition for decreasing $M_k$ is also \eqref{eqn:proof/cond-dec-Mk-SOL}, and the condition for increasing it is 
    \begin{equation}
        \label{eqn:proof/cond-inc-Mk-NC}
        \Delta_k \leq \tau_+ (1 - 2\mu)^2 \beta^2 \mu M_k^{-\frac{1}{2}} \omega_k^3.
    \end{equation}
     \begin{enumerate}
        \item When $k \in \cJ^1$ and $m_* > 0$, we can similarly use  \eqref{eqn:newton-cg-nc-decay} in \lemmaref{lem:newton-cg-nc} and \eqref{eqn:proof/cond-inc-Mk-NC} to show that $M_k \leq L_H / 3$. 
        \item When $m_* > 0$ and  $M_k \geq \tau_-^{-1} (3\beta(1-2\mu))^{-1} L_H$ and $\tau_-\omega_k^{\supfallback}  \leq\omega_k$, then \lemmaref{lem:newton-cg-nc} shows that \eqref{eqn:proof/cond-dec-Mk-SOL} holds. Therefore,  $k \in \cJ^{-1}$.
        \item When $m_* = 0$, we show that $M_{k+1}$ will not increase, since otherwise \eqref{eqn:newton-cg-nc-decay-ls0} and \eqref{eqn:proof/cond-inc-Mk-NC} imply that $1 > (1 - 2\mu)^2\beta^2 \tau_+ \geq 1$, leading to a contradiction.
        \item When $m_* = 0$ and $\tau_-\omega_k^{\supfallback} \leq \omega_k$, we know \eqref{eqn:proof/cond-dec-Mk-SOL} holds from \eqref{eqn:newton-cg-nc-decay-ls0} and $\tau_- < 1$, and hence $k \in \cJ^{-1}$.
        \item When $m_* > m_{\mathrm{max}}$ and $\hat m_* > m_{\mathrm{max}}$, then \lemmaref{lem:newton-cg-nc} shows that $M_k \leq L_H / (3 - 6\mu)$,
        and the algorithm directly increases $M_k$ so that $k \in \cJ^1$.
     \end{enumerate}
    The above arguments show that when $k \in \cJ^1$, we have $M_k \leq L_H / \min(1, 3 - 6\mu) \leq \tilde C_5 L_H$,
    and when $\omega_k \geq \tau_- \omega_k^{\supfallback}$ and $M_k > \tilde C_4 L_H \geq \tau_-^{-1} (3\beta(1 - 2\mu))^{-1} L_H$, we have $k \in \cJ^{-1}$, i.e., the first property of \lemmaref{lem:appendix/decreasing-Mk-condition} is true for \texttt{NC} case.

    \paragraph{The cardinality of $\cJ^i$}
    By the definition of $\cJ^i$, we have 
    \begin{align*}
    \log_\gamma M_k = \log_\gamma M_0  + |I_{0,k} \cap \cJ^1|-|I_{0,k} \cap \cJ^{-1}|.
    \end{align*}
    For each $k$ we know $M_{k+1} > M_k$ only if $M_k \leq \tilde C_5 L_H$, 
    then $\sup_{k} M_k \leq \max(M_0, \gamma \tilde C_5 L_H)$, 
    and hence \eqref{eqn:cardinarlity-of-M-set-a} holds.
    Adding $|I_{0,k} \setminus \cJ^1|$ to both sides of \eqref{eqn:cardinarlity-of-M-set-a}, we find
    \eqref{eqn:cardinarlity-of-M-set} holds.

    \paragraph{The descent inequality}
    The $D_k$ dependence in \eqref{eqn:summarized-descent-inequality} directly follow from \lemmaref{lem:newton-cg-nc,lem:newton-cg-sol}.
     For the preleading coefficients, we consider the following three cases.
     (1). When $k \in \cJ^1$, the result also follows from the two lemmas and the fact that $M_k \geq 1$.
     We also note that the $L_H^{-\frac{5}{2}}$ dependence only comes from the case where $\text{d\_type} = \texttt{SOL}$ and $m$ does not exist, and for other cases the coefficient is of order $L_H^{-2}$;
     (2). When $k \in \cJ^{-1}$, the result follows from the algorithmic rule of decreasing $M_k$;
     (3). When $k \in \cJ^0$,
     we know the rules in the algorithm for increasing $M_k$ fail to hold, yielding an $M_k^{-\frac{1}{2}}$ dependence of the coefficient.
\end{proof}

\subsection{Proof of Lemma~\ref{lem:main/lower-bound-of-Vk}}
\label{sec:appendix/proof-lower-bound-of-Vk}

\begin{proof}[Proof of \lemmaref{lem:main/lower-bound-of-Vk}]
    When $\omega_k^{\supfallback} = \sqrt{g_k}$, the upper bound over $V_k$ follows from the monotonicity of $\log\log\frac{3A}{a}$.
    On the other hand, when $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$, 
    we know $3\epsilon_{\ell_j-1} \geq 2\epsilon_{\ell_j-1} \geq 2\epsilon_{\ell_{j+1}-1}$ since $\{ \epsilon_k \}_{k \geq 0}$ is non-increasing.
    Then, we can apply \lemmaref{lem:summing-log-log-sequence} below with $a = 3$ to obtain
    \begin{align*}
        V_k 
        &\leq 
        \sum_{j=1}^{J_k-1}
        \log\log \frac{3\epsilon_{\ell_j-1}}{\epsilon_{\ell_{j+1}-1}}
        + \log \log \frac{3\epsilon_{\ell_{J_k-1}}}{\epsilon_{k}} 
        \\
        \overset{\eqref{eqn:summing-log-log-sequence}}&{\leq}
        \frac{1}{\log 3} \log \frac{\epsilon_{\ell_1-1}}{\epsilon_k} + J_k \log\log 3
        \leq \log \frac{\epsilon_{0}}{\epsilon_k} + J_k,
    \end{align*}
    where we have used the fact that $\log 3 \geq 1$ and $\log \log 3 \leq 1$.
\end{proof}

\begin{lemma}
        \label{lem:summing-log-log-sequence}
    Let $\{ b_j \}_{j \geq 1} \subseteq (0, \infty)$ be a sequence, and $a \geq 3$, $ab_j\geq 2b_{j+1}$, then we have for any $k \geq 1$,
    \begin{equation}
        \label{eqn:summing-log-log-sequence}
        \sum_{j=1}^k \log\log \frac{ab_{j}}{b_{j+1}}
        \leq\frac{1}{\log a} \log \frac{b_1}{b_{k+1}}+ k \log \log a. 
    \end{equation}
\end{lemma}
\begin{proof}
    Using the fact $\log(1 + x) \leq x$ for $x > -1$, and $\log b_j - \log b_{j+1} \geq -\log a + \log 2 > -\log a$, we have
    \begin{align*}
        \sum_{j=1}^k \log\log \frac{ab_{j}}{b_{j+1}}
        &= 
        \sum_{j=1}^k \log\left( 1  + \frac{\log b_j - \log b_{j+1}}{\log a} \right) + k \log \log a \\
        &\leq 
        \sum_{j=1}^k \left( \frac{\log b_j - \log b_{j+1}}{\log a} \right) + k \log \log a \\
        &= \frac{\log b_1 - \log b_{k+1}}{\log a}+ k \log \log a,
    \end{align*}
    which completes the proof.
\end{proof}

\subsection{The counting lemma}\label{sec:proof-counting-lemma}

\begin{lemma}[Counting lemma]
    \label{lem:basic-counting-lemma}
    Let $\cJ^{-1}, \cJ^0, \cJ^1 \subset \N$ be the sets in \lemmaref{lem:lipschitz-constant-estimation}, then we have
    at least one of the following inequalities holds:
    \begin{align}
        \label{eqn:basic-counting-lemma-loss-descent}
        \Sigma_k &\geq 
        \frac{k}{5(U_k+2)} - [\log_\gamma (\tilde C_5 M_0^{-1} L_H)]_+ - 2, \\
        \label{eqn:basic-counting-lemma-gradient-descent}
        W_k&\geq \frac{k}{3(U_k+2)},
    \end{align}
    where $\Sigma_k := |I_{0,k} 
    \cap \cJ^{-1}| + \max\left( |S_k \cap \cJ^0|, |I_{0,k}
    \cap\cJ^0| - W_k - U_kJ_k \right)$, and $S_k \subseteq I_{0,k}$, 
    $U_k \geq 0$, $J_k -1 = |S_k|$ and $W_k \in \R$, and $\tilde C_5$ is defined in \lemmaref{lem:lipschitz-constant-estimation}, $M_0$ is the input in \Cref{alg:adap-newton-cg}.
\end{lemma}
\begin{proof}
    Denote $B_k = (U_k + 2)^{-1}|
    I_{0,k} \cap \cJ^0|$
    and $\Gamma_k = [\log_\gamma(\gamma\tilde C_5 M_0^{-1}L_H)]_+$. 
    We consider the following five cases, where the first three cases deal with $J_k < B_k$, and the last two cases are the remaining parts.  
    We also note that the facts $|I_{0,k}|=k$ and  $1 \geq \frac{2}{U_k + 2}$ are frequently used.

    \paragraph{Case 1}
    When $J_k < B_k$ and $W_k < B_k$, we have 
    \begin{align*}
        \Sigma_k 
        &\geq 
        |%
        I_{0,k}\cap \cJ^{-1}| 
        + |%
        I_{0,k}\cap \cJ^{0}| 
        - U_kJ_k - W_k
        >
        |%
        I_{0,k}\cap \cJ^{-1}| 
        + \frac{|%
        I_{0,k}\cap \cJ^{0}|}{U_k+2} \\
        &\geq 
        \frac{2|%
        I_{0,k}\cap \cJ^{-1}| + |%
        I_{0,k}\cap \cJ^{0}|}{U_k+2}
        \overset{\eqref{eqn:cardinarlity-of-M-set}}{\geq} 
        \frac{k - \Gamma_k}{U_k+2}.
    \end{align*}
    
    \paragraph{Case 2}
    When $J_k < B_k \leq W_k$, and $|%
    I_{0,k}\cap \cJ^0| \leq \frac{k}{3}$, 
    then by \eqref{eqn:cardinarlity-of-M-set} we know 
    $k \leq 2|%
    I_{0,k}\cap \cJ^{-1}| + \frac{k}{3} + \Gamma_k$, and hence, 
    $\Sigma_k \geq |%
    I_{0,k}\cap \cJ^{-1}| \geq \frac{k}{3} - \frac{1}{2}\Gamma_k$.

    \paragraph{Case 3}
    When $J_k < B_k \leq W_k$, and $|%
    I_{0,k}\cap \cJ^0| > \frac{k}{3}$, 
    then $W_k \geq B_k > \frac{k}{3(U_k+2)}$.

    \paragraph{Case 4}
    When $|S_k \cap \cJ^0| > B_k/2$, we have
    \begin{align*}
        \Sigma_k 
        \geq 
        |%
        I_{0,k}\cap \cJ^{-1}| + |S_k\cap \cJ^0|
        \geq \frac{2|%
        I_{0,k}\cap \cJ^{-1}|+|%
        I_{0,k}\cap\cJ^0|}{2(U_k+2)}
        \overset{\eqref{eqn:cardinarlity-of-M-set}}{\geq} 
        \frac{k - \Gamma_k}{2(U_k+2)}.
    \end{align*}

    \paragraph{Case 5}
     When $J_k \geq B_k$ and $|S_k \cap \cJ^0| \leq B_k/2$, we have
    \begin{align*}
        B_k - 1 \leq J_k - 1
        = |S_k| 
        &=
         |S_k \cap \cJ^0|
        + |S_k \cap \cJ^1|
        + |S_k \cap \cJ^{-1}|\\
        &\leq \frac{B_k}{2}
        + |%
        I_{0,k}\cap \cJ^1|
        + |%
        I_{0,k} \cap \cJ^{-1}|\\
        \overset{\eqref{eqn:cardinarlity-of-M-set-a}}&{\leq}
        \frac{B_k}{2}
        + 2|%
        I_{0,k}\cap \cJ^{-1}|
        + \Gamma_k.
    \end{align*}
    Therefore, we have
    \begin{align*}
        \Sigma_k 
        &\geq 
        |%
        I_{0,k}\cap \cJ^{-1}|\\
        &=
        \frac{1}{5}|%
        I_{0,k} \cap \cJ^{-1}|
        + \frac{4}{5}|%
        I_{0,k} \cap \cJ^{-1}| \\
        &\geq 
        \frac{1}{5}\cdot\frac{8|%
        I_{0,k} \cap \cJ^{-1}|}{4(U_k+2)}
        + \frac{4}{5}\left( \frac{B_k}{4} - \frac{1}{2} - \frac{\Gamma_k}{2} \right) \\
        &=
        \frac{1}{5} \left ( 
            \frac{8|%
            I_{0,k}\cap\cJ^{-1}| + 4|%
            I_{0,k}\cap\cJ^0|}{4(U_k+2)}
            - 2 - 2\Gamma_k
            \right ) \\ 
        \overset{\eqref{eqn:cardinarlity-of-M-set}}&{\geq}
        \frac{1}{5} \left ( 
            \frac{k - \Gamma_k}{U_k+2}
            - 2 - 2\Gamma_k
            \right ).
    \end{align*}
    Summarizing the above cases, we conclude that 
    \begin{align*}
        \Sigma_k \geq 
        \frac{k}{5(U_k+2)} - \Gamma_k - \frac{2}{5}
        \geq \frac{k}{5(U_k+2)} - [\log_\gamma (\tilde C_5 M_0^{-1}L_H)]_+ - 2,
    \end{align*}
    and the proof is completed. %
\end{proof}

\subsection{Technical lemmas for Lemma~\ref{lem:main/iteration-in-a-subsequence}} \label{sec:summing-lemmas}

This section establishes two crucial lemmas for proving \lemmaref{lem:main/iteration-in-a-subsequence} (a.k.a. \lemmaref{lem:proof/iteration-in-a-subsequence} in the appendix). 
\lemmaref{lem:accumulated-descent-lower-bound}, mentioned in the ``sketch of the idea'' part of \lemmaref{lem:main/iteration-in-a-subsequence}, is  specifically applied to the case  $\theta = 0$.
For $\theta > 0$, we employ a modified version of this result as detailed in \lemmaref{lem:accumulated-mixed-descent-lower-bound}.

\begin{lemma}
    \label{lem:accumulated-descent-lower-bound}
    Given $K \in \N$, $p > q > 0$, and $A \geq  a > 0$, and let $\{ g_j \}_{0 \leq j \leq K+1}$ be such that 
    $A = g_0 \geq g_1 \geq \dots \geq g_K \geq g_{K+1} = a$.
    Then, for any subset $S \subseteq [K]$, we have
    \begin{align}
        \sum_{i \in S} \frac{g_{i+1}^p}{g_{i}^q} 
        \geq \max(0, |S| - R_a - 2) \ce^{-q} a^{p-q} 
        ,
        \label{eqn:accumulated-descent-lower-bound}
    \end{align}
    where $R_a := \left \lfloor \log \log \frac{3A}{a} - \log \log \frac{p}{q} \right \rfloor \leq \log \log \frac{3A}{a}$.
\end{lemma}
\begin{proof}
    It suffices to consider the case where $A = 1$, since for general cases, we can invoke the result of $A = 1$ with $g_j, a$ replaced with $g_j / A$, $a / A$, respectively.
    Let $\tau = p/q$ and $\cI_k = \{ j \in [K] : \exp(\tau^{k}) a \leq g_j < \exp(\tau^{k+1}) a \}$ with $0 \leq k \leq R_a$ and $\cI_{-1} = \{ j \in [K] : a \leq g_j < \ce a \}$. 
    Let $\zeta_k = \exp(\tau^k)$ for $k \geq 0$ and $\zeta_{-1} = 1$, then we have $\zeta_k^p \zeta_{k+1}^{-q} \geq \ce^{-q}$.
    Note that $\{ \cI_k \}_{-1 \leq k \leq R_a}$ is a partition of $[K]$, then we have
    \begin{align}
        \sum_{i\in S} \frac{g_{i+1}^p}{g_{i}^q}
        &= 
        \sum_{k=-1}^{R_a} \sum_{j \in \cI_k \cap S} \frac{g_{j+1}^p}{g_{j}^q} 
        = \sum_{k=-1}^{R_a}
        \left(  
         \sum_{\substack{j \in S\\ j,j+1 \in \cI_k}} \frac{g_{j+1}^p}{g_{j}^q} 
         + \sum_{\substack{j \in S \\j \in \cI_k, j+1\notin \cI_k}} \frac{g_{j+1}^p}{g_{j}^q} 
        \right)
         \nonumber\\
        &\geq \sum_{k=-1}^{R_a} 
        \sum_{\substack{j \in S \\j, j+1 \in \cI_k}} \frac{\left( \zeta_k a \right)^p}{\left (\zeta_{k+1}a\right )^q}
        \geq 
        \sum_{k=-1}^{R_a} 
        \sum_{\substack{j \in S \\j, j+1 \in \cI_k}} \ce^{-q}a^{p-q}
        = |\cI_S| \ce^{-q}a^{p-q}
        ,
        \label{eqn:newton-pq-superlinear-penalty}
    \end{align}
    where $\cI_S := \{ j \in S : j, j + 1 \in \cI_k, -1 \leq k \leq R_a \}$.
    By the monotonicity of $g_j$, we know for each $k$, there exists at most one $j \in \cI_k$ such that $j + 1 \notin \cI_k$. 
    Hence, $|\cI_S| \geq |S| - (R_a + 2)$.
\end{proof}

\begin{lemma}
    \label{lem:accumulated-mixed-descent-lower-bound}
    Given $K \in \N$, $p_1 > q_1 > 0$, $p_2 > q_2 > 0$ and $A \geq a > 0$, and let $\{ g_j \}_{0 \leq j \leq K+1}$ be such that 
    $A = g_0 \geq g_1 \geq \dots \geq g_K \geq g_{K+1} = a$.
    Then, for any subset $S \subseteq [K]$, we have
    \begin{align}
        &\sum_{i \in S}
        \min \left ( 
            A^{q_1 - p_1}\frac{g_{i+1}^{p_1}}{g_{i}^{q_1}},
            A^{q_2 - p_2}\frac{g_{i}^{p_2}}{g_{i-1}^{q_2}} 
        \right )  
        \nonumber
        \\
        &\quad\quad\quad \geq 
        \max(0, |S| - R_{a,1} - R_{a,2} - 4) \min \left ( \left( A^{-1}a \right)^{p_1 - q_1}, \left( A^{-1}a \right)^{p_2 - q_2} \right ).
        \label{eqn:accumulated-mixed-descent-lower-bound}
    \end{align}
    where 
    $R_{a,i} := \left \lfloor \log \log \frac{3A}{a} - \log \log \frac{p_i}{q_i} \right \rfloor \leq \log\log\frac{3A}{a}$ 
    for $i = 1, 2$.
\end{lemma}


\begin{proof}%
    Similar to \lemmaref{lem:accumulated-descent-lower-bound}, it suffices to show that \eqref{eqn:accumulated-mixed-descent-lower-bound} is true for $A = 1$.
    Let $\tau_i = p_i/q_i$ for $i = 1, 2$ and $\cI_k = \{ j \in [K] : \exp(\tau_1^{k}) a \leq g_j < \exp(\tau_1^{k+1}) a \}$ with $0 \leq k \leq R_{a,1}$ and $\cI_{-1} = \{ j \in [K] : a \leq g_j < \ce a \}$. 
    Note that $\{ \cI_k \}_{-1 \leq k \leq R_{a,1}}$ is a partition of $[K]$, then similar to \eqref{eqn:newton-pq-superlinear-penalty} we have
    \begin{align*}
        \sum_{i\in S} 
        \min \left ( 
            \frac{g_{i+1}^{p_1}}{g_{i}^{q_1}},
            \frac{g_{i}^{p_2}}{g_{i-1}^{q_2}} 
        \right )  
        &\geq 
        \sum_{k=-1}^{R_{a,1}} 
        \sum_{\substack{j \in S \\j, j+1 \in \cI_k} }
        \min \left ( 
            \ce^{-q_1}a^{p_1 - q_1},
            \frac{g_{i}^{p_2}}{g_{i-1}^{q_2}} 
        \right ) \\
        &\geq \sum_{j \in \cI_S}
        \min \left ( 
           \ce^{-q_1} a^{p_1 - q_1},
            \frac{g_{i}^{p_2}}{g_{i-1}^{q_2}} 
        \right )
        .
    \end{align*}
    where $\cI_S := \{ j \in S : j, j + 1 \in \cI_k, -1 \leq k \leq R_{a,1} \}$ and we have used the fact that $\min(\alpha_1, \beta) \geq \min(\alpha_2, \beta)$ if $\alpha_1 \geq \alpha_2$.
    Moreover, we can also conclude that $|\cI_S| \geq |S| - R_{a,1} - 2$.

    Next, we consider the partition of $\cI_S$ and lower bound the summation in the above display.
    Let $\cJ_k = \{ j \in \cI_S : \exp(\tau_2^k)a \leq g_j < \exp(\tau_2^{k+1})a \}$ with $0 \leq k \leq R_{a,2}$, $\cJ_{-1} = \{ j \in \cI_S : a \leq g_j < \ce a \}$, and $\cJ_S := \{ j \in S : j, j - 1 \in \cJ_k, -1 \leq k \leq R_{a,2} \}$. 
    Then, similar to \eqref{eqn:newton-pq-superlinear-penalty} we have
    \begin{align*}
        \sum_{j \in \cI_S}
        \min \left ( 
            \ce^{-q_1} a^{p_1 - q_1},
            \frac{g_{i}^{p_2}}{g_{i-1}^{q_2}} 
        \right ) 
        &\geq 
        \sum_{k=-1}^{R_{a,2}} 
        \sum_{j, j-1 \in \cJ_k} 
        \min \left ( 
           \ce^{-q_1} a^{p_1 - q_1},
           \ce^{-q_2} a^{p_2 - q_2}
        \right )
         \\
        &=
        |\cJ_S| \min \left ( 
            \ce^{-q_1}a^{p_1 - q_1},
            \ce^{-q_2}a^{p_2 - q_2}
        \right ).
    \end{align*}
    Therefore, the proof is completed by noticing that $|\cJ_S| \geq |\cI_S| - R_{a,2} - 2$.
\end{proof}
