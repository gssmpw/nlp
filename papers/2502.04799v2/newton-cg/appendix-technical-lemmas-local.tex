
\section{Technical lemmas for local rates}


\subsection{Standard properties of the Newton step}
\label{sec:properties-of-newton-step}

This section provides the proofs of \lemmaref{lem:gradient-decay-of-newton-step,lem:asymptotic-newton-step}, which are the detailed version of \lemmaref{lem:main/asymptotic-newton-properties}.

The following lemma is used to show that $\nabla^2\varphi(x)\succ 0$ in a neighborhood of $x^*$. 
It can be found in, e.g., \citet[Lemma 7.2.12]{facchinei2003finite}.
\begin{lemma}[Perturbation lemma]
    \label{lem:perturbation-lemma}
    Let $A, B \in \R^{n \times n}$ with $\| A^{-1} \| \leq \alpha$. 
    If $\| A - B \| \leq \beta$ and $\alpha\beta < 1$, then 
    \begin{equation}
        \label{eqn:perturbation-lemma}
        \|B^{-1} \| \leq \frac{\alpha}{1 - \alpha\beta}.
    \end{equation}
\end{lemma}

\begin{corollary}
    \label{cor:perturbation-lemma}
    Under Assumption~\ref{assumption:local-strong-convexity}, we have the following properties:
    \begin{enumerate}
        \item When $x \in B_{\delta_0}(x^*)$, we know $\nabla^2\varphi(x) \succeq \frac{\alpha}{2} \Id$ and $\| (\nabla^2\varphi(x))^{-1} \| \leq \frac{2}{\alpha}$.
        \item $\frac{\alpha}{2} \| x - y \| \leq \| \nabla \varphi(x) - \nabla \varphi(y) \| \leq L_g \| x - y \|$ for $x, y \in B_{\delta_0}(x^*)$.
    \end{enumerate}
\end{corollary}
\begin{proof}
    The first part directly follows from \lemmaref{lem:perturbation-lemma}.
    Since $\nabla^2\varphi$ is $L_H$-Lipschitz, then 
    \begin{align*}
    \sup_{x \in B_{\delta_0}(x^*)} \| \nabla^2 \varphi(x) \| 
    \leq \| \nabla^2\varphi(x^*) \| + L_H \delta_0 = L_g,
    \end{align*}
    implying that $\nabla \varphi$ is $L_g$-Lipschitz on $B_{\delta_0}(x^*)$.
    Then, the second part follows from \citet[Section 1]{nesterov2018lectures}.
\end{proof}

\begin{proof}[Proof of \lemmaref{lem:gradient-decay-of-newton-step}]
    From \corollaryref{cor:perturbation-lemma}, we know $H \succeq \frac{\alpha}{2} \Id$ and 
    $\| H^{-1} \| \leq \frac{2}{\alpha}$ for every $x \in B_{\delta}(x^*)$ and $H = \nabla^2\varphi(x)$.
    Then, let $\epsilon = M^{\frac{1}{2}}\omega$ and note that by the choice in \Cref{alg:adap-newton-cg}, $\tilde \eta \leq M^{\frac{1}{2}} \omega = \epsilon$, we have
    \begin{align}
    \nonumber
        \| x^* - (x + d) \|
        &\leq 
        \| (H + 2\epsilon \Id)^{-1} \nabla \varphi(x) + (x^* - x) \|
        + \| d + (H + 2\epsilon \Id)^{-1} \nabla \varphi(x)  \| \\
    \nonumber
        \overset{\eqref{eqn:capped-cg-hessian-upperbound}}&{\leq}
       \|(H + 2\epsilon \Id)^{-1} \|
       \left ( 
        \| \nabla \varphi(x) + H (x^* - x)  \|
        + 2\epsilon \| x^* - x \|
        + \tilde \eta    \| \nabla \varphi(x) \|
        \right ) \\
    \nonumber
        &\leq 
        \frac{2}{\alpha}
       \left ( 
        \| \nabla \varphi(x) + H (x^* - x)  \|
        + 2\epsilon \| x^* - x \|
        + 2\epsilon \| \nabla \varphi(x) \|
        \right ) \\
        \overset{\eqref{eqn:hessian-lip-gradient-inequ}}&{\leq}
        \frac{2}{\alpha}
       \left ( 
        L_H \| x^* - x \|^{2}
        + 2\epsilon \| x^* - x \|
        + 2\epsilon \| \nabla \varphi(x) \|
        \right ).
        \label{eqn:proof/newton-distance-decay-asymptotic}
    \end{align}
    From \corollaryref{cor:perturbation-lemma}, we know $\frac{\alpha}{2} \| x - x^*\| \leq \| \nabla \varphi(x) \| \leq L_g \| x - x^* \|$, 
    yielding \eqref{eqn:distance-decay-of-newton-step}.

    Furthermore, we have
    \begin{align*}
        \| \nabla \varphi(x + d)\|
        \leq  L_g \| x^* - (x + d) \|
        \overset{\eqref{eqn:proof/newton-distance-decay-asymptotic}}&{\leq} 
        \frac{2L_g}{\alpha}
       \left ( 
        L_H \| x^* - x \|^{2}
        + 2\epsilon \| x^* - x \|
        + 2\epsilon \| \nabla \varphi(x) \|
        \right ) \\
        &\leq 
        \frac{2L_g}{\alpha}
       \left ( 
        \frac{4L_H}{\alpha^2} \| \nabla \varphi(x) \|^{2}
        + \frac{4 + 2\alpha}{\alpha} \epsilon \| \nabla \varphi(x) \|
        \right ).
    \end{align*}
\end{proof}

\begin{proof}[Proof of \lemmaref{lem:asymptotic-newton-step}]
    Let $r_k = \| x_k - x^* \|$, the proof is divided to three steps.

    \paragraph{Step 1}
    We show that $\text{d\_type}_k = \texttt{SOL}$ for $x_k \in B_{\delta_1}(x^*)$ regardless of whether the trial step or the fallback step is taken.
    By \corollaryref{cor:perturbation-lemma}, we have
     $\nabla^2\varphi(x) \succeq \frac{\alpha}{2} \Id$ 
    for $x \in B_{\delta_0}(x^*)$.
    From \lemmaref{lem:capped-cg}, 
    when the fallback step is taken, then $\text{d\_type}_k = \texttt{SOL}$.
    On the other hand, if the trial step is taken, 
    we will also invoke \lemmaref{lem:capped-cg} as follows.
    Let $a = (1 + 2\theta)^{-1} \in (0, 1]$, we have
    \begin{enumerate}
        \item When $\omega^{\supsucc}_k = g_k^{\frac{1}{2}} \min(1, g_k^\theta g_{k-1}^{-\theta})$, 
        we know  $(\omega^{\supsucc}_k)^a \geq g_k^{\frac{1}{2}} U_\varphi^{-a\theta} = \omega_k^{\supfallback} U_\varphi^{-a\theta}$;
        \item When $\omega^{\supsucc}_k = \epsilon_k^{\frac{1}{2} + \theta} \epsilon_{k-1}^{-\theta}$, 
        it still holds that $(\omega^{\supsucc}_k)^a\geq \omega_k^{\supfallback} U_\varphi^{-a\theta}$.
    \end{enumerate}
    Therefore, let $\bar\rho = \tau \sqrt{M_k}\omega_k^{\supfallback}$ and $\rho = \sqrt{M_k} \omega_k^{\supsucc}$, 
    and note that from \lemmaref{lem:lipschitz-constant-estimation} we have $M_k \leq  U_M$, 
    then let $b = \tau U_\varphi^{a\theta} U_M^{\frac{1-a}{2}}$, we know
    \begin{align*}
        \rho^a &= M_k^{\frac{a}{2}} (\omega_k^{\supsucc})^a
        \geq M_k^{\frac{a}{2}} \omega_k^{\supfallback} U_{\varphi}^{-a\theta}
        =  \tau^{-1} U_{\varphi}^{-a\theta} M_k^{\frac{a-1}{2}} \bar \rho
        \overset{(a \leq 1)}{\geq}
        \tau^{-1} U_{\varphi}^{-a\theta} U_M^{\frac{a-1}{2}} \bar \rho
        = b^{-1} \bar \rho.
    \end{align*}
    Since the map $U \mapsto C(\alpha, a, b, U)$ defined in \lemmaref{lem:capped-cg} is non-increasing, 
    we know 
    \begin{align*}
    \inf_{x\in B_{\delta_0}(x^*)} C(\alpha / 2, a, b, \|\nabla^2\varphi(x)\|)
    \geq C(\alpha / 2, a, b, \|\nabla^2\varphi(x^*)\| + L_H \delta_0 ) =: \tilde c > 0.
    \end{align*}
    From \corollaryref{cor:perturbation-lemma}, 
    we know for $x_k \in B_{\delta_1}(x^*)$, 
    \begin{align*}
        \rho =  
        \sqrt{M_k} \omega_k^{\supsucc}
        \leq U_M^{\frac{1}{2}} g_k^{\frac{1}{2}}
        \leq U_M^{\frac{1}{2}} (L_g \delta_1)^{\frac{1}{2}}
        \leq \min\left( \eta, \tilde c \right).
    \end{align*}
    Thus, \texttt{CappedCG} is invoked with $\xi = \rho$ and the premises of the fourth item in \lemmaref{lem:capped-cg} are satisfied, which leads to $\text{d\_type}_k = \texttt{SOL}$.

    \paragraph{Step 2}
    This is a standard step showing that the Newton direction will be taken (see, e.g., \citet{facchinei1995minimization,facchinei2003finite}).

    We show that $m_k = 0$ for $x_k \in B_{\delta_2}(x^*)$ regardless of whether the trial step or the fallback step is taken.
    Define $\omega_k = \omega_k^{\supsucc}$ if the $k$-th step is accepted and $\omega_k = \omega_k^{\supfallback}$ otherwise, and denote $d_k$ as the direction generated in \texttt{NewtonStep} with such $\omega_k$. 
    By the assumption and \lemmaref{lem:gradient-decay-of-newton-step},
    we have for $x_k \in B_{\delta_1}(x^*)$, it holds that $\omega_k \leq g_k^{\frac{1}{2}} \leq L_g^{\frac{1}{2}} r_k^{\frac{1}{2}}$, 
    and $\sup_{x \in B_{\delta_1}(x^*)} \| \nabla^2\varphi(x) \| \leq L_g$, and 
    \begin{equation}
        \label{eqn:proof/newton-step-decay}
        \| x_k + d_k - x^* \| 
        \overset{\eqref{eqn:distance-decay-of-newton-step}}{\leq}  
        \frac{2}{\alpha}\left( 
            L_H r_k^2 + 2M_k^{\frac{1}{2}} (1 + L_g) r_k\omega_k
         \right) \leq c_1 r_k^{\frac{3}{2}},
    \end{equation}
    where we have used \lemmaref{lem:lipschitz-constant-estimation} to obtain $M_k \leq U_M$.
    Using the mean-value theorem and noticing that $\nabla \varphi(x^*) = 0$, 
    there exist $\zeta, \xi \in (0, 1)$ and $H_\zeta = \nabla^2 \varphi(x^* + \zeta (x_k - x^*))$, 
    $H_\xi = \nabla^2 \varphi(x^* + \xi(x_k + d_k - x^*))$ such that for $x_k \in B_{\delta_1}(x^*)$, 
    \begin{align*}
        \varphi(x_k) - \varphi(x^*) &= \frac{1}{2}(x_k - x^*)^\top H_\zeta(x_k - x^*), \\
        \varphi(x_k + d_k) - \varphi(x^*) &= \frac{1}{2}(x_k + d_k - x^*)^\top H_\xi(x_k + d_k - x^*)
         \overset{\eqref{eqn:proof/newton-step-decay}}{\leq} 
         \frac{L_gc_1^2}{2} r_k^3.
    \end{align*}
    Combining them, we have for $x_k \in B_{\delta_1}(x^*)$, 
    \begin{align}
        \nonumber
        &\peq \varphi(x_k +d_k) - \varphi(x_k) - \frac{1}{2} \nabla \varphi(x_k)^\top d_k \\
        \nonumber
        &\leq 
         \frac{L_gc_1^2}{2} r_k^{3} 
        -\frac{1}{2}(x_k - x^*)^\top H_\zeta(x_k - x^*)
        -\frac{1}{2}\nabla \varphi(x_k)^\top d_k
        \\
        &=
         \frac{L_gc_1^2}{2} r_k^{3}
        -\frac{1}{2}(x_k + d_k - x^*)^\top H_\zeta(x_k - x^*)
        -\frac{1}{2}(\nabla \varphi(x_k) - H_\zeta(x_k - x^*))^\top d_k 
        .
        \label{eqn:nonasymp-inexact-newton-line-search-final}
    \end{align}
    Let $\bar x = x^* + \zeta(x_k - x^*)$ and note $\nabla \varphi(x^*) = 0$, then
    \begin{align*}
        &\peq \| \nabla \varphi(x_k) - \zeta^{-1} \nabla \varphi(\bar x) \|
        = \| (\nabla \varphi(x_k) - \nabla\varphi(x^*)) - \zeta^{-1} (\nabla \varphi(\bar x) - \nabla \varphi(x^*)) \| \\
        &= \left \| \int_0^1 \nabla^2\varphi(x^* + t(x_k - x^*)) (x_k - x^*) \dd t
        - \zeta^{-1} \int_0^1 \nabla^2\varphi(x^* + t(\bar x  - x^*)) (\bar x - x^*) \dd t \right \| \\
        &= \left \| \int_0^1 (\nabla^2\varphi(x^* + t(x_k - x^*)) - \nabla^2\varphi(x^* + t(\bar x - x^*))) (x_k - x^*) \dd t \right \| \\
        &\leq L_H \int_0^1 t \| x_k - \bar x \| r_k \dd t 
        = L_H \int_0^1 t (1 - \zeta) \| x_k - x^* \| r_k \dd t 
        \leq L_H r_k^2.
    \end{align*}
    Therefore, we have for $x_k \in B_{\delta_1}(x^*)$,
    \begin{align}
        \nonumber
        &\peq \left \|  
          \nabla \varphi(x_k) - H_\zeta(x_k - x^*)
        \right \| \\ 
        \nonumber
        &\leq  
        \left \|  
          \zeta^{-1}\nabla \varphi(\bar x) - H_\zeta(x_k - x^*)
        \right \| + \| \zeta^{-1} \nabla \varphi(\bar x) - \nabla \varphi(x_k) \| \\
        \nonumber
        & = 
        \zeta^{-1}\left \|  
          \nabla \varphi(\bar x) - \nabla\varphi(x^*) - H_\zeta(\bar x - x^*)
        \right \| + \| \zeta^{-1} \nabla \varphi(\bar x) - \nabla \varphi(x_k) \| \\
        \label{eqn:appendix/proof-newton-step-intermediate-1}
        &\leq \zeta^{-1} L_H \| \bar x - x^* \|^2
        + L_H r_k^2
        = (\zeta + 1)L_H r_k^2
        \leq 2L_H r_k^2
        \leq 2L_H \delta_1^{\frac{1}{2}} r_k^{\frac{3}{2}}.
    \end{align}
    We also note that by the definition $\delta_2^{\frac{1}{2}} \leq 1 / (2c_1)$.
    Hence, $1 - c_1 \delta_2^{\frac{1}{2}} \geq 1/2$ and for $x_k \in B_{\delta_2}(x^*)$, 
    \begin{align}
        \label{eqn:inexact-descent-direction-upper-bound}
        \| d_k \| &\leq \| x_k + d_k - x_* \| + \| x_k - x_* \| 
        \overset{\eqref{eqn:proof/newton-step-decay}}{\leq} 
        c_1 r_k^{\frac{3}{2}} +  r_k
        \leq (c_1\delta_2^{\frac{1}{2}} + 1) r_k 
        \leq 2 r_k
        , \\
        \label{eqn:inexact-descent-direction-lower-bound}
        \| d_k \| &\geq \| x_k - x_* \| - \| x_k + d_k - x_* \| 
        \overset{\eqref{eqn:proof/newton-step-decay}}{\geq} 
        r_k- c_1 r_k^{\frac{3}{2}}
        \geq (1 - c_1\delta_2^{\frac{1}{2}}) r_k
        \geq \frac{r_k}{2}
        .
    \end{align}
    Combining the above two inequalities, we find for $x_k \in B_{\delta_2}(x^*)$, 
    \begin{align}
        | (\nabla \varphi(x_k) - H_\zeta (x_k - x_*))^\top d_k |
        \overset{\eqref{eqn:appendix/proof-newton-step-intermediate-1}}&{\leq} 
        4L_H \delta_1^{\frac{1}{2}} r_k^{\frac{5}{2}},
        \\
        | (x_k + d_k - x_*)^\top H_\zeta(x_k - x_*) |
        \overset{\eqref{eqn:proof/newton-step-decay}}&{\leq}
        L_g c_1 r_k^{\frac{5}{2}}.
    \end{align}
    Since $\text{d\_type}_k = \texttt{SOL}$, then using \lemmaref{lem:capped-cg} and note that $\nabla^2\varphi(x_k) \succeq \frac{\alpha}{2}\Id$, we know
    \begin{align}
        \nonumber
        \nabla \varphi(x_k)^\top d_k
        \overset{\eqref{eqn:capped-cg-descent-direction}}&{=} 
        -d_k^\top (\nabla^2\varphi(x_k) + 2M_k^{\frac{1}{2}}\omega_k I) d_k
        \leq 
        -\frac{\alpha}{2} \|d_k\|^2  
        \overset{\eqref{eqn:inexact-descent-direction-lower-bound}}{\leq} 
        - \frac{\alpha}{8}r_k^2
        .
    \end{align}
    Substituting them back to \eqref{eqn:nonasymp-inexact-newton-line-search-final}, and note that $\mu \in (0, 1/2)$, we have for $x_k \in B_{\delta_2}(x^*)$, 
    \begin{align*}
        &\peq \varphi(x_k +d_k) - \varphi(x_k) - \mu \nabla \varphi(x_k)^\top d_k  \\
        &\leq 
        \left( \frac{1}{2} - \mu \right)
        \nabla \varphi(x_k)^\top d_k
        + \left (
            \varphi(x_k + d_k) - \varphi(x_k) - \frac{1}{2} \nabla\varphi(x_k)^\top d_k
            \right ) \\
        &\leq 
        -\left( \frac{1}{2} - \mu \right)
        \frac{\alpha}{8} r_k^2
        + \frac{1}{2}\left( 
            L_gc_1^2 \delta_1^{\frac{1}{2}}
            + L_g c_1
            + 4L_H \delta_1^{\frac{1}{2}}
        \right) r_k^{\frac{5}{2}}
        .
    \end{align*}
    We can see that the above term is negative as long as $r_k \leq \delta_2$, and therefore, the linesearch \eqref{eqn:smooth-line-search-sol} holds with $m_k = 0$.

    \paragraph{Step 3}
    We show that the trial step (i.e., the step with using $\omega_k^{\supsucc}$) is accepted.
    Since $\text{d\_type}_k = \texttt{SOL}$, then \texttt{NewtonStep} will not return a \texttt{FAIL} state, so it suffices to show $g_{k+\frac{1}{2}} = \| \nabla \varphi(x_k + d_k) \| \leq g_k$, 
    where $d_k$ is the direction generated by \texttt{NewtonStep} with $\omega = \omega_k^{\supsucc} \leq \sqrt{g_k}$.
    Then, by \lemmaref{lem:gradient-decay-of-newton-step} and \eqref{eqn:appendix/proof/basic-gradient-descent-of-newton} we have 
    $g_{x + \frac{1}{2}} \leq g_k$ for $x_k \in B_{\delta_3}(x^*)$.
\end{proof}

\subsection{Local rate boosting lemma}
\label{sec:appendix/local-rate-boosting}

In this section, we establish a generalized version of \lemmaref{lem:superlinear-rate-boosting} in \lemmaref{lem:appendix/superlinear-rate-boosting-generalized} and \corollaryref{cor:appendix/quadratic-rate-boosting}, 
which extends to the case of a $\nu$-H\"older continuous Hessian and reduces the Lipschitz Hessian in Assumption~\ref{assumption:liphess} when $\nu = 1$.
The results in \lemmaref{lem:appendix/superlinear-rate-boosting-generalized} primarily characterize the behavior for $\theta \in [0, \nu]$,
while the case of $\theta > \nu$ is analyzed separately in \corollaryref{cor:appendix/quadratic-rate-boosting}.
This division into two cases is mainly a technical necessity, 
as merging them could result in the preleading coefficient $c_k$ in \eqref{eqn:appendix/superlinear-rate-boosting-logc} becoming unbounded.


\begin{lemma}
    \label{lem:appendix/superlinear-rate-boosting-generalized}
    Let $\{ g_k \}_{k \geq 0} \subseteq (0, \infty)$, $c_0\geq 1$, $c \geq 1$, $1\ge \nu>0$, %
    $\nu_0 = \bar \nu := \frac{\nu}{1+\nu}$,
    and $\theta \geq 0$.
    If $\log g_1 \leq \log c_0 + (1 + \nu_0) \log g_0$ and 
    the following inequality holds for $k \geq 1$,
    \begin{equation}
    g_{k+1} \leq c g_k^{1 + \nu} + c g_k^{1 + \bar\nu} \frac{g_k^\theta}{g_{k-1}^\theta},
    \label{eqn:appendix/superlinear-boosting-inequality}
    \end{equation}
    and $g_0 \leq \min\big (1, (2c)^{-\frac{1}{\bar \nu}}, c_0^{-\frac{1}{\bar \nu}} \big )$, 
    then we have $g_{k + 1} \leq g_k$ and the following inequality holds for every $k\geq 0$:
    \begin{equation}
        \label{eqn:appendix/superlinear-rate-boosting}
    \log g_{k+1} \leq \log c_k +  (1 + \nu_k) \log g_k,
    \end{equation}
    where we define $\bar \theta = \min(\theta, \nu)$ and 
    $\nu_\infty = -\frac{1}{2}(1 - \bar\nu-\bar\theta) + \frac{1}{2}\sqrt{(1-\bar\nu-\bar\theta)^2+4\bar \nu} \in [\bar\nu,\nu]$ 
    is  the positive root of the equation $\bar\nu + \frac{\bar\theta\nu_\infty}{1 + \nu_\infty} = \nu_\infty$, and\footnote{We define $\nu_{-1} = 0$.}
    \begin{align}
        \label{eqn:appendix/superlinear-rate-boosting-logc}
        \log c_k &:= \log(2c) + \frac{\bar\theta}{1 + \nu_{k-1}} \log c_{k-1}
        \leq \left(1+\frac{1}{\bar\nu}\right) \log (2c)  + \log c_0
        , \\
        \label{eqn:appendix/superlinear-rate-boosting-nu}
        \nu_k &:= \min\left( \nu, \bar\nu + \frac{\bar \theta \nu_{k-1}}{1 + \nu_{k-1}} \right) 
        \geq \nu_\infty - \frac{\bar\theta^k(\nu_\infty - \bar\nu)}{(1+\bar\nu)^{2k}} 
        \geq \nu_\infty - \frac{\bar\theta^k}{(1+\bar\nu)^{2k}} .
    \end{align}
    In particular, when $\theta\geq\nu$, we have $\nu_\infty = \nu$ and $v_k \geq \nu - \frac{\nu^k(\nu-\bar\nu)}{(1 + \bar\nu)^{2k}}$.
\end{lemma}
\begin{proof}
    We first show that $\nu_\infty \in [\bar \nu, \nu]$. 
    Define the map $T(\alpha) = \bar \nu + \frac{\bar \theta \alpha}{1 + \alpha} - \alpha$ for $\alpha \in [\bar\nu, \nu]$.
    By reformulating it as $T(\alpha) = \bar \nu + \bar \theta + 1 - \left( \frac{\bar \theta}{1 + \alpha} + (1 + \alpha) \right)$, 
    we see that $T$ is strictly decreasing whenever $1 + \alpha \geq \sqrt{\bar \theta}$,
    which holds since $1 + \alpha \geq 1 + \bar\nu > 1 \geq \nu \geq \bar \theta$.
    Then, there exists a unique $\nu_\infty \in [\bar\nu, \nu]$ such that $T(\nu_\infty) = 0$
    because $T(\bar\nu) = \frac{\bar\theta\bar\nu}{1 + \bar\nu} \geq 0$ and $T(\nu) = \frac{\nu(\bar\theta-\nu)}{1 + \nu} \leq 0$.

    Let $\cI \subseteq \N$ be the set such that $k \in \cI$ if and only if
    \begin{align*}
        g_{k+1} &\leq g_{k}, c_k \geq 1, \nu_k \leq  \nu_\infty,
        \text{ and }
        \eqref{eqn:appendix/superlinear-rate-boosting},
        \eqref{eqn:appendix/superlinear-rate-boosting-nu}
        \text{ hold, } \\
       & \text{ and }
        \log c_k 
        \leq \frac{1 - (1 + \bar\nu)^{-k}}{1 - (1 + \bar\nu)^{-1}} \log (2c) + \log c_0.
    \end{align*}

    First, we show that $0 \in \cI$.
    Since $\nu_0 = \bar \nu$ and $g_0^{\bar \nu} \leq c_0^{-1}$, 
    we have $g_1 \leq c_0g_0^{1 + \bar \nu} \leq g_0$.
    The other parts hold by assumption, and we have used $\nu_\infty \geq \bar \nu$ and the definition that $\nu_{-1} = 0$ in \eqref{eqn:appendix/superlinear-rate-boosting-nu} for $k = 0$.

    Next, we prove $\cI = \N$ by induction.
    Suppose $0, \dots, j - 1 \in \cI$ for some $j \geq 1$, we will show that $j\in \cI$.
    Since $j-1\in\cI$, from \eqref{eqn:appendix/superlinear-rate-boosting} we have 
    $g_j \leq c_{j-1} g_{j-1}^{1 + \nu_{j-1}}$, 
    and equivalently,
    $g_{j-1}^{-1} \leq \left (c_{j-1}^{-1}g_{j}\right )^{-\frac{1}{1 + \nu_{j-1}}}$.
    Note that $c_{j - 1} \geq 1$ 
    and $g_j \leq g_{j-1}$,
    and 
    $\frac{g_j^\theta}{g_{j-1}^\theta} \leq \frac{g_j^{\bar\theta}}{g_{j-1}^{\bar\theta}}$ for $\theta \geq \bar \theta$,
    we have
    \begin{align*}
        g_{j+1}
        \overset{\eqref{eqn:appendix/superlinear-boosting-inequality}}&{\leq}
        c g_j^{1 + \nu} + c g_j^{1 + \bar\nu} \frac{g_j^{\bar\theta}}{g_{j-1}^{\bar\theta}}
        \leq 
        c g_j^{1 + \nu} + c c_{j-1}^{\frac{\bar\theta}{1 + \nu_{j-1}}} g_j^{1 + \bar\nu + \frac{\bar\theta\nu_{j-1}}{1 + \nu_{j-1}}} \\
        \overset{(c, c_{j-1} \geq 1)}&{\leq}
        2 c c_{j-1}^{\frac{\bar\theta}{1 + \nu_{j-1}}} 
        \max \left ( g_j^{1 + \nu}, g_j^{1 + \bar\nu + \frac{\bar\theta\nu_{j-1}}{1 + \nu_{j-1}}} \right )
        .
    \end{align*}
    Therefore, we find that
    \begin{align}
        \log g_{j+1}
        \leq 
        \underbrace{\log (2c) + \frac{\bar\theta}{1 + \nu_{j-1}} \log c_{j-1} }_{\log c_{j}}
        + 
        \underbrace{\min\left(1 + \nu,  1 + \bar\nu + \frac{\bar\theta\nu_{j-1}}{1 + \nu_{j-1}} \right)}_{1 + \nu_{j}} \log g_j.
        \label{eqn:appendix/proof-superlinear-boosting-recursive}
    \end{align}
    Thus, \eqref{eqn:appendix/superlinear-rate-boosting} holds for $k=j$, and $\log c_j \geq \log(2c) \geq \log 2 \geq 0$, i.e., $c_j \geq 1$.

    Since $[j-1]\subseteq\cI$, we know $\{g_i\}_{0 \leq i \leq j}$ is non-increasing, $g_{j}^{\bar \nu} \leq g_0^{\bar\nu}\leq (2c)^{-1}$, and $g_j\leq g_{j-1}$. 
    Note that $\bar \nu \leq \nu$ and $g_j \leq g_0 \leq 1$, 
    then 
    $g_{j+1} 
    \leq c g_j^{1 + \nu} + cg_j^{1+\bar\nu} (g_j g_{j-1}^{-1})^{\theta}
    \leq 2cg_j^{1+\bar\nu} \leq g_j$.

    By \eqref{eqn:appendix/superlinear-rate-boosting-nu}, $\nu_{j-1} \geq \min(\bar\nu, \nu) = \bar\nu$ and we have 
    \begin{align*}
        \log c_j 
        &\leq \log(2c) + \frac{\bar\theta}{1 + \bar\nu} \log c_{j-1} \\
        \overset{(\bar\theta \leq 1)}&{\leq} \log(2c) + \frac{1}{1 + \bar\nu} \left ( \frac{1 - (1 + \bar\nu)^{-(j-1)}}{1 - (1 + \bar\nu)^{-1}} \log (2c) + \log c_0 \right )
        \\
        &\leq \frac{1 - (1 + \bar\nu)^{-j}}{1 - (1 + \bar\nu)^{-1}} \log (2c) + \log c_0.
    \end{align*}

    Finally, we show $\nu_j \leq \nu_\infty$ and \eqref{eqn:appendix/superlinear-rate-boosting-nu} holds for $k=j$. 
    Define the map $F(\alpha) = \bar\nu + \frac{\bar\theta\alpha}{1 + \alpha}$.
    We know $F(\alpha)$ is non-decreasing for $\alpha > 0$,
    and $F(\nu_\infty) = \nu_\infty$ by its definition.
    Since  $\nu_{j-1} \leq \nu_\infty$ 
    and $F(\nu_{j-1}) \leq F(\nu_\infty) = \nu_\infty \leq \nu$, 
    then
    $\nu_j = \min(\nu, F(\nu_{j-1})) = F(\nu_{j-1})
   \leq \nu_\infty$.
    Moreover,  we have
    \begin{align*}
        0 \leq \nu_\infty - \nu_j
        &= F(\nu_\infty) - F(\nu_{j-1})
        = \frac{\bar\theta(\nu_\infty-\nu_{j-1})}{(1+\nu_\infty)(1 + \nu_{j-1})} \\
        &\leq \frac{\bar\theta(\nu_\infty-\nu_{j-1})}{(1+\bar \nu)^2}
        \leq \frac{\bar\theta^j(\nu_\infty-\bar\nu)}{(1+\bar \nu)^{2j}}
        ,
    \end{align*}
    where the last inequality follows from the induction assumption.

    
    Thus, we have $j \in \cI$ and by induction $\cI = \N$.
\end{proof}


\begin{corollary}
    \label{cor:appendix/quadratic-rate-boosting}
    Under the assumptions of \lemmaref{lem:appendix/superlinear-rate-boosting-generalized},
    if $\theta > \nu$ and $k \geq k_0 := 
    \frac{\log\frac{\theta-\nu\bar\nu}{\theta-\nu}-\log\nu}{2\log(1+\bar\nu)-\log\nu} + 1$, 
    then $g_k$ converges superlinearly with order $1 + \nu$:
    \begin{align}
        \log g_{k}
        \leq
        \left( 1 + \theta + \frac{1}{\bar\nu} \right)\log (2c) + \theta  \log c_0
        + (1 + \nu) \log g_{k-1}.
    \end{align}
\end{corollary}
\begin{proof}
    Since the assumptions are the same as those in \lemmaref{lem:appendix/superlinear-rate-boosting-generalized}, the results therein are all valid.
    Furthermore, we note that in the proof of \lemmaref{lem:appendix/superlinear-rate-boosting-generalized}, 
    the following stronger variant of \eqref{eqn:appendix/proof-superlinear-boosting-recursive} can be obtained from \eqref{eqn:appendix/superlinear-boosting-inequality}:
    \begin{align}
        \log g_{j+1}
        \leq 
        \underbrace{\log (2c) + \frac{\theta}{1 + \nu_{j-1}} \log c_{j-1}}_{\hat c_j}
        + 
        \underbrace{\min\left(1 + \nu,  1 + \bar\nu + \frac{\theta\nu_{j-1}}{1 + \nu_{j-1}} \right)}_{1 + \hat \nu_j} \log g_j.
        \label{eqn:appendix/proof-superlinear-boosting-recursive-strong}
    \end{align}
    Let $\alpha = \left (\frac{\theta}{\nu - \bar \nu} - 1\right )^{-1} = \left (\frac{\theta}{\nu\bar\nu} - 1\right )^{-1}$.
    Since $\theta > \nu$, 
    then $\alpha > 0$ and $\frac{1}{\alpha} = \frac{\theta}{\nu\bar\nu} - 1 > \frac{1}{\bar\nu} - 1 = \frac{1}{\nu}$, i.e., $\alpha \in (0, \nu)$.
    When $\nu_{k-1} \geq \alpha$, we have
    \begin{align*}
       \hat \nu_{k} &=
       \min\left( \nu, \bar\nu + \frac{\theta\nu_{k-1}}{1+\nu_{k-1}} \right)
        =
        \min\left( \nu, \bar\nu + \frac{\theta}{\nu_{k-1}^{-1}+1} \right) \\
        &\geq
        \min\left( \nu, \bar\nu + \frac{\theta}{\alpha^{-1}+1} \right)
        =\nu.
    \end{align*}
    From \lemmaref{lem:appendix/superlinear-rate-boosting-generalized},
    we know $\nu_\infty = \nu$, and when $k - 1 \geq k_0 - 1 \geq \log_{\frac{\nu}{(1 + \bar\nu)^2}}(\nu - \alpha) = \frac{-\log(\nu-\alpha)}{2\log(1+\bar\nu)-\log\nu}$, the following inequality holds since $\nu \in (0, 1]$ and $1 + \bar\nu > 1$.
    \begin{align*}
       \nu_{k-1} \overset{\eqref{eqn:appendix/superlinear-rate-boosting-nu}}{\geq}
       \nu - \frac{\nu^{k-1}(\nu-\bar\nu)}{(1+\bar\nu)^{2(k-1)}}
       \geq \nu - \frac{\nu^{k-1}}{(1+\bar\nu)^{2(k-1)}}
       \geq \alpha.
    \end{align*}
    Thus, for any $k \geq k_0$, we have $\hat \nu_j = \nu$, and 
    \begin{align*}
        \log g_{k}
        \overset{\eqref{eqn:appendix/proof-superlinear-boosting-recursive-strong}}&{\leq}
        \log (2c) + \theta \log c_{k-1}
        + (1 + \nu) \log g_{k-1} \\
        \overset{\eqref{eqn:appendix/superlinear-rate-boosting-logc}}&{\leq}
        \left( 1 + \theta + \frac{1}{\bar\nu} \right)\log (2c) + \theta  \log c_0
        + (1 + \nu) \log g_{k-1}.
    \end{align*}
    Finally, the proof is completed by noticing that 
    $\nu - \alpha = \nu - \frac{\nu\bar\nu}{\theta-\nu\bar\nu} = \frac{\nu(\theta-\nu)}{\theta-\nu\bar\nu}$.
\end{proof}


