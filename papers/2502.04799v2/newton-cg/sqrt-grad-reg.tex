

\section{Overview of the techniques} \label{sec:main/techniques-overview}


As mentioned in \Cref{sec:main/background}, the key to establishing a fast global rate is to show that the loss decreases by at least $L_H^{-\frac{1}{2}}\epsilon^{\frac{3}{2}}$ (i.e., \emph{sufficient descent}) for as many iterations as possible.
We summarize necessary properties of \Cref{alg:adap-newton-cg} in \lemmaref{lem:lipschitz-constant-estimation}, 
and will subsequently focus on how to leverage them to establish a global rate.


\begin{lemma}[Summarized descent lemma, see \Cref{sec:appendix/summarized-descent}]
    \label{lem:lipschitz-constant-estimation}
    Let $\{ x_k$, $M_k$, $\text{d\_type}_k$, $m_k \}_{k \ge 0}$ be the sequence generated by \Cref{alg:adap-newton-cg}, 
    and denote $\omega_k := \omega_k^{\supsucc}$ if the trial step is accepted and $\omega_k := \omega_k^{\supfallback}$ otherwise.
    Define the index sets $\cJ^{i} = \{ k : M_{k+1} = \gamma^{i} M_k \}$ for $i = -1, 0, 1$, and the constants
    $\tilde C_4 = \max\big ( 1, \tau_-^{-1}(9\beta)^{-\frac{1}{2}}, \tau_-^{-1}(3\beta(1 - 2\mu))^{-1}\big )$ and $\tilde C_5 = \min(2, 3 - 6\mu)^{-1}$,
    then 
    \begin{enumerate}
        \item If $k \in \cJ^1$, then $M_k \leq \tilde C_5 L_H$; %
        \item 
        For the regularizers in \theoremref{thm:newton-local-rate-boosted},
        if $M_k > \tilde C_4 L_H$ and $\tau_- \leq \min\big ( \delta_k^\alpha, \delta_{k+1}^\alpha \big )$, 
        then $k \in \cJ^{-1}$, where $\alpha = \max(2, 3\theta)$.
    \end{enumerate}
    Moreover, 
    we have
    $\bigcup_{i = -1, 0, 1} ( \cJ^{i} \cap I_{0,k}) = I_{0,k}$, and
    \begin{align}
        \label{eqn:cardinarlity-of-M-set-a}
        |\cJ^{1} \cap I_{0,k}| & \leq
        |\cJ^{-1} \cap I_{0,k}| + [\log_\gamma (\gamma \tilde C_5 M_0^{-1} L_H)]_+, \\
        \label{eqn:cardinarlity-of-M-set}
        k = | I_{0,k} | &\leq 2| \cJ^{-1} \cap I_{0,k} | + |\cJ^0 \cap I_{0,k}| + [\log_\gamma (\gamma \tilde C_5 M_0^{-1} L_H) ]_+,
    \end{align}
    and the following descent inequality holds:
    \begin{equation}
        \label{eqn:summarized-descent-inequality}
        \varphi(x_{k+1}) - \varphi(x_k) \leq 
         \begin{cases}
             0, & \text{ if } k \in \cJ^1, \\
             -\tilde C_1 M_k^{-\frac{1}{2}} D_k, & \text{ if } k \in \cJ^0 \cup \cJ^{-1},
        \end{cases}
    \end{equation}
    where
    $\tilde C_1 = \min\left( 9\beta^2(1 - 2\mu)^2\mu, 36\beta\mu(1-\mu)^2, 4\mu /33 \right)$, and
    \begin{equation}
        \label{eqn:summarized-descent-amount}
        D_k = \begin{cases}
            (\omega_k^{\supfallback})^3,
            & \text{ if } k \in \cJ^{-1}, \\
            \min\Big(  
                (\omega_k^{\supfallback})^3,
                \omega_k^3, 
                g_{k+1}^2 \omega_k^{-1}  
            \Big), & \text{ if } \text{d\_type}_k = \texttt{SOL} \text{ and } m_k = 0 \text{ and } k \notin \cJ^{-1}, 
            \\
            \min\Big( 
                (\omega_k^{\supfallback})^3,
                \omega_k^3
            \Big), & \text{ otherwise}.
        \end{cases}
    \end{equation}
\end{lemma}

Before proceeding, we discuss the dependence on $L_H$ in \eqref{eqn:summarized-descent-inequality}.
Since $M_k$ is increased (i.e., $k \in \cJ^{1}$) only if $M_k \leq O(L_H)$, 
then if there exists $k_{\mathrm{init}}$ such that $M_{k_{\mathrm{init}}} \le O(L_H)$, we know
$M_k \le O(L_H)$ for $k \geq k_{\mathrm{init}}$.
Furthermore, for $k \geq k_{\mathrm{init}}$, when $M_k$ remains unchanged or decreases (i.e., $k \in \cJ^0 \cup \cJ^{-1}$), 
the function descent satisfies $\varphi(x_{k+1}) - \varphi(x_k) \lesssim -L_H^{-\frac{1}{2}} D_k$,
which ensures the dependence of the sufficient descent on $L_H$. 
The only issue arises when $M_k$ needs to be increased. 
However, as shown in \eqref{eqn:cardinarlity-of-M-set-a},
the occurrence of such cases can be effectively controlled.

\subsection{The global iteration complexity}
\label{sec:main/sqrt-global-rate}

Since under the choices of regularizers, we have either $\omega_k^{\supfallback} = \sqrt{g_k}$ or $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$, then ensuring sufficient descent reduces to counting the occurrences of the event $D_k \geq (\omega_k^{\supfallback})^3$. 
We outline the key steps for it in this section and defer the proofs and intermediate lemmas to \Cref{sec:appendix/global-rate-under-local-boosting,sec:appendix/global-rate-technical-lemmas}.

Throughout this section, we partition %
$\N$ 
into a disjoint union of intervals $\N = \bigcup_{j\geq 1} I_{\ell_j, \ell_{j+1}}$ such that 
 $0 = \ell_1$ and $\ell_j < \ell_{j+1}$ for $j \geq 1$, where $I_{i,j}=\{i, .., j-1\}$ is defined in the notation section. %
These intervals are constructed such that the following conditions hold for every $j \geq 1$:
\begin{align}
    g_{\ell_j} \geq g_{\ell_j + 1} \geq \dots \geq g_{\ell_{j+1} - 1}
    \text{ and }
    g_{\ell_{j+1} - 1} < g_{\ell_{j+1}}.
    \label{eqn:proof/newton-partition}
\end{align}
In other words, the sequence $\{x_k\}_{k \ge0}$ is divided into subsequences where the gradient norms are non-increasing.
The following lemma shows that sufficient descent occurs during the transition between adjacent subsequences, 
provided that $\ell_j - 1\notin \cJ^1$.
The fallback step is primarily designed to ensure this lemma holds. 
Without the fallback step, a sudden gradient decrease (i.e., a small $\delta_k$) could result in a small regularizer, causing the sufficient descent guaranteed by this lemma to vanish.

\begin{lemma}[Transition between adjacent subsequences, see \lemmaref{lem:proof/transition-between-subsequences-give-valid-regularizer}]
    \label{lem:main/transition-between-subsequences-give-valid-regularizer}
    Under the regularizers in \theoremref{thm:newton-local-rate-boosted} with $\theta \geq 0$, 
    we have $\omega_{\ell_{j}-1} = \omega_{\ell_{j}-1}^{\supfallback}$ for each $j > 1$, and 
    \begin{equation}
        \label{eqn:main/inexact-mixed-newton-boundary-bound}
        \varphi(x_{\ell_{j}})
        - \varphi(x_{\ell_{j}-1}) 
        \leq 
        -\tilde C_1 M_{\ell_j-1}^{-\frac{1}{2}} \bone_{\{\ell_{j}-1 \notin \cJ^{1}\}} (\omega_{\ell_{j}-1}^{\supfallback})^3.
    \end{equation}
    Moreover, if $M_{\ell_j-1} > \tilde C_4 L_H$, then $\ell_j -1 \in\cJ^{-1}$.
\end{lemma}

The following lemma characterizes the overall decrease of the function within a subsequence.
It roughly states that there are at most $O\big (\log\log \frac{g_{\ell_j}}{g_k}\big )$ iterations with insufficient descent in the subsequence $I_{\ell_j,\ell_{j+1}}$,
since otherwise the gradient decreases superlinearly below $g_k$.

\begin{lemma}[Iteration within a subsequence, see \lemmaref{lem:proof/iteration-in-a-subsequence}]
    \label{lem:main/iteration-in-a-subsequence}
    Under the regularizers in \theoremref{thm:newton-local-rate-boosted} with $\theta \geq 0$, 
    then for $j \geq 1$ and $\ell_j  < k < \ell_{j+1}$, we have
    \begin{align}
        \varphi(x_{k})
        - \varphi(x_{\ell_{j}})
        \leq
         - C_{\ell_j,k}
        \left ( 
            |I_{\ell_j,k} \cap \cJ^{-1} |
            + 
            \max\left ( 0, | I_{\ell_j,k} \cap \cJ^0 | - T_{\ell_j,k} - 5 \right ) 
        \right ) (\omega_k^{\supfallback})^3
         ,
         \label{eqn:main/inexact-mixed-newton-inner-bound-theta0}
    \end{align}
    where $C_{i,j} = \tilde C_1 %
    \min_{i \leq l < j} M_l^{-\frac{1}{2}}$ and $T_{i,j}=2\log\log\big (3 (\omega_i^{\supfallback})^2 (\omega_j^{\supfallback})^{-2}\big)$.
\end{lemma}
\begin{proof}[Sketch of the idea]
To demonstrate the key ideas, we use the square root gradient regularizer  $\omega_i^{\supsucc} = \omega_i^{\supfallback} = \sqrt{g_i}$ 
and assume the Lipschitz constant estimation is precise (i.e., $\N = \cJ^{0}$).
Under this choice, we observe that $D_i \geq g^2_{i+1} g_i^{-\frac{1}{2}}$ for iterations within a subsequence, i.e., $i \in I_{\ell_j,\ell_{j+1}}$.
We can divide $I_{\ell_j,k}$ into subsets 
$I_{\ell_j,k}^{(l)} = \{ i \in I_{\ell_j,k} :  \exp(4^l)g_k \leq g_i < \exp(4^{l+1})g_k \}$ for $l \geq 0$ and $I_{\ell_j,k}^{(-1)} = \{ i \in I_{\ell_j,k} : g_k \leq g_i < \mathrm{e} g_k \}$. 
Then, we find that $D_i \geq \mathrm{e}^{-\frac{1}{2}}g_k^{\frac{3}{2}}$ if $i$ and $i + 1$ belong to the same subinterval, and the number of non-empty subintervals is $O(T_{\ell_j,k})$ (see \lemmaref{lem:accumulated-descent-lower-bound} for details).
The general case follows a similar approach but involves additional technical complexities, which are detailed in \Cref{sec:appendix/global-rate-under-local-boosting}.
\end{proof}

Combining \lemmaref{lem:main/transition-between-subsequences-give-valid-regularizer,lem:main/iteration-in-a-subsequence}, we have the following proposition about the accumulated function descent, 
and find that there are $\Sigma_k$ iterations with sufficient descent.

\begin{proposition}[Accumulated descent, see \propositionref{prop:proof/accumulated-descent}]
    \label{prop:main/accumulated-descent}
    Under the choices of \theoremref{thm:newton-local-rate-boosted} with $\theta \geq 0$, 
    for each $k \geq 0$, we have
    \begin{align}
        \varphi(x_{k})
        - \varphi(x_0)
        \leq
         - C_{0,k}
        \big (\underbrace{
            |I_{0,k} \cap \cJ^{-1}|
            + \max\big( |S_k \cap \cJ^0|, |I_{0,k} \cap \cJ^0| - V_k - 5J_k \big)
            }_{\Sigma_k} \big )
         \epsilon_k^{\frac{3}{2}}
         ,
        \label{eqn:main/newton-global-final-inequality}
    \end{align}
    where $V_k = \sum_{j=1}^{J_k-1} T_{\ell_j,\ell_{j+1}} + T_{\ell_{J_k},k}$,
    and $S_k = \bigcup_{j=1}^{J_k-1}\{\ell_{j+1}-1\}$,
    and $J_k = \max\{ j : \ell_j \leq k \}$.
\end{proposition}

The difference of the logarithmic factor in the iteration complexity of \theoremref{thm:newton-local-rate-boosted} arises from the following lemma, which provides an upper bound for $V_k$.
This lemma shows that the choice $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$ leads to a better control over $V_k$ due to the monotonicity of $\epsilon_k$, 
resulting in improved lower bound for $\Sigma_k$, as indicated by \lemmaref{lem:basic-counting-lemma}. %
\begin{lemma}[See \Cref{sec:appendix/proof-lower-bound-of-Vk}]
    \label{lem:main/lower-bound-of-Vk}
    Let $V_k, J_k$ be defined in \propositionref{prop:main/accumulated-descent}, then we have
    (1). If $\omega_k^{\supfallback} = \sqrt{g_k}$, then $V_k \leq J_k \log\log \frac{U_\varphi}{\epsilon_k}$;
    (2). If $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$, then $V_k \leq \log \frac{\epsilon_0}{\epsilon_k} + J_k$.
\end{lemma}


Finally, we need to determine the aforementioned hitting time $k_{\mathrm{init}}$ such that $M_{k_\mathrm{init}} \leq O(L_H)$,
and apply \propositionref{prop:main/accumulated-descent} for $\{ x_k \}_{k \geq k_{\mathrm{init}}}$ to achieve the $L_H^{-\frac{1}{2}}$ dependence in the iteration complexity.
The idea behind the following lemma is that when $M_k > \Omega(L_H)$ but $k \in \cJ^0$, we will find that the gradient decreases linearly, implying that this event can occur at most $O\big ( \log \frac{U_\varphi}{\epsilon_{k_{\mathrm{init}}}} \big )$ times.

\begin{proposition}[Initial phase, see \propositionref{prop:proof/initial-phase-decreasing-Mk}]
    \label{prop:main/initial-phase-decreasing-Mk}
    Let $k_{\mathrm{init}} = \min\{ j : M_j \leq O(L_H) \}$ and assume $M_0 > \Omega(L_H)$, then 
    for the first choice in \theoremref{thm:newton-local-rate-boosted}, we have
        $k_{\mathrm{init}} 
        \leq 
        O\Big ( \log \frac{M_0}{L_H} \log \frac{U_\varphi}{\epsilon_{k_{\mathrm{init}}}}\Big)$;
    and for the second choice, we have
        $k_{\mathrm{init}} 
        \leq 
        O\Big ( \log \frac{M_0}{L_H} + \log \frac{U_\varphi}{\epsilon_{k_{\mathrm{init}}}}\Big)$.
\end{proposition}


