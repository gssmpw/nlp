













\section{Discussions} 
\label{sec:main/conclusion}
In this paper, we present the adaptive regularized Newton-CG method  and show that two classes of regularizers achieve optimal global convergence order and quadratic local convergence.
Our techniques in \Cref{sec:main/techniques-overview} can be extended to Riemannian optimization, as only \lemmaref{lem:lipschitz-constant-estimation} needs to be modified.
For the setting with H\"older continuous Hessians, a variant of this lemma can be derived following \citet{he2023newton-hessian}, and the subsequent proof may also be generalized (see \Cref{sec:appendix/local-rate-boosting} for local rates).   
However, this case presents additional challenges since the H\"older exponent is also unknown and requires estimation, which we are currently investigating.  

It would also be interesting to investigate whether these regularizers are suitable for the convex settings studied in \citet{doikov2021minimizing,doikov2024super} and whether they can be extended to inexact methods such as \citet{yao2023inexact} and stochastic optimization.
