
\section{Background and our results} \label{sec:main/background}

In this section, we first provide background on the capped conjugate gradients and regularized Newton methods, 
and then give our results in \Cref{sec:main/newton-cg-our-results}, along with a discussion of additional related works in \Cref{sec:newton-cg-additional-related-works}.

\paragraph{Capped conjugate gradients}
The capped CG proposed by \citet{royer2020newton} solves the equation $\bar H \tilde d = -g$ using the standard CG, where $\bar H = H + 2\rho \Id$. %
It also monitors whether the iterates generated by the algorithm are negative curvature directions, or the algorithm converges slower than expected.
If such an evidence is found, the algorithm will output a negative curvature direction.

Specifically, 
given $\xi \in [0, 1]$, the algorithm outputs $(\text{d\_type}, \tilde d)$, where $\text{d\_type} \in \{ \texttt{SOL}, \texttt{NC} \}$. 
When $\text{d\_type} = \texttt{NC}$, $\tilde d$ is a negative curvature direction such that $\tilde d^\top H \tilde d \leq -\rho \| \tilde d \|^2$;
and when $\text{d\_type} = \texttt{SOL}$, the equation is approximately solved such that 
$\| \bar H \tilde d + g \| \leq \xi \| g \|$,
$\tilde d^\top\bar H\tilde d \geq \rho \| \tilde d \|^2$,
and $\| \tilde d \| \leq 2 \rho^{-1} \| g \|$.
In both cases, the solution can be found within $\min(n, \tilde O(\rho^{-\frac{1}{2}}))$ Hessian-vector products.
We provide the algorithm and its properties in \Cref{sec:appendix/capped-cg}.

\paragraph{Complexity of regularized Newton methods}
Continuing from \Cref{sec:main/intro}, we further discuss the regularized Newton method.
The key to proving a global rate is the following descent inequality, or its variants~\citep{birgin2017use,royer2020newton,mishchenko2023regularized,doikov2024gradient,he2023newton,he2023newton-hessian,zhu2024hybrid,gratton2024yet}:
\begin{equation}
    \varphi(x_{k+1}) - \varphi(x_k) \leq -C\min\left( g_{k+1}^2 \rho_k^{-1}, \rho_k^3 \right), \text{ where } k\ge0.
\end{equation}
The dependence on the future gradient $g_{k+1}$ arises from the inability to establish a lower bound on $\| d_k \|$ using only the information available at the current iterate,
since once the iterations enter a superlinear convergence region, the descent becomes small.
If we were able to choose $\rho_k$ such that the descent were at least $\epsilon^{\frac{3}{2}}$, 
then by telescoping the sum we would obtain $\varphi(x_k) - \varphi(x_0) \leq -C k \epsilon^{\frac{3}{2}}$. The optimal global rate would follow from $\varphi(x_k) - \varphi(x_0) \geq -\Delta_\varphi$. %

In the thread of work starting from \cite{royer2020newton}, $\rho_k \propto \sqrt \epsilon$, and the desired descent is guaranteed as long as $g_{k+1} \geq \epsilon$; otherwise, $x_{k+1}$ is a desired solution. 
Another line of works related to \citet{mishchenko2023regularized,gratton2024yet} use $\rho_k \propto \sqrt{g_k}$.
With this choice, the $g_k^{\frac{3}{2}}$ descent is achieved when $g_{k+1} \geq g_k$.
However, when $g_{k+1} < g_k$, the descent becomes $g_{k+1}^2 g_{k}^{-\frac{1}{2}}$, but the control over $g_{k+1}$ is lost. 
To resolve this issue, the iterations are divided into two sets: a successful set $\cI_s = \{ k : g_{k+1} \geq g_k / 2 \}$ and a failure set $\cI_f = \N \setminus \cI_s$. 
It is shown that when $|\cI_f|$ is large the gradient will decrease below $\epsilon$ rapidly;
and otherwise, sufficient descent is still achieved.
The logarithmic factor in the complexity of \citet{gratton2024yet} can be understood as follows: a sufficient descent occurs at least once in every $O(\log \frac{1}{\epsilon})$ iterations.
Yet, as shown in \lemmaref{lem:main/iteration-in-a-subsequence}, 
it actually occurs in every $O(\log\log\frac{1}{\epsilon})$ iterations.
Furthermore, the logarithmic factor disappears in the convex case because the gradient will not experience abrupt growth~\citep{mishchenko2023regularized,doikov2024gradient}.


Finally, we note that the superlinear local rate may disappear for a fixed regularizer as it can be verified that this results in a linear rate when applied to $\varphi(x) = \|x\|^2$. %
When using $\rho_k \propto g_k^{\bar \nu}$ for $\bar \nu \in (0, 1]$, we have a superlinear rate with order $1 + \bar \nu$~\citep{yamashita2001rate, dan2002convergence, li2004regularized, fan2005quadratic, bergou2020convergence, marumo2023majorization}.%
\footnote{A sequence $\{ a_k \}_{k \geq 0}$ has a superlinear local rate of order $1 + \bar\nu$ if $a_{k+1} = O(a_k^{1 + \bar\nu})$ for sufficiently large $k$.} 
Moreover, by inspecting the choice $\bar \nu = \frac{1}{2}$ for the optimal global rate, there appears a global-local trade-off between the regularizers.
One possible solution to achieve a quadratic is to drop the regularizer when $\lambda_{\mathrm{min}}(\nabla^2\varphi(x_k)) \geq \sqrt{g_k}$ if the minimal eigenvalue computation is allowed, as in \citet{goldfeld1966maximization,jiang2023universal}.
We will explore how to bridge this gap without this in \Cref{sec:main/boosting-local-rates}.

\subsection{Our results} \label{sec:main/newton-cg-our-results}
We adopt the standard assumption from \citet{royer2020newton}, 
which also guarantees $\Delta_\varphi < \infty$ and $U_\varphi < \infty$.
While the Lipschitz continuity assumption can be relaxed to hold only on the level set $L_\varphi(x_0)$ using techniques in \citet{he2023newton},
we retain this assumption for simplicity, as it is required for the descent lemma (\lemmaref{lem:lipschitz-constant-estimation}) and is orthogonal to our analysis.
\begin{assumption}[Smoothness]
    \label{assumption:liphess}
    The level set $L_\varphi(x_0) := \{ x \in \R^n : \varphi(x) \leq \varphi(x_0) \}$ is compact, and $\nabla^2 \varphi$ is $L_H$-Lipschitz continuous on an open neighborhood of $L_\varphi(x_0)$ containing the trial points generated in \Cref{alg:adap-newton-cg},
    where $x_0$ is the initial point.
\end{assumption}

Under this assumption, we have the following inequalities (see \citet{nesterov2018lectures}):
\begin{align}
    & \quad\,\,\, \| \nabla \varphi(x + d) - \nabla \varphi(x) - \nabla^2 \varphi(x)d \| \leq \frac{L_H}{2} \|d\|^2
    ,  
    \label{eqn:hessian-lip-gradient-inequ}
    \\
   & \varphi(x + d) \leq 
    \varphi(x)
    + \nabla \varphi(x)^\top d
    + \frac{1}{2}d^\top\nabla^2 \varphi(x) d
    + \frac{L_H}{6} \| d \|^3.
    \label{eqn:hessian-lip-value-inequ}
\end{align}

Our method is presented in \Cref{alg:adap-newton-cg}. 
The subroutine \texttt{NewtonStep} closely follows the version of \citet{royer2020newton} and \citet{he2023newton},
utilizing the \texttt{CappedCG} subroutine defined in \Cref{sec:appendix/capped-cg} to find a descent direction.
The key modification in this subroutine is the linesearch rule for selecting the stepsize when the negative curvature direction is not detected.
The criterion \eqref{eqn:smooth-line-search-sol} aligns with the classical globalization approach of Newton methods~\citep{facchinei1995minimization}, 
and can be shown to generate a unit stepsize (i.e., $\alpha = 1$) when the iteration is sufficiently close to a solution with a positive definite Hessian, 
leading to superlinear convergence (see \lemmaref{lem:asymptotic-newton-step}).
Furthermore, we introduce an additional criterion \eqref{eqn:smooth-line-search-sol-smaller-stepsize} to 
ensure that the number of function evaluations remains uniformly bounded as the iteration progresses.

Another modification is the introduction of the fifth parameter $\bar\rho$ and the additional \texttt{TERM} state of $\text{d\_type}$ in \texttt{CappedCG}.
This state is triggered when the iteration number exceeds $\tilde \Omega( \bar\rho^{-\frac{1}{2}})$, and is designed to ensure non-degenerate global complexity in terms of Hessian-vector products. 

At the end of \texttt{NewtonStep}, an estimation of the Lipschitz constant is computed (i.e., $M_k$) and will be used in $\rho_k = (M_k \omega^{\supsucc}_k)^{\frac{1}{2}}$.
If the linesearch of \eqref{eqn:smooth-line-search-sol-smaller-stepsize} or \eqref{eqn:smooth-line-search-nc} exceeds the allowed number of steps (i.e., $m_{\mathrm{max}}$), 
it indicates $M_k$ is an underestimation of the Lipschitz constant $L_H$.
In such cases, 
the estimation is updated, and the current iteration is skipped.
Otherwise, the subroutine proceeds and 
the remaining updating rules of $M_k$ are based on whether the loss decays as expected.
After approximately $\tilde O(1)$ iterations, it produces a desirable estimation of $L_H$.

The main loop of \Cref{alg:adap-newton-cg} invokes \texttt{NewtonStep} with varying regularization coefficients, 
the selection of which is crucial for achieving the optimal rate.
We highlight the existence of a fallback step in the main loop, which ensures the validity \lemmaref{lem:main/transition-between-subsequences-give-valid-regularizer} and will be explained therein.

\theoremref{thm:newton-local-rate-boosted,thm:newton-local-rate-boosted-oracle-complexity} summarize our main results, 
and \tableref{tab:rate-comparision-for-rmn} compares them with other regularized Newton methods for nonconvex optimization. 
All parameters aside from the regularizers can be chosen arbitrarily, provided they satisfy the requirements in \Cref{alg:adap-newton-cg}.


\begin{theorem}[Iteration complexity, proof in \Cref{sec:appendix/global-rate-proof,sec:appendix/proof-boosted-local-rates-theorem}]
    \label{thm:newton-local-rate-boosted}
    Let 
    $\{ x_k \}_{k \ge 0}$ 
    be generated by \Cref{alg:adap-newton-cg}. 
    Under Assumption~\ref{assumption:liphess} and define $\epsilon_k = \min_{0 \leq i \leq k} g_i$ with $g_{-1} = \epsilon_{-1} = g_0$,
    the following two iteration bounds hold for achieving the $\epsilon$-stationary point.
    \begin{enumerate}
        \item
        If $\omega_k^{\supfallback} = \sqrt{g_k}$, $\omega_k^{\supsucc} = \omega_k^{\supfallback} \delta_k^\theta$ with $\theta \geq 0$, and $\delta_k = \min ( 1, g_kg_{k-1}^{-1} )$, 
        then 
        \[
        k\le O\left ( 
            \Delta_\varphi L_H^{\frac{1}{2}} \epsilon^{-\frac{3}{2}}  \log\log \frac{U_\varphi}{\epsilon} 
            + |\log L_H| \log \frac{U_\varphi}{\epsilon}
            \right ).
        \]
        \item
        If $\omega_k^{\supfallback} = \sqrt{\epsilon_k}$, 
        $\omega_k^{\supsucc} = \omega_k^{\supfallback} \delta_k^\theta$ with $\theta \geq 0$,
        and $\delta_k = \epsilon_k\epsilon_{k-1}^{-1}$, 
        then 
        \[
        k\le O\left( 
            \Delta_\varphi L_H^{\frac{1}{2}} \epsilon^{-\frac{3}{2}} +  |\log L_H|
            + \log \frac{U_\varphi}{\epsilon}
            \right).
            \]
    \end{enumerate}
    Furthermore, there exists a subsequence $\{x_{k_j}\}_{j \geq 0}$ such that $\lim_{j \to \infty} x_{k_j} = x^*$ with $\nabla \varphi(x^*) = 0$.
    If $\theta > 1$ and $\nabla^2\varphi(x^*) \succ 0$, then the whole sequence $\{ x_k \}$ converges to a local minimum $x^*$,
    and for sufficiently large $k$, quadratic local rate exists for both of these choices, i.e., $g_{k+1} \leq O(g_k^2)$.
\end{theorem}
\begin{theorem}[Oracle complexity, proof in \Cref{sec:appendix/oracle-complexity-proof}]
    \label{thm:newton-local-rate-boosted-oracle-complexity}
    Each iteration in the main loop of \Cref{alg:adap-newton-cg} 
    requires
    at most $2(m_{\mathrm{max}}+1)$ function evaluations; 
    and at most $2$ gradient evaluations;
    and either $1$ Hessian evaluation or
    at most $\min\big(n, \tilde O( (\omega_k^{\supfallback})^{-\frac{1}{2}} ) \big )$ Hessian-vector products.
\end{theorem}

When $\theta = 0$, the regularization coefficient $\rho_k$ becomes $\sqrt{M_kg_k}$ or $\sqrt{M_k\epsilon_k}$, leading to a local rate of $\frac{3}{2}$. 
This square root gradient regularizer is similar to those employed by \citet{gratton2024yet} and \citet{he2023newton}. 
However, when $\theta > 0$, the extra term $\delta_k^\theta$ in $\omega_k^{\supsucc}$ decreases rapidly to zero as the iteration begins to converge superlinearly, 
gradually improving the local rate to faster than $\frac{3}{2}$, and achieving a quadratic rate when $\theta > 1$.
Finally, we note that for $\theta \in (0, 1]$, the local rate can also be improved, though it may not reach $2$, as illustrated in \figureref{fig:local-rate-for-nu1} and \lemmaref{lem:superlinear-rate-boosting}.

The complexity of each operation in the algorithm is characterized in \theoremref{thm:newton-local-rate-boosted-oracle-complexity}.
Specifically, for the regularizers in \Cref{thm:newton-local-rate-boosted}, 
the complexity in terms of Hessian-vector products is $\tilde O\big( \epsilon^{-\frac{7}{4}} \big)$,
matching the results in \citet{carmon2017convex,royer2020newton}.
Moreover, the complexity in terms of the second-order oracle outputting $
\{ \varphi(x), \nabla \varphi(x), \nabla^2\varphi(x) \}$ is $O\big (\epsilon^{-\frac{3}{2}} \big) + \tilde O(1)$, attaining the lower bound of \citet{carmon2020lower} up to an additive $\tilde O(1)$ term coming from the lack of prior knowledge about $L_H$.
Notably, the $L^{\frac{1}{2}}_H$ scaling in the iteration complexity is also optimal~\citep{carmon2020lower}.


\input{newton-cg/table/rate-comparsion}
\input{newton-cg/algo/newton-cg-main}

\subsection{Additional related work} \label{sec:newton-cg-additional-related-works}

In addition to the previously discussed work, we will discuss other second-order algorithms with fast global rates, and the adaptivity and universality of algorithms.

\paragraph{Second-order methods with fast global rates}
The trust-region method is another important approach to globalizing the Newton method.
By introducing a ball constraint $\| d \| \leq r_k$ to \eqref{eqn:basic-newton-direction}, it provides finer control over the descent direction.
Several variants of this method have achieved optimal or near-optimal rates~\citep{curtis2017trust,curtis2021trust,curtis2023worst,jiang2023universal}.
For example, \citet{curtis2021trust,jiang2023universal} incorporated a Levenberg-Marquardt regularizer into the trust-region subproblem.
\citet{hamad2022consistently,hamad2024simple} introduced an elegant and powerful trust-region algorithm that does not modify the subproblem, achieving both an optimal global order and a quadratic local rate.
In contrast, our results show that the regularized Newton method can also achieve both, while using less memory than \citet{hamad2024simple}, as shown in \Cref{sec:main/numerical}.
Interestingly, the disjunction of fast gradient decay and sufficient loss decay, as discussed above in the context of regularized Newton methods, is also reflected in several of these works.




It is worth noting that, previous to~\citet{royer2020newton}, a linesearch method with negative detection was proposed by~\citet{royer2018complexity}.
For convex problems, damped Newton methods achieving fast rates have also been developed~\citep{hanzely2022damped,hanzely2024damped}, and the method of \citet{jiang2023universal} can also be applied.

\paragraph{Adaptive and universal algorithms}
Since the introduction of cubic regularization, \emph{adaptive} cubic regularization attaining the optimal rate without using the knowledge of problem parameters (i.e., the Lipschitz constant) were developed by \citet{cartis2011adaptive-1,cartis2011adaptive-2}, 
and \emph{universal} algorithms based on this regularization that are applicable to different problem classes (e.g., functions with H\"older continuous Hessians with unknown H\"older exponents) are studied by \citet{grapiglia2017regularized,doikov2021minimizing}.
Recently, several universal algorithms for regularized Newton methods have also been proposed, including those by \citet{he2023newton-hessian,doikov2024super}.
Additionally, some adaptive trust-region methods have also been introduced~\citep{jiang2023universal,hamad2024simple}.


