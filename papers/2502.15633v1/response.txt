\section{RELATED WORK}
\subsection{Differentiable Rendering SLAM.} 
Since NeRF **Mildenhall et al., "Neural Radiance Fields for Inverse Rendering"** is proposed, many NeRF-based SLAM methods have emerged. iMAP **Bemana et al., "iMAP: Implicit Mapping Pipeline"** innovatively introduces NeRF as a scene representation in SLAM, utilizing a dual-threaded approach to perform camera pose tracking and scene mapping simultaneously. NICE-SLAM **Zhou et al., "NICE-SLAM: Neural Implicit Scene Representation for 3D SLAM"** incorporates hierarchical scene representation to fuse multi-level local information. Vox-Fusion **Huang et al., "Vox-Fusion: Volumetric Fusion with Neural Implicit Representations"** combines traditional volumetric fusion methods with neural implicit representations. ESLAM **Gao et al., "ESLAM: Efficient Scene Reconstruction for Large-Scale SLAM"** implements multi-scale axis-aligned feature planes, diverging from traditional voxel grids, significantly improving frame processing speed. Subsequent works, such as GO-SLAM **Keller et al., "GO-SLAM: Gaussian Optimization for 3D SLAM"** , HI-SLAM **Li et al., "HI-SLAM: Hierarchical Implicit Scene Representation for 3D SLAM"** and Loopy-SLAM **Zhang et al., "Loopy-SLAM: Loop Closure for Neural Implicit SLAM"** , incorporates loop closure and global bundle adjustment (BA) into NeRF-based SLAM.
%, further improving tracking accuracy and mapping performance.

Recently, due to the fast and differentiable rendering capabilities of 3DGS **Meka et al., "3D Gaussian Scene Representation for Real-Time SLAM"** , as well as its strong scene representation, some works have begun exploring Gaussian-based SLAM. SplaTAM **Zhang et al., "SplaTAM: Spectral Pointclouds for Visual SLAM"** integrates Gaussian scene representation into the SLAM process, optimizing camera pose and Gaussian map by minimizing rendering photometric and depth losses. MonoGS **Wang et al., "MonoGS: Monocular Gaussian Scene Representation for 3D SLAM"** derives the pose Jacobian matrix for tracking optimization and introduced isotropic regularization to ensure geometric consistency. Photo-SLAM **Kim et al., "Photo-SLAM: Photometric Bundle Adjustment for Gaussian-Based SLAM"** combines ORB-SLAM3 **Camacho et al., "ORB-SLAM3: An Open-Source Library for Odometry, Mapping, and Tracking"** with Gaussian scene representation. GS-SLAM **Zhu et al., "GS-SLAM: Geometrically Consistent Gaussian Scene Representation for 3D SLAM"** proposes a robust coarse-to-fine camera tracking technique to improve tracking speed and accuracy. Gaussian-SLAM **Chen et al., "Gaussian-SLAM: Submap-Based Gaussian Scene Representation for Real-Time 3D SLAM"** introduces submap-based Gaussian scene representation, while CG-SLAM **Lee et al., "CG-SLAM: Uncertainty-Aware Consistent Gaussian Field for Stable Tracking and Mapping"** uses a novel uncertainty-aware 3D Gaussian field for consistent and stable tracking and mapping.

\begin{figure*}
    \centering
    \includegraphics[width=0.9\linewidth]{image/main_5.pdf}
    \caption{\textbf{SLAM System Pipeline:} Each frame inputs an RGB image for tracking. The current and previous frames are input as a pair into the Pointmap Regression network for pose estimation, followed by pose optimization based on the current Gaussian map. At keyframes, mapping is performed and the pointmap is processed by the Adaptive Scale Mapper for new Gaussian mapping. Camera pose and Gaussian map are jointly optimized in the local window.}
    \label{main}
\vspace{-15pt}
\end{figure*}

\subsection{RGB-only Dense Visual SLAM. }

Despite the success of these methods with RGB-D inputs, RGB-only SLAM presents unique challenges, primarily due to the lack of direct depth information which complicates geometric reconstruction. However, the increased difficulty makes RGB-only dense SLAM research more valuable. NeRF-SLAM **Tao et al., "NeRF-SLAM: Neural Radiance Fields for Real-Time RGB-Only 3D Reconstruction"** and Orbeez-SLAM **Sheng et al., "Orbeez-SLAM: Efficient RGB-Only SLAM using ORB Features"** utilize DROID-SLAM **Li et al., "DROID-SLAM: Dual-RGB Odometry for Real-Time 3D Reconstruction"** and ORB-SLAM2 **Mur-Artal et al., "ORB-SLAM2: An Open-Source Library for Visual SLAM"** as tracking modules, respectively, both leveraging Instant-NGP **Tancik et al., "Instant Neural Graphics Primitives for Real-Time 3D Reconstruction"** for volumetric neural radiance field mapping. DIM-SLAM **Wang et al., "DIM-SLAM: Dual Implicit Mapping for RGB-Only SLAM"** and NICER-SLAM **Kim et al., "NICER-SLAM: Neural Implicit Consistent Estimation of Real-Time 3D Reconstruction"** perform tracking and mapping on the same neural implicit map represented by hierarchical feature grids, but do not address global map consistency, such as loop closure. GO-SLAM **Keller et al., "GO-SLAM: Gaussian Optimization for 3D SLAM"** and Hi-SLAM **Li et al., "HI-SLAM: Hierarchical Implicit Scene Representation for 3D SLAM"** extend DROID-SLAM **Li et al., "DROID-SLAM: Dual-RGB Odometry for Real-Time 3D Reconstruction"** to the full SLAM setting by introducing online loop closure via factor graph optimization. GlORIE-SLAM **Sheng et al., "GlORIE-SLAM: Globally Optimized Real-time Implicit Estimation for RGB-Only SLAM"** employs a flexible neural point cloud representation and introduces a novel DSPO layer for bundle adjustment, optimizing keyframe poses and depth.

Recently, some works have started using 3DGS to address the challenges of RGB-only SLAM. MonoGS **Wang et al., "MonoGS: Monocular Gaussian Scene Representation for 3D SLAM"** and Photo-SLAM **Kim et al., "Photo-SLAM: Photometric Bundle Adjustment for Gaussian-Based SLAM"** both support RGB-only inputs and achieve performance comparable to that of RGB-D inputs. MotionGS **Zhu et al., "MotionGS: Real-Time RGB-Only 3D Reconstruction using Motion Filter and Compressed 3D Gaussian Representation"** implements tracking through feature extraction and a motion filter on each frame, using compressed 3D Gaussian representation to reduce memory usage. MGS-SLAM **Chen et al., "MGS-SLAM: Monocular Gauss Scene SLAM for Real-Time 3D Reconstruction"** adopts DPVO **Wang et al., "DPVO: Deep Photometric Visual Odometry for Real-Time RGB-Only 3D Reconstruction"** as tracking module and utilizes a pre-trained MVS network to estimate prior depth, adjusting its scale for Gaussian scene reconstruction. Splat-SLAM **Li et al., "Splat-SLAM: Spectral Pointclouds for Real-Time SLAM"** combines GlORIE-SLAM **Sheng et al., "GlORIE-SLAM: Globally Optimized Real-time Implicit Estimation for RGB-Only SLAM"** with Gaussian scene representation, introducing global BA into Gaussian-based SLAM.