\section{Related Work}
\subsection{MML under Imbalanced Scenario}
The goal of multimodal learning~\cite{MML:conf/ijcai/YangWZX019,MMDL:conf/icml/JiaYXCPPLSLD21,MMDL:conf/icml/NgiamKKNLN11,MMDL:conf/cvpr/SinghHGCGRK22,MML:conf/nips/HuangDXCZH21} is to fuse the multimodal information from diverse sensors. Compared to unimodal methods, MML can mine data information from different perspectives, thus the performance of multimodal learning should be better~\cite{MML:conf/cvpr/LvCHDL21,MML:conf/nips/SimonyanZ14,MML:conf/cvpr/HuLL16,MML:conf/cvpr/GaoOGT20}. However, due to heterogeneity of multimodal data, multimodal learning often encounters imbalance problems~\cite{OGR-GB:conf/cvpr/WangTF20,ModalCompetition:conf/icml/HuangLZYH22} in practice, leading to performance degeneration of MML. 

Early pioneering works~\cite{OGR-GB:conf/cvpr/WangTF20,OGM:conf/cvpr/PengWD0H22,PMR:conf/cvpr/Fan0WW023,MML:conf/cvpr/Ge0G00AILZ23} focus more on adaptively adjusting the learning procedure for different modalities. Representative approaches in this category employ different learning strategies, e.g., gradient modulation~\cite{OGM:conf/cvpr/PengWD0H22,AGM:conf/iccv/LiLHLLZ23} and learning rate adjustment~\cite{MSLR:conf/acl/YaoM22}, to rebalance the learning of weak and strong modalities. Other approaches including MLA~\cite{MLA:conf/cvpr/ZhangYBY24}, DI-MML~\cite{DI-MML:conf/mm/FanXWLG24}, ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} and MAIE~\cite{MAIE:journals/corr/abs-2407-04587} take a different path, focusing on enhancing the interaction between modalities to address the modality imbalance problem. For example, MLA~\cite{MLA:conf/cvpr/ZhangYBY24} designs an alternating algorithm to train different modalities iteratively. During the training phase, the interaction is enhanced by transferring the learning information between different modalities. ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} balances modality learning by leveraging gradient boosting to capture information from other modalities during interactive learning. 

The aforementioned methods focus on rebalancing the learning process for weak and strong modalities while failing to explicitly facilitate the classification ability of the weak modality. In this paper, we aim to address the modality imbalance issue from facilitating the classification ability of weak modality and rebalancing the classification ability of weak and strong modalities. 

%It is worth mentioning that ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} also employs the gradient boosting algorithm for MML. However, unlike our approach, ReconBoost uses gradient boosting to iteratively learn complementary information across modalities, rather than focusing on enhancing the classification performance of weaker modalities.

 
\subsection{Boosting Method}

Boosting algorithm~\cite{LightGBM:conf/nips/KeMFWCMYL17,Adaboost:conf/eurocolt/FreundS95,GradientBoosting:journal/AS/FriedmanJ,XGBoost:conf/kdd/ChenG16,Boost:conf/cvpr/LiuHMT14,Boost:conf/nips/ProkhorenkovaGV18,Boost:conf/icml/CortesMS14} is one of the most important algorithms in ensemble learning. The core idea of boosting is to integrate multiple learners to create a strong learner. Adaboost~\cite{Adaboost:conf/eurocolt/FreundS95}, one of the earliest boosting algorithms, adjusts the weights of incorrectly classified data points, giving more attention to the harder-to-classify examples in each iteration. Gradient boosting~\cite{GradientBoosting:journal/AS/FriedmanJ}, on the other hand, builds models in a stage-wise fashion, minimizing a loss function through gradient descent. 

The key advantage of boosting lies in its ability to improve model accuracy without requiring complex individual models. Therefore, boosting becomes the natural choice for improving the performance of weak classifiers.
\begin{figure*}
\centering
\includegraphics[scale=0.65]{figures/arch_draw.pdf}
\caption{The framework of our proposed method. We utilize the video and audio modalities as examples, with the numbers of video and audio classifiers denoted by $n^v$ and $n^a$, respectively.}
\label{fig:ours}
\end{figure*}