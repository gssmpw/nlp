\section{Related Work}
\subsection{MML under Imbalanced Scenario}
The goal of multimodal learning____ is to fuse the multimodal information from diverse sensors. Compared to unimodal methods, MML can mine data information from different perspectives, thus the performance of multimodal learning should be better____. However, due to heterogeneity of multimodal data, multimodal learning often encounters imbalance problems____ in practice, leading to performance degeneration of MML. 

Early pioneering works____ focus more on adaptively adjusting the learning procedure for different modalities. Representative approaches in this category employ different learning strategies, e.g., gradient modulation____ and learning rate adjustment____, to rebalance the learning of weak and strong modalities. Other approaches including MLA____, DI-MML____, ReconBoost____ and MAIE____ take a different path, focusing on enhancing the interaction between modalities to address the modality imbalance problem. For example, MLA____ designs an alternating algorithm to train different modalities iteratively. During the training phase, the interaction is enhanced by transferring the learning information between different modalities. ReconBoost____ balances modality learning by leveraging gradient boosting to capture information from other modalities during interactive learning. 

The aforementioned methods focus on rebalancing the learning process for weak and strong modalities while failing to explicitly facilitate the classification ability of the weak modality. In this paper, we aim to address the modality imbalance issue from facilitating the classification ability of weak modality and rebalancing the classification ability of weak and strong modalities. 

%It is worth mentioning that ReconBoost____ also employs the gradient boosting algorithm for MML. However, unlike our approach, ReconBoost uses gradient boosting to iteratively learn complementary information across modalities, rather than focusing on enhancing the classification performance of weaker modalities.

 
\subsection{Boosting Method}

Boosting algorithm____ is one of the most important algorithms in ensemble learning. The core idea of boosting is to integrate multiple learners to create a strong learner. Adaboost____, one of the earliest boosting algorithms, adjusts the weights of incorrectly classified data points, giving more attention to the harder-to-classify examples in each iteration. Gradient boosting____, on the other hand, builds models in a stage-wise fashion, minimizing a loss function through gradient descent. 

The key advantage of boosting lies in its ability to improve model accuracy without requiring complex individual models. Therefore, boosting becomes the natural choice for improving the performance of weak classifiers.
\begin{figure*}
\centering
\includegraphics[scale=0.65]{figures/arch_draw.pdf}
\caption{The framework of our proposed method. We utilize the video and audio modalities as examples, with the numbers of video and audio classifiers denoted by $n^v$ and $n^a$, respectively.}
\label{fig:ours}
\end{figure*}