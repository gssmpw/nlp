\documentclass[10pt,twocolumn,letterpaper]{article}
\usepackage{cvpr}      
\newcommand{\red}[1]{{\color{red}#1}}
\newcommand{\todo}[1]{{\color{red}#1}}
\newcommand{\TODO}[1]{\textbf{\color{red}[TODO: #1]}}

\definecolor{cvprblue}{rgb}{0.21,0.49,0.74}
\usepackage[pagebackref,breaklinks,colorlinks,allcolors=cvprblue]{hyperref}

\usepackage[dvipsnames]{xcolor}

\usepackage{dsfont}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{dsfont}
\usepackage{pifont}
\usepackage{amssymb}
\usepackage{tikz}
\usepackage{colortbl}

\newcommand{\PreserveBackslash}[1]{\let\temp=\\#1\let\\=\temp}
\newcolumntype{C}[1]{>{\PreserveBackslash\centering}p{#1}}
\newcolumntype{R}[1]{>{\PreserveBackslash\raggedleft}p{#1}}
\newcolumntype{L}[1]{>{\PreserveBackslash\raggedright}p{#1}}

\usepackage{algorithm,algorithmicx,algpseudocode}
\usepackage{rotating}

\algrenewcommand\algorithmiccomment[1]{{\color{blue}\hfill$\rhd$ #1}}

\newcommand{\first}[1]{\bf\cellcolor{orange!50}{#1}}
\newcommand{\second}[1]{\cellcolor{orange!35}{#1}}
\newcommand{\third}[1]{\cellcolor{orange!15}{#1}}

\newcommand{\reference}[1]{{\scriptsize\color{blue} [#1]}}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}


\newcommand{\tikzxmark}{%
% \color{red!90}
\tikz[scale=0.23] {
    \draw[line width=1,line cap=round] (0,0) to [bend left=6] (1,1);
    \draw[line width=1,line cap=round] (0.2,0.95) to [bend right=3] (0.8,0.05);
}}
\newcommand{\tikzcmark}{%
% \color{red!90}
\tikz[scale=0.23] {
    \draw[line width=1,line cap=round] (0.25,0) to [bend left=10] (1,1);
    \draw[line width=1,line cap=round] (0,0.35) to [bend right=1] (0.23,0);
}}


\def\CREMAD{{CREMAD}}
\def\Kinetics{{KSounds}}
\def\Twitter{{Twitter}}
\def\Sarcasm{{Sarcasm}}
\def\NVGesture{{NVGesture}}
\def\VGGSound{{VGGSound}}


\input{defs}

\def\paperID{1228} 
\def\confName{CVPR}
\def\confYear{2025}

% \title{Balanced Multimodal Learning by Facilitating Weak Modality Classification Ability}
\title{Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion}

\author{QingYuan Jiang\\
{\tt\small qyjiang24@gmail.com} \\
\and
Longfei Huang\\
{\tt\small hlf@njust.edu.cn} \\
\and
Yang Yang \thanks{Corresponding author}\\
{\tt\small yyang@njust.edu.cn} \\
\and
Nanjing University of Science and Technology
}

\begin{document}
\maketitle
\begin{abstract}
Although multimodal learning~(MML) has garnered remarkable progress, the existence of modality imbalance hinders multimodal learning from achieving its expected superiority over unimodal models in practice. To overcome this issue, mainstream multimodal learning methods have placed greater emphasis on balancing the learning process. However, these approaches do not explicitly enhance the classification ability of weaker modalities, leading to limited performance promotion. By designing a sustained boosting algorithm, we propose a novel multimodal learning approach to dynamically balance the classification ability of weak and strong modalities. Concretely, we first propose a sustained boosting algorithm in multimodal learning by simultaneously optimizing the classification and residual errors using a designed configurable classifier module. Then, we propose an adaptive classifier assignment strategy to dynamically facilitate the classification performance of weak modality. To this end, the classification ability of strong and weak modalities is expected to be balanced, thereby mitigating the imbalance issue. Empirical experiments on widely used datasets reveal the superiority of our method through comparison with various state-of-the-art~(SoTA) multimodal learning baselines. 
% The source code is available at \href{https://anonymous.4open.science/r/Our_CVPR25-2DFD}{repository}.
\end{abstract}


\section{Introduction}
% multimodal classification and imbalance problem
In recent years, multimodal learning~\cite{MMDL:conf/icml/NgiamKKNLN11,OGM:conf/cvpr/PengWD0H22,SMV:conf/cvpr/YakeRZD24,MLA:conf/cvpr/ZhangYBY24,ReconBoost:conf/icml/CongHua24} has received growing attention for its ability to effectively integrate heterogeneous information. As extra information from multimodal data can be utilized, MML is expected to achieve better performance compared with unimodal approaches. However, contrary to expectations, MML has been surprisingly shown to underperform compared to unimodal ones in certain scenarios~\cite{OGR-GB:conf/cvpr/WangTF20,OGM:conf/cvpr/PengWD0H22}.

\begin{figure}
\centering
\includegraphics[scale=0.9]{figures/intro.pdf}
\caption{Comparison with naive MML, gradient boosting based MML~(MML w/ GB), G-Blend~\cite{OGR-GB:conf/cvpr/WangTF20}, and Ours on \CREMAD~dataset. We find that enhancing the classification performance of the weak modality narrows the performance gap between the two modalities and improves overall performance.}
\label{fig:intro}
\end{figure}

% reason of multimodal imbalance
The root of this problem lies in the existence of the modality imbalance~\cite{OGR-GB:conf/cvpr/WangTF20}. Concretely, different modalities in a joint-training paradigm typically converge at different speeds~\cite{OGM:conf/cvpr/PengWD0H22,nvGesture:conf/icml/WuJCG22}. The faster-converging modality, i.e., strong modality~\cite{ARM:conf/ijcai/YangYZJ15}, tends to achieve higher performance, while the weak modality performs poorly. Subsequently, this disproportion in classification ability often leads to modality imbalance~\cite{OGR-GB:conf/cvpr/WangTF20}, ultimately resulting in lower performance.

% Therefore, the trained multimodal models exhibit a disproportion in classification ability~\cite{OGR-GB:conf/cvpr/WangTF20}.  

% As the greedy~\cite{nvGesture:conf/icml/WuJCG22} in the training process, the models tend to converge rapidly in strong modality while slowly in weak modality. This imbalanced learning process ultimately leads to weak modalities being insufficiently learned~\cite{ARM:conf/ijcai/YangYZJ15,MML:conf/mm/Zhou0CD023,MML:journals/corr/abs-2106-11059,MML:journals/pami/wei2024fly}, thereby impacting the overall performance.

% recent efforts
Researchers have explored the modality imbalance issue from various perspectives in multimodal learning~\cite{OGR-GB:conf/cvpr/WangTF20,OGM:conf/cvpr/PengWD0H22,MLA:conf/cvpr/ZhangYBY24}. Given the inconsistent learning progress between strong and weak modalities, a natural idea~\cite{OGR-GB:conf/cvpr/WangTF20,OGM:conf/cvpr/PengWD0H22,AGM:conf/iccv/LiLHLLZ23,MSLR:conf/acl/YaoM22}  is to manually intervene in their learning processes to achieve rebalancing. Another type of method is to bridge the information gap between modality training phases and enhance the interaction between different modalities during training. To be specific, impressive works~\cite{MLA:conf/cvpr/ZhangYBY24,DI-MML:conf/mm/FanXWLG24} such as MLA~\cite{MLA:conf/cvpr/ZhangYBY24}, ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} and DI-MML~\cite{DI-MML:conf/mm/FanXWLG24} focus on bridging the learning gap of different modalities through injecting the optimization information between modalities. 

% introduce our method.
Although the above methods can rebalance multimodal learning, they focus more on balancing the learning process while failing to enhance the classification ability explicitly. Compared to weaker modalities, stronger modalities typically yield more robust classifiers due to their more sufficient information~\cite{ARM:conf/ijcai/YangYZJ15}. Is there a way to directly improve the performance of weak classifiers to balance the classification performance between strong and weak modalities? A natural choice is boosting~\cite{Adaboost:conf/eurocolt/FreundS95,GradientBoosting:journal/AS/FriedmanJ}, which utilizes the ensemble technique to enhance the ability of the weak classifier. We conduct a toy experiment to illustrate this idea on \CREMAD~dataset~\cite{CREMAD:journals/taffco/CaoCKGNV14}, where the classifier of weak modality is enhanced by the gradient boosting~\cite{GradientBoosting:journal/AS/FriedmanJ}. The results in Figure~\ref{fig:intro} present the comparison among naive MML, a model learning adjustment-based MML approach~(G-Blend~\cite{OGR-GB:conf/cvpr/WangTF20}), and gradient boosting-based MML~(MML w/ GB). For MML w/ GB, we apply the gradient boosting algorithm to further improve the trained video model using naive MML, while keeping the audio model fixed. We can find that the classification gap between video and audio modalities of naive MML and G-Blend is relatively large. More importantly, for MML w/ GB, the accuracy of audio modality remains unchanged, but the accuracy of video is greatly improved, leading to the improvement of overall accuracy.

% \todo{For gradient boosting-based MML, multiple classifiers are trained to capture the residual error of the previous classifier, with each containing multiple neural network layers and a classification head. From Figure~\ref{fig:intro}, we can observe that the overall performance of MML can be improved by enhancing the classification ability of weak modality with gradient boosting, thus rebalancing the modality learning.}

According to the aforementioned observations, in this paper, we propose a novel multimodal learning approach by designing a sustained boosting algorithm to facilitate the classification ability of the weaker modality. Concretely, we first design a configurable classification module, called the configurable classifier. This module takes features extracted by the encoder as input and provides predictions for the given data. We propose a sustained boosting algorithm by using this module as a basic classifier. Then, we utilize OGM~\cite{OGM:conf/cvpr/PengWD0H22} score to monitor the learning status during joint training, and further propose an adaptive classifier assignment~(ACA) strategy to adjust the classifier of weak modality. To this end, we can enhance the classification ability for the weak modality, thereby rebalancing the classification ability of strong and weak modalities. In Figure~\ref{fig:intro}, we present the classification enhancement results of our method~(Ours). We can find that the performance of our method outperforms that of MML w/ GB thanks to the sustained boosting and adaptive classifier assignment strategy. Furthermore, it is worth mentioning that ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} also employs the gradient boosting algorithm for multimodal learning. However, unlike our approach, ReconBoost uses gradient boosting to iteratively learn complementary information across modalities. Our main contributions are outlined as follows:
\begin{itemize}
\item We propose a sustained boosting algorithm in MML to simultaneously minimize the classification and residual errors based on a designed configurable classifier module.
\item Based on the learning status, we propose a novel adaptive classifier assignment strategy to dynamically enhance the classification ability of weak modality, thus rebalancing the classification ability of all modalities.
\item We conduct comprehensive experiments to verify the effectiveness of our approach. Results demonstrate that our approach can outperform SoTA baselines to achieve the best performance by a large margin.
\end{itemize}

\section{Related Work}
\subsection{MML under Imbalanced Scenario}
The goal of multimodal learning~\cite{MML:conf/ijcai/YangWZX019,MMDL:conf/icml/JiaYXCPPLSLD21,MMDL:conf/icml/NgiamKKNLN11,MMDL:conf/cvpr/SinghHGCGRK22,MML:conf/nips/HuangDXCZH21} is to fuse the multimodal information from diverse sensors. Compared to unimodal methods, MML can mine data information from different perspectives, thus the performance of multimodal learning should be better~\cite{MML:conf/cvpr/LvCHDL21,MML:conf/nips/SimonyanZ14,MML:conf/cvpr/HuLL16,MML:conf/cvpr/GaoOGT20}. However, due to heterogeneity of multimodal data, multimodal learning often encounters imbalance problems~\cite{OGR-GB:conf/cvpr/WangTF20,ModalCompetition:conf/icml/HuangLZYH22} in practice, leading to performance degeneration of MML. 

Early pioneering works~\cite{OGR-GB:conf/cvpr/WangTF20,OGM:conf/cvpr/PengWD0H22,PMR:conf/cvpr/Fan0WW023,MML:conf/cvpr/Ge0G00AILZ23} focus more on adaptively adjusting the learning procedure for different modalities. Representative approaches in this category employ different learning strategies, e.g., gradient modulation~\cite{OGM:conf/cvpr/PengWD0H22,AGM:conf/iccv/LiLHLLZ23} and learning rate adjustment~\cite{MSLR:conf/acl/YaoM22}, to rebalance the learning of weak and strong modalities. Other approaches including MLA~\cite{MLA:conf/cvpr/ZhangYBY24}, DI-MML~\cite{DI-MML:conf/mm/FanXWLG24}, ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} and MAIE~\cite{MAIE:journals/corr/abs-2407-04587} take a different path, focusing on enhancing the interaction between modalities to address the modality imbalance problem. For example, MLA~\cite{MLA:conf/cvpr/ZhangYBY24} designs an alternating algorithm to train different modalities iteratively. During the training phase, the interaction is enhanced by transferring the learning information between different modalities. ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} balances modality learning by leveraging gradient boosting to capture information from other modalities during interactive learning. 

The aforementioned methods focus on rebalancing the learning process for weak and strong modalities while failing to explicitly facilitate the classification ability of the weak modality. In this paper, we aim to address the modality imbalance issue from facilitating the classification ability of weak modality and rebalancing the classification ability of weak and strong modalities. 

%It is worth mentioning that ReconBoost~\cite{ReconBoost:conf/icml/CongHua24} also employs the gradient boosting algorithm for MML. However, unlike our approach, ReconBoost uses gradient boosting to iteratively learn complementary information across modalities, rather than focusing on enhancing the classification performance of weaker modalities.

 
\subsection{Boosting Method}

Boosting algorithm~\cite{LightGBM:conf/nips/KeMFWCMYL17,Adaboost:conf/eurocolt/FreundS95,GradientBoosting:journal/AS/FriedmanJ,XGBoost:conf/kdd/ChenG16,Boost:conf/cvpr/LiuHMT14,Boost:conf/nips/ProkhorenkovaGV18,Boost:conf/icml/CortesMS14} is one of the most important algorithms in ensemble learning. The core idea of boosting is to integrate multiple learners to create a strong learner. Adaboost~\cite{Adaboost:conf/eurocolt/FreundS95}, one of the earliest boosting algorithms, adjusts the weights of incorrectly classified data points, giving more attention to the harder-to-classify examples in each iteration. Gradient boosting~\cite{GradientBoosting:journal/AS/FriedmanJ}, on the other hand, builds models in a stage-wise fashion, minimizing a loss function through gradient descent. 

The key advantage of boosting lies in its ability to improve model accuracy without requiring complex individual models. Therefore, boosting becomes the natural choice for improving the performance of weak classifiers.
\begin{figure*}
\centering
\includegraphics[scale=0.65]{figures/arch_draw.pdf}
\caption{The framework of our proposed method. We utilize the video and audio modalities as examples, with the numbers of video and audio classifiers denoted by $n^v$ and $n^a$, respectively.}
\label{fig:ours}
\end{figure*}

\section{Problem Definition}
\subsection{Notation}
In this paper, we use boldface lowercase letters like $\z$ to denote vectors. The symbol $\odot$ is used to denote the Hadamard product. We use $\Vert\cdot\Vert_2$ to denote the $L_2$ norm of vectors. Furthermore, $\delta(\cdot)$ denotes the indicator function, i.e., $\delta(true)=1$, otherwise $\delta(false)=0$. $\mathtt{min}(\cdot)$ denotes the function that returns the minimum value. $\mod(a,b)$ returns the remainder after division of $a$ by $b$.

\subsection{Multimodal Learning}
For simplicity, we use two modalities, i.e., audio and video, for illustration. It is worth mentioning that our method can be easily adapted to cases with more than two modalities.

Assume that we have $N$ data points, each of which has audio and video modalities. Without loss of generality, we use $\X=\{(\x_i^a,\x^v_i)\}_{i=1}^N$ to denote the multimodal data, where $\x_i^a$ and $\x_i^v$ denote the $i$-th data point of audio and video, respectively. In addition, we are also given a category labels set $\Y=\{\;\y_i~\vert~\y_i\in\{0,1\}^K\}_{i=1}^N$, where $K$ denotes the number of category labels. Given the above training information $\X$ and $\Y$, the goal of multimodal learning is to train a model to fuse the multimodal information and predict its category label as accurately as possible. 

\section{Methodology}\label{sec:AGB}
In this section, we present our method in detail. The architecture of our method is shown in Figure~\ref{fig:ours}, where the audio and video modalities are used as an example for illustration. 
\subsection{Sustained Boosting}
For the sake of simplicity, we use superscript $r$ to indicate the module corresponding to a specific modality in this section, where $r\in\{a,v\}$. With the rapid growth of deep learning, representative MML approaches~\cite{MMDL:conf/icml/NgiamKKNLN11,OGR-GB:conf/cvpr/WangTF20,PMR:conf/cvpr/Fan0WW023,AGM:conf/iccv/LiLHLLZ23} have adopted deep neural network~(DNN) for multimodal learning. Following these methods, we also utilize DNN to construct our models. Specifically, we use $\phi^r(\cdot)$ to denote encoders. Then the features can be calculated by $\bu^r=\phi^r(\x^r;\theta^r)$, where $\theta^r$ denotes the encoder parameters. Then, the prediction of given data can be calculated by a classifier $\psi^r(\cdot)$: $\p^r=\psi^r(\bu^r;\Theta^r),$
where $\Theta^r$ denotes the parameters of the classifier. Based on $\p^r$ and its ground-truth, the objective function can be written as:
\begin{align}
\LM_{\textit{CE}}(\X^r,\Y)=\frac{1}{N}\sum_{i=1}^{N}\ell(\p^r_i,\y_i)=-\frac{1}{N}\sum_{i=1}^{N}\y_i^\top\log(\p^r_{i}),\label{obj:mml}
\end{align}
where $\Phi^r \triangleq \{\theta^r,\Theta^r\}$ denotes the parameters to be learned, $\X^r\triangleq \{\x^r_i\}_{i=1}^N $ and $\ell(\cdot)$ denotes the cross entropy loss.

By training the model for each modality based on objective function~(\ref{obj:mml}), we can obtain multiple individual classifiers. Due to the existence of strong and weak modalities~\cite{ARM:conf/ijcai/YangYZJ15}, these classifiers exhibit different classification abilities. Hence, we can employ boosting technique~\cite{GradientBoosting:journal/AS/FriedmanJ} to improve the classification ability of weak modality. 

Concretely, assuming the classification performance of the $r$-th modality requires improvement, we first apply the gradient boosting algorithm to train $n$ classifiers for the $r$-th modality. Since feature extraction focuses on common patterns, we set the encoders of all classifiers to be shared. Then the $j$-th classifier can be defined as: $\Phi^r_t\triangleq \{\theta^r,\Theta^r_t\}, t\in\{1,\cdots, n\}$. In practice, we adopt multiple fully-connected layers and nonlinear activation rectified linear unit~(ReLU)~\cite{ReLU:journals/corr/abs-1803-08375} to construct our classification module. This module called the configurable classifier, is relatively independent and can be adjusted based on the classification ability. Furthermore, we adopt the shared head structure commonly used in MML~\cite{MLA:conf/cvpr/ZhangYBY24,MAIE:journals/corr/abs-2407-04587,MVIEW:conf/iccv/0065MW023} to strengthen the interaction between weak and strong modalities during training.

Inspired by gradient boosting~\cite{GradientBoosting:journal/AS/FriedmanJ}, the classification ability can be facilitated through minimizing the residual error introduced by previous classifiers. Concretely, when we learn $t$-th classifier, the residual labels are defined as:
\begin{align}
\hat\y^r_{it}=\y_i-\lambda\sum_{j=1}^{t-1}\y_i\odot\p^r_{ij},\nonumber
\end{align}
where $\lambda\in[0,1]$ is used to soften hard labels~\cite{LabelSmoothing:conf/cvpr/SzegedyVISW16} and we utilize $\y_i$ to mask non ground-truth labels to ensure the non-negativity of residual labels. Then the objective function can be defined as follows:
\begin{align}
\epsilon(\x^r_i,\y_i,t)=\ell\big(\p^r_{it},\hat\y^r_{it}\big),\label{eq:residual}
\end{align}
where $\p^r_{it}$ denotes the prediction obtained by $t$-th classifier for $i$-th data point. Since we utilize a shared encoder, the encoder will be updated when training the $t$-th classifier. Therefore, other classifiers must be updated simultaneously to prevent performance degradation. The corresponding objective can be formed as:
\begin{small}
\begin{align}
\epsilon_{o}(\x^r_i,\y_i,t)&=\ell\left(\p^r_{it}+\sum_{j=1}^{t-1}\p^r_{ij},\y_i\right)=\ell\left(\sum_{j=1}^{t}\p^r_{ij},\y_i\right).\label{eq:overall}
\end{align}
\end{small}

Meanwhile, we have to ensure the first $t-1$ classifiers are well-trained. Hence, we define the following objective for $t-1$ classifiers:
\begin{align}
\epsilon_{p}(\x^r_i,\y_i,t)&=\ell\left(\sum\nolimits_{j=1}^{t-1}\p^r_{ij},\y_i\right).\label{eq:previous}
\end{align}

By combining~(\ref{eq:residual}),~(\ref{eq:overall}), and~(\ref{eq:previous}), the objective can be defined as:
% \begin{small}
\begin{align}
L(\x^r_i,\y_i,t)=\epsilon(\x^r_i,\y_i,t)+\epsilon_{o}(\x^r_i,\y_i,t)+\epsilon_{p}(\x^r_i,\y_i,t).\label{eq:obj}
\end{align}
% \end{small}
Unlike traditional gradient boosting~\cite{GradientBoosting:journal/AS/FriedmanJ}, our method sustainedly minimizes classification and residual errors by optimizing~(\ref{eq:obj}).
Then the overall loss of sustained boosting can be formed as:
\begin{align}
\LM_{\textit{SUB}}(\X^r,\Y;\Phi^r)=&\frac{1}{N}\sum\nolimits_{i=1}^{N}L(\x^r_i,\y_i,n).\label{obj:gbm}
\end{align}


\begin{algorithm}[t]
\caption{ACA algorithm.}\label{algo:ACA}
\begin{algorithmic}[1]
\Require{OGM score $\s_t$.}
\Ensure{Classification weight ${\boldsymbol{\omega}}_t$ and assignment decision.}\\
\textbf{INIT} initialize assignment status as \textit{false}.
\If{$s^a_t-\sigma s^v_t>\tau$}\Comment{Make decision based on $\s_t$.}
\State Set assigning video classifier as \textit{true}.
\ElsIf{$s^a_t-\sigma s^v_t<\tau$}
\State Set assigning audio classifier as \textit{true}.
\EndIf
\Repeat\Comment{Learn classification weights ${\boldsymbol{\omega}}_t$.}
\State Minimize $\LM_{\textit{ACA}}$ according to SGD; %
\State Update ${\boldsymbol{\omega}}_t$;
\Until{Converge.}
\end{algorithmic}
\end{algorithm}



\subsection{Adaptive Classifier Assignment}
Thus far, we have defined a configurable classifier module and designed a sustained boosting in MML to enhance the classification performance of weak modality. However, recent studies~\cite{OGM:conf/cvpr/PengWD0H22} have shown that differences between modalities evolve dynamically due to imbalance issues in MML. This implies the need to design a strategy for enhancing classification ability that adapts to dynamic changes. Hence, we propose an adaptive classifier assignment strategy to adjust the number of the weak classifier. For simplicity, we redefine the modality classifiers as: $\Phi^r_t\triangleq \{\theta^r,\Theta^r_t\}, t\in\{1,\cdots, n^r\}$, where $n^r$ is the parameter to be updated. 

Then, we utilize OGM~\cite{OGM:conf/cvpr/PengWD0H22} score to monitor the learning status. At $t$-th iteration, OGM score can be calculated by:
\begin{align}
\forall r\in\{a,v\},s^r_t=\frac{1}{N}\sum_{i=1}^N\y_i^\top\left[\sum_{j=1}^{n^r}\p^r_{ij}\right].\nonumber
\end{align}
OGM score reflects the classification ability of the models. Hence, if $s^a_t-\sigma s^v_t>\tau$, we assign a new configurable classifier for video modality at this iteration, where $\sigma\ge 1$ is the coefficient. $\tau$ is the dead zone for fault tolerance. Unless otherwise specified, the default is $\sigma=1.0$ and $\tau=0.01$. On the contrary, we also assign a new configurable classifier for audio modality if $s^a_t-\sigma s^v_t<\tau$.

To precisely refine the imbalance between modalities, we use the learning state at the $t$-th iteration to compute the weights for each modality's classifier. More specifically, we utilize multiple layers DNN to learn the weights. Corresponding objective can be formed as:
\begin{align}
\LM_{\textit{ACA}}(\s_t)=\Vert\g_t-{\boldsymbol \omega}_t\Vert^2_2,\nonumber
\end{align}
where ${\boldsymbol \omega}_t=[\omega^a_t,\omega^v_t]$ denotes the weight to be learned. And $\g_t=[g_t^a,g_t^v]$ indicates the classification ability based on $s^r_t$, where $g_t^a=\delta(s_t^a=\mathtt{min}([s_t^a,s_t^v]))$, $g_t^v=1-g_t^a$. 

Then the prediction of each classifier is redefined as $\hat\p^r_{ij}=\omega^r_t\p^r_{ij}$. The learning algorithm of adaptive classifier assignment is summarized in Algorithm~(\ref{algo:ACA}). Based on ACA algorithm, we can adjust the number of classifiers based on learning status and impose the weight over training of each modality by substituting $\p^r_{ij}$ as $\hat\p^r_{ij}$ in problem~(\ref{obj:gbm}). Our algorithm is summarized in Algorithm~(\ref{algo:ours}). In practice, we perform ACA algorithm to determine if we need to adjust the classification ability every $T$ iterations.

\begin{algorithm}[t]
\caption{Learning algorithm of our proposed method.}\label{algo:ours}
\begin{algorithmic}[1]
\Require{Training data $\X$, category labels $\Y$.}
\Ensure{The learned DNN models for all modalities.}\\
\textbf{INIT} initialize the number of classifier $n^a=1$, $n^v=1$. Initialize iteration $t=1$. Initialize DNN parameters $\Phi^a_t$ and $\Phi^v_t$. Initialize ${\boldsymbol{\omega}}_t=[1,1]$.
\Repeat
\State {\color{blue}// {Learn MML models.}}
\State Sample a mini-batch $\X_t=\{(\x_i^a,\x_i^v)\}_{i=1}^{n_b}$; 
\State $\forall \x_i^a,\x_i^v\in\X_t$, calculate features $\bu_i^a$ and $\bu_i^v$;
\State Calculate predictions $\{\p^a_{ij}\}_{j=1}^{n^a}$ and $\{\p^v_{ij}\}_{j=1}^{n^v}$.
\State Calculate loss in~(\ref{obj:gbm}) based on predictions and ${\boldsymbol{\omega}}_t$;
\State Update DNN parameters $\Phi^a_t$ and $\Phi^v_t$ based on SGD;
\State {\color{blue}// {Call ACA algorithm.}}
\If{$\mod(t,T)=0$}
\State Calculate OGM score based on predictions;
\State Call ACA algorithm to update ${\boldsymbol{\omega}}_t$ and assignment decisions;
\If{Assigning audio classifier is \textit{true}}
\State Add a classifier for audio modality;
\State $n^a=n^a+1$;
\EndIf
\If{Assigning video classifier is \textit{true}}
\State Add a classifier for video modality;
\State $n^v=n^v+1$;
\EndIf
\EndIf
\State Update $t=t+1$;
\Until{Converge or reach maximum iterations.}
\end{algorithmic}
\end{algorithm}

\subsection{Model Inference}
After training, the learned multimodal models can be applied to perform classification for any unseen data point. More specifically, given data point $\x_i=(\x_i^a,\x_i^v)$, we utilize the following equation to obtain the predictions:
\begin{align}
\forall r\in\{a,v\},\;\bar\p_i^r=\sum_{t=1}^{n^r}\p^r_{it}=\sum_{t=1}^{n^r}\psi^r_t(\phi^r(\x_i^r;\theta^r);\Theta^r).
\nonumber
\end{align}
Based on $\bar\p_i^a$, $\bar\p_i^v$, and the learned weights ${\boldsymbol{\omega}}^*$, we can adopt a specific late fusion strategy to obtain the final prediction.
    
\section{Experiments}
\begin{table*}[t]
\centering
\captionof{table}{The accuracy results on \CREMAD, \Kinetics, and \NVGesture~datasets. The best accuracy is shown in boldface. The Top-3 results are highlighted with progressively darker shades of orange.}
\label{tab:main-exp}
\begin{tabular}[b]{|r||c|c|c||c|c|c||c|c|c|c|}
\hline
\multirow{2}{*}{Method}  & \multicolumn{3}{c||}{{\CREMAD}} & \multicolumn{3}{c||}{{\Kinetics}} & \multicolumn{4}{c|}{{\NVGesture}}\\\cline{2-11}
                              & Multi& Audio & Video & Multi& Audio & Video & Multi& RGB & OF & Depth \\\hline\hline    
G-Blend~\reference{CVPR'20}   & 0.6465 & 0.6075  & 0.4301 & 0.6710 & 0.5160 & 0.4275 & 0.8299 & 0.7054 & 0.7178 & 0.7252 \\\hline
MSLR~\reference{ACL'22}       & 0.6868 & \third{0.6357}  & 0.2903 & 0.6756 & 0.5199 & 0.3254 & 0.8237 & 0.3672 & 0.3755 & 0.5373 \\\hline
OGM~\reference{CVPR'22}       & 0.6612 & 0.6209  & 0.2903 & 0.6582 & 0.5013 & 0.3165 & $-$ & $-$ & $-$ & $-$ \\\hline
PMR~\reference{CVPR'23}       & 0.6659 & {0.6263}  & 0.4355 & 0.6675 & 0.4750 & 0.3772 & $-$ & $-$ & $-$ & $-$ \\\hline
AGM~\reference{ICCV'23}       & 0.6733 & 0.4798  & 0.3655 & 0.6787 & 0.5036 & 0.3869 & 0.8279 & 0.6598 & 0.6722 & 0.7324 \\\hline
MMPareto~\reference{ICML'24}  & 0.7487 & \second{0.6586} &  0.5108 & \third{0.7000} & \second{0.5226} & 0.4953 & \third{0.8382}& \second{0.7593} & \second{0.7925} & \first{0.8050} \\\hline
MLA~\reference{CVPR'24}       & \second{0.7943} & 0.5727  & \second{0.6491} & \second{0.7004} & \first{0.5572} & \second{0.5402} & {0.8340} & {0.7241} & \third{0.7573} & {0.7742} \\\hline
ReconBoost~\reference{ICML'24}& \third{0.7557} & 0.5966  & \third{0.6364} & {0.6855} & 0.4941 & \third{0.5031} & \second{0.8386}& \third{0.7290} & {0.7471} & \third{0.7782}  \\
\hline\hline
Ours                          & \first{0.8441} & \first{0.6788}  & \first{0.6770} & \first{0.7324} & \third{0.5219} & \first{0.5856} &\first{0.8575} & \first{0.7732} & \first{0.8070} & \second{0.8001}  \\
% Gain        & \\
\hline 
\end{tabular}
\end{table*}

\begin{table}[t]
\centering
\caption{The results on image-text~datasets, i.e., \Twitter~and \Sarcasm. The results are indicated similarly to those in Table~\ref{tab:main-exp}.}
\label{tab:image-text}
\begin{tabular}{|r||c|c||c|c|}
\hline
\multirow{2}{*}{Method}  & \multicolumn{2}{c||}{{Accuracy}}& \multicolumn{2}{c|}{{MacroF1}}\\\cline{2-5}
           &  \Twitter & \Sarcasm &  \Twitter & \Sarcasm\\\hline\hline
G-Blend    & 0.7309    & 0.8286 & \second{0.6799}   & 0.8215 \\\hline
MSLR       & 0.7232    & \second{0.8439} & 0.6382   & \second{0.8378}  \\\hline
OGM        & 0.7058    & 0.8360 & 0.6435    & 0.8293  \\\hline
PMR        & 0.7357    & 0.8310 & 0.6636    & 0.8256  \\\hline
AGM        & 0.7261    & 0.8360 & 0.6502    & 0.8293  \\\hline
MMPareto   & \third{0.7358}    & 0.8348 & 0.6729    & 0.8284 \\\hline
MLA        & 0.7352    & 0.8426 & 0.6713    & \third{0.8348} \\\hline
ReconBoost & \second{0.7442}    & \third{0.8437} & \first{0.6832}    & 0.8317 \\\hline\hline
Ours       & \first{0.7450}    & \first{0.8450} & \third{0.6794}    & \first{0.8384} \\\hline
\end{tabular}
\end{table}


\subsection{Dataset}
We carry out the experiments on six extensive multimodal datasets, i.e., \CREMAD~\cite{CREMAD:journals/taffco/CaoCKGNV14}, \Kinetics~\cite{Kinetics-Sound:conf/iccv/ArandjelovicZ17}, \NVGesture~\cite{NVGeasture:conf/cvpr/MolchanovYGKTK16}, \VGGSound~\cite{VGGSound:conf/icassp/ChenXVZ20}, \Twitter~\cite{Twitter15:conf/ijcai/Yu019}, and \Sarcasm~\cite{Sarcasm:conf/acl/CaiCW19}~datasets. The \CREMAD, \Kinetics, and \VGGSound~datasets consist of audio and video modalities. \NVGesture~dataset contains three modalities, i.e., RGB, optical flow~(OF), and Depth. \Twitter~and \Sarcasm~datasets consist of image and text modalities. The \CREMAD~dataset contains 7,442 clips, which are divided into training set with 6,698 samples and testing set with 744 samples. For \Kinetics~dataset, which contains 19,000 video clips, is divided into training set with 15,000 clips, validation set with 1,900 clips, and testing set with 1,900 clips. \VGGSound~dataset includes 168,618 videos for training and validation, and 13,954 videos for testing. The \NVGesture~dataset is divided into 1,050 samples for training and 482 samples for testing.\Twitter~dataset is divided into training set with 3,197 pairs, validation set with 1,122 pairs and testing set with 1,037 pairs. \Sarcasm~dataset includes 19,816 pairs for the training set, 2,410 pairs for the validation set, and 2,409 pairs for the testing set. More details are provided in the appendix.

\subsection{Experimental Settings}

\subsubsection{Baselines and Evaluation Metric}
We select various SoTA baselines for comparison, including G-Blend~\cite{OGR-GB:conf/cvpr/WangTF20}, MSLR~\cite{MSLR:conf/acl/YaoM22}, OGM~\cite{OGM:conf/cvpr/PengWD0H22}, PMR~\cite{PMR:conf/cvpr/Fan0WW023}, AGM~\cite{AGM:conf/iccv/LiLHLLZ23}, MMPareto~\cite{MMPareto:conf/icml/WeiH24}, MLA~\cite{MLA:conf/cvpr/ZhangYBY24}, and ReconBoost~\cite{ReconBoost:conf/icml/CongHua24}. Among these methods, ReconBoost employs the gradient boosting algorithm to capture the error caused by other modalities.

Following the setting of MLA~\cite{MLA:conf/cvpr/ZhangYBY24} and ReconBoost~\cite{ReconBoost:conf/icml/CongHua24}, we adopt accuracy, mean average precision~(MAP) and MacroF1 as evaluation metrics. The accuracy measures the proportion of correct predictions of total predictions. MAP returns the average precision of all samples. And MacroF1 calculates the average F1 across all categories.


\subsubsection{Implementation Details}
Following OGM~\cite{OGM:conf/cvpr/PengWD0H22}, we employ ResNet18~\cite{ResNet:conf/cvpr/HeZRS16} as the backbone to encode audio and video for \CREMAD, \Kinetics~and \VGGSound~datasets. All the parameters of the backbone are randomly initialized. For \NVGesture~dataset, we employ the I3D~\cite{I3D:conf/cvpr/CarreiraZ17} as unimodal branch following the setting of~\cite{nvGesture:conf/icml/WuJCG22}. We initialize the encoder with the pre-trained model trained on ImageNet. For the architecture of the configurable classifier, we explore a two-layer network, which can be denoted as ``Layer1$(D\times 256) \mapsto $ ReLU $ \mapsto $ Layer2 $(256\times K)$''. Here, $D$ denotes the output dimensions of encoders, ``Layer1''/``Layer2'' are fully connected layer, and ``ReLU'' denotes the ReLU~\cite{ReLU:journals/corr/abs-1803-08375} activation layer. Furthermore, the Layer2 is utilized as shared head for all modalities as described in Section~\ref{sec:AGB}. Both Layer1 and Layer2 are randomly initialized. In addition, all hyper-parameters are selected by using the cross-validation strategy. Specifically, we use stochastic gradient descent~(SGD) as the optimizer with a momentum of $0.9$ and weight decay of $1\times 10^{-4}$. The initial learning rate is set to be $1\times 10^{-2}$ for \CREMAD, \Kinetics, \VGGSound~, and \NVGesture~datasets. During training, the learning rate is progressively reduced by a factor of ten upon observing loss saturates. The batch size is set to be 64 for \CREMAD~and \Kinetics~datasets, 16 for \VGGSound~dataset, and 2 for \NVGesture~dataset. We set the iteration $T$ for checking whether to assign the classifier to 20 epochs for \CREMAD~dataset, 10 for \Kinetics, \NVGesture~datasets. For all datasets, we set $\lambda$ to be $0.2$. For the ACA algorithm in~(\ref{algo:ACA}), we adopt a three-layer network with ReLU to learn ${\boldsymbol \omega}_t$, and the init learning rate is $0.001$. The optimization algorithm of ACA is the same as that of the backbone. For \Twitter~and \Sarcasm~datasets, following~\cite{Twitter15:conf/ijcai/Yu019,Sarcasm:conf/acl/CaiCW19}, we adopt BERT~\cite{BERT:conf/naacl/DevlinCLT19} as the text encoder and ResNet50~\cite{ResNet:conf/cvpr/HeZRS16} as the image encoder. We use Adam~\cite{Adam:journals/corr/KingmaB14} as the optimizer, with an initial learning rate of $1\times 10^{-5}$. The batch size is set to 64. We set iteration $T$ as 5 for \Twitter~and 1 for \Sarcasm~dataset. The other parameter settings are the same as audio-video datasets. For comparison methods, the source codes of all baselines are kindly provided by their authors. For fair comparison, all baselines also adopt the same backbone and initialization strategy for the experiment. All experiments are conducted on an NVIDIA GeForce RTX 3090 and all models are implemented with pytorch.
\begin{table}[t]
\centering
\caption{The accuracy on \VGGSound~dataset. The accuracy is indicated similarly to those in Table~\ref{tab:main-exp}.}
\label{tab:vggsound}
\begin{tabular}{|r|c|c|c|}
\hline
\multirow{2}{*}{Method}  & \multicolumn{3}{c|}{{Accuracy}}\\\cline{2-4}
           & Multi    &  Audio & Video \\\hline\hline
AGM        &0.4711       &  0.4548&\third{0.2344} \\\hline
MLA        &\second{0.5165}      &  \third{0.4675}&\first{0.2616} \\\hline
MMPareto   &\third{0.5125}      &  \second{0.4735}&0.2485 \\\hline
ReconBoost &0.5097      &  0.4535&0.2263 \\\hline\hline
Ours       &\first{0.5304}       &  \first{0.4747}&\second{0.2515} \\\hline
\end{tabular}
\end{table}

\subsection{Experimental Results}
{\bf Classification Performance Comparison:} The classification results on all datasets are reported in Table~\ref{tab:main-exp}, Table~\ref{tab:image-text}, and Table~\ref{tab:vggsound}, where the best result is denoted as boldface, and the top-3 results are highlighted with progressively darker shades of orange. In Table~\ref{tab:main-exp}, ``$-$'' denotes that corresponding methods cannot applied to the dataset with more than two modalities. 

The results in Table~\ref{tab:main-exp} show the unimodal and multimodal accuracy on \CREMAD, \Kinetics, and \NVGesture~datasets. From Table~\ref{tab:main-exp}, we can see that: (1). Our method can outperform existing SoTA baselines to achieve the best accuracy in all cases for multimodal situations. Specifically, compared with the best baseline, our method achieves absolute boosts of 5.38\%, 3.8\%, and 2.86\% on three datasets respectively; (2). Our method can achieve the best accuracy for unimodal situations in almost all cases except audio on \Kinetics~dataset; (3). The accuracy on \NVGesture~dataset demonstrates that our method can extend to the case with more than two modalities and achieve the best performance. 

We further present the multimodal classification performance on image-text datasets, i.e., \Twitter~and \Sarcasm~datasets, in Table~\ref{tab:image-text}. The results in Table~\ref{tab:image-text} demonstrate that: (1). Compared with SoTA baselines, our method can achieve the best performance in terms of accuracy and MacroF1 in all cases for datasets with image and text modalities; (2). The absolute improvement on \Twitter~and \Sarcasm~datasets is relatively smaller than that on \CREMAD~and \Kinetics~datasets. One possible reason behind this phenomenon is that the modality imbalance between audio and video modalities is more serious than that between image and text modalities. 

As \VGGSound~is a relatively large dataset, we only choose a set of recent algorithms, including AGM~\cite{AGM:conf/iccv/LiLHLLZ23}, MLA, MMPareto, ReconBoost, for experiments. The results are shown in Table~\ref{tab:vggsound}. We can find that compared with SoTA baselines, our method can achieve the best performance, demonstrating the effectiveness of our method in the case of large-scale datasets. 

More results with different metrics and error bar are provided in the appendix due to space limitations.

\subsection{Sensitivity to Hyper-Parameters}
\noindent{\bf Sensitivity to Threshold $\sigma$: }We study the influence of threshold $\sigma$ on \CREMAD~dataset. The accuracy with different $\sigma\in[1,1.75]$ is shown in Figure~\ref{fig:sensitivity}~(a). We can find that our method is not sensitive to threshold $\sigma$ in a large range.

\begin{figure}[t] 
\centering
\begin{minipage}{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/sigma.pdf}\\
{(a). Threshold $\sigma$.}
\end{minipage} 
\begin{minipage}{.49\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/lambda.pdf}\\
{(b). Smoothing factor $\lambda$.}
\end{minipage}
\caption{Sensitivity to hyper-parameter $\sigma$ and $\lambda$.}
\label{fig:sensitivity}
\end{figure}

\begin{table}[t]
\centering
\caption{The results for ablation study on \CREMAD~dataset.}
\label{tab:ablation}
\begin{tabular}{|c|c|c|c||c|c|c|}
\hline
Metric&$\epsilon$&$\epsilon_{o}$&$\epsilon_{p}$& Multi  & Audio  & Video  \\\hline\hline
&\tikzcmark  &\tikzxmark   &\tikzcmark& 0.8240 & {0.6721} & \first{0.6842} \\\cline{2-7}
&\tikzxmark  &\tikzcmark   &\tikzxmark& 0.8246 & \first{0.6795} & {0.6714} \\\cline{2-7}
Accuracy&\tikzcmark  &\tikzcmark   &\tikzxmark& \second{0.8320} & \second{0.6794} & 0.6761 \\\cline{2-7}
&\tikzxmark  &\tikzcmark   &\tikzcmark& \third{0.8300} & 0.6754 & \second{0.6781} \\\cline{2-7}
&\tikzcmark  &\tikzcmark   &\tikzcmark& \first{0.8441} & \third{0.6788} & \third{0.6770} \\\hline\hline
&\tikzcmark  &\tikzxmark   &\tikzcmark& \third{0.9006} & 0.7247 & \first{0.7658} \\\cline{2-7}
&\tikzxmark  &\tikzcmark   &\tikzxmark& 0.8947 & \second{0.7382} & \third{0.7568} \\\cline{2-7}
MAP&\tikzcmark  &\tikzcmark   &\tikzxmark& \second{0.9066} & \third{0.7378} & 0.7559 \\\cline{2-7}
&\tikzxmark  &\tikzcmark   &\tikzcmark& {0.8994} & {0.7365} & \second{0.7625} \\\cline{2-7}
&\tikzcmark  &\tikzcmark   &\tikzcmark& \first{0.9121} & \first{0.7501} & {0.7547} \\\hline
\end{tabular}
\end{table}


\noindent{\bf Sensitivity to Smoothing Factor $\lambda$: }We explore the influence of smoothing factor $\lambda$ on \CREMAD~dataset. The accuracy with different $\lambda\in[0.1,1]$ is reported in Figure~\ref{fig:sensitivity}~(b). We can find that our method is not sensitive to hyper-parameter smoothing factor $\lambda$ in a large range.

\subsection{Ablation Study}
We investigate the effectiveness of our method by analyzing the influence of the key components of our objectives in Equation~(\ref{eq:residual}),~(\ref{eq:overall}), and~(\ref{eq:previous}), respectively denoted as $\epsilon$, $\epsilon_o$, and $\epsilon_p$. The results on \CREMAD~dataset are reported in Table~\ref{tab:ablation}. From Table~\ref{tab:ablation}, we can find that: (1). Both objectives in Equation~(\ref{eq:residual}),~(\ref{eq:overall}), and~(\ref{eq:previous}) can boost multimodal performance in terms of accuracy and MAP; (2). While the unimodal performance of the method using all objectives may not always reach the highest level, it achieves a more balanced classification performance across modalities.


\begin{figure}
\centering
\includegraphics[width=0.95\linewidth]{figures/residual-effect.pdf}
\caption{Performance comparison during training.}
\label{fig:residual}
\end{figure}

We further investigate the impact of residual learning on classification performance by comparing the performance of all $t$ classifiers with that of the first $t-1$ classifiers during the training. The results are presented in Figure~\ref{fig:residual}, where the former accuracy is denoted as ``Full Prediction'' and the latter is denoted as ``Prediction of $t-1$ CLS''. In Figure~\ref{fig:residual}, we also present the number of the video classifier. We observe that the number of classifiers for the video modality has increased, and the performance of all $t$ classifiers is generally superior to that of the first $t-1$ classifiers. This performance gain arises from our learning of the residual objective.

\subsection{Impact of Weak Classifier Assignment Strategy}
We conduct an experiment to study the influence of adaptive classifier assignment strategy. Specifically, we design a fixed classifier assignment strategy for comparison. This approach allocates $n^{(\text{fix})}$ classifiers for weak modality during the init stage. And we no longer dynamically adjust the number of classifiers during training for weak modality. 

\begin{table}[t]
\centering
\caption{The impact of weak classifier selection strategy.}  
\label{tab:selection-strategy}
\begin{tabular}{|R{1.15cm}|C{0.725cm}|C{0.725cm}||C{0.815cm}|C{0.815cm}|C{0.815cm}|}
\hline
\multirow{2}{*}{Strategy} &\multicolumn{2}{c||}{\#Classifier} &\multicolumn{3}{c|}{{Accuracy}}  \\\cline{2-6}
&          Audio&Video& Multi&Audio   & Video  \\\hline\hline
% Fixed      & 1&9  & 0.8226 & 0.6922 & 0.6358 \\\hline
Fixed      & 1&10 & \third{0.8091} & \second{0.6774} & \third{0.6156} \\\hline
Fixed      & 1&12 & \second{0.8118} & \third{0.6519} & \second{0.6277} \\\hline
Adaptive   & 1&10 & \first{0.8441} & \first{0.6788} & \first{0.6770} \\\hline
% Adaptive   & 1&20 & 0.8387 & 0.6734 & 0.6747 \\\hline
\end{tabular}
\end{table}

\begin{figure*}[t] 
\centering
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/naive_audio_tsne_6x8.pdf}\\
{(a). naive MML@Audio.}
\end{minipage} 
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/Recon_audio_tsne_6x8.pdf}\\
{(b). ReconBoost@Audio.}
\end{minipage}
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/our_audio_cat_all_tsne_6x8.pdf}\\
{(c). Ours@Audio.}
\end{minipage}\\
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/naive_video_tsne_6x8.pdf}\\
{(d). naive MML@Video.}
\end{minipage} 
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/Recon_video_tsne_6x8.pdf}\\
{(e). ReconBoost@Video.}
\end{minipage}
\begin{minipage}{.33\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/our_video_cat_all_tsne_6x8_circle_5.pdf}\\
{(f). Ours@Video.}
\end{minipage}
\caption{Visualization on \CREMAD~dataset. The video visualization highlights the need to improve weak modality classification.}
\label{fig:visualization}
\end{figure*}
  
The results on \CREMAD~dataset are reported in Table~\ref{tab:selection-strategy}, where $n^{(\text{fix})}$ is set to be 10 and 12. The results in Table~\ref{tab:selection-strategy} demonstrate that our proposed adaptive classifier assignment strategy can boost performance compared with fixed classifier strategy. This is because our method dynamically adjusts modality classification performance in response to modality imbalance during training.

\subsection{Impact of the Learnable Weight ${\boldsymbol{\omega}}$}
We exploit the influence of the weight learning strategy. Concretely, we compare the learning strategy with fixed weight strategy. The results with different $\omega_t^v$ are reported in Table~\ref{tab:learnable-weight}, where ``w/o ${\boldsymbol\omega}_t$'' denotes the method without weighting strategy. We can see that our method can achieve better performance by using weight learning strategy.

\subsection{Visualization Results}
We further study the property of embeddings through visualization. Specifically, we illustrate the t-SNE~\cite{tSNE:journal/jmlr/MaatenH08} results on \CREMAD~dataset for naive multimodal learning~(naive MML), ReconBoost~\cite{ReconBoost:conf/icml/CongHua24}, and our method in Figure~\ref{fig:visualization}. From Figure~\ref{fig:visualization}, we can find that: (1). Compared to naive MML, our method and ReconBoost can learn more discriminative multimodal features, as both approaches enhance the weak modality using information from the strong modality; (2). Compared to ReconBoost, our method demonstrates significantly superior classification performance on the video modality, with several distinct categories highlighted by circle markers in Figure~\ref{fig:visualization}~(f). This improvement is primarily attributed to our explicit enhancement of the classification capabilities of the weaker modality.

\begin{table}[t]
\centering
\caption{The impact of weight learning strategy.}  
\label{tab:learnable-weight}
\begin{tabular}{|c||c|c|c|}
\hline
$\omega_t^v$      & Multi          & Video          & Audio \\ \hline\hline
w/o ${\boldsymbol\omega}_t$ &\second{0.8360}&\first{0.6801}&\third{0.6694}\\\hline\hline
0.75             & 0.8333 & 0.6707& \first{0.6882} \\ \hline
0.5              & \third{0.8347}& 0.6653         & 0.6478         \\ \hline
0.25             & 0.8333 & \third{0.6667} & 0.6559 \\ \hline
Learnable~(Ours) & \first{0.8441} & \second{0.6788} & \second{0.6770}\\ \hline
\end{tabular}
\end{table}






\section{Conclusion}
To address the modality imbalance issue, we propose a novel multimodal learning approach by designing a sustained boosting algorithm to dynamically enhance the classification ability of weak modality. By designing a configurable classifier module, we propose a sustained boosting algorithm for multimodal learning. Then, we propose an adaptive classifier assignment strategy to dynamically facilitate the classification ability of weak modality. To this end, the classification ability can be rebalanced adaptively during the training procedure. Experiments on widely used datasets reveal that our method can achieve SoTA performance compared with various baselines by a large margin.
 
% \noindent{\bf Future Work: } 

% \noindent\textit{Extension to Early Fusion Approach: }Our proposed method mainly focuses on the classifier of each modality. For early fusion MML, our method can extend to balance the strong, weak, and fusion classification abilities. We leave it as a future work.

% \noindent\textit{Theoretical Analysis: }For our method, we enhance the classification ability of weak modality. The potential improvement upper bound of weak modality classification performance and its impact on balanced multimodal training merit further discussion.


{
    \small
    \bibliographystyle{ieeenat_fullname}
    \bibliography{full}
}

\end{document}
