\section{SETRec}\label{sec:method}
% To meet the two principles for identifier design, we propose a set-based-identifier for effective and efficient LLM-based generative recommendation. 
% In this section, we first elaborate the design of set-based item tokenization, and then delve into the decoding of set-based item identifier. 
% Lastly, we detail the instantiation of our proposed SETRec, including training and inference.  
To implement the set identifier paradigm, we propose a framework called SETRec for effective and efficient LLM-based generative recommendation, including order-agnostic item tokenization and simultaneous item generation as illustrated in Figure~\ref{fig:method_tokenizer}.  
% This section first elaborates on order-agnostic item tokenization and then delves into the simultaneous item generation. 
% Lastly, we detail the instantiation of our proposed SETRec, including training and inference.  


\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.02cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=0.88]{figures/method_tokenizer.pdf}
\caption{(a) demonstrates SETRec framework, including order-agnostic item tokenization, and simultaneous item generation. The dependencies within identifiers and query vectors are eliminated by the sparse attention mask (see Figure~\ref{fig:sparse_attn} for details). (b) illustrates order-agnostic item tokenization via CF and semantic tokenizers.}
\label{fig:method_tokenizer}
\end{figure}


\subsection{Order-agnostic Item Tokenization} 
% In SETRec, we integrate both semantic and CF information, 
% To assign each item with a set of tokens representing both CF and semantic information
% In SETRec, we introduce a set-based item tokenization strategy, which assigns each item with a set of tokens representing both CF and semantic information. 
Meeting the two principles, SETRec leverages a CF and a semantic tokenizer to endow multi-dimensional information into a set of order-agnostic continuous tokens\footnote{We do not use discrete tokens in SETRec because discretization inevitably suffers from information loss~\cite{lazebnik2008supervised}, potentially leading to suboptimal results.} as illustrated in Figure~\ref{fig:method_tokenizer}(b). 
% To integrate semantic and CF information into identifiers, 
% a possible solution is using a set of discrete tokens. 
% 为什么用连续的不用离散的？
% Using a set of discrete tokens is a possible solution. 
% Therefore, we alternatively adopt a set of continuous tokens for SETRec, aiming to harness the item information as much as possible. 
% 我们通过cf tokenizer和semantic tokenizer获取 CF token 和 semantic token
% To endow multi-dimensional features into the token set, we leverage a CF tokenizer and a semantic tokenizer to obtain CF tokens and semantic tokens, respectively. 

% \noindent$\bullet\quad$\textbf{Multi-dimensional Tokenizer}
\noindent$\bullet\quad$\textbf{CF Tokenizer.}
% CF tokenizer 
As shown in Figure~\ref{fig:method_tokenizer}(b), we utilize a pre-trained conventional recommender model (\eg SASRec~\cite{lazebnik2008supervised}) with a linear projection layer to obtain item CF embedding $\bm{z}_{\text{CF}}\in\mathbb{R}^{d}$, where $d$ is the hidden dimension of LLMs. 
Incorporating CF embeddings encourages LLM-based recommenders to facilitate recommendations for users/items with rich interactions. 
% The CF tokenizer ensures that items with similar interactions have similar identifiers. 
% facilitating recommendations for users and items with rich interactions
% As such, representing item with CF embeddings encourages LLM-based recommender to leverage collaborative information that is crucial for recommendation. 
% merely using CF embedding would struggle on cold-start items, thus leading to poor generalization. 

\noindent$\bullet\quad$\textbf{Semantic Tokenizer.} 
To fully utilize rich item semantic information, SETRec introduces a semantic tokenizer to obtain a set of semantic embeddings. 
Specifically, 
given the item semantic information $c$ such as title and categories, we first extract the item semantic representations $\bm{s}$ with a pre-trained semantic extractor (\eg SentenceT5~\cite{ni2021sentence}). 

To obtain the semantic embeddings, a straightforward approach is to compress semantic representation $\bm{s}$ into a single latent semantic embedding. 
% Nonetheless, single embedding for semantic tokens might suffer from the embedding collapse issue~\cite{}, potentially undermining the rich semantic content that distinguishes between items. 
Nonetheless, compressing multi-dimensional semantic information (\eg ``brand'' and ``price'') might suffer from the embedding collapse issue~\cite{guoembedding,pan2024ads}, potentially undermining the rich semantic content that distinguishes between items. 
To prevent this issue, as depicted in Figure~\ref{fig:method_tokenizer}(b), we tokenize each item into $N$ order-agnostic semantic embeddings via an AE: 
\begin{equation}
% \left\{
\begin{aligned}
&\bm{z} = \text{Encoder}(\bm{s}), 
\end{aligned}
% \right.
\end{equation}
where 
$\bm{z}=[\bm{z}_{S_{1}}, \bm{z}_{S_{2}},\dots,\bm{z}_{S_{N}}]\in\mathbb{R}^{Nd}$ denotes the concatenated semantic embeddings representing different latent semantic dimensions, and ${z}_{S_{n}}\in\mathbb{R}^{d}$ is the $n$-th semantic embedding. 
% where $d$ is the latent dimension of LLMs. 
% $N$ is the number of semantic embeddings, and $n\in[1,\dots, N]$. 
Notably, we utilize a unified AE instead of multiple independent AEs for two considerations: 
1) employing a single AE reduces the parameters with an approximate ratio of $\frac{1}{N}$, which is highly practical; 
2) alleviating the training instability that might be caused by multiple encoders' training~\cite{tang2023improving}. 
In addition, to encourage the semantic embeddings to preserve useful information as much as possible, 
a reconstruction loss is used to train the semantic tokenizer: 
\begin{equation}
    \mathcal{L}_{AE} = \|\bm{s}-\hat{\bm{s}}\|_2^{2}, 
\end{equation}
where 
$\hat{\bm{s}} = \text{Decoder}(\bm{z})$ is the reconstructed semantic representation. 

\noindent$\bullet\quad$\textbf{Token Corpus.} 
% 最终基于两个tokenizer，一个item的set-based identifier是什么，怎么表示的。
Based on the CF and the semantic tokenizer, we can obtain the set identifier for each item
$\tilde{i} = \{\bm{z}_{\text{CF}},\bm{z}_{S_{1}},\dots,\bm{z}_{S_{N}}\}$, consisting of a CF embedding and $N$ semantic embeddings. 
% 基于这两个之后，加一个token corpus。
We then can collect tokens from all items and obtain the token corpus for each information dimension, \ie $\mathcal{Z}_\text{CF}, \mathcal{Z}_{S_1}, \dots, \mathcal{Z}_{S_N}$. 
% where $\mathcal{Z}_k=\{\bm{z}_{k,j}|j\in\{1,\dots, |I|\}\}$. 
The collected token corpus is used as the grounding head for effective item grounding (\cf Section~\ref{sec:query-based_decoding}). 

\subsection{Simultaneous Item Generation} 
% To achieve item generation with set-based identifier, the key lies in generating the set 
% Given the user's historical interactions, SETRec gene
% Based on the set-based item tokenization, the user 
% without token dependencies, 
To efficiently and effectively generate set identifiers, it is crucial for SETRec to 
% the key lies in enabling simultaneous token generation in multiple dimensions.  
% To achieve this, 
1) guide LLMs to distinguish different dimensions and generate tokens aligning well with each dimension simultaneously (Section~\ref{sec:query-based_decoding}); 
2) ground the generated token set to existing items effectively (Section~\ref{sec:query-based_decoding}); 
3) eliminate the unnecessary dependencies introduced in user history (Section~\ref{sec:sparse_attention_mask}); 

\subsubsection{\textbf{Query-guided Generation}.}\label{sec:query-based_decoding}
% \noindent$\bullet\quad$\textbf{Query-based Simultaneous Decoding.}  
As shown in Figure~\ref{fig:method_tokenizer}(a), to guide LLMs to generate tokens that align well with the information dimensions, 
we introduce a set of learnable query vectors $\bm{q}\in\mathbb{R}^{d}$, where $d$ is the latent dimension of the LLMs, to guide the LLMs to distinguish between information dimensions (\eg CF and semantic) for token generation. 
Formally, the generated token $\hat{\bm{z}}_k$ for each dimension $k\in\{\text{CF}, S_1, S_2, \dots, S_N\}$ is obtained via:
\begin{equation}\label{eqn:query_decoding}
\left\{
\begin{aligned}
    &\bm{x} = [\{\bm{z}_{\text{CF}}, \bm{z}_{S_1}, 
     \dots, \bm{z}_{S_N}\}^{1}, \dots, \{\bm{z}_{\text{CF}}, \bm{z}_{S_1}, \dots,\bm{z}_{S_N}\}^{L}], \\
    &\hat{\bm{z}}_{k} = \text{LLM\_Layers}(\bm{x}, \bm{q}_k),
\end{aligned}
\right.
\end{equation}
where $\bm{q}_k$ is the learnable query vector to guide LLM generation for the information dimension $k$. 
Based on Eq. (\ref{eqn:query_decoding}), we can collect the generated token for all dimensions and obtain the generated set identifier  $\hat{i}=\{\hat{\bm{z}}_{\text{CF}}, \hat{\bm{z}}_{S_1}, \dots, 
\hat{\bm{z}}_{S_N}
\}$. 

\vspace{2pt}
\textbf{\textit{Token Generation Optimization.}} 
% 1. 为了教会LLM在生成的时候能做准确的item reocmmendation，我们鼓励llm能够生成target item的token。
To achieve accurate item recommendations, 
% we train SETRec to generate the set identifier of the target item. 
% 2. 介绍我们怎么做的 - specifically, 对于每一个feature 维度，我们encourage生成的token要和target token尽可能的相似. 
% Specifically, 
we encourage the generated token to align with the target token for every dimension: 
% 3. 公式 - xxx 介绍每个变量是什么
\begin{equation}\label{eqn:loss_gen}
    \mathcal{L}_{\text{Gen}} = - \frac{1}{|\mathcal{D}|} \sum_{\mathcal{D}} \sum_{k\in\mathcal{F}}\frac{\exp (sim(\hat{\bm{z}}_k,\bm{z}_{k}))}{\sum_{\bm{z}\in\mathcal{Z}_k}\exp(sim(\hat{\bm{z}}_k,\bm{z}))},
\end{equation}
where $\mathcal{F}=\{\text{CF}, S_1, \dots, S_N\}$, $sim(\cdot)$ is the similarity function (\eg inner product), and $\bm{z}_k$ is the target item token for the information dimension $k$. 
% 4. intuitive解释这样能够拉近正确的距离，拉远不相似的距离，鼓励也能鼓励item之间有所区分。
Intuitively, Eq. (\ref{eqn:loss_gen}) pushes the generated embedding closer to the target embedding and pulls away from other embeddings within the specific information dimension.  
% Besides, by pulling embeddings away from each other is also encouraged to distinguish between items for every feature dimensions, which further improves the embedding diversity. 


% 标题这里考虑换成两个黑体，token generatinon training, 和 token grounding for inference
% \noindent$\bullet\quad$\textbf{Token Set Grounding.}
\vspace{2pt}
\textit{\textbf{Token Generation Grounding.}} 
% 提出需要grounding的问题 - 解释在inference的时候生成token，得到上一步的set之后，直接使用是无法match到现有item上的。
Based on generated tokens obtained via Eq. (\ref{eqn:query_decoding}), the next step is to ground them to the existing items. 
% 顺应解释按照token去ground仍然会ooc - 一种简单的方法是用l2distance等等去match到token。但即便如此，match后得到的set，仍然很可能会out-of-corpus. 
% We introduce a two-step grounding strategy, which utilizes a 
However, this can be challenging since the possible combinations of the tokens from different information dimensions are much larger than the existing item corpus, \ie $\prod_{k\in\mathcal{F}}|\mathcal{Z}_k|\gg |\mathcal{I}|$. 
% 提出我们的解决方法：因此我们考虑借助token corpus来获取所有item上的logits. 
To solve this issue, we introduce a token set grounding strategy, which leverages the token corpus as grounding heads to obtain the item score. 
% specifically,（1. 根据token来获取每个feature维度的score代表item的score，2. 合并多维度之间的结果，获得全局的score）
% Specifically, we first obtain the item scores for each dimension $k$: 
% \begin{equation}\label{eqn:single_logits}
%     s_k=W_{\text{proj}}^{k}\bm{\hat{z}}_k,
% \end{equation}
% where $W_{\text{proj}}^{k}\in\mathbb{R}^{|I|\times d}$ is adopted from the token corpus $\mathcal{Z}_k$. 
% We then obtain the final matching scores with a linear combination of CF dimension and semantic dimension via 
% \begin{equation}\label{eqn:final_logits}
%     s = (1-\beta) s_\text{CF} + \beta \sum_{i=1}^{N} s_i,
% \end{equation}
% where $\beta$ is a hyper-parameter to balance the strength between CF and semantic dimensions. 
Formally, we have 
\begin{equation}\label{eqn:single_logits}\small
\left\{
\begin{aligned}    &s_k=W_{k}\bm{\hat{z}}_k, \\
&s = (1-\beta) s_\text{CF} + \beta \sum\nolimits_{{k\in \mathcal{F}\setminus{\text{CF}}}} s_{k}, \\
\end{aligned}
\right.
\end{equation}
% Specifically, we first obtain the item scores for each dimension $k$: 
% \begin{equation}\label{eqn:single_logits}
%     s_k=W_{\text{proj}}^{k}\bm{\hat{z}}_k,
% \end{equation}
where $W_{k}\in\mathbb{R}^{|I|\times d}$ is adopted from the token corpus $\mathcal{Z}_k$. 
The final item scores are obtained via a linear combination of CF and semantic dimensions, where $\beta$ is a hyper-parameter to balance the strength between CF and semantic dimensions. 
It is highlighted that the grounding heads for semantic dimensions are extendable to new items, leading to strong generalization ability (\cf Section~\ref{sec:overall_performance}). 


\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.02cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=1.1]{figures/sparse_attn.pdf}
\setlength{\fboxrule}{1pt}
\caption{Comparison between original attention and sparse attention ($N=1$). The sparse attention 1) eliminates the dependency over other tokens within the same item ({\color{myred}\fbox{\phantom{\rule{0.06cm}{0.06cm}}}}), and 2) boosts the efficiency with the flattened input, \ie query vectors are in the same sequence.}
\label{fig:sparse_attn}
\end{figure}

\vspace{2pt}
\subsubsection{\textbf{Sparse Attention Mask.}}\label{sec:sparse_attention_mask}
% \noindent$\bullet\quad$\textbf{Sparse Attention Mask.} 
% 提出问题 - 可能需要画一个图说明一下什么意思 。
% 两个问题 1.第一个问题是query在生成的时候batch会很浪费计算资源
% Based on the simultaneous decoding, each token can be generated independently at a single LLM call via batch decoding (Eq. (\ref{eqn:query_decoding})). 
% Nonetheless, this will cause repetitive self-attention computations for the user's historical interactions $\bm{x}$, thus leading to inefficiency. 
% % 1.第二个是transformer仍然需要序列送进去，如何瓦解item内部token的序列关系？
% Moreover, while simultaneous decoding bypasses the sequential generation of item identifier, the user's historical interactions are still sequentially encoded via causal attention mask\footnote{Most of LLMs adopt decoder-only architecture, which employs the causal attention mask. For the encoder-deocder LLMs, the bi-directional attention mask will not introduce noisy dependencies.}, potentially introducing dependencies between tokens within each identifier. 

% \vspace{2pt}
% To address these two challenges, we introduce a sparse attention mask, as illustrated in Figure~\ref{fig:sparse_attn}. 
% % 我们提出一个sparse attention mask 同时来解决这两个问题，获得efficient的set-based identifier generation的方法。
% Specifically, for the user's historical interactions, tokens associated with an identifier are treated as independent from each other (\eg CF embedding cannot attend to semantic embeddings for item 1). 
% However, these tokens can still attend to all tokens in previously interacted items (\eg a fully attended mask is applied to item 1 when calculating self-attention for tokens in item 2).
% This sparse attention mask ensures the order invariance of our proposed set-based identifier.
% %Meanwhile, they can attend all tokens in previously interacted items (\eg fully attended mask on item 1 when calculating self-attention for tokens of item 2). 
% %Based on the sparse mask, we can ensure the order invariance of our proposed set-based identifier. 

% % todo: 加一个证明，可以放到appendix里面

% 1. 先讲implicit order的事情，呼应intro以及方法部分。接着介绍我们的方法。
% 介绍完之后，说这样的sparse attention不仅移除了dependency，还能够有效的boost inference efficiency。
% 加一句解释说，eq 5 支持independent generation，但是利用original attention的话会重复计算。使用我们的sparse能够显著降低计算
% 跟上complexity analysis
% 1.第二个是transformer仍然需要序列送进去，如何瓦解item内部token的序列关系？
While simultaneous generation bypasses the sequential generation of item identifier, the flattened user's historical interactions are still sequentially encoded, inevitably introducing order information of tokens within each identifier (Figure~\ref{fig:sparse_attn}(a)). 
To combat this issue, we introduce a sparse attention mask as illustrated in Figure~\ref{fig:sparse_attn}(b). 
% 我们提出一个sparse attention mask 同时来解决这两个问题，获得efficient的set-based identifier generation的方法。
Specifically, for the user's historical interactions, tokens associated with an identifier are treated as independent from each other (\eg CF embedding cannot attend to semantic embeddings). 
However, these tokens can still attend to all tokens in previously interacted items (\eg a fully attended mask is applied to football when calculating self-attention for tokens in basketball).
Therefore, the sparse attention mask ensures the order agnosticism of the set identifier. 
% In addition, it also improves inference efficiency by reducing the duplicate self-attention calculations for the user's historical interactions $\bm{x}$ via original attention mask (Figure~\ref{fig:sparse_attn}(a)). 
% Based on the simultaneous decoding, each token can be generated independently at a single LLM call via batch decoding (Eq. (\ref{eqn:query_decoding})). 
% Nonetheless, this will cause repetitive self-attention computations for the user's historical interactions $\bm{x}$, thus leading to inefficiency. 

\vspace{2pt}
\noindent$\bullet\quad$\textbf{Time Complexity Analysis.} 
Moreover, the sparse attention mask can improve the generation efficiency by reducing the duplicate computations of the shared prefix via original attention mask (Figure~\ref{fig:sparse_attn}(a)). 
With $M$ information dimensions and $L$ historically interacted items, the time complexity for batch generation with the original attention mask is $M^3L^2d$. 
Remarkably, based on the flattened input with our proposed sparse attention mask, the time complexity reduces to $M^2L^2d$. 

% \noindent$\bullet\quad$\textbf{Identifier Order Invariance.} 

% The sparse attention mask not only accelerate the simultaneous decoding, 
% but also ensures the order invariance

% \noindent$\bullet\quad$\textbf{Order Invariance Analysis}

% 这里主要主张两件事 1. 提高效率。2. 这样子弄完了之后，是order-agnostic的。
% （可能写一个order-agnostic的分析） - 得参考一下这种equivariance怎么写



\subsection{Instantiation}
% 把SETRec instantiate到LLM上，我们通过一个overall loss来训练LLM和tokenizer:
To instantiate SETRec on LLMs, we optimize the CF and semantic tokenizers, learnable query vectors, and LLMs by minimizing: 
% 公式 - overall loss = L_AE + L_Gen
\begin{equation}\label{eqn:overall_loss}
    \mathcal{L} = \mathcal{L}_{\text{Gen}} + \alpha \mathcal{L}_{\text{AE}},
\end{equation}
where $\alpha$ is a hyper-parameter to control the strength of the tokenizer training. 
% 然后是inference的时候。我们利用tokenizer将所有item 进行tokenize，然后
During inference, SETRec first tokenizes all items into set identifiers and obtain token corpus $\mathcal{Z}$ for each information dimension. 
Then, to recommend item, SETRec transforms user history into identifier sequence and performs query-guided simultaneous generation with sparse attention mask via Eq. (\ref{eqn:query_decoding}) to generate tokens for all information dimensions.  
Finally, SETRec leverages token corpus as extendable grounding heads to ground the generated token set to the valid items via Eq. (\ref{eqn:single_logits}). 

% In this work, we instantiate SETRec on two representative generative language models with different architectures, \ie T5 and Qwen (refer to Section~\ref{sec:experiment}). 