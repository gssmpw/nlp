\section{Introduction}\label{sec:introduction}

\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.02cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=1.1]{figures/local_optima.pdf}
\caption{An example of local optima issue in beam search in autoregressive item generation. The target item fails to be generated because the initial token has a low probability and hence is discarded at the early steps by beam search.}
\label{fig:local_optima}
\end{figure}

% p1. LLM-based GR is promising, item tokenization is important
% llm-based generative rec展现了promising的results，引起research的广泛关注。
% 铺垫一下sequential setting, 建模item之间的dependency比较重要，
% 在实现过程中先执行item tokenization获取item identifier，然后为用户做推荐时，利用llm生成推荐的item identifier。
% （前面）
% llm很重要，怎么做的，最后再落到⬇️
% 还是要落到item tokenization很重要。
% Leveraging Large Language Models (LLMs) for generative recommendation has showcased promising results, attracting extensive attention~\cite{}. 
% Recent years have witnessed the success of leveraging Large Language Models (LLMs) for personalized recommendation~\cite{}. 
Large Language Models (LLMs) have recently demonstrated significant success in personalized recommendation, attracting widespread research interests~\cite{shi2024large,liao2024llara}.
Surpassing the traditional recommender models, LLMs excel in understanding complex user behaviors and diverse item characteristics due to their rich world knowledge and strong reasoning ability~\cite{touvron2023llama}. 
Typically, LLM-based recommenders transform the user's historical interactions into a token sequence to generate the target item as a recommendation. 
As shown in Figure~\ref{fig:intro}, a fundamental step of this process is item tokenization, which assigns each item an identifier to enable user history encoding and item generation. 
Therefore, item tokenization is essential in advancing LLM-based generative recommendation. 





% p2. 
% 有序token sequence：的问题（1. efficiency 2. local optima 3. 生成时利用上多信息）
% 单一的dense embedding (缺少用多个侧面，至少要用cf和semantic去表示，semantic也可以用多个表示)
% 落脚再需要重新思考一个关键的问题，一个item让llm理解和生成，identifier怎么表示？
Existing item identifiers for LLM-based generative recommendation can be broadly categorized into two groups: 
% Existing item identifiers can be broadly categorized into: 
\begin{itemize}[leftmargin=*]
    \item \textit{\textbf{Token-sequence identifier}} utilizes a discrete token sequence to represent multi-dimensional item information. 
    % Some studies adopt item meta information such as titles~\cite{bao2023bi}, descriptions~\cite{cui2022m6}, and tags~\cite{tan2024idgenrec}; 
    % while some works leverage external tokens entailed with hierarchical information, as seen in~\cite{rajput2023recommender,si2024generative,hua2023index}.
    % which can be obtained from RQ-VAE with codebooks~\cite{rajput2023recommender} and hierarchical clustering~\cite{si2024generative,hua2023index}. 
    To generate items, LLMs use beam search to generate the top-$K$ item identifiers. 
    Despite the effectiveness, token-sequence identifiers suffer from the 
    1) \textit{local optima} issue~\cite{zhou2005beam} in the beam search. 
    % As illustrated in Figure~\ref{fig:local_optima}, beam search prunes the identifiers with low-probability prefix at each generation step.
    As illustrated in Figure~\ref{fig:local_optima}, beam search greedily selects the sequence with top-$K$ probabilities at each generation step. 
    However, the initial tokens of the target identifier might not necessarily align with the user preference. 
    As such, the prefix of the target identifier has a low probability and will be pruned by beam search, causing inaccurate results. 
    2) \textit{Low generation efficiency} in the autoregressive generation, which requires multiple serial LLM calls, thereby causing unaffordable computing burdens~\cite{lin2024efficient} and severely hindering real-world deployments. 

    \item \textbf{\textit{Single-token identifier}} represents each item with a continuous token, \ie ID embedding or semantic embedding~\cite{liao2024llara,wang2024rethinking}. 
    % The user's historical interactions are then represented by a sequence of dense embeddings. 
    To recommend items, LLMs first generate the next item embedding, which is then grounded to the item IDs with a linear projection layer, as exemplified by E4SRec~\cite{li2023e4srec} and LITE-LLM4Rec~\cite{wang2024rethinking}. 
    However, using single embedding often yields suboptimal performance. 
    Precisely, ID embeddings rely on sufficient interactions to capture Collaborative Filtering (CF) information, thus being vulnerable to long-tailed users/items.  
    % and leading to poor generalization ability. 
    Conversely, semantic embedding overlooks the modeling of CF information that is essential for personalized recommendations. 
    % single embedding的问题主要是 要么只表达了cf，要么只表达了semantic
\end{itemize}




% p3. 提出一个我们要研究的关键问题：一个identifier应该怎么设计？objective:回答以上问题，我们展开问题去讲identifier应该怎么设计（提需求）
Facing the above issues, a fundamental question arises: 
% To achieve effective and efficient LLM-based recommendations, how do we design item identifiers? 
How can we design item identifiers to ensure effective and efficient LLM-based recommendations? 
% How do we design item identifiers for effective and efficient LLM-based recommendations? 
Based on the above insights, we posit two principles. 
1) \textbf{Integration of semantic and CF information}. 
Semantic information can harness rich knowledge in LLMs to strengthen the generalization ability (\eg cold-start recommendation). 
Meanwhile, CF information leverages user behaviors to enrich the semantic modeling of user preference, enabling effective recommendations for users and items with rich interactions. 
% 2) \textbf{Multi-token Identifier.} 
% Representing disentangled multi-dimensional item information (\eg semantic and collaborative) with a single token might be suboptimal due to potential conflicts as proven in~\cite{wang2024learnable,zhang2022re4} (see empirical results in Section~\ref{sec:exp_hyper_param}). 
% Therefore, a multi-token identifier is necessary to represent items and leverage multi-dimensional information effectively.  
% 3) \textbf{Order-agnostic Identifier.} 
% % 2) \textbf{Tokens within identifier should be order-agnostic}. 
% The key to mitigating the local optima issue lies in allowing the generation of all possible tokens within the target identifier at a single decoding step. 
% In other words, the decoding of the token within an identifier (\eg ``basketball'') should not rely on the success of the front tokens generation (\eg ``mini'').    
% Simultaneous decoding of an order-agnostic identifier bypasses the generation of fixed-ordered sequences, effectively preventing the pruning of target identifiers. 
% In addition, simultaneous decoding performs only a single LLM call, thereby significantly improving the efficiency. 
2) \textbf{Order-agnostic Identifier.} 
% 并且user history交互的items之间是有顺序transition，但是item features之间不一定是有序的，price和brand就不应该有顺序区别。并且有序容易引起local optima，我们期望order-agnostic identifier，能够满足xxx，可以多个无序tokens的并行生成，一方面 避免local optima，另一方面提高efficiency
Representing multi-dimensional item information (\eg semantic and CF information) with a single token might be suboptimal due to potential conflicts between different dimensions as proven in~\cite{wang2024learnable,zhang2022re4} (see empirical evidence in Section~\ref{sec:exp_hyper_param}). 
% Therefore, a multi-token identifier is necessary to represent items and leverage multi-dimensional information effectively.
Therefore, it is necessary to utilize a set of tokens to effectively represent items with multi-dimensional information.
% While the sequential modeling between items of user history is essential to infer user preference,
% Nevertheless, 
% dependencies between multi-dimensional information should be eliminated (\eg ``price'' and ``category'').
Nevertheless, 
multi-dimensional information is not necessarily dependent on each other (\eg ``price'' and ``category''). 
Moreover, ordered token sequences can risk the local optima issue. 
% Hence, it is essential to disregard token dependencies in identifiers, which further facilitates simultaneous token generation, thus significantly improving inference efficiency. 
Hence, it is beneficial to disregard token dependencies in identifiers, which further facilitates simultaneous token generation, thus significantly improving inference efficiency. 
% Additionally, ordered token sequence can risk local optima issue, making it essential to disregard token order in identifiers. 
% Because ordered token sequence can risk local optima issue, making it essential to disregard token order in identifiers. 
% Moreover, order-agnostic identifiers enable simultaneous token generation, significantly improving inference efficiency. 
% allowing all tokens to be generated in parallel at a single LLM step

% The key to mitigating the local optima issue lies in allowing the generation of all possible tokens within the target identifier at a single decoding step. 
% In other words, the decoding of the token within an identifier (\eg ``basketball'') should not rely on the success of the front tokens generation (\eg ``mini'').    
% Simultaneous decoding of an order-agnostic identifier bypasses the generation of fixed-ordered sequences, effectively preventing the pruning of target identifiers. 
% In addition, simultaneous decoding performs only a single LLM call, thereby significantly improving the efficiency. 



\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.02cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=0.75]{figures/intro.pdf}
\caption{Overview of SETRec. (a) Depiction of order-agnostic set identifiers representing items from multi-dimensional information. (b) SETRec emphasizes item sequential dependencies while removing token dependencies within items, which allows simultaneous generation to improve efficiency.}
\label{fig:intro}
\end{figure}

In this light, we introduce a novel paradigm of \textbf{\textit{set identifier}} for LLM-based generative recommendation.  
% In this light, we introduce a novel paradigm of \textbf{\textit{set identifier}} for LLM-based generative recommendation, as shown in Figure~\ref{fig:intro}. 
% To integrate semantic and CF information, it employs a set of CF and semantic tokens with multi-dimensional information. 
% As shown in  LLMs encode a sequence of set identifiers and generate the next set identifier simultaneously.  
% As shown in Figure~\ref{fig:intro}(b), it encodes item transitions in user history and simultaneously generates the next set identifier at a single LLM step. 
% that eliminates the unnecessary dependencies within identifiers and boosts the inference efficiency.  
% simultaneous 生成token set是比较难的
As shown in Figure~\ref{fig:intro}, it employs a set of order-agnostic tokens to represent each item with CF and semantic information. 
% Nevertheless, eliminating token dependencies is not trivial due to the following challenges: 
% Nonetheless, it is non-trivial to eliminate token dependencies due to the following challenges: 
Nonetheless, it is non-trivial to eliminate token dependencies due to the following challenges: 
% 多个disentangled的空间做一次并行生成，生成的时候就要对齐到多个不同的维度/空间，要让llm学会做这个事。
% 1. 要让prefilling的时候内部token是无序的
% 2. 并行生成的时候，独立到多个空间
 
\begin{itemize}[leftmargin=*]
    % \item For user history encoding, token orders within identifiers are naturally introduced in the flattened token sequence transformed from user history. 
    % LLMs should remove such implicit order information to preserve the order agnosticism of set identifiers.  
    \item For user history encoding, the transformed item sequence naturally introduces unnecessary token dependencies (\eg semantic tokens are dependent on the CF token), which might negatively affect user history encoding. 
    % LLMs should remove these unwanted dependencies for effective user history encoding.   
    % \item For item generation, since set identifiers contain tokens in diverse disentangled dimensions, it is crucial to guide LLMs to generate tokens aligning well with each information dimension respectively at a single generation step. 
    \item For the simultaneous generation of order-agnostic identifiers, 
    tokens are independently generated for each dimension (\eg CF). 
    This necessitates guidance on LLMs to generate tokens aligning well with each information dimension respectively. 
    % causal attention mask是需要改正的
    % 2. 是不同空间的组合成为item，并行生成，没有互相参考，组合很多，容易out-of-corpus ，不在现有item里
    \item 
    % Since the generated identifier is a combination of tokens generated independently from diverse dimensions, it is more susceptible to the out-of-corpus issue. 
    % Based on the tokens generated independently from diverse dimensions, 
    Since the tokens for different dimensions are generated independently, 
    the generated set identifier might be invalid items, which requires effective grounding to the existing items. 
    % todo: 这里可能需要加一个ablation
\end{itemize}

% \begin{itemize}[leftmargin=*]
%     % 同时生成多个token是不容易的。1）同时生成，如何确保每个维度之间不重复，不干扰 2）由于生成的时候可以有很多种组合，不一定能够对应到某一个item上，那要如何grounding？
%     \item It is non-trivial to decode all tokens simultaneously. Specifically, a naive independent decoding of each token could generate repetitive tokens based on the same user's interactions, thus causing suboptimal results. 
%     Hence, it is important to differentiate each feature dimension and guide the LLMs to generate tokens for each dimension accordingly. 
%     Moreover, independent decoding is more susceptible to out-of-corpus issue, making it difficult to perfectly match the generated token set with an existing token set. Therefore, how to effectively ground the generated identifier to the existing items is a crucial problem. 
%     % causal attention mask是需要改正的
%     \item From the perspective of efficiency, simultaneous decoding might cause duplicate computation for the same user's historical interactions, thus intensifying the demand for computational resources and lowering the decoding efficiency. 
%     % how to efficiently generate is also a 
%     % causal mask need no longer works. it implicitly introduce the dependency 
%     % todo: 这里可能需要加一个ablation
% \end{itemize}


% p5. 我们的方法是怎么做的（solution+exp结果）
% to this end，提出了set-based identifier for llm。
To this end, we propose SETRec, an effective implementation of the set identifier paradigm. 
% which aims to leverage dependencies from item to item for user encoding while suppressing the dependencies from token to token within each item for item decoding. 
% 这个方法利用了cf embedding和semantic embedding来构成一个identifier，使得相似交互相似identifier/相似sem相似identifier
% For item tokenization, SETRec assigns each item with a continuous token embedding set (\ie order-agnostic tokens) containing CF and semantic embeddings via CF and semantic tokenizers. 
\textit{To integrate semantic and CF information}, SETRec leverages CF and semantic tokenizers to assign each item with an order-agnostic token set containing CF and semantic embeddings. 
% It utilizes a pre-trained CF model to obtain the CF embeddings and an Auto-Encoder (AE) to extract the semantic embeddings. 
% in addition，为了不引入不必要的identifier内部的依赖，我们设计了一种query-based simultaneous decoding。
% For next-item generation, 
% 我们的方法：
% identifier的生成，
% 为了解决前面的两个难点，query-xxx decoding: 并行生成不同空间的token。 
% 1我们做了什么 要实现item内无序，在user encoding时候用attention mask解决，generation要无序生成，用query vector的技术，
% 2我们做了什么
% 3最后+grounding
% ***实验上面的优势要展示具体的细节-> 几个数据集，哪些setting，提升了多少，对比了什么。
% For next-item generation, 
% 1) to eliminate implicit order introduced in historical item identifiers, we propose a special sparse attention mask, which discards the visibility of other tokens within identifiers and retains access to previous items.  
\textit{To eliminate token dependencies}, 
1) for user history encoding, 
we propose a special sparse attention mask, which discards the visibility of other tokens within identifiers and retains access to previous identifiers. 
% to eliminate implicit order introduced in historical item identifiers
% 2) To achieve simultaneous token generation, 
2) For simultaneous token generation, 
we introduce a query-guided generation mechanism, which adopts learnable vectors to guide LLMs to generate the embedding for each specific information dimension. 
% 解决ground的问题
% lin200 (在介绍我们grounding方法时候) 把不同维度收集起来，extendable grounding head -> 限制grounding 空间
3) To ground the generated embedding set to existing items, SETRec collects embeddings from all items as grounding heads to obtain the item scores for ranking. 
% which aims to constrain the tokens into the subspace of the valid tokens for each dimension. 
% and utilizes each as an extendable grounding head for each feature dimension. 
% moreover，这样避免了autoregressive，可以大大提升efficiency。
% +实验的描述（节省了多少多少，提高了对少多少，尤其在cold上面提高了多少）此外我们还验证这个方法具备一定的scaling ability
% Moreover, we propose a special sparse attention mask to eliminate the token dependencies in historical item identifiers and reduce the duplicate computations of the LLMs for simultaneous decoding, thus improving generation efficiency. 
We instantiate SETRec on T5 and Qwen and evaluate it on four real-world datasets under various scenarios (\eg full ranking, warm- and cold-start ranking, and diverse item popularity groups) to demonstrate the effectiveness, efficiency, and generalization ability. 
Additionally, we evaluate SETRec on Qwen with different model sizes (\ie 1.5B, 3B, and 7B), exhibiting promising scalability on cold-start items as model size increases. 
% The code and datasets are available at~\url{https://anonymous.4open.science/r/SETRec}.

% % in addition，为了不引入不必要的identifier内部的依赖，我们设计了一种query-based simultaneous decoding。
% For next-item generation, 
% to achieve simultaneous token decoding, 
% we introduce a query-guided decoding mechanism for SETRec, which adopts learnable vectors to guide LLMs to generate the embedding for each specific feature dimension. 
% % 解决ground的问题
% To ground the generated token set to existing items, SETRec utilizes the token embeddings from all items as an extendable grounding head for each feature dimension. 
% % moreover，这样避免了autoregressive，可以大大提升efficiency。
% % +实验的描述（节省了多少多少，提高了对少多少，尤其在cold上面提高了多少）此外我们还验证这个方法具备一定的scaling ability
% Moreover, we propose a special sparse attention mask to eliminate the token dependencies in historical item identifiers and reduce the duplicate computations of the LLMs for simultaneous decoding, thus improving generation efficiency. 
% We instantiate SETRec on two representative generative language models (\ie T5 and Qwen). 
% Empirical results on four real-world datasets under various settings demonstrate the effectiveness of our proposed method. 
% Additionally, SETRec exhibits promising scalability. 
% The code and datasets are available at~\url{xxx}.

The main contributions of this work are summarized as follows:
% 1: 提出范式，怎么做的，哪里好（对应）
% 2. 为了实现，我们提出setrec - 能利用query-vector技术xxx->实现什么->解决challenge -
\begin{itemize}[leftmargin=*]
    % \item We reveal the key issues in existing item identifiers for LLM-based generative recommendation, abstracting two principles for identifier design, \ie tokens should be order-agnostic and should include both semantic and collaborative information. 
    \item We propose a novel set identifier paradigm for LLM-based generative recommendation, representing each item with a set of order-agnostic tokens integrating semantic and CF information. 
    \item 
    % To achieve the objectives, we introduce a set-based identifier (our method)
    We propose SETRec to implement the novel paradigm, which introduces a query-guided generation mechanism with a sparse attention mask to achieve simultaneous generation without token dependencies, significantly boosting inference efficiency. 
    % achieve simultaneous decoding,
    \item We instantiate SETRec on T5 and Qwen from 1.5B to 7B. Extensive experiments on four real-world datasets under various settings (\eg full ranking, warm- and cold-start ranking) validate its effectiveness, efficiency, generalization ability, and scalability. 
\end{itemize}