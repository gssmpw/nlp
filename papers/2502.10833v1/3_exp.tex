\section{Experiment}\label{sec:experiment}
We carry out extensive experiments on four real-world datasets to answer the following research questions: 
\begin{itemize}[leftmargin=*]
    % \item \textbf{RQ1:} How does our proposed DEALRec perform compared to the coreset selection baselines for LLM-based recommendation and the models trained with full data? 
    \item \textbf{RQ1:} How does our proposed SETRec perform compared to different identifier baselines on different architectures of LLMs? 
    % \item \textbf{RQ2:} How do the different components of DEALRec (\ie influence score, gap regularization, and stratified sampling) affect the performance, and is DEALRec generalizable to different surrogate models? 
    \item \textbf{RQ2:} How do the different components of SETRec (\ie CF embeddings, semantic embeddings, query vectors, and sparse attention) affect the performance?
    \item \textbf{RQ3:} How does SETRec perform when scaling up the model size and how does SETRec improve the overall performance? 
    \item \textbf{RQ4:} How does SETRec perform with different number of semantic embeddings, tokenizer training strength, and semantic strength for inference? 
\end{itemize}
\subsection{Experimental Settings}
\subsubsection{\textbf{Datasets}}
We conduct experiments on four real-world datasets across various domains. 
From Amazon review datasets\footnote{\url{https://jmcauley.ucsd.edu/data/amazon/}.}, we adopt three widely used benchmarks 
1)\textbf{Toys}, 2) \textbf{Beauty}, and 3) \textbf{Sports}. 
The three Amazon datasets contain rich user interactions over a specific category of e-commerce products, where each item is associated with rich textual meta information such as title, description, category, and brand. 
In addition, we use a video games dataset 4) \textbf{Steam}\footnote{\url{https://github.com/kang205/SASRec}.} proposed in~\cite{kang2018self}, which contains substantial user interactions on video games with abundant textual semantic information. 
For all datasets, we follow previous work~\cite{wang2023causal} to sort user interactions chronologically according to the timestamps and divide them into training, validation, and testing sets with a ratio of 8:1:1. 
In addition, we divide the items into warm and cold items\footnote{We denote warm- and cold-start items as warm and cold items for brevity.}, where the items that appear in the training set are warm items, otherwise cold items. 


\noindent$\bullet\quad$\textbf{Evaluation.} 
We adopt the widely used metrics Recall@$K$ and NDCG@$K$, where $K=5$ and $10$ to evaluate all methods. 
Additionally, 
we introduce three different settings that evaluate over 1) all items, 2) warm items only, and 3) cold items only, respectively.  
% todo: 这里数据集可能要解释一下xxx为了保证cold数量能多一点，切割的比例是xxx


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage[normalem]{ulem}
% \useunder{\uline}{\ul}{}
\begin{table*}[t]
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0.2cm}
\caption{Overall performance of baselines and SETRec instantiated on T5. The best results are in bold and the second-best results are underlined. $*$ implies the improvements over the second-best results are statistically significant ($p$-value < 0.01) under one-sample t-tests. ``Inf. Time'' denotes the inference time over all test users tested on a single NVIDIA RTX A5000 GPU.}
\setlength{\tabcolsep}{2mm}{
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|cccc|cccc|cccc|c}
\toprule
 &  & \multicolumn{4}{c|}{\textbf{All}} & \multicolumn{4}{c|}{\textbf{Warm}} & \multicolumn{4}{c|}{\textbf{Cold}} & \multicolumn{1}{l}{\textbf{Inf. Time (s)}} \\ \hline
\textbf{Dataset} & \textbf{Method} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{All Users} \\ \midrule
\multirow{9}{*}{\textbf{Toys}} & \textbf{DreamRec} & 0.0020 & 0.0027 & 0.0015 & 0.0018 & 0.0027 & 0.0039 & 0.0020 & 0.0024 & 0.0066 & 0.0168 & 0.0045 & 0.0082 & 912 \\
 & \textbf{E4SRec} & 0.0061 & 0.0098 & 0.0051 & 0.0064 & 0.0081 & 0.0128 & 0.0065 & 0.0082 & 0.0065 & 0.0122 & 0.0056 & 0.0078 & \textbf{55} \\ \cmidrule{2-15}
 & \textbf{BIGRec} & 0.0008 & 0.0013 & 0.0007 & 0.0009 & 0.0014 & 0.0019 & 0.0011 & 0.0013 & 0.0278 & 0.0360 & 0.0196 & 0.0223 & 2,079 \\
 & \textbf{IDGenRec} & 0.0063 & 0.0110 & 0.0052 & 0.0069 & 0.0109 & {\ul 0.0161} & 0.0081 & {0.0102} & {\ul 0.0318} & {\ul 0.0589} & {\ul 0.0236} & {\ul 0.0335} & 658 \\
 & \textbf{CID} & 0.0044 & 0.0082 & 0.0040 & 0.0053 & 0.0065 & 0.0128 & 0.0049 & 0.0071 & 0.0059 & 0.0111 & 0.0047 & 0.0066 & 810 \\
 & \textbf{SemID} & 0.0071 & 0.0108 & 0.0061 & 0.0074 & 0.0086 & 0.0153 & 0.0075 & 0.0100 & 0.0307 & 0.0507 & 0.0220 & 0.0292 & 1,215 \\
 & \textbf{TIGER} & 0.0064 & 0.0106 & 0.0060 & 0.0076 & 0.0091 & 0.0147 & 0.0080 & {\ul 0.0102} & 0.0315 & 0.0555 & 0.0228 & 0.0314 & 448 \\
 & \textbf{LETTER} & {\ul 0.0081} & {\ul 0.0117} & {\ul 0.0064} & {\ul 0.0077} & {\ul 0.0109} & 0.0155 & {\ul 0.0083} & 0.0101 & 0.0183 & 0.0395 & 0.0115 & 0.0190 & 448 \\  \cmidrule{2-15}
 & \cellcolor{gray!16}\textbf{SETRec} & \cellcolor{gray!16}\textbf{0.0110*} & \cellcolor{gray!16}\textbf{0.0189*} & \cellcolor{gray!16}\textbf{0.0089*} & \cellcolor{gray!16}\textbf{0.0118*} & \cellcolor{gray!16}\textbf{0.0139*} & \cellcolor{gray!16}\textbf{0.0236*} & \cellcolor{gray!16}\textbf{0.0112*} & \cellcolor{gray!16}\textbf{0.0147*} & \cellcolor{gray!16}\textbf{0.0443*} & \cellcolor{gray!16}\textbf{0.0812*} & \cellcolor{gray!16}\textbf{0.0310*} & \cellcolor{gray!16}\textbf{0.0445*} & \cellcolor{gray!16}{\ul 60} \\ \midrule\midrule
\multirow{9}{*}{\textbf{Beauty}} & \textbf{DreamRec} & 0.0012 & 0.0025 & 0.0013 & 0.0017 & 0.0016 & 0.0028 & 0.0016 & 0.0019 & 0.0078 & 0.0161 & 0.0065 & 0.0094 & 1,102 \\
 & \textbf{E4SRec} & 0.0061 & 0.0092 & 0.0052 & 0.0063 & 0.0080 & 0.0121 & 0.0067 & 0.0082 & 0.0072 & 0.0118 & 0.0065 & 0.0077 & \textbf{120} \\ \cmidrule{2-15}
 & \textbf{BIGRec} & 0.0054 & 0.0064 & 0.0051 & 0.0054 & 0.0008 & 0.0009 & 0.0006 & 0.0008 & 0.0106 & 0.0251 & 0.0095 & 0.0151 & 4,544 \\
 & \textbf{IDGenRec} & {\ul 0.0080} & 0.0115 & {\ul 0.0066} & {0.0078} & {\ul 0.0106} & 0.0165 & 0.0078 & 0.0099 & 0.0187 & 0.0350 & 0.0186 & 0.0224 & 840 \\
 & \textbf{CID} & 0.0071 & 0.0125 & 0.0060 & {\ul 0.0080} & 0.0098 & {0.0166} & 0.0077 & 0.0101 & 0.0087 & 0.0183 & 0.0071 & 0.0104 & 815 \\
 & \textbf{SemID} & 0.0071 & {\ul 0.0131} & 0.0056 & {0.0078} & 0.0098 & {\ul 0.0174} & 0.0074 & {\ul 0.0103} & {\ul 0.0260} & {\ul 0.0465} & 0.0178 & 0.0255 & 1,310 \\
 & \textbf{TIGER} & 0.0063 & 0.0098 & 0.0050 & 0.0062 & 0.0086 & 0.0131 & 0.0065 & 0.0082 & 0.0190 & 0.0325 & 0.0130 & 0.0178 & 430 \\ 
 & \textbf{LETTER} & 0.0071 & 0.0103 & 0.0061 & 0.0070 & 0.0094 & 0.0135 & {\ul 0.0079} & 0.0091 & 0.0251 & 0.0410 & {\ul 0.0241} & {\ul 0.0285} & 430 \\ \cmidrule{2-15}
 & \cellcolor{gray!16}\textbf{SETRec} & \cellcolor{gray!16}\textbf{0.0106*} & \cellcolor{gray!16}\textbf{0.0161*} & \cellcolor{gray!16}\textbf{0.0083*} & \cellcolor{gray!16}\textbf{0.0103*} & \cellcolor{gray!16}\textbf{0.0139*} & \cellcolor{gray!16}\textbf{0.0212*} & \cellcolor{gray!16}\textbf{0.0108*} & \cellcolor{gray!16}\textbf{0.0134*} & \cellcolor{gray!16}\textbf{0.0384*} & \cellcolor{gray!16}\textbf{0.0761*} & \cellcolor{gray!16}\textbf{0.0280*} & \cellcolor{gray!16}\textbf{0.0413*} & \cellcolor{gray!16}{\ul 126} \\ \midrule\midrule
\multirow{9}{*}{\textbf{Sports}} & \textbf{DreamRec} & 0.0027 & 0.0044 & 0.0025 & 0.0031 & 0.0032 & 0.0052 & 0.0028 & 0.0035 & 0.0045 & 0.0108 & 0.0026 & 0.0049 & 2,100 \\ 
 & \textbf{E4SRec} & 0.0079 & 0.0131 & 0.0075 & 0.0094 & 0.0092 & 0.0154 & 0.0085 & 0.0107 & 0.0031 & 0.0093 & 0.0019 & 0.0039 & \textbf{117} \\ \cmidrule{2-15}
 & \textbf{BIGRec} & 0.0033 & 0.0042 & 0.0030 & 0.0033 & 0.0001 & 0.0002 & 0.0001 & 0.0001 & 0.0059 & 0.0104 & 0.0043 & 0.0061 & 7,822 \\
 & \textbf{IDGenRec} & 0.0087 & 0.0127 & 0.0079 & 0.0092 & 0.0101 & 0.0149 & 0.0091 & 0.0107 & 0.0181 & 0.0302 & 0.0134 & 0.0179 & 1,724 \\
 & \textbf{CID} & 0.0077 & 0.0131 & 0.0073 & 0.0092 & 0.0074 & 0.0119 & 0.0045 & 0.0061 & 0.0082 & 0.0149 & 0.0075 & 0.0099 & 2,135 \\
 & \textbf{SemID} & {\ul 0.0094} & {\ul 0.0167} & {\ul 0.0088} & {\ul 0.0114} & {\ul 0.0119} & {\ul 0.0201} & {\ul 0.0104} & {\ul 0.0135} & {\ul 0.0254} & {\ul 0.0495} & {\ul 0.0175} & {\ul 0.0256} & 2,367 \\
 & \textbf{TIGER} & 0.0085 & 0.0129 & 0.0080 & 0.0095 & 0.0100 & 0.0151 & 0.0091 & 0.0109 & 0.0190 & 0.0310 & 0.0120 & 0.0159 & 481 \\
 & \textbf{LETTER} & 0.0077 & 0.0131 & 0.0073 & 0.0092 & 0.0074 & 0.0119 & 0.0045 & 0.0061 & 0.0082 & 0.0149 & 0.0075 & 0.0099 & 481 \\ \cmidrule{2-15}
 & \cellcolor{gray!16}\textbf{SETRec} & \cellcolor{gray!16}\textbf{0.0114*} & \cellcolor{gray!16}\textbf{0.0185*} & \cellcolor{gray!16}\textbf{0.0101*} & \cellcolor{gray!16}\textbf{0.0126*} & \cellcolor{gray!16}\textbf{0.0134*} & \cellcolor{gray!16}\textbf{0.0216*} & \cellcolor{gray!16}\textbf{0.0115*} & \cellcolor{gray!16}\textbf{0.0144*} & \cellcolor{gray!16}\textbf{0.0341*} & \cellcolor{gray!16}\textbf{0.0595*} & \cellcolor{gray!16}\textbf{0.0233*} & \cellcolor{gray!16}\textbf{0.0323*} & \cellcolor{gray!16}{\ul 136} \\ \midrule\midrule
\multirow{9}{*}{\textbf{Steam}} & \textbf{DreamRec} & 0.0029 & 0.0057 & 0.0037 & 0.0046 & 0.0042 & 0.0080 & 0.0045 & 0.0059 & 0.0017 & 0.0029 & 0.0013 & 0.0018 & 4,620 \\
 & \textbf{E4SRec} & 0.0194 & 0.0351 & 0.0220 & 0.0270 & 0.0312 & 0.0558 & 0.0283 & 0.0370 & 0.0006 & 0.0010 & 0.0006 & 0.0007 & \textbf{328} \\ \cmidrule{2-15}
 & \textbf{BIGRec} & 0.0030 & 0.0049 & 0.0046 & 0.0049 & 0.0048 & 0.0053 & 0.0061 & 0.0053 & 0.0099 & 0.0107 & {\ul 0.0129} & 0.0127 & 5,167 \\
 & \textbf{IDGenRec} & 0.0199 & 0.0307 & 0.0241 & 0.0265 & 0.0309 & 0.0479 & 0.0311 & 0.0363 & 0.0047 & 0.0151 & 0.0039 & 0.0078 & 2,846 \\
 & \textbf{CID} & 0.0200 & {\ul 0.0360} & {\ul 0.0249} & {\ul 0.0295} & 0.0314 & {\ul 0.0566} & {\ul 0.0315} & {\ul 0.0400} & 0.0008 & 0.0021 & 0.0006 & 0.0011 & 3,194 \\
 & \textbf{SemID} & 0.0155 & 0.0278 & 0.0192 & 0.0229 & 0.0248 & 0.0443 & 0.0246 & 0.0313 & 0.0017 & 0.0027 & 0.0015 & 0.0018 & 3,605 \\
 & \textbf{TIGER} & {\ul 0.0202} & 0.0348 & 0.0244 & 0.0287 & {\ul 0.0320} & 0.0552 & 0.0314 & 0.0393 & 0.0060 & {0.0152} & 0.0044 & 0.0078 & 1,747 \\
 & \textbf{LETTER} & 0.0164 & 0.0312 & 0.0195 & 0.0244 & 0.0268 & 0.0500 & 0.0253 & 0.0336 & {\ul 0.0115} & {\ul 0.0317} & {0.0077} & {\ul 0.0157} & 1,747 \\ \cmidrule{2-15}
 & \cellcolor{gray!16}\textbf{SETRec} & \cellcolor{gray!16}\textbf{0.0216*} & \cellcolor{gray!16}\textbf{0.0383*} & \cellcolor{gray!16}\textbf{0.0254*} & \cellcolor{gray!16}\textbf{0.0308*} & \cellcolor{gray!16}\textbf{0.0339*} & \cellcolor{gray!16}\textbf{0.0591*} & \cellcolor{gray!16}\textbf{0.0326*} & \cellcolor{gray!16}\textbf{0.0414*} & \cellcolor{gray!16}\textbf{0.0313*} & \cellcolor{gray!16}\textbf{0.0572*} & \cellcolor{gray!16}\textbf{0.0248*} & \cellcolor{gray!16}\textbf{0.0342*} & \cellcolor{gray!16}{\ul 347} \\ \hline
\end{tabular}
}}
\label{tab:overall_performance}
\end{table*}


\subsubsection{\textbf{Baselines}}
We compare SETRec with competitive baselines, including single-token identifiers (DreamRec, E4SRec) and token-sequence identifiers (BIGRec, IDGenRec, CID, SemID, TIGER, LETTER). 
1) \textbf{DreamRec}~\cite{yang2024generate} is a closely related method that leverages ID embedding to represent each item and adopts a diffusion model to refine the generated ID embedding from LLMs.  
2) \textbf{E4SRec}~\cite{li2023e4srec} utilizes a pre-trained CF model to obtain ID embedding, and uses a linear projection layer to obtain the item scores efficiently. 
3) \textbf{BIGRec}~\cite{bao2023bi} adopts item titles as identifiers, where the tokens are from human vocabulary. 
4) \textbf{IDGenRec}~\cite{tan2024idgenrec} is a learnable ID generator, which aims to generate concise but informative tags from human vocabulary to represent each item. 
5) \textbf{CID}~\cite{hua2023index} leverages hierarchical clustering to obtain token sequence, which utilizes item co-occurrence matrix to obtain identifiers to ensure items with similar interactions share similar tokens. 
6) \textbf{SemID}~\cite{hua2023index} also represents items with external token sequence, which is obtained based on the hierarchical item category. 
7) \textbf{TIGER}~\cite{rajput2023recommender} leverages RQ-VAE with codebooks to quantize item semantic information into token sequence with external tokens. The identifier sequentially contains coarse-grained to fine-grained information. 
8) \textbf{LETTER}~\cite{wang2024learnable} is one of the SOTA item tokenization methods, which incorporates both semantic and CF information into the training of RQ-VAE, achieving identifiers with multi-dimensional information and improved diversity. 

\subsubsection{\textbf{Implementation Details}} 
% 我们把所有的identifier方法都instantiate到了两个不同的LLMs上，T5-small 和 Qwen上，其中我们用1.5B来测试overall performance，然后还扩展到3B和7B上去验证scalability。
% 针对tokenizer的训练，对于使用到AE的方法（TIGER, LETTER，还有我们的方法），我们统一了隐藏层在"512,256,128". 
% 对于LLM的训练，我们为所有方法设置一样的prompt as "xxx"
% 对于T5-small模型，我们是全量微调。对于Qwen模型，我们采用parameter-efficeint tuning technique LoRA~\cite{}. 并且所有实验在4块A5000上跑。
% 针对我们的方法，N的数量在{1,2,3,4,5,6}里面选，alpha在0.1,0.3,0.5,0.7,0.9里选。而inference阶段的beta则是从0-1选。
We instantiate all methods on two LLMs with different architectures, \ie T5-small~\cite{raffel2020exploring} (encoder-decoder) and Qwen2.5~\cite{yang2024qwen2} (decoder-only). 
Specifically, we adopt Qwen\footnote{We denote T5-small and Qwen2.5 as T5 and Qwen for brevity.} with different sizes, including 1.5B, 3B, and 7B, for a comprehensive evaluation. 
To ensure a fair comparison, we set the hidden layer dimensions at 512, 256, and 128 with ReLU activation for methods that adopt AE in tokenizer training, including TIGER, LETTER, and our proposed SETRec. 
For LLM training, 
we adopt the same prompt for all methods as ``What would the user be likely to purchase next after buying items {history}?;'' for a fair comparison. 
We fully fine-tune the T5 model and perform parameter-efficient fine-tuning technique LoRA~\cite{hu2021lora} for Qwen. 
All experiments are conducted on four NVIDIA RTX A5000 GPUs. 
% For SETRec, 
% we use SASRec~\cite{kang2018self} as pre-trained CF model, and utilize 
% SentenceT5 and Qwen as semantic extractors for T5 and Qwen backend LLMs, respectively. 
For SETRec, 
we select $N$, $\alpha$, and $\beta$ from $\{1,2,3,4,5,6\}$, $\{0.1,0.3,0.5,0.7,0.9\}$, and $\{0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,1.0\}$, respectively. 


\begin{table*}[t]
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0.2cm}
\caption{Overall performance on Qwen-1.5B over Toys and Beauty. The best results are in bold and the second-best results are underlined. ``Inf. Time'' denotes the inference time over all test users tested on a single NVIDIA RTX A5000 GPU.}
\setlength{\tabcolsep}{2mm}{
\resizebox{\textwidth}{!}{
\begin{tabular}{l|l|cccc|cccc|cccc|c}
\toprule
 &  & \multicolumn{4}{c}{\textbf{All}} & \multicolumn{4}{c}{\textbf{Warm}} & \multicolumn{4}{c}{\textbf{Cold}} & \textbf{Inf. Time(s)} \\ \hline
\textbf{Dataset} & \textbf{Method} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \textbf{N@10} & \textbf{All Users} \\ \midrule
\multirow{9}{*}{\textbf{Toys}} & \textbf{DreamRec} & 0.0006 & 0.0013 & 0.0005 & 0.0008 & 0.0008 & 0.0019 & 0.0007 & 0.0012 & 0.0076 & 0.0137 & 0.0052 & 0.0074 & 1,093 \\
 & \textbf{E4SRec} & 0.0065 & 0.0108 & {\ul 0.0056} & 0.0072 & 0.0089 & 0.0144 & {\ul 0.0075} & {\ul 0.0096} & 0.0084 & 0.0235 & 0.0055 & 0.0111 & \textbf{905} \\ \cmidrule{2-15} 
 & \textbf{BIGRec} & 0.0009 & 0.0016 & 0.0009 & 0.0012 & 0.0011 & 0.0013 & 0.0010 & 0.0011 & 0.0194 & 0.0311 & 0.0147 & 0.0191 & 43,304 \\
 & \textbf{IDGenRec} & 0.0030 & 0.0053 & 0.0022 & 0.0031 & 0.0043 & 0.0086 & 0.0032 & 0.0048 & 0.0189 & 0.0364 & 0.0161 & 0.0224 & 30,720 \\
 & \textbf{CID} & 0.0027 & 0.0047 & 0.0025 & 0.0033 & 0.0055 & 0.0084 & 0.0044 & 0.0056 & 0.0055 & 0.0156 & 0.0044 & 0.0081 & {27,248} \\
 & \textbf{SemID} & 0.0024 & 0.0042 & 0.0018 & 0.0024 & 0.0034 & 0.0055 & 0.0026 & 0.0034 & 0.0140 & 0.0275 & 0.0095 & 0.0143 & 32,288 \\
 & \textbf{TIGER} & {\ul 0.0068} & {\ul 0.0117} & 0.0054 & {\ul 0.0072} & {\ul 0.0094} & {\ul 0.0159} & 0.0070 & 0.0095 & {\ul 0.0384} & {\ul 0.0715} & {\ul 0.0291} & {\ul 0.0408} & {13,800} \\
 & \textbf{LETTER} & 0.0057 & 0.0093 & 0.0050 & 0.0064 & 0.0080 & 0.0126 & 0.0066 & 0.0085 & 0.0217 & 0.0416 & 0.0170 & 0.0239 & 13,800 \\ \cmidrule{2-15} 
 & \cellcolor[HTML]{ECF4FF}\textbf{SETRec} & \cellcolor[HTML]{ECF4FF}\textbf{0.0116*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0188*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0095*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0120*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0144*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0236*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0118*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0151*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0531*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0883*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0382*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0507*} & \cellcolor[HTML]{ECF4FF}{\ul 926} \\ \midrule\midrule
\multirow{9}{*}{\textbf{Beauty}} & \textbf{DreamRec} & 0.0007 & 0.0009 & 0.0005 & 0.0005 & 0.0010 & 0.0011 & 0.0007 & 0.0007 & 0.0090 & 0.0167 & 0.0075 & 0.0103 & 1,326 \\
 & \textbf{E4SRec} & {\ul 0.0067} & {\ul 0.0109} & {\ul 0.0056} & {\ul 0.0072} & {\ul 0.0088} & {\ul 0.0146} & {\ul 0.0072} & {\ul 0.0094} & 0.0017 & 0.0071 & 0.0010 & 0.0029 & \textbf{910} \\ \cmidrule{2-15} 
 & \textbf{BIGRec} & 0.0006 & 0.0010 & 0.0006 & 0.0007 & 0.0010 & 0.0010 & 0.0008 & 0.0008 & 0.0141 & 0.0246 & 0.0094 & 0.0135 & 29,500 \\
 & \textbf{IDGenRec}  & 0.0042 & 0.0078 & 0.0030 & 0.0043 & 0.0045 & 0.0104 & 0.0033 & 0.0054 & {\ul 0.0254} & {\ul 0.0471} & {\ul 0.0207} & {\ul 0.0292} & 35,040 \\
 & \textbf{CID} & 0.0046 & 0.0077 & 0.0040 & 0.0052 & 0.0059 & 0.0107 & 0.0051 & 0.0068 & 0.0075 & 0.0155 & 0.0071 & 0.0096 & {27,792} \\
 & \textbf{SemID} & 0.0030 & 0.0045 & 0.0027 & 0.0033 & 0.0050 & 0.0076 & 0.0042 & 0.0052 & 0.0159 & 0.0227 & 0.0116 & 0.0159 & 45,160 \\
 & \textbf{TIGER} & 0.0041 & 0.0065 & 0.0032 & 0.0041 & 0.0054 & 0.0085 & 0.0042 & 0.0054 & 0.0083 & 0.0167 & 0.0064 & 0.0091 & {12,600} \\
 & \textbf{LETTER} & 0.0040 & 0.0069 & 0.0031 & 0.0042 & 0.0051 & 0.0088 & 0.0039 & 0.0054 & 0.0043 & 0.0129 & 0.0043 & 0.0071 & 12,600 \\ \cmidrule{2-15} 
 & \cellcolor[HTML]{ECF4FF}\textbf{SETRec} & \cellcolor[HTML]{ECF4FF}\textbf{0.0104*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0167*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0085*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0108*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0140*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0221*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0109*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0141*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0477*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0748*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0370*} & \cellcolor[HTML]{ECF4FF}\textbf{0.0464*} & \cellcolor[HTML]{ECF4FF}{\ul 1,050} \\ \bottomrule
\end{tabular}
}}
\label{tab:Overall_performance_on_Qwen}
\end{table*}



\subsection{Overall Performance (RQ1)}\label{sec:overall_performance}


\subsubsection{\textbf{Performance on T5.}} 
The performance comparison between baselines and SETRec instantiated on T5 are shown in Table~\ref{tab:overall_performance}, from which we have the following observations: 
\begin{itemize}[leftmargin=*]
    % 1. token-seq-based 整体会比单一的embedding表示好。这是因为他们利用了多token来表示丰富的item信息。针对用human vocab来表示的方法，他们能够利用上语言模型内部的pre-training知识；针对那些词表的方法，他们将信息压缩到了多个token里，让item的表示更加具有层次化。
    \item Token-sequence identifier (BIGRec, IDGenRec, CID, SemID, TIGER, LETTER) generally performs better than single-token identifier under ``all'', ``warm'', and ``cold'' settings. This is reasonable because token-sequence identifier represent each item with multiple tokens, which explicitly encode rich item information into different dimensions.   
    % 2. token-seq-based中，用codebook的比用human vocab的大部分情况要好一些。这主要是因为他们利用了hierarchy的信息，从粗粒度到细粒度，一定程度上缓解了local optima的问题。
    \item Among the token-sequence identifiers, methods with external tokens (CID, SemID, TIGER, LETTER) generally outperform those relying on human vocabulary (\eg BIGRec) under ``all'' and ``warm'' settings. 
    This is attributed to their hierarchically structured identifier, where the initial tokens represent coarse-grained semantics while subsequent tokens contain fine-grained semantics. 
    This aligns better with the autoregressive generation process, potentially alleviating the local optima issue~\cite{wang2024learnable}. 
    % 3. 分析一下在cold场景下哪些更好：对于只用cf的方法（dreamrec, e4srec, cid)，他们在cold上面效果不行。而那些利用了寓意信息的方法，在cold上表现就比较优秀。但是codebook的大多数情况仍然不如human vocab的那些方法。
    \item When recommending cold items\footnote{The higher values on cold performance are due to the limited number of cold items.}, methods that merely utilize CF information (DreamRec, E4SRec, and CID) fail to give satisfying results. 
    This is not surprising since CF information depends heavily on substantial interactions for training, thereby struggling with cold items. 
    In contrast, methods that integrate semantics into identifiers (BIGRec, IDGenRec, SemID, TIGER, and LETTER) generalize better on cold-start scenarios (superior performance under ``cold'' setting). 
    Specifically, BIGRec and IDGenRec tend to have competitive performance. 
    This is reasonable because they utilize readable human vocabulary to represent each item, which better leverages rich world knowledge encoded in LLMs. 

    % 4. 我们的方法significantly/constantly超过了其他方法。在accuracy上，我们在all，warm，和cold上都显著超越。我们利用了cf的信息，让那些拥有丰富交互的warm item能够被准确的推荐；此外我们利用了多维度的semantic信息，这让我们的模型能够泛化到cold item上面去。
    \item SETRec significantly outperform all baselines under ``all'', ``warm'', and ``cold'' settings across all four datasets. 
    The superior performance is attributed to 
    % 1) the incorporation of both CF and semantic information, which ensures the items with similar interactions have similar identifiers, thus recommending warm items accurately; 
    % 2) representation of rich semantics into multiple embeddings, which encourages the identifier to contain semantics of different dimensions, thus strengthening the cold-start generalization. 
    1) the incorporation of both CF and semantic information into a set of tokens, which ensures accurate warm item recommendation and strong generalization on cold items; 
    2) order agnosticism of identifier, which removes the possibly inaccurate dependencies across different tokens associated with an identifier. 
    
    \item From the perspective of efficiency, SETRec significantly reduces the inference time costs compared to the token-sequence identifiers. 
    SETRec achieves an average 15$\times$, 11$\times$, 18$\times$, and 8$\times$ speedup on Toys, Beauty, Sports, and Steam, respectively, compared to token-sequence identifiers. 
    The high efficiency is attributed to the simultaneous generation, which generates multiple tokens at a single LLM call, unlocking the real-world deployment of LLM-based generative recommendation. 
    % from perspective of efficiency, 我们的方法显著的超越了seq-based的这些方法，实现用一个single step就能够生成
\end{itemize}


% Overall performance on Qwen
% Please add the following required packages to your document preamble:
% \usepackage{multirow}

% \noindent$\bullet\quad$\textbf{Performance on Qwen-1.5B.} 




\subsubsection{\textbf{Performance on Qwen-1.5B}}
To evaluate SETRec on decoder-only LLMs, we instantiate SETRec and all baselines on Qwen-1.5B. We present the results on Toys and Beauty\footnote{We omit the results with similar observations on other datasets to save space.} in Table~\ref{tab:Overall_performance_on_Qwen}, from which we summarize several key different observations from performance on T5 as follows: 
% observations

\begin{itemize}[leftmargin=*]
    % 1. 在qwen上和t5不同的地方是，seq-based失去了它显著的效果，我们猜测这主要是因为qwen的参数量更大，他拥有更强的预训练知识。因此难以在数据有限的情况下很快的adpat到推荐任务上。相反的，E4SRec大部分情况能有非常competitive 的performance。我们猜测这主要是因为它把之前的vocabulary head换成了新的logits，这样利于大语言模型从原来的pre-training任务上adapt到推荐任务上，通过后面那个head高效调整。  
    \item Token-sequence identifiers show limited competitiveness compared to the counterparts on T5. 
    % We suspect that this might be caused by the magnified knowledge gap between the pre-training data and the recommendation data. 
    A possible reason is that Qwen-1.5B probably contains richer knowledge within its parameters, which amplifies the knowledge gap between the pre-training and recommendation tasks,  thereby hindering its adaptation to recommendation tasks with limited interaction data.  
    Conversely, E4SRec yields competitive performance in most cases. 
    This makes sense because E4SRec removes the original vocabulary head and replaces it with an item projection head, thus facilitating effective adaption to the recommendation tasks. 
    % 2. 和t5相比，这些用human vocab的在cold上面会有比较好的performance。-> 这个符合直觉。但是词表这种反而下降了，这个也符合直觉，因为需要更多的interaction来adapt，否则生成概率会偏低
    \item BIGRec and IDGenRec outperform their T5 counterparts on cold items on Beauty. Because they represent items with human vocabulary, which can leverage the rich world knowledge within Qwen-1.5B for better generalization. 
    On the contrary, identifiers with external tokens have inferior cold performance compared to their T5 counterparts. 
    This is also reasonable since it requires extensive interaction data to train external tokens. Otherwise, it is difficult for it to generalize to cold items accurately due to the low generation probability of these external tokens. 
    % 3. 我们的方法仍然能稳定的超过baseline,. 并且稳定的比t5要好。尤其是cold上面的performance。-> 在qwen上比较好的表现验证了我们方法在不同模型架构上的泛化能力。 
    \item SETRec constantly outperforms baselines, which is consistent with the observations on T5. 
    Notably, SETRec instantiated on Qwen-1.5B steadily surpasses SETRec on T5, especially under the ``cold'' setting. 
    This validates the strong generalization ability of SETRec on different architectures of LLMs. 
    Moreover, as the LLM size increases, the efficiency improvements over the token-sequence identifiers are more significant, resulting in an average of 20$\times$ speedup across the two datasets. 
    
\end{itemize}




\subsection{In-depth Analysis}

\subsubsection{\textbf{Ablation Study (RQ2)}} 
To study the effectiveness of each component of SETRec, we separately remove semantic tokens (``w/o Sem''), 
CF token (``w/o CF'').  
In addition, we replace learnable query vectors with random frozen vectors (``w/o Query'') and 
use the original attention mask (``w/o SA''), to evaluate the effect of query vectors and the sparse attention mask, respectively. 
The results of different ablation variants on T5 and Qwen-1.5B on Toys are presented in Figure~\ref{fig:ablation} and we omit the results on other datasets with similar observations to save space. 

From the figures, we can find similar observations on T5 and Qwen that 
% t5和qwen都有的现象：
% 1. 单独移除每一个元素在all,warm, cold 上performance都下降了。这验证了每个component的有效性。
1) removing each component causes performance drops under ``all'', ``warm'', and ``cold'' settings, which validates the effectiveness of each component of SETRec. 
% 2. 一处semantic对cold的影响非常大。这也说明了semantic对于cold start item的重要性。验证了引入semantic是必要的。
2) Discarding semantic tokens drastically degrades the recommendation accuracy under ``cold'' settings. 
This demonstrates the necessity of incorporating semantics into identifiers. 
% 3. 相比于移除cf embedding，移除semantic反而会让performance下降更多。这个interesting现象我们猜测是源于我们使用了多个embedding。这个现象也和XXX里的观测一致。我们补充了只有一个semantic的实验结果在appendix）
Interestingly, 
3) removing semantic tokens leads to worse performance compared to removing CF token. 
The possible reason for this is the utilization of multiple semantic tokens to represent each item, which highlights the significance of leveraging multi-dimensional semantic information. 
This observation is also consistent with the results in~\cite{lin2024bridging}. 
% t5和qwen不一样的现象：主要是在cold上，去掉cf有时候反而有更好的cold start performance。这个可能的原因是参数量大的模型能比参数量小的模型拥有更好的语义理解。更detialed analysis of the balance between cf and semantics are provided in XXX
Nonetheless, 
4) while removing CF tokens for T5 leads to inferior performance on cold items, using CF tokens for Qwen might negatively impact on cold items. 
A possible reason is that the larger-size Qwen is better at understanding semantics due to its stronger knowledge base encoded in the parameters, making the contribution of CF less significant. 



% % ablation figures on Toys
% \begin{figure}[t]
% % \vspace{-0.2cm}
% \setlength{\abovecaptionskip}{-0.15cm}
% \setlength{\belowcaptionskip}{-0cm}
%   \centering 
%   % \hspace{-0.7in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-t5-all.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-qwen-all.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-t5-warm.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-qwen-warm.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-t5-cold.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.35in]{figures/ablation-toys-qwen-cold.pdf}} 
%   % \hspace{-0.105in}
% \caption{Ablation study on Toys.}
%   \label{fig:ablation}
%   % \vspace{-0.3cm}
% \end{figure}



\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.02cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=1.2]{figures/ablation.pdf}
\caption{Ablation study on Toys.}
\label{fig:ablation}
\end{figure}

\subsubsection{\textbf{Item Group Analysis (RQ3)}}
To understand how SETRec improves performance, we evaluate it over items with different popularity. 
% item group是怎么划分的 - 我们根据item popularity 排序，然后分成5组到Group1-group5, （从最popular到最不popular）
We divide the items into 5 groups according to their frequencies and test the models over each group respectively. 
The performance comparison between SETRec and two competitive baselines from token-sequence identifiers (LETTER) and single-token identifiers (E4SRec) are reported in Figure~\ref{fig:group_analysis}. 
We can observe that 
% 1. 从most popular 到least popular, item group performance 是在逐渐下降的，这符合预期。因为交互越少的item，llm能够decode出来的概率就会更低，欠拟合
1) the performance gradually drops from G1 to G5. 
This makes sense since the less popular items have fewer interactions for LLMs to learn, thus leading to worse generation probabilities. 
% 2. e4srec在第一组比letter要强很多，但是随着item的popularity降低，letter慢慢超过e4srec。这也符合我们的直觉。e4srec是纯靠cf信息的，非常依赖于大量的交互来学习cf info。而letter同时利用了语义信息，会在sparse的item上面有更好的表现
Besides, 
2) E4SRec outperforms LETTER on most popular items (G1) but usually yields inferior performance on unpopular items (G2-G5). 
This is due to that E4SRec only uses CF information, which relies on substantial interactions and therefore struggle on unpopular items. 
In contrast, LETTER additionally incorporates semantics into identifiers, thus achieving better generalization on sparse items. 
% 3. 每一组里面我们的方法都稳定的超过了competitive baselines。除此之外提升的百分比是在unpopular的item上面有更强的优势。这也部分说明了我们方法的泛化能力很强。
3) SETRec consistently excels both E4SRec and LETTER over all groups. 
Notably, the improvements over sparse items are more significant, which partially explains the superiority of SETRec regarding overall performance.  



% group analysis figure
\begin{figure}[t]
\vspace{-0.2cm}
\setlength{\abovecaptionskip}{-0.15cm}
\setlength{\belowcaptionskip}{-0cm}
  \centering 
  % \hspace{-0.7in}
  \subfigure{
    \includegraphics[height=1.65in]{figures/group_analysis_R10.pdf}} 
  % \hspace{-0.105in}
  \subfigure{
    \includegraphics[height=1.65in]{figures/group_analysis_N10.pdf}} 
\caption{Performance of SETRec, LETTER, and E4SRec (T5) on item groups with different popularity on Toys.}
  \label{fig:group_analysis}
  % \vspace{-0.3cm}
\end{figure}

\subsubsection{\textbf{Scalability on Model Parameters (RQ3)}}
To investigate whether SETRec can bring continuous performance when expanding the model parameters, we test SETRec on Qwen with different model sizes (1.5B, 3B, and 7B). 
Performance comparisons between SETRec, E4SRec, and LETTER on Toys are shown in Table~\ref{tab:scaling_performance}. 
% and the results on other datasets with similar observations are omitted to save space.
From the results, we can find that 
% 1. SETRec在cold上有比较明显的scaling，这因为模型对语义理解的能力更强。这展现了在cold start上比较promising的scaling的能力
1) SETRec clearly shows continued improvements over cold-start items when the model size scales from 1.5B to 7B, demonstrating promising scalability on cold items. 
We attribute this to the continued improvements of better semantic understanding by expanding the model parameters. 
% 2. SETRec在warm上可能已经到达瓶颈了，随着参数量的提升，对cf信息的接受没有进一步的提升。这个在e4srec的结果上也可以看得出来
Nonetheless, 
2) the performance on the warm items fails to continuously improve, indicating a relatively limited scalability over warm items. 
This shows that the larger models do not necessarily lead to better CF information understanding, which can also be indicated by the limited improvements of E4SRec under ``warm'' setting. 
% 3. 对于LETTER这种利用语意的identifier方法，也面临瓶颈。主要是因为扩展词表，其实和模型内部的语义没有很好的align，继续scale模型对于cold的提升其实作用并不明显
Besides, 
3) LETTER shows weak scalability over the three settings. 
This is mainly due to the utilization of external tokens, which do not necessarily align with the pre-trained knowledge in LLMs, thus showing limited improvements by expanding the model parameters. 


% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \begin{table*}[t]
% \setlength{\abovecaptionskip}{0.05cm}
% \setlength{\belowcaptionskip}{0.2cm}
% \caption{Performance comparison between SETRec and competitive baselines with different LLM sizes on Qwen. }
% \setlength{\tabcolsep}{2.5mm}{
% \resizebox{\textwidth}{!}{
% \begin{tabular}{clccccccccccccl}
% \toprule
% \multicolumn{15}{c}{\textbf{Toys}} \\ \hline
% \multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{4}{c|}{\textbf{All}} & \multicolumn{4}{c|}{\textbf{Warm}} & \multicolumn{4}{c|}{\textbf{Cold}} & \multicolumn{1}{c}{\textbf{Inference Time (s)}} \\ \hline
% \multicolumn{1}{l|}{\textbf{Model Size}} & \multicolumn{1}{l|}{\textbf{Method}} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \multicolumn{1}{c|}{\textbf{N@10}} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \multicolumn{1}{c|}{\textbf{N@10}} & \textbf{R@5} & \textbf{R@10} & \textbf{N@5} & \multicolumn{1}{c|}{\textbf{N@10}} & \multicolumn{1}{c}{\textbf{All Users}} \\ \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{1.5B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0057 & 0.0093 & 0.005 & \multicolumn{1}{c|}{0.0064} & 0.008 & 0.0126 & 0.0066 & \multicolumn{1}{c|}{0.0085} & 0.0217 & 0.0416 & 0.017 & \multicolumn{1}{c|}{0.0239} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0065 & 0.0108 & 0.0056 & \multicolumn{1}{c|}{0.0072} & 0.0089 & 0.0144 & 0.0075 & \multicolumn{1}{c|}{0.0096} & 0.0084 & 0.0235 & 0.0055 & \multicolumn{1}{c|}{0.0111} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cellcolor{gray!16}\textbf{SETRec}} & \cellcolor{gray!16}\textbf{0.0116} & \cellcolor{gray!16}\textbf{0.0188} & \cellcolor{gray!16}\textbf{0.0095} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.012}} & \cellcolor{gray!16}\textbf{0.0144} & \cellcolor{gray!16}\textbf{0.0236} & \cellcolor{gray!16}\textbf{0.0118} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.0151}} & \cellcolor{gray!16}\textbf{0.0531} & \cellcolor{gray!16}\textbf{0.0883} & \cellcolor{gray!16}\textbf{0.0382} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.0507}} &  \\ \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{3B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0057 & 0.0109 & 0.0053 & \multicolumn{1}{c|}{0.0072} & 0.0078 & 0.0151 & 0.0069 & \multicolumn{1}{c|}{0.0097} & 0.0254 & 0.0471 & 0.0162 & \multicolumn{1}{c|}{0.0236} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0062 & 0.0096 & 0.0048 & \multicolumn{1}{c|}{0.0061} & 0.0082 & 0.0129 & 0.0062 & \multicolumn{1}{c|}{0.0081} & 0.0084 & 0.0218 & 0.0053 & \multicolumn{1}{c|}{0.0103} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{SETRec} & \textbf{0.0118} & \textbf{0.0195} & \textbf{0.0095} & \multicolumn{1}{c|}{\textbf{0.0123}} & \textbf{0.015} & \textbf{0.0258} & \textbf{0.0119} & \multicolumn{1}{c|}{\textbf{0.0159}} & \textbf{0.065} & \textbf{0.0964} & \textbf{0.0462} & \multicolumn{1}{c|}{\textbf{0.0571}} &  \\ \midrule
% \multicolumn{1}{c|}{\multirow{3}{*}{\textbf{7B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0057 & 0.0099 & 0.0044 & \multicolumn{1}{c|}{0.0061} & 0.0078 & 0.0137 & 0.0057 & \multicolumn{1}{c|}{0.0081} & 0.0215 & 0.0406 & 0.0144 & \multicolumn{1}{c|}{0.0216} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0048 & 0.0088 & 0.0041 & \multicolumn{1}{c|}{0.0057} & 0.0062 & 0.0114 & 0.0053 & \multicolumn{1}{c|}{0.0072} & 0.0064 & 0.0133 & 0.0037 & \multicolumn{1}{c|}{0.0065} &  \\
% \multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cellcolor{gray!16}\textbf{SETRec}} & \cellcolor{gray!16}\textbf{0.0107} & \cellcolor{gray!16}\textbf{0.0194} & \cellcolor{gray!16}\textbf{0.0083} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.0115}} & \cellcolor{gray!16}\textbf{0.0127} & \cellcolor{gray!16}\textbf{0.0239} & \cellcolor{gray!16}\textbf{0.01} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.014}} & \cellcolor{gray!16}\textbf{0.0632} & \cellcolor{gray!16}\textbf{0.1016} & \cellcolor{gray!16}\textbf{0.0482} & \multicolumn{1}{c|}{\cellcolor{gray!16}\textbf{0.0613}} &  \\ \bottomrule
% \end{tabular}
% }}
% \label{tab:scaling_performance}
% \end{table*}

\begin{table}[t]
\setlength{\abovecaptionskip}{0.05cm}
\setlength{\belowcaptionskip}{0.2cm}
\caption{Performance comparison between SETRec and competitive baselines with different LLM sizes on Qwen. }
\setlength{\tabcolsep}{2.2mm}{
\resizebox{0.46\textwidth}{!}{
\begin{tabular}{clcccccc}
\toprule
% \multicolumn{8}{c}{\textbf{Toys}} \\ \midrule
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \multicolumn{2}{c}{\textbf{All}} & \multicolumn{2}{c}{\textbf{Warm}} & \multicolumn{2}{c}{\textbf{Cold}} \\
\multicolumn{1}{l|}{} & \multicolumn{1}{l|}{} & \textbf{R@10} & \textbf{N@10} & \textbf{R@10} & \textbf{N@10} & \textbf{R@10} & \textbf{N@10} \\ \midrule\midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{1.5B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0093 & 0.0064 & 0.0126 & 0.0085 & 0.0416 & 0.0239 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0108 & 0.0072 & 0.0144 & 0.0096 & 0.0235 & 0.0111 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cellcolor{gray!16}\textbf{SETRec}} & \cellcolor{gray!16}\textbf{0.0188} & \cellcolor{gray!16}\textbf{0.0120} & \cellcolor{gray!16}\textbf{0.0236} & \cellcolor{gray!16}\textbf{0.0151} & \cellcolor{gray!16}\textbf{0.0883} & \cellcolor{gray!16}\textbf{0.0507} \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{3B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0109 & 0.0072 & 0.0151 & 0.0097 & 0.0471 & 0.0236 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0096 & 0.0061 & 0.0129 & 0.0081 & 0.0218 & 0.0103 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cellcolor{gray!16}\textbf{SETRec}} & \cellcolor{gray!16}\textbf{0.0195} & \cellcolor{gray!16}\textbf{0.0123} & \cellcolor{gray!16}\textbf{0.0258} & \cellcolor{gray!16}\textbf{0.0159} & \cellcolor{gray!16}\textbf{0.0964} & \cellcolor{gray!16}\textbf{0.0571} \\ \midrule
\multicolumn{1}{c|}{\multirow{3}{*}{\textbf{7B}}} & \multicolumn{1}{l|}{\textbf{LETTER}} & 0.0099 & 0.0061 & 0.0137 & 0.0081 & 0.0406 & 0.0216 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\textbf{E4SRec}} & 0.0088 & 0.0057 & 0.0114 & 0.0072 & 0.0133 & 0.0065 \\
\multicolumn{1}{c|}{} & \multicolumn{1}{l|}{\cellcolor{gray!16}\textbf{SETRec}} & \cellcolor{gray!16}\textbf{0.0194} & \cellcolor{gray!16}\textbf{0.0115} & \cellcolor{gray!16}\textbf{0.0239} & \cellcolor{gray!16}\textbf{0.0140} & \cellcolor{gray!16}\textbf{0.1016} & \cellcolor{gray!16}\textbf{0.0613} \\ \bottomrule
\end{tabular}
}}
\label{tab:scaling_performance}
\end{table}

\begin{figure*}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{-0.15cm}
\setlength{\belowcaptionskip}{-0cm}
  \centering 
  \hspace{-0.105in}
  \subfigure{
    \includegraphics[height=1.4in]{figures/hyper_alpha_R_10.pdf}} 
  % \hspace{-0.105in}
  \subfigure{
    \includegraphics[height=1.4in]{figures/hyper_alpha_N_10.pdf}} 
  \subfigure{
    \includegraphics[height=1.4in]{figures/hyper_N_R_10.pdf}}
  \subfigure{
    \includegraphics[height=1.4in]{figures/hyper_N_N_10.pdf}}
\caption{Performance of SETRec (T5) with different strength of AE loss $\alpha$ and different numbers of semantic tokens $N$.}
  \label{fig:hp}
  % \vspace{-0.3cm}
\end{figure*}

\subsubsection{\textbf{Effect of Semantic Strength $\bm{\beta}$ (RQ4)}}

\begin{figure}[t]
\vspace{-0.2cm}
\setlength{\abovecaptionskip}{-0.15cm}
\setlength{\belowcaptionskip}{-0.15cm}
  \centering 
  \hspace{-0.105in}
  \subfigure{
  \includegraphics[height=1.4in]{figures/hp_beta_warm.pdf}} 
  \hspace{-0.105in}
  \subfigure{    
  \includegraphics[height=1.4in]{figures/hp_beta_cold.pdf}} 
\caption{Performance of SETRec (T5) with different strength of semantics $\beta$ for inference.}
  \label{fig:hp_beta}
  % \vspace{-0.3cm}
\end{figure}

% 1. 如果只用cf的话，performance不行（significant inferior performance of beta=0 than beta>0)。这说明当同时利用cf和sem来encode user embedding的时候，decode也需要semantic的帮助。并且在cold上的提升要明显比warm上的提升大，这也说明了对cold start item推荐时semantic引入的必要性。
% 2. 持续增大到只用semantics时在warm和cold上仍然能取得不错的performance。说明了item rich semantic信息对于warm item的推荐也是有帮助的。这可能是因为在训练的时候semantic和cf之间achieve implicit alignment，无脑全入semantic也不会让performance掉太多。蕾丝的现象也在ablation里有观察到。
To investigate how semantic information contributes to the performance during inference, we vary $\beta$ from $0$ to $1$, where $\beta=0$ indicates that only CF score is used for ranking, and $\beta=1$ ranks items based solely on semantic scores (Eq. (\ref{eqn:single_logits})).  
From the results reported in Figure~\ref{fig:hp_beta}. 
we can find that 
1) Incorporating semantic information during inference is necessary (inferior performance of $\beta=0$ than $\beta>0$, which facilitates
global ranking over multi-dimensional information and lead to strong generalization ability. 
Notably, 
2) incorporating semantic scores brings more significant improvements on cold items, underscoring the critical role of semantic information for zero-shot scenarios.
Moreover, 
3) Gradually increase $\beta$ to rely solely on semantics ($\beta=1$), SETRec maintains competitive performance on warm items, which is probably attributed to the implicit alignment between CF and semantic tokens during training. 



\subsubsection{\textbf{Hyper-parameter Sensitivity (RQ4)}}\label{sec:exp_hyper_param}
We further study the hyper-parameter sensitivity to facilitate SETRec application.

\noindent$\bullet\quad$\textbf{Effect of $\bm{\alpha}$.} 
We vary the strength of AE loss $\alpha$ for SETRec training and present the results on Toys in Figure~\ref{fig:hp}(a-b). 
We can observe that 
% 1. alpha 从0慢慢增大，performance提升。这合理因为tokenizer肯定是需要随着llm一起优化的。
1) the performance is overall improved when $\alpha$ is increased from $0$ to $0.7$, which validates the effectiveness of reconstruction loss that encourages AE to preserve useful information in the latent space. 
% 2. 但是tokenizer的权重不能太高，和llm的loss一起做multi-task training的话，可能会导致模型偏向tokenizer更多，而这可能反过来影响llm，使他推荐能力学的差。（这个理由再想想，现在不太行）
Nonetheless, 
2) while continuously increasing $\alpha$ generally gives better performance on cold-start items, it might hurt the performance under ``warm'' setting. 
Based on the empirical results, we recommend setting $\alpha$ ranging from $0.5$ to $0.7$. 






\noindent$\bullet\quad$\textbf{Effect of $\bm{N}$.} 
We change the number of semantic tokens from $1$ to $6$ to investigate how $N$ affects the performance. 
From the results shown in Figure~\ref{fig:hp}(c-d), we can find that 
% 1. N提高，有提升。说明多侧面语义信息是有用的。
1) gradually increasing semantic tokens generally improves the performance, which validates the effectiveness of incorporating multiple tokens to mitigate the potential information conflicts~\cite{wang2024learnable} and embedding collapse issue~\cite{guoembedding}. 
% 2. 但是继续提升，反而会有所下降。这主要是因为XXX?
However, 
2) blindly increasing the number of semantic tokens might hurt the performance (decreased performance from $N=4$ to $N=6$). 
This is reasonable since it is non-trivial to recover the category-level preference aligning well with the real-world scenarios. 
Similar observations are also seen in~\cite{lin2024disentangled} and~\cite{lin2024temporally}. 





% \begin{figure}[t]
% % \vspace{-0.2cm}
% \setlength{\abovecaptionskip}{-0.15cm}
% \setlength{\belowcaptionskip}{-0cm}
%   \centering 
%   \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.4in]{figures/hyper_N_R@10.pdf}} 
%   % \hspace{-0.105in}
%   \subfigure{
%     \includegraphics[height=1.4in]{figures/hyper_N_N@10.pdf}} 
% \caption{Performance of SETRec (T5) with different number of semantic embeddings on Toys.}
%   \label{fig:hp_N}
%   % \vspace{-0.3cm}
% \end{figure}

