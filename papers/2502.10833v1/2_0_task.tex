\section{Preliminaries}\label{sec:task_formulation}
% In this section, we first introduce the task of LLM-based recommendation, and then retrospect existing item identifier designs and reveal their inherent critical issues. 

\vspace{2pt}
\noindent\textbf{LLM-based Generative Recommendation.} 
Harnessing LLMs' strong capabilities, LLM-based generative recommendation aims to use LLMs as recommenders to directly generate personalized recommendations. 
Formally, given the recommendation data 
$\mathcal{D}=\{\mathcal{S}_u|u\in\mathcal{U}, i\in\mathcal{I}\}$, where $\mathcal{S}_u = [i_1^{u}, i_2^{u}, \dots, i_L^{u}]$ is the user's historical interactions in chronological order and $L=|\mathcal{S}_u|$, 
% the target is to utilize a tokenizer $f(\cdot)$ and an LLM-based recommender model $\mathcal{M}(\cdot)$, to tokenize items into item identifiers $\tilde{\mathcal{I}}$, and encode the transformed user history (\ie identifier sequence) to generate recommended items. 
the target is to utilize a tokenizer $f(\cdot)$ to tokenize items into item identifiers $\tilde{\mathcal{I}}$, 
and an LLM-based recommender model $\mathcal{M}(\cdot)$ to encode the transformed user history $\bm{x}= [f(i_{1}), f(i_{2}), \dots, f(i_{L})]$ and generate next item identifier. 
% $\hat{\mathcal{I}}=\{\hat{i}\}$. 

% item tokenization起到了至关重要的作用, bridging the xxx

\vspace{2pt}
Bridging the language space and the item space, item identifier is a fundamental component for LLMs to encode user history and generate items. 
Existing identifiers can be divided into two groups:

% 先前的工作设计了不同的f来得到token sequence，为了更好的把pretraining的语言模型adapt到文本形式的推荐任务上。
\vspace{2pt}
\noindent$\bullet\quad$\textbf{\textit{Token-sequence identifier}} assigns each item with a discrete token sequence, \ie $\tilde{i} = [z_1, z_2, \dots, z_N]$, where $z_i$ is the discrete token. 
Given the user history $\mathcal{S}_u$, it is transformed to an identifier sequence $\bm{x}= [\tilde{i}_1, \tilde{i}_2, \dots, \tilde{i}_L]$, which is then encoded by LLMs to generate the next identifier via autoregressive generation: 
\begin{equation}
% \left\{
\begin{aligned}
    &\hat{y}_t = \mathop{\arg\max}_{v\in\mathcal{V}} \mathcal{M}(v|\hat{y}_{<t},\bm{x}), \\
\end{aligned}
% \right.
\end{equation} 
where $\mathcal{V}$ is the LLM vocabulary. 
Despite the effectiveness, generating token sequences would result in the local optima issue and inference inefficiency. 
As shown in Figure~\ref{fig:beam_size}, continuously increasing the beam size slightly improves recommendation accuracy, but remains inferior to globally optimal results. 
Worse still, the token-by-token generation requires multiple serial LLM calls, which significantly lowers the inference speed and hinders real-world applications.

\begin{figure}[t]
% \vspace{-0.2cm}
\setlength{\abovecaptionskip}{0.0cm}
\setlength{\belowcaptionskip}{-0.3cm}
\centering
\includegraphics[scale=0.4]{figures/beam_size.pdf}
\caption{Performance comparison between beam search and global search of LETTER on Toys. The global search is implemented by computing sequence probability for every item and ranking them based on the probabilities. }
\label{fig:beam_size}
\end{figure}


% of LLM-based generative recommendation. 



% 2. 而single embedding则是利用cf recommender 或者 feature extractor之类的 -> 获取到每个item的
\vspace{2pt}
\noindent$\bullet\quad$\textbf{\textit{Single-token identifier}} assigns each item with an ID or semantic embedding, \ie $\tilde{i}=\bm{z}$, 
% Existing work typically leverages a conventional CF recommender model as the tokenizer to obtain ID embedding 
which is usually obtained by a conventional CF recommender model (\eg SASRec~\cite{kang2018self})
or a pre-trained semantic extractor (\eg SentenceT5~\cite{ni2021sentence}). 
% Escaping from token-by-token generation, this line of work facilitates efficient item generation~\cite{wang2024rethinking}. 
Given the transformed user history $\bm{x}= [f(i_{1}), f(i_{2}), \dots, f(i_{L})]$, it first generates the embedding:
\begin{equation}
\begin{aligned}
% &x= [f(i_{1}), f(i_{2}), \dots, f(i_{L})] \\
&\hat{i} = \text{LLM\_Layers}(\bm{x}), \\
\end{aligned}
\end{equation}
where $\text{LLM\_Layers}(\cdot)$ is the attention layers from the LLM $\mathcal{M}(\cdot)$. 
Based on the generated item embedding $\hat{i}$, 
an additional grounding head is added on top of the LLM layers to obtain the scores for all items for ranking. 
% 虽然这类方法不会有beam search的问题，因为它只有一个token。
% 但是 - 只有单一维度的信息。只有CF会导致泛化能力弱，只有sem会导致与交互信息不对齐。
% 2) single-embedding-based（只有cf的难以泛化到只有sem的，而只有sem又可能和cf有所冲突）（-这个可以考虑加一下图）
Although it improves inference efficiency by bypassing the token-by-token autoregressive generation, 
representing items with a single ID embedding struggles with items with fewer interactions while a single semantic embedding overlooks the crucial CF information, thus leading to suboptimal results. 
% As shown in Figure~\ref{}, items with similar semantics might have different interactions. 

% \noindent$\bullet\quad${\textbf{\textit{Set identifier}}}. 
Based on the above insights, we
summarize two fundamental principles for identifier designs: 
1) integration of semantic and CF information, to leverage rich multi-dimensional item information, and  
2) order-agnostic identifier, to eliminate the unnecessary dependencies between tokens associated with an identifier, which can alleviate the local optima issue and improve generation efficiency.  
In this light, we introduce a novel set identifier paradigm, which employs a set of order-agnostic tokens to represent multi-dimensional item information. 
% facilitating globally optimal results and achieving simultaneous generation to boost efficiency. 
% It encodes the user history (\ie identifier sequence) and decodes the tokens in set identifiers simultaneously. 
% which removes the dependency from token to token within identifiers, thus alleviating the local optima issue and boost the efficiency. 

