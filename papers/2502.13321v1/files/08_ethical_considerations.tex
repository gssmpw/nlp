While modeling user trust can allow AI systems to help users overcome their cognitive biases, care must be taken that such user modeling is not taken advantage of to mislead or manipulate users. 
We emphasize that our trust-adaptive interventions are not intended to force or manipulate users into behaving a certain way, but to recognize when the user's trust may be hindering their reasoning.
We do not condone use of such user modeling to manipulate users by misrepresenting the AI's beliefs or causing the user any distress. 

People designing AI assistants with trust-adaptive interventions should ensure the interventions comply with the local ethical standards of the intended users. 
Further, we encourage promoting transparency and accountability by disclosing to users that the AI assistant is modeling user trust.