\begin{figure*}
    \centering
    \includegraphics[width=0.93\linewidth]{figs/dm-setup-nearfinal.pdf}
    \caption{In our user study, each user interacts with an AI for a sequence of $30$ decision-making problems. In each problem, the user first makes a decision by themselves, and then receives advice from the AI which they use to make a final decision. The user is then told what the correct decision is, and reports their trust in the AI (out of 10).}
    \label{fig:decision_making_setup}
\end{figure*}

We draw on a vast literature on measuring user trust in AI systems and evaluate how decision aids can be used for mitigating inappropriate reliance in moments of low or high trust.

\paragraph{Trust in Human-AI Interactions.} 
Much work has explored the nature of human trust in AI systems~\cite{lai2023towards}, particularly in situations characterized by risk and uncertainty~\cite{jacovi2021formalizing}. 
\citet{lee2004trust} provide the most commonly accepted definition of trust, as a person's attitude that an agent will help them achieve their goals. 
User trust is considered to be calibrated ~\cite{alizadeh2022building} when it aligns with the AI system's true capabilities~\cite{wright2010trust}, thus reducing AI misuse and disuse~\cite{alizadeh2022building}. 
While trust in AI has typically been attributed to socio-economic and individual factors~\cite{bach2024systematic}, recent work~\cite{dhuliawala2023diachronic, pareek2024trust} examines how trust develops as users interact with AI systems over multiple timesteps. 

\paragraph{Measuring Trust.} ~\citet{bach2024systematic} identify a variety of mechanisms for measuring user trust, such as questionnaires~\cite{schaffer2019can}, qualitative interviews~\cite{barda2020qualitative}, surveys~\cite{lin2019building}, and point scales~\cite{gulati2019design}. 
In AI-assisted decision-making, early works measured trust by observing user reliance behavior~\cite{yin2019understanding, zhang2020effect}; however, \citet{de2021defining} distinguish reliance, an observable behavior, from trust, a subjective belief. 
Self-reported trust levels from users, where users report their confidence in the AI's accuracy for a question, are a reasonable proxy for trust, albeit at a local, interaction level~\cite{pareek2024trust, dhuliawala2023diachronic}. 
Instead, we adopt a more global lens for eliciting trust scores by asking users to report their belief in the AI's helpfulness on a scale of 0 to 10.

\paragraph{Mitigating Inappropriate Reliance.} 
Inappropriate reliance, where users mistakenly accept incorrect AI predictions or reject correct ones~\cite{parasuraman1997humans}, is highly undesirable in high-stakes domains, such as healthcare and law~\cite{schemmer2023appropriate}. 
Appropriate reliance can be fostered through various decision aids, such as model confidences~\cite{zhang2020effect,vodrahalli2022uncalibrated}, explanations~\cite{wang2021explanations,bansal2021does}, uncertainty expressions~\cite{zhou-etal-2024-relying,kim2024m}, and providing sources~\cite{feng2019can}. 
Cognitive forcing functions~\cite{buccinca2021trust}, which insert friction~\cite{chen2024exploring,inan2025better} and promote deliberation~\cite{park2019slow,rastogi2022deciding,ma2024towards}, are effective at mitigating over-reliance. 
We demonstrate how strategically providing these decision aids to users during moments of low or high trust can mitigate trust-induced inappropriate reliance.