So far, we have applied interventions to mitigate inappropriate reliance based on user-reported trust levels. 
However, in real-world scenarios, the AI assistant is unlikely to be able to observe the user's trust level after each interaction. 
In this section, we attempt to model user trust directly based on user-AI interaction history, and highlight the difficulties of simple trust heuristics at identifying moments of low and high user trust.

We experiment with several simple heuristics that are computed by looking at interaction outcomes. \textbf{AIAcc5} estimates trust based on the AI's accuracy in the last 5 interactions. \textbf{CapabilityDiff} calculates the difference between the accuracy of the AI's prediction and the user's initial decision, over all interactions so far. 
\textbf{SmoothOutcomes} updates the trust $\tau_t \in [-1, 1]$ after interacton $t$, based on the AI's prediction correctness $a_t \in \{0,1\}$
\vspace{-0.5em}
\begin{align*}
    \tau_0 = 0; \tau_t = r \cdot (2 * a_t-1) + (1-r) \cdot \tau_{t-1}
\end{align*}

where $r$ is a smoothing parameter. \textbf{SmoothConfs} is similar to the above, except the update term is weighted by the AI's confidence $\aiconf_t$
\vspace{-0.5em}
\begin{align*}
    \tau_0 = 0; \tau_t = r \cdot (2 * a_t-1) \cdot \aiconf_t + (1-r) \cdot \tau_{t-1}
\end{align*}
Finally, we train a \textbf{TrustModel}, a linear regression model which estimates the change in user trust after each interaction. 
Appendix~\ref{} contains additional details about the trust model.

We use a set of 45 additional user sessions as training data for the model and selecting smoothing parameters, and evaluate on a held-out set of 30 user sessions, totaling 1350 and 900 user-AI interactions in train and test set. 
We evaluate the trust estimation methods on their correlation with user-reported trust levels for the test set interactions, and F1 for predicting low and high trust. 

\input{tables/modeling_trust_results}

Table~\ref{tab:trust_modeling} shows correlation between trust scores estimated by the various heuristics and the user-reported trust levels, on both train and tests, along with the F1 for detecting moments of low and high user trust. 
We observe that all of the above methods achieve only moderate correlation with user-reported trust, and achieve very low F1 ($\leq 0.5$) at detecting both low and high trust instances. 
These results point to the challenging nature of modeling user trust, and the inadequacy of surface-level interaction features. 
Future work can model trust using more sophisticated interaction features~\cite{fang2013trust}.