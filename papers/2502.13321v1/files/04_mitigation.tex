We hypothesize that AI systems can counterbalance the cognitive bias caused by trust by adapting their behavior according to the user's trust level.\footnote{Our setup assumes that the AI can observe the user's last-reported trust level at the start of every interaction.} 
% For instance, when the user has low trust and is more likely to disregard AI assistance, the assistant can provide extra information to support its prediction.
We introduce \emph{trust-adaptive interventions} for reducing inappropriate reliance. 
Trust-adaptive interventions are designed to correct for trust-induced cognitive bias, and are only applied when the user's trust level is either above or below a certain threshold. 
We hypothesize that uniformly applying these interventions, rather than only when trust is low or high, will worsen inappropriate reliance.

\paragraph{Experiment Setup.} We evaluate the utility of adaptive interventions through a between-subjects study.\footnote{We do not conduct a within-subjects study because the same user cannot be subjected to multiple conditions without introducing interaction effects.}
Each user is assigned to one of
three experimental conditions: a \textbf{No Intervention} baseline where the intervention is never applied, an \textbf{Intervention Always} baseline where the intervention is applied whenever the AI’s prediction differs from the user’s initial decision, and the \textbf{Trust-Adaptive Intervention} condition where the intervention is only applied when the user’s trust lies above or below a specified threshold.
Based on the relationship we observe between reported user trus and reliance (Figure~\ref{fig:initial_reliance_plots}), we select a threshold trust of 5 out of 10 for mitigating under-reliance, and 8 out of 10 for mitigating over-reliance. 
We conduct studies with 30 users assigned to each experimental condition in the \arcc\ and \arco\ task settings, and 20 users for each condition in the \diagc\ task setting.

% \paragraph{Baselines}

\paragraph{Evaluating Interventions.} 
We evaluate intervention conditions on \underreliance\ or \overreliance, depending on the type of inappropriate reliance the intervention is intended to mitigate. 
We also evaluate \textbf{\totalinapprel}, which is the sum of \underreliance\ and \overreliance, to check if mitigating one type of inappropriate reliance exacerbates the other type to the same degree. 
Finally, \textbf{\finalacc} captures the effect of interventions on decision-making performance. 

We compute metrics across all user interactions where the user and AI disagree ($\userdecision{\init}_i \neq \aidecision_i$) and also analyze subsets of these interactions based on user trust levels.
Aggregating over interactions rather than users may skew user representation, for example, a user with generally low trust will represent more low trust interactions than other users. 
On the other hand, macro-aggregation by averaging per-user metrics results in high inter-user variance, since some users have very few interactions meeting the specified criteria.
Macro-aggregation results are presented in Appendix~\ref{sec:appendix_macroaggregation}.

\begin{figure*}[t]
    \centering
    % \includegraphics[width=\linewidth]{figs/mitigating_underreliance.png}
    \includegraphics[width=\linewidth]{figs/mitigating_underreliance.png}
    \caption{Reliance metrics and decision accuracy for users, evaluating the utility of supporting explanations at mitigating under-reliance. $n$ represents the number of user-AI interactions that we aggregate over for the corresponding condition. Showing explanations adaptively reduces \underreliance\ and \totalinapprel\ while boosting \finalacc\ across all task settings, particularly when user trust is low.}
    \label{fig:mitigating_underreliance}
    \vspace{0.5em}
    \includegraphics[width=\linewidth]{figs/mitigating_overreliance.png}
    \caption{Reliance metrics and decision accuracy for users, evaluating the utility of counter-explanations at mitigating over-reliance. Showing counter-explanations adaptively reduces \overreliance\ and \totalinapprel\ while boosting \finalacc\ across almost all settings, particularly when trust is high.}
    \label{fig:mitigating_overreliance}
\end{figure*}


\subsection{Mitigating Under-reliance with Supporting Explanations}
\label{subsec:mitigate_underreliance}

AI explanations have been widely studied as a decision aid~\cite{bussone2015role,wang2021explanations,poursabzi2021manipulating}. 
Prior work has shown that natural language explanations supporting the AI prediction cause over-reliance~\cite{si2024large,sieker2024illusion,hashemi-chaleshtori-etal-2024-evaluating}. 
We hypothesize that providing natural language supporting explanations when user trust is low ($< 5$) can mitigate under-reliance. 

We generate supporting explanations for all problems by prompting GPT-4o~\cite{hurst2024gpt} to provide a 3--4 sentence explanation $E^s_i$ for each option $y_i \in \mathcal{Y}$ of each problem $P$ (prompts in Table~\ref{tab:prompts}). 
Explanations were manually reviewed by an author to ensure they entailed the corresponding prediction.
%, including for explanations of incorrect predictions. 
When the intervention was applied during a user-AI interaction, the AI would provide an augmented recommendation $\recommendation'_i = \recommendation_i + E^s_i$. 
To encourage users to read the explanation, they are allowed to make their final decision only 15 seconds after the AI advice is shown.

We separately evaluated the interventions across interactions where the user's initial decision disagrees with the AI prediction, and across the ``low trust'' subset of interactions. 
Figure~\ref{fig:mitigating_underreliance} shows that \textbf{providing supporting explanations \emph{adaptively} mitigates under-reliance when user trust is low}, especially when AI confidence is miscalibrated.

\paragraph{Trust-Adaptive Explanations Help.}
In the Trust-Adaptive condition, users exhibit lower \underreliance, lower inappropriate reliance, and higher decision accuracy \emph{across all task settings}. 
These improvements are particularly notable when user trust is low and an explanation is provided in the Trust-Adaptive Intervention condition but not in the No Intervention condition. 
Providing explanations when trust is low results in 13--31\% reduction in \underreliance, 9--38\% reduction in \totalinapprel, and 10--19\% improvement in \finalacc.

\paragraph{Explanations Offset AI Miscalibration.} 
The largest improvements occur when the AI system is over-confident, i.e. the \arco\ task setting, which had the largest number of interactions where user trust was low. 
This finding indicates that explanations are effective for mitigating AI disuse when AI confidences are miscalibrated.

\paragraph{Persistent Explanations Can Hurt.} 
The Intervention Always condition does not yield similar improvements and worsens decision accuracy in the Diagnosis task. 
In the \arcc\ task setting, showing explanations uniformly \emph{increases} \underreliance\ when trust is low. 
We suspect that showing explanations always rather than adaptively exposes the user to many misleading explanations, resulting in an overall loss of trust in the explanations' trustworthiness. 
In \arco\ and \diagc, explanations reduce inappropriate reliance when trust is low, but not across all user interactions.
% , indicating over-reliance on explanations when trust is not low.


\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figs/mitigate_both.png}
    % \includegraphics[width=\linewidth]{figs/mitigate_both-ai2arc39ce_calibrated-barplots.png}
    % \includegraphics[width=\linewidth]{figs/mitigate_both-ddxplus_calibrated-barplots.png}
    \caption{Effect of providing supporting explanations (/) and counter-explanations (\textbackslash), depending on user trust, within the same user session yields complementary benefits in inappropriate reliance and decision-making accuracy.}
    \label{fig:mitigating_both}
\end{figure*}



\subsection{Mitigating Over-reliance with Counter-Explanations}
\label{subsec:mitigate_overreliance}

Similar to how providing supporting explanations are an effective intervention when user trust is low, we investigate whether providing reasons for why the model prediction might be incorrect can counter-balance the cognitive effect of high trust ($> 8$). 
Such \emph{counter-explanations} have been shown to reduce over-reliance compared to regular supporting explanations~\cite{si2024large}. 

Similar to the supporting explanations, we generate natural language counter-explanations for each option by prompting GPT-4o to list 1--2 reasons why that option \emph{might} be incorrect, while not completely rejecting that option (e.g. ``I believe Bronchitis is the correct diagnosis due to ..., but it is possible that...''). 
The counter-explanations frequently include expressions of uncertainty (``could potentially'', ``it may be that''), alternative possibilities, and specific circumstances under which the model's prediction may be incorrect. 
Table~\ref{tab:sample_explanations} contains examples of generated counter-explanations.

Figure~\ref{fig:mitigating_overreliance} shows that \textbf{providing counter-explanations \emph{adaptively} mitigates over-reliance when user trust is high}.

\paragraph{Trust-Adaptive Counter-Explanations Help.}
Counter-explanations are effective at reducing over-reliance when user trust is high, across \emph{all} task settings, with 10--23\% reduction in \overreliance, 19--36\% reduction in \totalinapprel, and 8--20\% improvement in \finalacc. 
When considering all interactions where the user and AI have different prediction, improvements are also observed in almost all cases but to a lesser extent.

\paragraph{Persistent Counter-Explanations Are Not As Helpful.} The effects become less pronounced in the Intervention Always condition. 
In the \arco\ setting, users perform uniformly worse than in the No Intervention condition when trust is high. 
Users may be less inclined to closely evaluate counter-explanations when they are always shown.


\subsection{Mitigating Under- and Over-Reliance Simultaneously}
\label{subsec:mitigate_both}

We now investigate whether showing supporting explanations when trust is low ($<5$) and counter-explanations when trust is high ($>8$) in the same user-AI study yields complementary improvements in decision accuracy and inappropriate reliance. 

Figure~\ref{fig:mitigating_both} shows that \textbf{providing different types of explanations based on user trust yields complementary performance improvements} on both the ARC and Diagnosis tasks. 
The benefits observed by using supporting explanations during low trust mirror those previously observed in \S\ref{subsec:mitigate_underreliance}, while the benefits of using counter-explanation are similar to those observed in \S\ref{subsec:mitigate_overreliance}.

\subsection{Intervention through Deceleration}
\label{subsec:deceleration}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/mitigate_both-using_pauses-ai2arc39ce_calibrated-barplots.png}
    \caption{Effect of decelerating interventions on reducing inappropriate reliance and improving decision accuracy.}
    \label{fig:pause_results}
\end{figure*}

We have demonstrated that supporting and counter-explanations are useful for mitigating under- and over-reliance, respectively. 
We now investigate the utility of another type of intervention: slowing down the interaction to promote deliberation, without providing additional information to the user. 

To mitigate under-reliance by decelerating the interaction, we display a ``The AI is thinking...'' message for 10 seconds before the AI prediction is revealed to the user. 
To mitigate over-reliance, we reveal the AI prediction and ask the user to carefully consider the AI advice; the user is made to wait 10 seconds before making their final decision. 

\paragraph{Findings.} Figure~\ref{fig:pause_results} shows the effect of the above decelerating interventions at mitigating inappropriate reliance in the \arcc\ task setting. 
We see that telling users that the AI is thinking is not particularly effective at reducing \totalinapprel, even when trust is low. 
On the other hand, forcing users to consider the AI advice closely when trust is high improves \totalinapprel\ and \finalacc. 
Our findings extend those of~\citet{buccinca2021trust}, showing that cognitive forcing is particularly useful for reducing over-reliance when user trust is high.

