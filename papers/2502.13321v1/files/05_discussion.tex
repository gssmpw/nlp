Our results highlight the promise of trust-adaptive interventions based on experiments in a controlled setting. 
We highlight some considerations when designing trust-adaptive interventions for real-world decision-making scenarios.

\paragraph{Applying trust-adaptive interventions.} 
A key assumption of our setup does not hold in most real-world settings: that both parties have access to real-time feedback about decision accuracy. 
Environment feedback in response to user decisions will provide a sparse signal in some contexts. 
Users may still internally update their trust in the AI based on their confidence in their own decision and their (potentially incorrect) perception of their expertise in comparison to the AI assistant's ability.
Additionally, some interventions may not be suitable for certain tasks; for example, LLM-generated explanation frequently known to hallucinate details, which may be undesirable in high-stakes applications.

\paragraph{Can we model user trust?} 
In our setting, the AI assistant can observe the user's trust level after each interaction. 
In appendix~\ref{sec:appendix_trustmodeling}, we provide an analysis of several heuristic-based and learning-based trust models on their ability to predict user trust levels based on user-AI interaction history instead. 
We find that these heuristics only achieve moderate correlation with user-reported trust levels, and are especially poor at predicting moments of high and low user trust. 
Our results point to the challenging nature of modeling user trust, and the inadequacy of surface-level interaction features. 
Instead, user-specific features such as users' internal confidence, their prior experience with AI systems and task expertise may be better indicators of user trust.

\paragraph{Is all inappropriate reliance \emph{equally} bad?}
Our formulation of \totalinapprel\ treats both under- and over-reliance as equally undesirable.
However, a company developing an AI assistant for clinicians may be more wary of clinicians over-relying on their assistant, which may leave them liable. 
The relative importance of mitigating under- and over-reliance can be quantified through a balancing utility function.
Such a utility function can also be used as a reward signal for optimizing an intervention policy to signal when an intervention should be applied.
