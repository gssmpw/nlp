We explore the utility of \emph{trust-adaptive interventions} for mitigating inappropriate reliance when users have low or high trust in AI assistants. 
We demonstrate that low and high levels of trust result in increased inappropriate reliance on AI recommendations for laypeople answering science questions and for doctors making medical diagnoses.
We conduct controlled between-subjects studies and find that adaptively providing supporting explanations during low trust and counter-explanations during high trust reduces inappropriate reliance and improves users' decision accuracy. 
These findings generalize to decelerating interventions; forcing users to pause and deliberate before making their final decision helps reduce over-reliance.

Our findings present an initial exploration into adapting AI behaviors based on user trust levels.
We adopted a simple thresholding criterion for deciding when to intervene, but more sophisticated criteria that also account for user and AI confidence may have potential. 
Further, rather than looking at whether user trust is too high or low, we can consider whether the trust is calibrated with the assistant's trustworthiness. 
High user trust may not be as undesirable when the AI is significantly more accurate than the user on the task. 
We hope our findings inspire the community to more closely consider the effect of user trust in user-AI interactions and the potential benefits of modeling and adapting to user trust levels.


\newpage

