We study how user trust impacts reliance on AI advice over a sequence of decision-making problems (\S\ref{subsec:ai_assisted_decision_making}). 
User studies reveal that trust being too high or too low increases inappropriate reliance (\S\ref{subsec:initial_trust_experiments}).

\subsection{Sequential AI-Assisted Decision-Making}
\label{subsec:ai_assisted_decision_making}

We consider a setting where a human user interacts with an AI assistant on a sequence of $N$ decision-making problems, where each problem belongs to the same task, such as making medical diagnoses based on symptoms. 
Problems are tuples of input $x$, categorical choices $\mathcal{Y}$, and correct choice $y^*\in\mathcal{Y}$. 
The user solves each problem in three stages: \textbf{independent decision-making} based on their own knowledge, \textbf{decision revision} after viewing the AI recommendation, and \textbf{trust update} in the AI after observing decision accuracy (Figure~\ref{fig:decision_making_setup}).\footnote{Our setup assumes that both the user and AI can observe the ground-truth decision after each problem.}

\paragraph{1. Independent Decision-Making:} 
For the $i^{th}$ decision-making problem with input $x_i$, the user initially makes a decision $\userdecision{\init}_i \in \mathcal{Y}$. 

\paragraph{2. AI-Informed Decision Revision:} The user then views the AI prediction $\aidecision_i$ and confidence estimate $\aiconf_i$, and makes a final decision $\userdecision{\fin}_i$.

\paragraph{3. Trust Update:} After the user makes their final decision $\userdecision{\fin}_i$, the interface informs the user of the accuracy of their decision and the AI prediction $\aidecision_i$. 
Observing this feedback may alter the user's trust in the AI's ability to help them make better decisions. 
For instance, the AI misleading the user into making a wrong decision is likely to decay trust. 
After showing this feedback, we ask users to report how much they trust the AI to help them with the decision-making task based on all user-AI interactions so far, as an integer between 0 and 10.

Our trust operationalization captures a global belief in the AI's helpfulness, which is likely to influence how the user relies on AI advice in subsequent decision-making problems. 

\paragraph{Evaluating Appropriate Reliance.} 
We only evaluate interactions where the user's initial decision differs from the AI prediction, i.e. $\userdecision{\init}_i \neq \aidecision_i$. 
We capture the degree of users' reliance on AI assistance using \textbf{\switchrate}~\cite{yin2019understanding, zhang2020effect}, the fraction of interactions where the user switched their decision to the AI prediction. 
Following~\citet{ma2024you}, we capture the degree of \emph{appropriate} reliance on AI assistance using two metrics: \textbf{\overreliance} represents the fraction of interactions where the user switches to the AI's prediction when the AI is incorrect, while \textbf{\underreliance} represents the fraction of interactions where the user does not switch to the AI's prediction when the AI was in fact correct.

We hypothesize that very high and very low values of trust will increase the probability of inappropriate reliance in the \emph{next} user-AI interaction. 
Specifically, we hypothesize that low user trust biases users towards rejecting correct AI advice, i.e. higher \underreliance. 
Similarly, high trust leads to users accepting incorrect advice more frequently, i.e. higher \overreliance.


\subsection{Experiments}
\label{subsec:initial_trust_experiments}
We evaluate the impact of users' trust level\footnote{Henceforth, ``user trust'' refers to the user's trust level at the start of an interaction, i.e. the trust score reported by the user at the end of the previous interaction.} on their reliance behavior and decision-making performance in subsequent user-AI interactions.

\paragraph{Decision-Making Tasks.} 
We conduct user studies on two decision-making tasks. 
The \textbf{ARC} task consists of answering science questions from the ARC dataset~\cite{allenai:arc}. 
Each problem consists of a question and two options. 
The \textbf{Diagnosis} task involves making medical diagnoses based on patient intake forms, sourced from the DDXPlus dataset~\cite{fansi2022ddxplus}. 
Users must select from four possible diagnoses. 
Appendix~\ref{sec:appendix_taskdetails} contains more details about problem selection.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/ai_calibration.png}
    \caption{Calibration curves and Expected Calibration Error (ECE) of our simulated AI assistants.}
    \label{fig:ai_calibration}
\end{figure}


\paragraph{Simulated AI.} Our user studies use a simulated AI that provides a recommendation $\recommendation_i = (\aidecision_i, \aiconf_i)$ for each decision-making problem. 
We experiment with two types of AI assistants: one perfectly calibrated and one overconfident. 

For the calibrated AI assistant, for the $i^{th}$ problem, we first sample a confidence score $\aiconf_i \sim \texttt{Uniform}(0.5, 0.95)$. 
We then decide if the AI prediction $\aidecision_i$ will be the correct decision $y^*_i$ by sampling with probability $\aiconf_i$:
\vspace{-0.5em}
    \[ \aidecision_i =  
        \begin{cases}
        y^*_i & \text{w.p. } \aiconf_i, \\
        \sim \texttt{Uniform}(\mathcal{Y} \setminus \{y^*_i\}) & \text{w.p. } 1-\aiconf_i
        \end{cases}
    \]

For the overconfident AI assistant, we sample the AI confidence $\aiconf_i$ as above, and then sample another parameter $c_i'$ from the triangular distribution $\texttt{Tri}(0.5, \aiconf_i, \aiconf_i)$. 
The AI prediction is sampled as before, but with a correctness probability $c_i'$ lower than the confidence $\aiconf_i$ shown to the user.

This sampling procedure generates AI predictions and confidence scores for each decision-making problem. 
Figure~\ref{fig:ai_calibration} shows calibration curves for the calibrated and overconfident AI predictions.


\paragraph{Experiment Setup.} 

We perform user studies on the Prolific platform, on three task settings: the ARC task with a calibrated AI (\arcc), the ARC task with an overconfident AI (\arco), and the Diagnosis task with a calibrated AI (\diagc). 

For the ARC task, we recruit users with at least an undergraduate degree. 
We recruit two groups of 30 users, one each for the \arcc\ and \arco\ settings. 
Users achieve $67\%$ accuracy on this task without AI assistance, whereas the calibrated and overconfident AI achieve $71\%$ and $64\%$ accuracy, respectively. 
Users are paid \$1.0, plus a \$0.10 bonus for every correct final decision. 

For the Diagnosis task (\diagc\ task setting), we conduct studies with professional doctors, who achieve $74\%$ task accuracy without the AI. 
Due to the lower number of qualified participants on Prolific, we recruit 20 users. 
Users are paid \$2.0, plus a \$0.10 bonus for every correct final decision.

For each task setting, we sample 10 sequences $S_i = \{ P^i_1, P^i_2, ..., P^i_{30}\}$ of 30 decision-making problems $P^i_j = \{ (x_j, y^*_j), \recommendation_j \}$.
Users in each task setting are randomly assigned to a sequence $S_i$ upon starting the study. 
Appendix~\ref{sec:appendix_taskdetails} contains additional details about the user study setup.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{figs/trust_reliance_relation.png}
    \caption{Reliance metrics at different levels of user trust. In each plot, $r$ represents the weighted Pearson correlation coefficient. All correlations are statistically significant, with $p < 0.001$. Bar shades correspond to number of user interactions at each trust level. }
    \label{fig:initial_reliance_plots}
\end{figure}

\paragraph{Findings.} 
Figure~\ref{fig:initial_reliance_plots} shows the relationship between user trust and reliance, aggregated across all users in the same task setting. 
We highlight a few key takeaways (\textbf{T$^*$}). 
\takeaway{1}: \switchrate\ is strongly correlated with user trust, which suggests that users' internal trust influences how likely they are to accept AI advice. 
\takeaway{2}: User trust has moderate to strong correlation with \overreliance. We further observe that \overreliance\ is highest at \emph{high} values of user trust ($9$--$10$). 
\takeaway{3}: User trust has a strong negative correlation with \underreliance. At lower values of trust ($< 5$), users exhibit the most \underreliance.
\textbf{These findings suggest that extreme values of user trust act as a cognitive bias, resulting in higher inappropriate reliance in subsequent human-AI interactions.} 
