Trust biases how users rely on AI recommendations in AI-assisted decision-making tasks, with low and high levels of trust resulting in increased under- and over-reliance, respectively. 
We propose that AI assistants should adapt their behavior through \emph{trust-adaptive interventions} to mitigate such inappropriate reliance. 
For instance, when user trust is low, providing an explanation can elicit more careful consideration of the assistant's advice by the user. 
In two decision-making scenarios---laypeople answering science questions and doctors making medical diagnoses---we find that providing supporting and counter-explanations during moments of low and high trust, respectively, yields up to 38\% reduction in inappropriate reliance and 20\% improvement in decision accuracy. 
We are similarly able to reduce over-reliance by adaptively inserting forced pauses to promote deliberation. 
Our results highlight how AI adaptation to user trust facilitates appropriate reliance, presenting exciting avenues for improving human-AI collaboration.