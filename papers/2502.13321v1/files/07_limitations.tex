Our user studies were performed using a simulated AI assistant, rather than a real system such as a Large Language Model. 
Similar to~\citet{buccinca2021trust,dhuliawala2023diachronic}, we use a simulated AI for the controllability of AI accuracy and confidence calibration. 
However, a simulated AI may be unrealistic compared to real AI systems that people use. 
For instance, since the answer correctness is decided independently for each problem, our AI assistant may provide contradicting predictions for near-identical problems. 

Furthermore, the behavior of Prolific participants interacting with an AI in a user study may differ from users of a live system in a real-world scenario~\cite{mcgrath1995methodology}, particularly due to misalignment of motivations~\cite{deci1999meta}. 
We test for the effect of two confounding variables: trust reporting and user experience with AI (Appendix~\ref{sec:appendix_confoundingvariables}), finding that they do not affect user reliance behavior, but there may be other confounding variables we have not yet considered.

We conducted all our user studies with participants from the U.K. and U.S.A. who were fluent in English. 
Users from other countries and cultures may have different attitudes towards AI systems, and thus behave differently.

Finally, our findings would be more reliable if we could obtain data from more participants, more tasks and more interventions, however this is not possible due to financial restrictions and the dearth of professional doctors on the Prolific platform.