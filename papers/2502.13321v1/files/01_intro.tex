AI systems are being deployed to assist humans in a wide range of decision-making tasks~\cite{cai2019hello,chiang2023two,che2024integrating}.
AI-assisted decision-making~\cite{lai2023towards} typically consists of an AI system providing a recommendation for the user's consideration. 
A key factor modulating how users incorporate AI advice is \emph{user trust}, which is the user's belief that the AI will help them achieve their goals in situations of uncertainty and vulnerability~\cite{lee2004trust}. 
Having higher trust makes users more likely to accept the AI's recommendation, all else being equal~\cite{dzindolet2003role}. 
Moreover, trust is not a static belief, but instead continuously evolves as the user interacts with the AI and observes decision outcomes~\cite{dhuliawala2023diachronic}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{figs/fig1.pdf}
    \caption{User trust in AI systems evolves over a series of decision-making interactions, impacting how carefully the user considers future AI recommendations. To mitigate the effects of extreme trust and encourage critical reliance, AI systems should adapt their behavior to users' trust levels. For instance, when trust is low, providing explanations reduces under-reliance.}
    \label{fig:fig1}
\end{figure}

User trust does not always align with AI assistant trustworthiness, i.e. its true capability to help the user~\cite{wright2010trust}. 
Miscalibrated trust~\cite{jacovi2021formalizing} may develop due to recency bias, the user's internal biases towards AI, or the assistant's inability to communicate its reasoning or limitations. 
Miscalibrated trust acts as a cognitive bias~\cite{lee2024power} and hinders critical evaluation of AI recommendations, resulting in \emph{inappropriate reliance}~\cite{parasuraman1997humans}. 
For example, we find that doctors mistakenly accept 26\% of AI misdiagnoses when their trust is high, compared to 8\% when trust is lower, indicating over-reliance. 
Conversely, when user trust is low, doctors reject correct AI diagnoses 68\% of the time, up from 40\% otherwise, indicating a bias towards under-reliance. 

We posit that AI assistants should adapt their behavior in response to users' trust levels in order to mitigate inappropriate reliance caused by extreme (low or high) trust. 
For instance, when trust is low, the assistant can reduce risk of disuse by providing the user with additional reasoning to support its recommendation (Figure~\ref{fig:fig1}). 
Similarly, when users are too trusting, the assistant can highlight reasons its recommendation may be incorrect, or can simply slow down the interaction. 
We hypothesize that strategically introducing these \emph{trust-adaptive interventions} will prompt users to engage more carefully with AI advice, rather than accepting or rejecting advice without due consideration. 

We examine the effect of trust-adaptive AI interventions at mitigating inappropriate reliance on two decision-making tasks: answering science trivia questions and making medical diagnoses based on patient symptoms. 
We first validate our premise that, when interacting with the AI assistant over a sequence of decision-making problems, users' trust level at the start of a given interaction affects their reliance behavior and decision-making performance (\S\ref{subsec:initial_trust_experiments}), with extreme levels of user trust (too high or too low) resulting in increased inappropriate reliance. 
Through controlled studies, we find that strategically providing supporting explanations when user trust is low reduces under-reliance and improves decision-making accuracy (\S\ref{subsec:mitigate_underreliance}). 
Similarly, providing counter-explanations reduces over-reliance when trust is high (\S\ref{subsec:mitigate_overreliance}). 
Combining these interventions to mitigate under- and over-reliance yields complementary improvements in decision-making accuracy and inappropriate reliance (\S\ref{subsec:mitigate_both}). 
We also evaluate the utility of intervening by decelerating the interaction, finding that it helps reduce over-reliance but not under-reliance (\S\ref{subsec:deceleration}). 

Our findings highlight the utility of adapting to user trust in AI-assisted decision-making and present exciting avenues for facilitating appropriate reliance and improving human-AI collaboration.