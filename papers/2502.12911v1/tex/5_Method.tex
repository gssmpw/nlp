\section{Knapsack Optimization-based Schema Linking Agent (KaSLA)} \label{sec:Knapsack Optimization-based Schema Linking Agent (KaSLA}

In this section, we introduce the proposed KaSLA framework, which focuses on ensuring the linking of relevant schema elements to avoid missing and minimizing the inclusion of redundant ones. As illustrated in Figure~\ref{fig: KaSLA main figure}, We begin by detailing a binary-probabilistic score function used for assessing the relevance of schema elements and an estimation function for the redundancy tolerance of each schema linking process. We then outline the knapsack optimization approach to link potential relevant element while reducing the potential redundant ones under the tolerance constraint. Finally, we describe the hierarchical schema linking process, demonstrating its efficiency for reducing the column linking candidate.

\subsection{Relevance Estimation}\label{sec:Relevance Estimation}
To precisely assess the relevance of each schema element $s$ in relation to the given query $q$, we employ a hybrid binary-probabilistic score function. This function integrates a generative model for binary scoring alongside an encoding model for probabilistic scoring, allowing for a comprehensive estimation of relevance.

\paragraph{Binary scoring function.}
This component is tasked with providing binary relevance assessments, determining whether a schema element is relevant or not in a straightforward manner:
\begin{equation}
f_{\text{binary}}: (q, \gS_q) \mapsto \{r^b_1, \cdots, r^b_{|\gS_q|}\},
\end{equation}
where $r^b_i \in \{0, 1\}$ represents the binary score assigned to the $i$-th element $s_i \in \gS_q$. A score of 1 indicates relevant while 0 signifies redundant.

Considering efficiency, we finetune a lightweight LLM, DeepSeek-Coder-1.3B~\citep{guo2024deepseek}, using LoRA~\citep{hu2021lora}, a well-established parameter-efficient technique. We employ a strategy whereby the initial generation of a simulated SQL provides a contextual foundation for the subsequent generation of schema linking. The loss function during fine-tuning is defined as follows:
\begin{equation}
\mathcal{L}_{\text{binary}} = - \sum_{q \in Q} \log P({\text{SQL}}^*_q, \mS_q^* \mid q, \gS_q),
\end{equation} where $Q$ is the set of queries in the training dataset. ${\text{SQL}}^*_q$ and $\mS^*$ are the ground truth SQL and linking result, respectively. This formulation utilizes a joint generation of a simulated SQL and the schema linking result, thereby ensuring that the linking generation benefits from the contextual alignment provided by the simulated SQL during both fine-tuning and inference. Noted that the simulated SQL generated during this process will not be utilized in subsequent steps. The input and output examples are provided in Table~\ref{tb: Input formats for generation with LLMs} in Appendix~\ref{section: Input formats for generation with LLMs}.

\paragraph{Probabilistic scoring function.}
In conjunction with the binary scoring model, we introduce a probabilistic scoring model to enhance prediction robustness. The primary motivation is to mitigate the risk of the binary model misclassifing relevant elements. The probabilistic scoring model assigns a soft score to each element as the linking probability to reduce the likelihood of relevant elements missing , thereby supporting the binary model:
\begin{equation}
f_{\text{prob.}}: (q, \gS_q) \mapsto \{r^p_1, \cdots, r^p_{|\gS_q|}\},
\end{equation}
where  $r^p_i \in [0, 1]$ represents the probabilistic score assigned to the $i$-th element $s_i \in \gS_q$.

For the probabilistic scoring model, we utilize an encoding model, RoBERTa-Large~\citep{liu2019roberta}, to derive the semantic embeddings for the query and all schema elements. Following the methodology of \citep{li2023resdsql,li2024codes}, we employ a cross-attention network to jointly embed the semantic representations of tables and their columns. The dot product of the query embedding and the element embedding yields the final probabilistic score. This probabilistic model is trained using the presence of $s$ in $\mS^*$ as the learning objective. The loss function is defined as follows:
\begin{equation}\label{eq:training loss of recall model}
\mathcal{L}_{\text{prob.}} = \sum_{q \in Q} \text{FL}(\mathbbm{1}(s \in \mS^*_q), f_{\text{prob.}}(q,\gS_q)),
\end{equation}
where $\text{FL}(\cdot)$ denotes the focal loss function \citep{ross2017focal}, designed to emphasize learning from hard negative samples.

\paragraph{Relevance scoring estimation.}
The final function for estimating the relevance of each schema element integrates the scores derived from both the binary and probabilistic functions:
\begin{equation}\label{Relevance scoring estimation}
\begin{split}
    f_{\text{relevance}}:& (q, \gS_q) \mapsto \{r_1, \ldots, r_{|\gS_q|}\} \\
    & = \left\{\min(1, r^b_i + r^p_i) \mid s_i \in \gS_q \right\},
\end{split}
\end{equation}
where $r_i$ denotes the relevance score assigned to the $i$-th schema element $s_i \in \gS_q$ given the query $q$. An upper limit of 1 is imposed on $r_i$ to prevent excessively strong contrasts between potentially relevant elements, as each relevant schema element is equally important for accurate SQL generation.


\subsection{Redundancy Estimation}
To minimize the inclusion of redundant elements during schema linking, we define both a redundancy score for each schema element $s \in \gS_q$ and the upper redundancy tolerance for query $q$.

\paragraph{Redundancy scoring estimation.}
Recognizing that elements with lower relevance scores are more likely to be redundant, whereas those with higher scores are more essential, we assign a redundancy score $w_i$ to each schema element $s_i \in \gS_q$ in relation to query $q$. This score is calculated as the inverse of its relevance score $r_i$, formulated as follows:
\begin{equation}
w_i = {r_i}^{-1}.
\end{equation}
It ensures elements with lower relevance scores contribute more to redundancy, thereby effectively prioritizing more relevant schema elements.

\paragraph{Upper Redundancy Tolerance.}
Redundancy tolerance defines the limit on the total redundancy score of elements in the schema linking results, as an excess of redundant elements can significantly disrupt subsequent SQL generation. Estimating it during inference is challenging. For the SQL generation of some queries, a lenient tolerance may be beneficial since it allowing the inclusion of more potentially relevant elements in the linking results to prevent missing. However, for other queries, a lenient tolerance could introduce to many redundant elements, which may significantly disrupt the final SQL generation process.

To address this challenge, we hypothesize that similar queries exhibit comparable tolerance levels for redundancy and leverage queries from the training dataset, where ground truth linking results are available, to aid in predicting redundancy tolerance for given query \(q\) during inference.
We first retrieve the top-\(K\) queries in the training dataset that closely resemble \(q\) using sentence similarity measures~\citep{gao2021simcse}. For a query \(q_k\) in the retrieved top-\(K\) queries, we calculate the total redundancy score for its ground truth linking results $\gS^{*}_{q_k}$  
and use it as its upper redundancy tolerance. The maximum of these upper redundancy tolerances from the \(K\) most similar queries in the training dataset is then used as the redundancy tolerance estimate for query \(q\):
\begin{equation}
u_q = \max_{q_k \in \text{TopK}(q)} \bigg( \sum_{s_j \in \gS^{*}_{q_k}} w_j\bigg)
\end{equation}
By capturing the maximum sum from the most similar queries, this approach effectively defines an upper bound on redundancy tolerance that is both efficient and adaptable, ensuring that while redundant elements are excluded, the risk of missing relevant elements is minimized.




\subsection{Schema Linking with Knapsack Optimization}
Given the relevance score $r_i$, the redundancy score $w_i$ for each $s_i \in \gS_q$, and the upper redundancy tolerance $u_q$ for query $q$, we introduce a hierarchical schema linking strategy with knapsack optimization. This approach is designed to ensure the linking of relevant schema elements to avoid missing and minimize the inclusion of redundant ones for a precise schema linking.

\paragraph{Knapsack Optimization}
The 0-1 knapsack problem~\citep{freville2004multidimensional} is a classic combinatorial optimization challenge. It involves selecting items from a finite set, each item characterized by a specific value and weight, to place into a knapsack with a limited weight capacity. The objective of 0-1 knapsack optimization is to choose items that maximize the total value while ensuring the total weight remains below the knapsack's capacity.

Inspired by this, we develop a knapsack optimization-based schema linking strategy to ensure the inclusion of relevant schema elements while minimizing redundant ones, thus ach ieving optimal schema linking. Formally, the optimization objective are defined as follows:
\begin{equation}
\begin{split}
    f_{\text{Knap}}:& (q, \gS_q) \mapsto \widehat{\mS}_q  = \arg\max \sum_{s_i \in \widehat{\mS}_q} r_i \\&  \text{s.t.} \quad \sum_{s_i \in \widehat{\mS}_q} w_i \leq u_q.
\end{split}
\end{equation}
This formulation aims to maximize the total relevance scores of the linked schema elements. The constraint ensures that the cumulative redundancy score remains within the upper redundancy tolerance of query $q$. To tackle this optimization problem and achieve the optimal linking result, We employ an efficient dynamic programming approach. Detailed pseudocode of this dynamic programming algorithm is provided in Algorithm~\ref{sec: Implementation Details} in Appendix.

\paragraph{Hierarchical Linking Strategy}
For efficiency, our KaSLA employs a hierarchical process in schema linking: initially linking tables and subsequently linking columns within each selected table. This strategy effectively reduces the dimensionality of candidate columns, enhancing KaSLA's capacity to handle large-scale schema linking and text-to-SQL generation tasks.

In the initial phase for a given query $q$, KaSLA aims to identify the relevant tables from the set of all available tables in schema $\gS_q$, denoted as $\gT_q$:
\begin{equation}
\widehat{\mT}_q  = f_{\text{Knap}}(q, \gT_q).
\end{equation}
After determining tables, for each $t \in \widehat{\mT}_q$, let $\gC^t_q$ represent the columns in table $t$. KaSLA then optimizes column linking within each selected table:
\begin{equation}
\widehat{\mC}^t_q  = f_{\text{Knap}}(q, \gC^t_q).
\end{equation}
The final schema linking result $\widehat{\mS}_q$ is the union of all selected tables and their corresponding selected columns, represented as:
\begin{equation}
\widehat{\mS}_q = \widehat{\mT}_q \cup \bigcup_{t \in \widehat{\mT}_q} \widehat{\mC}^t_q.
\end{equation}
We provide a complexity analysis is detailed in Appendix~\ref{sec: Complexity Analysis}, offering insights into KaSLA's implementation and computational efficiency.




















