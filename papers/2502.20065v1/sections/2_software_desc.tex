\section{Software description}
\label{sec:software_description}
    
    Finding efficient route choice strategies for a group of vehicles (like a fleet of AVs) in a mixed traffic system is a dynamic optimization problem. RouteRL models this as a multi-agent decision-making task and integrates it with MARL.
    %finds and tests efficient route choice strategies for a group of vehicles (like a fleet of AVs) as a partially observable stochastic game (POSG)
     RouteRL main class (\texttt{TrafficEnvironment}) adheres to PettingZoo Agent Environment Cycle (AEC) API, providing a standardized interface for MARL. Experimenting using RouteRL is similar to working with any standard PettingZoo environment (as demonstrated with the code snippet below). The training pipeline involves an environment instantiation and training policies by interacting with it. Moreover, the MARL training procedure fully integrates with TorchRL, enabling users to leverage optimized and actively maintained implementations of state-of-the-art MARL algorithms, while preserving customizability.
    
    By integrating widespread frameworks (such as PettingZoo, TorchRL, and TraCI) and standard input/output formats (keyword arguments, \texttt{OSM} files, \texttt{CSV} files), RouteRL ensures interoperability across frameworks and streamlines experiment setup and data analysis. Figure \ref{fig:training_flow} illustrates a high-level workflow for conducting MARL training using RouteRL.
    
    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.9\textwidth]{figures/diagrams/RouteRL.png}
        \caption{
        \textbf{A typical multi-agent training pipeline using RouteRL}: The training procedure alternates between two stages: (i) \textit{Sampling} (left), where traffic microsimulator (SUMO) returns rewards resulting from given route choices made by all agents, and (ii) \textit{Training} (right), where the collected experiences are used to train AV policies via TorchRL’s MARL algorithm implementations. Policy parameters are iteratively updated and fed back into the sampling loop.
        }
        \label{fig:training_flow}
    \end{figure}
    
    
    
    \subsection{Software architecture}
    \label{sec:architecture}
    % Give a short overview of the overall software architecture; provide a pictorial overview where possible

        RouteRL is a Python open-source package built with a modular design, where each component serves distinct but interconnected purposes, and their modules decompose responsibilities into isolated units. It has three subpackages: \texttt{environment} (contains components related to the environment's workflow), \texttt{human\textunderscore learning} (includes human route choice and learning models), and \texttt{services} (I/O operations and data visualization). The UML class diagram in \ref{sec:class_diagram} illustrates the class hierarchy of RouteRL, while the sequence diagram in \ref{sec:seq_diagram} depicts the user interaction.
        Figure \ref{fig:routerl_flow} illustrates the workflow of \texttt{RouteRL}, showing the interactions during a typical experiment run and the resulting input-output flow. 
        
        The \texttt{environment.TrafficEnvironment} class functions as the central orchestrator and a MARL environment. It provides users with an intuitive interface for key functionalities, such as: 
        \begin{compactitem}
            \item \textbf{Agent-environment interaction}: Functioning as a MARL environment and handling agent interactions for training.
            \item \textbf{Simulation control}: Through a \texttt{SumoSimulator} object, handling the simulation control, executing agent actions in SUMO, and retrieving episode statistics for training.
            \item \textbf{Demand generation}: Generating travel demand data (origin, destination, and departure of each agent) synthetically or importing from user-provided \texttt{CSV} files.
            \item \textbf{Mutation}: Handling how human agents transition into AVs and initiating MARL training loop.
            \item \textbf{Data recording and visualization}: With a \texttt{Recorder} object storing episode data during training, used (after the training) by a \texttt{Plotter} object to generate insightful summaries with data visualizations (as depicted in \ref{sec:plots}).  
        \end{compactitem}
        
        
    
    \subsection{Software functionalities}
    Key RouteRL functionality is to run experiments like the one depicted in Figure \ref{fig:routerl_flow} with a unified workflow, flexibility in parameterization, and the reproducibility needed for scientific rigor. Users can configure custom scenarios via keyword arguments during environment initialization (as detailed in our online documentation). Possible customizations include specifying the network, demand pattern, human route choice model, mutation day, the share of AVs, and their behaviors. Moreover, users can implement and experiment with their custom human behavioral models and custom MARL implementations. %, not necessarily conforming to TorchRL’s standards. 
    At the end of a training, RouteRL outputs raw and aggregated results.
    %Users can hyper-parameterize the MARL algorithm, and control the length of training (episodes), observation space, and testing phase.

        \subsubsection{Modeling the decision process}
        \label{sec:interactions}
            From an individual agent’s perspective, daily route choice is a single-step decision: each agent makes a route choice, and the episode ends upon their arrivals at their destinations. The environment is built on the PettingZoo AEC API, therefore the agent interactions are sequential (in order of departure time). This allows agents to observe the most recent traffic conditions before making route choice decisions.
            \emph{State transitions} are governed by SUMO, which is a state-of-the-art, microscopic, agent-based traffic simulator. Each vehicle navigates the road network according to the Intelligent Driver Model (IDM) \cite{Treiber_2000}. In each episode, SUMO simulates traffic flow dynamics based on individual route choices and computes agents' travel times.
            
            For each agent, the \emph{action space} includes discrete route options, created with a dedicated path generator tool JanuX \cite{janux}. 
            The \emph{observation} includes past route choices from earlier timesteps within the same episode. Users can also incorporate departure times into observations, which is useful for centralized or shared-parameter training.
            The AV \emph{reward signal} is designed to represent a selected AV behavior. RouteRL provides users with a selection of AV route choice behaviors to experiment with (see Table \ref{tab:behaviors}), defined through different AV reward formulations (adopted from \cite{akman_impact, jamroz}). 
            \input{tables/behaviors}

        \subsubsection{Human learning}
        \label{sec:humans}
            The \texttt{human\textunderscore learning} package includes three state-of-the-art discrete route choice models: \texttt{Gawron} \cite{gawron}, \texttt{CULO} \cite{culo}, and \texttt{WeightedAverage} \cite{cascetta}. These models, popular within the transportation community, emulate human agents as \textit{utility maximizers}, where individual utilities are influenced by individual characteristics \cite{cascetta}—unlike MARL algorithms that primarily focus on cost minimization.

        \subsubsection{Traffic networks}
        \label{sec:networks}
            RouteRL includes eleven traffic networks varying in size and characteristics, including: "arterial", "cologne", "grid", "ingolstadt" (from RESCO benchmark \cite{resco}), "grid6", "square" (from SUMO's repository \cite{sumo_repo}), "ortuzar", "nguyen" (from SUMO-RL \cite{sumorl}), "manhattan", "two\_route\_yield" and "csomor" networks (first included in RouteRL). Network layouts are shown in our online documentation.
    

        \subsubsection{Reproducibility}
        \label{sec:reproducibility}
            Providing a fixed random seed to \texttt{TrafficEnvironment} ensures complete experiment reproducibility. The seed is used to initialize all stochastic events within the environment, including simulation dynamics (SUMO), demand generation, route generation (JanuX), and human decision-making. This facilitates reliability for benchmarking, debugging, and comparative analysis, which are crucial for scientific impact.
      
    \subsection{Sample code snippets analysis}
    \label{sec:snippets}
    
        Listing \ref{code:snippet} demonstrates how a user interacts with RouteRL to conduct training on a standard MARL algorithm implementation via TorchRL.
    
%\begin{minipage}{\textwidth}
\begin{lstlisting}[
style=customStyle, 
caption={\textbf{Standard RouteRL usage}: In this simplified code, the user initializes the environment, imports the road network and demand pattern (agents) (1), connects with SUMO (2), simulates learning of human drivers (3) which, after mutation (4), interact with the environment and collect data (5) to train MARL policies (6), which are finally tested and reported (7).}, 
label={code:snippet}]
# (1) initialize the traffic environment
env = TrafficEnvironment(seed=42, **env_params)

# (2) start the connection with SUMO
env.start()

# (3) human learning 
for episode in range(human_learning_episodes):
    env.step()

# (4) some human agents transition into AV agents
env.mutation()

# (5) collects experience by running the policy in the environment (TorchRL)
collector = SyncDataCollector(env, policy, ...)

# (6) training of the AVs
for tensordict_data in collector:        
    # update the policies of the learning agents
    for _ in range(num_epochs):
      subdata = replay_buffer.sample()
      loss_vals = loss_module(subdata)
      optimizer.step()
    collector.update_policy_weights_()

# (7) testing phase and storing results
policy.eval()
for episode in range(100):
    env.rollout(len(env.machine_agents), policy=policy)
env.plot_results() # plot the results
env.stop_simulation() # close the connection with SUMO
\end{lstlisting}
%\end{minipage}