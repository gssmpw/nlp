\section{Illustrative examples}
\label{sec:examples}


RouteRL, thanks to the above functionalities, can be impactful for future scientific discoveries. We demonstrate it with three distinct traffic scenarios on the urban networks of Cologne, Ingolstadt, and Manhattan (Figure \ref{fig:networks}), which represent small, medium, and real-size problem instances respectively. As illustrated in Figure \ref{fig:routerl_flow} and the code snippet from the previous section, each experiment starts with only human agents, each learning the subjectively optimal route choices for 100 episodes (representing 100 days of commute). Every day agents choose from action space (routes between given origin-destination points), generated with JanuX, as illustrated in Figure \ref{fig:routes}. When human agents stabilize their choices (after 100 days), RouteRL is ready to introduce AV agents into the system. Typically, a predefined share of human drivers transition into AVs and start learning to find an efficient strategy using different MARL algorithms. After optimizing the policy to maximize expected rewards, the AVs enter the testing phase, where they execute 100 episodes to sample the performance of the learned policy.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/networks/cologne.png}
        \caption{Part of Cologne network.}
        \label{fig:cologne_network}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/networks/ingolstadt.png}
        \caption{Part of Ingolstadt network.}
        \label{fig:ingolstadt_network}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.7\textwidth]{figures/networks/manhattan.png.png}
        \caption{Manhattan network.}
        \label{fig:manhattan_network}
    \end{subfigure}
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{figures/plots/0_0.png}
        \caption{JanuX route generation in Cologne network.}
        \label{fig:routes}
    \end{subfigure}
    \caption{\textbf{Networks}: Small (part of Cologne), medium (part of Ingolstadt), and big (Manhattan) networks, which agents (human drivers and AVs) traverse to reach their destinations in illustrative experiments. Every day, agents choose routes from discrete options, illustrated on (d) for the Cologne network.}
    \label{fig:networks}
\end{figure}

We use the MARL algorithm implementations from TorchRL appropriate for respective experimental settings. For selfish AVs (Table \ref{tab:behaviors}), where agents are independent, we employ Independent Q-Learning (IQL) algorithm \cite{iql} as the initial baseline, and the Independent Proximal Policy Optimization (IPPO) algorithm \cite{ippo}, which proved efficiency in similar tasks \cite{yu2022surprisingeffectivenessppocooperative, papoudakis2021benchmarkingmultiagentdeepreinforcement}. For malicious strategies, where the reward signal is shared, we use algorithms designed for collaborative tasks: Value Decomposition Networks (VDN) \cite{vdn} (factorizes the joint Q-function as the sum of individual Q-values) and QMIX (adopts a mixing network with a monotonicity constraint) \cite{qmix}. Additionally, we report experiments with the multi-agent version of the Proximal Policy Optimization (PPO) algorithm \cite{ppo}, Multi-Agent PPO (MAPPO) \cite{mappo}.
Presented experiments, along with additional ones, are reproducible and described as tutorials on our repository, showcasing various experiment setups using RouteRL.


% In these experiments, AV agents pursue the reward as described in Table \ref{tab:behaviors}. Selfish AVs aim to minimize their own travel time and do not rely on shared information or coordination with other agents. For this reason, we employ independent MARL algorithms from the TorchRL library, including the Independent Q-Learning (IQL) algorithm \cite{iql} as initial baseline, the Independent Proximal Policy Optimization (IPPO) algorithm \cite{ippo}, as it has shown strong benchmark performance across various tasks \cite{yu2022surprisingeffectivenessppocooperative, papoudakis2021benchmarkingmultiagentdeepreinforcement}, and the Independent SAC (ISAC), which is the multi-agent version of the Soft Actor-Critic (SAC) algorithm \cite{sac}. 



% For the malicious behavior, all AV agents have the same reward value in each episode. Therefore, we use algorithms designed for collaborative tasks, including the Value Decomposition Networks (VDN) \cite{vdn} and the QMIX that adopts a mixing network with a monotonicity constraint \cite{qmix}. Additionally, we report experiments with the multi-agent version of the Proximal Policy Optimization (PPO) algorithm \cite{ppo}, Multi-Agent PPO (MAPPO) \cite{mappo}. 






%% Janux screenshots
\begin{comment}
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.25\textwidth]{figures/plots/0_0.png}
    \caption{Illustration of action space: Three paths generated with \texttt{JanuX}) between selected origin-destination pairs in Cologne.}
    \label{fig:routes}
\end{figure}
\end{comment}


\begin{comment}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/0_0.png} % I've removed , trim=8cm 0cm 0cm 0cm, clip options
        \caption{}
        \label{fig:od_0_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/0_1.png}
        \caption{}
        \label{fig:od_0_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/1_0.png}
        \caption{}
        \label{fig:od_1_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.24\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/1_1.png}
        \caption{}
        \label{fig:od_1_1}
    \end{subfigure}
    \caption{Three distinct paths between different origin-destination node pairs in the Cologne network. The color gradient intensifies as the path progresses from the origin to the destination, visually indicating proximity to the destination.}
    \label{fig:routes}
\end{figure}
\end{comment}


\begin{comment}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/0_0.png} % I've removed , trim=8cm 0cm 0cm 0cm, clip options
        \caption{Origin destination points 0 - 0.}
        \label{fig:od_0_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/0_1.png}
        \caption{Origin destination points 0 - 1.}
        \label{fig:od_0_1}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/1_0.png}
        \caption{Origin destination points 1 - 0.}
        \label{fig:od_1_0}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=0.65\textwidth]{figures/plots/1_1.png}
        \caption{Origin destination points 1 - 1.}
        \label{fig:od_1_1}
    \end{subfigure}
    \caption{Three distinct paths between different origin-destination node pairs in the Cologne network. The color gradient intensifies as the path progresses from the origin to the destination, visually indicating proximity to the destination.}
    \label{fig:routes}
\end{figure}
\end{comment}


\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/action_shifts_rewards.png}
    \caption{\textbf{Results from the experiment on the Cologne network.} 40 out of 100 human drivers transition into AVs with `malicious` behavior. %The left plot displays the route choice probabilities of each group of agents, human drivers, AVs, and both. 
    These results suggest that actions made by AVs in the Cologne network can be different from those made by humans (left) and AV's actions will not only make AVs arrive faster but also make the humans travel longer (right).}
    \label{fig:action_shifts_travel_times}
\end{figure}


\begin{comment}
\begin{figure}[ht]
\centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/actions_shifts.png}
        \caption{Agents route choice probabilities for three selected agents before and after mutation. \texttt{RouteRL} suggests that actions made by autonomous vehicles will be different than those made by humans. }
        \label{fig:action_shifts}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/travel_times.png}
        \caption{In Cologne, the AVs actions will not only make them arrive faster, but also the humans will travel longer.}
        \label{fig:travel_times}
    \end{subfigure}
    \caption{Results from the experiment on the Cologne network. Initially, there are 100 human agents in the network, and 40 of these agents transition into AVs with `malicious` behavior.}
    \label{fig:action_shifts_travel_times}
    \end{figure}
\end{comment}

\begin{comment}
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/ingolstadt_selfish.png}
        \caption{Ingolstadt network, where 20 out of 100 human agents transition to AVs.}
        \label{fig:ingolstadt_selfish}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/cologne_malicious.png}
        \caption{Cologne network, where 40 out of 100 human agents transition to AVs.}
        \label{fig:cologne_malicious}
    \end{subfigure}
    \caption{Mean rewards of AV agents.}
    \label{fig:mean_rewards}
\end{figure}
\end{comment}

\paragraph{Will AVs arrive earlier than human drivers in Cologne?}
Results from RouteRL experiment on our Cologne network reveal that the introduction of AVs into urban traffic can influence human agents' decision-making and increase their travel time. 
Figure \ref{fig:action_shifts_travel_times} illustrates the fraction of drivers split among available routes and the average travel time of all the agents in the system. Such insight highlights the need for research towards a better understanding of the effects of AV introduction, such as increased congestion or $\text{CO}_\text{2}$ emissions.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/rewards_cologne_ingolstadt.png}
    \caption{\textbf{Mean rewards of AVs  with various algorithms and strategies.} In Ingolstadt, 20 out of 100 human agents transition to selfish AVs (left), and in Cologne, 40 out of 100 human agents transition to malicious AVs (right). The results show IQL and VDN yield superior returns in their respective experiments.
}
    \label{fig:mean_rewards}
\end{figure}
%% iN manhatan there is advantage of being CAV regardless of reward formulation 
%however the advantage it is descreasing for bigger market shares
%remove 50%

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/cav_advantage.png}
    \caption{
    \textbf{AV advantage:} In the Manhattan network (Figure \ref{fig:manhattan_network}), being an AV provides an advantage regardless of the reward formulation. However, this advantage decreases with larger market shares. Y-axis is the ratio of the mean travel time of human drivers to that of AV agents, values greater than 1 indicate that being an AV is more advantageous than being a human driver.
    }
    \label{fig:cav_advantage}
\end{figure}


\paragraph{Which RL algorithms are efficient for collaborative and individual routing problems?}
AV agents can use different MARL algorithms to optimize their choices, each stabilizing in different solutions (Figure \ref{fig:mean_rewards}). With RouteRL, one can compare a variety of state-of-the-art MARL algorithms and benchmark them. Figure \ref{fig:mean_rewards} shows that while IQL worked best for individual reward, VDN performed best in the collaborative task. 

\paragraph{Will the benefit of AVs over humans increase with their growing share?}
%Apart from answering this question, here we apply RouteRL on a real-size network of Manhattan.
Figure \ref{fig:cav_advantage} shows the ratio of average travel times of humans over AVs in our Manhattan experiment. %the outcomes of replacing a fraction of human drivers with AVs. 
In all tested AV behaviors, being an AV allows arriving earlier than human drivers (values greater than one). However, this benefit decreases when the share of AVs increases from 12.5\% to 20\%.




The sample results presented in this section highlight the significance, flexibility, and applicability of the RouteRL framework. RouteRL supports experiments with varying numbers of agents, urban networks, reward formulations, and share of AVs.%, and different learning dynamics, where some groups of agents learn while others rely on prior knowledge, or all agents learn simultaneously. 
%Additionally, RouteRL facilitates experimentation with various MARL algorithms for training AVs and different human behavior models to enhance the realism of human decision-making in simulations. With these capabilities, RouteRL serves as a suitable framework for studying the impact of AVs on future cities, providing an adaptable environment to analyze their interactions with human drivers and urban infrastructure.





%RouteRL allows us to experiment with varying numbers of agents (both human drivers and AVs), explore different reward formulations imposed on AVs by the fleet operator, and test various MARL algorithms available in the TorchRL library. 


%impact of malicious, selfish fleets. Test different state-of-the art algorithms.


\begin{comment}
To demonstrate the capabilities of RouteRL, we showcase a traffic scenario with 20 agents. The network used in this scenario is illustrated in Figure \ref{fig:two_route_network}. Initially, the environment consists exclusively of human agents, each learning the optimal route for 100 episodes, converging towards a Nash equilibrium \cite{nash1951noncooperative}. Subsequently, 50\% of the human agents transition into AVs, which begin learning for an additional 400 episodes using the Independent Proximal Policy Optimization (IPPO) algorithm \cite{ippo}. During this period, human agents cease learning.  In the final phase, learning is re-enabled for the human agents while the AVs cease adapting offering valuable insights into how various AV behaviors influence the travel decisions of human drivers.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/two_route_yield.png}
        \caption{Two-route network}
        \label{fig:two_route_network}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plots/two_route_yield.png}
        \caption{Description of second plot}
        \label{fig:second_plot}
    \end{subfigure}
    \caption{Comparison of the two-route network and second example.}
    \label{fig:combined_plots}
\end{figure}




Figure \ref{fig:action_shifts} depicts the action shifts of the agents included in the system across episodes, with the experiment repeated for each distinct AV behavior. Figure \ref{fig:rewards} illustrates the average rewards of both human and AV agents for each configurations. In the malicious case, when human learning is reintroduced after the machine learning phase, some human agents revise their decisions, revealing interesting interactions between agents. 
\end{comment}

\begin{comment}
\begin{figure}[htbp]
    \centering
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/plots/actions_shifts_selfish.png}
        \caption{Action shifts of the agents during training. This plot depicts the agents that have origin-destination points 0-0.}
        \label{fig:actions_shifts_ppo}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=0.8\linewidth]{figures/plots/rewards_mappo_selfish.png}
        \caption{Rewards of the agents during training.}
        \label{fig:rewards_ppo}
    \end{subfigure}
    \caption{Plots of the reward trends and action shifts for selfish agents during training.}
    \label{fig:side_by_side_plots}
\end{figure}
\end{comment}


\begin{comment}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/action_shifts.png}
    \caption{Action shifts of agents within the system across episodes. The x-axis represents the number of agents scaled according to the size of their group.}
    \label{fig:action_shifts}
\end{figure}

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{figures/plots/rewards.png}
    \caption{Average rewards of human and AV agents across episodes.}
    \label{fig:rewards}
\end{figure}
\end{comment}
