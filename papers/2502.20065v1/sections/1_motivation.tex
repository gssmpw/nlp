\section{Motivation and significance}\
\label{sec:motivation}

Autonomous vehicles (AVs) will soon be ready to enter our cities and make routing decisions \cite{av_predictions, urban_future}. Using computational power, access to data, and real-time information, AVs will be able to develop efficient route choice strategies to reach their destinations. Presumably, their route choices will deviate from those made by human drivers, currently using our urban networks. Yet the routing strategies to be adopted by AVs as well as their impact on the urban traffic systems and other users (human drivers) remain unknown. 

To better understand this newly arising class of problems, we introduce the RouteRL framework. RouteRL reproduces the traffic flow on urban networks (simulated with SUMO \cite{SUMO}) shared by human drivers and (connected) autonomous vehicles. Humans, while selecting routes, maximize perceived utility (reward), represented with state-of-the-art behavioral route-choice models. Meanwhile, AVs employ various multi-agent reinforcement learning (MARL) algorithms to develop efficient route-choice strategies. Every day, each agent selects subjectively optimal routes on consecutive days of the learning process. RouteRL simulates how experienced humans and trained MARL agents traverse urban networks, and eventually, outputs a series of system performance indicators.

RouteRL allows for a broad experimental schema to investigate the impact of the future of AVs on urban networks. The pipeline of a typical RouteRL experiment, like the ones presented in the paper, is depicted in Figure \ref{fig:routerl_flow}. Given a road network (from OpenStreetMap (OSM) \cite{OpenStreetMap}) and a demand pattern (agents' origins, destinations, and departure times), RouteRL first computes the day-to-day route-choice process of human agents that eventually achieve stabilization (though not necessarily reaching a Nash User Equilibrium \cite{wardrop_road_1952}). Then, a predefined portion of human drivers \emph{mutate} to AVs. Notably, unlike most recent research, here AVs have the same traffic properties (acceleration rate, gap acceptance, etc.) as human drivers - which allows disentangling effects of optimized route choice from improved traffic performance. AVs may use a variety of MARL algorithms either from TorchRL \cite{torchrl} or from custom implementations to train their policies (individually or collaboratively) and maximize their rewards (representing behaviors ranging from selfish to social or malicious).
Specifically, each AV every day selects from the finite action space (possible routes) the optimal (in terms of expected reward) route to reach its destination under the current state observation. This choice is either individual or managed by some central fleet manager. Agents do not change the action within the day and follow the chosen route up to the destination, having a chance to switch routes only on the following day (episode). Depending on the scenario, humans may have a chance to adapt their decisions, follow the previous choices, or transition into AVs,~each possibly triggering new day-to-day dynamics. RouteRL records travel times, rewards, and space-time trajectories of individual vehicles, aggregating them into key performance indicators (KPIs) to assess the impact on system performance and different user groups.


\begin{figure}[ht]
                \centering
                \includegraphics[width=0.99\textwidth]{figures/diagrams/routerl_flow.png}
                \caption{
                    \textbf{The software at a glance:} 
                    RouteRL can simulate any OSM network on which a set of agents demands to reach their destinations. SUMO traffic microsimulator simulates how vehicles traverse the network. In a sequence of days (episodes), agents iteratively learn how to optimize their routing decisions (arrive faster). Humans use behavioral models, and AVs use policies trained with state-of-the-art MARL algorithms implemented via TorchRL.
                }
                \label{fig:routerl_flow}
            \end{figure}

Through such experiments, RouteRL allows investigating relevant questions arising with the introduction of AVs into our cities, such as:
\begin{compactitem}
    \item In the Cologne network, if 40\% of drivers switch to AVs and collaboratively maximize human travel time using the QMIX algorithm \cite{qmix} what will be the impact on total travel times (Figure \ref{fig:action_shifts_travel_times})? 
    \item In the Ingolstadt network, if 20\% of drivers transition to AVs and individually maximize their reward, what is the most efficient algorithm (Figure \ref{fig:mean_rewards}a)?
    \item In the Manhattan network, if 12.5\% of users become altruistic  AVs (aim to minimize travel time of all the drivers in the network), is it more advantageous to remain a human driver or switch (Figure \ref{fig:cav_advantage})?
\end{compactitem}

It is fundamental to address this class of problems before allowing AVs to integrate into urban networks. Upfront experiments with RouteRL are instrumental in informing: 
\begin{compactitem}
    \item policymakers (on potential consequences for system performance),
    \item transportation engineers (on impact on urban traffic), and
    \item the machine learning community (on the efficiency of (MA)RL algorithms for route choice).
\end{compactitem}