\begin{figure}[!h]
    \centering
    \vspace{-1em}
    \includegraphics[width=0.6\linewidth]{images/f1e.pdf}
    \vspace{-.5em}
    \caption{Sample images generated by different methods on the HPSv2, Geneval, and Pickscore benchmarks. After fine-tuning SD 2.1 with \( \text{SFT}_{\text{Chosen}} \), Diffusion-DPO, MaPO, and DCPO on Pick-a-Picv2 and Pick-Double Caption datasets, DCPO produces images with notably higher preference and visual appeal (See more examples in Appendix \ref{sec:app_additional_examples}).}
    \label{fig:enter-label}
    \vspace{-1em}
\end{figure}

\section{Introduction}


\begin{figure}[!t]
    \centering
    
    \includegraphics[width=1\linewidth]{images/pipelines-overview.pdf}
    \caption{The overview of our 3 Dual Preference Optimization (DCPO) pipelines: DCPO-c, DCPO-p, and DCPO-h, all of which require a duo of a captioned preferred image $(x^w_0, z^w)$ and a captioned less-preferred image $(x^l_0, z^l)$.  \textbf{DCPO-c (Top Left):} We use a captioning model to generate distinctive captions respectively for images  $x^w_0$ and $x^l_0$ given the shared prompt $c$. \textbf{DCPO-p (Bottom Left):} We take prompt $c$ as the caption for image $x^w_0$, then we use a Large Language Model (LLM) to generate a semantically perturbed prompt $z_p^l$ given prompt $c$ as the caption for image $x^l_0$. \textbf{DCPO-h (Right): } A hybrid method where the generated caption $z^l$ is now perturbed into $z_p^l$ for image $x^l_0$. Our \textit{Pick-Double Caption} Dataset discussed in Section \ref{sec:pick-double-caption} is constructed with the DCPO-c pipeline.}
    % \yiran{add more details}
    \label{fig:overview}
\end{figure}


Image synthesis models \citep{rombach2022high, esser2024scaling} have achieved remarkable advancements in generating photo-realistic and high-quality images. Text-conditioned diffusion \citep{song2020denoising} models have led this progress due to their strong generalization abilities and proficiency in modeling high-dimensional data distributions. As a result, they have found wide range of applications in image editing \citep{brooks2023instructpix2pix}, video generation \citep{wu2023tune} and robotics \citep{carvalho2023motion}. Consequently, efforts have focused on aligning them with human preferences, targeting specific attributes like safety \citep{liu2024latent}, style \citep{patel2024steering, Everaert_2023_ICCV}, spatial understanding \citep{chatterjee2024getting}, and personalization \citep{ruiz2023dreambooth}, thereby improving their usability and adaptability.

% Recently, text-to-image models have demonstrated remarkable performance across a range of tasks, including image, audio, and text generation. Their scalability makes them well-suited for large-scale applications, establishing them as state-of-the-art in image generation. The high capability of diffusion models, combined with their strong performance in various domains, has led to their widespread adoption in numerous human-centered applications. To better align with human preferences, these models are fine-tuned for specific attributes such as safety, style, and personalization, enhancing their usability and adaptability.

Similar to the alignment process of Large Language Models (LLMs), aligning diffusion models involves two main steps: \textbf{1.} Pre-training and \textbf{2.} Supervised Fine-Tuning (SFT). Recent fine-tuning based methods have been introduced to optimize diffusion models according to human preferences by leveraging Reinforcement Learning with Human Feedback (RLHF) \citep{ouyang2022training}, the aim of which is to maximize an explicit reward. However, challenges such as reward hacking have led to the adoption of Direct Preference Optimization (DPO) \citep{rafailov2024direct} techniques like Diffusion-DPO \citep{wallace2024diffusion}.  Intuitively, Diffusion-DPO involves maximizing the difference between a preferred and a less preferred image for a given prompt.

% Although DPO-based methods are effective in comparison to SFT-based approaches, applying direct optimization under multi-modal settings presents certain challenges.

% In this approach, the diffusion model works to maximize the difference between a preferred and less preferred image for a given prompt. Through repeated iterations, the model directly optimizes based on the data provided, enhancing its performance.

% Although DPO-based methods are effective in comparison to SFT-based approaches, applying direct optimization under multi-modal settings presents certain challenges. Current preference optimization datasets consist of a preferred ($x^w$) and a less preferred ($x^l$) image for a given prompt ($c$). Ideally, $x^w$ should show a high correlation with $c$, whereas $x^l$ should be unrelated. However, we find that in current datasets, both the images share the same distribution for the given prompt $c$, which we refer to as \textit{conflict distribution} in the data. This entails that there is a lack of sufficient distinguishing features between the two pairs ($x^w$, $c$), ($x^l$, $c$), thereby increasing the complexity of the optimization process. Overall, we identify two key challenges: \textbf{1.} conflict in data distribution, which contradicts the core idea of direct optimization, and \textbf{2.} \textit{irrelevant prompts} for less preferred images, which can hinder the learning process of the diffusion model during optimization.

Although DPO-based methods are effective in comparison to SFT-based approaches, applying direct optimization in multi-modal settings presents certain challenges. Current preference optimization datasets consist of a preferred ($x^w$) and a less preferred ($x^l$) image for a given prompt ($c$). Ideally, $x^w$ should show a higher correlation with $c$ compared to $x^l$. However, we find that in current datasets, both the images share the same distribution for the given prompt $c$, which we refer to as \textit{conflict distribution} in the data. Additionally, irrelative information in $c$ restricts the U-Net's ability to predict noises from $x^l$ in the diffusion reverse process, which we refer to as \textit{irrelevant prompts}.
This entails that there is a lack of sufficient distinguishing features between the two pairs ($x^w$, $c$), ($x^l$, $c$), thereby increasing the complexity of the optimization process. 

Overall, we identify two key challenges: \textbf{1.} \textit{conflict distribution} in data, which contradicts the core idea of direct optimization, and \textbf{2.} \textit{irrelevant prompts} for less preferred images, which can hinder the learning process of the diffusion model during optimization.

To address the aforementioned bottleneck, we propose \textbf{DCPO: Dual Caption Preference Optimization}, a novel preference optimization technique designed to align diffusion models by utilizing two distinct captions corresponding to the preferred and less preferred image. DCPO broadly consists of two steps - a text generation framework that develops better aligned captions and a novel objective function that utilizes these captions as part of the training process.

The text generation framework seeks to alleviate the \textit{conflict distribution} present in existing datasets. We hypothesize that $c$ does not serve as the optimal signal for optimization because they do not convey the reasons why an image is preferred or dis-preferred; based on the above, we devise the following techniques to generate better aligned captions. The \textit{first} method involves using a captioning model $Q_\phi(z^i|x^i, c)$; which generates a new prompt ${z^i}$ based on an image ${x^i}$ and the original prompt ${c}$, where $i \in (w, l)$. The \textit{second} method introduces perturbation techniques $f$, such that $c=z^w, z^l = f(c);$ i.e. generating $z^l$, to represent the less preferred image, considering the original prompt $c$ as the prompt aligned with the preferred image. We investigate multiple semantic variants of $f$, where each variant differs in the degree of perturbation applied to the original caption $c$. Finally, we also explore a hybrid combination of the above methods, where we combine the strong prior of the captioning model and the efficient nature of the perturbation method. All the above methods are designed to generate captions that effectively discriminate between the preferred and less preferred images.

We introduce a novel objective function that allows DCPO to incorporate $z^w$ and $z^l$ into its optimization process. Specifically, during optimization, the policy model $p_\theta$ increases the likelihood of the preferred image $x^w$ conditioned on the prompt $z^w$, while simultaneously decreasing the likelihood of the less preferred image $x^l$ conditioned on the prompt $z^l$. The results in Tables \ref{tab:pickapic} and \ref{tab:app_geneval_results} demonstrate that DCPO consistently outperforms other methods, with notable improvements of +0.21 in Pickscore, +0.45 in HPSv2.1, +1.8 in normalized ImageReward, +0.15 in CLIPscore, and +3\% in GenEval. Additionally, DCPO achieved 58\% in general preference and 66\% in visual appeal compared to Diffusion-DPO on the PartiPrompts dataset, as evaluated by GPT-4o (see Figure \ref{fig:evaluation_gpt4o}).

% \textbf{Add a sentence or two about the results. Mention improvements on PickScore. Mention improvements on other benchamrks such as GenEval etc etc}

% The \textit{first} method involves using a captioning model $Q_\phi(z^i|x^i, c)$; which generates a new prompt ${z^i}$ based on an image ${x^i}$ and the original prompt ${c}$, where $i \in (w, l)$. The \textit{second} method introduces perturbation techniques $f$, such that $c=z^w, z^l = f(c);$ i.e. generating $z^l$, to represent the less preferred image, considering the original prompt $c$ as the prompt aligned with the preferred image. We investigate multiple semantic variants of $f$, where each variant differs in the degree of perturbation applied to the original caption $c$. Both methods are designed to generate captions that effectively discriminate between the preferred and less preferred images.

% To incorporate the generated captions into the training process, we also introduce a modified version of Diffusion-DPO with the following optimization function: 

% \begin{equation*}
% \begin{split}
%     \mathcal{L}_{\text{DCPO}}(\theta) = -\mathit{\mathbb{E}}_{z^w, z^l, x^{w}_0, x^{l}_0} [\log \sigma( 
%     \beta \mathit{\mathbb{E}}_{x^{w}_{1:T},x^{l}_{1:T}}
%     [\log \frac{p_{\theta} (x^{w}|z^w)}{p_{\text{ref}}(x^{w}|z^w)} - \log \frac{p_\theta (x^{l}|z^l)}{p_{\text{ref}}(x^{l}|z^l)}])]
% \end{split}
% \end{equation*} 

% where policy model \(p_\theta\) maximum the likelihood of preferred image \(x^w\) given caption \(z^w\) and minimize the likelihood of less preferred image \(x^l\) given caption \(z^l\).

% \textbf{Mention the loss function here, add 1 equation and write the rest in words}
In summary, our contributions are as follows : 

% \textbf{Modify the below italicized paragraph as bullet points. Add the captioning methods/models here, such as LLaVA, Emu, T5-XXL}

\begin{itemize}

    \item \textbf{Double Caption Generation}: We introduce the Captioning and Perturbation methods to address the conflict distribution issue, as illustrated in Figure \ref{fig:conflict-distribution-figure}. In the Captioning method, we employ state-of-the-art models like LLaVA \citep{liu2024visual} and Emu2 \citep{sun2024generative} to generate a caption \( z \) based on the image \( x \) and prompt \( c \). Additionally, we use DIPPER \citep{dipper}, a paraphrase generation model built by fine-tuning the T5-XXL model to create three levels of perturbation from the prompt \( c \).
    
    \item \textbf{Dual Caption Preference Optimization (DCPO):} We propose DCPO, a modified version of Diffusion-DPO, that leverages the U-Net encoder embedding space for preference optimization. This method enhances diffusion models by aligning them more closely with human preferences, using two distinct captions for the preferred and less preferred images during optimization.
    
    \item \textbf{Improved Model Performance:} We demonstrate that our approach significantly outperforms SD 2.1, SFT, Diffusion-DPO, and MaPO across metrics such as Pickscore, HPSv2.1, GenEval, CLIPscore, normalized ImageReward, and GPT-4o \citep{achiam2023gpt} evaluations.
    
\end{itemize}




% \textit{{We demonstrate the effectiveness of DCPO through a detailed empirical evaluation, including both quantitative and qualitative analysis. In this work, we introduce the first dual-caption preference dataset generated using state-of-the-art captioning models based on the Pick a Picv2.1 dataset. We evaluate DCPO against Diffusion-DPO and MaPO methods across several metrics, including Pickscore, GenEval, HPSv2, ImageReward, and CLIPscore, all tested on Stable Diffusion 2.1. Our results show that DCPO outperforms other alignment methods on most metrics. Additionally, we experimentally prove that the dual-caption approach successfully mitigates \textit{conflict distribution} issue and that increasing the separation between the distributions of preferred and less preferred images positively impacts model performance.}}


% to generate a less preferred prompt $z^l$ from the original prompt \textit{c}, with three levels of perturbation defined and a new hypothesis proposed around this approach. These methods aim to mitigate the conflict distribution and improve model performance.

% We employed state-of-the-art captioning models, LLaVA 1.6-34B and Emu2-37B, ensuring that the token size of the generated prompt \textit{z} closely matches that of the original prompt \textit{c} for a fair comparison. 
% The \textit{second} method introduces a perturbation technique to generate a less preferred prompt \textit{z} from the original prompt \textit{c}, with three levels of perturbation defined and a new hypothesis proposed around this approach. These methods aim to mitigate the conflict distribution and improve model performance. To in






% Ideally, the data used for these models should include a preferred image and a less preferred image for the same prompt \textit{c}, with the less preferred image being unrelated to the prompt. [rewrite it] [However, our analysis shows that in traditional datasets, both preferred and less preferred images share the same distribution for a given prompt \textit{c}, which we refer to as a \textit{conflict distribution} in the data.]

% [Motivated by the goal of improving prediction accuracy in the reverse process of the UNet model, we introduce the \textit{Dual Caption Preference Optimization (DCPO)} method, a novel preference optimization method to align the diffusion models by leveraging two distinct captions, one for the preferred image and another for the less preferred image given a dataset $D=\{z^w, z^l, x^w, x^l\}$. During optimization, the policy model $p_\theta$ increases the likelihood of the preferred image $x^w$ based on the prompt $z^w$ while decreasing the likelihood of the less preferred image $x^l$ using the prompt $z^l$.]

% [rewrite it and add DCPO here] [Additionally, optimizing a diffusion model with the same prompt for two different images introduces further complications. During the reverse process of optimization, using the same prompt for two different images can cause the UNet model to struggle in correctly predicting noise and distinguishing between the images, as the prediction relies on the prompt \textit{c}.] Overall, we identify two key challenges: \textbf{1)} conflict in data distribution, which contradicts the core idea of direct optimization, and \textbf{2)} irrelevant prompts for less preferred images, which can hinder the diffusion model's learning process during optimization.

% In this work, we explore the conflict distribution between preferred and less preferred images for a given prompt \textit{c} in traditional datasets and propose two methods to address this issue. The \textit{first} method involves using a captioning model $Q_\phi(z|x, c)$ to generate a new prompt \textit{z} based on an image \textit{x} and the original prompt \textit{c}. We employed state-of-the-art captioning models, LLaVA 1.6-34B and Emu2-37B, ensuring that the token size of the generated prompt \textit{z} closely matches that of the original prompt \textit{c} for a fair comparison. The \textit{second} method introduces a perturbation technique to generate a less preferred prompt \textit{z} from the original prompt \textit{c}, with three levels of perturbation defined and a new hypothesis proposed around this approach. These methods aim to mitigate the conflict distribution and improve model performance.


