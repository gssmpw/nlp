\section{Experiments}
\label{sec:experimets}
\input{Tables/pickapic}
We fine-tuned the U-Net model of Stable Diffusion 2.1 (SD 2.1) using DCPO on the \textit{Pick-Double Caption} dataset and compared it to SD 2.1 models fine-tuned with \( \text{SFT}_{\text{Chosen}} \), Diffusion-DPO, and MaPO on Pick-a-Pic v2 across various metrics. We first describe the \textit{Pick-Double Caption} dataset and compare it to Pick-a-Pic v2. Subsequently, we provide an in-depth analysis of the results. More comparisons are found in Appendix \ref{sec:appendix_more_insights}. Also, for further details on the fine-tuning, refer to Appendix \ref{sec:appendix_details_train}.

\subsection{Pick-Double Caption Dataset}
\label{sec:pick-double-caption}
% \begin{figure}[t]
%     \centering
%     \caption{Examples from Pick-Double Caption Dataset }
%     \includegraphics[width=1\linewidth]{/images/dataset_preview.pdf}
    
%     \label{fig:dataset-preview}
% \end{figure}

\input{Tables/geneval_main}
Motivated by the \textit{conflict distribution} observed in previous preference datasets, we applied the captioning method described in Section \ref{sec:dcpo-c} to generate unique captions for each image in the Pick-a-Pic v2 dataset. For the \textit{Pick-Double Caption} dataset, we sampled 20,000 instances from Pick-a-Pic v2 and cleaned the samples as detailed in Appendix \ref{sec:appendix_double_caption_dataset}. We then employed two state-of-the-art captioning models, LLaVa-1.6-34B and Emu2-37B, to generate captions for both the preferred and less preferred images, as illustrated in Figure \ref{fig:overview}.


To generate the captions, we used two different prompting strategies: 1) \textbf{Conditional prompt:} where the model was explicitly instructed to generate a caption for image \( x \) based on the given prompt \( c \), and 2) \textbf{Non-conditional prompt:} where the model provided a general description of the image in one sentence without referring to a specific prompt. More details are in Appendix \ref{sec:appendix_double_caption_dataset}.



We evaluated the captions generated by LLaVA and Emu2 using CLIPscore, which revealed several key insights. LLaVA produced captions that have more correlation with the images for both preferred and less preferred samples compared to Emu2 and the original captions, although LLaVA's captions were significantly longer (see Table \ref{tab:tokens-detail} in Appendix \ref{sec:appendix_double_caption_dataset}). Models fine-tuned on captions from the conditional prompt strategy outperformed those using the non-conditional approach, though the conditional prompt captions were twice as long. Interestingly, despite Emu2 generating much shorter captions, the models fine-tuned on Emu2 were comparable to those fine-tuned on the original prompts from Pick-a-Pic v2.
\footnotetext[2]{Note that we rerun all the models on same seeds to have a fair comparison.}  

A key challenge is generating captions for the less preferred images using the captioning method. We observed that in both prompting strategies, the captions for the preferred images are more aligned with the original prompt \( c \) distribution. However, the non-conditional prompt strategy often produces captions for less preferred images that are out-of-distribution (OOD) from the original prompt \( c \) in most cases. We will explore this further in Section \ref{sec:ablation}.




Finally, we observe that the key advantage of the \textit{Pick-Double Caption} dataset is the greater difference in CLIPscore (\( \Delta \mu \)) between preferred and less preferred images compared to the original prompts. Specifically, while the original prompt has a \( \Delta \mu \) of \textbf{1.3}, LLaVA shows a much larger difference at \textbf{4.3}, and Emu2 at \textbf{2.8}. This increased gap reflects improved alignment performance in models fine-tuned on this dataset, indicating that the captioning method mitigates the \textit{conflict distribution}.

\subsection{Performance Comparisons}


We evaluated all methods on 2,500 unique prompts from the Pick-a-Pic v2 \citep{kirstain2023pickapic} dataset, measuring performance using Pickscore \citep{kirstain2023pickapic}, CLIPscore \citep{hessel2022clipscorereferencefreeevaluationmetric}, and Normalized ImageReward \citep{xu2023imagerewardlearningevaluatinghuman}. Additionally, we generated images from 3,200 prompts in the HPSv2 \citep{wu2023human2} benchmark and evaluated them using the HPSv2.1 model. To provide a comprehensive evaluation, we also compared the methods using GenEval \citep{ghosh2023genevalobjectfocusedframeworkevaluating}, focusing on how well the fine-tuned models generated images with the correct number of objects, accurate colors, and proper object positioning.


We compared different versions of DCPO, including the captioning (DCPO-c), perturbation (DCPO-p), and hybrid (DCPO-h) methods, with other approaches, as outlined in Section \ref{sec:dcpo}. For more information on the fine-tuning process of the models, refer to Appendix \ref{sec:appendix_details_train}.

The results in Tables \ref{tab:pickapic} and \ref{tab:app_geneval_results} show that DCPO-h significantly outperforms the best scores from other methods, with improvements of \textbf{+0.21} in Pickscore, \textbf{+0.45} in HPSv2.1, \textbf{+1.8} in ImageReward, \textbf{+0.15} in CLIPscore, and\textbf{ +3\%} in GenEval. Additionally, the results demonstrate that DCPO-c outperforms all other methods on GenEval, Pickscore, and CLIPscore. While DCPO-p performs slightly worse than DCPO-c, it still exceeds SD 2.1, SFT, Diffusion-DPO, and MaPO on GenEval. However, its scores on ImageReward and Pickscore suggest that it underperforms compared to the other approaches. Importantly, DCPO-p shows significant improvement over the other methods on HPSv2.1, highlighting the effectiveness of the perturbation method.



\subsection{Ablation Studies and Analysis}



\label{fig:dcpo-p}


\label{sec:ablation}



\paragraph{Support of Hypothesis 1.} As described in Section \ref{sec:dcpo-p}, we defined three levels of perturbation: weak, medium, and strong. In Hypothesis 1, we proposed that increasing the distance between the distributions of preferred and less preferred images  \( \Delta \mu \) improves model alignment performance. To explore this, we fine-tuned SD 2.1 using the DCPO-h method with three levels of perturbation applied to the less preferred captions \(z^l\) generated by LLaVA. The results in Figure \ref{fig:hyphotesis1} show that increasing the distance \( \Delta \mu \) between the two distributions enhances performance. However, this distance must be controlled and kept below a threshold \( t \), a hyperparameter that may vary depending on the task. These findings support our hypothesis.

% \setlength{\intextsep}{}


\paragraph{Support of Hypothesis 2.} To illustrate the impact of the correlation between the prompt \( c \) and image \( x \) on the perturbation method, we perturbed both the original prompt \( c \) and the less preferred caption \( z^w \), generated by the model \( Q_{\phi} \), where \( z^w \sim W_{\phi}(z^w | Q_{\phi}(z^w | x^w, c)) \). At the same time, we kept the caption generated by \( Q_{\phi} \) for the preferred image as the preferred caption, \( z^w \sim Q(z^w | x^w, c) \). In this case, we assume \(Q_{\phi} = \text{LLaVA}\) and \(W_{\phi} = \text{DIPPER}\). The results in Table \ref{tab:tokens-detail} in Appendix \ref{sec:appendix_double_caption_dataset} show that the caption \( z \) generated by LLaVA is more correlated with the image \( x \) than the original prompt \( c \), indicating that \( S(z,x) > S(c,x) \). Based on the results in Table \ref{tab:hyphotesis2-results}, we conclude that perturbing more correlated captions leads to better performance.

\begin{figure}[t]
    \centering
    
    \includegraphics[width=1\linewidth]{images/h1_plot.pdf}
    \vspace{-2em}
    \caption{Performance comparison of DCPO-c and DCPO-h on different perturbation levels. We plotted regression lines for the four models, showing that as \( \Delta \mu \) increases, performance improves but drops after a threshold \( t \) (orange boundary).}
    \label{fig:hyphotesis1}
\end{figure}


\input{Tables/support_hyphotesis2}



\paragraph{In- vs. Out-of Distribution.} Inspired by 
\citep{Verma_2024_CVPR}, which examined the impact of out-of-distribution data in vision language models, we assessed DCPO using both in-distribution and out-of-distribution (OOD) data. As discussed in Section \ref{sec:pick-double-caption}, the captioning model can generate OOD captions. To explore this, we fine-tuned SD 2.1 with DCPO-h using LLaVA and Emu2 captions at a medium perturbation level. Figure \ref{fig:in-vs-out} shows that in-distribution data improve alignment performance, while OOD results for LLaVA in GenEval, Pickscore, and CLIPscore are comparable to Diffusion-DPO. Similar behavior was observed for DCPO-c, as noted in Appendix \ref{sec:appendix_details_train}.


\begin{figure}[t]
    \centering
    \includegraphics[width=1\textwidth]{images/in_vs_out_plot.pdf}
    \vspace{-2em}
    \caption{Comparison of DCPO-h performance on in-distribution and out-of-distribution data.}
    \label{fig:in-vs-out}
\end{figure}

\input{Tables/compare_dpo}


\paragraph{Effectiveness of the DCPO.} Our analysis shows that LLaVA captions are twice the length of the original prompt \( c \), raising the question of \textit{whether DCPO's improvement is due to data quality or the optimization method}. To explore this, we fine-tuned SD 2.1 with Diffusion-DPO using LLaVA and Emu2 captions instead of the original prompt. The results in Table \ref{tab:compared_dpo} show that models fine-tuned on LLaVA captions outperform Diffusion-DPO with the original prompt. However, DCPO-h still surpasses the new Diffusion-DPO models, demonstrating the effectiveness of the proposed optimization algorithm.

\begin{wrapfigure}{r}{0.5\linewidth}
\vspace{-1.5em}
\includegraphics[width=1.0\linewidth]{images/beta_plot_2.pdf}
\caption{DCPO-h performance comparison across various \( \beta \) values, evaluated on HPSv2.1 and GenEval.}
\vspace{-1.75em}
\label{fig:different-beta}
% \yiran{add more details}
\end{wrapfigure} 

\paragraph{Explore on $\beta$.} In DCPO, \( \beta \) is a key hyperparameter. To evaluate its impact, we fine-tuned SD 2.1 using different values of \( \beta = \{500, 1000, 1500, 2500, 5000\} \). Interestingly, in Figure \ref{fig:different-beta} we observed that \( \beta = 500 \) showed significant improvements on HPSv2.1 and GenEval, even surpassing DCPO-h with \( \beta = 5000 \), our best-reported model. Additional results for different \( \beta \) values can be found in Appendix \ref{sec:appendix_details_train}.




\begin{figure}[t]
    \centering
    \vspace{-1.5em}
    \includegraphics[width=1\linewidth]{images/partiprompt.pdf}
    \vspace{-1.5em}
    \caption{\textbf{(Left)} PartiPrompts benchmark results for three evaluation questions, as voted by GPT-4o. \textbf{(Right)} Qualitative comparison between DCPO-h and Diffusion-DPO fine-tuned on SD 2.1. DCPO-h shows better prompt adherence and realism, with outputs that align more closely with human preferences, emphasizing high contrast, vivid colors, fine detail, and well-focused composition.}
    \label{fig:evaluation_gpt4o}
\end{figure}


\paragraph{DCPO-h vs Diffusion-DPO on GPT-4o Judgment.} We evaluated DCPO-h and Diffusion-DPO using GPT-4o on the PartiPrompts benchmark, consisting of 1,632 prompts. GPT-4o assessed images based on three criteria: \textbf{Q1)} General Preference (\textit{Which image do you prefer given the prompt?}), \textbf{Q2)} Visual Appeal (\textit{Which image is more visually appealing?}), and \textbf{Q3)} Prompt Alignment (\textit{Which image better fits the text description?}). As shown in Figure \ref{fig:evaluation_gpt4o}, DCPO-h outperformed Diffusion-DPO in Q1 and Q2, with win rates of 58\% and 66\%. To see the style of the prompts, refer to Appendix \ref{gpt4o_evaluator}.