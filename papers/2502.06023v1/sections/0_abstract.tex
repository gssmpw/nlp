\begin{abstract}

% Backup

% Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a \textit{conflict distribution}. Additionally, we identified a performance issue in previous optimization methods, where using the same prompt for preferred and less preferred images, known as the
% \textit{irrelevant prompt} issue, restricts model performance. To address these challenges, we propose \textbf{Dual Caption Preference Optimization (DCPO)}, a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the \textit{Pick-Double Caption} dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, \(\text{SFT}_{\text{Chosen}}\), Diffusion-DPO and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone.


Recent advancements in human preference optimization, originally developed for Large Language Models (LLMs), have shown significant potential in improving text-to-image diffusion models. These methods aim to learn the distribution of preferred samples while distinguishing them from less preferred ones. However, existing preference datasets often exhibit overlap between these distributions, leading to a \textit{conflict distribution}. 
Additionally, we identified that input prompts contain irrelevant information for less preferred images, limiting the denoising network's ability to accurately predict noise in preference optimization methods, known as the \textit{irrelevant prompt} issue.
To address these challenges, we propose \textbf{Dual Caption Preference Optimization (DCPO)}, a novel approach that utilizes two distinct captions to mitigate irrelevant prompts. To tackle conflict distribution, we introduce the \textit{Pick-Double Caption} dataset, a modified version of Pick-a-Pic v2 with separate captions for preferred and less preferred images. We further propose three different strategies for generating distinct captions: captioning, perturbation, and hybrid methods. Our experiments show that DCPO significantly improves image quality and relevance to prompts, outperforming Stable Diffusion (SD) 2.1, \(\text{SFT}_{\text{Chosen}}\), Diffusion-DPO and MaPO across multiple metrics, including Pickscore, HPSv2.1, GenEval, CLIPscore, and ImageReward, fine-tuned on SD 2.1 as the backbone. Our code and dataset are available in \href{https://github.com/sahsaeedi/DCPO/}{\textcolor{purple}{Github}}.
% Project page \url{https://github.com/sahsaeedi/DCPO/}
\end{abstract}