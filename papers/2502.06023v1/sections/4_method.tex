\section{Method}

In this section, we present the \textit{conflict distribution} issue in preference datasets, where preferred and less-preferred images generated from the same prompt \( c \) exhibit significant overlap. We also explain the \textit{irrelevant prompt} issue found in previous direct preference optimization methods. To address these challenges, we propose \textbf{Dual Caption Preference Optimization (DCPO)}, a method that uses distinct captions for preferred and less preferred images to improve diffusion model alignment.
\newline

\subsection{The Challenges}
\label{sec:challenges}
\begin{wrapfigure}{r}{0.4\linewidth}
\vspace{-2.4em}
\includegraphics[width=1.0\linewidth]{images/conflict_distribution.pdf}
\caption{The \textit{conflict distribution} issue in the Pick-a-Pic v2 dataset. $\mu^l$ and $\mu^w$ represent the average CLIPscore of preferred and less preferred images for prompt $c$, respectively. Also, $\Delta \mu$ shows the difference between the distributions.}
\vspace{-0.5em}
\label{fig:conflict-distribution-figure}
\end{wrapfigure} 
Generally, to optimize a Large Language Model (LLM) using preference algorithms, we need a dataset \( D = \{c, y^w, y^l\} \), where \( y^w \) and \( y^l \) represent the preferred and less preferred responses to a given prompt \( c \). Ideally, the distributions of these responses should differ significantly. Similarly, in diffusion model alignment, the distributions of preferred and less preferred images should be distinct for the same prompt \( c \). However, our analysis shows a substantial overlap between these distributions, which we call \textit{conflict distribution}, as illustrated in Figure \ref{fig:conflict-distribution-figure}.


Another issue emerges when direct preference optimizes a diffusion model. In the reverse denoising process, the U-Net model predicts noise for both preferred and less preferred images using the same prompt \( c \). As prompt \( c \) is more relevant to the preferred image, it becomes less effective for predicting the less preferred one, leading to reduced performance. We call this the \textit{irrelevant prompts} problem.

\subsection{DCPO: Dual Caption Preference Optimization}
\label{sec:dcpo}
Motivated by the \textit{conflict distribution} and \textit{irrelevant prompts} issues, we propose DCPO, a new preference optimization method that optimizes diffusion models using two distinct captions. DCPO is a refined version of Diffusion-DPO designed to address these challenges. More details are in Appendix \ref{sec:appendix_proof_dcpo}. \footnote{For additional background about diffusion and preference optimization, refer to Appendix \ref{sec:appendix_background}.}

We start with a fixed dataset \( D = \{c, x_0^w, x_0^l\} \), where each entry contains a prompt \( c \) and a pair of images generated by a reference model \( p_{ref} \). The human labels indicate a preference, with \( x_0^w \) preferred over \( x_0^l \). We assume the existence of a model \( R_{\phi}(z|c, x) \), which generates a caption \( z \) given a prompt \( c \) and an image \( x \). Using this model, we transform the dataset into \( D' = \{z^w, z^l, x_0^w, x_0^l\} \), where \( z^w \) and \( z^l \) are captions for the preferred image \( x_0^w \) and the less-preferred image \( x_0^l \), respectively. 
Our goal is to train a new model \( p_{\theta} \), aligned with human preferences, to generate outputs that are more desirable than those produced by the reference model.

% Specifically, similar to \citep{bansal2024comparing}, the DCPO objective aims to learn an aligned model $p_\theta$ by upweighting the joint probability of preferred images $p_\theta(x^w_0, z^w)$ over less preferred images $p_\theta(x^l_0, z^l)$.

% The key challenge lies in the fact that the parameterized distribution \( p_{\theta}(x_0|z) \) is intractable, as argued in Diffusion-DPO. To address this, they employed the evidence lower bound (ELBO). The reward for the entire chain, based on the latent variables \( x_{1:T} \), is defined as follows:

% \begin{equation}
%     r(c,x_0) = \mathit{\mathbb{E}}_{p_\theta(x_{1:T}|x_0,c)} [R(c,x_{0:T})]
%     \label{new_reward}
% \end{equation}

% Following prior work \citep{wallace2024diffusion}, we minimize the upper bound on the joint KL-divergence in Equation \ref{rlhf_obj} in Appendix \ref{sec:appendix_background}, based on the reward $r(c,x_0) = \mathit{\mathbb{E}}_{p_\theta(x_{1:T}|x_0,c)} [R(c,x_{0:T})]$. 


The objective of RLHF is to maximize the reward \( r(c, x_0) \) for the reverse process \( p_{\theta}(x_{0:T}|z) \), while maintaining alignment with the original reference reverse process distribution. 
% Inspired by \citep{wallace2024diffusion, bansal2024comparing}, formally, the optimization objective for DCPO minimizes:
% the DCPO objective is defined by direct optimization through the conditional distribution \( p_{\theta}(x_{0:T}|z) \) as follows:
Building on prior work \citep{wallace2024diffusion}, the DCPO objective is defined by direct optimization through the conditional distribution \( p_{\theta}(x_{0:T}|z) \) as follows:

\begin{equation}
\begin{split}
    \mathcal{L}_{\text{DCPO}}(\theta) = -\mathit{\mathbb{E}}_{(x^{w}_0, x^{l}_0, z^l, z^w) \sim \mathcal{D'}} \log \sigma( 
    \beta \mathit{\mathbb{E}}_{x^{w}_{1:T}\sim p_\theta(x^{w}_{1:T}|x^{w}_0, z^w),x^{l}_{1:T} \sim p_\theta (x^{l}_{1:T}, x^{l}_0|z^l)} \\
    [\log \frac{p_{\theta} (x^{w}_{0:T}|z^w)}{p_{\text{ref}}(x^{w}_{0:T}|z^w)} - \log \frac{p_\theta (x^{l}_{0:T}|z^l)}{p_{\text{ref}}(x^{l}_{0:T}|z^l)}])
\label{dcpo_loss}
\end{split}
\end{equation}

% Further, we show that \ref{dcpo_loss} reduces to the DPO formulation (Equation \ref{eq:dpo_loss} in Appendix \ref{sec:appendix_background}) when the captions $z^w=z^l$ in Appendix \ref{sec:app_proof}. 
However, as noted in Diffusion-DPO \citep{wallace2024diffusion}, the sampling process \( x_{1:T} \sim p(x_{1:T} | x_0) \) is inefficient and intractable. To overcome this, we follow a similar approach by applying Jensen's inequality and utilizing the convexity of the \( -\log(\cdot) \) function to bring the expectation outside. By approximating the reverse process \( p_\theta(x_{1:T}|x_0, z) \) with the forward process \( q(x_{1:T}|x_0) \), and through algebraic manipulation and simplification, the DCPO loss can be expressed as:
% This results in the following bound:
% \begin{equation}
% \begin{split}
%     \mathcal{L}_{\text{DCPO}}(\theta) \leq -\mathit{\mathbb{E}}_{(x^{w}_0, x^{l}_0) \sim \mathcal{D}, t \sim \mu (0, T), 
%     x^{w}_{t-1, t} \sim p_\theta (x^{w}_{t-1,t}|x^{w}_0, z^w), x^{l}_{t-1, t} \sim p_\theta(x^{l}_{t-1,t}|x^{l}_0, z^l)}  \\
%     \log \sigma (\beta T \log \frac{p_\theta(x^{w}_{t-1}|x^{w}_t, z^w)}{p_{\text{ref}}(x^{w}_{t-1}|x^{w}_t, z^w)} - \beta T \log \frac{p_\theta(x^{l}_{t-1}| x^{l}_t, z^l)}{p_{\text{ref}}(x^{l}_{t-1}|x^{l}_t, z^l)})
% \end{split}
% \end{equation}

\begin{equation}
\begin{split}
    \mathcal{L}_\text{DCPO}(\theta) = -\mathit{\mathbb{E}}_{(x^{w}_0, x^{l}_0) \sim \mathcal{D'}, t\sim \mu (0,T), x^{w}_t \sim q(x^{w}_t | x^{w}_0), x^{l}_t \sim q(x^{l}_t | x^{l}_0)} \\
    \log \sigma (-\beta T w (\lambda_t) 
    (||\epsilon^w - \epsilon_\theta (x^{w}_t, z^w, t)||^{2}_{2} - || \epsilon^w - \epsilon_\text{ref}(x^{w}_t, z^w, t)||^{2}_2 \\
    - (|| \epsilon^l - \epsilon_\theta (x^{l}_t,z^l,t) ||^{2}_2 - || \epsilon^l - \epsilon_\text{ref}(x^{l}_t,z^l, t)||^{2}_2 ))
\end{split}
\end{equation}

where \( x_t^* = \alpha_t x_0^* + \sigma_t \epsilon^* \), and \( \epsilon^* \sim \mathcal{N}(0, I) \) is a sample drawn from \( q(x_t^* | x_0^*) \). \( \lambda_t = \alpha_t^2 / \sigma_t^2 \) represents the signal-to-noise ratio, and \( \omega(\lambda_t) \) is a weighting function.


To optimize a diffusion model using DCPO, a dataset \( D = \{z^w, z^l, x_0^w, x_0^l\} \) is required, where captions are paired with the images. However, the current preference dataset only contains prompts \( c \) and image pairs without captions. To address this, we propose three methods for generating captions \( z \) and introduce a new high-quality dataset, \textit{Pick-Double Caption}, which provides specific captions for each image, based on Pick-a-Pic v2~\citep{kirstain2023pickapic}.


\subsubsection{DCPO-c: Captioning Method}
\label{sec:dcpo-c}
In this method, the captioning model \( Q_{\phi}(z|c, x) \) generates the caption \( z \) based on the image \( x \) and the original prompt \( c \). As a result, we obtain a preferred caption \( z^w \sim Q_{\phi}(z^w|c, x^w) \) for the preferred image and a less preferred caption \( z^l \sim Q_{\phi}(z^l|c, x^l) \) for the less preferred image, as illustrated in a sample in Figure \ref{fig:overview}. Thus, based on the generated captions \( z^w \) and \( z^l \), we can optimize a diffusion model using the DCPO method.


In the experiment section, we evaluate the performance of DCPO-c and demonstrate that this method effectively mitigates the \textit{conflict distribution} by creating two differentiable distributions. However, the question of how much divergence is needed between the two distributions remains. To investigate this, we propose Hypothesis 1.

\textbf{Hyphothesis 1.} \textit{Let \( d(z, x) \) represent the semantic distribution between a caption \( z \) and an image \( x \), with \( \mu \) being the mean of the distribution \( d \), and \( \Delta \mu = \mu(d(z_0^w, x_0^w)) - \mu(d(z_0^l, x_0^l)) \) as the difference between the two distributions. Increasing \( \Delta \mu \) between the preferred and less-preferred image distributions in a preference dataset beyond a threshold \( t \) (i.e., \( \Delta \mu > t \)), can improve the performance of the model \( p_\theta \).}

Our hypothesis suggests that increasing the distance between the two distributions up to a certain threshold \( t \) can improve alignment performance. To examine this, we propose the perturbation method to control the distance between the two distributions, represented by \(\Delta \mu\).

\subsubsection{DCPO-p: Perturbation Method}
\label{sec:dcpo-p}

\begin{figure}[t]
    \centering
    
    \includegraphics[width=1\linewidth]{images/perturbation_shifts.pdf}
    \vspace{-2em}
    \caption{Effect of the perturbation method on semantic distributions in terms of CLIPScore. \textbf{(a)} shows the distributions that feature the captions \( z^w \) and \( z^l \) generated by the LLaVA model, while \textbf{(b)}, \textbf{(c)}, and \textbf{(d)} represent different levels of perturbation on top of the caption \( z^l \). The figure demonstrates that as the level of perturbation increases, the distance between the distributions of captions \( z^w \) and \( z^l \) increases. For more details on the perturbation method, refer to Appendix \ref{sec:appendix_perturbation}.}
    \vspace{-.5em}
    \label{fig:perturbation-clipscore}
\end{figure}

While using a captioning model is an effective way to address the \textit{conflict distribution}, it risks deviating from the original distribution of prompt \( c \), and the distributions of preferred and less preferred images may still remain close. To tackle these issues, we propose a perturbation method. In this approach, we assume that prompt \( c \) is highly relevant to the preferred image \( x_0^w \) and aim to generate a less relevant caption, denoted as \( c_p \), based on prompt \( c \). To achieve this, we use the model \( W_{\phi}(c_p|c) \), which generates a perturbed version of prompt \( c \), altering its semantic meaning. In this framework, prompt \( c \) corresponds to the preferred caption \( z^w \) (\(c = z^w\)), while the perturbed prompt \( c_p \) represents the less-preferred caption \( z^l \) (\(c_p = z^l\)). For the perturbation model \( W_{\phi} \), we utilized the DIPPER model \citep{dipper} built by fine-tuning the T5-XXL  ~\citep{t5} to produce a degraded version of the prompt \( c \).

We define three levels of perturbation: \textbf{1) Weak:} where prompt \( c_p \) has high semantic similarity to prompt \( c \), with minimal differences. \textbf{2) Medium:} where the semantic difference between prompt \( c_p \) and \( c \) is more pronounced than in the weak level. \textbf{3) Strong:} where the majority of the semantics in prompt \( c_p \) differ significantly from prompt \( c \). Further details can be found in Appendix \ref{sec:appendix_perturbation}.

% \begin{enumerate}
%         \item \textbf{Weak:} where prompt \( z \) has high semantic similarity with prompt \( c \) with less difference.
%         \item \textbf{Medium:} where the semantic difference between prompt \( z \) and \( c \) is greater than in the weak level.
%         \item \textbf{Strong:} where most of the semantics of prompt \( z \) differ from prompt \( c \).
% \end{enumerate}


The main advantage of DCPO-p is to reduce the captioning process cost while staying closer to the original data distribution by using prompt \( c \) as the preferred caption. However, we observe that the quality of captions in DCPO-c outperforms that of the original prompt \( c \), as shown in Table \ref{tab:tokens-detail} in Appendix \ref{sec:appendix_double_caption_dataset}. Based on this observation, we propose a hybrid method to improve the alignment performance by combining captioning and perturbation techniques.



\subsubsection{DCPO-h: Hybrid Method}
In this method, instead of perturbing the prompt \( c \), we perturb the caption \( z \) generated by the model \( Q_{\phi}(z|x, c) \) based on the image \( x \) and prompt \( c \). As discussed in Section \ref{sec:dcpo-c}, the goal of the perturbation method is to increase the distance between the two distributions. However, the correlation between the image \( x_0 \) and prompt \( c \) significantly impacts alignment performance. Therefore, we propose Hypothesis 2.

\paragraph{Hypothesis 2.} \textit{Let \( S(c, x) \) represent the correlation score between prompt \( c \) and image \( x \), and \( P(p_\theta(c_1, c_2)) \) denote the performance of model \( p_\theta \) optimized on captions \( c_1 \) and \( c_2 \) with DCPO, where \( W_{\phi} \) is the perturbation model. If \( S(z, x) > S(c, x) \), then \( P(p_\theta(z^w, z^w_p \sim W_{\phi}(z^w_p|z^w))) > P(p_\theta(c, c_p \sim W_{\phi}(c_p|c))) \).}


In Section \ref{sec:ablation}, we provide experimental evidence supporting Hypothesis 2 and investigate the potential of using \( z^{l}_{p} \sim W_{\phi}(z^{l}_{p} | z^l) \) as the less-preferred caption \( z^l \), instead of \( z^{w}_{p} \sim W_{\phi}(z^{w}_{p} | z^w) \) as originally proposed in Hypothesis 2.