\section{Conclusion and Limitations}
In this paper, we present a novel preference optimization method for aligning text-to-image diffusion models called Dual Caption Preference Optimization (DCPO). We tackle two major challenges in previous preference datasets and optimization algorithms: the \textit{conflict distribution} and \textit{irrelevant prompt}. To overcome these issues, we introduce the \textit{Pick-Double Caption} dataset, a modified version of the Pick-a-Pic v2 dataset. We also identify difficulties in generating captions, particularly the risk of out-of-distribution captions for images, and propose three approaches: 1) captioning (DCPO-c), 2) perturbation (DCPO-p), and 3) a hybrid method (DCPO-h). Our results show that DCPO-h significantly enhances alignment performance, outperforming methods like MaPO and Diffusion-DPO across multiple metrics.

\paragraph{Limitations.} Although DCPO shows strong performance across various metrics, the captioning and perturbation methods are resource-intensive. We encourage future research to explore cost-effective alternatives to these methods. Additionally, the potential of using different backbones, such as Stable Diffusion XL (SDXL) \citep{rombach2022high}, has not been explored in the context of DCPO. We also invite researchers to investigate DCPO's effectiveness on other tasks, such as safety. We believe our work will have a significant impact on the alignment research community.