\section{Background}
\label{sec:appendix_background}
\subsection{Diffusion Models}
Based on samples from a data distribution $q(x_0)$, a noise scheduling function $\alpha_t$ and $\sigma_t$ \citep{rombach2022high} denoising diffusion models \citep{song2020score} are generative models $p_\theta(x_0)$  that operate through a discrete-time reverse process structured as a Markov Decision Proces where

\begin{equation}
    p_{\theta}(x_{t-1}|x_t) = \mathcal{N}(x_{t-1}; \mu_\theta(x_t), \sigma^{2}_{t|t-1} \frac{\sigma^{2}_{t-1}}{\sigma^{2}_t}I).
\end{equation}

The training process involves minimizing the evidence lower bound (ELBO) associated with this model \citep{song2021maximum}:

\begin{equation}
    L_{DM} = \mathit{\mathbb{E}}_{x_0,\epsilon,t,x_t}[\omega(\lambda_t)||\epsilon-\epsilon_\theta(x_t,t)||^{2}_2]
\end{equation}


where $\epsilon \sim \mathcal{N}(0,I)$, $t \sim \mathcal{U}(o,T)$, $x_t ~ q(x_t|x_0)= \mathcal{N}(x_t; \alpha_t x_0, \sigma^2_tI).\lambda_t = \alpha^2_t / \sigma^2_t$ is a signal-to-noise ratio \citep{kingma2021variational}, $\omega(\lambda_t)$ is a predefined weighting function \citep{song2019generative}.

\subsection{Preference Optimization}
Aligning a generative model typically involves fine-tuning it to produce outputs that are more aligned with human preferences. Estimating the reward model \( r \) based on human preference is generally challenging, as we do not have direct access to the reward model. However, if we assume the availability of ranked data generated under a given condition \( c \), where \( x_0^w \succ x_0^l | c \) (with \( x_0^w \) representing the preferred sample and \( x_0^l \) the less-preferred sample), we can apply the Bradley-Terry theory to model these preferences. The Bradley-Terry (BT) model expresses human preferences as follows:

\begin{equation}
    p_{BT}(x^{w}_0 \succ x^{l}_0|c) = \sigma(r(c,x^{w}_0)-r(c,x^{l}_0))
\end{equation}

where \( \sigma \) denotes the sigmoid function, and \( r(x_0, c) \) is derived from a neural network parameterized by \( \phi \), which is estimated through maximum likelihood training for binary classification as follows:

\begin{equation}
    L_{BT}(\phi) = -\mathit{\mathbb{E}}_{c,x^{w}_0,x^{l}_0[\log \sigma (r_\phi(c,x^{w}_0)-r_\phi(c,x^{l}_0))]}
\end{equation}


where the prompt \( c \) and data pairs \( x_0^w \), \( x_0^l \) are sourced from a dataset that humans have annotated.

This approach to reward modeling has gained popularity in aligning large language models, particularly when combined with reinforcement learning (RL) techniques like proximal policy optimization (PPO) \citep{schulman2017proximal} to fine-tune the model based on rewards learned from human preferences, known as Reinforcement Learning from Human Feedback (RLHF) \citep{ouyang2022training}. The goal of RLHF is to optimize the conditional distribution \( p(x_0|c) \) (where \( c \sim D_c \)) such that the reward model \( r(c, x_0) \) is maximized, while keeping the policy model within the desired distribution using a KL-divergence term to ensure it remains reachable under the following objective:

\begin{equation}
    \max\limits_{p_{\theta}} \mathit{\mathbb{E}}_{c \sim \mathcal{D}_c,x_0\sim p_\theta (x_0|c)}[r(c,x_0)]-\beta\mathit{\mathbb{D}}_{KL}[p_\theta (x_0 | c) || p_{\text{ref}} (x_0|c)]
    \label{rlhf_obj}
\end{equation}

where \( \beta \) controls how far the policy model \(p_\theta\) can deviate from the reference model \(p_{ref}\). It can be demonstrated that the objective in Equation \ref{rlhf_obj} converges to the following policy model:

\begin{equation}
    p^{*}_\theta (x_0|c) = p_{\text{ref}}(x_0|c)\exp (r(c,x_0)/\beta)/Z(c)
    \label{p_start}
\end{equation}

where \( Z \) is the partition function, the training objective for \( p_{\theta} \), inspired by DPO, has been derived to be equivalent to Equation \ref{p_start} without the need for an explicit reward model \( r(x, c) \). Instead, it learns directly from the preference data \( (c, x^w_0, x^l_0) \sim D \).

\begin{equation}
    L_{\text{DPO}}(\theta) = -\mathit{\mathbb{E}}_{c,x^{w}_0,x^{l}_0} [\log \sigma (\beta \log \frac{p_\theta (x^{w}_0|c)}{p_{\text{ref}}(x^{w}_0|c)} - \beta \log \frac{p_\theta(x^{l}_0|c)}{p_{\text{ref}}(x^{l}_0|c)})]
\label{eq:dpo_loss}
\end{equation}

where \(\sigma\) represents the sigmoid function.

Through this reparameterization, instead of first optimizing the reward function \( r \) and then applying reinforcement learning, the method directly optimizes the conditional distribution \( p_{\theta}(x_0|c) \).