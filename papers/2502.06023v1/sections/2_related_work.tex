\section{Related Works}

\paragraph{Aligning Diffusion Models.} Recent advances in preference alignment methods of text-to-image diffusion models have shown that RL-free methods \citep{wallace2024diffusion, yang2023using, li2024aligning, yuan2024selfplay, gambashidze2024aligningdiffusionmodelsnoiseconditioned, park2024directunlearningoptimizationrobust} outperforms RL-based approaches \citep{pmlr-v202-fan23b, fan2023dpok, hao2023optimizing,lee2023aligning,  xu2023imagerewardlearningevaluatinghuman, prabhudesai2024aligning, black2024training, clark2024directly} mainly because they eliminates the need for an explicit reward model \citep{saeidi2024insights, chatterjee2024revision}. Initially, Direct Preference Optimization (DPO)~\citep{rafailov2024direct} and Triple Preference Optimization~\citep{saeidi2024triple} method reformulate the RLHF objective in a closed-form manner and introduce it as an implicit reward model with a simple classification objective. Diffusion-DPO \citep{wallace2024diffusion}  directly adopts DPO method into text-to-image diffusion models, utilizing pairwise preference datasets consisting of text and images to guide alignment. Diffusion-KTO \citep{li2024aligning} incorporates Kahneman \& Tversky model of human utility to align these models, simplifying the process by using images with binary feedback signals, i.e., likes or dislikes instead of pairwise preference data. To enhance flexibility, \citet{hong2024marginawarepreferenceoptimizationaligning} introduce MaPO, an alignment technique independent of a reference model previously used by other methods, enabling greater control over stylistic adaptations. However, previous methods optimize diffusion models based on a single prompt for a pair of images, which supports the \textit{irrelevant prompts} issue explored in Section \ref{sec:challenges}.


\paragraph{Text-to-image Preference Datasets.} Text-to-image image preference datasets commonly involve the text prompt to generate the images, and two or more images are ranked according to human preference. HPS \citep{wu2023human1} and HPSv2 \citep{wu2023human2} create multiple images using a series of image generation models for a single prompt, and the images are ranked according to real-world human preferences. Moreover, a classifier is trained using the gathered preference dataset, which can be used as a metric for image-aligning tasks. Also, Pick-a-Pic v2 \citep{kirstain2023pickapic} follows a similar structure to create a pairwise preference dataset along with their CLIP \citep{radford2021learningtransferablevisualmodels} based scoring function, Pickscore. While these datasets are carefully created, having only one prompt for both or all the images introduces \textit{conflict distribution}, which will be further discussed in Section \ref{sec:challenges}. For this reason, we modified the Pick-a-Pic v2 dataset using recaptioning and perturbation methods to improve image alignment performance. 