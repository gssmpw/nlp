\section{Formal Proofs Regarding DCPO}
\label{sec:appendix_proof_dcpo}

\subsection{Optimizing the DCPO Loss is Optimizing the DPO Loss}
Inspired by \citet{bansal2024comparing},  we can intuitively assume that the DCPO objective is to learn an aligned model $p_\theta$ by weighting the joint probability of preferred images $p_\theta(x^w_0, z^w)$ over less preferred images $p_\theta(x^l_0, z^l)$. We set the optimization objective of DCPO is to minimize the following:

\begin{equation}
\begin{split}
    \mathcal{L}_{\text{DCPO}}(\theta) = -\mathit{\mathbb{E}}_{(x^{w}_0, x^{l}_0, z^l, z^w) \sim \mathcal{D'}} \log \sigma( 
    \beta \mathit{\mathbb{E}}_{x^{w}_{1:T}\sim p_\theta(x^{w}_{1:T}|x^{w}_0, z^w),x^{l}_{1:T} \sim p_\theta (x^{l}_{1:T}, x^{l}_0,z^l)} \\
    [\log \frac{p_{\theta} (x^{w}_{0:T},z^w)}{p_{\text{ref}}(x^{w}_{0:T},z^w)} - \log \frac{p_\theta (x^{l}_{0:T},z^l)}{p_{\text{ref}}(x^{l}_{0:T}|z^l)}])
\label{dcpo_loss}
\end{split}
\end{equation}
\\
Here, we highlight that reducing $\mathcal{L}_{\text{DCPO}}(\theta)$ is equivalently reducing $\mathcal{L}_{\text{DPO}}(\theta)$ when the captions are the same for the preferred and less preferred images.

\paragraph{Lemma 1.} \textit{Under the case where $\mathcal{D}_{\text{define}}=\{x^w_0, c, x^l_0, c\}$, that is, the image captions are identical for the given pair of preferred and less preferred images $(x^w_0, x^l_0)$, we have $L_{\text{DPO}}(\theta; \mathcal{D}_{\text{DPO}}; \beta; p_{\text{ref}}) = L_{\text{DCPO}}(\theta; \mathcal{D}_{\text{define}}; \beta; p_{\text{ref}})$, in which $\mathcal{D}_{\text{DPO}}=\{c, x^w_0, x^l_0\}.$}

\paragraph{\textbf{Proof of Lemma 1.}}

\begin{equation}
    \begin{split}
        \mathcal{L}_{\text{DCPO}}(\theta;\mathcal{D'}, \beta, p_{\text{ref}}) & = \mathit{\mathbb{E}}_{(x^w_0, x^l_0, z^w, z^l) \sim \mathcal{D'}}
        \\ &
        \left[\log \left(\sigma \left(\beta \log \frac{p_\theta(x^w_0, z^w)}{p_{\text{ref}}(x^w_0, z^w)} - 
        \beta \log \frac{p_\theta(x^l_0, z^l)}{p_{\text{ref}}(x^l_0, z^l)} \right) \right)\right] \\
        & = \mathit{\mathbb{E}}_{(x^w_0, x^l_0, z^w, z^l) \sim \mathcal{D'}}
        \\ &
        \left[\log \left( \sigma \left(\beta \log 
        \frac{p_\theta (x^w_0|z^w) p_\theta(z^w)}{p_{\text{ref}}(x^w_0|z^w)p_{\text{ref}}(z^w)} \right. \right. \right.  \left. \left. \left. - \beta \log \frac{p_\theta(x^l_0|z^l) p_\theta(z^l)}{p_{\text{ref}}(x^l_0|z^l)p_{\text{ref}}(z^l)} \right) \right) \right] \\
    \end{split}
\end{equation}

\begin{equation}
\label{eq:dcpo-lemma1}
    \begin{split}
         \mathcal{L}_{\text{DCPO}}(\theta;\mathcal{D}_{\text{define}}, \beta, p_{\text{ref}}) & \stackrel{z^w=z^l=c}{=} 
         \mathit{\mathbb{E}}_{(x^w_0, c, x^l_0, c) \sim \mathcal{D}_\text{define}} 
         \\ &
         \left[ \log \left(\sigma \left(\frac{p_\theta(c)}{p_{\text{ref}}(c)} \left ( \beta \log \frac{p_\theta(x^w_0|c)}{p_{\text{ref}}(x^w_0|c)} - \beta \log \frac{p_\theta (x^l_0|c)}{p_\text{ref}(x^l_0|c)} \right) \right) \right) \right] \\
         & \stackrel{\frac{p_\theta(c)}{p_{\text{ref}}(c)}=C}{=} \mathit{\mathbb{E}}_{(x^w_0, x^l_0, c) \sim \mathcal{D}_\text{DPO}} 
         \\ &
         \left[ \log \left(\sigma \left(C \cdot \beta \log \frac{p_\theta(x^w_0|c)}{p_{\text{ref}}(x^w_0|c)} - C \cdot  \beta \log \frac{p_\theta (x^l_0|c)}{p_\text{ref}(x^l_0|c)} \right) \right) \right] \\
         & = \mathcal{L}_{\text{DPO}}(\theta; \mathcal{D}_\text{DPO}, \beta, p_{\text{ref}} )
    \end{split}
\end{equation}
In Equation \ref{eq:dcpo-lemma1}, $C$ is a constant value that equates 
 to $\frac{p_\theta(c)}{p_{\text{ref}}(c)}$. The proof above follows the Bayes rule by substituting $c$ according to $z^w=z^l=c$.


\subsection{Analyses of DCPO's Effectiveness}
In this section, we present the formal proofs of why our DCPO leads to a more optimized $L(\theta)$ of a Diffusion-based model and, consequently, better performance in preference alignment tasks.

\paragraph{Proof 1.} \textit{Increasing the difference between $\Delta_{\text{preferred}}$ and $ \Delta_{\text{less-preferred}}$ improves the optimization of $L(\theta)$.}
\\
For better clarity, the loss function $L(\theta)$ can be written as:
$$
L(\theta) = -\mathbb{E} \left[ \log \sigma \big( -\beta T \omega(\lambda_t) \cdot M \big) \right]
$$
where $\sigma(x) $ is the sigmoid function that squashes its input $x$ into the output range $ (0, 1) $, and $\ M = \Delta_{\text{preferred}} - \Delta_{\text{less-preferred}} $, i.e., the margin between the respective importance of the preferred and less preferred predictions. 
\\
\\
Characteristically, the gradient of $ \sigma(x) $ is at its maximum near $ x = 0 $ and decreases as $ |x| $ increases. 
A larger margin in terms of $M$ makes it easier for the optimization to drive the sigmoid function towards its asymptotes, reducing loss.

\begin{itemize}
    \item When $ M $ is small ($ |M| \approx 0 $): The sigmoid $ \sigma(-\beta T \omega(\lambda_t) \cdot M) $ is near 0.5 (its midpoint). Also, the gradient of $ \log \sigma(x) $ is the largest near this point, meaning the model struggles to differentiate between preferred and less preferred predictions effectively.

    \item When $ M $ is large ($ |M| \gg 0 $): The sigmoid $ \sigma(-\beta T \omega(\lambda_t) \cdot M) $ moves closer to 0 or 1, depending on the sign of $ M $. For a well-aligned model, if the preferred predictions are correct, $ M > 0 $ and $ \sigma(-\beta T \omega(\lambda_t) \cdot M) $ approach 1, thus minimizing the loss.
    
\end{itemize}
Intuitively, an ideally large $ M $ represents a clear distinction between the preferred image-caption versus the less preferred image-caption. Thus, by maximizing $ M $, we may push the loss $ L(\theta) $ towards its minimum, leading to better soft-margin optimization. 
\\
\\
\paragraph{Proof 2.} \textit{Replacing caption $ c $ with the specifically generated caption $ z^l $ for the less-preferred image $ \mathbf{x}_0^l $ decreases $\Delta_{\text{less-preferred}}$.}
\\
\\
To analyze how replacing $ \mathbf{c} $ with $ \mathbf{z}^l $, where $ \mathbf{c} \subset \mathbf{z}^l $ and  $\mathbf{z}^l \sim Q(\mathbf{z}^l | x^l, c)$, for the less-preferred image $ \mathbf{x}_0^l $ improves the optimization, we delve into how the loss function is affected by this substitution.
\\
The term relevant to the less-preferred image $ \mathbf{x}_t^l $ in the loss is:
$$
\Delta_{\text{less-preferred}} = \| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{c}) \|_2^2 - \| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^l, t, \mathbf{c}) \|_2^2.
$$

Replacing $ \mathbf{c} $ with $ \mathbf{z}^l $ modifies the predicted noise term $ \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{c}) $ to $ \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{z}^l) $. Since $ \mathbf{z}^l $ better represents $ \mathbf{x}_t^l $, we have:

\begin{equation}
\label{equ:appendix_1}
\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{z}^l) \|_2^2 < \| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{c}) \|_2^2 
\end{equation}

When $ \| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, \mathbf{z}^l) \|_2^2 $ becomes smaller, the term $ \Delta_{\text{less-preferred}} $ decreases. This leads to $\Delta_{\text{preferred}} - \Delta_{\text{less-preferred}}$ becoming larger, which improves the soft-margin optimization in the loss function $ L(\theta) $ that we have shown in Proof 1.


% Proof 3: Let's assume that replacing the caption $ c $ with a more detailed caption $ z^l $ (where $ c \subset z^l $) helps the neural network $ \boldsymbol{\epsilon}_\theta $ predict the noise better for the less preferred sample $ \mathbf{x}_t^l $.


We further elaborate on why Equation \ref{equ:appendix_1} is true. In the context of mean squared error (MSE) minimization, the optimal predictor of $ \boldsymbol{\epsilon}^l $ given some information is the conditional expectation:

\begin{itemize}
\item When conditioned on $ (\mathbf{x}_t^l, t, c) $:
$$
\boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, c) = \mathbb{E}\left[ \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right]
$$
\item When conditioned on $ (\mathbf{x}_t^l, t, z^l) $:
$$
\boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, z^l) = \mathbb{E}\left[ \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right]
$$
\end{itemize}
The total variance of $ \boldsymbol{\epsilon}^l $ can be decomposed as by the Law of Total Variance (conditional variance formula) \citep{ross2014introduction}:

$$
\operatorname{Var}\left( \boldsymbol{\epsilon}^l \right) = \mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right) \right] + \operatorname{Var}\left( \mathbb{E}\left[ \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right] \right)
$$
Similarly, when conditioning on $ z^l $:
$$
\operatorname{Var}\left( \boldsymbol{\epsilon}^l \right) = \mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right) \right] + \operatorname{Var}\left( \mathbb{E}\left[ \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right] \right)
$$

Since $ c \subset z^l $, the information provided by $ z^l $ is richer than that of $ c $. In probability theory, conditioning on more information does not increase the conditional variance:

\begin{equation}
\label{equ:appendix_2}
\operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right) \leq \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right)
\end{equation}
This inequality holds because conditioning on additional information ($ z^l $) can only reduce or leave unchanged the uncertainty (variance) about $ \boldsymbol{\epsilon}^l $.


The expected squared error when using the optimal predictor is equal to the conditional variance:

$$
\mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, c) \right\|_2^2 \right] = \mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right) \right]
$$

Similarly,

$$
\mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, z^l) \right\|_2^2 \right] = \mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right) \right]
$$

From \ref{equ:appendix_2}, we have:

$$
\operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right) \leq \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right)
$$

Taking expectations on both sides:

$$
\mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, z^l \right) \right] \leq \mathbb{E}\left[ \operatorname{Var}\left( \boldsymbol{\epsilon}^l \mid \mathbf{x}_t^l, t, c \right) \right]
$$

Therefore,

$$
\mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, z^l) \right\|_2^2 \right] \leq \mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, c) \right\|_2^2 \right]
$$

Assuming that the neural network $ \boldsymbol{\epsilon}_\theta $ is capable of approximating the optimal predictor $ \boldsymbol{\epsilon}_\theta^\ast $, especially as training progresses and the model capacity is sufficient, we can write:

$$
\left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, z^l) \right\|_2^2 \approx \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, z^l) \right\|_2^2 
$$

Similarly for $ c $

$$
\left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, c) \right\|_2^2 \approx \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta^\ast(\mathbf{x}_t^l, t, c) \right\|_2^2 .
$$

Therefore, the expected squared error satisfies:

$$
\mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, z^l) \right\|_2^2 \right] \leq \mathbb{E}\left[ \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, c) \right\|_2^2 \right]
$$


Since the term of $ \Delta_{\text{less-preferred}} $ in the loss function involves the difference of squared errors, using $ z^l $ instead of $ c $ for the less preferred sample results in a lower error term:

$$
\Delta_{\text{less-preferred}}^{(z^l)} = \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, z^l) \right\|_2^2 - \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^l, t, z^l) \right\|_2^2
$$

Comparing with the original:

$$
\Delta_{\text{less-preferred}}^{(c)} = \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^l, t, c) \right\|_2^2 - \left\| \boldsymbol{\epsilon}^l - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^l, t, c) \right\|_2^2
$$

Assuming the reference model $ \boldsymbol{\epsilon}_{\text{ref}} $ remains the same or also benefits similarly from the additional information in $ z^l $, the net effect is that the first term decreases more than the second term, leading to a reduced $ \Delta_{\text{less-preferred}} $.
% Law of Total Variance (conditional variance formula): Ross, S. M. (2014). Introduction to Probability Models (11th ed.). Academic Press. 
\\
\\
\paragraph{Proof 3} \textit{Replacing caption $ c $ with the specifically generated caption $ z^w $ for the preferred image $ \mathbf{x}_0^w $ increases $\Delta_{\text{preferred}}$.}
\\
\\
To prove that replacing $ \mathbf{c} $ with $ \mathbf{z}^w  \sim Q(z^w|x^w, c) $, where $ \mathbf{c} \subset \mathbf{z}^w $, for $ \mathbf{x}_0^w $ also contributes to a better optimized loss $L(\theta)$, we examine how this particular substitution affects the loss function.
\\
We let
$$
R_\theta(\mathbf{c}) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^w, t, \mathbf{c}) \|_2^2,
$$
$$
R_{\text{ref}}(\mathbf{c}) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^w, t, \mathbf{c}) \|_2^2.
$$
\\
The rate of decrease in $ R_\theta $ due to $ \mathbf{z}^w $ is proportional to the model's ability to exploit the additional conditioning. Since $ \boldsymbol{\epsilon}_\theta $ is learnable, it can more effectively leverage $ \mathbf{z}^w $ than $ \boldsymbol{\epsilon}_{\text{ref}} $, yielding:
$$
\Delta R_\theta = R_\theta(\mathbf{c}) - R_\theta(\mathbf{z}^w) \gg \Delta R_{\text{ref}} = R_{\text{ref}}(\mathbf{c}) - R_{\text{ref}}(\mathbf{z}^w).
$$

% Thus:
% $$
% \Delta_{\text{preferred}} = R_\theta - R_{\text{ref}}
% $$
% increases as $ R_\theta $ decreases faster than $ R_{\text{ref}} $ we will show in Proof 5.



% In conclusion, replacing $ \mathbf{c} $ with $ \mathbf{z}^w $ for the preferred image $ \mathbf{x}_t^w $ improves optimization by increasing $ \Delta_{\text{preferred}} $, leading to a larger margin $ \Delta_{\text{preferred}} - \Delta_{\text{less-preferred}} $. This enhances soft-margin optimization, resulting in faster convergence and better differentiation between preferred and less preferred predictions.


% Proof 5: 
We further elaborate on why the learnable model's noise prediction residual ($ R_\theta $) decreases faster than the reference model's residual ($ R_{\text{ref}} $) when $ \mathbf{c} $ is replaced by $ \mathbf{z}^w $. The residuals for the learnable and reference models are defined as:

$$
R_\theta(\mathbf{c}) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^w, t, \mathbf{c}) \|_2^2,
$$
$$
R_{\text{ref}}(\mathbf{c}) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^w, t, \mathbf{c}) \|_2^2.
$$

When $ \mathbf{c} $ is replaced with $ \mathbf{z}^w $ (where $ \mathbf{c} \subset \mathbf{z}^w $), the residuals become:

$$
R_\theta(\mathbf{z}^w) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_\theta(\mathbf{x}_t^w, t, \mathbf{z}^w) \|_2^2,
$$
$$
R_{\text{ref}}(\mathbf{z}^w) = \| \boldsymbol{\epsilon}^w - \boldsymbol{\epsilon}_{\text{ref}}(\mathbf{x}_t^w, t, \mathbf{z}^w) \|_2^2.
$$

The rate of decrease for each residual is defined as:
$$
\Delta R_\theta = R_\theta(\mathbf{c}) - R_\theta(\mathbf{z}^w),
$$
$$
\Delta R_{\text{ref}} = R_{\text{ref}}(\mathbf{c}) - R_{\text{ref}}(\mathbf{z}^w).
$$

The quality of conditioning, $ Q(\mathbf{c}) $, represents how well the conditioning $ \mathbf{c} $ aligns with the true noise~$ \boldsymbol{\epsilon}^w $. We assume that
$$
Q(\mathbf{z}^w) > Q(\mathbf{c}),
$$

where the improvement in conditioning quality $ \Delta Q $ is defined as
$$
\Delta Q = Q(\mathbf{z}^w) - Q(\mathbf{c}).
$$


The residual for $ R_\theta $ is proportional to the misalignment between $ Q(\mathbf{c}) $ and $ \boldsymbol{\epsilon}^w $:
 $$
 R_\theta(\mathbf{c}) \propto \frac{1}{Q(\mathbf{c})}.
 $$

Replacing $ \mathbf{c} $ with $ \mathbf{z}^w $ (higher $ Q $) results in a larger proportional reduction:
 $$
 R_\theta(\mathbf{z}^w) \propto \frac{1}{Q(\mathbf{z}^w)} \quad \text{with} \quad \Delta R_\theta \propto \Delta Q.
 $$


The reference model's residual $ R_{\text{ref}} $ depends weakly on $ Q(\mathbf{c}) $, as it is fixed or less adaptable:
$$
R_{\text{ref}}(\mathbf{c}) \propto \frac{1}{Q_{\text{ref}}(\mathbf{c})},
$$

where $ Q_{\text{ref}}(\mathbf{c}) $ is less sensitive to changes in $ \mathbf{c} $.

Thus, the proportional improvement in $ R_\theta $ due to $ \Delta Q $ is significantly larger than for $ R_{\text{ref}} $.



The preferred difference term is:
$$
\Delta_{\text{preferred}} = R_\theta - R_{\text{ref}}.
$$

As $ R_\theta $ decreases significantly more than $ R_{\text{ref}} $, the gap $ R_\theta - R_{\text{ref}} $ becomes larger, increasing $ \Delta_{\text{preferred}} $:
$$
\Delta R_\theta \gg \Delta R_{\text{ref}} \implies \Delta_{\text{preferred}} \text{ increases.}
$$


The learnable model $ \boldsymbol{\epsilon}_\theta $ benefits more from the improved conditioning $ \mathbf{z}^w $ because of its adaptability and training dynamics. This results in a larger reduction in $ R_\theta $ compared to $ R_{\text{ref}} $. Mathematically, the relative rate of decrease:
$$
\text{Relative Rate} = \frac{\Delta R_\theta}{\Delta R_{\text{ref}}} \gg 1,
$$

which ensures that $ \Delta_{\text{preferred}} $ also increases, hence improving the optimization process in $L(\theta)$ and helping the model distinguish predictions on preferred and less preferred image-captions more effectively.


\section{More Insights on DCPO}
\label{sec:appendix_more_insights}



A preference alignment dataset, such as Pick-a-Pic \citep{kirstain2023pickapic}, is defined as $ D = \{c, x^w, x^l\} $, where $ x^w $ and $ x^l $ represent the preferred and less preferred images for the prompt $ c $. Diffusion-KTO \citep{li2024aligning} hypothesizes the optimization of a diffusion model using only a single preference label based on whether an image $ x $ is suitable or not for a given prompt $ c $.  Diffusion-KTO uses a differently formatted input dataset $ D = \{c, x\} $, where $ x $ is a generated image corresponding to the prompt $ c $.

\begin{table*}[h]
\centering
\small
\begin{tabular}{c|ccccc}
\toprule
\textbf{Method} & \textbf{GenEval ($\uparrow$)}         & \textbf{Pickscore ($\uparrow$)}      & \textbf{HPSv2.1 ($\uparrow$)} & \textbf{ImageReward ($\uparrow$)} & \textbf{CLIPscore ($\uparrow$)} \\ \midrule
Diffusion-KTO  &   0.5008   &  20.41   &   24.80    &   55.5   &    26.95  \\ 
DCPO-h & \textbf{0.5100}   & \textbf{20.57}  &   \textbf{25.62}   &    \textbf{58.2}    &   \textbf{27.13}   \\ \bottomrule
\end{tabular}
\caption{Comparison of DCPO-h and Diffusion-KTO across various benchmarks.}
\label{tab:comparsion_dcpo_kto}
\end{table*}



Diffusion-KTO's hypothesis is fundamentally different from our DCPO's. While Diffusion-KTO focuses on binary preferences (like/dislike) for individual image-prompt pairs, our approach involves paired preferences. We observe that using the same prompt $ c $ for both preferred and less preferred images may not be ideal. To address this, we propose optimizing a diffusion model using a dataset in terms of $ D = \{z^w, z^l, x^w, x^l\} $, where $ z^w $ and $ z^l $ are the captions generated by a static captioning model $ Q_\phi $ for the preferred and less preferred images, respectively, referring to the original prompt.

% \input{Tables/support_hyphotesis2}
% \input{Tables/compare_dpo}
We nonetheless conduct comparisons between Diffusion-KTO and DCPO on various preference alignment benchmarks. The results in Table \ref{tab:comparsion_dcpo_kto} show that our DCPO-h consistently outperforms Diffusion-KTO on all benchmarks, demonstrating the effectiveness of our DCPO method.

