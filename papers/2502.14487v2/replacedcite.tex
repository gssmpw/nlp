\section{Related Work}
% \noindent \textbf{ANN-SNN Conversion} 
% ANN-SNN conversion aims to adapt pre-trained ANN parameters for SNNs, ensuring minimal accuracy loss by aligning activation outputs (e.g., ReLU, SiLU, and Sigmoid) with SNN firing rates. Early work____ validated this concept, with subsequent methods like weight normalization____ and reset-by-subtraction____ enhancing conversion accuracy. Advanced methods like dynamic threshold adjustments____ and novel activation functions____ further improved SNN performance. Approaches to address conversion errors include tailored weight normalization____, quantization clip-floor-shift activation____, and burst spikes____. Adjustments to membrane potential____ have also been implemented to boost accuracy. Our work is part of this category.

% \noindent \textbf{Direct Training of SNN}
% Direct training of SNNs leverages methods like spatial-temporal backpropagation through time (BPTT) and surrogate gradients____, addressing the challenges of non-differentiable spike functions and high computational costs in deep architectures. Direct training focuses on optimizing synaptic weights, firing thresholds____, and leaky factors____. Novel loss functions____ and hybrid training methods____ have improved performance under low latency. Recent advancements like Ternary Spike____ and reversible SNNs____ further enhance training efficiency and accuracy. 

% \noindent \textbf{Statistical SNNs}
% Bayesian methods like the mixDPFA model by____ cluster neurons based on dynamic Poisson factor analyzers and Markov chain Monte Carlo (MCMC) sampling to sample the posterior distribution, capturing temporal dependencies and interactions between clusters. ____ extended this with a bi-clustering method that clusters neural activity both spatially and temporally using latent trajectories and synchronous local dynamics. This non-parametric model avoids predefined cluster numbers and uses MCMC for accurate results.