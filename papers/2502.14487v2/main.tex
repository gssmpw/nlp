%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

%%%%% Added packages %%%%%%
% \usepackage{wrapfig}
\usepackage{amsthm}         % proof
\usepackage{amsmath}
\usepackage{thmtools,thm-restate} % restatable theorem (used in proof)
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
% \usepackage{xcolor}       % colors
\usepackage{multirow}       % table, support multi rows
\usepackage{threeparttable} % table
\usepackage{tabularx}       % table
\usepackage{graphicx}       % to use scalebox command in the table, For resizing boxes, including tables
\usepackage{xspace}         % use \xspace in the macros
\usepackage[table]{xcolor}  % for cellcolor, newcommand, gray

\usepackage{booktabs}       % For better table lines
\usepackage{array}          % For better columns management
\usepackage{makecell}       % For multi-line cells

\usepackage{hyperref}       % web link
\usepackage{amsmath}
\usepackage{thmtools,thm-restate}%
\usepackage{todonotes}
\usepackage{subcaption}
\usepackage{lipsum}         % For generating random text
% \usepackage{tabularx}
\usepackage{wrapfig}
\usepackage{algorithm}      % use the icml algorithm.sty and algorithmic.sty files
\usepackage{algorithmic}

\usepackage{diagbox}        % table 
\usepackage{threeparttable}
\usepackage{colortbl}
\usepackage[most]{tcolorbox} % wxf comment color setting


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                              macros                                      %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\def\method{ProbSNN\xspace}
\newcommand{\cmark}{\color{ForestGreen}\ding{51}}
\newcommand{\xmark}{\color{Red}\ding{55}}

%%% comments by xiaofeng wu
% \newcommand{\wxf}[1]{{\color{magenta}#1}}
% \newcommand{\wxf}[1]{{\colorbox{blue}{\textcolor{white}{#1}}}}

\newcommand{\wxf}[1]{%
    \begin{tcolorbox}[
        colback=blue,
        coltext=white,
        boxrule=0mm,
        arc=2mm,
        left=1mm,
        right=1mm,
        top=1mm,
        bottom=1mm,
        enhanced jigsaw,
        breakable
    ]
    #1
    \end{tcolorbox}%
}

% \newcommand{\veb}[1]{{\color{blue}#1}}

\newcommand{\veb}[1]{%
    \begin{tcolorbox}[
        colback=red,
        coltext=white,
        boxrule=0mm,
        arc=2mm,
        left=1mm,
        right=1mm,
        top=1mm,
        bottom=1mm,
        enhanced jigsaw,
        breakable
    ]
    #1
    \end{tcolorbox}%
}




%%% gray cell to highlight the best results, usage \gc{74.86} 
\newcommand{\gc}[1]{\cellcolor{gray!35} #1}
% \def\relu{{\text{ReLU }}}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\bX}{\mathbf{X}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\bg}{\mathbf{g}}
\newcommand{\bW}{\mathbf{W}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\bk}{\mathbf{k}}
\newcommand{\bs}{\mathbf{s}}
\newcommand{\cR}{\mathcal{R}}
\newcommand{\bh}{\mathbf{h}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\bH}{{\mathbb{H}}}
% \newcommand{\bW}{\mathbf{W}}
\newcommand{\bu}{\mathbf{u}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\expE}{\mathop{\mathbb{E}}}


\newcommand{\thr}{{\rm{th}}}
\newcommand{\relu}{{\rm ReLU} }
\newcommand{\silu}{{\rm SiLU} }
\newcommand{\lup}{{(l)}}
\newcommand{\lupm}{{(l-1)}}
\newcommand{\cC}{\mathcal{C}}

%%% Theorem, Proposition, Lemma
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\theoremstyle{definition}  % Set the style for subsequent environments
\newtheorem{rmk}{Remark}


\definecolor{SkyBlue}{rgb}{0.53, 0.81, 0.92} % define color SkyBlue

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%                           renew command                                  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Redefine the thickness of the top rule to adjust the thickness of the \toprule.
\renewcommand{\heavyrulewidth}{0.2em}  % Set this to your desired thickness



% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
% \newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{arxiv_icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
% \theoremstyle{remark}
% \newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}








% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Temporal Misalignment and Probabilistic Neurons}

\begin{document}

\twocolumn[
\icmltitle{Temporal Misalignment in ANN-SNN Conversion and \\ Its Mitigation via Probabilistic Spiking Neurons}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{V. X. BojWu}{equal,amal}
\icmlauthor{Velibor Bojkovi\'c}{equal,mbzuai}
\icmlauthor{Xiaofeng Wu}{equal,citym}
\icmlauthor{Bin Gu}{juni}
% \icmlauthor{Firstname5 Lastname5}{yyy}
% \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% \icmlauthor{Firstname7 Lastname7}{comp}
% %\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{amal}{Amalgamation of first authors' names}
% \icmlaffiliation{equal}{Equal contribution}
\icmlaffiliation{mbzuai}{Department of ML, MBZUAI, Abu Dhabi, UAE}
\icmlaffiliation{citym}{Faculty of Data Science, City University of Macau, Macau, China}
\icmlaffiliation{juni}{School of Artificial Intelligence, Jilin University, China}

\icmlcorrespondingauthor{Velibor Bojkovi\'c}{first.last@mbzuai.ac.ae}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
% In the age of large neural network models and their high energy demand, Spiking Neural Networks (SNNs) offer a compelling alternative to Artificial Neural Networks (ANNs) due to their energy efficiency and resemblance to biological brains. However, directly training SNNs with spatio-temporal backpropagation remains challenging due to their discrete signal processing and temporal dynamics. Alternative methods, notably ANN-SNN conversion, have enabled SNNs to achieve performance in various machine learning tasks, comparable to ANNs. The present work identifies a phenomenon we term ``temporal misalignment'', where random spike rearrangement through time in the converted SNN model improves its performance. To account for this, we propose two-phase probabilistic spiking neurons to be used in ANN-SNN conversion, as well as their two-phase deterministic counterparts. Finally, we propose a method to decrease the latency of such constructed converted SNNs through the first-order synaptic models. We showcase the benefits of our proposed methods on CIFAR-10/100 and a large-scale dataset ImageNet over a variety of architectures, reaching SOTA performance. Code is available on GitHub. \\

% \wxf{"deals with ANN-SNN setting" is not formal (I also ask DeepSeek and ChatGPT); The abstract is too lengthy, with too much setup at the beginning, and it does not explain the key/essence of the proposed method. It is recommended not to use ``temporal misalignment'', as it is not direct and difficult to understand. }

% In the context of increasingly large neural network models and their associated high energy consumption, Spiking Neural Networks (SNNs) present a compelling alternative to Artificial Neural Networks (ANNs) due to their energy efficiency and closer alignment with biological neural principles. However, directly training SNNs with spatio-temporal backpropagation remains challenging due to their discrete signal processing and temporal dynamics. Alternative methods, notably ANN-SNN conversion, have enabled SNNs to achieve performance in various machine learning tasks, comparable to ANNs, but often to the expense of long latency needed to achieve such performance, especially on large scale complex datasets. The present work deals with ANN-SNN setting and identifies a new phenomenon we term ``temporal misalignment'', where random spike rearrangement through time in the converted SNN model improves its performance. To account for this, we propose bio-plausible, two-phase probabilistic (TPP) spiking neurons to be used in ANN-SNN conversion. We showcase the benefits of our proposed methods both theoretically and empirically through extensive experiments on CIFAR-10/100 and a large-scale dataset ImageNet over a variety of architectures, reaching SOTA performance. % Code is available on GitHub. 

% Spiking Neural Networks (SNNs) offer an energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, making them well-suited for addressing the growing energy demands of large neural models. However, harnessing the full potential of SNNs remains challenging due to their discrete signal processing and temporal dynamics. To overcome these challenges, ANN-SNN conversion has emerged as a practical approach, enabling SNNs to achieve competitive performance on complex machine learning tasks, albeit often at the cost of long latency. In this work, we uncover a phenomenon called "temporal misalignment," where random spike rearrangement improves SNN performance. We further exploit it with the introduction of bio-plausible two-phase probabilistic (TPP) spiking neurons, enhancing ANN-SNN conversion. Our approach achieves state-of-the-art performance on CIFAR-10/100 and ImageNet across various architectures, as demonstrated through extensive theoretical and empirical evaluations.

% Spiking Neural Networks (SNNs) offer an energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, making them well-suited for addressing the growing energy demands of large neural models. However, harnessing the full potential of SNNs remains challenging due to their discrete signal processing and temporal dynamics. To overcome these challenges, ANN-SNN conversion has emerged as a practical approach, enabling SNNs to achieve competitive performance on complex machine learning tasks. In this work, we uncover a phenomenon in ANN-SNN conversion framework we name "temporal misalignment," where random spike rearrangement throughout SNN layers improves SNN performance. We further exploit it with the introduction of bio-plausible two-phase probabilistic (TPP) spiking neurons, enhancing ANN-SNN conversion. Our approach achieves state-of-the-art performance on CIFAR-10/100 and ImageNet across various architectures, as demonstrated through extensive theoretical and empirical evaluations.


Spiking Neural Networks (SNNs) offer a more energy-efficient alternative to Artificial Neural Networks (ANNs) by mimicking biological neural principles, establishing them as a promising approach to mitigate the increasing energy demands of large-scale neural models. However, fully harnessing the capabilities of SNNs remains challenging due to their discrete signal processing and temporal dynamics. ANN-SNN conversion has emerged as a practical approach, enabling SNNs to achieve competitive performance on complex machine learning tasks. In this work, we identify a phenomenon in the ANN-SNN conversion framework, termed \textit{temporal misalignment}, in which random spike rearrangement across SNN layers leads to performance improvements. Based on this observation, we introduce biologically plausible two-phase probabilistic (TPP) spiking neurons, further enhancing the conversion process. We demonstrate the advantages of our proposed method both theoretically and empirically through comprehensive experiments on CIFAR-10/100, CIFAR10-DVS, and ImageNet across a variety of architectures, achieving state-of-the-art results.
\end{abstract}

\section{Introduction}
\label{sec:intro}

\begin{figure}
\centering
\includegraphics[width=\linewidth]{./figures/image.pdf}
% \includegraphics[width=0.93\linewidth]{./figures/init_exp.png}
\caption{\footnotesize The initial experiment: After ANN-SNN conversion, we compared the accuracy of the baseline model QCFS \citep{bu2022optimal} with its ``permuted'' version and our proposed TPP neurons (setting is VGG16 - ImageNet, ANN acc. 74.29\%).}
\label{fig-init-exps}
\vspace{-10pt}
\end{figure}
\vspace{-2pt} % Adjusted value to reduce space by approximately two lines of text


% Spiking neural networks (SNNs), often referred to as the third generation of neural networks~\cite{maas1997third}, are inspired by and designed to mimic how biological neurons process and share information~\cite{mcculloch1943logical,hodgkin1952quantitative,izhikevich2003simple}. The efficiency of biological brains in terms of both energy use and task performance has long inspired the development of neural networks with similar capabilities. This inspiration has driven the growing interest in SNNs, particularly in an era when large machine learning models demand increasingly high energy consumption. The main difference from the artificial neural networks (ANNs)~\cite{braspenning1995artificial} comes from the way spiking neurons in an SNN process information. Spiking neurons communicate through a series of (discrete, often binary) spikes, emulating the biological brain's communication via electrical pulses. The (weighted) incoming spikes are accumulated in the neuron's membrane potential, and a spike is emitted only when the potential reaches a threshold. This makes SNN processing event-driven and binary, and multiplication, as an energy demanding operation, is eliminated from the process. In contrast, ANNs process information using floating-point operations, which rely on multiplication, leading to energy-inefficient deep learning models at large scales~\cite{roy2019towards}. 

% Furthermore, recent advancements in neuromorphic chip production~\cite{pei2019towards, debole2019truenorth, loihi2, DBLP:journals/corr/abs-2312-17582} further emphasize the advantages of SNN models. These chips, specifically designed to support and embed SNN models in hardware aware and efficient way, have opened new aspects of interest in SNNs, and various SNN models have been challenging traditional neural networks in various domains, including object detection~\cite{kim2020spiking, cheng2020lisnn}, object tracking~\cite{yang2019dashnet}, video reconstruction~\cite{zhu2022event}, event camera and point clouds~\cite{DBLP:journals/iclr/SpikePoint}, speech recognition~\cite{DBLP:conf/aaai/WangZHWZX23} and generative models~\cite{kamata2022fully} such as SpikingBERT~\cite{bal2023spikingbert} and SpikeGPT~\cite{DBLP:journals/corr/abs-2302-13939, wang2023masked}, to name a few. 


Spiking neural networks (SNNs), often referred to as the third generation of neural networks~\cite{maas1997third}, closely mimic biological neuronal communication through discrete spikes~\cite{mcculloch1943logical,hodgkin1952quantitative,izhikevich2003simple}. While biological energy efficiency has historically influenced neural network design, SNNs uniquely achieve this through event-driven processing: weighted inputs integrate into membrane potentials, emitting binary spikes only upon crossing activation thresholds. This differs significantly from artificial neural networks (ANNs)~\cite{braspenning1995artificial}, which are based on continuous floating-point operations that require energy-intensive and computationally costly multiplication operations~\cite{roy2019towards}. The spike-driven paradigm inherently circumvents these costly computations, suggesting that SNNs may offer a promising approach for energy-efficient AI. Recent developments in neuromorphic hardware~\cite{pei2019towards, debole2019truenorth, loihi2, DBLP:journals/corr/abs-2312-17582} have enabled efficient SNN deployment. These specialized chips are inherently designed for spike-based computation, driving breakthroughs across domains: object detection~\cite{kim2020spiking, cheng2020lisnn}, tracking~\cite{yang2019dashnet}, event-based vision~\cite{zhu2022event, DBLP:journals/iclr/SpikePoint}, speech recognition~\cite{DBLP:conf/aaai/WangZHWZX23}, and generative AI through models like SpikingBERT~\cite{bal2023spikingbert} and SpikeGPT~\cite{DBLP:journals/corr/abs-2302-13939, wang2023masked}. These advancements underscore the potential of SNNs as a viable alternative to conventional ANNs.

Training SNNs is inherently challenging due to the same characteristics that confer their advantages: their discrete processing of information. Unsupervised direct training, inspired by biological learning mechanisms, leverages local learning rules and spike timing to update weights~\cite{diehl2015unsupervised}. While these methods are computationally efficient and can be executed on specialized hardware, SNNs trained in this manner often underperform compared to models trained with alternative approaches. Further research is needed to better understand and improve this method. In contrast, supervised training can be categorized into direct training via spatio-temporal backpropagation (e.g., surrogate gradient methods~\cite{neftci2019surrogate}) and ANN-SNN conversion methods~\cite{diehl2015fast, cao2015spiking}. The present work focuses on the latter approach.

% On the other side, supervised training methods can be categorized in two branches: direct training and ANN-SNN conversion based methods. The main challenge for direct training methods lies in the discrete nature of spike production. Namely, the operation of comparison of the membrane potential with the threshold is not differentiable, or, where it is, does not produce useful gradients. The success of direct training hinges on the development of spatio-temporal backpropagation through time (BPTT) and surrogate gradient methods~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio, bellec2018long, fang2021deep, fang2021incorporating, zenke2021remarkable, mukhoty2024direct}. Although, they address and overcome the main problem of non-differentiability of spikes, these methods encounter further challenges with deep architectures due to gradient instability and high computational costs during training simulations. Direct training focuses on optimizing not only synaptic weights but also dynamic parameters like firing thresholds~\cite{wei2023temporal} and leaky factors~\cite{DBLP:journals/tnn/RathiR23}. Novel loss functions such as rate-based counting loss~\cite{zhu2024exploring} and distribution-based loss~\cite{guo2022recdis} were proposed to provide sufficient positive gradients and rectify the distribution of membrane potential during the propagation of binary spikes. Furthermore, hybrid training methods~\cite{DBLP:journals/corr/abs-2205-07473} combine ANN-SNN conversion with BPTT to achieve higher performance with low latency. Recent advancements include Ternary Spike~\cite{DBLP:conf/aaai/GuoCLPZHM24} for enhanced information capacity and the reversible SNN~\cite{zhang2023memory} to reduce memory costs during training.

\begin{figure}[!t]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/shuffle-3d.png}
  \caption{Spike train permutation: spikes at different time steps are shuffled to alter their temporal order.}
  \label{fig:permutation}
\end{figure}

The core idea of ANN-SNN conversion is to leverage pre-trained ANN models to train SNNs. This process begins by transferring the weights from the ANN to the SNN, which shares the same architecture, and initializing the spiking neuron parameters (e.g., thresholds, time constants) so that the spike rates approximate the activation values of the corresponding ANN layers. This method is advantageous because it typically requires no additional computation for training the SNN, thereby eliminating the need for gradient computation or limiting it to fine-tuning the SNN.

\textbf{The initial experiment: Temporal information in SNNs after ANN-SNN conversion.} We begin by identifying a counter-intuitive phenomenon that, to the best of our knowledge, has not been previously documented. We investigate the extent to which individual spike timing affects the overall performance of the SNN model in ANN-SNN conversion. We explore this question in the context of various types of conversion-related issues described in the literature, such as \textit{phase lag}~\cite{li2022bsnn} and \textit{unevenness of the spike input}~\cite{bu2022optimal}, where prior work has indicated that the timing of spike trains in SNNs obtained through ANN-SNN conversion may not be optimal and can lead to performance degradation under low-latency conditions.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.95\linewidth]{figures/prob-mode-3d.png}
  \caption{Two-Phase Probabilistic (TPP) spiking neuron mechanism operates in two phases: accumulation of inputs and probabilistic spiking based on membrane potential. The model uses a Bernoulli process for spike generation, with membrane potential updates over time steps.}
  \label{fig:prob-mode}
  \vspace{-5pt}
\end{figure}
% \raggedbottom

In our initial experiment, we began by examining several widely-used ANN-SNN conversion methods and analyzing the spike outputs of spiking layers in the corresponding SNN models. To evaluate the importance of spike timing relative to firing rates, we randomly permuted the spike trains after each spiking layer. Specifically, when processing samples through the baseline model, we rearranged the temporal order of spikes within each spike train after each spiking layer (see Figure \ref{fig:permutation}). The permuted spike trains were then passed to the subsequent layer, and this process was repeated for each layer until the output layer. The results of one of these initial experiments, comparing the performance of the ``permuted'' model with the baseline, are presented in Figure~\ref{fig-init-exps}. For each latency, we conducted multiple trials with different permutations, and the ``permuted'' model consistently outperformed the baseline, achieving the original ANN accuracy at lower latencies. The impact of permutations on performance was particularly pronounced at lower latencies.

We refer to this phenomenon as ``\textit{temporal misalignment}'' in ANN-SNN conversion and further investigate it by providing a conceptual interpretation in the form of bursting probabilistic spiking neurons, which are designed to mimic the effects of permutations in SNNs. The proposed neurons operate in two phases, as illustrated in Figure~\ref{fig:prob-mode}. In the first phase, they accumulate input, often surpassing the threshold, while in the second phase, they emit spikes probabilistically with varying temporal probabilities. Our proposed spiking neurons are characterized by two key properties: (1) the accumulation of membrane potential beyond the threshold, resulting in a firing phase, commonly referred to as bursting, and (2) probabilistic firing. Both properties exhibit biological plausibility and have been extensively studied in the neuroscience literature (see Section~\ref{sec bio inspired}).

% Namely, when performing ANN-SNN conversion, the main assumption is that the sole carrier of information is the rate of the spiking activity, and precise timing of the spikes should not affect the performance of the SNN~\cite{bu2023rate}. \wxf{START: this is vague.} We challenged this assumption by using a baseline SNN obtained through ANN-SNN conversion, following methods proposed in recent literature. \wxf{END.} Then, when passing the samples to the baseline model,  after each spiking layer we permuted the spike trains by rearranging the spikes in the temporal dimension. Specifically, the temporal order of spikes within each spike train was randomly shuffled. The permuted spike trains were then passed to the following layer in the SNN, and this process was continued until the output layer. The results of one of these initial experiments, comparing the performance of the ``permuted'' model with the original model, are presented in Figure~\ref{fig-init-exps}. For every latency we performed this experiment, the ``permuted'' model surpassed the baseline and reached the original ANN accuracy much earlier. We dubbed this occurrence ``the temporal misalignment'' in ANN-SNN conversion and further explored it by giving it a more conceptual flavor in form of the bursting probabilistic spiking neurons which are designed to mimic the effect of permutations in SNNs. \wxf{START: ``in a probabilistic manner'' is vage. Describe exactly waht does the probabilistic neuron / method work, its procedure and the essence behind it.} The proposed neurons work in two-phases, in the first phase they collect the input (often beyond the threshold) while in the second they output spikes in a probabilistic manner with varying temporal probabilities. Two crucial properties that define our proposed spiking neurons, namely the accumulation of membrane potential beyond the threshold and entering into a firing phase (bursting), and probabilistic firing are bio-plausible, and were extensively studied in the neuroscience literature (see Section \ref{sec bio inspired}). 

The main contributions of this paper are summarized as:

$\bullet{}$ We recognize and study the ``temporal misalignment'' phenomenon in ANN-SNN conversion, and we propose a framework for its exploitation in ANN-SNN conversion utilizing two-phase probabilistic spiking neurons. We provide theoretical insights into their functioning and superior performance, as well as support for their biological grounding.

$\bullet{}$ We present a comprehensive experimental validation demonstrating that our method outperforms SOTA conversion as well as the other training methods in terms of accuracy on CIFAR-10/100, CIFAR10-DVS, and ImageNet datasets.



% \begin{itemize}
% \item We recognize and study the ``temporal misalignment'' phenomenon in ANN-SNN conversion, and we propose a framework for its exploitation in ANN-SNN conversion utilizing two-phase probabilistic spiking neurons. We provide theoretical insights into their functioning and superior performance, as well as support for their biological grounding. 
% \item We present a comprehensive experimental validation that demonstrates that our proposed method outperforms state-of-the-art conversion as well as the other training methods, in terms of accuracy on large scale CIFAR-10/100 and ImageNet datasets.
% \end{itemize}


% \section{Related Work}

% \noindent \textbf{ANN-SNN Conversion} 
% ANN-SNN conversion aims to adapt pre-trained ANN parameters for SNNs, ensuring minimal accuracy loss by aligning activation outputs (e.g., ReLU, SiLU, and Sigmoid) with SNN firing rates. Early work~\cite{cao2015spiking} validated this concept, with subsequent methods like weight normalization~\cite{Diehl2015Fast} and reset-by-subtraction~\cite{rueckauer2017conversion} enhancing conversion accuracy. Advanced methods like dynamic threshold adjustments~\cite{han2020rmp, stockl2021optimized, ho2021tcl, DBLP:journals/tnn/WuCZLLT23} and novel activation functions~\cite{DBLP:conf/iclr/BuFDDY022} further improved SNN performance. Approaches to address conversion errors include tailored weight normalization~\cite{Sengupta2018Going}, quantization clip-floor-shift activation~\cite{DBLP:conf/iclr/BuFDDY022}, and burst spikes~\cite{DBLP:conf/ijcai/Li022}. Adjustments to membrane potential~\cite{li2021free, DBLP:conf/iclr/HaoDB0Y23} have also been implemented to boost accuracy. Our work is part of this category.

% \noindent \textbf{Direct Training of SNN}
% Direct training of SNNs leverages methods like spatial-temporal backpropagation through time (BPTT) and surrogate gradients~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio}, addressing the challenges of non-differentiable spike functions and high computational costs in deep architectures. Direct training focuses on optimizing synaptic weights, firing thresholds~\cite{wei2023temporal}, and leaky factors~\cite{DBLP:journals/tnn/RathiR23}. Novel loss functions~\cite{zhu2024exploring, guo2022recdis} and hybrid training methods~\cite{DBLP:journals/corr/abs-2205-07473} have improved performance under low latency. Recent advancements like Ternary Spike~\cite{DBLP:conf/aaai/GuoCLPZHM24} and reversible SNNs~\cite{zhang2023memory} further enhance training efficiency and accuracy. 

% \noindent \textbf{Statistical SNNs}
% Bayesian methods like the mixDPFA model by~\cite{DBLP:conf/nips/WeiSW22} cluster neurons based on dynamic Poisson factor analyzers and Markov chain Monte Carlo (MCMC) sampling to sample the posterior distribution, capturing temporal dependencies and interactions between clusters. \cite{wei2023bayesian} extended this with a bi-clustering method that clusters neural activity both spatially and temporally using latent trajectories and synchronous local dynamics. This non-parametric model avoids predefined cluster numbers and uses MCMC for accurate results.


\section{Background and Related Work} \label{sec:preliminaries} 
The base model employed in this work is Integrate-and-Fire (IF) spiking neuron, whose internal dynamics, after discretization, are given by the equations
% \begin{subequations} 
\begin{align}\label{eq: dynamics}
\bv^\lup[t] &= \bv^\lup[t-1] +\bW^\lup \theta^{(l-1)}\cdot \bs^{(l-1)}[t]-\theta^\lup \cdot\bs[t-1],\\
\bs^\lup[t] &= H(\bv^\lup[t]- \theta^\lup).%\nonumber
\end{align}
% \end{subequations}
Here, $\theta^\lup$ is the threshold, $H(\cdot)$ is the Heaviside function, while the superscript $l$ denotes the layer in the SNN. Later, we will modify these equations and use more advanced neuron models. By expanding the equations over $t=1,\dots,T$, and rearranging the terms, we obtain
\begin{align}
 \theta^\lup \frac{\sum_{t=1}^T\bs^{(l)}[t]}{T} &= \bW^{(l)}V^{(l-1)}_\thr\frac{\sum_{t=1}^T\bs^{(l-1)}[t]}{T}\label{eq: rel}\\
 &+\frac{\bv^{(l)}[T] - \bv^{(l)}[0]}{T}.\label{eq: rel err}
 \end{align}
 % \end{equation}
On the ANN side, the transformation between layers is given by 
\begin{equation}\label{eq: ann eq}
a^{(l)}= \mathcal{A}^{(l)}(\bW^{(l)} a^{(l-1)}),
\end{equation}
where $\mathcal{A}^\lup$ is the activation function. The ANN-SNN conversion process begins by transferring the weights (and biases) of a pre-trained ANN to an SNN with the same architecture. By comparing the equations for the ANN outputs \eqref{eq: ann eq} and the average output of the SNN \eqref{eq: rel} \cite{rueckauer2016theory}, the goal is to achieve a relation of the form
 \begin{equation}\label{eq: ann snn}
 a^{(l)}_i \approx  V^{(l)}_\thr \frac{\sum_{t=1}^T\bs^{(l)}_i[t]}{T}.
 \end{equation}
The most commonly used activation function $\mathcal{A}$ is \relu, due to its simplicity and non-negative output, which aligns well with the properties of IF neurons. It is important to note the importance of the three components in the conversion: 1) the threshold value $\theta$, 2) the initialization $\bv[0]$, 3) the ANN activation function $\mathcal{A}$.

\subsection{Related work}

% \veb{The structure of this section should be:
% \begin{itemize}
%     \item Pure ANN-SNN, here the only idea is to find appropriate thresholds and mem potential initialization without further training
%     \item Maybe to mention in the previus section that many of the methods are based on quantization of the ANN activation, and add here all the references mentioned in the rebuttal by the reviewers.
%     \item Hybrid method, they continue upon ANN-SNN by further training
% \end{itemize}
% }


% \textbf{ANN-SNN conversion} leverages pre-trained ANNs to initialize SNNs, aiming to minimize accuracy degradation by aligning ANN activations with SNN firing rates, as demonstrated in early works~\cite{rueckauer2016theory, cao2015spiking}. Subsequent studies addressed conversion errors and improved temporal accuracy through techniques like weight normalization~\cite{Diehl2015Fast}, soft-reset mechanisms~\cite{rueckauer2017conversion, han2020rmp}, and dynamic threshold adjustment~\cite{stockl2021optimized, ho2021tcl, DBLP:journals/tnn/WuCZLLT23}. Efficient conversion with fewer spikes was achieved through rate-coding and time-coding methods~\cite{DBLP:conf/nips/KimKK20a}, as well as specialized weight renormalization~\cite{Sengupta2018Going}.

% A recent direction involves modifying the ANN activation functions to reduce conversion errors. Methods using thresholded ReLU activation~\cite{ding2021optimal} and quantized activation functions~\cite{bu2022optimal, liu2022spikeconverter, hu2023fast, shen2024conventional} have achieved high accuracy at lower latencies. However, these approaches often reduce the original ANN accuracy, limiting the potential performance of the converted SNN. Techniques like~\cite{DBLP:conf/ijcai/Li022, wang2022signed, liu2022spikeconverter} propose modifications to the inner function of IF neurons to reduce conversion errors. Notably, a two-phase spiking neuron mechanism similar to ours has been used in \cite{liu2022spikeconverter}.


% Membrane potential and threshold initialization play crucial roles in reducing conversion errors. Many methods utilize layer-wise maximum ANN activations, or some percentile of them, for threshold initialization~\cite{rueckauer2016theory, DBLP:conf/iclr/DengG21, li2021free}. Detailed studies on membrane potential initialization and threshold settings are provided in~\cite{DBLP:conf/aaai/HaoBD0Y23, bojkovic2024data}. Post-conversion weight calibration~\cite{li2021free, bojkovic2024data} further enhances SNN performance, leading to hybrid training methods that combine ANN-SNN conversion with fine-tuning.


% In general, one can argue that ANN-SNN conversion based methods of training SNNs can be classified in two categories. The first line of thought deals with modification on the ANN side, most notably in quantization of the ANN activation functions, in order to reduce the conversion error in low latency. The second line deals with modification on ANN side, where the spiking neuron mechanisms are modified in order to reduce this error. The advantage in the former case comes from the lower latency to have a good performance, but the disadvantage comes from the fact that quantization of the ANN activations in general, yields the poorer ANN performance, hence limits the SNN performance as well. In the latter case, the situation is reversed, the ANNs utilized have higher performance, but SNNs sometimes need longer latency to achieve it. Our approach belongs to the second category.


% \textbf{Direct training} allows SNNs to exploit precise spike timing and operate within a few timesteps. The success of direct training hinges on the development of spatio-temporal backpropagation through time (BPTT) and surrogate gradient methods~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio, bellec2018long, fang2021deep, fang2021incorporating, zenke2021remarkable, mukhoty2024direct}. However, these methods encounter challenges with deep architectures due to gradient instability and high computational costs during training simulations. Various gradient-based methods leverage surrogate gradients~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio, bellec2018long, fang2021deep, fang2021incorporating, zenke2021remarkable, mukhoty2024direct} to address the non-differentiable nature of spike functions. Direct training focuses on optimizing not only synaptic weights but also dynamic parameters like firing thresholds~\cite{wei2023temporal} and leaky factors~\cite{DBLP:journals/tnn/RathiR23}. Novel loss functions such as rate-based counting loss~\cite{zhu2024exploring} and distribution-based loss~\cite{guo2022recdis} were proposed to provide sufficient positive gradients and rectify the distribution of membrane potential during the propagation of binary spikes. Furthermore, hybrid training methods~\cite{DBLP:journals/corr/abs-2205-07473} combine ANN-SNN conversion with BPTT to achieve higher performance with low latency. Recent advancements include Ternary Spike~\cite{DBLP:conf/aaai/GuoCLPZHM24} for enhanced information capacity and the reversible SNN~\cite{zhang2023memory} to reduce memory costs during training.


\textbf{ANN-SNN Conversion}. This methodology aligns ANNs and SNNs through activation-firing rate correspondence, as initially demonstrated in~\cite{rueckauer2016theory, cao2015spiking}. Subsequent research has systematically improved conversion fidelity through four principal approaches: (i) weight normalization~\cite{diehl2015fast}, (ii) soft-reset mechanisms~\cite{rueckauer2017conversion, han2020rmp}, (iii) adaptive threshold configurations~\cite{stockl2021optimized, ho2021tcl, DBLP:journals/tnn/WuCZLLT23}, and (iv) spike coding optimization~\cite{DBLP:conf/nips/KimKK20a, Sengupta2018Going}. Recent innovations focus on ANN activation function adaptations, including thresholded ReLU~\cite{ding2021optimal} and quantization strategies~\cite{bu2022optimal, liu2022spikeconverter, hu2023fast, shen2024conventional}. However, these approaches introduce inherent accuracy-compression tradeoffs. Parallel efforts modify integrate-and-fire neuron dynamics~\cite{DBLP:conf/ijcai/Li022, wang2022signed, liu2022spikeconverter}, with~\cite{liu2022spikeconverter} proposing a dual-phase mechanism resembling our approach.

Crucial for achieving high conversion efficacy, threshold initialization methodologies employ layer-wise activation maxima or percentile statistics~\cite{rueckauer2016theory, DBLP:conf/iclr/DengG21, li2021free, wu2024ftbc}, augmented with post-conversion weight calibration~\cite{li2021free, bojkovic2024data}. Contemporary strategies can be categorized into two paradigms: (1) ANN activation quantization for temporal efficiency at the cost of accuracy, and (2) SNN neuron modification preserving ANN expressiveness with extended temporal requirements. Our methodology adheres to the second paradigm.

\textbf{Direct Training}. This approach leverages spatio-temporal spike patterns through backpropagation-through-time with differentiable gradient approximations~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio, bellec2018long, fang2021deep, fang2021incorporating, zenke2021remarkable, mukhoty2024direct}. Advancements encompass joint optimization of synaptic weights and neuronal parameters (threshold dynamics~\cite{wei2023temporal}, leakage factors~\cite{DBLP:journals/tnn/RathiR23}), novel loss formulations for spike distribution regularization~\cite{zhu2024exploring, guo2022recdis}, and hybrid conversion-training pipelines~\cite{DBLP:journals/corr/abs-2205-07473}. State-of-the-art developments introduce ternary spike representations for enhanced information density~\cite{DBLP:conf/aaai/GuoCLPZHM24} and reversible architectures for memory-efficient training~\cite{zhang2023memory}.


\section{Methodology} 
\label{sec:method}

\subsection{Motivation}

In ANN-SNN conversion methodologies, constant and rate coding are commonly utilized in the resulting spiking neural network models, based on the principle that the expected input at each time step matches the original input to the ANN model. Notably, the encoding lacks temporal information, as spike timing does not convey additional information. In constant encoding, this is evident, while in rate encoding, for a fixed input channel, the spike probability remains constant across all time steps, with the channel value assumed to lie between 0 and 1.

The resulting SNN model is initialized to approximate the outputs of the original ANN model based on the principle that, for each spiking neuron, the expected number of spikes it generates should closely match the output of the corresponding ANN neuron. In particular, it is assumed that no temporal information is present throughout the SNN model; that is, the spike train outputs of each SNN layer should convey no additional temporal information beyond spike firing rates.

Previous studies examining conversion errors and their classifications \cite{li2022bsnn, bu2022optimal, ding2021optimal, bojkovic2024data} suggest that SNNs obtained through ANN-SNN conversion may generate spikes that are suboptimally positioned in the temporal domain, leading to a degradation in model performance, particularly at low latencies. Our initial experiments (see Introduction and Figure~\ref{fig-init-exps}) further validate this observation while also revealing a novel insight: random temporal displacements of spike trains after spiking layers significantly enhance model performance. This phenomenon, which we refer to as temporal misalignment -- wherein the original spike trains exhibit temporal misalignment, thereby impairing model performance -- serves as the foundation and motivation for our proposed method, which is elaborated upon in the next section. Additional experiments on permutations in the ANN-SNN context, along with an explanation of their impact on model performance, are provided in Appendix \ref{app permutations}.

% \subsection{Permuting spike trains}
% \veb{This and next section should be better and made shorter, because theorems will take a lot of space.}

% To test the initial hypothesis of the absence of ``temporal information'', we designed an experiment where for an SNN model obtained through the ANN-SNN conversion, after each layer we would collect the output spike trains, and permute them through the temporal dimension. More precisely, for a fixed latency $T$ and for each spiking layer, we would collect the output spike trains of temporal length $T$, permute them, and pass them to the next layer, and continue this process until the output layer. We used the constant encoding for the input. We further compared the performance of this model with the original base SNN model, whose output spike trains have not been manipulated through permutations. 

% The performance of the base and ``permuted'' SNN models has been compared in two ways. First, for the latency $T$ and for the latencies $t<T$. What we discovered is that if we consider the latency $T_{top}$ where the base model achieves the top accuracy, the performance of the two models is pretty much the same. However, if we consider the latency $T<T_{top}$, the ``permuted'' model outperforms the base model, in some cases drastically. Moreover, the situation becomes more contrasted if we consider the latencies $t<T$. The reader can refer to the Figure \ref{fig-init-exps} for more information, while the details of the experiment are in the Appendix.

% The conclusion of these initial experiments is that, contrary to the expectation, ANN-SNN conversion is not invariant under the temporal manipulation of output spike trains. Moreover, the effect of permuting the spike trains yields better performance of the converted SNN model, a phenomenon to which we refer as \textbf{temporal misalignment} in ANN-SNN conversion. 


\subsection{From permutations to Bursting Probabilistic Spiking Neurons}\label{sec latency}

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{figures/permute-tpp-tpd.pdf}
%   \caption{ (a) The ``permutation'' layer collects the spike outputs of the layer in the first \textbf{A}ccumulation phase, while in the second \textbf{S}piking phase it outputs the same number of spikes as the original spike train, but with permuted firing times. (b) Bursting probabilistic spiking neurons accumulate the weighted outputs from the previous layer and then output them according to their inner dynamics.}
%   \label{fig permutation}
% \end{figure}

This work aims to address the following question: How to incorporate the action of permutation of the output spike trains into the dynamics of the spiking neurons? We approach this problem in two steps.

% Suppose that we want to permute the spikes trains coming from the layer $\ell$. A general idea would be to have a ``permutator''- a layer immediately after, whose role would be to collect all the spikes, and outputs them in a permu--ted fashion  (c.f. Figure \ref{fig:permutation}). This immediately suggests the \textbf{two-phase} nature of the ``permutator'', namely, in the first phase the incoming spikes are accumulated and the firing is delayed until the beginning of the second, firing phase. 

Consider the scenario where spike trains from layer $\ell$ are to be permuted. A general approach involves introducing a ``permutator'' -- a subsequent layer tasked with collecting all incoming spikes and re-emitting them in a permuted manner, as illustrated in Figure \ref{fig:permutation}. This inherently implies the \textbf{two-phase} nature of the ``permutator'': specifically, during the first phase, incoming spikes are accumulated, and firing is deferred until the onset of the second phase, where spikes are emitted.

The second step focuses on the output mechanism of the ``permutator''. Specifically, it is desirable to design a spiking neuron mechanism that retains the stochastic component of the permutations. This consideration motivates the adoption of probabilistic firing in spiking neurons.

% The final question that we consider is, can we make the situation more compact, by using probabilistic spiking neuron which would collect the weighted input of the previous layer (rather than the spikes of the spiking layer)? 

The final question we explore is whether a more compact approach can be achieved by employing probabilistic spiking neurons that aggregate weighted inputs from the previous layer, rather than directly processing spikes from a spiking layer.

\textbf{TPP neurons}: To address the aforementioned questions, we propose \textbf{two-phase probabilistic spiking neurons} (TPP) (see Figure \ref{fig:prob-mode}). Specifically, in the first phase, the neurons will only accumulate the (weighted) input coming from the previous layer, while in the second phase, the neurons will spike. More precisely, suppose that at a particular layer $\ell$ the spiking neurons accumulate the whole output of the previous layer, without emitting spikes. Denote the accumulated membrane potential by $\bv^\lup[0]$. The subsequent spiking phase is governed by the following equations:
\begin{equation}\label{eq probabilistic spiking}
\begin{aligned}
\bs^\lup[t] &= B\left(\frac{1}{\theta^\lup\cdot (T-t+1)}\cdot\bv^\lup[t-1]\right),\\
\bv^\lup[t] &= \bv^\lup[t-1] -\theta^\lup \cdot\bs^\lup[t],
\end{aligned}
\end{equation}
where $t=1,\dots, T$. Here, $B(x)$ is a Bernoulli random variable with bias $x$, extended for $x\in \mathbb{R}$ in a natural way ($B(x)= B(\max(\min(x,1),0))$). If the weights of the SNN network are not normalized, the produced spikes will be scaled with the thresholds $\theta^\lup\cdot \bs^\lup[t]$, before being sent to the next layer.

We observe that the presence of $T-t+1$ in the denominator of the bias in $B$ demonstrates that the probability of spiking depends not only on the current membrane potential, but also on the temporal step: in the absence of spiking, for the same membrane potential, the probability of spiking increases through time. Figure \ref{fig:prob-mode} provides a visual representation of the functioning of TPP neurons.

% \begin{figure}{ht}
% \centering
% \includegraphics[width=0.5\linewidth]{figures/explain-theory.pdf}
% \caption{\wxf{the figure is not informative. so revise.} \small (a) ReLU activation with inputs of $\frac{3}{4}$ and $\frac{1}{4}$, and corresponding weights of $+1$ and $-1$. After summing, the ground truth output is $\frac{1}{2}$; (b) Baseline case: input spike trains without permutation yields an ANN-SNN conversion error $\frac{1}{4}$ due to delayed spike at $t=4$ (orange spike); (c) Spike trains with permutation applied to move delayed spikes at $t=4$ forward to $t=2$. This adjustment heuristically aligns the output with the original ANN output $\frac{1}{2}$.}
% \label{fig:explain-theory}
% \end{figure}


% \textbf{Total output} Although the proposed spiking activity is probabilistic, the total output of the spiking neuron (the number of spikes) expresses little variability, which is seen in the following. 

% \veb{Here we will have a big theorem, I will add it next just to see how to manage space. We should also provide explanation (from the Rebuttal) of why the permutations increase accuracy.}

% \begin{restatable}{theorem}{probabilisticmain}\label{thm probabilistic main}
%     Suppose that for some $0<t<T$, we have $t\cdot\theta^\lup\leq\bv^\lup[0]<(t+1)\cdot \theta^\lup$, and we are in the setting of \eqref{eq probabilistic spiking}. Then, the probability that the neuron will spike more than $t+1$ times, or less than $t$ times is zero. Moreover, the probability of having a spike at any given time step $t=1,\dots,T$ is non-zero.
% \end{restatable}

The following theorem provides a comprehensive characterization of the functioning of TPP neurons and their applications in ANN-SNN conversion when approximating ANN outputs (here $\relu_\theta(x)=\min(\relu(x),\theta)$). 

\begin{restatable}{theorem}{mainthm}\label{thm main}
    Let $X^{(l)}$ be the input of the ANN layer with \relu activation and suppose that, during the accumulation phase, the corresponding SNN layer of TPP neurons accumulated $T\cdot X^{(l)}$ quantity of voltage. \\
    % \begin{itemize}
        (a) For every time step $t=1,\dots,T$, we have         
        \begin{equation}
            \frac{(T-t+1)\cdot\theta^\lup}{T}\cdot \mathbb{E}\left[\sum_{i=1}^t s^{(l)}[i]\right] = \relu_{\theta^\lup}(X^{(l)}).
        \end{equation}        
        (b) Suppose that for some $t=1,\dots,T$, the TPP layer produced $s^{(l)}[1],\dots,s^{(l)}[t-1]$ vector spike trains for the first $t-1$ steps, and the residue voltage for neuron $i$ is higher than zero. Then,
        \begin{equation}
            \frac{(T-t+1)\cdot\theta^\lup}{T}\cdot\mathbb{E}\left[s_i^{(l)}[t]\right]+\frac{\theta^\lup}{T}\cdot\sum_{i=1}^{t-1} s_i^{(l)}[i] = \relu_{\theta^\lup}(X_i^{(l)}).
        \end{equation}        
        (c) If $s^{(l)}[1],\dots,s^{(l)}[T]$ are the output vectors of spike trains of the TPP neurons during $T$ time steps, then        
        \begin{equation}
            \frac{\theta^\lup}{T}\cdot\sum_{i=1}^{T} s^{(l)}_j[i]=\relu_{\theta^\lup}(X^{(l)}_j),
        \end{equation}
        if $\relu_{\theta^\lup}(X^{(l)}_j)$ is a multiple of $\frac{\theta^\lup}{T}$, or        
        \vspace{-5pt}
        \begin{align}
            \frac{\theta^\lup}{T}\cdot\sum_{i=1}^{T} s^{(l)}_j[i]&=\frac{\theta^\lup}{T}\cdot \lfloor\frac{T}{\theta^\lup}\relu_{\theta^\lup}(X^{(l)}_j)\rfloor\label{eq out 1}\\
            &\text{ or } \frac{\theta^\lup}{T}\cdot \lfloor\frac{T}{\theta^\lup}\relu_{\theta^\lup}(X^{(l)}_j)\rfloor+\frac{\theta^\lup}{T},\label{eq out 2}
        \end{align}        
        if $\relu_{\theta^\lup}(X^{(l)}_j)$ is not a multiple of $\frac{\theta^\lup}{T}$.
        % \begin{equation}
        %     \frac{\theta}{T}\sum_{i=1}^{t-1} s^{(l)}_j[i]=\begin{cases}
        %         \relu_\theta(X^{(l)}_j),\quad\text{if $\relu_\theta(X^{(l)}_j)$ is a multiple of $\frac{\theta}{T}$}, \\
        %         \frac{\theta}{T}\cdot \lfloor\frac{T}{\theta}\relu_\theta(X^{(l)}_j)\rfloor \text{ or } \frac{\theta}{T}\cdot \lfloor\frac{T}{\theta}\relu_\theta(X^{(l)}_j)\rfloor+\frac{\theta}{T}, \quad \text{otherwise}. 
        %     \end{cases}
        % \end{equation}
        
        $(d)$ Suppose that $\max X^{(l)}\leq \theta$ and that the same weights $W^{(l+1)}$ act on the outputs of layer $(l)$ of ANN and SNN as above, and let $X^{(l+1)}$ (resp. $T\cdot \tilde{X}^{(l+1)}$) be the inputs to the $(l+1)$th ANN layer (resp. the accumulated voltage for the $(l+1)$th SNN layer of TPP neurons), Then 
        \begin{equation}\label{eq final approximation}
            ||X^{(l+1)}-\tilde{X}^{(l+1)}||_{\infty} \leq ||W^{(l+1)}||_\infty\cdot \frac{\theta^\lup}{T}.
        \end{equation} 
        \vspace{-5pt}
    % \end{itemize}
\end{restatable}
\textbf{Remarks:} The proof of the previous result is presented in the Appendix. Here, we offer an interpretation of its statements.

    $\bullet{}$ We contrast the statement $(a)$ with Theorem 2 of \cite{bu2022optimal}. Specifically, the authors demonstrate that if the membrane potential is initialized at half of the threshold, the expectation of the conversion error (layerwise) is 0. However, this result in \cite{bu2022optimal} relies on the underlying assumption that the layerwise distribution of ANN activation values is \textbf{uniform}, which does not generally hold in practice (see, for example, \cite{bojkovic2024data}). Our result $(a)$ above shows that \textbf{after every $t\leq T$ time steps}, our expected spiking rate aligns well with the clipping of the $\relu$ activation by the threshold, as it should, without imposing any prior assumptions on the distribution of the ANN activation values.
    
    $\bullet{}$ Result $(b)$ demonstrates that the activity of a TPP neuron \textbf{adapts to the output} it already produced. In particular, as long as the neuron is still active and contains residual membrane potential, the expectation of its output at the next time step takes into account the previously produced spikes and will yield the ANN counterpart.
    
    $\bullet{}$ The results $(c)$ and $(d)$ show that during the accumulation phase, the TPP neuron closely approximate ANN neurons with \relu activation. In particular, the only remaining source of errors in layerwise approximation is the clipping error due to the threshold $\theta$, and the quantization error due to the discrete outputs of the spiking neurons. We also note in equations \eqref{eq out 1} and \eqref{eq out 2} two possibilities for the output, which come from the probabilistic nature of spiking.

% The proof is given in the Appendix, but we may note that the result states that TPP neurons output the exact number of spikes as they should, and those spikes can have arbitrary positioning throughout the time steps. In other words, they act somewhat as a ``permutation'' on the output spike trains.

% \textbf{Heuristics behind permutations} We come back to the original motivation, and the mysterious effect of temporal misalignment. To this end, we notice that permutations may act as a ``uniformizer'' of the inputs to the spiking neuron, which is highly related to notions of phase lag or unevenness of the inputs (see \cite{li2022bsnn} and \cite{bu2022optimal}, respectively). 
% \begin{restatable}{theorem}{exppermutations}\label{thm permutations expected} Suppose we have $N$ spiking neurons that produced spike trains $s_i[1], s_i[2],\dots, s_i[T]$, $i=1,\dots,N$. Furthermore, suppose that these spike trains are modulated with weights $w_1,\dots,w_N$, and as such give input to a neuron (say from the following layer) in the form $x[t]=\sum w_i s_i[t]$, for $t=1,\dots, T$. For a given permutation $\pi = (\pi_1,\dots,\pi_N)$, 
% let $\pi s_i$ denote the permutation of the spike train $s_i$. Then, for every $t_1,t_2\in \{1,2,\dots, T\}$, 
% $$
% E_{\pi}[\sum w_i\pi s_i[t_1]]=E_{\pi}[\sum w_i\pi s_i[t_2]].
% $$
% \end{restatable}
% The previous result deals with the expected outputs with respect to the permutations. When it comes to the action of a single permutation, we make the following observation. The effect of a single permutation is mostly visible on spike trains that have a \textbf{low number of spikes}. This, in turn, is related to the situation where the input to the neuron is low throughout time, and it takes longer for a neuron to accumulate enough potential in order to spike, hence the neuron spikes at a later time during latency. In this case, a single permutation of the output spike(s) actually move the spikes forward in time (in general) and as such contributes to the elimination of the unevenness error, which appears when the input to a neuron in the beginning is higher than the average input through time (hence, the neuron produces superfluous spikes in the beginning, which shouldn't be the case), see Figure \ref{fig:explain-theory}.

% \begin{theorem}
%     Suppose that for some $0<t<T$, we have $t\cdot\theta^\lup\leq\bv^\lup[T]<(t+1)\cdot \theta^\lup$, and we are in the setting of \eqref{eq deterministic spiking}. Then, $\sum_{t'=1}^T\bs^\lup[T+t']=t$.
% \end{theorem}
% The previous result, which we prove in the Appendix in more generality, states that the outputs of the TPD neuron and the \relu function given the same input $\bv[T]$ will differ by at most $\theta$. 
% \begin{theorem} The expected output of the two-phase neurons is given by:\\

% In the probabilistic regime, cf. \eqref{eq probabilistic spiking}
% $$
% \expE{\big[\sum_{t=1}^T\bs^\lup[T+t]\big]} = \big\lfloor \frac{T}{\theta}\bv^\lup[T]\big\rfloor\cdot \frac{\theta}{T}.
% $$
%  In the deterministic regime, cf. \eqref{eq deterministic spiking}
% $$
% \expE{\big[\sum_{t=1}^T\bs^\lup[T+t]\big]} = \big\lfloor \frac{1}{\theta}\bv^\lup[T]\big\rfloor\cdot T.
% $$
% \end{theorem}

\subsection{Bio-plausibility and hardware implementation of TPP neurons}\label{sec bio inspired}
Our proposed neurons have two distinct properties: The two-phase regime and probabilistic spike firing. Both properties are biologically plausible and extensively studied in the neuroscience literature. For example, the two phase regime is related to firing after a delay of biological spiking neurons, where a neuron collects the input beyond the threshold value and fires after delay or after some condition is met. It could also be related to the bursting, when a biological neuron starts emitting bursts of spikes, after a certain condition is met, effectively dumping their accumulated potential. See \cite{izhikevich2007dynamical, connors1990intrinsic,llinas1982electrophysiology,krahe2004burst} for more details. 

On the other side, stochastic firing of biological neurons has been well studied as well, and different aspects of noise introduction into firing have been proposed. Refer to \cite{shadlen1994noise,faisal2008noise,softky1993highly,maass1997networks,pagliarini2019probabilistic,stein2005neuronal} for some examples.

Regarding the implementation of TPP neurons on neuromorphic hardware, two phase regime can be easily achieved on various modern neuromorphic that support programmable spiking neurons. Stochastic firing can be achieved through random sampling which, for example, is supported by IBM TrueNorth \cite{merolla2014million}, Intel Loihi \cite{davies2018loihi}, BrainScaleS-2 \cite{pehle2022brainscales}, SpiNNaker \cite{furber2014spinnaker} neuromorphic chips. For example, TrueNorth incorporates stochastic neuron models using on-chip pseudo-random number generators, enabling probabilistic firing patterns that mirror our approach. Similarly, Loihi~\cite{gonzalez2024spinnaker2} supports stochastic operations by adding uniformly distributed pseudorandom noise to neuronal variables, facilitating the implementation of probabilistic spiking neurons. 

% The probabilistic spiking mechanism we introduce aligns with the stochastic firing behaviors observed in biological neurons, a feature that has been effectively implemented in neuromorphic hardware such as IBM’s TrueNorth~\cite{debole2019truenorth, merolla2014million}, Intel’s Loihi~\cite{loihi2, davies2018loihi}, BrainScaleS-2~\cite{pehle2022brainscales}, SpiNNaker and SpiNNaker2. For example, TrueNorth incorporates stochastic neuron models using on-chip pseudo-random number generators, enabling probabilistic firing patterns that mirror our approach. Similarly, Loihi~\cite{gonzalez2024spinnaker2} supports stochastic operations by adding uniformly distributed pseudorandom noise to neuronal variables, facilitating the implementation of probabilistic spiking neurons. 


% \paragraph{Latency }
% Since at every layer we are collecting the complete (weighted) output spike train load from the previous layer for $T$ time steps, and use the same $T$ time steps to produce the new output, the latency for classifying one sample increases with the number of layers $L$, and in total will be $\sim L\cdot (T+c)$, where $c$ is the ``time'' needed to sum up a spike train of length $T$ (in the worst case $c=T$, but on a general purpose hardware, this can be done in $log(T)$ steps). However, once a particular layer of neurons has finished emanating the spikes to the next, it can immediately start collecting the output from the previous layer that are produced by introducing the next sample: The process of classifying a dataset can be serialized, where a new sample is introduced to the model after $(T+c)$ time steps. In particular, we can consider the average latency to classify a given dataset, or the throughput latency, which for larger datasets will be $\sim (T+c)$. In the tables that follow in the experimental section, this is the latency we report. 

To reduce the overall latency for processing inputs with our models, which yields linear dependence on the number of layers (implied by the two phase regime), we note that as soon as a particular layer has finished the firing phase, it can start receiving the input from the previous one: The process of classifying a dataset can be serialized. This has already been observed, for example in \cite{liu2022spikeconverter}. Neuromophic hardware implementation of this serialization has been proposed as well, see for example \cite{das2023design, song2021design, varshika2022design}.

% Our proposed method focuses on optimizing the overall average timesteps required for inference across the entire dataset, similar to SEENN~\cite{DBLP:conf/nips/LiGKP23} \veb{I think you wanted to add the Li reference from above, not this one} \wxf{The paper that I am going to cite here is SEENN. Solved here.}, aiming to improve the accuracy without introducing additional latency. A potential concern is that accumulating membrane potentials over multiple timesteps might introduce delays, potentially impacting the real-time performance of SNNs.

% However, neuromorphic hardware can efficiently handle such accumulation through pipelining~\cite{das2023design, song2021design, varshika2022design}. Pipelining allows for the overlapping of different computational stages. By dividing the neuron computation into multiple pipeline stages, each stage can process different data simultaneously, enabling simultaneous processing of membrane potential accumulation for different data and enhancing throughput and reducing overall latency.

% \subsection{Reducing the latency through first-order synaptic models}
% When converting very deep models, such as RegNetX or EfficientNet, we need high latency in order the propagate precise ANN information through the converted SNN, and to deal with this problem, we propose first order models for weights and thresholds. We briefly explain the situation for \relu activation, while the more general approach is presented in the Appendix. The models we propose take the form of equations $\tau_W \frac{d W(t)}{dt}=-W(t)$ and $\tau_\theta \frac{d \theta(t)}{dt} = -\theta(t)$, which, after discretization, become
% \begin{equation}\label{eq weights thresholds rec}
% W[t]= \left(1-\frac{1}{\tau_W}\right)W[t-1],\quad \theta[t]= \left(1-\frac{1}{\tau_\theta}\right)\theta[t-1].
% \end{equation}
% Setting up properly the constants $\tau_W$ and $\tau_\theta$, we can obtain recursion $W[t]= \frac{1}{2}W[t-1]$, and similarly for $\theta[t]$. 

% \begin{restatable}{theorem}{weightsmain}\label{thm weight decay}
%     Let the accumulated membrane potential after $T$ time steps of a TPD neuron be $\bv[T]>0$, and let the threshold be initialized with $\theta:=\theta[T]>\bv[T]$. Assume further that the sequence $\theta[t]$ satisfies the recursion $\theta[t+1]=\frac{1}{2}\theta[t]$. Then, 
%     \begin{equation}
%         |\bv[T]-\sum_{t=1}^T\theta[T+t]\cdot \bs[T+t]\big|\leq \frac{1}{2^T}\theta.
%     \end{equation}
% \end{restatable}
% The previous result then suggests how to practically implement the weights and thresholds in \eqref{eq weights thresholds rec}. Namely, the thresholds will be initialized with a value $\theta^\lup=\theta^\lup[T]$ which is larger than the $\bv[T]$. A good choice is the maximum activation value $\max a^\lup$ of the corresponding activation layer in ANN. Then, the weights $W^{(l+1)}=W^{(l+1)}[T]$ will be initialized with $\theta^\lup\cdot W^{(l+1)}$, where $W^{(l+1)}$ are the corresponding ANN weights. 

% More details and more general setting is discussed in the appendix.

\section{Experiments} % page 5 or page 

In this section, we verify the effectiveness and efficiency of our proposed methods. We compare it with state-of-the-art methods for image classification via converting ResNet-20, ResNet-34~\cite{he2016residual}, VGG-16~\cite{simonyan2015deep}, RegNet~\cite{DBLP:conf/cvpr/RadosavovicKGHD20} on CIFAR-10~\cite{lecun1998gradient, krizhevsky2010cifar}, CIFAR-100~\cite{krizhevsky2009learning}, CIFAR10-DVS~\cite{li2017cifar10} and ImageNet~\cite{deng2009imagenet}. Our experiments use PyTorch~\cite{paszke2019pytorch}, PyTorch vision models~\cite{torchvision2016}, and the PyTorch Image Models (Timm) library~\cite{rw2019timm}.
%\footnote{\url{https://github.com/huggingface/pytorch-image-models}} 


To demonstrate the wide applicability of the TPP neurons and the framework we propose, we combine them with three representative methods of ANN-SNN conversion from recent literature, each of which has their own particularities. These methods are: QCFS~\cite{DBLP:conf/iclr/BuFDDY022}, RTS~\cite{DBLP:conf/iclr/DengG21}, and SNNC~\cite{li2021free}. The particularity of QCFS method is that it uses step function instead of \relu in ANN models during their training, in order to obtain higher accuracy in lower latency after the conversion. RTS method uses thresholded \relu activation in ANN models during their training, so that the outliers are eliminated among the activation values, which helps to reduce the conversion error. Finally, SNNC uses standard ANN models with \relu activation, and performs grid search on the activation values to find optimal initialization of the thresholds in the converted SNNs. 

We initialize our SNNs following the standard ANN-SNN conversion process described in Section~\ref{sec:method} (and detailed in \ref{app conversion steps}), starting with a pre-trained model given by the baseline, or with training an ANN model using default settings in QCFS~\cite{DBLP:conf/iclr/BuFDDY022}, RTS~\cite{DBLP:conf/iclr/DengG21}, and SNNC~\cite{li2021free}. ANN ReLU activations were replaced with layers of TPP neurons initialized properly. All experiments were conducted using NVIDIA RTX 4090 and Tesla A100 GPUs. For comprehensive details on all setups and configurations, see Appendix~\ref{appendix:config}.












\subsection{Comparison with the State-of-the-art ANN-SNN Conversion methods}

We evaluate our approach against previous state-of-the-art ANN-SNN conversion methods, including ReLU-Threshold-Shift (RTS)~\cite{DBLP:conf/iclr/DengG21}, SNN Calibration (SNNC-AP)~\cite{li2021free}, Quantization Clip-Floor-Shift activation function (QCFS)~\cite{DBLP:conf/iclr/BuFDDY022}, SNM~\cite{wang2022signed}, Burst~\cite{DBLP:conf/ijcai/Li022}, OPI~\cite{bu2022optimized}, SRP~\cite{DBLP:conf/aaai/HaoBD0Y23}, DDI~\cite{bojkovic2024data} and FTBC~\cite{wu2024ftbc}.

% \begin{table*}[htbp]
% \caption{Comparison between our method and the other ANN-SNN conversion methods on ImageNet. We provide the average accuracy and the associated standard deviation across 5 experiments (for our methods, we need extra $c$ steps for summation, see Section \ref{sec latency}).}
% \label{tab:ann-snn-imagenet}
% \centering
% \scalebox{0.62}{
% \begin{threeparttable}
% \begin{tabular}{@{}ccccccccc@{}}
% \toprule
% Arch. & Method & ANN & T=4  & T=8 & T=16 & T=32 & T=64 & T=128 \\
% \toprule
% \multirow{8}{*}{ResNet-34}
% & RTS~\cite{DBLP:conf/iclr/DengG21} & 75.66 & -- & -- & -- & 33.01 & 59.52 & 67.54 \\
% & SNNC-AP\tnote{*}~\cite{li2021free} & 75.66 & -- & -- & -- & 64.54 & 71.12 & 73.45 \\
% & QCFS~\cite{DBLP:conf/iclr/BuFDDY022} & 74.32 & -- & -- & 59.35 & 69.37 & 72.35 & 73.15 \\
% & SRP~\cite{DBLP:conf/aaai/HaoBD0Y23} & 74.32 & \gc{66.71} & \gc{67.62} & 68.02 & 68.40 & 68.61 & --\\
% % & MiCE~\cite{DBLP:journals/esticas/HoC23} & 74.83 & -- & 45.24 & 65.20 & 69.98 & 72.56 \\
% & FTBC(+QCFS)~\cite{wu2024ftbc} & 74.32 & 49.94 & 65.28 & 71.66 & 73.57 & 74.07 & 74.23 \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + QCFS} & 74.32 & 0.09 & 8.85 & 57.10 & 70.75 & 73.04 & 73.33 \\
% & \textbf{Ours (TPP) + QCFS} & 74.32 & {37.23 (0.07)} & {67.32 (0.06)} & \gc{72.03 (0.02)} & 72.97 (0.03) & 73.24 (0.02) & 73.30 (0.02) \\
% \cmidrule{2-9}
% & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 75.65 & 2.69 (0.03) & 49.24 (0.23) & 69.97 (0.10) & \gc{74.07 (0.06)} & \gc{75.23 (0.03)} & \gc{75.51 (0.05)} \\
% \midrule
% \multirow{12}{*}{VGG-16}
% & SNNC-AP\tnote{*}~\cite{li2021free} & 75.36 & -- & -- & -- & 63.64 & 70.69 & 73.32 \\
% & SNM\tnote{*}~\cite{wang2022signed} & 73.18 & -- & -- & -- & 64.78 & 71.50 & 72.86 \\
% & RTS~\cite{DBLP:conf/iclr/DengG21} & 72.16 & -- & -- & 55.80 & 67.73 & 70.97 & 71.89 \\
% & QCFS~\cite{DBLP:conf/iclr/BuFDDY022} & 74.29 & -- & -- & 50.97 & 68.47 & 72.85 & 73.97 \\
% & Burst~\cite{DBLP:conf/ijcai/Li022} & 74.27 & -- & -- & -- & 70.61 & 73.32 & 73.00 \\
% & OPI\tnote{*}~\cite{bu2022optimized} & 74.85 & -- & 6.25 & 36.02 & 64.70 & 72.47 & 74.24 \\
% & SRP~\cite{DBLP:conf/aaai/HaoBD0Y23} & 74.29 & 66.47 & 68.37 & 69.13 & 69.35 & 69.43 & --\\
% % & MiCE~\cite{DBLP:journals/esticas/HoC23} & 74.56 & -- & 48.00 & 67.28 & 71.12 & 73.42 \\
% & FTBC(+QCFS)~\cite{wu2024ftbc} & 73.91 & 58.83 & 69.31 & 72.98 & 74.05 & 74.16 & 74.21 \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + RTS} & 72.16 & 41.65 & 60.64 & 68.62 & 71.27 & 72.01 & 72.10 \\
% & \textbf{Ours (TPP) + RTS} & 72.16 & 30.50 (1.19) & 56.69(0.67) & 67.34 (0.25) & 70.63 (0.11) & 71.75 (0.05) & 72.05 (0.03) \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + QCFS} & 74.22 & 0.16 & 42.63 & 71.2 & 74.09 & 74.46 & 74.44 \\
% & \textbf{Ours (TPP) + QCFS} & 74.22 & \gc{68.39 (0.08)} & \gc{72.99 (0.05)} & \gc{73.98 (0.07)} & 74.23 (0.03) & 74.29 (0.00) & 74.33 (0.01) \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD)\tnote{*} + SNNC w/o Cali.} & 75.37 & 16.4 & 57.52 & 70.95 & 74.06 & 75.01 & 75.31 \\
% & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 75.37 & 54.14 (0.59) & 69.75 (0.27) & 73.44 (0.02) & \gc{74.72 (0.06)} & \gc{75.14 (0.02)} & \gc{75.25 (0.03)} \\
% \toprule
% \multirow{3}{*}{RegNetX-4GF}
% & RTS~\cite{DBLP:conf/iclr/DengG21} & 80.02 & -- & -- & -- & 0.218 & 3.542 & 48.60 \\
% & SNNC-AP\tnote{*}~\cite{li2021free} & 80.02 & -- & -- & -- & 55.70 & 70.96 & 75.78 \\ 
% \cmidrule{2-9}
% % & \textbf{Ours (TPD)\tnote{*} + SNNC w/o Cali.} & 78.45 & -- & -- & 0.11 & 0.58 & 30.24 & 72.11 \\
% & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 78.45 & -- & -- & \gc{22.71 (2.98)} & \gc{66.51 (0.44)} & \gc{75.54 (0.07)} & \gc{77.83 (0.04)} \\
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
% \item[*] Without modification to ReLU of ANNs.
% \end{tablenotes}
% \end{threeparttable}
% }
% \vspace{-40pt}
% \end{table*}

\noindent \textbf{ImageNet dataset:} Table~\ref{tab:ann-snn-imagenet-cifar} compares the performance of our proposed methods with state-of-the-art ANN-SNN conversion methods on ImageNet. Our method outperforms the baselines across all simulation time steps for VGG-16, and RegNetX-4GF. For instance, on VGG-16 at $T=32$, our method achieves 74.72\% accuracy, surpassing other baselines even at $T=128$. Moreover, at $T=128$, our method nearly matches the original ANN performance with only a 0.12\% drop in VGG-16 and a 0.14\% drop in ResNet-34.

We see similar patterns in combining our methods with RTS and QCFS baselines, which use modified \relu activations to reduce conversion errors. Table \ref{tab:ann-snn-imagenet-cifar} shows these results. For instance, applying TPP with QCFS on ResNet-34 at $T=16$ improves performance from 59.35\% to 72.03\%, a 12.68\% increase. Similarly, for VGG-16 at $T=16$, combining TPP with QCFS boosts performance from 50.97\% to 73.98\%, a 23.01\% increase. Using TPP with RTS also shows significant improvements, such as a 12.82\% increase for VGG-16 at $T=16$. These results demonstrate the benefits of integrating TPP with other optimization approaches, solidifying its role as a comprehensive solution for ANN-SNN conversion challenges.

% \begin{table*}[htbp]
% \caption{Comparison between our proposed method and other ANN-SNN conversion methods on CIFAR-100 dataset. The average accuracy and standard deviation of the TPP method are reported over 5 experiments (for our methods, we need extra $c$ steps for summation, see Section \ref{sec latency}).}
% \label{tab:ann-snn-cifar100}
% \centering
% \scalebox{0.62}
% {
% \begin{threeparttable}
% \begin{tabular}{@{}ccccccccc@{}}
% \toprule
% Architecture & Method & ANN & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
% \toprule
% \multirow{10}{*}{ResNet-20}
% & TSC\tnote{*}~\cite{han2020deep} & 68.72 & -- & -- & -- & -- & -- & 58.42 \\
% & RMP\tnote{*}~\cite{han2020rmp} & 68.72 & -- & -- & -- & 27.64 & 46.91 & 57.69 \\
% & SNNC-AP\tnote{*}~\cite{li2021free} & 77.16 & -- & -- & 76.32 & 77.29 & 77.73 & 77.63 \\
% & RTS~\cite{DBLP:conf/iclr/DengG21} & 67.08 & -- & -- & 63.73 & 68.40 & 69.27 & 69.49 \\
% & OPI\tnote{*}~\cite{bu2022optimized} & 70.43 & -- & 23.09 & 52.34 & 67.18 & 69.96 & 70.51 \\
% & QCFS\tnote{+}~\cite{DBLP:conf/iclr/BuFDDY022} & 67.09 & 27.87 & 49.53 & 63.61 & 67.04 & 67.87 & 67.86 \\
% & Burst\tnote{*}~\cite{DBLP:conf/ijcai/Li022} & 80.69 & -- & -- & -- & 76.39 & 79.83 & 80.52 \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + QCFS} & 67.10 & 0.95 & 15.92 & 54.19 & 65.11 & 67.02 & 67.62 \\
% & \textbf{Ours (TPP) + QCFS} & 67.10 & \gc{46.88 (0.40)} & 64.77 (0.20) & 67.25 (0.12) & 67.74 (0.06) & 67.77 (0.05) & 67.79 (0.04) \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD)\tnote{*} + SNNC w/o Cali.} & 81.89 &  &  &  &  &  &  &  &  \\
% & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 81.89 & 39.67 (0.99) & \gc{71.05 (0.68)} & \gc{78.97 (0.24)} & \gc{81.06 (0.05)} & \gc{81.61 (0.08)} & \gc{81.62 (0.05)} \\
% \midrule
% \multirow{12}{*}{VGG-16} 
% & TSC\tnote{*}~\cite{han2020deep} & 71.22 & -- & -- & -- & -- & -- & 69.86 \\
% & SNM\tnote{*}~\cite{wang2022signed} & 74.13 & -- & -- & -- & 71.80 & 73.69 & 73.95 \\
% & SNNC-AP\tnote{*}~\cite{li2021free} & 77.89 & -- & -- & -- & 73.55 & 77.10 & \gc{77.86} \\
% & RTS\tnote{$\circ$}~\cite{DBLP:conf/iclr/DengG21} & 76.13 & 23.76 & 43.81 & 56.23 & 67.61 & 73.45 & 75.23 \\
% & OPI\tnote{*}~\cite{bu2022optimized} & 76.31 & -- & 60.49 & 70.72 & 74.82 & 75.97 & 76.25 \\
% & QCFS\tnote{+}~\cite{DBLP:conf/iclr/BuFDDY022} & 76.21 & 69.29 & 73.89 & 75.98 & 76.53 & 76.54 & 76.60 \\
% % & Burst\tnote{*}~\cite{DBLP:conf/ijcai/Li022} & 78.49 & -- & -- & -- & 74.98 & \gc{78.26} & \gc{78.66} \\
% & DDI~\cite{bojkovic2024data} & 70.44 & 51.21 & 53.65 & 57.12 & 61.61 & 70.44 & 73.82 \\
% & FTBC(+QCFS)~\cite{wu2024ftbc} & 76.21 & 71.47 & 75.12 & 76.22 & 76.48 & 76.48 & 76.48 \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + RTS} & 76.13 & 47.75 & 71.25 & 74.65 & 75.98 & 75.76 & 76.06 \\
% & \textbf{Ours (TPP) + RTS} & 76.13 & 37.88 (0.35) & 65.81 (0.27) & 73.05 (0.12) & 75.17 (0.17) & 75.64 (0.12) & 75.9 (0.08) \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD) + QCFS} & 76.21 & 3.05 & 73.82 & 76.43 & 76.73 & 76.71 & 76.63 \\
% & \textbf{Ours (TPP) + QCFS} & 76.21 & \gc{73.93 (0.22)} & \gc{76.03 (0.23)} & \gc{76.43 (0.07)} & 76.55 (0.03) & 76.55 (0.07) & 76.52 (0.04) \\
% \cmidrule{2-9}
% % & \textbf{Ours (TPD)\tnote{*} + SNNC w/o Cali.} & 77.87 & 55.28 & 72.33 & \gc{76.48} & \gc{77.46} & {77.67} & {77.73} \\
% & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 77.87 & 59.23 (0.65) & 73.16 (0.17) & 76.05 (0.26) & \gc{77.16 (0.09)} & \gc{77.56 (0.13)} & 77.64 (0.04) \\
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
% \item[*] Without modification to ReLU of ANNs.
% \item[+] Using authors' provided models and code. 
% \item[$\circ$] Self implemented.
% \end{tablenotes}
% \end{threeparttable}
% }
% \end{table*}




\begin{table*}[ht]
\vspace{-10pt}
\caption{Comparison between our method and the other ANN-SNN conversion methods on ImageNet and CIFAR-100. We provide the average accuracy and the associated standard deviation across 5 experiments.}
\label{tab:ann-snn-imagenet-cifar}
\centering
\scalebox{0.7}{
\begin{threeparttable}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Dataset & Arch. & Method & ANN & T=4  & T=8 & T=16 & T=32 & T=64 & T=128 \\
\toprule
\multirow{23}{*}{ImageNet} 
& \multirow{8}{*}{ResNet-34}
& RTS~\cite{DBLP:conf/iclr/DengG21}\textsuperscript{ICLR} & 75.66 & -- & -- & -- & 33.01 & 59.52 & 67.54 \\
& & SNNC-AP\tnote{*}~\cite{li2021free}\textsuperscript{ICML} & 75.66 & -- & -- & -- & 64.54 & 71.12 & 73.45 \\
& & QCFS~\cite{DBLP:conf/iclr/BuFDDY022}\textsuperscript{ICLR} & 74.32 & -- & -- & 59.35 & 69.37 & 72.35 & 73.15 \\
& & SRP~\cite{DBLP:conf/aaai/HaoBD0Y23}\textsuperscript{AAAI} & 74.32 & \gc{66.71} & \gc{67.62} & 68.02 & 68.40 & 68.61 & --\\
& & FTBC(+QCFS)~\cite{wu2024ftbc}\textsuperscript{ECCV} & 74.32 & 49.94 & 65.28 & 71.66 & 73.57 & 74.07 & 74.23 \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + QCFS} & 74.32 & {37.23 (0.07)} & {67.32 (0.06)} & \gc{72.03 (0.02)} & 72.97 (0.03) & 73.24 (0.02) & 73.30 (0.02) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 75.65 & 2.69 (0.03) & 49.24 (0.23) & 69.97 (0.10) & \gc{74.07 (0.06)} & \gc{75.23 (0.03)} & \gc{75.51 (0.05)} \\
\cmidrule{2-10}
& \multirow{12}{*}{VGG-16}
& SNNC-AP\tnote{*}~\cite{li2021free}\textsuperscript{ICML} & 75.36 & -- & -- & -- & 63.64 & 70.69 & 73.32 \\
& & SNM\tnote{*}~\cite{wang2022signed}\textsuperscript{IJCAI} & 73.18 & -- & -- & -- & 64.78 & 71.50 & 72.86 \\
& & RTS~\cite{DBLP:conf/iclr/DengG21}\textsuperscript{ICLR} & 72.16 & -- & -- & 55.80 & 67.73 & 70.97 & 71.89 \\
& & QCFS~\cite{DBLP:conf/iclr/BuFDDY022}\textsuperscript{ICLR} & 74.29 & -- & -- & 50.97 & 68.47 & 72.85 & 73.97 \\
& & Burst~\cite{DBLP:conf/ijcai/Li022}\textsuperscript{IJCAI} & 74.27 & -- & -- & -- & 70.61 & 73.32 & 73.00 \\
& & OPI\tnote{*}~\cite{bu2022optimized}\textsuperscript{AAAI} & 74.85 & -- & 6.25 & 36.02 & 64.70 & 72.47 & 74.24 \\
& & SRP~\cite{DBLP:conf/aaai/HaoBD0Y23}\textsuperscript{AAAI} & 74.29 & 66.47 & 68.37 & 69.13 & 69.35 & 69.43 & --\\
& & FTBC(+QCFS)~\cite{wu2024ftbc}\textsuperscript{ECCV} & 73.91 & 58.83 & 69.31 & 72.98 & 74.05 & 74.16 & 74.21 \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + RTS} & 72.16 & 30.50 (1.19) & 56.69(0.67) & 67.34 (0.25) & 70.63 (0.11) & 71.75 (0.05) & 72.05 (0.03) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + QCFS} & 74.22 & \gc{68.39 (0.08)} & \gc{72.99 (0.05)} & \gc{73.98 (0.07)} & 74.23 (0.03) & 74.29 (0.00) & 74.33 (0.01) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 75.37 & 54.14 (0.59) & 69.75 (0.27) & 73.44 (0.02) & \gc{74.72 (0.06)} & \gc{75.14 (0.02)} & \gc{75.25 (0.03)} \\
\cmidrule{2-10}
& \multirow{3}{*}{RegNetX-4GF}
& RTS~\cite{DBLP:conf/iclr/DengG21}\textsuperscript{ICLR} & 80.02 & -- & -- & -- & 0.218 & 3.542 & 48.60 \\
& & SNNC-AP\tnote{*}~\cite{li2021free}\textsuperscript{ICML} & 80.02 & -- & -- & -- & 55.70 & 70.96 & 75.78 \\ 
\cmidrule{3-10}
& & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 78.45 & -- & -- & \gc{22.71 (2.98)} & \gc{66.51 (0.44)} & \gc{75.54 (0.07)} & \gc{77.83 (0.04)} \\
\midrule
\multirow{22}{*}{CIFAR-100}
& \multirow{10}{*}{ResNet-20}
& TSC\tnote{*}~\cite{han2020deep}\textsuperscript{ECCV} & 68.72 & -- & -- & -- & -- & -- & 58.42 \\
& & RMP\tnote{*}~\cite{han2020rmp}\textsuperscript{CVPR} & 68.72 & -- & -- & -- & 27.64 & 46.91 & 57.69 \\
& & SNNC-AP\tnote{*}~\cite{li2021free}\textsuperscript{ICML} & 77.16 & -- & -- & 76.32 & 77.29 & 77.73 & 77.63 \\
& & RTS~\cite{DBLP:conf/iclr/DengG21}\textsuperscript{ICLR} & 67.08 & -- & -- & 63.73 & 68.40 & 69.27 & 69.49 \\
& & OPI\tnote{*}~\cite{bu2022optimized}\textsuperscript{AAAI} & 70.43 & -- & 23.09 & 52.34 & 67.18 & 69.96 & 70.51 \\
& & QCFS\tnote{+}~\cite{DBLP:conf/iclr/BuFDDY022}\textsuperscript{ICLR} & 67.09 & 27.87 & 49.53 & 63.61 & 67.04 & 67.87 & 67.86 \\
& & Burst\tnote{*}~\cite{DBLP:conf/ijcai/Li022}\textsuperscript{IJCAI} & 80.69 & -- & -- & -- & 76.39 & 79.83 & 80.52 \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + QCFS} & 67.10 & \gc{46.88 (0.40)} & 64.77 (0.20) & 67.25 (0.12) & 67.74 (0.06) & 67.77 (0.05) & 67.79 (0.04) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 81.89 & 39.67 (0.99) & \gc{71.05 (0.68)} & \gc{78.97 (0.24)} & \gc{81.06 (0.05)} & \gc{81.61 (0.08)} & \gc{81.62 (0.05)} \\
\cmidrule{2-10}
& \multirow{12}{*}{VGG-16} 
& TSC\tnote{*}~\cite{han2020deep}\textsuperscript{ECCV} & 71.22 & -- & -- & -- & -- & -- & 69.86 \\
& & SNM\tnote{*}~\cite{wang2022signed}\textsuperscript{ICLR} & 74.13 & -- & -- & -- & 71.80 & 73.69 & 73.95 \\
& & SNNC-AP\tnote{*}~\cite{li2021free}\textsuperscript{ICML} & 77.89 & -- & -- & -- & 73.55 & 77.10 & \gc{77.86} \\
& & RTS\tnote{$\circ$}~\cite{DBLP:conf/iclr/DengG21}\textsuperscript{ICLR} & 76.13 & 23.76 & 43.81 & 56.23 & 67.61 & 73.45 & 75.23 \\
& & OPI\tnote{*}~\cite{bu2022optimized}\textsuperscript{AAAI} & 76.31 & -- & 60.49 & 70.72 & 74.82 & 75.97 & 76.25 \\
& & QCFS\tnote{+}~\cite{DBLP:conf/iclr/BuFDDY022}\textsuperscript{ICLR} & 76.21 & 69.29 & 73.89 & 75.98 & 76.53 & 76.54 & 76.60 \\
& & DDI~\cite{bojkovic2024data}\textsuperscript{AISTATS} & 70.44 & 51.21 & 53.65 & 57.12 & 61.61 & 70.44 & 73.82 \\
& & FTBC(+QCFS)~\cite{wu2024ftbc}\textsuperscript{ECCV} & 76.21 & 71.47 & 75.12 & 76.22 & 76.48 & 76.48 & 76.48 \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + RTS} & 76.13 & 37.88 (0.35) & 65.81 (0.27) & 73.05 (0.12) & 75.17 (0.17) & 75.64 (0.12) & 75.9 (0.08) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP) + QCFS} & 76.21 & \gc{73.93 (0.22)} & \gc{76.03 (0.23)} & \gc{76.43 (0.07)} & 76.55 (0.03) & 76.55 (0.07) & 76.52 (0.04) \\
\cmidrule{3-10}
& & \textbf{Ours (TPP)\tnote{*} + SNNC w/o Cali.} & 77.87 & 59.23 (0.65) & 73.16 (0.17) & 76.05 (0.26) & \gc{77.16 (0.09)} & \gc{77.56 (0.13)} & 77.64 (0.04) \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\item[] *: Without modification to ReLU of ANNs. +: Using authors' provided models and code. $\circ$: Self implemented.
% \item[*] Without modification to ReLU of ANNs.
% \item[+] Using authors' provided models and code. 
% \item[$\circ$] Self implemented.
\end{tablenotes}
\end{threeparttable}
}
\vspace{-15pt}
\end{table*}



\noindent \textbf{CIFAR dataset:} We further evaluate the performance of our methods on CIFAR-100 dataset and present the results in Table ~\ref{tab:ann-snn-imagenet-cifar}. We observe similar patterns as with the ImageNet. When comparing our method with ANN-SNN conversion methods which use non-\relu activations, e.g. QCFS and RTS, our method constantly outperforms RTS on ResNet-20 and VGG16. QCFS baseline suffers from necessity to train ANN models from scratch with custom activations, while our method is applicable to any ANN model with \relu-like activation. Furthermore, custom activation functions sometimes sacrifice the ANN performance as can be seen from the corresponding ANN accuracies. 

% \veb{DVSCIFAR dataset}
\noindent \textbf{CIFAR10-DVS dataset:} 
We evaluate our method on the event-based CIFAR10-DVS~\cite{li2017cifar10} dataset, comparing it with state-of-the-art direct training and ANN-SNN conversion methods (Table~\ref{tab:other-snn-training-methods}). Our approach demonstrates superior performance, achieving \gc{82.40\%} accuracy at just 8 timesteps and further improving to \gc{83.20\%} at 64 timesteps. Notably, our method outperforms the direct training method Spikformer~\cite{zhouSpikformerWhenSpiking2022} (80.90\%) and the ANN-SNN conversion method AdaFire~\cite{wang2024adaptive} (81.25\%).

\subsection{Comparison with other types of SNN training methods and models}

%%% state-of-the-art of all SNNs: https://github.com/zhouchenlin2096/Awesome-Spiking-Neural-Networks
%%% num of params (some): https://colab.research.google.com/drive/1qV3QrR1JBDvVRD48tJcQjlAe10YFKuoR?usp=sharing
%%% excel table data: https://cityuedumo-my.sharepoint.com/:x:/g/personal/xiaofengwu_cityu_edu_mo/ER4uC7WN6x9CoDz_EuWhrgUB9Rdjr39Lp3RGZaNVHrCUoQ?e=wvOxVG

\begin{table}[!ht]
    \vspace{-5pt}
    \caption{Comparison of direct and hybrid training methods for SNNs on CIFAR-100, ImageNet and CIFAR10-DVS. Baseline results include the highest reported accuracy and corresponding latency.}
    \label{tab:other-snn-training-methods}
    \renewcommand\arraystretch{1.2}
    \centering
    \scalebox{0.65}{
    \begin{threeparttable}    
    \begin{tabular}{cccc}
        \toprule
        Method & Category & Timesteps & Accuracy \\
        \toprule
        \multicolumn{4}{c}{\textbf{VGG-16 [CIFAR-100]}} \\
        \midrule
        LM-H~\cite{hao2023progressive}\textsuperscript{ICLR} & Hybrid Training & 4 & 73.11 \\
        SEENN-II~\tnote{*}~\cite{DBLP:conf/nips/LiGKP23}\textsuperscript{NeurIPS} & Direct Training & 1.15\tnote{*} & 72.76 \\
        Dual-Phase~\cite{DBLP:journals/corr/abs-2205-07473} & Hybrid Training & 4 / 8 & 70.08 / 75.06 \\
        \textbf{Ours (TPP) + QCFS} & ANN-SNN & 4 / 8 & \gc{73.93 / 76.03} \\
        \midrule
        \multicolumn{4}{c}{\textbf{ResNet-20 [CIFAR-100]}} \\
        \midrule
        LM-H~\cite{hao2023progressive}\textsuperscript{ICLR} & Hybrid Training & 4 & 57.12 \\
        TTS~\cite{DBLP:conf/aaai/GuoCLPZHM24}\textsuperscript{AAAI} & Direct Training & 4 & 74.02 \\
        \textbf{Ours (TPP) + SNNC w/o Cali.} & ANN-SNN & 16 & \gc{78.97} \\             
        \midrule
        \multicolumn{4}{c}{\textbf{ResNet-34 [ImageNet]}} \\
        \midrule
        SEENN-I~\cite{DBLP:conf/nips/LiGKP23}\textsuperscript{NeurIPS} & Direct Training & 3.38~\tnote{*} & 64.66 \\
        RMP-Loss~\cite{DBLP:conf/iccv/GuoLCZPZHM23}\textsuperscript{ICCV} & Direct Training & 4 & 65.17 \\
        RecDis-SNN~\cite{guo2022recdis}\textsuperscript{CVPR} &  Direct Training & 6 & 67.33 \\
        SpikeConv~\cite{liu2022spikeconverter}\textsuperscript{AAAI} & Hybrid Training & 16 & 70.57 \\
        GAC-SNN~\cite{DBLP:conf/aaai/QiuZC0DL24}\textsuperscript{AAAI} & Direct Training & 6 & 70.42 \\
        TTS~\cite{DBLP:conf/aaai/GuoCLPZHM24}\textsuperscript{AAAI} & Direct Training & 4 & 70.74 \\
        SEENN-I~\cite{DBLP:conf/nips/LiGKP23}\textsuperscript{NeurIPS} & ANN-SNN & 29.53~\tnote{*} & 71.84 \\
        \textbf{Ours (TPP) + QCFS} & ANN-SNN & 16 & \gc{72.03} \\
        \textbf{Ours (TPP)+ SNNC w/o Cali.} & ANN-SNN & 32 & \gc{74.07} \\
        \midrule
        \multicolumn{4}{c}{\textbf{ResNet-18 [CIFAR10-DVS]}} \\
        \midrule % the following is from paper https://arxiv.org/pdf/2311.14265
        TA-SNN~\cite{yaoTemporalwiseAttentionSpiking2021}\textsuperscript{ICCV} & Direct Training & 10 & 72.00 \\
        PLIF~\cite{fangIncorporatingLearnableMembrane2021}\textsuperscript{ICCV} & Direct Training & 20 & 74.80 \\
        Dspkie~\cite{liDifferentiableSpikeRethinking2021}\textsuperscript{NeurIPS} & Direct Training & 10 & 75.40 \\
        DSR ~\cite{mengTrainingHighPerformanceLowLatency2022}\textsuperscript{CVPR} & Direct Training & 10 & 77.30 \\
        Spikformer ~\cite{zhouSpikformerWhenSpiking2022}\textsuperscript{ICLR} & Direct Training & 10 & 80.90 \\
        AdaFire ~\cite{wang2024adaptive}\textsuperscript{AAAI} & ANN-SNN & 8 & 81.25 \\
        \textbf{Ours (TPP) + SNNC w/o Cali.}  & ANN-SNN & \makecell{4 \\ \gc{8} \\ 16 \\ 32 \\ 64} & \makecell{77.90 \\ \gc{82.40} \\ 82.80 \\83.00 \\ 83.20} \\
        \bottomrule
    \end{tabular}
\begin{tablenotes}
\item[*] The average number of timesteps during inference on the test dataset.
\end{tablenotes}
\end{threeparttable}
}
\vspace{-10pt}
\end{table}
\raggedbottom % remove space below the table.


We compare our approach with several state-of-the-art direct training and hybrid training methods as presented in Table~\ref{tab:other-snn-training-methods}. The comparison is founded on performance metrics like accuracy and the number of timesteps utilized during inference on the CIFAR-100 and ImageNet datasets. We benchmark our method against prominent approaches such as LM-H~\cite{hao2023progressive}, SEENN~\cite{DBLP:conf/nips/LiGKP23}, Dual-Phase~\cite{DBLP:journals/corr/abs-2205-07473}, TTS~\cite{DBLP:conf/aaai/GuoCLPZHM24}, RMP-Loss~\cite{DBLP:conf/iccv/GuoLCZPZHM23}, RecDis-SNN~\cite{guo2022recdis}, SpikeConv~\cite{liu2022spikeconverter}, and GAC-SNN~\cite{DBLP:conf/aaai/QiuZC0DL24}. We showcase the best accuracy comparable to state-of-the-art methods achieved by our approach with minimal timesteps. We prioritize accuracy, but direct training and hybrid training opt for a lower number of timesteps and sacrifice accuracy. We outperform LM-H~\cite{hao2023progressive} and Dual-Phase~\cite{DBLP:journals/corr/abs-2205-07473} for VGG-16 on CIFAR-100. For ResNet-20 on CIFAR-100, we have higher accuracy but longer timesteps. Additionally, for ResNet-34 on the ImageNet dataset, the accuracy of our method with QCFS with 16 timesteps is higher than that of SpikeConv~\cite{liu2022spikeconverter} with the same number of timesteps. We also achieve higher accuracy with longer timesteps as expected. Overall, our approach demonstrates promising performance and competitiveness in comparison with the existing SNN training methods.


% \subsection{Latency reduction for TPD neurons}

% Theorem \ref{thm weight decay} implies that if impose first order model on weights, we can reduce the latency logarithmically at no expense for the accuracy in case of TPD neurons. This is because the information that is output by the neuron remains the same after it is scaled with the weights. In particular, all the results reported in Tables \ref{tab:ann-snn-imagenet} and \ref{tab:ann-snn-cifar100} for TPD can remain the same, but in latency $\log_2(T)$ where $T$ is read from the respective tables. 

% Latency reduction in combination with TPP neurons does not work well in general. This is because of the initial high values of the weights in combination with non-zero probability of spiking at any early step (Theorem \ref{thm probabilistic main}), may cause network to fire spikes for which it cannot compensate at later time steps. 

\begin{figure*}[!ht]
\centering
% \subfigbottomskip=2pt
% \subfigcapskip=-5pt
\subfigure[Layer 4]{
    \includegraphics[width=0.32\linewidth]{figures/layer_4.pdf}}
\subfigure[Layer 8]{
    \includegraphics[width=0.32\linewidth]{figures/layer_8.pdf}}
\subfigure[Layer 12]{
    \includegraphics[width=0.32\linewidth]{figures/layer_12.pdf}}    
\vskip -0.1in
\caption{The membrane potential distributions of the first channel (randomly selected) across three modes (baseline, shuffle, and probabilistic) in VGG-16 on CIFAR-100. For comparative analysis, the first two timesteps ($t=1$, $t=2$) from a total of eight timesteps ($T=8$) are selected for each mode. The baseline mode (blue) attains an accuracy of 24.22\%, while the shuffle mode (light green) enhances accuracy to 70.54\%, and the probabilistic mode (dark orange) further improves accuracy to 73.42\%. The distributions are presented prior to neuronal firing, with the red dashed line indicating the threshold voltage (Vth) for the respective layer (see Appendix~\ref{appendix:mem-pot-dist}).}
 \label{fig:mem-pot-distribution}
 \vspace{-5pt}
\end{figure*}


\subsection{Spiking activity}
The event driven nature of various neuromorphic chips implies that the energy consumption is directly proportional to the spiking activity, i.e., the number of spikes produced throughout the network: the energy is consumed in the presence of spikes. To this end, we tested our proposed method (TPP) for the spike activity and compared with the baselines. For a given model, we counted the average number of spikes produced after each layer, per sample, for both the baseline and our method. Figure~\ref{fig:spike-counts} shows the example of RTS and RTS + TPP.  Both the baseline and our method exhibit similar spike counts. In particular, our method constantly outperforms the baselines, and possibly in doing so it needs longer average latency per sample ($T$). However, the energy consumed is approximately the same as that for the baseline in time $T$. The complete tables are present in Appendix \ref{appendix:firing-counts}, where we provide more detailed picture of spike activities.
\begin{figure}[h]
\centering
\includegraphics[width=0.9\linewidth]{figures/vgg16-cifar100-rts.pdf}
\vskip -0.2in
\caption{\small Spike counts of VGG-16 on CIFAR-100 of RTS baseline compared with RTS+TPP. Note: The bar height from bottom indicates the spike counts after each timestep T, and the color of longer Ts is overlaid by shorter Ts (see Appendix \ref{appendix:firing-counts}) }
\label{fig:spike-counts}
\vspace{-10pt}
\end{figure}

\subsection{Membrane potential distribution in early time steps} 

In Figure \ref{fig:mem-pot-distribution} we compare the membrane potential distributions for baseline models and two methods that we studied in the paper, the permutations applied on spike trains and TPP method. Once again, it can be seen another reason for performance degradation of baseline models in low latency, as the membrane potential is not variable enough to produce spike informative spike trains, which is particularly visible in deeper layers. On the other side ``permuted'' and TPP models show sufficient variability throughout the layers. 

By increasing the latency, the baseline models can recover some of the variability and spike production, as can be seen in Figure \ref{fig:spike-counts}. But, due to the misplacement of spike trains through temporal dimension, they are still not able to pick up on the ANN performance. 


\section{Conclusions and future work} 
\label{sec:conclusion}

This work identified the phenomenon of ``temporal misalignment'' in ANN-SNN conversion, where random spike rearrangement enhances performance. We introduced two-phase probabilistic (TPP) spiking neurons, designed to intrinsically perform the effect of spike permutations. We show biological plausibility of such neurons as well as the hardware friendlines of the underlying mechanisms. We demonstrate their effectiveness through exhaustive experiments on large scale datasets, showing their competing performance compared to SOTA ANN-SNN conversion and direct training methods. 

In the future work, we aim to study the effect of permutations and probabilistic spiking in combination with directly trained SNN models.

% \newpage


% \begin{table}[h]
%     \centering
%     \begin{tcolorbox}[colframe=blue!70!black, colback=blue!10, sharp corners=all]
%         \begin{tabular}{|c|c|c|c|c|c|}
%             \hline
%             A1 & B1 & C1 & D1 & E1 & F1 \\
%             \hline
%             A2 & B2 & C2 & D2 & E2 & F2 \\
%             \hline
%             A3 & B3 & C3 & D3 & E3 & F3 \\
%             \hline
%             A4 & B4 & C4 & D4 & E4 & F4 \\
%             \hline
%         \end{tabular}
%     \end{tcolorbox}
%     \begin{tcolorbox}[colframe=red!70!black, colback=red!10, sharp corners=all]
%         \begin{tabular}{|c|c|c|c|c|c|}
%             \hline
%             A5 & B5 & C5 & D5 & E5 & F5 \\
%             \hline
%             A6 & B6 & C6 & D6 & E6 & F6 \\
%             \hline
%             A7 & B7 & C7 & D7 & E7 & F7 \\
%             \hline
%             A8 & B8 & C8 & D8 & E8 & F8 \\
%             \hline
%         \end{tabular}
%     \end{tcolorbox}
% \end{table}



% \begin{table}[htbp]
% \caption{Comparison of ANN-SNN conversion methods. Highlighted boxes indicate results for specific datasets. Colored entries show grouped $T$ values.}
% \label{tab:ann-snn-single-column}
% \centering
% \scalebox{0.85}{
% \begin{threeparttable}
% \begin{tabular}{@{}cccccc@{}}
% \toprule
% \textbf{Arch.} & \textbf{Method} & \textbf{ANN} & \textbf{$T=4, 8, 16$} & \textbf{$T=32, 64, 128$} \\ 
% \midrule

% \rowcolor{green!10}
% \multicolumn{5}{l}{\begin{tcolorbox}[colframe=green!70!black, colback=green!5, boxrule=1pt, sharp corners]
% \textbf{ImageNet Results}
% \end{tcolorbox}} \\

% ResNet-34 & RTS~\cite{DBLP:conf/iclr/DengG21} & 75.66 & -- & 33.01, 59.52, 67.54 \\ 
%  & SNNC-AP~\cite{li2021free} & 75.66 & -- & 64.54, 71.12, 73.45 \\ 
%  & QCFS~\cite{DBLP:conf/iclr/BuFDDY022} & 74.32 & --, --, 59.35 & 69.37, 72.35, 73.15 \\ 
%  & SRP~\cite{DBLP:conf/aaai/HaoBD0Y23} & 74.32 & \cellcolor{red!15}66.71, 67.62, 68.02 & 68.40, 68.61, -- \\ 
%  & \textbf{Ours (TPP) + QCFS} & 74.32 & 37.23, 67.32, \cellcolor{red!15}72.03 & 72.97, 73.24, 73.30 \\ 

% \cmidrule{1-5}
% \rowcolor{blue!10}
% \multicolumn{5}{l}{\begin{tcolorbox}[colframe=blue!70!black, colback=blue!5, boxrule=1pt, sharp corners]
% \textbf{CIFAR-100 Results}
% \end{tcolorbox}} \\

% ResNet-20 & TSC~\cite{han2020deep} & 68.72 & -- & --, --, 58.42 \\ 
%  & RMP~\cite{han2020rmp} & 68.72 & -- & 27.64, 46.91, 57.69 \\ 
%  & SNNC-AP~\cite{li2021free} & 77.16 & --, --, 76.32 & 77.29, 77.73, 77.63 \\ 
%  & RTS~\cite{DBLP:conf/iclr/DengG21} & 67.08 & --, --, 63.73 & 68.40, 69.27, 69.49 \\ 
%  & \textbf{Ours (TPP) + QCFS} & 67.10 & \cellcolor{yellow!15}46.88, 64.77, 67.25 & 67.74, 67.77, 67.86 \\ 

% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% \end{table}



\section*{Impact Statements}

This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.


\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\setcounter{theorem}{0} % Reset theorem counter

\onecolumn
% \section{You \emph{can} have an appendix here.}


\section{Conversion steps}\label{app conversion steps}

% \subsection{Conversion steps}\label{app conversion steps}

\paragraph{Copying ANN architecture and weights.} ANN-SNN conversion process starts with a pre-trained ANN model, whose weights (and biases) will be copied to an SNN model following the same architecture. In this process, one considers ANN models whose non-activation layers become linear during the inference. In particular, these include fully connected, convolutional, batch normalization and average pooling layers.  

\paragraph{Approximating ANN activation functions.} The second step of the process considers the activation layers and their activation functions in ANN. Here, the idea is to initialize the spiking neurons in the corresponding SNN layer in such a way that their average spiking rate approximates the values of the corresponding activation functions. For the \relu (or \relu-like such as quantized or thresholded \relu) activations, this process is rather well understood. The spiking neuron threshold is usually set to correspond to the maximum activation ANN channel or layerwise, or to be some percentile of it. If we denote by $f$ the ANN actiavtion, then ideally, after setting the thresholds, one would like to have
\begin{equation}\label{eq main approx}
f(\bv[T])\approx \frac{\theta}{T}\cdot\sum_{t=1}^T \bs[t].
\end{equation} 
 
 If we recall the equations for the IF neuron (equations \eqref{eq: dynamics} in the article)
 \begin{align}\label{eq dynamics help}
\bv^\lup[t] &= \bv^\lup[t-1] +\bW^\lup \theta^{(l-1)}\cdot \bs^{(l-1)}[t]-\theta^\lup \cdot\bs[t-1],\\
\bs^\lup[t] &= H(\bv^\lup[t]- \theta^\lup),%\nonumber
\end{align}
we see that the value with which we are comparing the membrane potential (threshold) is the same as the value with which we are scaling the output spikes. In particular, as soon as our membrane potential has reached $\theta$, it will produce the value $\theta$. This can be loosely described as, whatever the input is, the output will be approximately that value (or zero, if the input is negative), which is exactly what \relu does. 

\paragraph{Absorbing thresholds.} Finally, we notice that, once we produce a spike $\bs^\lup[t]$, the value $\theta^\lup\cdot \bs^\lup[t]$ will be sent to the next layer, and will further be weighted with weights $W^{(l+1)}$ and the bias $b^{(l+1)}$ will be applied. As we want SNNs to operate only using ones and zeros (to avoid multiplication due to energy efficiency), the values $\theta^\lup$ will be absorbed into $W^{(l+1)}$, i.e. $W^{(l+1)}\leftarrow \theta^\lup W^{(l+1)}$.

% \section{Two-phase Deterministic neurons}

% \subsection{Inner and outer thresholds}
% In order to convert functions beyond \relu, we introduce the notion of the \textbf{inner thresholds} $\theta_{i}$. These are the values that we use to compare our membrane potential with, and should be distinguished from the \textbf{outer thresholds} $\theta_o$, which is the value with which we scale our output spikes. Moreover, as the outer thresholds are absorbed in the subsequent weights (so that, indeed, the layers of the SNN communicate in form of the binary spikes), they should be independent of the time step at which we are using them (throughout the latency $T$). On the other side, we are at liberty to change the inner thresholds through time, as this is what allows us to convert different non piece-wise linear activation functions from ANN.

% \subsection{Rewriting dynamical equations} Above being said, we rewrite the previous equations as
% \begin{align}\label{eq dynamics new}
% \bv^\lup[t] &= \bv^\lup[t-1] +\bW^\lup \theta^{(l-1)}_o\cdot \bs^{(l-1)}[t]-\theta^\lup_i[t-1] \cdot\bs[t-1],\\
% \bs^\lup[t] &= H(\bv^\lup[t]- \theta^\lup_i[t]),%\nonumber
% \end{align}
% while on the side of two-phase deterministic neurons, we have the following equations corresponding to  \eqref{eq deterministic spiking}
% \begin{subequations} \label{eq new deterministic spiking}
% \begin{equation}\label{eq new deterministic spiking spike}
% \bs^\lup[T+t] = H\left(\bv^\lup[T+t-1]-\theta^\lup_i[T+t]\right),
% \end{equation}
% \begin{equation}\label{eq new deterministic spiking reset}
% \bv^\lup[T+t] = \bv^\lup[t-1] -\theta^\lup_i[T+t] \cdot\bs[T+t-1].
% \end{equation}
% \end{subequations} 

% We emphasize that in \eqref{eq new deterministic spiking}, the \textbf{scaling of the output spikes is done with the value $\theta_o$}. 

% \begin{theorem}\label{thm main help}
%     Let $f:[a,b]\to [0,c]$ be an increasing continuous function, with $f(a)=0$ and $f(b)=c$, and let $T>0$ be an integer. Let us further put $\theta_o := \frac{c}{T}$, and let $\theta_i[T+1]:=f^{-1}(\theta_o)$, while $\theta_i[T+t]:=f^{-1}(t\cdot\theta_o)-\theta_i[T+t-1]$, for $t=2,\dots,T$. 
    
%     Then, if $\bv[T]\in [a,b]$, and we initialize the system \eqref{eq new deterministic spiking} with our set values of $\theta_o$ and $\theta_i$, we have
%     \begin{equation}
%         \big|f(\bv[T])-\sum_{t=1}^T\theta_o\cdot \bs[T+t]\big|<\theta_o.
%     \end{equation}
%     More precisely, 
%     \begin{equation}
%         \sum_{t=1}^T\theta_o\cdot \bs[T+t] = f(\sum_{t'=1}^t\theta_i[T+t'])=t\cdot \theta_o,
%     \end{equation}
%     where $t$ is the maximum such that $\bv[T]\leq \sum_{t'=1}^t\theta_i[T+t'] $.
% \end{theorem}
% \begin{proof}
%     The reader can refer to Figure \ref{fig function graph} for the visual representation of the context. Looking at the equation \eqref{eq new deterministic spiking reset} and unrolling it through time $t$, we notice that the spiking will occur for all $t$ for which $\bv[T]\geq\sum_{t'=1}^t\theta_i[T+t]=f^{-1}(t\cdot \theta_o)$.  
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=.5\linewidth]{figures/function_graph.png}
%   \caption{Something}
%   \label{fig function graph}
% \end{figure}
%     Let $t$ be such that $\bv[T]\geq\sum_{t'=1}^t\theta_i[T+t]$ and $\bv[T]<\sum_{t'=1}^{t+1}\theta_i[T+t]$ (the extreme case of $\bv[T]=b$ can be done in a similar fashion). In this case, the output of the TPD after $T$ time steps will be $\sum_{t'=1}^t \theta_o\cdot \bs[T+t']= t\cdot \theta_o$. Then, we can write 
%     \begin{align*}
%     \sum_{t'=1}^ t \theta_o\cdot \bs[T+t'] &= \sum_{t'=1}^ t \theta_o= t\cdot \theta_0 \\
%     &= f(\sum_{t'=1}^t\theta_i[T+t'])\leq f(\bv[T])<f(\sum_{t'=1}^{t+1}\theta_i[T+t'])=(t+1)\cdot \theta_o.
%     \end{align*}
% The claim follows.
% \end{proof}
% \begin{rmk}\label{rmk step approximation}
%     In fact, the previous theorem and its proof show that through the mechanism of equations  \eqref{eq new deterministic spiking spike} and \eqref{eq new deterministic spiking reset}, we are approximating the function $f$ with its step-version in the form 
%     $$
%     f(x)\approx f(\sum_{t'=1}^t\theta_i[T+t']),
%     $$
%     which is depicted in the green color in Figure \ref{fig function graph}. 
% \end{rmk}

% \begin{rmk}\label{rmk extension}
%     Although, the previous results is stated for the functions $f$ with compact support (on a segment $[a,b]$), it is not difficult to see that it generalizes to the increasing functions with non-compact support. In particular, for continuous increasing functions $f:(a,b)\to (0,c)$, where $0<c<\infty$ and $\lim_{x\to a}f(x)=0$ and $\lim_{x\to b}f(x)=c$. This also holds for $a=-\infty$ and $b=\infty$ cases.

%     Moreover, a case where $f(x)=0$ for $x<a$, also satisfies the conclusion of the theorem, with the same initialization of parameters $\theta_o$ and $\theta_i$. Namely, in this case, 
% \end{rmk}
% \begin{rmk}
%     Note that by construction, $\theta_i[T+t]>0$, for $t>1$ because $f$ is increasing. In general, it can happen that $\theta[T+1]<0$, which depends on function $f$ and $T$. 
% \end{rmk}

% \subsection{\relu activation: $f=\relu$}
% Note that $f=\relu$ satisfies the conditions of Theorem \ref{thm main help} in combination with Remark \ref{rmk extension}, as long as we restrict its support from above with some constant $b$. By construction, in this case $\theta_i[T+t]=\theta_o$, for $t=1,\dots, T$, and we recover the familiar dynamics for the IF neurons used in ANN-SNN conversion of \relu activated ANNs.

% \textbf{Implementation} In practice, the constant $b$ is set to be the maximum activation of \relu (at a particular layer), when the training dataset or its representative is passed through the network. For a fixed latency $T$, the thresholds are set as $\theta=\theta_o=\theta_i[T+t]=\frac{b}{T}$.

% \subsection{Sigmoid activation: $f=\sigma$}
% The sigmoid activation $f=\sigma$ also satisfies the extension of \ref{thm main help} through Remark \ref{rmk extension}. In particular, in this case, we take $c:=1$, as this is supremum of $\{\sigma(x)\mid x\in \mathbb{R}\}$. 

% \textbf{Implementation} For a fixed latency $T$, we set $\theta_o:=\frac{1}{T+1}$, while the inner thresholds are set according to the Theorem \ref{thm main help}, as $\theta_i[T+1]:=\sigma^{-1}(t\cdot \theta_o)$, for $t=1,\dots,T$. 

% \subsection{SiLU activation: $f=\text{SiLU}$}

% The SiLU activation function does not satisfy the conditions of the Theorem \ref{thm main help} nor its extension in Remark \ref{rmk extension} (for example, it is strictly decreasing on an open segment in its support). 

% However, for the practical purposes, there is a way around this. We note that the SiLU function has global minimum, at point $x\approx -1.28$ and it is approximately $d=-0,28$. If we bound the input size on the upper side by some constant $b>0$ In particular, we can apply the previous result on the restriction of the function $\silu'(x)=\silu(x)-d$ to the interval $(-1.28,b)$, extended to the left by setting $\silu'(x)=0$, for $x\leq -1.28$.

% On the other side, if we consider the restriction of the function $\silu''(x) = -\silu(-x)-d$ on the interval $(1.28, \infty)$, then it also satisfies the conditions of Remark \ref{rmk extension}. 

% \textbf{Implementation} When considering $\silu$ as an activation in an ANN and at a particular layer, we can find $b$ by passing the training dataset (or its representative) through the network and record the maximum of the input. 

% Then, \silu can be approximated with two neurons, one of which will accumulate the input $\bv[T]$, while the other will accumulate (and receive) its negative. The first neuron will output the approximation for $\silu'$ while the second will output the approximation for $\silu''$. Their difference will be an approximation for the function $\silu(x)-d$. Finally, the term $d$ can be absorbed in the subsequent weights in the form of the bias.

% In this way, the approximation of $f(\bv[T])$ will be up to $\max\{\theta_o',\theta_o''\}$, where $\theta_o'$ and $\theta_o''$ are the found values for the outer thresholds for functions $\silu'$ and $\silu''$, respectively. In practice, the maximum is usually dominated will be dominated by $\theta'_o$, as $\silu(b)>>0.28$.

% For $\silu'$, we set $\theta_o:=\frac{\silu'(b)}{T}$, and $\theta_i[T+t]:=\silu'^{-1}(t\cdot \theta_o)$, $t=1,\dots, T$. We do similarly for $\silu''$.

% \subsection{Latency reduction}
% We consider latency reduction through weights and thresholds first order models stated in the main article, that is suitable for approximating \relu activations in ANN. in this case, the discrete TPD neuron equations take the form
% \begin{equation}\label{eq deterministic spiking first model}
% \begin{aligned}
% \bs^\lup[T+t] &= H\left(\bv^\lup[T+t-1]-\theta^\lup[T+t]\right),\\
% \bv^\lup[T+t] &= \bv^\lup[t-1] -\theta^\lup[T+t] \cdot\bs[T+t-1].
% \end{aligned}
% \end{equation}

% \weightsmain*

% \begin{proof}
%     Note that by binary expansions, we can write $\frac{\bv[T]}{\theta}=\sum_{t=1}^T\frac{c_t}{2^{t}}+r$, where $c_t\in\{0,1\}$ and $0\leq r\leq\frac{1}{2^T}$. Then, we notice that the sequence $c_t$ exactly coincides with the sequence $\bs[T+t]$, which proves the result.
% \end{proof}

% \begin{theorem}\label{thm general weight decay}
% Let $f:[0,b]\to [0,b]$ be the identity function $f(x)=x$, with $f(a)=0$ and $f(b)=c$, and let $T>0$ be an integer. Let us further put $\theta_o:=c$, and let $\theta_i[T+t]:=f^{-1}(2^{T-t}\cdot\theta_o)$, $t=1,\dots, T$. 

% Then, if $\bv[T]\in[a,b]$, and we initialize the system \eqref{eq new deterministic spiking} with our set values of $\theta_o$ and $\theta_i$, we have
% \begin{equation}
%     \big|f(\bv[T])-\sum_{t=1}^T\frac{1}{2^{T-t+1}}\cdot\theta_o\cdot \bs[T+t]\big|\leq\frac{1}{2^T}.
% \end{equation}
% \end{theorem}

% \begin{theorem}\label{thm general weight decay}
% Let $f:[0,b]\to [0,b]$ be the identity function $f(x)=x$, with $f(a)=0$ and $f(b)=c$, and let $T>0$ be an integer. Let us further put $\theta_o:=c$, and let $\theta_i[T+t]:=f^{-1}(2^{T-t}\cdot\theta_o)$, $t=1,\dots, T$. 

% Then, if $\bv[T]\in[a,b]$, and we initialize the system \eqref{eq new deterministic spiking} with our set values of $\theta_o$ and $\theta_i$, we have
% \begin{equation}
%     \big|f(\bv[T])-\sum_{t=1}^T\frac{1}{2^{T-t+1}}\cdot\theta_o\cdot \bs[T+t]\big|\leq\frac{1}{2^T}.
% \end{equation}
% \end{theorem}
% \begin{proof}
% We can assume that $\bv[T]<b$, as the other case is trivial. Note that there exists $0<t<T$ such that $\theta_i[T+t]\leq \bv[T]<\theta_i[T+t-1]$, 

% On the other side, $\theta_i[T+t]\leq \bv[T]<\theta_i[T+t-1]$ if and only if $2^{T-t}\theta_o\leq f(\bv[T])<2^{T+t-1}$. 
%     We will prove the following, more precise statement: For any $0<t'\leq T$, we have
%     \begin{subequations}
%     \begin{equation}\label{eq log approx help}
%         \big|f(\bv[T])-\sum_{t=1}^{t'}\frac{c}{2^{T-t+1}}\cdot \bs[T+t]\big|\leq\frac{c}{2^{t'}},
%     \end{equation}
%     \begin{equation} \label{eq log approx help 2}
%         f(\bv[T+t'])<\frac{c}{2^{t'}}.
%     \end{equation}
%     \end{subequations}
%     We proceed by induction on $t'$. For $t'=1$, we note that $\bs[t]=1$ if and only if $\bv[T]\geq\theta[T+1]=f^{-1}(2^{T-1}\cdot\theta_o)$ if and only if $f(\bv[T])\geq \frac{c}{2}$. In particular, bot equations \eqref{eq log approx help} and \eqref{eq log approx help 2} hold. 
    
%     Now, suppose that \eqref{eq log approx help} and \eqref{eq log approx help 2} hold for all $1\leq t''<t'=t''+1$ and let us also prove that it holds for $t'$ as well. Then claim will follow by induction. To this end, we note that $\bs[T+t']=1$ if and only if $\bv[T+t'']\geq\theta_i[T+t']$ which holds if and only if $f(\bv[T+t''])\geq \frac{c}{2^{t'}}$. In particular, we have that 
%     $$
%     f(\bv[T+t'])<\frac{c}{2^{t'}}.
%     $$
%     Furthermore, 
% \end{proof}

% \textbf{Implementation} For \relu function at a fixed layer in an ANN model,  we can find an approximation to its maximum input/activation, say $b$ when a training dataset (or its representative) is passed through the network. Then, the values for the threshold $\theta=\theta[T]$, and further $\theta[T+t]$ are found according to the recursion. Furthermore, $\theta$ will be absorbed into the subsequent weights, which will follow the recursion of the form $W[T+t+1]=\frac{1}{2}W[T+t]$, so that we have $W[T+t]=\frac{1}{2^t} W[T] = \frac{1}{2^t}W$.   
% \section{Two-phase Probabilistic neurons}
% We prove the main theorem about TPP neurons, which for the convenience we restate here.
\section{Proof of the theoretical results}
\label{appendix-sec:conversion_error_analysis}
We prove the main theorem from the article, which we restate here. 
\mainthm*

\begin{proof}
    We start by rewriting equation \eqref{eq probabilistic spiking} as 
    \begin{align}
    \bs^\lup[t] &= B\left(\frac{1}{\theta^\lup\cdot (T-t+1)}\cdot\bv^\lup[t-1]\right)\\
    &= B\left(\frac{1}{\theta^\lup\cdot (T-t+1)}\cdot\left(T\cdot X^\lup-\theta^\lup\cdot\sum_{i=1}^{t-1}s^\lup[i]\right)\right)\\
    &= B\left(\frac{1}{T-t+1}\cdot\left(T\cdot \frac{X^\lup}{\theta^\lup}-\sum_{i=1}^{t-1}s^\lup[i]\right)\right),\label{eq last expr}
    % &= B\left(\frac{X^\lup}{\theta^\lup}+\frac{1}{T-t+1}\cdot\left((t-1)\cdot \frac{X^\lup}{\theta^\lup}-\sum_{i=1}^{t-1}s^\lup[i]\right)\right)\\
    % &= B\left(\frac{X^\lup}{\theta^\lup}+\frac{1}{T-t+1}\cdot\left((t-1)\cdot \frac{X^\lup}{\theta^\lup}-S[t-1]\right)\right), \label{eq last expr}
    \end{align}
    obtained by unrolling through time the expression for the membrane potential. 
    % and where we put $S[t-1]:=\sum_{i=1}^{t-1}s^\lup[i]$. 

    We next consider various settings of the theorem.
For the first three statements, we can assume that the vector $X^\lup$ is one dimensional, i.e. $X^\lup\in\mathbb{R}$. We start by first considering the situation where $X^\lup>\theta^\lup$, so that the output of the ANN is $\relu_{\theta^\lup}(X^\lup)=\theta^\lup$. In that case, we notice that the spiking neuron will fire at every time step $t=1,\dots, T$, because the bias for the Bernoulli random variable will always be bigger than or equal to 1, as follows from the previous equations. Similarly, if $X^\lup\leq0$, the bias will always be non-positive, and both the output of ANN and of SNN will be 0. Consequently, the first three statements follow directly for both of these cases.

Suppose now that $0\leq X^\lup<\theta^\lup$ (or, equivalently, $0\leq\frac{X^\lup}{\theta^\lup}<1$) and let $a$ be minimal non-negative integer such that $a\cdot\theta^\lup\leq T\cdot X^\lup<(a+1)\cdot \theta$, i.e. $a:=\lfloor \frac{T\cdot X^\lup}{\theta^\lup}\rfloor$. In particular, $0\leq a<T$. We proceed to prove statement $(b)$ above. We note that in general, $\sum_{i=1}^{t-1}s^\lup[i]\leq a+1$, because after at most $a+1$ spikes, the residue membrane potential would become negative. Also, due to the condition of the statement that the residue membrane potential is still non-negative, we may assume that $\sum_{i=1}^{t-1}s^\lup[i]\leq a$. Then, it becomes clear that the bias in the last equation \eqref{eq last expr} is a non-negative number smaller than 1, and it follows that 
\begin{equation}
    \mathbb{E}\left[s_i^{(l)}[t]\right] = \frac{1}{T-t+1}\cdot\left(T\cdot \frac{X^\lup}{\theta^\lup}-\sum_{i=1}^{t-1}s^\lup[i]\right),
\end{equation}
so that we finally have 
\begin{align}
    \frac{\theta^\lup\cdot (T-t+1)}{T}&\cdot\mathbb{E}\left[s_i^{(l)}[t]\right]+\frac{\theta^\lup}{T}\cdot\sum_{i=1}^{t-1} s_i^{(l)}[i]\\
    &=\frac{\theta^\lup\cdot (T-t+1)}{T}\cdot\frac{1}{T-t+1}\cdot\left(T\cdot \frac{X^\lup}{\theta^\lup}-\sum_{i=1}^{t-1}s^\lup[i]\right)+\frac{\theta^\lup}{T}\cdot\sum_{i=1}^{t-1} s_i^{(l)}[i]\\
    &= X^\lup = \relu_{\theta^\lup}(X^\lup).
\end{align}
Statement $(a)$ follows from $(b)$, while for the statement $(c)$ for the first case, we notice that after every spike, the residue membrane potential is a multiple of $\theta^\lup$, hence we will have exactly $a$ spikes (notation above), while the second case follows from statement $(b)$. Finally, statement $(d)$ is a direct consequence of the above discussion. 
\end{proof}
% \begin{equation}
% \frac{\theta}{t}\cdot\left(\mathbb{E}\left[s_i^{(l)}[t]\right]+\sum_{i=1}^{t-1} s_i^{(l)}[i]\right) = \relu_\theta(X_i^{(l)}).
% \end{equation}



% such that $a\cdot \theta^\lup=\relu_{\theta^\lup}(X^\lup)$. Equation \eqref{eq last expr} then simplifies to 
%     \begin{equation}\label{eq simple 1}
%     s^\lup[t] = B\left(\frac{X^\lup}{\theta^\lup}+\frac{1}{T-t+1}\cdot\left((t-1)\cdot \frac{X^\lup}{\theta^\lup}-S[t-1]\right)\right)
%     \end{equation}
%     \eqref{eq simple 1}
% \end{proof}
% \begin{proof}\veb{TODO: Finish this proof}
%     % Notice that whenever there is a spike, the membrane potential decreases by $\theta^\lup$. In particular, after at most $t+1$ spikes, by the condition in the Theorem, the membrane potential will be negative. Hence, probability of having a spike will be 0. On the other side, if for $T-t$ time steps, we did not have a spike, this would mean that the bias $x$ of the Bernoulli variable $B(x)$ is larger than 1, which consequently will yield a spike with probability 1. Furthermore, after spiking, the bias remains bigger than 0. This means that we will have $t$ spikes with probability 1. The other cases are done in a similar way. The rest of the claim is easy.
% \end{proof}

% \exppermutations*


\section{Experiments Details}

\subsection{Datasets}

\noindent \textbf{CIFAR-10}: The CIFAR-10 dataset~\cite{krizhevsky2010cifar} contains 60,000 color images of 32x32 pixels each, divided into 10 distinct classes (e.g., airplanes, cars, birds), with each class containing 6,000 images. The dataset is split into 50,000 training images and 10,000 test images.

\noindent \textbf{CIFAR-100}: The CIFAR-100 dataset~\cite{krizhevsky2010cifar} consists of 60,000 color images of 32x32 pixels, distributed across 100 classes, with each class having 600 images. Similar to CIFAR-10, it is divided into 50,000 training images and 10,000 test images.

% \veb{DVSCIFAR description}

\noindent \textbf{CIFAR10-DVS}: The CIFAR10-DVS~\cite{li2017cifar10} dataset is a neuromorphic vision benchmark derived from the CIFAR-10 dataset, converting 10,000 static images (1,000 per class across 10 categories like airplanes and cars) into event streams using a Dynamic Vision Sensor (DVS). Generated via Repeated Closed-Loop Smooth (RCLS) movement to simulate realistic motion, the dataset captures spatio-temporal intensity changes as asynchronous ON/OFF events (128×128 resolution) with inherent noise (e.g., 60Hz LCD artifacts, later corrected). Designed for moderate complexity between MNIST-DVS and N-Caltech101, it challenges event-driven algorithms, achieving initial low accuracies (~22–29\%) with methods like spiking neural networks and SVM, highlighting its utility for advancing neuromorphic research. Publicly available on Figshare\footnote{\url{https://figshare.com/articles/dataset/CIFAR10-DVS_New/4724671}}, CIFAR10-DVS bridges traditional and event-based vision, fostering innovation in brain-inspired computing and real-world dynamic scene analysis.

\noindent \textbf{ImageNet}: The ImageNet dataset~\cite{deng2009imagenet} comprises 1,281,167 images spanning 1,000 classes in the training set, with a validation set and a test set containing 50,000 and 100,000 images, respectively. Unlike the CIFAR datasets, ImageNet images vary in size and resolution. The validation set is frequently used as the test set in various applications.

\subsection{Configuration and Setups}
\label{appendix:config}

\subsubsection{Ours + QCFS}

\noindent \textbf{CIFAR}: We followed the original paper's training configurations to train ResNet-20 and VGG-16 on CIFAR-100. The Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 was used. The initial learning rate was set to 0.02, with a weight decay of $5 \times 10^{-4}$. A cosine decay scheduler adjusted the learning rate over 300 training epochs. The quantization steps $L$ were set to 8 for ResNet-20 and 4 for VGG-16. All models were trained for 300 epochs.

\noindent \textbf{ImageNet}: We utilized checkpoints for ResNet-34 and VGG-16 from the original paper's GitHub repository. For ImageNet, $L$ was set to 8 and 16 for ResNet-34 and VGG-16, respectively. 

\subsubsection{Ours + RTS}
\noindent \textbf{CIFAR}: We trained models using the recommended settings from the original paper.

\noindent \textbf{ImageNet}: We used pre-trained checkpoints for ResNet-34 and VGG-16 from the original paper's GitHub repository. Subsequently, all ReLU layers were replaced with spiking neuron layers. 


For all datasets, we initialize TPP membrane potential to zero, while in the baselines we do as they propose.

\subsubsection{Ours + SNNC w/o Calibration}

\noindent \textbf{CIFAR}: We adhered to the original paper's configurations to train ResNet-20 and VGG-16 on CIFAR-100. The SGD optimizer with a momentum of 0.9 was used. The initial learning rate was set to 0.01, with a weight decay of $5 \times 10^{-4}$ for models with batch normalization. A cosine decay scheduler adjusted the learning rate over 300 training epochs. All models were trained for 300 epochs with a batch size of 128.

\noindent \textbf{ImageNet}: We used pre-trained checkpoints for ResNet-34 and VGG-16 from the original paper's GitHub repository. Subsequently, all ReLU layers were replaced with our proposed spiking neuron layers.

% \subsubsection{Ours TPD method}

% \noindent \textbf{ImageNet} For ImageNet experiments, we used pre-trained models from various sources. The EfficientNet-B7 model was obtained from the PyTorch \texttt{torchvision.models} library. Models for RegNetv-040, RegNety-160, and RegNety-1280 were sourced from the HuggingFace Timm library with pre-trained weights. Training and testing datasets were configured with respective input sizes: 600x600 pixels for EfficientNet-B7, and varying sizes from 224x224 to 384x384 pixels for the RegNet models. Standard preprocessing steps, including normalization and resizing, were applied. Training and validation batch sizes were set to 16. To accurately replace ReLU activations with spiking neuron equivalents, we profiled the maximum activation values for each ReLU layer. 

% \noindent \textbf{CIFAR-100} To fine-tune EfficientNet-V2-L for CIFAR-100, we used a pre-trained model from the PyTorch \texttt{torchvision.models} library. The final classification layer was replaced with a fully connected layer for 100 classes. Standard normalization and data augmentation techniques were applied to the training data. The SGD optimizer with a learning rate of 0.001 and momentum of 0.9 was used. The model was trained for 10 epochs with batch sizes of 32 for training and 128 for validation. Performance metrics such as accuracy were monitored, and the best model was saved for further analysis.

% After fine-tuning, the activations in EfficientNet-V2-L were converted to spiking neuron activation functions. The fine-tuned model weights were loaded, and specific activation functions, including SiLU and sigmoid, were replaced with their spiking counterparts. Finally, the model's performance on the CIFAR-100 test set was evaluated. 

% \noindent \textbf{CIFAR-10} To fine-tune EfficientNet-V2-L for CIFAR-10, we used a similar approach. The final classification layer was adjusted for 10 classes, and the same normalization and data augmentation methods were applied. The model was trained with the SGD optimizer (learning rate of 0.001, momentum of 0.9) for 10 epochs, using batch sizes of 32 for training and 128 for validation. The performance was evaluated on the CIFAR-10 test set, and the best model was saved. The achieved accuracy is 97.95\%.

\clearpage

\section{Algorithms}
\label{appendix:algo}
The baseline SNN neuron forward function (Algorithm \ref{algo:snn-forward}) initializes the membrane potential to zero and iteratively updates it by adding the layer output at each timestep. Spikes are generated when the membrane potential exceeds a defined threshold, $\theta$, and the potential is reset accordingly. This function captures the core dynamics of spiking neurons. The Shuffle Mode (Algorithm \ref{algo:snn-shuffle-forward}) is an extension of the baseline forward function. After generating the spikes across the simulation length, this mode shuffles the spike train. 

The TPP Mode (Algorithm \ref{algo:snn-tpp-forward}) introduces a probabilistic component to the spike generation process. Instead of a deterministic threshold-based spike generation, it uses a Bernoulli process where the probability of spiking is determined by the current membrane potential relative to the threshold adjusted for the remaining timesteps. 

% The TPD Mode (Algorithm \ref{algo:snn-shuffle-forward}) also modifies the baseline function but remains deterministic. In this mode, the spike generation follows a two-phase process where the spikes are generated and accumulated in a deterministic manner across the timesteps, ensuring consistent spike patterns that are useful for certain types of signal processing within the SNN.



\begin{algorithm}[H]
\caption{SNN Neuron Forward Function and Additional Modes}
\label{algo:snn-forward}
\begin{algorithmic}[1]
\REQUIRE SNN Layer $\mathcal{\ell}$; Input tensor $\mathbf{x}$; Threshold $\theta$; Simulation length $T$.

% Baseline SNN Neuron Forward Function
\FUNCTION{\textsc{BaselineSNN}($\mathcal{\ell}, \mathbf{x}, \theta, T$)}
    \STATE $\mathbf{v} \gets 0$ \COMMENT{Initialize membrane potential}
    \FOR{$t = 1$ \textbf{to} $T$}
        \STATE $\mathbf{v} \gets \mathbf{v} + \mathcal{\ell}(\mathbf{x}(t))$
        \STATE $\mathbf{s} \gets (\mathbf{v} \geq \theta) \times \theta$
        \STATE $\mathbf{v} \gets \mathbf{v} - \mathbf{s}$
        \STATE Store $\mathbf{s}(t)$
    \ENDFOR
    \STATE \textbf{return} $\mathbf{s}$
\ENDFUNCTION

\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{SNN Neuron Forward Function of Shuffle Mode}
\label{algo:snn-shuffle-forward}
\begin{algorithmic}[1]
\REQUIRE SNN Layer $\mathcal{\ell}$; Input tensor $\mathbf{x}$; Threshold $\theta$; Simulation length $T$.

% Shuffle Mode
\FUNCTION{\textsc{ShuffleMode}($\mathcal{\ell}, \mathbf{x}, \theta, T$)}
    \STATE $\mathbf{v} \gets 0$ \COMMENT{Initialize membrane potential}
    \FOR{$t = 1$ \textbf{to} $T$}
        \STATE $\mathbf{v} \gets \mathbf{v} + \mathcal{\ell}(\mathbf{x}(t))$
        \STATE $\mathbf{s} \gets (\mathbf{v} \geq \theta) \times \theta$
        \STATE $\mathbf{v} \gets \mathbf{v} - \mathbf{s}$
        \STATE Store $\mathbf{s}(t)$
    \ENDFOR
    \STATE Shuffle the stored spikes $\mathbf{s}(1), \mathbf{s}(2), \ldots, \mathbf{s}(T)$
    \STATE \textbf{return} shuffled $\mathbf{s}$
\ENDFUNCTION

\end{algorithmic}
\end{algorithm}


\begin{algorithm}[H]
\caption{SNN Neuron Forward Function of TPP Mode}
\label{algo:snn-tpp-forward}
\begin{algorithmic}[1]
\REQUIRE SNN Layer $\mathcal{\ell}$; Input tensor $\mathbf{x}$; Threshold $\theta$; Simulation length $T$.

% Two-Phase Probabilistic Mode (TPP)
\FUNCTION{\textsc{TPPMode}($\mathcal{\ell}, \mathbf{x}, \theta, T$)}
    % \STATE $\mathbf{v} \gets 0$ \COMMENT{Initialize membrane potential}
    \STATE $\mathbf{v} \gets \sum_{t=1}^{T} \mathbf{x}(t)$ \COMMENT{Initialize membrane potential with the sum of inputs}
    \FOR{$t = 1$ \textbf{to} $T$}
        % \STATE $\mathbf{v} \gets \mathbf{v} + \mathcal{\ell}(\mathbf{x}(t))$
        \STATE $\mathbf{p} \gets \text{Clamp}(\mathbf{v} / (\theta \times (T - t + 1)), 0, 1)$
        \STATE $\mathbf{s} \gets \text{Bernoulli}(\mathbf{p}) \times \theta$
        \STATE $\mathbf{v} \gets \mathbf{v} - \mathbf{s}$
        \STATE Store $\mathbf{s}(t)$
    \ENDFOR
    \STATE \textbf{return} $\mathbf{s}$
\ENDFUNCTION
\end{algorithmic}
\end{algorithm}

% \begin{algorithm}[H]
% \caption{SNN Neuron Forward Function of TPD Mode}
% \label{algo:snn-shuffle-forward}
% \begin{algorithmic}[1]
% \REQUIRE SNN Layer $\mathcal{\ell}$; Input tensor $\mathbf{x}$; Threshold $\theta$; Simulation length $T$.
% % Two-Phase Deterministic Mode (TPD)
% \FUNCTION{\textsc{TPDMode}($\mathcal{\ell}, \mathbf{x}, \theta, T$)}
%     % \STATE $\mathbf{v} \gets 0$ \COMMENT{Initialize membrane potential}
%     \STATE $\mathbf{v} \gets \sum_{t=1}^{T} \mathbf{x}(t)$ \COMMENT{Initialize membrane potential with the sum of inputs}    
%     \FOR{$t = 1$ \textbf{to} $T$}
%         % \STATE $\mathbf{v} \gets \mathbf{v} + \mathcal{\ell}(\mathbf{x}(t))$
%         \STATE $\mathbf{s} \gets (\mathbf{v} \geq \theta) \times \theta$
%         \STATE $\mathbf{v} \gets \mathbf{v} - \mathbf{s}$
%         \STATE Store $\mathbf{s}(t)$
%     \ENDFOR
%     \STATE \textbf{return} $\mathbf{s}$
% \ENDFUNCTION
% \end{algorithmic}
% \end{algorithm}

% The Spiking Sigmoid Activation Function (Algorithm \ref{algo:spiking-sigmoid}) modifies the standard sigmoid function to produce spike trains over a given simulation length. The spikes are generated based on the input tensor transformed by the sigmoid function. Similarly, the Spiking SiLU Activation Function (Algorithm \ref{algo:spiking-silu}) adapts the Sigmoid-Weighted Linear Unit (SiLU) activation to SNNs. This function applies the SiLU activation followed by a spiking mechanism controlled by a defined threshold and probability. To address the high latency required in very deep models like RegNetX or EfficientNet, we propose using first-order synaptic models for weights and thresholds. This approach reduces the latency by adjusting the weight and threshold constants over time.

% \begin{algorithm}[htbp]
% \caption{Spiking Sigmoid Activation Function}
% \label{algo:spiking-sigmoid}
% \begin{algorithmic}[1]
% \REQUIRE Input tensor $\mathbf{x}$; Simulation length $T$; Mode $m$; Probability $p$.
% \STATE Initialize $\theta \gets \frac{T}{T+1}$ if $m = 0$ else $\frac{2^T}{2^T+1}$
% \STATE $\mathbf{x} \gets \sigma(\mathbf{x})$ \COMMENT{Apply Sigmoid activation}
% \STATE Initialize $\mathbf{s} \gets []$ \COMMENT{Initialize spike container}
% \IF{$m = $ LONG MODE}
%     \FOR{$t = 1$ \textbf{to} $T$}
%         \STATE $\mathbf{spike} \gets (\mathbf{x} \geq \frac{\theta}{T}) \times \frac{\theta}{T}$
%         \STATE $\mathbf{x} \gets \mathbf{x} - \mathbf{spike}$
%         \STATE Append $\mathbf{spike}$ to $\mathbf{s}$
%     \ENDFOR
% \ELSE
%     \FOR{$t = 1$ \textbf{to} $T$}
%         \STATE $\mathbf{spike} \gets (\mathbf{x} \geq \frac{\theta}{2^t}) \times \frac{\theta}{2^t}$
%         \STATE $\mathbf{x} \gets \mathbf{x} - \mathbf{spike}$
%         \STATE Append $\mathbf{spike}$ to $\mathbf{s}$
%     \ENDFOR
% \ENDIF
% \STATE $\mathbf{s} \gets \sum_{t=1}^{T} \mathbf{s}(t)$
% \STATE \textbf{return} $\mathbf{s}$
% \end{algorithmic}
% \end{algorithm}

% \begin{algorithm}[htbp]
% \caption{Spiking SiLU Activation Function}
% \label{algo:spiking-silu}
% \begin{algorithmic}[1]
% \REQUIRE Input tensor $\mathbf{x}$; Simulation length $T$; Probability $p$; Max activation $m$; Mode $md$.
% \STATE $d \gets 0.27846$
% \STATE $\theta \gets m + d$
% \STATE $\mathbf{x} \gets \sigma(\mathbf{x}) + d$ \COMMENT{Apply SiLU activation and shift by $d$}
% \STATE Initialize $\mathbf{s} \gets []$ \COMMENT{Initialize spike container}
% \IF{$md =$ LONG MODE}
%     \FOR{$t = 1$ \textbf{to} $T$}
%         \STATE $\mathbf{spike} \gets (\mathbf{x} \geq \frac{\theta}{T}) \times \frac{\theta}{T}$
%         \STATE $\mathbf{x} \gets \mathbf{x} - \mathbf{spike}$
%         \STATE Append $\mathbf{spike}$ to $\mathbf{s}$
%     \ENDFOR
% \ELSE
%     \FOR{$t = 1$ \textbf{to} $T$}
%         \STATE $\mathbf{spike} \gets (\mathbf{x} \geq \frac{\theta}{2^t}) \times \frac{\theta}{2^t}$
%         \STATE $\mathbf{x} \gets \mathbf{x} - \mathbf{spike}$
%         \STATE Append $\mathbf{spike}$ to $\mathbf{s}$
%     \ENDFOR
% \ENDIF
% \STATE $\mathbf{s} \gets \sum_{t=1}^{T} \mathbf{s}(t)$
% \STATE \textbf{return} $\mathbf{s} - d$
% \end{algorithmic}
% \end{algorithm}


\section{Additional Experiments}

\subsection{SNNC} % ~\cite{li2021free}

We show extra experiment results about the comparison among permutation method and two-phase probabilistic method. We validated ResNet-20 and VGG-16 on the CIFAR-10/100 dataset , and ResNet-34, VGG-16 and RegNetX-4GF on ImageNet with batch and channel-wise normalization enabled. Using a batch size of 128, the experiment was run five times with different random seeds to ensure reliable and reproducible results.


\begin{table}[H]
\caption{Comparison between our proposed methods and ANN-SNN conversion SNNC method on \textbf{CIFAR-10}. The average accuracy and standard deviation of the TPP method are reported over 5 experiments.}
\label{tab:ann-snn-snnc-cifar10}
\centering
\scalebox{0.8}
{
\begin{threeparttable}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Architecture & Method & ANN & T=1 & T=2 & T=4 & T=8 & T=16 & T=32 & T=64 \\ 
\toprule
\multirow{3}{*}{ResNet-20}
& SNNC-AP~\cite{li2021free} & 96.95 & 51.20 & 66.07 & 83.60 & 92.79 & 95.62 & 96.58 & 96.85 \\
& \textbf{Ours (Permute)} & 96.95 & 34.05 & 61.46 & 90.54 & 95.05 & 96.12 & 96.62 & 96.77 \\
& \textbf{Ours (TPP)} & 96.95 & 10.05 (0.02) & 17.30 (0.52)	& 79.19 (0.67) & 93.72 (0.05) & 95.87 (0.09) & 96.67 (0.04) & 96.80 (0.01) \\
\midrule
\multirow{4}{*}{VGG-16}
& SNNC-AP~\cite{li2021free} & 95.69 & 60.72 & 75.82 & 82.18 & 91.93 & 93.27 & 94.97 & 95.40 \\
& \textbf{Ours (Permute)} & 95.69 & 38.01 & 64.40 & 84.65 & 92.24 & 92.80 & 93.33 & 94.10 \\
& \textbf{Ours (TPP)} & 95.69 & 11.46 (0.35) & 32.24 (1.40) & 86.85 (0.42) & 94.34 (0.12) & 94.86 (0.06) & 95.48 (0.03) & 95.60 (0.04) \\
% & \textbf{Ours (TPD)} & 95.69 & 10.00 & 25.02 & 87.58 & 94.17 & 95.16 & 95.54 & 95.60 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}

% \noindent \textbf{SNNC on CIFAR-100}

\begin{table}[H]
\caption{Comparison between our proposed methods and ANN-SNN conversion SNNC method on \textbf{CIFAR-100}. The average accuracy and standard deviation of the TPP method are reported over 5 experiments.}
\label{tab:ann-snn-snnc-cifar100}
\centering
\scalebox{0.8}
{
\begin{threeparttable}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Architecture & Method & ANN & T=1 & T=2 & T=4 & T=8 & T=16 & T=32 & T=64 \\ 
\toprule
\multirow{3}{*}{ResNet-20}
& SNNC-AP~\cite{li2021free} & 81.89 & 17.91 & 34.08 &	54.78 & 72.28 & 78.57 & 81.20 & 81.95 \\
& \textbf{Ours (Permute)} & 81.89 &	5.64 & 19.54 & 52.46 & 75.21 & 79.76 & 81.12 & 81.52 \\
& \textbf{Ours (TPP)} & 81.89 &	1.94 (0.11) & 5.15 (0.44) & 39.67 (0.99) & 71.05 (0.68) & 78.97 (0.24) & 81.06 (0.05) & 81.61 (0.08) \\
\midrule
\multirow{4}{*}{VGG-16}
& SNNC-AP~\cite{li2021free} & 77.87 & 28.64 & 34.87 & 50.95 & 64.30 & 71.93 & 75.39 & 77.05 \\
& \textbf{Ours (Permute)} & 77.87 & 12.50 & 34.98 & 60.81 & 69.42 & 72.78 & 73.50 & 75.14 \\
& \textbf{Ours (TPP)} & 77.87 & 2.05 (0.27) & 15.90 (0.71) & 59.23 (0.65) & 73.16 (0.17) & 76.05 (0.26) & 77.16 (0.09) & 77.56 (0.13) \\
% & \textbf{Ours (TPD)} & 77.87 & 1.01 & 5.58 & 55.28 & 72.33 & 76.48 & 77.46 & 77.67 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}

% \noindent \textbf{SNNC on ImageNet}

\begin{table}[H]
\caption{Comparison between our proposed methods and ANN-SNN conversion SNNC method on ImageNet. The average accuracy and standard deviation of the TPP method are reported over 5 experiments.}
\label{tab:ann-snn-snnc-imagenet}
\centering
\scalebox{0.8}
{
\begin{threeparttable}
\begin{tabular}{@{}cccccccccc@{}}
\toprule
Architecture & Method & ANN & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
\toprule
\multirow{3}{*}{ResNet-34}
& SNNC-AP~\cite{li2021free} & 75.65 & -- & -- &	-- & 64.54 & 71.12 & 73.45 \\
& \textbf{Ours (Permute)} & 75.65 &	10.51 & 57.57 & 70.94 & 74.00 & 75.06 & 75.47 \\
& \textbf{Ours (TPP)} & 75.65 & 2.69 (0.03) & 49.24 (0.23) & 69.97 (0.10) & 74.07 (0.06) & 75.23 (0.03) & 75.51 (0.05) \\
\midrule
\multirow{4}{*}{VGG-16}
& SNNC-AP~\cite{li2021free} & 75.37 & -- & -- & -- & 63.64 & 70.69 & 73.32 \\
& \textbf{Ours (Permute)} & 75.37 & 38.61 & 67.29 & 73.35 & 74.34 & 74.82 & 75.11 \\
& \textbf{Ours (TPP)} & 75.37 & 54.14 (0.59) & 69.75 (0.27) & 73.44 (0.02) & 74.72 (0.06) & 75.14 (0.02) & 75.25 (0.03) \\
% & \textbf{Ours (TPD)} & 75.37 & 16.4 & 57.52 & 70.95 & 74.06 & 75.01 & 75.31 \\
\midrule
\multirow{4}{*}{RegNetX-4GF}
& SNNC-AP~\cite{li2021free} & 80.02 & -- & -- & -- & 55.70 & 70.96 & 75.78 \\
& \textbf{Ours (Permute)} & 78.45 & -- & -- & 43.45 & 68.12 & 75.63 & 77.63 \\
& \textbf{Ours (TPP)} & 78.45 & -- & -- & 22.71 (2.98) & 66.51 (0.44) & 75.54 (0.07) & 77.83 (0.04) \\
% & \textbf{Ours (TPD)} & 78.45 & -- & -- & 0.11 & 0.58 & 30.24 & 72.11 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}



\subsection{RTS} 

\begin{table}[H]
\caption{Comparison between our proposed methods and ANN-SNN conversion RTS method on CIFAR-10/100 and ImageNet. The average accuracy and standard deviation of the TPP method are reported over 5 experiments.}
\label{tab:ann-snn-rts-cifar10}
\centering
\scalebox{0.75}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Architecture} & \textbf{Method} & \textbf{ANN} & \textbf{T=4} & \textbf{T=8} & \textbf{T=16} & \textbf{T=32} & \textbf{T=64} & \textbf{T=128} \\ 
\toprule
\multirow{6}{*}{CIFAR-10}
& \multirow{3}{*}{VGG-16} 
& RTS\tnote{*}~\cite{DBLP:conf/iclr/DengG21} & 94.99 & 88.64 & 91.67 & 93.64 & 94.50 & 94.76 & 94.91 \\
& & \textbf{Ours (Permute)} & 94.99 & 91.22 & 93.70 & 94.50 & 94.86 & 94.88 & 94.97 \\
& & \textbf{Ours (TPP)} & 94.99 & 91.49 (0.21) & 94.11 (0.09) & 94.72 (0.08) & 94.84 (0.06) & 94.91 (0.02) & 94.98 (0.02) \\
\cline{2-10}
& \multirow{3}{*}{ResNet-20}
& RTS\tnote{*}~\cite{DBLP:conf/iclr/DengG21} & 91.07 & 27.08 & 40.88 & 65.13 & 84.75 & 90.12 & 90.76 \\
& & \textbf{Ours (Permute)} & 91.07 & 68.18 & 86.57 & 90.20 & 90.81 & 91.04 & 90.99 \\
& & \textbf{Ours (TPP)} & 91.07 & 72.87 (0.22) & 88.27 (0.14) & 90.44 (0.08) & 90.86 (0.14) & 90.94 (0.04) & 91.01 (0.03) \\
\cline{1-10}
\multirow{3}{*}{CIFAR-100} 
& \multirow{3}{*}{VGG-16}
& RTS\tnote{$\circ$}~\cite{DBLP:conf/iclr/DengG21} & 76.13 & 23.76 & 43.81 & 56.23 & 67.61 & 73.45 & 75.23 \\
& & \textbf{Ours (Permute)} & 76.13 & 35.31 & 62.84 & 71.20 & 74.34 & 75.53 & 75.92 \\
& & \textbf{Ours (TPP) + RTS} & 76.13 & 37.88 (0.35) & 65.81 (0.27) & 73.05 (0.12) & 75.17 (0.17) & 75.64 (0.12) & 75.90 (0.08) \\
\cline{1-10}
\multirow{3}{*}{ImageNet} 
& \multirow{3}{*}{VGG-16}
& RTS~\cite{DBLP:conf/iclr/DengG21} & 72.16 & -- & -- & 55.80 & 67.73 & 70.97 & 71.89 \\
& & \textbf{Ours (Permute)} & 72.16 & 33.77 & 58.31 & 67.80 & 70.89 & 71.65 & 71.95 \\
& & \textbf{Ours (TPP)} & 72.16 & 30.50 (1.19) & 56.69(0.67) & 67.34 (0.25) & 70.63 (0.11) & 71.75 (0.05) & 72.05 (0.03) \\
\bottomrule
\end{tabular}
% \begin{tablenotes}
% \item[*] Self-implemented.
% \end{tablenotes}
\end{threeparttable}
}
\end{table}

\subsection{QCFS} 

\begin{table}[H]
\caption{Comparison between our proposed methods and ANN-SNN conversion QCFS method on CIFAR-10/100 and ImageNet. The average accuracy and standard deviation of the TPP method are reported over 5 experiments.}
\label{tab:ann-snn-qcfs-cifar10}
\centering
\scalebox{0.8}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{Architecture} & \textbf{Method} & \textbf{ANN} & \textbf{T=4} & \textbf{T=8} & \textbf{T=16} & \textbf{T=32} & \textbf{T=64} \\ 
\toprule
\multirow{6}{*}{CIFAR-10}
& \multirow{3}{*}{VGG-16} 
& QCFS\tnote{*}~\cite{bu2022optimal} & 95.76 & 94.33& 95.21 & 95.65 &95.87  &95.99  \\
& & \textbf{Ours (Permute)}& 95.76 & 95.15& 95.58 & 95.83 & 95.95 & 95.97 &\\
& & \textbf{Ours (TPP)} & 95.76 & 95.28(0.09) & 95.84(0.1) & 95.95(0.05) & 95.98(0.06) & 95.97 (0.03) \\
\cline{2-10}
& \multirow{3}{*}{ResNet-20}
& QCFS~\cite{bu2022optimal}& 92.43 &79.45 & 88.56 & 91.94 & 92.79 & 92.82 \\
& & \textbf{Ours (Permute)}&92.43  &84.85 &91.24  & 92.67 & 92.82 & 92.85 \\
& & \textbf{Ours (TPP)}& 92.43 & 86.24(0.18)&92.08(0.11)  &92.70(0.1)  &92.78(0.04  &92.68(0.06) \\
\cline{1-10}
\multirow{6}{*}{CIFAR-100} 
& \multirow{3}{*}{VGG-16}
& QCFS\tnote{$\circ$}~\cite{bu2022optimal} &76.3  &69.29 & 73.89 & 75.98 & 76.52 & 76.54 \\
& & \textbf{Ours (Permute)} & 76.3 &74.28 & 75.97 &76.54  &76.60  &76.64 \\
& & \textbf{Ours (TPP) } & 76.3 &74.0(0.15) & 76.06(0.08) &76.37(0.1)  &76.55(0.09)  &76.51(0.07) \\
\cline{2-10}
& \multirow{3}{*}{ResNet-20}
& QCFS~\cite{bu2022optimal}&67.0  &27.44 & 49.35 & 63.12 & 66.84 & 67.77 \\
& & \textbf{Ours (Permute)}&67.0  & 45.33&62.81  &66.93  &67.85  & 67.96 \\
& & \textbf{Ours (TPP)}& 67.0 & 47.0(0.2)& 64.66(0.25) & 67.28(0.12) & 67.61(0.1) & 67.77(0.06) \\
\cline{1-10}
\multirow{3}{*}{ImageNet} 
& \multirow{3}{*}{VGG-16}
& QCFS~\cite{bu2022optimal} & 74.29 & -- & -- & 50.97 & 68.47 & 72.85 \\
& & \textbf{Ours (Permute)} & 73.89 & 55.54 & 71.12 & 73.65 & 74.28 & 74.28 \\
& & \textbf{Ours (TPP)} & 74.22 & 68.39 (0.08) & 72.99 (0.05) & 73.98 (0.07) & 74.23 (0.03) & 74.29 (0.00) \\
\bottomrule
\end{tabular}
% \begin{tablenotes}
% \item[*] Self-implemented.
% \end{tablenotes}
\end{threeparttable}
}
\end{table}


% \subsection{Comparison with other types of SNN training methods on CIFAR-10.}


% \begin{table}[!h]
% \small
% \caption{Comparison with other types of SNN training methods on CIFAR-10.}
% \centering
% \resizebox{.85 \textwidth}{!}{
% \begin{threeparttable}
% \begin{tabular}{ccccc}
% \toprule
% \multicolumn{1}{c}{Category}
% &\multicolumn{1}{c}{Methods}
% &\multicolumn{1}{c}{Architecture}
% &\multicolumn{1}{c}{Timesteps}
% &\multicolumn{1}{c}{Acc.} \\
% \midrule
% % category
% % method              % arch   % param % T % acc. 
% \multicolumn{1}{c}{\multirow{6}{*}{{Direct Training}}}
%  &\multicolumn{1}{c}{\multirow{1}{*}{RecDis-SNN~\cite{guo2022recdis}}}  
%  & ResNet-19 & 6 & 95.55 \\
%  & TET~\cite{DBLP:conf/iclr/DengLZG22} & ResNet-19 & 6 & 94.50 \\
%  & TEBN~\cite{DBLP:conf/nips/DuanDCY022} & ResNet-19 & 6 & 95.60 \\ 
%  & GAC-SNN~\cite{DBLP:conf/aaai/QiuZC0DL24} & MS-ResNet-18 & 6 & 96.46 \\ 
%  & Spike-driven Transfromer~\cite{DBLP:conf/nips/YaoHZ000L23} & Spike-driven Transformer & 4 & 95.60 \\
%  & SEENN-I~\cite{DBLP:conf/nips/LiGKP23} & ResNet-19 & 1.34\tnote{*} & 96.44 \\
% \midrule
% \multicolumn{1}{c}{\multirow{8}{*}{{ANN-SNN}}} % 15 ->
%  &\multicolumn{1}{c}{\multirow{2}{*}{RTS~\cite{DBLP:conf/iclr/DengG21}}}
%  & ResNet-20 & 128 & 93.56 \\
%  && VGG-16 & 128 & 92.24 \\
%  &\multicolumn{1}{c}{\multirow{2}{*}{QCFS~\cite{DBLP:conf/iclr/BuFDDY022}}} 
%  & ResNet-20 & 256 & 95.41 \\
%  && VGG-16 & 256 & 95.79 \\
%  & Burst~\cite{DBLP:conf/ijcai/Li022} & ResNet-20 & 256 & 95.61 \\
%  &\multicolumn{1}{c}{\multirow{2}{*}{STA~\cite{jiangspatio24}}}
%  & ViT-B/32                                            & 128 & 95.82 \\
%  && ViT-B/32                                            & 256 & 95.68 \\
%  \cline{2-5}
%  &\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Ours (DFT)}}} 
%   &\multicolumn{1}{c}{\multirow{2}{*}{EfficientNet-V2-L}} 
%   &   16 & 96.24 \\
%   &&& 32 & \gc{97.94} \\  
% \bottomrule
% \end{tabular}
% \begin{tablenotes}
% \item[*] The average number of timesteps during inference on the test dataset.
% \end{tablenotes}
% \end{threeparttable}
% }
% \label{tab:other-snn-training-methods-cifar10}
% \end{table}


\clearpage

\subsection{Spiking activity}
\label{appendix:firing-counts}

The percentage difference between the baseline and our method in TPP mode is calculated as follows: $\text{Percentage Difference} = \frac{\text{Ours} - \text{Baseline}}{\text{Baseline}} \times 100 $.


\begin{figure*}[!ht]
\centering
% \subfigbottomskip=2pt
% \subfigcapskip=-5pt
\subfigure[RTS]{
    \includegraphics[width=0.32\linewidth]{figures/vgg16-cifar100-rts.pdf}}
\subfigure[QCFS]{
    \includegraphics[width=0.32\linewidth]{figures/vgg16-cifar100-qcfs.pdf}}
\subfigure[SNNC]{
    \includegraphics[width=0.32\linewidth]{figures/vgg16-cifar100-snnc.pdf}}    
% \vskip -0.1in
\caption{Spike counts of VGG-16 on CIFAR-100 after different timesteps (T). Note: The bar height from bottom indicates the spike counts after each timestep T, and the color of longer Ts is overlaid by shorter Ts.}
 \label{fig:spike-counts-all}
\end{figure*}


% \begin{figure}[!htbp]
%     \centering
%     \begin{subfigure}
%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-rts.pdf}
%     \caption{RTS}
%     \label{fig:first}
%     \end{subfigure}
    
%     \vspace{0.5cm} % Adjust spacing between figures
%     \begin{subfigure}
%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-qcfs.pdf}
%     \caption{QCFS}
%     \label{fig:second}
%     \end{subfigure}
%     \vspace{0.5cm} % Adjust spacing between figures
%     \begin{subfigure}        
%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-snnc.pdf}
%     \caption{Spike counts of VGG-16 on CIFAR-100 after different timesteps (T). Note: The bar height from bottom indicates the spike counts after each timestep T, and the color of longer Ts is overlaid by shorter Ts.}
%     \end{subfigure}
%   \label{fig:spike-counts}
% \end{figure}

% \begin{figure}[!htbp]
%     \centering
%     \begin{subfigure}
%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-rts.pdf}
%     \caption{RTS}
%     \label{fig:first}
    
%     \vspace{0.5cm} % Adjust spacing between figures

%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-qcfs.pdf}
%     \caption{QCFS}
%     \label{fig:second}
    
%     \vspace{0.5cm} % Adjust spacing between figures

%     \includegraphics[width=0.8\textwidth]{figures/vgg16-cifar100-snnc.pdf}
%     \caption{Spike counts of VGG-16 on CIFAR-100 after different timesteps (T). Note: The bar height from bottom indicates the spike counts after each timestep T, and the color of longer Ts is overlaid by shorter Ts.}
%   \label{fig:spike-counts}
% \end{figure}


















\begin{table}[!htbp]
\caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for VGG-16 on CIFAR-100 using QCFS.}
\label{tab:spike-counts-cifar100-vgg16-cifar100}
\centering
\scalebox{0.85}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
\toprule
1 & 1.073 & 0.528 & 0.261 & 0.136 & 0.065 & 0.033 \\
\midrule
2 & 2.629 & 1.022 & 0.438 & 0.206 & 0.102 & 0.050 \\
\midrule
3 & 0.049 & 0.230 & 0.185 & 0.109 & 0.056 & 0.028 \\
\midrule
4 & -0.867 & -0.664 & -0.419 & -0.228 & -0.118 & -0.060 \\
\midrule
5 & 0.073 & 0.515 & 0.350 & 0.182 & 0.090 & 0.044 \\
\midrule
6 & 0.701 & 0.010 & -0.098 & -0.074 & -0.041 & -0.021 \\
\midrule
7 & -1.071 & -0.865 & -0.470 & -0.246 & -0.122 & -0.063 \\
\midrule
8 & 1.009 & 1.193 & 0.731 & 0.385 & 0.196 & 0.096 \\
\midrule
9 & 0.504 & 0.417 & 0.205 & 0.108 & 0.051 & 0.024 \\
\midrule
10 & -0.112 & 0.842 & 0.647 & 0.375 & 0.198 & 0.100 \\
\midrule
11 & 2.071 & 2.438 & 1.614 & 0.898 & 0.465 & 0.235 \\
\midrule
12 & 0.797 & 0.943 & 0.756 & 0.461 & 0.247 & 0.127 \\
\midrule
13 & 4.503 & 2.156 & 1.209 & 0.655 & 0.343 & 0.171 \\
\midrule
14 & 25.898 & 13.883 & 7.770 & 3.852 & 1.887 & 0.936 \\
\midrule
15 & 33.585 & 16.864 & 8.945 & 4.474 & 2.227 & 1.108 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}



% REMOVE:

% \begin{table}[htbp]
% \caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for VGG-16 on CIFAR-100 using RTS.}
% \label{tab:spike-counts-cifar100}
% \centering
% \scalebox{0.75}
% {
% \begin{threeparttable}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
% \toprule
% 1 & 22.693 & 9.785 & 4.639 & 2.311 & 1.160 & 0.584 \\
% \midrule
% 2 & -12.736 & -9.338 & -4.570 & -2.247 & -1.131 & -0.566 \\
% \midrule
% 3 & 55.214 & 32.863 & 16.273 & 7.936 & 3.915 & 1.939 \\
% \midrule
% 4 & -0.879 & -8.136 & -6.222 & -3.544 & -1.850 & -0.940 \\
% \midrule
% 5 & 5.136 & 6.593 & 5.243 & 3.116 & 1.650 & 0.841 \\
% \midrule
% 6 & 11.068 & 6.270 & 2.621 & 1.009 & 0.416 & 0.192 \\
% \midrule
% 7 & 16.390 & 8.539 & 3.952 & 1.786 & 0.834 & 0.404 \\
% \midrule
% 8 & 28.752 & 19.346 & 10.815 & 5.552 & 2.790 & 1.399 \\
% \midrule
% 9 & 28.204 & 17.877 & 10.146 & 5.246 & 2.645 & 1.327 \\
% \midrule
% 10 & 31.209 & 20.807 & 12.730 & 6.839 & 3.521 & 1.780 \\
% \midrule
% 11 & 26.458 & 23.820 & 15.663 & 8.635 & 4.500 & 2.290 \\
% \midrule
% 12 & 3.109 & 3.345 & 1.670 & 0.679 & 0.303 & 0.141 \\
% \midrule
% 13 & 5.622 & 1.112 & -0.407 & -0.409 & -0.238 & -0.119 \\
% \midrule
% 14 & 24.492 & 11.369 & 5.182 & 2.522 & 1.268 & 0.644 \\
% \midrule
% 15 & 25.599 & 12.071 & 5.349 & 2.501 & 1.220 & 0.613 \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% \end{table}




% \begin{table}[htbp]
% \caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for VGG-16 on CIFAR-100 using SNNC.}
% \label{tab:spike-counts-cifar100}
% \centering
% \scalebox{0.75}
% {
% \begin{threeparttable}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
% \toprule
% 1 & 10.697 & 6.496 & 4.279 & 2.631 & 1.509 & 0.779 \\
% \midrule
% 2 & -20.469 & -22.048 & -16.936 & -11.948 & -7.195 & -3.949 \\
% \midrule
% 3 & -11.107 & -9.095 & -4.363 & -3.514 & -1.407 & -0.529 \\
% \midrule
% 4 & 27.428 & 24.277 & 16.612 & 12.599 & 7.329 & 3.421 \\
% \midrule
% 5 & 2.546 & 4.040 & 4.261 & 2.237 & 0.788 & 0.426 \\
% \midrule
% 6 & 23.288 & 24.507 & 8.126 & 7.021 & 6.225 & 3.313 \\
% \midrule
% 7 & 49.113 & 37.484 & 32.735 & 23.705 & 13.602 & 8.298 \\
% \midrule
% 8 & 11.001 & 19.927 & 9.762 & 9.577 & 7.019 & 4.871 \\
% \midrule
% 9 & 30.676 & 35.836 & 27.881 & 22.820 & 14.760 & 8.530 \\
% \midrule
% 10 & 68.423 & 54.246 & 46.202 & 32.534 & 19.744 & 9.833 \\
% \midrule
% 11 & -17.972 & -19.279 & -9.419 & -8.396 & -6.448 & -4.106 \\
% \midrule
% 12 & -23.264 & -14.900 & -15.433 & -7.045 & -0.254 & 2.443 \\
% \midrule
% 13 & 0.855 & 13.670 & 4.708 & 7.580 & 9.435 & 7.776 \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% \end{table}



\begin{table}[!htbp]
\caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for ResNet-34 on ImageNet using QCFS.}
\label{tab:spike-counts-cifar100-resnet34-imagenet}
\centering
\scalebox{0.85}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
\toprule
1 & 0.587 & 0.306 & 0.149 & 0.079 & 0.036 & 0.018 \\
\midrule
2 & -0.921 & -0.435 & -0.212 & -0.108 & -0.053 & -0.025 \\
\midrule
3 & 0.353 & 0.189 & 0.082 & 0.036 & 0.019 & 0.010 \\
\midrule
4 & -2.786 & -1.583 & -0.920 & -0.506 & -0.270 & -0.141 \\
\midrule
5 & 0.469 & 0.277 & -0.107 & -0.020 & -0.019 & -0.011 \\
\midrule
6 & -3.955 & -1.865 & -0.705 & -0.344 & -0.166 & -0.086 \\
\midrule
7 & -0.381 & 0.321 & -0.090 & -0.031 & -0.020 & -0.013 \\
\midrule
8 & 6.615 & 3.261 & 1.494 & 0.628 & 0.290 & 0.131 \\
\midrule
9 & -5.116 & -3.006 & -1.555 & -0.794 & -0.391 & -0.195 \\
\midrule
10 & -2.938 & 3.431 & 3.096 & 1.794 & 0.975 & 0.498 \\
\midrule
11 & 1.184 & 0.466 & 0.359 & 0.102 & 0.053 & 0.022 \\
\midrule
12 & -17.739 & -7.302 & -1.788 & -0.609 & -0.270 & -0.132 \\
\midrule
13 & 0.105 & -0.138 & -0.287 & -0.292 & -0.166 & -0.087 \\
\midrule
14 & -8.597 & -2.626 & 0.006 & 0.327 & 0.289 & 0.140 \\
\midrule
15 & -0.522 & -0.214 & -0.273 & -0.299 & -0.173 & -0.094 \\
\midrule
16 & -11.196 & -5.194 & -1.990 & -0.813 & -0.405 & -0.217 \\
\midrule
17 & -3.828 & -1.192 & -0.320 & -0.192 & -0.105 & -0.058 \\
\midrule
18 & -6.869 & -2.392 & -0.644 & 0.007 & -0.002 & 0.001 \\
\midrule
19 & 0.092 & -0.299 & -0.181 & -0.138 & -0.074 & -0.035 \\
\midrule
20 & -5.639 & -0.308 & 0.923 & 0.796 & 0.448 & 0.234 \\
\midrule
21 & 0.399 & -0.968 & -0.796 & -0.509 & -0.275 & -0.145 \\
\midrule
22 & -4.474 & 3.712 & 4.440 & 3.033 & 1.700 & 0.880 \\
\midrule
23 & 0.456 & -0.901 & -0.703 & -0.533 & -0.281 & -0.145 \\
\midrule
24 & -5.863 & 4.241 & 5.617 & 3.797 & 2.090 & 1.074 \\
\midrule
25 & 1.433 & -0.464 & -0.774 & -0.632 & -0.347 & -0.182 \\
\midrule
26 & -5.034 & 4.908 & 6.328 & 4.362 & 2.459 & 1.271 \\
\midrule
27 & 0.661 & -0.914 & -1.156 & -0.931 & -0.530 & -0.284 \\
\midrule
28 & -15.667 & 4.763 & 9.616 & 6.975 & 4.062 & 2.096 \\
\midrule
29 & -9.747 & 1.663 & 3.836 & 2.455 & 1.384 & 0.673 \\
\midrule
30 & -0.151 & 16.639 & 15.387 & 9.638 & 5.334 & 2.769 \\
\midrule
31 & -5.403 & 0.917 & 1.957 & 1.555 & 1.009 & 0.574 \\
\midrule
32 & 17.796 & 6.777 & 3.728 & 3.231 & 2.507 & 1.583 \\
\midrule
33 & -4.935 & -2.141 & 2.055 & 2.931 & 2.395 & 1.561 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}



\newpage
\clearpage


\begin{table}[!htbp]
\caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for VGG-16 on ImageNet using QCFS.}
\label{tab:spike-counts-cifar100-vgg16-imagenet}
\centering
\scalebox{0.85}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccc@{}}
\toprule
Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
\toprule
1 & 5.487 & 2.776 & 1.444 & 0.712 & 0.363 & 0.179 \\
\midrule
2 & 0.418 & 0.173 & -0.005 & 0.007 & 0.007 & 0.006 \\
\midrule
3 & -2.375 & -0.883 & -0.351 & -0.128 & -0.062 & -0.031 \\
\midrule
4 & 6.170 & 2.181 & 0.627 & 0.121 & 0.024 & -0.002 \\
\midrule
5 & -3.338 & -0.318 & 0.327 & 0.306 & 0.173 & 0.097 \\
\midrule
6 & 7.036 & 2.769 & 0.993 & 0.385 & 0.173 & 0.078 \\
\midrule
7 & -5.722 & -3.482 & -1.661 & -0.800 & -0.400 & -0.200 \\
\midrule
8 & -6.155 & 0.310 & 1.411 & 0.955 & 0.507 & 0.269 \\
\midrule
9 & -0.718 & 1.172 & 0.725 & 0.337 & 0.162 & 0.081 \\
\midrule
10 & -12.833 & -9.060 & -4.882 & -2.359 & -1.145 & -0.564 \\
\midrule
11 & 12.966 & 11.241 & 7.718 & 4.443 & 2.344 & 1.188 \\
\midrule
12 & -11.194 & -14.874 & -12.032 & -7.889 & -4.437 & -2.395 \\
\midrule
13 & -37.388 & -30.782 & -20.701 & -12.296 & -6.527 & -3.377 \\
\midrule
14 & -23.619 & -12.312 & -3.929 & -0.233 & 0.585 & 0.382 \\
\midrule
15 & -10.988 & -18.476 & -13.953 & -7.904 & -4.091 & -2.015 \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}



% \begin{table}[htbp]
% \caption{Comparison of firing counts percentage difference between the baseline and our proposed TPP method for VGG-16 on ImageNet using SNNC.}
% \label{tab:spike-counts-cifar100}
% \centering
% \scalebox{0.75}
% {
% \begin{threeparttable}
% \begin{tabular}{@{}ccccccc@{}}
% \toprule
% Layer & T=4 & T=8 & T=16 & T=32 & T=64 & T=128 \\ 
% \toprule
% 1 & 2.917 & 2.251 & 1.957 & 1.284 & 0.848 & 0.495 \\
% \midrule
% 2 & -12.318 & -8.427 & -7.360 & -5.294 & -3.220 & -1.766 \\
% \midrule
% 3 & -11.742 & -10.877 & -6.427 & -1.745 & 0.135 & 0.218 \\
% \midrule
% 4 & 9.294 & 3.265 & 4.267 & 0.978 & -0.174 & -0.247 \\
% \midrule
% 5 & -4.037 & 2.576 & 1.391 & 0.820 & 0.207 & 0.320 \\
% \midrule
% 6 & -6.549 & -6.965 & -3.684 & -2.016 & -0.681 & 0.005 \\
% \midrule
% 7 & 39.709 & 38.353 & 29.181 & 20.251 & 14.024 & 7.889 \\
% \midrule
% 8 & 12.685 & 14.771 & 11.521 & 8.170 & 4.972 & 3.033 \\
% \midrule
% 9 & 6.072 & 11.185 & 9.021 & 8.981 & 6.778 & 4.692 \\
% \midrule
% 10 & 43.126 & 59.868 & 39.143 & 26.498 & 15.063 & 9.531 \\
% \midrule
% 11 & 16.175 & 38.376 & 30.769 & 19.448 & 13.838 & 8.475 \\
% \midrule
% 12 & -36.152 & -33.784 & -36.928 & -26.449 & -23.619 & -14.705 \\
% \midrule
% 13 & 38.133 & 137.594 & 100.379 & 101.332 & 71.166 & 44.560 \\
% \midrule
% 14 & -9.792 & -8.682 & -12.950 & -7.936 & -6.394 & -4.703 \\
% \midrule
% 15 & 82.055 & 24.074 & 11.915 & 0.547 & -4.589 & -5.356 \\
% \bottomrule
% \end{tabular}
% \end{threeparttable}
% }
% \end{table}

% \newpage
% \clearpage




\newpage
\section{Membrane potential Distribution}
\label{appendix:mem-pot-dist}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/combined_ours.pdf}
\caption{The membrane potential distributions of the first channel (randomly selected) across three modes (baseline, shuffle, and probabilistic) in VGG-16 on CIFAR-100. For comparison, the first two timesteps (t=1, t=2) from a total of eight timesteps (T=8) are selected for each mode. The baseline mode (blue) achieves an accuracy of 24.22\%, while the shuffle mode (light green) improves accuracy to 70.54\%, and the probabilistic mode (dark orange) further increases accuracy to 73.42\%. The distributions are shown before firing, and the red dashed line indicates the threshold voltage (Vth) for the layer.}
\label{fig-original-prob-membrane-pot}
\end{figure}


\newpage
\clearpage


\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/combined_baseline.pdf}
\caption{The membrane potential of the first channel (randomly selected) from layer 1 in SNNC baseline mode using VGG-16 on CIFAR-100 achieves an accuracy of 24.22\% before firing. }
\label{fig-baseline-membrane-pot}
\end{figure}

The first two timesteps exhibit an abnormal distribution compared to those at t=4 to t=8. This discrepancy arises from the initially incorrect membrane potential before firing, which affects the firing rate and propagates errors layer by layer. A detailed quantifiable error analysis is provided in Appendix Section~\ref{appendix-sec:conversion_error_analysis}. Furthermore, as shown in Figure~\ref{fig-shuffle-membrane-pot}, shuffling the membrane potential effectively alleviates this effect.

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/combined_shuffle.pdf}
\caption{Membrane potential of the first channel (randomly selected) before firing in SNNC shuffle mode using VGG-16 on CIFAR-100. The achieved accuracy is 70.54\%, indicating the impact of random spike rearrangement.}
\label{fig-shuffle-membrane-pot}
\end{figure}




\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/combined_prob.pdf}
\caption{Membrane potential of the first channel (randomly selected) before firing in SNNC probabilistic mode using VGG-16 on CIFAR-100. The accuracy increases to 73.42\%.}
\label{fig-prob-membrane-pot}
\end{figure}






%%% todo: It would be great if we could have such a plot for SNNC, because there the effects could be more visible than in QCFS, 
%%% Figure 11 Accuracy of the model when a permutation is applied on a single layer.




\newpage
\clearpage

\section{Permutations and ANN-SNN conversion}\label{app permutations}

\textbf{Heuristics behind permutations} We come back to the original motivation, and the mysterious effect of temporal misalignment. To this end, we notice that permutations may act as a ``uniformizer'' of the inputs to the spiking neuron, which is highly related to notions of phase lag or unevenness of the inputs (see \cite{li2022bsnn} and \cite{bu2022optimal}, respectively). 
\begin{restatable}{theorem}{exppermutations}\label{thm permutations expected} Suppose we have $N$ spiking neurons that produced spike trains $s_i[1], s_i[2],\dots, s_i[T]$, $i=1,\dots,N$. Furthermore, suppose that these spike trains are modulated with weights $w_1,\dots,w_N$, and as such give input to a neuron (say from the following layer) in the form $x[t]=\sum w_i s_i[t]$, for $t=1,\dots, T$. For a given permutation $\pi = (\pi_1,\dots,\pi_N)$, 
let $\pi s_i$ denote the permutation of the spike train $s_i$. Then, for every $t_1,t_2\in \{1,2,\dots, T\}$, 
$$
E_{\pi}[\sum w_i\pi s_i[t_1]]=E_{\pi}[\sum w_i\pi s_i[t_2]].
$$
\end{restatable}
\begin{proof}
It is enough to prove that for each $i=1,\dots,N$, 
\begin{equation}\label{eq exp perm}
    E_\pi[s_i[t_1]] = E_\pi[s_i[t_2]].
\end{equation}
Let $A(t_i)$ be the cardinality of the set of all the permutations that end up with a spike in step $t_i$, and note that the probability of having a spike at $t_i$ is then $\frac{A(t_i)}{T!}$. But, for each permutation that ends up with a spike at $t_i$, one can find a permutation that ends up with a spike at $t_2$ (by simply applying a cyclic permutation) and moreover this correspondence is bijective. In particular $A(t_i)$ is independent of $i$. The equation \eqref{eq exp perm} and the statement follow.
\end{proof}
The previous result deals with the expected outputs with respect to the permutations. When it comes to the action of a single permutation, we make the following observation. The effect of a single permutation is mostly visible on spike trains that have a \textbf{low number of spikes}. This, in turn, is related to the situation where the input to the neuron is low throughout time, and it takes longer for a neuron to accumulate enough potential in order to spike, hence the neuron spikes at a later time during latency. In this case, a single permutation of the output spike(s) actually move the spikes forward in time (in general) and as such contributes to the elimination of the unevenness error, which appears when the input to a neuron in the beginning is higher than the average input through time (hence, the neuron produces superfluous spikes in the beginning, which shouldn't be the case).

\begin{table}[H]
\caption{Recorded accuracy after $t\leq T$ time steps, when the baseline model is "permuted" in latency $T$. Setting is VGG-16, CIFAR-100.}
\label{tab: stabilization}
\centering
\scalebox{0.85}
{
\begin{threeparttable}
\begin{tabular}{@{}ccccccccc@{}}
\toprule
 Method & ANN & t=1 & t=2 & t=4 & t=8 & t=16 & t=32  \\ 
\toprule
% \multirow{3}{*}{ResNet-20}
%  SNNC-AP~\cite{li2021free} &  &  &  &	 &  &  &  &  \\
%  \textbf{Ours (Permute)} & T=4 &  &  &	 &  &  &  & \\
% \textbf{Ours ((Permute)} & T=8 &  &  &	 &  &  &  & \\
%  \textbf{Ours ((Permute)} & T=16  &  &  &	 &  &  &  & \\
%  \textbf{Ours (Permute)} & T=32 &  &  &	 &  &  &  & \\
% \midrule
 QCFS~\cite{bu2022optimal} &  & 49.09 & 63.22 & 69.29 & 73.89 & 75.98 & 76.52 &  \\
 \textbf{Ours (Permute)} & T=4 & 68.11 & 71.91 & 74.2 &  &  &  & \\
 \textbf{Ours (Permute)} & T=8 & 71.76 & 74.11 & 75.53 & 75.86 &  &  & \\
 \textbf{Ours (Permute)} & T=16  & 72.75 & 74.27 &	75.63 & 76.0 & 76.39 &  & \\
 \textbf{Ours ((Permute)} & T=32 & 73.15 & 75.23 & 75.74 & 76.27 & 76.59 & 76.52 & \\
 % \textbf{Ours ((Permute)} & T=64  &  &  &	 &  &  &  & \\
\midrule
 RTS~\cite{deng2021optimal} &  & 1.0 & 1.03 & 23.76 & 43.81 & 56.23 & 67.61 &  \\
 \textbf{Ours (Permute)} & T=4 & 22.9 & 30.78 &	34.54 &  &  &  & \\
\textbf{Ours ((Permute)} & T=8 & 45.11 & 52.7 &	59.2 & 62.58 &  &  & \\
 \textbf{Ours ((Permute)} & T=16  & 54.58 & 64.37 &	68.6 & 70.8 & 71.79 &  & \\
 \textbf{Ours (Permute)} & T=32 & 62.76 & 69.12 & 71.76 & 73.31 & 74.09 & 74.6 & \\
\bottomrule
\end{tabular}
\end{threeparttable}
}
\end{table}

% \wxf{I prefer removing "Comments:" and write like the main section. It is not formal.}
\textbf{Remarks:} 
\begin{enumerate}
    \item In Table \ref{tab: stabilization} we combine permutations with baseline models in fixed latency $T$. Afterwards, we record the accuracies of such "permuted" model for lower latencies $t$. We can notice a sharp increase in the accuracies compared to the baselines, and in particular, the variance in accuracies across $t$ is reduced. 
    \item \textbf{Baseline analysis:} 
    \begin{enumerate}
    \item SNN models converted from a pretrained ANN aim to approximate the ANN activation values with firing rates. In particular, in lower time steps, the approximation is too coarse as the firing rate has only few possibilities to use to approximate the ANN (continuous) values. For example, in $T=1$, the baselines are attempting to approximate ANN activations with binary values $0$ and $\theta$.
    \item Moreover, at each spiking layer, the spiking neurons at early time steps, use only the outputs of the previous spiking layer from the same, early, time steps. As this information is already too coarse, \textbf{the approximation error accumulates throughout the network}, finally yielding in models that are underperfoming in low latencies.
    \item With longer latencies, the model is using more spikes and is able to approximate the ANN values more accurately, and to correct the results from the first time steps. 
    \end{enumerate}
    \item \textbf{Effect of permutations:}
    \begin{enumerate}
        \item When performing permutations on spike trains after spiking layers in the baseline models, the input to the next spiking layer in lower time steps, \textbf{no longer depends only on the outputs of the previous layer in the same lower time steps, but it depends on the outputs in all time steps $T$}. 
        \item In particular, when spiking layer is producing spikes at time step $t=1$, it does so "taking into account" (via permutation) outputs at all the time steps from the previous spiking layer.
        \item As a way of example, consider two spiking neurons $N_1$ and $N_2$, where $N_2$ receives the weighted input from $N_1$. If a spiking neuron $N_1$ in one layer has produced spike train $s = [1,0,0,0]$, in approximating ANN value of $.25$, then a spiking neuron $N_2$ at the first time step will use 1 as the approximation and will receive the input $W\cdot 1$ from neuron $N_1$. However, after a generic permutation of $s$, the probability of having zero at the first time step of output of neuron $N_1$ is $\frac{3}{4}$ (as oppose to having 1 with probability $\frac{1}{4}$), and at the first time step neuron $N2$ will most likely receive the input $W\cdot 0=0$ from neuron $N_1$, which is a rather better approximation for $W\cdot .25$ than $W$ itself.
        \item This property of receiving input at lower $t$ but taking into account the previous layer spike outputs at all the time steps is not only exclusive to lower $t$. Indeed, at every time step $t\leq T$, the input at a spiking layer is formed by taking into account spiking train outputs from the previous layer at all the time steps, but having already accounted for for the observed input at the first $t<1$ steps. 
        \item In general, the permutations overall increase the performance of the baselines because the spike trains are "uniformized" in accordance to their rate, and the accumulation error is reduced. If a layer $l$ has produced spike outputs that well approximate the $l$ layer in ANN, then, after a generic permutation, at each time step starting with the first, the next layer is receiving the most likely binary approximation of those rates. 
        \item This is nothing but Theorem \ref{thm permutations expected} in visible action. 
        \item Besides Table \ref{tab: stabilization}, we provide further evidence on how permutation affect the baselines through the observed membrane potential in the following sections. 
    \end{enumerate}
\end{enumerate}

\subsection{The effect of permutations on performance: Further experiments}

% \wxf{make the figure a little bit smaller.}

% {'(0, 1, 2, 3)': 70.36, '(0, 1, 3, 2)': 70.84, '(0, 2, 1, 3)': 71.26, '(0, 2, 3, 1)': 71.9, '(0, 3, 1, 2)': 71.6, '(0, 3, 2, 1)': 71.99, '(1, 0, 2, 3)': 72.99, '(1, 0, 3, 2)': 73.03, '(1, 2, 0, 3)': 74.18, '(1, 2, 3, 0)': 75.2, '(1, 3, 0, 2)': 75.1, '(1, 3, 2, 0)': 74.97, '(2, 0, 1, 3)': 74.7, '(2, 0, 3, 1)': 74.76, '(2, 1, 0, 3)': 74.62, '(2, 1, 3, 0)': 74.53, '(2, 3, 0, 1)': 74.55, '(2, 3, 1, 0)': 74.99, '(3, 0, 1, 2)': 75.35, '(3, 0, 2, 1)': 75.26, '(3, 1, 0, 2)': 74.62, '(3, 1, 2, 0)': 74.97, '(3, 2, 0, 1)': 75.3, '(3, 2, 1, 0)': 75.0}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/order_acc.pdf}
\caption{Accuracy comparison for all \( T! \) permutations of input order over \( T=4 \) time steps using QCFS with VGG-16 on CIFAR-100. Results of permuted orders outperform the original, non-permuted order $(0,1,2,3)$. Baseline accuracy is $69.31\%$, The ANN accuracy is $76.21\%$.}
\label{fig-order-acc-1}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=\textwidth]{./figures/plot_fix_perm.pdf}
\caption{Accuracy comparison for permutations over 8 time steps, fixing given pairs of time steps. Setting is VGG-16, CIFAR-100. The baseline (QCFS) accuracy is $73.89\%$, ANN accuracy is $76.21\%$.}
\label{fig-order-acc-2}
\end{figure}

% \wxf{make the following two figures into one figure with two subfigures span across the row.}

\begin{figure}[!htbp]
\centering
\includegraphics[width=.7\textwidth]{./figures/single_perm.pdf}
\caption{Accuracy of the model when a permutation is applied on a single layer using QCFS baseline. Setting is VGG-16, $T=4$, CIFAR-100. Baseline accuracy is $69.31\%$, ANN accuracy is $76.31\%$}
\label{fig-order-acc-3}
\end{figure}

\begin{figure}[!htbp]
\centering
\includegraphics[width=.7\textwidth]{./figures/accuracy_per_layer.pdf}
\caption{Accuracy of the model when a permutation is applied on a single layer using SNNC baseline. Setting is VGG-16, $T=8$, CIFAR-100. Baseline accuracy without calibration is $24.22\%$, ANN accuracy is $77.87\%$}
\label{fig-order-acc-4}
\end{figure}



% \newpage
% \section{Conversion error analysis} \label{appendix-sec:conversion_error_analysis}
% For this section, we use the terminology of \cite{bu2022optimal} for the classification of conversion errors. We shortly recall three classes and we refer the reader to the original paper for more details: 
% \begin{enumerate}
%     \item \textbf{Clipping error}: When performing the ANN-SNN conversion, one uses some heuristics to set up the threshold, based on the corresponding distribution of the activation values. In particular, if $\mathcal{A}$ is the ANN activation, and $\theta$ is the set threshold for this particular layer, then the clipping error manifest itself in approximating $\mathcal{A}(\cdot)$ with $\min(\mathcal{A}(\cdot),\theta)$ (which is the maximum output of the spiking layer (before normalization)). 
%     \item \textbf{Quantization error}: As the spiking neurons produce discrete spikes (values $0$ or $\theta$ (before normalization)), the quantization error manifest itself in using $\frac{\theta}{T}\cdot\max(0,\lfloor\frac{T}{\theta}\cdot x\rfloor)$ (which is tentative output of the spiking neuron) to approximate $\mathcal{A}(x)$.
%     \item \textbf{The unevenness error}: This error potentially occur due to the non-uniformity of the input to the spiking neurons. In particular, it can happen that the neurons receive streams of positive input during certain time period, while receiving stream of negative input during another period. Ideally, two streams should cancel each other parts of each other, but, due to their temporal mismatch, the neurons fire superfluous spikes, or they do not fire enough spikes as they theoretically should. 
% \end{enumerate}
% To study what is the main source of errors when performing ANN-SNN conversion with TPP neurons, we consider in detail the situation of a single layer of ANN neurons, and corresponding layer of SNN TPP neurons. 
% For a function $f$ and constant $c$, we denote by $f_c$ the clipping of $f$ by $c$, that is $f_c(x)=\min(f(x),c)$. For example, $\relu_\theta(x):= \min(\relu(x),\theta)=\min(\max(0,x),\theta)$. 
% \wxf{rewrite in another way more like paper but not in this way like rebuttal and reply; The theorem is precious, let's move it to the main section of the paper. Theorem and math are always elegant and appreciated!}
% \begin{theorem}
%     Let $X^{(l)}$ be the input of the ANN layer with \relu activation and suppose that, during the accumulation phase, the corresponding SNN layer of TPP neurons accumulated $T\cdot X^{(l)}$ quantity of voltage. 
%     \begin{itemize}
%         \item[(a)] For every time step $t=1,\dots,T$, we have 
%         \begin{equation}
%             \frac{\theta}{t}\cdot \mathbb{E}\left[\sum_{i=1}^t s^{(l)}[i]\right] = \relu_\theta(X^{(l)}).
%         \end{equation}
%         \item[(b)] Suppose that for some $t=1,\dots,T$, the TPP layer produced $s^{(l)}[1],\dots,s^{(l)}[t-1]$ vector spike trains for the first $t-1$ steps, and the residue voltage for neuron $i$ is higher than zero. Then,
%         \begin{equation}
%             \frac{\theta}{t}\left(\mathbb{E}\left[s_i^{(l)}[t]\right]+\sum_{i=1}^{t-1} s_i^{(l)}[i]\right) = \relu_\theta(X_i^{(l)}).
%         \end{equation}
%         \item[(c)] If $s^{(l)}[1],\dots,s^{(l)}[T]$ are the output vectors of spike trains of the TPP neurons during $T$ time steps, then
%         \begin{equation}
%             \frac{\theta}{T}\sum_{i=1}^{t-1} s^{(l)}_j[i]=\begin{cases}
%                 \relu_\theta(X^{(l)}_j),\quad\text{if $\relu_\theta(X^{(l)}_j)$ is a multiple of $\frac{\theta}{T}$}, \\
%                 \frac{\theta}{T}\cdot \lfloor\frac{T}{\theta}\relu_\theta(X^{(l)}_j)\rfloor \text{ or } \frac{\theta}{T}\cdot \lfloor\frac{T}{\theta}\relu_\theta(X^{(l)}_j)\rfloor+\frac{\theta}{T}, \quad \text{otherwise}. 
%             \end{cases}
%         \end{equation}
%         \item[(d)] Suppose that $\max X^{(l)}\leq \theta$ and that the same weights $W^{(l+1)}$ act on the outputs of layer $(l)$ of ANN and SNN as above, and let $X^{(l+1)}$ (resp. $T\cdot \tilde{X}^{(l+1)}$) be the inputs to the $(l+1)$th ANN layer (resp. the accumulated voltage for the $(l+1)$th SNN layer of TPP neurons), Then 
%         \begin{equation}\label{eq final approximation}
%             ||X^{(l+1)}-\tilde{X}^{(l+1)}||_{\infty} \leq ||W^{(l+1)}||_\infty\cdot \frac{\theta}{T}.
%         \end{equation} 
%     \end{itemize}
% \end{theorem}
% \textbf{Comments:}
% \begin{enumerate}
%     \item[(a)] We contrast this result with Theorem 2 of \cite{bu2022optimal}. Namely, there the authors show that if one uses half of the threshold as the initialization of the membrane potential, the expectation of the conversion error (layerwise) is 0. However, the authors in \cite{bu2022optimal} use the underlying assumption that the distribution of the ANN values layerwise is \textbf{uniform}, which in practice is not the case (see for example \cite{bojkovic2024data}). Our result $(a)$ above shows that \textbf{after every $t\leq T$ time steps}, our expected spiking rate aligns well with the clipping of the $\relu$ activation by the threshold, as it should, without any prior assumptions on the distribution of the ANN activation values.
%     \item[(b)] The point of result $(b)$ is that the activity of TPP neuron adapts to the observed output it already produced. In particular, as long as the neuron is still active and contains residue membrane potential, the expectation of its output at the next time step takes into account the previously produced spikes and will yield the ANN counterpart.
%     \item[(c) (d)] The results $(c)$ and $(d)$ show that during the accumulation phase, the TPP neuron approximate well the ANN neurons with \relu activation. In particular, the only remaining source of errors in layerwise approximation is the clipping error due to the set threshold $\theta$, and the quantization error due to the discrete outputs of the spiking neurons.  We also note in Equation \eqref{eq final approximation} the two possibilities of the output in the second case ("otherwise"). 
% \end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}