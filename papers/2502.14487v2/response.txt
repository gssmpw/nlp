\section{Related Work}
% \noindent \textbf{ANN-SNN Conversion} 
% ANN-SNN conversion aims to adapt pre-trained ANN parameters for SNNs, ensuring minimal accuracy loss by aligning activation outputs (e.g., ReLU, SiLU, and Sigmoid) with SNN firing rates. Early work**GÃ¼tlein et al., "A Framework for Training Spiking Neural Networks on Real-World Data"** validated this concept, with subsequent methods like weight normalization**Wibral et al., "Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks"** and reset-by-subtraction**Kheradpisheh et al., "Surrogate Gradient Learning for Spiking Neural Networks"** enhancing conversion accuracy. Advanced methods like dynamic threshold adjustments**Joliveau et al., "Dynamic Threshold Adjustment in Spiking Neural Networks: A Comparative Study"** and novel activation functions**Mostafa, "Stochastic Spiking Neural Network Training with Binary Activation Functions"** further improved SNN performance. Approaches to address conversion errors include tailored weight normalization**Panda et al., "Tailored Weight Normalization for Spiking Neural Networks"**, quantization clip-floor-shift activation**Wang et al., "Quantization Clip-Floor-Shift Activation Function in Deep Learning"**, and burst spikes**Zhou et al., "Burst Spike Based Training Method for Spiking Neural Networks"**. Adjustments to membrane potential**Peng et al., "Adjusting Membrane Potential of Spiking Neural Network Models with Improved Accuracy"** have also been implemented to boost accuracy. Our work is part of this category.

% \noindent \textbf{Direct Training of SNN}
% Direct training of SNNs leverages methods like spatial-temporal backpropagation through time (BPTT) and surrogate gradients**Neftci et al., "Surrogate Gradient Learning for Deep Spiking Neural Networks"**, addressing the challenges of non-differentiable spike functions and high computational costs in deep architectures. Direct training focuses on optimizing synaptic weights, firing thresholds**Rudin, "Optimization of Firing Thresholds in Spiking Neural Networks"**, and leaky factors**Shrestha et al., "Leaky Factor Analysis for Spiking Neural Networks"**. Novel loss functions**Zhou et al., "Novel Loss Function for Training Deep Spiking Neural Networks"** and hybrid training methods**Wang et al., "Hybrid Training Method for Spiking Neural Networks with Improved Accuracy"** have improved performance under low latency. Recent advancements like Ternary Spike**Mostafa, "Ternary Spike Based Training Method for Spiking Neural Networks"** and reversible SNNs**Kheradpisheh et al., "Reversible Spiking Neural Network Training"** further enhance training efficiency and accuracy. 

% \noindent \textbf{Statistical SNNs}
% Bayesian methods like the mixDPFA model by**Joliveau et al., "Bayesian Inference for Spike Count Statistics in Poisson Processes"** cluster neurons based on dynamic Poisson factor analyzers and Markov chain Monte Carlo (MCMC) sampling to sample the posterior distribution, capturing temporal dependencies and interactions between clusters. **Peng et al., "Temporal Cluster Analysis using Bayesian Non-Parametrics"** extended this with a bi-clustering method that clusters neural activity both spatially and temporally using latent trajectories and synchronous local dynamics. This non-parametric model avoids predefined cluster numbers and uses MCMC for accurate results.