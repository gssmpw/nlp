\section{Related Work}
% \noindent \textbf{ANN-SNN Conversion} 
% ANN-SNN conversion aims to adapt pre-trained ANN parameters for SNNs, ensuring minimal accuracy loss by aligning activation outputs (e.g., ReLU, SiLU, and Sigmoid) with SNN firing rates. Early work~\cite{cao2015spiking} validated this concept, with subsequent methods like weight normalization~\cite{Diehl2015Fast} and reset-by-subtraction~\cite{rueckauer2017conversion} enhancing conversion accuracy. Advanced methods like dynamic threshold adjustments~\cite{han2020rmp, stockl2021optimized, ho2021tcl, DBLP:journals/tnn/WuCZLLT23} and novel activation functions~\cite{DBLP:conf/iclr/BuFDDY022} further improved SNN performance. Approaches to address conversion errors include tailored weight normalization~\cite{Sengupta2018Going}, quantization clip-floor-shift activation~\cite{DBLP:conf/iclr/BuFDDY022}, and burst spikes~\cite{DBLP:conf/ijcai/Li022}. Adjustments to membrane potential~\cite{li2021free, DBLP:conf/iclr/HaoDB0Y23} have also been implemented to boost accuracy. Our work is part of this category.

% \noindent \textbf{Direct Training of SNN}
% Direct training of SNNs leverages methods like spatial-temporal backpropagation through time (BPTT) and surrogate gradients~\cite{DBLP:conf/iclr/OConnorGRW18, zenke2018superspike, wu2018spatio}, addressing the challenges of non-differentiable spike functions and high computational costs in deep architectures. Direct training focuses on optimizing synaptic weights, firing thresholds~\cite{wei2023temporal}, and leaky factors~\cite{DBLP:journals/tnn/RathiR23}. Novel loss functions~\cite{zhu2024exploring, guo2022recdis} and hybrid training methods~\cite{DBLP:journals/corr/abs-2205-07473} have improved performance under low latency. Recent advancements like Ternary Spike~\cite{DBLP:conf/aaai/GuoCLPZHM24} and reversible SNNs~\cite{zhang2023memory} further enhance training efficiency and accuracy. 

% \noindent \textbf{Statistical SNNs}
% Bayesian methods like the mixDPFA model by~\cite{DBLP:conf/nips/WeiSW22} cluster neurons based on dynamic Poisson factor analyzers and Markov chain Monte Carlo (MCMC) sampling to sample the posterior distribution, capturing temporal dependencies and interactions between clusters. \cite{wei2023bayesian} extended this with a bi-clustering method that clusters neural activity both spatially and temporally using latent trajectories and synchronous local dynamics. This non-parametric model avoids predefined cluster numbers and uses MCMC for accurate results.