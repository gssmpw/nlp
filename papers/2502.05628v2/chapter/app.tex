\section{Experimental Setup}\label{app:exp}
\subsection{Datasets}
UnKEBench \cite{UnKE} constructs a dataset containing 1,000 counterfactual unstructured texts, where knowledge is presented in an unstructured and relatively lengthy form, going beyond simple knowledge triplets or linear fact chains. These texts originate from ConflictQA \cite{conflictqa}, a benchmark specifically designed to distinguish LLMs' parameter memory from anti-memory. This approach is crucial for preventing the model from merging knowledge obtained during pretraining with knowledge acquired during the editing process. Moreover, it addresses the key challenge of determining whether the model learns target knowledge during training or editing, ensuring a clear boundary between pretraining knowledge and edited knowledge.

AKEW benchmark \cite{AKEW} considers three aspects: (1) \textit{Structured Facts}: Each structured fact consists of an isolated triplet for editing, sourced from existing datasets or knowledge bases. (2) \textit{Unstructured Facts}: Knowledge is presented in unstructured text form. To enable fair comparisons, each unstructured fact contains the same knowledge update as its corresponding structured fact. Compared to structured facts, unstructured facts exhibit greater complexity in natural language format, as they often encapsulate more implicit knowledge. (3) \textit{Extracted Triplets}: Triplets are extracted from unstructured facts using automated methods to investigate whether they can facilitate knowledge editing methods in handling unstructured knowledge. In this work, we primarily focus on unstructured factual knowledge.

EditEverything dataset integrates question-answering data from multiple domains, forming long and diverse knowledge formats that are more challenging to edit. Specifically, for mathematics, we select longer samples from the Orca-Math dataset \cite{math}, which includes grade school math word problems. For coding, we use the MBPP dataset \cite{code}, which consists of approximately 1,000 crowd-sourced Python programming problems solvable by entry-level programmers, covering programming fundamentals and standard library functionalities. For chemistry, we sample from the Camel-Chemistry dataset \cite{chemistry}, which contains problem-solution pairs generated from 25 chemistry topics, each with 25 subtopics and 32 problems per topic-subtopic pair. Lastly, for the news and poetry categories, since they often contain real-world knowledge that LLMs may already possess, we generate synthetic data using GPT-4o to ensure that the information is not already known by the model.

We present sample instances from the dataset in Figure \ref{fig:sample1}, Figure \ref{fig:sample2}, and Figure \ref{fig:sample3}.

\subsection{Evaluation Metrics} \label{app:exp_metric}
Following previous research on model editing for structured knowledge \cite{ROME, MEND}, existing evaluation metrics primarily focus on triplet-structured knowledge, where the goal is to assess the modification of factual triples (\textit{subject, relation, object}). Specifically, given an LLM $f$, an editing knowledge pair $(x, y)$, an equivalent knowledge query $x_e$, and unrelated knowledge pairs $(x_{loc}, y_{loc})$, three standard evaluation metrics are commonly used:

\textbf{Efficacy.} This metric quantifies the success of modifying the target knowledge in $f_{\mathcal{W}}$. It evaluates whether the edited LLM generates the desired target output $y$ when given the input $x$. Formally, it is defined as:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f}(y'\left|x\right.)\right\}.
\end{equation}

\textbf{Generalization.} This metric assesses whether the model has generalized the newly edited knowledge beyond its specific form. It measures if the LLM correctly produces $y$ when given a semantically equivalent input $x_e$, indicating the degree to which the update propagates correctly across paraphrased or restructured queries:
\begin{equation}
\mathbb{E}\left\{y=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f}(y'\left|x_e\right.)\right\}.
\end{equation}

\textbf{Specificity.} This metric evaluates whether the editing operation is localized, ensuring that unrelated knowledge remains intact. It measures whether the model's response to an unrelated query $x_{loc}$ remains consistent with its original output $y_{loc}$:
\begin{equation}
\mathbb{E}\left\{y_{loc}=\mathop{\arg\max}\limits_{y'}\mathbb{P}_{f}(y'\left|x_{loc}\right.)\right\}.
\end{equation}

While these metrics are well-suited for structured knowledge editing, they are insufficient for evaluating long-form and diverse-formatted knowledge. Such knowledge is often verbose and complex, making it challenging to assess correctness solely based on Efficacy. In these cases, the model may generate an answer that captures the essential information yet fails an exact-match evaluation. To address this, we primarily follow the existing benchmarks for unstructured knowledge editing, incorporating more flexible evaluation methods suited for long-form responses.

Lexical similarity metrics include BLEU \cite{bleu} and various ROUGE scores (ROUGE-1, ROUGE-2, and ROUGE-L) \cite{rouge}. These are computed based on the \textit{original questions}, \textit{paraphrase question}, and \textit{sub-questions}, providing insights into the lexical and n-gram alignment between the model-generated text and the target answer. These metrics serve as the foundation for assessing the surface-level accuracy of edited content.

Semantic similarity is also considered (Bert Score) \cite{bertscore}, as word-level overlap alone is insufficient to capture the nuanced understanding required by the model. To address this, we utilize embedding-based encoders, specifically the all-MiniLM-L6-v2 model \footnote{https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}, to measure semantic similarity. This ensures a more balanced evaluation that extends beyond lexical matching, quantifying the depth of the model's comprehension.

\subsection{Baseline Methods}
\begin{itemize}
    \item \textbf{FT-L} \cite{FTw} is a knowledge editing approach that fine-tunes specific layers of the LLM using an autoregressive loss function. We reimplemented this method following the hyperparameter from the original paper.
    
    \item \textbf{MEND} \cite{MEND} is a hypernetwork-based efficient knowledge editing method. It trains a hypernetwork to capture patterns in knowledge updates by mapping low-rank decomposed fine-tuning gradients to LLM parameter modifications, enabling efficient and localized edits. Our implementation follows the original hyperparameter settings and completes training over the full dataset. 
    
    \item \textbf{ROME} \cite{ROME} is a method for modifying factual associations in LLM parameters. It identifies critical neuron activations in MLP layers through perturbation-based knowledge localization and modifies MLP layer weights using Lagrange remainders. Since ROME is not designed for large-scale edits, we follow the original paper’s settings and conduct multiple rounds of single-instance editing for evaluation.
    
    \item \textbf{MEMIT} \cite{MEMIT} extends ROME by enabling batch updates of factual knowledge. It utilizes least squares approximation to modify specific layer parameters across multiple layers, allowing simultaneous updates of large numbers of knowledge facts. We evaluate MEMIT in lifelong editing scenarios using the original paper’s configuration.
    
    \item \textbf{AlphaEdit} \cite{AlphaEdit} is a method designed to mitigate interference in LLM lifelong knowledge editing. It introduces a null-space projection mechanism that ensures parameter updates preserve previously edited knowledge while incorporating new updates. AlphaEdit has demonstrated state-of-the-art (SOTA) performance across multiple evaluation metrics while maintaining strong transferability. We follow the original paper’s hyperparameter configuration in our implementation.
    
    \item \textbf{UnKE} \cite{UnKE} improves knowledge editing by refining both the layer and token dimensions. In the layer dimension, it replaces local key-value storage with a non-local block-based mechanism, enhancing the representation capability of key-value pairs while integrating attention-layer knowledge. In the token dimension, it replaces "term-driven optimization" with "cause-driven optimization," which directly edits the final token while preserving contextual coherence. This eliminates the need for explicit term localization and prevents context loss.
\end{itemize}

\subsection{Implementation Details}
Our AnyEdit and AnyEdit* primarily follow the baseline configurations of MEMIT and UnKE, while other baselines adhere to their original implementation settings. All experiments were conducted on a single A100 GPU (80GB).
\begin{itemize}
    \item \textbf{AnyEdit on Llama3-8B-Instruct:} We select layers 4 to 8 for editing and apply a clamp norm factor of 4. The fact token is defined as the last token. The optimization process involves 25 gradient steps for updating the key-value representations, with a learning rate of 0.5. The loss is applied at layer 31, and we use a weight decay of 0.001. To maintain distributional consistency, we introduce a Kullback-Leibler (KL) regularization term with a factor of 0.0625. Furthermore, we enable hyperparameter $\lambda$ with an update weight of 15,000, using 100,000 samples from the Wikipedia dataset with a data type of float32. The module configurations follow MEMIT, where edits are applied to the MLP down projection layers of the selected transformer blocks. Additionally, for chunked editing, we set a chunk size of 40 tokens with no overlap.
    \item \textbf{AnyEdit on Qwen2.5-7B-Instruct:} Same as the above, except that the loss is applied at layer 27 and the chunk size is set to 50 tokens.
    \item \textbf{AnyEdit* on Llama3-8B-Instruct:} We select layer 7 for editing and apply a clamp norm factor of 4. The fact token is defined as the last token. The optimization process involves updating all parameters in both the attention and MLP layers. The gradient descent process utilizes a learning rate of 0.0002 with 50 optimization steps. For updating key-value representations, we use 25 gradient steps with a learning rate of 0.5. The loss is applied at layer 31, and we use a weight decay of 0.001. To preserve original knowledge, we sample 20 data points to constrain parameter updates. Additionally, for chunked editing, we set a chunk size of 40 tokens with no overlap.
    \item \textbf{AnyEdit* on Qwen2.5-7B-Instruct:} Same as the above, except that the loss is applied at layer 27 and the chunk size is set to 50 tokens.
\end{itemize}

\section{Locate-Then-Edit Paradigm \& Related Proof}
\subsection{Locate-Then-Edit Paradigm}\label{app:model_edit}
Following prior works on model editing, the detailed descriptions of specific methods in this section are based on MEMIT \cite{MEMIT}, AlphaEdit \cite{AlphaEdit} and ECE \cite{ECE}. We adhere to their formulations and methodological explanations to ensure consistency and clarity in presenting these approaches.

The locate-then-edit method primarily focuses on triplet-structured knowledge in the form of $(s, r, o)$, such as modifying $(\text{Olympics}, \text{were held in}, \text{Tokyo})$ to $(\text{Olympics}, \text{were held in}, \text{Paris})$. Given new knowledge $(x_e, y_e)$, a triplet can be represented as $x_e = (s, r)$ and $y_e = o$.

We first refine the auto-regressive language model formulation in Section \ref{sec:method:pre}. Let $f$ be a decoder-only model with $L$ layers, processing input sequence $x = (x_0, x_1, \dots, x_T)$ to predict the next token:
\begin{equation}
    \begin{aligned}
        \vh_t^l(x) &= \vh_t^{l - 1}(x) + \va_t^l(x) + \vm_t^l(x), \\
        \va_t^l &= \text{attn}^l(\vh_0^{l - 1}, \vh_1^{l - 1}, \dots, \vh_t^{l - 1}), \\
        \vm_t^l &= \mW_{\text{out}}^l \sigma(\mW_{\text{in}}^l \gamma(\vh_t^{l - 1}+\va_t^l)),
    \end{aligned}
\end{equation}
where $\vh_t^l$ denotes the hidden state of token $t$ at layer $l$, $\va_t^l$ is the attention output, and $\vm_t^l$ is the feedforward (FFN) output. Here, $\mW_{\text{in}}^l$ and $\mW_{\text{out}}^l$ are weight matrices, $\sigma$ is a nonlinear activation function, and $\gamma$ denotes layer normalization.

\textbf{Key-Value Memory Structure}. Locate-then-edit assumes that factual knowledge is stored in the FFN layers and treats them as linear associative memory \cite{key_value}. Specifically, $\mW_{\text{out}}^l$ is conceptualized as a key-value memory structure:
\begin{equation}
    \begin{aligned}
        \underbrace{\vm_t^l}_{\vv} = \mW_{\text{out}}^l \underbrace{\sigma(\mW_{\text{in}}^l \gamma(\vh_t^{l-1}+\va^l))}_{\vk}. \label{eqapp:define_kv}
    \end{aligned}
\end{equation}
Here, the MLP input-output pair at token $t$ and layer $l$ serves as the key-value pair. Casual Tracing is typically used to locate the target token and layer by injecting Gaussian noise into hidden states and incrementally restoring them to analyze output recovery. For more details, please refer to ROME \cite{ROME}.

\textbf{Computing Key-Value.} For editing knowledge $(x_e, y_e)$, we compute its corresponding key-value pair $(\vk^*, \vv^*)$. The key $\vk^*$ is derived via forward propagation of $x_e$, while the value $\vv^*$ is optimized using gradient descent:
\begin{equation}
    \vv^* = \vv + \arg \min_{\bm{\delta}^l} \left( -\log \mathbb{P}_{f(\vh_t^l + \bm{\delta}^l)} [y_e \mid x_e] \right).
\end{equation}
Here, $f(\vh_t^l + \bm{\delta}^l)$ represents the model output when the FFN output $\vh_t^l$ is replaced with $\vh_t^l + \bm{\delta}^l$. 

Methods such as ROME \cite{ROME}, MEMIT \cite{MEMIT}, and AlphaEdit \cite{AlphaEdit} focus on triplets $(s, r, o)$, selecting the last token of the subject $s$ as the target token. In contrast, UnKE \cite{UnKE} extends to unstructured text, using the last token of $x_e$ as the target.

To insert new knowledge $(\vk^*, \vv^*)$ into the key-value memory, we solve the constrained least squares problem:
\begin{align*}
    \min_{\hat{\mW}} &\quad \left\lVert \hat{\mW}\mK - \mV \right\rVert \\
    \text{s.t.} &\quad \hat{\mW}\vk^* = \vv^*.
\end{align*}
The final parameter update can be computed via ROME/MEMIT/AlphaEdit's closed-form solution or UnKE's gradient-based optimization.

For clarity, let $\tilde{\mW}$ denote the edited weight of $\mW_{\text{out}}^l$ in the MLP, and let $\mW$ represent its original weight. The final parameter update can be computed using the closed-form solutions of ROME/MEMIT/AlphaEdit or the gradient-based optimization method in UnKE.

\textbf{Weights Update in ROME.} The ROME method derives a closed-form solution to the constrained least-squares problem for updating MLP parameters:
\begin{equation}
    \tilde{\mW} = \mW + \frac{(\vv^\ast - \mW\vk^\ast) (\mC^{-1} \vk^\ast) ^ {T}}{(\mC^{-1} \vk^\ast) ^ {T} \vk^\ast},
\end{equation}
where $\mC = \mK \mK^T$. The matrix $\mC$ is estimated using 100,000 samples of hidden states $\vk$ obtained from tokens sampled in-context from the entire Wikipedia dataset.

\textbf{Weights Update in MEMIT.} Since the above solution updates only a single knowledge sample at a time, MEMIT improves upon it by avoiding Lagrange multipliers and instead using a relaxed constraint formulation. The problem is reformulated by maintaining a factual set $\{\mK_1, \mV_1\}$ containing $u$ new associations while preserving the original set $\{\mK_0, \mV_0\}$ with $n$ existing associations:
\begin{equation}
\begin{gathered}
    \mK_0 = \left[\vk_1 \mid \vk_2 \mid \dots \mid \vk_n\right], \quad \mV_0 = \left[\vv_1 \mid \vv_2 \mid \dots \mid \vv_n\right], \\
    \mK_1 = \left[\vk^\ast_{n+1} \mid \vk^\ast_{n+2} \mid \dots \mid \vk^\ast_{n+u}\right], \quad \mV_1 = \left[\vv^\ast_{n+1} \mid \vv^\ast_{n+2} \mid \dots \mid \vv^\ast_{n+u}\right].
\end{gathered}
\end{equation}
Here, $\vk$ and $\vv$ are defined as in Eq.~\ref{eqapp:define_kv}, and their subscripts denote knowledge indices. The objective function is given by:
\begin{equation}
    \tilde{\mW} \triangleq \argmin_{\hat{\mW}} \left( \sum_{i=1}^{n} \left\| \hat{\mW} \vk_i - \vv_i \right\|^2 + \sum_{i=n+1}^{n+u} \left\| \hat{\mW} \vk_i - \vv^\ast_i \right\|^2 \right).
\end{equation}
Applying the normal equation \citep{normal_equation}, the closed-form solution is:
\begin{equation}
    \tilde{\mW} = \left( \mV_1 - \mW \mK_1 \right) \mK_1^T \left( \mK_0 \mK_0^T + \mK_1 \mK_1^T \right)^{-1} + \mW.
\end{equation}

\textbf{Weights Update in AlphaEdit.} AlphaEdit addresses the imbalance between old and new knowledge in lifelong learning. It protects existing knowledge using a null-space projection constraint, ensuring that the update $\bm{\Delta}$ to $\mW_{\text{out}}^l$ is always projected onto the null space of $\mK_0 \mK_0^T$. The final parameter update, refining MEMIT, is:
\begin{equation}
    \tilde{\mW} = \left( \mV_1 - \mW \mK_1 \right) \mK_1^T \mP \left( \mK_p \mK_p^T \mP + \mK_1 \mK_1^T \mP + \mI \right)^{-1}+ \mW.
\end{equation}

\textbf{Weights Update in UnKE.} Unlike previous methods, UnKE considers the entire input to layer $l$, denoted as $f^l$, rather than just the MLP input. The output remains $f^l$'s activation values. The parameter update is applied to the entire layer rather than a single weight matrix. Given the knowledge sets $\{\mK_0, \mV_0\}$ and $\{\mK_1, \mV_1\}$, the optimization objective is formulated as:
\begin{equation}
    \tilde{\Theta}^l \triangleq \argmin_{\hat{\Theta}^l} \left( \sum_{i=1}^{n} \left\|  f_{\hat{\Theta}^l}^l(\vk_i) - \vv_i \right\|^2 + \sum_{i=n+1}^{n+u} \left\|  f_{\hat{\Theta}^l}^l(\vk_i) - \vv^\ast_i \right\|^2 \right),
\end{equation}
where $\Theta^l$ denotes the entire set of parameters in layer $l$. Since a closed-form solution is not feasible, UnKE employs gradient descent to iteratively update $\Theta^l$.

\subsection{Proof of Optimization-Conditional Mutual Information Equivalence} \label{app:proof_cmi}
\begin{theorem}
The optimization objective  
\begin{equation}
    \bm{\delta}^* = \argmin_{\bm{\delta}} \left( -\log \mathbb{P}_{f(\vh_t+\bm{\delta})}(Y \mid X) \right), \label{eq:opt}
\end{equation}  
is equivalent to maximizing the conditional mutual information (CMI) between $X$ and $Y$ given the perturbed hidden state $\vh'$:  
\begin{equation}
    \vh' = \argmax_{\vh'} I(X; Y \mid \vh'). \label{eq:cmi}
\end{equation}
\end{theorem}

\begin{proof}
Starting from the definition of CMI, we expand it via the integral form:  
\begin{equation}
I(X; Y \mid \vh') = \int p(x, y, \vh') \log \frac{p(y \mid x, \vh')}{p(y \mid \vh')} \, dx dy d\vh'.
\end{equation}  
% Applying Bayes’ rule $p(y \mid x, \vh') = \frac{p(x, y \mid \vh')}{p(x \mid \vh')}$, we rewrite the integrand:  
% \begin{equation}
% I(X; Y \mid \vh') = \int p(x, y, \vh') \log \frac{p(x, y \mid \vh')}{p(x \mid \vh') p(y \mid \vh')} \, dx dy d\vh'.
% \end{equation}  
This splits into two entropy terms:  
\begin{align}
I(X; Y \mid \vh') = \underbrace{\int p(x, y, \vh') \log p(y \mid x, \vh') \, dx dy d\vh'}_{\text{Term } \mathcal{A}} - \underbrace{\int p(x, y, \vh') \log p(y \mid \vh') \, dx dy d\vh'}_{\text{Term } \mathcal{B}}. \label{eq:split}
\end{align}  

Term $\mathcal{A}$ simplifies to the expectation:  
\begin{equation}
\mathcal{A} = \mathbb{E}_{p(\vh')} \mathbb{E}_{p(x, y \mid \vh')} \left[ \log p(y \mid x, \vh') \right],
\end{equation}  
while Term $\mathcal{B}$ is independent of $X$ given $\vh'$. Since $\mathcal{B}$ does not affect the optimization over $\vh'$, we focus on maximizing $\mathcal{A}$.  

By definition, $\mathbb{P}_{f(\vh')}(Y \mid X) = p(y \mid x, \vh')$. Thus, minimizing the negative log-likelihood in \eqref{eq:opt} directly maximizes $\mathcal{A}$, which is equivalent to maximizing $I(X; Y \mid \vh')$. Substituting $\vh' = \vh_t + \bm{\delta}^*$, we conclude:  
\begin{equation}
\vh' = \argmax_{\vh'} I(X; Y \mid \vh'),
\end{equation}  
thereby establishing the equivalence.  
\end{proof}

\subsection{Proof of the Decomposition of Mutual Information}\label{app:proof_decom}
To rigorously derive Equation \eqref{eq:final_MI}, we start from the mutual information (MI) decomposition given in Equation \eqref{eq:decom}:
\begin{equation}
    I(X; Y \mid \vh'_1, \dots, \vh'_K) = \sum_{k=1}^{K} I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K).
\end{equation}

\textbf{Step 1: Application of the First Property.}
The first key property states that later hidden states do not influence earlier token generation:
\begin{equation}
    H(Y_p \mid \vh'_q) = H(Y_p), \quad \text{for } p < q.
\end{equation}
Since mutual information is defined as:
\begin{equation}
    I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K) = H(Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K) - H(Y_k \mid X, Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K).
\end{equation}
Since $\vh'_q$ for $q > k$ does not affect $Y_k$, we can simplify:
\begin{equation}
    H(Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K) = H(Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_k).
\end{equation}

\textbf{Step 2: Application of the Second Property.}
The second key property states that once $Y_k$ is determined, conditioning on $Y_k$ subsumes conditioning on $\vh'_k$:
\begin{equation}
    H(\cdot \mid Y_k) = H(\cdot \mid Y_k, \vh'_k).
\end{equation}
Using this, we rewrite the MI term:
\begin{equation}
    I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_1, \dots, \vh'_K) = I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_k).
\end{equation}

\textbf{Step 3: Applying the Conditional Mutual Information Decomposition.}
Using the decomposition formula for conditional mutual information, each term can be written as:
\begin{equation}
    I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_k) = I(X, Y_1, \dots, Y_{k-1}; Y_k \mid \vh'_k) - I(Y_1, \dots, Y_{k-1}; Y_k \mid \vh'_k).
\end{equation}
In the optimization process, since we are given $X$, the second term is ignored:
\begin{equation}
    I(X; Y_k \mid Y_1, \dots, Y_{k-1}, \vh'_k) = I(X, Y_1, \dots, Y_{k-1}; Y_k \mid \vh'_k).
\end{equation}
Substituting this result back into our summation, we arrive at the desired decomposition:
\begin{equation}
    I(X; Y \mid \vh'_1, \dots, \vh'_K) = \sum_{k=1}^{K} I(X, Y_1, \dots, Y_{k-1}; Y_k \mid \vh'_k).
\end{equation}
This completes the proof.

\section{More Experimental Results}\label{app:exp_result}
\subsection{Case Study}

We selected several editing samples from the UnKEBench and EditEverything datasets as case studies to analyze the long-form knowledge edit performance. 

From the experimental results, AnyEdit effectively restores the target text in terms of both textual consistency and generalization to paraphrase questions. In contrast, other baseline methods often fail to achieve effective editing. Although the UnKE method also demonstrates strong editing performance, it exhibits a noticeable performance drop on paraphrase questions.


\subsubsection{Case 1}

\begin{tcolorbox}[boxrule=0.5pt, left=0pt, right=0pt, top=2.5pt, bottom=2.5pt, title={Model Editing Case Study on Llama3-8B-Instruct}]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{p{0.18\linewidth} p{0.76\linewidth}}
        \makecell[c]{\textbf{Input}} & \makecell[c]{What is John Mayne's background and experience in journalism?} \\
        \hline
        \makecell[c]{\textbf{Paraphrase}} & \makecell[c]{Can you provide information about John Mayne's journalism background and experience?} \\
        \hline
        \makecell[c]{\textbf{Target Output}} & \makecell*[{{p{13cm}}}]{John Mayne is a journalist who has been working in the field for over a decade. He has written for several reputable publications, including \tar{The New York Times} and \tar{The Guardian}. In an interview with \tar{The Huffington Post}, Mayne discussed his passion for journalism and his commitment to reporting on important issues. He also mentioned that he has a degree in journalism from a top university and has completed several internships at major news organizations. Mayne's extensive experience and education make him a highly qualified journalist and a valuable asset to any newsroom.} \\
        \hline
        \multicolumn{2}{c}{\textbf{\large LLM-edited Generation}} \\ % Bold and centered
        \hline
        \makecell[c]{\textbf{MEMIT}} & \makecell*[{{p{13cm}}}]{John Mayne is a journalist and writer, but I couldn't find much about his personal background. However, he has been in the industry for several years, contributing to various publications. According to LinkedIn, he has worked for major outlets including BBC and \tar{The Guardian}(...) John Mayne's experience spans politics, business, and culture, and he has been recognized for his in-depth reporting.} \\
        \hline
        \makecell[c]{\textbf{AlphaEdit}} & \makecell*[{{p{13cm}}}]{John Mayne is a British journalist and author with a long career in media. He has written for major newspapers such as The Times and The Sunday Times. Mayne has covered diverse topics, including politics and culture, and worked as a foreign correspondent reporting from the Middle East, Africa, and Asia... In addition, he has taught journalism at universities and received multiple awards for his writing.} \\
        \hline
        \makecell[c]{\textbf{UnKE}} & \makecell*[{{p{13cm}}}]{John Mayne is a journalist who has been working in the field for several years. He has written for a number of publications and has a strong track record of producing high-quality, well-researched articles. In addition to his writing experience, Mayne has also worked as an editor and has a strong understanding of the journalism industry. He is well-respected by his peers and is known for his ability to produce engaging and informative articles.} \\
        \hline
        \makecell[c]{\textbf{AnyEdit}} & \makecell*[{{p{13cm}}}]{ John Mayne is a journalist who has been working in the field for over a decade. He has written for several reputable publications, including \tar{The New York Times} and \tar{The Guardian}. In an interview with \tar{The Huffington Post}, Mayne discussed his passion for journalism and his commitment to reporting on important issues. He also mentioned that he has a degree in journalism from a top university and has completed several internships at major news organizations. Mayne's extensive experience and education make him a highly qualified journalist and a valuable asset to any newsroom.} \\
    \end{tabular}
\end{tcolorbox}

\subsubsection{Case 2}

\begin{tcolorbox}[boxrule=0.5pt, left=0pt, right=0pt, top=2.5pt, bottom=2.5pt, title={Model Editing Case Study on Qwen2.5-7B-Instruct}]
    \centering
    \renewcommand{\arraystretch}{1.8}
    \begin{tabular}{p{0.18\linewidth} p{0.76\linewidth}}
        \makecell[c]{\textbf{Input}} & \makecell[c]{find the value of $log_y(x^4)$ if $log_x(y^3) = 2$} \\
        \hline
        \makecell[c]{\textbf{Target Output}} & \makecell*[{{p{13cm}}}]{To find the value of $log_y(x^4)$, we can use the property of logarithms that states: $log_b(a) = 1 / log_a(b)$. Given that $log_x(y^3) = 2$, we can rewrite this as: $y^3 = x^2$. Now, we want to find $log_y(x^4)$. We can use the property mentioned above: $log_y(x^4) = 1 / log_x(y^4)$. But we don't have $log_x(y^4)$; we have $log_x(y^3)$. So, we need to manipulate the expression to use the given information. We know that $y^3 = x^2$, so we can raise both sides to the power of $4/3$ to get $y^4$: $(y^3)^{(4/3)} = (x^2)^{(4/3)}$. $y^4 = x^{(8/3)}$. Now we can write $log_x(y^4)$ in terms of $log_x(y^3)$: $log_x(y^4) = log_x((y^3)^{(4/3)})$. Using the power rule of logarithms ($log_b(a^c) = c * log_b(a)$), we get: $log_x(y^4) = (4/3) * log_x(y^3)$. Since we know $log_x(y^3) = 2$, we can substitute this value in: $log_x(y^4) = (4/3) * 2$. $log_x(y^4) = 8/3$. Now we can find $log_y(x^4)$ using the reciprocal property: $log_y(x^4) = 1 / log_x(y^4)$. $log_y(x^4) = 1 / (8/3)$. $log_y(x^4) = 3/8$. Therefore, the value of $log_y(x^4)$ is \tar{$3/8$}.} \\
        \hline
        \multicolumn{2}{c}{\textbf{\large LLM-edited Generation}} \\ % Bold and centered
        \hline
        \makecell[c]{\textbf{MEMIT}} & \makecell*[{{p{13cm}}}]{To find the value of $log_y(x^4)$, we can use the change of base formula and the given information. Given: $log_x(y^3) = 2$. Using the change of base formula, we can rewrite this as: $log_x(y^3) = (log_y(y^3)) / (log_y(x))$. Since $log_y(y^3) = 3$ (because $y^3$ is the argument raised to the power of 3), we have: $2 = 3 / (log_y(x))$. Now, we can solve for $log_y(x)$: $log_y(x) = 3/2$. Next, we want to find $log_y(x^4)$. Using the power rule of logarithms, we can write: $log_y(x^4) = 4 * log_y(x)$. Substituting the value we found for $log_y(x)$: $log_y(x^4) = 4 * (3/2)$. Simplifying: $log_y(x^4) = 6$. Therefore, the value of $log_y(x^4)$ is $6$.} \\
        \hline
        \makecell[c]{\textbf{AlphaEdit}} & \makecell*[{{p{13cm}}}]{To find the value of $log_y(x^4)$, we can use the change of base formula and the given information: $log_x(y^3) = 2$. Using the change of base formula, we can rewrite this as: $y^3 = x^2$. Now, we want to find $log_y(x^4)$. Using the properties of logarithms, we can rewrite this as: $log_y(x^4) = 4 * log_y(x)$. We can express $log_y(x)$ in terms of $log_x(y)$ using the change of base formula: $log_y(x) = 1 / log_x(y)$. Since we know that $log_x(y^3) = 2$, we can write: $log_x(y^3) = 3 * log_x(y) = 2$. Therefore: $log_x(y) = 2/3$. Substituting this into our expression for $log_y(x)$: $log_y(x) = 1 / (2/3) = 3/2$. Finally, substituting this back into our expression for $log_y(x^4)$: $log_y(x^4) = 4 * (3/2) = 6$. Therefore, the value of $log_y(x^4)$ is $6$.} \\
        \hline
        \makecell[c]{\textbf{UnKE}} & \makecell*[{{p{13cm}}}]{To find the value of $log_y(x^4)$, we can use the property of logarithms that states: $log_b(a) = 1 / log_a(b)$. Given that $log_x(y^3) = 2$, we can rewrite this as $y^3 = x^2$. Now, we can use the property to find $log_y(x^4)$: $log_y(x^4) = 1 / log_x(y^3)$ $= 1 / (1 / x^2)$ $= x^2$. So, the value of $log_y(x^4)$ is $x^2$. Therefore, the correct answer is $x^2$.} \\
        \hline
        \makecell[c]{\textbf{AnyEdit}} & \makecell*[{{p{13cm}}}]{To find the value of $log_y(x^4)$, we can use the property of logarithms that states: $log_b(a) = 1 / log_a(b)$. Given that $log_x(y^3) = 2$, we can rewrite this as: $y^3 = x^2$. Now, we want to find $log_y(x^4)$. We can use the property mentioned above: $log_y(x^4) = 1 / log_x(y^4)$. But we don't have $log_x(y^4)$; we have $log_x(y^3)$. So, we need to manipulate the expression to use the given information. We know that $y^3 = x^2$, so we can raise both sides to the power of $4/3$ to get $y^4$: $(y^3)^{(4/3)} = (x^2)^{(4/3)}$. $y^4 = x^{(8/3)}$. Now we can write $log_x(y^4)$ in terms of $log_x(y^3)$: $log_x(y^4) = log_x((y^3)^{(4/3)})$. Using the power rule of logarithms ($log_b(a^c) = c * log_b(a)$), we get: $log_x(y^4) = (4/3) * log_x(y^3)$. Since we know $log_x(y^3) = 2$, we can substitute this value: $log_x(y^4) = (4/3) * 2$. $log_x(y^4) = 8/3$. Now we can use the property of logarithms: $log_y(x^4) = 1 / log_x(y^4)$. $log_y(x^4) = 1 / (8/3)$. $log_y(x^4) = 3/8$. Therefore, the value of $log_y(x^4)$ is \tar{$3/8$}.} \\
    \end{tabular}
\end{tcolorbox}
\clearpage
\newpage

\subsection{Supplementary Experimental Results on RQ1 \& RQ2} \label{app:exp_result_1}
We present a comprehensive evaluation of all metrics on the UnKEBench and AKEW datasets in Table \ref{tab:app_1} and Table \ref{tab:app_2}. The results demonstrate that UnKE consistently outperforms other baselines across both original and paraphrase question evaluations. Notably, UnKE+, which integrates AnyEdit’s autoregressive editing paradigm, achieves even higher scores in lexical similarity (BLEU, ROUGE-1/2/L) and semantic similarity (BERT Score), indicating its superior ability to preserve and generalize edited knowledge. In contrast, MEMIT and AlphaEdit struggle with paraphrase generalization, showing significantly lower performance on the right side of `/', suggesting that these methods fail to robustly transfer edited knowledge across rephrased contexts. While MEMIT+ and AlphaEdit+ improve over their base versions, their performance still lags behind UnKE and UnKE+.

Overall, UnKE+ achieves the best balance between precise knowledge modification and robust generalization, confirming that combining UnKE with autoregressive fine-tuning leads to stronger and more reliable knowledge editing in LLMs.
\begin{table*}[h]
\caption{Performance comparison in UnKEBench. The `+' symbol indicates results incorporating AnyEdit's autoregressive editing paradigm. The left side of `/' represents the LLM's edited output for original questions, while the right side represents the edited output for paraphrase questions.}
    \label{tab:app_1}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l cccc ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Lexical Similarity}} & \multicolumn{1}{c}{\textbf{Semantic Similarity}} & \textbf{Sub Questions} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} 
        & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-L & BERT Score & ROUGE-L \\
        \midrule
        \multicolumn{7}{l}{\textbf{Based on Llama3-8B-Instruct}} \\
        \midrule
        UnKE        & 93.56 / 78.09  & 93.61 / 79.26  & 91.42 / 71.73  & 93.33 / 78.42  & 98.34 / 93.38    & 37.87 \\
        UnKE+       & 99.67 / 84.60  & 99.69 / 86.31  & 99.57 / 81.18  & 99.68 / 85.75  & 99.86 / 94.70    & 41.45 \\
        MEMIT       & 25.57 / 22.88  & 32.67 / 30.75  & 14.51 / 12.37  & 30.49 / 28.65  & 76.21 / 74.25    & 22.56 \\
        MEMIT+      & 88.88 / 81.38  & 93.26 / 86.53  & 90.32 / 80.61  & 92.96 / 85.91  & 97.76 / 95.60    & 41.67 \\
        AlphaEdit   & 21.29 / 20.24  & 28.62 / 27.99  & 11.36 / 10.24  & 26.59 / 25.92  & 73.92 / 72.96    & 20.71 \\
        AlphaEdit+  & 75.02 / 66.35  & 81.70 / 73.47  & 74.35 / 62.74  & 80.92 / 72.22  & 94.19 / 91.51    & 40.56 \\
        \midrule
        \multicolumn{7}{l}{\textbf{Based on Qwen2.5-7B-Instruct}} \\
        \midrule
        UnKE        & 91.92 / 70.61  & 91.41 / 68.47  & 87.75 / 56.34  & 91.01 / 67.00  & 96.97 / 89.17    & 38.12 \\
        UnKE+       & 98.52 / 82.48  & 98.85 / 83.36  & 98.43 / 77.03  & 98.82 / 82.60  & 99.35 / 94.81    & 42.24 \\
        MEMIT       & 45.07 / 40.81  & 40.73 / 36.75  & 19.59 / 15.87  & 38.04 / 34.07  & 78.03 / 76.50    & 24.75 \\
        MEMIT+      & 91.31 / 77.23  & 95.10 / 80.88  & 92.93 / 72.50  & 94.89 / 79.98  & 98.05 / 93.56    & 42.38 \\
        AlphaEdit   & 49.71 / 45.21  & 45.42 / 41.06  & 24.63 / 19.85  & 42.77 / 38.26  & 80.48 / 78.38    & 25.37 \\
        AlphaEdit+  & 97.77 / 83.09  & 98.20 / 84.18  & 97.40 / 77.38  & 98.14 / 83.40  & 99.08 / 94.51    & 41.58 \\
        \bottomrule
    \end{tabular}
    }
    
\end{table*}

\begin{table*}[h]
\caption{Performance comparison in AKEW (Counterfact). The `+' symbol indicates results incorporating AnyEdit's autoregressive editing paradigm. The left side of `/` represents the LLM's edited output for original questions, while the right side represents the edited output for paraphrase questions.}
    \label{tab:app_2}
    \centering
    \renewcommand{\arraystretch}{1.2}
    \setlength{\tabcolsep}{4pt}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{l cccc ccc}
        \toprule
        \multirow{2}{*}{\textbf{Method}} & \multicolumn{4}{c}{\textbf{Lexical Similarity}} & \multicolumn{1}{c}{\textbf{Semantic Similarity}} & \textbf{Sub Questions} \\
        \cmidrule(lr){2-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7} 
        & BLEU & ROUGE-1 & ROUGE-2 & ROUGE-L & BERT Score & ROUGE-L \\
        \midrule
        \multicolumn{7}{l}{\textbf{Based on Llama3-8B-Instruct}} \\
        \midrule
        MEMIT       & 33.44 / 18.13  & 34.46 / 17.44  & 16.29 / 4.74   & 32.20 / 16.10  & 76.44 / 47.80  & 39.98\\
        MEMIT+      & 85.41 / 38.78  & 96.07 / 47.61  & 94.21 / 32.37  & 95.87 / 46.00  & 97.76 / 62.63  & 64.07\\
        UnKE        & 98.43 / 36.99  & 98.43 / 34.58  & 97.78 / 19.37  & 98.37 / 32.89  & 99.62 / 59.62  & 63.22\\
        UnKE+       & 99.98 / 45.23  & 99.98 / 46.57  & 99.96 / 35.41  & 99.98 / 45.31  & 99.95 / 64.24  & 59.03\\
        AlphaEdit   & 23.36 / 16.25  & 26.92 / 15.00  & 10.81 / 3.61   & 24.95 / 13.79  & 72.63 / 44.67  & 35.76 \\
        AlphaEdit+  & 79.60 / 40.67  & 84.49 / 41.11  & 78.00 / 26.60  & 83.76 / 39.51  & 96.51 / 65.14  & 57.05 \\
        \midrule
        \multicolumn{7}{l}{\textbf{Based on Qwen2.5-7B-Instruct}} \\
        \midrule
        MEMIT       & 45.29 / 32.83  & 41.68 / 28.01  & 20.38 / 8.79   & 38.95 / 25.73  & 77.19 / 56.04  & 43.51\\
        MEMIT+      & 90.55 / 44.32  & 95.33 / 45.56  & 93.12 / 27.38  & 95.09 / 43.49  & 98.08 / 65.40  & 55.10\\
        UnKE        & 91.53 / 38.59  & 90.91 / 31.53  & 87.06 / 12.11  & 90.44 / 29.27  & 97.34 / 59.29  & 49.97\\
        UnKE+       & 98.95 / 34.68  & 99.01 / 35.23  & 98.59 / 15.59  & 98.99 / 32.95  & 99.63 / 60.78  & 51.58\\
        AlphaEdit   & 49.97 / 34.65  & 48.15 / 30.02  & 27.76 / 10.38  & 45.55 / 27.69  & 80.66 / 56.99  & 45.12\\
        AlphaEdit+  & 97.61 / 46.97  & 97.80 / 47.63  & 96.89 / 30.31  & 97.73 / 45.84  & 99.10 / 66.10  & 54.99\\
        \bottomrule
    \end{tabular}
    }
    
\end{table*}

\subsection{Supplementary Experimental Results on RQ4}\label{app:exp_result_4}
\begin{figure}[t]
\begin{center}
\includegraphics[width=0.6\linewidth, keepaspectratio]{figures/exp_3.png}
\caption{The relationship between AnyEdit's editing performance and chunk size in long-form diverse-formatted knowledge.}
\label{fig:exp_3}
\end{center}
\end{figure}


 The experimental results of relationship between AnyEdit's editing performance and chunk size in long-form diverse-formatted knowledge are presented in Figure \ref{fig:exp_3}. Based on these results, we draw the following observation:.

\begin{itemize}[leftmargin=*]
    \item \textbf{Obs 7: The editing performance of AnyEdit is influenced by chunk size.}  
    As the chunk size increases beyond a certain threshold, the editing performance of AnyEdit declines. Specifically, when the chunk size is smaller, each iteration of editing becomes more manageable, leading to improved overall performance. However, this improvement comes at the cost of increased editing time due to the larger number of iterations required for longer texts. Conversely, when the chunk size is larger, it becomes challenging to achieve effective edits within a single iteration, resulting in degraded performance. Based on this trade-off, we recommend using a balanced chunk size of 40 for most editing scenarios.
\end{itemize}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/data1.png}
    \vspace{-5mm}
    \caption{A Sample of the AKEW (Counterfact) dataset.}
    \label{fig:sample1}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/data2.png}
    \vspace{-5mm}
    \caption{A Sample of the UnKEBench dataset.}
    \label{fig:sample2}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/data3.png}
    \vspace{-5mm}
    \caption{Samples of the EditEverything dataset.}
    \label{fig:sample3}
\end{figure}