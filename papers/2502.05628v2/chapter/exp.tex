\section{Experiments} \label{sec:exp}
In this section, we conduct extensive experiments to address the following research questions:
\begin{table*}[t]
\caption{Long-form knowledge editing performance with different methods. ``Pre-edited'' refers to the unedited pre-trained LLM, and  ``Ori.'' and ``Para.'' denote the outputs of the tested LLM for original questions and paraphrased questions respectively. The best results are highlighted in bold.}
\Large
\renewcommand{\arraystretch}{1.2} % Adjust row height
\resizebox{\textwidth}{!}{%
% \fontsize{14}{16}
\begin{tabular}{c|c|cccc|cccc|cc}
\toprule[1.5pt]
& & \multicolumn{4}{c|}{\textbf{UnKEBench}}& \multicolumn{4}{c|}{\textbf{AKEW (Counterfact)}}& \multicolumn{2}{c}{\textbf{AKEW (MQUAKE)}} \\ 
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-12}
& & \multicolumn{2}{c|}{\textbf{Ori.}} & \multicolumn{2}{c|}{\textbf{Para.}} & \multicolumn{2}{c|}{\textbf{Ori.}} & \multicolumn{2}{c|}{\textbf{Para.}} & \multicolumn{2}{c}{\textbf{Ori.}} \\ 
\cmidrule(lr){3-6} \cmidrule(lr){7-10} \cmidrule(lr){11-12}
{\multirow{-3.5}{*}{LLM}} & {\multirow{-3.5}{*}{Method}}&\textbf{Bert Score} & \multicolumn{1}{c|}{\textbf{Rouge-L}} &\textbf{Bert Score} & \multicolumn{1}{c|}{\textbf{Rouge-L}} &\textbf{Bert Score} & \multicolumn{1}{c|}{\textbf{Rouge-L}} &\textbf{Bert Score} & \multicolumn{1}{c|}{\textbf{Rouge-L}} &\textbf{Bert Score} & \multicolumn{1}{c}{\textbf{Rouge-L}} \\ 
\midrule[1.0pt]
\multirow{8}{*}{\rotatebox{90}{{Llama3-8B-It}}} &Pre-edited&{63.18\std{0.38}}&{23.67\std{0.52}}&{62.73\std{0.31}}&{23.52\std{0.51}} & {64.03\std{0.32}} & {15.74\std{0.42}} & {40.20\std{0.46}} & {5.52\std{0.10}}  & {65.77\std{0.37}} & {16.25\std{0.47}}  \\
&FT-L&{40.31\std{0.45}}&{11.39\std{0.56}}&{37.29\std{0.41}}&{8.51\std{0.51}} & {42.89\std{0.43}} & {13.12\std{0.58}} & {31.44\std{0.49}} & {5.24\std{0.08}}  & {45.87\std{0.43}} & {12.99\std{0.52}}  \\
&MEND&{68.73\std{0.34}}&{29.24\std{0.52}}&{64.11\std{0.32}}&{28.05\std{0.54}} & {68.81\std{0.31}} & {30.30\std{0.48}} & {41.56\std{0.40}} & {10.95\std{0.57}}  & {67.85\std{0.33}} & {22.48\std{0.56}}  \\
&ROME&{72.16\std{0.31}}&{23.74\std{0.46}}&{70.54\std{0.25}}&{22.39\std{0.54}} & {72.90\std{0.36}} & {25.86\std{0.52}} & {43.59\std{0.51}} & {12.37\std{0.55}}  & {70.10\std{0.43}} & {21.07\std{0.59}}  \\
&MEMIT&{76.21\std{0.36}}&{30.49\std{0.52}}&{74.25\std{0.31}}&{28.65\std{0.61}} & {76.44\std{0.33}} & {32.20\std{0.48}} & {47.80\std{0.34}} & {16.09\std{0.59}}  & {75.31\std{0.37}} & {22.73\std{0.61}}  \\
&AlphaEdit&{73.92\std{0.29}}&{26.59\std{0.49}}&{72.96\std{0.26}}&{25.92\std{0.51}} & {72.63\std{0.31}} & {24.95\std{0.50}} & {44.67\std{0.46}} & {13.79\std{0.49}}  & {69.85\std{0.36}} & {23.04\std{0.59}}  \\
&AnyEdit&\textbf{97.76\std{0.11}}&\textbf{92.96\std{0.24}}&\textbf{96.60\std{0.19}}&\textbf{95.60\std{0.35}} & \textbf{97.76\std{0.14}} & \textbf{95.87\std{0.23}} & \textbf{62.63\std{0.44}} & \textbf{46.51\std{0.59}}  & \textbf{96.33\std{0.21}} & \textbf{94.32\std{0.23}}  \\
\cmidrule(lr){2-12}
&UnKE&{98.34\std{0.15}}&{93.33\std{0.26}}&{93.38\std{0.21}}&{78.42\std{0.32}} & {98.62\std{0.14}} & {96.37\std{0.22}} & {59.62\std{0.44}} & {32.89\std{0.59}}  & {98.33\std{0.13}} & {95.42\std{0.20}}  \\
&AnyEdit*&\textbf{99.86\std{0.08}}&\textbf{99.68\std{0.21}}&\textbf{94.70\std{0.12}}&\textbf{85.75\std{0.23}} & \textbf{99.95\std{0.01}} & \textbf{99.98\std{0.01}} & \textbf{64.24\std{0.48}} & \textbf{45.31\std{0.55}}  & \textbf{99.89\std{0.06}} & \textbf{99.69\std{0.09}}  \\
\midrule[1pt]
\midrule[1pt]
\multirow{8}{*}{\rotatebox{90}{{Qwen2.5-7B-It}}} &Pre-edited&{64.18\std{0.37}}&{25.88\std{0.59}}&{64.39\std{0.34}}&{24.02\std{0.55}} & {65.50\std{0.34}} & {18.24\std{0.60}} & {44.74\std{0.41}} & {17.29\std{0.51}}  & {67.71\std{0.37}} & {19.58\std{0.49}}  \\
&FT-L&{44.02\std{0.43}}&{13.78\std{0.56}}&{40.33\std{0.36}}&{12.93\std{0.49}} & {46.66\std{0.48}} & {14.63\std{0.58}} & {32.34\std{0.50}} & {12.31\std{0.62}}  & {47.47\std{0.42}} & {15.75\std{0.55}}  \\
&MEND&{69.49\std{0.38}}&{27.77\std{0.61}}&{62.01\std{0.44}}&{27.92\std{0.57}} & {69.54\std{0.54}} & {25.47\std{0.49}} & {52.86\std{0.40}} & {22.81\std{0.54}}  & {69.40\std{0.32}} & {32.39\std{0.44}}  \\
&ROME&{74.73\std{0.33}}&{31.52\std{0.42}}&{71.90\std{0.21}}&{28.12\std{0.38}} & {75.89\std{0.38}} & {36.42\std{0.45}} & {55.67\std{0.47}} & {25.79\std{0.59}}  & {72.18\std{0.373}} & {35.61\std{0.49}}  \\
&MEMIT&{78.03\std{0.30}}&{38.04\std{0.47}}&{76.50\std{0.31}}&{28.65\std{0.50}} & {77.19\std{0.32}} & {38.95\std{0.48}} & {56.04\std{0.40}} & {25.73\std{0.57}}  & {73.15\std{0.32}} & {34.39\std{0.54}}  \\
&AlphaEdit&{80.48\std{0.29}}&{42.77\std{0.36}}&{78.38\std{0.21}}&{38.26\std{0.38}} & {80.66\std{0.25}} & {45.55\std{0.37}} & {56.99\std{0.49}} & {27.69\std{0.59}}  & {74.35\std{0.31}} & {41.07\std{0.44}}  \\
&AnyEdit&\textbf{98.05\std{0.16}}&\textbf{94.89\std{0.29}}&\textbf{93.56\std{0.15}}&\textbf{79.98\std{0.28}} & \textbf{98.08\std{0.15}} & \textbf{95.09\std{0.19}} & \textbf{65.40\std{0.38}} & \textbf{43.49\std{0.47}}  & \textbf{98.14\std{0.13}} & \textbf{96.39\std{0.18}}  \\
\cmidrule(lr){2-12}
&UnKE&{96.97\std{0.18}}&{91.01\std{0.24}}&{89.17\std{0.15}}&{67.00\std{0.29}} & {97.34\std{0.13}} & {90.44\std{0.16}} & {59.29\std{0.48}} & {29.27\std{0.61}}  & {95.04\std{0.23}} & {87.60\std{0.25}}  \\
&AnyEdit*&\textbf{99.35\std{0.12}}&\textbf{98.82\std{0.24}}&\textbf{94.81\std{0.13}}&\textbf{82.60\std{0.26}} & \textbf{99.63\std{0.09}} & \textbf{98.99\std{0.10}} & \textbf{60.78\std{0.39}} & \textbf{32.95\std{0.59}}  & \textbf{99.09\std{0.07}} & \textbf{97.98\std{0.10}}  \\
\bottomrule[1.5pt]
\end{tabular}
}
\label{tab:performance}
\end{table*}
\begin{itemize}[leftmargin=*]
    \item \textbf{RQ1:} How does AnyEdit perform compared to other baselines on tasks involving long-form knowledge?
    \item \textbf{RQ2:} How does AnyEdit compare to other baselines in handling diverse-formatted knowledge?
    \item \textbf{RQ3:} Can AnyEdit improve the performance of other locate-then-edit methods?
    \item \textbf{RQ4:} How does the token length of each chunk affect the performance of long-form knowledge editing in AnyEdit?
\end{itemize}

\subsection{Experimental Setup}  

In this subsection, we summarize the LLMs, baseline methods, datasets, and evaluation metrics used in our experiments. Further details are provided in Appendix \ref{app:exp}.

\textbf{LLMs \& Baseline Methods.}  
We conducted experiments using two widely adopted LLMs: Llama3-8B-Instruct and Qwen2.5-7B-Instruct. For comparison with our method, we evaluated against several model editing methods, including FT-L \cite{FTw}, MEND \cite{MEND}, ROME \cite{ROME}, MEMIT \cite{MEMIT}, AlphaEdit \cite{AlphaEdit}, and UnKE \cite{UnKE}.

\textbf{Datasets and Evaluation Metrics.}  
To evaluate the performance of unstructured long-form knowledge editing, we employed existing benchmarks, including UnKEBench \cite{UnKE} and AKEW \cite{AKEW}. To further assess the editing performance across diverse-formatted knowledge, we constructed a dataset named \textbf{EditEverything}. For evaluation metrics, we assessed the similarity between the model's edited outputs and the editing targets from three perspectives: original questions, paraphrased questions, and sub-questions. The evaluation was conducted using both semantic similarity (BERT Score \cite{bertscore}) and lexical similarity (ROUGE Scores \cite{rouge}) to determine the editing success rate.

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/exp_1.png}
    \caption{Performance comparison between the AnyEdit approach and baseline methods on long-form diverse-formatted knowledge. (a) The performance of various methods on the EditEverything dataset in relation to the number of target tokens edited. (b) A comparison of different editing methods across various types of knowledge. Knowledge types without underlining represent Rouge-L Score metrics, while those with underlining indicate Bert Score metrics. Best viewed in color.}
    \label{fig:exp_1}
\end{figure*}

\subsection{Long-Form Knowledge Editing (RQ1)}  
To evaluate long-form knowledge editing, we conducted experiments on two base LLMs, comparing AnyEdit with baselines on UnKEBench, AKEW (Counterfact), and AKEW (MQUAKE). Following UnKEâ€™s settings, we set the batch size to 1 and used a decoding temperature of 0.001.

Most locate-then-edit baseline methods perform edits by computing parameter updates for a single MLP layer using closed-form solutions, while UnKE uses gradient descent to update the parameters of an entire layer, trading efficiency for precision. To ensure fairness, we introduce \textbf{AnyEdit*}, which also updates the full layer via gradient descent, allowing direct comparison with UnKE. Table~\ref{tab:performance} presents the main results, with additional details in Appendix~\ref{app:exp_result}. Based on Table \ref{tab:performance}, we make the following observations:

\begin{itemize}[leftmargin=*]
    \item \textbf{Obs 1: AnyEdit outperforms all baselines across datasets, LLMs, and metrics.}  
    On UnKEBench, it improves BERT Score by over 20\%, demonstrating its effectiveness in editing long-form knowledge. AnyEdit* further enhances performance, refining complex knowledge representations.

     \item \textbf{Obs 2: AnyEdit shows strong generalization on paraphrase questions.}  
    It significantly outperforms baselines in paraphrase scenarios. On UnKEBench, AnyEdit improves BERT Score by 32\% and ROUGE-L by 56\% on Llama3-8B-Instruct. This highlights its ability to edit long-form knowledge while maintaining robustness to rephrased queries.
\end{itemize}



\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/exp_2.png}
    \caption{Performance improvements of baseline editing methods (\ie MEMIT, AlphaEdit and UnKE) after incorporating  autoregressive editing paradigm in AnyEdit. The yellow bars represent the original performance of each baseline, while the blue bars represent the performance after the addition. Best viewed in color.}
    \label{fig:exp_2}
\end{figure*}
\subsection{Diverse-Formatted Knowledge Editing (RQ2)}  

To further evaluate the generalization capabilities of \textbf{AnyEdit}, we constructed a dataset named \textbf{EditEverything}, which includes unstructured knowledge from various domains and formats to assess the performance of existing editing methods on complex knowledge. This dataset encompasses knowledge from diverse domains such as mathematics, poetry, news, computer code, and chemistry. The detailed evaluation results are shown in Figure \ref{fig:exp_1}(b). Furthermore, we evaluate the relationship between the editing performance and the number of target tokens by selecting long-form samples from the EditEverything dataset. The final results are illustrated in Figure \ref{fig:exp_1}(a). Based on these results, we draw the following observations:
\begin{itemize}[leftmargin=*]
    \item \textbf{Obs 3: AnyEdit achieves superior performance across diverse knowledge domains.}  
    AnyEdit demonstrates consistent improvements across various knowledge types in editing tasks. Notably, the performance gains are most significant in the \textit{Code} and \textit{News} categories, achieving increases of 60.58\% and 52.38\%, respectively, in the ROUGE-L metric. These results underscore the strong generalization capability of the AnyEdit method.

    \item \textbf{Obs 4: AnyEdit maintains stable performance as the number of target tokens increases.}  
    Specifically, methods such as MEMIT, AlphaEdit, and UnKE exhibit varying degrees of performance degradation when the number of target tokens exceeds a certain threshold. For instance, MEMIT and AlphaEdit experience significant performance drops when editing targets exceed 30 tokens. In contrast, AnyEdit remains robust under these conditions, further demonstrating its reliability and scalability.
\end{itemize}


\subsection{Boosting Current Editing Methods  (RQ3)} 

We further evaluate the effectiveness of AnyEdit when applied to several popular model editing methods. Specifically, we replace the locate stage of several locate-then-edit baseline methods with the  autoregressive editing paradigm, enabling the simultaneous identification and editing of multiple tokens' hidden states. The modified methods are evaluated directly on the UnKEBench dataset, and their performance is compared against their original versions. The detailed results are presented in Figure \ref{fig:exp_2}. Furthermore, to evaluate the time cost introduced by incorporating our strategy, we measured the average editing time per sample for each method combined with our approach. The experimental results are presented in Table \ref{tab:exp_2}. Based on these results, we draw the following observation:

\begin{itemize}[leftmargin=*]
    \item \textbf{Obs 5: AnyEdit significantly improves the editing performance of existing methods.}  
    After integrating AnyEdit, current methods achieve notable improvements across all metrics. These results highlight that AnyEdit serves as an effective plug-and-play approach, enhancing the capabilities of existing model editing methods.
    \item \textbf{Obs 6: Incorporating AnyEdit introduces a slight increase in editing time.}  
    Integrating AnyEdit with other model editing methods results in a modest increase in editing time, exhibiting an average relative increase of 24.7\% in editing time. However, given the substantial performance improvements achieved by AnyEdit, this trade-off is considered acceptable.
\end{itemize}


\subsection{Impact of Chunk Size (RQ4)} 
% \begin{figure}[t]
% % \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=1.02\columnwidth]{figures/exp_3.png}}
% \caption{The relationship between AnyEdit's editing performance and chunk size in long diverse-formatted knowledge.}
% \label{fig:exp_3}
% \end{center}
% \vskip -0.3in
% \end{figure}
We conclude by evaluating whether the choice of chunk size impacts the long-form knowledge editing performance of AnyEdit. Specifically, we adopt a sliding window to divide the target text into chunks and vary the chunk size to assess its influence on editing performance. Experimental results show that as the chunk size increases beyond a certain threshold, the editing performance of AnyEdit declines. Due to space limitations, detailed results and analysis can be found in the Appendix \ref{app:exp_result_4}.


\begin{table}[htbp]
\centering
\caption{Comparison of average editing time per sample (seconds) with baseline methods and their enhanced versions integrated with AnyEdit. The `+' symbol denotes integration with our method. Evaluation performed on Counterfact and MQUAKE from the AKEW benchmark suite, along with UnKEBench.}
\label{tab:exp_2}
\begin{tabular}{@{}l S[table-format=2.2] S[table-format=2.2] S[table-format=2.2]@{}}
\toprule
\textbf{Method} & \textbf{UnKEBench} & \textbf{Counterfact} & \textbf{MQUAKE} \\
\midrule
MEMIT          & 16.37 & 17.05 & 16.92 \\
MEMIT+         & 21.14 & 20.43 & 21.28 \\
AlphaEdit      & 17.25 & 18.16 & 17.89 \\
AlphaEdit+     & 22.03 & 21.57 & 22.95 \\
UnKE           & 22.91 & 24.12 & 23.84 \\
UnKE+          & 28.32 & 30.25 & 29.77 \\
\bottomrule
\end{tabular}
\end{table}