\section{Introduction}

\begin{figure*}[t]
    \centering
    \includegraphics[width=1.01\linewidth]{figures/introfjf.png}
    \vspace{-10pt}
    \caption{Comparison of current methods and our AnyEdit. (a) and (d) illustrate the editing processes; (c) and (e) show the editing efficacy as the number of tokens within the to-be-updated knowledge increases; (b) and (f) depict the type of knowledge that each method can edit. Best viewed in color.}
    \label{fig:intro}
\end{figure*}


Large language models (LLMs) have achieved impressive success by learning and storing vast amounts of knowledge \cite{GPT3, gpt2-xl,survey-llm}. However, they often suffer from hallucinations, producing incorrect or outdated information \cite{KE,MEND}. For instance, when queried with ``Where were the latest Olympics held?'', an LLM often provides an outdated response ``Tokyo'',  instead of the correct, updated answer ``Paris''. While retraining or fine-tuning can mitigate these issues, such approaches are resource-intensive and risk overfitting \cite{SERAC,ROME}. To overcome this, \textit{model editing} has emerged as a promising alternative. As illustrated in Figure \ref{fig:intro} (b), it typically begins by constructing a (subject, relation, object) triplet, such as (Olympics, were held in, Paris), to represent the knowledge to be updated. It then follows a locate-then-edit paradigm as Figure \ref{fig:intro} 
 (a) shows: (1) Locate the key token in the input prompt (\eg ``Olympics'') and the influential layers using causal tracing; (2) Edit the hidden states of the key token within these layers to align the model's output with the desired knowledge update (\eg modifying ``Tokyo'' to ``Paris''). This approach enables precise and efficient updates without the need for full-scale retraining or fine-tuning, showing the potential to model dynamic and evolving knowledge.
% Specifically, as shown in Figure 1 (a), it first locates the influential layers and a token in input (\eg ``Olympics'' in ``Where was the latest Olympics held?'') using causal tracing. Then, the token's hidden states in these influential layers is perturbed to edit the LLM’s output (\eg changing ``Tokyo'' to ``Paris''). Following this locate-then-edit paradigm, model editing could enable knowledge updates efficiently and precisely \cite{MEMIT,grace,wise}.

Despite their success, existing model editing methods mostly face significant limitations in the length and diversity of the knowledge they can update. As shown in Figure \ref{fig:intro} (c), even leading methods like AlphaEdit \cite{AlphaEdit} and RECT \cite{RECT} struggle to handle updates exceeding 100 tokens. Worse still, most methods are restricted to knowledge represented as structured (subject, relation, object) triples. However, real-world knowledge is often encoded in diverse formats (\eg mathematical derivations and code snippets as shown in Figure \ref{fig:intro} (f)) and frequently exceeds the 100-token threshold \cite{AKEW}. These constraints are ill-suited for real-world scenarios, significantly narrowing the scope of model editing and hindering its broader advancement.

Here we first conduct an in-depth analysis to identify why current methods fail for \textbf{long-form diverse-formatted knowledge}. Considering Figure \ref{fig:intro} (a) again, existing methods typically rely on locating a single token, assuming that altering its hidden states will suffice to edit the LLM’s output (\ie enabling the model to generate desired outputs reflecting new knowledge). However, long-form diverse-formatted knowledge is inherently more complex and information-dense than a single triplet, often requiring the integration of multiple critical tokens and intricate interdependencies among their hidden states. Thus, altering just a single token's hidden state is often insufficient to ensure consistent and accurate knowledge generation. We formalize this limitation as the \textit{efficacy barrier} of single-token editing\footnote{The ``Efficacy'' is the metric proposed by the model editing method ROME \cite{ROME},  aiming to evaluate the success rate of eding. For more details, please refer to Appendix \ref{app:exp_metric}.}, which is empirically validated in Section \ref{sec:lim}. As a consequence, existing methods remain constrained by the paradigm of locating a single token and, as a result, are unable to overcome the aforementioned limitation.

% Large Language Models (LLMs) have demonstrated remarkable performance across downstream tasks such as natural language understanding and generation \cite{GPT3, gpt2-xl,survey-llm}. Despite this, they often suffer from hallucinations, producing incorrect
% or outdated information \cite{KE,MEND}. While retraining or fine-tuning can mitigate these issues, such approaches are resource-intensive and risk overfitting or catastrophic forgetting \cite{SERAC,ROME}. To address these challenges, \textit{model editing} has emerged as a promising alternative, enabling efficient and precise updates to the knowledge encoded in LLMs \cite{MEND, ROME, GRACE,MEMIT}.

% Recent advancements in model editing has made significant progress in handling structured knowledge represented as triples (\ie in the form of (subject, relation, object) like (Olymics, were held in, Paris)) \cite{ROME, MEMIT, AlphaEdit}. However, a vast amount of real-world knowledge is often encoded as unstructured text, such as lengthy paragraphs found in news articles or scientific papers \cite{AKEW}. Such unstructured knowledge is inherently more complex, information-rich, and devoid of fixed formats, posing significant challenges for current model editing methods. Specifically, most leading editing methods are  based on the locate-then-edit paradigm \cite{ROME, MEMIT, AlphaEdit}, which perform well on structured triples,  but exhibit severe limitations when applied to unstructured knowledge \cite{UnKE,commensense,AKEW}.

% Here we first conduct an in-depth analysis to uncover why the locate-then-edit paradigm, effective for structured knowledge, is infeasible for unstructured knowledge. Specifically, this paradigm locates the influential parameters associated with subject tokens (\eg Olympics) using causal tracing and perturbs their hidden representations to edit the LLM’s output (\eg changing Tokyo to Paris), thereby updating structured knowledge. This process inevitably relies on the structural format of the text. However, decomposing a unstructured text into subjects, relations, and objects to convert it into structured triples is inherently challenging, rendering this process impractical. \cite{UnKE}. Worse still, locating and editing a single token often results in changes that are too minor to compel LLMs to generate the full unstructured text, which is may considerable lengthy and complex. Would editing multiple tokens in input be promising? Unfortunately, interference between these edits also render this approach infeasible \cite{NSE}. 
% % Hence, attempts to bypass structural dependencies (\eg by simply locating the last token) also fail to resolve this limitation \cite{UnKE}.

Hence, a critical question naturally arises for updating long-form diverse-formatted knowledge: ``\textit{Can multiple tokens be jointly located and edited to enable coherent knowledge updates?}'' A straightforward solution is to directly extend single-token editing to multiple tokens, however, it risks interference or conflicts between hidden state perturbations, undermining the coherence of to-be-updated knowledge and causing the performance drop as Figure \ref{fig:intro} (c) showcases. In sight of this, we introduce \textbf{AnyEdit}, an autoregressive editing paradigm that enables collaborative token-level updates. As illustrated in Figure \ref{fig:intro} (d), AnyEdit decomposes long-form knowledge into sequential chunks, treating each as independent sub-knowledge. During editing, we iteratively (1) locate the final token of the current chunk and (2) perturb its hidden states to maximize the likelihood of generating the subsequent chunk. Building on the Chain Rule of mutual information \cite{mutual_information} in Information Theory \cite{information}, we theoretically demonstrate that this autoregressive process ensures the generation of consistent, complete long-form knowledge. AnyEdit offers two key advantages:
(1) Adaptivity: The number of edited tokens can be adaptively adjusted with the knowledge length, avoiding redundant edits.
(2) Generality: It supports diverse knowledge formats (\eg poetry, code, math) by decoupling structure-specific constraints. By shifting from single-token to multi-token collaborative editing, AnyEdit overcomes the efficacy barrier of current methods, demonstrating the potential for practical, precise updates across diverse knowledge formats.


% Inspired by the success of auto-regressive training strategy, we propose \textbf{AutoEdit}, the auto-regressive model editing paradigm that achieves these two goals simultaneously. Specifically, guided by principles from Information Theory \cite{information}, AutoEdit first decomposes lengthy, unstructured text into shorter, manageable chunks using the Chain Rule of Mutual Information \cite{mutual_information}. It then sequentially edits the last token of each chunk, aiming to maximizing the output probability of the subsequent chunk. This auto-regressive editing process transforms the traditional locate-then-edit paradigm (\ie relying on a single subject token in input) into a collaborative effort involving multiple tokens in output, effectively resolving the issues of structural dependency and multi-token coordination. 
% % Based on these, the scope of locate and edit is extended from a single token in the input to multiple tokens in the output, allows for effectively resolving the issues of structural dependency and multi-token coordination. 
% Additionally, this paradigm could adaptively adjust the number of edited tokens based on the text length, avoiding insufficient or excessive edits. This enables AutoEdit to efficiently capture the complex semantics within long texts, further enhancing its capability to update unstructured knowledge. 

To validate AnyEdit, we conduct a comprehensive evaluation against leading model editing methods (\eg MEMIT \cite{MEMIT}, AlphaEdit \cite{AlphaEdit}, and UnKE \cite{UnKE}) on the prevailing LLMs such as Llama3-8B-Instruct\footnote{\href{https://llama.meta.com/llama3}{https://llama.meta.com/llama3}} and Qwen2.5-7B-Chat \cite{qwen2.5}. Beyond standard benchmark datasets (\eg Counterfact \cite{ROME} and ZsRE \cite{MEMIT}) that represent knowledge as triples, we curate \textbf{EditEverything}, a new benchmark for long-form knowledge in diverse formats. As shown in Figure \ref{fig:intro} (f), this dataset includes entries up to 458 tokens --- over twice the length of the longest sequences in existing benchmarks (\eg 156 tokens in AKEW \cite{AKEW}) --- and spans multiple domains, including mathematics, news, code, and biochemistry. Results on EditEverything and standard benchmarks demonstrate that AnyEdit surpasses all baselines, achieving a 21.5\% average improvement in editing accuracy with comparable computational overhead. Furthermore, as the first autoregressive editing framework, AnyEdit also enables seamless integration with existing locate-then-edit methods, as shown in Figure \ref{fig:intro} (e). This plug-and-play capability equips traditional approaches with the ability to handle knowledge with arbitrary length and format, significantly broadening the scope and practicality of LLM knowledge editing.
% endowing them with unstructured long texts editing capabilities. 

% Furthermore, our ablation studies demonstrate the crucial role of the auto-regressive paradigm in ReEdit's performance, suggesting potential applications in broader domains.

% These findings demonstrate that AutoEdit is an effective plug-and-play solution that successfully extends existing locate-then-edit methods to the challenging task of editing unstructured long texts.

% 定位策略，几部依赖于输入结构，又可以打破干扰？

% To further validate ReEdit's capabilities, we conducted extensive experiments on multiple LLMs including LLaMa-3-8B\footnote{\href{https://llama.meta.com/llama3}{https://llama.meta.com/llama3}}, Gemma-2-9B\footnote{\href{https://ai.google.dev/gemma}{https://ai.google.dev/gemma}} and Mistral-7B \cite{mistral7b}. Compared to existing parameter-modifying methods for lifelong editing (\eg MEMIT \cite{memit}, AlphaEdit \cite{alphaedit} and RECT \cite{rect}), ReEdit achieves optimal performance across all evaluation metrics while requiring only 1\% of the computation time on average, including the hypernetwork training overhead. Furthermore, as the first work to model lifelong editing tasks as the reinforcement learning paradigm, our ablation studies demonstrate the crucial role of the reinforcement learning framework in ReEdit's performance, suggesting potential applications in broader domains.