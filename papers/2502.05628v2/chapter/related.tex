\section{Related Work}

\textbf{Model Editing for Knowledge Update.}  
Traditional model editing methods modify a model's knowledge by either altering a small subset of parameters or introducing external memory. These approaches enable knowledge updates, mitigate model hallucination \cite{hallu_edit}, or facilitate information injection \cite{harm_edit}. \textit{Parameter-Modifying Methods} update existing model parameters to encode new knowledge. Meta-learning-based approaches, such as KE \cite{KE}, MEND \cite{MEND} and InstructEdit \cite{InstructEdit}, train hypernetworks to predict parameter updates, with MEND improving efficiency via low-rank gradient decomposition. Locate-then-edit methods, including ROME \cite{ROME} and MEMIT \cite{MEMIT}, use causal tracing to identify knowledge-relevant parameters and update them via least-squares optimization. NSE \cite{NSE} mitigates catastrophic forgetting in model editing by updating only a subset of neuron parameters. AlphaEdit \cite{AlphaEdit} extends this to lifelong editing with a null-space projection strategy. In contrast, \textit{Parameter-Preserving Methods} introduce additional parameters or memory instead of modifying existing ones. ICE \cite{ICE} and DeCK \cite{DeCK} achieve parameter-free model editing through in-context learning, enabling knowledge updates without modifying the model's parameters. SERAC \cite{SERAC} retrieves updates from external memory, while T-Patcher \cite{T-patcher} and CaliNet \cite{calinet} allocate new neurons to encode knowledge. GRACE \cite{grace} replaces hidden states with discrete codebook values, and WISE \cite{wise} integrates parameterized memory for efficient knowledge merging.

\textbf{Unstructured Knowledge Editing.}  
Recent research focuses on editing unstructured knowledge with free text beyond structured triples. \citet{AKEW} highlight the limitations of prior methods in evaluating unstructured text editing and introduce AKEW as a benchmark. UnKE \cite{UnKE} refines locate-then-edit methods by updating all parameters within a single layer, improving their capability to handle unstructured knowledge. They also introduce UnKEBench, a dedicated benchmark for unstructured knowledge editing. \citet{freetext} propose a dynamic perception module to efficiently locate commonsense knowledge parameters, enabling free-text updates. However, while these methods handle unstructured and lengthy knowledge, they remain limited to factual knowledge. In contrast, our work extends editing capabilities to \textit{diverse-formatted knowledge}, which encompasses various textual structures beyond factual statements.


