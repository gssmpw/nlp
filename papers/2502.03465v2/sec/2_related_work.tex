\section{Related Work}


% introduce CoDeF,...
% \noindent\textbf{Dynamic Gaussian Splatting.}
\noindent\textbf{Neural video representation.}  
Casually captured monocular videos are widely available and can be considered as 2D projections of dynamic 3D scenes. Efficient representations of these videos are crucial for various computer vision tasks, including video object segmentation, object tracking, depth estimation, and frame interpolation.
%
Early works~\cite{chen2021learning, sitzmann2020implicit, tancik2020fourier, ouyang2024codef} leveraged implicit neural representations, modeling images through coordinate-based Multilayer Perceptrons (MLPs) through per-image optimization. Later works expanded upon this by incorporating learnable deformation field~\cite{ouyang2024codef, wang2023tracking, liu2023robust, ye2022deformable}, reconstructing videos as canonical and deformable MLPs. However, these methods often struggle to capture complex motions due to the absence of explicit 3D structure. Recently, explicit Gaussian video representations~\cite{som2024, sun2024splatter, wang2024gflow, lei2024mosca} have emerged to address these limitations, representing videos explicitly as Gaussians in a 3D canonical space, each associated with a 3D motion projected onto the 2D video frames. But these methods still require computationally expensive per-video optimization, limiting their practical application.
%
In contrast, our NutWorld introduces a novel feed-forward Gaussian video representation, distinguishing itself from previous approaches. The NutWorld network, trained on video datasets, efficiently reconstructs videos as Gaussians through a single forward pass, delivering improved reconstruction quality with substantial speedup.





\noindent\textbf{Feed-Forward Gaussian Splatting.} Recent advance in large-scale 3D scene datasets~\cite{ling2024dl3dv, zhou2018stereo, liu2021infinite} has enabled feed-forward Gaussian approaches~\cite{pixelsplat, chen2025mvsplat, zhang2024transplat, zhang2025gs, wewer2024latentsplat,yi2024mvgamba,shen2024gamba}, which excel in efficiency and sparse view reconstruction. PixelSplat~\cite{pixelsplat} and LatentSplat~\cite{wewer2024latentsplat} employ the epipolar transformer~\cite{he2020epipolar, wang2022mvster} to establish cross-view correspondences and predict 3D Gaussians from multi-view image features, while MVSplat~\cite{chen2025mvsplat} utilizes cost volumes to jointly predict depth and Gaussian parameters. Alternatively, GS-LRM~\cite{zhang2025gs} and Long-LRM~\cite{ziwen2024long} consider Gaussian Splatting reconstruction as a sequence-to-sequence translation task, employing transformer-based~\cite{vaswani2017attention} or hybrid~\cite{mamba2} architectures to regress Gaussian primitives. 
%
However, these feed-forward reconstruction methods designed for static 3D scenes have limitations when generalized to unconstrained videos, primarily because accurate per-frame camera poses cannot be obtained even through advanced pose estimation methods~\cite{wang2024vggsfm, wang2024dust3r, zhang2024monst3r, mast3r}. To address this, we introduce a canonical camera space for building our NutWorld model, enabling robust 3D representation of dynamic scenes.

\label{sec:formatting}

