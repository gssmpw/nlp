\section{Experiment}
\label{sec:exp}

% \yxy{All experimental settings need to be cleared, for example, how many frames are used, resolution, samples}
% In this section, we conduct extensive qualitative and quantitative experiments to underscore the robustness and versatility of our proposed NutShell. 

% We also provide detailed information on the dataset setup, implementation details and experimental protocol.

\begin{figure*}[t]
\setlength{\abovecaptionskip}{0.2cm}
\setlength{\belowcaptionskip}{0cm}
\centering 
\includegraphics[width=0.9\linewidth]{img/NutShell_downstream.pdf} 
\vspace{-8pt}
\caption{Qualitative results in various downstream tasks, including video segmentation, editing, frame interpolation and consistent depth estimation. More visualization results for each task are presented in \textit{Appendix}.}  
\label{fig:exp2} 
\vspace{-6mm}
\end{figure*}




\subsection{Experimental Setup}

% We conduct experiments to underscore the robustness and versatility of our proposed method. Our representation is robust with a variety of deformations, encompassing rigid and non-rigid objects, as well as complex scenarios such as smog.
\noindent \textbf{Training Dataset.} 
% \sqh{need to be confirmed before Sat and make all data ready}
NutWorld is pre-trained on MiraData~\cite{ju2024miradatalargescalevideodataset} and RealEstate10K~\cite{zhou2018stereo}. MiraData is a high-quality video dataset consisting primarily of 3D engine-generated scenes and movie clips with diverse motion patterns. The RealEstate10K dataset contains indoor house tour videos that showcase various architectural scenes and camera movements.
~\footnote{Unlike previous Generalizable 3DGS approaches~\cite{chen2025mvsplat, pixelsplat, zhang2024transplat}, we treat RealEstate10K as a pure video dataset rather than a multi-view 3D scene dataset, thus not utilizing the COLMAP-calibrated camera poses}. 
During pre-processing, we segment the original videos into video cubes, each containing 10 consecutive frames as the basic processing unit. Detailed information on the description of the dataset, preprocessing, and splitting of trains is provided in the \textit{Appendix}.

% The RealEstate10K dataset consists of home tour videos, providing a wealth of scenes and a variety of viewpoint changes. Unlike the previous feed-forward 3DGS settings~\cite{xx,xx},xx.


% \noindent \textbf{Implementation Details.} NutShell was trained on 32 NVIDIA A100 (80GB) GPUs with a batch size of 256 for approximately 4 days. To enhance computational efficiency during both training and inference, we incorporated Flash-Attention-v2~\cite{xx}, gradient checkpointing~\cite{xx}, and mixed-precision training~\cite{xx} using BF16 data format. The model was optimized using AdamW with 0.05 weight decay and cosine learning rate decay, using $5 \times 10^{-4}$ for the static stage and $5 \times 10^{-5}$ for the dynamic stage, respectively. \yxy{resolution, loss weight, input views]}


% \noindent \textbf{Architecture Details.} \yxy{may in \textit{Appendix}}

% camera coordination 
% to report the number of Gaussians, here we need syntax including, input resolution, frames number, patch number, layer number, umsampler block number n, decoder ratio 


\noindent\textbf{Implementation Details.} NutWorld is trained on 32 NVIDIA A100 (80GB) GPUs with a batch size of 256 for around 4 days. 
To improve computational efficiency, we integrate Flash-Attention-v2~\cite{dao2022flashattention, dao2023flashattention}, gradient checkpointing~\cite{sohoni2019low}, and mixed-precision training with BF16~\cite{zamirai2020revisiting}.
The orthographic camera coordinates are bounded to $[-1, 1]$ along the $x$ and $y$ axes and to $[0, 1]$ along the $z$ axis. The input frames are resized to $512 \times 288$ to preserve the aspect ratio.
% With a transformer patch size of $16$ and $n=2$ upsampler blocks, our model processes $K=6$ input frames to produce feature maps of spatial resolution $(U, V) = (128, 72)$, generating $55,296$ dynamic Gaussians.
We adopt a two-phase training strategy: a static phase using single frames ($K=1$) with window size $\mathcal{W} = 576$, followed by a dynamic phase where we initialize from the static weights and expand the window to $\mathcal{W} = 3456$ to allow spatio-temporal attention in the hierarchical upsampler. Additional training details, including hyper-parameters and network architecture, are provided in the \textit{Appendix}.

\subsection{Video Reconstruction}


%由于optimize-based methods 需要花费的时间很多，所以我们只挑选了几段视频作为比较。

\noindent \textbf{Experimental Protocol.} We evaluated the video reconstruction performance of NutWorld on 50 randomly selected test video clips from RealEstate10K and MiraData, both with a default length of 90 frames via standard reconstruction quality metrics (PSNR, SSIM, and LPIPS~\cite{zhang2018perceptual}). As there are currently no other feed-forward dynamic Gaussian approaches, we compared with optimization-based methods including Splatter-a-Video (SaV)~\cite{sun2024splatter}, 4DGS~\cite{wu20244d}, RoDynRF~\cite{liu2023robust} and CoDeF~\cite{ouyang2024codef} as the most relevant baselines. For fair comparison, all methods incorporate the confined canonical space, depth and flow supervision. We used the official implementations for most methods while SaV was reproduced according to the implementation details provided in their paper. 

% We made our best effort to ensure optimal performance of all baseline implementations.
\noindent \textbf{Comparison with Baselines.}
We evaluate NutWorld's representation effectiveness through both qualitative and quantitative experiments on video reconstruction.
As shown in Figure~\ref{fig:main-comparison}, our pre-trained NutWorld effectively captures spatial details and temporal dynamics, outperforming both Gaussian-based SaV~\cite{sun2024splatter} and NeRF-based CoDeF~\cite{ouyang2024codef} in reconstruction quality. This superior performance can be attributed to STAG's elaborated deformable field and positional constraints, which provide more expressive and robust temporal modeling capabilities compared to SaV's Fourier series and CoDeF's 2D canonical representation.
Furthermore, as evidenced in Table~\ref{tab:main-comparison}, NutWorld achieves the best of both worlds in reconstruction quality and computational efficiency. Notably, 
NutWorld reconstructs a 90-frame video in just $1.8$ seconds, achieving a $1000\times$ speedup over optimization-based methods. Equipped with segment-based inference that limits the number of Gaussians per segment, NutWorld achieves a rendering speed of 450 FPS, substantially surpassing SaV's 149 FPS, which requires around $2 \times 10^6$ Gaussians for the same video.




% Compared to SaV, which also employs dynamic Gaussian Splatting, our approach offers superior reconstruction quality with significantly faster rendering. 
% The efficiency stems from our segment-based inference strategy that allocates a bounded number of Gaussians per frame segment, in contrast to SaV's use of approximately $2 \times 10^6$ Gaussians for 90 frames.  


% beyond quality: gs-based 比nerf的fps更好，我们比gs- based fpse等是因为我们的点数更少; 从inference time来说 ...
% quality: 结合contribution suprisingly分析一下为什么我们比SaV好, （1）表征相对于傅立叶表征有优势； （2）large scale pre-training, 对于motion的系统建模会更加鲁棒？ (3)...


\subsection{Video Downstream Tasks} 

Large-scale pretrained NutWorld empowers video applications including object segmentation, frame interpolation, video editing, novel view synthesis, and consistent depth prediction. We present representative qualitative results in Figure~\ref{fig:exp2}, with additional results provided in the \textit{Appendix}.
% \yxy{put methods together with results}

\noindent \textbf{Video object segmentation.} The explicit nature of STAG allows us to propagate object masks in a certain frame to subsequent frames. Specifically, the corresponding STAGs in the object mask can be explicitly selected in the first frame, following previous training free Gaussian segmentation methods~\cite{flashsplat, hu2024semantic}. As visualized in following Fig.~\ref{fig:point-track}, NutWorld predicts coherent deformation $u_i(t)$ for each Gaussian over time, object masks in subsequent frames can be rendered from the STAGs selected initially.
%
\begin{figure}[h!]
\vspace{-4mm}
\setlength{\abovecaptionskip}{0.1cm}
\setlength{\belowcaptionskip}{0cm}
\centering
\includegraphics[width=0.85\linewidth]{img/point-tracking.pdf}
\caption{Visualization of Gaussian trajectories. Trajectories of selected Gaussian centers are illustrated as point tracks.}
\vspace{-4mm}
\label{fig:point-track}
\end{figure}
%
In particular, this capability emerges as a by-product without specific training~\cite{sun2024splatter, wang2024gflow} in video Gaussian representation. 



\noindent \textbf{Frame interpolation.} The continuous trajectories learned for STAGs, regularized by calibrated optical flow, enable temporal interpolation of scene dynamics at arbitrary FPS. These interpolated STAGs, with smoothly varying dynamic attributes, facilitate intermediate frame rendering, a capability beyond the scope of per-frame methods~\cite{ren2024l4gm}.


\noindent \textbf{Consistent depth prediction.} The calibrated depth regularization prevents depth collapse while maintaining temporally coherent spatial configurations in scene geometry. Additionally, NutWorld demonstrates potential for distilling other image features, such as SAM~\cite{kirillov2023segment} and CLIP~\cite{radford2021learning}, which we consider a promising direction for future work.



\noindent \textbf{Video editing.} By integrating with a MLLM-guided editing model~\cite{fu2023guiding}, NutWorld enables precise frame-level painting and stylization by optimizing the sliced STAG representation. These edits propagate temporally while maintaining visual coherence throughout the video sequence. The visualization results are provided in the \textit{Appendix}.

\noindent \textbf{Novel view synthesis.} NutWorld achieves novel view synthesis within practical bounds by incorporating depth priors to mitigate spatial ambiguity. Camera extrinsic adjustment enables novel view rendering, while camera intrinsic manipulation allows for effects such as dolly zoom. Please refers to \textit{Appendix} for the visualization results.



% 1. 纯video task上，展示视频重建的效果:
%baseline: CoDeF, Splatter-a-video, 4DGS?; table进行数值比较

% 2. 表征可以很好的描述camera 和 motion的变化。downstream task 大图包括：1. nvs 2. frame interpolation 3. segmentation 4. video editting 5.point tracking (or consistent depth)


% ablation: 1. flow loss 和 depth loss的albation； table进行数值比较
% ablation: 2. representation 的ablation？--->是不是可以放基于optimize结果的