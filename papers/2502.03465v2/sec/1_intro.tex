
\vspace{-3mm}
\section{Introduction}

% 1: 第一句话直接点出我们要做的事情，introduce a novel video Gaussian representation; video processing的重要性和应用。
%2: 过往的方法大多建立在2D/2.5D的技术，存在的局限性: occluasion, 缺少空间关系等； Drawing inspration from the fact that video is a projection of dynamic 3D world; On the other hand, with 3D Gaussian spaltting advancing real-time and high-fidelity rendering enable us XXX. we ask: Is it possible to efficiently represent a video in intrinsic 3D form without per-scene optimization? --->这个地方是并列还是递进需要捋一捋思路。

% 现有方法（2D/2.5D）的局限 
% → 视频是3D世界的投影这一本质特性
% → 3D Gaussian Splatting在静态场景重建的成功
% → 我们的思考：能否将这种表征应用到动态视频？

%3: 但是使用Guassian Splatting作为single forward pass 视频表征会带来challenge (1) unpose, 无法disentangle motion和camera的变化 (2) 非结构化---> reprensetation, feed-forward network（3）深度问题: refers to splatter-a-video.

%4: To tackle the above challenge. (1) We first introduce a spatial-temporal aligned Gaussian representation, with 相机归一化 和正交投影 (2) optimization-free pipeline that enables fast reconstruction of videos into the above spatial-temporal aligned Gaussian representation. 介绍网络架构, relyed on the current LRMs with spatial,temporal attention; 介绍训练策略，depth和flow的distillation， 2D priors.---> 注意一一对应解决challenge.
% 5: 我们在real estate 和 mira数据集上验证了 STAG representation的可行性；and other downstream task support. 


% In this paper, we takes a significant step in the realm of explicit geometry and motion estimation for dynamic scenes captured as a monocular video.

% Accurately and efficiently modeling dynamic scenes and motions is considered so challenging a task due to temporal dynamics and motion complexity.

% \textcolor{blue}{[1. point out the task briefly: introduce a xxx representation for casual monocular videos; the importance of video processing]}

% 要重点提下feed-forward

% We consider the problem of efficiently representing casual monocular videos in a spatially- and temporally-coherent manner. 
Our natural world exhibits inherent dynamism ----from rustling leaves swaying in the wind to clouds drifting across the sky and ocean waves rolling along the shore---where objects maintain their structural integrity while undergoing continuous spatiotemporal evolution. A fundamental objective in video processing~\cite{tekalp2015digital,bovik2009essential} is to enable machines to interpret such dynamic information, allowing recovery of both object geometry and motion patterns while preserving the spatiotemporal coherence intrinsic to human perception.
This capability is crucial for numerous applications, ranging from autonomous driving~\cite{gao2024vista,li2024vdg} and robotics~\cite{huang2023voxposer,huang2024rekep,o2023open} to augmented reality and content creation~\cite{sun2024sora,kondratyuk2023videopoet,bao2024vidu,yi2024diffusion}, where an accurate understanding of dynamic scenes directly impacts system performance and user experience.




% \textcolor{blue}{[Current methods build on 2D / 2.5D techniques have several challenges struggle to handle]}
% 这个地方有点trickly，可能要细化一下基于spatiotemporal pixel的视频表征在什么具体的
% 1. struggle with occlusions and complex scenes 2.fragmented temporal coherence 3. geometric inconsistency across frames
Current neural-based video representations~\cite{ouyang2024codef, sitzmann2020implicit, ye2022deformable} predominantly rely on 2D and 2.5D techniques, treating videos as collections of spatiotemporal pixels~\cite{feichtenhofer2019slowfast, zhang2019comprehensive}. Although such discrete representations allow for basic temporal modeling through pixel matching~\cite{sun2018pwc} and tracking~\cite{karaev2023cotracker, ye2022ostrack}, they struggle to capture complex scene dynamics and maintain appearance consistency, particularly in scenarios involving occlusions and non-rigid deformations~\cite{newcombe2015dynamicfusion, ouyang2024codef}. Moreover, they inherently lack explicit 3D understanding, leading to unreliable motion captures and spatial arrangements.
% depth estimation~\cite{ranftl2021vision, zhang2018joint} and 



Drawing inspiration from the fact that a monocular video is a projection of the dynamic 3D world, we ask: \textit{Can videos be represented in a canonical 3D space form without per-scene optimization}? Recent advances in dynamic Gaussian Splatting~\cite{kerbl20233d} have shown remarkable capabilities in dynamic scene reconstruction~\cite{yang2024deformable, wu20244d, duan20244d} and video Gaussian representation~\cite{sun2024splatter, wang2024gflow, lei2024mosca}, achieving high-fidelity rendering with explicit 3D representations, albeit through per-scene optimization.
%
By modeling videos as flows of Gaussian primitives over time, we overcome the limitations of 2D representations and enable video Gaussian representation without per-scene optimization. As illustrated in Figure~\ref{fig:intro}, this paradigm treats space-time holistically and offers key advantages: each Gaussian acts as a flexible building block that adapts to local appearance structures, enabling seamless representation of complex scenes. Moreover, when endowed with dynamic attributes, these structured Gaussians naturally approximate the underlying spatiotemporal volume of monocular videos, capturing intrinsic motions with temporal consistency and facilitating various downstream tasks such as object segmentation~\cite{flashsplat, sun2024splatter}, video editing~\cite{ceylan2023pix2video, qi2023fatezero}, and frame interpolation~\cite{dong2023video, lu2022video}.

% By approximating a scene's underlying spatiotemporal 4D volume~\cite{ref18} via dynamic Gaussian Splatting

%
% in caption: where scene dynamics at each timestamp can be viewed as 4D spatial-temporal Gaussian ellipsoids sliced with different time queries~\cite{xx}



% However, mapping casually captured monocular videos to dynamic Gaussian representations in a feed-forward manner presents several challenges: 
However, transforming casually captured monocular videos to dynamic Gaussian representations \textit{instantly (in seconds per frame)} poses several challenges: 
\textbf{ (1) Unposed inputs}. Gaussian Splatting~\cite{kerbl20233d} and its variants~\cite{yu2024mip, yu2024gaussian, huang20242d} heavily rely on accurate camera poses obtained through Structure-from-Motion (SfM)~\cite{ullman1979interpretation, schonberger2016structure}, which are typically unavailable for casual monocular videos. Without such pose guidance, current methods~\cite{Fu_2024_CVPR, bian2023nope, lin2021barf, wang2021nerf} fail to disentangle camera motion from scene dynamics, resulting in deteriorated rendering quality and even collapsed reconstruction. 
\textbf{(2) Nonstructural Nature}. Most existing dynamic Gaussian Splatting either leverage per-scene optimized deformation networks~\cite{yang2024deformable, wu20244d, duan20244d, katsumata2025compact, kratimenos2025dynmf, yang2023real} or adopt per-frame Gaussian generation~\cite{ren2024l4gm} for dynamics modeling, both incompatible with our feed-forward prediction paradigm. 
The spatially unstructured property of Gaussian primitives further makes them prone to local minima in inverse rendering~\cite{zhu2025fsgs, chung2024depth}, impeding feed-forward modeling of dynamic underlying structures in monocular videos.
\textbf{(3) Spatial Ambiguity}. The absence of multi-view supervision and initialization from SfM points significantly limits the spatial modeling capability of Gaussian Splatting, leading to ambiguous scale, depth collapse, and inconsistent spatial arrangements in reconstructed scenes.

% (1) unpose (2) not structural for feed-forward prediction, spatially unstructured
% nature presents a significant challenge when applied to mainstream feed-forward frameworks (3) loss 3D information during 3D-2D projection]}


 % 但是使用Guassian Splatting作为single forward pass 视频表征会带来challenge (1) unpose, 无法disentangle motion和camera的变化 (2) 非结构化---> reprensetation, feed-forward network（3）深度问题: refers to splatter-a-video，--> prior


% \textcolor{blue}{[Introduce our basic solutions.]} 
To address these challenges, we introduce \textit{NutWorld} to efficiently transform monocular videos into dynamic Gaussian Splatting representations in a \textit{single forward pass}. Our method has three key components: (1) A structured spatial-temporal aligned Gaussian (STAG) representation in a canonical space (Section~\ref{stag}), enabling feed-forward prediction with pose-free, scale-invariant modeling. (2) An feed-forward pipeline (Section~\ref{NutShell}) that learns spatial-temporal correspondences and  motions across frames, swiftly transforming them into STAG representations. (3) Depth and flow regularization (Section~\ref{prior}), leveraging calibrated depth~\cite{chen2025video, yang2024depth} and optical flow priors~\cite{xu2023unifying} to resolve spatial ambiguity and motion-appearance entanglement in the ill-posed monocular setting~\cite{sun2024splatter, som2024}. With large-scale pre-training, \textit{NutWorld} processes arbitrarily long videos while preserving spatial-temporal consistency through segment-based inference (Section~\ref{train}).


% (1) We first introduce a spatial-temporal aligned Gaussian representation (2) optimization-free pipeline that enables fast reconstruction of videos into the above SPAG. (3) depth and flow priors.]


%potential advantages:
% Gaussians provide continuous probability distributions in space
% Structural Representation: Each Gaussian can represent meaningful local structures
%

% 
% More concretely, \yxy{More detailed implementation. 1. Detailed design of STAG; 2. network architecture 3. how to distill and 4. two-stage training}.
% % guiding the methods part



We performed qualitative and quantitative experiments on RealEstate10K~\cite{zhou2018stereo} and MiraData~\cite{ju2024miradatalargescalevideodataset} to verify the efficacy of \textit{NutWorld} in video reconstruction. Moreover, our method demonstrates real-time inference speed and flexibility across various downstream tasks, including novel view synthesis, consistent depth estimation, video segmentation, video editing, and frame interpolation, indicating its potential as a general-purpose video representation framework. Our main contributions include:
\begin{itemize}[leftmargin=*]
\item We present the \textit{first} framework to efficiently represent world dynamics in casually captured monocular videos via dynamic Gaussian Splatting in a \textit{single forward pass}.
% enabling spatially and temporally coherent processing.

\item Our NutWorld framework incorporates the STAG representation, elaborated network for feed-forward reconstruction, and effective regularization strategies for spatial and temporal coherent recovery from casual videos.

% \item Extensive experiments on several video downstream tasks demonstrate that NutWorld is a potentially groundbreaking tool in video processing.
\item Extensive experiments on video reconstruction and various downstream tasks confirm the spatial-temporal coherence and versatility of NutWorld.
\end{itemize}


% 1. 纯video task上，展示视频重建的效果，表格 xjb；frame interpolation

% 2. 我们首先先验证了表征可以很好的描述camera 和 motion的变化。 Re10K上表现novel-view 表格：表征的比较--> 4dgs, other dynamic gaussians.   -> 纯optimize-based. downstream task 1 NVS

% 3. Depth 和 flow的visualize，展示的目的是说明我们这种feed-forward 表征学习可以很好的融合2D prior ---> downstream task 2 fusion of 2D prior

%  4. tracking or editing (2 choose 1)


% convert everyday monocular videos of dynamic scenes into reconstructions
% how to feed-forward, how to tackle the challenges

% technical contribution 

% 1. propose first feed-forward 4D reconstruction framework from web (causal) videos. 
% 2. to accomodate this, a transformer-based network architecture, and a spatial temporal aligned dynamic gaussian splatting representation.
% 3. depth/flow supervision distillation.


\label{sec:intro}

