\clearpage

\maketitlesupplementary
\setcounter{page}{1}
\setcounter{section}{0}


\noindent The \textit{Appendix} is organized as follows:
\begin{itemize}[leftmargin=*]
    \item \textbf{Appendix~\ref{app:imp}:} provides additional details about the Nutworld pipeline, including the implementation of orthographic rasterization, Gaussian decoder, flow prior calibration, and segment-based inference.
    
    \item \textbf{Appendix~\ref{app:model}:} provides further details on experiment design and model configuration.
    
    % \item \textbf{Appendix~\ref{app:down}:} offers further experimental settings and implementation details for the video downstream tasks.

    \item \textbf{Appendix~\ref{app:result}:} presents additional experimental results on video reconstruction and downstream tasks.

        \item \textbf{Appendix~\ref{app:limit}:} discuss the limitations and potential future directions of NutWorld.

    % \item \textbf{Appendix~\ref{app:license}:} lists the licenses for datasets and pre-trained models.
\end{itemize}


\section{Implementation Details}
\label{app:imp}

\noindent\textbf{Orthographic rasterization.}
Orthographic projection is leveraged to circumvent the necessity for explicit camera pose estimation in our setting. 
%
Specifically, we employ a fixed orthographic camera model and modify the EWA projection~\cite{zwicker2002ewa} from perspective to orthographic in Gaussian rasterization. The EWA projection in original rasterization is formulated as:
\begin{equation}
\Sigma' = J W \Sigma W^T J^T,
\end{equation}
where \(J\) represents the Jacobian matrix of the projective transformation. In the case of perspective projection, the Jacobian \(J\) is formulated as:
%
\begin{equation}
\begin{aligned}
(u, v) &= \left( f_x \cdot x / z + c_x, f_y \cdot y / z + c_y \right), \\
J = \frac{\partial(u, v)}{\partial(x, y, z)} &= 
\begin{pmatrix}
f_x / z & 0 & -f_x \cdot x / z^2 \\
0 & f_y / z & -f_y \cdot y / z^2
\end{pmatrix}.
\end{aligned}
\end{equation}
In contrast, for orthographic projection, the EWA projection is modified as follows:
\begin{equation}
\begin{aligned}
(u, v) &= \left( f_x \cdot x + c_x, f_y \cdot y + c_y \right), \\
J = &\frac{\partial(u, v)}{\partial(x, y, z)} = 
\begin{pmatrix}
f_x & 0 & 0 \\
0 & f_y & 0
\end{pmatrix}.
\end{aligned}
\end{equation}
In this formulation, the near and far planes are set to $0$ and $1$, respectively, constraining the $z$ axis within this range. Additionally, the $x$ and $y$ axes are constrained to $[-1, 1]$ to facilitate structured prediction.


% This formulation allows us to bypass challenges related to camera pose estimation and intrinsic optimization, which are often computationally expensive and ill-conditioned. To the best of our knowledge, this is the first such formulation in the context of 


\noindent\textbf{STAG Parameterization.}
The parameterization of output parameters significantly impacts the model's convergence, despite STAG provides a relatively structured Gaussian representation. For reproducibility, we provide detailed configuration of the STAG decoder parameterization in Table~\ref{tab:act}. Common activation functions such as $\text{sigmoid}$, $\text{softplus}$, and $\text{normalize}$ are employed for most static Gaussian attributes, following previous works~\cite{pixelsplat, xu2024grm, tang2025lgm}. 
%
For the spatial aligned 3D Gaussian position, we predict it as $\mu = (u + \Delta_x, v + \Delta_y, d)$, where $u$ and $v$ are the aligned 2D pixel positions within the orthographic space, and both $\Delta_x$ and $\Delta_y$ are constrained using the $\tanh$ activation. For the depth value $d$, the $z$ axis in $[0, 1]$ is divided into 20 discrete bins. A discrete distribution is predicted over these bins, and the expectation is computed to robustly predict the depth value $d$.
%
For the deformable fields $\mu(t)$, the query timestamp $t$ is represented as $L=10$ sinusoidal expansions. The output of $\mathcal{F}_{\theta}$ remains unbounded, allowing invisible Gaussians to be driven out of the view space as needed.



\begin{table}[h]
\centering
\renewcommand{\arraystretch}{1.3}
\begin{tabular}{l|l}
\toprule
\rowcolor[HTML]{EFEFEF} \textbf{Attribute} & \textbf{parameterization} \\
\midrule
Position (static) & $\begin{array}{l}
x,y: \text{map} + \text{max} \cdot \tanh(\cdot) \\

\end{array}$ \\
\midrule
Position (dynamic) & unbounded \\
\midrule 
Depth & bin regression with \text{softmax} \\
\midrule
Scale & $0.1 \cdot \text{softplus}$ \\
\midrule
Rotation  & $\text{normalize}$ \\
% \midrule
% Rotation (dynamic) & $\text{normalize}$ \\
\midrule
Opacity & $\text{sigmoid}$ \\
\midrule
RGB & $\text{sigmoid}$ \\
% \midrule
% SH coefficients & none \\
\midrule
\bottomrule
\end{tabular}
\caption{Detailed STAG parameterization.}
\label{tab:act}
\end{table}


\noindent\textbf{Flow Prior Calibration.} The estimated global optical flow between video frames often contains noise, which can hinder model convergence during training. To mitigate these noises, we employ a calibration strategy for the flow regularization loss $\mathcal{L}_{\text{flow}}$. Specifically, when a STAG moves out of the view space, we set the corresponding flow mask value $M_{i, k} = 0$. Additionally, we filter out flows with the top $20\%$ motion magnitudes as outliers. With these adjustments, the calibrated flow regularization loss is expressed as:
\begin{equation}
\mathcal{L}_{\text{flow}} = \sum_{i=1}^{K \times U \times V}\sum_{k=0}^{K-1} M_{i, k}\|\bar{\mu}_{i}(t_k) - \mu^*_i(t_k)\|_1,
\label{eq:flow_cal}
\end{equation}
This calibration ensures that only reliable motion information contributes to the training process, reducing the impact of noise and extreme outliers, encouraging the model to learn coherent motion patterns effectively.



\noindent\textbf{Segment-based inference.} 
%
We adopt a segment-based inference approach to transform long casual videos into STAG representations. This strategy processes video sequences using a sliding window mechanism, where each window contains one overlapping frame. Temporal coherence between segments is maintained through spatially aligned Gaussians in the overlapping frames shared by adjacent segments. The coherence is achieved through token-wise correspondence, as spatial positions in STAG directly correspond to identical pixel locations.
%
Through quantitative comparison shown in Table.1 in the manuscript, we further demonstrate that this segment-based inference strategy not only efficiently processes video segments in parallel but also maintains a manageable number of Gaussians for each segment due to the slicing window design. In contrast, SaV~\cite{sun2024splatter} directly uses millions of Gaussians to represent entire frames, resulting in significantly longer rendering times per frame.




\section{Experiment Configuration}
\label{app:model}
\noindent\textbf{Network Configuration}. As illustrated in Table~\ref{tab:conifg}, the transformer-based encoder processes input frames at $288 \times 512$ resolution, with concatenated RGB and depth channels. The architecture comprises 10 attention layers with 12 heads and 768-dimensional features, operating on $16 \times 16$ patches. The hierarchical upsampler incorporates 3 blocks, each containing 2 attention layers with a window size of 3456. Through these blocks, the channel dimension progressively decreases from 768 to 64, while spatial resolution doubles at each stage. With $n=2$ upsampler blocks, our model processes $K=6$ input frames to produce feature maps of spatial resolution $(U, V) = (128, 72)$, generating $55,296$ dynamic Gaussians. The STAG decoder implements a lightweight MLP with a single 1024-dimensional hidden layer, utilizing attribute-specific activation functions: linear for position, $\tanh$ for dynamics, $Softplus$ for scale, normalization for rotation, and $Sigmoid$ for opacity and color.

% \noindent\textbf{Gaussian Parameterization.} The parameterization of output parameters significantly influences the model's convergence, despite STAG being a relatively structured Gaussian representation. For reproducibility, we provide detailed pseudo code of STAG parameterization in Table~\ref{tab:act}.

% \begin{table}[h]
% \centering
% \renewcommand{\arraystretch}{1.3}
% \begin{tabular}{l|l}
% \toprule
% \rowcolor{gray!10} \textbf{Attribute} & \textbf{parameterization} \\
% \midrule
% Position (static) & $\begin{array}{l}
% x,z: \text{map} + \text{max} \cdot \tanh(\cdot) \\

% \end{array}$ \\
% \midrule
% Position (dynamic) & $\tanh$ \\
% \midrule
% Scale & $0.1 \cdot \text{softplus}$ \\
% \midrule
% Rotation (static) & $\text{normalize}$ \\
% \midrule
% Rotation (dynamic) & $\text{normalize}$ \\
% \midrule
% Opacity & $0.05 + 0.95 \cdot \text{sigmoid}$ \\
% \midrule
% RGB & $\text{sigmoid}$ \\
% \midrule
% SH coefficients & none \\
% \bottomrule
% \end{tabular}
% \caption{Detailed Gaussian parameterization. }
% \label{tab:act}
% \end{table}

\noindent\textbf{Dataset Settings.} NutWorld is pre-trained on MiraData~\cite{ju2024miradatalargescalevideodataset} and RealEstate10K~\cite{zhou2018stereo}. For MiraData, we utilize its middle version containing approximately 34K video clips, each with a duration of about 2 minutes. To ensure data quality, we performed rigorous filtering by removing clips containing gaming pop-ups, black screens, and other artifacts, resulting in a curated dataset of 22K video clips~\cite{yi2023invariant}. The dataset is split randomly with a 95:5 ratio for training and testing. For RealEstate10K, unlike previous Generalizable 3DGS approaches~\cite{chen2025mvsplat, pixelsplat, zhang2024transplat}, we treat it as a pure video dataset rather than a multi-view 3D scene dataset, without utilizing COLMAP-calibrated camera poses. This dataset is similarly split with a 95:5 training-testing ratio. For evaluation, we randomly selected 40 videos from the MiraData test set and 10 from the RealEstate10K test set for both qualitative and quantitative comparison.
\begin{table*}[t]
\centering
\caption{Model Configuration of NutWorld}
\label{tab:modelconfig}
\vspace{-6pt}
\renewcommand{\arraystretch}{1.2}%
\begin{tabular}{>{\bfseries}p{0.4\linewidth} p{0.5\linewidth}}
\toprule
\rowcolor[HTML]{EFEFEF} 
\textbf{Parameter} & \textbf{Value} \\
\midrule
\multicolumn{2}{l}{\textbf{Encoder}} \\
\quad Input Resolution & $288 \times 512$ \\
\quad Input Channels & RGB (3) + Depth (1) \\
\quad Patch Size & $16$ \\
\quad Hidden Dimension & $768$ \\
\quad Attention Layers & $10$ \\
\quad Attention Heads & $12$ \\
\quad Window Size & $3456$ \\
\quad Positional Embedding & None \\
\midrule
\multicolumn{2}{l}{\textbf{Upsampler Block}} \\
\quad Scale Factor & $2$ \\
\quad Decoder Ratio & $2.0$ / $3.0$ \\
\quad Channel Width Decay & $768 \rightarrow 64$ \\
\quad Attention Layers per Block & $2$ \\
\quad Window Size & $3456$ \\
\quad Number of Blocks & $4$ \\
\midrule
\multicolumn{2}{l}{\textbf{STAG Decoder}} \\

\quad MLP Dimension & $1024$ \\
\quad Hidden Layers & $1$ \\
\quad Position Activation & None \\
\quad Dynamic Activation & $tanh$ \\
\quad Scale Activation & $Softplus$ \\
\quad Rotation Activation & $Normalize$ \\
\quad Opacity Activation & $Sigmoid$ \\
\quad Color Activation & $Sigmoid$ \\

\midrule
\multicolumn{2}{l}{\textbf{Training Details}} \\
\quad Optimizer & AdamW \\
\quad Learning Rate & $1 \times 10^{-4}$ \\
\quad Weight Decay & $0.05$ \\
\quad Warm-Up Ratio & $0.04$ \\
\quad Batch Size & $4$  \\
\quad Number of GPUs & $32$ \\
\quad Total Epochs & $100$ \\
\quad Input Frames & $6$ \\
\quad Output Frames & $10$ \\
\quad Mixed Precision & bf16 \\
\midrule
\multicolumn{2}{l}{\textbf{Loss Weights}} \\
\quad $\lambda_{\text{MSE}}$ & $0.5$  \\
\quad $\lambda_{\text{depth}}$ & $0.001$  \\
\quad $\lambda_{\text{flow}}$ & $0.2$ \\
\bottomrule
\end{tabular}
\label{tab:conifg}
\vspace{-12pt}
\end{table*}



\section{More Experiments Results}

Due to limited space in the manuscript, we provide more qualitative results on video reconstruction, object segmentation, frame interpolation, editing, novel view synthesis and depth prediction in the following figures. Note that we don't claim that NutWorld achieves state-of-the-art performance across all these downstream tasks. Instead, our focus is on demonstrating NutWorld as a versatile video representation framework with broad applicability and adaptability. We believe that with task-specific adaptations, NutWorld has the potential to compete with specialized state-of-the-art methods in these individual video domains. \textbf{Please refer to the attached material for video visualization.}

\begin{figure*}[t]
\centering 
\includegraphics[width=0.95\linewidth]{img/nutshell_app_rec.pdf} 

\caption{More qualitative results on video reconstruction.}  
\label{fig:app_re} 

\end{figure*}


\begin{figure*}[t]
\centering 
\includegraphics[width=0.98\linewidth]{img/nutshell_app_edit.pdf} 

\caption{More qualitative results on video editing.}  
\label{fig:app_edit} 

\end{figure*}



\begin{figure*}[t]
\centering 
\includegraphics[width=0.96\linewidth]{img/nutshell_app_depth.pdf} 

\caption{More qualitative results on consistent depth prediction.}  
\label{fig:app_depth} 

\end{figure*}




\begin{figure*}[t]
\centering 
\includegraphics[width=1.0\linewidth]{img/nutshell_app_frame.pdf} 

\caption{More qualitative results on frame interpolation. Note that \textbf{\textcolor{red}{Red Frame}} denotes the interpolated frame.}  
\label{fig:app_frame} 

\end{figure*}






\begin{figure*}[t]
\centering 
\includegraphics[width=1.0\linewidth]{img/nvs-vis.pdf} 

\caption{Qualitative results on novel view synthesis.}
\label{fig:app_vis} 

\end{figure*}


\begin{figure*}[b]
\centering 
\includegraphics[width=1.0\linewidth]{img/vos-supp.pdf} 

\caption{Qualitative results on video object segmentation.}
\label{fig:app_vos} 
\end{figure*}




% \yxy{nvs, may have point tracking; ablation: decoder ratio}
\label{app:result}

\section{Limitations}
\label{app:limit}
While our NutWorld framework demonstrates significant advances in spatial-temporal video modeling and rendering efficiency, several limitations warrant discussion. (1) The framework's reliance on depth and optical flow priors makes its performance inherently dependent on external models, which may not generalize effectively to challenging scenarios involving complex motion or suboptimal lighting conditions. (2) While the segment-based inference strategy enables accelerated rendering, it introduces a trade-off between processing speed and global temporal consistency by prioritizing localized frame segments over the complete video sequence. (3) Despite achieving substantial runtime efficiency, the framework's training process remains computationally intensive, potentially limiting its deployment on resource-constrained edge devices. (4) Unlike Langsplat~\cite{qin2024langsplat} and latentsplat~\cite{wewer2024latentsplat}, our current framework does not embed latent or semantic features into each STAG, necessitating integration with other pre-trained models during inference for high-level vision tasks such as editing and reasoning. However, future work could explore distilling rich visual features (e.g., SAM, CLIP) into our STAG representation and adapting our representation paradigm for video reasoning and generation tasks.

\clearpage 
\newpage


%decoder ratio =2 compression model?

% %%%%%%% More details to write %%%%%%%

% impl detail of orthographic rasterization, jacobian matrix 

% attribution decoding (static) in the decoder arch 

% optical flow prior calibartion details 

% segment-based inference impl details,  (follow co-tracker)

% preprocessing, and splitting of trains (MiraData and Realestate) is provided in the Appendix.

% training hyper-parameters and network architecture




% %%%%%%% Experimental results %%%%%%%
% More visualization results about consistent depth, frame interpolation and video segmentation 

% downstream tasks: video-editing and novel view systhesis results visualization, may include point tracking,  


% visualization results on realestate-10k

