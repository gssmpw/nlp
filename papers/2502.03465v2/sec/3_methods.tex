\vspace{-2mm}
\section{Preliminary: Dynamic Gaussian Splatting}
% \vspace{-1mm}
Dynamic Gaussian Splatting~\cite{yang2024deformable, wu20244d, duan20244d, katsumata2025compact, kratimenos2025dynmf, yang2023real} is an explicit 4D neural representation for reconstructing dynamic 3D scenes from multi-view videos through differentiable rasterization~\cite{kerbl20233d}. Dynamic Gaussian ${\{\mathcal{G}_{i}, \mathcal{D}_{i}(t)}\}$ decouples dynamic scenes into a static canonical 3D Gaussian $\mathcal{G}_i$ and a deformation motion field $\mathcal{D}_i$ to account for temporal variations in 3D space. Specifically, the static 3D Gaussian $\mathcal{G}_{i}$ is composed of a 3D center $\mu \in \mathbb{R}^3$,  3D scale $s \in \mathbb{R}^3$, associated color $c \in \mathbb{R}^3$, opacity $\alpha \in \mathbb{R}$, and rotation quaternion $q \in \mathbb{R}^4$. 
%
% For deformation fields $\mathcal{D}_{i}(t) = \{\mu_i(t), q_i(t), \alpha_i(t)\}$, while their parameterization varies in different works, the deformable attributes are consistently constrained to the center $\mu_i$, rotation $q_i$, and opacity $\alpha_i$, with other attributes remaining time-independent based on empirical observations. 
% 
For the deformation fields $\mathcal{D}_{i}(t) = \{\mu_i(t), q_i(t), \alpha_i(t)\}$, the deformable attributes and their parameterizations vary across different approaches but are generally limited to the center position $\mu_i$, rotation $q_i$, and opacity $\alpha_i$, with other attributes remaining independent of time.
When representing the scene at time $t$, each dynamic Gaussian is temporally sliced into 3D space by applying its corresponding deformation field $\mathcal{D}_{i}(t)$ to yield an static Gaussian primitive, e.g., with deformed position $\hat{\mu}_i = \mu_i + \mu_i(t)$, which can then be efficiently rendered into 2D images via tile-based rasterization pipeline.


% \yxy{simplification is not acceptable, may need other empirical reasons and may not present here.} In our setting, we simplify by modeling deformable fields solely over the center position, reducing the deformation field to $\mathcal{D}_{i}(t) = \mu_i(t)$.


% %

%  we only model deformable fields over the center position while others to be time-independent, since we experimentally do not observe improvement in rendering quality when applying time-conditioned function on other parameters.
\vspace{-1.5mm}

\section{Methodology}
% \sqh{check whether ambiguous words like dynamic 3d scene reconstruction is involvoed in the methodology part.}
% As illustrated in the above figure, the pipeline of our model is composed of.....
In this section, we present a framework for efficiently representing world dynamics from monocular video in a \textit{feed-forward manner}. As shown in Figure~\ref{fig:framework}, we first introduce our Spatial-Temporal Aligned Gaussian Splatting (STAG) representation (Section~\ref{stag}). To enable the mapping of videos to STAG in a single forward pass, we detail our transformer-based network (Section~\ref{NutShell}), which operates with calibrated depth and flow priors (Section~\ref{prior}). Finally, we discuss the overall training objectives and protocols for processing long video segments (Section~\ref{train}).
% train from static to dynamic

% first introduce the pipeline, feed-forward dynamic gaussian pipeline. 

% since xxx, we introduce depth and flow priors for supervision to generate stereo ....

\begin{figure}[t]
\centering
\vspace{-5mm}
\includegraphics[width=1.0\columnwidth]{img/STAG.pdf}
\caption{The illustration of STAG to represent dynamic scenes.}
\label{fig:stag}
% \vspace{-6mm}
\end{figure}


\begin{figure*}[t]
\centering 
\includegraphics[width=0.97\linewidth]{img/NutShell-framework.pdf} 
\vspace{-10pt}
\caption{\textbf{Overview of NutWorld.} We directly predict STAG in a canonical space from sparse input frames via a transformer-based reconstruction model, where calibrated depth and flow priors are leveraged to avoid depth ambiguity and motion uncertainty.}
\label{fig:framework}
\vspace{-1mm}
\end{figure*}

\subsection{Spatial-Temporal Aligned Gaussian}
\label{stag}
\noindent \textbf{Canonical camera space}. Given an unposed monocular video, we employ an orthographic camera coordinate system to interpret the input as a quasi-3D canonical volume~\cite{wang2023tracking,sun2024splatter} rather than an absolute 3D world space. This choice addresses two challenges: (1) the difficulty of obtaining consistent camera trajectories in dynamic scenes~\cite{ullman1979interpretation, schonberger2016structure, pan2024global, wang2024dust3r, wang2024vggsfm}, and (2) the scale ambiguity in feed-forward 3D reconstruction~\cite{pixelsplat, chen2025mvsplat, zhang2024transplat}, where perspective projection couples object size with camera distance. By imposing a fixed pose along the $z$ axis, orthographic projection removes perspective-induced distortions and eliminates the need for explicit camera estimation, enabling joint modeling of both camera and object motion. We detail the orthographic rasterization pipeline in the \textit{Appendix}.
% , also facilitating the incorporation of 2D priors supervision described in Section~\ref{prior}.
% \sqh{reviewer: The paper indicates they want to disentangle, but the rebuttal suggested otherwise.}


\noindent \textbf{Structured Dynamic Gaussian}. To overcome the unstructured nature in dynamic Gaussian Splatting and facilitate neural network integration, we introduce \textbf{S}patial-\textbf{T}emporal \textbf{A}ligned \textbf{G}aussian Splatting (\textbf{STAG}) within this canonical volume. STAG constrains each dynamic Gaussian to a specific pixel location and timestamp, in contrast to the previous approach of predicting unconstrained Gaussians with deformable fields across orthogonal space-time.
Formally, for an input frame $F_{k}$ with normalized timestamp $t_k \in [0, 1]$, we compute a Gaussian feature map $\mathcal{E}^k \in \mathbb{R}^{U \times V \times T}$, where $U$ and $V$ represent spatial dimensions, and $T$ denotes the channel dimension. Each $T$-dimensional pixel is decoded into a 3D Gaussian with an associated deformation field $\{\mathcal{G}_{i}, \mu_i(t)\}$~\footnote{ Note that based on our empirical observation, we simplify the deformation modeling by considering only the center position, such that the deformation field reduces to $\mathcal{D}_{i}(t) = \mu_i(t)$.} in a pixel-aligned manner~\cite{pixelsplat, szymanowicz2024splatter}.




Given a pixel at coordinates $(u, v)$ in the feature map $\mathcal{E}^k$, we define its corresponding 3D Gaussian center $\mu^k$ through unprojection along the corresponding ray: $\mu^k = (u + \Delta_{x}, v + \Delta_{y}, d)$,
where $\Delta_{x}$ and $\Delta_{y}$ are bounded offsets decoded from $\mathcal{E}^k$, maintaining pixel-level correspondence with local position refinement. The depth value $d$ specifies the $z$-coordinate of $\mu^k$ along the camera's viewing axis. To model temporal dynamics, for frame rendering at timestamp $t_j$, we deform each static Gaussian center $\mu^k$ during frame rendering at timestamp $t_j$ using the predicted deformation field $\mu^k(t)$:
\begin{equation}
\hat{\mu}^k = \mu^k + \mu^k(t_{j}) \mathbbm{1}(t_k, t_j),
\end{equation}
where $\mathbbm{1}(t_k, t_j)$ is a temporal slicing indicator function defined as:
\begin{equation}
\mathbbm{1}(t_k, t_j) = \left\{\begin{array}{ll}
0, & \text{if} \,\, t_j = t_k \text{ \small{\textcolor{gray}{(reference frame)}}}, \\
1, & \text{otherwise \text{\small{\textcolor{gray}{(non-reference frame)}}}}. 
\end{array}\right.
\label{eq:temporal_align}
\end{equation}
As shown in Figure~\ref{fig:stag}, the temporal slicing function $\mathbbm{1}(t_k, t_j)$ modulates the deformation field $\mu^k(t_{j})$ based on the temporal relationship between the rendering timestamp $t_j$ and the Gaussian's reference timestamp $t_k$. For $t_j = t_k$, the deformation is suppressed ($\mathbbm{1}(t_k, t_j) = 0$), preserving the original Gaussian position $\mu^k$ to maintain spatial alignment. When $t_j \neq t_k$, the deformation field is activated to adapt $\mu^k$ according to temporal differences, enabling per-pixel alignment across frames and enhancing consistency in our quasi-3D volume.




% Additionally, when rendering frames at timestamp $t_j$, the predicted deformation field $\mu^k(t)$ is applied to the static Gaussian centers $u_i^k$ according to its timestamp $t_k$ with a slicing window:
% \begin{equation}
% \hat{\mu}^k = \mu^k + \mu^k(t_{j}) \mathbbm{1}(t_k, t_j).
% \end{equation}
% Here $\mathbbm{1}$ is an indicator function:
% \begin{equation}
% \mathbbm{1}(t_k, t_j) \left\{\begin{array}{ll}
% 0, & if \,\, t_j = t_k \\
% 1, & otherwise
% \end{array}\right..
% \end{equation}
% That is, as illustrated in Figure~\ref{fig:xx}, 








\subsection{Encapsulate Dynamics within ``Nutshell''}
\label{NutShell}
In this section, we introduce the transformer-based model in NutWorld to transform unposed monocular videos into the proposed STAG. Formally, given an input sequence of $K$ video frames $\{F_{k}, t_{k} \}$, where each frame $F_{k} \in \mathbb{R}^{H \times W \times 3}$ is associated with a normalized timestamp $t_{k} \in [0, 1]$, we define NutWorld as an inverse mapping function $\Theta$:
\begin{equation}
\{\mathcal{G}_{i}, \mu_{i}(t)\} = \Theta(\{F_{k}, t_{k}\}).
\label{eq:model}
\end{equation}
This mapping generates a set of STAGs $\{\mathcal{G}_{i}, \mu_{i}(t)\}$ that can be rendered into arbitrary frames ($M \geq K$) through temporal interpolation of the deformation field. Specifically, the NutWorld model is composed of two main components:

% Building upon recent advances in transformer-based reconstruction~\cite{xx,xx}, we design NutShell with three key architectural components: A ViT-based encoder that processes spatiotemporal features, a hierarchical upsampler with efficient windowed attention~\cite{xx}, follwed by a STAG decoder.





\noindent{\textbf{Transformer-based Encoder}}. For each input frame $F_{k} \in \mathbb{R}^{H \times W \times 3}$, we augment pixels by concatenating their RGB values with corresponding depth coordinates $d^{*}_{k}$ along the channel dimension, obtained from a pre-trained video depth estimation model~\cite{chen2025video}. The augmented frames are first split into non-overlapping patches of size $p \times p$ using convolutions, which are then linearly transformed and concatenated across all frames to generate transformer input tokens. Notably, our architecture eliminates the need for explicit positional embeddings as used in ViT~\cite{dosovitskiy2020image,bai2024meissonic}, since depth coordinates inherently encode spatial information. The transformer blocks, comprising self-attention~\cite{vaswani2017attention} and MLP layers, process these concatenated tokens to capture spatiotemporal correspondence and produce encoded features $\mathcal{E}_{0} \in \mathbb{R}^{K \times h \times w \times C}$, where $h = H/p$ and $w = W/p$ denote the spatial resolution and $C$ denotes the feature dimension. To ensure sufficient STAGs for casual videos, we leverage a hierarchical upsampling network~\cite{zhang2025gs, xu2024grm} that progressively expands the spatial resolution of the encoded feature $\mathcal{E}_0$.  Each upsampler block first expands the feature dimension by a factor of \textbf{\textit{4}} through a linear layer, followed by a PixelShuffle~\cite{shi2016real} layer that doubles the spatial resolution. The resulting features are then processed by a local attention layer with window size $\mathcal{W}$, which balances computational efficiency and spatial-temporal feature aggregation:
\begin{equation}
\begin{aligned}
\hat{\mathcal{E}}_{j-1} &= \text{PixelShuffle}(\text{Linear}(\mathcal{E}_{j-1}), 2), \\
\mathcal{E}_{j} &= \text{WindowAttn}(\hat{\mathcal{E}}_{j-1}, \mathcal{W}).
\end{aligned}
\end{equation}
After cascading such blocks $n$ times, the final feature map $\mathcal{E}_{n}$ achieves a spatial resolution of $(U, V) = (2^n h, 2^n w)$.

\noindent{\textbf{STAG Decoder}}. The proposed decoder predicts both static Gaussian attributes and their deformable field from the upsampled feature map $\mathcal{E}_{n}$.
For decoding static 3D Gaussian, we employ a shared MLP with specialized sub-heads to predict each Gaussian attribute: center position $\mu$, opacity $\alpha$, scale $s$, rotation $q$, and color $c$ is RGB value $\in [-1, 1]^3$. Given our fixed canonical camera setup, we omit view-dependent effects in color prediction. Each attribute utilizes specific activation functions following established practices~\cite{pixelsplat}, with details provided in the \textit{Appendix}.

To model the deformation field $\mu(t)$ as a continuous function of time, we encode the timestamp $t$ using sinusoidal positional encoding followed by a learnable linear projection. The encoded time features are then processed by an MLP to enable differentiable temporal interpolation:
\begin{equation}
\mu(t) = \mathcal{F}_{\theta}(\text{Linear}(\gamma_{n}(t)), \mathcal{E}_n(k, u, v)),
\end{equation}
where $\mathcal{F}_{\theta}$ represents an MLP with learnable parameters $\theta$. A $tanh$ activation is applied to the output, constraining the deformation within the bounds $[-b, b]^3$. The function $\gamma(t) = \left( \sin(2^k \pi t), \cos(2^k \pi t) \right)_{k=0}^{L-1}$ represents the sinusoidal expansion of order $L$, which enhances the network's capacity to capture high-frequency components effectively.




% \sqh{detail the network architecture (switcher may be shown in the main figure.)}







\subsection{Calibrated 2D Priors Regularization}
\label{prior}
% \sqh{different input and ouput frames number shall be different in the objective definition here.}
% Our key insight is that monocular cues from foundation models can serve as strong priors to regularize the decoupling of motion and appearance modeling. 

% Without supervision from multi-view captures and geometric initialization from SFM, the â€œNutShellâ€ model tends to predict a collapsed spatial representation that diverges from real-world distributions. To address this issue, we leverage off-the-shelf prior models~\cite{xu2023unifying, yang2024depth} to recover spatial relationships and temporal motion using depth and flow priors, respectively.

Learning spatially-aware STAGs solely from monocular videos is inherently ill-posed due to depth ambiguity and motion uncertainty. Therefore, we leverage off-the-shelf foundation models~\cite{xu2023unifying, chen2025video} to recover spatial relationships and temporal motion through calibrated depth and flow priors, respectively. Note that in the quasi-3D canonical volume under the orthographic projection, the movement of STAG along the xy-coordinates directly corresponds to optical flow magnitude, whereas depth-related loss exclusively affects the z-coordinate, further facilitating the incorporation of those 2D priors. 

\noindent\textbf{Depth Regularization}. To enhance robustness against scale and shift variations in depth rendering, we employ a scale and shift invariant loss~\cite{bhat2023zoedepth, ranftl2020towards}. This loss function computes optimal scaling $\beta$ and shifting $\gamma$ factors that align the rendered depth $d$ with the pseudo depth $d^*$ estimated by the video depth prior~\cite{chen2025video}. 
% 
The optimal values for $\beta$ and $\gamma$ are obtained by minimizing the squared error between the scaled rendered depth and the pseudo depth, as follows:
\begin{equation}
\begin{aligned}
    \beta, t &= \arg \min_{\beta, \gamma} \sum_{i} M_{i} \left( \beta \cdot d_{i} + \gamma - d^{*}_{i} \right)^2, \\
    \mathcal{L}_{\text{depth}} &= \sum_{i=1}^{H \times W} | \tilde{d}_{i} - d^{*}_{i} | / {\sum M}, \quad \tilde{d}_{i} = \beta \cdot d_{i} + \gamma.
\end{aligned}
\label{eq:depth_prior}
\end{equation}
Here, $M$ is an outlier mask where $M_i=0$ for the top $10\%$ values in each depth map and $M_i=1$ otherwise, mitigating noise in the estimated pseudo depth $d^*$. This depth supervision effectively regularizes the training, making it robust for predicting relative depth in scenes.  



\begin{figure*}[t]
\centering 
\includegraphics[width=0.9\linewidth]{img/main-exp-results.pdf} 
% \vspace{-8pt}
\caption{Qualitative comparison of video reconstruction using our NutWorld and other optimization-based methods.}
\label{fig:main-comparison}
\end{figure*}



\begin{table*}[htbp]
% \vspace{-4mm}
% \setlength{\abovecaptionskip}{0.1cm}
% \setlength{\belowcaptionskip}{0cm}
  \centering
  \setlength{\tabcolsep}{6pt} 
  \renewcommand{\arraystretch}{1.0}  
  \resizebox{0.85\linewidth}{!}{\begin{tabular}{l S[table-format=2.2] S[table-format=1.4] S[table-format=1.4] l S[table-format=3.1] l}
    \toprule[1pt]  
    % \rowcolor{gray!10}
    \rowcolor[HTML]{EFEFEF}
    \textbf{Method} & {\textbf{PSNR}$\uparrow$} & {\textbf{SSIM}$\uparrow$} & {\textbf{LPIPS}$\downarrow$} & {\textbf{Inference Time}} & {\textbf{GPU Mem.}} & {\textbf{FPS}} \\
    \midrule[0.6pt]  
    4DGS~\cite{wu20244d} & {17.59} & {0.5725} & {0.5393} & {\small$\sim$40 mins} & {10G} & {145} \\
    RoDynRF~\cite{liu2023robust} & {24.63} & {0.7152} & {0.3927} & {\small$>$24 hours} & {24G} & {$<$ 0.01} \\
    CoDeF~\cite{ouyang2024codef} & {26.35} & {0.8045} & {0.2714} & {\small$\sim$30 mins} & {10G} & {8.8} \\
    Splatter-a-Video~\cite{sun2024splatter} & {24.85} & {0.7413} & {0.3523} & {\small$\sim$30 mins} & {10G} & {149} \\
    \midrule[0.4pt]  
    \textbf{Ours} & {29.18} & {0.9015} & {0.1415} & {\small$1.8$ seconds} & {4G} & {450} \\
    \bottomrule[1pt]  
  \end{tabular}}
  \caption{Quantitative comparison with state-of-the-art methods. GPU Memory is measured in GB and FPS indicates rendering frame rate.}
  \vspace{-6mm}
  \label{tab:main-comparison}
\end{table*}






\noindent\textbf{Flow Regularization}.  We extract global STAG trajectories by leveraging frame-to-frame optical flow associations~\cite{xu2023unifying}. In contrast to previous methods that solely employ iterative optimization between adjacent frames, our feed-forward framework utilizes global trajectory supervision to ensure consistent motion in a single forward pass. 

For each STAG, we define its estimated pseudo-trajectory $\bar{\mu}^*(t)$ across $K$ video frames. This trajectory is derived by sequential queries to the pre-computed optical flow field between adjacent frames, represented by a global flow matrix \(\mathbf{F}\):
\vspace{-2mm}
\begin{equation}
\vspace{-1mm}
\resizebox{0.95\columnwidth}{!}{$
\mathbf{F} = \begin{pmatrix}
(0, 0, \dots, 0) & \mathbf{f}_{1 \to 0} & \mathbf{f}_{2 \to 1} + \mathbf{f}_{1 \to 0} & \dots & \sum\limits_{k=1}^{K-1} \mathbf{f}_{k \to k-1} \\
\mathbf{f}_{0 \to 1} & (0, 0, \dots, 0) & \mathbf{f}_{2 \to 1} & \dots & \sum\limits_{k=2}^{K-1} \mathbf{f}_{k \to k-1} \\
\mathbf{f}_{0 \to 1} + \mathbf{f}_{1 \to 2} & \mathbf{f}_{1 \to 2} & (0, 0, \dots, 0) & \dots & \sum\limits_{k=3}^{K-1} \mathbf{f}_{k \to k-1} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
\sum\limits_{k=1}^{K-1} \mathbf{f}_{k-1 \to k} & \sum\limits_{k=2}^{K-1} \mathbf{f}_{k-1 \to k} & \sum\limits_{k=3}^{K-1} \mathbf{f}_{k-1 \to k} & \dots & (0, 0, \dots, 0)
\end{pmatrix}
$}
\end{equation}
The global flow matrix $\mathbf{F}$ is structured as a $K \times K$ matrix, where each entry $\mathbf{F}_{i, j}$ is structured as a vector of length $(U \times V)$ represents the 2D cumulative motion of each Gaussian from frame $j$ to frame $i$. The matrix structure is asymmetric: upper triangular entries encode cumulative backward flow $\mathbf{f}_{k \to k-1}(\bar{\mu}_{i})$, while lower triangular entries contain cumulative forward flow $\mathbf{f}_{k \to k+1}(\bar{\mu}_{i})$. Here, $\bar{\mu}_{i}$ denotes the projected 2D coordinates of the $i$-th Gaussian in frame $k$. For notational clarity, we omit the explicit flow query operations in the matrix entries.

Using the global flow matrix, we regularize the deformation field $\mu(t)$ by comparing its 2D projection $\bar{\mu}(t)$ against the estimated global trajectories $\mu^*(t)$ across $K$ frames:
\begin{equation}
\mathcal{L}_{\text{flow}} = \sum_{i=1}^{K \times U \times V}\sum_{k=0}^{K-1} \|\bar{\mu}_{i}(t_k) - \mu^*_i(t_k)\|_1,
\label{eq:flow_prior}
\end{equation}
where $\|\cdot\|_1$ denotes the L1 norm. An outlier filtering strategy is also applied to calibrate the flow prior (see \textit{Appendix} for flow calibration details). This flow loss enforces consistency between the predicted trajectories and the reference paths derived from optical flow, enabling NutWorld to learn coherent motion patterns from casually captured videos.









% Injecting 3D-aware features, DepthSplat, FreeGaussian, Splatter-a-video

% flow distillation, depth distillation,introduce calibration




% \subsection{Train from static to dynamic}

\subsection{Training and Inference}
\label{train}
\noindent\textbf{Overall objective.} During the training phase, we render RGB frames from $K = 6$ sparsely sampled frames and interpolate $M = 10$ intermediate frames for dense temporal supervision. Our training objective comprises three terms:

\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{MSE}} + \lambda_{\text{flow}}\mathcal{L}_{\text{flow}} + \lambda_{\text{depth}}\mathcal{L}_{\text{depth}},
\end{equation}
where $\mathcal{L}_{\text{MSE}}$ is the mean square error loss between the rendered and ground truth RGB frames. $\mathcal{L}_{\text{flow}}$ and $\mathcal{L}_{\text{depth}}$ denote the calibrated regularization term, respectively. The coefficients $\lambda_{\text{flow}}$ and $\lambda_{\text{depth}}$ balance the contribution of each term.


\noindent\textbf{Segment-based long video inference.} To handle casual videos with hundreds of frames, we propose a simple but effective segment-based strategy during inference. The input video is divided into overlapping segments, where the adjacent segments share one frame. Due to our pixel-level spatial-temporal representation, Gaussian trajectories can be seamlessly propagated across segments through these shared frames, enabling NutWorld to process arbitrarily long videos while maintaining spatial-temporal consistency. The details of segment-based inference is in \textit{Appendix}.

% Ubiquitous monocular videos usually have more than hundreds of frame numbers. To support such videos input, here we propose first to divide such video into segments of frames, and each segment have one frame overlap. As the nature of our spatial-temporal representation on the pixel level, we can directly derive the trajectory of Gaussians along segments.


% \subsection{}

% training from static to dyanmic, also can be part of progressive mask training.


