%File: formatting-instructions-latex-2025.tex
%release 2025.0
\documentclass[letterpaper]{article} % DO NOT CHANGE THIS
\usepackage{aaai25}  % DO NOT CHANGE THIS
\usepackage{times}  % DO NOT CHANGE THIS
\usepackage{helvet}  % DO NOT CHANGE THIS
\usepackage{courier}  % DO NOT CHANGE THIS
\usepackage[hyphens]{url}  % DO NOT CHANGE THIS
% \usepackage{tabu}
\usepackage{subcaption}
\usepackage{float}
\usepackage{mathrsfs}
\usepackage{tabularx}
\usepackage{xcolor}
% \usepackage{titlesec}

\usepackage{graphicx} % DO NOT CHANGE THIS
\urlstyle{rm} % DO NOT CHANGE THIS
\def\UrlFont{\rm}  % DO NOT CHANGE THIS
\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT
\frenchspacing  % DO NOT CHANGE THIS
\setlength{\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS
\setlength{\pdfpageheight}{11in}  % DO NOT CHANGE THIS

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{newfloat}
\usepackage{listings}
\DeclareCaptionStyle{ruled}{labelfont=normalfont,labelsep=colon,strut=off} % DO NOT CHANGE THIS
\lstset{%
	basicstyle={\footnotesize\ttfamily},% footnotesize acceptable for monospace
	numbers=left,numberstyle=\footnotesize,xleftmargin=2em,% show line numbers, remove this entire line if you don't want the numbers.
	aboveskip=0pt,belowskip=0pt,%
	showstringspaces=false,tabsize=2,breaklines=true}
\floatstyle{ruled}
\newfloat{listing}{tb}{lst}{}
\floatname{listing}{Listing}
\pdfinfo{
/TemplateVersion (2025.1)
}


\setcounter{secnumdepth}{0} 

\title{Blue Sky Ideas: Towards Physics-Guided Foundation Models\vspace{-1.25em}}
\author {
    Majid Farhadloo\textsuperscript{\rm 1}\equalcontrib,
    Arun Sharma\textsuperscript{\rm 1}\equalcontrib ,
    Mingzhou Yang\textsuperscript{\rm 1},
    Bharat Jayaprakash\textsuperscript{\rm 2},
    William Northrop\textsuperscript{\rm 2},
    Shashi Shekhar\textsuperscript{\rm 1}
}
\affiliations {
    % Affiliations
    \textsuperscript{\rm 1}Department of Computer Science, University of Minnesota, Twin Cities, USA\\
    \textsuperscript{\rm 2}Department of Mechanical Engineering, University of Minnesota, Twin Cities, USA\\

    \{farha043, sharm485, yang7492, jayap015, wnorthro, shekhar\}@umn.edu
}
\usepackage{bibentry}
\begin{document}
\vspace{-5em}
\maketitle
\vspace{-5em}
\begin{abstract}
Traditional foundation models are pre-trained on broad datasets to reduce the training resources (e.g., time, energy, labeled samples) needed for fine-tuning to a wide range of downstream tasks. However, traditional foundation models struggle with out-of-distribution prediction and can produce outputs that are unrealistic and physically infeasible.  We propose the notation of physics-guided foundation models (PGFM), that is, foundation models integrated with broad or general domain (e.g., scientific) physical knowledge applicable to a wide range of downstream tasks.
\end{abstract}
\vspace{-1.5em}
\section{Introduction}
Driven by the availability of large-scale datasets, advancements in computational power, and innovations in deep learning architectures, traditional foundation models (FMs) have significantly advanced the field of artificial intelligence \cite{chen2020simple, vaswani2017attention}. For a comprehensive survey on FMs and their history, interested readers may refer to \cite{zhou2024comprehensive}.

% Notable implementations include BERT's masked language modeling \cite{b9}, predicting masked words from bidirectional context, autoregressive modeling in GPT \cite{b10} for predicting the next sequence token based on the previous tokens, and contrastive learning in image processing \cite{chen2020simple}, which models train to differentiate between similar and dissimilar image pairs. Another notable implementation is the Harmonized Landsat and Sentinel Geospatial Foundation Model (a.k.a. Prithvi) \cite{b6}, which employs self-supervised learning techniques, including masked auto-encoder, to utilize vast amounts of satellite imagery data from Landsat and Sentinel. 

% This approach enhances geospatial analysis tasks such as burn scar and flood detection by leveraging multispectral data. It also helps crop detection tasks \cite{b18}. Foundation models have also been created for weather and climate domains using training samples from physics based models \cite{b17}.

\textbf{Limitations of Foundation Models: }  Despite their versatility, purely data-driven FMs exhibit major limitations in scientific and engineering domains. One is their struggle with out-of-distribution situations, in which their benefits to downstream applications may be limited. This shortcoming is particularly problematic in specialized tasks that require nuanced understanding and adaptation to specific domain knowledge, such as climate science and healthcare. For instance, the Geospatial Foundation Model (Prithvi) \cite{b6}, trained exclusively on satellite imagery data from Landsat and Sentinel with multi-spectral bands, may fail to generalize to other data types, such as hyperspectral imagery or newer satellites with different spectral resolutions.  Second, purely data-driven foundation models (FMs) often violate fundamental physical principles such as energy conservation and motion dynamics. Without physics-based constraints, these models produce unrealistic and physically infeasible outputs. For example, models trained to estimate energy consumption and generate velocity profiles using transportation data such as onboard diagnostics and trajectory datasets may display rapid and unrealistic variations in velocity. Figure \ref{fig1}(a) shows velocity profiles without a jerk penalty that exhibit sharp peaks and drops, which reflect abrupt speed changes not typical in realistic driving scenarios. Figure \ref{fig1}(b) demonstrates that omitting a jerk penalty results in excessive jerk values far beyond passenger comfort thresholds \cite{de2023standards}.
Third, their black-box nature reduces transparency and makes predictions difficult to interpret \cite{b12}. These challenges highlight the need for Physics-Guided Foundation Models (PGFMs), which integrate scientific laws and physical constraints to enhance prediction reliability, robustness, and domain trust.
% For example, neural networks trained to estimate energy consumption sometimes generate velocity profiles with rapid and implausible variations (see Fig. X) due to lack of explicit physics-based constraints. 
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.9\linewidth]{Figures/fig1.jpg}
% \caption{Neural network-generated velocity profiles exhibiting unrealistic variations. The left column displays velocity profiles, while the right column shows corresponding jerk values, with a 6 m/s\(^2\) bound for reference.}
% \label{fig1}
% \end{figure}

% \begin{figure}[h]
% \centering
% \begin{subfigure}[b]{0.7\linewidth}
%     \centering
%     \includegraphics[width=0.45\linewidth]{CameraReady/LaTeX/Figures/vel_profile_jerk_penalty.jpg}
%     \caption{Velocity profiles}
%     \label{fig:subfig1}
% \end{subfigure}
% \vskip 1em % Adds vertical space between the subfigures
% \begin{subfigure}[b]{0.7\linewidth}
%     \centering
%     \includegraphics[width=0.45\linewidth]{Figures/jerk_valv2.jpg}
%     \caption{Jerk values with 6 m/s\(^2\) bound}
%     \label{fig:subfig2}
% \end{subfigure}
% \caption{Neural network-generated velocity profiles and corresponding jerk values.}
% \label{fig1}
% \vspace{-2em}
% \end{figure}


\textbf{Contributions.} Our primary contributions include the following. We formally define the concept of Physics-Guided Foundation Models (PGFM) and list a few methods to incorporate broad-domain physical knowledge. Additionally, we provide concrete examples illustrating the limitations of existing FMs and the necessity of PGFM.

\vspace{-1em}
\section{Vision}\label{sec2}
A physics-guided foundation model is a foundational model incorporating broad domain knowledge and a general understanding of the domainâ€™s fundamental concepts and principles. For example, popular knowledge graphs and taxonomies \cite{ji2021survey} may strengthen foundation models for a broad range of downstream applications. Also, conservation laws (e.g., mass, energy) apply to a broad range of downstream physical science tasks. Concepts of speed, acceleration, and laws of motion are applicable to a broad range of tasks related to moving objects. Maxwell's laws are also widely applicable to tasks related to electromagnetism. A generalizable model of biogeochemistry may be used as a PGFM with extensive data training.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.55\linewidth} % Adjust width to fit both figures side by side
    \centering
    \includegraphics[width=\linewidth]{Figures/vel_profiles_jerk_penv3.jpg}
    \vspace{-1.5em}
    \caption{Velocity profiles}
    \label{fig:subfig1}
\end{subfigure}
\hfill % Adds horizontal space between the subfigures
\begin{subfigure}[b]{0.55\linewidth} % Adjust width to fit within the row
    \centering
    \includegraphics[width=\linewidth]{Figures/jerk_valv3.jpg}
    \vspace{-1.5em}
    \caption{Jerk values}
    % \caption{Jerk values with 6 m/s\(^2\) bound}
    \label{fig:subfig2}
\end{subfigure}
\vspace{-1em}
\caption{Neural network-generated velocity profiles and corresponding jerk values.}
\label{fig1}
\vspace{-2em} % Adjust vertical space below the figure
\end{figure}

A variety of approaches can be employed to embed physical knowledge into foundation models. \textit{Physics-constrained learning} enforces real-world constraints by introducing domain rules into the learning process through the loss function or regularization that safeguards model predictions to ensure physical feasibility (e.g., \cite{b13}). Similarly, \textit{architecture-level integration} (e.g., \cite{b13, jia2019physics}) incorporates domain principles directly into model structures, aligning network design with physics-based rules and patterns. In recent work \cite{b13}, we demonstrated that incorporating a physical constraint, such as a jerk penalty, in velocity profiling reduced the sharp fluctuations common in unconstrained models, as shown in Figure \ref{fig1}. This adjustment produced smoother outputs that better reflect realistic physical behaviors and practical driving dynamics.

The effectiveness of these approaches often depends on the type of physical knowledge embedded into the model. Wide physics knowledge refers to universal principles, such as conservation of energy or laws of motion, that apply across a broad range of scenarios. These principles are foundational and provide generalizable representations during pretraining. In contrast, narrow physics knowledge consists of task-specific principles tailored to specialized applications, such as aerodynamic drag models for vehicle energy consumption or battery discharge dynamics. For instance, Peukertâ€™s law \cite{doerffel2006critical}, which describes the nonlinear discharge of a battery under varying current loads, is highly specific to certain powertrain types and conditions. Instead of embedding this directly in pretraining, broader energy dynamics, such as the relationship between energy use and current, can be incorporated to develop transferable representations. Narrow principles like Peukertâ€™s law then become relevant during fine-tuning for downstream tasks, such as estimating the remaining driving range of an electric vehicle under specific operational constraints.

This distinction between wide and narrow physics knowledge is also evident in the datasets used to train Physics-Guided Foundation Models (PGFMs). Pretraining typically requires large-scale, diverse datasets that reflect broad physical principles. Examples include traffic flow datasets from highway cameras, vehicle trajectory datasets such as NGSIM \cite{coifman2017critical}, and energy datasets like FASTSim simulations \cite{brooker2015fastsim}, which are calibrated to US Environmental Protection Agency standards. These datasets enable the model to generalize fundamental relationships, such as energy conservation and speed-density dynamics. For downstream tasks, more specialized datasets tailored to specific applications are necessary. For example, aerodynamic drag models may be refined using vehicle-specific wind tunnel data, while battery discharge predictions may be fine-tuned with high-resolution battery management system data that capture real-world Peukertâ€™s law behavior.

The distinction between wide and narrow knowledge aligns closely with the broader differences between PGFMs and other machine learning paradigms, as summarized in Table \ref{tab:comparison}. Task-specific models, trained on narrow datasets, are limited in their ability to generalize across tasks and rarely integrate scientific or domain knowledge. Foundation models (FMs), while trained on broad datasets, often remain purely data-driven and lack integration of physical principles, which constrains their applicability to physics-informed domains. Physics-Guided Task-Specific (PGTS) models focus on integrating deep but narrowly defined domain knowledge into the learning process \cite{karpatne2017theory, daw2022physics}, producing highly specialized models that excel in specific tasks. PGFMs combine the strengths of these approaches by integrating broad training data with wide physics knowledge, enabling them to generalize across tasks, while leveraging narrow domain principles during fine-tuning to support task-specific requirements.
\vspace{-.8em}
% \subsection{Comparison with Other Machine Learning Models}\label{sec3}
% Table 1 compares PGFM with related concepts based on the criteria of breadth of training data, the extent of domain (e.g., scientific) knowledge, and the range of downstream tasks. In traditional task-specific (i.e., machine learning) models are trained on narrow datasets, which limits them to a restricted range of downstream tasks and generally lacks deep integration with specific scientific or domain knowledge. Foundation models (FMs) are trained on broad datasets, enabling them to support a wide range of downstream tasks; however, they are typically data-driven and have limited or non-existent integration of broad domain knowledge. Physics-guided task-specific (PGTS) model integrates deep but narrowly focused domain knowledge (e.g., scientific) into the learning process \cite{karpatne2017theory, daw2022physics}. It concentrates on specific tasks and datasets, leading to the development of specialized models. In future, this comparison may be expanded using additional criteria such as training methods and difficulty. In the following, we provide a conceptual example to distinct wide versus narrow knowledge.

% \vspace{-1em}
% \begin{table}[h]
% \setlength{\abovecaptionskip}{2pt} % Reduce space above caption
% \setlength{\belowcaptionskip}{2pt} % Reduce space below caption
% \setlength{\intextsep}{2pt} % Reduce space around table
% \renewcommand{\arraystretch}{0.9} % Reduce row spacing
% \captionsetup{skip=2pt} % Reduce space between caption and table
% \caption{Comparison of PGFMs with Related Concepts}
% \centering
% \scriptsize
% \begin{tabular}{|p{0.22\columnwidth}|p{0.15\columnwidth}|p{0.21\columnwidth}|p{0.22\columnwidth}|}
% \hline
% \textbf{Technique} & \textbf{Training Data} & \textbf{Downstream Tasks Range} & \textbf{Scientific Knowledge} \\
% \hline
% Task-specific Model & -- & Narrow & -- \\
% \hline
% FMs & Broad & Wide & -- \\
% \hline
% PGTS & -- & Narrow & Narrow \\
% \hline
% PGFMs & Broad & Wide & Wide \\
% \hline
% \end{tabular}
% \label{tab:comparison}
% \vspace{-2em}
% \end{table}


\begin{table}[h]
\setlength{\abovecaptionskip}{2pt} % Reduce space above caption
\setlength{\belowcaptionskip}{2pt} % Reduce space below caption
\setlength{\intextsep}{2pt} % Reduce space around table
\renewcommand{\arraystretch}{0.9} % Reduce row spacing
\captionsetup{skip=2pt} % Reduce space between caption and table
\caption{Comparison of PGFMs with Related Concepts}
\centering
\scriptsize
\begin{tabular}{|p{0.22\columnwidth}|p{0.15\columnwidth}|p{0.21\columnwidth}|p{0.22\columnwidth}|}
\hline
\textbf{Technique} & \textbf{Training Data} & \textbf{Scientific Knowledge} & \textbf{Downstream Tasks Range}  \\
\hline
Task-specific Model & -- & -- & Narrow \\
\hline
FMs & Broad & --  & Wide\\
\hline
PGTS & -- & Narrow & Narrow \\
\hline
PGFMs & Broad & Wide & Wide \\
\hline
\end{tabular}
\label{tab:comparison}
\vspace{-1em}
\end{table}


% \subsection{How Can Physical Knowledge Be Embedded in Foundation Models?}\label{sec4} A variety of approaches can be employed to embed physical knowledge into foundation models (FMs). We explore four key design strategies for integrating physics into FMs. First, \textit{physics-guided pre-training} leverages domain-specific datasets and augmentations to embed relevant knowledge, ensuring the model is equipped with essential background information \cite{b3}. Second, \textit{physics-augmented fine-tuning} refines pre-trained models for specific tasks by incorporating domain constraints, such as mobility patterns (e.g., \cite{lu2003evacuation, lu2005capacity}). Third, \textit{physics-constrained learning} enforces real-world constraints by introducing domain rules directly into the learning process through the loss function or regularization that safeguards model predictions to remain physically feasible (e.g., \cite{b13}). Fourth, \textit{architecture-level integration} (e.g., \cite{b13, jia2019physics}) incorporates domain principles directly into model structures that aligns network design with physics-based rules and patterns beyond purely data-driven methods. Notably, traditional learning algorithms like backpropagation often struggle to enforce hard constraints (e.g., mass balance), underscoring the need for alternative optimization methods that incorporate domain knowledge effectively.
\vspace{-1em}
\section{Conclusion and Future Work}\label{sec5}
PGFM enhances foundation models by systematically integrating domain-specific physcial knowledge, improving performance, robustness, and interpretability for diverse applications. Future research offers promising advancements in AI across fields like healthcare \cite{farhadloo2024spatial}, geospatial analysis, and engineering. We plan to develop and evaluate a PGFM model, focusing on domain knowledge integration using metrics like sample complexity. 

Advancing PGFM in future work will involve developing hybrid frameworks that combine the adaptability of retrieval-augmented generation \cite{lewis2020retrieval} with real-time updates to optimize domain adaptation and reduce training resource demands. We will also explore PGFMs in the context of recent multi-modal foundation models \cite{ravirathinam2024towards}, focusing on integrating physical knowledge \cite{sharma2024physics, sharma2022analyzing, sharma2022towards} from diverse input sources and its impact on training procedures and downstream applications. We will also explore additional criteria for comparing PGFMs with other machine learning models, including training methods and challenges such as model complexity, computational requirements, location dependency \cite{yang2025climate, ghosh2024towardssig, ghosh2024towardsarxiv, ghosh2024reducing, ghosh2024towardscosit, ghosh2023reducing, ghosh2022towards}, and calibration needs \cite{farhadloo2024towards, farhadloo2024spatial}. In the long term, we aim to address challenges such as explainability \cite{arrieta2020explainable, farhadloo2022samcnet}, robustness, bias, data quality, and adaptability. 

% Future PGFMs could include techniques that trace and explain how specific domain knowledge influences predictions and enhancing transparency.
\clearpage
\section*{Acknowledgments}{This material is based on work supported by the USDA under Grant No. 2023-67021-39829, the National Science Foundation under Grant No. 1901099, and the USDOE Office of Energy Efficiency and Renewable Energy under FOA No. DE-FOA0002044. We also thank Kim Kofolt and the Spatial Computing Research Group for their valuable comments and contributions.}
\bibliography{aaai25}

\end{document}
