%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}



% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{amsthm,amsmath,amssymb}
% \usepackage{algpseudocode} 
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\usepackage{mathrsfs}
\renewcommand{\algorithmiccomment}[1]{\hfill $\triangleright$ #1}
\usepackage{colortbl}
\usepackage{color}
\usepackage{multirow}
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
\newcommand{\RETURN}{\textbf{return }}
% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2025}

\begin{document}

\twocolumn[
\icmltitle{A General Error-Theoretical Analysis Framework for Constructing Compression Strategies}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Boyang Zhang}{comp,yyy,sch}
\icmlauthor{Daning Cheng}{yyy}
\icmlauthor{Yunquan Zhang}{yyy}
\icmlauthor{Meiqi Tu}{schku}
\icmlauthor{Fangmin Liu}{sch}
\icmlauthor{Jiake Tian}{schu}
% \icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
% \icmlauthor{Firstname8 Lastname8}{sch}
% \icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{yyy}{Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China}
\icmlaffiliation{comp}{University of Chinese Academy of Sciences, Beijing, China}
\icmlaffiliation{sch}{Peng Cheng Laboratory, Shenzhen, China}
\icmlaffiliation{schu}{the School of Microelectronics, South China University of Technology, Guangzhou, China}
\icmlaffiliation{schku}{The University of Hong Kong}
\icmlcorrespondingauthor{}
% \icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
The exponential growth in parameter size and computational complexity of deep models poses significant challenges for efficient deployment. The core problem of existing compression methods is that different layers of the model have significant differences in their tolerance to compression levels. For instance, the first layer of a model can typically sustain a higher compression level compared to the last layer without compromising performance. Thus, the key challenge lies in how to allocate compression levels across layers in a way that minimizes performance loss while maximizing parameter reduction.
To address this challenge, we propose a Compression Error Theory (CET) framework, designed to determine the optimal compression level for each layer. Taking quantization as an example, CET leverages differential expansion and algebraic geometry to reconstruct the quadratic form of quantization error as ellipsoids and hyperbolic paraboloids, and utilizes their geometric structures to define an error subspace. To identify the error subspace with minimal performance loss, by performing orthogonal decomposition of the geometric space, CET transforms the optimization process of the error subspace into a complementary problem. The final theoretical analysis shows that constructing the quantization subspace along the major axis results in minimal performance degradation. 
Through experimental verification of the theory, CET can greatly retain performance while compressing. Specifically, on the ResNet-34 model, CET achieves nearly 11$\times$ parameter compression while even surpassing performance comparable to the original model. 
\end{abstract}


\vspace{-0.5cm}
\section{Introduction}
Existing research shows that increasing model size and training data can significantly enhance the performance and learning capability of deep models. However, such improvements often come with a sharp increase in computational complexity and storage requirements, especially in resource-constrained hardware and latency-sensitive scenarios. Thus, a critical challenge is how to efficiently compress model parameters while maintaining nearly unchanged performance. 

Against this backdrop, mixed compression techniques have become a mainstream approach for model compression. Due to the varying contributions of different layers to overall model performance, different layers exhibit significantly different tolerance levels to compression. Adopting layer-wise differentiated compression strategies can thus maximize compression rates while minimizing performance degradation. For example, in low-rank decomposition, different layers have varying ranks. In quantization, some critical layers require higher precision, while others can use lower precision. However, the main challenge with this approach lies in the exponentially growing search space for determining the optimal mixed-precision quantization configuration. Specifically, for a neural network with $L$ layers, where each layer can select from four possible bit-widths (e.g., 2/3/4/8 bits), the search space grows to $4^L$. The huge search space makes it almost impossible to find the optimal configuration that can maintain good generalization performance and meet hardware efficiency.

Several representative compression methods have been proposed to address these challenges. For instance, DMBQ ~\cite{zhao2021distribution} pre-emptively searches for the optimal bit-width configuration in the distribution space and dynamically selects compression settings during training. The HAWQ ~\cite{2019HAWQ} series computes layer-wise Hessian information to assess the relative sensitivity of each layer, thereby determining the optimal compression configuration. However, these approaches suffer from the following limitations: (1) Lack of optimality explanation: Existing compression configurations often rely on heuristic approaches, lacking a clear theoretical foundation to justify their optimality. (2) Neglect of error correlation: These methods typically decouple the optimization of model performance error and compression error, failing to systematically analyze their theoretical interdependence. (3) Limited adaptability to aggressive quantization: At lower bit widths, these methods struggle to handle degradation caused by compression errors, which can disrupt retraining and result in significant performance drops.

\begin{figure*}
\centering
\includegraphics[width=16cm,height=5.0cm]{image/fig1.pdf}
\vspace{-0.4cm}
\caption{The CET framework completes the transition from algebraic to geometric analysis, providing a theoretical foundation for the quantization error vector. Based on the positive definiteness of the Hessian matrix at the convergence point, CET reconstructs the quadratic form of quantization error into ellipsoids or hyperbolic paraboloids. For ellipsoids, the direction along the long axis corresponds to the direction with the slowest increase in loss. For hyperbolic paraboloids, analogous to ellipsoids, the eigenvector corresponding to the negative eigenvalue defines the long-axis direction, representing the direction of loss reduction. Theoretical analysis suggests that the quantization error vector should be optimized along these two directions.}
\vspace{-0.4cm}
\label{fig1}
\end{figure*}

To address the aforementioned challenges, we propose a general Compression Error Theory (CET) framework, which systematically derives the optimal compression configuration. As shown in Figure \ref{fig1}, taking quantization as an example, CET first establishes the relationship between quantization-induced parameter errors and performance loss errors using total differentiation, and then applies algebraic geometry to transform the quadratic form of loss errors into a geometric representation. This geometric representation directly guides the selection of the quantization parameter space, clearly identifying the optimal quantization direction and range, and is more adaptable to low-bit-width compression. Next, by utilizing the theory of orthogonal complements, the process of solving the error subspace is converted into a complementary problem, allowing for the determination of the best quantization configuration. Unlike traditional methods, CET does not require retraining and can directly achieve the optimal compression configuration. Experimental results show that CET can successfully maximize model compression with minimal performance degradation. Specifically, for the ResNet-34 model, CET achieves nearly 11 parameter compression while even surpassing performance comparable to the original model. Existing compression error analysis methods rarely explore the problem from the perspective of algebraic geometry and spatial structure. Our approach leverages algebraic geometry and total differential analysis to examine the geometric properties and spatial structure of parameters during the compression process, using these insights to guide the direction and magnitude of compression. Although CET is based on quantization, its applicability extends beyond quantization and can be widely applied to other compression methods. 
The main contributions are as follows:

\textbf{$1)$} We propose a general Compression Error Theory (CET) framework, which derives the optimal compression configuration through theoretical analysis. CET is independent of specific compression methods and has broad applicability.

\textbf{$2)$} The proposed CET combines total differentiation and algebraic geometry to precisely guide the selection of the compression parameter space, avoiding the need for retraining commonly required by traditional methods.

\textbf{$3)$} Experimental results show that CET can maximize model compression with minimal performance degradation. For instance, on the ResNet-34 model, CET achieves nearly 11 parameter compression while even surpassing performance comparable to the original model. 



\vspace{-0.4cm}
\section{Related Works}
\subsection{Neural Network in Function}
We present the analysis of neural networks as composite functions. All our conclusions are independent of the structure of the neural network.
First, for an n-layer neural network model, the loss of the model is optimized according to the following equation
\begin{small} 
\begin{equation}
\begin{aligned} & \begin{aligned}
\min_{\textbf{W}}f(\textbf{W})=\mathbf{E}_{Sample}\ell(\textbf{W},Sample)=\frac{1}{m}\sum_{(x_{i},y_{i})\in\mathbb{D}}\ell(\textbf{W},x_{i},y_{i})\end{aligned},\\
&\ell(\textbf{W},x_{i},y_{i})=L(model_{n}(x_{i},\textbf{W}),y_{i}),\\  & model_{n}=h_1(h_2(h_3(h_4(\cdots(h_{n+1},w_{n})\cdots,w_4),w_3),w_2),w_1),\end{aligned}
\label{eq1}
\end{equation}\end{small}

\noindent where $f(\cdot)$ represents the loss of the model on a dataset, $\mathbf{E}$ stands for expectation, $m$ is the size of the dataset, $\ell(\cdot)$ is the loss function for a sample, and $(x_i,y_i)$ denotes a sample in the dataset along with its corresponding label, $L(\cdot)$ represents the loss function, such as the cross-entropy function; $h_i$, with $i\in[1,...,n]$ represents the ($n-i+1$)th layer in the neural network, $\textbf{W} = (w_n^T,w_{n-1}^T,\cdots,w_1^T)^T$, where $w_i$ is the parameter in $h_i(\cdot)$, and for the reason of a unified format, $h_{n+1}$ denotes the sample $x$.
When the model is treated as a complex high-dimensional nonlinear mapping, it encapsulates the structural constraints of the network and the characteristics of the loss function. Its local properties can be studied through differential and algebraic geometry methods to uncover the local shape characteristics of the function.
\subsection{Quantization for Compression}
The computational units of deep models are primarily composed of matrix multiplication. Quantization accelerates the multiplication process by converting floating-point parameters into lower-bit formats, thus speeding up the inference process. For a single layer of a neural network, it is represented as $Y=X\cdot W \in \mathbb{R}^{S\times C_{out}}$, where $X\in \mathbb{R}^{S\times C_{in}}$ is the activation input and $W \in \mathbb{R}^{C_{in}\times C_{out}}$ is the weight matrix. Taking integer uniform quantization as an example, the $b$-bit quantization process maps the FP16/32 weight tensor $W$ to a lower-bit integer $W_q$.
\begin{equation}
\begin{aligned}
Q(\textbf{W})&=\mathrm{clamp}\left(\left\lfloor\frac{\mathbf{W}}{\Delta}\right\rceil+z,0,2^b-1\right) \\
\mathrm{where}\quad\Delta&=\frac{\max(\mathbf{W})-\min(\mathbf{W})}{2^b-1},z=-\left\lfloor\frac{\min(\mathbf{W})}{\Delta}\right\rceil
\end{aligned}
\label{quant}
\end{equation}
The notation is $\lfloor\cdot\rceil$ means the nearest rounding operation, $\Delta$ is the quantization step size and $z$ represents the zero point.
When adopting minimum square error (MSE) as the criterion, the quantization process is expressed as the following minimization error problem:
\begin{equation}
\operatorname*{min}\|\textbf{W}-Q(\textbf{W})\|_{2}\quad s.t.Q(\textbf{W})\in\Pi_{b}
\end{equation}
$Q(\textbf{W})$:$\mathbb{R}^D\times\mathbb{Z}^+\to\Pi_b$ is the quantization function (Equation \ref{quant}).
Existing methods focus on reducing parameter errors through hybrid quantization schemes, which can be divided into two categories: search-based and tolerance-based methods.
Search-based methods, such as those by \cite{wang2019haq, lou2019autoq, wu2018mixed} treat the quantized network as a whole and use various bit-width allocations to evaluate the model. The evaluation results are then used to guide the search process to find the optimal solution. However, these methods are computationally expensive, difficult to parallelize, and due to their iterative search nature, require hundreds or thousands of GPU hours.

To optimize efficiency, another category of tolerance-based methods measures each layer's tolerance to quantization errors. When the tolerance of a layer is higher, the layer can be quantized with a lower bit-width. Various sensitivity metrics have been proposed in practice, such as the Kullback-Leibler divergence between the quantized layer output and the full-precision layer output (\cite{cai2020zeroq}), the maximum eigenvalue of the Hessian (\cite{2019HAWQ}), the trace of the Hessian (\cite{dong2019hawq, 2020HAWQV3}), the Gaussian-Newton matrix approximation of the Hessian (\cite{chen2021towards}), or the quantization factor (\cite{tang2022mixed}). All tolerance-based methods minimize the sum of tolerance across layers under the constraint of target compression ratio.
Although these methods are effective in practice, they lack a theoretical foundation to justify the optimality of their results. Furthermore, since these methods do not consider the relationship between model performance and parameter errors, they struggle to address model degradation caused by compression errors at lower bit widths, which lead to retraining failures or sharp performance drops.
\vspace{-0.4cm}
\section{Compression Error Theoretical Analysis Framework}
\subsection{Error Correlation and Optimization}
Typically, compression involves two types of errors: the change in parameters after compression, $\Delta_w$, and the change in loss caused by the parameter variation, $\Delta_L$. CET first establishes the relationship between the two and performs a unified analysis, aiming to determine the direction and magnitude of $\Delta_w$ to minimize $\Delta_L$. 

The mathematical essence of many compression schemes such as quantization and decomposition is to introduce compression errors into the original parameters. After compression, for a sample, the model loss $\bar{\ell}$ during inference is reformulated as the following equation
\begin{small}
\begin{align}
\bar{\ell}_k(w,x_{j},y_{j})  =L(h_1(h_2(\cdots h_{n}(x_i, w_{n}+\delta^k_{n})\cdots,\nonumber  \\w_2+\delta^k_2),w_1+\delta^k_1), y_{i}),
\end{align}
\end{small}  
where $\delta^k_{i}\in\Delta_w,i\in\left\{1,\cdots,n\right\}$ denotes the noise error on the weights after the k-level compression. In quantization, the k-level represents different bit widths.
CET directly associates the compression error and the change of the loss function through total differentials.
According to total differentials, the following equation can be obtained
\begin{equation}
	\begin{aligned}
		\Delta_L = \bar{\ell}(w,x_i,y_i)-\ell(w,x_i,y_i) =  \sum_{i=1}^n\frac{\partial\ell}{\partial w_i}\cdot\delta_i+ & \\ \frac{1}{2} \delta_i^T\mathbb{H}\delta_i+O(||\delta_i||^n)
	\end{aligned} \label{eq5}
 \end{equation}
where $\mathbb{H}$ represents the Hessian matrix and $O(||(\delta_i)||^n) $ represents the high-order term, $\cdot$ is inner product. For the loss on the whole dataset, we can gain
\begin{equation}
	\begin{aligned}
		\min\limits_{\delta\in \Delta}\Delta_L=\min\limits_{\delta\in \Delta}\bar{f}(w)-f(w)=\frac{1}{m}\sum\limits_{(x_j,y_j)\in\mathbb{D}}\sum\limits_{i=1}^{n}\frac{\partial\ell}{\partial w_i}\cdot\delta_i \\ +\frac{1}{2}\delta_i^T\mathbb{H} \delta_i+O(||\delta_i||^n)
	\end{aligned} \label{eq6}
 \end{equation}
where $\bar{f}(w) = \frac{1}{m}\sum \bar{\ell}(\cdot)$. This equation directly links compression $\Delta_w$ and model performance $\Delta_L$.
Although higher-order differentials provide theoretical support for CET, their usage requires meeting the following conditions. First, the function must be smooth and differentiable; second, the parameter changes must be small enough. According to the chain rule, multi-layer neural networks are continuously differentiable concerning all parameters, meaning that they are inherently smooth and differentiable. Thus, Eq. \ref{eq5} generally satisfies $C^k$ continuity. Since the scale of compression determines the parameter variation, we primarily focus on the magnitude of the error. When the variable $\delta$ is sufficiently small, the actual change in the loss function can be accurately described by the total differential 
$df$. Therefore, determining the "sufficiently small" threshold in the practical model is crucial. Since each layer can accommodate different sizes of parameter errors, we compute the gap between theory and practice, denoted as $U(x)$.
\begin{small} 
\begin{equation}
\begin{aligned}
U_{\delta^k}\left(x_i\right): |{\ell}(w \pm \delta^k_{i} ,x_{i},y_{i}) -(\ell(w,x_{i},y_{i})+\sum_{i=1}^{n}\frac{\partial\ell_k}{\partial w_i}\cdot\delta^k_{i})+& \\ \frac{1}{2}\delta_i^T\mathbb{H}\delta_i+O(||(\delta_i)||^n)|
\end{aligned}\label{eq7}
\end{equation}
\end{small}

The left-hand side of the equation represents the loss caused by actual noise interference, while the right-hand side represents the theoretical loss caused by noise. The parameter $k$ controls the compression level. When the neighborhood of compression error is smaller than $10^{-3}$, we consider the actual error to be close to the theoretical error. For weights, ideally, the first-order term in a well-trained model should be zero. Since higher-order terms are uncomputable, the impact of the second-order term is typically considered. Hence, the update for the optimization term is given by the following equation,

\begin{equation}
\begin{aligned}
\min\limits_{\delta\in \Delta}\Delta_L = \bar{f}(w)-f(w)=\frac{1}{2}\delta_i^T\mathbb{H} \delta_i
\end{aligned} \label{eq8}
\end{equation}
$\delta_i^T\mathbb{H} \delta_i$ is a quadratic expression, and $\mathbb{H}$ is composed of the second-order derivatives of the whole model. We hope that the loss decreases or increases small and slowly after compression. Next, CET mainly uses algebraic geometry to analyze this quadratic expression.



\subsection{Reconstruction of the Compression Subspace}
The expression $\delta_i^T\mathbb{H} \delta_i$ serves as an abstract representation of the quadratic term of a function, which can describe geometric surfaces in high-dimensional space, where $\mathbb{H}$ acts as the coefficient matrix. Its fundamental geometric form is given by: 
\begin{equation}
\delta_i^T\mathbb{H} \delta_i = c \label{eq9}
\end{equation}
where $c$ is a constant that represents the isosurface of the geometric shape. This equation constrains the weight vector $w$ within an n-dimensional space, where $n$ is the dimensionality of the weight vector. As shown in Figure \ref{fig1},
the eigenvalues and eigenvectors of $\mathbb{H}$ define the geometric properties of the surface: the eigenvalues indicate the degree of stretching or compression along the principal axes, while the eigenvectors determine the orientation of these axes. Furthermore, the definiteness of $\mathbb{H}$ dictates the global shape of the surface. Specifically, a positive definite matrix corresponds to a closed surface (e.g., an ellipsoid), whereas an indefinite matrix may result in an open surface (e.g., a hyperbolic paraboloid) \cite{hartshorne2013algebraic}. Hence, a key step in CET is to determine the overall geometric shape of the surface, which serves as the foundation for guiding the direction and magnitude of model compression.
We use the eigendecomposition to perform a standard form transformation on Eq.\ref{eq9}.
\begin{equation}
Q(\mathbf{y})=\lambda_1y_1^2+\lambda_2y_2^2+\cdots+\lambda_ny_n^2 \label{eq10}
\end{equation}
where $y=P^Tw$ is the new coordinate system after the eigenvector transformation. $P$ is an orthogonal matrix composed of $\mathbb{H}$(eigenvectors). The quadratic expression is a weighted sum in the direction of each principal axis, and its geometric shape is completely determined by the eigenvalues and eigenvectors. When $\mathbb{H}$ is positive definite at the convergence point $\textbf{W}$, the overall shape of the level surface 
$c$ forms a closed ellipsoid, indicating that the convergence point is locally convex. Conversely, if $\mathbb{H}$ is indefinite at the convergence point, the level surface becomes an open hyperbolic paraboloid. Negative definiteness at the convergence point, where the level surface would exhibit a fully concave open structure corresponding to a local maximum, is not possible in this context. This ensures that such cases are excluded.

Given that our objective is to minimize or slow the increase in loss after compression, the compression vector should align as closely as possible with the direction of the long axis of the ellipsoid or hyperbolic paraboloid. In this direction, the curvature is smaller, the changes in the level surface are more gradual, and the compression vector has a minimal impact on the loss value. Figure \ref{fig1} illustrates the transition of CET from a quadratic algebraic representation to a geometric interpretation, defining the optimization path for the compression vector through geometry. Next, after determining the direction of the quantized subspace, the next step is how to efficiently solve this space.



\subsection{Solution of the Compression Subspace}
To construct the subspace of the long axis, we leverage the concept of complementary spaces. In the $\mathbb{R}^n$ space defined by the curvature of the surface, the Hessian matrix's eigenvectors form a complete orthogonal basis. This allows us to decompose $\mathbb{R}^n$ into two complementary subspaces, satisfying the following equation
\begin{equation}
\mathbb{R}^n=V_{\mathrm{long}}\oplus V_{\mathrm{short}} \label{eq11}
\end{equation}
Here, $\oplus$ denotes the direct sum relationship, where $V_{\mathrm{long}}$ represents the subspace of the long axis, and $V_{\mathrm{short}}$ represents the subspace of the short axis.
Our goal is to find a compression vector that resides in the long-axis subspace. To achieve this, CET reformulates the problem by solving for the zero space of the short-axis subspace. This approach effectively identifies vectors orthogonal to the short-axis subspace and hence aligns with the long-axis subspace.
\begin{equation}
\begin{cases}
 \lambda_1 y^2_1(\delta_i) = 0, \\
\lambda_2 y^2_2(\delta_i) = 0, \\
\vdots \\
\lambda_m y^2_m(\delta_i) = 0
\end{cases} \label{eq12}
\quad i\in\{1, 2, ...,n\}
\end{equation}
where $\lambda_m$ represents the eigenvalues corresponding to the short-axis subspace, and $y_m$ denotes the transformed eigenvectors associated with these eigenvalues and noise $\delta_i$ on the weights. $n$ (the number of parameters) is much larger than $m$ (the number of eigenvalues), which means that this is an indeterminate system of equations. By constructing the zero space of the short-axis subspace, CET isolates vectors orthogonal to $V_{\mathrm{short}}$, thereby enabling the identification of compression vectors in the long-axis subspace.

The solution to the indeterminate equation system is ill-posed, thus requiring further constraints on Eq.\ref{eq12}:
\begin{itemize}
\item The ideal solution to Eq.\ref{eq12} is that every term $y$ equals zero, which corresponds to no quantization. This result is intuitive, as the absence of quantization minimizes parameter error. However, CET avoids this trivial solution by constraining the model size. Specifically, the condition Modelsize $M_{compress} < M_{orgin}$ is introduced, ensuring that the parameter error is strictly non-zero and the model volume is effectively compressed.
\item When the solved parameter error $\delta$ becomes excessively large, Eq.\ref{eq8} indicates that the loss change $\Delta_L$ will also increase significantly. Thus, we not only require the compression vector to exist within the long-axis subspace but also minimize its magnitude. A smaller magnitude implies reduced compression loss along the long-axis direction. CET imposes an additional minimization constraint on the parameter error, namely $min(\Vert \delta_i \Vert ^2)$. 
\end{itemize}
With this refinement, Eq.\ref{eq12} is updated to the following form:
\begin{equation}
\begin{cases} 
\lambda_1 y_1^2 (\delta_i) = 0, \\
\lambda_2 y_2^2 (\delta_i) = 0, \\
\vdots \\
\lambda_m y_m^2 (\delta_i) = 0
\end{cases}
\quad \text{s.t.} \quad 
\begin{cases} 
M_\text{compress} < M_{\text{origin}}, \\
\|\delta_i\|^2 < \epsilon. 
\end{cases}
 \label{eq13}
\end{equation}
% \vspace{-0.1cm}

where $\epsilon$ approaches 0, $i\in\{1, 2, ...,n\}$. By solving the above system of equations, we can directly obtain the parameter error $\Delta_w$ that minimizes $\Delta_L$, thereby determining the level of model compression.



\begin{algorithm}[!ht]
\caption{CET Algorithm for Quantization}
\begin{algorithmic}[1]
\REQUIRE Convergence point parameters $w$, calibration dataset $\mathcal{D}$, loss function $L(w)$.
\ENSURE Quantization bit-widths for each layer.
\STATE \textbf{Step 1: Compute Hessian matrix eigenvalues $\lambda$ and eigenvectors $v_i$ using the Lanczos algorithm on the calibration dataset $\mathcal{D}$}
\begin{itemize}
    \item Start with a random initial vector $v_1$ and iteratively perform matrix-vector multiplications with $H= \frac{\partial^2 L(w)}{\partial w^2}$.
    \item Orthogonalize the vectors to build a Krylov subspace and construct a tridiagonal matrix $T_k$.
    \item Solve $T_k$ to obtain approximate eigenvalues $\lambda_1, \lambda_2, \dots, \lambda_k$ and eigenvectors.
\end{itemize}
\STATE Return $\lambda$ and the corresponding eigenvectors $v_i$.

\STATE \textbf{Step 2: Randomly initialize perturbation $\delta$}
\STATE Randomly initialize $\delta_0\sim \mathcal{N}(0, \sigma^2)$ as the starting point for optimization. \COMMENT {Integration of multiple sampling results.}

\STATE \textbf{Step 3: Construct $y$ via canonical transformation}
\STATE Perform canonical transformation based on Eq.\ref{eq10}: $y = P^\top \delta$,
where $P = [v_1, v_2, \dots, v_k]$ is the matrix of eigenvectors. Each component $y_i$ in $y$ is given by:
\[
y_i = v_i^\top \delta.
\]

\STATE \textbf{Step 4: Select $m$ short-axis eigenvalues and solve for $\delta$}
\STATE Select $y_i$ corresponding to the $m$ smallest eigenvalues $\lambda_i$ and formulate the optimization problem based on Eq.\ref{eq13}.

\STATE Use gradient descent to iteratively solve for $\delta$:
\[
\delta^{(t+1)} = \delta^{(t)} - \eta \nabla_{\delta} \|\delta\|^2,
\]
Where $\eta$ is the learning rate.

\STATE \textbf{Step 5: Compute quantization bit-width from $\delta$}
\STATE Using the optimized perturbation $\delta$, compute the quantization bit-width $b_i$ for each layer:
\[
b_i = \log_2\left(\frac{1}{|\delta_i| + \alpha}\right),
\]
where $\alpha \to 0$ is a small positive constant. \COMMENT {or quantization error mapping}

\STATE \textbf{Step 6: Output the quantization bit-width for each layer}
\RETURN $\{b_1, b_2, \dots, b_k\}$, where $k$ is the number of layers in the model.
\end{algorithmic}
\end{algorithm}

\vspace{-0.3cm}
\subsection{CET Algorithm for Quantization}
We attempt to apply the CET (Compression Error Theoretical) framework to model quantization. CET regards quantization as a process of introducing noise perturbation into the model parameters, where the error grows as the bit-width decreases. The detailed procedure is illustrated in Algorithm 1. CET utilizes the Lanczos algorithm to compute the eigenvalues and eigenvectors of the Hessian matrix. Subsequently, it formulates the indeterminate equation system based on Eq.\ref{eq13} and solves for the perturbation vector $\delta$ using gradient descent. Once $\delta$ is obtained, two approaches are proposed to calculate the bit-width: \textcircled{1} The first method directly computes the bit-width for each layer using the quantization formula provided in the algorithm. \textcircled{2} The second method compares the actual quantization loss under different bit-widths and selects the $\delta$ that aligns with the true quantization error.
CET combines both approaches to determine the optimal bit-width. When the first method fails to yield accurate results due to the impact of outliers, the second method is adopted to ensure reliability. Regarding time complexity, the primary computational bottleneck lies in the Lanczos algorithm, with a complexity of $\mathcal{O}(n^2)$. The remaining steps of CET are computationally efficient, with a complexity of $\mathcal{O}(n)$.

CET is highly extensible and can be applied to other model compression techniques. Most compression methods, such as quantization, decomposition, and parameter sharing, can be viewed as processes that introduce noise perturbations into the model parameters. CET provides a theoretical framework for analyzing these compression-induced errors and determines the optimal compression level in practice. As a general-purpose method, CET is not tailored to any specific compression technique but can be generalized across a wide range of compression methods.

\begin{figure}[!ht]
\centering
\vspace{-0.1cm}
\includegraphics[width=8.5cm,height=3.0cm]{image/ablation1.pdf}
\vspace{-0.9cm}
\caption{The gap between theory and practice after adding different perturbations to each layer. This ensures that the theoretical approximation can effectively represent the actual value in a sufficiently small neighborhood.}
\vspace{-0.4cm}
\label{gap}
\end{figure}
\section{Experiments}
\subsection{Datasets and Details}
We evaluate the performance of CET on various models on ImageNet. The ImageNet-1K dataset\cite{krizhevsky2017imagenet} consists of 1.28 million training and 50K validation images. ImageNet-1K is usually used as the benchmark for model compression. The calibration set is taken from the ImageNet validation set. SWAG dataset~\cite{zellers2018swag} consists of 113K multiple-choice questions about grounded situations. The Stanford Question Answering Dataset (SQuAD)~\cite{rajpurkar2016squad} is a collection of question-answer pairs derived from Wikipedia articles. In SQuAD, the correct answers to questions can be any sequence of tokens in the given text. MNLI~\cite{williams2017broad} is a dataset for natural language reasoning tasks. Its corpus is a collection of textual implication annotations of sentences through crowdsourcing. The task is to predict whether the premise sentence and the hypothesis sentence are logically compatible (entailment, contradiction, neutral). 

Following existing compression work, CET utilizes the Lanczos algorithm to compute the eigenvalues and eigenvectors of the Hessian matrix, with the maximum number of iterations set to 100. The Lanczos algorithm avoids explicitly constructing the Hessian matrix, efficiently approximating its eigenvalues and significantly reducing computational complexity. In the higher-order expansion (Eq.\ref{eq7}), when the actual perturbation error and the theoretically derived perturbation error are both less than $10^{-3}$, the discrepancy between theory and practice becomes negligible. For Eq.\ref{eq13}, it is formulated as an optimization problem where quantization error is minimized using the Adam optimizer. To ensure experimental consistency, all models use the same settings without tuning hyperparameters. The quantization bit-widths for each layer are calculated using the quantization algorithm \cite{zhang2024fp}, without fine-tuning or retraining. All experiments are conducted on two NVIDIA A800 GPUs, and the code is implemented in PyTorch, which will be made publicly available.



\begin{table*}[ht]
\centering
\vspace{-0.4cm}
\caption{Comparison with existing uniform quantization methods and the latest mixed precision methods. MP refers to mixed precision quantization, and we report the lowest bits used for weights and activations. w-ratio and a-ratio represent the weight and activation compression ratios, respectively.}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{13.8pt}
\scalebox{0.83}{
\begin{tabular}{c|cccccccc} 
\hline
\textbf{Model}                        & \textbf{Method}                  & \textbf{Top-1/Full}          & \textbf{w-bit}                  & \textbf{a-bit}                  & \textbf{w-ratio}             & \textbf{a-ratio}              & \textbf{Top-1/Quant}         & \textbf{Top-1/Drop}           \\ 
\hline
\multirow{12}{*}{\textbf{ResNet-18 }} & LQ-Nets                          & 70.30                         & 3                               & 32                              & $\times$7.45                         & $\times$1.00                             & 69.30                         & -1.00                            \\
                                      & LQ-Nets                          & 70.30                         & 4                               & 32                              & $\times$6.10                          & $\times$1.00                             & 70.00                           & -0.30                          \\
                                      & MCKP                             & 69.76                        & 2$_{\textbf{MP}}$                             & 32                              & $\times$10.66                        & $\times$1.00                             & 69.50                         & -0.26                         \\
                                      & {\cellcolor[rgb]{0.941,1,1}}Ours &69.76 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}32  &$\times$\textbf{11.02} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}$\times$1.00 & 69.75{\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} \textbf{-0.01}  \\
                                      & MCKP                             & 69.76                        & 2$_{\textbf{MP}}$                             & 8                               & $\times$10.66                        & $\times$4.00                             & 69.39                        & -0.37                         \\
                                      & {\cellcolor[rgb]{0.941,1,1}}Ours &69.76 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}8   & $\times$\textbf{11.02} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}$\times$4.00 &69.71 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} \textbf{-0.05}  \\ 
\cline{2-9}
                                      & ABC-Net                          & 69.30                         & 5                               & 5                               & $\times$6.40                          & $\times$6.40                          & 65.00                           & -4.30                          \\
                                      & LQ-Nets                          & 70.30                         & 4                               & 4                               & $\times$6.10                          & $\times$7.98                          & 69.30                         & -1.00                            \\
                                      & DoReFa                           & 70.40                         & 5                               & 5                               & $\times$5.16                         & $\times$6.39                          & 68.40                         & -2.00                           \\
                                      & PACT                             & 70.40                         & 4                               & 4                               & $\times$6.10                          & $\times$7.98                          & 69.20                        & -1.20                          \\
                                      & MCKP                             & 69.76                        & 3$_{\textbf{MP}}$                             & 4$_{\textbf{MP}}$                             & $\times$8.32                         & $\times$8.00                             & 69.66                        & \textbf{-0.10 }                         \\
                                      & {\cellcolor[rgb]{0.941,1,1}}Ours &69.76 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}4$_{\textbf{MP}}$ & $\times $\textbf{11.02} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}$\times$8.00 & 69.01{\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} -0.65 \\ 
\hline
\multirow{2}{*}{\textbf{ResNet-34}}& {}Ours &73.22  & 2$_{\textbf{MP}}$ & 4$_{\textbf{MP}}$ & $\times $\textbf{13.44}  &$\times$8.00 & 72.60 & -0.62            \\
& {\cellcolor[rgb]{0.941,1,1}}Ours & 73.22 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}32 & $\times $\textbf{10.96} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}$\times$1.00 & 73.33{\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} \textbf{+0.11} \\


\hline
\multirow{12}{*}{\textbf{ResNet-50 }} & ABC-Net                          & 76.10                         & 5                               & 5                               &$\times$ 6.40                          & $\times$6.40                           &  70.10                         & -6.00                            \\
                                      & LQ-Nets                          & 76.40                         & 3                               & 3                               & $\times$5.99                         & $\times$10.64                         & 74.20                         & -2.20                          \\
                                      & LQ-Nets                          & 76.40                         & 4                               & 4                               & $\times$5.11                         & $\times$7.99                          & 75.10                         & -1.30                          \\
                                      & DoReFa                           & 76.90                         & 4                               & 4                               & $\times$5.11                         & $\times$7.99                          & 71.40                         & -5.50                          \\
                                      & PACT                             & 76.90                         & 32                              & 4                               & $\times$1.00                            & $\times$7.99                          & 75.90                         & -1.00                            \\
                                      & PACT                             & 76.90                         & 2                               & 4                               & $\times$7.24                         & $\times$7.99                          & 74.50                         & -2.40                          \\
                                      & AutoQ                            & 74.80                         & MP                              & MP                             & $\times$10.26                        & $\times$7.96                          & 72.51                        & -2.29                         \\
                                      & HAWQ                             & 77.39                        & 2$_{\textbf{MP}}$                             & 4$_{\textbf{MP}}$                             & $\times$12.28                        & $\times$8.00                             & 75.48                        & -1.91                         \\
                                      & HAWQ-v2                          & 77.39                        & 2$_{\textbf{MP}}$                             & 4$_{\textbf{MP}}$                             & $\times$12.24                        & $\times$8.00                             & 75.76                        & -1.63                         \\
                                      
                                      & MCKP                             & 76.13                        & 2$_{\textbf{MP}}$                             & 4$_{\textbf{MP}}$                             & $\times$12.24                        & $\times$8.00                             & 75.28                        & \textbf{-0.85}                         \\
                                      & {\cellcolor[rgb]{0.941,1,1}}Ours & 76.12 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}4$_{\textbf{MP}}$ & $\times$\textbf{13.53} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} $\times$8.00  & {\cellcolor[rgb]{0.941,1,1}} 75.13 & {\cellcolor[rgb]{0.941,1,1}} -0.99  \\
                                      & HAQ                              & 76.15                        & MP                             & 32                              & $\times$10.57                        & $\times$1.00                             & 75.30                         & -0.85                         \\
                                        & {\cellcolor[rgb]{0.941,1,1}}Ours & 76.12 {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}}2$_{\textbf{MP}}$ & {\cellcolor[rgb]{0.941,1,1}}32 & $\times$\textbf{12.74} {\cellcolor[rgb]{0.941,1,1}} & {\cellcolor[rgb]{0.941,1,1}} $\times$1.00  & {\cellcolor[rgb]{0.941,1,1}} 76.09 & {\cellcolor[rgb]{0.941,1,1}} \textbf{-0.03 } \\

                                      
\hline
\end{tabular}} \label{tab1}
\vspace{-0.3cm}
\end{table*}



\subsection{Ablation}
\textbf{Differential Expansion. }
Differential expansion requires ensuring that the neighborhood around the expansion point is sufficiently small to guarantee that the theoretical approximation effectively represents the actual values within this neighborhood. Figure \ref{gap} shows the discrepancies between theoretical and actual values under different perturbation errors introduced into each layer. Taking ResNet-18 as an example, the gap between theoretical and actual values is minimal across layers, which validates the effectiveness of CET. Additionally, error perturbation experiments reveal that different layers exhibit distinct levels of error tolerance.

\begin{figure}[!ht]
\centering
% \vspace{-0.1cm}
\includegraphics[width=5.9cm,height=4.0cm]{image/evalue.pdf}
\vspace{-0.3cm}
\caption{As the number of short-axis eigenvalues increases, the number of equations in the underdetermined system of Eq.\ref{eq13} also grows, leading to a decrease in model loss. However, this increase is accompanied by a corresponding rise in computational overhead (the area of the circle increases).}
\vspace{-0.6cm}
\label{evalue}
\end{figure}



\textbf{Gradient and Short Axis Eigenvalue. }
Theoretically, the gradient of a well-trained model approach zero. Practical inspection reveals the first-order gradient values are small, with most being less than $10^{-5}$. As a result, the influence of the first-order term on the loss can be safely neglected. Given the high dimensionality of the Hessian matrix, computing all eigenvalues directly is infeasible. To balance memory consumption and computational efficiency, CET truncates the short-axis eigenvalues, calculating only a subset of them. Figure \ref{evalue} illustrates the impact of truncated short-axis eigenvalues on the loss. When the number of eigenvalues is set to 50, 100, 200, and 500, the loss shows a decreasing trend. However, beyond 500 eigenvalues, the rate of loss reduction diminishes sharply, while the computational cost rises sharply. Consequently, we select 200 short-axis eigenvalues as a compromise in implementation.



\begin{table}
\centering
\vspace{-0.3cm}
\caption{Comparison with existing mixed precision methods on the MobileNet-V2 model.}
\renewcommand{\arraystretch}{1.06}
\setlength{\tabcolsep}{2.8pt}
\scalebox{0.83}{
\begin{tabular}{ccccccc} 
\hline
\textbf{Method}                & \textbf{w-bit} & \textbf{a-bit} & \textbf{w-ratio} & \textbf{a-ratio} & \textbf{Top-1/Quant} & \textbf{Top-1/Drop}  \\ 
\hline
DC                             & MP             & 32             & $\times$13.93            & $\times$1                & 58.07                & -13.8                \\
HAQ                            & MP             & 32             & $\times$14.07            & $\times$1                & 66.75                & -5.12                \\
MCKP                           & 2MP            & 8              &$\times$13.99            & $\times$4                & 68.52                & -3.36                \\
\rowcolor[rgb]{0.949,1,1} Ours & 2MP            & 32             &$\times$14.99                 &  $\times$1               &  70.14                    &  \textbf{ -1.69 }                  \\
\rowcolor[rgb]{0.949,1,1} Ours & 2MP            & 8              &  $\times$14.99                &   $\times$4              &    69.66                   &  \textbf{-2.17}                    \\
\hline
\end{tabular}} \label{tab2}
\vspace{-0.7cm}
\end{table}



\begin{table*}[!ht]
\centering
\vspace{-0.3cm}
\caption{Performance of the BERT\_base model on multiple language processing datasets when weights are quantized to $4_{MP}$ bits.}
\renewcommand{\arraystretch}{1.06}
\setlength{\tabcolsep}{17.5pt}
\scalebox{0.83}{
\begin{tabular}{c|ccccccc}
\hline
\multicolumn{1}{c|}{\multirow{2}{*}{Method}} & \multicolumn{3}{c}{SQuAD1.1} & \multicolumn{2}{c}{MNLI} & SWAG  & w-ratio \\ \cline{2-8} 
\multicolumn{1}{c|}{}                        & Acc on Val  & F1     & EM    & Acc on Val & Acc on Test & Acc   &         \\ \hline
Full Prec.                                   & 85.74       & 88.42  & 80.89 & 82.77      & 84.57       & 79.11 &  $\times$1.00       \\
ACIQ                                         & 80.11       & 84.18  & 77.34 & 76.37      & 79.64       & 76.52 &$\times$6.07   \\
Zhang et.al.                                 & 85.67       & 88.16  & 80.42 & \textbf{82.78}      & \textbf{83.92}       & 78.30  & $\times$1.58   \\
\rowcolor[rgb]{0.949,1,1} Ours                                         & \textbf{85.69}      & \textbf{88.16}  & \textbf{80.57} & 82.75      & 82.96       & \textbf{79.04} & \textbf{$\times$7.71}   \\ \hline
\end{tabular}} \label{tab3}
\vspace{-0.5cm}
\end{table*}
\vspace{-0.1cm}
\subsection{Comparison}
\textbf{Algorithm Accuracy.} As shown in Table \ref{tab1}, we conducted a comprehensive comparison between CET and existing quantization methods \cite{zhang2018lq, lin2017towards, zhou2016dorefa, choi2018pact, han2015deep, wang2019haq, dong2019hawq}. For ResNet-18 \cite{he2016deep}, CET achieved nearly lossless weight quantization, with model performance degrading by only 0.01\%, while significantly reducing the model size, achieving a compression ratio of over 11$\times$. This demonstrates the effectiveness of CET's geometric analysis based on second-order information, enabling precise weight compression along the long-axis direction and selecting appropriate error tolerance for each layer. Furthermore, to investigate the impact of activation compression on weight quantization, we performed quantization on activations with different bit widths (8-bit and 4-bit). The results show that CET can maximize model compression while maintaining accuracy. Notably, CET does not rely on fine-tuning but directly determines the optimal bit-width allocation, which distinguishes it from methods like MCKP that require fine-tuning.

For ResNet-50, CET demonstrates significant performance advantages under higher compression rates. Compared with HAWQ and HAWQ-V2, which also utilize second-order information, CET achieves smaller accuracy degradation under more extreme compression scenarios (-0.99\% vs. -1.91\%). Additionally, compared with HAQ, which searches for optimal bit-width allocation via reinforcement learning, CET achieves a higher weight compression rate with minimal accuracy degradation and significantly lower computational cost. For example, after achieving a 12.74$\times$ weight compression, CET reduces model accuracy by only 0.034\%, effectively realizing lossless compression.
Compared with MCKP, CET achieves minimal performance degradation even at a higher compression rate (13.53$\times$), while MCKP experiences similar performance degradation at a lower compression rate (12.24$\times$). This highlights CET's stronger robustness and generalization capabilities under higher compression demands, achieving superior performance retention at extreme compression ratios.

For ResNet-34, CET even improves model accuracy while achieving nearly 11$\times$ compression. This is due to ResNet-34's second-order geometric property, where the Hessian matrix at the convergence point is indefinite. CET performs quantization along the eigenvector direction corresponding to negative eigenvalues, reducing the loss and resulting in higher accuracy for the quantized model compared to the original. CET rigorously analyzes the Hessian matrix's shape using algebraic geometry, enabling it to select the optimal quantization direction.
In Table \ref{tab2}, finally, we further evaluate CET on the lightweight and efficient MobileNet-V2 architecture. The results show that CET achieves significant performance improvements even at higher compression rates. This further validates the rationality and broad applicability of CET's second-order geometric analysis.



Table \ref{tab3} presents CET's performance across various NLP datasets. With 4-bit mixed-precision weight quantization, CET achieves accuracy comparable to the original model. While Zhang et al.’s method shows a slight advantage on the MNLI dataset, its low compression rate makes it difficult to maintain stable accuracy under high compression.


\textbf{Algorithm Efficiency.} The CET algorithm demonstrates high computational efficiency when solving indeterminate equations using gradient descent. Experiments show that it converges in approximately 2000 iterations, taking only a few minutes. Moreover, the main computational complexity and time consumption of the CET algorithm stem from the Lanzcos algorithm. This is because it requires calculating the Hessian information for each sample in the calibration set. This time consumption characteristic aligns with the computational speed of existing algorithms.








% \begin{table}
% \centering
% \vspace{-0.2cm}
% \caption{Performance of different activation bit-width quantization on ResNet-34 when weights are quantized at very low bit-width.}
% \renewcommand{\arraystretch}{1.06}
% \setlength{\tabcolsep}{3.0pt}
% \scalebox{0.85}{
% \begin{tabular}{c|cccccc} 
% \hline
% Model                 & w-bit                          & a-bit                         & w-ratio                          & a-ratio                      & Top-1/Quant                      & Top-1/Drop                                \\ 
% \hline
% Origin                 & 32                             & 32                            & 1                                & 1                            & 73.22                            & 0                                         \\\hline
% \multirow{3}{*}{Ours} & 2MP                            & 4MP                           & 13.44                            & 8                            & 72.6                             & -0.62                                     \\
%                       & 2MP                            & 8                             & 10.96                            & 4                            & 73.04                            & -0.18                                     \\
%                       & {\cellcolor[rgb]{0.949,1,1}}2MP & {\cellcolor[rgb]{0.949,1,1}}32 & {\cellcolor[rgb]{0.949,1,1}}10.96 & {\cellcolor[rgb]{0.949,1,1}}1 & {\cellcolor[rgb]{0.949,1,1}}73.33 & {\cellcolor[rgb]{0.949,1,1}}\textbf{+0.11}  \\
% \hline
% \end{tabular}}\label{tab3}
% \vspace{-0.2cm}
% \end{table}

\subsection{Discussion}
\textbf{Limitations.} Ideally, the CET framework requires precise Hessian information to ensure accurate analysis of quantization errors. However, since directly computing the full Hessian matrix is infeasible in practice, we approximate it using the Lanzcos algorithm. While this approximation introduces some degree of error, our perturbation analysis of the Lanzcos algorithm reveals that it has a strong error tolerance. The approximate Hessian information obtained is robust in most cases, ensuring the practical applicability of the CET framework.

\textbf{Generality.} The CET framework is algorithm-agnostic and features strong interpretability and adaptability. It does not rely on any specific compression strategy but rather provides a universal theoretical foundation.
Consequently, CET can be seamlessly applied to various compression strategies. Offering an effective geometric analysis framework, it helps these methods identify optimal compression configurations.
\vspace{-0.5cm}
\section{Conclusion}
We proposed a Compression Error Theory (CET) framework designed to determine the optimal compression level for each layer of a model. During quantization, CET reconstructs the quadratic form of quantization error into different geometric structures and reformulates the optimization process as a complementary problem to solve the error subspace. Theoretical analysis shows that constructing the quantization subspace along the long axis minimizes the impact on model performance. Extensive experimental results further validate the effectiveness of CET. In the future, CET is a general compression theory framework that can be extended to other compression methods, such as weight decomposition.


% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2025 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited in pages, but the total file size may not exceed 10MB. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2025}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{42}^{nd}$ International Conference on Machine Learning},
% Vancouver, Canada, PMLR 267, 2025.
% Copyright 2025 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2025\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2025\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2025 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2025.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via OpenReview\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2025 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% \cref{alg:example} shows an example.

% \begin{algorithm}[tb]
%    \caption{Bubble Sort}
%    \label{alg:example}
% \begin{algorithmic}
%    \STATE {\bfseries Input:} data $x_i$, size $m$
%    \REPEAT
%    \STATE Initialize $noChange = true$.
%    \FOR{$i=1$ {\bfseries to} $m-1$}
%    \IF{$x_i > x_{i+1}$}
%    \STATE Swap $x_i$ and $x_{i+1}$
%    \STATE $noChange = false$
%    \ENDIF
%    \ENDFOR
%    \UNTIL{$noChange$ is $true$}
% \end{algorithmic}
% \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2025.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the OpenReview reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% % Acknowledgements should only appear in the accepted version.
% \section*{Acknowledgements}

% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% usually should) include acknowledgements.  Such acknowledgements
% should be placed at the end of the section, in an unnumbered section
% that does not count towards the paper page limit. Typically, this will 
% include thanks to reviewers who gave useful comments, to colleagues 
% who contributed to the ideas, and to funding agencies and corporate 
% sponsors that provided financial support.

% \section*{Impact Statement}

% Authors are \textbf{required} to include a statement of the potential 
% broader impact of their work, including its ethical aspects and future 
% societal consequences. This statement should be in an unnumbered 
% section at the end of the paper (co-located with Acknowledgements -- 
% the two may appear in either order, but both must be before References), 
% and does not count toward the paper page limit. In many cases, where 
% the ethical impacts and expected societal implications are those that 
% are well established when advancing the field of Machine Learning, 
% substantial discussion is not required, and a simple statement such 
% as the following will suffice:

% ``This paper presents work whose goal is to advance the field of 
% Machine Learning. There are many potential societal consequences 
% of our work, none which we feel must be specifically highlighted here.''

% The above statement can be used verbatim in such cases, but we 
% encourage authors to think about whether there is content which does 
% warrant further discussion, as this statement will be apparent if the 
% paper is later flagged for ethics review.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
