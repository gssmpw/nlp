\section{Related Works}
\subsection{Neural Network in Function}
We present the analysis of neural networks as composite functions. All our conclusions are independent of the structure of the neural network.
First, for an n-layer neural network model, the loss of the model is optimized according to the following equation
\begin{small} 
\begin{equation}
\begin{aligned} & \begin{aligned}
\min_{\textbf{W}}f(\textbf{W})=\mathbf{E}_{Sample}\ell(\textbf{W},Sample)=\frac{1}{m}\sum_{(x_{i},y_{i})\in\mathbb{D}}\ell(\textbf{W},x_{i},y_{i})\end{aligned},\\
&\ell(\textbf{W},x_{i},y_{i})=L(model_{n}(x_{i},\textbf{W}),y_{i}),\\  & model_{n}=h_1(h_2(h_3(h_4(\cdots(h_{n+1},w_{n})\cdots,w_4),w_3),w_2),w_1),\end{aligned}
\label{eq1}
\end{equation}\end{small}

\noindent where $f(\cdot)$ represents the loss of the model on a dataset, $\mathbf{E}$ stands for expectation, $m$ is the size of the dataset, $\ell(\cdot)$ is the loss function for a sample, and $(x_i,y_i)$ denotes a sample in the dataset along with its corresponding label, $L(\cdot)$ represents the loss function, such as the cross-entropy function; $h_i$, with $i\in[1,...,n]$ represents the ($n-i+1$)th layer in the neural network, $\textbf{W} = (w_n^T,w_{n-1}^T,\cdots,w_1^T)^T$, where $w_i$ is the parameter in $h_i(\cdot)$, and for the reason of a unified format, $h_{n+1}$ denotes the sample $x$.
When the model is treated as a complex high-dimensional nonlinear mapping, it encapsulates the structural constraints of the network and the characteristics of the loss function. Its local properties can be studied through differential and algebraic geometry methods to uncover the local shape characteristics of the function.
\subsection{Quantization for Compression}
The computational units of deep models are primarily composed of matrix multiplication. Quantization accelerates the multiplication process by converting floating-point parameters into lower-bit formats, thus speeding up the inference process. For a single layer of a neural network, it is represented as $Y=X\cdot W \in \mathbb{R}^{S\times C_{out}}$, where $X\in \mathbb{R}^{S\times C_{in}}$ is the activation input and $W \in \mathbb{R}^{C_{in}\times C_{out}}$ is the weight matrix. Taking integer uniform quantization as an example, the $b$-bit quantization process maps the FP16/32 weight tensor $W$ to a lower-bit integer $W_q$.
\begin{equation}
\begin{aligned}
Q(\textbf{W})&=\mathrm{clamp}\left(\left\lfloor\frac{\mathbf{W}}{\Delta}\right\rceil+z,0,2^b-1\right) \\
\mathrm{where}\quad\Delta&=\frac{\max(\mathbf{W})-\min(\mathbf{W})}{2^b-1},z=-\left\lfloor\frac{\min(\mathbf{W})}{\Delta}\right\rceil
\end{aligned}
\label{quant}
\end{equation}
The notation is $\lfloor\cdot\rceil$ means the nearest rounding operation, $\Delta$ is the quantization step size and $z$ represents the zero point.
When adopting minimum square error (MSE) as the criterion, the quantization process is expressed as the following minimization error problem:
\begin{equation}
\operatorname*{min}\|\textbf{W}-Q(\textbf{W})\|_{2}\quad s.t.Q(\textbf{W})\in\Pi_{b}
\end{equation}
$Q(\textbf{W})$:$\mathbb{R}^D\times\mathbb{Z}^+\to\Pi_b$ is the quantization function (Equation \ref{quant}).
Existing methods focus on reducing parameter errors through hybrid quantization schemes, which can be divided into two categories: search-based and tolerance-based methods.
Search-based methods, such as those by LeCun et al., "Optimizing Neural Networks"__Mao et al., "Learning Compact Neural Networks via Deep Group Lasso"
To optimize efficiency, another category of tolerance-based methods measures each layer's tolerance to quantization errors. When the tolerance of a layer is higher, the layer can be quantized with a lower bit-width. Various sensitivity metrics have been proposed in practice, such as the Kullback-Leibler divergence between the quantized layer output and the full-precision layer output (Hou et al., "Lossy Compression of Neural Networks via Bit-Packing Quantization")__, the maximum eigenvalue of the Hessian (Zhou et al., "Deep Learning Model Compression via Layer-Wise Optimal Quantization")__, the trace of the Hessian (Geng et al., "Layer-Wise Neural Network Pruning for Model Compression")__), the Gaussian-Newton matrix approximation of the Hessian (Wu et al., "Model Pruning via Weighted Group Lasso")__), or the quantization factor (Hou et al., "Deep Quantization with Layer-by-Layer Bit-Packing"__)_. All tolerance-based methods minimize the sum of tolerance across layers under the constraint of target compression ratio.
Although these methods are effective in practice, they lack a theoretical foundation to justify the optimality of their results. Furthermore, since these methods do not consider the relationship between model performance and parameter errors, they struggle to address model degradation caused by compression errors at lower bit widths, which lead to retraining failures or sharp performance drops.
\vspace{-0.4cm}