
\begin{figure}[t]
    \centering
    \includegraphics[width = 0.99\linewidth]{images/nc_ood_detect_transfer_corr_vgg_updated.png}
  \caption{In this paper, we show that there is a close inverse relationship between OOD detection and generalization with respect to the degree of representation collapse in DNN layers. This plot illustrates this relationship for VGG17 pretrained on ImageNet-100 using four OOD datasets, where we measure collapse and OOD performance for various layers. For OOD detection, there is a strong positive Pearson correlation ($R=0.77$) with the degree of neural collapse (NC1) in a DNN layer, whereas for OOD generalization, there is a strong negative correlation ($R=-0.60$). We rigorously examine this inverse relationship and propose a method to control NC at different layers. %This suggests that stronger neural collapse improves OOD detection, while weaker neural collapse enhances OOD generalization. $R$ denotes the Pearson correlation coefficient.
  %Neural collapse metric NC1 (lower values indicate stronger neural collapse) positively correlates with OOD detection error and negatively correlates with OOD transfer error. This implies the stronger the neural collapse, the lower the OOD detection error and vice-versa. And, the weaker the neural collapse, the lower the OOD transfer error. For this, we analyze different layers of VGG17 networks which are pre-trained on the ImageNet-100 (ID) dataset, and evaluated on four OOD datasets. %e.g., ImageNet-R-200, Flowers-102, NINCO-64, and STL-10. 
  %$R$ denotes the Pearson correlation coefficient.
  } 
  \label{fig:vis_abstract}
  \vspace{-0.21in}
\end{figure}



% \begin{figure}[t]
%     \centering
%     \includegraphics[width = 0.99\linewidth]{images/bar_plot_ood_nc_summary.png}
%   \caption{\textbf{Controlling neural collapse (NC) enhances OOD transfer at encoder and OOD detection at projector.} The encoder achieves higher OOD transfer (indicated by lower error averaged across 8 OOD datasets) while decreasing NC (higher scores indicate lower NC). On the contrary, the projector achieves higher OOD detection (indicated by lower FPR95 averaged across same 8 OOD datasets) while increasing NC (lower scores indicate higher NC). Therefore, NC shows a \emph{linear} relationship with OOD transfer and OOD detection. In nutshell, encoder is a good OOD generalizer but a bad OOD detector whereas opposite is true for projector. All values are percentages.} 
%   \label{fig:vis_abstract}
% \end{figure}




%\begin{figure}[t]
%    \centering
%    \includegraphics[width = 0.99\linewidth]{images/vgg17_tunnel_koleo.png}
%  \caption{\textbf{Mitigating the tunnel effect leads to improved generalization.} The tunnel effect causes impaired OOD generalization for linear probes trained on embeddings from later layers in overparameterized DNNs. Typically, a supervised learning (SL) model suffers from the tunnel effect, as shown by \textcolor{blue}{blue} curves, where OOD accuracy significantly degrades in top layers (9-16). Whereas, using mitigation approach such as KoLeo regularization mitigates the tunnel effect and improves OOD performance, as shown by \textcolor{orange}{orange} curves. In this comparison, SL achieves 15.14\% average accuracy over 8 OOD datasets whereas SL+KoLeo (27.15\%) improves average accuracy by absolute 12\%.} 
%  \label{fig:tunnel_effect_koleo}
%\end{figure}