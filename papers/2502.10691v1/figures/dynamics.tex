\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/train_dynamics.png}
        \caption{Impact of Entropy Regularization on NC1}
        \label{fig:entropy_reg}
    \end{subfigure}
    %\hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/train_dynamics_entropy.png}
        \caption{Entropy Dynamics}
        \label{fig:entropy-dynamics}
    \end{subfigure}
    %
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/train_dynamics_rank.png}
        \caption{Effective Rank Dynamics}
        \label{fig:effective-rank}
    \end{subfigure}
    %%
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/train_dynamics_l2.png}
        \caption{Impact of $L_2$ Normalization on NC1}
        \label{fig:l2_norm}
    \end{subfigure}
    
    \caption{\textbf{Analyzing entropy regularization \& $\mathbf{L_2}$ normalization.} 
    \textcolor{blue}{\textbf{(a)}} Entropy regularization reduces neural collapse (indicated by higher NC1 values) in the encoder. %, promoting OOD generalization. 
    \textcolor{blue}{\textbf{(b)}} Entropy regularization increases the entropy of encoder embeddings otherwise entropy remains unchanged.
    \textcolor{blue}{\textbf{(c)}} Entropy regularization increases the effective rank of encoder embeddings otherwise effective rank remains as low as the number of classes (i.e., 10 ImageNet classes).
    \textcolor{blue}{\textbf{(d)}} $L_2$ normalization increases neural collapse (indicated by lower NC1 values) in the projector. %, promoting OOD detection. 
    For this analysis, we train VGG17 networks on the ImageNet-10 subset (10 ImageNet classes) for 100 epochs.
    }
    \label{fig:nc_dynamics}
\end{figure*}