
\section{Problem Definition}
\label{sec:problem_definition}
%% OOD Generalization plus Detection

We consider a supervised multi-class classification problem where the input space is \( \mathcal{X} \) and the label space is \( \mathcal{Y} = \{1, 2, \dots, K\} \). 
A model parameterized by \( \theta \), \( f_\theta: \mathcal{X} \to \mathbb{R}^K \), is trained on ID data drawn from \( P_{\mathcal{X}\mathcal{Y}}^{\text{ID}} \), to produce logits, \(f_\theta (x)\) which are used to predict labels. 
For robust operation in real-world scenarios, the model must classify samples from \( P_{\mathcal{X}\mathcal{Y}}^{\text{ID}} \) correctly and identify OOD samples from \( P_{\mathcal{X}\mathcal{Y}'}^{\text{OOD}} \) which represents the distribution with no overlap with the ID label space, i.e., \( \mathcal{Y} \cap \mathcal{Y}' = \emptyset \).

At test time, the objective of OOD detection is to determine whether a given sample \( x \) originates from the ID or an OOD source. This can be achieved by using a threshold-based decision rule through level set estimation, defined as:
\begin{equation*}
    G_\lambda(x) = 
    \begin{cases} 
    \text{ID}, & S(x) \geq \lambda \\ 
    \text{OOD}, & S(x) < \lambda
    \end{cases}
\end{equation*}
where \( S (\cdot) \) is a scoring function, and samples with \( S(x) \geq \lambda \) are classified as ID, while those with \( S(x) < \lambda \) are classified as OOD. \( \lambda \) denotes the threshold.

On the OOD generalization part, the objective is to build a model \( f^*_\theta: \mathcal{X} \rightarrow \mathbb{R}^{K} \), using the ID data such that it learns \emph{transferable} representations and becomes adept at both ID task and OOD downstream tasks. This is a challenging problem since we do not have access to OOD data during training.
In both OOD detection and OOD generalization, the label space is disjoint between ID and OOD sets.

\textbf{Differences from prior works.}
Prior works~\cite{zhang2024best, wang2024bridging, bai2023feed} focusing on OOD detection and OOD generalization, define the problem differently than us. For OOD detection, they use ``\emph{semantic OOD}'' data from \( P_{\mathcal{X}\mathcal{Y}'}^{\text{SEM}} \) that has no semantic overlap with known label space from \( P_{\mathcal{X}\mathcal{Y}}^{\text{ID}} \), i.e., \( \mathcal{Y} \cap \mathcal{Y}' = \emptyset \). However, for OOD generalization, they use ``\emph{covariate-shifted OOD}'' data from \( P_{\mathcal{X}\mathcal{Y}}^{\text{COV}} \), that has the same label space as \( P_{\mathcal{X}\mathcal{Y}}^{\text{ID}} \) but with shifted marginal distributions \( P_\mathcal{X}^{\text{COV}} \) due to noise or corruption. Furthermore, they use additional semantic OOD training data, \( P_{\mathcal{X}\mathcal{Y}'}^{\text{SEM}} \) during the training phase.
Our problem definition is fundamentally more challenging and practical than the prior works because: (1) we aim to detect semantic OOD samples, \( P_{\mathcal{X}\mathcal{Y}'}^{\text{SEM}} \) without access to auxiliary OOD data during training, and (2) we aim to generalize to semantic OOD samples that belong to novel semantic categories.
%(2) we aim to generalize to semantic OOD samples (concept shifts, changes in the conditional distribution, \( P_\mathcal{Y|X} \)).
% \begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
%     \item We aim to detect semantic OOD samples, \( P_{\mathcal{X}\mathcal{Y}'}^{\text{SEM}} \) without seeing them during training, and
%     \item We aim to generalize to semantic OOD samples (concept shifts, changes in the conditional distribution, \( P_\mathcal{Y|X} \)).
% \end{enumerate}

%Also, ours is more realistic and intriguing because the real-world distribution will typically diverge from training distribution and training on all future variations is intractable. Moreover, our definition is a prerequisite for open-world learning scenarios. In this work, we focus on building representations that will result in an effective classifier, OOD detector, and universal feature extractor.




\textbf{Evaluation Metrics.} We define ID generalization error ($\mathcal{E}_{\text{ID}}$), OOD generalization error ($\mathcal{E}_{\text{GEN}}$), and OOD detection error ($\mathcal{E}_{\text{DET}}$) as follows:
\begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
    \item $\downarrow \mathcal{E}_{\text{ID}} := 1 - \mathbb{E}_{(\bar{x}, y) \sim P^{\text{ID}}} \big(\mathbbm{I}\{\hat{y}(f_\theta(\bar{x})) = y\}\big)$,
    \item $\downarrow \mathcal{E}_{\text{GEN}} := 1 - \mathbb{E}_{(\bar{x}, y) \sim P^{\text{OOD}}} \big(\mathbbm{I}\{\hat{y}(f_\theta(\bar{x})) = y\}\big)$,
    \item $\downarrow \mathcal{E}_{\text{DET}} := \mathbb{E}_{\bar{x} \sim P^{\text{OOD}}} \big(\mathbbm{I}\{G_\lambda(\bar{x}) = \text{ID}\}\big)$,
\end{enumerate}
where $\mathbbm{I}\{\cdot\}$ denotes the indicator function, and the arrows indicate that lower is better. For OOD detection, ID samples are considered positive. FPR95 (false positive rate at 95\% true positive rate) is used as $\mathcal{E}_{\text{DET}}$. Details are in Appendix.
%In particular, we use FPR95 (FPR at 95\% True Positive Rate) that evaluates OOD detection performance by measuring the fraction of OOD samples misclassified as ID where $\lambda$ is chosen when the true positive rate is 95\%. 
%Both OOD detection and OOD generalization tasks are evaluated on the \emph{same} OOD test set.


\textbf{OOD Detection.}
% There are many methods to define the scoring function for OOD detection such as MSP~\cite{hendrycks2016baseline}, ODIN~\cite{liang2018enhancing}, Mahalanobis~\cite{lee2018simple}, ReAct~\cite{sun2021react}, Energy~\cite{liu2020energy}, and so on.
Following earlier work~\cite{sun2021react, liu2020energy}, we consider the energy-based scoring~\cite{liu2020energy} since it operates with logits and does not require any fine-tuning or hyper-parameters.
Scoring in energy-based models is defined as  
%\begin{equation*}
%    S(x) = -\log \sum_{k=1}^K e^{f_k(x)}
%\end{equation*}
\begin{equation*}
    S(x) = -\log \sum_{k=1}^K \exp{\left( f_k\left(x\right) \right)}
\end{equation*}
where the \( k \)-th logit, \( f_k(x) \), denotes the model's confidence for assigning \( x \) to class \( k \). Note that \citet{liu2020energy} uses the negative energy, meaning that OOD samples should obtain high energy, hence low $S(x)$. See Fig.~\ref{fig:more_eng_id_ood} as an example.

\textbf{OOD Generalization.}
For evaluating OOD generalization, we consider linear probing which is widely used to evaluate the transferability of learned embeddings to OOD datasets~\cite{alain2016understanding,masarczyk2023tunnel,zhu2022ood_probe,waldis2024dive_probe,grill2020bootstrap,he2020momentum}. 
For a given OOD dataset, we extract embeddings from a pre-trained model. A linear probe (MLP classifier) is then attached to map these embeddings to OOD classes. The probe is trained and evaluated on OOD data.
% pre-trained backbone is frozen.


%% Details can be placed in Appendix
%At first, for a particular OOD dataset, we extract embeddings from the pre-trained model (pre-trained on ID dataset) at a particular layer $l$. Next, we attach a linear probe (an MLP classifier layer) to layer $l$ which then maps extracted embeddings to OOD classes. The linear probe is trained on the OOD training embeddings and finally evaluated on OOD test embeddings. Embeddings for each layer are produced via global average pooling. 
%Additional details are given in Appendix~\ref{sec:feat_extract_details}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\input{figures/nc_implication}

\section{Controlling Neural Collapse}
\label{sec:framework}

Typically, penultimate embeddings from a pre-trained DNN are used for downstream tasks. However, using the same embedding space for both OOD detection and OOD generalization is suboptimal due to their conflicting objectives. We therefore propose separate embedding spaces at different layers—one for OOD detection, another for OOD generalization. Specifically, we attach a \texttt{projector} network $g(\cdot)$ to the DNN backbone $f(\cdot)$ (the \texttt{encoder}) and add a \texttt{classifier head} $h(\cdot)$ on top. Given an input $\mathbf{x}$, the encoder outputs $\mathrm{\mathbf{f}} = f(\mathbf{x})$, e.g., a 512-dimensional vector for ResNet18. The projector then maps $\mathrm{\mathbf{f}}$ to $\mathrm{\mathbf{g}} = g(\mathrm{\mathbf{f}})$, and finally the classifier produces logits $\mathrm{\mathbf{h}} = h(\mathrm{\mathbf{g}}) \in \mathbb{R}^K$.

% Typically, penultimate embeddings of a pre-trained DNN are utilized to perform downstream tasks. However, given the conflict between OOD detection and OOD generalization, using the same embedding space to accomplish both tasks is sub-optimal.
% To address this, we propose constructing separate embedding spaces at different layers—one optimized for OOD detection and another for OOD generalization. 
% Specifically, we introduce a \texttt{projector} network $g(\cdot)$ attached to the DNN backbone $f(\cdot)$, referred to as \texttt{encoder}. 
% On the top, a \texttt{classifier head} $h(\cdot)$ is added. Given an input $\mathbf{x}$, encoder outputs an embedding, $\mathrm{\mathbf{f}} = f(\mathbf{x})$. For instance, $\mathrm{\mathbf{f}}$ is a 512-dimensional vector for the ResNet18 encoder after applying average pooling. Next, projector maps $\mathrm{\mathbf{f}}$ into a projector embedding $\mathrm{\mathbf{g}} = g(\mathrm{\mathrm{\mathbf{f}}})$. Finally, the classifier outputs logits, $\mathrm{\mathbf{h}} \in \mathbb{R}^K$, i.e., $\mathrm{\mathbf{h}} = h(g(f(\mathbf{x})))$.


The encoder is trained to prevent NC and encourage transferable representations for OOD generalization, while the projector is designed to induce NC, producing collapsed representations beneficial for OOD detection. 
%As a result, the encoder embeddings exhibit higher within-class covariance \( \Sigma_W \), whereas the projector embeddings shrink \( \Sigma_W \), inhibiting transferability. 
A high-level illustration is provided in Fig.~\ref{fig:nc_implication}.
For OOD detection and ID classification tasks, the entire network ($h \circ g \circ f$) is utilized, assuming projector embedding $\mathrm{\mathbf{g}}$ is most discriminative among all layers.
Whereas the encoder alone is utilized for OOD generalization, assuming encoder embedding $\mathrm{\mathbf{f}}$ is most transferable among all layers.
In the following subsections, we will portray how we can build these collapsed and transferable representations.


%To address this, we want to build suitable embedding spaces in different locations of a DNN, one for OOD detection and another for OOD generalization.
%Toward this goal, we want to incorporate a \texttt{projector} $g(\cdot)$ attached to the original DNN $f(\cdot)$ (referred to as \texttt{encoder}). 
%On the top, a \texttt{classifier head} $h(\cdot)$ is added. Given an input $\mathbf{x}$, encoder outputs an embedding, $\mathrm{\mathbf{f}} = f(\mathbf{x})$. For instance, $\mathrm{\mathbf{f}}$ is a 512 dimensional vector for ResNet18 encoder after applying average pooling. Next, projector maps $\mathrm{\mathbf{f}}$ into a projector embedding $\mathrm{\mathbf{g}} = g(\mathrm{\mathrm{\mathbf{f}}})$. Finally, classifier outputs logits, $\mathrm{\mathbf{h}} \in \mathbb{R}^K$, i.e., $\mathrm{\mathbf{h}} = h(g(f(\mathbf{x})))$.
%The key idea is to shape the embedding space such that the encoder prevents NC and forms \emph{transferable representations} for OOD generalization. Conversely, the projector induces NC and builds \emph{collapsed representations} for OOD detection. 
%Consequently, encoder embeddings will exhibit higher within-class covariance \( \Sigma_W \). On the contrary, \( \Sigma_W \) will shrink in the projector embeddings, inhibiting transferability. 
%An illustrative high-level overview is shown in Fig.~\ref{fig:nc_implication}.


%In the following subsections, we will portray how we can build these collapsed and transferable representations. %in a principled way. 
%Particularly, Sec.~\ref{sec:entropy_and_collapse} describes how entropy regularization improves OOD generalization and Sec.~\ref{sec:etf_projector_nc} describes how simplex ETF projector achieves OOD detection.


%%% Connection between NC and Entropy. Higher NC leads to lower entropy.


\subsection{Entropy Regularization Mitigates Neural Collapse}
\label{sec:entropy_and_collapse}

In this section, we provide a theoretical justification for using an entropy regularizer to prevent or mitigate \emph{intermediate neural collapse} (NC1) in deep networks. 
By ``intermediate'' we mean that the collapse occurs in hidden layers.
% By ``intermediate'' we mean that the collapse can occur not only in the penultimate layer but also in earlier hidden layers.

\paragraph{Setup and Notation.}
Let $L$ be the total number of layers in our network, and $\ell \in \{1,2,\ldots,L\}$ the intermediate layer index. 
% fix any intermediate layer index $\ell \in \{1,2,\ldots,L\}$.
We denote the embedding (activation) in layer~$\ell$ for the $i$-th sample $\mathbf{x}_i$ as $ \mathbf{z}_{\ell,i} = f_\ell(\mathbf{x}_i)$, where $\mathbf{z}_{\ell,i} \in \mathbb{R}^{d_\ell}$,
% \[
%   \mathbf{z}_{\ell,i} \;=\; f_\ell(\mathbf{x}_i), \quad
%   \mathbf{z}_{\ell,i} \in \mathbb{R}^{d_\ell},
% \]
% the embedding (activation) in layer~$\ell$ for the $i$-th sample $\mathbf{x}_i$.
Suppose we have $K$ classes, labeled by $1,\dots,K$.  We can view the random variable $\mathbf{Z}_\ell$ (the layer-$\ell$ embedding) as distributed under the data distribution according to
\begin{equation*}
  p_\ell(\mathbf{z}) 
  \;=\; 
  \sum_{k=1}^K \pi_k \, p_{\ell,k}(\mathbf{z})
\end{equation*}
where $\pi_k = \Pr(y = k)$ is the class prior, and $p_{\ell,k}(\mathbf{z})$ is the class--conditional distribution of $\mathbf{Z}_\ell$ for label~$k$. 

\paragraph{Intermediate Neural Collapse (NC1).}
Empirically, \emph{neural collapse} is observed when the within-class covariance of these embeddings shrinks as training proceeds. Formally, for each class~$k$, the distribution $p_{\ell,k}$ concentrates around its class mean $\boldsymbol{\mu}_{\ell,k} \in \mathbb{R}^{d_\ell}$, resulting in:
\[
  \mathrm{Trace}(\boldsymbol{\Sigma}_{\ell,k}) 
  \;\to\; 0,
  \quad\text{where}\quad
  \boldsymbol{\Sigma}_{\ell,k} 
  = 
  \mathrm{Cov}\bigl(\mathbf{Z}_\ell \,\mid\, y=k\bigr).
\]
Although often highlighted in the \emph{penultimate} layer, such collapse can appear across the final layers of a DNN~\cite{rangamani2023feature,harun2024what}.

\paragraph{Differential Entropy and Collapsing Distributions.}
For a continuous random variable $\mathbf{Z}_\ell \in \mathbb{R}^{d_\ell}$ with density $p_\ell(\mathbf{z})$, the \emph{differential entropy} is given by
\begin{equation*}
   H\bigl(p_\ell\bigr) 
   \;=\; 
   - \int_{\mathbb{R}^{d_\ell}}
     p_\ell(\mathbf{z}) \,\log p_\ell(\mathbf{z})\, d\mathbf{z}
\end{equation*}
It is well known that if $p_{\ell,k}$ collapses to a delta (or near-delta) around $\boldsymbol{\mu}_{\ell,k}$, then $H(p_{\ell,k}) \to -\infty$ \cite{cover1999elements}.  Consequently, a \emph{mixture} of such collapsing class--conditional distributions also attains arbitrarily negative entropy.  The following proposition formalizes this point. %Appendix~\ref{sec:details_proposition} contains the detailed proof.

\begin{proposition}[Entropy under Class--Conditional Collapse]
\label{prop:collapse_entropy}
Consider a mixture distribution 
\(
  p_\ell(\mathbf{z}) \;=\; \sum_{k=1}^K \pi_k\,p_{\ell,k}(\mathbf{z})
\)
on $\mathbb{R}^{d_\ell}$.
Suppose that, for each $k$, $p_{\ell,k}$ becomes arbitrarily concentrated around a single point $\boldsymbol{\mu}_{\ell,k}$.
In the limit where each $p_{\ell,k}$ approaches a Dirac delta, the differential entropy $H\bigl(p_\ell\bigr)$ diverges to $-\infty$. 
\end{proposition}


\begin{proof}[Proof Sketch]
If each $p_{\ell,k}$ is a family of densities approaching $\delta(\mathbf{z}-\boldsymbol{\mu}_{\ell,k})$, the individual entropies $H\bigl(p_{\ell,k}\bigr)$ go to $-\infty$. The entropy of the mixture can be bounded above by the weighted sum of $H\bigl(p_{\ell,k}\bigr)$ plus a constant that depends on the mixture overlap. Hence, the overall mixture entropy also diverges to $-\infty$. 
\end{proof}
Appendix~\ref{sec:details_proposition} contains the detailed proof.



\paragraph{Entropy Regularization to Mitigate Collapse.}
We see from Proposition~\ref{prop:collapse_entropy} that if \emph{all} class--conditional distributions collapse to near-deltas, the layer's overall density $p_\ell(\mathbf{z})$ has differential entropy $H(p_\ell)\to -\infty$.  Since standard classification objectives can favor very tight class clusters (e.g., to sharpen decision boundaries), one can counteract this by \emph{maximizing} $H\bigl(p_\ell\bigr)$.

Concretely, we augment the training loss $\mathcal{L}_{\mathrm{cls}}(\theta)$ with a negative--entropy penalty:
\begin{equation}
\label{eq:entropy_regularizer}
   \mathcal{L}_{\mathrm{total}}(\theta)
   \;=\;
   \mathcal{L}_{\mathrm{cls}}(\theta)
   \;-\;
   \alpha \, H\bigl(p_\ell(\mathbf{z}\mid\theta)\bigr),
\end{equation}
where $\alpha>0$ is a hyperparameter.  As $\boldsymbol{\Sigma}_{\ell,k}\to 0$ would force $H\bigl(p_\ell\bigr)$ to $\!-\infty$ (cf.\ Proposition~\ref{prop:collapse_entropy}), the additional term $-\alpha H\bigl(p_\ell\bigr)$ becomes unboundedly large.  Therefore, the model is compelled to maintain \emph{nonzero within--class variance} for each class distribution, preventing complete layer collapse.


%\paragraph{Conclusion.}
%By introducing a positive entropy term for an intermediate layer's activation distribution, we impose a strong incentive \emph{against} collapsing each class to an infinitesimal cluster.  This \emph{entropy regularizer} thus provides a direct mechanism to \emph{combat} intermediate neural collapse. In practice, one can implement $H\bigl(p_\ell\bigr)$ or its surrogates (like classwise covariance terms) via tractable approximations or variational bounds, ensuring the hidden features remain sufficiently ``spread out.''  

Since we do not have direct access to $p_\ell(\mathbf{z})$, we need to estimate $H\bigl(p_\ell\bigr)$ using a data-driven density estimation approach. %as a proxy. 
In particular, prior work~\cite{kozachenko1987sample, beirlant1997nonparametric} shows that the differential entropy can be estimated by nearest neighbor distances.

Given a batch of \( N \) random representations \( \{\mathbf{z}_n\}_{n=1}^N \), the nearest neighbor entropy estimate is given by
\begin{equation*}
    H\bigl(p_\ell\bigr) \approx \frac{1}{N} \sum_{n=1}^N \log \left( N \min_{i \in [N], i \neq n} \| \mathbf{z}_n - \mathbf{z}_i \|_2 \right) + \ln 2 + \text{EC}
\end{equation*}
where EC denotes the Euler constant. For our purposes, we can simplify the entropy maximization objective by removing affine terms, resulting in the following loss function:

\begin{equation*}
    \mathcal{L}_{\mathrm{reg}}(\theta) = -\frac{1}{N} \sum_{n=1}^N \log \left( \min_{i \in [N], i \neq n} \| \bar{\mathbf{z}}_n - \bar{\mathbf{z}}_i \|_2 \right)
\end{equation*}
Total loss becomes: $\mathcal{L}_{\mathrm{total}}(\theta) = \mathcal{L}_{\mathrm{cls}}(\theta) + \alpha \, \mathcal{L}_{\mathrm{reg}}(\theta)$.
Intuitively, $\mathcal{L}_{\mathrm{reg}}$ maximizes the distance between the nearest pairs in the batch, encouraging an even spread of representations across the embedding space.
The pairwise distances can be sensitive to outliers with large magnitudes. Therefore, in our method, the loss operates on the hyperspherical embedding space with the unit norm, i.e., \( \bar{\mathbf{z}} = \mathbf{z} / ||\mathbf{z}||_2 \). Note that unlike $\mathcal{L}_{\mathrm{cls}}$ acting on classifier head, $\mathcal{L}_{\mathrm{reg}}$ is applied in encoder for mitigating NC. 

%By maximizing entropy, $\mathcal{L}_{\mathrm{reg}}$ encodes more complex, diverse representation, while lower entropy indicates more redundant or compressed representations.

Although various loss functions including cross-entropy (CE) and mean-squared-error (MSE) lead to NC, others produce less transferable features than CE~\cite{zhou2022all, kornblith2021better}. We also find that CE outperforms MSE in both OOD detection and OOD generalization (see Table~\ref{tab:mse_ce_comp}).
Therefore, we consider CE loss for $\mathcal{L}_{\mathrm{cls}}$ in Equation~\ref{eq:entropy_regularizer}. 
It has been found that using label smoothing with CE loss intensifies NC properties when compared with the regular CE loss~\cite{zhou2022all, kornblith2021better}. Therefore, we use label smoothing with CE loss to expedite NC properties in the projector and classifier head.

% Note that unlike $\mathcal{L}_{\mathrm{cls}}$ acting on classifier head, $\mathcal{L}_{\mathrm{reg}}$ is applied in encoder for mitigating NC. 
%If we denoted encoder embeddings by $\mathbf{z}_{e}$ and classifier head's output or logits by $\mathbf{z}_{o}$, the overall loss becomes:

In addition to $\mathcal{L}_{\mathrm{reg}}$ mitigating NC, we consider alternatives to batch normalization (BN).
In the context of learning transferable representations in the encoder, batch dependency, especially using BN, is sub-optimal as OOD data statistically differs from ID data.
Therefore, for all layers in the encoder, we replace batch normalization with a batch-independent alternative, particularly, a combination of group normalization (GN)~\cite{wu2018group} and weight standardization (WS)~\cite{qiao2019micro} to enhance OOD generalization. 
%Group normalization combined with weight standardization rivals batch normalization in various settings~\cite{wu2018group, richemond2020byol}.
%Furthermore, group normalization helps enhance informative forward propagation and rank of the features~\cite{lubana2021beyond}. 


\begin{comment}

\section{Controlling Neural Collapse}
\label{sec:method}

The key idea is to shape the embedding space such that the encoder prevents NC and forms transferable representations. In contrast, the projector induces NC and forms simplex-ETF representations. Consequently, encoder embeddings will exhibit higher within-class covariance \( \Sigma_W \). On the contrary, \( \Sigma_W \) will shrink in the projector embeddings, inhibiting transferability. An illustrative high-level overview is shown in Fig.~\ref{fig:tunnel_effect_implication}.


%\subsection{Preventing Neural Collapse}

%We want to prevent NC to enhance OOD transfer at encoder layer. For this, we incorporate two criterion: a) entropy regularization and b) mutual independence. Collapsed layers exhibit feature variability suppression where between-class-covariance becomes significantly dominant and within-class-covariance diminishes. Maximizing entropy can mitigate this issue by encoding more \emph{spread-out} representations. %using all dimensions.

\subsection{Entropy Regularization}
Maximizing entropy in the encoder can mitigate NC by encoding more \emph{spread-out} representations.
%To quantify entropy, we consider two density estimation approaches: 1) Nearest neighbor and 2) Von Mises Fisher (vMF). Empirically, we found the nearest neighbor more effective than vMF.
%% Discuss and define reg loss
%Our objective is to maximize entropy in encoder embedding space. \textbf{\emph{How do we learn diverse and invariant features in the encoder?}} One avenue is \emph{entropy maximization} as a learning objective.
The representation generated by the encoder network can be treated as a random variable \( \mathbf{Z} \), where \( \mathbf{Z} = f_\theta(\mathbf{X}) \). Given that \( \mathbf{Z} \) is continuous, traditional discrete entropy measures are inapplicable. Instead, we could use \emph{differential entropy} to quantify the uncertainty in \( \mathbf{Z} \) with density \( p(\mathbf{z}) \). 
%Formally, the differential entropy of \( \mathbf{Z} \) is defined by
%\[
%H(\mathbf{Z}) = -\int p(\mathbf{z}) \log p(\mathbf{z}) \,d\mathbf{z},
%\]
%where \( p \) is the probability density function of \( \mathbf{Z} \). 
%Since directly accessing \( p(\mathbf{z}) \) would require knowledge of the underlying density \( P(\mathbf{X}) \), which is typically unavailable, 

Since we do not have direct access to \( p(\mathbf{z}) \), we need to estimate \( H(\mathbf{Z}) \) using a sample-based approach as a proxy. In particular, 
prior work~\cite{kozachenko1987sample, beirlant1997nonparametric} show that the differential entropy can be estimated by nearest neighbor distances.

Given a batch of \( N \) random representations \( \{\mathbf{z}_n\}_{n=1}^N \), the nearest neighbor entropy estimate is given by
\[
\hat{H}(\mathbf{Z}) \approx \frac{1}{N} \sum_{n=1}^N \log \left( N \min_{i \in [N], i \neq n} \| \mathbf{z}_n - \mathbf{z}_i \|_2 \right) + \ln 2 + \text{EC},
\]
where EC denotes the Euler constant. For our purposes, we can simplify the entropy maximization objective by removing affine terms, resulting in the following loss function:
\[
L_R(\theta) = -\frac{1}{N} \sum_{n=1}^N \log \left( \min_{i \in [N], i \neq n} \| \bar{\mathbf{z}}_n - \bar{\mathbf{z}}_i \|_2 \right).
\]
Intuitively, \( L_R \) maximizes the distance between the nearest pairs in the batch, encouraging an even spread of representations across the embedding space.
The pairwise distances can be sensitive to outliers with large magnitudes. Therefore, in our method, the loss operates on the hyperspherical embedding space with the unit norm, i.e., \( \bar{\mathbf{z}} = \mathbf{z} / ||\mathbf{z}||_2 \).


\subsection{Replacing Batch Normalization}
To further promote OOD transfer, we consider alternatives to batch normalization.
In the context of learning transferable representations in the encoder, batch dependency especially using batch normalization seems sub-optimal as OOD data statistically differs from ID data.
Therefore, for all layers in the encoder, we replace batch normalization with the batch-independent alternative, particularly, a combination of group normalization~\cite{wu2018group} and weight standardization~\cite{qiao2019micro} to improve learning transferable features. 
Group normalization combined with weight standardization rivals batch normalization in various settings~\cite{wu2018group, richemond2020byol}.
Furthermore, group normalization helps enhance informative forward propagation and rank of the features~\cite{lubana2021beyond}. 

%% How BN impacts NC was unexplored in prior work.

\end{comment}


%\subsubsection{Mutual Independence}
%Collapsed layers also exhibit lower rank in the weights and representations. Maximizing rank can remedy this by reducing dependence between dimensions (mutual independence).

\subsection{Simplex ETF Projector for Inducing Collapse}
\label{sec:etf_projector_nc}

%% Discuss about ETF Simples and MSE
%Our objective is to increase NC in the projector to improve OOD detection performance. Note that we do not propose a new OOD method but focus on the representation learning aspect so that learned representations in the projector result in a good separation between ID and OOD instances.
%Here we ask: \emph{How do we increase NC in the projector?}


When a DNN enters into NC phase, the class-means converge to a simplex ETF (equinorm and
maximal equiangularity) in collapsed layers (NC2 criterion). This implies that fixing the collapsed layers to be ETFs does not impair ID performance~\cite{rangamani2023feature, zhu2021geometric}.
In this work, we induce NC in the projector to improve OOD detection performance. 
% The projector must form collapsed features and ETF structure.
% Fixing the projector to be simplex ETF from the onset of training paves the way -- acting as an architectural inductive bias.
We do it by fixing the projector to be simplex ETF, acting as an architectural inductive bias.
% ETF configuration will maximize and expedite NC process (i.e., early access to NC characteristics) without sacrificing ID performance.
% This also helps to reduce the number of trainable parameters and training cost given that reaching NC phase (zero training error) typically requires longer training.

Our projector comprises two MLP layers sandwiched between encoder and classifier head. 
We set the projector weights to simplex ETFs and keep them frozen during training.
In particular, each MLP layer is set to be a rank $\mathcal{D}-1$ simplex ETF, where $\mathcal{D}$ denotes width or output feature dimension. 
The rank $\mathcal{D}$ canonical simplex ETF is:
\begin{equation*}
    \sqrt{\frac{\mathcal{D}}{\mathcal{D}-1}} (\mathbf{I}_\mathcal{D} - \frac{1}{\mathcal{D}} \mathbf{1}_\mathcal{D} \mathbf{1}_\mathcal{D}^T)
\end{equation*}
Details on the projector are given in Appendix~\ref{sec:arch_details}.
%The output dimension of the projector matches the feature dimension $d$ of the last encoder layer and the hidden dimension is set to a higher dim like $m \times d$ where $m>1$. In our case we, $m=4$. For example, in VGG17 ($d=512$), the projector has hidden dimension of 2048 and output dimension of 512.
We further apply $L_2$ normalization to the output of the projector since it constraints features to achieve equinormality 
% of simplex ETF 
and helps induce early neural collapse~\cite{haas2023linking}.



\begin{comment}

\noindent
\textbf{\textit{Accelerating NC by L2 normalization.}}
Prior work showed that NC can be accelerated and induced by different mechanisms. 
It is shown that simply normalizing the penultimate embedding vectors by $L_2$ norm can accelerate NC phenomenon~\cite{haas2023linking}. Typically, to achieve terminal phase training (zero training error) and NC, a DNN needs to be trained longer but L2 can accelerate this process.


While various losses (CE, MSE etc.) exhibit NC and result in identical test accuracy~\cite{zhou2022all}, we consider MSE in this work.
Minimizing following MSE loss we can obtain NC where learned weights converge to analytical formulation~\cite{han2022neural}.
\[
L_{MSE} (u, t, \kappa, M) = \frac{1}{K} \sum_{k=1}^K (\kappa t_k (u_k - M)^2 + (1 - t_k) u_k^2),
\]
where $\kappa$ and $M$ are hyperparameters. $k$ weights the loss for the ground truth class relative to incorrect classes, whereas M controls the magnitude of the ground truth class target. Setting $\kappa=15$ and $M=30$ works effectively in practice~\cite{kornblith2021better, zhou2022all}. Here, $u$ denotes the output logit vector and $t$ indicates the one-hot target.

In~\cite{han2022neural}, authors decomposed MSE loss and showed that learned weights result in analytical least-square (LS) weights which only depend on features. The analytical form is:
\[
W_{LS} = \frac{1}{C} M^\top (\Sigma_T + \mu_G \mu_G^\top + \lambda I)^{-1},
\]
where \( I \) is the identity matrix. 
For the i-th example in the c-th class, the penultimate features are denoted by \( z_{i,c} \in \mathbb{R}^d \). 
%For all C classes, the features can be stacked together to form the feature matrix, \( Z \in \mathbb{R}^{d \times CN} \).
Given, feature global mean \( \mu_G = \text{Ave}_{i,c} \, z_{i,c} \), 
feature class means \( \mu_c = \text{Ave}_{i} \, z_{i,c} \) for \( c = 1, \ldots, C \),
and feature within-class covariance \( \Sigma_W = \text{Ave}_{i,c} \, (z_{i,c} - \mu_c)(z_{i,c} - \mu_c)^\top \), 
the total covariance matrix and class-means matrix are defined as follows:
\[
\Sigma_T = \text{Ave}_{i,c} (z_{i,c} - \mu_G)(z_{i,c} - \mu_G)^\top \in \mathbb{R}^{d \times d},
\]
\[
M = [\mu_1, \ldots, \mu_C] \in \mathbb{R}^{d \times C}.
\]
It is worth noting that \( W_{LS} \) depends solely on features \( z_{i,c} \). We found analytical \( W_{LS} \) matches the real \( W \) (learned) in the final classification layer 
%(see Fig.~\ref{fig:least_square_vs_learned}).



\textbf{\textit{Building collapsed features using fixed ETF projector.}}
Projector is widely used in self-supervised learning to prevent model collapse~\cite{zhu2023variance}. Recently, supervised learning has been shown to benefit from using a projector, particularly in retaining intra-class variation and mitigating feature redundancy~\cite{wang2022revisiting, sariyildiz2023no}. The projector comprises MLP layer(s) sandwiched between the encoder (backbone) and classifier head. It has also been demonstrated that top layers of DNN can be fixed as simplex-ETF without hurting the performance while accessing NC criteria. In this work, our goal is to build collapsed features in the projector which is different from the traditional goal of the projector. Instead of enhancing feature variability, we want to enhance feature collapse (intra-class variation becomes close to zero and ETF structure is formed). For this, we keep the entire projector frozen during training and set projector weights to simplex-ETFs, following earlier work~\cite{rangamani2023feature}.
In particular, each MLP layer is set to be a rank $H-1$ simplex ETFs, where $H$ denotes width or output feature dimension. 
The rank $K$ canonical simplex ETF is:
\[
\sqrt{\frac{K}{K-1}} (\mathbf{I}_K - \frac{1}{K} \mathbf{1}_K \mathbf{1}_K^T).
\]

\end{comment}

While recent works have found that incorporating a projector improves transfer in supervised learning~\cite{mialon2209variance}, the objective of our projector is to impede transfer. The difference is that a projector is typically trained along with the backbone, whereas in our method the projector is configured as Simplex ETF and kept frozen during training.

% \textbf{Differences from prior works on projector.}
% Using an additional projector network is a common practice in self-supervised learning (SSL) paradigm.
% % The explanation on the role of the projector network in SSL has been blurry.
% It has been argued that adding a projector is useful for satisfying the invariance criterion in joint embedding SSL methods~\cite{mialon2209variance}. Recently, incorporating projector has been found effective in improving transfer in supervised learning~\cite{sariyildiz2023no, wang2022revisiting}.
% In contrast, the objective of our projector is to impede transfer, opposite of previous work. Also, typically the projector is trained along with the backbone whereas in our method, the projector is configured as Simplex ETF and kept frozen during training for obtaining collapsed representations in the projector.


%\subsection{Additional Mechanisms to Influence Neural Collapse}

%\textbf{Label Smoothing}
%It has been found that using label smoothing with cross-entropy loss impairs transferability and intensifies NC properties when compared with the standard cross-entropy loss~\cite{zhou2022all, kornblith2021better}. We use label smoothing with cross-entropy loss to maximize NC properties in the projector and classifier layer.


% \subsection{Replacing Batch Normalization to Enhance OOD Transfer}

% To further promote OOD transfer in encoder, we consider alternatives to batch normalization.
% In the context of learning transferable representations in the encoder, batch dependency especially using batch normalization seems sub-optimal as OOD data statistically differs from ID data.
% Therefore, for all layers in the encoder, we replace batch normalization with the batch-independent alternative, particularly, a combination of group normalization~\cite{wu2018group} and weight standardization~\cite{qiao2019micro} to improve learning transferable features. 
% Group normalization combined with weight standardization rivals batch normalization in various settings~\cite{wu2018group, richemond2020byol}.
% Furthermore, group normalization helps enhance informative forward propagation and rank of the features~\cite{lubana2021beyond}. 

%% How BN impacts NC was unexplored in prior work. We found that GN reduces NC in encoder.


%\subsection{OMNI: Putting Everything Together}

%We incorporate the above-mentioned approaches into our proposed method, OMNI to balance feature diversity and NC. OMNI jointly optimizes the MSE and regularization losses while imposing a simplex-ETF structure as an architectural inductive bias in the projector. By doing this, OMNI can build invariant features in encoder and NC features in the projector and last layer. For OOD detection the entire network (\texttt{encoder + projector + classifier head}) is used whereas only encoder is utilized for OOD generalization or transfer learning.