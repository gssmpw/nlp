\section{Background}
\label{sec:background}
\subsection{OOD Detection}
\vspace{-0.5em}

OOD detection methods aim to separate ID and OOD samples by leveraging the differences between their feature representations. Most existing OOD detection methods are \emph{post-hoc}, meaning they apply a scoring function to a model trained exclusively on ID data, without modifying the training process~\cite{salehi2022a}. These methods inherently rely on the properties of the learned feature space to distinguish ID from OOD samples.

Post-hoc detection techniques can be broadly categorized based on the source of their confidence estimates. Density-based methods model the ID distribution probabilistically and classify low-density test points as OOD~\cite{lee2018simple, zisselman2020deep, choi2018waic, jiang2023diverse}. More commonly, confidence-based approaches estimate OOD likelihood using model outputs~\cite{hendrycks2016baseline, liang2017enhancing, liu2020energy}, feature statistics~\cite{sun2021react, zhu2022boosting, sun2022out}, or gradient-based information~\cite{huang2021importance, wu2024low, lee2023probing, igoe2022useful}.

Since post-hoc methods depend on the representations learned during ID training, their effectiveness is fundamentally constrained by the quality of those features~\cite{roady2020open}. Highly compact, well-separated ID representations generally improve OOD detection by reducing feature overlap with OOD samples. For example, \citet{haas2023linking} demonstrated that $L_2$ normalization of penultimate-layer features induces NC, enhancing ID-OOD separability. Similarly, \citet{wu2024pursuing} introduced a regularization loss that enforces orthogonality between ID and OOD representations, leveraging NC-like properties to improve detection.

Another representation learning approach is to learn representations explicitly tailored for OOD detection by incorporating OOD samples during training~\cite{wu2024pursuing, bai2023feed, katz2022training, ming2022poem}. These methods encourage models to assign lower confidence~\cite{hendrycks2018deep} or higher energy~\cite{liu2020energy} to OOD inputs. However, this approach presents significant challenges, as the space of possible OOD data is essentially infinite, making it impractical to represent all potential OOD variations. Moreover, strong OOD detection performance often comes at the cost of degraded OOD generalization~\cite{zhang2024best}, as representations optimized for separability may lack the diversity needed for adaptation to novel distributions.



\subsection{Transfer Learning and OOD Generalization}
\vspace{-0.5em}

Transfer learning and OOD generalization methods focus on learning features that remain effective across distribution shifts. Robust transfer is particularly important in open-world learning scenarios, where models must not only adapt to new distributions but also improve sample efficiency over time, a key requirement for continual learning. To facilitate generalization, techniques such as feature alignment~\cite{li2018domain, ahuja2021invariance, zhao2020domain, ming2024hypo}, ensemble/meta-learning~\cite{balaji2018metareg, li2018metalearning, li2019episodic, bui2021exploiting}, robust optimization~\cite{rame2022fishr, cha2021swad, krueger2021out, shi2021gradient}, data augmentation~\cite{nam2021reducing, nuriel2021permuted, zhou2020learning}, and feature disentanglement~\cite{zhang2022towards} have been proposed.

Key properties of learned features significantly impact generalization to unseen distributions. Studies examining factors that affect OOD generalization emphasize that feature diversity is essential for robustness~\cite{masarczyk2023tunnel, kornblith2021better, fang2024does, ramanujan2024connection, kolesnikov2020big, vishniakov2024convnet}. Notably, recent work~\cite{kothapalli2023neural, masarczyk2023tunnel, harun2024what} suggests that progressive feature compression in deeper layers, linked to NC emergence, can hinder generalization by reducing representation expressivity. %These findings indicate that controlling the degree of NC may be crucial for balancing generalization and transferability.

\subsection{Neural Collapse}
\label{sec:nc_background}
\vspace{-0.5em}

As noted earlier, NC arises when class features become tightly clustered, often converging toward a Simplex ETF~\cite{papyan2020prevalence, kothapalli2023neural, zhu2021geometric, han2022neural}. Initially, NC was studied primarily in the final hidden layer, but later work demonstrated that NC manifests to varying degrees in earlier layers as well~\cite{rangamani2023feature, harun2024what}. In image classification experiments, \citet{harun2024what} showed that the degree of intermediate NC is heavily influenced by the properties of the training data, including the number of ID classes, image resolution, and the use of augmentations.

NC can be characterized by four main properties:
\begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
    \item \textbf{Feature Collapse} ($\mathcal{NC}1$): Features within each class concentrate around a single mean, exhibiting minimal intra-class variability.
    \item \textbf{Simplex ETF Structure} ($\mathcal{NC}2$): When centered at the global mean, class means lie on a hypersphere with maximal pairwise distances, forming a Simplex ETF.
    \item \textbf{Self-Duality} ($\mathcal{NC}3$): The last-layer classifiers align tightly with their corresponding class means, creating a nearly self-dual configuration.
    \item \textbf{Nearest Class Mean Decision} ($\mathcal{NC}4$): Classification behaves like a nearest-centroid scheme, assigning classes based on proximity to class means.
\end{enumerate}

While NC's structured representations can aid OOD detection by ensuring strong class separability~\cite{haas2023linking, wu2024pursuing}, the same compression may limit the feature diversity needed for generalization. One proposed explanation is the \emph{Tunnel Effect Hypothesis}~\cite{masarczyk2023tunnel}, which suggests that as features become increasingly compressed in deeper layers, generalization to unseen distributions is impeded. 

%Although prior works have studied NC independently in the contexts of OOD detection~\cite{haas2023linking, wu2024pursuing} and OOD generalization~\cite{kothapalli2023neural, masarczyk2023tunnel, harun2024what}, their impact on both tasks \emph{simultaneously} remains an open question. Our work is the first to establish a concrete connection between NC, OOD detection, and OOD generalization, offering new theoretical insights and empirical validation.

