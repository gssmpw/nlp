\begin{abstract}
Out-of-distribution (OOD) detection and OOD generalization are widely studied in Deep Neural Networks (DNNs), yet their relationship remains poorly understood. We empirically show that the degree of Neural Collapse (NC) in a network layer is inversely related with these objectives: stronger NC improves OOD detection but degrades generalization, while weaker NC enhances generalization at the cost of detection. This trade-off suggests that a single feature space cannot simultaneously achieve both tasks. To address this, we develop a theoretical framework linking NC to OOD detection and generalization. We show that entropy regularization mitigates NC to improve generalization, while a fixed Simplex Equiangular Tight Frame (ETF) projector enforces NC for better detection. Based on these insights, we propose a method to control NC at different DNN layers. In experiments, our method excels at both tasks across OOD datasets and DNN architectures. 

\begin{comment}   

Out-of-distribution (OOD) detection and OOD generalization are critical for deploying machine learning models in real-world scenarios. While substantial progress has been made in addressing these problems independently, few works have attempted to tackle them jointly. However, existing methods often rely on auxiliary OOD training data and primarily focus on covariate-shifted OOD data that share labels with in-distribution (ID) data. In contrast, we tackle the more realistic and challenging task of jointly detecting and generalizing to semantic OOD data with disjoint labels from the ID data, without auxiliary OOD training data.
Achieving both objectives simultaneously is inherently difficult due to a fundamental conflict — OOD generalization requires enhanced transferability, while OOD detection necessitates the inhibition of transfer.
To address this, we leverage insights from neural collapse (NC) — a phenomenon in deep networks where top-layer representations suppress feature variability and adopt a Simplex Equiangular Tight Frame (ETF) structure, impairing transferability. By controlling NC, we unify OOD detection and generalization: preventing NC enhances OOD transfer while inducing NC improves OOD detection.
Our proposed method excels at both tasks across various OOD datasets and architectures. 

\end{comment}


\end{abstract}