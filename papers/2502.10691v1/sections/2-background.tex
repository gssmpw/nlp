\section{Background}
\label{sec:background}



\subsection{OOD Detection}

For models trained exclusively on ID data, post-hoc detection methods are designed to apply scoring functions that differentiate between ID and OOD samples~\cite{salehi2022a}. A category of these methods, known as density-based approaches~\cite{lee2018simple, zisselman2020deep, choi2018waic, jiang2023diverse}, involves explicitly modeling ID data with probabilistic models, identifying test samples that fall within low-density regions as OOD. More commonly, methods derive confidence scores from model outputs~\cite{hendrycks2016baseline, liang2017enhancing, liu2020energy}, feature representations~\cite{sun2021react, zhu2022boosting, sun2022out}, or gradient information~\cite{huang2021importance, wu2024low, lee2023probing, igoe2022useful}. 
%For instance, the widely used maximum softmax probability method~\cite{hendrycks2016baseline} takes the probability assigned to the predicted class as a confidence measure, marking samples with low confidence scores as OOD.


Another set of methods incorporates OOD samples during training to improve robustness~\cite{wu2024pursuing, bai2023feed, katz2022training, ming2022poem}. These methods regularize models to assign lower confidence~\cite{hendrycks2018deep} or higher energy~\cite{liu2020energy} to OOD samples.
However, they rely on auxiliary OOD data, which is impractical due to the diversity of real-world distributions. %and computational constraints.
Existing SOTA methods improve OOD detection at the cost of degrading OOD generalization~\cite{zhang2024best}.
Recent works~\cite{zhang2024best, wang2024bridging, bai2023feed} attempt to mitigate these trade-offs, however, they predominantly focus on covariate-shifted OOD data, ignoring semantic OOD generalization.


Recent efforts explore NC as a structural property that can improve OOD detection. \citet{haas2023linking} finds that $L_2$ normalization of the features of the penultimate layer induces NC, improving the separation of OOD samples. \citet{wu2024pursuing} introduces a regularization loss that enforces orthogonality between ID and OOD representations, leveraging NC properties.

%For more details, we refer readers to these survey papers on OOD detection~\cite{miyai2024generalized, salehi2022a, yang2024generalized}.


\subsection{Transfer Learning}

Transfer learning~\cite{liu2021towards} aims to learn invariant representations across shifts in data distribution, enabling effective adaptation to unseen scenarios. %Among the most challenging shifts are semantic shifts (e.g., new object categories), which are the focus of this study. 
%Extensive research has explored strategies to enhance OOD generalization~\cite{liu2021towards}.
OOD generalization methods aim to extract stable semantic features across diverse domains. These approaches can be categorized into feature alignment techniques~\cite{li2018domain, ahuja2021invariance, zhao2020domain, ming2024hypo}, ensemble and meta-learning~\cite{balaji2018metareg, li2018metalearning, li2019episodic, bui2021exploiting}, robust optimization~\cite{rame2022fishr, cha2021swad, krueger2021out, shi2021gradient}, data augmentation~\cite{nam2021reducing, nuriel2021permuted, zhou2020learning}, and feature disentanglement~\cite{zhang2022towards}.

Recent work has also investigated various factors influencing OOD generalization~\cite{masarczyk2023tunnel, kornblith2021better, fang2024does, ramanujan2024connection, kolesnikov2020big, vishniakov2024convnet}. 
Notably, the effects of neural collapse have been studied in the context of transfer learning~\cite{kothapallineural, masarczyk2023tunnel, harun2024what}. NC, particularly in deeper layers, has been shown to degrade OOD generalization, making it a critical challenge for robust representation learning.


\subsection{Neural Collapse}

Neural Collapse~\cite{papyan2020prevalence, kothapalli2023neural, zhu2021geometric, han2022neural} is a phenomenon that occurs in DNNs trained for classification with cross-entropy loss where the last layer ``collapses'' into a relatively simple representation known as a simplex equiangular tight frame (ETF) as training progresses. Different loss functions lead to NC~\cite{han2022neural, zhou2022all}.
The NC phenomenon has recently gained substantial interest, as it offers a holistic view of feature learning in DNN. 
It has been found that NC not only forms in the last layer but also occurs in intermediate layers, referred to as intermediate neural collapse~\cite{rangamani2023feature}.
%It has been found that NC not only forms in the last layer but also occurs in intermediate layers, referred to as intermediate neural collapse~\cite{rangamani2023feature}.
% NC is characterized by four criteria described in the introduction.
%: feature collapse within classes, the emergence of ETF-structured class means, alignment between class means and classifier weights, and equivalence to a nearest-centroid classifier.
%It is typically studied as a function of epochs, where the last layer is measured to determine if it has these four criteria. 
%NC has been shown to even be present in CNNs trained on ImageNet with their final layer collapsing into a simplex ETF~\cite{papyan2020prevalence}. 
% not only forms in the last layer but also 
%A closely related phenomenon is ``the tunnel effect''~\cite{masarczyk2023tunnel, harun2024what} exhibiting compression in the top layers and hindering OOD generalization.
Although the implications of NC on OOD detection~\cite{haas2023linking, wu2024pursuing} and OOD generalization~\cite{kothapalli2023neural, masarczyk2023tunnel, harun2024what} have been discussed in earlier work, 
our work is the first to establish a concrete connection between NC and OOD detection/ generalization. %, providing new insights.



%By leveraging the NC insights, our approach provides a novel pathway to improve OOD robustness.