\section{Discussion}
\label{sec:discussion}


Our study highlights the impact of neural collapse on OOD detection and OOD generalization. Several promising directions remain for future research.  
Extending our approach to open-world continual learning~\cite{kim2025open, dong2024mr} presents an exciting challenge. 
While we focused on architectural and regularization-based techniques to control NC, another avenue is optimization-driven strategies. For instance, ~\citet{markou2024guiding} studies optimising towards the nearest simplex ETF to accelerate NC. 
Guiding NC to enhance task-specific representations or disentangle conflicting tasks could improve robustness and generalization.  
Moreover, beyond standard loss functions, alternative formulations could be explored to regulate NC. 

Our study utilized nearest neighbor density estimation for entropy regularization. Exploring parametric and adaptive approaches could offer more robust regularization techniques for improving OOD generalization.  
We demonstrated that controlling NC improves OOD detection and generalization, but a deeper theoretical understanding of this relationship is needed. Future work could establish theoretical frameworks that unify OOD detection and generalization from an NC perspective, offering a more comprehensive view of representation learning under distribution shifts.  



%% other tasks, domains, modalities

%% Open-world learning, continual learning

%In the future, it would be interesting to study problems where OOD detection and OOD generalization play crucial roles in learning from shifting distributions such as open-world continual learning~\cite{kim2025open, dong2024mr}. This problem has a lot of practical implications for handling the stream of data with shifted distributions in domains e.g., autonomous driving, cloud computing, smartphones, healthcare, and so on.

%% Guiding NC
%This work focused on architectural and regularization approaches to control NC.
%Another interesting future direction is to study guiding NC from the optimization perspective. For instance, ~\citet{markou2024guiding} has explored optimization approaches to accelerate NC. One may explore how to guide NC to build task-specific representations or disentangle representations to address conflicting tasks.

%% Regularization approach
%Besides regular loss functions, alternative loss functions have been explored to achieve and exploit NC~\cite{liu2023generalizing}. Achieving more favorable representation properties using alternative loss can be an avenue for future research. 

%% Density Estimation
%We explored nearest neighbor density estimation for entropy regularization loss. Other parametric and adaptive kernel density estimation approaches can be investigated.


%% Theory
%How NC impacts OOD generalization is an important research question~\cite{rangamani2023feature}. We showed mitigating NC improves OOD generalization. While the implications of NC on OOD detection have been studied, the interplay between OOD generalization and NC remains under-studied. More work could be done to delve into this topic and to jointly study OOD detection and generalization. Moreover, future research could build theoretical frameworks to combine OOD detection and OOD generalization from NC perspectives.
