\section{Experimental Setup}
\label{sec:exp_setting}

%\input{tables/main_results}


%\subsection{Experimental Design}

%Our objective is not to become state-of-the-art in OOD generalization and detection, but rather to showcase that our method can combinedly achieve two conflicting goals: a) OOD detection and b) OOD generalization in a unified framework. 
%%Our method can be used as a \emph{plug-and-play} to strengthen existing OOD generalization or detection methods.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{Datasets.}
For ID dataset, we use ImageNet-100~\cite{tian2020contrastive}- a subset (100 classes) of ImageNet-1K~\cite{russakovsky2015imagenet}. 
To assess OOD transferability and OOD detection, we study 8 commonly used OOD datasets: NINCO~\cite{bitterwolf2023ninco}, ImageNet-R~\citep{hendrycks2021many}, CIFAR-100~\cite{krizhevsky2014cifar}, Oxford 102 Flowers~\citep{nilsback2008automated}, CUB-200~\citep{wah2011caltech}, Aircrafts~\citep{maji2013fine}, Oxford-IIIT Pets~\citep{parkhi2012cats}, and STL-10~\citep{coates2011analysis}. %There is no semantic overlap between ID and OOD datasets.
Dataset details are given in Appendix~\ref{sec:datasets}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\noindent
\textbf{DNN Architectures.}
We train and evaluate a representative set of DNN architectures including VGG~\cite{Simonyan15}, ResNet~\cite{he2016deep}, and ViT~\cite{dosovitskiy2020image}. 
In total, we experiment with five backbones: VGG17, ResNet18, ResNet34, ViT-Tiny, and ViT-Small.
% For the encoder, we modified VGG19 by removing two FC layers and attaching a ResNet-style average-pool layer before the final layer.
Our projector is composed of two MLP layers for all DNN architectures. %with an input dimension of 512, a hidden dimension of 2048, and an output dimension of 512. The projector contains ReLU nonlinearity between the two layers. There is an L2 normalization layer after the projector. %%and the unit-length 512-dimensional embedding is fed to the classifier head. 
Details are given in Appendix~\ref{sec:arch_details}.

%We keep the entire projector frozen during training and construct projector weights with simplex-ETFs, following earlier work~\cite{rangamani2023feature}.
%In particular, each MLP layer is set to be a rank $H-1$ simplex ETFs, where $H$ denotes width or output feature dimension. 
%The rank $K$ canonical simplex ETF is:
%\[
%\sqrt{\frac{K}{K-1}} (\mathbf{I}_K - \frac{1}{K} \mathbf{1}_K %\mathbf{1}_K^T).
%\]
%More details are given in Appendix.


%\noindent
%\textbf{Evaluation Metrics.}
%We use top-1 error (\%) for OOD generalization performance and FPR95 (\%) for OOD detection. In both cases, a lower value indicates better performance. We use effective rank~\cite{garrido2023rankme} to measure feature diversity (higher rank indicates higher diversity). To measure NC, we use 3 metrics corresponding to 3 NC criteria. A lower NC value indicates a higher NC and vice-versa. We describe the NC metrics below.

\noindent
\textbf{NC Metrics ($\mathcal{NC}1$–$\mathcal{NC}4$).}
We use four metrics, $\mathcal{NC}1$, $\mathcal{NC}2$, $\mathcal{NC}3$, and $\mathcal{NC}4$, as described in~\cite{zhu2021geometric, zhou2022all}, to evaluate the NC properties of the DNN features and classifier. These metrics correspond to four NC properties outlined in Sec.~\ref{sec:nc_background}. 
Note that $\mathcal{NC}1$ is the most dominant indicator of neural collapse.
We describe each NC metric in detail in the Appendix~\ref{sec:nc_metrics}. 

\noindent
\textbf{Training Details.}
In our main experiments, we train different DNN architectures e.g., VGG17, ResNet18, and ViT-T on ImageNet-100 for 100 epochs.
% using CE loss and entropy regularization loss.
%Our DNN models include an additional projector consisting of 2 MLP layers.
The Entropy regularization loss $\mathcal{L}_{\mathrm{reg}}$ is modulated with $\alpha=0.05$. %The penultimate layer embeddings are $L_2$ normalized.
% The CE loss $\mathcal{L}_{\mathrm{cls}}$  applies a label smoothing of $0.1$.
We use AdamW~\cite{loshchilov2017decoupled}  optimizer and cosine learning rate scheduler with a linear warmup of 5 epochs. For a batch size of 512, we set the learning rate to $6\times 10^{-3}$ for VGG17, $0.01$ for ResNet18, and $8\times 10^{-4}$ for ViT-T. For all models, we set weight decay to $0.05$ and label smoothing of $0.1$. 
In all our experiments, we use $224\times 224$ images. And, we use random resized crop and random horizontal flip augmentations.
Linear probes are attached to the encoder and projector layers of a pre-trained model and trained on extracted embeddings of OOD data using AdamW optimizer and CE loss for 30 epochs. Additional implementation details are given in Appendix~\ref{sec:implementation}.


\noindent
\textbf{Baselines.}
% Since ours is the first study to unify OOD detection and OOD generalization, we cannot compare directly with other methods.
% Previous works focused on either OOD generalization or OOD detection. Some recent work focusing on both OOD detection and robustness (covariate OOD generalization), defined the problem differently from us and required additional OOD data during training. Our problem setup and learning goals are fundamentally distinct and adapting other methods to our problem setup will require major modifications, hence we cannot compare directly with them. In this work, our primary objective lies in learning representations by controlling NC in a unified framework for OOD detection and OOD generalization. We compare the proposed method with baselines that 
% use standard models (e.g., VGG17, ResNet18, and ViT-T) as baselines that 
% do not use any of our mechanisms e.g., entropy regularization or ETF projector. %This will show the limitations of current supervised representation learning paradigm.
Recent work defines the problem differently where they focus on OOD detection and covariate OOD generalization (same labels but different input distribution). Our problem setup focuses on OOD detection and semantic OOD generalization (different labels and different input distribution). Adapting other methods to our problem setup will require major modifications, hence we cannot compare directly with them. 
% In this work, our primary objective lies in learning representations by controlling NC in a unified framework for OOD detection and OOD generalization. 
We compare the proposed method with baselines that do not use any of our mechanisms e.g., entropy regularization or fixed simplex ETF projector.




\section{Experimental Results}
\label{sec:exp_results}


\subsection{Impact of Controlling NC}

\input{tables/enc_vs_proj}

We verify whether controlling NC improves OOD detection and OOD generalization by examining NC properties in the encoder and projector. Table~\ref{tab:main_results_summary} presents the summary result across 8 OOD datasets. The projector, which has lower NC values (stronger NC), yields superior OOD detection ($7.73\% - 22.52\%$ margin) and reduced ID error than the encoder. Meanwhile, the encoder’s higher NC values (weaker NC) enable better OOD generalization ($10.90\% - 24.51\%$ margin) than the projector. In the Appendix~\ref{sec:comprehensive_results}, we include comprehensive results for each of the 8 OOD datasets.

% We want to verify if controlling NC translates into good OOD detection and OOD generalization performance. For this, we examine the NC properties in the encoder and projector. Table~\ref{tab:main_results_summary} presents the summary results across 8 OOD datasets for each type of network. The projector achieves lower NC values than the encoder, demonstrating abilities to strengthen NC. Consequently, the projector achieves better OOD detection performance than the encoder ($7.73\% - 22.52\%$ absolute margin). The projector also reduces ID error compared to the encoder. In contrast, the encoder obtains higher NC values, showing efficacy in reducing NC. As a result, the encoder outperforms the projector in the OOD generalization by $10.90\% - 24.51\%$ absolute margin. In the Appendix, we include comprehensive results for each of the 8 OOD datasets.
% (see Tables~\ref{tab:main_results}, ~\ref{tab:resnet_results}, and~\ref{tab:vit_results}). 

We also visualize the encoder and projector embeddings in Fig~\ref{fig:umap_vis} and~\ref{fig:umap_vis_resnet} for deeper insights. Unlike encoder embeddings, projector embeddings cluster tightly around class means (reflecting stronger NC1).
Fig.~\ref{fig:umap_id_ood}, in the Appendix section, shows the projector exhibiting greater ID-OOD separation than the encoder. Finally, Fig.~\ref{fig:eng_id_ood} and~\ref{fig:more_eng_id_ood} in the Appendix show that the energy score distribution also reveals that the projector more effectively separates ID from OOD across multiple datasets than the encoder. These observations clarify why the projector excels at OOD detection by exploiting more collapsed features.

% We also visualize the embeddings in the encoder and projector to gain more insights. As visible in Fig~\ref{fig:umap_vis}
% % and Fig.~\ref{fig:umap_vis_resnet}
% , unlike encoder embeddings, projector embeddings form tight clusters around class-means, exhibiting a greater feature variability suppression (NC1). The projector also exhibits greater separation between ID and OOD embeddings than the encoder, as illustrated in Fig.~\ref{fig:umap_id_ood}. Moreover, energy score distribution reveals that the projector achieves much higher separation between ID and OOD data than the encoder for various OOD datasets, as exhibited in Fig.~\ref{fig:eng_id_ood} and~\ref{fig:more_eng_id_ood}. These qualitative results help explain why the projector is adept at OOD detection which benefits from collapsed features.

Finally, we analyze different layers of VGG17 and ResNet18 models and find that increasing NC strongly correlates with lower OOD detection error and reducing NC strongly correlates with lower OOD generalization error, as shown in Fig.~\ref{fig:vis_abstract} and~\ref{fig:nc_resnet}.
Our experimental results validate that \emph{controlling NC effectively enhances OOD detection and OOD generalization abilities.} 

% \begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
%     \item As shown in Table~\ref{tab:main_results_summary}, across all architectures, the projector achieves lower NC values than the encoder, demonstrating abilities to strengthen NC. Consequently, the projector achieves higher OOD detection performance than the encoder ($7.73\% - 22.52\%$ absolute margin). The projector also reduces ID error compared to the encoder.

%     \item In contrast, the encoder obtains higher NC values, showing efficacy in reducing NC. As a result, the encoder outperforms the projector in the OOD generalization by $10.90\% - 24.51\%$ absolute margin. 

%     \item Additionally, we analyze different layers of VGG17 and ResNet18 models and find that increasing NC strongly correlates with lower OOD detection error and reducing NC strongly correlates with lower OOD generalization error, as shown in Fig.~\ref{fig:vis_abstract} and Fig.~\ref{fig:nc_resnet}.

%     \item We also visualize the embeddings in the encoder and projector to gain more insights. As visible in Fig~\ref{fig:umap_vis} and Fig.~\ref{fig:umap_vis_resnet}, unlike encoder embeddings, projector embeddings form tight clusters around class-means, exhibiting a greater feature variability suppression (NC1). The projector also exhibits greater separation between ID and OOD embeddings than the encoder, as illustrated in Fig.~\ref{fig:umap_id_ood}. Moreover, energy score distribution reveals that the projector achieves much higher separation between ID and OOD data than the encoder for various OOD datasets, as exhibited in Fig.~\ref{fig:eng_id_ood} and~\ref{fig:more_eng_id_ood}.
%     These qualitative results help explain why the projector is adept at OOD detection which benefits from collapsed features.
    
% \end{enumerate}


\input{figures/umap_vis}

% Our experimental results validate that \emph{controlling NC effectively enhances OOD detection and OOD generalization abilities.} 
% Table~\ref{tab:main_results_summary} presents the summary results.
% In the Appendix, we include comprehensive results on 8 OOD datasets for different DNN architectures (see Tables~\ref{tab:main_results}, ~\ref{tab:resnet_results}, and~\ref{tab:vit_results}).


\subsection{Comparison with Baseline}

\input{tables/baseline_summary}


We want to check how standard DNNs perform without any mechanisms to control NC. As depicted in Table~\ref{tab:baseline_comp_summary}, different DNNs including VGG17, ResNet18, and ViT-T land on higher OOD detection error and OOD generalization error indicating that representations learned by these models cannot achieve both OOD detection and OOD generalization abilities. In contrast, our method shows significant improvements over these baselines. While being competitive in ID performance, our method controls NC unlike the baselines, and achieves better performance in OOD tasks. Particularly, OOD generalization is improved by $1.70 - 7.69\%$ (absolute) and OOD detection is improved by $7.01 - 29.82\%$ (absolute).
More comprehensive results on all OOD datasets are given in Table~\ref{tab:base_model} in the Appendix.


%%%%%%
%\textbf{NC Results (ImageNet-10)}
%\input{tables/nc_results}
%\textbf{OOD Detection Results (ImageNet-10)}
%\input{tables/ood_det}
%\textbf{NC Results (ImageNet-100)}
%\input{tables/nc_results_100c}
%\textbf{OOD Detection Results (ImageNet-100)}
%\input{tables/ood_det_100c}
%\textbf{OOD Generalization Results (ImageNet-100)}
%\input{tables/ood_gen_100c}
%\input{tables/toy_setting}
%%%%%%



\subsection{Entropy Regularization Mitigates NC}

\input{tables/entropy_results_summary}

At first, we measure entropy and NC1 across all VGG17 layers and observe that there lies a strong correlation between entropy and NC1 (Pearson correlation 0.88). As illustrated in Fig.~\ref{fig:entropy_nc}, stronger NC
% (i.e., lower NC1 value)
correlates with lower entropy whereas weaker NC correlates with higher entropy. This empirically demonstrates why using entropy regularization mitigates NC in the encoder. 
Next, we compare two identical VGG17 models, one uses entropy regularization and another omits it. The results are summarized in Table~\ref{tab:entropy_results_summary}. Entropy regularization mitigates NC in the encoder, as evidenced by its higher $\mathcal{NC}1$, and achieves better performance in all criteria compared to the model without entropy regularization. 
% It is worth noting that $\mathcal{NC}1$ is the most dominant indicator of NC.
% For the comprehensive results on various OOD datasets, see Table.~\ref{tab:reg_vs_no_reg} in the Appendix.

An implication of NC is that the collapsed layers exhibit lower rank in the weights and representations~\cite{rangamani2023feature}. In additional analyses provided in the Appendix~\ref{sec:analysis_entropy_reg}, we observe that our entropy regularization implicitly encourages higher rank in the encoder embeddings 
% (see Table~\ref{tab:entropy_loss_coeff}) 
and helps reduce dependence between dimensions (thereby promoting mutual independence).
% Additionally, we analyze training dynamics of entropy regularization which reveals that it effectively increases entropy and effective rank of encoder embeddings and weakens NC effect as training progresses. Results are illustrated in Fig.~\ref{fig:nc_dynamics}.
% Additional analyses on entropy regularization are provided in the Appendix~\ref{sec:analysis_entropy_reg}.




\input{figures/entropy_nc}


\subsection{Fixed Simplex ETF Projector Induces NC}


\input{tables/etf_l2_norm}


We train two identical models the same way (same hyperparameters and training protocol), one of them uses a regular trainable projector and the other one uses a frozen simplex ETF projector. 
We summarize our findings in Table~\ref{tab:etf_l2_results}. 
Our results indicate that the fixed simplex ETF projector strengthens NC more than a regular plastic projector as evidenced by lower $\mathcal{NC}1$. Consequently, the ETF projector outperforms plastic projector in OOD detection by an absolute 8.9\%. 
%Note that a fixed ETF projector reduces trainable parameters e.g., about 2 Million parameters for VGG17 saving compute cost. 

We also evaluate the impact of $L_2$ normalization over the projector embeddings. We train two models in an identical setting, the only variable we change is the $L_2$ normalization. We observe that $L_2$ normalization achieves a lower $\mathcal{NC}1$ value (thereby strengthening NC) and 3.83\% (absolute) lower OOD detection error than its counterpart. These results demonstrate that using $L_2$ normalization helps induce NC and thereby enhances OOD detection performance. Additional results are shown in the Appendix~\ref{sec:additional_exp_supp}.
% Fig.~\ref{fig:nc_dynamics} also demonstrates that $L_2$ normalization amplifies NC as training progresses.
% For detailed results, we refer readers to the Appendix (particularly, Tables~\ref{tab:plastic_proj} and~\ref{tab:l2_norm_nc}).




\subsection{Ablation Studies}


% \subsubsection{Projector Design Criteria}
\noindent
\textbf{Projector Design Criteria.}
Here we ask: \emph{does a deeper or wider projector achieve higher performance?}
Results are summarized in Table~\ref{tab:proj_results}. We find that the projector with depth 2 performs better than shallower or wider projectors. %All projectors achieve similar NC1 values. 
Table~\ref{tab:proj_design} in the Appendix contains comprehensive results.


\begin{table}[t]
\centering
  \caption{\textbf{Projector Configuration.} VGG17 models with different ETF projector configurations are trained on ImageNet-100 (ID) datasets and evaluated on 8 OOD datasets. $D$ and $W$ denote the depth and width of the projector, respectively.}
  \label{tab:proj_results}
  \centering
  \resizebox{\linewidth}{!}{
     \begin{tabular}{c|c|cccc|c|c}
     \hline %\hline
     \multicolumn{1}{c|}{\textbf{Config.}} &
     \multicolumn{1}{c|}{$\boldsymbol{\mathcal{E}}_{\text{ID}}$} &
     \multicolumn{4}{c|}{\textbf{Neural Collapse} $\downarrow$} &
     \multicolumn{1}{c|}{$\boldsymbol{\mathcal{E}}_{\text{GEN}}$} &
     \multicolumn{1}{c}{$\boldsymbol{\mathcal{E}}_{\text{DET}}$} \\
     & $\downarrow$ & $\mathcal{NC}1$ & $\mathcal{NC}2$ & $\mathcal{NC}3$ & $\mathcal{NC}4$ & Avg. $\downarrow$ & Avg. $\downarrow$ \\
    %\hline \hline
    \toprule
    $D=1$ & 12.86 & 0.375 & 0.649 & 0.500 & 1.157 & 45.37 & 87.37 \\
    %\hline

    \rowcolor{yellow!50}
    $\mathbf{D=2}$ & \textbf{12.62} & 0.393 & 0.490 & 0.468 & 0.316 & \textbf{41.85} & \textbf{65.10} \\
    %\hline
    %$D=3$ & 12.88 & 0.323 & 0.667 & 0.560 & 1.138 & 42.36 & 82.40 \\
    %\hline
    
    $W=2$ & 13.48 & 0.320 & 0.667 & 0.376 & 0.493 & 43.33 & 69.73 \\
    %\hline \hline
    \bottomrule
    \end{tabular}}
\end{table}



% \subsubsection{Group Normalization Enhances Transfer}
\noindent
\textbf{Group Normalization Enhances Transfer.}
How BN or GN impacts neural collapse was unexplored in previous work. Here, we compare BN with GN (GN is combined with weight standardization) and show the results in Table~\ref{tab:gn_results_summary}. 
% We study this aspect and provide additional insights.
% Table~\ref{tab:gn_results_summary} compares BN with GN (GN is combined with weight standardization).
We find that GN helps mitigate NC in the encoder as indicated by a higher $\mathcal{NC}1$ value than BN. This implies that, unlike GN, BN leads to stronger NC and impairs OOD transfer. This is further confirmed by GN outperforming BN by 10.11\% (absolute) in OOD generalization. Our results suggest that replacing BN is crucial for OOD generalization. Furthermore, using GN improves OOD detection by 4.37\% (absolute). 
%Additional results are provided in the Appendix.
Table~\ref{tab:bn_vs_gn} includes comprehensive results.



\begin{table}[t]
\centering
  \caption{\textbf{Impact of Group Normalization.} Reported OOD generalization and OOD detection correspond to the encoder and projector respectively. The comparison is based on ImageNet-100 pre-trained VGG17 networks and 8 OOD datasets. 
  Reported $\mathcal{NC}1$ corresponds to the encoder. 
  %Group norm is combined with weight standardization. 
  %\textcolor{brown}{GN+WS helps reduce NC in encoder as indicated by higher $\mathcal{NC}1$ than BN. GN+WS outperforms BN in both OOD detection and OOD generalization.}
  }
  \label{tab:gn_results_summary}
  \centering
  \resizebox{\linewidth}{!}{
     \begin{tabular}{c|c|c|c|c}
     \hline %\hline
     \multicolumn{1}{c|}{\textbf{Method}} &
     \multicolumn{1}{c|}{$\mathcal{NC}1$} &
     \multicolumn{1}{c|}{$\boldsymbol{\mathcal{E}}_{\text{ID}}$} &
     \multicolumn{1}{c|}{$\boldsymbol{\mathcal{E}}_{\text{GEN}}$} &
     \multicolumn{1}{c}{$\boldsymbol{\mathcal{E}}_{\text{DET}}$} \\
     & $\uparrow$ & $\downarrow$ & Avg. $\downarrow$ & Avg. $\downarrow$ \\
     
    %\hline \hline
    \toprule
    Batch Normalization & 1.401 & 12.52 & 51.96 & 69.47 \\
    \rowcolor{yellow!50}
    \textbf{Group Normalization} & \textbf{2.175} & 12.62 & \textbf{41.85} & \textbf{65.10} \\
    %\hline \hline
    \bottomrule
    \end{tabular}}
    \vspace{-0.3in}
\end{table}



% \subsubsection{Plastic vs. Fixed Classifier}

% \input{tables/etf_clf_summary}

% \noindent
% \textbf{Plastic vs. Fixed Classifier.}
% Here we ask: \emph{what happens if we configure the classifier head as a fixed Simplex ETF?}
% For this we trained two VGG17 networks in identical settings, one model comprises a plastic classifier and another one has a fixed ETF classifier. 
% As shown in Table~\ref{tab:etf_clf_results}, the plastic classifier outperforms the fixed ETF projector in all criteria.
% Table~\ref{tab:fixed_vs_plastic_head} contains detailed results.


% \subsubsection{Impact of Loss Functions: MSE Vs. CE}
\noindent
\textbf{Impact of Loss Functions: MSE Vs. CE.}
Both MSE and CE are effective loss functions to achieve NC properties~\cite{zhou2022all}. Unlike prior work, we evaluate their efficacy in both OOD detection and OOD generalization tasks.
As shown in Table~\ref{tab:mse_ce_comp}, CE outperforms MSE by 6.74\% (absolute) in OOD detection and by 17.71\% (absolute) in OOD generalization.
%compared to MSE, CE achieves lower values in all criteria e.g., ID error, OOD transfer error, and OOD detection error, thereby outperforming MSE. 
%Both MSE and CE induce NC but CE does that without sacrificing OOD transferability.
Our observations are consistent with prior work~\cite{kornblith2021better, hui2020evaluation}. %reporting that MSE rivals CE in ID performance but hurts OOD performance compared to CE. 
%To the best of our knowledge, MSE and CE were not compared in the OOD detection task. We find that CE leads to better OOD detection than MSE.


