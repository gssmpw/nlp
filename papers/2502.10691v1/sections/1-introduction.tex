\section{Introduction}
\label{sec:intro}


\input{figures/front_fig}


\begin{comment}


Out-of-distribution (OOD) detection and OOD generalization are both heavily studied in deep neural networks (DNNs). OOD detection enables DNNs to reject unfamiliar inputs, preventing overconfident mispredictions, while OOD generalization allows DNNs to transfer their knowledge to new distributions. 

For both tasks, the properties of the DNN's representations are critical for attaining strong performance. 
Recent work has shown that the degree of neural collapse (NC)—where representations become highly compact and align in a structured manner—could play a crucial role in both tasks.


Recent works suggest that \textit{representation collapse}—where class features become highly compact and align in a structured manner—plays a crucial role in both tasks. On one hand, \textbf{strong representation collapse has been shown to improve OOD detection} by forming compact and well-separated class representations, which enhances the ability to distinguish ID and OOD data~\cite{haas2023linking, wu2024pursuing, ming2022poem}. On the other hand, \textbf{representation collapse has also been shown to impair OOD generalization} by reducing feature diversity and hindering transferability to new distributions~\cite{kothapalli2023neural, masarczyk2023tunnel, harun2024what}. However, these studies focus on either OOD detection or generalization \textit{individually}, leaving open the question of how representation collapse affects both tasks \textit{simultaneously}. To the best of our knowledge, no prior work has theoretically or empirically examined this relationship.



For applications like open-world learning, where a model continuously encounters new concepts, \textit{both} capabilities are essential: OOD detection ensures reliable decision-making in uncertain environments, while OOD generalization facilitates forward transfer to improve learning efficiency. Despite their importance, these objectives have largely been studied in isolation.  

\end{comment}




Out-of-distribution (OOD) detection and OOD generalization are both heavily studied in deep neural networks (DNNs). OOD detection enables DNNs to reject unfamiliar inputs, preventing overconfident mispredictions, while OOD generalization allows DNNs to transfer their knowledge to new distributions. 
For both tasks, the properties of the DNN's representations are critical for attaining strong performance. 
Recent works suggest that \textit{Neural Collapse} (NC)—where class features become highly compact and align in a structured manner—plays a crucial role in both tasks~\cite{papyan2020prevalence, kothapalli2023neural, zhu2021geometric, rangamani2023feature}. 
On one hand, \textbf{strong neural collapse has been shown to improve OOD detection} by forming compact and well-separated class representations, which enhances the ability to distinguish in-distribution (ID) and OOD data~\cite{haas2023linking, wu2024pursuing, ming2022poem}. 
On the other hand, \textbf{neural collapse has also been shown to impair OOD generalization} by reducing feature diversity and hindering transferability to new distributions~\cite{kothapalli2023neural, masarczyk2023tunnel, harun2024what}. 
However, these studies focus on either OOD detection or generalization \textit{individually}, leaving open the question of how neural collapse affects both tasks \textit{simultaneously}. To the best of our knowledge, no prior work has theoretically or empirically examined this relationship. We present the first comprehensive study to address this research gap. 
We find that NC strongly correlates with both OOD detection and OOD generalization, as illustrated in Fig.~\ref{fig:vis_abstract}. This gives a novel pathway to synergize OOD detection and generalization by leveraging \textbf{Neural Collapse Criteria:}
\begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
    \item \textbf{Feature Collapse} ($\mathcal{NC}1$): Features within each class concentrate around a single mean, with almost no variability within classes.
    \item \textbf{Simplex ETF Structure} ($\mathcal{NC}2$): Class means, when centered at the global mean, are linearly separable, maximally distant, and form a symmetrical structure on a hypersphere known as a Simplex Equiangular Tight Frame (Simplex ETF).
    \item \textbf{Self-Duality} ($\mathcal{NC}3$): The last-layer classifiers align closely with their corresponding class means, forming a self-dual configuration.
    \item \textbf{Nearest Class Mean Decision} ($\mathcal{NC}4$): The classifier operates as a nearest class-center (NCC) decision rule, assigning classes based on proximity to class means. 
\end{enumerate}


Motivated by the insights from NC, we propose a novel framework that strategically controls NC to achieve the dual goals of OOD detection and generalization. By encouraging collapsed representations in the top layers while constraining earlier layers to reduce NC, our method enhances transferability and ensures robust OOD performance. This approach advances the understanding of representation learning in DNNs and provides a practical solution for models to simultaneously generalize well to new distributions and reliably identify them as distinct from the training data.



For applications like open-world learning, where a model continuously encounters new concepts, \textit{both} capabilities are essential: OOD detection ensures reliable decision-making in uncertain environments, while OOD generalization facilitates forward transfer to improve learning efficiency. Despite their importance, these objectives have largely been studied in isolation. 
Jointly optimizing both objectives is a challenging problem since they have conflicting goals~\cite{bai2023feed}.
Recently,~\citet{zhang2024best} reveals that current state-of-the-art methods for OOD detection achieve strong detection performance at the cost of OOD generalization abilities.
Another issue is that existing methods incorporate auxiliary OOD data during training, also known as Outlier Exposure (OE), to separate ID and OOD samples~\cite{hendrycks2018deep, wu2024pursuing, katz2022training}. And, the effectiveness of OE methods depends on the \emph{representativeness} of auxiliary OOD data, which is unknown a priori~\cite{roady2020improved}.

A few recent works focusing on OOD detection and generalization~\cite{zhang2024best, wang2024bridging, bai2023feed}, predominantly focus on covariate-shifted OOD generalization (semantic overlap with ID) and do not consider semantic OOD generalization (no semantic overlap with ID). Thus, none of the previous studies jointly optimize OOD detection and OOD generalization for semantic OOD data. Addressing this gap, we propose a unified framework that simultaneously enhances both objectives without relying on auxiliary OOD training data and explicitly targets semantic OOD scenarios.
\textbf{This work makes the following major contributions:}
\begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
    \item We present the first unified study linking neural collapse to both OOD detection and OOD generalization, revealing their strong correlation.
    \item We propose a method that mitigates neural collapse via entropy regularization for OOD generalization while leveraging a fixed Simplex ETF projector to induce neural collapse for OOD detection.
    \item Extensive experiments on diverse OOD datasets and architectures validate our approach, demonstrating strong performance in both OOD detection and generalization.
\end{enumerate}








\begin{comment}
    


Machine learning models deployed in real-world scenarios frequently encounter data with novel semantics—also known as Out-of-Distribution (OOD) data—that deviates from the training distribution. In such scenarios, a model must: (1) correctly classify known concepts, (2) reject unknown concepts without making erroneous predictions, and (3) adapt to novel inputs when necessary. These challenges necessitate proficiency in two fundamental tasks: OOD detection (identifying and rejecting novel inputs) and OOD generalization (adapting to novel distributions). However, these tasks impose conflicting demands: OOD detection requires highly discriminative representations to distinguish in-distribution (ID) data from OOD data, whereas OOD generalization benefits from learning transferable representations that adapt well to novel distributions. This inherent trade-off leaves the following research question open:
\begin{tcolorbox}[boxsep=1pt,left=2pt,right=2pt,top=0pt,bottom=0pt]
\textbf{RQ:} How can we design a learning algorithm that excels at both OOD detection and OOD generalization?
\end{tcolorbox}



While significant progress has been made in improving OOD detection and OOD generalization independently, these problems are rarely studied together~\cite{zhang2024best, wang2024bridging}. Current state-of-the-art methods for OOD detection incorporate auxiliary OOD data during training, also known as Outlier Exposure (OE)~\cite{hendrycks2018deep}, to separate ID and OOD samples~\cite{wu2024pursuing, bai2023feed, katz2022training, ming2022poem}. 
However, OE-based approaches suffer from two major limitations: (1) their effectiveness depends on the \emph{representativeness} of auxiliary OOD data, which is unknown a priori~\cite{roady2020improved}, and (2) they significantly degrade OOD generalization performance e.g., by up to 15\% in accuracy~\cite{zhang2024best}.
%However, their efficacy relies on the representativeness of the auxiliary dataset which cannot be known a prior~\cite{roady2020improved}. Moreover, performant OE approaches severely degrade OOD generalization performance ($\sim 15\%$ drop in accuracy)~\cite{zhang2024best} and it is impractical for addressing the infinite unforeseeable distribution shifts that may arise in real-world scenarios. 


Recently, a few works~\cite{zhang2024best, wang2024bridging, bai2023feed} have attempted to devise OE-based OOD detectors without sacrificing generalization capabilities.
However, for OOD generalization, they predominantly focus on covariate-shifted OOD data, where OOD samples share the same labels as ID data but undergo distributional shifts (e.g., corrupted or augmented versions of ID data).
This problem setup has limitations in open-world scenarios where deployed models will undoubtedly be exposed to semantic OOD data
—samples with entirely novel labels that do not overlap with ID data.
Thus, \textbf{none of the previous studies jointly optimize OOD detection and OOD generalization for semantic OOD data}. Addressing this gap, we propose a unified framework that simultaneously enhances both objectives without relying on auxiliary OOD training data and explicitly targets semantic OOD scenarios.
%To the best of our knowledge, our work is the first of its kind to tackle this problem.


%Recently, the phenomenon of ``Neural Collapse'' (NC)~\cite{papyan2020prevalence, kothapalli2023neural, zhu2021geometric, rangamani2023feature} has garnered attention as it provides a comprehensive understanding of how deep neural networks (DNNs) organize their representations. NC describes structured patterns that emerge in the final layer of trained DNNs:
%It is well-documented that the last-layer features and classifiers in trained DNNs tend to exhibit certain structured properties: 

A key insight in our work is that Neural Collapse (NC)~\cite{papyan2020prevalence, kothapalli2023neural, zhu2021geometric, rangamani2023feature} plays a crucial role in both OOD detection and OOD generalization. NC describes a structured organization of representations in deep neural networks (DNNs):
\begin{tcolorbox}[boxsep=1pt,left=2pt,right=2pt,top=0pt,bottom=0pt]
\textbf{The Neural Collapse Criteria:}
\begin{enumerate}
    \item \textbf{Feature Collapse} ($\mathcal{NC}1$): Features within each class concentrate around a single mean, with almost no variability within classes.
    \item \textbf{Simplex ETF Structure} ($\mathcal{NC}2$): Class means, when centered at the global mean, are linearly separable, maximally distant, and form a symmetrical structure on a hypersphere known as a Simplex Equiangular Tight Frame (Simplex ETF).
    \item \textbf{Self-Duality} ($\mathcal{NC}3$): The last-layer classifiers align closely with their corresponding class means, forming a self-dual configuration.
    \item \textbf{Nearest Class Mean Decision} ($\mathcal{NC}4$): The classifier operates similarly to the nearest class-center (NCC) decision rule, assigning classes based on proximity to the class means. 
\end{enumerate}
\end{tcolorbox}


%This structure reveals a consistent pattern of feature and classifier organization in DNNs. It has been found that NC not only forms in the last layer but also occurs in intermediate layers, referred to as intermediate neural collapse~\cite{rangamani2023feature}. A closely related phenomenon is ``the tunnel effect''~\cite{masarczyk2023tunnel, harun2024what} exhibiting compression in the deep layers and hindering OOD generalization.


The symmetric and compact structure of NC offers properties conducive to OOD detection by aligning features and classifiers tightly with class means from ID data, though it can impair transferability. Conversely, mitigating NC can enhance transferability, crucial for OOD generalization. As illustrated in Fig.~\ref{fig:vis_abstract}, NC strongly correlates with both OOD detection and OOD generalization.

Motivated by this insight, we propose a novel framework that strategically controls NC to achieve the dual goals of OOD detection and generalization. By encouraging collapsed representations in the top layers while constraining earlier layers to reduce NC, our method enhances transferability and ensures robust OOD performance. This approach advances the understanding of representation learning in DNNs and provides a practical solution for models to simultaneously generalize well to new distributions and reliably identify them as distinct from the training data.

%\paragraph{This work makes the following major contributions:}
\textbf{This work makes the following major contributions:}
\begin{enumerate}[noitemsep, nolistsep, leftmargin=*]
    %\item We present the first unified study addressing OOD detection and OOD generalization by controlling intermediate neural collapse. We demonstrate a strong correlation between neural collapse and OOD detection/generalization.
    \item We present the first unified study linking intermediate neural collapse to both OOD detection and OOD generalization, revealing their strong correlation.
    \item We propose a method that mitigates neural collapse via entropy regularization for OOD generalization while leveraging a fixed Simplex ETF projector to induce neural collapse for OOD detection.
    \item Extensive experiments on diverse OOD datasets and architectures validate our approach, demonstrating strong performance in both OOD detection and generalization.
\end{enumerate}


\end{comment}








