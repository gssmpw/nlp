\section{Related Work}
\label{sec:related-works}


The integration of deep learning into CFD processes has led to significant research efforts. The graph neural operator is among the first methods to explore neural operators in various geometries and meshes~\citep{li2020neural}. The architectures based on graph neural networks~\citep{ummenhofer2019lagrangian,sanchez2020learning,pfaff2020learning}, follow message passing and encounter similar computational challenges when dealing with realistic receptive fields. The u-shaped graph kernel, inspired by multipole methods and UNet~\citep{ronneberger2015u}, offers an innovative approach to graph and operator learning~\citep{li2020multipole}. However, the core computational challenges in 3D convolution remain nevertheless, even for FNO-based architectures that are widely deployed~\citep{li2022fourier,pathak2022fourcastnet,wen2023real}. 

Deep learning models in computer vision, for example, UNet, have been used to predict fluid average properties, such as final drag, for the automotive industry~\citep{jacob2021deep,trinh20243d}. Studies that incorporate signed distance functions (SDF) to represent geometry have gained attention in which CNNs are used as predictive models in CFD simulations \citep{guo2016convolutional,bhatnagar2019prediction}. The 3D representation of SDF inflicts significant computation costs on the 3D models, making them only scale to low-resolution SDF, missing the details in the fine-car geometries. 

Beyond partial differential equations (PDE) and scientific computing, various deep learning models have been developed to deal with fine-detail 3D scenes and objects.
In particular, for dense prediction tasks in 3D space, a network is tasked to make predictions for all voxels or points, for which 3D UNets have been widely used for, e.g., segmentation~\citep{li2018pointcnn,atzmon2018point,hermosilla2018mccnn,SubmanifoldSparseConvNet,choy20194d}. However, many of these networks exhibit poor scalability due to the cubic complexity of memory and computation $O(N^3)$ or slow neighbor search.

Recently, decomposed representations for 3D -- where multiple orthogonal 2D planes have been used to reconstruct 3D representation -- have gained popularity due to their efficient representation and have been used in generation~\citep{chan2022efficient,Shue_2023_CVPR} and reconstruction~\citep{Chen2022ECCV,kplanes_2023,cao2023hexplane}. This representation significantly reduces the memory complexity of implicit neural networks in 3D continuous planes. Despite relying on the decomposition of continuous planes and fitting a single neural network to a scene, this approach shares relevance with our factorized grid convolution approach.














Previous works in the deep learning literature, focusing on large-scale point clouds, range from the use of graph neural networks and pointnets to the u-shaped architectures along with advanced neighborhood search~\citep{qi2017pointnet,hamilton2017inductive,wang2019dynamic,choy20194d,shi2020pv}. 
However, these methods make assumptions that may not be valid when applied to CFD problems.
For example, the subsampling approach is a prominent approach to deal with the social network, classification, and segmentation to gain robustness and accuracy. However, in the automotive industry, dropping points could lead to a loss of fine-details in the geometry, the vital component of fluid dynamics evolution and car design. 
There is a need for a dedicated domain-inspired method that can work directly on fine-grained car geometry with meshes composed of $100M$ vertices~\citep{jacob2021deep}, a massive size that requires a unique design and treatment. 


\subsection{Factorization}

The factorization of weights in neural networks has been studied to reduce the computational complexity of deep learning models~\cite{panagakis2021tensor}. It has been applied to various layers, including fully connected~\cite{novikov2015tensorizing}, and most recently, low-rank adaptation of transformers~\citep{DBLP:journals/corr/abs-2106-09685},
and the training of neural operators~\citep{kossaifi2024multigrid}.
In the context of convolutions, the use of factorization was first proposed by~\citet{rigamonti2013learning}. 
This decomposition can be either implicit~\cite{chollet2017xception}, using separable convolutions, for instance~\citep{jaderberg2014speeding}, or explicit, e.g., using CP~\citep{astrid2017cp,lebedev2015speeding} or Tucker~\citep{yong2016compression} decompositions. All of these methods fit within a more general framework of decomposition of the kernels, where the full kernel is expressed in a factorized form, and the convolution is replaced by a sequence of smaller convolutions with the factors of the decomposition~\citep{9157354}.
Here, in contrast, we propose to factorize the \textbf{domain}, not the kernel, which allows us to perform \textbf{parallel} global convolution while remaining computationally tractable. The advantages include parallelism and better numerical stability, since we do not chain many operations.
Factorization of the domain can lead to efficient computation, but the challenge is to find an explicit representation of the domain (Sect.~\ref{sec:figconv}).
