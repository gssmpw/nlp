
\section{Implementation Details and Training}

We implement all baseline networks and FIG convnet using pytorch. In this section, we describe the implementation details of the FIG convnet and the training procedure.

\subsection{Efficient Radius Search and Point Convolution}
\label{sec:radius_search}

One of the most computationally intensive operations in our network is the radius search in Eq.~\ref{eq:point_conv} for which we leverage a hash grid to accelerate the search. We first create a hash grid using Warp~\citep{warp2022} with the voxel size as the radius. Then, we query all 27 neighboring voxels for each point in the point cloud and check if the point is within a unit sphere. For nonspherical neighborhoods, we scale the point cloud by the inverse of the covariance matrix $\Sigma$ and check if the point is within the unit cube.

We save the neighborhood indices and the number of neighbors per point in a compressed sparse row matrix format (CSR) and use batched sparse matrix multiplication to perform convolution in Eq.~\ref{eq:point_conv}. We provide a simple example of the radius search in the supplementary material.

\subsection{Factorized Implicit Global Convolution}
\label{sec:fig_conv_impl}

To implement 3D global convolution using factorized representations, we use a minimum of three factorized grids with one low-resolution axis. We first define the maximum resolution of the voxel grid that can represent the space explicitly, e.g. $512\times 512\times 512$. Then, we define the low resolution axis as $r_i$ for each factorized grid. Note that $r_i$ can be different for each factorized grid. For example, $512\times 512\times 2$, $512\times 3\times 512$, and $4\times 512\times 512$.

\subsection{Training Procedure and Baseline Implementation}

We train all networks using Adam Optimizer with a learning rate of $10^{-3}$, a learning rate scheduler with $\gamma=0.1$ and step size of 25 epochs, and batch size of 16 for 100 epochs on NVIDIA A100 80G GPUs. We use a single A100 if the batch size of 16 fits inside the memory and we use 2 GPUs with batch size 8 each if not to make sure that all experiments follow the same training configuration. The total training takes approximately 16 hours with two GPUs.
For pressure prediction, we first normalize the pressure as all units are in the metric system and range widely. We denote $\bar{P}$ as the normalized pressure where it has 0 mean and 1 standard deviation.
For both pressure prediction and drag prediction, we use the same mean squared error as the loss function. Training loss is simply the sum of both: $(\hat{c}_d - c_d)^2 + \frac{1}{N}\sum_i(\hat{\bar{P}}_i - \bar{P}_i)^2$ where $\hat{\cdot}$ denotes the prediction of $\cdot$ and $\bar{P}_i$ indicates the normalized pressure on the $i$-th face and $N$ the number of faces. We use the same training procedure, loss with the same batch size, learning rate, and training epochs for all baseline networks to ensure fair comparison.
There are many representative baselines, so we chose an open-source framework that supports a wide range of network architectures and is easy to implement new networks. Specifically, we use the OpenPoint library~\citep{qian2022pointnext} to implement PointNet segmentation variants, DGCNN, and transformer networks. We provide the network configuration yaml files in the supplementary material.
