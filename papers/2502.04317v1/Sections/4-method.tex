\input{Images/figconvunet}
\section{Factorized Implicit Global ConvNet}
\label{sec:method}

In this section, we introduce our factorized implicit global convolution and discuss how we create implicit factorized representations, reparameterize the convolution, implement global convolution, and fuse implicit grids. We then present a convolution block using factorized implicit grids and build a U-shaped network architecture for the prediction of the pressure and drag coefficients. An overview diagram is provided in Fig.~\ref{fig:figconvunet}.

\subsection{Factorized Implicit Grids}
\label{sec:factorized_grids}
\input{Images/convolution_diagram/convolution_comparison_figure}

Our problem domain resides in a 3D space with an additional channel dimension, mathematically represented as $\mathcal{X} = \mathbb{R}^{H_{\max} \times W_{\max} \times D_{\max} \times C}$ with high spatial resolution.
Explicitly representing an instance of the domain $X \in \mathcal{X}$ is extremely costly in terms of memory and computation due to its large size.
Instead, we propose using a set of factorized representations $\{F_m\}_{m=1}^M$, where $M$ is the number of factorized representations. Each $F_m \in \mathbb{R}^{H_m \times W_m \times D_m \times C}$ has different dimensions, collectively approximating $X(\cdot) \approx \hat{X}(\cdot;\{F_m\}_{m=1}^M)$.
These $\{F_m\}_{m=1}^M$ serve as \textbf{implicit representations} of the explicit representation $X$ because we have to decode the implicit representations to represent the original high-resolution grid, and we refer to each $F_m$ as a factorized implicit grid throughout this paper.

Mathematically, we use MLPs to project features from the factorized implicit grids $\{F_m\}_m$ to the explicit grid $X$:
\begin{align}
    X(v) \approx \hat{X}(v;\{F_m\}_m, \theta) & = \prod_m^M f(v, F_m; \theta_m) \\
    f(v, F_m; \theta_m) & = \prod_{i=i_v}^{i_v + 1} \prod_{j=j_v}^{j_v + 1} \prod_{k=k_v}^{k_v + 1} \text{MLP}(F_m[i,j,k], v; \theta_m),
\end{align}
where $(i_v,j_v,k_v)$ is the smallest integer grid coordinate closest to the query coordinate $v \in \mathbb{R}^3$ and $\theta_m$ is the parameters of the MLP, which takes the concatenated features from the implicit grid $F_m$ and position encoded $v$ as an input.

To efficiently capture the high-resolution nature of the explicit grid $X$, we propose to make only one axis of low resolution $F_m \in \mathbb{R}^{H_m \times W_m \times D_m}$.
For example, $F_1 \in \mathbb{R}^{4 \times W_{\max} \times D_{\max}}$ where $H_{\max} \gg 4$ and $F_2, F_3$ have low resolution $W$ and $D$, respectively. Thus, the cardinality of $X$, $|X|$ is much greater than that of the factorized grids, $|X| \gg \sum_m|F_m|$. 
Formally, this low-resolution size is \emph{rank} $r$ of our factorized grid.
For example, $F_x \in \mathbb{R}^{r_x \times W_{\max} \times D_{\max}}$, $F_y \in \mathbb{R}^{H_{\max} \times r_y \times D_{\max}}$, and $F_z \in \mathbb{R}^{H_{\max} \times W_{\max} \times r_z}$. In experiments, since we use 3D grids, the rank is a tuple of 3 values that we will denote as $(r_x, r_y, r_z)$, to represent the low-resolution components of $(F_x, F_y, F_z)$. In practice, we will use $r_i < 10$ in place of $H_{\max}, W_{\max}, D_{\max} > 100$, thus making the cardinality of factorized grids $|F_m|$ orders of magnitude smaller than that of $|X|$.

Note that when we use a rank of 1, that is, $(r_x, r_y, r_z) = (1, 1, 1)$, we have
an implicit representation that resembles the triplane representation proposed in~\citet{chan2022efficient} and \citet{Chen2022ECCV}. This is a special case of factorized implicit grids that are used for reconstruction without convolutions on the implicit grids, fusion (Sec.~\ref{sec:fusion}), or U-shape architecture (Sect.~\ref{sec:unet_for_drag}).

\subsection{Factorized Implicit Convolution}
\label{sec:figconv}
\input{Images/convolution_diagram/figconv3d}

In this section, we propose a convolution operation on factorized implicit grids. Specifically, we used a set of 3D convolutions on the factorized grids in parallel to approximate the 3D convolution on the explicit grid.
Let $N$ be the dimension of the high-resolution axis and $K$ be the kernel size $N \gg K$. Then, the computational complexity of the original 3D convolution is $O(N^3K^3)$ and the computational complexity of the 3D convolution on factorized grids is $O(M N^2K^2r)$, where $r$ is the dimension of the low-resolution axis, $M$ is the number of factorized grids.
Mathematically, we have:
\begin{align}
    Y = \text{Conv3D}(X; W) \approx \prod_m Y_m = \prod_m^M f(\text{Conv3D}(X_m; W_m); \theta_\text{m})
\end{align}
where $Y$ and $\hat{Y}$ are the output feature maps of the original and approximation, and $W$ and $W_m$ are the weights of the original and factorized implicit convolutions.


\subsection{Efficient Global 3D Convolution through 2D Reparameterization}
\label{sec:reparameterization}

Large convolution kernels allow output features to incorporate broader context, leading to more accurate predictions \citep{peng2017large,huang2023large}.
Experimentally, we find larger kernel sizes yield higher accuracy on the test set (Tab.~\ref{tab:drivaernet_local_vs_global}).
However, large kernel sizes can be impractical due to their computational complexity, which increases cubically with respect to the kernel size.
To enable a larger receptive field without making computation intractable, we propose a 2D reparameterization of 3D convolution that allows us to apply large convolution kernels while maintaining low computational complexity.
Mathematically, any N-D convolution can be represented as a sum of vector-matrix multiplications since the convolution weights can be represented as a band matrix.
Specifically, we focus on reparameterizing the 3D convolution to 2D convolution by flattening the low-resolution axis with the channel dimension to make use of the efficient \textbf{hardware acceleration} implemented in NVIDIA 2D convolution CUDA kernels.
Mathematically, the 3D convolution on the flattened feature map is equivalent to 2D convolution with shifted kernel weights:
\begin{align}
Y_m(i,j,k,c_\text{o}) & = \sum_{c_\text{in}}^{C} \sum_{i',j',k'}^{K} X_m(i+i',j+j',k+k',c_\text{in})W(i',j',k',c_\text{in},c_\text{o}) \\
& = \sum_{s=1}^{CK} \sum_{i',j'}^{K} X_m(i + i', j + j', k + \left\lfloor \frac{s}{C}\right\rfloor,s\mod C)W_m(i',j',s \mod C,c_{o})
\end{align}
This is simply flattening of the last spatial dimension with the channel dimension for both $X_m$ and $W_m$.\footnote{This is flattening operation \texttt{X\_m.permute(0, 3, 4, 1, 2).reshape(B, D * C, H, W)} in torch to flatten the last dimension and permute the channel to be the second axis.}
However, as we increase the kernel size $K \ge 2r - 1$ where $r$ is the chosen rank, controlling the dimension of the low-resolution axis, we can reparameterize the convolution kernel into a matrix and replace the convolution with a matrix multiplication with the flattened input.
For example, we can define a 1D convolution with kernel size $K=3$ and the axis of size 2 ($x_0, x_1$) as:
\begin{align}
    \begin{bmatrix}
        y_0 \\
        y_1
    \end{bmatrix}
    & = \begin{bmatrix}
        x_0 & x_1 & 0 \\
        0 & x_0 & x_1 \\
    \end{bmatrix}
    \begin{bmatrix}
        w_0 \\
        w_1 \\
        w_2
    \end{bmatrix} & \text{1-D spatial convolution with 1 channel}\\
    & = 
    \begin{bmatrix}
        w_0 & w_1 \\
        w_1 & w_2
    \end{bmatrix}
    \begin{bmatrix}
        x_0 \\ x_1
    \end{bmatrix} & \text{reparameterization to 0-D space 2-vector matmul}
\end{align}
Using this reparameterization, we can convert a $D$ dimensional convolution with large kernels to $D-1$-dimensional convolution with $C\times N_D$ channels  where $C$ is the original channel size and $N_D$ being the cardinality of the flattened dimension.
In addition, the flattened kernel becomes a global convolution kernel along the low-dimensional axis as $K \ge 2r - 1$.
Experimentally, we find that the larger convolution kernels outperform the smaller convolution kernels. However, if we do not use the reparameterization technique, the computation burden of the extra operations can outweigh the added benefit (Tab.~\ref{tab:drivaernet_local_vs_global}).
Note that this reparameterization does not change the underlying operation, but it reduces the computational complexity by removing redundant operations such as padding, truncation, and permutation involved in the 3D convolution as well as making use of the hardware acceleration of 2D convolution CUDA kernels.
Lastly, we name the final reparameterized convolution on the factorized implicit grids the factorized implicit global convolution (FIG convolution) as we apply global convolution on the factorized grids.

\subsection{Fusion of Factorized Implicit Grids}
\label{sec:fusion}

The convolution operation on the factorized implicit grids produces a set of feature maps $\{Y_m\}_m$ that, in combination, can represent the final feature map $\hat{Y}$ of a 3D convolution that approximates $Y$, which we do not explicitly represent.
Thus, if we apply the factorized implicit global convolution multiple times on the same factorized implicit grids, there would be no information exchange between the factorized representations.
To enable information exchange between the factorized representations, we fuse the factorized representations after each convolution by aggregating features from the other factorized grids.
Mathematically, we use trilinear interpolation to sample features from $M-1$ factorized grids $\{F_{m'}\}_{m'\neq m}$ and add the sampled features to the target grid $F_m$ by sampling from all the voxel locations $v_{ijk}$ of $F_m$. We visualize the final 3D convolution operation in Fig.~\ref{fig:figconv3d}.


\subsection{Continuous Convolution for Factorized Implicit Grids}
\input{Images/principal_conv_fig}

We discussed how we perform global convolution on the factorized implicit grids. In this section, we discuss how we initialize the factorized implicit grids from an input 3D point cloud or a mesh.
The traditional factorization of a large matrix of size $N$ requires $O(N^3)$ computational complexity, where $A \approx \hat{A} = P^TQ$. However, this decomposition is not ideal for our case, where the resolution of the domain is extremely high.
Instead, we propose learning the factorized implicit grids from the input point clouds or meshes rather than first converting to the explicit grid $X \in \mathbb{R}^{H_{\max} \times W_{\max} \times D_{\max} \times C}$ -- where $H_{\max}, W_{\max}, D_{\max}$ are the maximum resolutions of the domain and $C$ is the number of channels -- and then factorize.
We define a hyper parameter the number of factorized implicit grids $M$, as well as the size of the low-resolution axis $r$ and create $M$ factorized grids with different resolutions $\{F_m\}_m^M$, each with a different resolution $F_m \in \mathbb{R}^{H_m \times W_m \times D_m \times C}$.
Then, we use a continuous convolution in each voxel center $v_{m,ijk}$ of $F_m$ to update the feature of the voxel $f_{m,ijk}$ from the set of features $f_n$ on point $v_n$ of the point cloud. Note that the input mesh is converted to a point cloud where each point represent a face of a mesh. We use $(i,j,k)$ to represent voxels and $n$ to indicate points:
\begin{equation}
    f_{m,ijk} = \text{MLP} \left( \sum_{n \in \mathcal{N}(v_{ijk})} \text{MLP}(f_n, v_n, v_{ijk}) \right), \;\;\mathcal{N}(v, \Sigma) = \{i | \|\Sigma^{-1/2}(v_i - v)\| < 1 \}
    \label{eq:point_conv}
\end{equation}
where $\mathcal{N}(v, \Sigma)$ is the set of points around $v$ within an ellipsoid $(v_i-v)^T\Sigma^{-1}(v_i - v) < 1$ with covariance matrix $\Sigma \in \mathbb{R}^{3\times 3}$ that defines the ellipsoid of neighborhood in physical domain. We use an ellipsoid rather than a sphere since the factorized grids have a rectangular shape due to one low resolution axis. Each mlp before and after the summation uses different parameters.
To ensure the efficiency of the ellipsoid radius search, we leverage a hash grid provided by the GPU-acceleration library Warp~\citep{warp2022} and the pseudo-code is available in the Appendix.


\subsection{UNet for Pressure and Drag Prediction}
\label{sec:unet_for_drag}

We combine factorized implicit global convolution with 2D reparameterization, fusion, and learned factorization to create a U-shaped ConvNet for drag prediction.
Although drag can be directly regressed using a simple encoder architecture, the number of supervision points is extremely small compared to the number of parameters and the size of the dataset. Therefore, we add per-face pressure prediction as additional supervision, which is part of the ground truth since CFD simulation requires per-face pressure for drag simulation.
We use the encoder output for drag prediction and the decoder output for pressure prediction. The architecture is visualized in Fig.~\ref{fig:figconvunet}.
