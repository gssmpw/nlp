\clearpage
\appendix

\input{datasets}

\input{sentences}


\section{Dataset Specification}
\label{app:dataset}
The details of the 12 NLP datasets used in our experiments, including the domain, application task, input scheme, and class number, are summarized in Table~\ref{tab:datasets}. 


\section{Implementation Details} 
We use Llama-3.2-1B-Instruct with BF16 quantization as the LLM paraphraser. 
Llama-3.2 is one of the latest products of the Llama family. The Llama-3.2-1B model outperforms the Llama-3.2-3B and Llama-3.1-8B models on the rewriting task while requiring minimal memory and inference time\footnote{https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct}. 
We fine-tune the Llama model with the LlamaFactory framework~\cite{zheng2024llamafactory}. 
In the SFT stage, the learning rate is \(10^{-4}\). In the DPO stage, \(\beta\) in the loss function is set to \(0.1\) and the learning rate is \(5^{-6}\). 
The LLM is trained for 3 epochs with the AdamW optimizer, a cosine scheduler, and a warm-up ratio of \(0.1\) in each stage. 
The rank \(r\) for LoRA fine-tuning is \(8\). 
\(\mathcal{D}_{\text{SFT}}\) and \(\mathcal{D}_{\text{DPO}}\) contain 100,000 sentence pairs and 50,000 preference pairs, respectively. 
We use the embedding vector of the \texttt{[CLS]} token in the last layer of the \(\text{BERT}_{\text{base}}\) model as the embedding space \(\mathcal{E}\) for both preference dataset construction and dataset diversity evaluation. 
In the coreset selection step, the coreset ratio \(r_\text{augment} : r_\text{retain} : r_\text{prune}\) is \(1:1:1\). 
For each original sentence, we generate \(K=5\) paraphrases and sample the most diversified output. 
For downstream task evaluation, we train the BERT model for 3 epochs. 
The model is updated with the AdamW optimizer. The learning rate is \(5^{-5}\) with a linear scheduler and no warm-up. 
The LLM paraphraser and downstream task models are trained on two A100-40G GPUs with \texttt{transformers} 4.45.2, \texttt{pytorch} 2.5.1, and CUDA 12.4. 
It takes 32 and 36 minutes to train with SFT and DPO, respectively, and the LLM augmenter can paraphrase roughly 1 sentence per second. 



\section{Baseline Methods Implementation}

The error ratios for OCR and Keyboard are set to \(0.15\).
The ratios for its EDA's four operations in our experiments are set to \(0.1\), respectively. 
For BT, we use the en-zh and zh-en versions of the opus-mt model by Helsinki-NLP~\cite{TiedemannThottingal:EAMT2020} as the translator. 
The masking ratio for Unmask is set to \(0.15\) and we sample the top-1 predictions of the \(\text{BERT}_{\text{base}}\) model. 
Considering the cost of calling ChatGPT APIs, when implementing AugGPT, Grammar, Spelling, Chain, Hint, and Taboo, we use the open-source Llama-3.1-8B-Instruct model~\cite{dubey2024llama} as a replacement. Since it achieves competitive performance compared with the GPT 3.5 Turbo model (e.g. 69.4 v.s. 70.7 on MMLU 5-shot and 80.4 v.s. 69.9 on IFEval)~\cite{dubey2024llama}, replacing GPT with Llama hardly compromises the effectiveness of the baseline methods. 







\section{Augmentation Examples}

We include some augmentation examples of \Methodnamec and baseline methods in Table~\ref{tab:examples}. We can observe that \Methodnamec introduces more details (have trouble moving) and some novel vocabularies (fondness, sophisticated).

\section{Parameter Sensitivity Study}

Given the heavy time and computation cost of training LLMs, we follow most settings from existing research or default configurations from the library. Still, we conduct sensitivity studies on some key parameters unique to our method.

\subsection{Number of Generated Sentences.} 
First, we study the effect of \(K\), the number of total sentences generated by the LLM paraphraser when paraphrasing an original sentence. As Figure~\ref{subfig:num_seq} shows, the best performances are achieved when \(K\) is between \(5\) to \(8\). Intuitively, too small \(K\) limits the possibility and diversity of generated sentences and therefore affects the dataset diversity and task performance; on the other hand, too large \(K\) is likely to allow the LLM paraphraser go too far from the original semantics, breaking label preservability.




\subsection{Coreset Ratio.} 
We then use the MNLI dataset to study the effect of coreset ratio for data pruning and data augmentation. 
As presented in Figure~\ref{subfig:dropselect} and Figure~\ref{subfig:augmentselect}, both data pruning and data augmentation favor a moderate ratio, where data pruning ratio is embodied by \(r_\text{prune} / (r_\text{prune} + r_\text{retain} + r_\text{augment})\), and data augmentation is embodied by \(r_\text{augment} / (r_\text{retain} + r_\text{augment})\).




\subsection{Coreset Method.}
The choice of coreset methods is another factor that influences the final performance. 
So, we also investigate how the performance changes when augmentation is applied to different coresets. Figure~\ref{fig:coreset_sensitivity} shows that different datasets favor different coreset methods, however, in most cases, the suboptimal choices of coreset still outperform data augmentation without targeting coresets, and augmentation performance does not significantly degenerate on most suboptimal coresets. This result demonstrates our coreset-focused selective data augmentation method can benefit from appropriate coresets but is robust against suboptimal coresets. 


\begin{figure*}[htbp!]
    \centering
    \vspace{-5mm}
    \subfloat[Generated sentences]{
        \includegraphics[width=0.36\linewidth]{sensitivity_num_sequences_4.pdf}
        \label{subfig:num_seq}
    }
    \subfloat[Pruning ratio]{
        \includegraphics[width=0.27\linewidth]{sensitivity_drop_select_4.pdf}
        \label{subfig:dropselect}
    }
    \subfloat[Augmentation ratio]{
        \includegraphics[width=0.27\linewidth]{sensitivity_select_augment_4.pdf}
        \label{subfig:augmentselect}
    }
    \caption{Parameter sensitivity study on generated sentence count, coreset ratio for data pruning and augmentation on MNLI dataset.}
    \label{fig:sensitivity}
\end{figure*}


\begin{figure*}[htbp]
    \centering
    \subfloat[CoLA]{
        \includegraphics[width=0.4\linewidth]{sensitivity_coreset_COLA_4.pdf}
        \label{subfig:sen_core_cola}
    }
    % \\
    \subfloat[RCT]{
        \includegraphics[width=0.4\linewidth]{sensitivity_coreset_RCT_4.pdf}
        \label{subfig:sen_core_rct}
    }
    \\
    \subfloat[SST-2]{
        \includegraphics[width=0.4\linewidth]{sensitivity_coreset_SST-2_4.pdf}
        \label{subfig:sen_core_sst2}
    }
    % \\
    \subfloat[SUBJ]{
        \includegraphics[width=0.4\linewidth]{sensitivity_coreset_SUBJ_4.pdf}
        \label{subfig:sen_core_subj}
    }
    \caption{Sensitivity study on the choice of coreset method.}
    \label{fig:coreset_sensitivity}
\end{figure*}



\begin{figure*}[htbp]
    \centering
    \includegraphics[width=.90\linewidth]{qwen.pdf}
    \caption{Qwen performance on 12 datasets}
    \label{fig:qwen_12}
\end{figure*}

\vspace{-2mm}
\section{Full Results of LLM Architecture Exploration}
\vspace{-2mm}
\label{app:llm_arch}

Detailed results of performance on all 12 datasets and diversity in terms of 5 metrics are given in Figure~\ref{fig:qwen_12} and Figure~\ref{fig:qwen_diveristy_5}. 


\input{low_resource}

\begin{figure*}[thbp]
    \centering
    \subfloat[Distance]{
        \includegraphics[width=0.19\linewidth]{diversity_qwen_Distance.pdf}
        \label{subfig:qwen_dist_app}
    }
    \subfloat[Dispersion]{
        \includegraphics[width=0.18\linewidth]{diversity_qwen_Dispersion.pdf}
        \label{subfig:qwen_disp_app}
    }
    \subfloat[Distance]{
        \includegraphics[width=0.193\linewidth]{diversity_qwen_Radius.pdf}
        \label{subfig:qwen_rad_app}
    }
    \subfloat[Average accuracy]{
        \includegraphics[width=0.187\linewidth]{diversity_qwen_Homogeneity.pdf}
        \label{subfig:qwen_hom_app}
    }
    \subfloat[Dispersion]{
        \includegraphics[width=0.19\linewidth]{diversity_qwen_Vocabulary.pdf}
        \label{subfig:qwen_voc_app}
    }
    \caption{Diversity comparison between Llama and Qwen}
    \label{fig:qwen_diveristy_5}
\end{figure*}

\section{Alleviating the Low-resource Problem}

In our main experiments, we artificially create low-resource conditions by sampling a subset from the original dataset. In Table~\ref{tab:low_res}, we compare the results of \Methodnamec~against model performance on large subsets, which are two times the size of that used in our main experiments. 
The comparison shows that \Methodnamec~can alleviate the low-resource problem and even performs better than larger subsets in some cases. 





