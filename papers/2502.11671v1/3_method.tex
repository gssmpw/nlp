\begin{figure*}[htbp]
    \centering
    \vspace{-4mm}
    \includegraphics[width=0.85\linewidth]{framework_dasfaa-v1.pdf}
    \vspace{-2mm}
    \caption{An overall framework of \Methodnamec.}
    \label{fig:framework}
    \vspace{-3mm}
\end{figure*}


\vspace{-4mm}
\section{Methodology}
\subsection{Problem Formulation}

Given a parameterized data augmenter \(f_\theta\), the data augmentation process is expressed as \(f_\theta: \mathcal{S} = \{\mathbf{X}, \mathbf{t}\} \rightarrow \mathcal{\Tilde{S}} = \{\mathbf{\Tilde{X}}, \mathbf{\Tilde{t}}\} \), where \(\mathcal{S}\) is the original dataset composed of the feature vectors \(\mathbf{X}\) and target labels \(\mathbf{t}\), and \(\mathcal{\Tilde{S}}\) is the augmented dataset~\cite{wang2024comprehensive}. 
For a diversity metric \(D\), the diversity values of the original and the augmented datasets are \(D(\mathcal{S})\) and \(D(\mathcal{\Tilde{S}})\), respectively, and the diversity gain of that augmentation is defined as \(\Delta D(\mathcal{S}; \theta) = D(\mathcal{\Tilde{S}}) - D(\mathcal{S})\).

\Methodnamea~aims to optimize the parameter \(\theta^{\ast}\) for the data augmenter to maximize the diversity gain after augmentation:
\begin{equation}
\vspace{-3mm}
\theta^{\ast} = \mathop{\arg \max}_{\theta} \mathbbm{E} \Delta D(\mathcal{S}; \theta)
\end{equation}
When training models on the original and augmented datasets, their respective performances are evaluated by a performance metric \(P\), resulting \(P(\mathcal{S})\) for the original dataset and \(P(\mathcal{\Tilde{S}})\) for the augmented dataset. 
The performance gain is defined as \(\Delta P(\mathcal{S}; \theta) = P(\mathcal{\Tilde{S}}) - P(\mathcal{S})\).  
Moreover, by optimizing and employing the augmenter with maximum diversity gain, \Methodnamec~expects to achieve maximum performance gain under fixed conditions: 
\begin{equation}
\vspace{-3mm}
\Delta P(\mathcal{S}; \theta^{\ast}) = \mathop{\max}_{\theta} \Delta P(\mathcal{S}; \theta)
\end{equation}


\subsection{Framework Overview}

\Methodnamea~trains and employs an LLM capable of generating diverse paraphrases for data augmentation to enlarge the size of textual datasets, maintain coherence, and enhance diversity. 
As Figure~\ref{fig:framework} shows, the framework is organized as follows:
    
    \noindent$\bullet$ An LLM is first trained on a paraphrase dataset through supervised instruction fine-tuning, enabling it to function as a paraphraser that rewrites sentences while preserving the original semantics; 
    
    \noindent$\bullet$ The LLM paraphraser is then trained on a constructed preference dataset with the DPO (Direct Preference Optimization) algorithm that encourages diverse generation samples; 
    
    \noindent$\bullet$ For a given textual dataset, a coreset of samples is selected based on their importance, serving as the source sentences for paraphrase augmentation; 
    
    \noindent$\bullet$ Generated paraphrases are ranked and sampled according to their diversity, with the most diverse paraphrases integrated into the augmented dataset with original coreset samples. 

\subsection{Diverse Paraphraser Fine-tuning}

The proposed data augmentation method is constructed upon a general-purpose LLM. 
Pre-trained on a vast amount of corpus, LLMs now boast human-level understanding and generation abilities~\cite{ouyang2022training,dubey2024llama}. 
We leverage these abilities of an LLM and use it as a tool for our data augmentation process. 
The LLM is fine-tuned as a paraphraser that can rewrite sentences with alternative expressions while maintaining the original semantics. 
The LLM paraphraser is further fine-tuned to produce more diverse generation results and cover more linguistic alternatives. 
On the one hand, the change in sentence expressions can introduce diversity to the dataset. 
On the other hand, since the LLM is capable of capturing the semantics of the sentence and is prompted only to paraphrase the sentence, the coherence of the augmented data-label mapping is preserved. 

\subsubsection{LLM Paraphraser Training with PEFT} 
To fully leverage the understanding and generation abilities of the LLM, we use supervised fine-tuning (SFT) to train it to follow instructions to paraphrase existing sentences. 
Since the SFT phase of LLM training is heavily computation-consuming, we use the Parameter-Efficient Fine-Tuning (PEFT) technique to reduce the size of trainable parameters updated in the back-propagation pass to save the computation cost. 
Specifically, we adopt the Low-Rank Adaptation (LoRA) approach~\cite{hu2021lora}. 
Given a pre-trained LLM with weights \(W_0 \in  \mathbb{R}^{d \times k}\), LoRA represents its update \(\Delta W\) with \(BA\), where \(B \in \mathbb{R}^{d \times r}, A \in \mathbb{R}^{r \times k}\), and the rank \(r \ll min(d,k)\). 
During training, \(W_0\) is frozen and excluded from the gradient update, while \(A\) and \(B\) are updated instead. 
After training, \(W_0\) and \(\Delta W = BA\) are multiplied with the same input, and their outputs are summed, as in:
\vspace{-0.3cm}
\begin{equation}
    h = W_0 x + \Delta Wx = W_0 x + B A x
    \vspace{-0.15cm}
\end{equation}
In the SFT phase, we sample a subset \(\mathcal{D}_{\text{SFT}}\) from ChatGPT Paraphrases dataset\footnote{https://huggingface.co/datasets/humarin/chatgpt-paraphrases} to train the LLM. 


\subsubsection{LLM Generation Diversity Enhancement with DPO}

To align LLM generations to human preferences, recent LLMs adopt RLHF in their training process, which involves the PPO algorithm and a reward model~\cite{ouyang2022training,zhang2024prototypical}. 
However, fitting a reward model brings extra computation costs, and the gap between its prediction and actual human preference also poses threats to the effect of PPO. 
As an alternative, the DPO algorithm directly optimizes the LLMs' generation policy without training an additional reward model~\cite{rafailov2024direct}. 


\paragraph{Preference Dataset Construction.}
To construct a dataset \(\mathcal{D}_{\text{DPO}}\) for DPO training, we sample another subset from the original paraphrase dataset. Each original sentence corresponds to 5 paraphrases, and we embed the original sentence \(x\) and paraphrased sentences \([y_1, ..., y_5]\) and calculate the Euclidean distances in the embedding space \(\mathcal{E}\):
\vspace{-0.25cm}
\begin{equation}
    dist(y_i, x) = \sqrt{(e_{y_i} - e_x)^2},
    \vspace{-0.25cm}
\label{eq:dist}
\end{equation}
where \(e_x = \mathcal{E}(x)\). The paraphrase with maximum distances is considered the most diverse among possible generation results and used as the ``chosen'' (preferred) output. In contrast, the most similar is taken as the least varied generation and used as the ``rejected'' (dispreferred) one. This preference construction process is formulated in Eq.~\ref{eq:pref}: 
\vspace{-0.25cm}
\begin{equation}
    \begin{aligned}
    &y_w= \arg \max_{y_i} dist(y_i,x) \\
    \vspace{-2mm}
    &y_l= \arg \min_{y_i} dist(y_i,x),
    \end{aligned}
\vspace{-0.3cm}
\label{eq:pref}
\end{equation}
where \(y_w\) is the preferred paraphrase out of the pair \((y_w, y_l)\).


\paragraph{Training Objective.}
The goal of DPO training is to maximize the probability of generating the preferred output and minimize the probability of generating the dispreferred output. Unlike the PPO algorithm which requires a reward model and a reinforcement learning phase, DPO derives its objective by solving the optimal solution of PPO's optimization problem, as shown in Eq.~\ref{eq:dpo}:
\vspace{-3mm}
\begin{multline}
    \vspace{-3mm}
    \mathcal{L}_\text{DPO} = 
    - \mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}_{\text{DPO}}}
     \bigg[ \log \sigma \bigg(
    \\
    \beta \log \frac{\pi_{\theta}(y_w | x)} 
    {\pi_{\text{ref}}(y_w | x)} 
    % \right. 
    \left. \left. - \beta \log \frac{\pi_{\theta}(y_l | x)} 
    {\pi_{\text{ref}}(y_l | x)} 
     \right) \right],
\label{eq:dpo}
\end{multline}
where \(\pi_\theta\) denotes the generation probability of the current LLM, \(\pi_{\text{ref}}\) denotes that of the SFT model, and hyper-parameter \(\beta\) controls the deviation from the SFT model. 


\subsubsection{Diversity-oriented Sampling}


For each input sentence, we use beam search to generate \(K\) sequences from the LLM's output logits. 
We then rank these sequences based on their distances from the original sentences according to Eq.~\ref{eq:dist}. 
Only the most distant sentences are retained to reduce redundancy and prevent overfitting.

\begin{algorithm}[H]
\small
    \caption{Diversity-oriented and Coreset-focused Data Augmentation}
    \label{algo}
    \begin{algorithmic}[1]
    \REQUIRE An LLM \(f_{\theta}\), a paraphrase dataset \(\mathcal{D}_{\text{SFT}}\), a preference dataset \(\mathcal{D}_{\text{DPO}}\), and a target textual dataset \(\mathcal{S}\)
    \STATE Train the original LLM \(f_{\theta}\) on the paraphrase dataset \(\mathcal{D}_{\text{SFT}}\) with SFT
    \STATE Fine-tune the LLM on the preference dataset \(\mathcal{D}_{\text{DPO}}\) by optimizing the loss function in Eq.~\ref{eq:dpo}
    \STATE Initialize empty \(\mathcal{S}^\prime\)
    \FORALL{\((x,t) \in \mathcal{S}\) }
        \STATE Calculate importance score \(s\) for \(x\)
        \STATE \(\mathcal{S}^\prime\).\texttt{append}(\((x,t,s)\))
    \ENDFOR
    \STATE Rank samples in \(\mathcal{S}^\prime\) according to the score key \(s\)
    \STATE Split \(\mathcal{S}^\prime\) with ratio \(r_\text{augment} : r_\text{retain} : r_\text{prune}\) to \(\mathcal{S}_{\text{augment}}\), \(\mathcal{S}_{\text{retain}}\), and \(\mathcal{S}_{\text{prune}}\)
    \STATE Initialize empty \(\mathcal{\Tilde{S}}\)
    \FORALL{\((x,t,s) \in \mathcal{S}_{\text{augment}}\)}
        \STATE \(y = f_{\theta} (x)\)
        \STATE \(\mathcal{\Tilde{S}}\).\texttt{append}(\((x,t)\))
        \STATE \(\mathcal{\Tilde{S}}\).\texttt{append}(\((y,t)\))
    \ENDFOR
    \STATE \(\mathcal{\Tilde{S}} = \mathcal{\Tilde{S}} \cup \mathcal{S}_{\text{select}}\)
    \RETURN Augmented dataset \(\mathcal{\Tilde{S}}\)
    \end{algorithmic}
\end{algorithm}
\vspace{-6mm}

\subsection{Selective Coreset Data Augmentation}


Given the time consumption and computation cost of LLM-based data augmentation methods, it is non-trivial to recognize the most important samples and constrain the target for data augmentation to these samples. 
First, we train the downstream task model on the dataset and collect training dynamics and post-training post-training metrics. 
Then we calculate the EL2N~\cite{paul2021deep}, entropy~\cite{coleman2019selection}, variance~\cite{swayamdipta2020dataset}, and AUM~\cite{pleiss2020identifying} score to evaluate sample importance. 
We use score monotonic selection and coverage-centric selection (CCS)~\cite{zheng2022coverage} to derive the coresets.
\Methodnamea~performs a hierarchical coreset selection to prune some low-importance samples, retain the middle-importance samples, and augment the high-importance samples. 
Only samples of high importance are used as the seeds for data augmentation. 
The original sentences in the high-importance coreset and their paraphrases are combined with the middle-importance samples, composting the final results of our data augmentation process, as presented in Algorithm~\ref{algo}.
