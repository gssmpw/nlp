\section{Limitations}
This study has several limitations that should be acknowledged and addressed in future work.

\noindent\textbf{Diversity Exploration}: The evaluation of diversity lacks a universally accepted metric. 
In this study, we employed a subset of diversity-related evaluation methods, but other metrics, such as n-gram-based measures (e.g., assessing uniqueness or non-repetition), were not utilized. 
This limitation suggests that our assessment of diversity may not fully capture all aspects of the concept.

\noindent\textbf{Augmentation Validation}: Evaluating the correctness of generated data remains a challenging task. 
While both human evaluation and model-assisted evaluation are viable approaches, each comes with its own limitations. 
In this study, we relied on a sentiment-neutral large language model (LLM) for evaluation. 
However, this approach has inherent constraints, such as potential biases in the model and its inability to fully capture nuanced correctness in certain contexts.

\noindent\textbf{Generation Factors}: The quality and characteristics of generated samples are influenced by multiple factors, including the choice of prompts and the specific LLMs used. 
In this study, the prompts and LLMs were limited in number, and we did not exhaustively explore all possible configurations. 
This restriction may have impacted the diversity and quality of the generated samples.

\noindent\textbf{Evaluation Benchmarks}: Our evaluation was primarily focused on sentence classification tasks, and we did not extend our analysis to more general tasks, such as mathematical reasoning, instruction-following, creative writing, question-answering (QA), or chain-of-thought (CoT) reasoning. 
Additionally, we did not explore multimodality scenarios, which could provide a broader perspective on the applicability of our framework. 
Furthermore, the datasets used in this study were limited to English corpora, and we did not consider multilingual datasets, which could offer insights into cross-lingual or language-specific performance. 

\noindent\textbf{Potential Risks of Using LLM}: Leveraging LLMs for data augmentation might suffer from demographic bias and factual inaccuracies. First, LLMs may amplify demographic biases from their training data. Second, generation hallucinations may produce plausible but factually incorrect content. When task models train on such flawed data, their reliability and accuracy degrade, especially in high-stakes domains like healthcare or finance. Mitigating these risks requires rigorous validation, bias-detection frameworks, and human oversight to ensure the generated datasets uphold fairness and factual integrity.

These limitations highlight areas for future work, including the adoption of more comprehensive diversity metrics, broader validation approaches, exploration of additional prompts and LLMs, and evaluation across diverse tasks, modalities, and languages. 
