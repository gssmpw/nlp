
\vspace{-3mm}
\section{Experiments}

\vspace{-0.2cm}
\subsection{Experiment Settings}

\noindent\textbf{Evaluation Criterion.} 
We evaluate \textit{\textbf{diversity}} and \textit{\textbf{affinity}} for data distribution while measuring \textit{\textbf{performance}} on downstream tasks for effectiveness. 

\noindent \textit{\textbf{Diversity.}}
To comprehensively evaluate the effect of the proposed model \Methodname~on dataset diversity, we adopt several measures to evaluate  the augmented dataset's diversity: 

$\bullet$ \textit{Distance}: assesses the average distance between samples as follows:
    \({Distance(\mathcal{S}) = \frac{1}{|\mathcal{S}|} \sum_{x_i, x_j \in \mathcal{S}} \sqrt{(e_{x_i} - e_{x_j})^2}}\), 
    where \(e_x = \mathcal{E}(x)\) is the embedding of sample \(x\) in the embedding space \(\mathcal{E}\), and a larger distance indicates greater diversity.  

$\bullet$ \textit{Dispersion}~\cite{yu2022can}: is similar to cosine similarity but adjusted to make larger dispersion indicate greater diversity: 
        \(\mathit{Dispersion}(\mathcal{S}) = \frac{1}{|\mathcal{S}|} \sum_{x_i, x_j \in \mathcal{S}} 1 - \frac{e_{x_i} \cdot e_{x_j}}{\|e_{x_i}\|\|e_{x_j}\|}\). 

$\bullet$ \textit{Isocontour Radius}~\cite{lai2020diversity}: 
is the geometric mean of the radii, reflecting the spread of embeddings along each axis. 
    Assuming sample embeddings follow a multivariate Gaussian distribution, the dataset can be taken as an ellipsoid-shaped cluster, formulated as: \(\sum_{j=1}^{H} \frac{(e_j - \mu_j)^2}{\sigma_j^2} = c^2\),
    where \(\mu_j\) is the embeddings' mean along the \(j\)-th axis, and \(\sigma_j^2\) is the variance of the \(j\)-th axis. 
    Geometrically, the standard deviation \(\sigma_j\), is the radius \(r_j\) of the ellipsoid along the \(j\)-th axis. Thus, we have: 
        \(\mathit{Radius}(\mathcal{S}) = (\prod_{i=1}^{H} \sigma_i)^{1/H}\). 

$\bullet$ \textit{Homogeneity}~\cite{lai2020diversity}: is a metric that reflects the uniformity of a cluster distribution, suggesting that distinct samples in a diverse dataset should ideally cover the embedding space uniformly. 
    It begins by constructing a Markov chain model on the dataset embeddings. 
    The edge weight between sample \(i\) and \(j\) is defined as \(weight(i,j) = \left(\sqrt{(e_i - e_j) \cdot (e_i - e_j)}\right)^{\log{H}}\), and the transition probability from \(i\) to \(j\) is \(p(i \rightarrow j) = \frac{weight(i,j)}{\sum_{k} weight(i,k)}.\) 
    The entropy of the Markov chain is calculated by \(entropy(\mathcal{S}) = - \sum_{ij \in \mathcal{S}} v_i \cdot p(i \rightarrow j) \log{p(i \rightarrow j)}\), where \(v_i\) is the stationary distribution, assumed to be uniform. Homogeneity is then defined as, 
        \(\mathit{Homogeneity}(\mathcal{S}) = \frac{entropy(\mathcal{S})}{\log{(|\mathcal{S}|-1)}}\), 
    where \(\log{(|\mathcal{S}|-1)}\) is the entropy upper bound normalizes homogeneity into \([0,1]\)~\cite{lai2020diversity}. 
    
$\bullet$ \textit{Vocabulary Size}: evaluates dataset diversity at the token level, complementing four embedding-level diversity metrics. Given the token set of the textual dataset \(\mathcal{T}\), we count the number of unique tokens present: 
        \(\mathit{Vocabulary}(\mathcal{S}) = |\mathcal{T}|\). 



\noindent\textit{\textbf{Affinity.}} The affinity score reflects the coherence of an augmented dataset and is defined as the reciprocal of the average deviation of class centers from the original dataset: 
$
    \mathit{Affinity}(\Tilde{S}, S) = \left(\frac{1}{|\mathcal{C}|} \sum_{c_i \in \mathcal{C}} \sqrt{(\Tilde{\mu}_{c_i} - \mu_{c_i})^2}\right)^{-1},
$
where \(\mathcal{C}={c_i}\) is the set of all classes, \(\Tilde{\mu}_{c_i}\) and \(\mu_{c_i}\) are the augmented and original embedding centers respectively.

\noindent\textit{\textbf{Performance}} on downstream task. 
Following the practice in existing research on textual data augmentation, we train a \(\text{BERT}_{\text{base}}\) model~\cite{kenton2019bert} with a classification head on the original and augmented datasets to evaluate the effect of our proposed data augmentation approach. 
We report the prediction accuracy scores on each dataset to measure downstream task performance.

\noindent\textbf{Datasets.} 
We conduct extensive experiments on 12 NLP datasets to verify the effectiveness of \Methodnamec. 
As specified in Appendix~\ref{app:dataset}, 
our selection of datasets covers a wide range of text classification tasks and includes datasets with different characteristics. 
Following the settings in~\cite{yoo2021gpt3mix}, we sample a subset (1.2K samples) from the full dataset to unify the evaluation settings, enable a fair comparison between methods, and simulate a low-resource condition where data augmentation is of significant necessity. 




\noindent\textbf{Baseline Methods.}
We compare \Methodnamec~with eleven representative data augmentation methods. 
(1) OCR and (2) Keyboard perform common OCR or typing errors at the character level. 
(3) EDA randomly inserts, deletes, replaces, or swaps words in the sentences to create more samples. 
(4) Back-translation (BT) involves translating the source sentences to an intermediary language. 
(5) Unmask randomly replaces words with \texttt{[MASK]} and predicts the masked words with the BERT model. 
(6) AugGPT directly prompts ChatGPT for paraphrases without further fine-tuning~\cite{dai2025auggpt}.
(7) Grammar and (8) Spelling are two exemplar methods selected by Self-LLMDA~\cite{li2024empowering}, which prompt the LLM to simulate common grammatical variation or spelling errors made by humans.
(9) Chain, (10) Hint, and (11) Taboo generate paraphrases with three different diversity incentives~\cite{cegin2024effects}.

\input{results}

\vspace{-2mm}

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{aff_div_pfm.pdf}
    \vspace{-2mm}
    \vspace{-3mm}
    \caption{Diversity, affinity, and performance achieved by \Methodnamec~and baseline methods. Results are averaged on 12 datasets and the diversity rankings are further averaged on 5  metrics in this diagram. Smaller number for the rankings indicates better results. }
    \label{fig:overall}
    \vspace{-4mm}
\end{figure}

\vspace{-2mm}
\subsection{Overall Results}
\vspace{-1mm}


To verify the effectiveness of \Methodnamec, we evaluate downstream task accuracy alongside the diversity and affinity of the augmented dataset. 
We report the rankings of performance, diversity, and affinity averaged on 12 datasets achieved by \Methodnamec~and 11 baseline methods in Figure~\ref{fig:overall}. 
From these results, we have the following observations: 
\textbf{(1)} \textbf{\Methodnamea~achieves the highest performance on downstream tasks} compared to other SOTA data augmentation methods, as indicated by the color bar in Figure~\ref{fig:overall}. 
This demonstrates the high quality and superior adaptability of the datasets generated by our proposed method in real-world applications. 
\textbf{(2)} \textbf{\Methodnamea~achieves the highest diversity score and outperforms all other baseline methods.} This implies that \Methodnamea~effectively improves dataset diversity. 
\textbf{(3)} \textbf{\Methodnamea~achieves a considerably high position on the affinity rankings}, indicating that the sample semantics are preserved to the greatest extent possible. 
In sum, \Methodnamea~achieves the top position in the combined dataset diversity and affinity rankings. 
Additionally, it achieves the best downstream task performance, indicated by the lightest yellow. 




\input{diversity}

\subsection{Performance, Diversity, and Affinity}

\subsubsection{Performance Gains}

The full results for BERT classification performance on original and augmented datasets are presented in Table~\ref{tab:results}. The results show that \Methodnamec~surpasses all baseline methods on average. Specifically, it outperforms the baseline methods on 11 out of 12 datasets except on RTE. \Methodnamea~achieves performance gain of \(10.52\%\) on average, surpassing the runner-up method, namely Taboo, with 3.76 percentage points. 

\vspace{-1.5mm}
\subsubsection{Diversity Gain}
\vspace{-1.5mm}


We demonstrate the diversity gains in terms of all 5 diversity metrics along with their rankings achieved by \Methodnamec~and baseline methods in Figure~\ref{tab:diversity}. 
\Methodnamec~ranks the top on the chart with an average ranking of 3.0. Specifically, it achieves the best for the Distance and Dispersion metrics. For the Vocabulary metric, OCR and Keyboard include large amounts of out-of-vocabulary tokens due to character level perturbation. The three baselines with diversity incentives, namely Chian, Hint, and Taboo, also achieve reasonably good diversity gain, in line with the results of \cite{cegin2024effects}.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{affinity.pdf}
    \captionof{figure}{Affinity scores of \Methodnamec~and 10 baseline methods. The scores are averaged on 12 datasets.}
    \vspace{-4mm}
    \label{fig:affinity}
\end{figure}

\vspace{-1.5mm}
\subsubsection{Affinity and Paraphrase Validity}
\vspace{-1.5mm}


We present the affinity of \Methodnamec~and baseline methods in Figure~\ref{fig:affinity}, where \Methodnamea~outperforms other methods except Unmask, whose affinity score is 1.95. The Unmask method generates augmentation by replacing randomly selected words with ``\texttt{[MASK]}'' and predicts the masked words with the BERT model. Since the augmented samples are recovered from the BERT embeddings of the corrupted original samples and we use the BERT embeddings to calculate affinity, it is reasonable to yield extremely high affinity scores. 
Following \cite{cegin2023chatgpt,cegin2024effects}, we also investigate paraphrase validity at the sample level. We prompt the DeepSeek-V3 model to check if the paraphrases are semantically similar to the original samples and adhere to the original labels. Results show that \(97\%\) paraphrases are valid, suggesting the data augmenter introduces negligible noises to the dataset. 




\vspace{-2mm}
\subsection{Ablation Studies}
\vspace{-2mm}

To verify the effectiveness of our proposed method, we conduct ablation studies to show that all components in the method framework contribute to the final performance gains, as shown in Table~\ref{tab:ablation}, where w/o Coreset refers to applying augmentation on a random subset of the dataset without deriving a coreset of importance samples, w/o Selective refers to augmenting samples in both \(\mathcal{S}_\text{retain}\) and \(\mathcal{S}_\text{augment}\) instead of only augmenting the latter, w/o Aug refers to using the coreset directly for training without data augmentation, w/o DPO refers to using the LLM paraphraser from the SFT stage for data augmentation, w/o DS refers to removing the diversity-based sampling module, and w/o DPO|DS removes both steps. 
Thus, all components in our method framework contribute to the final performance gains. 
As Figure~\ref{fig:div_ablt} shows, we also study the effect of diversity-oriented fine-tuning and diversity-based sampling on the dataset diversity, demonstrating that all proposed components are effective. 
\input{ablation}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.85\linewidth]{diversity_radar_ablt_circle.pdf}
    \vspace{-3mm}
    \caption{Ablation study on diversity gains}
    \vspace{-3mm}
    \label{fig:div_ablt}
\end{figure}




\input{t5}

\begin{figure}[t]
    \centering
    \vspace{-3mm}
    \subfloat[Average accuracy]{
        \includegraphics[width=0.31\linewidth]{qwen_mean.pdf}
        \label{subfig:qwen_perf}
    }
    \subfloat[Distance]{
        \includegraphics[width=0.33\linewidth]{diversity_qwen_Distance.pdf}
        \label{subfig:qwen_dist}
    }
    \subfloat[Dispersion]{
        \includegraphics[width=0.315\linewidth]{diversity_qwen_Dispersion.pdf}
        \label{subfig:qwen_disp}
    }
    \vspace{-2mm}
    \caption{Performance and diversity comparison between Llama and Qwen.}
    \vspace{-3mm}
    \label{fig:qwen}
\end{figure}

\subsection{LLM Architectures Exploration}
To exhibit the generalizability of our proposed methodology, we replace the LLM augmenter and downstream task model with other LLM architectures respectively. The results show that \Methodnamec~is agnostic to LLM architectures.

For the LLM augmenter, we replace the Llama-3.2-1B-Instruct model with the similar-sized Qwen2.5-1.5B-Instruct model~\cite{qwen2.5}. As shown in Figure~\ref{fig:qwen}, datasets augmented by the Qwen model significantly outperform the original dataset and achieve comparable accuracy and diversity with those of the Llama model. Detailed results are given in Appendix~\ref{app:llm_arch}.

For the downstream task model, we replace \(\text{BERT}\), an encoder-only model with the GPT-based and T5-based classification models. 
GPT is a decoder-only LLM and is especially adept at generating texts from a prompt. Based on an encoder-decoder transformer architecture, T5 is trained to perform all NLP tasks in a unified text-to-text format and is favorable in broad cases. Specifically, we use GPT-2~\cite{radford2019language} and T5-large~\cite{raffel2020exploring} as the backbone of classification models, train these models on the MNLI, CoLA, and RCT datasets, and collect their performances. 
Experimental results in Table~\ref{tab:t5} show that \Methodnamec~ benefits both decoder-only models such as GPT and encoder-decoder models such as T5. 
