[
  {
    "index": 0,
    "papers": [
      {
        "key": "wei2023jailbroken",
        "author": "Alexander Wei and Nika Haghtalab and Jacob Steinhardt",
        "title": "Jailbroken: How Does LLM Safety Training Fail?"
      },
      {
        "key": "yong2024lowresource",
        "author": "Zheng-Xin Yong and Cristina Menghini and Stephen H. Bach",
        "title": "Low-Resource Languages Jailbreak GPT-4"
      },
      {
        "key": "yuan2024cipher",
        "author": "Youliang Yuan and Wenxiang Jiao and Wenxuan Wang and Jen-tse Huang and Pinjia He and Shuming Shi and Zhaopeng Tu",
        "title": "GPT-4 Is Too Smart To Be Safe: Stealthy Chat with LLMs via Cipher"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zou2023universal",
        "author": "Andy Zou and Zifan Wang and Nicholas Carlini and Milad Nasr and J. Zico Kolter and Matt Fredrikson",
        "title": "Universal and Transferable Adversarial Attacks on Aligned Language Models"
      },
      {
        "key": "liu2024autodan",
        "author": "Xiaogeng Liu and Nan Xu and Muhao Chen and Chaowei Xiao",
        "title": "AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models"
      },
      {
        "key": "paulus2024advprompter",
        "author": "Anselm Paulus and Arman Zharmagambetov and Chuan Guo and Brandon Amos and Yuandong Tian",
        "title": "AdvPrompter: Fast Adaptive Adversarial Prompting for LLMs"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "chao2024jailbreaking",
        "author": "Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong",
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries"
      },
      {
        "key": "mehrotra2024tree",
        "author": "Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi",
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
      },
      {
        "key": "ding2024wolf",
        "author": "Peng Ding and Jun Kuang and Dan Ma and Xuezhi Cao and Yunsen Xian and Jiajun Chen and Shujian Huang",
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "chao2024jailbreaking",
        "author": "Patrick Chao and Alexander Robey and Edgar Dobriban and Hamed Hassani and George J. Pappas and Eric Wong",
        "title": "Jailbreaking Black Box Large Language Models in Twenty Queries"
      },
      {
        "key": "mehrotra2024tree",
        "author": "Anay Mehrotra and Manolis Zampetakis and Paul Kassianik and Blaine Nelson and Hyrum Anderson and Yaron Singer and Amin Karbasi",
        "title": "Tree of Attacks: Jailbreaking Black-Box LLMs Automatically"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "ding2024wolf",
        "author": "Peng Ding and Jun Kuang and Dan Ma and Xuezhi Cao and Yunsen Xian and Jiajun Chen and Shujian Huang",
        "title": "A Wolf in Sheep's Clothing: Generalized Nested Jailbreak Prompts can Fool Large Language Models Easily"
      },
      {
        "key": "yu2024gptfuzzer",
        "author": "Jiahao Yu and Xingwei Lin and Zheng Yu and Xinyu Xing",
        "title": "GPTFUZZER: Red Teaming Large Language Models with Auto-Generated Jailbreak Prompts"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "robey2024smoothllm",
        "author": "Alexander Robey and Eric Wong and Hamed Hassani and George J. Pappas",
        "title": "SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks"
      },
      {
        "key": "kumar2025certifying",
        "author": "Aounon Kumar and Chirag Agarwal and Suraj Srinivas and Aaron Jiaxun Li and Soheil Feizi and Himabindu Lakkaraju",
        "title": "Certifying LLM Safety against Adversarial Prompting"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "alon2023detecting",
        "author": "Gabriel Alon and Michael Kamfonas",
        "title": "Detecting Language Model Attacks with Perplexity"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "cao2024rallm",
        "author": "Cao, Bochuan  and\nCao, Yuanpu  and\nLin, Lu  and\nChen, Jinghui",
        "title": "Defending Against Alignment-Breaking Attacks via Robustly Aligned {LLM}"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "phute2024selfexam",
        "author": "Mansi Phute and Alec Helbling and Matthew Hull and ShengYun Peng and Sebastian Szyller and Cory Cornelius and Duen Horng Chau",
        "title": "LLM Self Defense: By Self Examination, LLMs Know They Are Being Tricked"
      },
      {
        "key": "xie2023defending",
        "author": "Xie, Yueqi and Yi, Jingwei and Shao, Jiawei and Curl, Justin and Lyu, Lingjuan and Chen, Qifeng and Xie, Xing and Wu, Fangzhao",
        "title": "Defending chatgpt against jailbreak attack via self-reminders"
      },
      {
        "key": "zhang2024gp",
        "author": "Zhang, Zhexin  and\nYang, Junxiao  and\nKe, Pei  and\nMi, Fei  and\nWang, Hongning  and\nHuang, Minlie",
        "title": "Defending Large Language Models Against Jailbreaking Attacks Through Goal Prioritization"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "zhou2024icag",
        "author": "Zhou, Yujun  and\nHan, Yufei  and\nZhuang, Haomin  and\nGuo, Kehan  and\nLiang, Zhenwen  and\nBao, Hongyan  and\nZhang, Xiangliang",
        "title": "Defending Jailbreak Prompts via In-Context Adversarial Game"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "xu-etal-2024-safedecoding",
        "author": "Xu, Zhangchen  and\nJiang, Fengqing  and\nNiu, Luyao  and\nJia, Jinyuan  and\nLin, Bill Yuchen  and\nPoovendran, Radha",
        "title": "{S}afe{D}ecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding"
      },
      {
        "key": "liu2024aed",
        "author": "Liu, Quan  and\nZhou, Zhenhong  and\nHe, Longzhu  and\nLiu, Yi  and\nZhang, Wei  and\nSu, Sen",
        "title": "Alignment-Enhanced Decoding: Defending Jailbreaks via Token-Level Adaptive Refining of Probability Distributions"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "zhao2024LED",
        "author": "Zhao, Wei  and\nLi, Zhe  and\nLi, Yige  and\nZhang, Ye  and\nSun, Jun",
        "title": "Defending Large Language Models Against Jailbreak Attacks via Layer-specific Editing"
      },
      {
        "key": "ouyang2025layer",
        "author": "Yang Ouyang and Hengrui Gu and Shuhang Lin and Wenyue Hua and Jie Peng and Bhavya Kailkhura and Tianlong Chen and Kaixiong Zhou",
        "title": "Layer-Level Self-Exposure and Patch: Affirmative Token Mitigation for Jailbreak Attack Defense"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "li2025revisiting",
        "author": "Li, Tianlong  and\nWang, Zhenghua  and\nLiu, Wenhao  and\nWu, Muling  and\nDou, Shihan  and\nLv, Changze  and\nWang, Xiaohua  and\nZheng, Xiaoqing  and\nHuang, Xuanjing",
        "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective"
      },
      {
        "key": "shen2025JA",
        "author": "Guobin Shen and Dongcheng Zhao and Yiting Dong and Xiang He and Yi Zeng",
        "title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zou2023RepE",
        "author": "Andy Zou and Long Phan and Sarah Chen and James Campbell and Phillip Guo and Richard Ren and Alexander Pan and Xuwang Yin and Mantas Mazeika and Ann-Kathrin Dombrowski and Shashwat Goel and Nathaniel Li and Michael J. Byun and Zifan Wang and Alex Mallen and Steven Basart and Sanmi Koyejo and Dawn Song and Matt Fredrikson and J. Zico Kolter and Dan Hendrycks",
        "title": "Representation Engineering: A Top-Down Approach to AI Transparency"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhou2024explain",
        "author": "Zhou, Zhenhong  and\nYu, Haiyang  and\nZhang, Xinghua  and\nXu, Rongwu  and\nHuang, Fei  and\nLi, Yongbin",
        "title": "How Alignment and Jailbreak Work: Explain {LLM} Safety through Intermediate Hidden States"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "arditi2024refusal",
        "author": "Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Panickssery and Wes Gurnee and Neel Nanda",
        "title": "Refusal in Language Models Is Mediated by a Single Direction"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "li2025revisiting",
        "author": "Li, Tianlong  and\nWang, Zhenghua  and\nLiu, Wenhao  and\nWu, Muling  and\nDou, Shihan  and\nLv, Changze  and\nWang, Xiaohua  and\nZheng, Xiaoqing  and\nHuang, Xuanjing",
        "title": "Revisiting Jailbreaking for Large Language Models: A Representation Engineering Perspective"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "zheng2024DRO",
        "author": "Chujie Zheng and Fan Yin and Hao Zhou and Fandong Meng and Jie Zhou and Kai-Wei Chang and Minlie Huang and Nanyun Peng",
        "title": "On Prompt-Driven Safeguarding for Large Language Models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "shen2025JA",
        "author": "Guobin Shen and Dongcheng Zhao and Yiting Dong and Xiang He and Yi Zeng",
        "title": "Jailbreak Antidote: Runtime Safety-Utility Balance via Sparse Representation Adjustment in Large Language Models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "gao2024shaping",
        "author": "Lang Gao and Xiangliang Zhang and Preslav Nakov and Xiuying Chen",
        "title": "Shaping the Safety Boundaries: Understanding and Defending Against Jailbreaks in Large Language Models"
      }
    ]
  }
]