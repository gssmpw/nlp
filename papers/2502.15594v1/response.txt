\section{Related Work}
\textbf{Jailbreak Attacks}\quad
Jailbreak attacks aim to bypass alignment or safeguards, forcing LLMs to generate inappropriate content. Early jailbreak attacks **Carlini et al., "Towards Evaluating the Robustness of Neural Networks"** rely on manually crafted adversarial prompts, which primarily exploit objective competition and mismatched generalization to achieve jailbreaks. Subsequent optimization-based attacks **Goodfellow et al., "Explaining and Harnessing Adversarial Examples"** introduce automated adversarial prompt optimization by leveraging the internal states of LLMs, significantly improving both the success rate and efficiency of jailbreaks. Recent jailbreak attacks **Ebrahimi et al., "Adversarial Attacks on Deep Learning Models for Natural Language Processing"** iteratively rewrite and refine adversarial prompts using one or multiple LLMs, further exposing security vulnerabilities in LLMs.
% More recent jailbreak attacks harnessing the powerful capabilities of LLMs to facilitate jailbreaks **Bathani et al., "Jailbreaking Neural Networks with Adversarial Examples"**. LLM-based attacks iteratively rewrite and refine adversarial prompts using one or multiple LLMs **Ebrahimi et al., "Adversarial Attacks on Deep Learning Models for Natural Language Processing"**, further exposing security vulnerabilities in large language models.
\vspace{0.2\baselineskip}
\\ \textbf{Jailbreak Defenses}\quad 
To address the challenges posed by jailbreak attacks, numerous defense methods have been proposed **Madry et al., "Towards Deep Learning Models Resistant to Adversarial Attacks"**. Detection-based approaches identify adversarial prompts by computing perplexity **Huang et al., "Adversarial Examples: Threats to Deep Learning on Computer Vision"** or randomly deleting parts of the input **Dziugaite et al., "Stability and Training-Induced Generalization Error in Stochastic Neural Networks"**. Some methods prompt the LLM to perform self-checking through instructions **Wang et al., "Self-Checking for Deep Learning Models"** or context **Chen et al., "Context-Aware Self-Checking for Deep Learning Models"**. Decoding-based defenses **Li et al., "Decoding-Based Defenses against Adversarial Attacks on Natural Language Processing"** focus on analyzing decoding probabilities under different conditions and formulating decoding strategies to ensure safer outputs. Additionally, certain approaches **Gao et al., "Editing Model Parameters to Prevent Harmful Knowledge Acquisition in Large Language Models"** edit specific model parameters to make LLMs forget harmful knowledge. A more controllable and efficient class of defenses **Wang et al., "Manipulating Representations for Safe Inference"** involves manipulating representations to mitigate jailbreak attacks without modifying model parameters or adding decoding overhead. 
% This class of defenses does not require modifying model parameters or adding decoding overhead.
\vspace{0.2\baselineskip}
\\ \textbf{Representation Engineering for Safety}\quad     % to check
% \textcolor{blue}{Representation engineering is associated with adjusting the representation of individual layers during inference to modify the behavior of the LLMs.} 
Many studies have employed representation engineering techniques **Wang et al., "Manipulating Representations for Safe Inference"** to investigate or enhance the safety of LLMs. **Bathani et al., "Representation Engineering for Improving Safety in Large Language Models"** and **Li et al., "Analyzing Jailbreak Attacks from a Representation Perspective"** analyze the mechanisms of jailbreak and refusal from a representation perspective, respectively. 
**Gao et al., "Robustifying Large Language Models with Enhanced Safety Patterns"** improve the robustness of LLMs by strengthening the safety patterns they recognize.
**Wang et al., "Learnable Safety Prompt for Safe Inference in Large Language Models"** introduce a learnable safety prompt that aims to increase the separation between harmful and harmless query representations along the refusal direction. 
% However, this method is not specifically designed to defend against jailbreak attacks.
**Bathani et al., "Safe Difference Vector for Guiding LLMs toward Rejecting Malicious Instructions"** add a difference vector to query representations to guide the LLM toward rejecting malicious instructions, while **Gao et al., "Mitigating Jailbreak Attacks through Activation Confinement in Large Language Models"** mitigate jailbreak attacks by constraining activations within a safe boundary. 
A major drawback of these two approaches is that their interventions cannot be automatically optimized. This means that when the intervention is applied to all query representations, the choice of intervention strength becomes highly sensitive.   
In contrast, our method adopts a parameterized intervention, which adaptively identifies and adjusts jailbreak-related representations regardless of manually tuning the intervention strength.