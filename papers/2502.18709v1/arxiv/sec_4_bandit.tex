\section{Bandit Feedback}
\label{sec:bandit}
In this section, we present two online structured prediction algorithms in the bandit feedback setup and analyze their regret.  
Our results here are mostly special cases of the regret bounds obtained when handling bandit and delayed feedback (\cref{sec:bandit_and_delayed}). 
Nevertheless, by focusing on the case without delay, we provide a clearer exposition of the core ideas.

\subsection{Randomized Decoding with Uniform Exploration}
\begin{algorithm}[t]
    \caption{Randomized decoding with uniform exploration (RDUE) $\psi_\Omega$}
    \label{ALG:randomized decoding with uniform exploration}
    \begin{algorithmic}[1]
        \Require{$\thb\in\R^n$, $q \in [0,1]$}
        \State {Sample $X \sim \mathrm{Ber}(q)$ 
        }
        \LineIf{$X=0$}{$\bm{\hat{y}}\leftarrow\phi_\Omega(\thb)$}
        \LineIf{$X=1$}{Sample $\bm{y}^\ast \sim \mathrm{Unif}(\mathcal{Y})$ and $\yh\leftarrow\bm{y}^\ast$}
        \Ensure{$\psi_\Omega(\thb)=\yh$}
    \end{algorithmic}
\end{algorithm}
Here, we discuss the properties of the decoding function, \emph{Randomized Decoding with Uniform Exploration (RDUE)}, which will be used in both algorithms to convert scores into outputs.  
As discussed in \cref{subsec:randomized_decoding}, in online structured prediction with full information feedback, the randomized decoding (\cref{ALG: randomized decoding}) was introduced as a decoding function \citep{pmlr-v247-sakaue24a}.  
However, naively applying the randomized decoding does not lead to a satisfactory regret bound under bandit feedback.  
We extend the framework of the randomized decoding to handle bandit feedback effectively.

RDUE (\cref{ALG:randomized decoding with uniform exploration}) is a procedure that, with probability $q \in [0,1]$, selects $\hat{\bmy}$ uniformly at random from $\yy$,  
and with probability $1-q$, selects the output of the randomized decoding.  
Using RDUE, we define $p_t(\bm{y})$ as the probability that $\hat{\bm{y}}_t$ coincides with $\bm{y}$ at round $t$.  
Note that for any $\bm{y} \in \mathcal{Y}$, it holds that  
$
p_t(\bm{y})\geq\frac{q}{\K}.
$
Furthermore, similar to the property of the randomized decoding in \cref{lem:expected_target_bound}, RDUE satisfies the following property:
\begin{lemma}
    \label{lem:bound of randomized decoding with uniform exploration}
    For any $(\thb,\bm{y})\in\R^d\times\mathcal{Y}$, RDUE $\psi_\Omega$ satisfies
    \[
        \E\brk*{L(\psi_\Omega(\thb);\bm{y})}\leq\frac{4\gamma}{\lambda\nu}(1-q)S_\Omega(\thb;\bmy)+q\frac{\K-1}{\K  },
    \]
  where the expectation is taken with respect to the internal randomness of RDUE. 
\end{lemma}
\begin{proof}
Since the output of the randomized decoding is chosen with probability $1-q$,  
and a uniformly random output is chosen with probability $q$, we have  
$
\E\brk*{L(\psi_\Omega(\thb);\bmy)}=(1-q)\E\brk*{L(\phi_\Omega(\thb);\bmy)}+q\frac{\K-1}{\K},
$
where we used $\L(\cdot;\cdot)\leq 1$ and $\phi_\Omega$ is the randomized decoding.  
Hence, combining this with \cref{lem:expected_target_bound}, we obtain the desired bound.
\end{proof}

Additionally, this section makes the following assumption:
\begin{assumption}
    \label{asp:bandit_a}
    There exists $a\in\prn{0,1}$ such that  
    \[
    \expect{L_t(\yht)}\leq(1-a)\sw+q.
    \]  
    Here, $\expect{\cdot}$ denotes the conditional expectation given the random variables $\hat{\bmy}_1,\dots,\hat{\bmy}_{t-1}$.  
\end{assumption}
This assumption can be satisfied by using RDUE and letting   
$a \leq 1-\frac{4\gamma}{\lambda\nu}(1-q)$  
if $\lambda>\frac{4\gamma}{\nu}(1-q)$, according to \cref{lem:bound of randomized decoding with uniform exploration}.  
In what follows, let $a = 1-\frac{4\gamma}{\lambda\nu}$.
Note that $\lambda \geq \frac{4\gamma}{\nu}$ holds in the cases of multiclass classification, multilabel classification, and ranking, as discussed in \cref{subsec:pre_examples}.


\subsection{Online Gradient Descent}\label{subsec:ogd}
The algorithm in this section uses the adaptive Online Gradient Descent (OGD, \citealt{streeter2010regretonlineconditioning}) as $\alg$.
OGD updates $\wt$ to $\W_{t+1}$ using a gradient $\bm{G}_t$ and learning rate $\eta_t$ by
$
    \W_{t+1} \leftarrow \Pi_{\ww} \prn*{\wt - \eta_{t} \bm{G}_t},
$
where $\Pi_{\ww}(\bm Z) = \argmin_{\bm{X} \in \ww} \nrm{\bm X - \bm Z}_{\F}$.
OGD with appropriately chosen learning rate $\eta_t$ achieves the following bound:
\begin{lemma}[{\citealt[Theorem 4.14]{orabona2023modernintroductiononlinelearning}}]
    \label{lem:ogd}
    Suppose that we set the learning rate to $\eta_t=\frac{B}{\sqrt{2 \sum_{i=1}^t\nrm{\bm{G}_i}_{\mathrm{F}}^2}}$ and do not update on rounds when $\bm{G}_t$ is the all-zero matrix.
    Then, for any $\U\in\ww$, OGD achieves 
    $
        \sumt{\inpr{\bm{G}_t, \W_t - \U}}
        \leq \sqrt{2}B\sqrt{\sumt{\nrm{\bm{G}_t}_{\mathrm{F}}^2}}.
    $
\end{lemma}

\subsection{$O(\sqrt{K T})$ Regret Algorithm}
\label{subsec:Bandit_Structured_Prediction_with_General_Losses}
Here, we present an algorithm that achieves a regret upper bound of $O(\sqrt{\K T})$.
\paragraph{Algorithm based on inverse-weighted estimator}
In the bandit setting, the true output $\yt$ is not observed,  
and thus it is necessary to estimate the gradient required for updating $\wt$.  
To deal with this, we use the following inverse-weighted gradient estimator:
\begin{equation}\label{eq:inverse_weighted_est}
    \gtil\coloneqq\frac{\ind[\yht=\yt]}{p_t(\yt)}\G_t,
\end{equation}
where we recall that $\G_t=\nabla S_t(\wt) = \prn{\hat{\bm{y}}_\Omega(\bm{\theta}_t) - \yt} \bm{x}_t^\top$.
Note that $\gtil$ is unbiased, i.e., $\E\brk[\big]{\gtil}=\G_t$.
We use RDUE with $q=B\sqrt{\K/T}$ as the decoding function (assuming $T \geq B^2 \K$ for simplicity).  
For $\alg$, we employ the adaptive OGD in \cref{subsec:ogd} with the learning rate of
$
\eta_t=\frac{B}{\sqrt{2 \sum_{i=1}^t\nrm{\tilde{\G}_i}_{\mathrm{F}}^2}}.
$

\begin{remark}\label{rem:zero-loss}
This study defines the bandit feedback as the value of the target loss function $L_t(\yht)$.
Note, however, that the above algorithm operates using only the weaker feedback of $\ind\brk*{\yht\neq\yt}$. 
\end{remark}

\paragraph{Regret bounds and analysis}
The above algorithm achieves the following regret bound:
\begin{theorem}\label{thm:bandit_regret_expectation_abstract}
    The regret of the above algorithm is bounded by
    $
        \E\brk{\reg}\leq \prn*{\frac{b}{2a}+1}B\sqrt{\K T}.
    $
\end{theorem}
The $O(\sqrt{\K T})$ bound has an optimal dependency on $T$  
and matches the $\sqrt{T}$ lower bound in the special case of online multiclass classification with bandit feedback \citep[Corollary 1]{NEURIPS2021_Hoeven}.  
Furthermore, our bound improves the existing $O(\sqrt{KT})$ bound by \citet{NEURIPS2020_Hoeven} by a factor of $\sqrt{\K}$. 
Note that, due to differences in the target loss function, our result is not directly comparable to the $\sqrt{\K T}$ bound in \citet{NEURIPS2021_Hoeven}. A more detailed discussion can be found in \cref{app:Discussio_on_the_Difference_in_Surrogate_Losses}.
\begin{proof}
From the convexity of $S_t$ and the unbiasedness of $\gtil$, we have 
$
    \E\brk*{\sumt{\prn{\sw-\su}}}
    \leq
    \E\brk*{\sumt{\inpr{\G_t,\wt-\U}}}
    =
    \E\brk*{\sumt{\inpr{\gtil,\wt-\U}}}.
 $
From \cref{lem:ogd}, this is further upper bounded as
$
    \E\brk*{\sumt{\inpr{\gtil,\wt-\U}}}
    \leq
    \sqrt{2}B\sqrt{\E\brk*{\sumt{{\nrm{\gtil}_{\mathrm{F}}^2}}}}
    \leq
    B\sqrt{\frac{2b\K }{q}\E\brk*{\sumt{\sw}}},
$
where in the first inequality we used Jensen's inequality and 
in the last inequality we used
$
\expect{\|\gtil\|_{\mathrm{F}}^2}
=
\frac{\|\G_t\|_{\mathrm{F}}^2}{p_t(\yt)}
\leq
\frac{\K }{q}\|\G_t\|_{\mathrm{F}}^2
\leq
\frac{b\K }{q}\sw,
$
which follows from $p_t(\bm{y}) \geq K /q$ and \eqref{eq:St_smooth}.
Therefore, from \cref{asp:bandit_a}, 
we have
$
    \E\brk{\reg}
    \leq
    \E\brk*{\sumt{\prn*{(1-a)\sw-\su}}}+qT
    \leq 
    B\sqrt{\frac{2b\K }{q} \E\brk*{\sumt{\sw}}}-a \E\brk*{\sumt{\sw}}+qT
    \leq
    \frac{bB^2\K }{2aq}+qT
    ,
$
where the last inequality follows from $c_1\sqrt{x}-c_2x\leq{c_1^2}/\prn{4c_2}$ for $x \geq 0$, $c_1\geq 0$, and $c_2>0$.
Finally, substituting $q=B\sqrt{\K/T}$ into the last inequality yields the desired bound.
\end{proof}

We can also prove the following high-probability bound:
\begin{theorem}\label{thm:bandit_high_prob}
Let $\delta \in (0,1)$.
Then with probability at least $1 - \delta$, the same algorithm as in \cref{thm:bandit_regret_expectation_abstract}, but with a different choice of $q$, achieves the regret bound of 
$\mathcal{R}_T = O\prn[\Big]{\sqrt{KT \log (1/\delta)} + \log(1/\delta)}$,
where we omit the dependencies on parameters other than $K$, $T$, and $\delta$.
\end{theorem}
A more precise statement and proof of this theorem are provided in \cref{app:proof_bandit_high_prob}.
To prove this theorem, we follow the analysis of \cref{thm:bandit_regret_expectation_abstract} and use Bernstein's inequality.  
To address the challenges posed by the randomness introduced by bandit feedback,  
we adopt an approach similar to that used by \citet{NEURIPS2021_Hoeven}, and
arguably, we have successfully simplified their analysis.


\input{arxiv/subsec_bandit_self_qt}