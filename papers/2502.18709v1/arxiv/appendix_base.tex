\section{Discussion on the Difference in Surrogate Loss Functions}\label{app:Discussio_on_the_Difference_in_Surrogate_Losses}

As in \eqref{eq:sur_regret}, the surrogate regret $\reg$ is defined by $\sumt{L(\yht;\yt)}=\sumt{S(\U\xt;\yt)}+\reg$, which means the choice of the surrogate loss $S$ affects the bound on the cumulative loss $\sumt{L(\yht;\yt)}$.
\Citet[Theorem~1]{NEURIPS2021_Hoeven}, which applies to a more general setup than bandit feedback, implies $\reg = O(K\sqrt{T})$ for the bandit setup with $S$ being a logistic loss defined with the base-$K$ logarithm. 
On the other hand, our bound of $\reg = O(\sqrt{KT})$ applies to the logistic loss $S$ defined with the base-$2$ logarithm. 
As a result, while our bound on $\reg$ is better, the $\sumt{S(\U\xt;\yt)}$ term can be worse; this is why we cannot directly compare our $O(\sqrt{KT})$ bound with the $O(K\sqrt{T})$ bound in \Citet[Theorem~1]{NEURIPS2021_Hoeven}. 
We may use the decoding procedure in \citet{NEURIPS2021_Hoeven}, instead of RDUE, to recover their bound that applies to the base-$K$ logistic loss.
It should be noted that their method is specific to multiclass classification;  
naively extending their method to structured prediction formulated as $|\yy|$-class classification results in the undesirable dependence on $K = |\yy|$, as is also discussed in \citet{pmlr-v247-sakaue24a}. 
By contrast, our pseudo-inverse estimator, combined with RDUE, can rule out the explicit dependence on $K$, at the cost of the increase from $\sqrt{T}$ to $T^{2/3}$.