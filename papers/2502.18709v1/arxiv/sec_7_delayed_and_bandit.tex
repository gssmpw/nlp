\section{Delayed Bandit Feedback}\label{sec:bandit_and_delayed}
Given the results so far, it is natural to explore online structured prediction with delayed bandit feedback.
We construct algorithms for this setup by combining the theoretical developments from \cref{sec:bandit} and \cref{sec:delay}. 
This section assumes \cref{asp:bandit_a}, as in \cref{sec:bandit}.
\subsection{$O(\sqrt{D K T})$ Regret Algorithm}\label{subsec:bandit_delay_general}
We use RDUE with $q=B\sqrt{DK/T}$ as the decoding function (assuming $T\geq DB^2K$ for simplicity), the gradient estimator $\gtil$ in \eqref{eq:inverse_weighted_est},
and ODAFTRL with the AdaHedgeD update as $\alg$. 
This algorithm attains the following bound:
\begin{theorem}
    \label{thm:delay_bandit_bound_general_abstract}
    The above algorithm achieves
    $
        \E\brk{\reg} = O\prn{\sqrt{DKT}}.
    $
\end{theorem}
The proof can be found in \cref{app:bandit_delayed_general}.  
Due to the introduction of the delay, the regret bound worsens by a factor of $\sqrt{D}$ compared to the non-delayed case,
which is natural when considering analyses of adversarial cases in delayed feedback \citep{joulani13online,zimmert20optimal,manwani2022delaytronefficientlearningmulticlass}.



\subsection{$O(D^{1/3} T^{2/3})$ Regret Algorithm}\label{subsec:bandit_delay_self}
Here, we make the same assumptions on the target loss function as in \cref{subsec:Bandit_Structured_Prediction_with_SELF}.
We provide an algorithm that improves the dependence on $\K$ from \cref{subsec:bandit_delay_general}.
We use RDUE with $q=\prn*{\frac{\omega B^2 \dix^2 D}{T}}^{1/3}$ as the decoding function (assuming $T \geq \omega B^2 \dix^2 D$ for simplicity), the gradient estimator $\gtil$ in \eqref{eq:gtil_self}, and ODAFTRL with the AdaHedgeD update as $\alg$.
This algorithm achieves the following bound:
\begin{theorem}
    \label{thm:delay_bandit_bound_self_abstract}
    The above algorithm achieves
    $
        \E\brk{\reg} = O\prn{
            D^{1/3} T^{2/3}
        }.
    $
\end{theorem}
The proof can be found in \cref{app:bandit_delayed_self}.  
Due to the introduction of the delay, the regret bound worsens by a factor of $D^{1/3}$  
compared to the non-delayed bandit feedback case.

