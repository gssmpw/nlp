\section{Additional Experiments}
\label{app: experiment}

\begin{figure}[t]
    \begin{tabular}{cc}
      \begin{minipage}[t]{0.5\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.23]{comparing_regret_noise0.0_theo_B10_d2_projectno.eps}
        \subcaption{$n^{\prime}=2,p=0.0$}
        \label{fig:regret_compare_noise0d2}
      \end{minipage}
      \begin{minipage}[t]{0.5\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.23]{comparing_regret_noise0.1_theo_B10_d2_projectno.eps}
        \subcaption{$n^{\prime}=2,p=0.1$}
        \label{fig:regret_compare_noise1d2}
      \end{minipage}\\
      \begin{minipage}[t]{0.5\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.23]{comparing_regret_noise0.0_theo_B10_d4_projectno.eps}
        \subcaption{$n^{\prime}=4,p=0.0$}
        \label{fig:regret_compare_noise0d4}
      \end{minipage}
      \begin{minipage}[t]{0.5\hsize}
        \centering
        \includegraphics[keepaspectratio, scale=0.23]{comparing_regret_noise0.1_theo_B10_d4_projectno.eps}
        \subcaption{$n^{\prime}=4,p=0.1$}
        \label{fig:regret_compare_noise1d4}
      \end{minipage}
    \end{tabular}
    \caption{Results of the synthetic experiments in multiclass classification with bandit feedback.
    In all figures, the horizontal axis represents the number of classes $\K$ and the vertical axis represents the cumulative prediction loss.}
    \label{fig:regret_compare_app}
\end{figure}


This section provides the results of numerical experiments for online multiclass classification with bandit feedback on synthetic data, which could not be included in the main text due to space constraints.


\subsection{Setup}
Here, we describe the experimental setup.
As \cref{sec: experiment}, we compare three algorithms:  
Gaptron~\Citep{NEURIPS2020_Hoeven} with logistic loss,  
Gappletron~\Citep{NEURIPS2021_Hoeven} with hinge loss,  
and our algorithm in \cref{subsec:bandit_delay_general}. 
All experiments were run on a system with 16GB of RAM, Apple M3 CPU, and in Python 3.11.7 on a macOS Sonoma 14.6.1.

\paragraph{Data generation}
    We describe the procedure for generating synthetic data.
    The synthetic data was generated by using the same method as \citet{NEURIPS2021_Hoeven}.
    The input vector consists of a binary vector with entries of $0$ and $1$, and is composed of two parts.
    The first part corresponds to a unique feature vector associated with the label, and the second part is randomly selected and unrelated to the label.
    Specifically, the data are generated as follows:
    Let $n^{\prime}$, $\K \in \mathbb{N}$, $r \in [0,1]$, and $T > 0$.
    We generate $\K$ unique feature vectors of length $10n^{\prime}$ using the following as follows.
    We first randomly select an integer $s$ uniformly from the range $n^{\prime}$ to $5n^{\prime}$. 
    After that, we randomly choose $s$ elements from a zero vector of length $10n^{\prime}$ and set them to 1.
    The input vector is obtained by concatenating the feature vector of a randomly chosen class with a vector of length $30n^{\prime}$, in which exactly $5n^{\prime}$ elements are randomly set to $1$.
    Additionally, as noise, with probability $r$, the corresponding class is replaced with a randomly selected class instead of the original one.
    The length of the generated input vector $n$ is $40n^{\prime}$.
    These input vectors are generated for $T$.    
    Based on the above procedure, we create data for $n^{\prime} \in \set{2, 4}$ and $r \in \set{0.0, 0.1}$.


\paragraph{Details of algorithms}
As the algorithm $\alg$ to update the linear estimator, we employ the adaptive OGD in \cref{subsec:ogd}.
Following \citet{NEURIPS2021_Hoeven}, 
we use the learning rate of $\eta_t=\frac{B}{\sqrt{2\prn*{10^{-8}+\sum_{i=1}^{t}\nrm{\gtil}_{\mathrm{F}}^2}}}$ and no orthogonal projection in conducted in OGD.
Since $B=\diam(\ww)$ is unknown, we fixed $B = 10$ for the experiments, regardless of whether this value represents the actual diameter.
All other parameters were set according to theoretical values.
Under these parameter settings, we repeated experiments for $20$ times.



\subsection{Results}
The results on synthetic data are provided in \cref{fig:regret_compare_app}.
Our algorithm achieves comparable or better results than the comparison methods for datasets when $\K \leq 24$.
By contrast, when $\K = 48, 96$, the cumulative losses of our algorithm are larger compared to the other two algorithms, Gaptron with the hinge loss and Gappletron with the logistic loss.
Note that these experimental results are consistent with existing theoretical results since for larger values of $\K$ the upper bound on the cumulative loss of Gappletron is better than our algorithm due to differences in the surrogate loss function (see the discussion in  \cref{app:Discussio_on_the_Difference_in_Surrogate_Losses} for details).
Nevertheless, in this experiment, the advantages of structural prediction algorithms are not fully exploited, making the setup more favorable for the algorithms in previous studies.
It is also worth noting that by using the same decoding function as theirs, our approach can achieve the same order of the cumulative losses in online multiclass classification with bandit feedback.
