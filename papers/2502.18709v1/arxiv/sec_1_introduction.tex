\section{Introduction}
In many machine learning problems, given an input vector from a vector space $\xx$, we aim to predict a vector in a finite output space $\yy$.  
Multiclass classification is one of the simplest examples, while in other cases output spaces may have more complex structures. 
\emph{Structured prediction} refers to such a class of problems with structured output spaces, including multiclass classification, multilabel classification, ranking, and ordinal regression, and it has applications in various fields, ranging from natural language processing to bioinformatics \citep{JMLR_Tsochantaridis_2005, bakir_2007_article}.
In structured prediction, training models that directly predict outputs in complex discrete output spaces is typically challenging. 
Therefore, we often adopt the \emph{surrogate loss framework} \citep{Bartlett_2006}---define an intermediate space of score vectors and train models that estimate score vectors from inputs based on surrogate loss functions.
Examples of surrogate losses include squared, logistic, and hinge losses, and a general framework encompassing them is the \emph{Fenchel--Young loss} \citep{JMLR_2020_blondel}, which we rely on in this study.


Structured prediction can be naturally extended to the online setting \citep{pmlr-v247-sakaue24a}.  
In this online setting, at each round $t=1,\dots,T$, the environment selects an input-output pair $(\xt,\yt)\in\xx\times\yy$.  
A learner then predicts $\yht \in \yy$ based on the input $\xt$ and incurs a loss $L(\yht;\yt)$, where $L:\yy\times\yy\to\R_{\geq0}$ is the target loss function. 
As with \citet{pmlr-v247-sakaue24a}, we focus on the simple yet fundamental case where the learner's model for estimating score vectors is linear. 

\begin{table*}[t]
\centering
\caption{Surrogate regret upper and lower bounds in online multiclass classification and online structured prediction. Here, $T$ is the time horizon, $K$ is the size of the set, $\yy$, of output vectors, and $D$ is the fixed delayed time.  
``FI'' is the abbreviation of full information.
Delayed feedback is considered only when ``Delayed'' appears in the feedback column. 
In the target loss column, ``SELF*'' means SELF that satisfies \cref{asp:self}.
Note that the $O(T^{2/3})$ bounds for SELF* in lines 6 and 9 do not explicitly depend on $\K$ but on $d$; 
in the case of multiclass classification with the 0-1 loss, the dependence on $\K$ appears as $d = \K$.
} 
\small
\begin{tabular}{@{}l@{\hspace{1ex}}l@{\hspace{1ex}}l@{\hspace{1ex}}l@{\hspace{1ex}}l@{}}
\toprule
& Problem setup &Feedback & Target loss &  Surrogate regret bound \\ 
\midrule
 \citet[Cor.~1]{NEURIPS2021_Hoeven} & Binary classification & Bandit & 0-1 loss & $\Omega(\sqrt{T})$ ($d=2$) \\ 
 \midrule
 \citet[Thm.~4]{NEURIPS2020_Hoeven} & Multiclass classification & Bandit & 0-1 loss & $O(\K \sqrt{T})$ \\ 
 \citet[Thm.~1]{NEURIPS2021_Hoeven} & Multiclass classification & Bandit & 0-1 loss & $O(\K \sqrt{T})$ \\ 
 \midrule
\citet[Thms.~7 and 8]{pmlr-v247-sakaue24a} & Structured prediction & FI & SELF & $O(1)$  \\
\textbf{This work} (\cref{thm:bandit_regret_expectation_abstract,thm:bandit_high_prob}) & Structured prediction &Bandit & SELF &  $O(\sqrt{\K T})$  \\
\textbf{This work}  (\cref{thm:bandit_regret_pseudo_estimator}) & Structured prediction & Bandit & SELF* &  $O(T^{2/3})$ \\
\textbf{This work}  (\cref{thm:delayed_regret_expectation_abstract,thm:delayed_regret_probability_abstract}) & Structured prediction & FI \& Delayed & SELF & $O(D^{2/3}T^{1/3})$  \\
\textbf{This work}  (\cref{thm:delay_bandit_bound_general_abstract}) & Structured prediction & Bandit \& Delayed & SELF & $O(\sqrt{DKT})$  \\
\textbf{This work}  (\cref{thm:delay_bandit_bound_self_abstract}) & Structured prediction & Bandit \& Delayed & SELF* & $O(D^{1/3}T^{2/3})$  \\
\bottomrule
\end{tabular}
\label{tab: regret order}
\end{table*}


The goal of the learner is to minimize the cumulative loss $\sumt{L(\yht;\yt)}$. 
On the other hand, the best the learner can do in the surrogate loss framework is to minimize the cumulative surrogate loss, namely $\sumt{S(\U\xt;\yt)}$, where $\U:\xx\to\R^d$ is the best offline linear estimator and $S:\R^d\times\yy\to\R_{\geq0}$ is a surrogate loss, which measures the discrepancy between the score vector $\U\xt \in \R^d$ and $\yt\in\yy$. 
Given this, a natural performance measure of the learner's predictions is the following \emph{surrogate regret} $\reg$: 
\begin{equation}\label{eq:sur_regret}
    \sumt{L(\yht;\yt)}=\sumt{S(\U\xt;\yt)}+\reg.
\end{equation}
The surrogate regret was introduced in the seminal paper by \Citet{NEURIPS2020_Hoeven} in the context of online multiclass classification.  
Recently, \citet{pmlr-v247-sakaue24a} showed that a finite surrogate regret bound  
can be achieved for online structured prediction under full information feedback, i.e., the learner can observe $\yt$ at the end of each round $t$.

However, the assumption that full information feedback is available is often demanding, especially when outputs have complex structures.
%
For example, in sequential ad assortment on an advertising platform, we may be able to observe only the click-through rate but not which ads were clicked, which boils down to the \emph{bandit feedback} setting \citep{Kakade2008EfficientBA,JMLR:v15:gentile:class_ranking}. 
Also, we may only have access to feedback from a while ago when designing an ad assortment for a new userâ€”namely, \emph{delayed feedback}  \citep{Weinberger_2002_delay,manwani2022delaytronefficientlearningmulticlass}.
%
Similar situations have led to a plethora of studies in various online learning settings.
In combinatorial bandits, algorithms under bandit feedback (referred to as full-bandit feedback in their context), instead of full information feedback, have been widely studied
\citep{comband,combes15combinatorial,rejwan20topk,du21combinatorial}. 
Delayed feedback is also explored in various settings, including full information and bandit feedback \citep{joulani13online,cesabianchi16delay}.
Due to space limitations, we defer a further discussion of the background to \cref{app:additional_related_work}.


\subsection*{Our Contributions}
To extend the applicability of online structured prediction, this study develops online structured prediction algorithms that can handle weaker feedback---bandit feedback and delayed feedback---instead of full information feedback.  
As with \citet{pmlr-v247-sakaue24a}, we consider the case where target loss functions belong to a class called the Structured Encoding Loss Function (SELF) \citep{ciliberto16consistent,NEURIPS2019_Blondel}, a general class including the 0-1 loss in multiclass classification and the Hamming loss in multilabel classification and ranking (see \cref{subsec:self} for the definition). 
\Cref{tab: regret order} summarizes the surrogate regret bounds provided in this study and comparisons with existing results.


One of the challenges of bandit feedback is that the true output $\yt$ is not observed, making it impossible to compute the true gradient of the surrogate loss. 
To deal with this, we use an inverse-weighted gradient estimator, a typical approach that assigns weights to gradients by the inverse of choosing each output, establishing an $O(\sqrt{\K T})$ surrogate regret upper bounds (\cref{thm:bandit_regret_expectation_abstract,thm:bandit_high_prob}), 
where $K = \abs{\yy}$ is the cardinality of $\yy$.  
The $O(\sqrt{\K T})$ bound has an optimal dependence on $T$; it matches the $\sqrt{T}$ lower bound in the special case of online multiclass classification with bandit feedback \Citep[Corollary 1]{NEURIPS2021_Hoeven}. 
Furthermore, our bound is better than the existing $O(\K\sqrt{T})$ bound of \Citet{NEURIPS2020_Hoeven} by a factor of $\sqrt{\K}$, while it is not directly comparable to the latest $O(\K\sqrt{T})$ bound in \Citet{NEURIPS2021_Hoeven} due to differences in surrogate loss functions. 
See \cref{app:Discussio_on_the_Difference_in_Surrogate_Losses} for a more detailed discussion. 

While the $O(\sqrt{\K T})$ bound is satisfactory when $K = \abs{\yy}$ is small, $K$ can be extremely large in some structured prediction problems: in multilabel classification with $m$ correct labels, we have $\K=\binom{d}{m}$, and in ranking problems with $m$ items, we have $\K=m!$.
%
To address this issue, we consider a special case of SELF (denoted by SELF* in \cref{tab: regret order}), which still includes the aforementioned examples: the 0-1 loss in multiclass classification and the Hamming loss in multilabel classification and ranking. 
A technical challenge to resolve the issue lies in designing an appropriate gradient estimator used in online learning methods.
To this end, we draw inspiration from pseudo-inverse estimators used in the adversarial linear/combinatorial bandit literature \citep{dani07price,abernethy08competing,comband}. 
While we cannot naively apply the existing estimators, we design a new gradient estimator that applies to various specific structured prediction problems belonging to the special SELF framework.
Armed with this gradient estimator, we achieve a surrogate regret upper bound of $O(T^{2/3})$, which does not explicitly depend on~$\K$~(\cref{thm:bandit_regret_pseudo_estimator}).

For the delayed feedback setting with a known fixed delay time of $D$, it is actually not difficult to obtain a surrogate regret bound of $O(\sqrt{D T})$ with standard Online Convex Optimization (OCO) algorithms for delayed feedback. 
Our finding is that we can achieve a surrogate regret bound of $O(D^{2/3} T^{1/3})$ in online structured prediction under delayed full information feedback (\cref{thm:delayed_regret_expectation_abstract,thm:delayed_regret_probability_abstract}) by leveraging ODAFTRL \citep{pmlr-v139-flaspohler21a}, a Follow-the-Regularized-Leader-type algorithm that achieves an AdaGrad-type regret upper bound in OCO under delayed feedback. 
This bound is better than $O(\sqrt{D T})$ as $D \le T$.


Given the contributions so far, it is natural to explore online structured prediction in environments where both delay and bandit feedback are present. 
We obtain algorithms for this setup by combining the theoretical developments for bandit feedback without delay and delayed full information feedback, offering surrogate regret bounds of $O(\sqrt{D K T})$ (\cref{thm:delay_bandit_bound_general_abstract}) and $O(D^{1/3} T^{2/3})$ (\cref{thm:delay_bandit_bound_self_abstract}).

We validate our algorithms through numerical experiments using both synthetic and real-world data.  
Specifically, we consider online multiclass classification with bandit feedback. 
We observe that, depending on the number of classes and the dataset, our algorithm, designed for general structured prediction, can achieve accuracy comparable to existing algorithms specialized for multiclass classification.


