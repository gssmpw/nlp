\section{Preliminaries}
We describe the detailed setup of online structured prediction and key tools used in this work: the Fenchel--Young loss, SELF, and randomized decoding.
\paragraph{Notation}
For any integer $n > 0$, let $\brk{n} = \set{1,2,\hdots,n}$.
Let $\nrm{\cdot}$ denote a norm with $\kappa\nrm{\bmy}\geq\nrm{\bmy}_2$ for some $\kappa>0$ for any $\bmy\in\mathbb{R}^d$. 
For a matrix $\W$, let $\nrm{\W}_{\mathrm{F}}=\sqrt{\tr\prn*{\W^\top \W}}$ be the Frobenius norm. 
Let $\bm1$ denote the all-ones vector and $\bm{\ee}_i$ the $i$th standard basis vector.
For $\mathcal{\mathcal{K}}\subset \mathbb{R}^d$, let $\conv(\mathcal{K})$ be its convex hull and $I_{\mathcal{K}}:\mathbb{R}^d\to\set{0, +\infty}$ be its indicator function, which takes zero if $\bmy \in \mathcal{K}$ and $+\infty$ otherwise.
For $\Omega:\mathbb{R}^d\to\mathbb{R}\cup\set{+\infty}$, let $\dom(\Omega) \coloneqq \set*{\bmy \in \mathbb{R}^d:\Omega(\bmy) < +\infty}$ be its effective domain and $\Omega^*(\thb) \coloneqq \sup\set*{\inpr{\thb, \bmy} - \Omega(\bmy):\bmy \in \mathbb{R}^d}$ be its convex conjugate.
\cref{tab: notation} in \Cref{app: notation} summarizes the notation used in this paper.


\subsection{Online Structured Prediction}
Here, we describe the problem setting of online structured prediction.
Let $\xx$ be the input vector space and $\yy$ be the output vector space.
Define $\K \coloneqq |\yy|$.
Following \citet{JMLR_2020_blondel} and \citet{pmlr-v247-sakaue24a}, we assume that $\yy$ is embedded into $\mathbb{R}^d$ in a standard manner.
For example, $\yy = \set{\bm{\mathrm{e}}_1,\hdots,\bm{\mathrm{e}}_d}$ in $d$-class multiclass classification.


A linear estimator $\W\in\ww$ for a convex domain $\ww$ is used to transform the input vector $\bm{x}$ into the score vector $\W\bm{x}$.  
In online structured prediction, at each round $t=1,\dots,T$:
\begin{enumerate}%[topsep=2pt,itemsep=0pt, partopsep=0pt, leftmargin=18pt]
    \item The environment selects an input $\xt\in\xx$ and the true output $\yt\in\yy$; 
    \item The learner receives $\xt$ and computes the score vector~$\tht=\wt\xt$ using the linear estimator~$\wt$;
    \item The learner selects a predicted output $\yht$ based on $\tht$ and incurs a loss of $L(\yht;\yt)$;
    \item The learner receives feedback based on the problem setup and updates $\wt$ to $\W_{t+1}$ using an online learning algorithm, $\alg$.
\end{enumerate}
The goal of the learner is to minimize the cumulative prediction loss $\sumt{L(\yht;\yt)}$, which is equivalent to minimizing the surrogate regret $\mathcal{R}_T$ in \eqref{eq:sur_regret}.  
We assume that the input and output are generated in an oblivious manner.  
Note that when $\yy = \set{\bm{\ee}_1, \dots, \bm{\ee}_d}$ and $L(\yht; \yt) = \ind[\yht \neq \yt]$, the above setting reduces to online multiclass classification, which was studied by \Citet{NEURIPS2020_Hoeven} and \Citet{NEURIPS2021_Hoeven}.
We will use $B\coloneqq\diam(\ww)$, $\dix\coloneqq\diam(\xx)$, and $\diy\coloneqq\diam(\yy)$ to denote the diameters of the sets $\ww$, $\xx$, and $\yy$, respectively.


The feedback observed by the learner depends on the problem setting.  
The most fundamental setting is the full information setup, where the true output $\yt$ is observed as feedback at the end of each round $t$. 
This setup was extensively investigated in \citet{pmlr-v247-sakaue24a}.  
By contrast, our study investigates the following weaker feedback:
\begin{itemize}%[topsep=2pt,itemsep=0pt, partopsep=0pt, leftmargin=18pt]
    \item \textbf{Bandit feedback}: Only the value of the loss function is observed. 
    Specifically, at the end of each round $t$, the learner observes the target loss value $L(\yht;\yt)$ as feedback.  
    \item \textbf{Delayed feedback}: The feedback is observed with a certain delay. 
    We consider a fixed $D$-round delay setting, i.e., no feedback is received for round $t\leq D$,  
    and for $t>D$, the learner observes either full information feedback $\bmy_{t-D}$ or bandit feedback $L(\hat{\bm{y}}_{t-D}; \bm{y}_{t-D})$.  
\end{itemize}


In this paper, we make the following assumptions:
\begin{assumption}
    \label{asp:online_structured_prediction}
    (1)~There exists $\nu>0$ such that for any distinct $\bm{y},\bm{y}^\prime\in \mathcal{Y}$, it holds that $\|\bm{y}-\bm{y}^\prime\|\geq\nu$.  
    (2) For each $\bm{y}\in\mathcal{Y}$, the target loss function $L(\cdot;\bm{y})$ is defined on $\conv(\mathcal{Y})$, is non-negative, and is affine with respect to its first argument.  
    (3) There exists $\gamma$ such that for any $\bm{y}^\prime\in\conv(\mathcal{Y})$ and $\bm{y}\in\mathcal{Y}$, it holds that $L(\bm{y}^\prime;\bm{y})\leq\gamma\|\bm{y}^\prime-\bm{y}\|$ and $L(\bmy^{\prime};\bmy)\leq 1$. 
    (4) It holds that $L(\bm{y}'; \bm{y})=0$ only if $\bm{y}'=\bm{y}$.
\end{assumption}

As discussed in \citet[Section 2.3]{pmlr-v247-sakaue24a}, these assumptions are natural and hold for a broad range of problem settings and target loss functions, including SELF (see \cref{subsec:self} for the formal definition).


\subsection{Fenchel--Young Loss}\label{subsec:fenchel-young}

We use the Fenchel--Young loss \citep{JMLR_2020_blondel} as the surrogate loss, which subsumes many representative surrogate losses, such as logistic loss, Conditional Random Field (CRF) loss \citep{lafferty01conditional}, and SparseMAP \citep{Niculae18sparse}.
See \citet[Table 1]{JMLR_2020_blondel} for more examples. 
\begin{definition}[{\citealt[Fenchel--Young loss]{JMLR_2020_blondel}}]
    \label{def: Fenchel--Young Loss}
    Let $\Omega:\mathbb{R}^d\rightarrow\mathbb{R}\cup\set{+\infty}$ be a regularization function with $\mathcal{Y}\subset\operatorname{dom}(\Omega)$.  
    The Fenchel--Young loss generated by $\Omega$, denoted by $S_\Omega:\operatorname{dom}(\Omega^\ast)\times\operatorname{dom}(\Omega)\rightarrow\mathbb{R}_{\geq0}$, is defined as
    \[
    S_{\Omega}(\thb;\bmy)\coloneqq\Omega^\ast(\thb)+\Omega(\bmy)-\inpr{\thb,\bmy}.
    \]
\end{definition}
The Fenchel--Young loss has the following properties, which will be useful in the subsequent discussion:
\begin{proposition}[{\citealt[Propositions 2 and~3]{JMLR_2020_blondel} and \citealt[Proposition 3]{pmlr-v247-sakaue24a}}]
    \label{prop:fenchel}
    Let $\Psi:\mathbb{R}^d\rightarrow\mathbb{R}\cup\set{+\infty}$ be a differentiable, Legendre-type function\footnote{
    A function is called Legendre-type if, for any sequence $x_1,x_2,\hdots$ in $\operatorname{int}(\dom(\Psi))$ that converges to a boundary point of $\operatorname{int}(\dom(\Psi))$, it holds that $\lim_{i\rightarrow\infty}\|\nabla\Psi(x_i)\|_2=+\infty$.}  
    that is $\lambda$-strongly convex with respect to $\|\cdot\|$, and suppose that $\conv(\mathcal{Y})\subset\dom(\Psi)$ and $\dom(\Psi^\ast)=\mathbb{R}^d$.  
    Define $\Omega=\Psi+I_{\conv(\yy)}$ and let $S_\Omega$ be the Fenchel--Young loss generated by $\Omega$.  
    For any $\thb\in\mathbb{R}^d$, we define the regularized prediction function as 
    \begin{align*}
        \yho(\thb)&\coloneqq\argmax\{\inpr{\thb,\bmy}-\Omega(\bmy)\::\:\bmy\in\mathbb{R}^d\}\\
        &=\argmax\set{\inpr{\thb,\bmy}-\Psi(\bmy)\::\:\bmy\in\conv(\mathcal{Y})}.
    \end{align*}
    Then, for any $\bmy\in\mathcal{Y}$, $S_\Omega(\thb,\bmy)$ is differentiable with respect to $\thb$, and it satisfies  
    $
    \nabla S_\Omega(\thb;\bmy)=\yho(\thb)-\bmy.
    $
    Furthermore, it holds that  
    $
    S_\Omega(\thb;\bmy)\geq\frac{\lambda}{2}\|\bmy-\yho(\thb)\|^2.
    $ 
\end{proposition}


In what follows, let $S_t(\W)\coloneqq S_\Omega(\W\xt;\yt)$ for simplicity.
Importantly, from the properties of the Fenchel--Young loss, there exists some $b>0$ such that for any $\W\in\ww$,  
\begin{equation}\label{eq:St_smooth}
    \nrm{\nabla S_t (\W)}_{\mathrm{F}}^2\leq b S_t(\W).
\end{equation}
Indeed, from \cref{prop:fenchel} and \cref{asp:online_structured_prediction}, we have 
$
\nrm{\nabla S_t(\W_t)}_\F^2
=
\nrm{\yho(\tht)-\yt}_2^2 \nrm{\xt}^2
\leq
\dix^2\kappa^2\nrm{\yho(\tht)-\yt}^2
\leq
\frac{2\dix^2\kappa^2}{\lambda}\sw
$, 
where we used $\nabla S_t(\wt) =  \prn{\hat{\bm{y}}_\Omega(\bm{\theta}_t) - \yt}\xt^\top$ and $\nrm{\cdot}_2 \leq \kappa \nrm{\cdot}$. 
Thus, \eqref{eq:St_smooth} holds with  $b=\frac{2\dix^2\kappa^2}{\lambda}$.
Below, let $L_t(\bmy)\coloneqq L(\bmy;\yt)$,
$\G_t\coloneqq\nabla S_t(\wt) =  \prn{\hat{\bm{y}}_\Omega(\bm{\theta}_t) - \yt} \bm{x}_t^\top$, and $\L\coloneqq\max_{\W\in\ww}\nrm{\nabla S_t(\W)}_\F$.


\subsection{Examples of Structured Prediction}\label{subsec:pre_examples} 
We present several special cases of structured prediction  
along with specific parameter values introduced so far; see \citet[Section 2.3]{pmlr-v247-sakaue24a} for further details.
\paragraph{Multiclass classification}
Let $\yy=\set{\mathbf{e}_1,\dots,\mathbf{e}_d}$ and $\|\cdot\| = \|\cdot\|_1$.
When using the 0-1 loss, $L(\bmy^{\prime};\bmy)=\ind\brk{\bmy^{\prime}\neq\bmy}$, the parameters in \cref{asp:online_structured_prediction} are $\nu=2$ and $\gamma=\frac{1}{2}$.
The logistic surrogate loss is a Fenchel--Young loss $S_\Omega$ generated by the entropy regularization function $\Omega=\mathsf{H}^s+I_{\Delta_d}$ (up to a constant factor), where $\mathsf{H}^s(\bmy)\coloneqq-\sum_{i=1}^d y_i\log y_i$ and $\Delta_d$ is the $(d-1)$-dimensional probability simplex.
Since $\Omega$ is a $1$-strongly convex function with respect to $\|\cdot\|_1$, we have $\lambda=1$.

\paragraph{Multilabel classification}
Let $\yy=\set{0,1}^d$ and $\|\cdot\| = \|\cdot\|_2$.
When using the Hamming loss as the target loss function $L(\bmy^{\prime};\bmy)=\frac{1}{d}\sum_{i=1}^{d}\ind\brk{y_{i}^{\prime}\neq y_i}$, \cref{asp:online_structured_prediction} is satisfied with $\nu=1$ and $\gamma=\frac{1}{d}$.
The SparseMAP surrogate loss $S_\Omega(\thb,\bmy)=\frac{1}{2}\nrm{\bmy-\thb}_2^2-\frac{1}{2}\nrm{\yho(\thb)-\thb}_2^2$ is a Fenchel--Young loss generated by $\Omega=\frac{1}{2}\nrm{\cdot}^2+I_{\conv(\yy)}$.
Since $\Omega$ is 1-strongly convex with respect to $\|\cdot\|_2$, we have $\lambda=1$.

\paragraph{Ranking}
We consider predicting the ranking of $m$ items. 
Let $\nrm{\cdot} = \nrm{\cdot}_1$, $d=m^2$, and $\yy\subset\set{0,1}^d$ be the set of all vectors representing $m \times m$ permutation matrices.  
We use the target loss function that counts mismatches, $L(\bmy^{\prime};\bmy)=\frac{1}{m}\sum_{i=1}^{m}\ind\brk{y_{i,j_i}^{\prime}\neq y_{i,j_i}}$, where $j_i\in[m]$ is a unique index with $y_{ij_i}=1$ for each $i\in[m]$.  
In this case, the parameters in \cref{asp:online_structured_prediction} satisfy $\nu=4$ and $\gamma=\frac{1}{2m}$.  
We use a surrogate loss given by $S_\Omega(\thb;\bmy)=\inpr{\thb,\yho(\thb)-\bmy}+\frac{1}{\zeta}\mathsf{H}^s(\yho(\thb))$, where $\Omega=-\frac{1}{\zeta}\mathsf{H}^s+I_{\conv(\yy)}$ and $\zeta$ is a parameter controlling the regularization strength. 
The first term in $S_\Omega$ measures the affinity between $\thb$ and $\bmy$, while the second term evaluates the uncertainty of $\yho(\thb)$.  
Since $\Omega$ is $\frac{1}{m\zeta}$-strongly convex, we have $\lambda=\frac{1}{m\zeta}$.


\subsection{Structured Encoding Loss Function (SELF)}\label{subsec:self}
Here we introduce a common class of target loss functions, called the Structured Encoding Loss Function (SELF).
A target loss function is SELF if it can be expressed as  
\begin{equation}\label{eq:self}
    L(\yt; \yht)=\inpr{\yht,\V\yt+\bm{b}}+c(\yt),
\end{equation}
where $\bm{b} \in \R^d$ is a constant vector, $\V\in\R^{d\times d}$ is a constant matrix, and $c:\yy\to\R$ is a function.  
The following loss examples, taken from \citet[Appendix A]{NEURIPS2019_Blondel}, belong to the SELF class: 
\begin{itemize}%[topsep=2pt,itemsep=0pt, partopsep=0pt, leftmargin=18pt]
\item Multiclass classification: the 0-1 loss is a SELF with $\V=\bm{1}\bm{1}^\top-\I$, $\bb=\bm{0}$, and $c(\bmy)=0$.  

\item Multilabel classification: the Hamming loss, 
$L(\bmy^{\prime};\bmy)=\frac{1}{d}\sum_{i=1}^{d}\ind\brk{y^{\prime}_{t,i}\neq y_{i}}$, is a SELF with $\V=-\frac{2}{d}\I$, $\bb=\frac{\bm{1}}{d}$, and $c(\bmy)=\frac{1}{d}\inpr{\bmy,\bm{1}}$, where the last factor is constant if the number of correct labels is fixed.

\item Ranking: the Hamming loss  
$L(\bmy^{\prime};\bmy)=\frac{1}{m}\sum_{i=1}^{m}\ind\brk{y_{i,j_i}^{\prime}\neq y_{i,j_{i}}}$, where $j_i\in[m]$ is a unique index with $y_{i,j_i}=1$ for each $i\in[m]$, is a SELF with  
$\V=-\frac{1}{m}\I$, $\bb=\bm{0}$, and $c(\bmy)=1$.
\end{itemize}
Following \citet{pmlr-v247-sakaue24a}, this study assumes that the target loss function $L$ is a SELF.

\subsection{Randomized Decoding}\label{subsec:randomized_decoding}
\begin{algorithm}[t]
    \caption{Randomized decoding $\phi_\Omega$}
    \label{ALG: randomized decoding}
    \begin{algorithmic}[1]
        \Require {$\btheta\in\mathbb{R}^d$}
            \State {$\yho(\btheta)\leftarrow\argmax\{\langle\btheta,\bm{y}\rangle-\Psi(\bm{y})\::\:\bm{y}\in\conv(\mathcal{Y})\}$}
            \State {$\bm{y}^\ast\leftarrow\argmin\{\|\bm{y}-\yho(\btheta)\|\::\:\bm{y}\in\mathcal{Y}\}$}
            \State {$\Delta^\ast\leftarrow\|\bm{y}^\ast-\yho(\btheta)\|,\:p\leftarrow\min\{1,2\Delta^\ast/\nu\}$}
            \State {Sample $Z \sim \mathrm{Ber}(p)$}
            \LineIf{$Z=0$}{$\yh\leftarrow\bm{y}^\ast$}
            \LineIf{$Z=1$}{$\yh\leftarrow\bm{\tilde{y}}$ where $\bm{\tilde{y}}$ is randomly drawn from $\yy$ so that $\E\brk*{\bm{\tilde{y}}|Z=1}=\yho(\btheta)$}
            \Ensure{$\phi_\Omega(\thb)=\yh$}
    \end{algorithmic}
\end{algorithm}


The procedure of converting the estimated score $\thb$ into a structured output $\bm{\hat{y}}$ is called decoding.  
For this, we employ randomized decoding \citep{pmlr-v247-sakaue24a},  
which plays an essential role particularly in deriving an upper bound independent of the output set size $K = \abs{\mathcal{Y}}$ in \cref{subsec:Bandit_Structured_Prediction_with_SELF}.
The randomized decoding (\cref{ALG: randomized decoding}) selects either the closest $\bm{y}^* \in \yy$ to $\hat{\bm{y}}_\Omega(\thb) \in \conv(\yy)$ or a random $\widetilde{\bm{y}} \in \yy$ satisfying $\E[\widetilde{\bm{y}} \mid Z=1] = \hat{\bm{y}}_\Omega(\thb)$, where $Z$ follows a Bernoulli distribution with a parameter $p$.  
Intuitively, the parameter $p$ is chosen so that if $\hat{\bm{y}}_\Omega(\thb)$ is close to $\bm{y}^*$, the decoding is more likely to return $\bm{y}^*$; otherwise, it is more likely to return $\widetilde{\bm{y}}$, reflecting uncertainty.  
An important property satisfied by the randomized decoding is the following lemma, which we use in the subsequent analysis:
\begin{lemma}[{\citealt[Lemma 4]{pmlr-v247-sakaue24a}}]
    \label{lem:expected_target_bound}
  For any $(\thb, \bmy) \in \mathbb{R}^d\times\yy$, the randomized decoding $\phi_\Omega$ satisfies
  \[
    \E[L(\phi_\Omega(\thb);\bmy)] \leq \frac{4\gamma}{\lambda\nu} S_\Omega(\thb;\bmy),
  \]
  where the expectation is taken with respect to the internal randomness of the randomized decoding.
\end{lemma}








