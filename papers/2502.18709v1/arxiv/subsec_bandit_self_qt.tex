\subsection{$O(T^{2/3})$ Regret Algorithm}
\label{subsec:Bandit_Structured_Prediction_with_SELF}
While the $O(\sqrt{KT})$ regret bound given above is desirable in terms of the dependence on $T$, the dependence on $K = \abs{\yy}$ is undesirable for general structured prediction.  
In fact, we have $\K=\binom{d}{m}$ in multilabel classification with $m$ correct labels and $\K=m!$ in ranking with $m$ items.  
To address this issue, we present an algorithm that significantly improves the dependence on $\K$ when the target loss function belongs to a special class of SELF satisfying the following assumptions:\looseness=-1
\begin{assumption}\label{asp:self}
(i) $\V$ is invertible, and $\bm{b}$ and $c(\cdot)$ are known and non-negative.
(ii) Let $\bm{Q} = \E_{\bm{y} \sim \mu} \brk{ \bm{y} \bm{y}^\top }$, where $\mu$ is the uniform distribution over $\yy$. At least one of the following two conditions holds: 
(ii-a) $\bm{Q}$ is invertible, or 
(ii-b) for any $\bm{y} \in \yy$, $\V \bm{y}$ lies in the linear subspace spanned by vectors in $\yy$. 
(iii) For some $\omega > 0$, it holds that\looseness=-1
\begin{equation}\label{eq:def_omega}
    \tr \prn*{ \V^{-1} \bm{Q}^+ \prn{\V^{-1}}^\top } \leq \omega.
    \nonumber
\end{equation} 
\end{assumption}
The first condition is true in the examples in \cref{subsec:self}, assuming that the number of correct labels is fixed in multilabel classification.
The second one is satisfied if $\yy$ consists of $d$ linearly independent vectors or $\V$ is proportional to the identity matrix; either is true in those examples. 
It is also not difficult to derive bounds on $\omega$ in those examples.\looseness=-1

\paragraph{Algorithm based on pseudo-inverse matrix estimator}
As in the case of \cref{subsec:Bandit_Structured_Prediction_with_General_Losses}, we begin by providing a method to estimate the gradient.  
Define $\pt \coloneqq\E_{\bmy\sim p_t}[\bmy\bmy^\top]$.
Then, we define the estimator $\ytilde$ of $\yt$ by
\[
    \ytilde\coloneqq\inverse{\V}\bm{P}_t^+\yht\inpr{\yht,\V\yt},
\]
where $\bm{P}_t^+$ is the Moore--Penrose pseudo-inverse matrix of $\bm{P}_t$.
It is important to note that, given that $\bm{b}$ and $c(\cdot)$ are known,  
$
\inpr{\yht,\V\yt}=L_t(\yht)-\inpr{\yht,\bm{b}}-c(\yt)
$
can be computed.  
Note that $\ytilde$ is unbiased, i.e.,
$
\expect{\ytilde}=\yt
$
from the first requirement of \cref{asp:self}.

Using this $\ytilde$, we define the gradient estimator $\gtil$ by
\begin{equation}
    \label{eq:gtil_self}
    \gtil\coloneqq\prn*{\yho(\tht)-\ytilde}\xt^\top,
\end{equation}
whose expectation is 
$
    \E\brk[\big]{\gtil}=\G_t.
$
Our estimator is based on the estimators used in adversarial linear bandits and adversarial combinatorial full-bandits \citep{dani07price,abernethy08competing,comband}.  

We use RDUE with $q=\prn*{\frac{4 \omega B^2\dix ^2}{ T }}^{1/3}$ as the decoding function  
(assuming $T \geq 4 \omega B^2\dix ^2$ for simplicity).  
For updating $\wt$, we employ the adaptive OGD in \cref{subsec:ogd} as $\alg$ with the learning rate of 
$
\eta_t=\frac{B}{\sqrt{2 \sum_{i=1}^t\nrm{\tilde{\G}_i}_{\mathrm{F}}^2}}.
$


\paragraph{Regret bounds}
The above algorithm achieves the following regret bound,  
which does not directly depend on $\K$:
\begin{theorem}
    \label{thm:bandit_regret_pseudo_estimator}
    The above algorithm achieves
    $
    \E\brk{\reg}
    \leq
    \frac{bB^2}{a}
    +
    O\prn[\big]{ \omega^{1/3} \prn*{ B \dix T}^{2/3} }.
    $
\end{theorem}
The proof can be found in \cref{app:sub_bandit_regret_pseudo_estimator}.  
By using the estimator based on the pseudo-inverse matrix, 
we can upper bound the second moment of the gradient estimator $\gtil$ without $\K$, which allows us to establish the improved regret bound that does not explicitly depend on $\K$.  

The regret bound in \cref{thm:bandit_regret_pseudo_estimator} yields the different bounds on each problem setup as follows:
\begin{corollary}\label{cor:thm_self}
The above algorithm achieves
$
    \E\brk{\reg}\leq\frac{bB^2}{a}+ O\prn*{ \prn{B \dix  d T}^{2/3}}
$
in multiclass classification with the 0-1 loss,
$
    \E\brk{\reg}\leq\frac{bB^2}{a}
    +
    O\prn*{ \prn{d^5 /m(d-m)}^{1/3} \prn{B \dix T}^{2/3}}
$
in multilabel classification with $m$ correct labels and the Hamming loss,
and 
$
    \E\brk{\reg}\leq\frac{bB^2}{a}+ O \prn*{ m^{5/3}\prn{B\dix T}^{2/3}}
$
in ranking with the number of items $m$ and the Hamming loss.
\end{corollary}
The proof of \cref{cor:thm_self} is deferred to \cref{app:SELF_upper_discussion_deferred}.
The bound for multilabel classification with $m$ correct labels significantly improved the $O(\sqrt{\K T})$ bound in \cref{subsec:Bandit_Structured_Prediction_with_General_Losses},  
which has a dependency of $\sqrt{\binom{d}{m}}$,
and
the bound for ranking significantly improved the $O(\sqrt{\K T})$ bound in \cref{subsec:Bandit_Structured_Prediction_with_General_Losses}, which has a dependency of $\sqrt{m!}$.\looseness=-1
