%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Related Work}

% Describe the other's work in the field, with the following purposes in mind: 

% \begin{itemize}
%  \item \textit{Is the overview concise?} Give an overview of the most relevant work to the needed extent. Make sure the reader can understand your work without referring to other literature.
%  \item \textit{Does the compilation of work help to define the ``niche'' you are working in?} Another purpose of this section is to lay the groundwork for showing that you did significant work. The selection and presentation of the related work should enable you to name the implications, differences and similarities sufficiently in the ``discussion'' section.
% \end{itemize}


Our work aims to create realistic nighttime images for autonomous driving based on single-view daytime images, it is closely related to novel view synthesis, day-to-night transformation and nighttime driving scene understanding. In this section, we will present an overview of the most relevant works and their limitations.

\section{Novel View Synthesis}
View synthesis is a fundamental task in computer vision that involves generating new images of a scene from different viewpoints. Early works focused on geometric reconstruction usually combine Structure-from-Motion (SfM) and Multi-View Stereo (MVS) that rely on sparse feature matching and depth estimation. However, these methods require multiple viewpoints of a single scene and cannot handle complex scenes~\cite{2006_MVS}. The recent influential Neural Radiance Fields (NeRF) technique~\cite{mildenhall2020nerf} shows strong ability in novel view synthesis and is capable for both indoor and outdoor scenes~\cite{mine2021iccv, lyu2022nrtf, srinivasan2021cvpr, rudnev2022nerfosr} or objects~\cite{nerv2021}. Different from previous works that leverage geometry information, it often designs as a multi-layer perceptron (MLP)~\cite{popescu2009mlp} that maps 3D coordinates to radiance and density values. NeRF learns to model the radiance field by minimizing the difference between the synthesized images and the actual training image during training, then renders different views of the scene at test time. Though being powerful and reliable in view synthesis, NeRF suffers from its high computational cost and limited generalizability. Many other works also explored view synthesis using generative networks, such as Variational Autoencoders (VAEs)~\cite{kingma2014vae}, Generative Adversarial Networks (GANs)~\cite{goodfellow2020gan} and Diffusion Models~\cite{ho2020denoising}. They have shown remarkable abilities in generating realistic images, but their lack of 3D understanding makes it hard to capture the underlying geometry and thus may generate artifacts in novel views.

\section{Day-to-Night Transformation}
Day-to-night transformation is a challenging task that aims to convert images captured during the day into realistic nighttime representations. This process relies on scene relighting, a core task in computer graphics and computer vision that involves modifying the lighting conditions and then rendering the original scene under new conditions. Many previous works have explored this with different methodologies. \cite{li2020inverse} learns inverse rendering from a single image, estimating the geometry and materials of the scene and spatially-varying illumination. \cite{Yang_2023_CVPR} proposed to complement the intrinsic estimation from volume rendering using NeRF and from inversing the photometric image formation model using convolutional neural networks (CNNs) for outdoor scene relighting. Differently, \cite{zhang2022simbar} leveraging explicit geometric representations from a single image by estimating depth information using an external network to perform scene relighting. All these methods have achieved remarkable performance in scene relighting. Nevertheless, those methods only handle daytime images for both input and output, neglecting the impact of internal light sources. As a result, they are inadequate for accomplishing effective day-to-night transformation. For day-to-night transformation, most works utilized generative methods, such as CycleGAN~\cite{CycleGAN2017}, pix2pix~\cite{pix2pix2017} and EnlightenGAN~\cite{jiang2021enlightengan}. Such purely data-driven approaches cannot accurately render spatially varying illumination, especially at night. Furthermore, although these methods sometimes do succeed in turning inactive light sources (e.g. street lights or windows) from off to on, the lights they produce are not accurate and realistic. Relighting daytime images to nighttime is also addressed in~\cite{Punnappurath_2022_CVPR}, which did not consider 3D geometry or materials and thus cannot model the interaction of light with the scene at night time. Moreover, nighttime-activated light sources are modelled in 2D instead of 3D, which leads to unrealistic illumination in the output image.

\section{Nighttime Driving Scene Understanding}
\label{sec:related_dataset}
Parsing and understanding the driving scene is a crucial ability for autonomous driving cars. Semantic segmentation has developed rapidly over the past few years and achieved remarkable progress. However, comprehension of nighttime driving scenes is still in its early stages, mainly due to the significant domain gap between daytime and nighttime scenes. Some works performed domain adaptation to close this gap.~\cite{Lengyel_2021_ICCV} Utilized a physics-based prior for domain adaptation, aiming to minimize the distribution shift between daytime and nighttime neural network feature maps.~\cite{2020_fda} then relied on the pixel-level adaptation via explicit transforms from source to target. An alternative method is to train traditional segmentation models on nighttime driving datasets, however, this requires annotated nighttime images which are hard to obtain. Though many datasets such as the Oxford RobotCar dataset and the BDD100K dataset have been including nighttime images~\cite{bdd100k, RCDRTKArXiv}, there has been a lack of emphasis on nighttime scene comprehension. As a result, these datasets do not offer adequate resources for training an effective model on nighttime image segmentation. A recently proposed autonomous driving dataset ACDC focused specifically on adverse conditions, contains 4006 images that are evenly distributed across four weather conditions: rain, fog, snow and night~\cite{SDV21ACDC}. Each image comes with a pixel-level semantic annotation and a reference image that is taken at the same location under normal conditions (clear daytime). Though the ACDC dataset puts a larger emphasis on nighttime (it includes 1006 nighttime images, with 400 from the training set, 106 from the validation set and 500 from the testing set), the gap still remains due to the shortage of annotated nighttime images caused by the difficulties of manual annotation.

\vspace{1cm}
\noindent Different from all methods discussed above, our method targets the generation of realistic nighttime images through simulation based on images from daytime datasets. In our image simulation pipeline, we utilize geometric information to reconstruct scene mesh and consider real-world light sources during relighting. As shown in the remaining sections of the paper, our work has the potential to close the gap in nighttime driving scene understanding.
