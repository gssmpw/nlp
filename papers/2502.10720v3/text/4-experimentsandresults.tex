%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Results and Experiment Plans}
\vspace{-3mm}
% Describe the evaluation you did in a way, such that an independent researcher can repeat it. Cover the following questions:
% \begin{itemize}
%  \item \textit{What is the experimental setup and methodology?} Describe the setting of the experiments and give all the parameters in detail which you have used. Give a detailed account of how the experiment was conducted.
%  \item \textit{What are your results?} In this section, a \emph{clear description} of the results is given. If you produced lots of data, include only representative data here and put all results into the appendix. 
% \end{itemize}

% \shutong{What to include here, maybe add some future experimental plans?}

% \mypara{Ideas:} \\
% 1. Qualitative comparison on reconstructed mesh and corresponding surface normal. \\
% 2. Qualitative comparison on nighttime images. \\
% 3. Retrained model performance.
% 4. Show some results from other datasets, our method not only work for ACDC dataset. 

In this section, we will apply our pipeline to images within the ACDC dataset and present the results of the Geometry Mesh Reconstruction component. Additionally, we will demonstrate the generalizability of our pipeline by applying it to other autonomous driving datasets. such as the Cityscapes dataset~\cite{Cordts2016Cityscapes}. Furthermore, we will outline our testing plans for the entire pipeline.
\vspace{-2mm}
\section{Geometry Mesh Reconstruction}
\subsection{Datasets and Metrics}
\label{sec:dataset}
\mypara{Datasets} In this work, we used images in the ACDC dataset~\cite{SDV21ACDC}, which is a large-scale dataset consisting of 4006 images evenly distributed across four different adverse conditions: snow, fog, rain and night. Each adverse condition image comes with a high-quality fine pixel-level semantic annotation and a reference image of the same scene taken under normal conditions (clear daytime). Our work mainly focused on applying our pipeline to the reference split of the ACDC dataset. Except for the ACDC dataset, we also use other autonomous driving datasets, including the BDD100K dataset~\cite{bdd100k}, Cityscapes dataset~\cite{Cordts2016Cityscapes} and the Dark Zurich dataset~\cite{SDV19}. In particular, we use daytime images and its corresponding semantic annotations for all selected datasets, showing that our pipeline has a strong adaptability.

\mypara{Metrics} The Geometry Mesh Reconstruction component generates scene mesh and geometric information such as depth and surface normal. However, none of the datasets mentioned above provide ground truth for evaluation. Thus, we conduct a qualitative comparison of the generated mesh and its surface normals to evaluate the Geometry Mesh Reconstruction component of our pipeline.

\subsection{Mesh Comparison}
In order to evaluate the qualitative result of our reconstructed mesh, we compare our mesh reconstruction result with results from the following settings: \textbf{(1)} mesh constructed by the Worldsheet~\cite{hu2021worldsheet} using MiDaS v2.1~\cite{Ranftl2021} as the external depth backbone based on input RGB. \textbf{(2)} mesh constructed by the Worldsheet using iDisc~\cite{piccinelli2023idisc} as the external depth backbone. \textbf{(3)} mesh reconstructed by method presented in the SIMBAR~\cite{zhang2022simbar} using Dense Prediction Transformer (DPT) monodepth models~\cite{Ranftl2022} as the external depth backbone, shown as~\myfigref{fig:mesh_comparison}. Moreover, we also compare surface normals of the generated mesh in~\myfigref{fig:normal_comparison}, further showing that our method can estimate most of the geometric characteristics and reconstruct better mesh. Noted that for surface normal, we treated estimation of iDisc~\cite{piccinelli2023idisc} as the ground truth. To show the effect of the mesh post-processing kernel, we compared our reconstructed scene mesh with  \textbf{(2)} and \textbf{(3)} from different viewing angles, as shown in~\myfigref{fig:mesh_angle}. We also present the optimization loss curve in~\myfigref{fig:loss_curve}. More qualitative results on scene mesh reconstruction are presented in~\myappendixref{apd:C}.
\begin{figure}
\label{fig:mesh_comp}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.19\linewidth]{images/mesh_recon/1_rgb.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/1_worldsheet.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/1_idisc.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/1_simbar.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/1_ours.png}  \\
\includegraphics[width=0.19\linewidth]{images/mesh_recon/2_rgb.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/2_worldsheet.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/2_idisc.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/2_simbar.png}  &
\includegraphics[width=0.19\linewidth]{images/mesh_recon/2_ours.png}  \\
\small (a) RGB input & \small (b) (1) &  \small (c) (2) & \small (c) (3) & \small (d) Ours\\
\end{tabular}
    \caption{\textbf{Mesh reconstruction result comparison.} We compared the mesh reconstruction result with mesh reconstructed by other methods with the following settings, (1): Worldsheet with MiDaS depth, (2): Worldsheet with iDisc depth, (3): SIMBAR reconstruction. Our method can preserve more accurate geometric information and construct smoother mesh surfaces than all other methods.}
    \label{fig:mesh_comparison}
\end{figure}
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_rgb.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_gt.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_worldsheet.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_idisc.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_simbar.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/1_our.png}  \\
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_rgb.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_gt.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_worldsheet.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_idisc.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_simbar.png}  &
\includegraphics[width=0.155\linewidth]{images/normal_compare/2_our.png}  \\
\small (a) RGB input & \small(b) iDisc~\cite{piccinelli2023idisc} & \small (c) (1) &  \small (d) (2) & \small (e) (3) & \small (f) Ours \\
\end{tabular}
    \caption{\textbf{Inferred surface normal comparison.} We compared the surface normal inferred from the reconstructed mesh with other methods (as stated in~\myfigref{fig:mesh_comp}). Our reconstructed meshes have more accurate surface normal with respect to iDisc~\cite{piccinelli2023idisc} surface normal.}
    \label{fig:normal_comparison}
\end{figure}
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.24\linewidth]{images/rotation_compare/1.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/1_idisc.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/1_simbar.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/1_our.png}  \\
\includegraphics[width=0.24\linewidth]{images/rotation_compare/2_rgb.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/2_idisc.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/2_simbar.png}  &
\includegraphics[width=0.24\linewidth]{images/rotation_compare/2_our.png}  \\
\small (a) RGB input & \small (b) (2) &  \small (c) (3) & \small (d) Ours \\
\end{tabular}
    \caption{\textbf{Mesh comparison from different angles and background mesh.} We compared the mesh reconstruction result with mesh reconstructed by other methods (, as stated in~\myfigref{fig:mesh_comp}.) with a $3$ meters zoom in and a $45$ degrees rotation clockwise as well as background mesh generated with our pipeline. The result shows that our method can efficiently break the unexpected connection between foreground and background objects and make the background mesh watertight.}
    \label{fig:mesh_angle}
\end{figure}
\begin{figure}
\label{fig:loss_curve}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.49\linewidth]{images/mesh_recon/1_loss.png}  &
\includegraphics[width=0.49\linewidth]{images/mesh_recon/2_loss.png}  \\
\small (a) Loss curve of upper image & \small(b) Loss curve of bottom image  \\
\end{tabular}
\vspace{-2mm}
    \caption{\textbf{Optimization loss curve.} We present the optimization loss curve of two examples shown in~\myfigref{fig:mesh_comp}.}
\end{figure}
\subsection{Generalization to Other Datasets}
Our pipeline can be generalized to a wide range of datasets, including the Cityscapes dataset~\cite{Cordts2016Cityscapes}, the BDD100K dataset~\cite{bdd100k} and the Dark Zurich dataset~\cite{SDV19} daytime split. To demonstrate this on the Geometric Mesh Reconstruction component, we replace the input RGB $\mathbf{I_d}$ and semantic annotation $\mathbf{S}$ with samples from the new dataset.~\myfigref{fig:mesh_generalize} illustrates the qualitative result of the scene mesh reconstructed from different datasets. We observe that our pipeline exhibits excellent generalization across various datasets.
\begin{figure}[H]
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_1.jpg}  &
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_1_mesh.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_1.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_1_mesh.png}  \\
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_2.jpg}  &
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_2_mesh.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_2.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_2_mesh.png}  \\
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_3.jpg}  &
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_3_mesh.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_3.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_3_mesh.png}  \\
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_4.jpg}  &
\includegraphics[width=0.24\linewidth]{images/generalize/bdd100k_4_mesh.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_4.png}  &
\includegraphics[width=0.24\linewidth]{images/generalize/cityscape_4_mesh.png}  \\
\small (a) & \small (b) &  \small (c)  & \small (d) \\
\end{tabular}
\vspace{-2mm}
\caption{\textbf{Mesh reconstructed from different datasets.} We showcase the mesh reconstruction based on two different datasets: BDD100K dataset~\cite{bdd100k} (column (a) and (b)) and Cityscapes dataset~\cite{Cordts2016Cityscapes} (column (c) and (d)). The above results demonstrate the capability of our pipeline to generate reasonable scene mesh when applied to other datasets. However, we notice that roads close to the camera are not correctly reconstructed, this is a result of inaccurate surface normal prediction caused by the ego vehicle.}
\vspace{-4mm}
\label{fig:mesh_generalize}
\end{figure}
\section{Testing Plans}
\subsection{Datasets and Metrics}
\vspace{-2mm}
\mypara{Datasets} For the experiment of the entire pipeline, we will use the ACDC dataset~\cite{SDV21ACDC} described in~\mysecref{sec:dataset}. In particular, we will apply our pipeline to reference images in which semantic annotations are available and conduct evaluation on the nighttime split testing set. The evaluation of the ACDC dataset is done via an online server. 

\mypara{Metrics} The goal of our experiment is to show that our synthetic nighttime images can serve as training data and improve the performance of current segmentation detection methods. To evaluate the retrained model, we will use the standard semantic segmentation evaluation method mean Intersection of Union (mIoU) and the Uncertainty-Aware semantic segmentation Intersection of Union (AUIoU) introduced by~\cite{SDV21ACDC}. The main distinguishing feature of those methods is the incorporation of image regions that possess indiscernible semantic content, referred as "invalid regions" during the process of annotation and evaluation.

\subsection{Experiments}
Our experiments aim to demonstrate the superiority of our generated nighttime images over the original images and other synthetic nighttime images for the task of semantic segmentation. To achieve this, we will conduct a comprehensive evaluation by training various state-of-the-art methods and architectures using different versions of nighttime images and then evaluate them using the ACDC dataset nighttime testing set. In particular, for the training data, we plan to use the original daytime reference images as the baseline, comparing it with dimmed daytime images, CycleGAN~\cite{CycleGAN2017} transferred nighttime images, our nighttime images, as well as the real nighttime images in the ACDC dataset~\cite{SDV21ACDC}. To ensure robustness and accuracy, we will train the model with various methods including DeepLabV3+~\cite{deeplabv3plus2018}, SegFormer~\cite{xie2021segformer}, Mask2Former~\cite{cheng2021mask2former}, and HRNet~\cite{SunXLW19}, as well as architectures including ResNet~\cite{He2015DeepRL}, Swin Transformer~\cite{liu2021Swin} and Vision Transformer~\cite{dosoViTskiy2020}. Moreover, for each combination listed above, we also plan to train them with different sizes of training datasets with nighttime images generated using our pipeline, showing that our pipeline is able to generate multiple nighttime images from one single daytime image by activating different light sources combinations described in~\mysecref{sec:light_active}. We will report the mIoU and AUIoU of all methods on the nighttime split of the ACDC dataset, as well as the IoU on each semantic class. Additionally, we will also present the qualitative comparison of selected semantic segmentation methods, trained using different training datasets listed above, on the ACDC dataset nighttime split. By presenting these qualitative results, we aim to better understand how different datasets impact the segmentation results and pinpoint the strengths and weaknesses of nighttime images generated using our pipeline.