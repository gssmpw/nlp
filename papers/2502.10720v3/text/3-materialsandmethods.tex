%% ----------------------------------------------------------------------------
% BIWI SA/MA thesis template
%
% Created 09/29/2006 by Andreas Ess
% Extended 13/02/2009 by Jan Lesniak - jlesniak@vision.ee.ethz.ch
%% ----------------------------------------------------------------------------
\newpage
\chapter{Materials and Methods}
% The objectives of the ``Materials and Methods'' section are the following: 
% \begin{itemize}
%  \item \textit{What are tools and methods you used?} Introduce the environment, in which your work has taken place - this can be a software package, a device or a system description. Make sure sufficiently detailed descriptions of the algorithms and concepts (e.g. math) you used shall be placed here.
%  \item \textit{What is your work?} Describe (perhaps in a separate chapter) the key component of your work, e.g. an algorithm or software framework you have developed.
% \end{itemize}
\vspace{-7mm}
In this section, we first formalize the problem setting. Next, we introduce our data preparation procedure. Finally, we introduce our data generation pipeline involving geometry mesh reconstruction and realistic nighttime scene relighting.~\myfigref{fig:method} shows an overview of our data generation pipeline.

\section{Problem Setting}
\vspace{-2mm}
Our goal is to generate realistic nighttime images based on daytime images in the ACDC dataset~\cite{SDV21ACDC} reference split using a physics-based method. As described in~\mysecref{sec:related_dataset}, the ACDC dataset reference split contains 2006 daytime images from the training and validation reference set, in which 1003 of them come with a corresponding semantic annotation. Given those 1003 images and their semantic annotations, we propose a data processing pipeline that generates realistic nighttime images for each input image.
\section{Data Preparation}
\vspace{-2mm}
\label{sec:data_prep}
 As shown in~\myfigref{fig:method}, except for daytime RGB images $I_d$ and its corresponding segmentation annotations $S$, our pipeline also takes a binary mask $M_i$ indicating the inactive light sources in $I_d$, and a set of nighttime light sources $E$ as input. In this section, we show the techniques employed for the creation of binary masks and the light sources.
 
 \mypara{Binary mask $\mathbf{M_i}$} Though some works have already been focusing on light source separation~\cite{Yoshida_2023_CVPR}, they are not sufficient in such complex situations. Thus, we manually annotated inactive light sources using Segments.ai~\cite{Multi-sensor}, an online image labelling platform based on super-pixels. To control the quality of our annotations, two annotators first annotated 100 images, then conducted a cross-check by checking each other's annotations and then modifying the annotation rule until a consensus was reached. Ultimately, we defined 12 classes of inactive light sources and categorized them into three primary groups: buildings, vehicles, and objects. We also propose that light sources from the same category should be mutually related, for example, windows on the same floor or car lights from the vehicle. Thus, to ensure the interconnection between light sources, we also defined 9 group classes, see Table~\ref{tab:annot_class} for all classes and their definitions. In this thesis, we annotated 230 out of 253 images from the nighttime reference split (we filtered out 23 images as they were taken at twilight).~\myfigref{fig:annot_stat} shows the statistics of annotated pixels and the number of instances for each class.~\myfigref{fig:annot_example} shows some visual examples of our annotation. More examples can be found in~\myappendixref{apd:A}.
 
\mypara{Inactive light source $\mathbf{I_d}$} To make our nighttime images as realistic as possible, we planned to collect the light source dataset $E$ from a real-world setting. Following the method described in~\cite{Punnappurath_2022_CVPR}, we will first place gray cards under different nighttime illuminations (i.e. near different light sources identified in Table~\ref{tab:annot_class}). Next, we will capture images of each gray card to extract both chromaticity value $[\frac{r}{g}, \frac{b}{g}]$ and strength value $s$.
\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/pipeline.pdf}
    \vspace{-8mm}
    \caption{\textbf{Method overview.} Our pipeline contains two components. The \textbf{Geometric Mesh Reconstruction} component first utilizes network $\mathbf{F_g}$ to estimate the geometric information of an input RGB image, then reconstruct scene mesh based on depth using the Worldsheet~\cite{hu2021worldsheet} (\textbf{A.II}:~\mysecref{sec:gmr}). It also contains a depth refinement kernel (\textbf{A.I}:~\mysecref{sec:drk}) and a mesh post-process kernel (\textbf{A.III}:~\mysecref{sec:mpk}) to optimize depth and mesh, respectively. The \textbf{Realistic Nighttime Scene Relighting} (\textbf{B}:~\mysecref{sec:rnsr}) component first generates nighttime light sources using probabilistic light source activation. Then predict the material characteristics using network $\mathbf{F_{ir}}$. Following that, it uses ray tracing to render the linear nighttime clear image. Last, it processes the linear nighttime image to simulate artifacts and finally generates the output nighttime image $\mathbf{I_n}$. In this thesis, we implemented the Geometric Mesh Reconstruction component.}
    \label{fig:method}
\end{figure}
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{}}
    \includegraphics[width=0.49\linewidth]{images/instance.pdf} &
    \includegraphics[width=0.49\linewidth]{images/pixel.pdf}  \\
    \small (a) Instance Statistics & \small (b) Pixel Statistics \\
\end{tabular}
\caption{\textbf{Annotation Statistics.} We show the annotation statistics of 230 images nighttime reference images. The left image shows the number of instances of each type of light source, and the right image shows the number of pixels occupied by each type of light source. All results are presented in the base-10 logarithm.}
\label{fig:annot_stat}
\end{figure}
\begin{samepage}
\begin{table*}
\newcolumntype{Z}{S[table-format=2.3,table-auto-round]}
\centering
\vspace{-0.5em}
\setlength{\tabcolsep}{3mm}
\small
\footnotesize
\centering
\begin{tabular}{|c|c|c|c|}\hline
           No. & Name & Category & Detailed explanation \\\hline
           1   & window\_building  & building& Building windows that may emit light at night \\\hline
           2   & window\_parked   &  vehicle & Windows of parked vehicles, mainly cars\\\hline
           3   & parked\_front    &  vehicle & Front light of parked vehicles, mainly cars \\\hline
           4   & parked\_rear     &  vehicle & Rear light of parked cars, mainly cars \\\hline
           5   & moving\_front    &  vehicle & Front light of moving vehicles, mainly cars \\\hline
           6   & moving\_rear     &  vehicle & Rear light of moving vehicles, mainly cars \\\hline
           7   & window\_transport&  vehicle & Windows of public transportation that may emit light\\\hline
           8   & street\_light\_HT &  object & High temperature traffic lights, usually brighter \\\hline
           9   & street\_light\_LT &  object  & Low temperature traffic lights, usually dimer \\\hline
           10   & advertisement   &  object  & Advertisements that may emit light at night \\\hline
           11   & clock           &  object  & Clocks that emit light at night, mostly appear at bus stop \\\hline
           12  & inferred        &  object  & Light sources whose light colour can be inferred from its daytime color\\\hline
           13  & windows\_group   &  group   & Group of windows that belong to the same floor of the same building\\\hline
           14  & car\_group       &  group   & Group of light sources that belong to the same car\\\hline
           15  & truck\_group     &  group   & Group of light sources that belong to the same truck\\\hline
           16  & bus\_group       &  group   & Group of light sources that belong to the same bus\\\hline
           17  & bicycle\_group   &  group   & Group of light sources that belong to the same bicycle\\\hline
           18  & motorcycle\_group&  group   & Group of light sources that belong to the same motorcycle\\\hline
           19  & train\_group     &  group   & Group of light sources that belong to the same train     \\\hline
           20  & traffic\_light\_group& group & Group of traffic lights that belong to the same panel\\\hline
           21  & traffic\_sign\_group & group & Group of light sources that belong to the same sign \\\hline
\end{tabular}
\vspace{-2mm}
\caption{{\bf Inactive light source class and their definition}. We defined 21 types of inactive light sources, each of them belonging to one category, including building, vehicle, object and group.}
\label{tab:annot_class}
\end{table*}
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.325\linewidth]{images/annotation/sample_1_image.png} &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_1_label.png}  &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_1.png}\\
\includegraphics[width=0.325\linewidth]{images/annotation/sample_2_image.png} &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_2_label.png}  &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_2.png}\\
\includegraphics[width=0.325\linewidth]{images/annotation/sample_3_image.png} &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_3_label.png}  &
\includegraphics[width=0.325\linewidth]{images/annotation/sample_3.png}\\
\small (a) Input daytime RGB image & \small (b) Inactive light source mask & 
\small (c) Superposition of (a) and (b)\\
\end{tabular}
\vspace{-2mm}
\caption{\textbf{Annotation Examples.} We present several examples of our inactive light source annotation. The left column shows the input daytime RGB image, the middle column shows the annotated inactive light source mask, where each instance has its own identity and bounding box, and the right column superimposes the RGB image and the light source mask.}
\label{fig:annot_example}
\end{figure}
\end{samepage}
\section{Geometric Mesh Reconstruction}
\label{sec:gmr}
The first component of our pipeline involves reconstructing scene mesh from a daytime RGB image and its corresponding segmentation annotation. Depth and surface normal are key geometric information that we rely on to reconstruct the mesh. In particular, we use iDisc~\cite{piccinelli2023idisc} as out Geometry Network ($F_g$) to estimate depth ($D$) and normal ($N$). For depth estimation, we directly utilize the model pre-trained on the KITTI dataset~\cite{Geiger2012CVPR}. For normal estimation, the pre-trained model on the NYUv2 dataset~\cite{Silberman:ECCV12} doesn't adapt well to the ACDC dataset~\cite{SDV21ACDC}, so we retrain the iDisc model using the DIODE dataset~\cite{diode_dataset} outdoor split, initializing the training with pre-trained swin transformer~\cite{liu2021Swin} weight. 

Inspired by previous work SIMBAR~\cite{zhang2022simbar}, we apply Worldsheet~\cite{hu2021worldsheet}, a novel view geometry scene synthesis method to reconstruct scene mesh. Worldsheet builds a scene mesh by warping a grid sheet onto the scene geometry via grid offset and depth. The grid exhibits a horizontal offset $\Delta \hat{x}$ and a vertical offset $\Delta \hat{y}$. Importantly, there is no need for predicting or adjusting individual vertices in the $x$ and $y$ directions. The mesh vertices are formulated from grid offset and depth as:
\begin{equation}
    V_{w, h}=\left[\begin{array}{c}
    d_{w, h} \cdot\left(\hat{x}_{w, h}+\Delta \hat{x}_{w, h}\right) \cdot \tan \left(\theta_F / 2\right) \\
    d_{w, h} \cdot\left(\hat{y}_{w, h}+\Delta \hat{y}_{w, h}\right) \cdot \tan \left(\theta_F / 2\right) \\
    d_{w, h}
\end{array}\right]
\end{equation}
Where $d$ denotes the external depth, $x$ and $y$ denote the horizontal and vertical location of vertices in the mesh coordinates equally spaced from $ - 1$ to $1$, and $\theta_F$ is the camera field of view. Thus, we are able to reconstruct the scene mesh based on external depth. However, this requires that the input depth maps we provide have high precision. In the next two sections, we will introduce the Depth Refinement Kernel that optimizes depth before the reconstruction and the Mesh Post-process Kernel after the reconstruction.

\section{Depth Refinement Kernel}
\label{sec:drk}
The main purpose of the Depth Refinement Kernel is to optimize depth based on segmentation annotations and predicted normal map, where we treat predicted normal as ground truth during optimization. This kernel consists of three parts: Dual-reference cross-bilateral filter, Dual-reference variance filter and Normal guided depth refinement.

\subsection{Dual-reference Cross-bilateral Filter}
Inspired by~\cite{SDHV18}, we apply the Dual-reference cross-bilateral filter as the first building block of our Depth Refinement Kernel. In our work, we optimize input depth $\hat{d}$ using the RGB image and the semantic annotation to obtain filtered depth $d$, shown as~\myeqref{equ:cross-bilateral}.
\begin{equation}
\label{equ:cross-bilateral}
d(\mathbf{p})=\frac{\sum_{q \in \mathcal{N}(\mathbf{p})} G_{\sigma_s}(\|\mathbf{q}-\mathbf{p}\|)\left[\delta(h(\mathbf{q})-h(\mathbf{p}))+\mu G_{\sigma_c}(\|\mathbf{J}(\mathbf{q})-\mathbf{J}(\mathbf{p})\|)\right] \hat{d}(\mathbf{q})}{\sum_{q \in \mathcal{N}(\mathbf{p})} G_{\sigma_s}(\|\mathbf{q}-\mathbf{p}\|)\left[\delta(h(\mathbf{q})-h(\mathbf{p}))+\mu G_{\sigma_c}(\|\mathbf{J}(\mathbf{q})-\mathbf{J}(\mathbf{p})\|)\right]}
\end{equation}
Similar to~\cite{SDHV18}, we use the CIELAB counterpart of the input RGB image $\mathcal{R}$, denoted by $\mathbf{J}$. Where $\mathbf{p}$, $\mathbf{q}$ means pixel locations, $\mathcal{N}$ means neighbouring pixels and $h$ means semantic classes. $\delta$ is the Kronecker delta, G denotes the Gaussian kernel, where $G_{\sigma_s}$ is the spatial Gaussian kernel and $G_{\sigma_c}$ is the colour Gaussian kernel lead by constant $\mu$. The numerator and denominator of this equation consist of two main components, in which $\delta(h(\mathbf{q})-h(\mathbf{p}))$ is for semantic references and  $\mu G_{\sigma_c}(\|\mathbf{J}(\mathbf{q})-\mathbf{J}(\mathbf{p})\|)$ is for colour references. The semantic component of this equation infers that only pixels from different semantic classes can contribute to this term, making the edge of each semantic object sharper. At the same time, the colour component helps to preserve depth that can be implied from the colour variation of the input RGB image. Following~\cite{SDHV18}, we initially set $\mu=5$ and $\sigma_s=10$. For the colour component, we discovered that when its weight grows larger, the depth change at semantic edges will become smoother and more spurious faces will be created (as described in~\mysecref{sec:variance_filter}). Thus, we decreased its weight and set $\sigma_c=5$.~\myfigref{fig:depth_comparison} shows the comparison of depth maps before and after the Dual-reference cross-bilateral filter.
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_1.png} &
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_1_before.png}  &
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_1_after.png}\\
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_2.png} &
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_2_before.png}  &
\includegraphics[width=0.325\linewidth]{images/dual_reference_filter/image_2_after.png}\\
\small (a) Original RGB image & \small (b) Depth before & 
\small (c) Depth after\\
\end{tabular}
    \caption{\textbf{Depth comparison before and after the Dual-reference cross-bilateral filter.} We present two examples for depth comparison. Column \textbf{(a)} shows the original RGB image, column \textbf{(b)} shows the depth estimated by iDisc~\cite{piccinelli2023idisc}, and column \textbf{(c)} shows the depth after dual-reference cross-bilateral filter optimization. The above comparison shows that the dual-reference cross-bilateral filter improves the depth estimation at the pixel level.}
    \vspace{-2mm}
    \label{fig:depth_comparison}
\end{figure}
\subsection{Dual-reference Variance Filter}
\label{sec:variance_filter}
One disadvantage of the Worldsheet~\cite{hu2021worldsheet} is that its generated scene mesh is an equal offsets mesh grid with only one layer of vertices. This creates spurious faces at depth discontinuities, connecting foreground objects with background objects. Although those spurious faces are not visible from the camera angle, they will still generate unrealistic reflections during the final relighting process. To solve this issue, we designed a Dual-reference variance filter to identify spurious faces based on depth maps and semantic annotations, as shown in~\myeqref{equ:cross-variance}. We propose that spurious faces usually happen at uncertain regions that meet the following two criteria: \textbf{(1)} regions that contain at least one semantic boundary. \textbf{(2)} regions that have a large variation in depth. For instance, consider a scenario where a region includes both a section of a moving car and a segment of the road or perhaps a part of a traffic sign alongside a portion of a building. We further define a set of foreground objects that are used to identify semantic boundaries, including vehicles, persons, poles, traffic lights and traffic signs.
\begin{equation}
\label{equ:cross-variance}
\mathcal{U}(r(\mathbf{p}, l)) = (\mathcal{V}(d(r(\mathbf{p}, l))) > \mu) \text{ and } (\mathcal{V}(h(r(\mathbf{p}, l))) > 0)
\end{equation}
In~\myeqref{equ:cross-variance}, $U(\cdot)$ denotes the binary value of the uncertain map, $\mathbf{p}$ denotes pixel location and $r(\mathbf{p}, l)$ represents the square region with a size of $l * l$ pixels with $\mathbf{p}$ as the upper left corner. The two components of~\myeqref{equ:cross-variance} correspond to depth and semantic annotations, respectively. In the depth component $d(\cdot)$, we alert the region to be uncertain if the variance of all depth inside region $r(\mathbf{p}, l)$ is larger than a constant $\mu$. In the semantic component $h(\cdot)$, we alert the region if variance of all semantic value inside region $r(\mathbf{p}, l)$ is larger than zero: when a semantic change happens. Two components are connected by a logic and operator, meaning the region will be marked as uncertain if both components are alerted simultaneously. In our experiment, we set $\mu = 0.001$ and $l = 8$.~\myfigref{fig:uncertain_region} shows two visual examples of the uncertain region detected by the Dual-reference variance filter.
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_1.png} &
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_1_semantic.png}  &
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_1_uncertain.png}\\
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_2.png} &
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_2_semantic.png}  &
\includegraphics[width=0.325\linewidth]{images/dual_variance_filter/image_2_uncertain.png}\\
\small (a) Original RGB image & \small (b) Semantic annotation & 
\small (c) Uncertain map\\
\end{tabular}
\caption{\textbf{Uncertain region detected by the Dual-reference variance filter.} Column \textbf{(a)} shows the original RGB image, column \textbf{(b)} shows the semantic annotation and column \textbf{(c)} shows the generated uncertain map, where the uncertain region is represented by the yellow region.}
\vspace{-2mm}
\label{fig:uncertain_region}
\end{figure}
\subsection{Normal-Guided Depth Refinement}
Differentiable optimization has proved useful to refine and increase the accuracy of learning-based method results~\cite{turpin2023fastgraspd, zhang2023handypriors}. In our reconstruction problem, depth estimation provides coarse geometric information regarding the input image, while surface normal offers further intricate local details. To increase depth accuracy, we propose an optimization-based depth refinement method based on surface normal. For each input image, we formulate a loss function based on the interrelationship between its depth and surface normal, then use gradient descent to optimize depth by minimizing the loss. Next, we will describe the loss terms we used for the normal-guided depth refinement.

\mypara{Normal loss.} Given a depth map, we can infer the surface normal by computing the cross product of gradient vectors between neighbouring pixels, as shown in~\myeqref{equ:compute_normal}.
\begin{equation}
\label{equ:compute_normal}
\hat{\vec{N}} = \vec{\nabla{X}} \times \vec{\nabla{Y}} = (1, 0, \frac{\partial z}{\partial x}) \times (0, 1, \frac{\partial z}{\partial y}) = (-\frac{\partial z}{\partial x}, -\frac{\partial z}{\partial y}, 1)
\end{equation}
Where $\frac{\partial z}{\partial x}$ and $\frac{\partial z}{\partial y}$ denotes the gradient of depth with respect to $x$ and $y$ in the camera space, which can be computed via chain rule $\frac{\partial z}{\partial x} = \frac{\partial z}{\partial u} \cdot \frac{\partial u}{\partial x} $ and $\frac{\partial z}{\partial y} = \frac{\partial z}{\partial v} \cdot \frac{\partial v}{\partial y}$. And the transformation between pixel space and camera space is shown as~\myeqref{equ:transform_x_y}.
\begin{align}
\label{equ:transform_x_y}
u \cdot d = f_x \cdot x + c_x \longleftrightarrow \frac{\partial u}{\partial x} = \frac{f_x}{d} \\ \nonumber
v \cdot d = f_y \cdot y + c_y \longleftrightarrow \frac{\partial v}{\partial y} = \frac{f_y}{d}
\end{align}
Where $f_x$ and $f_y$ denote the focal length along the $x$ and $y$ axis, $c_x$ and $c_y$ denote the principal point along the $x$ and $y$ axis, respectively. The final expression of this loss is formalized in~\myeqref{equ:normal_loss}. In this equation, we treated depth estimated by iDisc~\cite{piccinelli2023idisc} $\vec{N_{\text{est}}}$ as ground truth.
\begin{equation}
\label{equ:normal_loss}
L_{\text{normal}} = \lVert \vec{\hat{N}} - \vec{N_{\text{est}}} \rVert_{2}^{2}
\end{equation}

\mypara{Continuity loss.} Although the normal loss can optimize depth to align its inferred normal with the reference surface normal, it lacks the ability to account for sudden variations in depth. As a result, this creates spurious faces, as discussed in Section~\ref{sec:variance_filter}. To tackle this issue, we proposed a continuity loss that directly optimizes gradient vectors based on surface normal, formalized as~\myeqref{equ:continuity_loss}.
\begin{equation}
\label{equ:continuity_loss}
L_{continuity} = \frac{1}{n} \sum_{i=1}^{n} ((\vec{\nabla X_\text{i}} \cdot \vec{N_\text{i}})^2 + (\vec{\nabla Y_\text{i}} \cdot \vec{N_\text{i}})^2) \cdot (1 - \mathcal{U_\text{i}})
\end{equation}
In this equation, $\mathcal{U}$ is the uncertain map derived by the dual-reference variance filter in~\myeqref{equ:cross-variance}, $\vec{\nabla X}$ and $\vec{\nabla Y}$ are gradient vectors computed in ~\myeqref{equ:compute_normal}, $n$ is the total number of pixels of the input image and $i$ represents the index of currently computing pixel. This loss term is aware of depth discontinuities as the sudden depth change between neighbouring pixels will create a gradient vector that is opposite from the normal vector, resulting large value of the dot product. Thus, it can help eliminate spurious faces created by the normal loss. The participation of the uncertain mask term $(1 - \mathcal{U_\text{i}})$ deals with the case when foreground objects have similar normal as background objects, avoiding the optimization process pushing them into the background.

\mypara{Depth loss.} The optimization process should respect the initial depth predicted by iDisc~\cite{piccinelli2023idisc}, meaning that the optimized depth should not deviate significantly from the initial estimation. Thus, we add a depth loss that punishes any depth change with respect to the estimated depth, shown as~\myeqref{equ:depth_loss}.
\begin{equation}
\label{equ:depth_loss}
L_{depth} = \lVert \hat{d} - d_{est} \rVert_{2}^{2}
\end{equation}

In summary, the final loss we used to optimize depth is the weighted sum of each individual loss, shown in~\myeqref{equ:final_loss}, where $\lambda$â€™s are the weights for the loss terms. 
\begin{equation}
\label{equ:final_loss}
L_{final} = \lambda_1 L_{normal} + \lambda_2 L_{continuity} + \lambda_3 L_{depth}
\end{equation}
In our experiments, we applied grid search to determine those hyper-parameters and set $\lambda_1 = 1$, $\lambda_2 = 1$ and $\lambda_3 = 5$, more details can be found in~\myappendixref{apd:B}. With those differentiable loss terms, we optimize the predicted depth using the gradient-based optimizer Adam~\cite{Adam2015} for $1000$ steps with a learning rate of $0.0001$.

\section{Mesh Post-processing Kernel}
\label{sec:mpk}

As explained in Section~\ref{sec:variance_filter}, the scene mesh reconstructed by Worldsheet~\cite{hu2021worldsheet} constitutes a single-layer mesh grid based on conventional depth map prediction. Consequently, its outcomes contain spurious faces and inaccurately link foreground and background objects within the invisible areas of the input image. To solve this issue,~\cite{wimbauer2023behind} proposed to predict the density field of the input image and map the location in the frustum to volumetric density in order to learn the real 3D feature. However, the performance of their method on complex scenes such as images in the ACDC~\cite{SDV21ACDC} dataset is limited. To bridge the gap, we designed a Mesh Post-process Kernel that contains two steps: uncertain faces deletion and mesh completion.
\subsection{Uncertain Faces Deletion}
The first part of Mesh Post-process Kernel deals with the removal of spurious faces created by the Worldsheet~\cite{hu2021worldsheet}. Specifically, we first perform re-projection of all vertices from 3D to 2D and flagged vertices located within the uncertain area, as described in Section~\ref{sec:variance_filter}, as uncertain vertices. Furthermore, we defined faces that contain at least one uncertain vertex as spurious faces. Following this, we traverse through all faces and remove those that are identified as spurious faces. The resulting object contains the foreground and background object mesh separately, as well as point clouds between them.

\subsection{Mesh Completion}
The first part of the Mesh Post-process Kernel introduces holes in the mesh surface. The second part is designed to complete these holes and ensure the mesh surface becomes watertight. To achieve this, we first determine regions that need to be completed by finding the union of uncertain regions and their neighbouring foreground semantic segments. After that, we complete the mesh sheet by adding vertices into the newly determined uncertain region. In particular, for each newly determined uncertain region, we break it into horizontal lines with one-pixel height. Then adding linearly distributed vertices based on their left and right vertices location. In the end, we add faces between newly added vertices and boundary vertices to make the mesh sheet watertight.~\myfigref{fig:mesh_complete} shows an example of the newly determined uncertain region, and Algorithm 1 shows the operation process. Furthermore, to ensure the pixel-level accuracy of foreground objects, we also apply mesh completion to foreground objects. For the intersection of uncertain regions and foreground object semantics, we select a pixel that is adjacent to the persistent foreground semantics. Here, we assigned the vertex depth as the average of its neighbouring vertices. Following that, we extend this operation to all pixels within the intersection region by employing a breadth-first search approach. Lastly, we establish connections between the newly introduced foreground vertices and their corresponding persisting foreground mesh, effectively rendering each foreground object watertight.
\begin{figure}
\centering
\begin{tabular}{@{}c@{\hspace{1mm}}c@{\hspace{1mm}}c@{}}
\includegraphics[width=0.325\linewidth]{images/mesh_complete/1_rgb.png}  &
\includegraphics[width=0.325\linewidth]{images/mesh_complete/1_semantic.png}  &
\includegraphics[width=0.325\linewidth]{images/mesh_complete/1_uncertain.png}  \\
\includegraphics[width=0.325\linewidth]{images/mesh_complete/2_rgb.png}  &
\includegraphics[width=0.325\linewidth]{images/mesh_complete/2_semantic.png}  &
\includegraphics[width=0.325\linewidth]{images/mesh_complete/2_uncertain.png}  \\
\small (a) RGB input & \small (b) Semantic annotation &  \small (c) New uncertain region \\
\end{tabular}
\caption{\textbf{Completed uncertain region.} Column (\textbf{a}) shows the original RGB image, column (\textbf{b}) shows the semantic annotation and column (\textbf{c}) shows the completed uncertain region, where the uncertain region is represented by the yellow region.}
    \label{fig:mesh_complete}
\end{figure}
\begin{algorithm}
\label{alg:mesh}
\caption{Background Mesh Completion Algorithm}\label{alg:cap}
\begin{algorithmic}[1]
\Require Binary mask list $M$ indicating new uncertain maps, Incomplete mesh sheet $G_u$
\Ensure Each mask maps to a region that needs to be completed on the mesh sheet
\For {Every mask $m \in M$}
    \State $I_t \gets$ top row index of $m$ that has missing vertex
    \State $I_b \gets$ bottom row index of $m$ that has missing vertex
    \For {$i = I_t$ to $I_b$}
        \State $J_l \gets$ index of the leftmost vertex row $i$ that is missing
        \State $J_r \gets$ index of the rightmost vertex row $i$ that is missing
        \State $G_l \gets G_u[i][J_l-1]$
        \State $G_r \gets G_u[i][J_r+1]$
        \For {$j = J_l$ to $J_r$}
            \State $G_u[i][j] \gets \frac{G_l(j_r-j+1)+G_r(j-j_l+1)}{j_r-j_l+2}$ \Comment{Add vertices uniformly between $G_l$ and $G_r$}
        \EndFor
    \EndFor
\EndFor
\State Connect vertices in $G_u$ to form faces \Comment{Make G watertight triangle mesh}
\end{algorithmic}
\end{algorithm}
\section{Realistic Nighttime Scene Relighting}
\label{sec:rnsr}
The second component of our pipeline aims to relight the reconstructed scene for realistic nighttime images based on material characteristics and nighttime light sources, which involves four steps. Though we did not implement this component in this thesis, we will explain the detailed method in the rest of this section. First of all, we utilize an inverse rendering network $\mathbf{F_{ir}}$ to predict the material characteristics, and then we will use a probabilistic light source activation to generate light sources. After that, we plan to apply the traditional rendering technique ray tracing to render clear linear nighttime images. In the end, we will also run post-processing to the clear nighttime image to simulate artifacts caused by the camera, including exposure time, noise and ISP. The remaining part of this section illustrates each step in detail.

\subsection{Material Characteristics Prediction}
Many prior studies have been focused on predicting material characteristics on small objects~\cite{Li_2023_CVPR, Jin_2023_CVPR} or indoor scenes~\cite{neuralSengupta19, Wu_2023_CVPR, Munkberg_2022_CVPR, li2020inverse}. Nevertheless, only a few of them have made an attempt to tackle the same task with outdoor datasets~\cite{Wang_2023_CVPR}. In our work, we explored different existing methods and tested their performance on a synthetic outdoor optical flow dataset MPI Sintel~\cite{Butler:ECCV:2012}. Our testing results show that most indoor methods demonstrate good generalization capabilities on the outdoor dataset, likely due to the relatively small disparity between outdoor and indoor materials. Ultimately, based on these findings, we employe~\cite{li2020inverse} as our inverse rendering network $\mathbf{F_{ir}}$ to predict albedo and roughness from the ACDC~\cite{SDV21ACDC} RGB images. In the future, we will also test the performance of~\cite{Wang_2023_CVPR} once it is released and adapt it into our pipeline if it is proved to be better than~\cite{li2020inverse}.

\subsection{Probabilistic Light Source Activation and Relighting}
\label{sec:light_active}
Unlike most scene relighting methods that consider sunlight as the only light source~\cite{lyu2022nrtf, srinivasan2021cvpr, Yang_2023_CVPR}, our day-to-night simulation pipeline considers over 30 inactive light sources per scene (shown in~\myfigref{fig:annot_stat}). Thus, to generate realistic and diverse nighttime images, we design the probabilistic light source activation kernel to manage the activation of each light source. We define the activation of each light source as an independent random variable with a Bernoulli Distribution $f(\mathbf{p}, \mathbf{l})$, where $\mathbf{p}$ is the probability for event $\textbf{l}$ to happen, meaning that there is a  probability of $p$ for the light source to be active and $(1-p)$ to be inactive. Furthermore, we also consider that light sources exhibit interdependence in the real world. For instance, adjacent windows are likely to be active or inactive simultaneously, front light and rear light of a moving car are also to be active together. To capture this real-life characteristic, we utilize group masks, as defined in \mysecref{sec:data_prep}, to group light sources together. This grouping approach ensures that each group of light sources shares the same $\mathbf{p}$ parameter yet remains independent from other groups. By incorporating this interdependency feature into our light source activation model, we can accurately emulate real-world light sources' behaviour and enhance our simulations' authenticity. Once the scene mesh, material attributes, and light sources are established, we perform ray tracing~\cite{1980raytracing} to render the nighttime image. For each daytime image, we employ various activation parameters for the inactive sources, creating multiple nighttime images.


\subsection{Image Post-processing}
Nighttime images usually contain artifacts such as noise caused by low illumination and long exposure time at night. To simulate this real-world scenario, we will apply post-processing to clear linear images generated by ray tracing. Following~\cite{Punnappurath_2022_CVPR}, we plan to adopt the well-established heteroscedastic Gaussian model~\cite{2009_noise, 2008_noise, 2014_noise, 2013_noise} for noise. Given a nighttime clear image $I_c$, we will generate the nighttime noisy image $I_n$ with the following equation:
\begin{equation}
    \mathbf{I}_n = \mathbf{I}_c+\mathcal{N}\left(\mathbf{0}, \beta_1 \mathbf{I}_{\text {c}}+\beta_2\right)
\end{equation}
Where $\beta_1$ and $\beta_2$ are shot and read noise parameters, which we empirically determined based on measuring the noise of real noisy/clean nighttime image pairs for different ISO levels.
