

We now present our method \textbf{V}ariational phylogenetic \textbf{I}nference with \textbf{PR}oducts over bipartitions (\model). We begin by outlining a generative process for sampling from our variational distribution $q_\phi$. We then describe how to evaluate the density of our variational distribution. Finally, we describe the optimization procedure used to maximize our variational objective function. 

\subsection{Generative Process for Phylogenies}

We begin by describing a generative process for sampling from $q_\phi$, as our variational distribution is best understood by an algorithm for sampling from it. Our algorithm to sample an ultrametric tree with leaf vertices $\mathcal{X}$ proceeds similarly to Algorithm 1 of \citet{Bouckaert:2024}. Namely, we randomly draw each element of the distance matrix $\bfT$ ($t^{\{u,v\}}$ for all $\{u,v\} \subset \calX$) using a set of independent variational distributions with densities $q_{\phi}^{\{u,v\}}$. Then, we run single linkage clustering on $\bfT$ to form $(\tau,\bft)$.

Note that $q_{\phi}^{\{u,v\}}(t^{\{u,v\}})$ and $q_{\phi}(\tau,\bft)$ are closely related: $q_{\phi}^{\{u,v\}}$ describes the distribution over entry $t^{\{u,v\}}$ of $\bfT$, while $q_\phi$ describes the distribution over phylogenetic trees $(\tau,\bft)$ formed by running single-linkage clustering on $\bfT$. Algorithm \ref{alg:sample_q} presents pseudocode to sample from $q_\phi$ if $q_{\phi}^{\{u,v\}}$ is a log-normal distribution. Figure \ref{fig:Phylo_diag} visualizes the process of drawing $\bfT$ using $t^{\{u,v\}} \sim q_\phi^{\{u,v\}}$ and then using single-linkage clustering to map $\bfT$ to $(\tau,\bft)$.

\begin{algorithm}[ht]
\caption{{\tt Sample-q}$(\bfmu,\bfsigma,\calX)$}\label{alg:sample_q}
\begin{algorithmic}[1]
\STATE {\bfseries Input:} Parameters $\bfmu \in \bbR^{\binom{N}{2}}$ and $\bfsigma \in \bbR_{>0}^{\binom{N}{2}}$.
%
\STATE Draw $z^{\{u,v\}} \sim \calN(0,1)$ for all $\{u,v\} \subset \calX$
%
\STATE Define matrix $\bfT \in \bbR_{>0}^{\binom{N}{2}}$ such that:
$$
\log\left(t^{\{u,v\}}\right) = \mu^{\{u,v\}} + z^{\{u,v\}}\sigma^{\{u,v\}}
$$
\STATE \textbf{Return } \text{{\tt Single-Linkage Clustering}}$(\bfT,\calX)$
\end{algorithmic}
\end{algorithm}

\subsection{Density Evaluation}

While Algorithm~\ref{alg:sample_q} presents a straightforward way to sample from the variational distribution, \emph{a priori} it is unclear how to evaluate the variational density $q_\phi$ of a tree generated from Algorithm \ref{alg:sample_q}. This is because a given tree $(\tau,\bft)$ may have been generated from multiple distance matrices $\bfT$ (see Figure \ref{fig:Phylo_diag}). Remarkably, this sampling procedure yields a density with a closed-form solution, as shown in Proposition \ref{prop:q} below. 

\begin{proposition}\label{prop:q}

If the random variables $t^{\{u,v\}}$ are mutually independent, all $q_{\phi}^{\{u,v\}}$ are continuous in $\phi$ and $t$ for all $u,v \in \calX$, and $Q_{\phi}^{\{u,v\}}$ is the survival function of $t^{\{u,v\}}$, then $q_\phi(\tau,\bft)$ has the following form:

\begin{align}
    q_\phi(\tau,\bft) = \prod_{n=1}^{N-1}&\left(\left(\sum_{\substack{w \in\, W_n\\ z \in\, Z_n}} \frac{q_\phi^{\{w,z\}}(t_n)}{Q_\phi^{\{w,z\}}(t_n)}\right)\left(\prod_{\substack{w \in W_n \\ z \in Z_n}} Q_\phi^{\{w,z\}}(t_n)\right)\right).
    \label{eqn:q}
\end{align}
\end{proposition}

A derivation of Proposition \ref{prop:q} using induction is provided in Appendix A. Every taxa pair $\{u,v\}$ appears in the sum and product terms of Equation (\ref{eqn:q}) exactly once, as each taxa pair coalesces exactly once within a rooted phylogenetic tree. Thus, evaluating both $q_\phi(\tau,\bft)$ and $\nabla_\phi \log q_\phi(\tau,\bft)$ takes $\calO(N^2)$ time.

If $q_\phi^{\{u,v\}}$ is continuously differentiable, then $q_\phi$ is also continuously differentiable. In our \model implementation, $q_\phi^{\{u,v\}}$ is log-normal. We can thus compute gradients with respect to $\phi$. Note however that Proposition 1 holds for any continuous mutually independent $q_\phi^{\{u,v\}}$.

We now have almost everything we need to perform phylogenetic variational inference: an (unnormalized) phylogenetic posterior density $p(\tau,\bft,\bfY^{\ob})$, a variational family with density $q_\phi(\tau,\bft)$, and an objective function $L(\phi)$ to maximize in order to find a variational posterior distribution. However, as shown in Algorithm \ref{alg:VIPR}, optimizing $L(\phi)$ with stochastic gradient methods such as Adam \citep{Robbins:1951, Kingma:2014a} requires random estimates of the gradient $\nabla_{\phi} L(\phi)$. Thus, in the following section we consider three methods for gradient estimation: leave-one-out REINFORCE \citep{Mnih:2014,Shi:2022}, the reparameterization trick \citep{Kingma:2014}, and VIMCO \citep{Mnih:2016}.

\begin{algorithm}[ht]
\caption{{\tt VIPR}$(\phi,\calX,K)$}\label{alg:VIPR}
\begin{algorithmic}[1]
\STATE Initialize variational parameters $\phi$
%
\WHILE{not converged}
    \FOR{$k = 1,\ldots,K$}
        \STATE Draw $\bfT^{(k)} \in \bbR_{>0}^{\binom{N}{2}}$ with $t^{\{u,v\}} \sim q_{\phi}^{\{u,v\}}$
        %
        \STATE $(\tau^{(k)},\bft^{(k)}) \gets$ \texttt{Single-Linkage Clustering}$(\bfT^{(k)},\calX)$
    \ENDFOR
    \STATE Estimate gradient $\nabla_\phi L(\phi)$ using $(\tau^{(k)},\bft^{(k)})$ for $k = 1,\ldots,K$.
    %
    \STATE Update $\phi$ using gradient estimates and a stochastic optimization algorithm (Adam, SGD, etc.)
\ENDWHILE
%
\STATE \textbf{Return} $\phi$ 
\end{algorithmic}
\end{algorithm}

\subsection{Gradient Estimators for $q_\phi$} \label{sec:grad}


\subsubsection{The REINFORCE Estimator}

Define $f_{\phi}(\tau,\bft) \equiv \log(p(\tau,\bft,\bfY^{\ob})) - \log(q_{\phi}(\tau,\bft))$, so that $L(\phi) = \bbE_{q_\phi}[f_{\phi}(\tau,\bft)]$. We can interchange the gradients and the finite sum over $\tau$ due to the linearity of integrals. Further, for any fixed $\tau$, under standard regularity conditions \citep{Lecuyer:1995}, we can interchange the gradient and the integral from Equation (\ref{eqn:ELBO_int}). After performing some algebra, we obtain the \textit{leave-one-out REINFORCE} (LOOR) estimator \citep{Mnih:2014,Shi:2022}. See Appendix C for the precise formulation.

\subsubsection{The Reparameterization Trick}

The push out estimator \citep{rubinstein_sensitivity_1992}, popularized in the machine learning literature under the name of the  \textit{reparameterization trick} \citep{Kingma:2014}, is also included in our numerical experiments when $q_\phi^{\{u,v\}}$ is a log-normal distribution for all $u,v \in \calX$. In Algorithm \ref{alg:sample_q}, the candidate coalescent times $t^{\{u,v\}} \sim \text{Lognormal}(\mu^{\{u,v\}},\sigma^{\{u,v\}}) \iff t^{\{u,v\}} = \exp(\mu^{\{u,v\}} + \sigma^{\{u,v\}} z^{\{u,v\}})$ with $z^{\{u,v\}} \sim \mathcal{N}(0,1)$. Thus, we approximate the gradient $\nabla_{\phi} L(\phi)$ as
%
\begin{gather}
    \nabla_\phi L(\phi) \approx \frac{1}{K} \sum_{k=1}^K \nabla_{\phi} \log\left(\frac{p(\bfY, g_{\phi}(\bfZ^{(k)}))}{q_\phi(g_{\phi}(\bfZ^{(k)}))}\right) \label{eqn:grad_approx}\\
    %
    \bfZ^{(k)} \sim \calN(\cdot;\mathbf{0},I).
\end{gather}
%
Where $g_{\phi}(\bfZ) = \texttt{Single-Linkage Clustering}(\exp(\bfmu + \bfsigma \odot \bfZ),\calX)$ and $\calN(\cdot;\mathbf{0},I)$ is a $\binom{N}{2}$-dimensional standard normal. The gradient in Equation (\ref{eqn:grad_approx}) can be found using automatic differentiation in libraries such as PyTorch \citep{Paszke:2019}. This gradient approximation is biased because the interchange of the integral and gradient implied by Equation (\ref{eqn:grad_approx}) is not valid in this setting (see Appendix C). Because this estimator is biased, standard results on convergence of stochastic gradient do not hold. Nonetheless, in practice we find that these gradient estimates perform well.

\subsubsection{The VIMCO Estimator}

One drawback of the single-sample ELBO in Equation (\ref{eqn:ELBO}) is that variational distributions that target the ELBO tend to be mode-seeking (\ie, they can underestimate variance of the true posterior). As an alternative, \citet{Mnih:2016} suggest a $K$-sample ELBO that encourages mode-covering behaviour in the posterior:
%
\begin{equation}
     L_K(\phi) = \bbE_{q_{\phi}}\left[\log\left(\frac{1}{K} \sum_{i=1}^K\frac{p(\tau^{(k)},\bft^{(k)},\bfY^{\ob})}{q_\phi(\tau^{(k)},\bft^{(k)})}\right)\right]\!.
    \label{eqn:ELBO_K} 
\end{equation}
%
Here, $(\tau^{(k)},\bft^{(k)}) \sim q_{\phi}$ for $k = 1,\ldots,K$. This is the objective function used by \citet{Zhang:2024} to perform VBPI. When using this objective function, the VIMCO estimator is an analogous gradient estimator to the LOOR estimator for the single-sample ELBO. See Appendix C for the precise definition of the VIMCO estimator.
