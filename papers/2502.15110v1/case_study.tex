We compared the performance of our \model methods with that of \citealt{Zhang:2024} (denoted VBPI in this section). We studied eleven commonly used genetic datasets that are listed in \citet{Lakner:2008} denoted DS1 through DS11 (these are the names that are given to these datasets in~\citealt{Lakner:2008}). 

We also studied a dataset of 72 COVID-19 genomes obtained from GISAID (Global Initiative on Sharing All Influenza Data; \citealt{Khare:2021}). In particular, we obtained COVID-19 RNA sequences collected in Canada 
on January 2, 2025; submitted to GISAID prior to January 20, 2025; containing at least 29,000 sequenced base pairs; and of the strain JN.1. The 72 COVID-19 genomes studied here are all of the COVID-19 genomes provided by GISAID that satisfied all of these criteria. 
After obtaining these genomes, we aligned them using multiple sequence alignment in MAFFT using the FFT-NS-1 algorithm \citep{Katoh:2013}. Finally, we subset the genomes to $M = 3101$ non-homologous sites (i.e., we omit all sites that are the same across all 72 taxa). The final datasets range from $27-72$ total taxa $N$ and $378-3101$ total sites $M$.

For all methods considered (BEAST, VBPI and our \model methods), we used a Kingman coalescent prior on the phylogenies. We fixed the effective population size at $N_e = 5$ \citep{Kingman:1982} and assumed the Jukes-Cantor model for mutation~\citep{Jukes:1969}. These assumptions are summarized above in Sections 2.2 and 2.3. We also measured the branch lengths in terms of expected mutations per site, which is in line with BEAST and VBPI.

Each run for the experiments on the DS1 to DS11 datasets was executed on a supercomputer node. The runs were allocated 12 hours of wallclock time, 1 CPU, and 16GB of RAM. The supercomputer had a heterogeneous infrastructure involving in which each CPU make and model was Intel v4 Broadwell, Intel Caskade Lake or Skylake, or AMD EPYC 7302. Experiments on the COVID-19 dataset were run with identical conditions to those for DS1 to DS11, but without the 12 hour limit on wallclock time. Instead, they were run for 10,000 iterations (i.e., parameter updates) or 12 hours (whichever took longer).

\subsection{The BEAST Gold Standard}

To approximate the true posterior distribution of each dataset we ran 10 independent MCMC chains using BEAST, each with 10,000,000 iterations. We discarded the first 250,000 iterations as burn-in and thinned to every 1,000-th iteration. This yielded in a total of 97,500 trees that were used as a ``gold standard". We estimated ground-truth marginal log-likelihood values using the stepping-stone estimator \citep{Xie:2010}. For each dataset, we ran 100 path steps of 500,000 MCMC iterations and repeated this process ten times to obtain 10 independent estimates of the marginal log-likelihood.

\subsection{The VBPI Baseline}
The VBPI baseline requires MCMC runs to determine likely subsplits (\ie, evolutionary branching events). To provide these runs, we used BEAST to obtain a rooted subsplit support. We ran 10 independent MCMC chains for 1,000,000 iterations, with the first 250,000 discarded as burn-in. We then thinned to every 1,000-th iteration, yielding 7,500 trees for the VBPI subsplit support.
%
To fit the VBPI baseline, we used the VIMCO gradient estimator with $K$-sample ELBO for $K=10$ and $K=20$ (indicated by VBPI10 and VBPI20 in our plots and tables below). We used the Adam optimization algorithm implemented in PyTorch with four random restarts and learning rates of 0.003, 0.001, 0.0003, and 0.0001 \citep{Kingma:2014a, Paszke:2019}. We estimated the marginal log-likelihood every 100 iterations (i.e., parameter updates) using 500 importance-sampled particles.
%
Of the 16 runs for each VBPI batch size condition (4 learning rates and 4 random restarts), we retained the run with the highest average MLL in the last 10 estimates of the run. This run (with highest average MLL) is included in our plots and figures. We used the primary subsplit pair (PSP) parameterization of VBPI. Code for these experiments was adapted from \url{https://github.com/zcrabbit/vbpi}. See \citet{Zhang:2024} for more implementation details. 

\subsection{The \model Methods}
For our \model methods, we set the variational distributions $q_\phi^{\{u,v\}}$ to be log-normal, so the variational parameters $\phi$ were the means and standard deviations corresponding to the logarithm of the entries $\log(t^{\{u,v\}})$ of the matrix $\bfT$.
%
To initialize the parameters $\phi$, we computed the empirical distribution of coalescent times between taxa $\{u,v\}$ from the short MCMC runs used to establish the support for the VBPI baseline. We then set the initial mean and standard deviation of $q_\phi^{\{u,v\}}$ to be the mean and standard deviation of the empirical distribution.
%
We experimented with three gradient estimation techniques. We estimated $\nabla_{\phi} L(\phi)$ using (1) the LOOR estimator and (2) the reparameterization trick, both with batch sizes of 10 samples. We also estimated $\nabla_{\phi} L_K(\phi)$ using the VIMCO estimator with a batch size of $K=10$.
%
For each gradient estimation technique, we used the Adam optimizer in PyTorch with ten random restarts and learning rates of 0.001, 0.003, 0.01, and 0.03. We recorded the estimated marginal log-likelihood every 10 iterations (i.e., parameter updates) with 50 Monte Carlo samples. Of the 40 runs (4 learning rates and 10 random restarts), we retained the run with the highest average MLL in the last 10 estimates of the run. As for VBPI, the retained run (for each dataset and technique) is reported in the plots and figures below.

\section{Results}

Tables \ref{tbl:MLL} and \ref{tbl:ELBO} show the estimated marginal log-likelihoods (MLLs) and ELBOs for our variational inference experiments after 12 hours of compute time. The stepping-stone algorithm is not a variational method, so it has no entry in Table \ref{tbl:ELBO}.
%
The marginal log-likelihoods in Table~\ref{tbl:MLL} are reported by the gap between the MLL of the gold standard (BEAST/stepping stone run) and the method's MLL (the difference between the MLLs). So, methods with smaller gaps are closer to the gold standard, and the method with the highest MLL is bolded. Note that some VI methods surpass the gold standard, likely due to Monte Carlo error. 

VBPI tends to slightly outperform our \model methods in terms of MLL, but all methods are comparable in terms ELBO (with our methods outperforming VBPI on approximately half of the datasets). This is likely because VBPI directly targets the multi-sample ELBO for optimization, which produces mode-covering behaviour. In contrast, our \model methods target the single-sample ELBO.

\begin{table}[t]
\caption{\emph{{\bf \emph{Gap between gold standard and estimated marginal log-likelihoods for variational inference methods (in nats)}}. Marginal log-likelihoods for VI methods were estimated using importance sampling with 1,000 random samples from each variational distribution. Values indicate differences between gold standard MLL and each method's MLL. Gold standard MLLs (indicated by the \textsc{BEAST} column) are derived from 10 independent chains of the stepping-stone algorithm in BEAST. Datasets (\textsc{Data} column) DS1 to DS11 are from \citet{Lakner:2008}. Dataset COV is the COVID-19 dataset obtained from GISAID. VI methods are specified by columns: Variational Bayesian Phylogenetic Inference with $K$-sample ELBO, $K=10,20$ (\textsc{VBPI10} and \textsc{VBPI20}; \citealt{Zhang:2024}); \model using the leave-one-out REINFORCE estimator (\textsc{LOOR}); \model using the reparameterization trick (\textsc{REP}); \model using the Variational Inference for Monte Carlo Objectives estimator with $K=10$ (\textsc{VIMCO}).}}
\label{tbl:MLL}
%\vskip 0.15in
\begin{center}
\begin{sc}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lccccccr}
\toprule
Data & $(N,M)$ & BEAST & VBPI10 & VBPI20 & LOOR & REP & VIMCO \\
\midrule
DS1      & (27, 1949)        & $-7154.26(0.19)$      & $-0.53(0.09)$     & $\bf{0.36}(0.13)$ & $-2.29(0.15)$  & $-1.83(0.21)$    & $-0.95(0.46)$ \\
DS2      & (29, 2520)        & $-26566.42(0.26)$     & $\bf{0.16}(0.24)$      & $0.01(0.20)$ & $-0.76(0.14)$  & $-0.14(0.43)$    & $-0.37(0.29)$ \\
DS3      & (36, 1812)        & $-33787.62(0.36)$ & $-0.44(0.12)$ & $\bf{-0.38}(0.13)$ & $-3.66(0.53)$ & $-1.91(0.99)$ & $-2.63(0.50)$ \\
DS4      & (41, 1137)        & $-13506.05(0.32)$ & $0.03(0.53)$ & $\bf{0.46}(0.43)$ & $-2.48(0.43)$ & $-0.47(1.21)$ & $-1.73(0.23)$ \\
DS5      & (50, 378)         & $-8271.26(0.39)$ & $-1.70(0.35)$ & $-5.69(0.48)$ & $-0.29(1.82)$ & $-4.01(0.28)$ & $\bf{0.94}(2.08)$ \\
DS6      & (50, 1133)        & $-6745.31(0.55)$ & $-0.76(0.20)$ & $\bf{-0.32}(0.35)$ & $-3.96(0.34)$ & $-3.26(0.60)$ & $-2.72(0.37)$ \\
DS7      & (59, 1824)        & $-37323.88(0.66)$ & $\bf{0.27}(0.26)$ & $-0.24(0.17)$ & $-2.73(0.30)$ & $-2.82(0.31)$ & $-10.42(0.70)$ \\
DS8      & (64, 1008)        & $-8650.20(0.77)$ & $-0.82(0.27)$ & $\bf{0.47}(0.64)$ & $-3.28(0.99)$ & $-4.95(0.47)$ & $-2.88(0.60)$ \\
DS9      & (67, 955)         & $-4072.66(0.53)$ & $-5.32(0.31)$ & $-4.12(0.46)$ & $\bf{-3.12}(1.21)$ & $-5.79(0.74)$ & $-7.60(0.44)$ \\
DS10     & (67, 1098)        & $-10102.65(0.65)$ & $\bf{-0.88}(0.20)$ & $-1.44(0.22)$ & $-5.38(0.42)$ & $-3.98(1.14)$ & $-6.82(0.49)$ \\
DS11     & (71, 1082)        & $-6272.57(0.68)$ & $-18.79(0.41)$ & $-16.28(0.46)$ & $\bf{-6.79}(0.89)$ & $-7.31(0.71)$ & $-9.62(1.46)$ \\
COV      & (72, 3101)        & $-7861.61(0.74)$ & $-39.08(0.58)$ & $\bf{-33.26}(0.76)$ & $-611.84(1.80)$ & $-374.62(0.48)$ & $-214.25(0.42)$ \\
\bottomrule
\end{tabular}}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}

%%%%%%%

\begin{table}[t]
\caption{\emph{\bf \emph{Estimated evidence lower bounds for variational inference methods (in nats).}} \emph{ELBOs were estimated using importance sampling on 1,000 random samples from each variational distribution. Our \model methods beat the VBPI baseline on half of the datasets. Dataset names, method acronyms, and conditions match Table~\ref{tbl:MLL}.}}
\label{tbl:ELBO}
%\vskip 0.15in
\begin{center}
\begin{sc}
\resizebox{1.0\linewidth}{!}{
\begin{tabular}{lcccccr}
\toprule
Data & $(N,M)$ & VBPI10 & VBPI20 & LOOR & REP & VIMCO \\
\midrule
DS1              & (27, 1949) & $\bf{-7157.99}(0.15)$     & $-7158.18(0.16)$   & $-7159.56(0.10)$    & $-7159.54(0.09)$           & $-7161.60(0.20)$ \\
DS2              & (29, 2520) & $-26573.03(0.28) $        & $-26573.60(0.30)$  & $-26569.56(0.06)$   & $\bf{-26569.50}(0.08)$    & $-26570.74(0.13)$   \\
DS3              & (36, 1812) & $\bf{-33793.96}(0.20)$  & $-33794.75(0.28)$  & $-33794.96(0.08)$  & $-33794.77(0.07)$  & $-33796.53(0.15)$  \\
DS4              & (41, 1137) & $-13541.39(13.12)$  & $-13613.68(22.18)$  & $\bf{-13512.54}(0.11)$  & $-13512.60(0.11)$  & $-13513.41(0.14)$  \\
DS5              & (50, 378)  & $-8281.03(0.26)$  & $-8298.64(5.97)$  & $\bf{-8279.93}(0.11)$ & $-8280.35(0.11)$  & $-8282.03(0.17)$   \\
DS6              & (50,1133)  & $\bf{-6751.77}(0.22) $ & $-6752.60(0.21)$  & $-6754.36(0.12)$  & $-6755.29(0.14)$  & $-6756.10(0.21)$  \\
DS7              & (59, 1824) & $\bf{-37331.12}(0.22)$  & $-37331.82(0.31)$  & $-37333.36(0.19)$  & $-37332.04(0.14)$  & $-37352.10(0.42)$  \\
DS8              & (64, 1008) & $\bf{-8657.78}(0.30)$  & $-8658.83(0.22)$  & $-8662.26(0.16)$  & $-8661.88(0.16)$  & $-8664.54(0.26)$  \\
DS9              & (67, 955)  & $-4088.64(0.39)$  & $-4091.21(0.52)$  & $\bf{-4085.61}(0.18)$  & $-4087.25(0.20)$  & $-4090.52(0.22)$  \\
DS10             & (67, 1098) & $\bf{-10111.81}(0.29)$  & $-10112.80(0.28)$  & $-10114.76(0.15)$  & $-10115.16(0.16)$  & $-10119.70(0.26)$  \\
DS11             & (71, 1082) & $-6329.37(9.90)$  & $-6559.24(54.99)$  & $\bf{-6289.60}(0.17)$  & $-6289.70(0.18)$  & $-6294.31(0.20)$  \\
COV              & (72, 3101) & $-8100.96(109.62)$  & $\bf{-7913.84}(0.93)$  & $-8489.82(0.21)$  & $-8244.41(0.21)$  & $-8087.43(0.30)$  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{center}
\vskip -0.1in
\end{table}

Figures \ref{fig:DS1} and \ref{fig:DS14} show the trace of estimated log-likelihood vs iteration number for all VI methods on DS1 (Figure~\ref{fig:DS1}) and on the COVID-19 dataset (Figure~\ref{fig:DS14}). See Appendix B for results on DS2-11. These figures also display empirical distributions of tree metrics for each VI method's learned variational distribution in addition to the BEAST gold standard run (plotted with matplotlib's \emph{kde} function with default parameters, \citealt{Hunter:2007}). We removed 2 of the 1,000 trees sampled from VBPI20 for the COVID-19 experiment because they had extremely low log-likelihood ($<-$90,000), resulting in flat densities.

\model tended to underestimate the variance of tree length compared to BEAST, while VBPI tended to overestimate. In addition, the reinforce and reparameterization gradient estimates result in variational distributions with higher tree log-likelihoods on average, while VBPI and our \model with VIMCO tend to produce trees with more variable log-likelihood values. Again, this behaviour is consistent with the idea that the multi-sample ELBO tends to produce mode-covering variational distributions.
%
\model converged quickly on DS1 because the parameters were initialized in a region of high ELBO. However, convergence was slower for the COVID-19 dataset, and the parameters were initialized with low ELBO. This may be because the optimization was caught in relatively flat regions of the parameter space. This highlights the need for intelligent parameter initializations or annealing schedules.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{DS1-F2-n.png}
    \caption{{\bf \emph{Variational inference results for dataset DS1.}} \emph{\emph{left:} Density estimation for tree lengths.  \emph{center:} Density estimation for tree log-likelihoods. Estimates are formed from 1,000 samples from the variational posterior of each VI method and 97,500 samples from the BEAST gold standard.  \emph{right:} Trace plot of estimated marginal log-likelihood vs.\ iteration number (i.e., parameter update). Marginal log-likelihood was estimated using 500 importance samples for VBPI and 50 importance samples for \model methods.}}
    \label{fig:DS1}
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{DS14-F2-n.png}
    \caption{{\bf \emph{Variational inference results for the COVID-19 dataset.}} \emph{\emph{left:} Density estimation for tree lengths.  \emph{center:} Density estimation for tree log-likelihoods. \emph{right:} Trace plot of estimated marginal log-likelihood vs.\ iteration number (i.e., parameter update). Density estimation was performed in the same way as in Figure~\ref{fig:DS1}.}}
    \label{fig:DS14}
\end{figure}

\subsection{Computational Complexity}

To compare the time complexity of our algorithm against VBPI, we used the COVID-19 dataset (with 72 genomes), ran 1,000 iterations of each VI method (VBPI and the three \model methods), and recorded the average wallclock time per gradient evaluation. We then subset the COVID-19 dataset by randomly selecting 48, 24, 12, 6, and 3 taxa and repeated this procedure to find computation cost as a function of number of taxa (the same random selection was used for each method).
%
We plot the change in the logarithm of the wallclock time versus the logarithm of the number of taxa in Figure \ref{fig:computation}. The y-axis corresponds to the computational complexity of the algorithm (\textit{e.g.}, a y-value of 2 corresponds to a time complexity of $\calO(N^2)$).
%
This experiment supports that \model has an empirical time complexity roughly on the order of $N^2$. The VBPI method also appears to have a similar empirical scaling. \citet{Zhang:2024} show that the gradient calculation for their variational family has linear complexity with respect to the number of parameters, but we conjecture that the number of parameters may grow super-linearly with the number of taxa $N$.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.675\linewidth]{dydx_v_ntaxa-n.png}
    \caption{{\bf \emph{Slope of the logarithm of seconds-per-iteration vs.\ the logarithm of the number of taxa.}} \emph{Each VI method was run for 1,000 iterations on subsets of the COVID-19 dataset. The y-axis corresponds to the computational complexity of the algorithm as a function of number of taxa (\ie, 1 corresponds to linear complexity, 2 corresponds to quadratic complexity, etc.)}}
    \label{fig:computation}
\end{figure}