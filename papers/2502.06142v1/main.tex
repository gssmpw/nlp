\documentclass[twoside,11pt]{article}
\usepackage{blindtext}

% Any additional packages needed should be included after jmlr2e.
% Note that jmlr2e.sty includes epsfig, amssymb, natbib and graphicx,
% and defines many common macros, such as 'proof' and 'example'.
%
% It also sets the bibliographystyle to plainnat; for more information on
% natbib citation styles, see the natbib documentation, a copy of which
% is archived at http://www.jmlr.org/format/natbib.pdf

% Available options for package jmlr2e are:
%
%   - abbrvbib : use abbrvnat for the bibliography style
%   - nohyperref : do not load the hyperref package
%   - preprint : remove JMLR specific information from the template,
%         useful for example for posting to preprint servers.
%
% Example of using the package with custom options:
%

% \usepackage{jmlr2e}
\usepackage[preprint]{jmlr2e}
% \usepackage[plainnat, preprint]{jmlr2e}
% \usepackage[abbrvbib, preprint]{jmlr2e}


% Heading arguments are {volume}{year}{pages}{date submitted}{date published}{paper id}{author-full-names}
\usepackage{lastpage}
% \jmlrheading{23}{2025}{1-\pageref{LastPage}}{1/21; Revised 5/22}{9/22}{21-0000}{Wonyoung Kim, Sungwoo PARK, Garud Iyengar, Assaf Zeevi, and Min-hwan Oh}

% Short headings should be running head and authors last names
\ShortHeadings{Linear Bandits with Partially Observable Features}{Linear Bandits with Partially Observable Features} 
\firstpageno{1}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Personalized package
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{amsmath,amssymb,amsfonts,amsthm}       % blackboard math symbols
\usepackage{abbreviations}
% \usepackage{kotex} % 한국어 패키지
\usepackage{natbib} % citation 패키지: ICML 스타일
    \setcitestyle{authoryear,round,citesep={;},aysep={,},yysep={;}}
    \renewcommand{\cite}[1]{\citep{#1}} % cite가 기본 parenthesis
\usepackage{hyperref} % hyperlink 세팅
    \hypersetup{ %
        pdftitle={},
        pdfsubject={},
        pdfkeywords={},
        pdfborder=0 0 0,
        pdfpagemode=UseNone,
        colorlinks=true,
        linkcolor=black,
        citecolor=mydarkblue, 
        filecolor=mydarkblue,
        urlcolor=mydarkblue,
    }
\usepackage{booktabs}       % professional-quality tables
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
% \usepackage{todonotes}
\usepackage{tikz}
\usepackage{tikz-3dplot}
\usepackage{subfigure}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}
\Crefname{assumption}{Assumption}{Assumptions}
\Crefname{claim}{Claim}{Claims}
\Crefname{equation}{Eq.}{Equations}

\addtocontents{toc}{\protect\setcounter{tocdepth}{0}} % 본문의 내용은 appendix의 toc에 나오지 않음.

\usepackage{makecell}
\usepackage{placeins}
\FloatBarrier
\raggedbottom

\usepackage{caption}
\captionsetup{skip=5pt,font=small}
\setlength{\textfloatsep}{5pt} % Reduce space after floating environments

\renewcommand\ttdefault{txtt}
\newcommand{\sw}{\textcolor{olive}}
\newcommand{\wy}{\textcolor{orange}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% End of personalized package
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Linear Bandits with Partially Observable Features}

\author{\name Wonyoung Kim\thanks{Equal Contribution} \email wyk7@cau.ac.kr \\
       \addr 
       % Department of Industrial Engineering and Operations Research \\
       Chung-Ang University
       \AND
       \name Sungwoo Park\footnotemark[1] \email sungwoo.park@snu.ac.kr \\
       \addr 
       % Graduate School of Data Science\\
       Seoul National Univeristy
       \AND
       \name Garud Iyengar \email garud@ieor.columbia.edu \\
       \addr 
       % Department of Industrial Engineering and Operations Research\\
       Columbia University
       \AND
       \name Assaf Zeevi \email assaf@gsb.columbia.edu \\
       \addr 
       % Graduate School of Business \\
       Columbia University
       \AND
       \name Min-hwan Oh\thanks{Corresponding Author} \email minoh@snu.ac.kr \\
       \addr 
       % Graduate School of Data Science\\
       Seoul National Univeristy
       }

\editor{My editor}

\maketitle

\begin{abstract}%   <- trailing '%' for backward compatibility of .sty file
We introduce a novel linear bandit problem with partially observable features, resulting in partial reward information and spurious estimates.
Without proper address for latent part, regret possibly grows linearly in decision horizon $T$, as their influence on rewards are unknown.
To tackle this, we propose a novel analysis to handle the latent features and an algorithm that achieves sublinear regret.
The core of our algorithm involves (i) augmenting basis vectors orthogonal to the observed feature space, and (ii) introducing an efficient doubly robust estimator.
Our approach achieves a regret bound of $\tilde{O}(\sqrt{(d + d_h)T})$, where $d$ is the dimension of observed features, and $d_h$ is the \emph{unknown} dimension of the subspace of the unobserved features. 
Notably, our algorithm requires no prior knowledge of the unobserved feature space, which may expand as more features become hidden.
Numerical experiments confirm that our algorithm outperforms both non-contextual multi-armed bandits and linear bandit algorithms depending solely on observed features.
\end{abstract}

\begin{keywords}
    linear bandits, partially observable features, doubly robust estimation
\end{keywords}

\section{Introduction}
\label{sec:intro}
We consider a linear bandit problem where the learning agent has access to only a \emph{subset} of the features, while the reward is determined using the \emph{complete set} of features, including both observed and unobserved elements~\footnote{In this paper, we use the terms ``unobserved'' and ``latent'' interchangeably to refer to features that are not visible to the decision-making agent.}.
Conventional linear bandit problems rely on the assumption that the rewards are linear to only observed features, without accounting for the potential presence of unobserved features. 
However, in many real-world applications, rewards are often affected by the latent features that are not observable to the agent.
%However, in many real-world applications, rewards are often influenced not only by observed features but also by unobserved factors.
For example, in recommendation systems, the true reward — such as user satisfaction or purchase decisions — depends not only on observed features like user demographics or past behaviors but also on latent preferences, such as specific tastes in artists (for streaming services) or brands (in e-commerce). 
Accurately incorporating these latent features is essential for providing precise recommendations, while ignoring them causes bias or model misspecification errors in every decision-making.%, potentially leading to regret that grows linearly over the decision horizon.

To address the latent features, \citet{park2022regret}, \citet{kim2023contextual} and \citet{park2024thompson} rely on the assumption that observed features are linear to the latent features sampled from a specific distribution, e.g., a mean-zero Gaussian.
Establishing a regret bound sublinear in the decision horizon without such structural assumptions on the latent features remains a significant challenge and has not been accomplished yet.
%Previously proposed approaches for addressing unobserved features frequently assume and exploit underlying structures.
%One such approach is to model the true latent features as being sampled from a specific distribution, such as a Gaussian~\citep{park2022regret,kim2023contextual,park2024thompson}.
%In contrast, our focus is on a problem setting with minimal structural assumptions about the unobserved features. 
% \footnote{It is important to distinguish the structure on the reward function and the structure of unobserved features in relation to observability. 
% In this work, we study the linear bandit problem, hence the reward function has linear structure but we do not assume any structure on unobservability as done in~\citep{park2022regret,kim2023contextual,park2024thompson}.}
%While there may be some relationship between the observed and latent features, we do not assume that the latent features, which ``generate'' the corresponding observable features, follow any specific distribution. 
%Hence, our problem setting is more challenging with additional complexity to the partially observability of features.
Key challenges in the bandit problem with partially observable features arise from the complete lack of information on the latent features.
Indeed, we do not even know whether an agent observes features partially nor whether we should use the latent features.
To address these challenges, we propose a novel linear bandit algorithm that is agnostic to partially observability.
Despite the absence of knowledge regarding unobserved features, our algorithm attains a tighter regret bound than both linear bandit algorithms that considers only observed features and MAB algorithms that ignore features entirely.
Specifically, our proposed algorithm achieves a $\sqrt{T}$-rate regret bound, without requiring any prior knowledge of the unobserved features, where $T$ is the decision horizon. 
%and 

The key idea of our proposed algorithm lies in two main components: (i) reconstructing feature vectors to capture the influence of unobserved features on rewards, and (ii) constructing a novel doubly robust estimator to mitigate information loss due to unobservability.
For (i), we decompose reward into two additive terms: one projected onto the row space of observed features, and the other onto its orthogonal complement.
The former term maximally captures the effect of observed features, while the latter minimizes the impact of unobserved features.
We then \emph{augment} observed features with an orthogonal basis from the complement space.
This allows us to reformulate the problem within a conventional linear bandit framework, where the reward function is defined as a dot product of minimally augmented features and an unknown parameter.
However, since these augmented features are not identical to the unobserved features, potential estimation error may arise.
To mitigate these errors, we leverage (ii) a doubly robust estimator, which is widely used in statistical literature for its robustness to errors due to missing data.
These two approaches allow our algorithm to effectively compensate for missing information, improving both estimation accuracy and adaptability to the environment.
%our model restricts the agent to observing and acting on only a portion of the feature vector. 
%Although similar problems have been studied in the context of bandits with partially observable features~\citep{park2022regret,park2024thompson} or incomplete information~\citep{chen2022some,jang2022high,kim2023contextual}, 
%This decomposition of the feature vector further enables us to split the reward similarly into two parts, allowing for handling more general scenarios by defining a reward function specifically for the latent portion.
%However, the information gap from the latent features, and thus the latent reward portion, significantly impairs the ability of the agent to learn and apply the optimal policy, thereby impacting the overall decision-making process.
%To address this, we introduce a novel strategy that integrates the latent portion of the reward into the learnable parameter for estimation. 

\begin{table}[t]
    \centering
    \caption{An overview of regret bound range of our algorithm, \texttt{RoLF}, depending on $d_h \in [0,K-d]$, the dimension of the vector space spanned by the rows of the matrix of unobserved features influencing the reward.
    %Our algorithm incurs regret adaptive to $d_h$, and the regret bound does not exceed that of multi-armed bandit algorithms leveraging UCB, in the worst case.
    Note that $\tilde{O}$ denotes the big-O notation omitting logarithm factors.}
    \begin{tabular}{c c}
    \toprule
    \textbf{Feature space} & \textbf{Regret bound} \\
    \midrule
    span(observed features) $\supseteq$ span(latent features) & $ \tilde{O}(\sqrt{dT}) $ \\
    span(observed features) $\subseteq$ span(latent features) & $\tilde{O}(\sqrt{KT})$ \\
    otherwise & $\tilde{O}(\sqrt{(d+d_h)T})$\\
    \bottomrule
    \end{tabular}
    \vspace{0.8em} 
    \label{tab:regret_overview}
\end{table}

Our main contributions are summarized as follows:
\vspace{-8pt}
\begin{itemize}
    \item We propose a linear bandit problem with partially observable features.
    Our problem setting is more general and challenging than those in the existing literature on linear bandits with latent features, which often rely on specific structural assumptions governing the relationship between observed and latent features.
    In contrast, our approach assumes no additional structure for the unobserved features beyond the linearity of the reward function, which is commonly adopted in the linear bandit literature~(\cref{sec:preliminaries}).
      
    \item We introduce a novel estimation strategy by (i) efficiently augmenting the features that maximally captures the effect of reward projected onto the observed features, while minimizing the impact of unobserved features~(\cref{sec:method}), and (ii) constructing a doubly robust (DR) estimator that mitigates errors from unobserved features.
    By integrating augmented features with the DR estimator, we guarantee a $t^{-1/2}$ convergence rate on the rewards for \emph{all} arms in each round $t$~(\cref{thm:est_lasso}).
    %After projecting rewards onto the row space of the observed features, augments the observed features with a basis spanning a subspace of the orthogonal complement of the observable feature space. 
    %Our main message is that information loss  can be mitigated with a relatively simple strategy.

    \item We propose an algorithm named \textit{Robust to Latent Features} (\texttt{RoLF}) for general linear bandit framework with latent features~(\cref{alg:RoLF}).
    The algorithm achieves a regret bound of $\tilde{O}(\sqrt{(d + d_h)T})$~(\cref{thm:regret}), where $d_h$ is the dimension of the subspace formed by projecting the reward from unobserved features onto the orthogonal complement of the row space of observed features~(\cref{subsec:def_dr_lasso}) and $\tilde{O}(\cdot)$ is the Big-O notation omitting logarithmic factors. 
    \texttt{RoLF} requires no prior knowledge or modeling of unobserved features yet achieves a sharper regret bound than both linear bandit algorithms that consider only observed features~\citep{li2010contextual-bandit,abbasi-yadkori2011improved,agrawal2013thompson,kim2019doubly-robust} and MAB algorithms~\citep{auer2002finite-time}, as summarized in~\cref{tab:regret_overview}.
    %These regret bounds are faster than those of other linear bandit algorithms employing the OFU principle.
    %, with two versions: one using the Ridge estimator and the other using the Lasso estimator, both employing the doubly robust (DR) technique. 
    %Our algorithm has two key features: first, it uses the features of both selected and unselected arms for parameter estimation; second, it utilizes a resampling and coupling strategy, enabling the agent to explore and find actions that balance regret minimization with accurate reward estimation.
    
    %\item We establish that both versions of our algorithm exhibit sublinear dependence on either the number of arms or the dimension of the augmented feature vectors.
    %The version with the Ridge estimator shows a regret bound of $\tilde{O}(\sqrt{KT})$, where $K$ is the number of arms. 
    %The version with the Lasso estimator achieves a regret bound of $\tilde{O}(\sqrt{(d+d_h)T})$, where $d_h$ is the dimension of the subspace obtained by projecting the latent part of the reward onto the orthogonal complement of the row space of the observable features. 
    %These regret bounds are faster than those of other linear bandit algorithms employing the OFU principle.
    
    \item Our experiments confirm that our algorithm consistently outperforms MAB and linear bandit algorithms that \emph{solely} depend on observed features, validating both its practicality and theoretical guarantees.
\end{itemize}

\section{Related Works}
\label{sec:related_works}
In bandit problems, the learning agent learns only from the outcomes of chosen actions, leaving unchosen alternatives unknown~\citep{robbins1952some}. 
This constraint requires a balance between exploring new actions and exploiting actions learned to be good, known as the exploration-exploitation tradeoff. 
Efficiently managing this tradeoff is crucial for guiding the agent towards the optimal policy.
To address this, algorithms based on \textit{optimism in the face of uncertainty} (OFU)~\citep{lai1985asymptotically} are widely used and studied in linear bandits~\citep{abe1999associative,auer2002using,dani2008stochastic,rusmevichientong2010linearly}. 
Notable examples include \texttt{LinUCB}~\citep{li2010contextual-bandit,chu2011contextual} and \texttt{OFUL}~\citep{abbasi-yadkori2011improved}, known for their practicality and performance guarantees. 
However, existing approaches differ from ours in two key aspects: (i) they assume that the learning agent can observe the entire feature vector related to the reward, and (ii) their algorithms have regret that scales linearly with the dimension of the observed feature vector, i.e., $\tilde{O}(d\sqrt{T})$. 
% This also applies to the lower bound, meaning that no algorithm in this problem setting can achieve better performance than~$\tilde{O}(d\sqrt{T})$~\citep{dani2008stochastic,rusmevichientong2010linearly}.
% Since our feature augmentation strategy increases the dimension of each feature vector up to the number of arms $K$, directly applying the algorithms leads to higher regret than MAB algorithms, which achieves $\tilde{O}(\sqrt{KT})$ regret bound.

In contrast, we develop an algorithm that achieves a sublinear regret bound by employing the doubly robust (DR) technique, thereby avoiding the linear dependence on the dimension of the feature vectors.
The DR estimation in the framework of linear contextual bandits is first introduced by~\citet{kim2019doubly-robust}~and~\citet{dimakopoulou2019balanced}, and subsequent studies improve the regret bound in this problem setting by a factor of $\sqrt{d}$~\citep{kim2021doubly,kim2023double}.
A recent application~\citep{kim2023squeeze} achieves a regret bound of order $O(\sqrt{dT\log T})$ under IID features over rounds.
However, the extension to non-stochastic or non-IID features remains an open question.
To address this issue, we develop a novel analysis that applies the DR estimation to non-stochastic features, achieving a regret bound sublinear with respect to the dimension of the augmented feature vectors. 
Furthermore, we extend DR estimation to handle sparse parameters, thereby further improving the regret bound to be sublinear with respect to the reduced dimension.

Our problem is more general and challenging than misspecified linear bandits, where the assumed reward model fails to accurately reflect the true reward, such as when the true reward function is non-linear~\citep{lattimore2020bandit}, or a deviation term is added to the reward model~\citep{ghosh2017misspecified,bogunovic2021stochastic,he2022nearly}. 
While our work assumes that the misspecified (or inaccessible) portion of the reward is linearly related to certain unobserved features, misspecified linear bandit problems can be reformulated as a special case of our framework.
While the regret bounds in \citet{lattimore2020bandit}, \citet{bogunovic2021stochastic} and \citet{he2022nearly} incorporate the sum of misspecification errors that may accumulate over the decision horizon, our work establishes a regret bound that is sublinear in the decision horizon $T$, not affected by misspecification errors.
% without any misspecification errors.
\citet{ghosh2017misspecified} proposed a hypothesis test to decide between using linear bandits or MAB, demonstrating an $O(K\sqrt{T}\log T)$ regret bound when the total misspecification error exceeds $\Omega(d\sqrt{T})$.
In contrast, our algorithm achieves an $O(\sqrt{(d+d_h)T \log T})$ regret bound without requiring hypothesis tests for misspecification or partial observability.

Lastly, our problem appears similar to the bandits with partially observable features, as studied by~\citet{park2022regret}~and~\citet{zeng2024partially}.
Both works assume that observed features are related to latent features through a known linear mapping, with the latent features sampled from a certain Gaussian distribution. 
Notably, the latter work further assumes that the latent features evolves following a specific linear dynamics model.
In addition, both approaches aim to recover the latent features: the former introduces a known decoder mapping from observed features to their corresponding latent features, while the latter estimates them using a Kalman filter.
In contrast, our approach imposes no structural assumptions on either the observed or latent features, making our problem more general and fundamentally more challenging than those addressed in these works.
Furthermore, our work does not attempt to recover any information related to latent features.
Instead, our novel approach demonstrates that the best action can still be identified and chosen, even with an unobserved portion of the reward, by solely exploiting observed features.

\section{Preliminaries}
\label{sec:preliminaries}

\subsection{Notation}
\label{subsec:notation}
% In this paper, we denote scalars and functions by regular lower-case letters, vectors by bold lower-case letters, and matrices by bold upper-case letters. 
For any $n \in \NN$, let $[n]$ denote the set $\{1, 2, \dots, n\}$. 
%The linear concatenation of two vectors~$\vb_1 \in \RR^{d_1}$ and~$\vb_2 \in \RR^{d_2}$ is represented by~$\concat{\vb_1}{\vb_2} \in \RR^{d_1+d_2}$.
% \equiv [\ub^\top, \vb^\top]^\top
For a vector $\vb$, we denote its $L_1$, $L_2$ and supremum norm by $\norm{\vb}_1$, $\norm{\vb}_2$, and $\norm{\vb}_\infty$, respectively.
The $L_2$-norm weighted by a positive definite matrix $\Db$ is denoted by $\norm{\vb}_{\Db}$. 
For two vectors $\vb_1$ and $\vb_2$, the inner product is defined as the dot product, i.e., $\dotp{\vb_1}{\vb_2} := \vb_1^\top \vb_2$, and we use both notations interchangeably.
For a matrix $\Mb$, its minimum and maximum eigenvalue are denoted by $\lambda_{\min}(\Mb)$ and $\lambda_{\max}(\Mb)$, respectively.
We let $\mathrm{R}(\Mb)$ denote the row space of $\Mb$, i.e., a subspace spanned by the rows of $\Mb$.
%Finally, we use $\Acal_t$ to denote the action set containing available actions at the round $t$, and $\mathrm{R}(\Mb)$ to denote a row space of $\Mb$, i.e., a subspace spanned by the rows of $\Mb$.

\subsection{Problem Formulation}
\label{subsec:problem_formulation}
%In this section, we detail our problem setting. 
%This assumption implies that the action set also remains fixed for every round, i.e., $\Acal_t = \Acal$ for all $t \in [T]$ and $\abs{\Acal} = K$.
%Each action $a \in \Acal$ is uniquely identified by its index within the action set, meaning $a \in [K]$. 
%While we focus on fixed features, our algorithm can be adapted to arbitrary features with slight modifications.
In this section, we outline our problem setting and introduce several key assumptions.
Each arm $a\in[K]$ is associated with a true feature vector $\zb_a \in \RR^{d_z}$ that determines the rewards. 
However, the agent can observe only a subset of its elements, with the others remaining unobserved.
Specifically, $\zb_a$ is defined as follows:
\begin{equation}
    \label{eq:z_decompose}
    \zb_a := {\left[\textcolor{blue}{x_{a}^{(1)}}, \cdots, \textcolor{blue}{x_{a}^{(d)}},  \textcolor{red}{u_a^{(1)}}, \cdots, \textcolor{red}{u_a^{(d_u)}}\right]}^{\top},
\end{equation}
% where $\obs{}:=[\textcolor{blue}{x_{a}^{(1)}}, \cdots, \textcolor{blue}{x_{a}^{(d)}}]^\top\in\RR^d$ refers to the \emph{observed} part; $\ub_{a}:=[\textcolor{red}{u_a^{(1)}}, \cdots, \textcolor{red}{u_a^{(d_u)}}]^\top\in\RR^{d_u}$ represents the \emph{latent} part, which remains \emph{inaccessible} to the agent.
For clarity, we highlight the observed components in blue, while the unobserved components in red.
Note that the dimensions of the latent feature vector, $d_u = d_z - d$, and the true feature vector, $d_z$, are both \emph{unknown} to the agent.
Consequently, the agent is unaware of whether the features are partially observed, which introduces significant challenges in selecting appropriate strategies.

It is worth noting that the setting with fixed observed features\footnote{This assumption is standard in linear bandits with model misspecification \citep{ghosh2017misspecified,lattimore2020learning}, which is a special case of our partially observable feature setting.} includes linear bandits with misspecification error \citep{ghosh2017misspecified,bogunovic2021stochastic,he2022nearly} as special cases.
In \cref{sec:time_varying}, we present a setting with varying observed features and an algorithm that achieves $\sqrt{T}$-rate regret bound.
Moreover, if latent features were allowed to change arbitrarily over time, the problem would become non-learnable and thus ill-posed. 
Consequently, assuming fixed features is both natural and well-justified (see \cref{tab:learnability_results} for comparisons).
% While other linear bandits literature assumes time varying observed features, their distribution is limited to Gaussian and they further assume that the features are mean zero \citep{park2024thompson} or the observed features linear to the latent features \citep{kim2023contextual} with a full rank matrix in expectation.
% Either of the assumptions is essential to obtain a sublinear regret bound; 



% To insert
\begin{table}[t]
    \centering
    \caption{Summary of problem settings covered in this paper and the corresponding results. Note that if latent features arbitrarily change over time, the problem itself would become non-learnable, making the problem ill-posed~(see \cref{sec:time_varying} for details).}
    \begin{tabular}{cccc}
      \toprule
        % \makecell{\textbf{Observed} \\ \textbf{Features}} & \makecell{\textbf{Unobserved} \\ \textbf{Features}} & \textbf{Learnable?} & \textbf{Results} \\
        \textbf{Observed Features} & \textbf{Unobserved Features} & \textbf{Learnable?} & \textbf{Results} \\
    \midrule
    Fixed & Fixed & Yes & \cref{thm:regret} \\
        Varying & Fixed & Yes & \cref{thm:regret_ridge_time_varying} \\
        Varying & Varying & No & - \\
        \bottomrule
    \end{tabular}
    \vspace{0.8em} 
    \label{tab:learnability_results}
\end{table}
% \vspace{0.3cm}

% We use the following assumption on the features:
% \begin{assumption}[Fixed features]
% \label{asm:fixed_features}
% The true reward-generating features remain fixed throughout the entire decision horizon $T$ for all arms $a \in [K]$.
% \end{assumption}
% Under this assumption, it follows that the observable features $\xb_a$ associated with all arms are also fixed. 

%does not know $d_u$ and $d_z$ but knows $d$. 
%the agent is unaware of the extent of partial observability. 
%Given the inaccessibility of $\ub_a$, both its dimension, $d_u$, and the dimension of the full feature vector $\zb_a$, denoted by $d_z$, remain \emph{unknown}, meaning 
%For the theoretical analysis, we assume that $\norm{\zb_a}_{\infty} \leq 1$.

% While the entire $\zb_a$ influences the reward, only a subset is observable.
% A key aspect of our setup is that we do not impose any structure on $\ub_{a}$; whereas the existing methods for partially observable features assumes $\obs{} = \Ab \zb_a$ for a certain matrix $\Ab\in\RR^{d_u\times d_z}$~\citep{park2024thompson,kim2023contextual}.
% Consequently, the problem is more challenging since a portion of the reward remains latent and cannot be estimated.
% As $\ub_{a}$ is inaccessible, its dimension, $d_u$, is also unknown, as is the overall dimension of $\zb_a$, $d_z$

% \wy{its $L_\infty$ norm is bounded by 1, i.e., $\norm{\zb_a}_{\infty}\le 1$ (This assumption applies to the consistency of Ridge estimator)}.
%By the relationship between the $L_2$ norm and the $L_\infty$ norm, this also implies that $\norm{\zb_a}_{\infty} \le 1$.
% \wy{Can we change the assumption to $\|\zb_a\|_{\infty} \le 1$?}

%Each $\obs{}$ is considered an $\Fcal_{0}$-measurable random variable since the observable features are time-invariant and their randomness is determined in the initial round.
The reward associated with each arm is defined as the dot product of its true features $\zb_a$ and an unknown parameter $\thetaopt \in \RR^{d_z}$, given by $ y_{a,t} = \dotp{\zb_a}{\thetaopt} + \epsilon_t = \zb_a^\top \thetaopt + \epsilon_t $ for all $a\in[K]$.
The error term, $\epsilon_t$, captures the inherent randomness in the reward, and we adopt a standard assumption commonly used in bandit problems: 
\begin{assumption}[Sub-Gaussian noise]
\label{asm:sub-gaussian}
Let $\{\Fcal_{t}\}_{t\in[T]}$ denote history at round $t$, represented by a filtration of sigma algebras. 
The reward noise $\epsilon_t$ is assumed to be a $\sigma$-sub-Gaussian random variable conditioned on $\Fcal_t$.
Formally,
\begin{equation*}
    \condexp{\exp(\lambda\epsilon_t)}{\Fcal_{t-1}} \le \exp\left(\lambda^2\sigma^2\over 2\right),
\end{equation*}
for all $\lambda \in \RR$.
\end{assumption}
It follows that $\condexp{\epsilon_t}{\Fcal_{t-1}} = 0$, and $\condexp{y_{a,t}}{\Fcal_{t-1}} = \dotp{\zb_a}{\thetaopt}$ under this assumption. 
For brevity, we use $\EE_{t-1}[\cdot]$ to denote $\condexp{\cdot}{\Fcal_{t-1}}$ henceforth.
Given that $\epsilon_t$ is sampled after each action is observed, $\epsilon_t$ is $\Fcal_t$-measurable.
To eliminate issues of scale for analysis, we assume that the expected reward $|\langle \zb_{a}, \thetab_{\star} \rangle| \le 1$ for all $a\in[K]$.

Let $a_\star := \argmax_{a \in [K]} \dotp{\zb_a}{\thetaopt}$ denote the optimal action, considering both observed and latent features. 
The theoretical performance of our algorithm is evaluated through cumulative regret, which measures the total expected difference between the reward of the optimal action and the reward of the action selected in each round.
Formally, 
\begin{equation}
\begin{split}
    \regret(T) &= \EE\left[\sum_{t=1}^T \dotp{{\zb_\star} - {\zb_t}}{\thetao}\right] 
\end{split}
\label{eq:regret_decomposition}.
\end{equation}

Considering the composition of $\zb_a$ defined in~\cref{eq:z_decompose}, we can decompose the parameter as $\thetaopt = [(\textcolor{blue}{\thetao})^\top,(\textcolor{red}{\thetau})^\top]^\top$, where $\thetao\in\RR^d$ and $\thetau \in \RR^{d_u}$ are the parameters for observed and latent features, respectively.
Adopting this, the reward $y_{a_t,t}$ can also be decomposed into three terms: 
\begin{equation}
    y_{a_t,t} = \dotp{\obs{t}}{\thetao} + \epsilon_t + \dotp{\ub_{a_t}}{\thetau},
    \label{eq:reward_decompose}
\end{equation}
where the last term corresponds to the inaccessible portion of the reward.
This reward model is equivalent to that imposed in the linear bandits with misspecification error~\citep{lattimore2020learning}.
While the regret bound in~\citet{lattimore2020learning} includes misspecification error that grows linearly in decision horizon, our proposed method (\cref{sec:method}) addresses this misspecification error and achieves a regret bound that is sublinear in the decision horizon.

%\begin{align}
%    \regret(T) 
%    &= \EE\left[\sum_{t=1}^T \left(y_{a_\star, t} - y_{a_t,t}\right)\right]\label{eq:def_regret} \\
%    &= \EE\left[\sum_{t=1}^T \dotp{{\obs{\star}} - {\obs{t}}}{\thetao} + \sum_{t=1}^T \dotp{\ub_{a_\star}- \ub_{a_t}}{\thetau}\right] \label{eq:regret_decomposition}.
%\end{align}
Before presenting our method and algorithm, we first establish a lower bound for the regret incurred by algorithms that disregard the unobserved portion of rewards. 
Specifically, the following theorem establishes a lower bound for two algorithms relying solely on observed features: \texttt{OFUL}~\citep{abbasi-yadkori2011improved} and \texttt{LinTS}~\citep{agrawal2013thompson}.

\begin{theorem}[Regret lower bound of \texttt{OFUL} and \texttt{LinTS} ignoring latent features]
\label{thm:regret_linear_lower_bound}
% Suppose that $ T \ge 4d^2 $. 
% %We consider a set $\Xcal := \{\pm d/\sqrt{T} \}^d$ where observable features are generated.
% %Furthermore, the parameter that determines the observable portion of the reward is defined as $\thetao = (1/3d, \dots, 1/3d)^\top\in\RR^d$.
% %Let $a_{\star}$ denote the action that maximizes the expected reward based on the full feature vector, as defined in~\cref{eq:reward_decomposition}, while $ a_o $ represents the action that maximizes the observable portion of the expected reward, i.e., $a_o:=\argmax_{a\in\Acal}\dotp{\obs{}}{\thetao}$.
% %In this scenario, using linear bandit algorithms that only consider observable features, the lower bound of the cumulative regret over the total horizon is given by:
% For any algorithm $\Pi:=(\pi_1,\ldots,\pi_T)$ that consists of policies $\{\pi_t:t\in[T]\}$ that are dependent on observed features, there exists a set of features $\{\zb_{1},\ldots,\zb_{K}\}$ and a parameter $\thetaopt \in \RR^{d_z}$ such that the cumulative regret
% \begin{equation*}
%     \regret_{\Pi}(T, \theta_{\star},\zb_1,\ldots,\zb_K) \ge \frac{T}{3}. %+ \Omega(d\sqrt{T}).
% \end{equation*}
%Suppose that $T > 1024\sigma^2/9\log(1/\delta^\prime)$ for any $\delta'\in(0,1/2)$, where $\sigma$ is the sub-Gaussian parameter of reward noise.
%Consider a linear bandit problem with action set $\mathcal{A} = \{1,2\}$ and true feature set $\mathcal{Z} = \{[1,4]^\top, [2,5]^\top\}$, where $d = d_u = 1$ and the arm 2 is the optimal action.  
%The latent portion of each reward is ``large'', in the sense that there exists a constant $C > 0$ such that $| \dotp{\ub_{a_t}}{\thetau} | = | \mathbb{E}[y_{a_t,t}] - \dotp{\obs{t}}{\thetao} | > C$ (following~\cref{eq:reward_decompose}).
%Under this setup t
There exists a problem instance where the expected regret of both \texttt{OFUL} (\texttt{LinUCB}) and \texttt{LinTS} grows linearly in $T$.
\end{theorem}

\begin{sketchproof}
Consider a linear bandit problem with action set $\mathcal{A} := \{1,2\}$, where true feature set is defined as $\mathcal{Z} := \{[1,3]^\top, [2,19/4]^\top\}\subset \RR^{2}$ and the arm 2 is the optimal action.  
The latent portion of each reward is ``large'', in the sense that there exists a constant $C > 0$ such that $| \dotp{\ub_{a_t}}{\thetau} | = | \mathbb{E}[y_{a_t,t}] - \dotp{\obs{t}}{\thetao} | > C$ (following~\cref{eq:reward_decompose}).
Under this setup, the estimator associated with observed features is not consistent and the \texttt{OFUL} and \texttt{LinTS} selects the suboptimal arm with probability $\Theta(1)$.
Consequently, the regret grows linearly in $T$ for both cases.
\end{sketchproof}
% Under this setup, given that the estimator of the true parameter associated with observed features is inconsistent, we demonstrate that for \texttt{LinTS}, the probability of selecting the suboptimal arm remains $\Theta(1)$.
% Similarly, it can be shown that the probability of playing optimal arm exponentially decays if \texttt{OFUL} plays the suboptimal arm for initial $t$ rounds (as in Theorem 2 in \citet{ghosh2017misspecified}).

\cref{thm:regret_linear_lower_bound} implies that neglecting the latent portion of the reward in decision-making could result in a failure in the learning process of the agent.
The comprehensive proof is deferred to~\cref{appendix:proof_thm1}.
While \cref{thm:regret_linear_lower_bound} focuses on \texttt{OFUL} and \texttt{LinTS}, which are known to achieve the most efficient regret bounds for UCB and Thompson Sampling-based policies, we prove an algorithm-agnostic lower bound using different analysis (see~\cref{appendix:regret_lower_bound_general} for details).

\section{Robust Estimation for Partially Observable Features}
\label{sec:method}

We propose our estimation method to obtain a sublinear regret bound for linear bandits with latent features.
\cref{subsec:feature_augmentation} introduces the feature vector augmentation to handle the misspecification error and \cref{subsec:def_dr_lasso} presents the doubly robust estimation to further improve the regret bound.

\subsection{Feature Vector Augmentation with Orthogonal Projection}
\label{subsec:feature_augmentation}
In order to minimize regret, it is sufficient to estimate the $K$ expected rewards $\{ \zb_{a}^\top\thetaopt : a\in[K]\}$ rather than all components of $\thetaopt \in \RR^{d_z}$. 
A straightforward approach to this problem, which achieves a regret bound of $\tilde{O}(\sqrt{KT})$, is to disregard the observed features and apply MAB algorithms like \texttt{UCB1}~\citep{auer2002finite-time}.
However, these algorithms tend to incur higher regret than those leveraging features, particularly when the number of arms is significantly larger than the dimension of the feature vectors, i.e., $K \gg d$.
% and $d_u = 0$ (indicating full observability of the features).
% The challenge arises from the fact that we do not know whether latent features exist, which makes it difficult to choose between MAB algorithms and linear bandit algorithms.

We propose a unified approach to handle all cases of partially observable features and efficiently estimate all $K$ expected rewards.
Let $\Xb := ( \xb_1, \dots, \xb_K ) \in \RR^{d \times K}$ represent a matrix that concatenates the observed part of the true features, and $\Ub := (\ub_1^{(u)}, \ldots, \ub_K^{(u)}) \in \RR^{d_u \times K}$ represent the matrix that concatenates the latent complements of the true features for each arm.
%In addition, let $F(\Ub) := (f(\mathbf{z}_1^{(u)}), \ldots, f(\mathbf{z}_K^{(u)}))^\top \in \RR^ K$, which is a $K$-dimensional vector that concatenates the outputs of the function $f\in \Fcal$ applied to each $\mathbf{z}_a^{(u)}$.
We assume a set of $K$ vectors $\{\xb_1, \ldots, \xb_K\}$ spans $\RR^d$, without loss of generality.\footnote{When $d > K$, we can apply singular value decomposition on $\Xb$ to reduce the feature dimension to $\bar{d} \le K$ with $\mathrm{R}(\Xb)=\bar{d}$.}
We define $\Pb_{\Xb} := \Xb^\top (\Xb \Xb^\top)^{-1} \Xb$ as the projection matrix onto the row space of $\Xb$, denoted $\mathrm{R}(\Xb)$. 
% For $\bsym{\Ecal} := (\epsilon_1, \ldots, \epsilon_K)$, the rewards $\Yb = (y_1, \ldots, y_K)$ for all arms are now decomposed as:
%Let $\epsilon_t \bsym{1}_K :=(\epsilon_{1,t},\ldots,\epsilon_{K,t})$ denote the vector of $K$ sub-Gaussian errors.\sw{(We do not seem to assume the reward noise depends on the arms from definition~\cref{eq:regret_decomposition})}
Then the vector of rewards for all arms, $\Yb_t= (y_{1,t}, \ldots, y_{K,t})$, is now decomposed as:
\begin{align}
\Yb_t &= (\Xb^\top \thetao + \Ub^{\top} \thetau) + \epsilon_t \bsym{1}_K \notag  \\
&= \Pb_{\Xb} (\Xb^\top \thetao + \Ub^{\top} \thetau) + (\Ib_K-\Pb_{\Xb}) (\Xb^\top \thetao + \Ub^{\top} \thetau) + \epsilon_t \bsym{1}_K \notag \\
&= \Xb^\top (\thetao + (\Xb\Xb^\top )^{-1}\Xb \Ub^{\top} \thetau ) + (\Ib_K-\Pb_{\Xb}) \Ub^{\top} \thetau + \epsilon_t \bsym{1}_K, \label{eq:reward_decomposition_expanded_matrix}
\end{align}
where the first and the second term are the projected rewards onto $\mathrm{R}(\Xb)$ and $\mathrm{R}(\Xb)^{\perp}$, the subspace of $\RR^{K}$ perpendicular to $\mathrm{R}(\Xb)$.
We write the projected parameter as ${\mub_{\star}^{(o)}} := \thetao + (\Xb\Xb^\top )^{-1}\Xb \Ub^{\top} \thetau $.

To handle the second term in~\cref{eq:reward_decomposition_expanded_matrix}, we consider a set of basis $\{\bb_1, \ldots, \bb_{K-d}\} \in \mathrm{R}(\Xb)^{\perp}$.
Given the set, there exist coefficients ${\mu_{\star,1}^{(u)}},\dots,{\mu_{\star,K-d}^{(u)}}\in \RR$ that express the reward projection as:

\begin{equation}
\label{eq:unobs_reward_projection}
    (\Ib_K-\Pb_{\Xb})\Ub^{\top} \thetau = \sum_{i=1}^{K-d} \mu_{\star,i}^{(u)} \bb_i.
\end{equation}
%The coefficients $\mu_{\star,1}^{(u)},\ldots,\mu_{\star,K-d}^{(u)}$ vary depending on the choice of $\{\bb_1, \ldots, \bb_{K-d}\}$.
%While the exact projected vector $(\Ib_K-\Pb_{\Xb})\Ub^{\top} \thetau$ is unknown, there exists $j\in[d]$ such that $\mu_{\star,j}^{(u)}=0$ for some $\{\bb_1, \ldots, \bb_{K-d}\}$.
%Based on this observation, 
We denote the number of nonzero coefficients:
%introduce the definition of the following quantity:
%vector lies in the row space of $(\Ib_K-\Pb_{\Xb})\Ub^{\top}$, whose dimension is:
\begin{equation}
d_h(\bb_{1},\ldots,\bb_{K-d}):=|\{i\in[K-d]:\mu_{\star,i}^{(u)}\neq0\}|.
\label{eq:d_h}
\end{equation}
Note that $d_h = 0$ for any basis $\{\bb_{1},\ldots,\bb_{K-d}\}$ when the latent feature space is included in the observed feature space, i.e., $\mathrm{R}(\Ub) \subseteq \mathrm{R}(\Xb)$, since it implies $(\Ib_K - \Pb_X)\Ub^\top = \mathbf{0}_{K\times d_u}$.
If $\mathrm{R}(\Ub) \supseteq \mathrm{R}(\Xb)$, then $d_h=K-d$ for any basis $\{\bb_{1},\ldots,\bb_{K-d}\}$.
In other cases, the quantity $d_h$ depends on the choice of the basis $\{\bb_1,\ldots,\bb_{K-d}\}$.
Intuitively, $d_h$ tends to be small when $\mathrm{R}(\Xb)$ and $\mathrm{R}(\Ub)$ have a large intersection, while some choice of the basis may lead to a higher $d_h$ and consequently to a higher regret bound.
Nevertheless, for any choice of the basis, our algorithm achieves $\tilde{O}(\sqrt{(d+d_h)T})$ regret without prior knowledge of $d_h$, which is smaller than $\tilde{O}(\sqrt{KT})$ regret bound achieved by MAB algorithms ignoring features.
%Importantly, 

%\begin{equation}
%\begin{split}
%    d_h(\bb_{1},\ldots,\bb_{K-d}) &\!:= 
%    \!\mathrm{dim}\left\{\mathrm{R}\left((\Ib_K-\Pb_{\Xb})\Ub^{\top}\right)\right\} \\
%    &\!=\!\mathrm{dim}(\mathrm{R}(\Xb)^\perp \cap \mathrm{R}(\Ub)) \\
%    &\!=\! \mathrm{rank}(\Ub)-\mathrm{dim}(\mathrm{R}(\Xb) \cap \mathrm{R}(\Ub)). 
%\end{split}
%\label{eq:d_h}
%\end{equation}
%Although the coefficients $\mu_{\star,1}^{(u)}, \ldots, \mu_{\star,K-d}^{(u)}$ depend on the choice of the basis vectors $\{\bb_1, \ldots, \bb_{K-d}\}$, at most $d_h$ coefficients are nonzero for any choice of the basis vectors.
%This is because the $d_h$ basis vectors are sufficient to write the vector $(\Ib_K-\Pb_{\Xb})\Ub^{\top} \thetau$ which is in $\mathrm{R}((\Ib_K-\Pb_{\Xb})\Ub^{\top})$.
%The exact values of $\mu_{\star,1}^{(u)}, \ldots, \mu_{\star,K-d}^{(u)}$ and the number of basis vectors with non-zero coefficients to represent~\cref{eq:unobs_reward_projection} remain unknown.
If we define $\mub_{\star}$ as $[({\mub_{\star}^{(o)}})^\top, ({\mub_{\star}^{(u)}})^\top]^\top \in \RR^{K}$, where $\mub_{\star}^{(u)} = [{\mu_{\star,1}^{(u)}}, \dots, {\mu_{\star,K-d}^{(u)}}]^\top$, then~\cref{eq:reward_decomposition_expanded_matrix} becomes $\Yb_t = [\Xb^\top \; \bb_1 \cdots \bb_{K-d}] \mub_{\star} + \epsilon_t \bsym{1}_K $, implying that the reward for each $a \in [K]$ is:
\begin{equation}
\begin{split}
y_{a,t} &= \eb_a^\top \Yb \\ 
&= \eb_a^{\top} [\Xb^\top \; \bb_1 \cdots \bb_{K-d}] \mub_{\star} + \epsilon_t \\
&= [{\xb_{a}} \; \eb_a^{\top}\bb_1 \cdots \eb_a^{\top}\bb_{K-d}] \mub_{\star} + \epsilon_t, 
\end{split}
\label{eq:reward_decomposition_expanded}
\end{equation}
where $\eb_a \in \RR^K$ is a standard basis, with elements all zero except for $1$ in the $a$-th coordinate.
With this modification, the rewards are now represented as a linear function of the augmented feature vectors, $\tilde{\xb}_a := [\obs{}^\top \; \eb_a^{\top}\bb_1 \cdots \eb_a^{\top}\bb_{K-d}]^{\top} \in \RR^{K}$, \emph{without any misspecification error}.
A toy example illustrating our strategy is shown in~\cref{fig:method}.

\begin{figure}[t]
\caption{Illustration comparing conventional linear bandit algorithms (left) and our approach (right) in estimating rewards of $K=3$ arms.
Conventional algorithms use only observed features and find estimates within $\mathrm{R}(\Xb)$ thus accumulating errors from unobserved features.
However, our strategy projects the latent part of the reward onto the orthogonal complement of $\mathrm{R}(\Xb)$, denoted by $\bb_1^\top\widehat{\thetab}_t^{(u)}$, which enables reward estimation in $\RR^K$.
%mitigates information loss due to partial observability by 
Note that $\hat{\thetab}_t$ is the estimator of the parameter for observed features.}
\tdplotsetmaincoords{60}{120}
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[tdplot_main_coords, scale=0.65]
    \tikzset{font=\fontsize{8}{10}\selectfont}
    % Draw the main coordinate system
    \draw[thick,->] (0,0,0) -- (3,0,0) node[anchor=north]{$\qb_1$};
    \draw[thick,->] (0,0,0) -- (0,3,0);
    \draw[thick,->] (0,0,0) -- (0,0,3) node[anchor=south]{$\qb_3$};
    
    % Draw vectors
    \draw[thick,->] (0,0,0) -- (1,2,3) node[anchor=south]{$\EE_{t-1}[\Yb]$};
    \draw[thick,->] (0,0,0) -- (2,1,0);
    % \draw[dashed] (1,2,0) -- (1,2,3) node[anchor=west]{};

    \node[anchor=south east] at (1.5, 3.5, 0){$\mathrm{R}(\Xb)$};
    \node[anchor=north] at (0,3.3,0.2){$\qb_2$};
    \node[anchor=west] at (2.7,0.5,-0.2){$\mathbf{X}^\top \hat{\thetab}_t$};
\end{tikzpicture}
\end{minipage}
\hfill
\begin{minipage}{0.45\textwidth}
\centering
\begin{tikzpicture}[tdplot_main_coords, scale=0.65]
    \tikzset{font=\fontsize{8}{10}\selectfont}
    % Draw the main coordinate system
    \draw[thick,->] (0,0,0) -- (3,0,0) node[anchor=north]{$\qb_1$};
    \draw[thick,->] (0,0,0) -- (0,3,0);
    \draw[thick,->] (0,0,0) -- (0,0,3) node[anchor=south]{$\qb_3$};
    
    % Draw vectors
    \draw[thick,->] (0,0,0) -- (1,2,3) node[anchor=south]{$\EE_{t-1}[\Yb]$};
    \draw[thick,->] (0,0,0) -- (1,2,0);
    \draw[thick,->] (1,2,0) -- (1,2,3);

    \node[anchor=north] at (0,3.3,0.2){$\qb_2$};
    \node[anchor=south east] at (2.4, 2, 0){$\mathrm{R}(\Xb)$};
    \node[anchor=south west] at (1, 2, 1.5){$\bb_1^\top \widehat{\thetab}_{t}^{(u)}$};
    \node[anchor=west] at (1.6,1.5,-0.2){$\mathbf{X}^\top \hat{\thetab}_t$};
\end{tikzpicture}
\end{minipage}
\label{fig:method}
\end{figure}

The dimension of the augmented feature vectors $\{\tilde{\xb}_a:a\in[K]\}$ is $K \ge d$.
While applying \texttt{SupLinUCB} \citep{chu2011contextual} yields $\tilde{O}(\sqrt{KT})$ regret bound, it lacks practicality as it computes $\log T$ distinct batches and estimators requiring the knowledge of $T$ and, more critically, discards a significant portion of samples in each parameter update. 
We present an efficient algorithm that employs the doubly robust ridge estimator and achieves $\tilde{O}(\sqrt{KT})$ regret bound (see \cref{appendix:dr_ridge}).
%We propose an algorithm that employs the doubly robust ridge estimator and achieves $\tilde{O}(\sqrt{KT})$ regret bound (see \cref{appendix:dr_ridge}).
However, when $K>d$ and $d_u=0$, the regret is high compared to the linear bandits with conventional features.
Therefore, we propose a novel estimation strategy to avoid dependency on $K$ in the following section.
%In this paper, we explore two types of imputation and main estimators: the Lasso estimator and the Ridge estimator.
%We present the Lasso estimator and its corresponding algorithm in the following sections; the Ridge estimator and the associated algorithm can be found in~\cref{appendix:dr_ridge}.


%Note that $\mub_{\star}$ has at most $d + d_h$ non-zero entries.
%\subsection{Doubly Robust Pseudo Rewards with Resampling and Coupling}
%\label{subsec:dr_explained}
%In this section, our goal is to introduce a UCB-based algorithm tailored for linear bandits that achieves a regret bound capped at $\otil(\sqrt{(d+K)T})$. 
%Here, the inclusion of the $K$ term reflects the algorithm's consideration of unique factors associated with each arm. 
%Drawing from the principles of established linear UCB algorithms like \texttt{OFUL}~\citep{abbasiyadkori2011}, we can develop an algorithm with a regret bound of $\otil((d+K)\sqrt{T})$. 
%However, we hypothesize that such algorithms may not be optimal, particularly in scenarios where $K\gg d$, resulting in an inflated regret.

\subsection{Doubly Robust Lasso Estimator}
\label{subsec:def_dr_lasso}

% The Lasso estimator is widely used not only for estimating sparse parameters but also for regularizing an estimator by imposing an $L_1$ penalty term, serving as a technique to prevent overfitting~\citep{tibshirani1996regression}.
In~\cref{eq:reward_decomposition_expanded}, the parameter $\mub_{\star}$ is sparse depending on the dimension of the latent features and the augmented basis vectors.
Recall that $\mub_{\star}^{(u)}$ are the coefficients to express the projection of the reward as represented in~\cref{eq:unobs_reward_projection} and only $d_h$ basis vectors are required to express the projection of the reward; there are at most $d_h$ nonzero entries in $\mub_{\star}^{(u)}$.
%While the exact projected vector $(\Ib_K-\Pb_{\Xb})\Ub^{\top} \thetau$ is unknown, it is evident that the vector lies in the row space of $(\Ib_K-\Pb_{\Xb})\Ub^{\top}$, whose dimension is:
%\begin{equation}
%\label{eq:d_h}
%    d_h\!:=\!\mathrm{dim}\left\{\mathrm{R}\left((\Ib_K-\Pb_{\Xb})\Ub^{\top}\right)\right\}\!=\!\mathrm{dim}(\mathrm{R}(\Xb)^\perp \cap \mathrm{R}(\Ub)) \!=\! \mathrm{rank}(\Ub)-\mathrm{dim}(\mathrm{R}(\Xb) \cap \mathrm{R}(\Ub)).
%\end{equation}
%The dimension $d_h$ reflects how closely the latent features are related to the observed features.
%Specifically, $d_h \le \mathrm{rank}(\Ub)$ where equality holds if and only if $\mathrm{R}(\Ub) \subseteq \mathrm{R}(\Xb)^\perp$.
%Since $\mathrm{rank}(\Ub) \le \min\{d_u, K\}$, the dimension $d_h$ cannot exceed $\min\{d_u, K\}-d$.
%Additionally, if $\mathrm{R}(\Ub) \subset \mathrm{R}(\Xb)$, then $d_h=0$.

%To the best of our knowledge, the current algorithms that maintain sublinear regret with respect to $d$, the dimension of features, diverge into two distinct categories. 
%The first uses a phase-elimination UCB approach, such as \texttt{SupLinUCB}~\citep{chu2011contextual}, but is often criticized for impracticality due to its complex application requirements.
%The second category employs the Doubly Robust (DR) technique~\citep{dimakopoulou2019balanced,kim2019doubly-robust,kim2023improved,kim2021doubly,kim2023squeeze}, initially designed for missing data problems. 
%Previous applications of the DR method to linear bandits rely on features sampled from an IID stochastic process, with a positive definite expected Gram matrix. 
%However, our problem involves \emph{fixed} features with some unobserved elements, which lack statistical independence and variability. 
%Adapting the DR method to our setting is challenging and requires a novel strategy to integrate it effectively into our framework.

Let $\check{\mub}^{L}_t$ denote the Lasso estimator for $\mub_{\star}$ using augmented feature vectors:
\begin{equation}
\begin{split}
\check{\mub}^{L}_{t}&:= \argmin_{\mub} \sum_{\tau=1}^{t} (y_{a_\tau}-\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}\mub )^2 + 2\sigma \sqrt{\frac{2t}{p} \log \frac{2Kt^2}{\delta}} \Big\Vert\big(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^\top\big)^{\frac{1}{2}} \mub\Big\Vert_1.
\end{split}
\label{eq:lasso_impute}
\end{equation}
For the estimator in~\cref{eq:lasso_impute} to correctly identify the zero entries in $\mub_{\star}$, the compatibility condition must hold~\citep{geer2009conditions}, which requires the minimum eigenvalue of the Gram matrix, $\lambda_{\min} \left( t^{-1} \sum_{s=1}^{t} \tilde{\xb}_{a_s} \tilde{\xb}_{a_s}^\top \right)$, to be positive.
However, ensuring a sufficiently large minimum eigenvalue typically demands collecting a large number of exploration samples, thus increasing regret. 
Achieving this with fewer exploration samples remains a key challenge in bandit literature, as the minimum eigenvalue influences convergence rate of the estimator and, consequently, the regret bound~\citep{kim2021doubly, soare2014best-arm}.

We introduce a doubly robust (DR) estimator that employs the \emph{full feature} Gram matrix $\sum_{s=1}^{t} \sum_{a=1}^{K} \tilde{\xb}_{a} \tilde{\xb}_{a}^\top$ instead of $\sum_{s=1}^{t} \tilde{\xb}_{a_s} \tilde{\xb}_{a_s}^\top$.
The DR estimation originates from the statistical literature on missing data, where ``doubly robust'' means that the estimator is robust against errors in the estimation of both the observation probability and the response model.
In bandits, at each decision round $t\in[T]$, only the reward of the selected arm is observed, while the $K-1$ unselected rewards are \emph{missing}. 
Thus DR estimation is applied to impute these $K-1$ missing rewards and include corresponding $K-1$ feature vectors in the estimation.
Since the observation probability is given by the policy (which is known to the learner), the DR estimator is robust against errors in the estimated rewards.
While \citet{kim2019doubly-robust} proposed a DR Lasso estimator on IID features satisfying the compatibility condition, we propose another DR Lasso estimator that does not require the assumptions on features.

%\sw{(We also need to add intuition of this strategy - how can it amend the lack of IID necessary for DR method)}
%We introduce a resampling and coupling method to develop a DR estimator for fixed features. 
We improve the DR estimation by incorporating resampling and coupling methods.
For each $t$, let $\mathcal{E}_t \subseteq [t]$ denote an exploration phase such that for $\tau\in\mathcal{E}_t$ the action $a_{\tau}$ is sampled uniformly over $[K]$.
The $\Ecal_t$ is constructed as $\Ecal_0 = \emptyset$ and $\Ecal_t = \Ecal_{t-1} \cup {t}$ if $|\Ecal_t| \le C_e \log (2Kt^2/\delta)$, otherwise $\Ecal_t = \Ecal_{t-1}$, where $C_e:=8(\sqrt{K}+p^{-1})^2p^2(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$.
In round $t$, when $t \notin \Ecal_t$, the algorithm selects an action $a_t$ according to an $\epsilon_{t}$-greedy policy.
Then, we generate a \emph{pseudo-action} $\tilde{a}_t$ from a multinomial distribution:
\begin{equation}
\begin{split}
&phi_{a_t,t} := \PP(\tilde{a}_t=a_t|a_t) = p \quad \text{and}\quad \phi_{k,t} := \PP(\tilde{a}_t=k|a_t) = \frac{1-p}{K-1}\quad \forall k\in[K] \setminus \{a_t\},
\end{split}
\label{eq:psuedo_distribution}
\end{equation}
where $p\in(1/2,1)$ is coupling probability set by the algorithm.
To couple the policy of the actual action $a_t$ and the pseudo-action $\tilde{a}_t$, we resample both of them until they match.
This coupling yields a lower bound for the observation probability which reduces the variance of the DR pseudo-rewards in \cref{eq:pseudoY}.
Let $\Mcal_{t}$ denote the event where $\tilde{a}_t= a_t$ within a specified number of resamples. 
For given $\delta^\prime\in(0,1)$, we set the number of resamples as $\rho_{t}:=\log((t+1)^{2}/\delta^\prime)/\log(1/p)$ so that event $\Mcal_{t}$ occurs with probability at least $1-\delta^\prime/(t+1)^{2}$.
Resampling allows the algorithm to explore further to find an action that balances between regret minimization and reward estimation.

This coupling replaces $\epsilon_t$ greedy policy with a multinomial distribution $\phi_{1,t} ,\ldots, \phi_{K,t}$. When we use DR estimation with $\epsilon_t$ greedy policy, the inverse probability $\epsilon_t^{-1} := \sqrt{t}$ appears in the pseudo-reward (10), and thus the variance of the pseudo-reward explodes. Therefore, we couple the $\epsilon_t$ greedy policy with the multinomial distribution (9) to bound the inverse probability weight $\phi_{a,t}^{-1} = O(K)$.

With the pseudo-actions (coupled with the actual actions), we construct the unbiased pseudo-rewards for all $a\in[K]$,
\begin{equation}
\tilde{y}_{a,t}:={\tilde{\xb}_a}^{\top}\check{\mub}_t^L+\frac{\II(\tilde{a}_{t}=a)}{\phi_{a,t}}\big(y_{a,t}-\tilde{\xb}_a^{\top}\check{\mub}_t^L\big),
\label{eq:pseudoY}
\end{equation}
and note that $\check{\mub}_t^L$ is the imputation estimator that fills in the missing rewards of unselected arms in round $t$.
% and is defined based on the main estimator used to estimate $\mub_{\star}$. 

For $a \neq \tilde{a}_t$, i.e., an arm $a$ that is \emph{not} selected in the round $t$, we impute the missing rewards using $\tilde{\mathbf{x}}_k^\top \check{\mub}_t^L$.
For $a=\tilde{a}_t$, however, the term $\II(\tilde{a}_t=a)y_{a,t}/\phi_{a,t}$ calibrates the predicted reward to ensure the unbiasedness of the pseudo-rewards for all arms.
Given that $\EE_{\tilde{a}_t}[\II(\tilde{a}_t = a)] = \PP(\tilde{a}_t = a) = \phi_{a,t}$, taking the expectation over $\tilde{a}_t$ on both sides of~\cref{eq:pseudoY} gives $\EE_{\tilde{a}_t}[\tilde{Y}_{a,t}]= \EE_{t-1}[y_{a,t}] = \tilde{\xb}_a^{\top}{{\mub}_{\star}}$ for all $a\in [K]$.
Although the estimate $\tilde{\xb}_a^\top\check{\mub}_t$ may have high error, it is multiplied by the mean-zero random variable $(1-\II(\tilde{a}_t=a)/\phi_{a,t})$, making the pseudo-rewards robust to the error in $\tilde{\xb}_a^\top\check{\mub}_t$.
The pseudo-rewards can only be computed when $\tilde{a}_t = a_t$, so they are used in rounds when the chosen action $a_t$ and the pseudo-action $\tilde{a}_t$ match, indicated by the event $\Mcal_t$. 
Since $\Mcal_t$ occurs with high probability, we can compute the pseudo-rewards for almost all rounds.

Our DR Lasso estimator, which incorporates pseudo-rewards for estimation, is defined as follows:
\begin{equation}
\begin{split}
\widehat{\mub}^{L}_t &:= \argmin_{\mub}  \sum_{\tau=1}^{t}\II (\Mcal_{\tau})\sum_{a\in[K]} \left(\tilde{y}_{a,\tau}-\tilde{\xb}_a^\top {\mub} \right)^2 + \frac{10\sigma}{p}\sigma \sqrt{2t \log \frac{2Kt^2}{\delta}} \Big\Vert\big(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^\top\big)^{1/2}\mub\Big\Vert_1,
\end{split}
\label{eq:lasso_DR}
\end{equation}
%\begin{equation}
%\widehat{\wb}^{L}_t := \argmin_{\wb}  \sum_{\tau=1}^{t}\II (\Mcal_{\tau})\sum_{a\in[K]} \left(\tilde{y}_{a,\tau}-\tilde{\xb}_a^\top t^{-1/2}(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^\top)^{-1/2} {\wb} \right)^2 + \frac{10\sigma}{p} \sqrt{2t \log \frac{2Kt^2}{\delta}} \norm{\wb}_1,
%\label{eq:lasso_DR}
%\end{equation}
and the following theorem provides a theoretical guarantee that this estimator converges across all arms after several exploration rounds.

\begin{theorem}[Consistency of the DR Lasso estimator]
\label{thm:est_lasso}
Let $d_h$ denote the dimension of the projected latent rewards defined in \cref{eq:d_h}.
%and let $\tilde{\sigma}_{\min} = \lambda_{\min}(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^{\top})$ and $\tilde{\sigma}_{\max} = \lambda_{\max}(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^{\top})$.
Then for all round $t$ such that $t\ge |\Ecal_t|$, %\ge 8(\sqrt{K}+p^{-1})^2p^2(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$, 
with probability at least $1-2\delta/t^2$,
\begin{equation}
\max_{a\in[K]}|\tilde{\xb}_{a}(\widehat{\mub}_{t}-\mub_{\star})|\le\frac{20\sigma}{p}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{t}}.
\label{eq:lasso_est}
\end{equation}
\end{theorem}
% Although we use $K$-dimensional feature vectors, the error bound of the DR Lasso estimator is only logarithmic in $K$.
% This fast convergence rate is possible with the regularity conditions, such as the compatibility condition and the restrictive minimum eigenvalue condition~\citep{bhlmann2011statistics,geer2009conditions}.
% While the conventional Lasso estimation assumes that the feature vectors satisfy these conditions and so do the existing bandit methods based Lasso estimation~\citep{bastani2020online, kim2019doubly-robust, oh2021sparsity-agnostic, ariu2022thresholded, chakraborty2023thompson, lee2025lasso}, 
Although the DR Lasso estimator leverages $K$-dimensional feature vectors, its error bound depends only logarithmically on $K$. 
Usually, such fast convergence is made possible under classical regularity conditions, such as the compatibility condition and the restrictive minimum eigenvalue condition
\citep{bhlmann2011statistics,geer2009conditions}.
Also, the existing Lasso-based bandit approaches 
\citep{bastani2020online,kim2019doubly-robust,oh2021sparsity-agnostic,ariu2022thresholded,chakraborty2023thompson,lee2025lasso} 
impose these conditions directly on the feature vectors.
In contrast, our approach does not require this assumption, since our augmented features are orthogonal vectors in $\mathrm{R}(\Xb)^\perp$ and their average Gram matrix satisfies $\lambda_{\min}(\sum_{a\in[K]}\tilde{\xb}_a\tilde{\xb}_a^{\top})\ge \min\{1,\lambda_{\min}(\sum_{a\in[K]}\obs{}\obs{}^{\top} )\}$. 
%Note that the minimum eigenvalue $\tilde{\sigma}_{\min}$ is not always $O(1/K)$ because we assume $\|\tilde{x}_a\|_{\infty} \le 1$ instead of $\|\tilde{x}_a\|_2 \le 1$. 
Thus, the convergence rate has only $\sqrt{\log K}$ rate in terms of $K$.
%Under this condition, we prove a fast convergence rate in~\cref{lem:lasso}.

The consistency is proved by bounding the two components of the error in the pseudo-rewards defined in \eqref{eq:pseudoY}: (i) the noise of the reward and (ii) the error of the imputation estimator $\check{\mu}_t$. 
Since (i) is sub-Gaussian, it can be bounded using martingale inequalities. For (ii), the imputation error $\tilde{x}_a^\top (\check{\mu}^{L}_t - \mu_{\star} )$ is multiplied by the mean-zero random variable $\left(1-\frac{\II(\tilde{a}_t=a)}{\phi_{a,t}}\right)$ and thus it can be bounded by $\|\check{\mu}^L_t - \mu_{\star}\|_1/\sqrt{t}$.

%------------------------------------
% Algorithm
%------------------------------------
\begin{algorithm}[t]
    \caption{Robust to Latent Feature~(\texttt{RoLF})}
    \label{alg:RoLF}
\begin{algorithmic}[1]
    \State \textbf{INPUT:} features $\{\obs{}: a\in[K]\}$, coupling probability $p\in(1/2,1)$, confidence parameter $\delta>0$.
    \State Initialize $\widehat{\mub}_0 = \mathbf{0}_K$, the exploration phase $\Ecal_t=\emptyset$ and the exploration factor $C_{\mathrm{e}}:=8(\sqrt{K}+p^{-1})^2p^2(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$.
    \State Find orthogonal basis $\bb_1,\ldots,\bb_{K-d}$ in $\mathrm{R}(\Xb)^{\perp}$ to construct $\{\tildeobs{}:a\in[K]\}$
    \For{$t=1,\ldots,T$}
    \If{$|\Ecal_t| \le  C_{\mathrm{e}} \log (2Kt^2/\delta)$}
    \State Randomly sample $a_t$ uniformly over $[K]$ and $\Ecal_t = \Ecal_{t-1}\cup \{t\}$.
    \Else
    \State Compute $\widehat{a}_t := \arg\max_{a\in[K]}\tildeobs{}^\top \widehat{\mub}^{L}_{t-1}$
    \While{$\tilde{a}_t \neq a_t$ and $\text{count} \leq \rho_t$}
    \State Sample $a_t$ with $\PP(a_t = \widehat{a}_t)=1-(t^{-1/2})$ and $\PP(a_t =k)=t^{-1/2}/(K-1),\; \forall k\neq\widehat{a}_t$.
    \State Sample $\tilde{a}_t$ according to ~\cref{eq:psuedo_distribution}.
    \State $\text{count} = \text{count}+ 1$
    \EndWhile
    \EndIf
    \State Play $a_t$ and observe $y_{a_t,t}$.
    \If{$\tilde{a}_t \neq a_t$}
    \State Set $\widehat{\mub}^{L}_{t}:=\widehat{\mub}^{L}_{t-1}$
    \Else
    % \State Compute $\widehat{\mub}^{R}_t$ in~\cref{eq:ridge_DR} ($\widehat{\mub}^{L}_t$ in~\cref{eq:lasso_DR} for Lasso) with pseudo-rewards $\tilde{y}_{a,t}$ in~\cref{eq:pseudoY} and the imputation estimator $\check{\mub}^{R}_t$ in~\cref{eq:ridge_impute} ($\check{\mub}^{L}_t$ in~\cref{eq:lasso_impute} for Lasso).
    \State Update $\widehat{\mub}^{L}_t$ following~\cref{eq:lasso_DR} with $\tilde{y}_{a,t}$ and update $\check{\mub}^{L}_t$ following~\cref{eq:lasso_impute}.
    \EndIf
    \EndFor
   % \REPEAT
   % \State Initialize $noChange = true$.
   % \For{$i=1$ {\bfseries to} $m-1$}
   % \If{$x_i > x_{i+1}$}
   % \State Swap $x_i$ and $x_{i+1}$
   % \State $noChange = false$
   % \EndIf
   % \EndFor
   % \UNTIL{$noChange$ is $true$}
\end{algorithmic}
\end{algorithm}
%------------------------------------
% Algorithm
%------------------------------------

\section{Proposed Algorithm and Regret Analysis}
\label{sec:algorithm}
In this section, we propose~\cref{alg:RoLF}, a novel estimation-based algorithm for handling partially observable features.
% \texttt{RoLF} achieves a significantly improved regret bound over other bandit algorithms that neglect (unobserved) features, without requiring prior knowledge of latent features.

\subsection{Robust to Latent Features (\texttt{RoLF}) Algorithm}
\label{subsec:algorithm}
In the initialization step, when the observed features are given, our algorithm finds a set of orthogonal basis $\{\bb_1, \dots, \bb_{K-d}\}\in\mathrm{R}(\Xb)^{\perp}$ to augment each observed features.
% This is performed by applying the singular value decomposition (SVD) to $\Xb$ and selecting the $K-d$ vectors from the right singular matrix that correspond to the singular values of $0$.
After the forced-exploration phase, the algorithm computes the candidate action, denoted by $\widehat{a}_t$, and then resample both $\tilde{a}_t$ and $a_t$ until they match. 
When the resampling phase ends, and the agent selects $a_t$ and observes $y_{a_t, t}$.
If they match within $\rho_{t}:=\log((t+1)^{2}/\delta^\prime)/\log(1/p)$, the algorithm updates both the imputation and main estimators; otherwise neither estimator is updated.
% according to the equations provided in~\cref{eq:lasso_impute} and~\cref{eq:lasso_DR}.

The proposed algorithm does not require the knowledge of the dimension of the latent features $d_u$ and the dimension of the projected rewards from latent feature space onto the $\mathrm{R}(\Xb)^\perp$.
Although we present the algorithm on fixed feature vectors, the algorithm applies to arbitrary feature vectors that changes over time by updating the orthogonal basis.

\subsection{Regret Analysis}
\label{subsec:analysis}
% However, when $K$ is large compared to the dimension of features, the proposed algorithm with Ridge estimator establishes loose regret.
We analyze the regret bound of \texttt{RoLF} using Lasso estimators, as stated in the following theorem:
\begin{theorem}[Regret bound for Lasso \texttt{RoLF}]
\label{thm:regret}
Let $d_h$ denote the dimension of the projected latent rewards defined in \cref{eq:d_h}. 
Then for $\delta\in(0,1)$ and $p\in(1/2,1)$, the expected cumulative regret of the proposed algorithm is bounded by

\begin{equation}
\label{eq:regret_lasso}
    \begin{split}
    \EE[\regret(T)] & \le 10 \delta \log T+\frac{16K^{2}(\sqrt{K}+p^{-1})^{2}}{(1-p)^{2}} +\frac{4p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)} +\frac{80\sigma}{p}\sqrt{2(d+d_{h})T\log\frac{2KT^{2}}{\delta}},
    \end{split}
\end{equation}
\end{theorem}
%The comprehensive proofs for the respective theorems are provided in \cref{appendix:proof_thm4} and \cref{appendix:proof_thm5}.

To the best of our knowledge, \cref{thm:regret} is the first regret bound sublinear in $T$ for the latent features without any structural assumption.
%With slight modifications, the regret bound can also be applied to scenarios with time-varying features and misspecified linear bandit problems.
Note that the number of rounds for the exploration phase is $O(K^3 \log KT)$, which is only logarithmic in the horizon $T$.
The factor $K^3$ is not reducible since the algorithm must estimate all $K$ biases from the missing features.
Using the Gram matrix with \emph{full feature vectors}, $\sum_{a=1}^{K}\tilde{\xb}_{a}\tilde{\xb}_{a}^\top$ in combination with DR estimation reduces the exploration phase time from $O(K^4 \log KT)$ to $O(K^3 \log KT)$, reducing the complexity by a factor of $K$.
The convergence rate in the last term is proportional to $\sqrt{d + d_h}$ rather than $\sqrt{K}$, as shown in \cref{eq:lasso_est}. Thus, our regret bound is $O(\sqrt{(d+d_h)T\log KT})$.
%Because~$\norm{\tildeobs{}}_{\infty} \le 1$, instead of~$\norm{\tildeobs{}}_{2} \le 1$, the minimum eigenvalue $\tilde{\sigma}_{\min}$ becomes constant, independent of both $d$ and $K$.
%$\tilde{\sigma}_{\min}$ plays a crucial role in achieving this rate, as it measures the variability in the features to identify nonzero entries in $\mub^{(u)}_{\star}$.

%Note that $d + d_h \le K$ with equality holding if and only if $\mathrm{rank}(\Ub)=K$ and $\mathrm{R}(\Ub) \subseteq \mathrm{R}(\Xb)^{\perp}$.
%If $\mathrm{rank}(\Ub) < K$ or $\mathrm{R}(\Ub)$ does not fully contain $\mathrm{R}(\Xb)^{\perp}$, we obtain $d+d_h < K$ and a tighter regret bound compared to any other bounds for conventional MAB algorithms and linear bandit algorithms, for sufficiently large $T$.
%Note that the inequality $\mathrm{rank}(\Ub) \le \min\{d_u, K\}$ holds and the dimension $d_h$ approaches $d_u$ as $K \to \infty$. 
%For sufficiently large $T$ such that $K^3 \log 2K T^2 \le \sqrt{T(d+d_h) \log 2KT^2}$, the last term dominates the other terms, and the regret bound of the proposed algorithm grows logarithmically in $K$, whereas that of MAB algorithms grows as $\sqrt{K}$.

\section{Numerical Experiments}
\label{sec:experiment}

In this experiment, we simulate and compare two versions of our algorithm, presented in~\cref{alg:RoLF} and~\cref{alg:RoLF_ridge} (\cref{appendix:dr_ridge}), with linear bandit algorithms that use only observed features: \texttt{LinUCB}~\citep{li2010contextual-bandit, chu2011contextual} and \texttt{LinTS}. 
These algorithms use the UCB and Thompson sampling methods, respectively, when the reward is modeled as a linear function of the features. 
Additionally, since our algorithm incorporates DR estimation with the Lasso estimator, we include \texttt{DRLasso}~\citep{kim2019doubly-robust} in the comparison as well.
To further evaluate the performance of our algorithm in scenarios where latent features are expected but ignored, we also compare it with \texttt{UCB($\delta$)}~\citep{lattimore2020bandit}, an MAB algorithm without features.

For the simulation environment, we generate true features $\latent{}$ for each arm $a \in [K]$ from $\mathcal{N}(\mathbf{0}, \mathbf{I}_{d_z})$ and subsample $d$ elements to construct $\obs{}$. 
Orthogonal basis vectors $\{\mathbf{b}_1, \dots, \mathbf{b}_{K-d}\}$ are derived via singular value decomposition (SVD) on the observed feature matrix $\mathbf{X}$, ensuring orthogonality to $\mathrm{R}(\mathbf{X})$.
We augment $\mathbf{X}$ with the basis vectors via linear concatenation. Rewards are generated by sampling the unknown parameter $\thetab_\star \in \mathbb{R}^k$ from $\mathrm{Unif}(-1/2, 1/2)$. 
The hyperparameter $p$, for the sampling distribution of $\tilde{a}_t$, is set to $0.6$ (see~\cref{eq:psuedo_distribution}). 
The confidence parameter $\delta$ is $10^{-4}$, and the total decision horizon is $T = 1200$. 
To address both partial and full observability, $d_z \geq d$ is used, and we run 5 independent experiments. We compare the algorithms across three scenarios:

\begin{figure}[t]
    \centering     %%% not \center
    \subfigure[$d=1$]
        {\label{fig:1a}\includegraphics[width=0.32\linewidth]
        {figures/scenario1_case1.pdf}}
    \subfigure[$d=\floor{\frac{d_z}{2}}$]
        {\label{fig:1b}\includegraphics[width=0.32\linewidth]
        {figures/scenario1_case2.pdf}}
    \subfigure[$d=d_z-1$]
        {\label{fig:1c}\includegraphics[width=0.32\linewidth]
        {figures/scenario1_case3.pdf}}
    \caption{Cumulative regrets of the algorithms in comparison for scenario 1 ($K=50, d_z=31$).
    % The proposed \texttt{RoLF} algorithm performs better on the partially observable features on various dimension of observable feature vectors.
    }
        \label{fig:result1}
\end{figure}

\begin{figure}[t]
    \centering     %%% not \center
    \subfigure[$K = d = 20$]
        {\label{fig:2a}\includegraphics[width=0.32\linewidth]
        {figures/scenario2_case1.pdf}}
    \subfigure[$K = d = 30$]
        {\label{fig:2b}\includegraphics[width=0.32\linewidth]
        {figures/scenario2_case2.pdf}}
    \subfigure[$K = d = 40$]
        {\label{fig:2c}\includegraphics[width=0.32\linewidth]
        {figures/scenario2_case3.pdf}}
    \caption{Cumulative regrets of the algorithms in comparison for scenario 2 ($d_z=60$).
    % The proposed \texttt{RoLF} algorithm performs better on the partially observable features on various dimension of observable feature vectors.
    }
        \label{fig:result2}
\end{figure}

\begin{figure}[t]
    \centering     %%% not \center
    \subfigure[$K = 15$, $d = 30$]
        {\label{fig:3a}\includegraphics[width=0.32\linewidth]
        {figures/scenario3_case1_seed83.pdf}}
    \subfigure[$K = 20$, $d = 40$]
        {\label{fig:3b}\includegraphics[width=0.32\linewidth]
        {figures/scenario3_case2.pdf}}
    \subfigure[$K = 30$, $d = 60$]
        {\label{fig:3c}\includegraphics[width=0.32\linewidth]
        {figures/scenario3_case3.pdf}}
    \caption{Cumulative regrets of the algorithms in comparison for scenario 3 ($d_z=d$).
    % The proposed \texttt{RoLF} algorithm performs better on the partially observable features on various dimension of observable feature vectors.
    }
        \label{fig:result3}
\end{figure}

\paragraph{Scenario (i).}  
We examine algorithm performance as $d$, the number of observed elements, varies to assess the impact of observability. 
With $K = 50$ arms and $d_z = 31$, we compare results for $d = 1$, $\lfloor d_z/2 \rfloor = 15$, and $d_z - 1 = 30$. 
\cref{fig:result1} presents the results, showing that our algorithm consistently outperforms others in regret and robustness.
In contrast, \texttt{LinUCB}, \texttt{LinTS}, and \texttt{DRLasso} show significant dependence on the number of observed features, with performance deteriorating and variability increasing as observability decreases.

% \paragraph{Scenario (ii).}
% In this scenario, the number of arms is equal to the dimension of the observed features, i.e., $K = d$.
% The main objective of this experiment is to demonstrate that our algorithm remains robust to changes in the number of arms $K$, unlike MAB algorithms that disregard the observable features.
% Specifically, we compare the algorithms with $K$ set to 20, 30, and 40, and $d_z = 60$ for all cases.
% \cref{fig:result2} shows the results for this scenario.
% As we can observe, the performance of \texttt{UCB($\delta$)} worsens as the number of arms increases in each environment. 
% In contrast, our algorithm shows better performance in terms of both the level of regret and robustness.

\paragraph{Scenario (ii).}  
In this scenario, the number of arms is equal to the dimension of the observed features, i.e., $K = d$.
The main objective of this experiment is to demonstrate that our algorithm remains robust to changes in the number of arms $K$, unlike MAB algorithms that disregard the observed features.
Specifically, we compare the algorithms with $K$ set to 20, 30, and 40, and $d_z = 60$ for all cases.
\cref{fig:result2} shows the results for this scenario.
As we can observe, the performance of \texttt{UCB($\delta$)} worsens as the number of arms increases in each environment. 
In contrast, our algorithm shows better performance in terms of both the level of regret and robustness.

% \paragraph{Scenario (iii).}
% We demonstrate our empirical performance when the number of arms is fewer than the dimension of the observed features, by setting $d = 2K$ and varying $K$ as 15, 20, and 30.
% We set the dimension of complete feature vectors $d_z=d$.
% Before we put features in our proposed algorithms, we apply singular value decomposition to reduce the dimension.
% \cref{fig:result3} demonstrate that our strategy performs well in extreme cases.
% By applying dimension reduction through SVD, our algorithm remains applicable even when the matrix of feature vectors is not full rank. 
% Furthermore, the results suggest that our algorithm demonstrates superior performance even in the absence of partial observability.

\paragraph{Scenario (iii).}  
We evaluate performance when the number of arms is less than the dimension of observed features, setting $d = 2K$ and varying $K$ as 15, 20, and 30, with $d_z = d$. 
Before using the features in our algorithms, we apply singular value decomposition (SVD) for dimensionality reduction. \cref{fig:result3} shows that our algorithm performs well even in extreme cases. 
By applying dimension reduction through SVD, our algorithm remains applicable even when the matrix of feature vectors is not full rank. 
Furthermore, the results suggest that our algorithm demonstrates superior performance even in the absence of partial observability.



\section{Conclusion}
\label{sec:conclusion}
In this paper, we addressed the challenges due to partially observable features in linear bandit problems. 
We showed that algorithms relying solely on observed features may suffer linear regret due to model misspecification and introduced \texttt{RoLF}, a novel algorithm that incorporates latent features without prior knowledge of them. 
Our algorithm improves the regret bound over traditional methods that ignore latent features, with numerical experiments supporting the theoretical results. 
Moving forward, several open directions remain for future work. 
First, while the linear bandit framework allows our feature augmentation strategy in~\cref{subsec:feature_augmentation} to be interpreted as another linear bandit problem, extending this approach to more general reward functions, such as generalized linear models, would be interesting.
Furthermore, as noted in~\cref{subsec:problem_formulation}, we assumed the latent portion of reward to be linear with respect to the unobserved features. 
However, the unobserved reward can be viewed as an exogenous factor affecting the agent's learning. 
This perspective enables extending the unobserved reward to a general function class without structural assumptions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% References
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \clearpage

\bibliographystyle{plainnat}
\bibliography{references}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Appendix
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\appendix
\onecolumn

\renewcommand{\contentsname}{Contents of Appendix}
\addtocontents{toc}{\protect\setcounter{tocdepth}{2}}
{
  \hypersetup{hidelinks}
  \tableofcontents
}

\clearpage
\section{Robust to Latent Feature Algorithm with Ridge Estimator}
\label{appendix:dr_ridge}

\begin{algorithm}[t]
\caption{Robust to Latent Feature with Ridge Estimator~(\texttt{RoLF-Ridge}) }
\label{alg:RoLF_ridge}
\begin{algorithmic}[1]
\State \textbf{INPUT:} features $\{\obs{}: a\in[K]\}$, coupling probability $p\in(1/2,1)$, confidence parameter $\delta>0$.
\State Initialize $\widehat{\mub}_0 = \mathbf{0}_K$, the exploration phase $\Ecal_t=\emptyset$ and the exploration factor $C_{\mathrm{e}}:=32(1-p)^{-2}K^2$.
%\State Initialize $\widehat{\mub}_0 = \mathbf{0}_K$, the exploration phase $\Ecal_t=\emptyset$ and the exploration factor $C_{\mathrm{e}}:=(\sqrt{16K}+1)^{2}2(1-p)^{-2}K^{2}\tilde{\sigma}_{\max}^{2}\tilde{\sigma}_{\min}^{-2}$.
\State Find orthogonal basis $\bb_1,\ldots,\bb_{K-d}$ in $\mathrm{R}(\Xb)^{\perp}$ to construct $\{\tildeobs{}:a\in[K]\}$
\For{$t=1,\ldots,T$}
\If{$|\Ecal_t| \le  C_{\mathrm{e}} \log (2Kt^2/\delta)$}
\State Randomly sample $a_t$ uniformly over $[K]$ and $\Ecal_t = \Ecal_{t-1}\cup \{t\}$.
\Else
\State Compute $\widehat{a}_t := \arg\max_{a\in[K]}\tildeobs{}^\top \widehat{\mub}^{R}_{t-1}$
\While{$\tilde{a}_t \neq a_t$ and $\text{count} \leq \rho_t$}
\State Sample $a_t$ with $\PP(a_t = \widehat{a}_t)=1-(t^{-1/2})$ and $\PP(a_t =k)=t^{-1/2}/(K-1),\; \forall k\neq\widehat{a}_t$.
\State Sample $\tilde{a}_t$ according to ~\cref{eq:psuedo_distribution}.
\State $\text{count} = \text{count}+ 1$
\EndWhile
\EndIf
\State Play $a_t$ and observe $y_{a_t,t}$.
\If{$\tilde{a}_t \neq a_t$}
\State Set $\widehat{\mub}^{R}_{t}:=\widehat{\mub}^{R}_{t-1}$
\Else
% \State Compute $\widehat{\mub}^{R}_t$ in~\cref{eq:ridge_DR} ($\widehat{\mub}^{L}_t$ in~\cref{eq:lasso_DR} for Lasso) with pseudo-rewards $\tilde{y}_{a,t}$ in~\cref{eq:pseudoY} and the imputation estimator $\check{\mub}^{R}_t$ in~\cref{eq:ridge_impute} ($\check{\mub}^{L}_t$ in~\cref{eq:lasso_impute} for Lasso).
\State Update $\widehat{\mub}_t^R$ following~\cref{eq:ridge_DR} with $\tilde{y}_{a,t}$ and update $\check{\mub}_t^R$ following~\cref{eq:ridge_impute}.
\EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

Our Doubly robust (DR) ridge estimator is defined as follows:
\begin{equation}
\widehat{\mub}^{R}_t := \left(\sum_{\tau=1}^{t}\II(\Mcal_{\tau})\sum_{a\in[K]}\tildeobs{}\tildeobs{}^\top + I_K \right)^{-1} \left(\sum_{\tau=1}^{t} \II(\Mcal_{\tau}) \sum_{a\in[K]} \tildeobs{} \tilde{y}_{a,\tau}\right),
\label{eq:ridge_DR}
\end{equation}
where $\tilde{y}_{a,\tau}$ is the DR pseudo reward:
\[
\tilde{y}_{a,t}:={\tildeobs{}}^{\top}\check{\mub}_t^R+\frac{\II(\tilde{a}_{t}=a)}{\phi_{a,t}}\big(y_{a,t}-\tildeobs{}^{\top}\check{\mub}_t^R\big),
\]
and the imputation estimator $\check{\mub}^{R}_t$ is defined as
\begin{equation}
\check{\mub}^{R}_{t}:=\left(\sum_{\tau=1}^{t} \tildeobs{\tau}\tildeobs{\tau}^{\top} + p \Ib_K\right)^{-1} \left(\sum_{\tau=1}^{t} \tildeobs{\tau}y_{a_\tau,\tau} \right).
\label{eq:ridge_impute}
\end{equation}

The following theorem shows that this Ridge estimator is consistent, meaning it converges to the true parameter ${\mub}_{\star}$ with high probability as the agent interacts with the environment.

\begin{theorem}[Consistency of the DR Ridge estimator]
\label{thm:est_ridge}
% Let $\Vb_{t}:= t\sum_{a\in[K]}\tildeobs{}\tildeobs{}^{\top}$ denote the Gram matrix at round $t$.
%Let $\Vb_{t}:= \sum_{\tau=1}^t\sum_{a\in[K]}\tildeobs{}\tildeobs{}^{\top}$ denote the Gram matrix at round $t$.
For each $t$, let $\mathcal{E}_t \subseteq [t]$ denote an exploration phase such that for $\tau\in\mathcal{E}_t$ the action $a_{\tau}$ is sampled uniformly over $[K]$. 
Then for all round $t$ such that $|\mathcal{E}_t| \ge 32 (1-p)^{-2}K^{2}\log(2Kt^2/\delta)$, with probability at least $1-3\delta$,
\begin{equation*}
\max_{a\in[K]}|\tildeobs{}^\top(\widehat{\mub}^{R}_t-{\mub}_{\star})| \le\frac{2}{\sqrt{t}}\left(\frac{\sigma}{p}\sqrt{K\log\frac{t+1}{\delta}}+\sqrt{K}\right).
\end{equation*}
\end{theorem}

With $|\Ecal_t| = O(K^2 \log K t)$ number of exploration, the DR Ridge estimator achieves $O(\sqrt{K/t})$ convergence rate over all $K$ rewards.
This is possible because the DR pseudo-rewards defined in~\cref{eq:pseudoY} impute the missing rewards for all arms $a\in[K]$ using $\tildeobs{}^\top\check{\mub}_t$, based on the samples collected during the exploration phase, $\Ecal_t$.
With this convergence guarantee, we establish a regret bound for \texttt{RoLF-Ridge}, which is the adaptation of~\cref{alg:RoLF} using the Ridge estimator.
\begin{theorem}[Regret bound for Ridge \texttt{RoLF}]
\label{thm:ridge_regret}
For $\delta\in(0,1)$, the expected cumulative regret of the proposed algorithm using DR Ridge estimator is bounded by
\begin{equation*}
\regret(T)\le 6\delta\log T+\frac{2p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}+\frac{32K^{2}}{(1-p)^{2}}\log\frac{2dT^{2}}{\delta}+8\sqrt{KT}\left(\frac{\sigma}{p}\sqrt{\log\frac{T^2}{\delta}}+1\right).
\end{equation*}
\end{theorem}

The first and second terms come from the distribution of $a_t$ which is a combination of the $1-t^{-1/2}$-greedy policy and resampling up to~$\rho_t:=\log((t+1)^2/\delta)/\log(1/p)$ trials.
The third term is determined by the size of the exploration set, $\Ecal_t$, while the last term arises from the estimation error bounded by the DR estimator as described in~\cref{thm:est_ridge}.
The hyperparameter $p\in(1/2,1)$ balances the size of the exploration set in the third term and the estimation error in the last term.
Overall, the regret is $O(\sqrt{KT \log T})$, which shows a significant improvement compared to the regret lower bound in~\cref{thm:regret_linear_lower_bound} for any linear bandit algorithms that do not account for unobserved features and unobserved rewards.
% \sw{(As I have found of, the regret of \texttt{UCB} for MAB is $O(\sqrt{KT\log T})$... what about we just skip this algorithm?)}

\section{A Modified Algorithm for Time-Varying Observed Features}
\label{sec:time_varying}
In this section, we propose an algorithm for linear bandits with partially observable features under the setting where the observed features vary over time.


\subsection{Problem Formulation}

Let $\xb_{1,t},\ldots,\xb_{K,t}$ denote the observed features and $\ub_{1},\ldots,\ub_{K}$ denote the unobserved features.
Now the observed features arbitrary changes over $t$ but the unobserved features are fixed over time.
When the algorithms selects an arm $a_t$, the reward is 
\[y_{a_t,t}= \langle \xb_{a_t,t}, \thetab_{\star}^{(o)} \rangle + \langle \ub_{a_t},\thetab_{\star}^{(u)}\rangle + \epsilon_t,
\] where the $\epsilon_t$ is the Sub-Gaussian noise that follows \cref{asm:sub-gaussian}.
% Under \cref{asm:fixed_features}, 
% the features are fixed and 
The expected reward of each arm is sta over time, where MAB algorithms without using features are applicable to achieve $\tilde{O}(\sqrt{KT})$ regret bound.
When the observed features vary over time, the expected reward of each arm $\EE[y_{a_t},t]=\langle \xb_{a_t,t}, \thetab_{\star}^{(o)} \rangle + \langle \ub_{a_t},\thetab_{\star}^{(u)}\rangle$ also arbitrarily changes over time and MAB algorithms suffer regret linear in $T$.
To our knowledge, there is no other work that address this challenging setting. 

\subsection{Proposed Method: Orthogonal Basis Augmentation}
We address the problem by augmenting Euclidean basis $\eb_1,\ldots,\eb_K$ in $\RR^K$ to estimate bias caused by the unobserved features.
Let $\tilde{\xb}_{a,t} := \eb_a^\top [\Xb_t \; \eb_1 \; \cdots \; \eb_K] \in \RR^{d+K}$ and let $\Delta_{a} := \langle \ub_a, \theta_{\star}^{(u)} \rangle$ denote the bias stems from the latent features.
Then,
\begin{align*}
y_{a,t} &= \langle \xb_{a,t}^\top \thetab_{\star}^{(o)} \rangle + \langle \ub_{a,t}^\top \thetab_{\star}^{(u)} \rangle + \epsilon_{a,t}
\\ &= \langle \eb_a^\top [\Xb_t \eb_1 \cdots \eb_K],  [\thetab_{\star}^{(o)} \Delta_1 \cdots \Delta_K]\rangle + \epsilon_{a,t}.
\end{align*}
Therefore, applying the \texttt{RoLF-Ridge} algorithm to the new features $\tilde{\xb}_{a,t} := \eb_a^\top [\Xb_t \eb_1 \cdots \eb_K]$ yields the following regret bound.

\begin{theorem}[Regret bound for \texttt{Ridge-RoLF-V} with time varying observed features]
\label{thm:regret_ridge_time_varying}
If observed features are vary over time, for $\delta\in(0,1)$, the expected cumulative regret of the proposed algorithm \texttt{Ridge-RoLF-V} using DR Ridge estimator is bounded by
\begin{equation*}
\begin{split}
\regret(T)\le &6\delta\log T+\frac{2p\sqrt{T}}{d+K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}+\frac{32(K+d)^{2}}{(1-p)^{2}}\log\frac{2(K+d)T^{2}}{\delta}
\\
&+8\sqrt{(d+K)T}\left(\frac{\sigma}{p}\sqrt{\log\frac{T^2}{\delta}}+1\right).
\end{split}
\end{equation*}
\end{theorem}

The proof is similar to that in \cref{thm:ridge_regret} and we omit the proof.
The rate of the regret bound is $\tilde{O}(\sqrt{(d+K)T})$ and, to our knowledge, this is the first sublinear regret bound for the partially observable linear bandits (as well as misspecified linear bandits) with arbitral time-varying observed features.

\section{Missing Proofs}
\label{appendix:proofs}
\subsection{Proof of \cref{thm:regret_linear_lower_bound}}
\label{appendix:proof_thm1}
Throughout this paper, we consider a bandit problem where the agent observes only a subset of the reward-generating feature vector and cannot access or estimate the unobserved portion.
If the agent uses online decision-making algorithms that rely solely on observed features, as defined in \cref{def:obs_dependent_policy}, the resulting issue can be interpreted as a model misspecification.
Therefore, in this theorem, we present a problem instance where ``misspecified'' algorithms, considering only observed features, may incur regret that grows linearly in $T$.

Following the statement of~\cref{thm:regret_linear_lower_bound}, we assume that $d = d_u = 1$, which means $d_z = 2$.
Given the true feature set $\mathcal{Z} = \{[1,3]^\top, [2,19/4]^\top\}$, let the first element of each vector is observed to the agent; while the second element remains unobserved.
This results in $\xb_1 =x_1= 1$, $\xb_2 =x_2= 2$, $\ub_1 =u_1= 3$ and $\ub_2=u_2 = 19/4$.
We set the true parameter as $\thetaopt\in\RR^2 = [2, -1]^\top$, meaning $\thetao = \theta_\star^{(o)} = 2$ and $\thetau= \theta_\star^{(u)} = -1$.
Using the reward function from \cref{subsec:problem_formulation} and considering \cref{asm:sub-gaussian}, the expected reward for each arm is given by
\begin{equation*}
    \gamma_i:=\mathbb{E}[y_i]= \mathbf{z}_i^\top \theta_\star = x_i\theta_\star^{(o)} + u_i\theta_\star^{(u)}\quad \forall\ i\in\{1,2\}.
\end{equation*}
Plugging the values in, the true mean reward for each arm is directly computed as $\gamma_1 = 2-3 = -1$ and $\gamma_2 = 4-19/4 = -3/4$, which satisfies the assumption that its absolute value does not exceed 1~(\cref{subsec:problem_formulation}), and since $\gamma_1 < \gamma_2$, the arm 2 is the optimal action.

For brevity, we denote the latent reward components as $g_1:= u_1\theta_\star^{(u)}$ and $g_2:= u_2\theta_\star^{(u)}$, yielding that $g_1 = -3$ and $g_2 = -19/4$.
Note that since $\gamma_2 \ne 2\gamma_1$ and $\abs{g_i} \ge 3 > 0$ for all $i\in\{1,2\}$, thus our problem setup satisfies the ``large deviation'' criterion in Definition 1 and Theorem 2 of~\citet{ghosh2017misspecified}, by letting $l = 3$ and $\beta=0$.
Consequently, by applying the theorem, it follows that using \texttt{OFUL} in this problem instance results in linear regret with respect to $T$, i.e., $\Omega(T)$.
Inspired by this theorem, we demonstrate that \texttt{LinTS} encounters the same issue.


For each round $t\in[T]$, \texttt{LinTS} estimates the true parameter using the ridge estimator, given by:
\begin{equation}
\begin{split}
    \hat{\thetab}_t 
    &= (\Xb_t^\top \Xb_t + \lambda \Ib_d)^{-1}(\Xb_t^\top \Yb_t)  \\
    &= (\Xb_t^\top \Xb_t + \lambda \Ib_d)^{-1}(\Xb_t^\top (\Xb_t\thetao + \gb_t+ \epsb_t)) \\
    &= \thetao - \lambda \Vb_t^{-1}\thetao + \Vb_t^{-1}\Xb_t^\top \gb_t+ \Vb_t^{-1}\Xb_t^\top\epsb_t,
\end{split}
\label{eq:thm1_ridge_decompose}
\end{equation}
where $\Xb_t:=(\xb_{a_1}^\top,\dots,\xb_{a_t}^\top)\in\RR^{t\times d}$ is a matrix containing features chosen up to round $t$, $\Yb_t := (y_{a_1},\dots,y_{a_t})\in\RR^t$ is a vector of observed rewards, and $\epsb_t := (\epsilon_1,\dots, \epsilon_t)\in\RR^t$ contains noise attached to each reward. Unlike a typical ridge estimator, here the term $\gb_t:= (g_{a_1},\dots, g_{a_t})\in\RR^{t}$, the vector containing the latent portion of observed rewards is introduced due to model misspecification.
Note that $\Vb_t := (\Xb_t^\top \Xb_t + \lambda \Ib_d)\succ 0$.

For this problem instance, since $d = 1$,~\cref{eq:thm1_ridge_decompose} is equivalent to:
\begin{equation*}
    \hat{\theta}_t = \theta_\star^{(o)} - \frac{\theta_\star^{(o)}}{\sum_{\tau=1}^tx_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^tx_{a_\tau}g_{a_\tau}}{\sum_{\tau=1}^tx_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}\epsilon_{\tau}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1},
\end{equation*}
where we assume $\lambda = 1$.
Note also that we denote $\hat{\thetab}_t$ and $\thetao$ by $\hat\theta_t$ and $\theta_\star^{(o)}$, respectively, since both are scalars. 
Hence, the estimation error is computed as:
\begin{equation}
\hat{\theta}_t - \theta_\star^{(o)} = -\frac{\theta_\star^{(o)}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}g_{a_\tau}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}\epsilon_{\tau}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1}.
\label{eq:thm1_ridge_est_error}
\end{equation}
Let $ N_1 $ and $ N_2 $ denote the number of times arms 1 and 2 have been played up to round $ t $, respectively. 
This implies that $N_1 + N_2 = t$.
Then, for the numerator of the second term, since
\begin{equation*}
\sum_{\tau=1}^t x_{a_\tau}g_{a_\tau} = (\underbrace{g_1+\cdots+g_1}_{N_1} + \underbrace{2g_2+\cdots+2g_2}_{N_2}) = g_1 N_1 + 2g_2 N_2,
\end{equation*}
we can observe that
\begin{align*}
    \sum_{\tau=1}^t x_{a_\tau}g_{a_\tau}
    &= g_1 N_1 + 2g_2 N_2 \\
    &\ge \bar{g}N_1 + 2\bar{g}N_2 \\
    &= \bar{g}N_1 + 2\bar{g}(t - N_1) \\
    &= 2\bar{g}t - \bar{g}N_1 \\
    &\ge \bar{g}t \qquad\qquad (\because N_1 \le t) \\
    &= -\frac{19}{4}t,
\end{align*}
where $\bar{g} = \min\{g_1,g_2\} = -19/4$, which implies that $\sum_{\tau=1}^t x_{a_\tau}g_{a_\tau} = \Theta(t)$.
For the denominator, $\sum_{\tau=1}^t x_{a_\tau}^{2} + 1$, as it grows at a rate of $O(t)$, implying that the second term in~\cref{eq:thm1_ridge_est_error} is $\Theta(1)$, and that $\hat{\theta}_t$ is not consistent since it does not converge to $\theta_\star^{(o)}$ as $t\to\infty$.

For arm 2, which is optimal, to be selected in round $ t+1 $ under \texttt{LinTS}, the condition $ x_2 \tilde\theta_t \ge x_1 \tilde\theta_t $ must hold, where $ \tilde\theta_t \sim \mathcal{N}\left( \hat{\theta}_t, \frac{v^2}{\sum_{\tau=1}^t x_{a_\tau}^2 + 1} \right) $.
Given the assumptions that $ x_1 = 1 $ and $ x_2 = 2 $, arm 2 is selected whenever $ \tilde\theta_t \ge 0 $. Thus, for arm 1 to be chosen, we require $ \tilde\theta_t < 0 $. 
We will show that the probability of $ \tilde\theta_t < 0 $ does not diminish sufficiently to be ignored even the agent plays sufficiently large amount of time.
To clarify, let us define two events $E_{\tilde\theta} := \{\tilde\theta_t \ge 0\}$ and $E_{\hat\theta} := \{\hat\theta_t \ge 0\}$.

We revisit~\cref{eq:thm1_ridge_decompose} as follows:
\begin{align}
    \hat{\theta}_t &= \frac{\theta_\star^{(o)}\sum_{\tau=1}^t x_{a_\tau}^2}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}g_{a_\tau}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}\epsilon_{\tau}}{\sum_{\tau=1}^t x_{a_\tau}^{2} + 1} \notag \\
    &\le \theta_{\star}^{(o)} +\frac{g_1 N_1 + 2g_2 N_2}{N_1+4N_2 + 1} + \frac{\sum_{\tau=1}^t x_{a_\tau}\epsilon_{\tau}}{N_1+4N_2 + 1} \qquad\qquad\quad (\because \theta_\star^{(o)} > 0)\notag \\
    &\le \theta_{\star}^{(o)} +\frac{g_1 N_1 + 2g_2 N_2}{N_1+4N_2 + 1} + \frac{\max_{a\in\{1,2\}} x_a}{N_1+4N_2+1}\sum_{\tau=1}^{t}\epsilon_\tau \notag \\
    &= \theta_{\star}^{(o)} +\frac{g_1 N_1 + 2g_2 N_2}{N_1+4N_2 + 1} + \frac{2}{N_1+4N_2+1}\sum_{\tau=1}^{t}\epsilon_\tau, \label{eq:thm1_ridge_decompose_bound}
\end{align}
where for the second term of~\cref{eq:thm1_ridge_decompose_bound}, for $t \ge 20$, it is upper bounded by:
\begin{align*}
    \frac{g_1 N_1 + 2g_2 N_2}{N_1+4N_2 + 1} 
    & = \frac{-3N_1-\frac{19}{2}N_2}{N_1+4N_2+1} \\
    &\le \frac{-3N_1-\frac{19}{2}N_2}{N_1+4N_2+1} \\
    &\le \frac{-\frac{19}{8}(N_1+4N_2)}{N_1+4N_2+1} \\
    &= -\frac{19}{8} + \frac{19/8}{t+3N_2+1} \\
    &\le -\frac{19}{8} + \frac{19}{8t} \\
    &\le -\frac{9}{4},
\end{align*}

resulting in
\begin{equation}
\begin{split}
    \hat\theta_t
    &\le \theta_{\star}^{(o)} +\frac{g_1 N_1 + 2g_2 N_2}{N_1+4N_2 + 1} + \frac{2}{N_1+4N_2+1}\sum_{\tau=1}^{t}\epsilon_\tau \\
    &\le 2-\frac{9}{4} + \frac{2}{t+3N_2+1}\sum_{\tau=1}^t\epsilon_\tau \\
    &= -\frac{1}{4}+ \frac{2}{t+3N_2+1}\sum_{\tau=1}^t\epsilon_\tau \\
    &\le -\frac{1}{4}+ \frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau
\end{split}
\label{eq:ridge_upper_bound}
\end{equation}
Thus, we have the following: 
\begin{equation*}
    \PP(\hat\theta_t > 0) \le \PP\left(-\frac{1}{4}+ \frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau> 0\right) = \PP\left(\frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau> \frac{1}{4}\right).
\end{equation*}
Since $\epsilon_\tau$ is an IID sub-Gaussian random variable for all $\tau\in[t]$, by applying Hoeffding inequality we obtain:
\begin{equation*}
    \PP(\hat\theta_t > 0) \le \exp\left(-\frac{t}{128\sigma^2}\right).
\end{equation*}
Given this, we now bound the probability of the event $E_{\tilde{\theta}}$:
\begin{align}
    \mathbb{P}(E_{\tilde\theta}) &= \mathbb{P}(E_{\tilde\theta}\cap E_{\hat\theta}) + \mathbb{P}(E_{\tilde\theta}\cap E_{\hat\theta}^c) \notag \\
&= \mathbb{P}(E_{\tilde\theta}|E_{\hat\theta})\cdot \mathbb{P}(E_{\hat\theta}) + \mathbb{P}(E_{\tilde\theta} | E_{\hat\theta}^c) \cdot \mathbb{P}(E_{\hat\theta}^c) \notag \\
&= \mathbb{P}(\tilde\theta_t \ge 0|\hat\theta_t \ge 0)\cdot \mathbb{P}(\hat\theta_t \ge 0) + \mathbb{P}(\tilde\theta_t \ge 0|\hat\theta_t < 0)\cdot \mathbb{P}(\hat\theta_t < 0) \notag \\
&\le \exp\left(-\frac{t}{128\sigma^2}\right) + \mathbb{P}(\tilde\theta_t \ge 0|\hat\theta_t < 0). \label{eq:thm1_prob_bound}
\end{align}
Since the second term of~\cref{eq:thm1_prob_bound} is calculated under a Gaussian distribution, which does not exceed $1/2$ for all $t\in[T]$,
\begin{equation*}
\mathbb{P}(E_{\tilde\theta}^c) \ge \frac{1}{2} - \exp\left(-\frac{t}{128\sigma^2}\right).
\end{equation*}
Note that the total decision horizon $T > 256\sigma^2\log(1/\delta^\prime)$.
For any $t > 128\sigma^2\log(1/\delta^\prime)$, we have $\mathbb{P}(E_{\tilde\theta}^c) \ge 1/2 - \delta^\prime$
This implies that for the rounds more than $T/2$, the suboptimal arm is expected to be played at least $(1/2-\delta^\prime)T/2$ times for any $\delta^\prime\in(0,1/2)$, thus incurring
\begin{equation*}
    \EE[\regret_{\texttt{LinTS}}(T)] \ge (\gamma_2-\gamma_1)\left(\frac{1}{2}-\delta^\prime\right)\frac{T}{2} = \frac{1}{4}\left(\frac{1}{2}-\delta^\prime\right)\frac{T}{2}.
\end{equation*}

For \texttt{OFUL}, we also present another analysis that requires no assumption such that the suboptimal arm is played for initial $t$ rounds, which is taken in Theorem 2 of~\citet{ghosh2017misspecified}.
The optimal arm, arm 2, is selected when 
\begin{equation}
x_2 \widehat{\theta}_t + \frac{x_2}{\sqrt{1 + \sum_{\tau=1}^{t-1}x_{a_\tau}^2 }} > x_1 \widehat{\theta}_t + \frac{x_1}{\sqrt{1 + \sum_{\tau=1}^{t-1}x_{a_\tau}^2 }},
\label{eq:OFUL_condition}
\end{equation}
where $\widehat{\theta}_t$ is the same ridge estimator as in \texttt{LinTS}.
The inequality \cref{eq:OFUL_condition} is equivalent to $\widehat{\theta}_t > (1 + \sum_{\tau=1}^{t-1}x_{a_\tau}^2 )^{-1/2}$, which implies $\widehat{\theta}_t > 1/\sqrt{2t}$.
By \cref{eq:ridge_upper_bound}, 
\begin{align*}
\PP\left(\widehat{\theta}_t > 1/\sqrt{2t} \right) &\le \PP\left( -\frac{1}{4}+ \frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau > \frac{1}{\sqrt{2t}}\right) \\
&= \PP\left(\frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau > \frac{1}{\sqrt{2t}}+\frac{1}{4}\right) \\
&\le \PP\left(\frac{2}{t}\sum_{\tau=1}^t\epsilon_\tau > \frac{1}{4}\right) \le \exp\left(-\frac{t}{128\sigma^2}\right).
\end{align*}
Thus, for $t \ge 128\sigma^2 \log (2)$, the probability of selecting arm 2 is less than $1/2$ and for $ T > 256 \sigma^2 \log (2)$,
\[
\EE[\regret_{\texttt{OFUL}}(T)] \ge (\gamma_2-\gamma_1)\cdot\frac{1}{2} \cdot \frac{T}{2} = \frac{T}{16},
\]
and the algorithm suffers expected regret linear in $T$.
\hfill \qed

\subsection{Proof of \cref{thm:est_lasso}}
\label{appendix:proof_thm3}
Let $\Vb_{t}:=\sum_{\tau=1}^{t}\sum_{a\in[K]}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}$.
Then
\begin{equation*}
\max_{a\in[K]} |\tildeobs{}^\top (\widehat{\mub}_t-{\mub}_{\star})| \le \sqrt{\sum_{a\in[K]} |\tildeobs{}^\top (\widehat{\mub}_t-{\mub}_{\star})|^2} = t^{-1/2} \|\widehat{\mub}_t-{\mub}_{\star}\|_{\Vb_t}
\end{equation*}
Recall that $\widehat{\wb}_t := \Vb_t^{1/2}\widehat{\mub}_t$ and $\wb_t := \Vb_t^{1/2}\mub_{\star}$.
Then
\[
\max_{a\in[K]} |\tildeobs{}^\top (\widehat{\mub}_t-{\mub}_{\star})| \le t^{-1/2} \norm{\widehat{\wb}_t-\wb_t}_2.
\]
To use Lemma~\ref{lem:lasso}, we prove a bound for 
\begin{equation*}
\left\Vert \sum _{\tau=1}^{t} \sum_{a\in[K]} (\tilde{y}_{a,\tau} - \tildeobs{}^\top \Vb_t^{-1/2} {\wb}_{t}) \Vb_t^{-1/2}\tildeobs{} \right\Vert_{\infty}.
\end{equation*}
Let $\check{\wb}_{t}^{L}:=\Vb_{t}^{1/2}\check{\mub}_{t}^{L}$.
By definition of $\tilde{y}_{a,\tau}$,
\begin{align*}
&\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}(\tilde{y}_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t})\Vb_{t}^{-1/2}\tilde{\xb}_{a}\right\Vert _{\infty}.\\&=\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\left(1-\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)+\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\left(y_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t}\right)\Vb_{t}^{-1/2}\tilde{\xb}_{a}\right\Vert _{\infty}\\&\le\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\left(1-\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}\\&\quad+\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\left(y_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\right\Vert _{\infty}
\end{align*}
With probability at least $1-\delta$, the event $\Mcal_{\tau}$ happens for all $ \tau \ge 1$ and we obtain a pair of matching sample $\tilde{a}_{\tau}$ and $a_{\tau}$. 
Thus, the second term is equal to,
\begin{align*}
\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\left(y_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\right\Vert _{\infty}&=\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(a_{\tau}=a)}{\phi_{a,\tau}}\left(y_{a,\tau}-\mathbf{\tilde{x}}_{a}^{\top}\mub_{\star}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\right\Vert _{\infty}\\&=\frac{1}{p}\left\Vert \sum_{\tau=1}^{t}\epsilon_{a,\tau}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}\right\Vert _{\infty}.
\end{align*}
Because $\norm{\vb}_\infty = \max_{i\in[d]}\abs{e_i^\top \vb}$ for any $\vb\in\RR^{d}$,
\[
\frac{1}{p}\left\Vert \sum_{\tau=1}^{t}\epsilon_{a,\tau}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}\right\Vert _{\infty}=\frac{1}{p}\max_{a\in[K]}\abs{\sum_{\tau=1}^{t}\epsilon_{a,\tau}\eb_{a}^{\top}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}}
\]
Applying Lemma~\ref{lem:exp_bound}, with probability at least $1-\delta/t^2$,
\begin{align*}
\max_{a\in[K]}\abs{\sum_{\tau=1}^{t}\epsilon_{a,\tau}\eb_{a}^{\top}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}}&\le\max_{a\in[K]}\sigma\sqrt{2\sum_{\tau=1}^{t}\left(\eb_{a}^{\top}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}\right)^{2}\log\frac{2Kt^{2}}{\delta}}\\&=\max_{a\in[K]}\sigma\sqrt{2\eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\sum_{\tau=1}^{t}\mathbf{\tilde{x}}_{a_{\tau}}\mathbf{\tilde{x}}_{a_{\tau}}^{\top}\right)\Vb_{t}^{-1/2}\eb_{a}\log\frac{2Kt^{2}}{\delta}}\\&\le\max_{a\in[K]}\sigma\sqrt{2\eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}\right)\Vb_{t}^{-1/2}\eb_{a}\log\frac{2Kt^{2}}{\delta}}\\
&=\sigma\sqrt{2\log\frac{2Kt^{2}}{\delta}},
\end{align*}
and thus,
\begin{equation}
\frac{1}{\sqrt{p}} \left\Vert \sum_{\tau=1}^{t}\epsilon_{a,\tau}\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a_{\tau}}\right\Vert _{\infty} \le \sigma\sqrt{\frac{2}{p}\log\frac{2Kt^{2}}{\delta}} 
\label{eq:est_error_norm}
\end{equation}
Let $\Ab_{t}:=\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}$.
Then the first term,
\begin{equation}
\begin{split}
&\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}\left(1-\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}\\
&=\left\Vert \Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}.
\end{split}
\label{eq:est_1st_term}
\end{equation}
Since $\norm{\vb}_\infty = \max_{i\in[d]}\abs{\eb_i^\top \vb}$ for any $\vb\in\RR^{d}$,
\begin{align*}
\left\Vert \Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}=&\max_{a\in[K]}|\eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)|\\
\le&\max_{a\in[K]}\left\Vert \eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right\Vert _{2}\!\!\left\Vert \check{\wb}_{t}^{L}-\wb_{t}\right\Vert _{2}.
\end{align*}
Because $\widehat{\wb}_t$ is a minimizer of \cref{eq:lasso_impute}, by Lemma~\ref{lem:lasso} and \cref{eq:est_error_norm},
\begin{equation*}
\left\Vert \check{\wb}_{t}^{L}-\wb_{t}\right\Vert _{\Vb_{t}^{-1/2}\frac{1}{p}\sum_{\tau=1}^{t}\mathbf{\tilde{x}}_{a_{\tau}}\mathbf{\tilde{x}}_{a_{\tau}}^{\top}\Vb_{t}^{-1/2}}\le4\sigma\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p\lambda_{\min}\left(\Vb_{t}^{-1/2}\frac{1}{p}\sum_{\tau=1}^{t}\mathbf{\tilde{x}}_{a_{\tau}}\mathbf{\tilde{x}}_{a_{\tau}}^{\top}\Vb_{t}^{-1/2}\right)}}.
\end{equation*}
Because $\phi_{a_\tau,\tau}=p$ and the coupling event $\cap_{\tau \ge 1}\Mcal_\tau$ holds with probability at least $1-\delta/t^2$,
\begin{align*}
\sum_{\tau=1}^{t}\frac{1}{p}\mathbf{\tilde{x}}_{a_{\tau}}\mathbf{\tilde{x}}_{a_{\tau}}^{\top}
&=\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(a_{\tau}=a)}{p}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\\
&=\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(a_{\tau}=a)}{\phi_{a,\tau}}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\\
&=\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\\
&:=\Ab_{t}.
\end{align*}
Thus, under the coupling event $\cap_{\tau=1}^{t}\Mcal_\tau$,
\[
\left\Vert \check{\wb}_{t}^{L}-\wb_{t}\right\Vert _{\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}}\le4\sigma\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p\lambda_{\min}\left(\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\right)}}.
\]
By \cref{cor:Gram_matrix}, with $\epsilon \in (0,1)$ to be determined later, for $t\ge8\epsilon^{-2}(1-p)^{-2}K^{2}\log\frac{2dt^{2}}{\delta}$, with probability at least $1-\delta/t^2$,
\begin{equation}
\left\Vert \Ib_{K}-\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\right\Vert_{2} \le \epsilon,
\label{eq:lasso_Gram_bound}
\end{equation}
which implies, $(1-\epsilon) \Ib_K \preceq \Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}$
Thus,
\[
\left\Vert \check{\wb}_{t}^{L}-\wb_{t}\right\Vert _{2}\le\frac{4\sigma}{1-\epsilon}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p}}.
\]
Now \cref{eq:est_1st_term} is bounded by,
\begin{equation}
\begin{split}
&\left\Vert \Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}\\
&\le\max_{a\in[K]}\left\Vert \eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right\Vert _{2}\frac{4\sigma}{1-\epsilon}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p}}.
\end{split}
\label{eq:est_1st_term_2}
\end{equation}
With simple algebra,
\begin{align*}
&\max_{a\in[K]}\left\Vert \eb_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right\Vert _{2}\\&=\max_{a\in[K]}\sqrt{\lambda_{\max}\left(\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\eb_{a}\eb_{a}^{\top}\right)\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right)}\\&\le\max_{a\in[K]}\sqrt{\lambda_{\max}\left(\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\Ib_{K}\Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right)}\\&=\left\Vert \Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\right\Vert _{2}\\&=\left\Vert \Ib_{K}-\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\right\Vert _{2}\le\epsilon
\end{align*}
Thus,
\[
\left\Vert \Vb_{t}^{-1/2}\left(\Vb_{t}-\Ab_{t}\right)\Vb_{t}^{-1/2}\left(\check{\wb}_{t}^{L}-\wb_{t}\right)\right\Vert _{\infty}\le\frac{4\sigma\epsilon}{1-\epsilon}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p}}.
\]
Now we obtain,
\begin{align*}
\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}(\tilde{y}_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t})\Vb_{t}^{-1/2}\tilde{\xb}_{a}\right\Vert _{\infty}&\le\frac{4\sigma\epsilon}{1-\epsilon}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{p}}+\frac{\sigma}{p}\sqrt{2\log\frac{2Kt^{2}}{\delta}}\\&\le\frac{4\sigma\epsilon}{1-\epsilon}\sqrt{\frac{2K\log\frac{2Kt^{2}}{\delta}}{p}}+\frac{\sigma}{p}\sqrt{2\log\frac{2Kt^{2}}{\delta}}\\&=\left(\frac{4\epsilon\sqrt{K}}{1-\epsilon}+\frac{1}{p}\right)\sigma\sqrt{2\log\frac{2Kt^{2}}{\delta}}
\end{align*}
Setting $\epsilon = p^{-1}/(\sqrt{K}+p^{-1})$ gives $\frac{\epsilon \sqrt{K}}{{1-\epsilon}} = p^{-1}$ and for $t\ge8(\sqrt{K}+p^{-1})^2p^2(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$,
\[
\left\Vert \sum_{\tau=1}^{t}\sum_{a\in[K]}(\tilde{y}_{a,\tau}-\tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\wb_{t})\Vb_{t}^{-1/2}\tilde{\xb}_{a}\right\Vert _{\infty}\le\frac{5\sigma}{p}\sqrt{2\log\frac{2Kt^{2}}{\delta}}
\]
Because $\widehat{\wb}_t$ is a minimizer of \eqref{eq:lasso_DR}, by \cref{lem:lasso},
\[
\left\Vert \widehat{\wb}_{t}-\wb_{t}\right\Vert _{\Vb_{t}^{-1/2}(\sum_{\tau=1}^{t}\tilde{\xb}_{a}\tilde{\xb}_{a})\Vb_{t}^{-1/2}}\le\frac{20\sigma}{p}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{\lambda_{\min}\left(\Vb_{t}^{-1/2}(\sum_{\tau=1}^{t}\tilde{\xb}_{a}\tilde{\xb}_{a})\Vb_{t}^{-1/2}\right)}},
\]
which is equivalent to,
\[
\left\Vert \widehat{\wb}_{t}-\wb_{t}\right\Vert _{2}\le\frac{20\sigma}{p}\sqrt{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}.
\]
This concludes,
\[
\max_{a\in[K]}|\tilde{\xb}_{a}(\widehat{\mub}_{t}-\mub_{\star})|\le\frac{20\sigma}{p}\sqrt{\frac{2(d+d_{h})\log\frac{2Kt^{2}}{\delta}}{t}},
\]
which conmpletes the proof. \hfill\qed


\subsection{Proof of~\cref{thm:regret}}
\label{appendix:proof_thm5}
Because the regret is bounded by $2$ and the number of rounds for the exploration phase is at most $|\Ecal_{T}|\le8(\sqrt{K}+p^{-1})^2p^2(1-p)^{-2}K^{2}\log\frac{2KT^{2}}{\delta}$.
\begin{align*}
\regret(T)&\le \frac{16K^{2}(\sqrt{K}+p^{-1})^{2}}{(1-p)^{2}}\log\frac{2Kt^{2}}{\delta}+\sum_{t\in [T]\setminus \Ecal_T}\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\\
= & \frac{16K^{2}(\sqrt{K}+p^{-1})^{2}}{(1-p)^{2}}+\sum_{t\in [T]\setminus \Ecal_T}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \\
 & +\sum_{t\in [T]\setminus \Ecal_T}\left\{ \II\left(a_{t}\neq\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} .
\end{align*}
By \cref{thm:est_lasso}, on the event $\{a_{t}=\widehat{a}_{t}\}$, 
\begin{align*}
\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]
&= \tilde{\boldsymbol{x}}_{a_{\star}}^{\top}{\mub}_{\star}-\tilde{\boldsymbol{x}}_{\widehat{a}_{t}}^{\top}{\mub}_{\star}\\
&\le 2\max_{a\in[K]}\left|\tilde{\xb}_{a}^{\top}\left({\mub}_{\star}-\widehat{\boldsymbol{\mu}}_{t-1}^{L}\right)\right|+\tilde{\boldsymbol{x}}_{a_{\star}}^{\top}\widehat{\boldsymbol{\mu}}_{t-1}^{L}-\tilde{\boldsymbol{x}}_{\widehat{a}_{t}}^{\top}\widehat{\boldsymbol{\mu}}_{t-1}^{L}\\
&\le 2\max_{a\in[K]}\left|\tilde{\xb}_{a}^{\top}\left({\mub}_{\star}-\widehat{\boldsymbol{\mu}}_{t-1}^{L}\right)\right|\\
&\le  \frac{40\sigma}{p}\sqrt{\frac{2(d+d_{h})}{t}\log\frac{2Kt^{2}}{\delta}},
\end{align*}
with probability at least $1-5\delta/t^2$ for each $t\in [T]\setminus \Ecal_T$.
Summing over $t$ gives,
\begin{equation*}
 \sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \le\frac{80\sigma}{p}\sqrt{\frac{2(d+d_{h})}{t}\log\frac{2Kt^{2}}{\delta}}.
\end{equation*}
By resampling at most $\rho_{t}$ times, the probability of the event
$\{a_{t}\neq\widehat{a}_{t}\}$ is 
\begin{align*}
\PP\left(a_{t}\neq\widehat{a}_{t}\right)= & \sum_{m=1}^{\rho_{t}}\frac{p}{(K-1)\sqrt{t}}\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{m-1} \cdot \PP(\text{Resample succeed at trial }m)\\
\le& \sum_{m=1}^{\rho_{t}}\frac{p}{(K-1)\sqrt{t}}\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{m-1}\\
= & \frac{p}{(K-1)\sqrt{t}}\left(\frac{p}{(K-1)\sqrt{t}}\right)^{-1}\left\{ 1-\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{\rho_{t}}\right\} \\
= & 1-\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{\rho_{t}}\\
\le & \frac{p\rho_{t}}{(K-1)\sqrt{t}},
\end{align*}
where the last inequality uses $(1+x)^{n}\ge1+nx$ for $x\ge-1$ and
$n\in\NN$. Then the expected sum of regret, 
\begin{align*}
 & \EE\left[\sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \right]\\
 & \le2\sum_{t\in[T]\setminus\Ecal_{T}}\PP\left(a_{t}\neq\widehat{a}_{t}\right)\\
 & \le\frac{4p\sqrt{T}}{K-1}\rho_{T}\\
 & =\frac{4p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}.
\end{align*}
Thus,
\begin{align*}
\EE[\regret(T)] & \le 10 \delta \log T+\frac{16K^{2}(\sqrt{K}+p^{-1})^{2}}{(1-p)^{2}}+\frac{4p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}\\
 & \quad+\frac{80\sigma}{p}\sqrt{2(d+d_{h})T\log\frac{2KT^{2}}{\delta}},
\end{align*}
which concludes the proof. \hfill\qed

\subsection{Proof of \cref{thm:est_ridge}}
\label{appendix:proof_thm2}
Let $\tilde{\Vb}_{t}:=\sum_{\tau=1}^{t}\II(\mathcal{M}_{\tau})\sum_{a\in[K]}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}+\Ib_{K}$
and $\Vb_{t}:=\sum_{\tau=1}^{t}\sum_{a\in[K]}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}+\Ib_{K}$.
By definition of $\widehat{\mub}_{t}^{R}$,
\begin{equation*}
\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})=\tilde{\xb}_{a}^{\top}\tilde{\Vb}_t^{-1}\left\{ \sum_{\tau=1}^{t}\II(\Mcal_{\tau})\sum_{a\in[K]}\tilde{\xb}_{a}\left(\tilde{y}_{a,\tau}-\tilde{\xb}_{a}^{\top}{\mub}_{\star}\right)-{\mub}_{\star}\right\} .
\end{equation*}
By definition of the pseudo-rewards,
\begin{equation*}
\tilde{y}_{a,\tau}-\tilde{\xb}_{a}^{\top}{\mub}_{\star}=\left(1-\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,t}}\right)\tilde{\xb}_{a}^{\top}\left(\check{\mub}_t^{R}-{\mub}_{\star}\right)+\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\epsilon_{a,\tau}.
\end{equation*}
Let $\tilde{\Ab}_{t}:=\sum_{\tau=1}^{t}\II(\mathcal{M}_{\tau})\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,t}}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}+\Ib_{K}$
and $\Ab_{t}:=\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,t}}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}+\Ib_{K}$
Then,
\begin{equation*}
\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})=\tilde{\xb}_{a}^{\top}\tilde{\Vb}_{t}^{-1}\left\{ \left(\tilde{\Vb}_{t}-\tilde{\Ab}_{t}\right)\left(\check{\mub}_t^{R}-{\mub}_{\star}\right)+\sum_{\tau=1}^{t}\II(\Mcal_{\tau})\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\tilde{\xb}_{a}\epsilon_{a,\tau}-{\mub}_{\star}\right\} .
\end{equation*}
By definition of the imputation estimator $\check{\mub}_t$,
%\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}
\begin{align*}
\check{\mub}_t^{R}-{\mub}_{\star}=
&\left(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_\tau}\tilde{\boldsymbol{x}}_{a_\tau}^\top+p\Ib_{K}\right)^{-1} \left(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_\tau}\epsilon_{a_{\tau},\tau}-p{\mub}_{\star}\right) \\
=&\left(\sum_{\tau=1}^{t}\frac{1}{\phi_{a_{\tau},\tau}}\tilde{\boldsymbol{x}}_{a_\tau}\tilde{\boldsymbol{x}}_{a_\tau}^{\top}+\Ib_{K}\right)^{-1}\left(\sum_{\tau=1}^{t}\frac{1}{\phi_{a_{\tau},\tau}}\tilde{\boldsymbol{x}}_{a_\tau}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right)\\
&=\left(\sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}\tilde{\boldsymbol{x}}_{a_\tau}\tilde{\boldsymbol{x}}_{a_\tau}^{\top}+\Ib_{K}\right)^{-1}\left(\sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_\tau}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right),
\end{align*}
where the second equality holds because $\phi_{a_{\tau},\tau}=p$.
Under the coupling event $\cap_{\tau=1}^{t}\Mcal_{\tau}$, 
\begin{align*}
\sum_{\tau=1}^{t}\II(\Mcal_\tau)\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,t}}\tilde{\xb}_{a}\tilde{\xb}_{a}^{\top}+\Ib_{K}= & \sum_{\tau=1}^{t}\sum_{a\in[K]}\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,t}}\tilde{\xb}_{a}\tilde{\xb}_{a}^{\top}+\Ib_{K}\\
:= & \Ab_{t},
\end{align*}
and
\begin{align*}
\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})= & \tilde{\xb}_{a}^{\top}\Vb_{t}^{-1}\left\{ \left(\Vb_{t}-\Ab_{t}\right)\Ab_{t}^{-1}\left(\sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right)+\sum_{\tau=1}^{t}\frac{\epsilon_{a_{\tau}}}{\phi_{a_{\tau},\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}-{\mub}_{\star}\right\} \\
= & \tilde{\xb}_{a}^{\top}\Vb_{t}^{-1}\left\{ \left(\Vb_{t}-\Ab_{t}\right)\Ab_{t}^{-1}+\Ib_{K}\right\} \left(\sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right)\\
= & \tilde{\xb}_{a}^{\top}\Vb_{t}^{-1/2}\left(\Vb_{t}^{1/2}\Ab_{t}^{-1}\Vb_{t}^{1/2}\right)\Vb_{t}^{-1/2}\left(\sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right).
\end{align*}
Taking absolute value on both sides, by Cauchy-Schwarz inequality,
\begin{equation*}
\max_{a\in[K]}|\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})|\le\max_{a\in[K]}\|\tilde{\xb}_{a}\|_{\Vb_{t}^{-1}}\|\Vb_{t}^{1/2}\Ab_{t}^{-1}\Vb_{t}^{1/2}\|_{2}\left\Vert \sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right\Vert _{\Vb_{t}^{-1}}.
\end{equation*}
By Corollary which implies $\Ib_{K}-\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\preceq\epsilon\Ib_{K}.$
Rearraging the terms,
\begin{equation*}
\Vb_{t}^{1/2}\Ab_{t}^{-1}\Vb_{t}^{1/2}\preceq(1-\epsilon)^{-1}\Ib_{K}.
\end{equation*}
Thus,
\begin{align*}
\max_{a\in[K]}|\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})|&\le \frac{\max_{a\in[K]}\|\tilde{\xb}_{a}\|_{\Vb_{t}^{-1}}}{1-\epsilon}\left\Vert \sum_{\tau=1}^{t}\frac{1}{p}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}-{\mub}_{\star}\right\Vert _{\Vb_{t}^{-1}}\\
&\le \frac{\max_{a\in[K]}\|\tilde{\xb}_{a}\|_{\Vb_{t}^{-1}}}{1-\epsilon}\left(\frac{1}{p}\left\Vert \sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}\right\Vert _{\Vb_{t}^{-1}}+\left\Vert {\mub}_{\star}\right\Vert _{\Vb_{t}^{-1}}\right).
\end{align*}
Note that the matrix $\Vb_{t}$ is deterministic. 
By Lemma 9 in~\citep{abbasi-yadkori2011improved},
with probability at least $1-\delta,$
\begin{align*}
\left\Vert \sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}\right\Vert _{\Vb_{t}^{-1}}&\le \left\Vert \sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}\right\Vert _{\left(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}+\Ib_{K}\right)^{-1}}\\
&\le \sigma\sqrt{2\log\frac{\det(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}+\Ib_{K}){}^{1/2}}{\delta}}\\
&\le \sigma\sqrt{\log\frac{\det(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}+\Ib_{K})}{\delta}},
\end{align*}
for all $t\ge1$. 
Because 
\begin{align*}
\det\left(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}+\Ib_{K}\right)&\le \left\{ \frac{\text{Tr}\left(\sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\tilde{\boldsymbol{x}}_{a_{\tau}}^{\top}\right)+K}{K}\right\} ^{K}\\
&\le \left\{ \frac{t\max_{a\in[K]}\|\tilde{\boldsymbol{x}}_{a_{\tau}}\|_{2}+K}{K}\right\} ^{K}\\
&\le \left\{ t+1\right\} ^{K},
\end{align*}
where the last inequality holds by $\|\tilde{\boldsymbol{x}}_{a_{\tau}}\|_{2}\le\sqrt{K}\|\tilde{\boldsymbol{x}}_{a_{\tau}}\|_{\infty}\le K$.
Thus,
\begin{equation*}
\left\Vert \sum_{\tau=1}^{t}\tilde{\boldsymbol{x}}_{a_{\tau}}\epsilon_{a_{\tau},\tau}\right\Vert _{\Vb_{t}^{-1}}\le\sigma\sqrt{K\log\frac{t+1}{\delta}},
\end{equation*}
which proves,
\begin{align*}
\max_{a\in[K]}|\tilde{\xb}_{a}^{\top}(\widehat{\mub}_{t}-{\mub}_{\star})|&\le \frac{\max_{a\in[K]}\|\tilde{\xb}_{a}\|_{\Vb_{t}^{-1}}}{1-\epsilon}\left(\frac{\sigma}{p}\sqrt{K\log\frac{t+1}{\delta}}+\left\Vert {\mub}_{\star}\right\Vert _{\Vb_{t}^{-1}}\right)\\
&\le \frac{1}{\sqrt{t}}\cdot\frac{1}{1-\epsilon}\left(\frac{\sigma}{p}\sqrt{K\log\frac{t+1}{\delta}}+\left\Vert {\mub}_{\star}\right\Vert _{\Vb_{t}^{-1}}\right).
\end{align*}
Because $\Vert {\mub}_{\star}\Vert_{\Vb_{t}^{-1}} \le \Vert {\mub}_{\star}\Vert_{2} \le \sqrt{K}$, setting $\epsilon=1/2$ completes the proof.\hfill\qed


\subsection{Proof of \cref{thm:ridge_regret}}
\label{appendix:proof_thm4}
Because the regret is bounded by $1$ and the number of rounds for the exploration phase is at most $|\Ecal_{T}|\le32(1-p)^{-2}K^{2}\log\frac{2dT^{2}}{\delta}.$
\begin{align*}
\regret(T)&\le 32(1-p)^{-2}K^{2}\log\frac{2dT^{2}}{\delta}+\sum_{t\in[T]\setminus\Ecal_{T}}\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\\
= & 32(1-p)^{-2}K^{2}\log\frac{2dT^{2}}{\delta}+\sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \\
 & +\sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}\neq\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} .
\end{align*}
On the event $\{a_{t}=\widehat{a}_{t}\}$, 
\begin{align*}
\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]= & \tilde{\boldsymbol{x}}_{a_{\star}}^{\top}{\mub}_{\star}-\tilde{\boldsymbol{x}}_{\widehat{a}_{t}}^{\top}{\mub}_{\star}\\
&\le 2\max_{a\in[K]}\left|\tilde{\xb}_{a}^{\top}\left({\mub}_{\star}-\widehat{\boldsymbol{\mu}}_{t-1}^{R}\right)\right|+\tilde{\boldsymbol{x}}_{a_{\star}}^{\top}\widehat{\boldsymbol{\mu}}_{t-1}^{R}-\tilde{\boldsymbol{x}}_{\widehat{a}_{t}}^{\top}\widehat{\boldsymbol{\mu}}_{t-1}^{R}\\
&\le 2\max_{a\in[K]}\left|\tilde{\xb}_{a}^{\top}\left({\mub}_{\star}-\widehat{\boldsymbol{\mu}}_{t-1}^{R}\right)\right|\\
&\le \frac{4}{\sqrt{t}}\left(\frac{\sigma}{p}\sqrt{K\log\frac{2t^2}{\delta}}+\sqrt{K}\right),
\end{align*}
with probability at least $1-3\delta/t$, by \cref{thm:est_ridge}. 
Summing over $t$ gives,
\begin{equation*}
\sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \le 8\sqrt{KT}\left(\frac{\sigma}{p}\sqrt{\log\frac{2T^2}{\delta}}+1\right).
\end{equation*}
By resampling at most $\rho_{t}$ times, the probability of the event
$\{a_{t}\neq\widehat{a}_{t}\}$ is 
\begin{align*}
\PP\left(a_{t}\neq\widehat{a}_{t}\right)= & \sum_{m=1}^{\rho_{t}}\frac{p}{(K-1)\sqrt{t}}\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{m-1}\\
= & \frac{p}{(K-1)\sqrt{t}}\left(\frac{p}{(K-1)\sqrt{t}}\right)^{-1}\left\{ 1-\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{\rho_{t}}\right\} \\
= & 1-\left(1-\frac{p}{(K-1)\sqrt{t}}\right)^{\rho_{t}}\\
\ge & \frac{p\rho_{t}}{(K-1)\sqrt{t}},
\end{align*}
where the last inequality uses $(1+x)^{n}\ge1+nx$ for $x\ge-1$ and $n\in\NN$. 
Then the expected sum of regret, 
\begin{align*}
\EE\left[\sum_{t\in[T]\setminus\Ecal_{T}}\left\{ \II\left(a_{t}=\widehat{a}_{t}\right)\left(\EE_{t-1}[y_{\star,t}]-\EE_{t-1}[y_{a_{t},t}]\right)\right\} \right]
&\le\sum_{t\in[T]\setminus\Ecal_{T}}\PP\left(a_{t}\neq\widehat{a}_{t}\right)\\
& \le\frac{2p\sqrt{T}}{K-1}\rho_{T}\\
& =\frac{2p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}.
\end{align*}
Thus,
\begin{equation*}
\EE[\regret(T)]\le 6 \delta \log T +\frac{32K^{2}}{(1-p)^{2}}\log\frac{2dT^{2}}{\delta}+\frac{2p\sqrt{T}}{K-1}\frac{\log\frac{(T+1)^{2}}{\delta}}{\log(1/p)}+8\sqrt{KT}\left(\frac{\sigma}{p}\sqrt{\log\frac{T+1}{\delta}}+1\right).
\hfill\qed
\end{equation*} 

\section{Algorithm-agnostic Lower Bound of Regret Ignoring Unobserved Features}
\label{appendix:regret_lower_bound_general}
In this section, extending our argument in~\cref{thm:regret_linear_lower_bound}, we show that there exists a problem instance where linear bandit algorithms relying solely on observed features can incur regret that grows linearly in $T$. 
We begin by formally defining such algorithms.
\begin{definition}[Policy dependent on observed features]
\label{def:obs_dependent_policy}
For each $t \in [T]$, let $\pi_t:\RR^{d}\times\RR^{t-1}\to[0,1]$, be a policy that maps an observed feature vector $\xb\in\{\obs{}: a \in [K]\}$, given past reward observation $\{y_{a_s,s}: s \in [t-1]\}$, to a probability of selection.
Then the policy $\pi_t$ is dependent \emph{only} on observed features if, for any $y_{a_1,1},\ldots,y_{a_{t-1},t-1}$, it holds that $\xb_1 = \xb_2$ implies $\pi_t(\xb_1|y_{a_1,1},\ldots,y_{a_{t-1},t-1})=\pi_t(\xb_2|y_{a_1,1},\ldots,y_{a_{t-1},t-1})$.
\end{definition}
For instance, the UCB and Thompson sampling-based policies for linear bandits (with observed features), considered in~\cref{thm:regret_linear_lower_bound}, satisfy~\cref{def:obs_dependent_policy}, as they assign the same selection probability as long as the observed features are the same.
In contrast, the policy in the MAB algorithms (that disregard observed features) may assign different selection probability although the observed features are equal and is not dependent on the observed features.
In the theorem below, we particularly provide a lower bound for algorithms that employ policies that are dependent on the observed features.

\begin{theorem}[Regret Lower Bound under Policies Dependent on Observed Features]
\label{thm:regret_lower_bound_general}
    % Suppose that $ T \ge 4d^2 $. 
%We consider a set $\Xcal := \{\pm d/\sqrt{T} \}^d$ where observable features are generated.
%Furthermore, the parameter that determines the observable portion of the reward is defined as $\thetao = (1/3d, \dots, 1/3d)^\top\in\RR^d$.
%Let $a_{\star}$ denote the action that maximizes the expected reward based on the full feature vector, as defined in~\cref{eq:reward_decomposition}, while $ a_o $ represents the action that maximizes the observable portion of the expected reward, i.e., $a_o:=\argmax_{a\in\Acal}\dotp{\obs{}}{\thetao}$.
%In this scenario, using linear bandit algorithms that only consider observable features, the lower bound of the cumulative regret over the total horizon is given by:
For any algorithm $\Pi:=(\pi_1,\ldots,\pi_T)$ that consists of policies $\{\pi_t:t\in[T]\}$ that are dependent on observed features, there exists a set of features $\{\zb_{1},\ldots,\zb_{K}\}$ and a parameter $\thetaopt \in \RR^{d_z}$ such that the cumulative regret
\begin{equation*}
    \regret_{\Pi}(T, \theta_{\star},\zb_1,\ldots,\zb_K) \ge \frac{T}{6}. %+ \Omega(d\sqrt{T}).
\end{equation*}
\end{theorem}
\begin{proof}
We start the proof by providing a detailed account of the scenario described in the theorem. 
Without loss of generality, we consider the case where $K = 3$.
As stated in the theorem, $ a_\star $ represents the index of the optimal action when considering the entire reward, including both observed and latent components. 
In contrast, $a_o$ denotes the index of the optimal action when considering only the observed components.
We introduce an additional notation, $ a' $, which refers to an action whose observed features are identical to those of $ a_\star $, but with a distinct latent component. 
Specifically, this implies that $ a' \ne a_\star $ and $\mathbf{z}_{a'} \ne \mathbf{z}_{a_\star} $, but $\mathbf{x}_{a'} = \mathbf{x}_{a_\star} $.
By definition of the policy $\pi_t$ that depends on the observed features, $\pi_t(\xb_{a_{\star}})=\pi_t(\xb_{a'})$ and the probability of selecting an optimal arm is $\pi_t(\xb_{a_{\star}})\le1/2$.

Taking this scenario into account, the observed part of the features associated with $a_\star$, $a'$, and $a_o$ are defined as follows:
\begin{equation*}
    \xb_{a_\star} := \left[-\frac{1}{2},\dots,-\frac{1}{2}\right]^\top, 
    \xb_{a'}:= \left[-\frac{1}{2}, \dots, -\frac{1}{2} \right]^\top, 
    \xb_{a_o}:= \left[\frac{1}{2},\dots, \frac{1}{2}\right]^\top.
\end{equation*}
Additionally, we define the unobserved feature vectors for actions $a_\star$, $a'$, and $a_o$ as follows:
\begin{equation*} 
\ub_{a_\star} := \left[1, \dots, 1\right]^\top, 
\ub_{a'} := \left[-1, \dots, -1\right]^\top, 
\ub_{a_o} := \left[-1, \dots, -1,1,\dots, 1\right]^\top, 
\end{equation*}
where in $\ub_{a_o}$, the number of 1's and -1's are equal.
% \wy{Since $T \ge 4d^2$, it can be observed that the supremum norms of $\zb_{a_\star}$, $\zb_{a'}$, and $\zb_{a_o}$ — each constructed by concatenating the observable and corresponding unobserved parts — do not exceed 1.}
This ensures that the scenario aligns with the assumption imposed on the feature vectors throughout this paper.
We further define the true parameter as follows: 
\begin{equation*} \thetaopt:=\left[{\frac{1}{3d}},\dots,{\frac{1}{3d}}, {\frac{2}{3d_u}},\dots,{\frac{2}{3d_u}}\right]^\top\in\RR^{d_z},
\end{equation*}
thus it follows that $\thetao = [1/3d,\dots,1/3d]^\top\in\RR^{d}$ and $\thetau = [2/3d_u,\dots,2/3d_u]^\top\in\RR^{d_u}$.
Note that it is straightforward to verify that $|\langle \zb_a, \thetab_{\star} \rangle| \le 1$, thereby satisfying the assumption on the mean reward~(\cref{subsec:problem_formulation}).
With this established, we can also observe that the expected reward for the three actions are defined as:
\begin{align*}
    &\dotp{\latent{\star}}{\thetaopt} = \dotp{\obs{\star}}{\thetao} +\dotp{\ub_{a_\star}}{\thetau} = - \frac{1}{6} + \frac{2}{3} = \frac{1}{2}, \\
    &\dotp{{\zb}_{a'}}{\thetaopt} = \dotp{\xb_{a'}}{\thetao} +\dotp{\ub_{a'}}{\thetau} = -\frac{1}{6} - \frac{2}{3} = -\frac{5}{6}, \\
    &\dotp{{\zb}_{a_o}}{\thetaopt} = \dotp{\xb_{a_o}}{\thetao} + \dotp{\ub_{a_o}}{\thetau} = \frac{1}{6} + 0 = \frac{1}{6},
\end{align*}
respectively, and it is straightforward to verify that $\dotp{\latent{\star}}{\thetaopt} - \dotp{{\zb}_{a_o}}{\thetaopt} = 2/3 > 0$ and that $\dotp{\latent{\star}}{\thetaopt} - \dotp{{\zb}_{a'}}{\thetaopt} = 4/3 > 0$,
which confirms that $a_\star$ is optimal when considering the full feature set.

At each round $t\in[T]$, for any policy $\pi_t$ satisfying~\cref{def:obs_dependent_policy}, we have $\pi_t(\xb_{a_\star}|y_{a_1,1},\ldots,y_{a_{t-1},t-1}) = \pi_t(\xb_{a'}|y_{a_1,1},\ldots,y_{a_{t-1},t-1})$.
This implies $\PP(a_t=a_{\star}=\PP(a_t=a')$ and
\[
\PP(a_t=a_\star) = 1 - \PP(a_t=a') - \PP(a_t=a_o) \le 1 - \PP(a_t=a') = 1 - \PP(a_t=a_{\star}),
\]
and the probability of selecting an optimal arm cannot exceed $1/2$.
Thus, the expected regret,
\[
\regret_{\Pi}(T, \theta_{\star},\zb_{a_\star},\zb_{a'},\zb_{a_o}) \ge \Big(\frac{1}{2} - \frac{1}{6}\Big)\sum_{t=1}^T \PP(a_t \neq a_{\star}) \ge \frac{T}{6},
\]
which completes the proof.
%the action selected in round $t$, $a_t$, is based solely on $\obs{t}$, thereby neglecting the unobserved portion of the reward.
%Given this action $a_t$, the instantaneous regret incurred by these algorithms in round $t$ is defined and decomposed as follows:
%\begin{align*}
%    \regret(t) 
%    &= \dotp{\latent{\star}}{\thetaopt} - \dotp{\latent{t}}{\thetaopt} \\
%    &= \underbrace{\dotp{\latent{\star} - \zb_{a_o}}{\thetaopt}}_{(*)} + \dotp{\zb_{a_o}- \latent{t}}{\thetaopt}.
%\end{align*}
%We consider the first part denoted by $(*)$, and this term is equal to $2/3$ as verified above.
%Hence, the cumulative regret becomes:
%\begin{align}
%    \regret(T) &= \sum_{t=1}^T \regret(t) \notag \\
%    &= \sum_{t=1}^T \left(\dotp{\latent{\star} - \zb_{a_o}}{\thetaopt} + \dotp{\zb_{a_o}- \latent{t}}{\thetaopt} \right)\notag \\
%    &= \sum_{t=1}^T \frac{2}{3}+ \sum_{t=1}^T  \dotp{\zb_{a_o}- \latent{t}}{\thetaopt} \notag \\
%    &= \frac{2}{3}T + \underbrace{\sum_{t=1}^T  \dotp{\zb_{a_o}- \latent{t}}{\thetaopt}}_{(**)}. \label{eq:lower_bound_linear}
%\end{align}
%For the term denoted by $(**)$, it can be further decomposed as follows:
%\begin{equation*}
%    \sum_{t=1}^T \left(\dotp{\xb_{a_o}}{\thetao} - \dotp{\obs{t}}{\thetao} \right)+ \sum_{t=1}^T \left(\dotp{\ub_{a_o}}{\thetau} - \dotp{\ub_{a_t}}{\thetau} \right),
%\end{equation*}
%where the first term corresponds to regret induced by any  linear bandit algorithms, and by definition, this term is always greater than or equal to 0.

%The second term of this decomposition depends on how often $a_t$ matches $a_\star$, since selecting $a_\star$ makes this term negative.
%Following the definitions of $\obs{\star}$ and $\xb_{a'}$, the agent that adopts a policy that depends on the observed features cannot distinguish between the two actions when their respective unobserved part of features are excluded, as $\obs{\star}$ and $\xb_{a'}$ are identical.
%As a result, for any set of policies $\Pi$ that depend on the observed features, one of the two actions is chosen uniformly at random.
%This implies that, even in the worst case, the optimal action $a_\star$ is selected at most $T/2$ times in expectation.
%Thus, the second term is bounded below by $-T/3$, leading to the following inequality:
%\begin{align*}
%    \frac{2}{3}T + \sum_{t=1}^T  \dotp{\zb_{a_o}- \latent{t}}{\thetaopt}
%    &\ge \frac{2}{3}T + \sum_{t=1}^T \left(\dotp{\xb_{a_o}}{\thetao} - \dotp{\obs{t}}{\thetao} \right) - \frac{T}{3} \\ 
%    &\ge \frac{T}{3} + \sum_{t=1}^T \left(\dotp{\xb_{a_o}}{\thetao} - \dotp{\obs{t}}{\thetao} \right)\\
%    &\ge \frac{T}{3}.
%\end{align*}
\end{proof}

\section{Technical Lemmas}
\label{appendix:lemmas}
\begin{lemma}
\label{lem:exp_bound} (Exponential martingale inequality) If a martingale $(\Xb_{t};t\ge0)$, adapted to filtration $\Fcal_{t}$, satisfies $\condexp{\exp(\lambda \Xb_{t})}{\Fcal_{t-1}} \le\exp(\lambda^{2}\sigma_{t}^{2}/2)$
for some constant $\sigma_{t}$, for all $t$, then for any $a\ge0$,
\begin{equation*}
\PP\left(\abs{X_{T}-X_{0}}\ge a\right)\le2\exp\left(-\frac{a^{2}}{2\sum_{t=1}^{T}\sigma_{t}^{2}}\right)
\end{equation*}
Thus, with probability at least $1-\delta$,
\begin{equation*}
\abs{X_{T}-X_{0}}\le\sqrt{2\sum_{t=1}^{T}\sigma_{t}^{2}\log\frac{2}{\delta}}.
\end{equation*}
\end{lemma}




\subsection{A Hoeffding bound for Matrices}
\def\CE#1#2{\mathbb{E}\left[\left.#1\right|#2\right]}%
\def\CP#1#2{\mathbb{P}\left(\left.#1\right|#2\right)}%

\begin{lemma} 
\label{lem:matrix_hoeffding} 
Let $\{\Mb_{\tau}:\tau\in[t]\}$
be a $\mathbb{R}^{d\times d}$-valued stochastic process adapted to
the filtration $\{\mathcal{F_{\tau}}:\tau\in[t]\}$, i.e., $\Mb_{\tau}$
is $\Fcal_{\tau}$-measurable for $\tau\in[t]$. Suppose that the
matrix $\Mb_{\tau}$ is symmetric and the eigenvalues of the difference
$\Mb_{\tau}-\condexp{\Mb_{\tau}}{\Fcal_{\tau-1}}$ lie in $[-b,b]$
for some $b>0$. Then for $x>0$, 
\begin{equation*}
\mathbb{P}\left(\left\Vert \sum_{\tau=1}^{t}\Mb_{\tau}-\condexp{\Mb_{\tau}}{\Fcal_{\tau-1}} \right\Vert _{2}\ge x\right)\le2d\exp\left(-\frac{x^{2}}{2tb^{2}}\right)
\end{equation*}

\end{lemma}
\begin{proof}
The proof is an adapted version of Hoeffding's inequality for matrix
stochastic process with the argument of~\citep{tropp2012user-friendly}. Let
$\Db_{\tau}:=\Mb_{\tau}-\condexp{\Mb_{\tau}}{\Fcal_{\tau-1}}$. Then,
for $x>0$, 
\begin{equation*}
\mathbb{P}\left(\left\Vert \sum_{\tau=1}^{t}\Db_{\tau}\right\Vert _{2}\ge x\right)\le\mathbb{P}\left(\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\ge x\right)+\mathbb{P}\left(\lambda_{\max}\left(-\sum_{\tau=1}^{t}\Db_{\tau}\right)\ge x\right)
\end{equation*}
We bound the first term and the second term is bounded with similar
arguement. For any $v>0$, 
\begin{equation*}
\mathbb{P}\left(\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\ge x\right)\le\mathbb{P}\left(\exp\left\{ v\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \ge e^{vx}\right)\le e^{-vx}\mathbb{E}\left[\exp\left\{ v\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \right].
\end{equation*}
Since $\sum_{\tau=1}^{t}\Db_{\tau}$ is a real symmetric matrix, 
\begin{align*}
\exp\left\{ v\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} = & \lambda_{\max}\left\{ \exp\left(v\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \le\text{Tr}\left\{ \exp\left(v\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} ,
\end{align*}
where the last inequality holds since $\exp(v\sum_{\tau=1}^{t}\Db_{\tau})$
has nonnegative eigenvalues. Taking expectation on both side gives,
\begin{align*}
\mathbb{E}\left[\exp\left\{ v\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \right]
&\le  \mathbb{E}\left[\text{Tr}\left\{ \exp\left(v\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \right]\\
&=  \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t}\Db_{\tau}\right)\right]\\
&= \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-1}\Db_{\tau}+\log\exp(v\Db_{t})\right)\right].
\end{align*}
By Lieb's theorem~\citep{tropp2015introduction} the mapping $\Db \mapsto\exp(\Hb + \log \Db)$
is concave on positive symmetric matrices for any symmetric positive
definite $H$. 
By Jensen's inequality,
\begin{equation*}
\text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-1}\Db_{\tau}+\log\exp(v\Db_{t})\right)\right]\le\text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-1}\Db_{\tau}+\log\CE{\exp(v\Db_{t})}{\Fcal_{t-1}}\right)\right]
\end{equation*}
By Hoeffding's lemma, 
\begin{equation*}
e^{vx}\le\frac{b-x}{2b}e^{-vb}+\frac{x+b}{2b}e^{vb}
\end{equation*}
for all $x\in[-b,b]$. Because the eigenvalue of $\Db_{\tau}$ lies
in $[-b,b]$, we have 
\begin{align*}
\CE{\exp(v\Db_{t})}{\Fcal_{t-1}}
&\preceq \CE{\frac{e^{-vb}}{2b}\left(b\Ib_{d}-\Db_{t}\right)+\frac{e^{vb}}{2b}\left(\Db_{t}+b\Ib_{d}\right)}{\Fcal_{t-1}}\\
&= \frac{e^{-vb}+e^{vb}}{2}\Ib_{d}\\
&\preceq \exp(\frac{v^{2}b^{2}}{2})\Ib_{d}.
\end{align*}
Recursively,
\begin{align*}
\mathbb{E}\left[\exp\left\{ v\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\right\} \right]&\le \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-1}\Db_{\tau}+\log\CE{\exp(v\Db_{t})}{\Fcal_{t-1}}\right)\right]\\
&\le \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-1}\Db_{\tau}+(\frac{v^{2}b^{2}}{2})\Ib_{d}\right)\right]\\
&\le \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-2}\Db_{\tau}+(\frac{v^{2}b^{2}}{2})\Ib_{d}+\log\CE{\exp(vD_{t-1})}{\Fcal_{t-2}}\right)\right]\\
&\le \text{Tr}\mathbb{E}\left[\exp\left(v\sum_{\tau=1}^{t-2}\Db_{\tau}+(\frac{2v^{2}b^{2}}{2})\Ib_{d}\right)\right]\\
& \vdots \vdots\\
&\le \text{Tr}\exp\left((\frac{tv^{2}b^{2}}{2})\Ib_{d}\right)\\
&= \exp\left(\frac{tv^{2}b^{2}}{2}\right)\text{Tr}\left(\Ib_{d}\right)\\
&= d\exp\left(\frac{tv^{2}b^{2}}{2}\right).
\end{align*}
Thus we have 
\begin{equation*}
\mathbb{P}\left(\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\ge x\right)\le d\exp\left(-vx+\frac{tv^{2}b^{2}}{2}\right).
\end{equation*}
Minimizing over $v>0$ gives $v=x/(tb^{2})$ and
\begin{equation*}
\mathbb{P}\left(\lambda_{\max}\left(\sum_{\tau=1}^{t}\Db_{\tau}\right)\ge x\right)\le d\exp\left(-\frac{x^{2}}{2tb^{2}}\right),
\end{equation*}
which proves the lemma.
\end{proof}

\subsection{A Bound for the Gram Matrix}
The Hoeffding bound for matrices (\cref{lem:matrix_hoeffding}) implies the following bound for the two Gram matrices $\Ab_t:=\sum_{\tau=1}^{t}\tilde{\xb}_{a_\tau}\tilde{\xb}_{a_\tau}^\top$ and $\Vb_t:=\sum_{\tau=1}^{t} \sum_{a\in[K]}\tilde{\xb}_{a}\tilde{\xb}_{a}^\top$
\begin{corollary}
\label{cor:Gram_matrix}
For any $\epsilon \in (0,1)$ and $t\ge8\epsilon^{-2}(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$, with probability at least $1-\delta/t^{2},$
\begin{equation*}
\left\Vert\Ib_{K}-\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\right\Vert_2\le\epsilon,
\end{equation*}
\end{corollary}

\begin{proof}
Note that
\begin{equation*}
\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}-\Ib_{K}=\Vb_{t}^{-1/2}\left\{ \sum_{\tau=1}^{t}\sum_{a\in[K]}\left(\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}-1\right)\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\right\} \Vb_{t}^{-1/2},
\end{equation*}
and the martingale difference matrix for each $\tau\in[t]$,
\begin{align*}
\left\Vert \sum_{a\in[K]}\left(\frac{\II(\tilde{a}_{\tau}=a)}{\phi_{a,\tau}}-1\right)\Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\Vb_{t}^{-1/2}\right\Vert _{2}&\le \left(\frac{K-1}{1-p}+K-2\right)\max_{a\in[K]}\left\Vert \Vb_{t}^{-1/2}\mathbf{\tilde{x}}_{a}\mathbf{\tilde{x}}_{a}^{\top}\Vb_{t}^{-1/2}\right\Vert _{2}\\
&\le \frac{2K}{1-p}\max_{a\in[K]}\left\Vert \mathbf{\tilde{x}}_{a}\right\Vert _{\Vb_{t}^{-1}}^{2}\\
&\le \frac{2K}{1-p}\cdot\frac{1}{t},
\end{align*}
where the last inequality holds by Sherman-Morrison formula. 
By Hoeffding bound for matrix (\cref{lem:matrix_hoeffding}), for $x>0$
\begin{equation*}
\PP\left(\left\Vert\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}-\Ib_{K}\right\Vert_{2}>x\right)\le 2K\exp\left(-\frac{(1-p)^{2}tx^{2}}{8K^{2}}\right).
\end{equation*}
Setting $x=\epsilon\in(0,1)$ which will be determined later, for
$t\ge8\epsilon^{-2}(1-p)^{-2}K^{2}\log\frac{2Kt^{2}}{\delta}$ with
probability at least $1-\delta/t^{2},$
\begin{equation*}
\left\Vert\Ib_{K}-\Vb_{t}^{-1/2}\Ab_{t}\Vb_{t}^{-1/2}\right\Vert_2\le\epsilon,
\end{equation*}
\end{proof}

\subsection{An error bound for the Lasso estimator}

\begin{lemma}[An error bound for the Lasso estimator with unrestricted minimum eigenvalue]
\label{lem:lasso}  
Let $\{\xb_{\tau}\}_{\tau\in[t]}$ denote the covariates in $[-1,1]^{d}$ and $y_{\tau}=\xb_{\tau}^{\top}\bar{\wb}+e_{\tau}$ for
some $\bar{\wb}\in\RR^{d}$ and $e_{\tau}\in\RR$. 
For $\lambda>0$,
let 
\begin{equation*}
\widehat{\wb}_{t}=\argmin_{\wb}\sum_{\tau=1}^{t}\left(y_{\tau}-\xb_{\tau}^{\top}\wb\right)^{2}+\lambda\norm \wb_{1}.
\end{equation*}
Let $\bar{\mathcal{S}}:=\{i\in[d]:\bar{\wb}(i)\neq0\}$ and $\Sigmab_{t}:=\sum_{\tau=1}^{t}\xb_{\tau}\xb_{\tau}^{\top}$.
Suppose $\Sigmab_t$ has positive minimum eigenvalue and $\|\sum_{\tau=1}^{t}e_{\tau}\xb_{\tau}\|_{\infty}\le \lambda/2$.
Then,
\begin{equation*}
\norm{\widehat{\wb}_{t}-\bar{\wb}}_{\Sigmab_t}\le\frac{2\lambda\sqrt{\abs{\bar{\mathcal{S}}}}}{\sqrt{\lambda_{\min}\left(\Sigmab_t\right)}}.
\end{equation*}
\end{lemma}


\begin{proof}
The proof is similar to that of Lemma B.4 in~\citep{kim2024doubly}, but we provide a new proof for the (unrestricted) minimum eigenvalue condition.
Let $\Xb_{t}^{\top}:=(\xb_{1},\ldots,\xb_{t})\in[-1,1]^{d\times t}$ and $\mathbf{e}_{t}^{\top}:=(e_{1},\ldots,e_{t})\in\RR^{t}$. 
We write $\Xb_{t}(j)$ and $\widehat{\wb}_{t}(j)$ as the $j$-th column of $\Xb_{t}$ and $j$-th entry of $\widehat{\wb}_{t}$, respectively. By definition of $\widehat{\wb}_{t}$,
\begin{equation*}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)+\mathbf{e}_{t}}_{2}^{2}+\lambda\norm{\widehat{\wb}_{t}}_{1}\le\norm{\mathbf{e}_{t}^{(j)}}_{2}^{2}+\lambda\norm{\bar{\wb}}_{1},
\end{equation*}
which implies
\begin{align*}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2}+\lambda\norm{\widehat{\wb}_{t}}_{1}&\le 2\left(\widehat{\wb}_{t}-\bar{\wb}\right)^{\top}\Xb_{t}^{\top}\mathbf{e}_{t}+\lambda\norm{\bar{\wb}}_{1}\\
&\le 2\norm{\widehat{\wb}_{t}-\bar{\wb}}_{1}\norm{\Xb_{t}^{\top}\mathbf{e}_{t}}_{\infty}+\lambda\norm{\bar{\wb}}_{1}\\
&\le \lambda \norm{\widehat{\wb}_{t}-\bar{\wb}}_{1}+\lambda\norm{\bar{\wb}}_{1},
\end{align*}
where the last inequality uses the bound on $\lambda$. On the left
hand side, by triangle inequality,
\begin{align*}
\norm{\widehat{\wb}_{t}}_{1}= & \sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)}+\sum_{i\in[d]\setminus\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)}\\
\ge & \sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)}-\sum_{i\in\mathcal{S}_{\star}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}+\sum_{i\in[d]\setminus\bar{\mathcal{S}}}\abs{\bar{\wb}(i)}\\
= & \norm{\bar{\wb}}_{1}-\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}+\sum_{i\in[d]\setminus\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)}
\end{align*}
and for the right-hand side,
\begin{equation*}
\norm{\widehat{\wb}_{t}-\bar{\wb}}_{1}=\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}+\sum_{i\in[d]\setminus\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)}.
\end{equation*}
Plugging in both sides and rearranging the terms,
\begin{equation}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2} \le 2\lambda\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}.
\label{eq:w_basic}
\end{equation}
Because $\Xb_t^\top \Xb_t$ is positive definite,
\begin{align*}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2}\ge & \lambda_{\min}(\Xb_{t}^{\top}\Xb_{t})\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}^{2}\\
\ge & \frac{\lambda_{\min}(\Xb_{t}^{\top}\Xb_{t})}{\abs{\bar{\mathcal{S}}}}\left(\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}\right)^{2},
\end{align*}
where the last inequality holds by Cauchy-Schwarz inequality. Plugging in \cref{eq:w_basic} gives,
\begin{align*}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2}
\le&2\lambda\sum_{i\in\bar{\mathcal{S}}}\abs{\widehat{\wb}_{t}(i)-\bar{\wb}(i)}\\
\le&2\lambda\sqrt{\frac{\abs{\bar{\mathcal{S}}}}{\lambda_{\min}(\Sigmab_{t})}}\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}\\
\le&\frac{2\lambda^{2}\abs{\bar{\mathcal{S}}}}{\lambda_{\min}(\Sigmab_{t})}+\frac{1}{2}\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2},
\end{align*}
where the last inequality uses $ab\le a^{2}/2+b^{2}/2$. 
Rearranging the terms,
\begin{equation*}
\norm{\Xb_{t}\left(\bar{\wb}-\widehat{\wb}_{t}\right)}_{2}^{2}\le\frac{4\lambda^{2}\abs{\bar{\mathcal{S}}}}{\lambda_{\min}(\Sigmab_{t})},
\end{equation*}
which proves the result. 
\end{proof}

\subsection{Eigenvalue bounds for the Gram matrix.}

\begin{lemma} \label{lem:eig_bound} For $a\in[K]$, let $\tilde{\mathbf{x}}_{a}:=[\mathbf{x}_{a}^{\top},\mathbf{e}_{a}^{\top}\mathbf{p}_{1},\cdots,\mathbf{e}_{a}^{\top}\mathbf{p}_{K-d}]^{\top}\in\mathbb{R}^{d}$
denote augmented features. 
Then, an eigenvalue of $\sum_{a\in[K]}\tilde{\mathbf{x}}_{a}\tilde{\mathbf{x}}_{a}^{\top}$
is in the following interval
\begin{equation*}
    \left[\min\left\{\lambda_{\min}\left(\sum_{a\in[k]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top}\right),1\right\},\max\left\{\lambda_{\max}\left(\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top}\right),1\right\}\right].
\end{equation*}

\end{lemma} 
\begin{proof}
Let $\mathbf{P}:=(\mathbf{p}_{1},\ldots,\mathbf{p}_{K-d})\in\mathbb{R}^{K\times(K-d)}$.
Because the columns in $\mathbf{P}$ are orthogonal each other and
to $\mathbf{x}_{1},\ldots,\mathbf{x}_{K}$, 
\begin{align*}
\sum_{a\in[K]}\tilde{\mathbf{x}}_{a}\tilde{\mathbf{x}}_{a}^{\top}= & \begin{bmatrix}\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top} & \sum_{a\in[K]}\mathbf{x}_{a}\mathbf{e}_{a}^{\top}\mathbf{P}\\
\sum_{a\in[K]}\mathbf{P}^{\top}\mathbf{e}_{a}\mathbf{x}_{a}^{\top} & \mathbf{P}^{\top}\mathbf{P}
\end{bmatrix}\\
= & \begin{bmatrix}\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top} & \sum_{a\in[K]}\mathbf{x}_{a}\mathbf{e}_{a}^{\top}\mathbf{P}\\
\sum_{a\in[K]}\mathbf{P}^{\top}\mathbf{e}_{a}\mathbf{x}_{a}^{\top} & I_{K-d}
\end{bmatrix}\\
= & \begin{bmatrix}\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top} & \sum_{a\in[K]}\mathbf{X}\mathbf{e}_{a}\mathbf{e}_{a}^{\top}\mathbf{P}\\
\sum_{a\in[K]}\mathbf{P}^{\top}\mathbf{e}_{a}\mathbf{e}_{a}^{\top}\mathbf{X} & I_{K-d}
\end{bmatrix}\\
= & \begin{bmatrix}\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top} & \mathbf{X}\mathbf{P}\\
\mathbf{P}^{\top}\mathbf{X}^{\top} & I_{K-d}
\end{bmatrix}\\
= & \begin{bmatrix}\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top} & \mathbf{O}\\
\mathbf{O} & I_{K-d}
\end{bmatrix}.
\end{align*}
Thus, for any $\lambda\in\RR$, $\det(\sum_{a\in[K]}\tilde{\mathbf{x}}_{a}\tilde{\mathbf{x}}_{a}^{\top}-\lambda\mathbf{I}_{K})=\det(\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top}-\lambda\mathbf{I}_{d})(1-\lambda)^{K-d}$.
Solving $\det(\sum_{a\in[K]}\mathbf{x}_{a}\mathbf{x}_{a}^{\top}-\lambda\mathbf{I}_{d})(1-\lambda)^{K-d}=0$
gives the eigenvalues and the lemma is proved.
\end{proof}

\end{document}
