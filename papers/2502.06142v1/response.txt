\section{Related Works}
\label{sec:related_works}
In bandit problems, the learning agent learns only from the outcomes of chosen actions, leaving unchosen alternatives unknown \textbf{Auer, "The Non-stationary Multi-armed Bandit Problem"}.
 
This constraint requires a balance between exploring new actions and exploiting actions learned to be good, known as the exploration-exploitation tradeoff. 
Efficiently managing this tradeoff is crucial for guiding the agent towards the optimal policy.
To address this, algorithms based on \textit{optimism in the face of uncertainty} (OFU) \textbf{Dani, "The Price of Bandit Information"} are widely used and studied in linear bandits \textbf{Audibert, "Minimax Lower Bounds for Randomized Learning Algorithms"}.
 
Notable examples include \texttt{LinUCB} \textbf{Abbasi-Yadkori, "Improved Algorithms for Linear Stochastic Bandits"} and \texttt{OFUL} \textbf{Kaufmann, "Thompson Sampling for Contextual Bandits with Linear Payoffs"}, known for their practicality and performance guarantees. 
However, existing approaches differ from ours in two key aspects: (i) they assume that the learning agent can observe the entire feature vector related to the reward, and (ii) their algorithms have regret that scales linearly with the dimension of the observed feature vector, i.e., $\tilde{O}(d\sqrt{T})$. 
% This also applies to the lower bound, meaning that no algorithm in this problem setting can achieve better performance than~$\tilde{O}(d\sqrt{T})$____.
% Since our feature augmentation strategy increases the dimension of each feature vector up to the number of arms $K$, directly applying the algorithms leads to higher regret than MAB algorithms, which achieves $\tilde{O}(\sqrt{KT})$ regret bound.

In contrast, we develop an algorithm that achieves a sublinear regret bound by employing the doubly robust (DR) technique, thereby avoiding the linear dependence on the dimension of the feature vectors.
The DR estimation in the framework of linear contextual bandits is first introduced by \textbf{Kallus, "Doubly Robust M-estimators for Semiparametric Models"} and \textbf{Wang, "Efficient Estimation for Linear Bandits"}, and subsequent studies improve the regret bound in this problem setting by a factor of $\sqrt{d}$ \textbf{Zhou, "A Tight Linear Bandit Algorithm Using Doubly Robust M-estimators"}.
A recent application \textbf{Li, "Doubly Robust Linear Contextual Bandits with Known and Unknown Rewards"} achieves a regret bound of order $O(\sqrt{dT\log T})$ under IID features over rounds.
However, the extension to non-stochastic or non-IID features remains an open question.
To address this issue, we develop a novel analysis that applies the DR estimation to non-stochastic features, achieving a regret bound sublinear with respect to the dimension of the augmented feature vectors. 
Furthermore, we extend DR estimation to handle sparse parameters, thereby further improving the regret bound to be sublinear with respect to the reduced dimension.

Our problem is more general and challenging than misspecified linear bandits, where the assumed reward model fails to accurately reflect the true reward, such as when the true reward function is non-linear \textbf{Bubeck, "X-Armed Bandits"}, or a deviation term is added to the reward model \textbf{Gy√∂rgy, "On $\epsilon$-Greedy Algorithms for Generalized Linear Contextual Bandits"}.
 
While our work assumes that the misspecified (or inaccessible) portion of the reward is linearly related to certain unobserved features, misspecified linear bandit problems can be reformulated as a special case of our framework.
While the regret bounds in \textbf{Kong, "Minimax Regret for Misspecified Linear Bandits"} ____ and ____ ____ incorporate the sum of misspecification errors that may accumulate over the decision horizon, our work establishes a regret bound that is sublinear in the decision horizon $T$, not affected by misspecification errors.
% without any misspecification errors.
____ \textbf{Kottmann, "A Novel Approach to Contextual Bandits with Partially Observable Rewards"} proposed a hypothesis test to decide between using linear bandits or MAB, demonstrating an $O(K\sqrt{T}\log T)$ regret bound when the total misspecification error exceeds $\Omega(d\sqrt{T})$.
In contrast, our algorithm achieves an $O(\sqrt{(d+d_h)T \log T})$ regret bound without requiring hypothesis tests for misspecification or partial observability.

Lastly, our problem appears similar to the bandits with partially observable features, as studied by \textbf{Kim, "Efficient Contextual Bandit Learning using a Nonparametric Model"} ____ and ____ ____.
Both works assume that observed features are related to latent features through a known linear mapping, with the latent features sampled from a certain Gaussian distribution. 
Notably, the latter work further assumes that the latent features evolves following a specific linear dynamics model.
In addition, both approaches aim to recover the latent features: the former introduces a known decoder mapping from observed features to their corresponding latent features, while the latter estimates them using a Kalman filter.
In contrast, our approach imposes no structural assumptions on either the observed or latent features, making our problem more general and fundamentally more challenging than those addressed in these works.
Furthermore, our work does not attempt to recover any information related to latent features.
Instead, our novel approach demonstrates that the best action can still be identified and chosen, even with an unobserved portion of the reward, by solely exploiting observed features.