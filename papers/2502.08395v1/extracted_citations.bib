@inproceedings{bang2024measuringpoliticalbias,
    title = "Measuring Political Bias in Large Language Models: What Is Said and How It Is Said",
    author = "Bang, Yejin  and
      Chen, Delong  and
      Lee, Nayeon  and
      Fung, Pascale",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.600/",
    doi = "10.18653/v1/2024.acl-long.600",
    pages = "11142--11159",
    abstract = "We propose to measure political bias in LLMs by analyzing both the content and style of their generated content regarding political issues. Existing benchmarks and measures focus on gender and racial biases. However, political bias exists in LLMs and can lead to polarization and other harms in downstream applications. In order to provide transparency to users, we advocate that there should be fine-grained and explainable measures of political biases generated by LLMs. Our proposed measure looks at different political issues such as reproductive rights and climate change, at both the content (the substance of the generation) and the style (the lexical polarity) of such bias. We measured the political bias in eleven open-sourced LLMs and showed that our proposed framework is easily scalable to other topics and is explainable."
}

@article{fujimoto2023revisiting,
  title={Revisiting the political biases of ChatGPT},
  author={Fujimoto, Sasuke and Kazuhiro, Takemoto},
  journal={Frontiers in Artificial Intelligence},
  volume={6},
  year={2023},
  publisher={Frontiers}
}

@article{hartmann2023political,
  title={The political ideology of conversational AI: Converging evidence on ChatGPT's pro-environmental, left-libertarian orientation},
  author={Hartmann, Jochen and Schwenzow, Jasper and Witte, Maximilian},
  journal={arXiv preprint arXiv:2301.01768},
  year={2023}
}

@article{lum2024bias,
  title={Bias in Language Models: Beyond Trick Tests and Toward RUTEd Evaluation},
  author={Lum, Kristian and Anthis, Jacy Reese and Nagpal, Chirag and D'Amour, Alexander},
  journal={arXiv preprint arXiv:2402.12649},
  year={2024}
}

@inproceedings{moore2024consistent,
    title = "Are Large Language Models Consistent over Value-laden Questions?",
    author = "Moore, Jared  and
      Deshpande, Tanvi  and
      Yang, Diyi",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.891/",
    doi = "10.18653/v1/2024.findings-emnlp.891",
    pages = "15185--15221",
    abstract = "Large language models (LLMs) appear to bias their survey answers toward certain values. Nonetheless, some argue that LLMs are too inconsistent to simulate particular values. Are they? To answer, we first define value consistency as the similarity of answers across 1) \textit{paraphrases} of one question, 2) related questions under one \textit{topic}, 3) multiple-choice and open-ended \textit{use-cases} of one question, and 4) \textit{multilingual} translations of a question to English, Chinese, German, and Japanese. We apply these measures to a few large, open LLMs including llama-3, as well as gpt-4o, using eight thousand questions spanning more than 300 topics. Unlike prior work, we find that \textit{models are relatively consistent} across paraphrases, use-cases, translations, and within a topic. Still, some inconsistencies remain. Models are more consistent on uncontroversial topics (e.g., in the U.S., {\textquotedblleft}Thanksgiving{\textquotedblright}) than on controversial ones (e.g. {\textquotedblleft}euthanasia{\textquotedblright}). Base models are both more consistent compared to fine-tuned models and are uniform in their consistency across topics, while fine-tuned models are more inconsistent about some topics (e.g. {\textquotedblleft}euthanasia{\textquotedblright}) than others (e.g. {\textquotedblleft}Women`s rights{\textquotedblright}) like our human participants."
}

@article{motoki2023more,
  title={More human than human: Measuring ChatGPT political bias},
  author={Motoki, Fabio and Neto, Valdemar Pinho and Rodrigues, Victor},
  journal={Public Choice},
  pages={1--21},
  year={2023},
  publisher={Springer}
}

@inproceedings{potter2024hiddenpersuaders,
    title = "Hidden Persuaders: {LLM}s' Political Leaning and Their Influence on Voters",
    author = "Potter, Yujin  and
      Lai, Shiyang  and
      Kim, Junsol  and
      Evans, James  and
      Song, Dawn",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.244/",
    doi = "10.18653/v1/2024.emnlp-main.244",
    pages = "4244--4275",
    abstract = "Do LLMs have political leanings and are LLMs able to shift our political views? This paper explores these questions in the context of the 2024 U.S. presidential election. Through a voting simulation, we demonstrate 18 open-weight and closed-source LLMs' political preference for Biden over Trump. We show how Biden-leaning becomes more pronounced in instruction-tuned and reinforced models compared to their base versions by analyzing their responses to political questions related to the two nominees. We further explore the potential impact of LLMs on voter choice by recruiting 935 U.S. registered voters. Participants interacted with LLMs (Claude-3, Llama-3, and GPT-4) over five exchanges. Intriguingly, although LLMs were not asked to persuade users to support Biden, about 20{\%} of Trump supporters reduced their support for Trump after LLM interaction. This result is noteworthy given that many studies on the persuasiveness of political campaigns have shown minimal effects in presidential elections. Many users also expressed a desire for further interaction with LLMs on political subjects. Further research on how LLMs affect users' political views is required, as their use becomes more widespread."
}

@inproceedings{rottger2024political,
    title = "Political Compass or Spinning Arrow? Towards More Meaningful Evaluations for Values and Opinions in Large Language Models",
    author = {R{\"o}ttger, Paul  and
      Hofmann, Valentin  and
      Pyatkin, Valentina  and
      Hinck, Musashi  and
      Kirk, Hannah  and
      Schuetze, Hinrich  and
      Hovy, Dirk},
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.816/",
    doi = "10.18653/v1/2024.acl-long.816",
    pages = "15295--15311",
    abstract = "Much recent work seeks to evaluate values and opinions in large language models (LLMs) using multiple-choice surveys and questionnaires. Most of this work is motivated by concerns around real-world LLM applications. For example, politically-biased LLMs may subtly influence society when they are used by millions of people. Such real-world concerns, however, stand in stark contrast to the artificiality of current evaluations: real users do not typically ask LLMs survey questions. Motivated by this discrepancy, we challenge the prevailing *constrained* evaluation paradigm for values and opinions in LLMs and explore more realistic *unconstrained* evaluations. As a case study, we focus on the popular Political Compass Test (PCT). In a systematic review, we find that most prior work using the PCT *forces models to comply with the PCT`s multiple-choice format. We show that models give substantively different answers when not forced; that answers change depending on how models are forced; and that answers lack paraphrase robustness. Then, we demonstrate that models give different answers yet again in a more realistic open-ended answer setting. We distill these findings into recommendations and open challenges in evaluating values and opinions in LLMs."
}

@article{rozado2023political,
  title={The political biases of chatgpt},
  author={Rozado, David},
  journal={Social Sciences},
  volume={12},
  number={3},
  pages={148},
  year={2023},
  publisher={MDPI}
}

@article{rozado2024political,
  author = {David Rozado},
  title = {The political preferences of LLMs},
  journal = {PLOS ONE},
  year = {2024},
  volume = {19},
  number = {7},
  pages = {e0306621},
  doi = {10.1371/journal.pone.0306621},
  url = {https://doi.org/10.1371/journal.pone.0306621}
}

@article{rutinowski2024self,
  title={The Self-Perception and Political Biases of ChatGPT},
  author={Rutinowski, J{\'e}r{\^o}me and Franke, Sven and Endendyk, Jan and Dormuth, Ina and Roidl, Moritz and Pauly, Markus},
  journal={Human Behavior and Emerging Technologies},
  volume={2024},
  number={1},
  pages={7115633},
  year={2024},
  publisher={Wiley Online Library}
}

@article{santurkar2023opinionqa,
  title={Whose opinions do language models reflect?},
  author={Santurkar, Shibani and Durmus, Esin and Ladhak, Faisal and Lee, Cinoo and Liang, Percy and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2303.17548},
  year={2023}
}

@inproceedings{taubenfeld2024systematic,
    title = "Systematic Biases in {LLM} Simulations of Debates",
    author = "Taubenfeld, Amir  and
      Dover, Yaniv  and
      Reichart, Roi  and
      Goldstein, Ariel",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-main.16/",
    doi = "10.18653/v1/2024.emnlp-main.16",
    pages = "251--267",
    abstract = "The emergence of Large Language Models (LLMs), has opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. Current research suggests that LLM-based agents become increasingly human-like in their performance, sparking interest in using these AI agents as substitutes for human participants in behavioral studies. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. Hence, it is crucial to study and pinpoint the key behavioral distinctions between humans and LLM-based agents. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates on topics that are important aspects of people`s day-to-day lives and decision-making processes. Our findings indicate a tendency for LLM agents to conform to the model`s inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations."
}

@inproceedings{wright2024llmtropes,
    title = "{LLM} Tropes: Revealing Fine-Grained Values and Opinions in Large Language Models",
    author = "Wright, Dustin  and
      Arora, Arnav  and
      Borenstein, Nadav  and
      Yadav, Srishti  and
      Belongie, Serge  and
      Augenstein, Isabelle",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.995/",
    doi = "10.18653/v1/2024.findings-emnlp.995",
    pages = "17085--17112",
    abstract = "Uncovering latent values and opinions embedded in large language models (LLMs) can help identify biases and mitigate potential harm. Recently, this has been approached by prompting LLMs with survey questions and quantifying the stances in the outputs towards morally and politically charged statements. However, the stances generated by LLMs can vary greatly depending on how they are prompted, and there are many ways to argue for or against a given position. In this work, we propose to address this by analysing a large and robust dataset of 156k LLM responses to the 62 propositions of the Political Compass Test (PCT) generated by 6 LLMs using 420 prompt variations. We perform coarse-grained analysis of their generated stances and fine-grained analysis of the plain text justifications for those stances. For fine-grained analysis, we propose to identify tropes in the responses: semantically similar phrases that are recurrent and consistent across different prompts, revealing natural patterns in the text that a given LLM is prone to produce. We find that demographic features added to prompts significantly affect outcomes on the PCT, reflecting bias, as well as disparities between the results of tests when eliciting closed-form vs. open domain responses. Additionally, patterns in the plain text rationales via tropes show that similar justifications are repeatedly generated across models and prompts even with disparate stances."
}

