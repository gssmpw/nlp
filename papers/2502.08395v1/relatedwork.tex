\section{Related Work: Issue Bias in LLMs}
\label{sec: related work}

Most prior work has used multiple-choice questions to measure issue bias in LLMs.
The popular OpinionQA datasets, for example, test LLMs on multiple-choice questions from large-scale social surveys \citep{santurkar2023opinionqa,durmus2024globalopinionqa}.
Other works use questionnaires like the Political Compass Test to place LLMs on a political spectrum \citep{fujimoto2023revisiting,hartmann2023political,motoki2023more,rutinowski2024self,rozado2023political,rozado2024political}.
Evaluations like these, however, bear little resemblance to real user interactions with LLMs, which has led others to call for greater ecological validity in measuring LLM bias \citep{lum2024bias,rottger2024political,saxon2024benchmarks}.
IssueBench follows this call by testing LLMs with prompts that mirror real LLM usage for the popular use case of writing assistance.
Other recent work has also evaluated LLM issue biases in open-ended settings \citep{bang2024measuringpoliticalbias,moore2024consistent,potter2024hiddenpersuaders,taubenfeld2024systematic,wright2024llmtropes}.
A table in Appendix~\ref{app: related work} compares these works to our own.
In short, IssueBench is much larger, covering more diverse issues with thousands of realistic prompts per issue, leading to more robust and comprehensive results.
IssueBench is also the only dataset that is explicitly grounded in realistic LLM usage at the prompt level, substantially increasing its ecological validity compared to prior work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%