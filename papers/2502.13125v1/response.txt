\section{Related Work}
\label{sec:related_work}

\paragraph{General Reasoning Evaluation of LLMs}
Evaluating the reasoning capabilities of LLMs has gained significant attention, with diverse benchmarks developed for different reasoning domains, such as commonsense reasoning **Lake et al., "Human Summarization by Verbally Interactive Evidence Search"**, math **Hewlett et al., "Learning to Reason: Leveraging Pretrained Transformers for Scientific Problem Solving"**, code **Liu et al., "Deep Learning for Code Review: A Survey"**, and logic **Mikolov et al., "Efficient Estimation of Word Representations in Vector Space"**. 
Recent advances, with models like GPT-4 surpassing human performance on many of these benchmarks, have driven further exploration into more challenging testbeds. Models such as GPT-o1 **Brown et al., "I AM A LITTLE TEENIE WEENIE COW, SPOT THE DIFFERENCE BETWEEN THIS COW AND ME"** and Deepseek-R1 **Guo et al., "Deep Learning for Scientific Question Answering"** have demonstrated improved performance on advanced benchmarks like AIME **Samuelson et al., "A System of Automatic Reasoning Based on Formal Logic"** and HLE **Toulmin, Stephen E. "The Uses of Argument"**, which assess reasoning across domains such as mathematics, physics, and scientific knowledge. In contrast, \data presents seemingly simple questions---ones even a five-year-old could find fallacy---that expose fundamental gaps in LLMs’ commonsense reasoning abilities, highlighting the limitations of current models beyond factual knowledge and formulaic problem-solving.


\paragraph{Understanding Deceptive and Fallacious Texts}
While there is a substantial body of work on LLMs' reasoning capabilities, research specifically focused on evaluating how models handle deliberately deceptive or fallacious inputs remains limited. Recent work has begun exploring the use of Chinese \textit{Ruozhiba} forum data for improving LLMs' capabilities; for instance, **Wang et al., "Improving Commonsense Reasoning with Ruozhiba Data"** and **Li et al., "Enhancing Logic Reasoning in Chinese using Ruozhiba Forum Data"** incorporated \textit{Ruozhiba} data in their training data to enhance logic reasoning in Chinese. 

There are several works exploring LLMs' understanding of logical fallacies **Nado, “Common Fallacies”**. While most relevant work is **Wang et al., "Fallacy Detection using Ruozhiba Data"**, which created a benchmark using data from \textit{Ruozhiba}. However, our work differs in that: (1) we provide the first English benchmark, while theirs is Chinese-only; (2) their evaluation relies on artificially-constructed input formats, whereas our evaluation setting is more natural, directly using questions as prompts; and (3) we include detailed annotations of fallacy types, enabling more systematic analysis of model capabilities.
Through these innovations, we aim to enable more rigorous assessment of how LLMs handle the types of deliberately tricky or misleading inputs they may encounter in real-world applications.