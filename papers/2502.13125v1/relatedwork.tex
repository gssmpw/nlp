\section{Related Work}
\label{sec:related_work}

\paragraph{General Reasoning Evaluation of LLMs}
Evaluating the reasoning capabilities of LLMs has gained significant attention, with diverse benchmarks developed for different reasoning domains, such as commonsense reasoning \cite{commonsenseqa,hellaswag,arc,Bisk2020}, math \cite{gsm8k,math}, code \cite{humaneval, mbpp}, and logic \cite{liu2020logiqachallengedatasetmachine,logiqa2,liu2023evaluatinglogicalreasoningability}. 
Recent advances, with models like GPT-4 surpassing human performance on many of these benchmarks, have driven further exploration into more challenging testbeds. Models such as GPT-o1 \cite{openai2024openaio1card} and Deepseek-R1 \cite{deepseekai2025deepseekr1incentivizingreasoningcapability} have demonstrated improved performance on advanced benchmarks like AIME \cite{aime} and HLE \cite{phan2025humanitysexam}, which assess reasoning across domains such as mathematics, physics, and scientific knowledge. In contrast, \data presents seemingly simple questions---ones even a five-year-old could find fallacy---that expose fundamental gaps in LLMsâ€™ commonsense reasoning abilities, highlighting the limitations of current models beyond factual knowledge and formulaic problem-solving.


\paragraph{Understanding Deceptive and Fallacious Texts}
While there is a substantial body of work on LLMs' reasoning capabilities, research specifically focused on evaluating how models handle deliberately deceptive or fallacious inputs remains limited. Recent work has begun exploring the use of Chinese \textit{Ruozhiba} forum data for improving LLMs' capabilities; for instance, \citet{lin2024baichuanalignmenttechnicalreport} and \citet{bai2024coigcqia} incorporated \textit{Ruozhiba} data in their training data to enhance logic reasoning in Chinese. 

There are several works exploring LLMs' understanding of logical fallacies \cite{lei-huang-2024-boosting,payandeh2023susceptiblellmslogicalfallacies,li2024reasonfallacyenhancinglarge}. While most relevant work is \citet{li2024when}, which created a benchmark using data from \textit{Ruozhiba}. However, our work differs in that: (1) we provide the first English benchmark, while theirs is Chinese-only; (2) their evaluation relies on artificially-constructed input formats, whereas our evaluation setting is more natural, directly using questions as prompts; and (3) we include detailed annotations of fallacy types, enabling more systematic analysis of model capabilities.
Through these innovations, we aim to enable more rigorous assessment of how LLMs handle the types of deliberately tricky or misleading inputs they may encounter in real-world applications.