\vspace{-3mm}
\section{Related Works}\label{sec:related}

\paragraph{Machine Unlearning}

Machine unlearning is gaining recognition for its significance and potential, yet it remains a relatively under-explored field. Recent studies \cite{chen2023unlearn, jang2022knowledge, ilharco2022editing, zhang2023composing} have begun to address aspects of text generation within this context.
Prior research \cite{qiu2024pistol, tofu} has highlighted the limitations of current unlearning methods, noting their extreme sensitivity to hyperparameter tuning and a lack of robustness in structural unlearning. These challenges complicate their deployment in practical, real-world applications.
Moreover, several survey papers \cite{liu2024rethinking, nguyen2022survey} have started to establish insightful connections between LLMs unlearning and related domains, such as model explainability within activation spaces. Our study includes several widely recognized unlearning baselines in Appendix \ref{app:unlearning_methods_baselines}.

%First and the most standard one is the Gradient Ascent (GA) \cite{jang2022knowledge, yao2023large}. It is the most straightforward method that performs gradient ascent on the forget data to maximize the likelihood of mispredictions for those samples within the forget set.
%The Gradient Difference (GD) method \cite{liu2022continual} builds on the concept of GA by optimizing two distinct losses: one that maximizes mispredictions on the forget set and another that minimizes mispredictions on the retained set. This dual approach facilitates the unlearning of the forget set while preserving performance on the retained set. 
%The KL method seeks to minimize the Kullback-Leibler (KL) divergence between the predictions of the original fine-tuned model and the unlearned model on the retained set \(\mathcal{D}_r\), thereby enhancing model utility on retained data while maximizing loss on the forget set \cite{tofu}. 
%The DPO approach aligns the model to avoid disclosing information from the forget set by computing loss using question-answer pairs \(x_{idk} = [q, a_{idk}]\) from the forget set \(\mathcal{D}_f\), with answers replaced by variations of 'I don't know'. 
%NPO, negative preference optimization \cite{npo}, is another alignment-inspired method specifically designed for LLM unlearning, aiming to mitigate the catastrophic collapse to the divergent nature of the GA algorithm. The aim of NPO is to treat response given the instruction by providing only negative response without any positive response and optimize for the negative responses.
%EUL \cite{eul} presents an efficient unlearning method that updates LLMs without necessitating full model retraining after data removal. It introduces lightweight unlearning layers, learned through a selective teacher-student objective, into transformers. However, EUL adds an additional unlearning layer to the pre-trained model and employs a fusion mechanism to integrate different unlearning layers, each designed to forget distinct data sets, thereby managing a sequence of forgetting operations. 


%\paragraph{LLM features and activations} LLMs are widely believed to represent features, or concepts, as linear directions within their activation space \cite{mikolov2013linguistic, elhage2022toy, park2023linear}. Recent research has investigated the linear representation of specific features, such as harmlessness \cite{wolf2024tradeoffs, zheng2024prompt}, sentiment \cite{tigges2023linear}, and refusal \cite{single_direction}, among others. These features are often derived from contrastive input pairs \cite{panickssery2023steering} and have been shown to enable effective inference-time control of model behavior \cite{hernandez2023inspecting, stickland2024steering} or the targeted removal of knowledge from model parameters \cite{ravfogel2020null}. On the other hand, the difference-in-means method effectively isolates key feature directions, as demonstrated in prior work \cite{marks2023geometry, stickland2024steering}, allowing for effective separation and steering of the LLM in the activation space. 
\vspace{-3mm}
\paragraph{LLM Features and Activations}
LLMs are widely believed to represent features or concepts as linear directions within their activation space \cite{mikolov2013linguistic, elhage2022toy, park2023linear}. Recent research has explored the linear representation of specific features, such as harmlessness \cite{wolf2024tradeoffs, zheng2024prompt}, sentiment \cite{tigges2023linear}, and refusal \cite{single_direction}, among others. These features are often derived from contrastive input pairs \cite{panickssery2023steering} and have been shown to enable effective inference-time control of model behavior \cite{hernandez2023inspecting, stickland2024steering} or the targeted removal of knowledge from model parameters \cite{ravfogel2020null}. 
Additionally, the difference-in-means method has proven effective in isolating key feature directions, as demonstrated in prior work \cite{marks2023geometry, stickland2024steering}. This approach allows for effectively separating and steering LLMs within the activation space.
This paper extends these approaches by subjecting linear features to perturbations applied to the forget set of the modelâ€™s embedding space during unlearning. This establishes a link between interpretability and robust unlearning methods for LLMs.


%\paragraph{Adversarial attacks}......
% \cite{fan2024not}.








