\vspace{-2mm}
\section{Conclusion}
%We propose \lunar, a simple and effective method for LLM unlearning. \lunar makes steps towards addressing key challenges, including achieving a desirable balance between unlearning efficacy and retained model utility, and mitigating common side effects of existing approaches, particularly the lack of controllability in unlearned models. Our results show that \lunar achieves SOTA performance in unlearning effectiveness while enhancing controllability, enabling unlearned models to coherently, and contextually express their inability to respond to unlearned data. Additionally, through theoretical analysis, we prove the existence of a closed-form solution for \lunar that ensures convergence and demonstrate its significant computational efficiency gains. Empirical analysis further demonstrates \lunar's robustness against adversarial attacks and its versatility in addressing real-world applications.
We propose \lunar, a simple and effective LLM unlearning method that balances unlearning efficacy and model utility while mitigating common side effects, particularly the lack of controllability. \lunar achieves SOTA unlearning performance, enabling models to coherently and contextually express their inability to respond. Additionally, through theoretical analysis, we prove the existence of a closed-form solution for \lunar that ensures convergence and demonstrates its significant computational efficiency gains. Empirical analysis further demonstrates \lunar's robustness against adversarial attacks and its versatility in addressing real-world applications.

\vspace{-2mm}
\paragraph{Limitations and Future Works.}
\lunar redirects the representation of the forget set to regions that activate its ability to express an inability to respond. This relies on the model’s capacity to refuse by triggering safety mechanisms or acknowledging a lack of knowledge. Future work could explore reference datasets with improved precision for mapping these regions or enhancing the model’s general refusal capabilities, particularly for fine-tuned models. This study also represents an initial step in bridging recent advancements in the interpretability of LLMs with robust unlearning. Further research could explore how other interpretability tools might improve unlearning effectiveness and controllability, advancing reliable unlearning methodologies.