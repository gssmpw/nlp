\begin{table}[h]
    \centering
    \captionsetup{font=small,labelfont=bf}
    \caption{Performance of applying LoRA atop LUNAR (Base) across base models on the PISTOL dataset. It demonstrates that LUNAR is compatible with LoRA, which can yield additional speed improvements while maintaining similar unlearning performance. }
    \scalebox{0.7}{
    \begin{tabular}{cccc}
        \toprule
         & \textbf{Forget} & \textbf{Retain} & \textbf{Refusal}\\
        \textbf{Model} & \textbf{ROUGE1 $\downarrow$} & \textbf{ROUGE1 $\uparrow$} & \textbf{Quality}\\
        \midrule
        Llama2-7B & 0.070 & 0.923 & 0.566 \\
        Gemma-7B & 0.020 & 0.995 &  0.758 \\
        Qwen2-7B & 0.075 & 0.952 &  0.160 \\
        \bottomrule
    \end{tabular}
    \label{tab:result_lora}
    }
\end{table}