\begin{table}[h]
\centering
\captionsetup{font=small,labelfont=bf}
\caption{Performance of sequential unlearning. First, we ensure best possible unlearning of all data points between entities A and B. Then we perform unlearning of all data points between entities A and C in the PISTOL dataset. It is worth noting that unlearning sequentially using the baselines is proved to be an even more difficult hyper-parameter tuning job. For example, unlearning using GD on the Gemma model, a minor increase of learning rate could result in an insufficiently unlearned model to collapse both for the forget set and retain set ROUGE1 scores.
}
\scalebox{0.6}{
\begin{tabular}{@{}llccc@{}}
\toprule
 && \textbf{Forget} & \textbf{Retain} & \textbf{Refusal}\\
\textbf{Model}   & \textbf{Method}    &\textbf{ROUGE1 $\downarrow$} & \textbf{ROUGE1 $\uparrow$} & \textbf{Quality $\uparrow$}\\ \midrule
\multirow{7}{*}{\textbf{Llama2-7B}} 
    & Retrain & 0.247 $\pm$ 0.010 & 1.000 $\pm$ 0.000 & 0.352 $\pm$ 0.007\\
    & GA &  0.112 $\pm$ 0.007 & 0.145 $\pm$ 0.032 & 0.332 $\pm$ 0.024\\
    & GD &  0.495 $\pm$ 0.008 & 0.850 $\pm$ 0.007   & 0.346 $\pm$ 0.052 \\
    & UKL &  0.102 $\pm$ 0.096  & 0.213 $\pm$ 0.042 & 0.314 $\pm$ 0.009 \\
    & DPO & 0.141 $\pm$ 0.049 & 0.565 $\pm$  0.012& 0.603 $\pm$ 0.024\\
    & NPO & 0.165 $\pm$ 0.034 & 0.419 $\pm$ 0.024 & 0.347 $\pm$ 0.001 \\
    & LUNAR (Base)   & 0.003 $\pm$ 0.001 & 0.848 $\pm$ 0.009 &   0.630 $\pm$ 0.042 \\
    & LUNAR (Top-K)  & 0.012 $\pm$ 0.003 & 0.856 $\pm$ 0.009  & 0.662 $\pm$ 0.010\\ \midrule
\multirow{7}{*}{\textbf{Gemma-7B}} 
    & Retrain & 0.209 $\pm$ 0.009 & 1.000 $\pm$ 0.000 & 0.356  $\pm$ 0.004 \\
    & GA & 0.000 $\pm$ 0.000 & 0.017 $\pm$ 0.006 & 0.404 $\pm$ 0.006 \\
    & GD &  0.731 $\pm$ 0.147 & 0.241 $\pm$ 0.167  & 0.384  $\pm$ 0.034\\
    & UKL & 0.975  $\pm$ 0.000  & 1.000 $\pm$ 0.000 & 0.350 $\pm$ 0.001\\
    & DPO & 0.586  $\pm$  0.075 & 0.947  $\pm$ 0.023 & 0.527 $\pm$ 0.014 \\
    & NPO & 0.056 $\pm$ 0.013 & 0.172 $\pm$ 0.022 & 0.422 $\pm$ 0.005 \\
    & LUNAR (Base)   & 0.098 $\pm$ 0.091 & 0.823 $\pm$ 0.018 & 0.636 $\pm$ 0.026\\
    & LUNAR (Top-K)  & 0.103 $\pm$ 0.084 & 0.828 $\pm$ 0.010  & 0.635 $\pm$ 0.017\\ \midrule
\multirow{7}{*}{\textbf{Qwen2-7B}} 
    & Retrain & 0.209 $\pm$ 0.004 & 1.000 $\pm$ 0.000 & 0.350  $\pm$ 0.004  \\
    & GA &  0.060 $\pm$ 0.037  & 0.227 $\pm$ 0.015 & 0.350 $\pm$ 0.009 \\
    & GD &  0.265 $\pm$  0.021  & 0.688 $\pm$ 0.010  & 0.361  $\pm$ 0.022 \\
    & UKL & 0.228  $\pm$ 0.034   & 0.328 $\pm$ 0.034 & 0.483 $\pm$ 0.050\\
    & DPO &  0.250 $\pm$ 0.006  & 0.672 $\pm$ 0.018  &  0.608   $\pm$ 0.019 \\
    & NPO & 0.121 $\pm$ 0.019 & 0.500 $\pm$ 0.066 & 0.354 $\pm$ 0.001\\
    & LUNAR (Base)   & 0.052 $\pm$ 0.024 & 0.777 $\pm$ 0.033 & 0.620 $\pm$ 0.003\\
    & LUNAR (Top-K)  & 0.044 $\pm$ 0.039 & 0.806 $\pm$ 0.021  & 0.625 $\pm$ 0.014 \\ \bottomrule
\end{tabular}
\label{tab:seq}
}
\end{table}