\section* {Appendix}

%\section{Refusal Token}
%\begin{table*}[ht]
%\label{tab:refusal_tokens}
%\caption{The refusal token set \( \mathcal{R} \) used for each model %family, along with their corresponding refusal phrases.}
%\centering
%\scalebox{0.9}{
%\begin{tabular}{c|c|c}
%\toprule
%\textbf{Model Family} & \textbf{Refusal Token Set \( \mathcal{R} \)} & %\textbf{Corresponding Refusal Phrases} \\
%
%\midrule
%
%\textbf{Llama-3 Instruct} & \{40\} & \{`I'\} \\ 
%\textbf{Llama-2 Chat} & \{306\} & \{`I'\} \\ 
%
%\bottomrule
%
%\end{tabular}
%}
%\end{table*}
\section{Algorithm}\label{app:algo}
\input{section/3_algo}

\section{Proofs}\label{app:proofs}
\begin{lemma} \label{app:lemma_1}
Let \( [H_f, H_r] \in \mathbb{R}^{m \times n} \) (with \( m \geq n \)). The Gram matrix \( [H_f, H_r]^\top [H_f, H_r] \) is invertible if and only if the columns of \( [H_f, H_r] \) are linearly independent.
\end{lemma}

\begin{proof}
Let \(G = [H_f, H_r]^\top [H_f, H_r] \) be a Gram matrix, where \( G \in \mathbb{R}^{n \times n} \) and \( G_{ij} = \langle [H_f, H_r]_i, [H_f, H_r]_j \rangle \), the inner product of column vectors \( [H_f, H_r]_i \) and \( [H_f, H_r]_j \).

% 2. **Invertibility and Linear Independence**:
%    The Gram matrix \( G \) is invertible if and only if its determinant is nonzero. This happens if and only if the columns of \( [H_f, H_r] \) are linearly independent. To see this:
   
Suppose \( G \) is not invertible, then there exists a nonzero vector \( v \in \mathbb{R}^n \) such that:
     \[
     G v = [H_f, H_r]^\top [H_f, H_r] v = 0.
     \]
Multiplying \( v^\top \), we have:
     \[
     v^\top G v = v^\top [H_f, H_r]^\top [H_f, H_r] v = \| [H_f, H_r] v \|_2^2 = 0.
     \]
It follows that \( [H_f, H_r] v = 0 \), implying \( v \) lies in the null space of \( [H_f, H_r] \). Therefore, if \( v \neq 0 \), the columns of \( [H_f, H_r] \) are linearly dependent. Conversely, if the columns of \( [H_f, H_r] \) are linearly independent, then \( [H_f, H_r] v = 0 \) implies \( v = 0 \). Hence, the null space of \( [H_f, H_r] \) is trivial, and \( G = [H_f, H_r]^\top [H_f, H_r] \) is invertible.
\end{proof}


\subsection{Close-form solution of weight optimization}\label{app:closed_form_solution_proof}
We have shown in Section \ref{sec:method_training} that the activation recalibration is equivalent to solving the following optimization problem:
\[
\widehat{W} = \arg\min_{W} \| [H_f, H_r] W - [A_f, A_r] \|_2^2,
\]
where \( [H_f, H_r] \) is a matrix formed by horizontally concatenating two feature matrices \( H_f \) and \( H_r \), \( [A_f, A_r] \) is the target matrix formed by horizontally concatenating \( A_f \) and \( A_r \), \( W \) is the weight of down-projection layer to be optimized, and \( \| \cdot \|_2 \) denotes the Frobenius norm.

Expanding the Frobenius norm, we have:
\begin{align*}
\| [H_f, H_r] W - [A_f, A_r] \|_2^2 
&= \text{tr} \left( ([H_f, H_r] W - [A_f, A_r])^\top ([H_f, H_r] W - [A_f, A_r]) \right) \\
&= \text{tr} \left( ([H_f, H_r] W)^\top [H_f, H_r] W \right) \\
&\quad - 2 \, \text{tr} \left( W^\top [H_f, H_r]^\top [A_f, A_r] \right) \\
&\quad + \cancel{\text{tr} \left( [A_f, A_r]^\top [A_f, A_r] \right)}.
\end{align*}

where \( \text{tr}(\cdot) \) denotes the trace of a matrix and we ignore the last term for optimization purposes as it is constant with respect to \( W \). 

We compute the gradient of the objective function with respect to \( W \).

\begin{align*}
\frac{\partial}{\partial W} \| \cdot \|_2^2 
&= \frac{\partial}{\partial W} \text{tr} \left( W^\top [H_f, H_r]^\top [H_f, H_r] W \right) 
- 2 \frac{\partial}{\partial W} [ \text{tr} \left( W^\top [H_f, H_r]^\top [A_f, A_r] \right) \\[5pt]
&= 2 [H_f, H_r]^\top [H_f, H_r] W 
    - 2 [H_f, H_r]^\top [A_f, A_r].
\end{align*}

Setting this to zero, we have:
\[
2 [H_f, H_r]^\top [H_f, H_r] W - 2 [H_f, H_r]^\top [A_f, A_r] = 0.
\]

\[
[H_f, H_r]^\top [H_f, H_r] W = [H_f, H_r]^\top [A_f, A_r].
\]

\[
W = \left([H_f, H_r]^\top [H_f, H_r]\right)^{-1} [H_f, H_r]^\top [A_f, A_r].
\]

Should $[H_f, H_r]$ be not full rank, Lemma \ref{app:lemma_1} implies the inverse or pseudo-inverse operation of $[H_f, H_r]^\top [H_f, H_r]$ may be unstable or ill-defined. Hence, we introduce a Tikhonov regularization and modify the objective function as follows:
\[
\widehat{W} = \arg\min_{W} \|[H_f, H_r]W - [A_f, A_r]\|_2^2 + \lambda \|W\|_2^2,
\]
where $\lambda \geq 0$ is the regularization parameter. When $\lambda > 0$, this term penalizes large norm solutions and ensures invertibility of the modified system.

Following the same approach, it is trivial to derive the modified solution as:
\[
W = ([H_f, H_r]^\top [H_f, H_r] + \lambda I)^{-1}[H_f, H_r]^\top [A_f, A_r].
\]

This concludes the derivation of a closed-form solution of weight optimization.


%\section{Layer Selection} \label{app:layer_selection}

%\xinchi{insert layer results}

\clearpage

\section{Experiments Setup}\label{app:exp_setup}

\subsection{Dataset}\label{app:dataset}
We evaluate \lunar and all baseline methods using the PISTOL dataset \citep{qiu2024pistol} and TOFU dataset \citep{tofu}, both of which are designed specifically for assessing unlearning methods in LLMs.

\textbf{PISTOL Dataset.} The PISTOL dataset is derived from the PISTOL dataset compilation pipeline, which is designed to flexibly create synthetic knowledge graphs with arbitrary topologies for studying structural LLM unlearning. Our experiments are conducted on Sample Dataset 1, provided by the dataset authors, which includes 20 contractual relationships, each with 20 question-answer pairs. The dataset benefits from entirely random generation of information, such as entity names and addresses, ensuring independence from GPT or other pretrained models. This removes confounding risks with the pretrained data corpus and provides a more controlled environment for studying LLM unlearning. Additionally, the PISTOL dataset offers concise ground truth in the QA pairs, minimizing the influence of text length on evaluation metrics like mean reciprocal rank (MRR) and top hit ratio (THR). This ensures more consistent comparisons of unlearning performance across methods.

\textbf{TOFU Dataset.} TOFU is another synthetic dataset widely used for evaluating LLM unlearning. It comprises 200 fictitious author profiles, each containing 20 question-answer pairs generated by GPT-4 based on predefined attributes. In our experiments, following the standard setup for unlearning tasks, we unlearn all QA pairs associated with the "forgetting" author.


\subsection{Metrics}\label{app:metrics}
We assess \lunar and all baseline methods in terms of both the Unlearning Effectiveness and Refusal Quality.

In terms of Unlearning Effectiveness, a post-unlearning model should achieve a favorable tradeoff between forget quality and model utility. Unlearning efficacy measures the model's inability to generate unlearned data, resembling a model that has never been trained on the forget dataset. Model utility evaluates how well the model responds to retained data and its ability to maintain other factual knowledge embedded in the pre-trained model. We follow prior research \cite{qiu2024pistol} and utilize a diverse set of metrics, including the ROUGE score (widely used for QA tasks), Mean Reciprocal Rank (MRR), and Top Hit Ratio (THR) (commonly employed in information retrieval and knowledge graph completion). These metrics are drawn from both QA and information retrieval domains to ensure a comprehensive evaluation of unlearning performance.

\textbf{ROUGE score:} We compute the ROUGE score, a metric that measures the accuracy of the model’s response compared to reference answers. Specifically, we focus on the ROUGE-1 recall score \citep{lin2004rouge}, which serves as a proxy for accuracy in question-answering tasks. This metric is particularly suited for scenarios where the phrasing of the output may differ slightly from the ground truth while preserving semantic correctness.

\textbf{Mean reciprocal rank (MRR).} MRR ia a metric commonly used in LLM evaluation to measure the quality of its ranked predictions. A LLM generated response is usually composed of multiple tokens. Therefore, we use the reciprocal average of the rank of each target (ground truth) token to measure the model’s memorization of names. Given a prefix $Q$, an output answer token sequence $E = {e_1, ..., e_n}$, with the length of $|E|$, the model predicts the rank of the target token as
$rank(e_i|Q)$, and then MRR for the name $E$ is calculated as follows:
\begin{equation}
    MRR = \frac{\sum_{i=1}^{|E|} 1/rank(e_i,Q)}{|E|}
\end{equation}

\textbf{Top hit ratio (THR).} THR is a binary score for each output token, indicating the presence of the correct token at the top $m$ values in the output logits, denotes as $hit(e_i, m)$. Also, given the output sequence $E = {e_1, ..., e_n}$, and we set $m=100$ in our experiments.
\begin{equation}
    Hit = \frac{\sum_{i=1}^{|E|}hit(e_i, m)}{|E|}
\end{equation}

\textbf{Refusal Quality.} It measures the cosine similarity between the sentence-level embeddings of responses generated by the unlearned model and a set of desirable responses which provide coherent and reasoned phrases such as `I apologize, but this information cannot be provided', `I don’t have the specifics you’re looking for', or `I cannot access or provide information that is not publicly available'. A higher Refusal Quality score indicates more controlled outputs with better alignment with the desired response behavior — specifically, generating coherent responses that accurately convey the unlearned model's inability to respond. The rationale for introducing this metric is to address the lack of controllability in text generation with existing unlearning methods, which often produce hallucinations \citep{farquhar2024detecting} or incoherence. We consider these issues critical to resolve for unlearning to be viable in real-world commercial applications.


\section{Unlearning method baselines}\label{app:unlearning_methods_baselines}

We experiment with several unlearning methods summarized in the survey paper \cite{liu2024rethinking, tofu}, each of which is introduced in detail in the section. 

\textbf{GA-based methods.} 
A major branch of LLM unlearning methods is built on the concept of performing Gradient Ascent (GA) on the forget data \cite{jang2022knowledge, yao2023large}, which is mathematically equivalent to applying Gradient Descent on the negative cross-entropy loss function (Eq. \ref{eq:GA_prediction_loss}). The objective of GA is to maximize the likelihood of mispredictions for samples in the forget set, effectively reducing the model's ability to recall or generate the unlearned information.

\vspace{-5mm}
\begin{equation}
    \mathcal{L}_{\phi} (\mathcal{D}_f)= - \mathbb{E}_{\text{D}_f}\left[{-\log \phi_\theta(y|x)} \right] 
    = \mathbb{E}_{\text{D}_f}\left[ \log \phi_\theta(y|x) \right].
    \label{eq:GA_prediction_loss}
\end{equation}

Several unlearning methods build upon GA to improve the tradeoff between forget quality and model utility by linearly combining an additional loss term with the GA loss. Gradient Difference (GD) method \cite{liu2022continual} extends the GA approach by optimizing two distinct loss functions: one to maximize mispredictions on the forget set and another to minimize mispredictions on the retained set. Another GA-based variant (GA + KL) aims to minimize the Kullback-Leibler (KL) divergence between the predictions of the original fine-tuned model and the unlearned model on the retained set \cite{tofu}. These dual-objective framework aims to balance effective forgetting with the preservation of model utility.


%\textbf{Gradient Difference (GD).} 
%Several unlearning methods build upon GA to improve the tradeoff between forget quality and model utility by linearly combining an additional loss term with the GA loss. The GD method \cite{liu2022continual} extends the GA approach by optimizing two distinct loss functions, as described in Eq. \ref{eq:GD_loss}: one to maximize mispredictions on the forget set and another to minimize mispredictions on the retained set. This dual-objective framework aims to balance effective forgetting with the preservation of model utility.

%\vspace{-1mm}
%\begin{equation}
%    \mathcal{L}_{\phi} = - \mathcal{L}_{\phi}(\mathcal{D}_f) + \mathcal{L}_{\phi}%(\mathcal{D}_r) 
%    \label{eq:GD_loss}
%\end{equation}
%
%\textbf{GA + KL.} 
%Another GA-based variant aims to minimize the Kullback-Leibler (KL) divergence between the predictions of the original fine-tuned model and the unlearned model on the retained set, as described in Eq. \ref{eq:GA_KL_loss}. This approach enhances model utility on the retained data while simultaneously maximizing the loss on the forget set, achieving a better balance between forget quality and model utility \cite{tofu}.

%\vspace{-1mm}
%\begin{equation}
%    \mathcal{L}_{\phi} = -\mathcal{L}_{\phi}(\mathcal{D}_f) + \frac{1}%{|\mathcal{D}_r|} \sum_{x\in \mathcal{D}_r} \frac{1}{|x|} \sum_{i=2}^{|x|} %\text{KL}(M_{\text{pretrained}}(x_{<i} || M_{\text{unlearn}}(x_{<i}))
%    \label{eq:GA_KL_loss}
%\end{equation}

\textbf{Preference optimization-based methods.} 
DPO \citep{rafailov2024direct} is a preference alignment method that aligns the model to avoid disclosing information from the forget set by computing loss using question-answer pairs \(x_{idk} = [q, a_{idk}]\) from the forget set \(\mathcal{D}_f\), with answers replaced by variations of 'I don't know'. Unlike GA and its variants, DPO does not employ gradient ascent. Drawing inspiration from DPO, NPO \citep{npo} focuses on generating only negative responses to given instructions, without providing any positive or informative answers. The method optimizes exclusively for these negative responses, ensuring the model avoids revealing information from the forget set while maintaining stability. 

%The loss function is defined as (Eq. \ref{eq:dpo_loss}):

%\vspace{-1mm}
%\begin{equation}
%    \mathcal{L}_{\phi} = \mathcal{L}_{\phi}(\mathcal{D}_r) + \mathcal{L}_{\phi}%(\mathcal{D}_{f,idk})
%    \label{eq:dpo_loss}
%\end{equation}


%\textbf{Negative Preference Optimization (NPO).} 
%NPO \cite{npo} is an alignment-inspired method specifically developed for LLM unlearning, addressing the issue of catastrophic collapse often associated with the divergent nature of GA algorithms. Drawing inspiration from DPO, NPO focuses on generating only negative responses to given instructions, without providing any positive or informative answers. The method optimizes exclusively for these negative responses, ensuring the model avoids revealing information from the forget set while maintaining stability and alignment.


%EUL \cite{eul} presents an efficient unlearning method that updates LLMs without necessitating full model retraining after data removal. It introduces lightweight unlearning layers, learned through a selective teacher-student objective, into transformers. However, EUL adds an additional unlearning layer to the pre-trained model and employs a fusion mechanism to integrate different unlearning layers, each designed to forget distinct data sets, thereby managing a sequence of forgetting operations. 


\textbf{Hyperparameters.} We combine both forget dataset and retain dataset and randomly select from them to form a mini-batch for all of \lunar training. All baseline unlearning methods exhibit high sensitivity to learning rate tuning, necessitating extensive effort to avoid minimal unlearning or catastrophic collapse of model utility. Each method requires individualized tuning for every model and forget dataset to achieve optimal performance - specifically, learning rates were tuned to minimize the ROUGE1 score on the forget dataset, while ensuring that model utility - measured by the ROUGE1 score on the retain dataset - remains above circa 0.8. Table \ref{tab:LR_baselines} summarizes the tuned learning rates used for our experiments:
\input{section/table_and_pic/LR_baselines}




\clearpage

\section{Additional Experimental Results}\label{app:add_exp_results}

\subsection{TOFU Examples of Responses Post-Unlearning} \label{app:tofu_examples}
The table below provides examples of responses generated after applying \lunar and baseline methods on Llama2-7B fine-tuned with the TOFU dataset. These examples demonstrate that \lunar significantly enhances the coherence and contextual awareness of responses compared to baseline methods. 
\input{section/table_and_pic/4_tofu_examples}

\clearpage
\subsection{Additional Results on Unlearning Performance}
\label{app:additional_results}
%Tables \ref{tab:main_table_original} and \ref{tab:mrr} present additional results on the performance of \lunar compared to the baselines.

\input{section/table_and_pic/4_table}
\input{section/table_and_pic/app_mrr}

%Tables \ref{tab:main_table_original} and \ref{tab:mrr} present additional results, evaluating \lunar with re-finetuning using retained dataset as well as gradient-based and preference optimization-based baseline methods across PO-aligned models (Llama2-7B-chat and Gemma-7B-IT) and an FT-aligned model (Qwen2-7B-instruct), using ROUGE1, MRR and THR metrics. Hyperparameters for all methods were tuned to minimize the ROUGE1 score on the forget dataset, while ensuring that model utility - measured by the ROUGE1 score on the retain dataset - remains above circa 0.8. 


%Table \ref{tab:result_lora} presents the results of applying LoRA atop \lunar. It demonstrates that \lunar is compatible with LoRA, which can yield additional speed improvements while maintaining similar unlearning performance. 

\input{section/table_and_pic/4_table_lora}

\clearpage
%Table \ref{tab:seq} presents the results on the unlearning sequentially on the PISTOL dataset over various base models. It is worth noting that unlearning sequentially using the baselines is proved to be an even more difficult hyper-parameter tuning job. For example, unlearning using GD on the Gemma model, a minor increase of learning rate could result in an insufficiently unlearned model to collapse both for the forget set and retain set ROUGE1 scores.

\input{section/table_and_pic/4_table_seq}

%the learning rate of $2e-5$ gives ROUGES1 scores of $0.021$ and $0.241$ for forget and retained set respectively, but if we decrease the learning rate a little to $1e-5$, the ROUGE1 score will increase to $0.731$ and $0.958$ for the forget and retained set, which sit in two extreme case of 

\clearpage
\subsection{Examples of Responses After Layer-Skip Attack}\label{app:attack_res}
Table below presents the ground truth answers and the generations produced by the \lunar-unlearned model after applying the Layer Skip attack on Llama2-7B and Gemma-7B models fine-tuned with the PISTOL dataset. While the post-attack model correctly guessed some binary-choice questions, the unlearned model remains largely non-usable on the forget set, as evidenced by inaccurate responses (highlighted in \sethlcolor{pink}\hl{pink}) to the vast majority of questions.


\input{section/table_and_pic/app_skip_layer_attack_examples_llama2}

\input{section/table_and_pic/app_skip_layer_attack_examples_gemma}

%\subsection{Additional Results for Sequential Unlearning} \label{app:seq}
%\input{section/table_and_pic/app_seq}

%\subsection{Additional Results for Sequential Unlearning} \label{app:seq}
%\input{section/table_and_pic/app_seq}