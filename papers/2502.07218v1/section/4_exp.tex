\vspace{-2mm}
\section{Experiment} \label{sec:exp}
We propose our method as a novel, robust, and efficient alternative for LLM unlearning. In this section, we conduct experiments to evaluate \lunar's performance, focusing on the following research questions:

\begin{itemize}[noitemsep,topsep=0pt,parsep=2pt,partopsep=0pt]
    \item[\textbf{RQ1}] Does \lunar improve unlearning performance while maintaining model utility? (Sec.\ref{sec:exp_unlearning_performance})
    \item[\textbf{RQ2}] Does \lunar improve the controllability of LLM unlearning via 
    generating dynamic, contextually aware and coherent responses? (Sec.\ref{sec:exp_unlearning_performance})
    \item[\textbf{RQ3}] Is \lunar versatile in handling real-world applications, including unlearning data from different training stages and handling sequential unlearning tasks? (Secs.\ref{sec:exp_factual} and \ref{sec:unlearn_seq})
    \item[\textbf{RQ4}] Is \lunar robust against various adversarial attacks, both white-box and black-box? (Sec.\ref{sec:exp_attack})
\end{itemize}

\subsection{Experimental Setup} \label{sec:exp_setup}

\paragraph{Datasets} We evaluate \lunar on two LLM unlearning benchmark datasets: \textbf{TOFU} \citep{tofu} and \textbf{PISTOL} \citep{qiu2024pistol}. These datasets are specifically tailored for studying LLM unlearning in a controlled environment, featuring fictitious entities to mitigate confounding risks with data from the pre-training corpus. In addition to evaluating unlearning synthetic PISTOL or TOFU data from fine-tuned models (SFT data), we also examine \texttt{LUNAR}’s effectiveness on unlearning pre-trained data from the base model by utilizing a factual dataset provided by \citep{tofu}, which consists of common knowledge (e.g., `Who wrote the play Romeo and Juliet').

To conduct activation redirection, we use either \emph{harmful prompts dataset} \citep{arditi2024refusal} to activate the model's internal safety guardrails or an \emph{unverifiable prompts dataset} that we composed using GPT-4 consisting of 200 questions about fictitious objects (e.g., imaginary countries, creatures, laws, etc.) to which the base model responds with apologies and an acknowledgment of its inability to provide an answer. 

\paragraph{Metrics.} We evaluate \emph{unlearning effectiveness} by assessing the forget efficacy (how much the unlearned model's outputs deviate from the forget data) and model utility (the unlearned model's retained capabilities on data outside the forget set). Given the equal importance of the two competing objectives, we measure unlearning effectiveness by the Euclidean distance of forget and retained dataset ROUGE1 scores to their respective optimal states, which we call `Deviation Score (DS)'. In equation form,
$\text{DS} = 100 \times \sqrt{ \text{ROUGE1}_\text{forget}^2 + (1 - \text{ROUGE1}_\text{retain})^2}$.
Additionally, we extend the evaluation scope beyond prior work by measuring \emph{controllability} of unlearning (defined in Sec.\ref{sec:intro}) via `Refusal Quality'. We measure it by the cosine similarity between sentence-level embeddings of generated output and a set of desirable refusal phrases. More details and other supplementary metrics, including MRR and the Top Hit Rate, can be found in Appendix \ref{app:dataset} and \ref{app:metrics}.


\vspace{-2mm}
\paragraph{Models} Previous research has demonstrated that unlearning performance can vary depending on the base model \citep{qiu2024pistol}. We provide a comprehensive evaluation of the generality of \lunar by examining a range of model families, including Llama2-7B, Gemma-7B, and Qwen2-7B, encompassing models aligned via Preference Optimization (PO) and Fine-Tuning (FT) \citep{meade2024universal}.


\vspace{-2mm}

\paragraph{Unlearning Baselines} We compare \lunar against three gradient-based methods: Gradient Ascent (GA) \citep{jang2022knowledge, yao2023large}, Gradient Difference (GD) \citep{liu2022continual}, and GA with KL-divergence regularization (UKL), as well as two preference optimization (PO)-based methods: Direct Preference Optimization (DPO) \citep{rafailov2024direct} and Negative Preference Optimization (NPO) \citep{zhang2024negative}. These baseline methods are highly sensitive to learning rate and require extensive tuning to balance the competing objectives of forget efficacy and model utility. Each method demands separate tuning for each model and dataset. Since an unlearning method cannot be deemed successful if it massively degrades model utility on the retained dataset, we let retained performance above a threshold and select a learning rate that maximizes forget capability within this constraint. We have also compared with `retrain from scratch' (a form of exact unlearning), which is fine-tuning the base model using only the retained dataset. Detailed training parameters are provided in Appendix \ref{app:unlearning_methods_baselines}. 


\input{section/table_and_pic/unlearn_example_fig}

\subsection{Unlearning Performance}
\label{sec:exp_unlearning_performance}

Table \ref{tab:main_table} shows that \lunar achieves SOTA unlearning performance, as evidenced by lower deviation scores (up to 11.7x reduction vs. \lunar (base) on the PISTOL dataset with Gemma-7B model) and higher refusal quality scores. Additionally, examples in Table \ref{tab:pistol_examples} and Appendix \ref{app:tofu_examples} further visualize \lunar's superior controllability, significantly reducing hallucinations and improving the coherent expression of its inability to respond within the conversational context.


Interestingly, we also found that fine-tuning with the retained set (a form of exact unlearning) does not guarantee sufficient content regulation, as unlearned knowledge can be reintroduced in-context, allowing the model to behave as if it retains the forgotten knowledge. This echoes with arguments in \citep{shumailov2024ununlearning}. In contrast, \lunar achieves significantly improved unlearning by operating in the activation space, effectively but locally disrupting the model's generalization capabilities around the forget set.

Also, Table \ref{tab:result_lora} (Appendix \ref{app:add_exp_results}) presents results for combining PEFT methods, such as LoRA, with \lunar. The results demonstrate that \lunar maintains comparable unlearning performance, further underscoring its robustness and potential for further computational efficiency improvement.

%\input{section/table_and_pic/4_pistol_examples}
\input{section/table_and_pic/4_table_new}

\vspace{-2mm}
\subsection{Unlearning Pre-trained Data from Base Models} \label{sec:exp_factual}

We observe that modern LLMs exhibit, to some extent, an ability to express a lack of knowledge when prompted with fictitious or unverifiable questions. This ability is often significantly stronger in pre-trained models compared to SFT models. While unlearning SFT data is more effective through the redirection of residual stream activations to those of harmful features, unlearning pre-trained data is equally effective by redirecting the activations of the forget set to those either associated with the harmful prompts or unverifiable prompts. The effectiveness of \lunar in unlearning pre-trained data is presented in Table \ref{tab:factual}.
\input{section/table_and_pic/factual_unlearn}


\subsection{Unlearning Sequentially} \label{sec:unlearn_seq}
Another practical scenario in LLM unlearning deployment involves private data being removed incrementally over time, as unlearning requests arrive sequentially. Table \ref{tab:seq} (Appendix \ref{app:add_exp_results}) demonstrates that \lunar is robust to handle sequential unlearning,  whereas baseline methods exhibit brittleness when unlearning additional data on top of an already unlearned model. \lunar consistently achieves strong results across different models, comparable to the performance observed in single-round unlearning.

%\input{section/table_and_pic/4_table_seq}

%\subsection{Ablation Study}
%As outlined earlier, the loss function in Eq.\ref{eq:loss} redirects activations of the forget set while minimizing shifts in the activations of the retain set. The inclusion of the retain set ensures that the unlearned model remains aligned with the base model when handling queries outside the forget set. For \lunar to be effective, the retain set does not need to include all remaining data points — a requirement often computationally-heavy or even impractical in real-world scenarios — but can instead be a sampled subset of those points. Our empirical studies shows that the retained performance remains stable when the retain set is $1$ times of the forget set size. However, as expected, the performance declines if the retain set is further reduced. 

%Additionally, we investigate whether the distribution of the sampled retain set impacts unlearning performance. Specifically, when unlearning factual data points from the base model (Sec.\ref{sec:exp_factual}), we test this by mixing the retain set (composed of sampled factual data points assumed to share the same distribution as the forget set) with data from a different distribution (e.g., harmless prompts). Table \ref{} shows that model utility is preserved, indicating that \lunar is robust to sampling the retain set from a mixed distribution.


\section{Robustness Study on \lunar} \label{sec:exp_attack}
\input{section/table_and_pic/attack_table}
Given the nascent nature of LLM unlearning, its evaluation scope may sometimes be overly optimistic on the efficacy of unlearning methods, particularly as some simple yet effective attacks can yield unexpected results, revealing information that should have been successfully unlearned \citep{thaker2024position, liu2024rethinking, zhang2024does}. Therefore, we advocate for incorporating robustness studies in future research to ensure the integrity of the unlearning process.

In this section, we show the robustness of \lunar through three white-box attacks, which operate under strong assumptions that the attacker \textit{at least} possesses full knowledge of the model weights. Such attacks are of less concern if \lunar is performed on closed-source models. We also show \lunar is robust against prompt paraphrase attack which could target both white and black-box scenarios. Notably, when combining LoRA with \lunar, we merge the updated parameters back into the model to prevent adversarial attackers from bypassing the unlearning process by simply removing the modified model weights.

\vspace{-2mm}
\subsection{Layer Skip Attack} \label{sec:skip_attack}
The layer skip attack is designed to bypass the layers where MLPs have been optimized for activation redirection. 
We hypothesize that this attack is effective due to the ensemble nature of transformer architectures~\citep{NIPS2016_37bc2f75,chen2024jetexpansionsresidualcomputation}, where the final prediction can theoretically be interpreted as an ensemble of diverse computational paths.
Empirically, a number of recent works use layer skipping to accelerate inference speed \citep{chen2020low,Fan2020Reducing, fan2024not, elhoushi-etal-2024-layerskip}. In this work, however, we repurpose layer skipping to evaluate the robustness of our unlearning method against malicious attacks.

To address this, we perform activation redirection on the top-K layers identified through the layer selection process. For Llama2-7B model, selecting top-3 layers is an effective defense with the ROUGE-1 score only increasing marginally to c.0.1 (Table \ref{tab:attack}), indicating a minimal recovery of unlearned information. A closer examination of the generated outputs reveals that this minor increase primarily stems from two factors: (1) unigram matches between the generated text and the ground truth rather than accurate responses in its entirety, and (2) questions with binary choices where the model occasionally guesses correctly (refer to examples of post-attack responses in Appendix \ref{app:attack_res}). Overall, the unlearned model remains non-usable on the forget set, underscoring the robustness of \lunar against such attacks.


\vspace{-3mm}
\subsection{Reverse Direction Attack} \label{sec:reverse_direction_attack}

This attack strategy assumes a `white-box' attacker has full knowledge of the layer selection and the exact Unlearning Vectors (UVs) $\mathbf{r}^{(l)}_{UV}$ used in the unlearning process. In this case, the attacker performs reverse engineering in order to recover pre-unlearning activations by ablating the UV from post-unlearning activations of the selected layer. This is achieved by doing: $\mathbf{a}_{attack}^{(l)}(x) \leftarrow \mathbf{a}_{unlearned}^{(l)}(x) - \mathbf{r}_{\text{UV}}^{(l)}v$. 

We report the attack results in Table \ref{tab:attack}, demonstrating that it is ineffective against the \lunar unlearned model. We hypothesize that this robustness arises because the activation region corresponding to the refusal behavior is significantly broader than the specific activation region associated with the forget set. As for the forget set, knowledge is more precise, and even a small divergence from the correct answer can result in incorrect responses. This broader region, combined with the stochastic nature of the unlearning optimization process, prevents the loss from fully converging to zero during training. As a result, reversing the activation redirection process fails to map the activations back to their original state, thereby rendering the attack ineffective.

\vspace{-2mm}
\subsection{Quantization Attack} \label{sec:quantization_attack}

Unlearning methods tend to be subject to `utility' constraints, which require that the model retain as much of its original functionality and performance as possible on data that was not marked for unlearning. As recently observed by \citet{zhang2024does}, since the original models are finely converged, methods from the GA and PO families tend to be applied with small learning rates, thus modifying the model surgically and keeping the distance to the original parameters constrained. \citet{zhang2024does} observe that mere quantization to $8$ or $4$ bits is sufficient to bring such models close to the quantized form of their original parameters before the unlearning process, increasing their retention of intended forgotten knowledge by up to $4 \times$.

Our method, \lunar, was designed to heavily modify a subset of parameters, the down projections in an LLM, rather than subtly modifying most parameters. Thus, we postulate that it is likely to be far more resilient to quantization attacks than the GA and PO-based baselines, and we evaluate this by reproducing both the $4$-bit and $8$-bit attacks of \citet{zhang2024does}. We report the $4$-bit attacks in \cref{tab:attack}, as the $8$-bit quantization proved ineffective in our experiments.

As shown in \cref{tab:attack}, quantization attack only proves marginally effective for the Llama2-7B model, with the resultant model remaining non-usuable. Moreover, the decay in forget effectiveness is far below the one reported by \citet{zhang2024does} for GA and NPO. For the other models, quantization either does not change forget performance~(Gemma-7B) or further enhances forgetting~(Qwen2-7B).

%As discussed in Section \ref{sec:attack_method}, evaluating whether the proposed unlearning method can be effectively attacked and reversed to reveal the forgotten dataset is crucial for assessing the method's effectiveness. Here, we implement the strongest white-box attack, assuming adversarial attackers have full knowledge of the model parameters and the unlearning method. We have also implemented the SOTA attack method specifically designed for the LLM unlearning task. The experimental results are presented in Table \ref{tab:attack}. The results show that none of the attack methods can successfully reverse and reveal the forgotten dataset. The quantization method is particularly weak, as it only improves the ROUGE score from \add{x} to \add{x}. 

%\subsection{Resilience to query modification} \label{sec:query_modification}
%Prior literature showed that one vulnerability of existing methods is that modifications to queries can systematically either reveal unlearned information or degrade performance on retained information \ref{thaker2024position}.

\subsection{Prompt Paraphrase Attack} \label{sec:paraphrase}
%Effective LLM unlearning methods must accurately classify whether a prompt belongs to the forget set or the retain set. A simple yet effective attack attempts to trick the unlearned model into misclassifying a forgotten prompt as belonging to the retain set, thereby revealing information that should remain unlearned 

A common limitation in evaluating existing unlearning methods is their focus on accuracy degradation for queries directly related to the forget set. However, effective unlearning must generalize to similar samples sharing characteristics with the forget set, ensuring the process extends beyond specific instances to broader concepts. This enhances robustness, particularly against paraphrasing attacks \citep{thaker2024position, yao2023large}. To evaluate this, we compiled a set of paraphrased prompts from the PISTOL dataset using GPT-4 and ran inference on the \lunar unlearned model. \cref{tab:attack} demonstrates that paraphrased prompts fail to extract unlearned information from the \lunar unlearned model, showcasing its robustness against such attacks.




%the impact of the retained set’s distribution. In both the PISTOL and TOFU experiments, the retained set is sampled from the same distribution as the forget set. Our results indicate that mixing the retained set with factual data from the pretrained dataset yields equivalent retained performance, provided the retained set size remains constant. However, when unlearning factual data from the base model , we find that mixing the retained set with data from different distributions still preserves model utility.

%We hypothesize that the distributional limitation observed in the PISTOL and TOFU experiments stems from the constraints of the fine-tuned model, rather than the \lunar method itself.

%maintains robustness against  `white-box' attacks in Sec.\ref{sec:exp_attack}. Additionally, \lunar generalizes effectively across diverse model families and alleviates the need for extensive hyperparameter tuning that is essential for existing baselines.

%Our experimental results indicate that \add{our methods} is an efficient and effective unleanring method. Following our unlearning pipeline detailed in the methodology section, we will first use our method to choose the top-k layer to unlearn and then perform the unlearning training. The exact index of blocks to be unlearned can be found in the Appendix \ref{app:layer_selection} and the main results can be found in the Table \ref{tab:main}. 

%In summary, the results demonstrate that our method outperforms the baselines significantly across both datasets and across all pre-trained models. In particular, 

%Table \ref{tab:main_table} demonstrates that \lunar achieves SOTA unlearning performance, {\color{blue}evidenced by superior deviation scores (lower score indicates better balance between ROUGE1 scores for the forget and retain sets), as well as enhanced controllability of generated text post-unlearning (reflected in higher refusal quality scores).} Moreover, we provide examples of generated responses in Table \ref{tab:pistol_examples} and Appendix \ref{app:tofu_examples}. Those demonstrate the superior controllability of \lunar, not only significantly reducing hallucinations but also enhancing the unlearned model's ability to coherently communicate its inability to respond within the context of the conversation. 
