
\section{Background}\label{sec:method_background}



%\subsection{Background} \label{sec:method_background}
\paragraph{Transformers} 
We focus on transformer architecture and let $\mathcal{Z}$ denote an input space (e.g., sequences of tokens), $c \in \mathbb{N}^+$ the number of classes (e.g., vocabulary size), $\mathcal{Y} = \mathbb{R}^c$ the output logit space, and $d \in \mathbb{N}^+$ the hidden dimension. We consider the following functions $q: \mathcal{Z} \to \mathcal{Y}$:
%\vspace{-0.5cm}
\begin{equation}
q = \upsilon \circ h_L, \; \text{where }h_L: \mathcal{Z} \to \mathbb{R}^d, \; h_L = \bigcirc_{l=1}^L \beta_l \circ \eta
\label{eq:q_function}
\end{equation}

where $L \in \mathbb{N}^+$ is the number of residual blocks (i.e., layers), $\eta: \mathcal{Z} \to \mathbb{R}^d$ is the token embedding, and $\bigcirc$ denotes repeated functional composition. The residual blocks $\beta_l: \mathbb{R}^d \to \mathbb{R}^d $ for $l \in [L]$ and the output decoding module $\upsilon: \mathbb{R}^d \to \mathcal{Y}$ are defined as:

\vspace{-0.5cm}
\begin{equation}
\beta_l(x) = \mathrm{id}(x) + \gamma_l(x), \; \gamma_l: \mathbb{R}^d \to \mathbb{R}^d
\label{eq:residual_blocks}
\end{equation}
\vspace{-0.5cm}
\begin{equation}
\upsilon(x) = U \gamma_{L+1}(x), \; U \in \mathbb{R}^{c \times d}, \; \gamma_{L+1}: \mathbb{R}^d \to \mathbb{R}^d
\label{eq:decoder}
\end{equation}

where $\mathrm{id}$ is the identity map, $\gamma_l$ represents nonlinear transformations (e.g., input-normalized causal self-attentions or MLPs), $U$ is an unembedding projection applied after a layer normalization $\gamma_{L+1}$. 

Optimized for next-token prediction in autoregressive models, $q$ outputs logits as $P_q(\text{`$z$ belongs to class $i$'} \mid z) = \mathrm{Softmax}[q(z)]_i, \; z \in \mathcal{Z}.$

%An autoregressive transformer language model \citep{vaswani2017attention, radford2018improving} is defined as: \( \mathcal{M} : \mathcal{X} \rightarrow \mathcal{Y} \) over vocabulary \(\mathcal{V}\) mapping an input token sequence \( x = [x_1, \dots, x_t] \in \mathcal{X} \), with \( x_i \in \mathcal{V} \), to a probability distribution \( y \in \mathcal{Y} \subset \mathbb{R}^{|\mathcal{V}|} \) that predicts the next token \( x_{t+1} \). 
%

%The model is composed of $L$ residual blocks. Within each block $l \in [L]$, two modules compute updates that are added to the block input $a^{(l-1)}(x_i)$ (residual stream activation of the previous layer): (1) a multi-head self-attention module that outputs $\text{attn}^{(l)}(a^{(l-1)}(x_i))$ and (2) a multi-layer perceptron (MLP) that outputs $\text{mlp}^{(l)}(a^{(l-1)}(x_i) + \text{attn}^{(l)}(a^{(l-1)}(x_i)))$. Putting together, each token at position $i$ at block $l$ has a residual stream activation $a^{(l)}(x_i)$ computed as:

%\vspace{-0.5cm}
%\begin{multline} \label{eq:transformer}
%a^{(l)}(x_i) = a^{(l-1)}(x_i) + \text{attn}^{(l)}(a^{(l-1)}(x_i)) \\
%+ \text{mlp}^{(l)}\big(a^{(l-1)}(x_i) + \text{attn}^{(l)}(a^{(l-1)}(x_i))\big)
%\end{multline}
%\vspace{-0.5cm}

%\yihong{the above formula should be modified as follows $a^{(l)}(x_i) = a^{(l-1)}(x_i) + \text{atten}^{(l)}(a^{(l-1)}(x_i)) + \text{mlp}^{(l)}(a^{(l-1)}(x_i) + \text{atten}^{(l)}(a^{(l-1)}(x_i)))$}
%a^{(l)}(x_i) = a^{(l-1)}(x_i) + \text{atten}^{(l)}(x_i) + \text{mlp}^{(l)}(x_i)
%with contributions from the previous layer hidden state \( h^{(l-1)}_i \), the global attention \( a^{(l)}_i \) and local MLP \( m^{(l)}_i \).
%
%The residual stream activations post the final layer of the transformer are passed through the output block to get the final \(logits_{(i)} \), which are then transformed into probabilities over output tokens via a softmax function:  
%$y_i = \text{Softmax}(logits_{(i)}) \in \mathbb{R}^{|\mathcal{V}|}$.

\vspace{-2mm}
\paragraph{Unlearning} 
Given an original model $\mathcal{M}$, the unlearning algorithms aim to produce an unlearned model $\mathcal{M}'$, in which $\mathcal{M}$ effectively `forgets' the information in the forget set $\mathcal{D}_f$ while maintaining performance in the retain set $\mathcal{D}_r$. Ideally, the unlearned model $\mathcal{M}’$ should be indistinguishable from a model trained solely on $\mathcal{D}_r$ \citep{sekhari2021remember}. However, since measuring indistinguishability is usually intractable, performance comparisons between the re-trained model and the unlearned model are commonly used as a practical proxy \citep{kurmanji2024towards}.

%As explained in Sec.\ref{sec:intro}, existing evaluations of unlearning critically overlook the issue of uncontrollability in unlearning methods and we , including hallucination, rigid and monotonous responses, and nonsensical outputs. To address this, we define \textbf{controllability} as \textit{the unlearned model's ability to explicitly communicate its inability to respond to prompts related to the forget set in a dynamic, contextually aware, and coherent manner}. We measure it by the cosine similarity between sentence-level embeddings of generated output and a set of refusal phrases.

%\paragraph{LLM Safety} 
%Modern language models, especially those used in commercial applications, are often equipped with safety and alignment mechanisms \cite{huang2024survey, dai2023safe}, enabling them to effectively refuse certain types of requests. Leveraging and building upon this inherent capability, we aim to design a more controllable unlearning method that explicitly provides clear refusals to queries in the forget set. This approach eliminates the risk of generating random or misleading responses, which could otherwise confuse end users.

%\subsection{Unlearning Vectors (UV)}\label{sec:method_uv}

%\subsection{Unlearning via Neural Activation Redirection}\label{sec:method_training}
\section{\lunar}\label{sec:method_training}
In this section, we introduce \lunar method and layer selection in Secs.\ref{sec:method_training} and \ref{sec:layer_selection}, and conclude with an analysis of \lunar's memory and computational costs. Algorithm pseudo-code can be found in Appendix \ref{app:algo}.

\subsection{Unlearning via Neural Activation Redirection}\label{sec:method_training}

Previous works \cite{panickssery2023steering, marks2023geometry} have shown that contrastive features can be delineated by computing the `steering vector': 
\( \mathbf{r} = \bar{\mathbf{a}}(x) - \bar{\mathbf{a}}(y)\) 
(i.e. the difference in mean residual stream activations $\bar{\mathbf{a}}$ between pairs of positive $x$ and negative $y$ examples of a particular behavior). These steering vectors have significant implications for influencing model behavior. For instance, a `steering vector' computed out of contrastive harmful versus harmless prompts can be added to the residual stream activations of harmful prompts to circumvent the model's safety guardrails \citep{single_direction}.

However, given the remarkable ability of transformer architectures to aggregate information and capture abstract representations through high-dimensional residual stream activations, particularly in intermediate layers \cite{grosse2023studying, dwivedi2020generalization}, we conjecture that it is not strictly necessary for two features to be explicitly contrastive in a human-comprehensible sense to compute and utilize `steering vectors'. Instead, those can be employed more generally to map a shared hidden feature underlying one group of prompts (i.e., the source feature abstracted by the transformer in intermediate layers) to another group of prompts (i.e., the target feature). We term this process `\emph{Activation Redirection}'. This mapping can effectively trigger the model to resemble the behavior associated with the target feature.

In the context of LLM unlearning, the objective is to create an unlearned model that closely mimics the behavior of a retrained model, which explicitly and lively communicates its inability to respond to prompts related to the forget set. To achieve this, we redirect the activations of forget set across all token positions to activations representing the state of inability as follows:

\vspace{-0.5cm}
\begin{equation} \label{eq:activation_addition}
\mathbf{a'}_{f}^{(l)}(x) \leftarrow \mathbf{a}_{f}^{(l)}(x) + \mathbf{r}_{\text{UV}}^{(l)}
\end{equation}
\vspace{-0.5cm}

where $\mathbf{r}_{\text{UV}}^{(l)}$, the \textit{unlearning vector (UV)} as a linear intervention in the residual stream activations, is defined as below:

%the difference in mean activations of \textit{any} forget set prompts $D_f$ of a base model and activations of a set of reference prompts, $D_{\text{ref}}$, which can trigger the model's inherent capability of expressing its inability to respond (such as due to harmfulness or ignorance). 

\vspace{-0.5cm}
\begin{equation} \label{eq:unlearning_vector}
\mathbf{r}_{\text{UV}}^{(l)} = \frac{1}{|D_{\text{ref}}|} \sum_{x \in D_{\text{ref}}} \mathbf{a}^{(l)}(x) - \frac{1}{|D_{f}|} \sum_{x \in D_{f}} \mathbf{a}^{(l)}(x)
\end{equation}
\vspace{-0.5cm}
%
%where ${\mathbf{a}}^{(l)}(x)$ is the residual stream activations post layer $l$, $D_{\text{ref}}$ is a set of reference prompts that can trigger model's inherent capability of expressing its inability to respond (such as due to harmfulness or ignorance), and $D_f$ is the prompts of the forget set.

In Eq.~\ref{eq:unlearning_vector}, $D_f$ is the forget set and $D_{\text{ref}}$ is a set of reference prompts associated with the target feature. In one instance, provided the base model is safety-aligned, $D_{\text{ref}}$ can be the prompts that activate the model’s internal safety mechanisms to state its inability to positively engage with the unlearned queries. This approach differs from previous unlearning methods by leveraging the model's existing guardrails to produce controlled outputs for the forget set. 
%
Alternatively, we observed that the latest LLMs are capable of stating `a lack of knowledge' to fictitious prompts (such as `What is the capital of the country \$7\&a\#!'). As such, $D_{\text{ref}}$ can be a set of fictitious prompts. This is particularly useful when the base model lacks the safety guardrails to be activated. 

As outlined before, the unlearning task necessitates a dual focus: ensuring effective forgetting performance while preserving the model's utility. In this sense, the training objective is two-fold. First, it ensures the activations of the forget set are redirected to the perturbed activation according to Eq.~\ref{eq:activation_addition}. Second, it restricts the retain set from moving away from the original activation space. Therefore, the loss can be defined below: %To balance these two objectives, we propose the following as \lunar's unlearning loss function:

\vspace{-0.5cm}
\begin{align} \label{eq:loss}
    \mathcal{L}_\text{\lunar} = \mathbb{E}_{D_f}|| \mathbf{a} - \mathbf{a'}_{f}^{(l)}(x) ||_2, \text{if } x \in D_f \notag \\
    \mathcal{L}_\text{\lunar} =  \mathbb{E}_{D_r}|| \mathbf{a} - \mathbf{a}_{r}^{(l)}(x) ||_2, \text{if } x \in D_r
\end{align}
\vspace{-0.5cm}

In light of the pivotal role of MLP layers in storing knowledge within transformer-based LLMs \cite{meng2022locating} — functioning as memory banks where values are stored and accessed by $W_{out}^l$ through subject tokens acting as keys — we focus on the down-projection of MLPs as the target for weight optimization in the unlearning process. We, therefore, set the residual stream activations in Eq.\ref{eq:activation_addition} and \ref{eq:unlearning_vector} as the MLP output and optimize the weights of the down-projection layer using Eq.~\ref{eq:loss} while keeping the rest frozen.


\subsection{Layer Selection}\label{sec:layer_selection}

In the transformer architecture, different residual blocks (i.e., layers) exhibit distinct generalization patterns (e.g., intermediate layers accumulate information and capture more abstract representations) \citep{grosse2023studying}. As a result, activation redirection is intuitively most effective when performed in the middle layers of the model. To identify the exact layer, two primary objectives are considered: (1) the model should most effectively state its inability to respond, and (2) the response should correctly convey its reason. 

To assess the first objective, prior work computes a binary refusal score by string-matching common `refusal substrings' (e.g., ``I’m sorry" or ``As an AI") \citep{robey2023smoothllm, lermen2023lora, liu2023autodan} or uses the probability of `I' as the first token as a proxy for refusal \citep{single_direction}. However, the substring-matching approach may fail to evaluate the lexical or semantic coherence of the responses \citep{huang2023catastrophic, meade2024universal, qi2023fine}, while we found the token-probability method can lead to gibberish-like responses of multiple `I's as the probability of `I' increases. Thus, we propose an alternative by computing the cosine similarity ($s_1$) between the sentence-level embeddings of the generated responses and desired responses (e.g., `I apologize that I don’t have access to this information that you are asking for').
%`I cannot provide you with the information' or

Additionally, to ensure the responses correctly convey the intended reason, we simultaneously minimize the cosine similarity ($s_2$) between the embeddings of the response and reasons unrelated to the unlearning task (e.g., harmfulness, danger, or unethicality). Overall, we select the layer that maximizes $(s_1 - s_2)$, thereby ensuring effective convey of the unlearned model's inability to respond in a coherent and contextually appropriate manner.





\subsection{Memory and Computational Costs} \label{sec:method_computational_costs}
The cost of unlearning methods is critical for determining their adoption.
% \Cref{tab:computation} shows the costs comparison between \lunar, in both its optimizer-based and analytical executions and baseline methods adopting LoRA \citep{lermen2023lora}.
Unlike previous proposals that require re-training the full model or a reduced representation while keeping the full model frozen, e.g., LoRA-based methods, \lunar only requires training a single down-projection layer.
As such, \lunar's memory footprint is represented by the frozen full model during procedures 1 and 2 (see \Cref{algo:1}) and a single dense layer during procedure 3.
This extreme reduction of the trainable parameters goes beyond a lower impact on the memory, resulting in significant computational efficiency.
In practice, reducing the memory footprint allows for the use of more data examples per step, which results in higher throughput \citep{mao2024surveylora}.

The number of trainable parameters for a LoRA-based method are $N_{\textit{LoRA}} = 2 \cdot L \cdot (d_{\textit{model}} \cdot r + r \cdot d_{\textit{ff}})$, where $L$ is the number layers, $d_{model}$ is the input dimension of a feed-forward network, $d_{ff}$ is the hidden dimension of the feed-forward network, and $r$ is the low rank of LoRA.
For \lunar applied on $K$ layers using LoRA modules, the number of trainable parameters are $N_{\text{\lunar}} = K \cdot (d_{\textit{model}} \cdot r + r \cdot d_{\textit{ff}}) = \frac{K}{2L} \cdot N_{\textit{LoRA}}$.
Since $K \leq \frac{L}{2}$ or, in most cases, $K = 1$, $N_{\text{\lunar}} < N_{\textit{LoRA}}$.
% For \lunar applied on $K$ layers using full modules, the number of trainable parameters are $N_{\text{\lunar}} = K \cdot d_{\textit{model}} \cdot d_{\textit{ff}}$.

As in previous works \citep{kaplanscalinglaws} assuming standard optimization conditions, the computational cost per token (FLOPs/token) $C$ for training an LLM is estimated as $C \approx 6N$, where $N$ is the total number of trainable (non-embedding) parameters.
Fully frozen blocks in a transformer, like those used in LoRA, execute only the forward pass contributing for $C_{\textit{fwd}} \approx 2N$ FLOPs per token.
The LoRA modules execute both forward and backward passes for the total cost of $C_{\textit{LoRA}} = 6N_{\textit{LoRA}}$.
\lunar during the first two procedures (see \Cref{algo:1}) executes a complete forward pass on the full frozen model for each of the $K$ layers sampled at the cost of $C_{\textit{\lunar}|1,2} = 2NK$ FLOPs per token.
For training the $K$ down-projection layers (using their LoRA modules) during the third step of \lunar (see \Cref{algo:1}), the FLOPs per token can be estimated as $C_{\textit{\lunar}|3} = 6 \cdot K \cdot (d_{\textit{model}} \cdot r + r \cdot d_{\textit{ff}})$.
% For training the $K$ down-projection layers (using the full modules) during the third step of \lunar (see \Cref{algo:1}), the FLOPs per token can be estimated as $C_{\textit{\lunar}|3} = 6 \cdot K \cdot d_{\textit{model}} \cdot d_{\textit{ff}}$.\



%%% I am commenting down here with the real numbers for LLaMa2-7B
% L=32, K=L/2=16, d_{model}=4096, d_{ff} = 2d_{model} = 8192, r = 8, N=7B
% --> N_{LoRA} = 6.3M
% --> N_{LUNAR,full} = 33.5M \times K
% --> N_{LUNAR,LoRA} = 0.0984375M \ times K

%


% Form lunar, divide the discussion in the three procedures
% proc 1 is just a forward pass for each sample for each middle layer (usually up to ten) to get the activation vectors before and after the down-projection MLP component in the transformer block
% proc 2: correct the activations of the last token of each sample with the steering vector, execute the subsequent forward pass to compute the final activation, which are compared against a reference for each middle layer. layer with the highest similarity is selected
%1+2 ---> 2 * N * K, k is the number of layers
% proc 3: keep only the selected layer's activations and the final activations and then optimize a single layer MLP for minimizing a distance function for which we are given the targets
% 3 ---> 6 * d_{model} * d_{ff}
% N_{LoRA} = 2 * L * ((d_{model} * r) + (r * d_{ff}))
% N_{LUNAR} = K * d_{model} * d_{ff}
% FLOPs_{LoRA} = 2 * N + 6 * 2 * L * ((d_{model} * r) + (r * d_{ff}))
% FLOPs_{LUNAR} = 2 * N * K + K * 6 * d_{model} * d_{ff}
% K <= L / 2; r \in {4,8} is the compression rate for LoRA, LLaMa-7B uses r=8


%(1) LoRA
% (1.1) gradient ascent: only optimized over the forget set
% (1.2) other method: optimized over forget set + retain set
%(2) our method, which only trains the trainsformer block, then we only have 1 mlp block with input size p and output size q; optimized over both forget set and retain set
%(3) our analytical solution, which is the matrix multiplication, with H of size (nxp), and A with size (nxq), with n the size of (forget set + retain set)
%

% \input{section/table_and_pic/computation}


\section{Analytical Solution and Convergence Study}
In transformer architectures, the down-projection layer functions as a fully connected layer without activation functions. By framing the optimization objective for this layer with \(\mathcal{L}_\text{\lunar}\), a closed-form solution can be derived analytically, implying its convergence. 
%MSE loss of a linear system is convex and has Lipschitz continuous gradients, SGD with a constant learning rate converges in expectation to the optimal solution with a rate of O(1/T).
%\xinchi{also want to mention our solution can converge so dont need a convergence analysis}

Let \( n \) and \( m \) denote the number of tokens in the forget set and the retained set, respectively. The input dimension of the selected down-projection layer is represented by \( p \), while \( q \) be the output dimension. Hidden states before the down-projection layer are therefore \( H_f = [h_{1,f}^T, h_{2,f}^T, ..., h_{n,f}^T] \in \mathbb{R}^{n \times p}\) for the forget set and \( H_r = [h_{1,r}^T, h_{2,r}^T, ..., h_{m,r}^T] \in \mathbb{R}^{m \times p}\) for the retained set, where \( h_{i,f}^T \) and \( h_{i,r}^T \) are p-dimensional vectors representing each token in the forget and retained set respectively. Let the original MLP output activations be \( A_f^{\text{origin}} = [a_{1,f}^T, a_{2,f}^T, ..., a_{n,f}^T] \in \mathbb{R}^{n \times q} \) and \( A_r^{\text{origin}} = [a_{1,r}^T, a_{2,r}^T, ..., a_{m,r}^T] \in \mathbb{R}^{m \times q} \). \lunar introduces a redirection in the activation space for the forget set, resulting in \( A_f = [a_{1,f}^T + r_{UV}^T, a_{2,f}^T+ r_{UV}^T, ..., a_{n,f}^T+ r_{UV}^T] \), while the activations for the retained set remain unchanged, i.e., \( A_r = [a_{1,r}^T, a_{2,r}^T, ..., a_{m,r}^T]  \).


% and \( A = [A_f, A_r] \in \mathbb{R}^{(n+m) \times q} \).

%Let \( H \in \mathbb{R}^{n \times p} \) represent the input matrix for linear regression, where \( n \) denotes the number of samples, comprising both the forget set and the retained set, and \( p \) is the input dimension of the MLP (Multi-Layer Perceptron) layer. In this context, \( H \) consists of \( n \) rows, each representing a hidden state prior to activation. 
%Then, consider \( A \in \mathbb{R}^{n \times q} \), where \( q \) is the output dimension of the MLP layer. Here, each column of \( A \) corresponds to the perturbed activations if the input belongs to the forget set, and the original activations if the input is part of the retained set. Let \( W \in \mathbb{R}^{p \times q} \) denote the weight parameters of the MLP layer. 

The objective is to optimize the weights of down-projection layer $W^{l}_{out}$ to minimize the distance between the redirected MLP output and the original output, as follows:

\vspace{-0.4cm}
\begin{equation} \label{eq:optimization_problem1}
    \widehat{W} = \arg \min_{W} ||[H_f, H_r] W - [A_f,A_r]||_{2}
\end{equation}
\vspace{-0.5cm}

%This is a linear least square optimization problem. Hence, the closed-form solution for \( W \) can directly be given by:
One can show that there exists a unique solution in the following form: (Proofs of the closed-form solution \ref{app:closed_form_solution_proof} and the associated Lemma \ref{app:lemma_1} provided in Appendix \ref{app:proofs}):

\vspace{-0.6cm}
\begin{equation} \label{eq:analytical}
    \widehat{W} = ([H_f, H_r]^\top [H_f, H_r] + \lambda I)^{-1}[H_f, H_r]^\top [A_f, A_r]
\end{equation}
\vspace{-0.4cm}

The computational cost for Eq. (\ref{eq:analytical}) is mainly dominated by the matrix inverse computation and normally has the cost of \( O(p^3) \), making SGD-based optimization more practical in real deployment. However, the existence of a closed-form solution guarantees the convergence of \lunar.



%As all baseline methods involves optimization steps across all model parameters and we chose to use LoRA \cite{lermen2023lora} to improve the efficiency. Table \ref{tab:computation} compares the computational efficiency comparing our methods to the baselines. It is obvious that because our method only update the MLP layers up to certain number of blocks, the effciency is significant. \xinchi{analytical solution should be more efficient because it doesnt need to do multiple epoch, need to check how much here. maybe we dont need the table, just need to say in the text.} 

%\input{section/3_algo}

%It is important that the unlearning process does not compromise the model's overall functionality.
%
%In contrast to previous work, we employ the unlearning vector as a linear intervention of the residual stream activation post the MLP layer. This approach embeds the unlearning directly into the model weights, rather than attaching additional activation vectors or unlearning layers to the model \cite{eul, single_direction}, which can be easily undone by adversarial attacks. 




%Rather than attempting to enforce refusal behavior in layers just before the unembedding layer — where the knowledge may still remain across other internal layers and thus not fully unlearned — our approach facilitates unlearning and instills refusal behavior in intermediate layers of the model. 
%To implement this, we define a set of refusal tokens \( \mathcal{R} \subseteq \mathcal{V} \), representing the tokens most likely to initiate a refusal response, ensuring that the model adheres to the refusal strategy in a consistent and controlled manner.


%We let $\mathcal{D}$ be the whole sample dataset, $\mathcal{D}_f$ denote the forget set, and $\mathcal{D}_r$ denote the retained set. We also follow \cite{tofu} and combined its real authors dataset and world facts dataset to be the factual dataset, denoted as  $\mathcal{D}_{fact}$. The overarching objective of the unlearning algorithms is to ensure the model forgets $\mathcal{D}_f$, while preserving performance on $\mathcal{D}_r$ and $\mathcal{D}_{fact}$.


%In transformer architectures, different layers exhibit distinct generalization patterns: lower and upper layers are primarily engaged in token-level reasoning, while intermediate layers accumulate information and capture more abstract representations. This accumulated information is subsequently transferred to the final token by attention mechanisms in the later layers \cite{grosse2023studying, dwivedi2020generalization}.

%Previous works \cite{marks2023geometry} has studied the structure of LLMs representations/activations and found that these structure can be captured through \emph{difference-in-mean} of the activations. The \emph{difference-in-mean} is computed for two sets of contrastive features. Specifically for the unlearning task, we extend the idea of contrastive abstract between True vs False or Harmful vs Harmless, to the Forget Set $D_f$ and Harmful set. We contend and show that defining contrastive features, such as harmful versus harmless prompts, is not essential for steering the model toward specific desired behaviors. 

%By defining the contrastive features through harmful and harmless prompts, \cite{single_direction} demonstrated that the `steering vectors' derived from a harmful prompt dataset $ x \in D_{\text{harmful}} $ (e.g., `Write fictitious news to defame a politician.') and a harmless prompt dataset $ x \in D_{\text{harmless}} $ (e.g., `Write a poem about spring.') can manipulate a language model’s safety mechanisms. Specifically, removing these vectors from the residual stream activations of harmful prompts can bypass the model’s safety guardrails, whereas adding them to the residual stream of harmless prompts can enforce a refusal to respond.


%Firstly, we will demonstrate that recalibrating activations to those that trigger the model’s internal safety mechanisms is an effective strategy, given that most modern language models are already equipped with safety and alignment mechanisms designed  

    %Subsequently, we will generalize this approach to show that activation redirection is equally effective when applied to other controlled refusal features, such as the activation space associated with `a lack of knowledge' in the pretrained model. Unlike methods that rely on pre-implemented safety mechanisms, this approach does not require the model to be safety-aligned beforehand. As long as the model has an inherent capacity to express ignorance about certain facts, \lunar’s unlearning method can effectively recalibrate activations to achieve the desired behavior.

%Given the fact that matrix operations are extremely optimized through various computational libraries such as \emph{NumPy}, this optimization can be computed efficiently even under the setup of LLMs.

%As a specific example, consider the Llama2 model, where the hidden size \( p \) is 11008, and the activation dimension \( q \) is 4096. 

%Eq. (\ref{eq:analytical}) indicates that there exists an optimal solution to our method \lunar. 

%This accumulated information is subsequently transferred to the final token by attention mechanisms in the later layers \citep{dwivedi2020generalization}. 


%For assessing the effectiveness of refusals, prior work has compiled a list of common `refusal substrings' (e.g., `I’m sorry' or `As an AI') \citep{robey2023smoothllm, lermen2023lora, liu2023autodan}, and applied string-matching techniques to compute a binary refusal score. \citet{single_direction} complemented this and used the probability of `I' as the first token as a proxy for refusal, as many refusal phrases begin with `I'. However, these methods have limitations because they fail to assess whether the generated responses are lexically or semantically coherent \citep{huang2023catastrophic, meade2024universal, qi2023fine}. We further found that optimizing certain layers with activation redirection can indeed increase the likelihood of `I' appearing as the first token, but this often leads to incoherent responses resembling gibberish — an undesirable outcome for controlled response generation post-unlearning. To address this, we alternatively propose to compare and maximize the cosine similarity $s_1$ between the sentence-level embeddings of responses generated after applying using the UV (Eq. \ref{eq:activation_addition}) with the embeddings of characteristic refusal responses, such as `I cannot provide you with the information' or `I apologize that I don’t have access to this information.' Meanwhile, we let $s_2$ represent the cosine similarity relative to a set of potential reasons behind refusals that are unrelated to the unlearning task (e.g., harmfulness, danger, or unethicality). We aim to select the top-k layers that maximize $(s_1 - s_2)$, thereby ensuring both effective refusal and coherent, contextually appropriate outputs. Details of the string sets used for comparison can be found in \Cref{app:layer_selection}.


%\subsubsection{The selection of Top-K} \label{sec:top_k_layer_selection}
%Previous research has demonstrated that LLMs can generate outputs resembling their final predictions at earlier layers, a technique referred to as \textit{early exiting} \citep{elhoushi-etal-2024-layerskip, chen2020low}. This capability allows models to remain effective even when intermediate computations are bypassed, supporting the concept of layer skipping \citep{Fan2020Reducing, fan2024not}. Such efficiency, along with observed redundancy across layers, may stem from the recursive nature of residual computations, which effectively perform an exponential ensembling of sub-networks \citep{NIPS2016_37bc2f75, chen2024jetexpansionsresidualcomputation}.

%While this issue is less critical when the party performing unlearning also owns the closed-source LLM (as they can simply perform activation redirection on the most effective layer identified in \ref{sec:layer_selection}), it becomes crucial to address scenarios involving adversarial attacks. In particular, we must consider larger values of K to mitigate the risk of adversaries exploiting the early exiting feature by bypassing modified layers in a white-box attack scenario. White-box attacks are the most potent, assuming that attackers have full knowledge of the model, including its weights, the selected layers for unlearning, and the unlearning vectors.

%To counter this, the layer selection process dynamically enforces that activation redirection is applied to a minimum of K layers. This ensures that bypassing all the modified layers would still reduce the model’s ability to recall the forget data below an acceptable threshold. An in-depth analysis of the top-K selection process and its effectiveness in defending against layer skipping attacks is provided in Appendix \ref{app:attack_res}.

%When the dataset size is small and the number of features is manageable, using Eq. (\ref{eq:analytical}) is typically more efficient than optimizing \( \mathcal{L}_\text{\lunar} \) using SGD because it provides a direct solution. Choosing between a SGD-based optimization or closed-form solution can be highly dependent on the specific model and forget and retained token size being optimized in each case. 

