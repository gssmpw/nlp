\vspace{-0.7cm}
\section{Introduction}\label{sec:intro}

\input{section/table_and_pic/1_fig}

Large Language Models (LLMs) exhibit exceptional natural language generation capabilities, supporting diverse applications %from goal-oriented dialogues to general writing assistance 
\cite{gpt, gpt4} and often achieving near-human quality \cite{abdali2024decoding}.
%
However, their outputs are not always appropriate, as they may include personally identifiable information (PII), exhibit bias or toxicity, or contain inaccurate or outdated information \cite{kotek2023gender, motoki2023more, bender2021dangers, wen2023unveiling, nasr2023scalable, barrett2023identifying}. 
%
Furthermore, there is a growing need for LLMs to be tailored to local or private datasets while complying with regulations such as the General Data Protection Regulation \cite{gdpr17}, which enforces the `right to be forgotten'. 
%
Addressing these challenges through extensive data management or model retraining is impractical due to the enormous size of training datasets and the substantial computational costs involved \cite{llm_cost}. Consequently, it is increasingly critical to develop efficient LLM unlearning methods to address privacy, security, and commercial concerns effectively \cite{cc100, wenzek2019ccnet}.

Existing unlearning methods often claim success by producing outputs that deviate from the ground truth for the data designated for forgetting while maintaining accuracy on the remaining data \citep{liu2024rethinking}. 
%
However, they do not always achieve a desirable balance of these two objectives, as evidenced by insufficient unlearning or significant degradation in the model utility \citep{qiu2024pistol}. 

Moreover, while existing unlearning methods focus on balancing unlearning efficacy and retained model utility, they often neglect other undesirable side effects \citep{yao2023large, wang2024unlearning, liu2024rethinking, blanco2025digital}, including hallucinations, 
%(i.e., producing plausible but inaccurate responses that could mislead users) \cite{farquhar2024detecting}
rigid and monotonous responses (as opposed to dynamic and contextually aware ones), 
%(i.e., replying by static and formulaic response of `I don't know')
or the generation of nonsensical outputs 
%(i.e., gibberish) 
when being prompted with unlearned data (Figure \ref{fig:image1}(a)). We term this problem a lack of \textit{controllability}. We \textbf{define controllability as the unlearned model's
ability to explicitly convey its inability to respond} while ensuring that the generated
text remains dynamic, contextually aware, and coherent. We advocate for incorporating controllability as
a key criterion in the evaluation of LLM unlearning methods in future studies.

%
%Therefore, we argue that a more desirable unlearning approach should prevent the model from generating the sensitive information targeted for removal and explicitly communicate its inability to answer related queries coherently. 
%
Additionally, widely adopted unlearning methods, whether gradient-ascent-based \citep{jang2022knowledge, yao2023large, liu2022continual} or preference-optimization-based \citep{rafailov2024direct, zhang2024negative}, are associated with high computational costs (Sec.\ref{sec:method_computational_costs}), particularly as LLMs scale up.
%These methods are also constrained by the substantial effort required for hyperparameter tuning and their sensitivity to several factors, such as the dataset being unlearned, the batch size during the unlearning process, and, critically, the specific pre-trained model employed \cite{qiu2024pistol}. 
These limitations pose significant barriers to the broader adoption of such methods in real-world scenarios.


To address the limitations of current LLM unlearning methods, we propose a novel method \lunar. It leverages recent insights from mechanistic interpretability and representation engineering \citep{zou2023representation}, which show important observable behaviors are associated with linear subspaces of the representations internally created by models. In particular, \lunar optimizes selected MLP down-projections to alter the model so that the conceptual representation of data points to be unlearned are in the regions that trigger the model’s inherent ability to express its inability to answer. In summary, our contributions are:
\begin{itemize}
    \vspace{-2mm}
    \item We introduce \lunar, a novel unlearning method via activation redirection technique that achieves SOTA performance in unlearning \textit{effectiveness} and \textit{controllability}. We also provide a closed-form solution that implies the convergence of \lunar. %
    \vspace{-2mm}
    \item We further show through extensive experiments that \lunar is \textit{versatile} in real-world applications, including unlearning both pre-trained and fine-tuned data, as well as handling sequential unlearning tasks.
    \vspace{-2mm}
    %
    \item We demonstrate \lunar's robustness against adversarial attacks, safeguarding the unlearned model against exploitation. Notably, we show \lunar is unaffected by quantization, irrespective of whether this is carried out to enhance efficiency or as an adversarial attack.
    \vspace{-2mm}
    %
    \item We show that \lunar is inherently both memory and computationally efficient. Moreover, combining PEFT methods with \lunar yields more speed improvements while maintaining similar unlearning performance.
    %\item We show that \lunar is an especially computationally-efficient unlearning method, 
    %achieving SOTA \textbf{efficiency} by a \add{xx speed up} compared to the baselines. 
\end{itemize}


\begin{table}[ht]
    \setlength{\abovecaptionskip}{10pt} % Reduce space above the caption
    \setlength{\belowcaptionskip}{-10pt} % Reduce space below the caption
    \captionsetup{font=small,labelfont=bf}
    \centering    \input{section/table_and_pic/4_pistol_examples}
    \caption{provides examples of responses generated after applying \lunar and baseline methods (refer to Sec.\ref{sec:exp_setup}) on Llama2-7B finetuned on the PISTOL dataset. \lunar exhibits superior \textbf{controllability} as it generates \sethlcolor{lightgreen}\hl{coherent and contextually aware responses} that accurately convey the model’s inability to respond, contrasting with baseline methods, which often exhibit common unlearning side effects such as \sethlcolor{pink}\hl{hallucinations} and \sethlcolor{lightyellow}\hl{incoherence}.}
    \label{tab:pistol_examples}
\end{table}

%In this work, we We further detail the optimization of down-projection MLPs, including the derivation of a closed-form optimal solution that is computationally efficient for managing relatively small datasets and model feature dimensions. Through extensive theoretical analysis and experiments, we demonstrate that \lunar not only achieves state-of-the-art performance on the PISTOL \cite{qiu2024pistol} and Tofu \cite{tofu} datasets, but also resolves the controllability challenges of existing unlearning methods while significantly reducing computational overhead.




%
%(1) \textbf{Effectiveness}: the method should achieve superior performance in forgetting targeted knowledge while preserving the model’s utility across both the retained dataset and broader general tasks; 
%
%(2) \textbf{Controllability}: the method should enable the model to produce more predictable and interpretable responses post unlearning, avoiding the generation of unverifiable or random outputs that could mislead users; 
%
%(3) \textbf{Efficiency}: the approach should be more resource-efficient compared to existing baselines, with reduced hardware and computational requirements as well as shorter wall-clock training times, making it accessible for users with constrained resources; 
%
%(4) \textbf{Safety}: the unlearning methodology should be resilient to adversarial attacks, ensuring the integrity of the unlearning process and safeguarding the unlearned model against exploitation.
%
%\textbf{Robustness}: the proposed method should demonstrate robustness across varying configurations, minimizing the dependence on meticulous hyper-parameter optimization and improving adaptability for real-world unlearning deployments.



%In summary, this paper presents the following contributions: 1) We propose a novel robust, controllable, and efficient unlearning method termed \lunar. 2) We conduct extensive experiments using four widely adopted pre-trained models: Llama 2 \cite{llama}, Llama 3 \cite{llama3}, Qwen 2 \cite{bai2023qwen}, and Gemma \cite{team2024gemma}, across two of the most representative LLM unlearning datasets, PISTOL \cite{qiu2024pistol} and Tofu \cite{tofu}. 3) We demonstrate that our method achieves state-of-the-art (SOTA) performance. 4) We show that our method achieves a \add{xx speed up} compared to the baselines. 5) Furthermore, we design two strong white-box attacks to demonstrate the robustness of our method.

%Large language models~(LLMs) have shown impressive capabilities in natural language generation, aiding in diverse applications from goal-oriented dialogues to general writing assistance \cite{gpt, gpt4}, and often achieving human-like quality ~\cite{abdali2024decoding}. However, their generation is not always appropriate for all audiences, due to issues such as generating biased or toxic content, memorizing personally identifiable information~(PII), and producing inaccurate or outdated information \cite{kotek2023gender, motoki2023more, bender2021dangers, wen2023unveiling, nasr2023scalable, barrett2023identifying}. Additionally, LLMs are trained on vast web-based datasets comprising trillions of tokens, which complicates data management and updates due to the impracticality and high costs of re-training from scratch when data needs to be modified or removed for privacy, security, or commercial reasons \cite{cc100, wenzek2019ccnet, mistral}. Addressing these challenges is crucial for the safe deployment of LLMs, ensuring they meet the diverse needs of different user groups and sectors.
% \cite{valadi2023fedval, qiu2023exact,zhao2024breaking}.


%Existing unlearning methods claim successful unlearning by producing outputs that deviate from the ground truth. These methods require meticulous tuning of hyperparameters, particularly the learning rate, to balance effective unlearning (reduced accuracy on forget set prompts) with preserved model utility (higher accuracy on remain set prompts). Identifying optimal hyperparameters for this balance is not only time-consuming but also highly dependent on the specific dataset, batch size during the forgetting process, and, crucially, the chosen pre-trained model \cite{qiu2024pistol}. Typically, to maintain model utility, the perturbation applied in unlearning is modest, resulting in coherent yet unfactual responses to forget set prompts rather than completely nonsensical gibberish. This poses a significant risk in deployment, as users may interpret these coherent but unfactual responses as reliable information. We argue that a superior unlearning approach should not only prevent the model from providing factual answers but also explicitly communicate its lack of knowledge on the requested information.

%previous research has shown that balancing these two objectives requires extensive hyperparameter tuning for popular methods — a process highly sensitive to the dataset being unlearned, the batch size used during the unlearning process, and, critically, the specific pre-trained model involved \cite{qiu2024pistol}. 
%
%Moreover, to prevent a collapse in the utility of the retained model, the perturbations applied during unlearning are typically modest. This can result in responses to prompts from the forget set that, while coherent, are not factual. This poses significant risks in deployment, as users may misinterpret such 'hallucinated' responses as reliable information.


%Achieving effective unlearning requires not only that the model be incapable of recalling the specific sensitive information targeted for removal, but also that it responds with explicit refusals to answer such queries. This strategy mitigates the common side effects of unlearning, where models, in the absence of the forgotten information, may generate random or inaccurate responses — phenomena often referred to as hallucinations \cite{farquhar2024detecting}. 


%This heavy dependence on hardware resources makes these methods impractical for deployment by end-users with limited computational capabilities, posing significant challenges for real-world applications.

    %For direction, we introduce 'Unlearning Vectors (UV)' — a generalized form of the "steering vector" \citep{panickssery2023steering, marks2023geometry, arditi2024refusal} — which does not necessarily rely on explicitly contrastive features in a human-comprehensible sense. For target, we propose two activation recalibration targets responsible for expressing a model's inability to respond. The first leverages the model's existing safety mechanisms to generate controlled outputs for prompts from the forget set, while the second adopts a more generalized approach, utilizing an LLM's inherent capacity to express uncertainty or ignorance about specific facts. 