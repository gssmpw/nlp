In an effort to understand the inner mechanism of DNNs, several branches of research have emerged:

\textbf{Concept based.} \citet{CAV} show that a model can be rigorously understood by assigning meaning to the activations, referred to as concept activation vectors. Subsequent works \citep{ACE, NMF} have explored more complex methods for extracting these meanings, however, the relationships between concepts remain understudied. \citet{CRAFT, VCC} address this limitation by constructing a graph of concepts with edges that quantify the relations. Their main intention is to see the evolution of concepts throughout the network layers. Nevertheless, they are unable to explain which parts of the model are responsible for these concepts.

% \textbf{Neuron and circuit based.} \citet{Multifaceted, Polysemantic, Olah, mu2020compositional} invest effort in studying the meaning of neurons. Showing that it also has meaning and analyzing the neurons sheds light on how the model encodes features. However, one limitation is that only one neuron is analyzed at a time. This could be infeasible for large networks, and would not give a comprehensive explanation of the inner mechanism. 
% % Recent papers \citep{GPT2circuit} handcrafted a subgraph in the computational graph (circuit) of GPT2 \citep{Gpt2} that is responsible for a specific task. To address the drawback of constructing manually, \citet{ACDC} further shows that they can automate the process by finding the important attention heads and pruning out unimportant weights. Still, the meanings and relations of the components are found by hand. 
% A recent paper \citep{Invert} proposes a way to label the concept for all neurons in the classifier layer. In turn, a circuit is formed to show the interconnection and the relations between them. But this circuit is constructed manually.

\textbf{Neuron based.} \citet{Multifaceted, mu2020compositional, Polysemantic, Invert} invest effort in studying the meaning of neurons, in parallel, \citet{NEUCEPT, neuron_shapley, critical_pathway} propose different approaches in identifying important neurons to the model output. These researches shed light on the function of individual neurons and their impact on the prediction of the model. Recently, \citep{Olah, Invert, concept_relevance_propagation} connect the neurons to form circuits that explain the behavior of a model throughout the layers, nevertheless, the circuits are constructed manually. Furthermore, a major limitation of all previous works is that they only analyze one neuron at a time. This approach is prone to the complex nature of neuron, namely polysemantic neurons, where neurons may encode multiple distinct features, making model interpretation via neurons challenging \citep{Olah, Polysemantic}. Lastly, \citet{hint, falcon} find the group of neurons that encode the same concept, however, the relations among the groups and the influence of a group on the model's outputs are left unexplored.

\textbf{Graph based.} \citet{input_output_DNNs, input_output_generizable} try to approximate the mechanism of a model by considering the causal relations between the inputs and outputs. 
%Yet, the inner representation is left unexplored. 
Another notable method \citep{InterpretCNN} generates a graph that highlights what visual features activate a feature map, for multiple layers. While this approach can be modified to form a circuit, the graph lacks meaningful edge weights. Consequently, it cannot quantify the contribution of each CNNs component to others and to the final prediction, unable to explain the inner mechanism (They use and-or-graph \citep{And_or_graph} to form relations between components. However, unlike circuit, this new graph disregards the original structure of the model, where ``concepts" of the first convolution could interact directly with ``concepts" of the last convolution.). Subsequent work \citep{DecisiontreeCNN} fixes this issue by building a decision tree to quantify the contribution of each feature map to the final predictions. 
%Yet, it is only built on the last convolutional layer and requires additional architecture modification.

Our work aligns the most with explaining neurons and forming circuits. We address the common limitations of manual neuron labeling and circuit construction. We also propose a way to look at neurons not individually but in groups 
%, making it feasible to explain large number of neurons.
to overcome the common problem of polysemantic neurons. Additionally, we prioritize exploring the interactions between neuron groups across layers rather than focusing solely on the relationship between individual neurons and the model's output. Table \ref{tab: related work} in Appendix \ref{sec:related_work_summarization} provides a comparison of our method with the most relevant existing studies.
% We compare our method with most relevant existing works in Table \ref{tab: related work}. \mt{Was this sentence here before? Pls refer to Appendix C as I don't see Table 2 in the main paper}