We outlines some applications of NeurFlow.
We hypothesize that, as one neuron can have multiple meanings, a DNN looks at a group of neurons rather than individually to determine the exact features of the input. Hence, we propose a metric that assesses a model's confidence in determining whether the input contains a specific visual feature. For a group $G$ with core concept neurons $\sS_G = \{s_{G,1}, \dots, s_{G, |\sS_G|}\}$, the metric denoted as $ M(v, \sS_G, \mathcal{D}) = \exp(\frac{1}{|\sS_G|}\sum_{s \in \sS_G}\log(\|\phi_s(v)/\max(\phi_s, \mathcal{D}))\|)$, where $v \in \mathcal{D}$ and $\max(\phi_s, \mathcal{D})$ is the highest value of activation of neuron $s \in \sS_G$ on dataset $\mathcal{D}$.
This returns high score when all neurons in $G$ have high activation (indicating high confidence), while resulting in almost zero if any neuron in the group has low activation (indicating low confidence). We can use this metric to determine how similar the features in the input image are to the predetermined neuron groups concept. 
The specific setup can be found in the Appendix \ref{debugging_setup}. Figures \ref{fig:img debug} and \ref{fig:debias} demonstrate the usage of the metric and the concept circuit.
We use the term \emph{NGC} to denote the concept of a neuron group. 
\begin{figure}[t]
    \centering
    \begin{minipage}{.45\textwidth}
       \vspace{-12mm}
        \includegraphics[width=0.95\columnwidth]{figures/image_debug.png}
     %   \vspace{-3mm}
        \caption{\textbf{Using NeurFlow to reveal the reason behind model's prediction.} The top concepts can be traced throughout the circuit.}\label{fig:img debug}   
        \vspace{-10pt}
    \end{minipage}
    \hfill
    \begin{minipage}{.5\textwidth}
        \vspace{-12mm}
        \includegraphics[width=0.95\columnwidth]{figures/MLLM_captioning.pdf}
    %    \vspace{-5mm}
        \caption{Demonstration for automatically labelling and explaining the relation of NGCs on class ``great white shark" using GPT4-o \citep{Gpt4o}. The captions and the names of the NGCs are highlighted in blue, while the relations are in black.} \label{fig:captioning}
        \vspace{-10pt}
    \end{minipage}
    \vspace{-5pt}
\end{figure}
% Note that, a recent work \citep{VCC}, a concept-based method that also explains the inner mechanism of DNNs, has a similar application of debugging images. They measure how close the activations of misclassified images are to the concept vectors using $l_2$ norm. However, they provide no objective proof. In contrast, one advantage of learning via neurons is that we can edit the neurons related to a concept to see whether it has significant impact on the final predictions (see section \ref{debugging}).

\subsection{Image debugging}
\label{debugging}
We aim to use the concept circuit to identify concepts contributing to false prediction, which we call \textit{image debugging}. If a concept contributes to a class when it should not, we say that the prediction (or equivalently, the model) is \textit{biased} by that concept. \citet{Debias} propose a framework for detecting biases in a vision model by generating captions for the predicted images and tracking the common keywords found in the captions. With this method, they concluded that the pretrained ResNet50 is biased by ``flower pedals" in the class ``bee". However, correlational features do not imply causation and can lead to misjudgments. We verify and enhance the causality of their claim by examining the concept circuit of class ``bee", and conducting experiments on the probabilities of the final predictions with and without neurons that related to ``flowers". Additionally, we discover that the model also suffers from ``green background" bias (resemble ``leaves"), which is not mentioned in \citet{Debias}. 

Figure \ref{fig:debias} shows the process of debugging false positive images. Three different concepts are presented in \textit{layer4.2} of ResNet50, representing ``pink pedals", ``green background", and ``bee" respectively (we choose this layer as it has a small set of NGCs, however, our following experiment is consistent for multiple layers and with different classes). We discover that most of the false positive images have high metric score for ``pedal" and ``green background". 
To further verify the impact of these biased features, we mask all neurons in the groups of the respective concepts and find that the probability of the predictions are distorted drastically (and predictions is no longer ``bee"), as opposed to masking random neurons, which yield negligible changes. 

\begin{figure}
\begin{center}
\vspace{-12mm}
\includegraphics[width=0.9\textwidth]{figures/bias.pdf} 
\end{center}
\vspace{-3mm}
\caption{(left) The metric scores of false positive images for each concept in \textit{layer4.2} of ResNet50. (right) Showing the images that have the greatest drop in the activation of the logit neuron when masking each group concept. Verifying that the neuron groups indeed reflect the concepts.} \label{fig:debias}
\vspace{-15pt}
\end{figure}
This implies the dependence on the biased concept. \textit{But how do we know that the groups reflect the respective visual features?} If these groups indeed represent the visual features, then masking them should hinder the classification probability for images that include those features. We highlight the top images that have the largest decrease in the value of the logit neuron (corresponding to class ``bee") on both validation set of the target class and augmented dataset (see Section \ref{subsec:identify_node}). As shown in Figure \ref{fig:debias}, this process indeed yields the images that contain the respective features.

To demonstrate how NeurFlow's findings differ from those of existing methods, we conduct a qualitative experiment comparing the core concept neurons identified by NeurFlow with those identified by NeuCEPT \citet{NEUCEPT}. Detailed information about this experiment is provided in Appendix \ref{sec:compare_neucept_img_debugging}. Our observations indicate that 
% NeurFlow identifies concepts more closely resembling the original images. Additionally, 
the top logit drop images identified by NeurFlow align better with the representative examples of the corresponding concepts. Moreover, masking the core concept neuron groups identified by NeurFlow resulted in more significant changes to prediction probabilities while utilizing fewer neurons compared to the groups identified by NeuCEPT.


\subsection{Automatic identification of layer-by-layer relations}
\label{labeling}
\vspace{-5pt}
While automatically discovering concepts from inner representation has been a prominent field of research \citep{CRAFT}, automatically explaining the resulting concepts is often ignored, relying on manual annotations. \citet{Invert} utilize label description in ImageNet dataset to generate caption for neurons, however, these annotations is limited and can not be used to label low level concepts. Drawing inspiration from \citet{hoang2024llm, falcon}, we go one step further and not only use MLLM to label the (group of) neurons but also explain the relations between them in consecutive layers. Thus, we show the prospect of completing the whole picture of abstracting and explaining the inner representation in a systematic manner. 

Specifically, for two consecutive layers, we ask MLLM to describe the common visual features in a NGC, then matching with those of the top NGC (with the highest weights) at the preceding layer. This can be done iteratively throughout the concept circuit, generating a comprehensive explanation without human effort. We use a popular technique \citep{Chain_of_thought} to guide GPT4-o \citep{Gpt4o} step by step in captioning and in visual feature matching. Figure \ref{fig:captioning} shows an example of applying this technique to concept circuit of class ``great white shark". We observe that MLLM can correctly identify the common visual features within exemplary images of NGCs. Furthermore, MLLM is able to match the features from lower level NGCs to those at higher level, detailing formation of new features, showing the potential of explaining in automation, capturing the gradual process of constructing the output of the model. The prompt used in this experiment is available in Appendix \ref{prompt}.
