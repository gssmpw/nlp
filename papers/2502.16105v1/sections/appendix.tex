\section{Notations}
We summarize the notations used in this work in Table \ref{fig:notation}.
\label{sec:appendix_notation}
% \mt{Let blue this whole table to reflect it's new. The more blue, the more substantial revision we have made}
\begin{table}[htb]
\caption{Notations summarization.} \label{fig:notation}
\centering
\begin{tabular}{|l|l|}
\hline
Notation & Meaning \\ \hline
$a$ & A neuron in a model at layer $l+1$ \\ \hline
$S$ & A set of neurons at layer $l$ \\ \hline
$\mathcal{V}_a$ & Concept of $a$: the top-$k$ highest image patches that activate $a$ \\ \hline
$\mathcal{V}^{\overline{S}}_{a}$  & Concept of $a$ when knocking out $S$ \\ \hline
$\mathcal{V}_{a, j}$ & The $j$-th semantic group of $a$ \\ \hline
$\tau$  & The number of core concept neurons of $a$ \\ \hline
$\sS_a$  & The set of core concept neurons of $a$        \\ \hline
$\phi^{1, l}$  & The function that maps from the dataset to the activation at layer $l$ of the model \\ \hline
$T(a, s_i, \mathcal{V}_a)$  & The importance score of $s_i \in \sS_a$ w.r.t $a$ on $\mathcal{V}_a$    \\ \hline
$w(a, s_i, \mathcal{V}_{a, j})$ & The normalized importance score of $s_i$ w.r.t $a$ on $\mathcal{V}_{a, j}$ \\ \hline
$r(v)$ & The activation vector of an input $v$  \\ \hline
$\mathcal{V}_{s_i, j}$ & The representative activation vector of the $j$-th semantic group  \\ \hline
$G$ & A neuron group \\ \hline
$\sS_G$ & The set of neurons of $G$ \\ \hline
$\sV_G$ & The concept of $G$ \\ \hline
$W(G_i, G_j)$ & The edge weight between $G_i$ and $G_j$ \\ \hline
\end{tabular}
\end{table}

\section{Related works summarization}
\label{sec:related_work_summarization}
Table \ref{tab: related work} compares our proposed method and existing approaches.

\begin{table}[tbh]
\center
\caption{Comparison of NeurFlow and existing approaches. \label{tab: related work}}
\renewcommand{\arraystretch}{1.2}
\resizebox{1\linewidth}{!}{
\begin{tabular}{l|l|l|l}
\toprule
\textbf{Method} & \textbf{Objectives} & \textbf{Level of granularity} & \textbf{Interaction quantification} \\ \midrule
\cite{NEUCEPT} & \multirow{3}{*}{\begin{tabular}[c]{@{}l@{}}Finding critical neurons to the \\ model's output \end{tabular}}  & \multirow{9}{*}{Neuron} & \multirow{9}{*}{N/A} \\ \cline{1-1}
\cite{neuron_shapley} &  &  &  \\ \cline{1-1}
\cite{critical_pathway} &  &  &  \\ \cline{1-2}
\cite{Polysemantic} & \multirow{7}{*}{\begin{tabular}[c]{@{}l@{}}Individual neuron explanation\end{tabular}} &  &  \\ \cline{1-1}
\cite{mu2020compositional} &  &  &  \\ \cline{1-1}
\cite{explaining_neuron} &  &  &  \\ \cline{1-1}
\cite{linear_explanation_neuron} &  &  &  \\ \cline{1-1}
\cite{mu2020compositional} &  &  &  \\ \cline{1-1}
\cite{Invert} &  &  &  \\ \cline{1-1} \cline{3-3}
\cite{falcon} &  & \multirow{2}{*}{Group of neurons}  &  \\ \cline{1-1}
\cite{hint} &  &  &  \\ \midrule
\cite{VCC} & Determining concept connectivity & Concept & Concept interaction \\ \midrule
\textbf{NeurFlow (Ours)} & \begin{tabular}[c]{@{}l@{}} {Determining groups of neurons'} \\ {function and interaction}\end{tabular} & {Group of neurons} & {Neuron group interaction} \\ \toprule
\end{tabular}
}
\end{table}

\section{{Limitations and discussion}}
{While we view NeurFlow as a significant step toward understanding the function and interaction of neuron groups, it is not without limitations. Our approach defines the concept of neurons as the top-$k$ most activated visual features, a common practice in the field \citep{Polysemantic, mu2020compositional, Multifaceted}. However, other researchers have broadened this definition to include concepts spanning a wider range of activation patterns \citep{explaining_neuron, linear_explanation_neuron}. This limitation highlights a promising direction for future research: developing more flexible frameworks that incorporate both top-$k$ activation and more distributed neural activation patterns.}

% {Our framework includes several hyperparameters to enhance flexibility, such as $\tau$. 
% We will investigate the issue raised by the Reviewer and improve the framework to enhance its robustness. }

{Furthermore, our research primarily focus on CNNs, which follows the main focus of a range of previous works in the field \citep{Olah, Polysemantic, Multifaceted, mu2020compositional}. However, we can apply our framework onto different DNN architectures by following several steps: 1) define the granularity level of neurons (i.e. individual units, feature maps, attention heads etc.); 2) iteratively identify the target neuron concept and the core concept neurons; 3) cluster the core concept neurons into groups and construct the concept graph. While exploring the differences in the inner workings of various architectures is valuable, we leave this promising direction for future works.}

% \section{{Detailed settings of experiments}}
% \textcolor{red}{@Tue: move detailed information regarding the experiment settings into this section.}

% {In our experiments, we use 50 original images per class (the standard number in the validation set). These images are cropped into patches of three different sizesâ€”$100\%$, $50\%$, and $25\%$ of the original dimensions. The cropping is performed using a sliding window with a $50\%$ overlap, resulting in approximately $2500$ patches in total.
% }

\section{Ablation studies}
\subsection{Comparison of attribution methods}
\label{sec:compare_scoring}
In this section, we run an ablation study on different choices of attribution method apart form our integrated gradient (IG) approach, verifying that IG-based score is the most suitable for the quantification of edge weights. We assess four additional common pixel attribution methods, including LRP \citep{lrp}, Guided Backpropagation \citep{guided_backprop}, SmoothGrad \citep{smoothgrad}, Saliency \citep{saliency}, Gradient Shap \citep{shapley}. Notably, SmoothGrad and Gradient Shap are a follow-up versions of IG. Furthermore, we also evaluate attribution method used in \citet{NEUCEPT}, which also find important neurons and attributing scores to them, referred to as Knockoff \citep{knockoff}. We run on the same setup as in Section \ref{sec:edge_verification} for $\tau$ ranging from 0 to 50. For easier comparison, we report the mean correlations of all values of $\tau$. Figure \ref{fig:compare_scoring} show the mean correlations across the last 10 layers of ResNet50 \citep{Resnet} and GoogLeNet \citep{Googlenet}. The Integrated Gradient consistently yields higher correlations compared to other attribution method, surpassing its follow-up version SmoothGrad, {while being comparable with Gradient Shap}. Furthermore, Knockoff shows a poor performance in ranking the importance of neurons compared to other attribution methods.  

Additionally, we also assess the running time of each method. Specifically, we recorded the run time of each method on 50 images on CPU (we implement Knockoff on KnockPy library \citep{knockpy} which does not run on GPU, hence, we evaluate all others on CPU for a fair comparison) across all layers of GoogLeNet. The results in Figure \ref{fig:run time} show that IG maintain a small running time compared to the follow-up method (i.e. SmoothGrad and Gradient Shap), while yielding the best correlations among the attribution methods. Hence, we choose IG-based score to assign the edge weights in NeurFlow.

\begin{figure}[tbh]
    \centering
    \begin{minipage}{.4\textwidth}
        \includegraphics[width=1\columnwidth]{figures/Adding_inference_time_Gradient_Shap.png}
        \vspace{-3mm}
        \caption{{The comparison of average inference time across all layers in GoogLeNet on CPU.}}\label{fig:run time}   
    \end{minipage}
    \hfill
    \begin{minipage}{.57\textwidth}
        \vspace{-2.5mm}
        \includegraphics[width=1\columnwidth]{figures/Adding_Gradient_Shap_for_correlation.png} 
        \vspace{-5mm}
        \caption{{The comparison of different attribution methods for edge weight quantification.}}\label{fig:compare_scoring}
    \end{minipage}
\end{figure}

\subsection{{Neuron group relation weights aggregation}}
\label{sec:weight_aggregation}
{In this experiment, we compare our choice of summing the edge weights with averaging the edge weights in forming $W(G_i, G_j)$ in Section \ref{sub_sec:constructing concept circuit}. Our aim is to verify that: \textit{groups of neurons with higher sum of scores will have higher impact on a target neuron, regardless of the number of neurons in the group}.}

{We randomly sample $500$ groups of neurons of varying sizes, ranging from $\{1, 5, 10, 20, 50\}$. For a target neuron in the upper layer, we analyzed the correlation between the loss function (defined in \ref{sec:analysis} and two metrics: the average edge weights within each group and our original scoring method, which sums the edge weights of neurons in the group. Higher absolute correlation values indicate a more effective scoring method. The results in figure \ref{fig:sum_vs_avg} are the average of 10 neurons of different labels in both GoogLeNet and ResNet50.}

\begin{figure}[tbh] 
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.75\textwidth]{figures/Correlation_sum_vs_avg.png} 
\end{center}
\vspace{-3mm}
\caption{{The correlations across 10 layers of our proposed aggregation (denoted as Original) and average aggregation (denoted as Average) on GoogLeNet and ResNet50.}}\label{fig:sum_vs_avg}
\end{figure}

\subsection{{Qualitative comparison of image debugging with NeuCEPT}}
\label{sec:compare_neucept_img_debugging}
{We conduct a qualitative experiment to compare the set of critical neurons identified by \citet{NEUCEPT} (the core concept neuron w.r.t the output logit of the model) and our set in the image debugging experiment. Specifically, following the setups in the experiment in section \ref{debugging}, we identify the top $\tau = 16$ core concept neurons at layer 4.2 of ResNet50 for both methods, which are used to determine the top-$2$ groups of core concept neurons for a given misclassified image. Groups of neurons were identified following the methodology described in section \ref{concept_circuit}, where the groups with the highest metric scores (defined in equation \ref{sec:application}) are selected. Furthermore, to quantify the contributions of the selected groups to the model output, we mask all of neurons in each groups and measure the changes of probability of the final predictions. The higher the changes, the more ``critical'' the groups of neurons.}
We select three classes, without cherry-picking, namely: Bald Eagle, Great White Shark, and Bee (corresponding to the classes in figure \ref{fig:img debug}, \ref{fig:captioning}, and \ref{fig:debias}). The results are presented in figure \ref{fig:compare_neucept_22}, \ref{fig:compare_neucept_2}, and \ref{fig:compare_neucept_309}.

\begin{figure}[tbh]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/class_22_Flow_vs_CEPT_rebuttal.png}
    \caption{{The comparison of the top-2 groups of neurons with the highest metric score of our method and \citet{NEUCEPT} on class \textit{Bald eagle}. The top logit drop images of NeurFlow are more resemble the original concept (i.e. NeurFlow concept 1 vs NeuCEPT concept 1). And the prediction probability changes when masking our core concept neurons are more significant while masking fewer neurons.}}
    \label{fig:compare_neucept_22}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/class_2_Flow_vs_CEPT_rebuttal.png}
    \caption{{The comparison of the top-2 groups of neurons with the highest metric score of our method and \citet{NEUCEPT} on class \textit{Great white shark.} The top logit drop images of NeurFlow are more resemble the original concept (i.e. NeurFlow concept 2 vs NeuCEPT concept 2). And the prediction probability changes when masking our core concept neurons are more significant while masking fewer neurons.}}
    \label{fig:compare_neucept_2}
\end{figure}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.95\textwidth]{figures/class_309_Flow_vs_CEPT_rebuttal.png}
    \caption{{The comparison of the top-2 groups of neurons with the highest metric score of our method and \citet{NEUCEPT} on class \textit{Bee}. The top logit drop images of both methods are similar to the exemplary image of the concept. And, both methods are able to alter the prediction of the model.}}
    \label{fig:compare_neucept_309}
\end{figure}

{Qualitatively, we observed that our method identified the top-$2$ concepts more closely resembling the original images. Additionally, our top logit drop images (i.e., "images showing the largest decrease in the target logit value" as described in \ref{debugging}) better matched the representative examples of the identified concepts. Furthermore, masking the core concept neuron groups identified by our method resulted in more significant changes to the prediction probabilities, using fewer neurons, compared to the groups identified by NeuCEPT \citep{NEUCEPT}. For instance, with the labels Bald Eagle and Great White Shark, masking NeuCEPTâ€™s core concept neurons had no effect on prediction probabilities, whereas masking the neurons identified by our method substantially altered the predictions. These findings suggest that our approach identifies more impactful neurons and concepts directly related to the modelâ€™s predictions compared to NeuCEPT.}

\subsection{{Quantitative comparison of core concept neurons of the model output}}
\label{sec:compare_to_model_output}
{We run an experiment to further verify: although our method focuses on the set of core concept neurons w.r.t a specific target neuron, our identified neurons also have strong influence to the performance of the model.}

{Specifically, we evaluate the overlaps between our core concept neurons and the critical neurons (which are specifically designed to find important neurons for the model output) determined by \citet{neuronmct} and \citet{NEUCEPT}, then average the results across all layers of ResNet50 and GoogLeNet of 10 random classes. The numbers of core concept neurons are set to be the same for all three methods. We measure the $F_1$ scores of the overlaps, which are shown in table \ref{tab:overlap}. The results imply that NeurFlow contains mostly similar core concept neurons to NeuronMCT while not directly identifying core concept neurons of the output.}

\begin{table}[t]
\caption{{Overlapping ratio of critical neurons between NeuronMCT \citep{neuronmct}, NeuCEPT \citep{NEUCEPT}, and core concept neurons of NeurFlow}}
\label{tab:overlap}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.9\linewidth}{!}{
\begin{tabular}{lccc}
\hline
\textbf{{Overlap}} & \textbf{{NeuronMCT-NeurFlow}} & \textbf{{NeuronMCT-NeuCEPT}} & \textbf{{NeurFlow-NeuCEPT}} \\
\hline
{ResNet50}   & {0.72} & {0.48} & {0.49} \\
\hline
{GoogLeNet}  & {0.79} & {0.55} & {0.56} \\
% \hline
\end{tabular}
}
\end{center}
\end{table}

\subsection{{Quantitative comparison of core concept neurons of a target neuron}}
\label{sec:compare_to_target_neuron}
{We assess our method of identifying core concept neurons given a specific target neuron with the method used in \citet{Olah}. In \citet{Olah}, neurons are ranked based on the top neurons with the highest $L_2$ weights connected to the target neuron. Note that this method is not applicable in other experiments since calculating weight magnitude is limited to consecutive layers.}

{For this comparison, we identify the top $\tau = 16$ core concept neurons in two consecutive layers (separated by one convolution layer, as per the setup in \citet{Olah}) using both methods. We then knock out these core concept neurons to observe how the target neuronâ€™s concept is affected. The extent of this change is quantified by the loss function defined in \ref{sec:analysis}, where a lower loss indicates better performance. We randomly selected 100 neurons across 10 different convolution layers from both models and calculated the average difference in losses between the two methods. A negative result indicates our method produces a better loss, while a positive result indicates otherwise.}

{The results are summarized in table \ref{tab:loss-subtraction}. These findings demonstrate that our method is more effective at identifying core concept neurons. Additionally, gradient-based approaches are more versatile, as they can be applied to non-consecutive layers (e.g., ResNet Block 4.2 â†’ ResNet Block 4.1 in our experiments), whereas the $L_2$-weight-based approach is limited to consecutive layers.}

\begin{table}[t]
\caption{{Average subtraction of the losses. Negative means our loss is better and vice versa}}
\label{tab:loss-subtraction}
\begin{center}
\renewcommand{\arraystretch}{1.2}
\resizebox{0.5\linewidth}{!}{
\begin{tabular}{lc}
\hline
\textbf{{Model}} & \textbf{{Average Subtraction of the Losses}} \\
\hline
{ResNet50}  & {-0.082} \\
{GoogLeNet} & {-0.013} \\
\hline
\end{tabular}
}
\end{center}
\end{table}


\subsection{{Dependence on the choices of $\tau$}}
\label{sec:choice_of_tau}
{\textbf{The trade-off of the parameter $\tau$:} In this experiment, we aim to study the choices of parameter $\tau$ on the set of core concept neurons of a model. Specifically, in the experiment ``Fidelity of core concept neurons", the choice of $\tau$ can be seen as a trade-off between simplicity (the number of core concept neurons) and performance (the accuracy of the prediction when retaining only the core concept neurons). However, for $\tau = 4, 8$ the results are vary across our tested models. We conduct additional experiment to highlight that for sufficiently large $\tau$, the results are less dependent on the parameter.}

{We evaluate on 10 different labels with the same setups as in the experiment ``Fidelity of core concept neurons" for $\tau = 20, 24$. The results in figure \ref{fig:dependence on tau} show that with these higher $\tau$ values, the performance drops of the model become negligible. Furthermore, the differences between retaining for $\tau = 20$ and $\tau = 24$ at all layers are minimal, suggesting that the dependence on $\tau$ decreases as we increase the value.}

\begin{figure}[tb] 
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/Dependence_on_tau.png} 
\end{center}
\vspace{-3mm}
\caption{{Effects of neuron groups on model's performance for $\tau = 16, 20, 24$. The effect of increasing $\tau$ are negligible for most of the layers in both models.}}\label{fig:dependence on tau}
\end{figure}

{\textbf{Completeness of core concept neurons on the output:} Additionally, we run an experiment to assess the completeness of NeurFlow in identifying the important neurons for the model's output. By greedily adding 50\% more neurons in each layer, of which the neurons are ranked by the importance scores defined in \citet{neuronmct}. The higher the scores, the stronger the influence on the prediction of the model. We then re-run the ``Fidelity of core concept neurons" for $\tau = 16$ (denoted as ``CC") and its extended version (50\% more neurons - denoted as ``Extended"). The results in figure \ref{fig:completeness} show that, for ResNet 50, adding non-core-concept neurons had almost no effect on improving model performance. For GoogleNet, only in the most critical case (where the retaining operation is applied up to layer 5b), adding $50\%$ more non-core-concept nodes led to an improvement in model performance by $25\%$ only at layer 5b in the retaining setup. These results show that when $\tau$ is sufficiently large, our algorithm ensures completeness.}

\begin{figure}[tb] 
\vspace{-3mm}
\begin{center}
\includegraphics[width=0.9\textwidth]{figures/Adding_more_neurons_for_figure_5.png} 
\end{center}
\vspace{-3mm}
\caption{{The comparison of the influences on models' performances of core concept neurons and the extended set of core concept neurons}}\label{fig:completeness}
\end{figure}

\subsection{{Dependence on the choices of $k$}}
\label{sec:choice_of_k}
{To evaluate the dependence of the results on the choice of $k$, we conducted additional experiments with various values of $k$ and measured the number of core concept neurons overlapping with the baseline setup of $k=50$. Greater overlap indicates less dependence on the choice of $k$.}

{Table \ref{tab:choice_of_k} summarizes the results with $\tau = 16$ (i.e., the maximum number of core concept neurons per target neuron is 16) and $k \in \{30, 40, 50, 60, 70, 90, 110, 130, 150, 170, 190\}$, evaluated across 50 random neurons. The results show that for all tested values of $k$, the overlap ratio is always at least $14/16$ ($>86\%$), demonstrating that the results of our proposed algorithm are independent of the choice of $k$.}

\begin{table}[t]
\caption{{The overlap of sets of core concept neurons of different $k$ compared to the baseline $k=50$}}
\label{tab:choice_of_k}
\begin{center}\renewcommand{\arraystretch}{1.2}
\resizebox{0.8\linewidth}{!}{
\begin{tabular}{c|ccccccccccc}
\hline
\multicolumn{1}{c|}{\bf {K}} & \bf {30} & \bf {40} & \bf {50} & \bf {60} & \bf {70} & \bf {90} & \bf {110} & \bf {130} & \bf {150} & \bf {170} & \bf {190} \\
\hline
{GoogLeNet} & {15.0} & {15.4} & \textbf{{16.0}} & {15.5} & {15.3} & {15.3} & {15.0} & {15.0} & {14.9} & {14.9} & {14.9} \\
{ResNet50}  & {14.9} & {15.6} & \textbf{{16.0}} & {15.6} & {15.3} & {14.7} & {14.5} & {14.3} & {14.1} & {14.0} & {14.0} \\
% \hline
\end{tabular}
}
\end{center}
\end{table}

\subsection{Multiple crop sizes augmentation}
\label{sec:multi_crop_sizes}
For a target class $c$, our input dataset is created by randomly cropping the images that the DNN classified as class $c$, similar to \citet{CRAFT}. However, since each neuron can detect feature at different granularity, we crop the images into multiple crop sizes in order to capture features at different levels. Intuitively, small crop sizes indicate low level while large crop sizes indicate high level features. 
{In our experiments, we crop the original images into patches of three different sizesâ€” 100\%, 50\%, and 25\% of the original dimensions. The cropping is performed using a sliding window with a 50\% overlap, resulting in roughly 2500 patches in total.}

Figure \ref{fig:crop_sizes} shows the percentages of each crop size in the concepts of core concept neurons throughout the networks. As demonstrated, lower layer's neurons often activated on small crop size images and vice versa. This aligns with the common believe that high level features are detected at the later stages of DNNs. This approach can be improved further by including more complex augmentation methods. However, in this work, our main focus is functional of groups of neurons and their interactions.
\begin{figure}[tb] 
\vspace{-3mm}
\begin{center}
\includegraphics[width=1\textwidth]{figures/Multi_crop_sizes.png} 
\end{center}
\vspace{-3mm}
\caption{Illustration of the percentages of crop sizes in the concepts of core concept neurons. 50 random classes are assessed for three models and three different crop sizes. The layer names are abbreviated (e.g.``feature.12" to ``f12").}\label{fig:crop_sizes}
\end{figure}

\section{Detailed algorithms}
\label{sec:main_algo}

\subsection{Identifying core concept neurons and constructing neuron circuit}
%Pseudo code for core concept neuron identification algorithm}
Algorithms \ref{alg:find critical neuron}, \ref{alg:determin_semantic_groups}, and \ref{algo:neuron circuit} provide detailed pseudocode for identifying core concept neurons, determining the semantic groups, and constructing the neuron circuit respectively.
\begin{algorithm}[tbh]
    \caption{Identifying core concept neurons}
    \textbf{Input}: Target neuron $a$, dataset $\mathcal{D}$, constraint $\tau$ \\
    \textbf{Output}: Set of core concept neurons $S_a$
    %\vspace{-3mm}
    \begin{algorithmic}[0]
        \STATE $\mathcal{V}_a \leftarrow \underset{\mathcal{V} \subset \mathcal{D}; |\mathcal{V}| = k}{\arg \max} \sum_{v \in \mathcal{V}} \phi_{a}(v)$
        \STATE $T \leftarrow$ calculate $T(a,s_i, \mathcal{V}_a), \: \forall s_i \in \sS$
        \STATE $\sS_a \leftarrow$ select the top-$\tau$ neurons with the highest $\|T\|$
        \STATE \textbf{return} $\sS_a$
   % \vspace{1.4mm}
    \end{algorithmic}\label{alg:find critical neuron}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Determining semantic groups}
    \textbf{Input}: Neuron concept $\mathcal{V}_a$ \\
    \textbf{Parameter}: Max number of clusters $N_{cluster}$ \\
    \textbf{Output}: Semantic groups $\mathcal{V}_{a, j}, \: \: \forall j$
    \begin{algorithmic}[0]
        \STATE $r(v^i_a) \leftarrow$ Calculate the representative vectors $\forall v^i_a \in \mathcal{V}_a$
        \STATE $best\_sil\_score \leftarrow -1$ 
        \STATE $best\_n \leftarrow$ Initialize 
        \FOR{Number of clusters $n$ in $\{ 2, \dots, N_{cluster} \}$}
            \STATE $\mathcal{V}_{a, j}' \leftarrow$ Agglomerative clustering with $n$ clusters on $\{r(v^i_a), \: \forall v^i_a \in \mathcal{V}_a\}$
            \STATE $sil\_score \leftarrow$ calculate the Silhouettes score given the results of clustering
            \IF{$best\_sil\_score < sil\_score$}
                \STATE $best\_sil\_score \leftarrow sil\_score$
                \STATE $best\_n \leftarrow n$
            \ENDIF
        \ENDFOR
        \STATE $\mathcal{V}_{a, j} \leftarrow$ Agglomerative clustering with $best\_n$ clusters on $\{r(v^i_a), \: \forall v^i_a \in \mathcal{V}_a\}$
        \RETURN $\mathcal{V}_{a, j}, \: \: \forall j \in \{ 1, \dots, best\_n \}$
    \end{algorithmic}\label{alg:determin_semantic_groups}
\end{algorithm}
\begin{algorithm}[H]
    \caption{Forming neuron circuit}
    \textbf{Input}: Logit neuron $a_c$, dataset $\mathcal{D}$, constraint $\tau$ \\
    \textbf{Output}: Neuron circuit $\mathcal{H}_c$
   % \vspace{-3mm}
    \begin{algorithmic}[0]
        \STATE $\mathcal{H}_c \leftarrow \{\,\}$; $S_L \leftarrow \{a_c\}$; $\mathcal{H}_c \leftarrow \mathcal{H}_c \cup S_L$
        \FOR{Layer $l$ in $\{L-1, \dots, 2, 1\}$}
        \STATE $S_l \leftarrow \{\,\}$
        \FOR{Target neuron $a$ in $S_{l+1}$}
        \STATE $S_l \leftarrow S_l \, \cup$ Identify core concept neurons (Alg.\ref{alg:find critical neuron}) of $a$ 
        \STATE $\mathcal{V}_{a,j} \leftarrow$ Determine semantic groups (Alg.\ref{alg:determin_semantic_groups}), $\: \forall j$
        \STATE $w(s_i, \mathcal{V}_{a,j}) \leftarrow T(a, s_i, \mathcal{V}_{a,j})/\sum_{s\in \mathbb{S}_a} \|T(a, s, \mathcal{V}_{a,j})\|$
        \ENDFOR
        \STATE $\mathcal{H}_c \leftarrow \mathcal{H}_c \cup S_l$
        \ENDFOR
        \RETURN $\mathcal{H}_c$
    \end{algorithmic}\label{algo:neuron circuit}
\end{algorithm}

% \subsection{Forming neuron groups} 
% \label{sec:forming neuron groups}
% To determining the neuron groups, we perform clustering on the representative vectors of semantic groups. These vectors are defined as follow. Specifically, for core concept neuron in layer $l$: $\sS^l$, we have a set of semantic groups $\sV_l = \{ 
% \mathcal{V}_{s, j_s}, \:\: \forall s \in \sS^l, \: \: \forall j_s \in \{ 1, \dots, \#semantic\_group\_of\_s \} \}$. 
% For each group $\mathcal{V}_{s, j_s}$, we calculate the representation vector 
% $\overrightarrow{r_{s, j_s}}$: 
% $\mathcal{R}_l = \{ \overrightarrow{r_{s, j_s}} := \frac{1}{|\mathcal{V}_{s, j_s}|}\sum_{v_s} mean(\phi^{1, l-1}(v_s)), \: \: v_s \in \mathcal{V}_{s, j_s}, \:\: \forall s \in \sS^l, \: \: \forall j_s \in \{ 1, \dots, \#semantic\_group\_of\_s \} \}$ 
% (where $\phi^{1,l-1}: \sD \xrightarrow{} \sR^{m \times w \times h}$ is the first $l-1$ layer of the model, mapping the input to the activation space; and the notation $mean()$ is taking the average value along the dimensions $w \times h$). 
% Then agglomerative clustering can then be applied on the set of representation vectors $\mathcal{R}_l$ to assign the semantic groups into different clusters. Given a cluster of semantic groups, the neurons, corresponding to those semantic groups, is assigned as a group of neurons (\textit{a neuron can be in different groups} as for multiple semantic groups).

\section{Image debugging setup}
\label{debugging_setup}
For an arbitrary input $v \in \mathcal{D}$, we want to see which parts of $v$ are detected by the group of neurons $G$. Thus, we crop the image into multiple crops, similar to what we do in Section \ref{subsec:identify_node}. The crops, denoted as $v_i$ are passed into the model to get the activations, which we can then measure the metric $M(v_i, \sS_G, \mathcal{D}), \: \forall v_i$. Then we can set a threshold for each group, so that, the crops with the scores above the threshold can be visualized. 

However, since the metric can be greatly affected by only one neuron in the group (i.e one neuron with low activation leads to a low metric score), the metric is prone to outliers. Thus, we only assess the metric on the subset $\sS_G' \subseteq \sS_G$. In practice, $\sS_G'$ contains the top-$5$ neurons that are closest to the group's center, where each neuron is represented as $\overrightarrow{r_{s_i,j}}$ for a neuron $s_i \in \sS_G$ with the semantic group's index $j$. The center of the cluster is the average of all representative vectors, and the distance between a pair of neurons is evaluated using $l_2$ distance.


\section{MLLM prompt for automatic concept labelling}
\label{prompt}
In this section, we provide our prompts for reproducibility. We employ two types of prompt, which are responsible either captioning the common concepts in the exemplary images of a neuron concept, or describing how a NGC formed from NGCs at the preceding layers. Our prompts include three parts. Firstly, we provide a role for MLLM model, marked as \textit{role description}. Secondly, the \textit{main prompt} is presented where it shows the general instruction for the task that MLLM should do. The \textit{role description }and \textit{main prompt} is the same for all setups. The last part is the \textit{answer form} where we give specific instruction on how to generate appropriate captions and the template of the answer. The structure of the whole prompts are: \textit{Role description} + \textit{Main prompt} + \textit{Answer form}.

\subsection{Role description and Main prompt}
\textbf{Role descriptions:} 
``Act as an Image Captioning Language Model."

\textbf{Main prompt:} 

``\# Core Responsibilities:

- Analyze a set of similar images to identify common features.

- Generate descriptive captions that highlight these common features.

- You must adapt to detect both simple and complex features.

\# Important notes:

- You don't have to generate captions for every image, focus on the common features.

- Outliers exist in the images, you could ignore them if they are not relevant to the common theme.

- You should describe the images with objective visual features, not subjective (like powerful or beautiful or scary etc., because these are only your opinion).

- You should only describe visual features, not the context or the story behind the images.

- You should keep a succinct caption, keep it one or two sentences long, that only describe a few most common features.

\# Role Summary:

Your role is to provide accurate and coherent captions for a set of similar images by identifying and describing common features. These features can range from simple elements like edges and colors to complex patterns such as a specific object in a particular setting."

\subsection{Answer form}

\textbf{Answer form for single concept captioning:} 

\emph{``\# Answer form:}

- Common features: a list of features

- Caption: your caption in one or two sentences"

\textbf{Answer form for describing NGC's formation:}

\emph{\# Key note of the input:}

- There are many different groups of images, make sure you get the number of groups right.

- Each group of images has a common feature.

- The higher level feature is the first group.

- Other groups are lower level features that combine to form the higher level feature of the first group.

\emph{\# Key note of the output:}

- You should not only focus on the common features of the images but also describe how the features from the lower level groups combine to form the higher-level feature of the first group.

- You should focus on the common features that shared among both the high and low level.

\emph{``\# Step by step:}

- Find the lists of common features in Group 2, \dots, N.

- For each feature from those lists: match it with the features in Group 1.

- Some of the features in the lists might have no matches: they might be combined with others to form new features, match the features in Group 1 with some simple combination of the features in Group 2, \dots, N (e.g. blue and green $\xrightarrow{}$ blue-green, multiple curve orientations $\xrightarrow{}$ a circle, two edges with different orientations $\xrightarrow{}$ an angle, etc.).

- If you don't find any visual features that match, please don't describe features that is not presented, instead, you can say "There is no matches".

- From the matched features, derive the common features in Group 1. 

- Generate caption for Group 1.

\emph{\# Answer form:}

- Group 1 Common Features: list of common features

- Group 2 Common Features: list of common features

- ...

- Group N Common Features: list of common features

Feature Evolution:

    - Group 2: has feature A - match feature A in Group 1 (for Group 2 to N, if there is no matches, please say "There is no matches")
    
    - ...
    
    - Group N: has feature B - match feature B in Group 1
    
Caption: one or two sentences capturing the common features and their evolution"