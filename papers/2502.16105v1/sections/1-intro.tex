The explainable AI (XAI) field has seen significant advancement in understanding the mechanisms of deep neural networks (DNNs). This field emerges from the growing need in decoding the internal representations, in hope of reverse engineering deep models into human interpretable program. Prior works have initiated on breaking down convolutional neural networks (CNNs) into interpretable neurons, understanding the models in the most fundamental units \citep{Multifaceted, Deconv, Polysemantic, Invert}. Extending further, one can examine the relation between neurons to gain insights on how the model works, within one layer \citep{NEUCEPT}, and between multiple layers \citep{Olah}. Ultimately, recent works try to generate circuits \citep{Olah, Invert, GPT2circuit, ACDC} that create exhaustive explanations of how features are processed and evolve throughout the model.

The majority of existing methods focuses on individual neurons \cite{oikarinen2024linear, NEURIPS2023_debd0ae2} and their relationship to the model's final predictions \cite{ghorbani2020neuron, wang2022hint, NEURIPS2020_41c542df}, while giving less attention to exploring and quantifying the relationships and interactions between neurons across different layers.
These approaches are not only constrained by scalability challenges arising from the extensive number of neurons, but they also hinder a comprehensive understanding of the underlying mechanisms of DNNs. 
A notable example is the polysemantic phenomenon \cite{mu2020compositional, Polysemantic, olah2020zoom}, where a single neuron is activated by several unrelated concepts.
This phenomenon complicates the task of associating each neuron with a distinct feature and hampers the interpretation of how a model processes concepts based on the relationships among neurons.
Drawing inspiration from human inference, which synthesizes information from a variety of sources, we contend that, in addition to individual neuron encoding multiple concepts (as demonstrated in prior studies \cite{Polysemantic, olah2020zoom}), groups of neurons within each layer also collectively encode the same concept. Furthermore, the decision-making process in neural networks is shaped not solely by the interactions between individual neurons, but rather by interactions among neuron groups.

This study seeks to explore the roles and interactions of neuron groups in shaping and developing concepts, enabling the execution of specific tasks.
Due to the complex connections between large number of neurons, identifying those functions and there interactions is a daunting task. 
To overcome this, we demonstrate that for a particular task, only a subset of neurons—referred to as \textit{core concept neurons}—play a crucial role as influential and concept-defining elements in neural networks. These neurons, when deactivated, significantly alter the associated concepts.
 
Focusing on core concept neurons allows us to view the intricate network in a simplified way, revealing the most important interactions between the groups of neurons. 
Therefore, we propose NeurFlow framework that (1) identifies core concept neurons, (2) clusters these neurons into groups, and (3) investigates the functions and interactions of these groups. To enhance interpretability, we represent each  neuron group by the set of visual features it encodes (i.e., named as neuron group’s concept). Focusing on classification models, we construct, for each class of interest, %c$, 
a hierarchical tree in which nodes represent neuron groups (defined by the concepts they encode), and edge weights quantify the interactions between these groups.

Our key contributions are summarized as follows:
%\begin{itemize}

\noindent i) We introduce an innovative framework that systematically builds a circuit to elucidate the mechanisms by which core concept neuron groups operate and interact to achieve specific tasks. This entire process is automated, necessitating no human intervention or predefined concept labels. To our knowledge, we are the first to employ neuron groups as the fundamental units for explaining the internal workings of deep neural networks.

\noindent ii) We perform empirical studies to validate the proposed framework, demonstrating the optimality and fidelity of core concept neurons, and the reliability of interaction weights between core concept neuron groups. 

\noindent iii) We provide experimental evidence showing that our framework can be applied to various tasks, including image debugging and automatic neuron concept labeling. Specifically, we confirm the biases found by \citet{Debias} on ImageNet \citep{Imagenet}, which have not been proven, by masking the core concept neurons related to the biased features.
%\end{itemize}



% However, existing frameworks exhibit several limitations. Most noticeably, circuits are often constructed by hand. Although \citet{ACDC} show that the process of finding the circuit can be automated, determining the meaning and relation of the each component still requires extensive human effort. Moreover, most approaches are often relying on brute-force method, analyzing the circuits' neurons one by one \citep{Olah, Invert, Multifaceted, Polysemantic}. Generally, this is infeasible for large models \citep{Olah}. Lastly, although learning the the meaning of each component has gained popularity, there is no work that automatically explicate the resulting circuit, still depending on post-hoc human evaluations and annotations.

% As it stands, there has been no work that comprehensively and automatically labels the concepts neurons, as well as, explores the relations between them. In this work, focusing on CNNs on image classification task, we provide a framework that systematically constructing and explaining circuits. Unlike prior works, we do not take the brute-force approach of examining one neuron at a time. Instead, we identify which neuron is important and which can be neglected, and further considering neurons as a group, for multiple layers. As a result, our framework generate an abstraction of inner mechanisms without the need to analyze each neuron in detail. In turn, we can debug misclassified images, detect biases, label the meanings of neurons and how the they are formed with multi-modal large language model (MLLM), all without human intervention. Our contributions are:

% \begin{itemize}
%     \item We propose a framework that systematically identifies the neuron circuit by considering only important neurons. This circuit helps us understand both the meaning of neurons and the relation between them.
%     \item We propose a way to look at neurons not individually but as a group, allowing us to analyze on different levels of granularity, without having to scan every neuron. This has not yet been proposed by previous works that explain neurons.
%     \item We show that our framework can be applied to debug misclassified images and detect biases. We confirm the biases found by \citet{Debias} on ImageNet \citep{Imagenet}, which have not been proven, by masking the neurons related to the biased features.
%     \item We utilize MLLM to automatically label each neuron, and further describe how the meaning of the neuron formed from neurons at the previous layer.
% \end{itemize}