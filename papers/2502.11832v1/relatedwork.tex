\section{Background and Related Work}
\label{sec:background}
\subsection{Normalization Operation within LLM}
\label{sec:layernorm}

Normalization operation has been widely utilized in the modern LLM architecture. Normalization can help in faster convergence during training by reducing the internal covariate shift problem~\cite{ba2016layer}. It ensures that the distributions of inputs to each layer remain stable throughout training. Two common normalization operations are widely employed: LayerNorm and RMSNorm~\cite{zhang2019root}. RMSNorm is utilized in LLMs such as the LLaMA series~\cite{touvron2023llama, touvron2023llama2} and Mistral~\cite{jiang2023mistral}, while LayerNorm is employed in LLMs like OPT~\cite{zhang2022opt}, the GPT series~\cite{brown2020language}, and Megatron-LM~\cite{shoeybi2019megatron}.
Specifically, for layer normalization function, the function takes a vector $\boldsymbol{z}=[z_1,\cdots,z_N]^T$ and generates an output $\boldsymbol{s}=[s_1,\cdots,s_N]^T$, both have a length of N. It is defined as follows:
\begin{equation}
\label{eqn:layernorm}
\small
\boldsymbol{s}=\alpha\frac{\boldsymbol{z}-\mu_{z}}{\sigma_{z}} + \beta
\end{equation}
where $\mu_{z}=\frac{\sum_{i} z_{i}}{N}$ is the mean of the vector $\boldsymbol{z}$, $\sigma_{z} = \sqrt{\frac{\sum_{i} (z_{i}-\mu_{z})^{2}}{N}}$. $\alpha$ and $\beta$ are learnable vectors which has the identical shape as $\boldsymbol{z}$. $\alpha$ and $\beta$ will both be fixed during the LLM execution. To apply LayerNorm on the input with a dimension $B\times L\times E$, the LayerNorm will apply to the $B\times L$ vectors, with each has a length of $E$, and $B$ and $L$ denote the batch size and token length, respectively. The intermediate results will then proceed with affine transformation by multiplying with a $E\times 1$ vector $\alpha$ and sum with a $E\times 1$ vector $\beta$.

RMSNorm adopts a more efficient strategy by rescaling the input vectors using only the RMS value of the input vector. Instead of re-centering the data, it directly normalizes the activations based on their RMS value. This approach provides higher computational efficiency compared to LayerNorm. Specifically, RMSNorm can be expressed as:
\begin{equation}
\label{eqn:rmsnorm}
\small
RMSNorm(\boldsymbol{z})=\alpha\frac{\boldsymbol{z}}{r_{z}} + \beta
\end{equation}
where $r_{z} = \sqrt{\frac{\sum_{i} (z_{i})^{2}}{N}}$ is the RMS value of $\boldsymbol{z}$.


\subsection{Efficient Normalization within LLM}
\label{sec:bg:related_work}
Multiple studies have been conducted to accelerate the normalization operation within the transformer architecture. The previous methods including DFX~\cite{hong2022dfx}, ~\cite{jeong2023low} and~\cite{lu2020hardware} modified the way for variance computation, which further improves the parallelism and reduce the processing latency of the layernorm computation. 
In~\cite{wang2023sole}, the intermediate results are dynamically compressed for the variance computation with low precision, leading to a reduction on energy and latency consumption. In~\cite{liu2021hardware}, the parameters of LayerNorm is quantized using 8 bits. However, this will produce a degradation on the accuracy performance.
All the previous mentioned work are proposed for conventional transformer acceleration, which has entirely different scale and data distribution from LLM. These methods often involve rigid quantization and approximate computation, which can be costly for LLMs due to their large size and the high expense of fine-tuning. HAAN selects skipped normalization layers offline with minimal accuracy impact and can be reconfigured to optimize precision and input subsampling for LayerNorm without additional fine-tuning costs.