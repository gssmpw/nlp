[
  {
    "index": 0,
    "papers": [
      {
        "key": "ba2016layer",
        "author": "Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E",
        "title": "Layer normalization"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhang2019root",
        "author": "Zhang, Biao and Sennrich, Rico",
        "title": "Root mean square layer normalization"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "touvron2023llama",
        "author": "Hugo Touvron and others",
        "title": "LLaMA: Open and Efficient Foundation Language Models"
      },
      {
        "key": "touvron2023llama2",
        "author": "Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others",
        "title": "Llama 2: Open foundation and fine-tuned chat models"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "jiang2023mistral",
        "author": "Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others",
        "title": "Mistral 7B"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "zhang2022opt",
        "author": "Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others",
        "title": "Opt: Open pre-trained transformer language models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Tom B. Brown and others",
        "title": "Language Models are Few-Shot Learners"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "shoeybi2019megatron",
        "author": "Shoeybi, Mohammad and Patwary, Mostofa and Puri, Raul and LeGresley, Patrick and Casper, Jared and Catanzaro, Bryan",
        "title": "Megatron-lm: Training multi-billion parameter language models using model parallelism"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "hong2022dfx",
        "author": "Hong, Seongmin and Moon, Seungjae and Kim, Junsoo and Lee, Sungjae and Kim, Minsub and Lee, Dongsoo and Kim, Joo-Young",
        "title": "DFX: A low-latency multi-FPGA appliance for accelerating transformer-based text generation"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "jeong2023low",
        "author": "Jeong, Seongho and Seo, Minseok and Nguyen, Xuan Truong and Lee, Hyuk-Jae",
        "title": "A Low-Latency and Lightweight FPGA-Based Engine for Softmax and Layer Normalization Acceleration"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "lu2020hardware",
        "author": "Lu, Siyuan and Wang, Meiqi and Liang, Shuang and Lin, Jun and Wang, Zhongfeng",
        "title": "Hardware accelerator for multi-head attention and position-wise feed-forward in the transformer"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "wang2023sole",
        "author": "Wang, Wenxun and Zhou, Shuchang and Sun, Wenyu and Sun, Peiqin and Liu, Yongpan",
        "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "liu2021hardware",
        "author": "Liu, Zejian and Li, Gang and Cheng, Jian",
        "title": "Hardware acceleration of fully quantized bert for efficient natural language processing"
      }
    ]
  }
]