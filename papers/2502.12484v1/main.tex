%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{amssymb}
\usepackage{multirow}
% \usepackage{apacite}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}

\usepackage{color}
\usepackage{xspace}
\newcommand{\name}[0]{ \xspace} 
% general commands
\newcommand{\TODO}[1]{{\textbf{\textcolor{red}{TODO: #1}}}}
\newcommand{\NOTE}[1]{{\textbf{\textcolor{red}{NOTE: #1}}}}
\newcommand{\HK}[1]{\textcolor{red}{[KunHe: #1]}}
\newcommand{\WJR}[1]{\textcolor{blue}{[WJR: #1]}}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers}

\begin{document}

\twocolumn[
%\icmltitle{LocalEscaper: A Weakly-Supervised Neural Solver for solving Large-Scale Traveling Salesman Problems}
\icmltitle{LocalEscaper: A Weakly-supervised Framework with Regional Reconstruction for Scalable Neural TSP Solvers}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Junrui Wen}{hust}
\icmlauthor{Yifei Li}{hust}
\icmlauthor{Bart Selman}{Cornell} %selman@cs.cornell.edu
\icmlauthor{Kun He}{hust}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{hust}{School of Computer Science, Huazhong University of Science and Technology, China}
\icmlaffiliation{Cornell}{Department of Computer Science, Cornell University, United States}

\icmlcorrespondingauthor{Kun He}{brooklet60@hust.edu.cn}


% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{combinatorial optimization, neural solver, weakly-supervised learning, TSP}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

% \printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.
\footnotetext[1]{School of Computer Science, Huazhong University of Science and Technology, China. ${}^2$Department of Computer Science, Cornell University, United States. Correspondence to: Kun He $<$brooklet60@hust.edu.cn$>$.\\\\Preprint.}

% \footnote{Department of Computer Science, Cornell University, United States}

\begin{abstract}
Neural solvers have shown significant potential in solving the Traveling Salesman Problem (TSP), yet current approaches face significant challenges.  Supervised learning (SL)-based solvers require large amounts of high-quality labeled data, while reinforcement learning (RL)-based solvers, though less dependent on such data, often suffer from inefficiencies. 
%Current approaches rely primarily on supervised learning (SL) or reinforcement learning (RL). While SL-based solvers require large volumes of high-quality labeled data, RL-based solvers, though free from this dependency, often suffer from lower efficiency. 
To address these limitations, we propose \textit{LocalEscaper}, a novel weakly-supervised learning framework for large-scale TSP. 
LocalEscaper effectively combines the advantages of both SL and RL, enabling effective training on datasets with low-quality labels. To further enhance solution quality, we introduce a \textit{regional reconstruction strategy}, which %effectively escapes local optima,
mitigates the problem of local optima, a common issue in existing local reconstruction methods. 
Additionally, %to alleviate the computational burden of large-scale TSPs, we develop an attention mechanism with linear complexity, significantly reducing overhead without sacrificing performance. 
we propose a linear-complexity attention mechanism that reduces computational overhead, enabling the efficient solution of large-scale TSPs without sacrificing performance. 
Experimental results on both synthetic and real-world datasets demonstrate that LocalEscaper outperforms existing neural solvers, achieving state-of-the-art results. Notably, %it efficiently solves TSP instances with up to 50,000 cities, setting a new benchmark for scalability and effectiveness in the field.  
 it sets a new benchmark for scalability and efficiency, solving TSP instances with up to 50,000 cities.
\end{abstract}

\section{Introduction}
\label{01Intro}
\input{01Intro}

\section{Related Work}
\label{02RW}
\input{02RW}

\section{Preliminaries}
\label{03Pre}
\input{03Pre}

\section{Methodology}
\label{04Method}
\input{04Method}

\section{Experiments}
\label{05Exp}
\input{05Exp}

\section{Conclusion}
In this work, we propose LocalEscaper, a weakly-supervised framework with regional reconstruction for scalable neural TSP solvers.
Our method leverages RL-based heuristics to refine solutions and iteratively enhance low-quality training labels, thereby reducing reliance on expert-annotated data. 
For solution improvement, we introduce a regional reconstruction strategy that enhances subsequence reconstruction by improving node reordering and escaping local optima.
Furthermore, we introduce a TSP-specific linear attention mechanism that reduces computational complexity, enabling our model to efficiently handle large-scale instances.
The LocalEscaper framework achieves state-of-the-art performance among neural combinatorial optimization solvers, outperforming previous methods in solution accuracy, inference speed, and generalization to large-scale TSP instances.
%In future work, we aim to further enhance the scalability of LocalEscaper and explore its applicability to other combinatorial optimization problems.



%换页
\newpage

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

%\section*{Software and Data}

%If a paper is accepted, we strongly encourage the publication of software and data with the camera-ready version of the paper whenever appropriate. This can be done by including a URL in the camera-ready copy. However, \textbf{do not} include URLs that reveal your institution or identity in your submission for review. Instead, provide an anonymous URL or upload the material as ``Supplementary Material'' into the OpenReview reviewing system. Note that reviewers are not required to look at this material when writing their review.

% Acknowledgements should only appear in the accepted version.
%\section*{Acknowledgements}

\section*{Impact Statement}
This paper presents work whose goal is to %advance the field of Machine Learning. 
Explore approaches to combine deep learning techniques in solving combinatorial optimization problems in large scale, that could be applicable for real world applications. There are many potential societal consequences 
of our work, none of which we feel must be specifically highlighted here.


% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite
\nocite{langley00}

\bibliography{main}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\iffalse 
\newpage
\appendix
\onecolumn
\section{You \emph{can} have an appendix here.}

You can have as much text here as you want. The main body must be at most $8$ pages long.
For the final version, one more page can be added.
If you want, you can use an appendix like this one.  

The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
\fi 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
