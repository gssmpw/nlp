\begin{figure*}[t]
\centering
\includegraphics[width=160mm]{Z_Framework.pdf}
\caption{The overview of the proposed framework. The framework consists of a dataset and three tasks, which can be executed in parallel. In this paper, we adopt a sequential execution approach for Task 1 and Task 2.}
\label{fig:Framework}
\end{figure*}

In this section, we present a weakly supervised learning framework to address the challenges of large-scale TSP outlined in Section I. An overview of our framework is shown in Figure \ref{fig:Framework}. The weakly-supervised learning framework consists of a dataset and three tasks: Task 1 to improve the dataset, Task 2 to train reconstruction models, and Task 3 to train the constructive model. These tasks can be executed in parallel. In this study, we adopt a structure in which these tasks are distributed across two processes: Process 1 alternates between Task 1 and Task 2, while Process 2 continuously executes Task 3. 

\subsection{Weakly Supervised Learning Framework}
We define the dataset as $\mathcal{D}^t(\boldsymbol{\mathcal{G}}, \boldsymbol{\mathcal{T}}^t)$, where $\boldsymbol{\mathcal{G}}=\{\mathcal{G}_1,\mathcal{G}_2,\dots,\mathcal{G}_N\}$ represents the set of $N$ TSP graphs, $\boldsymbol{\mathcal{T}}^t = \{ \boldsymbol{\tau}_1^t, \boldsymbol{\tau}_2^t, \dots, \boldsymbol{\tau}_N^t\}$ represents the set of corresponding tour labels, and $t \in \{0,1,\dots,T\}$ is the iteration counter of the improvement.
Given the graph set $\boldsymbol{\mathcal{G}}$, the initial label set $\boldsymbol{\mathcal{T}}^0$ is generated using the random insertion algorithm.
% We use the dataset $\mathcal{D}$ to train a constructive model with a light encoder and a heavy decoder \cite{luo2023neural}. The model constructs solutions by sequentially selecting the next city to visit.
% The weakly supervised learning framework is divided into two processes: 1) training of a SL-based model, and 2) improvement of the inaccurate dataset and training of an RL-based improver.
We use the dataset $\mathcal{D}$ to train a constructive model, parameterized by $\boldsymbol{\theta}$, 
following the learning to construct partial solutions scheme \cite{luo2023neural}.
% that generates solutions by sequentially selecting the next node to visit. 
% that follows a supervised learning setup \cite{luo2023neural}. 
% The model follows supervised learning setup of LEHD \cite{luo2023neural}. 
We sample a subsequence $\boldsymbol{\tau}^{\prime}$ from the label $\boldsymbol{\tau}$ where $\boldsymbol{\tau}^{\prime}$ provides an indication $y_i$ for each available node $v_i$ on whether it should be visited, with $y_i=1$ for selection and $y_i=0$ for non-selection. 
In each training step, the model predicts the selection probability $p_i$ for each available node $v_i$. 
The optimization objective is the cross-entropy loss:
\begin{equation}
{\mathcal{L}(\boldsymbol{\theta})=-\sum\limits_{i=1}^{u-1}{y_i\text{log}(p_i)}},
\label{eq:Balance}
\end{equation}
where $u$ is the number of available nodes. 

% We use an improver $\mathcal{I}$ , which is independent of the model $\boldsymbol{\theta}$ to improve the data set $\mathcal{D}$. 
We use an improver, denoted as $\mathcal{I}$, to improve the label quality of the dataset $\mathcal{D}$. 
The improvement process is defined as $\mathcal{D}^{t+1} = \mathcal{I}(\mathcal{D}^t)$, and it satisfies $L_{\text{total}}(\boldsymbol{\tau}^{t+1}) \leq L_{\text{total}}(\boldsymbol{\tau}^{t})$ for any $\boldsymbol{\tau}^{t} \in \boldsymbol{\mathcal{T}}^{t}$. 
After each improvement, we train the improver $\mathcal{I}$ using reinforcement learning and train the constructive model $\boldsymbol{\theta}$ using supervised learning on the updated dataset $\mathcal{D}^{t+1}$. 

Based on the above, our weakly supervised learning framework can be divided into two processes: 1) training the constructive model $\boldsymbol{\theta}$, and 2) improving the inaccurate dataset $\mathcal{D}$ and training of the improver $\mathcal{I}$. 
Since the improver $\mathcal{I}$ is independent of the model $\boldsymbol{\theta}$ in terms of parameters, these two processes can be performed in parallel. Next, we will provide a detailed description of the improver.

\subsection{Improver for Escaping Local Optima}
We introduce the improver, a key component of our algorithm. 
During training, the improver is used to improve the label quality of the dataset. During inference, the improver can serve as a neural-based heuristic to further improve the solution quality. 
Our improver helps escape local optima encountered in subsequence reconstruction by utilizing regional reconstruction.
% , defined as $\mathcal{I}$. 
% The improver takes a solution $\boldsymbol{\tau}^\text{in}$ as input and outputs a improved solution $\boldsymbol{\tau}^\text{out}=\mathcal{I}(\boldsymbol{\tau}^\text{in})$, such that $L_{\text{total}}(\boldsymbol{\tau}^\text{out}) \leq L_{\text{total}}(\boldsymbol{\tau}^\text{in})$. 
Each improvement consists of three steps: subsequence reconstruction, 2-opt, and regional reconstruction.

\textbf{Subsequence Reconstruction}: We follow \cite{ye2024glop} to randomly decompose a solution of length $n$ into $\lfloor \frac{n}{m} \rfloor$, each of length $m$. 
To improve the homogeneity of the input, we perform a transformation on the node coordinates of each subsequence graph. 
For the coordinates $\boldsymbol{x}$ and $\boldsymbol{y}$, we apply $\boldsymbol{x}^{\prime}=\frac{\boldsymbol{x}-\bar{x}}{\sigma}$ and $\boldsymbol{y}^{\prime}=\frac{\boldsymbol{y}-\bar{y}}{\sigma}$, where $\sigma$ is the maximum absolute value among the elements of $\boldsymbol{x}$ and $\boldsymbol{y}$. 
% We add a bias to the $x$- and $y$-coordinates to center their means at 0, and then scale 
A neural model, parameterized by $\boldsymbol{\psi}$, reconstructs all subsequences, replacing each original subsequence $\boldsymbol{\tau}^{\prime}$ with its corresponding reconstruction \textbf{$\boldsymbol{\tau}^{\prime\prime}$},
if $L_{\text{sub}}(\boldsymbol{\tau}^{\prime\prime}) \leq L_{\text{sub}}(\boldsymbol{\tau}^{\prime})$.
The model follows the multiple trajectory approach of POMO \cite{kwon2020pomo}, with the distinction that we adopt a non-autoregressive neural model. Specifically, the encoder generates the heatmap $\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}}$ of edge selection probabilities in one pass, and the decoder iteratively constructs new subsequences based on $\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}}$. 
% The heatmap $\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}}$ is computed as follows:
% \begin{equation}
% {\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}} = },
% \label{eq:Heatmap Subsequence Reconstruction}
% \end{equation}
Non-autoregressive models offer fast construction speeds. When the subsequence length $m$ is small (e.g., within 100), the drawback of coarser construction becomes less pronounced.

\textbf{2-opt}: We apply the classic operations research algorithm \cite{lin1973effective}, checking the effect of all edge swaps in a graph. If a swap that improves the solution exists, it is executed, and then we recheck for the next beneficial swaps. The process is repeated until no further improvements can be made. 

\textbf{Regional Reconstruction}: 
Based on our definition of the regional reconstruction problem, when we remove the edges $\mathcal{E}_{\boldsymbol{\tau}}^{\prime}$ from set $\mathcal{E}_{\boldsymbol{\tau}}$, if $|\mathcal{E}_{\boldsymbol{\tau}}^{\prime}|=k$, the remaining edges $\mathcal{E}_{\boldsymbol{\tau}} \setminus \mathcal{E}_{\boldsymbol{\tau}}^{\prime}$ can compose $k$ subsequences 
% $\mathcal{T}^{\prime} = 
$\mathcal{S}=\{ \boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \dots, \boldsymbol{\tau}_{k}^{\prime} \}$ (Insensitive to the direction of each subsequence in $\mathcal{S}$).  To ensure that the reconstructed solution is a Hamiltonian circuit, we transform the process of 
% finding $\mathcal{E}^{\text{add}}$ 
reconstruction 
into a permutation problem. We permute all subsequences in 
% $\{ \boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \dots, \boldsymbol{\tau}_{k}^{\prime} \}$ 
$\mathcal{S}$
to form a subsequence list $\mathcal{X}=(\boldsymbol{x}_{1}, \boldsymbol{x}_{2}, \dots, \boldsymbol{x}_{k})$. During the permutation, we can independently decide whether to reverse each subsequence. 
For a subsequence $\boldsymbol{\tau}^{\prime} = (\tau_1^{\prime}, \tau_2^{\prime}, \dots, \tau_{m}^{\prime})$, and its reverse is denoted as $\neg \boldsymbol{\tau}^{\prime} = (\tau_{m}^{\prime}, \tau_{m-1}^{\prime}, \dots, \tau_{1}^{\prime})$. 
After the permutation, all subsequences in $\mathcal{X}$ are concatenated to form a new Hamiltonian circuit $\boldsymbol{\tau}^{\text{new}}$.

We model this permutation task as a MDP. 
% Markov Decision Process (MDP).
The subsequence list $\mathcal{X}$ is initialized as an empty list, and the set of available subsequences is initialized as $\mathcal{C}=\{\boldsymbol{\tau}_{1}^{\prime}, \neg\boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \neg\boldsymbol{\tau}_{2}^{\prime},\dots,\boldsymbol{\tau}_{k}^{\prime}, \neg\boldsymbol{\tau}_{k}^{\prime}\}$. Then, we gradually append subsequence from $\mathcal{C}$ to $\mathcal{X}$.
The MDP includes: states, actions, transition, and rewards. 
% In step $t \in \{1,2,\dots,k\}$, they are expressed as

\textit{States}: List $\mathcal{X}$ and available subsequences $\mathcal{C}$.

\textit{Actions}: The subsequence $\boldsymbol{\tau}_{\text{act}}$ that is selected from $\mathcal{C}$ and to be appended to $\mathcal{X}$.

\textit{Transition}: After performing an action, append $\boldsymbol{\tau}_{\text{act}}$ to $\mathcal{X}$, and remove both $\boldsymbol{\tau}_{\text{act}}$ and its reverse $\neg\boldsymbol{\tau}_{\text{act}}^{\prime}$ from $\mathcal{C}$ to transition to the next state. The reconstruction is complete when $\mathcal{C}$ is an empty set.

\textit{Rewards}: 
% In step $t$, before appending $\boldsymbol{\tau}_{\text{act}}$ to $\mathcal{X}$, the last node of the last subsequence in $\mathcal{X}$ is denoted by $s_{t-1}^{\text{last}}$, the first node of the first subsequence in $\mathcal{X}$ is denoted by $s_{1}^{\text{first}}$. The is defined as: 
% \begin{equation}
% {
% r(t)=\left \{
% \begin{array}{lcl}
% 0       &      & {t=1,}\\
% \text{cost}(s_{t-1}^{\text{last}}, \tau_{\text{act}}^{\text{first}})+\text{cost}(s_{1}^{\text{first}}, \tau_{\text{act}}^{\text{last}})     &      & {t=k,}\\
% \text{cost}(s_{t-1}^{\text{last}}, \tau_{\text{act}}^{\text{first}})     &      & {\text{otherwise,}}
% \end{array} \right.
% }
% \label{eq:Balance}
% \end{equation}
% where $\tau_{\text{act}}^{\text{first}}$ and $\tau_{\text{act}}^{\text{last}}$ are the first and last node in $\boldsymbol{\tau}_{\text{act}}$ respectively. 
When the reconstruction is completed, the total reward is defined as
\begin{equation}
{R(\mathcal{X})=-\text{cost}(x_{1}^{\text{first}}, x_{k}^{\text{last}})-\sum\limits_{i=1}^{k-1}\text{cost}(x_{i}^{\text{last}}, x_{i+1}^{\text{first}})},
\label{eq:Reward}
\end{equation}
where $x_{i}^{\text{first}}$ and $x_{i}^{\text{last}}$ is the first and last node of subsequence $\boldsymbol{x}_i$. 

We follows the multiple trajectory approach \cite{kwon2020pomo} to employ the REINFORCE algorithm \cite{williams1992simple} for training a regional reconstruction policy model, parameterized by $\boldsymbol{\phi}$. 
For one regional reconstruction problem, we sample a set of subsequence list $\boldsymbol{\mathcal{X}}=\{\mathcal{X}^{1}, \mathcal{X}^{2}, \dots, \mathcal{X}^{N} \}$. 
The gradient of policy network is calculated by an approximation 
\begin{equation}
\begin{aligned}
\triangledown_{\boldsymbol{\phi}}J(\boldsymbol{\phi}) &\approx \frac{1}{N}\sum\limits_{i=1}^{N} \frac{(R(\mathcal{X}^{i})) - \mu(\boldsymbol{\mathcal{X}})}{\delta(\boldsymbol{\mathcal{X}})} \triangledown_{\boldsymbol{\phi}} \text{log} ~ p_{\boldsymbol{\phi}}(\mathcal{X}^{i}| \mathcal{S}) 
\\
\mu(\boldsymbol{\mathcal{X}}) &= \frac{1}{N}\sum\limits_{i=1}^{N} R(\mathcal{X}^{i}) \\
\delta(\boldsymbol{\mathcal{X}}) &= \sqrt{\frac{1}{N}\sum\limits_{i=1}^{N} (R(\mathcal{X}^{i})-\mu(\boldsymbol{\mathcal{X}}))^2}
, 
\label{eq:Regional Reconstruction Gradient}
\end{aligned}
\end{equation}
% where $\mu(\boldsymbol{\mathcal{X}})$ and $\delta(\boldsymbol{\mathcal{X}})$ are the mean and standard deviation of the rewards for multiple subsequence lists in $\boldsymbol{\mathcal{X}}$, with $\mu(\boldsymbol{\mathcal{X}}) = \frac{1}{N}\sum\limits_{i=1}^{N} R(\mathcal{X}^{i})$ and $\delta(\boldsymbol{\mathcal{X}}) = \sqrt{\frac{1}{N}\sum\limits_{i=1}^{N} (R(\mathcal{X}^{i})-\mu(\boldsymbol{\mathcal{X}}))^2}$.
where $p_{\boldsymbol{\phi}}(\mathcal{X}^{i}| \mathcal{S})$ is the probability of the policy model $\boldsymbol{\phi}$ constructing the solution $\mathcal{X}^{i}$ under the condition of the regional reconstruction problem $\mathcal{S}$. We use the same coordinate transformation in subsequence reconstruction, and the new tour replaces the original one if its cost is lower.

\subsection{Linear Attention for TSP}
% The constructive model $\boldsymbol{\theta}$, subsequence reconstruction model $\boldsymbol{\psi}$, and regional reconstruction model $\boldsymbol{\phi}$ all adopt the self-attention mechanism \cite{vaswani2017attention} in their network architectures. 
% The computational complexity of these models is $O(n^2)$ with respect to the number of nodes $n$. 
% Since the constructive model $\boldsymbol{\theta}$ takes the complete TSP graph as input, using a vanilla self-attention is not scalable.
Using the vanilla attention mechanism \cite{vaswani2017attention} in the constructive model $\boldsymbol{\theta}$ is not feasible for large-scale problems. We argue that pairwise attention calculations between nodes may be unnecessary in TSP, and therefore, we propose an attention mechanism with linear computational complexity to build a lightweight network architecture. 

Our attention mechanism aggregates the global features of the graph into representative nodes, and then broadcasts them to all nodes as shown in Figure ?. 
The representative nodes consist of anchor nodes, starting node, and destination node.
Define each node $v \in \mathcal{V}$ to be assigned a node coordinate $\mathbf{c}_v \in [0,1]^2$. 
% Define the $x$ and $y$ coordinates within $[0,1]$ for $\forall v \in \mathcal{V}$. 
% Let $\mathbb{R}_{\text{xy}} = [0, 1]$ denote the coordinate space for nodes, where both the $x$ and $y$ coordinates of any node lie within the interval $[0, 1]$. 
We set 
% $n_{\text{a}} \times n_{\text{a}}$ 
anchor nodes $\mathcal{A}$ in a grid-like, evenly spaced manner within the coordinate space. 
To distinguish from anchor nodes $\mathcal{A}$, we refer to nodes $\mathcal{V}$ as city nodes. 
According to the learning to construct partial solutions scheme \cite{luo2023neural}, model $\boldsymbol{\theta}$ uses the last and first nodes of the current route as the starting and destination nodes for the route to be constructed. 
Next, we will introduce our encoder and decoder. 

\textbf{Encoder}: For each city node $v \in \mathcal{V}$ and anchor node $a \in \mathcal{A}$, the encoder uses two distinct linear projections, to transform their coordinates ${\mathbf{c}}_{v},{\mathbf{c}}_{a} \in [0,1]^{2}$ into embeddings $\mathbf{h}_{v},\mathbf{h}_{a} \in \mathbb{R}^d$. 

\textbf{Decoder}: 
We feed anchor nodes $\mathcal{A}$ and a subset of city nodes $\mathcal{V}^{\prime} \subseteq \mathcal{V}$ into the decoder. The city nodes $\mathcal{V}^{\prime}$ include both the available nodes, the starting node, and the destination node. The number of available nodes is denoted as $\tilde{n}$ and the number of anchor nodes is $|\mathcal{A}|=A$. 
The decoder consists of $L$ linear attention decoding layers, indexed by $l$. 
Each decoding layer consists of two phases: aggregation and broadcasting. 
In the aggregation phase, we use the embeddings of the representative nodes as query $\mathbf{Q}_{}^{l} \in \mathbb{R}^{(A+2) \times d}$, and use the embeddings of the city nodes $\mathcal{V}^{\prime}$ as key $\mathbf{K}^{l} \in \mathbb{R}^{(\tilde{n}+2) \times d}$ and values $\mathbf{V}^{l} \in \mathbb{R}^{(\tilde{n}+2) \times d}$, respectively. The aggregation is calculated by: 
\begin{equation}
{\tilde{H}^{l}=\text{softmax}\left(\frac{\mathbf{Q}_{}^{l} W_{Q}^{l}(\mathbf{K}_{}^{l} W_{K}^{l})}{\sqrt{d}}\right)\mathbf{V}^{l}W_{V}^{l}},
\label{eq:Aggregation}
\end{equation}
where $\tilde{H}^{l} \in \mathbb{R}^{(A+2) \times d}$ represents the aggregated embeddings of representative nodes, $W_{Q}^{l}$, $W_{K}^{l}$ and $W_{V}^{l}$ are learning matrices. In the broadcasting phase, we concatenate the embeddings of the anchor nodes $\mathcal{A}$ and the city nodes $\mathcal{V}^{\prime}$ as query $\hat{\mathbf{Q}}_{}^{l} \in \mathbb{R}^{(A+\tilde{n}) \times d}$, and use the aggregated embeddings $\tilde{H}^{l} \in \mathbb{R}^{(A+2) \times d}$ as key and value. The broadcasting is calculated as: 
\begin{equation}
{\hat{H}^{l}=\text{softmax}\left(\frac{\hat{\mathbf{Q}}_{}^{l} \hat{W}_{Q}^{l}(\tilde{H}^{l} \hat{W}_{K}^{l})}{\sqrt{d}}\right)\tilde{H}^{l}\hat{W}_{V}^{l}},
\label{eq:Broadcasting}
\end{equation}
where $\hat{H}^{l} \in \mathbb{R}^{(A+\tilde{n}) \times d}$ represents the broadcasted embeddings of anchor nodes and city nodes, $\hat{W}_{Q}^{l}$, $\hat{W}_{K}^{l}$ and $\hat{W}_{V}^{l}$ are learning matrices. 
We transform the broadcasted embeddings $\hat{H}^{l}$ into the output of this decoding layer $H^{l} \in \mathbb{R}^{(A+\tilde{n}) \times d}$, through the same feed-forward and residual calculations as in vanilla attention. Then, $H^{l}$ is reassigned to the anchor nodes $\mathcal{A}$ and the city nodes $\mathcal{V}^{\prime}$ as their updated embeddings. 

Based on Equations \eqref{eq:Aggregation} and \eqref{eq:Broadcasting}, 
each decoding layer performs $(2A+4)\tilde{n}+A^2+4A+4$ tensor multiplication between node embeddings during the aggregation and broadcasting phases.
Since $A$ is a constant, the computational complexity of our attention mechanism is ${O}(\tilde{n})$, which is linearly related to the number of available nodes $\tilde{n}$.
\begin{figure}[t]
\centering
\includegraphics[width=82mm]{Z_LinearAttention.pdf}
\caption{The illustration of the proposed of the linear attention architecture.}
\label{fig:LinearAttention}
\end{figure}
