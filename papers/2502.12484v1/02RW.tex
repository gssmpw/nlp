 In this section, we provide literature review from the following three perspectives on NCO Solvers. %, basing on supervised learning, reinforcement learning, and refinement search. 
 
%\subsection{Supervised Learning for NCO}
\subsection{SL-based NCO Solvers}
Supervised learning (SL)-based NCO solvers rely on large labeled datasets to train models.  
Early work in this domain was pioneered by \citet{vinyals2015pointer}, who proposed the Pointer Network.  Their approach used SL techniques to solve small-scale TSP problems, employing a recurrent neural network (RNN) with attention mechanisms to iteratively construct solutions. 
\citet{joshi2019efficient} advanced this by incorporating graph convolutional networks (GCNs) \cite{kipf2016semi} to predict the probability of each edge being part of the optimal solution. This helped the model better capture the underlying graph structure of the TSP. 

\citet{sun2023difusco} utilized an anisotropic graph neural network \cite{bresson2018experimental} to construct a diffusion model, which iteratively denoises the solution and predicts a heatmap to guide the Monte Carlo Tree Search (MCTS) algorithm. 
\citet{luo2023neural} introduced the Light
Encoder and Heavy Decoder (LEHD) model, which combines a lightweight encoder and a heavy decoder, making it particularly suitable for SL-based training. 
These SL-based methods have shown success for small-scale problems but still face challenges when scaling to larger TSP instances due to their reliance on large labeled datasets. 

%\subsection{Reinfoecement Learning for NCO}
\subsection{RL-based NCO Solvers}
Reinforcement learning (RL)-based NCO solvers, unlike SL-based methods, do not require labeled instances for training. 
Instead, they rely on reward signals to guide learning.  
\citet{kool2018attention} introduced a self-attention-based \cite{vaswani2017attention} NCO solver,  trained using RL, and demonstrated that it outperforms earlier SL-based methods in solving TSP instances. 
Building on this, \citet{kwon2020pomo} introduced the POMO solver, which generates multiple trajectories for a single TSP instance by starting from different nodes. 
During inference, POMO augments the input data with techniques like flipping and folding, enabling the model to produce diverse solutions and select the optimal one.  

\citet{jin2023pointerformer} improved POMO by incorporating a reversible residual network architecture to reduce memory consumption, enabling the model to scale to 500-node TSP instances with promising results. 
\citet{drakulic2024bq} proposed the BQ-NCO model, which reduces the state space of Markov Decision Processes (MDPs), thereby  improving RL efficiency. 

Despite these advancements, RL-based methods often struggle with issues such as getting stuck in local optima during training and the problem of sparse rewards \cite{bengio2021machine, min2024unsupervised}.
These challenges  hinder the ability of RL-based models to generalize to larger-scale problems. 

%\subsection{Improvement Approaches for NCO}
\subsection{Search-based NCO Solvers}
Search-based NCO solvers typically begin with an initial feasible solution and iteratively refine it to improve the outcome. Neural network models often play a direct or indirect role in guiding this refinement process. 
For instance, \citet{xin2021neurolkh} and \citet{zheng2023reinforced} integrated SL and RL into the classical heuristic Lin-Kernighan-Helsgaun (LKH) solver \cite{helsgaun2000effective,helsgaun2009general,helsgaun2017extension}, improving its efficiency by using learning-based methods to construct candidate node sets. 

Some search-based approaches leverage MCTS to enhance solution quality~\cite{fu2021generalize,qiu2022dimes,sun2023difusco,xia2024position}. These methods typically use GCNs to generate heatmaps that guide MCTS in finding better results. However, they are heavily reliant on MCTS, and constructing solutions greedily based solely on these heatmaps often leads to suboptimal performance. 
Additionally, MCTS is computationally expensive, making it difficult to scale for large TSP instances. 

Other search-based solvers use neural-based heuristics to directly refine solutions. 
For instance, \citet{luo2023neural} applied subsequence reconstruction on top of a constructive model to improve solutions. They also employed this approach in a self-improved learning solver \cite{luo2024self}, where the model is trained on a dataset with low-quality labels and then refines these labels using subsequence reconstruction. 
\citet{cheng2023select} and \citet{ye2024glop} generated initial solutions using random insertion and applied subsequence reconstruction via RL-based models to enhance them. 
However, subsequence reconstruction methods have limited ability to modify the global relationships between nodes, often leading to the solver getting stuck in local optima. This limitation is a significant challenge for search-based NCO solvers and motivates our proposed approach, which aims to improve solution quality by addressing global relationships and escaping local optima. 
% In future work, we will focus on enhancing the scalability of LocalEscaper and extending it to more combinatorial optimization problems.

% Cheng~\cite{cheng2023select} introduces a destroy-and-repair operator to escape local optima. The operator removes long-connection and random selected edges to make the fragment sizes more uniform. Then, they apply the Linâ€“Kernighan algorithm~\cite{helsgaun2000effective} repair the broken fragments to a complete solution. However, this method randomly breaks edges, often resulting in solutions that resemble the original, making it inefficient at both improvement and escaping local optima. 
% Luo et al. enhance solutions by applying subsequence reconstruction on top of end-to-end solution construction. 
