\subsection{Constructive NCO}

In earlier work, Vinyals et al. \cite{vinyals2015pointer} propose the Pointer Network and employed SL techniques to solve TSP problems on small-scale instances. They developed an RNN model based on attention mechanisms that iteratively constructs solutions to the problem.
Bello et al. \cite{bello2016neural} used a policy gradients-based RL approach to train a Pointer Network, achieving better results on TSP with 20 to 100 nodes compared to the same model trained with SL.
Nazari et al. \cite{nazari2018reinforcement} improve the Pointer Network and train it with a RL-based approach to solve vehicle routing problem (VRP).
All the studies based on Pointer Networks are limited to solving small-scale problems with no more than 100 nodes.

In recent years, the success of the Transformer model \cite{vaswani2017attention} has inspired the development of new NCO solvers utilizing this architecture.
Kool et al. \cite{kool2018attention} design an NCO solver using the Transformer model and train it with the REINFORCE approach. Their method outperforms methods based on the Pointer Network model.
Building on the work of Kool \cite{kool2018attention}, Kwon et al. \cite{kwon2020pomo} introduce the POMO solver. Their method generates multiple trajectory for the same instance by starting from different nodes. During inference, they augment the input data with techniques such as flipping and folding, enabling the model to simultaneously produce diverse solutions for a single instance. Ultimately, the POMO solver select the optimal solution among these as the final output. Their method demonstrate superior performance but remains limited to solving small-scale problems with up to 100 nodes.
Jin et al. \cite{jin2023pointerformer} improve the POMO solver using a reversible residual network architecture to reduce memory consumption. Experimental results showed that their method could scale to 500-node TSP instances and achieve promising results.

The RL-based methods mentioned above tend to get stuck in local optima during training, and due to the issue of sparse rewards \cite{bengio2021machine, min2024unsupervised,xia2024position}, they struggle to generalize to larger-scale problems. 
On the other hand, NCO models based on the transformer architecture typically employ a balanced encoder-decoder structure. Luo et al. \cite{luo2023neural} argue that the dynamic nature of the decoding process demands a more complex model. They introduce LEHD model with a light encoder and heavy decoder which is better suited for training with SL. Their approach successfully scaled to 1000-node TSP instances, achieving state-of-the-art results.
To address the issue of requiring high-quality labeled data for SL training, Luo et al. \cite{luo2024self} propose the Self-Improved Learning (SIL) method. This approach first generates low-quality solutions as initial labels using low-computation traditional methods, such as random insertion. After each epoch of training, the model iteratively refines these labels through local reconstruction technique, allowing the improved labels to better guide model training. In turn, the model trained with improved labels can further enhance the local reconstruction process. However, their method cannot alter the global order of nodes during the reconstruction process, leading to the issue of getting trapped in local optima. 
% Designing a more efficient reconstruction method that allows SIL to overcome local optima is significant.
Our reconstruction method integrates two optimization techniques: cross-edge elimination and regional cut-and-reconnect. These techniques not only effectively improve solution quality but also alter the global node order, helping to escape local optima.

\subsection{Improvement NCO}
These NCO solvers start with an initial feasible solution and iteratively refine it to achieve better outcomes. In these approaches, neural network models directly or indirectly contribute to the refinement process.
Chen et al. \cite{chen2019learning} propose the NeuRewriter solver, which employs the Advantage Actor-Critic reinforcement learning approach to train the model. They designed two operators, region-pick and rule-pick, which the model applies to improve solutions. 
Several other NCO solvers \cite{d2020learning,kim2021learning,hudson2021graph} leverage neural networks to guide optimization operators in iteratively refining solutions. 
Xin \cite{xin2021neurolkh} and Zheng \cite{zheng2023reinforced} integrate RL into the classical heuristic solver Lin-Kernighan-Helsgaun (LKH) \cite{helsgaun2000effective,helsgaun2009general}. By using reinforcement learning to construct candidate node sets for the LKH algorithm, they enhanced the LKH's efficiency.

Some studies directly use neural network models to refine solutions. Cheng \cite{cheng2023select} and Ye \cite{ye2024glop} generate initial solutions using Random Insertion (RI) and then decomposed them into several parts. Each part is treated as a subproblem, which is subsequently improved by the neural network model. Cheng \cite{cheng2023select} introduces a destroy-and-repair method to escape local optima. They design a destroy operator to remove long-connection edges and randomly remove other edges to make the fragment sizes more uniform. Then, they apply the Linâ€“Kernighan algorithm \cite{helsgaun2000effective} repair the broken fragments to a complete solution. However, this method randomly breaks edges, often resulting in solutions that resemble the original, making it inefficient at both improvement and escaping local optima.

Other approaches leverage Monte Carlo Tree Search (MCTS) to enhance solution quality \cite{fu2021generalize,qiu2022dimes,sun2023difusco}. These methods utilize Graph Convolutional Networks (GCNs) \cite{kipf2016semi} to generate heatmaps that guide the MCTS in finding improved results. However, they are heavily reliant on MCTS, as greedily constructing solutions based solely on the heatmaps leads to suboptimal performance. Additionally, MCTS is highly time-consuming, making it prohibitively costly when tackling large-scale problems.
