\label{08Tab_random}
\input{08Tab_random}
\subsection{Experimental Setup}
\textbf{Problem Settings}: We follow the standard data generation process from the previous work \cite{kool2018attention}. 
The training set consists of 8192 instances, each with 1000 nodes randomly sampled from a uniform distribution. 
% over the unit square. 
The test set includes synthetic data and real-world data TSPLIB \cite{reinelt1991tsplib}. 
The synthetic data contains instances of three problem sizes: 1000, 5000, and 10000 nodes, referred to as TSP-1000/5000/10000. 
The TSP-1000 and TSP-10000 test sets use the same instances as previous work \cite{fu2021generalize}, with 128 and 16 instances, respectively. The TSP-5000 test set is generated using the same standard, with 16 instances. 
For subsequence reconstruction problems, we set the length of each subsequence to $m=100$. For regional reconstruction problems, the coordinates of the center point $\boldsymbol{c}$ are sampled from the $[0,1]^2$ space, and the number of removed edges is set to $k=60$. 

\textbf{Model Settings}: The constructive model $\boldsymbol{\theta}$ has $L=6$ decoding layers with $A = 5 \times 5$ anchor nodes. The number of encoding layers for both the subsequence reconstruction model $\boldsymbol{\psi}$ and regional reconstruction model $\boldsymbol{\phi}$ is 6. 
We evaluate LocalEscaper in three modes: (1) \textbf{LocalEscaper greedy}: Using the constructive model to greedily construct the solution; (2) \textbf{LocalEscaper Rec100}: After greedy construction, refining the solution with 100 improvement iterations by our improver; (3) \textbf{LocalEscaper Rec500}: Similar to the previous mode, but with up to 500 improvement iterations.

\textbf{Training}: Inspired by curriculum learning \cite{wang2021survey}, we train the constructive model $\boldsymbol{\theta}$ by starting with shorter subsequences, sampling $|\boldsymbol{\tau}^{\prime}| \in [30,130]$ for each batch. After each epoch, the lower and upper bounds of the sampling range are incremented by 5, continuing until the upper bound reaches 1000 nodes. The optimizer is Adam \cite{kingma2014adam}, with an initial learning rate of 1e-4 and a decay rate 0.99 every 5 epochs. Both reconstruction models $\boldsymbol{\psi}$ and $\boldsymbol{\phi}$ are trained using the Adam optimizer with a learning rate of 1e-4. 

\textbf{Baselines}: We compare our method with (1) \textbf{Traditional solvers}: Concorde \cite{applegate2009certification} and LKH-3 \cite{helsgaun2017extension}; (2) \textbf{Constructive solver}s: POMO \cite{kwon2020pomo}, Pointerformer \cite{jin2023pointerformer}, H-TSP \cite{pan2023h}, INViT-3V\cite{fang2024invit} and LEHD greedy \cite{luo2023neural}; (3) \textbf{Search-based solvers}: Att-GCN \cite{fu2021generalize}, DIFUSCO \cite{sun2023difusco}, GLOP \cite{ye2024glop} and LEHD RRC100 \cite{luo2023neural}. 

\textbf{Metrics and Inference}: 
We present three metrics: average tour length (Obj.), average optimality gap (Gap) and run-time (Time). 
Since traditional solvers are run on a single CPU, their run-time should not be directly compared with methods running on a GPU. 
All neural solvers are run on an RTX 3090 GPU for inference. We set a maximum inference time limit of 6 hours for each problem size. Methods exceeding this time limit do not report results, and their performance is marked as 'N/A'. 


\input{08Tab_TSPLIB}
\begin{figure}[t]
\centering
\includegraphics[width=60mm]{Z_Decomposition.pdf}
\caption{Ablations of three steps of the improver
on TSP-1000.}
\label{fig:Decomposition}
\end{figure}

\subsection{Results and Analysis}
The main experimental results on the uniform distribution instances are shown in Table \ref{Table: TSP random}. 
We report the total time taken to solve all instances. 
In comparison with constructive solvers, LocalEscaper exhibits fast inference speed under greedy decoding and achieves the lowest gap across all three sizes of instances. Our constructive model $\boldsymbol{\theta}$ is based on LEHD, and it performs similarly to LEHD on TSP-1000. However, it shows better performance on instances with more than 1000 nodes. Due to LEHD is trained on small-scale instances with 100 nodes, while our model employs a lightweight linear attention mechanism that reduces computational overhead, enabling training on instances with up to 1000 nodes. As a result, it performs better  generalization on large-scale instances. 

In comparison with search-based solvers, LocalEscaper also achieves the lowest gap across all sizes of instances. LocalEscaper Rec100 achieves competitive solutions in a short time, while LocalEscaper Rec500 further improves the solution by spending more time, reducing the gap for TSP-1000 to below $1\%$, and the gaps for TSP-5000 and TSP-10000 to below $2\%$. 


The experimental results on TSPLIB are shown in Table \ref{Table: TSPLIB}. We categorize the instances into three groups based on their size: $1\sim1000$ nodes, $1000\sim5000$ nodes, and over 5000 nodes. For each group, we solve all instances sequentially and report the average run-time. 
We place greater emphasis on the gap, and therefore, we compare LocalEscaper with other search-based methods. LocalEscaper Rec500 achieves the lowest gap across all instance categories. LocalEscaper Rec100 not only outperforms Att-GCN, LEHD RRC100, and DIFUSCO in inference speed, but also achieves a similar or better gap compared to other methods. 



\input{08Tab_WSL_vs_SL}
% \input{08Tab_RR}
\input{08Tab_TSP-5W}

\subsection{Ablation Study} 
\textbf{Weakly-supervised Learning vs. Supervised Learning}: 
To explore the gap between weakly SL and SL for training the constructive model, we present a comparison in Table \ref{Table: weakly SL vs. SL}. 
Both methods are trained on the same dataset with 1000-node instances, and the labels for SL are generated by LKH-3.  
Although weakly SL slightly outperforms SL on TSP-1000, the difference is only $0.05\%$, indicating that their performance is essentially comparable. On instances with more than 1000 nodes, SL exhibits better generalization.

\textbf{The Effect Decomposition of Improver}: 
To investigate the impact of each component of the improver, we conducted an ablation study on the TSP-1000. 
Figure \ref{fig:Decomposition} illustrates the solution improvement process over 500 iterations, with one or two components of the improver removed. Removing regional reconstruction or both regional reconstruction and 2-opt has the most significant impact on performance, indicating that relying solely on subsequence reconstruction quickly converges to relatively low-quality local optima. 

% \textbf{The impact of the scale of regional reconstruction}: 
% We conducted a comparison of regional reconstruction at different scales in LocalEscaper Rec500. We set the number of removed edges $k=30$ and $k=90$, and retrained two regional reconstruction models. 
% Table \ref{Table: Regional_Reconstruction_Scale} shows the impact across the three regional reconstruction scales.
% \textbf{The Impact of the Number of Anchor Nodes}: 




\subsection{Performance on Larger Scale Instances} 
Table \ref{Table: TSP-5W} presents the evaluation results of LocalEscaper on TSP-20000 and TSP-50000, each with 16 instances. We removed the 2-opt step from the improver in TSP50000 to adapt to the scale of these instances. The experimental results show that the generalization of the constructive model decreases on TSP-50000, but the improver still efficiently enhances the solution quality. 


