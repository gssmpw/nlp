
The Traveling Salesman Problem (TSP) is a classic combinatorial optimization (CO) problem with widespread applications in fields such as transportation \cite{wang2021deep}, logistics \cite{castaneda2022optimizing}, and circuit design \cite{alkaya2013application}. Due to its NP-hard nature, finding the optimal solution for large-scale TSP instances is challenging. Over the past few decades, scholars have focused on employing mathematical programming and heuristic methods to find locally optimal solutions \cite{applegate2009certification,lin1973effective,helsgaun2000effective}. These methods are time-consuming and therefore unsuitable for practical applications.
% While these methods cannot achieve a global optimum, they are still highly significant for practical applications. Finding solutions of acceptable quality within a reasonable timeframe remains crucial. However, these methods inevitably require substantial computational resources and time when scaled to large-scale problems.

Recently, neural combinatorial optimization (NCO) methods have demonstrated significant potential in solving TSP  \cite{kwon2020pomo,jin2023pointerformer,drakulic2024bq}. 
% NCO solvers leverage deep neural networks to automatically learn strategies for combinatorial optimization problems. After training, they can generate high-quality solutions with low computational cost and exhibit superior scalability to large instances. 
% NCO solvers are generally categorized into two types. The first type is constructive solvers, which generate a complete solution by incrementally adding nodes to an initially empty set through a neural network model. The second type is improvement solvers, which iteratively optimize a feasible solution by guiding local Operations Research (OR) operators with a neural network model.
% In terms of training, 
NCO solvers are typically trained using either supervised learning (SL) \cite{joshi2019efficient,hottung2021learning,luo2023neural} or reinforcement learning (RL) methods \cite{bello2016neural,kwon2020pomo,jin2023pointerformer}. 
    SL-based methods train neural network models using a large dataset of labeled instances. These labels represent high-quality solutions, typically generated by other methods, such as exact solvers or heuristics. Although SL-based methods are efficient in learning, their efficiency depends on large amounts of data. Moreover, generating high-quality labels typically involves significant computational cost, especially for large-scale problems. RL-based methods do not require labeled instances. These models learn better solving strategies through reward signals. However, due to the lack of optimal solution guidance, RL methods encounter severe sparse reward issues when solving large-scale TSP and are prone to getting stuck in local optima \cite{bengio2021machine, min2024unsupervised}, both of which lead to low learning efficiency. 

Whether based on SL or RL, NCO solvers face scalability limitations due to device memory constraints, with most solvers plateauing at 500 to 1,000 nodes. Furthermore, since most NCO solvers rely on Transformer neural network architectures \cite{vaswani2017attention}, their computational complexity scales quadratically with problem size. As a result, training NCO solvers to generate high-quality solutions for large-scale TSP is exceedingly challenging.
To obtain better solutions for large-scale TSP, many NCO solvers adopt divide-and-conquer techniques based on the Partial Optimization Metaheuristic Under Special Intensification Conditions (POPMUSIC) \cite{taillard2019popmusic} framework to enhance solution quality \cite{cheng2023select,luo2023neural,ye2024glop,zheng2024udc}.
These methods divide the tour of large-scale TSP into several non-overlapping subsequences and then reconstruct each subsequence independently. They transform the challenging task of solving large-scale problems into the relatively simpler task of solving smaller sub-problems. However, these techniques only reconstruct the visiting order of nodes within subsequences while keeping the starting and destination nodes fixed, without altering the global relationships between subsequences, thereby limiting their ability to escape local optima.
% Recently, a self-improved approach has emerged \cite{luo2024self}, offering the potential to address the need for high-quality label in supervised learning through reconstruction techniques. However, its reconstruction method also faces limitations as it struggles to escape local optima.
Therefore, based on the above, the research on NCO needs to address three main challenges: (1) Overcoming the limitations between SL and RL, (2) Reducing computational costs, and (3) Escaping local optima.

In this work, we propose LocalEscaper method to address these challenges. 
For Challenge (1), SL is efficient but relies on high-quality labels, while RL avoids this requirement but is less efficient. 
% To address these limitations, 
We propose a weakly supervised learning framework that begins SL training with low-quality labels and progressively refines them using RL approaches. This framework combines the efficiency of SL with the flexibility of RL, avoiding the need for high-quality labels and leveraging their complementary strengths.

For Challenge (2), some studies \cite{yang2023memory, luo2024self} argue that the pairwise attention computation among tokens in the Transformer-based NCO solvers is not always necessary, and efforts have been made to reduce computational complexity. We propose a lightweight neural network structure that aggregates node information in a TSP graph through virtual nodes, enabling both computational complexity and memory usage to scale linearly with problem size.

For Challenge (3), we consider that POPMUSIC-based methods are prone to local optima because they restrict changes in node order to nodes with close sequence distances, neglecting spatial relationships between nodes. This subsequence reconstruction approach makes it difficult to adjust relationships between nodes that are close in spatial distance but far in sequence distance. 
To address this, we propose a regional reconstruction approach, which focuses on densely breaking edges within a specific region of the solution and reconnecting them to obtain a better feasible solution. We implement the regional reconstruction and subsequence reconstruction approaches using RL to construct an improver. This improver efficiently enhances solution quality during both training and inference stages for our weakly supervised learning framework. 

Experimental results demonstrate that our LocalEscaper method achieves state-of-the-art performance among current NCO solvers on large-scale TSP instances ranging from 1,000 to 10,000. Our contributions can be summarized as follows:
\begin{itemize}
\item We propose a novel weakly supervised learning framework to train the neural network model for TSP. Our method does not require high-quality labeled datasets for training. It continuously refines the label quality throughout the training process.
\item We design a lightweight neural network architecture with linear complexity to scale up to large-scale problems. Our model significantly reduces memory usage and improves the forward propagation speed.
\item  We propose a regional reconstruction approach and construct an RL-based heuristics improver to enhance solution quality during both training and inference stages. Experimental results show that regional reconstruction is highly effective in escaping local optima caused by subsequence reconstruction.
\end{itemize}
