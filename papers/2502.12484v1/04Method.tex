\begin{figure}[t]
\centering
\includegraphics[width=82mm]{Z_Framework1.pdf}
\caption{Overview of the proposed framework, which includes a dataset and three tasks that can be executed in parallel. 
%In this paper, we adopt a sequential execution approach for Task 1 and Task 2.
}
\label{fig:Framework}
\end{figure}

In this section, we introduce a weakly-supervised learning framework to address the challenges of large-scale TSP discussed in the Introduction. An overview of the proposed framework is presented in Figure \ref{fig:Framework}. Our framework is designed to operate on a dataset consisting of three main tasks: 
Task 1 for dataset improvement, Task 2 for training reconstruction models, and Task 3 for training the constructive model. 
These tasks can be executed in parallel. For practical purposes, we adopt a sequential execution strategy where Task 1 and Task 2 alternate, while Task 3 runs continuously. 

\subsection{Weakly-supervised Learning Framework}
We define the dataset as $\mathcal{D}^t(\boldsymbol{\mathcal{G}}, \boldsymbol{\mathcal{T}}^t)$, where 
% $\boldsymbol{\mathcal{G}}=\{\mathcal{G}_1,\mathcal{G}_2,\dots,\mathcal{G}_N\}$ 
$\boldsymbol{\mathcal{G}}$ 
represents the set of TSP graphs, 
% $\boldsymbol{\mathcal{T}}^t = \{ \boldsymbol{\tau}_1^t, \boldsymbol{\tau}_2^t, \dots, \boldsymbol{\tau}_N^t\}$ 
$\boldsymbol{\mathcal{T}}^t$ 
corresponds to the set of tour labels, with  $t \in \{0,1,\dots,T\}$ indicating the iteration counter. 
The initial label set, $\boldsymbol{\mathcal{T}}^0$, is generated using a random insertion algorithm based on the graph set $\boldsymbol{\mathcal{G}}$.

We employ a weakly-supervised learning approach, where the dataset $\mathcal{D}$ is used to train a constructive model parameterized by $\boldsymbol{\theta}$, following the "learning to construct partial solutions" paradigm proposed in \citet{luo2023neural}.
This model generates TSP solutions by sequentially selecting the next node to visit. 

At each training step, we sample a subsequence $\boldsymbol{\tau}^{\prime}$ from a given tour $\boldsymbol{\tau}$, where $\boldsymbol{\tau}^\prime$ provides a binary indication $b_i$ for each available node $v_i$ (1 for selection, 0 for non-selection). 
The model then predicts the selection probability $p_i$ for each available node $v_i$. 
The objective function is the cross-entropy loss:
% \begin{equation}
${\mathcal{L}(\boldsymbol{\theta}) = -\sum_{i=1}^{u-1}{b_i\text{log}(p_i)}}$,
% \label{eq:Balance}
% \end{equation}
where $u$ is the number of available nodes in the subsequence. 

 
To improve the label quality of the dataset, we introduce an improver $\mathcal{I}$, which is independent of the model $\boldsymbol{\theta}$, 
to enhance the label quality of the dataset $\mathcal{D}$. 
The label improvement process is defined as $\mathcal{D}^{t+1} = \mathcal{I}(\mathcal{D}^t)$, and we impose the constraint that the total loss for the updated labels satisfies: 
$L_{\text{total}}(\boldsymbol{\tau}^{t+1}) \leq L_{\text{total}}(\boldsymbol{\tau}^{t})$, $\forall \boldsymbol{\tau}^{t} \in \boldsymbol{\mathcal{T}}^{t}$. 
% \begin{equation}
% L_{\text{total}}(\boldsymbol{\tau}^{t+1}) \leq L_{\text{total}}(\boldsymbol{\tau}^{t}) ~\text{for all}~ \boldsymbol{\tau}^{t} \in \boldsymbol{\mathcal{T}}^{t}.
% \end{equation}

After each improvement, we retrain the model $\boldsymbol{\theta}$ using supervised learning on the updated dataset $\mathcal{D}^{t+1}$.
Simultaneously, we train the improver $\mathcal{I}$ using reinforcement learning. 
% Thus, our weakly supervised learning framework consists of two main processes: 1) training the constructive model $\boldsymbol{\theta}$, and 2)improving the accuracy of the dataset $\mathcal{D}$ and training the improver $\mathcal{I}$. 
% Since the improver $\mathcal{I}$ is independent of the model $\boldsymbol{\theta}$ in terms of parameters, these two processes can be performed in parallel. 

Next, we provide a detailed description of the improver.

\subsection{Improver for Escaping Local Optima}
A crucial component of our framework is the improver $\mathcal{I}$, designed to help escape local optima by refining solutions through regional reconstruction. 
The improver operates both during training (to improve the label quality of the dataset) and during inference (to enhance the quality of the generated solutions). %Its primary role is to address escape local optima encountered in subsequence reconstruction by leveraging regional reconstruction.

The improver follows a three-step process: Subsequence Reconstruction, 2-opt, and Regional Reconstruction.

\textbf{Subsequence Reconstruction}: Building upon the approach in \citet{ye2024glop}, we randomly decompose a TSP solution of length $n$ into $\lfloor \frac{n}{m} \rfloor$ subsequences, each of length $m$. 
To improve the homogeneity of the input, we apply a coordinate transformation to the $x$ and $y$ coordinates of the nodes: 
$\boldsymbol{x} \leftarrow \frac{\boldsymbol{x}-\bar{x}}{\sigma},  ~~\boldsymbol{y}\leftarrow\frac{\boldsymbol{y}-\bar{y}}{\sigma}, $
where $\sigma$ is the maximum absolute value among the $x$ and $y$ coordinates. 
Each subsequence is then reconstructed using a neural model parameterized by $\boldsymbol{\psi}$. 
The model replaces the original subsequence $\boldsymbol{\tau}^{\prime}$ with a newly reconstructed subsequence \textbf{$\boldsymbol{\tau}^{\prime\prime}$},   if $L_{\text{sub}}(\boldsymbol{\tau}^{\prime\prime}) \leq L_{\text{sub}}(\boldsymbol{\tau}^{\prime})$.

Our model follows the multiple trajectories approach of POMO~\cite{kwon2020pomo}, with the distinction that we employ a non-autoregressive neural model. Specifically, the encoder generates a heatmap 
% $\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}}$ 
of edge selection probabilities in a single pass, and the decoder constructs new subsequences iteratively based on 
% $\boldsymbol{\mathcal{H}}_{\boldsymbol{\psi}}$. 
the heatmap. 
Non-autoregressive models offer fast construction speeds. When the subsequence length $m$ is small (e.g., less than 100), the drawback of such coarse construction~\cite{ye2024glop} is minimal. %much less pronounced.

\textbf{2-opt}: We apply the classical 2-opt algorithm~\cite{lin1973effective}, which iteratively checks all possible  edge swaps to improve the solution. 
If a swap leads to a shorter solution, it is executed, and the process continues  until no further improvements are possible. This helps fine-tune the solution by removing suboptimal edges.

\textbf{Regional Reconstruction}: 
In this step, we remove a set of edges $\mathcal{E}_{\boldsymbol{\tau}}^{\prime}$ from the original TSP solution $\mathcal{E}_{\boldsymbol{\tau}}$.  Let $|\mathcal{E}_{\boldsymbol{\tau}}^{\prime}|=k$, the remaining edges $\mathcal{E}_{\boldsymbol{\tau}} \setminus \mathcal{E}_{\boldsymbol{\tau}}^{\prime}$ can compose $k$ subsequences 
$\mathcal{S}=\{ \boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \dots, \boldsymbol{\tau}_{k}^{\prime} \}$% (Insensitive to the direction of each subsequence in $\mathcal{S}$).  
, where each subsequence is independent of the direction of traversal.

To ensure that the reconstructed solution is a valid Hamiltonian circuit, we transform the reconstruction process into a permutation problem. We permute the subsequences in $\mathcal{S}$ to form a subsequence list $\mathcal{X}=(\boldsymbol{\eta}_{1}, \boldsymbol{\eta}_{2}, \dots, \boldsymbol{\eta}_{k})$. 
During the permutation, we have the option to reverse the direction of any subsequence.  
For instance, for a subsequence $\boldsymbol{\tau}^{\prime} = (\tau_1^{\prime}, \tau_2^{\prime}, \dots, \tau_{m}^{\prime})$, its reverse is denoted as $\neg \boldsymbol{\tau}^{\prime} = (\tau_{m}^{\prime}, \tau_{m-1}^{\prime}, \dots, \tau_{1}^{\prime})$. 

Once the subsequences are permuted and reversed as necessary, they are concatenated to form a new Hamiltonian circuit $\boldsymbol{\tau}^{\text{new}}$. This reconstructed solution provides a refined path that potentially escapes local optima by incorporating a broader set of possible configurations.

This permutation process can be modeled as a Markov Decision Process (MDP), detailed in the next subsection. 

\subsection{The Permutation as an MDP}
We model the permutation task as a Markov Decision Process (MDP) to optimally select subsequences and reconstruct the final solution. 
In this framework, %the subsequence list $\mathcal{X}$ is initialized as an empty list, and the set of available subsequences is initialized as $\mathcal{C}=\{\boldsymbol{\tau}_{1}^{\prime}, \neg\boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \neg\boldsymbol{\tau}_{2}^{\prime},\dots,\boldsymbol{\tau}_{k}^{\prime}, \neg\boldsymbol{\tau}_{k}^{\prime}\}$. Then, we gradually append subsequence from $\mathcal{C}$ to $\mathcal{X}$.
the subsequence list $\mathcal{X}$ is gradually built by appending subsequences from a set of available subsequences $\mathcal{C}$.

The MDP  formulation consists of the following elements: states, actions, transition, and rewards. 

\textit{States}: The state is characterized by the current subsequence list $\mathcal{X}$ and the set of available subsequences $\mathcal{C}$.
Initially, $\mathcal{X}$ is an empty list, and $\mathcal{C}$ contains the subsequences as well as their reverses:
$\mathcal{C}=\{\boldsymbol{\tau}_{1}^{\prime}, \neg\boldsymbol{\tau}_{1}^{\prime}, \boldsymbol{\tau}_{2}^{\prime}, \neg\boldsymbol{\tau}_{2}^{\prime},\dots,\boldsymbol{\tau}_{k}^{\prime}, \neg\boldsymbol{\tau}_{k}^{\prime}\}$. 
At each step, subsequences are selected from $\mathcal{C}$ and added to $\mathcal{X}$.

\textit{Actions}: An action is the selection of a subsequence $\boldsymbol{\tau}_{\text{act}}$ from the available set $\mathcal{C}$ to be appended to the subsequence list $\mathcal{X}$.

\textit{Transition}: After performing an action, the subsequence $\boldsymbol{\tau}_{\text{act}}$ is appended to $\mathcal{X}$, and both $\boldsymbol{\tau}_{\text{act}}$ and its reverse $\neg\boldsymbol{\tau}_{\text{act}}^{\prime}$ are removed from $\mathcal{C}$.
The process continues until all subsequences are appended, and  $\mathcal{C}$ becomes empty, signaling the completion of the reconstruction. 

\textit{Rewards}: The reward function is designed to evaluate the quality of the reconstructed solution. 
Once the reconstruction is complete, the total reward is defined as  the negative of the solution's cost, which is the sum of the edge costs between adjacent subsequences: 
\begin{equation}
{R(\mathcal{X})=-\text{cost}(\eta_{1}^{\text{first}}, \eta_{k}^{\text{last}})-\sum\limits_{i=1}^{k-1}\text{cost}(\eta_{i}^{\text{last}}, \eta_{i+1}^{\text{first}})},
\label{eq:Reward}
\end{equation}
where $\eta_{i}^{\text{first}}$ and $\eta_{i}^{\text{last}}$ represent the first and last nodes of subsequence $\boldsymbol{\eta}_i$, respectively. 
The goal is to minimize this total cost by carefully selecting subsequences during the reconstruction process. 

%We follow the multiple trajectory approach~\cite{kwon2020pomo} to 
We employ the REINFORCE algorithm~\cite{williams1992simple} to train a non-autoregressive policy model $\boldsymbol{\phi}$ to select subsequences during regional reconstruction.  
The policy model is trained by sampling multiple trajectory lists $\boldsymbol{\mathcal{X}}=\{\mathcal{X}^{1}, \mathcal{X}^{2}, \dots, \mathcal{X}^{N} \}$, where each $\mathcal{X}^i$ represents a possible subsequence list generated by the policy. 

The gradient of the policy network is approximated as: 
\begin{equation}
\begin{aligned}
\triangledown_{\boldsymbol{\phi}}J(\boldsymbol{\phi}) &\approx \frac{1}{N}\sum\limits_{i=1}^{N} \frac{R(\mathcal{X}^{i}) - \mu(\boldsymbol{\mathcal{X}})}{\delta(\boldsymbol{\mathcal{X}})} \triangledown_{\boldsymbol{\phi}} \text{log} ~ p_{\boldsymbol{\phi}}(\mathcal{X}^{i}| \mathcal{S}),
\\
\mu(\boldsymbol{\mathcal{X}}) &= \frac{1}{N}\sum\limits_{i=1}^{N} R(\mathcal{X}^{i}), \\
\delta(\boldsymbol{\mathcal{X}}) &= \sqrt{\frac{1}{N}\sum\limits_{i=1}^{N} \left(R(\mathcal{X}^{i})-\mu(\boldsymbol{\mathcal{X}})\right)^2} . 
\label{eq:Regional Reconstruction Gradient}
\end{aligned}
\end{equation}
Here, $p_{\boldsymbol{\phi}}(\mathcal{X}^{i}| \mathcal{S})$ is the probability of the policy model $\boldsymbol{\phi}$ constructing the solution $\mathcal{X}^{i}$ given the regional reconstruction problem $\mathcal{S}$. 

%We use the same coordinate transformation in subsequence reconstruction, and the new tour replaces the original one if its cost is lower.
The training process aims to optimize the policy so that subsequences are selected in a way that minimizes the total cost of the final reconstructed solution. During this process, the same coordinate transformation used in subsequence reconstruction is applied, and the new solution replaces the original one if its cost is lower.

\subsection{Linear Attention for TSP}
The vanilla attention mechanism~\cite{vaswani2017attention} is computationally expensive, especially for large-scale problems like the TSP, where pairwise attention calculations between nodes can become prohibitively costly. 
%We argue that pairwise attention calculations between nodes may be unnecessary in TSP, and therefore we propose an attention mechanism with linear computational complexity to build a lightweight network architecture. 
To address this issue, we propose a linear attention mechanism that significantly reduces computational complexity while still effectively capturing the relevant information for TSP. 

Our attention mechanism operates by aggregating the global features of the graph into representative nodes,  which are then broadcast to all other nodes.  This method reduces the need for pairwise attention between all nodes, as illustrated in Figure \ref{fig:LinearAttention}. 
The representative nodes consist of anchor nodes, the starting node, and the destination node. 

Each node $v \in \mathcal{V}$ (city node) is assigned a coordinate $\mathbf{c}_v \in [0,1]^2$, representing its position in 2D space. 
Anchor nodes $\mathcal{A}$ are placed in a grid-like, evenly spaced manner within this coordinate space. 
%To distinguish from the anchor nodes $\mathcal{A}$, we refer to the nodes $\mathcal{V}$ as the city nodes. 
 Following the "learning to construct partial solutions" paradigm~\cite{luo2023neural}, the constructive model $\boldsymbol{\theta}$ uses the last and first nodes of the current route as the starting and destination nodes for constructing the next route. 
 
We now describe the encoder and decoder components of the network.

\textbf{Encoder}: The encoder transforms the coordinates of both city nodes and anchor nodes into embeddings. 
For each city node $v \in \mathcal{V}$ and anchor node $a \in \mathcal{A}$, we use distinct 2-layer perceptrons to map their coordinates ${\mathbf{c}}_{v},{\mathbf{c}}_{a} \in [0,1]^{2}$ into embeddings $\mathbf{h}_{v},\mathbf{h}_{a} \in \mathbb{R}^d$, where 
$d$ is the embedding dimension. 

\textbf{Decoder}: 
The decoder takes in the anchor nodes $\mathcal{A}$ and a subset of the city nodes $\mathcal{V}^{\prime} \subseteq \mathcal{V}$, which includes both available nodes as well as the starting and destination nodes. 
The number of available nodes is denoted as $\tilde{n}$ and the number of anchor nodes is $|\mathcal{A}|=A$. 

The decoder consists of $L$ linear attention decoding layers, indexed by $l$. 
Each decoding layer comprises two phases: aggregation and broadcasting.

\textit{Aggregation Phase}: In this phase, the embeddings of the representative nodes (anchor nodes, starting nodes, and destination nodes) serve as queries, and the embeddings of the city nodes $\mathcal{V}^{\prime}$ serve as keys and values. The aggregation is computed as: 
\begin{equation}
{\tilde{H}^{l}=\text{softmax}\left(\frac{\mathbf{Q}_{}^{l} W_{Q}^{l}(\mathbf{K}_{}^{l} W_{K}^{l})}{\sqrt{d}}\right)\mathbf{V}^{l}W_{V}^{l}},
\label{eq:Aggregation}
\end{equation}
where $W_{Q}^{l}$, $W_{K}^{l}$ and $W_{V}^{l}$ are learnable  weight matrices. The output $\tilde{H}^{l} \in \mathbb{R}^{(A+2) \times d}$ represents the aggregated embeddings of the representative nodes. 

\textit{Broadcasting Phase}: In the broadcasting phase, we concatenate the embeddings of the anchor nodes $\mathcal{A}$ and the city nodes $\mathcal{V}^{\prime}$ as the query $\hat{\mathbf{Q}}_{}^{l} \in \mathbb{R}^{(A+\tilde{n}+2) \times d}$. 
The aggregated embeddings $\tilde{H}^{l} \in \mathbb{R}^{(A+2) \times d}$serve as both the key and value. The broadcasting is calculated as: 
\begin{equation}
{\hat{H}^{l} = \text{softmax}\left(\frac{\hat{\mathbf{Q}}_{}^{l} \hat{W}_{Q}^{l}(\tilde{H}^{l} \hat{W}_{K}^{l})}{\sqrt{d}}\right)\tilde{H}^{l}\hat{W}_{V}^{l}},
\label{eq:Broadcasting}
\end{equation}
where $\hat{W}_{V}^{l}$ are learnable weight matrices. 
The result, $\hat{H}^{l} \in \mathbb{R}^{(A+\tilde{n}+2) \times d}$,  represents the broadcasted embeddings of both anchor nodes and city nodes.  

After broadcasting, we transform $\hat{H}^{l}$ into the output $H^{l} \in \mathbb{R}^{(A+\tilde{n}+2) \times d}$ through the same feed-forward and residual connections used in vanilla attention. These embeddings are then updated for both the anchor nodes $\mathcal{A}$ and the city nodes $\mathcal{V}^{\prime}$ as their updated embeddings. 

The total computational cost for each decoding layer, as derived from the aggregation and broadcasting phases, involves tensor multiplications on the order of $(2A+4)\tilde{n}+A^2+6A+8$  where $A$ is constant. 
Thus, the overall computational complexity of our attention mechanism is ${O}(\tilde{n})$, which is linearly dependent on the number of available nodes $\tilde{n}$.

\begin{figure}[t]
\centering
\includegraphics[width=82mm]{Z_LinearAttention.pdf}
% \vspace{-2em}
\caption{Illustration of the proposed linear attention architecture.}
\label{fig:LinearAttention}
\end{figure}
