
The Traveling Salesman Problem (TSP) is a classic combinatorial optimization (CO) problem with widespread applications in fields such as transportation~\cite{wang2021deep}, logistics~\cite{castaneda2022optimizing}, and circuit design~\cite{alkaya2013application}. Due to its NP-hard nature, finding the optimal solution for large-scale TSP instances remains a significant challenge. Over the past few decades, researchers have focused on mathematical programming and heuristic methods to find locally optimal solutions~\cite{applegate2009certification,lin1973effective,helsgaun2000effective}. However, these methods tend to be time-consuming, making them unsuitable for real-world, large-scale applications.


Recently, neural combinatorial optimization (NCO) methods have shown considerable promise in solving TSP~\cite{kwon2020pomo,jin2023pointerformer,drakulic2024bq}. 
Current NCO solvers typically rely on either supervised learning (SL)~\cite{joshi2019efficient,hottung2021learning,luo2023neural} or reinforcement learning (RL)~\cite{bello2016neural,kwon2020pomo,jin2023pointerformer}.  
%SL-based methods train neural network models using a large dataset of labeled instances. These labels represent high-quality solutions, typically generated by other methods, such as exact solvers or heuristics. 
SL-based methods require large datasets of labeled instances, where labels represent high-quality solutions typically obtained from exact solvers or heuristics. 
%Although SL-based methods are efficient in learning, their efficiency depends on large amounts of data. Moreover, generating high-quality labels typically involves significant computational cost, especially for large-scale instances. 
While SL-based methods are efficient in training, they depend on large amounts of labeled data, and generating high-quality labels for large-scale instances is computationally expensive.
On the other hand, RL-based methods do not rely on labeled data, learning to generate solutions through reward signals. However, 
%due to the lack of optimal solution guidance, RL methods encounter severe sparse reward issues when solving large-scale TSP and are prone to getting stuck in local optima
RL methods face issues such as sparse rewards and a tendency to get stuck in local optima~\cite{bengio2021machine, min2024unsupervised}, which can reduce learning efficiency.

%Whether based on SL or RL, NCO solvers face the scalability challenge due to device memory constraints, with most solvers plateauing at 500 to 1,000 scales.
In addition to these challenges, scalability is a major concern for both SL- and RL-based NCO solvers, as most solvers plateau at problem sizes of around 500 to 1,000 nodes due to memory constraints. 
Moreover, most NCO solvers employ Transformer network architectures~\cite{vaswani2017attention}, which exhibit quadratic computational complexity as the problem scale increases. This makes it particularly difficult to train models capable of generating high-quality solutions for large-scale TSP.

To address the scalability issue, a number of NCO solvers have adopted divide-and-conquer strategies based on the Partial Optimization Metaheuristic Under Special Intensification Conditions (POPMUSIC) framework~\cite{taillard2019popmusic}, improving the solution quality by dividing the TSP tour into smaller, non-overlapping subsequences and reconstructing each subsequence independently~\cite{cheng2023select,luo2023neural,ye2024glop,zheng2024udc}. 
%They transform the challenging task of solving large-scale problems into the relatively simpler task of solving smaller sub-problems. However, these techniques only reconstruct the visiting order of nodes within subsequences while keeping the starting and destination nodes fixed, without altering the global relationships between subsequences, thereby limiting their ability to escape local optima.
While these methods perform local search using subsequence reconstruction and reduce the complexity of solving large-scale problems, they are limited in their ability to escape local optima, as they only reconstruct the visiting order within subsequences while leaving the global relationships between subsequences unchanged. 

Given these challenges, research on NCO methods for TSP faces three key obstacles: (1) Overcoming the limitations of SL and RL approaches, (2) reducing computational costs, and (3) escaping  local optima.
To address these challenges, we introduce LocalEscaper, a novel model designed to enhance TSP solving. 

For the first challenge, %SL is efficient but relies on high-quality labels, while RL avoids this requirement but is less efficient. 
%We propose a weakly-supervised learning framework that begins SL training with low-quality labels and progressively refines them using RL approaches. This framework combines the efficiency of SL with the flexibility of RL, avoiding the need for high-quality labels and leveraging their complementary strengths.
we propose a weakly-supervised learning framework that combines SL and RL to leverage the strengths of both. Our method begins with SL using low-quality labels and progressively refines them through RL, enabling effective training without requiring high-quality labeled data.

For the second challenge, some researchers~\cite{yang2023memory, luo2024self} argue that pairwise attention computation among tokens in Transformer-based NCO solvers is not always necessary. %, and some efforts have been made to reduce computational complexity. 
We address the computational burden by designing a lightweight neural network architecture with linear complexity. %This approach aggregates node information in a TSP graph through virtual nodes, enabling both computational complexity and memory usage to scale linearly with the problem size.
This approach reduces both memory usage and computational overhead, enabling scalability to large-scale TSP instances.

For the third challenge, %we consider that POPMUSIC-based methods are prone to local optima because they restrict changes in node order to nodes with close sequence distances, neglecting spatial relationships between nodes. This subsequence reconstruction approach makes it difficult to adjust relationships between nodes that are close in spatial distance but far in sequence distance. To address this, we propose a regional reconstruction approach, which focuses on densely breaking edges within a specific region of the solution and reconnecting them to obtain a better feasible solution. We implement the regional reconstruction and subsequence reconstruction approaches using RL to construct an improver. This improver efficiently enhances solution quality during both training and inference stages for our weakly-supervised learning framework. 
we introduce a regional reconstruction strategy to overcome the limitations of existing divide-and-conquer approaches. Unlike traditional methods, which only adjust the visiting order within subsequences, our regional reconstruction method focuses on breaking and reconnecting edges within specific regions of the solution. This approach, coupled with an RL-based heuristic improver, enhances the solution quality by better escaping local optima during both training and inference stages.

%Experimental results demonstrate that LocalEscaper achieves state-of-the-art performance among existing NCO solvers on large-scale TSP instances ranging from 1,000 to 10,000 cities.
Experimental results demonstrate that LocalEscaper achieves state-of-the-art performance among existing NCO solvers on large-scale TSP instances ranging from 1,000 to 10,000 cities. In particular, LocalEscaper can handle large-scale instances with up to 50,000 cities quickly, while most existing NCO solvers either run out of memory or take too long.
The main contributions of our work are as follows: 
\begin{itemize}
\item %We propose a novel weakly-supervised learning framework to train the neural network model for TSP. Our method does not require high-quality labeled datasets for training. Instead, it continuously refines the label quality throughout the training process.
We propose a novel weakly-supervised learning framework that enables training without high-quality labeled datasets, progressively refining label quality during the training process.
\item We design a lightweight neural network architecture with linear complexity% to scale up to large-scale problems. Our model significantly reduces memory usage and improves the forward propagation speed.
, which scales efficiently to large-scale problems and significantly reduces memory usage and computation time.
\item  %We propose a regional reconstruction approach and construct an RL-based heuristic improver to enhance the solution quality during training and inference stages. Experimental results show that the regional reconstruction is highly effective in escaping local optima caused by subsequence reconstruction.
 We introduce a regional reconstruction approach, coupled with an RL-based improver, to enhance solution quality and effectively escape local optima during both training and inference stages. 
 \item  Experimental results demonstrate the superior performance of the proposed LocalEscaper over  existing NCO solvers on TSP instances ranging from 1,000 to 50,000 cities. 
\end{itemize}
