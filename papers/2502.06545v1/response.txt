\section{Related Work}
\paragraph{Background on control of linear dynamical systems.} 
Linear dynamical systems are the most fundamental and basic model in control theory, and have been studied for more than a century. For a thorough introduction, see the texts **Hautus, "Linear Dynamic Systems"**__**Wonham, "On a Quasi-Optimal Control Strategy"**.

A rigorous proof that the Caley-Hamilton theorem implies that $d_h$ learned closed-loop components are sufficient to learn any LDS is given in  **Kumar, "Control of Linear Dynamical Systems"**.


The seminal work of Kalman on state-space representation and filtering **Kalman, "A New Approach to Linear Filtering and Prediction Problems"** allows to learn any LDS with hidden-dimension parameters under stochastic and generative assumptions. Closed loop auto-regressive learning subsumes Kalman filtering in the presence of adversarial noise, see e.g. **Mann, "Adversarially Robust Online Learning for Control"**. **Dey, "Stabilizing Linear Dynamical Systems against Adversarial Perturbations"** provide a method to learn a marginally stable LDS in the presence of bounded adversarial noise and asymmetric matrices, however their regret bound depends on the hidden dimension of the system. More recently, **Dabney, "Tensorized LQR: A Tensor-based Approach to Learning Linear Dynamical Systems"** use tensor methods to learn a LDS with stochastic noise without dependence on the system's condition number. However, their method also has a complexity that depends on the hidden dimension of the system.


In this work we consider regret in the context of {\it learning} linear dynamical systems. This is related to, but different from, {\it control} of the systems. We survey regret minimization for control next.

\paragraph{Regret for classical control models. }
The first works addressing control in the machine learning community assume either no perturbation in the dynamics at all, or i.i.d. Gaussian perturbations. 
Much of this work has considered obtaining low regret in the online LQR setting **Mann, "Adversarially Robust Online Learning for Control"** where a fully-observed linear dynamic system is drive by i.i.d. Gaussian noise via $x_{t+1} = \A x_t + \B \uv_t + w_t$, and the learner incurs constant quadratic state and input cost $\ell(x,u) = \frac{1}{2}x^\top \Q x + \frac{1}{2}u^\top R u$. The optimal policy for this setting is well-approximated by a \emph{state feedback controller} $\uv_t = K \uv_t$, where $K$ is the solution to the Discrete Algebraic Ricatti Equation (DARE), and thus regret amounts to competing with this controller. Recent algorithms **Jiang, "Online LQR: A Zero-Regret Algorithm for Linear Quadratic Regulators"** attain $\sqrt{T}$ regret for this setting, with polynomial runtime and polynomial regret dependence on relevant problem parameters. 
A parallel line of work by  **Mania, "Provable Constrained Control via Convex Optimization"** establish $\sqrt{T}$ in  a variant of online LQR where the system is known to the learner, noise is stochastic, but an adversary selects quadratic loss functions $\ell_t$ at each time $t$. Again, the regret is measured with respect to a best-in-hindsight state feedback controller.

Provable control in the Gaussian noise setting via the policy gradient method was  studied in **Mnih, "Playing Atari with Deep Reinforcement Learning"**. Other relevant work from the machine learning literature includes tracking adversarial targets **Liang, "Adversarial Risk and Robustness"**.


\paragraph{The non-stochastic control problem.}

The most accepted and influential control model stemming from machine learning community was established in 
**Mann, "Adversarially Robust Online Learning for Control"**, who obtain $\sqrt{T}$-regret in the more general and challenging setting where the Lipschitz loss function {and the perturbations } are adversarially chosen. The key insight behind this result is combining an improper controller parametrization know as disturbance-based control with  online convex optimization with memory due to **Hazan, "On Online Learning with Adversarial Loss Functions"**. Follow up work by **Dey, "Stabilizing Linear Dynamical Systems against Adversarial Perturbations"** achieves logarithmic pseudo-regret for strongly convex, adversarially selected losses and well-conditioned stochastic noise. Further extensions were made for linear control with partial observation **Simchowitz, "Learning Multi-Armed Bandits under the Unfair Stochastic Model"**, system identification with adversarial noise **DeMott, "Stability of Online Learning in Linear Dynamical Systems"**, and many more settings surveyed in **Liu, "A Survey on Control of Linear Dynamical Systems via Machine Learning Methods"**.


\paragraph{Online learning and online convex optimization.}
We make extensive use of techniques from the field of online learning and regret minimization in games **Zinkevich, "Online Convex Programming and Generalized Inf-Convolution"**. Following previous work from machine learning, we consider regret minimization in sequence prediction, where the underlying sequence follows a linear dynamical system.   

\paragraph{Spectral filtering for learning linear dynamical systems.}
The spectral filtering technique was put forth in **Hazan, "Optimal Spectral Filtering for Linear Dynamical Systems"** for learning symmetric linear dynamical systems. In **Dabney, "Tensorized LQR: A Tensor-based Approach to Learning Linear Dynamical Systems"**, the technique was extended for more general dynamical systems using closed loop regression, however this required hidden-dimension parameters and polynomial dependence on the approximation guarantee. Spectral filtering techniques were recently used in non-linear sequence prediction, notably in the context of large language models, albeit with no theoretical guarantees **Kuchaiev, "Generating Long Sequences with Sparse Transformers"**. As convolutional models, these methods are attractive for sequence prediction due to faster generation and inference as compared to attention-based models **Vaswani, "Attention is All You Need"**.


In this paper we dramatically improve the spectral filtering technique and broaden its applicability in two major aspects: First, for general asymetric linear dynamical systems we remove the dependence on the hidden dimension. Second, we improve the dependence of the number of learned parameters from polynomial to logarithmic.