

\section{Discussion of Result}

\subsection{Decomposing the Components of Learning a Linear Dynamical Systems}
\label{section:lds_breakdown}
Consider a noiseless linear dynamical system (LDS) with input vectors $\uv_1, \dots, \uv_T \in \R^{d_{\textrm{in}}}$ and output vectors $\y_1, \dots \y_T \in \R^{d_{\textrm{out}}}$ which follow the law 
\begin{align*}
    \mat{x}_{t+1} & = \A \mat{x}_t + \B \uv_t \\
    \y_t & = \C \mat{x}_t + \mat{D} \uv_t,
\end{align*}
where $\mat{x}_0, \dots, \mat{x}_T \in \R^{d_{\textrm{hidden}}}$ is a sequence of hidden states and $(\A, \B, \C, \mat{D})$ are matrices which parameterize the LDS. We assume w.l.o.g. that $\mat{D} = 0$. Observe that $\y_t$ can be equivalently expressed as
\begin{equation}
\label{eqn:lds}
    \y_t = \sum_{s = 1}^{t} \C \A^{t-s} \B \uv_{s}.
\end{equation}
By algebraically manipulating Eq~\eqref{eqn:lds} one can see that any linear combination of $n$ autoregressive terms $\y_t, \dots, \y_{t-n}$ with coefficients $c_0, \dots, c_n$ is equivalent to 
\begin{equation}
\label{eqn:polynomial_translation}
    \sum_{i = 0}^n c_i \y_{t-i} = \sum_{s = 0}^{n-1} \sum_{i = 1}^n c_i \C \A^{\max (0, i-n+s)}\B \uv_{t-s} + \sum_{s = 1}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s},
\end{equation}
where we define the degree-$n$ polynomial $p_n(x)$ based on the coefficients $c_0, \dots, c_n$,
\begin{equation*}
    p_n(x) = c_0 x^n + \dots + c_n x^0.
\end{equation*}
Letting $\Q_j =  \C \A^j \B$ and assuming for simplicity that $c_0=1$, this immediately shows that there are matrices $\Q_1, \dots, \Q_n$ such that
\begin{center}
%\fbox{
\begin{equation}
\label{eqn:lds_breakdown}
    \y_t = \underbrace{\sum_{i = 1}^n -c_i \y_{t-i} + \sum_{s = 0}^{n-1} \sum_{i = 1}^n c_i \Q_{\max (0, i-n+s)} \uv_{t-s} }_{\text{Linear Autoregressive Term}: \prod_{LIN}^{n,n}}+ \underbrace{\sum_{s = 0}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s}}_{\text{Spectral Filtering Term:} \prod_{SF}^k}.
\end{equation}
%}
\end{center}

\subsection{Auto-regressive vs. Spectral Learning}

Eq~\eqref{eqn:lds_breakdown} gives the key intuition for our result. The two left most terms are captured by the class of autoregressive predictors using a history $n$ inputs and using $n$ autoregressive terms. The right-most term has a special structure that is captures by the class of spectral filtering predictors. 

The main insight we derive from this expression is the inherent tension between the two approaches. The coefficients $c_i$ control two terms that are competing: The polynomial $p_n(x)$ and its corresponding coefficients $c_0, \dots, c_n$ control two terms that are competing:
\begin{enumerate}
    \item The auto-regressive coefficients grow larger with the degree $n$ of the polynomial and the magnitude of the coefficients $c_i$. A higher degree polynomial and larger coefficients increase the diameter of the search space over the auto-regressive coefficients, and therefore increase the regret bound. 

    \item On the other hand, a larger search space can allow a broader class of polynomials $p_n(\cdot)$ which can better control of the magnitude of $p_n(\A)$, and therefore reduce the search space of the spectral component. 
\end{enumerate}
This intuition is formalized by Theorem~\ref{thm:main_regret} which guarantees a regret bound of 
$$ \tilde{O} \left(  (1 + \max_{i \in [n]} \abs{c_i}) (1 + T^{7/6} \max_{\alpha \in \complex_\beta} \max \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \}) \sqrt{T}\right), $$
when compared to the best linear dynamical predictor which assumes the spectrum of $\A$ is contained in $\complex_\beta$. 

A prime example of this tension is exhibited by the characteristic polynomial of $\A$. Let $p_n(x)$ be the characteristic polynomial for $\A$ and note that it has degree hidden dimension, i.e. $n =  \dhidden$. Then by the Cayley-Hamilton theorem, $ p_n(\A) = 0$.
This means that the spectral filtering term in Eq~\eqref{eqn:lds_breakdown} is canceled out and therefore it is sufficient to use only the class of autoregressive predictors $\prod_{\textrm{LIN}}^{\dhidden,\dhidden}$ to accurately model the LDS. However, there is a major obstacle to using this insight in order to design an efficient algorithm-- the resulting algorithm would require learning hidden dimension many matrices $\Q_1, \dots, \Q_{\dhidden}$ which can be prohibitive when the hidden dimension is large.

\subsection{Universal vs. Learned Polynomials}

The characteristic polynomial coefficients depend on the system matrix $\mat{A}$. This is a serious downside: imprecision or noise will render the spectral component non-zero, and rule out exact learning. Another downside of learned polynomials is that implementing the auto-regressive component is inherently sequential, and prohibits parallel implementations. We thus ask: {\bf are there universal polynomials that guarantee efficient learning in linear dynamical systems?} 

The answer is positive, as we show, and in a very strong sense. The coefficients of the Chebychev polynomial of the first type have favorable properties in several regards. First and foremost - they are {\it universal}, meaning that they do not depend on the system at hand and can be applied to any LDS learning setting without prior knowledge. Second, we rigorously prove that only $O(\log T)$ coefficients are needed to strongly bound the diameter of the spectral component, ie we can use the degree $O(\log(T))$ Chebyshev polynomial. This is entirely dependent of hidden dimension, unlike the characteristic polynomial and implies that we only need to learn $\log(T)$ many autoregressive components.

