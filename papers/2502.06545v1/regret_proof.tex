\section{Proof of Theorem~\ref{thm:main_regret}}
\label{appendix:main_regret}
We prove the regret bound on the following (equivalent) algorithm, where we rescale the parameter $\M_j$ by $\sqrt{T}$ and the input $\langle \phi_j, \uv_{(t-n-i):1} \rangle$ by $1/\sqrt{T}$. We account for this rescaling by increasing the size of the domain for $\M$ by $\sqrt{T}$. 
The proof of Theorem~\ref{thm:main_regret} is composed of two parts: first, we show in Lemma~\ref{lemma:ogd} that Algorithm~\ref{alg:new_sf} achieves the stated regret with respect to the best predictor in the class of hybrid spectral-auto-regressive predictor. Then we proceed with an approximation lemma, Lemma~\ref{lemma:approximation}, showing that the best predictor in the class of hybrid spectral-auto-regressive predictors approximates the best LDS predictor. 


\subsection{Online Gradient Descent Convergence}
The following lemma provides the regret of Algorithm~\ref{alg:new_sf} when compared to the best predictor in the class of hybrid spectral-auto-regressive predictors, denoted by $\K$.




\begin{proof}[Proof of Lemma~\ref{lemma:ogd}]
Let $G = \max_{t \in [T]} \norm{\nabla_{\Q,\M} \ell_t(\Q^t, \M^t, L)}$ and let $$D = \max_{(\Q^1, \M^1), (\Q^2, \M^2) \in \K} \norm{(\Q^1, \M^1) - (\Q^2, \M^2)}.$$ By Theorem 3.1 from \cite{hazan2016introduction}, 
    \begin{equation*}
        \sum_{t = 1}^T \ell_t(\Q^t, \M^t) -  \min_{\M^* \in \K}  \sum_{t = 1}^T \ell_t(\Q^*, \M^*) \leq \frac{3}{2} GD \sqrt{T}.
    \end{equation*}
    Therefore it remains to bound $G$ and $D$. By definition of $\K$ we have
    \begin{equation*}
        D \leq n^2 R_\Q + k R_\M \leq C' k n^2 \gamma \log(T)^{1/6} \left( 1 + T^{2/3}2^{-n}\right).
    \end{equation*}
    Therefore, up to poly logarithmic factors in $T$ (assuming $n$ is polynomially bounded in $T$),
     \begin{equation*}
        D \leq c' \gamma \left( 1 + T^{2/3}2^{-n}\right).
    \end{equation*}
    For $G$ we compute the subgradient at any $\Q_i$ and $\M_i$. 
    For notational simplicity, let $v_j = \uv_{t-n-1:1} \phi_j T^{-1/2}$ and note that $\norm{v_j}_2 \leq 1$ since $\norm{u_{t-n-1:1} \phi_j T^{-1/2}}_2 \leq \norm{u_{t-n-1:1}}_{\infty} \norm{\phi_j}_1 T^{-1/2} \leq 1$, where we used that $\norm{\phi_j}_2 = 1$ implies $\norm{\phi_j}_1 \leq \sqrt{T}$. Also for convenience, let $\mathrm{ind}(i,s) = \max(0, i-n+s) + 1$. Recall $r = \max_{i \in [n]} \abs{c_i}$ denotes the maximum coefficient of the polynomial. Using this, we bound the subgradient norm as follows,
  \begin{align*}
        \norm{ \nabla_{\M_j} \ell_t(\Q,\M) }  
        & = \left\|  \mathrm{sign} \left( y_t - \left(\sum_{i = 1}^n -c_i y_{t-i} + \sum_{s = 0}^{n-1} \sum_{i = 1}^n c_i \Q_{\mathrm{ind}(i,s)} \uv_{t-s} + \sum_{j = 1}^k \M_j v_j \right)  \right)  v_j^{\top} \right\| \\
            & \leq   \sqrt{d_{\textrm{out}}}. \tag{$\norm{v_j}_2 \leq 1$ and the $\sign(\cdot)$ has entries in $\{ \pm 1 \}$ and is dimension $d_{out}$.}
    \end{align*}
    Next we bound the subgradient with respect to $\Q$. 
    \begin{align*}
        \norm{ \nabla_{\Q_{\mathrm{ind}(i,s)}} \ell_t(\Q,\M) }  
        & = \left\|  \mathrm{sign}  \left( y_t - \left(\sum_{i = 1}^n -c_i y_{t-i} + \sum_{s = 0}^{n-1} \sum_{i = 1}^n c_i \Q_{\mathrm{ind}(i,s)} \uv_{t-s} + \sum_{j = 1}^k \M_j v_j \right)  \right)  c_i \uv_{t-s}^{\top} \right\| \\
        & \leq \sqrt{d_{\textrm{out}}} r. \tag{$\norm{u_{t-s}}_2 \leq 1$, $\max_{i \in [n]} \abs{c_i} \leq r$, and the $\sign(\cdot)$ has entries in $\{ \pm 1 \}$ and is dimension $d_{out}$.}
    \end{align*}
    Then $G \leq n^2 r \sqrt{d_{out}} + k \sqrt{d_{out}}$. Therefore, 
    \begin{align*}
        D G \leq (n^2 R_\Q + k R_\M) ( n^2 r +k) \sqrt{d_{out}} \leq 4 n^4 k^2 \sqrt{d_{\textrm{out}}} (1 + r) (R_\Q + R_\M).
    \end{align*}
    Therefore, we have 
\begin{equation*}
        \sum_{t = 1}^T f_t(\hat{y}_t)  -  \min_{(\Q^*,\M^*) \in \K}  \sum_{t = 1}^T f_t(\hat{y}_t(\Q^*, \M^*)) \leq  6 n^4 k^2 \sqrt{d_{\textrm{out}}} (1 + r) (R_\Q + R_\M).
    \end{equation*}

\end{proof}


\subsection{The hybrid autoregressive-spectral-filtering class contains the LDS predictor class}
\label{appendix:M_matrices}
In this section we prove Lemma~\ref{lemma:approximation} which includes the proof of Lemma~\ref{lemma:M_matrices} from the main body of the paper. We require a critical lemma in order to do so, which we present now. 

\begin{lemma}
\label{lemma:spectral_filtering_property}
Given polynomial $p_n(\cdot)$, let $\mu_{p_n}(\alpha)$ be as defined in Eq~\eqref{eqn:mu_alpha} and $ \mat{Z}_{p_n} $ be as defined in Eq~\eqref{eqn:Z_def}.
There is a universal constant $C>0$ such that for $\phi_1, \dots, \phi_{T-n}$ as the eigenvectors of $\mat{Z}_{p_n}$
\begin{equation}
    \max_{\alpha \in S} \abs{\mu_{p_n}(\alpha)^{\top} \phi_j} \leq C \log(T)^{1/6} T^{2/3} c^{-j/6 \log T} \cdot \max_{\complex_{\beta}} \max \left \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \right \}.
\end{equation}
\end{lemma}

The proof of Lemma~\ref{lemma:spectral_filtering_property} is nontrivial and therefore we dedicate a separate section to it in Appendix~\ref{appendix:spectral_filtering_property}. 
\begin{proof}[Proof of Lemma~\ref{lemma:approximation}]
If $\pi(\A, \B, \C)$ is an LDS predictor paramterized by matrices $(\A, \B, \C)$ then
\begin{equation*}
 \y_t^{\pi(\A, \B, \C)} = \sum_{s = 1}^{t} \C \A^{t-s} \B \uv_{s}.
\end{equation*}
Using the derivation of Eq~\eqref{eqn:lds_breakdown} we have
\begin{equation}
\label{eqn:helper}
     \y_t^{\pi(\A, \B, \C)} = \sum_{i = 1}^n -c_i \y_{t-i} + \sum_{s = 0}^{n-1} \sum_{i = 1}^n c_i \C\A^{\max (0, i-n+s)} \B \uv_{t-s} + \sum_{s = 0}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s}.
\end{equation}
Since $\max_{i \in [n]} \norm{\C\A^i \B} \leq \gamma$ then for $\Q_s \defeq \C\A^{\max (0, i-n+s)} \B $ it is the case that $\norm{\Q_s} \leq R_\Q$. Next we turn our attention to the spectral filtering parameters. Recall our definition of $\mu_{p_n}(\alpha)$,
\begin{equation*}
    \mu_{p_n}(\alpha) \defeq p_n(\alpha) \begin{bmatrix}
        1 & \alpha & \dots & \alpha^{t-n-1}
    \end{bmatrix}^{\top}
\end{equation*}
and
\begin{equation}
    \uv_{t:1} \defeq \begin{bmatrix}
        & \uv_t \\
        & \uv_{t-1} \\
        & \vdots \\
        & \uv_1
    \end{bmatrix}.
\end{equation}
We henceforth assume that $\A$ is diagonalizable over the complex numbers. This is w.l.o.g., since we can perturb $\A$ with an arbitrary small perturbation, and we know that the set of diagonalizable matrices over the complex numbers is dense.  
Observe that if $P$ diagonalizes $\A$ and if $D_A$ is the diagonalization of $\A$, 
\begin{align*}
    \sum_{s = 1}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s} & =  \sum_{s = 1}^{t-n-1} \C P  p_n(D_A) D_A^s P^{\top} \B \uv_{t-n-s}  \\
     & =  \sum_{s = 1}^{t-n-1} \C P  \sum_{\ell = 1}^{d_{\textrm{hidden}}}  p_n(\alpha_{\ell}) \alpha_{\ell}^s e_{\ell} e_{\ell}^{\top}  P^{\top} \B \uv_{t-n-s}  \\
  & = \sum_{\ell=1}^{d_{\textrm{hidden}}}  \left( \C P  e_{\ell} \right)  
   \left( P^{\top} \B^{\top}  e_{\ell} \right)^{\top} \sum_{s = 1}^{t-n-1} p_n(\alpha_{\ell}) \alpha_{\ell}^s \uv_{t-n-s} \\
   & = \sum_{\ell=1}^{d_{\textrm{hidden}}}  \left( \C P  e_{\ell} \right)  
   \left( P^{\top} \B^{\top}  e_{\ell} \right)^{\top} \mu_{p_n}(\alpha)^{\top} \uv_{(t-n-1):1} \\
   & = \sum_{\ell=1}^{d_{\textrm{hidden}}}  \left( \C P  e_{\ell} \right)  
   \left( P^{\top} \B^{\top}  e_{\ell} \right)^{\top} \mu_{p_n}(\alpha) \left( \sum_{j = 1}^{T-n} \phi_j \phi_j^{\top} \right)u_{(t-n-1):1} \tag{Orthonormality of the filters} \\
   & = \sum_{j = 1}^{T-n} \left( \sum_{\ell=1}^{d_{\textrm{hidden}}}  \left( \C P  e_{\ell} \right)  
   \left( P^{\top} \B^{\top}  e_{\ell} \right)^{\top} \mu_{p_n}(\alpha)^{\top} \phi_j \right) \phi_j^{\top} \uv_{(t-n-1):1} \\
   & = \sum_{j = 1}^{T-n} \left(  C\B^{\top} \mu_{p_n}(\alpha)^{\top} \phi_j \right) \phi_j^{\top} \uv_{(t-n-1):1} \\
   & =  \sum_{j = 1}^{T-n} \left(  T^{1/2} \C \B^{\top} \mu_{p_n}(\alpha)^{\top} \phi_j \right) \left( \phi_j^{\top} \uv_{(t-n-1):1} T^{-1/2} \right) .
\end{align*}
Then for $\M_j =  T^{1/2} C\B^{\top} \mu_{p_n}(\alpha)^{\top} \phi_j$,
\begin{align*}
    \sum_{s = 1}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s}= \sum_{j = 1}^{T-n} \M_j \left( \phi_j^{\top} \uv_{(t-n-1):1} T^{-1/2} \right) .
\end{align*}
By Lemma~\ref{lemma:spectral_filtering_property} and the assumption that $\max_{\alpha \in S} \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \} \leq \pbnd$,
\begin{align*}
    \norm{\M_j}& \leq T^{1/2} \gamma\cdot \max_{\alpha \in S} \abs{\mu_{p_n}(\alpha)^{\top} \phi_j} \\
    & \leq  T^{1/2} n^2 \norm{\C}  \norm{\B} \cdot \left( \C\log(T)^{1/6}  T^{2/3} c^{-j/6 \log T} \right) \cdot \max_{\alpha \in S} \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \} \tag{Lemma~\ref{lemma:spectral_filtering_property}}\\
    & \leq C \gamma \log(T)^{1/6} T^{7/6}  c^{-j/6 \log T} \pbnd \tag{Assumption that $\max_{\alpha \in S} \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \} \leq \pbnd$} \\
    & \leq R_\M  c^{-j/6 \log T} \\
    & \leq R_\M.
\end{align*}
Define
\begin{equation*}
    \y^{\pi(\Q, \M)} \defeq \sum_{i = 1}^n -c_i y_{t-i} + \sum_{s = 0}^{n-1} \Q_{\max(0, n-i+s)} \uv_{t-s} + \sum_{j = 0}^{k} \M_j \phi_j^{\top} \uv_{(t-n-1):1}.
\end{equation*}
Observe that $\pi(\Q, \M) \in \prod_{H}^{\K}$. Next we show that $\pi(\Q, \M)$ is an $\epsilon$-approximation for $\y_t^{\pi(\A, \B, \C)}$. We have
\begin{align*}
    \norm{ \y_t^{\pi(\A, \B, \C)} -  \y^{\pi(\Q, \M)}}_2^2 & = \norm{ \sum_{j = k+1}^{T-n} \M_j \phi_j^{\top} \uv_{(t-n-1):1}T^{-1/2}}_2^2 \\
    & \leq \sum_{j = k+1}^{T-n} \norm{ \M_j } \norm{\phi_j^{\top} \uv_{(t-n-1):1}T^{-1/2}}_2 \\
     & \leq \sum_{j = k+1}^{T-n} \norm{ \M_j } \norm{\phi_j^{\top}}_1 \norm{ \uv_{(t-n-1):1}}_{\infty} T^{-1/2} \\
     & \leq \sum_{j = k+1}^{T-n} \norm{ \M_j } \tag{$\norm{\phi_j}_2 = 1$ and therefore $\norm{\phi_j}_1 \leq \sqrt{T} $ } \\
      & \leq R_\M  c^{-j/6 \log T}  \\
      & \leq \epsilon. \tag{Definition of $k$.} 
\end{align*}
\end{proof}

