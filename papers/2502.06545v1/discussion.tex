\section{Conclusion and Discussion}

This paper studies learning in linear dynamical systems, a fundamental setting in learning and control. We have significantly expanded the known systems that can be efficiently learned to those that are:
\begin{itemize}
    \item Marginally stable: systems whose  their spectral radius is arbitrarily close to one. Intuitively this captures long term effects. 
    \item Large hidden dimension: systems with larger hidden dimension are more expressive. 
    \item General transition matrices: systems that can have asymmetric transition matrices. 
    \end{itemize}

To our knowledge, no previous work is able to provably learn systems that have this combination without depending on hidden dimension. We are able to accomplish this by introducing new techniques: a new method of spectral filtering and incorporating the Chebyshev polynomials into  auto-regressive learning. Notably we extend spectral filtering to the complex domain under mild assumptions on the boundedness of the imaginary domain of the system eigenstructure. 

An open question remains: what is the full characterization of LDS that can be learned without dependence on hidden dimension? While our work significantly extends the learnable class from symmetric matrices to asymmetric ones with bounded imaginary components, the theoretical limits of learnability remain an open question. In particular, our lower bound shows that systems with largest imaginary eigenvalue of $1-\delta$ require $\frac{1}{\delta}$ parameters to learn, and our upper bounds are still far away from this regime. 
