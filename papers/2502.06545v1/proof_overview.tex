\section{Proof Overview}
The proof proceeds in two parts. The first is to show that any linear dynamical predictor in the class $\prod_{L}^{\beta, \gamma }$ is well approximated by a predictor in the hybrid class $\prod_{H}^{\K}$. 
\begin{lemma}[Approximation Lemma]
\label{lemma:approximation}
    Let $\pi(\A, \B, \C) \in \prod_{L}^{\beta, \gamma }$ be any LDS predictor. Suppose $$\max_{ \substack{ \alpha \in \complex \\  \mathrm{Im}(\alpha) \leq \beta  \\ \abs{\alpha} \leq 1 } } \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \} \leq \pbnd . $$ 
    Then for $k \geq \log(n^2 BC T /\epsilon)^2$ and domain 
    \begin{align*}
    \K =  \{ (Q_1, \dots, Q_n, M_1, \dots, M_k) \textrm{ s.t. } & \norm{Q_i} \leq \gamma, \\
    & \norm{M_i} \leq C' \log(T)^{1/6} T^{2/3} \pbnd \}, 
\end{align*}
 there exists $\pi(\Q, \M) \in  \prod_H^{\K}$ such that for any $t \in [T]$,
    \begin{equation}
        \norm{ \y^{\pi(\A, \B, \C)}_t - \y^{\pi(\Q, \M)}_t} \leq \epsilon. 
    \end{equation}
\end{lemma}
The next result provides the regret of Online Gradient Descent when compared to the best $\pi(\Q^*, \M^*) \in \prod_{H}^{\K}$. 
\begin{lemma}[Online Gradient Descent]
\label{lemma:ogd}
Let the domain $\K$ in Algorithm~\ref{alg:new_sf} be
\begin{align*}
    \K =  \{ (Q_1, \dots, Q_n, M_1, \dots, M_k) \textrm{ s.t. } & \norm{Q_i} \leq R_Q \textrm{ and } \norm{M_i} \leq R_M \}. 
\end{align*}
Given coefficients $c_{1:n}$, let $p_n(x) = x^n + c_1 x^{n-1} + \dots + c_n$ and let $r$ denote the maximum absolute value of the coefficients. 
The iterates $\hat{y}_t^{\mA}$ output by Algorithm~\ref{alg:new_sf} satisfy 
\begin{equation*}
        \sum_{t = 1}^T f_t\left( \y_t^{\mA}\right) - \min_{\pi^* \in \prod_{H}^{\K}} f_t \left( \y_t^{\pi^*}\right) \leq  6 n^4 k^2 \sqrt{d_{\textrm{out}}} (1 + r) (R_Q + R_M).
    \end{equation*}
\end{lemma}
Combining Lemma~\ref{lemma:approximation} and Lemma~\ref{lemma:ogd} proves Theorem~\ref{thm:main_regret}. 
\begin{proof}[Proof of Theorem~\ref{thm:main_regret}]
    By Lemma~\ref{lemma:ogd} the iterates from Algorithm~\ref{alg:new_sf} $(Q^1, M^1)$, \dots, $(Q^T, M^T)$ satisfy
    \begin{equation*}
    \sum_{t = 1}^T f_t\left( \y_t^{\mA}\right) - \min_{\pi^* \in \prod_{H}^{\K}} f_t \left( \y_t^{\pi}\right)  \leq \tilde{O} \left(\gamma \sqrt{d_{\textrm{out}}} (1 + r) (1 + T^{ \frac{7}{6}} \pbnd) \sqrt{T}\right).
\end{equation*}
Next, by Lemma~\ref{lemma:approximation}, given an LDS predictor $\pi_L \in \prod_L^{\beta, \gamma}$, there exists $\pi_H \in \prod_{H}^{\K}$ such that 
\begin{equation}
        \norm{ \y^{\pi_L}_t - \y^{\pi_H}_t } \leq \epsilon. 
\end{equation}
Given any output sequence $\y_1, \dots, \y_t$, 
\begin{align*}
    f_t(\y^{\pi_L}_t) & = \norm{ \y_t - \y^{\pi_L}_t }_1 \\
    & \leq \norm{ \y_t - \y_t^{\pi_H} }_1 + \norm{ \y^{\pi_H}_t- \y^{\pi_L}_t}_1 \tag{Triangle inequality}\\
    & = f_t(\y_t^{\pi_H} ) + \epsilon \tag{Lemma~\ref{lemma:approximation} }\\
    & \leq f_t(\y_t^{\pi^*}) + \epsilon. \tag{$\y_t^{\pi^*}$ minimizes $f_t$}
\end{align*}
Therefore, if $\epsilon\leq T^{3/2}$, we get the stated result. 
\end{proof}

\subsection{Proving Approximation Lemma~\ref{lemma:approximation}}
The difficult result to prove is Lemma~\ref{lemma:approximation}. As discussed in Section~\ref{section:lds_breakdown}, any linear dynamical predictor can be written in two parts: a linear autoregressive term and a spectral filtering term (see Eq~\eqref{eqn:lds_breakdown}). The ``spectral filtering term'' is $\sum_{s = 0}^{t-n-1} \C p_n(\A) \A^s \B \uv_{t-n-s}$. We call it as such because its structure is amenable to spectral filtering. Indeed, we construct a set of spectral filters that satisfy the following.
\begin{lemma}
\label{lemma:M_matrices}
For any $(\A, \B, \C)$ there exist matrices $\M_1^*, \dots, \M^*_{T-n-1}$ such that, 
    \begin{equation}
        \sum_{s = 0}^{t-n-1}  \C p_n(\A) \A^s \B \uv_{t-n-s} = \sum_{j = 1}^{T} \M^*_{j} \langle \uv_{t-n-1:0}, \phi_{\ell} \rangle,
    \end{equation}
    for any $t \in [T]$.
    Moreover, if $\A$ has eigenvalues with imaginary component bounded by $\beta$, then for any $j \in [T-n-1]$,
    \begin{equation*}
        \norm{\M^*_j} \leq \tilde{O}(T^{2/3}) \max_{\complex_{\beta}} \max \left \{ \abs{p_n(\alpha)}, \abs{p_n'(\alpha)} \right \} e^{-j/6 \log T}.
    \end{equation*}
\end{lemma}

We prove Lemma~\ref{lemma:M_matrices} formally in Appendix~\ref{appendix:M_matrices}. This proof is inspired by previous spectral filtering literature but requires several key and nontrivial adaptations to incorporate the polynomial $p_n(\cdot)$, which impacts the choice of spectral filters, and to extend the analysis to the case where the spectrum of $\A$ may lie in the complex plane. 


\subsection{Using the Chebyshev Polynomial over the Complex Plane}

Theorem~\ref{thm:main_regret} states that Algorithm~\ref{alg:new_sf} instantiated with some choice of polynomial $p_n(\cdot)$ achieves regret
\begin{equation*}
    \tilde{O} \left(\gamma \sqrt{d_{\textrm{out}}} (1 + r) (1 + T^{ \frac{7}{6}} \cdot \max_{\alpha \in \complex_{\beta}} \max \{\abs{p_n(\alpha)}, \abs{p_n'(\alpha} \}) \sqrt{T}\right),
\end{equation*}
where we recall that $r$ bounds the maximum coefficient of $p_n(\cdot)$.  This leads us to the following question: \textbf{Is there a universal choice of polynomial $p_n(x)$, where $n$ is independent of hidden dimension, which guarantees sublinear regret? } \\ % for $\beta = \Omega(1/\log(T))$, the Spectral Filtering algorithm provides sub-linear regret?}\\

For the real line, the answer to this question is known to be positive using the Chebyshev polynomials of the first kind. In general, the $n^{\textrm{th}}$ (monic) Chebyshev polynomial $M_n(x)$ satisfies
\begin{align*}
    \max_{x \in [-1,1]} \abs{M_n(x)} \leq 2^{-(n-1)} \qquad \textrm{ and } \qquad  \max_{x \in [-1,1]} \abs{M_n'(x)} \leq n 2^{-(n-1)}.
\end{align*}

However, we are interested in a more general question over the complex plane. Since we care about linear dynamical systems that evolve according to a general asymetric matrix,  we need to extending our analysis to $\complex_{\beta}$. This is a nontrivial extension since, in general, functions that are bounded on the real line can grow exponentially on the complex plane. Indeed, $2^{n-1} M_n(x) = \cos ( n \arccos(x) )$ and while $\cos(x)$ is bounded within $[-1,1]$ for any $x \in \R$, over the complex numbers we have 
$$\cos(z) = \frac{1}{2}(e^{iz} + e^{-iz}) , $$ 
which is unbounded. Thus, we analyze the Chebyshev polynomial on the complex plane and provide the following bound.
\begin{lemma}
\label{lemma:cheby_bound}
    Let $z \in \complex$ be some complex number with magnitude $\abs{\alpha} \leq 1$. Let $M_n(\cdot)$ denote the $n$-th monic Chebyshev polynomial. If $\abs{\mathrm{Im}(z)} \leq 1/64n^2$, then
    \begin{equation*}
        \abs{M_n(z)} \leq \frac{1}{2^{n-2}} \qquad \textrm{ and } \qquad 
        \abs{M_n'(z)} \leq \frac{n^2}{2^{n-1}}.
    \end{equation*}
\end{lemma}
We provide the proof in Appendix~\ref{appendix:chebyshev}. We also must analyze the magnitude of the coefficients of the Chebyshev polynomial, which can grow exponentially with $n$. We provide the following result. 
\begin{lemma}
\label{lemma:cheby_coeffs_bound}
    Let $M_n(\cdot)$ have coefficients $c_0, \dots, c_n$. Then,
    \begin{equation*}
        \max_{k = 0, \dots, n} \abs{c_k} \leq 2^{0.3n}.
    \end{equation*}
\end{lemma}
The proof of Lemma~\ref{lemma:cheby_coeffs_bound} is in Appendix~\ref{appendix:chebyshev}. 