





\section{Methodology}
% We introduce our framework in this section and demonstrate the overview in Figure.

\subsection{Preliminary: Memory-Augmented LLM-Agents}
To address the limit of LLMs struggling in retaining information from long input text, recent studies have demonstrated that equipping LLM agents with memory can efficiently support long-turn conversations \cite{lu2023memochat,packer2023memgpt,zhong2024memorybank}. Therefore, we base our study on memory-augmented LLM-agents.


The general pipeline of memory-augmented LLM-agents comprises three stages: \textit{memorization}, \textit{retrieval}, and \textit{response}. In the \textit{memorization} stage, the model summarizes each dialogue session and stores these summaries as memory. During the \textit{retrieval} stage, the model retrieves the most relevant memory for the current dialogue session. This retrieved memory is then concatenated with the ongoing dialogue to generate the next \textit{response}.

For temporal reasoning in multi-session dialogues, we can apply this pipeline by summarizing each dialogue session, retrieving relevant memory for each temporal question, and using this information to select the answer. Specifically, we build our framework based on MemoChat \cite{lu2023memochat}, which effectively realizes this three-stage process through prompting and has demonstrated effectiveness in handling long-range dialogues.

\subsection{TReMu}
Building on the memory-augmented LLM-agent pipeline, we introduce our framework called \textbf{TReMu} as shown in Algorithm \ref{algo1}. The framework consists of two key components: \textit{time-aware memorization} and \textit{neuro-symbolic temporal reasoning}.

\begin{algorithm}
\small
\caption{TReMu} \label{algo1}
\begin{algorithmic}
\STATE \textbf{Initialize:} 
\STATE Time-aware Memorization Model LLM\textsubscript{mem}
\STATE Memory Retrieval Model LLM\textsubscript{retrieval}
\STATE Neuro-symbolic Reasoning Model LLM\textsubscript{code}
\STATE Python Executor $\mathcal{P}$
\STATE Memorization pool $\mathcal{M} \leftarrow \emptyset $
\STATE \COMMENT{\textit{Time-aware Memorization}}
\FOR{each dialogue session $d_i$ in dialogue $\mathcal{D}$}
    \STATE \( m_i \leftarrow \text{LLM}_\text{mem}(d_i) \)
    \STATE \( \mathcal{M} \leftarrow \mathcal{M} \cup m_i\)
\ENDFOR
\STATE \COMMENT{\textit{Neuro-symbolic Temporal Reasoning}}
\FOR{each temporal question $q$}
\STATE  \(m_{retrieved} \leftarrow \text{LLM}_\text{retrieval}(q, \mathcal{M})\)
\STATE \(c \leftarrow \text{LLM}_{code}(q, m_{retrieved})\)
\STATE \(o \leftarrow \mathcal{P}(c)\)
\STATE final answer \(a \leftarrow \text{LLM}(q, o)\)
\ENDFOR
\end{algorithmic}
\end{algorithm}
\vspace{-0.5cm}

\subsubsection{Time-aware Memorization} 
As LLMs may struggle with capturing temporal information in dialogues, particularly relative time, we propose a time-aware memorization strategy using timeline summarization \cite{steen-markert-2019-abstractive,rajaby-faghihi-etal-2022-crisiltlsum,sojitra2024timeline}. Specifically, for each dialogue session tagged with its timestamp, we prompt the LLM to generate its summary while also summarizing mentioned events occurring on different inferred dates. The prompt is shown in Appendix Sec.\ref{sec:timeline_prompt}. This allows us to obtain a series of summaries for a single dialogue session, with each summary is linked to a specific date. By distinguishing between when an event occurred and when it was mentioned, this time-aware memorization could improve temporal reasoning and enable more precise event tracking across the dialogue sessions.

\subsubsection{Neuro-symbolic Temporal Reasoning}
Allen's interval algebra \cite{allen1983maintaining} provides a foundation for solving various types of temporal reasoning by identifying event timestamps and performing temporal comparisons or calculations. For example, determining the difference between the timestamps of two events allows us to resolve temporal interval problems. Building on the interval algebras, previous works have developed various neuro-symbolic models to integrate symbolic reasoning with neural networks \cite{garcez2003reasoning,zhou2021temporal}.

Additionally, researchers have explored neuro-symbolic reasoning for LLMs. By providing LLMs with clear instructions about the grammar of the symbolic language and offering a few demonstrations as in-context examples, LLMs can accurately translate problems into different symbolic language, such as first-order logic \cite{han2022folio}. Once the problem is translated, symbolic solvers can be employed to solve it, and it has been demonstrated to outperform methods like CoT for multi-hop reasoning tasks \cite{pan2023logic}.

Inspired by the above motivations, we propose leveraging LLMs to translate temporal reasoning questions into Python code, which can then be executed to derive answers. The detailed prompts are shown in Appendix Sec.\ref{sec:reasoning_prompt}. The underlying reasons stem from the strong ability of state-of-the-art LLMs in generating Python code in multiple code generation benchmarks and by the availability of Python libraries, such as \textit{datetime} and \textit{dateutil}, which support temporal calculations. Particularly, \textit{dateutil} provides a function \textit{relativedelta} supporting relative time calculation, for example we can infer next Friday using \textit{TODAY + relativedelta(weekday=FR)}. Meanwhile, we provide some implemented functions that the model can directly call, such as "weekRange(\textit{t})" returns the start date and the end date of the week that \textit{t} lies in. As LLMs have familiarity with Python, we provide demonstration via in-context learning to guide it in generating Python code for temporal reasoning, based on the given question and retrieved memory. We then execute the output Python code, and the LLM is prompted again to select the correct answer by using the code.

Particularly, we believe our reasoning approach offers an alternative form of CoT. While the original CoT \cite{wei2022chain} performs step-by-step reasoning in natural language, our neuro-symbolic approach conducts temporal reasoning by executing generated code line-by-line in a programming language. This neuro-symbolic method retains the core concept of CoTâ€™s step-by-step reasoning while leveraging the precision and additional support provided by Python code and packages, enhancing the accuracy of the reasoning process.