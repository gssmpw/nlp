% \begin{figure*}
%     \centering
%     \includegraphics[width=\linewidth]{figures/benchmark_pipeline.png}
%     \caption{The overview of the pipeline for our benchmark construction.}
%     \label{fig:benchmark}
% \vspace{-5mm}
% \end{figure*}

\begin{table*}[ht]
\small
\centering
\scalebox{0.86}{
\begin{tabular}{lp{1.44cm}p{1.75cm}p{1.6cm}p{2.1cm}p{3.5cm}}
\toprule
\bf Dialogue Dataset & \bf Avg. Turns Per Conv. & \bf Avg. Sessions Per Conv. & \bf Avg. Tokens Per Conv. & \bf Time Interval & \bf Collection \\ \midrule
MSC \cite{xu-etal-2022-beyond} & 53.3 & 4 & 1,225.9 & few days & Crowdsourcing \\
Conversation Chronicles \cite{jang2023conversation} & 58.5 & 5 & 1,054.7 &few hours - years & LLM-generated \\
\bf LoCoMo \cite{maharana-etal-2024-evaluating} (Ours) &304.9 & 19.3 & 9,209.2 & few months & LLM-gen. + crowdsourcing \\
 \bottomrule
\end{tabular}}
\caption{Statistics of the chosen multi-session dialogue dataset, LoCoMo, compared to others.}
\label{tab:locomo_statistics}
\vspace{-0.3cm}
\end{table*}

\section{Benchmark Construction}
In this section, we introduce the construction pipeline to build our temporal QA benchmark for evaluating LLM-agents' temporal reasoning in multi-session dialogues. As mentioned earlier, we mainly focus on the two temporal characteristics in multi-session dialogues: \textbf{relative time} and \textbf{cross-session dependency}.

\begin{table*}[ht]
\small
\centering
\scalebox{0.9}{
\begin{tabular}{lllll}
\toprule
\bf Question Type & \bf Count & \bf $\#$ of Options & \bf $\#$ of Events & \bf Event Type \\ \midrule
Temporal Anchoring & 264 & 5 & 1 & relative time \\
Temporal Precedence & 102 & 3 & 2 & cross session dependency (+ relative time) \\
Temporal Interval & 234 & 5 & 2 & cross session dependency (+ relative time) \\ \midrule
Total & 600 & -- & -- & -- \\
\phantom{Total} - Unanswerable & 112 & -- & -- & -- \\ \midrule
LoCoMo \cite{maharana-etal-2024-evaluating} & 321 & -- & -- & -- \\ \bottomrule
\end{tabular}}
\caption{Dataset statistics and details of the constructed benchmark.\textit{$\#$ of Options} refers to the number of options for each temporal question. \textit{$\#$ of Events} refers to the number of selected events to create each temporal question. \textit{Event Type} specifies the type of temporal events chosen for question creation, where \textit{(+ relative time)} indicates that relative time was an additional consideration in event selection.  Note that our benchmark not only contains more temporal QAs than LoCoMo, but also include unanswerable questions.}
\label{tab:statistics}
\vspace{-0.5cm}
\end{table*}

\subsection{Benchmark Design}
We propose augmenting an existing multi-turn dialogue dataset to create a benchmark for evaluating LLM-agents' temporal reasoning in multi-session dialogues. After a thorough examination, we selected LoCoMo \cite{maharana-etal-2024-evaluating}, which comprises dialogues averaging 600 turns and 16,000 tokens across up to 32 sessions. In comparison to existing multi-session dialogue datasets, LoCoMo features the longest dialogues and the most sessions, thus presenting a greater challenge. The detailed statistics of LoCoMo and its comparison with other relevant datasets are shown in Table \ref{tab:locomo_statistics}.

As mentioned earlier, our benchmark focuses on two key temporal characteristics in multi-session dialogues: \textit{relative time} and \textit{cross-session dependency}. To achieve this, we follow previous benchmarks \cite{chen2dataset,xiong-etal-2024-large,wang-zhao-2024-tram} by creating temporal QA pairs based on temporal events in the dialogues. Specifically, we design each temporal QA based on either a single event or a pair of events:

$\bullet$ \textbf{Single Event}: We select events expressed with relative time and develop a temporal reasoning type called \textit{Temporal Anchoring}, which asks for the exact time of the event.

$\bullet$ \textbf{Two Events}: We choose pairs of relevant events from different sessions that exhibit cross-session dependency. We also consider relative time as an extra factor to increase the complexity of the questions. Two temporal reasoning types are applied: \textit{Temporal Precedence}, which asks which event occurred first, and \textit{Temporal Interval}, which asks for the duration between the two events.

\subsection{Construction Pipeline}
To construct our benchmark, we follow the design of our benchmark and utilize a systematic step-by-step approach with GPT-4o. 
% The overview of the complete pipeline is shown in Figure \ref{fig:benchmark}. 
The prompt for each step is shown in Appendix Sec.\ref{sec:benchmark_prompt}.

\noindent \textbf{Step 1: Temporal Event Extraction}
We begin by prompting GPT-4o to extract all temporal events from each dialogue session. In addition, we instruct GPT-4o to annotate the relative time expressions for these events, facilitating the selection process for creating temporal QAs.

\noindent \textbf{Step 2: Temporal Event Linking}
Next, we link the extracted events containing cross-session dependency within the dialogue. We prompt GPT-4o with the extracted events and instruct it to group those related to the same or relevant entities across different sessions, particularly those reflecting changes in attributes over time. For example, the event “Debra Ryan told her mentor about her business idea” from an early session is linked to “Debra Ryan started her own business” from a later session.

\noindent \textbf{Step 3: Temporal QA Creation}
Once the temporal events are processed, we prompt GPT-4o to select those events that meet the criteria for different temporal reasoning types and generate multiple-choice temporal QAs. Additionally, we create unanswerable questions, as in prior QA benchmarks \cite{rajpurkar2018know}, to comprehensively assess models' temporal reasoning capabilities.

\noindent \textbf{Step 4: Quality Control}
We observe various noises in generated QAs, such as incorrect inferences about exact times or the selection of events with ambiguous time expressions (e.g., "the other day"). To ensure the benchmark’s quality, we follow recent temporal reasoning benchmarks for LLMs, such as TGQA \cite{xiong-etal-2024-large}, to perform quality control. We manually review each question to verify that it aligns with our design specifications and that the answers are correctly grounded in the dialogue. We also revise well-constructed questions with incorrect answers and remove any unreasonable ones. The final temporal QA benchmark covers time intervals from days to months and its statistics and details are presented in Table \ref{tab:statistics}. Particularly, our final benchmark not only contains more temporal QAs than LoCoMo, but also include unanswerable questions, which are not covered in LoCoMo. We also include examples of QAs for different temporal reasoning types in Appendix Sec. \ref{sec:QAs}.