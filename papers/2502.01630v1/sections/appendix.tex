
% \begin{table*}[ht]
% \small
% \centering
% \scalebox{0.86}{
% \begin{tabular}{lp{1.44cm}p{1.75cm}p{1.6cm}p{2.1cm}p{3.5cm}}
% \toprule
% \bf Dialogue Dataset & \bf Avg. Turns Per Conv. & \bf Avg. Sessions Per Conv. & \bf Avg. Tokens Per Conv. & \bf Time Interval & \bf Collection \\ \midrule
% MSC \cite{xu-etal-2022-beyond} & 53.3 & 4 & 1,225.9 & few days & Crowdsourcing \\
% Conversation Chronicles \cite{jang2023conversation} & 58.5 & 5 & 1,054.7 &few hours - years & LLM-generated \\
% \bf LoCoMo \cite{maharana-etal-2024-evaluating} &304.9 & 19.3 & 9,209.2 & few months & LLM-gen. + crowdsourcing \\
%  \bottomrule
% \end{tabular}}
% \caption{Statistics of the chosen multi-session dialogue dataset, LoCoMo, compared to other multi-session dialogue datasets.}
% \label{tab:locomo_statistics}
% \end{table*}

% \section{Statistics of LoCoMo}
% \label{sec:locomo_statistics}
% We show the detailed statistics of our chosen multi-session dialogue dataset, LoCoMo, for augmentation in Table \ref{tab:locomo_statistics}. The average length of a conversation in
% LOCOMO is 9x that of MSC \cite{xu-etal-2022-beyond}, distributed over 6x more turns and 4x more sessions (on average). This shows that LoCoMo provides a more challenging setting and makes our augmented benchmark more difficult.


\section{Prompts for Benchmark Construction}
\label{sec:benchmark_prompt}
We use GPT-4o to construct a temporal reasoning benchmark for multi-session dialogues. The first step is the temporal event extraction using the prompt shown in Figure \ref{fig:event_extraction}. Then the prompt for the second step, temporal event linking, is shown in Figure \ref{fig:event_connection}. With the grouped temporal events, we use the prompt in Figure \ref{fig:qa_creation} to create temporal QAs.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/event_extraction.png}
    \caption{Prompt for Temporal Event Extraction.}
    \label{fig:event_extraction}
\vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/event_connection.png}
    \caption{Prompt for Temporal Event Linking.}
    \label{fig:event_connection}
\vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/qa_creation.png}
    \caption{Prompt for Temporal QA Creation.}
    \label{fig:qa_creation}
\vspace{-5mm}
\end{figure*}

\section{Examples of Temporal QAs in Constructed Benchmark}
\label{sec:QAs}

We show examples of final temporal QAs for different temporal reasoning types in Figure \ref{fig:anchoring}, \ref{fig:precedence} and \ref{fig:interval}. In each example, we highlight the ground truth answer as green and show the corresponding selected temporal events for constructing the question below the question.

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/temporal_anchoring.png}
    \caption{An example temporal QA for Temporal Anchoring}
    \label{fig:anchoring}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/temporal_precedence.png}
    \caption{An example temporal QA for Temporal Precedence}
    \label{fig:precedence}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/temporal_interval.png}
    \caption{An example temporal QA for Temporal Interval}
    \label{fig:interval}
\end{figure*}

\section{Prompt for Time-aware Memorization}
\label{sec:timeline_prompt}
We show the designed prompt for time-aware memorization in Figure \ref{fig:timeline_prompt}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/timeline_prompt.png}
    \caption{Prompt for Time-aware Memorization.}
    \label{fig:timeline_prompt}
\vspace{-5mm}
\end{figure*}

\section{Prompt for Neuro-symbolic Temporal Reasoning}
\label{sec:reasoning_prompt}
We show the designed prompts for neuro-symbolic temporal reasoning here. Specifically, we first perform memory retrieval with the prompt in Figure \ref{fig:retrieval_prompt}. Then we prompt to generate Python code via in-context learning as in Figure \ref{fig:code_prompt}. With the generated code and its execution result, we finally prompt the LLM to select the answer, and the prompt is shown in Figure \ref{fig:qa_prompt}.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/retrieval_prompt.png}
    \caption{Prompt for memory retrieval.}
    \label{fig:retrieval_prompt}
\vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/code_prompt.png}
    \caption{Prompt for generating Python code for temporal reasoning.}
    \label{fig:code_prompt}
\vspace{-5mm}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/qa_prompt.png}
    \caption{Prompt for temporal question answering.}
    \label{fig:qa_prompt}
\vspace{-5mm}
\end{figure*}