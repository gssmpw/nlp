%%
%% This is file `sample-acmsmall.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,journal,bibtex,acmsmall')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-acmsmall.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall, screen]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{none}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}

% %%
% %% These commands are for a JOURNAL article.
% \acmJournal{JACM}
% \acmVolume{37}
% \acmNumber{4}
% \acmArticle{111}
% \acmMonth{8}

\usepackage{adjustbox}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{pgf-pie}
\usepackage{pifont}
\usepackage{threeparttable}
\usepackage{hyperref}
\usepackage{makecell}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{ulem}
\usepackage[most]{tcolorbox}
%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{LLMs in Software Security: A Survey of Vulnerability Detection Techniques and Insights}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Ben Trovato}
% \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

\author{Ze Sheng}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station}
  \country{USA}}
\email{zesheng@tamu.edu}

\author{Zhicheng Chen}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station}
  \country{USA}}
\email{zhicheng@tamu.edu}

\author{Shuning Gu}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station}
  \country{USA}}
\email{shuning@tamu.edu}

\author{Heqing Huang}
\affiliation{%
  \institution{City University of Hong Kong}
  \city{Hong Kong}
  \country{China}}
\email{heqhuang@cityu.edu.hk}

\author{Guofei Gu}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station}
  \country{USA}}
\email{guofei@tamu.edu}

\author{Jeff Huang}
\affiliation{%
  \institution{Texas A\&M University}
  \city{College Station}
  \country{USA}}
\email{jeffhuang@tamu.edu}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Sheng et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Large Language Models (LLMs) are emerging as transformative tools for software vulnerability detection. Traditional methods, including static and dynamic analysis, face limitations in efficiency, false-positive rates, and scalability with modern software complexity. Through code structure analysis, pattern identification, and repair suggestion generation, LLMs demonstrate a novel approach to vulnerability mitigation.

This survey examines LLMs in vulnerability detection, analyzing problem formulation, model selection, application methodologies, datasets, and evaluation metrics. We investigate current research challenges, emphasizing cross-language detection, multimodal integration, and repository-level analysis. Based on our findings, we propose solutions addressing dataset scalability, model interpretability, and low-resource scenarios.

Our contributions include: (1) a systematic analysis of LLM applications in vulnerability detection; (2) a unified framework examining patterns and variations across studies; and (3) identification of key challenges and research directions. This work advances the understanding of LLM-based vulnerability detection. The latest findings are maintained at \href{https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection}{https://github.com/OwenSanzas/LLM-For-Vulnerability-Detection}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
% \begin{CCSXML}
% <ccs2012>
%  <concept>
%   <concept_id>00000000.0000000.0000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>500</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>300</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
%  <concept>
%   <concept_id>00000000.00000000.00000000</concept_id>
%   <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
%   <concept_significance>100</concept_significance>
%  </concept>
% </ccs2012>
% \end{CCSXML}

% \ccsdesc[500]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[300]{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc{Do Not Use This Code~Generate the Correct Terms for Your Paper}
% \ccsdesc[100]{Do Not Use This Code~Generate the Correct Terms for Your Paper}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Large Language Models, Vulnerability Detection, Cybersecurity}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
Vulnerability detection plays an important part in the design and maintenance of modern software. Statistical evidence indicates that approximately 70\% of security vulnerabilities originate from defects in the software development process \cite{Aslan2023}. According to the metrics provided by Common Vulnerabilities and Exposures Numbering Authorities (CNAs), a growth is witnessed that in the past 5 years, about 120,000 CVEs have been discovered and reported \cite{CWE_Metrics}. According to FBI's cybercrime report shown in Figure \ref{fig:complaint_loss}, the period from 2018 to 2023 suffers from a large amount of cybersecurity crimes and complaints. A recent example is the CrowdStrike incident in July 2024 \cite{GAO2024}, where a faulty software update caused widespread system crashes across critical infrastructure sectors including healthcare, transportation, and finance. Therefore, enhanced focus and investment in vulnerability detection technology is in demand.

\begin{figure}[h]
  \includegraphics[width=0.6\linewidth]{images/complaint_loss.png}
  \caption{Complaints and Financial Losses from 2018-2023}
  \Description{Complaint_Loss}
  \label{fig:complaint_loss}
\end{figure}

State-of-the-art vulnerability detection approaches/tools can be broadly classified into static analysis and dynamic analysis \cite{chess2004static, livshits2005finding, zaddach2014avatar, russo2010dynamic}. Static analysis examines source code or bytecode to identify potential security vulnerabilities while dynamic analysis does it during program execution, including techniques such as fuzz testing \cite{Serebryany2016}. Fuzz testing identifies potential vulnerabilities by inputting random or specific invalid data into applications and observing system responses. However, with the increasing scale of software systems, both traditional static and dynamic analysis approaches have distinct limitations. For example, static and dynamic analysis tools suffer from high false positive rates, low efficiency and vast effort for overwhelmed amount of types of vulnerabilities \cite{Johnson13, Venkatasubramanyam14}.

Large Language Models (LLMs), as an advancement in natural language processing (NLP), are trained by deep learning techniques, particularly the Transformer architecture to focus on diverse NLP tasks \cite{vaswani2023attentionneed}. Recently, LLMs have shown remarkable abilities in software development area \cite{Hou24}. Based on these capabilities, several LLM-driven vulnerability detection methodologies and tools have been proposed by researchers. This new trend has attracted attention from both cybersecurity experts and machine learning researchers, potentially revolutionizing this field. For example, in 2024 DARPA held the Artificial Intelligence Cyber Challenge (AIxCC), where competitors only leverage general-purpose LLMs (GPT series, Claude series, and Gemini series) for vulnerability detection, reproduction, and patching ~\cite{AIxCC}. The competition brought together leading experts in both cybersecurity and machine learning all over the world, indicating the significant potential of applying LLMs to vulnerability detection.

% \heqing{Add some transitions. E.g., This paper provide the first survey for thoroughly understanding the strengths and weaknesses of LLM-based detection methods. We construct our survey from four key aspects. The following paragraph and four RQs are quite repetitive. Maybe one is enough.}

The increasing adoption of LLMs in vulnerability detection is evident from initiatives like AIxCC, showcasing the rapid evolution of their capabilities and the diversity of approaches in this field. Despite growing interest and significant research efforts, a systematic survey that thoroughly examines LLM-based detection methods is still lacking.

This paper addresses this gap by presenting the first comprehensive survey focused on understanding the strengths and weaknesses of LLMs in vulnerability detection. To provide a structured and holistic analysis, we construct our survey around four key aspects: (1) identifying effective LLM architectures for security tasks, (2) evaluating benchmarks, datasets, and metrics for reliable assessment, (3) analyzing techniques to uncover best practices, and (4) recognizing challenges to guide future research directions. These aspects collectively highlight the current state of the field and its potential for advancement.

Based on these aspects, we focus on the following research questions (RQs):

\begin{itemize}
\item \textbf{RQ1}. What LLMs have been applied to vulnerability detection?
\item \textbf{RQ2}. What benchmarks, dataset and metrics have been designed to evaluate vulnerability detection?
\item \textbf{RQ3}. What techniques have been used in LLM for vulnerability detection?
\item \textbf{RQ4}. What are the challenges that LLMs are facing in detecting vulnerabilities and potential directions to solve them?

\end{itemize}


To analyze and summarize the RQs, we selected more than 80 papers (with 58 highly related papers) from over 500 papers to ensure the recency (2019-2024) and the relevance (focusing on LLMs in vulnerability detection).  Therefore, this survey will not discuss traditional machine learning approaches (conventional CNN, RNN and LSTM) in vulnerability detection, which were predominantly used between 2014 and 2020. 

In general, our key findings can be summarized as the positive and key gaps:

\begin{itemize}
    \item \textbf{The Positive: }Cybersecurity community has experienced a positive impact from LLM, evidenced by a substantial increase in published articles in recent years. The contributions span multiple areas, including vulnerability localization, detection and analysis. C, Java and Solidity have emerged as predominant focuses in this domain. Research methodologies consistently emphasize three key components: LLM implementation, prompt engineering, and semantic processing methods. Moreover, multi-agent approaches are widely used because of the decomposition of complex vulnerability detection challenges into manageable sub-problems.


    \item \textbf{Key Gaps:}
        \begin{itemize}
            \item \textbf{Narrow Scope and Limited Repository-Level Coverage:} Current work often restricts itself to binary classification of function-level vulnerabilities within small, specialized datasets. Moreover, few studies address the detection and reproduction of vulnerabilities at the repository level, where cross-file dependencies and longer call stacks pose significant challenges.

            \item \textbf{Rapid Advances in Frontier LLMs:} Breakthroughs in 2023--2024 indicate that fine-tuning and leveraging frontier LLMs will be critical for future progress, yet most research to date relies on less-capable traditional models.

            \item \textbf{Insufficient Context Awareness:} There is insufficient attention to complex, multi-file dependencies and long call stacks. Although neuro-Symbolic approaches (e.g. CodeQL, Bear with LLMs) have shown promise on large-scale projects, improved taint propagation modeling, more efficient LLM reasoning, and cross-language adaptation are still required.

            \item \textbf{Vulnerability Type Imbalance:} Memory-related vulnerabilities (e.g. buffer overflow issues) receive disproportionately higher detection accuracy, while logical vulnerabilities remain relatively underexplored.

            \item \textbf{Dataset Limitations:} Existing datasets are narrowly scoped and have a data leakage problem. A dedicated and comprehensive data set specifically tailored for LLM-based vulnerability detection is urgently needed to drive both fundamental and applied research.

        \end{itemize}
    % \item \textbf{Key Gaps: }Current research lacks breadth and comprehensiveness, with most studies limited to binary classification of function-level vulnerabilities within specific datasets, offering minimal contribution to repository-level vulnerability detection in practical development. To achieve practical impact, research should focus on repository-level vulnerability detection and reproduction. The rapid evolution of frontier LLMs in 2023-2024 has significantly outperformed traditional models, making their fine-tuning and utilization a key future direction. Additionally, we observe limited attention to repository-level vulnerabilities involving cross-file dependencies, long call stacks, and complex contextual relationships. While Neuro-Symbolic approaches using tools like CodeQL and Bear have achieved breakthroughs in large-scale projects, future research should focus on more refined taint propagation modeling, more efficient LLM reasoning optimization, and cross-language adaptation to enhance vulnerability detection capabilities. Moreover, current research lacks focus on specific vulnerability types, with LLMs demonstrating significantly stronger detection capabilities for memory vulnerabilities, particularly overflow issues, compared to logical vulnerabilities, necessitating further investigation. Regarding dataset research, challenges persist, with discussions primarily remaining theoretical rather than practical. Therefore, a dedicated dataset for LLM-based vulnerability detection is urgently needed.
    
\end{itemize}

The following sections present a comprehensive analysis of LLMs in vulnerability detection. Given the extensive scope of this survey, this section outlines the structure, main themes, and narrative flow of our analysis. A visualization of this paper's structure is shown in Figure \ref{fig:structure}.

\begin{figure*}
  \includegraphics[width=0.8\textwidth]{images/Paper_Structure_v2.drawio.png}
  \caption{Survey Structure \& Research Selection}
  \Description{Survey Structure \& Research Selection}
  \label{fig:structure}
\end{figure*}

\section{Background}
\subsection{Paper Selection And Scope}
To ensure a comprehensive and systematic review, we began our search with top-tier security conferences, such as IEEE Symposium on Security and Privacy (S\&P), USENIX Security, and ACM Conference on Computer and Communications Security (CCS), as well as journals like IEEE Transactions on Software Engineering. Then we searched by extracting key terms such as "vulnerability detection," "LLM," "large language model," and "AI" from papers published in conferences and journals. Using these keywords, we conducted iterative searches every three weeks, refining the selection over time. Over a two-month period, we screened approximately 500-600 papers and selected 58 highly relevant studies.

This survey focuses exclusively on the application of LLMs in vulnerability detection, analyzing techniques, datasets, benchmarks, and challenges. We reviewed works targeting programming languages like C/C++, Java, and Solidity, which are the primary focus areas for LLM-based vulnerability detection. Studies centered on traditional machine learning methods, such as CNNs and RNNs, and those unrelated to vulnerability detection, such as malware analysis or network intrusion detection, were excluded.

In terms of datasets, we primarily evaluated function-level and file-level granularity, noting that C/C++ datasets dominate the field. However, repository-level datasets that better reflect real-world development scenarios are significantly lacking. This limitation poses challenges for LLMs in generalizing to complex, multi-file vulnerabilities.

By clearly defining the scope and adopting a rigorous, keyword-driven selection process, we aim to ensure the robustness and relevance of this survey while laying a solid foundation for future research.

\subsection{Related Reviews}
In the past five years, many studies have been proposed on leveraging LLMs for vulnerability detection. Several comprehensive surveys have been presented on vulnerability detection techniques, covering traditional approaches (static/dynamic analysis) and machine learning methods (CNN, RNN) \cite{Aslan2023, Chakraborty2022reveal, Zhang21, eberendu2022systematic, zhu2023application}. They do not specifically address the integration of LLMs in vulnerability detection. Yao et al. \cite{Yao24} reviewed LLMs in the security and privacy domain, proposing their positive impacts, potential threats, and inherent threat. However, their analysis focuses on a broad overview of these issues rather than providing a methodological summary of LLM-based detection approaches. Xu et al. \cite{xu2024large} presented an overview of LLMs in the entire cybersecutiry domain, including malware analysis, network intrusion detection, etc. Our survey specifically focuses on LLM-based vulnerability detection with a more detailed summary of techniques and methodologies. At the same time, Zhou et al. \cite{zhou2024large} investigated how LLMs are adapted for vulnerability detection and repair. While their work provides valuable insights, our survey differs in several aspects: (1) by the time of writing, both OpenAI and Anthropic have released more powerful LLMs (GPT-4o, o1 and Claude 3.5 Sonnet) that have stronger inference abilities and larger context windows; (2) we conduct a comprehensive analysis of benchmarks and evaluation metrics for LLM-based vulnerability detection systems; (3) we focus more on the details of vulnerability detection and understanding.

\subsection{Large Language Models (LLMs)}
Large Language Models (LLMs) have emerged as a significant progress in the evolution of language models \cite{zhao2023survey}. The Transformer architecture has enabled unprecedented scaling capabilities. LLMs are characterized by their massive scale, typically incorporating hundreds of billions of parameters trained on vast corpus. Therefore, it leads to remarkable capabilities in general human tasks \cite{dai2023auggptleveragingchatgpttext}.

\subsection{Vulnerability Detection Problem}
\subsubsection{Domain Knowledge}
Some popular vulnerability databases, such as Common Weakness Enumeration (CWE)\footnote{\url{https://cwe.mitre.org/}}, Common Vulnerabilities and Exposures (CVE)\footnote{\url{https://www.cve.org/}}, Common Vulnerability Scoring System (CVSS)\footnote{\url{https://www.first.org/cvss/}} and National Vulnerability Database (NVD)\footnote{\url{https://nvd.nist.gov/}}, have been built to record the definition and evaluation of common vulnerabilities. CWE focuses on all vulnerabilities in the software development lifecycle (from development to maintenance.) Rather than focusing on specific real-world security vulnerabilities (e.g., Heartbleed and Log4Shell), CWE focuses on the root causes of these real-world vulnerabilities like Use-After-Free (CWE-416) and Out-of-bounds Write (CWE-787). CVE is a public community that identifies and catalogs security vulnerabilities in software and hardware. The community will allocate a unique identifier to each real-world vulnerability. For example, the identifier of Log4Shell is CVE-2021-44228. CVSS is a standardized framework for assessing the risk level of a vulnerability through a number of metrics: exploitability, impact, exploit code maturity, and remediation level, etc. These metrics result in an overall score on a scale of 0 to 10, and severity from low to critical. Log4Shell was given a CVSS score of 10 (severity: critical). NVD is a database that contains basic information about real-world vulnerabilities, such as CVE identifier, technical details of the vulnerability, CVSS score, and mitigation recommendations. For instance, NVD records that Log4Shell can be exploited to remotely execute code on the victim server by constructing malicious JNDI statements,  which is caused by Improper Input Validation (CWE-20) and Uncontrolled Resource Consumption (CWE-400).

\subsubsection{Vulnerability Detection}

% \begin{figure}[h]
%   \includegraphics[width=\linewidth]{images/Vul. Detection Sample.drawio.png}
%   \caption{An Example Workflow of Vul. Detection, Classification and Severity Prediction}
%   \Description{Sample problem}
%   \label{fig:Example_Problem}
% \end{figure}

\begin{figure}[H]
  \includegraphics[width=1\linewidth]{images/Vul._Detection_Sample.drawio.png}
  \caption{An Example Workflow of Vuln. Detection, Classification and Severity Prediction}
  \Description{Sample problem}
  \label{fig:Example_Problem}
\end{figure}


Vulnerability detection serves as the core focus of all selected papers in this survey, representing the primary application of LLMs in software security. The fundamental task can be formally defined as a binary classification problem:

Let $C_i$ denote the input source code and $VD_i$ represent an LLM-driven vulnerability detector. The output $Y_i \in \{0,1\}$ indicates the vulnerability status where $Y_i = 1$ indicates that the code is vulnerable, and $Y_i = 0$ indicates that the code is non-vulnerable.

% This formulation underlies the evaluation metrics used across our selected papers, enabling quantitative assessment of LLMs' effectiveness in identifying security vulnerabilities. While the formal definition appears straightforward, the complexity lies in training LLMs to accurately distinguish between vulnerable and non-vulnerable code patterns across diverse programming languages and vulnerability types. 
Figure \ref{fig:Example_Problem} shows an example workflow of vulnerability detection in most of selected researches, and two subproblems: vulnerability classification and severity prediction.

\subsubsection{Vulnerability Classification}
Beyond binary detection, some studies explore LLMs' capability in multi-class vulnerability classification to enhance model reliability. This task requires LLMs to not only identify the presence of vulnerabilities but also determine their specific types according to established standards like CWE.

Formally, vulnerability classification can be defined as: Let $C_i$ denote the input source code and $VC_i$ represent an LLM-driven vulnerability classifier. The output $Y_i$ indicates the specific vulnerability type:
$Y_i = VC_i(C_i) \in \{type_1, type_2, ..., type_n\}$, where $type_i$ could be vulnerability names (e.g., Buffer Overflow, SQL Injection) or standardized identifiers (e.g., CWE-119, CWE-89).

For example, when examining a code snippet, an LLM might not only detect its vulnerability but also classify it as "CWE-79: Cross-site Scripting (XSS)", providing more detailed guidance for security mitigation.

\subsubsection{Vulnerability Severity Prediction}
Some studies extend vulnerability analysis to include severity prediction alongside detection. This can be formulated as either a multi-class classification problem or a regression task, depending on the granularity of severity measurement.
Formally, let $C_i$ denote the input source code and $VS_i$ represent an LLM-driven severity predictor. The output \( Y_i \) can be defined in two forms: it may represent a severity score such as "low," "medium," or "high," based on the vulnerability severity level of the input source code \( C_i \).
% \begin{equation}
% Y_i = VS_i(C_i) \in \begin{cases}
% {low, medium, high}\\
% \textit{Severity Score}
% \end{cases}
% \end{equation}
Different studies have adopted varying approaches to severity prediction:
\begin{itemize}
\item Categorical Classification: Alam et al. \cite{alam2024detection} employ a three-level classification system (high, medium, low) for straightforward severity assessment.
\item Score Prediction: Fu et al.\cite{fu_chatgpt_2023} requires LLMs to predict numerical CVSS scores on a scale of 0 to 10, providing more precise severity measurements.
\end{itemize}
This additional severity information helps prioritize vulnerability remediation efforts and allocate security resources more effectively in practical applications.

\begin{figure*}[h]
  \includegraphics[width=\linewidth]{images/RQs.drawio.png}
  \caption{Research Questions}
  \Description{RQs}
  \label{fig:RQs}
\end{figure*}

\section{Research Results}
\subsection{Overview}
Our analysis shows that research on LLM-based vulnerability detection mainly focuses on C/C++, Java, and Solidity. Each language has unique challenges and research priorities. Studies on C/C++ focus on memory-related vulnerabilities, which are critical in this domain. Java research addresses framework-specific vulnerabilities and complex interactions across components in web applications. Solidity research targets vulnerabilities in smart contracts, which are central to blockchain security.

Recently, large decoder-only models, such as GPT and CodeLlama, have become the main choice due to their size and strong generalization abilities. These models are used in 65\% of fine-tuning experiments. For example, Alam et al. \cite{alam2024detection} fine-tuned GPT-4 and achieved 99\% accuracy in Solidity vulnerability detection. Before this, encoder-only models like CodeBERT and GraphCodeBERT were widely used. Advances in prompt engineering have also improved detection performance. Chain-of-Thought prompting is now common for large models and enhances their reasoning abilities for complex code \cite{ding_vulnerability_2024}.

Most current datasets focus on function-level and file-level vulnerabilities, with C/C++ as the dominant target language. Examples include the Devign \cite{zhou2019devign} and CVEFixes \cite{guru2021cvefixes} datasets. However, there is a lack of repository-level datasets that better reflect real-world scenarios. This gap limits the practical use of LLMs in vulnerability detection.

Future research should address challenges in cross-file and complex-context vulnerability detection. It should develop better methods for representing code semantics and create realistic repository-level datasets. These steps will improve the applicability and reliability of LLMs in this field.

\newtcolorbox{finding}[1]{
enhanced,
colback=white,
colframe=gray!75!black,
fonttitle=\bfseries,
title={Finding #1},
colbacktitle=gray!15,
coltitle=black,
attach boxed title to top left={yshift=-2mm, xshift=2mm},
boxed title style={
rounded corners,
size=small,
colframe=gray!75!black
},
boxrule=1pt,
sharp corners,
rounded corners
}

\subsection{RQ1. What LLMs have been applied to vulnerability detection?}
In this paper, we use the LLM categorization and taxonomy outlined by Pan et al. \cite{Pan_2024} and classify the primary LLMs into three architectural groups: 1) encoder-only, 2) encoder-decoder, and 3) decoder-only models. Due to space constraints, we will briefly introduce some representative LLMs in each category. Table~\ref{strengths_weaknesses} gives a clear overview of the strengths and weaknesses of each category of methods based on their inherent capabilities and limitations.

\textbf{Encoder-only LLMs}. Encoder-only LLMs utilize only the encoder component of the Transformer model \cite{Pan_2024}. These models are specifically designed to analyze and represent code or language context without generating output sequences, making them ideal for tasks that demand a detailed understanding of syntax and semantics. By employing attention mechanisms, encoder-only models encode input sequences into structured representations that capture essential syntactic and semantic information \cite{HAUROGNE2024100598}. In the software engineering (SE) domain, encoder-only models such as CodeBERT \cite{feng-etal-2020-codebert}, GraphCodeBERT \cite{guo2021graphcodebertpretrainingcoderepresentations}, CuBERT \cite{10.5555/3524938.3525412}, VulBERTa \cite{Hanif_2022}, CCBERT \cite{zhou2023ccbertselfsupervisedcodechange}, SOBERT \cite{he2024representationlearningstackoverflow}, and BERTOverflow \cite{tabassum2020codenamedentityrecognition} have been widely used.

\textbf{Encoder-Decoder LLMs}.Encoder-decoder models combine both the encoder and decoder components of the Transformer model, allowing them to handle tasks that require both understanding and generation of sequences. The encoder processes an input sequence, transforming it into a structured representation, which is then decoded to produce an output sequence. This structure makes encoder-decoder models versatile for tasks that involve translating, summarizing, or transforming text or code. Prominent examples include PLBART \cite{Ahmad2021UnifiedPF}, T5 \cite{raffel2023exploringlimitstransferlearning}, CodeT5 \cite{wang2021codet5identifierawareunifiedpretrained}, UniXcoder \cite{guo-etal-2022-unixcoder}, and NatGen \cite{10.1145/3540250.3549162}. 

\textbf{Decoder-only LLMs}. Decoder-only LLMs focus exclusively on the decoder component of the Transformer architecture to generate text or code based on input prompts. This approach leverages the model’s capacity to interpret and extend context, enabling it to produce complex and coherent sequences by predicting subsequent tokens. Widely adopted for tasks that emphasize generation, such as vulnerability detection and code suggestion, decoder-only models excel in identifying relevant patterns and potential issues within code. Notable examples in this category include the GPT series (GPT-2 \cite{Radford2019Language}, GPT-3 \cite{NEURIPS2020_1457c0d6}, GPT-3.5 \cite{openai2022gpt35}, GPT-4 \cite{openai2023gpt4}), as well as models tailored specifically for code in software engineering, such as CodeGPT \cite{lu2021codexgluemachinelearningbenchmark}, Codex \cite{chen2021evaluating}, Polycoder \cite{10.1145/3520312.3534862}, Incoder \cite{fried2023incodergenerativemodelcode}, CodeGen series\cite{nijkamp2023codegenopenlargelanguage}, Copilot \cite{github2023copilot}, Code Llama \cite{meta2023codellama}, and StarCoder \cite{li2023starcodersourceyou}. \cite{zhou_large_2024}

\begin{table}[h]
\centering
\caption{Strengths and Weaknesses of Different LLM Categories for Vulnerability Detection}
\label{strengths_weaknesses}
\begin{tabular}{l p{4cm} p{5cm}}
\toprule
\textbf{Category} & \textbf{Strengths} & \textbf{Weaknesses} \\
\midrule
\textbf{Encoder-Only} &
Strong code understanding and static analysis (e.g., CodeBERT) &
Poor at sequence generation and code modification \\
\textbf{Encoder-Decoder} &
Balanced analysis and generation capabilities (e.g., CodeT5) &
High compute cost; lacks task specialization \\
\textbf{Decoder-Only} &
Excels at code generation and patching (e.g., GPT-3.5) &
Limited in contextual understanding and dependency analysis \\
\bottomrule
\end{tabular}
\end{table}

In analyzing 58 studies on vulnerability detection, we identified 33 distinct LLMs used across various tasks. GPT-4 emerged as the most frequently used model, appearing in 29 instances, followed by GPT-3.5 with 25 mentions. Among the categories, encoder-only models represented 24.2\% of total usage, with CodeBERT, GraphCodeBERT, UniXcoder, and BERT as prominent examples. Encoder-decoder models, including CodeT5, made up 8.7\% of usage, serving dual roles in code generation and understanding. Decoder-only models, such as GPT series, CodeLlama, StarCoder, and WizardCoder, accounted for 67.1\% of usage and were primarily applied in code generation tasks.

% \begin{figure}[h]
%    \includegraphics[width=0.4\linewidth]{images/llm_distribution.png}
%   \caption{Distribution Word Cloud of LLMs for Vulnerability Detection}
%   \Description{llm_distribution}
%   \label{fig:llm_distribution}
% \end{figure}

In addition, Table \ref{top10_llm_used} presents the architecture and occurrences of the top 10 most commonly used LLMs in vulnerability detection research. It shows that most models for this task are decoder-only architectures, indicating a broader usage of this structure in detection tasks. Despite the general trend in the field, which often favors encoder-only architectures for understanding tasks, this table suggests that decoder-only models are also widely adopted for detection, likely due to their efficiency in processing and generating relevant sequences in code analysis.

\begin{table}[h]
\centering
\caption{Top 10 LLMs Used in Vulnerability Detection}
\label{top10_llm_used}
\begin{tabular}{l l l l}
\toprule
\textbf{Usages Ranking} & \textbf{LLM} & \textbf{Structure} & \textbf{Size} \\
\midrule
1  & GPT-4          & Decoder-only    & Unknown \\
2  & GPT-3.5        & Decoder-only    & Unknown \\
3  & BERT           & Encoder-only    & 109M \\
4  & CodeBERT       & Encoder-only    & 125M \\
5  & CodeLlama      & Decoder-only    & 7B, 13B, 34B, 70B \\
6  & LLaMA          & Decoder-only    & 7B, 13B, 70B \\
7  & StarCoder      & Decoder-only    & 15B  \\
8  & CodeT5         & Encoder-Decoder & 220M  \\
9  & Mistral        & Decoder-only    & 7B    \\
10 & GraphCodeBERT  & Encoder-only    & 125M  \\
\bottomrule
\end{tabular}
\end{table}


Among all LLMs, the GPT series (especially GPT-4 series) consistently performs well due to its robust capabilities in understanding and generating code. GPT-4 is widely regarded for advanced applications like vulnerability detection and code analysis, while GPT-3 and GPT-3.5 often serve as baselines or benchmarks in empirical studies. Specialized models such as CodeBERT and CodeT5 are frequently used for fine-tuned tasks involving code understanding and processing. Some research combines multiple models, such as pairing GPT-4 with GPT-3.5, to evaluate comparative performance or execute complementary tasks. This integrated approach, combining general-purpose LLMs with domain-specific models like CodeBERT, leverages the generalization power of LLMs and the task-specific precision of specialized models, resulting in improved performance and versatility.

\newtcolorbox{answerbox}[1]{
    enhanced,
    colback=white,
    colframe=blue!75!black,
    fonttitle=\bfseries,
    title={Answer to #1},
    colbacktitle=blue!15,
    coltitle=blue!75!black,
    attach boxed title to top left={yshift=-2mm, xshift=2mm},
    boxed title style={
        rounded corners,
        size=small,
        colframe=blue!75!black
    },
    boxrule=1pt,
    sharp corners,
    rounded corners
}

\begin{answerbox}{RQ1}
Recent trends show a shift from encoder-only models toward large decoder-only architectures like GPT and CodeLlama series in research. While encoder models still dominate non-fine-tuning studies (72.4\%), decoder-only models account for 65\% of fine-tuning experiments. Encoder-only and encoder-decoder architectures are increasingly positioned as baseline models for comparison.
\end{answerbox}

\subsection{RQ2. What benchmarks, dataset and metrics have been designed to evaluate vulnerability detection?}
In this section, we will start by examining the distribution of vulnerabilities across various programming languages and key software systems. We will then move on to discuss the benchmarks, datasets, and metrics commonly used in the field. Due to differences in design and feature, such as memory management in C/C++, unsafe deserialization in Python, and object injection and reflection in Java, different programming languages have different types of high-occurrence vulnerabilities. This is particularly significant, as many studies on vulnerability detection by LLMs focus on language-specific challenges \cite{Yıldırım2024_evaluating, ferreira2024-smartbugs, li_llm-assisted_2024}. Understanding these nuances is essential to evaluate and improve the effectiveness of vulnerability detection across different contexts.

We analyzed the CVE statistics across major software systems over the past five years (2019-2024), as shown in Table \ref{tab:cve_distribution}.


Table \ref{tab:cve_distribution} reveals several interesting patterns in vulnerability distribution across different software systems. Operating systems (Android, MacOS X, Linux Kernel, and Windows Server) dominate the vulnerability landscape, followed by web browsers (Chrome and Firefox) and development platforms (Gitlab). According to CVE statistics \cite{CWE_Metrics}, memory-related vulnerabilities have been the most prevalent type over the past five years. As memory-unsafe yet widely-used programming languages, C/C++ contributes to a significant number of memory corruption vulnerabilities, making vulnerability detection increasingly urgent. Based on our analysis of 56 selected papers, we collected statistics on all target programming languages, as shown in Figure \ref{fig:languages}.

\begin{table*}[h]
\caption{Distribution of CVEs across Different Software Systems}
\label{tab:cve_distribution}
\centering
\begin{tabular}{llccl}
\toprule
\textbf{Software System} & \textbf{Publisher} & \textbf{CVE Number} & \textbf{Primary Language} & \textbf{Software Type} \\
\midrule
Gitlab & Gitlab & 1068 & Ruby & Application \\
Chrome & Google & 3539 & C++ & Browser \\
Firefox & Mozilla & 2700 & C++ & Browser \\
Android & Google & 7215 & Java & Operating System \\
MacOS X & Apple & 3206 & C & Operating System \\
Linux Kernel & Linux & 5912 & C & Operating System \\
Windows Server 2022 & Microsoft & 1607 & C & Operating System \\
\bottomrule
\multicolumn{5}{l}{\small{* Data collected until November 3, 2024 from NVD database}} \\
\end{tabular}
\end{table*}

\begin{figure}[h]
  \centering
  \includegraphics[width=0.35\linewidth]{images/vulnerability_detection_languages_distribution_no_caption.png}
  \caption{Distribution of Target Programming Languages in LLM-based Vulnerability Detection Research}
  \Description{Target Language Distribution in Selected Papers}
  \label{fig:languages}
\end{figure}


\begin{finding}{I}
The research landscape shows a clear distribution in target programming languages: C/C++ dominates with 50\% of studies, followed by Java at 21.1\%. Solidity accounts for 11.8\% of research due to its critical role in smart contracts and financial transactions. The remaining 16.6\% covers other languages including Python, PHP, and Go.
\end{finding}

C/C++ remains the primary focus in vulnerability detection, covering 50\% of studies. This high proportion reflects memory-related vulnerabilities common in C/C++ projects. Java ranks second at 21.1\%, partly due to its popularity in enterprise-level software and Android development (Table \ref{tab:cve_distribution} shows Android contributes a large number of CVEs). Java’s type system and bytecode format also provide detailed information for LLMs, and its web applications often face SQL injection, Cross-Site Scripting (XSS), or insecure deserialization. Solidity follows at 11.8\%, as vulnerabilities in smart contracts directly threaten financial security on blockchain platforms. The remaining 16.6\% includes languages like Python, PHP, and Go.

To fine-tune LLMs and measure their performance in vulnerability detection, researchers have introduced various datasets, including BigVul \cite{fan2020bigvul}, CVEfixes \cite{guru2021cvefixes}, and Devign \cite{zhou2019devign}. Each dataset targets different scales, from identifying whether a single function has a vulnerability to scanning an entire GitHub repository. This variation reflects the diverse needs of vulnerability detection tasks.

% \begin{lstlisting}[language=C, escapechar=@, caption={An example of a pre-fix vunlerable function which has potential Unchecked Return Value (CWE-252) and CWE-787: Out-of-bounds Write (CWE-787).}, captionpos=b, label={lst:prefix_code}]
% void host_lookup(char *user_supplied_addr){
%     struct hostent *hp;
%     in_addr_t *addr;
%     char hostname[64];
%     in_addr_t inet_addr(const char *cp);

%     /* Routine that ensures user_supplied_addr is in the right format for conversion */
%     validate_addr_form(user_supplied_addr);
%     addr = inet_addr(user_supplied_addr);
%     @\highlightlinevuln{hp = gethostbyaddr(addr, sizeof(struct in\_addr), }@
%         @\highlightlinevuln{AF\_INET);}@
%     @\highlightlinevuln{strcpy(hostname, hp->h\_name);}@
% }
% \end{lstlisting}

% \begin{lstlisting}[language=C, escapechar=@, caption={An example of a post-fix nonvunlerable function.}, captionpos=b, label={lst:postfix_code}]
% void host_lookup(char *user_supplied_addr){
%     struct hostent *hp;
%     in_addr_t *addr;
%     char hostname[64];
%     in_addr_t inet_addr(const char *cp);

%     /* Routine that ensures user_supplied_addr is in the right format for conversion */
%     validate_addr_form(user_supplied_addr);
%     addr = inet_addr(user_supplied_addr);
%     @\highlightlinenonvuln{if (hp == NULL) $\{$}@
%         @\highlightlinenonvuln{return HOST\_ERROR;}@
%     @\highlightlinenonvuln{$\}$}@
%     @\highlightlinenonvuln{strncpy(hostname, hp->h\_name, sizeof(hostname) - 1);}@
%     @\highlightlinenonvuln{hostname[sizeof(hostname) - 1] = '$\backslash$0';}@
% }
% \end{lstlisting}

\textbf{Function-level}. Each data of these datasets contains the following attributes: function implementations (usually including both pre- and post-fix implementations, vulnerable flag (usually 1 for vulnerable and 0 for non-vulnerable). Frequently used datasets of this kind are BigVul \cite{fan2020bigvul} and Devign \cite{zhou2019devign} (also referred to FFmpeg and QEMU dataset). These datasets are often used for fine-tuning large models and evaluating the ability of LLMs to detect vulnerability, but they are not much practical. The reason is that real-world vulnerabilities are usually caused by multiple functions across files.


% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.6\linewidth]{images/file-level-dataset.pdf}
%   \caption{The test case 501129 of Juliet C/C++ 1.3 test suite as a file-level dataset.}
%   \label{fig:file_level}
% \end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth, height=0.25\textheight]{images/file-level-dataset.pdf}
        \caption{The test case 501129 of Juliet C/C++ 1.3 test suite as a file-level dataset.}
        \Description{A visualization of the file-level dataset structure from test case 501129 of Juliet C/C++ 1.3 test suite}

        \label{fig:file_level}
    \end{minipage}
    \hspace{0.05\textwidth} 
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/commit-level.pdf}
        \caption{An illustration for a commit-level dataset.}
        \Description{A sample visualization from the commit-level dataset showing the structure and relationships}
        \label{fig:commit_level}
    \end{minipage}
\end{figure}

\textbf{File-level}. Some datasets are structured not only in function level, but also in file level, like Juliet C/C++ \cite{2024nistjulietc} and Java 
\cite{2024nistjulietjava} test suites. Some of the vulnerabilities in the Juliet test suites largely mimic the structure of real-world vulnerabilities, including, but not limited to, cross-file function calls or cross-file access to global variables. Such vulnerabilities with complex contexts provide a significant challenge for LLM to detect vulnerabilities. A test case of Juliet C/C++ test suite has been shown in Figure \ref{fig:file_level}. We can see that in test case 501129, the files and lines where the vulnerabilities are introduced have been marked in the trace. This detail provides clues to LLMs' ability to detect vulnerabilities introduced by multiple files, but also suggests the need to improve LLMs' ability to detect across files.

% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.5\linewidth]{images/commit-level.pdf}
%   \caption{An illustration for a commit-level dataset (Pan2023).}
%   \label{fig:commit_level}
% \end{figure}

\textbf{Commit-level}. Many open-source software use the GitHub platform and modify the source code by submitting a commit. Obviously, trusted maintainers can submit commits containing malicious changes that make the target software vulnerable \cite{wu2021feasibility}. So it is also necessary to apply vulnerability detection for each commit. CVEfixes \cite{guru2021cvefixes} and Pan2023 \cite{pan2023} are both commit-level datasets. These datasets typically include the repository URL, commit hash (a unique identifier for each commit), and diff files (showing the differences before and after the commit). In this way, LLMs can analyze the code changes before and after the commit to determine whether the commit is vulnerable or not, and also fetch contextual information through the repository URL.

\textbf{Repository- and application-level}. These types of datasets are usually for vulnerability detection of the entire project. CWE-Bench-Java \cite{li_llm-assisted_2024} is a repository-level datasets focusing on Java projects. Each repository comes with metadata about the vulnerability, such as CWE ID, CVE ID, remediation commits and vulnerability version. This makes analysis and validation more systematic and reliable. And Ghera \cite{mitra2017ghera} is an application-level datasets. Each item contains three applications: a vulnerable application that contains vulnerability X, a malicious application that can attack the vulnerable application using vulnerability X, and a secure application that has no vulnerability X. Each item comes with instructions to build and run the application to demonstrate the vulnerability and its exploitation, thus verifying the presence or absence of the vulnerability and exploitation. An example of commit-level dataset is shown in Figure \ref{fig:commit_level}.

There are also other kinds of datasets. For example, with the rise of blockchain, datasets for smart contract have been built. These datasets, like FELLMVP \cite{luo2024fellmvp}, contains many smart contracts (contract-level) with logical vulnerabilities (e.g. reentrancy attacks and integer overflow/underflow). There are also datasets that focus only on specific vulnerabilities. For example, Code Gadgets \cite{li2018vuldeepecker} only focuses on two types of vulnerabilities in C/C++ programs, buﬀer error vulnerability (CWE-119) and resource management error vulnerability (CWE-399). SolidFi \cite{2020ghaleb-solifi} is only based on injection vulnerability. Frequently used datasets for vulnerability detection are shown in Table \ref{tab:datasets}. The number in parentheses in the size column represents the number of items that are vulnerable.
%TODO: # CWE


\begin{table*}
\centering
\caption{Datasets frequently used in LLMs for vulnerability detection}
\begin{adjustbox}{max width=\textwidth}
\begin{threeparttable}
\begin{tabular}{lllcllccc}
\toprule
\textbf{Dataset} & \textbf{Size*} & \textbf{Language} & \textbf{\# Vuln. Types**}  & \textbf{Scope} & \textbf{Source} & \textbf{labeled} & \textbf{Open-source}\\
\midrule
% Example rows
BigVul \cite{fan2020bigvul} & 264,919 (11,823) & C/C++ & 91 & function & real-world & {\color{green}\ding{51}} & \href{https://github.com/ZeoVan/MSR_20_Code_vulnerability_CSV_Dataset}{{\color{green}\ding{51}}} \\
CVEfixes \cite{guru2021cvefixes} & 12,107 (all) & C/C++, Python, PHP & 180 & commit & real-world & {\color{green}\ding{51}} & \href{https://github.com/secureIT-project/CVEfixes}{{\color{green}\ding{51}}}\\
Devign \cite{zhou2019devign} & 27,318 (12,460) & C/C++ & N/A & function & real-world & {\color{green}\ding{51}} & \href{https://github.com/epicosy/devign}{{\color{green}\ding{51}}}\\
Juliet C/C++ \cite{2024nistjulietc} & 64,099 (all) & C/C++ &  118 & file & synthesized & {\color{green}\ding{51}} & \href{https://samate.nist.gov/SARD/test-suites/112}{{\color{green}\ding{51}}}\\
ReVeal \cite{Chakraborty2022reveal} & 18,169 (1,664) & C/C++ & N/A & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/VulDetProject/ReVeal?tab=readme-ov-file}{{\color{green}\ding{51}}}\\
D2A \cite{zheng2021d2a} & 1,295,623 (18,653) & C/C++ & N/A & function & real-world & {\color{green}\ding{51}} & \href{https://github.com/IBM/D2A}{{\color{green}\ding{51}}}\\
SeVc \cite{li2022sevc} & 420,627 (56,395)  & C/C++ & 126 & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/SySeVR/SySeVR}{{\color{green}\ding{51}}}\\
DiverseVul \cite{chen2023diversevul} & 349,437 (18,945) & C/C++ & 150 & function & real-world & {\color{green}\ding{51}} & \href{https://github.com/wagner-group/diversevul?tab=readme-ov-file}{{\color{green}\ding{51}}}\\
SARD \cite{NIST_SARD} & $>$ 5,000,000 (all) & C/C++, Java, PHP & 150 & file & mixed & {\color{green}\ding{51}} & \href{https://samate.nist.gov/SARD/}{{\color{green}\ding{51}}}\\
Juliet Java \cite{2024nistjulietjava} & 28,281 (all) & Java & 112 & file & synthesized & {\color{green}\ding{51}} & \href{https://samate.nist.gov/SARD/test-suites/111}{{\color{green}\ding{51}}}\\
PrimeVul\cite{ding_vulnerability_2024} & 235,768 (6,968) & C/C++ & 140 & function & real-world  & {\color{green}\ding{51}} & \href{https://github.com/DLVulDet/PrimeVul}{{\color{green}\ding{51}}}\\
MAGMA \cite{ahmad2024_magma} & 138 (all) & C/C++, Lua, PHP & 11 & repository & real-world & {\color{green}\ding{51}} & \href{https://hexhive.epfl.ch/magma/}{{\color{green}\ding{51}}}\\
Smartbugs-curated \cite{ferreira2024-smartbugs} & 143 (all) & Solidity & 10 & contract & mixed & {\color{green}\ding{51}} & \href{https://github.com/smartbugs/smartbugs-curated}{{\color{green}\ding{51}}}\\
Smartbugs-wild \cite{ferreira2024-smartbugs} & 47,518 (N/A) & Solidity & N/A & contract & real-world & {\color{red}\ding{55}} & \href{https://github.com/smartbugs/smartbugs-wild}{{\color{green}\ding{51}}}\\
Code Gadgets \cite{li2018vuldeepecker} & 61,638 (17,725) & C/C++ & 2 & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/CGCL-codes/VulDeePecker}{{\color{green}\ding{51}}}\\
VulDeeLocator \cite{li2022_vuldeelocator} & 198,142 (40,450) & LLVM IR & 4 & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/VulDeeLocator/VulDeeLocator}{{\color{green}\ding{51}}}\\
Choi2017 \cite{choi2017_endtoend} & 14,000 ($\approx$ 7,054)  & C/C++ & 4 & function & synthesized & {\color{red}\ding{55}} & \href{https://github.com/mjc92/buffer_overrun_memory_networks}{{\color{green}\ding{51}}}\\
Guo2024	\cite{guo_outside_2024} & 13,532 (6,766)   & C/C++ & N/A & function & real-world & {\color{green}\ding{51}} & \href{https://zenodo.org/records/10975439}{{\color{green}\ding{51}}}\\
SVEN \cite{he2023_large} & 1,606 (all) & C/C++, Python & 9 & commit &  real-world & {\color{green}\ding{51}} & \href{https://github.com/eth-sri/sven}{{\color{green}\ding{51}}}\\
Lin2017	\cite{lin2017} & 6,486 (317) & C/C++ & N/A & function & real-world & {\color{green}\ding{51}} & \href{https://github.com/DanielLin1986/function_representation_learning?tab=readme-ov-file}{{\color{green}\ding{51}}}\\
Ye2024 \cite{ye2024_detecting} & 100 (all) & C/C++ & 1 & application & real-world & {\color{red}\ding{55}} & {\color{red}\ding{55}}\\
ExtractFix \cite{gao2021_beyond} & 7 (all) & C/C++ & 6 & application & real-world & {\color{green}\ding{51}} & \href{https://extractfix.github.io/}{{\color{green}\ding{51}}}\\
DBGbench \cite{marcel2017_how} & 27 (all) & C/C++ & 6 & application & real-world & {\color{green}\ding{51}} & \href{https://dbgbench.github.io/}{{\color{green}\ding{51}}}\\
VulBench \cite{gao_how_2023} & 455 (all) & C/C++, decompiled code & 9 & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/Hustcw/VulBench}{{\color{green}\ding{51}}}\\
VCMatch \cite{wang2022_vcmatch}	& 1,669 (all) & C/C++, Java and PHP & 7 & commit & real-world & {\color{green}\ding{51}} & \href{https://figshare.com/s/0f3ed11f9348e2f3a9f8?file=32403518}{{\color{green}\ding{51}}}\\
Pan2023 \cite{pan2023} & 6,541 (all) & C/C++, PHP, Java & 78 & commit & real-world & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
Ullah2023 \cite{ullah_llms_2024} & 228 (all) &  C/C++, Python & 8 & function & mixed & {\color{green}\ding{51}} & \href{https://github.com/ai4cloudops/SecLLMHolmes}{{\color{green}\ding{51}}}\\
Fang2024 \cite{fang_llm_2024} & 15 (all) & Go, Java, PHP & 9 & application & real-world & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
Ponta2019 \cite{ponta2019_manually} & 1,282 (all) & Java & 6 & commit & real-world & {\color{green}\ding{51}} & \href{https://github.com/SAP/project-kb/tree/main/MSR2019}{{\color{green}\ding{51}}}\\
CWE-Bench-Java \cite{li_llm-assisted_2024}& 120 (all) & Java & 4 & repository & real-world & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
Vulcorpus \cite{kouliaridis_assessing_2024} & 100 (all) & Java & 10 & function & synthesized & {\color{green}\ding{51}} & \href{https://github.com/billkoul/vulcorpus-2024}{{\color{green}\ding{51}}}\\
Vul4j \cite{bui2022_vul4j} & 79 (all) & Java & 25 & commit & real-world & {\color{green}\ding{51}} & \href{https://github.com/tuhh-softsec/vul4j}{{\color{green}\ding{51}}}\\
Ghera \cite{mitra2017ghera} & 69 (all) & Java & 25 & application & synthesized & {\color{green}\ding{51}} & \href{https://secure-it-i.bitbucket.io/ghera/index.html}{{\color{green}\ding{51}}}\\
VjBench	\cite{wu2023_effective}& 42 (all) &  Java & 23 & commit & real-world & {\color{green}\ding{51}} & \href{https://github.com/lin-tan/llm-vul}{{\color{green}\ding{51}}}\\
Yıldırım2024 \cite{Yıldırım2024_evaluating}& 40 (all) & Python & 10 & function & synthesized  & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
FELLMVP \cite{luo2024fellmvp} & 15,637 (820) & Solidity & 8 & contract & real-world & {\color{green}\ding{51}} & \href{https://drive.google.com/drive/folders/1uSXaY7vOvcwQIwXs5JwD9C2hxK9bFMsZ}{{\color{green}\ding{51}}}\\
SolidiFI \cite{2020ghaleb-solifi} & 50 (all) &  Solidity & 7 & contract & real-world & {\color{green}\ding{51}} & \href{https://github.com/DependableSystemsLab/SolidiFI}{{\color{green}\ding{51}}}\\
Ma2024 \cite{ma_combining_2024} & 3,544 (1,734) & Solidity & 5 & function & mixed & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
SC-LOC \cite{zhang2024_empirical}& 1,369 (all) & Solidity & N/A & function & real-world & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
LLM4Vuln \cite{sun_llm4vuln_2024}  & 194 (97) & Java, Solidity & 82 & function & real-world & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
SmartFix \cite{so2023_smartfix}& 361 (all) & Solidity & 5 & contract & mixed & {\color{green}\ding{51}} & {{\color{red}\ding{55}}}\\
Hu2023 \cite{hu_large_2023} & 13 (all) & Solidity & 5 & contract & real-world & {\color{green}\ding{51}} & \href{https://github.com/git-disl/GPTLens}{{\color{green}\ding{51}}}\\
\bottomrule
\end{tabular}
\begin{tablenotes}
\small
\item[*] The number in parentheses represents the number of items that are vulnerable. For example, there are a total of 264,919 functions in the BigVul dataset, of which 11,823 are vulnerable.
\item[**] "N/A" indicates that the authors did not provide detailed information about the number of vulnerabilities in their papers.
\end{tablenotes}
\end{threeparttable}
\end{adjustbox}
\label{tab:datasets}
\end{table*}

%takeaway by chen
% Analysis of existing datasets reveals a significant limitation: most are function-level, whereas real-world vulnerability detection requires repository-wide analysis including commit history and multi-file projects. Additionally, these datasets cover limited vulnerability types. To make LLMs practical for vulnerability detection, we need more comprehensive datasets at file, commit, and repository levels with broader vulnerability coverage.
%takeaway end

\begin{finding}{II}
Current vulnerability datasets exhibit two major limitations: (1) Language imbalance - with C/C++ dominating at around 60\% coverage while Java, despite being widely used in enterprise and Android development, lacks comprehensive datasets; (2) Scope gaps - there is a significant shortage of repository-level datasets that reflect real-world development scenarios where vulnerabilities often span multiple files and dependencies. This scarcity of realistic, large-scale repository datasets poses a critical limitation for practical applications of LLMs in vulnerability detection.
\end{finding}

The evaluation of LLM-based vulnerability detection systems requires multiple metrics. These metrics can be categorized into three groups: classification metrics, generation metrics, and efficiency metrics.

\subsubsection{Evaluation Metrics}
% \begin{figure}[h]
% \centering
% \includegraphics[width=0.5\linewidth]{images/Metrics.drawio.png}
% \caption{Evaluation Metrics of LLM in Vuln. Detection }
% \Description{Metrics}
% \label{fig:metrics}
% \end{figure}
\subsubsection{Classification Metrics}
Vulnerability detection systems commonly use several standard metrics: Accuracy ($\frac{TP + TN}{TP + TN + FP + FN}$) measures overall correctness; Precision ($\frac{TP}{TP + FP}$) indicates true positive rate; Recall ($\frac{TP}{TP + FN}$) shows vulnerability coverage; and F1-Score ($2 \times \frac{Precision \times Recall}{Precision + Recall}$) balances precision and recall. For imbalanced datasets, Matthews Correlation Coefficient (MCC) is also useful:
\begin{equation}
MCC = \frac{TP \times TN - FP \times FN}{\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}.
\end{equation}
\subsubsection{Generation and Explainability Metrics}
To evaluate the quality of generated vulnerability descriptions, Alam et al. and Ghosh et al. \cite{alam2024detection} and \cite{ghosh_cve-llm_2024} employ BLEU and ROUGE metrics. BLEU considers brevity penalty and n-gram precision, while ROUGE measures overlap between generated and reference texts. These metrics help assess both the accuracy and explainability of LLM-based vulnerability detection systems.

% \newtcolorbox{finding}[1]{
% enhanced,
% colback=white,
% colframe=gray!75!black,
% fonttitle=\bfseries,
% title={Finding #1},
% colbacktitle=gray!15,
% coltitle=black,
% attach boxed title to top left={yshift=-2mm, xshift=2mm},
% boxed title style={
% rounded corners,
% size=small,
% colframe=gray!75!black
% },
% boxrule=1pt,
% sharp corners,
% rounded corners
% }
% \begin{finding}{III}
%  As LLMs evolve in vulnerability detection, evaluation metrics have expanded beyond traditional classification metrics (accuracy, precision, etc.) to address more complex real-world requirements. Binary classification of vulnerable code is no longer sufficient for practical development needs. This has led to the emergence of new research directions including vulnerability classification, localization, and explanation. Consequently, more sophisticated metrics have been adopted: Matthews Correlation Coefficient (MCC) for handling imbalanced vulnerability datasets, BLEU and ROUGE scores from NLP for assessing explanation quality, and execution time measurements for evaluating detection efficiency on large codebases. This diversification of metrics reflects the field's progression from simple detection towards more comprehensive vulnerability analysis and understanding.
% \end{finding}

% \begin{finding}{III}
% As LLMs evolve in vulnerability detection, evaluation metrics have expanded beyond traditional classification metrics (accuracy, precision, etc.) for real-world complex requirements. New approaches is also no longer limited to binary classification and is gradually expanding to vulnerability classification, localization, and explanation tasks. Consequently, more sophisticated metrics have been adopted: Matthews Correlation Coefficient (MCC) for handling imbalanced vulnerability datasets, BLEU and ROUGE scores for assessing explanation quality, and execution time measurements for evaluating detection efficiency on large codebases. The diversification of metrics marks a shift from simple detection to comprehensive vulnerability analysis and understanding.
% \end{finding}

% \newtcolorbox{answerbox}[1]{
%     enhanced,
%     colback=white,
%     colframe=blue!75!black,
%     fonttitle=\bfseries,
%     title={Answer to #1},
%     colbacktitle=blue!15,
%     coltitle=blue!75!black,
%     attach boxed title to top left={yshift=-2mm, xshift=2mm},
%     boxed title style={
%         rounded corners,
%         size=small,
%         colframe=blue!75!black
%     },
%     boxrule=1pt,
%     sharp corners,
%     rounded corners
% }

% \begin{answerbox}{RQ2}
% Most benchmarks and datasets for LLM-based vulnerability detection focus on function-level or file-level granularity, with C/C++ as the dominant target language. Examples include Devign and CVEFixes for function-level data and the Juliet for file-level vulnerabilities. Classification metrics like accuracy, precision, recall, and F1-score are widely used, with Matthews Correlation Coefficient (MCC) adopted for imbalanced datasets. Metrics such as BLEU and ROUGE assess the quality of generated descriptions, while execution time evaluates efficiency. However, current datasets are limited by their focus on C/C++ and lack of repository-level data reflecting real-world scenarios. These gaps hinder LLMs' ability to generalize across languages and detect complex, multi-file vulnerabilities. Future research should create diverse, large-scale datasets for underrepresented languages and repository-level data to improve LLM applicability and performance.
% \end{answerbox}

\begin{answerbox}{RQ2}
Most benchmarks and datasets for LLM-based vulnerability detection focus on function-level or file-level scope, with C/C++ as the dominant target language.  Classification metrics, like accuracy and precision, are widely used, with Matthews Correlation Coefficient (MCC) adopted for imbalanced datasets. Metrics such as BLEU and ROUGE assess the quality of generated descriptions, while execution time evaluates efficiency. However, current datasets are limited by their focus on C/C++ and lack of repository-level data. These gaps hinder LLMs' ability to generalize across languages and detect complex, multi-file vulnerabilities. Future research should create diverse, large-scale datasets to simulate the real-world scenarios. 
\end{answerbox}


\subsection{RQ3. What techniques are used in LLMs for vulnerability detection?}
Currently, LLM-based vulnerability detection faces several key challenges: (1) data leakage leading to inflated performance metrics, (2) difficulty in understanding complex code context, (3) positional bias in large context windows causing information loss, and (4) high false positive rates and poor performance on zero-day vulnerabilities. Researchers have conducted extensive studies to address these challenges. This section summarizes and discusses current techniques applied to LLM-based vulnerability detection.
\subsubsection{Code Data Preprocessing}
Code processing techniques serve two primary objectives: (1) optimizing the utilization of LLMs' limited context window to improve efficiency, and (2) enhancing LLMs' comprehension of semantic information within the code to improve vulnerability detection capability. 

\textbf{Abstract Syntax Tree Analysis}. Abstract Syntax Tree (AST) provides a hierarchical representation of program structure, where code elements are organized into a tree format based on their syntactic relationships \cite{welty1997augmenting}. This structural representation eliminates non-essential syntax details while preserving the semantic relationships between code components.  Fig. \ref{fig:ast} represents the AST for a code snippet. AST applications in vulnerability detection can be categorized into several primary approaches: code segmentation and structural representation, where ASTs parse code into function-level segments for efficient processing within LLMs' context limits, as demonstrated by Zhou et al. \cite{zhou_comparison_2024} and Mao et al. \cite{mao_towards_2024}; semantic enhancement, where ASTs are integrated with natural language annotations to form structured comment trees (SCT), as implemented in SCALE framework \cite{wen_scale_2024} to capture vulnerability patterns beyond syntactic relationships; multi-graph analysis, where ASTs are combined with control flow graphs (CFG) and data flow graphs (DFG) to provide comprehensive code structure analysis, as shown in DefectHunter \cite{wang2023defecthunter}; pattern detection integrated with graph attention networks (GATs), exemplified by VulnArmor \cite{sindhwad_vulnarmor_2024}, GRACE \cite{lu_grace_2024}, and \cite{mahyari_harnessing_2024}; and error localization for code evolution as demonstrated in \cite{zhang2024_empirical}. Empirical evaluations across diverse datasets including FFmpeg, QEMU, and Big-Vul demonstrate that AST-augmented approaches significantly improve vulnerability detection performance.

% \begin{figure}[h]
%    \includegraphics[width=0.6\linewidth]{images/ast.png}
%   \caption{An Example Illustrating AST}
%   \Description{ast}
%   \label{fig:ast}
% \end{figure}

% chenzc todo: 这里还可以加点pdg的叙述
\textbf{Data/Control Flow Analysis}. AST lacks a representation of the data and control flow of a program. Thus, in some papers \cite{tamberg_harnessing_2024, liu_exploration_2024, khare2023understanding, lu_grace_2024, sun_llm4vuln_2024, li_llm-assisted_2024}, data flow graph (DFG) and control flow graph (CFG) have been applied to help LLMs understand the interprocedural data flow and control flow in a program. Fig. \ref{fig:dfg} is a concise example illustrating a Java method and its corresponding DFG. DFG summarizes the possible execution paths in a program, using nodes (or basic blocks, i.e., statements that are executed sequentially without any branching operation) to represent program constructs, and edges to represent the flow of data. CFG has the same basic blocks as nodes, but edges are used to represent the branching operations between basic code blocks. There are two main usages of DFG/CFG like providing extra contextual information in prompt and knowledge base. Combine source code with its DFG and CFG into prompt will result in a significant improvement in LLm's performance in identifying vulnerabilities \cite{tamberg_harnessing_2024, liu_exploration_2024, khare2023understanding, lu_grace_2024}; Sun et al. has proposed that DFG and CFG can be used in knowledge base, with graph-based similarity-search algorithm, to provide LLMs with the information of code segments with similar data and control flow structure \cite{sun_llm4vuln_2024}. Except for DFG and CFG, call graph has been used in vulnerability detection \cite{luo2024fellmvp} to give LLM more information about dependencies between functions.

% \begin{figure}[h]
%    \includegraphics[width=0.6\linewidth]{images/dfg.png}
%   \caption{An Example Illustrating DFG}
%   \Description{dfg}
%   \label{fig:dfg}
% \end{figure}

\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/ast.png}
        \caption{An Example Illustrating AST}
        \label{fig:ast}
    \end{minipage}
    \hspace{0.05\textwidth} 
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/dfg.png}
        \caption{An Example Illustrating DFG}
        \label{fig:dfg}
    \end{minipage}
\end{figure}

% chenzc todo: 可以那篇文章举个更详细的例子说明哪些document，vulnerability和code被作为knowledge 加入
\textbf{
Retrieval-augmented Generation}. Retrieval Augmented Generation (RAG) enhances the capabilities of LLMs by integrating an information retrieval system that provides extra related information to LLMs \cite{nlprag}. Fig. \ref{fig:ast} illustrates the principle of RAG. LLMs receives user input and applies a searcher to find relevant documents or pieces of information from a knowledge base. The retrieved information is combined with the original prompt to generate a response. In this way, RAG can solve the problem of insufficient knowledge of LLMs in certain domains, illusions and that large language models cannot update data in real time. Many papers have discussed how to choose the right knowledge as a high priority when building RAG for LLMs. \cite{sun_llm4vuln_2024, cao_llm-cloudsec_2024, mathews_llbezpeky_2024, kouliaridis_assessing_2024, keltek_lsast_2024}. Cao et al. directly use CWE database as external knowledge \cite{cao_llm-cloudsec_2024}. Many papers focused on combining code snippets, static analysis results with documentation of corresponding vulnerabilities as knowledge \cite{mathews_llbezpeky_2024, kouliaridis_assessing_2024, keltek_lsast_2024}. In addition to the knowledge mentioned above, Sun et al. used GPT-4 to summarize existing knowledge and thus create two knowledge bases (VectorDB with Vulnerability Report and Summarized Knowledge) \cite{sun_llm4vuln_2024}. RAG has been proven to improve LLM's ability to detect vulnerabilities \cite{keltek_lsast_2024}.

% \begin{figure}[h]
%    \includegraphics[width=0.5\linewidth]{images/rag.png}
%   \caption{Principle of RAG}
%   \Description{cot}
%   \label{fig:rag}
% \end{figure}

\textbf{Program Slicing}. Program slicing technique has been used to reduce vulnerability-irrelevant lines of code and keep critical lines related to trigger vulnerability \cite{mahyari_harnessing_2024, zhang2024_empirical, purba_software_2023, cao_realvul_2024}. Purba et al. apply program slicing technique to extract code snippets for buffer-overflow detection \cite{purba_software_2023}. These code snippets usually contains key functions, like \texttt{strcmp} and \texttt{memset}, and statements related to call these functions. Cao et al. use program slicing in similar way \cite{cao_realvul_2024}. They first find all potential vulnerability triggers, and apply program slicing technique to collect all statements related to these triggers. While Purba et al. \cite{purba_software_2023} and Cao et al. \cite{cao_realvul_2024} only use the program slicing technique for code preprocess, Zhang et al. proposes to fine-tune LLMs with sliced code to improve the performance of LLMs vulnerability detection \cite{zhang2024_empirical}. Instead of setting explicit slicing criteria, LLMs learns to segregate vulnerable lines of code from a given function during training. This approach helps the model to focus on relevant parts of the code and identify vulnerabilities more accurately. The program slicing technique has been shown to improve LLMs' ability to detect vulnerability \cite{purba_software_2023, zhang2024_empirical}.

\textbf{LLVM Intermediate Representation}.
By converting source code to LLVM Intermediate Representation (IR) \cite{LLVM:CGO04}, analysis and detection methods do not need to be specifically adapted to different programming languages. This improves the versatility of vulnerability detection methods, and LLVM IR also preserves program structure and semantics, making it easier for LLM to analyze potential dependencies in the code. But the downside is obvious: LLVM IR doesn't work with Java and Javascript. To make the approach generalizable to programming languages, the authors converted the source code to LLVM IR and trained LLMs on these IR \cite{mahyari_harnessing_2024}.
% 
% \paragraph{\textbf{Dangerous Flow Extraction}} 
% 这个没必要再单独开了吧，这个更像是enhancing static analysis那篇文章造的词，实际上还是用的CFG/DFG

% TODO: Abstract Syntax Tree, Program Slicing, Dangerous Flow Extraction, Data Flow Graph, LLVM IR form ...... 

%takeaway by chen
We can see that in the field of LLMs in vulnerability detection, the techniques mainly comes from traditional software analysis and LLM research. And basically the outputs of these techniques are used as part of the prompts to evaluate the LLMs' vulnerability detection capabilities. These approaches not only optimize the efficiency of LLMs' context window utilization, but also improve its understanding of potential vulnerabilities by preserving or extracting semantic information from the code. However, it also inevitably increases token consumption, and there is also the possibility that too much prompt content may reduce the ability of LLMs to detect vulnerabilities.
%takeaway end

\begin{finding}{III}
Our analysis reveals that 41.3\% of studies employed code processing techniques - including graph representations, Retrieval-Augmented Generation (RAG), and code slicing - to better utilize LLMs' limited context windows. While these approaches show modest improvements over direct code prompting, their effectiveness diminishes significantly when dealing with complex, cross-file vulnerabilities. Notably, as larger LLMs (like the GPT series) emerge, the performance gains from the models themselves tend to outweigh those from code processing techniques. This suggests that while current code semantic processing methods offer benefits, developing more effective ways to represent complex code context remains a crucial challenge.
\end{finding}

\subsubsection{Prompt Engineering Techniques}
This is one of the most widely used strategies for optimizing LLM-based vulnerability detection systems, as it enables precise control over model responses by tailoring input prompts.

\textbf{Chain-of-Thought Prompting}. Chain-of-Thought (CoT) Prompting is a technique where LLMs are guided to follow step-by-step instructions to enhance reasoning accuracy before generating a final output. Fig. \ref{fig:cot_fsl} illustrates the COT reasoning process for vulnerability detection. In LLM-based vulnerability detection, CoT prompting involves instructing the model to first summarize the functionality of the given code, then assess potential errors that might introduce vulnerabilities, and finally determine if the code is vulnerable. This structured prompt strategy has been shown to improve precision and recall in vulnerability detection tasks by helping the model reason through complex code in a more organized manner. However, while CoT prompting often enhances precision, its impact on recall can vary across different scenarios \cite{sun_llm4vuln_2024}. 

% \begin{figure}[h]
%    \includegraphics[width=0.7\linewidth]{images/cot_fsl.png}
%   \caption{Principles of Chain-of-Thought and Few-shot Learning}
%   \Description{cot_fsl}
%   \label{fig:cot_fsl}
% \end{figure}
\begin{figure}[h]
    \centering
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/mlevel.png}
        \caption{An Instance of Multi-level Prompting for Vulnerability Detection}
        \label{fig:mlevel}
    \end{minipage}
    \hspace{0.05\textwidth} 
    \begin{minipage}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{images/cot_fsl.png}
        \caption{Principles of Chain-of-Thought and Few-shot Learning}
        \label{fig:cot_fsl}
    \end{minipage}
\end{figure}


\textbf{Few-shot Learning}. In LLM-based vulnerability detection, few-shot learning (FSL) enables models to leverage a small set of labeled examples within prompts to improve task-specific performance. In this approach, vulnerability detection can be enhanced by embedding classification standards, such as CWE, directly into the prompt \cite{gao_how_2023}. By incorporating CWE vulnerability categories—complete with numbers and descriptive names—the model gains contextual knowledge that aids in identifying and classifying vulnerabilities accurately. Fig. \ref{fig:cot_fsl} also illustrates the principle of few-shot learning, where the model is provided with labeled examples (e.g., Solidity Code 1 and Solidity Code 2) to understand the task before analyzing a new testing set. These examples, combined with the task-specific prompt, guide the fine-tuned LLM to generate accurate outputs.

\textbf{Hierarchical Context Representation}. Hierarchical context representation is a technique used to manage the context length limitations of LLMs when analyzing extensive codebases. In vulnerability detection, code can be organized hierarchically into modules, classes, functions, and statements. By representing the code in this hierarchical manner, the LLM can process and analyze the code at different levels of abstraction. This approach allows the model to focus on higher-level structures before delving into detailed code segments, effectively managing the context and improving the detection of vulnerabilities within the constraints of LLMs’s maximum input length.

\textbf{Multi-level Prompting}. The multi-level prompting strategy involves breaking down the vulnerability detection task into multiple prompts, each targeting a specific level of analysis. Instead of presenting the entire code and task in a single prompt, the strategy divides the process into stages. For example, the first prompt may ask the LLM to provide a high-level summary of the code's functionality. The second prompt might focus on identifying potential security issues, and subsequent prompts could request detailed analyses of specific sections. This layered approach helps the LLM to systematically process complex code, enhancing its ability to detect vulnerabilities by focusing on one aspect at a time. Fig. \ref{fig:mlevel} illustrates an instance of multi-level prompting.

% \begin{figure}[h]
%    \includegraphics[width=0.7\linewidth]{images/mlevel.png}
%   \caption{An Instance of Multi-level Prompting for Vulnerability Detection}
%   \Description{mlevel}
%   \label{fig:mlevel}
% \end{figure}

\textbf{Multiple Prompt Agents and Templates}. This technique employs several specialized prompt agents, each designed with a specific template to perform distinct roles in the vulnerability detection process. For instance, one agent might be tasked with code summarization using a template that guides the LLM to extract key functionalities. Another agent could focus on vulnerability identification, utilizing a template that prompts the model to look for common security weaknesses. By using multiple agents with tailored templates, the system leverages the strengths of each specialized prompt, leading to more accurate and comprehensive vulnerability detection results.

In general, prompt engineering effectiveness varies with model size. Small models benefit from few-shot learning and structured prompting to compensate for limited capabilities, while large models perform better with chain-of-thought prompting and zero-shot approaches due to their stronger generalization abilities and domain knowledge.

\begin{finding}{IV}
As LLMs' inherent capabilities grow, their context window capacity expands accordingly. Chain-of-Thought (CoT) prompting emerges as the dominant approach for large models (>10B parameters), with 100\% of recent studies adopting CoT to enhance generation. For smaller models with limited text processing capacity, zero-shot or minimal few-shot approaches prove more effective, as CoT and extra few-shot examples may cause irrelevant output.
\end{finding}

\begin{table*}
\centering
\caption{Fine-tuning Methods in LLM-based Vulnerability Detection}
\adjustbox{max width=\textwidth}{%
\begin{tabular}{lcccccc}
\toprule
\textbf{Paper} & \textbf{Target Language} & \textbf{FT Method} & \textbf{Main Dataset} & \textbf{Model (F1-Score)} & \textbf{Open-source} \\
\midrule
Alam et al. \cite{alam2024detection} & Solidity & PEFT & VulSmart\cite{alam2024detection} & GPT-4o-mini (0.99) & \color{red}\ding{55} \\
Boi et al. \cite{boi2024smart} & Solidity & PEFT & VulHunt\cite{boi_vulnhunt-gpt_2024} & Llama2-7B-chat-hf & \color{red}\ding{55} \\
Cao et al. \cite{cao_realvul_2024} & PHP & PEFT & RealVul\cite{cao_realvul_2024} & GPT-4 (0.58) & \color{green}\ding{51} \\
Ding et al. \cite{ding_vulnerability_2024} & C/C++ & FFT & PrimeVul\cite{ding_vulnerability_2024} & UnixCoder (0.21) & \color{green}\ding{51} \\
Du et al. \cite{du_generalization-enhanced_2024} & C/C++ & FFT+IFT & Devign\cite{zhou2019devign} & CodeLlama (0.71) & \color{green}\ding{51} \\
Ghosh et al. \cite{ghosh_cve-llm_2024} & C/C++ & DAPT & NVD & MPT-7B (0.93) & \color{red}\ding{55} \\
Gonçalves et al. \cite{gonçalves2024scopeevaluatingllmssoftware} & C/C++ & FFT\&PEFT & CVEFixes\cite{guru2021cvefixes} & NatGen (0.53) & \color{green}\ding{51} \\
Guo et al. \cite{guo_outside_2024} & C/C++ & FFT\&PEFT & Devign\cite{zhou2019devign} & CodeLlama-7bB (0.97) & \color{red}\ding{55} \\
Haurogné et al. \cite{haurogne_advanced_2024} & C/C++ & FFT & DiverseVul\cite{chen2023diversevul} & BERT (0.69) & \color{red}\ding{55} \\
Liu et al. \cite{liu_vuldetectbench_2024} & C/C++ & FFT & SARD \cite{NIST_SARD} & Llama-3-8B & \color{green}\ding{51}  \\
Luo et al. \cite{luo2024fellmvp} & C/C++ & PEFT & Liu2023\cite{liu2023rethinking} & Gemma-7B (0.90) & \color{red}\ding{55} \\
Ma et al. \cite{ma_combining_2024} & Solidity & PEFT & Ma2024\cite{ma_combining_2024} & CodeLlama-13B (0.91) & \color{red}\ding{55} \\
Mao et al. \cite{mao_towards_2024} & C/C++ & PEFT+IFT & SeVC\cite{li2022sevc} & CodeLlama-13B (0.92) & \color{red}\ding{55} \\
Purba et al. \cite{purba_software_2023} & C/C++ & FFT & Code Gadget\cite{li2018vuldeepecker} & Davinci (0.73) & \color{red}\ding{55} \\
Sakaoglu et al. \cite{sakaoglu2023kartal} & HTML/JavaScript & FFT & Sakaolu\cite{sakaoglu2023kartal} & DistilRoBERTa (0.82) & \color{red}\ding{55} \\
Shestov et al. \cite{shestov_finetuning_2024} & Java & PEFT & CVEFixes\cite{guru2021cvefixes} & WizardCoder (0.71) & \color{red}\ding{55}  \\
Taghavi et al. \cite{taghavi_using_2024} & C/C++/Java & PEFT & Mixed Dataset & GPT-4 (0.90) & \color{red}\ding{55}  \\
Wang et al. \cite{wang2023defecthunter} & C/C++ & Model Innovation & QEMU & UnixCoder (0.71) & \color{green}\ding{51}  \\
Wen et al. \cite{wen_scale_2024} & C/C++ & FFT & QEMU & UnixCoder (0.65) & \color{red}\ding{55}  \\
Yang et al. \cite{yang2024large} & C/Java/Python & PEFT & LLMAO\cite{yang2024large} & CodeGen-16B & \color{green}\ding{51}  \\
Yin et al. \cite{yin_multitask-based_2024} & C/C++ & FFT & Big-Vul\cite{fan2020bigvul} & DeepSeek-Coder-6.7B (0.81) & \color{red}\ding{55}  \\
Zhang, C. et al. \cite{zhang2024vtt} & C/C++ & PEFT & Zhang2024\cite{zhang2024vtt} & Llama-7B (0.85) & \color{red}\ding{55} \\
Zhang, J. et al. \cite{zhang2024_empirical} & C/C++/Solidity & FFT & Big-Vul\cite{fan2020bigvul} & CodeLlama-7B (0.82) & \color{red}\ding{55}  \\
Zhou et al. \cite{zhou_comparison_2024} & C/Java/Python & FFT\&PEFT & Zhou2024\cite{zhou_comparison_2024} & Llama-3-8B & \color{red}\ding{55} \\
\bottomrule
\end{tabular}}
\label{tab:finetuning_methods}
\end{table*}


\subsubsection{Fine-tuning}
Fine-tuning helps Large Language Models (LLMs) learn specific tasks better. It works by training pre-trained models again with new data for these tasks.
Fine-tuning is important for three main reasons \cite{zhou2024large, Yao24}: (1) security problems in code follow special patterns that LLMs must learn to find, (2) computer code is different from normal text, so LLMs need to learn how to read and understand code better, and (3) finding security problems needs to be very accurate - missing real problems or reporting false ones can both cause serious issues. As shown in \ref{tab:finetuning_methods}, approximately 30\% of studies choose to fine-tune existing LLMs as their primary proposed method.

\begin{table*}
\centering
\caption{Comparisons of Different Fine-tuning Methods in LLM-based Vulnerability Detection}
\label{tab:ft_comparison}
\begin{tabular}{l p{4cm} p{4cm}}
\toprule
\textbf{Dimension} & \textbf{Method} & \textbf{Properties} \\
\midrule
Parameter Scale & Full Fine-tuning & Updates all parameters for high adaptation capability \\
Parameter Scale & PEFT & Updates subset of parameters for resource efficiency \\
\midrule
Learning Approach & Discriminative & Binary/multi-class classification for precise detection \\
Learning Approach & Generative & Sequence-to-sequence learning with rich output format \\
\bottomrule
\end{tabular}
\end{table*}
\textbf{Full Fine-tuning (FFT). } FFT updates all model parameters during training. Due to computational constraints, most research utilized models with fewer than 15 billion parameters, such as CodeT5, CodeBERT, and UnixCoder. Ding et al.\cite{ding_vulnerability_2024} experimented with five models under 7B parameters, achieving only 0.21 F1-score even when training and validating on PrimeVul. Guo et al.\cite{guo_outside_2024} utilized CodeBERT with FFT for 50 epochs, achieving 0.099 F1-score on PrimeVul but 0.66 F1-score on the Choi2017 dataset. Haurogne et al. \cite{haurogne_advanced_2024} achieved 0.69 F1-score on the DiverseVul dataset, while Purba et al.\cite{purba_software_2023} achieved 0.73 F1-score in buffer overflow detection.

\textbf{Parameter Efficient Fine-tuning (PEFT). } PEFT methods modify only a subset of parameters while keeping most pre-trained weights frozen. Adapters introduce additional trainable layers between original model layers, with Yang et al. \cite{yang2024large} achieving 60\% Top-5 accuracy in fault localization. LoRA represents weight updates as low-rank decompositions, with studies like Du et al. \cite{du_generalization-enhanced_2024} achieving 0.72 F1-score and Guo et al. \cite{guo_outside_2024} reaching 0.97 F1-score on their respective datasets. QLoRA combines parameter quantization with LoRA, as demonstrated by Boi et al.\cite{boi2024smart} achieving 59\% accuracy with lower memory usage.

\textbf{Discriminative Fine-tuning. }For a token sequence \(X = \{x_1, x_2, \ldots, x_L\}\), the model processes it to output vulnerability labels. Zhang et al.~\cite{zhang2024_empirical} and Yin et al.~\cite{yin_multitask-based_2024} demonstrated that a fine-tuned CodeLlama achieves a 0.62 F1-score improvement compared to its non-fine-tuned counterpart.

\textbf{Generative Fine-tuning.} This approach enables sequence-to-sequence learning for generating structured outputs like vulnerability descriptions or vulnerable line identification. Yin et al.\cite{yin_multitask-based_2024} showed that fine-tuning pre-trained LMs outperforms fine-tuning LLMs in generative tasks, with CodeT5+ achieving a ROUGE score of 0.722 compared to DeepSeek-Coder 6.7B's 0.425.

\begin{finding}{V}
Fine-tuning enhances LLM-based vulnerability detection through full and parameter-efficient methods (PEFT). Large models (>10B) with PEFT achieve optimal results, while base models like GPT-4 and CodeLlama deliver F1 scores near 0.9. Discriminative strategies excel in precise detection, requiring datasets with at least 10K samples. However, computational limits and dataset quality remain critical challenges.
\end{finding}

\begin{answerbox}{RQ3}
LLM-based vulnerability detection techniques fall into three categories. First, code preprocessing—such as AST analysis, data/control flow analysis, RAG, and program slicing—enhances context utilization but struggles with complex, cross-file vulnerabilities. Second, prompt engineering—like CoT prompting, few-shot learning, and specialized agents—improves accuracy, with large models (>10B) benefiting from chain-of-thought methods, while smaller models favor simpler prompts. Finally, fine-tuning—both full and parameter-efficient approaches like LoRA—achieves near 0.9 F1-scores, particularly in larger models. As models advance, their inherent strengths may surpass preprocessing benefits, highlighting the need to address complex contexts and cross-file vulnerabilities.
\end{answerbox}

% \paragraph{Training Configurations} The performance of fine-tuned models depends not only on the model architecture but also on training configurations. Through analysis of our selected studies, we collected comprehensive training configuration information to understand their impact on model performance and scalability. For computational resources, FFT approaches typically utilized 4-8 GPUs with 24GB+ memory, while PEFT methods significantly reduced resource requirements, often enabling training on a single GPU. Learning rates commonly ranged from 1e-5 to 5e-5, with batch sizes varying from 8 to 32 depending on available memory and model size. Most studies employed Adam optimizer and Binary Cross-Entropy loss for vulnerability detection tasks. Training epochs typically ranged from 3 to 10, with early stopping based on validation performance. Model-specific settings showed that maximum sequence lengths varied from 512 to 2048 tokens, with most studies using mixed-precision training (FP16) to optimize memory usage. These configuration patterns provide important insights for reproducing research results, scaling implementations, and optimizing resource utilization in vulnerability detection tasks.

\subsection{RQ4. What are the challenges that LLMs are facing in detecting vulnerabilities and potential directions to solve them?}

% \heqing{I think RQ4 requires more data to show our deep thought/thorough analysis about the problem. The challenges and problem listed currently are too vague/shallow for the community. If we only show all the challenges already mentioned from the related work with citations, it seems they may have been solved in their paper and we only gather them together.}

The field currently faces four major challenges, along with corresponding potential directions, as illustrated in Figure \ref{fig:roadmap_rq4}. First, researchers struggle to obtain high-quality datasets. Second, large language models (LLMs) show reduced effectiveness when dealing with complex vulnerabilities. Third, these models have limited success in real-world repository applications. Fourth, the models lack robust generation capabilities. Multiple studies confirm these challenges as the main barriers to progress. The following sections examine each challenge in detail.

\begin{figure*}[h]
   \includegraphics[width=1\linewidth]{images/challenge.drawio.png}
  \caption{Challenges and Potential Directions}
  \Description{roadmap_rq4}
  \label{fig:roadmap_rq4}
\end{figure*}

% \begin{table}
% \renewcommand{\arraystretch}{0.87}
% \centering
% \caption{Challenges in Selected Researches}
% \label{tab:tools_challenges}
% \begin{tabular}{lc}
% \toprule
% \textbf{Study} & \textbf{Challenge} \\
% \midrule
% Alam et al.\cite{alam2024detection} & C2, C3, C4 \\
% Bae et al.\cite{bae_enhancing_2024} & C1, C2, C4 \\
% VulnHunt-GPT \cite{boi_vulnhunt-gpt_2024} & C1, C2, C3 \\
% Boi et al.\cite{boi2024smart} & C1, C2, C4 \\
% LLM-CloudSec\cite{cao_llm-cloudsec_2024} & C1, C3, C4 \\
% RealVul\cite{cao_realvul_2024} & C1, C2, C4 \\
% Chen et al.\cite{chen2023chatgpt} & C1, C2, C3, C4 \\
% Smartguard \cite{ding_smartguard_2024} & C2, C3 \\
% PrimeVul \cite{ding_vulnerability_2024} & C1, C2, C4 \\
% VulLLM \cite{du_generalization-enhanced_2024} & C1, C2, C3, C4 \\
% Fu et al. \cite{fu_chatgpt_2023} & C1, C2, C4 \\
% Smartguard \cite{gao_how_2023} & C2, C3 \\
% CVE-LLM \cite{ghosh_cve-llm_2024} & C1, C2, C4 \\
% ScoPE \cite{gonçalves2024scopeevaluatingllmssoftware} & C1, C2, C4 \\
% Guo et al. \cite{guo_outside_2024} & C1, C3, C4 \\
% Haurogne et al. \cite{HAUROGNE2024100598} & C2, C3, C4 \\
% Hossain et al. \cite{hossain_malicious_2024} & C1, C3 \\
% Smartguard \cite{hu_large_2023} & C2, C4 \\
% LSAST \cite{keltek_lsast_2024} & C1, C2 \\
% Khare et al. \cite{khare2023understanding} & C3, C4 \\
% Kholoosi et al. \cite{kholoosi_qualitative_2024} & C1, C2, C3 \\
% kouliaridis \cite{kouliaridis_assessing_2024} & C1, C3, C4 \\
% LLift \cite{li_enhancing_2024} & C2, C4 \\
% LOVA \cite{li_attention_2024} & C1, C3, C4 \\
% IRIS \cite{li_llm-assisted_2024} & C1, C2, C3 \\
% VulDetectBench \cite{liu_vuldetectbench_2024} & C1, C4 \\
% Vul-GPT \cite{liu_software_2023} & C2, C3 \\
% Liu et al. \cite{liu_exploration_2024} & C1, C2, C4 \\
% GRACE \cite{lu_grace_2024} & C1, C3, C4 \\
% FELLMVP \cite{luo2024fellmvp} & C2, C4 \\
% iAudit \cite{ma_combining_2024} & C1, C3, C4 \\
% Mahyari et al. \cite{mahyari_harnessing_2024} & C1, C2, C3 \\
% LLMVulExp \cite{mao_towards_2024} & C1, C3, C4 \\
% Mao et al. \cite{mao_multi-role_2024} & C1, C2 \\
% LLbezpeky \cite{mathews_llbezpeky_2024} & C2, C4 \\
% Mishra et al. \cite{mishra_ai-augmented_2024} & C1, C3, C4 \\
% Purba et al. \cite{purba_software_2023} & C1, C2 \\
% KARTAL \cite{sakaoglu2023kartal} & C1, C3, C4 \\
% Shestov et al. \cite{shestov_finetuning_2024} & C1, C2, C4 \\
% VulnArmor \cite{sindhwad_vulnarmor_2024} & C1, C3, C4 \\
% LLM4Vuln \cite{sun_llm4vuln_2024} & C1, C2, C4 \\
% Taghavi et al. \cite{taghavi_using_2024} & C3, C4 \\
% DefectHunter \cite{wang2023defecthunter} & C1, C2, C4 \\
% ANVIL \cite{wang2024anvil} & C1, C2, C3, C4 \\
% SCALE \cite{wen_scale_2024} & C1, C2, C4 \\
% Yang et al. \cite{yang_iot_2023} & C2, C4 \\
% SLFHunter \cite{ye2024_detecting} & C2, C3, C4 \\
% Yin et al. \cite{yin_multitask-based_2024} & C1, C2, C4 \\
% VulnArmor \cite{zhang2024vtt} & C1, C2, C3 \\
% VTT-LLM \cite{sindhwad_vulnarmor_2024} & C1, C3, C4 \\
% Zhang et al. \cite{zhang2024_empirical} & C1, C2, C4 \\
% Zhou et al. \cite{zhou_large_2024} & C1, C2, C4 \\
% \bottomrule
% \end{tabular}
% \end{table}



\textbf{Challenge 1: Limited scope of research problems: } Current research focuses primarily on determining whether a given code snippet is vulnerable or not. In this survey, approximately 40 studies (83\%) concentrate on the analysis of isolated code snippets, where LLM performance often exceeds the results observed in real-world code detection scenarios \cite{gao_how_2023}. While this provides a controlled environment for evaluation, it overlooks the complexities present in practical applications, such as analyzing entire codebases or addressing vulnerabilities that emerge during collaborative development. This indicates that research focused solely on isolated functions or snippets has limited usefulness for real-world scenarios.

\textbf{Potential Directions: }
Beyond analyzing isolated code segments, future research should be structured around the following key problems in real-world development:

Research problems categorized by code evolution in development:
\begin{itemize}

\item \textbf{Full-scale/Incremental Detection:} Full-scale detection requires analyzing entire codebases across multiple files, while incremental detection focuses on new code commits. Current LLMs excel at analyzing isolated code segments but struggle with broader contexts \cite{jiang_when_2024}. As a result, LLMs typically assist static analysis tools or support fuzzing tasks \cite{yang_whitefox_2024, lyu_prompt_2024, yang_kernelgpt_2024} rather than performing standalone analysis. For commit-level detection, existing methods combine commits with static analysis results \cite{li_enhancing_2024, yang_iot_2023}, but may fail when encountering APIs outside their training corpus \cite{jiang_when_2024}.

\end{itemize}

Research problems categorized by vulnerability report workflow:
\begin{itemize}
\item \textbf{Reproducing Vulnerabilities}: Vulnerability reproduction will be essential in future research and a key to reducing false positives. For each detected vulnerability, LLMs should attempt reproduction using input drivers such as fuzzers \cite{jiang_when_2024}. By generating inputs that trigger potential vulnerabilities, LLMs can provide evidence of vulnerability existence \cite{yang_whitefox_2024, xia_fuzz4all_2024}. This approach validates the detection and ensures findings are actionable, thus improving vulnerability report reliability.

\item \textbf{Vulnerability Repair}: While many studies discussed vulnerability repair, practical implementation in real-world projects remains challenging \cite{zhou_large_2024}. Successful vulnerability repair in production environments must meet several criteria:
\begin{itemize}
\item Repaired code must pass all existing tests
\item Repaired code must prevent vulnerability reproduction
\item Repaired code should not introduce new vulnerabilities
\end{itemize}
Current limitations in dataset quality and LLM capabilities hinder effective vulnerability repair validation. While LLMs can identify vulnerable code, they often misidentify vulnerability locations, leading to incorrect explanations and repairs. The ability to generate proof-of-concept exploits and simulate program operations would improve validation, but this requires significant advances in LLM capabilities \cite{kulsum_case_2024}.

\end{itemize}

Research problems categorized by vulnerability characteristics:
\begin{itemize}
\item \textbf{Specialized Vulnerability Detection: }
The development of targeted detection approaches for specific vulnerability categories represents a significant challenge. Current research \cite{gao2021_beyond} demonstrates that LLMs exhibit varying capabilities across different vulnerability types - achieving high accuracy in detecting Out-of-bounds Write vulnerabilities (CWE-787) while performing poorly with Missing Authorization issues (CWE-862). This performance variation necessitates specialized detection mechanisms for distinct CWE categories, particularly for high-frequency vulnerabilities such as memory-related issues. The lack of category-specific research and datasets has left this critical area largely unexplored.

\item \textbf{Vulnerability Classification and Severity Assessment: }
Alam et al. \cite{alam2024detection} and Gao et al. \cite{gao2021_beyond} highlight two fundamental challenges:
First, accurately categorizing detected vulnerabilities according to established frameworks (e.g., CWE) remains difficult. This classification problem is essential to practical development workflows, as different vulnerability types require distinct remediation approaches.
Second, predicting vulnerability severity levels directly influences remediation priorities and timelines, with high-risk vulnerabilities requiring expedited mitigation. 
\end{itemize}

\begin{finding}{VI}
When selecting experimental research problems, researchers should prioritize addressing real-world challenges in software vulnerability detection. For instance, research can be categorized by codebase analysis methods, such as full-scale scanning or incremental scanning, focusing on comprehensive or commit-level vulnerability detection. Additionally, based on the logical workflow of vulnerability reporting in real-world development, research can be divided into vulnerability discovery, reproduction, and repair. Beyond these, other feasible directions include vulnerability classification, severity prediction, and targeted detection for specific vulnerability types.
\end{finding}

\textbf{Challenge 2: Complexity of Representing Vulnerability Semantics.}
Vulnerability patterns are often very complex \cite{li_nuances_2023}. More than 95\% of studies report that code complexity—such as external dependencies, multiple function calls, global variables, and complicated software states—makes it hard to detect vulnerabilities. We can measure this complexity by lines of code or cyclical complexity \cite{tehrani2024assessing}, and visualize it using program dependency graphs (DFGs) or call graphs. However, most methods focus on single code blocks at the function or file level \cite{zhang2024_empirical, du_generalization-enhanced_2024}, which is not very helpful for large projects. When dealing with complex projects, LLMs often have limited input and face much “unseen code” \cite{jiang_when_2024}.

In simpler situations—like a single function of about 500 lines from synthetic datasets—LLMs can detect vulnerabilities well, even in zero-shot settings \cite{cao_realvul_2024, xia_fuzz4all_2024}. But many studies show that more information is needed to handle larger projects \cite{gao_how_2023, ashiwal_llm-based_2024}, especially when the online corpus is sparse. In such cases, we must provide extra documents and specifications \cite{zhang_how_2024}. Also, some functions rely on their callers for protection, so analyzing them alone may lead to incorrect conclusions. We need to give LLMs enough context to identify vulnerabilities accurately.

\textbf{Potential Directions: }
Two core approaches can address this challenge. The first approach focuses on enabling LLMs to read more code. This increases their understanding of the entire repository. The second approach emphasizes using abstract representations to simplify code semantics. This enhances LLMs' comprehension of code structure and behavior.

\begin{itemize}
    \item \textbf{Dynamic Code Knowledge Expansion:} Through feedback loops and adaptive mechanisms, LLMs should be enabled to freely access and understand repository code \cite{yang_kernelgpt_2024, zhang_how_2024}. This would address high false positive rates by providing broader context for vulnerability analysis.

    \item \textbf{Optimized Code Representation:} Studies \cite{tamberg_harnessing_2024, liu_exploration_2024, khare2023understanding, lu_grace_2024, sun_llm4vuln_2024} utilize AST, CFG, and DFG representations to reduce token counts for limited context windows. While current implementations haven't significantly improved detection accuracy, future research opportunities include sophisticated semantic processing, multi-method integration, better context preservation, and hybrid graph representations.

    \item \textbf{Specialized LLM Agents}
    Research explores optimization through specialized LLM agents. Studies \cite{Wang_2024_survey} demonstrate that task division among multiple agents increases output robustness. Each agent specializes in specific aspects of vulnerability detection. Prompt engineering research \cite{liu_exploration_2024} evaluates zero-shot, few-shot, and chain-of-thought approaches for detection accuracy. However, code complexity introduces challenges. Multiple agents show accuracy degradation with complex code. Few-shot and chain-of-thought methods cannot provide sufficient additional context.
    
    \item \textbf{Integration with External Tools}
    External tools provide important support mechanisms. LangChain improves efficiency through simplified and asynchronous LLM calls. Retrieval-Augmented Generation (RAG) gains popularity due to its cost-effectiveness and efficiency. Studies \cite{cao_llm-cloudsec_2024, du_vul-rag_2024} implement RAG to vectorize code contexts and enhance detection through LLM-based retrieval. However, code contexts differ fundamentally from natural language. This difference necessitates specialized approaches for code semantic extraction, storage, and comparison.


These optimization techniques could potentially bridge the gap between LLMs' current capabilities and the complex requirements of real-world vulnerability detection in large codebases.
\end{itemize}


\begin{finding}{VII}
The complexity of vulnerabilities indicates that vulnerable code often involves intricate control flows. Addressing this requires improving LLMs' ability to efficiently store and process code information. Researchers can use retrieval-augmented generation (RAG) or similar tools to dynamically expand knowledge. Code semantics can be compressed using control flow graphs (CFGs), abstract syntax trees (ASTs), or neural-symbolic methods. Additionally, specialized agents optimized for specific tasks can be employed within vulnerability detection systems. These approaches enhance the efficiency and effectiveness of LLMs in handling complex code structures.
\end{finding}

\textbf{Challenge 3: Intrinsic Limitations of LLMs.}
Detection solutions must maintain robustness against data changes and adversarial attacks \cite{du_generalization-enhanced_2024}. However, research by Yin et al. \cite{yin_multitask-based_2024} reveals that LLMs lack this robustness. They show vulnerability to data perturbations. These findings indicate the need for more reliable approaches.

Additionally, LLMs need better explainability and consistency for real-world applications. Haurogne et al. and Du et al. \cite{haurogne_advanced_2024, du_generalization-enhanced_2024} demonstrate that LLMs produce inconsistent vulnerability explanations. They cannot guarantee correct explanations in every instance. The outputs show randomness across different runs. Even when LLMs correctly identify vulnerable code, they often fail to provide accurate vulnerability explanations. This limitation creates significant problems for subsequent repair and review processes. Current research in this area remains insufficient.

% Early LLMs, such as Llama2, BERT and CodeT5, also exhibit limitations in token input size and comprehension capabilities, preventing them from processing complete codebases effectively. This significantly increases vulnerability detection errors. Explainability remains another critical issue, as LLM outputs often fail to provide accurate explanations of detected vulnerabilities.

Future research should focus on these key areas: improving accuracy, enhancing robustness, and increasing output reliability and explainability. These improvements will make LLM-based solutions more practical for real-world use.

\textbf{Potential Directions: }
The core to this challenge is to improve the inherent vulnerability detection ability of LLMs. Researchers may focus on training new models or fine-tuning LLMs.
\begin{itemize}
    \item \textbf{Fine-tune Frontier LLMs: }  
    Recent findings on scaling laws \cite{kaplan2020scalinglawsneurallanguage} indicate that larger decoder-only language models, such as GPT-4 and Claude3.5-Sonnet, can achieve systematically improved performance as model size, training data, and compute are scaled up. Improvements in model capabilities enhance vulnerability detection, classification, and explanation. Research by Alam et al. \cite{alam2024detection} shows that GPT-4 and GPT-3.5 achieve higher detection accuracy than earlier models like Llama2 and CodeT5 under identical prompts. The release of GPT-O1, with its visible reasoning process, suggests improvements in both detection capability and output explainability.
    
    Fine-tuning approaches show promise. Several studies \cite{alam2024detection, cao_realvul_2024, du_generalization-enhanced_2024, guo_outside_2024, mao_multi-role_2024, ma_combining_2024} fine-tune open-source models like CodeLlama and CodeBERT. These achieve results comparable to general LLMs. However, dataset limitations present challenges. Research \cite{vieira2024much} indicates that practical applications require datasets of at least 100,000 examples. This creates significant training cost barriers. Researchers can enhance vulnerability detection performance through the following approaches:
    
    \begin{itemize}
        \item Vulnerability-specific Fine-tuning: Fine-tune models on specific vulnerability types. Research \cite{alam2024detection, cao_realvul_2024, du_generalization-enhanced_2024} shows models trained on specific vulnerability categories (e.g., memory-related issues, injection flaws) achieve higher detection accuracy. This targeted approach allows models to learn deeper patterns within each vulnerability type.
    
        \item Repository-adaptive Fine-tuning: Adapt models to specific codebases through fine-tuning on repository-specific data. Studies \cite{Hanif_2022, mahyari_harnessing_2024} demonstrate this approach improves detection accuracy by helping models understand project-specific coding patterns and architecture. This method benefits large, complex projects with unique coding conventions.
    \end{itemize}
    
\end{itemize}

\begin{itemize}
   \item \textbf{Ensemble Learning and Domain-Adaptive Pretraining:} Combining predictions from multiple models effectively reduces false positives and improves detection accuracy. DAPT can refine LLMs' understanding of specific contexts by leveraging curated datasets including both public records (e.g., NVD) and domain-specific data. This enables better identification of niche vulnerabilities and improved generalization.

   \item \textbf{Adaptive Learning Mechanisms:} To address the dynamic nature of security threats and enhance model robustness, adaptive learning \cite{martin2020systematic} mechanisms allow continuous knowledge updates through feedback loops and periodic retraining. Advanced optimization techniques can further improve prompt configurations and learning rates, ensuring reliability in real-world applications.
\end{itemize}


\begin{finding}{VIII}
Enhancing robustness and explainability in LLMs is essential for effective vulnerability detection. Fine-tuning on specific vulnerability types, such as memory issues or injection flaws, improves detection accuracy by focusing on targeted patterns. Repository-adaptive fine-tuning helps models learn project-specific coding conventions, further increasing accuracy. Ensemble learning combines predictions from multiple models to reduce false positives, while domain-adaptive pretraining (DAPT) refines model understanding of niche contexts using curated datasets. Adaptive learning mechanisms, incorporating feedback loops and periodic updates, ensure LLMs remain robust against evolving threats. These methods address LLM limitations and improve their real-world applicability.
\end{finding}

\textbf{Challenge 4: Lack of High-Quality Datasets.}
High-quality vulnerability benchmark datasets remain scarce. Current datasets face several problems. These include data leakage, incorrect labels, small size, and limited scope \cite{mathews_llbezpeky_2024, wu2023_effective, chen2023diversevul}. 

\textbf{Dataset Incorrectness:}
A critical issue in these challenges is the incorrect labeling of vulnerabilities, which harms the reliability and effectiveness of datasets. Automated collection methods \cite{nie23Understanding} can gather large amounts of data quickly but cannot ensure correct labels without human review, leading to many mislabeled or inaccurately annotated samples. Research \cite{wu2023_effective} also shows frequent data leakage, as LLMs often train on sources like GitHub, old software versions, and external libraries with inadequate version control or deduplication. As a result, models may encounter the same test data seen during training, inflating performance metrics and undermining real-world validity. In conclusion, a high-quality dataset for vulnerability detection should meet several requirements.

\begin{itemize}
    \item \textbf{Accurate Labels.} Since labels are crucial in supervised learning, incorrect annotations can lead to serious issues in production environments.
    \item \textbf{Minimal Data Leakage.} Large-scale LLMs trained on broad codebases risk seeing identical vulnerabilities during testing. Countermeasures include code obfuscation, synthesis, and using updated datasets.
    \item \textbf{Comprehensive Annotations.} For repository-level data, providing call sequences and control flows that reproduce the vulnerability, as well as detailed descriptions, helps LLMs create more reliable detection reports.
\end{itemize}


\textbf{Potential Directions: }
The key to addressing this challenge lies in researchers' focus on constructing datasets based on specific research scopes. As discussed in Challenge 1, different research problems in LLM-based vulnerability detection require distinct types of datasets, all of which currently lack sufficient accurate data or cases. Researchers should approach dataset construction with targeted focus on specific research problems. Researches can be developed on:

\begin{itemize}
    \item \textbf{Dataset Quality and Scope Enhancement.}
    Research can focus on developing smaller, high-quality test sets to effectively measure progress in vulnerability detection. One approach combines existing verified samples from multiple studies \cite{alam2024detection, zibaeirad2024vulnllmevalframeworkevaluatinglarge, liu_vuldetectbench_2024}. This creates a reliable test benchmark that the research community can maintain and expand over time. Moreover, recent advances in LLMs, particularly GPT-4o with its 128k context window, enable comprehensive repository-level vulnerability analysis, allowing researchers to detect and repair vulnerabilities across entire codebases rather than just at function-level.

    \item \textbf{Scalability and Long-Tailed Vulnerability Handling:} Handling long-tailed distributions of vulnerability types requires both scalable models and data augmentation techniques \cite{liu_enhancing, deng_improving}. Generating synthetic samples for rare vulnerability types can improve LLMs’ ability to detect low-frequency events. Integrating structured information, such as CWE classifications, can further enhance the model's capability to prioritize and address critical vulnerabilities effectively \cite{atiiq2024generalist}.
\end{itemize}

\begin{finding}{IX}
High-quality datasets are essential for advancing LLM-based vulnerability detection. Repository-level datasets with detailed annotations, including call sequences and control flows, enhance real-world applicability. Targeted datasets aligned with specific research scopes address distinct detection challenges. Synthetic data generation mitigates data leakage and handles rare vulnerability types. Combining verified samples with scalable data augmentation ensures robust benchmarks for repository-wide vulnerability detection.
\end{finding}

\begin{answerbox}{RQ4}
The main challenges in LLM-based vulnerability detection include research scopes, dataset quality, vulnerability complexity, and model robustness. Key research directions involve improving model capabilities, developing advanced usage methods, enhancing datasets, strengthening detection robustness, and specializing vulnerability detection approaches. There is still a long way to go.
\end{answerbox}


\section{Limitations}
Several factors may affect this survey's comprehensiveness. First, approximately 60\% of research in LLM-based vulnerability detection appears as preprints on arXiv. This reflects the field's emerging nature. Second, terminology variations in concepts like "LLM" and "vulnerability detection" may lead to oversights in initial searches. To mitigate these risks, we implemented a systematic approach. We began by analyzing published papers from established conferences and journals. We extracted core keywords from these sources. Over a two-month period, we refined our selection from approximately 500 papers to 58 highly relevant studies. Future versions of this survey will incorporate new developments in this rapidly evolving field. This ongoing process will ensure more comprehensive and timely coverage of research literature.

\section{Conclusion}
This study presents a systematic analysis of LLM applications in vulnerability detection. Through extensive literature review, we provide a comprehensive examination of the current research landscape, systematically addressing four key questions: the application of LLMs in vulnerability detection, the design of evaluation benchmarks and datasets, current technical approaches, and existing challenges with future directions.

Our findings demonstrate that LLMs exhibit significant potential in code comprehension and vulnerability detection. Through techniques such as fine-tuning and prompt engineering, LLMs can effectively improve detection accuracy. Experiments across multiple benchmark datasets indicate that recent large-scale LLMs, such as GPT-4 and Claude-3.5, have achieved notable progress in vulnerability detection tasks.
However, significant challenges remain in applying LLMs to practical security development. The primary obstacle is the scarcity of high-quality datasets, which constrains model training and evaluation. Additionally, current LLMs show notable limitations in handling complex code structures and repository-level vulnerability detection. Furthermore, issues regarding output randomness and model explainability require further investigation.

Based on these findings, we propose several promising research directions: enhancing model adaptation to code evolution, improving vulnerability reproduction and repair capabilities, developing high-quality datasets, and strengthening model robustness and explainability. Advances in these areas will drive the broader adoption of LLMs in vulnerability detection.

In the future, we plan to enrich this review by adding more vulnerability-related tasks, such as vulnerability localization, vulnerability assessment and vulnerability patching.




% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% Thanks to my advisor Dr. Jeff Huang, who led us win the AIxCC semifinals in 2024, helped me complete this survey.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
% \bibliographystyle{ACM-Reference-Format}
%\bibliography{ref}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

% //////////////////
% \section{Template Overview}
% As noted in the introduction, the ``\verb|acmart|'' document class can
% be used to prepare many different kinds of documentation --- a
% double-anonymous initial submission of a full-length technical paper, a
% two-page SIGGRAPH Emerging Technologies abstract, a ``camera-ready''
% journal article, a SIGCHI Extended Abstract, and more --- all by
% selecting the appropriate {\itshape template style} and {\itshape
%   template parameters}.

% This document will explain the major features of the document
% class. For further information, the {\itshape \LaTeX\ User's Guide} is
% available from
% \url{https://www.acm.org/publications/proceedings-template}.

% \subsection{Template Styles}

% The primary parameter given to the ``\verb|acmart|'' document class is
% the {\itshape template style} which corresponds to the kind of publication
% or SIG publishing the work. This parameter is enclosed in square
% brackets and is a part of the {\verb|documentclass|} command:
% \begin{verbatim}
%   \documentclass[STYLE]{acmart}
% \end{verbatim}

% Journals use one of three template styles. All but three ACM journals
% use the {\verb|acmsmall|} template style:
% \begin{itemize}
% \item {\texttt{acmsmall}}: The default journal template style.
% \item {\texttt{acmlarge}}: Used by JOCCH and TAP.
% \item {\texttt{acmtog}}: Used by TOG.
% \end{itemize}

% The majority of conference proceedings documentation will use the {\verb|acmconf|} template style.
% \begin{itemize}
% \item {\texttt{sigconf}}: The default proceedings template style.
% \item{\texttt{sigchi}}: Used for SIGCHI conference articles.
% \item{\texttt{sigplan}}: Used for SIGPLAN conference articles.
% \end{itemize}

% \subsection{Template Parameters}

% In addition to specifying the {\itshape template style} to be used in
% formatting your work, there are a number of {\itshape template parameters}
% which modify some part of the applied template style. A complete list
% of these parameters can be found in the {\itshape \LaTeX\ User's Guide.}

% Frequently-used parameters, or combinations of parameters, include:
% \begin{itemize}
% \item {\texttt{anonymous,review}}: Suitable for a ``double-anonymous''
%   conference submission. Anonymizes the work and includes line
%   numbers. Use with the \texttt{\string\acmSubmissionID} command to print the
%   submission's unique ID on each page of the work.
% \item{\texttt{authorversion}}: Produces a version of the work suitable
%   for posting by the author.
% \item{\texttt{screen}}: Produces colored hyperlinks.
% \end{itemize}

% This document uses the following string as the first command in the
% source file:
% \begin{verbatim}
% \documentclass[acmsmall]{acmart}
% \end{verbatim}

% \section{Modifications}

% Modifying the template --- including but not limited to: adjusting
% margins, typeface sizes, line spacing, paragraph and list definitions,
% and the use of the \verb|\vspace| command to manually adjust the
% vertical spacing between elements of your work --- is not allowed.

% {\bfseries Your document will be returned to you for revision if
%   modifications are discovered.}

% \section{Typefaces}

% The ``\verb|acmart|'' document class requires the use of the
% ``Libertine'' typeface family. Your \TeX\ installation should include
% this set of packages. Please do not substitute other typefaces. The
% ``\verb|lmodern|'' and ``\verb|ltimes|'' packages should not be used,
% as they will override the built-in typeface families.

% \section{Title Information}

% The title of your work should use capital letters appropriately -
% \url{https://capitalizemytitle.com/} has useful rules for
% capitalization. Use the {\verb|title|} command to define the title of
% your work. If your work has a subtitle, define it with the
% {\verb|subtitle|} command.  Do not insert line breaks in your title.

% If your title is lengthy, you must define a short version to be used
% in the page headers, to prevent overlapping text. The \verb|title|
% command has a ``short title'' parameter:
% \begin{verbatim}
%   \title[short title]{full title}
% \end{verbatim}

% \section{Authors and Affiliations}

% Each author must be defined separately for accurate metadata
% identification.  As an exception, multiple authors may share one
% affiliation. Authors' names should not be abbreviated; use full first
% names wherever possible. Include authors' e-mail addresses whenever
% possible.

% Grouping authors' names or e-mail addresses, or providing an ``e-mail
% alias,'' as shown below, is not acceptable:
% \begin{verbatim}
%   \author{Brooke Aster, David Mehldau}
%   \email{dave,judy,steve@university.edu}
%   \email{firstname.lastname@phillips.org}
% \end{verbatim}

% The \verb|authornote| and \verb|authornotemark| commands allow a note
% to apply to multiple authors --- for example, if the first two authors
% of an article contributed equally to the work.

% If your author list is lengthy, you must define a shortened version of
% the list of authors to be used in the page headers, to prevent
% overlapping text. The following command should be placed just after
% the last \verb|\author{}| definition:
% \begin{verbatim}
%   \renewcommand{\shortauthors}{McCartney, et al.}
% \end{verbatim}
% Omitting this command will force the use of a concatenated list of all
% of the authors' names, which may result in overlapping text in the
% page headers.

% The article template's documentation, available at
% \url{https://www.acm.org/publications/proceedings-template}, has a
% complete explanation of these commands and tips for their effective
% use.

% Note that authors' addresses are mandatory for journal articles.

% \section{Rights Information}

% Authors of any work published by ACM will need to complete a rights
% form. Depending on the kind of work, and the rights management choice
% made by the author, this may be copyright transfer, permission,
% license, or an OA (open access) agreement.

% Regardless of the rights management choice, the author will receive a
% copy of the completed rights form once it has been submitted. This
% form contains \LaTeX\ commands that must be copied into the source
% document. When the document source is compiled, these commands and
% their parameters add formatted text to several areas of the final
% document:
% \begin{itemize}
% \item the ``ACM Reference Format'' text on the first page.
% \item the ``rights management'' text on the first page.
% \item the conference information in the page header(s).
% \end{itemize}

% Rights information is unique to the work; if you are preparing several
% works for an event, make sure to use the correct set of commands with
% each of the works.

% The ACM Reference Format text is required for all articles over one
% page in length, and is optional for one-page articles (abstracts).

% \section{CCS Concepts and User-Defined Keywords}

% Two elements of the ``acmart'' document class provide powerful
% taxonomic tools for you to help readers find your work in an online
% search.

% The ACM Computing Classification System ---
% \url{https://www.acm.org/publications/class-2012} --- is a set of
% classifiers and concepts that describe the computing
% discipline. Authors can select entries from this classification
% system, via \url{https://dl.acm.org/ccs/ccs.cfm}, and generate the
% commands to be included in the \LaTeX\ source.

% User-defined keywords are a comma-separated list of words and phrases
% of the authors' choosing, providing a more flexible way of describing
% the research being presented.

% CCS concepts and user-defined keywords are required for for all
% articles over two pages in length, and are optional for one- and
% two-page articles (or abstracts).

% \section{Sectioning Commands}

% Your work should use standard \LaTeX\ sectioning commands:
% \verb|\section|, \verb|\subsection|, \verb|\subsubsection|,
% \verb|\paragraph|, and \verb|\subparagraph|. The sectioning levels up to
% \verb|\subsusection| should be numbered; do not remove the numbering
% from the commands.

% Simulating a sectioning command by setting the first word or words of
% a paragraph in boldface or italicized text is {\bfseries not allowed.}

% Below are examples of sectioning commands.

% \subsection{Subsection}
% \label{sec:subsection}

% This is a subsection.

% \subsubsection{Subsubsection}
% \label{sec:subsubsection}

% This is a subsubsection.

% \paragraph{Paragraph}

% This is a paragraph.

% \subparagraph{Subparagraph}

% This is a subparagraph.

% \section{Tables}

% The ``\verb|acmart|'' document class includes the ``\verb|booktabs|''
% package --- \url{https://ctan.org/pkg/booktabs} --- for preparing
% high-quality tables.

% Table captions are placed {\itshape above} the table.

% Because tables cannot be split across pages, the best placement for
% them is typically the top of the page nearest their initial cite.  To
% ensure this proper ``floating'' placement of tables, use the
% environment \textbf{table} to enclose the table's contents and the
% table caption.  The contents of the table itself must go in the
% \textbf{tabular} environment, to be aligned properly in rows and
% columns, with the desired horizontal and vertical rules.  Again,
% detailed instructions on \textbf{tabular} material are found in the
% \textit{\LaTeX\ User's Guide}.

% Immediately following this sentence is the point at which
% Table~\ref{tab:freq} is included in the input file; compare the
% placement of the table here with the table in the printed output of
% this document.

% \begin{table}
%   \caption{Frequency of Special Characters}
%   \label{tab:freq}
%   \begin{tabular}{ccl}
%     \toprule
%     Non-English or Math&Frequency&Comments\\
%     \midrule
%     \O & 1 in 1,000& For Swedish names\\
%     $\pi$ & 1 in 5& Common in math\\
%     \$ & 4 in 5 & Used in business\\
%     $\Psi^2_1$ & 1 in 40,000& Unexplained usage\\
%   \bottomrule
% \end{tabular}
% \end{table}

% To set a wider table, which takes up the whole width of the page's
% live area, use the environment \textbf{table*} to enclose the table's
% contents and the table caption.  As with a single-column table, this
% wide table will ``float'' to a location deemed more
% desirable. Immediately following this sentence is the point at which
% Table~\ref{tab:commands} is included in the input file; again, it is
% instructive to compare the placement of the table here with the table
% in the printed output of this document.

% \begin{table*}
%   \caption{Some Typical Commands}
%   \label{tab:commands}
%   \begin{tabular}{ccl}
%     \toprule
%     Command &A Number & Comments\\
%     \midrule
%     \texttt{{\char'134}author} & 100& Author \\
%     \texttt{{\char'134}table}& 300 & For tables\\
%     \texttt{{\char'134}table*}& 400& For wider tables\\
%     \bottomrule
%   \end{tabular}
% \end{table*}

% Always use midrule to separate table header rows from data rows, and
% use it only for this purpose. This enables assistive technologies to
% recognise table headers and support their users in navigating tables
% more easily.

% \section{Math Equations}
% You may want to display math equations in three distinct styles:
% inline, numbered or non-numbered display.  Each of the three are
% discussed in the next sections.

% \subsection{Inline (In-text) Equations}
% A formula that appears in the running text is called an inline or
% in-text formula.  It is produced by the \textbf{math} environment,
% which can be invoked with the usual
% \texttt{{\char'134}begin\,\ldots{\char'134}end} construction or with
% the short form \texttt{\$\,\ldots\$}. You can use any of the symbols
% and structures, from $\alpha$ to $\omega$, available in
% \LaTeX~\cite{Lamport:LaTeX}; this section will simply show a few
% examples of in-text equations in context. Notice how this equation:
% \begin{math}
%   \lim_{n\rightarrow \infty}x=0
% \end{math},
% set here in in-line math style, looks slightly different when
% set in display style.  (See next section).

% \subsection{Display Equations}
% A numbered display equation---one set off by vertical space from the
% text and centered horizontally---is produced by the \textbf{equation}
% environment. An unnumbered display equation is produced by the
% \textbf{displaymath} environment.

% Again, in either environment, you can use any of the symbols and
% structures available in \LaTeX\@; this section will just give a couple
% of examples of display equations in context.  First, consider the
% equation, shown as an inline equation above:
% \begin{equation}
%   \lim_{n\rightarrow \infty}x=0
% \end{equation}
% Notice how it is formatted somewhat differently in
% the \textbf{displaymath}
% environment.  Now, we'll enter an unnumbered equation:
% \begin{displaymath}
%   \sum_{i=0}^{\infty} x + 1
% \end{displaymath}
% and follow it with another numbered equation:
% \begin{equation}
%   \sum_{i=0}^{\infty}x_i=\int_{0}^{\pi+2} f
% \end{equation}
% just to demonstrate \LaTeX's able handling of numbering.

% \section{Figures}

% The ``\verb|figure|'' environment should be used for figures. One or
% more images can be placed within a figure. If your figure contains
% third-party material, you must clearly identify it as such, as shown
% in the example below.
% \begin{figure}[h]
%   \centering
%   \includegraphics[width=\linewidth]{sample-franklin}
%   \caption{1907 Franklin Model D roadster. Photograph by Harris \&
%     Ewing, Inc. [Public domain], via Wikimedia
%     Commons. (\url{https://goo.gl/VLCRBB}).}
%   \Description{A woman and a girl in white dresses sit in an open car.}
% \end{figure}

% Your figures should contain a caption which describes the figure to
% the reader.

% Figure captions are placed {\itshape below} the figure.

% Every figure should also have a figure description unless it is purely
% decorative. These descriptions convey what’s in the image to someone
% who cannot see it. They are also used by search engine crawlers for
% indexing images, and when images cannot be loaded.

% A figure description must be unformatted plain text less than 2000
% characters long (including spaces).  {\bfseries Figure descriptions
%   should not repeat the figure caption – their purpose is to capture
%   important information that is not already provided in the caption or
%   the main text of the paper.} For figures that convey important and
% complex new information, a short text description may not be
% adequate. More complex alternative descriptions can be placed in an
% appendix and referenced in a short figure description. For example,
% provide a data table capturing the information in a bar chart, or a
% structured list representing a graph.  For additional information
% regarding how best to write figure descriptions and why doing this is
% so important, please see
% \url{https://www.acm.org/publications/taps/describing-figures/}.

% \subsection{The ``Teaser Figure''}

% A ``teaser figure'' is an image, or set of images in one figure, that
% are placed after all author and affiliation information, and before
% the body of the article, spanning the page. If you wish to have such a
% figure in your article, place the command immediately before the
% \verb|\maketitle| command:
% \begin{verbatim}
%   \begin{teaserfigure}
%     \includegraphics[width=\textwidth]{sampleteaser}
%     \caption{figure caption}
%     \Description{figure description}
%   \end{teaserfigure}
% \end{verbatim}

% \section{Citations and Bibliographies}

% The use of \BibTeX\ for the preparation and formatting of one's
% references is strongly recommended. Authors' names should be complete
% --- use full first names (``Donald E. Knuth'') not initials
% (``D. E. Knuth'') --- and the salient identifying features of a
% reference should be included: title, year, volume, number, pages,
% article DOI, etc.

% The bibliography is included in your source document with these two
% commands, placed just before the \verb|\end{document}| command:
% \begin{verbatim}
%   \bibliographystyle{ACM-Reference-Format}
%   \bibliography{bibfile}
% \end{verbatim}
% where ``\verb|bibfile|'' is the name, without the ``\verb|.bib|''
% suffix, of the \BibTeX\ file.

% Citations and references are numbered by default. A small number of
% ACM publications have citations and references formatted in the
% ``author year'' style; for these exceptions, please include this
% command in the {\bfseries preamble} (before the command
% ``\verb|\begin{document}|'') of your \LaTeX\ source:
% \begin{verbatim}
%   \citestyle{acmauthoryear}
% \end{verbatim}


%   Some examples.  A paginated journal article \cite{Abril07}, an
%   enumerated journal article \cite{Cohen07}, a reference to an entire
%   issue \cite{JCohen96}, a monograph (whole book) \cite{Kosiur01}, a
%   monograph/whole book in a series (see 2a in spec. document)
%   \cite{Harel79}, a divisible-book such as an anthology or compilation
%   \cite{Editor00} followed by the same example, however we only output
%   the series if the volume number is given \cite{Editor00a} (so
%   Editor00a's series should NOT be present since it has no vol. no.),
%   a chapter in a divisible book \cite{Spector90}, a chapter in a
%   divisible book in a series \cite{Douglass98}, a multi-volume work as
%   book \cite{Knuth97}, a couple of articles in a proceedings (of a
%   conference, symposium, workshop for example) (paginated proceedings
%   article) \cite{Andler79, Hagerup1993}, a proceedings article with
%   all possible elements \cite{Smith10}, an example of an enumerated
%   proceedings article \cite{VanGundy07}, an informally published work
%   \cite{Harel78}, a couple of preprints \cite{Bornmann2019,
%     AnzarootPBM14}, a doctoral dissertation \cite{Clarkson85}, a
%   master's thesis: \cite{anisi03}, an online document / world wide web
%   resource \cite{Thornburg01, Ablamowicz07, Poker06}, a video game
%   (Case 1) \cite{Obama08} and (Case 2) \cite{Novak03} and \cite{Lee05}
%   and (Case 3) a patent \cite{JoeScientist001}, work accepted for
%   publication \cite{rous08}, 'YYYYb'-test for prolific author
%   \cite{SaeediMEJ10} and \cite{SaeediJETC10}. Other cites might
%   contain 'duplicate' DOI and URLs (some SIAM articles)
%   \cite{Kirschmer:2010:AEI:1958016.1958018}. Boris / Barbara Beeton:
%   multi-volume works as books \cite{MR781536} and \cite{MR781537}. A
%   couple of citations with DOIs:
%   \cite{2004:ITE:1009386.1010128,Kirschmer:2010:AEI:1958016.1958018}. Online
%   citations: \cite{TUGInstmem, Thornburg01, CTANacmart}.
%   Artifacts: \cite{R} and \cite{UMassCitations}.

% \section{Acknowledgments}

% Identification of funding sources and other support, and thanks to
% individuals and groups that assisted in the research and the
% preparation of the work should be included in an acknowledgment
% section, which is placed just before the reference section in your
% document.

% This section has a special environment:
% \begin{verbatim}
%   \begin{acks}
%   ...
%   \end{acks}
% \end{verbatim}
% so that the information contained therein can be more easily collected
% during the article metadata extraction phase, and to ensure
% consistency in the spelling of the section heading.

% Authors should not prepare this section as a numbered or unnumbered {\verb|\section|}; please use the ``{\verb|acks|}'' environment.

% \section{Appendices}

% If your work needs an appendix, add it before the
% ``\verb|\end{document}|'' command at the conclusion of your source
% document.

% Start the appendix with the ``\verb|appendix|'' command:
% \begin{verbatim}
%   \appendix
% \end{verbatim}
% and note that in the appendix, sections are lettered, not
% numbered. This document has two appendices, demonstrating the section
% and subsection identification method.

% \section{Multi-language papers}

% Papers may be written in languages other than English or include
% titles, subtitles, keywords and abstracts in different languages (as a
% rule, a paper in a language other than English should include an
% English title and an English abstract).  Use \verb|language=...| for
% every language used in the paper.  The last language indicated is the
% main language of the paper.  For example, a French paper with
% additional titles and abstracts in English and German may start with
% the following command
% \begin{verbatim}
% \documentclass[sigconf, language=english, language=german,
%                language=french]{acmart}
% \end{verbatim}

% The title, subtitle, keywords and abstract will be typeset in the main
% language of the paper.  The commands \verb|\translatedXXX|, \verb|XXX|
% begin title, subtitle and keywords, can be used to set these elements
% in the other languages.  The environment \verb|translatedabstract| is
% used to set the translation of the abstract.  These commands and
% environment have a mandatory first argument: the language of the
% second argument.  See \verb|sample-sigconf-i13n.tex| file for examples
% of their usage.

% \section{SIGCHI Extended Abstracts}

% The ``\verb|sigchi-a|'' template style (available only in \LaTeX\ and
% not in Word) produces a landscape-orientation formatted article, with
% a wide left margin. Three environments are available for use with the
% ``\verb|sigchi-a|'' template style, and produce formatted output in
% the margin:
% \begin{description}
% \item[\texttt{sidebar}:]  Place formatted text in the margin.
% \item[\texttt{marginfigure}:] Place a figure in the margin.
% \item[\texttt{margintable}:] Place a table in the margin.
% \end{description}

% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

% %%
% %% The next two lines define the bibliography style to be used, and
% %% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{main}


% %%
% %% If your work has an appendix, this is the place to put it.
% \appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-acmsmall.tex'.
