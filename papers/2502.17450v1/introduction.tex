
%LLMs are increasingly adopted as assistants in code development tasks. 
LLMs are becoming increasingly adopted as assistants in code development tasks. 
For instance, the GitHub Copilot extension~\cite{Copilotvscode} for Visual Studio Code~\cite{vscode} has surpassed 22 million installations, making it %the most installed AI extension for coding and 
one of the most widely used coding extensions overall in this IDE (Integrated Development Environment). Similarly, plugins for integrating LLMs, such as ChatGPT~\cite{ChatGPT} and Gemini~\cite{gemini}, are also increasingly becoming available for the most popular IDEs~\cite{codegpt,EclipseGemini}. 

Empirical studies demonstrate that AI assistants effectively generate implementations that assist developers in understanding, documenting, and validating their code. For instance, Corso et al.~\cite{Corso:EmpiricalAssessment:ICPC:2024} report that up to one-third of the method implementations can be generated automatically using LLMs. Similarly, the study by Pinto et al.~\cite{10.1145/3644815.3644949} reports positive feedback from the developers who use these tools. The study by Liang et al.~\cite{liang2024large} shows that, although the usability of these tools must still be improved, they are widely used with a positive impact on software development. Tufano et al.~\cite{10.1145/3643991.3644918} analyzed GitHub commits, pull requests, and issues showing how GitHub developers are using ChatGPT for several tasks, including feature implementation and enhancement, documentation, and testing.

The recommendations generated by LLMs are however dependent on several parameters that affect the \emph{correctness} and the \emph{determinism} of the responses. For instance, the \emph{temperature} can be used to control the creativity of the models, while the parameter $\textit{top-p}$ can be used to control the selection of the tokens that shall compose the output. Further, the non-determinism of the models requires multiple repetitions for each request to be assessed.

Despite the role played by these parameters, \emph{how the models are configured when used to generate code is underrated}. Studies often do not report the values used for model parameters, nor do they report the number of repetitions completed to assess the results~\cite{sakib2024extending,Siddiq:QualityCghatGPT:2024:MSR}. Sometimes studies try to minimize the non-determinism of the models to avoid doing many repetitions (e.g., setting the temperature to $0.0$), without considering the consequences of this decision on the correctness of the results~\cite{Yihong:CodeGenerationChatGPT:TOSEM:2024,Yue:RefiningChatGPTGeneratedCode:TOSEM:2024}. This spectrum of cases generates issues with the results themselves and their reproducibility. 

Early evidence confirms that these parameters and repetitions matter. For instance, the study by Arora et al.~\cite{arora2024optimizing} investigates the role of temperature and top-p when GPT is prompted to generate the body of a method from its signature. The study considers only 13 methods, reporting a significant role of both the temperature and top-p on the correctness of the responses. Moreover, low values of these parameters seem to reduce creativity, while increasing the consistency and correctness of the results. Similar results have been reported in Liu et al.~\cite{liu2024your}, where again low values of temperature and top-p are suggested to be beneficial for the correctness of the responses. The temperature has been specifically studied also in Ouyang et al.~\cite{ouyang2023empirical}, again confirming that low values may guarantee more consistent results from GPT models.  

Although these studies provide initial evidence of how these parameters may influence results in code generation tasks, they do not consider the \emph{impact of these parameters from the perspective of the non-deterministic nature of the models}, which imposes prompting the models multiple times to assess their effectiveness. For example, a low-temperature value may increase the consistency of the responses, not necessarily generating a proper implementation of a higher number of methods across repetitions. In particular, as discovered in this study, higher temperature values may \emph{decrease the absolute percentage of correct results} (i.e., we observe fewer correct responses to prompts, as reported in other studies), but may \emph{increase the range of methods correctly implemented} (i.e., creativity in responses enables the generation of implementations that can hardly be obtained otherwise). 

Practical \emph{recommendations on the number of repetitions} for each prompt are also lacking. Indeed researchers and practitioners make very different decisions, with little empirical justification. 

This paper addresses both these concerns by presenting an empirical study that systematically considers the impact of temperature and top-p in method generation tasks. The effect of these parameters is assessed in a context where the same inputs are submitted multiple times, and the range of responses generated is studied to derive practical implications about the \emph{setup and usage of these models}. We investigate these questions using ChatGPT, 
which is the most widely used LLM among developers \cite{MostUsedAI}, as well as one of the most frequently studied LLMs in various other research \cite{liu2024your,ouyang2023empirical,Yihong:CodeGenerationChatGPT:TOSEM:2024,Siddiq:QualityCghatGPT:2024:MSR,Yue:RefiningChatGPTGeneratedCode:TOSEM:2024,10.1145/3643991.3644918}.

%one of the most used LLMs extensively used in several other studies. %considering the inluence of  temperature and $top\_k$. 

%Further, we study the value of iterating the requests multiple times, providing recommendations on the number of repetitions that should be considered when interacting with LLMs. 

%This is due to a lack of knowledge about how to handle these parametrs. 
% 4 temp X 10 req, + 5 ref X 2 top-p aggiuntive 
More in detail we carefully selected a corpus of $548$ methods whose implementation is used to query GPT-4o, the most recent version of GPT at the time this paper has been written. Each query is submitted multiple times for multiple configurations, for a total of $27,400$ requests assessed. The analysis revealed surprising results that may influence future usage of LLMs in code generation. In particular:
\begin{itemize}
    \item Contrarily to current belief, using low-temperature values, although resulting in a higher number of requests responded correctly, decreases the total number of methods that could be implemented correctly considering repetitions (i.e., considering the submission of the same prompt multiple times).
    \item Although most of the studies report the value of the temperature used in the evaluation without mentioning the value of the top-p parameter, our study shows that top-p has a much stronger impact on the results than the temperature value, which has a marginal impact.
    \item  Submitting the same prompt multiple times is fundamental to accommodate for the non-deterministic nature of the queried models.
\end{itemize}

The study results in practical recommendations that may help researchers better set up and report their studies and may help practitioners use LLMs more effectively to generate code.

In a nutshell, our study provides the following contributions:
\begin{itemize}
\item Systematically examines how temperature and top-p affect method generation tasks in GPT-4o, releasing novel empirical evidence about the effect of these parameters on the correctness of the generated code.

\item Studies how to define the number of repetitions appropriately, to cost-effectively generate code.

\item Presents actionable recommendations for effectively configuring and utilizing GPT models in code generation tasks.

\item Releases an open dataset that can be used to reproduce the results reported in the paper and to conduct additional studies on the same subject.
\end{itemize}

The paper is organized as follows. Section~\ref{sec:methodology} presents the research questions we investigated and the methodology we designed to answer them. Section~\ref{sec:results} describes the results that we obtained to answer the research questions. Section~\ref{sec:related} discusses related work. Section~\ref{sec:conclusions} provides final remarks. 
