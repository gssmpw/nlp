
\subsection{RQ1 - Temperature}

Figure~\ref{fig:RequestsCorrectness} shows the percentage of invalid, incorrect, and plausible responses generated with different temperature values. ChatGPT tends more to generate either invalid or plausible code, with a smaller percentage (between 12\% and 14.7\%) of incorrect code, that is, if the code compiles there is a good change it also passes the test cases. Response-wise, differences are small and non-significant, with a percentage of valid implementations ranging between 36.4\% (temperature $0.0$) and 37.7\% (temperature $2.0$).

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/percentage_of_requests.png}
        \caption{Percentage of invalid, incorrect, and plausible responses for different \emph{temperature} values.}
        \label{fig:RequestsCorrectness}
\end{figure}

However, if we consider the capability of the multiple temperature values to generate a plausible implementation at least once in the scope of the ten repetitions of the same request, the model's performance changes for different temperature values. In particular, Figure~\ref{fig:MethodsCorrectness} shows for each temperature value the percentage of methods with at least a plausible implementation in the ten repetitions.  It is apparent how higher temperature values can generate a correct implementation for a higher number of methods, with temperatures $1.2$ and $2.0$ achieving the best results.

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/percentage_of_methods.png}
        \caption{Percentage of \emph{methods} with at least a plausible, incorrect, or invalid implementation, considering the best result returned by a configuration across 10 repetitions.}
        \label{fig:MethodsCorrectness}
\end{figure}

Considering the almost invariant number of responses with plausible method implementation (Figure~\ref{fig:RequestsCorrectness}) paired with the higher number of methods with at least one plausible implementation generated (Figure~\ref{fig:MethodsCorrectness}), we can conclude that the higher creativity obtained with higher temperature values lets ChatGPT generates plausible code for methods that cannot be otherwise generated, at the cost of decreasing the rate of plausible code for the easier methods.  

This is confirmed by the Eulero-Venn diagram of the methods with at least one plausible implementation for each temperature value shown in Figure~\ref{fig:EuleroVenn}, where temperatures $1.2$ and $2.0$ generated plausible code at least once for the highest number of methods, nearly subsuming the results obtained for temperatures $0.0$ and $0.8$.  

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/venn_diagram_metodi_test.png}
        \caption{Eulero-Venn diagram with the plausible methods generated at least once for each temperature value.}
        \label{fig:EuleroVenn}
\end{figure}

Indeed, by thoroughly sampling the generated code, it is possible to recognize how using low-temperature values may cause ChatGPT to systematically fail the generation of code, even for tasks that might initially appear to be relatively straightforward. For instance, the code displayed in Listing~\ref{lst:example1} serves as an example of a method that was consistently generated incorrectly across all ten repetitions, with ChatGPT repeatedly making a subset of the same mistakes in each attempt. Specifically, ChatGPT often failed to include the main parentheses of the method (lines 1 and 7), incorrectly reused variable names by changing \texttt{columnCount} to \texttt{column} (lines 2 and 4), used parentheses without providing arguments (line 2), and frequently assigned the wrong name to the method.
Conversely, a plausible and correct implementation of this method becomes achievable when higher temperature values are used (e.g., temperatures of $1.2$ and $2.0$). Under these configurations, the model's enhanced creativity is effectively leveraged, allowing it to overcome its inherent bias toward generating flawed implementations.


\begin{lstlisting}[language=Java,
xleftmargin=.12in,
label=lst:example1,caption=Invalid code obtained with Temperature 0.0.]
private static boolean isJaggedMatrix(double[][] m) 
    int columnCount = m[].length;
    for (double row : m) {
        if (row.length != column) {
            return
        }
    }
\end{lstlisting}

\textbf{\textit{In a nutshell}}, although creativity has a relatively marginal impact on the correctness of individual responses on a per-request basis, it has proven to be valuable when considering the overall capability of generating plausible code for each method. Specifically, a temperature setting of $1.2$ yielded slightly better results compared to $2.0$ in terms of the total number of plausible methods successfully addressed.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{RQ2 - Top-p} 

Figure~\ref{fig:RequestsTopP} illustrates the percentage of invalid, incorrect, and plausible code returned by the requests when using temperature equal to $1.2$, which was the best-performing temperature value in RQ1, and multiple values of the top-p parameter. The results indicate that the top-p parameter exerts a considerably more significant influence on the generation of plausible code compared to the temperature setting. %\todo{vogliamo verificarlo con un test statisstico?} 

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/probabilities_stacked_bar_chart_three_datasets.png}
        \caption{Percentage of invalid, incorrect, and plausible responses for different values of \textit{top-p}.}
        \label{fig:RequestsTopP}
\end{figure}

On one hand, the proportion of invalid code exhibits a noticeable decrease, moving from 49.7\% when top-p is set to $0.95$, down to 42.6\% and 36.3\% for top-p equal to $0.5$ and $0.0$, respectively. On the other hand, the percentage of plausible code generated increases correspondingly, rising from 37.6\% for top-p equal to $0.95$, to 40.0\% and 49.3\% for top-p equal to $0.5$ and $0.0$, respectively.

The dramatic positive impact of low top-p values is clearly confirmed when we focus on the methods that have a plausible implementation in at least one of the repetitions.  Figure~\ref{fig:MethodsTopP} illustrates the Eulero-Venn diagram representing the number of different plausible methods that were generated at least once by each specific top-p value configuration. 

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/venn_diagram_metodi_test_per_top_p_modificato_v4.png}
        \caption{Eulero-Venn diagrams of the plausible methods generated at least once with each \textit{top-p} value.}
        \label{fig:MethodsTopP}
\end{figure}

The smaller the top-p value, the more effective the configuration appears to be in practice. In particular, when using top-p set to $0.0$, there are $217$ plausible methods that are uniquely generated by this configuration, which also subsumes the other studied configurations, with the exception of $3$ plausible methods generated with top-p equal to $0.5$. This outcome aligns well with previous studies, which have consistently reported a higher effectiveness associated with low top-p values~\cite{arora2024optimizing}.

\begin{lstlisting}[language=Java, xleftmargin=.12in,label=lst:example2,caption= Systematically wrong implementation of the \texttt{evaluate}\\ method with \textit{top-p} 0.95.] 

public boolean evaluate(Geometry b, 
                    TopologyPredicate predicate) {
    if (b.isEmpty()) {
        return true;
    }
    return false;
}
\end{lstlisting}


Indeed, by sampling the generated code, it is possible to recognize how using high top-p values may cause ChatGPT to systematically fail the generation of code. For example, with top-p equal to $0.95$, ChatGPT never 
adds some of the fundamental checks required in the function 
\texttt{\small public boolean evaluate(Geometry b, TopologyPredicate predicate)}, thereby producing an incomplete or incorrect implementation for each attempt. 
Listing~\ref{lst:example2} shows a typical faulty implementation where conditions on \texttt{Geometry b} and \texttt{TopologyPredicate predicate} are not properly handled or are entirely omitted.

On the contrary, when using top-p equal to $0.0$, ChatGPT tends to be more consistent and systematically covers the necessary logic for the same method. In Listing~\ref{lst:example3}, we show a plausible implementation that checks whether \texttt{b} is valid and non-empty, and handles multiple values of \texttt{predicate} through proper conditional structures.

\begin{lstlisting}[language=Java, xleftmargin=.18in, label=lst:example3, caption=Plausible implementation with \textit{top-p} 0.0.]
public boolean evaluate(Geometry b, 
                    TopologyPredicate predicate){
  if (b == null || b.isEmpty()) 
    return false;
  switch (predicate) {
    case DISJOINT: 
        return !b.intersects(this.geometry);
    case TOUCHES:  
        // ...
    default:       
        return false;
  }
}
\end{lstlisting}



\textbf{\textit{In a nutshell}}, we observe top-p playing a bigger role in method generation than temperature, with smaller top-p values being more effective than higher values. %In addition to setting low top-p values, \change{these findings stress} the need to report the full configuration of ChatGPT and not only the used temperature value, since results may differ greatly based on the set of tokens considered to compose the response, which is the factor controlled by the top-p parameter. 
In addition to setting low top-p values, these findings highlight the importance of reporting the full configuration of ChatGPT, not just the temperature setting, as results may vary significantly depending on the set of tokens considered for generating the response, which is controlled by the top-p parameter.

\subsection{RQ3 - Repetitions}
With this research question, we investigate the probability of observing a plausible solution in the scope of multiple repetitions, that is, the multiple submissions of the same prompt. Figure~\ref{fig:passKTemperature} presents the pass@k metric, displaying its values for increasing values of $k$ and across different temperature configurations.

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/passk_temperature_selezionatee.png}
        \caption{Value of pass@k for multiple temperature values.}
        \label{fig:passKTemperature}
\end{figure}

While there is almost no difference in the performance of ChatGPT for individual requests, we can see how the performance of the model changes when multiple repetitions are considered. In particular, temperature $0.0$ leads to the discovery of little additional plausible implementations when increasing the number of repetitions. On the contrary, the other studied temperature values can discover additional plausible implementations for increasing values of $k$. 

This result suggests that, although using a temperature equal to $0.0$ may motivate empirical setup with no, or very little, repetitions, it also affects the effectiveness of the result.  

We can observe how higher creativity is useful to cover a broader range of implementations, with a value equal to $1.2$ representing a good compromise between creativity and the soundness of the behavior of the model. 

We also studied the combination of repetitions and top-p. Figure~\ref{fig:passKTopP} shows how values of pass@k change with an increasing number of repetitions.

The results confirm the relevance of the top-p parameter and its impact on the capability of generating plausible code. In particular, while top-p values equal to $0.95$ and $0.5$ saturate quite quickly, a top-p value of $0.0$ allows the model to generate plausible code for the vast majority of the methods. Notably, five repetitions yield the best results. %In particular, five repetitions achieve the best results. 

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/passk_top_p_confrontoooo.png}
        \caption{Value of pass@k for multiple \textit{top-p} values.}
        \label{fig:passKTopP}
\end{figure}

To illustrate why multiple attempts can be critical, let us consider the \texttt{checkSymmetry} method, which verifies if a given matrix is symmetric. In the first attempts, ChatGPT produces an incomplete or incorrect implementation, as shown in Listing~\ref{lst:example4}, where the code checks only the first and last elements of each row instead of comparing \texttt{matrix[i][j]} and \texttt{matrix[j][i]}:

\begin{lstlisting}[language=Java, xleftmargin=.12in, label=lst:example4, 
caption=Example of an incorrect implementation of\\ \texttt{checkSymmetry} in the early attempts.]
public boolean checkSymmetry(int[][] matrix) {
    for (int i = 0; i < matrix.length; i++) {
        if (matrix[i][0] != matrix[i]
                     [matrix.length - 1]) {
            return false;
        }
    }
    return true;
}
\end{lstlisting}

By repeatedly submitting the same prompt (e.g., up to the fifth attempt), the model finally produces a plausible version of the method. Listing~\ref{lst:example5} shows how the code correctly ensures that the matrix is square and systematically checks all symmetrical pairs of elements:
\begin{lstlisting}[language=Java, xleftmargin=.18in, label=lst:example5, 
caption=Plausible implementation of \texttt{checkSymmetry} obtained after\\ five attempts.]
public boolean checkSymmetry(int[][] matrix) {
    if (matrix == null || matrix.length == 0) {
        return false;
    }
    int n = matrix.length;
    for (int i = 0; i < n; i++) {
        if (matrix[i].length != n) {
            return false;
        }
        for (int j = i + 1; j < n; j++) {
            if (matrix[i][j] != matrix[j][i]) {
                return false;
            }
        }
    }
    return true;
}
\end{lstlisting}

These two snippets highlight how a single attempt may fail, but repeated submissions can yield a functional implementation that passes all test cases, thereby improving the pass@k performance.

\textbf{\textit{In a nutshell}}, it is important to submit the same prompt multiple times to address the non-determinism of the model. Low-temperature values reduce the need to run multiple repetitions, yet require submitting a prompt more than once. However, low-temperature values also decrease the range of plausible implementations that can be obtained. The best trade-off is achieved by using a temperature equal to $1.2$, with top-p equal to $0.0$, and completing five repetitions.

\subsection{RQ4 - Impact of Methods Length and Complexity}

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/complexity_probability_plot.png}
        \caption{Average probability of producing a plausible implementation for increasing values of complexity.}
        \label{fig:AvgProbPlauvsCC}
\end{figure}

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/loc_probability_plot.png}
        \caption{Average probability of producing a plausible implementation for increasing lengths.}
        \label{fig:AvgProbPlauvsLength}
\end{figure}

Figures~\ref{fig:AvgProbPlauvsCC}  and~\ref{fig:AvgProbPlauvsLength}
show how the average probability of producing a plausible response relates to the cyclomatic complexity and length of the method that has to be generated. For the cyclomatic complexity, the plot reports average probabilities using an interval of 2.5 points. For instance, the data point associated with 5 indicates the average probability of generating plausible code for the methods whose complexity is between 2.5 and 5. We report values only for complexity intervals that include at least 50 data points. For the length of the method, the plot reports exactly the same information, using an interval of 10 locs.

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.48\textwidth]{images/scatter_plot_temp_1.2.png}
        \caption{Scatter plot of probabilities for temperature 1.2.}
        \label{fig:ScatterProbT1.2}
\end{figure}

We can notice how the trend is strongly linear. However, the linear trend of average probabilities is not linear when considering the individual probabilities of producing plausible code for the methods in the dataset. In fact, Figure~\ref{fig:ScatterProbT1.2} shows the scatter plot of these probabilities for temperature 1.2. Many data points overlap. We can notice how the probabilities associated with the individual methods are very diverse, and the linear trend shows up only at the level of the overall average values, and it is not evident when considering the individual data points. All the plots are similar, also considering other temperature values and using method length rather than complexity. All the plots are available as part of our online material.

We checked for the presence of any correlation by computing the Pearson correlation coefficient and its significance. We report the results in Table~\ref{tab:correlation}.

\begin{table}[ht]
\caption{Pearson correlation coefficient and its significance for multiple temperature values in relation to cyclomatic complexity and lines of code. Significant p-values in bold.}

\centering
\begin{tabular}{ccc}

\textbf{Temperature} & \textbf{Corr. Coef.} & \textbf{P-Value}  \\
\toprule
\multicolumn{3}{c}{Correlation with CC} \\
\midrule
0.0 & -0.10 & \textbf{0.02} \\
0.8 & -0.12 & \textbf{0.01} \\
1.2 & -0.12 & \textbf{0.01} \\
2.0 & -0.11 & \textbf{0.01} \\
\toprule
\multicolumn{3}{c}{Correlation with locs} \\
\midrule
0.0 & -0.18 & \textbf{$<$0.01} \\
0.8 & -0.20 & \textbf{$<$0.01} \\
1.2 & -0.20 & \textbf{$<$0.01} \\
2.0 & -0.21 & \textbf{$<$0.01} \\
\bottomrule

\end{tabular}
\label{tab:correlation}
\end{table}

We can notice how the correlation is always significant, but its strength is low, with a slightly stronger correlation with method length. This is a direct consequence of the average linear trend combined with the high variability of the values. That is, this result indicates that the longer or the more complex methods are, the more difficult is to generate a plausible implementation. However, this observation is very sensitive to the individual cases, because there are lengthy and complex methods that are easy to generate (e.g., methods that resemble well-known algorithms), and short methods that are hard to generate (e.g., methods that use APIs that were not existing at the time of the training), making method length and method complexity inaccurate predictors of the chance of obtaining plausible code. 

We can finally observe how all the temperature values perform very similarly, indicating no major difference in the capability of dealing with methods of varying length and complexity.

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/complexity_correctness_plot_combined.png}
        \caption{Average probability of producing a plausible implementation for increasing complexity.}
        \label{fig:corrCCTopp}
\end{figure}

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.5\textwidth]{images/Average_probability_LOC.png}
        \caption{Average probability of producing a plausible implementation for increasing lengths.}
        \label{fig:corrLocTopp}
\end{figure}


We repeated the same analysis for top-p values. Figures~\ref{fig:corrCCTopp}  and~\ref{fig:corrLocTopp} show how the average probability of producing a plausible response relates to the cyclomatic complexity and length of the method that has to be generated, for different top-p values. The trend here is less evident, with lines that are quite flat. When considering individual probabilities, again they are very diverse spanning the full range of possible values (see for instance the results for top-p equal to $0.0$ in Figure~\ref{fig:corrCCTopp}).

\begin{figure}[ht]
        \centering
        \includegraphics[width=0.48\textwidth]{images/scatter_plot_CC_vs_probPassaTest.png}
        \caption{Average probability of producing a plausible implementation for increasing complexity.}
        \label{fig:corrCCTopp}
\end{figure}

We checked for the presence of any correlation by computing the Pearson correlation coefficient and its significance. We report the results in Table~\ref{tab:correlationTopP}.

\begin{table}[ht]
\caption{Pearson correlation coefficient and its significance for multiple temperature values in relation to cyclomatic complexity and lines of code. Significant p-values in bold.}

\centering
\begin{tabular}{ccc}
\textbf{Top-p} & \textbf{Corr. Coef.} & \textbf{P-Value}  \\
\toprule
\multicolumn{3}{c}{Correlation with CC} \\
\midrule
0.0 & -0.06 & 0.13 \\
0.5 & -0.09 & 0.08 \\
0.95 & -0.12 & \textbf{$<$0.01} \\
\toprule
\multicolumn{3}{c}{Correlation with locs} \\
\midrule
0.0 & -0.13 & \textbf{$<$0.01}  \\
0.5 &  -0.17 & \textbf{$<$0.01}  \\
0.95 & -0.20 & \textbf{$<$0.01}  \\
\bottomrule

\end{tabular}
\label{tab:correlationTopP}
\end{table}

We can observe how the size of the method correlates with the probability of generating plausible code, while the correlation with the cyclomatic complexity holds only for high top-p values. In practice, low top-p values increase the effectiveness of code generation, eliminating the (mild) correlation between the generation of plausible code with the method complexity. Interestingly, method length remains a relevant factor even for low top-p values. Lastly, the correlation is in all the cases mild, again confirming that there is a heterogeneity of cases and that both size and complexity are factors that cannot explain alone the effectiveness of code generation.


This evidence further confirms the relevance of the configuration in the code generation process and the importance of using low top-p to address a wider range of cases. 


\textbf{\textit{In a nutshell}}, method length and complexity have a mild impact on the capability of generating plausible code. Contrarily to other configurations, low top-p values do not show any correlation with method complexity, only retaining the correlation with method length, confirming their better suitability for code generation tasks. 



%\subsection{Qualitative Analysis}
%We inspected the code generated by ChatGPT for the various configurations to identify the nature of the obstacles that could not be effectively faced by specific configurations.

%\todo{aggiugnere esempio codice temperatura 0 insiste con un errore, ma temperatura 1.2 ha successo}

%\todo{aggiugnere esempio codice temperatura 1.2 ma top-p=0.95 insiste con un errore, ma top-p = 0 invece ha successo}

\subsection{Threats to Validity}
Our study faces several threats. One threat is the data leakage problem, that is, the model might have observed during the training phase the same methods we used in the evaluation. To address this threat, we carefully selected only the methods that have been committed on GitHub after the official model training date. We expect this practice to nearly eliminate the threat. The only possibility for the model to know the implementation of a method is the case of cross-repository code clones, that is, developers who cloned code produced before the training date from another repository. Although this is in principle possible, we expect it to occur rarely.

Another threat is about checking the responses produced by ChatGPT. The scale of our study, consisting of 548 methods whose implementations have been requested multiple times with different configurations, for a total of 27,400 implementations assessed, does not allow for the manual inspection of the code by one or more developers. For this reason, we relied on the notion of plausible method implementation, that is, an implementation that passes the test cases implemented by the developer to test the method, where the test cases are required to cover at least 80\% of the code (97.4\% in average) present in the original method implementation. We assume this is a notion strong enough to assess the relative performance of the various configurations and the impact of the parameters. We do not claim that plausible code is correct code, as shown in other studies there is a gap between plausible and correct code~\cite{Corso:EmpiricalAssessment:ICPC:2024}, but rather that plausible code is a useful metric to study the impact of the parameters on the responses.   

A final threat is the generality of our findings. Our study considers ChatGPT and the generation of full-method implementations. We do not know if the results are also valid for other assistants and tasks. However, we expect the results to remain valid as long as the assistants are based on the same model architecture and the task is the same. Additionally, our study targets Java code. This limitation may restrict the generalizability of our findings to other programming languages. In fact LLMs may exhibit different behaviors with different languages. %However, we expect similar trends in languages sharing key features with Java, especially for method-level code generation tasks.
%
%\change{Additionally, our experiments focused solely on Java projects. While Java is similar to other statically-typed, object-oriented languages (e.g., C# or Kotlin), it is unclear if the results extend to languages with different paradigms. However, we expect similar trends in languages sharing key features with Java, especially for method-level code generation tasks.}



\subsection{Findings}

Our study generates some actionable findings that are discussed below:

\begin{itemize}
\item \textbf{Low-temperature values undermine the capability of the model to generate plausible code}. A reasonably high level of creativity was demonstrated to be necessary to successfully generate plausible code for some methods. Although, as reported in other studies~\cite{arora2024optimizing,liu2024your,ouyang2023empirical}, low-temperature values guarantee consistency in the responses, they also decrease the spectrum of methods that can be addressed with ChatGPT. This provides new evidence challenging the belief that low-temperature values should be preferred over high values.%This is new evidence contrasting the belief that low-temperature values should be preferred over high values. 
%that influences how ChatGPT models must be configured in the future for code generation tasks.

\item \textbf{Higher temperature values can generate a higher range of plausible method implementations, with $1.2$ representing a good trade-off between creativity and controllability}. In our experiments, a degree of creativity was necessary to address some methods, that cannot be addressed with lower creativity values. Using the highest creativity levels (i.e., temperature equal to $2$) is not strictly necessary. In fact, a temperature equal to $1.2$ has been sufficient to address the available methods, with a value equal to $2.0$ introducing a small decrease in the percentage of plausible methods generated.

\item \textbf{Small top-p values produce a higher number of plausible methods than higher values}. Our experiments show how small values of top-p are beneficial to the generation of plausible code, with top-p equal to $0.0$ obtaining results that are significantly better than all other configurations. Practitioners should thus consider not using the default values when using ChatGPT for generating code.

\item \textbf{The configuration of the parameters has a major impact on the correctness of the results}. Our study shows how the choice of the parameters may have a relevant impact on the results. In fact, the range of plausible method implementations ranged from $36.4\%$ (for temperature equal to $0.0$ and top-p equal to $0.95$) to $49.3\%$ (for temperature equal to $1.2$ and top-p equal to $0.0$). It is thus important that these parameters are carefully controlled and reported.

\item \textbf{Setting temperature to $0.0$ to diminish non-determinism must be avoided in code generation tasks}. Although setting the temperature to $0.0$ is sometimes used to increase the degree of determinism of ChatGPT, and avoid submitting the same prompts multiple times, this practice should be avoided. First, even with a temperature equal to $0.0$ non-determinism is not fully prevented, and multiple repetitions (e.g., 2 or 3) are needed to properly assess the capability of the model. Moreover, higher temperature values achieved better performance in method generation tasks, and should thus be preferred to using temperature equal to $0.0$.

\item \textbf{Method length and complexity have little impact on the probability of generating plausible implementations}. In the range of the configurations extensively studied (i.e., methods with complexity below 10 and length below 80 locs), the capability of generating plausible configurations has been affected only to a small extent by length and complexity. Developers should thus not have an expectation about the correctness of the code dependent on the length of the code to be generated or its complexity. Moreover, low top-p values did not show any significant dependence on complexity and negligible dependency on length, reinforcing the importance of not using the default values for top-p in code generation tasks. 

\item \textbf{A possibly effective recommendation for the method generation task is using temperature equal to $1.2$, top-p equal to $0.0$, and repeating the requests 5 times}. In this study, we investigated a range of configurations that can be used when querying ChatGPT. Although there are multiple reasonable choices that can produce good results, there is one configuration that performed better than others. Indeed, we cannot consider it a global optimum. It is however a good configuration to start from when employing ChatGPT to generate method implementations: temperature equals $1.2$, top-p equals 0, and completing at least $5$ repetitions.


\end{itemize}
