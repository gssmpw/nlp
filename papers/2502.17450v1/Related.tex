
This work is mainly related to work on code generation with LLMs and studies about the influence of the parameters that influence LLMs on the correctness of the generated code.

\textbf{Code generation tasks with LLMs}. Code generation tasks assisted by LLMs have been studied according to multiple perspectives~\cite{Liguo:LLM4CG:arXiv:2024,jiang2024surveylargelanguagemodels}. For instance, Corso et al.~\cite{Corso:EmpiricalAssessment:ICPC:2024} compared the effectiveness of multiple AI assistants in method generation tasks, reporting Copilot and ChatGPT as the most effective tools. Yet, only up to one-third of the generated methods were correct.

Research has also examined how the content and structure of prompts affect interactions with AI assistants. For instance, Fagadau et al. investigated the impact of multiple prompts on the generated code~\cite{Fagadau:PromptInfluence:ICPC:2024}.

In addition to studies considering the effectiveness of AI tools in the context of technical tasks, several studies collected data about how developers perceive the utility of these tools. The study by Liang et al.~\cite{liang2024large} about the usability of AI assistants reveals that these tools are already extensively used by practitioners to develop their code, yet there are several areas for improvement.
Similarly, the study by Pinto et al.~\cite{10.1145/3644815.3644949} reports positive feedback by the developers who use these tools. 

LLMs have been reported to be particularly useful to deal with repetitive tasks~\cite{abs-2406-07765}, as well as to ease project onboarding \cite {10662989}, revealing specific use cases where their utility could be maximized. 

Indeed, traces of developers using LLMs, ChatGPT in particular, are extensively available on public repositories. For instance, Tufano et al.~\cite{10.1145/3643991.3644918} analyzed GitHub commits, pull requests, and issues revealing how GitHub developers are using ChatGPT for a number of tasks, including feature implementation and enhancement, documentation, and testing. Broadly speaking, LLMs are already helpful in addressing a number of software engineering tasks, not only code generation~\cite{FanGHLSYZ23}.

This study complements this body of knowledge revealing insights on how to configure and use LLMs, and ChatGPT in particular. In fact, choosing appropriate values for the parameters that control LLMs, and deciding the number of requests to collect for the same prompt, are design decisions that can significantly impact the effectiveness of the interaction with an assistant.

\textbf{Studies about the influence of parameters}. Our study is not the only one considering the impact of the main parameters that affect the output of LLMs in code generation tasks. 

The study by Arora et al.~\cite{arora2024optimizing} investigates the role of temperature and top-p when GPT-3.5-turbo is prompted to generate method implementations, using a dataset of 13 methods. With our study, we extend this preliminary body of evidence by considering GPT-4o, a large-scale dataset with 548 methods, and studying not only the impact of parameters but also the influence of repetitions.  

Liu et al.~\cite{liu2024your} and Ouyang et al.~\cite{ouyang2023empirical} have also studied the effects of these parameters. Their studies show, consistent with our study, that controlling temperature and top-p matter. However, they recommend limiting creativity and randomness by using low values of both temperature and top-p. This is in contrast with our findings. Our analysis of ChatGPT's behavior with repeated submissions reveals that creativity facilitates the generation of challenging implementations. If it is true that low-temperature values may produce more consistent results, this may also limit the range of implementations that can be obtained. In fact, we ultimately recommend using five repetitions with temperature values greater or equal to $1.2$, rather than using low-temperature values that may limit the effectiveness of the ChatGPT. 



