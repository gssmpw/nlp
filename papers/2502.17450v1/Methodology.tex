In this section, we first discuss the parameters that can be defined by the user of ChatGPT and that may influence the results (Section~\ref{sec:parameters}). We then introduce the investigated research questions (Section~\ref{sec:rq}). We describe the dataset that we built to study the research questions (Section~\ref{ßec:dataset}). We finally report the experimental process that we defined to answer the research questions (Section~\ref{sec:process}).


\subsection{Parameters Influencing the Responses} \label{sec:parameters}

Some key parameters that influence the responses produced by GPT models are:
\begin{itemize}
\item \emph{Temperature} is often referred to as the parameter that determines the level of creativity of a model. In practice, it is a variable between $0.0$ and $2.0$ that affects the probabilities of occurrences of tokens, with values above $1.0$ inflating the probabilities of less likely tokens, and values below $1.0$ focusing on the most likely tokens only.

\item \emph{Top-p} is a parameter ranging between $0.0$ and $1.0$, controlling the number of tokens considered for the next token prediction\footnote{Not all implementations support $0.0$, some models may require a small but positive \emph{top-p} value.}. In particular, the model considers only enough tokens whose cumulative probabilities sum up to \emph{top-p} when predicting the next token~\cite{Holtzman:TopP:ICLR:2020}. Thus, the model considers fewer tokens for low \emph{top-p} values, and more tokens for high \emph{top-p} values. %$0$, only one token is considered, if \emph{top-p} is $1$, every possible token is considered, with intermediate values making a selection among the tokens.

\item \emph{Frequency penalty} is a parameter ranging between $-2.0$ and $+2.0$, assigning a positive (negative values of the parameter) or a negative (positive values of the parameter) reward for repeating tokens either in the prompt or in the response.

\item \emph{Presence penalty} is the same as the frequency penalty but it applies to the response only.

\end{itemize}


Since in code generation tasks, the same tokens may occur multiple times (e.g., the same variables occurring multiple times in a method implementation), we do not introduce any penalty using a value of $0.0$ for frequency penalty and presence penalty, while we systematically study the impact of temperature and top-p.

\subsection{Research Questions} \label{sec:rq}

With this study, we want to answer four key research questions about the impact of temperature and top-p, as well as the number of responses that must be collected for the same prompts to account for the non-determinism of the model properly. We also analyze if any interaction exists between the parameter values and the method's complexity and length.

\textbf{RQ1 - What is the impact of \emph{Temperature} on the correctness of the results?} This research question considers multiple values of the temperature, namely $0.0$, $0.8$, $1.2$, and $2.0$, investigating its impact on the correctness of the results.

\textbf{RQ2 - What is the impact of \emph{Top-p} on the correctness of the results?} This research question considers multiple values of \emph{top-p}, namely $0.0$, $0.5$, and $0.95$, investigating its impact on the correctness of the results.


\textbf{RQ3 - How many times should the same prompt be submitted to likely collect the correct results?} This research question studies how the non-determinism of the models impacts the correctness of the results, identifying an optimal number of repetitions to be completed.

\textbf{RQ4 - How does the effectiveness of the studied configurations change with the complexity and length of the methods?} This research question studies if the configurations obtained with different values of temperature and top-p perform differently for methods of different cyclomatic complexity and length.


\begin{figure*}
\centering
\includegraphics[width=\textwidth]{images/methodselection.png}
\caption{Dataset construction process.}
\label{fig:methodology}
\end{figure*}

\subsection{Dataset Construction} \label{ßec:dataset}

To build a dataset of methods whose implementation has to be generated by ChatGPT, we analyzed GitHub looking for relevant repositories and methods. The process applied to construct the dataset is illustrated in Figure~\ref{fig:methodology}.

We started by using the GitHub API to select all the repositories of Java applications ranked with at least 500 stars, to guarantee the popularity of the repository, and with commits occurring in the time window from October 3rd, 2023 to June 6th, 2024. Since GPT-4o has been trained before October 2023, this time window prevents any data leakage. To ease the automatic analysis of the code, we selected maven projects only. These criteria returned 910 eligible repositories.  

From this set of repositories, we looked for commits created between October 3rd, 2023 and June 6th, 2024 that add a full method implementation (i.e., the method has been added to the repository after GPT-4o has been trained) of at least 5 lines (to prevent dealing with trivial methods) in \texttt{\small src/main} (to focus on the core code of applications, discarding tests and auxiliary methods). Since we use the Javadoc comment as a method specification in the prompt created for ChatGPT, we only select the methods with at least five lines of Javadoc. Finally, since we need test cases to check the correctness of any plausible implementation of the method, we only select a method if there exists a test class with test cases in \texttt{\small src/test} associated with the selected method. After applying these criteria, we obtained 4,624 methods.

We check out all the selected methods and associated tests and compute the percentage of statements in the methods covered by the tests using JaCoCo\footnote{\url{https://www.eclemma.org/jacoco/}}. We discard methods whose coverage is below 80\% since the tests are supposed to not exercise well enough the behavior of the method, and thus they would not be a reliable proxy of correctness for any candidate implementation. From this step, we obtained 653 methods, whose coverage ranges from 80\% to 100\% with an average value of 97.4\%.

To make the final selection of the methods to be used for the study, we compute their cyclomatic complexity using Understand\footnote{\url{http://scitools.com}}. We then divide the methods into buckets, one bucket for each complexity value. Inside each bucket, we ordered the available methods first by the number of stars of the repository they have been extracted from, then by coverage value (giving higher priority to the methods with higher coverage), and finally by the commit date (higher priority to the most recent methods). We finally selected a total of 548 methods that are well distributed in terms of complexity values for our study. We selected a different number of methods from each bucket to reflect the non-uniform size of the buckets: method complexity ranges from 1 to 38, and as complexity increases, the number of methods in the bucket decreases.


\subsection{Experimental Process} \label{sec:process}

\emph{To answer RQ1}, for each method in the dataset, we submit to ChatGPT the request to produce an implementation starting from its signature, Javadoc descriptions, and the rest of the code in the class as context.
We repeat this request ten times, to account for non-determinism. Moreover, we repeat this process four times using four temperature values: $0.0$, $0.8$ (default value at the time we performed the study), $1.2$, and $2$. We keep top-p to its default value of 0.95. This results in a total of 27,450 method implementations collected (i.e., 4 temperature values $\times$ 10 repetitions $\times$ 548 methods).

To assess the quality of the recommended code, we distinguish three cases that can be determined automatically:

%\noindent 
\begin{itemize}

\item \emph{Invalid}: the returned code does not compile.

%\noindent 
\item \emph{Incorrect}: the returned code compiles but fails at least one of the available test cases.

%\noindent 

\item \emph{Plausible}: the returned code compiles and passes all the available test cases. 
\end{itemize}

We label the code as plausible and not correct since passing all the available test cases does not guarantee the correctness of the resulting code. On the other hand, the fact that the returned code compiles and passes a set of test cases exercising at least 80\% of the code in the implementation (97.4\% on average) provided by the developers represents a good indicator of the quality of the returned code. We relied on this proxy measure of correctness to complete a large-scale assessment of ChatGPT.

To determine the role of the parameter temperature, we compare the rate of invalid, incorrect, and plausible implementations returned while changing the temperature.  

\emph{To answer RQ2}, we select the temperature value producing the most promising results with RQ1 and submit the same prompts to ChatGPT using different top-p values: $0.95$ (already used in RQ1), $0.5$, and $0.0$. We execute each prompt five times to account for non-determinism. This leads to a total of 4,480 additional method implementations collected (i.e., 2 more top-p values $\times$ 5 repetitions $\times$ 548 methods).

Again, we compare the results considering the rate of invalid, incorrect, and plausible implementations returned while changing top-p.  

\emph{To answer RQ3}, we analyze the data collected for RQ1 and RQ2, considering the number of repetitions needed to collect a correct answer. In particular, we compute the $pass@k$ metric that returns, for a given method, the probability of collecting at least a plausible implementation with $k$ submissions of the same prompts. 

For each method, we compute the value $pass@k$, for $k=1\ldots n$, where $n=10$ is the number of repetitions we completed. In particular, we  compute the metric as recommended by Lyu et al.~\cite{lyu2024top} according to the formula 


\[
\text{\textit{pass@k(m)}} = 1 - \dfrac{\dbinom{n - c}{k}}{\dbinom{n}{k}}
\]

\noindent
where $c$ is the number of plausible method implementation for method $m$, and $n=10$ is the total number of repetitions. Note that \(\binom{n - c}{k}\) represents the number of combinations of $k$ sequential responses without any plausible implementation, and \(\binom{n}{k}\) is the total number of $k$ sequential responses that can be returned. 

Based on the value of the metric $pass@k$, we discuss the values of $k$ that could guarantee a good compromise between the number of repetitions and the probability of collecting a plausible implementation.

\emph{To answer RQ4}, we analyze the data collected for RQ1 and RQ2, studying the percentage of plausible answers provided for methods of increasing cyclomatic complexity and length. We also compute the Pearson correlation coefficient, to determine the existence of any correlation between the rate of plausible method implementations generated, and the complexity and length of the methods. The analysis is repeated for each studied configuration.


\subsection{Tool Support}
We automated both the dataset construction process and the prompt submission process, to scale to the magnitude required by this study. Both tools are implemented as a set of Python scripts that we released publicly, jointly with our dataset, the response generated by ChatGPT, and the code to generate the visualizations used in this paper. The experimental material is available in the following repository \urlRepo.