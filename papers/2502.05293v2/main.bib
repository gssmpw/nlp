% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

 @inproceedings{Acar_Blelloch_Blumofe_2000, address={New York, NY, USA}, series={SPAA ’00}, title={The data locality of work stealing}, ISBN={978-1-58113-185-7}, url={https://dl.acm.org/doi/10.1145/341800.341801}, DOI={10.1145/341800.341801}, abstractNote={This paper studies the data locality of the work-stealing scheduling algorithm on hardware-controlled shared-memory machines. We present lower and upper bounds on the number of cache misses using work stealing, and introduce a locality-guided work-stealing algorithm along with experimental validation.As a lower bound, we show that there is a family of multi-threaded computations Gn each member of which requires Θ(n) total instructions (work) for which when using work-stealing the number of cache misses on one processor is constant, while even on two processors the total number of cache misses is Ω(n). This implies that for general computations there is no useful bound relating multiprocessor to uninprocessor cache misses. For nested-parallel computations, however, we show that on P processors the expected additional number of cache misses beyond those on a single processor is bounded by O(C⌈m/s PT∞), where m is the execution time of an instruction incurring a cache miss, s is the steal time, C is the size of cache, and T∞ is the number of nodes on the longest chain of dependences. Based on this we give strong bounds on the total running time of nested-parallel computations using work stealing.For the second part of our results, we present a locality-guided work stealing algorithm that improves the data locality of multi-threaded computations by allowing a thread to have an affinity for a processor. Our initial experiments on iterative data-parallel applications show that the algorithm matches the performance of static-partitioning under traditional work loads but improves the performance up to 50% over static partitioning under multiprogrammed work loads. Furthermore, the locality-guided work stealing improves the performance of work-stealing up to 80%.}, booktitle={12th Annual ACM Symposium on Parallel Algorithms and Architectures}, publisher={Association for Computing Machinery}, author={Acar, Umut A. and Blelloch, Guy E. and Blumofe, Robert D.}, year={2000}, month=jul, pages={1–12}, collection={SPAA ’00} }

 @article{Acar_Charguéraud_Muller_Rainey_2013, title={Atomic Read-Modify-Write Operations are Unnecessary for Shared-Memory Work Stealing}, abstractNote={We present a work-stealing algorithm for total-store memory architectures, such as Intel’s X86, that does not rely on atomic read-modify-write instructions such as compare-and-swap. In our algorithm, processors communicate solely by reading from and writing (non-atomically) into weakly consistent memory. We also show that join resolution, an important problem in scheduling parallel programs, can also be solved without using atomic read-modify-write instructions. At a high level, our work-stealing algorithm closely resembles traditional work-stealing algorithms, but certain details are more complex. Instead of relying on atomic read-modify-write operations, our algorithm uses a steal protocol that enables processors to perform load balancing by using only two memory cells per processor. The steal protocol permits data races but guarantees correctness by using a time-stamping technique. Proving the correctness of our algorithms is made challenging by weakly consistent shared-memory that permits processors to observe sequentially inconsistent views. We therefore carefully specify our algorithms and prove them correct by considering a costed refinement of the X86-TSO model, a precise characterization of total-store-order architectures. We show that our algorithms are practical by implementing them as part of a C++ library and performing an experimental evaluation. Our results show that our work-stealing algorithm is competitive with the state-of-the-art implementations even on current architectures where atomic read-modify-write instructions are cheap. Our join resolution algorithm incurs a relatively small overhead compared to an efficient algorithm that uses atomic read-modify-write instructions.}, author={Acar, Umut and Charguéraud, Arthur and Muller, Stefan and Rainey, Mike}, year={2013}, month=sep }

 @article{alakeel_guide_2009, title={A Guide to Dynamic Load Balancing in Distributed Computer Systems}, volume={10}, abstractNote={Load balancing is the process of redistributing the work load among nodes of the distributed system to improve both resource utilization and job response time while also avoiding a situation where some nodes are heavily loaded while others are idle or doing little work. A dynamic load balancing algorithm assumes no a priori knowledge about job behavior or the global state of the system, i.e., load balancing decisions are solely based on the current status of the system. The development of an effective dynamic load balancing algorithm involves many important issues: load estimation, load levels comparison, performance indices, system stability, amount of information exchanged among nodes, job resource requirements estimation, job’s selection for transfer, remote nodes selection, and more. This paper presents and analyses the aforementioned issues that need to be considered in the development or study of a dynamic load balancing algorithm.}, journal={International Journal of Computer Science and Network Security (IJCSNS)}, author={Alakeel, Ali}, year={2009}, month=nov }

@inproceedings{ompss12,
  title={Productive programming of {GPU} clusters with {OmpSs}},
  author={Bueno, Javier and Planas, Judit and Duran, Alejandro and Badia, Rosa M and Martorell, Xavier and Ayguade, Eduard and Labarta, Jes{\'u}s},
  booktitle={International Parallel and Distributed Processing Symposium},
  year={2012},
}
@INPROCEEDINGS{legion_overview,  author={Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex},  booktitle={International Conference on High Performance Computing, Networking, Storage and Analysis},   title={Legion: Expressing locality and independence with logical regions},   year={2012},  volume={},  number={},  doi={10.1109/SC.2012.71}}

@article{workstealing_locality02,
  author    = {Umut A. Acar and
               Guy E. Blelloch and
               Robert D. Blumofe},
  title     = {The Data Locality of Work Stealing},
  journal   = {Theory of Computing Systems},
  volume    = 35,
  number    = 3,
  pages     = {321--347},
  year      = 2002,
}
@article{xkaapi15,
  TITLE = {{Design and analysis of scheduling strategies for multi-{CPU} and multi-{GPU} architectures}},
  AUTHOR = {Ferreira Lima, Joao Vicente and Gautier, Thierry and Danjean, Vincent and Raffin, Bruno and Maillard, Nicolas},
  JOURNAL = {{Parallel Computing}},
  PUBLISHER = {{Elsevier}},
  VOLUME = {44},
  PAGES = {37-52},
  YEAR = {2015},
  KEYWORDS = {Work stealing ; Data-flow dependencies ; Task parallelism ; Accelerators ; Parallel programming},
  PDF = {https://hal.inria.fr/hal-01132037/file/parco2014.pdf},
  HAL_ID = {hal-01132037},
  HAL_VERSION = {v1},
}
@Article{starpu,
  author 	= {C{\'e}dric Augonnet and Samuel Thibault and Raymond Namyst and Pierre-Andr{\'e} Wacrenier},
  title		= {{StarPU}: A Unified Platform for Task Scheduling on Heterogeneous Multicore Architectures},
  journal	= {Concurrency and Computation: Practice and Experience, Special Issue: Euro-Par 2009},
  volume	= 23,
  issue		= 2,
  year		= 2011,
  publisher	= {John Wiley & Sons, Ltd.},
  doi		= {10.1002/cpe.1631},
  keywords      = {StarPU}
}
@article{parsec,
  TITLE = {{PaRSEC: A programming paradigm exploiting heterogeneity for enhancing scalability}},
  AUTHOR = {Bosilca, George and Bouteiller, Aur{\'e}lien and Danalis, Anthony and Faverge, Mathieu and H{\'e}rault, Thomas and Dongarra, Jack},
  JOURNAL = {{Computing in Science and Engineering}},
  PUBLISHER = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  VOLUME = {15},
  NUMBER = {6},
  YEAR = {2013},
  HAL_ID = {hal-00930217},
  HAL_VERSION = {v1},
}
@inproceedings{yoo2013MST,
author = {Yoo, Richard M. and Hughes, Christopher J. and Kim, Changkyu and Chen, Yen-Kuang and Kozyrakis, Christos},
title = {Locality-Aware Task Management for Unstructured Parallelism: A Quantitative Limit Study},
year = 2013,
doi = {10.1145/2486159.2486175},
booktitle = {{ACM} Symposium on Parallelism in Algorithms and Architectures ({SPAA})},
}
@inproceedings{HongKung81,
 author = {J.-W. Hong and H.T. Kung},
 title = {{I/O} complexity: {T}he red-blue pebble game},
 booktitle = {13th ACM Symposium on Theory of Computing},
 year = {1981},
 pages = {326-333},
 publisher = {ACM Press}
}
@article{10.1145/209937.209958,
author = {Blumofe, Robert D. and Joerg, Christopher F. and Kuszmaul, Bradley C. and Leiserson, Charles E. and Randall, Keith H. and Zhou, Yuli},
title = {Cilk: An efficient multithreaded runtime system},
year = {1995},
issue_date = {Aug. 1995},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {30},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/209937.209958},
doi = {10.1145/209937.209958},
abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical path” of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of optimal.The Cilk runtime system  currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the *Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship.},
journal = {SIGPLAN Not.},
month = aug,
pages = {207–216},
numpages = {10}
}

@inproceedings{10.1145/209936.209958, author = {Blumofe, Robert D. and Joerg, Christopher F. and Kuszmaul, Bradley C. and Leiserson, Charles E. and Randall, Keith H. and Zhou, Yuli}, title = {Cilk: an efficient multithreaded runtime system}, year = {1995}, isbn = {0897917006}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/209936.209958}, doi = {10.1145/209936.209958}, abstract = {Cilk (pronounced “silk”) is a C-based runtime system for multi-threaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the “work” and “critical path” of a Cilk computation can be used to accurately model performance. Consequently, a Cilk programmer can focus on reducing the work and critical path of his computation, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of “fully strict” (well-structured) programs, the Cilk scheduler achieves space, time and communication bounds all within a constant factor of optimal.The Cilk runtime system  currently runs on the Connection Machine CM5 MPP, the Intel Paragon MPP, the Silicon Graphics Power Challenge SMP, and the MIT Phish network of workstations. Applications written in Cilk include protein folding, graphic rendering, backtrack search, and the *Socrates chess program, which won third prize in the 1994 ACM International Computer Chess Championship.}, booktitle = {5th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, pages = {207–216}, numpages = {10}, location = {Santa Barbara, California, USA}, series = {PPOPP '95}}


@inproceedings{10.1145/3018743.3018768,
author = {Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu},
title = {An Efficient Abortable-locking Protocol for Multi-level NUMA Systems},
year = {2017},
isbn = {9781450344937},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3018743.3018768},
doi = {10.1145/3018743.3018768},
abstract = {The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its protocol on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits a timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts to be lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog. HMCS-T, used in an MPI runtime lock, mitigated the poor scalability of an MPI+OpenMP BFS code and resulted in 4.3x superior scaling.},
booktitle = {22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
pages = {61–74},
numpages = {14},
keywords = {abortable lock, hierarchical lock, mcs lock, numa, queuing lock, spin lock, synchronization, timeout lock},
location = {Austin, Texas, USA},
series = {PPoPP '17}
}

@article{10.1145/3155284.3018768,
author = {Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu},
title = {An Efficient Abortable-locking Protocol for Multi-level {NUMA} Systems},
year = {2017},
issue_date = {August 2017},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {52},
number = {8},
issn = {0362-1340},
url = {https://doi.org/10.1145/3155284.3018768},
doi = {10.1145/3155284.3018768},
journal = {SIGPLAN Notices},
month = jan,
pages = {61–74},
numpages = {14},
keywords = {abortable lock, hierarchical lock, mcs lock, numa, queuing lock, spin lock, synchronization, timeout lock}
}


@article{10.1145/3275443,
author = {Amer, Abdelhalim and Lu, Huiwei and Balaji, Pavan and Chabbi, Milind and Wei, Yanjie and Hammond, Jeff and Matsuoka, Satoshi},
title = {Lock Contention Management in Multithreaded {MPI}},
year = {2019},
issue_date = {September 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {5},
number = {3},
issn = {2329-4949},
url = {https://doi.org/10.1145/3275443},
doi = {10.1145/3275443},
journal = {ACM Transactions on Parallelel Computing},
month = jan,
articleno = {12},
numpages = {21},
keywords = {threads, runtime contention, critical section, MPI}
}
@inproceedings{cosma2019,
  author    = {Grzegorz Kwasniewski and
               Marko Kabic and
               Maciej Besta and
               Joost VandeVondele and
               Raffaele Solc{\`{a}} and
               Torsten Hoefler},
  title     = {Red-blue pebbling revisited: near optimal parallel matrix-matrix multiplication},
  booktitle = {Int. Conf. for High Performance Computing,
               Networking, Storage and Analysis, {SC} 2019},
  year      = {2019},
  doi       = {10.1145/3295500.3356181},
  timestamp = {Sat, 09 Nov 2019 12:06:02 +0100},
  biburl    = {https://dblp.org/rec/conf/sc/KwasniewskiKBVS19.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@INPROCEEDINGS{gonthier-ipdps,
  author={Gonthier, Maxime and Marchal, Loris and Thibault, Samuel},
  booktitle={IEEE International Parallel and Distributed Processing Symposium},
  title={Memory-Aware Scheduling of Tasks Sharing Data on Multiple {GPUs} with Dynamic Runtime Systems},
  year={2022},
  doi={10.1109/IPDPS53621.2022.00073}}

 @article{Amer_Lu_Balaji_Chabbi_Wei_Hammond_Matsuoka_2019, title={Lock Contention Management in Multithreaded MPI}, volume={5}, ISSN={2329-4949}, DOI={10.1145/3275443}, abstractNote={In this article, we investigate contention management in lock-based thread-safe MPI libraries. Specifically, we make two assumptions: (1) locks are the only form of synchronization when protecting communication paths; and (2) contention occurs, and thus serialization is unavoidable. Our work distinguishes between lock acquisitions with respect to work being performed inside a critical section; productive vs. unproductive. Waiting for message reception without doing anything else inside a critical section is an example of unproductive lock acquisition. We show that the high-throughput nature of modern scalable locking protocols translates into better communication progress for throughput-intensive MPI communication but negatively impacts latency-sensitive communication because of overzealous unproductive lock acquisition. To reduce unproductive lock acquisitions, we devised a method that promotes threads with productive work using a generic two-level priority locking protocol. Our results show that using a high-throughput protocol for productive work and a fair protocol for less productive code paths ensures the best tradeoff for fine-grained communication, whereas a fair protocol is sufficient for more coarse-grained communication. Although these efforts have been rewarding, scalability degradation remains significant. We discuss techniques that diverge from the pure locking model and offer the potential to further improve scalability.}, number={3}, journal={ACM Trans. Parallel Comput.}, author={Amer, Abdelhalim and Lu, Huiwei and Balaji, Pavan and Chabbi, Milind and Wei, Yanjie and Hammond, Jeff and Matsuoka, Satoshi}, year={2019}, month=jan, pages={12:1-12:21} }

 @inproceedings{Arnautov_Felber_Fetzer_Trach_2017, title={FFQ: A Fast Single-Producer/Multiple-Consumer Concurrent FIFO Queue}, ISSN={1530-2075}, url={https://ieeexplore.ieee.org/document/7967181}, DOI={10.1109/IPDPS.2017.41}, abstractNote={With the spreading of multi-core architectures, operating systems and applications are becoming increasingly more concurrent and their scalability is often limited by the primitives used to synchronize the different hardware threads. In this paper, we address the problem of how to optimize the throughput of a system with multiple producer and consumer threads. Such applications typically synchronize their threads via multi-producer/multi-consumer FIFO queues, but existing solutions have poor scalability, as we could observe when designing a secure application framework that requires high-throughput communication between many concurrent threads. In our target system, however, the items enqueued by different producers do not necessarily need to be FIFO ordered. Hence, we propose a fast FIFO queue, FFQ, that aims at maximizing throughput by specializing the algorithm for single-producer/multiple-consumer settings: each producer has its own queue from which multiple consumers can concurrently dequeue. Furthermore, while we provide a wait-free interface for producers, we limit ourselves to lock-free consumers to eliminate the need for helping. We also propose a multi-producer variant to show which synchronization operations we were able to remove by focusing on a single producer variant. Our evaluation analyses the performance using micro-benchmarks and compares our results with other state-of-the-art solutions: FFQ exhibits excellent performance and scalability.}, booktitle={2017 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, author={Arnautov, Sergei and Felber, Pascal and Fetzer, Christof and Trach, Bohdan}, year={2017}, month=may, pages={907–916} }

 @inproceedings{Arora_Blumofe_Plaxton_1998, address={New York, NY, USA}, series={SPAA ’98}, title={Thread scheduling for multiprogrammed multiprocessors}, ISBN={978-0-89791-989-0}, url={https://dl.acm.org/doi/10.1145/277651.277678}, DOI={10.1145/277651.277678}, booktitle={10th Annual ACM Symposium on Parallel Algorithms and Architectures}, publisher={Association for Computing Machinery}, author={Arora, Nimar S. and Blumofe, Robert D. and Plaxton, C. Greg}, year={1998}, month=jun, pages={119–129}, collection={SPAA ’98} }

 @article{Ayguade_Copty_Duran_Hoeflinger_Lin_Massaioli_Teruel_Unnikrishnan_Zhang_2009, title={The Design of OpenMP Tasks}, volume={20}, ISSN={1558-2183}, DOI={10.1109/TPDS.2008.105}, abstractNote={OpenMP has been very successful in exploiting structured parallelism in applications. With increasing application complexity, there is a growing need for addressing irregular parallelism in the presence of complicated control structures. This is evident in various efforts by the industry and research communities to provide a solution to this challenging problem. One of the primary goals of OpenMP 3.0 is to define a standard dialect to express and efficiently exploit unstructured parallelism. This paper presents the design of the OpenMP tasking model by members of the OpenMP 3.0 tasking sub-committee which was formed for this purpose. The paper summarizes the efforts of the sub-committee (spanning over two years) in designing, evaluating and seamlessly integrating the tasking model into the OpenMP specification. In this paper, we present the design goals and key features of the tasking model, including a rich set of examples and an in-depth discussion of the rationale behind various design choices. We compare a prototype implementation of the tasking model with existing models, and evaluate it on a wide range of applications. The comparison shows that the OpenMP tasking model provides expressiveness, flexibility, and huge potential for performance and scalability.}, number={3}, journal={IEEE Transactions on Parallel and Distributed Systems}, author={Ayguade, Eduard and Copty, Nawal and Duran, Alejandro and Hoeflinger, Jay and Lin, Yuan and Massaioli, Federico and Teruel, Xavier and Unnikrishnan, Priya and Zhang, Guansong}, year={2009}, month=mar, pages={404–418} }

 @inproceedings{parsl, address={New York, NY, USA}, series={HPDC ’19}, title={Parsl: Pervasive Parallel Programming in Python}, ISBN={978-1-4503-6670-0}, url={https://dl.acm.org/doi/10.1145/3307681.3325400}, DOI={10.1145/3307681.3325400}, abstractNote={High-level programming languages such as Python are increasingly used to provide intuitive interfaces to libraries written in lower-level languages and for assembling applications from various components. This migration towards orchestration rather than implementation, coupled with the growing need for parallel computing (e.g., due to big data and the end of Moore’s law), necessitates rethinking how parallelism is expressed in programs. Here, we present Parsl, a parallel scripting library that augments Python with simple, scalable, and flexible constructs for encoding parallelism. These constructs allow Parsl to construct a dynamic dependency graph of components that it can then execute efficiently on one or many processors. Parsl is designed for scalability, with an extensible set of executors tailored to different use cases, such as low-latency, high-throughput, or extreme-scale execution. We show, via experiments on the Blue Waters supercomputer, that Parsl executors can allow Python scripts to execute components with as little as 5 ms of overhead, scale to more than 250000 workers across more than 8000 nodes, and process upward of 1200 tasks per second. Other Parsl features simplify the construction and execution of composite programs by supporting elastic provisioning and scaling of infrastructure, fault-tolerant execution, and integrated wide-area data management. We show that these capabilities satisfy the needs of many-task, interactive, online, and machine learning applications in fields such as biology, cosmology, and materials science.}, booktitle={28th International Symposium on High-Performance Parallel and Distributed Computing}, publisher={Association for Computing Machinery}, author={Babuji, Yadu and Woodard, Anna and Li, Zhuozhao and Katz, Daniel S. and Clifford, Ben and Kumar, Rohan and Lacinski, Lukasz and Chard, Ryan and Wozniak, Justin M. and Foster, Ian and Wilde, Michael and Chard, Kyle}, year={2019}, month=jun, pages={25–36}, collection={HPDC ’19} }
 
@inproceedings{legion_overview_dup, address={Washington, DC, USA}, series={SC ’12}, title={Legion: Expressing locality and independence with logical regions}, ISBN={978-1-4673-0804-5}, abstractNote={Modern parallel architectures have both heterogeneous processors and deep, complex memory hierarchies. We present Legion, a programming model and runtime system for achieving high performance on these machines. Legion is organized around logical regions, which express both locality and independence of program data, and tasks, functions that perform computations on regions. We describe a runtime system that dynamically extracts parallelism from Legion programs, using a distributed, parallel scheduling algorithm that identifies both independent tasks and nested parallelism. Legion also enables explicit, programmer controlled movement of data through the memory hierarchy and placement of tasks based on locality information via a novel mapping interface. We evaluate our Legion implementation on three applications: fluid-flow on a regular grid, a three-level AMR code solving a heat diffusion equation, and a circuit simulation.}, booktitle={International Conference on High Performance Computing, Networking, Storage and Analysis}, publisher={IEEE Computer Society Press}, author={Bauer, Michael and Treichler, Sean and Slaughter, Elliott and Aiken, Alex}, year={2012}, month=nov, pages={1–11}, collection={SC ’12} }

 @inproceedings{Beamer_Asanović_Patterson_2012, address={Washington, DC, USA}, series={SC ’12}, title={Direction-optimizing breadth-first search}, ISBN={978-1-4673-0804-5}, abstractNote={Breadth-First Search is an important kernel used by many graph-processing applications. In many of these emerging applications of BFS, such as analyzing social networks, the input graphs are low-diameter and scale-free. We propose a hybrid approach that is advantageous for low-diameter graphs, which combines a conventional top-down algorithm along with a novel bottom-up algorithm. The bottom-up algorithm can dramatically reduce the number of edges examined, which in turn accelerates the search as a whole. On a multi-socket server, our hybrid approach demonstrates speedups of 3.3--7.8 on a range of standard synthetic graphs and speedups of 2.4--4.6 on graphs from real social networks when compared to a strong baseline. We also typically double the performance of prior leading shared memory (multicore and GPU) implementations.}, booktitle={International Conference on High Performance Computing, Networking, Storage and Analysis}, publisher={IEEE Computer Society Press}, author={Beamer, Scott and Asanović, Krste and Patterson, David}, year={2012}, month=nov, pages={1–10}, collection={SC ’12} }
 @article{Berenbrink_Friedetzky_Goldberg_2003, title={The Natural Work-Stealing Algorithm is Stable}, volume={32}, DOI={10.1137/S0097539701399551}, abstractNote={In this paper we analyze a very simple dynamic work-stealing algorithm. In the work-generation model, there are n (work) generators. A generator-allocation function is simply a function from the n generators to the n processors. We consider a fixed, but arbitrary, distribution cal D over generator-allocation functions. During each time step of our process, a generator-allocation function h is chosen from cal D, and the generators are allocated to the processors according to h. Each generator may then generate a unit-time task, which it inserts into the queue of its host processor. It generates such a task independently with probability łambda. After the new tasks are generated, each processor removes one task from its queue and services it. For many choices of cal D, the work-generation model allows the load to become arbitrarily imbalanced, even when łambda < 1. For example, cal D could be the point distribution containing a single function h which allocates all of the generators to just one processor. For this choice of cal D, the chosen processor receives around łambda n units of work at each step and services one. The natural work-stealing algorithm that we analyze is widely used in practical applications and works as follows. During each time step, each empty processor (with no work to do) sends a request to a randomly selected other processor. Any nonempty processor having received at least one such request in turn decides (again randomly) in favor of one of the requests. The number of tasks which are transferred from the nonempty processor to the empty one is determined by the so-called work-stealing functionf . In particular, if a processor that accepts a request has ell tasks stored in its queue, then f(ell) tasks are transferred to the currently empty one. A popular work-stealing function is f(ell)=łfloor ell/2rfloor, which transfers (roughly) half of the tasks. We analyze the long-term behavior of the system as a function of łambda and f. We show that the system is stable for any constant generation rate łambda < 1 and for a wide class of functions f. Most intuitively sensible functions are included in this class (for example, every monotonically nondecreasing function f which satisfies 0 łeq f(ell)łeq ell/2 and f(ell)=ømega(1) as a function of ell is included). Furthermore, we give upper bounds on theaverage system load (as a function of f and n). Our proof techniques combine Lyapunov function arguments with domination arguments, which are needed to cope with dependency.}, number={5}, journal={SIAM Journal on Computing}, author={Berenbrink, Petra and Friedetzky, Tom and Goldberg, Leslie Ann}, year={2003}, pages={1260–1279} }

 @inproceedings{blagodurov_case_2010, address={Vienna Austria}, title={A case for NUMA-aware contention management on multicore systems}, ISBN={978-1-4503-0178-7}, url={https://dl.acm.org/doi/10.1145/1854273.1854350}, DOI={10.1145/1854273.1854350}, abstractNote={On multicore systems, contention for shared resources occurs when memory-intensive threads are co-scheduled on cores that share parts of the memory hierarchy, such as last-level caches and memory controllers. Previous work investigated how contention could be addressed via scheduling. A contention-aware scheduler separates competing threads onto separate memory hierarchy domains to eliminate resource sharing and, as a consequence, to mitigate contention. However, all previous work on contention-aware scheduling assumed that the underlying system is UMA (uniform memory access latencies, single memory controller). Modern multicore systems, however, are NUMA, which means that they feature non-uniform memory access latencies and multiple memory controllers.}, booktitle={19th International Conference on Parallel Architectures and Compilation Techniques}, publisher={ACM}, author={Blagodurov, Sergey and Zhuravlev, Sergey and Fedorova, Alexandra and Kamali, Ali}, year={2010}, month=sep, pages={557–558}, language={en} }

 @article{Blumofe_Leiserson_1999, title={Scheduling multithreaded computations by work stealing}, volume={46}, ISSN={0004-5411}, DOI={10.1145/324133.324234}, abstractNote={This paper studies the problem of efficiently schedulling fully strict (i.e., well-structured) multithreaded computations on parallel computers. A popular and practical method of scheduling this kind of dynamic MIMD-style computation is “work stealing,” in which processors needing work steal computational threads from other processors. In this paper, we give the first provably good work-stealing scheduler for multithreaded computations with dependencies.Specifically, our analysis shows that the expected time to execute a fully strict computation on P processors using our work-stealing scheduler is T1/P + O(T ∞ , where T1 is the minimum serial execution time of the multithreaded computation and (T ∞ is the minimum execution time with an infinite number of processors. Moreover, the space required by the execution is at most S1P, where S1 is the minimum serial space requirement. We also show that the expected total communication of the algorithm is at most O(PT ∞( 1 + nd)Smax), where Smax is the size of the largest activation record of any thread and nd is the maximum number of times that any thread synchronizes with its parent. This communication bound justifies the folk wisdom that work-stealing schedulers are more communication efficient than their work-sharing counterparts. All three of these bounds are existentially optimal to within a constant factor.}, number={5}, journal={J. ACM}, author={Blumofe, Robert D. and Leiserson, Charles E.}, year={1999}, month=sep, pages={720–748} }
 @inproceedings{Calvin_Lewis_Valeev_2015, address={New York, NY, USA}, series={IA3 ’15}, title={Scalable task-based algorithm for multiplication of block-rank-sparse matrices}, ISBN={978-1-4503-4001-4}, url={https://dl.acm.org/doi/10.1145/2833179.2833186}, DOI={10.1145/2833179.2833186}, abstractNote={A task-based formulation of Scalable Universal Matrix Multiplication Algorithm (SUMMA), a popular algorithm for matrix multiplication (MM), is applied to the multiplication of hierarchy-free, rank-structured matrices that appear in the domain of quantum chemistry (QC). The novel features of our formulation are: (1) concurrent scheduling of multiple SUMMA iterations, and (2) fine-grained task-based composition. These features make it tolerant of the load imbalance due to the irregular matrix structure and eliminate all artifactual sources of global synchronization. Scalability of iterative computation of square-root inverse of block-rank-sparse QC matrices is demonstrated; for full-rank (dense) matrices the performance of our SUMMA formulation usually exceeds that of the state-of-the-art dense MM implementations (ScaLAPACK and Cyclops Tensor Framework).}, booktitle={5th Workshop on Irregular Applications: Architectures and Algorithms}, publisher={Association for Computing Machinery}, author={Calvin, Justus A. and Lewis, Cannada A. and Valeev, Edward F.}, year={2015}, month=nov, pages={1–8}, collection={IA3 ’15} }
 @inproceedings{Chabbi_Amer_Wen_Liu_2017, address={New York, NY, USA}, series={PPoPP ’17}, title={An Efficient Abortable-locking Protocol for Multi-level NUMA Systems}, ISBN={978-1-4503-4493-7}, url={https://doi.org/10.1145/3018743.3018768}, DOI={10.1145/3018743.3018768}, abstractNote={The popularity of Non-Uniform Memory Access (NUMA) architectures has led to numerous locality-preserving hierarchical lock designs, such as HCLH, HMCS, and cohort locks. Locality-preserving locks trade fairness for higher throughput. Hence, some instances of acquisitions can incur long latencies, which may be intolerable for certain applications. Few locks admit a waiting thread to abandon its protocol on a timeout. State-of-the-art abortable locks are not fully locality aware, introduce high overheads, and unsuitable for frequent aborts. Enhancing locality-aware locks with lightweight timeout capability is critical for their adoption. In this paper, we design and evaluate the HMCS-T lock, a Hierarchical MCS (HMCS) lock variant that admits a timeout. HMCS-T maintains the locality benefits of HMCS while ensuring aborts to be lightweight. HMCS-T offers the progress guarantee missing in most abortable queuing locks. Our evaluations show that HMCS-T offers the timeout feature at a moderate overhead over its HMCS analog. HMCS-T, used in an MPI runtime lock, mitigated the poor scalability of an MPI+OpenMP BFS code and resulted in 4.3x superior scaling.}, booktitle={22nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, publisher={Association for Computing Machinery}, author={Chabbi, Milind and Amer, Abdelhalim and Wen, Shasha and Liu, Xu}, year={2017}, month=jan, pages={61–74}, collection={PPoPP ’17} }
 @article{Chen_Guo_2018, title={Contention and Locality-Aware Work-Stealing for Iterative Applications in Multi-Socket Computers}, volume={67}, ISSN={1557-9956}, DOI={10.1109/TC.2017.2783932}, abstractNote={Modern large-scale computers have shifted to Multi-socket Multi-core (MSMC) architectures, where multiple CPU chips are integrated into a machine as sockets and multiple memory nodes are integrated into the shared main memory (NUMA). To improve the hardware utilization of MSMC computers, multiple programs are often executed concurrently. However, most work-stealing schedulers are designed for single-socket architectures and contention-free scenarios. Work-stealing programs sufferfrom very-high-frequency remote memory access and serious interference from co-located programs in MSMC architectures, which in turn significantly degrade their performance. To solve these two problems, we propose a Contentionand Locality-Aware Work-Stealing (CLAWS) scheduler. CLAWS first evenly distributes the data set of a program to all the memory nodes and allocates a task to the socket where the local memory node stores its data. Then, according to the real-time contention situation for each socket collected at runtime, CLAWS dynamically migrates data and re-allocates the corresponding tasks to balance the workload and reduce remote memory accesses. Experimental results show that CLAWS can improve the performance of memory-bound programs by 40.1 percent on average compared with traditional work-stealing schedulers. Meanwhile, CLAWS is also more energy efficient than traditional work-stealing schedulers.}, number={6}, journal={IEEE Transactions on Computers}, author={Chen, Quan and Guo, Minyi}, year={2018}, month=jun, pages={784–798} }
 @inproceedings{Chen_Guo_Guan_2014, address={Munich Germany}, title={LAWS: locality-aware work-stealing for multi-socket multi-core architectures}, ISBN={978-1-4503-2642-1}, url={https://dl.acm.org/doi/10.1145/2597652.2597665}, DOI={10.1145/2597652.2597665}, abstractNote={Modern mainstream powerful computers adopt Multi-Socket Multi-Core (MSMC) CPU architecture and NUMA-based memory architecture. While traditional work-stealing schedulers are designed for single-socket architectures, they incur severe shared cache misses and remote memory accesses in these computers, which can degrade the performance of memory-bound applications seriously. To solve the problem, we propose a Locality-Aware Work-Stealing (LAWS) scheduler, which better utilizes both the shared cache and the NUMA memory system. In LAWS, a load-balanced task allocator is used to evenly split and store the data set of a program to all the memory nodes and allocate a task to the socket where the local memory node stores its data. Then, an adaptive DAG packer adopts an auto-tuning approach to optimally pack an execution DAG into many cache-friendly subtrees. Meanwhile, a triple-level work-stealing scheduler is applied to schedule the subtrees and the tasks in each subtree. Experimental results show that LAWS can improve the performance of memory-bound programs up to 54.2% compared with traditional work-stealing schedulers.}, booktitle={28th ACM international conference on Supercomputing}, publisher={ACM}, author={Chen, Quan and Guo, Minyi and Guan, Haibing}, year={2014}, month=jun, pages={3–12}, language={en} }
 @inproceedings{Chen_Guo_Huang_2012, address={San Servolo Island, Venice Italy}, title={CATS: cache aware task-stealing based on online profiling in multi-socket multi-core architectures}, ISBN={978-1-4503-1316-2}, url={https://dl.acm.org/doi/10.1145/2304576.2304599}, DOI={10.1145/2304576.2304599}, abstractNote={Multi-socket Multi-core architectures with shared caches in each socket have become mainstream when a single multicore chip cannot provide enough computing capacity for high performance computing. However, traditional task-stealing schedulers tend to pollute the shared cache and incur severe cache misses due to their randomness in stealing. To address the problem, this paper proposes a Cache Aware TaskStealing (CATS) scheduler, which uses the shared cache eﬃciently with an online proﬁling method and schedules tasks with shared data to the same socket. CATS adopts an online DAG partitioner based on the proﬁling information to ensure tasks with shared data can eﬃciently utilize the shared cache. One outstanding novelty of CATS is that it does not require any extra user-provided information. Experimental results show that CATS can improve the performance of memory-bound programs up to 74.4% compared with the traditional task-stealing scheduler.}, booktitle={26th ACM international conference on Supercomputing}, publisher={ACM}, author={Chen, Quan and Guo, Minyi and Huang, Zhiyi}, year={2012}, month=jun, pages={163–172}, language={en} }

 @article{Dagum_Menon_1998, title={OpenMP: an industry standard API for shared-memory programming}, volume={5}, ISSN={1558-190X}, DOI={10.1109/99.660313}, abstractNote={At its most elemental level, OpenMP is a set of compiler directives and callable runtime library routines that extend Fortran (and separately, C and C++ to express shared memory parallelism. It leaves the base language unspecified, and vendors can implement OpenMP in any Fortran compiler. Naturally, to support pointers and allocatables, Fortran 90 and Fortran 95 require the OpenMP implementation to include additional semantics over Fortran 77. OpenMP leverages many of the X3H5 concepts while extending them to support coarse grain parallelism. The standard also includes a callable runtime library with accompanying environment variables.}, number={1}, journal={IEEE Computational Science and Engineering}, author={Dagum, L. and Menon, R.}, year={1998}, month=jan, pages={46–55} }

 @inproceedings{Das_Fujimoto_1993, address={New York, NY, USA}, series={PADS ’93}, title={A performance study of the cancelback protocol for Time Warp}, ISBN={978-1-56555-055-1}, url={https://dl.acm.org/doi/10.1145/158459.158476}, DOI={10.1145/158459.158476}, abstractNote={This work presents results from an experimental evaluation of the space-time tradeoffs in Time Warp augmented with the cancelback protocol for memory management. An implementation of the cancelback protocol on Time Warp is described that executes on a shared memory multiprocessor, a 32 processor Kendall Square Research Machine (KSR1). The implementation supports canceling back more than one object when memory has been exhausted. The limited memory performance of the system is evaluated for three different workloads with varying degrees of symmetry. These workloads provide interesting stress cases for evaluating limited memory behavior. We, however, make certain simplifying assumptions (e.g., uniform memory requirement by all the events in the system) to keep the experiments tractable.  The experiments are extensively monitored to determine the extent to which various overheads affect performance. It is observed that (i) depending on the available memory and asymmetry in the workload, canceling back several (called the salvage parameter) events at one time may improve performance significantly, by reducing certain overheads, (ii) a performance nearly equivalent to that with unlimited memory can be achieved with only a modest amount of memory depending on the degree of asymmetry in the workload.}, booktitle={7th Workshop on Parallel and distributed Simulation}, publisher={Association for Computing Machinery}, author={Das, Samir R. and Fujimoto, Richard M.}, year={1993}, month=jul, pages={135–142}, collection={PADS ’93} }

 @inproceedings{David_Guerraoui_Trigonakis_2013, address={New York, NY, USA}, series={SOSP ’13}, title={Everything you always wanted to know about synchronization but were afraid to ask}, ISBN={978-1-4503-2388-8}, url={https://dl.acm.org/doi/10.1145/2517349.2522714}, DOI={10.1145/2517349.2522714}, abstractNote={This paper presents the most exhaustive study of synchronization to date. We span multiple layers, from hardware cache-coherence protocols up to high-level concurrent software. We do so on different types of architectures, from single-socket -- uniform and non-uniform -- to multi-socket -- directory and broadcast-based -- many-cores. We draw a set of observations that, roughly speaking, imply that scalability of synchronization is mainly a property of the hardware.}, booktitle={24th ACM Symposium on Operating Systems Principles}, publisher={Association for Computing Machinery}, author={David, Tudor and Guerraoui, Rachid and Trigonakis, Vasileios}, year={2013}, month=nov, pages={33–48}, collection={SOSP ’13} }

 @inproceedings{deters_numa-aware_2018, title={A {NUMA}-Aware Provably-Efficient Task-Parallel Platform Based on the Work-First Principle}, url={http://arxiv.org/abs/1806.11128}, DOI={10.1109/IISWC.2018.8573486}, abstractNote={Task parallelism is designed to simplify the task of parallel programming. When executing a task parallel program on modern NUMA architectures, it can fail to scale due to the phenomenon called work inflation, where the overall processing time that multiple cores spend on doing useful work is higher compared to the time required to do the same amount of work on one core, due to effects experienced only during parallel executions such as additional cache misses, remote memory accesses, and memory bandwidth issues. It’s possible to mitigate work inflation by co-locating the computation with the data, but this is nontrivial to do with task parallel programs. First, by design, the scheduling for task parallel programs is automated, giving the user little control over where the computation is performed. Second, the platforms tend to employ work stealing, which provides strong theoretical guarantees, but its randomized protocol for load balancing does not discern between work items that are far away versus ones that are closer. In this work, we propose NUMA-WS, a NUMA-aware task parallel platform engineering based on the work-first principle. By abiding by the work-first principle, we are able to obtain a platform that is work efficient, provides the same theoretical guarantees as the classic work stealing scheduler, and mitigates work inflation. Furthermore, we implemented a prototype platform by modifying Intel’s Cilk Plus runtime system and empirically demonstrate that the resulting system is work efficient and scalable.}, note={arXiv:1806.11128 [cs]}, booktitle={2018 IEEE International Symposium on Workload Characterization (IISWC)}, author={Deters, Justin and Wu, Jiaye and Xu, Yifan and Lee, I.-Ting Angelina}, year={2018}, month=sep, pages={59–70} }
 @article{Dijkstra_1965, title={Solution of a problem in concurrent programming control}, volume={8}, ISSN={0001-0782}, DOI={10.1145/365559.365617}, number={9}, journal={Commun. ACM}, author={Dijkstra, E. W.}, year={1965}, month=sep, pages={569} }
 @inproceedings{Dinan_Larkins_Sadayappan_Krishnamoorthy_Nieplocha_2009, address={New York, NY, USA}, series={SC ’09}, title={Scalable work stealing}, ISBN={978-1-60558-744-8}, url={https://doi.org/10.1145/1654059.1654113}, DOI={10.1145/1654059.1654113}, abstractNote={Irregular and dynamic parallel applications pose significant challenges to achieving scalable performance on large-scale multicore clusters. These applications often require ongoing, dynamic load balancing in order to maintain efficiency. Scalable dynamic load balancing on large clusters is a challenging problem which can be addressed with distributed dynamic load balancing systems. Work stealing is a popular approach to distributed dynamic load balancing; however its performance on large-scale clusters is not well understood. Prior work on work stealing has largely focused on shared memory machines. In this work we investigate the design and scalability of work stealing on modern distributed memory systems. We demonstrate high efficiency and low overhead when scaling to 8,192 processors for three benchmark codes: a producer-consumer benchmark, the unbalanced tree search benchmark, and a multiresolution analysis kernel.}, booktitle={Conference on High Performance Computing Networking, Storage and Analysis}, publisher={Association for Computing Machinery}, author={Dinan, James and Larkins, D. Brian and Sadayappan, P. and Krishnamoorthy, Sriram and Nieplocha, Jarek}, year={2009}, month=nov, pages={1–11}, collection={SC ’09} }
 @article{Dongarra_Gates_Haidar_Kurzak_Luszczek_Wu_Yamazaki_Yarkhan_Abalenkovs_Bagherpour_etal._2019, title={PLASMA: Parallel Linear Algebra Software for Multicore Using OpenMP}, volume={45}, ISSN={0098-3500}, DOI={10.1145/3264491}, abstractNote={The recent version of the Parallel Linear Algebra Software for Multicore Architectures (PLASMA) library is based on tasks with dependencies from the OpenMP standard. The main functionality of the library is presented. Extensive benchmarks are targeted on three recent multicore and manycore architectures, namely, an Intel Xeon, Intel Xeon Phi, and IBM POWER 8 processors.}, number={2}, journal={ACM Trans. Math. Softw.}, author={Dongarra, Jack and Gates, Mark and Haidar, Azzam and Kurzak, Jakub and Luszczek, Piotr and Wu, Panruo and Yamazaki, Ichitaro and Yarkhan, Asim and Abalenkovs, Maksims and Bagherpour, Negin and Hammarling, Sven and Šístek, Jakub and Stevens, David and Zounon, Mawussi and Relton, Samuel D.}, year={2019}, month=may, pages={16:1-16:35} }
 @article{Duff_View_Profile_Heroux_View_Profile_Pozo_View_Profile_2002, title={An overview of the sparse basic linear algebra subprograms}, volume={28}, DOI={10.1145/567806.567810}, abstractNote={We discuss the interface design for the Sparse Basic Linear Algebra Subprograms (BLAS), the kernels in the recent standard from the BLAS Technical Forum that are concerned with unstructured sparse matrices. The motivation for such a standard is to encourage portable programming while allowing for library-specific optimizations. In particular, we show how this interface can shield one from concern over the specific storage scheme for the sparse matrix. This design makes it easy to add further functionality to the sparse BLAS in the future.We illustrate the use of the Sparse BLAS with examples in the three supported programming languages, Fortran 95, Fortran 77, and C.}, number={2}, journal={ACM Transactions on Mathematical Software}, publisher={Association for Computing Machinery}, author={Duff, Iain S. and View Profile and Heroux, Michael A. and View Profile and Pozo, Roldan and View Profile}, year={2002}, month=jun, pages={239–267} }
 @inproceedings{duran_barcelona_2009, address={USA}, series={ICPP ’09}, title={Barcelona {OpenMP Tasks Suite}: A Set of Benchmarks Targeting the Exploitation of Task Parallelism in {OpenMP}}, ISBN={978-0-7695-3802-0}, url={https://doi.org/10.1109/ICPP.2009.64}, DOI={10.1109/ICPP.2009.64}, abstractNote={Traditional parallel applications have exploited regular parallelism, based on parallel loops. Only a few applications exploit sections parallelism. With the release of the new OpenMP specification (3.0), this programming model supports tasking. Parallel tasks allow the exploitation of irregular parallelism, but there is a lack of benchmarks exploiting tasks in OpenMP. With the current (and projected) multicore architectures that offer many more alternatives to execute parallel applications than traditional SMP machines, this kind of parallelism is increasingly important. And so, the need to have some set of benchmarks to evaluate it. In this paper, we motivate the need of having such a benchmarks suite, for irregular and/or recursive task parallelism. We present our proposal, the Barcelona OpenMP Tasks Suite (BOTS), with a set of applications exploiting regular and irregular parallelism, based on tasks. We present an overall evaluation of the BOTS benchmarks in an Altix system and we discuss some of the different experiments that can be done with the different compilation and runtime alternatives of the benchmarks.}, booktitle={International Conference on Parallel Processing}, publisher={IEEE Computer Society}, author={Duran, Alejandro and Teruel, Xavier and Ferrer, Roger and Martorell, Xavier and Ayguade, Eduard}, year={2009}, month=sep, pages={124–131}, collection={ICPP ’09} }

 @article{Feldman_Dechev_2015, title={A wait-free multi-producer multi-consumer ring buffer}, volume={15}, ISSN={1559-6915}, DOI={10.1145/2835260.2835264}, abstractNote={The ring buffer is a staple data structure used in many algorithms and applications. It is highly desirable in high-demand use cases such as multimedia, network routing, and trading systems. This work presents a new ring buffer design that is, to the best of our knowledge, the only array-based first-in-first-out ring buffer to provide wait-freedom. Wait-freedom guarantees that each thread completes its operation within a finite number of steps. This property is desirable for real-time and mission critical systems.This work is an extension and refinement of our earlier work. We have improved and expanded algorithm descriptions and pseudo-code, and we have performed all new performance evaluations.In contrast to other concurrent ring buffer designs, our implementation includes new methodology to prevent thread starvation and livelock from occurring.}, number={3}, journal={SIGAPP Appl. Comput. Rev.}, author={Feldman, Steven and Dechev, Damian}, year={2015}, month=oct, pages={59–71} }

 @article{Feldman_Valera-Leon_Dechev_2016, title={An Efficient Wait-Free Vector}, volume={27}, ISSN={1558-2183}, DOI={10.1109/TPDS.2015.2417887}, abstractNote={The vector is a fundamental data structure, which provides constant-time access to a dynamically-resizable range of elements. Currently, there exist no wait-free vectors. The only non-blocking version supports only a subset of the sequential vector API and exhibits significant synchronization overhead caused by supporting opposing operations. Since many applications operate in phases of execution, wherein each phase only a subset of operations are used, this overhead is unnecessary for the majority of the application. To address the limitations of the non-blocking version, we present a new design that is wait-free, supports more of the operations provided by the sequential vector, and provides alternative implementations of key operations. These alternatives allow the developer to balance the performance and functionality of the vector as requirements change throughout execution. Compared to the known non-blocking version and the concurrent vector found in Intel’s TBB library, our design outperforms or provides comparable performance in the majority of tested scenarios. Over all tested scenarios, the presented design performs an average of 4.97 times more operations per second than the non-blocking vector and 1.54 more than the TBB vector. In a scenario designed to simulate the filling of a vector, performance improvement increases to 13.38 and 1.16 times. This work presents the first ABA-free non-blocking vector. Unlike the other non-blocking approach, all operations are wait-free and bounds-checked and elements are stored contiguously in memory.}, number={3}, journal={IEEE Transactions on Parallel and Distributed Systems}, author={Feldman, Steven and Valera-Leon, Carlos and Dechev, Damian}, year={2016}, month=mar, pages={654–667} }
 @inproceedings{Frigo_Leiserson_Randall_1998, address={New York, NY, USA}, series={PLDI ’98}, title={The implementation of the Cilk-5 multithreaded language}, ISBN={978-0-89791-987-6}, url={https://dl.acm.org/doi/10.1145/277650.277725}, DOI={10.1145/277650.277725}, abstractNote={The fifth release of the multithreaded language Cilk uses a provably good “work-stealing” scheduling algorithm similar to the first system, but the language has been completely redesigned and the runtime system completely reengineered. The efficiency of the new implementation was aided by a clear strategy that arose from a theoretical analysis of the scheduling algorithm: concentrate on minimizing overheads that contribute to the work, even at the expense of overheads that contribute to the critical path. Although it may seem counterintuitive to move overheads onto the critical path, this “work-first” principle has led to a portable Cilk-5 implementation in which the typical cost of spawning a parallel thread is only between 2 and 6 times the cost of a C function call on a variety of contemporary machines. Many Cilk programs run on one processor with virtually no degradation compared to equivalent C programs. This paper describes how the work-first principle was exploited in the design of Cilk-5’s compiler and its runtime system. In particular, we present Cilk-5’s novel “two-clone” compilation strategy and its Dijkstra-like mutual-exclusion protocol for implementing the ready deque in the work-stealing scheduler.}, booktitle={ACM SIGPLAN Conference on Programming Language Design and Implementation}, publisher={Association for Computing Machinery}, author={Frigo, Matteo and Leiserson, Charles E. and Randall, Keith H.}, year={1998}, month=may, pages={212–223}, collection={PLDI ’98} }
 @inproceedings{Guo_Barik_Raman_Sarkar_2009, title={Work-first and help-first scheduling policies for async-finish task parallelism}, ISSN={1530-2075}, url={https://ieeexplore.ieee.org/document/5161079}, DOI={10.1109/IPDPS.2009.5161079}, abstractNote={Multiple programming models are emerging to address an increased need for dynamic task parallelism in applications for multicore processors and shared-address-space parallel computing. Examples include OpenMP 3.0, Java Concurrency Utilities, Microsoft Task Parallel Library, Intel Thread Building Blocks, Cilk, X10, Chapel, and Fortress. Scheduling algorithms based on work stealing, as embodied in Cilk’s implementation of dynamic spawn-sync parallelism, are gaining in popularity but also have inherent limitations. In this paper, we address the problem of efficient and scalable implementation of X10’s async-finish task parallelism, which is more general than Cilk’s spawn-sync parallelism. We introduce a new work-stealing scheduler with compiler support for async-finish task parallelism that can accommodate both work-first and help-first scheduling policies. Performance results on two different multicore SMP platforms show significant improvements due to our new work-stealing algorithm compared to the existing work-sharing scheduler for X10, and also provide insights on scenarios in which the help-first policy yields better results than the work-first policy and vice versa.}, booktitle={2009 IEEE International Symposium on Parallel & Distributed Processing}, author={Guo, Yi and Barik, Rajkishore and Raman, Raghavan and Sarkar, Vivek}, year={2009}, month=may, pages={1–12} }
 @inproceedings{Guo_Zhao_Cave_Sarkar_2010, address={New York, NY, USA}, series={PPoPP ’10}, title={SLAW: a scalable locality-aware adaptive work-stealing scheduler for multi-core systems}, ISBN={978-1-60558-877-3}, url={https://doi.org/10.1145/1693453.1693504}, DOI={10.1145/1693453.1693504}, abstractNote={This poster introduces SLAW, a Scalable Locality-aware Adaptive Work-stealing scheduler. The SLAW features an adaptive task scheduling algorithm combined with a locality-aware scheduling framework. Past work has demonstrated the pros and cons of using fixed scheduling policies, such as work-first and help-first, in different cases without a clear winner. Prior work also assumes the availability and successful execution of a serial version of the parallel program. This assumption can limit the expressiveness of dynamic task parallel languages. The SLAW scheduler supports both work-first and help-first policies simultaneously. It does so by using an adaptive approach that selects a scheduling policy on a per-task basis at runtime. The SLAW scheduler also establishes bounds on the stack usage and the heap space needed to store tasks. The experimental results for the benchmarks studied show that SLAW’s adaptive scheduler achieves 0.98x - 9.2$x speedup over the help-first scheduler and 0.97x - 4.5x speedup over the work-first scheduler for 64-thread executions, thereby establishing the robustness of using an adaptive approach instead of a fixed policy. In contrast, the help-first policy is 9.2x slower than work-first in the worst case for a fixed help-first policy, and the work-first policy is 3.7x slower than help-first in the worst case for a fixed work-first policy. Further, for large irregular recursive parallel computations, the adaptive scheduler runs with bounded stack usage and achieves performance (and supports data sizes) that cannot be delivered by the use of any single fixed policy. The SLAW scheduler is designed for programming models where locality hints are provided to the runtime by the programmer or compiler, and achieves locality-awareness by grouping workers into places. Locality awareness can lead to improved performance by increasing temporal data reuse within a worker and among workers in the same place. Our experimental results show that locality-aware scheduling can achieve up to 2.6x speedup over locality-oblivious scheduling, for the benchmarks studied.}, booktitle={15th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming}, publisher={Association for Computing Machinery}, author={Guo, Yi and Zhao, Jisheng and Cave, Vincent and Sarkar, Vivek}, year={2010}, month=jan, pages={341–342}, collectionx={PPoPP ’10} }

 @article{Heldens_Hijma_Werkhoven_Maassen_Belloum_Van_Nieuwpoort_2020, title={The Landscape of Exascale Research: A Data-Driven Literature Analysis}, volume={53}, ISSN={0360-0300}, DOI={10.1145/3372390}, abstractNote={The next generation of supercomputers will break the exascale barrier. Soon we will have systems capable of at least one quintillion (billion billion) floating-point operations per second (1018 FLOPS). Tremendous amounts of work have been invested into identifying and overcoming the challenges of the exascale era. In this work, we present an overview of these efforts and provide insight into the important trends, developments, and exciting research opportunities in exascale computing. We use a three-stage approach in which we (1) discuss various exascale landmark studies, (2) use data-driven techniques to analyze the large collection of related literature, and (3) discuss eight research areas in depth based on influential articles. Overall, we observe that great advancements have been made in tackling the two primary exascale challenges: energy efficiency and fault tolerance. However, as we look forward, we still foresee two major concerns: the lack of suitable programming tools and the growing gap between processor performance and data bandwidth (i.e., memory, storage, networks). Although we will certainly reach exascale soon, without additional research, these issues could potentially limit the applicability of exascale computing.}, number={2}, journal={ACM Comput. Surv.}, author={Heldens, Stijn and Hijma, Pieter and Werkhoven, Ben Van and Maassen, Jason and Belloum, Adam S. Z. and Van Nieuwpoort, Rob V.}, year={2020}, month=mar, pages={23:1-23:43} }

 @inproceedings{Huang_Lin_Guo_Wong_2019, title={Cpp-{T}askflow: Fast Task-Based Parallel Programming Using Modern {C}++}, ISSN={1530-2075}, url={https://ieeexplore.ieee.org/document/8821011}, DOI={10.1109/IPDPS.2019.00105}, abstractNote={In this paper we introduce Cpp-Taskflow, a new C++ tasking library to help developers quickly write parallel programs using task dependency graphs. Cpp-Taskflow leverages the power of modern C++ and task-based approaches to enable efficient implementations of parallel decomposition strategies. Our programming model can quickly handle not only traditional loop-level parallelism, but also irregular patterns such as graph algorithms, incremental flows, and dynamic data structures. Compared with existing libraries, Cpp-Taskflow is more cost efficient in performance scaling and software integration. We have evaluated Cpp-Taskflow on both micro-benchmarks and real-world applications with million-scale tasking. In a machine learning example, Cpp-Taskflow achieved 1.5-2.7× less coding complexity and 14-38% speed-up over two industrial-strength libraries OpenMP Tasking and Intel Threading Building Blocks (TBB).}, booktitle={2019 IEEE International Parallel and Distributed Processing Symposium (IPDPS)}, author={Huang, Tsung-Wei and Lin, Chun-Xun and Guo, Guannan and Wong, Martin}, year={2019}, month=may, pages={974–983} }
 @inproceedings{Huss-Lederman_Jacobson_Tsao_Turnbull_Johnson_1996, address={USA}, series={Supercomputing ’96}, title={Implementation of Strassen’s algorithm for matrix multiplication}, ISBN={978-0-89791-854-1}, url={https://dl.acm.org/doi/10.1145/369028.369096}, DOI={10.1145/369028.369096}, abstractNote={In this paper we report on the development of an efficient and portable implementation of Strassen’s matrix multiplication algorithm for matrices of arbitrary size. Our technique for defining the criterion which stops the recursions is more detailed than those generally used, thus allowing enhanced performance for a larger set of input sizes. In addition, we deal with odd matrix dimensions using a method whose usefulness had previously been in question and had not so far been demonstrated. Our memory requirements have also been reduced, in certain cases by 40 to more than 70 percent over other similar implementations. We measure performance of our code on the IBM RS/6000, CRAY YMP C90, and CRAY T3D single processor, and offer comparisons to other codes. Finally, we demonstrate the usefulness of our implementation by using it to perform the matrix multiplications in a large application code.}, booktitle={ACM/IEEE Conference on Supercomputing}, publisher={IEEE Computer Society}, author={Huss-Lederman, Steven and Jacobson, Elaine M. and Tsao, Anna and Turnbull, Thomas and Johnson, Jeremy R.}, year={1996}, month=nov, pages={32-es}, collection={Supercomputing ’96} }
 @inproceedings{Kaiser_Heller_Adelstein-Lelbach_Serio_Fey_2014, address={New York, NY, USA}, series={PGAS ’14}, title={HPX: A Task Based Programming Model in a Global Address Space}, ISBN={978-1-4503-3247-7}, url={https://doi.org/10.1145/2676870.2676883}, DOI={10.1145/2676870.2676883}, abstractNote={The significant increase in complexity of Exascale platforms due to energy-constrained, billion-way parallelism, with major changes to processor and memory architecture, requires new energy-efficient and resilient programming techniques that are portable across multiple future generations of machines. We believe that guaranteeing adequate scalability, programmability, performance portability, resilience, and energy efficiency requires a fundamentally new approach, combined with a transition path for existing scientific applications, to fully explore the rewards of todays and tomorrows systems. We present HPX -- a parallel runtime system which extends the C++11/14 standard to facilitate distributed operations, enable fine-grained constraint based parallelism, and support runtime adaptive resource management. This provides a widely accepted API enabling programmability, composability and performance portability of user applications. By employing a global address space, we seamlessly augment the standard to apply to a distributed case. We present HPX’s architecture, design decisions, and results selected from a diverse set of application runs showing superior performance, scalability, and efficiency over conventional practice.}, booktitle={8th International Conference on Partitioned Global Address Space Programming Models}, publisher={Association for Computing Machinery}, author={Kaiser, Hartmut and Heller, Thomas and Adelstein-Lelbach, Bryce and Serio, Adrian and Fey, Dietmar}, year={2014}, month=oct, pages={1–11}, collection={PGAS ’14} }
 @article{Kale_Krishnan_1993, title={{CHARM++}: A portable concurrent object oriented system based on {C}++}, volume={28}, ISSN={0362-1340}, DOI={10.1145/167962.165874}, number={10}, journal={SIGPLAN Not.}, author={Kale, Laxmikant V. and Krishnan, Sanjeev}, year={1993}, month=oct, pages={91–108} }
 @article{Kukanov_Voss_2007, title={The Foundations for Scalable Multi-core Software in Intel Threading Building Blocks.}, volume={11}, ISSN={1535-864X}, DOI={10.1535/itj.1104.05}, abstractNote={This paper describes two features of Intel® Threading Building Blocks (Intel® TBB) [1] that provide the foundation for its robust performance: a work-stealing task scheduler and a scalable memory allocator. Work-stealing task schedulers efficiently balance load while maintaining the natural data locality found in many applications. The Intel TBB task scheduler is available to users directly through an API and is also used in the implementation of the algorithms included in the library. In this paper, we provide an overview of the TBB task scheduler and discuss three manual optimizations that users can make to improve its performance: continuation passing, scheduler bypass, and task recycling. In the Experimental Results section of this paper, we provide performance results for several benchmarks that demonstrate the potential scalability of applications threaded with TBB, as well as the positive impact of these manual optimizations on the performance of fine-grain tasks. The task scheduler is complemented by the Intel TBB scalable memory allocator. Memory allocation can often be a limiting bottleneck in parallel applications. Using the TBB scalable memory allocator eliminates this bottleneck and also improves cache behavior. We discuss details of the design and implementation of the TBB scalable allocator and evaluate its performance relative to several commercial and non-commercial allocators, showing that the TBB allocator is competitive with these other allocators.}, number={4}, journal={Intel Technology Journal}, publisher={Intel Corporation}, author={Kukanov, Alexey and Voss, Michael J.}, year={2007}, month=nov, pages={309–322}, language={eng} }

 @article{Kwak_Song_Miller_2005, title={Performance analysis of exponential backoff}, volume={13}, ISSN={1558-2566}, DOI={10.1109/TNET.2005.845533}, abstractNote={New analytical results are given for the performance of the exponential backoff (EB) algorithm. Most available studies on EB focus on the stability of the algorithm and little attention has been paid to the performance analysis of EB. In this paper, we analyze EB and obtain saturation throughput and medium access delay of a packet for a given number of nodes N. The analysis considers the general case of EB with backoff factor r; binary exponential backoff (BEB) algorithm is the special case with r=2. We also derive the analytical performance of EB with maximum retry limit M (EB-M), a practical version of EB. The accuracy of the analysis is checked against simulation results.}, number={2}, journal={IEEE/ACM Transactions on Networking}, author={Kwak, Byung-Jae and Song, Nah-Oak and Miller, L.E.}, year={2005}, month=apr, pages={343–355} }

 @article{Laborde_Feldman_Dechev_2017, title={A Wait-Free Hash Map}, volume={45}, ISSN={1573-7640}, DOI={10.1007/s10766-015-0376-3}, abstractNote={In this work we present the first design and implementation of a wait-free hash map. Our multiprocessor data structure allows a large number of threads to concurrently insert, get, and remove information. Wait-freedom means that all threads make progress in a finite amount of time—an attribute that can be critical in real-time environments. This is opposed to the traditional blocking implementations of shared data structures which suffer from the negative impact of deadlock and related correctness and performance issues. We only use atomic operations that are provided by the hardware; therefore, our hash map can be utilized by a variety of data-intensive applications including those within the domains of embedded systems and supercomputers. The challenges of providing this guarantee make the design and implementation of wait-free objects difficult. As such, there are few wait-free data structures described in the literature; in particular, there are no wait-free hash maps. It often becomes necessary to sacrifice performance in order to achieve wait-freedom. However, our experimental evaluation shows that our hash map design is, on average, 7 times faster than a traditional blocking design. Our solution outperforms the best available alternative non-blocking designs in a large majority of cases, typically by a factor of 15 or higher.}, number={3}, journal={International Journal of Parallel Programming}, author={Laborde, Pierre and Feldman, Steven and Dechev, Damian}, year={2017}, month=jun, pages={421–448}, language={en} }

 @article{leiserson_theres_2020, title={There’s plenty of room at the Top: What will drive computer performance after {M}oore’s law?}, volume={368}, DOI={10.1126/science.aam9744}, abstractNote={The doubling of the number of transistors on a chip every 2 years, a seemly inevitable trend that has been called Moore’s law, has contributed immensely to improvements in computer performance. However, silicon-based transistors cannot get much smaller than they are today, and other approaches should be explored to keep performance growing. Leiserson et al. review recent examples and argue that the most promising place to look is at the top of the computing stack, where improvements in software, algorithms, and hardware architecture can bring the much-needed boost. Science, this issue p. eaam9744 The miniaturization of semiconductor transistors has driven the growth in computer performance for more than 50 years. As miniaturization approaches its limits, bringing an end to Moore’s law, performance gains will need to come from software, algorithms, and hardware. We refer to these technologies as the “Top” of the computing stack to distinguish them from the traditional technologies at the “Bottom”: semiconductor physics and silicon-fabrication technology. In the post-Moore era, the Top will provide substantial performance gains, but these gains will be opportunistic, uneven, and sporadic, and they will suffer from the law of diminishing returns. Big system components offer a promising context for tackling the challenges of working at the Top.}, number={6495}, journal={Science}, author={Leiserson, Charles E. and Thompson, Neil C. and Emer, Joel S. and Kuszmaul, Bradley C. and Lampson, Butler W. and Sanchez, Daniel and Schardl, Tao B.}, year={2020}, pages={eaam9744} }
 @inproceedings{Lifflander_Krishnamoorthy_Kale_2014, title={Optimizing Data Locality for Fork/Join Programs Using Constrained Work Stealing}, ISSN={2167-4337}, url={https://ieeexplore.ieee.org/document/7013057}, DOI={10.1109/SC.2014.75}, abstractNote={We present an approach to improving data locality across different phases of fork/join programs scheduled using work stealing. The approach consists of: (1) user-specified and automated approaches to constructing a steal tree, the schedule of steal operations, and (2) constrained work-stealing algorithms that constrain the actions of the scheduler to mirror a given steal tree. These are combined to construct work-stealing schedules that maximize data locality across computation phases while ensuring load balance within each phase. These algorithms are also used to demonstrate dynamic coarsening, an optimization to improve spatial locality and sequential overheads by combining many finer-grained tasks into coarser tasks while ensuring sufficient concurrency for locality-optimized load balance. Implementation and evaluation in Cilk demonstrate performance improvements of up to 2.5x on 80 cores. We also demonstrate that dynamic coarsening can combine the performance benefits of coarse task specification with the adaptability of finer tasks.}, booktitle={International Conference for High Performance Computing, Networking, Storage and Analysis}, author={Lifflander, Jonathan and Krishnamoorthy, Sriram and Kale, Laxmikant V.}, year={2014}, month=nov, pages={857–868} }

 @book{Lucas_Ang_Bergman_Borkar_Carlson_Carrington_Chiu_Colwell_Dally_Dongarra_etal._2014, title={DOE Advanced Scientific Computing Advisory Subcommittee (ASCAC) Report: Top Ten Exascale Research Challenges}, url={https://www.osti.gov/biblio/1222713}, DOI={10.2172/1222713}, abstractNote={Exascale computing systems are essential for the scientific fields that will transform the 21st century global economy, including energy, biotechnology, nanotechnology, and materials science. Progress in these fields is predicated on the ability to perform advanced scientific and engineering simulations, and analyze the deluge of data. On July 29, 2013, ASCAC was charged by Patricia Dehmer, the Acting Director of the Office of Science, to assemble a subcommittee to provide advice on exascale computing. This subcommittee was directed to return a list of no more than ten technical approaches (hardware and software) that will enable the development of a system that achieves the Department’s goals for exascale computing. Numerous reports over the past few years have documented the technical challenges and the non¬-viability of simply scaling existing computer designs to reach exascale. The technical challenges revolve around energy consumption, memory performance, resilience, extreme concurrency, and big data. Drawing from these reports and more recent experience, this ASCAC subcommittee has identified the top ten computing technology advancements that are critical to making a capable, economically viable, exascale system.}, institution={USDOE Office of Science (SC) (United States)}, author={Lucas, Robert and Ang, James and Bergman, Keren and Borkar, Shekhar and Carlson, William and Carrington, Laura and Chiu, George and Colwell, Robert and Dally, William and Dongarra, Jack and Geist, Al and Haring, Rud and Hittinger, Jeffrey and Hoisie, Adolfy and Klein, Dean Micron and Kogge, Peter and Lethin, Richard and Sarkar, Vivek and Schreiber, Robert and Shalf, John and Sterling, Thomas and Stevens, Rick and Bashor, Jon and Brightwell, Ron and Coteus, Paul and Debenedictus, Erik and Hiller, Jon and Kim, K. H. and Langston, Harper and Murphy, Richard Micron and Webster, Clayton and Wild, Stefan and Grider, Gary and Ross, Rob and Leyffer, Sven and Laros III, James}, year={2014}, month=feb, language={English} }

 @misc{Matsuoka_Domke_Wahib_Drozd_Hoefler_2023, title={Myths and Legends in High-Performance Computing}, url={https://arxiv.org/abs/2301.02432v3}, abstractNote={In this thought-provoking article, we discuss certain myths and legends that are folklore among members of the high-performance computing community. We gathered these myths from conversations at conferences and meetings, product advertisements, papers, and other communications such as tweets, blogs, and news articles within and beyond our community. We believe they represent the zeitgeist of the current era of massive change, driven by the end of many scaling laws such as Dennard scaling and Moore’s law. While some laws end, new directions are emerging, such as algorithmic scaling or novel architecture research. Nevertheless, these myths are rarely based on scientific facts, but rather on some evidence or argumentation. In fact, we believe that this is the very reason for the existence of many myths and why they cannot be answered clearly. While it feels like there should be clear answers for each, some may remain endless philosophical debates, such as whether Beethoven was better than Mozart. We would like to see our collection of myths as a discussion of possible new directions for research and industry investment.}, journal={arXiv.org}, author={Matsuoka, Satoshi and Domke, Jens and Wahib, Mohamed and Drozd, Aleksandr and Hoefler, Torsten}, year={2023}, month=jan, language={en} }

 @inproceedings{Mei_Zheng_Gioachin_Kalé_2010, address={New York, NY, USA}, series={TG ’10}, title={Optimizing a parallel runtime system for multicore clusters: A case study}, ISBN={978-1-60558-818-6}, url={https://doi.org/10.1145/1838574.1838586}, DOI={10.1145/1838574.1838586}, abstractNote={Clusters of multicore nodes have become the most popular option for new HPC systems due to their scalability and performance/cost ratio. The complexity of programming multicore systems underscores the need for powerful and efficient runtime systems that manage resources such as threads and communication sub-systems on behalf of the applications.In this paper, we study several multicore performance issues on clusters using Intel, AMD and IBM processors in the context of the Charm++ runtime system. We then present the optimization techniques that overcome these performance issues. The techniques presented are general enough to apply to other runtime systems as well. We demonstrate the benefits of these optimizations through both synthetic benchmarks and production quality applications including NAMD and ChaNGa on several popular multicore platforms. We demonstrate performance improvement of NAMD and ChaNGa by about 20% and 10%, respectively.}, booktitle={TeraGrid Conference}, publisher={Association for Computing Machinery}, author={Mei, Chao and Zheng, Gengbin and Gioachin, Filippo and Kalé, Laxmikant V.}, year={2010}, month=aug, pages={1–8}, collection={TG ’10} }

 @article{Mellor-Crummey_Scott_1991, title={Algorithms for scalable synchronization on shared-memory multiprocessors}, volume={9}, ISSN={0734-2071}, DOI={10.1145/103727.103729}, abstractNote={Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible flag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory.We present a new scalable algorithm for spin locks that generates 0(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than a swap-with-memory instruction. We also present a new scalable barrier algorithm that generates 0(1) remote references per processor reaching the barrier, and observe that two previously-known barriers can likewise be cast in a form that spins only on locally-accessible flag variables. None of these barrier algorithms requires hardware support beyond the usual atomicity of memory reads and writes.We compare the performance of our scalable algorithms with other software approaches to busy-wait synchronization on both a Sequent Symmetry and a BBN Butterfly. Our principal conclusion is that contention due to synchronization need not be a problem in large-scale shared-memory multiprocessors. The existence of scalable algorithms greatly weakens the case for costly special-purpose hardware support for synchronization, and provides a case against so-called “dance hall” architectures, in which shared memory locations are equally far from all processors. —From the Authors’ Abstract}, number={1}, journal={ACM Trans. Comput. Syst.}, author={Mellor-Crummey, John M. and Scott, Michael L.}, year={1991}, month=feb, pages={21–65} }
 @inproceedings{Meng_Zeng_Chen_Ye_2017, title={A cache-friendly concurrent lock-free queue for efficient inter-core communication}, ISSN={2472-8489}, url={https://ieeexplore.ieee.org/abstract/document/8230170}, DOI={10.1109/ICCSN.2017.8230170}, abstractNote={Buffer sharing based on pipeline parallelism is quite susceptible to inter-core communication overhead. Existing work on concurrent lock-free (CLF) queue algorithm did not take full advantage of CPU cache features to improve performance. In order to implement a fast single-producer-single-consumer (SPSC) buffer scheduling queue, this paper proposes a cache-friendly CLF queue scheduling algorithm (CFCLF), which concentrates on cache-level optimization and minimizing inter-core communication overheads in pipeline parallelism. CFCLF innovatively employs a matrix (2D array), instead of one-dimensional array to design the shared queue structure, making CFCLF has a good cache behavior so as to avoid cache false sharing, and cache consistency problem. Besides, the algorithm implements batch processing efficiently to improve throughput. A deadlock prevention method is also proposed. Experimental results show that on Intel Xeon and Cavium OCTEON, CFCLF outperforms B-Queue which is the state-of-the-art concurrent lock-free queue, by up to 25.5%, and CFCLF is more stable than other algorithms.}, booktitle={2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN)}, author={Meng, Xianghui and Zeng, Xuewen and Chen, Xiao and Ye, Xiaozhou}, year={2017}, month=may, pages={538–542} }
 @inproceedings{Michael_Scott_1996, address={New York, NY, USA}, series={PODC ’96}, title={Simple, fast, and practical non-blocking and blocking concurrent queue algorithms}, ISBN={978-0-89791-800-8}, url={https://dl.acm.org/doi/10.1145/248052.248106}, DOI={10.1145/248052.248106}, booktitle={15th Annual ACM Symposium on Principles of Distributed Computing}, publisher={Association for Computing Machinery}, author={Michael, Maged M. and Scott, Michael L.}, year={1996}, month=may, pages={267–275}, collection={PODC ’96} }

@inproceedings{10.1145/3295500.3356161,
author = {Shiina, Shumpei and Taura, Kenjiro},
title = {Almost deterministic work stealing},
year = {2019},
isbn = {9781450362290},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/3295500.3356161},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis},
location = {Denver, Colorado},
series = {SC '19}
}

@ARTICLE{963420,
  author={Mitzenmacher, M.},
  journal={IEEE Transactions on Parallel and Distributed Systems}, 
  title={The power of two choices in randomized load balancing}, 
  year={2001},
  doi={10.1109/71.963420}}

@InProceedings{10.1007/978-3-642-15277-1_21,
author="Quintin, Jean-No{\"e}l
and Wagner, Fr{\'e}d{\'e}ric",
editor="D'Ambra, Pasqua
and Guarracino, Mario
and Talia, Domenico",
title="Hierarchical Work-Stealing",
booktitle="Euro-Par 2010 - Parallel Processing",
year="2010",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="217--229",
isbn="978-3-642-15277-1"
}

@inproceedings{guo2009work,
  title={Work-first and help-first scheduling policies for terminally strict parallel programs},
  author={Guo, Yi and Barik, Rajkishore and Raman, Raghavan and Sarkar, Vivek},
  booktitle={23rd IEEE International Parallel and Distributed Processing Symposium},
  volume={10},
  year={2009}
}

 @article{Milman-Sela_Kogan_Lev_Luchangco_Petrank_2022, title={BQ: A Lock-Free Queue with Batching}, volume={9}, ISSN={2329-4949}, DOI={10.1145/3512757}, abstractNote={Concurrent data structures provide fundamental building blocks for concurrent programming. Standard concurrent data structures may be extended by allowing a sequence of operations to be submitted as a batch for later execution. A sequence of such operations can then be executed more efficiently than the standard execution of one operation at a time. In this article, we develop a novel algorithmic extension to the prevalent FIFO queue data structure that exploits such batching scenarios. An implementation in C++ on a multicore demonstrates significant performance improvement of more than an order of magnitude (depending on the batch lengths and the number of threads) compared to previous queue implementations.}, number={1}, journal={ACM Trans. Parallel Comput.}, author={Milman-Sela, Gal and Kogan, Alex and Lev, Yossi and Luchangco, Victor and Petrank, Erez}, year={2022}, month=mar, pages={5:1-5:49} }

@inproceedings{10.1145/1654059.1654113,
author = {Dinan, James and Larkins, D. Brian and Sadayappan, P. and Krishnamoorthy, Sriram and Nieplocha, Jarek},
title = {Scalable work stealing},
year = {2009},
isbn = {9781605587448},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
doi = {10.1145/1654059.1654113},
booktitle = {Conference on High Performance Computing Networking, Storage and Analysis},
location = {Portland, Oregon},
series = {SC '09}
}

 @article{Min_Iancu_Yelick, title={Hierarchical Work Stealing on Manycore Clusters}, abstractNote={Partitioned Global Address Space languages like UPC offer a convenient way of expressing large shared data structures, especially for irregular structures that require asynchronous random access. But the static SPMD parallelism model of UPC does not support divide and conquer parallelism or other forms of dynamic parallelism. We introduce a dynamic tasking library for UPC that provides a simple and effective way of adding task parallelism to SPMD programs. The task library, called HotSLAW, provides a high-level API that abstracts concurrent task management details and performs dynamic load balancing. To achieve scalability, we propose a topology-aware hierarchical work stealing strategy that exploits locality in distributed-memory clusters. Our approach, named HotSLAW, extends state of the art techniques in shared- and distributed-memory implementations with two mechanisms: Hierarchical Victim Selection (HVS) ﬁnds the nearest victim thread to preserve locality and Hierarchical Chunk Selection (HCS) dynamically determines the amount of work to steal based on the locality of the victim thread. We evaluate the performance of our runtime on shared- and distributed-memory systems using irregular applications. On shared memory, HotSLAW provides performance comparable or better than hand tuned OpenMP implementations. On distributed memory systems, the combination of Hierarchical Victim Selection and Hierarchical Chunk Selection provides better performance than state of the art approaches using a random victim selection with a StealHalf strategy.}, author={Min, Seung-Jai and Iancu, Costin and Yelick, Katherine}, language={en} }

 @inproceedings{Mitropoulou_Porpodas_Zhang_Jones_2016, address={Istanbul Turkey}, title={Lynx: Using OS and Hardware Support for Fast Fine-Grained Inter-Core Communication}, ISBN={978-1-4503-4361-9}, url={https://dl.acm.org/doi/10.1145/2925426.2926274}, DOI={10.1145/2925426.2926274}, abstractNote={Designing high-performance software queues for fast intercore communication is challenging, but critical for maximising software parallelism. State-of-the-art single-producer / single-consumer queues for streaming applications contain multiple sections, requiring the producer and consumer to operate independently on diﬀerent sections from each other. While these queues perform well for coarse-grained data transfers, they perform poorly in the ﬁne-grained case.}, booktitle={International Conference on Supercomputing}, publisher={ACM}, author={Mitropoulou, Konstantina and Porpodas, Vasileios and Zhang, Xiaochun and Jones, Timothy M.}, year={2016}, month=jun, pages={1–12}, language={en} }
@inproceedings{10.1145/1838574.1838586,
author = {Mei, Chao and Zheng, Gengbin and Gioachin, Filippo and Kal\'{e}, Laxmikant V.},
title = {Optimizing a parallel runtime system for multicore clusters: A case study},
year = {2010},
isbn = {9781605588186},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1838574.1838586},
doi = {10.1145/1838574.1838586},
booktitle = {TeraGrid Conference},
articleno = {12},
numpages = {8},
location = {Pittsburgh, Pennsylvania},
series = {TG '10}
}

@inproceedings{wozniak2013swift_short,
  title={{Swift/T}: Scalable data flow programming for many-task applications},
  author={Wozniak, Justin M and Armstrong, Timothy G and Wilde, Michael and Katz, Daniel S and Lusk, Ewing and Foster, Ian T},
  booktitle={18th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={309--310},
  year={2013}
}

@inproceedings{wozniak2013swift,
  title={Swift/{T}: Large-scale application composition via distributed-memory dataflow processing},
  author={Wozniak, Justin M and Armstrong, Timothy G and Wilde, Michael and Katz, Daniel S and Lusk, Ewing and Foster, Ian T},
  booktitle={13th IEEE/ACM International Symposium on Cluster, Cloud, and Grid Computing},
  pages={95--102},
  year={2013},
  organization={IEEE}
}

@article{NOOKALA2024444,
title = {{X-OpenMP — eXtreme} fine-grained tasking using lock-less work stealing},
journal = {Future Generation Computer Systems},
volume = {159},
pages = {444-458},
year = {2024},
issn = {0167-739X},
doi = {https://doi.org/10.1016/j.future.2024.05.019},
url = {https://www.sciencedirect.com/science/article/pii/S0167739X24002541},
author = {Poornima Nookala and Kyle Chard and Ioan Raicu},
keywords = {Runtime, Parallel, Tasking, Task, Openmp, Lock-less, Lockfree, Locks, Atomics, Parallel computing, Workstealing, Open cilk, Onetbb}
}
@inproceedings{gonthier2021locality,
  title={Locality-Aware Scheduling of Independent Tasks for Runtime Systems},
  author={Gonthier, Maxime and Marchal, Loris and Thibault, Samuel},
  booktitle={European Conference on Parallel Processing},
  pages={5--16},
  year={2021},
  organization={Springer}
}
@inproceedings{gonthier:hal-03290998,
  TITLE = {{Locality-Aware Scheduling of Independent Tasks for Runtime Systems}},
  AUTHOR = {Gonthier, Maxime and Marchal, Loris and Thibault, Samuel},
  URL = {https://hal.science/hal-03290998},
  BOOKTITLE = {{5th Workshop on Data Locality - 27th International European Conference on Parallel and Distributed Computing}},
  ADDRESS = {Lisbon, Portugal},
  PUBLISHER = {{Springer}},
  PAGES = {1-12},
  YEAR = {2021},
  MONTH = Aug,
  DOI = {10.1007/978-3-031-06156-1\_1},
  KEYWORDS = {Memory-aware scheduling ; Eviction policy ; Tasks sharing data ; Runtime systems ; Ordonnancement sous contrainte m{\'e}moire ; Politique d'{\'e}viction ; T{\^a}ches partageant des donn{\'e}es ; Support d'ex{\'e}cution},
  PDF = {https://hal.science/hal-03290998v1/file/coloc-cameraready-submitted.pdf},
  HAL_ID = {hal-03290998},
  HAL_VERSION = {v1},
}

 @article{Mitzenmacher_2001, title={The power of two choices in randomized load balancing}, volume={12}, ISSN={1558-2183}, DOI={10.1109/71.963420}, abstractNote={We consider the following natural model: customers arrive as a Poisson stream of rate /spl lambda/n, /spl lambda/<1, at a collection of n servers. Each customer chooses some constant d servers independently and uniformly at random from the n servers and waits for service at the one with the fewest customers. Customers are served according to the first-in first-out (FIFO) protocol and the service time for a customer is exponentially distributed with mean 1. We call this problem the supermarket model. We wish to know how the system behaves and in particular we are interested in the effect that the parameter d has on the expected time a customer spends in the system in equilibrium. Our approach uses a limiting, deterministic model representing the behavior as n/spl rarr//spl infin/ to approximate the behavior of finite systems. The analysis of the deterministic model is interesting in its own right. Along with a theoretical justification of this approach, we provide simulations that demonstrate that the method accurately predicts system behavior, even for relatively small systems. Our analysis provides surprising implications. Having d=2 choices leads to exponential improvements in the expected time a customer spends in the system over d=1, whereas having d=3 choices is only a constant factor better than d=2. We discuss the possible implications for system design.}, number={10}, journal={IEEE Transactions on Parallel and Distributed Systems}, author={Mitzenmacher, M.}, year={2001}, month=oct, pages={1094–1104} }

@article{olivier2013characterizing,
  title={Characterizing and mitigating work time inflation in task parallel programs},
  author={Olivier, Stephen L and De Supinski, Bronis R and Schulz, Martin and Prins, Jan F},
  journal={Scientific Programming},
  volume={21},
  number={3-4},
  pages={123--136},
  year={2013},
  publisher={IOS Press}
}

 @inproceedings{nookala_enabling_2021, address={Houston, TX, USA}, title={Enabling Extremely Fine-grained Parallelism via Scalable Concurrent Queues on Modern Many-core Architectures}, rights={https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html}, ISBN={978-1-66545-838-2}, url={https://ieeexplore.ieee.org/document/9614292/}, DOI={10.1109/MASCOTS53633.2021.9614292}, abstractNote={Enabling efﬁcient ﬁne-grained task parallelism is a signiﬁcant challenge for hardware platforms with increasingly many cores. Existing techniques do not scale to hundreds of threads due to the high cost of synchronization in concurrent data structures. To overcome these limitations we present XQueue, a novel lock-less concurrent queuing system with relaxed ordering semantics that is geared towards realizing scalability up to hundreds of concurrent threads. We demonstrate the scalability of XQueue using microbenchmarks and show that XQueue can deliver concurrent operations with latencies as low as 110 cycles at scales of up to 192 cores (up to 6900× improvement compared to traditional synchronization mechanisms) across our diverse hardware, including x86, ARM, and Power9. The reduced latency allows XQueue to provide orders of magnitude (3300×) better throughput that existing techniques. To evaluate the real-world beneﬁts of XQueue, we integrated XQueue with LLVM OpenMP and evaluated ﬁve unmodiﬁed benchmarks from the Barcelona OpenMP Task Suite (BOTS) as well as a graph traversal benchmark from the GAP benchmark suite. We compared the XQueue-enabled LLVM OpenMP implementation with the native LLVM and GNU OpenMP versions. Using ﬁne-grained task workloads, XQueue can deliver 4× to 6× speedup compared to native GNU OpenMP and LLVM OpenMP in many cases, with speedups as high as 116× in some cases.}, booktitle={29th International Symposium on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems}, publisher={IEEE}, author={Nookala, Poornima and Dinda, Peter and Hale, Kyle C. and Chard, Kyle and Raicu, Ioan}, year={2021}, month=nov, pages={1–8}, language={en} }
 @inproceedings{Olivier_Huan_Liu_Prins_Dinan_Sadayappan_Tseng_2006, address={Berlin, Heidelberg}, series={LCPC’06}, title={UTS: an unbalanced tree search benchmark}, ISBN={978-3-540-72520-6}, abstractNote={This paper presents an unbalanced tree search (UTS) benchmark designed to evaluate the performance and ease of programming for parallel applications requiring dynamic load balancing. We describe algorithms for building a variety of unbalanced search trees to simulate different forms of load imbalance. We created versions of UTS in two parallel languages, OpenMP and Unified Parallel C (UPC), using work stealing as the mechanism for reducing load imbalance. We benchmarked the performance of UTS on various parallel architectures, including shared-memory systems and PC clusters. We found it simple to implement UTS in both UPC and OpenMP, due to UPC’s shared-memory abstractions. Results show that both UPC and OpenMP can support efficient dynamic load balancing on shared-memory architectures. However, UPC cannot alleviate the underlying communication costs of distributed-memory systems. Since dynamic load balancing requires intensive communication, performance portability remains difficult for applications such as UTS and performance degrades on PC clusters. By varying key work stealing parameters, we expose important tradeoffs between the granularity of load balance, the degree of parallelism, and communication costs.}, booktitle={19th International Conference on Languages and Compilers for Parallel Computing}, publisher={Springer-Verlag}, author={Olivier, Stephen and Huan, Jun and Liu, Jinze and Prins, Jan and Dinan, James and Sadayappan, P. and Tseng, Chau-Wen}, year={2006}, month=nov, pages={235–250}, collection={LCPC’06} }
 @article{Orhean_Ballmer_Koehring_Hale_Sun_Trigalo_Hardavellas_Kapoor_Raicu, title={MYSTIC: Programmable Systems Research Testbed  to Explore a Stack-WIde Adaptive System fabriC}, abstractNote={The pervasiveness and the continuous advancement of computer systems have determined a sudden upsurge of digital data. The Big Data phenomena has resulted in a widespread adoption of parallel and distributed systems for eﬃciently storing and accessing data. Likewise computing applications are changing from computecentric to data-centric, where communication of data comprises a large part of the program and prominently dictates performance. Static conﬁgurations of network, memory, and storage under the form of computer clusters show signs of fatigue in the face of increasing scale of modern big data applications. This NSF-funded infrastructure project proposes to build a dynamically conﬁgurable cluster called MYSTIC to study system re-conﬁgurability across the entire computing stack, from the processor to memory, storage, and the network. It will allow low-level experimentation and reconﬁguration at the level of networks-on-chip (NoC), universal memory, and the network interconnect with multidimensional network topologies. Dynamically reconﬁguring interconnects, memory and storage will speed-up applications running on a heterogeneous computing environment. The goal of this research is to remove network, memory, and storage performance bottlenecks by dynamically reconﬁguring them to the needs of an application. In this testbed the organization of memory, storage, and their connectivity will change dynamically to adapt to the workload to improve overall performance of the executing programs. An overview of both the hardware and software stack will be presented, as well as how the community can get access to this exciting new research testbed.}, author={Orhean, Alexandru Iulian and Ballmer, Alexander and Koehring, Travis and Hale, Kyle and Sun, Xian-He and Trigalo, Ophir and Hardavellas, Nikos and Kapoor, Sanjiv and Raicu, Ioan}, language={en} }
 @phdthesis{Pilla_Ribeiro_Cordeiro_Bhatele_Navaux_Mehaut_Kalé_2011, type={report}, title={Improving Parallel System Performance with a NUMA-aware Load Balancer}, url={https://inria.hal.science/hal-00788813}, abstractNote={Multi-core nodes with Non-Uniform Memory Access (NUMA) are now a common architecture for high performance computing. On such NUMA nodes, the shared memory is physically distributed into memory banks connected by a network. Owing to this, memory access costs may vary depending on the distance between the processing unit and the memory bank. Therefore, a key element in improving the performance on these machines is dealing with memory affinity. We propose a NUMA-aware load balancer that combines the information about the NUMA topology with the statistics captured by the Charm++ runtime system. We present speedups of up to 1.8 for synthetic benchmarks running on different NUMA platforms. We also show improvements over existing load balancing strategies both in benchmark performance and in the time for load balancing. In addition, by avoiding unnecessary migrations, our algorithm incurs up to seven times smaller overheads in migration, than the other strategies.}, school={Inria}, author={Pilla, Laércio L. and Ribeiro, Christiane Pousa and Cordeiro, Daniel and Bhatele, Abhinav and Navaux, Philippe O. A. and Mehaut, Jean-François and Kalé, Laxmikant V.}, year={2011}, language={en} }
 @inproceedings{Podobas_Brorsson_Vlassov_2014, title={TurboBŁYSK: Scheduling for Improved Data-Driven Task Performance with Fast Dependency Resolution}, ISBN={978-3-319-11453-8}, DOI={10.1007/978-3-319-11454-5_4}, abstractNote={Data-driven task-parallelism is attracting growing interest and has now been added to OpenMP (4.0). This paradigm simplifies the writing of parallel applications, extracting parallelism, and facilitates the use of distributed memory architectures. While the programming model itself is becoming mature, a problem with current run-time scheduler implementations is that they require a very large task granularity in order to scale. This limitation goes at odds with the idea of task-parallel programing where programmers should be able to concentrate on exposing parallelism with little regard to the task granularity. To mitigate this limitation, we have designed and implemented TurboBŁYSK, a highly efficient run-time scheduler of tasks with explicit data-dependence annotations. We propose a novel mechanism based on pattern-saving that allows the scheduler to re-use previously resolved dependency patterns, based on programmer annotations, enabling programs to use even the smallest of tasks and scale well. We experimentally show that our techniques in TurboBŁYSK enable achieving nearly twice the peak performance compared with other run-time schedulers. Our techniques are not OpenMP specific and can be implemented in other task-parallel frameworks.}, author={Podobas, Artur and Brorsson, Mats and Vlassov, Vladimir}, year={2014}, month=sep, pages={45–57} }
 @inproceedings{Quintin_Wagner_2010, address={Berlin, Heidelberg}, series={EuroPar’10}, title={Hierarchical work-stealing}, ISBN={978-3-642-15276-4}, abstractNote={dynamic load-balancing on hierarchical platforms. In particular, we consider applications involving heavy communications on a distributed platform. The work-stealing algorithm introduced by Blumofe and Leiserson is a commonly used technique to balance load in a distributed environment but it suffers from poor performance with some communication-intensive applications. We describe here several variants of this algorithm found in the literature and in different grid middlewares like Satin and Kaapi. In addition, we propose two new variations of the work-stealing algorithm : HWS and PWS. These algorithms improve performance by considering the network structure. We conduct a theoretical analysis of HWS in the case of fork-join task graphs and prove that HWS reduces communication overhead. In addition, we present experimental results comparing the most relevant algorithms. Experiments on Grid’5000 show that HWS and PWS allow us to obtain performance gains of up to twenty per cent when compared to the classical workstealing algorithm. Moreover in some cases, PWS and HWS achieve speedup while classical work-stealing policies result in speed-down.}, booktitle={16th International Euro-Par Conference on Parallel processing: Part I}, publisher={Springer-Verlag}, author={Quintin, Jean-Noël and Wagner, Frédéric}, year={2010}, month=aug, pages={217–229}, collection={EuroPar’10} }
 @inproceedings{Rab_Marotta_Ianni_Pellegrini_Quaglia_2020, title={NUMA-Aware Non-Blocking Calendar Queue}, ISSN={1550-6525}, url={https://ieeexplore.ieee.org/document/9213639}, DOI={10.1109/DS-RT50469.2020.9213639}, abstractNote={Modern computing platforms are based on multi-processor/multi-core technology. This allows running applications with a high degree of hardware parallelism. However, medium-to-high end machines pose a problem related to the asymmetric delays threads experience when accessing shared data. Specifically, Non-Uniform-Memory-Access (NUMA) is the dominating technology-thanks to its capability for scaled-up memory bandwidth-which however imposes asymmetric distances between CPU-cores and memory banks, making an access by a thread to data placed on a far NUMA node severely impacting performance. In this article, we tackle this problem in the context of shared event-pool management, a relevant aspect in many fields, like parallel discrete event simulation. Specifically, we present a NUMA-aware calendar queue, which also has the advantage of making concurrent threads coordinate via a non-blocking scalable approach. Our proposal is based on work deferring combined with dynamic re-binding of the calendar queue operations (insertions/extractions) to the best suited among the concurrent threads hosted by the underlying computing platform. This changes the locality of the operations by threads in a way positively reflected onto NUMA tasks at the hardware level. We report the results of an experimental study, demonstrating the capability of our solution to achieve the order of 15% better performance compared to state-of-the-art solutions already suited for multicore environments.}, booktitle={IEEE/ACM 24th International Symposium on Distributed Simulation and Real Time Applications (DS-RT)}, author={Rab, Maryan and Marotta, Romolo and Ianni, Mauro and Pellegrini, Alessandro and Quaglia, Francesco}, year={2020}, month=sep, pages={1–9} }
 @inproceedings{Schweizer_Besta_Hoefler_2015, address={USA}, series={PACT ’15}, title={Evaluating the Cost of Atomic Operations on Modern Architectures}, ISBN={978-1-4673-9524-3}, url={https://doi.org/10.1109/PACT.2015.24}, DOI={10.1109/PACT.2015.24}, abstractNote={Atomic operations (atomics) such as Compare-and-Swap (CAS) or Fetch-and-Add (FAA) are ubiquitous in parallel programming. Yet, performance tradeoffs between these opera-tions and various characteristics of such systems, such as the structure of caches, are unclear and have not been thoroughly analyzed. In this paper we establish an evaluation methodology, develop a performance model, and present a set of detailed benchmarks for latency and bandwidth of different atomics. Weconsider various state-of-the-art x86 architectures: Intel Haswell, Xeon Phi, Ivy Bridge, and AMD Bulldozer. The results unveil surprising performance relationships between the considered atomics and architectural properties such as the coherence state of the accessed cache lines. One key finding is that all the tested atomics have comparable latency and bandwidth even if they are characterized by different consensus numbers. Another insight is that the design of atomics prevents any instruction-level parallelism even if there are no dependencies between the issued operations. Finally, we discuss solutions to the discovered performance issues in the analyzed architectures. Our analysis can be used for making better design and algorithmic decisions in parallel programming on various architectures deployed in both off-the-shelf machines and large compute systems.}, booktitle={International Conference on Parallel Architecture and Compilation}, publisher={IEEE Computer Society}, author={Schweizer, Hermann and Besta, Maciej and Hoefler, Torsten}, year={2015}, month=oct, pages={445–456}, collection={PACT ’15} }
 @inproceedings{Scogland_Feng_2015, address={New York, NY, USA}, series={ICPE ’15}, title={Design and Evaluation of Scalable Concurrent Queues for Many-Core Architectures}, ISBN={978-1-4503-3248-4}, url={https://dl.acm.org/doi/10.1145/2668930.2688048}, DOI={10.1145/2668930.2688048}, abstractNote={As core counts increase and as heterogeneity becomes more common in parallel computing, we face the prospect of programming hundreds or even thousands of concurrent threads in a single shared-memory system. At these scales, even highly-efficient concurrent algorithms and data structures can become bottlenecks, unless they are designed from the ground up with throughput as their primary goal.In this paper, we present three contributions: (1) a characterization of queue designs in terms of modern multi- and many-core architectures, (2) the design of a high-throughput, linearizable, blocking, concurrent FIFO queue for many-core architectures that avoids the bottlenecks and pitfalls common in modern queue designs, and (3) a thorough evaluation of concurrent queue throughput across CPU, GPU, and co-processor devices. Our evaluation shows that focusing on throughput, rather than progress guarantees, allows our queue to scale to as much as three orders of magnitude (1000x) faster than lock-free and combining queues on GPU platforms and two times (2x) faster on CPU devices. These results deliver critical insights into the design of data structures for highly concurrent systems: (1) progress guarantees do not guarantee scalability, and (2) allowing an algorithm to block can increase throughput.}, booktitle={6th ACM/SPEC International Conference on Performance Engineering}, publisher={Association for Computing Machinery}, author={Scogland, Thomas R.W. and Feng, Wu-chun}, year={2015}, month=jan, pages={63–74}, collection={ICPE ’15} }

@article{sewell_x86-tso_2010,
	title = {x86-{TSO}: A rigorous and usable programmer's model for x86 multiprocessors},
	volume = {53},
	issn = {0001-0782},
	shorttitle = {x86-{TSO}},
	url = {https://dl.acm.org/doi/10.1145/1785414.1785443},
	doi = {10.1145/1785414.1785443},
	abstract = {Exploiting the multiprocessors that have recently become ubiquitous requires high-performance and reliable concurrent systems code, for concurrent data structures, operating system kernels, synchronization libraries, compilers, and so on. However, concurrent programming, which is always challenging, is made much more so by two problems. First, real multiprocessors typically do not provide the sequentially consistent memory that is assumed by most work on semantics and verification. Instead, they have relaxed memory models, varying in subtle ways between processor families, in which different hardware threads may have only loosely consistent views of a shared memory. Second, the public vendor architectures, supposedly specifying what programmers can rely on, are often in ambiguous informal prose (a particularly poor medium for loose specifications), leading to widespread confusion.In this paper we focus on x86 processors. We review several recent Intel and AMD specifications, showing that all contain serious ambiguities, some are arguably too weak to program above, and some are simply unsound with respect to actual hardware. We present a new x86-TSO programmer's model that, to the best of our knowledge, suffers from none of these problems. It is mathematically precise (rigorously defined in HOL4) but can be presented as an intuitive abstract machine which should be widely accessible to working programmers. We illustrate how this can be used to reason about the correctness of a Linux spinlock implementation and describe a general theory of data-race freedom for x86-TSO. This should put x86 multiprocessor system building on a more solid foundation; it should also provide a basis for future work on verification of such systems.},
	number = {7},
	urldate = {2024-09-06},
	journal = {Commun. ACM},
	author = {Sewell, Peter and Sarkar, Susmit and Owens, Scott and Nardelli, Francesco Zappa and Myreen, Magnus O.},
	month = jul,
	year = {2010},
	pages = {89--97},
}


 @article{Sewell_Sarkar_Owens_Nardelli_Myreen_2010, title={x86-TSO: {A} rigorous and usable programmer’s model for x86 multiprocessors}, volume={53}, ISSN={0001-0782}, DOI={10.1145/1785414.1785443}, abstractNote={Exploiting the multiprocessors that have recently become ubiquitous requires high-performance and reliable concurrent systems code, for concurrent data structures, operating system kernels, synchronization libraries, compilers, and so on. However, concurrent programming, which is always challenging, is made much more so by two problems. First, real multiprocessors typically do not provide the sequentially consistent memory that is assumed by most work on semantics and verification. Instead, they have relaxed memory models, varying in subtle ways between processor families, in which different hardware threads may have only loosely consistent views of a shared memory. Second, the public vendor architectures, supposedly specifying what programmers can rely on, are often in ambiguous informal prose (a particularly poor medium for loose specifications), leading to widespread confusion.In this paper we focus on x86 processors. We review several recent Intel and AMD specifications, showing that all contain serious ambiguities, some are arguably too weak to program above, and some are simply unsound with respect to actual hardware. We present a new x86-TSO programmer’s model that, to the best of our knowledge, suffers from none of these problems. It is mathematically precise (rigorously defined in HOL4) but can be presented as an intuitive abstract machine which should be widely accessible to working programmers. We illustrate how this can be used to reason about the correctness of a Linux spinlock implementation and describe a general theory of data-race freedom for x86-TSO. This should put x86 multiprocessor system building on a more solid foundation; it should also provide a basis for future work on verification of such systems.}, number={7}, journal={Communications of the ACM}, author={Sewell, Peter and Sarkar, Susmit and Owens, Scott and Nardelli, Francesco Zappa and Myreen, Magnus O.}, year={2010}, month=jul, pages={89–97} }
 @inproceedings{Shiina_Taura_2019, address={New York, NY, USA}, series={SC ’19}, title={Almost deterministic work stealing}, ISBN={978-1-4503-6229-0}, url={https://dl.acm.org/doi/10.1145/3295500.3356161}, DOI={10.1145/3295500.3356161}, abstractNote={With task parallel models, programmers can easily parallelize divide-and-conquer algorithms by using nested fork-join structures. Work stealing, which is a popular scheduling strategy for task parallel programs, can efficiently perform dynamic load balancing; however, it tends to damage data locality and does not scale well with memory-bound applications. This paper introduces Almost Deterministic Work Stealing (ADWS), which addresses the issue of data locality of traditional work stealing by making the scheduling almost deterministic. Specifically, ADWS consists of two parts: (i) deterministic task allocation, which deterministically distributes tasks to workers based on the amount of work for each task, and (ii) hierarchical localized work stealing, which dynamically compensates load imbalance in a locality-aware manner. Experimental results show that ADWS is up to nearly 6 times faster than a traditional work stealing scheduler with memory-bound applications, and that dynamic load balancing works well while maintaining good data locality.}, booktitle={International Conference for High Performance Computing, Networking, Storage and Analysis}, publisher={Association for Computing Machinery}, author={Shiina, Shumpei and Taura, Kenjiro}, year={2019}, month=nov, pages={1–16}, collection={SC ’19} }
 @article{Suksompong_Leiserson_Schardl_2016, title={On the Efficiency of Localized Work Stealing}, volume={116}, ISSN={00200190}, DOI={10.1016/j.ipl.2015.10.002}, abstractNote={This paper investigates a variant of the work-stealing algorithm that we call the localized work-stealing algorithm. The intuition behind this variant is that because of locality, processors can benefit from working on their own work. Consequently, when a processor is free, it makes a steal attempt to get back its own work. We call this type of steal a steal-back. We show that the expected running time of the algorithm is $T_1/P+O(T_infty P)$, and that under the “even distribution of free agents assumption”, the expected running time of the algorithm is $T_1/P+O(T_inftylg P)$. In addition, we obtain another running-time bound based on ratios between the sizes of serial tasks in the computation. If $M$ denotes the maximum ratio between the largest and the smallest serial tasks of a processor after removing a total of $O(P)$ serial tasks across all processors from consideration, then the expected running time of the algorithm is $T_1/P+O(T_infty M)$.}, note={arXiv:1804.04773 [cs]}, number={2}, journal={Information Processing Letters}, author={Suksompong, Warut and Leiserson, Charles E. and Schardl, Tao B.}, year={2016}, month=feb, pages={100–106} }

 @article{Thibault, title={On Runtime Systems for Task-based Programming on Heterogeneous Platforms}, author={Thibault, Samuel}, language={en} }
 @inproceedings{Treichler_Bauer_Aiken_2013, address={New York, NY, USA}, series={OOPSLA ’13}, title={Language support for dynamic, hierarchical data partitioning}, ISBN={978-1-4503-2374-1}, url={https://doi.org/10.1145/2509136.2509545}, DOI={10.1145/2509136.2509545}, abstractNote={Applications written for distributed-memory parallel architectures must partition their data to enable parallel execution. As memory hierarchies become deeper, it is increasingly necessary that the data partitioning also be hierarchical to match. Current language proposals perform this hierarchical partitioning statically, which excludes many important applications where the appropriate partitioning is itself data dependent and so must be computed dynamically. We describe Legion, a region-based programming system, where each region may be partitioned into subregions. Partitions are computed dynamically and are fully programmable. The division of data need not be disjoint and subregions of a region may overlap, or alias one another. Computations use regions with certain privileges (e.g., expressing that a computation uses a region read-only) and data coherence (e.g., expressing that the computation need only be atomic with respect to other operations on the region), which can be controlled on a per-region (or subregion) basis.We present the novel aspects of the Legion design, in particular the combination of static and dynamic checks used to enforce soundness. We give an extended example illustrating how Legion can express computations with dynamically determined relationships between computations and data partitions. We prove the soundness of Legion’s type system, and show Legion type checking improves performance by up to 71% by eliding provably safe memory checks. In particular, we show that the dynamic checks to detect aliasing at runtime at the region granularity have negligible overhead. We report results for three real-world applications running on distributed memory machines, achieving up to 62.5X speedup on 96 GPUs on the Keeneland supercomputer.}, booktitle={ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages and Applications}, publisher={Association for Computing Machinery}, author={Treichler, Sean and Bauer, Michael and Aiken, Alex}, year={2013}, month=oct, pages={495–514}, collection={OOPSLA ’13} }

 @article{Wang_Zhang_Tang_Hua_2013, title={B-Queue: Efficient and Practical Queuing for Fast Core-to-Core Communication}, volume={41}, ISSN={1573-7640}, DOI={10.1007/s10766-012-0213-x}, abstractNote={Core-to-core communication is critical to the effective use of multi-core processors. A number of software based concurrent lock-free queues have been proposed to address this problem. Existing solutions, however, suffer from performance degradation in real testbeds, or rely on auxiliary hardware or software timers to handle the deadlock problem when batching is used, making those solutions good in theory but difficult to use in practice. This paper describes the pros and cons of existing concurrent lock-free queues in both dummy and real testbeds and proposes B-Queue, an efficient and practical single-producer-single-consumer concurrent lock-free queue that solves the deadlock problem gracefully by introducing a self-adaptive backtracking mechanism. Experiments show that in real massively-parallel applications, B-Queue is faster than FastForward and MCRingBuffer, the two state-of-the-art concurrent lock-free queues, by up to 10x and 5x, respectively. Moreover, B-Queue outperforms FastForward and MCRingBuffer in terms of stability and scalability, making it a good candidate for fast core-to-core communication on multi-core architectures.}, number={1}, journal={International Journal of Parallel Programming}, author={Wang, Junchang and Zhang, Kai and Tang, Xinan and Hua, Bei}, year={2013}, month=feb, pages={137–159}, language={en} }

 @article{Wang_Kulkarni_Lang_Arnold_Raicu_2016, title={Exploring the Design Tradeoffs for Extreme-Scale High-Performance Computing System Software}, volume={27}, ISSN={1045-9219}, DOI={10.1109/TPDS.2015.2430852}, abstractNote={Owing to the extreme parallelism and the high component failure rates of tomorrow&amp;#x0027;s exascale, high-performance computing (HPC) system software will need to be scalable, failure-resistant, and adaptive for sustained system operation and full system utilizations. Many of the existing HPC system software are still designed around a centralized server paradigm and hence are susceptible to scaling issues and single points of failure. In this article, we explore the design tradeoffs for scalable system software at extreme scales. We propose a general system software taxonomy by deconstructing common HPC system software into their basic components. The taxonomy helps us reason about system software as follows: (1) it gives us a systematic way to architect scalable system software by decomposing them into their basic components; (2) it allows us to categorize system software based on the features of these components, and finally (3) it suggests the configuration space to consider for design evaluation via simulations or real implementations. Further, we evaluate different design choices of a representative system software, i.e. key-value store, through simulations up to millions of nodes. Finally, we show evaluation results of two distributed system software, Slurm&amp;#x002B;&amp;#x002B; (a distributed HPC resource manager) and MATRIX (a distributed task execution framework), both developed based on insights from this work. We envision that the results in this article help to lay the foundations of developing next-generation HPC system software for extreme scales.}, number={4}, journal={IEEE Trans. Parallel Distrib. Syst.}, author={Wang, Ke and Kulkarni, Abhishek and Lang, Michael and Arnold, Dorian and Raicu, Ioan}, year={2016}, month=apr, pages={1070–1084} }

 @inproceedings{Wang_Gao_Fang_Huang_Wang_2021, address={Haikou, Hainan, China}, title={Characterizing {OpenMP} Synchronization Implementations on {ARMv8} Multi-Cores}, ISBN={978-1-66549-457-1}, url={https://ieeexplore.ieee.org/document/9780950/}, DOI={10.1109/HPCC-DSS-SmartCity-DependSys53884.2021.00111}, abstractNote={Synchronization operations like barriers are frequently seen in parallel OpenMP programs, where an inefﬁcient implementation can severely limit the application performance. While synchronization optimization has been heavily studied on traditional x86 architectures, there is no consensus on how synchronization can be best implemented on the ARMv8 multicore CPUs. This paper presents a study of OpenMP synchronization implementation on two representative ARMv8 multi-core architectures, Phytium 2000+ and ThunderX2, by considering various OpenMP synchronization mechanisms offered by two mainstreamed OpenMP compilers, GCC and LLVM. Our evaluation compares the performance, overhead and scalability of both compiler implementations. We show that there is no “one-ﬁts-forall” synchronization mechanism, and the efﬁciency of a scheme varies across hardware architectures and thread parallelism. We then share our insights and discuss how OpenMP synchronization operations can be better optimized on emerging ARMv8 multicores, offering quantiﬁed results for future research directions.}, booktitle={IEEE 23rd Int Conf on High Performance Computing and Communications; 7th Int Conf on Data Science & Systems; 19th Int Conf on Smart City; 7th Int Conf on Dependability in Sensor, Cloud & Big Data Systems & Application (HPCC/DSS/SmartCity/DependSys)}, publisher={IEEE}, author={Wang, Pengyu and Gao, Wanrong and Fang, Jianbin and Huang, Chun and Wang, Zheng}, year={2021}, month=dec, pages={669–676}, language={en} }

 @article{Wilde_Raicu_Espinosa_Zhang_Clifford_Hategan_Kenny_Iskra_Beckman_Foster_2009, title={Extreme-scale scripting: Opportunities for large task-parallel applications on petascale computers}, volume={180}, ISSN={1742-6596}, DOI={10.1088/1742-6596/180/1/012046}, abstractNote={Parallel scripting is a loosely-coupled programming model in which applications are composed of highly parallel scripts of program invocations that process and exchange data via files. We characterize here the applications that can benefit from parallel scripting on petascale-class machines, describe the mechanisms that make this feasible on such systems, and present results achieved with parallel scripts on currently available petascale computers.}, number={1}, journal={Journal of Physics: Conference Series}, author={Wilde, Michael and Raicu, Ioan and Espinosa, Allan and Zhang, Zhao and Clifford, Ben and Hategan, Mihael and Kenny, Sarah and Iskra, Kamil and Beckman, Pete and Foster, Ian}, year={2009}, month=jul, pages={012046}, language={en} }

 @article{williams_libfork_2024, title={Libfork: Portable continuation-stealing with stackless coroutines}, url={http://arxiv.org/abs/2402.18480}, abstractNote={Fully-strict fork-join parallelism is a powerful model for shared-memory programming due to its optimal time scaling and strong bounds on memory scaling. The latter is rarely achieved due to the difficulty of implementing continuation stealing in traditional High Performance Computing (HPC) languages -- where it is often impossible without modifying the compiler or resorting to non-portable techniques. We demonstrate how stackless coroutines (a new feature in C++20) can enable fully-portable continuation stealing and present libfork a lock-free fine-grained parallelism library, combining coroutines with user-space, geometric segmented-stacks. We show our approach is able to achieve optimal time/memory scaling, both theoretically and empirically, across a variety of benchmarks. Compared to openMP (libomp), libfork is on average 7.2x faster and consumes 10x less memory. Similarly, compared to Intel’s TBB, libfork is on average 2.7x faster and consumes 6.2x less memory. Additionally, we introduce non-uniform memory access (NUMA) optimizations for schedulers that demonstrate performance matching busy-waiting schedulers.}, journal={arXiv:2402.18480}, number={arXiv:2402.18480}, publisher={arXiv}, author={Williams, Conor John and Elliott, James}, year={2024}, month=feb, language={en} }

 @article{Wozniak_Armstrong_Wilde_Katz_Lusk_Foster_2013, title={Swift/T: scalable data flow programming for many-task applications}, volume={48}, ISSN={0362-1340}, DOI={10.1145/2517327.2442559}, abstractNote={Swift/T, a novel programming language implementation for highly scalable data flow programs, is presented.}, number={8}, journal={SIGPLAN Not.}, author={Wozniak, Justin M. and Armstrong, Timothy G. and Wilde, Michael and Katz, Daniel S. and Lusk, Ewing and Foster, Ian T.}, year={2013}, month=feb, pages={309–310} }
 @inproceedings{Yoo_Hughes_Kim_Chen_Kozyrakis_2013, address={New York, NY, USA}, series={SPAA ’13}, title={Locality-aware task management for unstructured parallelism: A quantitative limit study}, ISBN={978-1-4503-1572-2}, url={https://doi.org/10.1145/2486159.2486175}, DOI={10.1145/2486159.2486175}, abstractNote={As we increase the number of cores on a processor die, the on-chip cache hierarchies that support these cores are getting larger, deeper, and more complex. As a result, non-uniform memory access effects are now prevalent even on a single chip. To reduce execution time and energy consumption, data access locality should be exploited. This is especially important for task-based programming systems, where a scheduler decides when and where on the chip the code segments, i.e., tasks, should execute. Capturing locality for structured task parallelism has been done effectively, but the more difficult case, unstructured parallelism, remains largely unsolved - little quantitative analysis exists to demonstrate the potential of locality-aware scheduling, and to guide future scheduler implementations in the most fruitful direction.This paper quantifies the potential of locality-aware scheduling for unstructured parallelism on three different many-core processors. Our simulation results of 32-core systems show that locality-aware scheduling can bring up to 2.39x speedup over a randomized schedule, and 2.05x speedup over a state-of-the-art baseline scheduling scheme. At the same time, a locality-aware schedule reduces average energy consumption by 55% and 47%, relative to the random and the baseline schedule, respectively. In addition, our 1024-core simulation results project that these benefits will only increase: Compared to 32-core executions, we see up to 1.83x additional locality benefits. To capture such potentials in a practical setting, we also perform a detailed scheduler design space exploration to quantify the impact of different scheduling decisions. We also highlight the importance of locality-aware stealing, and demonstrate that a stealing scheme can exploit significant locality while performing load balancing. Over randomized stealing, our proposed scheme shows up to 2.0x speedup for stolen tasks.}, booktitle={25th Annual ACM symposium on Parallelism in Algorithms and Architectures}, publisher={Association for Computing Machinery}, author={Yoo, Richard M. and Hughes, Christopher J. and Kim, Changkyu and Chen, Yen-Kuang and Kozyrakis, Christos}, year={2013}, month=jul, pages={315–325}, collection={SPAA ’13} }
 @inproceedings{kale1993charm,
  title={Charm++: A portable concurrent object oriented system based on {C}++},
  author={Kale, Laxmikant V and Krishnan, Sanjeev},
  booktitle={8th Annual Conference on Object-oriented Programming Systems, Languages, and Applications},
  pages={91--108},
  year={1993}
}
@article{Zhao_Chen_Qiu_Wu_Shen_Leng_Li_Guo_2018, title={Bandwidth and Locality Aware Task-stealing for Manycore Architectures with Bandwidth-Asymmetric Memory}, volume={15}, ISSN={1544-3566}, DOI={10.1145/3291058}, abstractNote={Parallel computers now start to adopt Bandwidth-Asymmetric Memory architecture that consists of traditional DRAM memory and new High Bandwidth Memory (HBM) for high memory bandwidth. However, existing task schedulers suffer from low bandwidth usage and poor data locality problems in bandwidth-asymmetric memory architectures. To solve the two problems, we propose a Bandwidth and Locality Aware Task-stealing (BATS) system, which consists of an HBM-aware data allocator, a bandwidth-aware traffic balancer, and a hierarchical task-stealing scheduler. Leveraging compile-time code transformation and run-time data distribution, the data allocator enables HBM usage automatically without user interference. According to data access hotness, the traffic balancer migrates data to balance memory traffic across memory nodes proportional to their bandwidth. The hierarchical scheduler improves data locality at runtime without a priori program knowledge. Experiments on an Intel Knights Landing server that adopts bandwidth-asymmetric memory show that BATS reduces the execution time of memory-bound programs up to 83.5% compared with traditional task-stealing schedulers.}, number={4}, journal={ACM Transactions on Architecture and Code Optimization}, author={Zhao, Han and Chen, Quan and Qiu, Yuxian and Wu, Ming and Shen, Yao and Leng, Jingwen and Li, Chao and Guo, Minyi}, year={2018}, month=dec, pages={55:1-55:26} }

 @misc{miscname1, title = {{GNU Offloading and Multi-Processing Project (GOMP) - GNU Project}}, url={https://gcc.gnu.org/projects/gomp/} }

 @misc{upc, title = {{Berkeley UPC - Unified Parallel C}}, url={https://upc.lbl.gov/} }

 @misc{miscname2, title = {liblfds.org}, url={https://www.liblfds.org/} }
 @article{miscname3, title = {The Trouble with Locks}, journal={Dr Dobb’s}, url={https://www.drdobbs.com/cpp/the-trouble-with-locks/184401930} }
 @misc{miscname4, title = {Welcome to the documentation of {OpenMP in LLVM! — LLVM/OpenMP} 20.0.0git documentation}, url={https://openmp.llvm.org/} }


@inproceedings{sinclair_heterosync_2017,
	title = {{HeteroSync}: {A} benchmark suite for fine-grained synchronization on tightly coupled {GPUs}},
	shorttitle = {{HeteroSync}},
	url = {https://ieeexplore.ieee.org/document/8167781/?arnumber=8167781},
	doi = {10.1109/IISWC.2017.8167781},
	abstract = {Traditionally GPUs focused on streaming, data-parallel applications, with little data reuse or sharing and coarse-grained synchronization. However, the rise of general-purpose GPU (GPGPU) computing has made GPUs desirable for applications with more general sharing patterns and fine-grained synchronization, especially for recent GPUs that have a unified address space and coherent caches. Prior work has introduced microbenchmarks to measure the impact of these changes, but each paper uses its own set of microbenchmarks. In this work, we combine several of these sets together in a single suite, HeteroSync. HeteroSync includes several synchronization primitives, data sharing at different levels of the memory hierarchy, and relaxed atomics. We characterize the scalability of HeteroSync for different coherence protocols and consistency models on modern, tightly coupled CPU-GPU systems and show that certain algorithms, coherence protocols, and consistency models scale better than others.},
	urldate = {2024-10-02},
	booktitle = {{IEEE} {International} {Symposium} on {Workload} {Characterization}},
	author = {Sinclair, Matthew D. and Alsop, Johnathan and Adve, Sarita V.},
	month = oct,
	year = {2017},
	keywords = {Coherence, Graphics processing units, Hardware, Multicore processing, Protocols, Scalability, Synchronization},
	pages = {239--249},
	file = {Full Text PDF:/Users/haochen/Zotero/storage/UIJRRJB3/Sinclair et al. - 2017 - HeteroSync A benchmark suite for fine-grained syn.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/haochen/Zotero/storage/GB6JMLK4/8167781.html:text/html},
}

@inproceedings{navarro-torres_synchronization_2021,
	title = {Synchronization Strategies on Many-Core {SMT} Systems},
	url = {https://ieeexplore.ieee.org/document/9651585/?arnumber=9651585},
	doi = {10.1109/SBAC-PAD53543.2021.00017},
	abstract = {The complexity of efficient synchronization design increases with the continuous growth in the number of physical and logical cores on today's machines. Opinion is divided on which synchronization strategy is more powerful, opposing typical mechanisms, such as locks and atomic primitives, to emergent technologies, like transactional memory. We perform an extensive scalability study on many-core systems, evaluating most widely-used synchronization mechanisms in terms of application throughput and operation latency. We show that, from a performance perspective, current best-effort implementations of hardware transactional memory (HTM) are comparable to well-established locking or lock-free mechanisms. We also find that they scale better with the number of threads. We then showcase the ease-of-use of HTM in real-life applications. Finally, we analyze the impact of simultaneous multithreading (SMT) technologies on HTM performance. We propose a new cache replacement strategy that takes into account the transactional state of each cache line and aims to mitigate SMT-induced transactional overflow aborts.},
	urldate = {2024-10-02},
	booktitle = {{IEEE} 33rd {International} {Symposium} on {Computer} {Architecture} and {High} {Performance} {Computing} ({SBAC}-{PAD})},
	author = {Navarro-Torres, Agustín and Alastruey-Benedé, Jesús and Ibáñez-Marín, Pablo and Carpen-Amarie, Maria},
	month = oct,
	year = {2021},
	note = {ISSN: 2643-3001},
	keywords = {Computer architecture, Hardware, High performance computing, Instruction sets, Intel Transactional Synchronization Extensions, Many-core systems, Multithreading, Scalability, Throughput, Transactional memory},
	pages = {54--63},
	file = {Full Text PDF:/Users/haochen/Zotero/storage/QNMSS69Z/Navarro-Torres et al. - 2021 - Synchronization Strategies on Many-Core SMT System.pdf:application/pdf;IEEE Xplore Abstract Record:/Users/haochen/Zotero/storage/XTG4N43P/9651585.html:text/html},
}


@article{morrison_scaling_2016,
	title = {Scaling Synchronization in Multicore Programs: Advanced synchronization methods can boost the performance of multicore software.},
	volume = {14},
	issn = {1542-7730, 1542-7749},
	shorttitle = {Scaling {Synchronization} in {Multicore} {Programs}},
	url = {https://dl.acm.org/doi/10.1145/2984629.2991130},
	doi = {10.1145/2984629.2991130},
	abstract = {Designing software for modern multicore processors poses a dilemma. Traditional software designs, in which threads manipulate shared data, have limited scalability because synchronization of updates to shared data serializes threads and limits parallelism. Alternative distributed software designs, in which threads do not share mutable data, eliminate synchronization and offer better scalability. But distributed designs make it challenging to implement features that shared data structures naturally provide, such as dynamic load balancing and strong consistency guarantees, and are simply not a good fit for every program. Often, however, the performance of shared mutable data structures is limited by the synchronization methods in use today, whether lock-based or lock-free. To help readers make informed design decisions, this article describes advanced (and practical) synchronization methods that can push the performance of designs using shared mutable data to levels that are acceptable to many applications.},
	language = {en},
	number = {4},
	urldate = {2024-10-02},
	journal = {Queue},
	author = {Morrison, Adam},
	month = aug,
	year = {2016},
	pages = {56--79},
	file = {Full Text:/Users/haochen/Zotero/storage/RS8XMLRC/Morrison - 2016 - Scaling Synchronization in Multicore Programs Adv.pdf:application/pdf},
}

@misc{rdtscp,
title={{RDTSCP — Read Time-Stamp Counter and Processor ID}}, url={https://www.felixcloutier.com/x86/rdtscp} }

@misc{pthread, 
  title={{ISO/IEC/IEEE} 9945}, 
  note={\url{https://www.iso.org/standard/50516.html}}, 
  abstractNote={Information technology — Portable Operating System Interface (POSIX®) Base Specifications, Issue 7}, journal={ISO}, language={en} }

@misc{memlatency, 
  title={Designs, Lessons and Advice from Building Large Distributed Systems}, author={Dean, Jeff}, language={en},
  note = {\url{https://www.cs.cornell.edu/projects/ladis2009/talks/dean-keynote-ladis2009.pdf}},
  year = 2009
}


 @misc{vtune,
title={Fix Performance Bottlenecks with {Intel® VTune™} Profiler},
note={\url{https://www.intel.com/content/www/us/en/developer/tools/oneapi/vtune-profiler.html#gs.i6xhgk}} }

@misc{Nakamoto2008,
  author       = {Satoshi Nakamoto},
  title        = {Bitcoin: A Peer-to-Peer Electronic Cash System},
  howpublished = {\url{https://bitcoin.org/bitcoin.pdf}},
  year         = {2008},
  note         = {Accessed: 2025-01-21}
}

@inproceedings{Dziembowski2015,
  author    = {Stefan Dziembowski and Sebastian Faust and Vladimir Kolmogorov and Krzysztof Pietrzak},
  title     = {Proofs of Space},
  booktitle = {Advances in Cryptology - CRYPTO 2015},
  pages     = {585--605},
  year      = {2015},
  publisher = {Springer},
  doi       = {10.1007/978-3-662-48000-7_29}
}

@article{Cohen2019,
  title={The chia network blockchain},
  author={Cohen, Bram and Pietrzak, Krzysztof},
  journal={White Paper, Chia. net},
  volume={9},
  year={2019}
}

@misc{Oconnor2019BLAKE3,
  author       = {Jack O'Connor and others},
  title        = {BLAKE3: One function to rule them all},
  howpublished = {\url{https://github.com/BLAKE3-team/BLAKE3-specs}},
  year         = {2019},
  note         = {Accessed: 2025-01-21}
}

@misc{OpenMP5.0,
  author       = {OpenMP Architecture Review Board},
  title        = {{OpenMP} Application Programming Interface Version 5.0},
  howpublished = {\url{https://www.openmp.org/specifications/}},
  month        = {November},
  year         = {2018},
  note         = {Accessed: 2025-01-21}
}

@misc{ChiaKSizes,
  title        = {K-sizes in Chia Documentation},
  howpublished = {\url{https://docs.chia.net/k-sizes/}},
  note         = {Accessed: 2025-01-21}
}

@misc{ChiaGreenPaper2024,
  author       = {Chia Network},
  title        = {Chia Green Paper},
  howpublished = {\url{https://docs.chia.net/files/ChiaGreenPaper_20241008.pdf}},
  month        = {October},
  year         = {2024},
  note         = {Accessed: 2025-01-21}
}

@inproceedings{orhean2019mystic,
  title={Mystic: Programmable systems research testbed to explore a stack-wide adaptive system fabric},
  author={Orhean, AI and Ballmer, A and Koehring, T and Hale, K and Sun, XH and Trigalo, O and Hardavellas, N and Kapoor, S and Raicu, I},
  booktitle={8th Greater Chicago Area Systems Research Workshop (GCASR)},
  year={2019}
}
