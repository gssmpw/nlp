\section{Implementation}\label{sec:implementation}

\subsection{Generative Model}

\noindent\textbf{Dataset and model training.}
We curate two non-overlapping datasets for model training and testing.
The training dataset comprises 78,420 images sourced from public underwater
datasets~\cite{anantharajah2014local, marques2020l2uwe, islam2020suim,
islam2019fast, jmse7010016, 2018arXiv180704856G, li2019underwater, 10102831} and
135,000 general images sampled from ImageNet~\cite{imagenet100}.
For testing, we extract 1,216 underwater images from five scuba diving
videos~\cite{dataset_video_1, dataset_video_2, dataset_video_3, dataset_video_4,
dataset_video_5}, captured in various diving locations and conditions.

We initialize our model using a pre-trained TiTok model~\cite{yu2024image} and perform context-aware token distillation for 112 epochs on our training dataset,
followed by 60 epochs of error-resilient fine-tuning.
These two stages require 11 and 14 hours of training time on 4$\times$A100 GPUs,
respectively.

\parab{Model deployment.} Building upon the open-source code from~\cite{chen2022underwater}, we implement and deploy \sysname on the Android platform.
Our system follows the encoder-decoder architecture and training framework of TiTok,
where the codebook size is $K'=4096$, and a 256$\times$256 image is compressed into $M'=64$ tokens, equal to $64\times log_2{4096} = 768$ bits.
The neural network models, initially in PyTorch format, are converted into Android-compatible formats and optimized for mobile devices using PyTorch~\cite{pytorch_mobile} libraries.
With a total size of under 1 GB, these models can be loaded and executed
on modern smartphones, such as the Samsung S21~\cite{Samsung} used in our study.




\subsection{Transmitter}
The transmitter performs modulation and channel coding, converting serialized image bits from the image encoder into time-series acoustic signals. These signals are then broadcast into the water via the mobile device's speaker.
To ensure a high SNR for acoustic signals,
we select the 1.5--3.5 kHz frequency range for image transmission,
based on the measurement study presented in Fig.~\ref{fig:snr}.




\parab{Modulation.}
The CSS modulation technique encodes each symbol as a chirp signal that spans the entire bandwidth~\cite{TSN23,jung2021exploiting}. The chirp generation process is governed by two key parameters: the spreading factor ($SF$) and the bandwidth ($BW$).
To balance robustness and data rate, we set $SF$ $=$ 5,
achieving a data rate of 312 bps with $BW$ $=$ 2 kHz in underwater environments.
Consequently, the symbol time is $T_{s} = 2^{SF} / BW$ $=$ 16 ms.
After modulation, the mobile device transmits the acoustic signal at a sampling rate of
$f_{s} = 48$ kHz, generating $N_{s} = 768$ samples per symbol.




\parab{Channel coding.}
To enhance robustness and facilitate error recovery,
we employ encoding techniques commonly used in chirp-based communication systems~\cite {TSN23,LoRaWAN,chen2024hitting}:

\begin{itemize}[leftmargin=*]
    \item \textit{Hamming coding.} We first apply Hamming encoding~\cite{hamming1986coding} with a 4/7 code rate, encoding every 4 data bits with 3 parity check bits to form a 7-bit codeword.
    \item \textit{Diagonal interleaving.} We group every $SF$ $=$ 5 codewords into 7 symbols,
    distributing each bit of a codeword diagonally across 7 different
    symbols~\cite{TSN23}.
    This approach aligns with the fact that a (7, 4) Hamming code can correct up to 1-bit error per codeword.
    \item \textit{Gray coding.} %
    We use Gray coding~\cite{doran2007gray} to assign binary values to symbols so that adjacent
    symbols differ by only one bit. This minimizes the impact of demodulation errors
    and improves the effectiveness of Hamming coding in detecting and correcting bit errors.
\end{itemize}


\subsection{Receiver}
The receiver performs several key functions, including packet detection,
time synchronization, equalization, demodulation, and decoding,
to extract data bytes from the received raw signal.
These data bytes are subsequently forwarded to the image decoder for image
reconstruction.

\parab{Equalization.}
We apply MMSE-based time-domain equalization~\cite{MMSE} to each symbol group.
The equalization coefficients are estimated using each training symbol and then
applied to equalize the subsequent data symbols within the same group.
To balance performance and computational complexity,
we configure the equalizer with a tap size of 240 samples and an offset of 80 samples.
Furthermore, the time-domain equalization process helps mitigate symbol time
offsets during convolution operations.

\parab{Demodulation.}
After extracting each data symbol from the received packet, we apply a standard
de-chirp process to demodulate the symbol, following the approach in~\cite{TSN23}.


\parab{Channel decoding.}
The channel decoder applies the reverse process of the channel encoder,
incorporating Gray coding, diagonal de-interleaving, and Hamming decoding.
Once the data bytes are successfully decoded, they are passed to the image decoder
for final image reconstruction.
