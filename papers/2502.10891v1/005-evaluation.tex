\section{Evaluation}
\label{sec:evaluation}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/experiment_environment_remove_pool_new.pdf}
    \vspace{-30pt}
    \caption{Experimental environments from left to right: fishing dock,
    bridge, lake bank, pedal boat, and lake beach. The remaining images show
    a prototype of \sysname and the mounting setup used to secure phones
    to the measurement tool when close to and when far from the water.}
\label{fig:experiment_environment}

\end{figure*}

\subsection{Experimental Setup}
To thoroughly evaluate our system, we conducted a series of tests in various environments using customized setups.

\parab{Experimental environments.}
We tested our system across different environments to assess the impact of distance, depth, orientation, mobility, and any other environmental factors on its performance. The environments considered are listed below and the corresponding images are shown in Fig.~\ref{fig:experiment_environment}.

\begin{itemize}[leftmargin=*]
    \item \em{Fishing dock:} A quiet environment with limited space for setup, suitable for short-range experiments.
    \item \em{Bridge:} A realistic and quiet environment with ample space
    (a measurement distance of up to 20 m and an average water depth of 3 m),
    making it the primary test site.
    \item \em{Lake bank:} A quiet site that allows for easy control of the phone's orientation; bank side has a reflective surface.
    \item \em{Pedal boat:} A busy and noisy location with kayaks and pedal boats passing by; may simulate realistic underwater movement patterns.
    \item \em{Lake beach:} A quiet site for testing real underwater image transmission between two swimmers.
\end{itemize}

As shown in Fig.~\ref{fig:experiment_environment},
when operating far from water, we secure the phone pouch to a tape measure hook with a climbing lock. A barbell weight, attached to the same hook with a  0.5m rope, ensures the phone stays submerged. The tape measure works as both a rope and a depth gauge. The phone in the pouch floats vertically in the water, maintaining an upright orientation when it is static.
When operating close to water, we attach the phone in its pouch to a three-meter-long selfie stick using a 0.5m rope, causing the phone's depth and orientation to vary randomly during the test.

The choice of the waterproof tool is essential for transmission quality and phone safety. After evaluating several options, as shown in Fig.~\ref{fig:experiment_environment}, we choose a pouch for its minimal signal attenuation and reliable waterproofing (rated to withstand 30 hours at a depth of 100 feet)~\cite{TORRAS}.




\input{end_2_end_res}


\parab{Baseline and metrics.}
We consider AquaApp~\cite{chen2022underwater} as one of our baseline systems. AquaApp is originally designed for text transmission using OFDM protocol. We mainly compare our system with the transmission protocol part of AquaApp. We also implement a conventional CSS-based acoustic system based on~\cite{TSN23} as another baseline system. Specifically, it leverages additional 4 symbol chirps (2 up-chirps and 2 down-chirps) for symbol time synchronization.
We integrate both baseline systems with the generative image codec to make them comparable to \sysname. We refer them to as ``OFDM'' and ``CSS'' for simplicity in the following discussions.

We consider two metrics when evaluating \sysname and baseline schemes (unless otherwise stated).
First, we use \textit{Index Error Rate (IER)} as one performance metric since the image quality is directly determined by the correctness of indices, and it also reflects the effectiveness of the techniques at the PHY layer.
Second, we use \textit{LPIPS} as another metric to evaluate the fidelity of the received image. The image metric should reflect if the receiver can understand the semantic meaning of the original image from the received image.
To find such a metric, we manually score the received images with reference to the original image, categorizing them as cannot understand, moderate understanding, and well understanding. We then attempt to find correlations with the following image metrics: PSNR, SSIM~\cite{hore2010image}, MS-SSIM~\cite{wang2003multiscale}, VIF~\cite{sheikh2005visual}, LPIPS, CLIP score~\cite{radford2021learning}, and DinoV2 score~\cite{oquab2023dinov2}. CLIP and DinoV2 scores are cosine similarities between embeddings of received and ground truth images, calculated by large vision models known for semantic representation extraction. Among all the metrics, LPIPS shows the highest correlation, at $-0.4$. Hence, we use LPIPS as our image metric.






\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figs/combined_figure.pdf}
    \caption{Impact of different physical factors. An ``X'' mark indicates that the receiver does not receive the image packets.}
    \label{fig:eval_different_factors}
\end{figure*}


\subsection{Evaluation Results in the Wild}\label{sec:eval_results}

\noindent\textbf{End-to-end performance.}
Fig.~\ref{fig:eval_overall} shows the end-to-end performance of \sysname in the real underwater scenario. We perform the experiment in a lake. During the experiment, two people swim and tread water at random directions and speeds with mobile phones held in hand to mimic the real scuba diving scenario.
In the test, the sender sends four diverse images (shown in each row), and each image is sent 4 times (received images shown in each column).  15 out of 16 sent images are successfully received. The missing one is due to the failure of preamble detection. 14 out of the 15 received images (except for the last image in the first row) maintain their original meaning, proving \sysname's capability in reliably delivering high-fidelity images in realistic diving scenarios. Additionally, the decoder shows great recovery ability. For example, comparing the second and third images in the first row, we can find that although IER jumps from $1.5\%$ to $14\%$, the recovered image still maintains most of the semantic meaning. The success of transmitting diverse images also proves that \sysname's generalizability to different underwater scenarios such as transmitting simple (Image \#2) or complex (Image \#1) objects.


\parab{Effect of distance.} The first image in Fig.~\ref{fig:eval_different_factors} shows IER versus distance. We have several observations. When the distance between sender and receiver is small ($<$5m), all schemes have a small IER which is below 11\% (2\% BER). In particular, \sysname has no index error.
When the distance is between 10--15m, \sysname still shows great performance of low IER while IERs of CSS and OFDM surges to 31\% (6\% BER) and 46\% (19\% BER), respectively.
Even at 20m, the IER of \sysname is tolerable. The mark ``X'' in the figure indicates that the image packets are not received at the receiver side. Based on the result, \sysname can reliably deliver images with a distance of up to 20m, and the performance starts to decrease when the distance is larger than 20m due to signal attenuation.

\parab{Effect of depth and orientation.}
The second and third images in Fig.~\ref{fig:eval_different_factors} show IER versus depth and orientation, respectively. For depth, we placed the mobile phones close to the water surface, at the middle (2m) and the bottom (3m) of the lake, respectively, with a distance of 5m.
For orientation, we first set the speaker of the sender and the microphone of the receiver to face each other. Then, we rotate the receiver phone in the azimuth angle from $0^{\circ}$ to $90^{\circ}$ and $180^{\circ}$, respectively. Furthermore, we also rotate the pitch angle of the receiver phone to $90^{\circ}$ and $-90^{\circ}$ (named up and down) to test the effect when the speaker and microphone are not in the same plane. We mount the phone to a phone holder connected to a selfie stick and submerge the phone underwater to about 1.5m depth with our hands holding the stick.
The results are shown in Fig.~\ref{fig:eval_different_factors}, we can see that depth and orientation changes of the mobile phones have less impact on the systems and IER is generally below 15\% (2\% BER). The variance in the result may come from the slight movement during the experiment.

\parab{Effect of mobility.}
The last image in Fig.~\ref{fig:eval_different_factors} shows IER versus different mobility at the distance of 5m. We submerge two mobile phones connected to tape measures from the bridge in the lake and keep the sender still. We randomly move the receiver by moving one end of the tape measure moderately and intensively.
As depicted in the figure, \sysname shows great resilience to the mobility and maintains IER within 8\% (1.5\% BER) while over 50\% indices are wrong in CSS and OFDM when the mobile devices move.





\subsection{Deep Dive}

\noindent\textbf{Codecs comparisons.}
We compare traditional image codecs with a generative codec and analyze the impact of token errors. The results from experiments conducted on 400 high-quality images from the test dataset are shown in Fig.~\ref{fig:ByERvsLPIPS}. Our observations indicate that as the IER increases, the LPIPS score also rises, signifying a loss of visual details in the received image. Notably, even in the absence of transmission errors and despite having a much larger payload size, the two traditional image codecs perform significantly worse than \sysname.






\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{figs/ByERvsLPIPS.pdf}
    \vspace{-8mm}
    \caption{LPIPS achieved by \sysname at different IERs \emph{vs.}
    traditional codecs (PNG and JPEG).}
    \label{fig:ByERvsLPIPS}
\end{figure}




\parab{Fine-tuning.}
To illustrate the advantage of error-resilient fine-tuning, we consider two new baselines: TiTok~\cite{yu2024image} and \sysname-WE. TiTok is the state-of-the-art generative model trained on ImageNet~\cite{imagenet15russakovsky} and \sysname-WE is fine-tuned on our collected underwater image dataset without introducing transmission errors. We simulate the IER from 0\% to 20\% and the results are shown in Fig.~\ref{fig:eval_fine_tuning}. We find that as the IER increases, the LPIPS score and mean square error (MSE) of all approaches also rise. Furthermore, \sysname can achieve up to 8\% and 21\% gains in the LPIPS and MSE, respectively, compared to the baselines. This demonstrates \sysname is more robust to the transmission errors after fine-tuning.



\parab{Distilled token numbers.}
We train different transformer versions with 48, 64, and 80 distilled tokens from scratch, namely \sysname-48, \sysname-64, and \sysname-80, respectively.
The results are shown in Fig.~\ref{fig:eval_fine_tuning}. \sysname-80 has better LPIPS and MSE as it compresses the image with more (redundant) tokens which is more robust to the indice errors. To balance robustness and computation and communication overhead, we choose $M'$ = 64 distilled tokens.

\begin{figure*}[t]
    \centering
    \begin{subfigure}[b]{.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ablation_2_mse.pdf}
        \label{fig:MSE_VS_IER_fine_tune}
    \end{subfigure}
    \begin{subfigure}[b]{.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ablation_2_lpips.pdf}
        \label{fig:LPIPS_VS_IER_fine_tune}
    \end{subfigure}
        \begin{subfigure}[b]{.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ablation_2_mse_token.pdf}
        \label{fig:MSE_VS_IER_token}
    \end{subfigure}
    \begin{subfigure}[b]{.24\linewidth}
        \centering
        \includegraphics[width=\linewidth]{figs/ablation_2_lpips_token.pdf}
        \label{fig:LPIPS_VS_IER_token}
    \end{subfigure}
    \vspace{-5mm}
    \caption{Impact of fine-tuning and the number of distilled tokens $M'$
    on reconstructed image quality.}
    \label{fig:eval_fine_tuning}
    \vspace{3pt}
\end{figure*}


\parab{Visualization examples.} We compare the reconstructed images from different methods in Fig.~\ref{fig:semantic_compression_filtering_intuition}.
Our first observation is that the image reconstructed from distilled tokens exhibits similar or even better visual quality than the one reconstructed using the VQGAN model, highlighting the importance of context-aware customization for the generative models.
Our second observation is that when the perturbed tokens are identical—where we control both the perturbed tokens and their values to be the same in the last two images—our model, leveraging the recovery capability of error-resilient fine-tuning, retains many visual details. In contrast, the baseline images lose both the visual details and the semantic meaning of the original image.

\input{semantic_filtering_intuition}



















\parab{Time equalization and synchronization.}
We examine the importance of reliability enhancement techniques at the PHY layer proposed by \sysname, as illustrated in Fig.~\ref{fig:eval_ablation}. The SER is used as the evaluation metric because it accurately reflects errors (before channel decoding) in the chirp-based communication system. When only a single equalization is applied at the beginning of the entire image packet (referred to as ``one Equal.''), the system exhibits an average SER exceeding 50\%, indicating that a single initial equalization is insufficient to account for the evolving channel conditions in packets lasting several seconds. Introducing multiple training symbols for repeated equalization, but without employing symbol time synchronization (referred to as ``wo Sync.''), reduces the average SER to below 20\%. Utilizing cross-correlation to estimate the start time of each training symbol, but without applying the smoothed and bounded time offset correction (referred to as ``wo S+B''), mitigates the time offsets caused by movement, further lowering the SER to 11\%. Finally, applying the smoothed and bounded time offset correction, as described in \S\ref{sec:synchronization}, reduces the SER by an additional 2\%, sometimes correcting up to five additional index errors.






\subsection{System-level Performance}\label{sec:eval_system_metrics}

\noindent\textbf{Latency breakdown.}
The averaged end-to-end latency of \sysname is 9.2s with a standard deviation (std) of 0.66s. The average latency of the AquaApp is 10.9s and the std is 3.0s. Fig.~\ref{fig:latency_breakdown} shows latency breakdown of \sysname and AquaApp that contains image encoding, signal generation including channel coding and modulation, transmission, signal recovery including demodulation and channel decoding, and image decoding latency. We found transmission latency is the bottleneck of the end-to-end system latency. Compared to \sysname, AquaApp has much more variations in transmission latency as its data rate changes according to varying channel quality.



\parab{Energy.}
Fig.~\ref{fig:battery} illustrates the battery percentage drained at both the sender and receiver over approximately $50$ minutes of continuous image transmission. The results indicate that the battery level decreases by only 20\%, suggesting that the device could last for over three hours on a full charge. In contrast, recreational diving, typically limited by the size of the gas tank, lasts for up to an hour~\cite{mantacabo_scubadiving}, which is significantly shorter than the battery life of a mobile phone when using \sysname. Hence, \sysname is practical for recreational diving and can also support professional diving which lasts for a longer time.

\begin{figure*}[t]
    \centering
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/ablation.pdf}
        \caption{Ablation results at PHY.}
        \label{fig:eval_ablation}
    \end{minipage}\hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/latency_breakdown.pdf}
        \caption{Latency breakdown.}
        \label{fig:latency_breakdown}
    \end{minipage}\hfill
    \begin{minipage}{0.33\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/battery_levels_over_time.pdf}
        \caption{Battery drain over time.}
        \label{fig:battery}
    \end{minipage}
\end{figure*}




























