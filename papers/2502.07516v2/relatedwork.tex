\section{Related Work}
\textbf{Memorization in Generative Models: }Deep generative models have been shown to exhibit various forms of memorization, including training data extraction \cite{carlini2023extracting}, content replication \cite{somepalli2023diffusion}, and data copying \cite{somepalli2023understanding}. In the medical domain, \cite{akbar2023beware} found that diffusion models tend to memorize significantly more than GANs \cite{goodfellow}. Additionally, \cite{dar2023investigating} emphasized the need for robust mitigation strategies, highlighting the notable memorization in 3D Latent Diffusion Models (LDMs). \\\textbf{Mitigation Mechanisms: } Several mechanisms have been developed to mitigate memorization. \cite{somepalli2023understanding} introduced training and inference-time approaches, such as augmenting caption diversity. \cite{ren2024unveiling} presented a method that identifies memorized tokens by analyzing cross-attention scores, while \cite{wen2024detecting} devised an efficient procedure that leverages text-conditional noise for detection and mitigation. In medical image analysis, \cite{fernandez2023privacy} proposed a framework to remove samples that elevate memorization risk. Additionally, \cite{dutt2024memcontrol,dutt2024capacity} demonstrated that managing model capacity through Parameter-Efficient Fine-Tuning (PEFT) \cite{dutt2023parameter} can significantly reduce memorization. \\
Unlike prior studies that concentrate on mitigating memorization, our work underscores a fundamental flaw in data de-identification and employs established frameworks \cite{wen2024detecting,somepalli2023understanding} to demonstrate its connection to memorization.