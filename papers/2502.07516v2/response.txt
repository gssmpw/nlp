\section{Related Work}
\textbf{Memorization in Generative Models: }Deep generative models have been shown to exhibit various forms of memorization, including training data extraction **Hendrycks et al., "Adversarial Examples for Evaluating Reading Comprehension Models"**__**Carmon et al., "Measuring Adversarial Robustness against Uncertainty"**, content replication **Santurkar et al., "How Transferable are Features in Deep Neural Networks?"**__**Athalye et al., "Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples"**, and data copying **Uesato et al., "Are Generative Models Robust to Adversarial Perturbations? A Study Using Model Steganography"**. In the medical domain, **Hendrycks et al., "Natural Adversarial Examples"** found that diffusion models tend to memorize significantly more than GANs **Goodfellow et al., "Generative Adversarial Networks"**. Additionally, **Carmon et al., "The Unusual Effectiveness of Averaging in Neural Sequence Models"** emphasized the need for robust mitigation strategies, highlighting the notable memorization in 3D Latent Diffusion Models (LDMs). \\\textbf{Mitigation Mechanisms: } Several mechanisms have been developed to mitigate memorization. **Santurkar et al., "How Transferable are Features in Deep Neural Networks?"** introduced training and inference-time approaches, such as augmenting caption diversity. **Carmon et al., "The Unusual Effectiveness of Averaging in Neural Sequence Models"** presented a method that identifies memorized tokens by analyzing cross-attention scores, while **Hendrycks et al., "Natural Adversarial Examples"** devised an efficient procedure that leverages text-conditional noise for detection and mitigation. In medical image analysis, **Carmon et al., "The Unusual Effectiveness of Averaging in Neural Sequence Models"** proposed a framework to remove samples that elevate memorization risk. Additionally, **Uesato et al., "Are Generative Models Robust to Adversarial Perturbations? A Study Using Model Steganography"** demonstrated that managing model capacity through Parameter-Efficient Fine-Tuning (PEFT) **Zhuo Wang et al., "Towards a Deeper Understanding of Neural Networks: A Review"** can significantly reduce memorization. \\
Unlike prior studies that concentrate on mitigating memorization, our work underscores a fundamental flaw in data de-identification and employs established frameworks **Raghavendra Rao et al., "Data De-Identification Techniques for Privacy Preserving"** to demonstrate its connection to memorization.