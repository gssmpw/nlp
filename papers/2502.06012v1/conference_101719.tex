\documentclass[9pt,conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{tikz}
\usepackage{fancyhdr}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{pifont}
\usepackage{xcolor}
\usepackage{wasysym}
\usepackage{flushend} % to balance columns on last page
\usepackage{multirow}
\usepackage[normalem]{ulem}
\useunder{\uline}{\ul}{}
\usepackage{acronym}
\usepackage[citecolor=black,%black!40!red,
            colorlinks=true,
            linkcolor=black,%blue!10!black,
            urlcolor  = black,
            pdftitle={Speaker Embedding Informed Audiovisual Active Speaker Detection for Egocentric Recordings},
            pdfauthor={Jason Clarke, Yoshihiko Gotoh, and Stefan Goetze},
            pdfkeywords={Diarization, Audiovisual Active Speaker Detection, Video-based Face Recognition, Speaker Recognition},
            pdfsubject={Diarization, Audiovisual Active Speaker Detection, Video-based Face Recognition, Speaker Recognition}]{hyperref}
% redefine \autoref{} names (using babel package) to use capital initials 
\usepackage[english]{babel}
\addto\extrasenglish{\def\chapterautorefname{Chapter}}
\addto\extrasenglish{\def\sectionautorefname{Section}}
\addto\extrasenglish{\def\subsectionautorefname{Section}}
\addto\extrasenglish{\def\subsubsectionautorefname{Section}}
\addto\extrasenglish{\def\tableautorefname{Table}}
\addto\extrasenglish{\def\figureautorefname{Figure}}
% acronyms: 
\acrodef{AR}    {augmented reality}
\acrodef{ASD}   {audiovisual active speaker detection}
\acrodef{AVD}   {audiovisual diarization}
\acrodef{GRU}   {gated recurrent unit}
\acrodef{CP}    {conversation Participant}
\acrodef{DeiT}  {data-efficient image transformer}
\acrodef{FoV}   {field of view}
\acrodef{mAP}   {mean Average Precision}
\acrodef{MFCC}  {Mel frequency cepstral coefficient}
\acrodef{MLP}   {multilayer perceptron}
\acrodef{SCAN}   {speaker comparison auxiliary network} 
\acrodef{SNR}   {signal-to-noise-ratio}
\acrodef{SoTA}  {state-of-the-art}
\acrodef{V-TCN} {visual temporal convolutional network}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}    


\newcommand\copyrighttext{%
\footnotesize \textcopyright 2025 IEEE. Personal use of this material is permitted.
Permission from IEEE must be obtained for all other uses, in any current or future
media, including reprinting/republishing this material for advertising or promotional
purposes, creating new collective works, for resale or redistribution to servers or
lists, or reuse of any copyrighted component of this work in other works.}

\newcommand\copyrightnotice{%
\begin{tikzpicture}[remember picture,overlay]
\node[anchor=south,yshift=10pt] at (current page.south)
{\fbox{\parbox{\dimexpr\textwidth-\fboxsep-\fboxrule\relax}{\copyrighttext}}};
\end{tikzpicture}%
}    
\begin{document}

\title{Speaker Embedding Informed Audiovisual Active Speaker Detection for Egocentric Recordings\\
\copyrightnotice

% {\footnotesize \textsuperscript{*}Note: Sub-titles are not captured in Xplore and
% should not be used}
\thanks{This work was supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by UKRI [grant number EP/S023062/1]. This work was also funded in part by Meta.}
}

% \author{\IEEEauthorblockN{1\textsuperscript{st} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{2\textsuperscript{nd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{4\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{5\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% \and
% \IEEEauthorblockN{6\textsuperscript{th} Given Name Surname}
% \IEEEauthorblockA{\textit{dept. name of organization (of Aff.)} \\
% \textit{name of organization (of Aff.)}\\
% City, Country \\
% email address or ORCID}
% }

% Authors
\author{
  \IEEEauthorblockN{
    Jason Clarke$^1$, 
    Yoshihiko Gotoh$^1$, and 
    Stefan Goetze$^{1,2}$
  }
  \IEEEauthorblockA{
    $^1$Speech and Hearing (SPandH) group, School of Computer Science,
    The University of Sheffield, Sheffield, United Kingdom
  }
  \IEEEauthorblockA{
    $^2$South Westphalia University of Applied Sciences, Iserlohn, Germany
  }
  \IEEEauthorblockA{
    \{jclarke8, y.gotoh, s.goetze\}@sheffield.ac.uk, goetze.stefan@fh-swf.de
  }
}


\maketitle

\begin{abstract}
\Ac{ASD} addresses the task of determining the speech activity of a candidate speaker given acoustic and visual data. Typically, systems model the temporal correspondence of audiovisual cues, such as the synchronisation between speech and lip movement. Recent work has explored extending this paradigm by additionally leveraging speaker embeddings extracted from candidate speaker reference speech. This paper proposes the \ac{SCAN} which uses speaker-specific information from both reference speech and the candidate audio signal to disambiguate challenging scenes when the visual signal is unresolvable. Furthermore, an improved method for enrolling face-speaker libraries is developed, which implements a self-supervised approach to video-based face recognition. Fitting with the recent proliferation of wearable devices, this work focuses on improving speaker-embedding-informed \ac{ASD} in the context of egocentric recordings, which can be characterised by acoustic noise and highly dynamic scenes. \ac{SCAN} is implemented with two well-established baselines, namely TalkNet and Light-ASD; yielding a relative improvement in mAP of $14.5\%$ and $10.3\%$ on the Ego4D benchmark, respectively.
% The proposed network enables the extension of most \ac{ASD} systems, facilitating a frame-wise comparison between speaker-specific embeddings from reference and candidate speech. 
% Both the proposed auxilary network and improved face-speaker library enrollment method prove particularly effective when applied to egocentric recording which is characterised by an abundance of overlapping speech, acoustic noise, and highly dynamic scenes.    

% This approach requires a robust means of correlating the identity of the candidate speaker with their pre-diarized reference speech. Previous approaches to do this exhibit good performance on exocentric benchmarks, however, when applied to egocentric recordings, these methods prove highly susceptible to the challenges associated with egocentric data: overlapping speech, acoustic noise, and dynamic scenes. This paper presents a novel approach for enrolling face-speaker embedding libraries that is more robust to egocentric data, along with a parameter-efficient \ac{ASD} model achieving performance in line with current, more complex state-of-the-art.
\end{abstract}

\begin{IEEEkeywords}
Diarization, Audiovisual Active Speaker Detection, Video-based Face Recognition, Speaker Recognition
\end{IEEEkeywords}

\section{Introduction}
\Acf{ASD} revolves around determining the video-framewise speech activity of a candidate speaker. The task is typically formulated as a binary classification problem, where, given a mixed audio signal and sequence of temporally contiguous bounding boxes centered on the candidate speaker's face, a system identifies video frames where the candidate speaker is talking~\cite{talknet, 3DResNet, activespeakersincontext,asdtransformer,buffy,LeonAlcazar2021MAASMA}.

Previous \ac{ASD} research has mainly focused on improving performance for exocentric data (recorded from the third person perspective)~\cite{talknet, TS-talknet, ava-as, activespeakersincontext}, where the camera and microphone are typically stationary relative to the scene rendering the recording conditions favourable. With the recent proliferation of wearable devices, however, the flavour of data \ac{ASD} systems are likely to be deployed upon has shifted to egocentric recordings, where the audiovisual signal is acquired from the first person perspective, and the camera and microphone are dynamic relative to the scene. This change in recording perspective introduces several challenges: (i) low signal-to-noise ratios for speech signals, (ii) highly spontaneous conversations with overlapping speech, (iii) audiovisual distortion caused by the camera wearer's head movements, and (iv) situational obfuscation, where visual cues are occluded~\cite{clarke23-ASD-ASRU, Ego4D}. 
Since the paradigm observed in recent literature involves modelling the correspondence between audiovisual cues indicative of a candidate speaker talking~\cite{EASEE,activespeakersincontext,ASDNet,Liao_2023_CVPR} (like lip movement, cheek posture~\cite{asdtransformer}, and audible speech), this paper argues said approaches are not sufficiently robust to handle the aforementioned challenges associated with egocentric recordings. For example, when speech is present in the audio signal but the video signal is heavily corrupted, a typical system based on audiovisual correspondence will only be able to recognise the presence of speech, the system will not be able to attribute it to the candidate speaker~\cite{activespeakersincontext}. This is illustrated in~\autoref{fig:timeline}: degraded video frames of the candidate speaker (bottom panel) and active speech in the microphone channel (top panel) that is not spoken by the candidate speaker induces a false activity detection for a speaker-embedding-naive system~\cite{Liao_2023_CVPR} (blue line in the middle panel of ~\autoref{fig:timeline}). This problem is also demonstrated by the disparity in performance when evaluating \ac{ASD} systems on exocentric~\cite{ava-as} vs egocentric~\cite{clarke23-ASD-ASRU, LoCoNet, SPELL} benchmarks, where in the latter, challenging scenes are regularly prevalent. 

% One potential reason for this may be because of visual cues being frequently occluded or unresolvable due to blurring in egocentric video.
% This limitation likely stems from the absence of a robust mechanism to accurately distinguish between arbitrary speech, and speech emanating from the candidate speaker when visual cues become occluded. A situation which frequently arises in egocentric video. 

\begin{figure}[!ht]
  \centering
    \includegraphics[scale=0.05]{figures/timeline.png}
  \caption{%Degraded video signal of the candidate speaker (bottom panels) and active speech spoken by an arbitrary speaker in the audio signal (not spoken by the candidate speaker (top panel)) induces example speaker-embedding-naive system to score false positive (middle panel). 
  Example of typical false-positive \ac{ASD}: a) input audio signal; b) ground truth speaker activity of the candidate speaker (inactive throughout) and hypothesised speaker activity by a state-of-the-art speaker-embedding naive \ac{ASD} system~\cite{Liao_2023_CVPR}; c) selection of challenging video frames from a typical egocentric video track~\cite{Ego4D}.}
\label{fig:timeline}
\end{figure}

% Recent work has attempted to mitigate the described problem by injecting speaker-specific information into an established \ac{ASD} system\cite{talknet, TS-talknet}. This was done by feeding reference speech previously spoken by the candidate speaker into a pretrained speaker recognition model\cite{ecapa-tdnn}, the output embedding is then used as an additional source of information. Drawing inspiration from this, this paper presents \ac{SCAN}. \ac{SCAN} is an auxiliary module which is compatible with various end-to-end \ac{ASD} systems\cite{talknet, Liao_2023_CVPR}. Like previous work, it extracts speaker-specific information from reference speech. Novely: \ac{SCAN} also extracts speaker-specific information from the candidate audio signal and performs a framewise comparison between the two via a cross attention mechanism.

Recent work has attempted to mitigate the limitations of established \ac{ASD} systems~\cite{talknet} by injecting speaker-specific information. Specifically, TS-TalkNet~\cite{TS-talknet} uses a pre-trained speaker recognition model to extract speaker embeddings from reference speech based on the well-known ECAPA-TDNN architecture~\cite{ecapa-tdnn}. These speaker embeddings are then leveraged as an additional source of information. Drawing inspiration from this, this paper proposes the \acf{SCAN}, an auxiliary module that can be integrated with various end-to-end \ac{ASD} systems \cite{talknet, Liao_2023_CVPR}. Unlike TS-TalkNet~\cite{TS-talknet}, \ac{SCAN} extracts speaker-specific information from two distinct sources: reference speech, i.e.~previously diarised speech spoken by the candidate speaker, and the candidate audio signal. This technique has previously leveraged successfully in the domain of personal voice activity detection~\cite{ding2022personalvad20optimizing, 10446042}. The novelty of \ac{SCAN} lies in its ability to perform framewise comparisons between these two sources via a cross-attention mechanism. This enables \ac{SCAN} to identify similarities and distinctions between speaker-specific cues in the reference speech and the candidate audio signal with high temporal granularity. By doing so, \ac{SCAN} provides a mechanism for effectively disambiguating scenarios with low-quality video signals, resulting in improved \ac{ASD} reliability and robustness. \ac{SCAN} constitutes the first contribution of this paper. Additionally, this work demonstrates that existing methods for generating identity-speech libraries, which associate reference speech with the identity of the candidate speaker, are not robust to the challenges of egocentric video. To address this, this paper proposes an improved method for generating identity-speech libraries by extending and finetuning an existing face-recognition model to leverage the temporal context of video data via a self-supervised learning objective. This constitutes the second contribution of this paper.



% TS-TalkNet injects speaker-specific information from candidate speaker reference speech, thereby compensating for acoustically challenging scenes, and situations where the candidate speaker's face is visually occluded. Encouragingly, this extension demonstrates promising results on exocentric data, significantly surpassing its respective baselines.

% However, when applied to egocentric data the face-speaker library enrollment method is highly suscepitible to the cahllenge posed and performance

To summarise, the work outlined in this paper expands upon the concept of target-speaker \ac{ASD}, tailoring it to address the particular challenges posed by egocentric recordings with the following main contributions:

\newenvironment{tight_enumerate}{
\begin{enumerate}
  \setlength{\itemsep}{0pt}
  \setlength{\parskip}{0pt}
  \setlength{\topsep}{0pt}
}{\end{enumerate}}
\begin{tight_enumerate}
% \item A novel method to extract pertinent features from the full-scene image of each video frame consisting of a pretrained \ac{DeiT} and a mechanism to condition the extracted representation on the position of each visible interlocutor within the video frame.
\item The auxiliary module \ac{SCAN}, which leverages speaker-specific information to help disambiguate challenging scenes for \ac{ASD}.
\item A self-supervised method to finetune a pre-trained face recognition model on video data to enroll identity-speech libraries more robust to egocentric recordings.
\item Performance analysis on the egocentric Ego4D-AVD dataset~\cite{Ego4D} and exocentric AVA-ActiveSpeaker~\cite{ava-as} using three existing systems as baselines: two speaker-embedding-naive systems - TalkNet~\cite{talknet} and Light-ASD~\cite{Liao_2023_CVPR}; and the current state-of-the-art speaker-embedding-informed system TS-TalkNet~\cite{TS-talknet}.
\end{tight_enumerate}

% The system first enrolls a face-speaker library offline which creates a correspondence between candidate speaker identity and previously diarized reference speech belonging to the candidate speaker. The model then extracts reference speech relevant to the candidate speaker from the library and feeds this into a pretrained speaker recognition module\cite{ecapa-tdnn}.  speaker-specific information is then injected into the model to help mitigate the effects of visual occlusions. This method has exhibited promising results on exocentric data, yielding a significant improvement upon its respective baseline. Furthermore, this speaker-recognition enhancement should, in principal, increase the system robustness in the context of egocentric recordings since visual occlusions are wide-spread.
\section{Speaker Embedding Informed Audiovisual Active Speaker Detection}
\label{sec:method}

This section presents an overview of a typical \ac{ASD} architecture, introduces the proposed \ac{SCAN} module, and outlines the training protocol for fine-tuning an existing framewise face recognition model on egocentric video to construct more robust identity-speech (face-speaker\cite{TS-talknet}) libraries.
% This section first provides an overview of a typical \ac{ASD} architecture and details the proposed \ac{SCAN} module and then describes the training protocol used to finetune an existing framewise face recognition model on egocentric video for constructing more robust identity-speech (face-speaker\cite{TS-talknet}) libraries. 
% Specifically, it concerns the self-supervised video-based method used to fine-tune a pretrained face recognition model on soft labels.
% which leverages information from the identity-speech library enrolled by the preceding methods.

\begin{figure}[!ht]
  \centering
    \includegraphics[scale=0.05]{figures/generic-ASD_TS.png}
  \caption{\ac{SCAN} is shown in the top box which leverages speaker-specific information for framewise comparison of reference speech and input audio signal via cross-attention. The bottom box shows a typical \ac{ASD} architectural design (baseline). Dotted connections represent non-end-to-end passages in the framework.}
\label{fig:ASD}
\end{figure}
%\subsection{Speaker Embedding Informed Audiovisual Active Speaker Detection}
\label{ssec:met_SREASD}

% which performs long term temporal modelling



% # audiovisual encoders
% takes as input a sequence of temporally contiguous video frames $\mathcal{V}_\mathrm{S} = \{\mathbf{V}_{\mathrm{S},1}, ..., \mathbf{V}_{\mathrm{S}, T}\}$ centered on a single candidate speaker $\mathrm{S}$.
% The fronted of the proposed system comprises an audio encoder, a video encoder, and a pretrained speaker recognition module. As portrayed in figure x, the audio and video encoders embed the audio signal, $\mathcal{A}$, and the corresponding track of contiguous frames centred on the candidate speaker $\mathcal{V}_{\mathrm{S}}$, respectively. The speaker encoder is used to extract speaker-specific information from both $\mathcal{A}$ and $\mathcal{A}_{\mathrm{S}}$.   
\vspace{-10pt}
\subsection{Baseline Architecture Overview}
As shown in the lower part of~\autoref{fig:ASD}, conventional \ac{ASD} systems, such as the speaker-embedding-naive systems used as baselines in this paper (i.e.~TalkNet~\cite{talknet} and Light-ASD~\cite{Liao_2023_CVPR}), follow the current paradigm of comprising an audio encoder, a video encoder, a modality fusion mechanism, and a temporal decoder~\cite{talknet, TS-talknet, Liao_2023_CVPR, ASDNet}. These systems operate on the video signal $\mathcal{V}_{\mathrm{S}}$ and the audio signal $\mathcal{A}$. The video signal is a set  $\mathcal{V}_\mathrm{S} = \{\mathbf{V}_{\mathrm{S},1}, ..., \mathbf{V}_{\mathrm{S}, T}\}$ of temporally contiguous video frames, centred on a single candidate speaker $\mathrm{S}$. Each grayscale image in the set is denoted $\mathbf{V}_{\mathrm{S},t} \in \mathbb{R}^{H\times W}$ with time index $t \in \{1, ..., T\}$, height dimension $H$, and width dimension $W$. $\mathcal{A} = \{\mathbf{a}_{1}, ..., \mathbf{a}_{T_\mathrm{A}}\}$ denotes the mixed audio signal temporally correspondent to $\mathcal{V}_{\mathrm{S}}$ with time index $t_{\mathrm{A}} \in \{1, ..., T_{\mathrm{A}}\}$. The distinction between $T$ and $T_{\mathrm{A}}$ compensates for the discrepancy in modality sampling rates.

Video and audio encoders extract pertinent features from their respective modality inputs, and perform short-term temporal modelling to encapsulate the local inter-frame relationships. These encoders typically resemble 3D-ResNets\cite{3DResNet}, \acp{V-TCN}\cite{talknet}, or depth-wise separable convolutions\cite{Liao_2023_CVPR}. The audio and video encoders embed their respective inputs, resulting in two matrices $\mathbf{F}_{\mathrm{A}} \in \mathbb{R}^{T \times d}$ and $\mathbf{F}_{\mathrm{V}}\in \mathbb{R}^{T \times d}$, where $d$ is the embedding dimension of both encoders. A fusion mechanism is then applied to combine these two embeddings, generating a single $2$-dimensional output $\mathbf{F}_{\mathrm{ASV}}$. This fusion is often a simple channel-wise concatenation, summation~\cite{Liao_2023_CVPR}, or an attention-based approach~\cite{talknet, LoCoNet}. The temporal decoder performs two tasks: long-term temporal modelling on the mixed-modality embedding to capture the sequential nature of speech and framewise classification to predict the speaker activity of the candidate speaker.

% When visual cues indicative of the candidate speaker’s speech activity are occluded, either by aberration or physical obfuscation, it is difficult for systems which rely exclusively on audiovisual temporal correspondence to determine whether speech present in the audio signal emanates from the candidate speaker or an arbitrary speaker. To mitigate this problem, inspired by the TS-TalkNet \ac{ASD} system~\cite{TS-talknet}, a speaker recognition module is used to leverage speaker-specific information. Unlike in previous work, where only reference speech is embedded by the speaker recognition model, here both the pre-diarized reference speech signal $\mathbf{a}_{\mathrm{S}}$, and the raw waveform audio signal of the current track are embedded.

\subsection{Speaker Comparison Auxiliary Network (SCAN)}
The TS-TalkNet system, introduced in~\cite{TS-talknet}, builds upon the paradigm outlined in the bottom part of~\autoref{fig:ASD}. It does so by injecting speaker-specific information from pre-diarised reference speech of the candidate speaker $\mathcal{A}_{\mathrm{S}}$ into the model. A speaker embedding $f_{\phi}(\mathcal{A}_{\mathrm{S}})$ is extracted from $\mathcal{A}_{\mathrm{S}}$ via ECAPA-TDNN~\cite{ecapa-tdnn} which is then fused with $\mathbf{F}_{\mathrm{A}}$ and $\mathbf{F}_{\mathrm{V}}$ prior to temporal decoding. This injection of speaker-specific information relating to the candidate speaker results in a significant performance improvement over its respective baseline\cite{talknet}. This extension is motivated by the need to resolve ambiguous scenarios where visual cues indicative of speech activity are occluded, rendering the scenario intractable via audiovisual correspondence alone.

Inspired by this approach, \ac{SCAN} employs a distinct modification by extracting speaker embeddings from not only pre-diarised reference speech $\mathcal{A}_{\mathrm{S}}$, but also from the candidate audio signal $\mathcal{A}$. This enables an explicit comparison between speaker characteristics of the reference speech and the candidate audio signal via the cross-attention mechanism shown in~\autoref{fig:ASD}. As a result, it will be easier for the network to learn to identify similarities and distinctions between $\mathcal{A}$ and $\mathcal{A}_{\mathrm{S}}$.
% , i.e.~when the candidate speaker is talking vs.~when the candidate speaker is silent but possibly other speakers are active, respectively.

First, overlapping windows temporally centred around each video frame are extracted from the raw waveform input of $\mathcal{A}$, transforming $\mathcal{A}$ to a matrix $\mathbf{A}$. % \in \mathbb{R}^{T \times L_\mathrm{A} \, f_{\mathrm{s,A}}}$, where $L_\mathrm{A}$ is the audio frame length. 
This transformation is performed %with sample indices %$\left[ \frac{L_\mathrm{A}}{2} - t + \frac{L_\mathrm{A}}{2} \quad \forall t \in T \right]$
%$\left[L_\mathrm{A} (t_\mathrm{A}-1), L_\mathrm{A}(t_\mathrm{A}-1)+1, ..., L_\mathrm{A}(t_\mathrm{A}-1)+L_\mathrm{A}-1\right]$, 
such that a sufficient duration of audio exists at each time point for meaningful speaker embeddings to be extracted. $\mathcal{A}_{\mathrm{S}}$ and $\mathbf{A}$ are then embedded by a pre-trained speaker recognition model $\mathrm{f}_\phi$ and a cross-attention mechanism is employed along the temporal dimension of $\mathbf{A}$: 

\begin{equation}
    \label{eq:crossatt}
    \mathbf{F}_{\mathrm{S}} = \sigma\left(\frac{\mathrm{f}_\phi(\mathbf{A})\mathrm{f}_\phi(\mathcal{A}_\mathrm{S})^\top}{\sqrt{d_\phi}}\right)\mathrm{f}_\phi(\mathcal{A}_\mathrm{S}) 
\end{equation} 
%
The embedded audio signal $\mathrm{f}_\phi(\mathbf{A})$ is used as the queries, and the embedded reference speech $\mathrm{f}_\phi(\mathcal{A}_\mathrm{S})$ is used as the keys and values. $d_\phi$ denotes the embedding dimension of the speaker recognition model and $\sigma$ represents the softmax function. This is done to determine how well the speech in the current track's audio correlates with that of the reference speech. ECAPA-TDNN~\cite{ecapa-tdnn} pre-trained on the VoxCeleb dataset~\cite{voxceleb} is used as $\mathrm{f}_\phi$ due to its previous use in TS-TalkNet and robust performance on various benchmarks~\cite{TS-talknet}. The ECAPA-TDNN model parameters are frozen and therefore do not contribute to model training. 

% \subsubsection{Backend Speaker Detection}
% The backend speaker detection module is responsible for the fusion and temporal modelling of the multimodal embeddings. Specifically, $\mathbf{F}_{\mathrm{A}}$, $\mathbf{F}_{\mathrm{S}}$, and $\mathbf{F}_{\mathrm{V}}$ are fused via channel-wise concatenation forming $\mathbf{F}_{\mathrm{ASV}}$. The temporal modelling, also inspired by the Light-ASD system~\cite{Liao_2023_CVPR}, is performed by a bi-directional \ac{GRU}. Notably, the bi-directional \ac{GRU} has previously demonstrated superior performance compared to other mechanisms, such as self-attention~\cite{attention, talknet, TS-talknet}, owing to its capacity to exploit sequential inductive bias which are intrinsic to speech activity.

% Most datasets annotated for \ac{ASD} comprise short sequences $\mathcal{V}_\mathrm{S} = \{\mathbf{V}_{\mathrm{S},1}, ..., \mathbf{V}_{\mathrm{S}, T}\}$  of video frames $\mathbf{V}_{\mathrm{S},t}$ at video frame index $t$, focusing on a single candidate speaker $\mathrm{S}$ in conjunction to the corresponding audio signal $\mathcal{A} = \{\mathbf{a}_{1}, ..., \mathbf{a}_{T_\mathrm{A}}\}$, where $\mathbf{a}_{t_\mathrm{A}}$ denotes an audio frame at audio frame index $t_\mathrm{A}$. $T$, and $T_{\mathrm{A}}$ denote the total number of video and audio frames within the sequence, respectively \cite{Ego4D, ava-as}. These short sequences are commonly referred to as tracks, or tracklets. The length of each track is determined by how long the candidate speaker appears in the camera's \ac{FoV} uninterrupted and the constraints of the \ac{ASD} model employed, in terms of its maximum context length. Typically, tracks are between $10$-$300$ video frames (\SI{0.33}{\second}-\SI{10}{\second}) in duration \cite{talknet, TS-talknet}. 

% Generally, annotation stating the pseudo-identities of the candidate speakers is not provided, only the video-framewise speech activity of the candidate speaker is given. However, the work presented in this paper focuses on exploring \ac{ASD} for egocentric data. Specifically, this is in the context of the Ego4D dataset, which, due to its multimodal and multitask nature, includes additional annotation compared to existing datasets. The \ac{AVD} portion of Ego4D consists of a set of video clips $\mathcal{D}$. Within each video clip, faces that verbally engage with the camera wearer are annotated with bounding boxes. Temporally contiguous bounding boxes induce the creation of a track $\{\mathcal{V_{\mathrm{S}}, \mathcal{A}}\}$. Therefore, each video clip represents a set of tracks $\mathcal{C} =  \{ \left( \mathcal{V}_{\mathrm{S}}, \mathcal{A} \right)_1, ..., \left( \mathcal{V}_{\mathrm{S}}, \mathcal{A} \right)_N \}$, where $N$ is the total number of tracks in the clip. Since the visual components of a track focus on a single candidate speaker in isolation, the identity of each video frame within the track is consistent. Additionally, the \ac{AVD} portion of Ego4D provides intraclip identity annotation. This means that different tracks within the same video clip have their pseudo-identities provided. These pseudo-identities are however not consistent across different video clips; if the same candidate speaker appears in different video clips, they are assigned different pseudo-identities.   

% \autoref{fig:overallnetwork} shows the audiovisual \ac{ASD} system to determine the video-frame-wise speech activity of a candidate speaker $\mathrm{S}$ who is visibly present uninterrupted in a set $\mathcal{V} = \{\mathbf{V}^1, ..., \mathbf{V}^T\}$ of consecutive video frames $\mathbf{V}^t \in \mathbb{R}^{C \times H\times W}$ with time index $t \in \{1, ..., T\}$ and $C$, $H$, and $W$ being the channels, height, and width dimensions of each full-scene image, respectively. First, contiguous bounding boxes encapsulating the facial region of the candidate speaker $\mathcal{V}_\mathrm{S} = \{\mathbf{V}_\mathrm{S}^1, ..., \mathbf{V}_\mathrm{S}^T\}$ are extracted from each full-scene image. These contiguous bounding boxes are typically referred to as streams or tracklets. A temporally corresponding audio signal $\mathcal{A} = \{\mathbf{a}^{1}, ..., \mathbf{a}^{T_\mathrm{A}}\}$ is used in conjunction with $V_\mathrm{S}$ to infer the speech activity of the candidate speaker $\mathrm{S}$. It is worth noting that $\mathcal{A}$ has a time index $t_\mathrm{A} \in \{1, ..., T_\mathrm{A}\}$ which is distinct from $t$, to account for the discrepancy in modality sampling rates.
\vspace{-5pt}
\subsection{Identity-Speech Library Generation}
\label{sec:Identity-SpeechLibraryGeneration}

To exploit reference speech information for \ac{ASD}, a correspondence between candidate speaker identity and reference speech must be established. Following TS-TalkNet~\cite{TS-talknet}, this work generates an identity-speech library $\mathcal{E}: \mathbf{i}_{\mathrm{S}} \rightarrow \mathcal{A}_{\mathrm{S}}$ which is pre-enrolled offline, where $\mathbf{i}_{\mathrm{S}}$ is a vector representing the candidate speaker's identity. In \ac{ASD} datasets, identity annotations are typically not provided. However, by definition, tracks are identity-homogeneous. By clustering the identities of each track, this indirectly clusters each track's corresponding pre-diarised speech signal (if the track contains active speech). This is the premise of identity-speech library generation. 


% To generate the identity-speech library $\mathcal{E}: \mathbf{i}_{\mathrm{S}} \rightarrow \mathcal{A}_{\mathrm{S}}$ which is pre-enrolled offline, where $\mathbf{i}_{\mathrm{S}}$ is a vector representing the candidate speaker's identity. 
% The goal of this work is to leverage speaker-specific information to improve the robustness of \ac{ASD} systems in the context of egocentric recordings. Following the convention set in \cite{TS-talknet}, this work explores the use of a pretrained speaker recognition module to determine whether any speech in a given track's audio signal emanates from the candidate speaker by comparing the track's audio with reference speech of the candidate speaker. For this comparison to be made, reference speech from the candidate speaker must be available to the speaker recognition module. To provide this, an identity-speech, or `face-speaker', library $e: \mathbf{i}_{\mathrm{S}} \rightarrow \mathcal{A}_{\mathrm{S}}$ is pre-enrolled offline such that the information from the visual component of the track pertaining to the identity of the candidate speaker $\mathbf{i}_{\mathrm{S}}$ can be used to extract relevant speech $\mathcal{A}_{\mathrm{S}}$ from the library $e$. 

% The quality of the identity-speech library is critical to the performance of the proposed \ac{ASD} system. If $\mathcal{A}_{\mathrm{S}}$ contains speech that does not emanate from the candidate speaker, irrelevant speech will be fed into the speaker recognition module corrupting its output. The robustness of existing face recognition systems \cite{Zheng2018RingLC, arcface, Liu2017SphereFaceDH, Wang2018CosFaceLM}, despite their high performance on other benchmarks, may not be sufficient to withstand the challenges posed by the highly domain-specific nature of egocentric recordings. Consequently, the assumption of transferable performance was deemed to be imprudent. This work annotates a portion of the Ego4D-AVD dataset by psuedo-identity, as a means of evaluating the quality of implemented identity-aggregation methods. This is the first contribution of this work, which is outlined in \autoref{ssec:annotation}. 



% In egocentric data, the prevalence of difficulties such as aberration, occlusion, and blurring is observed. A method has been devised to mitigate the effects of these distortions when extracting identity embeddings from video frames. Rather than operating on individual frames in isolation with hard labels, this method leverages the temporal context provided by other video frames within the track in a self-supervised manner. Further elaboration on this contribution can be found in \autoref{ssec:VBFR}.

% \subsubsection{Cross Track Identity Aggregation}
% \label{ssec:annotation}

% To evaluate the efficacy of identity aggregation methods, it is necessary to acquire cross-video identity annotations. In the Ego4D-AVD dataset, there are over $100$ distinct identities, performing this annotation on the entire dataset conventionally would require significant effort and time, therefore an intuitive protocol to annotate a significant portion of the dataset was employed to minimise effort. This involved exploiting the intratrack and intraclip identity consistencies. \autoref{alg:1} describes a method to obtain a correspondence between cross-track identities, $i$, across clips containing non-overlapping identities.

% \begin{algorithm}
% \begin{algorithmic}[1]
% \STATE Define $\mathcal{L} : \forall \mathcal{C}\in {D}$ video clips sorted by total number of tracked video frames
% \STATE Initialise dictionary for clips comprising mutual identities $r$
% \STATE Initialise dictionary for tracks by identity $i$
% \STATE Initialise set of observed identities $\mathcal{I}$
% \STATE Initialise index $j = 0$
% \WHILE{$j < \alpha$}
% \STATE Define $\mathcal{I}_{\mathrm{C}}$ {set of identities in $\mathcal{L}[j]$}
% \IF{$\forall \mathcal{I}_{\mathrm{C}} \in \mathcal{I}$}
% \STATE $r[$mutual identity video$] = \mathcal{L}[j]$
% \ELSE
% \STATE Add $\mathcal{L}[j]$ as a key to the dictionary $r$ with no associated value
% \STATE Add  $\mathcal{I}_{\mathrm{C}}$ to $\mathcal{I}$ \COMMENT{Add current clip identities to observed identities}
% \FOR{each $\mathcal{V}_{\mathrm{S}}$ in $\mathcal{L}[j]$}
% \STATE Add $\mathcal{V}_{\mathrm{S}}$ to $i[\mathrm{S}]$ \COMMENT{Assign current track to relevant identity}
% \ENDFOR
% \ENDIF
% \STATE $j = j + 1$
% \ENDWHILE
% \caption{Generation of a face recognition evaluation set for Ego4D-AVD. $\mathcal{L}$ denotes an ordered set of all video clips in the dataset sorted by the total number of tracked frames in each clip. $r$ refers to a dictionary which creates a correspondence of video clips comprising mutual identities, $\mathcal{I}$ refers to a set of distinct identities observed during the annotation process. $i$ is a dictionary representing the identity-track association.}
% \label{alg:1}
% \end{algorithmic}
% \end{algorithm}

% Some video clips in Ego4D are acquired concurrently by recording from the perspectives of different camera wearers simultaneously. Additionally, identities (i.e.~the same speakers) can appear in different scenarios. This means identities can appear in drastically different lighting conditions and viewing angles. To effectively evaluate any identity aggregation, or face recognition, systems on egocentric recordings, it is necessary to include identities which occur mutually across different video clips. Therefore, a method to annotate these cross-clip identities was also devised. This involved iterating through the dictionary of mutual identity video clips, $r$, and manually creating the correspondence between cross-video identities. 
\subsubsection{Identity Aggregation}
\label{method:Identity_aggregation}
To construct the identity-speech library, identity embeddings $\mathbf{i}$ are extracted from the visual component of all tracks in a given dataset. Cosine similarity is used to assess the similarity between a pair of identity embeddings. If the similarity exceeds a static threshold, the identities within each track are considered to be the same, and pre-diarised speech within the track's corresponding audio signal is attributed to this identity in the library. 
% This is described in further detail in~\autoref{ssec:exps_identityagg}. 

\subsubsection{Self-Supervised Video-Based Face Recognition}
\label{ssec:VBFR}
The performance of the face recognition model used is critical to the quality of the identity-speech library and consequently the utility of the output of \ac{SCAN}. For example, if $\mathcal{E}:\mathbf{i}_{\mathrm{S}}$ contains speech not spoken by $\mathrm{S}$ when $\mathcal{A}_{\mathrm{S}}$ is sampled, irrelevant speech could be fed into the speaker recognition module, rendering its output uninformative, or even deleterious. The reliability of existing frame-based face recognition systems~\cite{Zheng2018RingLC, arcface, Liu2017SphereFaceDH, Wang2018CosFaceLM}, despite their robust performance on other benchmarks, will not be sufficient to withstand the challenges posed by the highly domain-specific nature of egocentric recordings.
% Consequently, the assumption of transferable performance was deemed to be imprudent. 
Subsequently, a method was devised to adapt and finetune an existing pre-trained face recognition model. Firstly, a means of modelling consecutive frames as a sequence was integrated into a frame-based face recognition model, enabling it to effectively encapsulate and leverage the temporal context associated with video data. Secondly, since hard labels describing the trackwise identities of each person within \ac{ASD} datasets are not typically provided, a self-supervised training objective was employed. 
% To this end, an existing pre-trained frame-based face recognition model was extended to leverage temporal information without hard labels. 


% This work annotates a portion of the Ego4D-AVD dataset by psuedo-identity, as a means of evaluating the quality of implemented identity-aggregation methods which in turn is a proxy for the identity-speech library quality.

% Egocentric video presents challenges which are highly domain specific. Because of this and due to there being a limited abundance of annotated egocentric data, it is necessary

\begin{figure}[!ht]
  \centering
    \includegraphics[scale=0.05]{figures/VB-face-recog.png}
  \caption{Self-supervised video-based face recognition model. impostor frames are randomly inserted into the parent track, resulting in polluted track $\mathcal{V}_{\mathrm{S}}'$. The training objective involves the model classifying frames as either native or impostor frames with respect to the parent track. $\diameter$ denotes mean average.}
\label{fig:VBFR}
\end{figure}

As depicted in~\autoref{fig:VBFR}, input tracks $\mathcal{V}_{\mathrm{S}}$ are polluted with impostor frames $\mathbf{V}_{\mathrm{I}_x}$ randomly chosen from other tracks. Frames within the polluted track $\mathcal{V}_{\mathrm{S}}'$ are then individually encoded by the face recognition model~\cite{glint360k}. This output is then fed through $L$ transformer encoder layers~\cite{attention} where the model has the capacity to attend across different frames within the track, thus leveraging the temporal context of video data. 
The model then determines whether each frame is either an impostor or native frame via binary classification. Learning this classification indirectly conditions the model to 
assign low weighting to frames not relevant to the track's overall identity (poor quality frames with significant visual distortion or occlusion) and high weighting to crisp frames where the parent track identity is clearly apparent and recognisable.

Once fine-tuned, the output of the last transformer encoder layer is averaged across its temporal dimension to generate a single embedding representative of the candidate speaker's identity $\mathbf{i}_{\mathrm{S}}$. 
% Difficulties such as, aberration, occlusion, blurring are rife, and it is often essential to leverage temporal context to model visual information as opposed to modelling video frames individually. 




% Since the output of the speaker recognition module is a single vector without a temporal component, this vector is replicated such that it shares the same temporal dimension as the number of video frames in the track. 
\section{Experiments}
\label{sec:experiments}
This section describes the datasets and experiments used to evaluate \ac{SCAN} for \ac{ASD} and the quality of the identity-speech library.

% \subsection{Identity Aggregation}
% \label{ssec:exps_identityagg}

% A face-recognition model\cite{An2022KillingTB} is used as a backbone which comprises ResNet-50 \cite{resnet} pretrained on the Glint360K dataset\cite{glint360k}.
% % To establish an identity aggregation method that is robust to egocentric video, various configurations for extracting and aggregating identity embeddings were tested. Fundamentally 
% Two types of identity aggregation methods were evaluated: \emph{frame-based} (FB)
% % , where the face recognition model is naive of any contextual information provided by neighbouring frames in the track
% , and \emph{video-based} (VB)
% % where the face recognition model can leverage contextual information(outlined in \autoref{ssec:met_SREASD}). 
% All systems fundamentally rely on a single face-recognition model\cite{An2022KillingTB} as a backbone, which comprises a ResNet-50\cite{resnet} pretrained on the Glint360K dataset\cite{glint360k}.

% \textbf{TS-TalkNet} refers to the first frame-based system implemented, it is identical to that used with the TS-TalkNet system~\cite{TS-talknet}. In this approach, $30$\% of frames in a given track are randomly selected to form the set of frames $\mathcal{V}_{\mathrm{S}}'$. Each selected frame is then encoded by the pre-trained face recognition model $\mathrm{f}_\omega$, outputting the vector $\mathbf{e}_{\mathrm{S},t}\in \mathbb{R}^{d_\omega}$ for each selected frame, where $d_\omega$ is the embedding dimension of the face recognition model: 
% \begin{align}
%   \mathbf{e}_{\mathrm{S},t} &= \mathrm{f}_\omega(\mathbf{V}_{\mathrm{S}, t}) : \forall \mathbf{V}_{\mathrm{S},t} \in \mathcal{V}_{\mathrm{S}}' 
%   \label{equation:eq1}
% \end{align}

% To obtain a single embedding which represents the identity of the whole track, the embedded track is averaged across its temporal dimension producing $\overline{\mathbf{e}_{\mathrm{S}}}\in \mathbb{R}^{d_\omega}$. \textbf{FB:1} replicates the configuration of \textbf{TS-TalkNet}, except, unlike \textbf{TS-TalkNet}, it extracts embeddings for all frames in the track. \textbf{FB:2} assumes the same approach as \textbf{FB:1}, however, additional preprocessing is applied when extracting bounding boxes. Specifically, smoothing is applied to mitigate the effects of poor detections, and padding is applied to partially occluded faces to prevent aberration.

% The model architecture outlined in \autoref{ssec:VBFR} is assumed for all video-based configurations (\textbf{VB:1-VB:4}). The distinguishing factors among these is the training protocol, embedding extraction, and embedding aggregation methods used. Each frame uses every frame in each track to acquire identity embeddings, and no additional preprocessing is applied.

% \textbf{VB:1} denotes the first video-based system where the face recognition model parameters are frozen and embeddings are extracted from the final transformer encoder layer. Comparatively, in \textbf{VB:2} all parameters are unfrozen. \textbf{VB:3} also has all parameters enabled for learning, but extracts embeddings directly from the face recognition model's output.

% \textbf{VB:4} sets all parameters as learnable, extracts embeddings from the face encoder output, and aggregates them differently using a weighted average of the embeddings. The weight for each element is its corresponding attention value as described by~\autoref{equation:eq3}. The attention weight tensor $\mathbf{W} \in \mathbb{R}^{L \times H \times T \times T}$ is collapsed via mean average across the head dimension and an arbitrary layer is selected producing attention weight matrix $\overline{\mathbf{W}_L} \in \mathbb{R}^{ T \times T}$. This is further reduced to a single vector $\mathbf{k}_L \in \mathbb{R}^{T}$ by summing across the query dimension. This process determines the significance of each video frame during the self-attention mechanism. Frames with distortion and obfuscated identity are attended to less than clear, crisp frames. This is desirable when determining an embedding representative of an entire track's identity; less clear frames should contribute less to the overall embedding than clear frames.
% %
% % \textbf{VB:1-VB:2} both assume the model architecture outlined in \autoref{ssec:met_SREASD}. The only trait which differs between these approaches is the embedding aggregation method used. Both these approaches use all frames in each track to acquire identity embeddings, and no additional pre-processing stage, outlined in \textbf{FB:1}, is used. \textbf{VB:1} extracts embeddings from the output of the pre-trained face recognition layers after fine-tuning and embeddings are aggregated consistent with the preceding methods. In \textbf{VB:2}, embeddings are aggregated using a weighted average, described in~\autoref{equation:eq2}. Attention weight tensor $\mathbf{W} \in \mathbb{R}^{L \times H \times T \times T}$, where $L$ and $H$ are the layer dimension and head dimensions, respectively, is collapsed via mean average across the head dimension and an arbitrary layer is selected producing attention weight matrix $\overline{\mathbf{W}_L} \in \mathbb{R}^{ T \times T}$. This is then further reduced to a single vector $\mathbf{k}_L \in \mathbb{R}^{T}$ by summing across the query dimension to determine how significant each video frame is during the self-attention mechanism. The rationale here is that frames which are distorted, and the identity obfuscated, will be attended to less than clear, crisp frames where the identity of the candidate is clearly visible.
% % \textbf{VB:1-VB:4} all assume the model architecture outlined in \autoref{ssec:met_SREASD}. The only trait which differs between these approaches is the exact training protocol, embedding extraction, and embedding aggregation methods used. Additionally, these approaches each use every frame in each track to acquire the identity embeddings, and no additional preprocessing stage, outlined in \textbf{FB:1}, is used. \textbf{VB:1} refers to the first video-based system implemented. Here the parameters of the face recognition model are frozen and the embeddings are extracted from the output of the final transformer encoder layer of the network. \textbf{VB:2} enables all the parameters in $\mathrm{f}_\omega$ for learning. With this configuration embeddings are also extracted from the output of the final transformer layer. \textbf{VB:3}, like \textbf{VB:2}, has all parameters set as learnable, however, embeddings in this configuration are extracted directly from the face encoder layer. \textbf{VB:4} sets all parameters as learnable, extracts the embeddings from the face encoder output but then aggregates them differently. The aggregation involves using a weighted average of the embeddings, where the weight for each element its corresponding attention value, as described by~\autoref{equation:eq2}. Attention weight tensor $\mathbf{W} \in \mathbb{R}^{L \times H \times T \times T}$, where $L$ and $H$ are the layer dimension and head dimensions, respectively, is collapsed via mean average across the head dimension and an arbitrary layer is selected producing attention weight matrix $\overline{\mathbf{W}_L} \in \mathbb{R}^{ T \times T}$. This is then further reduced to a single vector $\mathbf{k}_L \in \mathbb{R}^{T}$ by summing across the query dimension to determine how significant each video frame is during the self-attention mechanism. The rationale here is that frames which are distorted, and the identity obfuscated, will be attended to less than clear, crisp frames where the identity of the candidate is clearly visible. When determining an embedding representative of an entire track's identity, this is also desirable; frames which are poor in clarity should contribute less to the overall embedding than frames which are crisp and clear. 
% %
% \begin{align}  \overline{\mathbf{e}_{\mathrm{S}}} &= \sum^{T}_{t} k_{L,t}\mathbf{e}_{\mathrm{S},t}
%   \label{equation:eq3}
% \end{align}
\vspace{-5pt}
\subsection{Datasets}
\label{ssec:datasets}

\textbf{AVA-ActiveSpeaker}~\cite{ava-as} is a frequently-used, exocetric, large-scale audiovisual \ac{ASD} dataset, comprising $262$ Hollywood movie clips ($120$ for training, $33$ for validation, and $109$ for testing) with $3.65$ million human-labeled video frames ($38.5$ hours of face tracks) and corresponding audio.

\textbf{Ego4D-AVD}~\cite{Ego4D} records from the egocentric perspective. It comprises $572$ distinct video clips. Each video clip is $5$ minutes in length, some of which are recorded concurrently. All data is recorded monaurally using a variety of wearable devices. All video is sampled at $30$~Hz and uses high-definition resolution. The dataset is stratified as follows: $379$ clips for training, $50$ clips for validation, and $133$ clips for testing. The full validation fold of Ego4D-AVD was annotated by this work in terms of pseudo-identity. This was to provide a robust means of evaluating identity-speech libraries (cf. ~\autoref{tab:identity-speech}).

\subsection{Implementation Details}
\label{ssec:imps}

\textbf{Baselines:} All baseline models were implemented using the same input features, optimisers, and learning rates used in each systems's original implementations~\cite{talknet,Liao_2023_CVPR,TS-talknet}. Standard \ac{ASD} augmentation techniques were applied such as negative sampling of the audio signal, and flipping, cropping, and rotating of the video signal.

\textbf{SCAN:} The output of \ac{SCAN} contributed to each baseline system's loss function as an auxiliary loss, using binary cross entropy to perform framewise classification upon $\mathbf{F}_{\mathrm{S}}$. 
% The auxiliary loss weighting was $0.5$ for both $\mathbf{F}_{\mathrm{V}}$ and $\mathbf{F}_{\mathrm{S}}$. 
Raw waveform audio served as the reference speech input to the speaker recognition model ($1024$ channel ECAPA-TDNN~\cite{ecapa-tdnn}). The output of \ac{SCAN} $\mathbf{F}_{\mathrm{S}}$ used an embedding dimension of $64$. $1$ second windows of audio was used to extract each speaker embedding from $\mathbf{A}$. Reference speech was randomly selected from the relevant part of the identity-speech library to increase training variability. 
% Training utilised a batch size of $3600$ on an A$100$ GPU with mixed precision, achieving convergence within approximately $8$ hours in $25$ epochs.

\textbf{Identity-Speech Library:} For the finetuning of the face recognition model (cf.~\autoref{ssec:VBFR}), a cross-entropy loss function was used. Input to the system were tracks comprising colour images with a $30$\% impostor insertion rate. $4$ Transformer encoder layers ($L=4$) each comprising $8$ attention heads with a model dimension of $1024$ were trained for $10$ epochs on a single NVIDIA A$100$ GPU for $2$ hours with a batch size of $1800$. To create the identity-speech library a static comparison threshold of $0.9$ was used to construct the library and a $2.5$ second minimum duration of speech was enforced.
% Upon anecdotal analysis of the transformer layers, it was found that the first layer produced the most interpretable attention maps and was therefore used during the attention weighting expressed in (\ref{equation:eq3}).
\vspace{-5pt}
\subsection{Evaluation Metric}
\label{ssec:evaluation_metric}

% To evaluate the quality of the identity aggregation methods, the difference between the average cosine similarities of intra-identity embeddings and cross-identity embeddings is used. 
% To evaluate each system for \ac{ASD}, the Cartucho object detection\ac{mAP}~\cite{cartucho} was used, which adheres to the \ac{mAP} criterion from the PASCAL VOC2012 competition~\cite{pascal-voc-2012}. This is consistent with the approach employed as part of the Ego4D audiovisual diarization challenge and therefore maintains consistency with recent literature~\cite{Ego4D}. Since the ground truth annotation is not openly available for the test folds of Ego4D and AVA-ActiveSpeaker due to them contributing to active challenges, this work follows the current convention in \ac{ASD} by reporting result on the validation folds of each dataset~\cite{activespeakersincontext, ASDNet, clarke23-ASD-ASRU, SPELL, EASEE, LoCoNet} and uses the validation folds
% of each dataset for evaluation.

Evaluation of each system for \ac{ASD} is performed using the Cartucho object detection \ac{mAP}\cite{cartucho}, which adheres to the \ac{mAP} criterion from the PASCAL VOC2012 competition\cite{pascal-voc-2012}. This approach is consistent with the Ego4D audiovisual diarisation challenge~\cite{Ego4D} and recent literature~\cite{clarke23-ASD-ASRU}. Due to the unavailability of ground truth annotations for the test folds of Ego4D and AVA-ActiveSpeaker, results are reported on the validation folds of each dataset, following the convention in \ac{ASD}~\cite{activespeakersincontext, ASDNet, clarke23-ASD-ASRU, SPELL, EASEE, LoCoNet}.
\vspace{-10pt}
\section{Results}
\label{sec:Results}
This section demonstrates the performance of \ac{SCAN} when used in conjunction with two speaker-embedding-naive systems, TalkNet~\cite{talknet} and Light-ASD~\cite{Liao_2023_CVPR}. The identity-speech library generation method proposed by this paper (cf.~\autoref{sec:Identity-SpeechLibraryGeneration}) is also evaluated and compared with previous work when applied to egocentric recordings.

% \subsection{Identity Aggregation}
% \label{ssec:res_identityagg}

% \autoref{tab:identityagg} displays the performance of the proposed identity aggregation methods with the TS-TalkNet system, after testing on the identity evaluation set outlined in \autoref{ssec:datasets}.

% \begin{table}[h]
% \caption{Comparison of various configurations to aggregate identities. 
% $\Theta_{\text{Intra}}$ denotes the average cosine similarity of tracks belonging to the same identity, $\Theta_{\text{Cross}}$ denotes the average cosine similarity of tracks belonging to different identities.}
% \label{tab:identityagg}
% %\resizebox{\columnwidth}{!}{
% \begin{footnotesize}
% \begin{tabular}{lccc}
% \hline
% \textbf{Configuration} & \textbf{$\Theta_{\text{Intra}}$} & \textbf{$\Theta_{\text{Cross}}$} & \textbf{Difference} \\ \hline
% VB:2 & $0.95\pm0.04$ & $0.92\pm0.04$ & $0.03\pm0.05$ \\
% TS-TalkNet~\cite{TS-talknet} & $0.78\pm0.14$ & $0.72\pm0.15$ & $0.06\pm0.21$ \\
% FB:1 & $0.74\pm0.14$ & $0.63\pm0.15$ & $0.11\pm0.21$ \\
% FB:2 & $0.67\pm0.19$ & $0.48\pm0.21$ & $0.19\pm0.28$ \\
% VB:1 & $0.71\pm0.17$ & $0.46\pm0.18$ & $0.25\pm0.25$ \\
% VB:3 & $0.75\pm0.23$ & $0.38\pm0.37$ & $0.37\pm0.44$ \\
% \textbf{VB:4} & $0.77\pm0.19$ & $0.31\pm0.37$ & $0.46\pm0.42$ \\
% \end{tabular}
% \end{footnotesize}
% %}
% \end{table}




% % ADD PRECEDING FIGURES

% All frame-based configurations (TS-TalkNet \& \textbf{FB:1-FB:2}) perform as expected since they are not finetuned on egocentric data. Furthermore, these configurations have no mechanism to mitigate the impact of poor-quality frames, rendering them highly susceptible to aberration and occlusions. 
% % One notable observation exhibited here is that applying padding and smoothing (\textbf{FB:1}) to the bounding boxes is detrimental to performance compared to its equivalent baseline (\textbf{FB:2}). This is likely because the pretrained face recognition model is trained on a sufficient quantity of diverse data to be inherently robust to aberrations. Additionally, since ground truth bounding box annotations are being used, bounding box smoothing is unlikely to produce an improvement. 
% % Video-based configurations which extract embeddings from the output of the final transformer layer \textbf{VB:1-VB:2} perform worse than the configurations which extract embeddings from the output of the face encoder (\textbf{VB:3-VB:4}). This is particularly the case for \textbf{VB:2}, where all parameters are trainable. The authors speculate this may be due to parameters in the transformer layers being exclusively trained to distinguish between native and impostor frames. This being binary classification training objective as opposed to a metric learning one. Consequently these configurations may have less capacity to cluster embeddings of the same identity. This limitation is likely more pronounced in \textbf{VB:2}, where all parameters are learnable. This not only introduces more variability but also replaces the metric learning objective used during the pretraining of the back end face recognition model with a discriminative one.
% Embeddings extract from the final transformer layer in video-based configurations \textbf{VB:1-VB:2} underperform compared to those from the face recognition module, especially in \textbf{VB:2} where all paremters are trainable. The authors speculate that this is due to the transformer layers being trained solely for binary classification between native and impostor frames, limiting their ability to cluster embeddings of the same identity. This limitation is likely more pronounced in \textbf{VB:2}, where all parameters are learnable, introducing more variability and replacing the metric learning objective used during pretraining with a discriminative one. 

% Interestingly, \textbf{VB:4} shows only a slight improvement over configurations that do not weight intratrack frames by their attention value $k_{L,t}$. This implies that finetuning on domain-specific data is more beneficial than leveraging temporal context information. Despite testing several normalisation operations on the attention values to enhance their effect during the weighted average, a negligible difference was observed.
\vspace{-7pt}

\subsection{Audiovisual Active Speaker Detection}
\label{ssec:results_ASD}
The results of incorporating \ac{SCAN} with two speaker-embedding-naive systems, TalkNet~\cite{talknet} and Light-ASD~\cite{Liao_2023_CVPR}, are shown in~\autoref{tab:speaker_embeddings}. 

% \begin{table}[!ht]
% \centering
% \caption{Comparison of baseline systems on Ego4D-AVD and AVA validation fold in terms of \ac{mAP}. $\dagger$ denotes ground truth identity-speech library. Bold highlights best-performing system with hypothesised identity-speech library, underlined represents best system with ground truth identity-speech library.}
% \label{tab:speaker_embeddings}
% \begin{tabular}{ccc}
% \hline
% \textbf{System}         & \textbf{Ego4D-mAP {[}\%{]}} & \textbf{AVA-mAP {[}\%{]}} \\ \hline
% TS-TalkNet              & 52.2                        & 93.9                      \\
% TS-TalkNet$\dagger$     & 54.0                        &                           \\
% TalkNet                 & 51.0                        & 92.3                      \\
% TalkNet+SCAN            & \textbf{58.0}               &                           \\
% TalkNet+SCAN$\dagger$   & 58.4                        &                           \\
% Light-ASD               & 54.3                        & 94.1                      \\
% Light-ASD+SCAN          & 57.1                        &                           \\
% Light-ASD+SCAN$\dagger$ & {\ul 59.9}                  &                  \\ \hline
% \end{tabular}
% \end{table}
\vspace{-5pt}
\begin{table}[!ht]
\centering
\caption{Performance comparison on Ego4D-AVD and AVA validation fold. Identity-Speech Library$\dagger$ refers to ground truth identity-speech library. Bold highlights best-performing system with hypothesised identity-speech library, underlined represents best system with ground truth identity-speech library.}
\label{tab:speaker_embeddings}
\begin{tabular}{ccccc}
\hline
{} & \textbf{SCAN} & \textbf{Identity-Speech } & \multicolumn{2}{c}{\textbf{mAP {[}\%{]}}} \\
\multirow{-2}{*}{\textbf{Baseline}} & \textbf{used} & \textbf{Library$\dagger$} & \textbf{Ego4D} & \textbf{AVA} \\ \hline
{}              &\ding{55} & \ding{55} & 52.2                        & \textbf{93.9}                      \\
\multirow{-2}{*}{TS-TalkNet}     & \ding{55}& \checkmark & 54.0                        &      93.9                     \\
\hline
{}                 &\ding{55} &- & 51.0                        & 92.3                      \\
{TalkNet}           & \checkmark & \ding{55} & \textbf{58.0}               &            93.8               \\
{}   & \checkmark & \checkmark & 58.4                        &                    94.0       \\ 
\hline
{}              &\ding{55} & - & 54.3                        & 94.1                      \\
Light-ASD          & \checkmark & \ding{55}& 57.1                        &                \textbf{93.9}           \\
{} & \checkmark & \checkmark & {\ul 59.9}                  &                {\ul 94.2}  \\ \hline
\end{tabular}
\end{table}

On the Ego4D benchmark, \ac{SCAN} significantly improves performance of the respective baseline systems for both ground truth identity-speech library and hypothesised identity-speech library configurations. Ground truth identity-speech libraries referring to those which are created directly from the dataset's annotation, hypothesis identity-speech library referring to those created by the method outlined in~\autoref{sec:Identity-SpeechLibraryGeneration}. For the AVA benchmark (exocentric) the improvements are much more modest. This is likely because visually challenging multi-talker scenarios, in which \ac{SCAN} would be beneficial, are much less prevalent than in Ego4D. Nevertheless, both configurations provide a substantial improvement upon the TalkNet baseline system. Additionally, TalkNet+\ac{SCAN} outperforms TS-TalkNet, a previous speaker-embedding-informed system by $5.8$\% and $4.4$\% \ac{mAP} for ground truth and non-ground truth identity-speech libraries, respectively. Since a significant improvement upon the TS-TalkNet baseline is apparent when ground truth identity-speech libraries are used, it is fair to deduce \ac{SCAN}'s architectural implementation and method of extracting speaker-specific information from both the candidate audio signal and reference speech is more effective than relying solely on reference speech. 
% Shown in table ~\autoref{tab:speaker_embeddings}, \ac{SCAN} is incorporated with two speaker-embedding-naive systems: TalkNet~\cite{talknet} and Light-ASD~\cite{Liao_2023_CVPR} and evaluated on both egocentric and exocentric benchmarks. The use of \ac{SCAN} yields a significant improvement to both baseline systems on both egocentric and exocentric data. Additionally, \ac{SCAN} outperforms previous speaker-embedding-informed work of TS-TalkNet (TS-TalkNet compared with TalkNet+SCAN). This holds true even when ground truth identity-speech libraries are used (TS-TalkNet$\dagger$ vs. TalkNet+SCAN$\dagger$), meaning the architectural implementation of \ac{SCAN} and its means of extracting speaker-specific information from candidate audio signal prove superior to extracting speaker-specific information from reference speech exclusively.
Furthermore, the improvement yielded by incorporating \ac{SCAN} into the baseline systems renders both baseline systems almost competitive with state-of-the-art methods in the context of egocentric data. Specifically, \ac{SCAN} enhances the TalkNet and Light-ASD baselines by $14.5\%$ and $10.3\%$, respectively, bridging the gap with state-of-the-art performance, as shown in~\autoref{tab:sota}



\vspace{-5pt}
\begin{table}[!ht]
\centering
\caption{Comparison with the state-of-the-art \ac{ASD} systems on validation folds of Ego4D and AVA. Values for LoCoNet~\cite{LoCoNet} and SPELL~\cite{spellego4dchallenge} are from their original manuscripts.}
\label{tab:sota}
\begin{tabular}{cccc}
\hline
\textbf{System}      & \textbf{Spk. Emb. Inf.} & \textbf{Ego4D {[}\%{]}} & \textbf{AVA {[}\%{]}} \\ \hline
TalkNet~\cite{talknet}              & \ding{55}                        & 51.0                        & 92.3                      \\
TS-TalkNet$\dagger$           &  \checkmark                        & 54.0                        & 93.9                      \\
Light-ASD~\cite{Liao_2023_CVPR}            &   \ding{55}                      & 54.3                        & 94.1                      \\
LoCoNet~\cite{LoCoNet}              &     \ding{55}                    & 59.7                        & \textbf{95.2}             \\

SPELLL~\cite{spellego4dchallenge}                &   \ding{55}                      & \textbf{60.7}               & 94.2                      \\ \hline
 TalkNet+SCAN$\dagger$   &  \checkmark                       & 58.4                        &    94.0                       \\
 Light-ASD+SCAN$\dagger$ &   \checkmark                      & 59.9                        &    93.9   \\ \hline
\end{tabular}
\end{table}
\vspace{-10pt}


\subsection{Identity-Speech Library}
\label{ssec:results_ISL}

The results presented in~\autoref{tab:identity-speech} indicate a substantial improvement in the quality of the identity-speech library generated by the proposed method. This improvement is further demonstrated by~\autoref{fig:hist}. In the left panel (TS-TalkNet), it is impossible to differentiate same-identity pairs from different identity pairings while in the right panel (\ac{SCAN}) resolving the two pairings is easier. This is attributed to the face-recognition model's ability to leverage temporal context via self-attention. However, it is noted that the silhouette score of $0.16$ indicates only minor cluster separability, suggesting that further refinement of the proposed method might be necessary to achieve more robust future identity-speech library generation.
\vspace{-5pt}
\begin{table}[!ht]
\centering
\caption{Comparison of identity-speech library generation methods.}
\label{tab:identity-speech}
\begin{tabular}{cc}
\hline
\textbf{System} & \textbf{Ego4D-Silhouette} \\ 
\hline
TS-TalkNet      & -0.17            \\
SCAN            & \textbf{0.16}     \\ \hline
\end{tabular}
\end{table}
\vspace{-10pt}
\begin{figure}[!ht]
  \centering
    \includegraphics[scale=0.062]{figures/histogram_cosin.png}
  \caption{Similarity between same-identity embeddings and different-identity embeddings shown in green and red, respectively, for Ego4D validation fold~\cite{Ego4D}}
\label{fig:hist}
\end{figure}
\vspace{-10pt}

% ~\autoref{tab:ego4d} compares performance of the proposed system on the Ego4D-AVD dataset on both the validation fold (in line with other literature), and the test set (cf.~\autoref{ssec:datasets}). A significant improvement is observed compared with both baseline systems which this work builds upon (Light-ASD and TS-TalkNet). This has been achieved whilst retaining a parameter-efficient approach. 
% % The discrepancy between ground truth and modelled identity-speech library demonstrates

% \begin{table}[!ht]
% \centering
% \caption{Performance on Ego4D-AVD dataset by \ac{mAP} [\%]. Proposed methods in \textbf{bold} font. $\dagger$ denotes use of ground truth identity-speech library containing oracle reference speech. Values for SPELL and TalkNet are taken from existing literature, others were independently acquired.}
% \label{tab:ego4d}
% %\resizebox{\columnwidth}{!}{
% \begin{footnotesize}
% \begin{tabular}{cccc}
% \hline
% \textbf{System} & \textbf{Params. [M]} & \textbf{Ego4D-val } & \textbf{Ego4D-test*} \\ \hline
% TalkNet\cite{Ego4D} & 15.7 & 51.0 & 57.6 \\
% TS-TalkNet\cite{TS-talknet} & 15.7 & 52.2 & 57.6 \\
% TS-TalkNet$\dagger$ & 15.7 & 54.0 & 58.0 \\
% Light-ASD\cite{Liao_2023_CVPR} & 1 & 54.3 & 58.4 \\
% SPELL\cite{SPELL} & 22.5 & 60.7 & - \\
% \hline
% \multicolumn{1}{l}{\textbf{TS-Light-ASD}} & \textbf{2.52} & \textbf{57.1} & \textbf{60.1} \\
% \multicolumn{1}{l}{\textbf{TS-Light-ASD$\dagger$}} & \textbf{2.52} & \textbf{59.9} & \textbf{60.4}  \\
% \hline
% \end{tabular}
% \end{footnotesize}
% %}
% \end{table}

%\vspace*{-3ex}

\section{Conclusion}

This work proposes \ac{SCAN}, a speaker-embedding-informed extension to conventional \ac{ASD} systems. \ac{SCAN} assists in disambiguating challenging multi-talker scenarios involving visual noise and physical obfuscations. \ac{SCAN} builds upon previous work by extracting speaker-specific information from reference speech, but is able to leverage speaker-specific information inherently present in the candidate audio signal itself. Furthermore, \ac{SCAN} proposes a method to finetune frame-based face-recognition models on video data without hard identity labels by transformer encoder layers and a self-supervised training objective. This approach exhibits a significant performance improvement relative to previous work for identity-speech library generation.
% but significant work is still required to make these fully robust to the challenges posed by egocentric recordings.

% This work presents a speaker-embedding-informed approach to \ac{ASD} adapted to deal with the challenges of egocentric recordings, whilst assuming a parameter-efficient build. Experiments empirically demonstrate that the quality of the identity-speech library used is critical to performance. Future work may involve creating a learnt audiovisual correspondence between identity and speech when constructing the identity-speech library as opposed to using static thresholds, as well as investigating the possibility of a light-weight speaker recognition module specifically for egocentric recordings.



\bibliographystyle{IEEEtran}
\bibliography{mybib}


% \vspace{12pt}
% \color{red}
% IEEE conference templates contain guidance text for composing and formatting conference papers. Please ensure that all template text is removed from your conference paper prior to submission to the conference. Failure to remove the template text from your paper may result in your paper not being published.

\end{document}
