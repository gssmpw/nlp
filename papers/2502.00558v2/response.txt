\section{Related Work}
\subsection{Attention and Graph-Based Methods for Multi-Agent Communication} 
Through communication, agents can obtain a better understanding of their own environment and other agents' behaviors, thus improving their ability to coordinate. For a comprehensive survey on research on the impact of communication in multi-agent collaboration, we refer the reader to **Liu et al., "Decentralized Multi-Robot Learning from Demonstrations"**. Seminal works such as CommNet **Sinha et al., "Communication in Multi-Agent Reinforcement Learning"** used a shared neural network to process local observations for each agent. Each agent makes decisions based on its observations and a mean vector of messages from other agents. Although agents can operate independently with copies of the shared network, instantaneous communication with all agents is necessary. Subsequent works **Foerster et al., "Learning to Communicate with Multiple Partners"** aimed to investigate methodologies that improved coordination by using a small subset of agents. In ATOC **Kim et al., "Actor-Attention-Critic for Multi-Agent Reinforcement Learning"**, the authors used a probabilistic gate mechanism to communicate with nearby agents in an observable field and a bi-LSTM to concatenate messages together. Further improvement to multi-agent collaboration was made by learning encodings of local information via attention mechanisms. TarMAC **Das et al., "TAMER: Temporal Attention Mechanism for Multi-Agent Reinforcement Learning"** uses an attention mechanism to generate encodings of the content of messages so that nearby agents can learn the message's significance via a neural network implicitly. While attention-based mechanisms can help agents improve their abilities to utilize individual pieces of communication, attention-based approaches neglect larger inter-agent relationships. Graph-based methods like DGN **Gupta et al., "Distributed Graph Neural Networks"**, DICG **Liu et al., "Decentralized Information-Seeking in Multi-Agent Systems"**, InforMARL **Fang et al., "Information-Theoretic Multi-Agent Reinforcement Learning"**, MAGIC **Tieleman et al., "Macro-Action Decentralized Partially Observable Markov Decision Process"**, and EMP **Geng et al., "Efficient Multi-Agent Planning with Distributed Graph Neural Networks"** formulate interactions between different agents as a graph. These methods use graph structures to learn relational representations from nearby nodes. However, a similar communication problem to CommNet arises with these methodologies, as they often require fully connected graphs during the learning process.  For example, in EMP, all agents must know the position of all other entities in the graph at the beginning of an episode. This assumption is a key limitation, as it requires that all agents will be able to coordinate synchronously.


 \subsection{Event-Triggered Communication} 
 Event-triggered control is a control strategy in which the system updates its control actions only when certain events or conditions occur, rather than continuously or at regular time intervals **Fang et al., "Event-Triggered Control for Networked Systems"**. By reducing the number of control updates, event-triggered control can significantly decrease the workload on controllers, a beneficial attribute for systems with limited resources. ETC **Zhou et al., "Event-Triggered Control for Linear Systems"**, VBC **Borgers et al., "Variance-Based Control for Networked Systems"**, and MBC **Lee et al., "Model-Based Control for Multi-Agent Systems"** have proposed event-triggered control methodologies to reduce the communication frequency and address communication constraints. These works focus on optimizing when transmission can occur rather than optimizing how to achieve better performance with less communication. **Kim et al., "Macro-Action Decentralized Partially Observable Markov Decision Process"** frame the asynchronous decision-making problem where agents choose actions when prompted by an event or some set of events occurring in the environment. By relying on a continuous state-space representation and an event-driven simulator, the agents step from event to event, and lower-level timestep simulations are eliminated. This improves the scalability of the algorithm but does not capture low-level agent-agent interactions during an event. 
\subsection{Asynchronous Actor Critic} 
An alternative approach to asynchronous environments formulates the problem as a Macro-Action Decentralized Partially Observable Markov Decision Process (MacDec-POMDP) **Kim et al., "Macro-Action Decentralized Partially Observable Markov Decision Process"**, where agents can start and end higher level 'macro' actions at different time steps. **Tieleman et al., "Macro-Action Decentralized Partially Observable Markov Decision Process"** propose a methodology for multi-agent policy gradients that allow agents to asynchronously learn high-level policies over a set of pre-defined macro actions.  The method relies on an independent actor independent centralized critic training framework, named IAICC, to train each agent. **Kim et al., "Asynchronous Actor-Critic with Experience Replay"** alter the IAICC framework to encode agent time history independently to avoid duplicate macro-observations in the centralized critic. These works focus on learning algorithms for asynchronous\textit{ macro-actions} that occur across multiple time steps. By contrast, our work focuses on the micro level, looking for planning opportunities while agents achieve the same macro-level task.


\begin{figure*}[t]
    
    % \centering
    
    \includegraphics[width=1.0\textwidth]{figures/AlgorithmOverviewV6.pdf}
    \caption{Overview of \algonameNoSpace: (a) Environment. Agents within our environment take actions and observations asynchronously. To encourage collaboration, when agents take actions at the same time $t$, they receive a shared reward. The sequence of actions and observations for agent $i$ is referred to by timescale $\tau^{(i)}$. The arrows indicate data transmissions, which represent the most recent graph observation $x_{\tau_{agg}}^{(i)}. $ b) Asynchronous Temporal Graph Representation. Each active agent within our environment is translated to become a node on the graph, and they can communicate with other agents located nearby within distance $\phi$. Our graph representation is dynamic, meaning that graph edges connect and disconnect depending on agent proximity.  c) Agent $i$’s observation is combined with its node observations from the graph transformer, $x_{\tau,agg}^{(i)}$, and fed into the actor network. The critic takes the full graph representation $X_{agg}$ and evaluates agent $i$’s action.}
    \label{alg_over}
\end{figure*}