\section{Related Work}
\subsection{Attention and Graph-Based Methods for Multi-Agent Communication} 
Through communication, agents can obtain a better understanding of their own environment and other agents' behaviors, thus improving their ability to coordinate. For a comprehensive survey on research on the impact of communication in multi-agent collaboration, we refer the reader to \cite{comm_survey}. Seminal works such as CommNet \cite{CommNet} used a shared neural network to process local observations for each agent. Each agent makes decisions based on its observations and a mean vector of messages from other agents. Although agents can operate independently with copies of the shared network, instantaneous communication with all agents is necessary. Subsequent works \cite{IC3Net, ATOC} aimed to investigate methodologies that improved coordination by using a small subset of agents. In ATOC \cite{ATOC}, the authors used a probabilistic gate mechanism to communicate with nearby agents in an observable field and a bi-LSTM to concatenate messages together. Further improvement to multi-agent collaboration was made by learning encodings of local information via attention mechanisms. TarMAC \cite{TarMAC} uses an attention mechanism to generate encodings of the content of messages so that nearby agents can learn the message's significance via a neural network implicitly. While attention-based mechanisms can help agents improve their abilities to utilize individual pieces of communication, attention-based approaches neglect larger inter-agent relationships. Graph-based methods like DGN~\cite{DGN}, DICG~\cite{DICG}, InforMARL \cite{informarl_icml}, MAGIC \cite{magic}, and EMP~\cite{EMP} formulate interactions between different agents as a graph. These methods use graph structures to learn relational representations from nearby nodes. However, a similar communication problem to CommNet arises with these methodologies, as they often require fully connected graphs during the learning process.  For example, in EMP, all agents must know the position of all other entities in the graph at the beginning of an episode. This assumption is a key limitation, as it requires that all agents will be able to coordinate synchronously.


 \subsection{Event-Triggered Communication} 
 Event-triggered control is a control strategy in which the system updates its control actions only when certain events or conditions occur, rather than continuously or at regular time intervals \cite{ETC_descript}. By reducing the number of control updates, event-triggered control can significantly decrease the workload on controllers, a beneficial attribute for systems with limited resources. ETC~\cite{ETC}, VBC~\cite{VBC}, and MBC~\cite{MBC} have proposed event-triggered control methodologies to reduce the communication frequency and address communication constraints. These works focus on optimizing when transmission can occur rather than optimizing how to achieve better performance with less communication. \citet{Menda_2019} frame the asynchronous decision-making problem where agents choose actions when prompted by an event or some set of events occurring in the environment. By relying on a continuous state-space representation and an event-driven simulator, the agents step from event to event, and lower-level timestep simulations are eliminated. This improves the scalability of the algorithm but does not capture low-level agent-agent interactions during an event. 
\subsection{Asynchronous Actor Critic} 
An alternative approach to asynchronous environments formulates the problem as a Macro-Action Decentralized Partially Observable Markov Decision Process (MacDec-POMDP) \cite{macpomdp}, where agents can start and end higher level 'macro' actions at different time steps. \citet{IAICC} propose a methodology for multi-agent policy gradients that allow agents to asynchronously learn high-level policies over a set of pre-defined macro actions.  The method relies on an independent actor independent centralized critic training framework, named IAICC, to train each agent. \citet{AOCC}  alter the IAICC framework to encode agent time history independently to avoid duplicate macro-observations in the centralized critic. These works focus on learning algorithms for asynchronous\textit{ macro-actions} that occur across multiple time steps. By contrast, our work focuses on the micro level, looking for planning opportunities while agents achieve the same macro-level task.


\begin{figure*}[t]
    
    % \centering
    
    \includegraphics[width=1.0\textwidth]{figures/AlgorithmOverviewV6.pdf}
    \caption{Overview of \algonameNoSpace: (a) Environment. Agents within our environment take actions and observations asynchronously. To encourage collaboration, when agents take actions at the same time $t$, they receive a shared reward. The sequence of actions and observations for agent $i$ is referred to by timescale $\tau^{(i)}$. The arrows indicate data transmissions, which represent the most recent graph observation $x_{\tau_{agg}}^{(i)}. $ b) Asynchronous Temporal Graph Representation. Each active agent within our environment is translated to become a node on the graph, and they can communicate with other agents located nearby within distance $\phi$. Our graph representation is dynamic, meaning that graph edges connect and disconnect depending on agent proximity.  c) Agent $i$’s observation is combined with its node observations from the graph transformer, $x_{\tau,agg}^{(i)}$, and fed into the actor network. The critic takes the full graph representation $X_{agg}$ and evaluates agent $i$’s action.}
    \label{alg_over}
\end{figure*}