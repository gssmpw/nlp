\section{Related Work}
\subsection{Heterophilous Graph Neural Networks}  
Previous research has extensively studied the graph heterogeneity matching problem. Some approaches, like **Zhang et al., "Graph Heterogeneous Matching Network"**, aggregate information from first- and second-hop neighbors based on spectral graph theory. **Lee et al., "Graph Propagation Regularized Graph Neural Network"** adaptively optimizes generalized PageRank weights to control layer-wise propagation contributions. **Wang et al., "Flexible Aggregation Graph Convolutional Networks"** and **Liu et al., "Adaptive Channel Multi-Head Attention Graph Convolutional Networks"** use multi-channel filters to capture local information. However, these methods struggle to adapt to complex heterogeneous feature distributions, limiting their performance in heterogeneous scenarios. **Zhou et al., "Exploring Transformer for Graph Neural Networks"** features a sparse graph Transformer architecture, tackling the scalability issues of traditional graph Transformers on large datasets.  Utilizing virtual global nodes and expander graphs, it enables a sparse attention mechanism with linear complexity, boosting scalability remarkably.
**Chen et al., "Difformer: A Diffusion-Based Model for Graph Neural Networks"** offers a new neural network architecture for modeling intricate data instance dependencies.  It uses an energy-constrained diffusion model to encode instances as evolving states step by step, optimizing a regularized energy function to find the best diffusion strength and realize globally consistent representation learning.

 Spatial-domain methods improve heterogeneous scene representation by adjusting graph structures and refining message-passing rules. **Huang et al., "LinkX: A Link Aggregation Method for Graph Neural Networks"** aggregates the adjacency matrix and node features for representation learning, while **Li et al., "Graph Localized Graph Neural Network"** introduces a node correlation matrix for global information. **Fan et al., "Flexible Spatial Graph Neural Network"** adaptively aggregates neighbor information from different hops. However, these approaches require computing new topologies or similarity matrices, leading to high computational and memory costs. 

As a type of specialized GNN, the Graph Transformer(GT) uses global self-attention to capture higher-order homogeneous information, improving target node representation. **Xu et al., "Adaptive Neighbor Sampling Graph Transformer"** captures long-range dependencies via adaptive node sampling and hierarchical attention. **Zhu et al., "Node Aggregation Graph Former"** handles large graphs by treating nodes as feature sequences from neighbors at various hops. **Wang et al., "Simplified Global Attention Mechanism for Graph Transformers"** simplifies this with a single-layer global attention mechanism, achieving linear complexity for large graphs. However, global self-attention may introduce irrelevant global noise, limiting performance on homogeneous graphs. Additionally, the high computational complexity of self-attention restricts GT’s scalability in practical applications.


\subsection{MoE for GNNs}
% 混合专家（Mixture-of-Experts, MoE）机制作为一种增强图神经网络（GNN）适应性的重要方法，在处理具有不同同质性水平的图结构数据方面展现了巨大潜力。[NCGNN]提出了邻域混乱度（NC）指标，引导模型针对不同目标实施“分而治之”的策略。从 MoE 的视角来看，NCGNN 将 NC 指标用作门控机制，以分离学习架构作为专家网络，并通过设定阈值来实现负载均衡，从而在一定程度上减轻了偏差问题。[GMoE]将不同 hop size 的图卷积作为聚合专家，同时通过辅助损失函数实现负载均衡。[Mowst]通过将轻量级的多层感知机作为弱专家和图神经网络作为强专家相结合，并引入基于弱专家预测结果离散度的“置信度”机制，以适应不同目标节点的专家协作。
Recent advances in graph representation learning have witnessed the growing adoption of the MoE paradigm to enhance the adaptability of GNNs. A seminal work by **Chen et al., "Neighborhood Confusion Graph Neural Network"** proposed the Neighborhood Confusion (NC) metric to quantify structural heterogeneity within node neighborhoods, establishing a separated learning strategy through an MoE framework. Their approach employs the NC metric as a gating mechanism to route nodes to specialized expert networks while maintaining load balance via threshold-based regularization, effectively addressing potential bias in expert utilization. Building upon this foundation, **Zhou et al., "Graph Multi-Expert Network"** advanced the field by integrating multi-scale graph convolutions with varying receptive fields as aggregation experts, complemented by an auxiliary loss function to ensure balanced expert participation across diverse structural patterns. Further extending the MoE paradigm, **Liu et al., "Mixture-of-Experts for Weak and Strong Graph Neural Networks"** introduced a hierarchical expert architecture that combines lightweight Multi-Layer Perceptrons (MLPs) as weak experts with GNNs as strong experts, dynamically coordinating their collaboration through a confidence-aware mechanism based on prediction variance. While these approaches have demonstrated promising results in adapting to graph heterogeneity, limitations persist in the flexibility of expert specialization and the granularity of gating mechanisms, particularly in complex real-world graphs with diverse homophily patterns. These insights form the theoretical foundation and technical motivation for our work.