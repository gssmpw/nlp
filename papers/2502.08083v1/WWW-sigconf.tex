%%
%% This is file `sample-sigconf.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,sigconf')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[sigconf]{acmart}
% \documentclass[sigconf,anonymous,review]{acmart}

% \usepackage{times}  % DO NOT CHANGE THIS
% \usepackage{graphicx} % DO NOT CHANGE THIS

\usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}

\usepackage{newfloat}
\usepackage{listings}

\usepackage{bm}
% \usepackage{amsfonts}
% \usepackage{amsmath}
\usepackage{multirow}
\usepackage{arydshln}
\usepackage{hhline}
\usepackage{rotating}
\usepackage{diagbox}
\usepackage{makecell}
% \usepackage{ulem}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{enumitem}
% \usepackage{ifthen}
% \usepackage{amssymb}
% \newlist{checklist}{itemize}{1}
% \newcommand{\checkbox}[1]{\ifthenelse{\equal{#1}{true}}{$\boxtimes$}{$\square$}}
% \setlist[checklist]{label=\checkbox{false}}


\usepackage{xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\eg}{e.g.,\xspace}
\newcommand{\ie}{i.e.,\xspace}
\newcommand{\etc}{etc.}
\newcommand{\blue}[1]{\textcolor{blue}{#1}}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\newcommand{\green}[1]{\textcolor{teal}{#1}}
\newcommand{\cyan}[1]{\textcolor{cyan}{#1}}

% \newcommand{\graph}{HAMG\xspace}
\newcommand{\model}{\emph{GNNMoE}\xspace}
\newcommand{\addcite}{\textcolor{red}{[add ref]}\xspace}


%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2025}
\acmYear{2025}
\acmDOI{XXXXXXX.XXXXXXX}

%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[KDD'25]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{August 3-7, 2025}{Toronto, ON, Canada}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Mixture of Decoupled Message Passing Experts with Entropy Constraint for General Node Classification}
%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.

% \author{Xuanze Chen}
% \affiliation{%
%   \institution{Zhejiang University of Technology}
%   \city{Hangzhou}
%   \country{China}}
% \email{chenxuanze@zjut.edu.cn}

\author{Xuanze Chen}
\authornote{Both authors contributed equally to this research.}
% \email{chenxuanze@zjut.edu.cn}
\author{Jiajun Zhou}
\authornotemark[1]
\authornote{Corresponding Author.}
\affiliation{%
  \institution{Zhejiang University of Technology}
  \city{Hangzhou}
  \country{China}}
  \email{{chenxuanze, jjzhou}@zjut.edu.cn}

% \author{Jiajun Zhou}
% \authornote{Corresponding Author.}
% \affiliation{%
%   \institution{Zhejiang University of Technology}
%   \city{Hangzhou}
%   \country{China}}
% \email{jjzhou@zjut.edu.cn}

\author{Jinsong Chen}
\affiliation{%
  \institution{Huazhong University of Science and Technology}
  \city{Wuhan}
  \country{China}}
\email{chenjinsong@hust.edu.cn}


\author{Shanqing Yu}
\affiliation{%
  \institution{Zhejiang University of Technology}
  \city{Hangzhou}
  \country{China}}
\email{yushanqing@zjut.edu.cn}


\author{Qi Xuan}
\affiliation{%
  \institution{Zhejiang University of Technology}
  \city{Hangzhou}
  \country{China}}
\email{xuanqi@zjut.edu.cn}


% Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou 310023, China

% \author{Xuanze Chen}
% % \authornote{Both authors contributed equally to this research.}
% \email{trovato@corporation.com}
% \orcid{1234-5678-9012}
% \author{G.K.M. Tobin}
% \authornotemark[1]
% \email{webmaster@marysville-ohio.com}
% \affiliation{%
%   \institution{Institute for Clarity in Documentation}
%   \city{Dublin}
%   \state{Ohio}
%   \country{USA}
% }

% \author{Lars Th{\o}rv{\"a}ld}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{larst@affiliation.org}

% \author{Valerie B\'eranger}
% \affiliation{%
%   \institution{Inria Paris-Rocquencourt}
%   \city{Rocquencourt}
%   \country{France}
% }

% \author{Aparna Patel}
% \affiliation{%
%  \institution{Rajiv Gandhi University}
%  \city{Doimukh}
%  \state{Arunachal Pradesh}
%  \country{India}}

% \author{Huifen Chan}
% \affiliation{%
%   \institution{Tsinghua University}
%   \city{Haidian Qu}
%   \state{Beijing Shi}
%   \country{China}}

% \author{Charles Palmer}
% \affiliation{%
%   \institution{Palmer Research Laboratories}
%   \city{San Antonio}
%   \state{Texas}
%   \country{USA}}
% \email{cpalmer@prl.com}

% \author{John Smith}
% \affiliation{%
%   \institution{The Th{\o}rv{\"a}ld Group}
%   \city{Hekla}
%   \country{Iceland}}
% \email{jsmith@affiliation.org}

% \author{Julius P. Kumquat}
% \affiliation{%
%   \institution{The Kumquat Consortium}
%   \city{New York}
%   \country{USA}}
% \email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Chen et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  % 真实世界图呈现的不同程度的同质和异质性，始终限制着用于节点分类的图神经网络的通用性。本文以数据为中心的视角展开，揭示了不同图对不同消息编码方式的固有偏好：同质图偏好局部传播，而异质图青睐传播和转换的灵活组合。对此，我们进一步提出了一种基于混合专家机制的通用节点分类框架，\model。该框架首先通过重组细粒度的编码算子来构造不同的消息传递专家，然后设计软、硬门控层为每个节点分配最合适的专家网络来进行表征学习，以此提高模型的表达能力和对多样化图的适应性。此外，考虑到软门控机制可能会在同质场景下引入编码噪声，我们进一步引入了熵约束来引导软门控锐化，实现加权与topk选择的有机结合。大量实验表明，\model框架在节点分类性能和通用性上显著优于主流的GNN、异质图神经网络、graph transformer等模型。
  The varying degrees of homophily and heterophily in real-world graphs persistently constrain the universality of graph neural networks (GNNs) for node classification. Adopting a data-centric perspective, this work reveals an inherent preference of different graphs towards distinct message encoding schemes: homophilous graphs favor local propagation, while heterophilous graphs exhibit preference for flexible combinations of propagation and transformation. To address this, we propose \model, a universal node classification framework based on the Mixture-of-Experts (MoE) mechanism. The framework first constructs diverse message-passing experts through recombination of fine-grained encoding operators, then designs soft and hard gating layers to allocate the most suitable expert networks for each node's representation learning, thereby enhancing both model expressiveness and adaptability to diverse graphs. Furthermore, considering that soft gating might introduce encoding noise in homophilous scenarios, we introduce an entropy constraint to guide sharpening of soft gates, achieving organic integration of weighted combination and Top-K selection. Extensive experiments demonstrate that \model significantly outperforms mainstream GNNs, heterophilous GNNs, and graph transformers in both node classification performance and universality across diverse graph datasets.

  % Graph Neural Networks have made significant progress in representation learning for graphs, but still face challenges in handling heterophilous information and long-range dependencies. To address these issues, Graph Transformers have shown advantages in capturing long-range dependencies and processing heterophilous graphs through self-attention mechanisms, but their computational complexity and global noise issues continue to limit their application on large-scale graphs. To overcome these limitations, we propose a universal node classification model architecture—\model. This architecture decouples the message passing processes, flexibly combines different types of fine-grained operations, and incorporates a mixture of experts mechanism, improving the model's expressiveness and adaptability by assigning the most appropriate expert network weights to each node through adaptive architecture search. Furthermore, drawing inspiration from the residual connections and feed-forward network (FFN) design in Transformers, we introduce adaptive residual connections and enhanced FFN modules in \model to further improve node classification performance. Extensive experimental results demonstrate that \model performs excellently across various types of graph data, effectively avoiding over-smoothing problems and global noise, enhancing model robustness and adaptability, while ensuring computational efficiency on large-scale graphs.

\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
    <concept>
        <concept_id>10010147.10010257.10010293.10010294</concept_id>
        <concept_desc>Computing methodologies~Neural networks</concept_desc>
        <concept_significance>500</concept_significance>
        </concept>
  </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Neural networks}

%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Graph Neural Network, Node Classification, Mixture of Experts}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.


% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\section{Introduction}
% 真实世界中存在多样化的复杂交互场景，这些场景可以被抽象为各种图结构数据，如文献引用网络、社交网络、金融交易网络等。为了从这些场景中挖掘潜在信息并加以利用，研究者提出了多种图表征学习方法，其中最受瞩目的是图神经网络(GNNs)。图神经网络（Graph Neural Networks, GNNs）通过消息传递机制在节点分类任务（如欺诈检测、社交推荐）中取得了显著成效。然而，真实场景中图的特征与拓扑结构多样性，使得现有方法的通用性受限。
In the real world, there exist diverse and complex interaction scenarios, which can be abstracted as various graph-structured data, such as citation networks~\cite{liu2022deep}, social networks~\cite{fan2019graph}, financial transaction networks~\cite{zhou2022behavior}, and so on. To mine and utilize the potential information from these scenarios, researchers have proposed various graph representation learning methods, among which the most prominent is Graph Neural Networks (GNNs)~\cite{liu2022deep}. GNNs have achieved significant success in node classification tasks, such as fraud detection~\cite{Meta-IFD,zhou2022behavior} and social recommendations~\cite{wu2022graph}, through a message-passing mechanism. However, in real-world scenarios, the features and topological structures of graphs exhibit diversity, which limits the generalization ability of existing methods.

% 传统图卷积网络（GCN）采用权重共享设计，将邻域节点的信息传播视为等权聚合。这种同质化处理虽保证了计算效率，却难以应对节点影响力的差异性。例如欺诈检测场景中，目标节点的局部邻域可能混杂正常用户与恶意账户，简单的等权聚合会引入噪声干扰。尽管一些改进模型（如图注意力网络GAT、图Transformer）通过注意力机制区分邻居权重，但其核心仍遵循"先特征传播后非线性变换"（P→T）的固定架构。这种预设的消息传递流程存在本质局限：当异质图需要优先过滤噪声（T→P）或动态调整传播-变换顺序时，固定架构将导致模型表达能力受限。
% Traditional Graph Convolutional Networks (GCNs) use a weight-sharing design, treating the information propagation from neighboring nodes as an equally weighted aggregation. While this homogenized approach ensures computational efficiency, it struggles to address the variance in node influence. For instance, in fraud detection scenarios, a target node’s local neighborhood may consist of both normal users and malicious accounts, and simple equal-weight aggregation could introduce noise interference. Although some advanced models, such as Graph Attention Networks (GAT) and Graph Transformers, differentiate neighbor weights through attention mechanisms, their core still follows the fixed "feature propagation followed by nonlinear transformation" ($\boldsymbol{P \rightarrow T}$) architecture. This predefined message-passing flow presents inherent limitations: when dealing with heterophilous graphs that require prioritizing noise filtering ($\boldsymbol{T \rightarrow P}$) or dynamically adjusting the propagation-transformation order, the fixed architecture restricts the model's expressive power.

Traditional Graph Convolutional Networks (GCNs)~\cite{GCN} adopt a weight-sharing design, treating information propagation from neighboring nodes as an equal-weight aggregation. While this homogenized processing ensures computational efficiency, it struggles to handle the variance in node influence. For instance, in fraud detection, the local neighborhood of a target node may mix normal users with malicious accounts, and simple equal-weight aggregation introduces noise interference. Although some improved models, such as Graph Attention Networks (GAT)~\cite{GAT} and Graph Transformer~\cite{GT}, distinguish neighbor weights through attention mechanisms, their core still follows the fixed architecture of ``feature propagation (P) followed by nonlinear transformation (T)'' ($\boldsymbol{P \rightarrow T}$). This predefined message-passing process has inherent limitations: when heterophilous graphs need to prioritize noise filtering or dynamically adjust the propagation-transformation order, the fixed architecture constrains the model's expressive power.

\begin{figure}[!htb]
  \centering
  \includegraphics[width=1.0\linewidth]{Fig-1-obs1.pdf}
  \caption{Observation experiment 1. Preference for encoding strategies in node classification across different graphs.}
  \label{fig: obs1}
  % \vspace{-10pt} % 根据需要调整负值
\end{figure}
% 为突破上述限制，近期研究提出将消息传递机制解耦为特征传播（Propagation, P）与非线性变换（Transformation, T）的灵活组合。从解耦视角分析，主流GNN架构可归结为P和T算子的不同堆叠模式：GCN遵循PT模式以捕获局部平滑性，GPRGNN通过TP模式优先过滤高频噪声，而MLP则退化为纯变换的TT模式。如图1所示，我们在12个基准数据集上的实验进一步揭示了关键现象：同质图（如Computers，Photo，Coauthor CS，Coauthor Physics，Wiki-CS）倾向于依赖含P操作的编码器（PT/TP），因其结构一致性允许直接邻域传播；而异质图（如Actor，Chameleon-fix，Squirrel-fix，Tolokers，Roman-empire）对编码方式无显著偏好，需根据节点局部特征与拓扑动态调整PT组合。这一现象表明，节点分类性能的差异本质上源于目标节点对编码方式的差异化需求——位于同质子图的节点需PT捕获局部特征平滑性，而处于异质边界的节点则需TP先滤除跨社区噪声。
% To overcome the aforementioned limitations, recent research has proposed decoupling the message-passing mechanism into a flexible combination of feature propagation (\textbf{P}ropagation, \textbf{P}) and nonlinear transformation (\textbf{T}ransformation, \textbf{T}). From the perspective of this decoupling, mainstream GNN architectures can be categorized into different stacking patterns of the P and T operators: GCN follows the PT pattern to capture local smoothness, GPRGNN adopts the TP pattern to prioritize high-frequency noise filtering, and MLP degenerates into a pure transformation TT pattern. As shown in Figure ~\ref{fig: obs1}, experiments on 12 benchmark datasets further reveal key phenomena: homophilous graphs tend to rely on encoders with P operations (\textbf{PT/TP}), as their structural consistency allows for direct neighborhood propagation; whereas heterophilous graphs show no significant preference for encoding methods and require dynamic adjustment of the \textbf{PT} combination based on local node features and topology. This phenomenon suggests that the performance differences in node classification essentially arise from the differential demands of the target nodes for encoding methods—nodes within homophilous subgraphs require PT to capture local feature smoothness, while nodes at heterophilous boundaries need TP to first filter out cross-community noise.

To overcome these limitations, recent studies~\cite{SGC,SIGN} have proposed decoupling the message-passing mechanism into a flexible combination of feature propagation (P) and nonlinear transformation (T). From the decoupling perspective, mainstream GNN architectures can be reduced to different stacking patterns of P and T operators: GCN follows the PT pattern to capture local smoothness, GPRGNN~\cite{GPRGNN} prioritizes high-frequency noise filtering through the TP pattern, while MLP degenerates into the pure transformation TT pattern. Our observation experiments on 12 benchmark datasets further reveal key phenomena, as shown in Figure~\ref{fig: obs1}\footnote{The observation model consists of three main components: (1) \textbf{Input Layer}: A linear layer is employed to project all node features into a 64-dimensional space; (2) \textbf{Encoder}: Four distinct types of PT Blocks are available, and one of them is selected to learn node representations; (3) \textbf{Classifier}: This component is responsible for the final node classification task. All experiments are conducted in accordance with the settings and hyperparameter search range outlined in Section 4, and the best results are presented.}: homophilous graphs tend to rely on encoders with P operations (PT/TP) because their structural consistency allows for more direct and effective information propagation between neighbors; whereas heterophilous graphs show no significant preference for encoding methods, requiring dynamic adjustment of the PT combination based on local node features and topology. This phenomenon suggests that the differences in node classification performance stem fundamentally from the differentiated needs of target nodes for encoding methods — nodes within homophilous subgraphs require feature propagation to capture local feature smoothness, while nodes at heterogeneous boundaries require transformations to filter out cross-domain noise.

% 由此引出一个核心问题：能否为每个目标节点动态适配其最优的消息传递方式，以实现细粒度的编码器定制？现有方案可分为两类：其一通过参数冗余化设计超大模型覆盖所有模式，但面临过拟合与计算成本高昂的缺陷；其二采用启发式分组策略，如基于邻域混乱度指标[Zhou等人]或混合专家机制[GMoE]，然而前者依赖迭代计算导致效率低下，后者受限于专家类型单一性（仅支持GCN/GIN），难以覆盖多样化的PT组合需求。
% This leads to a core question: \textit{Is it possible to dynamically adapt the optimal message-passing strategy for each target node to achieve fine-grained encoder customization?} Existing approaches can be divided into two categories: The first approach involves designing ultra-large models through parameter redundancy to cover all modes, but it faces the challenges of overfitting and high computational costs. The second adopts heuristic grouping strategies, such as those based on neighborhood confusion metrics ~\cite{NCGNN} or the mixture of experts mechanism ~\cite{GMoE}. However, the former relies on iterative computations, leading to inefficiency, while the latter is limited by the restricted expert types, which only support GCN/GIN, making it difficult to meet the diverse PT combination needs.

This raises a core question: \emph{Can we dynamically adapt the optimal message-passing strategy for each target node to achieve fine-grained encoder customization?} Existing solutions can be classified into two categories: the first involves designing ultra-large models with parameter redundancy to cover all patterns, but faces issues of overfitting and high computational cost; the second employs heuristic grouping strategies, such as using neighborhood confusion metrics~\cite{NCGNN} or a mixture-of-experts mechanism~\cite{GMoE}. However, the former relies on iterative updates of the metrics, resulting in low computational efficiency, while the latter is limited by single type of expert, making it difficult to cover the diverse encoding needs.

% 针对上述局限性，我们设计了一种基于混合专家机制的通用节点分类框架——\model。该架构重组细粒度的消息传递模块，将重组得到的4个PT-blocks作为专家，每个专家都是一个独立的消息传递模块，拥有独立的可训练参数。然后，我们设计了软、硬门控机制帮助节点自适应的选择合适的专家，从而提高模型的表达能力和通用性。此外，鉴于软门控机制在同质场景下可能会导致编码噪声的引入，我们进一步引入了熵约束，以引导软门控机制实现更为精确的选择，从而有效地将加权与top-k选择结合起来。通过大量实验验证，\model框架在多种类型的图数据上表现出一致的优越性。在同质图和异质图中，\model能够有效为节点选择合适的聚合专家，取得优异的效果，展现出更强的鲁棒性和通用性。
% In this work, we propose a general node classification framework, \model, based on the mixture of experts mechanism. This architecture reorganizes fine-grained message-passing modules into four PT-blocks, each serving as an independent expert with its own trainable parameters. We design both soft and hard gating mechanisms to enable nodes to adaptively select the most suitable expert, enhancing the model’s expressiveness and versatility. To further improve the gating mechanism’s adaptability across different scenarios, we introduce an entropy constraint to refine the soft gating mechanism, combining weighted and Top-K selection strategies. Extensive experiments show that \model outperforms other models across diverse graph data types, effectively selecting the appropriate experts for each node and demonstrating robust generalization capabilities.

Inspired by existing research, we propose a mixture of decoupled message-passing expert framework for general node classification, named \model. This framework first reorganizes the P and T operators to form four types of message-passing experts, which constitute the expert network. A soft and hard gating mechanism is then designed to assist nodes in adaptively selecting the appropriate encoding strategies for representation learning. Additionally, considering that the soft gating mechanism might introduce encoding noise in homophilous scenarios, we further design an entropy-guided soft gating sharpening mechanism to enable more precise expert selection. Extensive experiments on 12 widely-used benchmark datasets demonstrate that the \model framework achieves state-of-the-art performance on various types of graph data, exhibiting strong generalization ability for node classification. The main contributions can be summarized as follows:
\begin{itemize}[leftmargin=10pt]
    \item We propose a novel framework that decouples propagation (P) and transformation (T) operations to construct four types of message-passing experts. By integrating soft and hard gating mechanisms, our approach enables node-level adaptive expert selection, providing fine-grained encoding strategies for graphs with varying homophily and heterophily levels.
    \item We introduce an entropy-guided regularization mechanism to optimize the gating selection. This approach adaptively balances weighted combinations and Top-K selection, effectively mitigating noise interference in homophilous graphs while enhancing flexibility in heterophilous scenarios.
    \item Extensive experiments were conducted on 12 benchmark datasets. The results demonstrate that GNNMoE significantly outperforms mainstream Graph Neural Networks (GNNs), heterophilous GNNs, and graph transformers in terms of node classification performance and generalization. 
\end{itemize}

\section{Preliminaries}
\subsection{Notations}
A graph is denoted as $G = (V, E, \boldsymbol{X}, \boldsymbol{Y})$, where $V$ and $E$ are the set of nodes and edges respectively, $\boldsymbol{X} \in \mathbb{R}^{|V| \times d}$ is the node feature matrix, and $\boldsymbol{Y} \in\mathbb{R}^{|V| \times C}$ is the node label matrix. 
Here we use $|V|$, $d$ and $C$ to denote the number of nodes, the dimension of the node features, and the number of classes, respectively. The graph topology information $(V, E)$ can also be denoted by an adjacency matrix $\boldsymbol{A} \in \mathbb{R}^{|V| \times |V|}$, where $\boldsymbol{A}_{ij}=1$ indicates the existence of an edge between $v_i$ and $v_j$, and $\boldsymbol{A}_{ij}=0$ otherwise. 
Node classification is a fundamental task in graph machine learning, and it involves assigning labels to the nodes of a graph based on their features and the graph topology structure.

\subsection{Disentanglement for GNN Architecture}
Currently, most existing GNNs follow a unified message-passing framework~\cite{MPNN}, in which the message passing phase is decomposed into three processes: message generation, aggregation, and node feature update.
\begin{equation}
  \begin{aligned}
       \textbf{message:~}& \boldsymbol{m}_{i\leftarrow j}^{(l)}=\text{MES}^{(l)}\left(\boldsymbol{h}_j^{(l-1)}, \boldsymbol{h}_i^{(l-1)}, \boldsymbol{e}_{ji}\right)\\
       \textbf{aggregation:~}& \boldsymbol{m}_i^{(l)}=\text{AGG}\left(\left\{\boldsymbol{m}_{i\leftarrow j}^{(l)}\mid j\in\mathcal{N}(i) \right\}\right)\\
       \textbf{update:~}& \boldsymbol{h}_i^{(l)}=\text{UPD}^{(l)}\left(\boldsymbol{h}_i^{(l-1)}, \boldsymbol{m}_i^{(l)}\right)\\
  \end{aligned}
\end{equation}
where $\boldsymbol{m}_{i\leftarrow j}^{(l)}$ denotes the message sent from node $v_j$ to $v_i$ at iteration step $l$, and depends on the feature $\boldsymbol{h}_j^{(l-1)}$ of the sending node, the feature $\boldsymbol{h}_i^{(l-1)}$ of the receiving node and the feature $\boldsymbol{e}_{ji}$ of edge between them. The message function $\text{MES}^{(l)}$ can be a parameterized model such as MLPs. $\text{AGG}$ is the aggregation function, such as summation, averaging and maximum, which is used to aggregate messages from the neighborhood $\mathcal{N}(i)$ of the target node $v_i$. The update function $\text{UPD}^{(l)}$ can be a neural network that integrates the current node state and the aggregated messages to produce a new node state. 
From a more decoupled perspective, the message-passing phase can be decomposed into two functionally independent operations: Propagation and Transformation~\cite{PT}.
\begin{equation}
    \begin{aligned}
         &\textbf{propagation:~} \boldsymbol{h}_i^{(l)} = \mathbf{P}\left(\boldsymbol{h}_i^{(l-1)}, \left\{\boldsymbol{h}_{j}^{(l-1)}, \boldsymbol{e}_{ji}\mid j\in\mathcal{N}(i) \right\}\right)\\
         &\textbf{transformation:~} \boldsymbol{h}_i^{(l)}=\mathbf{T}\left(\boldsymbol{h}_i^{(l)}\right)\\
    \end{aligned}
\end{equation}
where $\mathbf{P}$ is the propagation function that combines message generation and aggregation from neighbor node $v_j$ to target node $v_i$. $\mathbf{T}$ performs a non-linear transformation on the state of the nodes after propagation.
% 在PT分离的基础上,现存的GNN架构可以根据对传播和转换操作的堆叠顺序被粗略的,不严格地划分为PTPT,PPTT,TTPP和TPTP四种类型,如表1所示。
Based on the disentanglement, existing GNN architectures can be roughly and loosely categoried into four types according to the stacking order of propagation and transformation operations: $\mathbf{PTPT}$, $\mathbf{PPTT}$, $\mathbf{TTPP}$, and $\mathbf{TPTP}$, as listed in Table~\ref{tab: PT}.
\begin{table}[!htb]
  \centering
  \caption{Disentanglement for existing model architectures.}
  \label{tab: PT}
  \renewcommand\arraystretch{1.2}
  \resizebox{\linewidth}{!}{
  \begin{tabular}{c|c|c|c} 
  \hline
  \multicolumn{2}{c|}{Catrgory} & Method                & Message Passing  \\ 
  \hline
  NN                   & $\mathbf{TT}$     & MLP, LINKX              & $\mathbf{T}(\mathbf{T}(\boldsymbol{X}))$          \\ 
  \hline
  \multirow{4}{*}{GNN} & $\mathbf{PTPT}$   & GCN, GAT, SAGE, FSGNN   & $\mathbf{T}(\mathbf{P}(\mathbf{T}(\mathbf{P}(\boldsymbol{X}))))$    \\ 
  \cline{2-4}
                       & $\mathbf{PPTT}$   & ACMGCN                  & $\mathbf{T}(\mathbf{T}(\mathbf{P}(\mathbf{P}(\boldsymbol{X}))))$    \\ 
  \cline{2-4}
                       & $\mathbf{TTPP}$   & GPRGNN, H2GCN  & $\mathbf{P}(\mathbf{P}(\mathbf{T}(\mathbf{T}(\boldsymbol{X}))))$    \\ 
  \cline{2-4}
                       & $\mathbf{TPTP}$   & FAGCN                   & $\mathbf{P}(\mathbf{T}(\mathbf{P}(\mathbf{T}(\boldsymbol{X}))))$    \\
  \hline
  \end{tabular}}
\end{table}


% \subsection{Graph Transformer for Node Classification}
% % GT作为一种变相的图神经网络被提出，用于缓解GNNs在处理过挤压、长距离依赖、弱连接等问题时的局限性。不失一般性，这vanilla GT的架构大致上沿袭经典Transformer，由两个重要模块构成：多头自注意力模块（MHA）和前馈网络，如图3所示。MHA首先将节点特征转化为查询向量、键向量和值向量，然后利用前两者来计算注意力分数，最后用得到的分数对值向量进行加权聚合：
% % FFN后接于MHA，通过线性和非线性变换来增强GT的表达能力。此外，残差连接被使用来防止模型加深的情况下出现梯度消失问题，后续跟着层归一化来稳定模型训练并加速收敛。
% The graph transformer, serving as a new graph representation learning architecture, is proposed to alleviate the limitations of GNNs in dealing with issues such as over-squashing~\cite{alon2020bottleneck}, long-range dependency~\cite{graphgps, exphormer}, weak connectivity~\cite{Nodeformer, Graphormer}, etc. Generally, the vanilla GT architecture broadly follows the classic Transformer~\cite{Transformer} and consists of two essential modules: a multi-head self-attention module (MHA) and a feed-forward network (FFN), as shown in Figure~\ref{fig: architecture}(a). The MHA module first transforms the node features into query vectors ($\boldsymbol{Q}$), key vectors ($\boldsymbol{K}$) and value vectors ($\boldsymbol{V}$), then performs the inner-product operation on the former two to compute the attention scores, and ultimately employs the obtained scores for weighted aggregation of the value vector:
% \begin{equation}
%     \begin{gathered}
%     \boldsymbol{Q}=\boldsymbol{H} \boldsymbol{W}_Q,\quad  
%     \boldsymbol{K}=\boldsymbol{H} \boldsymbol{W}_K,\quad 
%     \boldsymbol{V}=\boldsymbol{H} \boldsymbol{W}_V \\
%     \boldsymbol{H}^{\prime}=\operatorname{softmax}\left(\frac{\boldsymbol{Q} \boldsymbol{K}^{\top}}{\sqrt{d^{\prime}}}\right) \boldsymbol{V}
%     \end{gathered}
% \end{equation}
% where $\boldsymbol{H}=\left[\boldsymbol{h}_1^\top,\cdots,\boldsymbol{h}_{|V|}^\top\right]\in\mathbb{R}^{|V|\times d}$ denotes the input node embedding matrix, $\boldsymbol{W}_Q$, $\boldsymbol{W}_K$, $\boldsymbol{W}_V\in\mathbb{R}^{d\times d^{\prime}}$ are the projection matrics, $d^\prime$ is the output hidden dimension.
% The FFN follows the MHA to enhance the expressiveness of GT through linear and non-linear transformations. Additionally, residual connection~\cite{res} is employed to prevent the gradient vanishing as the model depth increases, followed by layer normalization~\cite{layernorm} to stabilize model training and accelerate convergence.
\begin{figure*}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{Fig-2-GNNMoE-line.pdf}
  \caption{Illustration of \model architectures.}
  \label{fig: architecture}
  % \vspace{-10pt} % 根据需要调整负值
\end{figure*}

\subsection{Misture of Experts for GNNs}
The Mixture of Experts (MoE) framework can be effectively integrated into Graph Neural Networks (GNNs) to address the inherent heterophily and complexity of graph-structured data. In this setup, a set of specialized expert networks, each tailored to capture distinct graph patterns, are combined through a dynamic gating mechanism. The gating network, conditioned on the graph context, assigns routing scores to determine the contribution of each expert. This allows the model to adaptively focus on relevant substructures, enhancing both performance and efficiency. The MoE mechanism can be formulated as:
\begin{equation}
  \hat{\boldsymbol{Y}} = \sum_{i=1}^{K} \boldsymbol{w}^\textit{i} \cdot \mathcal{E}_\textit{i}\left(\boldsymbol{A}, \boldsymbol{X}, \boldsymbol{W}_i\right), \quad
  \boldsymbol{w} = \operatorname{Gate}\left( \boldsymbol{X}\right)
\end{equation}
where $\boldsymbol{Y}$ is the final output, $\mathcal{E}_\textit{i}$ is the $i$-th expert network, $\boldsymbol{w}^\textit{i}$ is the routing weight, and $\operatorname{Gate}$ is the gating network. 
% The gating network, conditioned on the graph context, assigns routing scores to determine the contribution of each expert. This allows the model to adaptively focus on relevant substructures, enhancing both performance and efficiency. 
\section{Method: GNNMoE}

\subsection{General MoE Architecture}


% To achieve universal node classification across different graphs, we propose \model, as illustrated in Figure~\ref{fig: architecture}. This framework combines the advantages of both GTs and GNNs, consisting of stackable PT-Blocks and enhanced FFN. It takes node features and adjacency information as input and outputs the final node representations. The main highlights are reflected in the following aspects: 1) utilizing a soft gating mechanism to adaptively combine contributions from different message passing experts for each node, enabling flexible processing of different graph messages; 2) employing a hard gating mechanism to select appropriate activation layers, enhancing the expressiveness of FFN; 3) utilizing adaptive residual connections to improve adaptability to different data characteristics.
% To achieve universal and adaptive node classification across different types of graphs, we propose the \model framework, as shown in Figure~\ref{fig: architecture}. This framework not only breaks the weight-sharing assumption and decouples the message-passing mechanism, but also integrates both mechanisms effectively. Specifically, it consists of stackable PT-Blocks and an enhanced FFN. The framework takes node features and adjacency information as input and outputs the final node representations. The main highlights are as follows: 1) utilizing a soft gating mechanism to adaptively combine the contributions of different message-passing experts for each node, enabling flexible handling of information for different target nodes; 2) take advantage of entropy-constrained component guides the soft gating to dynamically assign the optimal expert combination for each node, achieving fine-grained heterophilous-aware representation learning; 3) employing a hard gating mechanism to select appropriate activation layers, enhancing the expressiveness of FFN; 4) apply adaptive residual connections to improve the model's ability to adapt to varying data features.

To achieve general node classification on different types of graphs, we propose \model framework, as shown in Figure~\ref{fig: architecture}. Specifically, \model consists of stackable PT-Blocks and an enhanced feed-forward network (FFN), which takes node features and adjacency information as input and outputs the final node representations. The main highlights are as follows: 1) The soft gating mechanism adaptively combines the contributions of different message-passing experts for each node, enabling flexible handling of information from different target nodes; 2) The entropy-constrained gating sharpening mechanism guides the soft gating to more precisely allocate encoding experts to different nodes in different types of graphs, enhancing the generalization ability for node classification; 3) The hard gating mechanism selects the appropriate activation layers to enhance the expressive power of the FFN; 4) Adaptive residual connections are used to improve the adaptability to various data features. The specific design details are as follows.









% 首先，输入的特征矩阵将会经历一个线性变换，得到初始的节点编码：
First, the input features $\boldsymbol{X}$ will be transformed into an initial feature embedding through a linear transformation parameterized by $\boldsymbol{W}_0\in\mathbb{R}^{d\times d^\prime}$ and a ReLU activation:
\begin{equation}
  \boldsymbol{H}^{(0)}=\operatorname{ReLU}\left(\boldsymbol{X}\boldsymbol{W}_0\right)
\end{equation}
where $d^\prime$ is the hidden dimension.
% 紧接着，我们堆叠一系列消息传递块，即PT-block，来进一步学习节点表征。每一个PT-block由一个软门控层、一个专家网络，一个层归一化操作，以及初始残差连接组成，其中专家网络中包含四种消息传递专家，擅长处理不同的图特征。对于第l个PT-block，其接受第l-1个PT-block输出的节点表示作为输入，然后通过软门控机制来计算专家网络的分配权重，并对不同专家处理后的图消息进行加权聚合，最后生成新的节点表征：
Next, we stack a series of message passing blocks, called PT-blocks, to further learn node representations. Each PT-block consists of a soft gating layer $\operatorname{SG}(\cdot)$, an expert network $\mathcal{E}(\cdot)$, a layer normalization operation $\operatorname{LN}(\cdot)$, and an adaptive initial residual connection, where the expert network $\mathcal{E}=\left\{\text{PP}, \text{PT}, \text{TP}, \text{TT}\right\}$ contains four message passing experts specialized in handling different graph features. For the $(l)$-th PT-block, it takes the node representation output from the $(l-1)$-th PT-block as input, then calculates the allocation weights of the expert network through the soft gating mechanism: 
\begin{equation}
    \boldsymbol{w}_\text{sg}=\operatorname{SG}\left(\boldsymbol{H}^{(l-1)}\right) = \operatorname{Softmax}\left(\boldsymbol{W}_2 \cdot \operatorname{ReLU}\left( \boldsymbol{H}^{(l-1)}\boldsymbol{W}_1\right)\right)
\end{equation}
where $\boldsymbol{w}_\text{sg}\in\mathbb{R}^4$ is the allocation weights, $\boldsymbol{W}_1$ and $\boldsymbol{W}_2$ are the transformation weights.
Next, the graph messages processed by different experts are aggregated using allocation weights, and new node representations are generated through residual connections:
\begin{equation}
  \begin{aligned}
  &\boldsymbol{H}^{(l-1)} = \sum_{i=1}^{4} \boldsymbol{w}_\text{sg}^\textit{i} \cdot \mathcal{E}_\textit{i}\left(\boldsymbol{A}, \boldsymbol{H}^{(l-1)}\right) \\
  &\boldsymbol{H}^{(l)}= \operatorname{LN} \left( \alpha_l \cdot \boldsymbol{H}^{(0)} + (1 - \alpha_l) \cdot \boldsymbol{H}^{(l-1)} \right)
\end{aligned}
\end{equation}
% \begin{equation}
%   \begin{aligned}
%   \boldsymbol{H}^{(l-1)} &= \sum_{i=1}^{m} \operatorname{SG}\left(\boldsymbol{H}^{(l-1)}\right)_\textit{i} \cdot \mathcal{E}_\textit{i}\left(\boldsymbol{A}, \boldsymbol{H}^{(l-1)}\right) \\
%   \boldsymbol{H}^{(l)} &= \operatorname{LN} \left( \alpha_l \cdot \boldsymbol{H}^{(0)} + (1 - \alpha_l) \cdot \boldsymbol{H}^{(l-1)} \right)
% \end{aligned}
% \end{equation}
where $\alpha_\textit{l}$ is a learnable parameter that controls the adaptive initial residual connection.


% 经过以上的两层模型计算，初始节点特征已经通过交替堆叠的$P$和$T$学习到了丰富的节点特征表示。最后，通过可选激活函数的FFN层以及第三个由$\theta$控制的自适应初始化残差层得到模型的输出$\boldsymbol{h}^{l}_i$，并且将具体公式如下：

% 经过$l$个PT-block的处理，模型已经有效地融合了节点的属性信息与拓扑信息。进一步，受vanilla GT架构的启示，即后接FFN可以提高vanilla GNN的表达能力，我们在\model的架构中设计了一个增强的FFN模块。具体而言，增强的FFN模块由一个硬门控层，一个专家网络，一个层归一化操作，以及自适应残差连接组成。硬门控层接受H作为输入，计算


After message passing via $l$ PT-blocks, \model has effectively fused the attribute information of the nodes with the topological information. Furthermore, inspired by the vanilla GT architecture, where adding FFN can enhance the expressiveness of vanilla GNN, we design an enhanced FFN module in the \model architecture. Specifically, the enhanced FFN module consists of a hard gating layer $\operatorname{HG}(\cdot)$, an expert network $\mathcal{A}(\cdot)$, a layer normalization operation, and an adaptive residual connections, where the expert network $\mathcal{A}=\left\{\text{SwishGLU},\text{GEGLU},\text{REGLU} \right\}$ contains three activation function experts. SwishGLU~\cite{SwishGLU} combines Swish activation with gating mechanisms to promote more effective gradient propagation; GEGLU~\cite{SwishGLU} enhances nonlinear expressiveness through additive activation and gating; REGLU~\cite{SwishGLU} introduces gating on top of ReLU to reduce gradient vanishing and improve computational efficiency.

% 在增强的FFN中，经过l 次消息传递后的节点特征首先会被输入到一个 Hard Gating Layer 中，用于选择适当的激活函数专家来进行进一步的特征编码：
In the enhanced FFN, the node features encoded by $l$ PT-blocks are first input into a hard gating layer, which selects the appropriate activation function expert for further feature encoding:
\begin{equation}
  j=\operatorname{HG}\left(\boldsymbol{H}^{(l)}\right)=\operatorname{Gumbel\_Softmax}\left(\boldsymbol{H}^{(l)}\right) \in\{1,2,3\}
\end{equation}
% 然后，被选择的专家将会对H进一步编码来提升其表达能力，并后接一个自适应残差连接来生成最终的节点表示：
Then, the selected expert will further encode $\boldsymbol{H}^{(l)}$ to enhance its expressiveness, followed by an adaptive residual connection to generate the final node representation:
\begin{equation}
  \begin{aligned}
    &\boldsymbol{Z} = \mathcal{A}_\textit{j}\left(\boldsymbol{H}^{(l)}\right) =  \left( \sigma_\textit{j}\left(\boldsymbol{H}^{(l)}\boldsymbol{W}_3\right)\otimes \boldsymbol{H}^{(l)}\boldsymbol{W}_4 \right)\boldsymbol{W}_5\\
    &\boldsymbol{Z} = \operatorname{LN}\left(\beta \cdot \boldsymbol{H}^{(0)}+(1-\beta)\cdot \boldsymbol{Z}\right)
  \end{aligned}
\end{equation}
where $\sigma\in \left\{\text{Swish},\text{GELU},\text{ReLU}\right\}$, $\boldsymbol{W}_3$, $\boldsymbol{W}_4$, $\boldsymbol{W}_5\in\mathbb{R}^{d^\prime\times d^\prime}$ are the transformation weights, $\otimes$ is the element-wise multiplication, $\beta$ is a learnable parameter that controls the adaptive residual connection.




To achieve node classification, we finally use a prediction head $f_\text{pred}$ parameterized by $\boldsymbol{W}_6\in\mathbb{R}^{d^\prime \times C}$ and Softmax activation to obtain the node predictions. During model training, binary cross-entropy classification loss is used as the optimization objective.
\begin{equation}
  \hat{\boldsymbol{Y}}=\operatorname{Softmax}\left( \boldsymbol{Z}\boldsymbol{W}_6 \right), \quad
    \mathcal{L_\text{task}} = -\operatorname{trace}\left(\boldsymbol{Y}_\text{train}^\top \cdot \log \hat{\boldsymbol{Y}}_\text{train} \right)
\end{equation}
where the trace operation $\operatorname{trace}\left( \cdot \right)$ is used to compute the sum of the diagonal elements of the matrix.


\begin{table*}[]
  \centering
  \caption{Node classification results: average test accuracy (\%) $\pm$ standard deviation. ``Local Rank'' indicates the average performance ranking across homophilous or heterophilous datasets, ``Global Rank'' indicates the average performance ranking across all datasets. Highlighted are the top \textcolor[HTML]{1aa64d}{first}, \textcolor[HTML]{d86000}{second}, and \textcolor[HTML]{6666cc}{third} results.}
  \renewcommand\arraystretch{1.2}
  \label{tab: main}
  \renewcommand\arraystretch{1.2}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|ccccccc|ccccccc|c}
      \hline
      \multicolumn{2}{l|}{\diagbox{Method}{Dataset}}                                      & Computers             & Photo                 & \begin{tabular}[c]{@{}c@{}}Coauthor\\CS\end{tabular} & \begin{tabular}[c]{@{}c@{}}Coauthor\\Physics\end{tabular} & Facebook              & ogbn-arixv            & \begin{tabular}[c]{@{}c@{}}Local\\Rank\end{tabular} & Actor                 & \begin{tabular}[c]{@{}c@{}}Chameleon\\-fix\end{tabular} & Squirrel-fix          & \begin{tabular}[c]{@{}c@{}}Tolokers\\ROC-AUC($\uparrow$)\end{tabular}              & \begin{tabular}[c]{@{}c@{}}Roman\\-empire\end{tabular} & Penn94       & \begin{tabular}[c]{@{}c@{}}Local\\Rank\end{tabular} & \begin{tabular}[c]{@{}c@{}}Global\\Rank\end{tabular}  \\
      \hline
      \multirow{4}{*}{Vanilla}                 & MLP         & 85.01 $\pm$ 0.84 & 92.00 $\pm$ 0.56 & 94.80 $\pm$ 0.35                                           & 96.11 $\pm$ 0.14                                                & 76.86 $\pm$ 0.34 & 53.46 $\pm$ 0.35 & 21.50      & 37.14 $\pm$ 1.06 & 33.31 $\pm$ 2.32                                             & 34.47 $\pm$ 3.09 & 53.18 $\pm$ 6.35  & 65.98 $\pm$ 0.43                                            & 75.18 $\pm$ 0.35 & 20.17      & 20.88       \\
                                              & GCN         & 91.17 $\pm$ 0.54 & 94.26 $\pm$ 0.59 & 93.40 $\pm$ 0.45                                           & 96.37 $\pm$ 0.20                                                & 93.98 $\pm$ 0.34 & 69.71 $\pm$ 0.18 & 17.67      & 30.65 $\pm$ 1.06 & 41.85 $\pm$ 3.22                                             & 33.89 $\pm$ 2.61 & 70.34 $\pm$ 1.64  & 50.76 $\pm$ 0.46                                            & 80.45 $\pm$ 0.27 & 20.83      & 19.13       \\
                                              & GAT         & 91.44 $\pm$ 0.43 & 94.42 $\pm$ 0.61 & 93.20 $\pm$ 0.64                                           & 96.28 $\pm$ 0.31                                                & 94.03 $\pm$ 0.36 & 70.03 $\pm$ 0.42 & 16.83      & 30.58 $\pm$ 1.18 & 43.31 $\pm$ 3.42                                             & 36.27 $\pm$ 2.12 & 79.93 $\pm$ 0.77  & 57.34 $\pm$ 1.81                                            & 78.10 $\pm$ 1.28 & 19.33      & 17.99       \\
                                              & GraphSAGE   & 90.94 $\pm$ 0.56 & 95.41 $\pm$ 0.45 & 94.17 $\pm$ 0.46                                           & 96.69 $\pm$ 0.23                                                & 93.72 $\pm$ 0.35 & 69.15 $\pm$ 0.18 & 16.33      & 37.60 $\pm$ 0.95 & 44.94 $\pm$ 3.67                                             & 36.61 $\pm$ 3.06 & 82.37 $\pm$ 0.64  & 77.77 $\pm$ 0.49                                            & OOM          & 13.17      & 14.87       \\
      \hline
      \multirow{7}{*}{\begin{tabular}[c]{@{}c@{}}Hetero-\\GNN\end{tabular}}           & H2GCN       & 91.69 $\pm$ 0.33 & 95.59 $\pm$ 0.48 & 95.62 $\pm$ 0.27                                           & 97.00 $\pm$ 0.16                                                & 94.36 $\pm$ 0.32 & OOM          & 9.33       & 37.27 $\pm$ 1.27 & 43.09 $\pm$ 3.85                                             & 40.07 $\pm$ 2.73 & 81.34 $\pm$ 1.16  & 79.47 $\pm$ 0.43                                            & 75.91 $\pm$ 0.44 & 13.50      & 11.26       \\
                                              & GPRGNN      & 91.80 $\pm$ 0.55 & 95.44 $\pm$ 0.33 & 95.17 $\pm$ 0.34                                           & 96.94 $\pm$ 0.20                                                & 94.84 $\pm$ 0.24 & 69.95 $\pm$ 0.19 & 10.17       & 36.89 $\pm$ 0.83 & 44.27 $\pm$ 5.23                                             & 40.58 $\pm$ 2.00 & 73.84 $\pm$ 1.40  & 67.72 $\pm$ 0.63                                            & 84.34 $\pm$ 0.29 & 13.50      & 11.71       \\
                                              & FAGCN       & 89.54 $\pm$ 0.75 & 94.44 $\pm$ 0.62 & 94.93 $\pm$ 0.22                                           & 96.91 $\pm$ 0.27                                                & 91.90 $\pm$ 1.95 & 66.87 $\pm$ 1.48 & 17.67      & 37.59 $\pm$ 0.95 & 45.28 $\pm$ 4.33                                             & 41.05 $\pm$ 2.67 & 81.38 $\pm$ 1.34  & 75.83 $\pm$ 0.35                                            & 79.01 $\pm$ 1.09 & 10.83      & 14.51       \\
                                              & ACMGCN      & 91.66 $\pm$ 0.78 & 95.42 $\pm$ 0.39 & 95.47 $\pm$ 0.33                                           & 97.00 $\pm$ 0.27                                                & 94.27 $\pm$ 0.33 & 69.98 $\pm$ 0.11 & 10.00       & 36.89 $\pm$ 1.13 & 43.99 $\pm$ 2.02                                             & 36.58 $\pm$ 2.75 & 83.52 $\pm$ 0.87  & 81.57 $\pm$ 0.35                                            & 83.01 $\pm$ 0.46 & 13.50      & 11.62       \\
                                              & GloGNN      & 89.48 $\pm$ 0.63 & 94.34 $\pm$ 0.58 & 95.32 $\pm$ 0.29                                           & OOM                                                         & 84.57 $\pm$ 0.62 & OOM          & 20.33      & 37.30 $\pm$ 1.41 & 41.46 $\pm$ 3.89                                             & 37.66 $\pm$ 2.12 & 58.74 $\pm$ 13.41 & 66.46 $\pm$ 0.41                                            & 85.33 $\pm$ 0.27 & 15.83      & 18.26       \\
                                              & FSGNN       & 91.03 $\pm$ 0.56 & 95.50 $\pm$ 0.41 & 95.51 $\pm$ 0.32                                           & 96.98 $\pm$ 0.20                                                & 94.32 $\pm$ 0.32 & 71.09 $\pm$ 0.21 & 9.83       & 37.14 $\pm$ 1.06 & 45.79 $\pm$ 3.31                                             & 38.25 $\pm$ 2.62 & 83.87 $\pm$ 0.98  & 79.76 $\pm$ 0.41                                            & 83.87 $\pm$ 0.98 & 10.33       & 10.06        \\
                                              & LINKX       & 90.75 $\pm$ 0.36 & 94.58 $\pm$ 0.56 & 95.52 $\pm$ 0.30                                           & 96.93 $\pm$ 0.16                                                & 93.84 $\pm$ 0.32 & 66.16 $\pm$ 0.27 & 14.33      & 31.17 $\pm$ 0.61 & 44.94 $\pm$ 3.08                                             & 38.40 $\pm$ 3.54 & 77.55 $\pm$ 0.80  & 61.36 $\pm$ 0.60                                            & 84.97 $\pm$ 0.46 & 15.00      & 14.64       \\
      \hline
      \multirow{6}{*}{GT}                      & Vanilla GT  & 84.41 $\pm$ 0.72 & 91.58 $\pm$ 0.73 & 94.61 $\pm$ 0.30                                           & OOM                                                         & OOM          & OOM          & 22.50      & 37.08 $\pm$ 1.08 & 44.27 $\pm$ 3.98                                             & 39.55 $\pm$ 3.10 & 72.24 $\pm$ 1.17  & OOM                                                     & OOM          & 17.00      & 19.96       \\
                                              & ANS-GT      & 90.01 $\pm$ 0.38 & 94.51 $\pm$ 0.24 & 93.93 $\pm$ 0.23                                           & 96.28 $\pm$ 0.19                                                & 92.61 $\pm$ 0.16 & OOM          & 19.67      & \textcolor[HTML]{6666cc}{37.80 $\pm$ 0.95} & 40.74 $\pm$ 2.26                                             & 36.65 $\pm$ 0.80 & 76.91 $\pm$ 0.85  & 80.36 $\pm$ 0.71                                            & OOM          & 15.53      & 17.67       \\
                                              & NAGphormer   & 90.22 $\pm$ 0.42 & 94.95 $\pm$ 0.52 & 94.96 $\pm$ 0.25                                           & 96.43 $\pm$ 0.20                                                & 93.35 $\pm$ 0.28 & 70.25 $\pm$ 0.13 & 16.17      & 36.99 $\pm$ 1.39 & 46.12 $\pm$ 2.25                                             & 38.31 $\pm$ 2.43 & 66.73 $\pm$ 1.18  & 75.92 $\pm$ 0.69                                            & 73.98 $\pm$ 0.53 & 15.50      & 15.86       \\
                                              & SGFormer    & 90.70 $\pm$ 0.59 & 94.46 $\pm$ 0.49 & 95.21 $\pm$ 0.20                                           & 96.87 $\pm$ 0.18                                                & 86.66 $\pm$ 0.54 & 65.84 $\pm$ 0.24 & 17.33      & 36.59 $\pm$ 0.90 & 44.27 $\pm$ 3.68                                             & 38.83 $\pm$ 2.19 & 80.46 $\pm$ 0.91  & 76.41 $\pm$ 0.50                                            & 76.65 $\pm$ 0.49 & 15.50      & 16.49       \\
                                              & Exphormer   & 91.46 $\pm$ 0.51 & 95.42 $\pm$ 0.26 & 95.62 $\pm$ 0.29                                           & 96.89 $\pm$ 0.20                                                & 93.88 $\pm$ 0.40 & 71.59 $\pm$ 0.24 & 10.83      & 36.83 $\pm$ 1.10 & 42.58 $\pm$ 3.24                                             & 36.19 $\pm$ 3.20 & 82.26 $\pm$ 0.41  & \textcolor[HTML]{1aa64d}{87.55 $\pm$ 1.13}                                            & OOM          & 15.33      & 12.91       \\
                                              & Difformer   & 91.52 $\pm$ 0.55 & 95.41 $\pm$ 0.38 & 95.49 $\pm$ 0.26                                           & 96.98 $\pm$ 0.22                                                & 94.23 $\pm$ 0.47 & OOM          & 12.33      & 36.73 $\pm$ 1.27 & 44.44 $\pm$ 3.20                                             & 40.45 $\pm$ 2.51 & 81.04 $\pm$ 4.16  & 78.97 $\pm$ 0.54                                            & OOM          & 14.50      & 13.33       \\
      \hline
      \multirow{3}{*}{\textbf{GNNMoE}}                  & GCN-like P  & \textcolor[HTML]{d86000}{91.99 $\pm$ 0.42} & \textcolor[HTML]{1aa64d}{95.82 $\pm$ 0.43} & \textcolor[HTML]{1aa64d}{95.88 $\pm$ 0.26}                                           & \textcolor[HTML]{1aa64d}{97.20 $\pm$ 0.13}                                                & 95.12 $\pm$ 0.26 & \textcolor[HTML]{6666cc}{72.31 $\pm$ 0.27} & \textcolor[HTML]{1aa64d}{2.33}       & 37.60 $\pm$ 1.75 & \textcolor[HTML]{1aa64d}{47.98 $\pm$ 2.82}                                             & \textcolor[HTML]{d86000}{42.67 $\pm$ 2.28} & \textcolor[HTML]{d86000}{85.32 $\pm$ 0.62}  & 85.09 $\pm$ 0.73                                            & \textcolor[HTML]{d86000}{85.35 $\pm$ 0.33} & \textcolor[HTML]{1aa64d}{3.00}       & \textcolor[HTML]{1aa64d}{2.64}        \\
                                              & SAGE-like P & 91.87 $\pm$ 0.44 & 95.73 $\pm$ 0.24 & 95.72 $\pm$ 0.23                                           & \textcolor[HTML]{d86000}{97.16 $\pm$ 0.16}                                                & \textcolor[HTML]{6666cc}{95.28 $\pm$ 0.26} & 71.83 $\pm$ 0.18 & 4.00       & \textcolor[HTML]{1aa64d}{38.04 $\pm$ 0.99} & \textcolor[HTML]{d86000}{47.75 $\pm$ 2.79}                                             & \textcolor[HTML]{6666cc}{41.78 $\pm$ 2.39} & 83.86 $\pm$ 0.79  & 86.02 $\pm$ 0.51                                            & \textcolor[HTML]{1aa64d}{85.46 $\pm$ 0.27} & \textcolor[HTML]{1aa64d}{3.00}       & \textcolor[HTML]{6666cc}{3.54}        \\
                                              & GAT-like P  & 91.66 $\pm$ 0.55 & \textcolor[HTML]{6666cc}{95.78 $\pm$ 0.37} & \textcolor[HTML]{d86000}{95.84 $\pm$ 0.33}                                           & \textcolor[HTML]{6666cc}{97.16 $\pm$ 0.17}                                                & \textcolor[HTML]{d86000}{95.30 $\pm$ 0.22} & \textcolor[HTML]{1aa64d}{72.54 $\pm$ 0.23} & \textcolor[HTML]{6666cc}{3.17}       & 37.53 $\pm$ 1.00 & 46.69 $\pm$ 3.77                                             & 41.12 $\pm$ 2.23 & \textcolor[HTML]{6666cc}{85.29 $\pm$ 0.80}  & \textcolor[HTML]{d86000}{87.34 $\pm$ 0.62}                                            & \textcolor[HTML]{6666cc}{85.35 $\pm$ 0.34} & \textcolor[HTML]{6666cc}{4.00}       & \textcolor[HTML]{d86000}{3.55}        \\
      \hline
      \multirow{3}{*}{w/o $\mathcal{L}_\text{gate}$} & GCN-like P  & \textcolor[HTML]{1aa64d}{92.17 $\pm$ 0.50} & \textcolor[HTML]{d86000}{95.81 $\pm$ 0.41} & \textcolor[HTML]{6666cc}{95.81 $\pm$ 0.26}                                           & 97.03 $\pm$ 0.13                                                & \textcolor[HTML]{1aa64d}{95.53 $\pm$ 0.35} & 72.29 $\pm$ 0.16 & \textcolor[HTML]{d86000}{2.67}       & 37.59 $\pm$ 1.36 & \textcolor[HTML]{6666cc}{47.19 $\pm$ 2.93}                                             & \textcolor[HTML]{1aa64d}{44.02 $\pm$ 2.59} & 84.77 $\pm$ 0.93  & 85.05 $\pm$ 0.55                                            & 84.61 $\pm$ 0.39 & 4.67       & 3.59        \\
                                              & SAGE-like P & 91.85 $\pm$ 0.39 & 95.46 $\pm$ 0.24 & 95.68 $\pm$ 0.24                                           & 96.81 $\pm$ 0.22                                                & 94.63 $\pm$ 0.36 & 71.94 $\pm$ 0.25 & 8.00       & \textcolor[HTML]{d86000}{37.97 $\pm$ 1.01} & 45.73 $\pm$ 3.19                                             & 39.19 $\pm$ 2.84 & 83.96 $\pm$ 0.75  & 86.00 $\pm$ 0.45                                            & 84.05 $\pm$ 0.37 & 6.50       & 7.31        \\
                                              & GAT-like P  & \textcolor[HTML]{6666cc}{91.98 $\pm$ 0.46} & 95.71 $\pm$ 0.37 & 95.72 $\pm$ 0.23                                           & 97.05 $\pm$ 0.19                                                & 95.21 $\pm$ 0.25 & \textcolor[HTML]{d86000}{72.45 $\pm$ 0.32} & 3.83       & 37.76 $\pm$ 0.98 & 45.56 $\pm$ 3.94                                             & 39.19 $\pm$ 2.38 & \textcolor[HTML]{1aa64d}{85.45 $\pm$ 0.94}  & \textcolor[HTML]{6666cc}{87.29 $\pm$ 0.60}                                            & 81.98 $\pm$ 0.47 & 6.67       & 5.14       \\
      \hline
      \multirow{1}{*}{w/o FFN} & GCN-like P  & 91.74 $\pm$ 0.47 & 95.37 $\pm$ 0.34 & 95.39 $\pm$ 0.35                                           & 96.86 $\pm$ 0.21                                                & 95.23 $\pm$ 0.27 & 71.92 $\pm$ 0.15 & 9.83       & 33.72 $\pm$ 1.33 & 46.52 $\pm$ 3.13                                             & 40.92 $\pm$ 2.28 & 83.17 $\pm$ 1.63  & 82.41 $\pm$ 0.38                                            & 84.04 $\pm$ 1.16 & 9.67       & 9.76        \\
      \hline
    \end{tabular}}
\end{table*}



\subsection{Entropy-guided Soft Gating Sharpening}
% 先前的研究[GMoE, BP-MoE]将Top-K机制应用于门控网络，为目标节点选择合适的专家。然而，这种方法需要固定超参数K来指定专家网络的数量，增加了超参数调优的负担。[Link-MoE]则使用加权求和的方式来整合专家网络的输出，从而避免了超参数K的设置问题。然而，这种方法可能在处理某些复杂数据集时无法充分发挥不同专家的优势。为了探究更高效且通用的专家选择机制，我们在\model的基础上，在门控网络中分别使用Top-K机制和Mean机制进行了实验。
% 现有的研究通常采用topk选择或加权求和的方式来进行门控分配。而我们进一步通过观察实验发现，这两种做法适合不同的场景，在通用性上受到限制。具体而言，我们通过替换GNNMoE中的软门控（加权）为topk选择，并比较了替换前后框架在不同图数据上的节点分类表现。我们发现不同类型的图对不同的门控选择机制存在偏好。同质图更倾向于topk选择，而异质图更偏好加权求和的方式。结合图一中的观察实验1，一个合理的解释是，同质图中节点由于特征和结构的趋同性，往往仅需要特定的编码方式（特别是PT）就能学习到较好的节点表征（即topk选择具有优势），融合过多的编码方式反而会引入噪声；而异质图中的节点在特征和结构上呈现多样性，往往需要组合多种编码方式才能为所有节点学习到高质量的表征（即加权求和具有优势），单一的编码方式在通用性上受限。
% 实验中的超参数设置与**Section 4**中的配置一致. Top-K方法中$K \in \{1,2,3\}$。实验报告了最优超参数下的结果，.
Existing studies typically adopt either Top-K selection~\cite{GMoE, BP-MoE} or weighted summation~\cite{Link-MoE} for gating allocation. However, our further observation experiment~\footnote{The hyperparameter settings in observation experiment 2 are consistent with the configurations in Section 4. In Top-K selection, $K\in\{1,2,3\}$. The reported experimental results correspond to the optimal hyperparameter settings.} reveal that these two practices are suitable for different scenarios, limiting their generalizability. Specifically, we replace the soft gating (weighted summation) in \model with a Top-K selection mechanism and compare the node classification performance of the framework before and after this modification across different graph datasets, as shown in Figure~\ref{fig: obs2}. We find that different types of graphs exhibit preferences for different gating selection mechanisms. Homophilous graphs tend to favor Top-K selection, while heterophilous graphs perform better with weighted summation. In conjunction with Observation 1 in Figure~\ref{fig: obs1}, a reasonable explanation is that in homophilous graphs, due to the high similarity in node features and structure, a specific encoding method (particularly PT) is often sufficient to learn effective node representations (thus, Top-K selection is advantageous), while incorporating too many encoding methods may introduce noise. Conversely, in heterophilous graphs, nodes exhibit diversity in both features and structure, necessitating a combination of multiple encoding methods to learn high-quality representations for all nodes (thus, weighted summation is advantageous), whereas a single encoding method is limited in generalization ability.

% 基于\model框架（包括PT-block和增强FFN两个模块），我们通过替换门控网络中的专家选择机制，比较了平均求和方法和Top-K选择方法在分类精度上的表现差异。为评估模型性能，我们在6个同质图数据集和6个异质图数据集上进行了实验，所有实验均在最优超参数配置下进行，实验中的超参数设置与**Section 4**中的配置一致。在Top-K方法中$K \in \{1,2,3\}$.图3展示了三种消息传递操作（GCN、SAGE、GAT）下的最优结果。实验结果表明，不同的专家选择机制在同质图和异质图上存在显著的偏好差异。具体来说，Top-K方法在同质图数据集上表现出显著优势，而平均求和方法在异质图数据集上更为优越。结合Figure 1的观察结果，我们认为同质图对PT编码器有明显偏好，而Top-K方法能够有效选择与数据集偏好相符的专家，从而提高性能。而对于异质图数据集，因其对PT编码器的偏好存在多样性，平均求和方法能够在训练过程中融合多种消息传递模块的优势，从而在异质图上取得较好的表现。

% The previous studies ~\cite{GMoE, BP-MoE} applied the Top-K mechanism to gating networks to select suitable experts for target nodes. However, this approach requires a fixed hyperparameter K to specify the number of selected experts, which increases the burden of hyperparameter tuning. ~\cite{Link-MoE} uses a weighted sum to integrate the outputs of the expert network, thus avoiding the need to set the hyperparameter K. However, this method may fail to fully leverage the strengths of different experts when dealing with certain complex datasets. To explore a more efficient and generalizable expert selection mechanism, we conducted observation experiment in the \model framework, applying both the Top-K mechanism and the Mean mechanism within the gating network.


% Observation 1. 基于\model框架（包括PT-block和增强FFN两个模块），我们通过替换门控网络中的专家选择机制，比较了平均求和方法和Top-K选择方法在分类精度上的表现差异。为评估模型性能，我们在6个同质图数据集和6个异质图数据集上进行了实验，所有实验均在最优超参数配置下进行，实验中的超参数设置与**Section 4**中的配置一致。在Top-K方法中$K \in \{1,2,3\}$.图3展示了三种消息传递操作（GCN、SAGE、GAT）下的最优结果。实验结果表明，不同的专家选择机制在同质图和异质图上存在显著的偏好差异。具体来说，Top-K方法在同质图数据集上表现出显著优势，而平均求和方法在异质图数据集上更为优越。结合Figure 1的观察结果，我们认为同质图对PT编码器有明显偏好，而Top-K方法能够有效选择与数据集偏好相符的专家，从而提高性能。而对于异质图数据集，因其对PT编码器的偏好存在多样性，平均求和方法能够在训练过程中融合多种消息传递模块的优势，从而在异质图上取得较好的表现。
\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{Fig-3-obs2.pdf}
  \caption{Observation experiment 2. Preference for gating selection mechanisms across different graphs.}
  \label{fig: obs2}
\end{figure}
% \noindent\textbf{Observation 1.~} Based on the \model framework which includes the PT-block and enhanced FFN modules, we compared the performance of the average summation method and the Top-K selection method in terms of classification accuracy by replacing the expert selection mechanism in the gating network. To evaluate the model's performance, we conducted experiments on 6 homophilous graph datasets and 6 heterophilous graph datasets, with all experiments performed under the optimal hyperparameter configuration. The hyperparameter settings in the experiments are consistent with those in Section ~\ref{sec:experiments}. In the Top-K method, $K \in \{1,2,3\}$. Figure ~\ref{fig: obs2} shows the optimal results under three message-passing operations (GCN, SAGE, GAT). The experimental results indicate significant preference differences between the expert selection mechanisms on homophilous and heterophilous graphs. Specifically, the Top-K method exhibits a clear advantage on homophilous graph datasets, while the average summation method performs better on heterophilous graph datasets. Combining the observations in Figure ~\ref{fig: obs1}, we argue that homophilous graphs show a clear preference for PT encoders, and the Top-K method can effectively select experts that align with dataset preferences, thereby improving performance. For heterophilous graph datasets, due to the diversity of preferences for PT encoders, the average summation method is able to integrate the advantages of various message-passing modules during training, thus achieving better performance on heterophilous graphs.


% 受到上述观察实验的启发，我们发现当前的软门控设置（即加权求和）依然存在局限性。为了进一步提升门控网络对多样化图的适应性，我们设计了熵引导的软门控锐化机制。具体而言，软门控的分配权重$\boldsymbol{w}_\text{sg}$可以被视为专家选择的概率分布，通过计算这个概率分布的熵，我们可以观察到任意节点接收专家网络信息的多样性。我们计算所有节点的门控分配权重分布的熵的均值，并将其作为一个正则项加入到最终的优化目标中：
Inspired by the above observation, we conclude that the current soft gating mechanism (i.e., weighted summation) still has limitations. To further enhance the adaptability of the gating network to diverse graphs, we propose an entropy-guided soft gating sharpening mechanism.
Specifically, the allocation weights of the soft gating $\boldsymbol{w}_\text{sg}$ can be regarded as a probability distribution over expert selection. By computing the entropy of this probability distribution, we can observe the diversity of expert network information received by arbitrary node. We compute the mean entropy of the gating allocation weight distribution across all nodes and incorporate it as a regularization term in the final optimization objective:
% 所有节点的软门控熵均值来衡量各个节点的专家选择混乱度，并且将其作为一个额外的正则项加入到损失函数中。具体而言，其公式定义如下:
% Inspired by the above observation experiment, we base on the soft gating weights $\boldsymbol{w}_\text{sg}$, quantifying the disorder of expert choices for each node by calculating the mean of the node's soft gating entropy, and incorporating it as a regularization term to guide the model in learning a more reasonable and stable distribution of expert choices. Its mathematical expression can be defined as:
\begin{equation}
    \mathcal{L}_\text{gate}=\frac{1}{|V|}\sum_{i=1}^{|V|}\sum_{k=1}^{4}\boldsymbol{w}_\text{sg}^\textit{i,k}\cdot \log \boldsymbol{w}_\text{sg}^\textit{i,k}, 
    \quad
      \mathcal{L} = \mathcal{L_\text{task}} + \lambda \cdot \mathcal{L}_\text{gate}
\end{equation}
% 其中，$|V|$是节点的数量，$k$是专家网络的索引。此外，我们通过$\lambda\in\left{0.001,0.01,0.1}\right}$来控制$\mathcal{L}_{node}$正则化项的，通过引入$\mathcal{L}_{node}$，我们可以有效地引导模型更加灵活的选择合适的专家网络。
% 通过最小化这个正则项，我们可以引导软门控为部分专家网络分配更大的权重，也就是促进分配权重分布的锐化，实现类似的topk选择。为了在不同图场景下有效地权衡加权求和和topk选择，我们引入超参数来控制这个正则项的强度，使得GNNMoE可以更灵活的对不同节点进行自适应的表征学习。
where $|V|$ represents the total number of nodes, and $k$ is the index of the expert network.
By minimizing this regularization term $\mathcal{L}_\text{gate}$, we can guide the soft gating mechanism to assign higher weights to certain expert networks, thereby sharpening the allocation weight distribution to achieve a behavior similar to Top-K selection. To effectively balance weighted summation and Top-K selection across different graph scenarios, we introduce a hyperparameter $\lambda\in\{0.001, 0.01, 0.1\}$ to control the strength of this regularization term, allowing \model to more flexibly perform adaptive representation learning for different nodes.

% To control the influence of the regularization term, we introduce a hyperparameter $\lambda\in\{0.001, 0.01, 0.1\}$. By incorporating $\mathcal{L}_\text{gate}$ into the loss function, we can effectively guide the model to select more suitable expert networks more flexibly.




\section{Experiments}
\label{sec:experiments}
\subsection{Experiment Settings}
\subsubsection{Datasets and Baselines}
% 我们在12个基准数据集上进行了大量的探索和评估实验，数据集包括6个同质数据集：Computers，Photo，Coauthor CS，Coauthor Physics，Wiki-CS，Facebook，和6个异质数据集：Actor，Chameleon-fix，Squirrel-fix，Tolokers，Roman-empire，Penn94。所有数据集按照48%/32%/20%的比例被划分为训练、验证和测试集。
We conduct extensive experiments on 12 benchmark datasets, which include 
(1) Six homophilous datasets: Computers, Photo~\cite{mcauley2015I}, Coauthor CS, Coauthor Physics~\cite{Shchur2018PitfallsOG}, Facebook~\cite{rozemberczki20201R} and ogbn-arxiv~\cite{hu2020open}; and
(2) Six heterophilous datasets: Actor~\cite{tang2009S}, Squirrel-fix, Chameleon-fix, Tolokers, Roman-empire~\cite{platonov2023a} and Penn94~\cite{NEURIPS2021_ae816a80}.
For most datasets we use random splitting (48\% / 32\% / 20\% for training / validation / testing). For ogbn-arxiv, we use the public splits in OGB~\cite{hu2020open}.
We compare \model with three kinds of baselines, which include (1) Vanilla model: MLP, GCN~\cite{GCN}, GAT~\cite{GAT}, GraphSAGE~\cite{GraphSAGE}; (2) Heterophilous GNNs: LINKX~\cite{NEURIPS2021_ae816a80}, H2GCN~\cite{H2GCN2020}, GPRGNN~\cite{GPRGNN}, FAGCN~\cite{fagcn2021}, ACMGCN~\cite{luan2022revisiting}, GloGNN~\cite{li2022finding}, FSGNN~\cite{MAURYA2022101695}; and (3) GT models: vanilla GT, ANS-GT~\cite{ASN-GT}, NAGphormer~\cite{chennagphormer}, SGFormer~\cite{SGFormer}, Exphormer~\cite{exphormer} and Difformer~\cite{wu2023difformer}.

% \subsection{Baselines}
% 为了全面评估新架构的有效性和优越性，我们与15种baseline进行比较，包括(1) MLP; (2)classical GNNs: GCN, GAT, GraphSAGE; (3) 面向异质的GNN: LINKX, H2GCN, GPRGNN, FAGCN, ACMGCN, GloGNN, FSGNN; (4) Graph Transformer: vanilla GT, ANS-GT, SGformer, NAGphormer.

% To evaluate the effectiveness and superiority of the new GT architecture, we compare it against 15 baselines, including (1) MLP; (2) Classical GNNs: GCN~\cite{GCN}, GAT~\cite{GAT}, GraphSAGE~\cite{GraphSAGE}; (3) Heterophilous GNNs: LINKX~\cite{NEURIPS2021_ae816a80}, H2GCN~\cite{H2GCN2020}, GPRGNN~\cite{chien2021adaptive}, FAGCN~\cite{fagcn2021}, ACMGCN~\cite{luan2022revisiting}, GloGNN~\cite{li2022finding}, FSGNN~\cite{MAURYA2022101695}; (4) Graph Transformers: vanilla GT, ANS-GT~\cite{ASN-GT}, SGformer~\cite{SGFormer}, NAGphormer~\cite{NAGphormer}.
% More details are presented in Appendix B.

\subsubsection{Experimental Settings}
% 我们通过测量节点分类模型的准确性来评估我们提出的模型的有效性。为了保证可靠性，使用随机种子对每个模型进行了10次试验，并且记录average test accuracies (or AUC for Tolokers)使用固定的hidden layer维度64，使用AdamW优化器。每个方法下的数据集the maximum epochs to 500, with 100 epochs patience for early stopping。
%对GNNFormer的实现是使用Python（3.9.11）、Pytorch（1.11.0）和torch_geometric（2.1.0）开发的。所有实验都是在具有一个NVIDIA A100 40GB的Linux服务器上进行。
% 在GNNFormer实验中，using a learning rate of in {0.005, 0.01, 0.05, 0.1}, weight decay of {5e-5, 1e-5, 1e-4, 5e-4, 5e-3} and dropout rate in {0.1, 0.3, 0.5, 0.7, 0.9}.
% 在Baseline实验中, In NAGphormer, we try the hidden dimensions in {128, 256, 512}, using a learning rate of in {1e-3,  5e-3, 1e-4}, weight decay of {1e-3, 5e-4, 1e-5}. We also search the dropout rate in {0.1, 0.3, 0.5}. In SGFormer, we try the hidden dimensions in {32, 64, 128, 256}, using a learning rate of in {0.001, 0.005, 0.01, 0.05, 0.1}. We also search the dropout rate in {0.0, 0.2, 0.3, 0.5} and the number of layers in {1, 2, 3}.
% 为了确保结果的稳定性和可复现性，我们使用10个随机种子来固定数据分割和模型初始化。对所有的方法, we set the maximum epochs to 500 with 100 epochs patience for early stopping, the hidden dimension to 64, and the optimizer to AdamW. 在GNNFormer实验中，using a learning rate of in {0.005, 0.01, 0.05, 0.1}, weight decay of {5e-5, 1e-5, 1e-4, 5e-4, 5e-3} and dropout rate in {0.1, 0.3, 0.5, 0.7, 0.9}. 针对Baseline方法，我们搜索了相同的参数空间，如果他们的代码是公开可获取的。SGFormer中的关键参数$\alpha$ search in \{0.5, 0.8\}; NAGphormer中的关键参数hop search in \{3, 7\};
We utilize 10 random seeds to fix the data splits and model initialization, and report the average accuracy and standard deviation over 10 runs.
For all methods, we set the search space of common parameters as follows:
maximum epochs to 500 with 100 patience,
hidden dimension $d^\prime$ to 64,
optimizer to AdamW,
learning rate in \{0.005, 0.01, 0.05, 0.1\},
dropout rate in \{0.1, 0.3, 0.5, 0.7, 0.9\}.
For \model, the number of PT-blocks in \{3,4,5,6\} is searched for ogbn-arxiv while a fixed value of 2 is used for other datasets.
For all baselines, we search the common parameters in the same parameter spaces.
% In SGFormer, we searched for the key parameter $\alpha$ within \{0.5, 0.8\}, while in NAGphormer, the key parameter $K$ was explored within \{3, 7\}. 
% More details of parameter settings are reported in Appendix C.
% 对GNNFormer的实现是使用Python（3.9.11）、Pytorch（1.11.0）和torch_geometric（2.1.0）开发的。所有实验都是在具有一个NVIDIA A100 40GB的Linux服务器上进行。
% For hardware configuration, all experiments are conducted at Ubuntu 18.04.5 LTS with the Intel(R) Xeon(R) Gold 5218R CPU @ 2.10GHz, and NVIDIA Tesla V100S GPU (with 40GB memory each).
% For software configuration, all model are implemented in PyTorch 2.0.3, Pytorch-Geometric 1.8.0 with CUDA 10.2, scikit-learn 0.24.1 and Python 3.7.
Moreover, \model are implemented in PyTorch 1.11.0, Pytorch-Geometric 2.1.0 with CUDA 12.0 and Python 3.9.
All experiments are conducted at NVIDIA A100 40GB.
\subsection{Evaluation on Node Classification}
% 表2报告了所有方法在12个基准数据集上的节点分类结果，从中我们可以得出以下结论：
% 1）整体上，GNNFormer在同质和异质数据集上均有强大的性能表现，显示出更高的局部和全局平均性能排名，表明GNNFormer在性能的优越性和稳定性上显著超越三类baselines；
% 2）GNNFormer更倾向于使用先转换后传播的消息传递（即TPTP、TTPP）来进行节点表征学习

% 表2报告了所有方法在12个基准数据集上的节点分类结果，从中我们可以得出以下结论：
% 1）整体上，GNNFormer在同质和异质数据集上一致地表现出更高的局部和全局平均性能排名，表明其在性能的优越性和稳定性上显著超越三类baselines。
% 2）一个更具通用性的GNNFormer架构更倾向于使用先转换后传播的消息传递设置（即TPTP、TTPP）来进行节点表征学习，这被GNNFormer(TP+TP, TT+PP)更高的局部和全局性能排名所支持。。一个合理的解释是，转换操作一方面可以将不同类型的节点特征映射到统一的特征空间中，实现特征对齐，另一方面可以通过线性变换和非线性激活等方式来增强特征的表达能力。因此，先转换后传播的策略通过在消息传播前统一和增强节点/边的特征表示，改善了模型在不同类型图中的适应性和表现，从而在同质和异质场景下取得更好的通用性。
% 3）由于异质场景的多样性，因此适配不同异质数据集的最优GNNFormer架构也可以是变化的。例如，在Squirrel-fix上GNNFormer更倾向于进行多次传播（PP）来捕获高阶同质性，或进行多次转换(TT)来进行特征调整和变换以适应异质性；而在Tolokers和Penn94上，GNNFormer更倾向于交替进行转换和传播，通过特征的即时调整和变换来适应异质性。上述现象驱使我们在未来研究GNNFormer架构的自动搜索。

% Table~\ref{tab: main} reports the node classification results of all methods, from which we can draw the following conclusions: 

% 1) \textbf{Overall performance:} GNNFormer consistently exhibits superior local and global average performance rankings on both homophilous and heterophilous datasets, surpassing the three types of baselines significantly in terms of performance superiority and stability;

% 2) \textbf{Better universality for transformation-first message passing:}
% A more universal GNNFormer architecture tends to prioritize a message passing settings where transformation precedes propagation (i.e., $\mathbf{TPTP}$, $\mathbf{TTPP}$) for node representation learning. This is supported by the higher local and global performance rankings of GNNFormer(TP+TP, TT+PP).
% A reasonable explanation is that the transformation operation can map different types of node features into a unified feature space, achieving feature alignment, while also enhancing the expressive power of features through linear transformation and non-linear activation. Therefore, the strategy of transformation followed by propagation improves the model’s adaptability and performance across different types of graphs by unifying and enhancing node/edge feature representations before message propagation, resulting in better universality in both homophilous and heterophilous scenarios.

% 3) \textbf{More significant preferences in heterophilous scenarios:} Due to the diversity of heterophilous scenarios, the optimal architecture of GNNFormer varies for different heterophilous datasets. For example, on Squirrel-fix, GNNFormer tends to perform multiple propagations ($\mathbf{PP}$) to capture higher-order homophily, or multiple transformations ($\mathbf{TT}$) to adjust and transform features to adapt to heterophily. In contrast, on Tolokers and Penn94, GNNFormer prefers alternating transformation and propagation to adapt to heterophily via immediate feature adjustment and transformation. These observations drive us to explore the automatic search for GNNFormer architectures in future work.
Table~\ref{tab: main} reports the node classification results of all methods, from which we can draw the following conclusions: 1) \model consistently demonstrates higher local and global average performance rankings on both homophilous and heterophilous datasets, indicating its effectiveness, superiority, and stability in node classification tasks, significantly surpassing three categories of baselines; 2) As a general-purpose node classification framework, we dynamically assign the optimal message-passing method to each node, enabling fine-grained encoder customization. Notably, FSGNN achieves competitive ranking by aggregating neighbors' features at different hop levels through Softmax. This approach designs an adaptive encoding method for each target node, similar to our core idea. However, compared to FSGNN, \model shows significant improvements in both model performance and efficiency. 3) Our method successfully avoids encountering out-of-memory (OOM) issues, in contrast to certain GT-based methods and spatial-domain GNNs, which suggests the architectural efficiency of \model and its scalability in large-scale graph computations.


\begin{table*}
  \centering
  \caption{More Gateing Mechanism result on \model : average test accuracy (\%) $\pm$ standard deviation. Highlighted are the top \textcolor[HTML]{1aa64d}{first}, \textcolor[HTML]{d86000}{second}, and \textcolor[HTML]{6666cc}{third} results.}
  \label{tab:gate_all}
  \renewcommand\arraystretch{1.2}
  \resizebox{\textwidth}{!}{
    \begin{tabular}{c|c|cccccc|cccccc} 
      \hline
      \multicolumn{2}{l|}{}                                                                              & Computers                                & Photo                                    & \begin{tabular}[c]{@{}c@{}}Coauthor\\CS\end{tabular} & \begin{tabular}[c]{@{}c@{}}Coauthor\\Physics\end{tabular} & Facebook                                 & ogbn-arxiv                               & Actor                                    & \begin{tabular}[c]{@{}c@{}}Chameleon\\-fix\end{tabular} & Squirrel-fix                             & Tolokers                                 & \begin{tabular}[c]{@{}c@{}}Roman\\-empire\end{tabular} & Penn94                                    \\ 
      \hline
      \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GNNMoE\\Learnable Parameter\end{tabular}} & GCN-like P  & 90.92 ± 0.52                             & 94.73 ± 0.53                             & 95.46 ± 0.30                                         & 96.90 ± 0.16                                              & 93.93 ± 0.36                             & 70.13 ± 0.17                             & 36.70 ± 0.82                             & 46.24 ± 4.36                                            & 40.92 ± 2.87                             & 73.79 ± 1.13                             & 65.95 ± 0.46                                           & 84.54 ± 0.38                              \\
                                                                                           & SAGE-like P & 90.98 ± 0.52                             & 94.73 ± 0.41                             & 95.46 ± 0.28                                         & 96.91 ± 0.16                                              & 93.86 ± 0.33                             & 69.25 ± 0.26                             & 36.83 ± 0.67                             & 46.12 ± 3.07                                            & 41.46 ± 2.14                             & 73.62 ± 1.05                             & 65.89 ± 0.44                                           & 84.59 ± 0.43                              \\
                                                                                           & GAT-like P  & 90.93 ± 0.36                             & 94.75 ± 0.41                             & 95.49 ± 0.32                                         & 96.91 ± 0.17                                              & 93.89 ± 0.27                             & 70.32 ± 0.31                             & 36.41 ± 1.37                             & 46.12 ± 4.39                                            & 41.46 ± 2.49                             & 73.42 ± 0.83                             & 65.98 ± 0.39                                           & 84.50 ± 0.60                              \\ 
      \hline
      \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GNNMoE\\Mean\end{tabular}}                & GCN-like P  & \textcolor[HTML]{d86000}{92.15 ± 0.36} & 95.78 ± 0.29                             & 95.83 ± 0.23                                         & 97.12 ± 0.18                                              & 95.23 ± 0.31                             & 70.68 ± 0.22                             & 37.38 ± 0.96                             & \textcolor[HTML]{6666cc}{47.25 ± 2.80}                & 41.37 ± 2.17                             & \textcolor[HTML]{6666cc}{85.29 ± 0.83} & 83.88 ± 0.62                                           & 84.65 ± 0.35                              \\
                                                                                           & SAGE-like P & 91.85 ± 0.44                             & 95.61 ± 0.48                             & 95.53 ± 0.25                                         & 96.96 ± 0.24                                              & 94.92 ± 0.22                             & 69.74 ± 0.27                             & \textcolor[HTML]{d86000}{37.87 ± 1.27} & 45.17 ± 4.31                                            & 39.03 ± 2.16                             & 83.76 ± 1.14                             & 85.63 ± 0.63                                           & 83.91 ± 0.35                              \\
                                                                                           & GAT-like P  & 91.53 ± 0.52                             & 95.72 ± 0.46                             & 95.74 ± 0.31                                         & 97.03 ± 0.22                                              & 95.08 ± 0.37                             & 71.52 ± 0.15                             & 37.33 ± 1.14                             & 44.66 ± 3.25                                            & 39.48 ± 2.45                             & 85.24 ± 0.75                             & 85.32 ± 0.61                                           & 83.16 ± 0.52                              \\ 
      \hline
      \multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}GNNMoE\\Top-K\end{tabular}}               & GCN-like P  & \textcolor[HTML]{1aa64d}{92.15 ± 0.35}            & 95.78 ± 0.27                             & 95.82 ± 0.29                                         & \textcolor[HTML]{d86000}{97.18 ± 0.12}                  & 95.09 ± 0.27                             & 70.57 ± 0.22                             & 37.28 ± 1.36                             & 47.08 ± 3.72                                            & 41.55 ± 2.46                             & 84.80 ± 0.70                             & 84.72 ± 0.69                                           & 84.81 ± 0.33                              \\
                                                                                           & SAGE-like P & 91.80 ± 0.46                             & 95.71 ± 0.32                             & 95.64 ± 0.26                                         & 97.12 ± 0.17                                              & 95.20 ± 0.26                             & 69.48 ± 0.17                             & 37.49 ± 1.00                             & 46.46 ± 3.24                                            & 40.43 ± 2.55                             & 83.89 ± 0.71                             & \textcolor[HTML]{6666cc}{85.80 ± 0.85}               & 84.07 ± 0.39                              \\
                                                                                           & GAT-like P  & 91.87 ± 0.33                             & \textcolor[HTML]{d86000}{95.81 ± 0.46} & \textcolor[HTML]{d86000}{95.84 ± 0.21}             & 97.13 ± 0.17                                              & \textcolor[HTML]{6666cc}{95.26 ± 0.25} & 71.61 ± 0.15                             & 37.05 ± 0.95                             & 46.57 ± 2.40                                            & 41.66 ± 3.17                             & 85.23 ± 0.79                             & 85.14 ± 0.62                                           & 84.19 ± 0.27                              \\ 
      \hline
      \multirow{3}{*}{\textbf{GNNMoE}}                                                              & GCN-like P  & \textcolor[HTML]{6666cc}{91.99 ± 0.42} & \textcolor[HTML]{1aa64d}{95.82 ± 0.43}            & \textcolor[HTML]{1aa64d}{95.88 ± 0.26}                        & \textcolor[HTML]{1aa64d}{97.20 ± 0.13}                             & 95.12 ± 0.26                             & \textcolor[HTML]{d86000}{72.31 ± 0.27} & \textcolor[HTML]{6666cc}{37.60 ± 1.75} & \textcolor[HTML]{1aa64d}{47.98 ± 2.82}                           & \textcolor[HTML]{1aa64d}{42.67 ± 2.28}            & \textcolor[HTML]{1aa64d}{85.32 ± 0.62}            & 85.09 ± 0.73                                           & \textcolor[HTML]{d86000}{85.35 ± 0.33}  \\
                                                                                           & SAGE-like P & 91.87 ± 0.44                             & 95.73 ± 0.24                             & 95.72 ± 0.23                                         & \textcolor[HTML]{6666cc}{97.16 ± 0.16}                  & \textcolor[HTML]{d86000}{95.28 ± 0.26} & \textcolor[HTML]{6666cc}{71.83 ± 0.18} & \textcolor[HTML]{1aa64d}{38.04 ± 0.99}            & \textcolor[HTML]{d86000}{47.75 ± 2.79}                & \textcolor[HTML]{d86000}{41.78 ± 2.39} & 83.86 ± 0.79                             & \textcolor[HTML]{d86000}{86.02 ± 0.51}               & \textcolor[HTML]{1aa64d}{85.46 ± 0.27}             \\
                                                                                           & GAT-like P  & 91.66 ± 0.55                             & \textcolor[HTML]{6666cc}{95.78 ± 0.37} & \textcolor[HTML]{6666cc}{95.84 ± 0.33}             & 97.16 ± 0.17                                              & \textcolor[HTML]{1aa64d}{95.30 ± 0.22}            & \textcolor[HTML]{1aa64d}{72.48 ± 0.23}            & 37.53 ± 1.00                             & 46.69 ± 3.77                                            & 41.12 ± 2.23                             & \textcolor[HTML]{d86000}{85.29 ± 0.80} & \textcolor[HTML]{1aa64d}{87.34 ± 0.62}                          & \textcolor[HTML]{6666cc}{85.35 ± 0.34}  \\
      \hline
      \end{tabular}}
\end{table*}

\begin{figure*}[!htb]
  \centering
  \includegraphics[width=1.0\textwidth]{Fig-4-weight.pdf}
  \caption{Gating Weight Distributions under Entropy Constraints in Homophilic and Heterophilous Datasets}
  \label{fig:entropy}
\end{figure*}

We further conduct ablation studies to analyze the impact of individual components on the performance of \model. The results show that removing the Entropy Constrained module (w/o Entropy Constrained) and the FFN connection module (w/o FFN) leads to significant performance degradation across all datasets. As shown in Table~\ref{tab: main}, under three different propagation operators, the Entropy Constrained module consistently improves the global ranking of \model by approximately 0.2 to 1 times, further demonstrating its effectiveness. The FFN module results in an improvement of more than 2 percentage points in accuracy across the ogbn-arxiv, Actor, Tolokers, and Roman-empire datasets, highlighting its ability to effectively enhance the expressive power of \model.





\subsection{More Analysis}
\subsubsection{Analysis of Gating Mechanism}
% 我们进一步研究了不同门控机制在\model框架中的性能表现。具体而言，我们对比了四种门控机制：可学习参数门控（Learnable Parameter Gating）、平均门控机制（Mean Gating）、Top-K门控机制（Top-K Gating）以及我们提出的熵约束门控机制（Entropy-Constrained Gating）。实验结果如表3所示。分析表明，我们的熵约束门控机制在绝大多数数据集上均展现出显著优势。具体而言：（1）与可学习参数门控相比，熵约束机制通过引入信息熵正则化，避免了过拟合问题，从而提升了模型的泛化能力；（2）与平均门控机制相比，熵约束机制能够更灵活地适应节点的局部特性，帮助模型选择合适的专家网络，减少了噪声干扰（3）与Top-K门控机制相比，熵约束机制无需预设超参数K，通过自适应选择专家数量，显著降低了调参复杂性，同时保持了较高的性能。这些结果表明，熵约束门控机制在平衡灵活性、鲁棒性和实用性方面具有显著优势，能够有效提升\model框架在不同图数据上的表现。
We further investigate the performance of different gating mechanisms within the \model(GCN-like P). Specifically, we compare four gating mechanisms: Learnable Parameter Gating, Mean Gating, Top-K Gating, and our proposed Entropy-Constrained Gating. The experimental results are shown in Table~\ref{tab:gate_all}. Compared to the learnable parameter gating mechanism, the entropy-constrained mechanism effectively prevents overfitting by introducing information entropy regularization, thereby enhancing the model's generalization capability. In comparison to the mean gating mechanism, it adapts more flexibly to the local characteristics of the nodes, reduces noise interference, and improves the accuracy of expert network selection. Unlike the Top-K gating mechanism, the entropy-constrained mechanism does not require a predefined hyperparameter $K$, allowing for adaptive selection of the number of experts, significantly reducing hyperparameter tuning complexity while maintaining high performance. These advantages demonstrate that the entropy-constrained mechanism offers significant benefits in balancing flexibility, robustness, and practicality, effectively improving the performance of the \model framework across various graph datasets.


\begin{figure}[htbp]
  \centering
  \begin{minipage}[b]{0.45\linewidth}
      \centering
      \includegraphics[width=\textwidth, page=1]{Fig-5-efficiency-arxiv.pdf}
  \end{minipage}
  \hspace{0.2cm}
  \begin{minipage}[b]{0.45\linewidth}
      \centering
      \includegraphics[width=\textwidth, page=1]{Fig-5-efficiency-Penn94.pdf}
  \end{minipage}
  \caption{Efficiency analysis on ogbn-arxiv and Penn94.}
  \label{fig:efficiency}
\end{figure}
\begin{figure}[!htb]
  \centering
  \includegraphics[width=\linewidth]{Fig-6-oversmooth.pdf}
  \caption{Impact of model depth.}
  \label{fig: over}
\end{figure}

\subsubsection{Analysis of Entropy-guided Soft Gating Sharpening Mechansim}
Our \model framework introduces an entropy-constrained component, which enables nodes with varying neighborhood homophily levels to more flexibly select aggregation methods suited to their characteristics. To validate the effectiveness of this component, we analyzed the gating weight distributions of all nodes across 12 benchmark datasets for both \model and \model(w/o Entropy Constrained) under optimal parameter configurations, as shown in Figure~\ref{fig:entropy}. The experimental results demonstrate that: (1) In most datasets, the overall distribution trend of expert gating remains stable after introducing the entropy-constrained component, while the mechanism enables nodes to fine-tune their gating weight distributions to better adapt to their specific local aggregation needs; (2) In homophilic datasets, the gating weights exhibit a pronounced sharpening trend, whereas in heterophilous datasets, the gating values show diverse distribution patterns. These findings indicate that the entropy-constrained component effectively adapts to the characteristics of different graph structures, providing nodes with more adaptive aggregation strategies. (3) Unlike traditional Top-K mechanisms, our proposed entropy-constrained component does not require predefining the hyperparameter K. Instead, it adaptively selects one or multiple experts for aggregation based on node characteristics, eliminating the complexity of hyperparameter tuning while retaining the flexibility of the Top-K mechanism. This further enhances the practicality and generalization capability of the model by adaptively determining the number of experts.

\subsubsection{Efficiency Analysis}
Figure~\ref{fig:efficiency} illustrates the efficiency and accuracy of several representative methods on the ogbn-arxiv and Penn94 datasets, where the x-axis represents the number of epochs at which early stopping is triggered, the y-axis represents the total training time, and the bubble size reflects accuracy. On the ogbn-arxiv dataset, compared to the spatial-domain GNN method FSGNN and some GT-based methods, \model consumes 2-7 times less training time. As for the Penn94 dataset, \model converges in fewer epochs compared to traditional GNN methods and most spatial-domain GNN methods. In summary, \model demonstrates good computational efficiency while maintaining high performance.

\subsubsection{Impact on Model Depth}
Figure~\ref{fig: over} demonstrates the impact of model depth on performance. It is evident that Vanilla GNN's performance rapidly deteriorates as model depth increases, indicating the presence of over-smoothing. Meanwhile, H2GNN's performance gradually declines as model depth increases from 2 to 8 layers, and encounters memory overflow when model depth exceeds 16 layers. In contrast, our method maintains consistently stable performance while stacking PTblock message passing modules, demonstrating its immunity to the over-smoothing problem.

\section{Related Work}
\subsection{Heterophilous Graph Neural Networks}  
Previous research has extensively studied the graph heterogeneity matching problem. Some approaches, like \textbf{H2GCN}~\cite{H2GCN2020}, aggregate information from first- and second-hop neighbors based on spectral graph theory. \textbf{GPRGNN} adaptively optimizes generalized PageRank weights to control layer-wise propagation contributions. \textbf{FAGCN}~\cite{fagcn2021} and \textbf{ACMGCN}~\cite{luan2022revisiting} use multi-channel filters to capture local information. However, these methods struggle to adapt to complex heterogeneous feature distributions, limiting their performance in heterogeneous scenarios. \textbf{Exphormer}~\cite{exphormer} features a sparse graph Transformer architecture, tackling the scalability issues of traditional graph Transformers on large datasets.  Utilizing virtual global nodes and expander graphs, it enables a sparse attention mechanism with linear complexity, boosting scalability remarkably.
\textbf{Difformer}~\cite{wu2023difformer} offers a new neural network architecture for modeling intricate data instance dependencies.  It uses an energy-constrained diffusion model to encode instances as evolving states step by step, optimizing a regularized energy function to find the best diffusion strength and realize globally consistent representation learning.

Spatial-domain methods improve heterogeneous scene representation by adjusting graph structures and refining message-passing rules. \textbf{LINKX}~\cite{NEURIPS2021_ae816a80} aggregates the adjacency matrix and node features for representation learning, while \textbf{GloGNN}~\cite{li2022finding} introduces a node correlation matrix for global information. \textbf{FSGNN}~\cite{MAURYA2022101695} adaptively aggregates neighbor information from different hops. However, these approaches require computing new topologies or similarity matrices, leading to high computational and memory costs. 

As a type of specialized GNN, the Graph Transformer(GT) uses global self-attention to capture higher-order homogeneous information, improving target node representation. \textbf{ANS-GT}~\cite{ASN-GT} captures long-range dependencies via adaptive node sampling and hierarchical attention. \textbf{NAGFormer}~\cite{NAGphormer} handles large graphs by treating nodes as feature sequences from neighbors at various hops. \textbf{SGFormer}~\cite{SGFormer} simplifies this with a single-layer global attention mechanism, achieving linear complexity for large graphs. However, global self-attention may introduce irrelevant global noise, limiting performance on homogeneous graphs. Additionally, the high computational complexity of self-attention restricts GT’s scalability in practical applications.


\subsection{MoE for GNNs}
% 混合专家（Mixture-of-Experts, MoE）机制作为一种增强图神经网络（GNN）适应性的重要方法，在处理具有不同同质性水平的图结构数据方面展现了巨大潜力。[NCGNN]提出了邻域混乱度（NC）指标，引导模型针对不同目标实施“分而治之”的策略。从 MoE 的视角来看，NCGNN 将 NC 指标用作门控机制，以分离学习架构作为专家网络，并通过设定阈值来实现负载均衡，从而在一定程度上减轻了偏差问题。[GMoE]将不同 hop size 的图卷积作为聚合专家，同时通过辅助损失函数实现负载均衡。[Mowst]通过将轻量级的多层感知机作为弱专家和图神经网络作为强专家相结合，并引入基于弱专家预测结果离散度的“置信度”机制，以适应不同目标节点的专家协作。
Recent advances in graph representation learning have witnessed the growing adoption of the MoE paradigm to enhance the adaptability of GNNs. A seminal work by \textbf{NCGNN}~\cite{NCGNN} proposed the Neighborhood Confusion (NC) metric to quantify structural heterogeneity within node neighborhoods, establishing a separated learning strategy through an MoE framework. Their approach employs the NC metric as a gating mechanism to route nodes to specialized expert networks while maintaining load balance via threshold-based regularization, effectively addressing potential bias in expert utilization. Building upon this foundation, \textbf{GMoE}~\cite{GMoE} advanced the field by integrating multi-scale graph convolutions with varying receptive fields as aggregation experts, complemented by an auxiliary loss function to ensure balanced expert participation across diverse structural patterns. Further extending the MoE paradigm, \textbf{MOWST}~\cite{mowst} introduced a hierarchical expert architecture that combines lightweight Multi-Layer Perceptrons (MLPs) as weak experts with GNNs as strong experts, dynamically coordinating their collaboration through a confidence-aware mechanism based on prediction variance. While these approaches have demonstrated promising results in adapting to graph heterogeneity, limitations persist in the flexibility of expert specialization and the granularity of gating mechanisms, particularly in complex real-world graphs with diverse homophily patterns. These insights form the theoretical foundation and technical motivation for our work.



\section{Conclusion}

% 我们结合GNN和GT的优势，设计了一个通用的节点分类模型架构\model。该架构将自适应的消息传递封装为专家网络块，可以提供面向不同图类型的灵活编码能力。在12个基准数据集上的大量实验表明，相比于空域、谱域GNN以及GT方法，我们的架构在节点分类性能上具有优势，同时可以适应不同类型的图，展现出通用性。此外，面对一些固有的限制如过平滑和低效问题，我们的架构也能有效克服。未来，我们将对现有架构进一步完善，一方面优化门控网络的输入和结构，另一方面面向更多领域的图结构数据进行探索。

In this work, we introduce GNNMoE, a versatile and scalable framework for node classification that overcomes the limitations of existing GNNs when handling graphs with varying levels of homophily and heterophily.  By decoupling the message-passing process into fine-grained propagation (P) and transformation (T) operations, we construct a diverse set of message-passing experts, allowing for adaptive, node-specific encoding strategies.  The combination of soft and hard gating mechanisms, coupled with an entropy-constrained sharpening mechanism, enables the model to dynamically balance Mean and Top-K selection, effectively addressing the diverse challenges posed by homophilous and heterophilous graph structures.
Extensive experiments across 12 benchmark datasets demonstrate that GNNMoE consistently outperforms state-of-the-art GNNs, heterophilous GNNs, and Graph Transformers in terms of classification accuracy, robustness, and computational efficiency.  
Looking ahead, we plan to enhance the gating mechanism by incorporating more sophisticated structural information and expanding the framework’s applicability to other graph-related tasks, such as link prediction and graph classification.  Furthermore, we aim to extend GNNMoE to dynamic graphs, where the graph structure evolves over time, to further increase its applicability in real-world scenarios.
\begin{acks}
  This work was supported in part by the Key R\&D Program of Zhejiang under Grants 2022C01018, by the National Natural Science Foundation of China under Grants U21B2001.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.
\newpage
% \section{Appendix A}
\section{Dataset Details}\label{app: dataset}
\begin{itemize}[leftmargin=10pt]
    \item Computers and Photo~\cite{mcauley2015I} are segments of the Amazon co-purchase graph, where nodes represent products, edges represent the co-purchased relations of products, and features are bag-of-words vectors extracted from product reviews.
    \item Coauthor CS and Coauthor Physics~\cite{Shchur2018PitfallsOG} are co-authorship graphs based on the Microsoft Academic Graph from the KDD Cup 2016 challenge, where nodes represent authors, edge represent the corresponding authors have co-authored a paper, features consist of keywords from each author's published papers, and the class labels denote the most active research fields for each author.
    \item Facebook~\cite{rozemberczki20201R} is a page-page graph of verified Facebook sites, where nodes correspond to official Facebook pages, links to mutual likes between sites, and features are extracted from the site descriptions. 
    \item ogbn-arxiv~\cite{hu2020open} is a network dataset designed for predicting the subject areas of computer science arXiv papers. Each node represents a paper, and the directed edges indicate citation relationships between papers. The node features are 128-dimensional vectors obtained by averaging the word embeddings of the paper's title and abstract, where the embeddings are generated using the Skip-gram model over the MAG corpus. The task is to predict one of 40 subject areas (e.g., cs.AI, cs.LG) that are manually assigned by paper authors and arXiv moderators. The dataset is split by publication date, with training on papers published until 2017, validation on papers published in 2018, and testing on papers published since 2019.
    \item Actor~\cite{tang2009S} is a network dataset designed for analyzing co-occurrence relationships among actors, where node represents an actor, and the edges between nodes indicate their co-occurrence on the same Wikipedia page. The node features are constructed from keywords extracted from the respective actors' Wikipedia pages.
    \item Chameleon-fix and Squirrel-fix~\cite{platonov2023a} are two page-page networks focusing on specific topics in Wikipedia, where nodes represent web pages, and edges denote mutual links between the pages. The node features are composed of informative nouns extracted from the corresponding Wikipedia pages. The task of these datasets is to categorize the nodes into five distinct groups based on the average monthly traffic received by each web page.
    \item Tolokers~\cite{platonov2023a} is a social network extracted from the Toloka crowdsourcing platform, where nodes represent workers and two workers are connected if they participate in the same task. The node features are constructed from the workers' profile information and task performance statistics, while the labels indicate whether a worker is banned in a project.
    \item Roman-empire~\cite{platonov2023a} is derived from the Roman Empire article on Wikipedia, where nodes in the dataset represent words from the article, edges indicating word dependencies. The node features are constructed from word embeddings obtained using the FastText method, and labels denote the syntactic roles of the words.
    \item Penn94~\cite{NEURIPS2021_ae816a80} is a Facebook social network, where nodes denote students and are labeled with the gender of users, edges represent the relationship of students. Node features
    are constructed from basic information about students which are major, second major/minor, dorm/house, year and high school.
\end{itemize}
\begin{table}[!htb]
\renewcommand\arraystretch{1.3}
\centering
\caption{Summary of datasets used}
\label{tab: datasets}
\resizebox{\linewidth}{!}{
    \begin{tabular}{c|cccc} 
\hline
                 & Node Feature & Node Number & Edges   & Classes  \\ 
\hline
Computers        & 767          & 13,752       & 491,722  & 10       \\
Photo            & 745          & 7,650        & 238,162  & 8        \\
Coauthor CS      & 6,805         & 18,333       & 163,788  & 15       \\
Coauthor Physics & 8,415         & 34,493       & 495,924  & 5        \\
Facebook         & 128          & 22,470       & 342,004  & 4        \\
ogbn-arxiv       & 128          & 169,343      & 1,166,243 & 40       \\
Actor            & 932          & 7,600        & 30,019   & 5        \\
Chameleon-fix    & 2,325         & 890         & 13,584   & 5        \\
Squirrel-fix     & 2,089         & 2,223        & 65,718   & 5        \\
Tolokers         & 10           & 11,758       & 1,038,000 & 2        \\
Roman-empire     & 300          & 22,662       & 65,854   & 18       \\
Penn94           & 4,814         & 41,554       & 2,724,458 & 2        \\
\hline
\end{tabular}
}
\end{table}
\section{Baseline Details}\label{app: baselines}
% % Table~\ref{tab:bestparameters}展示了GNNFormer框架下的最优参数，其中包括learning rate，weight decay和dropout三个基本参数，同时也展示了最优model type和\mathbf{P}操作类型。
% Table~\ref{tab:bestparameters} presents the optimal parameters for the GNNFormer framework, encompassing the learning rate, weight decay, and dropout as the key hyperparameters. Additionally, it highlights the optimal model type and the $\mathbf{P}$ operation type.
      
\begin{itemize}[leftmargin=10pt]
    \item \textbf{MLP} is a two-layer linear neural network that based on the original features of the nodes, without any propagation or aggregation rules.
    \item \textbf{GCN} is a neural network that aggregates information among neighboring nodes through message passing.
    \item \textbf{GAT} is a neural network that leverages multi-head attention to weight node features effectively on graph data.
    \item \textbf{SAGE} is a graph neural network that learns node representations by sampling and aggregating neighborhood information.
    \item \textbf{H2GCN} constructs a neural network by separating ego and neighbor embeddings, aggregating higher-order neighborhood information, and combing intermediate representations.
    \item \textbf{GPRGNN} is a graph neural network that optimizes node feature and topology extraction by adaptively learning Generalized PageRank weights.
    \item \textbf{FAGCN} is a novel graph convolutional network that integrates low and high-frequency signals through an adaptive gating mechanism.
    \item \textbf{ACMGCN} adaptively employs aggregation, diversification, and identity channels to extract richer local information for each node at every layer.
    \item \textbf{GloGNN} generates node embeddings by aggregating global node information and effectively captures homophily by learning a correlation matrix between nodes.
    \item \textbf{FSGNN} is a simplified graph neural network model that enhances node classification performance by introducing a soft selection mechanism.
    \item \textbf{LINKX} combines independent embeddings of the adjacency matrix and node features, generating predictions through a multi-layer perceptron and simple transformations.
    \item \textbf{ANS-GT} is a graph transformer architecture that effectively captures long-range dependencies and global context information through adaptive node sampling and hierarchical graph attention mechanisms.
    \item \textbf{NAGFormer} is a novel graph transformer that handles node classification tasks on large graphs by treating each node as a sequence aggregated from features of neighbors at various hops.
    \item \textbf{SGFormer} is a simplified and efficient graph transformer model that handles large-scale graph data through a single-layer global attention mechanism, achieving node representation learning with linear complexity.
    \item \textbf{Exphormer} is a novel sparse graph Transformer architecture designed to address the scalability issues faced by traditional graph Transformers when handling large-scale graph data. By introducing virtual global nodes and expander graphs, it achieves a sparse attention mechanism with linear complexity, demonstrating enhanced scalability on large-scale datasets.
    \item \textbf{Difformer} is a novel neural network architecture for learning complex dependencies between data instances. It uses an energy-constrained diffusion model to encode instances as dynamically evolving states, progressively integrating information. By optimizing a regularized energy function, the model derives the optimal diffusion strength between instances, enabling globally consistent representation learning.
\end{itemize}
\section{More Parameter Settings}\label{app: parameter}
    % 我们使用了NNI（Neural Network Intelligence)调参工具完成对Baseline的实验。采用与我们的方法相同的基础参数与一些Baseline各自的特殊参数进行实验。特殊参数的总结如下：
    We used the Neural Network Intelligence (NNI) tool for hyper-parameter tuning to conduct experiments on the baseline models. The experiments were conducted using the same base parameters as our method, along with specific parameters unique to each baseline model. The special parameters are as follows:
    % \begin{table*}[!htb]
    %   \renewcommand\arraystretch{1.3}
    %   \centering
    %   \caption{Optimal parameters for GNNFormer}
    %   \label{tab:bestparameters}
    %   \resizebox{\textwidth}{!}{
    %       \begin{tabular}{c|cccccccccccc} 
    % \hline
    % \multicolumn{1}{l|}{} & Computers  & Photo      & Coauthor CS & Coauthor Physics & Wiki-CS     & Facebook    & Actor       & Chameleon-fix & Squirrel-fix & Tolokers   & Roman-empire & Penn94      \\ 
    % \hline
    % P-like                & GCN-like P & GAT-like P & GCN-like P  & GCN-like P       & SAGE-like P & SAGE-like P & SAGE-like P & SAGE-like P   & GAT-like P   & GCN-like P & SAGE-like P  & GCN-like P  \\
    % model type            & TPTP       & TPTP       & TTPP        & TPTP             & TPTP        & TPTP        & PTPT        & TTPP          & PPTT         & PTPT       & TPTP         & TPTP        \\
    % learning rate         & 5e-3       & 5e-3       & 1e-2        & 5e-3             & 5e-3        & 5e-3        & 5e-3        & 5e-3          & 1e-2         & 5e-2       & 5e-3         & 5e-3        \\
    % weight decay          & 5e-4       & 1e-5       & 1e-4        & 5e-5             & 5e-4        & 1e-4        & 1e-4        & 5e-4          & 5e-4         & 5e-3       & 5e-4         & 5e-5        \\
    % dropout               & 0.7        & 0.5        & 0.1         & 0.7              & 0.3         & 0.5         & 0.5         & 0.9           & 0.5          & 0.3        & 0.3          & 0.5         \\
    % \hline
    % \end{tabular}
    %   }
    % \end{table*}
    \begin{itemize}[leftmargin=10pt]
      \item GloGNN: norm\_layers $\in \{1,2,3\}$, orders  $\in\{2,3,4\}$, term weight $\in\{0,1\}$, weighting factor  $\in\{0,1,10\}$ and $\{0.1,1,10,100,1000\}$ and the balanced term parameters.
      \item FSGNN: aggregator $\in\{cat, sum\}$.
      \item ANS-GT: data\_augmentation $\in\{4,8,16,32\}$, n\_layer $\in\{2,3,4\}$ and batch size $\in\{8,16,32\}$.
      \item NAGFormer: hidden $\in\{128,256,512\}$, number of Transformer layers $\in\{1,2,3,4,5\}$ and number of propagation steps $\in\{7,10\}$.
      \item SGFormer: number of global attention layers is fixed as 1, number of GCN layers $\in\{1,2,3\}$, weight $\alpha$ $\in\{0.5,0.8\}$.
  \end{itemize}
  % \section{More Experimental Results}\label{app: result}
  % Table~\ref{tab:gate_all} presents a comprehensive comparison of the gating mechanisms within the \model framework. Specifically, the table summarizes the experimental results of three gating strategies: Learning Parameters, Mean, and Top-K, under three distinct message-passing operator configurations. These findings provide a detailed evaluation of the performance of each gating mechanism, offering valuable insights for further analysis and comparison.
\end{document}
% \endinput
%%
%% End of file `sample-sigconf.tex'.
