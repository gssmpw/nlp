\section{Related Work}
\label{sec:relate}
\noindent\textbf{3D Scene Reconstruction.} 3D scene reconstruction technology has evolved rapidly in autonomous driving. Early studies focused on mathematical methods~\cite{MonoScene} and camera parameters~\cite{3D_reconstruction_camera_para_1,3D_reconstruction_camera_para_2,3D_reconstruction_camera_para_3} to facilitate 3D scene reconstruction from 2D inputs. MonoScene~\cite{MonoScene} explored the path of scene reconstruction from 2D to 3D using monocular RGB images, setting a groundbreaking benchmark. Later, more learning methods were applied. 1) Multi-view reconstruction~\cite{Multi_View_reconstruction_1, Multi_View_reconstruction_2}, which integrates multiple perspectives for enhanced depth estimation. 2) Multimodal methods combining RGB with depth~\cite{multimodal_rgb_depth_1, multimodal_rgb_depth_2} or RGB with LiDAR~\cite{multimodal_rgb_lidar_1} data, improving the robustness and precision of reconstructions. 

\noindent\textbf{3D VONs.} 3D VONs extend the capabilities of 3D scene reconstruction technologies to enhance dynamic and precise environmental perception for autonomous vehicles. This task involves learning the associations between pixel-level visual cues and the occupancy states of 3D voxels. Recent approaches often employ Transformers~\cite{attention_is_all_you_need} to effectively model these complex relationships, achieving significant advancements in accuracy and efficiency. 3D occupancy prediction have utilized multiple data inputs such as LiDAR~\cite{Panoptic_PolarNet,PointOcc} and depth data~\cite{S3CNet_depth,Li_2019_CVPR_depth,Li_2020_CVPR_depth,See_and_think_depth,Li_Zou_Li_Zhao_Gao_2020_depth,Depth_Based_Semantic_Scene_depth,Two_Stream_3D_Semantic_Scene_Completion_depth} to predict voxel occupancy. However, with advancements in camera technology and image processing, current research predominantly adopts an image-based end-to-end approach~\cite{surroundOcc,TPVFormer,VoxFormer,selfocc,RenderOcc}. These modern methods use data from single frames to predict occupancy, focusing on integrating rich semantic information at lower costs. 

\noindent\textbf{History-aware VONs.} Recent advances in spatio-temporal perception have extended 3D VONs into temporal domains, primarily through two paradigms: (i) 4D VONs that estimate current and future states (e.g., Cam4DOcc~\cite{Cam4docc}, OccSora~\cite{OccSora}), and (ii) history-aware VONs~\cite{fb_occ,SparseOcc_Liu,bevdet4d,opus} leverage historical features to refine current-frame predictions. In this paper, we focus primarily on the second category, where existing approaches may lead to spatio-temporal redundancy when fusing historical features~\cite{opus} or fail to establish fine-grained correlations between pixel-level visual cues and occupied voxels~\cite{bevdet4d}. To address this issue, we propose a solution by introducing a lightweight, plug-and-play module that effectively discriminates and extracts relevant motion-static attention, thereby complementing the current occupancy outcome.

