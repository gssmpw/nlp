\section{Experiments}
\label{sec:exp}
\subsection{Experimental settings}

\noindent\textbf{Benchmark.}  We conduct experiments on two established 3D occupancy benchmarks: (i) nuScenes~\cite{nuScenes}, which provides instance-level annotations with manually labeled 3D bounding boxes (position/size/orientation) for dynamic objects, and (ii) Occ3D~\cite{Occ3D}, which generates voxel-level occupancy labels (0.4m resolution) through automated LiDAR point cloud aggregation and mesh reconstruction, including occlusion states. Both benchmarks share identical scene configurations of 1,050 driving scenes, each containing up to 40 timestamped frames. Every frame includes six synchronized camera views (front, front-left, front-right, back, back-left, back-right) at 1600$\times$900 resolution. In our experiments, we extend single-frame baselines~\cite{MonoScene,surroundOcc,viewformer} by aggregating features from $N$ historical keyframes. Additionally, we extract unlabeled intermediate frames from the ``sweeps'' folder~\cite{nuScenes} to provide implicit motion cues, enabling self-supervised temporal consistency learning.

\noindent\textbf{Implementation details.} For the nuScenes benchmark~\cite{nuScenes}, we follow the parameter settings of SurroundOcc~\cite{surroundOcc}, using $Cam=6$, $p=6$, $v=32$,$L=116$, and $W=200$. For the Occ3D benchmark~\cite{Occ3D}, we adopt ViewFormer's~\cite{viewformer} standard setup with $Cam=6$, $p=6$, $v=32$, $L=32$, and $W=88$. The output of the occupancy result on both benchmarks is formatted into a vector with dimensions $[200, 200, 16]$. In this vector, the first two dimensions (200 and 200) represent the length and width, while the third (16) indicates the height. The occupancy result covers a range from -50 meters to 50 meters in both width and length, and the vertical height varies from -5 meters to 3 meters. Each voxel corresponds to a cube measuring 0.5 meters on each side. Occupied voxels are categorized into one of 17~\cite{nuScenes,surroundOcc} and 18~\cite{Occ3D} semantic classes.
More details on implementation can be found in the supplementary material.

\subsection{Evaluation Metrics}

To validate the temporal consistency and occupancy accuracy of moving and static objects, objects are divided into two general classes~\cite{Cam4docc}: General Moving Objects (GMO) and General Static Objects (GSO). Detailed classification classes are introduced in the supplementary material.

\noindent\textbf{Occupancy Accuracy Metric.} To ensure rigorous evaluation across different benchmarks, we employ both Intersection over Union (IoU) and Mean Intersection over Union (mIoU) metrics. These metrics are widely adopted in 3D semantic occupancy prediction tasks~\cite{PASCAL, Microsoft_COCO, Cityscapes_dataset, Mask_R_CNN}. The mIoU are calculated separately for three category groups: All classes, GMO classes, and GSO classes.

\noindent\textbf{Temporal Consistency Metric.} \label{para:consistency_metric} To evaluate the effect achieved by integrating \ours\ with baseline models, we propose a temporal consistency metric. We aim to detect and measure changes in a scene from one frame to the next. This metric reflects the stability of prediction results, which directly impacts the user's visual experience. Let $\sigma_{i,n}^{(x,y,z)}$ denote the semantic label of the $n$-th voxel point (with coordinates $(x,y,z)$) in frame $i$, and define the indicator function $\delta(e_1,e_2) = \mathbb{I}(e_1 \neq e_2)$. 

In the occupancy results of frames $i$ and $j$, voxels at corresponding positions may undergo changes, which are categorized into two types: ``Static Object Change"~(SOC) and ``Moving Object Change"~(MOC). The definitions of these changes are as table \ref{tab:moc-soc}.

\begin{wrapfigure}[7]{l}{80mm}
\centering
% \setlength{\tabcolsep}{8pt}
\captionsetup{type=table}
    \begin{tabular}{c|c}
    \toprule
    \textbf{Type} & \textbf{Condition} \\  
    \midrule
    MOC & $\sigma_{i,n}^{(x,y,z)} \in \text{GMO} \lor \sigma_{j,n}^{(x,y,z)} \in \text{GMO}$ \\
    % \midrule
    SOC & $\sigma_{i,n}^{(x,y,z)} \wedge \sigma_{j,n}^{(x,y,z)} \in \text{GSO}$ \\  
    \bottomrule
    \end{tabular}
    \vspace{8pt}
    \caption{Definition of MOC and SOC. $N_{mc}$/$N_{sc}$ denote the number of MOC/SOC voxels, respectively.}
    \label{tab:moc-soc}
    % \vspace{-10pt}
\end{wrapfigure}

Based on these definitions, we can define disparity metrics~($\Delta_{m}$/$\Delta_{s}$) to quantify temporal inconsistencies across frames~($i$ and $j$). The process is defined as:
\begin{equation}
\begin{dcases}
\Delta_{m}(i,j) = \dfrac{1}{N_{mc}} \sum\limits_{n=1}^{N_{mc}} \delta\left(\sigma_{i,n}^{(x,y,z)}, \sigma_{j,n}^{(x,y,z)}\right) \\
\Delta_{s}(i,j) = \dfrac{1}{N_{sc}} \sum\limits_{n=1}^{N_{sc}} \delta\left(\sigma_{i,n}^{(x,y,z)}, \sigma_{j,n}^{(x,y,z)}\right).
\end{dcases}
\label{eq:disparity}
\end{equation}

The temporal consistency metrics -- $S_m$ (moving) and $S_s$ (static) -- are derived through aggregation of $\Delta_{m}$ and $\Delta_{s}$ across sequential frames. Formally, we have:
\begin{equation}
S_{m/s} = 1 - \dfrac{1}{M-1} \sum\limits_{k=1}^{M-1} \Delta_{m/s}(k,k+1),
\label{eq:consistency_scores}
\end{equation}
where $M$ is the scene's total frame count. Final metrics $\overline{S_m}$/$\overline{S_s}$ average across all scenes. A higher temporal consistency score indicates that the predictions within the scene are smoother and more consistent over time.

\begin{table}[t]
    % \centering
    \footnotesize 
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        % \captionsetup{justification=centering, singlelinecheck=false}
        \setlength{\tabcolsep}{2pt}
        \renewcommand{\arraystretch}{1.25}
        \begin{tabular}{r|cccc|cc}
            \toprule
            \multicolumn{1}{r|}{\multirow{2}{*}[-0.4em]{Method}} & \multicolumn{1}{c|}{\multirow{2}{*}[-0.4em]{IoU~$\uparrow$}} & \multicolumn{3}{c|}{mIoU~$\uparrow$} & \multicolumn{1}{c}{\multirow{2}{*}[-0.4em]{$\overline{S_m}\uparrow$}} & \multicolumn{1}{c}{\multirow{2}{*}[-0.4em]{$\overline{S_s}\uparrow$}} \\ \cmidrule(lr){3-5}
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & All & GMO & GSO & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ 
            \midrule        
            Atlas~\cite{Atlas} & 28.66 & 15.00 & 12.64 & 17.35 & \text{--} & \text{--}  \\
            BEVFormer{~\cite{BEVFormer}} & 30.50 & 16.75 & 14.17 & 19.33 & \text{--} & \text{--}  \\
            TPVFormer~\cite{TPVFormer} & 30.86 & 17.10 & 14.04 & 20.15 & \text{--} & \text{--} \\
            BEVDet4D-Occ~\cite{bevdet4d} & 24.26 & 14.22 & 11.10 & 17.34 & \text{--} & \text{--} \\
            MonoScene~\cite{MonoScene} & 10.04 & 1.15 & 0.24 & 2.07 & 46.53 & 81.77 \\
            % Cam4DOcc~\cite{Cam4docc} & 23.92 & 7.12 & 4.71 & 10.17 & 60.34 & 91.15 \\
            SurroundOcc~\cite{surroundOcc} & 31.49 & 20.30  & \cellcolor{gray!20}18.39 & 22.20 & 58.33 & 91.71 \\
            \midrule
            \makecell[r]{MonoScene \\ \textbf{+\ours}} & \makecell{13.10\\\textbf{+3.06}} & \makecell{1.69\\\textbf{+0.54}}  & \makecell{0.34\\\textbf{+0.10}} & \makecell{3.04\\\textbf{+0.98}} & \makecell{54.21\\\textbf{+7.68}} & \makecell{83.84\\\textbf{+2.07}} \\
            \midrule
            \makecell[r]{SurroundOcc \\ \textbf{+\ours}} & \cellcolor{gray!20}\makecell{33.12 \\ \textbf{+1.63}} & \cellcolor{gray!20}\makecell{20.67\\\textbf{+0.37}}  & \makecell{18.26\\-0.13} & \cellcolor{gray!20}\makecell{23.08\\\textbf{+0.88}} & \cellcolor{gray!20}\makecell{60.64\\\textbf{+2.31}} & \cellcolor{gray!20}\makecell{92.54\\\textbf{+0.83}} \\
            \bottomrule
        \end{tabular}
        \vspace{2mm}
        \caption{Occupancy prediction accuracy on \textbf{nuScenes benchmark~\cite{nuScenes}}. For a fair comparison, we ensure that all models have uniform input data. The best performance is highlighted in gray.}
        \label{tab:main-res-a}
    \end{minipage}\hfill
    \begin{minipage}[t]{0.48\textwidth}
        \centering
        % \captionsetup{justification=centering, singlelinecheck=false}
        \setlength{\tabcolsep}{2pt}
        \begin{tabular}{r|cccc|cc}
            \toprule
            \multicolumn{1}{r|}{\multirow{2}{*}[-0.4em]{Method}} & \multicolumn{1}{c|}{\multirow{2}{*}[-0.4em]{IoU~$\uparrow$}} & \multicolumn{3}{c|}{mIoU~$\uparrow$} & \multicolumn{1}{c}{\multirow{2}{*}[-0.4em]{$\overline{S_m}\uparrow$}} & \multicolumn{1}{c}{\multirow{2}{*}[-0.4em]{$\overline{S_s}\uparrow$}} \\ \cmidrule(lr){3-5}
            \multicolumn{1}{c|}{} & \multicolumn{1}{c|}{} & All & GMO & GSO & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ 
            \midrule    
            MonoScene~\cite{MonoScene} & \text{--} & 6.06 & 5.36 & 6.68 & \text{--} & \text{--} \\
            OccFormer~\cite{OccFormer} & \text{--} & 21.93 & 21.78 & 22.06 & \text{--} & \text{--} \\
            % CTF-Occ~\cite{Occ3D} & 28.53 & 27.42 & 29.52 & \text{--} & \text{--} \\
            FB-OCC~\cite{fb_occ} & \text{--} & 39.11  & 33.74 & 43.88 & \text{--} & \text{--} \\
            SparseOcc~\cite{SparseOcc_Liu} & \text{--} & 30.10  & \text{--} & \text{--} & \text{--} & \text{--} \\
            BEVDet4D-Occ~\cite{bevdet4d} & \text{--} & 39.30  & 29.09 & 42.16 & \text{--} & \text{--} \\ 
            OPUS-L~\cite{opus} & \text{--} & 36.20  & 31.25 & 40.44 & \text{--} & \text{--} \\      
            SurroundOcc~\cite{surroundOcc} & 51.89 & 7.24  & 0.36 & 13.35 & 65.35 & 89.54 \\
            ViewFormer~\cite{viewformer} & 70.39 & 40.46  & 33.73 & 46.45 & 67.26 & 86.06 \\
            \midrule
            \makecell[r]{SurroundOcc \\ \textbf{+\ours}} & \makecell{52.13\\\textbf{+0.24}}& \makecell{10.33\\\textbf{+3.09}}  & \makecell{1.98\\\textbf{+1.62}} & \makecell{17.76\\\textbf{+4.41}} & \makecell{69.60\\\textbf{+4.25}} & \cellcolor{gray!20}\makecell{90.91\\\textbf{+1.37}} \\
            \midrule
            \makecell[r]{ViewFormer \\ \textbf{+\ours}} & \cellcolor{gray!20}\makecell{70.63\\\textbf{+0.24}} & \cellcolor{gray!20}\makecell{41.30\\\textbf{+0.84}}  & \cellcolor{gray!20}\makecell{34.33\\\textbf{+0.60}} & \cellcolor{gray!20}\makecell{47.50\\\textbf{+1.05}} & \cellcolor{gray!20}\makecell{70.13\\\textbf{+2.87}} & \makecell{87.10\\\textbf{+1.04}} \\
            \bottomrule
        \end{tabular}
        \vspace{2mm}
        \caption{Occupancy prediction accuracy on \textbf{Occ3D benchmark~\cite{Occ3D}}. For a fair comparison, we ensure that all models have uniform input data. The best performance is highlighted in gray.}
        \label{tab:main-res-b}
    \end{minipage}
    \vspace{-3mm}
    % \caption{Occupancy prediction accuracy on two benchmarks. The best performance is highlighted in gray.}
    \label{tab:main-res}
    \vspace{-5mm}
\end{table}

\subsection{Comparison Results}

\noindent\textbf{Occupancy accuracy on nuScenes.} We compare our method against several SOTA models, including Atlas~\cite{Atlas}, BEVFormer~\cite{BEVFormer}, TPVFormer~\cite{TPVFormer}, MonoScene~\cite{MonoScene}, and SurroundOcc~\cite{surroundOcc}. For a fair comparison, all methods are trained on the same ground truth and follow the same training procedure. By combining methods such as MonoScene~\cite{MonoScene} and SurroundOcc~\cite{surroundOcc} with \ours, we evaluate the effect of \ours\ in performance enhancement. The results presented in \cref{tab:main-res-a} show that our performance improvement is significant. Notably, the incorporation of \ours\ into SurroundOcc~\cite{surroundOcc} has led to improved metrics that surpass those of all other models listed in this table. The results are improved by 1.63\% and 0.37\% compared with SurroundOcc~\cite{surroundOcc} in IoU and mIoU~(All), respectively.

\noindent\textbf{Occupancy accuracy on Occ3D.} We also conduct experiments on Occ3D~\cite{Occ3D} in \cref{tab:main-res-b}. To validate \ours, we conducted two sets of experiments: First, integrating \ours\ with the 3D VONs~\cite{surroundOcc,viewformer} improved one of the original models'~\cite{viewformer} performance by 0.24\% in IoU and 0.84\% in mIoU. Second, \ours\ consistently outperforms existing history-aware VONs~\cite{opus,bevdet,SparseOcc_Liu,fb_occ} by over 2\% mIoU, demonstrating the efficacy of the \ours.


\noindent\textbf{Temporal Consistency.} The results of $\overline{S_m}$ and $\overline{S_s}$ shown in \cref{tab:main-res} indicate that the integration of \ours\ improved the temporal consistency of occupancy across all frames in all scenes for all models, demonstrating \ours' effectiveness. This enhancement can be attributed to the incorporation of previous keyframes from the dataset~\cite{nuScenes,Occ3D}, along with the addition of intermediate frames from the ``sweeps''~\cite{nuScenes} directory for the SFE and MFE modules. These elements provide critical historical information and motion clues for the model.

\subsection{Ablation study}

Our ablation experiments are all conducted on the nuScenes benchmark~\cite{nuScenes}. The results are presented in~\cref{tab:ablation-studies}.

\begin{wrapfigure}{l}{90mm}
\centering
\captionsetup{type=table}
    \begin{subtable}[t]{0.48\textwidth}
    \centering
    \footnotesize
    \begin{tabular}{c|ccc|cccc}
    \toprule 
    Idx. & Pre & Cur & Mid & IoU$\uparrow$ & mIoU$\uparrow$ & $\overline{S_m}\uparrow$ & $\overline{S_s}\uparrow$ \\
    \midrule
    \textbf{M0} & \ding{55} & \ding{55} & \ding{55} & 31.49 & 20.30 & 58.33 & 91.71 \\
    \textbf{M1} & \ding{55} & \ding{51} & \ding{51} & 33.04 & 20.04 & 60.59 & 92.25 \\
    \textbf{M2} & \ding{51} & \ding{55} & \ding{51} & 33.05 & 19.98 & 60.09 & 92.44 \\
    \textbf{M3} & \ding{51} & \ding{51} & \ding{55} & 32.88 & 20.10 & 60.24 & 92.24 \\
    \textbf{M4} & \ding{51} & \ding{51} & \ding{51} & 31.97 & 20.11 & 60.19 & 92.01 \\
    \textbf{M5} & \ding{51} & \ding{51} & \ding{51} & \textbf{33.12} & \textbf{20.67} & \textbf{60.64} & \textbf{92.54} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \caption{Ablation study of \ours. \textbf{Cur}, \textbf{Pre} and \textbf{Mid} represent the $f_{cur}$, $f_{pre}$ and $f_{mid}$ input, respectively, in the MSI.}
    \label{tab:ablation-modules}
    \end{subtable}
% \hfill
    \vspace{2mm}
    
    \begin{subtable}[t]{0.48\textwidth}
    \centering
    \footnotesize
    \setlength{\tabcolsep}{7pt}
    \begin{tabular}{c|c|cccc}
    \toprule 
    Idx. & \makecell{Type of\\motion info.} & IoU~$\uparrow$ & mIoU~$\uparrow$ & $\overline{S_m}\uparrow$ & $\overline{S_s}\uparrow$ \\
    \midrule
    \textbf{I0} & -  & 31.49 & 20.30 & 58.33 & 91.71 \\
    \textbf{I1} & Raw Image  & 32.39 & 19.45 & 59.01 & 91.15 \\
    \textbf{I2} & Optical Flow  & 32.80 & 20.27 & 60.53 & 92.13 \\
    \textbf{I3} & Frame Diff.  & \textbf{33.12} & \textbf{20.67}  & \textbf{60.64} & \textbf{92.54} \\
    \bottomrule
    \end{tabular}
    \vspace{1mm}
    \caption{Effect of different types of motion information.}
    \label{tab:ablation-motion}
    \end{subtable}
    % \vspace{-2mm}
\caption{Ablation studies on \ours\ modules and motion information. Best results are \textbf{bolded}.}
\label{tab:ablation-studies}
\end{wrapfigure}

\noindent\textbf{Different combinations of \ours.} \label{para:aba-comb}\cref{tab:ablation-modules} presents the performance results of different combination of \ours's components for $N$=$1$. In \cref{tab:ablation-modules}, there are 6 different combinations: \textbf{M0} shows results from SurroundOcc~\cite{surroundOcc}, which represents the basic model without our method. \textbf{M1} means the model variant in which the part responsible for processing previous keyframes is removed, thereby excluding the input data \(F_{pre}^{1}\). \textbf{M2} refers to the model variant that omits the current feature \(F_{cur}\). \textbf{M3} indicates the model configuration that has \(F_{mid}^{1}\) removed. \textbf{M4} indicates that $I_{cur}$ is used to compute MHAM's query, while $I_{pre}$ and $I_{mid}$ are utilized to compute MHAM's key and value, which differs from the standard design. \textbf{M5} represents the full model with all components included.
% 验证了我们三种输入的必要性
\cref{tab:ablation-modules} clearly demonstrates that the removal of any single input from \ours\ module significantly reduces performance both in prediction accuracy and in temporal consistency. This validates the necessity of the three inputs. Furthermore, the comparison between \textbf{M4} and \textbf{M5} confirms that the cues provided by the previous keyframes and the intermediate frames are crucial for occupancy prediction.

\noindent\textbf{Impact of different types of motion information.} \label{para:motion-extracting} This experiment was conducted on the MFE module to investigate the effects of various types of motion information for $N=1$. The results are presented in \cref{tab:ablation-motion}. Specifically, \textbf{I0} served as the base model~\cite{surroundOcc} without using any motion information. \textbf{I1} employed raw intermediate frames as the input for the MFE. \textbf{I2} used optical flow~\cite{OpticalFlow} as the motion information input. \textbf{I3} used frame difference~\cite{Frame_difference} to capture motion information. It is clear that \textbf{I1} surpasses \textbf{I0} in terms of IoU metrics; however, it exhibits the lowest performance in mIoU, $\overline{S_m}$, and $\overline{S_s}$ metrics compared with \textbf{I1}, \textbf{I2}, and \textbf{I3}. This discrepancy is mainly because of the substantial amount of irrelevant information in the raw, intermediate frames, which complicates the extraction of motion features by the MFE. In addition, the results show that \textbf{I3} significantly outperforms \textbf{I2} in both IoU and mIoU metrics and slightly improves in $\overline{S_m}$ and $\overline{S_s}$ metrics. This indicates that frame difference more effectively captures sudden changes in a scene, such as the abrupt appearance of pedestrians or vehicles exiting intersections, while optical flow may experience delays in processing these sudden events. Furthermore, given the lightweight design of \ours, the frame difference method~\cite{Frame_difference} reduces data processing complexity by only processing simple differential data, thereby contributing to computing speed.

\begin{wrapfigure}[22]{l}{90mm}
    \centering
    \includegraphics[width=90mm]{assets/case_single_light.png}
    \caption{
    Several challenging scenarios are presented: pedestrians are partially occluded by vehicles in the first and second columns, and the road boundary appears visually obscure in the third column. Ours achieves more accurate predictions, while SOTA methods display significant artifacts.
    }
    \label{fig:case-extra}
\end{wrapfigure}

\noindent\textbf{Impact of different numbers of previous keyframes.}\label{para:ntrack} We conduct ablation experiments on $N$ to explore the performance of the model when $N$=$0$, $N$=$1$ and $N$=$2$. $N$=$0$ represents SurroundOcc~\cite{surroundOcc}, which does not use any previous keyframes. Detailed experiment results are documented in the supplementary material.

\begin{figure}[!t]
\centering
% \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
\includegraphics[width=\linewidth]{assets/case_big_light.png}
% \vspace{-7mm}
\caption{
Comparison under a T-junction scenario, where a pedestrian is partially and dynamically occluded in certain frames. Ours showcases robust predictions, with the pedestrian being consistently tracked, while SOTA methods show a flickering phenomenon.
}
\label{fig:case-study-big}
\vspace{-20pt}
\end{figure}

\subsection{Case analysis}
To visually evaluate the effectiveness of our method~(SurroundOcc+\ours), we compare it with the SOTA 3D VONs~\cite{surroundOcc} and the SOTA history-aware VONs~\cite{bevdet4d}.

\noindent\textbf{Temporal visualization case.} As shown in~\cref{fig:case-study-big} (Scene 277, Frames \#7-\#11), a pedestrian traversing the sidewalk parallel to the ego-motion trajectory is intermittently occluded by roadside vegetation. SurroundOcc~\cite{surroundOcc} exhibits severe instability in predictions (missing in Frames \#7/\#9), revealing fundamental limitations in temporal modeling. BEVDet4D-Occ~\cite{bevdet4d} alleviates this issue through data fusion but still suffers from occasional inconsistencies, such as detection dropout in Frame \#8. In contrast, our method completely eliminates flickering artifacts and maintains consistent detection across all occlusion states.

\noindent\textbf{Extra single frame visualization case.} ~\cref{fig:case-extra} highlights challenging scenarios: 
(i) Vehicle-pedestrian occlusion (Scene-0911 Frame \#15, Scene-0928 Frame \#14): Both SurroundOcc~\cite{surroundOcc} and BEVDet4D-Occ~\cite{bevdet4d} fail to recover the occluded pedestrian’s occupancy, while our method successfully localizes the target with precise geometry.
(ii) Curved road prediction (Scene-0923 Frame \#28): Our approach correctly anticipates the right-turn road geometry where baselines produce fragmented or erroneous occupancy, achieving superior shape consistency with real-world conditions.




\subsection{Overhead analysis}

For a fair comparison, all overhead analysis experiments are performed on a single NVIDIA L20 GPU.

\begin{figure}[htbp]
    \centering
    \begin{minipage}{0.48\textwidth}
        \footnotesize
        \begin{tabular}{r|ccc}
            \toprule
            Model & mIoU~$\uparrow$ & \makecell{Memory (MB)\\ Train~/~Test}~$\downarrow$ & Latency~$\downarrow$ \\
            \midrule
            FB-Occ~\cite{fb_occ} & 39.11 & 32,915~/~5,933 & 0.09s \\
            % SparseOcc~\cite{SparseOcc_Liu} & 30.10 & $>$49,140~/~7,147 & \cellcolor{Gray} 0.05s \\
            OPUS-L~\cite{opus} & 36.20 & OOM~/~10,579 & 0.16s \\
            OPUS-T~\cite{opus} & 33.20 & 48,532~/~6,711 & \textbf{0.03s} \\
            BEVDet4D-Occ~\cite{bevdet4d} & 39.30 & 22,833~/~4,689 & 0.26s \\
            \midrule
            ViewFormer+Ours & \textbf{41.30} & \textbf{16,619~/~4,687} & 0.12s \\
            \bottomrule
        \end{tabular}
        \vspace{2mm}
        \caption{Comparison of computational overhead. All models are benchmarked with ResNet-50 backbones. Our result (ViewFormer+\ours) in this table is measured for $N = 1$. OOM indicates out of CUDA memory. Best results are \textbf{bolded}.}
        \label{tab:efficiency}
    \end{minipage}\hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{assets/bubble_0304.png}  % 替换成你的图片
        % \vspace{-8mm}
        \caption{Comparison of memory and latency overheads. Lower-left positions indicate superior performance with reduced memory consumption and faster inference. Large circles indicate better mIoU quality.}
        \label{fig:bubble}
    \end{minipage}
\end{figure}



As illustrated in \cref{tab:efficiency} and \cref{fig:bubble}, we conducted a comparative study to evaluate the computational overhead of our model against existing temporal methods~\cite{bevdet4d,opus,fb_occ}. The analysis focuses on GPU memory consumption during the training/testing phases and per-sample inference latency. The result shows that our method establishes an optimal accuracy-memory balance, achieving state-of-the-art mIoU while maintaining minimal GPU memory consumption alongside sustained computational efficiency that avoids runtime bottlenecks. For quantitative benchmarking, we compare two baseline frameworks:
\begin{itemize}
    \item ViewFormer on Occ3D: (i) Training memory: ViewFormer+\ours\ requires 16 GB of GPU memory, with the \ours\ module consuming only 0.22 GB, accounting for \textbf{1.4\%} of total usage; (ii) Inference latency: Full sample processing takes 0.1218s, where \ours\ contributes merely 0.0043s, accounting for \textbf{3.5\%} of total computation.
    \item SurroundOcc on nuScenes: (i) Training memory: SurroundOcc+\ours\ consumes 39 GB of GPU memory, with \ours\ occupying only 0.69 GB, which is \textbf{1.8\%} of total memory; (ii) Inference latency: Complete sample inference requires 0.9200s, while \ours\ takes 0.0065s, contributing to \textbf{0.7\%} of total latency.
\end{itemize}

These measurements confirm that our architecture introduces negligible computational overhead while delivering competitive performance.