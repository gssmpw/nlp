\section{Methodology}
\label{sec:meth}

\subsection{Overview}
\label{sec:meth-overview}

\begin{wrapfigure}{l}{90mm}
\includegraphics[width=90mm]{assets/baseline_0308_full.png}
\caption{\textbf{Overview of \ourstitle. }OccHead represents the large occupancy decoder in the base model~\cite{surroundOcc,viewformer}. \ours\ represents the spatio-temporal correlation module we proposed.
   \ours's input pipeline consists of two modules: SFE and MFE. Specifically, SFE is used to extract static spatial features, while MFE is responsible for extracting motion features. \ours\ receives an array of spatial features from various moments and extracts the correlations between these features using the attention architecture.}
\label{fig:2}
\end{wrapfigure} % avoid blank space here


\cref{fig:2} shows the overall design of \ourstitle. Let $T_{cur}$ be the current moment, and $F_{cur}$ be the current keyframe. A keyframe consists of a set of multi-view images. Let $N$=$\{1, 2, 3, ...\}$ represent the indices of previous keyframes. The keyframe at $T_{cur-N}$ is defined as $F_{pre}^{N}$. The intermediate frames between $T_{cur-N+1}$ and $T_{cur-N}$ are denoted $F_{mid}^{N}$. For instance, when $N=2$, the input $F_{input}$=$\left \{ F_{cur}, F_{pre}^{1}, F_{pre}^{2}, F_{mid}^{1}, F_{mid}^{2} \right \}$, while $F_{input}$=$\left \{ F_{cur}, F_{pre}^{1}, F_{mid}^{1} \right \} $ when $N$=$1$. 


In contrast to previous approaches that depend on redundant historical data, the proposed \ours\ integrates static/motion information and employs an attention mechanism to distill essential cues from previous and intermediate frames, significantly improving the model's ability to predict occupancy in the current frame. 
In the input pipeline of \ours, two types of modules are incorporated: Static Feature Extraction (SFE) modules, which extract current and previous static spatial features, and Motion Feature Extraction (MFE) modules, which capture temporal motion features.
In \ours\, we design a novel Motion-Static Integration (MSI) module to fuse static spatial features and motion features using a Multi-Head Attention Mechanism (MHAM)~\cite{attention_is_all_you_need} to learn compact latent correlations. 
The output of \ours\ is the correction occupancy result (represented as $O_{co}$), which is added with the base occupancy result (represented as $O_{ba}$) from the OccHead to produce the final occupancy result (represented as $O_{fn}$), i,e., $O_{fn} = O_{ba} + O_{co}$. \ours\ can effectively incorporate essential temporal cues, significantly enhancing the base model's predictions. 

\subsection{Data Preprocessing: SFE and MFE Modules}
\label{sec:meth-data}

In the SFE module, we directly apply the Image BackBone and Image Neck modules from the base model, such as SurroundOcc~\cite{surroundOcc} and ViewFormer~\cite{viewformer}, to extract static spatial features $I_{cur}$ and $I_{pre}$ from pre-processed 2D static images ($F_{cur}$ and $F_{pre}^{n}$). 
In the MFE module, we initially process the data of intermediate frames ($F_{mid}^{n}$) and design a frame difference method to extract motion spatial features. Typically, $F_{mid}^{n}$ comprises five consecutive temporal RGB images for each camera. By calculating the frame differences~\cite{Frame_difference} between the first and the third frame, as well as between the third and the fifth frame, two frames that represent motion information are extracted. Finally, these images from six cameras are integrated to produce the output of the MFE module.

\subsection{\ours}
\label{sec:meth-ours-module}

\begin{wrapfigure}[23]{l}{90mm}
\includegraphics[width=90mm]{assets/model-0308.png}
% \captionsetup{labelformat=empty} % makes sure dummy caption is blank
\caption{\textbf{Detailed design of \ours.} \ours\ receives three types of input features: motion features, previous features, and current features. \ours\ enhances the base model's capability by introducing critical temporal motion information.}
\label{fig:3}
\end{wrapfigure} % avoid blank space here

\cref{fig:3} presents the detailed implementation of \ours. Its process includes Inputs, Encoder, Motion-Static Integration, and Decoder.

\noindent\textbf{Inputs.}
As shown in \cref{fig:2}, \ours\ receives three types of input: (i) The first type, named ``Current Features," is denoted by $I_{cur}$. $I_{cur}$ comprises features derived from the current keyframe after processing through the SFE module. (ii) The second type, called ``Previous Features," is denoted by $I_{pre}$. Unlike $I_{cur}$, $I_{pre}$ includes multiple keyframes, with the count $N$=$\{1, 2, 3, ...\}$. For instance, $F_{pre}^{1}$ and $F_{pre}^{2}$ represent the data from $T_{cur-1}$ and $T_{cur-2}$ respectively, which are also processed through the SFE modules and then concatenated into $I_{pre}$. (iii) The third type is ``Motion Features," similarly illustrated as $I_{mid}$. $I_{mid}$ is formed by processing the raw data $F_{mid}^{N}$ through the MFE modules.

 
Let $\Psi$ represent the function of SFE with model parameters $\theta$, and $\Gamma$ signifies the operation of the MFE module. 
Then, $I_{cur}$, $I_{cur}$, and $I_{cur}$ can be formulated as:
\begin{equation}
\begin{dcases}
%\nonumber
I_{cur} &= \Psi (F_{cur}; \theta)\\
I_{pre} &= \mathrm{Cat}(\{ \Psi (F_{pre}^{(i)}; \theta): i \in [1, ...,N]\}),\\
I_{mid} &= \mathrm{Cat}(\{ \Gamma(F_{mid}^{(i)}) : i \in [1, ...,N] \})\\
\end{dcases}
\end{equation}
where $\mathrm{Cat}$ represents the concatenation operation along the second dimension of the features. Notably, we provide a detailed analysis of the selection of the hyperparameter $N$ in the supplementary materials.

% Notably, the experimental section \cref{para:ntrack} presents a detailed analysis and experimental results concerning the impact of $N$ on the model's performance.

\noindent\textbf{Encoder.}
The purpose of the Encoder is to preprocess features and tokenize them to feed into the MSI module. Initially, the input feature of the Encoder undergoes preliminary feature extraction using a 1$\times$1 convolution to reduce the channel dimension, resulting in feature maps with dimensions $[Cam, v, L, W]$. Here, $v$ is the dimensional feature vector, $Cam$ represents the number of cameras, and $L$ and $W$ are the length and width of the images, respectively. A carefully designed image processing structure is then employed to further process these features. Specifically, for the $i$-th camera $Cam_i$, the features are subdivided into $p\times p$ blocks by unfolding along the spatial dimensions ($L$ and $W$). This results in $\lfloor L/p \rfloor \times \lfloor W/p \rfloor$ small patches~($\lfloor \cdot \rfloor$ represents the floor operation). The average value of all pixels within each patch is calculated to create a $v$-dimensional feature vector.

Subsequently, the feature vectors from different cameras are aggregated into a new feature matrix. Consequently, the feature matrix comprises $Cam \times \lfloor L/p \rfloor \times \lfloor W/p \rfloor$ rows, each containing $v$ columns, resulting in a shape of $[Cam \times \lfloor L/p \rfloor \times \lfloor W/p \rfloor, 1, v]$. This process effectively transforms the original high-dimensional image data into a set of compact feature vectors, thereby facilitating subsequent fusion. Formally, we have:
\begin{equation}
E = \overline{\mathcal{U}(p, p)(I\ast \Theta_{1\times1})}, 
\label{eq:qkv-process}
\end{equation}
where $\ast$ represents the convolution operation, $\mathcal{U}(p, p)(l)$ represents the unfolding operation on the feature $l$, which divides the image into $p\times p$ blocks in the spatial dimension. $\Theta_{1\times1}$ represents a 2D convolution with a kernel size of 1$\times$1. The symbol $\overline{~ \cdot ~ }$ denotes the mean operation. $E$ represents the processed result.

The designed Encoder not only significantly reduces the dimensionality of the data but also preserves essential spatial and motion information from the images, providing refined and effective inputs for subsequent modules.


\noindent\textbf{Motion-Static Integration.}
\label{para:tri-stream} 
Unlike existing history-aware VONs that may indiscriminately process redundant historical information for current frame prediction, we propose a Motion-Static Integration (MSI) module, which effectively extracts critical historical static cues and historical motion cues through information interaction between historical frames (i.e., previous keyframe and intermediate frames) and current frames. This process is efficiently implemented via the lightweight MHAM architecture, as shown in \cref{fig:3}. 

The MSI module operates through three sequential processing stages: (i) The $I_{pre}$ and $I_{mid}$ are encoded into query vectors $Q_{pre}$ and $Q_{mid}$ via separate Encoders. Meanwhile, the current features $I_{cur}$ is projected into key $K_{cur}$ and value $V_{cur}$ spaces using two fully connected (FC) layers with non-shared parameters. The FC layer parameters are represented as $\mathbf{\Phi}^K_{cur}$ and $\mathbf{\Phi}^V_{cur}$, respectively. To prevent mutual interference between MHAMs, structurally identical yet parameter-independent FC layers are applied to $I_{cur}$ to generate two key-value pairs ($K_{cur}$, $V_{cur}$). (ii) Historical static cue extraction where an MHAM uses $Q_{pre}$ as query with first pair of $K_{cur}$/$V_{cur}$ as key/value pairs, producing output tokens $f_{pre}$, which represents a static feature containing correlative information about the current keyframe; (iii) Motion cue extraction where another MHAM uses $Q_{mid}$ as query and second pair of $K_{cur}$/$V_{cur}$ as key/value pairs, generating motion correlation features $f_{mid}$. Formally: 
\begin{equation}
\begin{dcases}
f_{pre}=\mathcal{S}\left(\frac{Q_{pre}\mathbf{\Phi} _{pre}^Q (K_{cur}\mathbf{\Phi} _{pre}^K)^\top}{\sqrt{d_k}}\right)\cdot(V_{cur}\mathbf{\Phi} _{pre}^V)\\
f_{mid}=\mathcal{S}\left(\frac{Q_{mid}\mathbf{\Phi} _{mid}^Q  (K_{cur}\mathbf{\Phi} _{mid}^K)^\top}{\sqrt{d_k}}\right)\cdot(V_{cur}\mathbf{\Phi} _{mid}^V),
\end{dcases}
\label{eq:att}
\end{equation}
where $\mathcal{S}$ is the Softmax operation, $\sqrt{d_k}$ is the scaling factor. $\mathbf{\Phi}_{pre}^Q$, $\mathbf{\Phi}_{pre}^K$, $\mathbf{\Phi}_{pre}^V$, $\mathbf{\Phi}_{mid}^Q$, $\mathbf{\Phi}_{mid}^K$, and $\mathbf{\Phi}_{mid}^V$ are learnable weight parameters. 

\noindent\textbf{Decoder.} As shown in \cref{fig:3}, $f_{cur}$, $f_{pre}$ and $f_{mid}$ are input into the Decoder. The output feature dimensions of the MHAM are $[Cam \times \lfloor L/p \rfloor \times \lfloor W/p \rfloor, 1, v]$, which correspond to the feature dimensions of the input query, key, and value. Then, $f_{cur}$, $f_{pre}$ and $f_{mid}$ each undergo a deconvolution layer~\cite{Deconvolutional2010}, transforming the token features to recover to the output feature dimensions. The outputs of these deconvolution layers are $O_{pre}$, $O_{mid}$, and $O_{cur}$, which are then concatenated and fused using a 3D convolution with a kernel size of 3$\times$3$\times$3, ultimately yielding the Correction Occ Result (represented as $O_{co}$). Thus, we have:
\begin{equation}
O_{co} = (O_{cur} \oplus O_{mid}\oplus O_{pre})\ast \Theta_{3\times3\times3},
\label{eq:fusion}
\end{equation}
where $\oplus$ represents the concatenation along the second dimension and $\Theta_{3\times3\times3}$ is a 3D convolution with a kernel size of 3$\times$3$\times$3.

\subsection{Optimization}

To optimize \ours, we train the base network's OccHead with \ours using the original loss without freezing OccHead's parameters. Specifically, we adhere to SurroundOcc's~\cite{surroundOcc} loss on the nuScenes benchmark~\cite{nuScenes}: 
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{ce}} + \mathcal{L}_{\text{sem}} + \mathcal{L}_{\text{geo}},
\end{equation}
where $\mathcal{L}_{\text{ce}}$ (cross-entropy~\cite{cross_entropy_loss_2}) governs voxel occupancy classification, $\mathcal{L}_{\text{sem}}$~\cite{Semantic_loss} enforces semantic consistency, and $\mathcal{L}_{\text{geo}}$~\cite{geometry_affinity_loss} regulates geometric coherence. 
Following the convention~\cite{viewformer}, we deploy the follows equation on the Occ3D~\cite{Occ3D} benchmark: 
\begin{equation}
\mathcal{L} = \mathcal{L}_{\text{focal}} + \mathcal{L}_{\text{ce}} + \mathcal{L}_{\text{ls}} + \lambda\mathcal{L}_{\text{l1}},
\end{equation}
where $\mathcal{L}_{\text{focal}}$ represents focal loss~\cite{focal_loss} for class imbalance mitigation, $\mathcal{L}_{\text{ls}}$ represents Lovasz-softmax loss~\cite{lovasz_loss} for segmentation refinement, and $\mathcal{L}_{\text{l1}}$ represents L1 loss with weight $\lambda$ for motion regression.

Notably, we employ the same optimization functions across various benchmarks to supervise \ours. This helps mitigate gradient oscillations in OccHead while ensuring the rapid convergence of our module based on the stable output of fine-tuned OccHead.