\section{Introduction}
\label{sec:intro}

Vision-based occupancy networks (VONs) have emerged as a powerful technique for reconstructing surrounding environments from ego-centric multi-view images. This approach represents the environment as a collection of grid-like voxels~\cite{Occ3D}. Recent studies~\cite{BEVFormer,OccFormer,TPVFormer,VoxFormer,surroundOcc} have focused on learning an accurate mapping from 2D visual cues to 3D occupancy voxels, integrating both geometric and semantic information.

Despite these progresses, the occupancy results generated by existing VONs often exhibit inaccuracies and visual imperfections. A particularly problematic phenomenon, which we term {\bf flickering}, manifests as instability in the constructed scenes: objects may appear or disappear abruptly, accompanied by various artifacts. This lack of temporal consistency and accuracy not only confuses the driving control system but also degrades the visual experience for human drivers. The primary causes of flickering include sensor noise, occlusion, model limitations, and thresholding issues. As illustrated in~\cref{fig:moti}, a pedestrian occluded by a tree provides incomplete visual cues, posing a significant challenge to the prediction models. In this case, SurroundOcc~\cite{surroundOcc}, which operates in a frame-by-frame ``detection'' paradigm, fails to detect the pedestrian in Frame \#24. The temporary absence of the person will create a flickering effect when the results are displayed to the user.

% figure1 here @@@@@@@@@@@
% \begin{figure}[!t]
% \centering
% \includegraphics[width=\linewidth]{assets/figure1_0308_light.png}
% \caption{Enhancing existing VONs with \ours. (a) A standard 3D VON, which ignores historical data and thus misses objects; (b) A history-aware VON that leverages historical frames, but still generates incomplete voxels due to suboptimal data association; (c) Our method yields smoother and more complete occupancy via lightweight yet effective spatio-temporal correlation.
% }
% \label{fig:moti}
% \vspace{-15pt}
% \end{figure}

An effective approach to mitigate the temporal inconsistency problem is to incorporate historical information while predicting the current occupancy. Several recent methods~\cite{opus,bevdet4d,SparseOcc_Liu} have explored this history-aware approach. While successful in reducing missing objects, these methods often incur significant computational costs. To balance accuracy and fast reconstruction, OPUS~\cite{opus} employs a learnable query-based encoder-decoder architecture and retains coarse-grained concatenation of raw image features during early fusion stages. BEVDet4D-Occ~\cite{bevdet4d} reduces redundancy by performing spatio-temporal alignment from a Bird's Eye View (BEV) perspective. However, these methods aggregate spatio-temporal features and project them into the current latent space in a somewhat indiscriminate manner, leading to redundant information that may suppress object detection. As illustrated in the third row of \cref{fig:moti}, the pedestrian appears unclear in the BEVDet4D-Occ results, likely due to the coarse-grained integration of historical information during data fusion.

\begin{wrapfigure}{l}{90mm}
\includegraphics[width=90mm]{assets/figure1_0308_light.png}
\caption{Enhancing existing VONs with \ours. (a) A standard 3D VON, which ignores historical data and thus misses objects; (b) A history-aware VON that leverages historical frames, but still generates incomplete voxels due to suboptimal data association; (c) Our method yields smoother and more complete occupancy via lightweight yet effective spatio-temporal correlation.}
\label{fig:moti}
\end{wrapfigure} % avoid blank space here

We present a novel method, called {\bf \ours}, to address the aforementioned challenges. \ours is a plugin design that integrates with existing occupancy networks for improved performance. While adopting the basic history-aware approach, it improves upon prior work by attentively combining current and historical data for occupancy prediction, with an emphasis on {\em lightweight and effective} spatio-temporal correlation. \ours employs a three-stage framework. First, it consolidates historical static/motion cues through feature aggregation and spatial-temporal tokenization. Subsequently, a novel Motion-Static Integration (MSI) architecture correlates current features with historical cues in a shared latent space. Finally, the system generates {\em correction occupancy} that enables refinement of the base network's results.

Our method offers two key advantages: (i) Seamless integration with state-of-the-art~(SOTA) VONs, which significantly enhances prediction accuracy. When integrated with SurroundOcc~\cite{surroundOcc} on nuScenes~\cite{nuScenes} benchmark, it elevates IoU and mIoU by 5.18\% and 1.82\%, respectively. Similarly, when combined with ViewFormer~\cite{viewformer} on the Occ3D~\cite{Occ3D} benchmark, it achieves comparable accuracy gains; (ii) Low cost through compact history encoding, outperforming latest history-aware VONs~\cite{opus,bevdet4d,fb_occ} with much lower computational costs. For instance, OPUS-L~\cite{opus} requires 49GB of memory for training and 0.26 seconds for inference. In contrast, \ours, when combined with ViewFormer~\cite{viewformer}, requires only 17GB of memory for training and 0.12 seconds for inference, resulting in a 65\% reduction in training memory and a 54\% acceleration in inference time. The visual effect of \ours in \cref{fig:moti} further showcases its superiority. In summary, the contributions of this paper are three-fold:
\begin{itemize}
    \item We introduce the \ours method, which integrates easily with existing 3D VONs with consistent improvement in accuracy.
    \item We propose a Motion-Static Integration approach that establishes spatio-temporal correlations between pixel-level visual cues and occupied voxels within a shared, compact latent space.
    \item Our approach has demonstrated both high efficiency and effectiveness, achieving SOTA performance across major benchmarks.
\end{itemize}

