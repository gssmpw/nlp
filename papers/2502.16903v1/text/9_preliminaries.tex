\section{Preliminaries}

\subsection{Intent Function} 

The intention function refers to how humans evaluate the degree to which a response from an LLM aligns with or meets their actual requirements. For a response $R$, the intention function $\mathcal{F}$ of a specific task $T$ is defined implicitly by the task creator and can be scaled or calibrated to $[0,1]$. Formally,  
\begin{equation}
    \mathcal{F}_T(R) = \frac{1}{Z} \cdot \phi_T(R)
\end{equation}
where $\phi_T(R)$ is a function that measures how well $R$ aligns with the task $T$, and $Z$ is a normalization factor. The intention function is an abstraction of scoring functions; attack success rates and harmfulness scores are instances of intention functions.

\subsection{LLM Jailbreak Attack}

An LLM jailbreak method $J$ enables an LLM $M$ to generate responses to questions that it would otherwise refuse to answer under its safety mechanism. Here, $T$ is defined as ``jailbreak $M$ using a specific method''. Given a dataset of jailbreak questions $\mathcal{D} = \{D_i\}_{i=1}^{N}$, a proposed jailbreak method $J^*$ is defined as the function that maximizes the average of intention function $\mathcal{F}_T$ over the dataset:  
\begin{equation}
J^* = \arg\max_J \frac{1}{N} \sum_{i=1}^{N} \mathcal{F}_{T}[J(M,D_i)].
\end{equation}  

The choice of $\mathcal{F}_T$ depends on the design of the jailbreak attack. For example, in template-based jailbreaks, $\mathcal{F}_T$ can be measured by detecting keywords in the responses; in optimization-based jailbreaks, which exploit gradient-based methods in white-box LLMs to maximize the probability of generating a target sequence, $\mathcal{F}_T$ can be defined as $p_M(\bm{x}_{n+1:n+H}^* \mid \bm{x}_{1:n})$, where  
\begin{equation}
p_M(\bm{x}_{n+1:n+H}^* | \bm{x}_{1:n}) = \prod_{i=1}^{H} p_M(\bm{x}_{n+i}^* \mid \bm{x}_{1:n+i-1}).
\end{equation}  
which captures the likelihood of producing a specific harmful target $\bm{x}_{n+1:n+H}^*$ given an adversarial prefix or suffix $\bm{x}_{1:n}$~\cite{23gcg}.

Considering the nature of $M$, we distinguish between two types of jailbreak attacks. In a \textbf{black-box jailbreak}, the attack modifies only the input prompt $\bm{x}=\bm{x}(D)$. Formally, this means 
\begin{equation}
    J(M,D)=M(J(D))
\end{equation}  

In contrast, a \textbf{white-box jailbreak} allows modifications beyond the input prompt $\bm{x}=\bm{x}(D)$, extending to intermediate states, leading to  
\begin{equation}
    J(M,D)=J(M)(J(D))
\end{equation}  

However, much like the paradox of the Ship of Theseus, directly modifying the parameters of $M$ blurs the line between jailbreak and model alteration. To maintain the integrity of the original model, even white-box jailbreaks are restricted to manipulating intermediate computational states, such as activations, embeddings, or attention values, rather than altering the weights of the LLM.

In addition, we would like to clarify that the language for jailbreak methods is English, so the presentation of harmful questions in the benchmark is also in legitimate English sentences. If the jailbreak method requires some transformation of the original question, such as translating it into a low-resource language~\cite{24multijail} or using methods such as Base64 encoding~\cite{23base64}, converting the LLM response to standard English is also part of the work that jailbreak needs to do. Performing a direct evaluation of the harmfulness of the ciphered text may be inaccurate.

\subsection{Evaluation of Jailbreak Attacks}

Evaluation of jailbreak methods is typically defined as attacking multiple applicable victim LLMs on specific harmful question datasets using the jailbreak method to be evaluated and mapping each jailbreak response to a score on $\mathbb{R}$. In a single jailbreak case, the scoring function is formally noted as $\mathcal{S}:\text{str}\to\mathbb{R}$. It is not equivalent to the intention function $\mathcal{F}_T$, and it is neither necessary nor advisable to introduce a certain scoring function when designing jailbreak methods; in addition, $\mathcal{S}$ usually has an explicit formula.

In previous research, the attack success rate (ASR) is a frequently used criterion, calculated as the proportion of cases that successfully jailbreak in the tested dataset for all cases~\cite{23gcg}.
\begin{equation}
    \text{ASR} = \frac{\sum_{D_i \in \mathcal{D}} \mathbb{I}(\text{Success}(J(M,D_i)))}{|\mathcal{D}|}
\end{equation}

ASR appears in quite a few jailbreak attack works and is the most common criterion for cross-work comparison. However, as its definition suggests, it requires a binary evaluation for each jailbreak case (i.e., output ``successful'' or not). This is because ASR is a concept proposed early on and, at the time of its proposal and a period afterward, the methods used to evaluate jailbreak all produce binary outcomes. However, we can simply extend the binary scoring function in its definition to a more general scoring function $\mathcal{S}$, as long as we ensure that its range is calibrated to within $[0,1]$.

The design principles for $\mathcal{S}$ can be categorized into two types. One evaluates \textbf{whether the LLM refuses to answer harmful questions}. If the LLM refuses to respond, the attack fails; otherwise, it is deemed successful. This principle typically underlies binary evaluation methods, such as those based on detecting refusal keywords or the OpenAI Moderation API~\cite{openai_moderations_api}. The other evaluates \textbf{whether the LLM provides harmful information}. If the response contains the expected harmful content, the jailbreak attack is considered successful; otherwise, it fails. This principle typically underlies LLM-based evaluation systems, as rule-based methods struggle to determine the presence of harmful information.