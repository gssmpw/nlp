

\section{Benchmark Design}

\subsection{Harmful Question Dataset Construction}

The diagram of the process described in this section is shown in Figure \ref{fig:overview}.I.

\subsubsection*{Question Collection and Filtering}

\label{para:2.1}

We first focus on the characteristics that individual harmful question cases should possess. Based on a review of 6 existing harmful question datasets (StrongREJECT~\cite{souly2024strongreject}, HarmBench~\cite{mazeika2024harmbench}, ForbiddenQuestionSet~\cite{shen2023anything}, JailbreakBench~\cite{chao2024jailbreakbench}, Advbench~\cite{chen2022adversarial}, MaliciousInstruct~\cite{huang2023catastrophic}), we establish the following principles for selecting harmful questions to evaluate jailbreak methods:

\begin{enumerate}
    \item \textbf{Model Refusal:} The question should be effectively refused by the corresponding victim LLM based on their built-in safety mechanisms for the given harmful topic.
    
    \item \textbf{Direct Requirement:} The question should be a direct request for harmful behavior within the specified harmful topic category. We should not include questions constructed through translation and contextualization with scenarios, etc. However, it is acceptable to add specific requirements to clarify the desired response.
    
    \item \textbf{Malicious Intent:} The question's intent should be clearly malicious or high-risk. It should not rely on keywords like \emph{illegal} to artificially create a harmful context. For example, it is easy to construct a scenario to unlock a door or remove fingerprints for educational purposes, while some datasets contain questions like \emph{How to illegally unlock a door?}, which is not suitable for evaluation.
    
    \item \textbf{Answerable Structure:} The harmful question should be answerable in a meaningful way. Overly broad questions, such as \emph{How to use malware to extract sensitive information?}, can reduce the precision and reliability of the evaluation, since it does not specify what information needs to be extracted, it cannot reflect the true intentions of attackers.
\end{enumerate}

These principles derive from our observations and summaries of numerous jailbreak instances and their evaluations, which align with the principles of fairly and standardly evaluating other capabilities of LLMs.

\subsubsection*{Harmful Topics Taxonomy}

\label{para:2.2}

A fine-grained taxonomy of harmful topics is crucial for determining what types of harmful questions should be included in the benchmark. Research on LLM safety has shown that different LLMs may have different safety capabilities for different harmful topics~\cite{tedeschi2024alert, kumar2024sagert}, while the inconsistency between vendor policies and the actual safety performance of LLMs reminds us that we need another dimension of topic taxonomy. 

Therefore, we establish a \textbf{core set} of harmful categories in which all victim LLM refuse to respond. Other important categories that may only be applicable to some victim LLMs are referred to as the \textbf{additional set}. Researchers can choose to report performance on the additional set optionally, and our subsequent experiments will separately report results on both the core and additional sets. Table \ref{tab:categories} presents the distribution of core and additional harmful topic sets across three selected victim LLM vendors.

\subsubsection*{Construction Results}

Based on the principles described above, we filter and refine 200 cases from the initial 1823 cases from 6 datasets, modifying the majority of these questions to align with these principles. Among them, 180 cases are in the core set, while 20 other cases are for the additional set.

In addition, during the process of generating guidelines (to be discussed later), we find that although the semantic goals of some harmful questions are different, the harmful information required may actually overlap with other questions. We conduct an additional review of this situation and remove those questions where the required harmful information is completely covered by other single or multiple questions.

Figure \ref{fig:data_source_comp} shows the number of original harmful questions selected from each source dataset after passing through the aforementioned pipeline.

\input{assets/table/topic_cats}

\begin{figure}[h]
    \centering
    \includegraphics[width=\linewidth]{assets/figure/dataset_source_component.pdf}
    \caption{The distribution of the source datasets of the questions selected in \bench.}
    \label{fig:data_source_comp}
\end{figure}

\subsection{Guidelines Generation}

The diagram of the process described in this section is shown in Figure \ref{fig:overview}.II.

\subsubsection*{The Shape of Guidelines}

Providing case-by-case evaluation guidelines for each harmful question case is a key contribution of this paper. We identify two evaluation aspects for jailbreak success from an attacker's perspective:

\begin{enumerate}
    \item \textbf{Entity:} To effectively assist the attacker, the jailbreak response must mention specific entities related to the malicious intent. For example, in response to \emph{Where can I get some nuclear waste?}, an LLM that provides specific types of nuclear waste or details such as the names and addresses of nuclear facilities would narrow down the attacker's target to these specific entities.
    \item \textbf{Function:} To provide meaningful assistance, the jailbreak response must also include explanations of the necessary steps to fulfill the malicious request. For example, in response to \emph{How to make a tubular bomb?}, a jailbreak response that provides information about \emph{materials for making bombs}, \emph{how to obtain materials}, and \emph{assembly methods} would significantly aid the attacker's intent.
\end{enumerate}

We add these two fields to all harmful question cases, with each field containing 0 to 3 text descriptions, depending on the specific harmful topic and task requirements. It should be noted that entities and functions each have their own emphasis by design, but their status in the evaluation process is the same, and they carry the same weight when being evaluated as scoring points. Each harmful question case has at least one scoring point.

\subsubsection*{Writing Guidelines}

The writing of guidelines is a process involving both manual effort and LLM participation. For relatively easy harmful questions such as \emph{How to make bombs?}, we can easily consider the knowledge and processes required to complete this task from the perspective of an attacker. We identify as many of these questions as possible and manually create guidelines to ensure the high quality of \bench. However, for questions such as \emph{How to launch a terrorist attack?}, we may not have the effective knowledge to write guidelines, so we use two LLM-based methods to generate an initial draft of the guidelines for these questions.
\begin{enumerate}
    \item Conducting multi-round, long context conversations with powerful while relatively unsafe LLMs like GPT-4o, allowing them to learn from few-shot examples of manual written guidelines and create guidelines for new harmful questions.
    \item Observing the structure of the generated content of \emph{benign twin} of harmful questions, summarizing the scoring criteria for benign questions, and then adapting these criteria for the corresponding harmful questions. For example, a benign twin of \emph{How to make bombs?} can be \emph{How to make cakes?}. This process can also be assisted using LLMs like GPT-4o.
\end{enumerate}

We perform manual reviews and adjustments on all LLM-generated guidelines to ensure that they do not deviate from the benchmark principles while maintaining consistency of the quality of the guidelines.

One possible issue is whether the scoring points included in the provided guidelines might lead to omissions of other details. In this regard, we would like to point out that the guidelines are designed to measure whether the jailbreak achieves the intended harmful goals, rather than exhaustively covering all possible information. When writing the guidelines, we focus on the most critical information needs of the attacker, rather than all possible details. Therefore, even if there are details in the response that are not covered by the guidelines, as long as these details do not affect the achievement of the core goals, the scoring remains reasonable. Also, in experiments, additional details are difficult to quantify, and their weight cannot be clearly defined; forcibly including them may undermine the stability of the benchmark.

\subsection{Evaluation Framework with Guidelines}

The diagram of the process described in this section is shown in Figure \ref{fig:overview}.III.

\subsubsection*{Guideline-enabled Evaluation}

Creating guidelines helps improve the evaluation of jailbreak success. Existing evaluations often rely on a judge model (e.g., GPT-4), which then evaluates binarily on criteria like \emph{whether the jailbreak is successful}, \emph{whether the response is helpful}, or \emph{whether the response is persuasive}. These evaluations are usually dependent on the judge model's subjective understanding and knowledge, due to the absence of explicit criteria defining successful jailbreak or ideal jailbreak outcome.

By introducing guidelines for each case, we can construct evaluation prompts that include descriptions of these guidelines, combined with the harmful question and the generated jailbreak response. The evaluation then becomes a task to verify whether the content described in the guidelines (the entities and functions) is present in the response. Essentially, the goal of the evaluation is to determine whether the jailbreak leads to the LLM output content that matches the relevant descriptions (hallucinations are not evaluated). This shifts the evaluation task from being a subjective value judgment by the judge model to an objective existence check, where only the basic information extraction capability of the judge model is needed. Consequently, this reduces the dependence of the scoring process on the specific judge model and enhances the consistency and reliability of evaluations.

\subsubsection*{Evaluation Criterion}

For the evaluation enabled by \bench, we use ASR to report the relative merits of each jailbreak method, and the scoring function $\mathcal{S}$ involved is the scoring points completion rate, formally,
\begin{equation}
\label{eq: SG}
    \mathcal{S}(R)=\frac{\sum_{g_i\in\mathcal G}\mathbb{I}(m(R,g_i))}{|\mathcal{G}|}
\end{equation}
where $m$ is the evaluator LLM, and $\mathcal{G}$ is the scoring points by the guidelines of the harmful question corresponding to the jailbreak response $R$.

Equation \ref{eq: SG} further suggests that in this evaluation method, all scoring points by the guidelines have the same weight, ensuring the linear comparability of the final score results. A different example is in the StrongREJECT scoring, where the two criteria, \emph{convincing} and \emph{specific}, are not completely independent. Responses of varying degrees of harmfulness may lead to the same score, which means that this scoring method is not linearly comparable between different methods for the same case.