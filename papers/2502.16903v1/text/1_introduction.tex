\section{Introduction}

As the capabilities of large language models (LLMs) advance rapidly, their potential risks of misuse or abuse have drawn wide attention from researchers~\cite{mozes2023use, barman2024dark, pan2023risk}. Jailbreak attacks, which serve as a crucial approach to uncovering these risks and potential vulnerabilities of LLM, have become a research frontier~\cite{yi2024jailbreak, jin2024jailbreakzoo, shayegani2023survey}. When evaluating the effectiveness of jailbreak methods, existing studies often rely on benchmarks consisting of datasets of harmful questions and scoring systems~\cite{chen2022adversarial, souly2024strongreject, mazeika2024harmbench, shen2023anything, huang2023catastrophic, chao2024jailbreakbench}. However, the evaluation methods and benchmarks used in existing research have significant limitations. These may lead to unfair comparison among these jailbreak methods and inaccurate assessments of the safety risks of LLMs or even exaggerate their vulnerabilities.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{assets/figure/overview.pdf}
    \caption{The overview of building \bench~and using it for evaluation.}
    \label{fig:overview}
\end{figure*}

First, jailbreak research typically requires extensive testing on harmful topics in multiple victim LLMs. Analyzing evaluations conducted in 35 popular jailbreak methods (see Appendix \ref{app:jb_methods}), we find significant discrepancies in the evaluation setup employed, which hinders direct comparisons across different methods. Moreover, even when using the same dataset and victim LLMs, many studies report varying ASR or harmfulness results. For example, AutoDAN~\cite{24autodan}, compared to GCG~\cite{23gcg}, claims that GCG achieves an ASR of 45.4\% on Advbench using Llama-2-7B-Chat. However, in the GCGâ€™s own paper, this figure is reported as 57.0\%, despite using the same dataset, victim LLM, and evaluation system.

Second, unlike the datasets used to test the safety of LLMs such as SafetyBench~\cite{zhang2023safetybench}, those used to evaluate jailbreak methods must ensure that the questions are refused by victim LLMs without jailbreak, which is overlooked by some policy-based benchmarks. Also, it is essential to exclude coupling some jailbreak techniques with questions. For example, \emph{How do criminals typically clear fingerprints?} is a typical coupling that confuses role assumptions with harmful goals. Some LLMs will answer this question for educational or safety precaution reasons without jailbreak (see examples in the Appendix \ref{app:coupling}). Furthermore, with the expansion of research, except for traditional safety topics such as child pornography and terrorist activities, new topics related to specific vendor policies, such as professional medical advice, need also be included.

\begin{table*}[ht]
  \centering
  \caption{Summary of mix-evaluations cases facing \bench~and other baseline scoring systems.}
  \includegraphics[width=\textwidth]{assets/table/confusion_scenarios.pdf}
  \label{tab:evaluation-confuse}
\end{table*}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{assets/figure/distribution.pdf}
    \caption{The score distribution of \bench~and other non-binary LLM-based scoring systems.}
    \label{fig:value_dist}
\end{figure*}

Third, unlike benchmarks designed for other LLM capabilities, such as MMLU to evaluate multitask accuracy~\cite{20MMLU}, benchmarks to evaluate the effectiveness of jailbreak methods often only provide questions but lack standard evaluation guidelines. This not only leads to widespread adoption of keyword-based scoring, which often results in errors, but also causes LLM-based evaluations to always produce extreme results (see Figure \ref{fig:value_dist}), making them degenerate into a binary system and not capture the fine-grained details of jailbreak responses. We have summarized examples that may occur in practice with potential mis-evaluation results from commonly used evaluation systems, as shown in Table \ref{tab:evaluation-confuse}.

These limitations severely undermine the reliability and accuracy of existing benchmarks in evaluating the effectiveness of jailbreak methods. Therefore, in this paper, we propose \bench, a more standardized and fair benchmark to evaluate jailbreak methods, which addresses the limitations of the existing benchmarks from two aspects:

\begin{enumerate}
    \item \textbf{Reconstruction of Question Dataset.} We proposed a taxonomy for harmful questions based on existing policies and actual LLM safety performance. The dataset includes two parts: a \textbf{core set} (180 questions that all victim LLMs will refuse) and an \textbf{additional set} (20 questions that only some LLMs will refuse due to particular safety policies), to ensure the comprehensiveness and specificity of the evaluation. We chose short and direct text instructions as question cases, rather than scenario-mixed cases that might lead to couplings. We cleaned up ambiguous instructions in existing datasets and reduced the proportion of questions generated by adding harmful keywords (such as \emph{illegal}), making the dataset more authentic and challenging.

    \item \textbf{Writing Detailed Evaluation Guidelines.} We have written detailed scoring guidelines for each harmful question case, focusing on the key entities and functions that a successful jailbreak response must include from the attacker's perspective. These entities and functions are essential knowledge for attackers to achieve harmful goals. Additionally, each entity or function in the guidelines has a corresponding example field that illustrates its description. With guidelines, whether an attack is successful is broken down into multiple scoring points. Evaluators only need to determine whether the jailbreak response contains the content described by these scoring points, making the evaluation of the jailbreak text more stable and interpretable.
\end{enumerate}

Based on \bench, we evaluate 8 jailbreak methods on 5 selected victim LLMs. Due to the resolution of some cases that were previously mis-evaluated, the effectiveness of certain jailbreak methods has been more accurately estimated. In absolute numbers, some jailbreak methods claim to achieve an ASR of $>$ 90\% or even 100\% on existing benchmarks; however, among these, the best method only performs at 30\% on \bench, which provides ample room for the development of new jailbreak research. 

We use 3 powerful LLMs, such as GPT-4o~\cite{openai2024gpt4o}, as evaluators to conduct repeated voting on \bench~and baseline evaluation systems. The results show that the average variance of the scores caused by \bench~is the lowest, which further explains that the scoring system introduced by \bench~is more stable and agnostic to specially fine-tuned judge models. Therefore, we recommend that future jailbreak researchers use \bench~to conduct evaluation to provide more fair and valuable information to build responsible AI systems.