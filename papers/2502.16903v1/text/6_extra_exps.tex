
\section{\bench's Superiority}

Based on the experimental results in Section \ref{sec:leaderboard}, we also conduct additional experiments to provide evidence for the superiority of \bench.

\subsection{Being Agnostic to Judge Models}

LLM-based scoring systems rely on a specific LLM evaluator to perform scoring tasks, raising doubts about the validity of the scores. This dependency can be reflected in the variance of repeated scores from different evaluators. Suppose that $N$ LLM evaluator scores a single case under the same scoring system setup, resulting in scores $\{s_i\}_{i=1}^N$; the variance of repeated scores for this case is $\text{Var}(s_1,...,s_N)$. The larger this variance, the more likely that different LLM evaluators will give inconsistent scores.

Table \ref{tab:scoring-var} shows that among all LLM-based scoring systems, the scoring system proposed by \bench~consistently has the lowest repeat score variance. This indicates that the scoring system proposed by \bench~significantly reduces its dependency on LLM evaluators, allowing users to select a more advanced (ensuring basic context extraction and discernment capabilities) and less safe (ensuring that evaluators do not refuse the evaluation task after seeing harmful content) LLM evaluator APIs to conduct fair and stable evaluations using this benchmark, which has significant benefits for reducing the costs associated with LLM safety research.

\input{assets/table/scoring_var}

\subsection{Addressing Mis-evaluated Cases Before}

The scoring system of \bench~does not disrupt previous scoring results, while it further addresses more mis-evaluated cases described in Table \ref{tab:evaluation-confuse}. To illustrate this, we compile the number of various mis-evaluated cases caused by 8 jailbreak methods in Table \ref{tab:jailbreak-errors}. The identification of these cases is based on LLM, and detailed implementation details can be found in Appendix \ref{app:setup_mis}.

Table \ref{tab:fnfp-average} shows the average score of evaluations conducted on mis-evaluated cases using 6 different scoring systems. Most of these cases are unsuccessful jailbreaks, and a small number of cases provide some harmful information even under obvious jailbreak mistakes. The results in Table \ref{tab:fnfp-average} show that \bench~is particularly good at handling scenarios such as invalid rephrasing and misunderstanding, reducing scores by up to 58.92\% and 28.17\% respectively. This is because it is based on guidelines that can clearly aim to search for scoring points in jailbreak responses.

\input{assets/table/fnfp_counts}