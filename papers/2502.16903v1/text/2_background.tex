\section{Related Works}

\subsection*{LLM Safety Risks on Misuse and Abuse}

With the continuous improvement of the capabilities of LLMs, the safety risks caused by their potential abuse or misuse have raised concerns. Some research shows that LLMs can be used to generate malicious code~\cite{10431609}, assist in the planning of crimes, or provide detailed instructions on illegal activities~\cite{mozes2023use}. The ability to support these illegal behaviors highlights the shortcomings of existing safety alignment methods. In addition, LLMs are particularly prominent in generating false information, including fake news, conspiracy theories, and misleading statements, not only that originate from their inherent hallucinations, but also that may be affected by the defects of their safety measures~\cite{barman2024dark}. Some work has systematically analyzed the motivation and potential threat of LLMs to generate false content, highlighting their destructive impact in many important social areas such as health, finance and politics~\cite{pan2023risk, sun2024exploring, Papageorgiou2024}. In addition, LLMs can amplify social prejudices such as sexism and racial discrimination, which not only affect the quality of output of a single generation, but can also be further amplified in complex AI auxiliary systems developed based on LLMs with long-term social injustice~\cite{wan2023kelly, an2024measuring}. To reduce these risks of abuse and misuse, researchers have proposed a variety of methods, including human feedback reinforcement learning (RLHF) and adversarial training, in the hope of reducing the negative impact of LLMs on society through greater safety alignment and comprehensive preventive measures~\cite{ouyang2022training, xhonneux2024efficient}.

\subsection*{LLM Jailbreaks}

LLM jailbreaks refer to bypassing the safety alignment mechanism of LLMs through a specific method to answer questions that would have been refused. These questions usually involve high-risk areas of abuse and misuse of LLMs, or behaviors that are explicitly prohibited by the user policies of LLMs~\cite{OpenAI_Usage_Policies_2025,Anthropic_Usage_Policy_2024,Meta_Llama_Model_Use_Policy}. Depending on applicable information, LLM jailbreak can target black-box LLMs, such as commercial LLMs that interact through API; or white-box LLMs, such as open-source LLMs that users can deploy on their own devices. In the black-box scenario, because the interaction with the LLM is limited to input prompt, the jailbreak method is mostly based on cleverly designed prompt or multiround dialogue. For example, methods such as DAN~\cite{shen2023anything}, DeepInception~\cite{li2023deepinception} and Manyshot~\cite{manyshot} induce LLMs to generate content that should be prohibited through role-playing, distracting the model's attention from malicious intentions and other strategies. In contrast, white-box LLM jailbreak can use more internal information, such as residual flow embedding and activation values~\cite{Xu2024uncovering, turner2023activation} or gradient information~\cite{23gcg, 24autodan} of the model. By precisely adjusting these internal parameters, these techniques can subtly change the generation intention of the model and significantly increase the probability of answering malicious questions in affirmative tones.

\subsection*{Evaluations for LLM Safety and Jailbreaks}

Most jailbreak methods use some malicious question datasets to generate text on some victim LLMs, and then use judgment algorithms (based on keyword automatic matching or evaluator LLM) to judge whether the jailbreak is successful or not. Existing malicious question datasets such as Advbench~\cite{chen2022adversarial}, MaliciousInstruct~\cite{huang2023catastrophic} and JailbreakBench~\cite{chao2024jailbreakbench} are all cases of datasets with simple questions, and the evaluation of the success of the jailbreak is defined by different jailbreak works. Some datasets, such as StrongREJECT~\cite{souly2024strongreject}, have noticed the shortcomings such as duplication, unclear intentions, unanswerable questions, etc., so they have formulated more scenario-oriented datasets. Some datasets, such as HarmBench~\cite{mazeika2024harmbench}, also provide malicious question cases for copyright, multimodal and context. The existing work does not provide separate judgement guidelines for the evaluation of each malicious question without exception.