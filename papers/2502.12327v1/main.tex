\documentclass[fleqn,10pt]{wlscirep}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}

\usepackage{amsmath,amsthm,amssymb,amsfonts}
\usepackage{comment}
\usepackage{graphicx}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{appendix}
\usepackage{siunitx}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{tabularx}
\usepackage{bbm}
\usepackage{subcaption}
\usepackage{cases}
\usepackage{mathtools}

\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\pder}[2][]{\frac{\partial#1}{\partial#2}}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{lemma}[theorem]{Lemma}
\newenvironment{problem}[2][Problem]{\begin{trivlist}
        \item[\hskip \labelsep {\bfseries #1}\hskip \labelsep {\bfseries
                        #2.}]}{\end{trivlist}}

\title{Learning Plasma Dynamics and Robust Rampdown Trajectories with Predict-First Experiments at TCV}
\author[1,3,*]{Allen M. Wang}
\author[2]{Alessandro Pau}
\author[1]{Cristina Rea}
\author[3]{Oswin So}
\author[3]{Charles Dawson}
\author[2]{Olivier Sauter}
\author[4]{Mark D. Boyer}
\author[2]{Anna Vu}
\author[2]{Cristian Galperti}
\author[3]{Chuchu Fan}
\author[2]{Antoine Merle}
\author[2]{Yoeri Poels}
\author[2]{Cristina Venturini}
\author[2]{Stefano Marchioni}
\author[5]{the TCV Team}

\affil[1]{Plasma Science and Fusion Center, Massachusetts Institute of Technology, Cambridge, MA 02410, USA}
\affil[2]{Swiss Plasma Center, Ecole Polytechnique F\'ed\'erale de Lausanne, CH-1015 Lausanne, Switzerland}
\affil[3]{Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, Cambridge, MA 02410, USA}
\affil[4]{Commonwealth Fusion Systems, 117 Hospital Rd, Devens, MA 01434}
\affil[5]{See the author list of B. P. Duval et al 2024 Nucl. Fusion 64 112023 \cite{duval2024experimental}}
\affil[*]{Corresponding Author: allenw@mit.edu}

\begin{abstract}
The rampdown in tokamak operations is a difficult to simulate phase during which the plasma is often pushed towards multiple instability limits. To address this challenge, and reduce the risk of disrupting operations, we leverage recent advances in Scientific Machine Learning (SciML) to develop a neural state-space model (NSSM) that predicts plasma dynamics during Tokamak Ã  Configuration Variable (TCV) rampdowns. By integrating simple physics structure and data-driven models, the NSSM efficiently learns plasma dynamics during the rampdown from a modest dataset of 311 pulses with only five pulses in the reactor relevant high performance regime. The NSSM is parallelized across uncertainties, and reinforcement learning (RL) is applied to design trajectories that avoid multiple instability limits with high probability. Experiments at TCV ramping down high performance plasmas show statistically significant improvements in current and energy at plasma termination, with improvements in speed through continuous re-training. A predict-first experiment, increasing plasma current by 20\% from baseline, demonstrates the NSSM's ability to make small extrapolations with sufficient accuracy to design trajectories that successfully terminate the pulse. The developed approach paves the way for designing tokamak controls with robustness to considerable uncertainty, and demonstrates the relevance of the SciML approach to learning plasma dynamics for rapidly developing robust trajectories and controls during the incremental campaigns of upcoming burning plasma tokamaks.
\end{abstract}
\begin{document}
\flushbottom
\maketitle
\thispagestyle{empty}

\section{Introduction}
Upcoming burning plasma tokamaks, such as SPARC \cite{creely2020overview} and ITER \cite{shimada2007overview}, require reliable plasma control to avoid operational delays and machine damage due to plasma disruptions, a challenge that will only increase for tokamak pilot plants \cite{maris2023impact} like ARC \cite{sorbom2015arc} and DEMO \cite{federici2014overview}. Given this risk becomes intolerable at high plasma current, $I_p$, and stored energy, $W_{tot}$, a key mitigation strategy is to de-energize the plasma by performing a rampdown of the plasma current, but doing so typically pushes the plasma closer to multiple instability boundaries \cite{van2023scenarioA, teplukhina2017simulation,mehta2024automated}. Figure \ref{fig:intro-fig} depicts the phases of a tokamak pulse, beginning with rampup of the plasma current to the steady-state flattop phase, and ending with a rampdown. Notably, Figure \ref{fig:intro-fig} also shows an example of a quantity correlated with plasma instability growing during the rampdown phase, a challenge which motivates the algorithmic design of safe rampdown trajectories. This challenge is especially acute in reactor relevant high performance plasmas, which operate close to instability boundaries to achieve the high normalized plasma density, typically quantified by the Greenwald fraction $f_{GW}$ \cite{greenwald1988new}, and normalized plasma pressure, $\beta_N$ \cite{troyon1984mhd}, necessary for economical energy production. The importance of designing robust rampdowns for reactor relevant fusion plasmas is highlighted by the recent record-breaking high performance campaign at the Joint European Torus (JET), for which most disruptions occurred during the termination phase \cite{garzotti2023development}. For the baseline scenario, a $\approx15\%$ increase of the plasma current, from 3MA to 3.5MA, increased the disruptivity considerably from $\approx20\%$ to $\approx50\%$ \cite{garzotti2023development}. This challenge motivates the development of tools that can rapidly adapt rampdown trajectories to manage disruptivity as fusion performance is increased.

Due to the stochasticity of plasma dynamics, hardware and control imperfections, and the possibility of off-normal-events (ONEs), it is important to design scenarios, trajectories, and controllers with robustness to distributional uncertainty in the dynamics of the plasma. The biggest barrier to designing for robustness in this context is the difficulty of simulating plasma dynamics during the highly transient rampdown phase, during which multiple physical quantities, many of which are not well-modeled with a principles-based approach, can change drastically. Due to this challenge, prior rampdown studies using existing simulators \cite{teplukhina2017simulation, van2023scenarioA, van2023scenarioB, koechl2020evaluation, vincenzi2017eu, asp2022jintrac} typically make assumptions on important effects like the confinement regime transition which is subject to significant uncertainty. These simulation limitations motivated recent experiments at DIII-D designing rampdown trajectories with black box Bayesian Optimization on three control variables, which achieved improvements in the plasma current at time of termination \cite{mehta2024automated}. This experiment showed relatively small adjustments can make an out-sized impact; however, reported pulses, also known as shots, were all at low performance, and a predictive model-based approach is desired for upcoming tokamaks. These limitations motivate the development of models that efficiently learn difficult to simulate dynamics from experimental data, and which are massively parallelizable across uncertainties to enable robust model-based design of trajectories.

To address these challenges, this work takes a data-driven approach, leveraging recent advances made by the Scientific Machine Learning (SciML) community \cite{rackauckas2020universal, kidger2022neural, chen2018neural} and new machine learning frameworks, namely JAX \cite{jax2018github}, which enable the training of dynamics models that combine physics-based equations with  data-driven models. A data-driven approach is not without precedent; aircraft flight control and simulation primarily utilize data-driven models of aerodynamics derived from flight test data in lieu of computational fluid dynamics (CFD) \cite{chyczewski2022summary, allerton2009principles}, often with classical linear state-space models (SSMs) \cite{morelli2016aircraft}. While prior works on learning plasma dynamics using unstructured neural networks required large datasets, often spanning thousands of shots \cite{abbate2021data, char2024full, kit2024learning}, we gain sample efficiency by embedding physics structure into a Neural State Space Model (NSSM) \cite{suykens1995nonlinear, rivals1996black}. This model was trained to generate sufficiently accurate predictions using a modest amount of data, with 311 rampdowns at low performance and only five shots with incomplete rampdowns in the relevant high performance regime, with $\beta_N > 2$ and near the density limit. The model is capable of simulating $\approx 10^4$ rampdown trajectories per second on a single A100 GPU, enabling the usage of the NSSM in a reinforcement learning (RL) training environment. 

The RL environment is massively parallelized to design trajectories with robustness to uncertainties, including the initial conditions of the plasma and its time varying dynamics. In contrast with prior works on applying RL approaches to fusion, we leverage its capabilities for offline design of robust trajectories, which is more readily applicable to the safety-critical settings of upcoming tokamaks than RL for real-time control, as was done in prior works \cite{degrave2022magnetic, seo2024avoiding}. After a small number of initial trial shots, the plasma reliably terminated at low plasma current and stored energy for five consecutive high performance shots, with statistically significant improvements relative to baseline, although we encourage caution in interpreting the statistics of the result due to the small sample size. As a test of the viability of this approach for performing small extrapolations in an incremental high performance campaign, which upcoming tokamaks like SPARC and ITER will undergo, we design a rampdown trajectory and perform a predict-first experiment by increasing the plasma current by 20\%, from 140kA to 170kA, for a high $\beta_N$ plasma near the density limit, a scenario for which zero shots of rampdown data exists for TCV. In this extrapolation test, we \textit{a priori} predict the dynamics of key quantities to within sufficient accuracy to successfully terminate the plasma on both attempts.

The paper is organized as follows. We begin with an overview of the experiment and report the achieved rampdown improvements, as measured by the key figures of merits of plasma current $I_p$ and stored energy $W_{tot}$ at time of plasma termination. Then, an overview of the NSSM is provided along with medium scale validation metrics demonstrating its predictive power. This is followed by an analysis of two shots in the experiment demonstrating the importance of accounting for control errors in trajectory design for preventing a class of disruptions known as vertical displacement events (VDEs). Then, an analysis of 140kA shots in the experiment shows how incremental re-training between run days resulted in rampdowns that are both faster and less disruptive. Results from the predict-first extrapolation test are reported, demonstrating the ability of the NSSM to make small extrapolations. Finally, we discuss future work and implications for upcoming tokamaks like SPARC and ITER.

\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{img/intro_fig.png}
    \caption{Data from an example TCV pulse (\#73919), showing the rampup, flattop, and rampdown phases, which are defined by the plasma current $I_p$. The bottom subplot shows how the rampdown pushes the plasma closer to an instability limit, in this case the Greenwald density limit, defined by the Greenwald fraction $f_{GW} = 1$. Note this limit is approximate due to the, at present, incomplete physics understanding of the density limit \cite{giacomin2022first, maris2024correlation}. Also note that the flattop phase is abbreviated here to more clearly highlight the rampup and rampdown.}
    \label{fig:intro-fig}
\end{figure}

\newpage

\section{Results}
\subsection{Experiment Overview}
The reported experiment was conducted as a part of the 2024 TCV integrated control, high performance experimental campaign. Flattop plasmas operated at a high performance of $\beta_N > 2.0$ and near the density limit with a highly elongated diverted geometry with $\kappa\approx 1.6$ and $q_{95}\approx 4$. Initial shots in the experiment operated at $I_p=140$ kA, henceforth known as the \textit{baseline high performance} (HP) scenario, with a final \textit{extrapolation test} at $I_p=170$ kA. Successful rampdowns from these scenarios require careful management of multiple plasma instability limits that can be exacerbated by details of the plasma trajectories. To name a few considerations, an overly fast reduction in plasma current increases the Greenwald fraction, prompting a density limit event, managing the relatively high elongation diverted geometry introduces risks of a vertical displacement event (VDE), and fast reductions in elongation and minor radius can decrease the safety factor $q_{95}$, exciting magnetohydrodynamic (MHD) instabilities. Managing the density limit in this scenario is a particular challenge for fast terminations, as the relatively long particle confinement time-scale is a major constraint on the speed of the rampdown.

To address the problem, a NSSM dynamics model was trained on a modest dataset of past rampdowns, which contains only 5 incomplete rampdowns in the relevant high performance parameter space, as shown in Figure \ref{fig:overview}A-B. This NSSM is then used in a reinforcement learning (RL) environment to optimize a reward function, designed to minimize time to a goal plasma current of $40$kA and stored energy of $0.5$kJ without disrupting, as shown in Figure \ref{fig:overview}C. The action space was chosen to be plasma current, $I_p$, shaping parameters $\kappa$ and $a_{minor}$ and neutral beam injection (NBI) power $P_{NBI}$. User-specified constraints were set on the Greenwald fraction $f_{GW}$, safety factor $q_{95}$, vertical instability growth rate $\gamma_{vgr}$, and poloidal beta $\beta_p$. The optimized action trajectories were then manually programmed into the TCV plasma control system (PCS). The details of the reward function, chosen limits, and PCS programming process are further discussed in the Methods.

Every shot involved in the experiment is shown in Figure \ref{fig:overview}D, showing improvements on the normalized $I_p$ and $W_{tot}$ at time of plasma disruption over the course of the experimental runs. The un-optimized baseline rampdown trajectory for this scenario disrupted at relatively high current and stored energy in \#81101 and \#81102 at $I_p\approx 80$kA and $W_{tot}\approx 4$kJ. The experiment proceeded iteratively, with re-training of the NSSM on new data and trajectories done after shots \#81635, \#81745, \#81751, and \#81830. A preliminary optimized trajectory was deployed in TCV \#81635, which reached the goal $I_p$ and $W_{tot}$ before disrupting, but post shot analysis showed poor radial control and tracking of the target shape, which was determined to be due to a legacy software issue detailed in Figure \ref{fig:rmag-debug} in the Supplementary Information. Shots \#81741 and \#81745 were spent resolving this issue, with it properly resolved in \#81751, as shown in Figure \ref{fig:rmag-debug}. \#81751 still disrupted due to a VDE, which was found to be due to a large sensitivity of $\gamma_{vgr}$ to small control errors in the inner gap. After \#81751, an uncertainty distribution on gap errors was added to the RL training environment to gain robustness to this uncertainty, with subsequent shots experiencing similar control errors but without similar increases in $\gamma_{vgr}$, demonstrating the importance of designing trajectories with robustness to real-world uncertainties, as further discussed in Section \ref{subsec:prevent-vde}.

For the final run-day, trajectories were re-optimized, and predictions of the plasma dynamics were generated \textit{a priori} for both two reprisals of the baseline high performance scenario, but also for the extrapolation test. All four shots for both scenarios terminated successfully below the goal $I_p$, with the baseline scenarios realizing both faster and non-disruptive trajectories relative to baseline and successful a priori predictions of plasma dynamics for both scenarios.

Statistical significance of control results in fusion is typically difficult to establish due to the scarcity of experimental time and relevant data-points. This experiment also faces this, given the rampdown experiment involved only nine shots, two of which were dedicated to debugging a legacy software issue, with only five rampdowns in the database near the relevant high performance regime with $\beta_N > 2$. We use these five shots as our control set and define two test sets: one with the debugging shots and one without. As shown in Figure \ref{fig:hp-stats}, the Mann-Whitney U test \cite{mann1947test} shows a statistically significant improvement in $W_{tot}$ , ($p < 0.05$), at time of plasma termination of the experimental rampdowns for both definitions of the test set. Improvements in $I_p$ are only statistically significant when we do not include the debugging shots. While the results of this statistical test are encouraging, we urge caution in its interpretation given the small sample sizes involved, and the fact that tokamaks are highly drifting distributions in practice, with uncontrolled variables such as wall conditioning making a meaningful impact on experimental outcomes.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{img/overview.png}
    \caption{\textbf{A}) The training data distribution, with a modest dataset size at low performance and very few shots in the relevant high performance regime. The target scenarios for this work at 140kA and 170kA high normalized performance are shown. \textbf{B}) Depiction of the dynamics model training method, which involves comparing results from forward simulation of a NSSM against experimental data to compute the gradient of loss with respect to model parameters. \textbf{C}) Depiction of the trajectory optimization process. In addition to the trained dynamics model, the reinforcement learning (RL) training environment is defined by a reward function specifying the desired goal and a set of random variables that training environments are parallelized against, to find a trajectory that has robustness to uncertainties and off-normal events. \textbf{D}) The plasma current $I_p$ and stored energy $W_{tot}$ at plasma termination, relative to their values at the beginning of the termination phase for every shot during the experiment that reached the termination phase. \textbf{E}) A comparison of the current and stored energy trajectories for a baseline shot and an optimized shot, showing a faster, and non-disruptive, decrease in the plasma current and stored energy relative to the baseline.}
    \label{fig:overview}
\end{figure}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\linewidth]{img/hp_stats.png}
    \vspace{1em} % Add some spacing between the figure and the table
    \begin{tabular}{lcc} % Use 'cc' for centered columns
        \toprule
        & \multicolumn{1}{c}{$I_p$} & \multicolumn{1}{c}{$W_{tot}$} \\ % Center column headers
        \midrule
        Test Only & 0.024 & 0.0025 \\
        Test + Debug & 0.12 & 0.0020 \\
        \bottomrule
    \end{tabular}
    \caption{(Top) histograms of $I_p$ and $W_{tot}$ at time of plasma termination. (Bottom) p-values from the Mann-Whitney U test comparing performance of experimental shots relative to the control set of all shots in the database with $\beta_N$ > 1.5.}
    \label{fig:hp-stats}
\end{figure}
\newpage
\subsection{Medium-Scale Validation of NSSM Predictions}
The NSSM was developed and trained to predict the time-dependent dynamics of the set of observations in response to the set of actions listed in Table \ref{tab:obs_action}. The primary goal of the model is to predict the dynamics of key quantities relevant to completing the control task of a fast disruption-free plasma rampdown in response to actuation of controllable variables, to allow the trajectory optimization algorithm to decide on actions that avoid user-specified limits on key quantities correlated with disruptions.

The model underwent two training phases: an initial training phase on a wider dataset with 311 shots in the training dataset and 131 shots in the validation dataset. To improve the predictive power of the model for the relevant scenario, we fine-tune just the confinement scaling of the model by training only on 44 shots with $I_p \leq 200$kA, with all other model weights frozen. Due to the relatively small size of the fine-tuning dataset, we did not separate out a validation or testing dataset for this fine-tuning phase. As shown in Figure \ref{fig:model_val}, the trained model is able to predict the time-dependent dynamics of key 0D kinetic and disruptive quantities to within tens of percent for full rampdowns in the validation dataset, even in the 95th percentile of error. The percent errors for $\gamma_{vgr}$ can be relatively large, but, as shown in Figure \ref{fig:vgr-abs-err} this is largely attributable to the small value of $\gamma_{vgr}$ of limited plasmas as the absolute error is relatively low.

The NSSM was initially developed with a neural network predictor for the kinetic profiles on the full $\rho$ grid, and initial training runs found that the profile predictor can accurately predict kinetic profiles given the set of 0D scalars listed in Table \ref{tab:neural_op_in_out}. Figure \ref{fig:prof-predictor-results} provides an example comparison of predictions of the $T_e$ and $n_e$ profiles against Thomson measurements for a full shot in the validation dataset, showing accurate prediction across all phases of the shot. This result corroborates previous findings at NSTX-U that neural networks can accurately predict kinetic profiles given a set of similar 0D scalars \cite{boyer2021prediction}. Given this result suggests most of the relevant profile information is implicitly captured by 0D scalars, the profile predictor was disabled prior to running experiments to accelerate training, hence reported predictions of kinetic profiles are not predict-first.  A noteworthy feature of this profile predictor that should be explored in future work is its ability to function as a Thomson up-sampler, as the input variables are all sampled at a higher time resolution, 1ms, than the TCV Thomson Scattering system, which takes measurements every 17ms.

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.2} % Adjust spacing between rows for clarity
\begin{tabularx}{\textwidth}{@{}lXc@{}} % Add a dedicated column for units
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Units} \\
\midrule
\multicolumn{3}{c}{\textbf{Predicted Observations} $\mathbf{o}_t$} \\
\addlinespace
$W_{tot}$ & Plasma total stored thermal energy & kJ \\
$\bar{n}_{e,20}V$ & Line-averaged electron density times volume & $10^{20}$ \\
$f_{GW}$ & Greenwald fraction & - \\
$\beta_p$ & Plasma poloidal beta & - \\
$q_{95}$ & Safety factor at the 95\% flux surface & - \\
$\gamma_{vgr}$ & Vertical instability growth rate & 1000$s^{-1}$ \\
$T_e(\rho)$ & Electron temperature profile (not predict-first) & keV \\
$n_{e, 20}(\rho)$ & Electron density profile (not predict-first) & $10^{20} \, \text{m}^{-3}$ \\
\midrule
\multicolumn{3}{c}{\textbf{Actions} $\mathbf{a}_t$} \\
\addlinespace
$dI_p/dt$ & Plasma current ramp-rate & MA/s \\
$P_{NBI}$ & Neutral beam injection (NBI) heating & MW \\
$d\kappa/dt$ & Elongation ramp-rate & 1/s \\
$da_{minor}/dt$ & Minor radius ramp-rate & m/s \\
$d\delta/dt$ & Triangularity ramp-rate (zeroed for trajectory optimization) & 1/s\\
$g_{HFS}$ & Gap between the separatrix and the high-field side wall (constant 0.02m and zeroed after NBI  off) & m \\
$P_{ECRH}$ & Electron cyclotron resonance heating (ECRH) injected power (zeroed for trajectory optimization) & MW \\
$V_{gas}$ & Primary fueling gas valve voltage (zeroed for trajectory optimization) & Volts \\
\addlinespace
\bottomrule
\end{tabularx}
\caption{The set of observations predicted by the learned dynamics model in response to action inputs. The time derivatives of certain quantities are chosen as the action to allow the ramp-rate to be constrained during trajectory optimization. Note that certain actions are not optimized in this work, but are important to include for the purpose of training the dynamics model; these actions are simply set to zero during the trajectory optimization process.}
\label{tab:obs_action}
\end{table}
\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.75\textwidth]{img/initial_val.png}
    \caption{Model prediction accuracy as a function of prediction horizon during rampdowns on the validation set of 131 shots. Both individual data points and percentiles are shown. Figure \ref{fig:fine_tune_val} in the Supplementary Information shows similar model performance on the smaller scale fine tuning dataset.}
    \label{fig:model_val}
\end{figure}
\newpage

\begin{table}[!htbp]
\centering
\renewcommand{\arraystretch}{1.2} % Adjust spacing between rows for clarity
\begin{tabularx}{\textwidth}{@{}lXc@{}} % Define a table with left, expandable, and centered columns
\toprule
\textbf{Parameter} & \textbf{Description} & \textbf{Units} \\
\midrule
\multicolumn{3}{c}{\textbf{Inputs}} \\
\addlinespace
$W_{tot}$ & Plasma total stored energy & $10^4$ J \\
$\bar{n}_{e,20}$ & Line-averaged electron density (measured by interferometry) & $10^{20} \, \text{m}^{-3}$ \\
$P_{NBI}$ & Neutral beam injection (NBI) heating & MW \\
$P_{ECRH}$ & Electron cyclotron resonance heating (ECRH) injected power & MW \\
$I_p$ & Plasma current & $10^5$ A \\
$\kappa$ & Elongation & [-] \\
$a_{minor}$ & Minor radius & m \\
$\delta$ & Triangularity & [-] \\
\midrule
\multicolumn{3}{c}{\textbf{Predictions}} \\
\addlinespace
$T_e(\rho)$ & Electron temperature profile & keV \\
$n_{e, 20}(\rho)$ & Electron density profile & $10^{20} \, \text{m}^{-3}$ \\
\addlinespace
\bottomrule
\end{tabularx}
\caption{Input and output quantities for the profile predictor.}
\label{tab:neural_op_in_out}
\end{table}

\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{img/profile_predictor_results.png}
    \caption{(Left) an example of predictions made by the profile predictor on a validation dataset shot against Thomson measurements, showing both the ability of the model to up-sample Thomson measurements and provide a smoothing effect. (Right) the distribution of prediction percent errors on the validation set. The percent error is defined as the integrated error between the prediction and measurement normalized to the average value of the profile. For the $T_e$ profile this is: $100\frac{\int_0^1 |T_{e, Thom}(\rho) - T_{e, pred}(\rho)| d\rho}{\int_0^1 T_{e, Thom}(\rho) d\rho}$. Note that this metric is a pessimistic performance metric as it also captures error due to random measurement noise.}
    \label{fig:prof-predictor-results}
\end{figure}



\newpage
\subsection{Preventing VDEs by Designing for Robustness to Control Error}
\label{subsec:prevent-vde}
The experiment also clearly highlighted the importance of accounting for control errors when optimizing rampdown trajectories. The rampdowns for the initial shots of the experiment were designed without accounting for the impact of uncertainty in shape errors on the vertical growth rate $\gamma_{vgr}$. This uncertainty had a highly sensitive effect in TCV \#81751, which ended in a VDE. Even though the $\gamma_{vgr}$ at zero control error was tolerable, a small increase in the deviation of the high-field-side (HFS) gap, $g_{HFS}$, from the planned value caused an order-of-magnitude increase in $\gamma_{vgr}$, as shown in Figure \ref{fig:robustness_example}.

After \#81751, an uncertainty distribution on the gap errors was added to the RL training environments to encourage the optimization of trajectories that succeed despite this control error. The positive impact of optimizing for robustness to this uncertainty was realized with TCV \#82875, which experienced similar control errors at similar HFS gap values, but without the large increase in $\gamma_{vgr}$. This increased robustness is likely due to a change in the minor radius trajectory, which decreased the low-field-side (LFS) gap, thus increasing the stabilizing effect of the LFS wall whenever the plasma experiences an unexpected outward shift. Prior work at TCV has shown the importance of managing these gaps for vertical stability \cite{marchioni2024vertical}. The importance of accounting for this uncertainty is further highlighted by the fact that \#82875 is more stable in practice than \#81751, despite a higher elongation, the quantity most typically associated with vertical instability. In fact, we can see that \#81751, with its lower elongation, does have a lower $\gamma_{vgr}$ than \#82875 when gap error is near zero, but it is also much more sensitive to control errors.

This result demonstrates that the optimal trajectory for minor radius can differ, with significant consequence, once real-world errors and uncertainties are accounted for. Given that existing studies on rampdown design and optimization for ITER \cite{casper2013development} and DEMO \cite{van2023scenarioB}  find solutions involving large reductions in minor radius, these experimental results motivate the further advancement of techniques that enable trajectory design with robustness to uncertainty.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{img/gap_robustness.png}
    \caption{Comparison of a shot designed prior to adding control uncertainty, \#81751, and after, \#82875. We see that the vertical growth rate $\gamma_{vgr}$ is highly sensitive to gap control error in \#81751, but similar control errors experienced in \#82875 result in negligible changes in stability. Time shown is relative to 1.4 seconds for \#81751 and 1.3 seconds for \#82875 to align the moment the two shots experience similar control errors.}
    \label{fig:robustness_example}
\end{figure}
\newpage

\subsection{Improving Terminations by Incremental Re-training}
Both the NSSM and trajectories were incrementally re-trained on newly generated data from experimental run-days, which resulted in more robust and faster rampdowns for the baseline high performance scenario,  as shown in Figure \ref{fig:improve-140}. The speed of the model enabled re-training of both the model and trajectories in fewer than ten hours total on a single A100 GPU. The un-optimized solution in \#81101 involved a NBI power rampdown while keeping constant plasma current, to allow for a decrease in density to avoid the Greenwald limit, a solution which the RL approach initially decided on as well, as shown with \#81751, with an even more conservative current ramp and introducing a reduction in $\kappa$. As discussed in \ref{subsec:prevent-vde}, this shot resulted in a VDE, and, with the introduction of an uncertainty distribution on $g_{HFS}$, the solution in \#81830 resulted in less of a reduction in the minor radius $a_{minor}$, which helped eliminate the $\gamma_{vgr}$ spikes. Subsequent dynamics model training and trajectory optimization resulted in a solution in \#82876 which allows for an immediate reduction in $I_p$ without running into a density limit, highlighting the ability for the workflow to assist in gradually making improvements. All optimized trajectories involved a fast initial drop in $P_{NBI}$, followed by a slower ramp phase, although the rates and critical points for the transition differed from shot to shot.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{img/improving_140.png}
    \caption{Experimental traces of key actuators (left) and relevant quantities (right) for the baseline high performance scenario over the course of the experiment. Time is set relative to the beginning of the termination phase.}
    \label{fig:improve-140}
\end{figure}

\newpage
\subsection{Predict-First Results for the Extrapolation Test}\label{subsec:predict-first-170}
Learned dynamics models need not extrapolate far out of distribution to assist with control and trajectory design for net energy tokamaks, as their operations will involve incrementally moving towards higher performance. Thus, they simply need to be able to make reasonable predictions under small extrapolations, and rapidly learn from experiment with as few shots of data as possible. To test the viability of the developed approach in such a setting, we used the learned dynamics model to design trajectories for the extrapolation test scenario, for which zero shots of rampdown data exists in our training dataset for TCV, and generated \textit{a priori} predictions of the distribution of plasma dynamics during rampdown. 

As shown in Figure \ref{fig:170_predict_first}, experimental results from \#82878 largely fell within this distribution, with accurate predictions of the stored energy and density dynamics. Arguably the largest sources of error came from unreliable control of the plasma shape, contributing to errors in quantities like the rotational transform $\iota_{95}\equiv \frac{1}{q_{95}}$, and also leading to an earlier than expected HL back-transition. \#82878 also started near the density limit, a challenging situation which motivated the introduction of a delay to the $I_p$ ramp in the baseline scenario, but the RL algorithm was able to determine a trajectory to immediately decrease $I_p$, which is desirable, while keeping $f_{GW}$ roughly constant.  \#82877 fell further out of distribution due to a loss of NBI power, and the presence of a neo-classical tearing mode (NTM) at the beginning of rampdown that did not exist in TCV \#82878, as shown by Figure \ref{fig:ntm} in the Supplementary Information. Fortunately, these un-modeled off-normal events (ONEs) did not take the plasma far enough out of distribution to cause a disruption. As discussed earlier, the profile predictor was removed to help accelerate trajectory optimization, but
post shot evaluation of the profile predictor on the 0D scalars generated by the training environment, shown in Figure \ref{fig:profile-pred-170}, shows reasonable agreement against experimental Thomson measurements.

The results from this experiment demonstrate the ability for the learned dynamics model to make small extrapolations to sufficient accuracy to enable the design of robust disruption-free trajectories via RL, and even the prediction misses in TCV \#82877 further emphasize the importance of further advancing the developed methodology to design with robustness to as many ONEs as possible. 

\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{img/170kA_pred_first.png}
    \caption{Action trajectories and a priori predictions of plasma dynamics during rampdown, compared to experimental results from TCV \#82877 and \#82878. The RL training environment accounts for uncertainty in actuation with distributions on the action trajectory; the average of the distribution is used for shot programming. Control of the plasma shape proved to be a challenge for this phase, an issue also observed in previous rampdown studies \cite{mehta2024automated, van2023scenarioA}.}
    \label{fig:170_predict_first}
\end{figure}
\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{img/profile_pred_170kA.png}
    \caption{Post-hoc predictions of kinetic profiles compared to Thomson Scattering measurements.}
    \label{fig:profile-pred-170}
\end{figure}
\newpage
\section{Discussion}
Our results demonstrate that the developed approach to learning plasma dynamics can predict the highly transient rampdown phase with a modest dataset and even make small extrapolations to higher performance regimes. The relative sample efficiency of the approach, only requiring five shots in the relevant high performance regime, indicates this may be a viable approach for upcoming tokamaks like SPARC and ITER, which will initially operate at low performance before incrementally increasing performance. Developing robust terminations during such incremental campaigns is crucial, as highlighted by the 2020 JET high performance campaign where a 15\% increase in plasma current, from 3MA and 3.5MA, raised disruptivity from $\approx 20\%$ to $\approx 50\%$ \cite{garzotti2023development}. Prediction metrics on the validation dataset, as shown in Figure \ref{fig:model_val}, shows this approach yields accurate predictions for the majority of ramp-downs, but the 5\% worst cases can involve large prediction errors, meriting further investigation.

The developed RL approach for designing robust trajectories yielded promising improvements in the plasma current and stored energy at time of termination, with incremental re-training improving the ramp speed. This result represents one of the first successful demonstrations of trajectory design with robustness to real-world uncertainties for tokamaks, which has historically been infeasible due to the computational cost of simulation. A degree of statistical significance is shown, but the sample size is still relatively small; a larger scale study would more thoroughly determine the efficacy of the approach. Although a large set of uncertainties was accounted for, detailed further in Table \ref{tab:uncertainty_model}, experimental results involved additional uncertainties, such as the NBI failure in \#82877, that still need to be addressed to further improve the robustness of trajectories. Robustness to hardware failure is of particular interest for future work as an exhaustive survey of disruption causes at JET has revealed hardware failure as a significant contributing factor to disruptions \cite{de2011survey}. It is also noteworthy that the RL designed action trajectories tended to be relatively simple, suggesting that the key important ingredient is the fast and parallelized simulation model, as a human expert may be able to find similar trajectories if given access to the simulation model.

To improve the relevance of the developed approach to devices like SPARC and ITER, future work should model additional physics like impurity accumulation and neo-classical tearing mode dynamics, both of which are difficult to simulate, partially stochastic, and have been found to be significant contributing factors to disruptions at JET \cite{de2011survey}.  Accounting for such effects that can drastically change the plasma dynamics may motivate the employment of real-time adjustments to the rampdown trajectory, or the deployment of a library of trajectories as was done in previous simulation studies \cite{wang2024active}. Applying the developed approach to learning JET rampdown dynamics would also further inform the application of this approach to SPARC and ITER.

The developed approach also holds promise for full-shot simulation, which ongoing work is investigating \cite{wang2024plasma}. The ability for a neural network to predict kinetic profiles using 0D scalars, demonstrated both in this work and in prior work \cite{boyer2021prediction}, suggests a data-driven approach may be sufficient for certain control tasks without principles-based transport simulation, which can be extremely computationally expensive and require strong assumptions on edge temperature and density. The ability to deploy accurate, fast, and massively parallel simulators of tokamak plasmas would likely unlock new capabilities for tokamak trajectory and control design, allowing for more reliable access to higher performance plasmas, and ameliorating the risk posed by plasma disruptions to future tokamaks.

\section{Methods}
\subsection{The Neural State-Space Model}
Learning dynamical systems from data has been a core discipline within  control design for decades, including aircraft flight control \cite{chyczewski2022summary} and simulation \cite{allerton2009principles}, and has historically been known as \textit{system identification} \cite{morelli2016aircraft,aastrom1971system}. However, due to computational limitations of the time, classical approaches have typically been restricted to linear models, often in the form of linear state-space models (SSMs):
\begin{subequations}
\begin{align}
        \dot{\mathbf{x}} &= A\mathbf{x} + B\mathbf{a}\\
    \mathbf{o} &= C\mathbf{x} + D\mathbf{a}
\end{align}
\end{subequations}
Where $A$, $B$, $C$, and $D$ are the matrices to be learned from datasets of observables, $\mathbf{o}$, actions, $\mathbf{a}$, and, possibly, states, $\mathbf{x}$. We note that the controls literature typically uses the notation $\mathbf{y}$ in lieu of $\mathbf{o}$ and $\mathbf{u}$ in lieu of $\mathbf{a}$, reflecting a difference in notation between the controls and RL communities, but here we use RL notation for consistency. In the modern deep-learning learning era, this idea of learning dynamical systems from data was re-discovered from a different perspective, with the advent of the neural differential equation (NDE) \cite{chen2018neural}:
\begin{align}
    \dot{\mathbf{x}} = NN_\theta(\mathbf{x})
\end{align}
where it was discovered that, given datasets of $\mathbf{x}$, a neural network, $NN_\theta$, can be used as a system of differential equations that is integrated forward in time with a differential equations solver, and then \textit{adjoint back-propagation} methods can be used in conjunction with automatic differentiation to determine the gradient of loss with respect to the network parameters $\theta$ \cite{kidger2022neural,chen2018neural,rackauckas2020universal}. The introduction of flexible machine learning frameworks has enabled the development of the field of \textit{Scientific Machine Learning} (SciML) based around the core idea of extending NDEs to include physics, and other domain-specific, structure \cite{chen2018neural,rackauckas2020universal}. One extension that completes the circle with the classical linear SSM is the neural state-space model (NSSM), which re-introduces the concepts of actions and observations:
\begin{subequations}
    \begin{align}
        \dot{\mathbf{x}}(t) &= f_\theta(\mathbf{x}, \mathbf{a})\\
        \mathbf{o}(t) &= O_\theta(\mathbf{x}, \mathbf{a})
    \end{align}
\end{subequations}
Thanks to the power of new highly flexible machine learning frameworks such as JAX and the Julia SciML ecosystem, $f_\theta$ and $O_\theta$ can be programmed to include arbitrary combinations of neural networks, physics formulas, and even classical data-driven models such as power laws, a capability which we exploit in this work. The training process of a NSSM is shown in Figure \ref{fig:train-nssm}, and involves the simulation of the NSSM forward in time using an initial state $\mathbf{x}_0$ and a time series of actions $\mathbf{a}_{0:T}$ from an experimental database. The error of the simulation results against the experimental data is computed, and adjoint methods and automatic differentiation are used to determine the gradient to reduce the loss. In this work, the differential equation solver package diffrax \cite{kidger2022neural} is used, which includes the integration of multiple adjoint methods with the JAX automatic differentiation system, which allows backpropagation through all differential equation solvers in the package.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.95\linewidth]{img/nssm_train.png}
    \caption{The training process of a NSSM, showing quantities from the experimental database, computed quantities, and functions. The loss functional is defined as the time-integrated value of an instantaneous loss function $l$, and $\nabla_\theta\mathcal{L}$ denotes the gradient of the parameter vector w.r.t. the value of the loss functional.}
    \label{fig:train-nssm}
\end{figure}
\newpage
\subsubsection{The Dynamics Function $f_\theta(\mathbf{x}, \mathbf{a})$}\label{subsec:nssm}
We begin by defining the following confinement laws:
\begin{subequations}
\begin{align}
    \tau_{n, pred}(\mathbf{x}, \mathbf{a}) &= c_{n}I_p^{c_{I,n}}\bar{n}_{e,20}^{c_{n,n}}P_{input}^{c_{P,n}}\kappa^{c_{\kappa,n}}\epsilon^{c_{\epsilon,n}}|\dot{I}_p|^{c_{\dot{I}_p,n}}NN_{conf,0}(\mathbf{x}, \mathbf{a})(c_{n,h}\bar{n}_{e,20}^{c_{n,n,h}}P_{input}^{c_{P,n,h}})^{\text{hmode}(\mathbf{x}, \mathbf{a})}\\
    \tau_{E, pred}(\mathbf{x}, \mathbf{a}) &= \underbrace{c_{E}I_p^{c_{I,E}}\bar{n}_{e,20}^{c_{n,E}}P_{input}^{c_{P,E}}\kappa^{c_{\kappa,E}}\epsilon^{c_{\epsilon,E}}|\dot{I}_p|^{c_{\dot{I}_p,E}}}_{\text{L-mode}}\underbrace{NN_{conf,1}(\mathbf{x}, \mathbf{a})}_{\text{NN correction}}\underbrace{(c_{E,h}\bar{n}_{e,20}^{c_{n,E,h}}P_{input}^{c_{P,E,h}})^{\text{hmode}(\mathbf{x}, \mathbf{a})}}_{\text{H-mode correction}}\\
    \text{hmode}(\mathbf{x}, \mathbf{a}) &= \text{tanhHeaviside}(P_{input} - c_{h}\bar{n}_{e,20}^{c_{h,n}}a_{minor}^{c_{h,a}})\\
        \text{tanhClip}(x)&\equiv \tanh\left(\frac{2k}{\text{max} - \text{min}} (x - \text{center})\right) \frac{\text{max} - \text{min}}{2} + \text{center}\\
    \text{tanhHeaviside}(x) &\equiv \frac{1}{2}(\tanh(kx + 1))
\end{align}
\end{subequations}
where the parameters to be learned include all coefficients $c_*$ and neural network parameters. The laws are structured to multiply a portion corresponding to L-mode, a neural network correction factor, and an H-mode correction factor. The L-mode term reflects standard confinement scalings, but with the introduction of a $\dot{I}_p$, which was found to help better capture short-term effects of ramping plasma current. The neural network output includes a tanhClip final activation that constrains it's output to the range $[0.75, 1.25]$, thus controlling the maximum adjustment the network is allowed to make. The $\text{hmode}$ function includes a tanhHeaviside function which provides a smooth transition between one to zero once $P_{input}$ falls below the learned back-transition threshold, which is structured to reflect the Martin scaling \cite{martin2008power}. Note that the use of the $\text{hmode}$ function output as a power de-activates the H-mode correction term once $\text{hmode}$ transitions from one to zero. While, in principle, the neural network should be able to learn the effects of H-mode implicitly, we found that adding an explicit H-mode correction factor helped improve model predictions in our low-data regime. The $k$ factor controls the smoothness of both the tanhClip and tanhHeaviside functions.

These confinement laws are integrated as a part of the following 0D energy and particle balance equations, which is a model that blends simple physics principles, power laws, and neural networks:
\begin{subequations}
    \begin{align}
    \frac{dW_{tot}}{dt} &= -\underbrace{\frac{W_{tot}}{\tau_{E, pred}}}_{\text{Transport}} + \underbrace{I_p^2NN_{ohm, rad, 0}(\mathbf{x}, \mathbf{a})}_{\text{Ohmic Heating}} - \underbrace{\bar{n}_{e, 20}VNN_{ohm, rad, 1}(\mathbf{x}, \mathbf{a})}_{\text{Radiated Power}} + \underbrace{P_{NBI} + P_{ECRH}}_{\text{Aux. Heating}}\\
    \frac{d(\bar{n}_{e,20}V)}{dt} &= -\underbrace{\frac{\bar{n}_{e, 20}}{\tau_{n, pred}}}_{\text{Transport}} + \underbrace{c_{NBI}P_{NBI}}_{\text{NBI Fueling}} + \underbrace{c_{gas,0}\sigma(c_{gas,1}V_{gas} + c_{gas,2})}_{\text{Gas Valve Fueling}} + \underbrace{NN_{wall}(\mathbf{x}, \mathbf{a})\exp^{-c_{wall}g_{HFS}}}_{\text{Wall Effects}}
\end{align}
\end{subequations}
where $\sigma$ is the sigmoid function, $NN_{ohm, rad}$ is a network that predicts two quantities; the first is multiplied by $I_p^2$ to serve as an Ohmic heating term, and the second is multiplied by density and volume to serve as the radiated power term. $NN_{wall}$ is included to account for possible wall fueling effects when in a limited configuration, and is multiplied by an exponential in the HFS gap to de-activate it when diverted. Additional simple constants are included to account for fueling from both NBI and gas puffing. We note that, in both cases, the included terms do not capture important state dependencies and time delays, but they proved sufficient for this use case. All NNs used in the dynamics function $f$ are simple MLPs with GELU activations on the hidden state and tanhClip functions as final activations to constrain their outputs to reasonable ranges. The dynamics of density times volume are predicted; in cases where density itself is used (e.g. to compute the Greenwald Fraction), the following volume approximation is used to recover density:
\begin{align}
    V\approx 2\pi R^2\epsilon^2\kappa \left(\pi - \left(\pi - \frac{8}{3}\right)\epsilon\right)
\end{align}
Since time derivatives of quantities, $\dot{I}_p, \dot{\kappa}, \dot{a}_{minor}, \dot{\delta}$ are used as actions, their integrated values are also added as state variables with trivial dynamics.

\subsubsection{The Observation Function $O_\theta(\mathbf{x}, \mathbf{a})$}
The observation function consists of several components: a NN predictor for $\gamma_{vgr}$, a profile predictor, and simple physics formulae to compute derived quantities:
\begin{subnumcases}{O_\theta(\mathbf{x}, \mathbf{a})}
        \beta_p = \frac{8}{3}\frac{W_{tot}}{\mu_0R_0I_p^2}& \\
    f_{GW} = \frac{\bar{n}_{e, 20}\pi a_{minor}^2}{I_{p,MA}}&\\
    q_{95} = \frac{4.1 a_{minor}^2 B_0}{R_0  I_{p, MA}} 
 \left(1 + 1.2  (\kappa - 1) + 0.56 (\kappa - 1)^2\right) 
 \left(1 + 0.09  \delta + 0.16  \delta^2\right) 
 \frac{1 + 0.45  \delta  \epsilon}{1 - 0.74  \epsilon}&\\
    \gamma_{vgr} = NN_{vgr}(\mathbf{x}, \mathbf{a})&\\
    T_e(\rho), N_e(\rho) = NN_{prof}(\mathbf{x}, \mathbf{a})
\end{subnumcases}
where $\beta_p$ is computed in accordance to the LIUQE definition \cite{moret2015tokamak}, $f_{GW}$ is the usual Greenwald Fraction \cite{greenwald1988new}, $q_{95}$ is the approximation given in \cite{sauter2016geometric} with the squareness factor set to 1, $NN_{vgr}$ is a MLP with GELU activation and a scaled sigmoid output, and $NN_{prof}$ is a neural-operator based profile predictor, discussed further in the next subsection.

\newpage
\subsubsection{Neural Operator Based Profile Predictor}
 Prior work at NSTX-U trained a neural network to successfully predict kinetic profile shapes using their averages plus zero dimensional control parameters such as plasma current, shaping, and auxiliary heating. Building upon this prior work, we show that, on TCV data, kinetic profiles can be predicted to reasonable accuracy with a neural network using the stored energy $W_{tot}$, line-averaged electron density $\bar{n}_{e, 20}$, and control parameters. The key implication is that accurate prediction of the time-dependent dynamics of just two scalars, $W_{tot}$ and $\bar{n}_{e, 20}$, implies reasonable prediction of the dynamics of kinetic profiles.
\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{img/pca_ev_profiles.png}
    \caption{(Left) explained variance as a function of number of PCA modes, showing high explained variance with a relatively low dimensional representation of kinetic profiles on a database of 1341 shots. (Right) an example of kinetic profiles and their corresponding approximation with three PCA modes.}
    \label{fig:pca-ev-profiles}
\end{figure}

We leverage methods developed by the neural operator \cite{kovachki2023neural, li2020neural} literature, which has found success for solving machine learning problems in scientific domains involving PDEs. Letting $f_{in}$ denote an input function and $f_{out}$ denote an output function, a neural operator $\mathcal{F}$ parameterized by $\theta$ maps an input function to an output function:
\begin{align}
    f_{out} = \mathcal{F}_\theta(f_{in})
\end{align}
In practice, the functions involved are approximated using a set of basis functions, thus the practical implementation results in a neural network operating on basis function coefficients. In this work, we make use of cubic B-spline basis functions to represent the kinetic profiles:
\begin{align}
    T_{e}(\rho) = \sum_{i=1}^{n_{basis}}c_{T,i}B_{i, 3}(\rho) \quad n_{e, 20}(\rho) = \sum_{i=1}^{n_{basis}}c_{n, i}B_{i, 3}(\rho)
\end{align}
And we predict these profiles using a set of 0D scalars, where every scalar is a control parameter except stored energy $W_{tot}$ and $\bar{n}_{e,20}$. The full set of input and output parameters is listed in Table \ref{tab:neural_op_in_out}. During training, the $\rho$ grid corresponding to the dataset is chosen to evaluate the basis functions, but arbitrary alternative grids can be used during inference time.

\newpage

\subsection{Training Data Distribution}
The dataset used for training models in this work consists of 442 shots with rampdowns that are at least partially complete, gathered with DEFUSE (Disruption and Event analysis framework for FUSion Experiments) \cite{pau2023modern}. The initial training phase involved training on 311 shots of data, with the rest of the dataset used for validation. After the initial training phase, the model is further trained on a fine-tuning dataset of 44 shots, during this phase all of the model weights except those in the $\tau_E$ and $\tau_N$ hybrid confinement laws described in \ref{subsec:nssm} are frozen. As shown in Figure \ref{fig:data-distribution}, the dataset consists of only five shots of data anywhere near the relevant high-performance region.
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{img/data_distribution.png}
    \caption{The data distribution used for training the NSSM, visualized in ($I_p$, $f_{GW}$) space and ($I_p$, $\beta_N$) space.}
    \label{fig:data-distribution}
\end{figure}


\subsection{Reward Function}
The reward function is designed to balance the priority of achieving a low plasma current and energy against the risk of disrupting the plasma, and is given by:
\begin{equation} \label{eq:reward_fn}
    r(\mathbf{x}(t), \mathbf{a}(t)) = \underbrace{-c_{time}}_{\text{Penalty for time}} - \underbrace{c_{W}W_{tot}(t) - c_{I_p}I_{p}(t)}_{\text{Penalty for current and energy}} - \underbrace{\sum_{i=1}^{n_{soft}} c_{soft}s_i(\mathbf{x}(t))}_{\text{Soft chance-constraints}} - \underbrace{\sum_{i=1}^{n_{hard}} c_{hard}h_i(\mathbf{x}(t))}_{\text{Hard chance-constraints}}
\end{equation}
The reward function is active for every time step before hitting the goal state or maximum allowed training episode time. The goal state is chosen to be a stored energy of 500J and a plasma current of 40kA, as, for the 170kA extrapolation test scenario, 40kA approximately corresponds to the relative plasma current for an ITER 15MA benign termination, which is defined as 3MA \cite{de2017multi}. A constant penalty term is active for every time step before achieving the goal to encourage time minimization. In addition, penalty terms that scale with plasma current and energy are included to further prioritize moving towards a safer state. To avoid disruptive limits such as high Greenwald fraction during the rampdown, penalty terms are added for states that violate user-specified constraints on key quantities correlated with disruptions. 

One challenge with specifying constraint limits is the difference in severity of violating different constraints, and the, at times, weak correlations between physical quantities and disruptions. To address this issue, we partition constraints into ``soft'' constraints, which incur a small penalty to discourage, but not forbid, the algorithm from finding solutions that violates these limits, and ``hard'' constraints which incur a large penalty to strictly enforce constraint violation. We note that while methods in the constrained optimization literature often mathematically express constraints separately from the objective function being optimized, most practical implementations of constrained optimization algorithms enforce constraints by rewriting constraints as penalty terms in the objective function \cite{nesterov2018lectures,bertsekas2014constrained}, an approach we also adopt. Stochastic optimization across a distribution of outcomes introduces a challenge: trying to avoid limits for every scenario will likely result in excessively slow and conservative solutions \cite{bertsimas2004price}, which itself poses it's own risk. To address this challenge, we utilize \textit{chance-constraints}, a technique often utilized in the autonomous driving literature \cite{qin2023review, wang2020non}, and only activate the constraint if violation probability exceeds a certain threshold. In this set of experiments, this threshold is chosen as 5\%. Reward function parameters used for the final four shots are shown in Table \ref{tab:reward_params}.
\begin{table}[!htbp]
  \centering
  \begin{tabular}{llr}
      \toprule
      \textbf{Category} & \textbf{Parameter} & \textbf{Value} \\
      \midrule
      \multirow{2}{*}{Hard Limits} & $f_{GW}$ & 1.0 \\
       & $\iota_{95}$ & 0.5 \\
      \midrule
      \multirow{4}{*}{Soft Limits} & $f_{GW}$ & 0.8 \\
       & $\beta_p$ & 1.75 \\
       & $\gamma_{vgr}$ & 0.75 \\
       & $\iota_{95}$ & 0.313 \\
      \midrule
      \multirow{5}{*}{Parameters} 
       & $c_{time}$ & 5.0 \\
       & $c_{I_p}$ & 1.0 \\
       & $c_W$ & 1.0 \\
       & $c_{soft}$ & 1.0Ã10\textsuperscript{3} \\
       & $c_{hard}$ & 5.0Ã10\textsuperscript{4} \\
      \bottomrule
  \end{tabular}
  \caption{Reward function parameters. Note that the values corresponding to limits are the maximum allowed values.}
  \label{tab:reward_params}
\end{table}

\newpage
\subsection{Uncertainty Model}
In experimental reality, the time evolution of plasma dynamics is highly nonlinear and subject to considerable amounts of uncertainty, as evidenced by the two same-scenario shots shown in Figure \ref{fig:170_predict_first} which begin at drastically different initial conditions. To design trajectories that have robustness to large variability and off-normal events, we defined an uncertainty model for the RL training environments, and sampled from this uncertainty model for each training environment used during training. The uncertainty model includes random variables for both the initial state of the plasma during rampdown and disturbances/model uncertainties that affect the time-varying plasma dynamics. To account for the fact that accidental H-L back-transition implies the initial state of the plasma may start in either H or L-mode, the initial state distribution is modeled as a bi-modal mixture model, with a 50\% chance of any given RL training environment starting in either H or L-mode. In some cases, uncertainty distributions could easily be quantified from past experimental data (such as tracking error in the plasma current), or from model prediction accuracy (such as $\gamma_{vgr}$), but in other cases the distribution was chosen in ad-hoc fashion, upon identifying additional sources of uncertainty in experiment. Table \ref{tab:uncertainty_model} summarizes the random variables, parameterized distributions, and quantification methods used in this work. As discussed in Section \ref{subsec:predict-first-170}, this uncertainty model proved to be non-exhaustive in experiment, further highlighting the need to advance experimental uncertainty quantification and robust control in the context of fusion plasma control.
\begin{table}[!htbp]
\centering
\begin{tabularx}{\textwidth}{@{}lXl@{}}
\toprule
\textbf{Random Variable} & \textbf{Distribution (140kA/170kA scenarios)} & \textbf{Quantification Method} \\
\midrule
\multicolumn{3}{c}{\textbf{Disturbances}} \\
\addlinespace
H-L back-transition threshold mult. factor & $N(\mu=1.15,\sigma=0.15)$  / $N(\mu=1.15,\sigma=0.1)$ & Ad-hoc\\
Ohmic heating mult. factor & $N(\mu=1, \sigma=0.1)$ / $N(\mu=1, \sigma=0.1)$ & Model prediction error\\
Radiated power mult. factor & $N(\mu=1, \sigma=0.5)$ / $N(\mu=1, \sigma=0.5)$ & Model prediction error \\
Energy confinement time mult. factor & $N(\mu=1, \sigma=0.075)$ / $N(\mu=1, \sigma=0.075)$ & Model prediction error \\
Particle confinement time mult. factor & $N(\mu=1, \sigma=0.075)$ /  $N(\mu=1, \sigma=0.075)$ & Model prediction error \\
Wall effects NN mult. factor & $N(\mu=1, \sigma=0.2)$ / $N(\mu=1, \sigma=0.2)$ & Ad-hoc \\
$\gamma_{vgr}$ mult. factor & $N(\mu=1,\sigma=0.2)$ / $N(\mu=1,\sigma=0.2)$& Model prediction error \\
$g_{HFS}$ actuation mult. factor  & $N(\mu=1, \sigma=0.5)$ / $N(\mu=1, \sigma=0.5)$ & Database Control Errors \\
Limited $\tau_E$ mult. factor & $U(1, 1.15)$ / $U(1, 1.2)$ & Ad-hoc \\
Limited $\tau_n$ mult. factor & $U(1, 1.15)$ / $U(1, 1.2)$  & Ad-hoc \\
H-mode correction factor for $\tau_n$ & $U(0.3, 0.5)$ / $U(0.3, 0.5)$  & Ad-hoc \\
Jump in $\kappa$ at the diverted to limited transition & $U(-0.2, 0.0)$ / $U(-0.2, 0.0)$ & Database Control Errors \\
\midrule
\multicolumn{3}{c}{\textbf{L-mode Initial State}} \\
\addlinespace
$I_p$ & $N(\mu=1.4, \sigma=0.025)$ / $N(\mu=1.7, \sigma=0.04)$  & Database Control Errors \\
$W_{tot}$ & $N(\mu=10, 0.5)$ / $N(\mu=12, 0.5)$ & Database Control Errors \\
$\bar{n}_{e, 20} V$ & $N(\mu=0.8,\sigma=0.05)$ / $N(\mu=0.8,\sigma=0.05)$ & Database Control Errors \\
$\kappa$ & $N(\mu=1.575, 0.03)$ / $N(\mu=1.575, 0.03)$ & Database Control Errors \\
$a_{minor}$ & $N(\mu=0.21, 0.005)$ / $N(\mu=0.21, 0.005)$ & Database Control Errors \\
$\delta$ & $N(\mu=0.275, 0.01)$ / $N(\mu=0.275, 0.01)$ & Database Control Errors \\
$P_{NBI}$ & $U(1.05, 1.15)$ / $U(1.1, 1.2)$ & Database Control Errors \\

\midrule
\multicolumn{3}{c}{\textbf{H-mode Initial State}} \\
\addlinespace
$I_p$ & $N(\mu=1.4, \sigma=0.025)$ / $N(\mu=1.7, \sigma=0.04)$& Database Control Errors \\
$W_{tot}$ & $N(\mu=12.5, 0.5)$ / $N(\mu=16, 0.5)$ & Database Control Errors\\
$\bar{n}_{e, 20} V$ & $N(\mu=0.9,\sigma=0.05)$ / $N(\mu=1.1,\sigma=0.05)$ & Database Control Errors\\
$\kappa$ & $N(\mu=1.575, 0.03)$ / $N(\mu=1.575, 0.03)$ & Database Control Errors\\
$a_{minor}$ & $N(\mu=0.21, 0.005)$ / $N(\mu=0.21, 0.005)$ & Database Control Errors \\
$\delta$ & $N(\mu=0.275, 0.01)$ / $N(\mu=0.275, 0.01)$ & Database Control Errors \\
$P_{NBI}$ & $U(1.05, 1.15)$ / $U(1.1, 1.2)$ & Database Control Errors \\
\addlinespace
\bottomrule
\end{tabularx}
\caption{The uncertainty model used in RL training environments to design trajectories with robustness to distributional uncertainty.}
\label{tab:uncertainty_model}
\end{table}

\newpage
\subsection{RL Methods}
Standard RL problems involve optimizing a policy $\pi$ to map observations to actions:
\begin{align}
    \mathbf{a} = \pi(\mathbf{o})
\end{align}
from this perspective, trajectory optimization can be viewed as policy optimization where the only observable is time:
\begin{align}
    \mathbf{a} = \pi(t)
\end{align}
After an initial trial with Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, we found OpenAI-ES, an evolutionary strategy (ES) designed for policy optimization, to work better in practice \cite{salimans2017evolution}. This is possibly explained by the theoretical analysis given in the paper introducing OpenAI-ES, which suggests that RL problems with long time horizons and actions that have long-lasting effects may be better solved with ES approaches than the dominant paradigm of policy gradient methods \cite{salimans2017evolution}.
\subsection{Deployment to TCV}
Shape trajectories determined via RL were mapped to last-closed-flux-surface control points via re-scaling of the flat-top shape for the diverted phase, and using an analytic formula in the TCV MGAMS \cite{hofmann1994creation} algorithm for the limited phase. Feed-forward coil currents and voltages to achieve the desired plasma current and shaping trajectories were then determined with the free-boundary equilibrium code FBT and shot preparation algorithm MGAMS \cite{hofmann1994creation, hofmann1988fbt}, and the $P_{NBI}$ trajectory was programmed into the TCV supervisory control system SAMONE \cite{galperti2024overview,vu2021integrated}.

\section{Acknowledgments}
This work was funded in part by Commonwealth Fusion Systems. This work has been carried out within the frame-work of the EUROfusion Consortium, via the Euratom Research and Training Programme (Grant Agreement No 101052200 - EUROfusion) and funded by the Swiss State Secretariat for Education, Research, and Innovation (SERI). Views and opinions expressed are however those of the author(s) only and do not necessarily reflect those of the European Union, the European Commission, or SERI. Neither the European Union nor the European Commission nor SERI can be held responsible for them. The authors would like to acknowledge the Engaging cluster, managed by the MIT Office of Research Computing and Data, which was used in this work for model training.

\section{Author Contributions}
Allen M. Wang led the project, developed the dynamics modeling approach and RL problem formulation, and led writing of the paper. Alessandro Pau developed the machine learning dataset, high performance integrated control scenario, and integration of the approach into TCV. Oswin So and Charles Dawson worked with Allen M. Wang to develop the RL training environments and methods. Olivier Sauter enabled TCV integration, and identified and debugged the radial control issue. Cristina Rea managed the collaboration and advised the project on disruptions and machine learning, contributed to manuscript drafting and revisions. Mark D. Boyer advised the project from a controls perspective and motivated the approach taken for dynamics modeling. Anna Vu and Cristian Galperti enabled integration with the integrated control system. Chuchu Fan advised Allen M. Wang, Oswin So, and Charles Dawson on controls and RL approaches. Antoine Merle helped with MHD equilibrium codes. Yoeri Poels and Cristina Venturini helped with the TCV dataset. Stefano Marchioni developed the growth rate calculation method used in this work.

\section{Data Availability}
Data to generate figures found in this paper are available in the source code repository. The complete dataset used for training and validation can be obtained by contacting Alessandro Pau or Allen M. Wang.
\section{Code Availability}
Source code is provided in the supplementary materials.

\bibliography{references}

\newpage
\section{Supplementary Information}
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=\linewidth]{img/def_begin_term_end_term.png}
    \caption{Definition of the beginning and end of the termination phase for every shot in this experiment. Note that in 81830, a low level software issue triggered an unexpected spike in plasma current at the very end of the rampdown. We define the termination time for that shot as the lowest achieved plasma current, as it is expected that net energy tokamaks where disruptions are of concern will have protection systems that prevent such incidents.}
    \label{fig:def_begin_end}
\end{figure}

\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{img/rmag_debug.png}
    \caption{Plot of planned radial magnetic position $r_{mag}$ against observed values from equilibrium reconstruction with LIUQE, showing exceptionally poor control performance during initial rampdown attempts. The issue was identified as a legacy $\frac{1}{I_p^2}$ term used in generating the radial position observer feed-forward trajectory, with the issue finally fixed in TCV\#81751.}
    \label{fig:rmag-debug}
\end{figure}

\newpage
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{img/vgr_abs_error.png}
    \caption{Absolute errors for $\gamma_{vgr}$ prediction on the initial validation set of 131 shots, showing relatively low absolute prediction error in the majority of shots.}
    \label{fig:vgr-abs-err}
\end{figure}
\newpage
\begin{figure}[!htbp]
    \centering
    \includegraphics[width=0.85\textwidth]{img/fine_tune_val.png}
    \caption{Model prediction accuracy as a function of prediction horizon during rampdowns on the fine-tuning set of 44 shots.}
    \label{fig:fine_tune_val}
\end{figure}
\newpage
\begin{figure}[!htbp]
    \centering
\includegraphics[width=0.75\linewidth]{img/ntm.png}
    \caption{$n=1$ root mean squared (RMS) signal derived from magnetic diagnostics indicating the presence of a neo-classical tearing mode at the beginning of ramp-down for \#82877 that is not present in \#82878.}
    \label{fig:ntm}
\end{figure}
\newpage
\end{document}