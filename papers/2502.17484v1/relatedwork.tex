\section{Background and Related Work}
\subsection{ML Approaches}

Having data points coming from sources of variable reliability has inspired \gls{FL}, a \gls{ML} approach allowing decentralised devices or institutions to collaboratively train a model without sharing their data. Developed primarily to address data privacy issues, this method involves participants training the model locally with their own data and sending only model updates (e.g., gradients or weights) to a central server \cite{kairouz2021_FL}. 
Hospital-based studies have shown that \gls{FL} models are effective for disease prediction, but they also revealed that while some predictive factors of disorders are shared, many are specific to individual institutions \cite{oh2018_two_hospitals}. \gls{FL} often encounters challenges with non-identically distributed data among clients, leading to difficulties in model convergence and consistency. Variations in clients' computational resources and network conditions can cause inefficiencies, and as more clients join, managing and coordinating them becomes increasingly complex, affecting scalability \cite{kairouz2021_FL}.

Consequently, to address the major challenge of heterogeneity in training data from different sources, \gls{PFL} is employed. According to \cite{tan2022_PFL}, there are two major \gls{PFL} strategies: Global Model Personalisation and Learning Personalised Models. Global Model Personalisation trains a global model using standard \gls{FL} and then personalises it for each client through additional local training. In contrast, Learning Personalised Models creates individual models for each client by modifying the \gls{FL} aggregation process, with methods like \gls{MTL}, which seeks to jointly learn multiple related tasks, allowing knowledge from one task to benefit others and improve overall generalisation performance. \gls{MTL} uses methods like sharing important features between tasks, grouping similar tasks together, finding relationships between tasks, and breaking down complex tasks into simpler parts to enhance performance \cite{zhang2021survey_multitask_learning}. Given that the study involves non-uniform data sources, it can be considered a ``feature homogeneous multi-task environment" \cite{alex_sources}. Therefore, this study employs \gls{MTL} methods, specifically focusing on sharing key features across tasks and grouping similar tasks.

In \gls{ML}, cross-entropy is a widely used loss criterion for classification tasks that quantifies the difference between true class labels and predicted probabilities, prompting the model to adjust its outputs toward more accurate predictions \cite{cross_entropy}. During training, the cross-entropy reduction loss curve often reveals an "elbow" point, where the rate of loss reduction slows significantly. The presence of an elbow suggests that the model is approaching convergence, indicating it has learned most patterns in the training data and is nearing optimal performance with the current data and hyperparameters. The elbow marks a point where additional training epochs yield only marginal improvements. Therefore, the losses at the elbow can indicate the difficulty of prediction.


\subsection{The Minder Dataset}

Building on the methodology from previous research on \gls{UTI} prediction using the Minder dataset, this study uses the same subdataset and feature engineering techniques \cite{capstick2024digital_UTI_prediction}. As a longitudinal study, we have access to further data, with the training set covering 28 June 2021, to 1 January 2023, and the testing set from 1 January 2023, to 1 January 2024. Initially, the training set included 69 patients, and the test set had 55 but 4 patients were removed from the test set due to incomplete demographic information, leaving 51 patients. There are 41 participants common to both the train and test sets. Therefore the 10 patients unique to the test set were either excluded or retained based on the method used.

In the original work, patient data was first sorted by date. Data gaps exceeding one day between records triggered segmentation into smaller continuous segments, with segments shorter than three days discarded. Within qualifying segments, \gls{UTI} labels were confirmed for one day and then extended to cover three days on either side of that day, effectively converting one verified label into seven days of \gls{UTI} labels. Hence, the final training dataset comprised 1,839 data points, while the test dataset contained 1,145 data points.

The 20 input features used in this study are adopted from previous research and are all symptom-specific \cite{capstick2024digital_UTI_prediction}, comprising both raw and engineered components. The raw features include: (1) activation frequency in the bathroom, bedroom, hallway, kitchen, and lounge; (2) mean and standard deviation of nocturnal heart and respiratory rates; and (3) nocturnal awake occurrences. The engineered features include: (4) frequency of bathroom use during day and night, with moving averages and percentage changes; (5) mean and standard deviation of time taken to move from any location to the bathroom; (6) daily entropy in \gls{PIR} sensor activations; and (7) the number of previous \gls{UTI}s recorded to date.