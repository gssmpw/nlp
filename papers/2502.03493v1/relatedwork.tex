\section{Related Works}
\subsection{Diffusion Model and Representation Learning}
Diffusion models are regarded as multi-level DAEs with varied noise scales, which inherently capture meaningful representations within a latent space\cite{dif_rep_survey,ldm,INDM,EBM_34,EBM_57,EBM_68}. Therefore, leveraging the features learned during the diffusion process to effectively train downstream tasks\cite{rep_1,rep_2,rep_3,rep_4,rep_5}, such as segmentation and classification, proves both meaningful and advantageous\cite{dif_rep_cla_1,dif_rep_cla_2,dif_rep_cla_2,dif_rep_seg_2,dif_rep_multi_task,dif_rep,dif_rep}. However, while recent studies (See more in Appendix \ref{rw1}) primarily focus on methods for effectively leveraging features from the diffusion denoising process, few delve deeply into what representation learning truly entails or why it is effective.\par   

\subsection{Modality Alignment}
Huh et al. \cite{platonic} hypothesize the modalities involved in training data are shadows on the \enquote{cave wall}, which is mentioned in Platoâ€™s Allegory of the Cave. Tian et al. \cite{real_world_1} try to align the different modalities within the contrastive loss, and believe that the more views of physical-world involved in training, the better representation captured. Zimmermann et al. \cite{real_world_2} investigate the connection between contrastive learning, generative modeling, and nonlinear independent component analysis to reveal the alignment of implicit features. Inspired by these findings (see more in Appendix \ref{model-alignment}), we aim to explore the existence of feature embeddings learned from modality alignment and underlying principles, which we refer to as the MetaFE in this study.\par 

\subsection{Endoscopic Monocular Depth Estimation}

In order to overcome the absence of depth annotation, Zhou et al. \cite{rw_dl_1} propose a self-supervised approach that reformulates depth estimation as a view synthesis problem using warping methods. This framework includes both a DepthNet and a separate PoseNet, along with a predictive mask to handle challenging scenarios like object movement and occlusion/disocclusion. This foundation has led to the development of a range of refined optimization strategies\cite{rw_dl_2,rw_dl_3,rw_dl_4,rw_dl_5,rw_dl_6,rw_dl_7}.  Considering the challenge posed by minimally invasive surgical settings, such as inconsistent interframe brightness, limit the direct applicability of these methods to endoscopic images. Shao et al. \cite{bright} propose AF-Net in order to rescue the illumination-invariant by introducing optical flow estimation module. Yang et al. \cite{rw_dl_8} introduce the LiteMono framework to enhance computational efficiency in endoscopic depth estimation. Shao et al. \cite{rw_dl_9} employ a diffusion model and knowledge distillation to produce higher-quality depth images, surpassing those generated by the teacher network. Nevertheless, the aforementioned studies primarily focus on network modifications or surface-level issues, lacking a thorough investigation into the core process of features decoding for depth estimation.\par 

Unlike previous studies that treat depth estimation as a mere modality transformation, we posit the existence of a space that represents physical entities in endoscopic surgery. By validating this space and extracting its intrinsic features, depth interpretation can be achieved with greater accuracy in endoscopic image depth estimation.\par