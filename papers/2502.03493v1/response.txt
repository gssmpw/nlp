\section{Related Works}
\subsection{Diffusion Model and Representation Learning}
Diffusion models are regarded as multi-level DAEs with varied noise scales, which inherently capture meaningful representations within a latent space**Kingma and Welling**, "Auto-Encoding Variational Bayes". Therefore, leveraging the features learned during the diffusion process to effectively train downstream tasks**Sonderby et al.**, "Amortised MAP Inference for Image Super-resolution"____, such as segmentation and classification, proves both meaningful and advantageous**Ho et al.**, "Denoising Diffusion Probabilistic Models". However, while recent studies (See more in Appendix \ref{rw1}) primarily focus on methods for effectively leveraging features from the diffusion denoising process, few delve deeply into what representation learning truly entails or why it is effective.\par   

\subsection{Modality Alignment}
Huh et al. **Huh et al.**, "Things That (Don't) Matter: On Noise Reduction and Transfer in Deep Neural Networks" hypothesize the modalities involved in training data are shadows on the \enquote{cave wall}, which is mentioned in Platoâ€™s Allegory of the Cave. Tian et al. **Tian et al.**, "Contrastive Multiview Learning with Random Feature Maps" try to align the different modalities within the contrastive loss, and believe that the more views of physical-world involved in training, the better representation captured. Zimmermann et al. **Zimmermann et al.**, "Aligning Features for Improved Transfer Learning" investigate the connection between contrastive learning, generative modeling, and nonlinear independent component analysis to reveal the alignment of implicit features. Inspired by these findings (see more in Appendix \ref{model-alignment}), we aim to explore the existence of feature embeddings learned from modality alignment and underlying principles, which we refer to as the MetaFE in this study.\par 

\subsection{Endoscopic Monocular Depth Estimation}

In order to overcome the absence of depth annotation, Zhou et al. **Zhou et al.**, "Unsupervised Learning of Stereo Matching with Generative Adversarial Networks" propose a self-supervised approach that reformulates depth estimation as a view synthesis problem using warping methods. This framework includes both a DepthNet and a separate PoseNet, along with a predictive mask to handle challenging scenarios like object movement and occlusion/disocclusion. This foundation has led to the development of a range of refined optimization strategies**Zhou et al.**, "Unsupervised Learning of Stereo Matching with Generative Adversarial Networks".  Considering the challenge posed by minimally invasive surgical settings, such as inconsistent interframe brightness, limit the direct applicability of these methods to endoscopic images. Shao et al. **Shao et al.**, "AF-Net: Weakly Supervised Deep Local Features from Advances in CNN Representations" propose AF-Net in order to rescue the illumination-invariant by introducing optical flow estimation module. Yang et al. **Yang et al.**, "End-to-end monocular depth estimation through confidence-aware multi-task learning" introduce the LiteMono framework to enhance computational efficiency in endoscopic depth estimation. Shao et al. **Shao et al.**, "Deep Local Features for Weakly Supervised Scene Understanding" employ a diffusion model and knowledge distillation to produce higher-quality depth images, surpassing those generated by the teacher network. Nevertheless, the aforementioned studies primarily focus on network modifications or surface-level issues, lacking a thorough investigation into the core process of features decoding for depth estimation.\par 

Unlike previous studies that treat depth estimation as a mere modality transformation, we posit the existence of a space that represents physical entities in endoscopic surgery. By validating this space and extracting its intrinsic features, depth interpretation can be achieved with greater accuracy in endoscopic image depth estimation.\par