\section{Related Works}
\subsection{Safety Concerns of MLLMs}
LLMs have been revealed to pose significant risks in responding to malicious instructions \cite{zou2023universal, liu2023autodan, chao2023jailbreaking}. 
Since MLLMs are typically developed using LLMs as their backbone networks, the risks inherent in the LLM domain are directly transferred to MLLMs. 
More concerning, recent studies have revealed that non-text modal inputs to MLLMs present even more significant security threats. 
For example, leveraging the model's OCR capabilities in combination with malicious images \cite{gong2023figstep,luo2024jailbreakv} can significantly increase the response rate of malicious instructions. 
Furthermore, some work has used gradient-based searches to generate adversarial perturbations at the image level \cite{li2024images, qi2024visual, niu2024jailbreaking}, further exacerbating security risks.
Therefore, additional safety alignment for MLLMs is necessary to mitigate potential societal harm.

\subsection{Safety Alignment for MLLMs}
Safety alignment aims to align the safety awareness of the model with that of humans to prevent the generation of harmful content. 
This has been thoroughly researched in the field of LLMs, with widely used methods including SFT, Direct Preference Optimization (DPO) \cite{rafailov2024direct}, and Proximal Policy Optimization (PPO) \cite{schulman2017proximal}. 
Inspired by these works, researchers have created carefully crafted image-text pairs to align training in MLLMs, yielding promising results in improving model safety. 
However, producing high-quality multimodal alignment data is often costly.
% , which can dampen MLLM developers' enthusiasm for conducting thorough safety training. 
To achieve low-resource safety alignment, \citet{chakraborty2024cross} have revealed that textual unlearning can effectively enhance model safety. 
However, it has been noted that this is ineffective against attacks introduced solely from images. \cite{hu2024vlsbench}. 
Furthermore, most existing works have focused solely on image-based MLLMs, leaving the effectiveness of other modalities to be explored further. 
% In contrast, this paper aims to develop a resource-efficient safety alignment method that is effective in non-VSIL scenarios and universally applicable to various modalities of MLLMs.

\subsection{Safety Benchmark of MLLMs}
% Several safety benchmarks have been proposed, given the importance of quantifying the safety risks of MLLMs to promote the development of safer models.
% However, 
Most of the existing safety benchmarks focus on image-based MLLMs, including MM-SafetyBench \cite{liu2024mm}, Ch3ef \cite{shi2024assessment}, VLSafe \cite{chen2024dress}, Figstep \cite{gong2023figstep}, MLLMGuard \cite{gu2024mllmguard}, and Jailbreakv-28k \cite{luo2024jailbreakv}.
Furthermore, \citet{yang2024audio} utilized text-to-speech models to reveal harmful information in the audio modality, while SafeBench \cite{ying2024safebench} provides a unified benchmark that can test the safety of both image and audio modalities. 
Currently, there are no published safety assessment benchmarks for MLLMs in other modalities.