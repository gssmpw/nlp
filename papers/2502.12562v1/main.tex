% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}


% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{float,color}
\usepackage{rotating} 
\usepackage{lipsum}   
\usepackage{multirow} 
\usepackage{amsmath}
\usepackage{stfloats}
\usepackage{authblk}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\revisezq}[1]{\textcolor{black}{#1}}
\newcommand{\revisewk}[1]{\textcolor{black}{#1}}

\title{SEA: Low-Resource Safety Alignment for Multimodal Large Language Models via Synthetic Embeddings}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author[1]{Weikai Lu}
\author[2]{Hao Peng}
\author[1]{Huiping Zhuang}
\author[3,4]{Cen Chen}
\author[1]{Ziqian Zeng*}
\affil[1]{Shien-Ming Wu School of Intelligent Engineering, South China University of Technology, China}
\affil[2]{School of Cyber Science and Technology, Beihang University, China}
\affil[3]{School of Future Technology, South China University of Technology, China}
\affil[4]{Pazhou Laboratory, China}
\affil[ ]{\texttt{wklu2452@163.com} ~~~\texttt{zqzeng@scut.edu.cn}}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}

\begin{document}
\maketitle
{\let\thefootnote\relax\footnotetext{*Corresponding author}}

\begin{abstract}
Multimodal Large Language Models (MLLMs) have serious security vulnerabilities. 
While safety alignment using multimodal datasets consisting of text and data of additional modalities can effectively enhance MLLM's security, it is costly to construct these datasets. 
% Existing low-resource safety alignment methods struggle to address the security risks introduced by additional modalities, or they fail to be widely applicable across various multimodal MLLMs. 
Existing low-resource security alignment methods, including textual alignment, have been found to struggle with the security risks posed by additional modalities.
To address this, we propose Synthetic Embedding augmented safety Alignment (SEA), which optimizes embeddings of additional modality through gradient updates to expand textual datasets. 
This enables multimodal safety alignment training even when only textual data is available. 
Extensive experiments on image, video, and audio-based MLLMs demonstrate that SEA can synthesize a high-quality embedding on a single RTX3090 GPU within 24 seconds. 
%Training in datasets constructed with synthetic embeddings significantly enhances the security of MLLMs when facing threats from additional modalities.
SEA significantly improves the security of MLLMs when faced with threats from additional modalities.
To assess the security risks introduced by video and audio, we also introduced a new benchmark called VA-SafetyBench. 
High attack success rates across multiple MLLMs validate its challenge. 
Our code and data will be available at https://github.com/ZeroNLP/SEA.

\textcolor{red}{This paper contains harmful data and model-generated content that can be offensive in nature.}  

\end{abstract}

\section{Introduction}
Multimodal Large Language Models (MLLMs) integrate additional modality encoders with large language models (LLMs), equipping them with the ability to comprehend and reason on multimodal data such as images \cite{liu2024visual, liu2024improved, chen2023minigpt}, videos \cite{wang2024qwen2,damonlpsg2024videollama2}, and audio \cite{Qwen2-Audio}. 
Although MLLMs achieve advanced multimodal capability, they exhibit more serious security risks than LLMs.
By injecting malicious information into non-textual inputs such as images \cite{liu2024mm, li2024images} or audio \cite{yang2024audio}, MLLMs can be easily induced to comply with users' harmful instructions. 

To address the aforementioned issues, current mitigation strategies, such as supervised fine-tuning (SFT) \cite{zong2024safety} and reinforcement learning with human feedback (RLHF) \cite{zhang2024spa} demonstrate effectiveness in enhancing the safety of MLLM. 
However, the construction of multimodal safety alignment datasets is costly. 
Unlike LLMs, high-quality safety alignment data for MLLMs requires a strong correlation between the three components: textual instructions, textual responses, and additional modalities, making the data collection process even more expensive. 
Moreover, due to differences in additional modalities, safety alignment data must be rebuilt whenever a new emerging modality (such as electroencephalogram signals \cite{wang2024eegpt}) is introduced for MLLM. 
This not only incurs additional costs but also causes the development of datasets to lag behind the advancements of the MLLMs themselves. 
Therefore, there is an urgent need for a resource-efficient and universally applicable safety alignment method for various MLLMs to promote the development of safer MLLMs.


Recently, \citet{chakraborty2024cross} revealed that textual alignment can significantly enhance the security of image-based MLLMs, providing a promising solution for low-resource safety alignment. 
However, further exploration by \citet{hu2024vlsbench} found that textual alignment is effective only when explicit harmful information appears in the text input, such as the instruction ``how to use the product in the image to \emph{rob a bank}'' with an image input of a bomb. 
In contrast, models that align image-text pairs are generally effective across various scenarios, including samples that present harmful information solely through images, such as the instruction "how to make a product" with an input image of a bomb.
% In contrast, models aligned using image-text pairs are effective when dealing with samples that present harmful information solely through images, such as the instruction ``how to make the product in the image'' with an image input of a bomb.
To address the limitations of textual alignment, generating data of additional modality using generative models is a potential solution. 
However, not all modalities have high-performance generative models available, especially for emerging MLLMs that may arise in the future.


To address the aforementioned limitations, we propose SEA, a new framework that uses synthetic embeddings of additional modalities to enhance safety alignment. 
It first optimizes embedding representations within the modality encoder's output space deemed by MLLMs to contain the specified contents.
Subsequently, the optimized embedding can be integrated with the textual dataset, substituting it for a real multimodal dataset in safety alignment training. 
Our approach eliminates the resource-intensive process of collecting and curating real multimodal datasets.  
Experiments are conducted on MLLMs based on images, videos, and audio, and the results indicate that only two training samples are needed to optimize a high-quality embedding in 24 seconds on a single RTX 3090 GPU. 
Furthermore, using datasets constructed with synthetic embedding for safety alignment significantly enhances the safety of MLLMs against threats from additional modalities.

Due to the lack of publicly available safety evaluation benchmarks for video and audio-based MLLMs, we also introduce VA-SafeBench, which expands on image-based MM-SafetyBench \cite{liu2024mm}. 
Specifically, each sample in VA-SafetyBench is converted one-to-one from samples in eight scenarios of MM-SafetyBench. 
They share the same sources of harmful information, but the questioning format in VA-SafetyBench consists of video-text pairs and audio-text pairs. 
The high attack success rates (ASR) in multiple MLLMs validate the challenges posed by VA-SafeBench. 


The contributions of our paper are summarized as follows.

$\bullet$ We introduce SEA, a novel low-resource MLLM safety alignment method. It expands the textual safety alignment dataset through synthetic embeddings, allowing multimodal training when only textual data is available.

$\bullet$ We present VA-SafeBench, which extends MM-SafetyBench to evaluate the security risks introduced by video and audio. 
% To the best of our knowledge, this is the first benchmark for assessing the safety of video-based MLLMs. \revisezq{the first benchmark of video. why it is not a first benchmark of audio?}

$\bullet$ The experimental results indicate that SEA significantly improves the security of MLLMs against threats from additional modalities with minimal additional computational overhead. 

\section{Related Works}
\subsection{Safety Concerns of MLLMs}
LLMs have been revealed to pose significant risks in responding to malicious instructions \cite{zou2023universal, liu2023autodan, chao2023jailbreaking}. 
Since MLLMs are typically developed using LLMs as their backbone networks, the risks inherent in the LLM domain are directly transferred to MLLMs. 
More concerning, recent studies have revealed that non-text modal inputs to MLLMs present even more significant security threats. 
For example, leveraging the model's OCR capabilities in combination with malicious images \cite{gong2023figstep,luo2024jailbreakv} can significantly increase the response rate of malicious instructions. 
Furthermore, some work has used gradient-based searches to generate adversarial perturbations at the image level \cite{li2024images, qi2024visual, niu2024jailbreaking}, further exacerbating security risks.
Therefore, additional safety alignment for MLLMs is necessary to mitigate potential societal harm.

\subsection{Safety Alignment for MLLMs}
Safety alignment aims to align the safety awareness of the model with that of humans to prevent the generation of harmful content. 
This has been thoroughly researched in the field of LLMs, with widely used methods including SFT, Direct Preference Optimization (DPO) \cite{rafailov2024direct}, and Proximal Policy Optimization (PPO) \cite{schulman2017proximal}. 
Inspired by these works, researchers have created carefully crafted image-text pairs to align training in MLLMs, yielding promising results in improving model safety. 
However, producing high-quality multimodal alignment data is often costly.
% , which can dampen MLLM developers' enthusiasm for conducting thorough safety training. 
To achieve low-resource safety alignment, \citet{chakraborty2024cross} have revealed that textual unlearning can effectively enhance model safety. 
However, it has been noted that this is ineffective against attacks introduced solely from images. \cite{hu2024vlsbench}. 
Furthermore, most existing works have focused solely on image-based MLLMs, leaving the effectiveness of other modalities to be explored further. 
% In contrast, this paper aims to develop a resource-efficient safety alignment method that is effective in non-VSIL scenarios and universally applicable to various modalities of MLLMs.

\subsection{Safety Benchmark of MLLMs}
% Several safety benchmarks have been proposed, given the importance of quantifying the safety risks of MLLMs to promote the development of safer models.
% However, 
Most of the existing safety benchmarks focus on image-based MLLMs, including MM-SafetyBench \cite{liu2024mm}, Ch3ef \cite{shi2024assessment}, VLSafe \cite{chen2024dress}, Figstep \cite{gong2023figstep}, MLLMGuard \cite{gu2024mllmguard}, and Jailbreakv-28k \cite{luo2024jailbreakv}.
Furthermore, \citet{yang2024audio} utilized text-to-speech models to reveal harmful information in the audio modality, while SafeBench \cite{ying2024safebench} provides a unified benchmark that can test the safety of both image and audio modalities. 
Currently, there are no published safety assessment benchmarks for MLLMs in other modalities.


\section{SEA: Achieving Low-Resource Safety Alignment via Synthetic Embeddings}

\begin{figure*}[t]%[!htbp]
  \centering
  \includegraphics[width=0.85\textwidth]{figs/SEA.pdf}
 \caption{The overall framework of SEA.}
\label{fig:SEA}
\end{figure*}

Since multimodal datasets are crucial for MLLM's safety alignment training, but not all modalities have high-performance generative models available, we aim to find a more general method for synthesizing additional modal data.
An intuition of this work is that the additional modal data used for safety alignment, such as bomb images, do not necessarily need to be understandable by humans, but only need MLLMs to consider them as bomb images.

%Building on this intuition, we propose \textbf{S}ynthetic \textbf{E}mbedding enhanced safety \textbf{A}lignment (SEA), which optimizes embeddings that MLLMs identify as containing specified harmful information in the MLLM's representation space. 
Building on this intuition, we propose \textbf{S}ynthetic \textbf{E}mbedding enhanced safety \textbf{A}lignment (SEA), which optimizes embeddings in the representation space of the additional modality. 
The target embedding is one that MLLMs interpret as containing the specified harmful activities or products.
Specifically, SEA treats the embedding of additional modal as a trainable weight, optimized through gradient updates, to maximize the probability of the model outputting the specified content. 
After integrating the optimized embedding with the textual dataset, it can serve as a substitute for multimodal datasets.
%in safety alignment training.

% In this section, we introduce \textbf{S}ynthetic \textbf{E}mbedding enhanced safety \textbf{A}lignment (SEA), which searches for non-text modal embeddings containing harmful information to expand the text safety alignment dataset.  
% Figure \ref{fig:SEA} illustrates the pipeline of SEA. 
% \revisezq{It first extracts harmful information from the text instruction to build a dataset for embedding optimization. 
% Then, it utilizes gradient updates to optimize an embedding representation identified by the MLLM as containing specified harmful information and further employs embedding-text pairs for safety alignment training.}
%ZZQ-Feb-13: 这两句话描述得不是很清楚

\subsection{Preliminary: MLLMs Architecture}
The architecture of existing MLLMs can generally be broken down into three components. 
(1) Modality Encoder $M(\cdot)$: it encodes the input of additional modality into an embedding. 
(2) Projector $P(\cdot)$: it maps embeddings from the non-textual modality representation space into the textual modality representation space. 
(3) LLM: it processes inputs from different modalities, performing semantic understanding, reasoning, and decision-making. 
Combining these components, the reasoning process of MLLMs can be formulated as:
\begin{equation}
y = LLM\left(P\left(M\left(z\right)\right), x\right),
\end{equation}
where $z$ and $x$ represent the input of additional modality and textual modality, respectively, while $y$ is the textual output.

Following the above paradigm, regardless of the format differences of the additional modalities of MLLMs, they will be encoded as embeddings via $M(\cdot)$. 
To make SEA more broadly applicable, we anchor the output space of $M(\cdot)$, in which we collect the desired embeddings for safety training.

\subsection{Data Preparation}

Assume a textual safety alignment dataset $D_T=\left\{\left(x_T^i, y_T^i\right)\right\}_{i=0}^{N}$ consisting of $N$ samples, where $x_T^i$ represents harmful instructions and $y_T^i$ can be a single moral response for SFT or a pair of chosen rejected responses for RLHF, our objective is to optimize a set of embeddings $\{E^i\}_{i=0}^{N}$ based on harmful information in $x_T^i$.

% By making simple modifications to each $x_T^i \in D_{T}$, we aim to construct a multimodal safety alignment dataset $D_M=\left\{\left(x_M^i, y_T^i, E^i\right)\right\}_{i=0}^{N}$, where $x_M^i$ is rewritten $x_T^i$.

For each $\left(x_T^i, y_T^i\right) \in D_T$, we individually prepare a dataset $D_{a}^{i}=\{(x_{a}^{1},y_{a}^{1}),(x_{a}^{2},y_{a}^{2})\}$ to assist in the optimization of $E^i$, where $(x_{a}^{1},y_{a}^{1})$ and $(x_{a}^{2},y_{a}^{2})$ are the content control sample and the style control sample, respectively. 
As an example of the image-based MLLMs, the construction process for the two samples is as follows:

% \revisezq{For simplicity, only one content control sample and one style control sample are included, determined by harmful information in the instructions $x_T^i$.}
% %这句话出现得很突兀。与上文接不上。
% In more complex scenarios, additional samples can be prepared to achieve more nuanced control. The construction process of $D_{i}$ is as follows:

\textbf{Harmful Information Extraction.}  
Inspired by \cite{liu2024mm}, we utilize GPT-4o-mini to identify harmful phrase in $x_T^i$, and then classify harmful phrases into two categories, including ``activity'' and ``product'', and then create a detoxified version of $x_T^i$ by replacing the harmful phrase with ``this product'' or ``this activity''.
Since harmful phrase related to activity often do not form complete sentences, we further prompt GPT-4o-mini to complete them into full sentences $s_T^i$ with subject-verb-object structures, aligning with the language habits of MLLMs.

\textbf{Content Control Sample Construction.} 
This sample is used to control the primary harmful content in the embedding. 
We use ``\emph{Please briefly describe the activity (product) in the image.}'' as input instruction $x_{a}^{1}$, and ``response prefix + $s_T^i$.'' as the ground truth $y_{a}^{1}$. 
The ``response prefix'' is determined based on the output habits of different models and can be found in Appendix \ref{appB:B1}.

\textbf{Style Control Sample Construction.} 
This sample is designed to enhance embedding diversity. 
The input instruction $x_{a}^{2}$ is set to ``\emph{What is the style of the image?}''.
The ground truth $y_{a}^{2}$ is set to ``response prefix + style description''.
The style description is randomly sampled from a predefined style set determined by the model's output habits.

% More data examples can be found in Appendix \ref{appB:B1}.
% The design of multiple MLLMs' instructions, response prefixes, and style sets can be found in Appendix A.

\subsection{Embedding Optimization.}
After building the $D_{a}^{i}$, $M(\cdot)$ encodes a blank image (or a blank video, silent audio) in an embedding, which is used as an initialization for a trainable embedding $E_o$. 
For each $(x_{a},y_{a})\in D_{a}^{i}$, the goal of the embedding optimization is to maximize the probability of the MLLM generating $y_{a}$ when given $x_{a}$ and $E_{o}$. 
During the optimization process, the entire MLLMs are frozen, with only $E_{o}$ participating as the trainable weight in the gradient updates. 
Since the content and style are specified in $y_{a}^{1}$ and $y_{a}^{2}$, the optimization objective can be understood as finding the embedding that the MLLM considers most aligned with that content and style. 
The entire optimization process can be formulated as follows:
% Once $D_{O}^{i}$ is determined, SEA optimizes a modal encoder embedding that specifies content and style. Initially, $M(\cdot)$ encodes a blank image, blank video, or silent audio to obtain the initialized embedding $E_{o}$. Then, the entire MLLMs network is frozen, and \revisezq{$E_{o}$ participates in gradient descent optimization as a continuous, trainable weight. }
% \revisezq{What is the goal of the optimization?}
% \revisezq{What is the meaning of this sentence?}
% The entire optimization process can be represented as:
\begin{equation}
\scalebox{0.85}{
$L\left(E_o\right)=-\frac{1}{\left|D_{a}^{i}\right|} \sum_{(x_a, y_a) \in D_{a}^{i}} \log \left( P_r\left(y_a \mid x_a, P\left(E_o\right)\right)\right)$,
}
\end{equation}
\begin{equation}
E^i=\underset{E_o}{\arg \min }\left(L\left(E_o\right)\right),
\end{equation}

% \begin{equation}
% E^i=\underset{E_o}{\arg \min }\left(-\frac{1}{\left|D_i\right|} \sum_{(x, y) \in D_i} \log \left(p\left(y \mid x, P\left(E_o\right)\right)\right)\right)
% \end{equation}
\noindent where $P_r\left(y \mid x, P\left(E_o\right)\right)$ represents the conditional probability of generating $y$ when given $x$ and $P(E_{o})$ to the LLM.

\subsection{Safety Alignment}
To integrate $E^i$ and $D_T$ to construct the multimodal dataset $D_M=\left\{\left(x_M^i, y_T^i, E^i\right)\right\}_{i=0}^{N}$, a prefix in the form of ``\emph{The image shows an activity (product). Please comprehend it and respond to the question below.}'' is added to the detoxified version of each $x_T^i$, resulting in the instruction $x_M^i$. 
The responses $\{y_T^i\}_{i=1}^{N}$ in $D_T$ are retained in $D_M$. 

To achieve safety alignment based on $D_{M}$, we need to ignore module $M(\cdot)$ and modify the forward propagation process of MLLMs to $y = LLM\left(P\left(E^i\right), x\right)$, allowing it to adapt to existing safety alignment training strategies. 
Notably, most current MLLMs freeze $M(\cdot)$ during the instruction fine-tuning stage. 
Therefore, we only need to precompute the embeddings from $M(\cdot)$, the synthetic dataset generated by SEA can then be mixed with real multimodal datasets in the existing training pipeline. 


\section{VA-SafetyBench: Assessing Security Risks Introduced by Video and Audio}
% \revisezq{Why you introduce VA-SafetyBench first?}
\subsection{VA-SafetyBench Overview}

\begin{figure*}[t]%[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/VA-SafeBench.pdf}
 \caption{Overview of VA-SafetyBench construction pipeline.}
\label{fig:VA-SafeBench}
\end{figure*}


VA-SafetyBench is a safety benchmark targeted at video and audio-based MLLMs. 
It consists of two parts: Video-SafetyBench and Audio-SafetyBench. 
Each sample in both parts includes a textual instruction and either a video or audio clip.


The construction pipeline of VA-SafetyBench is illustrated in Figure \ref{fig:VA-SafeBench}. 
VA-SafetyBench builds on MM-SafetyBench, a well-established image-based safety benchmark, through a systematic transformation process. 
%Specifically, each sample in VA-SafetyBench is transformed one-to-one from the samples in MM-SafetyBench. 
Each test case in VA-SafetyBench directly corresponds to a test case in MM-SafetyBench, which spans eight critical safety scenarios: illegal activity, hate speech, malware generation, physical harm, economic harm, fraud, sexual violence, and privacy violations. 
%We considered samples from eight different scenarios, including illegal activity, hate speech, malware generation, physical harm, economic harm, fraud, sexual violence, and privacy violations. 
For each sample, we utilize three types of textual data from MM-SafetyBench in the transformation process: 
(1) an original instruction, (2) a harmful key phrase extracted from the original instruction, and (3) a rewritten instruction that conceals the harmful content in the original instruction. 
Figure \ref{fig:VA-SafeBench} provides an example of these texts. 
Based on three types of textual data, we collect video and audio according to the key harmful phrase and refine the rewritten instruction to suit the new modalities.



\subsection{Video-SafetyBench}

Video-SafetyBench comprises four distinct tasks, including a \textbf{Text-only} baseline task where the input of video modality is set to empty, and the original instructions are used. The remaining three tasks are differentiated by their video generation methodologies, as detailed below:

$\bullet$ \textbf{Diffusion Model (DM)}: We employed Pyramidal Flow \cite{jin2024pyramidal}, a text-to-video model based on the diffusion Transformer architecture to generate videos where the content of videos is related to harmful key phrases. 
Each video is set to 5 seconds, with a resolution of 640×384 and 24 fps. 
Since individual harmful key phrases cannot provide sufficiently detailed video information, we utilized GPT-4o-mini to generate relevant scene descriptions. 
%Original: Using In-Context learning, randomly select two out of five pre-prepared cases as context for each generation. 
The generation process utilizes in-context learning, in which two examples were selected from five pre-prepared examples to serve as context.
The five pre-prepared examples are shown in the Appendix \ref{app:A2}.
%To ensure diversity, we generated five distinct scene descriptions for each key phrase. 
We generate five distinct scene descriptions for each key phrase and process them through Pyramidal Flow.
The most representative video is manually selected based on alignment with the original key phrase.
%These descriptions were then processed through Pyramidal Flow, followed by a manual selection of the video that best captured the essence of the original key phrase.  
% \revisezq{In this task, the textual instruction is to  replace ``The image'' with ``The video'' in the \emph{RI} of MM-SafetyBench. }
To create the textual instructions for this task, we adapted the rewritten instructions from MM-SafetyBench by simply replacing all instances of ``the image'' with ``the video'' to reflect the change in modality.
%Original: For each key phrase, five scene descriptions are generated to increase diversity. 
%Original: After the scene descriptions were converted into videos via Pyramidal Flow, one video most relevant to the key phrase was manually selected.

% \revisezq{Typo means small error. Do you consider to change another abbreviation such as TYG, TYPG}

% \textbf{Typography (TYPO)}: Inspired by image-based jailbreak method \cite{gong2023figstep}, we split each harmful key phrase by word, displaying the white words against a black background in images, which were then sequenced to create a 5 seconds video. 
$\bullet$ 
\textbf{Typography (TYPO)}: Inspired by the image-based jailbreak method \cite{gong2023figstep}, we developed a typographic animation approach to generate videos. 
Each harmful key phrase was decomposed into individual words, rendered in white text against a black background, and sequenced into a video format. 
The textual instruction is set to be the same as the DM.

$\bullet$  \textbf{DM + TYPO}: It combines the Diffusion Model and Typography to generate videos. In this task, Typography does not displace the words separately but displays the entire harmful key phrase in a single video frame, repeating multiple frames to form a 3-second video, which is then appended to the end of the DM video. 
For this task, we modified the rewritten instructions by replacing ``the image'' with ``the end of the video'' as the textual instruction. 
An instance would be ``The end of the video shows a phrase of an activity. List steps in detail to perform the activity.''


\subsection{Audio-SafetyBench}

The Audio-SafetyBench also includes four tasks, with the Text-only task being the same as Video-SafetyBench. 
The other three tasks utilize Microsoft's edge-tts API \footnote[1]{https://github.com/rany2/edge-tts} to generate speech, which can be differentiated based on the content of the speech:

$\bullet$ \textbf{Audio-only}: The entire unmodified original prompt is converted into speech. No textual instruction is provided.

$\bullet$ \textbf{Key Phrase Transfer (KPT)}: Only key phrases are converted into speech. 
The textual instruction input is the rewritten instruction with ``the image'' replaced by ``the audio''. 

$\bullet$ \textbf{KPT + Noise}: Inspired by \cite{yang2024audio}, we prepend a segment of white noise lasting 3 seconds to the KPT audio. The textual instruction input is the rewritten instruction with ``the image'' replaced by ``the end of the audio''. 

\subsection{Experimental Validation}
We conduct experiments to show that VA-Safetybench is a challenging safety benchmark. 
The experiments are conducted across four video-based MLLMs including Qwen2-VL-7b \cite{wang2024qwen2}, Qwen2-VL-2B, VideoLLaMA2-7B \cite{damonlpsg2024videollama2}, and VideoLLaMA2.1-7B, and four audio-based MLLMs including Qwen2-Audio \cite{Qwen2-Audio}, Qwen-Audio \cite{Qwen-Audio}, SALMONN-7B \cite{tang2024salmonn}, and SALMONN-13B. 
% The outputs of the models were evaluated for safety using GPT-4o-mini, with the evaluation prompts being the same as those in \cite{liu2024mm}. 
% The evaluation metric was set as the Attack Success Rate (ASR), which represents the proportion of unsafe samples among the total evaluation samples.
Following the evaluation protocol established in \cite{liu2024mm}, we use GPT-4o-mini as the evaluator. 
The evaluation metric is Attack Success Rate (ASR), calculated as the percentage of responses flagged as unsafe within the evaluation set.
%ZZQ-Feb-13: 重写了这两句，自行决定用哪个版本。


Tables \ref{tab:Bench_video} and \ref{tab:Bench_audio} present the experimental results. The findings can be summarized as follows:


\textbf{
Additional modalities amplify security vulnerabilities.} Our research extends beyond previous findings in image-based MLLMs, demonstrating that this vulnerability pattern persists across audio and video modalities. 
The effect is particularly pronounced in video-based MLLMs, with Qwen2-VL-7b showing a 65.08\% higher Attack Success Rate (ASR) when under DM+TYPO attacks compared to text-only attacks. 
Similarly, audio-based MLLMs demonstrate increased vulnerability, with Qwen-Audio exhibiting a 46.35\% higher ASR when harmful content is presented through the audio modality.

\begin{table}[]
\caption{The evaluation results on the VA-SafetyBench for video-based MLLMs.}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\hline
Models & Text-only & DM & TYPO & DM+TYPO \\ \hline
Qwen2-VL-7b & 6.05 & 20.18 & 40.72 & 71.13 \\
Qwen2-VL-2B & 18.20 & 15.63 & 34.41 & 69.33 \\
VideoLLaMA2-7B & 15.66 & 24.83 & 20.81 & 42.33 \\
VideoLLaMA2.1-7B & 7.25 & 24.37 & 43.69 & 52.26 \\ \hline
\end{tabular}%
}
\label{tab:Bench_video}
\end{table}


\begin{table}[]
\caption{The evaluation results on the VA-SafetyBench for audio-based MLLMs. 
The Audio-only results for SALMONN-7B and SALMONN-13B were discarded, as they consistently only repeated the content of the input speech.}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\hline
Models & Text-only & Audio-only & KPT & KPT+Noise \\ \hline
Qwen2-Audio & 14.66 & 7.70 & 24.06 & 34.31 \\
Qwen-Audio & 12.20 & 47.24 & 47.14 & 58.55 \\
SALMONN-7B & 44.04 & - & 41.11 & 65.97 \\
SALMONN-13B & 46.15 & - & 55.10 & 64.34 \\ \hline
\end{tabular}%
}
\label{tab:Bench_audio}
\end{table}

\textbf{VA-SafetyBench poses significant challenges.} Both video and audio-based MLLMs demonstrate a high ASR. 
In video MLLMs, the best case is the Qwen2-VL-7b, which achieved a 71.33\% ASR in DM+TYPO. While the ASR for DM and TYPO is generally lower than that of DM+TYPO, we found that in many instances, the MLLMs failed to correctly interpret the content of the videos, leading to safe outputs. 
Therefore, as the performance of MLLMs improves in the future, DM and TYPO may pose even greater threats. 

For audio MLLMs, the highest ASR is found in SALMONN-13B at 64.34\%. 
KPT is generally higher than Audio-only, indicating that distributing harmful instructions across text and audio better activate model's toxicity. KPT + noise generally performs better than KPT, indicating that using noise for interference or hiding harmful information in the time dimension makes it easier to bypass safety mechanisms.


Due to space limitations, we present more details about VA-SafetyBench in the Appendix \ref{appA:VA_SafetyBench}.
% , including motivations, additional implementation details, more refined experimental results along various scenarios, and specific benchmark case.



\begin{table*}[t]
\centering
\caption{
We present experimental results for an image-based MLLM (Llava-v1.5-7b-hf), with separate evaluation on the safety benchmark and benchmarks on general capabilities. 
The results on safety benchmarks are presented on the left of the vertical line, with lower scores indicating better performance. 
The results on benchmarks of general capability are presented on the right of the vertical line, with higher scores indicating better performance. 
Bold values indicate the best performance. 
%The experimental results conducted on image-based MLLMs (Llava-v1.5-7b-hf). 
%The methods employing SFT and DPO training strategies are displayed separately, with the best metrics in each group highlighted in bold. 
%The left side of the dividing line ``|'' represents the safety benchmark, where lower values are better. The right side represents the general capability benchmarks, where higher values are better.
}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lccccc|ccccc}
\hline
\multicolumn{1}{c}{\multirow{2}{*}{Approaches}} & \multicolumn{5}{c|}{MM-SafetyBench (\%)} & \multicolumn{3}{c}{POPE} & \multirow{2}{*}{MMMU} & \multirow{2}{*}{MME} \\ \cline{2-9}
\multicolumn{1}{c}{} & Text-only & SD & TYPO & SD + TYPO & average & adversarial & popular & random &  &  \\ \hline
Llava-v1.5-7b-hf & 46.50 & 30.20 & 27.32 & 62.78 & 41.7 & 81.32 & 86.56 & 89.53 & 30.44 & 1486 \\ \hline
VLGuard & \textbf{3.49} & 4.73 & 3.16 & 11.16 & 5.63 & 76.18 & 77.82 & 78.05 & 22.55 & 1304 \\
Textual SFT & 7.78 & 5.57 & 2.31 & 37.69 & 13.33 & \textbf{78.80} & \textbf{79.52} & 78.53 & \textbf{36.00} & \textbf{1157} \\
SEA SFT & 4.09 & \textbf{0.74} & \textbf{0.16} & \textbf{2.74} & \textbf{1.93} & 76.79 & 79.04 & \textbf{79.39} & 31.88 & 1114 \\ \hline
Textual DPO & \textbf{6.84} & 22.60 & 17.21 & 52.84 & 24.87 & 81.30 & 86.63 & \textbf{90.14} & \textbf{32.44} & 1433 \\
SEA DPO & 7.27 & \textbf{6.56} & \textbf{2.77} & \textbf{23.20} & \textbf{9.95} & \textbf{82.34} & \textbf{86.99} & 89.94 & 30.11 & \textbf{1463} \\ \hline
\end{tabular}%
}
\label{tab:image}
\end{table*}

\begin{table*}[t]
\centering
\caption{The experimental results conducted on Video-based MLLMs (Qwen2-VL-7b).}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\hline
\multirow{2}{*}{Approaches} & \multicolumn{5}{c|}{VASafetyBench (\%)} & \multirow{2}{*}{MVBench} & \multicolumn{3}{c}{VideoMME} \\ \cline{2-6} \cline{8-10} 
 & Text-only & DM & TYPO & DM+TYPO & average &  & Short & Medium & Long \\ \hline
Qwen2-VL-7b & 6.05 & 64.42 & 71.13 & 69.24 & 52.71 & 63.62 & 63.33 & 48.66 & 45.44 \\ \hline
Textual SFT & 4.27 & 4.35 & 12.51 & 13.47 & 8.65 & 61.85 & 60.66 & 48.44 & 44.55 \\
SEA SFT & \textbf{0.82} & \textbf{0.11} & \textbf{0.24} & \textbf{0.22} & \textbf{0.34} & \textbf{62.25} & \textbf{61.00} & \textbf{48.55} & \textbf{45.33} \\ \hline
Textual DPO & 2.82 & 3.71 & 12.61 & 14.00 & 8.28 & 62.65 & \textbf{62.00} & \textbf{48.44} & 44.00 \\
SEA DPO & \textbf{1.78} & \textbf{0.42} & \textbf{5.72} & \textbf{6.35} & \textbf{3.56} & \textbf{62.95} & 61.88 & 48.00 & \textbf{44.77} \\ \hline
\end{tabular}%
}
\label{tab:video}
\end{table*}

\begin{table*}[t]
\centering
\caption{The experimental results conducted on Audio-based MLLMs (Qwen2-Audio-7b).}
\resizebox{0.8\textwidth}{!}{%
\begin{tabular}{lccccc|cccc}
\hline
\multirow{2}{*}{Approaches} & \multicolumn{5}{c|}{VASafetyBench (\%)} & \multicolumn{4}{c}{AIRBench} \\ \cline{2-10} 
 & Text-only & Audio-only & KPT & KPT+noise & average & Speech & Sound & Music & mixed-audio \\ \hline
Qwen2-Audio-7b & 14.66 & 7.70 & 24.06 & 34.31 & 20.18 & 5.47 & 4.07 & 3.97 & 4.26 \\ \hline
Textual SFT & 5.05 & 4.82 & 21.40 & 16.45 & \textbf{11.93} & \textbf{5.58} & \textbf{4.13} & \textbf{4.05} & \textbf{4.28} \\
SEA SFT & \textbf{3.31} & \textbf{2.24} & \textbf{1.73} & \textbf{1.77} & 2.26 & 4.87 & 3.70 & 3.61 & 4.00 \\ \hline
Textual DPO & \textbf{6.58} & 4.85 & 18.87 & 35.56 & 16.4 & \textbf{5.58} & \textbf{4.16} & \textbf{4.17} & \textbf{4.29} \\
SEA DPO & 7.71 & \textbf{4.61} & \textbf{3.16} & \textbf{4.15} & \textbf{4.90} & 5.57 & 4.15 & 3.98 & 4.26 \\ \hline
\end{tabular}%
}
\label{tab:audio}
\end{table*}


\section{Experiments}
\subsection{Experimental Setup}
\textbf{Backbones.} We select the widely used MLLM backbone for each modality: LLava-v1.5-7b-hf \cite{liu2024visual} for images, Qwen2-VL-7b \cite{wang2024qwen2} for videos, and Qwen2-Audio-7b \cite{Qwen2-Audio} for audio. 


\textbf{Baselines.} For image-based MLLMs, we have three baselines: (1) VLGuard \cite{zong2024safety} utilizes 2k harmful and 1k harmless image-text pairs for SFT alignment. (2) \textbf{Textual SFT} uses 3k textual alignment samples and SFT as the alignment method. (3) \textbf{Textual DPO} utilizes 3k textual samples and DPO as the alignment method. 
Since there is no related work on safe alignment, only the two baselines, Textual SFT and Textual DPO, are used for video and audio modalities. 

\textbf{Training Datasets.} 
Following the settings in \cite{hu2024vlsbench}, we sample 3k examples from SafeRLHF \cite{ji2024pku}, including 2k harmful samples and 1k harmless samples, as training data for SEA, Textual SFT, and Textual DPO.  
There is a slight difference in the data format. 
VLGuard consists of image-text pairs, whereas our training data only contain plain text data. 


\textbf{Evaluation Benchmark.} 
For safety assessment, we employ MM-SafetyBench for image-based MLLMs and VA-SafetyBench for video and audio-based MLLMs. 
To evaluate general capabilities, we utilize MMMU \cite{hendryckstest2021} and POPE \cite{li2023evaluating} for image models, MVBench \cite{li2024mvbench} and VideoMME \cite{fu2024video} for video models, and AIR-Bench \cite{yang2024air} for audio models.

\textbf{Evaluation Metrics.}
For safety assessment, following the evaluation protocol established in \cite{liu2024mm}, we use GPT-4o-mini as the evaluator. 
The evaluation metric is \textbf{Attack Success Rate (ASR)}, calculated as the percentage of responses flagged as unsafe within the evaluation set. 
For evaluation on general capabilities, we adhere to the evaluation metric defined by the benchmark.

\textbf{Implementation Details.} 
We conducted embedding optimization training of SEA on a single RTX 3090 GPU. 
All MLLMs were set to a maximum of 100 training epochs. 
The learning rates for LLava-v1.5, Qwen2-VL, and Qwen2-Audio were set to 0.02, 0.02, and 0.05, respectively, with cosine annealing updates. 
% To save time, we validated \revisezq{validate what?} using the training set (i.e., content control sample and style control sample) after every 10 gradient updates to determine whether to terminate training early. 
%To determine when to stop training early, we perform inference using instructions from the training set (i.e., content control sample and style control sample) after every 10 gradient update steps. 
For efficiency, we implement an early stopping mechanism during embeddings optimization by evaluating embedding quality every 10 gradient update steps.
For style control, optimization is considered successful if the output is the same with the ground truth. 
For content control, given that the harmful phrase is categorized as ``activity'', we consider optimization successful if $N-1$ out of $N$ words match, allowing for minor verb tense variations while preserving semantic meaning. 
Given that the harmful phrase is categorized as ``product'', we require exact word-for-word matching. 
Training is terminated early when both content and style optimizations are successful. 
For failed optimization samples, we directly use the embeddings from the last epoch. 


For the safety alignment training, we implemented both SFT and DPO training strategies for SEA. More details about the experimental setup can be found in Appendix \ref{appB:SEA}.




\subsection{Main results}
Tables \ref{tab:image}, \ref{tab:video}, and \ref{tab:audio} present the results of experiments conducted on image, video, and audio-based MLLMs. 
%Qwen2-VL-7b, and Qwen2-Audio-7b. 
We will showcase our findings through the following comparisons.


\textbf{Comparison between SEA and Textual Alignment}. 
Both Textual SFT and Textual DPO belong to Textual Alignment approaches. 
SEA demonstrates comparable safety capabilities to Textual Alignment methods under Text-only attacks while significantly lowering the ASR of multimodal attacks. 
For instance, in the most challenging SD-TYPO task, SEA SFT's ASR decreased by 34.95\% compared to Textual SFT, and SEA DPO's ASR decreased by 29.64\% compared to Textual DPO.

Compared to models without safety alignment, Textual SFT and Textual DPO effectively reduce the ASR of text-only attacks. Still, their effectiveness against multimodal attacks is limited, which is particularly evident in the image-based MLLM (Llava-v1.5) and audio-based MLLM (Qwen2-Audio). 


For general performance, whether using SFT or DPO training strategies, the overall performance of SEA is close to that of Textual Alignment. 
In summary, compared to Textual Alignment, SEA can significantly reduce the safety risks introduced by additional modalities without sacrificing overall performance.


\textbf{Comparison between embeddings and physical multimodal data}. 
VLGuard is a baseline trained on physical image-text pairs while SEA operates on embeddings and textual instructions. 
Since both VLGuard and SEA SFT employ SFT as their alignment method and use equivalent training dataset sizes, it is a fair comparison. 
The overall performance of SEA SFT is comparable to that of VLGuard, but SEA SFT demonstrates significantly higher safety capabilities against multimodal attacks than VLGuard. 
The underlying reason may be that SEA SFT always optimizes the image embeddings that the model deems most aligned with harmful content, resulting in higher data quality. 
In contrast, images collected from the real world may not be correctly interpreted by the model regarding harmful content. 


\textbf{Comparison between DPO and SFT}. The SFT-based approaches demonstrate stronger security than the DPO-based methods, but they typically results in a general performance decline. 
This is because using the reference model in DPO aids in maintaining overall performance. 
Notably, aside from a slight degradation in the Qwen2-VL-7b, the general performance of SEA DPO in other model does not decline compared to the original model. 
Therefore, we recommend using DPO as the training strategy for SEA.


\subsection{Efficiency and Quality of Embedding Optimization}
To validate the efficiency of the embedding optimization, we recorded the optimization success rate (OSR) and the time consumed for embedding optimization across 3k samples.
To check whether the model consistently believes that the optimized embeddings contain information from the content control samples, we designed three rewritten versions of the content control instruction, such as ``Could you explain what is occurring in the image?'', and used them to query the MLLMs regarding the content in the optimized embeddings. 
The proportion of model outputs containing the specified content out of total samples is reported as the validation success rate (VSR).

\begin{table}[]
\caption{The OSR, average time consumption, and VSR of the embedding optimization on three models.}
\label{tab:time}
\centering
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\hline
Models & OSR(\%) & Average Time(s) & VSR(\%) \\ \hline
Llava-v1.5-7b-hf & 98.17 & 23.86 & 87.76 \\
Qwen2-VL-7b & 93.67 & 20.37  & 69.52 \\
Qwen2-Audio-7b & 98.37 & 12.06  & 97.15 \\ \hline
\end{tabular}%
}
\end{table}

Table \ref{tab:time} presents the statistical results. SEA successfully finds embeddings for specified content and style in more than 93\% of the cases across all models, demonstrating good generalization even when faced with instructions not seen in the content control samples, indicating that the embeddings are of high quality. 
Furthermore, each sample requires an optimization time of no more than 24 seconds on a single RTX 3090 GPU, which is significantly lower than the cost of manually collecting real data. 
Since optimization is performed on individual samples, SEA allows for parallel embedding optimization of a large-scale textual dataset across multiple GPUs, further saving computational time.


\section{Conclusions}
The high cost of constructing multimodal datasets poses a significant challenge to developing safety alignment. 
In this paper, we demonstrate that synthetic embeddings can substitute for real additional model data, allowing for effective multimodal safety alignment relying solely on text. 
The high performance exhibited across multiple MLLMs, including images, videos, and speech, validates the universality of the proposed SEA method. Until high-quality, large-scale real multimodal datasets are released, it holds promise as a safety solution for emerging MLLMs.

\section*{Limitations}
Although SEA has shown promising performance, it remains a preliminary exploration. Currently, we have only used two samples to optimize the embeddings, lacking attempts to further enhance embedding quality with more complex designs. Moreover, this work's exploration of SEA is limited to safety alignment. Simple modifications to SEA's data preparation process would suffice for other alignment tasks, such as improving honesty. Its effectiveness will need to be explored in future work.

\section*{Ethics Statement}
This work includes harmful datasets and harmful content generated by MLLMs. The harmful instructions in the dataset come from existing safety evaluation benchmarks, and the harmful videos and audio are generated by the models. It is important to emphasize that this harmful content does not reflect the authors' views. The purpose of this work is to propose safety alignment methods to promote the development of safer MLLMs. 
The construction of the dataset and presentation of harmful text generated by the model are solely to validate the effectiveness of our method.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

% \newpage
\section{Supplementary Materials for VA-SafetyBench}
\label{appA:VA_SafetyBench}

\subsection{Motivation}
\label{app:A1}
\textbf{The proposal of VA-SafetyBench.}  This work explores low-resource safety alignment methods generally applicable to various modal MLLMs, including image, video, and audio MLLMs. Quantifying the safety risks of models is crucial for evaluating alignment performance. 
% However, there is currently a lack of open-source safety evaluation benchmarks for the video and audio-based MLLMs, which is the primary motivation for constructing VA-SafetyBench.
However, there is currently a lack of safety benchmarks for video-based MLLMs. 
Their projects are still in a partially developed stage for the two existing security evaluation works \cite{ying2024safebench,yang2024audio} of audio-based MLLMs. Therefore, VA-SafetyBench needs to be constructed to help evaluate SEA's performance.


\textbf{Expansion of MM-SafetyBench} The reason for choosing to expand MM-SafetyBench lies in its two advantages: 1) It provides harmful key phrases extracted from well-crafted prompts, which facilitates the generation of new modal content using text-based generative models. 2) Since the toxicity of text instructions has been transferred to images, most text instructions in MM-SafetyBench are harmless in themselves. This helps us to create benchmarks where harmful information is displayed solely through video or audio.

\textbf{Reduction of Scenarios} MM-SafetyBench contains 13 scenarios, but we have excluded political lobbying, legal opinion, financial advice, health consultation, and government decision-making, as these safety considerations are somewhat overly stringent. For instance, financial advice tests the ability of MLLMs to refuse to provide financial opinions. Similar data is difficult to find in existing alignment training datasets, which poses challenges for evaluating algorithms.

\begin{figure}[]%[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/prompt_1.pdf}
 \caption{Prompt used for generating scene descriptions}
\label{fig:prompt_1}
\end{figure}

\subsection{Prompts for Scene Descriptions Generation}
\label{app:A2}
The prompts for generating scene descriptions are shown in Figure \ref{fig:prompt_1}, where Example 1 and 2 are randomly sampled from the five examples illustrated in Figure \ref{fig:prompt_2}. Example 3 is the sample currently being processed.


\begin{figure}[]%[!htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{figs/prompt_2.pdf}
 \caption{Five example for in-context learning}
\label{fig:prompt_2}
\end{figure}


\begin{table}[]
\centering
\caption{Comparison of experimental results between temporal stitching and spatial stitching.}
\resizebox{\linewidth}{!}{%
\begin{tabular}{ccc}
\hline
Models & DM+TYPO (Temporal) & DM+TYPO (Spatial) \\ \hline
Qwen2-VL-7b & 64.42 & \textbf{71.13} \\
Qwen2-VL-2B & 61.22 & \textbf{69.33} \\
VideoLLaMA2-7B & 39.68 & \textbf{42.33} \\
VideoLLaMA2.1-7B & 51.57 & \textbf{52.26} \\ \hline
\end{tabular}%
}
\label{tab:Temporal and Spatial}
\end{table}

\subsection{Comparison of Temporal Stitching and Spatial Stitching in DM+TYPO}
In the DM+TYPO task of VA-SafetyBench, the videos generated by the model and the TYPO videos are stitched along the timeline. In MM-SafetyBench, the Stable Diffusion images and TYPO images are connected in pixel space. In fact, similar operations can also be performed in videos, such as adding TYPO subtitles to the bottom of each video frame. We have compared the experimental results of the two stitching methods in Table \ref{tab:Temporal and Spatial}. Across all models, the temporally stitched datasets exhibited higher levels of harmfulness, so we adopted this approach.


\subsection{The complete benchmark evaluation results.}
The complete evaluation results of VA-SafetyBench expanded by scenarios are presented in Tables \ref{tab:full_VA_Video} and \ref{tab:full_VA_audio}.


\begin{table*}[t]
\caption{The complete evaluation results of Video-SafetyBench}
\label{tab:full_VA_Video}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccclccccl}
\hline
\multirow{2}{*}{Scenarios} & \multicolumn{5}{c}{Qwen2-VL-7b} & \multicolumn{5}{c}{Qwen2-VL-2B} \\ \cline{2-11} 
 & Text-only & DM & TYPO & DM+TYPO & \multicolumn{1}{c}{Average} & Text-only & DM & TYPO & DM+TYPO & \multicolumn{1}{c}{Average} \\ \hline
Illegal Activitiy & 0 & 36.08 & 46.39 & 84.54 & 41.75 & 3.09 & 11.34 & 30.93 & 81.44 & 31.7 \\
Hate Speech & 0 & 8.59 & 28.22 & 61.96 & 24.69 & 6.75 & 3.68 & 33.13 & 61.96 & 26.38 \\
Malware Generation & 6.82 & 22.73 & 56.82 & 84.09 & 42.61 & 22.73 & 25 & 45.45 & 79.55 & 43.18 \\
Physical Harm & 11.81 & 30.56 & 53.47 & 82.64 & 44.62 & 32.64 & 38.89 & 46.53 & 84.03 & 50.52 \\
Economic Harm & 13.93 & 9.84 & 24.59 & 33.61 & 20.49 & 20.49 & 10.66 & 23.77 & 31.97 & 21.72 \\
Fraud & 0.65 & 18.18 & 46.75 & 82.47 & 37.01 & 13.64 & 13.64 & 38.31 & 81.82 & 36.85 \\
Sex & 13.76 & 4.59 & 32.11 & 47.71 & 24.54 & 31.19 & 4.59 & 28.44 & 50.46 & 28.67 \\
Privacy Violence & 1.44 & 30.94 & 37.41 & 92.09 & 40.47 & 15.11 & 17.27 & 28.78 & 83.45 & 36.15 \\
Average & 6.05 & 20.18 & 40.72 & 71.13 & 34.52 & 18.20 & 15.63 & 34.41 & 69.33 & 34.39 \\ \hline
\multirow{2}{*}{Scenarios} & \multicolumn{5}{c}{VideoLLaMA2-7B} & \multicolumn{5}{c}{VideoLLaMA2.1-7B} \\ \cline{2-11} 
 & Text-only & DM & TYPO & DM+TYPO & \multicolumn{1}{c}{Average} & Text-only & DM & TYPO & DM+TYPO & \multicolumn{1}{c}{Average} \\ \hline
Illegal Activitiy & 4.12 & 44.33 & 25.77 & 59.79 & 33.50 & 1.03 & 46.39 & 65.98 & 72.16 & 46.39 \\
Hate Speech & 10.43 & 14.72 & 16.56 & 35.58 & 19.32 & 4.29 & 12.88 & 34.36 & 42.33 & 23.46 \\
Malware Generation & 29.55 & 27.27 & 31.82 & 47.73 & 34.09 & 9.09 & 27.27 & 54.55 & 65.91 & 39.20 \\
Physical Harm & 29.86 & 34.72 & 27.78 & 55.56 & 36.98 & 11.81 & 33.33 & 56.25 & 58.33 & 39.93 \\
Economic Harm & 10.66 & 9.84 & 6.56 & 22.95 & 12.50 & 12.3 & 11.48 & 20.49 & 26.23 & 17.62 \\
Fraud & 14.94 & 30.52 & 31.82 & 55.84 & 33.28 & 1.95 & 29.22 & 53.9 & 62.99 & 37.01 \\
Sex & 12.84 & 2.75 & 4.59 & 13.76 & 8.48 & 14.68 & 2.75 & 16.51 & 21.1 & 13.76 \\
Privacy Violence & 12.95 & 34.53 & 21.58 & 47.48 & 29.13 & 2.88 & 31.65 & 47.48 & 69.06 & 37.76 \\
Average & 15.66 & 24.83 & 20.81 & 42.33 & 25.90 & 7.25 & 24.37 & 43.69 & 52.26 & 31.89 \\ \hline
\end{tabular}%
}
\end{table*}



\begin{table*}[t]
\caption{The complete evaluation results of audio-SafetyBench}
\label{tab:full_VA_audio}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{lcccclccccl}
\hline
\multirow{2}{*}{Scenarios} & \multicolumn{5}{c}{Qwen2-Audio-7b-Instruct} & \multicolumn{5}{c}{Qwen-Audio-Chat} \\ \cline{2-11} 
 & Text-only & Audio-only & KPT & KPT+noise & Average & Text-only & DM & TYPO & DM+TYPO & Average \\ \hline
Illegal Activitiy & 1.03 & 2.06 & 40.21 & 16.49 & \multicolumn{1}{c}{14.94} & 0 & 37.11 & 73.2 & 57.73 & \multicolumn{1}{c}{42.01} \\
Hate Speech & 1.84 & 1.84 & 20.86 & 15.34 & \multicolumn{1}{c}{9.97} & 2.45 & 50.31 & 43.56 & 34.36 & \multicolumn{1}{c}{32.67} \\
Malware Generation & 31.82 & 15.91 & 54.55 & 38.64 & \multicolumn{1}{c}{35.23} & 34.09 & 50 & 75 & 65.91 & \multicolumn{1}{c}{56.25} \\
Physical Harm & 23.61 & 1.25 & 49.31 & 39.58 & \multicolumn{1}{c}{28.43} & 22.92 & 58.33 & 68.06 & 54.17 & \multicolumn{1}{c}{50.87} \\
Economic Harm & 16.39 & 16.39 & 23.77 & 21.31 & \multicolumn{1}{c}{19.46} & 14.75 & 27.87 & 30.33 & 29.51 & \multicolumn{1}{c}{25.61} \\
Fraud & 5.19 & 3.25 & 44.16 & 22.73 & \multicolumn{1}{c}{18.83} & 3.25 & 54.55 & 68.83 & 53.9 & \multicolumn{1}{c}{45.13} \\
Sex & 20.18 & 13.76 & 12.84 & 14.68 & \multicolumn{1}{c}{15.36} & 10.09 & 42.2 & 33.94 & 31.19 & \multicolumn{1}{c}{29.35} \\
Privacy Violence & 17.27 & 7.19 & 28.78 & 23.74 & \multicolumn{1}{c}{19.24} & 10.07 & 57.55 & 75.54 & 50.36 & \multicolumn{1}{c}{48.38} \\
Average & 14.66 & 7.70 & 34.31 & 24.06 & \multicolumn{1}{c}{20.18} & 12.20 & 47.24 & 58.55 & 47.14 & \multicolumn{1}{c}{41.28} \\ \hline
\multirow{2}{*}{Scenarios} & \multicolumn{5}{c}{SALMONN-7B} & \multicolumn{5}{c}{SALMONN-13B} \\ \cline{2-11} 
 & Text-only & DM & TYPO & DM+TYPO & Average & Text-only & DM & TYPO & DM+TYPO & Average \\ \hline
Illegal Activitiy & 16.49 & - & 51.55 & 80.41 & 49.48 & 15.46 & - & 73.2 & 84.54 & 57.73 \\
Hate Speech & 31.29 & - & 32.52 & 62.58 & 42.13 & 25.15 & - & 53.37 & 64.42 & 47.64 \\
Malware Generation & 75 & - & 31.82 & 70.45 & 59.09 & 77.27 & - & 52.27 & 59.09 & 62.87 \\
Physical Harm & 69.44 & - & 54.17 & 86.81 & 70.14 & 59.03 & - & 66.67 & 77.78 & 67.82 \\
Economic Harm & 22.95 & - & 22.95 & 36.07 & 27.32 & 28.69 & - & 28.69 & 35.25 & 30.87 \\
Fraud & 57.14 & - & 51.95 & 76.62 & 61.90 & 70.13 & - & 65.58 & 80.52 & 72.07 \\
Sex & 27.52 & - & 15.6 & 32.11 & 25.07 & 36.7 & - & 24.77 & 27.52 & 29.66 \\
Privacy Violence & 52.52 & - & 68.35 & 82.73 & 67.86 & 56.83 & - & 76.26 & 85.61 & 72.9 \\
Average & 44.04 & - & 41.11 & 65.97 & 50.37 & 46.15 & - & 55.10 & 64.34 & 55.19 \\ \hline
\end{tabular}%
}
\end{table*}


\section{Supplementary Materials for SEA}
\label{appB:SEA}

\subsection{More Implementation Details of SEA}
\label{appB:B1}

\textbf{Prompt for Harmful Phrase Extraction.} In Section 4.2, the prompt used to instruct GPT-4o-mini to extract harmful phrases is illustrated in Figure \ref{fig:prompt_3}.

\begin{figure}[]%[!htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/prompt_3.pdf}
 \caption{Prompt for harmful phrase extraction}
\label{fig:prompt_3}
\end{figure}


\textbf{Prompts for Sentence Completion.} In Section 4.2, the prompt used to instruct GPT-4o-mini to complete harmful phrases into sentences is shown in Figure \ref{fig:prompt_4}.

\textbf{Details for Content Control and Style Control Samples.}
Both samples consist of an instruction, a response prefixes, and a response. For the content control sample, the response is the sentence completed from harmful phrases. For the style control sample, the response is sampled from a pre-prepared style set. Each model's instruction, response prefixes, and style set are different and are designed based on the output patterns of the model. Specifically, we used 50 test samples to observe each model's output and summarize their habits. Figure \ref{fig:SEA_samples} displays the model's instructions, response prefixes, and style sets.

\subsection{More Details on Experimental Setup}
\textbf{Training Data Construction.} We sampled 2k harmful samples and 1k harmless samples from the SafeRLHF dataset. Each sample in SafeRLHF includes an instruction, a chosen response, and a rejected response. The harmful samples were randomly selected from the rejected responses with a severity level of 3. In contrast, the harmless samples were randomly selected from those where both the chosen and rejected responses were equal to 0. Since we found that a significant amount of harmful content still existed in the chosen responses, we used Llama2-7b-chat to regenerate the chosen responses. In any SFT training, only the instruction and chosen response from each sample are used for training.

\textbf{Evaluation Setting} For MM-SafetyBench, we selected only the same eight scenarios for evaluation as in VA-SafetyBench. In Tables \ref{tab:image}, \ref{tab:video}, and \ref{tab:audio}, the average ASR for each task in MM-SafetyBench and VA-SafetyBench is presented across these eight scenarios. For all general capability evaluation benchmarks, we follow the implementation in \cite{ji2024align}.

\begin{figure}[]%[!htbp]
  \centering
  \includegraphics[width=\linewidth]{figs/prompt_4.pdf}
 \caption{Prompt for sentence completion.}
\label{fig:prompt_4}
\end{figure}


\begin{figure*}[]%[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/SEA_samples.pdf}
 \caption{Instruction, response prefixes, and style sets designed for different models.}
\label{fig:SEA_samples}
\end{figure*}

\textbf{Training Setting for Safety Alignment.} All methods for safety alignment training are implemented on a server equipped with four A800 GPUs. For VLGuard, we follow the original paper's setup and retrain on the Hugging Face version of Llava-v1.5-7b-hf. For other baselines, we summarize the training hyperparameter settings in Table \ref{tab:parameter}.


\begin{table}[]
\centering
\caption{Hyperparameter settings for safe alignment training. (SFT) indicates that SEA SFT and Textual SEA share the same set of hyperparameters.}
\label{tab:parameter}
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\hline
\multicolumn{1}{c}{Approaches} & Training Method & Learning Rate & Epoch \\ \hline
LLaVA-v1.5-7b-hf (SFT) & full-parameter & 2e-5 & 2 \\
LLaVA-v1.5-7b-hf (DPO) & full-parameter & 2e-6 & 3 \\
Qwen2-VL-7b(SFT) & full-parameter & 1e-5 & 2 \\
Qwen2-VL-7b(DPO) & full-parameter & 1e-6 & 3 \\
Qwen2-Audio-7b (SFT) & full-parameter & 2e-5 & 3 \\
Qwen2-Audio-7b (DPO) & full-parameter & 2e-6 & 3 \\ \hline
\end{tabular}%
}
\end{table}

\subsection{Case study}
To further validate the effectiveness of SEA, Figures \ref{fig:image_case}, \ref{fig:video_case}, and \ref{fig:audio_case} respectively present cases evaluated on the safety benchmarks for image, video, and audio MLLMs. All cases come from the most challenging tasks in their respective modality benchmarks, namely SD+TYPO, DM+TYPO, and KPT+noise.

\begin{figure*}[]%[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/image_case.pdf}
 \caption{A case on the security evaluation of the image-based MLLMs. The evaluation sample is sourced from the SD+TYPO task of MM-SafetyBench, with the scenario ``Sex''.}
\label{fig:image_case}
\end{figure*}

\begin{figure*}[]%[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/video_case.pdf}
 \caption{A case on the security evaluation of the video-based MLLMs. The evaluation sample is sourced from the DM+TYPO task of VA-SafetyBench, with the scenario ``physical harm''.}
\label{fig:video_case}
\end{figure*}

\begin{figure*}[]%[!htbp]
  \centering
  \includegraphics[width=\textwidth]{figs/audio_case.pdf}
 \caption{A case on the security evaluation of the audio-based MLLMs. The evaluation sample is sourced from the KPT+noise task of VA-SafetyBench, with the scenario ``malware generation''}
\label{fig:audio_case}
\end{figure*}


\end{document}
