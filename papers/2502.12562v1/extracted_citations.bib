@article{chakraborty2024cross,
  title={Cross-Modal Safety Alignment: Is textual unlearning all you need?},
  author={Chakraborty, Trishna and Shayegani, Erfan and Cai, Zikui and Abu-Ghazaleh, Nael and Asif, M Salman and Dong, Yue and Roy-Chowdhury, Amit K and Song, Chengyu},
  journal={arXiv preprint arXiv:2406.02575},
  year={2024}
}

@article{chao2023jailbreaking,
  title={Jailbreaking black box large language models in twenty queries},
  author={Chao, Patrick and Robey, Alexander and Dobriban, Edgar and Hassani, Hamed and Pappas, George J and Wong, Eric},
  journal={arXiv preprint arXiv:2310.08419},
  year={2023}
}

@inproceedings{chen2024dress,
  title={Dress: Instructing large vision-language models to align and interact with humans via natural language feedback},
  author={Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14239--14250},
  year={2024}
}

@article{gong2023figstep,
  title={Figstep: Jailbreaking large vision-language models via typographic visual prompts},
  author={Gong, Yichen and Ran, Delong and Liu, Jinyuan and Wang, Conglei and Cong, Tianshuo and Wang, Anyu and Duan, Sisi and Wang, Xiaoyun},
  journal={arXiv preprint arXiv:2311.05608},
  year={2023}
}

@article{gu2024mllmguard,
  title={MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models},
  author={Gu, Tianle and Zhou, Zeyang and Huang, Kexin and Liang, Dandan and Wang, Yixu and Zhao, Haiquan and Yao, Yuanqi and Qiao, Xingge and Wang, Keqing and Yang, Yujiu and others},
  journal={arXiv preprint arXiv:2406.07594},
  year={2024}
}

@article{hu2024vlsbench,
  title={Vlsbench: Unveiling visual leakage in multimodal safety},
  author={Hu, Xuhao and Liu, Dongrui and Li, Hao and Huang, Xuanjing and Shao, Jing},
  journal={arXiv preprint arXiv:2411.19939},
  year={2024}
}

@inproceedings{li2024images,
  title={Images are achillesâ€™ heel of alignment: Exploiting visual vulnerabilities for jailbreaking multimodal large language models},
  author={Li, Yifan and Guo, Hangyu and Zhou, Kun and Zhao, Wayne Xin and Wen, Ji-Rong},
  booktitle={European Conference on Computer Vision},
  pages={174--189},
  year={2024},
  organization={Springer}
}

@article{liu2023autodan,
  title={Autodan: Generating stealthy jailbreak prompts on aligned large language models},
  author={Liu, Xiaogeng and Xu, Nan and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2310.04451},
  year={2023}
}

@inproceedings{liu2024mm,
  title={Mm-safetybench: A benchmark for safety evaluation of multimodal large language models},
  author={Liu, Xin and Zhu, Yichen and Gu, Jindong and Lan, Yunshi and Yang, Chao and Qiao, Yu},
  booktitle={European Conference on Computer Vision},
  pages={386--403},
  year={2024},
  organization={Springer}
}

@article{luo2024jailbreakv,
  title={Jailbreakv-28k: A benchmark for assessing the robustness of multimodal large language models against jailbreak attacks},
  author={Luo, Weidi and Ma, Siyuan and Liu, Xiaogeng and Guo, Xiaoyu and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2404.03027},
  year={2024}
}

@article{niu2024jailbreaking,
  title={Jailbreaking attack against multimodal large language model},
  author={Niu, Zhenxing and Ren, Haodong and Gao, Xinbo and Hua, Gang and Jin, Rong},
  journal={arXiv preprint arXiv:2402.02309},
  year={2024}
}

@inproceedings{qi2024visual,
  title={Visual adversarial examples jailbreak aligned large language models},
  author={Qi, Xiangyu and Huang, Kaixuan and Panda, Ashwinee and Henderson, Peter and Wang, Mengdi and Mittal, Prateek},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={19},
  pages={21527--21536},
  year={2024}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@article{shi2024assessment,
  title={Assessment of multimodal large language models in alignment with human values},
  author={Shi, Zhelun and Wang, Zhipin and Fan, Hongxing and Zhang, Zaibin and Li, Lijun and Zhang, Yongting and Yin, Zhenfei and Sheng, Lu and Qiao, Yu and Shao, Jing},
  journal={arXiv preprint arXiv:2403.17830},
  year={2024}
}

@article{yang2024audio,
  title={Audio Is the Achilles' Heel: Red Teaming Audio Large Multimodal Models},
  author={Yang, Hao and Qu, Lizhen and Shareghi, Ehsan and Haffari, Gholamreza},
  journal={arXiv preprint arXiv:2410.23861},
  year={2024}
}

@article{ying2024safebench,
  title={Safebench: A safety evaluation framework for multimodal large language models},
  author={Ying, Zonghao and Liu, Aishan and Liang, Siyuan and Huang, Lei and Guo, Jinyang and Zhou, Wenbo and Liu, Xianglong and Tao, Dacheng},
  journal={arXiv preprint arXiv:2410.18927},
  year={2024}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

