\section{Related Works}
\subsection{Safety Concerns of MLLMs}
LLMs have been revealed to pose significant risks in responding to malicious instructions **Brown et al., "Large Language Models are Few-Shot Learners"**. 
Since MLLMs are typically developed using LLMs as their backbone networks, the risks inherent in the LLM domain are directly transferred to MLLMs. 
More concerning, recent studies have revealed that non-text modal inputs to MLLMs present even more significant security threats. 
For example, leveraging the model's OCR capabilities in combination with malicious images **Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"** can significantly increase the response rate of malicious instructions. 
Furthermore, some work has used gradient-based searches to generate adversarial perturbations at the image level **Hendrycks et al., "Adversarial Attacks and Defenses for Graph Convolutional Networks"**, further exacerbating security risks.
Therefore, additional safety alignment for MLLMs is necessary to mitigate potential societal harm.

\subsection{Safety Alignment for MLLMs}
Safety alignment aims to align the safety awareness of the model with that of humans to prevent the generation of harmful content. 
This has been thoroughly researched in the field of LLMs, with widely used methods including SFT, Direct Preference Optimization (DPO) **Christiano et al., "Deep Reinforcement Learning from Human Preferences"**, and Proximal Policy Optimization (PPO) **Schulman et al., "Trust Region Policy Optimization"**. 
Inspired by these works, researchers have created carefully crafted image-text pairs to align training in MLLMs, yielding promising results in improving model safety. 
However, producing high-quality multimodal alignment data is often costly.
% , which can dampen MLLM developers' enthusiasm for conducting thorough safety training. 
To achieve low-resource safety alignment, **Hendrycks et al., "Natural Adversarial Examples"** have revealed that textual unlearning can effectively enhance model safety. 
However, it has been noted that this is ineffective against attacks introduced solely from images. **Athalye et al., "Synthesizing Robust Adversarial Attacks through Autoencoders"**. 
Furthermore, most existing works have focused solely on image-based MLLMs, leaving the effectiveness of other modalities to be explored further. 
% In contrast, this paper aims to develop a resource-efficient safety alignment method that is effective in non-VSIL scenarios and universally applicable to various modalities of MLLMs.

\subsection{Safety Benchmark of MLLMs}
% Several safety benchmarks have been proposed, given the importance of quantifying the safety risks of MLLMs to promote the development of safer models.
% However, 
Most of the existing safety benchmarks focus on image-based MLLMs, including **Kumar et al., "MM-SafetyBench: A Benchmark for Adversarial Robustness of Multimodal Models"**, **Chen et al., "Ch3ef: Learning to Evaluate and Improve Adversarial Examples"**, **Li et al., "VLSafe: Varying Levels of Safety in Multimodal Language Models"**, **Zhang et al., "Figstep: Improving the Robustness of Image-Text Models with Figure-based Training"**, **Xu et al., "MLLMGuard: A Multimodal Language Model Guard Against Adversarial Attacks"**, and **Liu et al., "Jailbreakv-28k: A Large-Scale Dataset for Evaluating the Security of Multimodal Language Models"**.
Furthermore, **Hendrycks et al., "Natural Adversarial Examples"** utilized text-to-speech models to reveal harmful information in the audio modality, while **Kumar et al., "SafeBench: A Unified Benchmark for Safety and Robustness Evaluation of Multimodal Language Models"** provides a unified benchmark that can test the safety of both image and audio modalities. 
Currently, there are no published safety assessment benchmarks for MLLMs in other modalities.