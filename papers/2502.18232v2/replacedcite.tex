\section{Related Work}
\subsection{State Space Model}
SSMs have emerged as powerful architectures for sequence modeling____, with each iteration addressing key computational and performance challenges. The Linear State Space Layer (LSSL)____ pioneered the use of HIPPO (High-order Polynomial Projection Operators) matrices and structured matrices for capturing long-range dependencies. However, its computational complexity limited practical applications for extended sequences.

S4____ marked a significant advancement by introducing novel parameterization techniques that transform HIPPO matrices into diagonal plus low-rank representations. This innovation, coupled with implicit diagonalization, substantially reduced computational overhead for long sequence processing. Building on this foundation, S5____ further streamlined computations through diagonal approximation and parallel scanning mechanisms, achieving both improved efficiency and performance.

Mamba____ represents the current state-of-the-art through its S6 module, which revolutionizes the approach by converting fixed inputs into variable function forms. Its selective scanning mechanism effectively replaces traditional convolutions, enabling intelligent filtering of information flow. This architectural innovation allows Mamba to dynamically focus on relevant features while discarding redundant information.

\subsection{Liver segmentation}
Deep learning has transformed medical image segmentation, initially with UNet ____, later with nnUnet, establishing a foundational encoder-decoder architecture that efficiently combines high-resolution features with contextual information through skip connections. These architectures have spawned numerous innovations in liver segmentation____, each addressing specific aspects of the segmentation challenge.

Attention mechanisms have emerged as a key enhancement to the basic UNet architecture. AttentionUNet++____ introduced attention gates with dense connections, while Jin et al. developed a dual-branch approach combining attention residual learning with feature enhancement through a sophisticated trunk-and-mask architecture. SAR-UNet____ further refined this approach by incorporating squeeze-and-excitation blocks for adaptive feature enhancement and atrous spatial pyramid pooling for multi-scale context capture, while addressing gradient flow through residual connections.

Recent architectures have focused on multi-scale feature learning and boundary refinement. Xie et al.____ proposed a multi-scale context extraction module with external attention and boundary correction mechanisms. Liu et al.____ introduced a hybrid attention approach combining global dependencies with local feature focus. %Moving beyond traditional convolutional architectures, Jha et al.____ leveraged transformer-based vision models with residual connections, demonstrating superior computational efficiency and segmentation accuracy.

\begin{figure*} [!t]
    \centering
    \includegraphics[width=0.65\textwidth]{Figures/RMAMamba.pdf}
    \caption{Overview of the proposed \textit{RMA-Mamba} architecture.}
    \label{fig:RMAMamba}
\end{figure*}