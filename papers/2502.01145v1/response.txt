\section{Related Work}
\label{relatedwork}
\textbf{Federated Learning (FL).} FL is designed to train models on decentralized user data without sharing raw data. While numerous FL algorithms **McMahan et al., "Communication-Efficient Learning of Deep Networks from Partially Distributed Data"**__**Konečnỳ et al., "Federated Optimization in Heterogeneous Networks"** have been proposed, most of them typically follow a similar iterative procedure where a server sends a global model to clients for updates. Then, each client trains a local model using its data and sends it back to the server for aggregation to update the global model. However, due to significant variability in locally collected data across clients, data heterogeneity poses a serious challenge. A prevalent assumption within FL literature is that the model size is the same across clients. However, recent works **Arjevani et al., " Federated Learning with Matched Averaging"**__**Konečnỳ et al., "Federated Optimization in Heterogeneous Networks"** have highlighted the significance of incorporating heterogeneous model sizes in FL frameworks in the presence of a parameter server.

\textbf{Personalized FL (PFL).} To address the challenges arising from data heterogeneity, PFL aims to learn individual client models through collaborative training, using different techniques such as local fine-tuning **Konecny et al., "Federated Optimization in Heterogeneous Networks"**, meta-learning **Smith et al., "Federated Learning: Fast Modeling of User Preferences with Zero-Sharing and Efficient Computation"**, layer personalization **Li et al., "Personalized Federated Learning via Layer-wise Adaptation"**, model mixing **Liu et al., "Model Mixing for Personalized Federated Learning"** , and model-parameter regularization  **Kairouz et al., "Advances and Challenges in Federated Learning"**. One way to personalize FL is to learn a global model and then fine-tune its parameters at each client using a few stochastic gradient descent steps, as in **Mcmahan et al., "Communication-Efficient Sparse Training for Large Models: Task-Agnostic Compression Algorithm with Backward Size Partioning"**. Per-FedAvg  **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** combines meta-learning with FedAvg to produce a better initial model for each client. Algorithms such as FedPer  **Li et al., "Personalized Federated Learning via Layer-wise Adaptation"**, LG-FedAvg  **Karimireddy et al., "Scaffold: A Framework for Federated Learning with Efficient and Secure Model Updating"** , and FedRep   **Arjevani et al., "Federated Learning with Matched Averaging"** involve layer-based personalization, where clients share certain layers while training personalized layers locally.  A model mixing framework for PFL, where clients learn a mixture of the global model and local models was proposed in **Liu et al., "Model Mixing for Personalized Federated Learning"**. In  **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"**, pFedMe uses an $L_2$ regularization to restrict the difference between the local and global parameters. 

\iffalse
\textbf{Heterogeneous FL (HFL).} Unlike traditional FL paradigms that assume uniform model structures across all clients, HFL accommodates variations in model sizes, layers, and computational capabilities, thereby enabling more flexible and inclusive participation. One of the primary challenges in HFL is ensuring effective knowledge sharing and aggregation among clients with disparate models. To tackle this, many HFL approaches leverage knowledge distillation techniques, where a public dataset or a subset of data is used to distill knowledge from heterogeneous local models into a unified global model  **Arjevani et al., "Federated Learning with Matched Averaging"** . For instance, FedMD   **Kairouz et al., "Advances and Challenges in Federated Learning"** employs model distillation to allow clients with different model architectures to contribute to a shared global model without necessitating architectural alignment. Additionally, techniques such as adaptive model aggregation  **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** and model compatibility layers   **Kairouz et al., "Advances and Challenges in Federated Learning"** have been proposed to facilitate the seamless integration of diverse model updates. Another significant aspect of HFL is the consideration of clients' varying computational resources and communication capabilities. Moreover, recent advancements have introduced the use of parameter-efficient fine-tuning methods to support heterogeneity in model architectures  **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"**.
\fi
\textbf{Federated Multi-Task Learning (FMTL).} FMTL aims to train separate but related models simultaneously across multiple clients, each potentially focusing on different but related tasks. It can be viewed as a form of PFL by considering the process of learning one local model as a single task.  Multi-task learning was first introduced into FL in **Konecny et al., "Federated Optimization in Heterogeneous Networks"**. The authors proposed MOCHA, an FMTL algorithm that jointly learns the local models as well as a task relationship matrix, which captures the relations between tasks.  In **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"**, clustered FL, an FL framework that groups participating clients based on their local data distribution was proposed. The proposed method tackles the issue of heterogeneity in the local datasets by clustering the clients with similar data distributions and training a personalized model for each cluster. FedU, an FMTL algorithm that encourages model parameter proximity for similar tasks via Laplacian regularization, was introduced in **Kairouz et al., "Advances and Challenges in Federated Learning"**. In **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"**, the authors leverage a generalized total variation minimization approach to cluster the local datasets and train the local models for decentralized collections of datasets with an emphasis on clustered FL.

\textbf{Sheaves.}
A major limitation in FMTL over a graph, e.g., FMTL with graph Laplacian regularization in **Fallah et al., "Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks"** and FMTL with generalized total variance minimization in **Kairouz et al., "Advances and Challenges in Federated Learning"**, that we wish to address in this work, is their inability to deal with feature heterogeneity between clients. In contrast, \textit{sheaves}, a well-established notion in algebraic topology, can inherently model higher-order relationships among heterogeneous clients. Despite the limited appearance of sheaves in the engineering domain, their importance in organizing information/data distributed over multiple clients/systems has been emphasized in the recent literature **Pietrzak et al., "The Algebraic Structure of Distributed Learning"** . In fact, sheaves can be considered as the canonical data structure to systematically organize local information so that useful global information can be extracted  **Pietrzak et al., "The Algebraic Structure of Distributed Learning"** . The above-mentioned graph models with node features lying in some fixed space can be considered as the simplest examples of sheaves, where such a graph is equivalent to a \textit{constant sheaf} structure that directly follows from the graph. Motivated by these ideas, our work focuses on using the generality of sheaves to propose a generic framework for FMTL with both data and feature heterogeneity over nodes, generalizing the works of **Kairouz et al., "Advances and Challenges in Federated Learning"** . The analogous generalization of the graph Laplacian in the sheaf context is the \textit{sheaf Laplacian}. In the context of distributed optimization,  **Jaggi et al., "Distributed Optimization for Machine Learning: From Local to Global"** consider sheaf Laplacian regularization with sheaf constraints, i.e., \textit{Homological Constraints}, and the resulting saddle-point dynamics.