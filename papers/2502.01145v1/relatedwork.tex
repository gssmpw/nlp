\section{Related Work}
\label{relatedwork}
\textbf{Federated Learning (FL).} FL is designed to train models on decentralized user data without sharing raw data. While numerous FL algorithms \citep{mcmahan2017communication, karimireddy2020scaffold, li2020federated, Lin2020, elgabli22a} have been proposed, most of them typically follow a similar iterative procedure where a server sends a global model to clients for updates. Then, each client trains a local model using its data and sends it back to the server for aggregation to update the global model. However, due to significant variability in locally collected data across clients, data heterogeneity poses a serious challenge. A prevalent assumption within FL literature is that the model size is the same across clients. However, recent works \citep{zhang2022hybrid, liu2022no} have highlighted the significance of incorporating heterogeneous model sizes in FL frameworks in the presence of a parameter server.

\textbf{Personalized FL (PFL).} To address the challenges arising from data heterogeneity, PFL aims to learn individual client models through collaborative training, using different techniques such as local fine-tuning \citep{wang2019federated, yu2020salvaging}, meta-learning \citep{fallah2020personalized,chen2018federated, jiang2019improving}, layer personalization \citep{arivazhagan2019federated, liang2020think, collins2021exploiting}, model mixing \citep{hanzely2020federated, deng2020adaptive}, and model-parameter regularization \citep{t2020personalized, li2021ditto, huang2021personalized, liu2022privacy}. One way to personalize FL is to learn a global model and then fine-tune its parameters at each client using a few stochastic gradient descent steps, as in \citep{yu2020salvaging}. Per-FedAvg \citep{fallah2020personalized} combines meta-learning with FedAvg to produce a better initial model for each client. Algorithms such as FedPer \citep{arivazhagan2019federated}, LG-FedAvg \citep{liang2020think}, and FedRep \citep{collins2021exploiting} involve layer-based personalization, where clients share certain layers while training personalized layers locally.  A model mixing framework for PFL, where clients learn a mixture of the global model and local models was proposed in \citep{hanzely2020federated}. In \citep{t2020personalized}, pFedMe uses an $L_2$ regularization to restrict the difference between the local and global parameters. 

\iffalse
\textbf{Heterogeneous FL (HFL).} Unlike traditional FL paradigms that assume uniform model structures across all clients, HFL accommodates variations in model sizes, layers, and computational capabilities, thereby enabling more flexible and inclusive participation. One of the primary challenges in HFL is ensuring effective knowledge sharing and aggregation among clients with disparate models. To tackle this, many HFL approaches leverage knowledge distillation techniques, where a public dataset or a subset of data is used to distill knowledge from heterogeneous local models into a unified global model \citep{li2019fedmd, zhu2021data}. For instance, FedMD \citep{li2019fedmd} employs model distillation to allow clients with different model architectures to contribute to a shared global model without necessitating architectural alignment. Additionally, techniques such as adaptive model aggregation \citep{zhai2024adaptive} and model compatibility layers \citep{setayesh2023perfedmask} have been proposed to facilitate the seamless integration of diverse model updates. Another significant aspect of HFL is the consideration of clients' varying computational resources and communication capabilities. Moreover, recent advancements have introduced the use of parameter-efficient fine-tuning methods to support heterogeneity in model architectures \citep{chen2024feddat}.
\fi
\textbf{Federated Multi-Task Learning (FMTL).} FMTL aims to train separate but related models simultaneously across multiple clients, each potentially focusing on different but related tasks. It can be viewed as a form of PFL by considering the process of learning one local model as a single task.  Multi-task learning was first introduced into FL in \citep{smith2017federated}. The authors proposed MOCHA, an FMTL algorithm that jointly learns the local models as well as a task relationship matrix, which captures the relations between tasks.  In the context of FMTL, task similarity can be represented through graphs, matrices, or clustering. In \citep{sattler2020clustered}, clustered FL, an FL framework that groups participating clients based on their local data distribution was proposed. The proposed method tackles the issue of heterogeneity in the local datasets by clustering the clients with similar data distributions and training a personalized model for each cluster. FedU, an FMTL algorithm that encourages model parameter proximity for similar tasks via Laplacian regularization, was introduced in \citep{dinh2022new}. In \citep{sarcheshmehpour2023networked}, the authors leverage a generalized total variation minimization approach to cluster the local datasets and train the local models for decentralized collections of datasets with an emphasis on clustered FL. %A more in-depth comparison between PFL, HFL, and FMTL can be found in Appendix \ref{appendix:comparison}.

\textbf{Sheaves.}
A major limitation in FMTL over a graph, e.g., FMTL with graph Laplacian regularization in \citep{dinh2022new} and FMTL with generalized total variance minimization in \citep{sarcheshmehpour2023networked}, that we wish to address in this work, is their inability to deal with feature heterogeneity between clients. In contrast, \textit{sheaves}, a well-established notion in algebraic topology, can inherently model higher-order relationships among heterogeneous clients. Despite the limited appearance of sheaves in the engineering domain, their importance in organizing information/data distributed over multiple clients/systems has been emphasized in the recent literature \citep{robinson2014topological, robinson2013understanding, riess2022diffusion}. In fact, sheaves can be considered as the canonical data structure to systematically organize local information so that useful global information can be extracted \citep{robinson2017sheaves}. The above-mentioned graph models with node features lying in some fixed space can be considered as the simplest examples of sheaves, where such a graph is equivalent to a \textit{constant sheaf} structure that directly follows from the graph. Motivated by these ideas, our work focuses on using the generality of sheaves to propose a generic framework for FMTL with both data and feature heterogeneity over nodes, generalizing the works of \citep{dinh2022new, sarcheshmehpour2023networked}. The analogous generalization of the graph Laplacian in the sheaf context is the \textit{sheaf Laplacian}. In the context of distributed optimization, \citep{hansen2019distributed} consider sheaf Laplacian regularization with sheaf constraints, i.e., \textit{Homological Constraints}, and the resulting saddle-point dynamics.