\documentclass[10pt]{article} % For LaTeX2e
%\usepackage{tmlr}
% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{tmlr}
% To de-anonymize and remove mentions to TMLR (for example for posting to preprint servers), instead use the following:
\usepackage[preprint]{tmlr}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amssymb}
%\usepackage{subfigure}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{algorithm,algpseudocode,multirow,hhline}
\usepackage{booktabs,multirow}
\usepackage{url}
\usepackage{amsthm}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}

\title{Tackling Feature and Sample Heterogeneity in Decentralized Multi-Task Learning: A Sheaf-Theoretic Approach}

% Authors must not appear in the submitted version. They should be hidden
% as long as the tmlr package is used without the [accepted] or [preprint] options.
% Non-anonymous submissions will be rejected without review.

\author{\name Chaouki Ben Issaid \email chaouki.benissaid@oulu.fi \\
      \addr Centre for Wireless Communications\\
      University of Oulu, Finland
      \AND
      \name Praneeth Vepakomma \email vepakom@mit.edu\\
      \addr MBZUAI and Massachusetts Institute of Technology
      \AND
      \name Mehdi Bennis \email mehdi.bennis@oulu.fi\\
      \addr Centre for Wireless Communications\\
      University of Oulu, Finland}

% The \author macro works with any number of authors. Use \AND 
% to separate the names and addresses of multiple authors.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{{\ours}}{\texttt{Sheaf-FMTL}}
\newcommand{\F}{\mathcal{F}}
\newcommand{\G}{\mathcal{G}}

\def\month{MM}  % Insert correct month for camera-ready version
\def\year{YYYY} % Insert correct year for camera-ready version
\def\openreview{\url{https://openreview.net/forum?id=XXXX}} % Insert correct link to OpenReview for camera-ready version


\begin{document}


\maketitle

\begin{abstract}
Federated multi-task learning (FMTL) aims to simultaneously learn multiple related tasks across clients without sharing sensitive raw data. However, in the decentralized setting, existing FMTL frameworks are limited in their ability to capture complex task relationships and handle feature and sample heterogeneity across clients. To address these challenges, we introduce a novel sheaf-theoretic-based approach for FMTL. By representing client relationships using cellular sheaves, our framework can flexibly model interactions between heterogeneous client models. We formulate the sheaf-based FMTL optimization problem using sheaf Laplacian regularization and propose the {\ours} algorithm to solve it. We show that the proposed framework provides a unified view encompassing many existing federated learning (FL) and FMTL approaches. Furthermore, we prove that our proposed algorithm, {\ours}, achieves a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Extensive experiments demonstrate that {\ours} exhibits communication savings by sending significantly fewer bits compared to decentralized FMTL baselines.
\end{abstract}

\section{Introduction} \label{intro}
The growing demand for privacy-preserving distributed learning algorithms has steered the research community towards federated learning (FL) \citep{mcmahan2017communication}, a learning paradigm that allows several clients, such as mobile devices or organizations, to cooperatively train a model without revealing their raw data. By aggregating locally computed updates rather than raw data, FL aims to learn a global model that benefits from the different data distributions inherently present across the participating clients. Despite its promise, conventional FL faces significant hurdles when dealing with client data heterogeneity. In fact, while the global model may perform well on average, the statistically heterogeneous clients' data have been shown to affect the model's existence and convergence \citep{sattler2020clustered, Li2020}. These challenges are exacerbated in a decentralized environment where coordination is limited and direct control over the client models is not feasible. Recently, there have been several attempts to bring personalization into FL to learn distinct local models \citep{wang2019federated, fallah2020personalized, hanzely2020federated} since learning a personalized model per client is more suitable than a single global model to tackle data heterogeneity. These models are specifically learned to fit the heterogeneous local data distribution via techniques such as federated multi-task learning (FMTL) \citep{smith2017federated, dinh2022new} that model the interactions between the different personalized local models.

FMTL generalizes the FL framework by allowing the simultaneous learning of multiple related but distinct tasks across several clients. Unlike traditional FL, which focuses on training a single global model, FMTL acknowledges that different clients may be interested in solving distinct tasks that are related but not identical. By leveraging task relatedness, FMTL aims to improve the performance of individual task models through shared knowledge while maintaining task-specific uniqueness. This approach not only enhances the generalization performance of the models on individual tasks by leveraging shared information but also contributes to tackling the non-independent and identically distributed (non-IID) nature of data across clients. FMTL considers the different objectives and data distributions across clients, customizing models to perform optimally on each task while still benefiting from the federated structure of the problem as well as the similarity between these tasks.  However, existing FMTL frameworks are not without limitations. A major concern is the oversimplified view of task interdependencies, where relationships between tasks are often modeled using simple fixed scalar weights. This approach captures only the basic notion of task relatedness but fails to represent more intricate and higher-order dependencies that may exist among tasks. As a result, these models may overlook subtle interconnections and dynamic patterns of interdependence, leading to suboptimal knowledge sharing and reduced performance in heterogeneous and decentralized environments. For a comprehensive understanding of scenarios where task similarities are naturally defined in vector spaces, kindly refer to Appendix \ref{appendix:vector_space_scenarios}.  Furthermore, a critical issue with the current FMTL frameworks is the assumption that the models have the same size, which limits the applicability of FMTL in the case of different model sizes. Last but not least, to the best of our knowledge, apart from MOCHA \citep{smith2017federated}, which requires the presence of a server, the interactions between the tasks are assumed to be known and not learned during training. Therefore, to address these challenges, we seek to answer the following question:

\textbf{``How can we effectively model and learn the complex interactions between various tasks/models in an FMTL decentralized setting, even in the presence of different model sizes?"}

To answer this question, the concept of sheaves provides a novel lens through which the interactions between clients in a decentralized FMTL setting can be modeled. The mathematical notion of a sheaf initially invented and developed in algebraic topology, is a framework that systematically organizes local observations in a way that allows one to make conclusions about the global consistency of such observations \citep{robinson2014topological, robinson2013understanding, riess2022diffusion}. As such requirements are a central part of FL problems, it is natural to ask how one could utilize a sheaf-based framework to find an effective solution to the above question. Given an underlying topology of the client relationships, we employ the notion of a cellular sheaf that captures the underlying geometry, enabling a richer and more nuanced multi-task learning environment. Sheaves enable the representation of local models as \textit{sections} over the underlying space, offering a structured way to capture the relationships between tasks/models in FMTL settings. As an inherent feature of the sheaf-based framework, our approach can support heterogeneity over local models. More specifically, our framework naturally facilitates learning models with different model dimensions. Moreover, sheaves are inherently distributed in nature and hence facilitate decentralized training. A sheaf data structure consists of vector spaces and linear mappings between them. In this work, we model the underlying space as a graph, and vector spaces are defined over vertices and edges, capturing pairwise interactions. Crucially, we are required to learn these maps that constitute the sheaf structure, as a decision variable of our problem. Learning these maps is instrumental in comparing heterogeneous models by projecting them onto a common space. 

\textbf{Contributions.} This paper introduces a novel unified approach that fundamentally rethinks FMTL and gives it a new interpretation by incorporating principles from sheaf theory. In what follows, we summarize our main contributions
\begin{itemize}
    \item Our proposed framework demonstrates a high degree of flexibility as it addresses the challenges arising from both feature and sample heterogeneity in the context of FMTL exploiting sheaf theory. It may be regarded as a comprehensive and unified framework for FMTL, as it encompasses a multitude of existing frameworks, including personalized FL \citep{hanzely2020federated}, conventional FMTL \citep{dinh2022new}, hybrid FL \citep{zhang2022hybrid}, and conventional FL \citep{mcmahan2017communication}.
    \item To the best of our knowledge, this is the first work that proposes to solve the FMTL in a decentralized setting while modelling higher-order relationships among heterogeneous clients. Furthermore, unlike existing decentralized FMTL frameworks, we learn the interactions between the clients as part of our optimization framework.
    \item Our algorithm, coined {\ours}, exhibits high communication efficiency, as the size of shared vectors among clients is significantly smaller in practice compared to the original models. Furthermore, exchanging a modified version of the clients' models provides an additional layer of privacy.
    \item A detailed convergence analysis of our proposed algorithm shows that the average squared norm of the objective function gradient decreases at a rate of $\mathcal{O}(1/K)$, where $K$ is the number of iterations, recovering the convergence rate of state-of-the-art FMTL \citep{smith2017federated,dinh2022new}.
    \item Extensive simulation results demonstrate the performance of our proposed algorithms on several benchmark datasets compared to state-of-the-art approaches.
\end{itemize}
\section{Related Work}\label{relatedwork}
\textbf{Federated Learning (FL).} FL is designed to train models on decentralized user data without sharing raw data. While numerous FL algorithms \citep{mcmahan2017communication, karimireddy2020scaffold, li2020federated, Lin2020, elgabli22a} have been proposed, most of them typically follow a similar iterative procedure where a server sends a global model to clients for updates. Then, each client trains a local model using its data and sends it back to the server for aggregation to update the global model. However, due to significant variability in locally collected data across clients, data heterogeneity poses a serious challenge. A prevalent assumption within FL literature is that the model size is the same across clients. However, recent works \citep{zhang2022hybrid, liu2022no} have highlighted the significance of incorporating heterogeneous model sizes in FL frameworks in the presence of a parameter server.

\textbf{Personalized FL (PFL).} To address the challenges arising from data heterogeneity, PFL aims to learn individual client models through collaborative training, using different techniques such as local fine-tuning \citep{wang2019federated, yu2020salvaging}, meta-learning \citep{fallah2020personalized,chen2018federated, jiang2019improving}, layer personalization \citep{arivazhagan2019federated, liang2020think, collins2021exploiting}, model mixing \citep{hanzely2020federated, deng2020adaptive}, and model-parameter regularization \citep{t2020personalized, li2021ditto, huang2021personalized, liu2022privacy}. One way to personalize FL is to learn a global model and then fine-tune its parameters at each client using a few stochastic gradient descent steps, as in \citep{yu2020salvaging}. Per-FedAvg \citep{fallah2020personalized} combines meta-learning with FedAvg to produce a better initial model for each client. Algorithms such as FedPer \citep{arivazhagan2019federated}, LG-FedAvg \citep{liang2020think}, and FedRep \citep{collins2021exploiting} involve layer-based personalization, where clients share certain layers while training personalized layers locally.  A model mixing framework for PFL, where clients learn a mixture of the global model and local models was proposed in \citep{hanzely2020federated}. In \citep{t2020personalized}, pFedMe uses an $L_2$ regularization to restrict the difference between the local and global parameters. 

\iffalse
\textbf{Heterogeneous FL (HFL).} Unlike traditional FL paradigms that assume uniform model structures across all clients, HFL accommodates variations in model sizes, layers, and computational capabilities, thereby enabling more flexible and inclusive participation. One of the primary challenges in HFL is ensuring effective knowledge sharing and aggregation among clients with disparate models. To tackle this, many HFL approaches leverage knowledge distillation techniques, where a public dataset or a subset of data is used to distill knowledge from heterogeneous local models into a unified global model \citep{li2019fedmd, zhu2021data}. For instance, FedMD \citep{li2019fedmd} employs model distillation to allow clients with different model architectures to contribute to a shared global model without necessitating architectural alignment. Additionally, techniques such as adaptive model aggregation \citep{zhai2024adaptive} and model compatibility layers \citep{setayesh2023perfedmask} have been proposed to facilitate the seamless integration of diverse model updates. Another significant aspect of HFL is the consideration of clients' varying computational resources and communication capabilities. Moreover, recent advancements have introduced the use of parameter-efficient fine-tuning methods to support heterogeneity in model architectures \citep{chen2024feddat}.
\fi
\textbf{Federated Multi-Task Learning (FMTL).} FMTL aims to train separate but related models simultaneously across multiple clients, each potentially focusing on different but related tasks. It can be viewed as a form of PFL by considering the process of learning one local model as a single task.  Multi-task learning was first introduced into FL in \citep{smith2017federated}. The authors proposed MOCHA, an FMTL algorithm that jointly learns the local models as well as a task relationship matrix, which captures the relations between tasks.  In the context of FMTL, task similarity can be represented through graphs, matrices, or clustering. In \citep{sattler2020clustered}, clustered FL, an FL framework that groups participating clients based on their local data distribution was proposed. The proposed method tackles the issue of heterogeneity in the local datasets by clustering the clients with similar data distributions and training a personalized model for each cluster. FedU, an FMTL algorithm that encourages model parameter proximity for similar tasks via Laplacian regularization, was introduced in \citep{dinh2022new}. In \citep{sarcheshmehpour2023networked}, the authors leverage a generalized total variation minimization approach to cluster the local datasets and train the local models for decentralized collections of datasets with an emphasis on clustered FL. %A more in-depth comparison between PFL, HFL, and FMTL can be found in Appendix \ref{appendix:comparison}.

\textbf{Sheaves.}
A major limitation in FMTL over a graph, e.g., FMTL with graph Laplacian regularization in \citep{dinh2022new} and FMTL with generalized total variance minimization in \citep{sarcheshmehpour2023networked}, that we wish to address in this work, is their inability to deal with feature heterogeneity between clients. In contrast, \textit{sheaves}, a well-established notion in algebraic topology, can inherently model higher-order relationships among heterogeneous clients. Despite the limited appearance of sheaves in the engineering domain, their importance in organizing information/data distributed over multiple clients/systems has been emphasized in the recent literature \citep{robinson2014topological, robinson2013understanding, riess2022diffusion}. In fact, sheaves can be considered as the canonical data structure to systematically organize local information so that useful global information can be extracted \citep{robinson2017sheaves}. The above-mentioned graph models with node features lying in some fixed space can be considered as the simplest examples of sheaves, where such a graph is equivalent to a \textit{constant sheaf} structure that directly follows from the graph. Motivated by these ideas, our work focuses on using the generality of sheaves to propose a generic framework for FMTL with both data and feature heterogeneity over nodes, generalizing the works of \citep{dinh2022new, sarcheshmehpour2023networked}. The analogous generalization of the graph Laplacian in the sheaf context is the \textit{sheaf Laplacian}. In the context of distributed optimization, \citep{hansen2019distributed} consider sheaf Laplacian regularization with sheaf constraints, i.e., \textit{Homological Constraints}, and the resulting saddle-point dynamics. 
\section{Sheaf-based Federated Multi-Task Learning ({\ours})}\label{setup}
\subsection{FMTL Problem Setting}\label{setting}
We consider a connected network of $N$ clients modeled by a graph $\mathcal{G} = (\mathcal{V}, \mathcal{E})$, where $\mathcal{V} = [N] = \{1, \dots, N\}$ is the set of clients, and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ represents the set of edges, i.e., the set of pairs of clients that can communicate with each other. Each client $i \in \mathcal{V}$ has a local loss function $f_i: \mathbb{R}^{d_i} \rightarrow \mathbb{R}$ and only has access to its own local data distribution $\mathcal{D}_i$. Client $i$ can only communicate with the set of its neighbors defined as $\mathcal{N}_i = \{j \in \mathcal{V}|(i, j) \in \mathcal{E} \}$ whose cardinality is $|\mathcal{N}_i| = \delta_i$. In this work, we aim to fit different models, i.e., $\bm{\theta}_i \in \mathbb{R}^{d_i},$  $\forall i \in [N]$, to the local data of clients, while accounting for the interactions between these models. Finally, let $\bm{\theta} = [\bm{\theta}_1^T, \dots, \bm{\theta}_N^T]^T \in \mathbb{R}^{d}$ be the stack of the local decision variables $\{\bm{\theta}_i\}_{i=1}^N$, where $d = \sum_{i=1}^N d_i$.

\begin{figure*}[t]
    \centering
    \includegraphics[scale=0.35]{Figures/sheafIllustration.png}
    \caption{Schematic illustration of the sheaf-based modeling of FMTL.}
    \label{fig:sheaf_illustration}
\end{figure*}

\subsection{A Sheaf Theoretic Approach of the FMTL Problem}\label{sheaves}
A ``cellular sheaf" $\F$ of $\R$-vector spaces over a simple graph $\G = (\mathcal{V}, \mathcal{E})$, i.e., without loops and multiple edges, consists of the following assignments
\begin{itemize}
    \item For each $i \in \mathcal{V}$, a vector space $\F(i)=\R^{d_i}$ of dimension $d_i$,
    \item for each edge $e=(i,j) \in \mathcal{E}$, a vector space $\F(e)=\R^{d_{ij}}$ of dimension $d_{ij}$, and 
    \item for each edge $e \in \mathcal{E}$ and a vertex $i \in \mathcal{V}$ that is incident to the edge $e$, a linear transformation $\F_{i\trianglelefteq e} : \F(i)\rightarrow \F(e)$.
\end{itemize}
We shall refer to $\F(i)$ and $\F(e)$ as stalks over $i$ and $e$, respectively, and the map $\F_{i\trianglelefteq e}$ as the restriction map from $i$ to $e$. Also, given an edge $e=(i,j)\in \mathcal{E}$, we denote the matrix representation of $\F_{i\trianglelefteq e}$, with respect to a chosen basis such as the standard basis, by $\bm{P}_{ij}$. With an abuse of notation, we use  $\F_{i\trianglelefteq e}$ and $\bm{P}_{ij}$ interchangeably, as required, in the remainder of the paper.

Naturally associated with such a sheaf structure are the dual maps, $\F_{i\trianglelefteq e}^*: \F(e)\rightarrow \F(i)$, of the restriction maps. Note that we are using the identification of the dual of a finite-dimensional vector space with itself. It is a standard fact that the matrix representation of the dual map $\F_{i\trianglelefteq e}^*$ is given by the transpose of $\bm{P}_{ij}$. Similarly, with an abuse of notation, we shall also use $\F_{i\trianglelefteq e}^*$ and $\bm{P}_{ij}^T$ interchangeably. 

For each $i \in \mathcal{V}$, $\F(i)$ is the space in which the local model of client $i$ is parameterized, i.e., $\bm{\theta}_i \in \F(i)$. A choice $\{\bm{\theta}_i\}_{i \in \mathcal{V}}$ of local models lies in the total space $C^0(\F):=\bigoplus_{i \in \mathcal{V}} \F(i)$. Note that, we do not assume $d_i$ and $d_j$ to be the same for $i\neq j$. In particular, different clients can have different model sizes that could arise from feature heterogeneity and/or different learning tasks. Also note that an element of $C^0(\F)$ is not fully observable by a single client, as assumed in the FL setting.  

Therefore, any two models can be compared via the restriction maps, provided they share an edge. More specifically, as can be seen from Figure \ref{fig:sheaf_illustration}, for two clients $i$ and $j$ such that $e=(i,j) \in \mathcal{E}$, $\F(e)$ can be considered as the ``disclose space" in which models $\bm{\theta}_i$ and $\bm{\theta}_j$ are compared via the projections $\F_{i\trianglelefteq e} \left( \bm{\theta}_i \right) = \bm{P}_{ij}$ and $\F_{j\trianglelefteq e} \left( \bm{\theta}_j \right) = \bm{P}_{ji}$. Here, the local models \( \bm{\theta}_i \) are assigned to the vertices (the clients), while the restriction maps \( \bm{P}_{ij} \) project the local models onto the edge space capturing the shared features or relationships between the clients.

We refer the reader to Appendix \ref{appendix:discussion} for an interpretation of this viewpoint in the context of linear models. Given a choice of local models, the overall comparison of these models is done in the total space $C^1(\F):= \bigoplus_{e\in \mathcal{E}} \F(e)$ of the disclose spaces. The total discrepancy of such a choice of local models, as measured by comparing their projections onto the disclose spaces, can be formalized via the Laplacian quadratic form associated with the so-called ``sheaf Laplacian", the analogous to the graph Laplacian. To define the Laplacian in the sheaf setting, we first need to orient the edges and define the ``co-boundary map" $\delta: C^0(\F) \rightarrow C^1(\F)$. 

From now onwards, we shall fix an orientation for each edge and write $e=(i,j)$ for an oriented edge. For such an oriented edge $e=(i,j)$, write $e^+=j$ and $e^-=i$. Our discussion is not subjective to the choice of orientation; however, one can choose a canonical orientation associated with an ordering of the vertices, e.g., when vertices are indexed by numbers, by choosing $e^+=\max\{i,j\}$ and $e^-=\min\{i,j\}$ for an unoriented edge $e=\{i,j\}$. Given such an orientation, the co-boundary map $\delta$ is given as follows. For $\bm{\theta} = (\bm{\theta}_i)_{i \in \mathcal{V}}$, the co-boundary of $\bm{\theta}$,   $\delta\left(\bm{\theta} \right) = \left( \delta\left(\bm{\theta} \right) _e \right)_{e \in \mathcal{E}} \in C^1(\F)$, is defined by 
\begin{align}
\delta\left(\bm{\theta} \right) _e = \F_{e^+\trianglelefteq e}\left( \bm{\theta}_{e^+} \right) - \F_{e^-\trianglelefteq e}\left( \bm{\theta}_{e^-} \right).    
\end{align}
As in the case of restriction maps, one also has the dual $\delta^*: C^1(\F) \rightarrow C^0(\F)$ of the co-boundary map $\delta$. The sheaf Laplacian $L_\F : C^0(\F) \rightarrow C^0(\F)$ associated with the cellular sheaf $\F$ is then given by $L_\F = \delta^* \circ \delta$. For a given $\bm{\theta}$, the sheaf Laplacian is defined as $L_\F\left(\bm{\theta}\right) = \left( L_\F\left(\bm{\theta}\right)_j\right)_{j \in \mathcal{V}}$, where
$L_\F\left(\bm{\theta}\right)_j = \sum_{i \in \mathcal{V}} L_{j,i}\left(\bm{\theta}_i\right)$ is given by 
\begin{align}
\label{eqn: Laplacian_defn}
    L_{j,i} = \left\{
    \begin{array}{ll}
          \sum\limits_{i\trianglelefteq e} \F_{i\trianglelefteq e}^* \circ \F_{i\trianglelefteq e}, & \text{if } i = j, \\
           - \F_{j\trianglelefteq e}^* \circ \F_{i\trianglelefteq e} , & \text{if } e=(i, j) \in \mathcal{E},\\
           \bm{0}, & \text{otherwise.}
         \end{array}
    \right.
\end{align}
In particular, based on the ordering of $\mathcal{V}$ that is used to stack the local models, and the chosen bases for $\F(i)$'s and $\F(e)$'s , $L_\F$ is a block matrix structure indexed by $\mathcal{V}$, whose $(j,i)$ block can be directly obtained from (\ref{eqn: Laplacian_defn}). More specifically, with an abuse of notation, writing $L_{j,i}$ in terms of its matrix representation, we have that
\begin{align}
\label{eqn: Laplacian_matrix_form}
    L_{j,i} = \left\{
    \begin{array}{ll}
          \sum\limits_{j' \in  \mathcal{N}_i } \bm{P}_{ij'}^T\bm{P}_{ij'}, & \text{if } i = j, % \F_{i\trianglelefteq e}^* \circ \F_{i\trianglelefteq e}
          \\
           - \bm{P}_{ji}^T\bm{P}_{ij}, & \text{if } e=(i, j) \in \mathcal{E},\\ %\F_{j\trianglelefteq e}^T \circ \F_{i\trianglelefteq e} 
           \bm{0}, & \text{otherwise.}
         \end{array}
    \right.
\end{align}
In fact, one can write the matrix representation of the co-boundary map, $\bm{P}$, as follows. It has a block structure whose rows are indexed by edges and columns are indexed by vertices. Then, the submatrix $\bm{P}_{e,i}$ that corresponds to row $e\in \mathcal{E}$ and column $i \in \mathcal{V}$ is given by
\begin{align}
\label{eqn: Boundary_matrix_form}
    \bm{P}_{e,i} = \left\{
    \begin{array}{ll}
          \bm{P}_{ij}, & \text{if } e =(i,j) % \F_{i\trianglelefteq e}^* \circ \F_{i\trianglelefteq e}
          \\
           - \bm{P}_{ij}, & \text{if } e=(j, i) ,\\ %\F_{j\trianglelefteq e}^T \circ \F_{i\trianglelefteq e} 
           \bm{0}, & \text{otherwise.}
         \end{array}
    \right.
\end{align}
From the definition $L_\F = \delta^* \circ \delta$, one can get the matrix form of $L_\F$ as $L_\F = \bm{P}^T \bm{P}$. Assuming the matrix representation of $L_\F$, we often write $L_\F\left(\bm{\theta}\right) = L_\F\,\bm{\theta} = \bm{P}^T \bm{P} \bm{\theta}$. Note that this block structure aligns well with the distributed optimization goal of the FMTL setting.

The significance of the sheaf Laplacian $L_\F$ is characterized by the consensus property given by
\begin{align}
    \ker(L_\F) = \left\{ \left(\bm{\theta}_i\right)_{i \in \mathcal{V}} \in C^0(\F) \,|\, \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right)   =  \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right)  \text{  for  } e=(i,j)\in \mathcal{E}\right\}.
\end{align}
In other words, $\ker(L_\F)$ consists of the choices of local models that are in \textit{global consensus} so that any two comparable local models $\bm{\theta}_i$ and $\bm{\theta}_j$ agree when projected onto the disclose space $\F(e)$, where $e=(i,j) \in \mathcal{E}$. Accordingly, the global consensus constraint on $\bm{\theta} \in  C^0(\F)$ is given by $L_\F\, \bm{\theta}  = \bm{0}$.

Similar to that of a graph Laplacian, the sheaf Laplacian quadratic form $Q_\F(\bm{\theta}) = \bm{\theta}^T L_\F\, \bm{\theta}$ quantifies by how much a given $\bm{\theta}$ deviates from the constraint $L_\F\, \bm{\theta}  = \bm{0}$. The following lemma shows that the sheaf Laplacian quadratic form measures the total discrepancy between the projections of the local models onto the edge spaces, summed over all edges in the graph.
\begin{lemma}\label{lemma1}
\begin{align}
    \bm{\theta}^T L_\F \, \bm{\theta} = \sum_{e=(i,j) \in \mathcal{E}} \left\lVert \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right)  -  \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right) \right\rVert^2 = \sum_{e=(i,j) \in \mathcal{E}} \left\| \bm{P}_{ij} \bm{\theta}_{i} - \bm{P}_{ji} \bm{\theta}_{j} \right\|^2.
\end{align}    
\end{lemma}
\begin{proof}
The details of the proof are deferred to Appendix \ref{suppLemmas}.
\end{proof}
Next, we show that $\bm{\theta}$ being a global section, i.e., $\bm{\theta} \in \ker(L_\F)$, is equivalent to $\bm{\theta}$ minimizing the sheaf Laplacian quadratic form.
\begin{lemma}
\begin{align}
\ker(L_\F) = \argmin_{\bm{\theta} \in  C^0(\F) }  Q_\F(\bm{\theta}) = \argmin_{\bm{\theta}\in  C^0(\F) } \bm{\theta}^T L_\F\, \bm{\theta}.    
\end{align}    
\end{lemma}
\begin{proof}
 The proof is provided in Appendix \ref{suppLemmas}.
\end{proof}
In the context of FMTL, a cellular sheaf is a structured way to assign information to each client (node) and their connections (edges) in a network as follows
\begin{itemize}
\item Nodes (Clients): Each client $i$ has its own local model $\bm{\theta}_i \in \mathbb{R}^{d_i}$.
\item Edges (Interaction space): Each connection between clients $i$ and $j$ has a shared space $\mathcal{F}(e) \in \mathbb{R}^{d_{ij}}$.
\item Restriction Maps: The mappings $\bm{P}_{ij}: \mathbb{R}^{d_i} \rightarrow \mathbb{R}^{d_{ij}}$ project the local model of client $i$ into the shared space with client $j$. This projection facilitates meaningful comparisons and collaborations between clients, ensuring that heterogeneous models can still interact effectively within the network.
\end{itemize} 
This setup allows us to systematically ensure that the models of connected clients align within their shared interaction spaces, promoting consistency and cooperation across the network. To formulate the FMTL optimization problem, we aim to minimize the combined objectives of individual client losses and a regularization term that enforces consistency across client models. Specifically, each client seeks to minimize its own loss function based on its local data, while the regularization term penalizes discrepancies between connected clients in their shared interaction spaces. Building upon this, we can express the regularization term more succinctly using the sheaf Laplacian matrix \( L_{\mathcal{F}}(\bm{P}) \). This matrix encapsulates the structural relationships and shared interaction spaces between clients, allowing us to reformulate the optimization problem in a compact and mathematically elegant manner as follows
\begin{align}\label{P2}
    \min_{\bm{\theta}, \bm{P} } \Psi(\bm{\theta}, \bm{P}) = f(\bm{\theta}) + \frac{\lambda}{2} \bm{\theta}^T L_{\mathcal{F}}(\bm{P}) \bm{\theta},
\end{align}
where $f(\bm{\theta}) = \sum_{i=1}^N f_i(\bm{\theta}_i)$ and $L_{\mathcal{F}}(\bm{P})$ is the sheaf Laplacian for the choices of the restriction maps. The sheaf Laplacian regularization term \( \frac{\lambda}{2} \bm{\theta}^T L_{\mathcal{F}}(\bm{P}) \bm{\theta} \) enforces consistency between the projections of the local models onto the edge space, promoting collaboration among the clients. A more in-depth discussion on the rationale behind using Sheaf theory to model clients' interaction in the context of FMTL can be found in Appendix \ref{app:sheaf_theory}. In the next subsection, we show that our proposed framework is very general and covers many special cases previously introduced in the literature.
\subsection{Special Cases}\label{appendix:specialcases}
In this section, we show how some previous works can be considered as special cases of our framework. Most of the works that we refer to, except \citep{zhang2022hybrid}, consider the same model size for all the clients. Thus, unless otherwise stated, we assume in the rest of this subsection that all the local models are assumed to have the same size $p \in \mathbb{N}$, i.e., $\forall i \in \mathcal{V}, \bm{\theta}_i \in \mathbb{R}^p$. In particular, we chose $\forall i \in \mathcal{V}$ and $ \forall e\in \mathcal{E}$, $\F(i) = \F(e) = \mathbb{R}^{p}$. 
\iffalse
In Table \ref{table:specialcases}, we summarize the connections of our framework to state-of-the-art FL approaches.

\begin{table}[H]
\centering
\caption{
  Connection of the {\ours} framework to state-of-the-art FL.
  }
\label{table:specialcases} 
\vspace{0.5em}

\begin{tabular}{@{}lc@{}}
\toprule
\textbf{FL Framework} & \multicolumn{1}{c}{\textbf{Restriction Maps Choice}}   \\ \midrule
Conventional FMTL & $\bm{P}_{ij} = \sqrt{a_{ij}} \bm{I}_{p}$   \\
Conventional FL & $\bm{P}_{ij} = \bm{I}_{p}$  \\
Personalized FL & \multicolumn{1}{l}{$\bm{P}_{0i} = \bm{P}_{i0} =  \bm{I}_p$, $\bm{P}_{ij} = \bm{0}_p$}   \\
Hybrid FL &  $\bm{P}_{0i} = \bm{\Pi}_i, \bm{P}_{i0}=\bm{I}_{d_i}, \bm{P}_{ij} = \bm{0}_p$  \\ \bottomrule
\end{tabular}
\end{table}
Next, we show in detail how the choices of the sheaf maps, $\bm{P}_{ij}$, made in Table \ref{table:specialcases} recover some of the existing FL frameworks.
\fi
\\
\textbf{Connection with conventional FMTL \citep{dinh2022new}.} 
In conventional FMTL, we aim to solve the problem
\begin{align}
    \min_{\{\bm{\theta}_i\}_{i \in \mathcal{V}}} \sum_{i=1}^N f_i(\bm{\theta}_i) + \frac{\lambda}{2} \sum_{i=1}^N \sum_{j \in \mathcal{N}_i} a_{ij} \left\|  \bm{\theta}_{i} - \bm{\theta}_{j} \right\|^2, \label{eqn: conventional FMTL objective}
\end{align}
where the weights $\{a_{ij}\}$ are assumed to be known in advance. This problem is a special case associated with the sheaf $\F$ arising by choosing $\F_{i \trianglelefteq e} = \bm{P}_{ij} = \sqrt{a_{ij}} \bm{I}_{p}$, where $\bm{I}_{p}$ is the $p \times p$ identity map/matrix. The dimension of the projection space is $d_{ij} = p$ for all edges $(i, j)$. For this particular choice of the sheaf $\F$, the associated Laplacian quadratic form is precisely 
    \begin{align}
     Q_{\F}\left( \bm{\theta} \right)= \bm{\theta}^T {L}_{\F} \, \bm{\theta} = \sum_{i,j \triangleleft e} a_{ij}\left\lVert  \bm{\theta}_i - \bm{\theta}_j \right \rVert^2 = \sum_{(i,j) \in \mathcal{E}} a_{ij} \left\| \bm{\theta}_i - \bm{\theta}_j \right\|^2.  
    \end{align} 
Replacing this into (\ref{P1}), we get the conventional FMTL problem (\ref{eqn: conventional FMTL objective}).\\
\textbf{Connection with conventional FL \citep{ye2020decentralized}.} 
Setting the restriction maps to be $\bm{P}_{ij} = \bm{I}_{p}$ and $d_{ij} = p, ~\forall (i,j) \in [N] \times [N]$. Then, the sheaf Laplacian regularization is given by
\begin{align}
    Q_\mathcal{F}(\bm{\theta}) = \sum_{(i,j) \in \mathcal{E}} \left\| \bm{\theta}_i - \bm{\theta}_j \right\|^2.
\end{align}
Hence, (\ref{P1}) reduces to
\begin{align}
    \min_{\{\bm{\theta}_i\}_{i \in \mathcal{V}}} \sum_{i=1}^N f_i(\bm{\theta}_i) + \frac{\lambda}{2} \sum_{(i,j) \in \mathcal{E}} \left\| \bm{\theta}_i - \bm{\theta}_j \right\|^2
\end{align}
Taking $\lambda \rightarrow \infty$, as pointed out in Remark \ref{remark1}, one recovers the conventional FL problem
\begin{align}
    \nonumber &\min_{\{\bm{\theta}_i\}_{i \in \mathcal{V}}} \sum_{i=1}^N f_i(\bm{\theta}_i) \\
    &~\text{s.t } \bm{\theta}_i = \bm{\theta}_j, \forall (i,j) \in \mathcal{E}.
\end{align}
\textbf{Connection with personalized FL \citep{hanzely2020lower}.} Introducing the client $0$, e.g., a server, where $f_0 \triangleq 0$ and $\bm{\theta}_0 = \bm{\bar{\theta}}$. Furthermore, let $\bm{P}_{0i} = \bm{P}_{i0} =  \bm{I}_p, ~\forall i \in [N]$, and  $\bm{P}_{ij} = \bm{0}_p, ~\forall (i,j) \in [N] \times [N]$, where $\bm{0}_p$ is the $p \times p$ zero map/matrix. Hence, the set of neighbours of client $0$ is $\mathcal{N}_0 = [N]$, and the set of each client $i \in [N]$ is $\mathcal{N}_i = \{0\}$. We observe that this amounts to choosing the constant sheaf over the graph that connects each client to the server. Then, the associated Laplacian quadratic form can be written as
\begin{align}
Q_{\F}\left( \bm{\theta} \right) =  \sum_{i \in \mathcal{N}_0} \left\| \bm{P}_{0i} \bm{\theta}_{0} - \bm{P}_{i0} \bm{\theta}_{i} \right\|^2 =  \sum_{i=1}^N \left\|\bm{\bar{\theta}} - \bm{\theta}_{i} \right\|^2.  
\end{align}
Therefore, (\ref{P1}) reduces to the personalized FL objective \citep[Eq. (2)]{hanzely2020lower}
\begin{align}
    \min_{\{\bm{\theta}_i\}_{i \in \mathcal{V}}} \sum_{i=1}^N f_i(\bm{\theta}_i) + \frac{\lambda}{2} \sum_{i=1}^N  \left\|  \bm{\theta}_{i} - \bar{\bm{\theta}} \right\|^2.
\end{align}\\
\textbf{Connection with hybrid FL \citep{zhang2022hybrid}.}
In this case,  a communication framework is established between a server (with index $0$) and a set of clients ($i \in [N]$), with each client connected solely to the server. The server has access to all features, while clients are constrained by their local features. Hence, for each client $i$, $d_i \leq d_0$, where $\bm{\theta}_i \in \mathbb{R}^{d_i}$ and $\bm{\theta}_0 \in \mathbb{R}^{d_0}$ are the models of client $i$ and the server, respectively. Let $\bm{\Pi}_i$ denote binary matrices that prune the server model to align with the client local model, referred to as the \emph{selection matrices}. Given the above description, we have $\F(i)=\mathbb{R}^{d_i}$ and $\F(e)=\mathbb{R}^{d_i}$ for every $i \in [N]$ and edge of the form $e=(i,0)$. The associated restriction maps are $ \bm{P}_{0i} = \bm{\Pi}_i $ and $\bm{P}_{i0}=\bm{I}_{d_i}$, for $i\in [N]$ and $\bm{P}_{ij} = \bm{0}_p, ~\forall (i,j) \in [N] \times [N]$. With these choices, the Laplacian quadratic form of this sheaf is equal to the regularizer term  
\begin{align}
\label{eqn: regularizer_2_of_zang}
     Q_{\F}\left(\bm{\theta}, \bm{\Pi} \right) = \sum_{i=1}^N \left\lVert \bm{\theta}_i - \bm{\Pi}_i \bm{\theta}_0 \right\rVert^2.
\end{align}
Replacing this into (\ref{P1}), we get the hybrid FL objective \citep[Eq. (6) given $\mu_1 = 0$]{zhang2022hybrid}.
\subsection{Proposed Algorithm \& Convergence Analysis} \label{algorithm}
Note that problem (\ref{P2}) arising from the sheaf formulation can be re-written as follows
\begin{align}\label{P1}
    \min_{ \substack{\{\bm{\theta}_i\}_{i \in \mathcal{V}}, \\ \{\bm{P}_{ij}\}_{(i,j) \in \mathcal{E}}}} \sum_{i=1}^N f_i(\bm{\theta}_i) + \frac{\lambda}{2} \sum_{i=1}^N \sum_{j \in \mathcal{N}_i} \left\| \bm{P}_{ij} \bm{\theta}_{i} - \bm{P}_{ji} \bm{\theta}_{j} \right\|^2,
\end{align} 
where $\|\cdot\|$ is the Euclidean norm, and $\forall (i,j) \in \mathcal{E}$, $\bm{P}_{ij}$ is a matrix with size $(d_{ij}, d_i)$ such that $d_{ij} = d_{ji}$. In (\ref{P1}), we propose to jointly learn the models $\{\bm{\theta}_i\}_{i \in \mathcal{V}}$ and the matrices $\{\bm{P}_{ij}\}_{(i,j) \in \mathcal{E}}$. The matrix $\bm{P}_{ij}$ can be seen as an encoding or compression matrix since it maps the higher-dimensional vector $\bm{\theta}_i$ to a lower-dimensional space with dimension $d_{ij}$, effectively retaining only the most important features or information shared between the two clients $i$ and $j$. Hence, the term $(\bm{P}_{ij} \bm{\theta}_i - \bm{P}_{ji} \bm{\theta}_j)$ captures the dissimilarity or discrepancy between the two vectors $\bm{\theta}_i$ and $\bm{\theta}_j$ in this shared subspace. %More discussion on the interpretation of the role of $\bm{P}_{ij}$ and the choice of $d_{ij}$ in the case of linear models can be found in Appendix \ref{appendix:discussion}.
\begin{remark} \label{remark1}
In (\ref{P1}), the hyperparameter $\lambda$ controls the impact of the models of neighboring clients on each local model. When $\lambda > 0$, the minimization of the regularization term promotes the proximity among the models of neighboring clients. On the other hand, if $\lambda = 0$, (\ref{P1}) reduces to an individual learning problem, wherein each client independently learns its local model $\bm{\theta}_i$ solely from its local data, without engaging in any collaborative efforts with the other clients. Finally, as $\lambda \rightarrow \infty$, (\ref{P1}) boils down to the classical FL problem where the aim is to learn a global model \citep{hanzely2020federated}. 
\end{remark}

Next, we propose a communication-efficient algorithm to solve (\ref{P1}) by adopting an iterative optimization approach. Since the objective function is assumed to be differentiable, we can use gradient-based optimization methods. More specifically, we will use alternating gradient descent (AGD) updates for $\{\bm{\theta}_i\}_{i \in [N]}$ and $\{\bm{P}_{ij}\}_{(i,j) \in \mathcal{E}}$, respectively. At iteration $(k+1)$, client $i$ sends $\bm{P}_{ij}^{k} \bm{\theta}_i^{k}$ and receives $\{\bm{P}_{ji}^{k} \bm{\theta}_j^{k}\}_{j \in \mathcal{N}_i}$ from its neighbours, to update its model $\bm{\theta}_i$, using one gradient descent step
\begin{align}\label{thetaupdate}
    \bm{\theta}_i^{k+1}\!=\!\bm{\theta}_i^{k}\!-\!\alpha \left(\nabla f_i(\bm{\theta}_i^k)\!+\!\lambda \sum_{j \in \mathcal{N}_i} (\bm{P}_{ij}^k)^T ( \bm{P}_{ij}^k \bm{\theta}_i^{k}\!-\!\bm{P}_{ji}^k \bm{\theta}_j^{k}) \right).
\end{align}
Then, client $i$ sends $\bm{P}_{ij}^{k} \bm{\theta}_i^{k+1}$ and receives $\{\bm{P}_{ji}^{k} \bm{\theta}_j^{k+1}\}_{j \in \mathcal{N}_i}$ from its neighbours, to be able to update its matrices $\{\bm{P}_{ij}\}_{j \in \mathcal{N}_i}$, using one gradient descent step, according to
\begin{align}\label{pijaupdate}
    \bm{P}_{ij}^{k+1}\!=\!\bm{P}_{ij}^{k}\!-\! \eta \lambda  (\bm{P}_{ij}^{k} \bm{\theta}_i^{k+1} \!-\!\bm{P}_{ji}^{k} \bm{\theta}_j^{k+1}) (\bm{\theta}_i^{k+1})^T,
\end{align}
where $\alpha$ and $\eta$ are two learning rates. Note that, in our proposed algorithm, neighbouring clients only share vectors and no matrix exchange is needed in both updates (\ref{thetaupdate}) and (\ref{pijaupdate}). Our proposed method is summarized in Algorithm \ref{algo}.
\begin{remark}
Note that each neighbour of the node $i$ is required to send the vector $\bm{P}_{ji} \bm{\theta}_j$ in order to update $\bm{\theta}_i$ and $\bm{P}_{ij}$ as per (\ref{thetaupdate}) and (\ref{pijaupdate}). The dimension of $\bm{P}_{ji} \bm{\theta}_j$ is $d_{ij}$, which in practice could be much smaller than $d_j$, the size of $\bm{\theta}_j$. For example, a reasonable choice of $d_{ij}$ is $d_{ij} = \min(d_i, d_j)$. Hence, our proposed algorithm is more communication-efficient than sending the models $\{\bm{\theta}_i\}_{i \in \mathcal{V}}$.
\end{remark}

\begin{algorithm}[tb]
   \caption{Sheaf-based Federated Multi-Task Learning ({\ours})}
   \label{algo}
   \begin{algorithmic}[1]
      \State \textbf{Parameters:} Number of clients $N$, number of iterations $K$, learning rates $(\alpha, \eta)$, regularization parameter $\lambda$.
      \State \textbf{Initialization:} Initial models $\{\bm{\theta}_i^0\}_{i=1}^N$, initial matrices $\{\bm{P}_{ij}^0\}_{(i,j) \in \mathcal{E}}$.
      \For{$k = 0, \dots, K$}
         \For{$i = 1, \dots, N$ \textbf{in parallel}}
            \State $\triangleright$ Sends $\bm{P}_{ij}^{k} \bm{\theta}_i^{k}$ and receives $\bm{P}_{ji}^{k} \bm{\theta}_j^{k}$ from neighbors $j \in \mathcal{N}_i$
            \State $\triangleright$ Updates its model:
            \[
              \bm{\theta}_i^{k+1} = \bm{\theta}_i^{k} - \alpha \left(\nabla f_i(\bm{\theta}_i^k) + \lambda \sum_{j \in \mathcal{N}_i} (\bm{P}_{ij}^k)^T ( \bm{P}_{ij}^k \bm{\theta}_i^{k} - \bm{P}_{ji}^k \bm{\theta}_j^{k}) \right)
            \]
            \State $\triangleright$ Sends $\bm{P}_{ij}^{k} \bm{\theta}_i^{k+1}$ and receives $\bm{P}_{ji}^{k} \bm{\theta}_j^{k+1}$ from neighbors $j \in \mathcal{N}_i$
            \State $\triangleright$ Updates its matrix:
            \[
              \bm{P}_{ij}^{k+1} = \bm{P}_{ij}^{k} - \eta \lambda (\bm{P}_{ij}^{k} \bm{\theta}_i^{k+1} - \bm{P}_{ji}^{k} \bm{\theta}_j^{k+1}) (\bm{\theta}_i^{k+1})^T
            \]
         \EndFor
      \EndFor
   \end{algorithmic}
\end{algorithm}
Next, we turn to analyzing the convergence of {\ours}. To this end, we start by making the following standard assumptions.

\textbf{Assumption 1 (Smoothness).} $\forall i \in [N]$, the function $f_i$ is assumed to be $L$-smooth, i.e., there exists $L > 0$ such that $\forall i \in [N]$, $\forall \bm{\theta}_1, \bm{\theta}_2$, $\|\nabla f_i(\bm{\theta}_2) - \nabla f_i(\bm{\theta}_1)\| \leq L \|\bm{\theta}_2-\bm{\theta}_1\|$.

\textbf{Assumption 2 (Bounded domain).} There exists $D_{\theta} > 0$ such that $\|\bm{\theta}\| \leq D_{\theta}$.

Assumptions 1-2 are key assumptions that are often used in the context of distributed optimization \citep{karimireddy2020scaffold, li2020federated, deng2020distributionally, deng2024distributed}. In particular, Assumption 2 is commonly used in the convex-concave minimax literature \citep{deng2020distributionally, deng2024distributed}. The following theorem establishes the convergence rate of the {\ours} algorithm.
\begin{theorem}\label{theorem}
Let Assumptions 1 and 2 hold, and the learning rates $\alpha$ and $\eta$ satisfy the conditions $\alpha < \frac{2}{N L}$ and $\eta < \frac{2}{\lambda D_\theta^2}$, respectively. Then, the averaged gradient norm is upper bounded as follows
\begin{align}
\frac{1}{K}\sum_{k=0}^{K-1} \|\nabla \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 \leq \frac{1}{\rho K} (\Psi(\bm{\theta}^{0}, \bm{P}^0) - \Psi^\star),
\end{align}
where $\rho = \min\left\{\alpha \left(1 - \frac{\alpha N L}{2}\right), \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2} \right) \right\}$ and $\Psi^\star$ is the optimal value of $\Psi$.
\end{theorem}
\begin{proof}
The proof can be found in Appendix \ref{theorem_proof}.
\end{proof}
Theorem \ref{theorem} shows that the {\ours} algorithm converges to a stationary point of the objective function at a rate of $\mathcal{O}(1/K)$. The proof involves two main steps. First, we study the descent step in the variable $\bm{\theta}^{k+1}$ and then, we study the descent step in the variable $\bm{P}^{k+1}$.
\section{Experiments} \label{experiments} 
\subsection{Experimental Setup} \label{experimentalsetup}
To validate our theoretical foundations, we numerically evaluate the performance of our proposed algorithm {\ours} using two experiments: (i) the clients have the same model size in Section \ref{experiment1}, and (ii) the clients have different model sizes in Section \ref{experiment2}. \\
\textbf{Datasets.} In the first experiment, we examine two datasets: Rotated MNIST and Heterogeneous CIFAR-10. A detailed description of the datasets can be found in Appendix \ref{appendix:dataset}. In appendix \ref{appendix:moreResults}, we report additional results using four more datasets. In the second experiment, we used modified versions of the Vehicle and School datasets by randomly dropping features of the local datasets to make the model size different across clients.  More details on the generation of these two datasets can be found in Appendix \ref{appendix:moredetails}.

\begin{figure}
\centering
% Row 1
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/rotatedMNIST.pdf}
  %\caption{Vehicle dataset}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/cifar10.pdf}
  %\caption{GLEAM dataset}
\end{subfigure}%
% Add a caption for the entire second row
\caption{Test accuracy as a function of the number of communication rounds and the number of transmitted bits for the Rotated MNIST dataset (top), and the Heterogenous CIFAR-10 dataset (bottom).}
\label{har_school_datasets}
\end{figure}
\textbf{Baselines.} In the first experiment, we compare to the dFedU algorithm \citep{dinh2022new}. We implement our proposed algorithm using a mini-batch stochastic gradient in (\ref{thetaupdate}) to make the comparison fair. In the second one, we compare to a stand-alone baseline where each client trains on each local dataset without communicating with the rest of the clients. To the best of our knowledge, {\ours} is the only algorithm solving the FMTL problem over decentralized topology with the clients having different model sizes, hence the comparison to a stand-alone baseline in the second experiment. More details on the experimental Settings can be found in Appendix \ref{appendix:moredetails}.
\subsection{Experiment 1: same model size}\label{experiment1}
Figure \ref{har_school_datasets} illustrates the performance of the proposed {\ours} algorithm and dFedU on the Rotated MNIST and Heterogeneous CIFAR-10 datasets, respectively, showcasing the test accuracy as a function of the number of communication rounds and the total number of transmitted bits. We consider two values for $\gamma = \{0.01, 0.03\}$ such that the projection space dimension is $\gamma d$. In Figure \ref{har_school_datasets}(a), we can see that {\ours} manages to achieve similar test accuracies as dFedU. On the other hand, Figure \ref{har_school_datasets}(b) shows that {\ours} achieves higher test accuracy with fewer transmitted bits compared to the baseline, demonstrating its ability to learn effectively while minimizing communication overhead. For the Rotated MNIST dataset, using $\gamma = 0.01$ leads to almost similar test accuracy as dFedU, while it requires $100 \times$ less in terms of the number of transmitted bits to achieve this accuracy. For the Heterogeneous CIFAR-10, {\ours} requires slightly more communication rounds than dFedU but the benefit in terms of communication overhead is evident as it requires exchanging significantly less number of bits.

As illustrated in Table \ref{table:costs}, {\ours} incurs additional storage and computational costs due to the maintenance and training of restriction maps. Specifically, each restriction map requires storing a matrix of size $d_{ij} \times d_i$, leading to a cumulative storage requirement of $\mathcal{O}(|\mathcal{E}| \times d_{ij} \times d_i)$ across the network. Computationally, updating these maps involves matrix multiplications and gradient calculations, adding a complexity of $\mathcal{O}( d_{ij} \times d_i)$ per edge per iteration. However, these costs are significantly offset by the substantial communication savings achieved, particularly when $d_{ij}$ is chosen to be a small fraction of $d_i$, making {\ours} a viable option in resource-rich FL environments such as cross-silo FL settings. %For a more detailed analysis of the scalability of {\ours} with more complex data and larger parameter spaces, including neural networks, as well as strategies to mitigate computational and storage overheads, please refer to Appendix \ref{appendix:scalability}. 

Next, we compare the performance of our approach to three state-of-the-art PFL algorithms: DITTO, pFedMe, and Per-FedAvg. These algorithms are well-regarded in the literature for addressing heterogeneity in FL environments. We conducted additional experiments on two datasetsVehicle Sensor and HARto evaluate the performance of our method against these baselines. The test accuracy is averaged over 5 runs reporting both mean and standard deviation and the results are summarized in Table \ref{table:new}. The results in the table demonstrate that our proposed algorithm consistently achieves the highest test accuracy, comparable to or exceeding that of dFedU, while significantly reducing the number of transmitted bits. As expected, the PFL baselines (DITTO, pFedMe, Per-FedAvg) achieve lower communication overheads due to their star topology, leveraging a central parameter server.
\subsection{Experiment 2: different model sizes}\label{experiment2}
Figure \ref{fig:experiment2} compares the performance of the proposed {\ours} algorithm with the local training, i.e., training each model independently without communication with other clients, on two modified versions of the Vehicle and School datasets by plotting the test accuracy as a function of the number of iterations. We can see that {\ours} demonstrates a clear advantage over the local training for both datasets. This observation highlights the effectiveness of {\ours} in leveraging the shared information across clients while preserving the individual characteristics of their local models, a key feature of FL. By utilizing the sheaf structure to capture the relationships between the local models, {\ours} enables more efficient and accurate learning than the local approach when the model size is different across clients.

\begin{table}[h]
\centering
\caption{
  Comparative analysis of storage, computational overheads, and communication costs.
  }
\label{table:costs} 
\vspace{0.5em}
\begin{tabular}{@{}lccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}} \textbf{Method}\\ \textbf{ } \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Storage}\\ \textbf{per client}\end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Compute per }\\ \textbf{iteration per client}\end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Communication per }\\ \textbf{iteration per client}\end{tabular}  \\ \midrule
{\ours} & $d_i + \sum\limits_{j \in \mathcal{N}_i} d_{ij} \times d_i$ & $\mathcal{O}(d_i \times d_{ij})$ & $\sum\limits_{j \in \mathcal{N}_i} d_{ij}$  \\ \midrule
dFedU &  $d_i$ & $\mathcal{O}(d_i)$ & $\sum\limits_{j \in \mathcal{N}_i} d_{i}$ \\ \bottomrule
\end{tabular}
\end{table}

\begin{figure}
\centering
\includegraphics[scale=0.3]{Figures/fig2.pdf}
\caption{Test accuracy/MSE as a function of the number of iterations for  (a) Modified Vehicle dataset, and (b) Modified School dataset.}
\label{fig:experiment2}
\end{figure}


\begin{table}[h]
    \centering
        \caption{Performance of {\ours} compared to baselines for the Vehicle Sensor and HAR datasets.}
    \label{table:new}
    \vspace{0.5em}
    \begin{tabular}{@{}cccc@{}}
        \toprule
        \textbf{Dataset} & \textbf{Algorithm} & \textbf{Test Accuracy} & \textbf{Transmitted Bits (kB)} \\
        \midrule
        Vehicle Sensor & Sheaf-FMTL ($\gamma$ = 0.1) & 87.6  0.61 & 224 \\
        \cline{2-4}
        & Sheaf-FMTL ($\gamma$ = 0.3) & 87.6  0.52 & 672 \\
        \cline{2-4}
        & dFedU & 87.52  0.53 & 2262.4 \\
        \cline{2-4}
        & DITTO & 84.17  1.66 & 161.6 \\
        \cline{2-4}
        & pFedme & 84.82  0.65 & 161.6 \\
        \cline{2-4}
        & Per-FedAvg & 81.85  1.16 & 161.6 \\
        \midrule
        HAR & Sheaf-FMTL ($\gamma$ = 0.1) & 92.49  1.21 & 5392 \\
        \cline{2-4}
        & Sheaf-FMTL ($\gamma$ = 0.3) & 92.82  0.51 & 16176 \\
        \cline{2-4}
        & dFedU & 93.29  0.69 & 53952 \\
        \cline{2-4}
        & DITTO & 91.86  1.09 & 2697.6 \\
        \cline{2-4}
        & pFedme & 91.63  1.15 & 2697.6 \\
        \cline{2-4}
        & Per-FedAvg & 89.73  0.78 & 2697.6 \\
       \bottomrule
    \end{tabular}
\end{table}

\section{Conclusion} \label{conclusion}
In this work, we introduced a novel sheaf-based framework for federated multi-task learning that effectively tackles challenges arising from data and sample heterogeneity across clients. By leveraging cellular sheaves, our approach can flexibly model complex interactions between client models, even in the presence of varying feature spaces and model sizes. The proposed {\ours} algorithm is communication-efficient, preserves client privacy, and provides a unified view subsuming various existing FL methods. Theoretically, we analyzed the convergence properties of {\ours}, establishing a sublinear convergence rate in line with state-of-the-art decentralized FMTL algorithms. Empirically, extensive experiments on benchmark datasets demonstrated the communication savings of {\ours} compared to the baseline dFedU.




\bibliography{main}
\bibliographystyle{tmlr}

\appendix
\section{Sheaf-Theoretic Approach in FMTL}
\label{app:sheaf_theory}
\subsection{Rationale for Adopting Sheaf Theory in FMTL}
\label{subsec:rationale_sheaf}
Sheaf theory provides a powerful mathematical framework for modelling and analyzing complex relationships in FMTL. The adoption of this approach in our context is motivated by several key advantages
\begin{enumerate}
    \item \textbf{Heterogeneity modeling.} FMTL often involves clients with different data distributions, model architectures, or task objectives. Sheaf theory allows us to capture these heterogeneous relationships in a structured and mathematically rigorous manner.
    \item \textbf{Local-Global consistency.} Sheaves provide a natural way to ensure consistency between local (client-specific) and global (network-wide) information. This is crucial in FMTL scenarios where we aim to leverage network information to improve local performance. 
    \item \textbf{Flexible representation.} The sheaf structure allows for representing varying degrees of similarity or difference between clients. This nuanced representation is more sophisticated than traditional approaches that often assume uniform relationships across the network.
\end{enumerate}
\subsection{The Interaction Space and Client Relationships}
\label{subsec:interaction_space}
The interaction space, denoted as $\mathcal{F}(e)$, plays a central role in our sheaf-theoretic approach to FMTL. It serves as a shared space where local models $\bm{\theta}_i$ and $\bm{\theta}_j$ are projected using restriction maps $\bm{P}_{ij}$ and $\bm{P}_{ji}$, respectively. The projection into this interaction space provides a measure of client relationships for the following reasons
\begin{enumerate}
    \item \textbf{Common feature capture.} The interaction space captures the common or comparable features between clients, analogous to how principal component analysis (PCA) captures the most important features of a dataset.
    \item \textbf{Consistency enforcement.} Our approach enforces consistency between the projections of local models onto the interaction space. This is mathematically represented by the sheaf Laplacian term $\frac{\lambda}{2} \bm{\theta}^T L_{\mathcal{F}}(\bm{P}) \bm{\theta}$, which penalizes discrepancies between the projections of local models.
    \item \textbf{Collaboration encouragement.} By minimizing the sheaf Laplacian term, local models are encouraged to collaborate effectively, leveraging shared information to improve overall performance.
\end{enumerate}
\subsection{Restriction Maps and Their Interpretations}
\label{subsec:restriction_maps}
The restriction maps, represented by matrices $\bm{P}_{ij}$, are fundamental to our sheaf-theoretic approach. These maps project local models $\bm{\theta}_i$ onto the interaction space $\mathcal{F}(e)$. The intuition behind these maps can be understood as follows
\begin{enumerate}
    \item \textbf{Feature selection.} $\bm{P}_{ij}$ acts as a feature selection matrix, identifying common or comparable features between clients $i$ and $j$.
    \item \textbf{Information sharing.} The restriction maps facilitate information sharing between clients by projecting local models onto a common space, enabling effective collaboration even when local models have different dimensions or feature sets.
    \item \textbf{Model comparison.} In heterogeneous settings where clients have different model sizes, traditional FL methods relying on direct model aggregation or comparison fail. The restriction maps allow for meaningful comparisons by projecting onto a common space.
\end{enumerate}
\subsection{Dimensional Considerations and Trade-offs}
\label{subsec:dimensions}
The dimensions of the restriction map $\bm{P}_{ij}$ are determined by the dimensions of the local model $\bm{\theta}_i$ and the interaction space $\mathcal{F}(e)$. If $\bm{\theta}_i \in \mathbb{R}^{d_i}$ and the interaction space has dimension $d_{ij}$, then $\bm{P}_{ij}$ is of size $d_{ij} \times d_i$. The choice of $d_{ij}$ involves a trade-off
\begin{itemize}
    \item \textbf{Smaller $d_{ij}$:} Results in a more compact representation of shared information, leading to communication savings.
    \item \textbf{Larger $d_{ij}$:} Allows for more flexibility in capturing relationships between local models but increases communication costs.
\end{itemize}
In practice, the choice of $d_{ij}$ can be guided by factors such as the estimated overlap in feature spaces between clients, computational resources available, and the desired balance between model expressiveness and communication efficiency.
\subsection{Comparative Advantages over Traditional FMTL Methods}
\label{subsec:advantages}
Our sheaf-theoretic approach offers several advantages over traditional FMTL methods
\begin{enumerate}
    \item \textbf{Heterogeneity handling.} Unlike many traditional FMTL methods that assume homogeneous models across clients, our approach naturally accommodates heterogeneous model architectures and task objectives.
    \item \textbf{Nuanced relationships.} Traditional methods often assume uniform relationships between clients. Our approach allows for more nuanced modelling of inter-client relationships through the interaction space and restriction maps.
    \item \textbf{Privacy preservation.} By working in the interaction space rather than directly sharing model parameters, our method potentially offers enhanced privacy compared to traditional FMTL approaches.
\end{enumerate}
\section{Interpretation of $d_{ij}$ and $\textit{\textbf{P}}_{ij}$} \label{appendix:discussion}
In this appendix, we provide an interpretation of the restriction maps $\bm{P}_{ij} = \mathcal{F}_{i \trianglelefteq e}$ for the case when the local models are given by linear or logistic regression. We describe how to choose $d_{ij}$ and $\bm{P}_{ij}$ in a meaningful way and the constraints that can be imposed on them.

Consider a network of $N$ clients, where each client $i$ has a local model parameterized by $\bm{\theta}_i \in \mathbb{R}^{d_i}$. In the case of linear regression, the local model of client $i$ is given by $f_i(\bm{\theta}_i; \bm{x}_i) = \bm{\theta}_i^T \bm{x}_i$, where $\bm{x}_i \in \mathbb{R}^{d_i}$ is the input feature vector. For logistic regression, the local model is given by $f_i(\bm{\theta}_i; \bm{x}_i) = \sigma(\bm{\theta}_i^T \bm{x}_i)$, where $\sigma(\cdot)$ is the sigmoid function.

\textbf{Interpretation of $\bm{P}_{ij}$.} Consider two clients $i, j \in \mathcal{V}$ such that $e = (i, j) \in \mathcal{E}$. The restriction maps $\bm{P}_{ij}$ and $\bm{P}_{ji}$ aim to capture the relationships between the local models $\bm{\theta}_i$ and $\bm{\theta}_j$ by projecting them to a common interaction space $\mathcal{F}(e)$. In the context of linear or logistic regression, $\bm{P}_{ij}$ and $\bm{P}_{ji}$ can be interpreted as feature selection matrices that identify the common or comparable features between the two clients.

Let $\bm{P}_{ij} \in \mathbb{R}^{d{ij} \times d_i}$ and $\bm{P}_{ji} \in \mathbb{R}^{d{ij} \times d_j}$ be the restriction maps for clients $i$ and $j$, respectively, where $d_{ij} = \dim \mathcal{F}(e)$ is the dimension of the interaction space. The restriction maps should satisfy the following condition
\begin{align}
\bm{P}_{ij} \bm{\theta}_i \approx \bm{P}_{ji} \bm{\theta}_j,
\label{eqn: projection of models}
\end{align}
which ensures that the projected models in the interaction space are comparable.

To impose the condition in (\ref{eqn: projection of models}), we consider the following regularizer term
\begin{align}
Q_{ij} = \left\lVert \bm{P}_{ij} \bm{\theta}_i - \bm{P}_{ji} \bm{\theta}_j \right\rVert^2,
\label{eqn: regularizer}
\end{align}
which we aim to minimize. By adding the regularizer terms for all neighboring clients and then summing over all clients, we obtain the overall regularizer
\begin{align}
Q(\bm{\theta}) = \sum_{i=1}^N \sum_{j \in \mathcal{N}_i} \left\lVert \bm{P}_{ij} \bm{\theta}_i - \bm{P}_{ji} \bm{\theta}_j \right\rVert^2,
\label{eqn: overall regularizer}
\end{align}
which is exactly the sheaf quadratic form for the choice $\mathcal{F}_{i \trianglelefteq e} = \bm{P}_{ij}$ for $e = (i, j) \in \mathcal{E}$.\\
\textbf{Choice of $d_{ij}$.} The dimension of the interaction space, $d_{ij}$, determines the size of the restriction maps $\bm{P}_{ij}$ and $\bm{P}_{ji}$. In practice, $d_{ij}$ can be chosen based on the number of common or comparable features between clients $i$ and $j$. A smaller value of $d_{ij}$ implies a more compact representation of the shared information between the two clients, while a larger value allows for more flexibility in capturing the relationships between the local models.
\iffalse
\section{Differentiating PFL, HFL, and FMTL}\label{appendix:comparison}
FL has evolved to encompass various paradigms that address specific challenges inherent in decentralized data environments. Among these, PFL, HFL, and FMTL stand out due to their distinct objectives and methodologies. In what follows, we elaborate on the nuances between these paradigms.
\subsection{Objective}
FMTL, PFL, and HFL each pursue distinct goals within the FL landscape. FMTL aims to enable collaborative learning across multiple related tasks, ensuring that each task benefits from shared knowledge while maintaining task-specific optimizations. In contrast, PFL focuses on optimizing a single shared model that is then personalized for each client, enhancing performance tailored to individual client needs. This can be achieved by introducing an $L_2$ regularization term that restricts the difference between the local and global parameters. HFL seeks to facilitate collaboration among clients that may have heterogeneous models, allowing the integration of these models without requiring uniformity in their structures.
\subsection{Task Diversity}
FMTL supports multiple distinct yet related tasks across different clients, leveraging the similarities between tasks to improve overall learning efficiency and performance. PFL, on the other hand, operates under the assumption of a common task distributed among clients, with each client developing personalized adaptations of the shared model to better suit their specific data and requirements. HFL is designed to handle potentially heterogeneous models among clients, enabling collaboration without the necessity of explicitly modeling the relationships between clients.
\subsection{Model Architecture}
In terms of model architecture, FMTL is flexible in accommodating different architectures for each task or client, provided that the relationships between tasks are effectively modeled to facilitate knowledge sharing. PFL typically relies on a common base model that is augmented with personalized layers or adaptations for each client, ensuring a balance between shared knowledge and individual customization. HFL explicitly manages varying model architectures across clients by employing techniques such as knowledge distillation, which allows for the transfer of information between heterogeneous models without requiring a uniform architectural framework.
\subsection{Knowledge Sharing Mechanism}
The mechanisms for knowledge sharing vary significantly among FMTL, PFL, and HFL. FMTL leverages the relationships between tasks to enable effective knowledge transfer across related tasks, enhancing the learning process through shared insights. PFL shares a global model baseline that serves as the foundation for each clients personalized model, ensuring that the core knowledge is distributed while allowing for individual customization. HFL utilizes advanced techniques like knowledge distillation to transfer information between heterogeneous models, ensuring that valuable knowledge is shared even when clients operate with different model architectures.

\section{Scalability of {\ours}}\label{appendix:scalability}
Complex datasets necessitate models with substantial parameter counts to achieve optimal performance. The sheaf-based approach scales to these high-dimensional settings by leveraging the projection mechanism to distill essential shared information, thereby reducing the effective dimensionality required for inter-client communication. This ensures that even as the underlying data complexity increases, the communication overhead remains manageable compared to the baseline dFedU. While {\ours} offers significant advantages in terms of communication efficiency, it introduces additional computational and storage burdens due to the maintenance and updating of restriction maps as illustrated in Table \ref{table:costs}. To ensure that {\ours} remains scalable and efficient in handling complex and large-scale FL scenarios, the following strategies can be employed
\begin{itemize}
    \item \textbf{Sparse restriction maps.} Sparsity constraints on the restriction maps can be implemented to reduce the number of active parameters, thereby lowering both storage and computational requirements.
    \item \textbf{Low-rank approximations.} Low-rank matrix approximations for restriction maps can be used to significantly decrease the computational complexity and storage footprint without substantially compromising performance.
\end{itemize}
\fi

\section{Real-World Applications of {\ours}}\label{appendix:vector_space_scenarios}
In this appendix, we explore practical scenarios where task similarities are inherently defined within vector spaces, making them well-suited for the application of {\ours}. By examining representation learning and feature vector similarities in multi-modal tasks, we illustrate how our sheaf-theoretic framework effectively captures and leverages complex task relationships. These examples showcase the applicability of {\ours} in diverse FL environments.
\subsection{Representation Learning and Embedding Spaces}
In many ML applications, tasks are associated with high-dimensional data that can be effectively represented through embeddings in vector spaces. These embeddings capture semantic, syntactic, or feature-based relationships between tasks, facilitating the modeling of task similarities as vector operations. For example, if we consider the Natural Language Processing (NLP) field, then tasks such as sentiment analysis, topic classification, and named entity recognition can be embedded in a semantic space using techniques like Word2Vec or BERT. The proximity of these task vectors in the embedding space reflects their semantic relatedness. Similar NLP tasks often share underlying linguistic structures. By representing these tasks as vectors, {\ours} can capture and leverage their shared characteristics to enhance collaborative learning.
\subsection{Feature Vector Similarities in Multi-Modal Tasks}
In multi-modal learning scenarios, tasks often involve integrating and processing data from different modalities (e.g., text, image, audio). Task similarities can be defined based on the feature vectors extracted from these modalities, enabling {\ours} to model interactions across diverse data sources. For example, if we consider multi-modal sentiment analysis, then tasks that analyze sentiment from text, images, and audio can have their respective feature vectors embedded in a unified vector space. The similarities between these feature vectors can indicate shared sentiment characteristics across modalities. {\ours} can utilize these vector similarities to facilitate collaborative learning, enhancing sentiment detection accuracy by leveraging cross-modal information. Another example is healthcare applications, where tasks involving the analysis of patient data from various sources (e.g., medical imaging, electronic health records, genomic data) can define task similarities based on the integrated feature vectors representing different data modalities. By modeling these similarities in vector space, {\ours} can improve personalized treatment recommendations through effective knowledge sharing across related healthcare tasks.
\section{Supporting lemmas}\label{suppLemmas}
\begin{lemma}
\begin{align}
    \bm{\theta}^T L_\F \, \bm{\theta} = \sum_{e=(i,j) \in \mathcal{E}} \left\lVert \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right)  -  \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right) \right\rVert^2 = \sum_{e=(i,j) \in \mathcal{E}} \left\| \bm{P}_{ij} \bm{\theta}_{i} - \bm{P}_{ji} \bm{\theta}_{j} \right\|^2.
\end{align}
\end{lemma}
\begin{proof}
Using the block matrix structure of $L_\F$ from equation (\ref{eqn: Laplacian_matrix_form}), we can expand the quadratic form as follows
\begin{align}
\nonumber &\bm{\theta}^T L_\F \, \bm{\theta}\\
\nonumber &= \sum_{i \in \mathcal{V}} \sum_{j \in \mathcal{V}} \bm{\theta}_i^T L_{i,j} \bm{\theta}_j \\
\nonumber &= \sum_{i \in \mathcal{V}} \bm{\theta}_i^T \left( \sum_{j \in \mathcal{N}_i} \bm{P}_{ij}^T \bm{P}_{ij} \right) \bm{\theta}_i - 2 \sum_{e=(i,j) \in \mathcal{E}} \bm{\theta}_i^T \bm{P}_{ij}^T \bm{P}_{ji} \bm{\theta}_j \\
\nonumber &= \sum_{e=(i,j) \in \mathcal{E}} \left( \bm{\theta}_i^T \bm{P}_{ij}^T \bm{P}_{ij} \bm{\theta}_i + \bm{\theta}_j^T \bm{P}_{ji}^T \bm{P}_{ji} \bm{\theta}_j - 2 \bm{\theta}_i^T \bm{P}_{ij}^T \bm{P}_{ji} \bm{\theta}_j \right) \\
\nonumber &= \sum_{e=(i,j) \in \mathcal{E}} \left( \left\| \bm{P}_{ij} \bm{\theta}_i \right\|^2 + \left\| \bm{P}_{ji} \bm{\theta}_j \right\|^2 - 2 \left\langle \bm{P}_{ij} \bm{\theta}_i, \bm{P}_{ji} \bm{\theta}_j \right\rangle \right) \\
&= \sum_{e=(i,j) \in \mathcal{E}} \left\| \bm{P}_{ij} \bm{\theta}_i - \bm{P}_{ji} \bm{\theta}_j \right\|^2.
\end{align}
Recalling that $\F_{i\trianglelefteq e}$ and $\bm{P}_{ij}$ are used interchangeably to denote the restriction map from vertex $i$ to edge $e=(i,j)$, we obtain
\begin{align}
\bm{\theta}^T L_\F \bm{\theta} = \sum_{e=(i,j) \in \mathcal{E}} \left\| \bm{P}_{ij} \bm{\theta}_i - \bm{P}_{ji} \bm{\theta}_j \right\|^2 = \sum_{e=(i,j) \in \mathcal{E}} \left\| \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right) - \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right) \right\|^2.
\end{align}
\end{proof}
\begin{lemma}
\begin{align}
\ker(L_\F) = \argmin_{\bm{\theta} \in  C^0(\F) }  Q_\F(\bm{\theta}) = \argmin_{\bm{\theta}\in  C^0(\F) } \bm{\theta}^T L_\F\, \bm{\theta}.    
\end{align}     
\end{lemma}
\begin{proof}
Let $\bm{\theta} \in \ker(L_\F)$. Then, $L_\F \bm{\theta} = 0$. By the definition of $Q_\F(\bm{\theta})$, we have
\begin{align*}
Q_\F(\bm{\theta}) = \bm{\theta}^T L_\F \bm{\theta} = 0.
\end{align*}
Therefore, $\bm{\theta} \in \argmin_{\bm{\theta} \in C^0(\F)} Q_\F(\bm{\theta})$.

Conversely, let $\bm{\theta} \in \argmin_{\bm{\theta} \in C^0(\F)} Q_\F(\bm{\theta})$. Then, $Q_\F(\bm{\theta}) = 0$, which implies
\begin{align}
0 = Q_\F(\bm{\theta}) = \sum_{e=(i,j) \in \mathcal{E}} \left\| \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right) - \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right) \right\|^2.
\end{align}
Thus, we get
\begin{align}
     \F_{i\trianglelefteq e}\left( \bm{\theta}_i \right) = \F_{j\trianglelefteq e}\left( \bm{\theta}_j \right) \quad \forall e=(i,j) \in \mathcal{E}.
\end{align}
This concludes the proof.    
\end{proof}
\section{Proof of Theorem \ref{theorem}}\label{theorem_proof}
We start by introducing the matrices $\bm{J}_{ij} \in \mathbb{R}^{d_{ij} \times d_i}$ having all its entries equal to one. Then, let us define the block matrix $\bm{H}$ such that $\bm{H}_{ij} = \bm{J}_{ij}$ if $(i, j) \in  \mathcal{E}$, and $\bm{H}_{ij} = \bm{0}$, otherwise. Then, $\bm{\theta}$ and $\bm{P}$ can be updated using the following updates 
\begin{align}
   \bm{\theta}^{k+1} &= \bm{\theta}^k- \alpha 
   (\nabla f(\bm{\theta}^k) + \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^k), \\
   \bm{P}^{k+1} &= \bm{H} \odot \left(\bm{P}^k- \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T\right),
\end{align}
where $\odot$ is the Hadamard product and the matrix $\bm{H}$ is introduced to preserve the block structure of $\bm{P}$ by zeroing out the entries that do not correspond to edges in the graph.

Next, we analyze the convergence of the {\ours} algorithm by studying the descent steps in $\bm{\theta}$ and $\bm{P}$ separately. This approach allows us to establish bounds on the decrease of the objective function $\Psi(\bm{\theta}, \bm{P})$ in each descent step.

\begin{theorem}
Let Assumptions 1 and 2 hold. Assume the learning rates $\alpha$ and $\eta$ satisfy the conditions $\alpha < \frac{2}{N L}$ and $\eta < \frac{2}{\lambda D_\theta^2}$, respectively. Then, the averaged gradient norm is upper bounded as follows
\begin{align}
\frac{1}{K}\sum_{k=0}^{K-1} \|\nabla \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 \leq \frac{1}{\rho K} (\Psi(\bm{\theta}^{0}, \bm{P}^0) - \Psi^\star),
\end{align}
where $\rho = \min\left\{\alpha \left(1 - \frac{\alpha N L}{2}\right), \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2} \right) \right\}$ and $\Psi^\star$ is the optimal value of $\Psi$.
\end{theorem}

\begin{proof}
Using the Lipschitz continuity of the gradient of $f(\bm{\theta})$
\begin{align}
f(\bm{\theta}^{k+1}) \leq f(\bm{\theta}^{k}) + \langle \nabla f(\bm{\theta}^{k}), \bm{\theta}^{k+1} - \bm{\theta}^{k} \rangle + \frac{N L}{2} \|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|^2.    
\end{align}
Adding and subtracting $\frac{\lambda}{2}\bm{\theta}^{k+1}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k+1}$ to both sides of the inequality, we get
\begin{align}\label{23}
\nonumber &\Psi(\bm{\theta}^{k+1}, \bm{P}^k) \\
\nonumber &= f(\bm{\theta}^{k+1}) + \frac{\lambda}{2}\bm{\theta}^{k+1}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k+1}\\
\nonumber & \leq f(\bm{\theta}^{k}) + \langle \nabla f(\bm{\theta}^{k}), \bm{\theta}^{k+1} - \bm{\theta}^{k} \rangle + \frac{N L}{2} \|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|^2 + \frac{\lambda}{2}\bm{\theta}^{k+1}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k+1} \\
\nonumber & = f(\bm{\theta}^{k}) + \langle \nabla f(\bm{\theta}^{k}), \bm{\theta}^{k+1} - \bm{\theta}^{k} \rangle + \frac{N L}{2} \|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|^2 + \frac{\lambda}{2}\bm{\theta}^{k+1}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k+1}  \\
\nonumber & \quad \quad - \frac{\lambda}{2}\bm{\theta}^{k}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k} + \frac{\lambda}{2}\bm{\theta}^{k}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k} \\
\nonumber & = f(\bm{\theta}^{k}) + \langle \nabla f(\bm{\theta}^{k}), \bm{\theta}^{k+1} - \bm{\theta}^{k} \rangle + \frac{N L}{2} \|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|^2 + \frac{\lambda}{2}(\bm{\theta}^{k+1} - \bm{\theta}^{k})^T(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k+1} \\
\nonumber & \quad \quad + \frac{\lambda}{2}\bm{\theta}^{k}(\bm{P}^k)^T\bm{P}^k\bm{\theta}^{k} \\
& = \Psi(\bm{\theta}^{k}, \bm{P}^k) + \langle \nabla_{\bm{\theta}} \Psi(\bm{\theta}^{k}, \bm{P}^k), \bm{\theta}^{k+1} - \bm{\theta}^{k} \rangle + \frac{N L}{2} \|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|^2,
\end{align}
where the last equality follows from the definition of $\Psi(\bm{\theta}, \bm{P})$.

Using the update rule for $\bm{\theta}^{k+1}$, we have
\begin{align}\label{24}
\nonumber &\langle \nabla_{\bm{\theta}} \Psi(\bm{\theta}^k, \bm{P}^k), \bm{\theta}^{k+1} - \bm{\theta}^k \rangle \\
\nonumber &= \langle \nabla f(\bm{\theta}^k) + \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^k, -\alpha (\nabla f(\bm{\theta}^k) + \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^k) \rangle \\
\nonumber &= -\alpha \|\nabla f(\bm{\theta}^k) + \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^k\|^2 \\
&= -\alpha \|\nabla_{\theta} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2.
\end{align}
On the other hand, we have
\begin{align}\label{25}
\nonumber &\|\bm{\theta}^{k+1} - \bm{\theta}^{k}\|_2^2 \\
\nonumber &= \alpha^2 \|\nabla f(\bm{\theta}^k) + \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^k\|^2 \\
&= \alpha^2 \|\nabla_{\theta} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2.
\end{align}
Substituting (\ref{24}) and (\ref{25}) back into (\ref{23}), we obtain
\begin{align}\label{first}
\Psi(\bm{\theta}^{k+1}, \bm{P}^k) &\leq \Psi(\bm{\theta}^{k}, \bm{P}^k) - \alpha \left(1 - \frac{\alpha N L}{2}\right) \|\nabla_{\theta} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2.
\end{align}
By choosing $\alpha < \frac{2}{N L}$, we ensure that the term $1-\frac{\alpha N L}{2}$ is positive.

From the definition of $\Psi(\bm{\theta}, \bm{P})$, we have
\begin{align}
\Psi(\bm{\theta}^{k+1}, \bm{P}^{k+1}) = f(\bm{\theta}^{k+1}) + \frac{\lambda}{2} (\bm{\theta}^{k+1})^T (\bm{P}^{k+1})^T \bm{P}^{k+1} \bm{\theta}^{k+1}.
\end{align}
Using the update rule for $\bm{P}^{k+1}$, i.e., $\bm{P}^{k+1} = \bm{H} \odot (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)$, we can write
\begin{align}
\nonumber&(\bm{P}^{k+1})^T \bm{P}^{k+1} \\
\nonumber& = (\bm{H} \odot (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T))^T (\bm{H} \odot (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)) \\
&\preceq (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)^T (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T),
\end{align}
where the inequality follows from the fact that the Hadamard product with $\bm{H}$ zeros out some entries, which can only decrease the Frobenius norm.

Expanding the right-hand side, we get
\begin{align}
\nonumber &(\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)^T (\bm{P}^k - \eta \lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T) \\
&= (\bm{P}^k)^T \bm{P}^k - 2\eta \lambda (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T + \eta^2 \lambda^2 (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T
\end{align}

Substituting this back into the expression for $\Psi(\bm{\theta}^{k+1}, \bm{P}^{k+1})$, we obtain
\begin{align}
\nonumber &\Psi(\bm{\theta}^{k+1}, \bm{P}^{k+1}) \\
\nonumber &\leq f(\bm{\theta}^{k+1}) + \frac{\lambda}{2} (\bm{\theta}^{k+1})^T (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} - \eta \lambda^2 (\bm{\theta}^{k+1})^T (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T \bm{\theta}^{k+1} \\
&\quad + \frac{\eta^2 \lambda^3}{2} (\bm{\theta}^{k+1})^T (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T (\bm{\theta}^{k+1}) (\bm{\theta}^{k+1})^T \bm{\theta}^{k+1}.
\end{align}
Since the gradient of $\Psi$ with respect to $\bm{P}$ is given by $\nabla_P \Psi(\bm{\theta}, \bm{P}) = \lambda \bm{P} \bm{\theta} \bm{\theta}^T$, we can compute the following norm
\begin{align}
\nonumber &\|\nabla_P \Psi(\bm{\theta}^{k+1}, \bm{P}^{k})\|_F^2 \\
\nonumber &= \Tr((\lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)^T (\lambda \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T)) \\
\nonumber &= \lambda^2 \Tr(\bm{\theta}^{k+1}(\bm{\theta}^{k+1})^T (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T ) \\
&= \lambda^2 (\bm{\theta}^{k+1})^T (\bm{P}^k)^T \bm{P}^k \bm{\theta}^{k+1} (\bm{\theta}^{k+1})^T \bm{\theta}^{k+1},
\end{align}
where we have used the cyclic nature of the trace operator $\Tr(\cdot)$.

Therefore, we have the following bound
\begin{align}\label{second}
\nonumber &\Psi(\bm{\theta}^{k+1}, \bm{P}^{k+1})\\
\nonumber &\leq \Psi(\bm{\theta}^{k+1}, \bm{P}^k) - \eta  \left(1 - \frac{\eta \lambda}{2} \|\bm{\theta}^{k+1}\|^2\right) \|\nabla_P \Psi(\bm{\theta}^{k+1}, \bm{P}^{k})\|^2 \\
&\leq \Psi(\bm{\theta}^{k+1}, \bm{P}^k) - \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2}\right) \|\nabla_P \Psi(\bm{\theta}^{k+1}, \bm{P}^{k})\|^2,
\end{align}
where we have used Assumption 2.

Hence, to ensure $\left(1 - \frac{\eta \lambda D_{\theta}^2}{2}\right)$ is positive, we choose the value of $\eta$ to be $\eta < \frac{2}{\lambda D_\theta^2}$.
Combining the inequalities (\ref{first}) and (\ref{second}), we get
\begin{align}
\nonumber &\Psi(\bm{\theta}^{k+1}, \bm{P}^{k+1})\\
&\leq \Psi(\bm{\theta}^{k}, \bm{P}^k) - \alpha \left(1 - \frac{\alpha N L}{2}\right) \|\nabla_{\theta} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2  - \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2}\right) \|\nabla_P \Psi(\bm{\theta}^{k+1}, \bm{P}^{k})\|^2.
\end{align}
Summing up these inequalities from $k=0$ to $K-1$, we obtain
\begin{align}
\nonumber &\Psi(\bm{\theta}^{K}, \bm{P}^{K})\\
&\leq \Psi(\bm{\theta}^{0}, \bm{P}^0) - \alpha \left(1 - \frac{\alpha N L}{2}\right) \sum_{k=0}^{K-1} \|\nabla_{\theta} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 - \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2} \right) \sum_{k=0}^{K-1}\|\nabla_P \Psi(\bm{\theta}^{k+1}, \bm{P}^{k})\|^2.
\end{align}
Let $\Psi^\star$ be the optimal value of $\Psi$, and we define $\rho = \min\left\{\alpha \left(1 - \frac{\alpha N L}{2}\right), \eta  \left(1 - \frac{\eta \lambda D_{\theta}^2}{2} \right) \right\}$. Then, rearranging the terms, we can write
\begin{align}
\frac{1}{K}\sum_{k=0}^{K-1} \|\nabla \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 &\leq \frac{1}{\rho K} (\Psi(\bm{\theta}^{0}, \bm{P}^0) - \Psi(\bm{\theta}^{K}, \bm{P}^{K})) \leq \frac{1}{\rho K} (\Psi(\bm{\theta}^{0}, \bm{P}^0) - \Psi^\star),
\end{align}
where we have used that $\Psi^\star \leq \Psi(\bm{\theta}^{K}, \bm{P}^{K})$ and  $\|\nabla \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 = \|\nabla_{\bm{\theta}} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2 + \|\nabla_{\bm{P}} \Psi(\bm{\theta}^k, \bm{P}^k)\|^2$. 
\end{proof}
\section{Additional Experimental Details} \label{experimentsDetails}
\subsection{Datasets}\label{appendix:dataset}
 A summary of the datasets and the tasks used in Section \ref{experiment1} is presented in Table \ref{table:datasets}. These datasets are real-world datasets created in federated environments with varying degrees of heterogeneity. A detailed description of the datasets along with their specific data partitioning schemes is provided in Table \ref{table:nonIID}. To further quantify the Non-IIDness in our data partitions, we have incorporated quantitative metrics assessing the degree of Non-IIDness across different datasets in Table \ref{table:nonIID2}.
\begin{itemize}
    \item \textbf{Rotated MNIST (R-MNIST).} Following similar techniques as outlined in \citep{liu2022privacy}, we shuffle and then evenly separate the original MNIST dataset between 40 clients. Next, we randomly divide the clients into four groups, each containing 10 clients. We then apply rotations of \{0, 90, 180, 270\} to each group respectively. Therefore, clients within the same group share identical image rotations, resulting in the formation of four distinct clusters. The MNIST dataset is available under the CC BY-SA 3.0 license.
    \item \textbf{Heterogeneous CIFAR-10 (H-CIFAR-10).} The original CIFAR-10 dataset is split among 30 clients, and heterogeneity is introduced by assigning each client a random number of samples from 5 randomly selected classes out of the 10 available classes, following a similar approach as in \citep{t2020personalized, liu2022privacy}. The CIFAR-10 dataset is available under the MIT license.
    \item \textbf{Human Activity Recognition.} The dataset is composed of data gathered from the accelerometers and gyroscopes of smartphones used by 30 individuals, each performing one of six activities: walking, walking upstairs, walking downstairs, sitting, standing, or lying down. In this dataset, the data from each individual/client is treated as a unique task, with the primary objective being to differentiate between these activities. To identify each activity appropriately, feature vectors with 561 elements representing various time and frequency domain variables are used in the analysis. The dataset \citep{anguita2013public} is licensed under a Creative Commons Attribution 4.0 International (CC-BY 4.0) license.
    \item \textbf{Vehicle Sensor.} The dataset involves collecting data from a network of 23 wireless sensors, including acoustic (microphones), seismic (geophones), and infrared (polarized IR sensors), strategically placed along a specific road segment. This dataset aims to facilitate binary classification for identifying two types of vehicles: the Assault Amphibian Vehicle (AAV) and the Dragon Wagon (DW). Each sensor, treated as a unique task or client, gathers acoustic and seismic data encapsulated in a 100-dimensional feature vector, representing the recordings as vehicles pass by. The Vehicle dataset was originally made public by its authors as a research dataset \citep{duarte2004vehicle}.
    \item \textbf{Google Glass Eating and Motion (GLEAM).} The dataset is collected using Google Glass from 38 individuals. It captures high-resolution sensor data to identify specific activities such as eating. This extensive dataset, consisting of 27,800 entries, each with a 180-dimensional feature vector, records head movements for binary classification to determine if the wearer is eating or not. The data includes accelerometer, gyroscope, and magnetometer readings, analyzed for statistical, spectral, and temporal characteristics to distinguish eating from other activities like walking, talking, and drinking. The GLEAM dataset, released by its original authors \citep{rahman2015unintrusive}, is available for non-commercial use.
    \item \textbf{School.} The dataset, originally introduced in \citep{goldstein1991multilevel}, seeks to forecast the exam results of 15,362 students from 139 secondary schools. The dataset contains information for each school, with student numbers ranging from 22 to 251, and each student is described using a 28-dimensional feature vector. This vector contains information about the school's ranking, the student's birth year, and the availability of free meals at the school. The dataset has been made publicly available in \citep{zhou2011malsar}.
\end{itemize}


\begin{table}[H]
\centering
\caption{
  Summary of the datasets and tasks used in our empirical setup.
  }
\label{table:datasets} 
\vspace{0.5em}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Dataset} & \multicolumn{1}{l}{\textbf{Task}} & \multicolumn{1}{l}{\textbf{\# Clients/Tasks}} & \multicolumn{1}{l}{\textbf{Input Dimension}} \\ \midrule
R-MNIST & Classification & 40 & $28 \times 28 \times 1$ \\
H-CIFAR-10 & Classification & 30 &  $32 \times 32 \times 3$ \\
HAR & Classification & 30 &561 \\
Vehicle Sensor & Classification & 23  & 100 \\
GLEAM & Classification & 38  &180 \\
School & Regression & 139  & 28 \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Experimental Settings}\label{appendix:moredetails}
In the first experiment, we use a train/test split ratio of $75\%/25\%$ for all datasets as done in \citep{smith2017federated}. For the classification task, the model used in all experiments is the multinomial logistic regression model with $L_2$-regularized cross-entropy as the loss function. Similar to \citep{dinh2022new}, we reduce the data size by $80\%$ for half of the clients to mimic the real-world FL setting where some clients have small datasets and can benefit from collaboration. For the regression task, we consider a linear model and the loss function to be the regularized $L_2$ loss. For dFedU, the weights $\{a_{ij}\}$, defined in (\ref{eqn: conventional FMTL objective}), are taken to be $a_{ij} = 1, ~\forall (i, j) \in \mathcal{E}$.  

For each experiment, we report both the average test accuracy/MSE and its corresponding one standard error shaded area based on five runs. Since all models have the same size $d$, we choose the projection space dimension to be $\gamma d$, where $\gamma \in (0,1]$. In our experiments, the graph topology is based on the Erds-Rnyi model, where we randomly generate a network consisting of $N$ clients with a connectivity ratio $p=0.15$ for Rotated MNIST and Heterogenous CIFAR-10 datasets, and $p=0.2$ for the rest of datasets. Both learning rates are chosen small to satisfy the conditions in Theorem \ref{theorem}. Furthermore, {\ours} uses the same global learning rate ($\alpha$) as dFedU to update the models for a fair comparison. The linear maps are initialized randomly from a normal distribution with a mean of 0 and a variance of 1. The values of the regularization parameter ($\lambda$) used for every dataset are listed in Table \ref{table:regularization}.

For the second experiment, we generate the modified datasets as follows. To mimic the fact that each client has a different model size, we randomly drop a set of features with a drop factor sampled from the uniform distribution $\mathcal{U}([0,0.4])$. As an illustration, the model size distribution across the number of clients in the modified Vehicle dataset is plotted in Figure \ref{hist}.

\begin{table}[t]
\centering
\caption{
Data partitioning strategies across datasets where $n_k$ is the number of
training samples of client $k$.
  }
\label{table:nonIID} 
\vspace{0.5em}

\begin{tabular}{@{}lccc@{}}
\toprule
\begin{tabular}[c]{@{}c@{}} \textbf{Dataset}\\ \textbf{ } \end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Data split}\\ \textbf{ }\end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Domain }\\ \textbf{distribution}\end{tabular} & \begin{tabular}[c]{@{}c@{}} \textbf{Label}\\ \textbf{distribution}\end{tabular}  \\ \midrule
\begin{tabular}[c]{@{}c@{}} R-MNIST\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 1500$ \\ max $n_k = 1500$ \end{tabular}  & \begin{tabular}[c]{@{}c@{}} 4 groups with distinct\\  rotation angles \end{tabular} & \begin{tabular}[c]{@{}c@{}} Uniform distribution\\ within each rotation group \end{tabular} \\ \midrule 
\begin{tabular}[c]{@{}c@{}} H-CIFAR-10\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 1515$ \\ max $n_k = 1839$ \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Not explicitly \\ divided into domains \end{tabular} & \begin{tabular}[c]{@{}c@{}} Each client has data\\ from 5 random classes \end{tabular} \\ \midrule 
\begin{tabular}[c]{@{}c@{}} HAR\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 210$ \\ max $n_k = 306$ \end{tabular}  & \begin{tabular}[c]{@{}c@{}} All activity \\ classes per client \end{tabular} & \begin{tabular}[c]{@{}c@{}} Uniform across activity\\ classes within each client \end{tabular}  \\ \midrule 
\begin{tabular}[c]{@{}c@{}} Vehicle Sensor\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 872$ \\ max $n_k = 1933$\end{tabular}  & \begin{tabular}[c]{@{}c@{}} Same label set with\\ different feature distributions \end{tabular} & \begin{tabular}[c]{@{}c@{}} Balanced across vehicle\\ classes per sensor \end{tabular} \\ \midrule 
\begin{tabular}[c]{@{}c@{}} GLEAM\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 699$ \\ max $n_k = 776$ \end{tabular}  & \begin{tabular}[c]{@{}c@{}} All activity classes\\ with uniform distribution \end{tabular} & \begin{tabular}[c]{@{}c@{}} Balanced between eating\\ and non-eating classes \end{tabular}  \\ \midrule 
\begin{tabular}[c]{@{}c@{}} School\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} min $n_k = 15$ \\ max $n_k = 175$ \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Shared regression task\\ with uniform feature sets \end{tabular} & \begin{tabular}[c]{@{}c@{}} Continuous targets with\\ varying distributions per school \end{tabular} \\ \bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\centering
\caption{
  Degree of non-IIDness across datasets.
  }
\label{table:nonIID2} 
\vspace{0.5em}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Dataset} &  \textbf{Non-IID Metric} & \textbf{Description}   \\ \midrule
\begin{tabular}[c]{@{}c@{}} R-MNIST\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Rotation angle variance\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} High domain heterogeneity \\with 4 distinct rotation groups\end{tabular} \\ \midrule 
\begin{tabular}[c]{@{}c@{}} H-CIFAR-10\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Label distribution \\ \textbf{ }  \end{tabular}  & \begin{tabular}[c]{@{}c@{}} High label distribution  \\ skew among clients \end{tabular}  \\ \midrule 
\begin{tabular}[c]{@{}c@{}} HAR\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Inter-client variability\\ \textbf{ }  \end{tabular}  & \begin{tabular}[c]{@{}c@{}} High heterogeneity due to unique \\  individual data per client \end{tabular}   \\ \midrule 
\begin{tabular}[c]{@{}c@{}} Vehicle Sensor\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Feature distribution \\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} High feature heterogeneity\\ across different sensors \end{tabular}  \\ \midrule 
\begin{tabular}[c]{@{}c@{}} GLEAM\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Label distribution \\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Low heterogeneity \\ with balanced labels \end{tabular}   \\ \midrule 
\begin{tabular}[c]{@{}c@{}} School\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Continuous targets variance\\ \textbf{ } \end{tabular}  & \begin{tabular}[c]{@{}c@{}} Moderate to high heterogeneity based \\ on inter-school performance variability \end{tabular}  \\ \bottomrule
\end{tabular}
\end{table}


\begin{figure}[t]
    \centering
    \includegraphics[scale=0.5]{Figures/histogram.pdf}
    \caption{Model size distribution across clients in the modified Vehicle dataset.}
    \label{hist}
\end{figure}

\begin{table}[H]
\centering
\caption{
  Regularization parameters used during training.
  }
\label{table:regularization} 
\vspace{0.5em}
\begin{tabular}{@{}lcccccc@{}}
\toprule
\textbf{Dataset} & \multicolumn{1}{l}{HAR} & \multicolumn{1}{l}{Vehicle} & \multicolumn{1}{l}{GLEAM} & \multicolumn{1}{l}{School} & \multicolumn{1}{l}{R-MNIST} & \multicolumn{1}{l}{H-CIFAR-10} \\ \midrule
Regularization parameter ($\lambda$) & $0.05$ & $0.001$ & $0.001$ & $0.01$ & $0.001$ & $0.001$  \\ \bottomrule
\end{tabular}
\end{table}

\subsection{Hardware \& Code} \label{harwareCode}
Our experiments were carried out on a system equipped with an Intel(R) Xeon(R) CPU operating at 2.20GHz with 2 cores and 12 GB of RAM. The algorithms are implemented in Python using PyTorch \citep{paszke2019pytorch}, and NetworkX \citep{hagberg2008exploring}. %The code is available at ......

\section{Supplementary Experimental Results}\label{appendix:moreResults}
%\subsection{Performance on Additional Datasets}
Figure \ref{vehicle_gleam_datasets} compares the performance of our proposed {\ours} method with dFedU on four datasets: (a) the Vehicle dataset, (b) the School dataset, (c) the HAR dataset and (d) the GLEAM dataset using $\gamma = \{0.1, 0.3\}$. The experiments evaluate the performance of the model in terms of communication rounds and transmitted bits. For instance, {\ours} requires more communication rounds for the Vehicle dataset to achieve similar test accuracy compared to dFedU. As the number of communication rounds increases, the performance gap between the two methods narrows. However, when examining the number of transmitted bits, {\ours} demonstrates a clear advantage, requiring fewer bits to reach higher accuracy levels. For example, {\ours} with $\gamma = 0.1$ reaches a test accuracy of 0.85 with just 72 transmitted Kbits, while dFedU requires over 450 Kbits to approach this accuracy. Similar trends are observed for the other datasets. {\ours} recovers the same performance as dFedU in terms of test accuracy with a slight drop in test accuracy for the HAR and School datasets. Hence, the sheaf-based approach effectively captures the heterogeneous relationships among clients, achieving the same test accuracy using fewer transmitted bits than the baseline dFedU. %When examining the CPU plots, we can see that for the HAR and School datasets, {\ours} requires more time to reach the same level of accuracy, while it seems to be less time than dFedU for the other two datasets. This could be due to the fact that the Vehicle and GLEAM datasets have smaller clients (tasks) and fewer features compared to the other two datasets.
%\subsection{Ablation Study Results}

\begin{figure}[t]
\centering
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/vehicle.pdf}
  %\caption{Vehicle dataset}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/har.pdf}
  %\caption{Vehicle dataset}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/school.pdf}
  %\caption{GLEAM dataset}
\end{subfigure}%
\hfill
\begin{subfigure}[b]{\textwidth}
  \centering
  \includegraphics[scale=0.3]{Figures/gleam.pdf}
  %\caption{GLEAM dataset}
\end{subfigure}%
% Add a caption for the entire second row
\caption{Test/MSE accuracy as a function of the number of communication rounds and the number of transmitted bits for the Vehicle dataset (first row), the School dataset (second row), the HAR dataset (third row), and the GLEAM dataset (bottom).}
\label{vehicle_gleam_datasets}
\end{figure}

\iffalse

\section{Limitations and Future Work} \label{limitations}
In this section, we highlight some of the limitations of our approach, while outlining several potential directions for future work.

\textbf{Dimension of the projection space.} The current work assumes a fixed dimension for the projection spaces associated with the edges in the sheaf structure. However, the optimal dimension may depend on factors such as the complexity of the client models and the relationships between the tasks. Future work could explore methods for automatically learning or finding the dimension of the projection spaces.

\textbf{Restriction maps.} The proposed framework uses linear maps for the restriction and lifting operations between the client and edge spaces. While this allows for efficient computation, it may limit the ability to capture complex relationships between the tasks. Investigating the use of non-linear restriction maps could potentially improve the capabilities of the sheaf-based FMTL framework. 

\textbf{Storage requirement of the restriction maps.} Our proposed method requires the storage of additional $\bm{P}_{ij}$ matrices that are of size $d_i \times d_{ij}$. While this may not be considered an issue in a cross-silo FL setting where the clients or organizations involved typically have more substantial computational and storage capabilities,  this can be seen as a limitation to clients in massively distributed FL scenarios, e.g., mobile devices or IoT devices.

\textbf{Relaxing Assumption 2.} The theoretical analysis in the paper relies on Assumption 2, which assumes the models are bounded. Relaxing or removing this assumption would broaden the applicability of the theoretical results.
\fi
\end{document}
