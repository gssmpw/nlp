\section{Related Work}
In this section, we examine two primary machine learning approaches commonly used in affective computing: feature-based models and end-to-end models. We then discuss recent advancements in explainable affective computing, emphasizing their contribution and limitation to model transparency and interpretability.

\subsection{Feature and End-to-end Models in Affective Computing}
    Discriminative AC focuses on mapping human-centered data to emotion-related labels, employing two primary approaches: feature-based models and end-to-end models.
    
    Feature-based models ____ rely on manually extracted features derived from raw data, which are then used to train machine learning models to establish the relationship between features and labels. The strength of this approach lies in the interpretability of the features, which are often human-understandable and can provide valuable behavioral insights ____. Additionally, feature-based models typically operate on structured, tabular data, offering a computationally efficient solution ____. However, the reliance on handcrafted features may omit potentially important information embedded in the raw data, causing inevitable information loss ____. Furthermore, decoupling feature extraction from model training may introduce limitations, such as overfitting, particularly due to the structured nature of the input data ____.
    
    End-to-end models ____, on the other hand, learn directly from raw data, eliminating the need for manual feature engineering. Fully leveraging the representational power of deep neural networks, these models are particularly effective when trained on large datasets. However, their strength is also their weakness: the opacity of their learned representations often leads to what is referred to as the ``black-box'' problem, making these models difficult to interpret as they lack human-understandable intermediate representations ____. 
    This challenge persists in multi-task learning, where models are designed to predict multiple task labels simultaneously, such as emotion and AUs. Despite their multi-task design, emotion and AU predictions are learned independently, leaving the model as a black box, where the predicted AUs cannot explain the predicted emotions.

    As shown in Fig. \ref{fig_ven}, in this work, we propose a hybrid approach, integrating the strengths of the well-understandable feature-based model and the state-of-the-art black-box models through concept-based learning, where each concept serves as an embedded neural representation of the feature. This approach retains the interpretability inherent in feature-based models while harnessing the robust learning capabilities of end-to-end neural networks. 

    \begin{figure}[t]
    \centering
    \includegraphics[width=0.7\columnwidth]{fig_2.pdf}
       \caption{Feature-based approaches offer inherent interpretability and are easily understood by humans, while end-to-end models deliver state-of-the-art learning capabilities. This work seeks to integrate the strengths of both methods through a concept-based framework, which achieves a balance between high explainability and robust performance. Unlike traditional features, concepts are not static values. They serve as the neural embeddings of features that are trainable within the ML framework, spontaneously quantifying the contribution of individual concepts to the task label. }
    \label{fig_ven}
    \end{figure}
    

\subsection{XAI in Affective Computing} \label{xai_in_ac}
    Recent efforts to enhance the explainability of affective computing models have largely relied on post-hoc, map-based visualizations, and concept-based learning.

    Post-hoc approaches ____ retrospectively analyze the parameter importance of pre-trained black-box models after deployment. These methods attempt to explain the model by manipulating parameters in specific parts of the network to check their impact on the final prediction. Map-based approaches ____ are another common method used to interpret black-box models, typically highlighting the regions where the model focuses its attention. However, both of these approaches primarily focus on the importance scores within the neural network, without offering additional, domain-relevant information for experts. This limitation is particularly evident in AC, where conflicting indicators, such as AU12 (Lip Corner Puller) signaling positive emotion and AU15 (Lip Corner Depressor) indicating negative emotion, may appear in the same facial region. Therefore, simply presenting the weight importance or model attention provides little insight for domain experts like psychologists to understand the AI decision-making process. Furthermore, the distinct properties of multimodal data make incorporating multimodal alignment and co-learning in post-hoc or map-based XAI methods even more challenging, taking the risk of losing either accuracy or interpretability.

    Recent attempts on concept-based models ____ try to encapsulate specific, human-understandable features through concept embeddings $C$ that are learned in a fully supervised manner. These models learn the mapping $X \rightarrow C \rightarrow Z$, where $x \in X$ represents the raw image pixels and $z \in Z$ represents the task labels. Specifically, a concept generator $G$ generates concept embeddings, denoted as $\hat{c} = G(x)$, with $\hat{c} \in C$ representing the learned concepts within a bottleneck layer $C$. Subsequently, a facial expression predictor $y$ maps the concept embeddings to task labels $\hat{z} \in Z$, where $\hat{z} = y(\hat{c})$. While concept-based models offer a more interpretable framework than map-based approaches, ongoing research is focused on integrating this explainable architecture with multimodal learning and performance-optimized strategies ____. Moreover, a key challenge lies in integrating spatial explanations, which reveal \textit{where} the model is focusing, with concept-based explanations, which clarify \textit{what} contributes to the prediction. Achieving this synergy is essential for enhancing both the interpretability and practical utility of models in high-stakes applications.

    Table \ref{tab_previous_work} compares the proposed concept-based framework with previous feature-based, map-based, and black-box FER models in terms of explainability and performance. The proposed framework provides learnable domain-specific insights into the decision-making process for stakeholders while retaining map-based explanations that illustrate the model's areas of attention. A two-stage learning architecture with multimodal concept fusion is introduced, effectively addressing the alignment and co-learning challenges in multimodal interpretable AC. Furthermore, it achieves state-of-the-art performance through deep end-to-end training, successfully balancing the trade-off between interpretability and performance in high-stakes AC applications.

    \begin{table}[t]
    \centering
    \caption{Comparison of our work with previous works on FER in terms of explainability and performance, including feature-based approach, map-based approach, and deep end-to-end approach.}
    \begin{tabular}{ccccc}
    \toprule
                          & Ours & Feature & Map & Black-box \\ \midrule
    Feature-based Insight & +    & +       &     &     \\
    Map-based Explanation & +    &         & +   &     \\
    End-to-end Training   & +    &         & +   & +   \\
    Learnable Explanation & +    &         &     &     \\
    Multimodal Learning   & +    & +       &     & +   \\ 
    \bottomrule
    \end{tabular}
    \label{tab_previous_work}
    \end{table}