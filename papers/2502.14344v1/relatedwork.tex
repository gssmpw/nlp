\section{Related Work}
% SNN BNN BSNN
\subsection{Spiking Neural Network}

Brain-inspired SNNs have emerged as a new computing paradigm \cite{zhang2019mpd,tang2024neuromorphic}. Unlike traditional DNNs, SNNs perform computations using binary spikes over time, making them especially suitable for energy-efficient computing applications \cite{wang2024global,wei2023temporal}.
Given the non-differentiability of the spike generation function, the SNN research community has devoted considerable efforts to developing effective training algorithms. These approaches can be broadly classified into two categories: conversion-based methods and direct learning algorithms.
Conversion methods may not fully exploit the temporal processing capabilities of SNNs and often require multiple steps for accurate inference \cite{roy2019scaling,wang2020deep}. In contrast, direct learning methods can achieve high accuracy with low-latency inference \cite{wu2018spatio,wu2019direct,wang2024ternary}. 
Considering activation values in SNNs are binary, further quantizing the weights to binary would allow computation-intensive convolution operations to be executed using simple bitwise operations. This would further enhance the inherent efficiency advantages of SNNs.



\subsection{Binary Neural Network}
% BNN 权重是二值，xxx
As an extreme form of network quantization, binary neural networks (BNNs) constrain both weights and activations to -1 and +1, substantially reducing memory storage and computational overhead. However, BNNs experience substantial performance degradation compared to their full-precision counterparts. Subsequent research has focused on narrowing this performance gap between BNNs and full-precision neural networks. Several approaches have been proposed to address this issue.
XNOR-Net~\cite{rastegari2016xnor} employs a deterministic binarization scheme and minimizes quantization error by incorporating scaling factors in each layer.
Bi-Real~\cite{liu2018bi} explores piecewise polynomial functions and network architectures for quantization. Previous work has also focused on constraining the distribution of weights or activations in BNNs~\cite{xu2021recu}. 
Building on these advancements in the binary domain, binarization methods in SNNs remain a promising area for exploration.



\subsection{Binarization techniques in SNNs}


Several researchers have combined SNNs with binarization techniques.
For example,~\cite{qiao2021direct} employ the surrogate gradient (SG) method to train BSNNs directly. 
\cite{jang2021bisnn} propose a novel Bayesian-based algorithm for BSNNs, which shows significant advantages in accuracy and calibration compared to SG methods. 
\cite{kheradpisheh2022bs4nn} integrate binarization into temporal-coded SNNs, where each neuron emits at most one spike, offering substantial energy benefits. 
Recently, \cite{wei2024q} take inspiration from information theory and propose a weight-spike dual regulation (WS-DR) method to enhance the information capacity of BSNNs, achieving significant performance improvements. 
However, current BSNNs continue to encounter challenges when scaling to complex datasets such as ImageNet.
Therefore, it is essential to investigate the challenges inherent in the BSNN training process and to bridge the performance gap between BSNNs and FP-SNNs.