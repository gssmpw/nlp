@book{alim2016,
    author = {Alim, H. Samy and Rickford, John R. and Ball, Arnetha F.},
    title = "{Raciolinguistics: How Language Shapes Our Ideas About Race}",
    publisher = {Oxford University Press},
    year = {2016},
    month = {11},
    abstract = "{Raciolinguistics reveals the central role that language plays in shaping our ideas about race and vice versa. The book brings together a team of leading scholars—working both within and beyond the United States—to share research that helps us understand the increasingly vexed relationships between race, ethnicity, and language. Combining the innovative, cutting-edge approaches of race and ethnic studies with fine-grained linguistic analyses, chapters cover a wide range of topics, including the language use of African American Jews and the struggle over the very term “African American,” racialized language education debates within “majority-minority” immigrant communities as well as indigenous communities in the United States, the dangers of multicultural education in a Europe that is struggling to meet the needs of new migrants, and the sociopolitical and cultural meanings of linguistic styles used in Brazilian favelas, South African townships, Israeli neighborhoods, Mexican and Puerto Rican barrios in Chicago, and Korean American “cram schools.”In examining changing demographics in the United States—such as population resegregation, Asian and Latino patterns of immigration, and new African American (im)migration patterns, along with changing global cultural and media trends (like Hip Hop and social media), Raciolinguistics shapes the future of studies on race, ethnicity, and language. By taking a comparative look across a diverse range of language and literacy contexts, the volume seeks not only to set the research agenda in this burgeoning area but also to help resolve pressing educational and political problems in some of the most contested raciolinguistic contexts in the world.}",
    isbn = {9780190625696},
    doi = {10.1093/acprof:oso/9780190625696.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780190625696.001.0001},
}

@book{alim_articulate_2012,
	address = {Oxford, UNITED STATES},
	title = {Articulate {While} {Black}: {Barack} {Obama}, {Language}, and {Race} in the {U}. {S}.},
	isbn = {978-0-19-981297-4},
	shorttitle = {Articulate {While} {Black}},
	url = {http://ebookcentral.proquest.com/lib/cm/detail.action?docID=1026822},
	urldate = {2025-02-05},
	publisher = {Oxford University Press, Incorporated},
	author = {Alim, H. Samy and Smitherman, Geneva and Dyson, Michael Eric},
	year = {2012},
}

@article{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2204.05862},
	doi = {10.48550/ARXIV.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2024-10-14},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {Other
Data available at https://github.com/anthropics/hh-rlhf},
}

@book{baker2020linguistic,
  title={Linguistic justice: Black language, literacy, identity, and pedagogy},
  author={Baker-Bell, April},
  year={2020},
  publisher={Routledge}
}

@inproceedings{bang_multitask_2023,
	address = {Nusa Dua, Bali},
	title = {A {Multitask}, {Multilingual}, {Multimodal} {Evaluation} of {ChatGPT} on {Reasoning}, {Hallucination}, and {Interactivity}},
	url = {https://aclanthology.org/2023.ijcnlp-main.45/},
	doi = {10.18653/v1/2023.ijcnlp-main.45},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the 13th {International} {Joint} {Conference} on {Natural} {Language} {Processing} and the 3rd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	editor = {Park, Jong C. and Arase, Yuki and Hu, Baotian and Lu, Wei and Wijaya, Derry and Purwarianti, Ayu and Krisnadhi, Adila Alfa},
	month = nov,
	year = {2023},
	pages = {675--718},
}

@article{blodgett2021sociolinguistically,
  title={Sociolinguistically driven approaches for just natural language processing},
  author={Blodgett, Su Lin},
  year={2021}
}

@inproceedings{blodgett_demographic_2016,
	address = {Austin, Texas},
	title = {Demographic {Dialectal} {Variation} in {Social} {Media}: {A} {Case} {Study} of {African}-{American} {English}},
	shorttitle = {Demographic {Dialectal} {Variation} in {Social} {Media}},
	url = {https://aclanthology.org/D16-1120},
	doi = {10.18653/v1/D16-1120},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	keywords = {notion, r/rlhf\_aae\_bias},
	pages = {1119--1130},
}

@inproceedings{blodgett_language_2020,
	address = {Online},
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of “{Bias}” in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {https://aclanthology.org/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	abstract = {We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”—i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna},
	month = jul,
	year = {2020},
	keywords = {c/11711, notion, r/rlhf\_aae\_bias},
	pages = {5454--5476},
	file = {Full Text PDF:/Users/joelmire/Zotero/storage/8TZLS7PA/Blodgett et al. - 2020 - Language (Technology) is Power A Critical Survey .pdf:application/pdf},
}

@article{blodgett_racial_2017,
	title = {Racial {Disparity} in {Natural} {Language} {Processing}: {A} {Case} {Study} of {Social} {Media} {African}-{American} {English}},
	shorttitle = {Racial {Disparity} in {Natural} {Language} {Processing}},
	url = {https://www.semanticscholar.org/paper/Racial-Disparity-in-Natural-Language-Processing%3A-A-Blodgett-O%27Connor/59e94c9f21937643678ff494901f3d8b22af4e2f},
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	urldate = {2024-08-13},
	journal = {ArXiv},
	author = {Blodgett, Su Lin and O'Connor, Brendan T.},
	month = jun,
	year = {2017},
	annote = {[TLDR] An empirical analysis of racial disparity in language identification for tweets written in African-American English is conducted, and implications of disparity in NLP are discussed.},
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{casper_open_2023,
	title = {Open {Problems} and {Fundamental} {Limitations} of {Reinforcement} {Learning} from {Human} {Feedback}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=bx24KpJ4Eb},
	abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-layered approach to the development of safer AI systems.},
	language = {en},
	urldate = {2025-02-08},
	journal = {Transactions on Machine Learning Research},
	author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomek and Lindner, David and Freire, Pedro and Wang, Tony Tong and Marks, Samuel and Segerie, Charbel-Raphael and Carroll, Micah and Peng, Andi and Christoffersen, Phillip J. K. and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Biyik, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
	month = sep,
	year = {2023},
}

@article{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	url = {https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	urldate = {2024-10-13},
	journal = {ArXiv},
	author = {Christiano, P. and Leike, J. and Brown, Tom B. and Martic, Miljan and Legg, S. and Amodei, Dario},
	month = jun,
	year = {2017},
	annote = {[TLDR] This work explores goals defined in terms of (non-expert) human preferences between pairs of trajectory segments in order to effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion.},
}

@inproceedings{dacon-2022-towards,
    title = "Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of {A}frican-{A}merican {E}nglish",
    author = "Dacon, Jamell",
    editor = "Blodgett, Su Lin  and
      Daum{\'e} III, Hal  and
      Madaio, Michael  and
      Nenkova, Ani  and
      O'Connor, Brendan  and
      Wallach, Hanna  and
      Yang, Qian",
    booktitle = "Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.hcinlp-1.8",
    doi = "10.18653/v1/2022.hcinlp-1.8",
    pages = "55--63",
    abstract = "Currently, natural language processing (NLP) models proliferate language discrimination leading to potentially harmful societal impacts as a result of biased outcomes. For example, part-of-speech taggers trained on Mainstream American English (MAE) produce non-interpretable results when applied to African American English (AAE) as a result of language features not seen during training. In this work, we incorporate a human-in-the-loop paradigm to gain a better understanding of AAE speakers{'} behavior and their language use, and highlight the need for dialectal language inclusivity so that native AAE speakers can extensively interact with NLP systems while reducing feelings of disenfranchisement.",
}

@inproceedings{davidson-etal-2019-racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}

@misc{deas_evaluation_2023,
	title = {Evaluation of {African} {American} {Language} {Bias} in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2305.14291},
	abstract = {Warning: This paper contains content and language that may be considered offensive to some readers. While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged "standard" form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, as well as a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Deas, Nicholas and Grieser, Jessi and Kleiner, Shana and Patton, Desmond and Turcan, Elsbeth and McKeown, Kathleen},
	month = nov,
	year = {2023},
	note = {arXiv:2305.14291 [cs]},
	keywords = {notion, r/rlhf\_aae\_bias},
	annote = {Comment: EMNLP 2023 Camera-Ready},
	file = {2305.14291v2.pdf:/Users/joelmire/Downloads/2305.14291v2.pdf:application/pdf},
}

@misc{fleisig2024linguisticbiaschatgptlanguage,
      title={Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination}, 
      author={Eve Fleisig and Genevieve Smith and Madeline Bossi and Ishita Rustagi and Xavier Yin and Dan Klein},
      year={2024},
      eprint={2406.08818},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08818}, 
}

@book{green2002african,
  title={African American English: a linguistic introduction},
  author={Green, Lisa J},
  year={2002},
  publisher={Cambridge University Press}
}

@book{grieser2022black,
  title={The Black side of the river: Race, language, and belonging in Washington, DC},
  author={Grieser, Jessica A},
  year={2022},
  publisher={Georgetown University Press}
}

@inproceedings{groenwold-etal-2020-investigating,
    title = "Investigating {A}frican-{A}merican {V}ernacular {E}nglish in Transformer-Based Text Generation",
    author = "Groenwold, Sophie and Ou, Lily and Parekh, Aesha and Honnavalli, Samhita and Levy, Sharon and Mirza, Diba and Wang, William Yang",
    booktitle = "Proceedings of EMNLP",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.473",
    year = "2020"
}

@inproceedings{harris_exploring_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {Exploring the {Role} of {Grammar} and {Word} {Choice} in {Bias} {Toward} {African} {American} {English} ({AAE}) in {Hate} {Speech} {Classification}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533144},
	doi = {10.1145/3531146.3533144},
	abstract = {Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30\% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.},
	urldate = {2024-10-13},
	booktitle = {Proceedings of the 2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Harris, Camille and Halevy, Matan and Howard, Ayanna and Bruckman, Amy and Yang, Diyi},
	month = jun,
	year = {2022},
	pages = {789--798},
}

@inproceedings{harris_modeling_2024,
	address = {Miami, Florida, USA},
	title = {Modeling {Gender} and {Dialect} {Bias} in {Automatic} {Speech} {Recognition}},
	url = {https://aclanthology.org/2024.findings-emnlp.890/},
	doi = {10.18653/v1/2024.findings-emnlp.890},
	abstract = {Dialect and gender-based biases have become an area of concern in language-dependent AI systemsincluding around automatic speech recognition (ASR) which processes speech audio into text. These potential biases raise concern for discriminatory outcomes with AI systems depending on demographic- particularly gender discrimination against women, and racial discrimination against minorities with ethnic or cultural English dialects.As such we aim to evaluate the performance of ASR systems across different genders and across dialects of English. Concretely, we take a deep dive of the performance of ASR systems on men and women across four US-based English dialects: Standard American English (SAE), African American Vernacular English (AAVE), Chicano English, and Spanglish. To do this, we construct a labeled dataset of 13 hours of podcast audio, transcribed by speakers of the represented dialects. We then evaluate zero-shot performance of different automatic speech recognition models on our dataset, and further finetune models to better understand how finetuning can impact performance. Our work fills the gap of investigating possible gender disparities within underrepresented dialects.},
	urldate = {2025-02-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Harris, Camille and Mgbahurike, Chijioke and Kumar, Neha and Yang, Diyi},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {15166--15184},
}

@article{hendy_how_2023,
	title = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}? {A} {Comprehensive} {Evaluation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}?},
	url = {https://arxiv.org/abs/2302.09210},
	doi = {10.48550/ARXIV.2302.09210},
	abstract = {Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the field and helps to better understand the potential and limitations of GPT models for translation.},
	urldate = {2025-02-08},
	author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {[TLDR] This paper presents a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different G PT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation.},
}

@article{hofmann_ai_2024,
	title = {{AI} generates covertly racist decisions about people based on their dialect},
	volume = {633},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07856-5},
	doi = {10.1038/s41586-024-07856-5},
	abstract = {Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4–7. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models’ overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology.},
	language = {en},
	number = {8028},
	urldate = {2024-10-13},
	journal = {Nature},
	author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	pages = {147--154},
}

@misc{jiao_is_2023,
	title = {Is {ChatGPT} {A} {Good} {Translator}? {Yes} {With} {GPT}-4 {As} {The} {Engine}},
	shorttitle = {Is {ChatGPT} {A} {Good} {Translator}?},
	url = {https://arxiv.org/abs/2301.08745v4},
	abstract = {This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further, we explore an interesting strategy named \${\textbackslash}mathbf\{pivot{\textasciitilde}prompting\}\$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words, ChatGPT has already become a good translator. Please refer to our Github project for more details: https://github-com.cmu.idm.oclc.org/wxjiao/Is-ChatGPT-A-Good-Translator},
	language = {en},
	urldate = {2025-02-08},
	journal = {arXiv.org},
	author = {Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Wang, Xing and Shi, Shuming and Tu, Zhaopeng},
	month = jan,
	year = {2023},
}

@inproceedings{jorgensen-etal-2015-challenges,
    title = "Challenges of studying and processing dialects in social media",
    author = "J{\o}rgensen, Anna  and
      Hovy, Dirk  and
      S{\o}gaard, Anders",
    editor = "Xu, Wei  and
      Han, Bo  and
      Ritter, Alan",
    booktitle = "Proceedings of the Workshop on Noisy User-generated Text",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4302",
    doi = "10.18653/v1/W15-4302",
    pages = "9--18",
}

@misc{kantharuban_quantifying_2023,
	title = {Quantifying the {Dialect} {Gap} and its {Correlates} {Across} {Languages}},
	url = {http://arxiv.org/abs/2310.15135},
	doi = {10.48550/arXiv.2310.15135},
	abstract = {Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors. In this work, we conduct a comprehensive evaluation of the most influential, state-of-the-art large language models (LLMs) across two high-use applications, machine translation and automatic speech recognition, to assess their functionality on the regional dialects of several high- and low-resource languages. Additionally, we analyze how the regional dialect gap is correlated with economic, social, and linguistic factors. The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving the dialect gap. This work will lay the foundation for furthering the field of dialectal NLP by laying out evident disparities and identifying possible pathways for addressing them through mindful data collection.},
	language = {en},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Kantharuban, Anjali and Vulić, Ivan and Korhonen, Anna},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15135 [cs]},
	annote = {Comment: Accepted to EMNLP Findings 2023},
	file = {Kantharuban et al. - 2023 - Quantifying the Dialect Gap and its Correlates Acr.pdf:/Users/joelmire/Zotero/storage/DQI3X9SA/Kantharuban et al. - 2023 - Quantifying the Dialect Gap and its Correlates Acr.pdf:application/pdf},
}

@inproceedings{kirk_prism_2024,
	title = {The {PRISM} {Alignment} {Project}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	shorttitle = {The {PRISM} {Alignment} {Project}},
	url = {https://www.semanticscholar.org/paper/The-PRISM-Alignment-Project%3A-What-Participatory%2C-of-Kirk-Whitefield/60eb887f38b100d462be2f59d32676ec16bb4681?utm_content=title&utm_medium=unfurl&utm_source=slackbot},
	abstract = {Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.},
	urldate = {2024-04-29},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Rottger, Paul and Bean, Andrew M. and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	month = apr,
	year = {2024},
	keywords = {r/rlhf\_aae\_bias},
	annote = {[TLDR] This work introduces PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs, and demonstrates the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes.},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {https://arxiv.org/abs/2203.02155v1},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	urldate = {2025-02-08},
	journal = {arXiv.org},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
}

@article{posch_characterizing_2022,
	title = {Characterizing the {Global} {Crowd} {Workforce}: {A} {Cross}-{Country} {Comparison} of {Crowdworker} {Demographics}},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2330-8001},
	shorttitle = {Characterizing the {Global} {Crowd} {Workforce}},
	url = {https://hcjournal.org/index.php/jhc/article/view/106},
	doi = {10.15346/hc.v9i1.106},
	abstract = {Since its emergence roughly a decade ago, micro-task crowdsourcing has been attracting a heterogeneous set of workers from all over the globe. This paper sets out to explore the characteristics of the international crowd workforce to date and offers a cross-national comparison of crowdworker populations from ten hand-selected countries. We provide an analysis and comparison of demographic characteristics and shed light on the significance of micro-task income for workers situated in different national contexts. With over 11,000 individual responses, this study is the first large-scale country-level analysis of the characteristics of workers on the platform Figure Eight (formerly CrowdFlower), one of the two platforms dominating the micro-task market. We find large differences between the characteristics of the crowd workforces of different countries, both regarding demography and regarding the importance of micro-task income for workers. Furthermore, we find that the composition of the workforce in the ten countries was largely stable across samples taken at different points in time.},
	number = {1},
	urldate = {2024-10-16},
	journal = {Human Computation},
	author = {Posch, Lisa and Bleier, Arnim and Flöck, Fabian and Strohmaier, Markus},
	month = aug,
	year = {2022},
	annote = {[TLDR] This study is the first large-scale country-level analysis of the characteristics of workers on the platform Figure Eight (formerly CrowdFlower), one of the two platforms dominating the micro-task market.},
}

@inproceedings{rafailov_direct_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Direct preference optimization: your language model is secretly a reward model},
	shorttitle = {Direct preference optimization},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2024-10-12},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2024},
	pages = {53728--53741},
}

@inproceedings{robinson_chatgpt_2023,
	address = {Singapore},
	title = {{ChatGPT} {MT}: {Competitive} for {High}- (but {Not} {Low}-) {Resource} {Languages}},
	shorttitle = {{ChatGPT} {MT}},
	url = {https://aclanthology.org/2023.wmt-1.40/},
	doi = {10.18653/v1/2023.wmt-1.40},
	abstract = {Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs' MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world`s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1\% of languages we covered. Our analysis reveals that a language`s resource level is the most important feature in determining ChatGPT`s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Robinson, Nathaniel and Ogayo, Perez and Mortensen, David R. and Neubig, Graham},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	pages = {392--418},
}

@article{rosa_unsettling_2017,
	title = {Unsettling race and language: {Toward} a raciolinguistic perspective},
	volume = {46},
	issn = {0047-4045, 1469-8013},
	shorttitle = {Unsettling race and language},
	url = {https://www.cambridge.org/core/journals/language-in-society/article/unsettling-race-and-language-toward-a-raciolinguistic-perspective/30FFC5253F465905D75CDFF1C1363AE3},
	doi = {10.1017/S0047404517000562},
	abstract = {This article presents what we term a raciolinguistic perspective, which theorizes the historical and contemporary co-naturalization of language and race. Rather than taking for granted existing categories for parsing and classifying race and language, we seek to understand how and why these categories have been co-naturalized, and to imagine their denaturalization as part of a broader structural project of contesting white supremacy. We explore five key components of a raciolinguistic perspective: (i) historical and contemporary colonial co-naturalizations of race and language; (ii) perceptions of racial and linguistic difference; (iii) regimentations of racial and linguistic categories; (iv) racial and linguistic intersections and assemblages; and (v) contestations of racial and linguistic power formations. These foci reflect our investment in developing a careful theorization of various forms of racial and linguistic inequality on the one hand, and our commitment to the imagination and creation of more just societies on the other. (Race, language ideologies, colonialism, governmentality, enregisterment, structural inequality)*},
	language = {en},
	number = {5},
	urldate = {2024-10-13},
	journal = {Language in Society},
	author = {Rosa, Jonathan and Flores, Nelson},
	month = nov,
	year = {2017},
	pages = {621--647},
}

@inproceedings{ryan-etal-2024-unintended,
    title = "Unintended Impacts of {LLM} Alignment on Global Representation",
    author = "Ryan, Michael  and
      Held, William  and
      Yang, Diyi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.853",
    doi = "10.18653/v1/2024.acl-long.853",
    pages = "16121--16140",
    abstract = "Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.",
}

@inproceedings{sap_annotators_2022,
	address = {Seattle, United States},
	title = {Annotators with {Attitudes}: {How} {Annotator} {Beliefs} {And} {Identities} {Bias} {Toxic} {Language} {Detection}},
	shorttitle = {Annotators with {Attitudes}},
	url = {https://aclanthology.org/2022.naacl-main.431},
	doi = {10.18653/v1/2022.naacl-main.431},
	abstract = {The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and Zhou, Xuhui and Choi, Yejin and Smith, Noah A.},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	keywords = {r/rlhf\_aae\_bias},
	pages = {5884--5906},
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/P19-1163},
	doi = {10.18653/v1/P19-1163},
	abstract = {We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet's dialect they are significantly less likely to label the tweet as offensive.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	keywords = {r/rlhf\_aae\_bias},
	pages = {1668--1678},
}

@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {https://www.semanticscholar.org/paper/Proximal-Policy-Optimization-Algorithms-Schulman-Wolski/dce6f9d4017b1785979e7520fd0834ef8cf02f4b},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-10-14},
	journal = {ArXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = jul,
	year = {2017},
	annote = {[TLDR] A new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent, are proposed.},
}

@article{singhal_long_2023,
	title = {A {Long} {Way} to {Go}: {Investigating} {Length} {Correlations} in {RLHF}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {A {Long} {Way} to {Go}},
	url = {https://arxiv.org/abs/2310.03716},
	doi = {10.48550/ARXIV.2310.03716},
	abstract = {Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for "helpfulness" in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.},
	urldate = {2024-10-13},
	author = {Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	annote = {Other
21 pages, 13 figures, Accepted to COLM 2024},
}

@inbook{spears1998,
    author = {Spears, Arthur},
    year = {1998},
    month = {01},
    pages = {226-250},
    title = {African-American Language Use: Ideology and So-called Obscenity},
    isbn = {9781003165330},
    journal = {African-American English: Structure, History, and Use},
    doi = {10.4324/9781003165330-9}
}

@article{wang_helpsteer_2023,
	title = {{HelpSteer}: {Multi}-attribute {Helpfulness} {Dataset} for {SteerLM}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{HelpSteer}},
	url = {https://arxiv.org/abs/2311.09528},
	doi = {10.48550/ARXIV.2311.09528},
	abstract = {Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer},
	urldate = {2024-10-14},
	author = {Wang, Zhilin and Dong, Yi and Zeng, Jiaqi and Adams, Virginia and Sreedhar, Makesh Narsimhan and Egert, Daniel and Delalleau, Olivier and Scowcroft, Jane Polak and Kant, Neel and Swope, Aidan and Kuchaiev, Oleksii},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {[TLDR] HelpSteer is collected, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful, which produces a model that scores 7.54 on MT Bench, currently the highest score for open models that do not require training data from more powerful models.},
}

@inbook{wolfram2004varieties,
  title = {Social varieties of American English},
  url = {http://dx.doi.org/10.1017/CBO9780511809880.006},
  DOI = {10.1017/cbo9780511809880.006},
  booktitle = {Language in the USA},
  publisher = {Cambridge University Press},
  author = {Wolfram,  Walt},
  year = {2004},
  month = jun,
  pages = {58–75}
}

@inproceedings{xu_detoxifying_2021,
	address = {Online},
	title = {Detoxifying {Language} {Models} {Risks} {Marginalizing} {Minority} {Voices}},
	url = {https://aclanthology.org/2021.naacl-main.190},
	doi = {10.18653/v1/2021.naacl-main.190},
	abstract = {Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	keywords = {notion, r/rlhf\_aae\_bias},
	pages = {2390--2397},
}

@inproceedings{zhou2024relying,
  title={Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty},
  author={Zhou, Kaitlyn and Hwang, Jena D and Ren, Xiang and Sap, Maarten},
  booktitle={ACL},
  url={https://arxiv.org/abs/2401.06730},
  year={2024}
}

@inproceedings{zhou_challenges_2021,
	address = {Online},
	title = {Challenges in {Automated} {Debiasing} for {Toxic} {Language} {Detection}},
	url = {https://aclanthology.org/2021.eacl-main.274},
	doi = {10.18653/v1/2021.eacl-main.274},
	abstract = {Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	keywords = {r/rlhf\_aae\_bias},
	pages = {3143--3155},
}

