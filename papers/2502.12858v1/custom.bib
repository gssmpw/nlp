@inproceedings{ziems-etal-2023-multi,
    title = "Multi-{VALUE}: A Framework for Cross-Dialectal {E}nglish {NLP}",
    author = "Ziems, Caleb  and
      Held, William  and
      Yang, Jingfeng  and
      Dhamala, Jwala  and
      Gupta, Rahul  and
      Yang, Diyi",
    editor = "Rogers, Anna  and
      Boyd-Graber, Jordan  and
      Okazaki, Naoaki",
    booktitle = "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.acl-long.44",
    doi = "10.18653/v1/2023.acl-long.44",
    pages = "744--768",
    abstract = "Dialect differences caused by regional, social, and economic factors cause performance discrepancies for many groups of language technology users. Inclusive and equitable language technology must critically be dialect invariant, meaning that performance remains constant over dialectal shifts. Current systems often fall short of this ideal since they are designed and tested on a single dialect: Standard American English (SAE). We introduce a suite of resources for evaluating and achieving English dialect invariance. The resource is called Multi-VALUE, a controllable rule-based translation system spanning 50 English dialects and 189 unique linguistic features. Multi-VALUE maps SAE to synthetic forms of each dialect. First, we use this system to stress tests question answering, machine translation, and semantic parsing. Stress tests reveal significant performance disparities for leading models on non-standard dialects. Second, we use this system as a data augmentation technique to improve the dialect robustness of existing systems. Finally, we partner with native speakers of Chicano and Indian English to release new gold-standard variants of the popular CoQA task. To execute the transformation code, run model checkpoints, and download both synthetic and gold-standard dialectal benchmark datasets, see \url{http://value-nlp.org}.",
}

@inproceedings{sap_risk_2019,
	address = {Florence, Italy},
	title = {The {Risk} of {Racial} {Bias} in {Hate} {Speech} {Detection}},
	url = {https://aclanthology.org/P19-1163},
	doi = {10.18653/v1/P19-1163},
	abstract = {We investigate how annotators' insensitivity to differences in dialect can lead to racial bias in automatic hate speech detection models, potentially amplifying harm against minority populations. We first uncover unexpected correlations between surface markers of African American English (AAE) and ratings of toxicity in several widely-used hate speech datasets. Then, we show that models trained on these corpora acquire and propagate these biases, such that AAE tweets and tweets by self-identified African Americans are up to two times more likely to be labelled as offensive compared to others. Finally, we propose *dialect* and *race priming* as ways to reduce the racial bias in annotation, showing that when annotators are made explicitly aware of an AAE tweet's dialect they are significantly less likely to label the tweet as offensive.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Card, Dallas and Gabriel, Saadia and Choi, Yejin and Smith, Noah A.},
	editor = {Korhonen, Anna and Traum, David and Màrquez, Lluís},
	month = jul,
	year = {2019},
	keywords = {r/rlhf\_aae\_bias},
	pages = {1668--1678},
}
@inproceedings{zhou2024relying,
  title={Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty},
  author={Zhou, Kaitlyn and Hwang, Jena D and Ren, Xiang and Sap, Maarten},
  booktitle={ACL},
  url={https://arxiv.org/abs/2401.06730},
  year={2024}
}

@misc{lambert_rewardbench_2024,
	title = {{RewardBench}: {Evaluating} {Reward} {Models} for {Language} {Modeling}},
	shorttitle = {{RewardBench}},
	url = {http://arxiv.org/abs/2403.13787},
	doi = {10.48550/arXiv.2403.13787},
	abstract = {Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Lambert, Nathan and Pyatkin, Valentina and Morrison, Jacob and Miranda, L. J. and Lin, Bill Yuchen and Chandu, Khyathi and Dziri, Nouha and Kumar, Sachin and Zick, Tom and Choi, Yejin and Smith, Noah A. and Hajishirzi, Hannaneh},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13787 [cs]},
	keywords = {r/rlhf\_aae\_bias},
	annote = {Comment: 40 pages, 19 figures, 12 tables},
}

@inproceedings{rungta-etal-2022-geographic,
    title = "Geographic Citation Gaps in {NLP} Research",
    author = "Rungta, Mukund  and
      Singh, Janvijay  and
      Mohammad, Saif M.  and
      Yang, Diyi",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.89",
    doi = "10.18653/v1/2022.emnlp-main.89",
    pages = "1371--1383",
    abstract = "In a fair world, people have equitable opportunities to education, to conduct scientific research, to publish, and to get credit for their work, regardless of where they live. However, it is common knowledge among researchers that a vast number of papers accepted at top NLP venues come from a handful of western countries and (lately) China; whereas, very few papers from Africa and South America get published. Similar disparities are also believed to exist for paper citation counts. In the spirit of {``}what we do not measure, we cannot improve{''}, this work asks a series of questions on the relationship between geographical location and publication success (acceptance in top NLP venues and citation impact). We first created a dataset of 70,000 papers from the ACL Anthology, extracted their meta-information, andgenerated their citation network. We then show that not only are there substantial geographical disparities in paper acceptance and citation but also that these disparities persist even when controlling for a number of variables such as venue of publication and sub-field of NLP. Further, despite some steps taken by the NLP community to improve geographical diversity, we show that the disparity in publication metrics across locations is still on an increasing trend since the early 2000s. We release our code and dataset here: https://github.com/iamjanvijay/acl-cite-net",
}


@inproceedings{shah_predictive_2020,
	address = {Online},
	title = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}: {A} {Conceptual} {Framework} and {Overview}},
	shorttitle = {Predictive {Biases} in {Natural} {Language} {Processing} {Models}},
	url = {https://aclanthology.org/2020.acl-main.468},
	doi = {10.18653/v1/2020.acl-main.468},
	abstract = {An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Shah, Deven Santosh and Schwartz, H. Andrew and Hovy, Dirk},
	editor = {Jurafsky, Dan and Chai, Joyce and Schluter, Natalie and Tetreault, Joel},
	month = jul,
	year = {2020},
	keywords = {r/rlhf\_aae\_bias},
	pages = {5248--5264},
}

@inproceedings{zhou_challenges_2021,
	address = {Online},
	title = {Challenges in {Automated} {Debiasing} for {Toxic} {Language} {Detection}},
	url = {https://aclanthology.org/2021.eacl-main.274},
	doi = {10.18653/v1/2021.eacl-main.274},
	abstract = {Biased associations have been a challenge in the development of classifiers for detecting toxic language, hindering both fairness and accuracy. As potential solutions, we investigate recently introduced debiasing methods for text classification datasets and models, as applied to toxic language detection. Our focus is on lexical (e.g., swear words, slurs, identity mentions) and dialectal markers (specifically African American English). Our comprehensive experiments establish that existing methods are limited in their ability to prevent biased behavior in current toxicity detectors. We then propose an automatic, dialect-aware data correction method, as a proof-of-concept. Despite the use of synthetic labels, this method reduces dialectal associations with toxicity. Overall, our findings show that debiasing a model trained on biased toxic language data is not as effective as simply relabeling the data to remove existing biases.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 16th {Conference} of the {European} {Chapter} of the {Association} for {Computational} {Linguistics}: {Main} {Volume}},
	publisher = {Association for Computational Linguistics},
	author = {Zhou, Xuhui and Sap, Maarten and Swayamdipta, Swabha and Choi, Yejin and Smith, Noah},
	editor = {Merlo, Paola and Tiedemann, Jorg and Tsarfaty, Reut},
	month = apr,
	year = {2021},
	keywords = {r/rlhf\_aae\_bias},
	pages = {3143--3155},
}

@misc{gonen_demystifying_2022,
	title = {Demystifying {Prompts} in {Language} {Models} via {Perplexity} {Estimation}},
	url = {http://arxiv.org/abs/2212.04037},
	doi = {10.48550/arXiv.2212.04037},
	abstract = {Language models can be prompted to perform a wide variety of zero- and few-shot learning problems. However, performance varies significantly with the choice of prompt, and we do not yet understand why this happens or how to pick the best prompts. In this work, we analyze the factors that contribute to this variance and establish a new empirical hypothesis: the performance of a prompt is coupled with the extent to which the model is familiar with the language it contains. Over a wide range of tasks, we show that the lower the perplexity of the prompt is, the better the prompt is able to perform the task. As a result, we devise a method for creating prompts: (1) automatically extend a small seed set of manually written prompts by paraphrasing using GPT3 and backtranslation and (2) choose the lowest perplexity prompts to get significant gains in performance.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Gonen, Hila and Iyer, Srini and Blevins, Terra and Smith, Noah A. and Zettlemoyer, Luke},
	month = dec,
	year = {2022},
	note = {arXiv:2212.04037 [cs]},
	keywords = {r/rlhf\_aae\_bias},
}

@article{fisch2024robust,
  title={Robust preference optimization through reward model distillation},
  author={Fisch, Adam and Eisenstein, Jacob and Zayats, Vicky and Agarwal, Alekh and Beirami, Ahmad and Nagpal, Chirag and Shaw, Pete and Berant, Jonathan},
  journal={arXiv preprint arXiv:2405.19316},
  year={2024}
}

@inproceedings{kirk_prism_2024,
	title = {The {PRISM} {Alignment} {Project}: {What} {Participatory}, {Representative} and {Individualised} {Human} {Feedback} {Reveals} {About} the {Subjective} and {Multicultural} {Alignment} of {Large} {Language} {Models}},
	shorttitle = {The {PRISM} {Alignment} {Project}},
	url = {https://www.semanticscholar.org/paper/The-PRISM-Alignment-Project%3A-What-Participatory%2C-of-Kirk-Whitefield/60eb887f38b100d462be2f59d32676ec16bb4681?utm_content=title&utm_medium=unfurl&utm_source=slackbot},
	abstract = {Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.},
	urldate = {2024-04-29},
	author = {Kirk, Hannah Rose and Whitefield, Alexander and Rottger, Paul and Bean, Andrew M. and Margatina, Katerina and Ciro, Juan and Mosquera, Rafael and Bartolo, Max and Williams, Adina and He, He and Vidgen, Bertie and Hale, Scott A.},
	month = apr,
	year = {2024},
	keywords = {r/rlhf\_aae\_bias},
	annote = {[TLDR] This work introduces PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs, and demonstrates the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes.},
}

@inproceedings{feffer2023moral,
  title={Moral machine or tyranny of the majority?},
  author={Feffer, Michael and Heidari, Hoda and Lipton, Zachary C},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={5},
  pages={5974--5982},
  year={2023}
}

@inproceedings{huang2023culturally,
  title={Culturally aware natural language inference},
  author={Huang, Jing and Yang, Diyi},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
  pages={7591--7609},
  year={2023}
}

@article{kirk2023personalisation,
  title={Personalisation within bounds: A risk taxonomy and policy framework for the alignment of large language models with personalised feedback},
  author={Kirk, Hannah Rose and Vidgen, Bertie and R{\"o}ttger, Paul and Hale, Scott A},
  journal={arXiv preprint arXiv:2303.05453},
  year={2023}
}

@inproceedings{peng2023diagnosis,
  title={Diagnosis, feedback, adaptation: A human-in-the-loop framework for test-time policy adaptation},
  author={Peng, Andi and Netanyahu, Aviv and Ho, Mark K and Shu, Tianmin and Bobu, Andreea and Shah, Julie and Agrawal, Pulkit},
  booktitle={International Conference on Machine Learning},
  pages={27630--27641},
  year={2023},
  organization={PMLR}
}

@article{prabhakaran2021releasing,
  title={On releasing annotator-level labels and information in datasets},
  author={Prabhakaran, Vinodkumar and Davani, Aida Mostafazadeh and Diaz, Mark},
  journal={arXiv preprint arXiv:2110.05699},
  year={2021}
}

@inproceedings{bobu2024aligning,
  title={Aligning human and robot representations},
  author={Bobu, Andreea and Peng, Andi and Agrawal, Pulkit and Shah, Julie A and Dragan, Anca D},
  booktitle={Proceedings of the 2024 ACM/IEEE International Conference on Human-Robot Interaction},
  pages={42--54},
  year={2024}
}

@article{aich2024vernacular,
  title={Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping},
  author={Aich, Ankit and Liu, Tingting and Giorgi, Salvatore and Isman, Kelsey and Ungar, Lyle and Curtis, Brenda},
  journal={arXiv preprint arXiv:2406.12679},
  year={2024}
}

@book{baker2020linguistic,
  title={Linguistic justice: Black language, literacy, identity, and pedagogy},
  author={Baker-Bell, April},
  year={2020},
  publisher={Routledge}
}

@book{grieser2022black,
  title={The Black side of the river: Race, language, and belonging in Washington, DC},
  author={Grieser, Jessica A},
  year={2022},
  publisher={Georgetown University Press}
}

@book{green2002african,
  title={African American English: a linguistic introduction},
  author={Green, Lisa J},
  year={2002},
  publisher={Cambridge University Press}
}

@inproceedings{dacon-2022-towards,
    title = "Towards a Deep Multi-layered Dialectal Language Analysis: A Case Study of {A}frican-{A}merican {E}nglish",
    author = "Dacon, Jamell",
    editor = "Blodgett, Su Lin  and
      Daum{\'e} III, Hal  and
      Madaio, Michael  and
      Nenkova, Ani  and
      O'Connor, Brendan  and
      Wallach, Hanna  and
      Yang, Qian",
    booktitle = "Proceedings of the Second Workshop on Bridging Human--Computer Interaction and Natural Language Processing",
    month = jul,
    year = "2022",
    address = "Seattle, Washington",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.hcinlp-1.8",
    doi = "10.18653/v1/2022.hcinlp-1.8",
    pages = "55--63",
    abstract = "Currently, natural language processing (NLP) models proliferate language discrimination leading to potentially harmful societal impacts as a result of biased outcomes. For example, part-of-speech taggers trained on Mainstream American English (MAE) produce non-interpretable results when applied to African American English (AAE) as a result of language features not seen during training. In this work, we incorporate a human-in-the-loop paradigm to gain a better understanding of AAE speakers{'} behavior and their language use, and highlight the need for dialectal language inclusivity so that native AAE speakers can extensively interact with NLP systems while reducing feelings of disenfranchisement.",
}

@inproceedings{sap_annotators_2022,
	address = {Seattle, United States},
	title = {Annotators with {Attitudes}: {How} {Annotator} {Beliefs} {And} {Identities} {Bias} {Toxic} {Language} {Detection}},
	shorttitle = {Annotators with {Attitudes}},
	url = {https://aclanthology.org/2022.naacl-main.431},
	doi = {10.18653/v1/2022.naacl-main.431},
	abstract = {The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the *who*, *why*, and *what* behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (*who*) and beliefs (*why*), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle *what* is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Sap, Maarten and Swayamdipta, Swabha and Vianna, Laura and Zhou, Xuhui and Choi, Yejin and Smith, Noah A.},
	editor = {Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir},
	month = jul,
	year = {2022},
	keywords = {r/rlhf\_aae\_bias},
	pages = {5884--5906},
}

@misc{deas_evaluation_2023,
	title = {Evaluation of {African} {American} {Language} {Bias} in {Natural} {Language} {Generation}},
	url = {http://arxiv.org/abs/2305.14291},
	abstract = {Warning: This paper contains content and language that may be considered offensive to some readers. While biases disadvantaging African American Language (AAL) have been uncovered in models for tasks such as speech recognition and toxicity detection, there has been little investigation of these biases for language generation models like ChatGPT. We evaluate how well LLMs understand AAL in comparison to White Mainstream English (WME), the encouraged "standard" form of English taught in American classrooms. We measure large language model performance on two tasks: a counterpart generation task, where a model generates AAL given WME and vice versa, as well as a masked span prediction (MSP) task, where models predict a phrase hidden from their input. Using a novel dataset of AAL texts from a variety of regions and contexts, we present evidence of dialectal bias for six pre-trained LLMs through performance gaps on these tasks.},
	language = {en},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Deas, Nicholas and Grieser, Jessi and Kleiner, Shana and Patton, Desmond and Turcan, Elsbeth and McKeown, Kathleen},
	month = nov,
	year = {2023},
	note = {arXiv:2305.14291 [cs]},
	keywords = {notion, r/rlhf\_aae\_bias},
	annote = {Comment: EMNLP 2023 Camera-Ready},
	file = {2305.14291v2.pdf:/Users/joelmire/Downloads/2305.14291v2.pdf:application/pdf},
}

@inproceedings{blodgett_demographic_2016,
	address = {Austin, Texas},
	title = {Demographic {Dialectal} {Variation} in {Social} {Media}: {A} {Case} {Study} of {African}-{American} {English}},
	shorttitle = {Demographic {Dialectal} {Variation} in {Social} {Media}},
	url = {https://aclanthology.org/D16-1120},
	doi = {10.18653/v1/D16-1120},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Green, Lisa and O'Connor, Brendan},
	editor = {Su, Jian and Duh, Kevin and Carreras, Xavier},
	month = nov,
	year = {2016},
	keywords = {notion, r/rlhf\_aae\_bias},
	pages = {1119--1130},
}

@inproceedings{rafailov_direct_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {Direct preference optimization: your language model is secretly a reward model},
	shorttitle = {Direct preference optimization},
	abstract = {While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper, we leverage a mapping between reward functions and optimal policies to show that this constrained reward maximization problem can be optimized exactly with a single stage of policy training, essentially solving a classification problem on the human preference data. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for fitting a reward model, sampling from the LM during fine-tuning, or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds RLHF's ability to control sentiment of generations and improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.},
	urldate = {2024-10-12},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D. and Finn, Chelsea},
	month = may,
	year = {2024},
	pages = {53728--53741},
}

@misc{hofmann_dialect_2024,
	title = {Dialect prejudice predicts {AI} decisions about people's character, employability, and criminality},
	url = {http://arxiv.org/abs/2403.00742},
	doi = {10.48550/arXiv.2403.00742},
	abstract = {Hundreds of millions of people now interact with language models, with uses ranging from serving as a writing aid to informing hiring decisions. Yet these language models are known to perpetuate systematic racial prejudices, making their judgments biased in problematic ways about groups like African Americans. While prior research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice: we extend research showing that Americans hold raciolinguistic stereotypes about speakers of African American English and find that language models have the same prejudice, exhibiting covert stereotypes that are more negative than any human stereotypes about African Americans ever experimentally recorded, although closest to the ones from before the civil rights movement. By contrast, the language models' overt stereotypes about African Americans are much more positive. We demonstrate that dialect prejudice has the potential for harmful consequences by asking language models to make hypothetical decisions about people, based only on how they speak. Language models are more likely to suggest that speakers of African American English be assigned less prestigious jobs, be convicted of crimes, and be sentenced to death. Finally, we show that existing methods for alleviating racial bias in language models such as human feedback training do not mitigate the dialect prejudice, but can exacerbate the discrepancy between covert and overt stereotypes, by teaching language models to superficially conceal the racism that they maintain on a deeper level. Our findings have far-reaching implications for the fair and safe employment of language technology.},
	urldate = {2024-04-29},
	publisher = {arXiv},
	author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00742 [cs]},
	keywords = {notion, r/rlhf\_aae\_bias},
}

@inproceedings{dixon_measuring_2018,
	address = {New Orleans LA USA},
	title = {Measuring and {Mitigating} {Unintended} {Bias} in {Text} {Classification}},
	isbn = {978-1-4503-6012-8},
	url = {https://dl.acm.org/doi/10.1145/3278721.3278729},
	doi = {10.1145/3278721.3278729},
	language = {en},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2018 {AAAI}/{ACM} {Conference} on {AI}, {Ethics}, and {Society}},
	publisher = {ACM},
	author = {Dixon, Lucas and Li, John and Sorensen, Jeffrey and Thain, Nithum and Vasserman, Lucy},
	month = dec,
	year = {2018},
	keywords = {notion, r/rlhf\_aae\_bias},
	pages = {67--73},
}

@inproceedings{xu_detoxifying_2021,
	address = {Online},
	title = {Detoxifying {Language} {Models} {Risks} {Marginalizing} {Minority} {Voices}},
	url = {https://aclanthology.org/2021.naacl-main.190},
	doi = {10.18653/v1/2021.naacl-main.190},
	abstract = {Language models (LMs) must be both safe and equitable to be responsibly deployed in practice. With safety in mind, numerous detoxification techniques (e.g., Dathathri et al. 2020; Krause et al. 2020) have been proposed to mitigate toxic LM generations. In this work, we show that these detoxification techniques hurt equity: they decrease the utility of LMs on language used by marginalized groups (e.g., African-American English and minority identity mentions). In particular, we perform automatic and human evaluations of text generation quality when LMs are conditioned on inputs with different dialects and group identifiers. We find that detoxification makes LMs more brittle to distribution shift, especially on language used by marginalized groups. We identify that these failures stem from detoxification methods exploiting spurious correlations in toxicity datasets. Overall, our results highlight the tension between the controllability and distributional robustness of LMs.},
	urldate = {2024-04-29},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xu, Albert and Pathak, Eshaan and Wallace, Eric and Gururangan, Suchin and Sap, Maarten and Klein, Dan},
	editor = {Toutanova, Kristina and Rumshisky, Anna and Zettlemoyer, Luke and Hakkani-Tur, Dilek and Beltagy, Iz and Bethard, Steven and Cotterell, Ryan and Chakraborty, Tanmoy and Zhou, Yichao},
	month = jun,
	year = {2021},
	keywords = {notion, r/rlhf\_aae\_bias},
	pages = {2390--2397},
}

@MISC{Jones2020AAL,
  title        = "What's in a name? Why do some linguists not call it African
                  American Vernacular English ({AAVE}) anymore? —",
  author       = "Jones, Taylor",
  booktitle    = "Language Jones",
  month        =  dec,
  year         =  2020,
  howpublished = "\url{https://www.languagejones.com/blog-1/2020/12/21/whats-in-a-name-why-do-some-linguists-not-call-it-african-american-vernacular-english-aave-anymore}",
  note         = "Accessed: 2024-10-14",
  language     = "en"
}


@inproceedings{ryan-etal-2024-unintended,
    title = "Unintended Impacts of {LLM} Alignment on Global Representation",
    author = "Ryan, Michael  and
      Held, William  and
      Yang, Diyi",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.853",
    doi = "10.18653/v1/2024.acl-long.853",
    pages = "16121--16140",
    abstract = "Before being deployed for user-facing applications, developers align Large Language Models (LLMs) to user preferences through a variety of procedures, such as Reinforcement Learning From Human Feedback (RLHF) and Direct Preference Optimization (DPO). Current evaluations of these procedures focus on benchmarks of instruction following, reasoning, and truthfulness. However, human preferences are not universal, and aligning to specific preference sets may have unintended effects. We explore how alignment impacts performance along three axes of global representation: English dialects, multilingualism, and opinions from and about countries worldwide. Our results show that current alignment procedures create disparities between English dialects and global opinions. We find alignment improves capabilities in several languages. We conclude by discussing design decisions that led to these unintended impacts and recommendations for more equitable preference tuning. We make our code and data publicly available on Github.",
}

@article{casper2023open,
  title={Open problems and fundamental limitations of reinforcement learning from human feedback},
  author={Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, J{\'e}r{\'e}my and Rando, Javier and Freedman, Rachel and Korbak, Tomasz and Lindner, David and Freire, Pedro and others},
  journal={arXiv preprint arXiv:2307.15217},
  year={2023}
}

@article{blodgett2021sociolinguistically,
  title={Sociolinguistically driven approaches for just natural language processing},
  author={Blodgett, Su Lin},
  year={2021}
}

@inproceedings{blodgett_language_2020,
	address = {Online},
	title = {Language ({Technology}) is {Power}: {A} {Critical} {Survey} of “{Bias}” in {NLP}},
	shorttitle = {Language ({Technology}) is {Power}},
	url = {https://aclanthology.org/2020.acl-main.485},
	doi = {10.18653/v1/2020.acl-main.485},
	abstract = {We survey 146 papers analyzing “bias” in NLP systems, finding that their motivations are often vague, inconsistent, and lacking in normative reasoning, despite the fact that analyzing “bias” is an inherently normative process. We further find that these papers' proposed quantitative techniques for measuring or mitigating “bias” are poorly matched to their motivations and do not engage with the relevant literature outside of NLP. Based on these findings, we describe the beginnings of a path forward by proposing three recommendations that should guide work analyzing “bias” in NLP systems. These recommendations rest on a greater recognition of the relationships between language and social hierarchies, encouraging researchers and practitioners to articulate their conceptualizations of “bias”—i.e., what kinds of system behaviors are harmful, in what ways, to whom, and why, as well as the normative reasoning underlying these statements—and to center work around the lived experiences of members of communities affected by NLP systems, while interrogating and reimagining the power relations between technologists and such communities.},
	urldate = {2023-10-16},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Blodgett, Su Lin and Barocas, Solon and Daumé III, Hal and Wallach, Hanna},
	month = jul,
	year = {2020},
	keywords = {c/11711, notion, r/rlhf\_aae\_bias},
	pages = {5454--5476},
	file = {Full Text PDF:/Users/joelmire/Zotero/storage/8TZLS7PA/Blodgett et al. - 2020 - Language (Technology) is Power A Critical Survey .pdf:application/pdf},
}

@misc{hosking2024humanfeedbackgoldstandard,
      title={Human Feedback is not Gold Standard}, 
      author={Tom Hosking and Phil Blunsom and Max Bartolo},
      year={2024},
      eprint={2309.16349},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2309.16349}, 
}

@misc{aich2024vernacularibarelyknow,
      title={Vernacular? I Barely Know Her: Challenges with Style Control and Stereotyping}, 
      author={Ankit Aich and Tingting Liu and Salvatore Giorgi and Kelsey Isman and Lyle Ungar and Brenda Curtis},
      year={2024},
      eprint={2406.12679},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.12679}, 
}

@misc{wu2023stylesubstanceevaluationbiases,
      title={Style Over Substance: Evaluation Biases for Large Language Models}, 
      author={Minghao Wu and Alham Fikri Aji},
      year={2023},
      eprint={2307.03025},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.03025}, 
}

@misc{allam_biasdpo_2024,
	title = {{BiasDPO}: {Mitigating} {Bias} in {Language} {Models} through {Direct} {Preference} {Optimization}},
	shorttitle = {{BiasDPO}},
	url = {http://arxiv.org/abs/2407.13928},
	doi = {10.48550/arXiv.2407.13928},
	abstract = {Large Language Models (LLMs) have become pivotal in advancing natural language processing, yet their potential to perpetuate biases poses significant concerns. This paper introduces a new framework employing Direct Preference Optimization (DPO) to mitigate gender, racial, and religious biases in LLM-generated English text. By developing a loss function that favors less biased over biased completions, our approach cultivates a preference for respectful and non-discriminatory language in LLMs. We also contribute a manually designed dataset for training LLMs to recognize and correct biases. This dataset encompasses a diverse range of prompts paired with both biased and unbiased completions. Implementing this approach on the Microsoft Phi-2 model, we demonstrate substantial reductions in biased outputs as our model outperforms the baseline model on almost all bias benchmarks. Our model also achieves better performance compared to other open-source models on most benchmarks. By reducing biases in the language generated by the model, our study marks a significant step towards developing more ethical and socially responsible LLMs. We publicly release BiasDPO dataset on HuggingFace.},
	urldate = {2024-08-12},
	publisher = {arXiv},
	author = {Allam, Ahmed},
	month = jul,
	year = {2024},
	note = {arXiv:2407.13928 [cs]},
}


@article{ferrara_bias-aware_2024,
	title = {Bias-aware ranking from pairwise comparisons},
	volume = {38},
	issn = {1573-756X},
	url = {https://doi.org/10.1007/s10618-024-01024-z},
	doi = {10.1007/s10618-024-01024-z},
	abstract = {Human feedback is often used, either directly or indirectly, as input to algorithmic decision making. However, humans are biased: if the algorithm that takes as input the human feedback does not control for potential biases, this might result in biased algorithmic decision making, which can have a tangible impact on people’s lives. In this paper, we study how to detect and correct for evaluators’ bias in the task of ranking people (or items) from pairwise comparisons. Specifically, we assume we are given pairwise comparisons of the items to be ranked produced by a set of evaluators. While the pairwise assessments of the evaluators should reflect to a certain extent the latent (unobservable) true quality scores of the items, they might be affected by each evaluator’s own bias against, or in favor, of some groups of items. By detecting and amending evaluators’ biases, we aim to produce a ranking of the items that is, as much as possible, in accordance with the ranking one would produce by having access to the latent quality scores. Our proposal is a novel method that extends the classic Bradley-Terry model by having a bias parameter for each evaluator which distorts the true quality score of each item, depending on the group the item belongs to. Thanks to the simplicity of the model, we are able to write explicitly its log-likelihood w.r.t. the parameters (i.e., items’ latent scores and evaluators’ bias) and optimize by means of the alternating approach. Our experiments on synthetic and real-world data confirm that our method is able to reconstruct the bias of each single evaluator extremely well and thus to outperform several non-trivial competitors in the task of producing a ranking which is as much as possible close to the unbiased ranking.},
	language = {en},
	number = {4},
	urldate = {2024-08-12},
	journal = {Data Mining and Knowledge Discovery},
	author = {Ferrara, Antonio and Bonchi, Francesco and Fabbri, Francesco and Karimi, Fariba and Wagner, Claudia},
	month = jul,
	year = {2024},
	pages = {2062--2086},
}


@article{blodgett_racial_2017,
	title = {Racial {Disparity} in {Natural} {Language} {Processing}: {A} {Case} {Study} of {Social} {Media} {African}-{American} {English}},
	shorttitle = {Racial {Disparity} in {Natural} {Language} {Processing}},
	url = {https://www.semanticscholar.org/paper/Racial-Disparity-in-Natural-Language-Processing%3A-A-Blodgett-O%27Connor/59e94c9f21937643678ff494901f3d8b22af4e2f},
	abstract = {We highlight an important frontier in algorithmic fairness: disparity in the quality of natural language processing algorithms when applied to language from authors of different social groups. For example, current systems sometimes analyze the language of females and minorities more poorly than they do of whites and males. We conduct an empirical analysis of racial disparity in language identification for tweets written in African-American English, and discuss implications of disparity in NLP.},
	urldate = {2024-08-13},
	journal = {ArXiv},
	author = {Blodgett, Su Lin and O'Connor, Brendan T.},
	month = jun,
	year = {2017},
	annote = {[TLDR] An empirical analysis of racial disparity in language identification for tweets written in African-American English is conducted, and implications of disparity in NLP are discussed.},
}

@inproceedings{groenwold-etal-2020-investigating,
    title = "Investigating {A}frican-{A}merican {V}ernacular {E}nglish in Transformer-Based Text Generation",
    author = "Groenwold, Sophie and Ou, Lily and Parekh, Aesha and Honnavalli, Samhita and Levy, Sharon and Mirza, Diba and Wang, William Yang",
    booktitle = "Proceedings of EMNLP",
    url = "https://www.aclweb.org/anthology/2020.emnlp-main.473",
    year = "2020"
}

@inproceedings{
deas2024phonate,
title={Phon{AT}e: Impact of Type-Written Phonological Features of African American Language on Generative Language Modeling Tasks},
author={Nicholas Deas and Jessica A Grieser and Xinmeng Hou and Shana Kleiner and Tajh Martin and Sreya Nandanampati and Desmond U. Patton and Kathleen McKeown},
booktitle={First Conference on Language Modeling},
year={2024},
url={https://openreview.net/forum?id=rXEwxmnGQs}
}

@article{Monroe_Colaresi_Quinn_2017, title={Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict}, volume={16}, DOI={10.1093/pan/mpn018}, number={4}, journal={Political Analysis}, author={Monroe, Burt L. and Colaresi, Michael P. and Quinn, Kevin M.}, year={2017}, pages={372–403}}

@inproceedings{lui-baldwin-2011-cross,
    title = "Cross-domain Feature Selection for Language Identification",
    author = "Lui, Marco  and
      Baldwin, Timothy",
    editor = "Wang, Haifeng  and
      Yarowsky, David",
    booktitle = "Proceedings of 5th International Joint Conference on Natural Language Processing",
    month = nov,
    year = "2011",
    address = "Chiang Mai, Thailand",
    publisher = "Asian Federation of Natural Language Processing",
    url = "https://aclanthology.org/I11-1062",
    pages = "553--561",
}

@inproceedings{lui-baldwin-2012-langid,
    title = "langid.py: An Off-the-shelf Language Identification Tool",
    author = "Lui, Marco  and
      Baldwin, Timothy",
    editor = "Zhang, Min",
    booktitle = "Proceedings of the {ACL} 2012 System Demonstrations",
    month = jul,
    year = "2012",
    address = "Jeju Island, Korea",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P12-3005",
    pages = "25--30",
}

@inproceedings{jorgensen-etal-2015-challenges,
    title = "Challenges of studying and processing dialects in social media",
    author = "J{\o}rgensen, Anna  and
      Hovy, Dirk  and
      S{\o}gaard, Anders",
    editor = "Xu, Wei  and
      Han, Bo  and
      Ritter, Alan",
    booktitle = "Proceedings of the Workshop on Noisy User-generated Text",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W15-4302",
    doi = "10.18653/v1/W15-4302",
    pages = "9--18",
}

@inproceedings{davidson-etal-2019-racial,
    title = "Racial Bias in Hate Speech and Abusive Language Detection Datasets",
    author = "Davidson, Thomas  and
      Bhattacharya, Debasmita  and
      Weber, Ingmar",
    editor = "Roberts, Sarah T.  and
      Tetreault, Joel  and
      Prabhakaran, Vinodkumar  and
      Waseem, Zeerak",
    booktitle = "Proceedings of the Third Workshop on Abusive Language Online",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3504",
    doi = "10.18653/v1/W19-3504",
    pages = "25--35",
    abstract = "Technologies for abusive language detection are being developed and applied with little consideration of their potential biases. We examine racial bias in five different sets of Twitter data annotated for hate speech and abusive language. We train classifiers on these datasets and compare the predictions of these classifiers on tweets written in African-American English with those written in Standard American English. The results show evidence of systematic racial bias in all datasets, as classifiers trained on them tend to predict that tweets written in African-American English are abusive at substantially higher rates. If these abusive language detection systems are used in the field they will therefore have a disproportionate negative impact on African-American social media users. Consequently, these systems may discriminate against the groups who are often the targets of the abuse we are trying to detect.",
}


@article{hofmann_ai_2024,
	title = {{AI} generates covertly racist decisions about people based on their dialect},
	volume = {633},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07856-5},
	doi = {10.1038/s41586-024-07856-5},
	abstract = {Hundreds of millions of people now interact with language models, with uses ranging from help with writing1,2 to informing hiring decisions3. However, these language models are known to perpetuate systematic racial prejudices, making their judgements biased in problematic ways about groups such as African Americans4–7. Although previous research has focused on overt racism in language models, social scientists have argued that racism with a more subtle character has developed over time, particularly in the United States after the civil rights movement8,9. It is unknown whether this covert racism manifests in language models. Here, we demonstrate that language models embody covert racism in the form of dialect prejudice, exhibiting raciolinguistic stereotypes about speakers of African American English (AAE) that are more negative than any human stereotypes about African Americans ever experimentally recorded. By contrast, the language models’ overt stereotypes about African Americans are more positive. Dialect prejudice has the potential for harmful consequences: language models are more likely to suggest that speakers of AAE be assigned less-prestigious jobs, be convicted of crimes and be sentenced to death. Finally, we show that current practices of alleviating racial bias in language models, such as human preference alignment, exacerbate the discrepancy between covert and overt stereotypes, by superficially obscuring the racism that language models maintain on a deeper level. Our findings have far-reaching implications for the fair and safe use of language technology.},
	language = {en},
	number = {8028},
	urldate = {2024-10-13},
	journal = {Nature},
	author = {Hofmann, Valentin and Kalluri, Pratyusha Ria and Jurafsky, Dan and King, Sharese},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	pages = {147--154},
}


@article{singhal_long_2023,
	title = {A {Long} {Way} to {Go}: {Investigating} {Length} {Correlations} in {RLHF}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {A {Long} {Way} to {Go}},
	url = {https://arxiv.org/abs/2310.03716},
	doi = {10.48550/ARXIV.2310.03716},
	abstract = {Great success has been reported using Reinforcement Learning from Human Feedback (RLHF) to align large language models, with open preference datasets enabling wider experimentation, particularly for "helpfulness" in tasks like dialogue and web question answering. Alongside these improvements, however, RLHF also often drives models to produce longer outputs. This paper demonstrates, on three diverse settings, that optimizing for response length is, much more than previously thought, a significant factor behind RLHF. Studying the strategies RL optimization uses to maximize reward, we find improvements in reward to largely be driven by increasing response length, instead of other features. Indeed, we find that even a purely length-based reward reproduces most downstream RLHF improvements over supervised fine-tuned models. Testing a comprehensive set of length-countering interventions, we identify the dominant source of these biases to be reward models, which, by studying training dynamics, we find are non-robust and easily influenced by length biases in preference data.},
	urldate = {2024-10-13},
	author = {Singhal, Prasann and Goyal, Tanya and Xu, Jiacheng and Durrett, Greg},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 2},
	annote = {Other
21 pages, 13 figures, Accepted to COLM 2024},
}



@incollection{wolfram_chapter_2018,
	title = {Chapter 10. {Black} {Is}, {Black} {Isn}'t: {Perceptions} of {Language} and {Blackness}},
	isbn = {978-1-4696-3882-9},
	shorttitle = {Chapter 10. {Black} {Is}, {Black} {Isn}'t},
	url = {https://muse.jhu.edu/pub/12/edited_volume/chapter/2093291/pdf},
	language = {English},
	urldate = {2024-10-13},
	booktitle = {Language {Variety} in the {New} {South}: {Contemporary} {Perspectives} on {Change} and {Variation}},
	publisher = {The University of North Carolina Press},
	author = {Wolfram, Walt and Wojcik, Karissa and Wilbanks, Eric and Reaser, Jeffrey},
	year = {2018},
	pages = {203--222},
}



@article{roche_articulating_2019,
	title = {Articulating language oppression: colonialism, coloniality and the erasure of {Tibet}’s minority languages},
	volume = {53},
	issn = {0031-322X},
	shorttitle = {Articulating language oppression},
	url = {https://doi.org/10.1080/0031322X.2019.1662074},
	doi = {10.1080/0031322X.2019.1662074},
	abstract = {Roche’s article discusses ‘language oppression’ as a form of domination that is coherent with other forms of oppression along the lines of ‘race’, nation, colour and ethnicity. Scholars have defined language oppression as the ‘enforcement of language loss by physical, mental, social and spiritual coercion’. It is part of an evolving suite of concepts from linguistics, sociolinguistics and linguistic anthropology that examines issues of language discrimination, or ‘linguicism’. Roche explores one aspect of linguicism—language erasure—and how it relates to language oppression, focusing on Tibetans in the People’s Republic of China (PRC). He examines how language oppression is produced through practices of erasure: the ways in which certain populations and their languages are systematically rendered discursively invisible. He argues that the erasure of certain languages in the Tibetan context is systematically reproduced by two otherwise opposed political projects: the colonial project of the PRC state; and the international Tibet movement that seeks to resist it. He refers to the contingent cooperation between these two opposed projects as ‘articulated oppression’. In concluding the article he examines how the disarticulation of this oppression is a necessary condition for the emancipation of Tibet’s minority languages, and discusses the broader significance of this study for understanding language oppression, and its relation to other forms of oppression.},
	number = {5},
	urldate = {2024-10-13},
	journal = {Patterns of Prejudice},
	author = {Roche, Gerald},
	month = oct,
	year = {2019},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/0031322X.2019.1662074},
	pages = {487--514},
}



@article{rosa_unsettling_2017,
	title = {Unsettling race and language: {Toward} a raciolinguistic perspective},
	volume = {46},
	issn = {0047-4045, 1469-8013},
	shorttitle = {Unsettling race and language},
	url = {https://www.cambridge.org/core/journals/language-in-society/article/unsettling-race-and-language-toward-a-raciolinguistic-perspective/30FFC5253F465905D75CDFF1C1363AE3},
	doi = {10.1017/S0047404517000562},
	abstract = {This article presents what we term a raciolinguistic perspective, which theorizes the historical and contemporary co-naturalization of language and race. Rather than taking for granted existing categories for parsing and classifying race and language, we seek to understand how and why these categories have been co-naturalized, and to imagine their denaturalization as part of a broader structural project of contesting white supremacy. We explore five key components of a raciolinguistic perspective: (i) historical and contemporary colonial co-naturalizations of race and language; (ii) perceptions of racial and linguistic difference; (iii) regimentations of racial and linguistic categories; (iv) racial and linguistic intersections and assemblages; and (v) contestations of racial and linguistic power formations. These foci reflect our investment in developing a careful theorization of various forms of racial and linguistic inequality on the one hand, and our commitment to the imagination and creation of more just societies on the other. (Race, language ideologies, colonialism, governmentality, enregisterment, structural inequality)*},
	language = {en},
	number = {5},
	urldate = {2024-10-13},
	journal = {Language in Society},
	author = {Rosa, Jonathan and Flores, Nelson},
	month = nov,
	year = {2017},
	pages = {621--647},
}


@article{shelby_sociotechnical_2023,
	title = {Sociotechnical {Harms} of {Algorithmic} {Systems}: {Scoping} a {Taxonomy} for {Harm} {Reduction}},
	shorttitle = {Sociotechnical {Harms} of {Algorithmic} {Systems}},
	url = {https://dl.acm.org/doi/10.1145/3600211.3604673},
	doi = {10.1145/3600211.3604673},
	abstract = {Understanding the landscape of potential harms from algorithmic systems enables practitioners to better anticipate consequences of the systems they build. It also supports the prospect of incorporating controls to help minimize harms that emerge from the interplay of technologies and social and cultural dynamics. A growing body of scholarship has identified a wide range of harms across different algorithmic technologies. However, computing research and practitioners lack a high level and synthesized overview of harms from algorithmic systems. Based on a scoping review of computing research (n=172), we present an applied taxonomy of sociotechnical harms to support a more systematic surfacing of potential harms in algorithmic systems. The final taxonomy builds on and refers to existing taxonomies, classifications, and terminologies. Five major themes related to sociotechnical harms — representational, allocative, quality-of-service, interpersonal harms, and social system/societal harms — and sub-themes are presented along with a description of these categories. We conclude with a discussion of challenges and opportunities for future research.},
	language = {en},
	urldate = {2024-10-13},
	journal = {Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society},
	author = {Shelby, Renee and Rismani, Shalaleh and Henne, Kathryn and Moon, AJung and Rostamzadeh, Negar and Nicholas, Paul and Yilla-Akbari, N'Mah and Gallegos, Jess and Smart, Andrew and Garcia, Emilio and Virk, Gurleen},
	month = aug,
	year = {2023},
	note = {Conference Name: AIES '23: AAAI/ACM Conference on AI, Ethics, and Society
ISBN: 9798400702310
Place: Montr{\textbackslash}'\{e\}al QC Canada
Publisher: ACM},
	pages = {723--741},
	annote = {[TLDR] An applied taxonomy of sociotechnical harms is presented to support a more systematic surfacing of potential harms in algorithmic systems and builds on and refers to existing taxonomies, classifications, and terminologies.},
}



@article{christiano_deep_2017,
	title = {Deep {Reinforcement} {Learning} from {Human} {Preferences}},
	url = {https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd},
	abstract = {For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.},
	urldate = {2024-10-13},
	journal = {ArXiv},
	author = {Christiano, P. and Leike, J. and Brown, Tom B. and Martic, Miljan and Legg, S. and Amodei, Dario},
	month = jun,
	year = {2017},
	annote = {[TLDR] This work explores goals defined in terms of (non-expert) human preferences between pairs of trajectory segments in order to effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion.},
}


@article{mengesha_i_2021,
	title = {“{I} don’t {Think} {These} {Devices} are {Very} {Culturally} {Sensitive}.”—{Impact} of {Automated} {Speech} {Recognition} {Errors} on {African} {Americans}},
	volume = {4},
	issn = {2624-8212},
	url = {https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2021.725911/full},
	doi = {10.3389/frai.2021.725911},
	abstract = {{\textless}p{\textgreater}Automated speech recognition (ASR) converts language into text and is used across a variety of applications to assist us in everyday life, from powering virtual assistants, natural language conversations, to enabling dictation services. While recent work suggests that there are racial disparities in the performance of ASR systems for speakers of African American Vernacular English, little is known about the psychological and experiential effects of these failures paper provides a detailed examination of the behavioral and psychological consequences of ASR voice errors and the difficulty African American users have with getting their intents recognized. The results demonstrate that ASR failures have a negative, detrimental impact on African American users. Specifically, African Americans feel othered when using technology powered by ASR—errors surface thoughts about identity, namely about race and geographic location—leaving them feeling that the technology was not made for them. As a result, African Americans accommodate their speech to have better success with the technology. We incorporate the insights and lessons learned from sociolinguistics in our suggestions for linguistically responsive ways to build more inclusive voice systems that consider African American users’ needs, attitudes, and speech patterns. Our findings suggest that the use of a diary study can enable researchers to best understand the experiences and needs of communities who are often misunderstood by ASR. We argue this methodological framework could enable researchers who are concerned with fairness in AI to better capture the needs of all speakers who are traditionally misheard by voice-activated, artificially intelligent (voice-AI) digital systems.{\textless}/p{\textgreater}},
	language = {English},
	urldate = {2024-10-13},
	journal = {Frontiers in Artificial Intelligence},
	author = {Mengesha, Zion and Heldreth, Courtney and Lahav, Michal and Sublewski, Juliana and Tuennerman, Elyse},
	month = nov,
	year = {2021},
	note = {Publisher: Frontiers},
}



@article{craft_language_2020,
	title = {Language and {Discrimination}: {Generating} {Meaning}, {Perceiving} {Identities}, and {Discriminating} {Outcomes}},
	volume = {6},
	issn = {2333-9683, 2333-9691},
	shorttitle = {Language and {Discrimination}},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-linguistics-011718-011659},
	doi = {10.1146/annurev-linguistics-011718-011659},
	abstract = {Humans are remarkably efficient at parsing basic linguistic cues and show an equally impressive ability to produce and parse socially indexed cues from the language(s) they encounter. In this review, we focus on the ways in which questions of justice and equality are linked to these two abilities. We discuss how social and linguistic cues are theorized to become correlated with each other, describe listeners\&apos; perceptual abilities regarding linguistic and social cognition, and address how, in the context of these abilities, language mediates individuals’ negotiations with institutions and their agents—negotiations that often lead to discrimination or linguistic injustice. We review research that reports inequitable outcomes as a function of language use across education, employment, media, justice systems, housing markets, and health care institutions. Finally, we present paths forward for linguists to help fight against these discriminatory realities.},
	language = {en},
	number = {Volume 6, 2020},
	urldate = {2024-10-13},
	journal = {Annual Review of Linguistics},
	author = {Craft, Justin T. and Wright, Kelly E. and Weissler, Rachel Elizabeth and Queen, Robin M.},
	month = jan,
	year = {2020},
	note = {Publisher: Annual Reviews},
	pages = {389--407},
}



@inproceedings{cunningham_understanding_2024,
	address = {Bangkok, Thailand and virtual meeting},
	title = {Understanding the {Impacts} of {Language} {Technologies}' {Performance} {Disparities} on {African} {American} {Language} {Speakers}},
	url = {https://aclanthology.org/2024.findings-acl.761},
	doi = {10.18653/v1/2024.findings-acl.761},
	abstract = {This paper examines the experiences of African American Language (AAL) speakers when using language technologies. Previous work has used quantitative methods to uncover performance disparities between AAL speakers and White Mainstream English speakers when using language technologies, but has not sought to understand the impacts of these performance disparities on AAL speakers. Through interviews with 19 AAL speakers, we focus on understanding such impacts in a contextualized and human-centered manner. We find that AAL speakers often undertake invisible labor of adapting their speech patterns to successfully use language technologies, and they make connections between failures of language technologies for AAL speakers and a lack of inclusion of AAL speakers in language technology design processes and datasets. Our findings suggest that NLP researchers and practitioners should invest in developing contextualized and human-centered evaluations of language technologies that seek to understand the impacts of performance disparities on speakers of underrepresented languages and language varieties.},
	urldate = {2024-10-13},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics} {ACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Cunningham, Jay and Blodgett, Su Lin and Madaio, Michael and Daumé Iii, Hal and Harrington, Christina and Wallach, Hanna},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {12826--12833},
}


@inproceedings{harris_exploring_2022,
	address = {New York, NY, USA},
	series = {{FAccT} '22},
	title = {Exploring the {Role} of {Grammar} and {Word} {Choice} in {Bias} {Toward} {African} {American} {English} ({AAE}) in {Hate} {Speech} {Classification}},
	isbn = {978-1-4503-9352-2},
	url = {https://dl.acm.org/doi/10.1145/3531146.3533144},
	doi = {10.1145/3531146.3533144},
	abstract = {Language usage on social media varies widely even within the context of American English. Despite this, the majority of natural language processing systems are trained only on “Standard American English,” or SAE, the construction of English most prominent among white Americans. For hate speech classification, prior work has shown that African American English (AAE) is more likely to be misclassified as hate speech. This has harmful implications for Black social media users as it reinforces and exacerbates existing notions of anti-Black racism. While past work has highlighted the relationship between AAE and hate speech classification, no work has explored the linguistic characteristics of AAE that lead to misclassification. Our work uses Twitter datasets for AAE dialect and hate speech classifiers to explore the fine-grained relationship between specific characteristics of AAE such as word choice and grammatical features and hate speech predictions. We further investigate these biases by removing profanity and examining the influence of four aspects of AAE grammar that are distinct from SAE. Results show that removing profanity accounts for a roughly 20 to 30\% reduction in the percentage of samples classified as ’hate’ ’abusive’ or ’offensive,’ and that similar classification patterns are observed regardless of grammar categories.},
	urldate = {2024-10-13},
	booktitle = {Proceedings of the 2022 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Harris, Camille and Halevy, Matan and Howard, Ayanna and Bruckman, Amy and Yang, Diyi},
	month = jun,
	year = {2022},
	pages = {789--798},
}


@inproceedings{egede_for_2024,
	address = {New York, NY, USA},
	series = {{DIS} '24},
	title = {"{For} {Us} {By} {Us}": {Intentionally} {Designing} {Technology} for {Lived} {Black} {Experiences}},
	isbn = {9798400705830},
	shorttitle = {"{For} {Us} {By} {Us}"},
	url = {https://doi.org/10.1145/3643834.3661535},
	doi = {10.1145/3643834.3661535},
	abstract = {HCI research to date has only scratched the surface of the unique approaches racially minoritized communities take to building, designing, and using technology systems. While there has been an increase in understanding how people across racial groups create community across different platforms, there is still a lack of studies that explicitly center on how Black technologists design with and for their own communities. In this paper, we present findings from a series of semi-structured interviews with Black technologists who have used, created, or curated resources to support lived Black experiences. From their experiences, we find a multifaceted approach to design as a means of survival, to stay connected, for cultural significance, and to bask in celebratory joy. Further, we provide considerations that emphasize the need for centering lived Black experiences in design and share approaches that can empower the broader research community to conduct further inquiries into design focused on those in the margins.},
	urldate = {2024-10-13},
	booktitle = {Proceedings of the 2024 {ACM} {Designing} {Interactive} {Systems} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Egede, Lisa and Coney, Leslie and Johnson, Brittany and Harrington, Christina and Ford, Denae},
	month = jul,
	year = {2024},
	pages = {3210--3224},
}


@misc{fleisig2024linguisticbiaschatgptlanguage,
      title={Linguistic Bias in ChatGPT: Language Models Reinforce Dialect Discrimination}, 
      author={Eve Fleisig and Genevieve Smith and Madeline Bossi and Ishita Rustagi and Xavier Yin and Dan Klein},
      year={2024},
      eprint={2406.08818},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.08818}, 
}


@misc{OpenOrca,
  title = {OpenOrca: An Open Dataset of GPT Augmented FLAN Reasoning Traces},
  author = {Wing Lian and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  year = {2023},
  publisher = {HuggingFace},
  journal = {HuggingFace repository},
  howpublished = {\url{https://https://huggingface.co/Open-Orca/OpenOrca}},
}


@misc{dong2024rlhf,
      title={RLHF Workflow: From Reward Modeling to Online RLHF}, 
      author={Hanze Dong and Wei Xiong and Bo Pang and Haoxiang Wang and Han Zhao and Yingbo Zhou and Nan Jiang and Doyen Sahoo and Caiming Xiong and Tong Zhang},
      year={2024},
      eprint={2405.07863},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{daniele2023amplify-instruct,
  title={Amplify-Instruct: Synthetically Generated Diverse Multi-turn Conversations for efficient LLM Training.},
  author={Daniele, Luigi and Suphavadeeprasit},
  journal={arXiv preprint arXiv:(coming soon)},
  url={https://huggingface.co/datasets/LDJnr/Capybara},
  year={2023}
}

@misc{kim2023prometheus,
    title={Prometheus: Inducing Fine-grained Evaluation Capability in Language Models},
    author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo},
    year={2023},
    eprint={2310.08491},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{kim2024prometheus,
    title={Prometheus 2: An Open Source Language Model Specialized in Evaluating Other Language Models},
    author={Seungone Kim and Juyoung Suk and Shayne Longpre and Bill Yuchen Lin and Jamin Shin and Sean Welleck and Graham Neubig and Moontae Lee and Kyungjae Lee and Minjoon Seo},
    year={2024},
    eprint={2405.01535},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}

@misc{open_hermes_preferences,
  author = {Shengyi Costa Huang and Agustín Piqueres and Kashif Rasul and Philipp Schmid and Daniel Vila and Lewis Tunstall},
  title = {Open Hermes Preferences},
  year = {2024},
  publisher = {Argilla & Hugging Face},
  journal = {Hugging Face repository},
  howpublished = {\url{https://huggingface.co/datasets/argilla/OpenHermesPreferences}}
}

@misc{cui2023ultrafeedback,
      title={UltraFeedback: Boosting Language Models with High-quality Feedback}, 
      author={Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
      year={2023},
      eprint={2310.01377},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{guo2024controllable,
  title={Controllable Preference Optimization: Toward Controllable Multi-Objective Alignment},
  author={Guo, Yiju and Cui, Ganqu and Yuan, Lifan and Ding, Ning and Wang, Jiexin and Chen, Huimin and Sun, Bowen and Xie, Ruobing and Zhou, Jie and Lin, Yankai and others},
  journal={arXiv preprint arXiv:2402.19085},
  year={2024}
}

@misc{dong2023steerlm,
      title={SteerLM: Attribute Conditioned SFT as an (User-Steerable) Alternative to RLHF}, 
      author={Yi Dong and Zhilin Wang and Makesh Narsimhan Sreedhar and Xianchao Wu and Oleksii Kuchaiev},
      year={2023},
      eprint={2310.05344},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{hovy2021,
author = {Hovy, Dirk and Prabhumoye, Shrimai},
title = {Five sources of bias in natural language processing},
journal = {Language and Linguistics Compass},
volume = {15},
number = {8},
pages = {e12432},
doi = {https://doi.org/10.1111/lnc3.12432},
url = {https://compass.onlinelibrary.wiley.com/doi/abs/10.1111/lnc3.12432},
eprint = {https://compass.onlinelibrary.wiley.com/doi/pdf/10.1111/lnc3.12432},
abstract = {Abstract Recently, there has been an increased interest in demographically grounded bias in natural language processing (NLP) applications. Much of the recent work has focused on describing bias and providing an overview of bias in a larger context. Here, we provide a simple, actionable summary of this recent work. We outline five sources where bias can occur in NLP systems: (1) the data, (2) the annotation process, (3) the input representations, (4) the models, and finally (5) the research design (or how we conceptualize our research). We explore each of the bias sources in detail in this article, including examples and links to related work, as well as potential counter-measures.},
year = {2021}
}
@InProceedings{pmlr-v162-ethayarajh22a,
  title = 	 {Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information},
  author =       {Ethayarajh, Kawin and Choi, Yejin and Swayamdipta, Swabha},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {5988--6008},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher = {PMLR},
}

@misc{tunstall2023zephyr,
      title={Zephyr: Direct Distillation of LM Alignment}, 
      author={Lewis Tunstall and Edward Beeching and Nathan Lambert and Nazneen Rajani and Kashif Rasul and Younes Belkada and Shengyi Huang and Leandro von Werra and Clémentine Fourrier and Nathan Habib and Nathan Sarrazin and Omar Sanseviero and Alexander M. Rush and Thomas Wolf},
      year={2023},
      eprint={2310.16944},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{xiong2023gibbs,
  title={Gibbs sampling from human feedback: A provable kl-constrained framework for rlhf},
  author={Xiong, Wei and Dong, Hanze and Ye, Chenlu and Zhong, Han and Jiang, Nan and Zhang, Tong},
  journal={arXiv preprint arXiv:2312.11456},
  year={2023}
}

@article{bai_training_2022,
	title = {Training a {Helpful} and {Harmless} {Assistant} with {Reinforcement} {Learning} from {Human} {Feedback}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2204.05862},
	doi = {10.48550/ARXIV.2204.05862},
	abstract = {We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.},
	urldate = {2024-10-14},
	author = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and Joseph, Nicholas and Kadavath, Saurav and Kernion, Jackson and Conerly, Tom and El-Showk, Sheer and Elhage, Nelson and Hatfield-Dodds, Zac and Hernandez, Danny and Hume, Tristan and Johnston, Scott and Kravec, Shauna and Lovitt, Liane and Nanda, Neel and Olsson, Catherine and Amodei, Dario and Brown, Tom and Clark, Jack and McCandlish, Sam and Olah, Chris and Mann, Ben and Kaplan, Jared},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {Other
Data available at https://github.com/anthropics/hh-rlhf},
}


@article{wang_helpsteer_2023,
	title = {{HelpSteer}: {Multi}-attribute {Helpfulness} {Dataset} for {SteerLM}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{HelpSteer}},
	url = {https://arxiv.org/abs/2311.09528},
	doi = {10.48550/ARXIV.2311.09528},
	abstract = {Existing open-source helpfulness preference datasets do not specify what makes some responses more helpful and others less so. Models trained on these datasets can incidentally learn to model dataset artifacts (e.g. preferring longer but unhelpful responses only due to their length). To alleviate this problem, we collect HelpSteer, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful. Specifically, our 37k-sample dataset has annotations for correctness, coherence, complexity, and verbosity in addition to overall helpfulness of responses. Training Llama 2 70B using the HelpSteer dataset with SteerLM technique produces a model that scores 7.54 on MT Bench, which is currently the highest score for open models that do not require training data from more powerful models (e.g. GPT4). We release this dataset with CC-BY-4.0 license at https://huggingface.co/datasets/nvidia/HelpSteer},
	urldate = {2024-10-14},
	author = {Wang, Zhilin and Dong, Yi and Zeng, Jiaqi and Adams, Virginia and Sreedhar, Makesh Narsimhan and Egert, Daniel and Delalleau, Olivier and Scowcroft, Jane Polak and Kant, Neel and Swope, Aidan and Kuchaiev, Oleksii},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {[TLDR] HelpSteer is collected, a multi-attribute helpfulness dataset annotated for the various aspects that make responses helpful, which produces a model that scores 7.54 on MT Bench, currently the highest score for open models that do not require training data from more powerful models.},
}


@article{schulman_proximal_2017,
	title = {Proximal {Policy} {Optimization} {Algorithms}},
	url = {https://www.semanticscholar.org/paper/Proximal-Policy-Optimization-Algorithms-Schulman-Wolski/dce6f9d4017b1785979e7520fd0834ef8cf02f4b},
	abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
	urldate = {2024-10-14},
	journal = {ArXiv},
	author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
	month = jul,
	year = {2017},
	annote = {[TLDR] A new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent, are proposed.},
}

@inbook{spears1998,
    author = {Spears, Arthur},
    year = {1998},
    month = {01},
    pages = {226-250},
    title = {African-American Language Use: Ideology and So-called Obscenity},
    isbn = {9781003165330},
    journal = {African-American English: Structure, History, and Use},
    doi = {10.4324/9781003165330-9}
}

@book{alim2016,
    author = {Alim, H. Samy and Rickford, John R. and Ball, Arnetha F.},
    title = "{Raciolinguistics: How Language Shapes Our Ideas About Race}",
    publisher = {Oxford University Press},
    year = {2016},
    month = {11},
    abstract = "{Raciolinguistics reveals the central role that language plays in shaping our ideas about race and vice versa. The book brings together a team of leading scholars—working both within and beyond the United States—to share research that helps us understand the increasingly vexed relationships between race, ethnicity, and language. Combining the innovative, cutting-edge approaches of race and ethnic studies with fine-grained linguistic analyses, chapters cover a wide range of topics, including the language use of African American Jews and the struggle over the very term “African American,” racialized language education debates within “majority-minority” immigrant communities as well as indigenous communities in the United States, the dangers of multicultural education in a Europe that is struggling to meet the needs of new migrants, and the sociopolitical and cultural meanings of linguistic styles used in Brazilian favelas, South African townships, Israeli neighborhoods, Mexican and Puerto Rican barrios in Chicago, and Korean American “cram schools.”In examining changing demographics in the United States—such as population resegregation, Asian and Latino patterns of immigration, and new African American (im)migration patterns, along with changing global cultural and media trends (like Hip Hop and social media), Raciolinguistics shapes the future of studies on race, ethnicity, and language. By taking a comparative look across a diverse range of language and literacy contexts, the volume seeks not only to set the research agenda in this burgeoning area but also to help resolve pressing educational and political problems in some of the most contested raciolinguistic contexts in the world.}",
    isbn = {9780190625696},
    doi = {10.1093/acprof:oso/9780190625696.001.0001},
    url = {https://doi.org/10.1093/acprof:oso/9780190625696.001.0001},
}

@inproceedings{brewer2023,
    author = {Brewer, Robin N. and Harrington, Christina and Heldreth, Courtney},
    title = {Envisioning Equitable Speech Technologies for Black Older Adults},
    year = {2023},
    isbn = {9798400701924},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3593013.3594005},
    doi = {10.1145/3593013.3594005},
    abstract = {There is increasing concern that how researchers currently define and measure fairness is inadequate. Recent calls push to move beyond traditional concepts of fairness and consider related constructs through qualitative and community-based approaches, particularly for underrepresented communities most at-risk for AI harm. One in context, previous research has identified that voice technologies are unfair due to racial and age disparities. This paper uses voice technologies as a case study to unpack how Black older adults value and envision fair and equitable AI systems. We conducted design workshops and interviews with 16 Black older adults, exploring how participants envisioned voice technologies that better understand cultural context and mitigate cultural dissonance. Our findings identify tensions between what it means to have fair, inclusive, and representative voice technologies. This research raises questions about how and whether researchers can model cultural representation with large language models.},
    booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
    pages = {379–388},
    numpages = {10},
    location = {Chicago, IL, USA},
    series = {FAccT '23}
}

@misc{wu2023pairwiseproximalpolicyoptimization,
      title={Pairwise Proximal Policy Optimization: Harnessing Relative Feedback for LLM Alignment}, 
      author={Tianhao Wu and Banghua Zhu and Ruoyu Zhang and Zhaojin Wen and Kannan Ramchandran and Jiantao Jiao},
      year={2023},
      eprint={2310.00212},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2310.00212}, 
}

@inproceedings{gururangan-etal-2022-whose,
    title = "Whose Language Counts as High Quality? Measuring Language Ideologies in Text Data Selection",
    author = "Gururangan, Suchin  and
      Card, Dallas  and
      Dreier, Sarah  and
      Gade, Emily  and
      Wang, Leroy  and
      Wang, Zeyu  and
      Zettlemoyer, Luke  and
      Smith, Noah A.",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.165",
    doi = "10.18653/v1/2022.emnlp-main.165",
    pages = "2562--2580",
    abstract = "Language models increasingly rely on massive web crawls for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and news often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality filtering. Using a new dataset of U.S. high school newspaper articles{---}written by students from across the country{---}we investigate whose language is preferred by the quality filter used for GPT-3. We find that newspapers from larger schools, located in wealthier, educated, and urban zones (ZIP codes) are more likely to be classified as high quality. We also show that this quality measurement is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justification for the inclusion or exclusion of various texts.",
}

@misc{Nous-Hermes-2-Mistral-7B-DPO, 
      url={[https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO](https://huggingface.co/NousResearch/Nous-Hermes-2-Mistral-7B-DPO)}, 
      title={Nous Hermes 2 Mistral 7B DPO}, 
      author={"Teknium and theemozilla and karan4d and huemin\_art"}
}
@article{bellagente2024stable,
  title={Stable LM 2 1.6 B Technical Report},
  author={Bellagente, Marco and Tow, Jonathan and Mahan, Dakota and Phung, Duy and Zhuravinskyi, Maksym and Adithyan, Reshinth and Baicoianu, James and Brooks, Ben and Cooper, Nathan and Datta, Ashish and others},
  journal={arXiv preprint arXiv:2402.17834},
  year={2024}
}

@misc{park2024offsetbias,
      title={OffsetBias: Leveraging Debiased Data for Tuning Evaluators},
      author={Junsoo Park and Seungyeon Jwa and Meiying Ren and Daeyoung Kim and Sanghyuk Choi},
      year={2024},
      eprint={2407.06551},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@inbook{wolfram2004varieties,
  title = {Social varieties of American English},
  url = {http://dx.doi.org/10.1017/CBO9780511809880.006},
  DOI = {10.1017/cbo9780511809880.006},
  booktitle = {Language in the USA},
  publisher = {Cambridge University Press},
  author = {Wolfram,  Walt},
  year = {2004},
  month = jun,
  pages = {58–75}
}

@inproceedings{wenzel2023microaggressions,
author = {Wenzel, Kimi and Devireddy, Nitya and Davison, Cam and Kaufman, Geoff},
title = {Can Voice Assistants Be Microaggressors? Cross-Race Psychological Responses to Failures of Automatic Speech Recognition},
year = {2023},
isbn = {9781450394215},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3544548.3581357},
doi = {10.1145/3544548.3581357},
abstract = {Language technologies have a racial bias, committing greater errors for Black users than for white users. However, little work has evaluated what effect these disparate error rates have on users themselves. The present study aims to understand if speech recognition errors in human-computer interactions may mirror the same effects as misunderstandings in interpersonal cross-race communication. In a controlled experiment (N=108), we randomly assigned Black and white participants to interact with a voice assistant pre-programmed to exhibit a high versus low error rate. Results revealed that Black participants in the high error rate condition, compared to Black participants in the low error rate condition, exhibited significantly higher levels of self-consciousness, lower levels of self-esteem and positive affect, and less favorable ratings of the technology. White participants did not exhibit this disparate pattern. We discuss design implications and the diverse research directions to which this initial study aims to contribute.},
booktitle = {Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems},
articleno = {109},
numpages = {14},
keywords = {Automated Speech Recognition, Harm, Individual Differences, Language Technology, Microaggressions, Quantitative Methods, Race, Voice Assistants, Wizard-of-Oz},
location = {Hamburg, Germany},
series = {CHI '23}
}

@article{friedman1996vsd,
  title = {Value-sensitive design},
  volume = {3},
  ISSN = {1558-3449},
  url = {http://dx.doi.org/10.1145/242485.242493},
  DOI = {10.1145/242485.242493},
  number = {6},
  journal = {Interactions},
  publisher = {Association for Computing Machinery (ACM)},
  author = {Friedman,  Batya},
  year = {1996},
  month = dec,
  pages = {16–23}
}

@article{ganguli2022red,
  title={Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned},
  author={Ganguli, Deep and Lovitt, Liane and Kernion, Jackson and Askell, Amanda and Bai, Yuntao and Kadavath, Saurav and Mann, Ben and Perez, Ethan and Schiefer, Nicholas and Ndousse, Kamal and others},
  journal={arXiv preprint arXiv:2209.07858},
  year={2022}
}

@misc{cai2024internlm2,
      title={InternLM2 Technical Report},
      author={Zheng Cai and Maosong Cao and Haojiong Chen and Kai Chen and Keyu Chen and Xin Chen and Xun Chen and Zehui Chen and Zhi Chen and Pei Chu and Xiaoyi Dong and Haodong Duan and Qi Fan and Zhaoye Fei and Yang Gao and Jiaye Ge and Chenya Gu and Yuzhe Gu and Tao Gui and Aijia Guo and Qipeng Guo and Conghui He and Yingfan Hu and Ting Huang and Tao Jiang and Penglong Jiao and Zhenjiang Jin and Zhikai Lei and Jiaxing Li and Jingwen Li and Linyang Li and Shuaibin Li and Wei Li and Yining Li and Hongwei Liu and Jiangning Liu and Jiawei Hong and Kaiwen Liu and Kuikun Liu and Xiaoran Liu and Chengqi Lv and Haijun Lv and Kai Lv and Li Ma and Runyuan Ma and Zerun Ma and Wenchang Ning and Linke Ouyang and Jiantao Qiu and Yuan Qu and Fukai Shang and Yunfan Shao and Demin Song and Zifan Song and Zhihao Sui and Peng Sun and Yu Sun and Huanze Tang and Bin Wang and Guoteng Wang and Jiaqi Wang and Jiayu Wang and Rui Wang and Yudong Wang and Ziyi Wang and Xingjian Wei and Qizhen Weng and Fan Wu and Yingtong Xiong and Chao Xu and Ruiliang Xu and Hang Yan and Yirong Yan and Xiaogui Yang and Haochen Ye and Huaiyuan Ying and Jia Yu and Jing Yu and Yuhang Zang and Chuyu Zhang and Li Zhang and Pan Zhang and Peng Zhang and Ruijie Zhang and Shuo Zhang and Songyang Zhang and Wenjian Zhang and Wenwei Zhang and Xingcheng Zhang and Xinyue Zhang and Hui Zhao and Qian Zhao and Xiaomeng Zhao and Fengzhe Zhou and Zaida Zhou and Jingming Zhuo and Yicheng Zou and Xipeng Qiu and Yu Qiao and Dahua Lin},
      year={2024},
      eprint={2403.17297},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{yuan2024advancing,
      title={Advancing LLM Reasoning Generalists with Preference Trees}, 
      author={Lifan Yuan and Ganqu Cui and Hanbin Wang and Ning Ding and Xingyao Wang and Jia Deng and Boji Shan and Huimin Chen and Ruobing Xie and Yankai Lin and Zhenghao Liu and Bowen Zhou and Hao Peng and Zhiyuan Liu and Maosong Sun},
      year={2024},
      eprint={2404.02078},
      archivePrefix={arXiv},
}
@article{dong2023raft,
  title={Raft: Reward ranked finetuning for generative foundation model alignment},
  author={Dong, Hanze and Xiong, Wei and Goyal, Deepanshu and Pan, Rui and Diao, Shizhe and Zhang, Jipeng and Shum, Kashun and Zhang, Tong},
  journal={arXiv preprint arXiv:2304.06767},
  year={2023}
}
@misc{xiong2024iterative,
      title={Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint}, 
      author={Wei Xiong and Hanze Dong and Chenlu Ye and Ziqi Wang and Han Zhong and Heng Ji and Nan Jiang and Tong Zhang},
      year={2024},
      eprint={2312.11456},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}
@article{yang2024regularizing,
  title={Regularizing Hidden States Enables Learning Generalizable Reward Model for LLMs},
  author={Yang, Rui and Ding, Ruomeng and Lin, Yong and Zhang, Huan and Zhang, Tong},
  journal={arXiv preprint arXiv:2406.10216},
  year={2024}
}
@misc{ivison2023camels,
      title={Camels in a Changing Climate: Enhancing LM Adaptation with Tulu 2}, 
      author={Hamish Ivison and Yizhong Wang and Valentina Pyatkin and Nathan Lambert and Matthew Peters and Pradeep Dasigi and Joel Jang and David Wadden and Noah A. Smith and Iz Beltagy and Hannaneh Hajishirzi},
      year={2023},
      eprint={2311.10702},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{kim2024sdpo,
      title={sDPO: Don't Use Your Data All at Once}, 
      author={Dahyun Kim and Yungi Kim and Wonho Song and Hyeonwoo Kim and Yunsu Kim and Sanghoon Kim and Chanjun Park},
      year={2024},
      eprint={2403.19270},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{qwen2,
  title={Qwen2 Technical Report},
  year={2024}
}

@misc{argilla_dpo_mix_7k,
  title = {DPO-Mix-7k Dataset},
  author = {Argilla},
  year = {2024},
  url = {https://huggingface.co/datasets/argilla/dpo-mix-7k},
}

@article{jiang2023llm,
  title={Llm-blender: Ensembling large language models with pairwise ranking and generative fusion},
  author={Jiang, Dongfu and Ren, Xiang and Lin, Bill Yuchen},
  journal={arXiv preprint arXiv:2306.02561},
  year={2023}
}
@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
} 

@misc{ivison2024unpacking,
      title={{Unpacking DPO and PPO: Disentangling Best Practices for Learning from Preference Feedback}}, 
      author={Hamish Ivison and Yizhong Wang and Jiacheng Liu and Ellen Wu and Valentina Pyatkin and Nathan Lambert and Yejin Choi and Noah A. Smith and Hannaneh Hajishirzi},
      year={2024},
      eprint={2406.09279},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@article{qwen,
  title={Qwen Technical Report},
  author={Jinze Bai and Shuai Bai and Yunfei Chu and Zeyu Cui and Kai Dang and Xiaodong Deng and Yang Fan and Wenbin Ge and Yu Han and Fei Huang and Binyuan Hui and Luo Ji and Mei Li and Junyang Lin and Runji Lin and Dayiheng Liu and Gao Liu and Chengqiang Lu and Keming Lu and Jianxin Ma and Rui Men and Xingzhang Ren and Xuancheng Ren and Chuanqi Tan and Sinan Tan and Jianhong Tu and Peng Wang and Shijie Wang and Wei Wang and Shengguang Wu and Benfeng Xu and Jin Xu and An Yang and Hao Yang and Jian Yang and Shusheng Yang and Yang Yao and Bowen Yu and Hongyi Yuan and Zheng Yuan and Jianwei Zhang and Xingxuan Zhang and Yichang Zhang and Zhenru Zhang and Chang Zhou and Jingren Zhou and Xiaohuan Zhou and Tianhang Zhu},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}
@misc{kim2023solar,
      title={SOLAR 10.7B: Scaling Large Language Models with Simple yet Effective Depth Up-Scaling}, 
      author={Dahyun Kim and Chanjun Park and Sanghoon Kim and Wonsung Lee and Wonho Song and Yunsu Kim and Hyeonwoo Kim and Yungi Kim and Hyeonju Lee and Jihoo Kim and Changbae Ahn and Seonghoon Yang and Sukyung Lee and Hyunbyung Park and Gyoungjin Gim and Mikyoung Cha and Hwalsuk Lee and Sunghun Kim},
      year={2023},
      eprint={2312.15166},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}



@article{posch_characterizing_2022,
	title = {Characterizing the {Global} {Crowd} {Workforce}: {A} {Cross}-{Country} {Comparison} of {Crowdworker} {Demographics}},
	volume = {9},
	copyright = {https://creativecommons.org/licenses/by/4.0},
	issn = {2330-8001},
	shorttitle = {Characterizing the {Global} {Crowd} {Workforce}},
	url = {https://hcjournal.org/index.php/jhc/article/view/106},
	doi = {10.15346/hc.v9i1.106},
	abstract = {Since its emergence roughly a decade ago, micro-task crowdsourcing has been attracting a heterogeneous set of workers from all over the globe. This paper sets out to explore the characteristics of the international crowd workforce to date and offers a cross-national comparison of crowdworker populations from ten hand-selected countries. We provide an analysis and comparison of demographic characteristics and shed light on the significance of micro-task income for workers situated in different national contexts. With over 11,000 individual responses, this study is the first large-scale country-level analysis of the characteristics of workers on the platform Figure Eight (formerly CrowdFlower), one of the two platforms dominating the micro-task market. We find large differences between the characteristics of the crowd workforces of different countries, both regarding demography and regarding the importance of micro-task income for workers. Furthermore, we find that the composition of the workforce in the ten countries was largely stable across samples taken at different points in time.},
	number = {1},
	urldate = {2024-10-16},
	journal = {Human Computation},
	author = {Posch, Lisa and Bleier, Arnim and Flöck, Fabian and Strohmaier, Markus},
	month = aug,
	year = {2022},
	annote = {[TLDR] This study is the first large-scale country-level analysis of the characteristics of workers on the platform Figure Eight (formerly CrowdFlower), one of the two platforms dominating the micro-task market.},
}



@book{alim_articulate_2012,
	address = {Oxford, UNITED STATES},
	title = {Articulate {While} {Black}: {Barack} {Obama}, {Language}, and {Race} in the {U}. {S}.},
	isbn = {978-0-19-981297-4},
	shorttitle = {Articulate {While} {Black}},
	url = {http://ebookcentral.proquest.com/lib/cm/detail.action?docID=1026822},
	urldate = {2025-02-05},
	publisher = {Oxford University Press, Incorporated},
	author = {Alim, H. Samy and Smitherman, Geneva and Dyson, Michael Eric},
	year = {2012},
}



@inproceedings{ziems_value_2022,
	address = {Dublin, Ireland},
	title = {{VALUE}: {Understanding} {Dialect} {Disparity} in {NLU}},
	shorttitle = {{VALUE}},
	url = {https://aclanthology.org/2022.acl-long.258/},
	doi = {10.18653/v1/2022.acl-long.258},
	abstract = {English Natural Language Understanding (NLU) systems have achieved great performances and even outperformed humans on benchmarks like GLUE and SuperGLUE. However, these benchmarks contain only textbook Standard American English (SAE). Other dialects have been largely overlooked in the NLP community. This leads to biased and inequitable NLU systems that serve only a sub-population of speakers. To understand disparities in current models and to facilitate more dialect-competent NLU systems, we introduce the VernAcular Language Understanding Evaluation (VALUE) benchmark, a challenging variant of GLUE that we created with a set of lexical and morphosyntactic transformation rules. In this initial release (V.1), we construct rules for 11 features of African American Vernacular English (AAVE), and we recruit fluent AAVE speakers to validate each feature transformation via linguistic acceptability judgments in a participatory design manner. Experiments show that these new dialectal features can lead to a drop in model performance.},
	urldate = {2025-02-05},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Ziems, Caleb and Chen, Jiaao and Harris, Camille and Anderson, Jessica and Yang, Diyi},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	month = may,
	year = {2022},
	pages = {3701--3720},
}



@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {https://arxiv.org/abs/2203.02155v1},
	abstract = {Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	urldate = {2025-02-08},
	journal = {arXiv.org},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
}



@article{casper_open_2023,
	title = {Open {Problems} and {Fundamental} {Limitations} of {Reinforcement} {Learning} from {Human} {Feedback}},
	issn = {2835-8856},
	url = {https://openreview.net/forum?id=bx24KpJ4Eb},
	abstract = {Reinforcement learning from human feedback (RLHF) is a technique for training AI systems to align with human goals. RLHF has emerged as the central method used to finetune state-of-the-art large language models (LLMs). Despite this popularity, there has been relatively little public work systematizing its flaws. In this paper, we (1) survey open problems and fundamental limitations of RLHF and related methods; (2) overview techniques to understand, improve, and complement RLHF in practice; and (3) propose auditing and disclosure standards to improve societal oversight of RLHF systems. Our work emphasizes the limitations of RLHF and highlights the importance of a multi-layered approach to the development of safer AI systems.},
	language = {en},
	urldate = {2025-02-08},
	journal = {Transactions on Machine Learning Research},
	author = {Casper, Stephen and Davies, Xander and Shi, Claudia and Gilbert, Thomas Krendl and Scheurer, Jérémy and Rando, Javier and Freedman, Rachel and Korbak, Tomek and Lindner, David and Freire, Pedro and Wang, Tony Tong and Marks, Samuel and Segerie, Charbel-Raphael and Carroll, Micah and Peng, Andi and Christoffersen, Phillip J. K. and Damani, Mehul and Slocum, Stewart and Anwar, Usman and Siththaranjan, Anand and Nadeau, Max and Michaud, Eric J. and Pfau, Jacob and Krasheninnikov, Dmitrii and Chen, Xin and Langosco, Lauro and Hase, Peter and Biyik, Erdem and Dragan, Anca and Krueger, David and Sadigh, Dorsa and Hadfield-Menell, Dylan},
	month = sep,
	year = {2023},
}



@inproceedings{kirk_past_2023,
	address = {Singapore},
	title = {The {Past}, {Present} and {Better} {Future} of {Feedback} {Learning} in {Large} {Language} {Models} for {Subjective} {Human} {Preferences} and {Values}},
	url = {https://aclanthology.org/2023.emnlp-main.148/},
	doi = {10.18653/v1/2023.emnlp-main.148},
	abstract = {Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the 2023 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Kirk, Hannah Rose and Bean, Andrew M. and Vidgen, Bertie and Röttger, Paul and Hale, Scott A.},
	editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
	month = dec,
	year = {2023},
	pages = {2409--2430},
}


@incollection{lanehart_2015,
    author = {Lanehart, Sonja L. and Bloomquist, Jennifer and Malik, Ayesha M.},
    isbn = {9780199795390},
    title = {1Language Use in African American Communities: An Introduction},
    booktitle = {The Oxford Handbook of African American Language},
    publisher = {Oxford University Press},
    year = {2015},
    month = {07},
    abstract = {This chapter provides readers with an overview of analyses of traditional and contemporary work on language use in African American communities in the Oxford Handbook of African American Language (OHAAL). This introduction provides a justification for the need to provide a variety of scholarly perspectives on African American Language (AAL) with respect to sociohistorical origins and perspectives, regional variation, structure and description, child language acquisition and development, education and pedagogy, social and cultural contexts, attitudes and beliefs, and identity. In addition, this Introduction serves to provide a discussion on clarity and specificity in discussions about naming and defining AAL (or African American English) as well as about what it is and is not. Finally, this Introduction serves to highlight a need for collaborative perspectives and innovative thinking while reasserting the need for better research and communication on AAL within and outside the linguistic community in general and sociolinguistics in particular.},
    doi = {10.1093/oxfordhb/9780199795390.013.62},
    url = {https://doi.org/10.1093/oxfordhb/9780199795390.013.62},
    eprint = {https://academic.oup.com/book/0/chapter/212000574/chapter-ag-pdf/44596701/book\_28056\_section\_212000574.ag.pdf},
}




@inproceedings{tatman_gender_2017,
	address = {Valencia, Spain},
	title = {Gender and {Dialect} {Bias} in {YouTube}`s {Automatic} {Captions}},
	url = {https://aclanthology.org/W17-1606/},
	doi = {10.18653/v1/W17-1606},
	abstract = {This project evaluates the accuracy of YouTube`s automatically-generated captions across two genders and five dialect groups. Speakers' dialect and gender was controlled for by using videos uploaded as part of the “accent tag challenge”, where speakers explicitly identify their language background. The results show robust differences in accuracy across both gender and dialect, with lower accuracy for 1) women and 2) speakers from Scotland. This finding builds on earlier research finding that speaker`s sociolinguistic identity may negatively impact their ability to use automatic speech recognition, and demonstrates the need for sociolinguistically-stratified validation of systems.},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the {First} {ACL} {Workshop} on {Ethics} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Tatman, Rachael},
	editor = {Hovy, Dirk and Spruit, Shannon and Mitchell, Margaret and Bender, Emily M. and Strube, Michael and Wallach, Hanna},
	month = apr,
	year = {2017},
	pages = {53--59},
}


@inproceedings{harris_modeling_2024,
	address = {Miami, Florida, USA},
	title = {Modeling {Gender} and {Dialect} {Bias} in {Automatic} {Speech} {Recognition}},
	url = {https://aclanthology.org/2024.findings-emnlp.890/},
	doi = {10.18653/v1/2024.findings-emnlp.890},
	abstract = {Dialect and gender-based biases have become an area of concern in language-dependent AI systemsincluding around automatic speech recognition (ASR) which processes speech audio into text. These potential biases raise concern for discriminatory outcomes with AI systems depending on demographic- particularly gender discrimination against women, and racial discrimination against minorities with ethnic or cultural English dialects.As such we aim to evaluate the performance of ASR systems across different genders and across dialects of English. Concretely, we take a deep dive of the performance of ASR systems on men and women across four US-based English dialects: Standard American English (SAE), African American Vernacular English (AAVE), Chicano English, and Spanglish. To do this, we construct a labeled dataset of 13 hours of podcast audio, transcribed by speakers of the represented dialects. We then evaluate zero-shot performance of different automatic speech recognition models on our dataset, and further finetune models to better understand how finetuning can impact performance. Our work fills the gap of investigating possible gender disparities within underrepresented dialects.},
	urldate = {2025-02-08},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Harris, Camille and Mgbahurike, Chijioke and Kumar, Neha and Yang, Diyi},
	editor = {Al-Onaizan, Yaser and Bansal, Mohit and Chen, Yun-Nung},
	month = nov,
	year = {2024},
	pages = {15166--15184},
}



@inproceedings{bang_multitask_2023,
	address = {Nusa Dua, Bali},
	title = {A {Multitask}, {Multilingual}, {Multimodal} {Evaluation} of {ChatGPT} on {Reasoning}, {Hallucination}, and {Interactivity}},
	url = {https://aclanthology.org/2023.ijcnlp-main.45/},
	doi = {10.18653/v1/2023.ijcnlp-main.45},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the 13th {International} {Joint} {Conference} on {Natural} {Language} {Processing} and the 3rd {Conference} of the {Asia}-{Pacific} {Chapter} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Bang, Yejin and Cahyawijaya, Samuel and Lee, Nayeon and Dai, Wenliang and Su, Dan and Wilie, Bryan and Lovenia, Holy and Ji, Ziwei and Yu, Tiezheng and Chung, Willy and Do, Quyet V. and Xu, Yan and Fung, Pascale},
	editor = {Park, Jong C. and Arase, Yuki and Hu, Baotian and Lu, Wei and Wijaya, Derry and Purwarianti, Ayu and Krisnadhi, Adila Alfa},
	month = nov,
	year = {2023},
	pages = {675--718},
}



@misc{jiao_is_2023,
	title = {Is {ChatGPT} {A} {Good} {Translator}? {Yes} {With} {GPT}-4 {As} {The} {Engine}},
	shorttitle = {Is {ChatGPT} {A} {Good} {Translator}?},
	url = {https://arxiv.org/abs/2301.08745v4},
	abstract = {This report provides a preliminary evaluation of ChatGPT for machine translation, including translation prompt, multilingual translation, and translation robustness. We adopt the prompts advised by ChatGPT to trigger its translation ability and find that the candidate prompts generally work well with minor performance differences. By evaluating on a number of benchmark test sets, we find that ChatGPT performs competitively with commercial translation products (e.g., Google Translate) on high-resource European languages but lags behind significantly on low-resource or distant languages. As for the translation robustness, ChatGPT does not perform as well as the commercial systems on biomedical abstracts or Reddit comments but exhibits good results on spoken language. Further, we explore an interesting strategy named \${\textbackslash}mathbf\{pivot{\textasciitilde}prompting\}\$ for distant languages, which asks ChatGPT to translate the source sentence into a high-resource pivot language before into the target language, improving the translation performance noticeably. With the launch of the GPT-4 engine, the translation performance of ChatGPT is significantly boosted, becoming comparable to commercial translation products, even for distant languages. Human analysis on Google Translate and ChatGPT suggests that ChatGPT with GPT-3.5 tends to generate more hallucinations and mis-translation errors while that with GPT-4 makes the least errors. In other words, ChatGPT has already become a good translator. Please refer to our Github project for more details: https://github-com.cmu.idm.oclc.org/wxjiao/Is-ChatGPT-A-Good-Translator},
	language = {en},
	urldate = {2025-02-08},
	journal = {arXiv.org},
	author = {Jiao, Wenxiang and Wang, Wenxuan and Huang, Jen-tse and Wang, Xing and Shi, Shuming and Tu, Zhaopeng},
	month = jan,
	year = {2023},
}



@inproceedings{robinson_chatgpt_2023,
	address = {Singapore},
	title = {{ChatGPT} {MT}: {Competitive} for {High}- (but {Not} {Low}-) {Resource} {Languages}},
	shorttitle = {{ChatGPT} {MT}},
	url = {https://aclanthology.org/2023.wmt-1.40/},
	doi = {10.18653/v1/2023.wmt-1.40},
	abstract = {Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs' MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world`s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1\% of languages we covered. Our analysis reveals that a language`s resource level is the most important feature in determining ChatGPT`s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.},
	urldate = {2025-02-08},
	booktitle = {Proceedings of the {Eighth} {Conference} on {Machine} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Robinson, Nathaniel and Ogayo, Perez and Mortensen, David R. and Neubig, Graham},
	editor = {Koehn, Philipp and Haddow, Barry and Kocmi, Tom and Monz, Christof},
	month = dec,
	year = {2023},
	pages = {392--418},
}



@article{hendy_how_2023,
	title = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}? {A} {Comprehensive} {Evaluation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {How {Good} {Are} {GPT} {Models} at {Machine} {Translation}?},
	url = {https://arxiv.org/abs/2302.09210},
	doi = {10.48550/ARXIV.2302.09210},
	abstract = {Generative Pre-trained Transformer (GPT) models have shown remarkable capabilities for natural language generation, but their performance for machine translation has not been thoroughly investigated. In this paper, we present a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different GPT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation. We experiment with eighteen different translation directions involving high and low resource languages, as well as non English-centric translations, and evaluate the performance of three GPT models: ChatGPT, GPT3.5 (text-davinci-003), and text-davinci-002. Our results show that GPT models achieve very competitive translation quality for high resource languages, while having limited capabilities for low resource languages. We also show that hybrid approaches, which combine GPT models with other translation systems, can further enhance the translation quality. We perform comprehensive analysis and human evaluation to further understand the characteristics of GPT translations. We hope that our paper provides valuable insights for researchers and practitioners in the field and helps to better understand the potential and limitations of GPT models for translation.},
	urldate = {2025-02-08},
	author = {Hendy, Amr and Abdelrehim, Mohamed and Sharaf, Amr and Raunak, Vikas and Gabr, Mohamed and Matsushita, Hitokazu and Kim, Young Jin and Afify, Mohamed and Awadalla, Hany Hassan},
	year = {2023},
	note = {Publisher: arXiv
Version Number: 1},
	annote = {[TLDR] This paper presents a comprehensive evaluation of GPT models for machine translation, covering various aspects such as quality of different G PT models in comparison with state-of-the-art research and commercial systems, effect of prompting strategies, robustness towards domain shifts and document-level translation.},
}



@misc{finch_finding_2025,
	title = {Finding {A} {Voice}: {Evaluating} {African} {American} {Dialect} {Generation} for {Chatbot} {Technology}},
	shorttitle = {Finding {A} {Voice}},
	url = {https://arxiv.org/abs/2501.03441v1},
	abstract = {As chatbots become increasingly integrated into everyday tasks, designing systems that accommodate diverse user populations is crucial for fostering trust, engagement, and inclusivity. This study investigates the ability of contemporary Large Language Models (LLMs) to generate African American Vernacular English (AAVE) and evaluates the impact of AAVE usage on user experiences in chatbot applications. We analyze the performance of three LLM families (Llama, GPT, and Claude) in producing AAVE-like utterances at varying dialect intensities and assess user preferences across multiple domains, including healthcare and education. Despite LLMs' proficiency in generating AAVE-like language, findings indicate that AAVE-speaking users prefer Standard American English (SAE) chatbots, with higher levels of AAVE correlating with lower ratings for a variety of characteristics, including chatbot trustworthiness and role appropriateness. These results highlight the complexities of creating inclusive AI systems and underscore the need for further exploration of diversity to enhance human-computer interactions.},
	language = {en},
	urldate = {2025-02-09},
	journal = {arXiv.org},
	author = {Finch, Sarah E. and Paek, Ellie S. and Kwon, Sejung and Choi, Ikseon and Wells, Jessica and Chandler, Rasheeta and Choi, Jinho D.},
	month = jan,
	year = {2025},
}




@misc{kantharuban_quantifying_2023,
	title = {Quantifying the {Dialect} {Gap} and its {Correlates} {Across} {Languages}},
	url = {http://arxiv.org/abs/2310.15135},
	doi = {10.48550/arXiv.2310.15135},
	abstract = {Historically, researchers and consumers have noticed a decrease in quality when applying NLP tools to minority variants of languages (i.e. Puerto Rican Spanish or Swiss German), but studies exploring this have been limited to a select few languages. Additionally, past studies have mainly been conducted in a monolingual context, so cross-linguistic trends have not been identified and tied to external factors. In this work, we conduct a comprehensive evaluation of the most influential, state-of-the-art large language models (LLMs) across two high-use applications, machine translation and automatic speech recognition, to assess their functionality on the regional dialects of several high- and low-resource languages. Additionally, we analyze how the regional dialect gap is correlated with economic, social, and linguistic factors. The impact of training data, including related factors like dataset size and its construction procedure, is shown to be significant but not consistent across models or languages, meaning a one-size-fits-all approach cannot be taken in solving the dialect gap. This work will lay the foundation for furthering the field of dialectal NLP by laying out evident disparities and identifying possible pathways for addressing them through mindful data collection.},
	language = {en},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Kantharuban, Anjali and Vulić, Ivan and Korhonen, Anna},
	month = oct,
	year = {2023},
	note = {arXiv:2310.15135 [cs]},
	annote = {Comment: Accepted to EMNLP Findings 2023},
	file = {Kantharuban et al. - 2023 - Quantifying the Dialect Gap and its Correlates Acr.pdf:/Users/joelmire/Zotero/storage/DQI3X9SA/Kantharuban et al. - 2023 - Quantifying the Dialect Gap and its Correlates Acr.pdf:application/pdf},
}
