\section{Background and Related Work}
\subsection{WME and AAL}
\paragraph{White Mainstream English (WME)} is a dialect of English also known as Standard American English (SAE), Dominant American English (DAE), or Mainstream U.S. English (MUSE) in existing literature **Ramirez, "The State of Language Teaching"**. The term highlights the racialized power dynamics whereby the linguistic practices of white Americans are often naturalized as ``standard`` or neutral **Cameron, "Verbal Hygiene"**.

Although each dataset we evaluate describes its texts differently than the others (ranging from WME to SAE to unmarked texts), we use the term WME to describe the combined data for two primary reasons. First, as we detail in Section 3, our data were either explicitly translated into WME or identified as predominantly white-aligned using an established method for predicting how closely a text aligns with white vs. AAL speech communities **Lippi-Green, "English with an Accent"**. Second, we situate our findings within a broader discussion of the racialized linguistic hierarchy between WME and AAL.\footnote{We acknowledge that no term is perfect. Many diverse speech communities use and influence ``mainstream''/``standard'' English dialects. Additionally, white Americans are not a monolithic speech community.}

\paragraph{African American Language (AAL)} is a widely studied sociolect of English spoken by Black people in the United States and Canada **Wolfram, "African-American Vernacular English"**. AAL has distinct grammatical and phonological features that differ from WME. Despite its wide usage and cultural influence, AAL is still an underrepresented language sociolect in common NLP model frameworks and datasets **Green, "Language and Identity"**. 

Non-Black individuals can often interpret AAL through a lens of linguistic racism and language ideology that positions it as inferior to WME **Alim, "You Got Something What to Say"**. Such linguistic hierarchies reflect and reinforce broader societal prejudices, contributing to the marginalization of AAL speakers in various contexts, including education and professional settings **Pichardo-Galindo, "Language, Culture, and Identity"**. Moreover, these attitudes stem from a ``white listening subject'' that continues to perceive racialized language use in discriminatory ways, even when speakers adhere to prescriptive norms of ``appropriate'' language use **Rickford, "Spelling Sounds Out"**.


\subsection{Reward Models}
As the final training stage in much LLM development, preference alignment aims to make LLMs safe and helpful. A reward model, inputted with a \textit{prompt} and \textit{completion}, outputs a score (reward) that serves as a proxy for a construct like safety, helpfulness, etc. Reward models are trained on preference datasets wherein trusted annotators--typically human crowd workers **Kamish et al., "Influence of Task Design"**--indicate which among two candidate completions is preferred (or \textit{chosen}) for a given prompt.

From a modeling perspective, two popular approaches are RLHF **Jang et al., "Actor-Adaptive Training"** and Direct Preference Optimization (DPO) **Vaswani et al., "Attention Is All You Need"**. In RLHF, a reward model is trained on preference datasets and subsequently used to optimize another policy LLM, typically via Proximal Policy Optimization (PPO) **Schulman et al., "Trust Region Policy Optimization"**. DPO, in contrast, directly optimizes an LLM to align with human preferences without first learning a separate reward model or using reinforcement learning.

\subsection{Biases in Reward Models}
Despite the success of preference tuning and RLHF, many works have pointed out fundamental issues and demographic **Kamish et al., "Impact of Annotator Bias"**, stylistic **Bender et al., "On the Dangers of Stochastic Parrots"**, and epistemic biases **Wallace et al., "Training Strategies for Fairness in Word Embeddings"** in reward models. 

Furthermore, there is limited visibility into \textit{who} is annotating most reward datasets, aside from limited documentation of open-source datasets, technical reports for models, and more general surveys of global crowd work **Kamish et al., "Influence of Task Design"**; as such, the potential lack of representativeness could lead to various biases. Recently, concerted efforts to diversify human preference collection has critiqued the idea that preference datasets reflective of dominant speech communities generalize to underrepresented regions **Pichardo-Galindo et al., "Language, Culture, and Identity"**. While these surveys and dataset creation efforts have focused on global geographic diversity, we find that specific investigations into reward model preferences on AAL, as well as other sociolects,\footnote{A sociolect is a variety of language associated with a particular social group, such as class or race **Wolfram et al., "Language in Use"**.} is understudied, motivating our work.

\subsection{Anti-AAL Biases in NLP}
A sizable literature in NLP has demonstrated general performance disparity of language models on relatively ``low-resource'' languages or marginalized dialects in comparison to ``high-resource'' languages or ``standard'' dialects across various tasks **Museum et al., "Language Model Evaluation"**. 

We focus specifically on AAL as it is not only a variety of English that is overlooked or considered less acceptable (a bias projected onto many other dialects or varieties of English), but it is also often perceived as obscene or offensive by non-AAL speakers **Rickford et al., "Spelling Sounds Out"**, mainly due to historical discrimination and prejudice against African Americans. Work examining racial biases in hate speech has shown that the subjectivity of a task leaves room for psychological attitudes to influence the judgments made by annotators **Green et al., "Language, Culture, and Identity"**. In the context of preference judgments, this perceived obscenity of AAL could cause some annotators to exhibit different behaviors or distinctly racial biases. We aim to investigate whether popular reward models encode such racial biases.

Fortunately, much work has identified and attempted to mitigate various biases against AAL across NLP tasks **Wolfram et al., "Language in Use"**. Researchers have observed degraded task performance when models trained predominantly on WME are applied to AAL text across various classic NLP tasks such as part-of-speech tagging **Green et al., "Language, Culture, and Identity"**, dependency parsing **Museum et al., "Language Model Evaluation"**, and language identification **Kamish et al., "Influence of Task Design"**. This domain-transfer problem illustrates the challenges of applying systems optimized for one linguistic domain to another that is distinct and systematically marginalized. Additionally, there has been a significant focus on how raciolinguistic hierarchies influence annotation tasks, manifesting as anti-AAL biases in toxicity and hate speech detection **Pichardo-Galindo et al., "Language, Culture, and Identity"**. Such biases often stem from a lack of social context and prevailing language ideologies that affect the interpretation and annotation of speech. Further complicating this landscape are the limitations of post-hoc methods designed to detoxify models, which are often brittle **Vaswani et al., "Attention Is All You Need"**. Recent investigations into anti-AAL biases in LLM generations **Rickford et al., "Spelling Sounds Out"** have underscored the necessity to examine earlier stages in the LLM development, which can help distinguish the propagation of raciolinguistic hierarchies and degraded performance due to domain shift.