\section{Analysis}

\subsection{Confidence Score Compared to Reward Score from Reward Models}
We compare our self-generated confidence scores with established open-source reward model approaches.
A reward model is an additional scoring model used to evaluate the quality of generated responses~\citep{Christiano2017DeepRL}. Deployment of a reward model can introduce several limitations: (1) Reward scores are often unbounded or require dataset-specific normalization, thus difficult to apply a universal threshold for filtering or reweighting responses; (2) Running an extra reward model increases inference time; and (3) A dedicated reward model requires additional GPU memory, and is less efficient for large-scale deployment.


For our analysis, we use the following reward models for comparison:
for Llama-3.1-Instruct,
we use the reward model from RLHFlow~\footnote{\url{https://huggingface.co/RLHFlow/Llama3.1-8B-ORM-Mistral-Data}}~\citep{dong2024rlhf}; for Qwen-2.5, we utilize its official Process Reward Model (PRM)~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-Math-PRM-7B}}~\citep{prmlessons}. 
For PRM, we use the lowest reward score across all steps. 
We ensure the size of each reward model matches with their corresponding base models. 

% ~\jh{Can we explicitly mention the reward model size here, instead of only in the url? We can also claim that our method is comparable to reward model of what size.}

\begin{table}[]
\centering
\small
\begin{tabularx}{\columnwidth}{Xlrr}
\toprule
Model & Dataset  & Reward & Confidence \\ 
\midrule
      & MathQA   & 82.1   & \textbf{84.0}  \\ 
Llama & Object Counting  & \textbf{72.6}  & 72.0   \\ 
      & ARC\_Challenge  & 86.2   & \textbf{86.6}  \\ 
      & MathQA   & \textbf{87.5}  & 86.8   \\ 
Qwen  & Object Counting  & \textbf{76.6}  & 76.4   \\ 
      & ARC\_Challenge  & 89.6   & \textbf{89.8}   \\ 
\bottomrule
\end{tabularx}
\caption{Accuracy of Best-of-16 on two models (Llama-3.1-8B-Instruct and Qwen-2.5-7B-Instruct) on three datasets between self-generated confidence scores and reward scores from additional reward models.}
\vspace{-15pt}
\label{tab:performance_comparison_re}
\end{table}

Table~\ref{tab:performance_comparison_re} shows that our self-generated confidence scores achieve similar performance to reward model scores across all datasets when using Best-of-N. This means that our method, by generating approximately 10 additional tokens, achieves a performance comparable to that of an extra reward model of the same size.

\subsection{Performance Comparison Under Different Sample Budgets}

% In this section, we analyze how different methods perform under varying sample budgets. 
Increasing the sample budget allows for selecting higher-quality outputs but comes at the cost of greater computational expense. To evaluate this trade-off, we compare different methods across multiple sample budgets and visualize their performance trends.
As shown in Figure~\ref{fig:vary-budget}, all methods achieve better accuracy as the number of responses increases. Our confidence-guided approaches consistently outperform their original counterparts in most settings. When the sample budget is small, Best-of-N performs better than early stopping because early stopping might stop too soon with a low threshold, missing a better response.  


\begin{figure}
    \centering
    \includegraphics[width=0.95\linewidth]{figures/qwen_MATHQA.pdf}
    \caption{Accuracy over varying sample budgets of different inference strategies on MathQA using Self-Calibration trained Qwen-2.5-7B-Instruction. The results of other models and datasets are shown in Appendix~\ref{app:different_budgets}.}
    \vspace{-15pt}
    \label{fig:vary-budget}
\end{figure}

% As shown in Figure~\ref{fig:enter-label}, all methods improve their final performance with larger sample budgets. Our confidence-guided method outperforms its original counterpart in most settings. Notably, by applying a threshold for early stopping, it achieves an average sample budget between 1 and 2.~\jh{How do we derive this observation from the image?} In this setting, early stopping outperforms single-pass sampling~\jh{Which line is single pass sampling?}, demonstrating that confidence scores serve as an effective method for distributing computational resources under limited budgets.




\subsection{Can Other Confidence Querying Prompts Work Well?}
\label{sec:confideceprompt}
Since our confidence-based approach was trained using a specific confidence querying prompt, we explore whether alternative prompts can achieve similar performance during inference. This analysis is crucial for understanding the robustness of confidence querying prompts different from the training prompt.


\begin{table}[]
\small
\centering
\begin{tabularx}{\columnwidth}{X l c c}
\toprule
Dataset & Method          & Original & New \\ 
\midrule
        & Early Stopping  & 81.7            & 81.52$_{\pm 0.30}$ \\ 
MathQA  & ASC w/o conf.   & 81.9            & 81.80$_{\pm 0.21}$ \\ 
        & SC w/o conf.    & 82.1            & 81.63$_{\pm 0.20}$ \\ 
\midrule
        & Early Stopping  & 67.2            & 70.80$_{\pm 1.99}$ \\ 
Obj\_C. & ASC w/o conf.   & 74.8            & 74.07$_{\pm 1.03}$ \\ 
        & SC w/o conf.    & 74.4            & 73.40$_{\pm 0.75}$ \\ 
\midrule
        & Early Stopping  & 86.2            & 86.62$_{\pm 0.20}$ \\ 
ARC\_C. & ASC w/o conf.   & 86.6            & 86.63$_{\pm 0.05}$ \\ 
        & SC w/o conf.    & 86.4            & 86.35$_{\pm 0.25}$ \\ 
\bottomrule
\end{tabularx}
\caption{Accuracy comparison between the original prompt ``Is the answer correct? (Yes/No)'' and 6 alternative confidence querying prompts on three datasets of Llama-3.1B-Instruct-SC. Results are reported as mean$_{\pm \text{std}}$. We report the detailed results for each alternative prompt  respectively in Appendix~\ref{app:result_prompt}.}

% ~\jh{We report the detailed results for each alternative prompt ($I_2$ to $I_6$) respectively in Appendix xx.}}
\vspace{-15pt}
\label{tab:confidence_prompt_performance}
\end{table}

We evaluate 6 alternative prompts (listed in Appendix~\ref{app:query_prompt}) at inference time. 
Table~\ref{tab:confidence_prompt_performance} shows that despite training with a specific prompt, other prompts yield comparable performance across all datasets, with only minor variations. This suggests that our confidence querying approach is robust to prompt changes and our training framework improves model calibration rather than overfitting to a special prompt.
% making confidence scores more stable and reliable.


% \jh{Other possible analysis: (1) hyperparameter study? (2) varying the confidence querying prompt? (3) temperature used in sampling? }