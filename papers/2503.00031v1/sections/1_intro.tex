\section{Introduction}

% \ljc{I have a nuanced high-level comment. Overall, the story I'm getting is that you have a better method to obtain confidence scores, and using this confidence score you can do efficient test-time scaling. I feel that these two things are less coupled, in a sense that efficient test-time scaling should be (qualitatively) doable with some sort of confidence score, and your better confidence score (quantitatively) makes the test-time scaling more efficient. I think an important baseline is how well you can do test-time scaling with the basic confidence score (i.e. Kadavath's P(True) itself). It would be nice to show that your improved confidence estimate is useful in application, since this improvement is motivated by the application in test-time scaling.}

%Increasing test-time computation is a straightforward approach to enhancing the quality of responses in LLMs. 
Leveraging additional computation during inference can enhance the quality of responses generated by large language models (LLMs)~\citep{Snell2024ScalingLT, Yao2023TreeOT, Wu2024InferenceSL, Chen2025SETSLS}. Among these methods, repeated sampling~\citep{Brown2024LargeLM} such as Best-of-N~\citep{Cobbe2021TrainingVT} and Self-Consistency~\citep{Wang2022SelfConsistencyIC} generate multiple candidate responses and select the final answer by a scoring model or a majority voting rule. While these methods have proven effective,
% However, their performance is highly sensitive to the number of samples generated, making hyperparameter selection crucial~\citep{Sessa2024BONDAL}. 
they require a fixed amount of sampled responses for each query regardless of its difficulty and complexity.
Although increasing the sample size generally improves performance, it also increases computational costs and inference time~\cite{Amini2024VariationalBA}. This is particularly inefficient for simple questions like “2 + 3 = ?”, where a few samples are  sufficient to find the correct solution~\cite{Chen2024DoNT}, and extensive sampling is unnecessary.
% and a fixed number of samples leads to unnecessary resource consumption.

\begin{figure}
    \centering
    \includegraphics[width=0.85\linewidth]{figures/sc_comparison_by_responses.pdf}
    \caption{Accuracy over response numbers of standard Self-Consistency (SC) vs. confidence-weighted Self-Consistency (SC w/ conf.) on MathQA using our trained Llama-3.1-8B-Instruct model. The horizontal lines mark the response usage difference required for SC w/ conf. to reach the same accuracy with SC.}
    \vspace{-15pt}
    \label{fig:intro}
\end{figure}

% In this work, we focus on addressing this inefficiency in test-time scaling. 
% Previous methods have been proposed to adaptively determine the number of samples based on task complexity. 
Previous adaptive sampling strategies~\citep{Aggarwal2023LetsSS,Li2024EscapeSC,Wan2024ReasoningAS} typically design lightweight stopping criteria to determine whether additional responses should be sampled.
However, they often incorporate manually designed features or heuristic rules, such as stopping when the model generates the same response three times consecutively,
% ~\jh{can we add an example here? Otherwise, readers may be confused about how our early stopping differs from theirs}
which can limit their generalizability across different tasks and models. 
% Such handcrafted features typically require domain-specific tuning, reducing adaptability to diverse reasoning scenarios. 
Therefore, it is critical to design a task-independent, model-agnostic approach without heavy reliance on human-designed heuristics.

We propose an efficient test-time scaling method by using model confidence for dynamically sampling adjustment, since confidence can be seen as an intrinsic measure that directly reflects model uncertainty on different tasks. 
However, extracting accurate confidence can be challenging since LLMs are known to be overconfident on their own responses~\cite{lin2022teaching, xiong2023can, Leng2024TamingOI}, and their confidence often exceeds the actual accuracy. Self-Consistency~\cite{Wang2024SelfConsistencyBC} can provide a relatively accurate confidence estimation by aggregating answer counts from multiple sampled solutions~\cite{Tian2023JustAF}, but it again requires sampling a large number of responses for each query beforehand.
% However, existing calibration methods often require extra computational resources~\cite{Wang2024SelfConsistencyBC} or fail to provide sufficiently accurate confidence scores~\cite{Tian2023JustAF}, especially in scenarios where the performance of LLMs is suboptimal, requiring test-time scaling. 

To address this, we introduce \textbf{Self-Calibration} to train LLMs for accurate confidence estimation in only one forward pass, without requiring any human-labeled data. Specifically, we improve model calibration 
% leverage the model’s self-consistency score—a relatively accurate, but computationally expensive confidence estimation—as a training target. By doing so, we 
by distilling Self-Consistency-derived confidence into the model itself. This is done by constructing pseudo training tuples of query, answer, and confidence on a diverse training set.
At test time, we design \textbf{efficient test-time scaling strategies using these calibrated confidence scores}, such as early stopping for Best-of-N when sampled responses reach a target confidence, and Self-Consistency weighted by reliable confidence.
% To preserve its original generation ability, we also add the responses with high confidence as training data during the Self-Calibration, which is a strategy similar to Self-Improvement~\citep{Huang2022LargeLM} and SCPO~\citep{Prasad2024SelfConsistencyPO}.


%Building on efficiently produced confidence scores, we refine prior test-time scaling-up methods to better handle queries of various difficulty.
% Building on efficiently produced confidence scores, we refine prior test-time scaling-up methods to better handle queries of varying difficulty. Instead of relying on fixed heuristics or manually tuned thresholds, we leverage the model’s self-calibrated confidence to dynamically adjust the number of samples during inference. Specifically, when the confidence score is high, we reduce sampling to save computational resources, whereas for lower-confidence cases, we increase sampling to improve reliability. This score can also be used as the weight of each response when using voting-based methods like self-consistency.

%Experiments on two LLMs across several datasets demonstrate the efficiency of our approach in both resource utilization and performance.
Empirical experiments on three LLM architectures across six datasets demonstrate that our confidence-based test-time scaling approaches consistently outperform their baseline counterparts under the same sampling budget. Specifically, both Early Stopping for Best-of-N and confidence-weighted Self-Consistency improve MathQA accuracy over their baselines from 81.0 to 83.6 with an average sampling budget of 16 responses. More importantly, our approaches can achieve comparable performance with substantially fewer computational resources. As shown in Fig.~\ref{fig:intro}, confidence-weighted Self-Consistency can save 94.2\% samples to achieve an accuracy of 85.0, compared to standard Self-Consistency, demonstrating that reliable confidence estimation can significantly enhance the computational efficiency of test-time scaling.



% \begin{itemize}
%     \item We propose a self-calibration framework that enhances the model’s confidence estimation without requiring labeled data. By leveraging self-consistency as a training signal, our method improves model calibration with minimal additional computational cost.
    
%     \item We refine test-time scaling methods by dynamically adjusting the number of sampled responses using self-calibrated confidence scores. This enables adaptive sampling based on query difficulty and incorporates confidence-weighted voting to improve final predictions.

%     \item  Our comprehensive analysis reveals the applicability and efficiency of our methods. We show that our methods consistently outperform baselines under the same sample budget. We also demonstrate early stopping achieves optimal performance with an average sampling count of fewer than two.\jxc{isn't this redundant with last paragraph?}

% \end{itemize}