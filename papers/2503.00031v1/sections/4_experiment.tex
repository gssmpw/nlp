\section{Experiments}
\begin{table*}[th]
\centering
\small
\begin{tabular}{ll|cc|cc|cc}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{2}{c|}{\textbf{Llama-3.1-8B-Instruct}} & \multicolumn{2}{c|}{\textbf{Qwen2.5-7B-Instruct}} & \multicolumn{2}{c}{\textbf{DeepSeek-R1-Distill-1.5B}} \\\cmidrule(lr){3-4} \cmidrule(lr){5-6} \cmidrule(lr){7-8}
 & & \textbf{Vanilla} & \textbf{Self-Calibration} & \textbf{Vanilla} & \textbf{Self-Calibration} & \textbf{Vanilla} & \textbf{Self-Calibration} \\
\midrule
\multicolumn{8}{l}{\textit{In-Domain Datasets}} \\
 & ECE $\downarrow$ & 13.70 & \textbf{3.79} & 87.39 & \textbf{16.88} & 46.66 & \textbf{40.03} \\
 GSM8K & AUC $\uparrow$ & 72.43 & \textbf{75.36} & 68.61 & \textbf{82.21} & \textbf{64.31} & 55.57 \\
 & ACC $\uparrow$ & 77.44 & \textbf{80.43} & \textbf{89.41} & 88.74 & 73.38 & \textbf{75.36} \\
\midrule
 & ECE $\downarrow$ & 28.03 & \textbf{10.17} & 89.60 & \textbf{24.49} & 30.40 & \textbf{12.00} \\
 SVAMP & AUC $\uparrow$ & 74.17 & \textbf{75.79} & 75.10 & \textbf{87.46} & 49.33 & \textbf{71.27} \\
 & ACC $\uparrow$ & 72.60 & \textbf{75.29} & 90.48 & \textbf{92.00} & 52.27 & \textbf{57.48} \\
\midrule
 & ECE $\downarrow$ & 5.45 & \textbf{5.00} & 57.58 & \textbf{5.62} & 20.19 & \textbf{11.36} \\
 ARC\_easy & AUC $\uparrow$ & \textbf{81.16} & 76.89 & 66.10 & \textbf{76.75} & 62.89 & \textbf{66.86} \\
 & ACC $\uparrow$ & 87.73 & \textbf{89.21} & \textbf{92.11} & 92.01 & 54.00 & \textbf{56.74} \\
\midrule
\midrule
\multicolumn{8}{l}{\textit{Out-of-Domain Datasets}} \\
 & ECE $\downarrow$ & 7.01 & \textbf{6.03} & 55.19 & \textbf{10.11} & 11.42 & \textbf{5.46} \\
 ARC\_challenge & AUC $\uparrow$ & \textbf{80.67} & 80.41 & 64.21 & \textbf{78.33} & 64.07 & \textbf{65.27} \\
 & ACC $\uparrow$ & 80.87 & \textbf{82.38} & \textbf{89.37} & 89.05 & 43.39 & \textbf{45.77} \\
\midrule
 & ECE $\downarrow$ & 27.85 & \textbf{9.69} & 72.41 & \textbf{5.82} & 47.26 & \textbf{4.60} \\
 Object Counting & AUC $\uparrow$ & 53.84 & \textbf{59.47} & 68.07 & \textbf{81.02} & 50.39 & \textbf{67.61} \\
 & ACC $\uparrow$ & 60.68 & \textbf{65.88} & 72.41 & \textbf{74.22} & 55.33 & \textbf{58.13} \\
\midrule
 & ECE $\downarrow$ & 12.55 & \textbf{8.64} & 62.35 & \textbf{18.92} & 13.16 & \textbf{4.34} \\
 MathQA & AUC $\uparrow$ & 85.23 & \textbf{87.21} & \textbf{72.48} & 69.80 & \textbf{78.89} & 66.09 \\
 & ACC $\uparrow$ & 44.18 & \textbf{52.34} & 49.85 & \textbf{64.18} & 37.69 & \textbf{43.21} \\
\bottomrule
\end{tabular}
\caption{Self-Calibration results across both in-domain and out-of domain datasets on three different models.}
\vspace{-15pt}
\label{tab:sc_results}
\end{table*}
\subsection{Experiment Setup}
\paragraph{Models.}
To evaluate our self-calibration framework and our efficient test-time scaling methods, we conduct experiments on three open-source LLMs: Llama-8B-3.1-Instruct~\footnote{\url{https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct}}~\citep{Dubey2024TheL3}, Qwen2.5-7B-Instruct~\footnote{\url{https://huggingface.co/Qwen/Qwen2.5-7B-Instruct}}~\citep{qwen2.5} and DeepSeek-R1-Distill-Qwen-1.5B~\footnote{\url{https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B}}~\citep{deepseekai2025deepseekr1incentivizingreasoningcapability}.
These models represent diverse architectures and training strategies, allowing us to test the adaptability of our methods. 
% By using open-source models in evaluation, we ensure the reproducibility of our proposed methods and validate their broad applicability across different model architectures.
% \jxc{I do not see the point of this last sentence, it feels redundant as well, since applicability is already mentioned in the previous sentences.}


\paragraph{Seed Datasets.}
We construct our training dataset with diverse reasoning datasets, including: ARC\_easy~\citep{allenai:arc}, commonsense QA~\citep{talmor-etal-2019-commonsenseqa}, LogiQA~\citep{Liu2020LogiQAAC}, GSM8K~\citep{cobbe2021gsm8k}, OpenBookQA~\citep{Mihaylov2018CanAS}, ReClor~\citep{Yu2020ReClorAR}, SciQ~\citep{Welbl2017CrowdsourcingMC}, SVAMP~\citep{patel-etal-2021-nlp} and WindGrande~\citep{Sakaguchi2019WinoGrande}. For each dataset, we randomly sample 2,000 questions from the training set to construct our training data. Additional details are shown in Appendix~\ref{app:hyperparameter}.

\paragraph{Evaluation Datasets and Prompts.}
We evaluate our methods on three benchmark datasets: ARC-Challenge~\citep{allenai:arc}, Object-Counting~\citep{Suzgun2022ChallengingBT} and MathQA~\citep{Amini2019MathQATI}, covering mathematical and commonsense reasoning tasks in both multiple-choice and open-ended formats. ARC-Challenge includes difficult science questions requiring external knowledge and reasoning. Object-Counting focuses on numerical and spatial reasoning by counting objects in various contexts. MathQA tests mathematical problem-solving across arithmetic, algebra, and calculus. 

These three datasets are considered out-of-domain as they differ from the datasets used in training, which we refer as in-domain datasets. To evaluate performance in an in-domain setting, we also use the test sets of GSM8K, SVAMP, and ARC\_easy.
% These datasets provide a diverse and rigorous evaluation of our approach.
The system prompt and the task prompt of each dataset are shown in Appendix~\ref{app:prompt}.
% \paragraph{Hyperparameter}
% When evaluating the performance of t
\paragraph{Baseline Methods.}
In addition to the repeated sampling methods mentioned in Sec.~\ref{sec:basic}, we also include other adaptive test-time scaling methods such as Early-Stopping Self-Consistency (ESC)~\cite{Li2024EscapeSC} and Reasoning-Aware Self-Consistency (RASC)~\cite{Wan2024ReasoningAS} for comparison. 
% Instead of generating a predetermined number of reasoning paths, 
ESC divides the sampling process into sequential windows and halts further sampling when a high-confidence consensus is reached within a window.
RASC enhances sampling efficiency by dynamically evaluating both the generated answers and their corresponding reasoning paths. 
% Since ESC and RASC also aim to improve the efficiency of self-consistency, we include them as baseline methods to analyze the effectiveness of our approach in enhancing reasoning efficiency and accuracy.


% By integrating ESC and RASC as baseline methods, we aim to evaluate their effectiveness in improving reasoning efficiency and accuracy in our specific context.~\jh{Instead of saying we want to evaluate these baseline methods to test their effectiveness, we need to claim why we want to compare our methods against these baselines.}
\begin{table*}[h]
 \centering
 \small
 \setlength{\tabcolsep}{1.5pt}
 \resizebox{\textwidth}{!}{%
 \begin{tabular}{llll|lll|lll}
 \toprule
 & \multicolumn{3}{c}{Llama-3.1-8B-Instruct} & \multicolumn{3}{c}{Qwen2.5-7B-Instruct} & \multicolumn{3}{c}{DeepSeek-R1-Distill-1.5B} \\
 \cmidrule(lr){2-4} \cmidrule(lr){5-7} \cmidrule(lr){8-10}
 Methods & Obj\_C. & MathQA & ARC\_C. & Obj\_C. & MathQA & ARC\_C. & Obj\_C. & MathQA & ARC\_C. \\
 \midrule
 \rowcolor{gray!20} Pass@1 & 67.6 & 71.5 & 82.8 & 76.8 & 82.9 & 88.5 & 61.2 & 89.9 & 58.2 \\ 
 \midrule
 SC & 76.0 & 81.0 & 87.1 & 81.2 & 86.3 & \textbf{91.2} & \textbf{70.8} & 91.6 & 65.6 \\
 SC w/ Conf.* & \textbf{76.8} (+0.8) & 83.4 (+2.4)  & 87.4 (+0.3) & 80.8 (-0.4) & 87.5 (+1.2) & 90.5 (-0.7) & \textbf{70.8} (0.0) & \textbf{91.8} (+0.2) & 65.9 (+0.3) \\
 \rowcolor{blue!10}SC w/ Conf.  & \textbf{76.8} (+0.8) & \textbf{83.6} (+2.6) & \textbf{87.7} (+0.6) & 81.2 (0.0) & \textbf{87.8} (+1.5) & 90.8 (-0.4) & \textbf{70.8} (0.0) & \textbf{91.8} (+0.2) & \textbf{66.5} (+0.9) \\
 Best-of-N & 69.2 & 81.0 & 86.4 & 76.8 & 86.8 & 90.2 & 54.0 & 90.0 & 58.9 \\
 Early Stopping* & \textbf{76.8} (+7.6) & 83.4 (+2.4) & 87.3 (+0.9) & 80.8 (+4.0) & 87.5 (+0.7) & 90.5 (+0.3) & 64.8 (+10.8) & 91.6 (+1.6) & 65.9 (+7.0)\\
 \rowcolor{blue!10}Early Stopping & \textbf{76.8} (+7.6) & \textbf{83.6} (+2.6) & \textbf{87.7} (+1.3) & 81.2 (+4.4) & \textbf{87.8} (+1.0) & 90.8 (+0.6) & \textbf{70.8} (+16.8) & 91.6 (+1.6) & \textbf{66.5} (+7.6) \\
 ASC & 74.8 & 80.0 & 86.5 & \textbf{81.6} & 86.2 & 90.6 & 70.4 & 91.6 & 64.3 \\
 ASC w/ Conf.* & 74.8 (0.0) & 81.6 (+1.6) & 86.6 (+0.1) & \textbf{81.6} (0.0) & 86.9 (+0.7) & 90.4 (-0.2) & 70.4 (0.0) & 91.6 (0.0) & 64.7 (+0.4) \\
 \rowcolor{blue!10}ASC w/ Conf. & 75.2 (+0.4) & 81.9 (+1.9) & 86.6 (+0.1) & \textbf{81.6} (0.0) & 87.2 (+1.0) & \textbf{91.2} (+0.6) & 70.4 (0.0) & \textbf{91.8} (+0.2) & 65.1 (+0.8) \\
 ESC & 76.0 & 81.0 & 87.1 & 81.2 & 86.3 & {91.0} & \textbf{70.8} & 91.3 & 65.6 \\
 RASC & 76.0 & 81.4 & 87.3 & 81.2 & 86.4 & 90.3 & \textbf{70.8} & 91.4 & 65.8 \\
 \bottomrule
 \end{tabular}
 }
 \caption{
 Accuracy comparison of different test-time scaling methods across three language models when the sample budget equals to 16. The evaluation is conducted on three datasets: Obj\_C. (Object\_Counting), MathQA, and ARC\_C. (ARC\_Challenge). ``Sample budget'' refers to the average number of responses sampled per query. The improvements of confidence-augmented methods over their baselines are shown in parentheses. All methods use the same responses generated by Self-Calibration trained models, while methods marked with \textbf{*} use confidence scores from the vanilla model. The results when the sample budget equals 4 are shown in Appendix~\ref{app:full_table}. 
 }
 \vspace{-15pt}
 \label{tab:results}
\end{table*}
\subsection{Evaluation on Self-Calibration}
\label{sec:sc_results}
\paragraph{Evaluation Metrics.} 
We first evaluate how well our Self-Calibration approach enable models to output accurate confidence estimation.
We adopt three standard metrics for evaluating model calibration: Expected Calibration Error (ECE)~\citep{guo2017calibration}, Area Under the Receiver Operating Characteristic Curve (AUC)~\citep{hendrycks2016baseline}, and accuracy (ACC) on both in-domain and out-of-domain datasets. ECE measures the discrepancy between a modelâ€™s predicted confidence and its actual accuracy, defined as:
\vspace{-5pt}
\[
\text{ECE} = \sum_{m=1}^{M} \frac{|B_m|}{N} \left| \text{acc}(B_m) - \text{conf}(B_m) \right|,
\]
where $M$ is the number of bins, $B_m$ represents the set of samples in the $m$-th bin, and $N$ is the total number of samples.
A lower ECE value indicates better calibration, meaning the modelâ€™s confidence aligns more closely with its actual correctness.



\paragraph{Results.}


In Table~\ref{tab:sc_results}, 
% we present the results of our self-calibration framework across multiple in-domain and out-of-domain datasets. 
we compare our models trained on Self-Calibration objective with their vanilla base models on multiple in-domain and out-of-domain datasets. Self-Calibration trained models consistently lower the ECE score while generally improve accuracy. On GSM8K, Self-Calibration reduces ECE from 13.70 to 3.79 while improving accuracy from 77.44\% to 80.43\%. Even in cases where ECE slightly increases, such as ARC\_easy for Llama-3.1-8B-Instruct, accuracy still improves from 87.73\% to 89.21\%. Moreover, the strong results on out-of-domain tasks demonstrate the generalizability of our method, as seen in MathQA, where accuracy improves from 49.85\% to 64.18\% for Qwen2.5-7B-Instruct.


\paragraph{Ablation Study.}
\begin{table}[t]
\centering
\small
%\begin{tabular}{lcc|cc}
\begin{tabularx}{\columnwidth}{l c c | c c}
\toprule
 & \multicolumn{2}{c}{\textbf{MathQA}} & \multicolumn{2}{c}{\textbf{Object Counting}} \\
 \cmidrule(lr){2-3} \cmidrule(lr){4-5}
\textbf{Method} & \textbf{ECE $\downarrow$} & \textbf{ACC $\uparrow$} & \textbf{ECE $\downarrow$} & \textbf{ACC $\uparrow$} \\
\midrule
ours (full) & 8.64 & 52.34 & 9.69 & 65.88 \\
\hspace{0.5em}w/o EDT & 9.14 & 51.44 & 10.40 & 62.88 \\
\hspace{0.5em}w/o SSC & 10.85 & 52.18 & 16.02 & 61.12 \\
\hspace{0.5em}w/o L1-smooth & 6.43 & 50.86 & 10.48 & 56.48 \\
\bottomrule
%\end{tabular}
\end{tabularx}
\caption{Ablation study results on MathQA and Object Counting in Llama-3.1-8B-Instruct. ``w/o L1-smooth'' means using MSE loss instead of L1-smooth.}
\vspace{-15pt}
\label{tab:ablation}
\end{table}



% To further analyze the impact of each component in our Self-Calibration framework, 
We conduct an ablation study
% study on the MathQA and Object Counting datasets. Specifically, we 
to investigate the impact of key components in Self-Calibration, including Dynamic Temperature (EDT), Soft Self-Consistency (SSC), and L1-smooth loss. 
Table~\ref{tab:ablation} presents our ablation results on the MathQA and Object Counting datasets. 
% Our tuned Llama-3.1-8B-Instruct model achieves the best balance of low ECE and high accuracy, underscoring the effectiveness of both the dynamic temperature and soft self-consistency score components. 
Removing the dynamic temperature or the soft self-consistency score leads to noticeable increases in ECE and/or drops in accuracy. 
Meanwhile, replacing the L1-smooth objective with MSE achieves slightly lower ECE on MathQA but reduces accuracy on both tasks, suggesting that our chosen loss formulation is more robust overall.
These results demonstrate that each module contributes to model calibration and reasoning performance.

\subsection{Evaluation on Efficient Test-time Scaling}
% \jh{I think in sec. 5.2, we are comparing an uncalibrated model and a calibrated model, but in sec. 5.3, we are comparing different scaling strategies on the same calibrated models? That means we need to clearly convey this signal to the readers, to convince them that this is fair comparison. Please clearly explain this.} \ljc{To add to Jiaxin's point, I want to clarify if the basic scaling method (e.g. SC) uses the untuned model, while your confidence-weighted scaling method (e.g. SC w/ conf) uses the calibration-tuned model? If the underlying models are different, it might give your scaling method an unintended advantage, because the calibration-tuned model is trained on more targeted datasets (I understand you have in-domain and out-of-domain, but ARC-easy is pretty targeting for ARC-challenge). To ensure fair comparison, you might need to tune the default model with a supervised method on the same in-domain datasets.}


To ensure fair comparison across different test-time scaling methods, we use the same sample budgets for each of them. Sample budget refers to the average number of responses each method samples per query. 
% A higher sample budget typically enables more thorough exploration, but it also increases computational cost. 
For dynamic methods such as Early Stopping and ASC w/ Conf., we set internal thresholds so that the actual number of samples collected in practice is close to a target budget. To ensure a fair comparison, all methods use responses sampled from Self-Calibration trained models.

Table~\ref{tab:results} shows the accuracy comparison of different methods with a sample budget of 16. We observe that SC w/ Conf., Early Stopping, and ASC w/ Conf. consistently outperform their base counterparts. 
% For example, with a budget of 4 on Llama-3.1-8B-Instruct, SC w/ Conf. improves over SC on MathQA (78.8 to 82.1), while ASC w/ Conf. outperforms ASC (80.0 to 81.9). Similar trends appear with a budget of 16. 
On Llama-3.1-8B-Instruct, SC w/ Conf. surpasses SC on MathQA (81.0 to 83.6), while on DeepSeek-R1-Distill-1.B,
Early Stopping outperforms Best-of-N on ARC\_challenge (58.9 to 66.5). These results highlight that integrating calibrated confidence enhances test-time scaling with the same sampling budget. 
We also compare our approach with methods that use uncalibrated confidenc scores from the vanilla model (indicated by *). These methods generally underperform confidence from Self-Calibration trained model, indicating the necessity of confidence calibration. The results when the sample budget equals 4 are shown in Appendix~\ref{app:full_table}.







% At a budget of 4, each dynamic method yields higher accuracy by quickly filtering out or stopping unpromising samples, while at a budget of 16, allowing for more thorough sampling further improves final performance. 



% ~\jh{thorough sampling improves performance -- this is why we need test-time scaling, but how is this related to the paper's goal and claim? I am confused by the discussion of budget of 4 and 16 here.}
% These results confirm that incorporating confidence estimation and dynamic stopping mechanisms provides consistent gains over static approaches, and that allowing more sampling generally leads to stronger overall accuracy.~\jh{Make it clear here as well.}