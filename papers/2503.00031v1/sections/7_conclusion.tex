\section{Conclusion}

We improve the efficiency of test-time scaling methods in LLMs with reliable confidence estimation.
Our Self-Calibration enhances LLM confidence estimation in one forward pass, without requiring any labeled data. 
We then propose efficient test-time scaling by dynamically adjusting sampling strategies based on calibrated confidence scores, such as Early-Stopping for Best-of-N and Self-Consistency with calibrated confidence. Experiments show that our approaches consistently outperform baselines under the same sample budget. Our findings suggest that reliable confidence estimation and dynamic sampling can substantially enhance the effectiveness and efficiency of test-time scaling approaches.

\section*{Acknowledgment}
This research was supported in part by the NVIDIA Academic Grant Program.
