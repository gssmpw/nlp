\section{Repeated Sampling}
\label{sec:basic}
Repeated sampling~\citep{Brown2024LargeLM} is a framework that generates multiple responses with Chain-of-Thought prompting~\citep{Wei2022ChainOT}, then uses a verifier to get the final results.
We will introduce three fundamental repeated sampling strategies, which aim to enhance response quality by selecting the most suitable answer from multiple generated candidates.
\subsection{Best-of-N}
For each input query $x$, multiple candidate responses $\{y_i\}$ are sampled, where \(1 \leq i \leq N\). A scoring function---such as an additional reward model or a confidence generator---assigns each response a score \(c_i = \text{Score}(y_i)\). The simplest selection strategy, known as Best-of-N~\cite{Cobbe2021TrainingVT}, chooses the response with the highest score as the final answer as  
$
\hat{y} = \arg\max\limits_{y}\;c_j
$.

\subsection{Self-Consistency}
Self-Consistency~\cite{Wang2022SelfConsistencyIC} selects the most frequent response among multiple sampled candidates. Given candidate responses \(\{y_1, y_2, \dots, y_N\}\), the final answer is determined by majority voting:
{
$$
\hat{y} \;=\;
\arg\max_{z}\;
\sum_{i=1}^{N}\;\Iverson{y_i = z}.
$$
}
This approach enhances robustness by aggregating diverse model outputs rather than relying on a single highest-scoring response.

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{figures/Self-Calibration.pdf}
    \caption{Illustration of the Self-Calibration framework. Given a query from the seed dataset, we sample $N$ responses from the LLM. We use a confidence querying prompt to let LLM assign a confidence score to each response. Responses are then grouped by their answers, and the Soft Self-Consistency (SSC) score is computed for each group. During training, all data tuples contribute to improving the model's calibration, while higher-confidence data is used to enhance the LLM's generation ability.}
    \vspace{-15pt}
    \label{fig:self-Calibration}
\end{figure*}


\subsection{Adaptive Self-Consistency}
Adaptive Self-Consistency (ASC)~\cite{Aggarwal2023LetsSS} enhances the standard Self-Consistency approach by dynamically adjusting the number of samples based on agreement among generated responses. This method iteratively samples responses and calculates the cumulative frequency \( v_k(z) \) and relative frequency \( \hat{r}_k(z) \) of each unique answer \( z \) after \( k \) samples:
% \vspace{-5pt}
\[
v_k(z) = \sum_{i=1}^k \Iverson{y_i = z}, \quad
\hat{r}_k(z) = \frac{v_k(z)}{k}.
\]

The sampling process continues until the maximum relative frequency \( \hat{r}_k(z) \) exceeds a predefined threshold \( \tau \). Formally:
\vspace{-5pt}
\[
\begin{cases}
    k \gets k+1, & \text{if } \max\limits_{z} \hat{r}_k(z) < \tau, \\
    y = \arg\max\limits_{z} \hat{r}_k(z), & \text{otherwise}.
\end{cases}
\]

This adaptive strategy reduces computational costs by limiting the number of required samples while maintaining high accuracy in the final answer selection.