

\section{Self-Calibration}
\label{sec:self-cal}
In this section, we provide an overview of our proposed Self-Calibration framework, illustrated in
Fig.~\ref{fig:self-Calibration}. First, we synthesize a set of input-output-confidence tuples $(x_i, y_i, c_i)$ from a seed dataset for training, without requiring any ground-truth answer (Sec.~\ref{sec:data_gen}). Using this synthetic dataset, we can train a language model with a combined loss to output calibrated confidence scores (Sec.~\ref{sec:sctraining}). 
% Finally, we evaluate the calibration results to demonstrate the effectivessness of our proposed framework (Sec.~\ref{sec:sc_results}).

\subsection{Confidence Score Estimation}

% \ljc{This subsection makes it feel like you've already solved confidence scoring by this subsection. Perhaps start with ``A naive way to obtain confidence score is P(True) ...''}
A naive way to obtain a confidence score from LLM is $\operatorname{P}(\text{True})$~\citep{Kadavath2022LanguageM}.
Given the input-output pair $(x_i,y_i)$, we construct a prompt as $ x_i\oplus y_i\oplus I$, where $I$ is a confidence querying prompt, ``\text{Is the answer correct? (Yes/No)}''.
The confidence score is then defined as the probability of token ``\textbf{Yes}'' in the next position. $$c(x,y)=p_\theta (\textbf{Yes}|x,y,I)$$ Due to the KV-cache mechanism~\citep{Pope2022EfficientlyST}, the additional computational cost is roughly equivalent to generating 10 tokens, which is negligible compared to the typically longer input and output sequences. Empirical results suggest that $\operatorname{P}(\text{True})$ often lacks calibration, leading to overconfidence in incorrect answers~\cite{tian2023just}. So we aim to use supervised training to improve the calibration of $\operatorname{P}(\text{True})$, helping LLMs produce more reliable confidence scores.

\subsection{Training Data Generation}
% \jh{Here we want to emphasize that this data generation process does not need human annotated answer. Can you go over this subsection to refine it?}
\label{sec:data_gen}
% Here, we detail the process of generating training data $ (x,y,c)$. 
% The LLM first generates several different responses by dynamic temperature. Then we will use soft self-consistency to generate the confidence about $(x,y)$.~\jh{This paragraph is not a good summary because there is no logical chain. Instead of saying we do step 1, step 2, and step 3, we need to say, in order to fulfill goal 1, we do step 1; in order to fulfill goal 2, we do step 2. And the goals need to be related to your ultimate goal. This will make the paragraph more logical. Please go over all the method sections and make them more logical whenever you bring up a new technique/formula/equation.}

Our goal is to create a labeled dataset \(D_t = (x,y,c)_i\) without human annotations, 
where \((x,y)\) is a query--response pair and \(c\) is an accurate confidence. To achieve this, we first generate multiple candidate answers for each query and ensure diversity via Dynamic Temperature sampling. Next, we calibrate the confidence of each candidate through Soft Self-Consistency, which integrates the model's intrinsic probability estimate with the overall agreement among different responses.

\paragraph{Soft Self-Consistency Score.}
\label{sec:ssc}
Previous work has shown that self-consistency scores provide strong zero-shot calibration~\citep{Wang2024SelfConsistencyBC}, outperforming $\operatorname{P}(\text{True})$ or raw logits as confidence measures~\citep{Guo2017OnCO}. To further enhance the reliability of the confidence score in the training set, we introduce a soft self-consistency score, which integrates $\operatorname{P}(\text{True})$ with self-consistency and offers a more accurate and robust confidence estimation.

For each query $x$, we use the LLM to generate $N$ different responses, each with an associated confidence score. Given the set of triplets $ (x,y_n, c_n)$ where $1\leq n \leq N$, we compute the soft self-consistency (SSC) score as:
\[
\text{SSC}(y) = \frac{\sum_{i: y_i = y} c_i}{\sum_{i=1}^{N} c_i}.
\]
Using this score, we construct the final training set as $(x,y_i,\operatorname{SSC}(y_i))$, where $\operatorname{SSC}(y_i)$ provides a calibrated confidence estimation for each response.


\paragraph{Dynamic Temperature.}
\label{sec:dynam}
To generate more diverse and high-quality responses, we adopt the Entropy-based Dynamic Temperature (EDT) Sampling method~\citep{Zhang2024EDTIL} when generating each response \(y\).
By adaptively increasing the temperature when the entropy \(H\) of the output distribution is low, EDT promotes greater response diversity while preserving output quality.
Formally, the temperature \(T(H)\) is defined as:
\vspace{-5pt}
\[
T(H) =
\begin{cases}
T_0 \times M^{\gamma / H}, & \text{if } T_0 \times M^{\gamma / H} \ge \tau_0, \\
0, & \text{otherwise},
\end{cases}
\]
where \(T_0\) is the base temperature, \(M\) is a scaling factor, \(\gamma\) affects the scale of temperature variations, and \(\tau\) is a threshold that sets the temperature to zero if \(T_0 \times M^{\gamma / H}\) is below \(\tau_0\).




% ~\jh{explain these symbols, note that $\theta$ is already used as LLM parameter so we can replace the $\theta$ here with sth. else.}




\subsection{Training Objective}
\label{sec:sctraining}
We optimize the model's confidence estimation by minimizing the difference between the predicted confidence and the target confidence using the SmoothL1 loss. To ensure that training on confidence estimation does not degrade the model's reasoning ability, we incorporate the standard generation loss of Chain-of-Thought answers into the objective~\cite{Huang2022LargeLM}. Specifically, only responses with confidence scores above a threshold $\eta$ are selected for training to guarantee the quality of the reasoning path. A weighting coefficient $w$ is introduced to balance these two loss terms. The overall loss function is formulated as:
\[
\small
\begin{aligned}
\mathcal{L}_{\text{total}}(\theta)
&=\;\sum_{(x_j,\,y_j)\in \mathcal{D}}
\mathrm{SmoothL1}\Bigl(
\,p_\theta(\operatorname{Yes}\mid x_j,y_j,I),\; c_j
\Bigr)\,\\
&\quad +\omega \sum_{\substack{(x_i,\,y_i) \\ c_i > \eta}}
\Bigl(
-\log\,p_\theta\bigl(y_i \,\bigm\vert\, x_i\bigr)
\Bigr).
\\
\end{aligned}
\]



\section{Confidence-Guided Test-Time Scaling}
\label{sec:refined}
We then introduce how to incorporate reliable confidence scores obtained from Self-Calibration to existing test-time scaling methods.
% In this section, we discuss how the model's confidence score can enhance efficient repeated sampling methods. First, we present refined versions of the methods introduced in Sec.~\ref{sec:basic} by incorporating the confidence score. 
% Then, we will introduce a new metric, Relative Selection Gain, to evaluate the performance of each method (Sec.~\ref{sec:metric}).

% \subsection{Refined Version}


\subsection{Early Stopping with Confidence}
Early Stopping improves the efficiency of Best-of-N by dynamically terminating the sampling process once a response with sufficient confidence is found. Given a sequential sampling process where each response \( y_i \) is assigned a confidence score \( c_i \), we follow this rule:

\[
\begin{cases}
    k \gets k+1, & \text{if } c_i < \tau, \\
    y = y_i, & \text{otherwise}.
\end{cases}
\]

This means that we continue sampling responses one by one until a response meets the confidence threshold \( \tau \), and such a response is selected as the final answer, avoiding unnecessary additional sampling and reducing computational overhead.


\subsection{Self-Consistency with Confidence}  
Self-Consistency with Confidence extends the traditional Self-Consistency approach by incorporating confidence scores into the voting process. Instead of treating all sampled responses equally, we assign each response \( y_i \) a confidence score \( c_i \), leading to a weighted aggregation:
\vspace{-5pt}
\[
y \;=\;
\arg\max_{z}\;\sum_{i=1}^{N}\;c_i\,\Iverson{\,y_i = z\,}.
\]

This modification ensures that responses with higher confidence contribute more significantly to the final selection, enhancing robustness by prioritizing more reliable predictions.


\subsection{Adaptive Self-Consistency with Confidence}
Similar to Self-Consistency with Confidence, we use confidence as the weight when calculating the relative frequency in Adaptive Self-Consistency. 
% Then we will follow the rest part in Adaptive Self-Consistency.
\[
v_k(z) = \sum_{i=1}^k c_i\Iverson{y_i = z}, \quad
\hat{r}_k(z) = \frac{v_k(z)}{\sum_{i=1}^k c_i}.
\]

% \subsection{Relative Selection Gain}
% \label{sec:metric}
% To better evaluate the efficiency of different test-time scaling methods, we propose a new metric, Relative Selection Gain (\metric).

% We first revisit the upper bound of methods that rely on repeated sampling. Suppose we have an oracle selector capable of identifying the correct answer from candidate responses. The performance in this idealized setting is given by $\text{pass@}N$~\citep{Chen2021EvaluatingLL}:
% \[
% \text{pass@}N =
% \frac{1}{|\mathcal{D}|}
% \sum_{x \in \mathcal{D}}
% \Iverson{\exists i \in [1, N], y_i \text{ is correct}}.
% \]
% To quantify the effectiveness of test-time scaling, we measure its relative performance compared to $\text{pass@}N$. Specifically, we define \metric\space as:
% \[
% \text{RSG}_N = \frac{\text{Acc@}N - \text{pass@}1}{\text{pass@}N - \text{pass@}1},
% \]

% This metric captures how effectively a method recovers the maximum possible accuracy gain. A higher \metric\space indicates better utilization of multiple samples, approaching oracle-level performance.

