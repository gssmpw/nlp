\section{Related Work}

\subsection{Test-Time Scaling}
% \jh{I feel that there are a lot of work in test-time scaling in addition to the ones you mentioned. Can you check out this tutorial: https://cmu-l3.github.io/neurips2024-inference-tutorial/ and list a few highly-cited ones? Also, you can check out their slides on the website.}
\citet{snell2024scaling} studied optimal test-time compute allocation to significantly enhance efficiency.
Self-Enhanced tree search frameworks~\citep{bi2024forest, lample2022hypertreeproofsearchneural, koh2024treesearchlanguagemodel} aggregate multiple reasoning paths and employs sparse activation strategies. Beyond that, step-wise verifiers are leveraged to dynamically prune the search tree~\citep{wang2023selfconsistencyimproveschainthought, li2023makinglargelanguagemodels, lightman2023letsverifystepstep}.
Additionally, \citet{chen2024simple} developed a two-stage elimination-based approach where multiple candidates are iteratively refined through pairwise comparisons.
Combining different versions of the same query can also improve the final performance~\cite{Huang2024DivideRA}.
Scaling~\citep{chen2025sets, welleck2022generatingsequenceslearningselfcorrect, wang2024theoreticalunderstandingselfcorrectionincontext, chen2023teachinglargelanguagemodels, madaan2023selfrefineiterativerefinementselffeedback, aggarwal2024alphaverusbootstrappingformallyverified} that iteratively refines model outputs, leading to improved performance in complex tasks.
\citet{muennighoff2025s1} proposed \textit{s1}, a simple test-time scaling method that enforces a budget constraint on inference length to optimize computational resource utilization. 

\subsection{Model Calibration}

Model calibration aims to align a modelâ€™s confidence with its accuracy. LLMs often exhibit overconfidence~\citep{tian2023just, chen2024reconfidencing, xiong2023can, achiam2023gpt}. Prior research has explored scaling-based methods~\citep{deng2023great, guo2017calibration, zhang2020mix} and nonparametric techniques like binning~\citep{zadrozny2001obtaining}. More recent work has introduced verbalized confidence, prompting models to directly output confidence scores~\citep{lin2022teaching}. Most studies focus on pre-trained and instruction-tuned LLMs~\citep{lin2022teaching, han2024enhancing}, others investigate RLHF-trained LLMs and propose calibration through prompting strategies~\citep{xiong2023can, tian2023just}. Reinforcement learning has also been leveraged for calibration~\citep{xu2024sayself, tao2024trust}, aligning closely with our study. A more calibrated reward model can also help model calibration by PPO framework~\citep{Leng2024TamingOI}.


\subsection{LLM Verifier}
Recently, various LLM verifiers are developed to enhance the reasoning capabilities of LLMs. Our approach is closely related to LLM-based verifiers, as both aim to evaluate whether a generated response meets correctness criteria.
\citet{Lightman2023LetsVS} trained verifiers that assess the correctness of generated solutions, enhancing the selection of accurate responses.
LLM-as-a-Judge~\citep{Zheng2023JudgingLW} employs large language models to adjudicate between multiple generated outputs based on learned preferences.
\citet{Zhang2024GenerativeVR} trained verifiers using next-token prediction to enhance reasoning performance in large language models.
GenRM~\citep{Mahan2024GenerativeRM} is an iterative algorithm that trains large language models on self-generated reasoning traces to align synthetic preference labels with human judgments.


