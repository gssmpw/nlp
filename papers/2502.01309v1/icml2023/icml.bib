

@misc{oquab_dinov2_2024,
	title = {{DINOv2}: {Learning} {Robust} {Visual} {Features} without {Supervision}},
	shorttitle = {{DINOv2}},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a ViT model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, OpenCLIP (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	month = feb,
	year = {2024},
	note = {arXiv:2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/rupertmenneer/Zotero/storage/ATVIK5DY/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf:application/pdf;Snapshot:/Users/rupertmenneer/Zotero/storage/YCRJQVWS/2304.html:text/html},
}

@misc{stein_exposing_2023,
	title = {Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models},
	url = {http://arxiv.org/abs/2306.04675},
	doi = {10.48550/arXiv.2306.04675},
	abstract = {We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them. Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations. Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3. We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Stein, George and Cresswell, Jesse C. and Hosseinzadeh, Rasa and Sui, Yi and Ross, Brendan Leigh and Villecroze, Valentin and Liu, Zhaoyan and Caterini, Anthony L. and Taylor, J. Eric T. and Loaiza-Ganem, Gabriel},
	month = oct,
	year = {2023},
	note = {arXiv:2306.04675 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2023. 53 pages, 29 figures, 12 tables. Code at https://github.com/layer6ai-labs/dgm-eval, reviews at https://openreview.net/forum?id=08zf7kTOoh},
}

@misc{gokay_graph2pix_2021,
	title = {{Graph2Pix}: {A} {Graph}-{Based} {Image} to {Image} {Translation} {Framework}},
	shorttitle = {{Graph2Pix}},
	url = {http://arxiv.org/abs/2108.09752},
	doi = {10.48550/arXiv.2108.09752},
	abstract = {In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (http://artbreeder.com), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25\% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5\% of the time. Our source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Gokay, Dilara and Simsar, Enis and Atici, Efehan and Ahmetoglu, Alper and Yuksel, Atif Emre and Yanardag, Pinar},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FJWTDJ2D/Gokay et al. - 2021 - Graph2Pix A Graph-Based Image to Image Translatio.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/GSLEMF7B/2108.html:text/html},
}

@misc{pan_learning_2022,
	title = {Learning {Hierarchical} {Graph} {Representation} for {Image} {Manipulation} {Detection}},
	url = {https://arxiv.org/abs/2201.05730v1},
	abstract = {The objective of image manipulation detection is to identify and locate the manipulated regions in the images. Recent approaches mostly adopt the sophisticated Convolutional Neural Networks (CNNs) to capture the tampering artifacts left in the images to locate the manipulated regions. However, these approaches ignore the feature correlations, i.e., feature inconsistencies, between manipulated regions and non-manipulated regions, leading to inferior detection performance. To address this issue, we propose a hierarchical Graph Convolutional Network (HGCN-Net), which consists of two parallel branches: the backbone network branch and the hierarchical graph representation learning (HGRL) branch for image manipulation detection. Specifically, the feature maps of a given image are extracted by the backbone network branch, and then the feature correlations within the feature maps are modeled as a set of fully-connected graphs for learning the hierarchical graph representation by the HGRL branch. The learned hierarchical graph representation can sufficiently capture the feature correlations across different scales, and thus it provides high discriminability for distinguishing manipulated and non-manipulated regions. Extensive experiments on four public datasets demonstrate that the proposed HGCN-Net not only provides promising detection accuracy, but also achieves strong robustness under a variety of common image attacks in the task of image manipulation detection, compared to the state-of-the-arts.},
	language = {en},
	urldate = {2024-09-06},
	journal = {arXiv.org},
	author = {Pan, Wenyan and Zhou, Zhili and Ling, Miaogen and Geng, Xin and Wu, Q. M. Jonathan},
	month = jan,
	year = {2022},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/RMLK5NPW/Pan et al. - 2022 - Learning Hierarchical Graph Representation for Ima.pdf:application/pdf},
}

@misc{nazir_survey_2021,
	title = {Survey of {Image} {Based} {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2106.06307},
	doi = {10.48550/arXiv.2106.06307},
	abstract = {In this survey paper, we analyze image based graph neural networks and propose a three-step classification approach. We first convert the image into superpixels using the Quickshift algorithm so as to reduce 30\% of the input data. The superpixels are subsequently used to generate a region adjacency graph. Finally, the graph is passed through a state-of-art graph convolutional neural network to get classification scores. We also analyze the spatial and spectral convolution filtering techniques in graph neural networks. Spectral-based models perform better than spatial-based models and classical CNN with lesser compute cost.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Nazir, Usman and Wang, He and Taj, Murtaza},
	month = jun,
	year = {2021},
	note = {arXiv:2106.06307 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QGWBL7FW/Nazir et al. - 2021 - Survey of Image Based Graph Neural Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/2MM47NID/2106.html:text/html},
}

@misc{defferrard_convolutional_2017,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	url = {http://arxiv.org/abs/1606.09375},
	doi = {10.48550/arXiv.1606.09375},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	month = feb,
	year = {2017},
	note = {arXiv:1606.09375 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2016 final revision},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/98EV3V3W/Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/32PHVR9W/1606.html:text/html},
}

@article{liu_cnn-enhanced_2021,
	title = {{CNN}-{Enhanced} {Graph} {Convolutional} {Network} {With} {Pixel}- and {Superpixel}-{Level} {Feature} {Fusion} for {Hyperspectral} {Image} {Classification}},
	volume = {59},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/document/9268479},
	doi = {10.1109/TGRS.2020.3037361},
	abstract = {Recently, the graph convolutional network (GCN) has drawn increasing attention in the hyperspectral image (HSI) classification. Compared with the convolutional neural network (CNN) with fixed square kernels, GCN can explicitly utilize the correlation between adjacent land covers and conduct flexible convolution on arbitrarily irregular image regions; hence, the HSI spatial contextual structure can be better modeled. However, to reduce the computational complexity and promote the semantic structure learning of land covers, GCN usually works on superpixel-based nodes rather than pixel-based nodes; thus, the pixel-level spectral–spatial features cannot be captured. To fully leverage the advantages of the CNN and GCN, we propose a heterogeneous deep network called CNN-enhanced GCN (CEGCN), in which CNN and GCN branches perform feature learning on small-scale regular regions and large-scale irregular regions, and generate complementary spectral–spatial features at pixel and superpixel levels, respectively. To alleviate the structural incompatibility of the data representation between the Euclidean data-oriented CNN and non-Euclidean data-oriented GCN, we propose the graph encoder and decoder to propagate features between image pixels and graph nodes, thus enabling the CNN and GCN to collaborate in a single network. In contrast to other GCN-based methods that encode HSI into a graph during preprocessing, we integrate the graph encoding process into the network and learn edge weights from training data, which can promote the node feature learning and make the graph more adaptive to HSI content. Extensive experiments on three data sets demonstrate that the proposed CEGCN is both qualitatively and quantitatively competitive compared with other state-of-the-art methods.},
	number = {10},
	urldate = {2024-09-09},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Liu, Qichao and Xiao, Liang and Yang, Jingxiang and Wei, Zhihui},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Computational modeling, Convolution, Convolutional neural network (CNN), Decoding, Deep learning, Feature extraction, feature fusion, graph convolutional network (GCN), graph encoder and decoder, hyperspectral image (HSI) classification, Kernel, Training data},
	pages = {8657--8671},
	file = {IEEE Xplore Abstract Record:/Users/rupertmenneer/Zotero/storage/7CB7PRW6/9268479.html:text/html;IEEE Xplore Full Text PDF:/Users/rupertmenneer/Zotero/storage/YCAFY699/Liu et al. - 2021 - CNN-Enhanced Graph Convolutional Network With Pixe.pdf:application/pdf},
}

@misc{wan_multi-scale_2019,
	title = {Multi-scale {Dynamic} {Graph} {Convolutional} {Network} for {Hyperspectral} {Image} {Classification}},
	url = {http://arxiv.org/abs/1905.06133},
	doi = {10.48550/arXiv.1905.06133},
	abstract = {Convolutional Neural Network (CNN) has demonstrated impressive ability to represent hyperspectral images and to achieve promising results in hyperspectral image classification. However, traditional CNN models can only operate convolution on regular square image regions with fixed size and weights, so they cannot universally adapt to the distinct local regions with various object distributions and geometric appearances. Therefore, their classification performances are still to be improved, especially in class boundaries. To alleviate this shortcoming, we consider employing the recently proposed Graph Convolutional Network (GCN) for hyperspectral image classification, as it can conduct the convolution on arbitrarily structured non-Euclidean data and is applicable to the irregular image regions represented by graph topological information. Different from the commonly used GCN models which work on a fixed graph, we enable the graph to be dynamically updated along with the graph convolution process, so that these two steps can be benefited from each other to gradually produce the discriminative embedded features as well as a refined graph. Moreover, to comprehensively deploy the multi-scale information inherited by hyperspectral images, we establish multiple input graphs with different neighborhood scales to extensively exploit the diversified spectral-spatial correlations at multiple scales. Therefore, our method is termed 'Multi-scale Dynamic Graph Convolutional Network' (MDGCN). The experimental results on three typical benchmark datasets firmly demonstrate the superiority of the proposed MDGCN to other state-of-the-art methods in both qualitative and quantitative aspects.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Wan, Sheng and Gong, Chen and Zhong, Ping and Du, Bo and Zhang, Lefei and Yang, Jian},
	month = may,
	year = {2019},
	note = {arXiv:1905.06133 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/6H4BPGJY/Wan et al. - 2019 - Multi-scale Dynamic Graph Convolutional Network fo.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HJI5NZXJ/1905.html:text/html},
}

@inproceedings{krzywda_graph_2022,
	title = {Graph {Neural} {Networks} in {Computer} {Vision} -- {Architectures}, {Datasets} and {Common} {Approaches}},
	url = {http://arxiv.org/abs/2212.10207},
	doi = {10.1109/IJCNN55064.2022.9892658},
	abstract = {Graph Neural Networks (GNNs) are a family of graph networks inspired by mechanisms existing between nodes on a graph. In recent years there has been an increased interest in GNN and their derivatives, i.e., Graph Attention Networks (GAT), Graph Convolutional Networks (GCN), and Graph Recurrent Networks (GRN). An increase in their usability in computer vision is also observed. The number of GNN applications in this field continues to expand; it includes video analysis and understanding, action and behavior recognition, computational photography, image and video synthesis from zero or few shots, and many more. This contribution aims to collect papers published about GNN-based approaches towards computer vision. They are described and summarized from three perspectives. Firstly, we investigate the architectures of Graph Neural Networks and their derivatives used in this area to provide accurate and explainable recommendations for the ensuing investigations. As for the other aspect, we also present datasets used in these works. Finally, using graph analysis, we also examine relations between GNN-based studies in computer vision and potential sources of inspiration identified outside of this field.},
	urldate = {2024-09-05},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Krzywda, Maciej and Łukasik, Szymon and Gandomi, Amir H.},
	month = jul,
	year = {2022},
	note = {arXiv:2212.10207 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {1--10},
	annote = {Comment: 2022 International Joint Conference on Neural Networks (IJCNN), 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/AGKJZRCY/Krzywda et al. - 2022 - Graph Neural Networks in Computer Vision -- Archit.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/YW6MCIPB/2212.html:text/html},
}

@inproceedings{tarasiewicz_graph_2021,
	title = {A {Graph} {Neural} {Network} {For} {Multiple}-{Image} {Super}-{Resolution}},
	url = {https://ieeexplore.ieee.org/document/9506070},
	doi = {10.1109/ICIP42928.2021.9506070},
	abstract = {Super-resolution consists in reconstructing a high-resolution image from single or multiple low-resolution observations. Deep learning has been reported extremely successful for single-image super-resolution, but its applications to the multiple-image scenarios are limited due to the challenges that arise from feeding a network with a stack of images with sub-pixel translations. In this paper, we introduce Magnet—a new graph neural network that benefits from representing the input low-resolution images as a graph. This enables us to exploit the sub-pixel shifts among the input images while preserving the original low-resolution pixel values for feature extraction and information fusion. Despite a relatively simple architecture, Magnet outperforms the state-of-the-art methods for multiple-image super-resolution, and due to the flexible graph representation, it allows for using a variable number of low-resolution images for reconstruction.},
	urldate = {2024-09-05},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Tarasiewicz, Tomasz and Nalepa, Jakub and Kawulok, Michal},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Conferences, deep learning, Deep learning, Feature extraction, graph neural networks, Graph neural networks, Image reconstruction, Magnetic resonance imaging, Super-resolution reconstruction, Superresolution},
	pages = {1824--1828},
	file = {IEEE Xplore Abstract Record:/Users/rupertmenneer/Zotero/storage/Z3WMBUAT/9506070.html:text/html},
}

@misc{ho_classifier-free_2022,
	title = {Classifier-{Free} {Diffusion} {Guidance}},
	url = {https://arxiv.org/abs/2207.12598v1},
	abstract = {Classifier guidance is a recently introduced method to trade off mode coverage and sample fidelity in conditional diffusion models post training, in the same spirit as low temperature sampling or truncation in other types of generative models. Classifier guidance combines the score estimate of a diffusion model with the gradient of an image classifier and thereby requires training an image classifier separate from the diffusion model. It also raises the question of whether guidance can be performed without a classifier. We show that guidance can be indeed performed by a pure generative model without such a classifier: in what we call classifier-free guidance, we jointly train a conditional and an unconditional diffusion model, and we combine the resulting conditional and unconditional score estimates to attain a trade-off between sample quality and diversity similar to that obtained using classifier guidance.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Ho, Jonathan and Salimans, Tim},
	month = jul,
	year = {2022},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/BHSH458M/Ho and Salimans - 2022 - Classifier-Free Diffusion Guidance.pdf:application/pdf},
}

@misc{lam_graphcast_2022,
	title = {{GraphCast}: {Learning} skillful medium-range global weather forecasting},
	shorttitle = {{GraphCast}},
	url = {https://arxiv.org/abs/2212.12794v2},
	abstract = {Global medium-range weather forecasting is critical to decision-making across many social and economic domains. Traditional numerical weather prediction uses increased compute resources to improve forecast accuracy, but cannot directly use historical weather data to improve the underlying model. We introduce a machine learning-based method called "GraphCast", which can be trained directly from reanalysis data. It predicts hundreds of weather variables, over 10 days at 0.25 degree resolution globally, in under one minute. We show that GraphCast significantly outperforms the most accurate operational deterministic systems on 90\% of 1380 verification targets, and its forecasts support better severe event prediction, including tropical cyclones, atmospheric rivers, and extreme temperatures. GraphCast is a key advance in accurate and efficient weather forecasting, and helps realize the promise of machine learning for modeling complex dynamical systems.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Lam, Remi and Sanchez-Gonzalez, Alvaro and Willson, Matthew and Wirnsberger, Peter and Fortunato, Meire and Alet, Ferran and Ravuri, Suman and Ewalds, Timo and Eaton-Rosen, Zach and Hu, Weihua and Merose, Alexander and Hoyer, Stephan and Holland, George and Vinyals, Oriol and Stott, Jacklynn and Pritzel, Alexander and Mohamed, Shakir and Battaglia, Peter},
	month = dec,
	year = {2022},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/4B66MXYX/Lam et al. - 2022 - GraphCast Learning skillful medium-range global w.pdf:application/pdf},
}

@misc{karras_guiding_2024,
	title = {Guiding a {Diffusion} {Model} with a {Bad} {Version} of {Itself}},
	url = {http://arxiv.org/abs/2406.02507},
	doi = {10.48550/arXiv.2406.02507},
	abstract = {The primary axes of interest in image-generating diffusion models are image quality, the amount of variation in the results, and how well the results align with a given condition, e.g., a class label or a text prompt. The popular classifier-free guidance approach uses an unconditional model to guide a conditional model, leading to simultaneously better prompt alignment and higher-quality images at the cost of reduced variation. These effects seem inherently entangled, and thus hard to control. We make the surprising observation that it is possible to obtain disentangled control over image quality without compromising the amount of variation by guiding generation using a smaller, less-trained version of the model itself rather than an unconditional model. This leads to significant improvements in ImageNet generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using publicly available networks. Furthermore, the method is also applicable to unconditional diffusion models, drastically improving their quality.},
	urldate = {2024-09-05},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Kynkäänniemi, Tuomas and Lehtinen, Jaakko and Aila, Timo and Laine, Samuli},
	month = jun,
	year = {2024},
	note = {arXiv:2406.02507 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FRGGD8X5/Karras et al. - 2024 - Guiding a Diffusion Model with a Bad Version of It.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/T8WWF373/2406.html:text/html},
}

@misc{mishra_image_2024,
	title = {Image {Synthesis} with {Graph} {Conditioning}: {CLIP}-{Guided} {Diffusion} {Models} for {Scene} {Graphs}},
	shorttitle = {Image {Synthesis} with {Graph} {Conditioning}},
	url = {http://arxiv.org/abs/2401.14111},
	doi = {10.48550/arXiv.2401.14111},
	abstract = {Advancements in generative models have sparked significant interest in generating images while adhering to specific structural guidelines. Scene graph to image generation is one such task of generating images which are consistent with the given scene graph. However, the complexity of visual scenes poses a challenge in accurately aligning objects based on specified relations within the scene graph. Existing methods approach this task by first predicting a scene layout and generating images from these layouts using adversarial training. In this work, we introduce a novel approach to generate images from scene graphs which eliminates the need of predicting intermediate layouts. We leverage pre-trained text-to-image diffusion models and CLIP guidance to translate graph knowledge into images. Towards this, we first pre-train our graph encoder to align graph features with CLIP features of corresponding images using a GAN based training. Further, we fuse the graph features with CLIP embedding of object labels present in the given scene graph to create a graph consistent CLIP guided conditioning signal. In the conditioning input, object embeddings provide coarse structure of the image and graph features provide structural alignment based on relationships among objects. Finally, we fine tune a pre-trained diffusion model with the graph consistent conditioning signal with reconstruction and CLIP alignment loss. Elaborate experiments reveal that our method outperforms existing methods on standard benchmarks of COCO-stuff and Visual Genome dataset.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Mishra, Rameshwar and Subramanyam, A. V.},
	month = jan,
	year = {2024},
	note = {arXiv:2401.14111 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/N2KQJ33M/Mishra and Subramanyam - 2024 - Image Synthesis with Graph Conditioning CLIP-Guid.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/8DDAE7ZS/2401.html:text/html},
}

@misc{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2205.11487},
	doi = {10.48550/arXiv.2205.11487},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = may,
	year = {2022},
	note = {arXiv:2205.11487 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/VYD22Q4N/Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HFGA2FD3/2205.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/X2PGPYUH/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/BAM228DE/2006.html:text/html},
}

@misc{nichol_improved_2021,
	title = {Improved {Denoising} {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2102.09672},
	doi = {10.48550/arXiv.2102.09672},
	abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code at https://github.com/openai/improved-diffusion},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Nichol, Alex and Dhariwal, Prafulla},
	month = feb,
	year = {2021},
	note = {arXiv:2102.09672 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/P6UXZ9DK/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CIYM9I6Q/2102.html:text/html},
}

@article{luo_antigen-specific_2022,
	title = {Antigen-{Specific} {Antibody} {Design} and {Optimization} with {Diffusion}-{Based} {Generative} {Models} for {Protein} {Structures}},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/3fa7d76a0dc1179f1e98d1bc62403756-Abstract-Conference.html},
	language = {en},
	urldate = {2024-04-07},
	journal = {Advances in Neural Information Processing Systems},
	author = {Luo, Shitong and Su, Yufeng and Peng, Xingang and Wang, Sheng and Peng, Jian and Ma, Jianzhu},
	month = dec,
	year = {2022},
	pages = {9754--9767},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/WPYL8WZZ/Luo et al. - 2022 - Antigen-Specific Antibody Design and Optimization .pdf:application/pdf},
}

@misc{saharia_palette_2022,
	title = {Palette: {Image}-to-{Image} {Diffusion} {Models}},
	shorttitle = {Palette},
	url = {http://arxiv.org/abs/2111.05826},
	doi = {10.48550/arXiv.2111.05826},
	abstract = {This paper develops a unified framework for image-to-image translation based on conditional diffusion models and evaluates this framework on four challenging image-to-image translation tasks, namely colorization, inpainting, uncropping, and JPEG restoration. Our simple implementation of image-to-image diffusion models outperforms strong GAN and regression baselines on all tasks, without task-specific hyper-parameter tuning, architecture customization, or any auxiliary loss or sophisticated new techniques needed. We uncover the impact of an L2 vs. L1 loss in the denoising diffusion objective on sample diversity, and demonstrate the importance of self-attention in the neural architecture through empirical studies. Importantly, we advocate a unified evaluation protocol based on ImageNet, with human evaluation and sample quality scores (FID, Inception Score, Classification Accuracy of a pre-trained ResNet-50, and Perceptual Distance against original images). We expect this standardized evaluation protocol to play a role in advancing image-to-image translation research. Finally, we show that a generalist, multi-task diffusion model performs as well or better than task-specific specialist counterparts. Check out https://diffusion-palette.github.io for an overview of the results.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Chang, Huiwen and Lee, Chris A. and Ho, Jonathan and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = may,
	year = {2022},
	note = {arXiv:2111.05826 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/ANSM9N7I/Saharia et al. - 2022 - Palette Image-to-Image Diffusion Models.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/V3DZX29D/2111.html:text/html},
}

@misc{isola_image--image_2018,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	doi = {10.48550/arXiv.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv:1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, read},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/KIY3X6AE/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HMCJM4WH/1611.html:text/html},
}

@misc{radford_learning_2021,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	url = {http://arxiv.org/abs/2103.00020},
	doi = {10.48550/arXiv.2103.00020},
	abstract = {State-of-the-art computer vision systems are trained to predict a fixed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study the performance of this approach by benchmarking on over 30 different existing computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of fine-grained object classification. The model transfers non-trivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset specific training. For instance, we match the accuracy of the original ResNet-50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	month = feb,
	year = {2021},
	note = {arXiv:2103.00020 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/PYHVA52W/Radford et al. - 2021 - Learning Transferable Visual Models From Natural L.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LMT7GZXM/2103.html:text/html},
}

@misc{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2021 (Oral)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FMFAQUZ9/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/KCR276EC/2011.html:text/html},
}

@misc{xiao_tackling_2022,
	title = {Tackling the {Generative} {Learning} {Trilemma} with {Denoising} {Diffusion} {GANs}},
	url = {http://arxiv.org/abs/2112.07804},
	doi = {10.48550/arXiv.2112.07804},
	abstract = {A wide variety of deep generative models has been developed in the past decade. Yet, these models often struggle with simultaneously addressing three key requirements including: high sample quality, mode coverage, and fast sampling. We call the challenge imposed by these requirements the generative learning trilemma, as the existing models often trade some of them for others. Particularly, denoising diffusion models have shown impressive sample quality and diversity, but their expensive sampling does not yet allow them to be applied in many real-world applications. In this paper, we argue that slow sampling in these models is fundamentally attributed to the Gaussian assumption in the denoising step which is justified only for small step sizes. To enable denoising with large steps, and hence, to reduce the total number of denoising steps, we propose to model the denoising distribution using a complex multimodal distribution. We introduce denoising diffusion generative adversarial networks (denoising diffusion GANs) that model each denoising step using a multimodal conditional GAN. Through extensive evaluations, we show that denoising diffusion GANs obtain sample quality and diversity competitive with original diffusion models while being 2000\${\textbackslash}times\$ faster on the CIFAR-10 dataset. Compared to traditional GANs, our model exhibits better mode coverage and sample diversity. To the best of our knowledge, denoising diffusion GAN is the first model that reduces sampling cost in diffusion models to an extent that allows them to be applied to real-world applications inexpensively. Project page and code can be found at https://nvlabs.github.io/denoising-diffusion-gan},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Xiao, Zhisheng and Kreis, Karsten and Vahdat, Arash},
	month = apr,
	year = {2022},
	note = {arXiv:2112.07804 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2022 (Spotlight)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/EDG5TVTX/Xiao et al. - 2022 - Tackling the Generative Learning Trilemma with Den.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/L5GL793J/2112.html:text/html},
}

@misc{esser_taming_2021,
	title = {Taming {Transformers} for {High}-{Resolution} {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2012.09841},
	doi = {10.48550/arXiv.2012.09841},
	abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a context-rich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semantically-guided synthesis of megapixel images with transformers and obtain the state of the art among autoregressive models on class-conditional ImageNet. Code and pretrained models can be found at https://github.com/CompVis/taming-transformers .},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Esser, Patrick and Rombach, Robin and Ommer, Björn},
	month = jun,
	year = {2021},
	note = {arXiv:2012.09841 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Changelog can be found in the supplementary},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/AB7W4HNH/Esser et al. - 2021 - Taming Transformers for High-Resolution Image Synt.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/25SFCB45/2012.html:text/html},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, read},
	annote = {Comment: Added compute requirements, ImageNet 256\${\textbackslash}times\$256 upsampling FID and samples, DDIM guided sampler, fixed typos},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/YRQ8P9WD/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6XAVLAR9/2105.html:text/html},
}

@misc{didi_framework_2024,
	title = {A framework for conditional diffusion modelling with applications in motif scaffolding for protein design},
	url = {http://arxiv.org/abs/2312.09236},
	doi = {10.48550/arXiv.2312.09236},
	abstract = {Many protein design applications, such as binder or enzyme design, require scaffolding a structural motif with high precision. Generative modelling paradigms based on denoising diffusion processes emerged as a leading candidate to address this motif scaffolding problem and have shown early experimental success in some cases. In the diffusion paradigm, motif scaffolding is treated as a conditional generation task, and several conditional generation protocols were proposed or imported from the Computer Vision literature. However, most of these protocols are motivated heuristically, e.g. via analogies to Langevin dynamics, and lack a unifying framework, obscuring connections between the different approaches. In this work, we unify conditional training and conditional sampling procedures under one common framework based on the mathematically well-understood Doob's h-transform. This new perspective allows us to draw connections between existing methods and propose a new variation on existing conditional training protocols. We illustrate the effectiveness of this new protocol in both, image outpainting and motif scaffolding and find that it outperforms standard methods.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Didi, Kieran and Vargas, Francisco and Mathis, Simon V. and Dutordoir, Vincent and Mathieu, Emile and Komorowska, Urszula J. and Lio, Pietro},
	month = mar,
	year = {2024},
	note = {arXiv:2312.09236 [cs, q-bio]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Biomolecules},
	annote = {Comment: 9 pages},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/5HGUS4KZ/Didi et al. - 2024 - A framework for conditional diffusion modelling wi.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/XX67FVWX/2312.html:text/html},
}

@misc{wang_visnet_2023,
	title = {{ViSNet}: an equivariant geometry-enhanced graph neural network with vector-scalar interactive message passing for molecules},
	shorttitle = {{ViSNet}},
	url = {http://arxiv.org/abs/2210.16518},
	doi = {10.48550/arXiv.2210.16518},
	abstract = {Geometric deep learning has been revolutionizing the molecular modeling field. Despite the state-of-the-art neural network models are approaching ab initio accuracy for molecular property prediction, their applications, such as drug discovery and molecular dynamics (MD) simulation, have been hindered by insufficient utilization of geometric information and high computational costs. Here we propose an equivariant geometry-enhanced graph neural network called ViSNet, which elegantly extracts geometric features and efficiently models molecular structures with low computational costs. Our proposed ViSNet outperforms state-of-the-art approaches on multiple MD benchmarks, including MD17, revised MD17 and MD22, and achieves excellent chemical property prediction on QM9 and Molecule3D datasets. Additionally, ViSNet achieved the top winners of PCQM4Mv2 track in the OGB-LCS@NeurIPS2022 competition. Furthermore, through a series of simulations and case studies, ViSNet can efficiently explore the conformational space and provide reasonable interpretability to map geometric representations to molecular structures.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Wang, Yusong and Li, Shaoning and He, Xinheng and Li, Mingyu and Wang, Zun and Zheng, Nanning and Shao, Bin and Liu, Tie-Yan and Wang, Tong},
	month = aug,
	year = {2023},
	note = {arXiv:2210.16518 [physics, physics:quant-ph, q-bio]},
	keywords = {Quantitative Biology - Biomolecules, Physics - Chemical Physics, Quantum Physics},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/DBR6HWJZ/Wang et al. - 2023 - ViSNet an equivariant geometry-enhanced graph neu.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/R2QEDSR3/2210.html:text/html},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	doi = {10.48550/arXiv.2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, read},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/YPIYUCG8/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/C896URXX/2010.html:text/html},
}

@misc{ronneberger_u-net_2015,
	title = {U-{Net}: {Convolutional} {Networks} for {Biomedical} {Image} {Segmentation}},
	shorttitle = {U-{Net}},
	url = {http://arxiv.org/abs/1505.04597},
	doi = {10.48550/arXiv.1505.04597},
	abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
	month = may,
	year = {2015},
	note = {arXiv:1505.04597 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, read},
	annote = {Comment: conditionally accepted at MICCAI 2015},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/8SSNYG3D/Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/3GJ46FHX/1505.html:text/html},
}

@misc{velickovic_graph_2018,
	title = {Graph {Attention} {Networks}},
	url = {http://arxiv.org/abs/1710.10903},
	doi = {10.48550/arXiv.1710.10903},
	abstract = {We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods' features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of costly matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training).},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Veličković, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Liò, Pietro and Bengio, Yoshua},
	month = feb,
	year = {2018},
	note = {arXiv:1710.10903 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Social and Information Networks},
	annote = {Comment: To appear at ICLR 2018. 12 pages, 2 figures},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/UQY3RKDG/Veličković et al. - 2018 - Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/NPD2V9LG/1710.html:text/html},
}

@misc{graves_bayesian_2024,
	title = {Bayesian {Flow} {Networks}},
	url = {http://arxiv.org/abs/2308.07037},
	doi = {10.48550/arXiv.2308.07037},
	abstract = {This paper introduces Bayesian Flow Networks (BFNs), a new class of generative model in which the parameters of a set of independent distributions are modified with Bayesian inference in the light of noisy data samples, then passed as input to a neural network that outputs a second, interdependent distribution. Starting from a simple prior and iteratively updating the two distributions yields a generative procedure similar to the reverse process of diffusion models; however it is conceptually simpler in that no forward process is required. Discrete and continuous-time loss functions are derived for continuous, discretised and discrete data, along with sample generation procedures. Notably, the network inputs for discrete data lie on the probability simplex, and are therefore natively differentiable, paving the way for gradient-based sample guidance and few-step generation in discrete domains such as language modelling. The loss function directly optimises data compression and places no restrictions on the network architecture. In our experiments BFNs achieve competitive log-likelihoods for image modelling on dynamically binarized MNIST and CIFAR-10, and outperform all known discrete diffusion models on the text8 character-level language modelling task.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Graves, Alex and Srivastava, Rupesh Kumar and Atkinson, Timothy and Gomez, Faustino},
	month = feb,
	year = {2024},
	note = {arXiv:2308.07037 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/HZID29CN/Graves et al. - 2024 - Bayesian Flow Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/ADI2FCD4/2308.html:text/html},
}

@misc{vaswani_attention_2023,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	doi = {10.48550/arXiv.1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = aug,
	year = {2023},
	note = {arXiv:1706.03762 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, read},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/W7CL65MM/Vaswani et al. - 2023 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/5LVWDWX4/1706.html:text/html},
}

@misc{han_vision_2022,
	title = {Vision {GNN}: {An} {Image} is {Worth} {Graph} of {Nodes}},
	shorttitle = {Vision {GNN}},
	url = {http://arxiv.org/abs/2206.00272},
	doi = {10.48550/arXiv.2206.00272},
	abstract = {Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research. The PyTorch code is available at https://github.com/huawei-noah/Efficient-AI-Backbones and the MindSpore code is available at https://gitee.com/mindspore/models.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
	month = nov,
	year = {2022},
	note = {arXiv:2206.00272 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: NeurIPS 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/MENJE2US/Han et al. - 2022 - Vision GNN An Image is Worth Graph of Nodes.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/DZMV766N/2206.html:text/html},
}

@misc{gokay_graph2pix_2021,
	title = {{Graph2Pix}: {A} {Graph}-{Based} {Image} to {Image} {Translation} {Framework}},
	shorttitle = {{Graph2Pix}},
	url = {http://arxiv.org/abs/2108.09752},
	doi = {10.48550/arXiv.2108.09752},
	abstract = {In this paper, we propose a graph-based image-to-image translation framework for generating images. We use rich data collected from the popular creativity platform Artbreeder (http://artbreeder.com), where users interpolate multiple GAN-generated images to create artworks. This unique approach of creating new images leads to a tree-like structure where one can track historical data about the creation of a particular image. Inspired by this structure, we propose a novel graph-to-image translation model called Graph2Pix, which takes a graph and corresponding images as input and generates a single image as output. Our experiments show that Graph2Pix is able to outperform several image-to-image translation frameworks on benchmark metrics, including LPIPS (with a 25\% improvement) and human perception studies (n=60), where users preferred the images generated by our method 81.5\% of the time. Our source code and dataset are publicly available at https://github.com/catlab-team/graph2pix.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Gokay, Dilara and Simsar, Enis and Atici, Efehan and Ahmetoglu, Alper and Yuksel, Atif Emre and Yanardag, Pinar},
	month = aug,
	year = {2021},
	note = {arXiv:2108.09752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FJWTDJ2D/Gokay et al. - 2021 - Graph2Pix A Graph-Based Image to Image Translatio.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/GSLEMF7B/2108.html:text/html},
}

@misc{kim_consistency_2024,
	title = {Consistency {Trajectory} {Models}: {Learning} {Probability} {Flow} {ODE} {Trajectory} of {Diffusion}},
	shorttitle = {Consistency {Trajectory} {Models}},
	url = {http://arxiv.org/abs/2310.02279},
	doi = {10.48550/arXiv.2310.02279},
	abstract = {Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64x64 resolution (FID 1.92). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE solution trajectories. It consistently improves sample quality as computational budgets increase, avoiding the degradation seen in CM. Furthermore, unlike CM, CTM's access to the score function can streamline the adoption of established controllable/conditional generation methods from the diffusion community. This access also enables the computation of likelihood. The code is available at https://github.com/sony/ctm.},
	urldate = {2024-05-06},
	publisher = {arXiv},
	author = {Kim, Dongjun and Lai, Chieh-Hsin and Liao, Wei-Hsiang and Murata, Naoki and Takida, Yuhta and Uesaka, Toshimitsu and He, Yutong and Mitsufuji, Yuki and Ermon, Stefano},
	month = mar,
	year = {2024},
	note = {arXiv:2310.02279 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: International Conference on Learning Representations},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FWHFDPJA/Kim et al. - 2024 - Consistency Trajectory Models Learning Probabilit.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/WQ4JAZVB/2310.html:text/html},
}

@misc{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/2104.07636},
	doi = {10.48550/arXiv.2104.07636},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = jun,
	year = {2021},
	note = {arXiv:2104.07636 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QVE9Z6PV/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/7HRQJXBA/2104.html:text/html},
}

@misc{peebles_scalable_2023,
	title = {Scalable {Diffusion} {Models} with {Transformers}},
	url = {http://arxiv.org/abs/2212.09748},
	doi = {10.48550/arXiv.2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Peebles, William and Xie, Saining},
	month = mar,
	year = {2023},
	note = {arXiv:2212.09748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, read},
	annote = {Comment: Code, project page and videos available at https://www.wpeebles.com/DiT},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/3E2GRMB8/Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/GPUNW48J/2212.html:text/html},
}

@article{abramson_accurate_2024,
	title = {Accurate structure prediction of biomolecular interactions with {AlphaFold} 3},
	copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07487-w},
	doi = {10.1038/s41586-024-07487-w},
	abstract = {The introduction of AlphaFold 21 has spurred a revolution in modelling the structure of proteins and their interactions, enabling a huge range of applications in protein modelling and design2–6. In this paper, we describe our AlphaFold 3 model with a substantially updated diffusion-based architecture, which is capable of joint structure prediction of complexes including proteins, nucleic acids, small molecules, ions, and modified residues. The new AlphaFold model demonstrates significantly improved accuracy over many previous specialised tools: far greater accuracy on protein-ligand interactions than state of the art docking tools, much higher accuracy on protein-nucleic acid interactions than nucleic-acid-specific predictors, and significantly higher antibody-antigen prediction accuracy than AlphaFold-Multimer v2.37,8. Together these results show that high accuracy modelling across biomolecular space is possible within a single unified deep learning framework.},
	language = {en},
	urldate = {2024-05-09},
	journal = {Nature},
	author = {Abramson, Josh and Adler, Jonas and Dunger, Jack and Evans, Richard and Green, Tim and Pritzel, Alexander and Ronneberger, Olaf and Willmore, Lindsay and Ballard, Andrew J. and Bambrick, Joshua and Bodenstein, Sebastian W. and Evans, David A. and Hung, Chia-Chun and O’Neill, Michael and Reiman, David and Tunyasuvunakool, Kathryn and Wu, Zachary and Žemgulytė, Akvilė and Arvaniti, Eirini and Beattie, Charles and Bertolli, Ottavia and Bridgland, Alex and Cherepanov, Alexey and Congreve, Miles and Cowen-Rivers, Alexander I. and Cowie, Andrew and Figurnov, Michael and Fuchs, Fabian B. and Gladman, Hannah and Jain, Rishub and Khan, Yousuf A. and Low, Caroline M. R. and Perlin, Kuba and Potapenko, Anna and Savy, Pascal and Singh, Sukhdeep and Stecula, Adrian and Thillaisundaram, Ashok and Tong, Catherine and Yakneen, Sergei and Zhong, Ellen D. and Zielinski, Michal and Žídek, Augustin and Bapst, Victor and Kohli, Pushmeet and Jaderberg, Max and Hassabis, Demis and Jumper, John M.},
	month = may,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Drug discovery, Machine learning, Protein structure predictions, Structural biology},
	pages = {1--3},
}

@misc{qi_pointnet_2017,
	title = {{PointNet}: {Deep} {Learning} on {Point} {Sets} for {3D} {Classification} and {Segmentation}},
	shorttitle = {{PointNet}},
	url = {http://arxiv.org/abs/1612.00593},
	doi = {10.48550/arXiv.1612.00593},
	abstract = {Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds and well respects the permutation invariance of points in the input. Our network, named PointNet, provides a unified architecture for applications ranging from object classification, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efficient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.},
	urldate = {2024-05-10},
	publisher = {arXiv},
	author = {Qi, Charles R. and Su, Hao and Mo, Kaichun and Guibas, Leonidas J.},
	month = apr,
	year = {2017},
	note = {arXiv:1612.00593 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/S89VALTW/Qi et al. - 2017 - PointNet Deep Learning on Point Sets for 3D Class.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/F5Z58NV4/1612.html:text/html},
}

@misc{darcet_vision_2024,
	title = {Vision {Transformers} {Need} {Registers}},
	url = {http://arxiv.org/abs/2309.16588},
	abstract = {Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.},
	language = {en},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Darcet, Timothée and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
	month = apr,
	year = {2024},
	note = {arXiv:2309.16588 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Darcet et al. - 2024 - Vision Transformers Need Registers.pdf:/Users/rupertmenneer/Zotero/storage/3FPH26DX/Darcet et al. - 2024 - Vision Transformers Need Registers.pdf:application/pdf},
}

@article{brion_generalisable_2022,
	title = {Generalisable {3D} printing error detection and correction via multi-head neural networks},
	volume = {13},
	copyright = {2022 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-022-31985-y},
	doi = {10.1038/s41467-022-31985-y},
	abstract = {Material extrusion is the most widespread additive manufacturing method but its application in end-use products is limited by vulnerability to errors. Humans can detect errors but cannot provide continuous monitoring or real-time correction. Existing automated approaches are not generalisable across different parts, materials, and printing systems. We train a multi-head neural network using images automatically labelled by deviation from optimal printing parameters. The automation of data acquisition and labelling allows the generation of a large and varied extrusion 3D printing dataset, containing 1.2 million images from 192 different parts labelled with printing parameters. The thus trained neural network, alongside a control loop, enables real-time detection and rapid correction of diverse errors that is effective across many different 2D and 3D geometries, materials, printers, toolpaths, and even extrusion methods. We additionally create visualisations of the network’s predictions to shed light on how it makes decisions.},
	language = {en},
	number = {1},
	urldate = {2024-05-11},
	journal = {Nature Communications},
	author = {Brion, Douglas A. J. and Pattinson, Sebastian W.},
	month = aug,
	year = {2022},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computational science, Electrical and electronic engineering, Mechanical engineering, Techniques and instrumentation},
	pages = {4654},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/U6MEL6K4/Brion and Pattinson - 2022 - Generalisable 3D printing error detection and corr.pdf:application/pdf},
}

@article{brion_quantitative_2022,
	title = {Quantitative and {Real}-{Time} {Control} of {3D} {Printing} {Material} {Flow} {Through} {Deep} {Learning}},
	volume = {4},
	copyright = {© 2022 The Authors. Advanced Intelligent Systems published by Wiley-VCH GmbH},
	issn = {2640-4567},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.202200153},
	doi = {10.1002/aisy.202200153},
	abstract = {3D printing could revolutionize manufacturing through local and on-demand production while enabling uniquely complex and custom products. However, 3D printing's propensity for production errors prevents autonomous operation and the quality assurance necessary to realize this vision. Human operators cannot continuously monitor or correct errors in real time, while automated approaches predominantly only detect errors. New methodologies correct parameters either offline or with slow response times and poor prediction granularity, limiting their utility. A commonly available 3D printing process metadata is harnessed, alongside the video of the printing process, to build a unique image dataset. Regression models are trained to precisely predict how printing material flow should be altered to correct errors and this should be used to build a fast control loop capable of 3D printing parameter discovery and few-shot correction. Demonstrations show that the system can learn optimal parameters for unseen complex materials, and achieve rapid error correction on new parts. Similar metadata exists in many manufacturing processes and this approach could enable the adoption of fast data-driven control systems more widely in manufacturing.},
	language = {en},
	number = {11},
	urldate = {2024-05-11},
	journal = {Advanced Intelligent Systems},
	author = {Brion, Douglas A. J. and Pattinson, Sebastian W.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.202200153},
	keywords = {3D printing, additive manufacturing, closed-loop control, computer vision, error detection and correction, machine learning},
	pages = {2200153},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/3QCM6IY8/Brion and Pattinson - 2022 - Quantitative and Real-Time Control of 3D Printing .pdf:application/pdf;Snapshot:/Users/rupertmenneer/Zotero/storage/3UIJHT7I/aisy.html:text/html},
}

@article{brion_automated_2022,
	title = {Automated recognition and correction of warp deformation in extrusion additive manufacturing},
	volume = {56},
	issn = {2214-8604},
	url = {https://www.sciencedirect.com/science/article/pii/S2214860422002378},
	doi = {10.1016/j.addma.2022.102838},
	abstract = {Warp deformation is a common error encountered in additive manufacturing. It is typically caused by residual internal stresses in the manufactured part that arise as material cools. These errors are challenging to prevent or correct as they build over time and thus are only visible long after the actions that caused them. As a result, existing work in extrusion additive manufacturing has attempted warp detection but not correction or prevention. We report a hybrid approach combining deep learning, computer vision, and expert heuristics to correct or prevent warp. We train a deep convolutional neural network using diverse labelled images to recognise warp in real-time. We compute five metrics from detection candidates to predict the severity of warp deformation and proportionately update print settings. This enables the first demonstration of automated warp detection and correction both during printing and for future prints.},
	urldate = {2024-05-11},
	journal = {Additive Manufacturing},
	author = {Brion, Douglas A. J. and Shen, Matthew and Pattinson, Sebastian W.},
	month = aug,
	year = {2022},
	keywords = {Machine learning, Computer vision, Error detection and correction, Extrusion 3D printing, Warp deformation},
	pages = {102838},
	file = {Full Text:/Users/rupertmenneer/Zotero/storage/H9RXKIIA/Brion et al. - 2022 - Automated recognition and correction of warp defor.pdf:application/pdf;ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/7XCIXUTE/S2214860422002378.html:text/html},
}

@misc{larsen_-situ_2023,
	title = {In-situ {Anomaly} {Detection} in {Additive} {Manufacturing} with {Graph} {Neural} {Networks}},
	url = {http://arxiv.org/abs/2305.02695},
	doi = {10.48550/arXiv.2305.02695},
	abstract = {Transforming a design into a high-quality product is a challenge in metal additive manufacturing due to rare events which can cause defects to form. Detecting these events in-situ could, however, reduce inspection costs, enable corrective action, and is the first step towards a future of tailored material properties. In this study a model is trained on laser input information to predict nominal laser melting conditions. An anomaly score is then calculated by taking the difference between the predictions and new observations. The model is evaluated on a dataset with known defects achieving an F1 score of 0.821. This study shows that anomaly detection methods are an important tool in developing robust defect detection methods.},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Larsen, Sebastian and Hooper, Paul A.},
	month = may,
	year = {2023},
	note = {arXiv:2305.02695 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: 5 pages, 3 figures, published in ICLR 2023 workshop on machine learning for materials (ML4Materials)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/CM5NXMN2/Larsen and Hooper - 2023 - In-situ Anomaly Detection in Additive Manufacturin.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/Q3VP9FAK/2305.html:text/html},
}

@misc{he_masked_2021,
	title = {Masked {Autoencoders} {Are} {Scalable} {Vision} {Learners}},
	url = {http://arxiv.org/abs/2111.06377},
	doi = {10.48550/arXiv.2111.06377},
	abstract = {This paper shows that masked autoencoders (MAE) are scalable self-supervised learners for computer vision. Our MAE approach is simple: we mask random patches of the input image and reconstruct the missing pixels. It is based on two core designs. First, we develop an asymmetric encoder-decoder architecture, with an encoder that operates only on the visible subset of patches (without mask tokens), along with a lightweight decoder that reconstructs the original image from the latent representation and mask tokens. Second, we find that masking a high proportion of the input image, e.g., 75\%, yields a nontrivial and meaningful self-supervisory task. Coupling these two designs enables us to train large models efficiently and effectively: we accelerate training (by 3x or more) and improve accuracy. Our scalable approach allows for learning high-capacity models that generalize well: e.g., a vanilla ViT-Huge model achieves the best accuracy (87.8\%) among methods that use only ImageNet-1K data. Transfer performance in downstream tasks outperforms supervised pre-training and shows promising scaling behavior.},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Dollár, Piotr and Girshick, Ross},
	month = dec,
	year = {2021},
	note = {arXiv:2111.06377 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report. arXiv v2: add more transfer learning results; v3: add robustness evaluation},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/J8ZVTB6Q/He et al. - 2021 - Masked Autoencoders Are Scalable Vision Learners.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/ATRQ2SZZ/2111.html:text/html},
}

@misc{nitzan_lazy_2024,
	title = {Lazy {Diffusion} {Transformer} for {Interactive} {Image} {Editing}},
	url = {http://arxiv.org/abs/2404.12382},
	doi = {10.48550/arXiv.2404.12382},
	abstract = {We introduce a novel diffusion transformer, LazyDiffusion, that generates partial image updates efficiently. Our approach targets interactive image editing applications in which, starting from a blank canvas or an image, a user specifies a sequence of localized image modifications using binary masks and text prompts. Our generator operates in two phases. First, a context encoder processes the current canvas and user mask to produce a compact global context tailored to the region to generate. Second, conditioned on this context, a diffusion-based transformer decoder synthesizes the masked pixels in a "lazy" fashion, i.e., it only generates the masked region. This contrasts with previous works that either regenerate the full canvas, wasting time and computation, or confine processing to a tight rectangular crop around the mask, ignoring the global image context altogether. Our decoder's runtime scales with the mask size, which is typically small, while our encoder introduces negligible overhead. We demonstrate that our approach is competitive with state-of-the-art inpainting methods in terms of quality and fidelity while providing a 10x speedup for typical user interactions, where the editing mask represents 10\% of the image.},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Nitzan, Yotam and Wu, Zongze and Zhang, Richard and Shechtman, Eli and Cohen-Or, Daniel and Park, Taesung and Gharbi, Michaël},
	month = apr,
	year = {2024},
	note = {arXiv:2404.12382 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/WUPF2HQB/Nitzan et al. - 2024 - Lazy Diffusion Transformer for Interactive Image E.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/JMCUM5YD/2404.html:text/html},
}

@misc{rombach_high-resolution_2022,
	title = {High-{Resolution} {Image} {Synthesis} with {Latent} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2112.10752},
	doi = {10.48550/arXiv.2112.10752},
	abstract = {By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .},
	urldate = {2024-05-11},
	publisher = {arXiv},
	author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Björn},
	month = apr,
	year = {2022},
	note = {arXiv:2112.10752 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/L5S2FE6H/Rombach et al. - 2022 - High-Resolution Image Synthesis with Latent Diffus.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/BPY4ZVJS/2112.html:text/html},
}

@misc{heo_rotary_2024,
	title = {Rotary {Position} {Embedding} for {Vision} {Transformer}},
	url = {http://arxiv.org/abs/2403.13298},
	doi = {10.48550/arXiv.2403.13298},
	abstract = {Rotary Position Embedding (RoPE) performs remarkably on language models, especially for length extrapolation of Transformers. However, the impacts of RoPE on computer vision domains have been underexplored, even though RoPE appears capable of enhancing Vision Transformer (ViT) performance in a way similar to the language domain. This study provides a comprehensive analysis of RoPE when applied to ViTs, utilizing practical implementations of RoPE for 2D vision data. The analysis reveals that RoPE demonstrates impressive extrapolation performance, i.e., maintaining precision while increasing image resolution at inference. It eventually leads to performance improvement for ImageNet-1k, COCO detection, and ADE-20k segmentation. We believe this study provides thorough guidelines to apply RoPE into ViT, promising improved backbone performance with minimal extra computational overhead. Our code and pre-trained models are available at https://github.com/naver-ai/rope-vit},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Heo, Byeongho and Park, Song and Han, Dongyoon and Yun, Sangdoo},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13298 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 20 pages, 5 figures},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/GKQN2DNI/Heo et al. - 2024 - Rotary Position Embedding for Vision Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/C7NWEL9M/2403.html:text/html},
}

@misc{liu_swin_2021,
	title = {Swin {Transformer}: {Hierarchical} {Vision} {Transformer} using {Shifted} {Windows}},
	shorttitle = {Swin {Transformer}},
	url = {http://arxiv.org/abs/2103.14030},
	doi = {10.48550/arXiv.2103.14030},
	abstract = {This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with {\textbackslash}textbf\{S\}hifted {\textbackslash}textbf\{win\}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at{\textasciitilde}{\textbackslash}url\{https://github.com/microsoft/Swin-Transformer\}.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
	month = aug,
	year = {2021},
	note = {arXiv:2103.14030 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/L33IP3MP/Liu et al. - 2021 - Swin Transformer Hierarchical Vision Transformer .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/BSGD4TKB/2103.html:text/html},
}

@misc{li_swinv2-imagen_2022,
	title = {Swinv2-{Imagen}: {Hierarchical} {Vision} {Transformer} {Diffusion} {Models} for {Text}-to-{Image} {Generation}},
	shorttitle = {Swinv2-{Imagen}},
	url = {http://arxiv.org/abs/2210.09549},
	doi = {10.48550/arXiv.2210.09549},
	abstract = {Recently, diffusion models have been proven to perform remarkably well in text-to-image synthesis tasks in a number of studies, immediately presenting new study opportunities for image generation. Google's Imagen follows this research trend and outperforms DALLE2 as the best model for text-to-image generation. However, Imagen merely uses a T5 language model for text processing, which cannot ensure learning the semantic information of the text. Furthermore, the Efficient UNet leveraged by Imagen is not the best choice in image processing. To address these issues, we propose the Swinv2-Imagen, a novel text-to-image diffusion model based on a Hierarchical Visual Transformer and a Scene Graph incorporating a semantic layout. In the proposed model, the feature vectors of entities and relationships are extracted and involved in the diffusion model, effectively improving the quality of generated images. On top of that, we also introduce a Swin-Transformer-based UNet architecture, called Swinv2-Unet, which can address the problems stemming from the CNN convolution operations. Extensive experiments are conducted to evaluate the performance of the proposed model by using three real-world datasets, i.e., MSCOCO, CUB and MM-CelebA-HQ. The experimental results show that the proposed Swinv2-Imagen model outperforms several popular state-of-the-art methods.},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Li, Ruijun and Li, Weihua and Yang, Yi and Wei, Hanyu and Jiang, Jianhua and Bai, Quan},
	month = oct,
	year = {2022},
	note = {arXiv:2210.09549 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, 94A08, I.4.0},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/A7VYQPCW/Li et al. - 2022 - Swinv2-Imagen Hierarchical Vision Transformer Dif.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/YYI6PYD9/2210.html:text/html},
}

@misc{podell_sdxl_2023,
	title = {{SDXL}: {Improving} {Latent} {Diffusion} {Models} for {High}-{Resolution} {Image} {Synthesis}},
	shorttitle = {{SDXL}},
	url = {http://arxiv.org/abs/2307.01952},
	doi = {10.48550/arXiv.2307.01952},
	abstract = {We present SDXL, a latent diffusion model for text-to-image synthesis. Compared to previous versions of Stable Diffusion, SDXL leverages a three times larger UNet backbone: The increase of model parameters is mainly due to more attention blocks and a larger cross-attention context as SDXL uses a second text encoder. We design multiple novel conditioning schemes and train SDXL on multiple aspect ratios. We also introduce a refinement model which is used to improve the visual fidelity of samples generated by SDXL using a post-hoc image-to-image technique. We demonstrate that SDXL shows drastically improved performance compared the previous versions of Stable Diffusion and achieves results competitive with those of black-box state-of-the-art image generators. In the spirit of promoting open research and fostering transparency in large model training and evaluation, we provide access to code and model weights at https://github.com/Stability-AI/generative-models},
	urldate = {2024-05-13},
	publisher = {arXiv},
	author = {Podell, Dustin and English, Zion and Lacey, Kyle and Blattmann, Andreas and Dockhorn, Tim and Müller, Jonas and Penna, Joe and Rombach, Robin},
	month = jul,
	year = {2023},
	note = {arXiv:2307.01952 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FF39UTQ9/Podell et al. - 2023 - SDXL Improving Latent Diffusion Models for High-R.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/8F22JRL5/2307.html:text/html},
}

@misc{balaji_ediff-i_2023,
	title = {{eDiff}-{I}: {Text}-to-{Image} {Diffusion} {Models} with an {Ensemble} of {Expert} {Denoisers}},
	shorttitle = {{eDiff}-{I}},
	url = {http://arxiv.org/abs/2211.01324},
	doi = {10.48550/arXiv.2211.01324},
	abstract = {Large-scale diffusion-based generative models have led to breakthroughs in text-conditioned high-resolution image synthesis. Starting from random noise, such text-to-image diffusion models gradually synthesize images in an iterative fashion while conditioning on text prompts. We find that their synthesis behavior qualitatively changes throughout this process: Early in sampling, generation strongly relies on the text prompt to generate text-aligned content, while later, the text conditioning is almost entirely ignored. This suggests that sharing model parameters throughout the entire generation process may not be ideal. Therefore, in contrast to existing works, we propose to train an ensemble of text-to-image diffusion models specialized for different synthesis stages. To maintain training efficiency, we initially train a single model, which is then split into specialized models that are trained for the specific stages of the iterative generation process. Our ensemble of diffusion models, called eDiff-I, results in improved text alignment while maintaining the same inference computation cost and preserving high visual quality, outperforming previous large-scale text-to-image diffusion models on the standard benchmark. In addition, we train our model to exploit a variety of embeddings for conditioning, including the T5 text, CLIP text, and CLIP image embeddings. We show that these different embeddings lead to different behaviors. Notably, the CLIP image embedding allows an intuitive way of transferring the style of a reference image to the target text-to-image output. Lastly, we show a technique that enables eDiff-I's "paint-with-words" capability. A user can select the word in the input text and paint it in a canvas to control the output, which is very handy for crafting the desired image in mind. The project page is available at https://deepimagination.cc/eDiff-I/},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Balaji, Yogesh and Nah, Seungjun and Huang, Xun and Vahdat, Arash and Song, Jiaming and Zhang, Qinsheng and Kreis, Karsten and Aittala, Miika and Aila, Timo and Laine, Samuli and Catanzaro, Bryan and Karras, Tero and Liu, Ming-Yu},
	month = mar,
	year = {2023},
	note = {arXiv:2211.01324 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/88RPY47V/Balaji et al. - 2023 - eDiff-I Text-to-Image Diffusion Models with an En.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/TPL7BKBF/2211.html:text/html},
}

@misc{po_state_2023,
	title = {State of the {Art} on {Diffusion} {Models} for {Visual} {Computing}},
	url = {http://arxiv.org/abs/2310.07204},
	doi = {10.48550/arXiv.2310.07204},
	abstract = {The field of visual computing is rapidly advancing due to the emergence of generative artificial intelligence (AI), which unlocks unprecedented capabilities for the generation, editing, and reconstruction of images, videos, and 3D scenes. In these domains, diffusion models are the generative AI architecture of choice. Within the last year alone, the literature on diffusion-based tools and applications has seen exponential growth and relevant papers are published across the computer graphics, computer vision, and AI communities with new works appearing daily on arXiv. This rapid growth of the field makes it difficult to keep up with all recent developments. The goal of this state-of-the-art report (STAR) is to introduce the basic mathematical concepts of diffusion models, implementation details and design choices of the popular Stable Diffusion model, as well as overview important aspects of these generative AI tools, including personalization, conditioning, inversion, among others. Moreover, we give a comprehensive overview of the rapidly growing literature on diffusion-based generation and editing, categorized by the type of generated medium, including 2D images, videos, 3D objects, locomotion, and 4D scenes. Finally, we discuss available datasets, metrics, open challenges, and social implications. This STAR provides an intuitive starting point to explore this exciting topic for researchers, artists, and practitioners alike.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Po, Ryan and Yifan, Wang and Golyanik, Vladislav and Aberman, Kfir and Barron, Jonathan T. and Bermano, Amit H. and Chan, Eric Ryan and Dekel, Tali and Holynski, Aleksander and Kanazawa, Angjoo and Liu, C. Karen and Liu, Lingjie and Mildenhall, Ben and Nießner, Matthias and Ommer, Björn and Theobalt, Christian and Wonka, Peter and Wetzstein, Gordon},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07204 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/XWB39ADP/Po et al. - 2023 - State of the Art on Diffusion Models for Visual Co.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CCSEH2PD/2310.html:text/html},
}

@misc{chen_diffusiondet_2023,
	title = {{DiffusionDet}: {Diffusion} {Model} for {Object} {Detection}},
	shorttitle = {{DiffusionDet}},
	url = {http://arxiv.org/abs/2211.09788},
	doi = {10.48550/arXiv.2211.09788},
	abstract = {We propose DiffusionDet, a new framework that formulates object detection as a denoising diffusion process from noisy boxes to object boxes. During the training stage, object boxes diffuse from ground-truth boxes to random distribution, and the model learns to reverse this noising process. In inference, the model refines a set of randomly generated boxes to the output results in a progressive way. Our work possesses an appealing property of flexibility, which enables the dynamic number of boxes and iterative evaluation. The extensive experiments on the standard benchmarks show that DiffusionDet achieves favorable performance compared to previous well-established detectors. For example, DiffusionDet achieves 5.3 AP and 4.8 AP gains when evaluated with more boxes and iteration steps, under a zero-shot transfer setting from COCO to CrowdHuman. Our code is available at https://github.com/ShoufaChen/DiffusionDet.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Chen, Shoufa and Sun, Peize and Song, Yibing and Luo, Ping},
	month = aug,
	year = {2023},
	note = {arXiv:2211.09788 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ICCV2023 (Oral), Camera-ready},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/9SNQ3IWP/Chen et al. - 2023 - DiffusionDet Diffusion Model for Object Detection.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/4DCIWIYX/2211.html:text/html},
}

@misc{turner_denoising_2024,
	title = {Denoising {Diffusion} {Probabilistic} {Models} in {Six} {Simple} {Steps}},
	url = {http://arxiv.org/abs/2402.04384},
	doi = {10.48550/arXiv.2402.04384},
	abstract = {Denoising Diffusion Probabilistic Models (DDPMs) are a very popular class of deep generative model that have been successfully applied to a diverse range of problems including image and video generation, protein and material synthesis, weather forecasting, and neural surrogates of partial differential equations. Despite their ubiquity it is hard to find an introduction to DDPMs which is simple, comprehensive, clean and clear. The compact explanations necessary in research papers are not able to elucidate all of the different design steps taken to formulate the DDPM and the rationale of the steps that are presented is often omitted to save space. Moreover, the expositions are typically presented from the variational lower bound perspective which is unnecessary and arguably harmful as it obfuscates why the method is working and suggests generalisations that do not perform well in practice. On the other hand, perspectives that take the continuous time-limit are beautiful and general, but they have a high barrier-to-entry as they require background knowledge of stochastic differential equations and probability flow. In this note, we distill down the formulation of the DDPM into six simple steps each of which comes with a clear rationale. We assume that the reader is familiar with fundamental topics in machine learning including basic probabilistic modelling, Gaussian distributions, maximum likelihood estimation, and deep learning.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Turner, Richard E. and Diaconu, Cristiana-Diana and Markou, Stratis and Shysheya, Aliaksandra and Foong, Andrew Y. K. and Mlodozeniec, Bruno},
	month = feb,
	year = {2024},
	note = {arXiv:2402.04384 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/AWTP972G/Turner et al. - 2024 - Denoising Diffusion Probabilistic Models in Six Si.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/S7V9MG3A/2402.html:text/html},
}

@misc{karras_elucidating_2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2206.00364},
	doi = {10.48550/arXiv.2206.00364},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = oct,
	year = {2022},
	note = {arXiv:2206.00364 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NeurIPS 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/BP5TIQBN/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LSD3JSB8/2206.html:text/html},
}

@misc{zhang_adding_2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.05543},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({\textless}50k) and large ({\textgreater}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	month = nov,
	year = {2023},
	note = {arXiv:2302.05543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Multimedia},
	annote = {Comment: Codes and Supplementary Material: https://github.com/lllyasviel/ControlNet},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/HMXN35GU/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffus.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/MZ2ZAIEL/2302.html:text/html},
}

@misc{yun_graph_2020,
	title = {Graph {Transformer} {Networks}},
	url = {http://arxiv.org/abs/1911.06455},
	doi = {10.48550/arXiv.1911.06455},
	abstract = {Graph neural networks (GNNs) have been widely used in representation learning on graphs and achieved state-of-the-art performance in tasks such as node classification and link prediction. However, most existing GNNs are designed to learn node representations on the fixed and homogeneous graphs. The limitations especially become problematic when learning representations on a misspecified graph or a heterogeneous graph that consists of various types of nodes and edges. In this paper, we propose Graph Transformer Networks (GTNs) that are capable of generating new graph structures, which involve identifying useful connections between unconnected nodes on the original graph, while learning effective node representation on the new graphs in an end-to-end fashion. Graph Transformer layer, a core layer of GTNs, learns a soft selection of edge types and composite relations for generating useful multi-hop connections so-called meta-paths. Our experiments show that GTNs learn new graph structures, based on data and tasks without domain knowledge, and yield powerful node representation via convolution on the new graphs. Without domain-specific graph preprocessing, GTNs achieved the best performance in all three benchmark node classification tasks against the state-of-the-art methods that require pre-defined meta-paths from domain knowledge.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Yun, Seongjun and Jeong, Minbyul and Kim, Raehyun and Kang, Jaewoo and Kim, Hyunwoo J.},
	month = feb,
	year = {2020},
	note = {arXiv:1911.06455 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Neural Information Processing Systems (NeurIPS), 2019},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/I5G6EDFD/Yun et al. - 2020 - Graph Transformer Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/IEAQN89Z/1911.html:text/html},
}

@misc{gilmer_neural_2017,
	title = {Neural {Message} {Passing} for {Quantum} {Chemistry}},
	url = {http://arxiv.org/abs/1704.01212},
	doi = {10.48550/arXiv.1704.01212},
	abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
	month = jun,
	year = {2017},
	note = {arXiv:1704.01212 [cs]},
	keywords = {Computer Science - Machine Learning, I.2.6},
	annote = {Comment: 14 pages},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/2ICK5K6P/Gilmer et al. - 2017 - Neural Message Passing for Quantum Chemistry.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/MJTTC8BC/1704.html:text/html},
}

@misc{kipf_semi-supervised_2017,
	title = {Semi-{Supervised} {Classification} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1609.02907},
	doi = {10.48550/arXiv.1609.02907},
	abstract = {We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Kipf, Thomas N. and Welling, Max},
	month = feb,
	year = {2017},
	note = {arXiv:1609.02907 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Published as a conference paper at ICLR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QMVKA4M7/Kipf and Welling - 2017 - Semi-Supervised Classification with Graph Convolut.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LWB5G96C/1609.html:text/html},
}

@misc{brody_how_2022,
	title = {How {Attentive} are {Graph} {Attention} {Networks}?},
	url = {http://arxiv.org/abs/2105.14491},
	doi = {10.48550/arXiv.2105.14491},
	abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how\_attentive\_are\_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.},
	urldate = {2024-05-15},
	publisher = {arXiv},
	author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	month = jan,
	year = {2022},
	note = {arXiv:2105.14491 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published in ICLR 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/NPFJEURS/Brody et al. - 2022 - How Attentive are Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/C9KT3LEF/2105.html:text/html},
}

@misc{wu_medsegdiff-v2_2023,
	title = {{MedSegDiff}-{V2}: {Diffusion} based {Medical} {Image} {Segmentation} with {Transformer}},
	shorttitle = {{MedSegDiff}-{V2}},
	url = {http://arxiv.org/abs/2301.11798},
	doi = {10.48550/arXiv.2301.11798},
	abstract = {The Diffusion Probabilistic Model (DPM) has recently gained popularity in the field of computer vision, thanks to its image generation applications, such as Imagen, Latent Diffusion Models, and Stable Diffusion, which have demonstrated impressive capabilities and sparked much discussion within the community. Recent investigations have further unveiled the utility of DPM in the domain of medical image analysis, as underscored by the commendable performance exhibited by the medical image segmentation model across various tasks. Although these models were originally underpinned by a UNet architecture, there exists a potential avenue for enhancing their performance through the integration of vision transformer mechanisms. However, we discovered that simply combining these two models resulted in subpar performance. To effectively integrate these two cutting-edge techniques for the Medical image segmentation, we propose a novel Transformer-based Diffusion framework, called MedSegDiff-V2. We verify its effectiveness on 20 medical image segmentation tasks with different image modalities. Through comprehensive evaluation, our approach demonstrates superiority over prior state-of-the-art (SOTA) methodologies. Code is released at https://github.com/KidsWithTokens/MedSegDiff},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Wu, Junde and Ji, Wei and Fu, Huazhu and Xu, Min and Jin, Yueming and Xu, Yanwu},
	month = dec,
	year = {2023},
	note = {arXiv:2301.11798 [cs, eess]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing},
	annote = {Comment: Code will be released at https://github.com/KidsWithTokens/MedSegDiff},
}

@misc{schlichtkrull_modeling_2017,
	title = {Modeling {Relational} {Data} with {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1703.06103},
	doi = {10.48550/arXiv.1703.06103},
	abstract = {Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to deal with the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved by enriching them with an encoder model to accumulate evidence over multiple inference steps in the relational graph, demonstrating a large improvement of 29.8\% on FB15k-237 over a decoder-only baseline.},
	urldate = {2024-05-22},
	publisher = {arXiv},
	author = {Schlichtkrull, Michael and Kipf, Thomas N. and Bloem, Peter and Berg, Rianne van den and Titov, Ivan and Welling, Max},
	month = oct,
	year = {2017},
	note = {arXiv:1703.06103 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Databases},
	file = {arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/IS5JCZQA/1703.html:text/html},
}

@misc{su_roformer_2023,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {http://arxiv.org/abs/2104.09864},
	doi = {10.48550/arXiv.2104.09864},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: {\textbackslash}url\{https://huggingface.co/docs/transformers/model\_doc/roformer\}.},
	urldate = {2024-05-24},
	publisher = {arXiv},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = nov,
	year = {2023},
	note = {arXiv:2104.09864 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: fixed some typos},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/NI6F98Q4/Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/VB6DQ59L/2104.html:text/html},
}

@misc{pfaff_learning_2021,
	title = {Learning {Mesh}-{Based} {Simulation} with {Graph} {Networks}},
	url = {http://arxiv.org/abs/2010.03409},
	doi = {10.48550/arXiv.2010.03409},
	abstract = {Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.},
	urldate = {2024-05-28},
	publisher = {arXiv},
	author = {Pfaff, Tobias and Fortunato, Meire and Sanchez-Gonzalez, Alvaro and Battaglia, Peter W.},
	month = jun,
	year = {2021},
	note = {arXiv:2010.03409 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/65DNAT23/Pfaff et al. - 2021 - Learning Mesh-Based Simulation with Graph Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/SIT7FAN4/2010.html:text/html},
}

@misc{saimon_advancing_2024,
	title = {Advancing {Additive} {Manufacturing} through {Deep} {Learning}: {A} {Comprehensive} {Review} of {Current} {Progress} and {Future} {Challenges}},
	shorttitle = {Advancing {Additive} {Manufacturing} through {Deep} {Learning}},
	url = {http://arxiv.org/abs/2403.00669},
	doi = {10.48550/arXiv.2403.00669},
	abstract = {Additive manufacturing (AM) has already proved itself to be the potential alternative to widely-used subtractive manufacturing due to its extraordinary capacity of manufacturing highly customized products with minimum material wastage. Nevertheless, it is still not being considered as the primary choice for the industry due to some of its major inherent challenges, including complex and dynamic process interactions, which are sometimes difficult to fully understand even with traditional machine learning because of the involvement of high-dimensional data such as images, point clouds, and voxels. However, the recent emergence of deep learning (DL) is showing great promise in overcoming many of these challenges as DL can automatically capture complex relationships from high-dimensional data without hand-crafted feature extraction. Therefore, the volume of research in the intersection of AM and DL is exponentially growing each year which makes it difficult for the researchers to keep track of the trend and future potential directions. Furthermore, to the best of our knowledge, there is no comprehensive review paper in this research track summarizing the recent studies. Therefore, this paper reviews the recent studies that apply DL for making the AM process better with a high-level summary of their contributions and limitations. Finally, it summarizes the current challenges and recommends some of the promising opportunities in this domain for further investigation with a special focus on generalizing DL models for wide-range of geometry types, managing uncertainties both in AM data and DL models, overcoming limited and noisy AM data issues by incorporating generative models, and unveiling the potential of interpretable DL for AM.},
	urldate = {2024-06-04},
	publisher = {arXiv},
	author = {Saimon, Amirul Islam and Yangue, Emmanuel and Yue, Xiaowei and Kong, Zhenyu James and Liu, Chenang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.00669 [cs]
version: 1},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/H5H5FMIS/Saimon et al. - 2024 - Advancing Additive Manufacturing through Deep Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/ID3TKULZ/2403.html:text/html},
}

@misc{hu_heterogeneous_2020,
	title = {Heterogeneous {Graph} {Transformer}},
	url = {http://arxiv.org/abs/2003.01332},
	doi = {10.48550/arXiv.2003.01332},
	abstract = {Recent years have witnessed the emerging success of graph neural networks (GNNs) for modeling structured data. However, most GNNs are designed for homogeneous graphs, in which all nodes and edges belong to the same types, making them infeasible to represent heterogeneous structures. In this paper, we present the Heterogeneous Graph Transformer (HGT) architecture for modeling Web-scale heterogeneous graphs. To model heterogeneity, we design node- and edge-type dependent parameters to characterize the heterogeneous attention over each edge, empowering HGT to maintain dedicated representations for different types of nodes and edges. To handle dynamic heterogeneous graphs, we introduce the relative temporal encoding technique into HGT, which is able to capture the dynamic structural dependency with arbitrary durations. To handle Web-scale graph data, we design the heterogeneous mini-batch graph sampling algorithm---HGSampling---for efficient and scalable training. Extensive experiments on the Open Academic Graph of 179 million nodes and 2 billion edges show that the proposed HGT model consistently outperforms all the state-of-the-art GNN baselines by 9\%--21\% on various downstream tasks.},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Hu, Ziniu and Dong, Yuxiao and Wang, Kuansan and Sun, Yizhou},
	month = mar,
	year = {2020},
	note = {arXiv:2003.01332 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Published on WWW 2020},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/XSQJE9FU/Hu et al. - 2020 - Heterogeneous Graph Transformer.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/NHWJDXKI/2003.html:text/html},
}

@misc{ogoke_inexpensive_2023,
	title = {Inexpensive {High} {Fidelity} {Melt} {Pool} {Models} in {Additive} {Manufacturing} {Using} {Generative} {Deep} {Diffusion}},
	url = {http://arxiv.org/abs/2311.16168},
	doi = {10.48550/arXiv.2311.16168},
	abstract = {Defects in laser powder bed fusion (L-PBF) parts often result from the meso-scale dynamics of the molten alloy near the laser, known as the melt pool. For instance, the melt pool can directly contribute to the formation of undesirable porosity, residual stress, and surface roughness in the final part. Experimental in-situ monitoring of the three-dimensional melt pool physical fields is challenging, due to the short length and time scales involved in the process. Multi-physics simulation methods can describe the three-dimensional dynamics of the melt pool, but are computationally expensive at the mesh refinement required for accurate predictions of complex effects, such as the formation of keyhole porosity. Therefore, in this work, we develop a generative deep learning model based on the probabilistic diffusion framework to map low-fidelity, coarse-grained simulation information to the high-fidelity counterpart. By doing so, we bypass the computational expense of conducting multiple high-fidelity simulations for analysis by instead upscaling lightweight coarse mesh simulations. Specifically, we implement a 2-D diffusion model to spatially upscale cross-sections of the coarsely simulated melt pool to their high-fidelity equivalent. We demonstrate the preservation of key metrics of the melting process between the ground truth simulation data and the diffusion model output, such as the temperature field, the melt pool dimensions and the variability of the keyhole vapor cavity. Specifically, we predict the melt pool depth within 3 \${\textbackslash}mu m\$ based on low-fidelity input data 4\${\textbackslash}times\$ coarser than the high-fidelity simulations, reducing analysis time by two orders of magnitude.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Ogoke, Francis and Liu, Quanliang and Ajenifujah, Olabode and Myers, Alexander and Quirarte, Guadalupe and Beuth, Jack and Malen, Jonathan and Farimani, Amir Barati},
	month = nov,
	year = {2023},
	note = {arXiv:2311.16168 [cond-mat]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/5RQICEMK/Ogoke et al. - 2023 - Inexpensive High Fidelity Melt Pool Models in Addi.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/KL6X43WU/2311.html:text/html},
}

@misc{jain_latticegraphnet_2024,
	title = {{LatticeGraphNet}: {A} two-scale graph neural operator for simulating lattice structures},
	shorttitle = {{LatticeGraphNet}},
	url = {http://arxiv.org/abs/2402.01045},
	doi = {10.48550/arXiv.2402.01045},
	abstract = {This study introduces a two-scale Graph Neural Operator (GNO), namely, LatticeGraphNet (LGN), designed as a surrogate model for costly nonlinear finite-element simulations of three-dimensional latticed parts and structures. LGN has two networks: LGN-i, learning the reduced dynamics of lattices, and LGN-ii, learning the mapping from the reduced representation onto the tetrahedral mesh. LGN can predict deformation for arbitrary lattices, therefore the name operator. Our approach significantly reduces inference time while maintaining high accuracy for unseen simulations, establishing the use of GNOs as efficient surrogate models for evaluating mechanical responses of lattices and structures.},
	urldate = {2024-06-06},
	publisher = {arXiv},
	author = {Jain, Ayush and Haghighat, Ehsan and Nelaturi, Sai},
	month = feb,
	year = {2024},
	note = {arXiv:2402.01045 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computational Engineering, Finance, and Science},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/TXQMVPMX/Jain et al. - 2024 - LatticeGraphNet A two-scale graph neural operator.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/46B3B9F5/2402.html:text/html},
}

@article{guo_semi-supervised_2021,
	title = {Semi-supervised deep learning based framework for assessing manufacturability of cellular structures in direct metal laser sintering process},
	volume = {32},
	issn = {0956-5515},
	url = {https://doi.org/10.1007/s10845-020-01575-0},
	doi = {10.1007/s10845-020-01575-0},
	abstract = {In recent years, metal cellular structures have drawn attentions in various industrial sectors due to their design freedoms and abilities to achieve multi-functional mechanical properties. However, metal cellular structures are difficult to fabricate due to their complex geometries, even with modern additive manufacturing technologies such as the direct metal laser sintering (DMLS) process. Assessing the manufacturability of metal cellular structures via a DMLS process is a challenging task as the geometric features of the structures are complex. Besides, via a DMLS process, the manufacturability also depends on the cumulative deformation of the layers during the manufacturing process. Existing methods on Design for Additive Manufacturing (DFAM) provide design guidelines that are based on past successful printed designs. However, they are not effective in predicting the manufacturability of metal cellular structures. In this paper, we propose a semi-supervised deep learning based manufacturability assessment (SSDLMA) framework to assess whether a metal cellular structure can be successfully manufactured from a given DMLS process. To enable efficient learning, we represent the complex cellular structures as 3D binary arrays with a simple yet efficient voxelisation method. We then train a deep learning based classifier using only a small amount of experimental data by adopting a semi-supervised learning approach. By running real experiments and comparing with existing DFAM methods and machine learning models, we demonstrate the advantages of the proposed SSDLMA framework. The proposed framework can be extended to predict the manufacturability of various other complex geometries beyond cellular structure in a reliable way even with a small number of training data.},
	number = {2},
	urldate = {2024-06-20},
	journal = {Journal of Intelligent Manufacturing},
	author = {Guo, Yilin and Lu, Wen Feng and Fuh, Jerry Ying Hsi},
	month = feb,
	year = {2021},
	keywords = {Cellular structures, Design for additive manufacturing, Direct metal laser sintering, Manufacturability analysis, Semi-supervised deep learning},
	pages = {347--359},
}

@article{hertlein_generative_2021,
	title = {Generative adversarial network for early-stage design flexibility in topology optimization for additive manufacturing},
	volume = {59},
	issn = {0278-6125},
	url = {https://www.sciencedirect.com/science/article/pii/S027861252100087X},
	doi = {10.1016/j.jmsy.2021.04.007},
	abstract = {Topology optimization has become a valuable design tool for structures to be fabricated by additive manufacturing (AM). However, during early stage design, parameters are frequently evolving, resulting in multiple similar TO runs. Especially when design for manufacturing principles expand the parameter space, this iterative process is computationally burdensome, and does not take advantage of redundant information in each study. We introduce a deep learning-based framework that learns latent similarities between runs in a training set to predict near optimal designs, enabling efficient wholistic understanding of the problem setup space, which includes both loading conditions and, for the first time in this study, manufacturing process configuration. Learning was achieved using a conditional generative adversarial network (cGAN) trained on a dataset of randomized boundary conditions, loadings, and AM build orientations, and the corresponding optimal structures obtained through overhang-filtered TO. cGAN predictions showed good agreement with true optima. For even greater accuracy, predictions can be post-processed by applying a small number of TO iterations. Manifold learning techniques were used to provide further insight, and we were able to conclude that the cGAN error generally increases with distance between the load and the boundary conditions or build plate. Interestingly, in 9\% of test cases, the cGAN generated structures with compliances better than the corresponding TO-calculated structures, often by as much as 50 \% with an average of 7.8 \%. That some of these structures appeared qualitatively different in form suggests the potential value of the approach in other domains such as generative design, where a range of alternate near-optimal designs are used to guide the ideation process.},
	urldate = {2024-06-20},
	journal = {Journal of Manufacturing Systems},
	author = {Hertlein, Nathan and Buskohl, Philip R. and Gillman, Andrew and Vemaganti, Kumar and Anand, Sam},
	month = apr,
	year = {2021},
	keywords = {Additive manufacturing, Deep learning, Generative adversarial network, Topology optimization},
	pages = {675--685},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/IBRG5VBY/S027861252100087X.html:text/html},
}

@article{li_augmented_2022,
	title = {Augmented {Time} {Regularized} {Generative} {Adversarial} {Network} ({ATR}-{GAN}) for {Data} {Augmentation} in {Online} {Process} {Anomaly} {Detection}},
	volume = {19},
	issn = {1558-3783},
	url = {https://ieeexplore.ieee.org/document/9592834},
	doi = {10.1109/TASE.2021.3118635},
	abstract = {Supervised machine learning techniques, such as classification models, have been widely applied to online process anomaly detection in advanced manufacturing. However, since abnormal process states rarely occur in regular manufacturing settings, the data collected for model training may be highly imbalanced, which may result in significant training bias for supervised learning and, thus, further deteriorate the anomaly detection accuracy. To reduce the training bias, a natural idea is to incorporate data augmentation techniques to generate effective artificial sample data for the abnormal process states. However, most of the existing data augmentation methods do not effectively consider the temporal orders of the sensor signals, and they also usually require large amounts of actual samples to ensure satisfactory augmentation performance. To address these limitations, this article developed a novel data-driven methodology termed augmented time regularized generative adversarial network (ATR-GAN). By incorporating a proposed augmented generator, ATR-GAN is capable of generating more effective artificial samples for training supervised learning models. The novelty of this augmented generator in the proposed methodology can be summarized into three aspects: 1) an augmented filter layer is introduced in the augmented generator to identify the high-quality artificial samples; 2) in the augmented filter layer, a new distance metric termed time-regularized Hausdorff (TRH) distance is developed to accurately measure the similarity between actual samples and the generated artificial samples; and 3) batching techniques are also employed in the proposed augmented generator to further increase the diversity of the artificial data and fully utilize the relatively limited training data. In addition, the effectiveness of the proposed ATR-GAN is also validated by both numerical simulation and a real-world case study in additive manufacturing. Note to Practitioners—Online process anomaly detection currently plays a significant role in advanced manufacturing since unexpected anomalies may damage product quality and even result in catastrophic loss. In practice, processes are mostly under normal conditions, and anomalies rarely occur. Therefore, the data collected under abnormal conditions are very limited compared to normal conditions, which causes the data imbalanced issue, leading to deterioration in detection accuracy. Many existing data augmentation methods, such as generative adversarial network (GAN), cannot synthesize diversified high-quality artificial samples only using relatively limited actual samples. There is an urgent need in developing an effective data augmentation methodology to address the data imbalanced issue in process anomaly detection. This article developed a novel approach called augmented time regularized GAN (ATR-GAN) for online sensor data augmentation. With this new approach, the applications in additive manufacturing demonstrate that the performance of data augmentation can be improved effectively, and thereafter, the anomaly detection accuracy is also increased significantly. Moreover, the developed methodology is inherently integrated into a generic framework. Thus, it can be further transformed for applications in many other areas that need data augmentation.},
	number = {4},
	urldate = {2024-06-25},
	journal = {IEEE Transactions on Automation Science and Engineering},
	author = {Li, Yuxuan and Shi, Zhangyue and Liu, Chenang and Tian, Wenmeng and Kong, Zhenyu and Williams, Christopher B.},
	month = oct,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Automation Science and Engineering},
	keywords = {Advanced manufacturing, Anomaly detection, augmented generator, augmented time regularized generative adversarial network (ATR-GAN), data augmentation, Generative adversarial networks, Manufacturing, Manufacturing processes, online process anomaly detection, Three-dimensional printing, Training},
	pages = {3338--3355},
	file = {IEEE Xplore Abstract Record:/Users/rupertmenneer/Zotero/storage/32MXBTR9/9592834.html:text/html},
}

@article{song_hybrid_2023,
	title = {A {Hybrid} {Deep} {Generative} {Network} for {Pore} {Morphology} {Prediction} in {Metal} {Additive} {Manufacturing}},
	volume = {145},
	issn = {1087-1357},
	url = {https://doi.org/10.1115/1.4057012},
	doi = {10.1115/1.4057012},
	abstract = {Metal additive manufacturing (AM) has been receiving unprecedented attention for its transformational role in extending the AM materials from polymers to various metals. However, various quality issues, especially porosity, significantly impacts the mechanical properties and fatigue life of the final products, which imposes barriers for the widespread adoption of metal AM processes. In this study, we use the deep learning (DL) techniques to comprehensively investigate the relationships between pore microstructure and processing parameters. Specifically, a novel hybrid deep generative prediction network (HDGPN) that leverages both variational autoencoder and generative adversarial network is proposed to characterize the complex pore microstructure with in-depth representations and predict pore morphology under arbitrary processing parameters. By visualizing the predicted pore morphology, the complicated interaction dynamics between the processing parameters and pore microstructure are directly revealed, which may guide the optimization of metal AM manufacturing processes to fabricate defect-free products. A case study of a selective laser melting (SLM) process is conducted to validate the proposed modeling and prediction framework.},
	number = {071005},
	urldate = {2024-06-25},
	journal = {Journal of Manufacturing Science and Engineering},
	author = {Song, Zheren and Wang, Xinming and Gao, Yuanyuan and Son, Junbo and Wu, Jianguo},
	month = mar,
	year = {2023},
	file = {Snapshot:/Users/rupertmenneer/Zotero/storage/UNEBWJCU/A-Hybrid-Deep-Generative-Network-for-Pore.html:text/html;Song et al. - 2023 - A Hybrid Deep Generative Network for Pore Morpholo.pdf:/Users/rupertmenneer/Zotero/storage/YIYYYQ7Z/Song et al. - 2023 - A Hybrid Deep Generative Network for Pore Morpholo.pdf:application/pdf},
}

@article{jadhav_stressd_2023,
	title = {{StressD}: {2D} {Stress} estimation using denoising diffusion model},
	volume = {416},
	issn = {0045-7825},
	shorttitle = {{StressD}},
	url = {https://www.sciencedirect.com/science/article/pii/S004578252300467X},
	doi = {10.1016/j.cma.2023.116343},
	abstract = {Finite element analysis (FEA), a common approach for simulating stress distribution for a given geometry, is generally associated with high computational cost, especially when high mesh resolution is required. Furthermore, the non-adaptive nature of FEA requires the entire model to be solved even for minor geometric variations creating a bottleneck during iterative design optimization. This necessitates a framework that can efficiently predict stress distribution in geometries based on given boundary and loading conditions. In this paper, we present StressD, a framework for predicting von Mises stress fields based on the denoising diffusion model. The StressD framework involves two models, a U-net-based denoising diffusion model and an auxiliary network to generate and predict stress distribution in structures. The denoising diffusion model generates a normalized stress map based on the given geometry, boundary conditions and loading condition, while the auxiliary network is used to determine the scaling information needed to un-normalize the generated stress map. We evaluate the StressD framework on cantilever structures and show that it is able to accurately predict von Mises stress fields while significantly reducing computational cost compared to traditional FEA.},
	urldate = {2024-06-25},
	journal = {Computer Methods in Applied Mechanics and Engineering},
	author = {Jadhav, Yayati and Berthel, Joseph and Hu, Chunshan and Panat, Rahul and Beuth, Jack and Barati Farimani, Amir},
	month = nov,
	year = {2023},
	keywords = {Deep learning, Denoising Diffusion Probabilistic model, FEA Surrogate model, High resolution Stress field prediction, Vision Transformer},
	pages = {116343},
	file = {Jadhav et al. - 2023 - StressD 2D Stress estimation using denoising diff.pdf:/Users/rupertmenneer/Zotero/storage/FFZMGHXM/Jadhav et al. - 2023 - StressD 2D Stress estimation using denoising diff.pdf:application/pdf;ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/PR5S4FJS/S004578252300467X.html:text/html},
}

@misc{ogoke_deep_2024,
	title = {Deep {Learning} for {Melt} {Pool} {Depth} {Contour} {Prediction} {From} {Surface} {Thermal} {Images} via {Vision} {Transformers}},
	url = {http://arxiv.org/abs/2404.17699},
	doi = {10.48550/arXiv.2404.17699},
	abstract = {Insufficient overlap between the melt pools produced during Laser Powder Bed Fusion (L-PBF) can lead to lack-of-fusion defects and deteriorated mechanical and fatigue performance. In-situ monitoring of the melt pool subsurface morphology requires specialized equipment that may not be readily accessible or scalable. Therefore, we introduce a machine learning framework to correlate in-situ two-color thermal images observed via high-speed color imaging to the two-dimensional profile of the melt pool cross-section. Specifically, we employ a hybrid CNN-Transformer architecture to establish a correlation between single bead off-axis thermal image sequences and melt pool cross-section contours measured via optical microscopy. In this architecture, a ResNet model embeds the spatial information contained within the thermal images to a latent vector, while a Transformer model correlates the sequence of embedded vectors to extract temporal information. Our framework is able to model the curvature of the subsurface melt pool structure, with improved performance in high energy density regimes compared to analytical melt pool models. The performance of this model is evaluated through dimensional and geometric comparisons to the corresponding experimental melt pool observations.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Ogoke, Francis and Pak, Peter Myung-Won and Myers, Alexander and Quirarte, Guadalupe and Beuth, Jack and Malen, Jonathan and Farimani, Amir Barati},
	month = may,
	year = {2024},
	note = {arXiv:2404.17699 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/BDTF2ACG/Ogoke et al. - 2024 - Deep Learning for Melt Pool Depth Contour Predicti.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/DW5UW7Y7/2404.html:text/html},
}

@misc{pak_thermopore_2024,
	title = {{ThermoPore}: {Predicting} {Part} {Porosity} {Based} on {Thermal} {Images} {Using} {Deep} {Learning}},
	shorttitle = {{ThermoPore}},
	url = {http://arxiv.org/abs/2404.16882},
	doi = {10.48550/arXiv.2404.16882},
	abstract = {We present a deep learning approach for quantifying and localizing ex-situ porosity within Laser Powder Bed Fusion fabricated samples utilizing in-situ thermal image monitoring data. Our goal is to build the real time porosity map of parts based on thermal images acquired during the build. The quantification task builds upon the established Convolutional Neural Network model architecture to predict pore count and the localization task leverages the spatial and temporal attention mechanisms of the novel Video Vision Transformer model to indicate areas of expected porosity. Our model for porosity quantification achieved a \$R{\textasciicircum}2\$ score of 0.57 and our model for porosity localization produced an average IoU score of 0.32 and a maximum of 1.0. This work is setting the foundations of part porosity "Digital Twins" based on additive manufacturing monitoring data and can be applied downstream to reduce time-intensive post-inspection and testing activities during part qualification and certification. In addition, we seek to accelerate the acquisition of crucial insights normally only available through ex-situ part evaluation by means of machine learning analysis of in-situ process monitoring data.},
	urldate = {2024-06-25},
	publisher = {arXiv},
	author = {Pak, Peter Myung-Won and Ogoke, Francis and Polonsky, Andrew and Garland, Anthony and Bolintineanu, Dan S. and Moser, Dan R. and Heiden, Michael J. and Farimani, Amir Barati},
	month = apr,
	year = {2024},
	note = {arXiv:2404.16882 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/F9KWS2ED/Pak et al. - 2024 - ThermoPore Predicting Part Porosity Based on Ther.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/3KD3UQF2/2404.html:text/html},
}

@misc{he_deep_2015,
	title = {Deep {Residual} {Learning} for {Image} {Recognition}},
	url = {http://arxiv.org/abs/1512.03385},
	doi = {10.48550/arXiv.1512.03385},
	abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
	month = dec,
	year = {2015},
	note = {arXiv:1512.03385 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Tech report},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/GWASHZ7T/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/64PRDIHE/1512.html:text/html},
}

@misc{song_generative_2020,
	title = {Generative {Modeling} by {Estimating} {Gradients} of the {Data} {Distribution}},
	url = {http://arxiv.org/abs/1907.05600},
	doi = {10.48550/arXiv.1907.05600},
	abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Song, Yang and Ermon, Stefano},
	month = oct,
	year = {2020},
	note = {arXiv:1907.05600 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NeurIPS 2019 (Oral)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/UE3HEHZ4/Song and Ermon - 2020 - Generative Modeling by Estimating Gradients of the.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/WAXAZCRE/1907.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/EHJZ2EPV/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/FA4JIEGF/1503.html:text/html},
}

@article{vincent_connection_2011,
	title = {A {Connection} {Between} {Score} {Matching} and {Denoising} {Autoencoders}},
	volume = {23},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/23/7/1661-1674/7677},
	doi = {10.1162/NECO_a_00142},
	abstract = {Denoising autoencoders have been previously shown to be competitive alternatives to Restricted Boltzmann Machines for unsupervised pre-training of each layer of a deep architecture. We show that a simple denoising autoencoder training criterion is equivalent to matching the score (with respect to the data) of a speciﬁc energy based model to that of a non-parametric Parzen density estimator of the data. This yields several useful insights. It deﬁnes a proper probabilistic model for the denoising autoencoder technique which makes it in principle possible to sample from them or to rank examples by their energy. It suggests a different way to apply score matching that is related to learning to denoise and does not require computing second derivatives. It justiﬁes the use of tied weights between the encoder and decoder, and suggests ways to extend the success of denoising autoencoders to a larger family of energy-based models.},
	language = {en},
	number = {7},
	urldate = {2024-06-29},
	journal = {Neural Computation},
	author = {Vincent, Pascal},
	month = jul,
	year = {2011},
	pages = {1661--1674},
	file = {Vincent - 2011 - A Connection Between Score Matching and Denoising .pdf:/Users/rupertmenneer/Zotero/storage/7R5DBKYB/Vincent - 2011 - A Connection Between Score Matching and Denoising .pdf:application/pdf},
}

@misc{thies_deferred_2019,
	title = {Deferred {Neural} {Rendering}: {Image} {Synthesis} using {Neural} {Textures}},
	shorttitle = {Deferred {Neural} {Rendering}},
	url = {http://arxiv.org/abs/1904.12356},
	doi = {10.48550/arXiv.1904.12356},
	abstract = {The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.},
	urldate = {2024-07-10},
	publisher = {arXiv},
	author = {Thies, Justus and Zollhöfer, Michael and Nießner, Matthias},
	month = apr,
	year = {2019},
	note = {arXiv:1904.12356 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: Video: https://youtu.be/z-pVip6WeyY SIGGRAPH 2019},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/YZVIENGL/Thies et al. - 2019 - Deferred Neural Rendering Image Synthesis using N.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LRU6465Z/1904.html:text/html},
}

@misc{isola_image--image_2018-1,
	title = {Image-to-{Image} {Translation} with {Conditional} {Adversarial} {Networks}},
	url = {http://arxiv.org/abs/1611.07004},
	doi = {10.48550/arXiv.1611.07004},
	abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
	month = nov,
	year = {2018},
	note = {arXiv:1611.07004 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Website: https://phillipi.github.io/pix2pix/, CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/U3RFBNB7/Isola et al. - 2018 - Image-to-Image Translation with Conditional Advers.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/U8GHKRZL/1611.html:text/html},
}

@misc{tewari_advances_2022,
	title = {Advances in {Neural} {Rendering}},
	url = {http://arxiv.org/abs/2111.05849},
	doi = {10.48550/arXiv.2111.05849},
	abstract = {Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline. This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects...},
	urldate = {2024-07-14},
	publisher = {arXiv},
	author = {Tewari, Ayush and Thies, Justus and Mildenhall, Ben and Srinivasan, Pratul and Tretschk, Edgar and Wang, Yifan and Lassner, Christoph and Sitzmann, Vincent and Martin-Brualla, Ricardo and Lombardi, Stephen and Simon, Tomas and Theobalt, Christian and Niessner, Matthias and Barron, Jonathan T. and Wetzstein, Gordon and Zollhoefer, Michael and Golyanik, Vladislav},
	month = mar,
	year = {2022},
	note = {arXiv:2111.05849 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: 33 pages, 14 figures, 5 tables; State of the Art Report at EUROGRAPHICS 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/XVQT3YXB/Tewari et al. - 2022 - Advances in Neural Rendering.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/TN3E4R8Z/2111.html:text/html},
}

@article{margadji_iterative_2024,
	title = {Iterative learning for efficient additive mass production},
	volume = {89},
	issn = {2214-8604},
	url = {https://www.sciencedirect.com/science/article/pii/S2214860424003178},
	doi = {10.1016/j.addma.2024.104271},
	abstract = {Material extrusion could enable on-demand production of complex and personalized parts but is limited by low reliability, particularly in higher-volume production. Machine learning-based methods may enhance reliability, but are often themselves insufficiently reliable for use in production. Foundation artificial intelligence models have enabled significant improvements in performance across many tasks. Here, a vision-based control system is reported, coupling active learning and uncertainty awareness with a foundation model to continually learn to build a specific part better. The resulting framework is called Iterative Learning, as it improves performance by learning from its own errors during repeated build cycles of the same part. The iterative learning approach is shown to enable robust error detection and correction while being more space, time and computationally efficient compared to a naive fine-tuning approach. This provides a path showing how foundation models may be adapted to enhance reliability across a wider range of additive manufacturing processes.},
	urldate = {2024-07-15},
	journal = {Additive Manufacturing},
	author = {Margadji, Christos and Brion, Douglas A. J. and Pattinson, Sebastian W.},
	month = jun,
	year = {2024},
	keywords = {Deep learning, Artificial intelligence, Mass production, Process control},
	pages = {104271},
	file = {Margadji et al. - 2024 - Iterative learning for efficient additive mass pro.pdf:/Users/rupertmenneer/Zotero/storage/772GI2DV/Margadji et al. - 2024 - Iterative learning for efficient additive mass pro.pdf:application/pdf;ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/NARLQMY7/S2214860424003178.html:text/html},
}

@article{ngo_additive_2018,
	title = {Additive manufacturing ({3D} printing): {A} review of materials, methods, applications and challenges},
	volume = {143},
	issn = {1359-8368},
	shorttitle = {Additive manufacturing ({3D} printing)},
	url = {https://www.sciencedirect.com/science/article/pii/S1359836817342944},
	doi = {10.1016/j.compositesb.2018.02.012},
	abstract = {Freedom of design, mass customisation, waste minimisation and the ability to manufacture complex structures, as well as fast prototyping, are the main benefits of additive manufacturing (AM) or 3D printing. A comprehensive review of the main 3D printing methods, materials and their development in trending applications was carried out. In particular, the revolutionary applications of AM in biomedical, aerospace, buildings and protective structures were discussed. The current state of materials development, including metal alloys, polymer composites, ceramics and concrete, was presented. In addition, this paper discussed the main processing challenges with void formation, anisotropic behaviour, the limitation of computer design and layer-by-layer appearance. Overall, this paper gives an overview of 3D printing, including a survey on its benefits and drawbacks as a benchmark for future research and development.},
	urldate = {2024-07-15},
	journal = {Composites Part B: Engineering},
	author = {Ngo, Tuan D. and Kashani, Alireza and Imbalzano, Gabriele and Nguyen, Kate T. Q. and Hui, David},
	month = jun,
	year = {2018},
	keywords = {3D printing, Additive manufacturing, Aerospace, Biomaterials, Buildings, Ceramics, Composites, Concrete, Metal alloys, protective structures},
	pages = {172--196},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/U3NEALF3/S1359836817342944.html:text/html},
}

@incollection{najmon_2_2019,
	title = {2 - {Review} of additive manufacturing technologies and applications in the aerospace industry},
	isbn = {978-0-12-814062-8},
	url = {https://www.sciencedirect.com/science/article/pii/B9780128140628000029},
	abstract = {Additive manufacturing (AM) is transforming all segments of the aerospace industry, including commercial and military aircraft, space applications, as well as missiles systems. Such transformation is due to the unique ability of AM to produce parts with complex designs, reduce manufacturing costs (material waste, assembly due to part consolidation, and the need for tools and fixtures), and fabricate parts with premium materials with small production runs and short turnaround times. AM allows the realization of advanced part designs that provide additional space, multifunctional parts, multimaterial parts, part consolidation, and parts that are difficult to machine. The capability of AM to fabricate freeform designs makes it very suitable for the aerospace industry. To date, aerospace companies, such as Boeing, have installed tens of thousands AM parts (including 200 unique nonmetallic part references) on 16 commercial and military aircraft. It has also started the production of titanium AM parts that will allow savings of up to three million USD per aircraft in the near future. GE Aviation is using metal AM to manufacture thousands of fuel nozzles annually for its new LEAP engine. Similarly, Airbus is utilizing metal AM brackets and bleed pipes on its aircraft. It is currently collaborating with Arconic on the production of large-scale AM airframe components and expects to produce 30t of AM metal parts by December 2018. The main applications of AM in the aerospace industry are rapid prototyping, rapid tooling, and repair, as well as direct digital manufacturing (DDM) of parts made of metal, plastic, ceramic, and composite materials. Currently, the fastest growing application is DDM (final part manufacturing). For metal parts, the main AM technologies in aerospace applications are directed energy deposition and powder bed fusion. For nonmetallic parts, the dominant AM technologies are vat photopolymerization, material jetting, and material extrusion. This chapter reviews the applications, benefits, and opportunities of AM for the aerospace industry, describes the relevant AM technologies, and discusses the current challenges and potential applications.},
	urldate = {2024-07-15},
	booktitle = {Additive {Manufacturing} for the {Aerospace} {Industry}},
	publisher = {Elsevier},
	author = {Najmon, Joel C. and Raeisi, Sajjad and Tovar, Andres},
	editor = {Froes, Francis and Boyer, Rodney},
	month = jan,
	year = {2019},
	doi = {10.1016/B978-0-12-814062-8.00002-9},
	keywords = {Additive manufacturing applications, additive metal technologies, direct digital manufacturing, rapid prototyping, rapid tooling, repair},
	pages = {7--31},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/A3VW7LA2/B9780128140628000029.html:text/html},
}

@article{hager_3d_2016,
	series = {Ecology and new building materials and products 2016},
	title = {{3D} {Printing} of {Buildings} and {Building} {Components} as the {Future} of {Sustainable} {Construction}?},
	volume = {151},
	issn = {1877-7058},
	url = {https://www.sciencedirect.com/science/article/pii/S1877705816317453},
	doi = {10.1016/j.proeng.2016.07.357},
	abstract = {The paper presents the state-of-the-art concerning the current achievements in the field of 3D printing of buildings and building components. The 3D printing technologies, comparing to traditional techniques of constructing the buildings, could be considered as environmental friendly derivative giving almost unlimited possibilities for geometric complexity realizations. Two kinds of technologies were described in this paper with pointing to Contour Crafting as a promising technique that may be able to revolutionize construction industry in near future. Numerous advantages of this technology, such as reduction of the costs and time, minimizing the pollution of environment and decrease of injuries and fatalities on construction sites could be cited. Despite many advantages and hopes, some concerns are summarized in the conclusions, as the technology still has many limitations. A brief description of few examples of pioneering usage of 3D printing in construction industry are presented (Canal House in Amsterdam, WinSun company and printing application for building carried out by Skanska company). Creating a model that will be appropriate for 3D printers is possible in many different modelling programs. One of the most popular formats for sharing such models is STL format. In the paper sample models crated in Autodesk Inventor are shown, but also other tools suitable for preparing models for 3D printing are briefly discussed.},
	urldate = {2024-07-15},
	journal = {Procedia Engineering},
	author = {Hager, Izabela and Golonka, Anna and Putanowicz, Roman},
	month = jan,
	year = {2016},
	keywords = {3D printing, building materials, concrete, contour crafting, sustainable construction},
	pages = {292--299},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/F786WQAS/S1877705816317453.html:text/html},
}

@article{mohamed_optimization_2015,
	title = {Optimization of fused deposition modeling process parameters: a review of current research and future prospects},
	volume = {3},
	issn = {2195-3597},
	shorttitle = {Optimization of fused deposition modeling process parameters},
	url = {https://doi.org/10.1007/s40436-014-0097-7},
	doi = {10.1007/s40436-014-0097-7},
	abstract = {Fused deposition modeling (FDM) is one of the most popular additive manufacturing technologies for various engineering applications. FDM process has been introduced commercially in early 1990s by Stratasys Inc., USA. The quality of FDM processed parts mainly depends on careful selection of process variables. Thus, identification of the FDM process parameters that significantly affect the quality of FDM processed parts is important. In recent years, researchers have explored a number of ways to improve the mechanical properties and part quality using various experimental design techniques and concepts. This article aims to review the research carried out so far in determining and optimizing the process parameters of the FDM process. Several statistical designs of experiments and optimization techniques used for the determination of optimum process parameters have been examined. The trends for future FDM research in this area are described.},
	language = {en},
	number = {1},
	urldate = {2024-07-15},
	journal = {Advances in Manufacturing},
	author = {Mohamed, Omar A. and Masood, Syed H. and Bhowmik, Jahar L.},
	month = mar,
	year = {2015},
	keywords = {Additive manufacturing, Experimental design, Fused deposition modeling (FDM), Mechanical properties, Part quality, Process parameters},
	pages = {42--53},
}

@article{schuldt_systematic_2021,
	title = {A systematic review and analysis of the viability of {3D}-printed construction in remote environments},
	volume = {125},
	issn = {0926-5805},
	url = {https://www.sciencedirect.com/science/article/pii/S0926580521000935},
	doi = {10.1016/j.autcon.2021.103642},
	abstract = {3D-printed construction is an additive, layer-by-layer construction method with the potential to reduce material consumption, optimize design, decrease construction time, lower labor requirements, minimize logistical demand, improve sustainability, and reduce costs as compared to conventional construction. This paper presents the results of a systematic review of 4491 publications spanning from 1998 through 2019. The review presents the viability of 3D-printed construction as a replacement for conventional construction methods, specifically in remote, isolated, or expeditionary environments, where conventional construction capability may be limited. The paper includes an analysis and characterization of the existing body of 3D-printed construction literature before evaluating seven viability factors of the method: materials, structural design, process efficiency, logistics, labor, environmental impact, and cost. In addition, the paper highlights three case studies of 3D-printed construction in remote, isolated, and expeditionary environments. The paper concludes by suggesting areas of future research to ensure the viability of this technology, such as printing full-scale structures and components with locally sourced materials in uncontrolled environments, defining standards for 3D printing, automating additional construction processes, and performing both environmental impact and cost life-cycle analyses. With continued investment in research and development, 3D printing could become a more viable and accepted method of construction, transforming the way the industry is managed in remote, isolated, and expeditionary environments.},
	urldate = {2024-07-15},
	journal = {Automation in Construction},
	author = {Schuldt, Steven J. and Jagoda, Jeneé A. and Hoisington, Andrew J. and Delorit, Justin D.},
	month = may,
	year = {2021},
	keywords = {3D printing, 3D-printed construction, Additive construction, Remote environment, Review, Viability},
	pages = {103642},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/WKWWAJHV/S0926580521000935.html:text/html},
}

@article{xu_deformation_2019,
	title = {Deformation and fracture of {3D} printed disordered lattice materials: {Experiments} and modeling},
	volume = {162},
	issn = {0264-1275},
	shorttitle = {Deformation and fracture of {3D} printed disordered lattice materials},
	url = {https://www.sciencedirect.com/science/article/pii/S0264127518308530},
	doi = {10.1016/j.matdes.2018.11.047},
	abstract = {A method is presented to model deformation and fracture behavior of 3D printed disordered lattice materials under uniaxial tensile load. A lattice model was used to predict crack pattern and load-displacement response of the printed lattice materials. To include the influence of typical layered structures of 3D printed materials in the simulation, two types of printed elements were considered: horizontally and vertically printed elements. Strengths of these elements were measured: 3 mm cubic units consist of lattice elements with two printing directions were printed and their strengths were tested in uniaxial tension. Afterwards, the measured element strengths and bulk material strength, respectively, were used as model input. Uniaxial tensile tests were also performed on the printed lattice materials to obtain their crack pattern and load-displacement curves. Simulations and experimental results were comparatively analyzed. For both levels of disorder considered, only when measured strengths were assigned to the elements with identical printing direction, are the predicted crack patterns and load-displacement curves in agreement with experimental results. The results emphasize the importance of considering printing direction when simulating mechanical performance of 3D printed structures. The influence of disorder on lattice material mechanical properties was discussed based on the experiments and simulations.},
	urldate = {2024-07-15},
	journal = {Materials \& Design},
	author = {Xu, Yading and Zhang, Hongzhi and Šavija, Branko and Chaves Figueiredo, Stefan and Schlangen, Erik},
	month = jan,
	year = {2019},
	keywords = {3D printing, Fracture, Lattice material, Lattice model, Uniaxial tensile test},
	pages = {143--153},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/4D4WJQNX/S0264127518308530.html:text/html},
}

@article{xu_review_2022,
	title = {A review on cell damage, viability, and functionality during {3D} bioprinting},
	volume = {9},
	issn = {2054-9369},
	url = {https://doi.org/10.1186/s40779-022-00429-5},
	doi = {10.1186/s40779-022-00429-5},
	abstract = {Three-dimensional (3D) bioprinting fabricates 3D functional tissues/organs by accurately depositing the bioink composed of the biological materials and living cells. Even though 3D bioprinting techniques have experienced significant advancement over the past decades, it remains challenging for 3D bioprinting to artificially fabricate functional tissues/organs with high post-printing cell viability and functionality since cells endure various types of stress during the bioprinting process. Generally, cell viability which is affected by several factors including the stress and the environmental factors, such as pH and temperature, is mainly determined by the magnitude and duration of the stress imposed on the cells with poorer cell viability under a higher stress and a longer duration condition. The maintenance of high cell viability especially for those vulnerable cells, such as stem cells which are more sensitive to multiple stresses, is a key initial step to ensure the functionality of the artificial tissues/organs. In addition, maintaining the pluripotency of the cells such as proliferation and differentiation abilities is also essential for the 3D-bioprinted tissues/organs to be similar to native tissues/organs. This review discusses various pathways triggering cell damage and the major factors affecting cell viability during different bioprinting processes, summarizes the studies on cell viabilities and functionalities in different bioprinting processes, and presents several potential approaches to protect cells from injuries to ensure high cell viability and functionality.},
	number = {1},
	urldate = {2024-07-15},
	journal = {Military Medical Research},
	author = {Xu, He-Qi and Liu, Jia-Chen and Zhang, Zheng-Yi and Xu, Chang-Xue},
	month = dec,
	year = {2022},
	keywords = {Cell damage, Cell functionality, Cell viability, Shear stress, Three-dimensional bioprinting},
	pages = {70},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/4RAJAGAB/Xu et al. - 2022 - A review on cell damage, viability, and functional.pdf:application/pdf;Snapshot:/Users/rupertmenneer/Zotero/storage/LEVVRNTM/s40779-022-00429-5.html:text/html},
}

@article{haghiashtiani_3d_2020,
	title = {{3D} printed patient-specific aortic root models with internal sensors for minimally invasive applications},
	volume = {6},
	url = {https://www.science.org/doi/10.1126/sciadv.abb4641},
	doi = {10.1126/sciadv.abb4641},
	abstract = {Minimally invasive surgeries have numerous advantages, yet complications may arise from limited knowledge about the anatomical site targeted for the delivery of therapy. Transcatheter aortic valve replacement (TAVR) is a minimally invasive procedure for treating aortic stenosis. Here, we demonstrate multimaterial three-dimensional printing of patient-specific soft aortic root models with internally integrated electronic sensor arrays that can augment testing for TAVR preprocedural planning. We evaluated the efficacies of the models by comparing their geometric fidelities with postoperative data from patients, as well as their in vitro hemodynamic performances in cases with and without leaflet calcifications. Furthermore, we demonstrated that internal sensor arrays can facilitate the optimization of bioprosthetic valve selections and in vitro placements via mapping of the pressures applied on the critical regions of the aortic anatomies. These models may pave exciting avenues for mitigating the risks of postoperative complications and facilitating the development of next-generation medical devices.},
	number = {35},
	urldate = {2024-07-15},
	journal = {Science Advances},
	author = {Haghiashtiani, Ghazaleh and Qiu, Kaiyan and Zhingre Sanchez, Jorge D. and Fuenning, Zachary J. and Nair, Priya and Ahlberg, Sarah E. and Iaizzo, Paul A. and McAlpine, Michael C.},
	month = aug,
	year = {2020},
	note = {Publisher: American Association for the Advancement of Science},
	pages = {eabb4641},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/BMU6H2HR/Haghiashtiani et al. - 2020 - 3D printed patient-specific aortic root models wit.pdf:application/pdf},
}

@article{johnson_generalizable_2021,
	title = {A generalizable artificial intelligence tool for identification and correction of self-supporting structures in additive manufacturing processes},
	volume = {46},
	issn = {2214-8604},
	url = {https://www.sciencedirect.com/science/article/pii/S2214860421003535},
	doi = {10.1016/j.addma.2021.102191},
	abstract = {High mix, low volume processes such as additive manufacturing (AM) offer tremendous promise for increasing the customization in manufacturing but are hindered by the lack of efficient methods for identifying process parameters for complex new geometries exhibiting the desired performance. The search over the process space can be automated with analysis tools that can be applied in a time and resource efficient manner such that ambitious print designs are not dissuaded by the cost of process parameter discovery. In this work, we propose an image analysis tool that can classify spanning prints as one of five process-relevant archetypes, invariant of the span dimensions. We describe a modular design of the tool such that simple adjustments to image processing parameters allow for compatibility with different print processes and environments. Furthermore, we demonstrate how this tool may be incorporated into a fully automated workflow on multiple AM systems to facilitate rapid autonomous process parameter discovery and/or deeper scientific understanding.},
	urldate = {2024-07-15},
	journal = {Additive Manufacturing},
	author = {Johnson, Marshall V. and Garanger, Kevin and Hardin, James O. and Berrigan, J. Daniel and Feron, Eric and Kalidindi, Surya R.},
	month = oct,
	year = {2021},
	keywords = {Machine learning, Closed loop automation, Direct write, Parameter search, Self-supporting structures},
	pages = {102191},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/IN254DSQ/S2214860421003535.html:text/html},
}

@article{gardner_machines_2019,
	title = {Machines as {Craftsmen}: {Localized} {Parameter} {Setting} {Optimization} for {Fused} {Filament} {Fabrication} {3D} {Printing}},
	volume = {4},
	copyright = {© 2019 WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {2365-709X},
	shorttitle = {Machines as {Craftsmen}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/admt.201800653},
	doi = {10.1002/admt.201800653},
	abstract = {Quality control and repeatability of 3D printing must be enhanced to fully unlock its utility beyond prototyping and noncritical applications. Machine learning is a potential solution to improving 3D printing performance and is explored for areas including flaw identification and property prediction. However, critical problems must be resolved before machine learning can truly enable 3D printing to reach its potential, including the very large data sets required for training and the inherently local nature of 3D printing where the optimum parameter settings vary throughout the part. This work outlines an end-to-end tool for integrating machine learning into the 3D printing process. The tool selects the ideal parameter settings at each location, taking into consideration factors such as geometry, hardware and material response times, and operator priorities. The tool demonstrates its usefulness by correcting for visual flaws common in fused filament fabrication parts. An image recognition neural network classifies local flaws in parts to create training data. A gradient boosting classifier then predicts the local flaws in future parts, based on location, geometry, and parameter settings. The tool selects optimum parameter settings based on the aforementioned factors. The resulting prints show increased quality over prints that use global parameters only.},
	language = {en},
	number = {3},
	urldate = {2024-07-15},
	journal = {Advanced Materials Technologies},
	author = {Gardner, John M. and Hunt, Kevin A. and Ebel, Austin B. and Rose, Evan S. and Zylich, Sean C. and Jensen, Benjamin D. and Wise, Kristopher E. and Siochi, Emilie J. and Sauti, Godfrey},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/admt.201800653},
	keywords = {3D printing, additive manufacturing, machine learning, artificial intelligence, process automation},
	pages = {1800653},
	file = {Snapshot:/Users/rupertmenneer/Zotero/storage/8Z2PLQBZ/admt.html:text/html},
}

@article{baechle-clayton_failures_2022,
	title = {Failures and {Flaws} in {Fused} {Deposition} {Modeling} ({FDM}) {Additively} {Manufactured} {Polymers} and {Composites}},
	volume = {6},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2504-477X},
	url = {https://www.mdpi.com/2504-477X/6/7/202},
	doi = {10.3390/jcs6070202},
	abstract = {In this review, the potential failures and flaws associated with fused deposition modeling (FDM) or fused filament fabrication (FFF) 3D printing technology are highlighted. The focus of this article is on presenting the failures and flaws that are caused by the operational standpoints and which are based on the many years of experience with current and emerging materials and equipment for the 3D printing of polymers and composites using the FDM/FFF method. FDM or FFF 3D printing, which is also known as an additive manufacturing (AM) technique, is a material processing and fabrication method where the raw material, usually in the form of filaments, is added layer-by-layer to create a three-dimensional part from a computer designed model. As expected, there are many advantages in terms of material usage, fabrication time, the complexity of the part, and the ease of use in FDM/FFF, which are extensively discussed in many articles. However, to upgrade the application of this technology from public general usage and prototyping to large-scale production use, as well as to be certain about the integrity of the parts even in a prototype, the quality and structural properties of the products become a big concern. This study provides discussions and insights into the potential factors that can cause the failure of 3D printers when producing a part and presents the type and characteristics of potential flaws that can happen in the produced parts. Common defects posed by FDM printing have been discussed, and common nondestructive detection methods to identify these flaws both in-process and after the process is completed are discussed. The discussions on the failures and flaws in machines provides useful information on troubleshooting the process if they happen, and the review on the failures and flaws in parts helps researchers and operators learn about the causes and effects of the flaws in a practical way.},
	language = {en},
	number = {7},
	urldate = {2024-07-17},
	journal = {Journal of Composites Science},
	author = {Baechle-Clayton, Maggie and Loos, Elizabeth and Taheri, Mohammad and Taheri, Hossein},
	month = jul,
	year = {2022},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D printing, acrylonitrile butadiene styrene (ABS) filament, additive manufacturing (AM), fused deposition modeling (FDM), fused filament fabrication (FFF), nondestructive testing (NDT), polylactic acid (PLA) filament},
	pages = {202},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/9NVDZJWA/Baechle-Clayton et al. - 2022 - Failures and Flaws in Fused Deposition Modeling (F.pdf:application/pdf},
}

@article{jin_automated_2020,
	title = {Automated {Real}-{Time} {Detection} and {Prediction} of {Interlayer} {Imperfections} in {Additive} {Manufacturing} {Processes} {Using} {Artificial} {Intelligence}},
	volume = {2},
	copyright = {© 2019 The Authors. Published by WILEY-VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {2640-4567},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aisy.201900130},
	doi = {10.1002/aisy.201900130},
	abstract = {Although fused deposition modeling (FDM) additive manufacturing technologies have advanced in the past decade, interlayer imperfections such as delamination and warping are still dominant when printing complex parts. Herein, a self-monitoring system based on real-time camera images and deep learning algorithms is developed to classify the various extents of delamination in a printed part. In addition, a novel method incorporating strain measurements is established to measure and predict the onset of warping. Results show that the machine-learning model is capable of detecting different levels of delamination conditions, and the strain measurements setup successfully reflects and determines the extent and tendency of warping before it actually occurs in the print job. This multifunctional system can be applied to assess other manufacturing processes to realize autocalibration and prediagnosis of imperfections without human interaction.},
	language = {en},
	number = {1},
	urldate = {2024-07-17},
	journal = {Advanced Intelligent Systems},
	author = {Jin, Zeqing and Zhang, Zhizhou and Gu, Grace X.},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/aisy.201900130},
	keywords = {additive manufacturing, computer vision, machine learning, convolutional neural networks, fused deposition modeling},
	pages = {1900130},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/FGHTN58D/Jin et al. - 2020 - Automated Real-Time Detection and Prediction of In.pdf:application/pdf;Snapshot:/Users/rupertmenneer/Zotero/storage/K8W8EEH6/aisy.html:text/html},
}

@article{straub_initial_2015,
	title = {Initial {Work} on the {Characterization} of {Additive} {Manufacturing} ({3D} {Printing}) {Using} {Software} {Image} {Analysis}},
	volume = {3},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2075-1702},
	url = {https://www.mdpi.com/2075-1702/3/2/55},
	doi = {10.3390/machines3020055},
	abstract = {A current challenge in additive manufacturing (commonly known as 3D printing) is the detection of defects. Detection of defects (or the lack thereof) in bespoke industrial manufacturing may be safety critical and reduce or eliminate the need for testing of printed objects. In consumer and prototype printing, early defect detection may facilitate the printer taking corrective measures (or pausing printing and alerting a user), preventing the need to re-print objects after the compounding of a small error occurs. This paper considers one approach to defect detection. It characterizes the efficacy of using a multi-camera system and image processing software to assess printing progress (thus detecting completion failure defects) and quality. The potential applications and extrapolations of this type of a system are also discussed.},
	language = {en},
	number = {2},
	urldate = {2024-07-17},
	journal = {Machines},
	author = {Straub, Jeremy},
	month = jun,
	year = {2015},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {3D printing, 3D printer, image analysis, image software, object analysis},
	pages = {55--71},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/6NTHG7DK/Straub - 2015 - Initial Work on the Characterization of Additive M.pdf:application/pdf},
}

@article{cunha_situ_2021,
	title = {In {Situ} {Monitoring} of {Additive} {Manufacturing} {Using} {Digital} {Image} {Correlation}: {A} {Review}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {1996-1944},
	shorttitle = {In {Situ} {Monitoring} of {Additive} {Manufacturing} {Using} {Digital} {Image} {Correlation}},
	url = {https://www.mdpi.com/1996-1944/14/6/1511},
	doi = {10.3390/ma14061511},
	abstract = {This paper is a critical review of in situ full-field measurements provided by digital image correlation (DIC) for inspecting and enhancing additive manufacturing (AM) processes. The principle of DIC is firstly recalled and its applicability during different AM processes systematically addressed. Relevant customisations of DIC in AM processes are highlighted regarding optical system, lighting and speckled pattern procedures. A perspective is given in view of the impact of in situ monitoring regarding AM processes based on target subjects concerning defect characterisation, evaluation of residual stresses, geometric distortions, strain measurements, numerical modelling validation and material characterisation. Finally, a case study on in situ measurements with DIC for wire and arc additive manufacturing (WAAM) is presented emphasizing opportunities, challenges and solutions.},
	language = {en},
	number = {6},
	urldate = {2024-07-17},
	journal = {Materials},
	author = {Cunha, Filipa G. and Santos, Telmo G. and Xavier, José},
	month = jan,
	year = {2021},
	note = {Number: 6
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {additive manufacturing, digital image correlation, in situ, monitoring},
	pages = {1511},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/33NB42RS/Cunha et al. - 2021 - In Situ Monitoring of Additive Manufacturing Using.pdf:application/pdf},
}

@misc{mildenhall_nerf_2020,
	title = {{NeRF}: {Representing} {Scenes} as {Neural} {Radiance} {Fields} for {View} {Synthesis}},
	shorttitle = {{NeRF}},
	url = {http://arxiv.org/abs/2003.08934},
	doi = {10.48550/arXiv.2003.08934},
	abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$({\textbackslash}theta, {\textbackslash}phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
	urldate = {2024-07-17},
	publisher = {arXiv},
	author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
	month = aug,
	year = {2020},
	note = {arXiv:2003.08934 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics},
	annote = {Comment: ECCV 2020 (oral). Project page with videos and code: http://tancik.com/nerf},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FWKVZKZY/Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/WWIDGRIL/2003.html:text/html},
}

@article{lowe_distinctive_2004,
	title = {Distinctive {Image} {Features} from {Scale}-{Invariant} {Keypoints}},
	volume = {60},
	issn = {0920-5691},
	url = {http://link.springer.com/10.1023/B:VISI.0000029664.99615.94},
	doi = {10.1023/B:VISI.0000029664.99615.94},
	abstract = {This paper presents a method for extracting distinctive invariant features from images that can be used to perform reliable matching between different views of an object or scene. The features are invariant to image scale and rotation, and are shown to provide robust matching across a a substantial range of afﬁne distortion, change in 3D viewpoint, addition of noise, and change in illumination. The features are highly distinctive, in the sense that a single feature can be correctly matched with high probability against a large database of features from many images. This paper also describes an approach to using these features for object recognition. The recognition proceeds by matching individual features to a database of features from known objects using a fast nearest-neighbor algorithm, followed by a Hough transform to identify clusters belonging to a single object, and ﬁnally performing veriﬁcation through least-squares solution for consistent pose parameters. This approach to recognition can robustly identify objects among clutter and occlusion while achieving near real-time performance.},
	language = {en},
	number = {2},
	urldate = {2024-07-17},
	journal = {International Journal of Computer Vision},
	author = {Lowe, David G.},
	month = nov,
	year = {2004},
	pages = {91--110},
	file = {Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:/Users/rupertmenneer/Zotero/storage/RDBL6SD5/Lowe - 2004 - Distinctive Image Features from Scale-Invariant Ke.pdf:application/pdf},
}

@article{anhua_investigation_2012,
	title = {Investigation on reasons inducing error and measures improving accuracy in fused deposition modeling},
	volume = {4},
	doi = {10.4156/AISS.vol4.issue5.18},
	abstract = {Fused deposition modeling (FDM) is gaining distinct advantage in manufacturing industries because of its ability to manufacture part with complex shapes without any tooling requirement and human interface. But it is difficult to achieve higher precision part, improving part quality in FDM is one of the important concern. The part errors during in FDM are classified as dimension error, shaped error and roughness of surface. The shaped errors have many forms in fused deposition modeling (FDM), largely including war page deformation, stair-stepping effect, and so on. There are several reasons for the errors, including principle, process and equipment. The reasons inducing these errors are analyzed, and the measures improving part accuracy are proposed.},
	journal = {Advances in Information Sciences and Service Sciences},
	author = {Anhua, P. and Xingming, X.},
	month = mar,
	year = {2012},
	pages = {149--157},
}

@article{hyvarinen_estimation_nodate,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very diﬃcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data.},
	language = {en},
	author = {Hyvarinen, Aapo},
	file = {Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:/Users/rupertmenneer/Zotero/storage/DCIGB2TC/Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:application/pdf},
}

@article{hyvarinen_estimation_nodate-1,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difﬁcult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simpliﬁes to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete ﬁlter set for natural image data.},
	language = {en},
	author = {Hyvarinen, Aapo},
	file = {Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:/Users/rupertmenneer/Zotero/storage/WXGKR64R/Hyvarinen - Estimation of Non-Normalized Statistical Models by.pdf:application/pdf},
}

@article{hyvarinen_estimation_2005,
	title = {Estimation of {Non}-{Normalized} {Statistical} {Models} by {Score} {Matching}},
	volume = {6},
	issn = {1532-4435},
	abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
	journal = {J. Mach. Learn. Res.},
	author = {Hyvärinen, Aapo},
	month = dec,
	year = {2005},
	pages = {695--709},
}

@misc{bronstein_geometric_2021,
	title = {Geometric {Deep} {Learning}: {Grids}, {Groups}, {Graphs}, {Geodesics}, and {Gauges}},
	shorttitle = {Geometric {Deep} {Learning}},
	url = {http://arxiv.org/abs/2104.13478},
	doi = {10.48550/arXiv.2104.13478},
	abstract = {The last decade has witnessed an experimental revolution in data science and machine learning, epitomised by deep learning methods. Indeed, many high-dimensional learning tasks previously thought to be beyond reach -- such as computer vision, playing Go, or protein folding -- are in fact feasible with appropriate computational scale. Remarkably, the essence of deep learning is built from two simple algorithmic principles: first, the notion of representation or feature learning, whereby adapted, often hierarchical, features capture the appropriate notion of regularity for each task, and second, learning by local gradient-descent type methods, typically implemented as backpropagation. While learning generic functions in high dimensions is a cursed estimation problem, most tasks of interest are not generic, and come with essential pre-defined regularities arising from the underlying low-dimensionality and structure of the physical world. This text is concerned with exposing these regularities through unified geometric principles that can be applied throughout a wide spectrum of applications. Such a 'geometric unification' endeavour, in the spirit of Felix Klein's Erlangen Program, serves a dual purpose: on one hand, it provides a common mathematical framework to study the most successful neural network architectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand, it gives a constructive procedure to incorporate prior physical knowledge into neural architectures and provide principled way to build future architectures yet to be invented.},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Bronstein, Michael M. and Bruna, Joan and Cohen, Taco and Veličković, Petar},
	month = may,
	year = {2021},
	note = {arXiv:2104.13478 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Computational Geometry},
	annote = {Comment: 156 pages. Work in progress -- comments welcome!},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/5Z2K4HGP/Bronstein et al. - 2021 - Geometric Deep Learning Grids, Groups, Graphs, Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/PMZIV4B9/2104.html:text/html},
}

@misc{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	doi = {10.48550/arXiv.2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, finding that the latter are computationally more efficient and produce higher-quality samples.},
	urldate = {2024-07-19},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/7DAKVTPX/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation wit.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/3BLY598G/2204.html:text/html},
}

@misc{yang_diffusion-based_2022,
	title = {Diffusion-{Based} {Scene} {Graph} to {Image} {Generation} with {Masked} {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2211.11138},
	doi = {10.48550/arXiv.2211.11138},
	abstract = {Generating images from graph-structured inputs, such as scene graphs, is uniquely challenging due to the difficulty of aligning nodes and connections in graphs with objects and their relations in images. Most existing methods address this challenge by using scene layouts, which are image-like representations of scene graphs designed to capture the coarse structures of scene images. Because scene layouts are manually crafted, the alignment with images may not be fully optimized, causing suboptimal compliance between the generated images and the original scene graphs. To tackle this issue, we propose to learn scene graph embeddings by directly optimizing their alignment with images. Specifically, we pre-train an encoder to extract both global and local information from scene graphs that are predictive of the corresponding images, relying on two loss functions: masked autoencoding loss and contrastive loss. The former trains embeddings by reconstructing randomly masked image regions, while the latter trains embeddings to discriminate between compliant and non-compliant images according to the scene graph. Given these embeddings, we build a latent diffusion model to generate images from scene graphs. The resulting method, called SGDiff, allows for the semantic manipulation of generated images by modifying scene graph nodes and connections. On the Visual Genome and COCO-Stuff datasets, we demonstrate that SGDiff outperforms state-of-the-art methods, as measured by both the Inception Score and Fr{\textbackslash}'echet Inception Distance (FID) metrics. We will release our source code and trained models at https://github.com/YangLing0818/SGDiff.},
	urldate = {2024-07-21},
	publisher = {arXiv},
	author = {Yang, Ling and Huang, Zhilin and Song, Yang and Hong, Shenda and Li, Guohao and Zhang, Wentao and Cui, Bin and Ghanem, Bernard and Yang, Ming-Hsuan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11138 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code and models shall be released at https://github.com/YangLing0818/SGDiff},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/8A8QI2F7/Yang et al. - 2022 - Diffusion-Based Scene Graph to Image Generation wi.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/RXLS8TH5/2211.html:text/html},
}

@misc{farshad_scenegenie_2023,
	title = {{SceneGenie}: {Scene} {Graph} {Guided} {Diffusion} {Models} for {Image} {Synthesis}},
	shorttitle = {{SceneGenie}},
	url = {http://arxiv.org/abs/2304.14573},
	doi = {10.48550/arXiv.2304.14573},
	abstract = {Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging. To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieves state-of-the-art performance on two public benchmarks for image generation from scene graphs, surpassing both scene graph to image and text-based diffusion models in various metrics. Our results demonstrate the effectiveness of incorporating bounding box and segmentation map guidance in the diffusion model sampling process for more accurate text-to-image generation.},
	urldate = {2024-07-21},
	publisher = {arXiv},
	author = {Farshad, Azade and Yeganeh, Yousef and Chi, Yu and Shen, Chengzhi and Ommer, Björn and Navab, Nassir},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14573 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/PI73F66N/Farshad et al. - 2023 - SceneGenie Scene Graph Guided Diffusion Models fo.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CWVD8P6Q/2304.html:text/html},
}

@misc{fey_fast_2019,
	title = {Fast {Graph} {Representation} {Learning} with {PyTorch} {Geometric}},
	copyright = {MIT},
	url = {https://github.com/pyg-team/pytorch_geometric},
	abstract = {Graph Neural Network Library for PyTorch},
	urldate = {2024-07-24},
	author = {Fey, Matthias and Lenssen, Jan Eric},
	month = may,
	year = {2019},
	note = {original-date: 2017-10-06T16:03:03Z},
}

@article{hunter_matplotlib_2007,
	title = {Matplotlib: {A} {2D} {Graphics} {Environment}},
	volume = {9},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1521-9615},
	shorttitle = {Matplotlib},
	url = {http://ieeexplore.ieee.org/document/4160265/},
	doi = {10.1109/MCSE.2007.55},
	number = {3},
	urldate = {2024-07-24},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, John D.},
	year = {2007},
	pages = {90--95},
}

@misc{paszke_pytorch_2019,
	title = {{PyTorch}: {An} {Imperative} {Style}, {High}-{Performance} {Deep} {Learning} {Library}},
	shorttitle = {{PyTorch}},
	url = {http://arxiv.org/abs/1912.01703},
	doi = {10.48550/arXiv.1912.01703},
	abstract = {Deep learning frameworks have often focused on either usability or speed, but not both. PyTorch is a machine learning library that shows that these two goals are in fact compatible: it provides an imperative and Pythonic programming style that supports code as a model, makes debugging easy and is consistent with other popular scientific computing libraries, while remaining efficient and supporting hardware accelerators such as GPUs. In this paper, we detail the principles that drove the implementation of PyTorch and how they are reflected in its architecture. We emphasize that every aspect of PyTorch is a regular Python program under the full control of its user. We also explain how the careful and pragmatic implementation of the key components of its runtime enables them to work together to achieve compelling performance. We demonstrate the efficiency of individual subsystems, as well as the overall speed of PyTorch on several common benchmarks.},
	urldate = {2024-07-24},
	publisher = {arXiv},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Köpf, Andreas and Yang, Edward and DeVito, Zach and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	month = dec,
	year = {2019},
	note = {arXiv:1912.01703 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Mathematical Software},
	annote = {Comment: 12 pages, 3 figures, NeurIPS 2019},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/34XNWZ7D/Paszke et al. - 2019 - PyTorch An Imperative Style, High-Performance Dee.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6A4BQ8QG/1912.html:text/html},
}

@article{evangelidis_parametric_2008,
	title = {Parametric {Image} {Alignment} {Using} {Enhanced} {Correlation} {Coefficient} {Maximization}},
	volume = {30},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0162-8828},
	url = {http://ieeexplore.ieee.org/document/4515873/},
	doi = {10.1109/TPAMI.2008.113},
	abstract = {In this work we propose the use of a modiﬁed version of the correlation coefﬁcient as a performance criterion for the image alignment problem. The proposed modiﬁcation has the desirable characteristic of being invariant with respect to photometric distortions. Since the resulting similarity measure is a nonlinear function of the warp parameters, we develop two iterative schemes for its maximization, one based on the forward additive approach and the second on the inverse compositional method. As it is customary in iterative optimization, in each iteration the nonlinear objective function is approximated by an alternative expression for which the corresponding optimization is simple. In our case we propose an efﬁcient approximation that leads to a closed form solution (per iteration) which is of low computational complexity, the latter property being particularly strong in our inverse version. The proposed schemes are tested against the Forward Additive Lucas-Kanade and the Simultaneous Inverse Compositional algorithm through simulations. Under noisy conditions and photometric distortions our forward version achieves more accurate alignments and exhibits faster convergence whereas our inverse version has similar performance as the Simultaneous Inverse Compositional algorithm but at a lower computational complexity.},
	language = {en},
	number = {10},
	urldate = {2024-07-31},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Evangelidis, G.D. and Psarakis, E.Z.},
	month = oct,
	year = {2008},
	pages = {1858--1865},
	file = {Evangelidis and Psarakis - 2008 - Parametric Image Alignment Using Enhanced Correlat.pdf:/Users/rupertmenneer/Zotero/storage/QLQW745P/Evangelidis and Psarakis - 2008 - Parametric Image Alignment Using Enhanced Correlat.pdf:application/pdf},
}

@incollection{fischler_random_1987,
	address = {San Francisco (CA)},
	title = {Random {Sample} {Consensus}: {A} {Paradigm} for {Model} {Fitting} with {Applications} to {Image} {Analysis} and {Automated} {Cartography}},
	isbn = {978-0-08-051581-6},
	shorttitle = {Random {Sample} {Consensus}},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080515816500702},
	abstract = {A new paradigm, Random Sample Consensus (RANSAC), for fitting a model to experimental data is introduced, RANSAC is capable of interpreting/ smoothing data containing a significant percentage of gross errors, and is thus ideally suited for applications in automated image analysis where interpretation is based on the data provided by error-prone feature detectors. A major portion of this paper describes the application of RANSAC to the Location Determination Problem (LDP): Given an image depicting a set of landmarks with known locations, determine that point in space from which the image was obtained. In response to a RANSAC requirement, new results are derived on the minimum number of landmarks needed to obtain a solution, and algorithms are presented for computing these minimum-landmark solutions in closed form. These results provide the basis for an automatic system that can solve the LDP under difficult viewing and analysis conditions. Implementation details and computational examples are also presented.},
	urldate = {2024-07-31},
	booktitle = {Readings in {Computer} {Vision}},
	publisher = {Morgan Kaufmann},
	author = {Fischler, Martin A. and Bolles, Robert C.},
	editor = {Fischler, Martin A. and Firschein, Oscar},
	month = jan,
	year = {1987},
	doi = {10.1016/B978-0-08-051581-6.50070-2},
	pages = {726--740},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/NIHTH9V7/B9780080515816500702.html:text/html},
}

@article{zhu_rapid_2022,
	title = {Rapid residual stress prediction and feedback control during fused deposition modeling of {PLA}},
	volume = {118},
	issn = {1433-3015},
	url = {https://doi.org/10.1007/s00170-021-08158-0},
	doi = {10.1007/s00170-021-08158-0},
	abstract = {Residual stress plays a key role in the mechanical properties and geometry stability of the printing parts during Fused Deposition Modeling (FDM). Being the representative thermoplastic in this type of manufacturing process, the process parameters of polylactic acid (PLA) in FDM have a significant impact on the residual stress of PLA. According to the multiphysics model established, the residual stress decreases with increasing layer thickness, printing speed or platform temperature. Because such traditional finite element analysis takes a long time to calculate the stress, a back propagation (BP) neural network model is established for rapid stress prediction under different processing parameters during FDM. Only 0.56 s is required with such model during each run and the prediction error is controlled in 10\%. A close loop system is simulated with temperature modification to mimic an ideal real-time feedback. Meanwhile, by off-line adjusting the FDM process parameters, the residual stress along X direction can be controlled to a certain range from experiments. An intelligent additive manufacturing system can be envisaged with the possibility of stress state modification at any time and any position in the future.},
	language = {en},
	number = {9},
	urldate = {2024-08-03},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Zhu, Qi and Yu, Kang and Li, Hanqiao and Zhang, Qingqing and Tu, Dawei},
	month = feb,
	year = {2022},
	keywords = {BP neural networks, Feedback control, Fused deposition modeling, Multiphysics simulation, Residual stresses},
	pages = {3229--3240},
}

@article{zhu_rapid_2022-1,
	title = {Rapid residual stress prediction and feedback control during fused deposition modeling of {PLA}},
	volume = {118},
	issn = {1433-3015},
	url = {https://doi.org/10.1007/s00170-021-08158-0},
	doi = {10.1007/s00170-021-08158-0},
	abstract = {Residual stress plays a key role in the mechanical properties and geometry stability of the printing parts during Fused Deposition Modeling (FDM). Being the representative thermoplastic in this type of manufacturing process, the process parameters of polylactic acid (PLA) in FDM have a significant impact on the residual stress of PLA. According to the multiphysics model established, the residual stress decreases with increasing layer thickness, printing speed or platform temperature. Because such traditional finite element analysis takes a long time to calculate the stress, a back propagation (BP) neural network model is established for rapid stress prediction under different processing parameters during FDM. Only 0.56 s is required with such model during each run and the prediction error is controlled in 10\%. A close loop system is simulated with temperature modification to mimic an ideal real-time feedback. Meanwhile, by off-line adjusting the FDM process parameters, the residual stress along X direction can be controlled to a certain range from experiments. An intelligent additive manufacturing system can be envisaged with the possibility of stress state modification at any time and any position in the future.},
	language = {en},
	number = {9},
	urldate = {2024-08-03},
	journal = {The International Journal of Advanced Manufacturing Technology},
	author = {Zhu, Qi and Yu, Kang and Li, Hanqiao and Zhang, Qingqing and Tu, Dawei},
	month = feb,
	year = {2022},
	keywords = {BP neural networks, Feedback control, Fused deposition modeling, Multiphysics simulation, Residual stresses},
	pages = {3229--3240},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/BAQWH2GI/Zhu et al. - 2022 - Rapid residual stress prediction and feedback cont.pdf:application/pdf},
}

@misc{li_scalability_2024,
	title = {On the {Scalability} of {Diffusion}-based {Text}-to-{Image} {Generation}},
	url = {http://arxiv.org/abs/2404.02883},
	doi = {10.48550/arXiv.2404.02883},
	abstract = {Scaling up model and data size has been quite successful for the evolution of LLMs. However, the scaling law for the diffusion based text-to-image (T2I) models is not fully explored. It is also unclear how to efficiently scale the model for better performance at reduced cost. The different training settings and expensive training cost make a fair model comparison extremely difficult. In this work, we empirically study the scaling properties of diffusion based T2I models by performing extensive and rigours ablations on scaling both denoising backbones and training set, including training scaled UNet and Transformer variants ranging from 0.4B to 4B parameters on datasets upto 600M images. For model scaling, we find the location and amount of cross attention distinguishes the performance of existing UNet designs. And increasing the transformer blocks is more parameter-efficient for improving text-image alignment than increasing channel numbers. We then identify an efficient UNet variant, which is 45\% smaller and 28\% faster than SDXL's UNet. On the data scaling side, we show the quality and diversity of the training set matters more than simply dataset size. Increasing caption density and diversity improves text-image alignment performance and the learning efficiency. Finally, we provide scaling functions to predict the text-image alignment performance as functions of the scale of model size, compute and dataset size.},
	urldate = {2024-08-05},
	publisher = {arXiv},
	author = {Li, Hao and Zou, Yang and Wang, Ying and Majumder, Orchid and Xie, Yusheng and Manmatha, R. and Swaminathan, Ashwin and Tu, Zhuowen and Ermon, Stefano and Soatto, Stefano},
	month = apr,
	year = {2024},
	note = {arXiv:2404.02883 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: CVPR2024},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/LQ2V2647/Li et al. - 2024 - On the Scalability of Diffusion-based Text-to-Imag.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LP9KJL8S/2404.html:text/html},
}

@misc{brody_how_2022-1,
	title = {How {Attentive} are {Graph} {Attention} {Networks}?},
	url = {http://arxiv.org/abs/2105.14491},
	doi = {10.48550/arXiv.2105.14491},
	abstract = {Graph Attention Networks (GATs) are one of the most popular GNN architectures and are considered as the state-of-the-art architecture for representation learning with graphs. In GAT, every node attends to its neighbors given its own representation as the query. However, in this paper we show that GAT computes a very limited kind of attention: the ranking of the attention scores is unconditioned on the query node. We formally define this restricted kind of attention as static attention and distinguish it from a strictly more expressive dynamic attention. Because GATs use a static attention mechanism, there are simple graph problems that GAT cannot express: in a controlled problem, we show that static attention hinders GAT from even fitting the training data. To remove this limitation, we introduce a simple fix by modifying the order of operations and propose GATv2: a dynamic graph attention variant that is strictly more expressive than GAT. We perform an extensive evaluation and show that GATv2 outperforms GAT across 11 OGB and other benchmarks while we match their parametric costs. Our code is available at https://github.com/tech-srl/how\_attentive\_are\_gats . GATv2 is available as part of the PyTorch Geometric library, the Deep Graph Library, and the TensorFlow GNN library.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Brody, Shaked and Alon, Uri and Yahav, Eran},
	month = jan,
	year = {2022},
	note = {arXiv:2105.14491 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published in ICLR 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/DNUCWWKW/Brody et al. - 2022 - How Attentive are Graph Attention Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/AZJC3ZF9/2105.html:text/html},
}

@misc{xu_scene_2017,
	title = {Scene {Graph} {Generation} by {Iterative} {Message} {Passing}},
	url = {http://arxiv.org/abs/1701.02426},
	doi = {10.48550/arXiv.1701.02426},
	abstract = {Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with NYU Depth v2 dataset.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
	month = apr,
	year = {2017},
	note = {arXiv:1701.02426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/63RNQBLZ/Xu et al. - 2017 - Scene Graph Generation by Iterative Message Passin.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/PHJTMV3U/1701.html:text/html},
}

@misc{yang_graph_2018,
	title = {Graph {R}-{CNN} for {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/1808.00191},
	doi = {10.48550/arXiv.1808.00191},
	abstract = {We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	month = aug,
	year = {2018},
	note = {arXiv:1808.00191 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 16 pages, ECCV 2018 camera ready},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FCXAS7JG/Yang et al. - 2018 - Graph R-CNN for Scene Graph Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/7ZZKPC82/1808.html:text/html},
}

@misc{danel_spatial_2020,
	title = {Spatial {Graph} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1909.05310},
	doi = {10.48550/arXiv.1909.05310},
	abstract = {Graph Convolutional Networks (GCNs) have recently become the primary choice for learning from graph-structured data, superseding hash fingerprints in representing chemical compounds. However, GCNs lack the ability to take into account the ordering of node neighbors, even when there is a geometric interpretation of the graph vertices that provides an order based on their spatial positions. To remedy this issue, we propose Spatial Graph Convolutional Network (SGCN) which uses spatial features to efficiently learn from graphs that can be naturally located in space. Our contribution is threefold: we propose a GCN-inspired architecture which (i) leverages node positions, (ii) is a proper generalization of both GCNs and Convolutional Neural Networks (CNNs), (iii) benefits from augmentation which further improves the performance and assures invariance with respect to the desired properties. Empirically, SGCN outperforms state-of-the-art graph-based methods on image classification and chemical tasks.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Danel, Tomasz and Spurek, Przemysław and Tabor, Jacek and Śmieja, Marek and Struski, Łukasz and Słowik, Agnieszka and Maziarka, Łukasz},
	month = jul,
	year = {2020},
	note = {arXiv:1909.05310 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/JEYW94Y9/Danel et al. - 2020 - Spatial Graph Convolutional Networks.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CTRC98E8/1909.html:text/html},
}

@misc{hamilton_inductive_2018,
	title = {Inductive {Representation} {Learning} on {Large} {Graphs}},
	url = {http://arxiv.org/abs/1706.02216},
	doi = {10.48550/arXiv.1706.02216},
	abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general, inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
	month = sep,
	year = {2018},
	note = {arXiv:1706.02216 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Social and Information Networks},
	annote = {Comment: Published in NIPS 2017; version with full appendix and minor corrections},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/UEBQHLQH/Hamilton et al. - 2018 - Inductive Representation Learning on Large Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HAJBMLQF/1706.html:text/html},
}

@misc{wang_heterogeneous_2021,
	title = {Heterogeneous {Graph} {Attention} {Network}},
	url = {http://arxiv.org/abs/1903.07293},
	doi = {10.48550/arXiv.1903.07293},
	abstract = {Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its metapath based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Wang, Xiao and Ji, Houye and Shi, Chuan and Wang, Bai and Cui, Peng and Yu, P. and Ye, Yanfang},
	month = jan,
	year = {2021},
	note = {arXiv:1903.07293 [cs]},
	keywords = {Computer Science - Social and Information Networks},
	annote = {Comment: 10 pages},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QXZ233CH/Wang et al. - 2021 - Heterogeneous Graph Attention Network.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/GHDUAUTW/1903.html:text/html},
}

@misc{kingma_adam_2017,
	title = {Adam: {A} {Method} for {Stochastic} {Optimization}},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	doi = {10.48550/arXiv.1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv:1412.6980 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/ZDUG23LD/Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/BMBXXIIY/1412.html:text/html},
}

@misc{li_deepergcn_2020,
	title = {{DeeperGCN}: {All} {You} {Need} to {Train} {Deeper} {GCNs}},
	shorttitle = {{DeeperGCN}},
	url = {http://arxiv.org/abs/2006.07739},
	doi = {10.48550/arXiv.2006.07739},
	abstract = {Graph Convolutional Networks (GCNs) have been drawing significant attention with the power of representation learning on graphs. Unlike Convolutional Neural Networks (CNNs), which are able to take advantage of stacking very deep layers, GCNs suffer from vanishing gradient, over-smoothing and over-fitting issues when going deeper. These challenges limit the representation power of GCNs on large-scale graphs. This paper proposes DeeperGCN that is capable of successfully and reliably training very deep GCNs. We define differentiable generalized aggregation functions to unify different message aggregation operations (e.g. mean, max). We also propose a novel normalization layer namely MsgNorm and a pre-activation version of residual connections for GCNs. Extensive experiments on Open Graph Benchmark (OGB) show DeeperGCN significantly boosts performance over the state-of-the-art on the large scale graph learning tasks of node property prediction and graph property prediction. Please visit https://www.deepgcns.org for more information.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Li, Guohao and Xiong, Chenxin and Thabet, Ali and Ghanem, Bernard},
	month = jun,
	year = {2020},
	note = {arXiv:2006.07739 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: This work is still working in process. More results will be updated in the future version. Project website: https://www.deepgcns.org},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/6TFL9GST/Li et al. - 2020 - DeeperGCN All You Need to Train Deeper GCNs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/Y4RZ78J9/2006.html:text/html},
}

@misc{morris_weisfeiler_2021,
	title = {Weisfeiler and {Leman} {Go} {Neural}: {Higher}-order {Graph} {Neural} {Networks}},
	shorttitle = {Weisfeiler and {Leman} {Go} {Neural}},
	url = {http://arxiv.org/abs/1810.02244},
	doi = {10.48550/arXiv.1810.02244},
	abstract = {In recent years, graph neural networks (GNNs) have emerged as a powerful neural architecture to learn vector representations of nodes and graphs in a supervised, end-to-end fashion. Up to now, GNNs have only been evaluated empirically -- showing promising results. The following work investigates GNNs from a theoretical point of view and relates them to the \$1\$-dimensional Weisfeiler-Leman graph isomorphism heuristic (\$1\$-WL). We show that GNNs have the same expressiveness as the \$1\$-WL in terms of distinguishing non-isomorphic (sub-)graphs. Hence, both algorithms also have the same shortcomings. Based on this, we propose a generalization of GNNs, so-called \$k\$-dimensional GNNs (\$k\$-GNNs), which can take higher-order graph structures at multiple scales into account. These higher-order structures play an essential role in the characterization of social networks and molecule graphs. Our experimental evaluation confirms our theoretical findings as well as confirms that higher-order information is useful in the task of graph classification and regression.},
	urldate = {2024-08-09},
	publisher = {arXiv},
	author = {Morris, Christopher and Ritzert, Martin and Fey, Matthias and Hamilton, William L. and Lenssen, Jan Eric and Rattan, Gaurav and Grohe, Martin},
	month = nov,
	year = {2021},
	note = {arXiv:1810.02244 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: Extended version with proofs, accepted at AAAI 2019, added units of measurement of QM9 dataset into appendix, removed results from Wu et al., 2018 due to different units},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/RCU8MDUH/Morris et al. - 2021 - Weisfeiler and Leman Go Neural Higher-order Graph.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/XB7C9V2A/1810.html:text/html},
}

@misc{ha_neural_2017,
	title = {A {Neural} {Representation} of {Sketch} {Drawings}},
	url = {http://arxiv.org/abs/1704.03477},
	doi = {10.48550/arXiv.1704.03477},
	abstract = {We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.},
	urldate = {2024-08-10},
	publisher = {arXiv},
	author = {Ha, David and Eck, Douglas},
	month = may,
	year = {2017},
	note = {arXiv:1704.03477 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/VM4VGM6E/Ha and Eck - 2017 - A Neural Representation of Sketch Drawings.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/RGBTNJYI/1704.html:text/html},
}

@article{almasri_deep_2022,
	series = {32nd {CIRP} {Design} {Conference} ({CIRP} {Design} 2022) - {Design} in a changing world},
	title = {Deep {Learning} for {Additive} {Manufacturing}-driven {Topology} {Optimization}},
	volume = {109},
	issn = {2212-8271},
	url = {https://www.sciencedirect.com/science/article/pii/S2212827122007673},
	doi = {10.1016/j.procir.2022.05.317},
	abstract = {This paper investigates the potential of Deep Learning (DL) for data-driven topology optimization (TO). Unlike the rest of the literature that mainly applies DL to TO from a mechanical perspective, we developed an original approach to integrate mechanical and geometrical constraints simultaneously. Our approach takes as input the mechanical constraints (Boundary conditions, loads configuration, volume fraction) alongside the geometrical ones (total number of elements, minimum overhang, maximum length, minimum thickness) and generates a 2D design complying with these constraints. Thus, it combines the best of both mechanical (CAE) and geometrical design worlds. Conversely, geometrical design constraints are complex, not yet formalized, and contradictory between Additive Manufacturing (AM) processes, applications, and materials. Some are even descriptive, lacking a well-defined mathematical description, or are well-defined but proprietary and inaccessible. Hence, despite the synergy between AM and TO, integrating AM constraints into the TO formulation is still a hurdle. Furthermore, even when their integration is possible, TO’s convergence to a solution is compromised. On the other hand, DL has proven robust in capturing geometrical and spatial correlations. Consequently, our approach solves the previously listed setbacks by aligning DL to serve Design for AM (DfAM); there is no need to identify an analytical formula for a geometrical constraint but simply a sufficient number of examples describing it, and convergence is no longer a blockade when the DL model is trained on converged designs. Our approach tailors the design’s geometrical aspects with great flexibility and creativity. It reconciles design and manufacturing and accelerates the design life cycle of a part. Moreover, it can be easily updated to include additional constraints and can be implemented in the future into CAD software as a lighter and faster generative design module},
	urldate = {2024-08-13},
	journal = {Procedia CIRP},
	author = {Almasri, Waad and Danglade, Florence and Bettebghor, Dimitri and Adjed, Faouzi and Ababsa, Fakhreddine},
	month = jan,
	year = {2022},
	keywords = {Additive Manufacturing, Deep Learning, Generative Design, Topology Optimization},
	pages = {49--54},
	file = {Full Text:/Users/rupertmenneer/Zotero/storage/5KMZY8N7/Almasri et al. - 2022 - Deep Learning for Additive Manufacturing-driven To.pdf:application/pdf;ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/KF878EQ9/S2212827122007673.html:text/html},
}

@article{hertlein_generative_2021-1,
	title = {Generative adversarial network for early-stage design flexibility in topology optimization for additive manufacturing},
	volume = {59},
	issn = {0278-6125},
	url = {https://www.sciencedirect.com/science/article/pii/S027861252100087X},
	doi = {10.1016/j.jmsy.2021.04.007},
	abstract = {Topology optimization has become a valuable design tool for structures to be fabricated by additive manufacturing (AM). However, during early stage design, parameters are frequently evolving, resulting in multiple similar TO runs. Especially when design for manufacturing principles expand the parameter space, this iterative process is computationally burdensome, and does not take advantage of redundant information in each study. We introduce a deep learning-based framework that learns latent similarities between runs in a training set to predict near optimal designs, enabling efficient wholistic understanding of the problem setup space, which includes both loading conditions and, for the first time in this study, manufacturing process configuration. Learning was achieved using a conditional generative adversarial network (cGAN) trained on a dataset of randomized boundary conditions, loadings, and AM build orientations, and the corresponding optimal structures obtained through overhang-filtered TO. cGAN predictions showed good agreement with true optima. For even greater accuracy, predictions can be post-processed by applying a small number of TO iterations. Manifold learning techniques were used to provide further insight, and we were able to conclude that the cGAN error generally increases with distance between the load and the boundary conditions or build plate. Interestingly, in 9\% of test cases, the cGAN generated structures with compliances better than the corresponding TO-calculated structures, often by as much as 50 \% with an average of 7.8 \%. That some of these structures appeared qualitatively different in form suggests the potential value of the approach in other domains such as generative design, where a range of alternate near-optimal designs are used to guide the ideation process.},
	urldate = {2024-08-13},
	journal = {Journal of Manufacturing Systems},
	author = {Hertlein, Nathan and Buskohl, Philip R. and Gillman, Andrew and Vemaganti, Kumar and Anand, Sam},
	month = apr,
	year = {2021},
	keywords = {Additive manufacturing, Deep learning, Generative adversarial network, Topology optimization},
	pages = {675--685},
	file = {ScienceDirect Snapshot:/Users/rupertmenneer/Zotero/storage/J47MM8BJ/S027861252100087X.html:text/html},
}

@article{guo_semi-supervised_2021-1,
	title = {Semi-supervised deep learning based framework for assessing manufacturability of cellular structures in direct metal laser sintering process},
	volume = {32},
	doi = {10.1007/s10845-020-01575-0},
	abstract = {In recent years, metal cellular structures have drawn attentions in various industrial sectors due to their design freedoms and abilities to achieve multi-functional mechanical properties. However, metal cellular structures are difficult to fabricate due to their complex geometries, even with modern additive manufacturing technologies such as the direct metal laser sintering (DMLS) process. Assessing the manufacturability of metal cellular structures via a DMLS process is a challenging task as the geometric features of the structures are complex. Besides, via a DMLS process, the manufacturability also depends on the cumulative deformation of the layers during the manufacturing process. Existing methods on Design for Additive Manufacturing (DFAM) provide design guidelines that are based on past successful printed designs. However, they are not effective in predicting the manufacturability of metal cellular structures. In this paper, we propose a semi-supervised deep learning based manufacturability assessment (SSDLMA) framework to assess whether a metal cellular structure can be successfully manufactured from a given DMLS process. To enable efficient learning, we represent the complex cellular structures as 3D binary arrays with a simple yet efficient voxelisation method. We then train a deep learning based classifier using only a small amount of experimental data by adopting a semi-supervised learning approach. By running real experiments and comparing with existing DFAM methods and machine learning models, we demonstrate the advantages of the proposed SSDLMA framework. The proposed framework can be extended to predict the manufacturability of various other complex geometries beyond cellular structure in a reliable way even with a small number of training data.},
	journal = {Journal of Intelligent Manufacturing},
	author = {Guo, Yilin and Lu, Wen-Feng and Fuh, J.},
	month = feb,
	year = {2021},
	file = {Full Text PDF:/Users/rupertmenneer/Zotero/storage/YSFZLKVP/Guo et al. - 2021 - Semi-supervised deep learning based framework for .pdf:application/pdf},
}

@misc{simonyan_very_2015,
	title = {Very {Deep} {Convolutional} {Networks} for {Large}-{Scale} {Image} {Recognition}},
	url = {http://arxiv.org/abs/1409.1556},
	doi = {10.48550/arXiv.1409.1556},
	abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
	urldate = {2024-08-14},
	publisher = {arXiv},
	author = {Simonyan, Karen and Zisserman, Andrew},
	month = apr,
	year = {2015},
	note = {arXiv:1409.1556 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/G7G8HBQE/Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/S9JRH5FY/1409.html:text/html},
}

@misc{johnson_image_2018,
	title = {Image {Generation} from {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1804.01622},
	doi = {10.48550/arXiv.1804.01622},
	abstract = {To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.},
	urldate = {2024-08-31},
	publisher = {arXiv},
	author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
	month = apr,
	year = {2018},
	note = {arXiv:1804.01622 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: To appear at CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/VPJCYR3V/Johnson et al. - 2018 - Image Generation from Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/8U5BYNIX/1804.html:text/html},
}

@article{tian_image_nodate,
	title = {Image {Processing} {GNN}: {Breaking} {Rigidity} in {Super}-{Resolution}},
	abstract = {Super-Resolution (SR) reconstructs high-resolution images from low-resolution ones. CNNs and window-attention methods are two major categories of canonical SR models. However, these measures are rigid: in both operations, each pixel gathers the same number of neighboring pixels, hindering their effectiveness in SR tasks. Alternatively, we leverage the ﬂexibility of graphs and propose the Image Processing GNN (IPG) model to break the rigidity that dominates previous SR methods. Firstly, SR is unbalanced in that most reconstruction efforts are concentrated to a small proportion of detail-rich image parts. Hence, we leverage degree ﬂexibility by assigning higher node degrees to detail-rich image nodes. Then in order to construct graphs for SR-effective aggregation, we treat images as pixel node sets rather than patch nodes. Lastly, we hold that both local and global information are crucial for SR performance. In the hope of gathering pixel information from both local and global scales efﬁciently via ﬂexible graphs, we search node connections within nearby regions to construct local graphs; and ﬁnd connections within a strided sampling space of the whole image for global graphs. The ﬂexibility of graphs boosts the SR performance of the IPG model. Experiment results on various datasets demonstrates that the proposed IPG outperforms State-ofthe-Art baselines. Codes are available at this link.},
	language = {en},
	author = {Tian, Yuchuan and Chen, Hanting and Xu, Chao and Wang, Yunhe},
	file = {Tian et al. - Image Processing GNN Breaking Rigidity in Super-R.pdf:/Users/rupertmenneer/Zotero/storage/DSQRK35L/Tian et al. - Image Processing GNN Breaking Rigidity in Super-R.pdf:application/pdf},
}

@misc{karras_analyzing_2024,
	title = {Analyzing and {Improving} the {Training} {Dynamics} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2312.02696},
	doi = {10.48550/arXiv.2312.02696},
	abstract = {Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling. As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.},
	urldate = {2024-09-01},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Lehtinen, Jaakko and Hellsten, Janne and Aila, Timo and Laine, Samuli},
	month = mar,
	year = {2024},
	note = {arXiv:2312.02696 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QYY3L4C9/Karras et al. - 2024 - Analyzing and Improving the Training Dynamics of D.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/WECXB7UR/2312.html:text/html},
}

@misc{zheng_layoutdiffusion_2024,
	title = {{LayoutDiffusion}: {Controllable} {Diffusion} {Model} for {Layout}-to-image {Generation}},
	shorttitle = {{LayoutDiffusion}},
	url = {http://arxiv.org/abs/2303.17189},
	doi = {10.48550/arXiv.2303.17189},
	abstract = {Recently, diffusion models have achieved great success in image synthesis. However, when it comes to the layout-to-image generation where an image often has a complex scene of multiple objects, how to make strong control over both the global layout map and each detailed object remains a challenging task. In this paper, we propose a diffusion model named LayoutDiffusion that can obtain higher generation quality and greater controllability than the previous works. To overcome the difficult multimodal fusion of image and layout, we propose to construct a structural image patch with region information and transform the patched image into a special layout to fuse with the normal layout in a unified form. Moreover, Layout Fusion Module (LFM) and Object-aware Cross Attention (OaCA) are proposed to model the relationship among multiple objects and designed to be object-aware and position-sensitive, allowing for precisely controlling the spatial related information. Extensive experiments show that our LayoutDiffusion outperforms the previous SOTA methods on FID, CAS by relatively 46.35\%, 26.70\% on COCO-stuff and 44.29\%, 41.82\% on VG. Code is available at https://github.com/ZGCTroy/LayoutDiffusion.},
	urldate = {2024-09-01},
	publisher = {arXiv},
	author = {Zheng, Guangcong and Zhou, Xianpan and Li, Xuewei and Qi, Zhongang and Shan, Ying and Li, Xi},
	month = mar,
	year = {2024},
	note = {arXiv:2303.17189 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accepted by CVPR2023},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/Q54V8DUB/Zheng et al. - 2024 - LayoutDiffusion Controllable Diffusion Model for .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6W5JCMMQ/2303.html:text/html},
}

@misc{chen_geodiffusion_2024,
	title = {{GeoDiffusion}: {Text}-{Prompted} {Geometric} {Control} for {Object} {Detection} {Data} {Generation}},
	shorttitle = {{GeoDiffusion}},
	url = {http://arxiv.org/abs/2306.04607},
	doi = {10.48550/arXiv.2306.04607},
	abstract = {Diffusion models have attracted significant attention due to the remarkable ability to create content and generate data for tasks like image classification. However, the usage of diffusion models to generate the high-quality object detection data remains an underexplored area, where not only image-level perceptual quality but also geometric conditions such as bounding boxes and camera views are essential. Previous studies have utilized either copy-paste synthesis or layout-to-image (L2I) generation with specifically designed modules to encode the semantic layouts. In this paper, we propose the GeoDiffusion, a simple framework that can flexibly translate various geometric conditions into text prompts and empower pre-trained text-to-image (T2I) diffusion models for high-quality detection data generation. Unlike previous L2I methods, our GeoDiffusion is able to encode not only the bounding boxes but also extra geometric conditions such as camera views in self-driving scenes. Extensive experiments demonstrate GeoDiffusion outperforms previous L2I methods while maintaining 4x training time faster. To the best of our knowledge, this is the first work to adopt diffusion models for layout-to-image generation with geometric conditions and demonstrate that L2I-generated images can be beneficial for improving the performance of object detectors.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Chen, Kai and Xie, Enze and Chen, Zhe and Wang, Yibo and Hong, Lanqing and Li, Zhenguo and Yeung, Dit-Yan},
	month = feb,
	year = {2024},
	note = {arXiv:2306.04607 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Accept by ICLR 2024. Project Page: https://kaichen1998.github.io/projects/geodiffusion/},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/W6GQM3DM/Chen et al. - 2024 - GeoDiffusion Text-Prompted Geometric Control for .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/E54FUUJE/2306.html:text/html},
}

@misc{li_controlnet_2024,
	title = {{ControlNet}++: {Improving} {Conditional} {Controls} with {Efficient} {Consistency} {Feedback}},
	shorttitle = {{ControlNet}++},
	url = {http://arxiv.org/abs/2404.07987},
	doi = {10.48550/arXiv.2404.07987},
	abstract = {To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 11.1\% mIoU, 13.4\% SSIM, and 7.6\% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Li, Ming and Yang, Taojiannan and Kuang, Huafeng and Wu, Jie and Wang, Zhaoning and Xiao, Xuefeng and Chen, Chen},
	month = jul,
	year = {2024},
	note = {arXiv:2404.07987 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Camera Ready Version. Project Page: https://liming-ai.github.io/ControlNet\_Plus\_Plus; Code \& Data: https://github.com/liming-ai/ControlNet\_Plus\_Plus},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/LC5UT99S/Li et al. - 2024 - ControlNet++ Improving Conditional Controls with .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CHDLFYZ5/2404.html:text/html},
}

@misc{dhamo_semantic_2020,
	title = {Semantic {Image} {Manipulation} {Using} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2004.03677},
	doi = {10.48550/arXiv.2004.03677},
	abstract = {Image manipulation can be considered a special case of image generation where the image to be produced is a modification of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatio-semantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Dhamo, Helisa and Farshad, Azade and Laina, Iro and Navab, Nassir and Hager, Gregory D. and Tombari, Federico and Rupprecht, Christian},
	month = apr,
	year = {2020},
	note = {arXiv:2004.03677 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/K2SR7QEQ/Dhamo et al. - 2020 - Semantic Image Manipulation Using Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LWD34PBA/2004.html:text/html},
}

@misc{ashual_specifying_2019,
	title = {Specifying {Object} {Attributes} and {Relations} in {Interactive} {Scene} {Generation}},
	url = {http://arxiv.org/abs/1909.05379},
	doi = {10.48550/arXiv.1909.05379},
	abstract = {We introduce a method for the generation of images from an input scene graph. The method separates between a layout embedding and an appearance embedding. The dual embedding leads to generated images that better match the scene graph, have higher visual quality, and support more complex scene graphs. In addition, the embedding scheme supports multiple and diverse output images per scene graph, which can be further controlled by the user. We demonstrate two modes of per-object control: (i) importing elements from other images, and (ii) navigation in the object space, by selecting an appearance archetype. Our code is publicly available at https://www.github.com/ashual/scene\_generation},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Ashual, Oron and Wolf, Lior},
	month = nov,
	year = {2019},
	note = {arXiv:1909.05379 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Best Paper Honorable Mention in ICCV 2019},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/2V988GXX/Ashual and Wolf - 2019 - Specifying Object Attributes and Relations in Inte.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CQ8BPVQ5/1909.html:text/html},
}

@misc{mittal_interactive_2019,
	title = {Interactive {Image} {Generation} {Using} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1905.03743},
	doi = {10.48550/arXiv.1905.03743},
	abstract = {Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image). We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Mittal, Gaurav and Agrawal, Shubham and Agarwal, Anuva and Mehta, Sushant and Marwah, Tanya},
	month = may,
	year = {2019},
	note = {arXiv:1905.03743 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Published at ICLR 2019 Deep Generative Models for Highly Structured Data Workshop},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/ZTASFJ4J/Mittal et al. - 2019 - Interactive Image Generation Using Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6NTD22MK/1905.html:text/html},
}


@misc{heusel2018ganstrainedtimescaleupdate,
      title={GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium}, 
      author={Martin Heusel and Hubert Ramsauer and Thomas Unterthiner and Bernhard Nessler and Sepp Hochreiter},
      year={2018},
      eprint={1706.08500},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1706.08500}, 
}

@inproceedings{LAMA,
author = {Zejian Li and Jingyu Wu and Immanuel Koh and Yongchuan Tang and Lingyun Sun},
title = {Image Synthesis from Layout with Locality-Aware Mask Adaption},
year = {2021},
publisher = {IEEE},
pages = {13819--13828},
booktitle = {IEEE International Conference on Computer Vision (ICCV)}
}

@misc{bochkovskiy2020yolov4optimalspeedaccuracy,
      title={YOLOv4: Optimal Speed and Accuracy of Object Detection}, 
      author={Alexey Bochkovskiy and Chien-Yao Wang and Hong-Yuan Mark Liao},
      year={2020},
      eprint={2004.10934},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2004.10934}, 
}

@inproceedings{zhang2018perceptual,
  title={The Unreasonable Effectiveness of Deep Features as a Perceptual Metric},
  author={Zhang, Richard and Isola, Phillip and Efros, Alexei A and Shechtman, Eli and Wang, Oliver},
  booktitle={CVPR},
  year={2018}
}

@inproceedings{NIPS2012_c399862d,
 author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {F. Pereira and C.J. Burges and L. Bottou and K.Q. Weinberger},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {ImageNet Classification with Deep Convolutional Neural Networks},
 url = {https://proceedings.neurips.cc/paper_files/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf},
 volume = {25},
 year = {2012}
}

@misc{szegedy2015rethinkinginceptionarchitecturecomputer,
      title={Rethinking the Inception Architecture for Computer Vision}, 
      author={Christian Szegedy and Vincent Vanhoucke and Sergey Ioffe and Jonathon Shlens and Zbigniew Wojna},
      year={2015},
      eprint={1512.00567},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.00567}, 
}

@misc{eiras2024risksopportunitiesopensourcegenerative,
      title={Risks and Opportunities of Open-Source Generative AI}, 
      author={Francisco Eiras and Aleksandar Petrov and Bertie Vidgen and Christian Schroeder and Fabio Pizzati and Katherine Elkins and Supratik Mukhopadhyay and Adel Bibi and Aaron Purewal and Csaba Botos and Fabro Steibel and Fazel Keshtkar and Fazl Barez and Genevieve Smith and Gianluca Guadagni and Jon Chun and Jordi Cabot and Joseph Imperial and Juan Arturo Nolazco and Lori Landay and Matthew Jackson and Phillip H. S. Torr and Trevor Darrell and Yong Lee and Jakob Foerster},
      year={2024},
      eprint={2405.08597},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.08597}, 
}

@misc{radford2021learningtransferablevisualmodels,
      title={Learning Transferable Visual Models From Natural Language Supervision}, 
      author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
      year={2021},
      eprint={2103.00020},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2103.00020}, 
}