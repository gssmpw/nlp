\renewcommand{\thesection}{A\arabic{section}}
\renewcommand{\thesubsection}{A\arabic{section}.\arabic{subsection}}

\appendix
\clearpage
\onecolumn
\setcounter{page}{1}  % Reset page number if needed
\setcounter{section}{0}  % Reset section numbering to start at A1
\section*{Appendix}


\section{Sum of Random Unit Vectors}
\label{appendix:sum_random}

First, recall that for random zero-mean vectors, their expected Euclidean norm $\mathbb{E}[\|\mathbf{a}\|^2] = \mathbb{E}[a_1^2 + a_2^2 + \dots + a_d^2] $ is related to the variance, since $
\text{Var}(a_i) = \mathbb{E}[a_i^2] - (\mathbb{E}[a_i])^2 = \mathbb{E}[a_i^2]$. Due to the independence assumptions, we can rewrite the expected norm in terms of the variance as $\mathbb{E}[\|\mathbf{a}\|^2] = \mathbb{E}[a_1^2] + \mathbb{E}[a_2^2] + \dots + \mathbb{E}[a_d^2].$

Let us consider the sum of \(N\) independent random \textit{unit} vectors \(\mathbf{c} = \sum_{i=1}^{N} \mathbf{a}_i\), where due to independence zero-mean expectation \(\mathbb{E}[\mathbf{a}_i \mathbf{a}_j] = \mathbb{E}[\mathbf{a}_i]\mathbb{E}[\mathbf{a}_j] = 0\) for \(i \neq j\). Then,

\begin{equation}
\mathbb{E}[\|\mathbf{c}\|^2] = \frac{1}{N_c} \sum_{i=1}^{N_c} \mathbb{E} \left[ \left( \sum_{i=1}^{N}  \mathbf{a}_i \right)^2 \right]
\end{equation}

\begin{equation}
= \frac{1}{N_c} \sum_{i=1}^{N_c} \mathbb{E} \left[ \sum_{i=1}^{N} \mathbf{a}_i^2 + 2 \sum_{i=1}^{N}\sum^{N}_{\substack{j=1 \\ j < i}} \mathbf{a}_i \mathbf{a}_j \right]
\end{equation}

\begin{equation}
= \frac{1}{N_c} \sum_{i=1}^{N_c} \left[ \sum_{i=1}^{N}  \mathbb{E}[\mathbf{a}_i^2] 
+ 2 \sum^{N}_{i=1}\sum^{N}_{\substack{j=1 \\ j < i}} \underbrace{\mathbb{E}[\mathbf{a}_i \mathbf{a}_j]}_{= 0} \right]
\end{equation}


\begin{equation}
= \sum_{i=1}^{N} \mathbb{E}[\mathbf{a}_i^2].
\end{equation}

If the inputs are standardised, this further simplifies to:

\begin{equation}
\mathbb{E}[\|\mathbf{c}\|] = \sqrt{\sum_{i=1}^{N} \mathbb{E}[\mathbf{a}_i^2]} = \sqrt{N}.
\end{equation}

A normalised version of \(\mathbf{c}\) is therefore: 

\begin{equation}
\hat{\mathbf{c}} = \frac{\mathbf{c}}{\mathbb{E}[\|\mathbf{c}\|]} = \frac{\sum_{i=1}^{N} \mathbf{a}_i}{\sqrt{N}}.
\label{eq:normalised_c}
\end{equation}

We use this simple magnitude preserving summation formula for the aggregations across both meta paths ($\Phi$) and for our neighbourhood pooling operation. Since this is trivial to compute on the fly, this allows variable size neighbourhoods to be included into our operator in Equation \ref{eq:hignn_operator} without increasing its magnitude.

\section{EDM2 Mathematical Preliminaries}
\label{appendix:edm2_preliminaries}

We apply a brief summary of relevant equations for reader convenience. For a complete overview, readers should reference \cite{karras_analyzing_2024} and their exhaustive appendix.

We use forced weight magnitude preservation, as per their Equation 47:

\begin{equation}
\hat{w_i} = \frac{w_i}{\|w\|_2 + \epsilon}.
 \label{eq:karras_47}
\end{equation}

Where $\epsilon$ is a small delta to avoid numerical issues. Readers should reference Karras et al. Algorithm 1 for an implementation of forced weight normalisation. Additionally, we utilise magnitude preserving sum (their Equation 88) at several locations in the network, including integration with the control net:

\begin{equation}
\text{MP-Sum}(\mathbf{a}, \mathbf{b}, t) = \frac{(1 - t) \mathbf{a} + t \mathbf{b}}{\sqrt{(1 - t)^2 + t^2}},
 \label{eq:mp_sum}
\end{equation}

and magnitude preserving concatenation (their Equation 103):

\begin{equation}
\text{MP-Cat}(\mathbf{a}, \mathbf{b}, t) = \sqrt{\frac{N_a + N_b}{(1 - t)^2 + t^2}} \cdot \left[ \frac{1 - t}{\sqrt{N_a}} \mathbf{a} \oplus \frac{t}{\sqrt{N_b}} \mathbf{b} \right].
 \label{eq:mp_cat}
\end{equation}

\section{Implementation parameters}
\label{appendix:parameter_table}

% Computing restraints restrict us from doing full scale hyper-parameter tuning, as a rough guide we use default values chosen from the EDM2 paper. However, it is highly likely that these values are not sensible given the very different nature of our training task, for example, EMA length/tuning can contribute significantly to FID - however, with the base parameters we are unable to utilise power EMA, likely due to undertraining. 

\begin{table}[h]
\centering
\caption{Training details for HIGnn diffusion models.}
\begin{tabular}{@{}lcc@{}}
\toprule
\textbf{Model details}                      &\textbf{ M} & \textbf{XXL} \\ \midrule
Number of GPUs                     &  4 &  4   \\
Minibatch size                     &  256 & 256    \\
Duration                           & 40M &  15M   \\
Channel multiplier                 & 256 & 448    \\
Number of HIG blocks  &  4 &  4    \\
Dropout probability                & 10\% &  10\%   \\
Learning rate max ($\alpha_{ref}$)   & 0.0045 & 0.003    \\
Learning rate decay ($t_{ref}$)      & 10000 & 10000   \\
Noise distribution mean ($P_{mean}$) &  -0.4 &  -0.4   \\
Noise distribution std. ($P_{std}$)  &  1.0 &  1.0    \\ 
Base Model capacity (Mparams)  &  497.8 &  1523.2    \\
ControlNet capacity (Mparams)  &  163.2 &  495.3   \\
HIGnn capacity (Mparams)  &  7.0 &  18.9    \\
Total capacity (Mparams)  &  668.0 &  2037.5    \\
Training time (days) &  3.0 &  3.0    \\
EMA &  None &  None    \\
\bottomrule
\end{tabular}
\label{table:model_hparams}
\end{table}


\newpage

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_selected.pdf}
    \caption{Selected generated samples from HIG using COCO validation conditions.}
    \label{fig:appendix_best}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.5\linewidth]{icml2023/hig_fid_guidance.pdf}
    \caption{\text{FID} and $\text{FID}_{\textit{DINOv2}}$ vs guidance. To understand the relationship between auto-guidance strength and FID values we generate 5K images for coco validation (i.e. one seed). The relationship we witness is similiar to that reported in the original \cite{karras_analyzing_2024} work, they report optimal FID at 1.4, and optimal $\text{FID}_{\textit{DINOv2}}$ at 1.9. Our optimal values are both slightly lower than this.}
    \label{fig:fid_appendix}
\end{figure}

\newpage

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_div_mask.pdf}
    \caption{Diverse HIG samples using mask and bounding boxes from COCO validation set.}
    \label{fig:appendix_best_diversity}
\end{figure}

\newpage

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/best_selected_bounding_box.pdf}
    \caption{Diverse HIG samples using only bounding boxes from the COCO validation set. We naturally observe increased diversity in images when compared to Figure \ref{fig:appendix_best_diversity} that uses mask inputs.}
    \label{fig:appendix_best_box}
\end{figure}

\clearpage
\newpage

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/hig_a1.pdf}
    \caption{Head-to-head comparisons between generations from HIG-Medium and HIG-XXL. There is a notable enhancement in realism in the latter, which exhibit richer color depth and more convincing surface textures.}
    \label{fig:m_vs_xl_comparison}
\end{figure*}

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_animal_closeup.pdf}
    \caption{Selected generations using a single bounding box with an animal class label.}
    \label{fig:animal_appendix}
\end{figure*}   


\clearpage
\newpage

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_tree_rel.pdf}
    \caption{We showcase the ability of the HIG to disambiguate overlapping bounding boxes. In this figure we show bear, horse, elephant and cow bounding boxes overlapped with a tree bounding box,  with the relationship `in front' or 'behind'. We also show the bounding box when the tree is removed.}
    \label{fig:relationships appendx}
\end{figure*}   

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_color_animal.pdf}
    \caption{We showcase the ability to locally edit the attribute of an object i.e. the colour of a specific animal in an image.}
    \label{fig:animal_color_appendix}
\end{figure*}   


\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_editing_local.pdf}
    \caption{We showcase the ability to locally edit the attribute of an object i.e. the colour of a specific bus or flower in a generation.}
    \label{fig:color_appendix_1}
\end{figure*}   

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_color_obj.pdf}
    \caption{Generations with different colour attributes. }
    \label{fig:color_appendix_2}
\end{figure*}   

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_donut.pdf}
    \caption{Editing a scenes layout. We observe that when generating images with the same seed that object semantics stay relatively consistent as we add or remove objects from a scene. }
    \label{fig:donut_appendix}
\end{figure*}   

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/appendix_elephant.pdf}
    \caption{We showcase the ability to change the size and position of an object in a scene.}
    \label{fig:elephant_appendix}
\end{figure*}  




