\section{Related Work}
\label{sec:related_work}

Since the focus of this work is the effective and flexible conditioning of image synthesis with conditioning graphs, and we do not contribute to the theory behind diffusion, we direct readers to prior works \cite{sohl-dickstein_deep_2015, song_score-based_2021, ho_denoising_2020, karras_elucidating_2022} for the mathematical preliminaries. Readers should be aware of Karras et al. \cite{karras_analyzing_2024} which forms the basis of this work in regards to diffusion architecture design. A brief summary of conditional diffusion in this context is provided below for reader convenience.

\textbf{Conditional Diffusion.} Conditional diffusion extends the standard diffusion framework by introducing conditional variables into the generative process, allowing for control over the output. Instead of modeling the data distribution \( p(\mathbf{x}; \sigma(t)) \) with time-dependent noise level $\sigma$, conditional diffusion focuses on generating data given conditioning variables \( \mathbf{c} \), resulting in \( p(\mathbf{x} | \mathbf{c}; \sigma(t)) \). Both approaches can be described through SDE or ODE formulations. The probability ODE formulation from Karras et al. \cite{karras_elucidating_2022} describes the process both forward and backward in time:

\begin{equation}
    dx = -\dot{\sigma}(t)\sigma(t) \nabla_x \log p(x| \mathbf{c}; \sigma(t)) dt,
    \label{eq:karras_1}
\end{equation}

where \(\dot{\sigma}(t)\) is the time derivative, and \( \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{c}; \sigma(t)) \) represents the conditional score function that depends on the noise level and conditioning variables $\mathbf{c}$. This score function can be approximated through an L2 denoising objective \cite{karras_elucidating_2022, song_score-based_2021}. This approach allows for fast deterministic and higher-order sampling which has been shown to be highly effective \cite{karras_elucidating_2022}.

Typically, conditioning is done by explicitly providing conditioning signals such as text embeddings, low-resolution images, or other signals to the model \cite{saharia_photorealistic_2022, saharia_image_2021, li_controlnet_2024, dhariwal_diffusion_2021}. This can be done through simple concatenation \cite{saharia_image_2021}, adaptive normalisation layers \cite{peebles_scalable_2023}, or cross-attention mechanisms \cite{saharia_photorealistic_2022}. For example, in the \textit{Imagen} framework of Saharia et al. \cite{saharia_photorealistic_2022}, conditional superresolution is achieved by conditioning text latents from a pre-trained language model via cross-attention, and conditioning on low-resolution images via concatenation. Another effective conditioning approach was proposed in the \textit{ControlNet} framework of Zhang et al. \cite{zhang_adding_2023}, where diverse spatial-conditional generation was achieved by incorporating an additional network alongside a large, pre-trained text-to-image diffusion model with frozen parameters. Conditioning signals are incorporated by first transforming the conditions to latent image space and passing them as input. Finally, in the EDM2 model \cite{karras_analyzing_2024} - the focus of this work - they generate images conditioned on simple class labels via simple multiplication with a learned embedding and a zero-initialised gain parameter. Notably, this work also deeply analyses the training dynamics of diffusion models and highlights the importance of standardising weight magnitudes explicitly by design, an approach we adopt.

\textbf{Diffusion with Conditioning Graphs.} Graphs have previously been used to condition models for image synthesis. In particular, one popular task is generating images that adhere to a scene graph that represents the image \cite{johnson_image_2018, yang_diffusion-based_2022, dhamo_semantic_2020, farshad_scenegenie_2023, mittal_interactive_2019}. Typically, scene graphs are first converted to an intermediate layout (i.e. a latent image) before being processed by a secondary network such as diffusion models. Farshad et al. \cite{farshad_scenegenie_2023} process graphs with a GCN to predict object embeddings that control the network via sampling guidance. Yang et al. \cite{yang_diffusion-based_2022} pre-train a masked auto-encoder on scene graph triplets to generate local and global embeddings to condition the diffusion model. Modifying scene graphs have also been used as a way for users to interactively control image synthesis \cite{mittal_interactive_2019, dhamo_semantic_2020}. 

\textbf{Graphs for Image Processing.} GNNs have been used in the computer vision domain for various applications including image processing where images are viewed directly as graphs \cite{tarasiewicz_graph_2021, han_vision_2022, tian_image_nodate, krzywda_graph_2022, defferrard_convolutional_2017, liu_cnn-enhanced_2021, wan_multi-scale_2019}. For example, Han et al. \cite{han_vision_2022} demonstrate GNNs to be effective for image classification tasks by splitting images into nodes and connecting them via nearest neighbours. Tian et al. \cite{tian_image_nodate} use GNNs for super-resolution by leveraging variable node degree to focus on high-frequency areas. Scene graph generation typically processes images with traditional CNN architectures, and decodes them with GNNs \cite{xu_scene_2017, yang_graph_2018}. While prior work demonstrates that graph-based models are flexible and powerful for image processing, their potential to directly use them to condition image diffusion models remains underexplored.

% Additionally, research in combining traditional CNN architectures and GNNs has been proposed \cite{liu_cnn-enhanced_2021}, where GNNs are typically used to learn `super-pixel' representations for image classification.