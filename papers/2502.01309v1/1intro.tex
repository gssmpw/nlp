\section{Introduction}
\label{sec:intro}

\begin{figure}[t]
    \centering
\includegraphics[width=1\linewidth]{icml2023/hig_fig1.pdf}
    \vspace{-20bp}
    \caption{
\textbf{(a)} Our representation enables flexible conditioning for graph-to-image generation by modeling objects, attributes, and relationships as a graph. This integrates with a pixel grid, where pixels act as nodes connected to objects, defining spatial relationships. \textbf{(b)} Modifying conditions at the node and or edge level enables precise semantic control over the generation process.}
\label{fig:fig_1_intro}
\end{figure}

% Our representation offers full flexibility in conditioning via graph-to-image generation. Objects, attributes, and their relationships are represented in an unstructured format interconnected with a grid, wherein pixels are nodes linked to objects.

Conditional denoising diffusion models have become widely popular due to their proficiency for high-quality and controlled image synthesis \cite{saharia_image_2021, saharia_photorealistic_2022, karras_analyzing_2024, li_controlnet_2024, zhang_adding_2023, ramesh_hierarchical_2022}. Diffusion models transform pure noise into images through repeated application of denoising steps via a learned denoiser \cite{ho_denoising_2020, song_score-based_2021}. This sampling process can be modelled through differential equations, where each denoising step can be seen through the lens of score matching \cite{song_score-based_2021, hyvarinen_estimation_2005}. Typically, the image processing architecture is a U-Net model with intermediate self-attention layers \cite{ho_denoising_2020, saharia_image_2021, karras_analyzing_2024}. Controlling generation is achieved through two routes: explicit conditioning and guidance during sampling \cite{saharia_image_2021, karras_guiding_2024, ho_classifier-free_2022}. Explicit conditioning provides conditioning information directly to the model, for example text prompts or low-resolution images \cite{saharia_photorealistic_2022}. However, real-world applications such as image-editing, weather-modelling and additive manufacturing demand the generation of images conditioned on variable-length, and diversely structured data i.e. graph-to-image – a challenge that remains largely unaddressed.
% \begin{figure}[t]
%   \centering
%    \includegraphics[width=1\linewidth]{figs/cvpr_intro_fig.png}

%    \caption{An image may have geometric dependencies, like the printed layer image (left) generated from a process graph. The image can treated as a collection of image patch or pixel nodes (middle). Image nodes have underlying relationships with conditioning nodes (right), which also have relationships amongst themselves.}
%    \label{fig:intro}
% \end{figure}


Concurrently, the field of geometric deep learning has gained  traction by extending deep learning techniques to non-regular data-spaces such as graphs i.e.  graph neural networks (GNNs) \cite{kipf_semi-supervised_2017, hamilton_inductive_2018, velickovic_graph_2018, gilmer_neural_2017}. In particular, this has allowed advancements in weather modelling where data on graph nodes represent spatial relationships between geographical locations \cite{lam_graphcast_2022}. GNNs have also been applied in the image domain breaking the rigidity of traditional grid-based techniques \cite{tian_image_nodate, han_vision_2022}. Moreover, they have been explored in image synthesis with diffusion models, most notably for image generation from scene graphs that are designed to contain coarse structures of the scene images \cite{farshad_scenegenie_2023, johnson_image_2018, yang_diffusion-based_2022}. By processing a scene graph with a graph encoder, they produce local and global latents that aid in accurate generation. However, these fall short of a general-purpose approach that can seamlessly interweave heterogeneous, sparse, and variable length graphs whilst retaining the power of existing image models.


In this paper we introduce Heterogenous Image Graphs (HIG), a novel representation for conditional image generation that leverages the power of diffusion models in conjunction with GNNs. This representation, illustrated in Figure \ref{fig:fig_1_intro}, allows an image to switch between a standard image representation and a set of image patch or pixel nodes. We then consider a second `conditioning' graph and its relationships to both the image nodes and itself. By switching between these representations, we allow the generated image to be processed by tried-and-true architectures such as the U-Net \cite{ronneberger_u-net_2015, karras_analyzing_2024}, whereas the HIG representation can be processed intermediately by a GNN. This allows complex conditioning variables and relationships to be processed directly within the model architecture. We demonstrate that this method effectively conditions both semantic and relational information. To achieve this, we apply the magnitude-preserving formulation set out in EDM2 \cite{karras_analyzing_2024} to a custom graph convolution operator, and show it preserves magnitudes under certain conditions required of the graph data.

% To do this, we opt to follow a ControlNet style approach: a frozen pre-trained model, with a trainable copy the encoder and a conditioning HIGnn.

This approach proves to be an effective and adaptable method, seamlessly integrating diverse conditions from multiple datasets. We evaluate our trained model on a variety of tasks and show SOTA performance in multiple metrics. For the Visual Genome layout-to-Image task, our approach improves the previous record FID from 15.63 to 8.79, and  15.61 to 11.41 for the COCO-stuff mask-to-Image task, at higher resolution compared to previous work.

Our main contributions are:

\vspace{-4mm} 
\begin{itemize}
    \item The HIG representation, demonstrating its effectiveness in fine-grained image control through graph-based attributes and relationships. 
    \item SOTA results on layout-to-image and mask-to-image tasks (COCO-Stuff, VG) at 512×512 resolution.
\end{itemize}
