\section{Experiments}
\label{sec:experiments}

% \subsection{Empirically Verifying Magnitude Preservation}

% We

% As per prior work \cite{karras_analyzing_2024, rombach_high-resolution_2022} we use latent diffusion with a pre-trained autoencoder.
\subsection{Datasets}
We apply our method to two datasets: COCO-Stuff and Visual Genome both center cropped and resized to 512x512. We precompute latent images using the same autoencoder as in EDM2\footnote{\href{https://huggingface.co/stabilityai/sd-vae-ft-mse}{https://huggingface.co/stabilityai/sd-vae-ft-mse}}. We also employ random horizontal flipping and save our graph/image pairs to disk. For conditioning variables that involve natural language i.e. class labels, object relationships and image captions we opt to first encode them with a pretrained CLIP-ViT-Large\footnote{\href{https://huggingface.co/openai/clip-vit-large-patch14}{https://huggingface.co/openai/clip-vit-large-patch14}} model to get 768-d vectors for conditioning features. We precompute a vocabulary of class labels, attributes and relationships. This approach allows for generalised representations and for datasets to be fairly unfiltered when compared to previous work. Viewing all datasets through the lens of heterogeneous graphs allows us to jointly train on a variety of different conditioning signals, even when datasets contain different label modalities. Notably, these labels may be represented by vectors of variable length. 
% i.e. the visual genome dataset is typically filtered for object categories occurring at least 2000 times in the train set, along with other filters this results in only 62k of 108k images being used by previous methods.

\begin{table*}[t]
\label{table:metrics}
\centering
\caption{Results comparison between our method with guidance = 1.8 and LayoutDiffusion.}
\vspace{5bp}
\begin{tabular}{@{}lccccccc@{}}
\toprule & \multicolumn{4}{c}{\textbf{COCO-stuff}} & \multicolumn{3}{c}{\textbf{Visual Genome}} \\ 
\cmidrule(l){2-5} 
\cmidrule(l){6-8} 
\textbf{Methods} & \textbf{FID ↓} & \textbf{$\text{FID}_{\textit{DINOv2}}$ ↓} & \textbf{DS ↑} & \textbf{YOLOScore} ↑ & \textbf{FID ↓ }& \textbf{$\text{FID}_{\textit{DINOv2}}$ ↓}& \textbf{DS ↑} \\ \midrule

LayoutDiffusion (128$\times$128) &16.57&-&0.47±0.09&27.00&16.35&-&0.49±0.09\\
LayoutDiffusion (256$\times$256) &15.63&-&0.57±0.10&32.00&15.63&-&0.59±0.10\\
% \cmidrule[0.3pt]{2-4} \cmidrule[0.3pt]{5-6} \\
\textbf{HIG-Medium} (box only) & 11.63 &317.69&\textbf{0.67±0.10} &34.40& 8.99 & 367.89 & 0.68±0.10 \\
\textbf{HIG-Medium} & \textbf{11.42} & 256.93&0.59±0.11&\textbf{41.20}& N/A & N/A & N/A \\
\textbf{HIG-XXL} (box only) & 11.59 &  210.16 & 0.66±0.10 & 28.00 &\textbf{8.79}&213.80&0.64±0.10\\
\textbf{HIG-XXL} & 12.48 & \textbf{198.47}&0.56±0.12& 34.90&N/A&N/A& N/A \\
\bottomrule
\end{tabular}
\end{table*}

\textbf{COCO-stuff} consists of 118k training images. Each image is annotated with a semantic segmentation mask comprising of 183 classes (inc. an unlabeled class), instance bounding boxes and an image caption. For global caption conditioning, we use the same approach used for class labels in the original EDM2 paper but replace class labels with CLIP captions.
To construct graphs we create two types of nodes: mask nodes and instance nodes.
For mask nodes, we extract the number of unique classes present in the semantic mask. Each mask node is connected to the image via its corresponding class assignment (w.r.t the mask) at the latent resolution. Instance nodes are created per bounding box annotation and connected to pixels within the box. Each instance is connected to the class node of the same type. During training time we randomly drop out ($p=0.5$) mask nodes with uniform probability, ensuring the model does not rely too heavily on mask inputs and must infer the rest of the scene when partial masks are provided.
% Finally, we densely interconnect conditioning nodes of the same node type (i.e. class-to-class), to allow communication across higher level abstractions of the image.

\textbf{Visual Genome} consists of 108k training images with 3.8M object instances and 2.3M relationships between them. The dataset primarily comprises: object bounding boxes, attributes and relationships. Due to the volume of classes and different relationships between them, we step away from previous image synthesis research using scene graphs, and instead opt for a strategy of encoding classes, attributes and relationships through CLIP \cite{radford2021learningtransferablevisualmodels}, closer inline with prior text-to-image work. For example, to encode the relationship \textit{pulling(horse, carriage)} we extract the latent representation of ``\textit{pulling}'' from CLIP and use this as the directional edge attribute. We apply the same method to attributes via self-loops. Likewise, instance node features are populated in the same fashion via their class label, and are connected to the image w.r.t to their bounding box information. The HIG representation naturally lends itself to complex data and allows a method to account for overlapping regions and relationships to be naturally represented. We filter the attributes/relationships and objects to remove instances that occur less than 250 and 1000 times respectively, in contrast to previous methods that apply stricter thresholds of 500 and 2000.

\textbf{Implementation Details} 
We train models using 4 x A100 GPUs, using training recipes from EDM2, however, due to limited compute resources we reduce the batch-size from $2048$ to $256$ and half the learning-rate from the proposed values, although not optimal we find this to be quicker to train in practice. We train our medium and XXL base models for a combined total of 12 A100 GPU days each, yet do not witness convergence within this timeframe. Our medium model is comparable, in terms of trainable parameters, with previous SOTA, while the XXL is trained to demonstrate scalability. See Appendix Table \ref{table:model_hparams} for full details.

\subsection{Evaluation Metrics.}
We assess quality, diversity and controllability.

\textbf{Fréchet inception distance} (FID) is the primary evaluation metric for visual quality \cite{heusel2018ganstrainedtimescaleupdate}, measuring the distance between feature distributions of generated and real images. It computes the Fréchet distance using features from Inception-v3 \cite{szegedy2015rethinkinginceptionarchitecturecomputer}  with lower values indicating higher quality and realism. We also report $\text{FID}_{\textit{DINOv2}}$ \cite{oquab_dinov2_2024} which has been observed to align better with human preferences \cite{stein_exposing_2023}.
% Our FID is reported on the same validation splits as previous work ($\sim5$K samples for COCO, $\sim11$K samples for VG), however prior work has excessively filtered these sets to improve performance (i.e. selecting only images with 3-8 objects), a practice we do not adopt. FID is calculated between ground-truth validation splits and x5 generations using generated samples given the validation conditions. 

\textbf{Diversity Score} (DS), as introduced in \cite{zheng_layoutdiffusion_2024}, quantifies the perceptual similarity between two images of the same layout generated using different seeds. Specifically, it leverages the learned perceptual image patch similarity (LPIPS) metric \cite{zhang2018perceptual}. A higher LPIPS value corresponds to greater dissimilarity, meaning that a higher DS is desirable for our application.

% , which measures the distance between image patches in feature space as extracted by an AlexNet \cite{NIPS2012_c399862d}

\textbf{YOLOScore} \cite{LAMA} is used to assess the compliance of the generated samples against prior conditions. To do this, a YOLOv4 \cite{bochkovskiy2020yolov4optimalspeedaccuracy} is used to predict the bounding boxes for a generated sample which are then compared against the ground truth ones, previously used in the HIG for generation. YOLOScore is derived from a combination of several metrics,  including intersection over union, classification accuracy with respect to categorical labels, and confidence scores.

Our metrics are reported on the same validation splits as previous work (5K samples for COCO, 11K samples for VG), however prior work has excessively filtered the sets to improve performance, a practice we do not adopt. FID is calculated between ground-truth validation splits and $\times5$ generations given identical prior conditions. Reported DS values are calculated as the average between all available pair-wise comparisons, and error bars represent the standard deviation between them. Finally, YOLOScore is reported as the average mean precision (\%) of the scores calculated from single corresponding generations. For generations we use a default auto-guidance strength of 1.8, closely in line with the optimal value in EDM2 for $\text{FID}_{\textit{DINOv2}}$, however, as noted in their work, FID considers this optimal value to be a poor choice, and vice versa - see Appendix \ref{fig:fid_appendix}.

\subsection{Quantitative Results}

Quantitative comparisons of our work (at $512\times512$) and the previous SOTA (at $256\times256$) are shown in Table \ref{table:metrics}. For a fair comparison we present results on generated samples with and without mask inputs. We achieve superior performance in all metrics and experiments, despite adopting a less restrictive filtering strategy on validation data. 
% This suggests that our model not only breaks the previous record in terms of quality, but also in terms of controllability.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{icml2023/hig_fig3.pdf}
    \vspace{-20pt}
    \caption{Comparison with SOTA and reference on COCO-stuff.}
    \label{fig:3}    
\end{figure}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{icml2023/hig_fig4.pdf}
    \vspace{-20pt}
    \caption{Generations from the same seed demonstrate HIG's ability to control size, quantity and position of objects.}
    \label{fig:size_editing}    
\end{figure}

Across our presented models there are interesting disparities. We see much higher DS for our bounding box models when compared to our mask models - we interpret this as the increase in spatial control decreases the diversity of the generated images. For YOLOScore, we find the performance improves when the mask input is available, likely due to less ambiguity in the conditioning and overlapping bounding boxes taking less effect. In essence, the outline of the object is given for free which likely improves classification accuracy. We see that our XXL model performs worse on this task, suggesting that our larger models may
% produce higher fidelity images but are
be less controllable. Given compute constraints, the XXL model remains under-trained and did not plateau during training (notably, the original model was pre-trained for 291 GPU days, whereas ours was trained for only 12.) Perhaps given more training time this scenario would change.

Finally, we observe interesting trends in the FID scores. We notice that standard FID prefers our smaller model, whereas $\text{FID}_{\textit{DINOv2}}$ has a much clearer trend and greatly prefers our XXL model. While standard FID is more sensitive to low-level distributional shifts, $\text{FID}_{\textit{DINOv2}}$ better captures high-level semantic consistency, as detailed in the original DINO work \cite{szegedy2015rethinkinginceptionarchitecturecomputer}. Thus, this preference suggests that our XXL model produces images more close to the ground truth in terms of semantic coherence and object fidelity. This is further supported from visual inspection in the head-to-head comparison shown in Appendix Figure \ref{fig:m_vs_xl_comparison}, where high $\text{FID}_{\textit{DINOv2}}$ seems to align more closely with human-perceived quality.

\subsection{Qualitative Results} 

Our comparison with the current state-of-the-art method \cite{zheng_layoutdiffusion_2024}, illustrated in Figure \ref{fig:3}, demonstrates significant improvements in image quality achieved by our approach. Specifically, HIG with bounding boxes generates highly realistic images, outperforming previous SOTA in terms of detail and coherence. Furthermore, HIG with masks excels in accurately preserving the original sub-structures of the image, as explicitly guided by the given masks. This capability makes HIG with masks particularly valuable in scenarios where precise spatial control is essential. Refer to Appendix Figure \ref{fig:appendix_best} for a selection of high quality generated outputs, and Appendix Figures \ref{fig:appendix_best_diversity}-\ref{fig:appendix_best_box} for diverse generations that demonstrate the difference between mask and bounding box generation.

\textbf{Layout and attribute modification} involves altering the HIG condition through semantic and/or spatial alterations. To evaluate the controllability of the model, we conduct a series of custom generations to guide the model to a desired layout. In Figure \ref{fig:size_editing} we test the model's capability in adjusting size, layout and position of objects in a scene. To make a fair test we keep the random generation seed across all runs the same, which also leads to the effect that the image remains mostly unchanged, a desirable effect in image editing. The model generation is highly controllable through adjusting the spatial edges on the HIG, in all three tests.

We also test the model's capability to adjust semantics in the scene, for example modifying attributes (e.g. colour), objects (e.g. category) and relationships (e.g. behind/in front.) Our results in Figure \ref{fig:colour_editing} show unprecedented control, enabling high-quality image generation with precise localized edits (e.g., changing the color of a specific cow.) Furthermore, our model exhibits strong generalizability by effectively generating out-of-distribution object combinations, such as a cat-shaped broccoli, positioned beneath a hat. While object-dependent spatial masks pose challenges when combined with semantic guidance, the model preserves consistency and quality, further demonstrated in Figure \ref{fig:fusion}. Lastly, the model respects relationships between two ambiguous overlapping bounding boxes (e.g., positioning one giraffe in front of another), establishing a new benchmark for structured and context-aware image generation, see Appendix Figures \ref{fig:relationships appendx}-\ref{fig:elephant_appendix} for more examples.

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{icml2023/hig_fig5.pdf}
    \vspace{-20pt}
    \caption{HIG enables precise, localized control over semantic conditions, including attributes, objects, and their relationships.}
    \label{fig:colour_editing}
\end{figure}

\begin{figure}[t]
    \centering
\includegraphics[width=1\linewidth]{icml2023/hig_fig6.pdf}
    \vspace{-20bp}
    \caption{We show HIG can effectively generate consistent images, adhering to underlying mask and semantic conditions. In this case we show `HIG' written in trees, food, windows and kites.}
    \label{fig:fusion}
\end{figure}

\subsection{Ablation Experiments}
We conduct a small ablation study on various architecture design choices. We launch smaller training runs to isolate factors of GNN depth, control integration and magnitude preservation. In Table \ref{table:ablation} we show that decreasing the number of HIG blocks degrades performance. We also find that applying magnitude preservation functions to incoming conditioning features is essential for the ControlNet, by only using regular addition (and zero-gain) we find NaNs start to appear in the training loss as feature magnitudes explode. We also investigate whether preserving magnitudes in the GNN is essential or if standard PixNorm suffices. Interestingly, our tests show performance comparable to our base MP-GNN approach. Finally, we experiment whether full-fining is a viable alternative to ControlNet, despite scoring the lowest $\text{FID}_{\textit{DINOv2}}$ we witness poor perceptual quality in comparison to the base model. We leave it to future research to develop upon these results further, for example by training to convergence.
\begin{table}[t]
\centering
\caption{Ablation study results on COCO-stuff at 12M images.} 

\begin{tabular}{@{}lcc@{}}
\toprule

& \textbf{FID ↓} & \textbf{$\text{FID}_{\textit{DINOv2}}$ ↓} \\ \cmidrule{2-3}
HIG-Medium & \textbf{18.34} & 332.00 \\
\midrule 
Depth (2 Layers) & 20.40 & 366.09 \\
Without MP ControlNet & \textit{NaN} & \textit{NaN} \\
Without MP GNN (w/ PixNorm) & 18.56 & 328.67 \\
% Alt. GNN Placement & 20.40 & 366.09 \\
Full Fine-tuning & 19.66 & \textbf{325.297} \\
\bottomrule
\end{tabular}
\label{table:ablation}
\end{table}
