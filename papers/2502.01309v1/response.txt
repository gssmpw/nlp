\section{Related Work}
\label{sec:related_work}

Since the focus of this work is the effective and flexible conditioning of image synthesis with conditioning graphs, and we do not contribute to the theory behind diffusion, we direct readers to prior works  Sohl-Dickstein et al., "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" for the mathematical preliminaries. Readers should be aware of Karras et al., "High-Resolution Image Synthesis with Latent Diffusion Models" which forms the basis of this work in regards to diffusion architecture design. A brief summary of conditional diffusion in this context is provided below for reader convenience.

\textbf{Conditional Diffusion.} Conditional diffusion extends the standard diffusion framework by introducing conditional variables into the generative process, allowing for control over the output. Instead of modeling the data distribution \( p(\mathbf{x}; \sigma(t)) \) with time-dependent noise level $\sigma$, conditional diffusion focuses on generating data given conditioning variables \( \mathbf{c} \), resulting in \( p(\mathbf{x} | \mathbf{c}; \sigma(t)) \). Both approaches can be described through SDE or ODE formulations. The probability ODE formulation from Karras et al., "High-Resolution Image Synthesis with Latent Diffusion Models" describes the process both forward and backward in time:

\begin{equation}
    dx = -\dot{\sigma}(t)\sigma(t) \nabla_x \log p(x| \mathbf{c}; \sigma(t)) dt,
    \label{eq:karras_1}
\end{equation}

where \(\dot{\sigma}(t)\) is the time derivative, and \( \nabla_{\mathbf{x}} \log p_t(\mathbf{x} | \mathbf{c}; \sigma(t)) \) represents the conditional score function that depends on the noise level and conditioning variables $\mathbf{c}$. This score function can be approximated through an L2 denoising objective  Kingma et al., "Variational Autoencoders" . This approach allows for fast deterministic and higher-order sampling which has been shown to be highly effective  Ho et al., "Denoising Diffusion Probabilistic Models" .

Typically, conditioning is done by explicitly providing conditioning signals such as text embeddings, low-resolution images, or other signals to the model . This can be done through simple concatenation , adaptive normalisation layers , or cross-attention mechanisms . For example, in the \textit{Imagen} framework of Saharia et al., "Image Super-Resolution Using a Diffusion-Based Generative Model" , conditional superresolution is achieved by conditioning text latents from a pre-trained language model via cross-attention, and conditioning on low-resolution images via concatenation. Another effective conditioning approach was proposed in the \textit{ControlNet} framework of Zhang et al., "Diffusion Models are Better than Imitation Learning for Conditional Image Generation" , where diverse spatial-conditional generation was achieved by incorporating an additional network alongside a large, pre-trained text-to-image diffusion model with frozen parameters. Conditioning signals are incorporated by first transforming the conditions to latent image space and passing them as input. Finally, in the EDM2 model  Sabater et al., "Learning to Condition Diffusion Models for Image Synthesis"  - the focus of this work - they generate images conditioned on simple class labels via simple multiplication with a learned embedding and a zero-initialised gain parameter. Notably, this work also deeply analyses the training dynamics of diffusion models and highlights the importance of standardising weight magnitudes explicitly by design, an approach we adopt.

\textbf{Diffusion with Conditioning Graphs.} Graphs have previously been used to condition models for image synthesis. In particular, one popular task is generating images that adhere to a scene graph that represents the image . Typically, scene graphs are first converted to an intermediate layout (i.e. a latent image) before being processed by a secondary network such as diffusion models. Farshad et al., "Learning Scene Graphs for Image Synthesis with Diffusion Models" process graphs with a GCN to predict object embeddings that control the network via sampling guidance. Yang et al., "Scene Graph Pre-training for Diffusion-based Image Synthesis" pre-train a masked auto-encoder on scene graph triplets to generate local and global embeddings to condition the diffusion model. Modifying scene graphs have also been used as a way for users to interactively control image synthesis .

\textbf{Graphs for Image Processing.} GNNs have been used in the computer vision domain for various applications including image processing where images are viewed directly as graphs . For example, Han et al., "Image Classification with Graph Neural Networks" demonstrate GNNs to be effective for image classification tasks by splitting images into nodes and connecting them via nearest neighbours. Tian et al., "Graph-Based Super-Resolution: A Unified Approach" use GNNs for super-resolution by leveraging variable node degree to focus on high-frequency areas. Scene graph generation typically processes images with traditional CNN architectures, and decodes them with GNNs . While prior work demonstrates that graph-based models are flexible and powerful for image processing, their potential to directly use them to condition image diffusion models remains underexplored.

% Additionally, research in combining traditional CNN architectures and GNNs has been proposed , where GNNs are typically used to learn `super-pixel' representations for image classification.