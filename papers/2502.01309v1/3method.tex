\section{Method}
\label{sec:method}

\subsection{Weaknesses of Previous Conditioning Methods}

The most popular form of latent image conditioning typically converts conditioning signals to images, before processing them with typical image processing models. While this approach is powerful, it exhibits limitations in handling complex image synthesis tasks, particularly when incorporating heterogeneous or sparse input conditions. Some approaches, such as \textit{LayoutDiffusion} \cite{zheng_layoutdiffusion_2024}, tackle this with custom attention modules that attend to bounding boxes with learned positional embeddings. However, these approaches neglect to include multiple modalities and the relationships between them, which overlooks nuanced interactions between conditioning signals i.e. disambiguating spatial ordering between overlapping boxes. 

% For example, interactions between conditions which may not explicitly exist in the discrete spatial image domain.

% These approaches force diverse modalities, like mixed spatial and categorical information directly into a unified image space, which overlooks nuanced interactions between conditioning signals. For example, interactions between conditions which may not explicitly exist in the discrete spatial image domain.

Previous conditional diffusion research that utilise graph data opt for complex multi-stage training procedures such as masked contrastive pre-training using graph triplets \cite{yang_diffusion-based_2022}. This is not only time-consuming, but also fails to exploit potential benefits of training an end-to-end system that integrates graph data directly into image processing. 
% Furthermore, other work has shown that the repeated conditioning diffusion models (i.e. time or text conditioning) is superior to simply providing   

We tackle these problems by representing images and their conditioning signals as a single graph, which is processed by a bespoke GNN architecture. This allows repeated interactions between conditioning signals and the image throughout the synthesis process, enabling more flexible and dynamic representations that account for both the current image features and interactions between conditioning signals. By maintaining separate pathways for distinct input types, our approach supports heterogeneous and sparse conditioning, leading to better generalisation, finer control, and more precise manipulation of generated images. This simple yet powerful method can be easily integrated into a wide range of existing vision models.

\begin{figure}
    \centering    \includegraphics[width=1\linewidth]{icml2023/hig_fig2.pdf}
\vspace{-20pt}
    \caption{(\textbf{a}) Overview of the proposed architecture. The HIG is encoded into a latent representation through a MP-GNN which is then used as a condition $c_f$ in a ControlNet. (\textbf{b}) Details of the MP-GNN module. Note: HMP is shorthand for heterogenous magnitude preserving operations applied across all nodes.}
    \label{fig:architecture}
\end{figure}

\subsection{Heterogeneous Image Graphs}

To improve on previous approaches we develop a new approach to condition images via the HIG representation. In this manner, we fully exploit variable-length and heterogeneous conditions to aid in image synthesis.

\textbf{Image Graphs.} When faced with the challenge of conditioning images with graphs we first convert images into representations amenable for graph processing. We reshape image features into image nodes pixel-wise in line with other works \cite{liu_cnn-enhanced_2021, han_vision_2022}. In practice, these nodes represent more than a single pixel, for example a latent image patch. This can be due to performing latent image diffusion \cite{rombach_high-resolution_2022, podell_sdxl_2023} where images are first pre-compressed to latent images, or due to prior processing by the image processing model. In contrast to other works \cite{tian_image_nodate, han_vision_2022, tarasiewicz_graph_2021}, we decide to leave image nodes unconnected; this loosely decouples image conditioning from processing. Image nodes are conditioned and later converted back into an image representation, allowing existing architectures to handle processing. Connecting image nodes in a locally dense fashion gains little benefit over highly optimised $3 \times 3$ convolutional operations. Formally, image nodes exist in a discrete space \( f : \mathbb{Z}^2 \to \mathbb{R}^C \). For an image of size \(M \times N\), we define \( f(i, j) \) where \( i, j \in \mathbb{Z} \) and \( 0 \leq i < M \), \( 0 \leq j < N \).

\textbf{Conditioning Graphs}. Conditioning graphs consist of nodes and edges, where each node has features defined as $ g : \mathcal{V} \to \mathbb{R}^F$, where $\mathcal{V}$ represents the set of nodes and $\mathbb{R}^F$ the feature space. Nodes may have spatial ties to the image domain, which we materialise via edges linking image and conditioning nodes. We use conditioning nodes to indicate semantics within the scene, for instance, a node may represent an object (e.g., a \textit{person}). Whereas we utilise different edge types to represent both spatial, abstract relationships and additional semantics. For instance, an edge between two object nodes may encode interactions or attributes (e.g., a person \textit{wearing} a {\textit{yellow}} hat). The graph structure reflects real-world data: often sparse and heterogeneous. We therefore construct graphs on a per task-basis to best leverage the available data and its dependencies.
Formally, each edge \( e \in \mathcal{E} \) connects two nodes \( (v_i, v_j) \in \mathcal{V} \times \mathcal{V} \) and represents a relationship between them. Edges represent any dependency, allowing for abstract relationships to be included.

% To continue the example, if spatial information for both the \textit{person} and the \textit{hat} is available, the graph would contain a node for each object and an edge connecting them, with the edge encoding the relationship \textit{wearing}. 


% \textbf{Conditioning Graphs.} In contrast, conditioning graphs are represented by sets of nodes and edges, with each node having associated features defined by a function $( g : \mathcal{V} \to \mathbb{R}^F$, where $\mathcal{V}$ represents the set of nodes and $\mathbb{R}^F$ the feature space. Although nodes \textit{may} have explicit spatial ties to the discrete image domain, we materialise these through edges between image and conditioning nodes. However, these relationships may be the product of spatial properties of conditioning nodes. As such, subsets of $\mathbb{R^F}$ may represent spatial coordinates \( (x, y) \in \mathbb{R}^2 \) that satisfy \( 0 \leq x < M \) and \( 0 \leq y < N \). Conditioning nodes are not restricted to pixel grid positions, nor the number of spatial dimensions e.g. nodes may represent 3D properties of the real world. Nodes and edges may represent properties independent of spatial dimensions. For example, nodes in the graph can represent concrete objects in the image (e.g., a \textit{person}), while edges between them may represent abstract interactions or attributes (e.g., a person \textit{wearing} a {\textit{yellow}} hat). The graph structure may be sparse, and heterogeneous (multiple types of nodes and edges). Conditioning graphs are constructed on a per-task basis to optimally leverage available data and its dependencies. Formally, each edge \( e \in \mathcal{E} \) connects two nodes \( (v_i, v_j) \in \mathcal{V} \times \mathcal{V} \) and represents a relationship between them. To continue the example, if spatial information for both the \textit{person} and the \textit{hat} is available, the graph would contain a node for each object and an edge connecting them, with the edge encoding the relationship \textit{wearing}. Edges can represent any dependency, allowing for abstract relationships to be included in the graph.

\textbf{Connecting Image and Conditioning Nodes.} With image and conditioning nodes defined, we are close to the complete HIG representation. To enable conditioning between the image and conditioning graphs, we must construct edges between the two. These connections are determined on a per-task basis, depending on the available data, with explicit choices described in Section 4. However, when spatial information is available i.e. segmentation masks or bounding boxes, it enables direct connections between the image graph and the conditioning graph. Specifically, edges are created between image nodes relevant to spatial conditionings (i.e. pixels within the bounding box) and conditioning nodes representing the corresponding semantic class (i.e. class label). This linkage facilitates information flow across the graphs, integrating pixel-level details with higher-level semantic representations. 

% Additionally, the flexibility of heterogeneous GNNs allows for connections from the image back to the graph with different sets of learned weights. This approach enables the image to influence the graph structure while leveraging the rich semantic details present in the image—such as color or object sub-class—throughout much of the diffusion training scheme, while still respecting the different types of information carried by the node types.

\subsection{Model Architecture}

To be compatible with the EDM2 U-Net architecture \footnote{\href{https://github.com/NVlabs/edm2}{https://github.com/NVlabs/edm2}}, we propose the addition of a magnitude-preserving \textit{Heterogenous Image Graph Neural Network} (HIGnn) as the conditioning network to be used in a ControlNet strategy.

\textbf{HIGnn.} The general architecture of the HIG conditioning block requires two primary capabilities: representation switching and HIG processing. To handle switching between image features and image nodes on the HIG we consider the update function $\mathcal{U}_{\text{i}\rightarrow\text{g}}$. This update functions reshapes image features $\mathbf{x_i} \in \mathbb{R}^{N \times C \times H \times W}$ into image nodes pixel wise $\mathbf{x_g} \in \mathbb{R}^{N\cdot H \cdot W \times C}$ and applies an optional projection to ensure correct dimensionality. For the current set of image pixels $\mathbf{x_i}$, we retrieve HIG image nodes $\mathbf{x_g}$ by
\begin{equation}
\mathbf{x_g} = \mathcal{U}_{\text{i}\rightarrow\text{g}}(\mathbf{x_i}) = \hat{W}R(\mathbf{x_i}),  
 \label{eq:HIG_update}
\end{equation}
where $R$ reshapes the image, and $\hat{W}$ is a learned projection with forced magnitude preservation from \cite{karras_analyzing_2024}. Refer to Appendix \ref{appendix:edm2_preliminaries} for greater detail into the mathematical preliminaries of \cite{karras_analyzing_2024}. We consider the reverse operation of converting from graph nodes to an image $\mathcal{U}_{\text{g}\rightarrow\text{i}}$ in a similiar fashion. 

Once we have the HIG updated with current image nodes we can process it with a GNN. We identify several areas where magnitudes can grow and address them each in turn. In practice many varieties of heterogenous message passing GNN could be used, we create our own magnitude preserving graph convolutional operator similiar to Hamilton et al. \cite{hamilton_inductive_2018} for its simplicity and stability. The basic approach propagates information through two branches, a pseudo `skip-connection' applied to the current node, and a learned pooling operation of the local neighbourhood, and we add the ability to include edge information in the neighbourhood pooling. If edge attributes $\mathbf{a}_i$ are present we integrate them via magnitude preserving concatenation to the pooling branch. Formally, the HIGConv operator applied per meta-path to get updated node embeddings $\mathbf{x}_i'$ is defined as:
% \begin{equation}
%     \mathbf{x}_i' = \psi\left(\hat{W}^{\Phi}_1 \mathbf{x}_i +^\text{mp} \hat{W}^{\Phi}_2 \cdot \frac{1}{\sqrt{|\mathcal{N}^{\Phi}|}} \sum_{j \in \mathcal{N}^{\Phi}(i)} [\mathbf{x}_j \|^\text{mp} \mathbf{a}_j] \right),
%     \label{eq:hignn_operator}
% \end{equation}
\begin{equation}
    \mathbf{x}_g' = \psi\left(\hat{W}^{\Phi}_1 \mathbf{x}_g 
    \underset{0 \text{ if } |\mathcal{N}^{\Phi}(i)| = 0}{\underbrace{+^\text{mp} \hat{W}^{\Phi}_2 \cdot \frac{1}{\sqrt{|\mathcal{N}^{\Phi}(i)|}} \sum_{j \in \mathcal{N}^{\Phi}(i)} [\mathbf{x}_j \|^\text{mp} \mathbf{a}_j]}}\right)    \label{eq:hignn_operator}
\end{equation}

% \[
%     \mathbf{x}_i' = \psi\left(\hat{W}^{\Phi}_1 \mathbf{x}_i +^\text{mp} 
%     \underset{+ 0 \text{ if } |\mathcal{N}^{\Phi}(i)| = 0}{\underbrace{\hat{W}^{\Phi}_2 \cdot \frac{1}{\sqrt{|\mathcal{N}^{\Phi}(i)|}} \sum_{j \in \mathcal{N}^{\Phi}(i)} [\mathbf{x}_j \|^\text{mp} \mathbf{a}_j]}}\right).
% \]

where we choose $\psi$ to be magnitude preserving SiLU operator, and $+^\text{mp}$ the magnitude preserving sum (See Appendix \ref{appendix:edm2_preliminaries}), and both meta-path weights $\hat{W}^{\Phi}_1$ and $\hat{W}^{\Phi}_2$ have forced magnitude. $\mathcal{N}$ indicates the local node neighbourhood and is defined by the connectivity of graph. In order to achieve magnitude preservation we first assume all neighbourhood features to be of unit length, we then summate them scale them by the square root of the neighbourhood size ($\sqrt{|\mathcal{N}^{\Phi}|}$), see Appendix \ref{appendix:sum_random} for details. It is important to address unconnected or `zero-degree' nodes, in this case we ignore the right hand side of the equation, and only take the residual path. Note that simply setting the  neighbourhood to zero unintentionally changes the feature magnitudes when mp-sum is applied, since it assumes both vectors to be of unit length. Finally to combine information across meta-paths, we use the same method and sum across paths before normalising by the inverse square root of the number of incoming meta-paths ($|\Phi_i| = |\{\Phi_k \mid x_i \in \Phi_k\}|$)

% To formulate a heterogeneous GNN with learned projections per meta-path ($\mathbf{\Phi} = \{\Phi_1 ... \Phi_n\}$), we must preserve magnitudes when combining meta-paths.

\begin{equation}
\Tilde{\mathbf{x}}_g = \frac{1}{\sqrt{|\Phi_g|}} \sum_{\Phi \in \Phi_g} \mathbf{x}'_g,
\label{eq:meta_path}
\end{equation}

We verify that this approach is guaranteed to maintain magnitudes under certain conditions of the underlying graph data. In particular, for graph-data of sufficient size this approach holds for graphs which do not have identical features attached to the same node since this breaks the independence assumption. 

% An interesting interpretation of this formulation with respect to image synthesis is to observe how different receptive fields change. The typical convolutional operator used in U-Net models define a local image receptive field $\mathcal{R}$, self-attention  defines a global image receptive field $\mathcal{A}$, and the HIGnn defines receptive fields over meta-path relationships $\mathcal{N}^{\Phi}$ for both the image and conditioning variables. We postulate this to an advantage over other conditioning methods as it allows instant communication between different conditioning signals and parts of the image whilst remaining computationally tractable.

\textbf{EDM2 ControlNet Integration.} To integrate conditioning into a generative model, we adopt a strategy similar to ControlNet \cite{zhang_adding_2023}, i.e. a frozen EDM2 pre-trained model, with a trainable copy the encoder integrated with the conditioning HIGnn. Refer to Figure \ref{fig:architecture} for an overview of our proposed architecture, we employ 4 HIG blocks for our base model. The EDM2 checkpoints are only available for class-conditional generation of the 1000 ImageNet classes, yet we find them easy to adapt to our natural image datasets.  To facilitate this we unfreeze the embedding network. To integrate features we adopt $1\times1$ convolutions with a learnable zero-gain in a similar fashion to the original ControlNet, but we note that traditional summation may damage feature magnitudes. We find that naively integrating is harmful to training. Instead, we apply magnitude preserving summation, which, in contrast to the original ControlNet paper, directly alters the primary network features. This yields poor generative quality at step 0, but proves to be quick to train and to be best in practice.

In the trainable encoder we integrate our proposed HIGnn after the initial convolution block. We opt to keep the dimension of the GNN matched to that of the generative model. Finally, to generate samples we opt for the non-stochastic EDM2 sampler, and use the recent advancements in auto-guidance \cite{karras_guiding_2024}, we use our control model as the primary network, and use the unconditional XS ImageNet checkpoint released with EDM2 as the guidance network \cite{karras_analyzing_2024, karras_guiding_2024}. 

% We do not use EMA