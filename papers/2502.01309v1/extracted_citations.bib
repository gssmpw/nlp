@misc{defferrard_convolutional_2017,
	title = {Convolutional {Neural} {Networks} on {Graphs} with {Fast} {Localized} {Spectral} {Filtering}},
	url = {http://arxiv.org/abs/1606.09375},
	doi = {10.48550/arXiv.1606.09375},
	abstract = {In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Defferrard, Michaël and Bresson, Xavier and Vandergheynst, Pierre},
	month = feb,
	year = {2017},
	note = {arXiv:1606.09375 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: NIPS 2016 final revision},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/98EV3V3W/Defferrard et al. - 2017 - Convolutional Neural Networks on Graphs with Fast .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/32PHVR9W/1606.html:text/html},
}

@misc{dhamo_semantic_2020,
	title = {Semantic {Image} {Manipulation} {Using} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/2004.03677},
	doi = {10.48550/arXiv.2004.03677},
	abstract = {Image manipulation can be considered a special case of image generation where the image to be produced is a modification of an existing image. Image generation and manipulation have been, for the most part, tasks that operate on raw pixels. However, the remarkable progress in learning rich image and object representations has opened the way for tasks such as text-to-image or layout-to-image generation that are mainly driven by semantics. In our work, we address the novel problem of image manipulation from scene graphs, in which a user can edit images by merely applying changes in the nodes or edges of a semantic graph that is generated from the image. Our goal is to encode image information in a given constellation and from there on generate new constellations, such as replacing objects or even changing relationships between objects, while respecting the semantics and style from the original image. We introduce a spatio-semantic scene graph network that does not require direct supervision for constellation changes or image edits. This makes it possible to train the system from existing real-world datasets with no additional annotation effort.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Dhamo, Helisa and Farshad, Azade and Laina, Iro and Navab, Nassir and Hager, Gregory D. and Tombari, Federico and Rupprecht, Christian},
	month = apr,
	year = {2020},
	note = {arXiv:2004.03677 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2020},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/K2SR7QEQ/Dhamo et al. - 2020 - Semantic Image Manipulation Using Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LWD34PBA/2004.html:text/html},
}

@misc{dhariwal_diffusion_2021,
	title = {Diffusion {Models} {Beat} {GANs} on {Image} {Synthesis}},
	url = {http://arxiv.org/abs/2105.05233},
	doi = {10.48550/arXiv.2105.05233},
	abstract = {We show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models. We achieve this on unconditional image synthesis by finding a better architecture through a series of ablations. For conditional image synthesis, we further improve sample quality with classifier guidance: a simple, compute-efficient method for trading off diversity for fidelity using gradients from a classifier. We achieve an FID of 2.97 on ImageNet 128\${\textbackslash}times\$128, 4.59 on ImageNet 256\${\textbackslash}times\$256, and 7.72 on ImageNet 512\${\textbackslash}times\$512, and we match BigGAN-deep even with as few as 25 forward passes per sample, all while maintaining better coverage of the distribution. Finally, we find that classifier guidance combines well with upsampling diffusion models, further improving FID to 3.94 on ImageNet 256\${\textbackslash}times\$256 and 3.85 on ImageNet 512\${\textbackslash}times\$512. We release our code at https://github.com/openai/guided-diffusion},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Dhariwal, Prafulla and Nichol, Alex},
	month = jun,
	year = {2021},
	note = {arXiv:2105.05233 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, read},
	annote = {Comment: Added compute requirements, ImageNet 256\${\textbackslash}times\$256 upsampling FID and samples, DDIM guided sampler, fixed typos},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/YRQ8P9WD/Dhariwal and Nichol - 2021 - Diffusion Models Beat GANs on Image Synthesis.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6XAVLAR9/2105.html:text/html},
}

@misc{farshad_scenegenie_2023,
	title = {{SceneGenie}: {Scene} {Graph} {Guided} {Diffusion} {Models} for {Image} {Synthesis}},
	shorttitle = {{SceneGenie}},
	url = {http://arxiv.org/abs/2304.14573},
	doi = {10.48550/arXiv.2304.14573},
	abstract = {Text-conditioned image generation has made significant progress in recent years with generative adversarial networks and more recently, diffusion models. While diffusion models conditioned on text prompts have produced impressive and high-quality images, accurately representing complex text prompts such as the number of instances of a specific object remains challenging. To address this limitation, we propose a novel guidance approach for the sampling process in the diffusion model that leverages bounding box and segmentation map information at inference time without additional training data. Through a novel loss in the sampling process, our approach guides the model with semantic features from CLIP embeddings and enforces geometric constraints, leading to high-resolution images that accurately represent the scene. To obtain bounding box and segmentation map information, we structure the text prompt as a scene graph and enrich the nodes with CLIP embeddings. Our proposed model achieves state-of-the-art performance on two public benchmarks for image generation from scene graphs, surpassing both scene graph to image and text-based diffusion models in various metrics. Our results demonstrate the effectiveness of incorporating bounding box and segmentation map guidance in the diffusion model sampling process for more accurate text-to-image generation.},
	urldate = {2024-07-21},
	publisher = {arXiv},
	author = {Farshad, Azade and Yeganeh, Yousef and Chi, Yu and Shen, Chengzhi and Ommer, Björn and Navab, Nassir},
	month = apr,
	year = {2023},
	note = {arXiv:2304.14573 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/PI73F66N/Farshad et al. - 2023 - SceneGenie Scene Graph Guided Diffusion Models fo.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CWVD8P6Q/2304.html:text/html},
}

@misc{han_vision_2022,
	title = {Vision {GNN}: {An} {Image} is {Worth} {Graph} of {Nodes}},
	shorttitle = {Vision {GNN}},
	url = {http://arxiv.org/abs/2206.00272},
	doi = {10.48550/arXiv.2206.00272},
	abstract = {Network architecture plays a key role in the deep learning-based computer vision system. The widely-used convolutional neural network and transformer treat the image as a grid or sequence structure, which is not flexible to capture irregular and complex objects. In this paper, we propose to represent the image as a graph structure and introduce a new Vision GNN (ViG) architecture to extract graph-level feature for visual tasks. We first split the image to a number of patches which are viewed as nodes, and construct a graph by connecting the nearest neighbors. Based on the graph representation of images, we build our ViG model to transform and exchange information among all the nodes. ViG consists of two basic modules: Grapher module with graph convolution for aggregating and updating graph information, and FFN module with two linear layers for node feature transformation. Both isotropic and pyramid architectures of ViG are built with different model sizes. Extensive experiments on image recognition and object detection tasks demonstrate the superiority of our ViG architecture. We hope this pioneering study of GNN on general visual tasks will provide useful inspiration and experience for future research. The PyTorch code is available at https://github.com/huawei-noah/Efficient-AI-Backbones and the MindSpore code is available at https://gitee.com/mindspore/models.},
	urldate = {2024-04-09},
	publisher = {arXiv},
	author = {Han, Kai and Wang, Yunhe and Guo, Jianyuan and Tang, Yehui and Wu, Enhua},
	month = nov,
	year = {2022},
	note = {arXiv:2206.00272 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: NeurIPS 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/MENJE2US/Han et al. - 2022 - Vision GNN An Image is Worth Graph of Nodes.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/DZMV766N/2206.html:text/html},
}

@misc{ho_denoising_2020,
	title = {Denoising {Diffusion} {Probabilistic} {Models}},
	url = {http://arxiv.org/abs/2006.11239},
	doi = {10.48550/arXiv.2006.11239},
	abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
	month = dec,
	year = {2020},
	note = {arXiv:2006.11239 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/X2PGPYUH/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/BAM228DE/2006.html:text/html},
}

@misc{johnson_image_2018,
	title = {Image {Generation} from {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1804.01622},
	doi = {10.48550/arXiv.1804.01622},
	abstract = {To truly understand the visual world our models should be able not only to recognize images but also generate them. To this end, there has been exciting recent progress on generating images from natural language descriptions. These methods give stunning results on limited domains such as descriptions of birds or flowers, but struggle to faithfully reproduce complex sentences with many objects and relationships. To overcome this limitation we propose a method for generating images from scene graphs, enabling explicitly reasoning about objects and their relationships. Our model uses graph convolution to process input graphs, computes a scene layout by predicting bounding boxes and segmentation masks for objects, and converts the layout to an image with a cascaded refinement network. The network is trained adversarially against a pair of discriminators to ensure realistic outputs. We validate our approach on Visual Genome and COCO-Stuff, where qualitative results, ablations, and user studies demonstrate our method's ability to generate complex images with multiple objects.},
	urldate = {2024-08-31},
	publisher = {arXiv},
	author = {Johnson, Justin and Gupta, Agrim and Fei-Fei, Li},
	month = apr,
	year = {2018},
	note = {arXiv:1804.01622 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: To appear at CVPR 2018},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/VPJCYR3V/Johnson et al. - 2018 - Image Generation from Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/8U5BYNIX/1804.html:text/html},
}

@misc{karras_analyzing_2024,
	title = {Analyzing and {Improving} the {Training} {Dynamics} of {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2312.02696},
	doi = {10.48550/arXiv.2312.02696},
	abstract = {Diffusion models currently dominate the field of data-driven image synthesis with their unparalleled scaling to large datasets. In this paper, we identify and rectify several causes for uneven and ineffective training in the popular ADM diffusion model architecture, without altering its high-level structure. Observing uncontrolled magnitude changes and imbalances in both the network activations and weights over the course of training, we redesign the network layers to preserve activation, weight, and update magnitudes on expectation. We find that systematic application of this philosophy eliminates the observed drifts and imbalances, resulting in considerably better networks at equal computational complexity. Our modifications improve the previous record FID of 2.41 in ImageNet-512 synthesis to 1.81, achieved using fast deterministic sampling. As an independent contribution, we present a method for setting the exponential moving average (EMA) parameters post-hoc, i.e., after completing the training run. This allows precise tuning of EMA length without the cost of performing several training runs, and reveals its surprising interactions with network architecture, training time, and guidance.},
	urldate = {2024-09-01},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Lehtinen, Jaakko and Hellsten, Janne and Aila, Timo and Laine, Samuli},
	month = mar,
	year = {2024},
	note = {arXiv:2312.02696 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QYY3L4C9/Karras et al. - 2024 - Analyzing and Improving the Training Dynamics of D.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/WECXB7UR/2312.html:text/html},
}

@misc{karras_elucidating_2022,
	title = {Elucidating the {Design} {Space} of {Diffusion}-{Based} {Generative} {Models}},
	url = {http://arxiv.org/abs/2206.00364},
	doi = {10.48550/arXiv.2206.00364},
	abstract = {We argue that the theory and practice of diffusion-based generative models are currently unnecessarily convoluted and seek to remedy the situation by presenting a design space that clearly separates the concrete design choices. This lets us identify several changes to both the sampling and training processes, as well as preconditioning of the score networks. Together, our improvements yield new state-of-the-art FID of 1.79 for CIFAR-10 in a class-conditional setting and 1.97 in an unconditional setting, with much faster sampling (35 network evaluations per image) than prior designs. To further demonstrate their modular nature, we show that our design changes dramatically improve both the efficiency and quality obtainable with pre-trained score networks from previous work, including improving the FID of a previously trained ImageNet-64 model from 2.07 to near-SOTA 1.55, and after re-training with our proposed improvements to a new SOTA of 1.36.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Karras, Tero and Aittala, Miika and Aila, Timo and Laine, Samuli},
	month = oct,
	year = {2022},
	note = {arXiv:2206.00364 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NeurIPS 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/BP5TIQBN/Karras et al. - 2022 - Elucidating the Design Space of Diffusion-Based Ge.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/LSD3JSB8/2206.html:text/html},
}

@inproceedings{krzywda_graph_2022,
	title = {Graph {Neural} {Networks} in {Computer} {Vision} -- {Architectures}, {Datasets} and {Common} {Approaches}},
	url = {http://arxiv.org/abs/2212.10207},
	doi = {10.1109/IJCNN55064.2022.9892658},
	abstract = {Graph Neural Networks (GNNs) are a family of graph networks inspired by mechanisms existing between nodes on a graph. In recent years there has been an increased interest in GNN and their derivatives, i.e., Graph Attention Networks (GAT), Graph Convolutional Networks (GCN), and Graph Recurrent Networks (GRN). An increase in their usability in computer vision is also observed. The number of GNN applications in this field continues to expand; it includes video analysis and understanding, action and behavior recognition, computational photography, image and video synthesis from zero or few shots, and many more. This contribution aims to collect papers published about GNN-based approaches towards computer vision. They are described and summarized from three perspectives. Firstly, we investigate the architectures of Graph Neural Networks and their derivatives used in this area to provide accurate and explainable recommendations for the ensuing investigations. As for the other aspect, we also present datasets used in these works. Finally, using graph analysis, we also examine relations between GNN-based studies in computer vision and potential sources of inspiration identified outside of this field.},
	urldate = {2024-09-05},
	booktitle = {2022 {International} {Joint} {Conference} on {Neural} {Networks} ({IJCNN})},
	author = {Krzywda, Maciej and Łukasik, Szymon and Gandomi, Amir H.},
	month = jul,
	year = {2022},
	note = {arXiv:2212.10207 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	pages = {1--10},
	annote = {Comment: 2022 International Joint Conference on Neural Networks (IJCNN), 2022},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/AGKJZRCY/Krzywda et al. - 2022 - Graph Neural Networks in Computer Vision -- Archit.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/YW6MCIPB/2212.html:text/html},
}

@misc{li_controlnet_2024,
	title = {{ControlNet}++: {Improving} {Conditional} {Controls} with {Efficient} {Consistency} {Feedback}},
	shorttitle = {{ControlNet}++},
	url = {http://arxiv.org/abs/2404.07987},
	doi = {10.48550/arXiv.2404.07987},
	abstract = {To enhance the controllability of text-to-image diffusion models, existing efforts like ControlNet incorporated image-based conditional controls. In this paper, we reveal that existing methods still face significant challenges in generating images that align with the image conditional controls. To this end, we propose ControlNet++, a novel approach that improves controllable generation by explicitly optimizing pixel-level cycle consistency between generated images and conditional controls. Specifically, for an input conditional control, we use a pre-trained discriminative reward model to extract the corresponding condition of the generated images, and then optimize the consistency loss between the input conditional control and extracted condition. A straightforward implementation would be generating images from random noises and then calculating the consistency loss, but such an approach requires storing gradients for multiple sampling timesteps, leading to considerable time and memory costs. To address this, we introduce an efficient reward strategy that deliberately disturbs the input images by adding noise, and then uses the single-step denoised images for reward fine-tuning. This avoids the extensive costs associated with image sampling, allowing for more efficient reward fine-tuning. Extensive experiments show that ControlNet++ significantly improves controllability under various conditional controls. For example, it achieves improvements over ControlNet by 11.1\% mIoU, 13.4\% SSIM, and 7.6\% RMSE, respectively, for segmentation mask, line-art edge, and depth conditions. All the code, models, demo and organized data have been open sourced on our Github Repo.},
	urldate = {2024-09-02},
	publisher = {arXiv},
	author = {Li, Ming and Yang, Taojiannan and Kuang, Huafeng and Wu, Jie and Wang, Zhaoning and Xiao, Xuefeng and Chen, Chen},
	month = jul,
	year = {2024},
	note = {arXiv:2404.07987 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence},
	annote = {Comment: Camera Ready Version. Project Page: https://liming-ai.github.io/ControlNet\_Plus\_Plus; Code \& Data: https://github.com/liming-ai/ControlNet\_Plus\_Plus},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/LC5UT99S/Li et al. - 2024 - ControlNet++ Improving Conditional Controls with .pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/CHDLFYZ5/2404.html:text/html},
}

@article{liu_cnn-enhanced_2021,
	title = {{CNN}-{Enhanced} {Graph} {Convolutional} {Network} {With} {Pixel}- and {Superpixel}-{Level} {Feature} {Fusion} for {Hyperspectral} {Image} {Classification}},
	volume = {59},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/document/9268479},
	doi = {10.1109/TGRS.2020.3037361},
	abstract = {Recently, the graph convolutional network (GCN) has drawn increasing attention in the hyperspectral image (HSI) classification. Compared with the convolutional neural network (CNN) with fixed square kernels, GCN can explicitly utilize the correlation between adjacent land covers and conduct flexible convolution on arbitrarily irregular image regions; hence, the HSI spatial contextual structure can be better modeled. However, to reduce the computational complexity and promote the semantic structure learning of land covers, GCN usually works on superpixel-based nodes rather than pixel-based nodes; thus, the pixel-level spectral–spatial features cannot be captured. To fully leverage the advantages of the CNN and GCN, we propose a heterogeneous deep network called CNN-enhanced GCN (CEGCN), in which CNN and GCN branches perform feature learning on small-scale regular regions and large-scale irregular regions, and generate complementary spectral–spatial features at pixel and superpixel levels, respectively. To alleviate the structural incompatibility of the data representation between the Euclidean data-oriented CNN and non-Euclidean data-oriented GCN, we propose the graph encoder and decoder to propagate features between image pixels and graph nodes, thus enabling the CNN and GCN to collaborate in a single network. In contrast to other GCN-based methods that encode HSI into a graph during preprocessing, we integrate the graph encoding process into the network and learn edge weights from training data, which can promote the node feature learning and make the graph more adaptive to HSI content. Extensive experiments on three data sets demonstrate that the proposed CEGCN is both qualitatively and quantitatively competitive compared with other state-of-the-art methods.},
	number = {10},
	urldate = {2024-09-09},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Liu, Qichao and Xiao, Liang and Yang, Jingxiang and Wei, Zhihui},
	month = oct,
	year = {2021},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Computational modeling, Convolution, Convolutional neural network (CNN), Decoding, Deep learning, Feature extraction, feature fusion, graph convolutional network (GCN), graph encoder and decoder, hyperspectral image (HSI) classification, Kernel, Training data},
	pages = {8657--8671},
	file = {IEEE Xplore Abstract Record:/Users/rupertmenneer/Zotero/storage/7CB7PRW6/9268479.html:text/html;IEEE Xplore Full Text PDF:/Users/rupertmenneer/Zotero/storage/YCAFY699/Liu et al. - 2021 - CNN-Enhanced Graph Convolutional Network With Pixe.pdf:application/pdf},
}

@misc{mittal_interactive_2019,
	title = {Interactive {Image} {Generation} {Using} {Scene} {Graphs}},
	url = {http://arxiv.org/abs/1905.03743},
	doi = {10.48550/arXiv.1905.03743},
	abstract = {Recent years have witnessed some exciting developments in the domain of generating images from scene-based text descriptions. These approaches have primarily focused on generating images from a static text description and are limited to generating images in a single pass. They are unable to generate an image interactively based on an incrementally additive text description (something that is more intuitive and similar to the way we describe an image). We propose a method to generate an image incrementally based on a sequence of graphs of scene descriptions (scene-graphs). We propose a recurrent network architecture that preserves the image content generated in previous steps and modifies the cumulative image as per the newly provided scene information. Our model utilizes Graph Convolutional Networks (GCN) to cater to variable-sized scene graphs along with Generative Adversarial image translation networks to generate realistic multi-object images without needing any intermediate supervision during training. We experiment with Coco-Stuff dataset which has multi-object images along with annotations describing the visual scene and show that our model significantly outperforms other approaches on the same dataset in generating visually consistent images for incrementally growing scene graphs.},
	urldate = {2024-09-03},
	publisher = {arXiv},
	author = {Mittal, Gaurav and Agrawal, Shubham and Agarwal, Anuva and Mehta, Sushant and Marwah, Tanya},
	month = may,
	year = {2019},
	note = {arXiv:1905.03743 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
	annote = {Comment: Published at ICLR 2019 Deep Generative Models for Highly Structured Data Workshop},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/ZTASFJ4J/Mittal et al. - 2019 - Interactive Image Generation Using Scene Graphs.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/6NTD22MK/1905.html:text/html},
}

@misc{peebles_scalable_2023,
	title = {Scalable {Diffusion} {Models} with {Transformers}},
	url = {http://arxiv.org/abs/2212.09748},
	doi = {10.48550/arXiv.2212.09748},
	abstract = {We explore a new class of diffusion models based on the transformer architecture. We train latent diffusion models of images, replacing the commonly-used U-Net backbone with a transformer that operates on latent patches. We analyze the scalability of our Diffusion Transformers (DiTs) through the lens of forward pass complexity as measured by Gflops. We find that DiTs with higher Gflops -- through increased transformer depth/width or increased number of input tokens -- consistently have lower FID. In addition to possessing good scalability properties, our largest DiT-XL/2 models outperform all prior diffusion models on the class-conditional ImageNet 512x512 and 256x256 benchmarks, achieving a state-of-the-art FID of 2.27 on the latter.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Peebles, William and Xie, Saining},
	month = mar,
	year = {2023},
	note = {arXiv:2212.09748 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, read},
	annote = {Comment: Code, project page and videos available at https://www.wpeebles.com/DiT},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/3E2GRMB8/Peebles and Xie - 2023 - Scalable Diffusion Models with Transformers.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/GPUNW48J/2212.html:text/html},
}

@misc{saharia_image_2021,
	title = {Image {Super}-{Resolution} via {Iterative} {Refinement}},
	url = {http://arxiv.org/abs/2104.07636},
	doi = {10.48550/arXiv.2104.07636},
	abstract = {We present SR3, an approach to image Super-Resolution via Repeated Refinement. SR3 adapts denoising diffusion probabilistic models to conditional image generation and performs super-resolution through a stochastic denoising process. Inference starts with pure Gaussian noise and iteratively refines the noisy output using a U-Net model trained on denoising at various noise levels. SR3 exhibits strong performance on super-resolution tasks at different magnification factors, on faces and natural images. We conduct human evaluation on a standard 8X face super-resolution task on CelebA-HQ, comparing with SOTA GAN methods. SR3 achieves a fool rate close to 50\%, suggesting photo-realistic outputs, while GANs do not exceed a fool rate of 34\%. We further show the effectiveness of SR3 in cascaded image generation, where generative models are chained with super-resolution models, yielding a competitive FID score of 11.3 on ImageNet.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Ho, Jonathan and Chan, William and Salimans, Tim and Fleet, David J. and Norouzi, Mohammad},
	month = jun,
	year = {2021},
	note = {arXiv:2104.07636 [cs, eess]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Electrical Engineering and Systems Science - Image and Video Processing, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/QVE9Z6PV/Saharia et al. - 2021 - Image Super-Resolution via Iterative Refinement.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/7HRQJXBA/2104.html:text/html},
}

@misc{saharia_photorealistic_2022,
	title = {Photorealistic {Text}-to-{Image} {Diffusion} {Models} with {Deep} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2205.11487},
	doi = {10.48550/arXiv.2205.11487},
	abstract = {We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g. T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment. See https://imagen.research.google/ for an overview of the results.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Saharia, Chitwan and Chan, William and Saxena, Saurabh and Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour, Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S. Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho, Jonathan and Fleet, David J. and Norouzi, Mohammad},
	month = may,
	year = {2022},
	note = {arXiv:2205.11487 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, read},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/VYD22Q4N/Saharia et al. - 2022 - Photorealistic Text-to-Image Diffusion Models with.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HFGA2FD3/2205.html:text/html},
}

@misc{sohl-dickstein_deep_2015,
	title = {Deep {Unsupervised} {Learning} using {Nonequilibrium} {Thermodynamics}},
	url = {http://arxiv.org/abs/1503.03585},
	doi = {10.48550/arXiv.1503.03585},
	abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
	urldate = {2024-06-29},
	publisher = {arXiv},
	author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
	month = nov,
	year = {2015},
	note = {arXiv:1503.03585 [cond-mat, q-bio, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Condensed Matter - Disordered Systems and Neural Networks, Quantitative Biology - Neurons and Cognition},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/EHJZ2EPV/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/FA4JIEGF/1503.html:text/html},
}

@misc{song_score-based_2021,
	title = {Score-{Based} {Generative} {Modeling} through {Stochastic} {Differential} {Equations}},
	url = {http://arxiv.org/abs/2011.13456},
	doi = {10.48550/arXiv.2011.13456},
	abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a known prior distribution by slowly injecting noise, and a corresponding reverse-time SDE that transforms the prior distribution back into the data distribution by slowly removing the noise. Crucially, the reverse-time SDE depends only on the time-dependent gradient field ({\textbackslash}aka, score) of the perturbed data distribution. By leveraging advances in score-based generative modeling, we can accurately estimate these scores with neural networks, and use numerical SDE solvers to generate samples. We show that this framework encapsulates previous approaches in score-based generative modeling and diffusion probabilistic modeling, allowing for new sampling procedures and new modeling capabilities. In particular, we introduce a predictor-corrector framework to correct errors in the evolution of the discretized reverse-time SDE. We also derive an equivalent neural ODE that samples from the same distribution as the SDE, but additionally enables exact likelihood computation, and improved sampling efficiency. In addition, we provide a new way to solve inverse problems with score-based models, as demonstrated with experiments on class-conditional generation, image inpainting, and colorization. Combined with multiple architectural improvements, we achieve record-breaking performance for unconditional image generation on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99 bits/dim, and demonstrate high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.},
	urldate = {2024-04-07},
	publisher = {arXiv},
	author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
	month = feb,
	year = {2021},
	note = {arXiv:2011.13456 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: ICLR 2021 (Oral)},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FMFAQUZ9/Song et al. - 2021 - Score-Based Generative Modeling through Stochastic.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/KCR276EC/2011.html:text/html},
}

@inproceedings{tarasiewicz_graph_2021,
	title = {A {Graph} {Neural} {Network} {For} {Multiple}-{Image} {Super}-{Resolution}},
	url = {https://ieeexplore.ieee.org/document/9506070},
	doi = {10.1109/ICIP42928.2021.9506070},
	abstract = {Super-resolution consists in reconstructing a high-resolution image from single or multiple low-resolution observations. Deep learning has been reported extremely successful for single-image super-resolution, but its applications to the multiple-image scenarios are limited due to the challenges that arise from feeding a network with a stack of images with sub-pixel translations. In this paper, we introduce Magnet—a new graph neural network that benefits from representing the input low-resolution images as a graph. This enables us to exploit the sub-pixel shifts among the input images while preserving the original low-resolution pixel values for feature extraction and information fusion. Despite a relatively simple architecture, Magnet outperforms the state-of-the-art methods for multiple-image super-resolution, and due to the flexible graph representation, it allows for using a variable number of low-resolution images for reconstruction.},
	urldate = {2024-09-05},
	booktitle = {2021 {IEEE} {International} {Conference} on {Image} {Processing} ({ICIP})},
	author = {Tarasiewicz, Tomasz and Nalepa, Jakub and Kawulok, Michal},
	month = sep,
	year = {2021},
	note = {ISSN: 2381-8549},
	keywords = {Conferences, deep learning, Deep learning, Feature extraction, graph neural networks, Graph neural networks, Image reconstruction, Magnetic resonance imaging, Super-resolution reconstruction, Superresolution},
	pages = {1824--1828},
	file = {IEEE Xplore Abstract Record:/Users/rupertmenneer/Zotero/storage/Z3WMBUAT/9506070.html:text/html},
}

@article{tian_image_nodate,
	title = {Image {Processing} {GNN}: {Breaking} {Rigidity} in {Super}-{Resolution}},
	abstract = {Super-Resolution (SR) reconstructs high-resolution images from low-resolution ones. CNNs and window-attention methods are two major categories of canonical SR models. However, these measures are rigid: in both operations, each pixel gathers the same number of neighboring pixels, hindering their effectiveness in SR tasks. Alternatively, we leverage the ﬂexibility of graphs and propose the Image Processing GNN (IPG) model to break the rigidity that dominates previous SR methods. Firstly, SR is unbalanced in that most reconstruction efforts are concentrated to a small proportion of detail-rich image parts. Hence, we leverage degree ﬂexibility by assigning higher node degrees to detail-rich image nodes. Then in order to construct graphs for SR-effective aggregation, we treat images as pixel node sets rather than patch nodes. Lastly, we hold that both local and global information are crucial for SR performance. In the hope of gathering pixel information from both local and global scales efﬁciently via ﬂexible graphs, we search node connections within nearby regions to construct local graphs; and ﬁnd connections within a strided sampling space of the whole image for global graphs. The ﬂexibility of graphs boosts the SR performance of the IPG model. Experiment results on various datasets demonstrates that the proposed IPG outperforms State-ofthe-Art baselines. Codes are available at this link.},
	language = {en},
	author = {Tian, Yuchuan and Chen, Hanting and Xu, Chao and Wang, Yunhe},
	file = {Tian et al. - Image Processing GNN Breaking Rigidity in Super-R.pdf:/Users/rupertmenneer/Zotero/storage/DSQRK35L/Tian et al. - Image Processing GNN Breaking Rigidity in Super-R.pdf:application/pdf},
}

@misc{wan_multi-scale_2019,
	title = {Multi-scale {Dynamic} {Graph} {Convolutional} {Network} for {Hyperspectral} {Image} {Classification}},
	url = {http://arxiv.org/abs/1905.06133},
	doi = {10.48550/arXiv.1905.06133},
	abstract = {Convolutional Neural Network (CNN) has demonstrated impressive ability to represent hyperspectral images and to achieve promising results in hyperspectral image classification. However, traditional CNN models can only operate convolution on regular square image regions with fixed size and weights, so they cannot universally adapt to the distinct local regions with various object distributions and geometric appearances. Therefore, their classification performances are still to be improved, especially in class boundaries. To alleviate this shortcoming, we consider employing the recently proposed Graph Convolutional Network (GCN) for hyperspectral image classification, as it can conduct the convolution on arbitrarily structured non-Euclidean data and is applicable to the irregular image regions represented by graph topological information. Different from the commonly used GCN models which work on a fixed graph, we enable the graph to be dynamically updated along with the graph convolution process, so that these two steps can be benefited from each other to gradually produce the discriminative embedded features as well as a refined graph. Moreover, to comprehensively deploy the multi-scale information inherited by hyperspectral images, we establish multiple input graphs with different neighborhood scales to extensively exploit the diversified spectral-spatial correlations at multiple scales. Therefore, our method is termed 'Multi-scale Dynamic Graph Convolutional Network' (MDGCN). The experimental results on three typical benchmark datasets firmly demonstrate the superiority of the proposed MDGCN to other state-of-the-art methods in both qualitative and quantitative aspects.},
	urldate = {2024-09-09},
	publisher = {arXiv},
	author = {Wan, Sheng and Gong, Chen and Zhong, Ping and Du, Bo and Zhang, Lefei and Yang, Jian},
	month = may,
	year = {2019},
	note = {arXiv:1905.06133 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/6H4BPGJY/Wan et al. - 2019 - Multi-scale Dynamic Graph Convolutional Network fo.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/HJI5NZXJ/1905.html:text/html},
}

@misc{xu_scene_2017,
	title = {Scene {Graph} {Generation} by {Iterative} {Message} {Passing}},
	url = {http://arxiv.org/abs/1701.02426},
	doi = {10.48550/arXiv.1701.02426},
	abstract = {Understanding a visual scene goes beyond recognizing individual objects in isolation. Relationships between objects also constitute rich semantic information about the scene. In this work, we explicitly model the objects and their relationships using scene graphs, a visually-grounded graphical structure of an image. We propose a novel end-to-end model that generates such structured scene representation from an input image. The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. Our joint inference model can take advantage of contextual cues to make better predictions on objects and their relationships. The experiments show that our model significantly outperforms previous methods for generating scene graphs using Visual Genome dataset and inferring support relations with NYU Depth v2 dataset.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Xu, Danfei and Zhu, Yuke and Choy, Christopher B. and Fei-Fei, Li},
	month = apr,
	year = {2017},
	note = {arXiv:1701.02426 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: CVPR 2017},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/63RNQBLZ/Xu et al. - 2017 - Scene Graph Generation by Iterative Message Passin.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/PHJTMV3U/1701.html:text/html},
}

@misc{yang_diffusion-based_2022,
	title = {Diffusion-{Based} {Scene} {Graph} to {Image} {Generation} with {Masked} {Contrastive} {Pre}-{Training}},
	url = {http://arxiv.org/abs/2211.11138},
	doi = {10.48550/arXiv.2211.11138},
	abstract = {Generating images from graph-structured inputs, such as scene graphs, is uniquely challenging due to the difficulty of aligning nodes and connections in graphs with objects and their relations in images. Most existing methods address this challenge by using scene layouts, which are image-like representations of scene graphs designed to capture the coarse structures of scene images. Because scene layouts are manually crafted, the alignment with images may not be fully optimized, causing suboptimal compliance between the generated images and the original scene graphs. To tackle this issue, we propose to learn scene graph embeddings by directly optimizing their alignment with images. Specifically, we pre-train an encoder to extract both global and local information from scene graphs that are predictive of the corresponding images, relying on two loss functions: masked autoencoding loss and contrastive loss. The former trains embeddings by reconstructing randomly masked image regions, while the latter trains embeddings to discriminate between compliant and non-compliant images according to the scene graph. Given these embeddings, we build a latent diffusion model to generate images from scene graphs. The resulting method, called SGDiff, allows for the semantic manipulation of generated images by modifying scene graph nodes and connections. On the Visual Genome and COCO-Stuff datasets, we demonstrate that SGDiff outperforms state-of-the-art methods, as measured by both the Inception Score and Fr{\textbackslash}'echet Inception Distance (FID) metrics. We will release our source code and trained models at https://github.com/YangLing0818/SGDiff.},
	urldate = {2024-07-21},
	publisher = {arXiv},
	author = {Yang, Ling and Huang, Zhilin and Song, Yang and Hong, Shenda and Li, Guohao and Zhang, Wentao and Cui, Bin and Ghanem, Bernard and Yang, Ming-Hsuan},
	month = nov,
	year = {2022},
	note = {arXiv:2211.11138 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code and models shall be released at https://github.com/YangLing0818/SGDiff},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/8A8QI2F7/Yang et al. - 2022 - Diffusion-Based Scene Graph to Image Generation wi.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/RXLS8TH5/2211.html:text/html},
}

@misc{yang_graph_2018,
	title = {Graph {R}-{CNN} for {Scene} {Graph} {Generation}},
	url = {http://arxiv.org/abs/1808.00191},
	doi = {10.48550/arXiv.1808.00191},
	abstract = {We propose a novel scene graph generation model called Graph R-CNN, that is both effective and efficient at detecting objects and their relations in images. Our model contains a Relation Proposal Network (RePN) that efficiently deals with the quadratic number of potential relations between objects in an image. We also propose an attentional Graph Convolutional Network (aGCN) that effectively captures contextual information between objects and relations. Finally, we introduce a new evaluation metric that is more holistic and realistic than existing metrics. We report state-of-the-art performance on scene graph generation as evaluated using both existing and our proposed metrics.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Yang, Jianwei and Lu, Jiasen and Lee, Stefan and Batra, Dhruv and Parikh, Devi},
	month = aug,
	year = {2018},
	note = {arXiv:1808.00191 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 16 pages, ECCV 2018 camera ready},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/FCXAS7JG/Yang et al. - 2018 - Graph R-CNN for Scene Graph Generation.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/7ZZKPC82/1808.html:text/html},
}

@misc{zhang_adding_2023,
	title = {Adding {Conditional} {Control} to {Text}-to-{Image} {Diffusion} {Models}},
	url = {http://arxiv.org/abs/2302.05543},
	doi = {10.48550/arXiv.2302.05543},
	abstract = {We present ControlNet, a neural network architecture to add spatial conditioning controls to large, pretrained text-to-image diffusion models. ControlNet locks the production-ready large diffusion models, and reuses their deep and robust encoding layers pretrained with billions of images as a strong backbone to learn a diverse set of conditional controls. The neural architecture is connected with "zero convolutions" (zero-initialized convolution layers) that progressively grow the parameters from zero and ensure that no harmful noise could affect the finetuning. We test various conditioning controls, eg, edges, depth, segmentation, human pose, etc, with Stable Diffusion, using single or multiple conditions, with or without prompts. We show that the training of ControlNets is robust with small ({\textless}50k) and large ({\textgreater}1m) datasets. Extensive results show that ControlNet may facilitate wider applications to control image diffusion models.},
	urldate = {2024-05-14},
	publisher = {arXiv},
	author = {Zhang, Lvmin and Rao, Anyi and Agrawala, Maneesh},
	month = nov,
	year = {2023},
	note = {arXiv:2302.05543 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Artificial Intelligence, Computer Science - Graphics, Computer Science - Human-Computer Interaction, Computer Science - Multimedia},
	annote = {Comment: Codes and Supplementary Material: https://github.com/lllyasviel/ControlNet},
	file = {arXiv Fulltext PDF:/Users/rupertmenneer/Zotero/storage/HMXN35GU/Zhang et al. - 2023 - Adding Conditional Control to Text-to-Image Diffus.pdf:application/pdf;arXiv.org Snapshot:/Users/rupertmenneer/Zotero/storage/MZ2ZAIEL/2302.html:text/html},
}

