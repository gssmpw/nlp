\begin{abstract}
We introduce a novel method for conditioning diffusion-based image synthesis models with heterogeneous graph data. Existing approaches typically incorporate conditioning variables directly into model architectures, either through cross-attention layers that attend to text latents or image concatenation that spatially restrict generation. However, these methods struggle to handle complex scenarios involving diverse, relational conditioning variables, which are more naturally represented as unstructured graphs. This paper presents Heterogeneous Image Graphs (HIG), a novel representation that models conditioning variables and target images as two interconnected graphs, enabling efficient handling of variable-length conditioning inputs and their relationships. We also propose a magnitude-preserving GNN that integrates the HIG into the existing EDM2 diffusion model using a ControlNet approach. Our approach improves upon the SOTA on a variety of conditioning inputs for the COCO-stuff and Visual Genome datasets, and showcases the ability to condition on graph attributes and relationships represented by edges in the HIG.  
\end{abstract}

% Conditional diffusion models are the de facto gold standard in image synthesis. Typically, conditioning variables are incorporated directly into the model architecture, for example cross-attention layers that attend to text latents from pre-trained language models, or image concatenation that spatially restricts generation. However, in scenarios with diverse conditioning variables that may have underlying relationships, there is no flexible, expressive, and computationally efficient way to integrate this into image synthesis models in a simple and generalised manner.

% This paper presents a novel approach to incorporating spatial graph-conditioning into diffusion models for image synthesis. We introduce a new architecture that combines the existing EDM2 U-Net with Graph Neural Networks using a novel Heterogenous Image Graph (HIG) representation. This representation considers two interconnected graphs: one representing the target image, the other representing conditioning variables. This enables the seamless integration of heterogeneous, sparse, and variable-length conditioning graphs, including those with diverse meta-paths that capture complex relationships between conditioning variables and the image. We demonstrate this to be a highly effective approach that outperforms previous methods in mask-to-image and scene-graph conditioned image synthesis.

% ---

% We introduce a novel method for integrating diverse conditioning variables into diffusion-based image synthesis models. Existing approaches typically incorporate conditioning variables directly into the model via architectural modifications, such as cross-attention mechanisms for text conditioning or spatial concatenation for image conditioning. However, these methods struggle to generalize to scenarios with heterogeneous, relational conditioning variables.

% To address this, we propose a framework that combines the EDM2 U-Net architecture with Graph Neural Networks, leveraging a Heterogeneous Image Graph (HIG) representation. This representation models conditioning variables and target images as two interconnected graphs, enabling efficient handling of heterogeneous, sparse, and variable-length conditioning inputs. We offer seveeral GNN designs that align with the magnitude-preserving treatment introduced in EDM2, ensuring stable training dynamics and effective representation learning. Our approach naturally incorporates diverse meta-paths, capturing complex relationships between conditioning variables and image content.

% We evaluate this method on tasks such as mask-to-image and scene-graph conditioned synthesis, demonstrating state-of-the-art performance and showcasing its flexibility and efficacy in handling diverse, sparse and relational conditioning information.