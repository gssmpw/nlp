\section{Experiment Setup}
\label{sec:exp_setup}

We conduct experiments on three different tasks: 3D object selection task, open-vocabulary 3D object localization task, and open-vocabulary 3D semantic segmentation task. These tasks are closely related to the 3D search as described in Fig. 1 of the manuscript as well as the 3D scene understanding tasks~\cite{open_gaussian}.

\paragraph{3D object selection}
To evaluate the model's 3D awareness capability, we evaluate a 3D object selection task. We first extract text features from an open-vocabulary text query using the CLIP text encoder~\cite{clip}. Next, we compare these text features to the 3D Gaussian embeddings by computing the cosine similarity score. By thresholding the similarity, we identify the 3D Gaussians that are relevant to the given text query. The threshold value for each method is determined through a grid search to identify the optimal performance.

We use the LeRF-OVS dataset~\cite{lerf} with annotations by LangSplat~\cite{langsplat}. As the LeRF-OVS dataset lacks 3D ground truth, we follow the 2D segmentation-based evaluation method proposed by OpenGaussian~\cite{open_gaussian}. This approach evaluates 3D understanding by measuring multi-view 2D segmentation accuracy between the rendered occupancy mask from the selected 3D Gaussians and the GT object masks. Ground truth segmentation masks are manually annotated corresponding to text queries as described in~\cite{langsplat}. We evaluate the IoU and localization accuracy for the metric.

\paragraph{Open-vocabulary 3D object localization}
Given an open-vocabulary text query, we use the CLIP text encoder~\cite{clip} to extract a text feature of the given text query. Then, we compute the cosine similarity score between the query text feature and the Gaussian-registered embeddings. Finally, we select highly relevant 3D Gaussians by thresholding the obtained cosine similarities. We set the threshold of each method individually by searching the thresholds that show the best mIoU on the scenes used for evaluation.


\paragraph{Open-vocabulary 3D semantic segmentation}
We further evaluate our method using the open-vocabulary 3D semantic segmentation task. For a given set of open-vocabulary text queries representing categories, we use the CLIP text encoder to extract a language embedding for each query. We then compute the cosine similarity scores between the 3D Gaussian embeddings and the language features from the given text queries. Using the obtained cosine similarity scores, we assign each 3D Gaussian to the category with the highest the cosine similarity score.