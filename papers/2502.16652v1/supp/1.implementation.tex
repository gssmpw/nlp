\section{Implementation Details}
\label{sec:implementation}
Overall, our method consists of (1) a pre-processing stage that constructs the codebooks in Product Quantization, and pre-training of 3DGS, (2) a training stage that aggregates multiview CLIP embeddings into unified Gaussian-registered embeddings, and (3) Inference stage that directly referring to language-embedded 3D Gaussians for the downstream task.

\paragraph{Pre-Training stage} 
In the pre-processing stage, we need to extract per-patch CLIP embeddings to build PQ codebooks. 
It consists of a patch extraction step, and a CLIP embedding extraction step. 
To obtain patches, we utilize the LVIS dataset, a large-scale dataset having ground truth image segmentations. From the segmentation mask given in the LVIS dataset, we identify object regions and crop them into individual image patches. Each cropped patch is then processed and encoded using the OpenCLIP ViT-B/16 model. Based on these predictions, we continue to build PQ codebooks. We utilize FAISS~\cite{douze2024faiss} open-source library for our Product Quantization implementation.  
We use 128 sub-vectors per embedding, with each subvector assigned to one of 256 centroids, yielding an 8-bit index per subvector. This process is illustrated in \Fref{fig:product quantization}.

3D Gaussian parameters~$\Theta$~\cite{3dgs} are also optimized during the pre-processing stage. We typically care about this initialization of the 3D Gaussians, which can potentially impact the performance of the 3D scene understanding tasks. So, we follow the original 3D Gaussian Splatting method and utilize the optimized 3D Gaussians as our initial parameters. In other words, the pre-training is conducted using the default hyperparameters from 3DGS~\cite{3dgs} framework, running 30,000 iterations. Also, we consistently apply this paradigm across different methods for fair comparison. Especially, for LeGaussian~\cite{legaussian} that employs mutual training, we disabled 3D Gaussian updates during feature assignment in our experiments. 

\paragraph{Training stage}
Based on the PQ and the initial 3D Gaussian parameters~$\Theta$, we begin the training stage. All competing models and our proposed model are trained and evaluated on a single NVIDIA RTX A6000 GPU to ensure fair performance comparison. The training stage consists of three main steps: extracting pixel-wise CLIP embeddings from training images, the feature aggregation stage, and lastly feature registration stage.

Given multi-view images, we extract dense CLIP features assigned to each pixel. 
To obtain per-pixel CLIP features, we adopt the feature extraction scheme by LangSplat~\cite{langsplat}, which utilizes SAM~\cite{sam}. To collect per-patch embeddings, we followed the OpenGaussian framework and used a single-level mask, while LangSplat utilized multi-level masks. 

Once these CLIP features are extracted for all images, we proceed to the feature registration step.
In the feature registration step, we iteratively measure the contribution (weights) of pre-trained Gaussians for each ray assigned to the pixels in the training images, and update the 3D Gaussian embeddings. 
These weights are determined according to the volume rendering equation, which defines their influence during the color rasterization process (see Sec. 3.2 of the manuscript). After the registration process, we normalize the embeddings by dividing embeddings with L2 norms.

Lastly, we register aggregated features to 3D Gaussians. For memory efficiency, we quantize the aggregated Gaussians using the pre-trained PQ codebooks to encode features to indices, a set of 128-channel 8-bit integer indices (\Fref{fig:product quantization}). While registration, Gaussians that were never selected in the top-k process are pruned to reduce noise and memory consumption. At the end of the training, we retain a set of assigned Gaussians with 128 8-bit integer indices.


\paragraph{Inference stage}
Finally, in the inference stage, the PQ-assigned Gaussians from the previous steps are used. By recalling the PQ index list assigned to each 3D Gaussian, cosine similarity is computed between the embeddings of a given text query, extracted using the same CLIP encoder, and each 3D Gaussian. Detailed steps are provided in Sec. 3.3 of the manuscript. As the subvector norms do not sum to 1, normalization by the sum of the subvector L2 norms is applied. We can apply this by using the \texttt{search} function in the Faiss library. The resulting similarity scores are then used to perform various 3D tasks, as evaluated in the study. The following sections explain how the computed activation values are applied in each task.

