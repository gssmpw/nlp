\section{Additional Results}
\label{additional results}
In this section, we present additional results that are not shown in the manuscript due to space constraints. 

\subsection{Additional results on presented 3D tasks}
\label{3D tasks}
We first show more experimental results for the 3D object selection task in~\Fref{fig:3d_obj_sel}, and the 3D localization task in ~\Fref{fig:3d_loc} which are not included in the manuscript due to the space limit. Consistent with our earlier observations, the LangSplat model struggles to learn accurate 3D features. While it occasionally follows feature patterns, it frequently produces significant noise, making it unsuitable for real-world applications such as localization, object grabbing, or 3D image editing. Additionally, we observe persistent spatial bias in the OpenGaussian method, as previously noted, see red cup, plate, or wavy noodles, and bed cases in~\Fref{fig:3d_loc}, it fails to select relevant regions in others.
In contrast, our proposed method, which allows direct search and inference in 3D space, consistently identifies favorable localization performance. This demonstrates the robustness and practicality of our approach compared to competing methods.

In the Sec.\ref{metric}, we demonstrated that our metric provides superior volumetric alignment compared to existing approaches. To further validate the superiority of our model, we also evaluated its performance using the metric proposed by OpenGaussian. We confirm that our method outperforms even using other evaluation protocols as shown in~\Tref{table:metric_comparison}

\input{supp_figures/S5_3d_object_selection}
\input{supp_figures/S6_3d_localization}

\subsection{Experiments on the ScanNet200 dataset}
\label{scannet200}
The proposed model and its counterparts are designed to operate effectively in open-vocabulary settings. 
To evaluate performance under more comprehensive open-vocabulary cases, we conducted additional experiments using the ScanNet-200 annotation~\cite{scannet200}, which extends the ScanNet limited-label of 20 to 200 semantic categories, including tail categories such as armchair and windowsill. These rare classes provide a closer approximation to real-world scenarios and enable a robust assessment of the models' generalization capabilities. For consistency, experiments are conducted using the same scenes as previous benchmarks, following ground truth annotations as described in Sec.~\ref{metric}. 

The results, summarized in~\Tref{table:scannet_200}, demonstrate that the proposed model consistently outperforms its counterparts, which highlights superior generalization across diverse object spaces. The results validate the proposed model's ability to excel across both constrained and diverse object spaces, emphasizing its potential for practical application in complex real-world scenarios.

\input{supp_tables/scannet_200}

\subsection{Experiments on the city-scale dataset}
\label{city scale}
\input{supp_figures/large_scene/S7_langsplat_comp}
\input{supp_figures/large_scene/S8_green_red_comp}
\input{supp_figures/large_scene/S9_large_scene}

The proposed method is further evaluated in a large-scale scenario using the Waymo San Francisco Mission Bay dataset~\cite{tancik2022blocknerf}, which features expansive spatial contexts. For each scene, the dataset comprises approximately 12,000 images captured by 12 cameras, providing a challenging and diverse testing environment for 3D localization tasks. We select 3 blocks of the scene for large-scale scene tests.

We conducted comparisons against the LangSplat-m model for the 3D text-query localization task as shown in~\Fref{fig:langsplat_comp}. Our evaluation focused on qualitatively assessing how well each model performs in localizing queries within the 3D space. Our method consistently succeeds in localizing diverse text queries, demonstrating robust and accurate performance across various contexts. In contrast, LangSplat-m struggles to make precise predictions, particularly with its 3D Gaussian representations failing to align with the expected ground-truth values. These findings are consistent with our earlier observations regarding the limitations of LangSplat-m's approach.

As shown in~\Fref{fig:green_red_comp}, we can see that the results reflect not only objects, but also attributes like color to some extent. Additional visualizations of the results can be found in~\Fref{fig:large_scene} and the supplementary video, which provides a more comprehensive view of the qualitative differences between the methods. We strongly encourage readers to refer to these supplementary materials for further insights.

The differences between the methods become even more pronounced when considering search speed in large-scale scenarios. 
For example, the Waymo dataset contains over 2.9M Gaussians, with individual images requiring nearly 1M computations per image for over 100 images. The computational efficiency of the proposed method allows it to handle such large-scale data more effectively, highlighting its scalability and practical applicability in real-world scenarios.