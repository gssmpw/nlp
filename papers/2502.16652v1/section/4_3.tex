\paragraph{Product Quantization}
PQ~\cite{product_quantization} is a widely used technique for efficient embedding compression, particularly valuable in large-scale applications.
The PQ process begins by dividing the original $D$-dimensional feature vector $\mathbf{v}$ into $L$ sub-vectors: $\mathbf{v} = [\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_L]$. 
Each sub-vector $\mathbf{v}_i$ is then independently quantized to a predefined number of centroids $s_{ij}$ in a predefined codebook $S_i$ for that sub-vector. These centroids are learned via clustering, creating a codebook for each subspace. Once the centroids are established, each sub-vector is replaced by the index of the nearest centroid in its respective codebook. 
The centroid indices~$j_i = [j_{i1}, j_{i2}, \ldots, j_{iL}]$ are optimized by minimizing $\mathop{\arg\min}_{k} \lVert \mathbf{v}_i - s_{ik} \rVert$ to quantize a given vector $\mathbf{v}_i$ where $j_{ik}$ is an 8-bit unsigned integer. 

Then, we can measure the distance between the query and data by adding distances between coarse centroids. Once the distances between centroids are computed as a lookup table, the computation shifts to simple indexing, which reduces the search complexity from $\mathcal{O}(D)$ to $\mathcal{O}(1)$ for a $D$ dimension sample. This approach notably reduces computational complexity, making it suitable for large-scale search.

In our setup for language-based 3D scene understanding,
we build PQ centroids based on CLIP embeddings using a large-scale image dataset, the LVIS dataset~\cite{lvis_dataset}, that contains over $1.2$M instances covering various long-tail classes and ground truth segmentation. We extract instance patches from images and collect patch-wise CLIP embeddings. After we build this CLIP embedding database, we proceed with the construction of the centroid codebook for our PQ. Once PQ is trained, any query embedding can be approximated by assigning the closest centroid for each subvector. This is a one-time procedure; once we determine the codebook, we can use it for any scene generally.
In our setup, each embedding is represented as a sequence of centroid indices rather than a high-dimensional vector. Accordingly, our language embedded Gaussians are parametrized as $\Phi^\text{ours} = \{ \phi_i^\text{ours} \}_{i=1}^N = \{ \theta_i, j_i \}_{i=1}^N$. where the aggregated feature~$\dot{\mathbf{f}}_i$ are converted as a quantized feature~$\bar{\mathbf{f}}_i$ by the corresponding PQ index $j_i$.



\input{figures/6-loc_and_seg/6-2-seg}



\subsection{Text-query based 3D localization}
\label{subsec:4_3}

After training 3D Gaussians~$\Phi^\text{ours}$ with our feature registration process and PQ, we describe the details of an inference mode that facilitates direct interaction with 3DGS upon receiving input queries, such as text. 
This is related to similarity score computation between a query and sources, \ie Gaussian embeddings. Given a text, we first extract a query feature~$\mathbf{q}$ using CLIP text encoder~\cite{clip}. We reconstruct the quantized features~$\{ \bar{\mathbf{f}}_i \}_{i=1}^N$ from the stored PQ indices~$\{ j_i \}_{i=1}^N$. Then, we compute a cosine similarity score between the query feature~$\mathbf{q}$ and all quantized features.

Despite its simplicity, solely relying on the cosine similarity may result in diminished discriminability across certain similarity scores.
 
To address this limitation, we incorporate a re-ranking process based on relative activation with respect to the canonical feature. For this process, we adopt the relevancy scoring method proposed in LeRF~\cite{lerf}, which enables more precise similarity analysis for a query.
Specifically, each rendered language embedding, $\featmap$ and a text query feature~$\mathbf{q}$, yield a relevance score determined by, 
    $\mathop{\min}_i \frac{
        \exp(\featmap \cdot \mathbf{q} )
    }{
        \exp(\featmap \cdot \mathbf{q} ) + \exp(\featmap \cdot \mathbf{f}^\text{canon, i})
    },$
where $(\cdot)$ is an element-wise dot product operator and $\mathbf{f}^\text{canon,i}$ indicates CLIP embeddings of a designated canonical term selected from a set of ``object,'' ``things,'' ``stuff,'' and ``texture''. Then, we sample 3D Gaussians based on the relevance score for downstream tasks.