\subsection{3D object selection}
\label{subsec:5_1}

\noindentbold{Settings}
We first extract text features from an open-vocabulary text query using the CLIP model. Next, we compare text features to the 3D features embedded in each Gaussian using cosine similarity. By thresholding the similarity, we identify the 3D Gaussians that are relevant to the given text query. The selected 3D points are subsequently rendered into multi-view images using the 3DGS rasterization pipeline. 

\noindentbold{Results}
We compare our model quantitatively with 3DGS-based language-embedded models as shown in~\Tref{table:obj_sel}. 
The results demonstrate that our method performs better object selection in most scenes, showing an improvement of over 0.5 in mIoU and more than 4.5 in mAcc compared to counterpart models. Notably, the rasterization-based method, LangSplat-m, often underperforms in most scenes.

Qualitative results are shown in~\Fref{fig:5_obj_sel}. 
For LangSplat-m, the activations often shows random 3D Gaussians or fail to localize entirely (\eg, see ``coffee mug''), highlighting the limitations of rasterization-based methods and their unsuitability for 3D understanding, aligning the observation from~\cref{fig:motivation}. OpenGaussian frequently exhibits false activations with incorrect text-object pairs (\eg, ``apple'' and ``tea in a glass'') and struggles to distinguish between nearby objects (\eg, ``waldo,'' ``rubik's cube'').
This artifacts can be attributed to use of spatial clustering and limited encoder capacity.

In contrast, our model leverages general image features thanks to the general PQ, maintaining feature distinctiveness regardless of scene complexity.
Our feature registration considers the 3D geometry of the 3D Gaussians, which results in superior performance in 3D scene understanding tasks.

