\subsection{Feature registration process}
\label{subsec:4_1}

Our goal is to reconstruct a language embedded 3D space represented by 3D Gaussians~$\Phi$, which we can directly interact in 3D space without feature rendering~\cref{eq:feat_render_in_3dgs}. For that, following LangSplat~\cite{langsplat}, we begin by extracting per-pixel CLIP embedding maps $\mathbf{F}^\text{map} \in \mathbb{R}^{D\times H \times W}$ from training images of the target scenes, where $D$ is the dimension of CLIP embeddings, $H$ and $W$ are the height and width of the training images. Given training images, we extracts a dictionary of binary masks and language embeddings extracted from the images as: $\mathcal{F}^\text{map} = \{\mathbf{M}_j : \mathbf{f}^\text{map}_j \ |\ j=1,...,M\}$, where $\mathbf{M}_j\in \mathbb{R}^{H \times W}$ is a binary mask extracted using SAM~\cite{sam} and $\mathbf{f}^\text{map}_j \in \mathbb{R}^{D}$ is a corresponding CLIP embedding 
from
a cropped image with $\mathbf{M}_j$.
Each mask $\mathbf{M}_j$ belongs to an image, and the masks are not overlapped to each other. 
With this dictionary, a CLIP embedding map~$\mathbf{F}^\text{map}(\mathbf{I}, \mathbf{r})$ at a pixel $\mathbf{r}$ in a training image $\mathbf{I}$ is computed as:
\begin{equation}
    \mathbf{F}^\text{map}(\mathbf{I}, \mathbf{r}) = \sum\nolimits_{j=1}^M\mathbf{M}_j(\mathbf{I}, \mathbf{r})\cdot\mathbf{f}^\text{map}_j,
\end{equation}
where $\mathbf{M}_j(\mathbf{I}, \mathbf{r}) \in \{0,1\}$ indicates whether the mask $\mathbf{M}_j$ contains the pixel $\mathbf{r}$ in the image $\mathbf{I}$. 
Using $\mathbf{F}^\text{map}$, we reconstruct language embedded 3D Gaussians via a novel feature registration process as visualized in~\cref{fig:overview}. 

During the feature registration process, our algorithm iterates through training images of the scene. Using projection relation, we link 3D Gaussians~$\Phi$ to CLIP embeddings. Each Gaussian can link to multiple CLIP embeddings derived from different images. Then we aggregate collected embeddings to a single embedding to be assigned to each Gaussian.
To ensure a consistent aggregation of the embeddings from multi-view images, we first compute a weight $w_i(\mathbf{I}, \mathbf{r})$ representing the contribution of $\theta_i$ to construct each pixel $\mathbf{r}$ in a training image $\mathbf{I}$.
The weights are computed with the volume rendering equation \Eref{eq:vol_render_in_3dgs} as:
\begin{equation}
    w_i(\mathbf{I}, \mathbf{r}) = T_i(\mathbf{I}, \mathbf{r})\cdot\tilde{\alpha_i}(\mathbf{I}, \mathbf{r}),
    \label{eq:weight definition}
\end{equation}
where $T_i(\mathbf{I}, \mathbf{r})$ and $\tilde{\alpha_i}(\mathbf{I}, \mathbf{r})$ are the transmittance and the effective opacity value of $\theta_i$ for a pixel $\mathbf{r}$ in an image $\mathbf{I}$, stated in~\cref{eq:vol_render_in_3dgs}. With the per-pixel weights, we calculate $w_{ij}$ representing a weight between each Gaussian $\theta_i$ and corresponding language embedding maps $\mathbf{f}_j^\text{map}$, which is for aggregating CLIP embeddings from $\mathbf{F}^\text{map}$ and register the embedding to each Gaussian. The weights are computed as:
\begin{equation}
    w_{ij} = \sum\nolimits_{\mathbf{I}\in \mathcal{I}}\sum\nolimits_{\mathbf{r} \in \mathbf{I}} \mathbf{M}_j(\mathbf{I},\mathbf{r})\cdot w_i(\mathbf{I},\mathbf{r}),
\end{equation}
where $\mathcal{I}$ is the set of the training images. In this iterative process, we aggregate weights only for Top-$k$ Gaussians with the highest weights $w_i(\mathbf{I}, \mathbf{r})$, along the ray of each pixel ray~$\mathbf{r}$ (see \Fref{fig:inference}). After aggregation, we prune the Gaussians which are not assigned any weight, \ie, $\sum_{j=1}^M w_{ij}=0$.
This summation aggregates weights between Gaussians and the CLIP embeddings by linking per-pixel weights $w_i(\mathbf{I},\mathbf{r})$ of each Gaussian to its corresponding CLIP embeddings. 
With the obtained weights, we register an aggregated feature $\dot{\mathbf{f}}_i$ to each Gaussian with weighted-averaging as:
\begin{equation}
    \label{eq:weighted-averaging}
    \begin{gathered}
            \dot{\mathbf{f}}_i = \mathbf{f}_i / ||\mathbf{f}_i||_2, \,\, \textrm{where}\quad \mathbf{f}_i = \sum\nolimits_{j=1}^{M} \tfrac{w_{ij}}{\sum^M_{k=1} w_{ik}} \mathbf{f}^\text{map}_j.
    \end{gathered}
\end{equation}
This process enables 3D-aware feature registration to be consistent across various viewpoints, by aggregating features in the original high-dimensional feature space. The proposed process can be interpreted as an inverse volume rendering without gradient-based optimization, which enables our method to be faster than the prior methods requiring per-scene gradient-based optimization~\cite{langsplat, open_scene,legaussian} for feature registration in 3D space.

