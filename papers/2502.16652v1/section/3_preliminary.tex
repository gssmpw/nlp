\label{sec:preliminary}

\paragraph{Preliminary of 3D Gaussian Splatting}
3DGS~\cite{3dgs} encodes appearance and geometry of the target scene into the 3D Gaussian representation. Each 3D primitive representation is expressed as a 3D Gaussian distribution having mean~${\bmu}=[x_\mu, y_\mu, z_\mu]^\top$ for 3D position and covariance matrix~$\Sigma_\text{3D} \in \mathbb{R}^{3\times 3}$ for 3D volume, as well as the opacity value~$\alpha$ and the color~$\mathbf{c}$. In particular, the covariance matrix is decomposed into the scale matrix~$S \in \mathbb{R}^{3\times3}$ and the rotation matrix~$R \in SO(3)$,
$\Sigma_\text{3D} = R S S^\top R^\top$.
In brief, $N$ numbers of 3D Gaussians can be parametrized as~$\Theta = \{ \btheta_i \}_{i=1}^N = \{ \bmu_i, S_i, R_i, \alpha_i, \mathbf{c}_i \}_{i=1}^N$.
3D Gaussians~$\Theta$ are used to render a 2D pixel color~$\hat{\mathbf{c}}$ computed as:
\begin{equation}
\label{eq:vol_render_in_3dgs}
    \hat{\mathbf{c}}(\theta) {=} \sum\nolimits_{i=1}^{N} \!\!T_i\tilde{\alpha_i}\mathbf{c}_i,~\textrm{s.t.}~~\tilde{\alpha_i} {=} \alpha_i \mathrm{exp}\left({ - \tfrac{1}{2}  \mathbf{d}^\top \Sigma_\text{2D}^{-1} \mathbf{d} }\right) ,
\end{equation}
$T_i$ is a transmittance, $\tilde{\alpha_i}$ is an effective opacity value computed from the Gaussian's opacity 
$\alpha$, the pixel distance~$\mathbf{d} \in \mathbb{R}^{2\times 1}$ from the target pixel to the projected center location of the Gaussian in pixel, and $\Sigma_\text{2D}$ is the 2D covariance matrix in the image domain obtained from the splatting algorithm~\cite{3dgs,ewa}. The 3D Gaussian parameters~$\Theta$ of a scene are optimized by minimizing the rendering loss between the input image color $\mathbf{{c}}$ and the rendered color $\mathbf{\hat{c}(\theta)}$ in \Eref{eq:vol_render_in_3dgs} as $\mathop{\arg\min}_{\theta} \| \mathbf{{c}} - \mathbf{\hat{c}}(\theta) \|_F^2$.

\paragraph{Language embedded 3D Gaussian Splatting}
The basic idea of the language embedded Gaussian representation~\cite{langsplat, legaussian, zhou2024feature, semantic_gaussians, fastlgs, fmgs, gaussian_grasper, rethinking_open_vocab} is to replace the color rendering to language embedding rendering. 
Language embedded 3D Gaussians are parameterized as $\Phi = \{ \theta_i, \mathbf{\tilde f}_i \}_{i=1}^N = \{ \bmu_i, S_i, R_i, \alpha_i, \mathbf{c}_i, \mathbf{\tilde f}_i \}_{i=1}^N$, where $\mathbf{\tilde f}_i$ denotes Gaussian-registered language embeddings across $N$ numbers 3D Gaussians which will be discussed soon. Then, analogous to the color rendering \cref{eq:vol_render_in_3dgs}, the language embedding rendering is expressed as:
\begin{equation}
\label{eq:feat_render_in_3dgs}
    \mathbf{\hat f} = \sum\nolimits_{i=1}^{N} T_i\tilde{\alpha_i}\mathbf{\tilde f}_i, 
\end{equation}
where $\mathbf{\hat f}$ denotes a rendered language embedding.
Likewise, the Gaussian-registered language embeddings $\{\mathbf{\tilde f}\}$ are optimized by minimizing the rendering loss between the 2D language embedding~$\mathbf{f}$ extracted from an input image and a rendered language embedding map $\hat{\mathbf{f}} $ as $\mathop{\arg\min}_{\{\mathbf{\tilde f}\}} \| \mathbf{{f}} - \mathbf{\hat{f}} \|_F^2$ at each corresponding pixel. 
This can be regarded as distilling vision language models into Gaussian-registered language embedding~$\mathbf{\tilde{f}}$ through volume rendering~\cref{eq:feat_render_in_3dgs}.
The Gaussian-registered language embeddings are separately trained after pre-training and fixing the pre-trained 3DGS $\Theta$ for a scene.
The language embeddings to be distilled are typically obtained from CLIP~\cite{clip}.
Since storing 32-bit 512-D CLIP features~$\mathbf{f}$ in every 3D Gaussians is memory-expensive, one can use a compressed feature per scene depending on the needs~\cite{fastlgs,legaussian, zhou2024feature,fmgs, gaussian_grasper}.

\paragraph{Motivation}
Such language-embedded radiance fields provide useful representation and language interfaces for many practical and crucial applications.
While most of existing works focus on the training efficiency, the complexity in inference time has barely been discussed.
Considering a scenario to text-query a 3D location of the language-embedded Gaussians, \ie, 3D localization, the aforementioned methods first require rendering a 2D language embedding map at each specific camera pose.
We cannot directly retrieve over the distributed embeddings $\{\mathbf{\tilde f}_i\}$ in 3D Gaussians, because the embeddings do not carry language information directly, but their weighted summed (rendered) features $\mathbf{\hat{f}}$ do.
This issue becomes even severer with compressed features as in~\cite{langsplat}: their decompression decoders are not designed for and incompatible with directly applying to the distributed compressed language embeddings in each 3D Gaussian, yielding degenerated CLIP decoding (refer to~\cref{fig:motivation}).


This introduces multiple challenges and hassles.
First, it is challenging to find the best or proper camera rendering views that contain the object to find.
One may attempt to pre-compute the minimal number of cameras and their camera poses that cover all the 3D Gaussians in a scene with proper resolutions, similarly by point-based approach~\cite{openins3d}. However, this is a well-known set covering problem~\cite{garey1982computers} with constraints which is known to be an NP-hard problem.

Second, even with pre-computed rendered views, the retrieval complexity over the rendered images remains substantial~\cite{guedon2023macarons}.
Suppose a scene consisting of one million Gaussians, but just a \emph{single} rendered language embedding map in pixel domain already has nearly a million pixels; thus, we need a dedicated system to efficiently retrieve over all the views.
Third, since the retrieval is conducted in the 2D space, to find a 3D location, we need a separate mechanism to lift the localization to the 3D space, \ie, increasing the system complexity.
In addition, 32-bit floating 512-Dimension CLIP features for millions of Gaussian are memory intensive, which is often not manageable. To reduce this burden, the existing methods~\cite{open_gaussian} apply compressions with per-scene optimized codebooks, which hinders extension or generalization to other scenes.

To overcome these, we propose a training-free algorithm for the direct allocation of language embeddings to 3D Gaussians, allowing efficient computation and interaction within the 3D space. As a concurrent work, OpenGaussian~\cite{open_gaussian} tackles a similar challenge with our work, but still requires per-scene codebook construction~\cref{fig:teaser}.
