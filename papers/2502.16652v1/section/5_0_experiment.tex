\section{Experiments} 
\label{sec:experiment}

\noindentbold{Dataset}
We use two datasets to evaluate the 3D scene understanding performance. For the 3D object selection task (\Sref{subsec:5_1}), we use the LERF~\cite{lerf} dataset annotated by LangSplat~\cite{langsplat}, which consists of several multi-view images of 3D scenes containing long-tail objects and includes ground truth 2D ground truth annotations for texture queries. For 3D object localization \Sref{subsec:5_2} and 3D semantic segmentation \Sref{subsec:5_3} task, we employ the ScanNet~\cite{dai2017scannet} dataset. ScanNet is a large-scale benchmark that provides data on indoor scenes, including calibrated RGBD images and 3D point clouds with ground-truth semantic labels. We randomly select eight scenes from ScanNet for the experiments.

\noindentbold{Competing methods}
The only method available for a fair comparison with our method is the concurrent work, OpenGaussian~\cite{open_gaussian}.
To study the various aspects of our method, we introduce baseline methods modified from rasterization-based ones \cite{langsplat,legaussian}, for direct 3D referring operation, denoted as LangSplat-m and LEGaussians-m.
As discussed in \Sref{sec:related_works}, without modification, global search over a whole scene is quite demanding. To ensure fair evaluation, we use the same initial 3D Gaussians being trained only using RGB inputs for all comparing methods, and freeze the Gaussians during the language feature allocation process. Also, the per-pixel CLIP~\cite{clip} embedding maps are unified for SAM-based~\cite{sam} methods~\cite{langsplat, open_gaussian} including ours. We follow the hyperparameter settings favorable to each respective paper.





