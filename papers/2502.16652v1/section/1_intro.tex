\section{Introduction}
\label{sec:introduction}

Open-vocabulary 3D scene understanding represents a significant challenge in the field of computer vision, with applications spanning autonomous navigation, robotics, and augmented reality. 
This approach aims to enable the interpretation and referencing of 3D spatial information through natural language, allowing for applicability beyond a restricted set of predefined categories~\cite{point_transformer,point_mixer,point_next,mink,point_net,point_transformer_v3,super_point_trandsformer}. 
Previously, open-vocabulary 3D scene understanding has been explored using point-cloud-based methods~\cite{open_scene,regionplc,ov3d,openmask3d,openins3d,langsplat,lerf}. 
Recently, the 3D Gaussian Splatting (3DGS) ~\cite{3dgs} has introduced a continuous representation integrated on explicit 3D Gaussians, which differs from traditional point-cloud approaches, enabling rapid progress in practical applications~\cite{gaussian_grasper}. 
Current research has begun to explore methods for associating language-based features with 3D Gaussian splats to enhance scene understanding capabilities.

\input{figures/1-teaser/teaser}

Several recent approaches~\cite{langsplat, legaussian, fmgs} introduce 3D Gaussian representation~\cite{3dgs} into the open-vocabulary scene understanding. 
This unique representation uses 3D Gaussians to achieve high-quality scene rendering, offering a more structured representation that addresses some limitations of point clouds. 
Building on this, these methods employ 2D~vision-language models to transfer language knowledge to 3D Gaussians ``via rendered feature maps''.

Despite its promise, such rendering-based distillation methods~\cite{langsplat, legaussian} share two limitations. First, we found that there is a discrepancy between optimized embeddings in 3D Gaussians and 2D language-aligned embeddings. This gap arises mainly from an intermediate rendering step that may distort CLIP embeddings during training. Then, the reliance on rendering impedes holistic 3D scene understanding, additional task-processing such as 3D semantic segmentation and 3D object localization, and making full spatial coverage calculations less efficient than direct 3D Gaussian methods~\cite{open_gaussian} including ours as illustrated in~\Fref{fig:teaser}.

To address this issue, this work proposes \nickname. Our method bypasses the rendering stage, enabling direct interaction with 3D Gaussians for registering and referring the well-preserved language-aligned CLIP embeddings in the 3D space. This makes our \nickname clearly distinguishable from prior works, facilitating a seamless integration of representative embeddings from 2D vision language models into the 3D spatial structure without compromising exhaustive rendering process that has been exploited~\cite{langsplat, legaussian, zhou2024feature, fastlgs, fmgs, gaussian_grasper}. Moreover, we propose to use a Product Quantization (PQ) feature encoding method to represent embeddings compactly and efficiently without any per-scene optimization. Rather than storing full-length feature vectors or per-scene specifically compressed embeddings~\cite{langsplat, legaussian, zhou2024feature, fastlgs, fmgs, gaussian_grasper}, each Gaussian in our \nickname stores an index from a pre-trained PQ, significantly reducing memory usage up to 6.25$\%$ compression ratio. 
By preserving the richness of embeddings while reducing memory usage, PQ is integral to our frameworkâ€™s high scalability and its ability to perform 3D perception tasks, such as open-vocabulary 3D object localization, 3D object selection, and 3D semantic segmentation. Our contributions are summarized as follows:
\begin{itemize}
    \item We propose \nickname, direct registration and referencing of language-aligned features in 3D Gaussians, bypassing intermediate rendering and preserving feature accuracy.
    \item We introduce the PQ encoding method for compact feature representation, reducing memory usage while maintaining essential 3D feature properties.
    \item We present a novel evaluation protocol to assess accuracy of 3D localization and segmentation for 3D Gaussians, with pseudo-labeling methods and volume-aware metrics.
\end{itemize}