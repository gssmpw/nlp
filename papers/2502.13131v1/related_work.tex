% \cy{not sure if we need subsection here. Just using paragraph may also be great.}
% \subsection{Reward Modeling in LLM Alignment}
% \paragraph{The Heterogeneity of Reward Modeling} While the success of large-scale RL in LLM post-training has been demonstrated in mathematical reasoning tasks~\citep{guo2025deepseek}, in broader applications of LLMs such as chatbot, and creative writing, there are aspects that can not be directly evaluated by rule-based reward models~\citep{bai2022constitutional,mu2024rule}. In order to improve the capability of LLMs in those aspects, neural reward models are required to provide guidance to optimize LLM generation~\citep{bai2022training,stiennon2020learning,ouyang2022training}. Those reward models are mainly obtained through preferences annotations~\citep{christiano2017deep} using the Bradley-Terry model~\citep{bradley1952rank}, or alternatively from demonstration data~\citep{wulfmeier2024imitating}. 
% However, given the heterogeneity and diversity of human preferences, as well as the multiple attributes that humans may value, it is necessary to consider multi-objective alignment problem~\citep{lin2023mitigating,yang2024rewards,rame2024rewarded}. 
\noindent\textbf{The Heterogeneity of Reward Modeling.} 
Reward models \cite{lambert2024rewardbench,liu2024skywork} are typically trained using preference annotations from the Bradley-Terry model~\citep{christiano2017deep,bradley1952rank} or demonstration data~\citep{wulfmeier2024imitating,xiao2024leverage}. However, human preferences are diverse and complex, making it difficult to capture all relevant attributes with a single objective~\citep{yang2024rewards,rame2024rewarded,chakraborty2024maxmin}. To address this, researchers are exploring multi-objective preference learning. This includes collecting datasets that assess multiple attributes~\citep{wu2023fine,wang2023helpsteer,cui2023ultrafeedback,pitis2024improving} and developing multi-head reward models that learn diverse user preferences~\citep{quan2024dmoerm,wang2024interpretable}.


% Reinforcement learning in domains like chatbots comes with unique challenges, particularly in defining what makes an output high quality~\citep{bai2022constitutional,mu2024rule,liu2024skywork}. Neural reward models help guide the optimization of LLM outputs by providing more refined feedback~\citep{ouyang2022training,lambert2024rewardbench}. These models are typically trained using preference annotations based on the Bradley-Terry model~\citep{christiano2017deep, bradley1952rank} or demonstration data~\citep{wulfmeier2024imitating,xiao2024leverage}. However, human preferences are diverse and complex, making it difficult for a single objective to fully capture all the attributes users care about~\citep{yang2024rewards,rame2024rewarded,chakraborty2024maxmin}. To address this, researchers are exploring multi-objective alignment approaches for reward modeling. Some studies have introduced datasets that assess multiple aspects to support research in this area~\citep{wu2023fine, wang2023helpsteer, cui2023ultrafeedback, pitis2024improving}, while others focus on developing multi-head reward models to learn diverse user preferences~\citep{quan2024dmoerm, wang2024interpretable}.

\noindent\textbf{Embedding-based Reward Model.} 
Recent advances in reward modeling have highlighted embedding-based approaches for their efficiency and scalability~\citep{ahmed2024scalable,sun2023query,zhang2024general}. These models are backed by strong theoretical foundations~\citep{sun2024rethinking} and demonstrate high flexibility with competitive or superior performance~\citep{li2024q,tennenholtz2024embedding}. Additionally, they integrate well with established statistical learning tools~\citep{dykstra1960rank,springall1973response,han2020asymptotic} and offer greater transparency through statistical insights~\citep{shen2025reviving,feng2025pilaf}. Unlike prior methods, our approach represents human preferences through the final linear layer, enhancing interpretability and enabling the decomposition of preference components.

% Recent advancements in reward modeling have emphasized embedding-based approaches, known for their efficiency and scalability~\citep{ahmed2024scalable,sun2023query,zhang2024general}. These models are well-supported theoretically with established statistical properties~\citep{sun2024rethinking}. On the other hand, empirical evidence suggests that these models not only achieve comparable or superior performance alongside high flexibility and computational efficiency~\citep{li2024q,tennenholtz2024embedding}. Additionally, embedding-based methods enable the integration of established statistical learning tools~\citep{dykstra1960rank,springall1973response,han2020asymptotic} and may enhance the transparency of reward modeling through statistical insights~\citep{shen2025reviving,feng2025pilaf}. Unlike previous approaches, our method represents human preferences using the final linear layer, making the model more interpretable and facilitating the decomposition of preference components.

% \paragraph{Embedding-based Reward Model Research} In recent advance of reward modeling research, embedding-based reward models stands out given their computational efficiency and scalability~\citep{ahmed2024scalable,sun2023query}. Theoretically, the soundness and statistical property of embedding-based Bradley-Terry models have been justified~\citep{sun2024rethinking}; Empirically, those embedding-based reward models achieve on-par or superior performance while keeping high flexibility and computational efficiency~\citep{sun2025reusing}. 
% More over, building on top of those embeddings enable the usage of existing tools in the statistical learning literature~\citep{shen2025reviving,feng2025pilaf}.

\noindent\textbf{Dimensionality Reduction and Embedding Analysis.}
Linear dimensionality reduction techniques like PCA have proven effective in extracting latent dimensions that capture key human preferences for model alignment~\citep{freire2024uncovering}. Beyond alignment, broader studies have explored structuring high-dimensional data into more interpretable embeddings~\cite {huertas2023exploring,kanerva2000random}. Methods such as Q-Probe~\citep{li2024q} and DeepMDP~\citep{gelada2019deepmdp} further enhance model alignment by efficiently exploring embedding spaces. Recent work also shows that improving embedding quality leads to better model performance and generalization~\citep{li2024reward,yang2024regularizing}.


% Recent research has demonstrated the effectiveness of linear dimensionality reduction techniques, such as PCA, in extracting latent dimensions that capture key human preferences for model alignment and interpretability~\citep{freire2024uncovering}. Beyond alignment-specific applications, broader studies have highlighted the role of manifold learning, autoencoder-based models, and random indexing in transforming high-dimensional data into more structured and interpretable embeddings~\citep{huertas2023exploring,kanerva2000random}. 
% Additionally, methods like Q-Probe \cite{li2024q} and DeepMDP \cite{gelada2019deepmdp} have demonstrated practical advantages in efficiently exploring embedding spaces to optimize model alignment and outputs. 
% Parallel to these advancements, ongoing research demonstrates that enhancing the quality and applicability of embeddings can achieve even greater improvements in model performance and generalization~\citep{li2024reward,yang2024regularizing}.

% In previous research, linear dimension reduction tools such as PCA have been successfully applied to extract latent dimensions that capture key human preference signals~\citep{freire2024uncovering}. Beyond the alignment-specific techniques, broader investigations have highlighted the role of manifold learning, autoencoder-based models, and random indexing in reducing high-dimensional representations to more tractable, interpretable embeddings~\citep{huertas2023exploring,kanerva2000random}. Methods like Q-Probe~\citep{li2024q} and DeepMDP~\citep{gelada2019deepmdp} have further underscored the effectiveness of efficient embedding space exploration for aligning and optimizing model outputs. Evaluations of unsupervised dimensionality reduction for pre-trained sentence embeddings have also shown that these techniques can even enhance downstream performance by mitigating redundancy while preserving essential semantic information~\citep{zhang2024evaluating}. Those studies highlight the importance of dimensionality reduction and embedding space analysis. 
% In parallel with the embedding space analysis and understanding, improving those embeddings has highlits the potential of achieving further improvement gains from an different perspective~\citep{li2024reward,yang2024regularizing}.