
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference, times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{The Philosophical Foundations of Growing AI Like A Child}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.

\iclrfinalcopy
\author{Dezhi Luo, Yijiang Li, Hokin Deng\\
 University of Michigan, University of California San Diego, Carnegie Mellon University\\ 
\texttt{ihzedoul@umich.edu, yijiangli@ucsd.edu, hokind@andrew.cmu.edu}
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}

Despite excelling in high-level reasoning, current language models lack robustness in real-world scenarios and perform poorly on fundamental problem-solving tasks that are intuitive to humans. This paper argues that both challenges stem from a core discrepancy between human and machine cognitive development. While both systems rely on increasing representational power, the absence of core knowledge—foundational cognitive structures in humans—prevents language models from developing robust, generalizable abilities, where complex skills are grounded in simpler ones within their respective domains. It explores empirical evidence of core knowledge in humans, analyzes why language models fail to acquire it, and argues that this limitation is not an inherent architectural constraint. Finally, it outlines a workable proposal for systematically integrating core knowledge into future multi-modal language models through the large-scale generation of synthetic training data using a cognitive prototyping strategy.

\end{abstract}

\section{Introduction}
	
Recent advancements in artificial neural networks, particularly transformer-based large language models (LLMs), mark perhaps the most impressive progress in artificial intelligence (AI). Never had any AI models perform so well on tasks that are known to require high-level reasoning abilities, not limited to solving challenging mathematical problems \citep{ahn2024large}, writing codes for complex programs that runs smoothly \citep{zhuo2024bigcodebench}, drafting travel and business plans \citep{xie2024travelplanner}, and importantly, doing so all via natural languages which humans can effortlessly understand. However, despite such astonishing achievements, scientists and users have noticed limitations of LLMs that would seem bizarre for humans: their performances on these tasks could fall drastically over slight tweaks of details in the task conditions \citep{yuan2023revisiting}. 

Said limitations, which concern the robustness of problem-solving, have posed odds over the reliability of deploying LLMs to handle real-life tasks, at least in ideally low-supervised manners \citep{mitchell2023debate}. Interpretations over the underlying causes of such limitations vary, with a prominent account citing the distinction between formal and functional linguistic competences — the ability to produce fluent languages versus the ability to understand and use them in the real world — and arguing that the \textbf{robustness challenge} is due to LLMs possessing the former without the latter \citep{mahowald2024dissociating}. The extreme version of this account would be the one claiming that LLMs lack understanding altogether. In other words, they are “stochastic parrots” that can only solve tasks by abusing spurious correlations in the existing dataset, thereby failing to sensibly answer when the questions are dissimilar enough to what they have been trained on \citep{bender2021dangers}.

At the same time, a surge of attention has been put into benchmarking LLMs: assessing them on large-scale repositories of tasks systematically-developed to target distinct reasoning abilities. A key motivation for such efforts is the notoriously mysterious nature of how LLMs work. It is not apparent, even to engineers who built them, how next-token predictions could enable problem-solving that requires advanced inference. By applying experimental paradigms with controlled designs, researchers are able to differentiate what LLMs can and cannot do under specific task conditions. One particularly staggering kind of result from the benchmarking efforts is the discovery that LLMs could fail miserably at tasks that are easy to humans, despite their high achievements on much harder ones \citep{dentella2024testing}. Most interestingly, such a discrepancy could occur between tasks demanding abilities on seemingly the same domain. For example, GPT-4o, a state-of-the-art model open to the public, was shown giving wrong answers to a range of basic mathematical questions, some as simple as 1-digit counting, while crushing advanced problem sets that are challenging even to professional mathematicians \citep{williams2024easy}. 

As counterintuitive as it is, this situation is not unfamiliar to the AI research community. As Karpathy (2024) noted, phenomena as such simply reveals that LLMs are not exempt from \textbf{Moravec's Paradox}: tasks that are easy to humans could be extremely difficult to machines and vice versa \citep{moravec1988mind}. Like the robustness challenge discussed above, this generates doubt over whether LLMs truly understand what they are doing and discourages their real-life applications.

This paper does the following:

\begin{enumerate}
    \item First, it suggests that the robustness challenge and the Moravec’s paradox are two sides of the same coin: they both arise from the discrepancy between the cognitive development of humans and machines, which can be summarized as scaling up vs. growing up\footnote[1]{This statement of the distinction is likely used first by Josh Tenenbaum in some versions of the talk titled “Scaling AI the Human Way”. Earliest version containing the statement is possibly given at IPAM (UCLA; Nov. 8, 2024).}.
    \item Second, it argues that the key factor among said discrepancy preventing LLMs to overcome both problems is their absence of what is known as core knowledge. Empirical evidence of humans possessing core knowledge as well as machines lacking thereof is discussed.
    \item Third, it discusses possible interpretations of why core knowledge is not mastered by current LLMs. It is concluded that neither of the likely interpretations designate the current foundational architecture of LLMs to be theoretically incapable of developing core knowledge.
    \item Finally, it outlines a promising pathway to train the next generation LLMs with core knowledge, highlighting the large-scale development of synthetic data using a novel cognitive prototyping strategy.
\end{enumerate}

\section{Scaling Up vs. Growing Up}

While the differences between the computational architectures supporting LLMs and human intelligence are extensively discussed, there has been relatively little attention given to how LLMs and humans differ in their development. In the following section, we discuss how looking at such differences facilitates a unified account of the robustness challenge and the Moravec’s paradox.

A widely held belief in the current AI research community is that changes in LLMs’ reasoning performance can be directly attributed to changes in their scale, as tracked by the number of parameters they have in the neural networks and the size of the dataset they are trained on. This belief, hailed as the \textbf{scaling law}, has largely been supported by empirical results observed throughout LLMs’ progression throughout the past several years, and continues to be the primary strategy adopted by major companies for developing more advanced models \citep{kaplan2020scaling}. In this sense, scaling law has been taken as a theory regarding the nature of LLMs’ cognitive development, that is, increased computational power supporting a domain-general learning mechanism \citep{long2024nativism}. As Richard Sutton, in his seminal The Bitter Lesson, puts it:

\begin{quotation}
“One thing that should be learned from the bitter lesson is the great power of \textbf{general purpose methods}, of methods that continue to scale with increased computation even as the available computation becomes very great…we should stop trying to find simple ways to think about the contents of minds, such as \textbf{simple ways to think about space, objects, multiple agents, or symmetries}. All these are part of the arbitrary, intrinsically-complex, outside world. They are not what should be built in, as their complexity is endless; instead we should build in only the meta-methods that can find and capture this arbitrary complexity.” (\citep{sutton2019bitter}, p.2; emphasis added)
\end{quotation}

However, a principal limitation of scaling law as a theory of cognitive development is that it offers minimal explanation: it is not apparent how a neural network can master complex reasoning abilities just “emerge” through scaling up its number of parameters and the size of the dataset. This is not just for abilities yet to be achieved, but applies as well to those that have already emerged in bigger LLMs but not smaller ones. As Churchland and Sejnowski (1990) noted in the early days of connectionist research, an epistemological theory must not solely rely on a priori philosophy but should be grounded upon empirical data across levels of analysis \citep{churchland1990neural}. Claiming that all current barriers between LLMs and human intelligence can be resolved by scaling them up simply because it worked well in the past, without knowing how and why it worked well, would be an a priori argument with little scientific merit \citep{marcus2018deep, long2024nativism}. 

On the other hand, understanding how human cognition develops as we grow up, and eventually reaches the level of (adult) intelligence that AI strives to achieve, has been the objective of developmental psychologists over the past several decades \citep{spelke2023precis, tomasello2024agency}. The application of specialized experimental paradigms for probing cognitive abilities have produced a large amount of psychological and behavioral data regarding what children can and cannot do across different age groups, which fertilizes empirically-grounded theories of human cognitive development. 

Among these, many support the idea that human cognition develops along distinct stages marked by the acquisition of previously inaccessible abilities, with more complex abilities grounded on simpler ones \citep{brainerd1978stage, barrouillet2015theories}. Such theories have been referred to as \textbf{stage theories}, which framework is pioneered by the work of Jean Piaget, who defined four distinct stages of development: sensorimotor, preoperational, concrete operational, and formal operational \citep{piaget1952origins}. At each stage, children acquire abilities that are previously unavailable to them, each reshaping their understanding of the world on respective domains. For example, a landmark for the concrete operational stage is that children begin to understand that the quantity of things does not change with how they are arranged \citep{piaget1969child}. Coined as “conservation” by Piaget, this substitutes the naive and rigid understanding of quantity available to the preoperational child, which cannot overcome bias such as the length-equals-number strategy, e.g. thinking that a coin row has more coins than another for being more spreaded out \citep{houde1997numerical}. As the physical prototype for more abstract numerical operations, understanding the law of conservation lays the foundation for learning complex mathematical abilities later in life. Similarly, children’s elimination of egocentrism via the acquisition of basic perspective-taking abilities at a young age paves the way for higher-level theory-of-mind reasoning at an older age, such as understanding others' intentions \citep{piaget1969child}. Looking at a longer time frame, sensorimotor abilities such as object permanence, perceptual constancy, and the sense of continuity found to develop very early in life are indispensable for learning about physics and mechanics in school and beyond \citep{piaget1952origins}.

While many details in Piaget’s theory have been debated following more rigorous empirical investigations, modern literature generally support his idea that children first learn simple abilities to reason about the physical world and develop more complex, abstract abilities on top as they grow up\footnote[2]{Most of the debates have been regarding the specific ages of which children acquire different abilities. The difficulty of reaching consensus across empirical sources have largely led to the abandonment of the “stage” notion even among theorists who support neo-Piagetian approaches (Rochat, 2024). It is, however, important to note that the lack of clearcut age parameters do not go against the overall idea that children transition across a hierarchy of simple to complex abilities throughout development — the transition may just not be in a stage-to-stage fashion where each stage is qualitatively different from another in a definable way.} \citep{barsalou2008grounded, samuelson2000grounding, barsalou2010grounded, pezzulo2013computational}. This claim of a \textbf{grounding} nature of human cognitive development offers critical insights to the limitations in LLMs. Foremost, having learned simple abilities prior to complex abilities, humans are not subjected to the Moravec’s Paradox\footnote[3]{Although, to be fair, the Moravec’s Paradox is termed so on the basis that human intelligence is held as a standard for comparison.}. 

Moreover, since there is a causal relationships postulated between the primitive, simple abilities and the late-coming complex abilities, the grounding view implies that the absence of simple abilities at an earlier time frame would likely affect the complex abilities later, hence a mechanistic link between the Moravec’s Paradox and the robustness challenge, both of which are observed in LLMs. It therefore may be the case that LLMs fail to generalize across conditions like humans when solving complex reasoning tasks because the skills they employ are not grounded upon simpler abilities on relevant domains.

Note that this explanation hinges on the hypothesis that LLMs’ computational architecture share some aspects with that of humans, at least to the extent that the basic patterns of grounding apply. This may not be the case if human-level intelligence, specifically robust high-level reasoning, is multiple-realizable, which many argued to be likely \citep{bechtel1999multiple, mcgrath2024multiple}. Especially, be reminded of scaling law, it is often postulated that LLMs’ cognitive development is based on some kind of domain-general learning mechanism, whereas research into human cognition and its grounding seems to be highly domain-specific. 

Nevertheless, human cognitive development might be more mechanistically similar to that of LLMs than it would appear. It is a long tradition to think that what drives the transitions from simple to complex abilities are highly domain-general. Piaget proposed that cognitive development is fundamentally the project of assimilation and accommodation, which is fitting familiar stimuli to existing cognitive structures and reconstructing them to interpret novel stimuli, respectively. These are essentially data-driven processes that do not select for particular patterns of information. In particular, Piaget suggested that the major transition differentiating the concrete operational stage from the preoperational stage is not the acquisition of specific abilities like conservation and perspective-taking per se, but the ability to perform systematic mental operations over structured mental representations, which in turn support said abilities by enabling functions such as reversibility and simulation \citep{piaget1965number}. Jerry Fodor, in his seminal work The Language of Thought, likewise interpreted that:

\begin{quotation}
“...a reasonable account of the stages of cognitive development could be elaborated by referring to increases in the expressive power of such systems. What I think \textbf{one cannot have, however, is that concept learning provides the mechanisms for the stage- to-stage transitions}. That is, if the child's cognitive development is fundamentally the development of \textbf{increasingly powerful representational/conceptual systems}, then cognitive development cannot be the consequence of concept learning.” (\citep{fodor1975language}, p.89; emphasis added)
\end{quotation}

Fodor notoriously argues for the view that learning is not the matter of acquiring new concepts, but this cannot be taken as to suggest an entirely nativist theory of learning resembling that of Plato \citep{simon1976computer, marcus2018deep}. In concert with Piaget’s theory, what he proposed here is that human cognitive development may be driven by increasingly powerful representational abilities that are capable of supporting better ways of conducting hypothesis-testing, and concept learning is just a consequence of this systematic improvement. Neither Fodor’s nor Piaget’s theory is known to offer an explanation of how this improvement take place, which has largely remained to be an open question, yet it is apparent that it implies a remarkable similarity between the dynamic aspect of human cognitive development and the scaling of LLMs, as increased computational resources essentially support more complex patterns of manipulating representations.

From this perspective, it is therefore reasonable to think about LLMs’ limitations by mapping their differences with human cognition given a possible similarity between the common, foundational mechanism of their cognitive development. This is the view that both the robustness challenge and the Moravec’s Paradox are due to a missing link between simple abilities and complex abilities. Further, given that humans supposedly acquire complex abilities not by concept learning but rely on domain-general improvements, it is possible that this missing link is not due to problems with the way of increasing representational powers (i.e. scaling), but the failure to acquire simple abilities in the first place. In the next section, we begin to discuss theories and empirical results supporting this hypothesis, taken from both humans and LLMs.


\section{Core Knowledge}

A large body of works in cognitive science have demonstrated that humans possess a basic understanding of several key domains of the world at a very young age, henceforth \textbf{core knowledge} \citep{spelke2003makes, spelke2007core}. This set of knowledge is generally understood as consisting of simple principles regarding objects, actions, number, space, and social relations, including how they relate to each other. Core knowledge is essentially children’s “developmental start-up software” that enables them to navigate and learn about the rich and dynamic nature of the environment in their early life \citep{lake2017building}. 

Recent behavioural data have further supported a “child as scientist” proposal, showing that children appear to actively formulate intuitive hypothesis and validate them using these abilities to derive knowledge regarding previously unknown aspects of the world \citep{gopnik1996scientist, schulz2007preschool, gweon2010infants, koksal2018development}. These data, while supporting Fodor’s view that hypothesis-testing plays a foundational role in learning during the early days, highlights the domain-specific nature of the process, during which core knowledge of different aspects of the world is employed by the representational resources of the child. 

It is unclear how core knowledge comes about in the minds of individuals. Neuroimaging studies have suggested that functions within these key domains are likely organized modularly even in the children’s brain, exhibiting specialized neural networks supporting core systems of numerical operations (number), spatial navigation (space), theory-of-mind (social relations), and so on \citep{siegal2002neural, newcombe2004starting, nieder2009representation}. This evidence, paired with behavioural findings showing that these core-level abilities are shared by even the most remote, independently-evolved cultural groups, appears to favor the view that core knowledge is somehow innate. In contrast, others have speculated that it might just be early-developed knowledge instead of being “hardwired” in the brain at birth \citep{carey2011precis}. These discussions constitute a major venue of the modern nativist vs. empiricist debate that we would not go into in detail here. In any case, core knowledge appears so early in childhood that it certainly cannot be the result of language-based learning. In turn, languages are likely a set of complex abilities that is developed upon these simple abilities. This marks a major discrepancy between the growing up of humans and the scaling up of LLMs, which acquire all reasoning abilities through linguistic data. However, it would again be a priori to directly view this as an explanation against LLMs’ possession of simple abilities, which undermines the multiple-realizability of cognition. The rapid development of Multimodal LLMs (MLLMs), which are LLMs that can receive and process multimodal (mostly visual as in current models) input, further offers potentials toward this direction by supporting sensory-based learning within language model architectures. 

Whether LLMs could or already possess simple abilities like human core knowledge is fundamentally an empirical question. Fortunately, benchmarking approaches inspired by experimental paradigms in human research have provided a promising pathway toward answering such questions, in which LLMs are essentially evaluated the same way as human participants in psychology experiments \citep{binz2023using, shiffrin2023probing}. Following such approaches, a large-scale benchmark developed by \citep{li2025core} assessed twelve different cognitive abilities in MLLMs. The abilities tested range from simple, core-level abilities like object permanence and perceptual continuity to complex abilities like tool-using and intentionality understanding, spanning over all core knowledge domains. Tasks in said benchmark consist primarily of image or video-formatted adaptations of classic cognitive tasks used in the developmental psychology literature, such as visual cliff task (spatiality), three-mountain task (perspective-taking), and gear system (mechanical reasoning). Remarkably, they found that MLLMs generally perform much better in simple abilities than complex ones. As an example, state-of-the-art models like GPT-4o, while achieving near-human performance on abilities such as intentionality understanding—a higher-level theory-of-mind ability that typically emerges in humans no earlier than 6–7 years of age—fail catastrophically in simpler abilities like level-2 perspective-taking, which children are reported to master as early as 36 months of age \citep{moll2011does, linsley20243d, gao2024vision}.

Moreover, further in-depth analyses have shown that model performance on low-level cognitive abilities, such as perspective-taking, does not improve alongside model size \citep{gao2024vision, li2025core}. In other words, scaling does not seem to allow MLLMs to become better at such abilities. While still an ongoing effort, said results have begun to provide critical empirical support for the hypothesis that scaling up LLMs may not allow them to grasp simple abilities available to humans even in early childhood. 


\section{Interpreting Core Knowledge Deficits in LLMs}

Why do LLMs fail to acquire core knowledge through scaling, despite having access to vast linguistic datasets spanning virtually the entire internet and the computational power to process them? The answer lies in the fundamental differences between LLMs and humans—the only known intelligent agents that possess core knowledge. By systematically examining these differences in architecture and learning processes, we can develop testable hypotheses to identify the specific reasons behind LLMs' core knowledge deficits. If future models are designed based on these hypotheses and their performance begins to align with human abilities, it would provide a proof of concept for addressing these limitations. The following sections explore three key factors that may contribute to core knowledge deficits in LLMs. Below, we explore three potential factors that may underlie core knowledge deficits in LLMs.

\subsection{Lack of Hardwired Domain-specific Faculties}

To begin with, in line with a long tradition in nativist epistemology \citep{locke1824works, chomsky1980rules, cowie2002s}, it has been suggested that fundamental knowledge of the world—such as core knowledge—must be built upon "hardwired" cognitive faculties that are innate to the brain at birth. Neurobiological findings indicating specialized modularity in the human brain for processing certain types of information, such as numbers, objects, and social relations, support this perspective \citep{spelke2007core}. If this view holds, LLMs' core knowledge deficits may be fundamentally unresolvable, as their foundational architecture lacks such hardwired domain-specific vehicles.

However, this argument is inconclusive at best. To start, empirically validating the existence of innate, domain-specific cognitive mechanisms in humans is challenging, as what appears to be "innate knowledge" could instead be "early knowledge"—the result of domain-general learning mechanisms rapidly self-organizing into primitive forms of functional modularity upon first exposure to the environment during infancy, as Piaget posited in terms of accommodation \citep{piaget1952origins}. Moreover, even if humans do possess built-in cognitive faculties, they may not be strictly necessary for any system capable of acquiring core knowledge. To claim otherwise would impose a \textbf{learnability} constraint, asserting that certain cognitive capacities cannot be learned from data using domain-general empiricist mechanisms and instead require innate structures \citep{long2024nativism}.

Ultimately, a key argument regarding the learnability of core knowledge is that infants lack explicit symbolic systems and that there are no widely accepted mechanisms by which pre-linguistic intelligent systems could form representations of domain-specific concepts solely through perceptual interaction \citep{carey2011precis}. However, given LLMs' strong ability to manipulate linguistic symbols, they may be able to leverage abstract concepts through domain-general learning mechanisms to acquire core knowledge representations \citep{long2024nativism}.


\subsection{Buried Too Deeply}

Secondly, it may be argued that LLMs may already possess core knowledge but are unable to extract and apply it in reasoning. Since the early days of connectionist research, it has been suggested that artificial neural networks, due to their reliance on distributed representations, inevitably face a challenge: as network size increases through scaling, retrieving knowledge representations for reasoning becomes increasingly difficult and computationally costly \citep{hinton1986learning, clark1992presence}. As Chalmers (1990) remarked:

\begin{quotation}
“Not only is compositional structure encoded implicitly in a pattern of activation, but this implicit structure can be utilized by the familiar connectionist devices of feed-forward/back-propagation in a meaningful way. Such a conclusion is by no means obvious a priori—it might well have turned out that the structure was \textbf{"buried too deeply"} to be directly used, and that all useful processing would have had to proceed first through the step of extraction.” (\cite{chalmers1990syntactic}, p. 60; emphasis added)
\end{quotation}

This concern is particularly relevant for core knowledge. High-level details required for complex tasks are relatively easy to retrieve, as their representational patterns are likely clustered within the network, such as those encoding a specific historical event or a recipe for a particular dish. In contrast, identifying and utilizing core concepts in simple tasks is significantly more challenging for LLMs, as these concepts are distributed across numerous samples in the dataset with roughly the same level of salience. For example, while a vast number of instances in a model's training data may implicitly demonstrate perceptual constancy—the principle that an object's identity remains unchanged despite variations in sensory representation—this concept is not explicitly reinforced in a way that facilitates reliable retrieval \citep{garrigan2008perceptual, green2024perceptual}. Since basic concepts like this is embedded in diverse contexts rather than tied to specific, distinguishable patterns, LLMs may struggle to isolate and apply it systematically, leading to inconsistent or surface-level reasoning when tested on related tasks. Consequently, while scaling may provide sufficient computational power and data to encode core knowledge, models remain unable to effectively employ it for reasoning. This is because its compositional structure is encoded in an excessively dispersed manner across the vast parameter space of the network \citep{shani2023towards}.



\subsection{Groundless Learning Process}

Finally, a key distinction between human and machine learning lies in the temporal dynamics of \textbf{data exposure}. Humans progress through a structured developmental trajectory, where cognitive and representational capacities are initially limited, allowing them to process information only within a constrained scope. As they mature, their cognitive system gradually expands, building upon foundational core knowledge to integrate increasingly complex abstractions. This incremental learning process enables humans to develop a deep, structured understanding of the world, where high-level reasoning emerges as a natural extension of foundational cognitive abilities \citep{pezzulo2013computational}.

In contrast, LLMs do not follow this grounded developmental process. Instead, they are trained from the outset with access to vast datasets encompassing a broad spectrum of human knowledge, including highly abstract and complex information. Unlike humans, who first acquire intuitive principles through direct sensorimotor experiences before developing abstract knowledge on top, LLMs process high-level concepts alongside low-level ones without any structured progression. This lack of developmental scaffolding means that while LLMs can generate responses that resemble advanced reasoning, they may lack the fundamental conceptual grounding that allows humans to apply knowledge flexibly and coherently across different contexts \citep{mitchell2023debate}.

However, this difference in developmental trajectory does not necessarily preclude LLMs from acquiring core knowledge. If trained on data similar to those available to a child, LLMs might develop a structured understanding of fundamental concepts in a way that mirrors human learning. Rather than relying solely on large-scale text-based training, models could benefit from multimodal learning, where training data prioritizes direct representations of low-level cognitive concepts through rich perceptual information. By leveraging their ability to process symbolic representations, LLMs may be able to construct conceptual frameworks akin to core knowledge developed by humans.


\section{Moving Forward: Growing AI Like A Child}

The interpretations of potential reasons for core knowledge deficits in LLMs suggest a testable hypothesis: LLMs may be capable of acquiring core knowledge if trained on environmental stimuli similar to those available to a child. By leveraging their ability to process symbolic representations, LLMs might develop a structured understanding of fundamental concepts upon similar external stimuli accessible to a child developing the same concepts.

For this proposal to be viable, several key conditions would have to be considered:

\begin{enumerate}
    \item First, sensory grounding-models must be trained using multimodal inputs, incorporating both visual and linguistic data to approximate the rich perceptual experience of early human learning. 
    
    \item Second, large-scale exposure—a sufficiently vast dataset is needed to ensure that core knowledge representations are robust and not lost due to extraction difficulties inherent in distributed neural representations. 
    
    \item Third, minimizing environmental confounds—unlike a child, LLMs have unrestricted access to abstract information in their training data, making them prone to shortcut-taking by exploiting spurious correlations rather than developing genuine conceptual understanding. Ensuring that training data is curated to prevent such biases will be crucial.

\end{enumerate}

To this end, this paper outlines a practical solution that meets these conditions: the large-scale development of synthetic data using physical engine, operate on a novel cognitive prototyping strategy.

\subsection{Cognitive Prototype}

To generate training datasets that enhance the saliency of direct representations of low-level cognitive concepts while minimizing environmental confounds, a promising approach is to directly train MLLMs on cognitive experiments commonly used in developmental psychology laboratories. The recent adaptation of these experiments into machine-readable formats, as seen in cognitively inspired benchmarks \citep{binz2023using, li2025core}, provides a viable framework for constructing such datasets.

Each low-level cognitive concept can manifest in multiple ways, necessitating diverse experimental paradigms for its assessment. To systematically structure these training datasets, we propose the \textbf{cognitive prototype}, a standardized framework for curating data for each cognitive ability. A cognitive prototype consists of detailed specifications of a cognitive experiment that operationalize the target concept, along with schematic descriptions of task conditions that allow for controlled variations. This structured approach enables systematic data generation at scale while preserving the conceptual rigor of experimental paradigms. 

For example, the Three Mountain Task can be used to construct a cognitive prototype for training perspective-taking. In its standard form, this task involves presenting a child with a model featuring three distinct mountains—one covered in snow, another marked by a red cross, and the third topped with a hut. After allowing the child to observe the model from all angles, an experimenter introduces another individual, represented by a doll who views the model from a different vantage point. The child is then shown a set of photographs depicting various perspectives of the model and is asked to identify which image accurately represents what the other person sees \citep{piaget1969child}. Once the experimental paradigms are specified within the cognitive prototype, data generation can be systematically conducted at a large scale by varying task-relevant conditions in the experiments. These variations may include adjustments in the placement and appearance of the doll, changes in the observer’s viewpoint, modifications to the model’s design, or alterations in the probing questions \citep{gao2024vision}. This approach ensures diverse and representative training data that reinforces the foundational principles of core knowledge while preventing models from relying on spurious correlations. By leveraging cognitive prototypes for structured data generation, this strategy offers a scalable and theoretically grounded approach to training MLLMs on core cognitive abilities, addressing key limitations in their current developmental trajectory.


\subsection{Synthetic Data Via Physical Simulations}

Once the cognitive prototypes are established, systematically varying task conditions enables the generation of a vast dataset, where each instance presents a unique composition of task elements while consistently exemplifying the same underlying cognitive construct. However, manually labeling such large-scale data would be prohibitively costly and significantly limit the dataset’s scope. To address this challenge, we propose the use of \textbf{synthetic data generation}, leveraging computational tools to create large, diverse, and precisely labeled datasets.  

Rather than relying on real-world examples, data can be generated using physics engines such as Mujoco \citep{todorov2012mujoco} and the upcoming Genesis \citep{zhou2024genesis}. These engines allow for precise control over task parameters, enabling the automated creation of experimental scenarios that adhere to cognitive science paradigms while eliminating the inconsistencies and biases present in real-world datasets. By explicitly defining the variables within each experimental paradigm and establishing structured protocols for administering variations, physics engines can systematically produce a vast array of training instances. This ensures that each generated example accurately operationalizes the target cognitive concept while introducing controlled diversity in conditions. Taken together, this approach offers a scalable and theoretically grounded method for training MLLMs on core knowledge. By combining cognitive prototypes with physics-based synthetic data generation, we can create high-quality datasets that reinforce fundamental cognitive abilities, reduce reliance on spurious correlations, and ultimately provide a more structured developmental foundation for machine intelligence.



\section{Conclusion}

This paper proposes that the robustness challenge and Moravec’s Paradox, two key limitations of current LLMs with significant scientific and practical implications, may be jointly explained by differences in cognitive development between humans and machines. Specifically, these differences are not found within the dynamic process of improving representational power—a mechanism likely shared between scaling up in LLMs and growing up in humans. Instead, they stem from LLMs' absence of core knowledge, a set of foundational cognitive abilities present in humans from early childhood. This core knowledge serves as the basis for gradually acquiring more complex skills over time. Empirical evidence presented in this paper demonstrates that such abilities are indeed missing in LLMs. Further analysis of the underlying causes of this core knowledge deficit suggests a viable solution: systematically increasing low-level representations of core cognitive domains within LLM training datasets.

This analysis underscores the need for future research to explore how core knowledge can be effectively incorporated into LLMs. Rather than contradicting the general principle of scaling laws, this perspective challenges the assumption that intelligence can emerge solely from domain-general cognitive mechanisms. Just as humans rely on developmental start-up software, LLMs may require structured early training to scaffold their cognitive growth. Encouragingly, given their ability to process vast amounts of high-level information, LLMs may not require innate structures but rather training data that prioritizes salient representations of low-level concepts—analogous to the perceptual input available to a child. Based on said theorization, this paper proposes an engineering solution leveraging large-scale synthetic data generation via simulation engines to systematically generate task scenarios based on developmental psychology paradigms. Importantly, there appears to be no fundamental technical barrier to pretraining core knowledge. The next step is to implement this approach and rigorously evaluate whether it enhances human-like cognitive competence, particularly in real-world robustness. This research agenda can be best summarized as growing AI like a child—at scale \footnote[4]{The authors of this article are leading an engineering project on the conviction of this statement, and see https://growing-ai-like-a-child.github.io/ for details.}.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}



\end{document}
