% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@book{Aho:72,
    author  = {Alfred V. Aho and Jeffrey D. Ullman},
    title   = {The Theory of Parsing, Translation and Compiling},
    year    = "1972",
    volume  = "1",
    publisher = {Prentice-Hall},
    address = {Englewood Cliffs, NJ}
}

@book{APA:83,
    author  = {{American Psychological Association}},
    title   = {Publications Manual},
    year    = "1983",
    publisher = {American Psychological Association},
    address = {Washington, DC}
}

@article{Chandra:81,
	author = {Ashok K. Chandra and Dexter C. Kozen and Larry J. Stockmeyer},
	year = "1981",
	title = {Alternation},
	journal = {Journal of the Association for Computing Machinery},
	volume = "28",
	number = "1",
	pages = "114--133",
	doi = "10.1145/322234.322243",
}

@inproceedings{andrew2007scalable,
  title={Scalable training of {L1}-regularized log-linear models},
  author={Andrew, Galen and Gao, Jianfeng},
  booktitle={Proceedings of the 24th International Conference on Machine Learning},
  pages={33--40},
  year={2007},
}

@book{Gusfield:97,
    author  = {Dan Gusfield},
    title   = {Algorithms on Strings, Trees and Sequences},
    year    = "1997",
    publisher = {Cambridge University Press},
    address = {Cambridge, UK}
}

@article{rasooli-tetrault-2015,
    author    = {Mohammad Sadegh Rasooli and Joel R. Tetreault},
    title     = {Yara Parser: {A} Fast and Accurate Dependency Parser},
    journal   = {Computing Research Repository},
    volume    = {arXiv:1503.06733},
    year      = {2015},
    url       = {http://arxiv.org/abs/1503.06733},
    note    = {version 2}
}

@article{Ando2005,
	Acmid = {1194905},
	Author = {Ando, Rie Kubota and Zhang, Tong},
	Issn = {1532-4435},
	Issue_Date = {12/1/2005},
	Journal = {Journal of Machine Learning Research},
	Month = dec,
	Numpages = {37},
	Pages = {1817--1853},
	Publisher = {JMLR.org},
	Title = {A Framework for Learning Predictive Structures from Multiple Tasks and Unlabeled Data},
	Volume = {6},
	Year = {2005}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{zhu2023minigpt,
  title={Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  author={Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
  journal={arXiv preprint arXiv:2304.10592},
  year={2023}
}
@article{bi2024deepseek,
  title={Deepseek llm: Scaling open-source language models with longtermism},
  author={Bi, Xiao and Chen, Deli and Chen, Guanting and Chen, Shanhuang and Dai, Damai and Deng, Chengqi and Ding, Honghui and Dong, Kai and Du, Qiushi and Fu, Zhe and others},
  journal={arXiv preprint arXiv:2401.02954},
  year={2024}
}
@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

% 行业相关论文  
@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PMLR}
}
@article{wang2024qwen2,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
@article{glm2024chatglm,
  title={Chatglm: A family of large language models from glm-130b to glm-4 all tools},
  author={GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and others},
  journal={arXiv preprint arXiv:2406.12793},
  year={2024}
}

@article{li2024baichuan,
  title={Baichuan-omni technical report},
  author={Li, Yadong and Sun, Haoze and Lin, Mingan and Li, Tianpeng and Dong, Guosheng and Zhang, Tao and Ding, Bowen and Song, Wei and Cheng, Zhenglin and Huo, Yuqi and others},
  journal={arXiv preprint arXiv:2410.08565},
  year={2024}
}
@article{hong2024cogvlm2,
  title={Cogvlm2: Visual language models for image and video understanding},
  author={Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others},
  journal={arXiv preprint arXiv:2408.16500},
  year={2024}
}


@article{chen2024far,
  title={How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
  author={Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220101},
  year={2024},
  publisher={Springer}
}


        %%较早视觉大模型
@article{liu2024visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@article{openai2023gpt,
  title={Gpt-4 technical report. arxiv 2303.08774},
  author={OpenAI, R},
  journal={View in Article},
  volume={2},
  number={5},
  year={2023}
}

@article{zhang2023llama,
  title={Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
  author={Zhang, Renrui and Han, Jiaming and Liu, Chris and Gao, Peng and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Qiao, Yu},
  journal={arXiv preprint arXiv:2303.16199},
  year={2023}
}

@misc{liu2024llava,
  title={Llava-next: Improved reasoning, ocr, and world knowledge},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
  year={2024}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}


@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}


@article{zhang2023video,
  title={Video-llama: An instruction-tuned audio-visual language model for video understanding},
  author={Zhang, Hang and Li, Xin and Bing, Lidong},
  journal={arXiv preprint arXiv:2306.02858},
  year={2023}
}

@article{lu2024deepseek,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}


@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}


@inproceedings{chen2024internvl,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={24185--24198},
  year={2024}
}


@article{hu2024minicpm,
  title={Minicpm: Unveiling the potential of small language models with scalable training strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}


%%开源vlm





%视觉tokens压缩相关论文
@article{bolya2022token,
  title={Token merging: Your vit but faster},
  author={Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy},
  journal={arXiv preprint arXiv:2210.09461},
  year={2022}
}
@article{bai2023qwen,
  title={Qwen-vl: A frontier large vision-language model with versatile abilities},
  author={Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
  journal={arXiv preprint arXiv:2308.12966},
  year={2023}
}

@article{zhang2024dockylin,
  title={Dockylin: A large multimodal model for visual document understanding with efficient visual slimming},
  author={Zhang, Jiaxin and Yang, Wentao and Lai, Songxuan and Xie, Zecheng and Jin, Lianwen},
  journal={arXiv preprint arXiv:2406.19101},
  year={2024}
}

@article{li2024tokenpacker,
  title={Tokenpacker: Efficient visual projector for multimodal llm},
  author={Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei},
  journal={arXiv preprint arXiv:2407.02392},
  year={2024}
}
@inproceedings{cha2024honeybee,
  title={Honeybee: Locality-enhanced projector for multimodal llm},
  author={Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13817--13827},
  year={2024}
}
@article{yu2024texthawk2,
  title={Texthawk2: A large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens},
  author={Yu, Ya-Qi and Liao, Minghui and Zhang, Jiwen and Wu, Jihao},
  journal={arXiv preprint arXiv:2410.05261},
  year={2024}
}
@article{zhang2024sparsevlm,
  title={Sparsevlm: Visual token sparsification for efficient vision-language model inference},
  author={Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others},
  journal={arXiv preprint arXiv:2410.04417},
  year={2024}
}
@article{yang2024visionzip,
  title={VisionZip: Longer is Better but Not Necessary in Vision Language Models},
  author={Yang, Senqiao and Chen, Yukang and Tian, Zhuotao and Wang, Chengyao and Li, Jingyao and Yu, Bei and Jia, Jiaya},
  journal={arXiv preprint arXiv:2412.04467},
  year={2024}
}
@article{chu2024mobilevlm,
  title={Mobilevlm v2: Faster and stronger baseline for vision language model},
  author={Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
  journal={arXiv preprint arXiv:2402.03766},
  year={2024}
}
@article{shang2024llava,
  title={Llava-prumerge: Adaptive token reduction for efficient large multimodal models},
  author={Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan},
  journal={arXiv preprint arXiv:2403.15388},
  year={2024}
}

@inproceedings{chen2025image,
  title={An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models},
  author={Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao},
  booktitle={European Conference on Computer Vision},
  pages={19--35},
  year={2025},
  organization={Springer}
}
@inproceedings{sundararajan2020many,
  title={The many Shapley values for model explanation},
  author={Sundararajan, Mukund and Najmi, Amir},
  booktitle={International conference on machine learning},
  pages={9269--9278},
  year={2020},
  organization={PMLR}
}
@article{hu2024mplug,
  title={mplug-docowl2: High-resolution compressing for ocr-free multi-page document understanding},
  author={Hu, Anwen and Xu, Haiyang and Zhang, Liang and Ye, Jiabo and Yan, Ming and Zhang, Ji and Jin, Qin and Huang, Fei and Zhou, Jingren},
  journal={arXiv preprint arXiv:2409.03420},
  year={2024}
}

@article{zhang2024beyond,
  title={Beyond LLaVA-HD: Diving into High-Resolution Large Multimodal Models},
  author={Zhang, Yi-Fan and Wen, Qingsong and Fu, Chaoyou and Wang, Xue and Zhang, Zhang and Wang, Liang and Jin, Rong},
  journal={arXiv preprint arXiv:2406.08487},
  year={2024}
}

%模型融合相关论文
@inproceedings{wortsman2022model,
  title={Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time},
  author={Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others},
  booktitle={International conference on machine learning},
  pages={23965--23998},
  year={2022},
  organization={PMLR}
}

@article{yan2024corrective,
  title={Corrective retrieval augmented generation},
  author={Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua},
  journal={arXiv preprint arXiv:2401.15884},
  year={2024}
}

@article{crawshaw2020multi,
  title={Multi-task learning with deep neural networks: A survey},
  author={Crawshaw, Michael},
  journal={arXiv preprint arXiv:2009.09796},
  year={2020}
}

@article{matena2111merging,
  title={Merging models with fisher-weighted averaging, 2021},
  author={Matena, Michael and Raffel, Colin},
  journal={arXiv preprint arXiv:2111.09832}
}

@article{ilharco2022editing,
  title={Editing models with task arithmetic},
  author={Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali},
  journal={arXiv preprint arXiv:2212.04089},
  year={2022}
}
@article{wei2023vary,
  title={Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models},
  author={Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yang, Jinrong and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
  journal={arXiv preprint arXiv:2312.06109},
  year={2023}
}

@article{fu2024mmecomprehensiveevaluationbenchmark,
      title={MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models}, 
      author={Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Jinrui Yang and Xiawu Zheng and Ke Li and Xing Sun and Yunsheng Wu and Rongrong Ji},
      year={2024},
      journal={arXiv preprint arXiv:2306.13394}
}
@article{ortiz2024task,
  title={Task arithmetic in the tangent space: Improved editing of pre-trained models},
  author={Ortiz-Jimenez, Guillermo and Favero, Alessandro and Frossard, Pascal},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{jin2022dataless,
  title={Dataless knowledge fusion by merging weights of language models},
  author={Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang},
  journal={arXiv preprint arXiv:2212.09849},
  year={2022}
}

@article{yadav2023resolving,
  title={Resolving interference when merging models},
  author={Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin and Bansal, Mohit},
  journal={arXiv preprint arXiv:2306.01708},
  volume={1},
  year={2023}
}


%dataset 论文
%DocVQA
@inproceedings{mathew2021docvqa,  
  title={Docvqa: A dataset for vqa on document images},
  author={Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
  pages={2200--2209},
  year={2021}
}
%ChartVQA
@inproceedings{masry2022chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  booktitle={Findings of the Association for Computational Linguistics},
  year={2022},
  pages = "2263--2279"
}
%TextVQA
@inproceedings{singh2019towards,
  title={Towards vqa models that can read},
  author={Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={8317--8326},
  year={2019}
}
%InfoVQA
@inproceedings{mathew2022infographicvqa,
  title={Infographicvqa},
  author={Mathew, Minesh and Bagal, Viraj and Tito, Rub{\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1697--1706},
  year={2022}
}
%OCRBench
@article{liu2024ocrbench,
  title={OCRBench: on the hidden mystery of OCR in large multimodal models},
  author={Liu, Yuliang and Li, Zhang and Huang, Mingxin and Yang, Biao and Yu, Wenwen and Li, Chunyuan and Yin, Xu-Cheng and Liu, Cheng-Lin and Jin, Lianwen and Bai, Xiang},
  journal={Science China Information Sciences},
  volume={67},
  number={12},
  pages={220102},
  year={2024},
}
%OCRBench_v2 
@article{fu2024ocrbench,
  title={OCRBench v2: An Improved Benchmark for Evaluating Large Multimodal Models on Visual Text Localization and Reasoning},
  author={Fu, Ling and Yang, Biao and Kuang, Zhebin and Song, Jiajun and Li, Yuzhe and Zhu, Linghao and Luo, Qidi and Wang, Xinyu and Lu, Hao and Huang, Mingxin and others},
  journal={arXiv preprint arXiv:2501.00321},
  year={2024}
}
%MathVista 
@inproceedings{lu2024mathvista,
  author    = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  title     = {MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts},
  booktitle={International Conference on Learning Representations (ICLR)},
  year      = {2024}
}

%scienceQA
@article{lu2022learn,
  title={Learn to explain: Multimodal reasoning via thought chains for science question answering},
  author={Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={2507--2521},
  year={2022}
}








%%%internvl2data
@article{abdin2024phi3,
  author = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah and Hany Awadalla and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Harkirat Behl and et al.},
  title = {Phi-3 technical report: A highly capable language model locally on your phone},
  journal = {arXiv preprint arXiv:2404.14219},
  year = {2024}
}

@inproceedings{acharya2019tallyqa,
  author = {Manoj Acharya and Kushal Kafle and Christopher Kanan},
  title = {Tallyqa: Answering complex counting questions},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {8076--8084},
  year = {2019}
}

@article{openai2023gpt4,
  author = {Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and et al.},
  title = {GPT-4 technical report},
  journal = {arXiv preprint arXiv:2303.08774},
  year = {2023}
}

@misc{agentsea_wave_ui,
  author = {AgentSea},
  title = {Wave-ui},
  howpublished = {\url{https://huggingface.co/datasets/agentsea/wave-ui}},
  year = {2024}
}

@inproceedings{alayrac2022flamingo,
  author = {Jean-Baptiste Alayrac and Jeff Donahue and Pauline Luc and Antoine Miech and Iain Barr and Yana Hasson and Karel Lenc and Arthur Mensch and Katherine Millican and Malcolm Reynolds and et al.},
  title = {Flamingo: a visual language model for few-shot learning},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {23716--23736},
  year = {2022}
}

@article{amini2019mathqa,
  author = {Aida Amini and Saadia Gabriel and Peter Lin and Rik Koncel-Kedziorski and Yejin Choi and Hannaneh Hajishirzi},
  title = {Mathqa: Towards interpretable math word problem solving with operation-based formalisms},
  journal = {arXiv preprint arXiv:1905.13319},
  year = {2019}
}

@inproceedings{anonymous2024cgbench,
  author = {Anonymous},
  title = {CG-bench: Clue-grounded question answering benchmark for long video understanding},
  booktitle = {Submitted to The Thirteenth International Conference on Learning Representations},
  year = {2024},
  note = {under review}
}

@misc{claude3series2024,
  author = {Anthropic},
  title = {The claude 3 model family: Opus, sonnet, haiku},
  howpublished = {\url{https://www.anthropic.com}},
  year = {2024}
}

@article{austin2021program,
  author = {Jacob Austin and Augustus Odena and Maxwell Nye and Maarten Bosma and Henryk Michalewski and David Dohan and Ellen Jiang and Carrie Cai and Michael Terry and Quoc Le and et al.},
  title = {Program synthesis with large language models},
  journal = {arXiv preprint arXiv:2108.07732},
  year = {2021}
}

@inproceedings{azuma2022scanqa,
  author = {Daichi Azuma and Taiki Miyanishi and Shuhei Kurita and Motoaki Kawanabe},
  title = {Scanqa: 3d question answering for spatial scene understanding},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {19129--19139},
  year = {2022}
}

@article{ba2016layer,
  author = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E Hinton},
  title = {Layer normalization},
  journal = {arXiv preprint arXiv:1607.06450},
  year = {2016}
}

@article{bai2021uibert,
  author = {Chongyang Bai and Xiaoxue Zang and Ying Xu and Srinivas Sunkara and Abhinav Rastogi and Jindong Chen and et al.},
  title = {Uibert: Learning generic multimodal representations for ui understanding},
  journal = {arXiv preprint arXiv:2107.13731},
  year = {2021}
}

@article{bai2023qwenvl,
  author = {Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
  title = {Qwen-vl: A frontier large vision-language model with versatile abilities},
  journal = {arXiv preprint arXiv:2308.12966},
  year = {2023}
}

@inproceedings{chng2019art,
  author = {Chee Kheng Chng and Yuliang Liu and Yipeng Sun and Chun Chet Ng and Canjie Luo and Zihan Ni and ChuanMing Fang and Shuaitao Zhang and Junyu Han and Errui Ding and et al.},
  title = {Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages = {1571--1576},
  year = {2019}
}

@article{chung2024scaling,
  author = {Hyung Won Chung and Le Hou and Shayne Longpre and Barret Zoph and Yi Tay and William Fedus and Yunxuan Li and Xuezhi Wang and Mostafa Dehghani and Siddhartha Brahma and et al.},
  title = {Scaling instruction-finetuned language models},
  journal = {Journal of Machine Learning Research},
  volume = {25},
  number = {70},
  pages = {1--53},
  year = {2024}
}

@inproceedings{clark2017docqa,
  author = {Christopher Clark and Matt Gardner},
  title = {Simple and effective multi-paragraph reading comprehension},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  pages = {845--855},
  year = {2018}
}

@article{cobbe2021training,
  author = {Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Mark Chen and Heewoo Jun and Lukasz Kaiser and Matthias Plappert and Jerry Tworek and Jacob Hilton and Reiichiro Nakano and et al.},
  title = {Training verifiers to solve math word problems},
  journal = {arXiv preprint arXiv:2110.14168},
  year = {2021}
}

@misc{conover2023free,
  author = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and Ali Ghodsi and Patrick Wendell and Matei Zaharia and Reynold Xin},
  title = {Free dolly: Introducing the world’s first truly open instruction-tuned llm},
  howpublished = {Company Blog of Databricks},
  year = {2023}
}

@misc{contributors2020mmsegmentation,
  author = {MMSegmentation Contributors},
  title = {Mmsegmentation: Openmmlab semantic segmentation toolbox and benchmark},
  howpublished = {\url{https://github.com/open-mmlab/mmsegmentation}},
  year = {2020}
}

@misc{opencompass2023,
  author = {OpenCompass Contributors},
  title = {Opencompass: A universal evaluation platform for foundation models},
  howpublished = {\url{https://github.com/open-compass/opencompass}},
  year = {2023}
}

@misc{realworldqa,
  author = {X.AI Corp.},
  title = {Grok-1.5 vision preview: Connecting the digital and physical worlds with our first multimodal model},
  howpublished = {\url{https://x.ai/blog/grok-1.5v}},
  year = {2024}
}

@article{cui2023ultrafeedback,
  author = {Ganqu Cui and Lifan Yuan and Ning Ding and Guanming Yao and Wei Zhu and Yuan Ni and Guotong Xie and Zhiyuan Liu and Maosong Sun},
  title = {Ultrafeedback: Boosting language models with high-quality feedback},
  journal = {arXiv preprint arXiv:2310.01377},
  year = {2023}
}

@article{dai202415m,
  author = {Dawei Dai and YuTang Li and YingGe Liu and Mingming Jia and Zhang YuanHui and Guoyin Wang},
  title = {15m multimodal facial image-text dataset},
  journal = {arXiv preprint arXiv:2407.08515},
  year = {2024}
}

@article{dai2024nvlm,
  author = {Wenliang Dai and Nayeon Lee and Boxin Wang and Zhuolin Yang and Zihan Liu and Jon Barker and Tuomas Rintamaki and Mohammad Shoeybi and Bryan Catanzaro and Wei Ping},
  title = {Nvlm: Open frontier-class multimodal llms},
  journal = {arXiv preprint arXiv:2409.11402},
  year = {2024}
}

@inproceedings{das2017visdial,
  author = {Abhishek Das and Satwik Kottur and Khushi Gupta and Avi Singh and Deshraj Yadav and Jos{\'e} MF Moura and Devi Parikh and Dhruv Batra},
  title = {Visual dialog},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {326--335},
  year = {2017}
}

@inproceedings{davis2019deep,
  author = {Brian Davis and Bryan Morse and Scott Cohen and Brian Price and Chris Tensmeyer},
  title = {Deep visual template-free form parsing},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages = {134--141},
  year = {2019}
}

@inproceedings{dehghani2023vit22b,
  author = {Mostafa Dehghani and Josip Djolonga and Basil Mustafa and Piotr Padlewski and Jonathan Heek and Justin Gilmer and Andreas Peter Steiner and Mathilde Caron and Robert Geirhos and Ibrahim Alabdulmohsin and et al.},
  title = {Scaling vision transformers to 22 billion parameters},
  booktitle = {International Conference on Machine Learning},
  pages = {7480--7512},
  year = {2023}
}

@article{deitke2024molmo,
  author = {Matt Deitke and Christopher Clark and Sangho Lee and Rohun Tripathi and Yue Yang and Jae Sung Park and Mohammadreza Salehi and Niklas Muennighoff and Kyle Lo and Luca Soldaini and et al.},
  title = {Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models},
  journal = {arXiv preprint arXiv:2409.17146},
  year = {2024}
}

@inproceedings{deka2017rico,
  author = {Biplab Deka and Zifeng Huang and Chad Franzen and Joshua Hibschman and Daniel Afergan and Yang Li and Jeffrey Nichols and Ranjitha Kumar},
  title = {Rico: A mobile app dataset for building data-driven design applications},
  booktitle = {Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
  pages = {845--854},
  year = {2017}
}

@article{deng2009imagenet,
  author    = {Jia Deng and Wei Dong and Richard Socher and Li-Jia Li and Kai Li and Li Fei-Fei},
  title     = {Imagenet: A large-scale hierarchical image database},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {248--255},
  year      = {2009}
}

@article{deng2024mind2web,
  author    = {Xiang Deng and Yu Gu and Boyuan Zheng and Shijie Chen and Sam Stevens and Boshi Wang and Huan Sun and Yu Su},
  title     = {Mind2web: Towards a generalist agent for the web},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {36},
  year      = {2024}
}

@article{ding2023enhancing,
  author    = {Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},
  title     = {Enhancing chat language models by scaling high-quality instructional conversations},
  journal   = {arXiv preprint arXiv:2305.14233},
  year      = {2023}
}

@article{doan2024vintern,
  author    = {Khang T Doan and Bao G Huynh and Dung T Hoang and Thuc D Pham and Nhat H Pham and Quan Nguyen and Bang Q Vo and Suong N Hoang},
  title     = {Vintern-1b: An efficient multimodal large language model for vietnamese},
  journal   = {arXiv preprint arXiv:2408.12480},
  year      = {2024}
}

@article{dong2024xc24khd,
  author    = {Xiaoyi Dong and Pan Zhang and Yuhang Zang and Yuhang Cao and Bin Wang and Linke Ouyang and Songyang Zhang and Haodong Duan and Wenwei Zhang and Yining Li et al.},
  title     = {Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd},
  journal   = {arXiv preprint arXiv:2404.06512},
  year      = {2024}
}

@article{dosovitskiy2020image,
  author    = {Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly et al.},
  title     = {An image is worth 16x16 words: Transformers for image recognition at scale},
  journal   = {The International Conference on Learning Representations},
  year      = {2020}
}

@article{du2022glm,
  author    = {Zhengxiao Du and Yujie Qian and Xiao Liu and Ming Ding and Jiezhong Qiu and Zhilin Yang and Jie Tang},
  title     = {Glm: General language model pretraining with autoregressive blank infilling},
  journal   = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  pages     = {320--335},
  year      = {2022}
}

@inproceedings{duan2024vlmevalkit,
  author    = {Haodong Duan and Junming Yang and Yuxuan Qiao and Xinyu Fang and Lin Chen and Yuan Liu and Xiaoyi Dong and Yuhang Zang and Pan Zhang and Jiaqi Wang et al.},
  title     = {Vlmevalkit: An open-source toolkit for evaluating large multi-modality models},
  booktitle = {Proceedings of the 32nd ACM International Conference on Multimedia},
  pages     = {11198--11201},
  year      = {2024}
}

@article{dubey2024llama3,
  author    = {Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Amy Yang and Angela Fan et al.},
  title     = {The llama 3 herd of models},
  journal   = {arXiv preprint arXiv:2407.21783},
  year      = {2024}
}

@article{fang2024mmbench,
  author    = {Xinyu Fang and Kangrui Mao and Haodong Duan and Xiangyu Zhao and Yining Li and Dahua Lin and Kai Chen},
  title     = {Mmbench-video: A long-form multi-shot benchmark for holistic video understanding},
  journal   = {arXiv preprint arXiv:2406.14515},
  year      = {2024}
}

@article{fang2022eva,
  author    = {Yuxin Fang and Wen Wang and Binhui Xie and Quan Sun and Ledell Wu and Xinggang Wang and Tiejun Huang and Xinlong Wang and Yue Cao},
  title     = {Eva: Exploring the limits of masked visual representation learning at scale},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {19358--19369},
  year      = {2023}
}

@misc{FineVideo,
  author    = {Miquel Farré and Andi Marafioti and Lewis Tunstall and Leandro Von Werra and Thomas Wolf},
  title     = {Finevideo},
  year      = {2024},
  url       = {https://huggingface.co/datasets/HuggingFaceFV/finevideo}
}

@article{fu2023mme,
  author    = {Chaoyou Fu and Peixian Chen and Yunhang Shen and Yulei Qin and Mengdan Zhang and Xu Lin and Zhenyu Qiu and Wei Lin and Jinrui Yang and Xiawu Zheng et al.},
  title     = {Mme: A comprehensive evaluation benchmark for multimodal large language models},
  journal   = {arXiv preprint arXiv:2306.13394},
  year      = {2023}
}

@article{fu2024video,
  author    = {Chaoyou Fu and Yuhan Dai and Yondong Luo and Lei Li and Shuhuai Ren and Renrui Zhang and Zihan Wang and Chenyu Zhou and Yunhang Shen and Mengdan Zhang et al.},
  title     = {Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis},
  journal   = {arXiv preprint arXiv:2405.21075},
  year      = {2024}
}

@article{fu2024blink,
  author    = {Xingyu Fu and Yushi Hu and Bangzheng Li and Yu Feng and Haoyu Wang and Xudong Lin and Dan Roth and Noah A Smith and Wei-Chiu Ma and Ranjay Krishna},
  title     = {Blink: Multimodal large language models can see but not perceive},
  journal   = {arXiv preprint arXiv:2404.12390},
  year      = {2024}
}

@article{gao2024mini_internvl,
  author    = {Zhangwei Gao and Zhe Chen and Erfei Cui and Yiming Ren and Weiyun Wang and Jinguo Zhu and Hao Tian and Shenglong Ye and Junjun He and Xizhou Zhu et al.},
  title     = {Mini-internvl: A flexible-transfer pocket multimodal model with 5\% parameters and 90\% performance},
  journal   = {arXiv preprint arXiv:2410.16261},
  year      = {2024}
}

@inproceedings{garcia2015overview,
  author    = {Alba Garcia Seco De Herrera and Henning Müller and Stefano Bromuri},
  title     = {Overview of the imageclef 2015 medical classification task},
  booktitle = {Working Notes of CLEF 2015--Cross Language Evaluation Forum, CEUR},
  volume    = {1391},
  series    = {CEUR Workshop Proceedings},
  year      = {2015}
}

@misc{glaive_code_assistant_v3,
  author    = {GlaiveAI},
  title     = {Glaive code assistant v3 dataset},
  year      = {2024},
  url       = {https://huggingface.co/datasets/glaiveai/glaive-code-assistant-v3}
}

@article{goyal2017vqav2,
  author    = {Yash Goyal and Tejas Khot and Douglas Summers-Stay and Dhruv Batra and Devi Parikh},
  title     = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
  journal   = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {6904--6913},
  year      = {2017}
}

@article{gu2022wukong,
  author    = {Jiaxi Gu and Xiaojun Meng and Guansong Lu and Lu Hou and Niu Minzhe and Xiaodan Liang and Lewei Yao and Runze Zhang and Guodong Zhou and Jiwei Li},
  title     = {Wukong: A multimodal large language model for real-world applications},
  journal   = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  year      = {2022}
}

@article{gu2024aquilavl,
  author    = {Shuhao Gu and Jialing Zhang and Siyuan Zhou and Kevin Yu and Zhaohu Xing and Liangdong Wang and Zhou Cao and Jintao Jia and Zhuoyi Zhang and Yixuan Wang and et~al.},
  title     = {Infinity-mm: Scaling multimodal performance with large-scale and high-quality instruction data},
  journal   = {arXiv preprint arXiv:2410.18558},
  year      = {2024}
}

@article{guan2023hallusionbench,
  author    = {Tianrui Guan and Fuxiao Liu and Xiyang Wu and Ruiqi Xian and Zongxia Li and Xiaoyu Liu and Xijun Wang and Lichang Chen and Furong Huang and Yaser Yacoob and et~al.},
  title     = {Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models},
  journal   = {arXiv preprint arXiv:2310.14566},
  year      = {2023}
}

@inproceedings{guo2019eaten,
  author    = {He~Guo and Xiameng Qin and Jiaming Liu and Junyu Han and Jingtuo Liu and Errui Ding},
  title     = {Eaten: Entity-aware attention for single shot visual text extraction},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages     = {254--259},
  year      = {2019}
}

@inproceedings{gupta2016synthtext,
  author    = {Ankush Gupta and Andrea Vedaldi and Andrew Zisserman},
  title     = {Synthetic data for text localisation in natural images},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {2315--2324},
  year      = {2016}
}

@article{he2024olympiadbench,
  author    = {Chaoqun He and Renjie Luo and Yuzhuo Bai and Shengding Hu and Zhen~Leng Thai and Junhao Shen and Jinyi Hu and Xu~Han and Yujie Huang and Yuxiang Zhang and et~al.},
  title     = {Olympiadbench: A challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems},
  journal   = {arXiv preprint arXiv:2402.14008},
  year      = {2024}
}

@article{he2023wanjuan,
  author    = {Conghui He and Zhenjiang Jin and Chao Xu and Jiantao Qiu and Bin Wang and Wei Li and Hang Yan and Jiaqi Wang and Dahua Lin},
  title     = {Wanjuan: A comprehensive multimodal dataset for advancing english and chinese large models},
  journal   = {arXiv preprint arXiv:2308.10755},
  year      = {2023}
}

@inproceedings{he2018icpr2018,
  author    = {Mengchao He and Yuliang Liu and Zhibo Yang and Sheng Zhang and Canjie Luo and Feiyu Gao and Qi~Zheng and Yongpan Wang and Xin Zhang and Lianwen Jin},
  title     = {Icpr2018 contest on robust reading for multi-type web images},
  booktitle = {International Conference on Pattern Recognition},
  pages     = {7--12},
  year      = {2018}
}

@article{he2020pathvqa,
  author    = {Xuehai He and Yichen Zhang and Luntian Mou and Eric Xing and Pengtao Xie},
  title     = {Pathvqa: 30000+ questions for medical visual question answering},
  journal   = {arXiv preprint arXiv:2003.10286},
  year      = {2020}
}

@inproceedings{hendrycks2021imagenet_r,
  author    = {Dan Hendrycks and Steven Basart and Norman Mu and Saurav Kadavath and Frank Wang and Evan Dorundo and Rahul Desai and Tyler Zhu and Samyak Parajuli and Mike Guo and et~al.},
  title     = {The many faces of robustness: A critical analysis of out-of-distribution generalization},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {8340--8349},
  year      = {2021}
}

@inproceedings{hendrycks2020measuring,
  author    = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  title     = {Measuring massive multitask language understanding},
  booktitle = {The International Conference on Learning Representations},
  year      = {2020}
}

@inproceedings{DBLP:conf/nips/HendrycksBKABTS21,
  author    = {Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  title     = {Measuring mathematical problem solving with the {MATH} dataset},
  booktitle = {Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual},
  year      = {2021}
}

@inproceedings{hendrycks2021imagenet_a,
  author    = {Dan Hendrycks and Kevin Zhao and Steven Basart and Jacob Steinhardt and Dawn Song},
  title     = {Natural adversarial examples},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {15262--15271},
  year      = {2021}
}

@inproceedings{hessel2023androids,
  author    = {Jack Hessel and Ana Marasovi{\'c} and Jena~D. Hwang and Lillian Lee and Jeff Da and Rowan Zellers and Robert Mankoff and Yejin Choi},
  title     = {Do androids laugh at electric sheep? {Humor} ``understanding'' benchmarks from {The New Yorker Caption Contest}},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  year      = {2023}
}

@misc{hezarai_parsynth_ocr_200k,
  author    = {Hezarai},
  title     = {Parsynth-ocr-200k},
  howpublished = {\url{https://huggingface.co/datasets/hezarai/parsynth-ocr-200k}},
  year      = {2024}
}

@article{honovich2022unnatural,
  author    = {Or~Honovich and Thomas Scialom and Omer Levy and Timo Schick},
  title     = {Unnatural instructions: Tuning language models with (almost) no human labor},
  journal   = {arXiv preprint arXiv:2212.09689},
  year      = {2022}
}

@article{hosu2020koniq,
  author    = {Vlad Hosu and Hanhe Lin and Tamas Sziranyi and Dietmar Saupe},
  title     = {Koniq-10k: An ecologically valid database for deep learning of blind image quality assessment},
  journal   = {IEEE Transactions on Image Processing},
  volume    = {29},
  pages     = {4041--4056},
  year      = {2020}
}

@article{hsiao2022screenqa,
  author    = {Yu-Chung Hsiao and Fedir Zubach and Gilles Baechler and Victor Carbune and Jason Lin and Maria Wang and Srinivas Sunkara and Yun Zhu and Jindong Chen},
  title     = {Screenqa: Large-scale question-answer pairs over mobile app screenshots},
  journal   = {arXiv preprint arXiv:2209.08199},
  year      = {2022}
}

@article{hu2024mplug_docowl_1_5,
  author    = {Anwen Hu and Haiyang Xu and Jiabo Ye and Ming Yan and Liang Zhang and Bo~Zhang and Chen Li and Ji~Zhang and Qin Jin and Fei Huang and et~al.},
  title     = {mplug-docowl 1.5: Unified structure learning for ocr-free document understanding},
  journal   = {arXiv preprint arXiv:2403.12895},
  year      = {2024}
}

@article{hu2023medical,
  author    = {Xinyue Hu and L~Gu and Q~An and M~Zhang and L~Liu and K~Kobayashi and T~Harada and R~Summers and Y~Zhu},
  title     = {Medical-diff-vqa: a large-scale medical dataset for difference visual question answering on chest x-ray images},
  journal   = {PhysioNet},
  year      = {2023}
}

@inproceedings{huang2020movienet,
  author    = {Qingqiu Huang and Yu~Xiong and Anyi Rao and Jiaze Wang and Dahua Lin},
  title     = {Movienet: A holistic dataset for movie understanding},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {3555--3564},
  year      = {2020}
}

@inproceedings{huang2019icdar2019,
  author    = {Zheng Huang and Kai Chen and Jianhua He and Xiang Bai and Dimosthenis Karatzas and Shijian Lu and CV Jawahar},
  title     = {Icdar2019 competition on scanned receipt ocr and information extraction},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages     = {1516--1520},
  year      = {2019}
}

@inproceedings{hudson2019gqa,
  author    = {Drew A Hudson and Christopher D Manning},
  title     = {Gqa: A new dataset for real-world visual reasoning and compositional question answering},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {6700--6709},
  year      = {2019}
}

@inproceedings{jia2022egotaskqa,
  author    = {Baoxiong Jia and Ting Lei and Song-Chun Zhu and Siyuan Huang},
  title     = {Egotaskqa: Understanding human tasks in egocentric videos},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {35},
  pages     = {3343--3360},
  year      = {2022}
}

@article{jiang2024mantis,
  author    = {Dongfu Jiang and Xuan He and Huaye Zeng and Cong Wei and Max Ku and Qian Liu and Wenhu Chen},
  title     = {Mantis: Interleaved multi-image instruction tuning},
  journal   = {arXiv preprint arXiv:2405.01483},
  year      = {2024}
}

@article{jiao2024img,
  author    = {Qirui Jiao and Daoyuan Chen and Yilun Huang and Yaliang Li and Ying Shen},
  title     = {Img-diff: Contrastive data synthesis for multimodal large language models},
  journal   = {arXiv preprint arXiv:2408.04594},
  year      = {2024}
}

@misc{textocr_gpt4v_dataset,
  author    = {Jimmycarter},
  title     = {Textocr gpt-4v dataset},
  year      = {2023},
  url       = {https://huggingface.co/datasets/jimmycarter/textocr-gpt4v}
}

@article{joshi2017triviaqa,
  author    = {Mandar Joshi and Eunsol Choi and Daniel S Weld and Luke Zettlemoyer},
  title     = {Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  journal   = {arXiv preprint arXiv:1705.03551},
  year      = {2017}
}

@inproceedings{kafle2018dvqa,
  author    = {Kushal Kafle and Brian Price and Scott Cohen and Christopher Kanan},
  title     = {Dvqa: Understanding data visualizations via question answering},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {5648--5656},
  year      = {2018}
}

@article{kahou2017figureqa,
  author    = {Samira Ebrahimi Kahou and Vincent Michalski and Adam Atkinson and Ákos Kádár and Adam Trischler and Yoshua Bengio},
  title     = {Figureqa: An annotated figure dataset for visual reasoning},
  journal   = {arXiv preprint arXiv:1710.07300},
  year      = {2017}
}

@inproceedings{kapoor2025omniact,
  author    = {Raghav Kapoor and Yash Parag Butala and Melisa Russak and Jing Yu Koh and Kiran Kamble and Waseem AlShikh and Ruslan Salakhutdinov},
  title     = {Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web},
  booktitle = {European Conference on Computer Vision},
  pages     = {161--178},
  publisher = {Springer},
  year      = {2025}
}

@article{kazemi2023geomverse,
  author    = {Mehran Kazemi and Hamidreza Alvari and Ankit Anand and Jialin Wu and Xi Chen and Radu Soricut},
  title     = {Geomverse: A systematic evaluation of large models for geometric reasoning},
  journal   = {arXiv preprint arXiv:2312.12241},
  year      = {2023}
}

@inproceedings{kazemzadeh2014referitgame,
  author    = {Sahar Kazemzadeh and Vicente Ordonez and Mark Matten and Tamara Berg},
  title     = {Referitgame: Referring to objects in photographs of natural scenes},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing},
  pages     = {787--798},
  year      = {2014}
}

@inproceedings{kembhavi2016ai2d,
  author    = {Aniruddha Kembhavi and Mike Salvato and Eric Kolve and Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi},
  title     = {A diagram is worth a dozen images},
  booktitle = {European Conference on Computer Vision},
  pages     = {235--251},
  year      = {2016}
}

@inproceedings{kembhavi2017tqa,
  author    = {Aniruddha Kembhavi and Minjoon Seo and Dustin Schwenk and Jonghyun Choi and Ali Farhadi and Hannaneh Hajishirzi},
  title     = {Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4999--5007},
  year      = {2017}
}

@inproceedings{kiela2020hateful,
  author    = {Douwe Kiela and Hamed Firooz and Aravind Mohan and Vedanuj Goswami and Amanpreet Singh and Pratik Ringshia and Davide Testuggine},
  title     = {The hateful memes challenge: Detecting hate speech in multimodal memes},
  journal   = {Advances in Neural Information Processing Systems},
  volume    = {33},
  pages     = {2611--2624},
  year      = {2020}
}

@inproceedings{kim2022synthdog,
  author    = {Geewook Kim and Teakgyu Hong and Moonbin Yim and JeongYeon Nam and Jinyoung Park and Jinyeong Yim and Wonseok Hwang and Sangdoo Yun and Dongyoon Han and Seunghyun Park},
  title     = {Ocr-free document understanding transformer},
  booktitle = {European Conference on Computer Vision},
  pages     = {498--517},
  publisher = {Springer},
  year      = {2022}
}

@inproceedings{kirillov2023segment,
  author    = {Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C Berg and Wan-Yen Lo and others},
  title     = {Segment anything},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages     = {4015--4026},
  year      = {2023}
}

@misc{knowrohit07_know_saraswati_cot,
  author    = {knowrohit07},
  title     = {Know Saraswati COT dataset},
  year      = {2023},
  url       = {https://huggingface.co/datasets/knowrohit07/know-saraswati-cot}
}

@inproceedings{kuang2023visual,
  author    = {Jianfeng Kuang and Wei Hua and Dingkang Liang and Mingkun Yang and Deqiang Jiang and Bo Ren and Xiang Bai},
  title     = {Visual information extraction in the wild: practical dataset and end-to-end solution},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages     = {36--53},
  publisher = {Springer},
  year      = {2023}
}

@article{kuznetsova2020openimage,
  author    = {Alina Kuznetsova and Hassan Rom and Neil Alldrin and Jasper Uijlings and Ivan Krasin and Jordi Pont-Tuset and Shahab Kamali and Stefan Popov and Matteo Malloci and Alexander Kolesnikov and others},
  title     = {The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale},
  journal   = {IJCV},
  volume    = {128},
  number    = {7},
  pages     = {1956--1981},
  year      = {2020}
}

@article{naturalquestion,
  author    = {Tom Kwiatkowski and Jennimaria Palomaki and Olivia Redfield and Michael Collins and Ankur Parikh and Chris Alberti and Danielle Epstein and Illia Polosukhin and Jacob Devlin and Kenton Lee and others},
  title     = {Natural questions: a benchmark for question answering research},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {7},
  pages     = {453--466},
  year      = {2019}
}

@article{lai2017race,
  author    = {Guokun Lai and Qizhe Xie and Hanxiao Liu and Yiming Yang and Eduard Hovy},
  title     = {Race: Large-scale reading comprehension dataset from examinations},
  journal   = {arXiv preprint arXiv:1704.04683},
  year      = {2017}
}

@misc{laion_gpt4v_dataset,
  author    = {LAION},
  title     = {Gpt-4v dataset},
  year      = {2023},
  url       = {https://huggingface.co/datasets/laion/gpt4v-dataset}
}

@article{lau2018dataset,
  author    = {Jason J Lau and Soumya Gayen and Asma Ben Abacha and Dina Demner-Fushman},
  title     = {A dataset of clinically generated visual questions and answers about radiology images},
  journal   = {Scientific data},
  volume    = {5},
  pages     = {1--10},
  year      = {2018}
}

@article{2024docmatrix,
  author    = {Hugo Laurènçon and Andrés Marafioti and Victor Sanh and Léo Tronchon},
  title     = {Building and better understanding vision-language models: insights and future directions},
  journal   = {arXiv preprint arXiv:2408.12637},
  year      = {2024}
}
@article{laurenccon2024unlocking,
  author = {Hugo Lauren{\c{c}}on and L{\'e}o Tronchon and Victor Sanh},
  title = {Unlocking the conversion of web screenshots into html code with the websight dataset},
  journal = {arXiv preprint arXiv:2403.09029},
  year = {2024}
}

@inproceedings{lerner2022viquae,
  author = {Paul Lerner and Olivier Ferret and Camille Guinaudeau and Herv{\'e} Le~Borgne and Romaric Besan{\c{c}}on and Jos{\'e}~G Moreno and Jes{\'u}s Lov{\'o}n~Melgarejo},
  title = {Viquae, a dataset for knowledge-based visual question answering about named entities},
  booktitle = {Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages = {3108--3120},
  year = {2022}
}

@article{li2024llavaov,
  author = {Bo~Li and Yuanhan Zhang and Dong Guo and Renrui Zhang and Feng Li and Hao Zhang and Kaichen Zhang and Yanwei Li and Ziwei Liu and Chunyuan Li},
  title = {Llava-onevision: Easy visual task transfer},
  journal = {arXiv preprint arXiv:2408.03326},
  year = {2024}
}

@article{li2024seedbench2plus,
  author = {Bohao Li and Yuying Ge and Yi~Chen and Yixiao Ge and Ruimao Zhang and Ying Shan},
  title = {Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension},
  journal = {arXiv preprint arXiv:2404.16790},
  year = {2024}
}

@article{li2024r,
  author = {Chunyi Li and Jianbo Zhang and Zicheng Zhang and Haoning Wu and Yuan Tian and Wei Sun and Guo Lu and Xiaohong Liu and Xiongkuo Min and Weisi Lin and others},
  title = {R-bench: Are your large multimodal model robust to real-world corruptions?},
  journal = {arXiv preprint arXiv:2410.05474},
  year = {2024}
}

@article{li2023cmmlu,
  author = {Haonan Li and Yixuan Zhang and Fajri Koto and Yifei Yang and Hai Zhao and Yeyun Gong and Nan Duan and Timothy Baldwin},
  title = {Cmmlu: Measuring massive multitask language understanding in chinese},
  journal = {arXiv preprint arXiv:2306.09212},
  year = {2023}
}

@misc{li2024numinamath,
  author = {Jia Li and Edward Beeching and Lewis Tunstall and Ben Lipkin and Roman Soletskyi and Shengyi Huang and Kashif Rasul and Longhui Yu and Albert~Q Jiang and Ziju Shen and others},
  title = {Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions},
  note = {Hugging Face repository},
  year = {2024}
}

@article{li2024chemvlm,
  author = {Junxian Li and Di~Zhang and Xunzhi Wang and Zeying Hao and Jingdi Lei and Qian Tan and Cai Zhou and Wei Liu and Yaotian Yang and Xinrui Xiong and others},
  title = {Chemvlm: Exploring the power of multimodal large language models in chemistry area},
  journal = {arXiv preprint arXiv:2408.07246},
  year = {2024}
}

@article{li2023videochat,
  author = {KunChang Li and Yinan He and Yi~Wang and Yizhuo Li and Wenhai Wang and Ping Luo and Yali Wang and Limin Wang and Yu~Qiao},
  title = {Videochat: Chat-centric video understanding},
  journal = {arXiv preprint arXiv:2305.06355},
  year = {2023}
}

@inproceedings{li2024mvbench,
  author = {Kunchang Li and Yali Wang and Yinan He and Yizhuo Li and Yi~Wang and Yi~Liu and Zun Wang and Jilan Xu and Guo Chen and Ping Luo and others},
  title = {Mvbench: A comprehensive multi-modal video understanding benchmark},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {22195--22206},
  year = {2024}
}

@article{li2024multimodal,
  author = {Lei Li and Yuqi Wang and Runxin Xu and Peiyi Wang and Xiachong Feng and Lingpeng Kong and Qi~Liu},
  title = {Multimodal arxiv: A dataset for improving scientific comprehension of large vision-language models},
  journal = {arXiv preprint arXiv:2403.00231},
  year = {2024}
}

@article{li2024omnicorpus,
  author = {Qingyun Li and Zhe Chen and Weiyun Wang and Wenhai Wang and Shenglong Ye and Zhenjiang Jin and Guanzhou Chen and Yinan He and Zhangwei Gao and Erfei Cui and others},
  title = {Omnicorpus: An unified multimodal corpus of 10 billion-level images interleaved with text},
  journal = {arXiv preprint arXiv:2406.08418},
  year = {2024}
}

@article{li2024gmai,
  author = {Tianbin Li and Yanzhou Su and Wei Li and Bin Fu and Zhe Chen and Ziyan Huang and Guoan Wang and Chenglong Ma and Ying Chen and Ming Hu and others},
  title = {Gmai-vl \& gmai-vl-5.5 m: A large vision-language model and a comprehensive multimodal dataset towards general medical ai},
  journal = {arXiv preprint arXiv:2411.14522},
  year = {2024}
}

@inproceedings{lieffects,
  author    = {Wei Li and William E Bishop and Alice Li and Christopher Rawles and Folawiyo Campbell-Ajala and Divya Tyamagundlu and Oriana Riva},
  title     = {On the effects of data scale on ui control agents},
  booktitle = {The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track},
  year      = {2024}
}

@article{li2020widget,
  author    = {Yang Li and Gang Li and Luheng He and Jingjie Zheng and Hong Li and Zhiwei Guan},
  title     = {Widget captioning: Generating natural language description for mobile user interface elements},
  journal   = {arXiv preprint arXiv:2010.04295},
  year      = {2020}
}

@inproceedings{li2021improved,
  author    = {Yanghao Li and Chao-Yuan Wu and Haoqi Fan and Karttikeya Mangalam and Bo Xiong and Jitendra Malik and Christoph Feichtenhofer},
  title     = {Mvitv2: Improved multiscale vision transformers for classification and detection},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {4804--4814},
  year      = {2022}
}

@article{li2024miniGemini,
  author    = {Yanwei Li and Yuechen Zhang and Chengyao Wang and Zhisheng Zhong and Yixin Chen and Ruihang Chu and Shaoteng Liu and Jiaya Jia},
  title     = {Mini-gemini: Mining the potential of multi-modality vision language models},
  journal   = {arXiv preprint arXiv:2403.18814},
  year      = {2024}
}

@inproceedings{li2023pope,
  author    = {Yifan Li and Yifan Du and Kun Zhou and Jinpeng Wang and Wayne Xin Zhao and Ji-Rong Wen},
  title     = {Evaluating object hallucination in large vision-language models},
  booktitle = {The Conference on Empirical Methods in Natural Language Processing},
  pages     = {292--305},
  year      = {2023}
}

@article{li2023monkey,
  author    = {Zhang Li and Biao Yang and Qiang Liu and Zhiyin Ma and Shuo Zhang and Jingxu Yang and Yabo Sun and Yuliang Liu and Xiang Bai},
  title     = {Monkey: Image resolution and text label are important things for large multi-modal models},
  journal   = {arXiv preprint arXiv:2311.06607},
  year      = {2023}
}

@inproceedings{li2023superclevr,
  author    = {Zhuowan Li and Xingrui Wang and Elias Stengel-Eskin and Adam Kortylewski and Wufei Ma and Benjamin Van Durme and Alan L Yuille},
  title     = {Super-clevr: A virtual benchmark to diagnose domain robustness in visual reasoning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {14963--14973},
  year      = {2023}
}

@misc{SlimOrca,
  author    = {Wing Lian and Guan Wang and Bleys Goodson and Eugene Pentland and Austin Cook and Chanvichet Vong and "Teknium"},
  title     = {Slimorca: An open dataset of gpt-4 augmented flan reasoning traces, with verification},
  url       = {https://https://huggingface.co/Open-Orca/SlimOrca},
  year      = {2023}
}

@inproceedings{lin2024vila,
  author    = {Ji Lin and Hongxu Yin and Wei Ping and Pavlo Molchanov and Mohammad Shoeybi and Song Han},
  title     = {Vila: On pre-training for visual language models},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages     = {26689--26699},
  year      = {2024}
}

@article{lindstrom2022clevrmath,
  author    = {Adam Dahlgren Lindström and Savitha Sam Abraham},
  title     = {Clevr-math: A dataset for compositional language, visual and mathematical reasoning},
  journal   = {arXiv preprint arXiv:2208.05358},
  year      = {2022}
}

@inproceedings{liu2021slake,
  author    = {Bo Liu and Li-Ming Zhan and Li Xu and Lin Ma and Yan Yang and Xiao-Ming Wu},
  title     = {Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
  booktitle = {2021 IEEE 18th International Symposium on Biomedical Imaging},
  pages     = {1650--1654},
  year      = {2021}
}

@article{liu2020casia,
  author    = {Brian Liu and Xianchao Xu and Yu Zhang},
  title     = {Offline handwritten chinese text recognition with convolutional neural networks},
  journal   = {arXiv preprint arXiv:2006.15619},
  year      = {2020}
}

@article{liu2023vsr,
  author    = {Fangyu Liu and Guy Emerson and Nigel Collier},
  title     = {Visual spatial reasoning},
  journal   = {Transactions of the Association for Computational Linguistics},
  volume    = {11},
  pages     = {635--651},
  year      = {2023}
}

@article{liu2023lrv-instruction,
  author    = {Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang},
  title     = {Aligning large multi-modal model with robust instruction tuning},
  journal   = {arXiv preprint arXiv:2306.14565},
  year      = {2023}
}
@inproceedings{kembhavi2016diagram,
  title={A diagram is worth a dozen images},
  author={Kembhavi, Aniruddha and Salvato, Mike and Kolve, Eric and Seo, Minjoon and Hajishirzi, Hannaneh and Farhadi, Ali},
  booktitle={In Proceedings of European Conference on Computer Vision},
  pages={235--251},
  year={2016}
}
@article{liu2023mmc,
  author    = {Fuxiao Liu and Xiaoyang Wang and Wenlin Yao and Jianshu Chen and Kaiqiang Song and Sangwoo Cho and Yaser Yacoob and Dong Yu},
  title     = {Mmc: Advancing multimodal chart understanding with large-scale instruction tuning},
  journal   = {arXiv preprint arXiv:2311.10774},
  year      = {2023}
}

@article{liu2023improved,
  author    = {Haotian Liu and Chunyuan Li and Yuheng Li and Yong Jae Lee},
  title     = {Improved baselines with visual instruction tuning},
  journal   = {arXiv preprint arXiv:2310.03744},
  year      = {2023}
}
@article{liu2020ntu,
  author    = {Jun Liu and Amir Shahroudy and Mauricio Perez and Gang Wang and Ling-Yu Duan and Alex C Kot},
  title     = {Ntu rgb+d 120: A large-scale benchmark for 3d human activity understanding},
  journal   = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume    = {42},
  number    = {10},
  pages     = {2684--2701},
  year      = {2020}
}

@inproceedings{grounding_dino,
  author = {Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and et al.},
  title = {Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  booktitle = {European Conference on Computer Vision},
  pages = {38--55},
  year = {2025},
  publisher = {Springer}
}
@article{chen2024expanding,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}
@article{dong2024internlm,
  title={Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={arXiv preprint arXiv:2404.06512},
  year={2024}
}
@inproceedings{li2024llama,
  title={Llama-vid: An image is worth 2 tokens in large language models},
  author={Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
  booktitle={European Conference on Computer Vision},
  pages={323--340},
  year={2024}
}
@article{li2025eagle,
  title={Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models},
  author={Li, Zhiqi and Chen, Guo and Liu, Shilong and Wang, Shihao and VS, Vibashan and Ji, Yishen and Lan, Shiyi and Zhang, Hao and Zhao, Yilin and Radhakrishnan, Subhashree and others},
  journal={arXiv preprint arXiv:2501.14818},
  year={2025}
}
@article{liu2024cmmmath,
  author = {Wentao Liu and Qianjun Pan and Yi Zhang and Zhuo Liu and Ji Wu and Jie Zhou and Aimin Zhou and Qin Chen and Bo Jiang and Liang He},
  title = {Cmm-math: A chinese multimodal math dataset to evaluate and enhance the mathematics reasoning of large multimodal models},
  journal = {arXiv preprint arXiv:2409.02834},
  year = {2024}
}

@article{liu2024mminstruct,
  author = {Yangzhou Liu and Yue Cao and Zhangwei Gao and Weiyun Wang and Zhe Chen and Wenhai Wang and Hao Tian and Lewei Lu and Xizhou Zhu and Tong Lu and et al.},
  title = {Mminstruct: A high-quality multi-modal instruction tuning dataset with extensive diversity},
  journal = {arXiv preprint arXiv:2407.15838},
  year = {2024}
}

@article{liu2023mmbench,
  author = {Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and et al.},
  title = {Mmbench: Is your multi-modal model an all-around player?},
  journal = {arXiv preprint arXiv:2307.06281},
  year = {2023}
}

@article{liu2024points,
  author = {Yuan Liu and Zhongyin Zhao and Ziyuan Zhuang and Le Tian and Xiao Zhou and Jie Zhou},
  title = {Points: Improving your vision-language model with affordable strategies},
  journal = {arXiv preprint arXiv:2409.04828},
  year = {2024}
}

@article{liu2023ocrbench,
  author = {Yuliang Liu and Zhang Li and Hongliang Li and Wenwen Yu and Mingxin Huang and Dezhi Peng and Mingyu Liu and Mingrui Chen and Chunyuan Li and Lianwen Jin and et al.},
  title = {On the hidden mystery of ocr in large multimodal models},
  journal = {arXiv preprint arXiv:2305.07895},
  year = {2023}
}

@article{liu2024mmdu,
  author = {Ziyu Liu and Tao Chu and Yuhang Zang and Xilin Wei and Xiaoyi Dong and Pan Zhang and Zijian Liang and Yuanjun Xiong and Yu Qiao and Dahua Lin and et al.},
  title = {Mmdu: A multi-turn multi-image dialog understanding benchmark and instruction-tuning dataset for lvlms},
  journal = {arXiv preprint arXiv:2406.11833},
  year = {2024}
}

@article{liu2024oryx,
  author = {Zuyan Liu and Yuhao Dong and Ziwei Liu and Winston Hu and Jiwen Lu and Yongming Rao},
  title = {Oryx mllm: On-demand spatial-temporal understanding at arbitrary resolution},
  journal = {arXiv preprint arXiv:2409.12961},
  year = {2024}
}

@article{loshchilov2017adamw,
  author = {Ilya Loshchilov and Frank Hutter},
  title = {Decoupled weight decay regularization},
  journal = {arXiv preprint arXiv:1711.05101},
  year = {2017}
}

@article{lu2024deepseekvl,
  author = {Haoyu Lu and Wen Liu and Bo Zhang and Bingxuan Wang and Kai Dong and Bo Liu and Jingxiang Sun and Tongzheng Ren and Zhuoshu Li and Yaofeng Sun and et al.},
  title = {Deepseek-vl: Towards real-world vision-language understanding},
  journal = {arXiv preprint arXiv:2403.05525},
  year = {2024}
}

@article{lu2023mathvista,
  author = {Pan Lu and Hritik Bansal and Tony Xia and Jiacheng Liu and Chunyuan Li and Hannaneh Hajishirzi and Hao Cheng and Kai-Wei Chang and Michel Galley and Jianfeng Gao},
  title = {Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  journal = {arXiv preprint arXiv:2310.02255},
  year = {2023}
}

@article{lu2021geometry3k,
  author = {Pan Lu and Ran Gong and Shibiao Jiang and Liang Qiu and Siyuan Huang and Xiaodan Liang and Song-Chun Zhu},
  title = {Inter-gps: Interpretable geometry problem solving with formal language and symbolic reasoning},
  journal = {arXiv preprint arXiv:2105.04165},
  year = {2021}
}

@article{lu2022scienceqa,
  author = {Pan Lu and Swaroop Mishra and Tanglin Xia and Liang Qiu and Kai-Wei Chang and Song-Chun Zhu and Oyvind Tafjord and Peter Clark and Ashwin Kalyan},
  title = {Learn to explain: Multimodal reasoning via thought chains for science question answering},
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {2507--2521},
  year = {2022}
}

@article{lu2022tablemwp,
  author = {Pan Lu and Liang Qiu and Kai-Wei Chang and Ying Nian Wu and Song-Chun Zhu and Tanmay Rajpurohit and Peter Clark and Ashwin Kalyan},
  title = {Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
  journal = {arXiv preprint arXiv:2209.14610},
  year = {2022}
}

@article{lu2021iconqa,
  author = {Pan Lu and Liang Qiu and Jiaqi Chen and Tony Xia and Yizhou Zhao and Wei Zhang and Zhou Yu and Xiaodan Liang and Song-Chun Zhu},
  title = {Iconqa: A new benchmark for abstract diagram understanding and visual language reasoning},
  journal = {arXiv preprint arXiv:2110.13214},
  year = {2021}
}

@article{lu2024gui,
  author = {Quanfeng Lu and Wenqi Shao and Zitao Liu and Fanqing Meng and Boxuan Li and Botong Chen and Siyuan Huang and Kaipeng Zhang and Yu Qiao and Ping Luo},
  title = {Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices},
  journal = {arXiv preprint arXiv:2406.08451},
  year = {2024}
}
@misc{liu2024llavanext,
    title={LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
    url={https://llava-vl.github.io/blog/2024-01-30-llava-next/},
    author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
    year={2024}
}
@article{lu2024ovis,
  author = {Shiyin Lu and Yang Li and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and Han-Jia Ye},
  title = {Ovis: Structural embedding alignment for multimodal large language model},
  journal = {arXiv preprint arXiv:2405.20797},
  year = {2024}
}

@article{lu2024bluelm,
  author = {Xudong Lu and Yinghao Chen and Cheng Chen and Hui Tan and Boheng Chen and Yina Xie and Rui Hu and Guanxin Tan and Renshou Wu and Yan Hu and et al.},
  title = {Bluelm-v-3b: Algorithm and system co-design for multimodal large language models on mobile devices},
  journal = {arXiv preprint arXiv:2411.10640},
  year = {2024}
}

@article{lu2024wildvision,
  author = {Yujie Lu and Dongfu Jiang and Wenhu Chen and William Yang Wang and Yejin Choi and Bill Yuchen Lin},
  title = {Wildvision: Evaluating vision-language models in the wild with human preferences},
  journal = {arXiv preprint arXiv:2406.11069},
  year = {2024}
}

@article{luo2024mono_internvl,
  author = {Gen Luo and Xue Yang and Wenhan Dou and Zhaokai Wang and Jifeng Dai and Yu Qiao and Xizhou Zhu},
  title = {Mono-internvl: Pushing the boundaries of monolithic multimodal large language models with endogenous visual pre-training},
  journal = {arXiv preprint arXiv:2410.08202},
  year = {2024}
}

@article{luo2023wizardcoder,
  author = {Ziyang Luo and Can Xu and Pu Zhao and Qingfeng Sun and Xiubo Geng and Wenxiang Hu and Chongyang Tao and Jing Ma and Qingwei Lin and Daxin Jiang},
  title = {Wizardcoder: Empowering code large language models with evol-instruct},
  journal = {arXiv preprint arXiv:2306.08568},
  year = {2023}
}

@article{Maaz2024VideoGPT+,
  author = {Muhammad Maaz and Hanoona Rasheed and Salman Khan and Fahad Khan},
  title = {Videogpt+: Integrating image and video encoders for enhanced video understanding},
  journal = {arXiv preprint arXiv:2406.09418},
  year = {2024}
}

@inproceedings{mangalam2023egoschema,
  author = {Karttikeya Mangalam and Raiymbek Akshulakov and Jitendra Malik},
  title = {Egoschema: A diagnostic benchmark for very long-form video language understanding},
  booktitle = {Advances in Neural Information Processing Systems},
  pages = {46212--46244},
  year = {2023}
}

@inproceedings{mao2017deepart,
  author = {Hui Mao and Ming Cheung and James She},
  title = {Deepart: Learning joint representations of visual arts},
  booktitle = {Proceedings of the ACM International Conference on Multimedia},
  pages = {1183--1191},
  year = {2017}
}

@inproceedings{mao2016generation,
  author = {Junhua Mao and Jonathan Huang and Alexander Toshev and Oana Camburu and Alan L. Yuille and Kevin Murphy},
  title = {Generation and comprehension of unambiguous object descriptions},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {11--20},
  year = {2016}
}

@inproceedings{marino2019okvqa,
  author = {Kenneth Marino and Mohammad Rastegari and Ali Farhadi and Roozbeh Mottaghi},
  title = {Ok-vqa: A visual question answering benchmark requiring external knowledge},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {3195--3204},
  year = {2019}
}

@misc{MarkrAI_KOpen_HQ_Hermes_2.5_60K,
  author = {MarkrAI},
  title = {Kopen-hq-hermes-2.5-60k dataset},
  url = {https://huggingface.co/datasets/MarkrAI/KOpen-HQ-Hermes-2.5-60K},
  year = {2023}
}

@article{marti2002iam,
  author = {U-V Marti and Horst Bunke},
  title = {The iam-database: an english sentence database for offline handwriting recognition},
  journal = {International Journal on Document Analysis and Recognition},
  volume = {5},
  pages = {39--46},
  year = {2002}
}


@article{masry2023unichart,
  author = {Ahmed Masry and Parsa Kavehzadeh and Xuan Long Do and Enamul Hoque and Shafiq Joty},
  title = {Unichart: A universal vision-language pretrained model for chart comprehension and reasoning},
  journal = {arXiv preprint arXiv:2305.14761},
  year = {2023}
}





@article{mckinzie2024mm1,
  author = {Brandon McKinzie and Zhe Gan and Jean-Philippe Fauconnier and Sam Dodge and Bowen Zhang and Philipp Dufter and Dhruti Shah and Xianzhi Du and Futang Peng and Floris Weers and et al.},
  title = {Mm1: Methods, analysis \& insights from multimodal llm pre-training},
  journal = {arXiv preprint arXiv:2403.09611},
  year = {2024}
}

@article{meng2024mmiu,
  author = {Fanqing Meng and Jin Wang and Chuanhao Li and Quanfeng Lu and Hao Tian and Jiaqi Liao and Xizhou Zhu and Jifeng Dai and Yu Qiao and Ping Luo and et al.},
  title = {Mmiu: Multimodal multi-image understanding for evaluating large vision-language models},
  journal = {arXiv preprint arXiv:2408.02718},
  year = {2024}
}

@inproceedings{methani2020plotqa,
  author = {Nitesh Methani and Pritha Ganguly and Mitesh M Khapra and Pratyush Kumar},
  title = {Plotqa: Reasoning over scientific plots},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages = {1527--1536},
  year = {2020}
}

@inproceedings{mishra2019ocrvqa,
  author = {Anand Mishra and Shashank Shekhar and Ajeet Kumar Singh and Anirban Chakraborty},
  title = {Ocr-vqa: Visual question answering by reading text in images},
  booktitle = {International Conference on Document Analysis and Recognition},
  pages = {947--952},
  year = {2019}
}

@article{mitra2024orcamath,
  author = {Arindam Mitra and Hamed Khanpour and Corby Rosset and Ahmed Awadallah},
  title = {Orca-math: Unlocking the potential of slms in grade school math},
  journal = {arXiv preprint arXiv:2402.14830},
  year = {2024}
}

@article{nan2024openvid,
  author = {Kepan Nan and Rui Xie and Penghao Zhou and Tiehan Fan and Zhenheng Yang and Zhijie Chen and Xiang Li and Jian Yang and Ying Tai},
  title = {Openvid-1m: A large-scale high-quality dataset for text-to-video generation},
  journal = {arXiv preprint arXiv:2407.02371},
  year = {2024}
}

@article{obeid2020chart,
  author = {Jason Obeid and Enamul Hoque},
  title = {Chart-to-text: Generating natural language descriptions for charts by adapting the transformer model},
  journal = {arXiv preprint arXiv:2010.09142},
  year = {2020}
}

@misc{gpt4v,
  author = {OpenAI},
  title = {Gpt-4v(ision) system card},
  url = {https://cdn.openai.com/papers/GPTV\_System\_Card.pdf},
  year = {2023}
}

@inproceedings{patraucean2024perception,
  author = {Viorica Patraucean and Lucas Smaira and Ankush Gupta and Adria Recasens and Larisa Markeeva and Dylan Banarse and Skanda Koppula and Mateusz Malinowski and Yi Yang and Carl Doersch et al.},
  title = {Perception test: A diagnostic benchmark for multimodal video models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{peng2023kosmos2,
  author = {Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Furu Wei},
  title = {Kosmos-2: Grounding multimodal large language models to the world},
  journal = {arXiv preprint arXiv:2306.14824},
  year = {2023}
}

@inproceedings{radford2021clip,
  author = {Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark et al.},
  title = {Learning transferable visual models from natural language supervision},
  booktitle = {International Conference on Machine Learning},
  pages = {8748--8763},
  year = {2021}
}

@misc{no_robots,
  author = {Nazneen Rajani and Lewis Tunstall and Edward Beeching and Nathan Lambert and Alexander M. Rush and Thomas Wolf},
  title = {No robots},
  howpublished = {\url{https://huggingface.co/datasets/HuggingFaceH4/no_robots}},
  year = {2023}
}

@inproceedings{ranjan2021learning,
  author = {Viresh Ranjan and Udbhav Sharma and Thu Nguyen and Minh Hoai},
  title = {Learning to count everything},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {3394--3403},
  year = {2021}
}

@inproceedings{rawles2024androidinthewild,
  author = {Christopher Rawles and Alice Li and Daniel Rodriguez and Oriana Riva and Timothy Lillicrap},
  title = {Androidinthewild: A large-scale dataset for android device control},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@inproceedings{recht2019imagenetv2,
  author = {Benjamin Recht and Rebecca Roelofs and Ludwig Schmidt and Vaishaal Shankar},
  title = {Do imagenet classifiers generalize to imagenet?},
  booktitle = {International Conference on Machine Learning},
  pages = {5389--5400},
  year = {2019}
}

@article{reid2024gemini1_5,
  author = {Machel Reid and Nikolay Savinov and Denis Teplyashin and Dmitry Lepikhin and Timothy Lillicrap and Jean-baptiste Alayrac and Radu Soricut and Angeliki Lazaridou and Orhan Firat and Julian Schrittwieser et al.},
  title = {Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  journal = {arXiv preprint arXiv:2403.05530},
  year = {2024}
}

@inproceedings{rohrbach2015dataset,
  author = {Anna Rohrbach and Marcus Rohrbach and Niket Tandon and Bernt Schiele},
  title = {A dataset for movie description},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {3202--3212},
  year = {2015}
}

@inproceedings{sakaguchi2020winogrande,
  author = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
  title = {Winogrande: An adversarial winograd schema challenge at scale},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  pages = {8732--8740},
  year = {2020}
}

@inproceedings{schuhmann2022laion5b,
  author = {Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman et al.},
  title = {Laion-5b: An open large-scale dataset for training next generation image-text models},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {25278--25294},
  year = {2022}
}

@misc{schuhmann2022laioncoco,
  author = {Christoph Schuhmann and Andreas Köpf and Richard Vencu and Theo Coombes and Romain Beaumont},
  title = {Laion coco: 600m synthetic captions from laion2b-en},
  howpublished = {\url{https://laion.ai/blog/laion-coco/}},
  year = {2022}
}

@inproceedings{schwenk2022aokvqa,
  author = {Dustin Schwenk and Apoorv Khandelwal and Christopher Clark and Kenneth Marino and Roozbeh Mottaghi},
  title = {A-okvqa: A benchmark for visual question answering using world knowledge},
  booktitle = {European Conference on Computer Vision},
  pages = {146--162},
  year = {2022}
}

@inproceedings{seo2015solving,
  author = {Minjoon Seo and Hannaneh Hajishirzi and Ali Farhadi and Oren Etzioni and Clint Malcolm},
  title = {Solving geometry problems: Combining text and diagram interpretation},
  booktitle = {Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing},
  pages = {1466--1476},
  year = {2015}
}

@inproceedings{shah2019kvqa,
  author = {Sanket Shah and Anand Mishra and Naganand Yadati and Partha Pratim Talukdar},
  title = {Kvqa: Knowledge-aware visual question answering},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {33},
  pages = {8876--8884},
  year = {2019}
}

@inproceedings{shao2019objects365,
  author = {Shuai Shao and Zeming Li and Tianyuan Zhang and Chao Peng and Gang Yu and Xiangyu Zhang and Jing Li and Jian Sun},
  title = {Objects365: A large-scale, high-quality dataset for object detection},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages = {8430--8439},
  year = {2019}
}

@inproceedings{shi2017rctw17,
  author = {Baoguang Shi and Cong Yao and Minghui Liao and Mingkun Yang and Pei Xu and Linyan Cui and Serge Belongie and Shijian Lu and Xiang Bai},
  title = {Icdar2017 competition on reading chinese text in the wild (rctw-17)},
  booktitle = {International Conference on Document Analysis and Recognition},
  volume = {1},
  pages = {1429--1434},
  year = {2017}
}

@article{shi2024eagle,
  author = {Min Shi and Fuxiao Liu and Shihao Wang and Shijia Liao and Subhashree Radhakrishnan and De-An Huang and Hongxu Yin and Karan Sapra and Yaser Yacoob and Humphrey Shi and others},
  title = {Eagle: Exploring the design space for multimodal llms with mixture of encoders},
  journal = {arXiv preprint arXiv:2408.15998},
  year = {2024}
}

@inproceedings{sidorov2020textcaps,
  author = {Oleksii Sidorov and Ronghang Hu and Marcus Rohrbach and Amanpreet Singh},
  title = {Textcaps: a dataset for image captioning with reading comprehension},
  booktitle = {European Conference on Computer Vision},
  pages = {742--758},
  year = {2020}
}

@inproceedings{singh2019textvqa,
  author = {Amanpreet Singh and Vivek Natarajan and Meet Shah and Yu Jiang and Xinlei Chen and Dhruv Batra and Devi Parikh and Marcus Rohrbach},
  title = {Towards vqa models that can read},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {8317--8326},
  year = {2019}
}

@inproceedings{singh2021textocr,
  author = {Amanpreet Singh and Guan Pang and Mandy Toh and Jing Huang and Wojciech Galuba and Tal Hassner},
  title = {Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {8802--8812},
  year = {2021}
}

@inproceedings{singh2025benchmarking,
  author = {Shweta Singh and Aayan Yadav and Jitesh Jain and Humphrey Shi and Justin Johnson and Karan Desai},
  title = {Benchmarking object detectors with coco: A new path forward},
  booktitle = {European Conference on Computer Vision},
  pages = {279--295},
  year = {2025},
  publisher = {Springer}
}

@article{snell2024scaling,
  author = {Charlie Snell and Jaehoon Lee and Kelvin Xu and Aviral Kumar},
  title = {Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  journal = {arXiv preprint arXiv:2408.03314},
  year = {2024}
}

@article{suhr2018corpus,
  author = {Alane Suhr and Stephanie Zhou and Ally Zhang and Iris Zhang and Huajun Bai and Yoav Artzi},
  title = {A corpus for reasoning about natural language grounded in photographs},
  journal = {arXiv preprint arXiv:1811.00491},
  year = {2018}
}

@misc{sujet-finance,
  author = {Hamed Rahimi Sujet AI and Allaa Boutaleb},
  title = {Sujet-finance-qa-vision-100k: A large-scale dataset for financial document vqa},
  howpublished = {\url{https://huggingface.co/datasets/sujet-ai/Sujet-Finance-QA-Vision-100k}},
  year = {2024}
}

@article{sun2024parrot,
  author = {Hai-Long Sun and Da-Wei Zhou and Yang Li and Shiyin Lu and Chao Yi and Qing-Guo Chen and Zhao Xu and Weihua Luo and Kaifu Zhang and De-Chuan Zhan and others},
  title = {Parrot: Multilingual visual instruction tuning},
  journal = {arXiv preprint arXiv:2406.02539},
  year = {2024}
}

@article{sun2019investigating,
  author = {Kai Sun and Dian Yu and Dong Yu and Claire Cardie},
  title = {Investigating prior knowledge for challenging chinese machine reading comprehension},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {141--155},
  year = {2020}
}

@inproceedings{sun2023emu,
  author = {Quan Sun and Qiying Yu and Yufeng Cui and Fan Zhang and Xiaosong Zhang and Yueze Wang and Hongcheng Gao and Jingjing Liu and Tiejun Huang and Xinlong Wang},
  title = {Generative pretraining in multimodality},
  booktitle = {The International Conference on Learning Representations},
  year = {2024}
}

@article{sun2023moss,
  author = {Tianxiang Sun and Xiaotian Zhang and Zhengfu He and Peng Li and Qinyuan Cheng and Hang Yan and Xiangyang Liu and Yunfan Shao and Qiong Tang and Xingjian Zhao and others},
  title = {Moss: Training conversational language models from synthetic data},
  journal = {arXiv preprint arXiv:2307.15020},
  volume = {7},
  year = {2023}
}

@inproceedings{sun2019lsvt,
  author = {Yipeng Sun and Zihan Ni and Chee-Kheng Chng and Yuliang Liu and Canjie Luo and Chun Chet Ng and Junyu Han and Errui Ding and Jingtuo Liu and Dimosthenis Karatzas and others},
  title = {Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt},
  booktitle = {International Conference on Document Analysis and Recognition},
  volume = {1},
  pages = {1557--1562},
  year = {2019}
}

@article{sun2023aligning,
  author = {Zhiqing Sun and Sheng Shen and Shengcao Cao and Haotian Liu and Chunyuan Li and Yikang Shen and Chuang Gan and Liang-Yan Gui and Yu-Xiong Wang and Yiming Yang and others},
  title = {Aligning large multimodal models with factually augmented rlhf},
  journal = {arXiv preprint arXiv:2309.14525},
  year = {2023}
}

@article{suzgun2023bigbench,
  author = {Mirac Suzgun and Nathan Scales and Nathanael Sch{\"a}rli and Sebastian Gehrmann and Yi Tay and Hyung Won Chung and Aakanksha Chowdhery and Quoc V Le and Ed H Chi and Denny Zhou and others},
  title = {Challenging big-bench tasks and whether chain-of-thought can solve them},
  journal = {arXiv preprint arXiv:2210.09261},
  year = {2022}
}

@inproceedings{tanaka2021visualmrc,
  author = {Ryota Tanaka and Kyosuke Nishida and Sen Yoshida},
  title = {Visualmrc: Machine reading comprehension on document images},
  booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {35},
  pages = {13878--13888},
  year = {2021}
}

@article{tang2023vistext,
  author = {Benny J Tang and Angie Boggust and Arvind Satyanarayan},
  title = {Vistext: A benchmark for semantically rich chart captioning},
  journal = {arXiv preprint arXiv:2307.05356},
  year = {2023}
}

@article{tang2024mtvqa,
  author = {Jingqun Tang and Qi Liu and Yongjie Ye and Jinghui Lu and Shu Wei and Chunhui Lin and Wanqing Li and Mohamad Fitri Faiz Bin Mahmood and Hao Feng and Zhen Zhao and others},
  title = {Mtvqa: Benchmarking multilingual text-centric visual question answering},
  journal = {arXiv preprint arXiv:2405.11985},
  year = {2024}
}

@article{team2023gemini,
  author = {Gemini Team and Rohan Anil and Sebastian Borgeaud and Yonghui Wu and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M Dai and Anja Hauth and others},
  title = {Gemini: a family of highly capable multimodal models},
  journal = {arXiv preprint arXiv:2312.11805},
  year = {2023}
}

@misc{qwen2.5,
  author = {Qwen Team},
  title = {Qwen2.5: A party of foundation models},
  howpublished = {\url{https://qwenlm.github.io/blog/qwen2.5/}},
  month = {September},
  year = {2024}
}

@misc{qwq-32b-preview,
  author = {Qwen Team},
  title = {Qwq: Reflect deeply on the boundaries of the unknown},
  year = {2024},
  url = {https://qwenlm.github.io/blog/qwq-32b-preview/}
}

@misc{teknium2024hermes3,
  author = {Ryan Teknium, Jeffrey Quesnelle, and Chen Guang},
  title = {Hermes 3 technical report},
  year = {2024},
  journal = {arXiv preprint arXiv:2408.11857}
}

@misc{tian2024mminterleaved,
  author = {Changyao Tian, Xizhou Zhu, Yuwen Xiong, Weiyun Wang, Zhe Chen, Wenhai Wang, Yuntao Chen, Lewei Lu, Tong Lu, Jie Zhou, et~al.},
  title = {Mm-interleaved: Interleaved image-text generative modeling via multi-modal feature synchronizer},
  year = {2024},
  journal = {arXiv preprint arXiv:2401.10208}
}

@article{tito2023hier,
  author = {Rub{\`e}n Tito, Dimosthenis Karatzas, and Ernest Valveny},
  title = {Hierarchical multimodal transformers for multipage docvqa},
  journal = {Pattern Recognition},
  volume = {144},
  pages = {109834},
  year = {2023}
}

@misc{tong2024cambrian,
  author = {Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai~Charitha Akula, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, et~al.},
  title = {Cambrian-1: A fully open, vision-centric exploration of multimodal llms},
  year = {2024},
  journal = {arXiv preprint arXiv:2406.16860}
}

@misc{touvron2023llama2,
  author = {Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et~al.},
  title = {Llama 2: Open foundation and fine-tuned chat models},
  year = {2023},
  journal = {arXiv preprint arXiv:2307.09288}
}

@misc{ustalov2023toloka,
  author = {Dmitry Ustalov, Nikita Pavlichenko, Sergey Koshelev, Daniil Likhobaba, and Alisa Smirnova},
  title = {Toloka visual question answering benchmark},
  year = {2023},
  journal = {arXiv preprint arXiv:2309.16511}
}

@inproceedings{van2018inaturalist,
  author = {Grant Van~Horn, Oisin Mac~Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie},
  title = {The inaturalist species classification and detection dataset},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {8769--8778},
  year = {2018}
}

@misc{veit2016coco,
  author = {Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie},
  title = {Coco-text: Dataset and benchmark for text detection and recognition in natural images},
  year = {2016},
  journal = {arXiv preprint arXiv:1601.07140}
}

@misc{cyrillic,
  author = {Konstantin Verner},
  title = {Cyrillic handwriting dataset},
  year = {2020},
  url = {https://www.kaggle.com/datasets/constantinwerner/cyrillic-handwriting-dataset}
}

@inproceedings{wang2021screen2words,
  author = {Bryan Wang, Gang Li, Xin Zhou, Zhourong Chen, Tovi Grossman, and Yang Li},
  title = {Screen2words: Automatic mobile ui summarization with multimodal learning},
  booktitle = {The 34th Annual ACM Symposium on User Interface Software and Technology},
  pages = {498--510},
  year = {2021}
}

@misc{wang2024muirbench,
  author = {Fei Wang, Xingyu Fu, James~Y Huang, Zekun Li, Qin Liu, Xiaogeng Liu, Mingyu~Derek Ma, Nan Xu, Wenxuan Zhou, Kai Zhang, et~al.},
  title = {Muirbench: A comprehensive benchmark for robust multi-image understanding},
  year = {2024},
  journal = {arXiv preprint arXiv:2406.09411}
}

@inproceedings{wang2019imagenet_sketch,
  author = {Haohan Wang, Songwei Ge, Zachary Lipton, and Eric~P Xing},
  title = {Learning robust global representations by penalizing local predictive power},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {32},
  year = {2019}
}

@inproceedings{wang2023v3det,
  author = {Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin},
  title = {V3det: Vast vocabulary visual detection dataset},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages = {19844--19854},
  year = {2023}
}

@misc{wang2023lvisinstruct4v,
  author = {Junke Wang, Lingchen Meng, Zejia Weng, Bo~He, Zuxuan Wu, and Yu-Gang Jiang},
  title = {To see is to believe: Prompting gpt-4v for better visual instruction tuning},
  year = {2023},
  journal = {arXiv preprint arXiv:2311.07574}
}

@article{wang2024measuring,
  author = {Ke Wang and Junting Pan and Weikang Shi and Zimu Lu and Mingjie Zhan and Hongsheng Li},
  title = {Measuring multimodal mathematical reasoning with math-vision dataset},
  journal = {arXiv preprint arXiv:2402.14804},
  year = {2024}
}

@article{wang2024qwen2vl,
  author = {Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and others},
  title = {Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  journal = {arXiv preprint arXiv:2409.12191},
  year = {2024}
}

@article{one-peace,
  author = {Peng Wang and Shijie Wang and Junyang Lin and Shuai Bai and Xiaohuan Zhou and Jingren Zhou and Xinggang Wang and Chang Zhou},
  title = {One-peace: Exploring one general representation model toward unlimited modalities},
  journal = {arXiv:2305.11172},
  year = {2023}
}

@article{wang2023cogvlm,
  author = {Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and others},
  title = {Cogvlm: Visual expert for pretrained language models},
  journal = {arXiv preprint arXiv:2311.03079},
  year = {2023}
}

@article{wang2024mpo,
  author = {Weiyun Wang and Zhe Chen and Wenhai Wang and Yue Cao and Yangzhou Liu and Zhangwei Gao and Jinguo Zhu and Xizhou Zhu and Lewei Lu and Yu Qiao and Jifeng Dai},
  title = {Enhancing the reasoning ability of multimodal large language models via mixed preference optimization},
  journal = {arXiv preprint arXiv:2411.10442},
  year = {2024}
}

@article{wang2024allseeingv2,
  author = {Weiyun Wang and Yiming Ren and Haowen Luo and Tiantong Li and Chenxiang Yan and Zhe Chen and Wenhai Wang and Qingyun Li and Lewei Lu and Xizhou Zhu and others},
  title = {The all-seeing project v2: Towards general relation comprehension of the open world},
  journal = {arXiv preprint arXiv:2402.19474},
  year = {2024}
}

@inproceedings{wang2023allseeing,
  author = {Weiyun Wang and Min Shi and Qingyun Li and Wenhai Wang and Zhenhang Huang and Linjie Xing and Zhe Chen and Hao Li and Xizhou Zhu and Zhiguo Cao and others},
  title = {The all-seeing project: Towards panoptic visual recognition and understanding of the open world},
  booktitle = {The International Conference on Learning Representations},
  year = {2024}
}

@inproceedings{wang2023internimage,
  author = {Wenhai Wang and Jifeng Dai and Zhe Chen and Zhenhang Huang and Zhiqi Li and Xizhou Zhu and Xiaowei Hu and Tong Lu and Lewei Lu and Hongsheng Li and others},
  title = {Internimage: Exploring large-scale vision foundation models with deformable convolutions},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {14408--14419},
  year = {2023}
}

@inproceedings{wang2020general,
  author = {Xinyu Wang and Yuliang Liu and Chunhua Shen and Chun Chet Ng and Canjie Luo and Lianwen Jin and Chee Seng Chan and Anton van den Hengel and Liangwei Wang},
  title = {On the general value of evidence, and bilingual scene-text visual question answering},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {10126--10135},
  year = {2020}
}

@article{wang2024mementos,
  author = {Xiyao Wang and Yuhang Zhou and Xiaoyu Liu and Hongjin Lu and Yuancheng Xu and Feihong He and Jaehong Yoon and Taixi Lu and Gedas Bertasius and Mohit Bansal and others},
  title = {Mementos: A comprehensive benchmark for multimodal large language model reasoning over image sequences},
  journal = {arXiv preprint arXiv:2401.10529},
  year = {2024}
}

@article{wang2024xcoder80k,
  author = {Yejie Wang and Keqing He and Dayuan Fu and Zhuoma Gongque and Heyang Xu and Yanxu Chen and Zhexu Wang and Yujia Fu and Guanting Dong and Muxi Diao and others},
  title = {How do your code llms perform? empowering code instruction tuning with high-quality data},
  journal = {arXiv preprint arXiv:2409.03810},
  year = {2024}
}

@article{wang2024internvideo2,
  author = {Yi Wang and Kunchang Li and Xinhao Li and Jiashuo Yu and Yinan He and Guo Chen and Baoqi Pei and Rongkun Zheng and Jilan Xu and Zun Wang and others},
  title = {Internvideo2: Scaling video foundation models for multimodal video understanding},
  journal = {arXiv preprint arXiv:2403.15377},
  year = {2024}
}

@article{wang2024charxiv,
  author = {Zirui Wang and Mengzhou Xia and Luxi He and Howard Chen and Yitao Liu and Richard Zhu and Kaiqu Liang and Xindi Wu and Haotian Liu and Sadhika Malladi and others},
  title = {Charxiv: Charting gaps in realistic chart understanding in multimodal llms},
  journal = {arXiv preprint arXiv:2406.18521},
  year = {2024}
}

@article{wei2021finetuned,
  author = {Jason Wei and Maarten Bosma and Vincent Y Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M Dai and Quoc V Le},
  title = {Finetuned language models are zero-shot learners},
  journal = {arXiv preprint arXiv:2109.01652},
  year = {2021}
}

@article{wu2024star,
  author = {Bo Wu and Shoubin Yu and Zhenfang Chen and Joshua B Tenenbaum and Chuang Gan},
  title = {Star: A benchmark for situated reasoning in real-world videos},
  journal = {arXiv preprint arXiv:2405.09711},
  year = {2024}
}

@misc{pmccase,
  author = {Chaoyi Wu},
  title = {Pmc-casereport},
  howpublished = {\url{https://huggingface.co/datasets/chaoyi-wu/PMC-CaseReport}},
  year = {2023}
}

@article{wu2023towards,
  author = {Chaoyi Wu and Xiaoman Zhang and Ya Zhang and Yanfeng Wang and Weidi Xie},
  title = {Towards generalist foundation model for radiology},
  journal = {arXiv preprint arXiv:2308.02463},
  year = {2023}
}

@article{wu2024longvideobench,
  author = {Haoning Wu and Dongxu Li and Bei Chen and Junnan Li},
  title = {Longvideobench: A benchmark for long-context interleaved video-language understanding},
  journal = {arXiv preprint arXiv:2407.15754},
  year = {2024}
}

@article{xia2023structchart,
  author = {Renqiu Xia and Bo Zhang and Haoyang Peng and Hancheng Ye and Xiangchao Yan and Peng Ye and Botian Shi and Junchi Yan and Yu Qiao},
  title = {Structchart: Perception, structuring, reasoning for visual chart understanding},
  journal = {arXiv preprint arXiv:2309.11268},
  year = {2023}
}

@inproceedings{xiao2018upernet,
  author = {Tete Xiao and Yingcheng Liu and Bolei Zhou and Yuning Jiang and Jian Sun},
  title = {Unified perceptual parsing for scene understanding},
  booktitle = {European Conference on Computer Vision},
  pages = {418--434},
  year = {2018}
}

@inproceedings{xu2024wizardlm,
  author = {Can Xu and Qingfeng Sun and Kai Zheng and Xiubo Geng and Pu Zhao and Jiazhan Feng and Chongyang Tao and Qingwei Lin and Daxin Jiang},
  title = {Wizardlm: Empowering large pre-trained language models to follow complex instructions},
  booktitle = {The International Conference on Learning Representations},
  year = {2024}
}

@article{xu2024magpie,
  author = {Zhangchen Xu and Fengqing Jiang and Luyao Niu and Yuntian Deng and Radha Poovendran and Yejin Choi and Bill Yuchen Lin},
  title = {Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing},
  journal = {arXiv preprint arXiv:2406.08464},
  year = {2024}
}

@inproceedings{uninext,
  author = {B. Yan and Yi Jiang and Jiannan Wu and D. Wang and Ping Luo and Zehuan Yuan and Huchuan Lu},
  title = {Universal instance perception as object discovery and retrieval},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year = {2023}
}

@article{yang2024qwen2,
  author = {An Yang and Baosong Yang and Binyuan Hui and Bo Zheng and Bowen Yu and Chang Zhou and Chengpeng Li and Chengyuan Li and Dayiheng Liu and Fei Huang and others},
  title = {Qwen2 technical report},
  journal = {arXiv preprint arXiv:2407.10671},
  year = {2024}
}

@article{yang2024vript,
  author = {Dongjie Yang and Suyuan Huang and Chengqiang Lu and Xiaodong Han and Haoxin Zhang and Yan Gao and Yao Hu and Hai Zhao},
  title = {Vript: A video is worth thousands of words},
  journal = {arXiv preprint arXiv:2406.06040},
  year = {2024}
}

@misc{Firefly,
  author = {Jianxin Yang},
  title = {Firefly: A chinese conversational large language model},
  howpublished = {\url{https://github.com/yangjianxin1/Firefly}},
  year = {2023}
}

@article{yang2023longqlora,
  author = {Jianxin Yang},
  title = {Longqlora: Efficient and effective method to extend context length of large language models},
  journal = {arXiv preprint arXiv:2311.04879},
  year = {2023}
}

@article{yang2023open,
  author = {Wenjuan Yang and Xuhui Zhang and Bing Ma and Yanqun Wang and Yujia Wu and Jianxing Yan and Yongwei Liu and Chao Zhang and Jicheng Wan and Yue Wang and others},
  title = {An open dataset for intelligent recognition and classification of abnormal condition in longwall mining},
  journal = {Scientific Data},
  volume = {10},
  number = {1},
  pages = {416},
  year = {2023}
}

@inproceedings{yao2011human,
  author = {Bangpeng Yao and Xiaoye Jiang and Aditya Khosla and Andy Lai Lin and Leonidas Guibas and Li Fei-Fei},
  title = {Human action recognition by learning bases of action attributes and parts},
  booktitle = {2011 International Conference on Computer Vision},
  pages = {1331--1338},
  year = {2011}
}

@article{yao2024minicpm,
  author = {Yuan Yao and Tianyu Yu and Ao Zhang and Chongyi Wang and Junbo Cui and Hongji Zhu and Tianchi Cai and Haoyu Li and Weilin Zhao and Zhihui He and others},
  title = {Minicpm-v: A gpt-4v level mllm on your phone},
  journal = {arXiv preprint arXiv:2408.01800},
  year = {2024}
}

@article{ye2023mplug2,
  author = {Qinghao Ye and Haiyang Xu and Jiabo Ye and Ming Yan and Haowei Liu and Qi Qian and Ji Zhang and Fei Huang and Jingren Zhou},
  title = {mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  journal = {arXiv preprint arXiv:2311.04257},
  year = {2023}
}

@article{yi2019clevrer,
  author = {Kexin Yi and Chuang Gan and Yunzhu Li and Pushmeet Kohli and Jiajun Wu and Antonio Torralba and Joshua B Tenenbaum},
  title = {Clevrer: Collision events for video representation and reasoning},
  journal = {arXiv preprint arXiv:1910.01442},
  year = {2019}
}

@article{mmtbench,
  author = {Kaining Ying and Fanqing Meng and Jin Wang and Zhiqian Li and Han Lin and Yue Yang and Hao Zhang and Wenbo Zhang and Yuqi Lin and Shuo Liu and Jiayi Lei and Quanfeng Lu and Runjian Chen and Peng Xu and Renrui Zhang and Haozhe Zhang and Peng Gao and Yali Wang and Yu Qiao and Ping Luo and Kaipeng Zhang and Wenqi Shao},
  title = {Mmt-bench: A comprehensive multimodal benchmark for evaluating large vision-language models towards multitask agi},
  journal = {arXiv preprint arXiv:2404.16006},
  year = {2024}
}

@article{you2023ferret,
  author = {Haoxuan You and Haotian Zhang and Zhe Gan and Xianzhi Du and Bowen Zhang and Zirui Wang and Liangliang Cao and Shih-Fu Chang and Yinfei Yang},
  title = {Ferret: Refer and ground anything anywhere at any granularity},
  journal = {arXiv preprint arXiv:2310.07704},
  year = {2023}
}

@article{young2024yi,
  author = {Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and others},
  title = {Yi: Open foundation models by 01. ai},
  journal = {arXiv preprint arXiv:2403.04652},
  year = {2024}
}

@inproceedings{yu2016refcoco,
  author = {Licheng Yu and Patrick Poirson and Shan Yang and Alexander C Berg and Tamara L Berg},
  title = {Modeling context in referring expressions},
  booktitle = {European Conference on Computer Vision},
  pages = {69--85},
  year = {2016}
}

@article{yu2023metamath,
  author = {Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James T Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu},
  title = {Metamath: Bootstrap your own mathematical questions for large language models},
  journal = {arXiv preprint arXiv:2309.12284},
  year = {2023}
}

@article{yu2024rlaifv,
  author = {Tianyu Yu, Haoye Zhang, Yuan Yao, Yunkai Dang, Da Chen, Xiaoman Lu, Ganqu Cui, Taiwen He, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun},
  title = {Rlaif-v: Aligning mllms through open-source ai feedback for super gpt-4v trustworthiness},
  journal = {arXiv preprint arXiv:2405.17220},
  year = {2024}
}

@article{yu2023mmvet,
  author = {Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang},
  title = {Mm-vet: Evaluating large multimodal models for integrated capabilities},
  journal = {arXiv preprint arXiv:2308.02490},
  year = {2023}
}

@article{yu2024mmvetv2,
  author = {Weihao Yu, Zhengyuan Yang, Linfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang},
  title = {Mm-vet v2: A challenging benchmark to evaluate large multimodal models for integrated capabilities},
  journal = {arXiv preprint arXiv:2408.00765},
  year = {2024}
}

@article{yu2023paraphrasing,
  author = {Yijiong Yu},
  title = {"paraphrasing the original text" makes high accuracy long-context qa},
  journal = {arXiv preprint arXiv:2312.11193},
  year = {2023}
}

@article{yuan2019ctw,
  author = {Tai-Ling Yuan, Zhe Zhu, Kun Xu, Cheng-Jun Li, Tai-Jiang Mu, and Shi-Min Hu},
  title = {A large chinese text dataset in the wild},
  journal = {Journal of Computer Science and Technology},
  volume = {34},
  pages = {509--521},
  year = {2019}
}

@inproceedings{yuan2022syntax,
  author = {Ye Yuan, Xiao Liu, Wondimu Dikubab, Hui Liu, Zhilong Ji, Zhongqin Wu, and Xiang Bai},
  title = {Syntax-aware network for handwritten mathematical expression recognition},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {4553--4562},
  year = {2022}
}

@article{yue2023mmmu,
  author = {Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al.},
  title = {Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  journal = {arXiv preprint arXiv:2311.16502},
  year = {2023}
}

@article{yue2024mmmu,
  author = {Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al.},
  title = {Mmmu-pro: A more robust multi-discipline multimodal understanding benchmark},
  journal = {arXiv preprint arXiv:2409.02813},
  year = {2024}
}

@inproceedings{zala2023hierarchical,
  author = {Abhay Zala, Jaemin Cho, Satwik Kottur, Xilun Chen, Barlas Oguz, Yashar Mehdad, and Mohit Bansal},
  title = {Hierarchical video-moment retrieval and step-captioning},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {23056--23065},
  year = {2023}
}

@inproceedings{zellers2019hellaswag,
  author = {Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi},
  title = {Hellaswag: Can a machine really finish your sentence?},
  booktitle = {Proceedings of the Annual Meeting of the Association for Computational Linguistics},
  pages = {4791--4800},
  year = {2019}
}

@inproceedings{zhai2023siglip,
  author = {Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer},
  title = {Sigmoid loss for language image pre-training},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages = {11975--11986},
  year = {2023}
}

@inproceedings{zhang2019rmsnorm,
  author = {Biao Zhang and Rico Sennrich},
  title = {Root mean square layer normalization},
  booktitle = {Advances in Neural Information Processing Systems},
  volume = {32},
  year = {2019}
}

@inproceedings{zhang2024inifinitymath,
  author = {Bo-Wen Zhang, Yan Yan, Lin Li, and Guang Liu},
  title = {Infinitymath: A scalable instruction tuning dataset in programmatic mathematical reasoning},
  booktitle = {Proceedings of the 33rd ACM International Conference on Information and Knowledge Management},
  pages = {5405--5409},
  year = {2024}
}

@article{zhang2024mm1_5,
  author = {Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, et al.},
  title = {Mm1.5: Methods, analysis \& insights from multimodal llm fine-tuning},
  journal = {arXiv preprint arXiv:2409.20566},
  year = {2024}
}

@article{ferretv2,
  author = {Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al.},
  title = {Ferret-v2: An improved baseline for referring and grounding with large language models},
  journal = {arXiv preprint arXiv:2404.07973},
  year = {2024}
}

@article{zhang2024longcite,
  author = {Jiajie Zhang, Yushi Bai, Xin Lv, Wanjun Gu, Danqing Liu, Minhao Zou, Shulin Cao, Lei Hou, Yuxiao Dong, Ling Feng, and Juanzi Li},
  title = {Longcite: Enabling llms to generate fine-grained citations in long-context qa},
  journal = {arXiv preprint arXiv:2409.02897},
  year = {2024}
}

@inproceedings{zhang2025mathverse,
  author = {Renrui Zhang, Dongzhi Jiang, Yichi Zhang, Haokun Lin, Ziyu Guo, Pengshuo Qiu, Aojun Zhou, Pan Lu, Kai-Wei Chang, Yu Qiao, et al.},
  title = {Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems?},
  booktitle = {European Conference on Computer Vision},
  pages = {169--186},
  publisher = {Springer},
  year = {2025}
}

@article{zhang2024mavis,
  author = {Renrui Zhang, Xinyu Wei, Dongzhi Jiang, Yichi Zhang, Ziyu Guo, Chengzhuo Tong, Jiaming Liu, Aojun Zhou, Bin Wei, Shanghang Zhang, et al.},
  title = {Mavis: Mathematical visual instruction tuning},
  journal = {arXiv preprint arXiv:2407.08739},
  year = {2024}
}

@article{zhang2023pmc,
  author = {Xiaoman Zhang and Chaoyi Wu and Ziheng Zhao and Weixiong Lin and Ya Zhang and Yanfeng Wang and Weidi Xie},
  title = {Pmc-vqa: Visual instruction tuning for medical visual question answering},
  journal = {arXiv preprint arXiv:2305.10415},
  year = {2023}
}

@article{Zhang2023gaokao,
  author = {Xiaotian Zhang and Chunyang Li and Yi Zong and Zhengyu Ying and Liang He and Xipeng Qiu},
  title = {Evaluating the performance of large language models on gaokao benchmark},
  journal = {arXiv preprint arXiv:2305.12474},
  year = {2023}
}

@article{zhang2023llavar,
  author = {Yanzhe Zhang and Ruiyi Zhang and Jiuxiang Gu and Yufan Zhou and Nedim Lipka and Diyi Yang and Tong Sun},
  title = {Llavar: Enhanced visual instruction tuning for text-rich image understanding},
  journal = {arXiv preprint arXiv:2306.17107},
  year = {2023}
}

@article{zhang2024mme,
  author = {Yi-Fan Zhang and Huanyu Zhang and Haochen Tian and Chaoyou Fu and Shuangqing Zhang and Junfei Wu and Feng Li and Kun Wang and Qingsong Wen and Zhang Zhang and others},
  title = {Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans?},
  journal = {arXiv preprint arXiv:2408.13257},
  year = {2024}
}

@article{zhang2024video,
  author = {Yuanhan Zhang and Jinming Wu and Wei Li and Bo Li and Zejun Ma and Ziwei Liu and Chunyuan Li},
  title = {Video instruction tuning with synthetic data},
  journal = {arXiv preprint arXiv:2410.02713},
  year = {2024}
}

@article{zhao2024mirb,
  author = {Bingchen Zhao and Yongshuo Zong and Letian Zhang and Timothy Hospedales},
  title = {Benchmarking multi-image understanding in vision and language models: Perception, knowledge, reasoning, and multi-hop reasoning},
  journal = {arXiv preprint arXiv:2406.12742},
  year = {2024}
}

@article{zhao2023svit,
  author = {Bo Zhao and Boya Wu and Tiejun Huang},
  title = {Svit: Scaling up visual instruction tuning},
  journal = {arXiv preprint arXiv:2307.04087},
  year = {2023}
}

@article{zheng2024multimodal,
  author = {Mingyu Zheng and Xinwei Feng and Qingyi Si and Qiaoqiao She and Zheng Lin and Wenbin Jiang and Weiping Wang},
  title = {Multimodal table understanding},
  journal = {arXiv preprint arXiv:2406.08100},
  year = {2024}
}

@article{opencodeinterpreter,
  author = {Tianyu Zheng and Ge Zhang and Tianhao Shen and Xueling Liu and Bill Yuchen Lin and Jie Fu and Wenhu Chen and Xiang Yue},
  title = {Opencodeinterpreter: Integrating code generation with execution and refinement},
  journal = {arXiv preprint arXiv:2402.14658},
  year = {2024}
}

@inproceedings{zheng2021global,
  author = {Xinyi Zheng and Douglas Burdick and Lucian Popa and Xu Zhong and Nancy Xin Ru Wang},
  title = {Global table extractor (gte): A framework for joint table identification and cell structure recognition using visual context},
  booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages = {697--706},
  year = {2021}
}

@inproceedings{zhou2017ade20k,
  author = {Bolei Zhou and Hang Zhao and Xavier Puig and Sanja Fidler and Adela Barriuso and Antonio Torralba},
  title = {Scene parsing through ade20k dataset},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {633--641},
  year = {2017}
}

@article{zhou2024lima,
  author = {Chunting Zhou and Pengfei Liu and Puxin Xu and Srinivasan Iyer and Jiao Sun and Yuning Mao and Xuezhe Ma and Avia Efrat and Ping Yu and Lili Yu and others},
  title = {Lima: Less is more for alignment},
  journal = {Advances in Neural Information Processing Systems},
  volume = {36},
  year = {2024}
}

@article{MLVU,
  author = {Junjie Zhou and Yan Shu and Bo Zhao and Boya Wu and Shitao Xiao and Xi Yang and Yongping Xiong and Bo Zhang and Tiejun Huang and Zheng Liu},
  title = {Mlvu: A comprehensive benchmark for multi-task long video understanding},
  journal = {arXiv preprint arXiv:2406.04264},
  year = {2024}
}

@inproceedings{zhu2023minigpt4,
  author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
  title = {Minigpt-4: Enhancing vision-language understanding with advanced large language models},
  booktitle = {The International Conference on Learning Representations},
  year = {2024}
}

@inproceedings{zhu2016visual7w,
  author = {Yuke Zhu and Oliver Groth and Michael Bernstein and Li Fei-Fei},
  title = {Visual7w: Grounded question answering in images},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages = {4995--5004},
  year = {2016}
}
