\section{Experiments}
To validate the effectiveness of FCoT-VL, we evaluate on nine text-oriented mutimodal benchmarks: 
DocVQA\cite{mathew2021docvqa}, ChartQA\cite{masry2022chartqa}, TextVQA\cite{singh2019towards}, AI2D\cite{kembhavi2016diagram}, InfoVQA\cite{mathew2022infographicvqa}, OCRBench\cite{liu2024ocrbench}, OCRBench\_v2\cite{fu2024ocrbench},  MathVista and ScienceQA\cite{lu2022scienceqa}. 

\subsection{Main Results}
% 我们在internvl2系列模型上实现了我们的方案并且展示了在富文字理解任务上的表现。internvl2采用了高分辨率方案（动态patch）来提升MLLM的性能，每个patch提取256个token，因此输入LLM的视觉token数量很大。
We choose InternVL2-2B and InternVL2-8B as our baseline models, considering that their good adaption to high-resolution images and impressive performance.
As shown in Table \ref{tab:main_result}, we compress the visual tokens at ratio 50$\%$ and 75$\%$ of InternVL2-2B and InternVL2-8B, respectively. For the training-free FastV method,
we find significant performance drop on the different baseline VLLMs(i.e., LLaVA-1.5-7B\cite{liu2023improved}, LLava-NeXT\cite{liu2024llava} and InternVL2\cite{chen2024far}), particularly when the visual tokens drop to 1/4. 
For instance, at a compressing ratio of 50\%, the performance degradation is approximately 10\% on InternVL2-2B, but at 75\% compressing ratio, the performance drop exceeds 25\%.
This suggests that training-free
paradigm is insufficient in text-oriented tasks, specially for high-resolution and text-rich images.

% FastV on LLaVA-1.5 and LLaVA-NEXT does not perform as well as it does on natural images. Similarly, FastV shows a significant performance decline on InternVL2, particularly when the compression ratio drops to 1/4. For instance, at a compression ratio of 50\%, the performance degradation is approximately 10\%, and at 75\%, it exceeds 25\% for InternVL2-2B.

As for our FCoT-VL, it achieves more than 100\% average performance over baselines at the 50\% and 75\% visual token compressing ratios, respectively. More surprisingly,
even under an extreme compressing ratio of 75\%, FCoT-VL-2B exhibits only a slight performance degradation of approximately 5\% across most benchmarks when compared to baselines, making it a compelling choice for low-resource deployment inference.

% Additionally, we visualize the performance change across different tasks as shown in Figure~\ref{fig:percentagefcotvl}. We find FCoT-VL achieve performance improvement on OCRBench, OCRBench\_v2, AI2D and Mathvista, which requires
% complex vision understanding  and reasoning abilities. This validates that high-quality datas(discussed in In Section~\ref{abnormal}) are more important than simple resolution scaling law. 

Additionally, we visualize the percentage of performance variations across different tasks, as shown in Figure~\ref{fig:percentagefcotvl}. For DocVQA and InfoVQA, which heavily rely on high-resolution images, our FcoT-VL still inevitably incurs some performance degradation. 
This highlights the trade-off between token compressing and the performance fine-grained visual details in tasks demanding high-resolution inputs. In contrast,
FCoT-VL achieves performance improvements on tasks that demand advanced vision understanding and reasoning capabilities, such as OCRBench, OCRBench\_v2, AI2D, and MathVista. These obversations validate that high-quality data (as discussed in Section~\ref{abnormal}) plays a more critical role in enhancing performance than simply relying on resolution scaling laws.
% And our approach performs well with a 50\% compression ratio across most benchmarks, achieving a notable improvement in the overall model performance while reducing the number of visual tokens. In fact, it yields a 4.2\% performance boost for InternVL2-2B. We firmly believe that visual tokens are redundant in InternVL2, and redundant tokens fails to improve its performance.
% When the visual tokens are compressed to 1/4 of the original size, there is a slight performance decline of about 5\% across most benchmarks, but the model continues to perform reasonably well on average and does not show significant drawbacks. Furthermore, as illustrated in Figure~\ref{fig:performancechange}, we also tested the performance of checkpoints across three different tasks at various stages of SFT. All scores increased with training and stabilized by the third epoch. 
% These results suggest that, under the current high-resolution configurations, visual token redundancy is substantial, with redundancy potentially exceeding 50\%.
\begin{figure}[t]
\includegraphics[width=0.49\linewidth]{percentage2b.pdf}
\includegraphics[width=0.49\linewidth]{percentage8b.pdf}
  \caption {Performance percentage across multiple benchmarks under different compression ratios on the InternVL2-2B (left) and InternVL2-8B (right) models.}
  \label{fig:percentagefcotvl}
\end{figure}
% FastV on LLaVA-1.5 and LLaVA-NEXT do not perform well like they perform on natural images.
% % 同样，FastV同样在internvl2上也表现出明显的性能下降，尤其是当压缩率下降到1/4时。
% Similarly, FastV also shows a significant performance decline on InternVL2, especially when the compression ratio drops to 1/4.
% For instance, it causes a performance degradation of about 10\% at compress ratio of 50\% and more than 25\% at 75\% for InternVL2-2B. 
% % 但是，我们的方案在压缩率为1/2时，在绝大部分benchmarks上都有较好的性能，在视觉token数量下降的同时，还提升了模型的整体表现。
% And our scheme with a compression ratio of 50\% has a good performance in most benchmarks, improving the overall performance of the model while reducing the number of visual tokens. Actually, it even has got a 4.2\% performance boost for InternVL2-2B. We hold a strong belief the that the visual token  is redundant   for InternVL2 and redundant tokens fails to improve its performance.
% % 当视觉token压缩到原来的1/4时，模型的性能有一定的下降，但并未表现出明显的劣势。这些都说明了在目前的高分辨率方案下，视觉方案有很大的冗余，甚至视觉token的冗余度超过50%。
% When the visual token is compressed to 1/4 of the original, the performance of the model has a slight decline about 5\% in most benchmarks but it also maintains performance well on average. And it does not show a significant disadvantage. Besides, as shown in Figure~\ref{fig:performancechange}, we also test performance of checkpoints on  3 different tasks at different stages in SFT and all scores increase with training and stabilize in the third epoch. 





\subsection{Ablation Study}\label{ablationstudy}
% . While following similar model realignment procedures as mentioned earlier, our approach differs from traditional SFT-based realignment in that we specifically focused on datasets relevant to DocVQA, ChartVQA, and InfoVQA domains. Specifically, we performed SFT for 3 epochs using only the training splits of these datasets, along with the TextVQA training set.
% As demonstrated in Table~\ref{table:different_adapter}, CNN and pooling-based approaches exhibited comparable performance metrics. Notably, these methods achieved robust results across benchmarks despite being trained on a relatively small number of samples. Our findings suggest that with access to more comprehensive datasets, the performance could be further enhanced compared to our current results. Additionally, consistent with previous literature, we observed that Qformer exhibits training instability when faced with limited data availability.




% These methods experiments on internvl2-2B with several basic compressing adapters, Qformer, CNN and pooling (MaxPoiling). We realign models as illustrated before but it is different to SFT realigned models only with datasets that are relative to DocVQA, ChartVQA and InfoVQA. To be specific, we only use their train datasets, TextVQA train dataset to do SFT for 3 epochs. As shown in Table~\ref{tab:different_adapter}, the performance of CNN and pooling are almost same. They perform well in benchmarks even only trained with a few samples. And compared with main results, if datasets sufficient the final result will be better. Besides, the Qformer is hard to train for limited data as we expect and papers mentioned before.


\paragraph{Re-alignment}
We implement our FCoT-VL-2B and FCoT-VL-8B with CNN for compression. We craft diverse text-oriented understanding tasks, covering OCR-like tasks (i.e.,text recognition, image2markdown, chart2dict\cite{wei2023vary}). We sample a amount of 2 million image-text pairs and obtain fast and stable optimization as shown in Figure~\ref{fig:pretrain_loss}. Compared with scratch training like TextHawk2, which needs 100M data, our FCoT-VL-2B could finish pre-training about 24 hours with 64 GPUs(NPUs) resources.
\begin{table}[ht]
    \setlength{\tabcolsep}{1.5pt}
    \renewcommand{\arraystretch}{1.2}
    \centering
    \resizebox{\linewidth}{!}{
        \begin{tabular}{*{5}c}
            \hline
            \textbf{\makecell{Compress \\ Ratio}}\ & \textbf{DocVQA} & \textbf{ChartQA} & \textbf{InfoVQA} & \textbf{MME} \\
            \hline
            % \multicolumn{4}{c}{2B parameters} \\
            \textbf{0\%}    & 85.90 & 76.24 & 57.66 &   1440         \\
            \textbf{50\%}  &  74.27 & 55.24 & 47.86 & 1355       \\
            \textbf{75\%} & 63.40 & 49.20 & 38.76&1215\\
            \hline
        \end{tabular} 
    }
    \caption{The Performance of pretrain models under different compressing ratio.}
    \label{tab:pretrain_performance}
\end{table}
\begin{figure}[ht]
    \includegraphics[ width = \linewidth]{pretrainloss.pdf}
    \caption {The loss graphs of re-alignment pre-training. The loss undergoes a rapid loss reduction and a long smooth convergence.}
    \label{fig:pretrain_loss}
\end{figure}

We discuss the effects of different compressing ratio (50\% and 75\%) during re-alignment pre-training. 
As Table~\ref{tab:pretrain_performance}
lists, we compare the results of pretrain models of FCoT-VL-2B on the benchmark
DocVQA, ChartQA, InfoVQA and MME \cite{fu2024mmecomprehensiveevaluationbenchmark}. Although we employ OCR-like task for re-alignment, the vanilla model's abilities retain to a considerable extent. 
To alleviate the performance drops incurred by compressing visual token, we introduce a post-training stage in Section~\ref{sec:pt} to mitigate performance lost.

% After finishing pre-training, For FCoT-VL with 0.5 visual token pruning ratio, the 1M datas are all pretrain data so that we construct tests on DocVQA, ChartVQA, InfoVQA, and MME which models do not learn in this stage to  check the performance maintain. As shown in Table~\ref{tab:pretrain_performance},the models with 1/2 and 1/4 visual tokens realign visual modal and text modal and keep ability at the same time. Besides, the pretrain models are better than FastV. We only train the model with pretrain data of which almost are for text recognition tasks but the models maintain the performance well on benchmarks of other tasks like DocVQA, ChartVQA, InfoVQA, and even MME. That is, the model keep performance of other task when pretraining.



\paragraph{Visual token compression modules}
We test the different
compression modules in our FCoT-VL including: (1). Qformer: utilizing one cross-attention to sample fixed visual tokens from ViT backbone. 
(2). CNNs: applying a 1-d convolutional layer with a stride of $2$ for merging tokens.
(3). Pooling: passing
visual tokens via mean pooling operation for 2$\times$ downsampling. To compare the effects of above three methods,
% we employ same re-training pipeline to obtain pre-training models.
we perform a small-scale SFT on the same re-aligned model with 60k data for convenience of QA evaluation. As Table~\ref{tab:different_adapter} depicts, we find that Qformer suffers from serious performance drops under our data-constrained distillation training, sharing similar conclusion as in previous works\cite{yu2024texthawk2}. In contrast, both CNN and pooling-based architectures exhibit minimal performance degradation compared to the baseline InternVL-2B model. 
Furthermore, FCoT-VL with CNN architecture enjoys rapid loss decline at the beginning of training phases (consuming about 0.1M image-text samples), as illustrated in Figure \ref{fig:pretrain_loss}. Based on these empirical results, we select CNN as the compression module, leading to $2^n$ visual token token downsampling.
\begin{table}[ht]
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{lccc}
        \hline
    \textbf{\makecell{Compress \\ Module}} & \textbf{DocVQA} & \textbf{ChartQA} & \textbf{InfoVQA}\\
        \hline
        \textbf{original}  & 85.90 & 76.24  & 57.66           \\
        \textbf{Qformer}  &  48.23 & 42.32 & 26.36     \\
        \textbf{CNN} & 82.60 & 75.04 & 50.89           \\
        \textbf{Pooling} &  82.44 & 75.43  & 49.83         \\
        \hline
    \end{tabular}  
    \caption{Performance on different visual token compression module at the $50\%$ compressing ratio. }
    \label{tab:different_adapter}
\end{table}

\begin{figure}[ht]
  \includegraphics[width=\linewidth]{performancechange.pdf}
  \caption {Model performance changes across intermediate training iterations.}
  \label{fig:performancechange}
\end{figure}

\paragraph{Model merge.}
As shown in Figure \ref{fig:performancechange}, we 
observe a performance "see-saw" effect in several benchmarks during different training iterations, motivating us to explore model merging to mitigate this issue.
We compare No Merge (choosing final checkpoint) to three merge strategies with  five intermediate checkpoints, Simple Averaging with equal weight of 0.2, Task Arithmetic \cite{ilharco2022editing} (scaling factor of 0.5) and Sharply-based weighted fusion(Ours). 
As listed in Table~\ref{tab:different_merge_schemes}, Simple Averaging enhances performance compared to no merging across three benchmarks, while Task Arithmetic underperforms in InfoVQA, indicating that Task Arithmetic may not be well-suited for our unified SFT training pipeline. Our method achieves the best results, demonstrating the effectiveness of Sharply-based weight allocation in optimizing checkpoint contributions. Our empirical results suggest that model merging could help tiny-scale VLLMs to achieve heavy post-training.

\begin{table}[ht]
    \setlength{\tabcolsep}{5pt}
    \renewcommand{\arraystretch}{1.2}
    \centering
    \resizebox{\columnwidth}{!}{
        \begin{tabular}{lccccc}
            \hline
            \textbf{Merge Scheme} & \textbf{DocVQA} & \textbf{ChartQA} & \textbf{InfoVQA} \\
            \hline
            \textbf{No Merge}      &       85.12    &      77.31      &       54.43      \\
            \textbf{Simple Averaging} &     86.03      &    78.21        &     55.66        \\
            \textbf{Task Arithmetic}  &      85.31    &     77.43       &      53.14       \\
            \textbf{Ours}             &       \textbf{86.21}  &       \textbf{78.46}     &       \textbf{56.01}      \\
            \hline
        \end{tabular}
    }
    \caption{Performance of Different Merge Schemes on FCoT-VL-2B.}
\label{tab:different_merge_schemes}
\end{table}
\paragraph{Visualization Analysis}
We selected three typical types of text-oriented images, tables, web pages and PPT to visualize the visual token distribution.
As shown in Figure~\ref{fig:visaulanalysis}, the gray part indicates that the model has a lower attention on the concerned pixels. We also find gray part has the overlap with non-text regions, providing an evidence that visual tokens seems redundant even in the 50$\%$ visual token retained.

\paragraph{Inference Speed}\label{timecost}
As shown in Table~\ref{tab:time_cost}, we conduct experiments across three datasets with a single Ascend 910B4 NPU to test the the inference speed of our FCoT-VL models. If we take 75\% ratio as an example, FCoT-VL-2B achieves average 1.5$\times$ faster than baselines at the cost of 5$\%$ performance drops(Table~\ref{tab:main_result}). Furthermore, we observe that the inference efficiency becomes more significant as the LLM backbone is scaled up. Experimental results show that our model offers a cost-effective deployment with a considerably strong performance.
\begin{table}[h]
    \setlength{\tabcolsep}{0.5pt}
    \renewcommand{\arraystretch}{1.2}
    \centering
    \begin{tabular}{cccccccc}
        \hline
        \multirow{2}*{\textbf{\makecell{Model \\ Size}}}& \multirow{2}*{\textbf{\makecell{ Compress\\ Ratio}}} & \multicolumn{2}{c}{\textbf{DocVQA}} & \multicolumn{2}{c}{\textbf{ChartQA}} & \multicolumn{2}{c}{\textbf{InfoVQA}}\\
        ~ & ~ & time & $\delta$ & time & $\delta$ & time & $\delta$ \\
        \hline
        \multirow{3}*{2B} & 0\%  & 782  & -  & 544 & -  & 868 & - \\
        ~ & 50\% & 598 & 1.3$\times$ & 467 & 1.2$\times$ & 614 & 1.4$\times$ \\
        ~ & 75\% & 553 & \textbf{1.4$\times$} & 346 & \textbf{1.6$\times$} & 600 & \textbf{1.4$\times$}\\
        \hline
        \multirow{3}*{8B} & 0\% &1279 & -  & 704 &-  &1457 & - \\
        ~ & 50\% &838 & 1.5$\times$ & 544 &1.3$\times$ & 857 & 1.7$\times$\\
        ~ & 75\% &673 & \textbf{1.9$\times$} & 474 &\textbf{1.5$\times$} & 614 & \textbf{2.4$\times$}\\
        \hline
    \end{tabular}
    \caption{Inference time experiments on a single Ascend 910B4 NPU. Time is measured in milliseconds, and $\delta$ denotes the reduction ratio.}
    \label{tab:time_cost}
\end{table}


\begin{figure}[t]
  \includegraphics[width=\linewidth]{2_hotmap.png} 
  \includegraphics[width = \linewidth]{1_hotmap.png} 
  \includegraphics[width=\linewidth]{3_hotmap.png} 
  
  \caption { The attention score is calculated from the first layer of the LLM decoder. We use the same prompt: \textbf{please identify the text in the picture} to ask  FCoT-VL-2B(with 50\% reducing).}
  \label{fig:visaulanalysis}
\end{figure}



