[
  {
    "index": 0,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "zhu2023minigpt",
        "author": "Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed",
        "title": "Minigpt-4: Enhancing vision-language understanding with advanced large language models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "liu2024visual",
        "author": "Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae",
        "title": "Visual instruction tuning"
      },
      {
        "key": "liu2024llava",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "Llava-next: Improved reasoning, ocr, and world knowledge"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "masry2022chartqa",
        "author": "Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul",
        "title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "mathew2021docvqa",
        "author": "Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV",
        "title": "Docvqa: A dataset for vqa on document images"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "mathew2022infographicvqa",
        "author": "Mathew, Minesh and Bagal, Viraj and Tito, Rub{\\`e}n and Karatzas, Dimosthenis and Valveny, Ernest and Jawahar, CV",
        "title": "Infographicvqa"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "dong2024internlm",
        "author": "Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others",
        "title": "Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "wang2024qwen2vl",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "chen2024far",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites"
      },
      {
        "key": "hong2024cogvlm2",
        "author": "Hong, Wenyi and Wang, Weihan and Ding, Ming and Yu, Wenmeng and Lv, Qingsong and Wang, Yan and Cheng, Yean and Huang, Shiyu and Ji, Junhui and Xue, Zhao and others",
        "title": "Cogvlm2: Visual language models for image and video understanding"
      },
      {
        "key": "glm2024chatglm",
        "author": "GLM, Team and Zeng, Aohan and Xu, Bin and Wang, Bowen and Zhang, Chenhui and Yin, Da and Zhang, Dan and Rojas, Diego and Feng, Guanyu and Zhao, Hanlin and others",
        "title": "Chatglm: A family of large language models from glm-130b to glm-4 all tools"
      },
      {
        "key": "liu2024llavanext",
        "author": "Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae",
        "title": "LLaVA-NeXT: Improved reasoning, OCR, and world knowledge"
      },
      {
        "key": "wang2024qwen2vl",
        "author": "Peng Wang and Shuai Bai and Sinan Tan and Shijie Wang and Zhihao Fan and Jinze Bai and Keqin Chen and Xuejing Liu and Jialin Wang and Wenbin Ge and others",
        "title": "Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "chen2024internvl",
        "author": "Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others",
        "title": "Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks"
      },
      {
        "key": "chen2024far",
        "author": "Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others",
        "title": "How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "hu2024minicpm",
        "author": "Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others",
        "title": "Minicpm: Unveiling the potential of small language models with scalable training strategies"
      },
      {
        "key": "li2024baichuan",
        "author": "Li, Yadong and Sun, Haoze and Lin, Mingan and Li, Tianpeng and Dong, Guosheng and Zhang, Tao and Ding, Bowen and Song, Wei and Cheng, Zhenglin and Huo, Yuqi and others",
        "title": "Baichuan-omni technical report"
      },
      {
        "key": "li2025eagle",
        "author": "Li, Zhiqi and Chen, Guo and Liu, Shilong and Wang, Shihao and VS, Vibashan and Ji, Yishen and Lan, Shiyi and Zhang, Hao and Zhao, Yilin and Radhakrishnan, Subhashree and others",
        "title": "Eagle 2: Building Post-Training Data Strategies from Scratch for Frontier Vision-Language Models"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "chen2024expanding",
        "author": "Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others",
        "title": "Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "masry2022chartqa",
        "author": "Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul",
        "title": "Chartqa: A benchmark for question answering about charts with visual and logical reasoning"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "lu2024mathvista",
        "author": "Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng",
        "title": "MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "zhang2024sparsevlm",
        "author": "Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others",
        "title": "Sparsevlm: Visual token sparsification for efficient vision-language model inference"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "yang2024visionzip",
        "author": "Yang, Senqiao and Chen, Yukang and Tian, Zhuotao and Wang, Chengyao and Li, Jingyao and Yu, Bei and Jia, Jiaya",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "bolya2022token",
        "author": "Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy",
        "title": "Token merging: Your vit but faster"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "shang2024llava",
        "author": "Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan",
        "title": "Llava-prumerge: Adaptive token reduction for efficient large multimodal models"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "li2024llama",
        "author": "Li, Yanwei and Wang, Chengyao and Jia, Jiaya",
        "title": "Llama-vid: An image is worth 2 tokens in large language models"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "li2023blip",
        "author": "Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven",
        "title": "Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "cha2024honeybee",
        "author": "Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "chu2024mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others",
        "title": "Mobilevlm v2: Faster and stronger baseline for vision language model"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "bai2023qwen",
        "author": "Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren",
        "title": "Qwen-vl: A frontier large vision-language model with versatile abilities"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "li2024tokenpacker",
        "author": "Li, Wentong and Yuan, Yuqian and Liu, Jian and Tang, Dongqi and Wang, Song and Qin, Jie and Zhu, Jianke and Zhang, Lei",
        "title": "Tokenpacker: Efficient visual projector for multimodal llm"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "cha2024honeybee",
        "author": "Cha, Junbum and Kang, Wooyoung and Mun, Jonghwan and Roh, Byungseok",
        "title": "Honeybee: Locality-enhanced projector for multimodal llm"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "chu2024mobilevlm",
        "author": "Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others",
        "title": "Mobilevlm v2: Faster and stronger baseline for vision language model"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "bolya2022token",
        "author": "Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy",
        "title": "Token merging: Your vit but faster"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "shang2024llava",
        "author": "Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan",
        "title": "Llava-prumerge: Adaptive token reduction for efficient large multimodal models"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "zhang2024sparsevlm",
        "author": "Zhang, Yuan and Fan, Chun-Kai and Ma, Junpeng and Zheng, Wenzhao and Huang, Tao and Cheng, Kuan and Gudovskiy, Denis and Okuno, Tomoyuki and Nakata, Yohei and Keutzer, Kurt and others",
        "title": "Sparsevlm: Visual token sparsification for efficient vision-language model inference"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "yang2024visionzip",
        "author": "Yang, Senqiao and Chen, Yukang and Tian, Zhuotao and Wang, Chengyao and Li, Jingyao and Yu, Bei and Jia, Jiaya",
        "title": "VisionZip: Longer is Better but Not Necessary in Vision Language Models"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "yu2024texthawk2",
        "author": "Yu, Ya-Qi and Liao, Minghui and Zhang, Jiwen and Wu, Jihao",
        "title": "Texthawk2: A large vision-language model excels in bilingual ocr and grounding with 16x fewer tokens"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "chen2025image",
        "author": "Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao",
        "title": "An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large vision-language models"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "zhang2024dockylin",
        "author": "Zhang, Jiaxin and Yang, Wentao and Lai, Songxuan and Xie, Zecheng and Jin, Lianwen",
        "title": "Dockylin: A large multimodal model for visual document understanding with efficient visual slimming"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      },
      {
        "key": "yan2024corrective",
        "author": "Yan, Shi-Qi and Gu, Jia-Chen and Zhu, Yun and Ling, Zhen-Hua",
        "title": "Corrective retrieval augmented generation"
      }
    ]
  },
  {
    "index": 34,
    "papers": [
      {
        "key": "crawshaw2020multi",
        "author": "Crawshaw, Michael",
        "title": "Multi-task learning with deep neural networks: A survey"
      }
    ]
  },
  {
    "index": 35,
    "papers": [
      {
        "key": "matena2111merging",
        "author": "Matena, Michael and Raffel, Colin",
        "title": "Merging models with fisher-weighted averaging, 2021"
      }
    ]
  },
  {
    "index": 36,
    "papers": [
      {
        "key": "wortsman2022model",
        "author": "Wortsman, Mitchell and Ilharco, Gabriel and Gadre, Samir Ya and Roelofs, Rebecca and Gontijo-Lopes, Raphael and Morcos, Ari S and Namkoong, Hongseok and Farhadi, Ali and Carmon, Yair and Kornblith, Simon and others",
        "title": "Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time"
      }
    ]
  },
  {
    "index": 37,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  },
  {
    "index": 38,
    "papers": [
      {
        "key": "matena2111merging",
        "author": "Matena, Michael and Raffel, Colin",
        "title": "Merging models with fisher-weighted averaging, 2021"
      }
    ]
  },
  {
    "index": 39,
    "papers": [
      {
        "key": "matena2111merging",
        "author": "Matena, Michael and Raffel, Colin",
        "title": "Merging models with fisher-weighted averaging, 2021"
      }
    ]
  },
  {
    "index": 40,
    "papers": [
      {
        "key": "jin2022dataless",
        "author": "Jin, Xisen and Ren, Xiang and Preotiuc-Pietro, Daniel and Cheng, Pengxiang",
        "title": "Dataless knowledge fusion by merging weights of language models"
      }
    ]
  },
  {
    "index": 41,
    "papers": [
      {
        "key": "yadav2023resolving",
        "author": "Yadav, Prateek and Tam, Derek and Choshen, Leshem and Raffel, Colin and Bansal, Mohit",
        "title": "Resolving interference when merging models"
      }
    ]
  },
  {
    "index": 42,
    "papers": [
      {
        "key": "ilharco2022editing",
        "author": "Ilharco, Gabriel and Ribeiro, Marco Tulio and Wortsman, Mitchell and Gururangan, Suchin and Schmidt, Ludwig and Hajishirzi, Hannaneh and Farhadi, Ali",
        "title": "Editing models with task arithmetic"
      }
    ]
  }
]