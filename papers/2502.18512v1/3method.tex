\section{Method}
% 在本小节，出于以下三点考虑，我们提出了XXXXX和模型和并:(1)在信息密集领域下，压缩视觉token并只使用少量数据重新对齐视觉模态和文本模态；(2)重新对齐时尽可能的保留其他领域的能力；（3）利用训练过程中的不同阶段结果，无负担的提升多模态模型的表现
We propose FCoT-VL, a framework for compressing visual tokens in VLLMs. It has the following objectives: (1). The efficient realignment training stage. we propose a self-distillation framework to transfer visual token knowledge from rich-token VLLM to compressed-token VLLM. We only learn lightweight parameters with limited datas to acquire visual token compression ability while maintaining training and inference efficiency. (2). To boost text-oriented VLLM after visual token cutoff, we focus on advanced post-training and data augmentation techniques, enabling the student model to catch up with InternVL2.

\subsection{Architecture of FCoT-VL}
As shown in Figure~\ref{fig:Overvieww}, we present the architecture design of our FCoT-VL, which comprises a vanilla VLLM as the teacher model(i.e., InternVL2) and a VLLM with compressed visual tokens as the student model in the distillation process.
\subsubsection{Re-alignment}
\paragraph{Definition} As illustrated in Figure~\ref{fig:Overvieww}, the basic architecture of the re-alignment consists of five primary components: a shared visual encoder $ViT_{\phi}$, a shared large language model $LLM_{\theta}$, a teacher visual adaptor $A_t$ and a student visual adaptor $A_s$, and a visual token compression module $V_c$. Given a visual instruction input ($x_t$, $x_v$, $y$), then the responses are computed as follows:
\begin{equation}
    \left\{
    \begin{aligned}
        \hat{y_t} & = LLM_{\theta}[A_{t}(x_v);x_t] \\
        \hat{y_s} & = LLM_{\theta}[A_{s}(V_{c}((x_v);x_t]
    \end{aligned}
    \right.
\end{equation}
where $[;]$ means the concatenation operation, $x_v$ is the input image and $x_t$ is the text instruction embeddings. The $\hat{y_t}$ and $\hat{y_s}$ denote the probabilities of  responses for the teacher model $t$-VLLM and 
student model $s$-VLLM, respectively.
% Besides, the MLPs are private to each other.
\paragraph{Initialization} We initialize our student model inherited from the teacher model parameters. During re-alignment stage, we freeze all the parameters of the teacher models. The $LLM_\theta$ and $ViT_{\phi}$ in $s$-VLLM maintain frozen since their pre-trained parameters have already captured rich visual and language knowledge. Only the student adaptor $A_s$ and the visual token compression module $V_c$ are learnable to  bridge different modalities and compress visual tokens of the LLM part.
% and All the parameters of shared modules  $ViT_{s}$ and $LLM_{s}$ are frozen in re-alignment stage.
% \begin{itemize}
%     \item The parameters of shared modules are initialized with $M_o$, and both of them are frozen in re-alignment stage.
%     \item The parameters of the two visual adaptors are initialized from $M_o$, with the teacher adaptor $A_t$ frozen and the student adaptor $A_s$ training in re-alignment stage.
%     \item Also, if the visual token compression module is learnable, it is initialized from scratch.
% \end{itemize}
% 如图中所示，重新对齐阶段我们冻结了教师模型和学生模型公用的主干（vit,llm decoder）以及教师MLP，仅训练student MLP和压缩适配器，

\paragraph{Self-distillation} We compare different visual token adjustment methods like Qformer \cite{li2023blip}, pooling and convolution as $V_c$ as in Table~\ref{tab:different_adapter}. We find that
employing a simple convolutional layer could reduce visual tokens ($\times$4 and $\times$2 ratio) effectively.

% We find that training Qformer presents similar challenges to those encountered with previous Multimodal Large Language Models. Meanwhile, employing a simple convolutional layer could reduce visual tokens ($\times$4 and $\times$2 ratio) effectively.
% In contrast to previous training-based distillation methods that focus primarily on QA tasks, we argue that OCR-like tasks require models to perceive the dense information across the entire image. This requires efficient re-alignment to better facilitate our FoCT-VL.

We aim to re-align the visual tokens with text tokens in the $s$-VLLM using OCR-like tasks, which converts texts in the images into an editable text format. Different from previous training distilling methods, which focuses on QA tasks, we argue that OCR-like tasks  require models to perceive dense information of the whole image and benefit efficient re-alignment for FCoT-VL. Accordingly, our OCR data sources are sampled from $t$-VLLM(i.e., InternVL2 series) with a small amount 2M image-text pairs, covering text recognition, layout parsing from web, natural, document, table, chart and handwritten samples. 

To maintain and leverage the performance of the teacher model,
the training objective is to minimize the Kullback-Leibler (KL) divergence between the output logits of $t$-VLLM and $s$-VLLM. The objective function is:
% $\mathcal{L}_{\text{KL}}$:
\begin{equation}
    \mathcal{L}_{\text{KL}}(\hat{y_t} \parallel \hat{y_s}) = \sum_{i}^{N} \hat{y_t}(i) \log \left( \frac{\hat{y_t}(i)}{\hat{y_s}(i)} \right)
\end{equation}
Where $\hat{y_t}$ and $\hat{y_s}$ are the logits of the teacher model and student model, receptively. $N$ is the total token length. The output of the teacher model plays the role of soft labels to guide visual token compression. Additionally, we find that introducing ground-truth answers as hard labels contributes to stable training. The the Cross-Entropy loss is:
\begin{equation}
    \mathcal{L}_{\text{CE}} = -\sum_{i}^{N}\log\hat{y_s}(i))
\end{equation}
Then the optimization goal is to minimize the $\mathcal{L}=\mathcal{L}_{\text{KL}} + \mathcal{L}_{\text{CE}}$.
% We also hope that the student model can learn to overcome the limitations of the original model, guided by the teacher, so that we can obtain expected answers as hard labels to compute Cross-Entropy $\mathcal{L}_{\text{CE}}$.
% \begin{equation*}
%     \mathcal{L}_{\text{CE}} = - \sum_{i=1}^{V} y(i) \log(\hat{y_s}(i)) = - \log\hat{y_s}(i))
% \end{equation*}
% Where $y$ is the ground-truth answers, $V$ is the size of LLM vocabulary and due to the $y$ is one-hot encoding $\mathcal{L}_{\text{CE}}$ degenerates to the latter equation.

% Finally, the optimization goal is to minimize the $\mathcal{L}$:
% \begin{equation*}
% \mathcal{L} = \mathcal{L}_{\text{KL}} + \mathcal{L}_{\text{CE}}    
% \end{equation*}    
\subsubsection{Post-Train}\label{sec:pt}
% After re-alignment stage, the student backbone will be extracted separately form the distillation re-alignment architecture.(\textbf{??? only backbone keeps?}) We keep all parameters learnable and use SFT-datasets to finetune the aligned model.
In this section, we describe supervised fine-tuning(SFT) aimed at improving the student model's performance in text-oriented tasks. We accept many open-source datasets reported in previous VLLMs \cite{chen2024far}, covering a variety of downstream tasks. However, we find that many of these public datasets are not formatted in an instruction style. To overcome this, we leverage distillation from teacher models to acquire the conversation style. Subsequently, we prompt the InternLM2.5-7B \cite{cai2024internlm2} to rewrite the instruction datas with the tone of the teacher model. Moreover, we observe this rewriting method facilitates fast and stable training, which may be attributed to the strong alignment with the teacher model.


\paragraph{Chain-of-Thought pipeline.} \label{abnormal}For reasoning tasks like math, chart reasoning and calculation problems, we leverage Rejection Sampling (RS) to expand the SFT dataset using larger and stronger multimodal language models. Specifically, for the question $q$, we employ RS to generate a new response with COT, obtaining the reasoning steps $R_{cot}$ and final answer $R_{ans}$, respectively. We use rule-based verifications that verify the correctness of the concluded answer $R_{ans}$ for the given problem $q$ based on the ground truths. We find that the mixture of RS-augmented and vanilla data significantly enhances reasoning capabilities. For example, our FCoT-VL-2B, with half visual tokens retained, achieves a score of 58.96 on MathVista \cite{lu2024mathvista}, outperforming many 7B-scale VLLMs.

\paragraph{Data sampling pipeline.} Considering that our tasks cover diverse image understanding and reasoning tasks with varying difficulty levels in a single SFT stage, we develop a novel sampling strategy, termed post-training sampling, to address these potential issues. Specifically, we perform coarse training using a small subset of the entire dataset at first, and then analyze the training loss distributions across different tasks. For datasets exhibiting much lower loss values, indicating easier learning, we down-sample them in the subsequent formal training. Conversely, we identify tasks (excluding generation tasks) with higher loss values and increase their sampling probabilities, addressing the model's weaknesses, especially in reasoning tasks.
 
%cot renshe


% 时间复杂度计算

 % \hline
 %                 \multirow{3}*{LLaVa-1.5} & original & 0.8590 & 0.7624 & 0.7336 & 0.5766 & 784 & 0.7409 & & - \\
                 
 %                 ~ & \textbf{\makecell{minidoc-2B \\ (1/2 tokens)}} & 0.8500  & 0.7812 &0.7242 & 0.5543 & 811 & 0.8521 &  & 0.9043\\
                 
 %                 ~ & \textbf{\makecell{minidoc-2B \\ (1/4 tokens)}}  & 0.8351 & 0.7576 &0.7223 & 0.5215 & 810 & 0.8393&  & 0.9098\\










%%%model merge

%使用博弈论中的Shapley值进行模型融合是一种基于公平贡献评估的策略，可以帮助合理地整合不同模型的参数。通过这种方法，可以充分利用不同模型在不同任务中的优势，并且融合后的模型能够在性能上得到优化。这种方法对于多模态任务（如视觉-语言任务）尤其适用，因为不同模态和模块的贡献可能是非线性的，通过Shapley值可以精确地评估各个部分的相对重要性。



%提出总的方案
\paragraph{Model Merging.}
% Since we employ a unified SFT stage on the small-scale VLLMs, 
% Several well-established model merging methods, such as Simple Averaging, Fisher Averaging, Task Arithmetic, and others, typically involve training multiple models on different datasets with varying hyperparameters and then merging them. 
Since our SFT training covers many tasks,  we aim to merge the base model with weighted differences from each checkpoint during training. These checkpoints reflect different stages of fine-tuning, with each stage capturing important task-specific adaptations. During training, multiple intermediate checkpoints are saved, and they are merged using the following formula:
\begin{equation}
M_{\text{mge}} = \theta_{\text{base}} + \sum_{i=1}^{n} \alpha_i (\theta_{\text{cpt}_i} - \theta_{\text{base}})
\end{equation}
Where $M_{\text{mge}}$ is the merged model, \(\theta_{\text{base}}\) is typically used as the final model, and \(\alpha_i\) is the weight for the difference between the checkpoints $\theta_{\text{cpt}_i}$ and the base model. $n$ is set as 5.
The goal is to determine the optimal fusion weights, formulated as:
\begin{equation}
\arg\max_{\alpha_1, \dots, \alpha_n} f( \theta_{\text{base}} + \sum_{i=1}^{n} \alpha_i (\theta_{\text{cpt}_i} - \theta_{\text{base}}))
\end{equation}
Rather than relying on costly heuristic algorithms, we use Shapley values\cite{sundararajan2020many}, to fairly serve the merge weight \(\alpha_i\) to each checkpoint \(M_i\) based on its contribution to the final model performance. The weighted combination of checkpoints thus optimizes the final model's performance based on their individual contributions.
%Then final weight \(\alpha_i\) for each model is normalized as: $\alpha_i = \frac{\phi_i}{\sum_{j=1}^{n} \phi_j}$.

% The Shapley value for a model \(M_i\) is computed as \cite{}:
% \begin{equation}
% \resizebox{0.5\textwidth}{!}{\phi_i = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N|-|S|-1)!}{|N|!} \left[ C(S \cup \{i\}) - C(S) \right]}
% \end{equation}
% Where \(\phi_i\) represents the Shapley value of model \(M_i\), and \(C(S)\) is the performance function for the model combination \(S\). 
% Then final weight \(\alpha_i\) for each model is normalized as: $\alpha_i = \frac{\phi_i}{\sum_{j=1}^{n} \phi_j}$. This normalization ensures that the weights reflect each model's relative importance in determining the merged model's performance. The weighted combination of models thus optimizes the final model's performance based on their individual contributions.
\paragraph{Computation Complexity.} 
In this section, we analyze the computation complexity of FCoT-VL in our post-training stage. The computational burden in the FCoT-VL is predominantly attributed to the attention operations within the LLM decoders. Assuming the LLM decoders has \( L\) layers, we only compute the complexity of one self-attention and feed-forward network, yielding: 
\begin{equation}
O(L \cdot (n^2 \cdot d + n \cdot d^2))
\end{equation}
Where $n$ is the length of input vectors and $d$ is the dimension of LLM's input tokens. When the compress ratio is $r$, the computation complexity could be reduced as:
\begin{equation}\label{eq_7}
O(L \cdot (\frac{n^2 \cdot d}{r^2}  + \frac{n \cdot d^2}{r}))
\end{equation}
% In conclusion, compressing the sequence length \( n \) leads to a significant reduction in the computation time of LLM decoders which occupies the main time consumption of VLLMs.
% 总之，压缩序列长度 \( n \) 会显著减少 Transformer 模型的计算时间。

Since the computation cost of LLM decoders is dominant in the our FCoT-VL, the overall computation complexity will be reduced much, facilitating training and inference effectiveness. More quantitative experiments are discussed in Section~\ref{timecost}.

%depending on the relative sizes of \( n \) and \( d \).

%and We assume that the compression ratio is $r$. % 当序列长度 \( n \) 被压缩为原来的一半时，Transformer模型的训练和推理时间将显著减少。


% The main computation in the VLLMs comes from  that LLM decoders calculate the attention matrix and performing weighted summation. 
% 自注意力机制中的主要计算来自计算注意力矩阵和执行加权求和。
% The attention matrix is computed by taking the dot product between the query matrix and the key matrix, which results in an \( n \times n \) matrix, with a complexity of \( O(n^2 \cdot d) \). 
% % 这是因为注意力矩阵是通过查询矩阵 \( Q \) 和键矩阵 \( K \) 的点积计算的，得到一个 \( n \times n \) 的矩阵，其复杂度为 \( O(n^2 \cdot d) \)。
% Besides, each element of the attention matrix is used to perform weighted summation over the value matrix \( V \), which also has a complexity of \( O(n^2 \cdot d) \). % 接下来，注意力矩阵的每个元素都用来对值矩阵 \( V \) 进行加权求和，这一步的复杂度也是 \( O(n^2 \cdot d) \)。
% Thus, the computation complexity of the self-attention mechanism in a single layer is \( O(n^2 \cdot d) \). % 因此，单层自注意力机制的计算复杂度是 \( O(n^2 \cdot d) \)。
% And each layer in the decoders contains fully connected layers as well and the computation complexity is \( O(n \cdot d^2) \). % Transformer 中的每一层也包含全连接层。
% Assuming the LLM decoders has \( L \) layers, we only compute the complexity of one self-attention and fully connected calculation, yielding: 
% \begin{equation}
% O(L \cdot (n^2 \cdot d + n \cdot d^2))
% \end{equation}


% Since the self-attention mechanism typically dominates the computation time due to its \( O(n^2) \) complexity, reducing \( n \) significantly impacts the training and inference time. % 由于自注意力机制通常由于其 \( O(n^2) \) 的复杂度主导计算时间，减少 \( n \) 会显著影响训练和推理时间。

% In conclusion, compressing the sequence length \( n \) leads to a significant reduction in the computation time of LLM decoders which occupies the main time consumption of VLLMs.
% % 总之，压缩序列长度 \( n \) 会显著减少 Transformer 模型的计算时间。
% After compression, the overall training and inference time will be reduced by approximately $\frac{1}{r^2}$ to $\frac{1}{r}$ of its original time, depending on the relative sizes of \( n \) and \( d \). % 压缩后，总体训练和推理时间大约会减少到原来的二分之一到四分之一之间，具体减少的幅度取决于 \( n \) 和 \( d \) 的相对大小。






\begin{table*}
    \setlength{\tabcolsep}{1.5pt}
    \renewcommand{\arraystretch}{1.5}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{lc*{9}ccc|c}
            \hline
             \textbf{Base Model} & \textbf{Method} & \textbf{\makecell{Compress \\Ratio}}  & \textbf{DocVQA} & \textbf{ChartQA} & \textbf{TextVQA} & \textbf{InfoVQA} & \textbf{OCRBench} & \textbf{\makecell{OCRBench \\ v2 En}} & \textbf{\makecell{OCRBench \\ v2 Ch}} & \textbf{AI2D} & \textbf{MathVista} & \textbf{ScienceQA} & \textbf{\makecell{Avgs \\($\%$)}} \\
             \hline
             \multirow{2}*{\makecell{LLaVA-1.5 \\ 7B}}  &original & 0\% & 28.10 & 17.8 & 58.2 & 25.8 & - & - & - & 55.5 & 25.6 & - &  100 \\
             \cline{2-14}
             ~  &\multirow{1}*{FastV}& 50\% & -  & 17.7 & 45.5 & - & - & - & - & - & - & - & 88.81\\
             
             % ~  & ~ & 25\% & 83.51 & 75.76 & 72.23 & 52.15 & 81.00 & - & - & 83.93 & 90.98 & \\
             \hline
             \multirow{3}*{\makecell{LLaVA-NeXT \\ 8B}} &original & 0\% & 78.22 & 69.28 & 65.41 & - & - & 31.5& 9.1 & - & - & - & 100 \\
             \cline{2-14}
             ~  &\multirow{2}*{FastV}& 50\% & 73.92  & 67.60 & 65.15 & - & - & - & - & - & - & - & 97.23\\
             
             ~  & ~ & 75\% & 66.67 & 62.80 &  63.08 & - & - & - & - & - & - & - & 90.77 \\
             \hline
             % 2B
             \multirow{5}*{\makecell{InternVL2 \\ -2B}} &original & 0\% & 85.90 & 76.24 & 73.36 & 57.66 & 78.4 & 35.7 & 34.5 & 74.09 & 46.30 & 94.25 & 100 \\
             \cline{2-14}
             % FastV
             ~  &\multirow{2}*{FastV}& 50\% & 79.39 & 69.72 & 73.15 & 47.18  & 73.3 &33.2& 26.1 & - & - & - & 89.65\\
             % \makecell{ \\ \%}
             ~  & ~ & 75\% & 60.12  & 60.76  & 70.3 & 35.82  & 64.3  & 29.0 & 19.8 & - & - & - & 75.47 \\
             \cline{2-14}
             % ours
             ~  &\multirow{2}*{Ours}& 50\% & \textbf{86.21}& \textbf{78.46} & 72.90  & 56.01 & \textbf{80.2}  & 35.6  & \textbf{34.8}  & \textbf{85.80}  & \textbf{58.96}  & 90.68 & \textbf{104.20}\\
             
             ~  & ~ & 75\% & 83.60 & 75.84  & 72.37 & 52.52  & \textbf{81.2} & 33.5 & 34.4 & \textbf{84.20} & \textbf{52.90} & 91.72  & \textbf{100.91} \\
             \cline{2-14}
            \hline
             % 8B
              \multirow{5}*{\makecell{InternVL2 \\ -8B}}&original & 0\% &91.6 & 83.3 & 77.4 & 74.8 & 79.4 & 39.6& 36.3 & 83.77 & 58.3 & 97.22 & 100\\
              \cline{2-14}
             % FastV
             ~  &\multirow{2}*{FastV}& 50\% & 85.25  & 79.5 & \textbf{77.61}& 60.9 & 76.1  & 36.8 & 26.4 & - & - & - & 93.21 \\
             
             ~ & ~ & 75\% & 67.52 & 73.52 & 74.65 & 46.06 & 68.1 &28.7 & 21.2 & - & - & - & 81.15\\
             \cline{2-14}
             % ours
             ~  &\multirow{2}*{Ours}& 50\% & \textbf{91.88}  & \textbf{85.52} & \textbf{78.95} & 71.71 & \textbf{83.9}  & \textbf{42.1} & \textbf{40.1} & \textbf{93.80} & \textbf{63.3} & 95.14 & \textbf{103.43}\\
             
             ~  & ~ & 75\% & 89.91  & \textbf{84.16}  & \textbf{77.80}  & 67.11 & \textbf{82.0} & \textbf{41.8} & \textbf{36.7} & \textbf{93.48} & \textbf{62.00}  & 93.40  & \textbf{100.84} \\
             \hline
        \end{tabular}
    }
    \caption{Performance comparison across various text-oriented tasks under different compression ratios settings. This table summarizes the performance metrics of different models at different compression ratios. Our benchmarks include document, chart, natural, scientific and math images. Items that outperform the baseline are bolded in the table and the average performance across all tasks is also provided in the last column. }
    \label{tab:main_result}
\end{table*}