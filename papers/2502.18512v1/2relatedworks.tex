\section{Related Works}
\subsection{Vision Large Language Models}


%%丰富类内容

%% 应该包含 LLaVA ， Gemini，internvl2，qwenvl2，deepseek-v
In recent years, open-source  VLLMs have made significant advancements, driven by contributions from both academia and industry. 
Earlier models, such as BLIP-2 \cite{li2023blip}, MiniGPT\cite{zhu2023minigpt} and LLaVA\cite{liu2024visual,liu2024llava}, have proven to be effective for vision-language tasks via bridging off-the-shelf ViTs and LLMs. 
However, early VLLMs struggle with processing images containing
fine-grained details, especially for OCR-like tasks such as charts\cite{masry2022chartqa}, documents\cite{mathew2021docvqa},
and infographics\cite{mathew2022infographicvqa}. To this end, InternVL series propose
an adaptive cropping method to convert vanilla images as several fixed image patches. For example, InternLM-XComposer2-4KHD\cite{dong2024internlm} increases 336 pixels of CLIP to 4K resolution and gets strong document understanding ability. InternVL2
obtains promising results on text-oriented benchmarks via scaling up image resolution and ViT model parameters. Moreover, QwenVL2 \cite{wang2024qwen2vl} proposes a native dynamic processing of images at varying resolutions. This image processing setting generates more visual tokens and suppress adaptive cropping VLLMs. However,
high-resolution processing pipelines bring substantial computational overhead in both training and inference stages, hindering real-world deployment.



% Following works scale up image resolution to enhance the capabilities of fine-grained perception\cite{chen2024internvl,chen2024far,hong2024cogvlm2,glm2024chatglm,liu2024llavanext,wang2024qwen2vl}.
% InternVL families\cite{chen2024internvl,chen2024far} introduce a dynamic resolution cropping scheme to generate dynamic visual tokens, enhancing the capture of fine-grained details at the cost of higher visual tokens. However,
% this brings substantial computational overhead in both training and inference stages, hindering real-world deployment.



Beyond high-resolution tricks, many works reveal that high-quality datas are more important for advancing document understanding. Recent studies\cite{hu2024minicpm,li2024baichuan,li2025eagle} highlight the critical role of data quality in VLLMs. For instance, InternVL-2.5\cite{chen2024expanding} enhanced performance of previous version through collecting more diverse dataset and data processing pipelines.

In this paper, we also explore how to obtain high-quality post-training datas to match frontier open-source VLLMs. Specifically, Our FCoT-VL outperforms the base model InternVL2
on many benchmarks like ChartQA\cite{masry2022chartqa} and MathVista\cite{lu2024mathvista}, despite reducing visual tokens by 50$\%$.



\begin{figure*}[t]
  \centering
  \includegraphics[width=\textwidth]{overview.pdf}
  \caption{Overall Structure of FCoT-VL. FCoT-VL is a self-distillation architecture in which only the Student-Projector and Compress-Module are learned, while all the other modules remain frozen. The student and teacher models share the same ViT encoder and the LLM decoder.}
  \label{fig:Overvieww}
\end{figure*}
\subsection{Visual Compression Schemes}
% Introduce Compression Scheme Evolution from Vit to Vllm
% Since the appearance of Vision Transformer (VIT), many schemes on compressing the number of tokens extracted by VITs are proposed by different scholars to reduce visual infomation redundancy. 
Visual compression, a key focus in high-resolution VLLMs, aims to efficiently reduce the use of vision tokens, minimizing computational and memory overheads. The inherent redundancy of visual data, compared to dense textual data, underscores the importance of compression.

Solutions to visual compression can be broadly categorized into two main approaches: training-free and training-based ones. Training-free methods dynamically select more important vision tokens via various strategies during decoding stage. For instance, SparseVLM\cite{zhang2024sparsevlm} and VisionZip \cite{yang2024visionzip} prioritize tokens based on attention scores. ToMe\cite{bolya2022token} and LLaVA-PruMerge\cite{shang2024llava} cluster tokens using cosine similarity. However, the training-free paradigms suffer from significant performance drops in text-orientated benchmarks. 
In contrast, training-based methods focus on optimizing the visual adaptor by incorporating external modules for token reducing. For instance, LLaMA-VID\cite{li2024llama} enhances visual information extraction through Q-Former\cite{li2023blip} with context tokens. Similarly, models like C-Abstractor\cite{cha2024honeybee} and LDP\cite{chu2024mobilevlm} serve as promising alternatives for visual token compressing.

Training from scratch necessitates extensive alignment datasets and substantial computational resources, often consuming thousands of GPU days. In this work, we present an efficient training token-compressing framework that achieves comparable performance while significantly reducing both data and computational requirements.



% Early works focus on projectors in VLLMs , like QwenVL (\citeauthor{bai2023qwen}), applying the Qformer-like structure which utilize learnable queries to extract dense information  by cross-attention layers. Recently, Tokenpacker (\citeauthor{li2024tokenpacker}) use multi-level features as queries and values to improve the performance. Some other works, like C-abstractor (\citeauthor{cha2024honeybee}) and LDP (\citeauthor{chu2024mobilevlm}), prove that convolution layers can generate compressed tokens for VLLMs.

% Some recent works leverage merging tokens to compress visual information based on similarity. For instance, ToMe (\citeauthor{bolya2022token}) divides visual tokens into two sets, connects the most similar tokens based on cosine similarity between sets and averaging features before concatenating the sets; LLaVA-PruMerge (\citeauthor{shang2024llava}) dynamically selects the most important visual tokens and merges similar ones using a clustering approach. Besides, some works pay more attention to the attention score of models. SparseVLM (\cite{zhang2024sparsevlm}) selects the relevant text rater based on the attention score between vision and text, and then selects the more important visual token based on the attention score between the rater and vision. VisionZip (\cite{yang2024visionzip}) focuses on selecting the most informative token by the attention score and discarding the less informative tokens to reduce the number of overall tokens. But most of current works are based on LLaVa-1.5 which is not high-resolution schema. Few works, like TextHawk2 (\cite{yu2024texthawk2}), did some research on high-resolution schema.

% Except in the vit stage, some works propose token pruning methods happening in the LLM stage. FastV (\citeauthor{chen2025image}) ranks tokens by the average attention value with all other tokens, and prunes out the last R tokens. Additionally, DocKylin (\citeauthor{zhang2024dockylin}) also provide a method in the data processing stage called Adaptive Pixel Slimming (APS) that reduce the resolution of images.
 
% \subsection{Model merge}
% In recent years, model merging has emerged as a prominent research area, focusing on combining multiple task-specific models into a single, versatile model with broad capabilities (\citeauthor{wortsman2022model,yan2024corrective}). Unlike multi-task learning (\citeauthor{crawshaw2020multi}), which also seeks to develop a single model capable of handling multiple tasks, model merging prioritizes the integration of model parameters without the need for access to the original training data (\citeauthor{matena2111merging}). One well-known approach to model merging is Average Merging (\citeauthor{wortsman2022model}), where the merged model is constructed by averaging the parameters of the individual models. Another method, Task Arithmetic (\citeauthor{ilharco2022editing}), incorporates a pre-defined scaling factor to adjust the influence of different models. Fisher Merging (\citeauthor{matena2111merging}) applies weighted averaging of parameters, with the weights determined by the Fisher information matrix (\citeauthor{matena2111merging}). In contrast, RegMean (\citeauthor{jin2022dataless}) solves the merging problem by optimizing a linear regression framework with closed-form solutions. Lastly, TIES-Merging (\citeauthor{yadav2023resolving}) addresses task conflicts identified (\citeauthor{ilharco2022editing}) by trimming parameters with low magnitudes, resolving sign inconsistencies, and merging parameters with consistent signs in a disjoint manner.

    