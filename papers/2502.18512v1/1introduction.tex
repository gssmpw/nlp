\section{Introduction}

% These instructions are for authors submitting papers to *ACL conferences using \LaTeX. They are not self-contained. All authors must follow the general instructions for *ACL proceedings,\footnote{\url{http://acl-org.github.io/ACLPUB/formatting.html}} and this document contains additional instructions for the \LaTeX{} style files.

% The templates include the \LaTeX{} source of this document (\texttt{acl\_latex.tex}),
% the \LaTeX{} style file used to format it (\texttt{acl.sty}),
% an ACL bibliography style (\texttt{acl\_natbib.bst}),
% an example bibliography (\texttt{custom.bib}),
% and the bibliography for the ACL Anthology (\texttt{anthology.bib}).

% fan
% The rapid advancement of artificial intelligence has led to significant progress, particularly with the development of Large Language Models (LLMs) , followed by the rise of Multimodal Large Language Models (MLLMs) . This shift has fundamentally transformed how machines perceive and engage with the world. The advent of MLLMs such as GPT-4o , known for their remarkable multimodal capabilities and enhanced interaction features, has highlighted their crucial role in a wide range of real-world applications. These developments set new standards for the potential of human-computer interactions, especially in fields like minidoc, where seamless integration of multimodal understanding is key.

% Despite the significant advancements in MLLMs, current open-source implementations still face considerable limitations, especially in their multimodal functionality and the overall quality of user interaction. These challenges hinder the widespread deployment and impact of such models across various domains, including natural language processing  and computer vision. In particular, when applied to tasks like OCR in the minidoc domain, the primary challenge lies in the compression of visual tokens, which affects the model's ability to efficiently process and interpret complex visual information.

% %模型融合intro
% In this work, we focus on the exploration of multi-stage model fusion techniques within the context of MLLMs. By adapting multi-stage fusion strategies—where different modalities are combined at various stages of processing—we have uncovered new insights that significantly enhance modality alignment and interaction. Our findings reveal that employing a multi-stage fusion approach can better capture the complexity of multimodal data, leading to richer and more contextually aware MLLMs capable of handling diverse real-world tasks. 

% Maybe We Need a Big Picture of General Performance on Different Benchmark, like DocVqa,ChartVqa and others.

%tang
% 近期，多模态大模型引起了广泛的关注。其中LLAVA提供了一个简单的对齐方案，通过视觉编码器（比如CLIP）将图像转为视觉token，与文本token一起送到LLM（一般是训练过的LLM），视觉token与文本token在特征层面上对齐。后续大量工作基于这种范式。【提这个原因是因为我们是基于这种范式，除了llava这种范式，其余方式可能会有更好的降低视觉token的方法】。

The success of Large Language Models (LLMs) \cite{achiam2023gpt,yang2024qwen2,zhu2023minigpt,dubey2024llama3,bi2024deepseek,cai2024internlm2} has inspired efforts to extend their capabilities to other modalities, particularly vision. In vision-language tasks, VLLMs process visual features extracted from vision transformers (ViTs) \cite{radford2021clip}) and integrate them to LLMs. The performance of these models is often positively correlated with visual resolution.
% This can be challenging when dealing with high-resolution or detailed images, as fixed resolution limits the number of visual tokens that can be effectively extracted, which potentially reduces the quality of the model’s output.

Improving visual resolution in ViTs involves fixed high-resolution settings (e.g., CogVLM2 \cite{hong2024cogvlm2}, GLM4V9B \cite{glm2024chatglm}), slicing patch schemes (e.g., LLaVA 1.6 \cite{liu2024llavanext}, InternVL series \cite{chen2024far}), or simple dynamic resolution (Qwen2-VL \cite{wang2024qwen2vl}). These strategies enhance fine-grained visual understanding in models. However, higher resolutions drastically increase token count, imposing significant computational burdens. For example, Qwen2-VL processes 11,427 visual tokens for an image with a resolution of $8204 \times 1092$ pixels. This results in considerable computational overhead during both the training and inference phases, making high-resolution processing resource-intensive and challenging to scale-up. 

% The surprising success of Large Language Models (LLMs) \cite{qwen2.5} has been witnessed by the other modalities community, extending LLMs to other modalities has emerged as a prominent research frontier. In the context of vision-language (VL) tasks, the most widely adopted framework is known as LLaVA, which directly projects the visual features(extracted by ViTs like CliP \cite{radford2021clip}) as the prefilling tokens of LLMs. Thus the optimization  is to align and merge visual and language tokens in the well-trained ViTs and LLMs. Despite effective, early models are typically constrained by a fixed image input size (e.g., $224 \times 224$), hindering the performance when processing non-square and high resolution text-oriented images, like LLaVA-1.5 did not perform well in DocVQA. To this end, many works aim to enhance the visual resolution of vision transformer backbone through fixed high-resolution (e.g., CogVLm2 \cite{hong2024cogvlm2}, GLM4V9B \cite{glm2024chatglm}), slicing patch schemes (e.g., LLaVA 1.6, InternVL series \cite{chen2024far}) or naive dynamic resolution (Qwen2-VL \cite{wang2024qwen2vl}). The high-resolution processing brings brilliant fine-grained visual understanding capabilities. However, it will introduce heavy prefilling tokens for the input of LLMs, For instance, Qwen2-VL will feed 11427 visual tokens into LLM for an image with a resolution of $8204 \times 1092$ pixels. and incur substantial computational overhead during both training and inference phases.

\begin{figure}[t]
\includegraphics[width=0.48\linewidth]{dataset3.pdf}
    \includegraphics[width=0.48\linewidth]{fastvscore.pdf}
    \caption {Comparison of scores between FastV and FCoT-VL on different types of benchmarks. FastV gets a significant decline in tasks that require high resolution like DocVQA and InfoVQA. In contrast, our method shows a minor performance degradation.}
    \label{fig:fastv}
\end{figure}

% To resolve above issues, compressing visual tokens has caught the attention in a training-free manner like ToMe, FastV and Tokenpacker. 
To resolve above issues, reducing visual tokens in well-trained VLLMs has been studied in works like LLaVA-PruMerge \cite{shang2024llava}, SparseVLM \cite{zhang2024sparsevlm} and VisionZip \cite{yang2024visionzip}. For instance, VisionZip\cite{yang2024visionzip} selects informative tokens using attention scores to reduce the total number of tokens. However, training-free token pruning methods, like FastV \cite{chen2025image} in Figure~\ref{fig:fastv}, shows sub-optimal performance in text-oriented tasks that demand high-fidelity token representations. To this end,
training from scratch with reduced visual tokens is another alternative. For example, TextHawk2 \cite{yu2024texthawk2} uses 100M pre-training image-text pairs to train cascaded decoder layers, progressively downsampling visual tokens by $4\times$ ratio. TextHawk2 requires significant data and resources, posing challenges in low-resource settings. 
% These challenges are further exacerbated in OCR, reasoning, document-heavy tasks, and text-oriented scenarios with limited image-text data (e.g., 1 million pairs).
This raises a challenge: \textbf{Can we compress the visual tokens effectively under constraints of limited training datas and GPUs resources?}

% Despite the training-free advantages of pruning techniques, they still fall short in tasks involving document parsing, chart understanding, and fined-grained QAs. T

% Beyond increasing resolution, recently advanced open-source models(Internvl2,Qwen2-VL) start to explore the impact of scaling laws of pre-training. As reported in Qwen2-VL, Qwen2-VL will consume about 600 billion image-text tokens at the first pre-training stage to purse a robust understanding of visual-textual correlations and alignments, which narrows the performance gap with leading commercial multimodal models (GPT-4V, Claude 3.5). However, they incur substantial computational overhead during both training and inference phases.

% In this case, a question rises, how to retain fine-grained visual information with fewer visual tokens in VLLMs? Actually, pruning visual tokens in the well-trained MLLMs have been studied in LLaVA-PruMerge, SparseVLM and VisionZip[XXX,add more]. For instance, XXXX.
% Despite the training-free advantages of pruning techniques, they still fall short in tasks involving document parsing, chart understanding, and fined-grained QAs. To breakthrough the upper performance of training-free paradigm, training from scratch with reduced visual tokens offers a compelling alternative. 


% For example, TextHawk2 consumes
% 100M pre-training image-text data mixture to train a cascaded decoder layers to do $4\times$ down sampling visual tokens progressively. Additionally, we find that training text-oriented VLLMs from scratch with fewer visual tokens suffers from poor convergence in OCR-like tasks, particularly with limited image-text pairs(i.e.,1 million). This raises a challenge: Can we compress the visual tokens effectively in the presence of limited training datas and GPUs?

For this challenge, we \textbf{F}ocus on \textbf{Co}mpression of visual tokens in high-resolution \textbf{T}ext-oriented Large \textbf{V}ision-\textbf{L}anguage Models (FCoT-VL) while retaining fine-grained image detailed perception. To be special, we 
propose a self-distilling framework as shown in Figure~\ref{fig:Overvieww}, comprising a teacher model with abundant visual tokens and a student model with compressed token representations.
% a lightweight connector is integrated into the student model to facilitate token compression. 
To build upon established capabilities, we adopt the InternVL2 to initialize the teacher model and student model. During the self-distillation process, only a lightweight token compression module and projector in the student model are learnable with a small-scale set of image-text pairs(i.e., 2M).
This approach brings two advantages: 1). The student inherits the parameter from the teacher, avoiding large-scale training and preserve advanced capabilities of the teacher model. (2) We exclusively finetunes the token compression module, which can achieve promising performance even with limited training data.

In practice, we find distilled student model has performance drops(about 5$\%$) inevitably.
To enhance the performance of the student model relative to the teacher model, we introduce a post-training stage using high-quality instruction datasets, involving documents, mathematics, science, charts, and GUI images. Besides, we propose a multi-stage model fusion technique that iteratively merges models to improve adaptability across various tasks. The post-training improves the model’s ability to handle complex tasks, such as document parsing and reasoning-based QAs. 

Our contributions can be concluded as follows:
\begin{itemize}
    \item [(1).]
    We propose a self-distilling paradigm towards visual token cimpressing for high-resolution text-oriented VLLMs, enabling robust re-alignment while minimizing both data and computational demands.
    \item [(2).]
    We explore post-training strategies including synthesis of high-quality supervised fin-tuning data and training-free model merging schemes, facilitating the capabilities of compressed VLLMs.
    \item [(3).]
    We develop the proposed FCoT-VL in the InternVL2 series, achieving compression ratios of 2 and 4, respectively. Extensive empirical evaluations across multiple text-oriented benchmarks reveal that our proposed models achieve comparable or superior performance to existing token-rich VLLMs, while offering higher training and deployment efficiency.
\end{itemize}



% XXXXX is a Self-distilling framework which use base model as teacher model to teach model with compressed vision tokens. The student model has only one more adapter than the teacher model and what we train is the adapter. We only need a small amount of pre-training data, such as 2M, to make an existing MLLM re-aligned rapidly with compressed visual tokens. Besides, we need not to design extra special adapters and training method, just common networks, like CNN, MLP and max pooling. We fine-tune the distilled model to obtain a document-specific model at the SOTA level, and can effectively improve the document performance of the base model and  guarantee some of the remaining performance of the original model (on the natural image side). Additionally, we conduct Multi-stage model merging strategy on MLLMs to improve performance which is unburdened for inference.


% Our key contributions are summarized as follows



% Actually, removing abundant visual tokens in the well-pretrained MLLMs 


% The pioneering works like LLaVA craft a native but effective text-centric paradigms, i.e., alignment method between well-trained vision models and LLMs. To be special, LLaVA uses a visual encoder(CLIP) to embeds input images as visual tokens, which are  concatenated with textual tokens and fed into vanilla LLMs. The Follow-up work LLavA springs up. During the early stage, MLLMs adherent the lower resolutions of ViT(like $224 \times 224$  pixels of CLIP), hindering further fine-grained scene understanding like detailed caption and OCRs.  Proprietary
% commercial models typically employ a dynamic resolution
% approach, preserving the original aspect ratio to facilitate
% detailed scene and document understanding.





% 早期的LVLM是采用较小的的分辨率，如224，这导致了性能受限，尤其无法很好处理信息密集场景，如文档， 信息图表，海报等。为此，基于高分辨率的LVLM就被提出，比如固定高分辨率（cogvlm2,glm4v9b）,切片方案（llava1.6,internvl1.5,textmonkey,InternLM-XComposer2-4KHD等）以及原生分辨率（qwen2vl）,这些模型极大的提升了模型性能，尤其是文档性能，接近了闭源模型（GPT4o,cluade3.5）。
% During the early stage, MLLMs adopted lower resolutions (like $224 \times 224$  pixels), which leads to limitations on performances, especially for domains with intensive information such as documents, infographics, and posters. 
% To alleviate the limitations, the direct and effective way is to increase the visual resolution of MLLMs.
% For example, fixed high resolution (cogvlm2, glm4v9b), slicing schemes (llava1.6, internvl1.5, textmonkey, InternLM-XComposer2-4KHD) and native resolution (qwen2vl).
% These models perform extremely robustly and are even equivalent to closed-source models (GPT4o,Cluade3.5).

% 但是，这些方法引入了很多的视觉token,比如对于qwen2vl来说，一个8204*1092分辨率图像，会产生11427个视觉token。这些长视觉token给多模态训练（对齐）带来了极大的计算和时间负担，比如internvl2就需要20m+数据对齐，XXX a800训练。此外，在推理阶段，过长的prefill visual token 很容易让模型超过最大的推理长度，对长回复和时延带来挑战。
% However, another problem that too many visual tokens are introduced into LLM decoder follwes by high-resolution schemes. For qwen2vl, a image of $8204 \times 1092 $ pixels  generates 11427 visual tokens. The long visual tokens impose a significant computational and resource burden on both MLLMs training and alignment. The InternVL2 cost more than 20M image-text pairs and 
%多少 
% hours on a800 to train. 
% %这句话感觉怪怪的，或许得再改一下
% Besides, LLMs will exceed the maximum length easily caused by  excessively long visual tokens in the inference phase,  with long responses and latency challenges ensuing. 



% 因此，业界有许多降低token的方案【xxxxxxxxxx大致介绍他们的做法】
%,但是任然有一些问题需要考虑：
% 1）这些方案都是基于LLava1.5的原生分辨率去做的，而原生分辨率下一些特定领域下的模型表现较差
% 2）这些方案都是基于自然图像测试集进行测试的，自然图像本身是视觉token冗余的，且自然图像问答对分辨率要求不高（                    InternLM-XComposer2-4KHD）。
% 在高分辨率下场景下的的降低视觉token工作是很少的，少量工作比如texthawk2,直接设计一个小型decoder网络去16倍下采样token。 他们使用了100M预训练数据重新训练，这无疑是代价巨大的。我们也发现，降低token后，重新对齐训练是困难的，对数据量要求很高。 如何快速，在训练数据有限的情况中，有效的降低视觉token并且维持性能不降低太多是值得研究的工作。

% Although there are some methods to compress tokens in the field, like LLaVA-PruMerge, SparseVLM and VisionZip, but serious consideration should be given to some problems: 1)These schemes are based on the native resolution of LLaVa-1.5, and some models perform poorly in certain fields under the native resolution, like text-rich benchmarks. 2) These schemes are tested on a natural image test set, where natural images themselves have visual token redundancy and do not require high-resolution question-answer pairs.
% Research on domain with high-resolution is relatively rare and the cost is enormous. 
% For example, Texthawk2 designs a small decoder network to do $16\times$ downsampling tokens and uses 100M pre-training data to re-train.
% Additionally, We find it's difficult to train a MLLM from scratch with compressed visual tokens by experiments in the intensive-information domain.
% So it's worthwhile to investigate how to reduce visual tokens quickly, efficiently and without too much performance degradation in the presence of limited training data.
% 为此，我们提出了在高分辨率方案下的一个有效的token压缩方案，1）只需要使用少量的预训练数据，比如2M就可以将一个现成的MLLM基础上，重新对齐，模型可保留OCR和多数文档问答能力；2）并且我们方案对于不需要设计额外的网络架构和训练方法（texthawk），只需要最简单的，比如卷积，MLP ,max pooling等常规模型，简单的对齐方式. 3)我们创新性提出蒸馏的方式，将基模型作为教师模型，教学降低token后的模型，我们仅仅微调adptor模块，可降低视觉2倍，4倍token。4）我们在蒸馏后的模型上微调，得到一个SOTA级别的文档专用模型，并且可有效提升基模型的文档性能，并能保证部分原模型其余性能（自然图像那边）。
% 此外，我们 尝试了多阶段模型融合策略。 提升了XXX性能，对于推理是无负担的。

% In this paper, we propose a effective and efficient method XXXXX for high-resolution scheme, which can reduce visual redundancy of MLLMs with less data and maintain performance. XXXXX is a Self-distilling framework which use base model as teacher model to teach model with compressed vision tokens. The student model has only one more adapter than the teacher model and what we train is the adapter. We only need a small amount of pre-training data, such as 2M, to make an existing MLLM re-aligned rapidly with compressed visual tokens. Besides, we need not to design extra special adapters and training method, just common networks, like CNN, MLP and max pooling. We fine-tune the distilled model to obtain a document-specific model at the SOTA level, and can effectively improve the document performance of the base model and  guarantee some of the remaining performance of the original model (on the natural image side). Additionally, we conduct Multi-stage model merging strategy on MLLMs to improve performance which is unburdened for inference.
% 为了验证我们的模型的有效性和高效性，我们在主流多模态的benchemark的text-rich测试集上测试了实验结果。实验表明XXXX在降低了视觉token数量的同时在text-richbenchmarks 上依旧有较好的性能。我们的贡献总结如下：
% 1）我们提出的XXXXXX可以使用很少的数据完成低视觉token下的模型的重新对齐，并且这个网络结构对adapter的适应性很强，在简单的压缩adapter上有很好的表现。
% 2）我们使用对重新对齐的模型进行了SFT调优，对比未压缩的原模型，我们得到了一个不输于其的一个低视觉token的SOTA模型，在高分辨率下，在text-rich benchmarks上有很好的表现。
% 3）我们在text-rich benchmark上证明了方案的有效性，并且对比了不同参数规模以及不同压缩率下的模型表现，例如在保留1/2的token时，我们在docvqa上多少，在。。。。。。。
% To validate the effectiveness and efficiency of XXXXX, we test majority of the benchmarks on intensive-information domain in mainstream multi-modal evaluation benchmarks. Experiments demonstrate XXXXX reduces visual tokens while improving model performance in intensive-information field. Our contributions can be concluded as follows:
% \begin{itemize}
%     \item [1)]
%     We propose XXXXXX, which can re-align models under low vision tokens with very little data and is highly adaptable to adapters, showing good performance on simple compressed adapters.
%     \item [2)]
%     We fine-tune SFT on a re-aligned model, and we get a SOTA  model with fewer visual tokens that is comparable to the original uncompressed model. It performs well in high-resolution text-rich benchmarks.
%     \item [3)]
%     We conduct Multi-stage model merging strategy on MLLMs and improves the performance of the final model without inference delay.
% \end{itemize}







    



