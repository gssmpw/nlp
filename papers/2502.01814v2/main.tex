
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}
\usepackage{subcaption}
\usepackage{enumitem}

\usepackage{graphicx} 
\usepackage{amsmath}
\usepackage{wrapfig}

%algorithm table package
% \usepackage{fullpage}
\usepackage{times}
\usepackage{fancyhdr,graphicx,amsmath,amssymb}
\usepackage[ruled,vlined,linesnumbered]{algorithm2e}
\usepackage{multirow}


% to compile a preprint version, e.g., for submission to arXiv, add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2022}


% To compile a camera-ready version, add the [final] option, e.g.:
% \usepackage[final]{neurips_2022}


% To avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2022}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{siunitx} % For aligning numbers
% \usepackage{geometry} % To manage page layout



\usepackage{amsthm}
\theoremstyle{plain}
\newtheorem{thm}{Theorem}[section] % reset theorem numbering for each chapter
\newtheorem{lem}[thm]{Lemma}
\newtheorem{co}[thm]{Corollary}

\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition} % definition numbers are dependent on theorem numbers
\newtheorem{exmp}[thm]{Example} % same for example numbers

\title{PolyhedronNet: Representation Learning for Polyhedra with Surface-attributed Graph}

% Authors must not appear in the submitted version. They should be hidden
% as long as the \iclrfinalcopy macro remains commented out below.
% Non-anonymous submissions will be rejected without review.
\author{{Dazhou Yu} \quad
    {Genpei Zhang} \quad
    {Liang Zhao}\\
    Department of Computer Science, Emory University
\\
\texttt{dyu62@emory.edu}, \texttt{genpeizhang2024@gmail.com}, \texttt{liang.zhao@emory.edu}
}

% \author{Antiquus S.~Hippocampus, Natalia Cerebro \& Amelie P. Amygdale \thanks{ Use footnote for providing further information
% about author (webpage, alternative address)---\emph{not} for acknowledging} \\
% Department of Computer Science\\
% Cranberry-Lemon University\\
% Pittsburgh, PA 15213, USA \\
% \texttt{\{hippo,brain,jen\}@cs.cranberry-lemon.edu} \\
% \And
% Ji Q. Ren \& Yevgeny LeNet \\
% Department of Computational Neuroscience \\
% University of the Witwatersrand \\
% Joburg, South Africa \\
% \texttt{\{robot,net\}@wits.ac.za} \\
% \AND
% Coauthor \\
% Affiliation \\
% Address \\
% \texttt{email}
% }

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
Ubiquitous geometric objects can be precisely and efficiently represented as polyhedra. The transformation of a polyhedron into a vector, known as polyhedra representation learning, is crucial for manipulating these shapes with mathematical and statistical tools for tasks like classification, clustering, and generation. Recent years have witnessed significant strides in this domain, yet most efforts focus on the vertex sequence of a polyhedron, neglecting the complex surface modeling crucial in real-world polyhedral objects.
This study proposes \textbf{PolyhedronNet}, a general framework tailored for learning representations of 3D polyhedral objects.  We propose the concept of the surface-attributed graph to seamlessly model the vertices, edges, faces, and their geometric interrelationships within a polyhedron. 
To effectively learn the representation of the entire surface-attributed graph, we first propose to break it down into local rigid representations to effectively learn each local region's relative positions against the remaining regions without geometric information loss. Subsequently, we propose PolyhedronGNN to hierarchically aggregate the local rigid representation via intra-face and inter-face geometric message passing modules, to obtain a global representation that minimizes information loss while maintaining rotation and translation invariance.
Our experimental evaluations on four distinct datasets, encompassing both classification and retrieval tasks, substantiate PolyhedronNet's efficacy in capturing comprehensive and informative representations of 3D polyhedral objects.  Code and data are available at \href{https://github.com/dyu62/3D_polyhedron}{$github.com/dyu62/3D\_polyhedron$}.
\end{abstract}


\section{Introduction}
% Representing 3D objects can vary greatly in terms of accuracy and complexity. For basic classification tasks, simple raw data formats such as images or point clouds may suffice, providing adequate accuracy. However, these data types, being large and coarse, are unsuitable for long-term storage and precise operations. In contrast, 
In mathematics and computational geometry \cite{yuspatial}, a polyhedron is defined as a three-dimensional (3D) solid formed by flat polygon faces joined at edges and vertices. Ubiquitous geometric shapes can be precisely and efficiently modeled as polyhedra, ranging from basic 3D shapes (e.g., cubic, pyramid, and truncated tetrahedron) to compositions of them (e.g., shapes of buildings, furniture, and digital objects in CAD) as exemplified Figure \ref{fig:intro} (a). 
% Polyhedra serve as a precise and efficient method for modeling and manipulating 3D objects by directly recording the coordinates of vertices for each polygon face. 
% Commonly observed in the form of cubic and prismatic structures, polyhedra are prevalent in the architectural designs of skyscrapers and office buildings. Architects often employ faceted facades, which use angled planes to add visual interest by breaking up flat surfaces. Additionally, computers conceptualize even the most intricate 3D objects as polyhedra, undertaking tasks such as identifying and recognizing specific types like Platonic solids and prisms, detecting symmetries, generating novel polyhedra from random noise, and optimizing polyhedra for specific properties or constraints. These tasks underscore the importance of the mathematical representation of polyhedra.
In the real world, there are many tasks surrounding polyhedra such as classification (e.g., convex or concave); clustering polyhedra into different types (e.g., Platonic solids and prisms ); as well as generation and optimization (e.g., use faceted facades to break up flat surfaces) of polyhedra for design needs. However, the raw form of polyhedra cannot be directly input into machine learning models which require structured formats such as vectors, tensors, etc. Hence, a fundamental upstream task is to map a polyhedron into a vector representation, namely polyhedra representation learning, which is the focus of this paper.
% Representation learning of polyhedra focuses on capturing and encoding the intrinsic attributes of these shapes, which is essential for understanding and .interaction with our physical environment. This work aims to advance the field of representation learning, specifically tailored for polyhedral formats.
% % This capability proves especially valuable in fields like urban planning \citep{zhong2020open, spangler2023calculating, mu2022computation}, shape coding \citep{li2020approximating, yan2021graph, zorzi2022polyworld}, and building pattern recognition \citep{he2018recognition, yan2019graph, bei2019spatial}. , cartographic generalization \cite{feng2019learning, wang2023hexagon, ma2020polysimp}, and geographic question answering (GeoQA) \cite{fabbri2020template, scheider2021geo,kefalidis2023benchmarking}


Recent studies on polyhedral geometries can be broadly classified into two categories. The first category involves feature engineering on the faces of a polyhedron to generate descriptors for each face and aggregate these features \citep{qi2017graph, shi2020pointgnn, wang2019dynamic}. However, this manual selection of features is limited and biased by human knowledge, which can result in the loss of geometric information at the initial stage and often lacks generalizability to other tasks. The second category models the shapes of polyhedral faces directly using sequences of coordinates, preserving the original geometric information and learning features from the data, which can be generalized across various tasks \citep{mai2023towards,van2019deep,yan2021graph}. Nevertheless, these methods are constrained by the need for a specific order of input and do not consider the relationship among faces. Directly using coordinates also fails to account for rotation and translation invariance, thus limiting the ability to consistently interpret polygonal geometries regardless of their spatial orientation or position. Moreover, such approaches neglect face properties, which contain significant semantic information,  by focusing solely on the shapes of polygonal faces.  
Figure \ref{fig:intro} (b) illustrates how face attributes introduce semantic information that influences the appearances and functionalities of geometric objects. Although they share the same underlying polyhedral structure, the three objects are distinctly different. The first polyhedron is the Louvre Pyramid, which is characterized by four glass faces, with a concrete ground face. The middle one is a wireframe pyramid with empty faces, emphasizing the geometric structure and suggesting its use as a craft or model. The last one is an Egypt pyramid, featuring yellow stone faces.
% These examples underscore the importance of incorporating face learning into generalizable representations of polyhedral objects.
% \caption{Illustrations of three objects with the same polyhedron structure for (a) An Egypt pyramid, (b) A wireframe pyramid model, and (c) The Louvre Pyramid. Despite their varied appearances and contexts, these objects share the same underlying polyhedral geometry. However, differences in their face attributes, such as texture, material, and color, contribute to their distinct characteristics and functionalities. This highlights the importance of learning generalizable representations of polyhedral shapes that capture both geometric structure and face attributes.}

% \begin{wrapfigure}{r}{0.7\textwidth} % The 'r' places it on the right, and '0.5\textwidth' sets the width to half of the text area
%   \centering
%   \includegraphics[width=0.69\textwidth]{fig/intro.png} % Slightly less than the wrapfigure width to ensure it fits nicely
%   \caption{3D objects modeled as polyhedra.}
%   \label{fig:intro}
% \end{wrapfigure}

\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{fig/intro.png}
  \caption{3D objects modeled as polyhedra.}
  \label{fig:intro}
\end{figure*}

To address these limitations, we propose \textbf{PolyhedronNet}, a novel framework for polyhedra representation learning. 
% We introduce several contributions:
Firstly, we propose the \textbf{Surface-Attributed Graph (SAG)} to concisely encapsulate the information of a polyhedron.
% we transform it into a graph representation. While graphs adeptly represent shapes through nodes and edges, traditional graphs fall short as they do not explicitly model faces. 
Beyond simple graphs, SAG utilizes face-hyperedges to model the geometric relationships among vertices, edges, and faces and explicitly capture the face semantics, ensuring no information is lost.
Thus, learning the representation of a polyhedron is equivalent to learning SAG representation. We solve this problem by first decomposing the SAG using the \textbf{Local Rigid Representation} of SAG and then aggregating them to SAG's global representation.  In each local rigid representation, to preserve the current local region's geometric relation to the whole SAG, we calculate the second-order distances around a node and angles formed by its neighbors and associated faces to form a rigid body around the node. The set of local rigid bodies encapsulates complete geometric and semantic information in the SAG and provides rotation and translation invariance. 
% Secondly, we introduce a \textbf{Local Rigid Representation} to learn a global representation that preserves the geometric information of polyhedra with rotation and translation invariance. This approach is proven to encapsulate the complete spatial and semantic information present in the graph. 
Thirdly, we propose \textbf{PolyhedronGNN} to hierarchically aggregate the local rigid representation into a global representation that minimizes information loss while maintaining rotation and translation invariance of global representation. Considering faces are the pivots of a polyhedron, this model learns geometric information inside faces and across faces, based on the two-hop paths that suffice the preservation of local rigid information. This design adeptly captures the semantic heterogeneity of the surface-attributed graph, significantly enhancing the model’s ability to uniquely identify and differentiate diverse input graphs.
Moreover, we empirically validate our proposed method across four datasets and demonstrate its effectiveness in both classification and retrieval tasks, significantly outperforming state-of-the-art approaches by a substantial margin.


\section{Related work}
\subsection{3D Object Representation Learning}
Traditional methods render a three-dimensional object into two dimensions as an image or a set of images with different views \citep{qi2016volumetric, su2015multiview}. These methods involve significant information loss and cannot truly represent 3D objects. Some recent works \citep{qi2017pointnet, 8579057} utilize spatial point cloud to depict objects. PointNet \citep{qi2017pointnet} introduced a deep learning framework for directly processing point clouds, significantly advancing object classification and segmentation tasks. This was further expanded by \citet{8579057} through PointGrid, which combines point clouds with voxel grids to enhance geometric understanding. Voxel grid representation offers a volumetric approach to 3D shape analysis. \citet{chen2023polygnn} develop PolyGNN to reconstruct 3D building models using polyhedral decomposition from point cloud. \citet{wu20153dshapenets} developed 3D ShapeNets, a method that leverages convolutional neural networks on voxel grids to perform 3D shape recognition, providing a robust framework for capturing complex shapes. \citet{wang2017ocnn} introduced the Octree-based CNN, which improves efficiency by using octree structures for adaptive resolution in 3D space. These discrete methods fail to leverage the structural information like edges inherently by points or grids, making them less compatible with structured data. 
Mesh representation focuses on using triangles or quads to model 3D objects. \citet{bruna2013spectral} proposed spectral networks to operate on meshes. \citet{henaff2015deep} extended this concept by introducing convolutional networks for structured data, enhancing the analysis of mesh topology. Further advancements by \citet{defferrard2016convolutional} and \citet{monti2017geometric} applied localized filtering and mixture model CNNs to learn geometric features on meshes. \citet{Pang_2023} proposes a GNN-based approach to learn geodesic embeddings for polyhedral faces.  While these methods have significantly advanced the processing of 3D object, they face limitations due to their computational intensity with high-resolution models and their struggles with irregular geometries, inherent to the mesh format. Directly modeling objects with polyhedra is a promising method to address these issues.

\subsection{Polyhedral Representation Learning}
Recent advancements in the field of polyhedral geometry representation learning have been significant. Traditional feature engineering approaches \citep{pham2010fast, yan2019graph, he2018recognition}  transform polygonal shapes into predefined shape descriptors. GNNs are utilized to improve the handling of spatial relationships and structural complexities \citep{qi2017graph, shi2020pointgnn, wang2019dynamic}. However, these descriptors oversimplify the data, failing to capture the complete spectrum of shape information.  Polygon shape encoding methods \citep{van2019deep, mai2023towards, yan2021graph}, have demonstrated their effectiveness in shape classification and retrieval tasks. While beneficial for certain types of analysis, these methods do not fully meet the needs of polyhedral representation learning that requires capturing complex topological relationships between polygonal geometries. PolygonGNN \cite{yu2024polygongnn} is the first GNN-based method that captures both multi-polygon relationships and individual polygon shape information. However, it is designed specifically for 2D shapes. In relation to polyline representation learning \citep{jiang2021weakly, jiang2022weakly}, current works focus on processing continuous lines and curves that delineate the boundaries and configurations of shapes in spatial data.  Another category of research focuses on polyhedron generation. \citet{gillsjo2023polygon} extracts polygons from images by using heterogeneous graphs and wireframes to learn feature space. \citet{zorzi2023re} utilizes edge-aware GNNs to enhance polygon detection accuracy and applicability in scene parsing by considering both node and edge features. 
% Recently, in \citet{jones2022selfsupervised}, unsupervised techniques within encoder-decoder architectures have been utilized to extract geometric and topological data from unlabeled CAD sources, enabling automated interpretation of complex B-Rep faces without predefined labels. Diffusion model-based approaches have also been employed to manipulate and learn polyhedral structures in latent spaces \citep{koo2024salad, cheng2023sdfusion}. Additionally, \citet{zhang20233dshape2vecset} using radial basis functions and transformers to encode 3D shapes into neural field vectors provide a versatile and efficient approach to processing polyhedral data.

% \subsection{Graph Neural Network for Polyhedron}
% Recent studies have shown that Graph Neural Networks (GNNs) are highly effective in handling polyhedron, as they process various forms of polyhedral structures including point clouds, polygons, and directly managing polyhedra. For GNNs to process point clouds \cite{qi2017graph, shi2020pointgnn, wang2019dynamic}, they leverage the graph-based representation of point clouds to improve handling of spatial relationships and structural complexities inherent in 3D environments. 
% When it comes to polygons, \citet{antonietti2024agglomeration} enhance the analysis of geometric structures by maintaining mesh quality and improving computational processes, as demonstrated in multigrid solvers and scene parsing tasks. \citet{zorzi2023re} utilizes edge-aware GNNs to enhance polygon detection accuracy and applicability in scene parsing by considering both vertex and edge features.  \citet{gillsjo2023polygon} optimize room layout estimation by using heterogeneous graphs and wireframes, benefiting urban planning and interior design applications. For polyhedron, recent research highlights several advanced methods leveraging GNNs. \citet{Pang_2023} proposes a GNN-based approach to learn geodesic embeddings for polyhedral faces, significantly enhancing shape matching and segmentation tasks. \citet{allen2022learning} introduce a method to model face-to-face interactions within polyhedra, leading to more accurate rigid body dynamics simulations and superior performance on real-world data.  \citet{chen2023polygnn} develop PolyGNN to reconstruct 3D building models using polyhedral decomposition, achieving compact and semantically rich building reconstructions.




\section{Problem Formalization}\label{sec:problem form}
In this section we first introduce the formal definitions of polygons \citep{mai2023towards} and polyhedra \citep{weisstein_polyhedron}, and then formalize the problem of polyhedra representation learning.
% \begin{defn} \label{def1} Polygon. A polygon $p$ is defined as a tuple $p = (V, A)$, where $V = (v_1, v_2, \ldots, v_{N_{b}})$ representing the exterior boundary of $p$, where $v_i \in \mathbb{R}^3$ denotes the 3D coordinates of the $i$-th vertex, $N_{b}$ denotes the number of vertices. The vertices are coplanar, i.e., they lie in a single 2D plane embedded in the 3D space. $A$ is a set of attributes associated with the polygon, such as color, texture, or semantic labels. The polygon is assumed to be simple, i.e., it does not have any self-intersections or holes.

\begin{defn}[Polygon] \label{def1} 
A polygon $p_i$ is defined as an ordered sequence of vertices that form a closed shape: $p_i = (v_{i,1}, v_{i,2}, \ldots, v_{i,N_{b,i}})$, where $v_{i,j} \in \mathbb{R}^3$ denotes the 3D coordinates of the $j$-th vertex, $N_{b,i}$ denotes the number of vertices. The vertices of the polygon are coplanar, meaning they all lie within a single 2D plane that is embedded in 3D space. Additionally, the polygon is assumed to be simple, which implies that it does not have any self-intersections or holes.
% Altering the starting vertex in $p$ does not change the structure of the polygon.
% Additionally, a polygon remains unchanged under transformations such as translation and rotation applied to its coordinates.
\end{defn}

\begin{defn}[Polyhedron]
A polyhedron $q$ is a 3D solid that consists of a collection of polygonal faces $q=\{p_i\}_{i=1}^{N_f}$, where each face $p_i$ is a polygon as defined in Definition \ref{def1}. The vertices of each face are ordered in a counterclockwise direction when viewed from outside the polyhedron, ensuring a consistent orientation across all faces. The normal vector associated with each face $p_i$ can be obtained using the right-hand rule, pointing outward from the polyhedron. 
In addition to the geometric properties, each face $p_i$ may have semantic face attributes, which can include material, color or other application-specific data.
\end{defn}
This definition provides a unified data structure for both 2D polygons and 3D polyhedra. A polygon can be treated as a special case of a polyhedron with a single face. By defining the faces as oriented polygons, our representation implicitly captures the orientation and enclosure properties of the polyhedron.

\textbf{Polyhedra representation learning}. 
This paper aims to convert a polyhedron into a vector representation, denoted as $q \rightarrow q_v$, where $q_v \in R^d$ and $d$ represents the dimension of the vector. 
As depicted in Figure \ref{fig:intro}, face attributes collectively  identify object patterns, which is fundamental to understanding the concept of a polyhedron. 
The learned representation $q_v$ should capture the geometric and semantic properties of the polyhedron, while being invariant to rotation and translation transformations. Furthermore, the representation should be discriminative, enabling accurate classification, retrieval, and other downstream tasks on 3D shapes.
% Representation learning of polyhedron is an underexplored yet highly challenging problem due to the key technical hurdles:
% \begin{enumerate}
% \item Defining a unified representation for polyhedra that captures both geometries and face attributes.
% \item Ensuring that the learned representations capture the global information and are invariant to rigid transformations.
% \item Developing a learning framework that can effectively process and learn from the polyhedron representation.
% \end{enumerate}
% In the following sections, we will describe our proposed methodology, which addresses these challenges and aims to learn powerful and discriminative representations for polyhedra.

\section{Methodology}
\begin{figure*}[htb]
  \centering
  \includegraphics[width=\textwidth]{fig/arch.png}
  \caption{Illustration of the proposed framework.}
  \label{fig:arch}
\end{figure*}
To learn distinct representations for polyhedra by addressing the aforementioned challenges, we propose the PolyhedronNet framework, as shown in Figure \ref{fig:arch}. In Figure \ref{fig:arch} (a), to unify the characterization of vertices, edges, faces, and their relationships in a polyhedron, we propose a transformation that turns a polyhedron into a surface-attributed graph (SAG), as elaborated in Section \ref{sec:graph build}. This process is proven to be invertible, which maintains information in the polyhedron while converting it to a graph data format. In Figure \ref{fig:arch} (b), to learn a representation of the SAG, we decompose SAG into a set of local rigids for each 2-hop path within a polyhedron (Section \ref{sec:spatial rep}) with our local rigid representation. The representation is a five-tuple set that transforms absolute coordinates into vectors while preserving the original graph information and achieving rotation and translation invariance. In Figure \ref{fig:arch} (c), we propose a novel graph neural network, PolyhedronGNN (Section \ref{sec:hgnn}), to aggregate the local rigid representations into the final SAG representation.
\subsection{Transforming Polyhedron to Surface-attributed Graph}\label{sec:graph build}
Graphs provide a natural way to capture the geometric structure of a polygonal shape by representing vertices as nodes and edges as links between them. In recent years, graphs have been successfully applied for polygon-related tasks \citep{zhou2023move,zorzi2022polyworld,zorzi2023re}.
% such as building simplification \citep{zhou2023move} and polygonal scene parsing \citep{zorzi2022polyworld,zorzi2023re}. 
These studies have demonstrated the effectiveness of using graphs to capture the intricate geometric and topological properties of polygonal shapes.  Given that a polyhedron can be considered as a 3D extension of a polygon, it is natural to extend the graph representation to the polyhedron domain. We let each graph node represent a vertex of a polyhedron and each directed graph edge represent an edge of a face in the polyhedron. 
% It is important to note that the faces of a polyhedron are connected, and a shared edge between two faces are represented as two graph edges with opposing directions.


However, A polyhedron is characterized not only by the vertices and edges but also by the faces. Developing a comprehensive representation of polyhedra necessitates a unified data structure capable of encapsulating all the geometric information. While vertices and edges are naturally contained in a graph structure, we propose the concept of a surface-attributed graph to include the face attributes, specifically tailored for polyhedron contexts. This representation extends the traditional graph-based approach used in polygon representation by incorporating face-hyperedges in the graph, which encapsulate the geometric properties of a polyhedron's faces. 

\begin{defn}[Surface-attributed graph]
A surface-attributed graph ${G} = (V, E, F, a)$ is a directed graph, where $V$ is the set of nodes, $E$ is the set of edges, and surface $F$ is the set of face-hyperedges. Each node $v_i= (x_i,y_i,z_i) \in V$ corresponds to a vertex of the polyhedron and is defined by its coordinates, $x_i,y_i,z_i$ are the values of coordinates. Each directed edge $e_{i,j} = (v_i, v_j) \in E$ represents an edge of a face in the polyhedron.
Each face-hyperedge $f=(e_{1,2},e_{2,3},...,e_{N_{b,i},1}) \in F$ is a an ordered set of edges that forms a closed shape, associated with a set of face attributes $a(f)$. It is important to note that, unlike traditional hyperedges in a graph, which simply connect multiple nodes, face-hyperedges contain the connectivity order information of edges, which captures the hierarchical topology of a polyhedron.
\end{defn}


\textbf{Constructing SAG from a polyhedron}. Based on the discussion so far, we summarize the steps for constructing the SAG from a given polyhedron $q$ as follows: We treat the vertex set in the original polyhedron as the node set $V$ of SAG. Then for a face $p_i$ in the polyhedron $q$, consider each pair of consecutive vertices $(v_j, v_{j+1})$ as the endpoints of an edge $e_{j,j+1} = (v_j, v_{j+1})$. Doing so for $j=1,...,N_{b,i}-1$, and adding an edge between the last and first vertices of $p_i$ to ensure a closed boundary, we will have all the edges of this face. Hence a face-hyperedge is formed as: $f=(e_{1,2},e_{2,3},...,e_{N_{b,i},1})$. Doing this for all faces, we have $F$. we build a mapping $a$ from each $f$ to its attributes. Union all the edges generated from all the faces to form $E$.

% \begin{enumerate}
% \item For each face $p_i$ in the polyhedron $q$, add each vertex $v_i$ to the node set $V$ of the graph ${G}$.
% \item For each pair of consecutive vertices $(v_j, v_{j+1})$ in $p_i$, add an edge $e_{j,j+1} = (v_j, v_{j+1})$ to the edge set $E$ of the graph ${G}$. Additionally, add an edge between the last and first vertices of $p_i$ to ensure a closed boundary.
% \item For each face $p_i$, add it as a face-hyperedge to set $F$ of the graph ${G}$, and associate the face attributes $a_i$ with it.
% \end{enumerate}

By incorporating face-hyperedges, SAG provides a comprehensive representation of a polyhedron that captures all its vertex-level, edge-level and face-level properties. 
It is important to highlight that SAG inherently captures the adjacency information between faces. From this structural representation, two key observations can be made: 1) If two faces $f_i$ and $f_j$ are adjacent in the polyhedron, their adjacent edges share the same nodes but in opposite directions, such that $\exists e_{o,r} \in f_i, e_{r,o} \in f_j$. 2) Each edge in a face must have a corresponding opposite edge, which belongs to another face.  $\forall e_{o,r} \in f_i, \exists e_{r,o} \in f_j, i \neq j$.

% 3. face vs node: two face only share a node <how to use SAG language to express the telling of the above fact>
% 4. edge vs edge: whether two edges belong to the same face. <how to use SAG language to express the telling of the above fact>



\begin{lem}\label{lem:1}
Let $q=\{p_i\}_{i=1}^{N_f}$ be a polyhedron and ${G} = (V, E, F, a)$ be the SAG derived from ${q}$. The transformation from ${q}$ to ${G}$ is invertible.
\end{lem}
\begin{proof}
    The detailed proof is in Appendix \ref{appendix:lem1}.
\end{proof}



\subsection{Local Rigid Representation of SAG} \label{sec:spatial rep}
The geometric information in a SAG is encapsulated by the relative positions of nodes and the specific shape of each face, which are defined by the node coordinates and connection topology. 
So attaining a representation for the whole SAG requires the above local information, namely local rigid representation, to be preserved (as elaborated in this subsection) and then be aggregated with minimal information loss (as detailed in Section \ref{sec:hgnn}).
% Traditional message passing techniques in graph neural networks often struggle with fully capturing the global geometric structure of a graph because each node's information is processed locally within its immediate neighborhood.
% To address this challenge, our approach must satisfy two key criteria: (1) each node must preserve geometric information in relation to all other nodes, and (2) aggregating this information across all nodes should minimize information loss. Fulfilling both conditions ensures a complete and accurate representation of the graph’s global structure.

To achieve local rigid representation, relying solely on node coordinates is insufficient, as this does not preserve essential symmetries such as translation and rotation invariance. Moreover, calculating the distances to all other nodes is computationally expensive and overlook crucial topological features such as edges and faces.
Tackling this issue motivates us to seek to encode the relative position of a node through its local rigid, including its neighbor nodes, edges, and faces. Hence, we propose a novel five-tuple geometric representation that maintains the relative positioning of nodes within the graph while also respecting the integrity of its edges and faces.  We transform the absolute coordinates of a node into a vector, and once all other nodes are fixed, the position of the target node is determined by its representation. 
\begin{defn}[Two-hop Path]
For a node $v_i$ in a SAG, a two-hop path $\pi_{i,j,k}$ is an ordered sequence of three nodes $(v_i, v_j, v_k)$ where $v_j$ is adjacent to both $v_i$ and $v_k$. We denote the set of all two-hop paths converging to node $v_i$ as $\Pi^{i}_{2}$.
\end{defn}

% \begin{defn}[Local Rigid Representation]
% The local rigid representation of a two-hop path $\pi_{i,j,k}$ is a five-tuple $s(\pi_{i,j,k})$ defined as:
% \begin{equation}
% s(\pi_{i,j,k})=(d_{i,j}, d_{j,k}, \theta_{i,j,k}, \phi_{i,j,k}, \psi_{i,j,k})
% \end{equation}
% \end{defn}
% Specifically, consider $\Pi^{i}_{2}$, the set of all \textit{two-hop paths} converging to node $v_{i}$. In this context, $\pi_{i,j,k} \in \Pi^{i}_{2}$ indicates a \textit{two-hop path} $v_{i}\leftarrow v_{j}\leftarrow v_{k}$.  
\begin{defn}[Local Rigid Representation of SAG]
The SAG can be expressed as a collection of Local Rigid Representation tuples $s(\pi_{i,j,k})$ as shown in Figure \ref{fig:arch}:
\begin{align}
    &G=\{s(\pi_{i,j,k})|\pi_{i,j,k} \in \Pi^{i}_{2}, v_i \in V\}, \label{eq:path3} \\
    &s(\pi_{i,j,k})=(d_{i,j},d_{j,k},\theta_{i,j,k}, \phi_{i,j,k}, \psi_{i,j,k} ) \nonumber
\end{align}
where $d_{i,j}$ is the Euclidean distance between node $v_i$ and $v_j$, $d_{j,k}$ is the distance between node $v_j$ and $v_k$, $\theta_{i,j,k} \in [-\pi, \pi)$ is the angle at $v_j$ formed by the three nodes. $\phi_{i, j, k} \in [-\pi, \pi)$ is the dihedral angle between the two faces containing edge $e_{i,j}$ and $e_{j,k}$ respectively, $\psi_{i, j, k}$ denotes the indices of the face-hyperedge containing $e_{i,j}$ and $e_{j,k}$.
\end{defn}
Importantly, the representation is invariant under rotation and translation transformations, ensuring that the structural integrity of the graph is maintained regardless of its orientation or position.
We further affirm that this representation encapsulates all information of the graph.  So by incorporating the local rigid representation of each node, the network would be able to capture the global information of the whole graph as the layer number grows. In essence, utilizing the local rigid representation of the SAG, as detailed in Equation \ref{eq:path3}, enables us to reconstruct a graph that is equivalent to the original. 

\begin{thm}\label{lem-thm} 
Given the local rigid representation of a surface-attributed graph $G$, as articulated in Equation \ref{eq:path3}, one can reconstruct a graph that is equivalent to $G$.
\end{thm}
\begin{proof}
     The foundational concept of Theorem \ref{lem-thm} is that faces within a polyhedron are interconnected via shared edges. We first prove that starting from a random node, one can recover the shape of a face it associated with. Then one can iteratively combine the faces to reconstruct an equivalent SAG. The detailed proof is in Appendix \ref{appendix:lem-thm}.
\end{proof}

% \subsection{Local Rigid Representation of SAG} \label{sec:spatial rep}
% We begin by formally defining the key concepts needed for our local rigid representation of Surface-Attributed Graphs (SAG).

% where:
% \begin{itemize}
%     \item $d_{i,j}$ is the Euclidean distance between nodes $v_i$ and $v_j$
%     \item $d_{j,k}$ is the Euclidean distance between nodes $v_j$ and $v_k$
%     \item $\theta_{i,j,k} \in [-\pi, \pi)$ is the angle at $v_j$ formed by vectors $\overrightarrow{v_jv_i}$ and $\overrightarrow{v_jv_k}$
%     \item $\phi_{i,j,k} \in [-\pi, \pi)$ is the dihedral angle between the faces containing edges $e_{i,j}$ and $e_{j,k}$ respectively
%     \item $\psi_{i,j,k}$ denotes the indices of the face-hyperedge containing edges $e_{i,j}$ and $e_{j,k}$
% \end{itemize}
% \end{defn}

% The geometric information in a SAG is encapsulated by the relative positions of nodes and the specific shape of each face, which are defined by the node coordinates and connection topology. To achieve a complete representation of the SAG, we must preserve this local information and then aggregate it with minimal information loss (as detailed in Section \ref{sec:hgnn}).

% Traditional approaches relying solely on node coordinates are insufficient, as they neither preserve essential symmetries (translation and rotation invariance) nor capture the topological features of edges and faces efficiently. Our local rigid representation addresses these limitations by encoding the relative position of a node through its local neighborhood structure, including adjacent nodes, edges, and faces.

% Formally, we can express the entire SAG as a collection of local rigid representations:
% \begin{equation}\label{eq:path3}
% G=\{s(\pi_{i,j,k})|\pi_{i,j,k} \in \Pi^{i}_{2}, v_i \in V\}
% \end{equation}

% This representation is invariant under rotation and translation transformations, ensuring that the structural integrity of the graph is maintained regardless of its orientation or position. Furthermore, as we prove in Theorem \ref{lem-thm}, this representation completely characterizes the graph's structure.



\subsection{PolyhedronGNN Architecture}\label{sec:hgnn}
After obtaining the local rigid representations in the previous section, in this section, the second step of our approach solves the problem of aggregating them to obtain a global representation.
Specifically, we propose PolyhedronGNN, which operates on the surface-attributed graph $\mathcal{G} = (V, E, F, a)$ and learns to aggregate information from neighboring nodes and faces with a focus on utilizing different models to learn the different interactions in SAG.

In each layer, we utilize the local rigid representation and face attributes to guide the node embedding updating process.
As shown in Figure \ref{fig:arch} (c), considering a two-hop path $\pi_{i,j,k}$, the consisting edges can be within the same face or  different faces. The flow of information from one face to another is critical in learning the interrelation between faces, while intra-face flow enhances the understanding of shapes of a single face. We divide possible path types into two categories: $\psi(\pi_{i, j, k}) \in \{R_{inner}, R_{cross}\}$.
To distinguish between different paths, we propose a heterogeneous function for learning the message based on the path type. Let $\Psi^{(l, \psi(\pi_{i, j, k}))}$ be a multi-layer perceptron (MLP) model for path type $\psi(\pi_{i, j, k})$ at layer $l$, the learned message $m^{(l)}(\pi_{i, j, k})$ from path $\pi_{i,j,k}$ can be formulated as follows:
\begin{equation}
m^{(l)}(\pi_{i, j, k})=w^{(\psi(\pi_{i, j, k}))}\Psi^{(l, \psi(\pi_{i, j, k}))} (h_i^{(l)}, h_j^{(l)}, h_k^{(l)}, g^{(l)}),
\end{equation}
where $w^{(\psi(\pi_{i, j, k}))}$ is the weight for path type $\psi(\pi_{i, j, k})$, $g^{(l)}=\varphi^{(l)}(d_{i, j} \Vert d_{j, k} \Vert  \theta_{i, j, k} \Vert \phi_{i,j,k} \Vert a_{j,i} \Vert a_{k,j})$ is the guiding embedding calculated by an MLP function $\varphi^{(l)}$, where $\Vert$ denotes the concatenation operation, $a_{j,i},a_{k,j}$ are the face attributes of the faces containing $e_{j,i},e_{k,j}$, respectively.
We initialize node embeddings to zeroes. For a node $v_i$, let $h_i^{(l+1)}$ represent its updated embedding in the $l$-th layer. The node embedding update is formulated as follows:
\begin{equation}
    h_i^{(l+1)}=  \sum \{ m^{(l)}(\pi_{i, j, k})| \pi_{i,j,k} \in \Pi_2^i\},
\end{equation}

% The graph pooling module aggregates the learned face and node embeddings into a global representation for the entire polyhedron. We employ a hierarchical pooling approach that progressively coarsens the surface-attributed graph by merging neighboring faces and vertices.
% At each pooling level $l$, we compute a score $s_i$ for each face $p_i$ based on its embedding $\mathbf{h}'_i$:
% \begin{equation}
% s_i = \text{sigmoid}(\text{MLP}(\mathbf{h}'_i))
% \end{equation}
% We then select the top-k faces with the highest scores as the pooling centers and merge their neighboring faces and vertices to form a coarser surface-attributed graph $\mathcal{G}^{l+1}$. The embeddings of the merged faces and vertices are computed by max-pooling the embeddings of their constituents:
% \begin{equation}
% \mathbf{h}_i^{l+1} = \max\left(\mathbf{h}_j'^{l} \mid p_j^l \in \mathcal{M}(p_i^{l+1})\right)
% \end{equation}
% \begin{equation}
% \mathbf{v}_i^{l+1} = \max\left(\mathbf{v}_j'^{l} \mid v_j^l \in \mathcal{M}(v_i^{l+1})\right)
% \end{equation}
% where $\mathcal{M}(p_i^{l+1})$ and $\mathcal{M}(v_i^{l+1})$ denote the sets of faces and vertices merged into the face $p_i^{l+1}$ and vertex $v_i^{l+1}$ at level $l+1$, respectively.
% The pooling process is repeated for a fixed number of levels or until a desired graph size is reached. The final polyhedron embedding is obtained by concatenating the face and vertex embeddings from all levels:
% \begin{equation}
% \mathbf{z} = \left[\mathbf{h}_i^l \mid l = 1, \ldots, L, i = 1, \ldots, |F^l|\right] \parallel \left[\mathbf{v}_i^l \mid l = 1, \ldots, L, i = 1, \ldots, |V^l|\right]
% \end{equation}
% where $L$ is the total number of pooling levels, and $|F^l|$ and $|V^l|$ are the numbers of faces and vertices at level $l$, respectively.

To maximize discriminative power, the embeddings of all nodes are summed to form a graph embedding, and the graph embeddings from all layers are concatenated as the final graph representation $h_G$ for downstream tasks:
\begin{equation}
    h_{G}=\big\Vert_{l=1}^L \left(\sum\nolimits_{i=1}^{|V|} h_i^{(l)}\right),
\end{equation}
where $L$ is the number of GNN layers. 
PolyhedronGNN utilizes local rigid representation to achieves rotation and translation invariance, while retaining the ability to distinguish different graphs. Assuming the distance between any two nodes is bounded within a range, we demonstrate that our method can aggregate complete graph information with arbitrary precision:
\begin{thm}\label{thm-haus}
Suppose $\eta: \mathcal{S} \rightarrow \mathbb{R}$ be a continuous set function with respect to the Hausdorff distance $d_H(\cdot,\cdot)$. Let $S \in \mathcal{S}$ be the set of all two-hop paths of a surface-attributed graph $G$,  $S=\{s(\pi_{i,j,k}) | v_i \in V\}$, $\forall \epsilon>0, \exists K \in \mathbb{Z}$, such that for any $S \in \mathcal{S}$, 
\begin{equation}
    |\eta(S)-\zeta(\eta'(S))|<\epsilon,
\end{equation}
where $\zeta$ is a continuous function, and $\eta'(S)\in \mathbb{R}^K$ is the output of our proposed method.
\end{thm}
\begin{proof}
    The detailed proof is in Appendix \ref{appendix:thm-haus}. Similar to PointNet, in the worst case, our method divides the space into small granules. With a sufficiently large output dimension, our method maps each input into a unique granule.
\end{proof}

\section{Experiments}
We evaluate the effectiveness of our approach through two fundamental tasks—classification and retrieval—across four datasets. We first introduce the datasets and comparison methods then provide the main results and analysis. For detailed information on implementation specifics, please see Appendix \ref{appendix:Experimental Details}.
\subsection{Dataset}
We employ the following datasets for both classification and retrieval tasks, detailed as follows:
\textbf{MNIST-C}: This dataset contains 13,742 samples of digit polyhedra. We transform 2D polygon shapes from the MNIST-P dataset \citep{jiang2019ddsl} into 3D by stretching them along the z-axis. Each digit is color-coded (purple for the bottom face, red for the front face, green for side faces excluding the bottom, and blue for the back face) and randomly rotated in 3D space to highlight directional identification.
\textbf{Building}: Comprising 5,000 polyhedra, this dataset extends 2D polygons from the OpenStreetMap (OSM) building dataset \citep{yan2021graph} into 3D polyhedra. Each building is categorized into one of ten standard alphabetic shapes based on its shape. Unlike MNIST-C, these samples are not subjected to random rotations due to the original lack of alignment. 
\textbf{ShapeNet-P}: Derived from the ShapeNetCore dataset \citep{chang2015shapenet}, this dataset features 2,122 polyhedra across 15 object categories. We employ a mesh merge algorithm to combine coplanar meshes with identical properties into polyhedral objects. Files that still retain numerous mesh faces after merging are dropped. Random rotations are applied.
\textbf{ModelNet-P}: This dataset, based on ModelNet40 \citep{wu20153d}, contains 1,303 polyhedra spanning 14 object categories. The processing is the same as ShapeNet-P, including applying random rotations.

\subsection{Comparison method}
\textbf{ResNet1D} \citep{mai2023towards}: This model adapts the 1D variant of the Residual Network (ResNet) architecture, incorporating circular padding to effectively encode the exterior vertices of polygons. \textbf{VeerCNN} \cite{van2019deep}: A Convolutional Neural Network (CNN) designed for 1D inputs, VeerCNN employs zero padding and concludes with global average pooling. \textbf{ NUFT-DDSL} \citep{jiang2019ddsl}: A spatial domain polygon encoder that uses NUFT features and the DDSL model. \textbf{ NUFT-IFFT} \citep{mai2023towards}: A spatial domain polygon encoder that utilizes NUFT features and the inverse Fast Fourier transformation (IFFT). \textbf{ PolygonGNN} \citep{yu2024polygongnn}: A graph-based polygon encoder that models 2D multipolygon as visibility graph.

\subsection{Effectiveness Analysis for Classification Task}
Table \ref{table:performance} presents the performance comparison between the proposed method and competing models across four datasets. We utilized a range of metrics to assess performance, including Accuracy (Acc), Weighted Precision (Prec), Weighted F1 Score (F1), and Weighted ROC AUC Score (AUC).  The highest scores for each dataset are denoted in boldface. PolyhedronNet achieved the highest scores in accuracy, precision, F1, and AUC across all datasets,  enhancing the Precision score by 72\% over the average of other methods in the MNIST-C dataset. Notably on the Building dataset, PolyhedronNet achieved an AUC of 1.000.
For ShapeNet-P and ModelNet-P, where the challenge lies in handling a diverse range of complex 3D shapes and fine-grained object differences, PolyhedronNet still achieved solid results, with an AUC of 0.936 on ShapeNet-P and 0.824 on ModelNet-P. Although the performance on these datasets was slightly lower compared to MNIST-C and Building, the results still demonstrate its robustness in recognizing complex polyhedra. Overall, PolyhedronNet's performance across these diverse datasets underscores its versatility and strength in handling complex polyhedra, making it an effective solution for the challenging polyhedron classification task.
\begin{table*}[ht]
\small
  \caption{The performance of the proposed model and the comparison methods on the classification task. The best results are in bold. }
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c|r|ccccc|c}
  \toprule
    Dataset & Metric & NUFT-DDSL  &ResNet1D     & NUFT-IFFT & VeerCNN  &PolygonGNN   &PolyhedronNet\\
    \midrule
    \multirow{4}{*}{MNIST-C}
    % filename=["Sep09-16_39.pth", "Sep09-16_58.pth" , "Sep09-17_08.pth",  "Sep09-17_28.pth"] filename=["Aug01-06_09.pth", ]
    & Acc $\uparrow$ & 0.148 &   {0.152} & {0.239} & {0.127}&0.435 & \textbf{0.858}\\
    & Prec$\uparrow$   & 0.092 &   {0.139} & {0.220} & {0.104} &0.446 &\textbf{0.861}\\
    & F1$\uparrow$   & 0.102 &   {0.083} & {0.202} & {0.084} &0.427 &\textbf{0.856}\\
    & AUC$\uparrow$  & 0.474 &   {0.610} & {0.619} & {0.576} &0.801 &\textbf{0.985}\\
    \hline
    \multirow{4}{*}{Building}
    % filename=["Sep11-11_39.pth", "Sep11-11_46.pth" ,"Sep11-11_50.pth", "Sep11-11_57.pth"] filename=["Aug06-00_18.pth", ]
    & Acc$\uparrow$& 0.921 &   {0.919} & {0.941} & {0.874}&0.973 & \textbf{0.980}\\
    & Prec$\uparrow$  & 0.921 &   {0.921} & {0.942} & {0.876}&0.974  &\textbf{0.980}\\
    & F1$\uparrow$  & 0.921 &   {0.920} & {0.941} & {0.874} &0.973 &\textbf{0.980}\\
    & AUC$\uparrow$ & 0.994 &   {0.993} & {0.997} & {0.987} &0.999 &\textbf{1.000}\\
    \hline
    \multirow{4}{*}{ShapeNet-P}
    % filename=["Sep11-12_30.pth", "Sep11-12_33.pth", "Sep11-12_35.pth", "Sep11-12_38.pth"]  filename=["Aug01-08_25.pth", ]
    & Acc$\uparrow$& 0.097 &   {0.179} & {0.097} & {0.163}&0.573 & \textbf{0.627}\\
    & Prec$\uparrow$  & 0.103 &   {0.142} & {0.082} & {0.158}&0.589  &\textbf{0.640}\\
    & F1$\uparrow$  & 0.092 &   {0.147} & {0.083} & {0.148} &0.570 &\textbf{0.625}\\
    & AUC$\uparrow$ & 0.555 &   {0.625} & {0.564} & {0.639} &0.916 &\textbf{0.936}\\
    \hline
    \multirow{4}{*}{ModelNet-P} 
    % filename=["Sep11-13_15.pth", "Sep11-13_17.pth", "Sep11-13_18.pth", "Sep11-13_20.pth"] filename=["Aug06-23_25.pth", ]
    & Acc$\uparrow$& 0.153 &   {0.321} & {0.164} & {0.206} &0.430& \textbf{0.435}\\
    & Prec$\uparrow$  & 0.118 &   {0.381} & {0.148} & {0.221}&0.370  &\textbf{0.377}\\
    & F1$\uparrow$  & 0.114 &   {0.302} & {0.138} & {0.197} &0.385 &\textbf{0.393}\\
    & AUC$\uparrow$ & 0.575 &   {0.784} & {0.629} & {0.726}&0.821  &\textbf{0.824}\\
    \bottomrule
  \end{tabular}
  }
  \label{table:performance}
\end{table*}
\begin{table*}[ht]
\small
  \caption{The performance of the proposed model and the comparison methods on the retrieval task. The best results are in bold. }
  \centering
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{c|r|ccccc|c}
  \toprule
    Dataset & Metric & NUFT-DDSL  &ResNet1D     & NUFT-IFFT & VeerCNN  &PolygonGNN   &PolyhedronNet\\
    \midrule
    \multirow{5}{*}{MNIST-C}  
    % filename=["Sep09-16_39.pth", "Sep09-16_58.pth" , "Sep09-17_08.pth",  "Sep09-17_28.pth"] filename=["Aug01-06_09.pth", ]
    & Prec $\uparrow$ & 0.428 &   {0.448} & {0.367} & {0.307} &0.386& \textbf{0.713}\\
    & Recall$\uparrow$   & 0.430 &   {0.450} & {0.368} & {0.308}&0.388  &\textbf{0.715}\\
    & F1$\uparrow$   & 0.429 &   {0.449} & {0.368} & {0.307} &0.387 &\textbf{0.714}\\
    & MAP$\uparrow$  & 0.660 &   {0.696} & {0.559} & {0.477} &0.586 &\textbf{0.842}\\
    & NDCG$\uparrow$  & 0.897 &   {0.910} & {0.857} & {0.809}  &0.859 &\textbf{0.945}\\
    \hline
    \multirow{5}{*}{Building} 
    % filename=["Sep11-11_39.pth", "Sep11-11_46.pth" ,"Sep11-11_50.pth", "Sep11-11_57.pth"] filename=["Aug06-00_18.pth", ]
    & Prec $\uparrow$ & 0.279 &   {0.264} & {0.276} & {0.147} &0.788 & \textbf{0.838}\\
    & Recall$\uparrow$   & 0.282 &   {0.266} & {0.279} & {0.148} &0.796  &\textbf{0.847}\\
    & F1$\uparrow$   & 0.280 &   {0.265} & {0.277} & {0.148} &0.792 &\textbf{0.843}\\
    & MAP$\uparrow$  & 0.564 &   {0.481} & {0.550} & {0.327} &0.890 &\textbf{0.923}\\
    & NDCG$\uparrow$  & 0.809 &   {0.771} & {0.803} & {0.645} &0.953 &\textbf{0.966}\\
    \hline
    \multirow{5}{*}{ShapeNet-P}  
    % filename=["Sep11-12_30.pth", "Sep11-12_33.pth", "Sep11-12_35.pth", "Sep11-12_38.pth"]  filename=["Aug01-08_25.pth", ]
    & Prec $\uparrow$ & 0.098 &   {0.156} & {0.088} & {0.135} &0.317& \textbf{0.322}\\
    & Recall$\uparrow$   & 0.101 &   {0.161} & {0.091} & {0.139} &0.327 &\textbf{0.332}\\
    & F1$\uparrow$   & 0.100 &   {0.158} & {0.089} & {0.137} &0.322 &\textbf{0.327}\\
    & MAP$\uparrow$  & 0.201 &   {0.299} & {0.196} & {0.291} &0.476 &\textbf{0.486}\\
    & NDCG$\uparrow$  & 0.415 &   {0.525} & {0.405} & {0.513}  &0.670 &\textbf{0.674}\\
    \hline
    \multirow{5}{*}{ModelNet-P} 
    % filename=["Sep11-13_15.pth", "Sep11-13_17.pth", "Sep11-13_18.pth", "Sep11-13_20.pth"] filename=["Aug06-23_25.pth", ]
    & Prec $\uparrow$ & 0.113 &   {0.196} & {0.118} & {0.155} & 0.233 &\textbf{0.240}\\
    & Recall$\uparrow$   & 0.119 &   {0.206} & {0.123} & {0.163} &0.245  &\textbf{0.252}\\
    & F1$\uparrow$   & 0.116 &   {0.201} & {0.120} & {0.159} &0.239 &\textbf{0.246}\\
    & MAP$\uparrow$  & 0.286 &   {0.378} & {0.266} & {0.343} &0.415 &\textbf{0.421}\\
    & NDCG$\uparrow$  & 0.450 &   {0.557} & {0.440} & {0.517} &0.575 &\textbf{0.576}\\
    \bottomrule
  \end{tabular}
  }
  \label{table:performance2}
\end{table*}

\subsection{Effectiveness Analysis for Retrieval Task}
We repurpose the model trained on the classification task to execute the retrieval task by removing the downstream classifier and assessing the similarity among learned representations in the test set. For each test sample, we pre-determine the count of items within the same class and retrieve an equivalent number of samples. We then compute the average values for the following metrics: Precision (Prec), Recall, F1 Score (F1), Mean Average Precision (MAP), and Normalized Discounted Cumulative Gain (NDCG).

Table \ref{table:performance2} presents the performance comparison between the proposed method and competing models across four datasets. The highest scores for each dataset are denoted in boldface.
On the Building dataset, PolyhedronNet exhibited the most significant improvement, with Recall increasing by an average of 60\% and F1 showing a substantial boost compared to other methods. It also achieved the highest in other scores, reflecting its ability to retrieve and rank relevant architectural structures accurately. In the MNIST-C dataset, PolyhedronNet outperformed other models, with Precision improving by 32\% over the average of other methods, showcasing its effectiveness in retrieving polyhedral representations of handwritten digits. For the ShapeNet-P dataset, which involves distinguishing a wide variety of 3D shapes, PolyhedronNet delivered strong performance, achieving the top NDCG of 0.674, indicating its ability to retrieve and rank relevant shapes effectively. Similarly, in the ModelNet-P dataset, PolyhedronNet excelled, achieving the best NDCG of 0.576, further proving its capacity to handle fine-grained differences in 3D object retrieval. These results demonstrate the versatility and robustness of the representations learned by PolyhedronNet in handling polyhedra.


\subsection{Ablation study}
We conducted an ablation study to assess the importance of face attributes quantitatively. This involved masking the face attributes with zeroes and comparing the performance to that of the original PolyhedronNet on two specific tasks using the MNIST-C and ShapeNet-P datasets. It is important to note that the Building and ModelNet-P datasets do not possess face attributes, making such comparisons inapplicable. The outcomes of this study are detailed in Table \ref{table:ablation1} and Table \ref{table:ablation2}. Results demonstrate a noticeable decrease in both classification and retrieval tasks, which indicates the importance of face attributes. 

\begin{minipage}{0.5\textwidth}
\small
    \centering
    \captionof{table}{Ablation results in classification task}
    \label{table:ablation1}
    \begin{tabular}{@{}r cccc@{}}
        \toprule
        & \multicolumn{2}{c}{MNIST-C} & \multicolumn{2}{c}{ShapeNet-P} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Metric & {w/ face} & {w/o face} & {w/ face} & {w/o face} \\
        \midrule
        Acc $\uparrow$ & 0.858 & 0.360 & 0.627 & 0.578 \\
        Prec $\uparrow$ & 0.861 & 0.401 & 0.640 & 0.595 \\
        F1 $\uparrow$ & 0.856 & 0.343 & 0.625 & 0.568 \\
        AUC $\uparrow$ & 0.985 & 0.742 & 0.936 & 0.909 \\
        \bottomrule
    \end{tabular}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\small
    \centering
    \captionof{table}{Ablation results in retrieval task}
    \label{table:ablation2}
    \begin{tabular}{@{}r cccc@{}}
        \toprule
        & \multicolumn{2}{c}{MNIST-C} & \multicolumn{2}{c}{ShapeNet-P} \\
        \cmidrule(lr){2-3} \cmidrule(lr){4-5}
        Metric & {w/ face} & {w/o face} & {w/ face} & {w/o face} \\
        \midrule
        Prec $\uparrow$ & 0.713 & 0.348 & 0.322 & 0.318 \\
        Recall $\uparrow$ & 0.715 & 0.349 & 0.332 & 0.327 \\
        F1 $\uparrow$ & 0.714 & 0.348 & 0.327 & 0.322 \\
        MAP $\uparrow$ & 0.842 & 0.534 & 0.486 & 0.482 \\
        NDCG $\uparrow$ & 0.945 & 0.837 & 0.674 & 0.674 \\
        \bottomrule
    \end{tabular}
\end{minipage}

\setlength{\intextsep}{2pt} %vertical space
\setlength{\columnsep}{5pt} % Reduce as necessary
\begin{wrapfigure}{r}{0.5\textwidth} % The 'r' places it on the right, and '0.5\textwidth' sets the width to half of the text area
  \centering
  \includegraphics[width=0.5\textwidth]{fig/hyper.png} 
  \caption{Hyperparameter sensitivity}
  \label{fig:sensitivity}
\end{wrapfigure}
\subsection{Hyperparameter Sensitivity}
We delve into the sensitivity analysis of two critical hyperparameters within our proposed framework: the hidden dimension and the number of GNN layers, utilizing the MNIST-C dataset for evaluation.
The impact of the hidden dimension on model performance is illustrated in Figure \ref{fig:sensitivity} (a). Generally, the model exhibits low sensitivity to the hidden dimension size once it surpasses a certain threshold (in this case, 256 for the MNIST-C dataset). Nonetheless, dimensions that are too small may constrict the model's expressive capacity, resulting in suboptimal performance. These findings are consistent with the principles outlined in our Theorem \ref{thm-haus}.
Regarding the number of GNN layers, Figure \ref{fig:sensitivity} (b) shows that an optimal performance is achieved with approximately 4 GNN layers. The flat curve indicates low sensitivity to the number of layers. This may be attributed to the concatenation of embeddings from all layers.

\subsection{Case study}
% \vspace{-4pt}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/mnist.png}
    \caption{Test cases from the MNIST-C dataset correctly predicted by PolyhedronNet, displaying face-attributed and blank versions side by side. The blank models are rotated to show the possible ambiguity. Predictions from comparison methods are also presented below each image for comparison.}
    \label{fig:mnistvisualisation}
\end{figure}
\begin{figure}[htb]
    \centering
    \includegraphics[width=\linewidth]{fig/shapenet_visual.png}
    \caption{Test cases from the ShapeNet-P dataset correctly predicted by PolyhedronNet, displaying face-attributed and blank versions side by side. Predictions from comparison methods are also presented below each image for comparison.}
    \label{fig:realvisualisation}
\end{figure}

We conducted an in-depth analysis of PolyhedronNet's performance by selecting and visualizing several representative cases from the test sets of the MNIST-C and ShapeNet-P datasets. The selected cases demonstrate instances where PolyhedronNet's predictions align with the actual labels, and we also present comparative results from other methods for reference. The visualizations from the MNIST-C dataset are depicted in Figure \ref{fig:mnistvisualisation}.
We observed that numerous prediction errors by comparison methods were likely due to the ambiguity caused by rotating polyhedron digits, which can make digits such as '6'/'9' and '5'/'2' appear inverted or flipped. The face attributes within our PolyhedronNet model play a crucial role in indicating the direction of a digit, thereby effectively avoiding such errors.
Furthermore, PolyhedronNet demonstrated its ability to accurately handle complex cases where the orientation of digits could lead to misidentification. For instance, it correctly identified an irregularly shaped '7' that resembles a '4' (fourth sample in the first row), a '9' that appeared similar to a '5' (third sample in the second row), and a '4' that resembles a '5' (last sample in the second row). These successes can be partially attributed to the directional guidance provided by face attributes and also to the strong capabilities of our model.

Further visualizations from the ShapeNet-P dataset are shown in Figure \ref{fig:realvisualisation}. 
In the first case, all comparison methods mistakenly classified the "cabinet" as a "loudspeaker," a common error due to their similar cubic shapes and appearances. However, PolyhedronNet distinguishes the cabinet effectively by recognizing the different colors on its surface, which indicate the presence of drawers, thus negating the possibility of it being a loudspeaker. By adeptly leveraging both the face attributes and the geometric properties of objects, PolyhedronNet enhances prediction accuracy.  The ability to discern different parts of objects through attributes like color is particularly effective in complex cases involving multi-part objects such as loudspeakers, knives, lamps, and benches, facilitating accurate feature assembly.
% In conclusion, PolyhedronNet exhibits substantial benefits in managing complex polyhedral forms by synergistically leveraging geometries and face attributes. This robust capability underscores its advanced potential in accurately interpreting diverse and intricate polyhedra.


\section{Conclusion}
This work advances polyhedra representation learning by introducing a novel framework named PolyhedronNet. Central to this framework is the surface-attributed graph, a unified data structure for modeling polyhedra, coupled with the development of a local rigid representation and a custom-designed graph neural network, PolyhedronGNN. By directly modeling a polyhedron with SAG, we open the door for a variety of applications that require processing 3D polyhedral objects.
The effectiveness of PolyhedronNet has been rigorously validated through extensive experiments on four datasets in classification and retrieval tasks. 






\subsubsection*{Acknowledgments}
This work was partially supported by the NSF Grant No. 2007716, No. 2007976, No. 1942594, No. 1907805, Cisco Faculty Research Award, and NIH Grant No. R01AG089806.


\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{List of sambols}
The main mathematical symbols used throughout the paper are summarized in Table\ref{tab:symbols}, organized with their formal descriptions.

\begin{table}[ht]
\centering
\begin{tabular}{|l|p{0.7\textwidth}|}
\hline
\textbf{Symbol} & \textbf{Description} \\
\hline
$q$ & A polyhedron (3D solid formed by flat polygon faces) \\
\hline
$p_i$ & A polygon face in a polyhedron \\
\hline
$v_{i,j}$ & The $j$-th vertex of the $i$-th face, with 3D coordinates \\
\hline
$N_{b,i}$ & Number of vertices in the $i$-th face \\
\hline
$N_f$ & Number of faces in a polyhedron \\
\hline
$q_v$ & Vector representation of a polyhedron \\
\hline
$G$ & Surface-attributed graph (SAG) comprising $(V, E, F, a)$ \\
\hline
$V$ & Set of nodes in the SAG \\
\hline
$E$ & Set of edges in the SAG \\
\hline
$F$ & Set of face-hyperedges in the SAG \\
\hline
$f$ & Face-hyperedge, an ordered set of edges forming a closed shape \\
\hline
$a$ & Face attributes mapping function \\
\hline
$e_{i,j}$ & Directed edge from vertex $v_i$ to $v_j$ \\
\hline
$\pi_{i,j,k}$ & Two-hop path $v_i \leftarrow v_j \leftarrow v_k$ \\
\hline
$\Pi^i_2$ & Set of all two-hop paths converging to node $v_i$ \\
\hline
$d_{i,j}$ & Euclidean distance between nodes $v_i$ and $v_j$ \\
\hline
$\theta_{i,j,k}$ & Angle at $v_j$ formed by vectors $\overrightarrow{v_jv_i}$ and $\overrightarrow{v_jv_k}$ \\
\hline
$\phi_{i,j,k}$ & Dihedral angle between faces containing edges $e_{i,j}$ and $e_{j,k}$ \\
\hline
$\psi_{i,j,k}$ & Indices of face-hyperedge containing edges $e_{i,j}$ and $e_{j,k}$ \\
\hline
$\Psi^{(l,\psi(\pi_{i,j,k}))}$ & Multi-layer perceptron model for path type $\psi(\pi_{i,j,k})$ at layer $l$ \\
\hline
$\psi(\pi_{i,j,k})$ & Path type indicator ($R_{inner}$ or $R_{cross}$) \\
\hline
$w^{(\psi(\pi_{i,j,k}))}$ & Weight for path type $\psi(\pi_{i,j,k})$ \\
\hline
$a_{j,i}$ & Face attributes of the face containing edge $e_{j,i}$  \\
\hline
$g^{(l)}$ & Guiding embedding at layer $l$ \\
\hline
$\varphi^{(l)}$ & MLP function for calculating the guiding embedding at layer $l$ \\
\hline
$h^{(l)}_i$ & Node embedding of node $v_i$ at layer $l$ \\
\hline
$m^{(l)}(\pi_{i,j,k})$ & Learned message from path $\pi_{i,j,k}$ at layer $l$ \\
\hline
$h_G$ & Final graph representation \\
\hline
\end{tabular}
\caption{Key symbols and their descriptions}
\label{tab:symbols}
\end{table}
\section{Proof for Lemma \ref{lem:1}}\label{appendix:lem1}
\begin{proof}
The nodes in graph ${G}$ have a one-to-one correspondence with the vertices of the polyhedron ${q}$. Each polygon face $p_i$ is defined by an ordered set of points. To reconstruct ${q}$ from ${G}$, we first group the nodes of the graph into their corresponding faces using the face-hyperedge. 
Since nodes along the boundaries of faces are arranged in a counterclockwise direction when viewed from outside the polyhedron, we can reconstruct the boundary of each face by initiating traversal from any node and following the edges until the starting node is reached. This allows for straightforward identification of face boundaries through basic geometric computations.
The normal vector associated with each face can be computed using the cross product of two edges on the face boundary. Consequently, every face is reconstituted with its correct shape and orientation.
Hence, from graph ${G}$, we can uniquely reconstruct the original polyhedron ${q}$, ensuring that no information about the polyhedron's structure is lost.
\end{proof}

\section{Proof for Theorem \ref{lem-thm}}\label{appendix:lem-thm}
We first proof that starting from a random node, one can recover the shape of a face it associated with.

\begin{lem}\label{lem} Given the position of a starting node $v_{i}$, which is connected to $v_{j}$, and local rigid representations of SAG, we can determine the face shape whose starting edge is $e_{i,j}$ (i.e., $f=(e_{i,j}...)$) in a 2D plane.
\end{lem}

\begin{proof}
We establish a local 2D Cartesian coordinate system with node $v_j$ as the origin and the ray $\overrightarrow{ v_iv_j}$ as the positive x-axis. The coordinate of node 
$v_i$ now is $(-d_{i,j},0)$. Define the angle $\theta_{i,j,k}$ as the clockwise rotation from the ray $\overrightarrow{v_jv_i}$ to the ray $\overrightarrow{v_jv_k}$. A positive value of $\theta_{i,j,k}$  indicates a clockwise rotation, while a negative value indicates a counterclockwise rotation. Given this setup, the coordinates of node  $v_k$  relative to  $v_j$  can be calculated using trigonometric relations:
\begin{align*}
    \left\{
    \begin{array}{l}
        x_k = -d_{j,k} \cos(\theta_{i,j,k}), \\
        y_k =  d_{j,k} \sin(\theta_{i,j,k}) 
    \end{array}
    \right.
\end{align*}    
Therefore, by applying these trigonometric relations, we can uniquely determine the coordinates of $v_k$ in the local coordinate system. Then iteratively we can determine the coordinate of the next node following $v_k$ until we reach the starting node $v_i$ to form a closed shape. Since $\psi_{i,j,k}$ indicates whether two consecutive edges belong to the same face, this helps prevent deviations to different faces.  Hence, the shape of the face is determined and the lemma is thereby proven.
\end{proof}
Then we prove that we can combine faces to reconstruct an equavalent SAG.
\begin{proof}
An equivalent SAG is one that represents the same polyhedron, under any translation or rotation transformations. Without loss of generality, we start from a random node as delineated in Lemma \ref{lem}, then the first face shape can be determined. Given that the faces within a polyhedron are interconnected through shared edges, we can iteratively apply this process to determine the shapes of all faces in the graph. Since $\phi_{i,j,k}$ records the angles between two associated faces, we can connect two faces by first using $\psi_{i,j,k}$ to identify the adjacent faces, then querying their shared edges, and setting the faces to form an angle equal to $\phi_{i,j,k}$ at the shared edges. By repeating this process iteratively, the position of all faces are determined.
It's noteworthy that different initializations, which might lead to varying orientations or positions of the graph due to rotation or translation transformations, still correspond to the same multipolygon. Consequently, despite these transformations, the reconstructed graph retains its equivalence to the original heterogeneous visibility graph.  Hence, the theorem is proven.
\end{proof}

\section{Proof for Theorem \ref{thm-haus}}\label{appendix:thm-haus}
\begin{proof}
Since $\eta: \mathcal{S} \rightarrow \mathbb{R}$ is a continuous set function with respect to Hausdorff distance, $\forall \epsilon_1>0, \exists \delta_1 >0$ such that for any $S,S' \in \mathcal{S}$ with $d_H(S,S')<\delta_1$, we have $|\eta(S)-\eta(S')|<\epsilon_1$.
Assume, without loss of generality, that $S$ is a one-dimensional finite set contained within an interval$[a,b]$.
Denote this interval as $\Xi=[a,b]$, we can divide $\Xi$ into $K=\lceil \frac{b-a}{\delta}\rceil+1$ equal subintervals $[a+(k-1)\Delta,a+k\Delta],k=1,2,\dots,K$, where $\Delta=\frac{b-a}{K}$. 
Define a function $r: \mathbb{R} \rightarrow \mathbb{R}$ as $r(x)=a+\lfloor \frac{x-a}{\Delta}\rfloor \Delta$, which maps each $x \in S$ to the lower bound of its respective subinterval. Let $S'=\{r(x):x\in S\}$. By this construction, $d_H(S,S')\leq\frac{b-a}{K}<\delta_1$, hence $|\eta(S)-\eta(S')|<\epsilon_1$.

Next, define $\sigma_k: \mathbb{R} \rightarrow [0, +\infty)$ as the Hausdorff distance from any point $x$ to the complement of the $k$-th subinterval in $\Xi$. Specifically, $\sigma_k(x)=d_H(x,\Xi \backslash [a+(k-1)\Delta, a+k\Delta])$. 
Let symmetric function $v_k(S)=\sum_{x \in S} \sigma_k(x)$, indicating whether points of $S$ fall within the $k$-th subinterval.

With these definitions, we construct a mapping function $\tau: [0,+\infty)^K \rightarrow \mathcal{S}$ as $\tau(\mathbf{v})=\{a+(k-1)\Delta: v_k>0\}$, which maps the vector $\mathbf{v}=[v_1,\dots,v_K]$ to a set consisting of the lower bounds of the subintervals occupied by $S$, which exactly equals the set $S'$ constructed above, i.e., $\tau(\mathbf{v}(S))=S'$.

Let $\zeta: \mathbb{R}^K \rightarrow \mathbb{R}$ be a continuous function so that $\zeta(\mathbf{v})=\eta(\tau(\mathbf{v}))$. Denote $\boldsymbol{\sigma}=[\sigma_1,\dots,\sigma_K]$. Then we have
\begin{align*}
    &|\eta(S)-\zeta(\sum\{\boldsymbol{\sigma}(x):x \in S\})|\\
    =&|\eta(S)-\eta(\tau(\sum\{\boldsymbol{\sigma}(x):x \in S\}))|\\
    =&|\eta(S)-\eta(\tau(\mathbf{v}(S)))|\\
    =&|\eta(S)-\eta(S')|<\epsilon_1
\end{align*}
The continuous function $\boldsymbol{\sigma}$ can be approximated by a multilayer perceptron, according to the universal approximation theorem. Therefore, We have $|\eta(S)-\zeta(\sum\{m(x):x \in S\})|<\epsilon$, where $m$ is the MLP function. Considering the method described in Section \ref{sec:hgnn}, we can set $L=1$, making our proposed function $\eta'$ a sum of the messages from an MLP function. The sum operator is a special case of our method when $L=1$ and the message function is the MLP used above. Thus, we arrive at the conclusion that $|\eta(S)-\zeta(\eta'(S))|<\epsilon$. Hence, the theorem is proven.
\end{proof}
\section{Experimental Details} \label{appendix:Experimental Details}
\subsection{Implementation details}
Each dataset is randomly split into 60\%, 20\%, and 20\% for training, validation, and testing respectively.

We use CrossEntropyLoss as the loss function for all classification tasks. Adam optimizer and ReduceLROnPlateau scheduler are used to optimize the model. The learning rate is set to 0.001 across all tasks and models. The training batch and testing batch are set to 32 for the MNIST-C and Building datasets and 8 for the ShapeNet-P and ModelNet-P datasets. The downstream task model is a four-layer MLP function with batchnorm enabled for the classification task. All models are trained for a maximum of 500 epochs using an early stop scheme.

For the comparison method ResNet1D, VeerCNN, NUFT-DDSL, and NUFT-IFFT, we follow the original settings provided by the authors.

For the message encoding function $\Psi$, we use a four-layer MLP function with batchnorm enabled across all tasks. For the guiding embedding function $\varphi$, we leverage a one-layer MLP function with batchnorm enabled across all tasks. The downstream task classifier is a four-layer MLP function.

The hyperparameters we tuned include hidden dimensions in {64,128,256,512,1024}, and the number of GNN layers in {1,2,3,4,8}. We found the best hyperparameters for different datasets are: MNIST-C: [256,4];  Building: [512,4]; ShapeNet-P: [256,2]; ModelNet-P: [128,2].


\section{Performance of more comparison methods and ablated models}
We conduct additional experiments to evaluate our method against more comparison methods and ablated models. 
First, we compare different aggregation strategies, including mean and max aggregators, where results show that max aggregation generally achieves superior performance on MNIST-C and ShapeNet-P datasets, while mean aggregation performs better on Building and ModelNet-P datasets. To validate the effectiveness of our proposed heterogeneous geometric message passing mechanism, we conduct an ablation study (our w/o hetero) where we remove the heterogeneous message-passing modules. The significant performance drop demonstrates the importance of these components in capturing complex geometric relationships.
We also compare our method with established graph learning methods, including HGT and HAN. The results show that our approach substantially outperforms these methods across all datasets, indicating the advantage of our design. Furthermore, we benchmark against recent state-of-the-art point cloud methods, including LocoTrans \cite{chen2024local} and RISurConv \cite{zhang2024risurconv}. The experimental results demonstrate that our method achieves superior performance on most datasets, particularly showing significant improvements on MNIST-C and Building datasets. This suggests that our approach better captures the inherent geometric structure of 3D shapes compared to point-based methods.
% \begin{table}[htbp]
%     \centering
%     \caption{The performance of the proposed model and the comparison methods on the classification task. The best results are in bold.}
%     \label{tab:classification}
%     \resizebox{\textwidth}{!}{%
%     \begin{tabular}{lcccccccccccccccc}
%     \toprule
%     Dataset & \multicolumn{4}{c}{MNIST-C} & \multicolumn{4}{c}{Building} & \multicolumn{4}{c}{ShapeNet} & \multicolumn{4}{c}{ModelNet} \\
%     \cmidrule(lr){2-5} \cmidrule(lr){6-9} \cmidrule(lr){10-13} \cmidrule(lr){14-17}
%     Metric & Acc & Prec & F1 & AUC & Acc & Prec & F1 & AUC & Acc & Prec & F1 & AUC & Acc & Prec & F1 & AUC \\
%     \midrule
%     PolyhedronNet & 0.858 & 0.861 & 0.856 & 0.985 & 0.980 & 0.980 & 0.980 & \textbf{1.000} & \textbf{0.627} & 0.640 & \textbf{0.625} & \textbf{0.936} & 0.435 & 0.377 & 0.393 & 0.824 \\
%     Our agg\_mean & 0.858 & 0.868 & 0.859 & 0.984 & \textbf{0.981} & \textbf{0.981} & \textbf{0.981} & \textbf{1.000} & 0.587 & 0.596 & 0.571 & 0.919 & \textbf{0.576} & \textbf{0.581} & \textbf{0.564} & \textbf{0.895} \\
%     Our agg\_max & \textbf{0.885} & \textbf{0.890} & \textbf{0.885} & \textbf{0.990} & 0.973 & 0.973 & 0.973 & 0.999 & 0.620 & \textbf{0.648} & 0.618 & 0.921 & 0.508 & 0.508 & 0.500 & 0.891 \\
%     PolygonGNN & 0.435 & 0.446 & 0.427 & 0.801 & 0.973 & 0.974 & 0.973 & 0.999 & 0.573 & 0.589 & 0.570 & 0.916 & 0.430 & 0.370 & 0.385 & 0.821 \\
%     LocoTrans & 0.344 & 0.403 & 0.350 & 0.714 & 0.949 & 0.950 & 0.949 & 0.996 & 0.540 & 0.591 & 0.542 & 0.886 & 0.561 & 0.546 & 0.539 & 0.892 \\
%     RISurConv & 0.567 & 0.590 & 0.617 & 0.833 & 0.949 & 0.929 & 0.937 & 0.991 & 0.265 & 0.261 & 0.274 & 0.730 & 0.347 & 0.343 & 0.332 & 0.793 \\
%     \bottomrule
%     \end{tabular}%
%     }
% \end{table}

\begin{table}[t]
\caption{The performance of additional ablated models and comparison methods on the classification task.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cc|c|cc|cc}
\hline
Dataset & Metric & our agg\_mean & our agg\_max & our w/o hetero & HGT & HAN & LocoTrans & RISurConv \\
\hline
\multirow{4}{*}{MNIST-C} & Acc$\uparrow$ & 0.858 & {0.885} & 0.754 & 0.113 & 0.221 & 0.344 & 0.567 \\
& Prec$\uparrow$ & 0.868 & {0.890} & 0.784 & 0.103 & 0.157 & 0.403 & 0.590 \\
& F1$\uparrow$ & 0.859 & {0.885} & 0.742 & 0.093 & 0.159 & 0.350 & 0.617 \\
& AUC$\uparrow$ & 0.984 & {0.990} & 0.956 & 0.528 & 0.597 & 0.714 & 0.833 \\
\hline
\multirow{4}{*}{Building} & Acc$\uparrow$ & {0.981} & 0.973 & 0.797 & 0.897 & 0.900 & 0.949 & 0.949 \\
& Prec$\uparrow$ & {0.981} & 0.973 & 0.821 & 0.899 & 0.902 & 0.950 & 0.929 \\
& F1$\uparrow$ & {0.981} & 0.973 & 0.768 & 0.896 & 0.899 & 0.949 & 0.937 \\
& AUC$\uparrow$ & {1.000} & 0.999 & 0.922 & 0.991 & 0.987 & 0.996 & 0.991 \\
\hline
\multirow{4}{*}{ShapeNet-P} & Acc$\uparrow$ & 0.587 & {0.620} & 0.486 & 0.201 & 0.137 & 0.540 & 0.265 \\
& Prec$\uparrow$ & 0.596 & {0.648} & 0.506 & 0.161 & 0.130 & 0.591 & 0.261 \\
& F1$\uparrow$ & 0.571 & {0.618} & 0.463 & 0.155 & 0.113 & 0.542 & 0.274 \\
& AUC$\uparrow$ & 0.919 & {0.921} & 0.900 & 0.651 & 0.622 & 0.886 & 0.730 \\
\hline
\multirow{4}{*}{ModelNet-P} & Acc$\uparrow$ & {0.576} & 0.508 & 0.374 & 0.359 & 0.374 & 0.561 & 0.347 \\
& Prec$\uparrow$ & {0.581} & 0.508 & 0.327 & 0.360 & 0.371 & 0.546 & 0.343 \\
& F1$\uparrow$ & {0.564} & 0.500 & 0.333 & 0.300 & 0.333 & 0.539 & 0.332 \\
& AUC$\uparrow$ & {0.895} & 0.891 & 0.796 & 0.799 & 0.796 & 0.892 & 0.793 \\
\hline
\end{tabular}
}
\end{table}

\begin{table}[t]
\caption{The performance of additional ablated models and comparison methods on the retrieval task.}
\resizebox{\textwidth}{!}{%
\begin{tabular}{c|c|cc|c|cc|cc}
\hline
Dataset & Metric & our agg\_mean & our agg\_max & our w/o hetero & HGT & HAN & LocoTrans & RISurConv \\
\hline
\multirow{5}{*}{MNIST-C} & Prec$\uparrow$ & 0.690 & {0.717} & 0.579 & 0.284 & 0.389 & 0.417 & 0.533 \\
& Recall$\uparrow$ & 0.692 & {0.720} & 0.581 & 0.285 & 0.390 & 0.419 & 0.535 \\
& F1$\uparrow$ & 0.691 & {0.718} & 0.580 & 0.285 & 0.389 & 0.418 & 0.533 \\
& MAP$\uparrow$ & 0.828 & {0.847} & 0.751 & 0.455 & 0.592 & 0.594 & 0.710 \\
& NDCG$\uparrow$ & 0.940 & {0.945} & 0.915 & 0.799 & 0.865 & 0.860 & 0.927 \\
\hline
\multirow{5}{*}{Building} & Prec$\uparrow$ & {0.819} & 0.806 & 0.602 & 0.291 & 0.265 & 0.823 & 0.403 \\
& Recall$\uparrow$ & {0.828} & 0.814 & 0.608 & 0.294 & 0.268 & 0.831 & 0.405 \\
& F1$\uparrow$ & {0.824} & 0.810 & 0.605 & 0.293 & 0.266 & 0.828 & 0.405 \\
& MAP$\uparrow$ & {0.910} & 0.908 & 0.756 & 0.549 & 0.509 & 0.912 & 0.665 \\
& NDCG$\uparrow$ & {0.964} & 0.960 & 0.904 & 0.803 & 0.776 & 0.965 & 0.827 \\
\hline
\multirow{5}{*}{ShapeNet-P} & Prec$\uparrow$ & 0.262 & {0.323} & 0.238 & 0.269 & 0.272 & 0.322 & 0.170 \\
& Recall$\uparrow$ & 0.271 & {0.333} & 0.246 & 0.278 & 0.281 & 0.332 & 0.182 \\
& F1$\uparrow$ & 0.267 & {0.328} & 0.242 & 0.273 & 0.277 & 0.326 & 0.173 \\
& MAP$\uparrow$ & 0.462 & {0.499} & 0.406 & 0.483 & 0.492 & 0.497 & 0.387 \\
& NDCG$\uparrow$ & 0.664 & {0.690} & 0.614 & 0.688 & 0.693 & 0.686 & 0.602 \\
\hline
\multirow{5}{*}{ModelNet-P} & Prec$\uparrow$ & 0.334 & 0.334 & 0.234 & 0.318 & 0.326 & {0.378} & 0.201 \\
& Recall$\uparrow$ & 0.351 & 0.351 & 0.246 & 0.334 & 0.342 & {0.396} & 0.199 \\
& F1$\uparrow$ & 0.342 & 0.342 & 0.240 & 0.326 & 0.334 & {0.386} & 0.204 \\
& MAP$\uparrow$ & 0.528 & 0.517 & 0.406 & 0.514 & 0.521 & {0.554} & 0.366 \\
& NDCG$\uparrow$ & 0.680 & 0.670 & 0.568 & 0.673 & 0.678 & {0.683} & 0.539 \\
\hline
\end{tabular}
}
\end{table}



\section{Visualization of Retrieved Objects from ShapeNet-P}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{fig/retrieval.png}
    \caption{Retrieved samples from ShapeNet-P dataset by PolyhedronNet, the first column shows the query object.}
    \label{fig:retrieval}
\end{figure}


\end{document}
