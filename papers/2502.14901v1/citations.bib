
@article{northcutt_pervasive_2021,
	title = {Pervasive {Label} {Errors} in {Test} {Sets} {Destabilize} {Machine} {Learning} {Benchmarks}},
	url = {http://arxiv.org/abs/2103.14749},
	abstract = {We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results. Errors in test sets are numerous and widespread: we estimate an average of at least 3.3\% errors across the 10 datasets, where for example label errors comprise at least 6\% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51\% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets). Traditionally, machine learning practitioners choose which model to deploy based on test accuracy - our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets. Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data. For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6\%. On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5\%. Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.},
	urldate = {2022-04-21},
	journal = {arXiv:2103.14749 [cs, stat]},
	author = {Northcutt, Curtis G. and Athalye, Anish and Mueller, Jonas},
	month = nov,
	year = {2021},
	note = {arXiv: 2103.14749},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: Demo available at https://labelerrors.com/ and source code available at https://github.com/cleanlab/label-errors},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/WC3C5MMT/Northcutt et al. - 2021 - Pervasive Label Errors in Test Sets Destabilize Ma.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/WKCIMMJA/2103.html:text/html},
}

@book{efron_introduction_1993,
	address = {Philadelphia, PA},
	title = {An introduction to the bootstrap},
	isbn = {978-0-412-04231-7},
	publisher = {Chapman \& Hall/CRC},
	author = {Efron, Bradley and Tibshirani, Robert},
	year = {1993},
}

@article{vaswani_attention_2017,
	title = {Attention {Is} {All} {You} {Need}},
	url = {http://arxiv.org/abs/1706.03762},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
	urldate = {2022-05-11},
	journal = {arXiv:1706.03762 [cs]},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	note = {arXiv: 1706.03762},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 15 pages, 5 figures},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/BIIKYFY3/Vaswani et al. - 2017 - Attention Is All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/L5QDRN45/1706.html:text/html},
}

@misc{mullner_modern_2011,
	title = {Modern hierarchical, agglomerative clustering algorithms},
	url = {http://arxiv.org/abs/1109.2378},
	doi = {10.48550/arXiv.1109.2378},
	abstract = {This paper presents algorithms for hierarchical, agglomerative clustering which perform most efficiently in the general-purpose setup that is given in modern standard software. Requirements are: (1) the input data is given by pairwise dissimilarities between data points, but extensions to vector data are also discussed (2) the output is a "stepwise dendrogram", a data structure which is shared by all implementations in current standard software. We present algorithms (old and new) which perform clustering in this setting efficiently, both in an asymptotic worst-case analysis and from a practical point of view. The main contributions of this paper are: (1) We present a new algorithm which is suitable for any distance update scheme and performs significantly better than the existing algorithms. (2) We prove the correctness of two algorithms by Rohlf and Murtagh, which is necessary in each case for different reasons. (3) We give well-founded recommendations for the best current algorithms for the various agglomerative clustering schemes.},
	urldate = {2023-11-15},
	publisher = {arXiv},
	author = {Müllner, Daniel},
	month = sep,
	year = {2011},
	note = {arXiv:1109.2378 [cs, stat]
version: 1},
	keywords = {Statistics - Machine Learning, 62H30, Computer Science - Data Structures and Algorithms, I.5.3},
	annote = {Comment: 29 pages},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/KRHFBGVG/Müllner - 2011 - Modern hierarchical, agglomerative clustering algo.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/GYJ4E2LH/1109.html:text/html},
}

@article{shapiro_measuring_2022,
	title = {Measuring news sentiment},
	volume = {228},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407620303535},
	doi = {10.1016/j.jeconom.2020.07.053},
	abstract = {This paper demonstrates state-of-the-art text sentiment analysis tools while developing a new time-series measure of economic sentiment derived from economic and financial newspaper articles from January 1980 to April 2015. We compare the predictive accuracy of a large set of sentiment analysis models using a sample of articles that have been rated by humans on a positivity/negativity scale. The results highlight the gains from combining existing lexicons and from accounting for negation. We also generate our own sentiment-scoring model, which includes a new lexicon built specifically to capture the sentiment in economic news articles. This model is shown to have better predictive accuracy than existing “off-the-shelf” models. Lastly, we provide two applications to the economic research on sentiment. First, we show that daily news sentiment is predictive of movements of survey-based measures of consumer sentiment. Second, motivated by Barsky and Sims (2012), we estimate the impulse responses of macroeconomic variables to sentiment shocks, finding that positive sentiment shocks increase consumption, output, and interest rates and dampen inflation.},
	number = {2},
	urldate = {2024-02-23},
	journal = {Journal of Econometrics},
	author = {Shapiro, Adam Hale and Sudhof, Moritz and Wilson, Daniel J.},
	month = jun,
	year = {2022},
	pages = {221--243},
	file = {ScienceDirect Snapshot:/home/jonno/Zotero/storage/Q3S9MFL8/S0304407620303535.html:text/html;Submitted Version:/home/jonno/Zotero/storage/TK8ZHT4T/Shapiro et al. - 2022 - Measuring news sentiment.pdf:application/pdf},
}

@article{mello_combining_2023,
	title = {Combining sentiment analysis classifiers to explore multilingual news articles covering {London} 2012 and {Rio} 2016 {Olympics}},
	volume = {5},
	issn = {2524-7840},
	url = {https://doi.org/10.1007/s42803-022-00052-9},
	doi = {10.1007/s42803-022-00052-9},
	abstract = {This study aims to present an approach for the challenges of working with Sentiment Analysis (SA) applied to news articles in a multilingual corpus. It looks at the use and combination of multiple algorithms to explore news articles published in English and Portuguese. It presents a methodology that starts by evaluating and combining four SA algorithms (SenticNet, SentiStrength, Vader and BERT, being BERT trained in two datasets) to improve the quality of outputs. A thorough review of the algorithms’ limitations is conducted using SHAP, an explainable AI tool, resulting in a list of issues that researchers must consider before using SA to interpret texts. We propose a combination of the three best classifiers (Vader, Amazon BERT and Sent140 BERT) to identify contradictory results, improving the quality of the positive, neutral and negative labels assigned to the texts. Challenges with translation are addressed, indicating possible solutions for non-English corpora. As a case study, the method is applied to the study of the media coverage of London 2012 and Rio 2016 Olympic legacies. The combination of different classifiers has proved to be efficient, revealing the unbalance between the media coverage of London 2012, much more positive, and Rio 2016, more negative.},
	language = {en},
	number = {2},
	urldate = {2024-02-23},
	journal = {International Journal of Digital Humanities},
	author = {Mello, Caio and Cheema, Gullal S. and Thakkar, Gaurish},
	month = nov,
	year = {2023},
	keywords = {Explainable AI, News articles, NLP, Olympic legacy, Sentiment analysis},
	pages = {131--157},
	file = {Full Text PDF:/home/jonno/Zotero/storage/QBKHALQ2/Mello et al. - 2023 - Combining sentiment analysis classifiers to explor.pdf:application/pdf},
}

@article{noauthor_prepare_2023,
	title = {Prepare for truly useful large language models},
	volume = {7},
	copyright = {2023 Springer Nature Limited},
	issn = {2157-846X},
	url = {https://www.nature.com/articles/s41551-023-01012-6},
	doi = {10.1038/s41551-023-01012-6},
	abstract = {The consequences of their use will be far reaching.},
	language = {en},
	number = {2},
	urldate = {2024-02-23},
	journal = {Nature Biomedical Engineering},
	month = feb,
	year = {2023},
	note = {Number: 2
Publisher: Nature Publishing Group},
	keywords = {Biomedical Engineering/Biotechnology, Biomedicine, general},
	pages = {85--86},
	file = {Full Text PDF:/home/jonno/Zotero/storage/EYYAKZ88/2023 - Prepare for truly useful large language models.pdf:application/pdf},
}

@article{grzybowski_history_2024,
	title = {A {History} of {Artificial} {Intelligence}},
	issn = {0738-081X},
	url = {https://www.sciencedirect.com/science/article/pii/S0738081X23002687},
	doi = {10.1016/j.clindermatol.2023.12.016},
	abstract = {The development of the computer and what is now known as artificial intelligence (AI) has evolved over more than two centuries in a long series of steps. The date of the invention of the first computer is estimated at 1822, when Charles Babbage (1791-1871) developed his first design of a working computer on paper, based mainly on a Jacquard loom. He worked on his project together with Augusta Ada King, Countess Lovelace (née Byron) (Ada Lovelace) (1815-1852), whom he called the “Sorceress of Numbers.” This work will present the profile and achievements of Charles Babbage, Augusta Ada King, Countess Lovelace, and Alan Mathison Turing (1912 - 1954), who is considered the father of computer science and artificial intelligence, and then provide an outline of the tumultuous events affecting AI up to the present.},
	urldate = {2024-02-23},
	journal = {Clinics in Dermatology},
	author = {Grzybowski, Andrzej and Pawlikowska–Łagód, Katarzyna and Lambert, W. Clark},
	month = jan,
	year = {2024},
}

@misc{schreiner_gpt-4_2023,
	title = {{GPT}-4 architecture, datasets, costs and more leaked},
	url = {https://the-decoder.com/gpt-4-architecture-datasets-costs-and-more-leaked/},
	abstract = {A new report reveals the architecture, training datasets, cost, and more of OpenAI's GPT-4.},
	language = {en-US},
	urldate = {2024-02-24},
	journal = {THE DECODER},
	author = {Schreiner, Maximilian},
	month = jul,
	year = {2023},
	file = {Snapshot:/home/jonno/Zotero/storage/ZQUR2KFH/gpt-4-architecture-datasets-costs-and-more-leaked.html:text/html},
}

@misc{brown_language_2020,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/EALPLDP6/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/I2EGYS7A/2005.html:text/html},
}

@misc{devlin_bert_2019,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {http://arxiv.org/abs/1810.04805},
	doi = {10.48550/arXiv.1810.04805},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = may,
	year = {2019},
	note = {arXiv:1810.04805 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/F2U6PUMF/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/44EGKRGZ/1810.html:text/html},
}

@misc{touvron_llama_2023,
	title = {Llama 2: {Open} {Foundation} and {Fine}-{Tuned} {Chat} {Models}},
	shorttitle = {Llama 2},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/J3SSDZ63/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/P4EFXJPR/2307.html:text/html},
}

@misc{openai_gpt-4_2023,
	title = {{GPT}-4},
	url = {https://openai.com/research/gpt-4},
	abstract = {We’ve created GPT-4, the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.},
	language = {en-US},
	urldate = {2024-02-24},
	author = {OpenAI},
	month = mar,
	year = {2023},
	file = {Snapshot:/home/jonno/Zotero/storage/K3YYVGJV/gpt-4.html:text/html},
}

@misc{noauthor_bloggemmamd_2024,
	title = {blog/gemma.md at main · huggingface/blog},
	url = {https://github.com/huggingface/blog/blob/main/gemma.md},
	abstract = {Public repo for HF blog posts. Contribute to huggingface/blog development by creating an account on GitHub.},
	language = {en},
	urldate = {2024-02-24},
	journal = {GitHub},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/7YI8HSC4/gemma.html:text/html},
}

@misc{noauthor_gemma_nodate,
	title = {Gemma},
	url = {https://www.kaggle.com/models/google/gemma},
	abstract = {Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.},
	language = {en},
	urldate = {2024-02-24},
	file = {Snapshot:/home/jonno/Zotero/storage/2CK85D7H/gemma_instruct_7b_en.html:text/html},
}

@misc{noauthor_gemma_nodate-1,
	title = {Gemma},
	url = {https://www.kaggle.com/models/google/gemma},
	abstract = {Gemma is a family of lightweight, open models built from the research and technology that Google used to create the Gemini models.},
	language = {en},
	urldate = {2024-02-24},
	file = {Snapshot:/home/jonno/Zotero/storage/QD6UN329/gemma.html:text/html},
}

@misc{mesnard_gemma_2024,
	title = {Gemma},
	url = {https://www.kaggle.com/m/3301},
	doi = {10.34740/KAGGLE/M/3301},
	publisher = {Kaggle},
	author = {Mesnard, Thomas and Hardin, Cassidy and Dadashi, Robert and Bhupatiraju, Surya and Sifre, Laurent and Rivière, Morgane and Kale, Mihir Sanjay and Love, Juliette and Tafti, Pouya and Hussenot, Léonard and al, et},
	year = {2024},
	note = {Publisher: Kaggle},
}

@misc{brake_nineteenth-century_2008,
	title = {Nineteenth-{Century} {Serials} {Edition} ({NCSE}) (2008; 2018)},
	url = {https://ncse.ac.uk/periodicals/},
	urldate = {2024-02-24},
	author = {Brake, Laurel and Mussel, James and Turner, Mark},
	year = {2008},
	file = {NCSE\: Home:/home/jonno/Zotero/storage/9UU3NFMD/index.html:text/html},
}

@misc{iptc_newscodes_2024,
	title = {{NewsCodes}},
	url = {https://iptc.org/standards/newscodes/},
	abstract = {IPTC is the global standards body of the news media. We provide the technical foundation for the news ecosystem.},
	language = {en-GB},
	urldate = {2024-02-24},
	journal = {IPTC},
	author = {IPTC},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/98BTTF3B/newscodes.html:text/html},
}

@misc{noauthor_httpsarxivorgpdf200903300pdf_nodate,
	title = {https://arxiv.org/pdf/2009.03300.pdf},
	url = {https://arxiv.org/pdf/2009.03300.pdf},
	urldate = {2024-02-24},
}

@misc{hendrycks_measuring_2021,
	title = {Measuring {Massive} {Multitask} {Language} {Understanding}},
	url = {http://arxiv.org/abs/2009.03300},
	abstract = {We propose a new test to measure a text model’s multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. To attain high accuracy on this test, models must possess extensive world knowledge and problem solving ability. We ﬁnd that while most recent models have near random-chance accuracy, the very largest GPT-3 model improves over random chance by almost 20 percentage points on average. However, on every one of the 57 tasks, the best models still need substantial improvements before they can reach expert-level accuracy. Models also have lopsided performance and frequently do not know when they are wrong. Worse, they still have nearrandom accuracy on some socially important subjects such as morality and law. By comprehensively evaluating the breadth and depth of a model’s academic and professional understanding, our test can be used to analyze models across many tasks and to identify important shortcomings.},
	language = {en},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
	month = jan,
	year = {2021},
	note = {arXiv:2009.03300 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computers and Society},
	annote = {Comment: ICLR 2021; the test and code is available at https://github.com/hendrycks/test},
	file = {Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:/home/jonno/Zotero/storage/ABKD5WAJ/Hendrycks et al. - 2021 - Measuring Massive Multitask Language Understanding.pdf:application/pdf},
}

@misc{zellers_hellaswag_2019,
	title = {{HellaSwag}: {Can} a {Machine} {Really} {Finish} {Your} {Sentence}?},
	shorttitle = {{HellaSwag}},
	url = {http://arxiv.org/abs/1905.07830},
	doi = {10.48550/arXiv.1905.07830},
	abstract = {Recent work by Zellers et al. (2018) introduced a new task of commonsense natural language inference: given an event description such as "A woman sits at a piano," a machine must select the most likely followup: "She sets her fingers on the keys." With the introduction of BERT, near human-level performance was reached. Does this mean that machines can perform human level commonsense inference? In this paper, we show that commonsense inference still proves difficult for even state-of-the-art models, by presenting HellaSwag, a new challenge dataset. Though its questions are trivial for humans ({\textgreater}95\% accuracy), state-of-the-art models struggle ({\textless}48\%). We achieve this via Adversarial Filtering (AF), a data collection paradigm wherein a series of discriminators iteratively select an adversarial set of machine-generated wrong answers. AF proves to be surprisingly robust. The key insight is to scale up the length and complexity of the dataset examples towards a critical 'Goldilocks' zone wherein generated text is ridiculous to humans, yet often misclassified by state-of-the-art models. Our construction of HellaSwag, and its resulting difficulty, sheds light on the inner workings of deep pretrained models. More broadly, it suggests a new path forward for NLP research, in which benchmarks co-evolve with the evolving state-of-the-art in an adversarial way, so as to present ever-harder challenges.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
	month = may,
	year = {2019},
	note = {arXiv:1905.07830 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: ACL 2019. Project page at https://rowanzellers.com/hellaswag},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/52UZU3G2/Zellers et al. - 2019 - HellaSwag Can a Machine Really Finish Your Senten.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/7X3VX9NJ/1905.html:text/html},
}

@misc{sakaguchi_winogrande_2019,
	title = {{WinoGrande}: {An} {Adversarial} {Winograd} {Schema} {Challenge} at {Scale}},
	shorttitle = {{WinoGrande}},
	url = {http://arxiv.org/abs/1907.10641},
	doi = {10.48550/arXiv.1907.10641},
	abstract = {The Winograd Schema Challenge (WSC) (Levesque, Davis, and Morgenstern 2011), a benchmark for commonsense reasoning, is a set of 273 expert-crafted pronoun resolution problems originally designed to be unsolvable for statistical models that rely on selectional preferences or word associations. However, recent advances in neural language models have already reached around 90\% accuracy on variants of WSC. This raises an important question whether these models have truly acquired robust commonsense capabilities or whether they rely on spurious biases in the datasets that lead to an overestimation of the true capabilities of machine commonsense. To investigate this question, we introduce WinoGrande, a large-scale dataset of 44k problems, inspired by the original WSC design, but adjusted to improve both the scale and the hardness of the dataset. The key steps of the dataset construction consist of (1) a carefully designed crowdsourcing procedure, followed by (2) systematic bias reduction using a novel AfLite algorithm that generalizes human-detectable word associations to machine-detectable embedding associations. The best state-of-the-art methods on WinoGrande achieve 59.4-79.1\%, which are 15-35\% below human performance of 94.0\%, depending on the amount of the training data allowed. Furthermore, we establish new state-of-the-art results on five related benchmarks - WSC (90.1\%), DPR (93.1\%), COPA (90.6\%), KnowRef (85.6\%), and Winogender (97.1\%). These results have dual implications: on one hand, they demonstrate the effectiveness of WinoGrande when used as a resource for transfer learning. On the other hand, they raise a concern that we are likely to be overestimating the true capabilities of machine commonsense across all these benchmarks. We emphasize the importance of algorithmic bias reduction in existing and future benchmarks to mitigate such overestimation.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
	month = nov,
	year = {2019},
	note = {arXiv:1907.10641 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/ZXECZW5Y/Sakaguchi et al. - 2019 - WinoGrande An Adversarial Winograd Schema Challen.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/P3HTB5FN/1907.html:text/html},
}

@misc{hartvigsen_toxigen_2022,
	title = {{ToxiGen}: {A} {Large}-{Scale} {Machine}-{Generated} {Dataset} for {Adversarial} and {Implicit} {Hate} {Speech} {Detection}},
	shorttitle = {{ToxiGen}},
	url = {http://arxiv.org/abs/2203.09509},
	doi = {10.48550/arXiv.2203.09509},
	abstract = {Toxic language detection systems often falsely flag text that contains minority group mentions as toxic, as those groups are often the targets of online hate. Such over-reliance on spurious correlations also causes systems to struggle with detecting implicitly toxic language. To help mitigate these issues, we create ToxiGen, a new large-scale and machine-generated dataset of 274k toxic and benign statements about 13 minority groups. We develop a demonstration-based prompting framework and an adversarial classifier-in-the-loop decoding method to generate subtly toxic and benign text with a massive pretrained language model. Controlling machine generation in this way allows ToxiGen to cover implicitly toxic text at a larger scale, and about more demographic groups, than previous resources of human-written text. We conduct a human evaluation on a challenging subset of ToxiGen and find that annotators struggle to distinguish machine-generated text from human-written language. We also find that 94.5\% of toxic examples are labeled as hate speech by human annotators. Using three publicly-available datasets, we show that finetuning a toxicity classifier on our data improves its performance on human-written data substantially. We also demonstrate that ToxiGen can be used to fight machine-generated toxicity as finetuning improves the classifier significantly on our evaluation subset. Our code and data can be found at https://github.com/microsoft/ToxiGen.},
	urldate = {2024-02-24},
	publisher = {arXiv},
	author = {Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
	month = jul,
	year = {2022},
	note = {arXiv:2203.09509 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published as a long paper at ACL 2022. Code: https://github.com/microsoft/TOXIGEN},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/4ZND65RP/Hartvigsen et al. - 2022 - ToxiGen A Large-Scale Machine-Generated Dataset f.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/AGYC7GE2/2203.html:text/html},
}

@misc{noauthor_httpsstoragegoogleapiscomdeepmind-mediagemmagemma-reportpdf_nodate,
	title = {https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf},
	url = {https://storage.googleapis.com/deepmind-media/gemma/gemma-report.pdf},
	urldate = {2024-02-24},
}

@inproceedings{chiron_icdar2017_2017,
	title = {{ICDAR2017} {Competition} on {Post}-{OCR} {Text} {Correction}},
	volume = {01},
	url = {https://ieeexplore.ieee.org/document/8270163},
	doi = {10.1109/ICDAR.2017.232},
	abstract = {This paper describes the ICDAR2017 competition on post-OCR text correction and presents the different methods submitted by the participants. OCR has been an active research field for over the past 30 years but results are still imperfect, especially for historical documents. The purpose of this competition is to compare and evaluate automatic approaches for correcting (denoising) OCR-ed texts. The challenge consists of two independent tasks: 1) error detection and 2) error correction. An original dataset of 12M OCR-ed symbols along with an aligned ground truth was provided to the participants with 80\% of the dataset dedicated to the training and 20\% to the evaluation. Different sources were aggregated and namely contain newspapers and monographs covering 2 languages (English and French). 11 teams submitted results, while the difficulty of the task was underlined by the fact that only half of the submitted methods were able to denoise the evaluation dataset on average. In any case, this competition, which counted 35 registrations, illustrates the strong interest of the community in this essential problem, which is key to any digitization process involving textual data.},
	urldate = {2024-02-24},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Chiron, Guillaume and Doucet, Antoine and Coustaty, Mickaël and Moreux, Jean-Philippe},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Task analysis, Training, Competition, Error correction, Libraries, Measurement, Optical character recognition software, Post-OCR, Text analysis, Text Correction},
	pages = {1423--1428},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/E2UUNSHC/8270163.html:text/html;Submitted Version:/home/jonno/Zotero/storage/42MD5X3G/Chiron et al. - 2017 - ICDAR2017 Competition on Post-OCR Text Correction.pdf:application/pdf},
}

@article{nguyen_ocr_2021,
	title = {{OCR} error correction using correction patterns and self-organizing migrating algorithm},
	volume = {24},
	issn = {1433-755X},
	url = {https://doi.org/10.1007/s10044-020-00936-y},
	doi = {10.1007/s10044-020-00936-y},
	abstract = {Optical character recognition (OCR) systems help to digitize paper-based historical achieves. However, poor quality of scanned documents and limitations of text recognition techniques result in different kinds of errors in OCR outputs. Post-processing is an essential step in improving the output quality of OCR systems by detecting and cleaning the errors. In this paper, we present an automatic model consisting of both error detection and error correction phases for OCR post-processing. We propose a novel approach of OCR post-processing error correction using correction pattern edits and evolutionary algorithm which has been mainly used for solving optimization problems. Our model adopts a variant of the self-organizing migrating algorithm along with a fitness function based on modifications of important linguistic features. We illustrate how to construct the table of correction pattern edits involving all types of edit operations and being directly learned from the training dataset. Through efficient settings of the algorithm parameters, our model can be performed with high-quality candidate generation and error correction. The experimental results show that our proposed approach outperforms various baseline approaches as evaluated on the benchmark dataset of ICDAR 2017 Post-OCR text correction competition.},
	language = {en},
	number = {2},
	urldate = {2024-02-24},
	journal = {Pattern Analysis and Applications},
	author = {Nguyen, Quoc-Dung and Le, Duc-Anh and Phan, Nguyet-Minh and Zelinka, Ivan},
	month = may,
	year = {2021},
	keywords = {Context, Correction pattern, Evolutionary algorithm, N-grams, OCR, Similarity},
	pages = {701--721},
	file = {Full Text PDF:/home/jonno/Zotero/storage/D6IN7MGX/Nguyen et al. - 2021 - OCR error correction using correction patterns and.pdf:application/pdf},
}

@article{hegghammer_ocr_2022,
	title = {{OCR} with {Tesseract}, {Amazon} {Textract}, and {Google} {Document} {AI}: a benchmarking experiment},
	volume = {5},
	issn = {2432-2725},
	shorttitle = {{OCR} with {Tesseract}, {Amazon} {Textract}, and {Google} {Document} {AI}},
	url = {https://doi.org/10.1007/s42001-021-00149-1},
	doi = {10.1007/s42001-021-00149-1},
	abstract = {Optical Character Recognition (OCR) can open up understudied historical documents to computational analysis, but the accuracy of OCR software varies. This article reports a benchmarking experiment comparing the performance of Tesseract, Amazon Textract, and Google Document AI on images of English and Arabic text. English-language book scans (n = 322) and Arabic-language article scans (n = 100) were replicated 43 times with different types of artificial noise for a corpus of 18,568 documents, generating 51,304 process requests. Document AI delivered the best results, and the server-based processors (Textract and Document AI) performed substantially better than Tesseract, especially on noisy documents. Accuracy for English was considerably higher than for Arabic. Specifying the relative performance of three leading OCR products and the differential effects of commonly found noise types can help scholars identify better OCR solutions for their research needs. The test materials have been preserved in the openly available “Noisy OCR Dataset” (NOD) for reuse in future benchmarking studies.},
	language = {en},
	number = {1},
	urldate = {2024-02-24},
	journal = {Journal of Computational Social Science},
	author = {Hegghammer, Thomas},
	month = may,
	year = {2022},
	keywords = {OCR, Benchmarking, Cloud computing},
	pages = {861--882},
	file = {Full Text PDF:/home/jonno/Zotero/storage/3DSNQ8E2/Hegghammer - 2022 - OCR with Tesseract, Amazon Textract, and Google Do.pdf:application/pdf},
}

@inproceedings{soper_bart_2021,
	address = {Online},
	title = {{BART} for {Post}-{Correction} of {OCR} {Newspaper} {Text}},
	url = {https://aclanthology.org/2021.wnut-1.31},
	doi = {10.18653/v1/2021.wnut-1.31},
	abstract = {Optical character recognition (OCR) from newspaper page images is susceptible to noise due to degradation of old documents and variation in typesetting. In this report, we present a novel approach to OCR post-correction. We cast error correction as a translation task, and fine-tune BART, a transformer-based sequence-to-sequence language model pretrained to denoise corrupted text. We are the first to use sentence-level transformer models for OCR post-correction, and our best model achieves a 29.4\% improvement in character accuracy over the original noisy OCR text. Our results demonstrate the utility of pretrained language models for dealing with noisy text.},
	urldate = {2024-02-24},
	booktitle = {Proceedings of the {Seventh} {Workshop} on {Noisy} {User}-generated {Text} ({W}-{NUT} 2021)},
	publisher = {Association for Computational Linguistics},
	author = {Soper, Elizabeth and Fujimoto, Stanley and Yu, Yen-Yun},
	editor = {Xu, Wei and Ritter, Alan and Baldwin, Tim and Rahimi, Afshin},
	month = nov,
	year = {2021},
	pages = {284--290},
	file = {Full Text PDF:/home/jonno/Zotero/storage/CYG5BH4T/Soper et al. - 2021 - BART for Post-Correction of OCR Newspaper Text.pdf:application/pdf},
}

@misc{wikipedia_rivers_2024,
	title = {Rivers of {Blood} speech},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Rivers_of_Blood_speech&oldid=1208509853},
	abstract = {The "Rivers of Blood" speech was made by British Member of Parliament (MP) Enoch Powell on 20 April 1968, to a meeting of the Conservative Political Centre in Birmingham, England. His speech made various remarks, which included strong criticism of significant Commonwealth immigration to the United Kingdom and the proposed Race Relations Act, which made it illegal to refuse housing, employment, or public services to a person on the grounds of colour, race, ethnic or national origins in the country. It became known as the "Rivers of Blood" speech, although Powell always referred to it as "the Birmingham speech". The former name alludes to a prophecy from Virgil's Aeneid which Powell, a former classical scholar, quoted:

As I look ahead, I am filled with foreboding; like the Roman, I seem to see 'the River Tiber foaming with much blood'.
The speech caused a political storm, making Powell one of the most talked about and divisive politicians in the country; it led to his controversial dismissal from the Shadow Cabinet by Conservative Party Leader Edward Heath. According to most accounts, the popularity of Powell's perspective on immigration may have been a decisive factor in the Conservatives' surprise victory in the 1970 general election, although he became one of the most persistent opponents of the subsequent Heath government.},
	language = {en},
	urldate = {2024-02-25},
	journal = {Wikipedia},
	author = {Wikipedia},
	month = feb,
	year = {2024},
	note = {Page Version ID: 1208509853},
	file = {Snapshot:/home/jonno/Zotero/storage/WBTKPVQY/Rivers_of_Blood_speech.html:text/html},
}

@misc{riaa_gold_2022,
	title = {Gold \& {Platinum}},
	url = {https://www.riaa.com/gold-platinum/},
	language = {en-US},
	urldate = {2024-02-25},
	journal = {RIAA},
	author = {RIAA},
	month = oct,
	year = {2022},
	file = {Snapshot:/home/jonno/Zotero/storage/I2EN7PM4/gold-platinum.html:text/html},
}

@misc{b_bodak_nodate,
	title = {Bodak {Yello}},
	author = {B, Cardi},
}

@misc{b_bodak_2017,
	title = {Bodak {Yellow}},
	copyright = {© 2018 Atlantic Recording Corporation for the United States and WEA International Inc. for the world outside of the United States. A Warner Music Group Company},
	language = {English},
	publisher = {Atlantic Records, KSR},
	author = {B, Cardi},
	year = {2017},
}

@book{twain_adventures_1884,
	title = {The {Adventures} of {Huckleberry} {Finn}},
	copyright = {Public domain},
	url = {https://www.gutenberg.org/ebooks/76},
	author = {Twain, Mark},
	year = {1884},
}

@book{lawrence_lady_1927,
	title = {Lady {Chatterly}'s {Lover}},
	isbn = {978-1-4549-5312-8},
	publisher = {Union Square \& Co.},
	author = {Lawrence, D.H},
	year = {1927},
}

@misc{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	doi = {10.48550/arXiv.1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2024-02-25},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv:1301.3781 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/6YP28HLR/Mikolov et al. - 2013 - Efficient Estimation of Word Representations in Ve.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/XDZIJD3Z/1301.html:text/html},
}

@article{jaillant_applying_2023,
	title = {Applying {AI} to digital archives: trust, collaboration and shared professional ethics},
	volume = {38},
	issn = {2055-7671},
	shorttitle = {Applying {AI} to digital archives},
	url = {https://doi.org/10.1093/llc/fqac073},
	doi = {10.1093/llc/fqac073},
	abstract = {Policy makers produce digital records on a daily basis. A selection of records is then preserved in archival repositories. However, getting access to these archival materials is extremely complicated for many reasons—including data protection, sensitivity, national security, and copyright. Artificial Intelligence (AI) can be applied to archives to make them more accessible, but it is still at an experimental stage. While skills gaps contribute to keeping archives ‘dark’, it is also essential to examine issues of mistrust and miscommunication. This article argues that although civil servants, archivists, and academics have similar professional principles articulated through professional codes of ethics, these are not often communicated to each other. This lack of communication leads to feelings of mistrust between stakeholders. Mistrust of technology also contributes to the barriers to effective implementation of AI tools. Therefore, we propose that surfacing the shared professional ethics between stakeholders can contribute to deeper collaborations between humans. In turn, these collaborations can lead to the building of trust in AI systems and tools. The research is informed by semi-structured interviews with thirty government professionals, archivists, historians, digital humanists, and computer scientists. Previous research has largely focused on preservation of digital records, rather than access to these records, and on archivists rather than records creators such as government professionals. This article is the first to examine the application of AI to digital archives as an issue that requires trust and collaboration across the entire archival circle (from record creators to archivists, and from archivists to users).},
	number = {2},
	urldate = {2024-02-26},
	journal = {Digital Scholarship in the Humanities},
	author = {Jaillant, Lise and Rees, Arran},
	month = jun,
	year = {2023},
	pages = {571--585},
	file = {Full Text PDF:/home/jonno/Zotero/storage/UK3DRPTG/Jaillant and Rees - 2023 - Applying AI to digital archives trust, collaborat.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/DPDYCX8C/6832097.html:text/html},
}

@misc{noauthor_post-ocr_nodate,
	title = {Post-{OCR} {Text} {Correction} - {Dataset}},
	url = {https://sites.google.com/view/icdar2017-postcorrectionocr/dataset},
	abstract = {Following an agreement with the BnF (French National Library) and in accordance with copyrights, we propose for the competition a dataset that is a subpart of the corpus collected in the context of the AmeliOCR project, led by the L3i laboratory (University of La Rochelle, France) and the BnF.
The},
	language = {en-GB},
	urldate = {2024-02-28},
	file = {Snapshot:/home/jonno/Zotero/storage/CSI54WV5/dataset.html:text/html},
}

@misc{noauthor_post-ocr_nodate-1,
	title = {Post-{OCR} {Text} {Correction}},
	url = {https://sites.google.com/view/icdar2017-postcorrectionocr/home},
	abstract = {The accuracy of Optical Character Recognition (OCR) technologies considerably impacts the way digital documents are indexed, accessed and exploited. During the last decades, OCR engines have been constantly improving and are today able to return exploitable results on mainstream documents. But in},
	language = {en-GB},
	urldate = {2024-02-28},
	file = {Snapshot:/home/jonno/Zotero/storage/J4FPFTB9/home.html:text/html},
}

@inproceedings{chiron_icdar2017_2017-1,
	title = {{ICDAR2017} {Competition} on {Post}-{OCR} {Text} {Correction}},
	volume = {01},
	url = {https://ieeexplore.ieee.org/document/8270163},
	doi = {10.1109/ICDAR.2017.232},
	abstract = {This paper describes the ICDAR2017 competition on post-OCR text correction and presents the different methods submitted by the participants. OCR has been an active research field for over the past 30 years but results are still imperfect, especially for historical documents. The purpose of this competition is to compare and evaluate automatic approaches for correcting (denoising) OCR-ed texts. The challenge consists of two independent tasks: 1) error detection and 2) error correction. An original dataset of 12M OCR-ed symbols along with an aligned ground truth was provided to the participants with 80\% of the dataset dedicated to the training and 20\% to the evaluation. Different sources were aggregated and namely contain newspapers and monographs covering 2 languages (English and French). 11 teams submitted results, while the difficulty of the task was underlined by the fact that only half of the submitted methods were able to denoise the evaluation dataset on average. In any case, this competition, which counted 35 registrations, illustrates the strong interest of the community in this essential problem, which is key to any digitization process involving textual data.},
	urldate = {2024-02-28},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Chiron, Guillaume and Doucet, Antoine and Coustaty, Mickaël and Moreux, Jean-Philippe},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Task analysis, Training, Competition, Error correction, Libraries, Measurement, Optical character recognition software, Post-OCR, Text analysis, Text Correction},
	pages = {1423--1428},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/R86JMVXK/8270163.html:text/html;Submitted Version:/home/jonno/Zotero/storage/PGQVNWXW/Chiron et al. - 2017 - ICDAR2017 Competition on Post-OCR Text Correction.pdf:application/pdf},
}

@misc{noauthor_post-ocr_nodate-2,
	title = {Post-{OCR} {Text} {Correction} - {Challenge}},
	url = {https://sites.google.com/view/icdar2017-postcorrectionocr/challenge},
	abstract = {This competition invites researchers from any field that can be applied to document analysis (e.g. natural language processing, data analysis, text data mining...) to challenge their method(s) for improving/denoising OCR-ed texts, on a testbed of more than 12 million characters. Given the noisy},
	language = {en-GB},
	urldate = {2024-02-29},
	file = {Snapshot:/home/jonno/Zotero/storage/UGWSPVSD/challenge.html:text/html},
}

@inproceedings{chiron_icdar2017_2017-2,
	title = {{ICDAR2017} {Competition} on {Post}-{OCR} {Text} {Correction}},
	volume = {01},
	url = {https://ieeexplore.ieee.org/document/8270163},
	doi = {10.1109/ICDAR.2017.232},
	abstract = {This paper describes the ICDAR2017 competition on post-OCR text correction and presents the different methods submitted by the participants. OCR has been an active research field for over the past 30 years but results are still imperfect, especially for historical documents. The purpose of this competition is to compare and evaluate automatic approaches for correcting (denoising) OCR-ed texts. The challenge consists of two independent tasks: 1) error detection and 2) error correction. An original dataset of 12M OCR-ed symbols along with an aligned ground truth was provided to the participants with 80\% of the dataset dedicated to the training and 20\% to the evaluation. Different sources were aggregated and namely contain newspapers and monographs covering 2 languages (English and French). 11 teams submitted results, while the difficulty of the task was underlined by the fact that only half of the submitted methods were able to denoise the evaluation dataset on average. In any case, this competition, which counted 35 registrations, illustrates the strong interest of the community in this essential problem, which is key to any digitization process involving textual data.},
	urldate = {2024-02-29},
	booktitle = {2017 14th {IAPR} {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Chiron, Guillaume and Doucet, Antoine and Coustaty, Mickaël and Moreux, Jean-Philippe},
	month = nov,
	year = {2017},
	note = {ISSN: 2379-2140},
	keywords = {Task analysis, Training, Competition, Error correction, Libraries, Measurement, Optical character recognition software, Post-OCR, Text analysis, Text Correction},
	pages = {1423--1428},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/WIAKIK6K/8270163.html:text/html;Submitted Version:/home/jonno/Zotero/storage/D9SM4ACY/Chiron et al. - 2017 - ICDAR2017 Competition on Post-OCR Text Correction.pdf:application/pdf},
}

@incollection{dillane_researching_2018,
	title = {Researching a periodical genre: {Classifications}, codes, and relational terms},
	isbn = {978-1-315-60561-6},
	shorttitle = {Researching a periodical genre},
	abstract = {This chapter addresses how the efforts to unpack George Eliot's periodical work for the Westminster Review were enabled by methodological questions that turned on the issue of genre. The slippage between "form" and "genre" illustrates why nomenclature is an issue in any discussion of researching a periodical genre. The periodical presents multiple, interlocking genre factors that relate to differentiated texts, such as poems, reviews, or letters to the editor, and to the capacious genre in which the work appears, the periodical. This genre admixture presents difficulties for researchers investigating a periodical genre, and it is one reason why a genre approach to periodical studies is sometimes controversial. A genre approach calls for particular attention to the repetitive structures, paratexts, editorial conventions, and patterns of organization, which together with thematic content and interacting genre conditions constitute key aspects of the periodical's identity as a form that is distinct from other print materials.},
	booktitle = {Researching the {Nineteenth}-{Century} {Periodical} {Press}},
	publisher = {Routledge},
	author = {Dillane, Fionnuala},
	year = {2018},
	note = {Num Pages: 17},
}

@incollection{bateman_mapping_2006,
	title = {Mapping the {Multimodal} {Genres} of {Traditional} and {Electronic} {Newspapers}},
	isbn = {978-0-203-35777-4},
	abstract = {This chapter is concerned with the application of the functional linguistic concept of ‘genre’ to multimodal documents—that is, to documents whose total meaning is constructed out of combinations of text, layout, and graphics/images. Although the term genre has been used with respect to multi-modal documents for many years, this use has generally relied on the everyday dictionary sense of the term as inherited, in an increasingly weakened form, from traditions of literary analysis and rhetoric. Here we relate genre to the more formally specified and technical usage of linguistics and, particularly, of functional linguistics. We show how the phenomenon of multi-modality requires an extension of the term, not only over and above traditional usages but also with respect to the more technical constructs hitherto employed within linguistics. Our particular motivation in attempting this extension in the meaning of genre is as follows. Genre within functional linguistics attempts to provide a theoretical mechanism that is both predictive and explanatory. Membership of some text in a genre allows firm predictions to be made concerning the linguistic details of that text and, conversely, the occurrence of particular combinations of linguistic features can in turn be a strong indicator of genre membership. This, coupled with the now common interpretation of genre as a socially significant activity, provides a theoretically and practically useful link between social context and language.},
	booktitle = {New {Directions} in the {Analysis} of {Multimodal} {Discourse}},
	publisher = {Routledge},
	author = {Bateman, John and Delin, Judy and Henschel, Renate},
	year = {2006},
	note = {Num Pages: 26},
}

@inproceedings{crowston_framework_2004,
	title = {A framework for creating a facetted classification for genres: addressing issues of multidimensionality},
	shorttitle = {A framework for creating a facetted classification for genres},
	url = {https://ieeexplore.ieee.org/document/1265268},
	doi = {10.1109/HICSS.2004.1265268},
	abstract = {People recognize and use document genres as a way of identifying useful information and of participating in mutually understood communicative acts. Crowston and Kwasnik discuss the possibility of improving information access in large digital collections through the identification and use of document genre metadata. They draw on the definition of genre proposed by Orlikowski and Yates (1994), who describe genre as "a distinctive type of communicative action, characterized by a socially recognized communicative purpose and common aspects of form". Scholars infields such as rhetoric and library science have attempted to describe and systematize the notion of genre, and have offered many different definitions of genre. We like Orlikowski and Yates's definition because it takes into account all three aspects of genre that we recognize as fundamental: content, form, and purpose. A document's genre is a subtle and complex concept in which the content and form of a document are fused with its purpose or function. As such, a document's genre cannot be separated from the context in which it is used; the same document may be construed as being of a different genre depending on how it is invoked in a given situation. Starting from the document, a letter may be a personal communication, or a piece of evidence in a court of law, or an agreement, or even a work of art. Starting from the situation, we note that differences in an information situation are often reflected in the kind of document that is considered helpful (e.g., a problem set vs. a lesson plan vs. a tutorial about mathematics, for instance). Thus, we see genre as a multidimensional phenomenon, which takes into account not only the attributes of the document itself, but also of its role in human endeavor. In this paper, we discuss some considerations in developing a facetted classification for genres to address the problem of multi-dimensionality.},
	urldate = {2024-03-06},
	booktitle = {37th {Annual} {Hawaii} {International} {Conference} on {System} {Sciences}, 2004. {Proceedings} of the},
	author = {Crowston, K. and Kwasnik, B.H.},
	month = jan,
	year = {2004},
	keywords = {Libraries, Context, Art, Character recognition, Humans, Information retrieval, Mathematics, Multidimensional systems, Rhetoric},
	pages = {9 pp.--},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/8EIGRYB4/1265268.html:text/html;IEEE Xplore Full Text PDF:/home/jonno/Zotero/storage/8RMLTBB9/Crowston and Kwasnik - 2004 - A framework for creating a facetted classification.pdf:application/pdf},
}

@inproceedings{lee_text_2002,
	address = {New York, NY, USA},
	series = {{SIGIR} '02},
	title = {Text genre classification with genre-revealing and subject-revealing features},
	isbn = {978-1-58113-561-9},
	url = {https://doi.org/10.1145/564376.564403},
	doi = {10.1145/564376.564403},
	abstract = {Subject or prepositional content has been the focus of most classification research. Genre or style, on the other hand, is a different and important property of text, and automatic text genre classification is becoming important for classification and retrieval purposes as well as for some natural language processing research. In this paper, we present a method for automatic genre classification that is based on statistically selected features obtained from both subject-classified and genre classified training data. The experimental results show that the proposed method outperforms a direct application of a statistical learner often used for subject classification. We also observe that the deviation formula and discrimination formula using document frequency ratios also work as expected. We conjecture that this dual feature set approach can be generalized to improve the performance of subject classification as well.},
	urldate = {2024-03-06},
	booktitle = {Proceedings of the 25th annual international {ACM} {SIGIR} conference on {Research} and development in information retrieval},
	publisher = {Association for Computing Machinery},
	author = {Lee, Yong-Bae and Myaeng, Sung Hyon},
	month = aug,
	year = {2002},
	keywords = {classification, idf, statistical method, subject categories, text genre, tf},
	pages = {145--150},
}

@article{kuzman_assessing_2022,
	title = {Assessing {Comparability} of {Genre} {Datasets} via {Cross}-{Lingual} and {Cross}-{Dataset} {Experiments}},
	abstract = {This article explores comparability of an English and a Slovene genre-annotated dataset via monolingual and cross-lingual experiments, performed with two Transformer models. In addition, we analyze whether translating the Slovene dataset into English with a machine translation system improves monolingual and cross-lingual performance. Results show that cross-lingual transfer is possible despite the differences between the datasets in terms of genre schemata and corpora construction methods. Furthermore, the XLM-RoBERTa model was shown to provide good results in both settings already when learning on less than 1,000 instances. In contrast, the trilingual CroSloEngual BERT model was revealed to be less suitable for this text classification task. Moreover, the results reveal that although the English dataset is 40 times larger than the Slovene dataset, it provides similar or worse classification results.},
	language = {en},
	journal = {Digital Humanities},
	author = {Kuzman, Taja and Ljubesˇic, Nikola and Pollak, Senja},
	year = {2022},
	file = {Kuzman et al. - 2022 - Assessing Comparability of Genre Datasets via Cros.pdf:/home/jonno/Zotero/storage/MNDJ8NHB/Kuzman et al. - 2022 - Assessing Comparability of Genre Datasets via Cros.pdf:application/pdf},
}

@article{zhu_research_2022,
	title = {The {Research} {Trends} of {Text} {Classification} {Studies} (2000–2020): {A} {Bibliometric} {Analysis}},
	volume = {12},
	issn = {2158-2440},
	shorttitle = {The {Research} {Trends} of {Text} {Classification} {Studies} (2000–2020)},
	url = {https://doi.org/10.1177/21582440221089963},
	doi = {10.1177/21582440221089963},
	abstract = {Text Classification (TC) is the process of assigning several different categories to a set of texts. This study aims to evaluate the state of the arts of TC studies. Firstly, TC-related publications indexed in Web of Science were selected as data. In total, 3,121 TC-related publications were published in 760 journals between 2000 and 2020. Then, the bibliographic information was mined to identify the publication trends, important contributors, publication venues, and involved disciplines. Besides, a thematic analysis was performed to extract topics with increasing/decreasing popularity. The findings showed that TC has become a fast-growing interdisciplinary area, and that emerging research powers such as China are playing increasingly important roles in TC research. Moreover, the thematic analysis showed increased interest in topics concerning advanced classification algorithms, performance evaluation methods, and the practical applications of TC. This study will help researchers recognize the recent trends in the area.},
	language = {en},
	number = {2},
	urldate = {2024-03-06},
	journal = {SAGE Open},
	author = {Zhu, Haoran and Lei, Lei},
	month = apr,
	year = {2022},
	note = {Publisher: SAGE Publications},
	pages = {21582440221089963},
	file = {SAGE PDF Full Text:/home/jonno/Zotero/storage/8AFSR6LN/Zhu and Lei - 2022 - The Research Trends of Text Classification Studies.pdf:application/pdf},
}

@article{kuzman_automatic_2023,
	title = {Automatic genre identification: a survey},
	issn = {1574-0218},
	shorttitle = {Automatic genre identification},
	url = {https://doi.org/10.1007/s10579-023-09695-8},
	doi = {10.1007/s10579-023-09695-8},
	abstract = {Automatic genre identification (AGI) is a text classification task focused on genres, i.e., text categories defined by the author’s purpose, common function of the text, and the text’s conventional form. Obtaining genre information has been shown to be beneficial for a wide range of disciplines, including linguistics, corpus linguistics, computational linguistics, natural language processing, information retrieval and information security. Consequently, in the past 20 years, numerous researchers have collected genre datasets with the aim to develop an efficient genre classifier. However, their approaches to the definition of genre schemata, data collection and manual annotation vary substantially, resulting in significantly different datasets. As most AGI experiments are dataset-dependent, a sufficient understanding of the differences between the available genre datasets is of great importance for the researchers venturing into this area. In this paper, we present a detailed overview of different approaches to each of the steps of the AGI task, from the definition of the genre concept and the genre schema, to the dataset collection and annotation methods, and, finally, to machine learning strategies. Special focus is dedicated to the description of the most relevant genre schemata and datasets, and details on the availability of all of the datasets are provided. In addition, the paper presents the recent advances in machine learning approaches to automatic genre identification, and concludes with proposing the directions towards developing a stable multilingual genre classifier.},
	language = {en},
	urldate = {2024-03-06},
	journal = {Language Resources and Evaluation},
	author = {Kuzman, Taja and Ljubešić, Nikola},
	month = nov,
	year = {2023},
	keywords = {Automatic genre identification, Genre datasets, Genre schemata, Survey paper, Text genre, Web genre},
	file = {Full Text PDF:/home/jonno/Zotero/storage/9Y33XNSB/Kuzman and Ljubešić - 2023 - Automatic genre identification a survey.pdf:application/pdf},
}

@article{jimenez_review_2022,
	title = {A review of ship energy efficiency research and directions towards emission reduction in the maritime industry},
	volume = {366},
	issn = {0959-6526},
	url = {https://www.sciencedirect.com/science/article/pii/S0959652622024817},
	doi = {10.1016/j.jclepro.2022.132888},
	abstract = {To improve energy efficiency of ships, initiatives have been taken by the international and regional regulatory bodies, ship owners, shipping lines, and other involved stakeholders. While existing literature has investigated ship energy efficiency from different perspectives, a comprehensive review on measures, practices, policies, feasibility, and alternatives remains absent. This study presents a retrospective review of ship energy efficiency literature by utilizing a hybrid review method that combines bibliometric and content analysis approaches. For bibliometric analysis, the bibliometrix package in the R software was used in combination with the VOSviewer software. The outcomes of bibliometric analysis are the most ranked articles, journals, authors, and institutions related to ship energy efficiency research. The bibliographic coupling analysis identified five research clusters: (1) decarbonization and emission reduction measures, (2) speed management, (3) policy and regulations, (4) economic and organizational factors, and (5) alternative energy sources. Future research directions are proposed for each cluster.},
	urldate = {2024-03-07},
	journal = {Journal of Cleaner Production},
	author = {Jimenez, Veronica Jaramillo and Kim, Hyungju and Munim, Ziaul Haque},
	month = sep,
	year = {2022},
	keywords = {Bibliometric analysis, Decarbonization, Fuel consumption, Ship energy efficiency, Shipping industry},
	pages = {132888},
	file = {ScienceDirect Snapshot:/home/jonno/Zotero/storage/4SLKLHZC/S0959652622024817.html:text/html},
}

@article{noauthor_era_2024,
	title = {The {Era} of 1-bit {LLMs}: {All} {Large} {Language} {Models} are in 1.58 {Bits}},
	shorttitle = {Paper page - {The} {Era} of 1-bit {LLMs}},
	url = {https://huggingface.co/papers/2402.17764},
	abstract = {Join the discussion on this paper page},
	urldate = {2024-03-08},
	month = feb,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/AAQSRW3A/2402.html:text/html},
}

@misc{dettmers_llmint8_2022,
	title = {{LLM}.int8(): 8-bit {Matrix} {Multiplication} for {Transformers} at {Scale}},
	shorttitle = {{LLM}.int8()},
	url = {http://arxiv.org/abs/2208.07339},
	abstract = {Large language models have been widely adopted but require signiﬁcant GPU memory for inference. We develop a procedure for Int8 matrix multiplication for feed-forward and attention projection layers in transformers, which cut the memory needed for inference by half while retaining full precision performance. With our method, a 175B parameter 16/32-bit checkpoint can be loaded, converted to Int8, and used immediately without performance degradation. This is made possible by understanding and working around properties of highly systematic emergent features in transformer language models that dominate attention and transformer predictive performance. To cope with these features, we develop a two-part quantization procedure, LLM.int8(). We ﬁrst use vector-wise quantization with separate normalization constants for each inner product in the matrix multiplication, to quantize most of the features. However, for the emergent outliers, we also include a new mixed-precision decomposition scheme, which isolates the outlier feature dimensions into a 16-bit matrix multiplication while still more than 99.9\% of values are multiplied in 8-bit. Using LLM.int8(), we show empirically it is possible to perform inference in LLMs with up to 175B parameters without any performance degradation. This result makes such models much more accessible, for example making it possible to use OPT-175B/BLOOM on a single server with consumer GPUs. We open source our software.},
	language = {en},
	urldate = {2024-03-08},
	publisher = {arXiv},
	author = {Dettmers, Tim and Lewis, Mike and Belkada, Younes and Zettlemoyer, Luke},
	month = nov,
	year = {2022},
	note = {arXiv:2208.07339 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	annote = {Comment: Published at NeurIPS 2022. Camera-ready version},
	file = {Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf:/home/jonno/Zotero/storage/K5GFT6QT/Dettmers et al. - 2022 - LLM.int8() 8-bit Matrix Multiplication for Transf.pdf:application/pdf},
}

@article{boros_post-correction_2024,
	title = {Post-correction of {Historical} {Text} {Transcripts} with {Large} {Language} {Models}: {An} {Exploratory} {Study}},
	shorttitle = {Post-correction of {Historical} {Text} {Transcripts} with {Large} {Language} {Models}},
	abstract = {The quality of automatic transcription of heritage documents, whether from printed, manuscripts or audio sources, has a decisive impact on the ability to search and process historical texts. Although significant progress has been made in text recognition (OCR, HTR, ASR), textual materials derived from library and archive collections remain largely erroneous and noisy. Effective post-transcription correction methods are therefore necessary and have been intensively researched for many years. As large language models (LLMs) have recently shown exceptional performances in a variety of text-related tasks, we investigate their ability to amend poor historical transcriptions. We evaluate fourteen foundation language models against various post-correction benchmarks comprising different languages, time periods and document types, as well as different transcription quality and origins. We compare the performance of different model sizes and different prompts of increasing complexity in zero and few-shot settings. Our evaluation shows that LLMs are anything but efficient at this task. Quantitative and qualitative analyses of results allow us to share valuable insights for future work on post-correcting historical texts with LLMs},
	journal = {LaTeCH-CLfL 2024},
	author = {Boros, Emanuela and Ehrmann, Maud and Romanello, Matteo and Najem-Meyer, Sven and Kaplan, Frédéric},
	year = {2024},
	note = {ISBN: 9798891760691
Meeting Name: The 8th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature
Publisher: The Association for Computational Linguistics},
	keywords = {evaluation, historical texts, large language models, OCR post-correction},
}

@misc{zhou_universalner_2024,
	title = {{UniversalNER}: {Targeted} {Distillation} from {Large} {Language} {Models} for {Open} {Named} {Entity} {Recognition}},
	shorttitle = {{UniversalNER}},
	url = {http://arxiv.org/abs/2308.03279},
	abstract = {Large language models (LLMs) have demonstrated remarkable generalizability, such as understanding arbitrary entities and relations. Instruction tuning has proven effective for distilling LLMs into more cost-efficient models such as Alpaca and Vicuna. Yet such student models still trail the original LLMs by large margins in downstream applications. In this paper, we explore targeted distillation with mission-focused instruction tuning to train student models that can excel in a broad application class such as open information extraction. Using named entity recognition (NER) for case study, we show how ChatGPT can be distilled into much smaller UniversalNER models for open NER. For evaluation, we assemble the largest NER benchmark to date, comprising 43 datasets across 9 diverse domains such as biomedicine, programming, social media, law, finance. Without using any direct supervision, UniversalNER attains remarkable NER accuracy across tens of thousands of entity types, outperforming general instruction-tuned models such as Alpaca and Vicuna by over 30 absolute F1 points in average. With a tiny fraction of parameters, UniversalNER not only acquires ChatGPT's capability in recognizing arbitrary entity types, but also outperforms its NER accuracy by 7-9 absolute F1 points in average. Remarkably, UniversalNER even outperforms by a large margin state-of-the-art multi-task instruction-tuned systems such as InstructUIE, which uses supervised NER examples. We also conduct thorough ablation studies to assess the impact of various components in our distillation approach. We release the distillation recipe, data, and UniversalNER models to facilitate future research on targeted distillation.},
	language = {en},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Zhou, Wenxuan and Zhang, Sheng and Gu, Yu and Chen, Muhao and Poon, Hoifung},
	month = jan,
	year = {2024},
	note = {arXiv:2308.03279 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at ICLR 2024. Project page: https://universal-ner.github.io/},
	file = {Zhou et al. - 2024 - UniversalNER Targeted Distillation from Large Lan.pdf:/home/jonno/Zotero/storage/RXW67ETJ/Zhou et al. - 2024 - UniversalNER Targeted Distillation from Large Lan.pdf:application/pdf},
}

@misc{sanh_distilbert_2020,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/KHPIX48K/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/IUY5TE52/1910.html:text/html},
}

@misc{hinton_distilling_2015,
	title = {Distilling the {Knowledge} in a {Neural} {Network}},
	url = {http://arxiv.org/abs/1503.02531},
	doi = {10.48550/arXiv.1503.02531},
	abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow deployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators have shown that it is possible to compress the knowledge in an ensemble into a single model which is much easier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full models confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
	urldate = {2024-03-22},
	publisher = {arXiv},
	author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	month = mar,
	year = {2015},
	note = {arXiv:1503.02531 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: NIPS 2014 Deep Learning Workshop},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/SDU3UF8Z/Hinton et al. - 2015 - Distilling the Knowledge in a Neural Network.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/LU6CU5KD/1503.html:text/html},
}

@misc{zhang_instruction_2024,
	title = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10792},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of (INSTRUCTION, OUTPUT) pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users’ objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and application, along with analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research.},
	language = {en},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	month = mar,
	year = {2024},
	note = {arXiv:2308.10792 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: V2; Last update: March 12, 2024},
	file = {Zhang et al. - 2024 - Instruction Tuning for Large Language Models A Su.pdf:/home/jonno/Zotero/storage/MF2KD5NL/Zhang et al. - 2024 - Instruction Tuning for Large Language Models A Su.pdf:application/pdf},
}

@misc{yuan_evaluating_2023,
	title = {Evaluating {Instruction}-{Tuned} {Large} {Language} {Models} on {Code} {Comprehension} and {Generation}},
	url = {http://arxiv.org/abs/2308.01240},
	abstract = {Instruction tuning has been proposed to enhance the generalization capability of large language models (LLMs) on new downstream tasks. To date, many efforts have been dedicated into evaluating instructed LLMs, covering not only general NLP tasks but also specific domains. However, little evaluation of instructed LLMs is diving into the software engineering domain, except the NL-to-Code task (generating a function for the given natural language description), which is only one of the coderelated tasks in software development and maintenance. Although some recent work explores the capability of the instructed models such as ChatGPT on SE tasks, these commercial models are closed-source, thus lacking transparency and reproducibility. Overall, it still remains unclear how the recent open-source instructed LLMs perform on diverse code comprehension and generation tasks.},
	language = {en},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Yuan, Zhiqiang and Liu, Junwei and Zi, Qiancheng and Liu, Mingwei and Peng, Xin and Lou, Yiling},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Yuan et al. - 2023 - Evaluating Instruction-Tuned Large Language Models.pdf:/home/jonno/Zotero/storage/QTGAMZ2N/Yuan et al. - 2023 - Evaluating Instruction-Tuned Large Language Models.pdf:application/pdf},
}

@misc{noauthor_httpsarxivorgpdf230801240pdf_nodate,
	title = {https://arxiv.org/pdf/2308.01240.pdf},
	url = {https://arxiv.org/pdf/2308.01240.pdf},
	urldate = {2024-03-23},
}

@misc{zhang_balancing_2023,
	title = {Balancing {Specialized} and {General} {Skills} in {LLMs}: {The} {Impact} of {Modern} {Tuning} and {Data} {Strategy}},
	shorttitle = {Balancing {Specialized} and {General} {Skills} in {LLMs}},
	url = {http://arxiv.org/abs/2310.04945},
	doi = {10.48550/arXiv.2310.04945},
	abstract = {This paper introduces a multifaceted methodology for fine-tuning and evaluating large language models (LLMs) for specialized monetization tasks. The goal is to balance general language proficiency with domain-specific skills. The methodology has three main components: 1) Carefully blending in-domain and general-purpose data during fine-tuning to achieve an optimal balance between general and specialized capabilities; 2) Designing a comprehensive evaluation framework with 45 questions tailored to assess performance on functionally relevant dimensions like reliability, consistency, and business impact; 3) Analyzing how model size and continual training influence metrics to guide efficient resource allocation during fine-tuning. The paper details the design, data collection, analytical techniques, and results validating the proposed frameworks. It aims to provide businesses and researchers with actionable insights on effectively adapting LLMs for specialized contexts. We also intend to make public the comprehensive evaluation framework, which includes the 45 tailored questions and their respective scoring guidelines, to foster transparency and collaboration in adapting LLMs for specialized tasks.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Zhang, Zheng and Zheng, Chen and Tang, Da and Sun, Ke and Ma, Yukun and Bu, Yingtong and Zhou, Xun and Zhao, Liang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04945 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/9FEPNLNC/Zhang et al. - 2023 - Balancing Specialized and General Skills in LLMs .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/NDJH4TR5/2310.html:text/html},
}

@misc{zhang_instruction_2024-1,
	title = {Instruction {Tuning} for {Large} {Language} {Models}: {A} {Survey}},
	shorttitle = {Instruction {Tuning} for {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2308.10792},
	doi = {10.48550/arXiv.2308.10792},
	abstract = {This paper surveys research works in the quickly advancing field of instruction tuning (IT), a crucial technique to enhance the capabilities and controllability of large language models (LLMs). Instruction tuning refers to the process of further training LLMs on a dataset consisting of {\textbackslash}textsc\{(instruction, output)\} pairs in a supervised fashion, which bridges the gap between the next-word prediction objective of LLMs and the users' objective of having LLMs adhere to human instructions. In this work, we make a systematic review of the literature, including the general methodology of IT, the construction of IT datasets, the training of IT models, and applications to different modalities, domains and applications, along with an analysis on aspects that influence the outcome of IT (e.g., generation of instruction outputs, size of the instruction dataset, etc). We also review the potential pitfalls of IT along with criticism against it, along with efforts pointing out current deficiencies of existing strategies and suggest some avenues for fruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and Wang, Guoyin},
	month = mar,
	year = {2024},
	note = {arXiv:2308.10792 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: V2; Last update: March 12, 2024},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/DYY35JTY/Zhang et al. - 2024 - Instruction Tuning for Large Language Models A Su.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/SI47CLEJ/2308.html:text/html},
}

@misc{zhang_balancing_2023-1,
	title = {Balancing {Specialized} and {General} {Skills} in {LLMs}: {The} {Impact} of {Modern} {Tuning} and {Data} {Strategy}},
	shorttitle = {Balancing {Specialized} and {General} {Skills} in {LLMs}},
	url = {http://arxiv.org/abs/2310.04945},
	doi = {10.48550/arXiv.2310.04945},
	abstract = {This paper introduces a multifaceted methodology for fine-tuning and evaluating large language models (LLMs) for specialized monetization tasks. The goal is to balance general language proficiency with domain-specific skills. The methodology has three main components: 1) Carefully blending in-domain and general-purpose data during fine-tuning to achieve an optimal balance between general and specialized capabilities; 2) Designing a comprehensive evaluation framework with 45 questions tailored to assess performance on functionally relevant dimensions like reliability, consistency, and business impact; 3) Analyzing how model size and continual training influence metrics to guide efficient resource allocation during fine-tuning. The paper details the design, data collection, analytical techniques, and results validating the proposed frameworks. It aims to provide businesses and researchers with actionable insights on effectively adapting LLMs for specialized contexts. We also intend to make public the comprehensive evaluation framework, which includes the 45 tailored questions and their respective scoring guidelines, to foster transparency and collaboration in adapting LLMs for specialized tasks.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Zhang, Zheng and Zheng, Chen and Tang, Da and Sun, Ke and Ma, Yukun and Bu, Yingtong and Zhou, Xun and Zhao, Liang},
	month = oct,
	year = {2023},
	note = {arXiv:2310.04945 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/6P62Z6YN/Zhang et al. - 2023 - Balancing Specialized and General Skills in LLMs .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/WIZM7USR/2310.html:text/html},
}

@misc{sanh_distilbert_2020-1,
	title = {{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	shorttitle = {{DistilBERT}, a distilled version of {BERT}},
	url = {http://arxiv.org/abs/1910.01108},
	doi = {10.48550/arXiv.1910.01108},
	abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
	urldate = {2024-03-23},
	publisher = {arXiv},
	author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	month = feb,
	year = {2020},
	note = {arXiv:1910.01108 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: February 2020 - Revision: fix bug in evaluation metrics, updated metrics, argumentation unchanged. 5 pages, 1 figure, 4 tables. Accepted at the 5th Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS 2019},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/Y2WNX82W/Sanh et al. - 2020 - DistilBERT, a distilled version of BERT smaller, .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/RPQY2D9X/1910.html:text/html},
}

@misc{ai_yi_2024,
	title = {Yi: {Open} {Foundation} {Models} by 01.{AI}},
	shorttitle = {Yi},
	url = {http://arxiv.org/abs/2403.04652},
	doi = {10.48550/arXiv.2403.04652},
	abstract = {We introduce the Yi model family, a series of language and multimodal models that demonstrate strong multi-dimensional capabilities. The Yi model family is based on 6B and 34B pretrained language models, then we extend them to chat models, 200K long context models, depth-upscaled models, and vision-language models. Our base models achieve strong performance on a wide range of benchmarks like MMLU, and our finetuned chat models deliver strong human preference rate on major evaluation platforms like AlpacaEval and Chatbot Arena. Building upon our scalable super-computing infrastructure and the classical transformer architecture, we attribute the performance of Yi models primarily to its data quality resulting from our data-engineering efforts. For pretraining, we construct 3.1 trillion tokens of English and Chinese corpora using a cascaded data deduplication and quality filtering pipeline. For finetuning, we polish a small scale (less than 10K) instruction dataset over multiple iterations such that every single instance has been verified directly by our machine learning engineers. For vision-language, we combine the chat language model with a vision transformer encoder and train the model to align visual representations to the semantic space of the language model. We further extend the context length to 200K through lightweight continual pretraining and demonstrate strong needle-in-a-haystack retrieval performance. We show that extending the depth of the pretrained checkpoint through continual pretraining further improves performance. We believe that given our current results, continuing to scale up model parameters using thoroughly optimized data will lead to even stronger frontier models.},
	urldate = {2024-04-04},
	publisher = {arXiv},
	author = {AI, 01 and Young, Alex and Chen, Bei and Li, Chao and Huang, Chengen and Zhang, Ge and Zhang, Guanwei and Li, Heng and Zhu, Jiangcheng and Chen, Jianqun and Chang, Jing and Yu, Kaidong and Liu, Peng and Liu, Qiang and Yue, Shawn and Yang, Senbin and Yang, Shiming and Yu, Tao and Xie, Wen and Huang, Wenhao and Hu, Xiaohui and Ren, Xiaoyi and Niu, Xinyao and Nie, Pengcheng and Xu, Yuchi and Liu, Yudong and Wang, Yue and Cai, Yuxuan and Gu, Zhenyu and Liu, Zhiyuan and Dai, Zonghong},
	month = mar,
	year = {2024},
	note = {arXiv:2403.04652 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/MPATH2F8/AI et al. - 2024 - Yi Open Foundation Models by 01.AI.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/TVPG778I/2403.html:text/html},
}

@misc{gunasekar_textbooks_2023,
	title = {Textbooks {Are} {All} {You} {Need}},
	url = {http://arxiv.org/abs/2306.11644},
	doi = {10.48550/arXiv.2306.11644},
	abstract = {We introduce phi-1, a new large language model for code, with significantly smaller size than competing models: phi-1 is a Transformer-based model with 1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook quality" data from the web (6B tokens) and synthetically generated textbooks and exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains pass@1 accuracy 50.6\% on HumanEval and 55.5\% on MBPP. It also displays surprising emergent properties compared to phi-1-base, our model before our finetuning stage on a dataset of coding exercises, and phi-1-small, a smaller model with 350M parameters trained with the same pipeline as phi-1 that still achieves 45\% on HumanEval.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio César Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and Salim, Adil and Shah, Shital and Behl, Harkirat Singh and Wang, Xin and Bubeck, Sébastien and Eldan, Ronen and Kalai, Adam Tauman and Lee, Yin Tat and Li, Yuanzhi},
	month = oct,
	year = {2023},
	note = {arXiv:2306.11644 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 26 pages; changed color scheme of plot. fixed minor typos and added couple clarifications},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/GM9LE9GT/Gunasekar et al. - 2023 - Textbooks Are All You Need.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/BDGR98M2/2306.html:text/html},
}

@misc{hoffmann_training_2022,
	title = {Training {Compute}-{Optimal} {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2203.15556},
	doi = {10.48550/arXiv.2203.15556},
	abstract = {We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4\${\textbackslash}times\$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5\% on the MMLU benchmark, greater than a 7\% improvement over Gopher.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and Hennigan, Tom and Noland, Eric and Millican, Katie and Driessche, George van den and Damoc, Bogdan and Guy, Aurelia and Osindero, Simon and Simonyan, Karen and Elsen, Erich and Rae, Jack W. and Vinyals, Oriol and Sifre, Laurent},
	month = mar,
	year = {2022},
	note = {arXiv:2203.15556 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/GXQFT93I/Hoffmann et al. - 2022 - Training Compute-Optimal Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/TAIUWXFQ/2203.html:text/html},
}

@misc{kaplan_scaling_2020,
	title = {Scaling {Laws} for {Neural} {Language} {Models}},
	url = {http://arxiv.org/abs/2001.08361},
	doi = {10.48550/arXiv.2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2024-04-08},
	publisher = {arXiv},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv:2001.08361 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	annote = {Comment: 19 pages, 15 figures},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/I5AN7IHN/Kaplan et al. - 2020 - Scaling Laws for Neural Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/GHUB4NZ7/2001.html:text/html},
}

@article{dua_drop_nodate,
	title = {{DROP}: {A} {Reading} {Comprehension} {Benchmark} {Requiring} {Discrete} {Reasoning} {Over} {Paragraphs}},
	abstract = {Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96kquestion benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7\% F1 on our generalized accuracy metric, while expert human performance is 96.4\%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0\% F1.},
	language = {en},
	author = {Dua, Dheeru and Wang, Yizhong and Dasigi, Pradeep and Stanovsky, Gabriel and Singh, Sameer and Gardner, Matt},
	file = {Dua et al. - DROP A Reading Comprehension Benchmark Requiring .pdf:/home/jonno/Zotero/storage/ZMQW5K3L/Dua et al. - DROP A Reading Comprehension Benchmark Requiring .pdf:application/pdf},
}

@misc{noauthor_open_nodate,
	title = {Open {LLM} {Leaderboard}: {DROP} deep dive},
	shorttitle = {Open {LLM} {Leaderboard}},
	url = {https://huggingface.co/blog/open-llm-leaderboard-drop},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-04-11},
	file = {Snapshot:/home/jonno/Zotero/storage/GTL4ZTHP/open-llm-leaderboard-drop.html:text/html},
}

@inproceedings{tjong_kim_sang_introduction_2003,
	title = {Introduction to the {CoNLL}-2003 {Shared} {Task}: {Language}-{Independent} {Named} {Entity} {Recognition}},
	shorttitle = {Introduction to the {CoNLL}-2003 {Shared} {Task}},
	url = {https://aclanthology.org/W03-0419},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the {Seventh} {Conference} on {Natural} {Language} {Learning} at {HLT}-{NAACL} 2003},
	author = {Tjong Kim Sang, Erik F. and De Meulder, Fien},
	year = {2003},
	pages = {142--147},
	file = {Full Text PDF:/home/jonno/Zotero/storage/MYI3I48F/Tjong Kim Sang and De Meulder - 2003 - Introduction to the CoNLL-2003 Shared Task Langua.pdf:application/pdf},
}

@misc{he_debertav3_2023,
	title = {{DeBERTaV3}: {Improving} {DeBERTa} using {ELECTRA}-{Style} {Pre}-{Training} with {Gradient}-{Disentangled} {Embedding} {Sharing}},
	shorttitle = {{DeBERTaV3}},
	url = {http://arxiv.org/abs/2111.09543},
	doi = {10.48550/arXiv.2111.09543},
	abstract = {This paper presents a new pre-trained language model, DeBERTaV3, which improves the original DeBERTa model by replacing mask language modeling (MLM) with replaced token detection (RTD), a more sample-efficient pre-training task. Our analysis shows that vanilla embedding sharing in ELECTRA hurts training efficiency and model performance. This is because the training losses of the discriminator and the generator pull token embeddings in different directions, creating the "tug-of-war" dynamics. We thus propose a new gradient-disentangled embedding sharing method that avoids the tug-of-war dynamics, improving both training efficiency and the quality of the pre-trained model. We have pre-trained DeBERTaV3 using the same settings as DeBERTa to demonstrate its exceptional performance on a wide range of downstream natural language understanding (NLU) tasks. Taking the GLUE benchmark with eight tasks as an example, the DeBERTaV3 Large model achieves a 91.37\% average score, which is 1.37\% over DeBERTa and 1.91\% over ELECTRA, setting a new state-of-the-art (SOTA) among the models with a similar structure. Furthermore, we have pre-trained a multi-lingual model mDeBERTa and observed a larger improvement over strong baselines compared to English models. For example, the mDeBERTa Base achieves a 79.8\% zero-shot cross-lingual accuracy on XNLI and a 3.6\% improvement over XLM-R Base, creating a new SOTA on this benchmark. We have made our pre-trained models and inference code publicly available at https://github.com/microsoft/DeBERTa.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {He, Pengcheng and Gao, Jianfeng and Chen, Weizhu},
	month = mar,
	year = {2023},
	note = {arXiv:2111.09543 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, cs.CL, cs.GL, I.2, I.7},
	annote = {Comment: 16 pages, 10 tables, 2 Figures. The DeBERTaV3 model significantly improves performance of the downstream NLU tasks over models with a similar structure, e.g. DeBERTaV3 large achieves 91.37\% average GLUE score which is 1.37\% over DeBERTa large. XSmall has only 22M backbone parameters, but significantly outperforms RoBERTa/XLNet-base. Paper is published as a conference paper at ICLR 2023},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/C356ZPV8/He et al. - 2023 - DeBERTaV3 Improving DeBERTa using ELECTRA-Style P.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/IUW9CEBK/2111.html:text/html},
}

@misc{ingle_gladiatormicrosoft-deberta-v3-large_ner_conll2003_2022,
	title = {Gladiator/microsoft-deberta-v3-large\_ner\_conll2003 · {Hugging} {Face}},
	url = {https://huggingface.co/Gladiator/microsoft-deberta-v3-large_ner_conll2003},
	abstract = {We’re on a journey to advance and democratize artificial intelligence through open source and open science.},
	urldate = {2024-04-14},
	author = {Ingle, Atharva},
	month = sep,
	year = {2022},
	file = {Snapshot:/home/jonno/Zotero/storage/M2SM69XZ/microsoft-deberta-v3-large_ner_conll2003.html:text/html},
}

@misc{anthropic_claude_2024,
	title = {The {Claude} 3 {Model} {Family}: {Opus}, {Sonnet}, {Haiku}},
	url = {https://www-cdn.anthropic.com/de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/Model_Card_Claude_3.pdf},
	language = {en},
	publisher = {Anthropic},
	author = {Anthropic},
	month = apr,
	year = {2024},
}

@misc{jiang_mixtral_2024,
	title = {Mixtral of {Experts}},
	url = {http://arxiv.org/abs/2401.04088},
	doi = {10.48550/arXiv.2401.04088},
	abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = jan,
	year = {2024},
	note = {arXiv:2401.04088 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: See more details at https://mistral.ai/news/mixtral-of-experts/},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/PAAB3BQ2/Jiang et al. - 2024 - Mixtral of Experts.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/ABP77D6H/2401.html:text/html},
}

@inproceedings{van_strien_assessing_2020,
	address = {Valletta, Malta},
	title = {Assessing the {Impact} of {OCR} {Quality} on {Downstream} {NLP} {Tasks}:},
	isbn = {978-989-758-395-7},
	shorttitle = {Assessing the {Impact} of {OCR} {Quality} on {Downstream} {NLP} {Tasks}},
	doi = {10.5220/0009169004840496},
	language = {en},
	booktitle = {Proceedings of the 12th {International} {Conference} on {Agents} and {Artificial} {Intelligence}},
	publisher = {SCITEPRESS - Science and Technology Publications},
	author = {Van Strien, Daniel and Beelen, Kaspar and Ardanuy, Mariona and Hosseini, Kasra and McGillivray, Barbara and Colavizza, Giovanni},
	year = {2020},
	pages = {484--496},
	file = {Van Strien et al. - 2020 - Assessing the Impact of OCR Quality on Downstream .pdf:/home/jonno/Zotero/storage/932HQ8RA/Van Strien et al. - 2020 - Assessing the Impact of OCR Quality on Downstream .pdf:application/pdf},
}

@article{levenshtein_binary_1966,
	title = {Binary codes capable of correcting deletions, insertions, and reversals},
	volume = {10},
	journal = {Soviet physics doklady},
	author = {Levenshtein, Vladimir},
	year = {1966},
	pages = {707--710},
}

@incollection{terras_rise_2011,
	address = {Rotterdam},
	title = {The {Rise} of {Digitization}},
	isbn = {978-94-6091-299-3},
	abstract = {Prior to digitisation, the retrieving and re-filing of the prints consumed staff time. Batches of prints to be re-filed hung round on trolleys like patients in a busy A\&E Department. The sight of a pink print request form approaching the Local Studies counter caused the heart to sink - it foretold at least half an hour wrestling with ancient filing cabinets, not to mention the wait for the notorious Central Library lift. And what could be more dispiriting, for staff and customer, to discover that the prints selected from the card index were not what were wanted at all? The idea of digitising the Print Collection was very attractive. It would reduce the wear and tear on the original prints and transform public access by allowing customers to browse the whole collection (Moorhouse 2004, 62).},
	language = {en},
	booktitle = {Digitisation {Perspectives}},
	publisher = {SensePublishers},
	author = {Terras, Melissa M.},
	editor = {Rikowski, Ruth},
	year = {2011},
	doi = {10.1007/978-94-6091-299-3_1},
	keywords = {British Library, Digital Library, Digital Library Research, Digitization Effort, High Education Fund Council},
	pages = {3--20},
	file = {Full Text PDF:/home/jonno/Zotero/storage/NCSUQ9ID/Terras - 2011 - The Rise of Digitization.pdf:application/pdf},
}

@inproceedings{chiron_impact_2017,
	title = {Impact of {OCR} {Errors} on the {Use} of {Digital} {Libraries}: {Towards} a {Better} {Access} to {Information}},
	shorttitle = {Impact of {OCR} {Errors} on the {Use} of {Digital} {Libraries}},
	url = {https://ieeexplore.ieee.org/document/7991582},
	doi = {10.1109/JCDL.2017.7991582},
	abstract = {Digital collections are increasingly used for a variety of purposes. In Europe only, we can conservatively estimate that tens of thousands of users consult digital libraries daily. The usages are often motivated by qualitative and quantitative research. However, caution must be advised as most digitized documents are indexed through their OCRed version, which is far from perfect, especially for ancient documents. In this paper, we aim to estimate the impact of OCR errors on the use of a major online platform: The Gallica digital library from the National Library of France. It accounts for more than 100M OCRed documents and receives 80M search queries every year. In this context, we introduce two main contributions. First, an original corpus of OCRed documents composed of 12M characters along with the corresponding gold standard is presented and provided, with an equal share of English- and French-written documents. Next, statistics on OCR errors have been computed thanks to a novel alignment method introduced in this paper. Making use of all the user queries submitted to the Gallica portal over 4 months, we take advantage of our error model to propose an indicator for predicting the relative risk that queried terms mismatch targeted resources due to OCR errors, underlining the critical extent to which OCR quality impacts on digital library access.},
	urldate = {2024-04-14},
	booktitle = {2017 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	author = {Chiron, Guillaume and Doucet, Antoine and Coustaty, Mickael and Visani, Muriel and Moreux, Jean-Philippe},
	month = jun,
	year = {2017},
	keywords = {Libraries, Optical character recognition software, Engines, Europe, Gold, Layout, Standards},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/U4BEL26E/7991582.html:text/html;IEEE Xplore Full Text PDF:/home/jonno/Zotero/storage/7XYFHYBF/Chiron et al. - 2017 - Impact of OCR Errors on the Use of Digital Librari.pdf:application/pdf},
}

@inproceedings{traub_impact_2015,
	address = {Cham},
	title = {Impact {Analysis} of {OCR} {Quality} on {Research} {Tasks} in {Digital} {Archives}},
	isbn = {978-3-319-24592-8},
	doi = {10.1007/978-3-319-24592-8_19},
	abstract = {Humanities scholars increasingly rely on digital archives for their research instead of time-consuming visits to physical archives. This shift in research method has the hidden cost of working with digitally processed historical documents: how much trust can a scholar place in noisy representations of source texts? In a series of interviews with historians about their use of digital archives, we found that scholars are aware that optical character recognition (OCR) errors may bias their results. They were, however, unable to quantify this bias or to indicate what information they would need to estimate it. This, however, would be important to assess whether the results are publishable. Based on the interviews and a literature study, we provide a classification of scholarly research tasks that gives account of their susceptibility to specific OCR-induced biases and the data required for uncertainty estimations. We conducted a use case study on a national newspaper archive with example research tasks. From this we learned what data is typically available in digital archives and how it could be used to reduce and/or assess the uncertainty in result sets. We conclude that the current knowledge situation on the users’ side as well as on the tool makers’ and data providers’ side is insufficient and needs to be improved.},
	language = {en},
	booktitle = {Research and {Advanced} {Technology} for {Digital} {Libraries}},
	publisher = {Springer International Publishing},
	author = {Traub, Myriam C. and van Ossenbruggen, Jacco and Hardman, Lynda},
	editor = {Kapidakis, Sarantos and Mazurek, Cezary and Werla, Marcin},
	year = {2015},
	keywords = {Digital humanities, Digital libraries, OCR quality},
	pages = {252--263},
	file = {Full Text PDF:/home/jonno/Zotero/storage/CZTH5PPK/Traub et al. - 2015 - Impact Analysis of OCR Quality on Research Tasks i.pdf:application/pdf},
}

@book{smith_research_2018,
	title = {A {Research} {Agenda} for {Historical} and {Multilingual} {Optical} {Character} {Recognition}},
	url = {http://hdl.handle.net/2047/D20297452},
	language = {en},
	publisher = {North Eastern University},
	author = {Smith, David A and Cordell, Ryan},
	year = {2018},
	file = {Smith and Cordell - A Research Agenda for Historical and Multilingual .pdf:/home/jonno/Zotero/storage/HZA79XWC/Smith and Cordell - A Research Agenda for Historical and Multilingual .pdf:application/pdf},
}

@article{nguyen_survey_2021,
	title = {Survey of {Post}-{OCR} {Processing} {Approaches}},
	volume = {54},
	issn = {0360-0300},
	url = {https://dl.acm.org/doi/10.1145/3453476},
	doi = {10.1145/3453476},
	abstract = {Optical character recognition (OCR) is one of the most popular techniques used for converting printed documents into machine-readable ones. While OCR engines can do well with modern text, their performance is unfortunately significantly reduced on historical materials. Additionally, many texts have already been processed by various out-of-date digitisation techniques. As a consequence, digitised texts are noisy and need to be post-corrected. This article clarifies the importance of enhancing quality of OCR results by studying their effects on information retrieval and natural language processing applications. We then define the post-OCR processing problem, illustrate its typical pipeline, and review the state-of-the-art post-OCR processing approaches. Evaluation metrics, accessible datasets, language resources, and useful toolkits are also reported. Furthermore, the work identifies the current trend and outlines some research directions of this field.},
	number = {6},
	urldate = {2024-04-14},
	journal = {ACM Computing Surveys},
	author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Coustaty, Mickael and Doucet, Antoine},
	month = jul,
	year = {2021},
	keywords = {error model, language model, machine learning, OCR merging, Post-OCR processing, statistical and neural machine translation},
	pages = {124:1--124:37},
	file = {Full Text PDF:/home/jonno/Zotero/storage/SRRSZYTC/Nguyen et al. - 2021 - Survey of Post-OCR Processing Approaches.pdf:application/pdf},
}

@inproceedings{neudecker_survey_2021,
	address = {New York, NY, USA},
	series = {{HIP} '21},
	title = {A survey of {OCR} evaluation tools and metrics},
	isbn = {978-1-4503-8690-6},
	url = {https://dl.acm.org/doi/10.1145/3476887.3476888},
	doi = {10.1145/3476887.3476888},
	abstract = {The millions of pages of historical documents that are digitized in libraries are increasingly used in contexts that have more specific requirements for OCR quality than keyword search. How to comprehensively, efficiently and reliably assess the quality of OCR results against the background of mass digitization, when ground truth can only ever be produced for very small numbers? Due to gaps in specifications, results from OCR evaluation tools can return different results, and due to differences in implementation, even commonly used error rates are often not directly comparable. OCR evaluation metrics and sampling methods are also not sufficient where they do not take into account the accuracy of layout analysis, since for advanced use cases like Natural Language Processing or the Digital Humanities, accurate layout analysis and detection of the reading order are crucial. We provide an overview of OCR evaluation metrics and tools, describe two advanced use cases for OCR results, and perform an OCR evaluation experiment with multiple evaluation tools and different metrics for two distinct datasets. We analyze the differences and commonalities in light of the presented use cases and suggest areas for future work.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Historical} {Document} {Imaging} and {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Neudecker, Clemens and Baierer, Konstantin and Gerber, Mike and Clausner, Christian and Antonacopoulos, Apostolos and Pletschacher, Stefan},
	month = oct,
	year = {2021},
	keywords = {evaluation, accuracy, metrics, optical character recognition},
	pages = {13--18},
	file = {Full Text PDF:/home/jonno/Zotero/storage/HH5DUCIU/Neudecker et al. - 2021 - A survey of OCR evaluation tools and metrics.pdf:application/pdf},
}

@article{von_ahn_recaptcha_2008,
	title = {{reCAPTCHA}: {Human}-{Based} {Character} {Recognition} via {Web} {Security} {Measures}},
	volume = {321},
	issn = {0036-8075, 1095-9203},
	shorttitle = {{reCAPTCHA}},
	url = {https://www.science.org/doi/10.1126/science.1160379},
	doi = {10.1126/science.1160379},
	abstract = {CAPTCHAs (Completely Automated Public Turing test to tell Computers and Humans Apart) are widespread security measures on the World Wide Web that prevent automated programs from abusing online services. They do so by asking humans to perform a task that computers cannot yet perform, such as deciphering distorted characters. Our research explored whether such human effort can be channeled into a useful purpose: helping to digitize old printed material by asking users to decipher scanned words from books that computerized optical character recognition failed to recognize. We showed that this method can transcribe text with a word accuracy exceeding 99\%, matching the guarantee of professional human transcribers. Our apparatus is deployed in more than 40,000 Web sites and has transcribed over 440 million words.},
	language = {en},
	number = {5895},
	urldate = {2024-04-14},
	journal = {Science},
	author = {Von Ahn, Luis and Maurer, Benjamin and McMillen, Colin and Abraham, David and Blum, Manuel},
	month = sep,
	year = {2008},
	pages = {1465--1468},
	file = {Von Ahn et al. - 2008 - reCAPTCHA Human-Based Character Recognition via W.pdf:/home/jonno/Zotero/storage/4WPW3CHF/Von Ahn et al. - 2008 - reCAPTCHA Human-Based Character Recognition via W.pdf:application/pdf},
}

@book{holley_many_2009,
	title = {Many {Hands} {Make} {Light} {Work} : {Public} {Collaborative} {OCR} {Text} {Correction} in {Australian} {Historic} {Newspapers}},
	shorttitle = {Many {Hands} {Make} {Light} {Work}},
	url = {http://eprints.rclis.org/12907/},
	abstract = {An overview of public collaborative OCR text correction in the 'Australian Newspapers' service.  The service has been available for 6 months and is innovative in enabling the public to enhance and improve the raw OCR text of newspapers.  This is the first library in the world to have considered this as a viable idea and implemented it. The issues and results surrounding this are discussed.},
	language = {en},
	urldate = {2024-04-14},
	publisher = {National Library of Australia},
	author = {Holley, Rose},
	month = mar,
	year = {2009},
	file = {Full Text PDF:/home/jonno/Zotero/storage/UW85EA2Y/Holley - 2009 - Many Hands Make Light Work  Public Collaborative .pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/DWD8HB46/12907.html:text/html},
}

@article{suissa_toward_2020,
	title = {Toward the optimized crowdsourcing strategy for {OCR} post-correction},
	volume = {72},
	issn = {2050-3806},
	url = {https://doi.org/10.1108/AJIM-07-2019-0189},
	doi = {10.1108/AJIM-07-2019-0189},
	abstract = {Purpose Digitization of historical documents is a challenging task in many digital humanities projects. A popular approach for digitization is to scan the documents into images, and then convert images into text using optical character recognition (OCR) algorithms. However, the outcome of OCR processing of historical documents is usually inaccurate and requires post-processing error correction. The purpose of this paper is to investigate how crowdsourcing can be utilized to correct OCR errors in historical text collections, and which crowdsourcing methodology is the most effective in different scenarios and for various research objectives. Design/methodology/approach A series of experiments with different micro-task’s structures and text lengths were conducted with 753 workers on the Amazon’s Mechanical Turk platform. The workers had to fix OCR errors in a selected historical text. To analyze the results, new accuracy and efficiency measures were devised. Findings The analysis suggests that in terms of accuracy, the optimal text length is medium (paragraph-size) and the optimal structure of the experiment is two phase with a scanned image. In terms of efficiency, the best results were obtained when using longer text in the single-stage structure with no image. Practical implications The study provides practical recommendations to researchers on how to build the optimal crowdsourcing task for OCR post-correction. The developed methodology can also be utilized to create golden standard historical texts for automatic OCR post-correction. Originality/value This is the first attempt to systematically investigate the influence of various factors on crowdsourcing-based OCR post-correction and propose an optimal strategy for this process.},
	number = {2},
	urldate = {2024-04-14},
	journal = {Aslib Journal of Information Management},
	author = {Suissa, Omri and Elmalech, Avshalom and Zhitomirsky-Geffet, Maayan},
	month = jan,
	year = {2020},
	note = {Publisher: Emerald Publishing Limited},
	keywords = {Crowdsourcing, OCR post-correction, Digital humanities, Historical texts, Task decomposition, Task optimization},
	pages = {179--197},
	file = {Snapshot:/home/jonno/Zotero/storage/UIZI6JFE/html.html:text/html;Submitted Version:/home/jonno/Zotero/storage/E4RUYZ8L/Suissa et al. - 2020 - Toward the optimized crowdsourcing strategy for OC.pdf:application/pdf},
}

@article{mechaca_web_2021,
	title = {A web platform for collaborative semi-automatic {OCR} {Post}-processing},
	copyright = {info:eu-repo/semantics/openAccess},
	issn = {2683-8966},
	url = {https://ri.conicet.gov.ar/handle/11336/173940},
	abstract = {Digital Humanities researchers often make use of software that helps them in the task of finding non-trivial relationships among characters in historical text. Usually, the source texts that contain such information come from OCR acquired volumes, carrying high amounts of errors within them. This work explains the development of a web platform for the task of OCR post-processing and ground-truth generation. This platform employs machine learning to predict the correct texts accurately from OCR noisy strings. The method used for this task involves transformers for character-based denoising language models. An active learning workflow is proposed, as the users can feed their corrections to the platform, generating new annotated data for re-training the underlying machine learning correction models.},
	language = {eng},
	urldate = {2024-04-14},
	journal = {50 JAIIO: 50th Jornadas Argentinas de Informática. Buenos Aires, Argentina 10/2021},
	author = {Mechaca, Ana Lidia and Marmanillo, Walter Gabriel and Xamena, Eduardo and Ramirez Orta, Juan and Maguitman, Ana Gabriela and Milios, Evangelos E.},
	year = {2021},
	note = {Accepted: 2022-10-19T15:06:08Z
Publisher: Sociedad Argentina de Investigación Operativa},
	file = {Full Text PDF:/home/jonno/Zotero/storage/RY3G7LLY/Mechaca et al. - 2021 - A web platform for collaborative semi-automatic OC.pdf:application/pdf},
}

@article{karthikeyan_ocr_2022,
	title = {An {OCR} {Post}-{Correction} {Approach} {Using} {Deep} {Learning} for {Processing} {Medical} {Reports}},
	volume = {32},
	issn = {1558-2205},
	doi = {10.1109/TCSVT.2021.3087641},
	abstract = {According to a recent Deloitte study, the COVID-19 pandemic continues to place a huge strain on the global health care sector. Covid-19 has also catalysed digital transformation across the sector for improving operational efficiencies. As a result, the amount of digitally stored patient data such as discharge letters, scan images, test results or free text entries by doctors has grown significantly. In 2020, 2314 exabytes of medical data was generated globally. This medical data does not conform to a generic structure and is mostly in the form of unstructured digitally generated or scanned paper documents stored as part of a patient’s medical reports. This unstructured data is digitised using Optical Character Recognition (OCR) process. A key challenge here is that the accuracy of the OCR process varies due to the inability of current OCR engines to correctly transcribe scanned or handwritten documents in which text may be skewed, obscured or illegible. This is compounded by the fact that processed text is comprised of specific medical terminologies that do not necessarily form part of general language lexicons. The proposed work uses a deep neural network based self-supervised pre-training technique: Robustly Optimized Bidirectional Encoder Representations from Transformers (RoBERTa) that can learn to predict hidden (masked) sections of text to fill in the gaps of non-transcribable parts of the documents being processed. Evaluating the proposed method on domain-specific datasets which include real medical documents, shows a significantly reduced word error rate demonstrating the effectiveness of the approach.},
	number = {5},
	journal = {IEEE Transactions on Circuits and Systems for Video Technology},
	author = {Karthikeyan, Srinidhi and de Herrera, Alba G. Seco and Doctor, Faiyaz and Mirza, Asim},
	month = may,
	year = {2022},
	note = {Conference Name: IEEE Transactions on Circuits and Systems for Video Technology},
	keywords = {Task analysis, Biomedical imaging, Optical character recognition software, Engines, Medical diagnostic imaging, medical documents, Medical services, natural language processing (NLP), Optical character recognition (OCR), Portable document format, robustly optimized bidirectional encoder representations from transformers (RoBERTa)},
	pages = {2574--2581},
	file = {Accepted Version:/home/jonno/Zotero/storage/TMBUFFRH/Karthikeyan et al. - 2022 - An OCR Post-Correction Approach Using Deep Learnin.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/7N3FDLLA/9448197.html:text/html},
}

@article{amrhein_supervised_2018,
	title = {Supervised {OCR} {Error} {Detection} and {Correction} {Using} {Statistical} and {Neural} {Machine} {Translation} {Methods}},
	volume = {33},
	copyright = {info:eu-repo/semantics/openAccess},
	issn = {0175-1336},
	url = {https://jlcl.org/content/2-allissues/1-heft1-2018/jlcl_2018-1_3.pdf},
	doi = {10.5167/uzh-162394},
	abstract = {For indexing the content of digitized historical texts, optical character recognition (OCR) errors are a hampering problem. To explore the effectivity of new strategies for OCR post-correction, this article focuses on methods of character-based machine translation, specifically neural machine translation and statistical machine translation. Using the ICDAR 2017 data set on OCR post-correction for English and French, we experiment with different strategies for error detection and error correction. We analyze how OCR post-correction with NMT can profit from using additional information and show that SMT and NMT can benefit from each other for these tasks. An ensemble of our models reached best performance in ICDAR’s 2017 error correction subtask and performed competitively in error detection. However, our experimental results also suggest that tuning supervised learning for OCR post-correction of texts from different sources, text types (periodicals and monographs), time periods and languages is a difficult task: the data on which the MT systems are trained have a large influence on which methods and features work best. Conclusive and generally applicable insights are hard to achieve.},
	language = {eng},
	number = {1},
	urldate = {2024-04-14},
	journal = {Journal for Language Technology and Computational Linguistics (JLCL)},
	author = {Amrhein, Chantal and Clematide, Simon},
	year = {2018},
	note = {Number: 1
Publisher: Gesellschaft für Sprachtechnologie und Computerlinguistik (GSCL)},
	pages = {49--76},
	file = {Full Text PDF:/home/jonno/Zotero/storage/4PTMXMQY/Amrhein and Clematide - 2018 - Supervised OCR Error Detection and Correction Usin.pdf:application/pdf},
}

@inproceedings{mokhtar_ocr_2018,
	title = {{OCR} {Error} {Correction}: {State}-of-the-{Art} vs an {NMT}-based {Approach}},
	shorttitle = {{OCR} {Error} {Correction}},
	doi = {10.1109/DAS.2018.63},
	abstract = {Although the performance of the state-of-the-art OCR systems is very high, they can still introduce errors due to various reasons, and when it comes to historical documents with old manuscripts the performance of such systems gets even worse. That is why Post-OCR error correction has been an open problem for many years. Many state-of-the-art approaches have been introduced through the recent years. This paper contributes to the field of Post-OCR Error Correction by introducing two novel deep learning approaches to improve the accuracy of OCR systems, and a post processing technique that can further enhance the quality of the output results. These approaches are based on Neural Machine Translation (NMT) and were motivated by the great success that deep learning introduced to the field of Natural Language Processing. Finally, we will compare the state-of-the-art approaches in Post-OCR Error Correction with the newly introduced systems and discuss the results.},
	urldate = {2024-04-14},
	booktitle = {2018 13th {IAPR} {International} {Workshop} on {Document} {Analysis} {Systems} ({DAS})},
	author = {Mokhtar, Kareem and Bukhari, Syed Saqib and Dengel, Andreas},
	month = apr,
	year = {2018},
	keywords = {Error correction, Optical character recognition software, Computer architecture, Decoding, Dictionaries, Earth Observing System, Neural Machine Translation (NMT), Neural networks, OCR Error Correction, OCR Post-Processing},
	pages = {429--434},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/BRSMJIVD/8395234.html:text/html},
}

@article{mei_statistical_2018,
	title = {Statistical learning for {OCR} error correction},
	volume = {54},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457317307823},
	doi = {10.1016/j.ipm.2018.06.001},
	abstract = {Modern OCR engines incorporate some form of error correction, typically based on dictionaries. However, there are still residual errors that decrease performance of natural language processing algorithms applied to OCR text. In this paper, we present a statistical learning model for post-processing OCR errors, either in a fully automatic manner or followed by minimal user interaction to further reduce error rate. Our model employs web-scale corpora and integrates a rich set of linguistic features. Through an interdependent learning pipeline, our model produces and continuously refines the error detection and suggestion of candidate corrections. Evaluated on a historical biology book with complex error patterns, our model outperforms various baseline methods in the automatic mode and shows an even greater advantage when involving minimal user interaction. Quantitative analysis of each computational step further suggests that our proposed model is well-suited for handling volatile and complex OCR error patterns, which are beyond the capabilities of error correction incorporated in OCR engines.},
	number = {6},
	urldate = {2024-04-14},
	journal = {Information Processing \& Management},
	author = {Mei, Jie and Islam, Aminul and Moh’d, Abidalrahman and Wu, Yajing and Milios, Evangelos},
	month = nov,
	year = {2018},
	keywords = {Error correction, OCR error, OCR post-processing, Statistical learning},
	pages = {874--887},
	file = {ScienceDirect Snapshot:/home/jonno/Zotero/storage/LXLFBPFC/S0306457317307823.html:text/html},
}

@misc{ramirez-orta_post-ocr_2022,
	title = {Post-{OCR} {Document} {Correction} with large {Ensembles} of {Character} {Sequence}-to-{Sequence} {Models}},
	url = {http://arxiv.org/abs/2109.06264},
	abstract = {In this paper, we propose a novel method to extend sequenceto-sequence models to accurately process sequences much longer than the ones used during training while being sampleand resource-efﬁcient, supported by thorough experimentation. To investigate the effectiveness of our method, we apply it to the task of correcting documents already processed with Optical Character Recognition (OCR) systems using sequence-tosequence models based on characters. We test our method on nine languages of the ICDAR 2019 competition on post-OCR text correction and achieve a new state-of-the-art performance in ﬁve of them. The strategy with the best performance involves splitting the input document in character n-grams and combining their individual corrections into the ﬁnal output using a voting scheme that is equivalent to an ensemble of a large number of sequence models. We further investigate how to weigh the contributions from each one of the members of this ensemble. Our code for post-OCR correction is shared at https://github.com/jarobyte91/post ocr correction.},
	language = {en},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Ramirez-Orta, Juan and Xamena, Eduardo and Maguitman, Ana and Milios, Evangelos and Soto, Axel J.},
	month = jan,
	year = {2022},
	note = {arXiv:2109.06264 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Ramirez-Orta et al. - 2022 - Post-OCR Document Correction with large Ensembles .pdf:/home/jonno/Zotero/storage/33J9G7E6/Ramirez-Orta et al. - 2022 - Post-OCR Document Correction with large Ensembles .pdf:application/pdf},
}

@misc{he_deberta_2021,
	title = {{DeBERTa}: {Decoding}-enhanced {BERT} with {Disentangled} {Attention}},
	shorttitle = {{DeBERTa}},
	url = {http://arxiv.org/abs/2006.03654},
	doi = {10.48550/arXiv.2006.03654},
	abstract = {Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9\% (90.2\% vs. 91.1\%), on SQuAD v2.0 by +2.3\% (88.4\% vs. 90.7\%) and RACE by +3.6\% (83.2\% vs. 86.8\%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {He, Pengcheng and Liu, Xiaodong and Gao, Jianfeng and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2006.03654 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, cs.CL, cs.GL, I.2, I.7},
	annote = {Comment: 20 pages,5 figures, 13 tables. In v2, we scale up DeBERTa to 1.5B parameters and it surpasses the human performance on SuperGLUE leaderboard for the first time as of December 29, 2020. In v3, we replace MLM with RTD objective which significantly improves the model performance},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/VQV8IEC7/He et al. - 2021 - DeBERTa Decoding-enhanced BERT with Disentangled .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/T4WI2YQQ/2006.html:text/html},
}

@article{wang_superglue_2019,
	title = {{SuperGLUE}: {A} {Stickier} {Benchmark} for {General}-{Purpose} {Language} {Understanding} {Systems}},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difﬁcult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	language = {en},
	journal = {NeurIPS},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R},
	year = {2019},
	file = {Wang et al. - SuperGLUE A Stickier Benchmark for General-Purpos.pdf:/home/jonno/Zotero/storage/PTFJRN6T/Wang et al. - SuperGLUE A Stickier Benchmark for General-Purpos.pdf:application/pdf},
}

@misc{xu_human_2022,
	title = {Human {Parity} on {CommonsenseQA}: {Augmenting} {Self}-{Attention} with {External} {Attention}},
	shorttitle = {Human {Parity} on {CommonsenseQA}},
	url = {http://arxiv.org/abs/2112.03254},
	doi = {10.48550/arXiv.2112.03254},
	abstract = {Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4{\textbackslash}\% in comparison to the human accuracy of 88.9{\textbackslash}\%.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Xu, Yichong and Zhu, Chenguang and Wang, Shuohang and Sun, Siqi and Cheng, Hao and Liu, Xiaodong and Gao, Jianfeng and He, Pengcheng and Zeng, Michael and Huang, Xuedong},
	month = may,
	year = {2022},
	note = {arXiv:2112.03254 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 1 figure, 7 tables},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/WGUJ8ZLJ/Xu et al. - 2022 - Human Parity on CommonsenseQA Augmenting Self-Att.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/VJRPMDC9/2112.html:text/html},
}

@misc{xu_human_2022-1,
	title = {Human {Parity} on {CommonsenseQA}: {Augmenting} {Self}-{Attention} with {External} {Attention}},
	shorttitle = {Human {Parity} on {CommonsenseQA}},
	url = {http://arxiv.org/abs/2112.03254},
	doi = {10.48550/arXiv.2112.03254},
	abstract = {Most of today's AI systems focus on using self-attention mechanisms and transformer architectures on large amounts of diverse data to achieve impressive performance gains. In this paper, we propose to augment the transformer architecture with an external attention mechanism to bring external knowledge and context to bear. By integrating external information into the prediction process, we hope to reduce the need for ever-larger models and increase the democratization of AI systems. We find that the proposed external attention mechanism can significantly improve the performance of existing AI systems, allowing practitioners to easily customize foundation AI models to many diverse downstream applications. In particular, we focus on the task of Commonsense Reasoning, demonstrating that the proposed external attention mechanism can augment existing transformer models and significantly improve the model's reasoning capabilities. The proposed system, Knowledgeable External Attention for commonsense Reasoning (KEAR), reaches human parity on the open CommonsenseQA research benchmark with an accuracy of 89.4{\textbackslash}\% in comparison to the human accuracy of 88.9{\textbackslash}\%.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Xu, Yichong and Zhu, Chenguang and Wang, Shuohang and Sun, Siqi and Cheng, Hao and Liu, Xiaodong and Gao, Jianfeng and He, Pengcheng and Zeng, Michael and Huang, Xuedong},
	month = may,
	year = {2022},
	note = {arXiv:2112.03254 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: 8 pages, 1 figure, 7 tables},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/5SCKCFYJ/Xu et al. - 2022 - Human Parity on CommonsenseQA Augmenting Self-Att.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/RZA7NWS6/2112.html:text/html},
}

@misc{zhang_retrospective_2020,
	title = {Retrospective {Reader} for {Machine} {Reading} {Comprehension}},
	url = {http://arxiv.org/abs/2001.09694},
	abstract = {Machine reading comprehension (MRC) is an AI challenge that requires machines to determine the correct answers to questions based on a given passage. MRC systems must not only answer questions when necessary but also tactfully abstain from answering when no answer is available according to the given passage. When unanswerable questions are involved in the MRC task, an essential veriﬁcation module called veriﬁer is especially required in addition to the encoder, though the latest practice on MRC modeling still mostly beneﬁts from adopting well pre-trained language models as the encoder block by only focusing on the “reading”. This paper devotes itself to exploring better veriﬁer design for the MRC task with unanswerable questions. Inspired by how humans solve reading comprehension questions, we proposed a retrospective reader (Retro-Reader) that integrates two stages of reading and veriﬁcation strategies: 1) sketchy reading that brieﬂy investigates the overall interactions of passage and question, and yields an initial judgment; 2) intensive reading that veriﬁes the answer and gives the ﬁnal prediction. The proposed reader is evaluated on two benchmark MRC challenge datasets SQuAD2.0 and NewsQA, achieving new state-of-the-art results. Signiﬁcance tests show that our model is signiﬁcantly better than strong baselines.},
	language = {en},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Zhang, Zhuosheng and Yang, Junjie and Zhao, Hai},
	month = dec,
	year = {2020},
	note = {arXiv:2001.09694 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: Accepted by AAAI 2021},
	file = {Zhang et al. - 2020 - Retrospective Reader for Machine Reading Comprehen.pdf:/home/jonno/Zotero/storage/R77IU9FD/Zhang et al. - 2020 - Retrospective Reader for Machine Reading Comprehen.pdf:application/pdf},
}

@misc{brown_language_2020-1,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	doi = {10.48550/arXiv.2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/INC57YKW/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/9F24Y4EZ/2005.html:text/html},
}

@misc{brown_language_2020-2,
	title = {Language {Models} are {Few}-{Shot} {Learners}},
	url = {http://arxiv.org/abs/2005.14165},
	abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by ﬁne-tuning on a speciﬁc task. While typically task-agnostic in architecture, this method still requires task-speciﬁc ﬁne-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art ﬁnetuning approaches. Speciﬁcally, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or ﬁne-tuning, with tasks and few-shot demonstrations speciﬁed purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-ﬂy reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we ﬁnd that GPT-3 can generate samples of news articles which human evaluators have difﬁculty distinguishing from articles written by humans. We discuss broader societal impacts of this ﬁnding and of GPT-3 in general.},
	language = {en},
	urldate = {2024-04-14},
	publisher = {arXiv},
	author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	month = jul,
	year = {2020},
	note = {arXiv:2005.14165 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 40+32 pages},
	file = {Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:/home/jonno/Zotero/storage/S3I2ZJHY/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf:application/pdf},
}

@article{taylor_cloze_1953,
	title = {“{Cloze} {Procedure}”: {A} {New} {Tool} for {Measuring} {Readability}"},
	doi = {10.1177/10776990530300040},
	urldate = {2024-04-14},
	journal = {10.1177/10776990530300040},
	author = {Taylor, Wilson},
	year = {1953},
	file = {“Cloze Procedure”\: A New Tool for Measuring Readability - Wilson L. Taylor, 1953:/home/jonno/Zotero/storage/H6NZGSC7/107769905303000401.html:text/html},
}

@misc{white_prompt_2023,
	title = {A {Prompt} {Pattern} {Catalog} to {Enhance} {Prompt} {Engineering} with {ChatGPT}},
	url = {http://arxiv.org/abs/2302.11382},
	doi = {10.48550/arXiv.2302.11382},
	abstract = {Prompt engineering is an increasingly important skill set needed to converse effectively with large language models (LLMs), such as ChatGPT. Prompts are instructions given to an LLM to enforce rules, automate processes, and ensure specific qualities (and quantities) of generated output. Prompts are also a form of programming that can customize the outputs and interactions with an LLM. This paper describes a catalog of prompt engineering techniques presented in pattern form that have been applied to solve common problems when conversing with LLMs. Prompt patterns are a knowledge transfer method analogous to software patterns since they provide reusable solutions to common problems faced in a particular context, i.e., output generation and interaction when working with LLMs. This paper provides the following contributions to research on prompt engineering that apply LLMs to automate software development tasks. First, it provides a framework for documenting patterns for structuring prompts to solve a range of problems so that they can be adapted to different domains. Second, it presents a catalog of patterns that have been applied successfully to improve the outputs of LLM conversations. Third, it explains how prompts can be built from multiple patterns and illustrates prompt patterns that benefit from combination with other prompt patterns.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {White, Jules and Fu, Quchen and Hays, Sam and Sandborn, Michael and Olea, Carlos and Gilbert, Henry and Elnashar, Ashraf and Spencer-Smith, Jesse and Schmidt, Douglas C.},
	month = feb,
	year = {2023},
	note = {arXiv:2302.11382 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/HVM8N27G/White et al. - 2023 - A Prompt Pattern Catalog to Enhance Prompt Enginee.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/2XA6CCJW/2302.html:text/html},
}

@inproceedings{sorensen_information-theoretic_2022,
	title = {An {Information}-theoretic {Approach} to {Prompt} {Engineering} {Without} {Ground} {Truth} {Labels}},
	url = {http://arxiv.org/abs/2203.11364},
	doi = {10.18653/v1/2022.acl-long.60},
	abstract = {Pre-trained language models derive substantial linguistic and factual knowledge from the massive corpora on which they are trained, and prompt engineering seeks to align these models to specific tasks. Unfortunately, existing prompt engineering methods require significant amounts of labeled data, access to model parameters, or both. We introduce a new method for selecting prompt templates {\textbackslash}textit\{without labeled examples\} and {\textbackslash}textit\{without direct access to the model\}. Specifically, over a set of candidate templates, we choose the template that maximizes the mutual information between the input and the corresponding model output. Across 8 datasets representing 7 distinct NLP tasks, we show that when a template has high mutual information, it also has high accuracy on the task. On the largest model, selecting prompts with our method gets 90{\textbackslash}\% of the way from the average prompt accuracy to the best prompt accuracy and requires no ground truth labels.},
	urldate = {2024-04-15},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	author = {Sorensen, Taylor and Robinson, Joshua and Rytting, Christopher Michael and Shaw, Alexander Glenn and Rogers, Kyle Jeffrey and Delorey, Alexia Pauline and Khalil, Mahmoud and Fulda, Nancy and Wingate, David},
	year = {2022},
	note = {arXiv:2203.11364 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	pages = {819--862},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/3JBEQPS5/Sorensen et al. - 2022 - An Information-theoretic Approach to Prompt Engine.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/7SJEUAR5/2203.html:text/html},
}

@misc{wang_prompt_2024,
	title = {Prompt {Engineering} for {Healthcare}: {Methodologies} and {Applications}},
	shorttitle = {Prompt {Engineering} for {Healthcare}},
	url = {http://arxiv.org/abs/2304.14670},
	doi = {10.48550/arXiv.2304.14670},
	abstract = {Prompt engineering is a critical technique in the field of natural language processing that involves designing and optimizing the prompts used to input information into models, aiming to enhance their performance on specific tasks. With the recent advancements in large language models, prompt engineering has shown significant superiority across various domains and has become increasingly important in the healthcare domain. However, there is a lack of comprehensive reviews specifically focusing on prompt engineering in the medical field. This review will introduce the latest advances in prompt engineering in the field of natural language processing for the medical field. First, we will provide the development of prompt engineering and emphasize its significant contributions to healthcare natural language processing applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare natural language processing researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire for research and application in medical natural language processing.},
	urldate = {2024-04-15},
	publisher = {arXiv},
	author = {Wang, Jiaqi and Shi, Enze and Yu, Sigang and Wu, Zihao and Ma, Chong and Dai, Haixing and Yang, Qiushi and Kang, Yanqing and Wu, Jinru and Hu, Huawen and Yue, Chenxi and Zhang, Haiyang and Liu, Yiheng and Pan, Yi and Liu, Zhengliang and Sun, Lichao and Li, Xiang and Ge, Bao and Jiang, Xi and Zhu, Dajiang and Yuan, Yixuan and Shen, Dinggang and Liu, Tianming and Zhang, Shu},
	month = mar,
	year = {2024},
	note = {arXiv:2304.14670 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/Z9TJVIF9/Wang et al. - 2024 - Prompt Engineering for Healthcare Methodologies a.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/CCUUCLVI/2304.html:text/html},
}

@inproceedings{rigaud_icdar_2019,
	title = {{ICDAR} 2019 {Competition} on {Post}-{OCR} {Text} {Correction}},
	url = {https://ieeexplore.ieee.org/document/8978127},
	doi = {10.1109/ICDAR.2019.00255},
	abstract = {This paper describes the second round of the ICDAR 2019 competition on post-OCR text correction and presents the different methods submitted by the participants. OCR has been an active research field for over the past 30 years but results are still imperfect, especially for historical documents. The purpose of this competition is to compare and evaluate automatic approaches for correcting (denoising) OCR-ed texts. The present challenge consists of two tasks: 1) error detection and 2) error correction. An original dataset of 22M OCR-ed symbols along with an aligned ground truth was provided to the participants with 80\% of the dataset dedicated to training and 20\% to evaluation. Different sources were aggregated and contain newspapers, historical printed documents as well as manuscripts and shopping receipts, covering 10 European languages (Bulgarian, Czech, Dutch, English, Finish, French, German, Polish, Spanish and Slovak). Five teams submitted results, the error detection scores vary from 41 to 95\% and the best error correction improvement is 44\%. This competition, which counted 34 registrations, illustrates the strong interest of the community to improve OCR output, which is a key issue to any digitization process involving textual data.},
	urldate = {2024-04-15},
	booktitle = {2019 {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Rigaud, Christophe and Doucet, Antoine and Coustaty, Mickaël and Moreux, Jean-Philippe},
	month = sep,
	year = {2019},
	note = {ISSN: 2379-2140},
	keywords = {Task analysis, Training, Error correction, Libraries, Optical character recognition software, OCR, Dictionaries, text recognition, Tools},
	pages = {1588--1593},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/UY922TE3/8978127.html:text/html;Submitted Version:/home/jonno/Zotero/storage/5QDFFFQ5/Rigaud et al. - 2019 - ICDAR 2019 Competition on Post-OCR Text Correction.pdf:application/pdf},
}

@inproceedings{ehrmann_language_2020,
	address = {Marseille, France},
	title = {Language {Resources} for {Historical} {Newspapers}: the {Impresso} {Collection}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Language {Resources} for {Historical} {Newspapers}},
	url = {https://aclanthology.org/2020.lrec-1.121},
	abstract = {Following decades of massive digitization, an unprecedented amount of historical document facsimiles can now be retrieved and accessed via cultural heritage online portals. If this represents a huge step forward in terms of preservation and accessibility, the next fundamental challenge– and real promise of digitization– is to exploit the contents of these digital assets, and therefore to adapt and develop appropriate language technologies to search and retrieve information from this `Big Data of the Past'. Yet, the application of text processing tools on historical documents in general, and historical newspapers in particular, poses new challenges, and crucially requires appropriate language resources. In this context, this paper presents a collection of historical newspaper data sets composed of text and image resources, curated and published within the context of the `impresso - Media Monitoring of the Past' project. With corpora, benchmarks, semantic annotations and language models in French, German and Luxembourgish covering ca. 200 years, the objective of the impresso resource collection is to contribute to historical language resources, and thereby strengthen the robustness of approaches to non-standard inputs and foster efficient processing of historical documents.},
	language = {English},
	urldate = {2024-04-15},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Ehrmann, Maud and Romanello, Matteo and Clematide, Simon and Ströbel, Phillip Benjamin and Barman, Raphaël},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Moreno, Asuncion and Odijk, Jan and Piperidis, Stelios},
	month = may,
	year = {2020},
	pages = {958--968},
	file = {Full Text PDF:/home/jonno/Zotero/storage/UQCEQLPQ/Ehrmann et al. - 2020 - Language Resources for Historical Newspapers the .pdf:application/pdf},
}

@article{strobel_improving_2019,
	title = {Improving {OCR} of {Black} {Letter} in {Historical} {Newspapers}: {The} {Unreasonable} {Effectiveness} of {HTR} {Models} on {Low}-{Resolution} {Images}},
	shorttitle = {Improving {OCR} of {Black} {Letter} in {Historical} {Newspapers}},
	url = {https://www.zora.uzh.ch/id/eprint/177164},
	doi = {10.5167/UZH-177164},
	language = {en},
	urldate = {2024-04-15},
	journal = {Proceedings of the Digital Humanities 2019, (DH2019)},
	author = {Ströbel, Phillip Benjamin and Clematide, Simon},
	year = {2019},
	note = {Publisher: [object Object]},
	file = {Ströbel and Clematide - 2019 - Improving OCR of Black Letter in Historical Newspa.pdf:/home/jonno/Zotero/storage/LHHUI67C/Ströbel and Clematide - 2019 - Improving OCR of Black Letter in Historical Newspa.pdf:application/pdf},
}

@inproceedings{romanello_optical_2021,
	address = {New York, NY, USA},
	series = {{HIP} '21},
	title = {Optical {Character} {Recognition} of 19th {Century} {Classical} {Commentaries}: the {Current} {State} of {Affairs}},
	isbn = {978-1-4503-8690-6},
	shorttitle = {Optical {Character} {Recognition} of 19th {Century} {Classical} {Commentaries}},
	doi = {10.1145/3476887.3476911},
	abstract = {Together with critical editions and translations, commentaries are one of the main genres of publication in literary and textual scholarship, and have a century-long tradition. Yet, the exploitation of thousands of digitized historical commentaries was hitherto hindered by the poor quality of Optical Character Recognition (OCR), especially on commentaries to Greek texts. In this paper, we evaluate the performances of two pipelines suitable for the OCR of historical classical commentaries. Our results show that Kraken + Ciaconna reaches a substantially lower character error rate (CER) than Tesseract/OCR-D on commentary sections with high density of polytonic Greek text (average CER 7\% vs. 13\%), while Tesseract/OCR-D is slightly more accurate than Kraken + Ciaconna on text sections written predominantly in Latin script (average CER 8.2\% vs. 8.4\%). As part of this paper, we also release GT4HistComment, a small dataset with OCR ground truth for 19th classical commentaries and Pogretra, a large collection of training data and pre-trained models for a wide variety of ancient Greek typefaces.},
	booktitle = {Proceedings of the 6th {International} {Workshop} on {Historical} {Document} {Imaging} and {Processing}},
	publisher = {Association for Computing Machinery},
	author = {Romanello, Matteo and Najem-Meyer, Sven and Robertson, Bruce},
	month = oct,
	year = {2021},
	keywords = {evaluation, historical classical commentaries, Optical Character Recognition (OCR), polytonic Greek},
	pages = {1--6},
	file = {Full Text PDF:/home/jonno/Zotero/storage/359W6GJP/Romanello et al. - 2021 - Optical Character Recognition of 19th Century Clas.pdf:application/pdf},
}

@inproceedings{platanou_handwritten_2022,
	address = {Marseille, France},
	title = {Handwritten {Paleographic} {Greek} {Text} {Recognition}: {A} {Century}-{Based} {Approach}},
	shorttitle = {Handwritten {Paleographic} {Greek} {Text} {Recognition}},
	url = {https://aclanthology.org/2022.lrec-1.708},
	abstract = {Today classicists are provided with a great number of digital tools which, in turn, offer possibilities for further study and new research goals. In this paper we explore the idea that old Greek handwriting can be machine-readable and consequently, researchers can study the target material fast and efficiently. Previous studies have shown that Handwritten Text Recognition (HTR) models are capable of attaining high accuracy rates. However, achieving high accuracy HTR results for Greek manuscripts is still considered to be a major challenge. The overall aim of this paper is to assess HTR for old Greek manuscripts. To address this statement, we study and use digitized images of the Oxford University Bodleian Library Greek manuscripts. By manually transcribing 77 images, we created and present here a new dataset for Handwritten Paleographic Greek Text Recognition. The dataset instances were organized by establishing as a leading factor the century to which the manuscript and hence the image belongs. Experimenting then with an HTR model we show that the error rate depends on the century of the image.},
	urldate = {2024-04-15},
	booktitle = {Proceedings of the {Thirteenth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Platanou, Paraskevi and Pavlopoulos, John and Papaioannou, Georgios},
	editor = {Calzolari, Nicoletta and Béchet, Frédéric and Blache, Philippe and Choukri, Khalid and Cieri, Christopher and Declerck, Thierry and Goggi, Sara and Isahara, Hitoshi and Maegaard, Bente and Mariani, Joseph and Mazo, Hélène and Odijk, Jan and Piperidis, Stelios},
	month = jun,
	year = {2022},
	pages = {6585--6589},
	file = {Full Text PDF:/home/jonno/Zotero/storage/6TTHQJI3/Platanou et al. - 2022 - Handwritten Paleographic Greek Text Recognition A.pdf:application/pdf},
}

@inproceedings{evershed_correcting_2014,
	address = {New York, NY, USA},
	series = {{DATeCH} '14},
	title = {Correcting noisy {OCR}: context beats confusion},
	isbn = {978-1-4503-2588-2},
	shorttitle = {Correcting noisy {OCR}},
	url = {https://dl.acm.org/doi/10.1145/2595188.2595200},
	doi = {10.1145/2595188.2595200},
	abstract = {We describe a system for automatic post OCR text correction of digital collections of historical texts. Documents, such as old newspapers, are often degraded, so even the best OCR tools can yield garbled text. When keywords are corrupted, text is invisible to search tools. Manual correction is not feasible for large collections. Our non-interactive OCR correction method uses a "noisy channel" approach. The error model uses statistically weighted multiple character edits and a novel visual correlation adjustment using low resolution "reverse OCR". The language model uses normal and also "gap" word 3-grams, plus some 5-grams. Word correction candidates are generated by a deep heuristic search of weighted edit combinations guided by a trie. Testing shows good improvements in word error rate. Experiments demonstrate resilience and justify the use of a deep candidate search.},
	urldate = {2024-04-14},
	booktitle = {Proceedings of the {First} {International} {Conference} on {Digital} {Access} to {Textual} {Cultural} {Heritage}},
	publisher = {Association for Computing Machinery},
	author = {Evershed, John and Fitch, Kent},
	month = may,
	year = {2014},
	keywords = {OCR, automatic correction, historical documents, noisy text},
	pages = {45--51},
	file = {Full Text PDF:/home/jonno/Zotero/storage/V8ZC96PN/Evershed and Fitch - 2014 - Correcting noisy OCR context beats confusion.pdf:application/pdf},
}

@misc{zhang_record_2018,
	title = {{ReCoRD}: {Bridging} the {Gap} between {Human} and {Machine} {Commonsense} {Reading} {Comprehension}},
	shorttitle = {{ReCoRD}},
	url = {http://arxiv.org/abs/1810.12885},
	doi = {10.48550/arXiv.1810.12885},
	abstract = {We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.},
	urldate = {2024-04-17},
	publisher = {arXiv},
	author = {Zhang, Sheng and Liu, Xiaodong and Liu, Jingjing and Gao, Jianfeng and Duh, Kevin and Van Durme, Benjamin},
	month = oct,
	year = {2018},
	note = {arXiv:1810.12885 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 14 pages},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/EPSW5GDL/Zhang et al. - 2018 - ReCoRD Bridging the Gap between Human and Machine.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/CXUR3VXW/1810.html:text/html},
}

@inproceedings{levesque_winograd_2012,
	title = {The {Winograd} {Schema} {Challenge}},
	abstract = {In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. A Winograd schema is a pair of sentences that differ only in one or two words and that contain a referential ambiguity that is resolved in opposite directions in the two sentences. We have compiled a collection of Winograd schemas, designed so that the correct answer is obvious to the human reader, but cannot easily be found using selectional restrictions or statistical techniques over text corpora. A contestant in the Winograd Schema Challenge is presented with a collection of one sentence from each pair, and required to achieve human-level accuracy in choosing the correct disambiguation.},
	language = {en},
	booktitle = {Proceedings of the {Thirteenth} {International} {Conference} on {Principles} of {Knowledge} {Representation} and {Reasoning}},
	author = {Levesque, Hector and Davis, Ernest and Morgenstern, Leora},
	year = {2012},
	file = {Levesque et al. - The Winograd Schema Challenge.pdf:/home/jonno/Zotero/storage/6XJ37KTT/Levesque et al. - The Winograd Schema Challenge.pdf:application/pdf},
}

@inproceedings{khashabi_looking_2018,
	address = {New Orleans, Louisiana},
	title = {Looking {Beyond} the {Surface}: {A} {Challenge} {Set} for {Reading} {Comprehension} over {Multiple} {Sentences}},
	shorttitle = {Looking {Beyond} the {Surface}},
	url = {https://aclanthology.org/N18-1023},
	doi = {10.18653/v1/N18-1023},
	abstract = {We present a reading comprehension challenge in which questions can only be answered by taking into account information from multiple sentences. We solicit and verify questions and answers for this challenge through a 4-step crowdsourcing experiment. Our challenge dataset contains 6,500+ questions for 1000+ paragraphs across 7 different domains (elementary school science, news, travel guides, fiction stories, etc) bringing in linguistic diversity to the texts and to the questions wordings. On a subset of our dataset, we found human solvers to achieve an F1-score of 88.1\%. We analyze a range of baselines, including a recent state-of-art reading comprehension system, and demonstrate the difficulty of this challenge, despite a high human performance. The dataset is the first to study multi-sentence inference at scale, with an open-ended set of question types that requires reasoning skills.},
	urldate = {2024-04-17},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Khashabi, Daniel and Chaturvedi, Snigdha and Roth, Michael and Upadhyay, Shyam and Roth, Dan},
	editor = {Walker, Marilyn and Ji, Heng and Stent, Amanda},
	month = jun,
	year = {2018},
	pages = {252--262},
	file = {Full Text PDF:/home/jonno/Zotero/storage/KP29CMZG/Khashabi et al. - 2018 - Looking Beyond the Surface A Challenge Set for Re.pdf:application/pdf},
}

@misc{zhong_toward_2022,
	title = {Toward {Efficient} {Language} {Model} {Pretraining} and {Downstream} {Adaptation} via {Self}-{Evolution}: {A} {Case} {Study} on {SuperGLUE}},
	shorttitle = {Toward {Efficient} {Language} {Model} {Pretraining} and {Downstream} {Adaptation} via {Self}-{Evolution}},
	url = {http://arxiv.org/abs/2212.01853},
	doi = {10.48550/arXiv.2212.01853},
	abstract = {This technical report briefly describes our JDExplore d-team's Vega v2 submission on the SuperGLUE leaderboard. SuperGLUE is more challenging than the widely used general language understanding evaluation (GLUE) benchmark, containing eight difficult language understanding tasks, including question answering, natural language inference, word sense disambiguation, coreference resolution, and reasoning. [Method] Instead of arbitrarily increasing the size of a pretrained language model (PLM), our aim is to 1) fully extract knowledge from the input pretraining data given a certain parameter budget, e.g., 6B, and 2) effectively transfer this knowledge to downstream tasks. To achieve goal 1), we propose self-evolution learning for PLMs to wisely predict the informative tokens that should be masked, and supervise the masked language modeling (MLM) process with rectified smooth labels. For goal 2), we leverage the prompt transfer technique to improve the low-resource tasks by transferring the knowledge from the foundation model and related downstream tasks to the target task. [Results] According to our submission record (Oct. 2022), with our optimized pretraining and fine-tuning strategies, our 6B Vega method achieved new state-of-the-art performance on 4/8 tasks, sitting atop the SuperGLUE leaderboard on Oct. 8, 2022, with an average score of 91.3.},
	urldate = {2024-04-18},
	publisher = {arXiv},
	author = {Zhong, Qihuang and Ding, Liang and Zhan, Yibing and Qiao, Yu and Wen, Yonggang and Shen, Li and Liu, Juhua and Yu, Baosheng and Du, Bo and Chen, Yixin and Gao, Xinbo and Miao, Chunyan and Tang, Xiaoou and Tao, Dacheng},
	month = dec,
	year = {2022},
	note = {arXiv:2212.01853 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Technical report},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/MSDWZ5TS/Zhong et al. - 2022 - Toward Efficient Language Model Pretraining and Do.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/W5FGZ9HT/2212.html:text/html},
}

@misc{evershed_overproof_2014,
	title = {{OverProof} - {Evaluation}},
	url = {http://overproof.projectcomputing.com/evaluation},
	urldate = {2024-04-19},
	author = {Evershed, John and Fitch, Kent},
	year = {2014},
	file = {OverProof - Evaluation:/home/jonno/Zotero/storage/EDMLDFNF/evaluation.html:text/html},
}

@misc{agarwal_many-shot_2024,
	title = {Many-{Shot} {In}-{Context} {Learning}},
	url = {http://arxiv.org/abs/2404.11018},
	abstract = {Large language models (LLMs) excel at few-shot in-context learning (ICL) -- learning from a few examples provided in context at inference, without any weight updates. Newly expanded context windows allow us to investigate ICL with hundreds or thousands of examples -- the many-shot regime. Going from few-shot to many-shot, we observe significant performance gains across a wide variety of generative and discriminative tasks. While promising, many-shot ICL can be bottlenecked by the available amount of human-generated examples. To mitigate this limitation, we explore two new settings: Reinforced and Unsupervised ICL. Reinforced ICL uses model-generated chain-of-thought rationales in place of human examples. Unsupervised ICL removes rationales from the prompt altogether, and prompts the model only with domain-specific questions. We find that both Reinforced and Unsupervised ICL can be quite effective in the many-shot regime, particularly on complex reasoning tasks. Finally, we demonstrate that, unlike few-shot learning, many-shot learning is effective at overriding pretraining biases and can learn high-dimensional functions with numerical inputs. Our analysis also reveals the limitations of next-token prediction loss as an indicator of downstream ICL performance.},
	language = {en},
	urldate = {2024-04-21},
	publisher = {arXiv},
	author = {Agarwal, Rishabh and Singh, Avi and Zhang, Lei M. and Bohnet, Bernd and Chan, Stephanie and Anand, Ankesh and Abbas, Zaheer and Nova, Azade and Co-Reyes, John D. and Chu, Eric and Behbahani, Feryal and Faust, Aleksandra and Larochelle, Hugo},
	month = apr,
	year = {2024},
	note = {arXiv:2404.11018 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Agarwal et al. - 2024 - Many-Shot In-Context Learning.pdf:/home/jonno/Zotero/storage/IMTH6JJ8/Agarwal et al. - 2024 - Many-Shot In-Context Learning.pdf:application/pdf},
}

@misc{humanities_chronicling_2005,
	type = {Text},
	title = {Chronicling {America} {\textbar} {Library} of {Congress}},
	copyright = {Text is U.S. Government Work},
	url = {https://chroniclingamerica.loc.gov/},
	abstract = {Search America's historic newspaper pages from 1756-1963 or use the U.S. Newspaper Directory to find information about American newspapers published between 1690-present.},
	language = {eng},
	urldate = {2024-05-01},
	author = {Humanities, National Endowment for the},
	year = {2005},
	file = {Snapshot:/home/jonno/Zotero/storage/I7EX3GJ6/chroniclingamerica.loc.gov.html:text/html},
}

@techreport{aimeta_llama_2024,
	title = {Llama 3 {Model} {Card}},
	url = {https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md},
	institution = {Meta AI},
	author = {{AI@Meta}},
	year = {2024},
}

@misc{lazar_filling_2021,
	title = {Filling the {Gaps} in {Ancient} {Akkadian} {Texts}: {A} {Masked} {Language} {Modelling} {Approach}},
	shorttitle = {Filling the {Gaps} in {Ancient} {Akkadian} {Texts}},
	url = {http://arxiv.org/abs/2109.04513},
	doi = {10.48550/arXiv.2109.04513},
	abstract = {We present models which complete missing text given transliterations of ancient Mesopotamian documents, originally written on cuneiform clay tablets (2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on contextual cues to manually fill in missing parts in the text in a subjective and time-consuming process. We identify that this challenge can be formulated as a masked language modelling task, used mostly as a pretraining objective for contextualized language models. Following, we develop several architectures focusing on the Akkadian language, the lingua franca of the time. We find that despite data scarcity (1M tokens) we can achieve state of the art performance on missing tokens prediction (89\% hit@5) using a greedy decoding scheme and pretraining on data from other languages and different time periods. Finally, we conduct human evaluations showing the applicability of our models in assisting experts to transcribe texts in extinct languages.},
	urldate = {2024-05-07},
	publisher = {arXiv},
	author = {Lazar, Koren and Saret, Benny and Yehudai, Asaf and Horowitz, Wayne and Wasserman, Nathan and Stanovsky, Gabriel},
	month = oct,
	year = {2021},
	note = {arXiv:2109.04513 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to EMNLP 2021 (Main Conference)},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/A8ZJL4TJ/Lazar et al. - 2021 - Filling the Gaps in Ancient Akkadian Texts A Mask.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/VHW86QRM/2109.html:text/html},
}

@inproceedings{fono_embible_2024,
	address = {St. Julian's, Malta},
	title = {Embible: {Reconstruction} of {Ancient} {Hebrew} and {Aramaic} {Texts} {Using} {Transformers}},
	shorttitle = {Embible},
	url = {https://aclanthology.org/2024.findings-eacl.56},
	abstract = {Hebrew and Aramaic inscriptions serve as an essential source of information on the ancient history of the Near East. Unfortunately, some parts of the inscribed texts become illegible over time. Special experts, called epigraphists, use time-consuming manual procedures to estimate the missing content. This problem can be considered an extended masked language modeling task, where the damaged content can comprise single characters, character n-grams (partial words), single complete words, and multi-word n-grams.This study is the first attempt to apply the masked language modeling approach to corrupted inscriptions in Hebrew and Aramaic languages, both using the Hebrew alphabet consisting mostly of consonant symbols. In our experiments, we evaluate several transformer-based models, which are fine-tuned on the Biblical texts and tested on three different percentages of randomly masked parts in the testing corpus. For any masking percentage, the highest text completion accuracy is obtained with a novel ensemble of word and character prediction models.},
	urldate = {2024-05-07},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EACL} 2024},
	publisher = {Association for Computational Linguistics},
	author = {Fono, Niv and Moshayof, Harel and Karol, Eldar and Assraf, Itai and Last, Mark},
	editor = {Graham, Yvette and Purver, Matthew},
	month = mar,
	year = {2024},
	pages = {846--852},
	file = {Full Text PDF:/home/jonno/Zotero/storage/6VNSMUJ7/Fono et al. - 2024 - Embible Reconstruction of Ancient Hebrew and Aram.pdf:application/pdf},
}

@misc{gloeckle_better_2024,
	title = {Better \& {Faster} {Large} {Language} {Models} via {Multi}-token {Prediction}},
	doi = {10.48550/arXiv.2404.19737},
	language = {en},
	publisher = {arXiv},
	author = {Gloeckle, Fabian and Idrissi, Badr Youbi and Rozière, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
	month = apr,
	year = {2024},
	file = {Gloeckle et al. - Better & Faster Large Language Models via Multi-to.pdf:/home/jonno/Zotero/storage/BPBX38YQ/Gloeckle et al. - Better & Faster Large Language Models via Multi-to.pdf:application/pdf},
}

@inproceedings{thomas_leveraging_2024,
	address = {Torino, Italia},
	title = {Leveraging {LLMs} for {Post}-{OCR} {Correction} of {Historical} {Newspapers}},
	isbn = {978-2-493-81446-3},
	abstract = {Poor OCR quality continues to be a major obstacle for humanities scholars seeking to make use of digitised primary sources such as historical newspapers. Typical approaches to post-OCR correction employ sequence-to-sequence models for a neural machine translation task, mapping erroneous OCR texts to accurate reference texts. We shift our focus towards the adaptation of generative LLMs for a prompt-based approach. By instruction-tuning Llama 2 and comparing it to a fine-tuned BART on BLN600, a parallel corpus of 19th century British newspaper articles, we demonstrate the potential of a prompt-based approach in detecting and correcting OCR errors, even with limited training data. We achieve a significant enhancement in OCR quality with Llama 2 outperforming BART, achieving a 54.51\% reduction in the character error rate against BART’s 23.30\%. This paves the way for future work leveraging generative LLMs to improve the accessibility and unlock the full potential of historical texts for humanities research.},
	language = {en},
	booktitle = {Proceedings of the {Third} {Workshop} on {Language} {Technologies} for {Historical} and {Ancient} {Languages} ({LT4HALA}) @ {LREC}-{COLING}-2024},
	publisher = {ELRA and ICCL},
	author = {Thomas, Alan and Gaizauskas, Robert and Lu, Haiping},
	month = may,
	year = {2024},
	pages = {116--121},
	file = {Thomas et al. - Leveraging LLMs for Post-OCR Correction of Histori.pdf:/home/jonno/Zotero/storage/JQ2CAVHA/Thomas et al. - Leveraging LLMs for Post-OCR Correction of Histori.pdf:application/pdf},
}

@misc{chung_scaling_2022,
	title = {Scaling {Instruction}-{Finetuned} {Language} {Models}},
	url = {http://arxiv.org/abs/2210.11416},
	abstract = {Finetuning language models on a collection of datasets phrased as instructions has been shown to improve model performance and generalization to unseen tasks. In this paper we explore instruction ﬁnetuning with a particular focus on (1) scaling the number of tasks, (2) scaling the model size, and (3) ﬁnetuning on chain-of-thought data. We ﬁnd that instruction ﬁnetuning with the above aspects dramatically improves performance on a variety of model classes (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and evaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation, RealToxicityPrompts). For instance, Flan-PaLM 540B instruction-ﬁnetuned on 1.8K tasks outperforms PaLM 540B by a large margin (+9.4\% on average). Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2\% on ﬁve-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction ﬁnetuning is a general method for improving the performance and usability of pretrained language models.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Castro-Ros, Alex and Pellat, Marie and Robinson, Kevin and Valter, Dasha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason},
	month = dec,
	year = {2022},
	note = {arXiv:2210.11416 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Public checkpoints: https://huggingface.co/docs/transformers/model\_doc/flan-t5},
	file = {Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:/home/jonno/Zotero/storage/WCEPD4UJ/Chung et al. - 2022 - Scaling Instruction-Finetuned Language Models.pdf:application/pdf},
}

@misc{raffel_exploring_2023,
	title = {Exploring the {Limits} of {Transfer} {Learning} with a {Unified} {Text}-to-{Text} {Transformer}},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = sep,
	year = {2023},
	note = {arXiv:1910.10683 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computation and Language},
	file = {Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:/home/jonno/Zotero/storage/PGW8WBJK/Raffel et al. - 2023 - Exploring the Limits of Transfer Learning with a U.pdf:application/pdf},
}

@misc{zhao_lora_2024,
	title = {{LoRA} {Land}: 310 {Fine}-tuned {LLMs} that {Rival} {GPT}-4, {A} {Technical} {Report}},
	shorttitle = {{LoRA} {Land}},
	url = {http://arxiv.org/abs/2405.00732},
	abstract = {Low Rank Adaptation (LoRA) has emerged as one of the most widely adopted methods for Parameter Efficient Fine-Tuning (PEFT) of Large Language Models (LLMs). LoRA reduces the number of trainable parameters and memory usage while achieving comparable performance to full fine-tuning. We aim to assess the viability of training and serving LLMs fine-tuned with LoRA in real-world applications. First, we measure the quality of LLMs fine-tuned with quantized low rank adapters across 10 base models and 31 tasks for a total of 310 models. We find that 4-bit LoRA fine-tuned models outperform base models by 34 points and GPT-4 by 10 points on average. Second, we investigate the most effective base models for fine-tuning and assess the correlative and predictive capacities of task complexity heuristics in forecasting the outcomes of fine-tuning. Finally, we evaluate the latency and concurrency capabilities of LoRAX, an open-source Multi-LoRA inference server that facilitates the deployment of multiple LoRA fine-tuned models on a single GPU using shared base model weights and dynamic adapter loading. LoRAX powers LoRA Land, a web application that hosts 25 LoRA fine-tuned Mistral-7B LLMs on a single NVIDIA A100 GPU with 80GB memory. LoRA Land highlights the quality and cost-effectiveness of employing multiple specialized LLMs over a single, general-purpose LLM.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Zhao, Justin and Wang, Timothy and Abid, Wael and Angus, Geoffrey and Garg, Arnav and Kinnison, Jeffery and Sherstinsky, Alex and Molino, Piero and Addair, Travis and Rishi, Devvret},
	month = apr,
	year = {2024},
	note = {arXiv:2405.00732 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A.pdf:/home/jonno/Zotero/storage/DKNKA9HJ/Zhao et al. - 2024 - LoRA Land 310 Fine-tuned LLMs that Rival GPT-4, A.pdf:application/pdf},
}

@misc{zheng_llamafactory_2024,
	title = {{LlamaFactory}: {Unified} {Efficient} {Fine}-{Tuning} of 100+ {Language} {Models}},
	shorttitle = {{LlamaFactory}},
	url = {http://arxiv.org/abs/2403.13372},
	abstract = {Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LLAMAFACTORY, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LLAMABOARD. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github. com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Zheng, Yaowei and Zhang, Richong and Zhang, Junhao and Ye, Yanhan and Luo, Zheyan and Ma, Yongqiang},
	month = mar,
	year = {2024},
	note = {arXiv:2403.13372 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 12 pages, preprint},
	file = {Zheng et al. - 2024 - LlamaFactory Unified Efficient Fine-Tuning of 100.pdf:/home/jonno/Zotero/storage/XSFZ9VHM/Zheng et al. - 2024 - LlamaFactory Unified Efficient Fine-Tuning of 100.pdf:application/pdf},
}

@misc{tay_ul2_2023,
	title = {{UL2}: {Unifying} {Language} {Learning} {Paradigms}},
	shorttitle = {{UL2}},
	url = {http://arxiv.org/abs/2205.05131},
	abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a uniﬁed framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives – two concepts that are commonly conﬂated. Next, we present a generalized and uniﬁed perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream ﬁne-tuning is associated with speciﬁc pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and ﬁnd that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classiﬁcation, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. We release Flax-based T5X model checkpoints for the 20B model at https: //github.com/google-research/google-research/tree/master/ul2.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
	month = feb,
	year = {2023},
	note = {arXiv:2205.05131 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Updated Q1 2023 with Flan-UL2 20B release! :)},
	file = {Tay et al. - 2023 - UL2 Unifying Language Learning Paradigms.pdf:/home/jonno/Zotero/storage/2HGAUK37/Tay et al. - 2023 - UL2 Unifying Language Learning Paradigms.pdf:application/pdf},
}

@misc{biderman_lora_2024,
	title = {{LoRA} {Learns} {Less} and {Forgets} {Less}},
	url = {http://arxiv.org/abs/2405.09673},
	abstract = {Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (≈100K prompt-response pairs) and continued pretraining (≈10B unstructured tokens) data regimes. Our results show that, in most settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA exhibits a desirable form of regularization: it better maintains the base model’s performance on tasks outside the target domain. We show that LoRA provides stronger regularization compared to common techniques such as weight decay and dropout; it also helps maintain more diverse generations. We show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.},
	language = {en},
	urldate = {2024-06-05},
	publisher = {arXiv},
	author = {Biderman, Dan and Ortiz, Jose Gonzalez and Portes, Jacob and Paul, Mansheej and Greengard, Philip and Jennings, Connor and King, Daniel and Havens, Sam and Chiley, Vitaliy and Frankle, Jonathan and Blakeney, Cody and Cunningham, John P.},
	month = may,
	year = {2024},
	note = {arXiv:2405.09673 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {Biderman et al. - 2024 - LoRA Learns Less and Forgets Less.pdf:/home/jonno/Zotero/storage/2FRNPEP8/Biderman et al. - 2024 - LoRA Learns Less and Forgets Less.pdf:application/pdf},
}

@misc{piryani_chroniclingamericaqa_2024,
	title = {{ChroniclingAmericaQA}: {A} {Large}-scale {Question} {Answering} {Dataset} based on {Historical} {American} {Newspaper} {Pages}},
	shorttitle = {{ChroniclingAmericaQA}},
	url = {http://arxiv.org/abs/2403.17859},
	doi = {10.1145/3626772.3657891},
	abstract = {Question answering (QA) and Machine Reading Comprehension (MRC) tasks have significantly advanced in recent years due to the rapid development of deep learning techniques and, more recently, large language models. At the same time, many benchmark datasets have become available for QA and MRC tasks. However, most existing large-scale benchmark datasets have been created predominantly using synchronous document collections like Wikipedia or the Web. Archival document collections, such as historical newspapers, contain valuable information from the past that is still not widely used to train large language models. To further contribute to advancing QA and MRC tasks and to overcome the limitation of previous datasets, we introduce ChroniclingAmericaQA, a largescale temporal QA dataset with 487K question-answer pairs created based on the historical newspaper collection Chronicling America. Our dataset is constructed from a subset of the Chronicling America newspaper collection spanning 120 years. One of the significant challenges for utilizing digitized historical newspaper collections is the low quality of OCR text. Therefore, to enable realistic testing of QA models, our dataset can be used in three different ways: answering questions from raw and noisy content, answering questions from cleaner, corrected version of the content, as well as answering questions from scanned images of newspaper pages. This and the fact that ChroniclingAmericaQA spans the longest time period among available QA datasets make it quite a unique and useful resource.},
	language = {en},
	urldate = {2024-06-05},
	author = {Piryani, Bhawna and Mozafari, Jamshid and Jatowt, Adam},
	month = may,
	year = {2024},
	note = {arXiv:2403.17859 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at SIGIR 2024},
	file = {Piryani et al. - 2024 - ChroniclingAmericaQA A Large-scale Question Answe.pdf:/home/jonno/Zotero/storage/X2EWAXAM/Piryani et al. - 2024 - ChroniclingAmericaQA A Large-scale Question Answe.pdf:application/pdf},
}

@inproceedings{kuhn_semantic_2022,
	title = {Semantic {Uncertainty}: {Linguistic} {Invariances} for {Uncertainty} {Estimation} in {Natural} {Language} {Generation}},
	shorttitle = {Semantic {Uncertainty}},
	url = {https://openreview.net/forum?id=VD-AYtP0dve},
	abstract = {We introduce a method to measure uncertainty in large language models. For tasks like question answering, it is essential to know when we can trust the natural language outputs of foundation models. We show that measuring uncertainty in natural language is challenging because of "semantic equivalence"—different sentences can mean the same thing. To overcome these challenges we introduce semantic entropy—an entropy which incorporates linguistic invariances created by shared meanings. Our method is unsupervised, uses only a single model, and requires no modifications to off-the-shelf language models. In comprehensive ablation studies we show that the semantic entropy is more predictive of model accuracy on question answering data sets than comparable baselines.},
	language = {en},
	urldate = {2024-08-07},
	author = {Kuhn, Lorenz and Gal, Yarin and Farquhar, Sebastian},
	month = sep,
	year = {2022},
	file = {Full Text PDF:/home/jonno/Zotero/storage/4QZ6DLCL/Kuhn et al. - 2022 - Semantic Uncertainty Linguistic Invariances for U.pdf:application/pdf},
}

@article{farquhar_detecting_2024,
	title = {Detecting hallucinations in large language models using semantic entropy},
	volume = {630},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07421-0},
	doi = {10.1038/s41586-024-07421-0},
	abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often ‘hallucinate’ false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations—confabulations—which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
	language = {en},
	number = {8017},
	urldate = {2024-08-07},
	journal = {Nature},
	author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
	month = jun,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Information technology},
	pages = {625--630},
	file = {Full Text PDF:/home/jonno/Zotero/storage/DGXX3NPR/Farquhar et al. - 2024 - Detecting hallucinations in large language models .pdf:application/pdf},
}

@article{hullermeier_aleatoric_2021,
	title = {Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods},
	volume = {110},
	issn = {1573-0565},
	shorttitle = {Aleatoric and epistemic uncertainty in machine learning},
	url = {https://doi.org/10.1007/s10994-021-05946-3},
	doi = {10.1007/s10994-021-05946-3},
	abstract = {The notion of uncertainty is of major importance in machine learning and constitutes a key element of machine learning methodology. In line with the statistical tradition, uncertainty has long been perceived as almost synonymous with standard probability and probabilistic predictions. Yet, due to the steadily increasing relevance of machine learning for practical applications and related issues such as safety requirements, new problems and challenges have recently been identified by machine learning scholars, and these problems may call for new methodological developments. In particular, this includes the importance of distinguishing between (at least) two different types of uncertainty, often referred to as aleatoric and epistemic. In this paper, we provide an introduction to the topic of uncertainty in machine learning as well as an overview of attempts so far at handling uncertainty in general and formalizing this distinction in particular.},
	language = {en},
	number = {3},
	urldate = {2024-08-07},
	journal = {Machine Learning},
	author = {Hüllermeier, Eyke and Waegeman, Willem},
	month = mar,
	year = {2021},
	keywords = {Bayesian inference, Calibration, Conformal prediction, Credal sets and classifiers, Deep neural networks, Ensembles, Epistemic uncertainty, Gaussian processes, Generative models, Likelihood-based methods, Probability, Set-valued prediction, Uncertainty, Version space learning},
	pages = {457--506},
	file = {Full Text PDF:/home/jonno/Zotero/storage/NQVMA47H/Hüllermeier and Waegeman - 2021 - Aleatoric and epistemic uncertainty in machine lea.pdf:application/pdf},
}

@misc{yadkori_believe_2024,
	title = {To {Believe} or {Not} to {Believe} {Your} {LLM}},
	url = {http://arxiv.org/abs/2406.02543},
	doi = {10.48550/arXiv.2406.02543},
	abstract = {We explore uncertainty quantification in large language models (LLMs), with the goal to identify when uncertainty in responses given a query is large. We simultaneously consider both epistemic and aleatoric uncertainties, where the former comes from the lack of knowledge about the ground truth (such as about facts or the language), and the latter comes from irreducible randomness (such as multiple possible answers). In particular, we derive an information-theoretic metric that allows to reliably detect when only epistemic uncertainty is large, in which case the output of the model is unreliable. This condition can be computed based solely on the output of the model obtained simply by some special iterative prompting based on the previous responses. Such quantification, for instance, allows to detect hallucinations (cases when epistemic uncertainty is high) in both single- and multi-answer responses. This is in contrast to many standard uncertainty quantification strategies (such as thresholding the log-likelihood of a response) where hallucinations in the multi-answer case cannot be detected. We conduct a series of experiments which demonstrate the advantage of our formulation. Further, our investigations shed some light on how the probabilities assigned to a given output by an LLM can be amplified by iterative prompting, which might be of independent interest.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Yadkori, Yasin Abbasi and Kuzborskij, Ilja and György, András and Szepesvári, Csaba},
	month = jul,
	year = {2024},
	note = {arXiv:2406.02543 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/UUSTC6F3/Yadkori et al. - 2024 - To Believe or Not to Believe Your LLM.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/H3P8DRDA/2406.html:text/html},
}

@misc{wang_docllm_2023,
	title = {{DocLLM}: {A} layout-aware generative language model for multimodal document understanding},
	shorttitle = {{DocLLM}},
	url = {http://arxiv.org/abs/2401.00908},
	doi = {10.48550/arXiv.2401.00908},
	abstract = {Enterprise documents such as forms, invoices, receipts, reports, contracts, and other similar records, often carry rich semantics at the intersection of textual and spatial modalities. The visual cues offered by their complex layouts play a crucial role in comprehending these documents effectively. In this paper, we present DocLLM, a lightweight extension to traditional large language models (LLMs) for reasoning over visual documents, taking into account both textual semantics and spatial layout. Our model differs from existing multimodal LLMs by avoiding expensive image encoders and focuses exclusively on bounding box information to incorporate the spatial layout structure. Specifically, the cross-alignment between text and spatial modalities is captured by decomposing the attention mechanism in classical transformers to a set of disentangled matrices. Furthermore, we devise a pre-training objective that learns to infill text segments. This approach allows us to address irregular layouts and heterogeneous content frequently encountered in visual documents. The pre-trained model is fine-tuned using a large-scale instruction dataset, covering four core document intelligence tasks. We demonstrate that our solution outperforms SotA LLMs on 14 out of 16 datasets across all tasks, and generalizes well to 4 out of 5 previously unseen datasets.},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Wang, Dongsheng and Raman, Natraj and Sibue, Mathieu and Ma, Zhiqiang and Babkin, Petr and Kaur, Simerjot and Pei, Yulong and Nourbakhsh, Armineh and Liu, Xiaomo},
	month = dec,
	year = {2023},
	note = {arXiv:2401.00908 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 16 pages, 4 figures},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/LP6PA9HM/Wang et al. - 2023 - DocLLM A layout-aware generative language model f.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/UCTYM35A/2401.html:text/html},
}

@inproceedings{bagla_noisy_2024,
	address = {New York, NY, USA},
	series = {{AIMLSystems} '23},
	title = {Noisy {Text} {Data}: foible of popular {Transformer} based {NLP} models},
	isbn = {9798400716492},
	shorttitle = {Noisy {Text} {Data}},
	url = {https://doi.org/10.1145/3639856.3639889},
	doi = {10.1145/3639856.3639889},
	abstract = {In the past few years, researchers working in natural language processing have created a number of new models based on transformer architecture. These models have shown remarkable performance for various NLP tasks on benchmark datasets, often surpassing SOTA results. Buoyed with these results, industry practitioners are actively experimenting with fine-tuning these models to build NLP applications for industry use cases. However, for most datasets used by practitioners (to build these applications), it is hard to guarantee the complete absence of any noise in the data. While most transformer-based NLP models have performed exceedingly well in transferring the learnings from one dataset to another for various tasks, it remains unclear how these models perform when fine-tuned on noisy text. In this paper, we precisely do this. We explore the sensitivity of popular transformer-based NLP models to noise in the text data. We show (via experimental results) that the popular transformer-based NLP models such as BERT, RoBERTa, ALBERT, XLNet and T5 perform badly on fundamental NLP tasks (namely, text classification, textual similarity, NER, question answering, text summarization) when fine-tuned on a noisy version of benchmark datasets. We further show that as the quantum of noise in the data increases, the performance of these models degrades rapidly. Our findings suggest that one must cautious of presence of any noise in their datasets before fine-tuning popular transformer-based NLP models on their datasets.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the {Third} {International} {Conference} on {AI}-{ML} {Systems}},
	publisher = {Association for Computing Machinery},
	author = {Bagla, Kartikay and Gupta, Shivam and Kumar, Ankit and Gupta, Anuj},
	month = may,
	year = {2024},
	pages = {1--5},
}

@misc{li_can_2023,
	title = {Can {Pretrained} {Language} {Models} {Derive} {Correct} {Semantics} from {Corrupt} {Subwords} under {Noise}?},
	url = {http://arxiv.org/abs/2306.15268},
	abstract = {For Pretrained Language Models (PLMs), their susceptibility to noise has recently been linked to subword segmentation. However, it is unclear which aspects of segmentation affect their understanding. This study assesses the robustness of PLMs against various disrupted segmentation caused by noise. An evaluation framework for subword segmentation, named Contrastive Lexical Semantic (CoLeS) probe, is proposed. It provides a systematic categorization of segmentation corruption under noise and evaluation protocols by generating contrastive datasets with canonical-noisy word pairs. Experimental results indicate that PLMs are unable to accurately compute word meanings if the noise introduces completely different subwords, small subword fragments, or a large number of additional subwords, particularly when they are inserted within other subwords.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Li, Xinzhe and Liu, Ming and Gao, Shang},
	month = jun,
	year = {2023},
	note = {arXiv:2306.15268 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Li et al. - 2023 - Can Pretrained Language Models Derive Correct Sema.pdf:/home/jonno/Zotero/storage/B792GZDY/Li et al. - 2023 - Can Pretrained Language Models Derive Correct Sema.pdf:application/pdf},
}

@inproceedings{li_textbugger_2019,
	title = {{TextBugger}: {Generating} {Adversarial} {Text} {Against} {Real}-world {Applications}},
	shorttitle = {{TextBugger}},
	url = {http://arxiv.org/abs/1812.05271},
	doi = {10.14722/ndss.2019.23138},
	abstract = {Deep Learning-based Text Understanding (DLTU) is the backbone technique behind various applications, including question answering, machine translation, and text classiﬁcation. Despite its tremendous popularity, the security vulnerabilities of DLTU are still largely unknown, which is highly concerning given its increasing use in security-sensitive applications such as sentiment analysis and toxic content detection. In this paper, we show that DLTU is inherently vulnerable to adversarial text attacks, in which maliciously crafted texts trigger target DLTU systems and services to misbehave. Speciﬁcally, we present TEXTBUGGER, a general attack framework for generating adversarial texts. In contrast to prior works, TEXTBUGGER differs in signiﬁcant ways: (i) effective – it outperforms state-of-the-art attacks in terms of attack success rate; (ii) evasive – it preserves the utility of benign text, with 94.9\% of the adversarial text correctly recognized by human readers; and (iii) efﬁcient – it generates adversarial text with computational complexity sub-linear to the text length. We empirically evaluate TEXTBUGGER on a set of real-world DLTU systems and services used for sentiment analysis and toxic content detection, demonstrating its effectiveness, evasiveness, and efﬁciency. For instance, TEXTBUGGER achieves 100\% success rate on the IMDB dataset based on Amazon AWS Comprehend within 4.61 seconds and preserves 97\% semantic similarity. We further discuss possible defense mechanisms to mitigate such attack and the adversary’s potential countermeasures, which leads to promising directions for further research.},
	language = {en},
	urldate = {2024-08-07},
	booktitle = {Proceedings 2019 {Network} and {Distributed} {System} {Security} {Symposium}},
	author = {Li, Jinfeng and Ji, Shouling and Du, Tianyu and Li, Bo and Wang, Ting},
	year = {2019},
	note = {arXiv:1812.05271 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Cryptography and Security},
	annote = {Comment: To appear in NDSS 2019},
	file = {Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf:/home/jonno/Zotero/storage/SLTRUSYM/Li et al. - 2019 - TextBugger Generating Adversarial Text Against Re.pdf:application/pdf},
}

@misc{huang_noisyag-news_2024,
	title = {{NoisyAG}-{News}: {A} {Benchmark} for {Addressing} {Instance}-{Dependent} {Noise} in {Text} {Classification}},
	shorttitle = {{NoisyAG}-{News}},
	url = {http://arxiv.org/abs/2407.06579},
	abstract = {Existing research on learning with noisy labels predominantly focuses on synthetic label noise. Although synthetic noise possesses well-defined structural properties, it often fails to accurately replicate real-world noise patterns. In recent years, there has been a concerted effort to construct generalizable and controllable instance-dependent noise datasets for image classification, significantly advancing the development of noise-robust learning in this area. However, studies on noisy label learning for text classification remain scarce. To better understand label noise in real-world text classification settings, we constructed the benchmark dataset NoisyAG-News through manual annotation. Initially, we analyzed the annotated data to gather observations about real-world noise. We qualitatively and quantitatively demonstrated that real-world noisy labels adhere to instance-dependent patterns. Subsequently, we conducted comprehensive learning experiments on NoisyAG-News and its corresponding synthetic noise datasets using pre-trained language models and noise-handling techniques. Our findings reveal that while pre-trained models are resilient to synthetic noise, they struggle against instancedependent noise, with samples of varying confusion levels showing inconsistent performance during training and testing. These real-world noise patterns pose new, significant challenges, prompting a reevaluation of noisy label handling methods. We hope that NoisyAG-News will facilitate the development and evaluation of future solutions for learning with noisy labels.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Huang, Hongfei and Liang, Tingting and Sun, Xixi and Jin, Zikang and Yin, Yuyu},
	month = jul,
	year = {2024},
	note = {arXiv:2407.06579 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 20 pages , 13 figure},
	file = {Huang et al. - 2024 - NoisyAG-News A Benchmark for Addressing Instance-.pdf:/home/jonno/Zotero/storage/BK2CYXCL/Huang et al. - 2024 - NoisyAG-News A Benchmark for Addressing Instance-.pdf:application/pdf},
}

@misc{naplava_understanding_2021,
	title = {Understanding {Model} {Robustness} to {User}-generated {Noisy} {Texts}},
	url = {http://arxiv.org/abs/2110.07428},
	abstract = {Sensitivity of deep-neural models to input noise is known to be a challenging problem. In NLP, model performance often deteriorates with naturally occurring noise, such as spelling errors. To mitigate this issue, models may leverage artiﬁcially noised data. However, the amount and type of generated noise has so far been determined arbitrarily. We therefore propose to model the errors statistically from grammatical-error-correction corpora. We present a thorough evaluation of several state-of-the-art NLP systems’ robustness in multiple languages, with tasks including morpho-syntactic analysis, named entity recognition, neural machine translation, a subset of the GLUE benchmark and reading comprehension. We also compare two approaches to address the performance drop: a) training the NLP models with noised data generated by our framework; and b) reducing the input noise with external system for natural language correction. The code is released at https://github.com/ufal/kazitext.},
	language = {en},
	urldate = {2024-08-07},
	publisher = {arXiv},
	author = {Náplava, Jakub and Popel, Martin and Straka, Milan and Straková, Jana},
	month = nov,
	year = {2021},
	note = {arXiv:2110.07428 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted to W-NUT 2021},
	file = {Náplava et al. - 2021 - Understanding Model Robustness to User-generated N.pdf:/home/jonno/Zotero/storage/P5J9YMXG/Náplava et al. - 2021 - Understanding Model Robustness to User-generated N.pdf:application/pdf},
}

@article{hicks_chatgpt_2024,
	title = {{ChatGPT} is bullshit},
	volume = {26},
	issn = {1388-1957},
	url = {https://link.springer.com/epdf/10.1007/s10676-024-09775-5},
	doi = {10.1007/s10676-024-09775-5},
	abstract = {Recently, there has been considerable interest in large language models: machine learning systems which produce human-like text and dialogue. Applications of these systems have been plagued by persistent inaccuracies in their output; these are often called “AI hallucinations”. We argue that these falsehoods, and the overall activity of large language models, is better understood as bullshit in the sense explored by Frankfurt (On Bullshit, Princeton, 2005): the models are in an important way indifferent to the truth of their outputs. We distinguish two ways in which the models can be said to be bullshitters, and argue that they clearly meet at least one of these definitions. We further argue that describing AI misrepresentations as bullshit is both a more useful and more accurate way of predicting and discussing the behaviour of these systems.},
	language = {en},
	number = {2},
	urldate = {2024-08-07},
	journal = {Ethics and Information Technology},
	author = {Hicks, Michael Townsen and Humphries, James and Slater, Joe},
	year = {2024},
}

@inproceedings{booth_bln600_2024,
	address = {Torino, Italia},
	title = {{BLN600}: {A} {Parallel} {Corpus} of {Machine}/{Human} {Transcribed} {Nineteenth} {Century} {Newspaper} {Texts}},
	shorttitle = {{BLN600}},
	url = {https://aclanthology.org/2024.lrec-main.219},
	abstract = {We present a publicly available corpus of nineteenth-century newspaper text focused on crime in London, derived from the Gale British Library Newspapers corpus parts 1 and 2. The corpus comprises 600 newspaper excerpts and for each excerpt contains the original source image, the machine transcription of that image as found in the BLN and a gold standard manual transcription that we have created. We envisage the corpus will be helpful for the training and development of OCR and post-OCR correction methodologies for historical newspaper machine transcription—for which there is currently a dearth of publicly available resources. In this paper, we discuss the rationale behind gathering such a corpus, the methodology used to select, process, and align the data, and the corpus' potential utility for historians and digital humanities researchers—particularly within the realms of neural machine translation-based post-OCR correction approaches, and other natural language processing tasks that are critically affected by erroneous OCR.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the 2024 {Joint} {International} {Conference} on {Computational} {Linguistics}, {Language} {Resources} and {Evaluation} ({LREC}-{COLING} 2024)},
	publisher = {ELRA and ICCL},
	author = {Booth, Callum William and Thomas, Alan and Gaizauskas, Robert},
	editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
	month = may,
	year = {2024},
	pages = {2440--2446},
	file = {Full Text PDF:/home/jonno/Zotero/storage/JIWDKEBP/Booth et al. - 2024 - BLN600 A Parallel Corpus of MachineHuman Transcr.pdf:application/pdf},
}

@inproceedings{edunov_understanding_2018,
	address = {Brussels, Belgium},
	title = {Understanding {Back}-{Translation} at {Scale}},
	url = {https://aclanthology.org/D18-1045},
	doi = {10.18653/v1/D18-1045},
	abstract = {An effective method to improve neural machine translation with monolingual data is to augment the parallel training corpus with back-translations of target language sentences. This work broadens the understanding of back-translation and investigates a number of methods to generate synthetic source sentences. We find that in all but resource poor settings back-translations obtained via sampling or noised beam outputs are most effective. Our analysis shows that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. We also compare how synthetic data compares to genuine bitext and study various domain effects. Finally, we scale to hundreds of millions of monolingual sentences and achieve a new state of the art of 35 BLEU on the WMT'14 English-German test set.},
	urldate = {2024-08-07},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Edunov, Sergey and Ott, Myle and Auli, Michael and Grangier, David},
	editor = {Riloff, Ellen and Chiang, David and Hockenmaier, Julia and Tsujii, Jun'ichi},
	month = oct,
	year = {2018},
	pages = {489--500},
	file = {Full Text PDF:/home/jonno/Zotero/storage/KABASYN2/Edunov et al. - 2018 - Understanding Back-Translation at Scale.pdf:application/pdf},
}

@inproceedings{rychalska_models_2019,
	address = {Cham},
	title = {Models in the {Wild}: {On} {Corruption} {Robustness} of {Neural} {NLP} {Systems}},
	isbn = {978-3-030-36718-3},
	shorttitle = {Models in the {Wild}},
	doi = {10.1007/978-3-030-36718-3_20},
	abstract = {Natural Language Processing models lack a unified approach to robustness testing. In this paper we introduce WildNLP - a framework for testing model stability in a natural setting where text corruptions such as keyboard errors or misspelling occur. We compare robustness of deep learning models from 4 popular NLP tasks: Q\&A, NLI, NER and Sentiment Analysis by testing their performance on aspects introduced in the framework. In particular, we focus on a comparison between recent state-of-the-art text representations and non-contextualized word embeddings. In order to improve robustness, we perform adversarial training on selected aspects and check its transferability to the improvement of models with various corruption types. We find that the high performance of models does not ensure sufficient robustness, although modern embedding techniques help to improve it. We release the code of WildNLP framework for the community.},
	language = {en},
	booktitle = {Neural {Information} {Processing}},
	publisher = {Springer International Publishing},
	author = {Rychalska, Barbara and Basaj, Dominika and Gosiewska, Alicja and Biecek, Przemysław},
	editor = {Gedeon, Tom and Wong, Kok Wai and Lee, Minho},
	year = {2019},
	keywords = {Deep learning, Adversarial examples, Natural Language Processing, Robustness},
	pages = {235--247},
	file = {Full Text PDF:/home/jonno/Zotero/storage/TUANS2AI/Rychalska et al. - 2019 - Models in the Wild On Corruption Robustness of Ne.pdf:application/pdf},
}

@misc{dubey_llama_2024,
	title = {The {Llama} 3 {Herd} of {Models}},
	url = {http://arxiv.org/abs/2407.21783},
	doi = {10.48550/arXiv.2407.21783},
	abstract = {Modern artificial intelligence (AI) systems are powered by foundation models. This paper presents a new set of foundation models, called Llama 3. It is a herd of language models that natively support multilinguality, coding, reasoning, and tool usage. Our largest model is a dense Transformer with 405B parameters and a context window of up to 128K tokens. This paper presents an extensive empirical evaluation of Llama 3. We find that Llama 3 delivers comparable quality to leading language models such as GPT-4 on a plethora of tasks. We publicly release Llama 3, including pre-trained and post-trained versions of the 405B parameter language model and our Llama Guard 3 model for input and output safety. The paper also presents the results of experiments in which we integrate image, video, and speech capabilities into Llama 3 via a compositional approach. We observe this approach performs competitively with the state-of-the-art on image, video, and speech recognition tasks. The resulting models are not yet being broadly released as they are still under development.},
	urldate = {2024-08-08},
	publisher = {arXiv},
	author = {Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and Goyal, Anirudh and Hartshorn, Anthony and Yang, Aobo and Mitra, Archi and Sravankumar, Archie and Korenev, Artem and Hinsvark, Arthur and Rao, Arun and Zhang, Aston and Rodriguez, Aurelien and Gregerson, Austen and Spataru, Ava and Roziere, Baptiste and Biron, Bethany and Tang, Binh and Chern, Bobbie and Caucheteux, Charlotte and Nayak, Chaya and Bi, Chloe and Marra, Chris and McConnell, Chris and Keller, Christian and Touret, Christophe and Wu, Chunyang and Wong, Corinne and Ferrer, Cristian Canton and Nikolaidis, Cyrus and Allonsius, Damien and Song, Daniel and Pintz, Danielle and Livshits, Danny and Esiobu, David and Choudhary, Dhruv and Mahajan, Dhruv and Garcia-Olano, Diego and Perino, Diego and Hupkes, Dieuwke and Lakomkin, Egor and AlBadawy, Ehab and Lobanova, Elina and Dinan, Emily and Smith, Eric Michael and Radenovic, Filip and Zhang, Frank and Synnaeve, Gabriel and Lee, Gabrielle and Anderson, Georgia Lewis and Nail, Graeme and Mialon, Gregoire and Pang, Guan and Cucurell, Guillem and Nguyen, Hailey and Korevaar, Hannah and Xu, Hu and Touvron, Hugo and Zarov, Iliyan and Ibarra, Imanol Arrieta and Kloumann, Isabel and Misra, Ishan and Evtimov, Ivan and Copet, Jade and Lee, Jaewon and Geffert, Jan and Vranes, Jana and Park, Jason and Mahadeokar, Jay and Shah, Jeet and van der Linde, Jelmer and Billock, Jennifer and Hong, Jenny and Lee, Jenya and Fu, Jeremy and Chi, Jianfeng and Huang, Jianyu and Liu, Jiawen and Wang, Jie and Yu, Jiecao and Bitton, Joanna and Spisak, Joe and Park, Jongsoo and Rocca, Joseph and Johnstun, Joshua and Saxe, Joshua and Jia, Junteng and Alwala, Kalyan Vasuden and Upasani, Kartikeya and Plawiak, Kate and Li, Ke and Heafield, Kenneth and Stone, Kevin and El-Arini, Khalid and Iyer, Krithika and Malik, Kshitiz and Chiu, Kuenley and Bhalla, Kunal and Rantala-Yeary, Lauren and van der Maaten, Laurens and Chen, Lawrence and Tan, Liang and Jenkins, Liz and Martin, Louis and Madaan, Lovish and Malo, Lubo and Blecher, Lukas and Landzaat, Lukas and de Oliveira, Luke and Muzzi, Madeline and Pasupuleti, Mahesh and Singh, Mannat and Paluri, Manohar and Kardas, Marcin and Oldham, Mathew and Rita, Mathieu and Pavlova, Maya and Kambadur, Melanie and Lewis, Mike and Si, Min and Singh, Mitesh Kumar and Hassan, Mona and Goyal, Naman and Torabi, Narjes and Bashlykov, Nikolay and Bogoychev, Nikolay and Chatterji, Niladri and Duchenne, Olivier and Çelebi, Onur and Alrassy, Patrick and Zhang, Pengchuan and Li, Pengwei and Vasic, Petar and Weng, Peter and Bhargava, Prajjwal and Dubal, Pratik and Krishnan, Praveen and Koura, Punit Singh and Xu, Puxin and He, Qing and Dong, Qingxiao and Srinivasan, Ragavan and Ganapathy, Raj and Calderer, Ramon and Cabral, Ricardo Silveira and Stojnic, Robert and Raileanu, Roberta and Girdhar, Rohit and Patel, Rohit and Sauvestre, Romain and Polidoro, Ronnie and Sumbaly, Roshan and Taylor, Ross and Silva, Ruan and Hou, Rui and Wang, Rui and Hosseini, Saghar and Chennabasappa, Sahana and Singh, Sanjay and Bell, Sean and Kim, Seohyun Sonia and Edunov, Sergey and Nie, Shaoliang and Narang, Sharan and Raparthy, Sharath and Shen, Sheng and Wan, Shengye and Bhosale, Shruti and Zhang, Shun and Vandenhende, Simon and Batra, Soumya and Whitman, Spencer and Sootla, Sten and Collot, Stephane and Gururangan, Suchin and Borodinsky, Sydney and Herman, Tamar and Fowler, Tara and Sheasha, Tarek and Georgiou, Thomas and Scialom, Thomas and Speckbacher, Tobias and Mihaylov, Todor and Xiao, Tong and Karn, Ujjwal and Goswami, Vedanuj and Gupta, Vibhor and Ramanathan, Vignesh and Kerkez, Viktor and Gonguet, Vincent and Do, Virginie and Vogeti, Vish and Petrovic, Vladan and Chu, Weiwei and Xiong, Wenhan and Fu, Wenyin and Meers, Whitney and Martinet, Xavier and Wang, Xiaodong and Tan, Xiaoqing Ellen and Xie, Xinfeng and Jia, Xuchao and Wang, Xuewei and Goldschlag, Yaelle and Gaur, Yashesh and Babaei, Yasmine and Wen, Yi and Song, Yiwen and Zhang, Yuchen and Li, Yue and Mao, Yuning and Coudert, Zacharie Delpierre and Yan, Zheng and Chen, Zhengxing and Papakipos, Zoe and Singh, Aaditya and Grattafiori, Aaron and Jain, Abha and Kelsey, Adam and Shajnfeld, Adam and Gangidi, Adithya and Victoria, Adolfo and Goldstand, Ahuva and Menon, Ajay and Sharma, Ajay and Boesenberg, Alex and Vaughan, Alex and Baevski, Alexei and Feinstein, Allie and Kallet, Amanda and Sangani, Amit and Yunus, Anam and Lupu, Andrei and Alvarado, Andres and Caples, Andrew and Gu, Andrew and Ho, Andrew and Poulton, Andrew and Ryan, Andrew and Ramchandani, Ankit and Franco, Annie and Saraf, Aparajita and Chowdhury, Arkabandhu and Gabriel, Ashley and Bharambe, Ashwin and Eisenman, Assaf and Yazdan, Azadeh and James, Beau and Maurer, Ben and Leonhardi, Benjamin and Huang, Bernie and Loyd, Beth and De Paola, Beto and Paranjape, Bhargavi and Liu, Bing and Wu, Bo and Ni, Boyu and Hancock, Braden and Wasti, Bram and Spence, Brandon and Stojkovic, Brani and Gamido, Brian and Montalvo, Britt and Parker, Carl and Burton, Carly and Mejia, Catalina and Wang, Changhan and Kim, Changkyu and Zhou, Chao and Hu, Chester and Chu, Ching-Hsiang and Cai, Chris and Tindal, Chris and Feichtenhofer, Christoph and Civin, Damon and Beaty, Dana and Kreymer, Daniel and Li, Daniel and Wyatt, Danny and Adkins, David and Xu, David and Testuggine, Davide and David, Delia and Parikh, Devi and Liskovich, Diana and Foss, Didem and Wang, Dingkang and Le, Duc and Holland, Dustin and Dowling, Edward and Jamil, Eissa and Montgomery, Elaine and Presani, Eleonora and Hahn, Emily and Wood, Emily and Brinkman, Erik and Arcaute, Esteban and Dunbar, Evan and Smothers, Evan and Sun, Fei and Kreuk, Felix and Tian, Feng and Ozgenel, Firat and Caggioni, Francesco and Guzmán, Francisco and Kanayet, Frank and Seide, Frank and Florez, Gabriela Medina and Schwarz, Gabriella and Badeer, Gada and Swee, Georgia and Halpern, Gil and Thattai, Govind and Herman, Grant and Sizov, Grigory and Guangyi and Zhang and Lakshminarayanan, Guna and Shojanazeri, Hamid and Zou, Han and Wang, Hannah and Zha, Hanwen and Habeeb, Haroun and Rudolph, Harrison and Suk, Helen and Aspegren, Henry and Goldman, Hunter and Molybog, Igor and Tufanov, Igor and Veliche, Irina-Elena and Gat, Itai and Weissman, Jake and Geboski, James and Kohli, James and Asher, Japhet and Gaya, Jean-Baptiste and Marcus, Jeff and Tang, Jeff and Chan, Jennifer and Zhen, Jenny and Reizenstein, Jeremy and Teboul, Jeremy and Zhong, Jessica and Jin, Jian and Yang, Jingyi and Cummings, Joe and Carvill, Jon and Shepard, Jon and McPhie, Jonathan and Torres, Jonathan and Ginsburg, Josh and Wang, Junjie and Wu, Kai and U, Kam Hou and Saxena, Karan and Prasad, Karthik and Khandelwal, Kartikay and Zand, Katayoun and Matosich, Kathy and Veeraraghavan, Kaushik and Michelena, Kelly and Li, Keqian and Huang, Kun and Chawla, Kunal and Lakhotia, Kushal and Huang, Kyle and Chen, Lailin and Garg, Lakshya and A, Lavender and Silva, Leandro and Bell, Lee and Zhang, Lei and Guo, Liangpeng and Yu, Licheng and Moshkovich, Liron and Wehrstedt, Luca and Khabsa, Madian and Avalani, Manav and Bhatt, Manish and Tsimpoukelli, Maria and Mankus, Martynas and Hasson, Matan and Lennie, Matthew and Reso, Matthias and Groshev, Maxim and Naumov, Maxim and Lathi, Maya and Keneally, Meghan and Seltzer, Michael L. and Valko, Michal and Restrepo, Michelle and Patel, Mihir and Vyatskov, Mik and Samvelyan, Mikayel and Clark, Mike and Macey, Mike and Wang, Mike and Hermoso, Miquel Jubert and Metanat, Mo and Rastegari, Mohammad and Bansal, Munish and Santhanam, Nandhini and Parks, Natascha and White, Natasha and Bawa, Navyata and Singhal, Nayan and Egebo, Nick and Usunier, Nicolas and Laptev, Nikolay Pavlovich and Dong, Ning and Zhang, Ning and Cheng, Norman and Chernoguz, Oleg and Hart, Olivia and Salpekar, Omkar and Kalinli, Ozlem and Kent, Parkin and Parekh, Parth and Saab, Paul and Balaji, Pavan and Rittner, Pedro and Bontrager, Philip and Roux, Pierre and Dollar, Piotr and Zvyagina, Polina and Ratanchandani, Prashant and Yuvraj, Pritish and Liang, Qian and Alao, Rachad and Rodriguez, Rachel and Ayub, Rafi and Murthy, Raghotham and Nayani, Raghu and Mitra, Rahul and Li, Raymond and Hogan, Rebekkah and Battey, Robin and Wang, Rocky and Maheswari, Rohan and Howes, Russ and Rinott, Ruty and Bondu, Sai Jayesh and Datta, Samyak and Chugh, Sara and Hunt, Sara and Dhillon, Sargun and Sidorov, Sasha and Pan, Satadru and Verma, Saurabh and Yamamoto, Seiji and Ramaswamy, Sharadh and Lindsay, Shaun and Lindsay, Shaun and Feng, Sheng and Lin, Shenghao and Zha, Shengxin Cindy and Shankar, Shiva and Zhang, Shuqiang and Zhang, Shuqiang and Wang, Sinong and Agarwal, Sneha and Sajuyigbe, Soji and Chintala, Soumith and Max, Stephanie and Chen, Stephen and Kehoe, Steve and Satterfield, Steve and Govindaprasad, Sudarshan and Gupta, Sumit and Cho, Sungmin and Virk, Sunny and Subramanian, Suraj and Choudhury, Sy and Goldman, Sydney and Remez, Tal and Glaser, Tamar and Best, Tamara and Kohler, Thilo and Robinson, Thomas and Li, Tianhe and Zhang, Tianjun and Matthews, Tim and Chou, Timothy and Shaked, Tzook and Vontimitta, Varun and Ajayi, Victoria and Montanez, Victoria and Mohan, Vijai and Kumar, Vinay Satish and Mangla, Vishal and Ionescu, Vlad and Poenaru, Vlad and Mihailescu, Vlad Tiberiu and Ivanov, Vladimir and Li, Wei and Wang, Wenchen and Jiang, Wenwen and Bouaziz, Wes and Constable, Will and Tang, Xiaocheng and Wang, Xiaofang and Wu, Xiaojian and Wang, Xiaolan and Xia, Xide and Wu, Xilun and Gao, Xinbo and Chen, Yanjun and Hu, Ye and Jia, Ye and Qi, Ye and Li, Yenda and Zhang, Yilin and Zhang, Ying and Adi, Yossi and Nam, Youngjin and Yu and Wang and Hao, Yuchen and Qian, Yundi and He, Yuzi and Rait, Zach and DeVito, Zachary and Rosnbrick, Zef and Wen, Zhaoduo and Yang, Zhenyu and Zhao, Zhiwei},
	month = jul,
	year = {2024},
	note = {arXiv:2407.21783 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/6L7HFWGH/Dubey et al. - 2024 - The Llama 3 Herd of Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/AQRLU4RM/2407.html:text/html},
}

@article{chauhan_comprehensive_2023,
	title = {A {Comprehensive} {Survey} on {Various} {Fully} {Automatic} {Machine} {Translation} {Evaluation} {Metrics}},
	volume = {55},
	issn = {1573-773X},
	url = {https://doi.org/10.1007/s11063-022-10835-4},
	doi = {10.1007/s11063-022-10835-4},
	abstract = {The fast advancement in machine translation models necessitates the development of accurate evaluation metrics that would allow researchers to track the progress in text languages. The evaluation of machine translation models is crucial since its results are exploited for improvements of translation models. However fully automatically evaluating the machine translation models in itself is a huge challenge for the researchers as human evaluation is very expensive, time-consuming, unreproducible. This paper presents a detailed classification and comprehensive survey on various fully automated evaluation metrics, which are used to assess the performance or quality of machine translated output. Various fully automatic evaluation metrics are classified into five categories that are lexical, character, semantic, syntactic, and semantic \& syntactic evaluation metrics for better understanding purpose. Taking account of the challenges posed in the field of machine translation evaluation by Statistical Machine Translation and Neural Machine Translation, along with a discussion on the advantages, disadvantages, and gaps for each fully automatic machine translation evaluation metric has been provided. The presented study will help machine translation researchers in quickly identifying automatic machine translation evaluation metrics that are most appropriate for the improvement or development of their machine translation model, as well as researchers in gaining a general understanding of how automatic machine translation evaluation research evolved.},
	language = {en},
	number = {9},
	urldate = {2024-08-09},
	journal = {Neural Processing Letters},
	author = {Chauhan, Shweta and Daniel, Philemon},
	month = dec,
	year = {2023},
	keywords = {Automated metrics, Machine translation, Machine translation evaluation, Metrics},
	pages = {12663--12717},
	file = {Full Text PDF:/home/jonno/Zotero/storage/BPMYEX67/Chauhan and Daniel - 2023 - A Comprehensive Survey on Various Fully Automatic .pdf:application/pdf},
}

@inproceedings{papineni_bleu_2002,
	address = {USA},
	series = {{ACL} '02},
	title = {{BLEU}: a method for automatic evaluation of machine translation},
	shorttitle = {{BLEU}},
	url = {https://dl.acm.org/doi/10.3115/1073083.1073135},
	doi = {10.3115/1073083.1073135},
	abstract = {Human evaluations of machine translation are extensive but expensive. Human evaluations can take months to finish and involve human labor that can not be reused. We propose a method of automatic machine translation evaluation that is quick, inexpensive, and language-independent, that correlates highly with human evaluation, and that has little marginal cost per run. We present this method as an automated understudy to skilled human judges which substitutes for them when there is need for quick or frequent evaluations.},
	urldate = {2024-08-09},
	booktitle = {Proceedings of the 40th {Annual} {Meeting} on {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei-Jing},
	month = jul,
	year = {2002},
	pages = {311--318},
	file = {Full Text PDF:/home/jonno/Zotero/storage/HD8VNTEC/Papineni et al. - 2002 - BLEU a method for automatic evaluation of machine.pdf:application/pdf},
}

@misc{jiang_gutenberg-hathitrust_2021,
	title = {The {Gutenberg}-{HathiTrust} {Parallel} {Corpus}: {A} {Real}-{World} {Dataset} for {Noise} {Investigation} in {Uncorrected} {OCR} {Texts}},
	copyright = {Copyright 2021 is held by Ming Jiang, Yuerong Hu, Glen Worthey, Ryan C. Dubnicek, Boris Capitanu, Deren Kudeki, and J. Stephen Downie. Copyright permissions, when appropriate, must be obtained directly from the authors.},
	shorttitle = {The {Gutenberg}-{HathiTrust} {Parallel} {Corpus}},
	url = {https://hdl.handle.net/2142/109695},
	abstract = {This paper proposes large-scale parallel corpora of English-language publications for exploring the effects of optical character recognition (OCR) errors in the scanned text of digitized library collections on various corpus-based research. We collected data from: (1) Project Gutenberg (Gutenberg) for a human-proofread clean corpus; and, (2) HathiTrust Digital Library (HathiTrust) for an uncorrected OCR-impacted corpus. Our data is parallel regarding the content. So far as we know, this is the first large-scale benchmark dataset intended to evaluate the effects of text noise in digital libraries. In total, we collected and aligned 19,049 pairs of uncorrected OCR-impacted and human-proofread books in six domains published from 1780 to 1993.},
	language = {eng},
	urldate = {2024-08-13},
	publisher = {University of IllInois},
	author = {Jiang, Ming and Hu, Yuerong and Worthey, Glen and Dubnicek, Ryan C. and Capitanu, Boris and Kudeki, Deren and Downie, J. Stephen},
	month = mar,
	year = {2021},
	note = {Publisher: iSchools},
	file = {Full Text PDF:/home/jonno/Zotero/storage/H7YIAXT7/Jiang et al. - 2021 - The Gutenberg-HathiTrust Parallel Corpus A Real-W.pdf:application/pdf},
}

@inproceedings{dereza_have_2024,
	address = {Torino, Italia},
	title = {“{To} {Have} the `{Million}' {Readers} {Yet}”: {Building} a {Digitally} {Enhanced} {Edition} of the {Bilingual} {Irish}-{English} {Newspaper} an {Gaodhal} (1881-1898)},
	shorttitle = {“{To} {Have} the `{Million}' {Readers} {Yet}”},
	url = {https://aclanthology.org/2024.lt4hala-1.9},
	abstract = {This paper introduces the `An Gaodhal' project, which aims to serve the historically under-resourced and endangered language of Irish (known as Gaeilge) by providing new digital tools and resources. The initial goal of the project was the extraction of full text of `An Gaodhal', a monthly bilingual Irish-English newspaper produced from 1881 to 1898, to the highest possible degree of accuracy via Optical Character Recognition (OCR), with a view to making its printed content searchable. The methodology applied toward achieving this goal yielded additional digital outputs including: 1. a new OCR model for the Irish language as printed in Cló Gaelach type; 2. a new OCR model for bilingual Irish-English content printed in Cló Gaelach and Roman types respectively; 3. a BART-based OCR post-correction model for historical bilingual Irish-English data; 4. a historical Irish training set for Named Entity Recognition (NER). All but the first of these four additional outputs appear to be the first of their kind. Each of the project outputs, including the full-text OCR outputs in ALTO XML format, is set for public release to enable open-access research. The paper also identifies the challenges historical Irish data poses to Natural Language Processing (NLP) in general and OCR in particular, and reports on project results and outputs to date. Finally, it contextualises the project within the wider field of NLP and considers its potential impact on under-resourced languages worldwide.},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the {Third} {Workshop} on {Language} {Technologies} for {Historical} and {Ancient} {Languages} ({LT4HALA}) @ {LREC}-{COLING}-2024},
	publisher = {ELRA and ICCL},
	author = {Dereza, Oksana and Ní Chonghaile, Deirdre and Wolf, Nicholas},
	editor = {Sprugnoli, Rachele and Passarotti, Marco},
	month = may,
	year = {2024},
	pages = {65--78},
	file = {Full Text PDF:/home/jonno/Zotero/storage/K9RF4V6P/Dereza et al. - 2024 - “To Have the `Million' Readers Yet” Building a Di.pdf:application/pdf},
}

@inproceedings{nguyen_deep_2019,
	title = {Deep {Statistical} {Analysis} of {OCR} {Errors} for {Effective} {Post}-{OCR} {Processing}},
	url = {https://ieeexplore.ieee.org/abstract/document/8791206},
	doi = {10.1109/JCDL.2019.00015},
	abstract = {Post-OCR is an important processing step that follows optical character recognition (OCR) and is meant to improve the quality of OCR documents by detecting and correcting residual errors. This paper describes the results of a statistical analysis of OCR errors on four document collections. Five aspects related to general OCR errors are studied and compared with human-generated misspellings, including edit operations, length effects, erroneous character positions, real-word vs. non-word errors, and word boundaries. Based on the observations from the analysis we give several suggestions related to the design and implementation of effective OCR post-processing approaches.},
	urldate = {2024-08-13},
	booktitle = {2019 {ACM}/{IEEE} {Joint} {Conference} on {Digital} {Libraries} ({JCDL})},
	author = {Nguyen, Thi-Tuyet-Hai and Jatowt, Adam and Coustaty, Mickael and Nguyen, Nhu-Van and Doucet, Antoine},
	month = jun,
	year = {2019},
	keywords = {Libraries, Optical character recognition software, Character recognition, Error analysis, Feature extraction, OCR errors, OCR post-processing, post OCR text correction, Statistical analysis, Transducers},
	pages = {29--38},
	file = {Full Text:/home/jonno/Zotero/storage/VH9HGM8Z/Nguyen et al. - 2019 - Deep Statistical Analysis of OCR Errors for Effect.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/UBE8LZX5/8791206.html:text/html},
}

@article{hill_quantifying_2019,
	title = {Quantifying the impact of dirty {OCR} on historical text analysis: {Eighteenth} {Century} {Collections} {Online} as a case study},
	volume = {34},
	issn = {2055-7671},
	shorttitle = {Quantifying the impact of dirty {OCR} on historical text analysis},
	url = {https://doi.org/10.1093/llc/fqz024},
	doi = {10.1093/llc/fqz024},
	abstract = {This article aims to quantify the impact optical character recognition (OCR) has on the quantitative analysis of historical documents. Using Eighteenth Century Collections Online as a case study, we first explore and explain the differences between the OCR corpus and its keyed-in counterpart, created by the Text Creation Partnership. We then conduct a series of specific analyses common to the digital humanities: topic modelling, authorship attribution, collocation analysis, and vector space modelling. The article concludes by offering some preliminary thoughts on how these conclusions can be applied to other datasets, by reflecting on the potential for predicting the quality of OCR where no ground-truth exists.},
	number = {4},
	urldate = {2024-08-13},
	journal = {Digital Scholarship in the Humanities},
	author = {Hill, Mark J and Hengchen, Simon},
	month = dec,
	year = {2019},
	pages = {825--843},
	file = {Accepted Version:/home/jonno/Zotero/storage/QUUWEUVY/Hill and Hengchen - 2019 - Quantifying the impact of dirty OCR on historical .pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/KITGGMFI/5476122.html:text/html},
}

@techreport{jiang_prototype_2021,
	title = {A {Prototype} {Gutenberg}-{HathiTrust} {Sentence}-level {Parallel} {Corpus}},
	url = {https://databank.illinois.edu/datasets/IDB-1685085},
	language = {en},
	urldate = {2024-08-13},
	institution = {University of Illinois},
	author = {Jiang, Ming and Dubnicek, Ryan and Worthey, Glen and Underwood, Ted and Downie, J. Stephen},
	year = {2021},
	note = {Publisher: University of Illinois at Urbana-Champaign},
	file = {Snapshot:/home/jonno/Zotero/storage/X76LFQWG/IDB-1685085.html:text/html},
}

@inproceedings{dhondt_generating_2017,
	address = {Taipei, Taiwan},
	title = {Generating a {Training} {Corpus} for {OCR} {Post}-{Correction} {Using} {Encoder}-{Decoder} {Model}},
	url = {https://aclanthology.org/I17-1101},
	abstract = {In this paper we present a novel approach to the automatic correction of OCR-induced orthographic errors in a given text. While current systems depend heavily on large training corpora or external information, such as domain-specific lexicons or confidence scores from the OCR process, our system only requires a small amount of (relatively) clean training data from a representative corpus to learn a character-based statistical language model using Bidirectional Long Short-Term Memory Networks (biLSTMs). We demonstrate the versatility and adaptability of our system on different text corpora with varying degrees of textual noise, including a real-life OCR corpus in the medical domain.},
	urldate = {2024-08-13},
	booktitle = {Proceedings of the {Eighth} {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Asian Federation of Natural Language Processing},
	author = {D'hondt, Eva and Grouin, Cyril and Grau, Brigitte},
	editor = {Kondrak, Greg and Watanabe, Taro},
	month = nov,
	year = {2017},
	pages = {1006--1014},
	file = {Full Text PDF:/home/jonno/Zotero/storage/IVSY5C4Z/D'hondt et al. - 2017 - Generating a Training Corpus for OCR Post-Correcti.pdf:application/pdf},
}

@inproceedings{plenz_graph_2024,
	address = {Bangkok, Thailand},
	title = {Graph {Language} {Models}},
	url = {https://aclanthology.org/2024.acl-long.245},
	abstract = {While Language Models (LMs) are the workhorses of NLP, their interplay with structured knowledge graphs (KGs) is still actively researched. Current methods for encoding such graphs typically either (i) linearize them for embedding with LMs – which underutilize structural information, or (ii) use Graph Neural Networks (GNNs) to preserve the graph structure – but GNNs cannot represent text features as well as pretrained LMs. In our work we introduce a novel LM type, the Graph Language Model (GLM), that integrates the strengths of both approaches and mitigates their weaknesses. The GLM parameters are initialized from a pretrained LM to enhance understanding of individual graph concepts and triplets. Simultaneously, we design the GLM's architecture to incorporate graph biases, thereby promoting effective knowledge distribution within the graph. This enables GLMs to process graphs, texts, and interleaved inputs of both. Empirical evaluations on relation classification tasks show that GLM embeddings surpass both LM- and GNN-based baselines in supervised and zero-shot setting, demonstrating their versatility.},
	urldate = {2024-08-19},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Plenz, Moritz and Frank, Anette},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {4477--4494},
	file = {Full Text PDF:/home/jonno/Zotero/storage/KA82ZTKP/Plenz and Frank - 2024 - Graph Language Models.pdf:application/pdf},
}

@article{shumailov_ai_2024,
	title = {{AI} models collapse when trained on recursively generated data},
	volume = {631},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07566-y},
	doi = {10.1038/s41586-024-07566-y},
	abstract = {Stable diffusion revolutionized image creation from descriptive text. GPT-2 (ref. 1), GPT-3(.5) (ref. 2) and GPT-4 (ref. 3) demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here we consider what may happen to GPT-\{n\} once LLMs contribute much of the text found online. We find that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. We refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). We build theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. We demonstrate that it must be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.},
	language = {en},
	number = {8022},
	urldate = {2024-08-25},
	journal = {Nature},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science},
	pages = {755--759},
	file = {Full Text PDF:/home/jonno/Zotero/storage/VXZ8G69S/Shumailov et al. - 2024 - AI models collapse when trained on recursively gen.pdf:application/pdf},
}

@misc{tan_15-pints_2024,
	title = {1.5-{Pints} {Technical} {Report}: {Pretraining} in {Days}, {Not} {Months} -- {Your} {Language} {Model} {Thrives} on {Quality} {Data}},
	shorttitle = {1.5-{Pints} {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.03506},
	doi = {10.48550/arXiv.2408.03506},
	abstract = {This paper presents a compute-efficient approach to pre-training a Language Model-the "1.5-Pints"-in only 9 days, while outperforming state-of-the-art models as an instruction-following assistant.Based on MT-Bench (a benchmark that emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and Microsoft's Phi.This is achieved by a carefully curated pre-training dataset of 57 billion tokens, using a mix of automated workflows and manual human review. The selection of the dataset prioritizes content that is considered expository and "textbook-like" to aid the model in reasoning and logical deduction, culminating in its overall ability as a strong and versatile AI model. In terms of the model architecture, we employed a modified Mistral tokenizer, alongside a Llama-2 architecture for wider compatibility. For training, we adopted the methodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints demonstrates that by focusing on data quality over quantity in LLM training, we can significantly reduce training time and resources required. We believe this approach will not only make pre-training more accessible but also reduce our carbon footprint. Our findings and resources from this research are open-sourced, aiming to facilitate further advancements in the field. The 1.5-Pints model is available in two versions: 2K and 16K context windows.},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Tan, Calvin and Wang, Jerome},
	month = aug,
	year = {2024},
	note = {arXiv:2408.03506 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Technical Report for 1.5-Pints},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/G625LXY5/Tan and Wang - 2024 - 1.5-Pints Technical Report Pretraining in Days, N.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/M9H2IEJS/2408.html:text/html},
}

@misc{li_textbooks_2023,
	title = {Textbooks {Are} {All} {You} {Need} {II}: phi-1.5 technical report},
	shorttitle = {Textbooks {Are} {All} {You} {Need} {II}},
	url = {http://arxiv.org/abs/2309.05463},
	doi = {10.48550/arXiv.2309.05463},
	abstract = {We continue the investigation into the power of smaller Transformer-based language models as initiated by {\textbackslash}textbf\{TinyStories\} -- a 10 million parameter model that can produce coherent English -- and the follow-up work on {\textbackslash}textbf\{phi-1\}, a 1.3 billion parameter model with Python coding performance close to the state-of-the-art. The latter work proposed to use existing Large Language Models (LLMs) to generate ``textbook quality" data as a way to enhance the learning process compared to traditional web data. We follow the ``Textbooks Are All You Need" approach, focusing this time on common sense reasoning in natural language, and create a new 1.3 billion parameter model named {\textbackslash}textbf\{phi-1.5\}, with performance on natural language tasks comparable to models 5x larger, and surpassing most non-frontier LLMs on more complex reasoning tasks such as grade-school mathematics and basic coding. More generally, {\textbackslash}textbf\{phi-1.5\} exhibits many of the traits of much larger LLMs, both good -- such as the ability to ``think step by step" or perform some rudimentary in-context learning -- and bad, including hallucinations and the potential for toxic and biased generations -- encouragingly though, we are seeing improvement on that front thanks to the absence of web data. We open-source {\textbackslash}textbf\{phi-1.5\} to promote further research on these urgent topics.},
	urldate = {2024-08-28},
	publisher = {arXiv},
	author = {Li, Yuanzhi and Bubeck, Sébastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
	month = sep,
	year = {2023},
	note = {arXiv:2309.05463 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/IA7Y6ZWT/Li et al. - 2023 - Textbooks Are All You Need II phi-1.5 technical r.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/PE8W2GRF/2309.html:text/html},
}

@article{shumailov_ai_2024-1,
	title = {{AI} models collapse when trained on recursively generated data},
	volume = {631},
	copyright = {2024 The Author(s)},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/s41586-024-07566-y},
	doi = {10.1038/s41586-024-07566-y},
	abstract = {Stable diffusion revolutionized image creation from descriptive text. GPT-2 (ref. 1), GPT-3(.5) (ref. 2) and GPT-4 (ref. 3) demonstrated high performance across a variety of language tasks. ChatGPT introduced such language models to the public. It is now clear that generative artificial intelligence (AI) such as large language models (LLMs) is here to stay and will substantially change the ecosystem of online text and images. Here we consider what may happen to GPT-\{n\} once LLMs contribute much of the text found online. We find that indiscriminate use of model-generated content in training causes irreversible defects in the resulting models, in which tails of the original content distribution disappear. We refer to this effect as ‘model collapse’ and show that it can occur in LLMs as well as in variational autoencoders (VAEs) and Gaussian mixture models (GMMs). We build theoretical intuition behind the phenomenon and portray its ubiquity among all learned generative models. We demonstrate that it must be taken seriously if we are to sustain the benefits of training from large-scale data scraped from the web. Indeed, the value of data collected about genuine human interactions with systems will be increasingly valuable in the presence of LLM-generated content in data crawled from the Internet.},
	language = {en},
	number = {8022},
	urldate = {2024-08-28},
	journal = {Nature},
	author = {Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
	month = jul,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Computer science, Computational science},
	pages = {755--759},
	file = {Full Text PDF:/home/jonno/Zotero/storage/WXVXU8XB/Shumailov et al. - 2024 - AI models collapse when trained on recursively gen.pdf:application/pdf},
}

@misc{gupte_lights_2021,
	title = {Lights, {Camera}, {Action}! {A} {Framework} to {Improve} {NLP} {Accuracy} over {OCR} documents},
	url = {http://arxiv.org/abs/2108.02899},
	doi = {10.48550/arXiv.2108.02899},
	abstract = {Document digitization is essential for the digital transformation of our societies, yet a crucial step in the process, Optical Character Recognition (OCR), is still not perfect. Even commercial OCR systems can produce questionable output depending on the fidelity of the scanned documents. In this paper, we demonstrate an effective framework for mitigating OCR errors for any downstream NLP task, using Named Entity Recognition (NER) as an example. We first address the data scarcity problem for model training by constructing a document synthesis pipeline, generating realistic but degraded data with NER labels. We measure the NER accuracy drop at various degradation levels and show that a text restoration model, trained on the degraded data, significantly closes the NER accuracy gaps caused by OCR errors, including on an out-of-domain dataset. For the benefit of the community, we have made the document synthesis pipeline available as an open-source project.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Gupte, Amit and Romanov, Alexey and Mantravadi, Sahitya and Banda, Dalitso and Liu, Jianjie and Khan, Raza and Meenal, Lakshmanan Ramu and Han, Benjamin and Srinivasan, Soundar},
	month = aug,
	year = {2021},
	note = {arXiv:2108.02899 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Accepted to the Document Intelligence Workshop at KDD 2021. The source code of Genalog is available at https://github.com/microsoft/genalog},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/H7RJPEUG/Gupte et al. - 2021 - Lights, Camera, Action! A Framework to Improve NLP.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/LCA3BG8D/2108.html:text/html},
}

@inproceedings{ott_fairseq_2019,
	address = {Minneapolis, Minnesota},
	title = {fairseq: {A} {Fast}, {Extensible} {Toolkit} for {Sequence} {Modeling}},
	shorttitle = {fairseq},
	url = {https://aclanthology.org/N19-4009},
	doi = {10.18653/v1/N19-4009},
	abstract = {fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto},
	urldate = {2024-08-29},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics} ({Demonstrations})},
	publisher = {Association for Computational Linguistics},
	author = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
	editor = {Ammar, Waleed and Louis, Annie and Mostafazadeh, Nasrin},
	month = jun,
	year = {2019},
	pages = {48--53},
	file = {Full Text PDF:/home/jonno/Zotero/storage/KJYJ3JKM/Ott et al. - 2019 - fairseq A Fast, Extensible Toolkit for Sequence M.pdf:application/pdf},
}

@misc{hu_lora_2021,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/GJ9HADAG/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/84JT9Q5I/2106.html:text/html},
}

@misc{kalajdzievski_rank_2023,
	title = {A {Rank} {Stabilization} {Scaling} {Factor} for {Fine}-{Tuning} with {LoRA}},
	url = {http://arxiv.org/abs/2312.03732},
	doi = {10.48550/arXiv.2312.03732},
	abstract = {As large language models (LLMs) have become increasingly compute and memory intensive, parameter-efficient fine-tuning (PEFT) methods are now a common strategy to fine-tune LLMs. A popular PEFT method is Low-Rank Adapters (LoRA), which adds trainable low-rank "adapters" to selected layers. Each adapter consists of a low-rank matrix product, multiplicatively scaled by a rank-dependent factor. This scaling factor, which divides adapters by a factor of the rank, results in slowed learning and stunted performance for LoRA with higher-rank adapters. Consequently, the use of LoRA in practice has generally been limited to very low ranks. In this work, we study the impact of the scaling factor on the learning process and prove that LoRA adapters should be divided by a factor of the square root of the rank. Modifying LoRA with the appropriate scaling factor, which we call the rank-stabilized LoRA (rsLoRA) method, easily provides for a fine-tuning compute/performance trade-off, where larger ranks can be used to trade off increased computational resources during training for better fine-tuning performance, with no change in inference computing cost.},
	urldate = {2024-08-29},
	publisher = {arXiv},
	author = {Kalajdzievski, Damjan},
	month = nov,
	year = {2023},
	note = {arXiv:2312.03732 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, I.2.7},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/5H9WGZDN/Kalajdzievski - 2023 - A Rank Stabilization Scaling Factor for Fine-Tunin.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/RAKE29W5/2312.html:text/html},
}

@misc{zha_data-centric_2023,
	title = {Data-centric {Artificial} {Intelligence}: {A} {Survey}},
	shorttitle = {Data-centric {Artificial} {Intelligence}},
	url = {http://arxiv.org/abs/2303.10158},
	doi = {10.48550/arXiv.2303.10158},
	abstract = {Artificial Intelligence (AI) is making a profound impact in almost every domain. A vital enabler of its great success is the availability of abundant and high-quality data for building machine learning models. Recently, the role of data in AI has been significantly magnified, giving rise to the emerging concept of data-centric AI. The attention of researchers and practitioners has gradually shifted from advancing model design to enhancing the quality and quantity of the data. In this survey, we discuss the necessity of data-centric AI, followed by a holistic view of three general data-centric goals (training data development, inference data development, and data maintenance) and the representative methods. We also organize the existing literature from automation and collaboration perspectives, discuss the challenges, and tabulate the benchmarks for various tasks. We believe this is the first comprehensive survey that provides a global view of a spectrum of tasks across various stages of the data lifecycle. We hope it can help the readers efficiently grasp a broad picture of this field, and equip them with the techniques and further research ideas to systematically engineer data for building AI systems. A companion list of data-centric AI resources will be regularly updated on https://github.com/daochenzha/data-centric-AI},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Zha, Daochen and Bhat, Zaid Pervaiz and Lai, Kwei-Herng and Yang, Fan and Jiang, Zhimeng and Zhong, Shaochen and Hu, Xia},
	month = jun,
	year = {2023},
	note = {arXiv:2303.10158 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Databases},
	annote = {Comment: 38 pages, 6 figues, 5 tables. A companion list of data-centric AI resources is available at https://github.com/daochenzha/data-centric-AI},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/7PCZAIFB/Zha et al. - 2023 - Data-centric Artificial Intelligence A Survey.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/49LLABND/2303.html:text/html},
}

@article{salehi_data-centric_2024,
	title = {Data-{Centric} {Green} {Artificial} {Intelligence}: {A} {Survey}},
	volume = {5},
	issn = {2691-4581},
	shorttitle = {Data-{Centric} {Green} {Artificial} {Intelligence}},
	url = {https://ieeexplore.ieee.org/abstract/document/10251541},
	doi = {10.1109/TAI.2023.3315272},
	abstract = {With the exponential growth of computational power and the availability of large-scale datasets in recent years, remarkable advancements have been made in the field of artificial intelligence (AI), leading to complex models and innovative applications. However, these models consume a significant unprecedented amount of energy, contributing to greenhouse gas emissions and a growing carbon footprint in the AI industry. In response, the concept of green AI has emerged, prioritizing energy efficiency and sustainability alongside accuracy and related measures. To this end, data-centric approaches are very promising to reduce the energy consumption of AI algorithms. This article presents a comprehensive overview of data-centric technologies and their impact on the energy efficiency of AI algorithms. Specifically, it focuses on methods that utilize training data in an efficient manner to improve the energy efficiency of AI algorithms. We have identified multiple data-centric approaches, such as active learning, knowledge transfer/sharing, dataset distillation, data augmentation, and curriculum learning that can contribute to the development of environmentally-friendly implementations of machine learning algorithms. Finally, the practical applications of these approaches are highlighted, and the challenges and future directions in the field are discussed.},
	number = {5},
	urldate = {2024-08-30},
	journal = {IEEE Transactions on Artificial Intelligence},
	author = {Salehi, Shirin and Schmeink, Anke},
	month = may,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Artificial Intelligence},
	keywords = {Machine learning, Training, Energy efficiency, Artificial intelligence, Computational efficiency, Computational modeling, Data-centric artificial intelligence (AI) (DCAI), data-efficiency, energy-efficiency, green AI, Green products},
	pages = {1973--1989},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/YTNSN9GA/10251541.html:text/html},
}

@article{strickland_andrew_2022,
	title = {Andrew {Ng}: {Unbiggen} {AI} - {IEEE} {Spectrum}},
	shorttitle = {Andrew {Ng}},
	url = {https://spectrum.ieee.org/andrew-ng-data-centric-ai},
	abstract = {The AI pioneer says it’s time for smart-sized, “data-centric” solutions to big issues},
	language = {en},
	urldate = {2024-08-30},
	journal = {IEEE Spectrum},
	author = {Strickland, Eliza},
	month = feb,
	year = {2022},
	file = {Snapshot:/home/jonno/Zotero/storage/XTCSDE5M/andrew-ng-data-centric-ai.html:text/html},
}

@misc{hu_lora_2021-1,
	title = {{LoRA}: {Low}-{Rank} {Adaptation} of {Large} {Language} {Models}},
	shorttitle = {{LoRA}},
	url = {http://arxiv.org/abs/2106.09685},
	doi = {10.48550/arXiv.2106.09685},
	abstract = {An important paradigm of natural language processing consists of large-scale pre-training on general domain data and adaptation to particular tasks or domains. As we pre-train larger models, full fine-tuning, which retrains all model parameters, becomes less feasible. Using GPT-3 175B as an example -- deploying independent instances of fine-tuned models, each with 175B parameters, is prohibitively expensive. We propose Low-Rank Adaptation, or LoRA, which freezes the pre-trained model weights and injects trainable rank decomposition matrices into each layer of the Transformer architecture, greatly reducing the number of trainable parameters for downstream tasks. Compared to GPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable parameters by 10,000 times and the GPU memory requirement by 3 times. LoRA performs on-par or better than fine-tuning in model quality on RoBERTa, DeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher training throughput, and, unlike adapters, no additional inference latency. We also provide an empirical investigation into rank-deficiency in language model adaptation, which sheds light on the efficacy of LoRA. We release a package that facilitates the integration of LoRA with PyTorch models and provide our implementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at https://github.com/microsoft/LoRA.},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Hu, Edward J. and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	month = oct,
	year = {2021},
	note = {arXiv:2106.09685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Draft V2 includes better baselines, experiments on GLUE, and more on adapter latency},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/U4NFAEFG/Hu et al. - 2021 - LoRA Low-Rank Adaptation of Large Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/3EB4D2QX/2106.html:text/html},
}

@misc{han_parameter-efficient_2024,
	title = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}: {A} {Comprehensive} {Survey}},
	shorttitle = {Parameter-{Efficient} {Fine}-{Tuning} for {Large} {Models}},
	url = {http://arxiv.org/abs/2403.14608},
	doi = {10.48550/arXiv.2403.14608},
	abstract = {Large models represent a groundbreaking advancement in multiple application fields, enabling remarkable achievements across various tasks. However, their unprecedented scale comes with significant computational costs. These models, often consisting of billions of parameters, require vast amounts of computational resources for execution. Especially, the expansive scale and computational demands pose considerable challenges when customizing them for particular downstream tasks, particularly over the hardware platforms constrained by computational capabilities. Parameter Efficient Fine-Tuning (PEFT) provides a practical solution by efficiently adjusting the large models over the various downstream tasks. In particular, PEFT refers to the process of adjusting the parameters of a pre-trained large models to adapt it to a specific task or domain while minimizing the number of additional parameters introduced or computational resources required. This approach is particularly important when dealing with large-scale language models with high parameter counts, as fine-tuning these models from scratch can be computationally expensive and resource-intensive, posing considerable challenges in the supporting system platform design. In this survey, we present comprehensive studies of various PEFT algorithms, examining their performance and computational overhead. Moreover, we provide an overview of applications developed using different PEFT algorithms and discuss common techniques employed to mitigate computation costs for PEFT. In addition to providing an extensive survey from an algorithmic standpoint, we also examine various real-world system designs to investigate the implementation costs associated with different PEFT approaches. This survey serves as an indispensable resource for researchers aiming to understand both the PEFT algorithm and its system implementation, offering detailed ......},
	urldate = {2024-08-30},
	publisher = {arXiv},
	author = {Han, Zeyu and Gao, Chao and Liu, Jinyang and Zhang, Jeff and Zhang, Sai Qian},
	month = jul,
	year = {2024},
	note = {arXiv:2403.14608 [cs]},
	keywords = {Computer Science - Machine Learning},
	annote = {Comment: 42 pages, 12 figures. Due to word limit, the abstract here is truncated. The full abstract is available in the PDF},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/TBAMUBZD/Han et al. - 2024 - Parameter-Efficient Fine-Tuning for Large Models .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/P6D6M4AJ/2403.html:text/html},
}

@article{fu_effectiveness_2023,
	title = {On the {Effectiveness} of {Parameter}-{Efficient} {Fine}-{Tuning}},
	volume = {37},
	copyright = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/26505},
	doi = {10.1609/aaai.v37i11.26505},
	abstract = {Fine-tuning pre-trained models has been ubiquitously proven to be effective in a wide range of NLP tasks. However, fine-tuning the whole model is parameter inefficient as it always yields an entirely new model for each task. Currently, many research works propose to only fine-tune a small portion of the parameters while keeping most of the parameters shared across different tasks. These methods achieve surprisingly good performance and are shown to be more stable than their corresponding fully fine-tuned counterparts. However, such kind of methods is still not well understood. Some natural questions arise: How does the parameter sparsity lead to promising performance? Why is the model more stable than the fully fine-tuned models? How to choose the tunable parameters? In this paper, we first categorize the existing methods into random approaches, rule-based approaches, and projection-based approaches based on how they choose which parameters to tune. Then, we show that all of the methods are actually sparse fine-tuned models and conduct a novel theoretical analysis of them. We indicate that the sparsity is actually imposing a regularization on the original model by controlling the upper bound of the stability. Such stability leads to better generalization capability which has been empirically observed in a lot of recent research works. Despite the effectiveness of sparsity grounded by our theory, it still remains an open problem of how to choose the tunable parameters. Currently, the random and rule-based methods do not utilize task-specific data information while the projection-based approaches suffer from the projection discontinuity problem. To better choose the tunable parameters, we propose a novel Second-order Approximation Method (SAM) which approximates the original problem with an analytically solvable optimization function. The tunable parameters are determined by directly optimizing the approximation function. We conduct extensive experiments on several tasks. The experimental results show that our proposed SAM model outperforms many strong baseline models and it also verifies our theoretical analysis. The source code of this paper can be obtained from https://github.com/fuzihaofzh/AnalyzeParameterEff{\textbackslash}/icientFinetune .},
	language = {en},
	number = {11},
	urldate = {2024-08-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Fu, Zihao and Yang, Haoran and So, Anthony Man-Cho and Lam, Wai and Bing, Lidong and Collier, Nigel},
	month = jun,
	year = {2023},
	note = {Number: 11},
	keywords = {SNLP: Other Foundations of Speech \& Natural Language Processing},
	pages = {12799--12807},
	file = {Full Text PDF:/home/jonno/Zotero/storage/Y8JQHZSK/Fu et al. - 2023 - On the Effectiveness of Parameter-Efficient Fine-T.pdf:application/pdf},
}

@misc{bourne_clocr-c_2024,
	title = {{CLOCR}-{C}: {Context} {Leveraging} {OCR} {Correction} with {Pre}-trained {Language} {Models}},
	shorttitle = {{CLOCR}-{C}},
	url = {https://arxiv.org/abs/2408.17428v1},
	abstract = {The digitisation of historical print media archives is crucial for increasing accessibility to contemporary records. However, the process of Optical Character Recognition (OCR) used to convert physical records to digital text is prone to errors, particularly in the case of newspapers and periodicals due to their complex layouts. This paper introduces Context Leveraging OCR Correction (CLOCR-C), which utilises the infilling and context-adaptive abilities of transformer-based language models (LMs) to improve OCR quality. The study aims to determine if LMs can perform post-OCR correction, improve downstream NLP tasks, and the value of providing the socio-cultural context as part of the correction process. Experiments were conducted using seven LMs on three datasets: the 19th Century Serials Edition (NCSE) and two datasets from the Overproof collection. The results demonstrate that some LMs can significantly reduce error rates, with the top-performing model achieving over a 60\% reduction in character error rate on the NCSE dataset. The OCR improvements extend to downstream tasks, such as Named Entity Recognition, with increased Cosine Named Entity Similarity. Furthermore, the study shows that providing socio-cultural context in the prompts improves performance, while misleading prompts lower performance. In addition to the findings, this study releases a dataset of 91 transcribed articles from the NCSE, containing a total of 40 thousand words, to support further research in this area. The findings suggest that CLOCR-C is a promising approach for enhancing the quality of existing digital archives by leveraging the socio-cultural information embedded in the LMs and the text requiring correction.},
	language = {en},
	urldate = {2024-09-04},
	journal = {arXiv.org},
	author = {Bourne, Jonathan},
	month = aug,
	year = {2024},
	file = {Full Text PDF:/home/jonno/Zotero/storage/E2V9JBWF/Bourne - 2024 - CLOCR-C Context Leveraging OCR Correction with Pr.pdf:application/pdf},
}

@misc{hart_project_1971,
	title = {Project {Gutenberg}},
	url = {https://www.gutenberg.org},
	author = {Hart, Michael S. and Team, the Project Gutenberg},
	year = {1971},
	annote = {Accessed: 2024-09-04},
}

@misc{lovelace_sketch_1842,
	title = {Sketch of the {Analytical} {Engine} invented by {Charles} {Babbage}, {Esq}./{Notes} by the {Translator}},
	url = {https://en.wikisource.org/wiki/Scientific_Memoirs/3/Sketch_of_the_Analytical_Engine_invented_by_Charles_Babbage,_Esq./Notes_by_the_Translator},
	language = {en},
	urldate = {2024-09-04},
	author = {Lovelace, Ada},
	month = oct,
	year = {1842},
	file = {Snapshot:/home/jonno/Zotero/storage/PD9GUQXU/Notes_by_the_Translator.html:text/html},
}

@misc{galileo_llm_2024,
	title = {{LLM} {Hallucination} {Index} {RAG} {Special} - {Galileo}},
	url = {https://www.rungalileo.io/hallucinationindex/methodology},
	abstract = {LLM Hallucination Index RAG Special - Galileo. A Ranking \& Evaluation Framework For LLM Hallucinations},
	language = {en},
	urldate = {2024-09-05},
	journal = {Galileo},
	author = {Galileo},
	month = sep,
	year = {2024},
}

@misc{li_halueval_2023,
	title = {{HaluEval}: {A} {Large}-{Scale} {Hallucination} {Evaluation} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {{HaluEval}},
	url = {https://arxiv.org/abs/2305.11747v3},
	abstract = {Large language models (LLMs), such as ChatGPT, are prone to generate hallucinations, i.e., content that conflicts with the source or cannot be verified by the factual knowledge. To understand what types of content and to which extent LLMs are apt to hallucinate, we introduce the Hallucination Evaluation benchmark for Large Language Models (HaluEval), a large collection of generated and human-annotated hallucinated samples for evaluating the performance of LLMs in recognizing hallucination. To generate these samples, we propose a ChatGPT-based two-step framework, i.e., sampling-then-filtering. Besides, we also hire some human labelers to annotate the hallucinations in ChatGPT responses. The empirical results suggest that ChatGPT is likely to generate hallucinated content in specific topics by fabricating unverifiable information (i.e., about \$19.5{\textbackslash}\%\$ responses). Moreover, existing LLMs face great challenges in recognizing the hallucinations in texts. However, our experiments also prove that providing external knowledge or adding reasoning steps can help LLMs recognize hallucinations. Our benchmark can be accessed at https://github.com/RUCAIBox/HaluEval.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Li, Junyi and Cheng, Xiaoxue and Zhao, Wayne Xin and Nie, Jian-Yun and Wen, Ji-Rong},
	month = may,
	year = {2023},
	file = {Full Text PDF:/home/jonno/Zotero/storage/PGN4TN9Z/Li et al. - 2023 - HaluEval A Large-Scale Hallucination Evaluation B.pdf:application/pdf},
}

@misc{hsieh_ruler_2024,
	title = {{RULER}: {What}'s the {Real} {Context} {Size} of {Your} {Long}-{Context} {Language} {Models}?},
	shorttitle = {{RULER}},
	url = {https://arxiv.org/abs/2404.06654v3},
	abstract = {The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the "needle") from long distractor texts (the "haystack"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.},
	language = {en},
	urldate = {2024-09-05},
	journal = {arXiv.org},
	author = {Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
	month = apr,
	year = {2024},
	file = {Full Text PDF:/home/jonno/Zotero/storage/KQLMDALK/Hsieh et al. - 2024 - RULER What's the Real Context Size of Your Long-C.pdf:application/pdf},
}

@article{blyth_simpsons_1972,
	title = {On {Simpson}'s {Paradox} and the {Sure}-{Thing} {Principle}},
	volume = {67},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1972.10482387},
	doi = {10.1080/01621459.1972.10482387},
	abstract = {This paradox is the possibility of P(A{\textbar}B)},
	number = {338},
	urldate = {2024-09-10},
	journal = {Journal of the American Statistical Association},
	author = {Blyth, Colin R.},
	month = jun,
	year = {1972},
	note = {Publisher: ASA Website
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1972.10482387},
	pages = {364--366},
}

@article{simpson_interpretation_1951,
	title = {The {Interpretation} of {Interaction} in {Contingency} {Tables}},
	volume = {13},
	issn = {0035-9246},
	url = {https://doi.org/10.1111/j.2517-6161.1951.tb00088.x},
	doi = {10.1111/j.2517-6161.1951.tb00088.x},
	abstract = {The definition of second order interaction in a (2 × 2 × 2) table given by Bartlett is accepted, but it is shown by an example that the vanishing of this second order interaction does not necessarily justify the mechanical procedure of forming the three component 2 × 2 tables and testing each of these for significance by standard methods.},
	number = {2},
	urldate = {2024-09-10},
	journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
	author = {Simpson, E. H.},
	month = jul,
	year = {1951},
	pages = {238--241},
	file = {Snapshot:/home/jonno/Zotero/storage/WLYNAAWI/7026675.html:text/html},
}

@misc{banerjee_llms_2024,
	title = {{LLMs} {Will} {Always} {Hallucinate}, and {We} {Need} to {Live} {With} {This}},
	url = {http://arxiv.org/abs/2409.05746},
	doi = {10.48550/arXiv.2409.05746},
	abstract = {As Large Language Models become more ubiquitous across domains, it becomes important to examine their inherent limitations critically. This work argues that hallucinations in language models are not just occasional errors but an inevitable feature of these systems. We demonstrate that hallucinations stem from the fundamental mathematical and logical structure of LLMs. It is, therefore, impossible to eliminate them through architectural improvements, dataset enhancements, or fact-checking mechanisms. Our analysis draws on computational theory and Godel's First Incompleteness Theorem, which references the undecidability of problems like the Halting, Emptiness, and Acceptance Problems. We demonstrate that every stage of the LLM process-from training data compilation to fact retrieval, intent classification, and text generation-will have a non-zero probability of producing hallucinations. This work introduces the concept of Structural Hallucination as an intrinsic nature of these systems. By establishing the mathematical certainty of hallucinations, we challenge the prevailing notion that they can be fully mitigated.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Banerjee, Sourav and Agarwal, Ayushi and Singla, Saloni},
	month = sep,
	year = {2024},
	note = {arXiv:2409.05746 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/D2UDMZJU/Banerjee et al. - 2024 - LLMs Will Always Hallucinate, and We Need to Live .pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/8NGHT6PG/2409.html:text/html},
}

@misc{shah_flashattention-3_2024,
	title = {{FlashAttention}-3: {Fast} and {Accurate} {Attention} with {Asynchrony} and {Low}-precision},
	shorttitle = {{FlashAttention}-3},
	url = {http://arxiv.org/abs/2407.08608},
	doi = {10.48550/arXiv.2407.08608},
	abstract = {Attention, as a core layer of the ubiquitous Transformer architecture, is the bottleneck for large language models and long-context applications. FlashAttention elaborated an approach to speed up attention on GPUs through minimizing memory reads/writes. However, it has yet to take advantage of new capabilities present in recent hardware, with FlashAttention-2 achieving only 35\% utilization on the H100 GPU. We develop three main techniques to speed up attention on Hopper GPUs: exploiting asynchrony of the Tensor Cores and TMA to (1) overlap overall computation and data movement via warp-specialization and (2) interleave block-wise matmul and softmax operations, and (3) block quantization and incoherent processing that leverages hardware support for FP8 low-precision. We demonstrate that our method, FlashAttention-3, achieves speedup on H100 GPUs by 1.5-2.0\${\textbackslash}times\$ with FP16 reaching up to 740 TFLOPs/s (75\% utilization), and with FP8 reaching close to 1.2 PFLOPs/s. We validate that FP8 FlashAttention-3 achieves 2.6\${\textbackslash}times\$ lower numerical error than a baseline FP8 attention.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Shah, Jay and Bikshandi, Ganesh and Zhang, Ying and Thakkar, Vijay and Ramani, Pradeep and Dao, Tri},
	month = jul,
	year = {2024},
	note = {arXiv:2407.08608 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/LD33FBNK/Shah et al. - 2024 - FlashAttention-3 Fast and Accurate Attention with.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/CRBYYYMA/2407.html:text/html},
}

@misc{gu_mamba_2024,
	title = {Mamba: {Linear}-{Time} {Sequence} {Modeling} with {Selective} {State} {Spaces}},
	shorttitle = {Mamba},
	url = {http://arxiv.org/abs/2312.00752},
	doi = {10.48550/arXiv.2312.00752},
	abstract = {Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5\${\textbackslash}times\$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Gu, Albert and Dao, Tri},
	month = may,
	year = {2024},
	note = {arXiv:2312.00752 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/5JPPKNWU/Gu and Dao - 2024 - Mamba Linear-Time Sequence Modeling with Selectiv.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/FAW3ANYI/2312.html:text/html},
}

@misc{dao_transformers_2024,
	title = {Transformers are {SSMs}: {Generalized} {Models} and {Efficient} {Algorithms} {Through} {Structured} {State} {Space} {Duality}},
	shorttitle = {Transformers are {SSMs}},
	url = {https://arxiv.org/abs/2405.21060v1},
	abstract = {While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.},
	language = {en},
	urldate = {2024-09-17},
	journal = {arXiv.org},
	author = {Dao, Tri and Gu, Albert},
	month = may,
	year = {2024},
	file = {Full Text PDF:/home/jonno/Zotero/storage/ACWHXIC6/Dao and Gu - 2024 - Transformers are SSMs Generalized Models and Effi.pdf:application/pdf},
}

@techreport{hall_training_2024,
	title = {Training {Giant} {Neural} {Networks} {Using} {Weight} {Streaming} on {Cerebras} {Wafer}-{Scale} {Clusters}},
	url = {https://8968533.fs1.hubspotusercontent-na1.net/hubfs/8968533/Virtual%20Booth%20Docs/CS%20Weight%20Streaming%20White%20Paper.pdf},
	abstract = {State-of-the-art language models are extremely challenging to train; they require huge compute budgets and complex distributed compute techniques. As a result, few organizations train large language models (LLMs) from scratch. In this paper, we present a new training execution flow called weight streaming. By disaggregating parameter storage from primary compute, weight streaming enables the training of models two orders of magnitude larger than the current state-of-the-art. Because weight streaming runs in strictly data parallel form on Cerebras CS-2 systems, it avoids the complex and time-consuming distributed computing techniques that bedevil ML practitioners. Weight streaming demonstrates near perfect linear scaling across clusters of Cerebras CS-2 systems. We present experimental results showing scaling of large GPT-style large language models across clusters of up to 64 CS-2s, containing 54 million AI cores. We also show how the weight streaming architecture enables the harvesting of dynamic, static, structured and unstructured sparsity.},
	language = {en},
	institution = {Cerebras},
	author = {Hall, Stewart and Schreiber, Rob and Lie, Sean and Systems, Cerebras},
	year = {2024},
	file = {Hall et al. - Training Giant Neural Networks Using Weight Stream.pdf:/home/jonno/Zotero/storage/2SJYSDZB/Hall et al. - Training Giant Neural Networks Using Weight Stream.pdf:application/pdf},
}

@inproceedings{yang_optimized_2024,
	address = {Denver, CO, USA},
	title = {Optimized {Simulation} {Methodology} of {Warpage} and {Localized} {Stress} {Hotspot} {Prediction} for {Assembly} {Risk} {Assessment}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10564995/},
	doi = {10.1109/ectc51529.2024.00162},
	abstract = {Generative Artificial Intelligence workloads, like Large Language Models, are growing in computational demand by 1000\% every year, while Moore’s Law scaling is only supplying 3\% more transistors/mm2 every year. To close the gap between these wildly diverging demand and supply exponentials, the industry not only needs better chip-to-chip interconnects, but also ways to integrate more silicon into a single package. This paper we will focus on advanced packaging modeling of the Groq Language Processing Unit (LPUTM) inference engine, the highest performance Large Language Model Inference Engine to date. More specifically the paper will focus on the accurate warpage prediction, which has emerged as a pivotal challenge with profound implications for design reliability and manufacturability.},
	language = {en},
	urldate = {2024-09-17},
	booktitle = {2024 {IEEE} 74th {Electronic} {Components} and {Technology} {Conference} ({ECTC})},
	publisher = {IEEE},
	author = {Yang, Zhi and Mellachervu, Krishna and Arsovski, Igor and Harames, Clint and Miller, Jim},
	month = may,
	year = {2024},
	pages = {1011--1017},
	file = {Yang et al. - 2024 - Optimized Simulation Methodology of Warpage and Lo.pdf:/home/jonno/Zotero/storage/KEUC54QZ/Yang et al. - 2024 - Optimized Simulation Methodology of Warpage and Lo.pdf:application/pdf},
}

@techreport{nvidia_nvidia_2024,
	title = {{NVIDIA} {Blackwell} {Architecture} {Technical} {Overview}},
	url = {https://resources.nvidia.com/en-us-blackwell-architecture},
	abstract = {NVIDIA's Blackwell GPU architecture revolutionizes AI with unparalleled performance, scalability and efficiency. Anchored by the Grace Blackwell GB200 superchip and GB200 NVL72, it boasts 30X more performance and 25X more energy efficiency over its predecessor.},
	language = {en},
	urldate = {2024-09-17},
	institution = {NVIDIA},
	author = {NVIDIA},
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/2FBWD4X7/en-us-blackwell-architecture.html:text/html},
}

@techreport{sambanova_accelerated_2021,
	title = {Accelerated {Computing} with a {Reconfigurable} {Dataflow} {Architecture}},
	url = {https://sambanova.ai/hubfs/23945802/SambaNova_Accelerated-Computing-with-a-Reconfigurable-Dataflow-Architecture_Whitepaper_English-1.pdf},
	urldate = {2024-09-17},
	institution = {SambaNova},
	author = {SambaNova},
	year = {2021},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the {Dangers} of {Stochastic} {Parrots}: {Can} {Language} {Models} {Be} {Too} {Big}?},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://dl.acm.org/doi/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2024-09-17},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
	file = {Full Text PDF:/home/jonno/Zotero/storage/6J9KJR2J/Bender et al. - 2021 - On the Dangers of Stochastic Parrots Can Language.pdf:application/pdf},
}

@misc{kallini_mission_2024,
	title = {Mission: {Impossible} {Language} {Models}},
	shorttitle = {Mission},
	url = {http://arxiv.org/abs/2401.06416},
	doi = {10.48550/arXiv.2401.06416},
	abstract = {Chomsky and others have very directly claimed that large language models (LLMs) are equally capable of learning languages that are possible and impossible for humans to learn. However, there is very little published experimental evidence to support such a claim. Here, we develop a set of synthetic impossible languages of differing complexity, each designed by systematically altering English data with unnatural word orders and grammar rules. These languages lie on an impossibility continuum: at one end are languages that are inherently impossible, such as random and irreversible shuffles of English words, and on the other, languages that may not be intuitively impossible but are often considered so in linguistics, particularly those with rules based on counting word positions. We report on a wide range of evaluations to assess the capacity of GPT-2 small models to learn these uncontroversially impossible languages, and crucially, we perform these assessments at various stages throughout training to compare the learning process for each language. Our core finding is that GPT-2 struggles to learn impossible languages when compared to English as a control, challenging the core claim. More importantly, we hope our approach opens up a productive line of inquiry in which different LLM architectures are tested on a variety of impossible languages in an effort to learn more about how LLMs can be used as tools for these cognitive and typological investigations.},
	urldate = {2024-09-17},
	publisher = {arXiv},
	author = {Kallini, Julie and Papadimitriou, Isabel and Futrell, Richard and Mahowald, Kyle and Potts, Christopher},
	month = aug,
	year = {2024},
	note = {arXiv:2401.06416 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/RHVTMIHM/Kallini et al. - 2024 - Mission Impossible Language Models.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/8YYUMGTK/2401.html:text/html},
}

@misc{ma_era_2024,
	title = {The {Era} of 1-bit {LLMs}: {All} {Large} {Language} {Models} are in 1.58 {Bits}},
	shorttitle = {The {Era} of 1-bit {LLMs}},
	url = {http://arxiv.org/abs/2402.17764},
	doi = {10.48550/arXiv.2402.17764},
	abstract = {Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary \{-1, 0, 1\}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Ma, Shuming and Wang, Hongyu and Ma, Lingxiao and Wang, Lei and Wang, Wenhui and Huang, Shaohan and Dong, Li and Wang, Ruiping and Xue, Jilong and Wei, Furu},
	month = feb,
	year = {2024},
	note = {arXiv:2402.17764 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Work in progress},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/43NJXQBX/Ma et al. - 2024 - The Era of 1-bit LLMs All Large Language Models a.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/XKRRAQFA/2402.html:text/html},
}

@misc{han_unsloth_2024,
	title = {Unsloth},
	copyright = {Apache 2.0},
	url = {https://docs.unsloth.ai/get-started/all-our-models},
	abstract = {See the list below for all our 4bit bnb uploaded models},
	urldate = {2024-09-22},
	publisher = {unsloth.ai},
	author = {Han, Daniel and Han, Michael},
	month = sep,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/4MAFWIQA/all-our-models.html:text/html},
}

@misc{bourne_scrambled_2024,
	title = {Scrambled text {Datasets} from the paper},
	doi = {https://doi.org/10.5522/04/27108334.v1},
	language = {en},
	publisher = {UCL figshare},
	author = {Bourne, Jonathan},
	month = sep,
	year = {2024},
}

@article{nikolaidou_survey_2022,
	title = {A survey of historical document image datasets},
	volume = {25},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-022-00405-8},
	doi = {10.1007/s10032-022-00405-8},
	abstract = {This paper presents a systematic literature review of image datasets for document image analysis, focusing on historical documents, such as handwritten manuscripts and early prints. Finding appropriate datasets for historical document analysis is a crucial prerequisite to facilitate research using different machine learning algorithms. However, because of the very large variety of the actual data (e.g., scripts, tasks, dates, support systems, and amount of deterioration), the different formats for data and label representation, and the different evaluation processes and benchmarks, finding appropriate datasets is a difficult task. This work fills this gap, presenting a meta-study on existing datasets. After a systematic selection process (according to PRISMA guidelines), we select 65 studies that are chosen based on different factors, such as the year of publication, number of methods implemented in the article, reliability of the chosen algorithms, dataset size, and journal outlet. We summarize each study by assigning it to one of three pre-defined tasks: document classification, layout structure, or content analysis. We present the statistics, document type, language, tasks, input visual aspects, and ground truth information for every dataset. In addition, we provide the benchmark tasks and results from these papers or recent competitions. We further discuss gaps and challenges in this domain. We advocate for providing conversion tools to common formats (e.g., COCO format for computer vision tasks) and always providing a set of evaluation metrics, instead of just one, to make results comparable across studies.},
	language = {en},
	number = {4},
	urldate = {2024-10-04},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Nikolaidou, Konstantina and Seuret, Mathias and Mokayed, Hamam and Liwicki, Marcus},
	month = dec,
	year = {2022},
	keywords = {Machine learning, Artificial Intelligence, Document image analysis, Historical documents, Image datasets},
	pages = {305--338},
	file = {Full Text PDF:/home/jonno/Zotero/storage/3FMV3QUA/Nikolaidou et al. - 2022 - A survey of historical document image datasets.pdf:application/pdf},
}

@misc{biten_ocr-idl_2022,
	title = {{OCR}-{IDL}: {OCR} {Annotations} for {Industry} {Document} {Library} {Dataset}},
	shorttitle = {{OCR}-{IDL}},
	url = {http://arxiv.org/abs/2202.12985},
	doi = {10.48550/arXiv.2202.12985},
	abstract = {Pretraining has proven successful in Document Intelligence tasks where deluge of documents are used to pretrain the models only later to be finetuned on downstream tasks. One of the problems of the pretraining approaches is the inconsistent usage of pretraining data with different OCR engines leading to incomparable results between models. In other words, it is not obvious whether the performance gain is coming from diverse usage of amount of data and distinct OCR engines or from the proposed models. To remedy the problem, we make public the OCR annotations for IDL documents using commercial OCR engine given their superior performance over open source OCR models. The contributed dataset (OCR-IDL) has an estimated monetary value over 20K US\$. It is our hope that OCR-IDL can be a starting point for future works on Document Intelligence. All of our data and its collection process with the annotations can be found in https://github.com/furkanbiten/idl\_data.},
	urldate = {2024-10-04},
	publisher = {arXiv},
	author = {Biten, Ali Furkan and Tito, Rubèn and Gomez, Lluis and Valveny, Ernest and Karatzas, Dimosthenis},
	month = feb,
	year = {2022},
	note = {arXiv:2202.12985 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/home/jonno/Zotero/storage/X397R62P/Biten et al. - 2022 - OCR-IDL OCR Annotations for Industry Document Lib.pdf:application/pdf;arXiv.org Snapshot:/home/jonno/Zotero/storage/SW82TWUE/2202.html:text/html},
}

@misc{jiang_mixtral_2024-1,
	title = {Mixtral of {Experts}},
	url = {https://arxiv.org/abs/2401.04088v1},
	abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
	language = {en},
	urldate = {2024-10-08},
	journal = {arXiv.org},
	author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, Lélio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Théophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timothée and Sayed, William El},
	month = jan,
	year = {2024},
	file = {Full Text PDF:/home/jonno/Zotero/storage/J2MC9T3D/Jiang et al. - 2024 - Mixtral of Experts.pdf:application/pdf},
}

@article{jacobs_adaptive_1991,
	title = {Adaptive {Mixtures} of {Local} {Experts}},
	volume = {3},
	issn = {0899-7667},
	url = {https://ieeexplore.ieee.org/abstract/document/6797059},
	doi = {10.1162/neco.1991.3.1.79},
	abstract = {We present a new supervised learning procedure for systems composed of many separate networks, each of which learns to handle a subset of the complete set of training cases. The new procedure can be viewed either as a modular version of a multilayer supervised network, or as an associative version of competitive learning. It therefore provides a new link between these two apparently different approaches. We demonstrate that the learning procedure divides up a vowel discrimination task into appropriate subtasks, each of which can be solved by a very simple expert network.},
	number = {1},
	urldate = {2024-10-08},
	journal = {Neural Computation},
	author = {Jacobs, Robert A. and Jordan, Michael I. and Nowlan, Steven J. and Hinton, Geoffrey E.},
	month = mar,
	year = {1991},
	note = {Conference Name: Neural Computation},
	pages = {79--87},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/HRPYRBSB/6797059.html:text/html},
}

@inproceedings{kettunen_creating_2018,
	address = {Helsinki},
	title = {Creating and using ground truth {OCR} sample data for {Finnish} historical newspapers and journals},
	url = {https://ceur-ws.org/Vol-2084/shortplus1.pdf},
	abstract = {The National Library of Finland (NLF) has digitized historical newspapers, journals and ephemera published in Finland since the late 1990s. The present collection consists of about 12.9 million pages mainly in Finnish and Swedish. Out of these about 7.36 million pages are freely available on the web site digi.kansalliskirjasto.fi. The copyright restricted part of the collection can be used at six legal deposit libraries in different parts of Finland. The time period of the open collection is from 1771 to 1929. The years 1920–1929 were opened in January 2018.},
	language = {en},
	booktitle = {Digital {Humanities} in the {Nordic} {Countries} 3rd {Conference}},
	publisher = {The National Library of Finland},
	author = {Kettunen, Kimmo and Kervinen, Jukka and Koistinen, Mika},
	month = mar,
	year = {2018},
	file = {Kettunen et al. - Creating and using ground truth OCR sample data fo.pdf:/home/jonno/Zotero/storage/BH9UB3UL/Kettunen et al. - Creating and using ground truth OCR sample data fo.pdf:application/pdf},
}

@misc{sapkota_comparing_2024,
	title = {Comparing {YOLOv8} and {Mask} {RCNN} for object segmentation in complex orchard environments},
	url = {http://arxiv.org/abs/2312.07935},
	doi = {10.48550/arXiv.2312.07935},
	abstract = {Instance segmentation, an important image processing operation for automation in agriculture, is used to precisely delineate individual objects of interest within images, which provides foundational information for various automated or robotic tasks such as selective harvesting and precision pruning. This study compares the one-stage YOLOv8 and the two-stage Mask R-CNN machine learning models for instance segmentation under varying orchard conditions across two datasets. Dataset 1, collected in dormant season, includes images of dormant apple trees, which were used to train multi-object segmentation models delineating tree branches and trunks. Dataset 2, collected in the early growing season, includes images of apple tree canopies with green foliage and immature (green) apples (also called fruitlet), which were used to train single-object segmentation models delineating only immature green apples. The results showed that YOLOv8 performed better than Mask R-CNN, achieving good precision and near-perfect recall across both datasets at a confidence threshold of 0.5. Specifically, for Dataset 1, YOLOv8 achieved a precision of 0.90 and a recall of 0.95 for all classes. In comparison, Mask R-CNN demonstrated a precision of 0.81 and a recall of 0.81 for the same dataset. With Dataset 2, YOLOv8 achieved a precision of 0.93 and a recall of 0.97. Mask R-CNN, in this single-class scenario, achieved a precision of 0.85 and a recall of 0.88. Additionally, the inference times for YOLOv8 were 10.9 ms for multi-class segmentation (Dataset 1) and 7.8 ms for single-class segmentation (Dataset 2), compared to 15.6 ms and 12.8 ms achieved by Mask R-CNN's, respectively.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Sapkota, Ranjan and Ahmed, Dawood and Karkee, Manoj},
	month = jul,
	year = {2024},
	note = {arXiv:2312.07935},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/EAHQZERR/Sapkota et al. - 2024 - Comparing YOLOv8 and Mask RCNN for object segmenta.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/4BBPBWPF/2312.html:text/html},
}

@misc{wei_general_2024,
	title = {General {OCR} {Theory}: {Towards} {OCR}-2.0 via a {Unified} {End}-to-end {Model}},
	shorttitle = {General {OCR} {Theory}},
	url = {http://arxiv.org/abs/2409.01704},
	doi = {10.48550/arXiv.2409.01704},
	abstract = {Traditional OCR systems (OCR-1.0) are increasingly unable to meet people's usage due to the growing demand for intelligent processing of man-made optical characters. In this paper, we collectively refer to all artificial optical signals (e.g., plain texts, math/molecular formulas, tables, charts, sheet music, and even geometric shapes) as "characters" and propose the General OCR Theory along with an excellent model, namely GOT, to promote the arrival of OCR-2.0. The GOT, with 580M parameters, is a unified, elegant, and end-to-end model, consisting of a high-compression encoder and a long-contexts decoder. As an OCR-2.0 model, GOT can handle all the above "characters" under various OCR tasks. On the input side, the model supports commonly used scene- and document-style images in slice and whole-page styles. On the output side, GOT can generate plain or formatted results (markdown/tikz/smiles/kern) via an easy prompt. Besides, the model enjoys interactive OCR features, i.e., region-level recognition guided by coordinates or colors. Furthermore, we also adapt dynamic resolution and multi-page OCR technologies to GOT for better practicality. In experiments, we provide sufficient results to prove the superiority of our model.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and Han, Chunrui and Zhang, Xiangyu},
	month = sep,
	year = {2024},
	note = {arXiv:2409.01704},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/S3NCYENN/Wei et al. - 2024 - General OCR Theory Towards OCR-2.0 via a Unified .pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/YKN4L65R/2409.html:text/html},
}

@misc{da_vision_2023,
	title = {Vision {Grid} {Transformer} for {Document} {Layout} {Analysis}},
	url = {http://arxiv.org/abs/2308.14978},
	doi = {10.48550/arXiv.2308.14978},
	abstract = {Document pre-trained models and grid-based models have proven to be very effective on various tasks in Document AI. However, for the document layout analysis (DLA) task, existing document pre-trained models, even those pre-trained in a multi-modal fashion, usually rely on either textual features or visual features. Grid-based models for DLA are multi-modality but largely neglect the effect of pre-training. To fully leverage multi-modal information and exploit pre-training techniques to learn better representation for DLA, in this paper, we present VGT, a two-stream Vision Grid Transformer, in which Grid Transformer (GiT) is proposed and pre-trained for 2D token-level and segment-level semantic understanding. Furthermore, a new dataset named D\${\textasciicircum}4\$LA, which is so far the most diverse and detailed manually-annotated benchmark for document layout analysis, is curated and released. Experiment results have illustrated that the proposed VGT model achieves new state-of-the-art results on DLA tasks, e.g. PubLayNet (\$95.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$96.2{\textbackslash}\%\$), DocBank (\$79.6{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$84.1{\textbackslash}\%\$), and D\${\textasciicircum}4\$LA (\$67.7{\textbackslash}\%\$\${\textbackslash}rightarrow\$\$68.8{\textbackslash}\%\$). The code and models as well as the D\${\textasciicircum}4\$LA dataset will be made publicly available {\textasciitilde}{\textbackslash}url\{https://github.com/AlibabaResearch/AdvancedLiterateMachinery\}.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Da, Cheng and Luo, Chuwei and Zheng, Qi and Yao, Cong},
	month = aug,
	year = {2023},
	note = {arXiv:2308.14978},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/UX6VLNG5/Da et al. - 2023 - Vision Grid Transformer for Document Layout Analys.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/LSMZUUV2/2308.html:text/html},
}

@misc{zhang_vsr_2021,
	title = {{VSR}: {A} {Unified} {Framework} for {Document} {Layout} {Analysis} combining {Vision}, {Semantics} and {Relations}},
	shorttitle = {{VSR}},
	url = {http://arxiv.org/abs/2105.06220},
	doi = {10.48550/arXiv.2105.06220},
	abstract = {Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Zhang, Peng and Li, Can and Qiao, Liang and Cheng, Zhanzhan and Pu, Shiliang and Niu, Yi and Wu, Fei},
	month = may,
	year = {2021},
	note = {arXiv:2105.06220},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/E8FPAVZH/Zhang et al. - 2021 - VSR A Unified Framework for Document Layout Analy.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/CZP5BPBB/2105.html:text/html},
}

@misc{li_dit_2022,
	title = {{DiT}: {Self}-supervised {Pre}-training for {Document} {Image} {Transformer}},
	shorttitle = {{DiT}},
	url = {http://arxiv.org/abs/2203.02378},
	doi = {10.48550/arXiv.2203.02378},
	abstract = {Image Transformer has recently achieved significant progress for natural image understanding, either using supervised (ViT, DeiT, etc.) or self-supervised (BEiT, MAE, etc.) pre-training techniques. In this paper, we propose {\textbackslash}textbf\{DiT\}, a self-supervised pre-trained {\textbackslash}textbf\{D\}ocument {\textbackslash}textbf\{I\}mage {\textbackslash}textbf\{T\}ransformer model using large-scale unlabeled text images for Document AI tasks, which is essential since no supervised counterparts ever exist due to the lack of human-labeled document images. We leverage DiT as the backbone network in a variety of vision-based Document AI tasks, including document image classification, document layout analysis, table detection as well as text detection for OCR. Experiment results have illustrated that the self-supervised pre-trained DiT model achieves new state-of-the-art results on these downstream tasks, e.g. document image classification (91.11 \${\textbackslash}rightarrow\$ 92.69), document layout analysis (91.0 \${\textbackslash}rightarrow\$ 94.9), table detection (94.23 \${\textbackslash}rightarrow\$ 96.55) and text detection for OCR (93.07 \${\textbackslash}rightarrow\$ 94.29). The code and pre-trained models are publicly available at {\textbackslash}url\{https://aka.ms/msdit\}.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Li, Junlong and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Zhang, Cha and Wei, Furu},
	month = jul,
	year = {2022},
	note = {arXiv:2203.02378},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/VEX99MW7/Li et al. - 2022 - DiT Self-supervised Pre-training for Document Ima.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/W6TRBMX3/2203.html:text/html},
}

@misc{huang_layoutlmv3_2022,
	title = {{LayoutLMv3}: {Pre}-training for {Document} {AI} with {Unified} {Text} and {Image} {Masking}},
	shorttitle = {{LayoutLMv3}},
	url = {http://arxiv.org/abs/2204.08387},
	doi = {10.48550/arXiv.2204.08387},
	abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose {\textbackslash}textbf\{LayoutLMv3\} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv3\}.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
	month = jul,
	year = {2022},
	note = {arXiv:2204.08387},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/KTV5QNPA/Huang et al. - 2022 - LayoutLMv3 Pre-training for Document AI with Unif.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/8EX8DQL6/2204.html:text/html},
}

@misc{zhong_publaynet_2019,
	title = {{PubLayNet}: largest dataset ever for document layout analysis},
	shorttitle = {{PubLayNet}},
	url = {http://arxiv.org/abs/1908.07836},
	doi = {10.48550/arXiv.1908.07836},
	abstract = {Recognizing the layout of unstructured digital documents is an important step when parsing the documents into structured machine-readable format for downstream applications. Deep neural networks that are developed for computer vision have been proven to be an effective method to analyze layout of document images. However, document layout datasets that are currently publicly available are several magnitudes smaller than established computing vision datasets. Models have to be trained by transfer learning from a base model that is pre-trained on a traditional computer vision dataset. In this paper, we develop the PubLayNet dataset for document layout analysis by automatically matching the XML representations and the content of over 1 million PDF articles that are publicly available on PubMed Central. The size of the dataset is comparable to established computer vision datasets, containing over 360 thousand document images, where typical document layout elements are annotated. The experiments demonstrate that deep neural networks trained on PubLayNet accurately recognize the layout of scientific articles. The pre-trained models are also a more effective base mode for transfer learning on a different document domain. We release the dataset (https://github.com/ibm-aur-nlp/PubLayNet) to support development and evaluation of more advanced models for document layout analysis.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Zhong, Xu and Tang, Jianbin and Yepes, Antonio Jimeno},
	month = aug,
	year = {2019},
	note = {arXiv:1908.07836},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jonno/Zotero/storage/IQM87IA7/Zhong et al. - 2019 - PubLayNet largest dataset ever for document layou.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/QAYXHK99/1908.html:text/html},
}

@inproceedings{park_cord_2019,
	title = {{CORD}: {A} {Consolidated} {Receipt} {Dataset} for {Post}-{OCR} {Parsing}},
	shorttitle = {{CORD}},
	url = {https://openreview.net/forum?id=SJl3z659UH},
	abstract = {OCR is inevitably linked to NLP since its final output is in text. Advances in document intelligence are driving the need for a unified technology that integrates OCR with various NLP tasks, especially semantic parsing. Since OCR and semantic parsing have been studied as separate tasks so far, the datasets for each task on their own are rich, while those for the integrated post-OCR parsing tasks are relatively insufficient. In this study, we publish a consolidated dataset for receipt parsing as the first step towards post-OCR parsing tasks. The dataset consists of thousands of Indonesian receipts, which contains images and box/text annotations for OCR, and multi-level semantic labels for parsing. The proposed dataset can be used to address various OCR and parsing tasks.},
	language = {en},
	urldate = {2024-10-14},
	author = {Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
	month = nov,
	year = {2019},
	file = {Full Text PDF:/home/jonno/Zotero/storage/NKB6TN44/Park et al. - 2019 - CORD A Consolidated Receipt Dataset for Post-OCR .pdf:application/pdf},
}

@inproceedings{jaume_funsd_2019,
	title = {{FUNSD}: {A} {Dataset} for {Form} {Understanding} in {Noisy} {Scanned} {Documents}},
	shorttitle = {{FUNSD}},
	url = {http://arxiv.org/abs/1905.13538},
	abstract = {We present a new dataset for form understanding in noisy scanned documents (FUNSD) that aims at extracting and structuring the textual content of forms. The dataset comprises 199 real, fully annotated, scanned forms. The documents are noisy and vary widely in appearance, making form understanding (FoUn) a challenging task. The proposed dataset can be used for various tasks, including text detection, optical character recognition, spatial layout analysis, and entity labeling/linking. To the best of our knowledge, this is the first publicly available dataset with comprehensive annotations to address FoUn task. We also present a set of baselines and introduce metrics to evaluate performance on the FUNSD dataset, which can be downloaded at https://guillaumejaume.github.io/FUNSD/.},
	urldate = {2024-10-14},
	booktitle = {{ICADAR}-{OST} 2019},
	publisher = {arXiv},
	author = {Jaume, Guillaume and Ekenel, Hazim Kemal and Thiran, Jean-Philippe},
	month = oct,
	year = {2019},
	note = {arXiv:1905.13538},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Information Retrieval, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/WAHNVA3S/Jaume et al. - 2019 - FUNSD A Dataset for Form Understanding in Noisy S.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/E6FGEQEQ/1905.html:text/html},
}

@inproceedings{dell_american_2024,
	address = {Red Hook, NY, USA},
	series = {{NIPS} '23},
	title = {American stories: a large-scale structured text dataset of historical {U}.{S}. newspapers},
	shorttitle = {American stories},
	abstract = {Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress's public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people's ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of reproduced content, and news story clustering. Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.},
	urldate = {2024-10-14},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Dell, Melissa and Carlson, Jacob and Bryan, Tom and Silcock, Emily and Arora, Abhishek and Shen, Zejiang and D'Amico-Wong, Luca and Le, Quan and Querubin, Pablo and Heldring, Leander},
	month = may,
	year = {2024},
	pages = {80744--80772},
}

@misc{sven_page_2022,
	title = {Page {Layout} {Analysis} of {Text}-heavy {Historical} {Documents}: a {Comparison} of {Textual} and {Visual} {Approaches}},
	shorttitle = {Page {Layout} {Analysis} of {Text}-heavy {Historical} {Documents}},
	url = {http://arxiv.org/abs/2212.13924},
	doi = {10.48550/arXiv.2212.13924},
	abstract = {Page layout analysis is a fundamental step in document processing which enables to segment a page into regions of interest. With highly complex layouts and mixed scripts, scholarly commentaries are text-heavy documents which remain challenging for state-of-the-art models. Their layout considerably varies across editions and their most important regions are mainly defined by semantic rather than graphical characteristics such as position or appearance. This setting calls for a comparison between textual, visual and hybrid approaches. We therefore assess the performances of two transformers (LayoutLMv3 and RoBERTa) and an objection-detection network (YOLOv5). If results show a clear advantage in favor of the latter, we also list several caveats to this finding. In addition to our experiments, we release a dataset of ca. 300 annotated pages sampled from 19th century commentaries.},
	urldate = {2024-10-14},
	publisher = {arXiv},
	author = {Sven, Najem-Meyer and Matteo, Romanello},
	month = dec,
	year = {2022},
	note = {arXiv:2212.13924},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/4VAW3GEH/Sven and Matteo - 2022 - Page Layout Analysis of Text-heavy Historical Docu.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/6KNTBLUM/2212.html:text/html},
}

@misc{zhao_doclayout-yolo_2024,
	title = {{DocLayout}-{YOLO}: {Enhancing} {Document} {Layout} {Analysis} through {Diverse} {Synthetic} {Data} and {Global}-to-{Local} {Adaptive} {Perception}},
	shorttitle = {{DocLayout}-{YOLO}},
	url = {http://arxiv.org/abs/2410.12628},
	doi = {10.48550/arXiv.2410.12628},
	abstract = {Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.},
	urldate = {2024-10-21},
	publisher = {arXiv},
	author = {Zhao, Zhiyuan and Kang, Hengrui and Wang, Bin and He, Conghui},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/J36YPZ4S/Zhao et al. - 2024 - DocLayout-YOLO Enhancing Document Layout Analysis.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/QUZBSLCL/2410.html:text/html},
}

@misc{zhao_doclayout-yolo_2024-1,
	title = {{DocLayout}-{YOLO}: {Enhancing} {Document} {Layout} {Analysis} through {Diverse} {Synthetic} {Data} and {Global}-to-{Local} {Adaptive} {Perception}},
	shorttitle = {{DocLayout}-{YOLO}},
	url = {http://arxiv.org/abs/2410.12628},
	doi = {10.48550/arXiv.2410.12628},
	abstract = {Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.},
	urldate = {2024-10-22},
	publisher = {arXiv},
	author = {Zhao, Zhiyuan and Kang, Hengrui and Wang, Bin and He, Conghui},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/KQR7KZ7C/Zhao et al. - 2024 - DocLayout-YOLO Enhancing Document Layout Analysis.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/VQ4IJJD9/2410.html:text/html},
}

@inproceedings{bohacek_when_2024,
	address = {Mexico City, Mexico},
	title = {When {XGBoost} {Outperforms} {GPT}-4 on {Text} {Classification}: {A} {Case} {Study}},
	shorttitle = {When {XGBoost} {Outperforms} {GPT}-4 on {Text} {Classification}},
	url = {https://aclanthology.org/2024.trustnlp-1.5},
	doi = {10.18653/v1/2024.trustnlp-1.5},
	abstract = {Large language models (LLMs) are increasingly used for applications beyond text generation, ranging from text summarization to instruction following. One popular example of exploiting LLMs' zero- and few-shot capabilities is the task of text classification. This short paper compares two popular LLM-based classification pipelines (GPT-4 and LLAMA 2) to a popular pre-LLM-era classification pipeline on the task of news trustworthiness classification, focusing on performance, training, and deployment requirements. We find that, in this case, the pre-LLM-era ensemble pipeline outperforms the two popular LLM pipelines while being orders of magnitude smaller in parameter size.},
	urldate = {2024-10-26},
	booktitle = {Proceedings of the 4th {Workshop} on {Trustworthy} {Natural} {Language} {Processing} ({TrustNLP} 2024)},
	publisher = {Association for Computational Linguistics},
	author = {Bohacek, Matyas and Bravansky, Michal},
	editor = {Ovalle, Anaelia and Chang, Kai-Wei and Cao, Yang Trista and Mehrabi, Ninareh and Zhao, Jieyu and Galstyan, Aram and Dhamala, Jwala and Kumar, Anoop and Gupta, Rahul},
	month = jun,
	year = {2024},
	pages = {51--60},
	file = {Full Text PDF:/home/jonno/Zotero/storage/79WPS6XC/Bohacek and Bravansky - 2024 - When XGBoost Outperforms GPT-4 on Text Classificat.pdf:application/pdf},
}

@inproceedings{clausner_enp_2015,
	title = {The {ENP} image and ground truth dataset of historical newspapers},
	url = {https://ieeexplore.ieee.org/document/7333898},
	doi = {10.1109/ICDAR.2015.7333898},
	abstract = {This paper presents a research dataset of historical newspapers comprising over 500 page images, uniquely representative of European cultural heritage from the digitization projects of 12 national and major European libraries, created within the scope of the large-scale digitisation Europeana Newspapers Project (ENP). Every image is accompanied by comprehensive ground truth (Unicode encoded full-text, layout information with precise region outlines, type labels, and reading order) in PAGE format and searchable metadata about document characteristics and artefacts. The first part of the paper describes the nature of the dataset, how it was built, and the challenges encountered. In the second part, a baseline for two state-of-the-art OCR systems (ABBYY FineReader Engine 11 and Tesseract 3.03) is given with regard to both text recognition and segmentation/layout analysis performance.},
	urldate = {2024-11-07},
	booktitle = {2015 13th {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR})},
	author = {Clausner, Christian and Papadopoulos, Christos and Pletschacher, Stefan and Antonacopoulos, Apostolos},
	month = aug,
	year = {2015},
	keywords = {Optical character recognition software, Engines, Europe, historical documents, document analysis, ground truth, image dataset},
	pages = {931--935},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/HQNG9NP2/7333898.html:text/html},
}

@misc{shen_olala_2021,
	title = {{OLALA}: {Object}-{Level} {Active} {Learning} for {Efficient} {Document} {Layout} {Annotation}},
	shorttitle = {{OLALA}},
	url = {http://arxiv.org/abs/2010.01762},
	abstract = {Document images often have intricate layout structures, with numerous content regions (e.g. texts, ﬁgures, tables) densely arranged on each page. This makes the manual annotation of layout datasets expensive and inefﬁcient. These characteristics also challenge existing active learning methods, as image-level scoring and selection suffer from the overexposure of common objects. Inspired by recent progresses in semi-supervised learning and self-training, we propose an Object-Level Active Learning framework for efﬁcient document layout Annotation, OLALA. In this framework, only regions with the most ambiguous object predictions within an image are selected for annotators to label, optimizing the use of the annotation budget. For unselected predictions, the semiautomatic correction algorithm is proposed to identify certain errors based on prior knowledge of layout structures and rectiﬁes them with minor supervision. Additionally, we carefully design a perturbation-based object scoring function for document images. It governs the object selection process via evaluating prediction ambiguities, and considers both the positions and categories of predicted layout objects. Extensive experiments show that OLALA can signiﬁcantly boost model performance and improve annotation efﬁciency, given the same labeling budget. Code for this paper can be accessed via https://github.com/ lolipopshock/detectron2\_al.},
	language = {en},
	urldate = {2024-11-07},
	publisher = {arXiv},
	author = {Shen, Zejiang and Zhao, Jian and Dell, Melissa and Yu, Yaoliang and Li, Weining},
	month = mar,
	year = {2021},
	note = {arXiv:2010.01762 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 12 pages, 7 figures, 5 tables},
	file = {Shen et al. - 2021 - OLALA Object-Level Active Learning for Efficient .pdf:/home/jonno/Zotero/storage/EGFURVGQ/Shen et al. - 2021 - OLALA Object-Level Active Learning for Efficient .pdf:application/pdf},
}

@misc{unknown_end_1800,
	title = {End of the {Hunt}},
	copyright = {Public domain},
	author = {Unknown},
	year = {1800},
}

@misc{dosovitskiy_image_2021,
	title = {An {Image} is {Worth} 16x16 {Words}: {Transformers} for {Image} {Recognition} at {Scale}},
	shorttitle = {An {Image} is {Worth} 16x16 {Words}},
	url = {http://arxiv.org/abs/2010.11929},
	abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
	language = {en},
	urldate = {2024-11-10},
	publisher = {arXiv},
	author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
	month = jun,
	year = {2021},
	note = {arXiv:2010.11929 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Fine-tuning code and pre-trained models are available at https://github.com/google-research/vision\_transformer. ICLR camera-ready version with 2 small modifications: 1) Added a discussion of CLS vs GAP classifier in the appendix, 2) Fixed an error in exaFLOPs computation in Figure 5 and Table 6 (relative performance of models is basically not affected)},
	file = {Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:/home/jonno/Zotero/storage/ZALMB2QW/Dosovitskiy et al. - 2021 - An Image is Worth 16x16 Words Transformers for Im.pdf:application/pdf},
}

@unpublished{wilde_woman_1893,
	address = {London},
	type = {Play},
	title = {A {Woman} of {No} {Importance}},
	copyright = {Public domain},
	language = {English},
	author = {Wilde, Oscar},
	month = apr,
	year = {1893},
}

@misc{auer_docling_2024,
	title = {Docling {Technical} {Report}},
	url = {http://arxiv.org/abs/2408.09869},
	abstract = {This technical report introduces Docling, an easy to use, self-contained, MITlicensed open-source package for PDF document conversion. It is powered by state-of-the-art specialized AI models for layout analysis (DocLayNet) and table structure recognition (TableFormer), and runs efficiently on commodity hardware in a small resource budget. The code interface allows for easy extensibility and addition of new features and models.},
	language = {en},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Auer, Christoph and Lysak, Maksym and Nassar, Ahmed and Dolfi, Michele and Livathinos, Nikolaos and Vagenas, Panos and Ramis, Cesar Berrospi and Omenetti, Matteo and Lindlbauer, Fabian and Dinkla, Kasper and Mishra, Lokesh and Kim, Yusik and Gupta, Shubham and Lima, Rafael Teixeira de and Weber, Valery and Morin, Lucas and Meijer, Ingmar and Kuropiatnyk, Viktor and Staar, Peter W. J.},
	month = aug,
	year = {2024},
	note = {arXiv:2408.09869 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering, Computer Science - Computer Vision and Pattern Recognition},
	file = {Auer et al. - 2024 - Docling Technical Report.pdf:/home/jonno/Zotero/storage/PVRNV9BG/Auer et al. - 2024 - Docling Technical Report.pdf:application/pdf},
}

@misc{agrawal_pixtral_2024,
	title = {Pixtral {12B}},
	url = {http://arxiv.org/abs/2410.07073},
	abstract = {We introduce Pixtral 12B, a 12–billion-parameter multimodal language model. Pixtral 12B is trained to understand both natural images and documents, achieving leading performance on various multimodal benchmarks, surpassing a number of larger models. Unlike many open-source models, Pixtral is also a cutting-edge text model for its size, and does not compromise on natural language performance to excel in multimodal tasks. Pixtral uses a new vision encoder trained from scratch, which allows it to ingest images at their natural resolution and aspect ratio. This gives users flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Pixtral 12B substanially outperforms other open models of similar sizes (Llama-3.2 11B \& Qwen-2-VL 7B). It also outperforms much larger open models like Llama-3.2 90B while being 7x smaller. We further contribute an open-source benchmark, MM-MT-Bench, for evaluating vision-language models in practical scenarios, and provide detailed analysis and code for standardized evaluation protocols for multimodal LLMs. Pixtral 12B is released under Apache 2.0 license.},
	language = {en},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Agrawal, Pravesh and Antoniak, Szymon and Hanna, Emma Bou and Bout, Baptiste and Chaplot, Devendra and Chudnovsky, Jessica and Costa, Diogo and Monicault, Baudouin De and Garg, Saurabh and Gervet, Theophile and Ghosh, Soham and Héliou, Amélie and Jacob, Paul and Jiang, Albert Q. and Khandelwal, Kartik and Lacroix, Timothée and Lample, Guillaume and Casas, Diego Las and Lavril, Thibaut and Scao, Teven Le and Lo, Andy and Marshall, William and Martin, Louis and Mensch, Arthur and Muddireddy, Pavankumar and Nemychnikova, Valera and Pellat, Marie and Platen, Patrick Von and Raghuraman, Nikhil and Rozière, Baptiste and Sablayrolles, Alexandre and Saulnier, Lucile and Sauvestre, Romain and Shang, Wendy and Soletskyi, Roman and Stewart, Lawrence and Stock, Pierre and Studnia, Joachim and Subramanian, Sandeep and Vaze, Sagar and Wang, Thomas and Yang, Sophia},
	month = oct,
	year = {2024},
	note = {arXiv:2410.07073 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Agrawal et al. - 2024 - Pixtral 12B.pdf:/home/jonno/Zotero/storage/28ND9HG8/Agrawal et al. - 2024 - Pixtral 12B.pdf:application/pdf},
}

@incollection{wilde_critic_1891,
	edition = {Third},
	title = {The {Critic} as {Artist}},
	isbn = {1-64423-003-8},
	url = {https://www.gutenberg.org/cache/epub/887/pg887-images.html},
	language = {English},
	booktitle = {Intentions},
	publisher = {Methuen and Co.},
	author = {Wilde, Oscar},
	year = {1891},
}

@misc{su_roformer_2023,
	title = {{RoFormer}: {Enhanced} {Transformer} with {Rotary} {Position} {Embedding}},
	shorttitle = {{RoFormer}},
	url = {http://arxiv.org/abs/2104.09864},
	abstract = {Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: https://huggingface.co/docs/transformers/model\_doc/roformer.},
	language = {en},
	urldate = {2024-11-12},
	publisher = {arXiv},
	author = {Su, Jianlin and Lu, Yu and Pan, Shengfeng and Murtadha, Ahmed and Wen, Bo and Liu, Yunfeng},
	month = nov,
	year = {2023},
	note = {arXiv:2104.09864 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: fixed some typos},
	file = {Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:/home/jonno/Zotero/storage/4TVMYVNK/Su et al. - 2023 - RoFormer Enhanced Transformer with Rotary Positio.pdf:application/pdf},
}

@inproceedings{molina_fetch--set_2024,
	address = {Cham},
	title = {Fetch-{A}-{Set}: {A} {Large}-{Scale} {OCR}-{Free} {Benchmark} for {Historical} {Document} {Retrieval}},
	isbn = {978-3-031-70442-0},
	shorttitle = {Fetch-{A}-{Set}},
	doi = {10.1007/978-3-031-70442-0_21},
	abstract = {This paper introduces Fetch-A-Set (FAS), a comprehensive benchmark tailored for legislative historical document analysis systems, addressing the challenges of large-scale document retrieval in historical contexts. The benchmark comprises a vast repository of documents dating back to the XVII century, serving both as a training resource and an evaluation benchmark for retrieval systems. It fills a critical gap in the literature by focusing on complex extractive tasks within the domain of cultural heritage. The proposed benchmark tackles the multifaceted problem of historical document analysis, including text-to-image retrieval for queries and image-to-text topic extraction from document fragments, all while accommodating varying levels of document legibility. This benchmark aims to spur advancements in the field by providing baselines and data for the development and evaluation of robust historical document retrieval systems, particularly in scenarios characterized by wide historical spectrum.},
	language = {en},
	booktitle = {Document {Analysis} {Systems}},
	publisher = {Springer Nature Switzerland},
	author = {Molina, Adrià and Terrades, Oriol Ramos and Lladós, Josep},
	editor = {Sfikas, Giorgos and Retsinas, George},
	year = {2024},
	keywords = {Historical documents, Datasets, Document Retrieval, Information Extraction, Legislative Documents},
	pages = {347--362},
	file = {Full Text PDF:/home/jonno/Zotero/storage/JP6SHTWB/Molina et al. - 2024 - Fetch-A-Set A Large-Scale OCR-Free Benchmark for .pdf:application/pdf},
}

@article{hamdi_-depth_2023,
	title = {In-depth analysis of the impact of {OCR} errors on named entity recognition and linking},
	volume = {29},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/indepth-analysis-of-the-impact-of-ocr-errors-on-named-entity-recognition-and-linking/C732399FF72BAFE8FF830BB1F5ED7576},
	doi = {10.1017/S1351324922000110},
	abstract = {Named entities (NEs) are among the most relevant type of information that can be used to properly index digital documents and thus easily retrieve them. It has long been observed that NEs are key to accessing the contents of digital library portals as they are contained in most user queries. However, most digitized documents are indexed through their optical character recognition (OCRed) version which include numerous errors. Although OCR engines have considerably improved over the last few years, OCR errors still considerably impact document access. Previous works were conducted to evaluate the impact of OCR errors on named entity recognition (NER) and named entity linking (NEL) techniques separately. In this article, we experimented with a variety of OCRed documents with different levels and types of OCR noise to assess in depth the impact of OCR on named entity processing. We provide a deep analysis of OCR errors that impact the performance of NER and NEL. We then present the resulting exhaustive study and subsequent recommendations on the adequate documents, the OCR quality levels, and the post-OCR correction strategies required to perform reliable NER and NEL.},
	language = {en},
	number = {2},
	urldate = {2024-11-14},
	journal = {Natural Language Engineering},
	author = {Hamdi, Ahmed and Pontes, Elvys Linhares and Sidere, Nicolas and Coustaty, Mickaël and Doucet, Antoine},
	month = mar,
	year = {2023},
	keywords = {Information retrieval, Neural networks, Document indexing, Named entity recognition and linking, Optical character recognition},
	pages = {425--448},
	file = {Full Text PDF:/home/jonno/Zotero/storage/BTGULDSF/Hamdi et al. - 2023 - In-depth analysis of the impact of OCR errors on n.pdf:application/pdf},
}

@misc{zhao_doclayout-yolo_2024-2,
	title = {{DocLayout}-{YOLO}: {Enhancing} {Document} {Layout} {Analysis} through {Diverse} {Synthetic} {Data} and {Global}-to-{Local} {Adaptive} {Perception}},
	shorttitle = {{DocLayout}-{YOLO}},
	url = {http://arxiv.org/abs/2410.12628},
	doi = {10.48550/arXiv.2410.12628},
	abstract = {Document Layout Analysis is crucial for real-world document understanding systems, but it encounters a challenging trade-off between speed and accuracy: multimodal methods leveraging both text and visual features achieve higher accuracy but suffer from significant latency, whereas unimodal methods relying solely on visual features offer faster processing speeds at the expense of accuracy. To address this dilemma, we introduce DocLayout-YOLO, a novel approach that enhances accuracy while maintaining speed advantages through document-specific optimizations in both pre-training and model design. For robust document pre-training, we introduce the Mesh-candidate BestFit algorithm, which frames document synthesis as a two-dimensional bin packing problem, generating the large-scale, diverse DocSynth-300K dataset. Pre-training on the resulting DocSynth-300K dataset significantly improves fine-tuning performance across various document types. In terms of model optimization, we propose a Global-to-Local Controllable Receptive Module that is capable of better handling multi-scale variations of document elements. Furthermore, to validate performance across different document types, we introduce a complex and challenging benchmark named DocStructBench. Extensive experiments on downstream datasets demonstrate that DocLayout-YOLO excels in both speed and accuracy. Code, data, and models are available at https://github.com/opendatalab/DocLayout-YOLO.},
	urldate = {2024-11-14},
	publisher = {arXiv},
	author = {Zhao, Zhiyuan and Kang, Hengrui and Wang, Bin and He, Conghui},
	month = oct,
	year = {2024},
	note = {arXiv:2410.12628},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/7R5FCWF2/Zhao et al. - 2024 - DocLayout-YOLO Enhancing Document Layout Analysis.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/TLGVHIZD/2410.html:text/html},
}

@article{almutairi_newspaper_2024,
	title = {Newspaper elements detection and newspaper pages categorization using {CNNs} and transformers},
	issn = {1433-2825},
	url = {https://doi.org/10.1007/s10032-024-00503-9},
	doi = {10.1007/s10032-024-00503-9},
	abstract = {Newspaper digitization has gained wide interest around the world. Archives of digitized newspapers and magazines contain a wealth of information that spans decades. To extract this abundance of information, optical character recognition (OCR) techniques with extensive manual page annotation have been employed. The OCR techniques extract the text from the raw image of the page, while page annotation adds meta-data about the content of the page such as the category of the page and the location of articles and other elements. To automate this process, I propose a framework for detecting newspaper pages elements and categorizing newspaper pages. The framework will use visual features of the digitized newspaper to classify the printed media type and decompose the newspaper page into its main elements (news articles, advertisements, and page headers) using object detection. Then, it will use both visual and textual features to categorize the newspaper page into its main sections (first page, politics, economy, sports, and advertisement). The element detection stage is leveraged by using only news articles for categorizing the pages, since other elements, such as advertisements, may contain visual and textual features that are not related to the page section. This framework will prepare the newspaper page for the OCR methods to extract meaningful information. The page elements detection phase of the framework is language-agnostic, which allows it to extract the articles from newspapers in different languages (e.g., Arabic, English, French, German, etc). The framework will use two deep neural networks architectures, Faster R-CNN which is based on the convolutional neural network (CNN) architecture and Transformers to classify and detect elements in the printed media.},
	language = {en},
	urldate = {2024-12-10},
	journal = {International Journal on Document Analysis and Recognition (IJDAR)},
	author = {Almutairi, Abdullah},
	month = oct,
	year = {2024},
	keywords = {Multimodal classification, Newspaper page categorization, Newspaper page element detection, Transformers},
	file = {Full Text PDF:/home/jonno/Zotero/storage/CHUC6DYC/Almutairi - 2024 - Newspaper elements detection and newspaper pages c.pdf:application/pdf},
}

@phdthesis{wu_multilayered_2019,
	type = {report},
	title = {Multilayered {Analysis} of {Newspaper} {Structure} and {Design}},
	url = {https://inria.hal.science/hal-02177784},
	abstract = {Understanding newspaper structure and design remains a challenging task due to the complex composition of pages with many visual and textual elements. Current approaches have focused on simple design types and analysed only broad classes for the components in a page. In this paper, we propose an approach to obtain a comprehensive understanding of a newspaper page through a multi-layered analysis of structure and design. Taking images of newspaper front pages as input, our approach uses a combination of computer vision techniques to segment newspapers with complex layouts into meaningful blocks of varying degrees of granularity, and convolutional neural network (CNN) to classify each block. The final output presents a visualization of the various layers of design elements present in the newspaper. Compared to previous approaches, our method introduces a much larger set of design-related labels (23 labels against less than 10 before) resulting in a very fine description of the pages, with high accuracy (83\%). As a whole, this automated analysis would have potential applications such as cross-medium content adaptation, digital archiving, and UX design.},
	language = {en},
	urldate = {2024-12-10},
	school = {UCA, Inria},
	author = {Wu, Hui-Yin and Kornprobst, Pierre},
	month = jul,
	year = {2019},
	file = {Full Text PDF:/home/jonno/Zotero/storage/A94T5H82/Wu and Kornprobst - 2019 - Multilayered Analysis of Newspaper Structure and D.pdf:application/pdf},
}

@article{wu_multilayered_nodate,
	title = {Multilayered {Analysis} of {Newspaper} {Structure} and {Design}},
	abstract = {Understanding newspaper structure and design remains a challenging task due to the complex composition of pages with many visual and textual elements. Current approaches have focused on simple design types and analysed only broad classes for the components in a page. In this paper, we propose an approach to obtain a comprehensive understanding of a newspaper page through a multi-layered analysis of structure and design. Taking images of newspaper front pages as input, our approach uses a combination of computer vision techniques to segment newspapers with complex layouts into meaningful blocks of varying degrees of granularity, and convolutional neural network (CNN) to classify each block. The ﬁnal output presents a visualization of the various layers of design elements present in the newspaper. Compared to previous approaches, our method introduces a much larger set of design-related labels (23 labels against less than 10 before) resulting in a very ﬁne description of the pages, with high accuracy (83\%). As a whole, this automated analysis would have potential applications such as cross-medium content adaptation, digital archiving, and UX design.},
	language = {en},
	author = {Wu, Hui-Yin and Kornprobst, Pierre},
	file = {Wu and Kornprobst - Multilayered Analysis of Newspaper Structure and D.pdf:/home/jonno/Zotero/storage/IV2WKAT4/Wu and Kornprobst - Multilayered Analysis of Newspaper Structure and D.pdf:application/pdf},
}

@misc{wang_yolov10_2024,
	title = {{YOLOv10}: {Real}-{Time} {End}-to-{End} {Object} {Detection}},
	shorttitle = {{YOLOv10}},
	url = {http://arxiv.org/abs/2405.14458},
	doi = {10.48550/arXiv.2405.14458},
	abstract = {Over the past years, YOLOs have emerged as the predominant paradigm in the field of real-time object detection owing to their effective balance between computational cost and detection performance. Researchers have explored the architectural designs, optimization objectives, data augmentation strategies, and others for YOLOs, achieving notable progress. However, the reliance on the non-maximum suppression (NMS) for post-processing hampers the end-to-end deployment of YOLOs and adversely impacts the inference latency. Besides, the design of various components in YOLOs lacks the comprehensive and thorough inspection, resulting in noticeable computational redundancy and limiting the model's capability. It renders the suboptimal efficiency, along with considerable potential for performance improvements. In this work, we aim to further advance the performance-efficiency boundary of YOLOs from both the post-processing and model architecture. To this end, we first present the consistent dual assignments for NMS-free training of YOLOs, which brings competitive performance and low inference latency simultaneously. Moreover, we introduce the holistic efficiency-accuracy driven model design strategy for YOLOs. We comprehensively optimize various components of YOLOs from both efficiency and accuracy perspectives, which greatly reduces the computational overhead and enhances the capability. The outcome of our effort is a new generation of YOLO series for real-time end-to-end object detection, dubbed YOLOv10. Extensive experiments show that YOLOv10 achieves state-of-the-art performance and efficiency across various model scales. For example, our YOLOv10-S is 1.8\${\textbackslash}times\$ faster than RT-DETR-R18 under the similar AP on COCO, meanwhile enjoying 2.8\${\textbackslash}times\$ smaller number of parameters and FLOPs. Compared with YOLOv9-C, YOLOv10-B has 46{\textbackslash}\% less latency and 25{\textbackslash}\% fewer parameters for the same performance.},
	urldate = {2025-01-08},
	publisher = {arXiv},
	author = {Wang, Ao and Chen, Hui and Liu, Lihao and Chen, Kai and Lin, Zijia and Han, Jungong and Ding, Guiguang},
	month = oct,
	year = {2024},
	note = {arXiv:2405.14458 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Code: https://github.com/THU-MIG/yolov10; NeurIPS 2024 Camera-ready Version},
	file = {Preprint PDF:/home/jonno/Zotero/storage/FL5AWJGL/Wang et al. - 2024 - YOLOv10 Real-Time End-to-End Object Detection.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/EQ9NQALT/2405.html:text/html},
}

@inproceedings{carlson_efficient_2024,
	address = {Bangkok, Thailand},
	title = {Efficient {OCR} for {Building} a {Diverse} {Digital} {History}},
	url = {https://aclanthology.org/2024.acl-long.440/},
	doi = {10.18653/v1/2024.acl-long.440},
	abstract = {Many users consult digital archives daily, but the information they can access is unrepresentative of the diversity of documentary history. The sequence-to-sequence architecture typically used for optical character recognition (OCR) – which jointly learns a vision and language model – is poorly extensible to low-resource document collections, as learning a language-vision model requires extensive labeled sequences and compute. This study models OCR as a character level image retrieval problem, using a contrastively trained vision encoder. Because the model only learns characters' visual features, it is more sample efficient and extensible than existing architectures, enabling accurate OCR in settings where existing solutions fail. Crucially, it opens new avenues for community engagement in making digital history more representative of documentary history.},
	urldate = {2025-01-08},
	booktitle = {Proceedings of the 62nd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Carlson, Jacob and Bryan, Tom and Dell, Melissa},
	editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
	month = aug,
	year = {2024},
	pages = {8105--8115},
	file = {Full Text PDF:/home/jonno/Zotero/storage/D8UQM2ZS/Carlson et al. - 2024 - Efficient OCR for Building a Diverse Digital Histo.pdf:application/pdf},
}

@misc{schultze_chronicling_2024,
	title = {Chronicling {Germany}: {An} {Annotated} {Historical} {Newspaper} {Dataset}},
	shorttitle = {Chronicling {Germany}},
	url = {http://arxiv.org/abs/2401.16845},
	doi = {10.48550/arXiv.2401.16845},
	abstract = {The correct detection of article layout in historical newspaper pages remains challenging but is important for Natural Language Processing ( NLP) and machine learning applications in the field of digital history. Digital newspaper portals typically provide Optical Character Recognition ( OCR) text, albeit of varying quality. Unfortunately, layout information is often missing, limiting this rich source's scope. Our dataset is designed to address this issue for historic German-language newspapers. The Chronicling Germany dataset contains 581 annotated historical newspaper pages from the time period between 1852 and 1924. Historic domain experts have spent more than 1,500 hours annotating the dataset. The paper presents a processing pipeline and establishes baseline results on in- and out-of-domain test data using this pipeline. Both our dataset and the corresponding baseline code are freely available online. This work creates a starting point for future research in the field of digital history and historic German language newspaper processing. Furthermore, it provides the opportunity to study a low-resource task in computer vision.},
	urldate = {2025-01-15},
	publisher = {arXiv},
	author = {Schultze, Christian and Kerkfeld, Niklas and Kuebart, Kara and Weber, Princilia and Wolter, Moritz and Selgert, Felix},
	month = jun,
	year = {2024},
	note = {arXiv:2401.16845 [cs]
version: 2},
	keywords = {Computer Science - Digital Libraries},
	annote = {Comment: Dataset available at: https://gitlab.uni-bonn.de/digital-history/Chronicling-Germany-Dataset . Baseline code: https://github.com/Digital-History-Bonn/Chronicling-Germany-Code},
	file = {Preprint PDF:/home/jonno/Zotero/storage/J4N7QNF9/Schultze et al. - 2024 - Chronicling Germany An Annotated Historical Newsp.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/3TSMCJPD/2401.html:text/html},
}

@article{ehrmann_language_nodate,
	title = {Language {Resources} for {Historical} {Newspapers}: the {Impresso} {Collection}},
	abstract = {Following decades of massive digitization, an unprecedented amount of historical document facsimiles can now be retrieved and accessed via cultural heritage online portals. If this represents a huge step forward in terms of preservation and accessibility, the next fundamental challenge– and real promise of digitization– is to exploit the contents of these digital assets, and therefore to adapt and develop appropriate language technologies to search and retrieve information from this ‘Big Data of the Past’. Yet, the application of text processing tools on historical documents in general, and historical newspapers in particular, poses new challenges, and crucially requires appropriate language resources. In this context, this paper presents a collection of historical newspaper data sets composed of text and image resources, curated and published within the context of the ‘impresso - Media Monitoring of the Past’ project. With corpora, benchmarks, semantic annotations and language models in French, German and Luxembourgish covering ca. 200 years, the objective of the impresso resource collection is to contribute to historical language resources, and thereby strengthen the robustness of approaches to non-standard inputs and foster efﬁcient processing of historical documents.},
	language = {en},
	author = {Ehrmann, Maud and Romanello, Matteo and Clematide, Simon and Ströbel, Phillip Benjamin and Barman, Raphaël},
	file = {Ehrmann et al. - Language Resources for Historical Newspapers the .pdf:/home/jonno/Zotero/storage/XEDXFV78/Ehrmann et al. - Language Resources for Historical Newspapers the .pdf:application/pdf},
}

@inproceedings{manrique-gomez_historical_2024,
	address = {Miami, USA},
	title = {Historical {Ink}: 19th {Century} {Latin} {American} {Spanish} {Newspaper} {Corpus} with {LLM} {OCR} {Correction}},
	shorttitle = {Historical {Ink}},
	url = {https://aclanthology.org/2024.nlp4dh-1.13/},
	doi = {10.18653/v1/2024.nlp4dh-1.13},
	abstract = {This paper presents two significant contributions: First, it introduces a novel dataset of 19th-century Latin American newspaper texts, addressing a critical gap in specialized corpora for historical and linguistic analysis in this region. Second, it develops a flexible framework that utilizes a Large Language Model for OCR error correction and linguistic surface form detection in digitized corpora. This semi-automated framework is adaptable to various contexts and datasets and is applied to the newly created dataset.},
	urldate = {2025-01-15},
	booktitle = {Proceedings of the 4th {International} {Conference} on {Natural} {Language} {Processing} for {Digital} {Humanities}},
	publisher = {Association for Computational Linguistics},
	author = {Manrique-Gomez, Laura and Montes, Tony and Rodriguez Herrera, Arturo and Manrique, Ruben},
	editor = {Hämäläinen, Mika and Öhman, Emily and Miyagawa, So and Alnajjar, Khalid and Bizzoni, Yuri},
	month = nov,
	year = {2024},
	pages = {132--139},
	file = {Full Text PDF:/home/jonno/Zotero/storage/4ZJQBQN5/Manrique-Gomez et al. - 2024 - Historical Ink 19th Century Latin American Spanis.pdf:application/pdf},
}

@misc{noauthor_historical_nodate,
	title = {Historical {Newspapers} – {BnL} {Open} {Data}},
	url = {https://data.bnl.lu/data/historical-newspapers/},
	language = {en-US},
	urldate = {2025-01-15},
	file = {Snapshot:/home/jonno/Zotero/storage/B5DJSQQZ/historical-newspapers.html:text/html},
}

@misc{noauthor_impresso_nodate,
	title = {impresso {\textbar} {Media} {Monitoring} of the {Past}},
	url = {https://impresso-project.ch/app/?sq=},
	urldate = {2025-01-15},
	file = {impresso | Media Monitoring of the Past:/home/jonno/Zotero/storage/BLP56U2W/app.html:text/html},
}

@misc{johnson_history_2013,
	title = {The {History} of the {Parliamentary} {Franchise}},
	url = {https://researchbriefings.files.parliament.uk/documents/RP13-14/RP13-14.pdf},
	publisher = {House of Commons},
	author = {Johnson, Neil},
	month = mar,
	year = {2013},
}

@article{phillips_great_1995,
	title = {The {Great} {Reform} {Act} of 1832 and the {Political} {Modernization} of {England}},
	volume = {100},
	issn = {0002-8762},
	url = {https://www.jstor.org/stable/2169005},
	doi = {10.2307/2169005},
	number = {2},
	urldate = {2025-01-29},
	journal = {The American Historical Review},
	author = {Phillips, John A. and Wetherell, Charles},
	year = {1995},
	note = {Publisher: [Oxford University Press, American Historical Association]},
	pages = {411--436},
	file = {JSTOR Full Text PDF:/home/jonno/Zotero/storage/F25E6W2K/Phillips and Wetherell - 1995 - The Great Reform Act of 1832 and the Political Mod.pdf:application/pdf},
}

@misc{uk-government_married_1882,
	title = {Married {Women}’s {Property} {Act} 1882},
	url = {https://www.legislation.gov.uk/ukpga/Vict/45-46/75/enacted},
	language = {en},
	urldate = {2025-01-29},
	author = {UK-Government},
	year = {1882},
	note = {Publisher: King's Printer of Acts of Parliament},
	file = {Snapshot:/home/jonno/Zotero/storage/SFL8AG32/enacted.html:text/html},
}

@misc{ashley-cooper_act_1842,
	title = {An {Act} to prohibit the {Employment} of {Women} and {Girls} in {Mines} and {Collieries}, to regulate the {Employment} of {Boys}, and to make other {Provisions} relating to {Persons} working therein.},
	url = {https://www.legislation.gov.uk/ukpga/Vict/5-6/99/contents/enacted},
	language = {eng},
	urldate = {2025-01-29},
	author = {Ashley-Cooper, Anthony},
	year = {1842},
	note = {Publisher: King's Printer of Acts of Parliament},
	file = {Snapshot:/home/jonno/Zotero/storage/JKF5KWNR/enacted.html:text/html},
}

@misc{uk-government_slavery_1833,
	title = {Slavery {Abolition} {Act} 1833 (repealed 19.11.1998)},
	url = {https://www.legislation.gov.uk/ukpga/Will4/3-4/73/section/12/1991-02-01},
	abstract = {An Act for the Abolition of Slavery throughout the British Colonies.},
	language = {eng},
	urldate = {2025-01-29},
	author = {UK-Government},
	year = {1833},
	note = {Publisher: Statute Law Database},
	file = {Snapshot:/home/jonno/Zotero/storage/65F6FBV4/1991-02-01.html:text/html},
}

@misc{beyer_paligemma_2024,
	title = {{PaliGemma}: {A} versatile {3B} {VLM} for transfer},
	shorttitle = {{PaliGemma}},
	url = {http://arxiv.org/abs/2407.07726},
	doi = {10.48550/arXiv.2407.07726},
	abstract = {PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.},
	urldate = {2025-01-29},
	publisher = {arXiv},
	author = {Beyer, Lucas and Steiner, Andreas and Pinto, André Susano and Kolesnikov, Alexander and Wang, Xiao and Salz, Daniel and Neumann, Maxim and Alabdulmohsin, Ibrahim and Tschannen, Michael and Bugliarello, Emanuele and Unterthiner, Thomas and Keysers, Daniel and Koppula, Skanda and Liu, Fangyu and Grycner, Adam and Gritsenko, Alexey and Houlsby, Neil and Kumar, Manoj and Rong, Keran and Eisenschlos, Julian and Kabra, Rishabh and Bauer, Matthias and Bošnjak, Matko and Chen, Xi and Minderer, Matthias and Voigtlaender, Paul and Bica, Ioana and Balazevic, Ivana and Puigcerver, Joan and Papalampidi, Pinelopi and Henaff, Olivier and Xiong, Xi and Soricut, Radu and Harmsen, Jeremiah and Zhai, Xiaohua},
	month = oct,
	year = {2024},
	note = {arXiv:2407.07726 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: v2 adds Appendix H and I and a few citations},
	file = {Preprint PDF:/home/jonno/Zotero/storage/BAXNPMU9/Beyer et al. - 2024 - PaliGemma A versatile 3B VLM for transfer.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/VGBD9WUR/2407.html:text/html},
}

@article{girdhar_digitizing_2024,
	title = {Digitizing {History}: {Transitioning} {Historical} {Paper} {Documents} to {Digital} {Content} for {Information} {Retrieval} and {Mining}—{A} {Comprehensive} {Survey}},
	volume = {11},
	issn = {2329-924X},
	shorttitle = {Digitizing {History}},
	url = {https://ieeexplore.ieee.org/document/10495892},
	doi = {10.1109/TCSS.2024.3378419},
	abstract = {Historical document processing (HDP) corresponds to the task of converting the physical-bind form of historical archives into a web-based centrally digitized form for their conservation, preservation, and ubiquitous access. Besides the conservation of these invaluable historical collections, the key agenda is to make these geographically distributed historical repositories available for information mining and retrieval in a web-centralized touchless mode. Being a matter of interest for interdisciplinary scholars, the endeavor has garnered the attention of many researchers resulting in an immense body of the literature dedicated to digitization strategies. The present study first assembles the prevalent tasks essential for HDP into a pipeline and frames an outline for a generic workflow for historical document digitization. Then, it reports the latest task-specific state of the art which gives a brief discourse on the methods and open challenges in handling historical printed and handwritten script images. Next, grounded on various layout attributes, it further talks about the evaluation metrics and datasets available for observational and analytical purposes. The current study is an attempt to trail the contours of undergoing research and its bottlenecks thus, providing readers with a comprehensive view and understanding of existing studies and unfolding the open avenues for the future outlook.},
	number = {5},
	urldate = {2025-01-29},
	journal = {IEEE Transactions on Computational Social Systems},
	author = {Girdhar, Nancy and Coustaty, Mickaël and Doucet, Antoine},
	month = oct,
	year = {2024},
	note = {Conference Name: IEEE Transactions on Computational Social Systems},
	keywords = {Task analysis, Image segmentation, Layout, Archival images, Handwriting recognition, handwritten, historical document, Image databases, image processing, Image recognition, information retrieval, layout analysis, natural language processing, Natural language processing, printed, Surveys, Text recognition},
	pages = {6151--6180},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/B28WH6GW/10495892.html:text/html},
}

@article{stone_literacy_1969,
	title = {Literacy and {Education} in {England} 1640-1900},
	issn = {0031-2746},
	url = {https://www.jstor.org/stable/650183},
	number = {42},
	urldate = {2025-01-30},
	journal = {Past \& Present},
	author = {Stone, Lawrence},
	year = {1969},
	note = {Publisher: [Oxford University Press, The Past and Present Society]},
	pages = {69--139},
	file = {JSTOR Full Text PDF:/home/jonno/Zotero/storage/PQ7KV298/Stone - 1969 - Literacy and Education in England 1640-1900.pdf:application/pdf},
}

@misc{bourne_scrambled_2024-1,
	title = {Scrambled text: training {Language} {Models} to correct {OCR} errors using synthetic data},
	shorttitle = {Scrambled text},
	url = {http://arxiv.org/abs/2409.19735},
	doi = {10.48550/arXiv.2409.19735},
	abstract = {OCR errors are common in digitised historical archives significantly affecting their usability and value. Generative Language Models (LMs) have shown potential for correcting these errors using the context provided by the corrupted text and the broader socio-cultural context, a process called Context Leveraging OCR Correction (CLOCR-C). However, getting sufficient training data for fine-tuning such models can prove challenging. This paper shows that fine-tuning a language model on synthetic data using an LM and using a character level Markov corruption process can significantly improve the ability to correct OCR errors. Models trained on synthetic data reduce the character error rate by 55\% and word error rate by 32\% over the base LM and outperform models trained on real data. Key findings include; training on under-corrupted data is better than over-corrupted data; non-uniform character level corruption is better than uniform corruption; More tokens-per-observation outperforms more observations for a fixed token budget. The outputs for this paper are a set of 8 heuristics for training effective CLOCR-C models, a dataset of 11,000 synthetic 19th century newspaper articles and scrambledtext a python library for creating synthetic corrupted data.},
	language = {en},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Bourne, Jonathan},
	month = sep,
	year = {2024},
	note = {arXiv:2409.19735 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: 21 pages, 6300 words, 6 Figures, 5 tables},
	file = {Bourne - 2024 - Scrambled text training Language Models to correc.pdf:/home/jonno/Zotero/storage/7Z7ISLY2/Bourne - 2024 - Scrambled text training Language Models to correc.pdf:application/pdf},
}

@misc{openai_gpt-4_2024,
	title = {{GPT}-4 {Technical} {Report}},
	url = {http://arxiv.org/abs/2303.08774},
	doi = {10.48550/arXiv.2303.08774},
	abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformer-based model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {OpenAI and Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and Avila, Red and Babuschkin, Igor and Balaji, Suchir and Balcom, Valerie and Baltescu, Paul and Bao, Haiming and Bavarian, Mohammad and Belgum, Jeff and Bello, Irwan and Berdine, Jake and Bernadett-Shapiro, Gabriel and Berner, Christopher and Bogdonoff, Lenny and Boiko, Oleg and Boyd, Madelaine and Brakman, Anna-Luisa and Brockman, Greg and Brooks, Tim and Brundage, Miles and Button, Kevin and Cai, Trevor and Campbell, Rosie and Cann, Andrew and Carey, Brittany and Carlson, Chelsea and Carmichael, Rory and Chan, Brooke and Chang, Che and Chantzis, Fotis and Chen, Derek and Chen, Sully and Chen, Ruby and Chen, Jason and Chen, Mark and Chess, Ben and Cho, Chester and Chu, Casey and Chung, Hyung Won and Cummings, Dave and Currier, Jeremiah and Dai, Yunxing and Decareaux, Cory and Degry, Thomas and Deutsch, Noah and Deville, Damien and Dhar, Arka and Dohan, David and Dowling, Steve and Dunning, Sheila and Ecoffet, Adrien and Eleti, Atty and Eloundou, Tyna and Farhi, David and Fedus, Liam and Felix, Niko and Fishman, Simón Posada and Forte, Juston and Fulford, Isabella and Gao, Leo and Georges, Elie and Gibson, Christian and Goel, Vik and Gogineni, Tarun and Goh, Gabriel and Gontijo-Lopes, Rapha and Gordon, Jonathan and Grafstein, Morgan and Gray, Scott and Greene, Ryan and Gross, Joshua and Gu, Shixiang Shane and Guo, Yufei and Hallacy, Chris and Han, Jesse and Harris, Jeff and He, Yuchen and Heaton, Mike and Heidecke, Johannes and Hesse, Chris and Hickey, Alan and Hickey, Wade and Hoeschele, Peter and Houghton, Brandon and Hsu, Kenny and Hu, Shengli and Hu, Xin and Huizinga, Joost and Jain, Shantanu and Jain, Shawn and Jang, Joanne and Jiang, Angela and Jiang, Roger and Jin, Haozhun and Jin, Denny and Jomoto, Shino and Jonn, Billie and Jun, Heewoo and Kaftan, Tomer and Kaiser, Łukasz and Kamali, Ali and Kanitscheider, Ingmar and Keskar, Nitish Shirish and Khan, Tabarak and Kilpatrick, Logan and Kim, Jong Wook and Kim, Christina and Kim, Yongjik and Kirchner, Jan Hendrik and Kiros, Jamie and Knight, Matt and Kokotajlo, Daniel and Kondraciuk, Łukasz and Kondrich, Andrew and Konstantinidis, Aris and Kosic, Kyle and Krueger, Gretchen and Kuo, Vishal and Lampe, Michael and Lan, Ikai and Lee, Teddy and Leike, Jan and Leung, Jade and Levy, Daniel and Li, Chak Ming and Lim, Rachel and Lin, Molly and Lin, Stephanie and Litwin, Mateusz and Lopez, Theresa and Lowe, Ryan and Lue, Patricia and Makanju, Anna and Malfacini, Kim and Manning, Sam and Markov, Todor and Markovski, Yaniv and Martin, Bianca and Mayer, Katie and Mayne, Andrew and McGrew, Bob and McKinney, Scott Mayer and McLeavey, Christine and McMillan, Paul and McNeil, Jake and Medina, David and Mehta, Aalok and Menick, Jacob and Metz, Luke and Mishchenko, Andrey and Mishkin, Pamela and Monaco, Vinnie and Morikawa, Evan and Mossing, Daniel and Mu, Tong and Murati, Mira and Murk, Oleg and Mély, David and Nair, Ashvin and Nakano, Reiichiro and Nayak, Rajeev and Neelakantan, Arvind and Ngo, Richard and Noh, Hyeonwoo and Ouyang, Long and O'Keefe, Cullen and Pachocki, Jakub and Paino, Alex and Palermo, Joe and Pantuliano, Ashley and Parascandolo, Giambattista and Parish, Joel and Parparita, Emy and Passos, Alex and Pavlov, Mikhail and Peng, Andrew and Perelman, Adam and Peres, Filipe de Avila Belbute and Petrov, Michael and Pinto, Henrique Ponde de Oliveira and Michael and Pokorny and Pokrass, Michelle and Pong, Vitchyr H. and Powell, Tolly and Power, Alethea and Power, Boris and Proehl, Elizabeth and Puri, Raul and Radford, Alec and Rae, Jack and Ramesh, Aditya and Raymond, Cameron and Real, Francis and Rimbach, Kendra and Ross, Carl and Rotsted, Bob and Roussez, Henri and Ryder, Nick and Saltarelli, Mario and Sanders, Ted and Santurkar, Shibani and Sastry, Girish and Schmidt, Heather and Schnurr, David and Schulman, John and Selsam, Daniel and Sheppard, Kyla and Sherbakov, Toki and Shieh, Jessica and Shoker, Sarah and Shyam, Pranav and Sidor, Szymon and Sigler, Eric and Simens, Maddie and Sitkin, Jordan and Slama, Katarina and Sohl, Ian and Sokolowsky, Benjamin and Song, Yang and Staudacher, Natalie and Such, Felipe Petroski and Summers, Natalie and Sutskever, Ilya and Tang, Jie and Tezak, Nikolas and Thompson, Madeleine B. and Tillet, Phil and Tootoonchian, Amin and Tseng, Elizabeth and Tuggle, Preston and Turley, Nick and Tworek, Jerry and Uribe, Juan Felipe Cerón and Vallone, Andrea and Vijayvergiya, Arun and Voss, Chelsea and Wainwright, Carroll and Wang, Justin Jay and Wang, Alvin and Wang, Ben and Ward, Jonathan and Wei, Jason and Weinmann, C. J. and Welihinda, Akila and Welinder, Peter and Weng, Jiayi and Weng, Lilian and Wiethoff, Matt and Willner, Dave and Winter, Clemens and Wolrich, Samuel and Wong, Hannah and Workman, Lauren and Wu, Sherwin and Wu, Jeff and Wu, Michael and Xiao, Kai and Xu, Tao and Yoo, Sarah and Yu, Kevin and Yuan, Qiming and Zaremba, Wojciech and Zellers, Rowan and Zhang, Chong and Zhang, Marvin and Zhao, Shengjia and Zheng, Tianhao and Zhuang, Juntang and Zhuk, William and Zoph, Barret},
	month = mar,
	year = {2024},
	note = {arXiv:2303.08774 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 100 pages; updated authors list; fixed author names and added citation},
	file = {Preprint PDF:/home/jonno/Zotero/storage/5R64W2VR/OpenAI et al. - 2024 - GPT-4 Technical Report.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/FD22MU4P/2303.html:text/html},
}

@misc{team_gemini_2024,
	title = {Gemini 1.5: {Unlocking} multimodal understanding across millions of tokens of context},
	shorttitle = {Gemini 1.5},
	url = {http://arxiv.org/abs/2403.05530},
	doi = {10.48550/arXiv.2403.05530},
	abstract = {In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval ({\textgreater}99\%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75\% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and Mariooryad, Soroosh and Ding, Yifan and Geng, Xinyang and Alcober, Fred and Frostig, Roy and Omernick, Mark and Walker, Lexi and Paduraru, Cosmin and Sorokin, Christina and Tacchetti, Andrea and Gaffney, Colin and Daruki, Samira and Sercinoglu, Olcan and Gleicher, Zach and Love, Juliette and Voigtlaender, Paul and Jain, Rohan and Surita, Gabriela and Mohamed, Kareem and Blevins, Rory and Ahn, Junwhan and Zhu, Tao and Kawintiranon, Kornraphop and Firat, Orhan and Gu, Yiming and Zhang, Yujing and Rahtz, Matthew and Faruqui, Manaal and Clay, Natalie and Gilmer, Justin and Co-Reyes, J. D. and Penchev, Ivo and Zhu, Rui and Morioka, Nobuyuki and Hui, Kevin and Haridasan, Krishna and Campos, Victor and Mahdieh, Mahdis and Guo, Mandy and Hassan, Samer and Kilgour, Kevin and Vezer, Arpi and Cheng, Heng-Tze and Liedekerke, Raoul de and Goyal, Siddharth and Barham, Paul and Strouse, D. J. and Noury, Seb and Adler, Jonas and Sundararajan, Mukund and Vikram, Sharad and Lepikhin, Dmitry and Paganini, Michela and Garcia, Xavier and Yang, Fan and Valter, Dasha and Trebacz, Maja and Vodrahalli, Kiran and Asawaroengchai, Chulayuth and Ring, Roman and Kalb, Norbert and Soares, Livio Baldini and Brahma, Siddhartha and Steiner, David and Yu, Tianhe and Mentzer, Fabian and He, Antoine and Gonzalez, Lucas and Xu, Bibo and Kaufman, Raphael Lopez and Shafey, Laurent El and Oh, Junhyuk and Hennigan, Tom and Driessche, George van den and Odoom, Seth and Lucic, Mario and Roelofs, Becca and Lall, Sid and Marathe, Amit and Chan, Betty and Ontanon, Santiago and He, Luheng and Teplyashin, Denis and Lai, Jonathan and Crone, Phil and Damoc, Bogdan and Ho, Lewis and Riedel, Sebastian and Lenc, Karel and Yeh, Chih-Kuan and Chowdhery, Aakanksha and Xu, Yang and Kazemi, Mehran and Amid, Ehsan and Petrushkina, Anastasia and Swersky, Kevin and Khodaei, Ali and Chen, Gowoon and Larkin, Chris and Pinto, Mario and Yan, Geng and Badia, Adria Puigdomenech and Patil, Piyush and Hansen, Steven and Orr, Dave and Arnold, Sebastien M. R. and Grimstad, Jordan and Dai, Andrew and Douglas, Sholto and Sinha, Rishika and Yadav, Vikas and Chen, Xi and Gribovskaya, Elena and Austin, Jacob and Zhao, Jeffrey and Patel, Kaushal and Komarek, Paul and Austin, Sophia and Borgeaud, Sebastian and Friso, Linda and Goyal, Abhimanyu and Caine, Ben and Cao, Kris and Chung, Da-Woon and Lamm, Matthew and Barth-Maron, Gabe and Kagohara, Thais and Olszewska, Kate and Chen, Mia and Shivakumar, Kaushik and Agarwal, Rishabh and Godhia, Harshal and Rajwar, Ravi and Snaider, Javier and Dotiwalla, Xerxes and Liu, Yuan and Barua, Aditya and Ungureanu, Victor and Zhang, Yuan and Batsaikhan, Bat-Orgil and Wirth, Mateo and Qin, James and Danihelka, Ivo and Doshi, Tulsee and Chadwick, Martin and Chen, Jilin and Jain, Sanil and Le, Quoc and Kar, Arjun and Gurumurthy, Madhu and Li, Cheng and Sang, Ruoxin and Liu, Fangyu and Lamprou, Lampros and Munoz, Rich and Lintz, Nathan and Mehta, Harsh and Howard, Heidi and Reynolds, Malcolm and Aroyo, Lora and Wang, Quan and Blanco, Lorenzo and Cassirer, Albin and Griffith, Jordan and Das, Dipanjan and Lee, Stephan and Sygnowski, Jakub and Fisher, Zach and Besley, James and Powell, Richard and Ahmed, Zafarali and Paulus, Dominik and Reitter, David and Borsos, Zalan and Joshi, Rishabh and Pope, Aedan and Hand, Steven and Selo, Vittorio and Jain, Vihan and Sethi, Nikhil and Goel, Megha and Makino, Takaki and May, Rhys and Yang, Zhen and Schalkwyk, Johan and Butterfield, Christina and Hauth, Anja and Goldin, Alex and Hawkins, Will and Senter, Evan and Brin, Sergey and Woodman, Oliver and Ritter, Marvin and Noland, Eric and Giang, Minh and Bolina, Vijay and Lee, Lisa and Blyth, Tim and Mackinnon, Ian and Reid, Machel and Sarvana, Obaid and Silver, David and Chen, Alexander and Wang, Lily and Maggiore, Loren and Chang, Oscar and Attaluri, Nithya and Thornton, Gregory and Chiu, Chung-Cheng and Bunyan, Oskar and Levine, Nir and Chung, Timothy and Eltyshev, Evgenii and Si, Xiance and Lillicrap, Timothy and Brady, Demetra and Aggarwal, Vaibhav and Wu, Boxi and Xu, Yuanzhong and McIlroy, Ross and Badola, Kartikeya and Sandhu, Paramjit and Moreira, Erica and Stokowiec, Wojciech and Hemsley, Ross and Li, Dong and Tudor, Alex and Shyam, Pranav and Rahimtoroghi, Elahe and Haykal, Salem and Sprechmann, Pablo and Zhou, Xiang and Mincu, Diana and Li, Yujia and Addanki, Ravi and Krishna, Kalpesh and Wu, Xiao and Frechette, Alexandre and Eyal, Matan and Dafoe, Allan and Lacey, Dave and Whang, Jay and Avrahami, Thi and Zhang, Ye and Taropa, Emanuel and Lin, Hanzhao and Toyama, Daniel and Rutherford, Eliza and Sano, Motoki and Choe, HyunJeong and Tomala, Alex and Safranek-Shrader, Chalence and Kassner, Nora and Pajarskas, Mantas and Harvey, Matt and Sechrist, Sean and Fortunato, Meire and Lyu, Christina and Elsayed, Gamaleldin and Kuang, Chenkai and Lottes, James and Chu, Eric and Jia, Chao and Chen, Chih-Wei and Humphreys, Peter and Baumli, Kate and Tao, Connie and Samuel, Rajkumar and Santos, Cicero Nogueira dos and Andreassen, Anders and Rakićević, Nemanja and Grewe, Dominik and Kumar, Aviral and Winkler, Stephanie and Caton, Jonathan and Brock, Andrew and Dalmia, Sid and Sheahan, Hannah and Barr, Iain and Miao, Yingjie and Natsev, Paul and Devlin, Jacob and Behbahani, Feryal and Prost, Flavien and Sun, Yanhua and Myaskovsky, Artiom and Pillai, Thanumalayan Sankaranarayana and Hurt, Dan and Lazaridou, Angeliki and Xiong, Xi and Zheng, Ce and Pardo, Fabio and Li, Xiaowei and Horgan, Dan and Stanton, Joe and Ambar, Moran and Xia, Fei and Lince, Alejandro and Wang, Mingqiu and Mustafa, Basil and Webson, Albert and Lee, Hyo and Anil, Rohan and Wicke, Martin and Dozat, Timothy and Sinha, Abhishek and Piqueras, Enrique and Dabir, Elahe and Upadhyay, Shyam and Boral, Anudhyan and Hendricks, Lisa Anne and Fry, Corey and Djolonga, Josip and Su, Yi and Walker, Jake and Labanowski, Jane and Huang, Ronny and Misra, Vedant and Chen, Jeremy and Skerry-Ryan, R. J. and Singh, Avi and Rijhwani, Shruti and Yu, Dian and Castro-Ros, Alex and Changpinyo, Beer and Datta, Romina and Bagri, Sumit and Hrafnkelsson, Arnar Mar and Maggioni, Marcello and Zheng, Daniel and Sulsky, Yury and Hou, Shaobo and Paine, Tom Le and Yang, Antoine and Riesa, Jason and Rogozinska, Dominika and Marcus, Dror and Badawy, Dalia El and Zhang, Qiao and Wang, Luyu and Miller, Helen and Greer, Jeremy and Sjos, Lars Lowe and Nova, Azade and Zen, Heiga and Chaabouni, Rahma and Rosca, Mihaela and Jiang, Jiepu and Chen, Charlie and Liu, Ruibo and Sainath, Tara and Krikun, Maxim and Polozov, Alex and Lespiau, Jean-Baptiste and Newlan, Josh and Cankara, Zeyncep and Kwak, Soo and Xu, Yunhan and Chen, Phil and Coenen, Andy and Meyer, Clemens and Tsihlas, Katerina and Ma, Ada and Gottweis, Juraj and Xing, Jinwei and Gu, Chenjie and Miao, Jin and Frank, Christian and Cankara, Zeynep and Ganapathy, Sanjay and Dasgupta, Ishita and Hughes-Fitt, Steph and Chen, Heng and Reid, David and Rong, Keran and Fan, Hongmin and Amersfoort, Joost van and Zhuang, Vincent and Cohen, Aaron and Gu, Shixiang Shane and Mohananey, Anhad and Ilic, Anastasija and Tobin, Taylor and Wieting, John and Bortsova, Anna and Thacker, Phoebe and Wang, Emma and Caveness, Emily and Chiu, Justin and Sezener, Eren and Kaskasoli, Alex and Baker, Steven and Millican, Katie and Elhawaty, Mohamed and Aisopos, Kostas and Lebsack, Carl and Byrd, Nathan and Dai, Hanjun and Jia, Wenhao and Wiethoff, Matthew and Davoodi, Elnaz and Weston, Albert and Yagati, Lakshman and Ahuja, Arun and Gao, Isabel and Pundak, Golan and Zhang, Susan and Azzam, Michael and Sim, Khe Chai and Caelles, Sergi and Keeling, James and Sharma, Abhanshu and Swing, Andy and Li, YaGuang and Liu, Chenxi and Bostock, Carrie Grimes and Bansal, Yamini and Nado, Zachary and Anand, Ankesh and Lipschultz, Josh and Karmarkar, Abhijit and Proleev, Lev and Ittycheriah, Abe and Yeganeh, Soheil Hassas and Polovets, George and Faust, Aleksandra and Sun, Jiao and Rrustemi, Alban and Li, Pen and Shivanna, Rakesh and Liu, Jeremiah and Welty, Chris and Lebron, Federico and Baddepudi, Anirudh and Krause, Sebastian and Parisotto, Emilio and Soricut, Radu and Xu, Zheng and Bloxwich, Dawn and Johnson, Melvin and Neyshabur, Behnam and Mao-Jones, Justin and Wang, Renshen and Ramasesh, Vinay and Abbas, Zaheer and Guez, Arthur and Segal, Constant and Nguyen, Duc Dung and Svensson, James and Hou, Le and York, Sarah and Milan, Kieran and Bridgers, Sophie and Gworek, Wiktor and Tagliasacchi, Marco and Lee-Thorp, James and Chang, Michael and Guseynov, Alexey and Hartman, Ale Jakse and Kwong, Michael and Zhao, Ruizhe and Kashem, Sheleem and Cole, Elizabeth and Miech, Antoine and Tanburn, Richard and Phuong, Mary and Pavetic, Filip and Cevey, Sebastien and Comanescu, Ramona and Ives, Richard and Yang, Sherry and Du, Cosmo and Li, Bo and Zhang, Zizhao and Iinuma, Mariko and Hu, Clara Huiyi and Roy, Aurko and Bijwadia, Shaan and Zhu, Zhenkai and Martins, Danilo and Saputro, Rachel and Gergely, Anita and Zheng, Steven and Jia, Dawei and Antonoglou, Ioannis and Sadovsky, Adam and Gu, Shane and Bi, Yingying and Andreev, Alek and Samangooei, Sina and Khan, Mina and Kocisky, Tomas and Filos, Angelos and Kumar, Chintu and Bishop, Colton and Yu, Adams and Hodkinson, Sarah and Mittal, Sid and Shah, Premal and Moufarek, Alexandre and Cheng, Yong and Bloniarz, Adam and Lee, Jaehoon and Pejman, Pedram and Michel, Paul and Spencer, Stephen and Feinberg, Vladimir and Xiong, Xuehan and Savinov, Nikolay and Smith, Charlotte and Shakeri, Siamak and Tran, Dustin and Chesus, Mary and Bohnet, Bernd and Tucker, George and Glehn, Tamara von and Muir, Carrie and Mao, Yiran and Kazawa, Hideto and Slone, Ambrose and Soparkar, Kedar and Shrivastava, Disha and Cobon-Kerr, James and Sharman, Michael and Pavagadhi, Jay and Araya, Carlos and Misiunas, Karolis and Ghelani, Nimesh and Laskin, Michael and Barker, David and Li, Qiujia and Briukhov, Anton and Houlsby, Neil and Glaese, Mia and Lakshminarayanan, Balaji and Schucher, Nathan and Tang, Yunhao and Collins, Eli and Lim, Hyeontaek and Feng, Fangxiaoyu and Recasens, Adria and Lai, Guangda and Magni, Alberto and Cao, Nicola De and Siddhant, Aditya and Ashwood, Zoe and Orbay, Jordi and Dehghani, Mostafa and Brennan, Jenny and He, Yifan and Xu, Kelvin and Gao, Yang and Saroufim, Carl and Molloy, James and Wu, Xinyi and Arnold, Seb and Chang, Solomon and Schrittwieser, Julian and Buchatskaya, Elena and Radpour, Soroush and Polacek, Martin and Giordano, Skye and Bapna, Ankur and Tokumine, Simon and Hellendoorn, Vincent and Sottiaux, Thibault and Cogan, Sarah and Severyn, Aliaksei and Saleh, Mohammad and Thakoor, Shantanu and Shefey, Laurent and Qiao, Siyuan and Gaba, Meenu and Chang, Shuo-yiin and Swanson, Craig and Zhang, Biao and Lee, Benjamin and Rubenstein, Paul Kishan and Song, Gan and Kwiatkowski, Tom and Koop, Anna and Kannan, Ajay and Kao, David and Schuh, Parker and Stjerngren, Axel and Ghiasi, Golnaz and Gibson, Gena and Vilnis, Luke and Yuan, Ye and Ferreira, Felipe Tiengo and Kamath, Aishwarya and Klimenko, Ted and Franko, Ken and Xiao, Kefan and Bhattacharya, Indro and Patel, Miteyan and Wang, Rui and Morris, Alex and Strudel, Robin and Sharma, Vivek and Choy, Peter and Hashemi, Sayed Hadi and Landon, Jessica and Finkelstein, Mara and Jhakra, Priya and Frye, Justin and Barnes, Megan and Mauger, Matthew and Daun, Dennis and Baatarsukh, Khuslen and Tung, Matthew and Farhan, Wael and Michalewski, Henryk and Viola, Fabio and Quitry, Felix de Chaumont and Lan, Charline Le and Hudson, Tom and Wang, Qingze and Fischer, Felix and Zheng, Ivy and White, Elspeth and Dragan, Anca and Alayrac, Jean-baptiste and Ni, Eric and Pritzel, Alexander and Iwanicki, Adam and Isard, Michael and Bulanova, Anna and Zilka, Lukas and Dyer, Ethan and Sachan, Devendra and Srinivasan, Srivatsan and Muckenhirn, Hannah and Cai, Honglong and Mandhane, Amol and Tariq, Mukarram and Rae, Jack W. and Wang, Gary and Ayoub, Kareem and FitzGerald, Nicholas and Zhao, Yao and Han, Woohyun and Alberti, Chris and Garrette, Dan and Krishnakumar, Kashyap and Gimenez, Mai and Levskaya, Anselm and Sohn, Daniel and Matak, Josip and Iturrate, Inaki and Chang, Michael B. and Xiang, Jackie and Cao, Yuan and Ranka, Nishant and Brown, Geoff and Hutter, Adrian and Mirrokni, Vahab and Chen, Nanxin and Yao, Kaisheng and Egyed, Zoltan and Galilee, Francois and Liechty, Tyler and Kallakuri, Praveen and Palmer, Evan and Ghemawat, Sanjay and Liu, Jasmine and Tao, David and Thornton, Chloe and Green, Tim and Jasarevic, Mimi and Lin, Sharon and Cotruta, Victor and Tan, Yi-Xuan and Fiedel, Noah and Yu, Hongkun and Chi, Ed and Neitz, Alexander and Heitkaemper, Jens and Sinha, Anu and Zhou, Denny and Sun, Yi and Kaed, Charbel and Hulse, Brice and Mishra, Swaroop and Georgaki, Maria and Kudugunta, Sneha and Farabet, Clement and Shafran, Izhak and Vlasic, Daniel and Tsitsulin, Anton and Ananthanarayanan, Rajagopal and Carin, Alen and Su, Guolong and Sun, Pei and V, Shashank and Carvajal, Gabriel and Broder, Josef and Comsa, Iulia and Repina, Alena and Wong, William and Chen, Warren Weilun and Hawkins, Peter and Filonov, Egor and Loher, Lucia and Hirnschall, Christoph and Wang, Weiyi and Ye, Jingchen and Burns, Andrea and Cate, Hardie and Wright, Diana Gage and Piccinini, Federico and Zhang, Lei and Lin, Chu-Cheng and Gog, Ionel and Kulizhskaya, Yana and Sreevatsa, Ashwin and Song, Shuang and Cobo, Luis C. and Iyer, Anand and Tekur, Chetan and Garrido, Guillermo and Xiao, Zhuyun and Kemp, Rupert and Zheng, Huaixiu Steven and Li, Hui and Agarwal, Ananth and Ngani, Christel and Goshvadi, Kati and Santamaria-Fernandez, Rebeca and Fica, Wojciech and Chen, Xinyun and Gorgolewski, Chris and Sun, Sean and Garg, Roopal and Ye, Xinyu and Eslami, S. M. Ali and Hua, Nan and Simon, Jon and Joshi, Pratik and Kim, Yelin and Tenney, Ian and Potluri, Sahitya and Thiet, Lam Nguyen and Yuan, Quan and Luisier, Florian and Chronopoulou, Alexandra and Scellato, Salvatore and Srinivasan, Praveen and Chen, Minmin and Koverkathu, Vinod and Dalibard, Valentin and Xu, Yaming and Saeta, Brennan and Anderson, Keith and Sellam, Thibault and Fernando, Nick and Huot, Fantine and Jung, Junehyuk and Varadarajan, Mani and Quinn, Michael and Raul, Amit and Le, Maigo and Habalov, Ruslan and Clark, Jon and Jalan, Komal and Bullard, Kalesha and Singhal, Achintya and Luong, Thang and Wang, Boyu and Rajayogam, Sujeevan and Eisenschlos, Julian and Jia, Johnson and Finchelstein, Daniel and Yakubovich, Alex and Balle, Daniel and Fink, Michael and Agarwal, Sameer and Li, Jing and Dvijotham, Dj and Pal, Shalini and Kang, Kai and Konzelmann, Jaclyn and Beattie, Jennifer and Dousse, Olivier and Wu, Diane and Crocker, Remi and Elkind, Chen and Jonnalagadda, Siddhartha Reddy and Lee, Jong and Holtmann-Rice, Dan and Kallarackal, Krystal and Liu, Rosanne and Vnukov, Denis and Vats, Neera and Invernizzi, Luca and Jafari, Mohsen and Zhou, Huanjie and Taylor, Lilly and Prendki, Jennifer and Wu, Marcus and Eccles, Tom and Liu, Tianqi and Kopparapu, Kavya and Beaufays, Francoise and Angermueller, Christof and Marzoca, Andreea and Sarcar, Shourya and Dib, Hilal and Stanway, Jeff and Perbet, Frank and Trdin, Nejc and Sterneck, Rachel and Khorlin, Andrey and Li, Dinghua and Wu, Xihui and Goenka, Sonam and Madras, David and Goldshtein, Sasha and Gierke, Willi and Zhou, Tong and Liu, Yaxin and Liang, Yannie and White, Anais and Li, Yunjie and Singh, Shreya and Bahargam, Sanaz and Epstein, Mark and Basu, Sujoy and Lao, Li and Ozturel, Adnan and Crous, Carl and Zhai, Alex and Lu, Han and Tung, Zora and Gaur, Neeraj and Walton, Alanna and Dixon, Lucas and Zhang, Ming and Globerson, Amir and Uy, Grant and Bolt, Andrew and Wiles, Olivia and Nasr, Milad and Shumailov, Ilia and Selvi, Marco and Piccinno, Francesco and Aguilar, Ricardo and McCarthy, Sara and Khalman, Misha and Shukla, Mrinal and Galic, Vlado and Carpenter, John and Villela, Kevin and Zhang, Haibin and Richardson, Harry and Martens, James and Bosnjak, Matko and Belle, Shreyas Rammohan and Seibert, Jeff and Alnahlawi, Mahmoud and McWilliams, Brian and Singh, Sankalp and Louis, Annie and Ding, Wen and Popovici, Dan and Simicich, Lenin and Knight, Laura and Mehta, Pulkit and Gupta, Nishesh and Shi, Chongyang and Fatehi, Saaber and Mitrovic, Jovana and Grills, Alex and Pagadora, Joseph and Munkhdalai, Tsendsuren and Petrova, Dessie and Eisenbud, Danielle and Zhang, Zhishuai and Yates, Damion and Mittal, Bhavishya and Tripuraneni, Nilesh and Assael, Yannis and Brovelli, Thomas and Jain, Prateek and Velimirovic, Mihajlo and Akbulut, Canfer and Mu, Jiaqi and Macherey, Wolfgang and Kumar, Ravin and Xu, Jun and Qureshi, Haroon and Comanici, Gheorghe and Wiesner, Jeremy and Gong, Zhitao and Ruddock, Anton and Bauer, Matthias and Felt, Nick and GP, Anirudh and Arnab, Anurag and Zelle, Dustin and Rothfuss, Jonas and Rosgen, Bill and Shenoy, Ashish and Seybold, Bryan and Li, Xinjian and Mudigonda, Jayaram and Erdogan, Goker and Xia, Jiawei and Simsa, Jiri and Michi, Andrea and Yao, Yi and Yew, Christopher and Kan, Steven and Caswell, Isaac and Radebaugh, Carey and Elisseeff, Andre and Valenzuela, Pedro and McKinney, Kay and Paterson, Kim and Cui, Albert and Latorre-Chimoto, Eri and Kim, Solomon and Zeng, William and Durden, Ken and Ponnapalli, Priya and Sosea, Tiberiu and Choquette-Choo, Christopher A. and Manyika, James and Robenek, Brona and Vashisht, Harsha and Pereira, Sebastien and Lam, Hoi and Velic, Marko and Owusu-Afriyie, Denese and Lee, Katherine and Bolukbasi, Tolga and Parrish, Alicia and Lu, Shawn and Park, Jane and Venkatraman, Balaji and Talbert, Alice and Rosique, Lambert and Cheng, Yuchung and Sozanschi, Andrei and Paszke, Adam and Kumar, Praveen and Austin, Jessica and Li, Lu and Salama, Khalid and Perz, Bartek and Kim, Wooyeol and Dukkipati, Nandita and Baryshnikov, Anthony and Kaplanis, Christos and Sheng, XiangHai and Chervonyi, Yuri and Unlu, Caglar and Casas, Diego de Las and Askham, Harry and Tunyasuvunakool, Kathryn and Gimeno, Felix and Poder, Siim and Kwak, Chester and Miecnikowski, Matt and Mirrokni, Vahab and Dimitriev, Alek and Parisi, Aaron and Liu, Dangyi and Tsai, Tomy and Shevlane, Toby and Kouridi, Christina and Garmon, Drew and Goedeckemeyer, Adrian and Brown, Adam R. and Vijayakumar, Anitha and Elqursh, Ali and Jazayeri, Sadegh and Huang, Jin and Carthy, Sara Mc and Hoover, Jay and Kim, Lucy and Kumar, Sandeep and Chen, Wei and Biles, Courtney and Bingham, Garrett and Rosen, Evan and Wang, Lisa and Tan, Qijun and Engel, David and Pongetti, Francesco and Cesare, Dario de and Hwang, Dongseong and Yu, Lily and Pullman, Jennifer and Narayanan, Srini and Levin, Kyle and Gopal, Siddharth and Li, Megan and Aharoni, Asaf and Trinh, Trieu and Lo, Jessica and Casagrande, Norman and Vij, Roopali and Matthey, Loic and Ramadhana, Bramandia and Matthews, Austin and Carey, C. J. and Johnson, Matthew and Goranova, Kremena and Shah, Rohin and Ashraf, Shereen and Dasgupta, Kingshuk and Larsen, Rasmus and Wang, Yicheng and Vuyyuru, Manish Reddy and Jiang, Chong and Ijazi, Joana and Osawa, Kazuki and Smith, Celine and Boppana, Ramya Sree and Bilal, Taylan and Koizumi, Yuma and Xu, Ying and Altun, Yasemin and Shabat, Nir and Bariach, Ben and Korchemniy, Alex and Choo, Kiam and Ronneberger, Olaf and Iwuanyanwu, Chimezie and Zhao, Shubin and Soergel, David and Hsieh, Cho-Jui and Cai, Irene and Iqbal, Shariq and Sundermeyer, Martin and Chen, Zhe and Bursztein, Elie and Malaviya, Chaitanya and Biadsy, Fadi and Shroff, Prakash and Dhillon, Inderjit and Latkar, Tejasi and Dyer, Chris and Forbes, Hannah and Nicosia, Massimo and Nikolaev, Vitaly and Greene, Somer and Georgiev, Marin and Wang, Pidong and Martin, Nina and Sedghi, Hanie and Zhang, John and Banzal, Praseem and Fritz, Doug and Rao, Vikram and Wang, Xuezhi and Zhang, Jiageng and Patraucean, Viorica and Du, Dayou and Mordatch, Igor and Jurin, Ivan and Liu, Lewis and Dubey, Ayush and Mohan, Abhi and Nowakowski, Janek and Ion, Vlad-Doru and Wei, Nan and Tojo, Reiko and Raad, Maria Abi and Hudson, Drew A. and Keshava, Vaishakh and Agrawal, Shubham and Ramirez, Kevin and Wu, Zhichun and Nguyen, Hoang and Liu, Ji and Sewak, Madhavi and Petrini, Bryce and Choi, DongHyun and Philips, Ivan and Wang, Ziyue and Bica, Ioana and Garg, Ankush and Wilkiewicz, Jarek and Agrawal, Priyanka and Li, Xiaowei and Guo, Danhao and Xue, Emily and Shaik, Naseer and Leach, Andrew and Khan, Sadh MNM and Wiesinger, Julia and Jerome, Sammy and Chakladar, Abhishek and Wang, Alek Wenjiao and Ornduff, Tina and Abu, Folake and Ghaffarkhah, Alireza and Wainwright, Marcus and Cortes, Mario and Liu, Frederick and Maynez, Joshua and Terzis, Andreas and Samangouei, Pouya and Mansour, Riham and Kępa, Tomasz and Aubet, François-Xavier and Algymr, Anton and Banica, Dan and Weisz, Agoston and Orban, Andras and Senges, Alexandre and Andrejczuk, Ewa and Geller, Mark and Santo, Niccolo Dal and Anklin, Valentin and Merey, Majd Al and Baeuml, Martin and Strohman, Trevor and Bai, Junwen and Petrov, Slav and Wu, Yonghui and Hassabis, Demis and Kavukcuoglu, Koray and Dean, Jeff and Vinyals, Oriol},
	month = dec,
	year = {2024},
	note = {arXiv:2403.05530 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jonno/Zotero/storage/47N9QD4V/Team et al. - 2024 - Gemini 1.5 Unlocking multimodal understanding acr.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/A4MR67N6/2403.html:text/html},
}

@misc{qwen_qwen25_2025,
	title = {Qwen2.5 {Technical} {Report}},
	url = {http://arxiv.org/abs/2412.15115},
	doi = {10.48550/arXiv.2412.15115},
	abstract = {In this report, we introduce Qwen2.5, a comprehensive series of large language models (LLMs) designed to meet diverse needs. Compared to previous iterations, Qwen 2.5 has been significantly improved during both the pre-training and post-training stages. In terms of pre-training, we have scaled the high-quality pre-training datasets from the previous 7 trillion tokens to 18 trillion tokens. This provides a strong foundation for common sense, expert knowledge, and reasoning capabilities. In terms of post-training, we implement intricate supervised finetuning with over 1 million samples, as well as multistage reinforcement learning. Post-training techniques enhance human preference, and notably improve long text generation, structural data analysis, and instruction following. To handle diverse and varied use cases effectively, we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base and instruction-tuned models, with quantized versions available. In addition, for hosted solutions, the proprietary models currently include two mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier performance on a wide range of benchmarks evaluating language understanding, reasoning, mathematics, coding, human preference alignment, etc. Specifically, the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and proprietary models and demonstrates competitive performance to the state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5 times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness while performing competitively against GPT-4o-mini and GPT-4o respectively. Additionally, as the foundation, Qwen2.5 models have been instrumental in training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and multimodal models.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Qwen and Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and Lin, Huan and Yang, Jian and Tu, Jianhong and Zhang, Jianwei and Yang, Jianxin and Yang, Jiaxi and Zhou, Jingren and Lin, Junyang and Dang, Kai and Lu, Keming and Bao, Keqin and Yang, Kexin and Yu, Le and Li, Mei and Xue, Mingfeng and Zhang, Pei and Zhu, Qin and Men, Rui and Lin, Runji and Li, Tianhao and Tang, Tianyi and Xia, Tingyu and Ren, Xingzhang and Ren, Xuancheng and Fan, Yang and Su, Yang and Zhang, Yichang and Wan, Yu and Liu, Yuqiong and Cui, Zeyu and Zhang, Zhenru and Qiu, Zihan},
	month = jan,
	year = {2025},
	note = {arXiv:2412.15115 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jonno/Zotero/storage/D579HDRA/Qwen et al. - 2025 - Qwen2.5 Technical Report.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/QCQZM2IB/2412.html:text/html},
}

@misc{faysse_colpali_2024,
	title = {{ColPali}: {Efficient} {Document} {Retrieval} with {Vision} {Language} {Models}},
	shorttitle = {{ColPali}},
	url = {http://arxiv.org/abs/2407.01449},
	doi = {10.48550/arXiv.2407.01449},
	abstract = {Documents are visually rich structures that convey information through text, as well as tables, figures, page layouts, or fonts. While modern document retrieval systems exhibit strong performance on query-to-text matching, they struggle to exploit visual cues efficiently, hindering their performance on practical document retrieval applications such as Retrieval Augmented Generation. To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieving tasks spanning multiple domains, languages, and settings. The inherent shortcomings of modern systems motivate the introduction of a new retrieval model architecture, ColPali, which leverages the document understanding capabilities of recent Vision Language Models to produce high-quality contextualized embeddings solely from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically faster and end-to-end trainable.},
	urldate = {2025-01-30},
	publisher = {arXiv},
	author = {Faysse, Manuel and Sibille, Hugues and Wu, Tony and Omrani, Bilel and Viaud, Gautier and Hudelot, Céline and Colombo, Pierre},
	month = oct,
	year = {2024},
	note = {arXiv:2407.01449 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: Under Review},
	file = {Preprint PDF:/home/jonno/Zotero/storage/UEFVAPIP/Faysse et al. - 2024 - ColPali Efficient Document Retrieval with Vision .pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/SAYQBFGR/2407.html:text/html},
}

@article{legge_does_2011,
	title = {Does print size matter for reading? {A} review of findings from vision science and typography},
	volume = {11},
	issn = {1534-7362},
	shorttitle = {Does print size matter for reading?},
	url = {https://doi.org/10.1167/11.5.8},
	doi = {10.1167/11.5.8},
	abstract = {The size and shape of printed symbols determine the legibility of text. In this paper, we focus on print size because of its crucial role in understanding reading performance and its significance in the history and contemporary practice of typography. We present evidence supporting the hypothesis that the distribution of print sizes in historical and contemporary publications falls within the psychophysically defined range of fluent print size—the range over which text can be read at maximum speed. The fluent range extends over a factor of 10 in angular print size (x-height) from approximately 0.2° to 2°. Assuming a standard reading distance of 40 cm (16 inches), the corresponding physical x-heights are 1.4 mm (4 points) and 14 mm (40 points). We provide new data on the distributions of print sizes in published books and newspapers and in typefounders' specimens, and consider factors influencing these distributions. We discuss theoretical concepts from vision science concerning visual size coding that help inform our understanding of historical and modern typographical practices. While economic, social, technological, and artistic factors influence type design and selection, we conclude that properties of human visual processing play a dominant role in constraining the distribution of print sizes in common use.},
	number = {5},
	urldate = {2025-01-31},
	journal = {Journal of Vision},
	author = {Legge, Gordon E. and Bigelow, Charles A.},
	month = aug,
	year = {2011},
	pages = {8},
	file = {Accepted Version:/home/jonno/Zotero/storage/NTLJA34Y/Legge and Bigelow - 2011 - Does print size matter for reading A review of fi.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/MKHNDUFE/article.html:text/html},
}

@inproceedings{brown_document_2001,
	title = {Document restoration using {3D} shape: a general deskewing algorithm for arbitrarily warped documents},
	volume = {2},
	shorttitle = {Document restoration using {3D} shape},
	url = {https://ieeexplore.ieee.org/abstract/document/937649},
	doi = {10.1109/ICCV.2001.937649},
	abstract = {We present a framework for restoring arbitrarily warped and deformed documents to their original planar shape. The impetus for this work is the need for tools and techniques to help digitally preserve and restore fragile manuscripts. Current digitization is performed under the assumption that the documents are flat, with subsequent image-processing and restoration algorithms either relying on this assumption or attempting to overcome it without shape information. Although most manuscripts were originally flat, many become deformed from damage and deterioration. Physical flattening is not possible without risking further, possibly irreversible, damage. Our framework addresses this restoration problem with two primary contributions. First, we present a working 3D digitization setup that acquires a 3D model with accurate shape-to-texture registration under multiple lighting conditions. Second, we show how the 3D model and a mass-spring particle system can be used together as a framework for digital flattening. We show that this restoration process can correct document deformations and can significantly improve subsequent document analysis.},
	urldate = {2025-02-01},
	booktitle = {Proceedings {Eighth} {IEEE} {International} {Conference} on {Computer} {Vision}. {ICCV} 2001},
	author = {Brown, M.S. and Seales, W.B.},
	month = jul,
	year = {2001},
	keywords = {Optical character recognition software, Computer science, Facsimile, Image restoration, Optical distortion, Optical materials, Optical sensors, Shape, Software libraries, Springs},
	pages = {367--374 vol.2},
	file = {Full Text PDF:/home/jonno/Zotero/storage/QE2W6HPN/Brown and Seales - 2001 - Document restoration using 3D shape a general des.pdf:application/pdf;IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/LRPGGBQJ/937649.html:text/html},
}

@incollection{ursell_feudalism_1988,
	address = {London},
	title = {From {Feudalism} to {Industrial} {Capital}},
	isbn = {978-1-349-19514-5},
	url = {https://doi.org/10.1007/978-1-349-19514-5_3},
	abstract = {Capitalism, said Marx, constitutes a radical change in the relations of interdependence. What he had in mind was, in one respect, the creation of landless labourers reliant on wages for survival and, in another, the political ascendance of those in command of productive, wealth-generating capacities. Speaking generally of the impact of capitalism on labouring people, Marx understood it as a process away from the personalised relations of subordination which characterised feudalism towards the alienation of labour power from its human source, and its commodification for sale on the market. Under the ethos of capitalist market relations, the seller of labour should logically be free both to contract (that is, to select and negotiate with the purchaser on equal terms) and to associate (so as to improve the commodity’s price by controlling its supply). According to this logic the transition from feudalism to capitalism should be a transition in constitutional-legal terms from status to contract relations.},
	language = {en},
	urldate = {2025-02-01},
	booktitle = {State, {Capital} and {Labour}: {Changing} {Patterns} of {Power} and {Dependence}},
	publisher = {Palgrave Macmillan UK},
	author = {Ursell, Gill and Blyton, Paul},
	editor = {Ursell, Gill and Blyton, Paul},
	year = {1988},
	doi = {10.1007/978-1-349-19514-5_3},
	pages = {52--76},
}

@incollection{hampton_new_2008,
	title = {New {Journalism}, {Nineteenth}-{Century}},
	isbn = {978-1-4051-8640-7},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781405186407.wbiecn012},
	abstract = {“The New Journalism,” a phrase made famous by cultural critic Matthew Arnold in 1887, refers to a wide range of changes in British → Newspaper and → magazine content and format, aimed at making print culture more accessible to working class and female readers. The controversial changes, some influenced by American practice, included formatting innovations, such as headlines, and new types of content, such as interviews, human interest stories, celebrity features, and a shifting emphasis from opinion to → news, facilitated by the emergence of Reuters and other → news agencies. Lengthier columns were replaced by paragraphs, often derisively called “snippets,” and the tone grew more personal. To its detractors (such as Arnold), the New Journalism entailed a challenge to the mid-nineteenth-century daily newspaper's authority and political seriousness. To its defenders, including such innovating editors and proprietors as T. P. O'Connor, George Newnes, and Alfred Harmsworth (subsequently Lord Northcliffe), the New Journalism represented an awareness that life was broader than parliamentary politics and the belief that press content should reflect readers' actual tastes rather than an elite's conception of readers' needs (→ Exposure to Communication Content; News Audience).},
	language = {en},
	urldate = {2025-02-02},
	booktitle = {The {International} {Encyclopedia} of {Communication}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Hampton, Mark},
	year = {2008},
	doi = {10.1002/9781405186407.wbiecn012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781405186407.wbiecn012},
	keywords = {1800–1899, history, journalism, Media History, United Kingdom of Great Britain and Northern Ireland},
	file = {Snapshot:/home/jonno/Zotero/storage/ZPIPFSHX/9781405186407.html:text/html},
}

@article{arnold_up_1887,
	title = {Up to {Easter}},
	volume = {21},
	number = {123},
	journal = {The Nineteenth Century},
	author = {Arnold, Matthew},
	month = may,
	year = {1887},
	pages = {629--643},
}

@inproceedings{smith_overview_2007,
	title = {An {Overview} of the {Tesseract} {OCR} {Engine}},
	volume = {2},
	url = {https://ieeexplore.ieee.org/document/4376991},
	doi = {10.1109/ICDAR.2007.4376991},
	abstract = {The Tesseract OCR engine, as was the HP Research Prototype in the UNLV Fourth Annual Test of OCR Accuracy, is described in a comprehensive overview. Emphasis is placed on aspects that are novel or at least unusual in an OCR engine, including in particular the line finding, features/classification methods, and the adaptive classifier.},
	urldate = {2025-02-03},
	booktitle = {Ninth {International} {Conference} on {Document} {Analysis} and {Recognition} ({ICDAR} 2007)},
	author = {Smith, R.},
	month = sep,
	year = {2007},
	note = {ISSN: 2379-2140},
	keywords = {Open source software, Optical character recognition software, Text recognition, Filters, Independent component analysis, Inspection, Pipelines, Prototypes, Search engines, Testing},
	pages = {629--633},
	file = {IEEE Xplore Abstract Record:/home/jonno/Zotero/storage/4EXBGWIZ/4376991.html:text/html;Submitted Version:/home/jonno/Zotero/storage/VLZ4AKYZ/Smith - 2007 - An Overview of the Tesseract OCR Engine.pdf:application/pdf},
}

@misc{reimers_sentence-bert_2019,
	title = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
	shorttitle = {Sentence-{BERT}},
	url = {http://arxiv.org/abs/1908.10084},
	doi = {10.48550/arXiv.1908.10084},
	abstract = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textasciitilde}65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Reimers, Nils and Gurevych, Iryna},
	month = aug,
	year = {2019},
	note = {arXiv:1908.10084 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Published at EMNLP 2019},
	file = {Preprint PDF:/home/jonno/Zotero/storage/NDEM5PAT/Reimers and Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese B.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/QCIEGNQB/1908.html:text/html},
}

@misc{aarsen_sentence-transformersstatic-retrieval-mrl-en-v1_2025,
	title = {sentence-transformers/static-retrieval-mrl-en-v1},
	url = {https://huggingface.co/sentence-transformers/static-retrieval-mrl-en-v1},
	author = {Aarsen, Tom},
	month = jan,
	year = {2025},
}

@misc{mcinnes_umap_2020,
	title = {{UMAP}: {Uniform} {Manifold} {Approximation} and {Projection} for {Dimension} {Reduction}},
	shorttitle = {{UMAP}},
	url = {http://arxiv.org/abs/1802.03426},
	doi = {10.48550/arXiv.1802.03426},
	abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {McInnes, Leland and Healy, John and Melville, James},
	month = sep,
	year = {2020},
	note = {arXiv:1802.03426 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computational Geometry},
	annote = {Comment: Reference implementation available at http://github.com/lmcinnes/umap},
	file = {Preprint PDF:/home/jonno/Zotero/storage/8DY5SGX4/McInnes et al. - 2020 - UMAP Uniform Manifold Approximation and Projectio.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/35NWHUSG/1802.html:text/html},
}

@misc{zaratiana_gliner_2023,
	title = {{GLiNER}: {Generalist} {Model} for {Named} {Entity} {Recognition} using {Bidirectional} {Transformer}},
	shorttitle = {{GLiNER}},
	url = {http://arxiv.org/abs/2311.08526},
	doi = {10.48550/arXiv.2311.08526},
	abstract = {Named Entity Recognition (NER) is essential in various Natural Language Processing (NLP) applications. Traditional NER models are effective but limited to a set of predefined entity types. In contrast, Large Language Models (LLMs) can extract arbitrary entities through natural language instructions, offering greater flexibility. However, their size and cost, particularly for those accessed via APIs like ChatGPT, make them impractical in resource-limited scenarios. In this paper, we introduce a compact NER model trained to identify any type of entity. Leveraging a bidirectional transformer encoder, our model, GLiNER, facilitates parallel entity extraction, an advantage over the slow sequential token generation of LLMs. Through comprehensive testing, GLiNER demonstrate strong performance, outperforming both ChatGPT and fine-tuned LLMs in zero-shot evaluations on various NER benchmarks.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Zaratiana, Urchade and Tomeh, Nadi and Holat, Pierre and Charnois, Thierry},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08526 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language},
	annote = {Comment: Work in progress},
	file = {Preprint PDF:/home/jonno/Zotero/storage/73YC5WCG/Zaratiana et al. - 2023 - GLiNER Generalist Model for Named Entity Recognit.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/CDWES3RK/2311.html:text/html},
}

@misc{iab_interactiveadvertisingbureautaxonomies_2025,
	title = {{InteractiveAdvertisingBureau}/{Taxonomies}},
	url = {https://github.com/InteractiveAdvertisingBureau/Taxonomies},
	abstract = {Easy access to IAB Tech Lab taxonomies, including Content, Audience and Ad Product},
	urldate = {2025-02-04},
	publisher = {IAB Tech Lab},
	author = {IAB},
	month = jan,
	year = {2025},
	note = {original-date: 2023-04-12T13:36:58Z},
}

@misc{iptc_news_2019,
	title = {News {Categories} {Taxonomy} for the {Media}},
	url = {https://iptc.org/standards/subject-codes/},
	author = {IPTC},
	year = {2019},
}

@misc{warner_smarter_2024,
	title = {Smarter, {Better}, {Faster}, {Longer}: {A} {Modern} {Bidirectional} {Encoder} for {Fast}, {Memory} {Efficient}, and {Long} {Context} {Finetuning} and {Inference}},
	shorttitle = {Smarter, {Better}, {Faster}, {Longer}},
	url = {http://arxiv.org/abs/2412.13663},
	doi = {10.48550/arXiv.2412.13663},
	abstract = {Encoder-only transformer models such as BERT offer a great performance-size tradeoff for retrieval and classification tasks with respect to larger decoder-only models. Despite being the workhorse of numerous production pipelines, there have been limited Pareto improvements to BERT since its release. In this paper, we introduce ModernBERT, bringing modern model optimizations to encoder-only models and representing a major Pareto improvement over older encoders. Trained on 2 trillion tokens with a native 8192 sequence length, ModernBERT models exhibit state-of-the-art results on a large pool of evaluations encompassing diverse classification tasks and both single and multi-vector retrieval on different domains (including code). In addition to strong downstream performance, ModernBERT is also the most speed and memory efficient encoder and is designed for inference on common GPUs.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Warner, Benjamin and Chaffin, Antoine and Clavié, Benjamin and Weller, Orion and Hallström, Oskar and Taghadouini, Said and Gallagher, Alexis and Biswas, Raja and Ladhak, Faisal and Aarsen, Tom and Cooper, Nathan and Adams, Griffin and Howard, Jeremy and Poli, Iacopo},
	month = dec,
	year = {2024},
	note = {arXiv:2412.13663 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jonno/Zotero/storage/V9XCU8PF/Warner et al. - 2024 - Smarter, Better, Faster, Longer A Modern Bidirect.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/BTH9PNYL/2412.html:text/html},
}

@misc{huang_layoutlmv3_2022-1,
	title = {{LayoutLMv3}: {Pre}-training for {Document} {AI} with {Unified} {Text} and {Image} {Masking}},
	shorttitle = {{LayoutLMv3}},
	url = {http://arxiv.org/abs/2204.08387},
	doi = {10.48550/arXiv.2204.08387},
	abstract = {Self-supervised pre-training techniques have achieved remarkable progress in Document AI. Most multimodal pre-trained models use a masked language modeling objective to learn bidirectional representations on the text modality, but they differ in pre-training objectives for the image modality. This discrepancy adds difficulty to multimodal representation learning. In this paper, we propose {\textbackslash}textbf\{LayoutLMv3\} to pre-train multimodal Transformers for Document AI with unified text and image masking. Additionally, LayoutLMv3 is pre-trained with a word-patch alignment objective to learn cross-modal alignment by predicting whether the corresponding image patch of a text word is masked. The simple unified architecture and training objectives make LayoutLMv3 a general-purpose pre-trained model for both text-centric and image-centric Document AI tasks. Experimental results show that LayoutLMv3 achieves state-of-the-art performance not only in text-centric tasks, including form understanding, receipt understanding, and document visual question answering, but also in image-centric tasks such as document image classification and document layout analysis. The code and models are publicly available at {\textbackslash}url\{https://aka.ms/layoutlmv3\}.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yutong and Wei, Furu},
	month = jul,
	year = {2022},
	note = {arXiv:2204.08387 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: ACM Multimedia 2022},
	file = {Preprint PDF:/home/jonno/Zotero/storage/G33PX88N/Huang et al. - 2022 - LayoutLMv3 Pre-training for Document AI with Unif.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/8V5FTWYE/2204.html:text/html},
}

@misc{shehzadi_bridging_2023,
	title = {Bridging the {Performance} {Gap} between {DETR} and {R}-{CNN} for {Graphical} {Object} {Detection} in {Document} {Images}},
	url = {http://arxiv.org/abs/2306.13526},
	doi = {10.48550/arXiv.2306.13526},
	abstract = {This paper takes an important step in bridging the performance gap between DETR and R-CNN for graphical object detection. Existing graphical object detection approaches have enjoyed recent enhancements in CNN-based object detection methods, achieving remarkable progress. Recently, Transformer-based detectors have considerably boosted the generic object detection performance, eliminating the need for hand-crafted features or post-processing steps such as Non-Maximum Suppression (NMS) using object queries. However, the effectiveness of such enhanced transformer-based detection algorithms has yet to be verified for the problem of graphical object detection. Essentially, inspired by the latest advancements in the DETR, we employ the existing detection transformer with few modifications for graphical object detection. We modify object queries in different ways, using points, anchor boxes and adding positive and negative noise to the anchors to boost performance. These modifications allow for better handling of objects with varying sizes and aspect ratios, more robustness to small variations in object positions and sizes, and improved image discrimination between objects and non-objects. We evaluate our approach on the four graphical datasets: PubTables, TableBank, NTable and PubLaynet. Upon integrating query modifications in the DETR, we outperform prior works and achieve new state-of-the-art results with the mAP of 96.9{\textbackslash}\%, 95.7{\textbackslash}\% and 99.3{\textbackslash}\% on TableBank, PubLaynet, PubTables, respectively. The results from extensive ablations show that transformer-based methods are more effective for document analysis analogous to other applications. We hope this study draws more attention to the research of using detection transformers in document image analysis.},
	urldate = {2025-02-04},
	publisher = {arXiv},
	author = {Shehzadi, Tahira and Hashmi, Khurram Azeem and Stricker, Didier and Liwicki, Marcus and Afzal, Muhammad Zeshan},
	month = jun,
	year = {2023},
	note = {arXiv:2306.13526 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/home/jonno/Zotero/storage/5W2R2UAR/Shehzadi et al. - 2023 - Bridging the Performance Gap between DETR and R-CN.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/5DXCLU6D/2306.html:text/html},
}

@incollection{twycross_rise_2024,
	address = {Cham},
	title = {The {Rise} of the {New} {Journalism}},
	isbn = {978-3-031-61213-8},
	url = {https://doi.org/10.1007/978-3-031-61213-8_2},
	abstract = {The 1880s marked a pivotalNew Journalism moment in the development of the popular press, with the arrival of the New Journalism paving the way for the mass-market populism of the century that followed. Although the New Journalism brought with it a greater emphasis on attractive layouts and a prioritisation of the visual, illustration was already firmly embedded within Victorian print culture, where it ranged from the cultural respectability of Punch’s comic journalism to the sensationalism of the Illustrated Police News. Around the middle of the 1880s, this tradition began to blend with the new populist ethos, in which brevity, accessibility and an appealing visual style were becoming central to the offerings of titles such as Tit-Bits and the Pall Mall Gazette. Towards the end of the decade, the political cartoon, which had hitherto been particularly aligned with comic journalism began to transition across into the evening newspaper market. Spearheaded by the work of F.C Gould for the Pall Mall Budget and the Pall Mall Gazette, this development represented a key developmental milestone in the evolutionary journey towards the daily newspaper strip.},
	language = {en},
	urldate = {2025-02-05},
	booktitle = {British {Newspaper} {Strips}: {A} {Contextual} {History}},
	publisher = {Springer International Publishing},
	author = {Twycross, Adam},
	editor = {Twycross, Adam},
	year = {2024},
	doi = {10.1007/978-3-031-61213-8_2},
	pages = {11--25},
}

@misc{labelbox_labelbox_2025,
	title = {Labelbox},
	url = {https://labelbox.com},
	author = {Labelbox},
	year = {2025},
}

@misc{liu_roberta_2019,
	title = {{RoBERTa}: {A} {Robustly} {Optimized} {BERT} {Pretraining} {Approach}},
	shorttitle = {{RoBERTa}},
	url = {http://arxiv.org/abs/1907.11692},
	doi = {10.48550/arXiv.1907.11692},
	abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
	urldate = {2025-02-07},
	publisher = {arXiv},
	author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2019},
	note = {arXiv:1907.11692 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/home/jonno/Zotero/storage/U6EPHGTY/Liu et al. - 2019 - RoBERTa A Robustly Optimized BERT Pretraining App.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/VCKAS9PU/1907.html:text/html},
}

@article{flesch_new_1948,
	title = {A new readability yardstick},
	volume = {32},
	issn = {1939-1854},
	doi = {10.1037/h0057532},
	abstract = {The author provides a revised system for determining the comprehension difficulty of written material through the use of two new formulae which measure reading ease and human interest. The following elements are used in analyzing text passages: (1) average sentence length in words; (2) average word length in syllables; (3) average percentage of "personal words"; (4) average percentage of personal sentences. A step-by-step procedure for using the formulae, and interpretative table of scores, and an analysis of passages in "Life" and "The New Yorker" are given. 20-item bibliography. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	number = {3},
	journal = {Journal of Applied Psychology},
	author = {Flesch, Rudolph},
	year = {1948},
	note = {Place: US
Publisher: American Psychological Association},
	keywords = {Commerce, Comprehension, Interests, Readability, Reading Comprehension, Sentences},
	pages = {221--233},
	file = {Snapshot:/home/jonno/Zotero/storage/2D32B4GN/doiLanding.html:text/html},
}

@book{badem_ottoman_2010,
	title = {The {Ottoman} {Crimean} {War} (1853-1856)},
	isbn = {978-90-04-19096-2},
	url = {https://library.oapen.org/handle/20.500.12657/38173},
	abstract = {This book analyzes the Crimean War from the Ottoman perspective based mainly on Ottoman and Russian primary sources, and includes an assessment of the War’s impact on the Ottoman state and Ottoman society. Readership: All those interested in the Crimean War, military history, Ottoman history, European history and Russian history.},
	language = {English},
	urldate = {2025-02-09},
	publisher = {Brill},
	author = {Badem, Candan},
	year = {2010},
	doi = {10.1163/ej.9789004182059.i-432},
	note = {Accepted: 2017-03-01 23:55:55
ISSN: 1380-6076},
	keywords = {Russia, History, Istanbul, Kars, Middle East and Islamic Studies, Ottoman Empire, Sublime Porte, thema EDItEUR::N History and Archaeology::NH History::NHG Middle Eastern history},
	file = {Full Text PDF:/home/jonno/Zotero/storage/AKF38EGC/Badem - 2010 - The Ottoman Crimean War (1853-1856).pdf:application/pdf},
}

@book{goldfrank_origins_2014,
	address = {London},
	title = {The {Origins} of the {Crimean} {War}},
	isbn = {978-1-315-83691-1},
	abstract = {The Crimean War (1853-56) between Russia, Turkey, Britain, France and the Kingdom of Sardinia was a diplomatically preventable conflict for influence over an unstable Near and Middle East. It could have broken out in any decade between Napoleon and Wilhelm II; equally, it need never have occurred. In this masterly study, based on massive archival research, David Goldfrank argues that the European diplomatic roots of the war stretch far beyond the `Eastern Question' itself, and shows how the domestic concerns of the participants contributed to the outbreak of hostilities.},
	publisher = {Routledge},
	author = {Goldfrank, David M.},
	month = jun,
	year = {2014},
	doi = {10.4324/9781315836911},
}

@article{phillips_saving_2012,
	title = {Saving civilization from empire: {Belligerency}, pacifism and the two faces of civilization during the {Second} {Opium} {War}},
	volume = {18},
	issn = {1354-0661},
	shorttitle = {Saving civilization from empire},
	url = {https://doi.org/10.1177/1354066111416020},
	doi = {10.1177/1354066111416020},
	abstract = {Conventional accounts of international society’s expansion have traditionally emphasized the role played by ‘civilizing missions’ in facilitating and legitimating European aggression and imperial expansion. Conversely, in this article, I demonstrate that the relationship between imperial violence and the rhetoric of ‘civilizing missions’ was far more contested and contingent than International Relations scholars have generally assumed. Using the parliamentary debate surrounding Britain’s involvement in the Second Opium War as a case study, I reveal that civilizational rhetoric in the 1857 ‘China debate’ was equally implicated in both anti-imperialist and imperialist agendas. Richard Cobden’s victory in the debate over Palmerston’s pro-war ministry further illustrates the political potency of appeals to civilization as a brake — however temporary — on Britain’s imperial expansion. An appreciation of civilization’s janus-faced character — as a rhetorical commonplace that at different times abetted and inhibited imperial aggression — is therefore critical if we are to comprehend the halting and arrhythmic pattern of international society’s progressive expansion under British leadership in the mid-Victorian period.},
	language = {en},
	number = {1},
	urldate = {2025-02-09},
	journal = {European Journal of International Relations},
	author = {Phillips, Andrew},
	month = mar,
	year = {2012},
	note = {Publisher: SAGE Publications Ltd},
	pages = {5--27},
}

@article{luckhurst_great_1951,
	title = {The {Great} {Exhibition} of 1851},
	volume = {99},
	issn = {0035-9114},
	url = {https://www.jstor.org/stable/41365158},
	number = {4845},
	urldate = {2025-02-09},
	journal = {Journal of the Royal Society of Arts},
	author = {Luckhurst, K. W.},
	year = {1951},
	note = {Publisher: Royal Society for the Encouragement of Arts, Manufactures and Commerce},
	pages = {413--456},
}

@book{atkinson_qualitative_2000,
	title = {Qualitative {Researching} with {Text}, {Image} and {Sound}: {A} {Practical} {Handbook} for {Social} {Research}},
	isbn = {978-0-7619-6481-0},
	shorttitle = {Qualitative {Researching} with {Text}, {Image} and {Sound}},
	abstract = {How do you collect and analyze social data in the form of texts (interviews and documents), images (photographs, film and television footage), and sounds (noise and music)? This text shows students which methods are most suitable for particular research problems and what is good practice for each method. Focusing on the pursuit of quality in social research, the authors; explore different ways of collecting and analyzing data; introduce the main analytical approaches for text, image and sound; cover computer-based analysis; and address problems in interpretation and quality criteria for qualitative research. The book has been extensively tested with prostgraduate research methods students at the London School of Economics.},
	language = {en},
	publisher = {SAGE},
	author = {Atkinson, Paul and Bauer, Martin W. and Gaskell, George},
	month = jun,
	year = {2000},
	note = {Google-Books-ID: UQewQ4FzHowC},
	keywords = {Psychology / Research \& Methodology, Reference / Research, Social Science / Anthropology / Cultural, Social Science / Research},
}

@article{wankhade_survey_2022,
	title = {A survey on sentiment analysis methods, applications, and challenges},
	volume = {55},
	issn = {1573-7462},
	url = {https://doi.org/10.1007/s10462-022-10144-1},
	doi = {10.1007/s10462-022-10144-1},
	abstract = {The rapid growth of Internet-based applications, such as social media platforms and blogs, has resulted in comments and reviews concerning day-to-day activities. Sentiment analysis is the process of gathering and analyzing people’s opinions, thoughts, and impressions regarding various topics, products, subjects, and services. People’s opinions can be beneficial to corporations, governments, and individuals for collecting information and making decisions based on opinion. However, the sentiment analysis and evaluation procedure face numerous challenges. These challenges create impediments to accurately interpreting sentiments and determining the appropriate sentiment polarity. Sentiment analysis identifies and extracts subjective information from the text using natural language processing and text mining. This article discusses a complete overview of the method for completing this task as well as the applications of sentiment analysis. Then, it evaluates, compares, and investigates the approaches used to gain a comprehensive understanding of their advantages and disadvantages. Finally, the challenges of sentiment analysis are examined in order to define future directions.},
	language = {en},
	number = {7},
	urldate = {2025-02-09},
	journal = {Artificial Intelligence Review},
	author = {Wankhade, Mayur and Rao, Annavarapu Chandra Sekhara and Kulkarni, Chaitanya},
	month = oct,
	year = {2022},
	keywords = {Machine learning, Sentiment analysis, Text analysis, Artificial Intelligence, Social media, Word embedding},
	pages = {5731--5780},
	file = {Full Text PDF:/home/jonno/Zotero/storage/J6TNPZ4R/Wankhade et al. - 2022 - A survey on sentiment analysis methods, applicatio.pdf:application/pdf},
}

@article{medhat_sentiment_2014,
	title = {Sentiment analysis algorithms and applications: {A} survey},
	volume = {5},
	issn = {2090-4479},
	shorttitle = {Sentiment analysis algorithms and applications},
	url = {https://www.sciencedirect.com/science/article/pii/S2090447914000550},
	doi = {10.1016/j.asej.2014.04.011},
	abstract = {Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA is the computational treatment of opinions, sentiments and subjectivity of text. This survey paper tackles a comprehensive overview of the last update in this field. Many recently proposed algorithms' enhancements and various SA applications are investigated and presented briefly in this survey. These articles are categorized according to their contributions in the various SA techniques. The related fields to SA (transfer learning, emotion detection, and building resources) that attracted researchers recently are discussed. The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details. The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas.},
	number = {4},
	urldate = {2025-02-09},
	journal = {Ain Shams Engineering Journal},
	author = {Medhat, Walaa and Hassan, Ahmed and Korashy, Hoda},
	month = dec,
	year = {2014},
	keywords = {Transfer learning, Sentiment analysis, Building resources, Emotion detection, Feature selection, Sentiment classification},
	pages = {1093--1113},
	file = {ScienceDirect Snapshot:/home/jonno/Zotero/storage/CSPARFZT/S2090447914000550.html:text/html},
}

@book{pilehvar_embeddings_2020,
	title = {Embeddings in {Natural} {Language} {Processing}: {Theory} and {Advances} in {Vector} {Representations} of {Meaning}},
	isbn = {978-1-63639-022-2},
	shorttitle = {Embeddings in {Natural} {Language} {Processing}},
	abstract = {Embeddings have undoubtedly been one of the most influential research areas in Natural Language Processing (NLP). Encoding information into a low-dimensional vector representation, which is easily integrable in modern machine learning models, has played a central role in the development of NLP. Embedding techniques initially focused on words, but the attention soon started to shift to other forms: from graph structures, such as knowledge bases, to other types of textual content, such as sentences and documents. This book provides a high-level synthesis of the main embedding techniques in NLP, in the broad sense. The book starts by explaining conventional word vector space models and word embeddings (e.g., Word2Vec and GloVe) and then moves to other types of embeddings, such as word sense, sentence and document, and graph embeddings. The book also provides an overview of recent developments in contextualized representations (e.g., ELMo and BERT) and explains their potential in NLP. Throughout the book, the reader can find both essential information for understanding a certain topic from scratch and a broad overview of the most successful techniques developed in the literature.},
	language = {en},
	publisher = {Morgan \& Claypool Publishers},
	author = {Pilehvar, Mohammad Taher and Camacho-Collados, Jose},
	month = nov,
	year = {2020},
	note = {Google-Books-ID: U90MEAAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Artificial Intelligence / Natural Language Processing, Language Arts \& Disciplines / Linguistics / Semantics},
}

@inproceedings{lewis_retrieval-augmented_2020,
	title = {Retrieval-{Augmented} {Generation} for {Knowledge}-{Intensive} {NLP} {Tasks}},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html},
	abstract = {Large pre-trained language models have been shown to store factual knowledge in their parameters, and achieve state-of-the-art results when fine-tuned on downstream NLP tasks. However, their ability to access and precisely manipulate knowledge is still limited, and hence on knowledge-intensive tasks, their performance lags behind task-specific architectures. Additionally, providing provenance for their decisions and updating their world knowledge remain open research problems. Pre-trained models with a differentiable access mechanism to explicit non-parametric memory can overcome this issue, but have so far been only investigated for extractive downstream tasks. We explore a general-purpose fine-tuning recipe for retrieval-augmented generation (RAG) -- models which combine pre-trained parametric and non-parametric memory for language generation. We introduce RAG models where the parametric memory is a pre-trained seq2seq model and the non-parametric memory is a dense vector index of Wikipedia, accessed with a pre-trained neural retriever. We compare two RAG formulations, one which conditions on the same retrieved passages across the whole generated sequence, the other can use different passages per token. We fine-tune and evaluate our models on a wide range of knowledge-intensive NLP tasks and set the state-of-the-art on three open domain QA tasks, outperforming parametric seq2seq models and task-specific retrieve-and-extract architectures. For language generation tasks, we find that RAG models generate more specific, diverse and factual language than a state-of-the-art parametric-only seq2seq baseline.},
	urldate = {2025-02-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and Küttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rocktäschel, Tim and Riedel, Sebastian and Kiela, Douwe},
	year = {2020},
	pages = {9459--9474},
	file = {Full Text PDF:/home/jonno/Zotero/storage/9MIPZLBS/Lewis et al. - 2020 - Retrieval-Augmented Generation for Knowledge-Inten.pdf:application/pdf},
}

@misc{turner_ncse_2024,
	title = {ncse: {The} {Monthly} {Repository}, 1806-1837},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_Monthly_Repository_1806-1837/25108519/1},
	doi = {10.18742/25108519.v1},
	abstract = {Contains data for The Monthly Repository derived from the Nineteenth-Century Serials Edition (ncse). The Monthly Repository was the foremost Unitarian monthly magazine in the early nineteenth century. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The Monthly Repository was published in ncse alongside five other publications: The Northern Star (1838-1852); The Leader (1850-1860); The English Woman’s Journal (1858-1864); The Tomahawk (1867-1870); The Publishers’ Circular (1880-1890).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/BJ3AEG3L/25108519.html:text/html},
}

@misc{turner_ncse_2024-1,
	title = {ncse: {The} {English} {Woman}'s {Journal}, 1858-1864},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_English_Woman_s_Journal_1858-1864/25109275/1},
	doi = {10.18742/25109275.v1},
	abstract = {Contains data for The English Woman’s Journal derived from the Nineteenth-Century Serials Edition (ncse). The English Woman's Journal is one of the most influential women's periodicals from the period. A miscellaneous monthly magazine, it engaged with issues ranging from rights within marriage to suitable work for women while also publishing correspondence and literary reviews. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The English Woman’s Journal was published in ncse alongside five other publications: The Monthly Repository (1806-1837); The Northern Star (1838-1852); The Leader (1850-1860); The Tomahawk (1867-1870); The Publishers’ Circular (1880-1890).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/6FP2TGQ8/25109275.html:text/html},
}

@misc{turner_ncse_2024-2,
	title = {ncse: {The} {Northern} {Star}, 1838-1852},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_Northern_Star_1838-1852/25106895/1},
	doi = {10.18742/25106895.v1},
	abstract = {Contains data for The Northern Star derived from the Nineteenth-Century Serials Edition (ncse). The Northern Star was the leading radical newspaper in the early nineteenth century and one of the most successful weekly newspapers of its time. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The Northern Star was published in ncse alongside five other publications: The Monthly Repository (1806-1837); The Leader (1850-1860); The English Woman’s Journal (1858-1864); The Tomahawk (1867-1870); The Publishers’ Circular (1880-1890).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/TDQC2U62/25106895.html:text/html},
}

@misc{turner_ncse_2024-3,
	title = {ncse: {The} {Tomahawk}, 1867-1870},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_Tomahawk_1867-1870/25109047/1},
	doi = {10.18742/25109047.v1},
	abstract = {Contains data for The Tomahawk derived from the Nineteenth-Century Serials Edition (ncse). The Tomahawk was a weekly satirical magazine known for its sharp wit. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The Tomahawk was published in ncse alongside five other publications: The Monthly Repository (1806-1837); The Northern Star (1838-1852); The Leader (1850-1860); The English Woman’s Journal (1858-1864); The Publishers’ Circular (1880-1890).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/EN38SGYC/25109047.html:text/html},
}

@misc{turner_ncse_2024-4,
	title = {ncse: {The} {Leader}, 1850-1860},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_Leader_1850-1860/25103786/1},
	doi = {10.18742/25103786.v1},
	abstract = {Contains data for The Leader derived from the Nineteenth-Century Serials Edition (ncse). The Leader was a high-minded, radical weekly that took a strong interest in political and cultural affairs. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The Leader was published in ncse alongside five other publications: The Monthly Repository (1806-1837); The Northern Star (1838-1852); The English Woman’s Journal (1858-1864); The Tomahawk (1867-1870); The Publishers’ Circular (1880-1890).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/IBHUBDNL/25103786.html:text/html},
}

@misc{turner_ncse_2024-5,
	title = {ncse: {The} {Publishers}' {Circular}, 1880-1890},
	shorttitle = {ncse},
	url = {https://kcl.figshare.com/articles/dataset/ncse_The_Publishers_Circular_1880-1890/25101957/1},
	doi = {10.18742/25101957.v1},
	abstract = {Contains data for The Publishers’ Circular derived from the Nineteenth-Century Serials Edition (ncse). The Publishers’ Circular was the leading trade journal for the publishing industry in the nineteenth century. Ncse was a free, online edition of six nineteenth-century periodicals and newspapers, developed in a collaboration between the British Library, King’s College London and Birkbeck, University of London. Ncse was first published in 2008 with a second edition in 2018. The Publishers’ Circular was published in ncse alongside five other publications: The Monthly Repository (1806-1837); The Northern Star (1838-1852); The Leader (1850-1860); The English Woman’s Journal (1858-1864); The Tomahawk (1867-1870).Within this dataset are two versions of the journal. The first contains an archive of every issue of the journal, including OCR transcripts, images of individual items, PDFs of individual issues, accompanying metadata, and an XML table of contents. The second is a simple set of folders containing PDFs of every issue of the journal. The first archive is more suited to digital re-use; the second can be downloaded and browsed more readily by non-expert users. Documentation within the dataset sets out the content in more detail},
	language = {en},
	urldate = {2025-02-10},
	publisher = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/WPCD7CLH/25101957.html:text/html},
}

@misc{turner_nineteenth-century_2024,
	title = {Nineteenth-{Century} {Serials} {Edition}: {Investigating} an {Archival} {Future}},
	shorttitle = {Nineteenth-{Century} {Serials} {Edition}},
	url = {https://kclpure.kcl.ac.uk/portal/en/projects/nineteenth-century-serials-edition-investigating-an-archival-futu/datasets/},
	language = {en},
	urldate = {2025-02-10},
	journal = {King's College London},
	author = {Turner, Mark},
	month = oct,
	year = {2024},
	file = {Snapshot:/home/jonno/Zotero/storage/TCWNRCHT/datasets.html:text/html},
}

@misc{bourne_codebase_2025,
	title = {Codebase for creating the {NCSE} v2.0 dataset},
	copyright = {MIT},
	url = {https://github.com/JonnoB/reading_the_unreadable},
	urldate = {2025-02-10},
	author = {Bourne, Jonathan},
	month = feb,
	year = {2025},
	note = {original-date: 2024-06-01T06:22:56Z},
}

@misc{bourne_ncse_2025,
	title = {{NCSE} v2.0: {A} {Dataset} of {OCR}-{Processed} 19th {Century} {English} {Newspapers}},
	doi = {https://doi.org/10.5522/04/28381610.v1},
	publisher = {University College London},
	author = {Bourne, Jonathan},
	month = feb,
	year = {2025},
}

@article{schmidt_reichsanzeiger-gt_2024,
	title = {Reichsanzeiger-{GT}: {An} {OCR} ground truth dataset based on the historical newspaper “{Deutscher} {Reichsanzeiger} und {Preußischer} {Staatsanzeiger}” ({German} {Imperial} {Gazette} and {Prussian} {Official} {Gazette}) (1819–1945)},
	volume = {54},
	issn = {2352-3409},
	shorttitle = {Reichsanzeiger-{GT}},
	url = {https://www.sciencedirect.com/science/article/pii/S2352340924002439},
	doi = {10.1016/j.dib.2024.110274},
	abstract = {Reichsanzeiger-GT is a ground truth dataset for OCR training and evaluation based on the historical German newspaper “Deutscher Reichsanzeiger und Preußischer Staatsanzeiger” (German Imperial Gazette and Prussian Official Gazette), which was published from 1819 to 1945 and printed mostly in the typeface Fraktur (Black Letter). The dataset consists of 101 newspaper pages for the years 1820–1939, that cover a wide variety of topics, page layouts (lists, tables, and advertisements) as well as different typefaces. Using the transcription software Transkribus and the open-source OCR engine Tesseract we automatically created and manually corrected layout segmentations and transcriptions for each page, resulting in 65,563 text regions, 412 table regions, 119,429 text lines and 490,679 words. By applying transcription guidelines that preserve the printing conditions, the dataset contains language and printing specific phenomena like the historical use of glyphs like long s (ſ), rotunda r (ꝛ), and historical currency symbols (M, ₰) among others. The dataset is provided in two variants in PAGE XML format. The first one contains ground truth data with table regions transformed to text regions for easier processing. The second variant preserves all table regions. Researchers can reuse this dataset to train new or finetune existing text recognition or layout segmentation models. The dataset can also be used to evaluate the accuracy of existing OCR models. Using specific, community driven transcription guidelines our dataset is easily interoperable and reusable with other datasets based on the same transcription level.},
	urldate = {2025-02-11},
	journal = {Data in Brief},
	author = {Schmidt, Thomas and Kamlah, Jan and Weil, Stefan},
	month = jun,
	year = {2024},
	keywords = {OCR, Text recognition, Ground truth, Historical newspapers},
	pages = {110274},
	file = {Full Text:/home/jonno/Zotero/storage/6PW8YJUP/Schmidt et al. - 2024 - Reichsanzeiger-GT An OCR ground truth dataset bas.pdf:application/pdf;ScienceDirect Snapshot:/home/jonno/Zotero/storage/BU6Y42AJ/S2352340924002439.html:text/html},
}

@misc{hogan_large_2025,
	title = {Large {Language} {Models}, {Knowledge} {Graphs} and {Search} {Engines}: {A} {Crossroads} for {Answering} {Users}' {Questions}},
	shorttitle = {Large {Language} {Models}, {Knowledge} {Graphs} and {Search} {Engines}},
	url = {http://arxiv.org/abs/2501.06699},
	doi = {10.48550/arXiv.2501.06699},
	abstract = {Much has been discussed about how Large Language Models, Knowledge Graphs and Search Engines can be combined in a synergistic manner. A dimension largely absent from current academic discourse is the user perspective. In particular, there remain many open questions regarding how best to address the diverse information needs of users, incorporating varying facets and levels of difficulty. This paper introduces a taxonomy of user information needs, which guides us to study the pros, cons and possible synergies of Large Language Models, Knowledge Graphs and Search Engines. From this study, we derive a roadmap for future research.},
	urldate = {2025-02-12},
	publisher = {arXiv},
	author = {Hogan, Aidan and Dong, Xin Luna and Vrandečić, Denny and Weikum, Gerhard},
	month = jan,
	year = {2025},
	note = {arXiv:2501.06699 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Information Retrieval, Computer Science - Symbolic Computation},
	file = {Preprint PDF:/home/jonno/Zotero/storage/59GSQMCU/Hogan et al. - 2025 - Large Language Models, Knowledge Graphs and Search.pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/NN848H29/2501.html:text/html},
}

@article{hodel_general_2021,
	title = {General {Models} for {Handwritten} {Text} {Recognition}: {Feasibility} and {State}-of-the {Art}. {German} {Kurrent} as an {Example}},
	volume = {7},
	issn = {2059-481X},
	shorttitle = {General {Models} for {Handwritten} {Text} {Recognition}},
	url = {https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.46},
	doi = {10.5334/johd.46},
	abstract = {Existing text recognition engines enables to train general models to recognize not only one specific hand but a multitude of historical hands within a particular script, and from a rather large time period (more than 100 years). This paper compares different text recognition engines and their performance on a test set independent of the training and validation sets. We argue that both, test set and ground truth, should be made available by researchers as part of a shared task to allow for the comparison of engines. This will give insight into the range of possible options for institutions in need of recognition models. As a test set, we provide a data set consisting of 2,426 lines which have been randomly selected from meeting minutes of the Swiss Federal Council from 1848 to 1903. To our knowledge, neither the aforementioned text lines, which we take as ground truth, nor the multitude of different hands within this corpus have ever been used to train handwritten text recognition models. In addition, the data set used is perfect for making comparisons involving recognition engines and large training sets due to its variability and the time frame it spans. Consequently, this paper argues that both the tested engines, HTR+ and PyLaia, can handle large training sets. The resulting models have yielded very good results on a test set consisting of unknown but stylistically similar hands.},
	language = {en-US},
	number = {0},
	urldate = {2025-02-14},
	journal = {Journal of Open Humanities Data},
	author = {Hodel, Tobias and Schoch, David and Schneider, Christa and Purcell, Jake},
	month = jul,
	year = {2021},
	file = {Full Text PDF:/home/jonno/Zotero/storage/PGMAE6TP/Hodel et al. - 2021 - General Models for Handwritten Text Recognition F.pdf:application/pdf},
}

@article{smits_fully-searchable_2025,
	title = {A {Fully}-{Searchable} {Multimodal} {Dataset} of the\&nbsp;{Illustrated} {London} {News}, 1842\&ndash;1890},
	volume = {11},
	issn = {2059-481X},
	url = {https://openhumanitiesdata.metajnl.com/articles/10.5334/johd.284},
	doi = {10.5334/johd.284},
	abstract = {This paper introduces a dataset of 72,081 wood-engraved images extracted from the Illustrated London News (ILN) from the years 1842 to 1890. In the mid-19th century, the ILN revolutionized news consumption by combining text with high-quality wood-engraved illustrations published at scale. While digitization has facilitated text-based analysis of historical periodicals, visual content remains challenging to explore systematically. We address this gap by providing a large-scale dataset of 19th-century news illustrations and their multimodal embeddings. Our methodology involved six steps: 1) Collecting 56,699 scanned ILN pages from the Internet Archive; 2) Annotating 908 pages to finetune a YOLOv8 object detection model; 3) Using the finetuned model to extract illustrations; 4) Applying an Open-CLIP model to generate multimodal embeddings; 5) Using Tesseract OCR to convert illustration captions into machine-readable text; 6) Developing a Flask application for text and image-based multimodal retrieval. The resulting dataset and application allow flexible analysis of 19th-century visual representations of news, and suggest new avenues for research in computational humanities, media history, periodical studies, and visual culture studies. By releasing the dataset, the project code, and the embeddings, our project aims to facilitate similar efforts with other historical materials, contributing to a broader understanding of visual culture. At the same time, this paper also underscores the limitations of interpreting historical imagery with modern AI models, identifying the potential effects of bias and interpretive distortion.},
	language = {en-US},
	number = {1},
	urldate = {2025-02-14},
	journal = {Journal of Open Humanities Data},
	author = {Smits, Thomas and Warner, Bethany and Fyfe, Paul and Lee, Benjamin Charles Germain},
	month = feb,
	year = {2025},
	file = {Full Text PDF:/home/jonno/Zotero/storage/Z4DC28PE/Smits et al. - 2025 - A Fully-Searchable Multimodal Dataset of the&nbsp\;.pdf:application/pdf},
}

@misc{kanerva_ocr_2025,
	title = {{OCR} {Error} {Post}-{Correction} with {LLMs} in {Historical} {Documents}: {No} {Free} {Lunches}},
	shorttitle = {{OCR} {Error} {Post}-{Correction} with {LLMs} in {Historical} {Documents}},
	url = {http://arxiv.org/abs/2502.01205},
	doi = {10.48550/arXiv.2502.01205},
	abstract = {Optical Character Recognition (OCR) systems often introduce errors when transcribing historical documents, leaving room for post-correction to improve text quality. This study evaluates the use of open-weight LLMs for OCR error correction in historical English and Finnish datasets. We explore various strategies, including parameter optimization, quantization, segment length effects, and text continuation methods. Our results demonstrate that while modern LLMs show promise in reducing character error rates (CER) in English, a practically useful performance for Finnish was not reached. Our findings highlight the potential and limitations of LLMs in scaling OCR post-correction for large historical corpora.},
	urldate = {2025-02-17},
	publisher = {arXiv},
	author = {Kanerva, Jenna and Ledins, Cassandra and Käpyaho, Siiri and Ginter, Filip},
	month = feb,
	year = {2025},
	note = {arXiv:2502.01205 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: To be published in RESOURCEFUL 2025},
	file = {Preprint PDF:/home/jonno/Zotero/storage/ILIICDHU/Kanerva et al. - 2025 - OCR Error Post-Correction with LLMs in Historical .pdf:application/pdf;Snapshot:/home/jonno/Zotero/storage/ITDZ6PLL/2502.html:text/html},
}

@article{schmidt_reichsanzeiger-gt_2024-1,
	title = {Reichsanzeiger-{GT}: {An} {OCR} ground truth dataset based on the historical newspaper “{Deutscher} {Reichsanzeiger} und {Preußischer} {Staatsanzeiger}” ({German} {Imperial} {Gazette} and {Prussian} {Official} {Gazette}) (1819–1945)},
	volume = {54},
	issn = {2352-3409},
	shorttitle = {Reichsanzeiger-{GT}},
	url = {https://www.sciencedirect.com/science/article/pii/S2352340924002439},
	doi = {10.1016/j.dib.2024.110274},
	abstract = {Reichsanzeiger-GT is a ground truth dataset for OCR training and evaluation based on the historical German newspaper “Deutscher Reichsanzeiger und Preußischer Staatsanzeiger” (German Imperial Gazette and Prussian Official Gazette), which was published from 1819 to 1945 and printed mostly in the typeface Fraktur (Black Letter). The dataset consists of 101 newspaper pages for the years 1820–1939, that cover a wide variety of topics, page layouts (lists, tables, and advertisements) as well as different typefaces. Using the transcription software Transkribus and the open-source OCR engine Tesseract we automatically created and manually corrected layout segmentations and transcriptions for each page, resulting in 65,563 text regions, 412 table regions, 119,429 text lines and 490,679 words. By applying transcription guidelines that preserve the printing conditions, the dataset contains language and printing specific phenomena like the historical use of glyphs like long s (ſ), rotunda r (ꝛ), and historical currency symbols (M, ₰) among others. The dataset is provided in two variants in PAGE XML format. The first one contains ground truth data with table regions transformed to text regions for easier processing. The second variant preserves all table regions. Researchers can reuse this dataset to train new or finetune existing text recognition or layout segmentation models. The dataset can also be used to evaluate the accuracy of existing OCR models. Using specific, community driven transcription guidelines our dataset is easily interoperable and reusable with other datasets based on the same transcription level.},
	urldate = {2025-02-17},
	journal = {Data in Brief},
	author = {Schmidt, Thomas and Kamlah, Jan and Weil, Stefan},
	month = jun,
	year = {2024},
	keywords = {OCR, Text recognition, Ground truth, Historical newspapers},
	pages = {110274},
	file = {Full Text:/home/jonno/Zotero/storage/3BBIYB52/Schmidt et al. - 2024 - Reichsanzeiger-GT An OCR ground truth dataset bas.pdf:application/pdf;ScienceDirect Snapshot:/home/jonno/Zotero/storage/9XBNFZZD/S2352340924002439.html:text/html},
}

@book{buzard_victorian_2007,
	title = {Victorian {Prism}: {Refractions} of the {Crystal} {Palace}},
	isbn = {978-0-8139-2603-2},
	shorttitle = {Victorian {Prism}},
	abstract = {From the moment it opened on the first of May in the Crystal Palace in Hyde Park, London, the Great Exhibition of 1851 was one of the defining events of the Victorian period. It stood not only as a visible symbol of British industrial and technological progress but as a figure for modernity--a figure that has often been thought to convey one coherent message and vision of culture and society. This volume examines the place occupied both materially and discursively by the Crystal Palace and other nineteenth- and twentieth-century exhibitions in the struggle to understand what it means to be modern. Initiated in part by a number of conferences held in 2001 to commemorate the 150th anniversary of the Crystal Palace, Victorian Prism provides new perspectives to historians, literary critics, art historians, and others interested in how a large glass building in a London park could refract meaning from Caracas to Calcutta. In its investigations of the ways of knowing and shaping the world that emerged during the planning and execution of this first "world's fair," Victorian Prism not only restores the multiplicity of experiences and other determining factors to our picture of the Great Exhibition; it makes reevaluation of the exhibition and its legacies the occasion for reevaluating modernity itself in its broadest sense--as the cultures, potentialities, and liabilities of the Enlightenment. With essays by a number of leading scholars in their fields, the collection as a whole focuses on how these exhibitions, in attempting to define the cultures of their day, incorporated a range of conflicting ideologies and agendas. In doing so, it offers a richer, more complex understanding of the experience of modernity than we have previously acknowledged. The volume also addresses the ways in which the cultural processes and tendencies brought together in these exhibitions have been refracted down to the present, thus informing and complicating our own relationship to both modernity and postmodernity.},
	language = {en},
	publisher = {University of Virginia Press},
	author = {Buzard, James and Childers, Joseph W. and Gillooly, Eileen},
	year = {2007},
	keywords = {History / Europe / Great Britain / General, History / Europe / Great Britain / Victorian Era (1837-1901), History / Social History, Literary Criticism / European / English, Irish, Scottish, Welsh, Social Science / Customs \& Traditions},
}

@article{addis_crystal_2006,
	title = {The {Crystal} {Palace} and its {Place} in {Structural} {History}},
	volume = {21},
	issn = {0956-0599},
	url = {https://doi.org/10.1260/026635106777641199},
	doi = {10.1260/026635106777641199},
	abstract = {Completed in 1851 to house the Exhibition of All Nations in London, the Crystal Palace was the first large public building that departed completely from traditional construction materials and methods. It was the first major building to be conceived by its design engineers, William Barlow and Charles Fox, as a rigid-jointed iron frame and one of the earliest to use horizontal and vertical cross-bracing to carry wind loads. Working closely with the contractor John Henderson, the designers also applied their knowledge of modern production engineering methods to ensure the building was constructed in the incredibly short time of 190 days. Within twenty years the iron frame, supporting thin walls of masonry, would become established as a viable alternative to load-bearing masonry walls for large buildings.},
	language = {en},
	number = {1},
	urldate = {2025-02-17},
	journal = {International Journal of Space Structures},
	author = {Addis, Bill},
	month = mar,
	year = {2006},
	note = {Publisher: SAGE Publications Ltd STM},
	pages = {3--19},
	file = {SAGE PDF Full Text:/home/jonno/Zotero/storage/623SAPKZ/Addis - 2006 - The Crystal Palace and its Place in Structural His.pdf:application/pdf},
}
