\section{Related Work}
\label{sec:related_work}
\paragraph{Prompt Optimization}
Effective prompt engineering is crucial for maximizing LLM performance, motivating various optimization techniques \cite{chang2024efficient, li-2023-practical, diao2023blackbox, sun2022bbt}. Methods like INSTINCT \cite{lin2024use} utilize neural bandits and LLM embeddings for efficient prompt search, while ZOPO \cite{hu2024localized} improves efficiency through localized search. BATprompt \cite{shi2024robustnessawareautomaticpromptoptimization} incorporates robustness considerations in in-context learning by leveraging natural language perturbations. However, these methods often suffer from prompt fragility, exhibiting high sensitivity to even minor prompt alterations, particularly after fine-tuning. This limits LLM generalization in real-world applications. Our work addresses this limitation by prioritizing robustness across diverse prompt formulations, rather than optimizing for a single prompt.

\paragraph{Supervised Fine-Tuning (SFT)}
SFT is a dominant paradigm for adapting LLMs, valued for its efficiency. Two main SFT approaches exist: soft prompt tuning (optimizing continuous vectors prepended to the input while freezing base model parameters) \cite{li-liang-2021-prefix, liu-etal-2022-p}, and full/parameter-efficient fine-tuning (PEFT) \cite{shu2024ferret, ouyang2022training, liu2021pretrainpromptpredictsystematic, lester-etal-2021-power}. Among PEFT techniques, Low-Rank Adaptation (LoRA) \cite{hu2022lora} is widely used, freezing pre-trained parameters and introducing low-rank trainable matrices. Advanced LoRA variants further aim to mitigate overfitting and enhance generalization \cite{chen2023lorashearefficientlargelanguage, si2024unleashingpowertaskspecificdirections, wei2024flexora}.  However, these methods, while mitigating parameter-level overfitting, typically rely on fixed training prompts, thus neglecting prompt robustness. This is particularly problematic for soft prompt tuning, where models exhibit high sensitivity to prompt variations. Consequently, minor deviations from training prompts can drastically degrade performance. To address this, we propose \ours{}, a novel framework that prioritizes prompt robustness while preserving  computational advantages. By decoupling model performance from specific prompt formulations, \ours{} significantly enhances the adaptability and reliability of fine-tuned models.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-3}
\vspace{-5mm}
\caption{The performance of the base model, the SFT model, and the \ours{} model is compared on multiple reasoning and reading comprehension tasks. This is a visual comparison to Figure~\ref{fig:prompt_impact} to illustrate the effectiveness of \ours{}, where the probability distribution plots show the distribution of accuracy of different models on the test prompts that were not used during \ours{} training. The \ours{} model shows superior performance compared to the base model and the SFT model, achieving higher accuracy and lower variance in all tasks.}
\label{fig:main_result}
\vspace{-3mm}
\end{figure*}