% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{array}
\usepackage{longtable}
\usepackage[most]{tcolorbox}
\usepackage{bibentry}
\usepackage{bm} 
\usepackage{tabularx}
\usepackage{multirow} 
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{arydshln}
\usepackage{appendix}

\makeatletter
\def\blankfootnote{\xdef\@thefnmark{}\@footnotetext}
\makeatother

\definecolor{royalblue(web)}{rgb}{0.25, 0.41, 0.88}
\definecolor{blue-violet}{rgb}{0.54, 0.17, 0.89}
\definecolor{brightmaroon}{rgb}{0.76, 0.13, 0.28}
\definecolor{darkmagenta}{rgb}{0.55, 0.0, 0.55}
\definecolor{bleudefrance}{rgb}{0.19, 0.55, 0.91}
\definecolor{palatinateblue}{rgb}{0.15, 0.23, 0.89}
\definecolor{royalblue(web)}{rgb}{0.25, 0.41, 0.88}
\definecolor{whitesmoke}{rgb}{0.96, 0.96, 0.96}
\definecolor{thulianpink}{rgb}{0.87, 0.44, 0.63}
\definecolor{amber(sae/ece)}{rgb}{1.0, 0.49, 0.0}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}
\definecolor{alizarin}{rgb}{0.82, 0.1, 0.26}
\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}
\definecolor{darkspringgreen}{rgb}{0.09, 0.45, 0.27}
\definecolor{columbiablue}{rgb}{0.61, 0.87, 1.0}
\definecolor{wildblueyonder}{rgb}{0.64, 0.68, 0.82}
\definecolor{trolleygrey}{rgb}{0.5, 0.5, 0.5}
\definecolor{paleaqua}{rgb}{0.74, 0.83, 0.9}
\definecolor{bubblegum}{rgb}{0.99, 0.76, 0.8}
\definecolor{coralred}{rgb}{1.0, 0.25, 0.25}
\definecolor{green(ryb)}{rgb}{0.4, 0.69, 0.2}
\definecolor{flame}{rgb}{0.89, 0.35, 0.13}
\definecolor{bittersweet}{rgb}{1.0, 0.44, 0.37}
\definecolor{darksalmon}{rgb}{0.91, 0.59, 0.48}
\definecolor{emerald}{rgb}{0.31, 0.78, 0.47}
\definecolor{green(pigment)}{rgb}{0.0, 0.65, 0.31}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.94}
\definecolor{bluegray}{rgb}{0.3, 0.38, 0.47}
\definecolor{whitesmoke}{rgb}{0.96, 0.96, 0.96}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.94}

\tcbuselibrary{breakable}
\usepackage{listings}
\lstdefinestyle{mystyle}{
  basicstyle=\scriptsize\ttfamily,
  frame=single, % 添加边框
  columns=fixed, % 设置列宽为固定值
}

\newtheorem{assumption}{Assumption}
\newtheorem{hypothesis}{Hypothesis}
\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}
\newtheorem{remark}{Remark}



\newcommand{\ours}{{\fontfamily{qpl}\selectfont PAFT}}
\input{math_commands}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{\ours{}: Prompt-Agnostic Fine-Tuning}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

\author{
  Chenxing Wei$^{* \dagger \S}$, Yao Shu$^{* \S}$, Mingwen Ou$^{\circ}$, Ying Tiffany He$^{\dagger}$, Fei Richard Yu$^{\# \dagger \ddagger}$\\
$^\dagger$College of Computer Science and Software Engineering, Shenzhen University, China \\
$^\circ$Tsinghua Shenzhen International Graduate School, Tsinghua University, China \\
$^{\S}$Guangdong Lab of AI and Digital Economy (SZ), China \\
$^{\ddagger}$School of Information Technology, Carleton University, Canada \\
\texttt{weichenxing2023@email.szu.edu.cn}, \texttt{shuyao@gml.ac.cn}\\ \texttt{omm23@mails.tsinghua.edu.cn}, \texttt{richard.yu@ieee.org}
}

\begin{document}
\maketitle
\begin{abstract}
While Large Language Models (LLMs) adapt well to downstream tasks after fine-tuning, this adaptability often compromises prompt robustness, as even minor prompt variations can significantly degrade performance. To address this, we propose \textit{\underline{p}rompt-\underline{a}gnostic \underline{f}ine-\underline{t}uning} (\ours{}), a simple yet effective approach that dynamically adjusts prompts during fine-tuning. This encourages the model to learn underlying task principles rather than overfitting to specific prompt formulations. \ours{} operates in two stages: First, a diverse set of meaningful, synthetic candidate prompts is constructed. Second, during fine-tuning, prompts are randomly sampled from this set to create dynamic training inputs.  Extensive experiments across diverse datasets and LLMs demonstrate that models trained with \ours{} exhibit strong robustness and generalization across a wide range of prompts, including unseen ones. This enhanced robustness improves both model performance and inference speed while maintaining training efficiency. Ablation studies further confirm the effectiveness of \ours{}.
\blankfootnote{$*$ Equal contribution, ${\#}$ corresponding author.}

% 1. 鲁棒性 2. 性能提升 3. 推理时间 done

% 鲁棒性 done
% Preliminaries 实验描述 done 
% train 和 test 的多样性是否有交叉，多样性， 有点难做，rebuttal补充吧
% 选模型的依据 done
% Limitations 写清楚 done

\end{abstract}

\begin{figure}[t]
\centering
\includegraphics[width=0.45\textwidth, trim={6cm 3cm 7cm 2cm}, clip]{figure-4}
\vspace{-5mm}
\caption{This figure shows how small changes in prompts can drastically affect the accuracy of a model. Two examples show the same user question, but the prompts differ by only one word, resulting in different answers. The first prompt achieves 86.27\% accuracy across the entire dataset, while the second prompt drops significantly to 66.93\%. This highlights how even small modifications can lead to large swings in performance if a model lacks prompt robustness.}
\label{fig:diglog}
\vspace{-4mm}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figure-1}
\vspace{-5mm}
\caption{An overview of \ours{}: This figure compares Traditional Supervised Fine-tuning (SFT) and Prompt-Agnostic Fine-Tuning (\ours{}), highlighting their main differences. SFT relies on a fixed dataset and predefined prompts, which limits its robustness and generalization to different prompts. In contrast, \ours{} dynamically selects prompts during training, which improves robustness and generalization to a wide range of prompts. By leveraging a commercial LLM to generate candidate prompts, \ours{} provides a more general and scalable solution.}
\label{fig:framework}
\vspace{-3mm}
\end{figure*}


\section{Introduction}
\label{introduction}

Large language models (LLMs) have demonstrated remarkable success across a diverse range of natural language processing (NLP) tasks \cite{zhao2024surveylargelanguagemodels, xu2023parameterefficientfinetuningmethodspretrained}. To further enhance the performance of LLMs on specific downstream tasks, supervised fine-tuning (SFT) has emerged as a widely adopted strategy \cite{ouyang2022training, devlin-etal-2019-bert}. This approach typically involves augmenting input data with task-specific instructions and constructing dialogue datasets with expected outputs, enabling the model to effectively learn task-specific patterns during fine-tuning. Empirical studies have shown that SFT can substantially improve model performance on downstream tasks \cite{raffel2023exploringlimitstransferlearning, hu2023llmadapters, wei2022finetuned}. However, a critical limitation of this paradigm is its reliance on fixed instruction templates \cite{mishra2022crosstaskgeneralizationnaturallanguage, chung2022scalinginstructionfinetunedlanguagemodels} for each downstream task. This rigidity often leads to overfitting, whereby models become excessively dependent on specific instruction patterns \cite{zhang2024instructiontuninglargelanguage, kung2023models}. Consequently, during inference on downstream tasks, even minor deviations between user-provided instructions and the training instructions can result in significant performance degradation \cite{mialon2023augmented, raman2023modeltuning}. This issue is particularly pronounced when LLM practitioners, who may lack domain expertise, provide prompts that deviate substantially from those used during SFT. In such scenarios, carefully fine-tuned models may experience drastic performance drops, occasionally approaching random guessing levels \cite{voronov-etal-2024-mind}. Previous research has primarily focused on prompt tuning—introducing trainable vectors (soft prompts) to optimize performance \cite{liu-etal-2022-p, li-liang-2021-prefix, lester-etal-2021-power}—however, these methods inadvertently increase sensitivity to prompt variations \cite{wen2023hard, qin-eisner-2021-learning}, resulting in significant performance fluctuations and increased costs associated with prompt engineering \cite{han2024parameterefficientfinetuninglargemodels, longpre2023flancollectiondesigningdata}. Prompt robustness in SFT has received limited attention, with most existing work focusing on in-context learning \cite{DBLP:conf/lamps/Zhu0ZW0WY000024, shi2024robustnessawareautomaticpromptoptimization,ishibashi-etal-2023-evaluating}.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-2}
\vspace{-5mm}
\caption{This figure presents the results of preliminary experiments conducted on four datasets to evaluate the accuracy of the base model and the SFT model across over 450 diverse prompts. The probability distribution plots illustrate the distribution of accuracy for models. The results show that while the SFT model has an overall improvement in accuracy compared to the base model, the accuracy of some prompts is still relatively low, and the standard deviation of the SFT model is high, indicating that the accuracy varies greatly between different prompts, which highlights the impact of prompt design and the need for further optimization through model fine-tuning.}
\label{fig:prompt_impact}
\vspace{-3mm}
\end{figure*}

% done。ft比prompt tuning 更好。 尽管few work 在研究 prompt robust 但是 集中在 上下文学和prompt tuning上，在ft 上没人研究，强调 我们是第一个在ft上做的，并且比其他的方法效果好。
To address this critical gap, we present \ours{}, an innovative fine-tuning framework designed to dynamically adapt to diverse prompts during training. To our knowledge, this is the first systematic approach to enhancing prompt robustness in SFT, a vital yet under-explored area. Unlike traditional methods, which often overfit to specific prompt patterns, \ours{} enables models to grasp underlying task semantics, ensuring robust performance across various human-written prompts.As shown in Figure~\ref{fig:framework}, \ours{} operates in two phases: (1) Candidate Prompt Construction (Section~\ref{sec:prompt}) and (2) Dynamic Fine-Tuning (Section~\ref{sec:finetuning}). Initially, a diverse set of high-quality synthetic prompts is generated, capturing essential task semantics while maintaining linguistic variability. During fine-tuning, a dynamic prompt sampling strategy is employed, randomly selecting prompts from our curated set to expose the model to a wide range of formulations. Extensive evaluations reveal that \ours{} achieves three primary objectives: (1) significantly boosting model robustness and generalization across diverse prompts; (2) maintaining state-of-the-art performance on downstream tasks; and (3) potentially enhancing inference speed while preserving training efficiency. These findings indicate that \ours{} represents a promising direction for developing more robust and user-friendly language models. Our key contributions are: (a) Through comprehensive experiments, we demonstrate that fine-tuning with fixed prompts significantly undermines the model's robustness to prompt variations, leading to poor generalization on unseen prompts and severe performance degradation; and (b) We propose \ours{}, comprising candidate prompt construction and dynamic fine-tuning, a novel approach to enhance the prompt robustness of fine-tuned models. This approach ensures consistent and robust performance across a variety of test prompts, including those not encountered during training.




\section{Related Work}
\label{sec:related_work}
\paragraph{Prompt Optimization}
Effective prompt engineering is crucial for maximizing LLM performance, motivating various optimization techniques \cite{chang2024efficient, li-2023-practical, diao2023blackbox, sun2022bbt}. Methods like INSTINCT \cite{lin2024use} utilize neural bandits and LLM embeddings for efficient prompt search, while ZOPO \cite{hu2024localized} improves efficiency through localized search. BATprompt \cite{shi2024robustnessawareautomaticpromptoptimization} incorporates robustness considerations in in-context learning by leveraging natural language perturbations. However, these methods often suffer from prompt fragility, exhibiting high sensitivity to even minor prompt alterations, particularly after fine-tuning. This limits LLM generalization in real-world applications. Our work addresses this limitation by prioritizing robustness across diverse prompt formulations, rather than optimizing for a single prompt.

\paragraph{Supervised Fine-Tuning (SFT)}
SFT is a dominant paradigm for adapting LLMs, valued for its efficiency. Two main SFT approaches exist: soft prompt tuning (optimizing continuous vectors prepended to the input while freezing base model parameters) \cite{li-liang-2021-prefix, liu-etal-2022-p}, and full/parameter-efficient fine-tuning (PEFT) \cite{shu2024ferret, ouyang2022training, liu2021pretrainpromptpredictsystematic, lester-etal-2021-power}. Among PEFT techniques, Low-Rank Adaptation (LoRA) \cite{hu2022lora} is widely used, freezing pre-trained parameters and introducing low-rank trainable matrices. Advanced LoRA variants further aim to mitigate overfitting and enhance generalization \cite{chen2023lorashearefficientlargelanguage, si2024unleashingpowertaskspecificdirections, wei2024flexora}.  However, these methods, while mitigating parameter-level overfitting, typically rely on fixed training prompts, thus neglecting prompt robustness. This is particularly problematic for soft prompt tuning, where models exhibit high sensitivity to prompt variations. Consequently, minor deviations from training prompts can drastically degrade performance. To address this, we propose \ours{}, a novel framework that prioritizes prompt robustness while preserving  computational advantages. By decoupling model performance from specific prompt formulations, \ours{} significantly enhances the adaptability and reliability of fine-tuned models.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-3}
\vspace{-5mm}
\caption{The performance of the base model, the SFT model, and the \ours{} model is compared on multiple reasoning and reading comprehension tasks. This is a visual comparison to Figure~\ref{fig:prompt_impact} to illustrate the effectiveness of \ours{}, where the probability distribution plots show the distribution of accuracy of different models on the test prompts that were not used during \ours{} training. The \ours{} model shows superior performance compared to the base model and the SFT model, achieving higher accuracy and lower variance in all tasks.}
\label{fig:main_result}
\vspace{-3mm}
\end{figure*}

\section{Preliminaries}
\label{preliminaries}
% done, 融合在一点，然后sencond作为比较严重的例子
To systematically study the impact of prompt variations on fine-tuned models, we use LoRA \cite{hu2022lora} as an illustrative example and conduct comprehensive preliminary experiments on multiple downstream tasks to assess prompt sensitivity and robustness. These tasks include natural language inference, question answering, and reading comprehension, using the LLaMA3-8B \cite{llama3} model. We constructed a comprehensive set of over 450 prompts, covering a wide range of language styles, task-specific instructions, and formatting variations. Figure~\ref{fig:prompt_impact} presents a statistical analysis of the accuracy distribution for both the base model and SFT model across these prompts, revealing a key finding: prompt selection significantly influences model performance, with considerable accuracy variation observed across prompts, irrespective of the downstream task. Only a small fraction (typically less than 10\%) of prompts yields near-optimal performance; some even degrade accuracy to near-random levels. Minor prompt modifications (e.g., rephrasing, punctuation, reordering) induce substantial fluctuations. For example, the addition of "Question" improves accuracy by 20\% (Figure~\ref{fig:diglog}). This sensitivity highlights the fragility of current fine-tuning methods and their strong dependence on specific prompt formulations. These findings align with prior work \cite{he2024doespromptformattingimpact, voronov-etal-2024-mind, salinas2024butterflyeffectalteringprompts, min-etal-2022-noisy, gao-etal-2021-making}; however, we demonstrate that this sensitivity persists across tasks, suggesting a fundamental limitation of current PEFT paradigms. Motivated by these findings, we propose \ours{}, addressing prompt robustness by decoupling performance from specific formulations, ensuring consistent results across diverse prompts, and significantly enhancing the practical applicability of fine-tuned models in real-world scenarios where prompt variations are inevitable.



\section{The \ours{} Framework}
To improve the prompt robustness of LLMs, we propose the \ours{} framework in Figure~\ref{fig:framework}. As shown in Figure~\ref{fig:framework}, the \ours{} framework consists of two key stages: candidate prompt construction (see Section~\ref{sec:prompt} for details) and dynamic fine-tuning (see Section~\ref{sec:finetuning} for details).
\label{framework}
\subsection{Candidate Prompt Construction}\label{sec:prompt}
To ensure the robustness and effectiveness of \ours{} across diverse prompts, we design a comprehensive prompt construction framework that aims to generate diverse and meaningful candidate prompts efficiently, enabling the model to generalize across different prompt formats. Our approach leverages the powerful generative capabilities of LLMs \cite{kohl2024generativeaitoolkit} and comprises three key phases: First, recognizing the inherent variability in how different LLMs interpret downstream tasks due to variations in pre-training data, model architectures, and optimization objectives \cite{minaee2024largelanguagemodelssurvey, zhao2024surveylargelanguagemodels}, we employ a multi-model approach, selecting 10 mainstream LLMs according to their generation capabilities, including models from \citet{openai2024gpt4technicalreport,bai2023qwentechnicalreport, ouyang2022training}, and other widely used commercial LLMs, for prompt generation. This diverse selection ensures broad coverage of potential prompt formulations, capturing variations in linguistic style, task interpretation, and instructional clarity, thereby mitigating biases towards any single model's prompt generation tendencies. Second, we employ a dual-strategy approach, combining few-shot and zero-shot techniques to balance prompt quality and diversity. For few-shot prompting, we leverage principles from in-context learning, providing each LLM with carefully curated, human-crafted examples to guide the generation of semantically coherent and task-relevant prompts, ensuring meaningfulness and alignment with the intended task. For zero-shot prompting, we prioritize diversity by allowing LLMs to generate prompts without explicit examples, thus encouraging a wider range of linguistic styles, structural variations, and task formulations. Specifically, we generate 20 prompts using each strategy, resulting in a comprehensive set encompassing both high-quality prompts (derived from few-shot prompting) and diverse, potentially less optimal prompts (derived from zero-shot prompting). This balanced approach exposes the model to a realistic distribution of prompt quality during training, thereby enhancing its robustness to real-world scenarios where prompt quality may vary significantly. Finally, to rigorously evaluate the robustness of \ours{}, we randomly partition the generated prompts into training and test sets using an 8:1 ratio. Crucially, the training and test sets contain entirely distinct prompts, ensuring evaluation on completely unseen formulations. This partitioning strategy enables the construction of training data that exposes the model to a wide range of prompt styles while providing a robust testbed for assessing generalization to novel prompts. By decoupling training and test prompts, we confirm that performance improvements reflect a genuine ability to handle diverse and unseen prompt formulations, rather than overfitting to specific prompt patterns. This comprehensive framework ensures that \ours{} learns task semantics independently of specific prompt patterns, enabling effective generalization across a wide range of real-world scenarios, and provides a scalable and cost-effective solution for improving prompt robustness in LLMs. 

\subsection{Dynamic Fine-Tuning}\label{sec:finetuning}
The dynamic fine-tuning process in our \ours{} framework is designed to enhance the robustness of LLMs to diverse prompt formulations while preserving high performance on downstream tasks. As illustrated in Algorithm~\ref{algo:paft}, during each training epoch t, a prompt p is randomly sampled from a diverse set of synthetically generated candidate prompts $\sP$ (line 4 in Algorithm~\ref{algo:paft}), ensuring exposure to a wide range of linguistic styles and task formulations. For each data point $(x, y) \in \sD$ (line 6 in Algorithm~\ref{algo:paft}), the selected prompt $p$ is reused for $K$ consecutive training steps (lines 7-9 in Algorithm~\ref{algo:paft}), and the input $\tI = \text{InputConstruction}(x, p)$ is constructed by combining the prompt $p$ with the data point $x$ (line 7 in Algorithm~\ref{algo:paft}). The model parameters $\theta$ are then updated using stochastic gradient-based optimization methods, such as SGD~\citep{sgd} or AdamW~\citep{adamw} (line 8 in Algorithm~\ref{algo:paft}), enabling the model to learn task-specific semantics while adapting to the formulation of prompt. After every $K$ steps, a new prompt is sampled from $\sP$ to replace the current one (lines 10-11 in Algorithm~\ref{algo:paft}), ensuring that the model is exposed to multiple prompts within a single epoch. At the end of each epoch, the model parameters $\theta_{t+1}^0$ are initialized with the final parameters from the previous epoch, $\theta_{t}^K$ (line 12 in Algorithm~\ref{algo:paft}), ensuring continuity in the learning process. After $T$ epochs, the fine-tuned model parameters $\theta^* = \theta_{T}$ achieve consistent performance across a wide range of prompts (line 16 in Algorithm~\ref{algo:paft}), including those not encountered during training. This makes \ours{} particularly suitable for real-world applications where prompt quality and style may vary significantly, such as when users lack domain expertise or when prompts are generated automatically. By decoupling model performance from fixed prompt formulations, \ours{} addresses a key limitation of traditional fine-tuning methods, ensuring robust performance without requiring extensive prompt engineering. The dynamic fine-tuning strategy enhances both the robustness and generalization of fine-tuned models while maintaining computational efficiency, making it a practical solution for improving the adaptability of LLMs in diverse settings.
\begin{algorithm}[t]
\caption{The \ours{} Framework}
\label{algo:paft}
\small
\begin{algorithmic}[1] %[1] enables line numbers
    \STATE {\bfseries Input:} Generate a good candidate prompt training set $\sP$; A task-specific dataset $\sD$; The number of training epochs $T$; The number of same prompt training $K$; Initialized trainable parameters $\theta_0^0$; Learning rate $\eta_{\theta}$
    \STATE {\bfseries Output:} Fine-tuned model parameters $\theta^*$. 
    \FOR{each epoch $t=0$ {\bfseries to} $T-1$}
      \STATE $p \gets \text{RandomlySample}(\sP)$ \COMMENT{Randomly select a prompt from the candidate set}
      \STATE $k \gets 0$ \COMMENT{Initialize the step counter}
      \FOR{each data point $(x, y) \in \sD$}
        \STATE $\tI \gets \text{InputConstruction}(x, p)$ \COMMENT{Construct input using prompt $p$ and data $x$}
        \STATE $\theta_{t}^{k + 1} \gets \theta_{t}^k - \eta_{\theta} \nabla_{\theta} \ell(\theta, \tI)|_{\theta = \theta_{t}^k}$ \COMMENT{Update model parameters}
        \STATE $k \gets k + 1$ \COMMENT{Increment the step counter}
        \IF{$k \mod K == 0$}
                \STATE $p \gets \text{RandomlySample}(\sP)$ \COMMENT{Update prompt every $K$ steps}        \ENDIF
      \ENDFOR
      \STATE $\theta_{t+1}^0 \gets \theta_{t}^k$ \COMMENT{Carry over parameters to the next epoch}
    \ENDFOR
    \STATE {\bf return} $\theta^* = \theta_{T}$ \COMMENT{Return the final fine-tuned parameters}
\end{algorithmic}
\end{algorithm}

% \begin{table*}[t!]
%     \centering
%     \caption{Performance comparison of different fine-tuning methods on the test prompt sets across various reasoning and reading comprehension tasks using the LLaMA3-8B \cite{llama3} with LoRA rank 8. Results are reported as average accuracy (\(\pm\) standard deviation). The \textbf{Base Model} represents the pre-trained model without fine-tuning, \textbf{user-specified prompt }\cite{wei2024flexora} refers to fine-tuning with LoRA using human-designed prompts, \textbf{TopAccuracy prompt }refers to fine-tuning with LoRA using the prompt exhibiting the highest accuracy on the training set, \textbf{BATprompt} refers to fine-tuning with LoRA using the most robust prompt generated by BATprompt \cite{shi2024robustnessawareautomaticpromptoptimization}, and \textbf{ZOPO prompt} refers to fine-tuning with LoRA using the optimal prompt selected by ZOPO \cite{hu2024localized} from the training prompt set. \textbf{\ours{}} (our proposed method) demonstrates superior performance, achieving the highest accuracy and lowest variance across all tasks. The last two rows show the comparison of \ours{} with the second-best performing method (underlined). The red font indicates the percentage of test prompts with a correct rate of 90\% for Hellaswag, 80\% for Winogrande, and 85\% for other datasets.}
%     % basemodel，LoRA + user-specified prompt， LoRA + BATprompt，  LoRA + ZOPO prompt, PAFT  . 说明在test prompt set 上 泛化更好。 }
%     \label{tab:accuracy_comparison}
%     \resizebox{\textwidth}{!}{
%         \begin{tabular}{l*{6}{c}}
%             \toprule
%             \textbf{Methods} & \textbf{Hellaswag} & \textbf{PIQA} & \textbf{Winogrande} & \textbf{RACE-mid}& \textbf{RACE-high} & \textbf{Average} \\
%             \midrule
%             Base Model & 47.36 ($\pm$ 9.78)\textcolor{red}{(0\%)} & 74.68 ($\pm$ 6.24)\textcolor{red}{(0\%)} & 45.15 ($\pm$ 11.78)\textcolor{red}{(0\%)} & 71.39 ($\pm$ 7.33)\textcolor{red}{(0\%)} & 67.62 ($\pm$ 6.78)\textcolor{red}{(0\%)} & 61.24 ($\pm$ 8.38)\textcolor{red}{(0\%)} \\
%             user-specified prompt & 92.35 ($\pm$ 2.78)\textcolor{red}{(0\%)} & 77.87 ($\pm$ 2.36)\textcolor{red}{(0\%)} & \underline{78.16} ($\pm$ 7.97)\textcolor{red}{(0\%)} & 79.88 ($\pm$ 6.32)\textcolor{red}{(22\%)} & 81.05 ($\pm$ 4.45)\textcolor{red}{(4\%)} & 81.86 ($\pm$ 4.78)\textcolor{red}{(5\%)} \\
%             TopAccuracy prompt & 91.27($\pm$ 2.79)\textcolor{red}{(86\%)} & 75.96 ($\pm$ 3.89)\textcolor{red}{(0\%)} & 66.77 ($\pm$ 3.94)\textcolor{red}{(0\%)} & \underline{84.81} ($\pm$ \underline{4.06})\textcolor{red}{(59\%)} & \underline{82.45} ($\pm$ \underline{3.26})\textcolor{red}{(14\%)} & 80.25 ($\pm$ 3.63)\textcolor{red}{(0\%)} \\
%             BATprompt & 90.30($\pm$ \underline{1.79})\textcolor{red}{(78\%)} & 83.41 ($\pm$ \underline{1.74})\textcolor{red}{(16\%)} & 69.01 ($\pm$ 4.45)\textcolor{red}{(0\%)} & 83.92 ($\pm$ 5.38)\textcolor{red}{(65\%)} & 81.33 ($\pm$ 4.21)\textcolor{red}{(12\%)} & 81.56 ($\pm$ \underline{3.51})\textcolor{red}{(32\%)} \\
%             ZOPO prompt & \underline{92.46} ($\pm$ 2.43)\textcolor{red}{(86\%)} & \underline{83.52} ($\pm$ 2.23)\textcolor{red}{(27\%)} & 74.75 ($\pm$ \underline{3.81})\textcolor{red}{(0\%)} & 83.50 ($\pm$ 5.05)\textcolor{red}{(51\%)} & 82.36 ($\pm$ 4.53)\textcolor{red}{(35\%)} & \underline{83.32} ($\pm$ 3.61)\textcolor{red}{(34\%)} \\
%             \midrule
%             \textbf{\ours{} } & \textbf{93.83} ($\pm$ \textbf{0.70})\textcolor{red}{(\textbf{100\%})} & \textbf{89.33} ($\pm$ \textbf{0.63})\textcolor{red}{(\textbf{100\%})} & \textbf{82.09} ($\pm$ \textbf{0.81})\textcolor{red}{(\textbf{100\%})} & \textbf{87.26} ($\pm$ \textbf{2.23})\textcolor{red}{(\textbf{94\%})} & \textbf{85.17} ($\pm$ \textbf{1.71})\textcolor{red}{(\textbf{73\%})} & \textbf{87.57} ($\pm$ \textbf{1.57})\textcolor{red}{(\textbf{94\%})} \\
%             \ours{} Improvement (Std) &  -1.09 & -1.11 & -3.00 & -1.83 & -1.55 & -1.94 \\ 
%             \ours{} Improvement (Mean) &  +1.37 & +5.81 & +3.93 & +2.45 & +2.72 & +4.25 \\ 

%             \bottomrule
%         \end{tabular}
%     }
%     \vspace{-3mm}
% \end{table*}

\begin{table*}[t!]
    \centering
    \caption{Performance comparison of different fine-tuning methods on the test prompt sets across various reasoning and reading comprehension tasks using the LLaMA3-8B \cite{llama3} with LoRA rank 8. Results are reported as average accuracy, standard deviation, and percentage of test prompts exceeding a specific score threshold (90\% for Hellaswag, 80\% for Winogrande, and 85\% for other datasets). The \textbf{Base Model} represents the pre-trained model without fine-tuning, \textbf{user-specified prompt} \cite{wei2024flexora} refers to fine-tuning with LoRA using human-designed prompts, \textbf{TopAccuracy prompt} refers to fine-tuning with LoRA using the prompt exhibiting the highest accuracy on the training set, \textbf{BATprompt} refers to fine-tuning with LoRA using the most robust prompt generated by BATprompt \cite{shi2024robustnessawareautomaticpromptoptimization}, and \textbf{ZOPO prompt} refers to fine-tuning with LoRA using the optimal prompt selected by ZOPO \cite{hu2024localized} from the training prompt set. \textbf{\ours{}} (our proposed method) demonstrates superior performance, achieving the highest accuracy and lowest variance across all tasks. The last rows show the comparison of \ours{} with the second-best performing method (underlined). The Top column indicates the percentage of test prompts with a correct rate of 90\% for Hellaswag, 80\% for Winogrande, and 85\% for other datasets.}
    \label{tab:accuracy_comparison}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l|ccc|ccc|ccc|ccc|ccc|ccc}
            \toprule
            \textbf{Methods} & \multicolumn{3}{c|}{\textbf{Hellaswag}} & \multicolumn{3}{c|}{\textbf{PIQA}} & \multicolumn{3}{c|}{\textbf{Winogrande}} & \multicolumn{3}{c|}{\textbf{RACE-mid}}& \multicolumn{3}{c|}{\textbf{RACE-high}} & \multicolumn{3}{c}{\textbf{Average}} \\
            \midrule
            Metric & Mean & Std & Top & Mean & Std & Top & Mean & Std & Top & Mean & Std & Top & Mean & Std & Top & Mean & Std & Top \\
            \midrule
            Base Model & 47.36 & $\pm$9.78 & 0\% & 74.68 & $\pm$6.24 & 0\% & 45.15 & $\pm$11.78 & 0\% & 71.39 & $\pm$7.33 & 0\% & 67.62 & $\pm$6.78 & 0\% & 61.24 & $\pm$8.38 & 0\% \\
            user-specified prompt & 92.35 & $\pm$2.78 & 0\% & 77.87 & $\pm$2.36 & 0\% & \underline{78.16} & $\pm$7.97 & 0\% & 79.88 & $\pm$6.32 & 22\% & 81.05 & $\pm$4.45 & 4\% & 81.86 & $\pm$4.78 & 5\% \\
            TopAccuracy prompt & 91.27 & $\pm$2.79 & \underline{86\%} & 75.96 & $\pm$3.89 & 0\% & 66.77 & $\pm$3.94 & 0\% & \underline{84.81} & \underline{$\pm$4.06} & 59\% & \underline{82.45} & \underline{$\pm$3.26} & 14\% & 80.25 & $\pm$3.63 & 32\% \\
            BATprompt & 90.30 & \underline{$\pm$1.79} & 78\% & 83.41 & \underline{$\pm$1.74} & 16\% & 69.01 & $\pm$4.45 & 0\% & 83.92 & $\pm$5.38 & \underline{65\%} & 81.33 & $\pm$4.21 & 12\% & 81.56 & \underline{$\pm$3.51} & 34\% \\
            ZOPO prompt & \underline{92.46} & $\pm$2.43 & \underline{86\%} & \underline{83.52} & $\pm$2.23 & \underline{27\%} & 74.75 & \underline{$\pm$3.81} & 0\% & 83.50 & $\pm$5.05 & 51\% & 82.36 & $\pm$4.53 & \underline{35\%} & \underline{83.32} & $\pm$3.61 & \underline{40\%} \\
            \midrule
            \textbf{\ours{}} & \textbf{93.83} & $\pm$\textbf{0.70} & \textbf{100\%} & \textbf{89.33} & $\pm$\textbf{0.63} & \textbf{100\%} & \textbf{82.09} & $\pm$\textbf{0.81} & \textbf{100\%} & \textbf{87.26} & $\pm$\textbf{2.23} & \textbf{94\%} & \textbf{85.17} & $\pm$\textbf{1.71} & \textbf{73\%} & \textbf{87.57} & $\pm$\textbf{1.57} & \textbf{94\%} \\
            \ours{} Improvement & +1.37 & -1.09 & 14\% & +5.81 & -1.11 & 73\% & +3.93 & -3.00 & 100\% & +2.45 & -1.83 & 29\% & +2.72 & -1.55 & 38\% & +4.25 & -1.94 & 54\% \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
\end{table*}
\begin{table}[t!]
    \centering
    \caption{Comparison of inference time (in hours) for different fine-tuning methods. The base model represents the pre-trained model without fine-tuning, while the other rows show the inference time of models fine-tuned with LoRA using different prompts. PAFT shows better inference efficiency than other methods. The last line shows the multiple of \ours{} improvement.}
    \label{tab:infer_time_comparison}
    \resizebox{0.48\textwidth}{!}{
        \begin{tabular}{l*{6}{c}}
            \toprule
            \textbf{Inference time/h} & \textbf{Hellaswag} & \textbf{PIQA} & \textbf{Winogrande} & \textbf{RACE} & \textbf{Average} \\
            \midrule
            Base Model & \underline{3.97} & 1.35 & \underline{1.72} & \underline{6.24} & \underline{3.32} \\
            user-specified prompt & 6.52 & 0.98 & 3.27 & 8.23 & 4.75 \\
            TopAccuracy prompt & 5.75 & 1.13 & 2.76 & 7.56 & 4.30 \\
            BATprompt & 4.57 & 1.57 & 3.14 & 7.98 & 4.32 \\
            ZOPO prompt & 5.12 & \underline{0.87} & 3.23 & 8.28 & 4.38 \\
            \midrule
            \textbf{\ours{} } & \textbf{1.19} & \textbf{0.39} & \textbf{0.45} & \textbf{2.08} & \textbf{1.02} \\
            \ours{} Improvement & $\times$3.3 & $\times$2.23 & $\times$3.82 & $\times$3.00 & $\times$3.25\\
            \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
\end{table}
\section{Empirical Results}\label{sec:results}
In this section, we conduct extensive experiments to evaluate the effectiveness and efficiency of our proposed \ours{} framework. We begin by detailing the datasets and experimental setup in Section~\ref{sec:setup}, followed by a comprehensive analysis of the main results in Section~\ref{sec:main result}. Additionally, we perform ablation studies to investigate the impact of key components of our framework, as discussed in Section~\ref{ablation study}.


\subsection{Datasets and Setup}\label{sec:setup}
To evaluate the performance of our proposed \ours{} method, we focus on reasoning and reading comprehension tasks, as these domains are particularly susceptible to prompt variations. As \ours{} is the first work to address the prompt robustness problem in large language models (LLMs) through training, we generate task-specific candidate prompts for each downstream task. Following the dataset selection process of \citet{hu-etal-2023-llm, wei2024flexora}, we select the Winogrande \cite{WinoGrande}, PIQA \cite{PIQA}, and Hellaswag \cite{HellaSwag} reasoning benchmarks and additionally include the RACE \cite{RACE} reading comprehension benchmark. These datasets are widely recognized for their ability to assess reasoning and comprehension, provide independent training, validation, and test sets, and employ accuracy as the performance metric. As described in Section~\ref{sec:prompt}, we generate a diverse set of 400 training prompts and 50 test prompts, ensuring that the test prompts are distinct from the training prompts, see the Appendix~\ref{app:prompt} for details. This separation rigorously evaluates the ability of model to generalize to unseen prompt formulations. We establish five baselines for comparison to isolate the impact of prompt engineering on fine-tuning: the pre-trained model without fine-tuning (Base Model); fine-tuning with human-designed prompts (User-Specified Prompt) as in \citet{wei2024flexora}; fine-tuning with the prompt exhibiting the highest accuracy on the training set (Top-Accuracy Prompt); fine-tuning with the most robust prompt generated by BATprompt \cite{shi2024robustnessawareautomaticpromptoptimization} (BATprompt); and fine-tuning with the optimal prompt selected by ZOPO \cite{hu2024localized} from the training prompt set (ZOPO Prompt). The key distinction between these methods lies in the prompt selection for fine-tuning. Critically, all models, including the baselines, are evaluated using the same set of 50 test prompts. This consistent evaluation protocol allows us to directly compare performance consistency and variation across methods. Our implementation leverages the Llama-factory framework \citep{LlamaFactory} and is evaluated using the Opencompass framework \citep{OpenCompass}. Detailed experimental configurations are provided in Appendix~\ref{app:setup}. All experiments are conducted on NVIDIA A100, V100, 4090, and L40 GPUs to ensure efficient and scalable evaluation.
\subsection{Main Results}\label{sec:main result}
\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-6}
\vspace{-5mm}
\caption{The performance of TopAccuracy, User-specified, BATprompt, ZOPO, and \ours{} models is compared on multiple reasoning and reading comprehension tasks. Results are reported in terms of their correct distribution. The tests are conducted on a test set of 50 unseen prompts, different from the ones used in training. The \ours{} model shows superior performance compared to other baselines, achieving higher accuracy and lower variance in all tasks.}
\label{fig:number of prompts}
\vspace{-3mm}
\end{figure*}
% done 加一个figures-3类似的图，增加paft结果

\paragraph{\ours{} demonstrates strong prompt robustness} 
As shown in Table~\ref{tab:accuracy_comparison}, Figure~\ref{fig:main_result}, and Figure~\ref{fig:baseline}, \ours{} exhibits remarkably low variance across all evaluation tasks, indicating excellent prompt robustness. Compared to other methods, \ours{} achieves significantly lower variance, attributable to its unique dynamic prompt selection strategy. This strategy continuously adjusts the prompt during training, compelling the model to learn essential task features rather than overfitting to a specific prompt format. This contrasts sharply with the other baseline models. User-specified prompts rely on manually designed prompts, making it challenging to ensure both quality and diversity, especially without domain expertise. While TopAccuracy and ZOPO select the prompt exhibiting the highest accuracy on the training set, they are prone to overfitting to specific prompts and exhibit poor generalization. Although BATprompt also considers prompt robustness, its generated robust prompts are less effective than \ours{}. In summary, the low variance of \ours{} implies more stable performance and stronger generalization across diverse prompts, leading to higher reliability in practical applications. Specifically, models trained with \ours{} can be used to develop more user-friendly question-answering systems, agent systems independent of input-output formats, and even to better decouple LLM capabilities from prompts, enabling more accurate LLM evaluation. \ours{} achieves top performance on the majority of prompts, significantly outperforming all baselines (Table~\ref{tab:accuracy_comparison}, Top column). Furthermore, \ours{} maintains high training efficiency, A detailed discussion of training efficiency is provided in Appendix~\ref{app:train time}.
\begin{table*}[t!]
    \centering
    \caption{Performance comparison of \ours{} with varying hyperparameters \(K\) (number of iterations per prompt) and \(T\) (number of epochs) across multiple reasoning and reading comprehension tasks. Results are reported as mean accuracy (\(\pm\) standard deviation) on the Hellaswag, PIQA, Winogrande, RACE-mid, and RACE-high datasets. The best results for each metric are highlighted in bold.}
    \label{tab:kt_comparison}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l*{6}{c}}
            \toprule
            \textbf{\# \( K \) and \( T \)} & \textbf{Hellaswag} & \textbf{PIQA} & \textbf{Winogrande} & \textbf{RACE-mid}& \textbf{RACE-high} & \textbf{Average} \\
            \midrule
            \( K \) = 1, \( T \) = 3 & 93.58 ($\pm$ 1.47) & \textbf{89.33}($\pm$ 0.63) & 81.78 ($\pm$ 1.11) & 86.30 ($\pm$ 2.73) & 84.35 ($\pm$ 2.24) & 87.07 ($\pm$ 1.64) \\
            \( K \) = 2, \( T \) = 3 & 93.59 ($\pm$ 1.24) & 88.37 ($\pm$ \textbf{0.49}) & \textbf{82.09} ($\pm$ \textbf{0.81}) & 86.30 ($\pm$ 2.64) & 84.02 ($\pm$ 2.24) & 86.87 ($\pm$ 1.48) \\
            \( K \) = 4, \( T \) = 3 & \textbf{93.83}($\pm$ 1.10) & 89.07 ($\pm$ 0.53) & 81.96 ($\pm$ 1.15) & \textbf{87.26} ($\pm$ \textbf{2.23}) & \textbf{85.17} ($\pm$ 1.71) & \textbf{87.46} ($\pm$ \textbf{1.34}) \\
            \( K \) = 8, \( T \) = 3 & \textbf{93.83} ($\pm$ \textbf{0.70}) & 88.99 ($\pm$ 0.59) & 82.69 ($\pm$ 0.97) & 86.25 ($\pm$ 2.75) & 84.36 ($\pm$ 2.06) & 87.22 ($\pm$ 1.41) \\
            \( K \) = 1, \( T \) = 6 & 93.37 ($\pm$ 1.47) & 88.32 ($\pm$ 0.68) & 81.05($\pm$ 3.44) & 84.40 ($\pm$ 2.30) & 83.34($\pm$ \textbf{1.66}) & 86.10 ($\pm$ 1.91) \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
\end{table*}

\begin{figure*}[t]
\centering
\includegraphics[width=0.9\textwidth]{figure-5}
\caption{Scaling Law of Training Prompt Numbers: Mean and Standard Deviation of Accuracy Across Different Datasets. The x-axis represents the number of prompts on a logarithmic scale, while the y-axis shows the mean accuracy (left) and standard deviation of accuracy (right) for each dataset.}
\label{fig:baseline}
\vspace{-3mm}
\end{figure*}
\paragraph{\ours{} achieves state-of-the-art performance}
As shown in Table~\ref{tab:accuracy_comparison}, Figure~\ref{fig:main_result}, and Figure~\ref{fig:baseline}, \ours{} achieves the highest average accuracy across all evaluated reasoning and reading comprehension tasks, significantly outperforming other baseline models. Specifically, \ours{} surpasses other methods on tasks such as HellaSwag, PIQA, Winogrande, RACE, demonstrating its excellent performance across diverse natural language processing tasks. This superior performance stems from \ours{}'s prompt robustness, enabling the model to better grasp the core essence of each task and maintain high performance across diverse prompt formulations. For instance, strong performance of \ours{} on the open text generation task (HellaSwag) can be attributed to its dynamic prompt selection strategy, facilitating improved capture of contextual information. Its success on the physical common sense reasoning task (PIQA) can be attributed to its enhanced ability to utilize common sense knowledge. Similarly, its performance on the reference resolution task (Winogrande) can be attributed to its improved understanding of sentence structure and semantic relations, while its success on the reading comprehension task (RACE) can be attributed to its improved capture of topic and key information. In essence, this performance gain arises from \ours{}'s decoupling of the prompt from the task itself, allowing the model to focus on learning the fundamental aspects of the downstream tasks.

\paragraph{\ours{} enhances inference efficiency}
In addition to robustness and performance, \ours{} also significantly enhances inference efficiency. By fundamentally enhancing the ability of model to understand the core semantics of tasks, \ours{} enables the model to solve problems more effectively, generating fewer tokens. This capability directly translates to faster inference speeds, as the model avoids redundant or unnecessary outputs and focuses on concise, accurate responses. To quantify this improvement, we measured the average end-to-end inference time across all test prompts and datasets, from the input prompt to the final output. As shown in Table~\ref{tab:infer_time_comparison}, models trained with \ours{} consistently achieve the fastest inference speeds compared to the baseline methods. This improvement is a direct result of \ours{}'s inherent prompt robustness. By decoupling model performance from the specific prompt wording, \ours{} operates consistently and efficiently regardless of the input prompt. In essence, \ours{} promotes more effective generalization and eliminates the need for prompt-specific adaptation during inference. Additionally, our training regime covers a wide range of prompt wordings, avoiding the potential performance degradation or increased computation typically required to handle unexpected or unevenly distributed prompts during inference. This consistency and efficiency is especially valuable in real-world applications that require fast response times, such as dialogue systems or time-sensitive information retrieval. Our enhanced inference efficiency translates to a better user experience and reduced computational resources required for deployment, making it a more practical and scalable solution.


\subsection{Ablation Studies} \label{ablation study}
\paragraph{Hyperparameter robustness} This ablation study demonstrates the robustness of \ours{} to the hyperparameters \(K\) (iterations per prompt) and \(T\) (epochs). As shown in Table~\ref{tab:kt_comparison}, \ours{} achieves stable performance across a broad range of \(K\) (1 to 8) and \(T\) (3 to 6) values, with minimal fluctuations in accuracy and variance. Notably, \ours{} achieves near-optimal performance with default settings (\(K=4\), \(T=3\)), attaining an average accuracy of 87.46\%(\(\pm 1.34\)) across all tasks. This robustness reduces the need for extensive hyperparameter tuning, making \ours{} a practical and efficient solution for real-world applications.


\paragraph{\ours{} achieves strong performance with limited training prompts}
We conduct an ablation study to investigate the impact of varying numbers of training prompts on model performance, thus validating the effectiveness of \ours{}. The experimental results, shown in Figure~\ref{fig:number of prompts}, demonstrate that as the number of prompts increases, the average accuracy of the model significantly improves, while the standard deviation decreases, indicating more stable and reliable performance. However, the performance gains diminish as the number of prompts increases, with only marginal improvements observed beyond a certain threshold. This suggests that while adding prompts can enhance performance, \ours{} achieves competitive results with a minimal number of prompts, rendering excessive prompts unnecessary. In most cases, \ours{} achieves strong performance with as few as 10 high-quality prompts, and further increases yield only marginal gains. The efficiency of \ours{} is particularly notable, as it delivers excellent performance with a minimal number of prompts, making it highly suitable for resource-constrained scenarios where computational efficiency is critical. These findings underscore the practicality and efficiency of \ours{}, offering a robust and efficient solution for real-world applications.




\section{Conclusion}
\ours{} offers a compelling solution for enhancing the prompt robustness of LLMs. By dynamically adjusting prompts during fine-tuning, \ours{} significantly improves model generalization and performance across diverse prompt formulations. Notably, \ours{} boosts inference speed with maintained training cost. This approach paves the way for more reliable and efficient LLM deployment in real-world applications.

\newpage
\section*{Limitations}
In this section, we discuss potential limitations of \ours{} and outline promising directions for future research. While \ours{} demonstrates significant progress in enhancing the prompt robustness of Large Language Models (LLMs), certain aspects warrant further investigation. A key area for improvement lies in the dynamic prompt selection strategy employed during fine-tuning.  Currently, \ours{} utilizes a random sampling approach, which, while exposing the model to a diverse range of prompts, may not be the most efficient or effective method.  Exploring more sophisticated sampling techniques, such as curriculum learning or importance sampling, could potentially optimize the training process and further enhance robustness. For instance, prioritizing prompts that induce higher loss or those that are more representative of the overall prompt distribution could lead to faster convergence and improved generalization. Furthermore, integrating adversarial learning into the dynamic fine-tuning phase presents a compelling avenue for future work. Generating adversarial prompts on-the-fly, perhaps through gradient-based updates, could further challenge the model and encourage it to learn more robust task representations. This approach could be particularly beneficial in mitigating the impact of maliciously crafted or unexpected prompts. However, the well-known instability of adversarial training remains a significant hurdle.  Stabilizing the training process, perhaps through techniques like robust optimization or regularization, is crucial for realizing the full potential of this approach.  Investigating different adversarial prompt generation strategies and their impact on model robustness would be a valuable contribution.



\section*{Ethics Statement}
We have manually reevaluated the dataset we created to ensure it is free of any potential for discrimination, human rights violations, bias, exploitation, and any other ethical concerns.
% Entries for the entire Anthology, followed by custom entries
\newpage

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\appendix
\onecolumn
\label{sec:appendix}
\section{Experimental setting}\label{app:setup}
In the main experiment, we compared \ours{} with the baseline. The datasets and experimental parameters are as follows:
\subsection{Dataset}

In this section, we introduce the statistics of the dataset. The statistics of the dataset are shown in Table~\ref{tab:Statistics For Data}. 

\begin{table*}[h]
% \scriptsize
\centering
\caption{Number of samples in the train, validation, and test datasets for various dateset.}
\label{tab:Statistics For Data}
\begin{tabular}{l*{4}{c}}
\toprule
\textbf{Number of samples} &\textbf{train dataset} & \textbf{validation dataset} & \textbf{test dataset} \\ 
\midrule
Hellaswag & 39900 & 10000 & 10000  \\
 PIQA & 16000 & 2000 & 3000  \\
Winogrande & 40398 & 1267 & 1767  \\
 RACE & 87866 & 4887 & 4934  \\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Specific experimental parameters} 
Based on the LLaMA3-8B model configuration, several adjustments were made to optimize model performance. In the baseline model experiment, generation parameters were adjusted to ensure the correct output. In the LoRA experiment, adjustments to the generation parameters were retained, and LoRA-related parameters were adjusted. In the \ours{} experiment, the size of the validation set was adjusted to control the time required to search for the optimal layer. For specific experimental parameters, see the table~\ref{tab:parameters}.


\begin{table*}[h]
\centering
% \scriptsize
\caption{Detailed experimental parameters. This table lists the specific parameters we used in the experiments for various methods. These parameters include the target module of LoRA (Lora Target), the maximum sequence length (Max Length), the number of samples for supervised fine-tuning (SFT Samples), the learning rate (LR), the number of training prompts (Training Prompts). Epoch(Epoch) represents the epoch of training. All other parameters not listed here remain consistent across all experiments. }
\label{tab:parameters}
\begin{tabular}{l*{7}{c}}
\toprule
\textbf{Methods} &\textbf{LoRA Target} & \textbf{Max Length} & \textbf{SFT Samples} & \textbf{LR} & \textbf{Training Prompts} & \textbf{Epoch} \\ 
\midrule
LoRA & q \& v Proj & 1024 & 20000 & 0.0001 & 1 & 3 \\
\midrule
\ours{} & q \& v Proj & 1024 & 20000 & 0.0001 & 400 & 3\\
\bottomrule
\end{tabular}
\end{table*}

\section{Training cost and inference time}\label{app:train time}
\paragraph{\ours{} Maintains Training Efficiency} We now turn our attention to the training efficiency of \ours{}. A critical consideration for any practical fine-tuning approach is its impact on training time.  Introducing complex mechanisms or additional computational overhead can significantly hinder the training process, especially when dealing with large language models and extensive datasets.  Therefore, it is essential to demonstrate that \ours{} does not introduce such burdens.

To rigorously evaluate the training time implications of \ours{}, we conducted a series of experiments, using Low-Rank Adaptation (LoRA) \cite{hu2022lora} as a representative example of a parameter-efficient fine-tuning method. LoRA has gained popularity due to its ability to adapt pre-trained models with minimal computational cost, making it a suitable baseline for our analysis.  Our experiments, the results of which are presented in Table 3, directly compare the training time required for traditional LoRA fine-tuning with the training time required for \ours{} integrated with LoRA.

The key finding from our analysis is that \ours{} does not introduce any noticeable increase in training time.  The data in Table~\ref{tab:time_comparison} clearly demonstrates that the training duration remains virtually identical whether we employ standard LoRA or incorporate \ours{}'s dynamic prompt selection mechanism. This crucial observation underscores the efficiency of \ours{}.  The dynamic prompt selection process, which is central to \ours{}'s ability to enhance prompt robustness, is implemented in a way that does not add significant computational overhead.  This is because the selection process is lightweight and seamlessly integrated into the existing training loop.  Rather than requiring complex computations or extensive data manipulations, \ours{} efficiently chooses from a diverse set of prompts, allowing the model to experience a wider range of input formulations without incurring a substantial time penalty.  This efficient dynamic prompt selection is critical for the practical applicability of \ours{}, ensuring that it can be readily deployed without compromising training efficiency.  Furthermore, this efficiency allows for more extensive experimentation and exploration of different prompt variations, ultimately leading to more robust and generalizable models.

\begin{table*}[t!]
    \centering
    % \scriptsize
    \caption{Training Time Comparison of Different Fine-tuning Methods on the Test Prompt Sets Across Various Reasoning and Reading Comprehension Tasks Using the LLaMA3-8B\cite{llama3} Model with LoRA Rank 8. Experiments were conducted on an NVIDIA RTX 4090 GPU. Results are reported as training time in hours. \textbf{LoRA + TopAccuracy prompt } prompt refers to the prompt with the highest accuracy in the training set, \textbf{LoRA + user-specified prompt }\cite{wei2024flexora} refers to fine-tuning with human-designed prompts, \textbf{LoRA + BATprompt} \cite{shi2024robustnessawareautomaticpromptoptimization} uses the most robust prompt generated by BATprompt, and \textbf{LoRA + ZOPO prompt} \cite{hu2024localized} employs the optimal prompt selected by ZOPO from the training prompt set.}
     
    \label{tab:time_comparison}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l*{6}{c}}
            \toprule
            \textbf{Training time/h} & \textbf{Hellaswag} & \textbf{PIQA} & \textbf{Winogrande} & \textbf{RACE} & \textbf{Average} \\
            \midrule
            LoRA + user-specified prompt & 3.01 & 2.35 & 3.27 & 3.95 & 3.15 \\
            LoRA + TopAccuracy prompt & 3.00 & 2.29 & 2.98 & 3.93 & 3.05 \\
            LoRA + BATprompt & 3.02 & \textbf{2.23} & 3 & 3.93 & 3.05 \\
            LoRA + ZOPO prompt & \textbf{2.97} & 2.3 & \textbf{2.97} & 3.83 & \textbf{3.02} \\
            \midrule
            \textbf{\ours{} } & 2.98 & 2.32 & 3.38 & \textbf{3.81} & 3.12 \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
\end{table*}

\paragraph{Efficient Candidate Prompt Generation}  A key aspect of \ours{}'s effectiveness lies in its ability to generate a diverse and high-quality set of candidate prompts efficiently.  The process of constructing these candidate prompts involves leveraging the capabilities of external large language models (LLMs), which naturally raises the question of associated costs.  Specifically, we sought to quantify the token usage required for candidate prompt generation, as this directly translates to the expense incurred when interacting with commercial LLM APIs.

To address this, we conducted a detailed analysis of the token consumption during the candidate prompt generation phase of \ours{}.  Our investigation, the results of which are summarized in Table 1, focuses on the number of tokens required to produce a sufficient variety of prompts suitable for subsequent selection and fine-tuning.  We meticulously tracked the token usage across various prompts generated for different tasks, considering factors such as prompt length, complexity, and diversity.

The findings presented in Table~\ref{tab:tokens} demonstrate that \ours{} requires remarkably few tokens to generate a substantial pool of candidate prompts.  This efficiency stems from \ours{}'s strategic approach to prompt engineering.  Rather than relying on brute-force generation or computationally intensive search methods, \ours{} employs a carefully designed prompting strategy that encourages the external LLMs to produce a wide range of prompt formulations with minimal token consumption.  This is achieved through techniques such as few-shot prompting with carefully chosen examples,  targeted instructions that guide the LLM towards desired prompt characteristics, and potentially iterative refinement of prompts based on preliminary evaluation.  The low token count is crucial for practical applications, as it minimizes the cost associated with using commercial LLM APIs.  Moreover, this efficiency enables the exploration of a broader range of potential prompts within a fixed budget, increasing the likelihood of discovering highly effective prompts that contribute to improved model robustness.  This efficient prompt generation process is a significant advantage of \ours{}, enabling it to achieve superior performance without incurring prohibitive costs.

\begin{table*}[t!]
    \centering
    \scriptsize
    \caption{Token Usage for Candidate Prompt Generation. This table shows the number of tokens used to generate approximately 400 candidate prompts for each task. The average token usage is 11.75k. The number of generated prompts can be adjusted based on the scaling law observed in Figure~\ref{fig:number of prompts} to control costs.}
    \label{tab:tokens}
    \resizebox{\textwidth}{!}{
        \begin{tabular}{l*{6}{c}}
            \toprule
            \textbf{Tokens} & \textbf{Hellaswag} & \textbf{PIQA} & \textbf{Winogrande} & \textbf{RACE} & \textbf{Average} \\
            \midrule
            Total Tokens & 11.7k & 12.1k & 10.9k & 12.3k & 11.75k \\
            \bottomrule
        \end{tabular}
    }
    \vspace{-3mm}
\end{table*}
\section{Prompt}\label{app:prompt}
In this section, we present a selection of training and test prompts to illustrate the efficacy of our prompt construction algorithm and to provide a clearer understanding of operational process of \ours{}. Due to space constraints, we only list 10 prompts as examples. Section~\ref{train prompt} showcases examples of training prompts, Section~\ref{test prompt} highlights test prompts, and Section~\ref{baseline prompt} outlines the prompts utilized by the baseline method.
\subsection{Train prompt}\label{train prompt}
In this section, we present the prompts generated using the method outlined in Section~\ref{sec:prompt} across various datasets. All prompts listed here are utilized for training purposes.
\begin{tcolorbox}[title={\textbf{\small Train Prompt of Hellaswag}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1. Based on the given context {ctx}, which of the following options correctly predicts the outcome?
Choose the correct letter option.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
2. Considering the scenario described in {ctx}, identify the most accurate prediction of the 
final result:Select the correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
3. Given the information in {ctx}, which option best forecasts the correct ending?Provide the 
correct letter choice.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
4. From the context {ctx}, which of the following options accurately predicts the conclusion?Write
down the correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
5. Using the details provided in {ctx}, select the option that correctly predicts the final outcome:
Enter the correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
6. Based on the context {ctx}, which option is the most accurate prediction of the ending?Choose the
correct letter option.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
7. Given the scenario in {ctx}, identify the option that correctly forecasts the outcome:Select the
correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
8. Considering the details in {ctx}, which option best predicts the correct conclusion?Provide the
correct letter choice.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
9.Analyze the context {ctx} and determine the correct prediction of the outcome:Indicate the 
correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
10. Analyze the given context {ctx} and determine the most accurate prediction of the final result:
Indicate the correct letter.\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Train Prompt of PIQA}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.In order to {goal}, which of the following options is the most logical choice based on common
knowledge?\nA. {sol1}\nB. {sol2}\nAnswer:
2.Consider the scenario where you need to {goal}. Which option would be the most appropriate 
according to general understanding?\nA. {sol1}\nB. {sol2}\nAnswer:
3.When trying to {goal}, which of the following would be the best course of action based on everyday
reasoning?\nA. {sol1}\nB. {sol2}\nAnswer:
4.To achieve {goal}, which option aligns best with common sense?\nA. {sol1}\nB. {sol2}\nAnswer:
5.Based on typical knowledge, which of the following is the correct choice to {goal}?
\nA. {sol1}\nB. {sol2}\nAnswer:
6.If you want to {goal}, which of these options would be the most sensible according to common
reasoning?\nA. {sol1}\nB. {sol2}\nAnswer:
7.Using general knowledge, determine the best option to {goal}.\nA. {sol1}\nB. {sol2}\nAnswer:
8.To {goal}, which of the following choices is the most reasonable based on common sense?
\nA. {sol1}\nB. {sol2}\nAnswer:
9.When considering how to {goal}, which option would be the most logical based on everyday knowledge?
\nA. {sol1}\nB. {sol2}\nAnswer:
10.According to common reasoning, which of the following is the best way to {goal}?
\nA. {sol1}\nB. {sol2}\nAnswer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Train Prompt of Winogrande}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.Choose the correct answer to complete the sentence.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
2.elect the appropriate option to fill in the blank.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
3.Fill in the blank with the correct answer.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
4.Identify the correct choice to complete the statement.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
5.Choose the right answer to fill in the gap .{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
6.Select the correct option to complete the sentence.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
7.Fill in the blank with the correct answer.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
8.Identify the correct choice to complete the sentence.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
9.Choose the right answer to fill in the blank. {ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
10.Select the appropriate option to complete the statement.{ctx}
\nA. {only_option1}\nB. {only_option2}\nAnswer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Train Prompt of RACE}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.Carefully read the following article and answer the question by selecting the correct option.
Respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
2.Read the passage below and choose the best answer to the question.
Reply with the letter A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
3.After reading the article, answer the following question by selecting the correct option.
Please respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
4.Examine the article provided and answer the question by choosing the most appropriate option.
Reply with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
5.Read the following text and answer the question by selecting the correct letter.
Respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
6.Carefully read the article and choose the best answer to the question.
Reply with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
7.Read the passage and answer the question by selecting the correct option.
Respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
8.After reading the article, choose the correct answer to the question.
Reply with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
9.Read the provided text and answer the question by selecting the best option.
Respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
10.Examine the article and answer the question by choosing the correct letter.
zReply with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
\end{lstlisting}
}
\end{tcolorbox}
\subsection{Test prompt}\label{test prompt}
In this section, we present the prompts generated using the method outlined in Section~\ref{sec:prompt} across various datasets. All prompts listed here are utilized for testing purposes, and they are not visible during training.
\begin{tcolorbox}[title={\textbf{\small Test Prompt of Hellaswag}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.Based on the information provided, please select the most probable conclusion: {ctx}
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n 
Remember to consider the implications of each option. Answer:
2.In the scenario described by {ctx}, there is only one correct way the story or situation could end.
When predicting the right ending, consider the cause-and-effect relationships established within 
the context.An option that logically follows from the preceding events is likely the correct one.
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n You may choose from 'A', 'B', 'C', 'D'.\n Answer:
3.Based on the given context {ctx}, which of the following options correctly predicts the outcome?
Choose the correct letter option.
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
4.To solve this problem based on {ctx}, weigh the significance of each potential ending:
A. {A}\nB. {B}\nC. {C}\nD. {D}\n You may choose from 'A', 'B', 'C', 'D'.\n Answer:
5.Analyzing the context of {ctx}, think about the relationships and conflicts presented.
Which option is most likely to resolve these issues and lead to a satisfying ending?
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
6.{ctx}\nQuestion: Taking into account the context, which outcome is the most expected?
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
7.From the detailed description provided, choose the option that best completes the scenario:{ctx}\
n A. {A}\nB. {B}\nC. {C}\nD. {D}\n 
Consider all aspects of the scenario to make an informed decision on the correct ending.\n Answer:
8.Given the scenario described in {ctx}, which of the following conclusions seems most plausible? 
Consider all the details and clues provided to make an informed guess.
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
9.To unlock the hidden treasure in {ctx}, you need to choose the correct key.
Which option will open the treasure chest?
A. {A} B. {B} C. {C} D. {D}\n You may choose from 'A', 'B', 'C', 'D'.\n Answer:
10.{ctx}\nQuestion: Reflecting on the emotional stakes and the structure of the narrative, 
which conclusion feels the most genuine?
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n Answer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Test Prompt of PIQA}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.Solve the following single-choice question by using your common sense reasoning skills.
Choose the correct option and reply with the corresponding letter.
\nQuestion: {goal}\nA. {sol1}\nB. {sol2}\nAnswer:
2.For the situation described by {goal}, consider which solution aligns more closely with how things 
usually work in real life: A. {sol1}\nB. {sol2}. Use logical reasoning to guide your choice. Answer:
3.Given the context of the question, choose the answer that demonstrates the best common 
sense reasoning: {goal}\nA. {sol1}\nB. {sol2}\n Answer format: A/B \nAnswer:
4.In considering the aim set forth in {goal}, visualize the potential consequences of each action 
as if you were directly involved. This visualization can help you identify the better choice:\n
Question: {goal}\nA. {sol1}\nB. {sol2}\nAnswer:
5.Which solution fits the goal based on common sense?
{goal}\n A. {sol1}\nB. {sol2}\n Answer format: A/B \nAnswer:
6.Analyze the following scenario and select the answer that reflects logical reasoning: {goal}
\nA. {sol1}\nB. {sol2}\n Answer format: A/B \nAnswer:
7.Identify the most logical outcome for the situation described: {goal} A. {sol1} B. {sol2} 
Answer format: A/B Remember, the trick is to apply your general knowledge to the scenario. Answer:
8.According to common reasoning, which of the following is the best way to {goal}?
\nA. {sol1}\nB. {sol2}\nAnswer:
9.Which solution best fits the goal based on your general knowledge? {goal}
\n A. {sol1}\nB. {sol2}\n Answer format: A/B \nAnswer:
10.You are about to answer a question that relies on your understanding of basic logic.
Please respond with A or B to indicate your choice.
\nQuestion: {goal}\nA. {sol1}\nB. {sol2}\nAnswer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Test Prompt of Winogrande}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.In the context of {prompt}, which word best completes the sentence? 
Choose: A. {only_option1}. B. {only_option2}.\nAnswer:.
2.When analyzing {prompt}, think about the overall theme. What fits best? 
A. {only_option1}. B. {only_option2}.\nAnswer:.
3.For {prompt}, consider the emotional tone. Which option resonates more?
A. {only_option1}. B. {only_option2}.\nAnswer:.
4.Reflect on {prompt}. Which word logically fills the gap?
A. {only_option1}. B. {only_option2}.\nAnswer:.
5.In {prompt}, which choice aligns with the preceding ideas?
A. {only_option1}. B. {only_option2}.\nAnswer:.
6.When faced with {prompt}, think about the context. What completes it best?
A. {only_option1}. B. {only_option2}.\nAnswer:.
7.For {prompt}, identify the word that maintains the flow of the sentence.
Choose: A. {only_option1}. B. {only_option2}.\nAnswer:.
8.In the case of {prompt}, which option best conveys the intended meaning?
A. {only_option1}. B. {only_option2}.\nAnswer:.
9.Analyze {prompt} for clues. Which word fits the context?
A. {only_option1}. B. {only_option2}.\nAnswer:.
10.When considering {prompt}, which option enhances the clarity of the statement? 
A. {only_option1}. B. {only_option2}.\nAnswer:.
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Test Prompt of RACE}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
1.After reading the article, analyze the question and choose the best answer
based on the details and themes discussed. Look for clues within the text that
align with one of the options.\nArticle:\n{article}\n\nQuestion:
{question}\nOptions: \nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
2.Article:\n{article}\nAfter reading the passage, please answer the following question:
\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:
3.Carefully read the following article and answer the question by selecting the correct option.
Respond with A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
4.Read the text carefully and answer the question by choosing the most appropriate option.
Evaluate the relevance of each choice to the main points discussed.
\nArticle:\n{article}\n\nQuestion: {question}\nOptions: \nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer:
5.Describe the setting of the article. 
{question}\n{article}\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:
6.While reading the {article}, highlight or make mental notes of significant details. 
The {question} is asking [describe the specific query]. 
Now evaluate the options:\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:
7.After carefully analyzing {article}, determine which of the following options best 
answers the question: 
{question}. A. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:
8.Read {article} with a focus on answering {question}. Choose the most suitable option. 
Article: {article} Question:{question} Options: A. {A} B. {B} C. {C} D. {D} 
Trick: Be cautious of answer choices that seem too extreme. Your answer is just one letter. Answer:
9.Article:\n{article}\nFrom the information in the article, identify the correct 
answer to the following question: \n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:
10.When {article} mentions {question}, which option best describes the author's attitude?
\nA. {A}\nB. {B}\nC. {C}\nD. {D} \n// Pay attention to the tone of the author.
Look for words that convey emotions or opinion to determine the attitude.Answer:
\end{lstlisting}
}
\end{tcolorbox}
\subsection{Baseline prompt}\label{baseline prompt}
In this section, we present the best prompts generated or filtered using the baseline for training.
\begin{tcolorbox}[title={\textbf{\small Prompt of Hellaswag}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
TopAccuracy prompt: 
Given the context {ctx}, predict the correct ending by choosing the most logical option.
\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n You may choose from 'A', 'B', 'C', 'D'.\n Answer:

User-specified prompt: 
{ctx}\n Question: {Question}\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n 
You may choose from 'A', 'B', 'C', 'D'.\n Answer:

BATprompt : 
Given the context below, predict the most logical ending by choosing the correct option 
from the provided choices. Ensure your choice aligns with the context and is the most coherent
conclusion. \n Context: {ctx}\n 
Question: Which ending makes the most sense?\n A. {A}\nB. {B}\nC. {C}\nD. {D}\n 
You may choose from 'A', 'B', 'C', 'D'.\n Answer:

ZOPO prompt:
Based on {ctx}, which option is the most likely correct ending?
Consider the overall context, character motivations, and any foreshadowing. 
Trick: Analyze the consistency of each option with the established details. 
A. {A}\nB. {B}\nC. {C}\nD. {D}\n You may choose from 'A', 'B', 'C', 'D'.\n Answer:

\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Prompt of PIQA}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
TopAccuracy prompt: 
Use both common sense and logical reasoning to determine the correct solution for the goal:
{goal}\n A. {sol1}\nB. {sol2}\n Answer format: A/B \nAnswer:

User-specified prompt: 
There is a single choice question. Answer the question by replying A or B.'\n 
Question: {goal}\nA. {sol1}\nB. {sol2}\nAnswer:

BATprompt : 
You should use both common sense and logical reasoning to determine the most appropriate 
solution for the following goal. Carefully evaluate the provided options and choose the 
one that best aligns with the goal. Goal: {goal}\nA. {sol1}\nB. {sol2}\nAnswer:

ZOPO prompt:
To solve this common sense reasoning question, consider which of the two options seems 
more plausible based on everyday knowledge and logic.
\nQuestion: {goal}\nA. {sol1}\nB. {sol2}\n
Think about the practical implications of each choice to determine the correct answer.\nAnswer:
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Prompt of Winogrande}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
TopAccuracy prompt: 
Question: {prompt}\nA. {only_option1}\nB. {only_option2}\nAnswer:

User-specified prompt: 
There is a single choice question,  you need to choose the correct option to fill in the blank. 
Answer the question by replying A or B.\n 
Question:{prompt}\nA. {only_option1}\nB. {only_option2}\nAnswer:

BATprompt : 
Complete the following sentence by selecting the most contextually appropriate option. 
Carefully consider the meaning and context of the sentence to make your choice. 
Question: {prompt}\nA. {only_option1}\nB. {only_option2}\nAnswer:

ZOPO prompt:
Question: Choose the correct modal verb: {prompt}\nA. {only_option1}\nB. {only_option2}\nAnswer:.
\end{lstlisting}
}
\end{tcolorbox}
\begin{tcolorbox}[title={\textbf{\small Prompt of RACE}},
colback=whitesmoke, colframe=darkblue, , boxrule=2pt, arc=0mm]
{\scriptsize
\begin{lstlisting}[style=mystyle]
TopAccuracy prompt: 
Read the following article carefully: {article}. After reading, answer the question: {question}. 
Choose the correct option from the choices provided: 
\nA. {A}\nB. {B}\nC. {C}\nD. {D} \n 
Trick: Focus on the main idea and supporting details in the article. 
Output: Only the letter of the correct answer.\nAnswer:

User-specified prompt: 
Article:\n{article}\nQuestion:\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:

BATprompt : 
Please read the passage carefully, focusing on the main ideas and supporting details. 
Answer the question that follows by choosing the best option from the choices provided. 
Ensure your response is based solely on the information in the passage. Output only the 
letter of the correct answer. Article:\n{article}
\nQuestion:\n{question}\nA. {A}\nB. {B}\nC. {C}\nD. {D} \nAnswer:

ZOPO prompt:
A reading comprehension question is before you. Read the article and answer the question 
by selecting A, B, C, or D.\n\nArticle:\n{article}\n\n
Q: {question}\n\nA. {A}\nB. {B}\nC. {C}\nD. {D}\nAnswer: 
\end{lstlisting}
}
\end{tcolorbox}
\end{document}
