[
  {
    "index": 0,
    "papers": [
      {
        "key": "chang2024efficient",
        "author": "Kaiyan Chang and Songcheng Xu and Chenglong Wang and Yingfeng Luo and Xiaoqian Liu and Tong Xiao and Jingbo Zhu",
        "title": "Efficient Prompting Methods for Large Language Models: A Survey"
      },
      {
        "key": "li-2023-practical",
        "author": "Li, Yinheng",
        "title": "A Practical Survey on Zero-Shot Prompt Design for In-Context Learning"
      },
      {
        "key": "diao2023blackbox",
        "author": "Shizhe Diao and Zhichao Huang and Ruijia Xu and Xuechun Li and LIN Yong and Xiao Zhou and Tong Zhang",
        "title": "Black-Box Prompt Learning for Pre-trained Language Models"
      },
      {
        "key": "sun2022bbt",
        "author": "Tianxiang Sun and Yunfan Shao and Hong Qian and Xuanjing Huang and Xipeng Qiu",
        "title": "Black-Box Tuning for Language-Model-as-a-Service"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "lin2024use",
        "author": "Xiaoqiang Lin and Zhaoxuan Wu and Zhongxiang Dai and Wenyang Hu and Yao Shu and See-Kiong Ng and Patrick Jaillet and Bryan Kian Hsiang Low",
        "title": "Use Your {INSTINCT}: {INST}ruction optimization for {LLM}s usIng Neural bandits Coupled with Transformers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "hu2024localized",
        "author": "Wenyang Hu and Yao Shu and Zongmin Yu and Zhaoxuan Wu and Xiaoqiang Lin and Zhongxiang Dai and See-Kiong Ng and Bryan Kian Hsiang Low",
        "title": "Localized Zeroth-Order Prompt Optimization"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "shi2024robustnessawareautomaticpromptoptimization",
        "author": "Zeru Shi and Zhenting Wang and Yongye Su and Weidi Luo and Fan Yang and Yongfeng Zhang",
        "title": "Robustness-aware Automatic Prompt Optimization"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "li-liang-2021-prefix",
        "author": "Li, Xiang Lisa  and\nLiang, Percy",
        "title": "Prefix-Tuning: Optimizing Continuous Prompts for Generation"
      },
      {
        "key": "liu-etal-2022-p",
        "author": "Liu, Xiao  and\nJi, Kaixuan  and\nFu, Yicheng  and\nTam, Weng  and\nDu, Zhengxiao  and\nYang, Zhilin  and\nTang, Jie",
        "title": "{P}-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "shu2024ferret",
        "author": "Yao Shu and Wenyang Hu and See-Kiong Ng and Bryan Kian Hsiang Low and Fei Richard Yu",
        "title": "Ferret: Federated Full-Parameter Tuning at Scale for Large Language Models"
      },
      {
        "key": "ouyang2022training",
        "author": "Long Ouyang and Jeffrey Wu and Xu Jiang and Diogo Almeida and Carroll Wainwright and Pamela Mishkin and Chong Zhang and Sandhini Agarwal and Katarina Slama and Alex Gray and John Schulman and Jacob Hilton and Fraser Kelton and Luke Miller and Maddie Simens and Amanda Askell and Peter Welinder and Paul Christiano and Jan Leike and Ryan Lowe",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "liu2021pretrainpromptpredictsystematic",
        "author": "Pengfei Liu and Weizhe Yuan and Jinlan Fu and Zhengbao Jiang and Hiroaki Hayashi and Graham Neubig",
        "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing"
      },
      {
        "key": "lester-etal-2021-power",
        "author": "Lester, Brian  and\nAl-Rfou, Rami  and\nConstant, Noah",
        "title": "The Power of Scale for Parameter-Efficient Prompt Tuning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "hu2022lora",
        "author": "Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen",
        "title": "Lo{RA}: Low-Rank Adaptation of Large Language Models"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "chen2023lorashearefficientlargelanguage",
        "author": "Tianyi Chen and Tianyu Ding and Badal Yadav and Ilya Zharkov and Luming Liang",
        "title": "LoRAShear: Efficient Large Language Model Structured Pruning and Knowledge Recovery"
      },
      {
        "key": "si2024unleashingpowertaskspecificdirections",
        "author": "Chongjie Si and Zhiyi Shi and Shifan Zhang and Xiaokang Yang and Hanspeter Pfister and Wei Shen",
        "title": "Unleashing the Power of Task-Specific Directions in Parameter Efficient Fine-tuning"
      },
      {
        "key": "wei2024flexora",
        "author": "Chenxing Wei and Yao Shu and Ying Tiffany He and Fei Richard Yu",
        "title": "Flexora: Flexible Low-Rank Adaptation for Large Language Models"
      }
    ]
  }
]