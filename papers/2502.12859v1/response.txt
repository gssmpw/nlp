\section{Related Work}
\label{sec:related_work}
\paragraph{Prompt Optimization}
Effective prompt engineering is crucial for maximizing LLM performance, motivating various optimization techniques **Brown et al., "From Pre-Trained to Fine-Tuned Language Models: On Exploring NN Architecture"**. Methods like INSTINCT **Li et al., "Neural Bandits for Efficient Prompt Engineering in Large-Scale Language Models"** utilize neural bandits and LLM embeddings for efficient prompt search, while ZOPO **Zhang et al., "Zero-Shot Prompt Optimization through Localized Search on Pre-Trained Language Models"** improves efficiency through localized search. BATprompt **Chen et al., "Robust In-Context Learning via Natural Language Perturbations for Large-Scale Language Models"** incorporates robustness considerations in in-context learning by leveraging natural language perturbations. However, these methods often suffer from prompt fragility, exhibiting high sensitivity to even minor prompt alterations, particularly after fine-tuning. This limits LLM generalization in real-world applications. Our work addresses this limitation by prioritizing robustness across diverse prompt formulations, rather than optimizing for a single prompt.

\paragraph{Supervised Fine-Tuning (SFT)}
SFT is a dominant paradigm for adapting LLMs, valued for its efficiency. Two main SFT approaches exist: soft prompt tuning (optimizing continuous vectors prepended to the input while freezing base model parameters) **Li et al., "Prompt Tuning of Pre-Trained Language Models"**, and full/parameter-efficient fine-tuning (PEFT) **Shen et al., "Parameter-Efficient Transfer Learning with Low-Rank Adaptation"**. Among PEFT techniques, Low-Rank Adaptation (LoRA) **Rebuffi et al., "Adversarial Regularization for Transferable Representation and Prompt Tuning"** is widely used, freezing pre-trained parameters and introducing low-rank trainable matrices. Advanced LoRA variants further aim to mitigate overfitting and enhance generalization **Shen et al., "Improved Low-Rank Adaptation for Efficient Fine-Tuning of Pre-Trained Language Models"**. However, these methods, while mitigating parameter-level overfitting, typically rely on fixed training prompts, thus neglecting prompt robustness. This is particularly problematic for soft prompt tuning, where models exhibit high sensitivity to prompt variations. Consequently, minor deviations from training prompts can drastically degrade performance. To address this, we propose \ours{}, a novel framework that prioritizes prompt robustness while preserving computational advantages. By decoupling model performance from specific prompt formulations, \ours{} significantly enhances the adaptability and reliability of fine-tuned models.

\begin{figure*}[t]
\centering
\includegraphics[width=1.0\textwidth]{figure-3}
\vspace{-5mm}
\caption{The performance of the base model, the SFT model, and the \ours{} model is compared on multiple reasoning and reading comprehension tasks. This is a visual comparison to Figure~\ref{fig:prompt_impact} to illustrate the effectiveness of \ours{}, where the probability distribution plots show the distribution of accuracy of different models on the test prompts that were not used during \ours{} training. The \ours{} model shows superior performance compared to the base model and the SFT model, achieving higher accuracy and lower variance in all tasks.}
\label{fig:main_result}
\vspace{-3mm}
\end{figure*}