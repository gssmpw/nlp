1
@article{chadi2023understanding,
  title={Understanding reinforcement learning algorithms: The progress from basic Q-learning to proximal policy optimization},
  author={Chadi, Mohamed-Amine and Mousannif, Hajar},
  journal={arXiv preprint arXiv:2304.00026},
  year={2023}
}
2
@article{li2017deep,
  title={Deep reinforcement learning: An overview},
  author={Li, Yuxi},
  journal={arXiv preprint arXiv:1701.07274},
  year={2017}
}
3
@article{grondman2012survey,
  title={A survey of actor-critic reinforcement learning: Standard and natural policy gradients},
  author={Grondman, Ivo and Busoniu, Lucian and Lopes, Gabriel AD and Babuska, Robert},
  journal={IEEE Transactions on Systems, Man, and Cybernetics, part C (applications and reviews)},
  volume={42},
  number={6},
  pages={1291--1307},
  year={2012},
  publisher={IEEE}
}
4
@article{sutton1999policy,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David and Singh, Satinder and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}

5
@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

6
@article{wilcox2022monte,
  title={Monte carlo augmented actor-critic for sparse reward deep reinforcement learning from suboptimal demonstrations},
  author={Wilcox, Albert and Balakrishna, Ashwin and Dedieu, Jules and Benslimane, Wyame and Brown, Daniel and Goldberg, Ken},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={2254--2267},
  year={2022}
}

7
@article{dqn,
  title={Playing atari with deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  journal={arXiv preprint arXiv:1312.5602},
  year={2013}
}

8
@inproceedings{brittain2024improving,
  title={Improving autonomous separation assurance through distributed reinforcement learning with attention networks},
  author={Brittain, Marc W and Alvarez, Luis E and Breeden, Kara},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  pages={22857--22863},
  year={2024}
}

9
@inproceedings{wang2023dm2,
  title={DM$^2$: Decentralized Multi-Agent Reinforcement Learning via Distribution Matching},
  author={Wang, Caroline and Durugkar, Ishan and Liebman, Elad and Stone, Peter},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={11699--11707},
  year={2023}
}

10
@article{weihs2020allenact,
  title={Allenact: A framework for embodied ai research},
  author={Weihs, Luca and Salvador, Jordi and Kotar, Klemen and Jain, Unnat and Zeng, Kuo-Hao and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2008.12760},
  year={2020}
}

11
@inproceedings{szot2023large,
  title={Large language models as generalizable policies for embodied tasks},
  author={Szot, Andrew and Schwarzer, Max and Agrawal, Harsh and Mazoure, Bogdan and Metcalf, Rin and Talbott, Walter and Mackraz, Natalie and Hjelm, R Devon and Toshev, Alexander T},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}

12
@inproceedings{chen2023end,
  title={End-to-end entity linking with hierarchical reinforcement learning},
  author={Chen, Lihan and Zhu, Tinghui and Liu, Jingping and Liang, Jiaqing and Xiao, Yanghua},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  pages={4173--4181},
  year={2023}
}

13
@article{jiang2024rocket,
  title={Rocket Landing Control with Random Annealing Jump Start Reinforcement Learning},
  author={Jiang, Yuxuan and Yang, Yujie and Lan, Zhiqian and Zhan, Guojian and Li, Shengbo Eben and Sun, Qi and Ma, Jian and Yu, Tianwen and Zhang, Changwu},
  journal={arXiv preprint arXiv:2407.15083},
  year={2024}
}

14
@book{kakade2003sample,
  title={On the sample complexity of reinforcement learning},
  author={Kakade, Sham Machandranath},
  year={2003},
  publisher={University of London, University College London (United Kingdom)}
}

15
@article{neumann2022scaling,
  title={Scaling laws for a multi-agent reinforcement learning model},
  author={Neumann, Oren and Gros, Claudius},
  journal={arXiv preprint arXiv:2210.00849},
  year={2022}
}

16
@incollection{li2023deep,
  title={Deep reinforcement learning},
  author={Li, Shengbo Eben},
  booktitle={Reinforcement learning for sequential decision and optimal control},
  pages={365--402},
  year={2023},
  publisher={Springer}
}

17
@article{alphago,
  title={Mastering the game of Go with deep neural networks and tree search},
  author={Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  journal={nature},
  volume={529},
  number={7587},
  pages={484--489},
  year={2016},
  publisher={Nature Publishing Group}
}

18
@article{alphastar,
  title={Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author={Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal={nature},
  volume={575},
  number={7782},
  pages={350--354},
  year={2019},
  publisher={Nature Publishing Group}
}

19
@article{openaifive,
  title={Dota 2 with large scale deep reinforcement learning},
  author={Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal={arXiv preprint arXiv:1912.06680},
  year={2019}
}

20
@misc{reverb,
      title={Reverb: A Framework For Experience Replay}, 
      author={Albin Cassirer and Gabriel Barth-Maron and Eugene Brevdo and Sabela Ramos and Toby Boyd and Thibault Sottiaux and Manuel Kroiss},
      year={2021},
      eprint={2102.04736},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.04736}, 
}


21
@inproceedings{isard2007dryad,
  title={Dryad: distributed data-parallel programs from sequential building blocks},
  author={Isard, Michael and Budiu, Mihai and Yu, Yuan and Birrell, Andrew and Fetterly, Dennis},
  booktitle={Proceedings of the 2nd ACM SIGOPS/EuroSys European conference on computer systems 2007},
  pages={59--72},
  year={2007}
}

22
@article{dean2012large,
  title={Large scale distributed deep networks},
  author={Dean, Jeffrey and Corrado, Greg and Monga, Rajat and Chen, Kai and Devin, Matthieu and Mao, Mark and Ranzato, Marc'aurelio and Senior, Andrew and Tucker, Paul and Yang, Ke and others},
  journal={Advances in neural information processing systems},
  volume={25},
  year={2012}
}

23
@article{nair2015massively,
  title={Massively parallel methods for deep reinforcement learning},
  author={Nair, Arun and Srinivasan, Praveen and Blackwell, Sam and Alcicek, Cagdas and Fearon, Rory and De Maria, Alessandro and Panneershelvam, Vedavyas and Suleyman, Mustafa and Beattie, Charles and Petersen, Stig and others},
  journal={arXiv preprint arXiv:1507.04296},
  year={2015}
}

24
@inproceedings{espeholt2018impala,
  title={Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures},
  author={Espeholt, Lasse and Soyer, Hubert and Munos, Remi and Simonyan, Karen and Mnih, Vlad and Ward, Tom and Doron, Yotam and Firoiu, Vlad and Harley, Tim and Dunning, Iain and others},
  booktitle={International conference on machine learning},
  pages={1407--1416},
  year={2018},
  organization={PMLR}
}

25
@inproceedings{liang2018rllib,
  title={RLlib: Abstractions for distributed reinforcement learning},
  author={Liang, Eric and Liaw, Richard and Nishihara, Robert and Moritz, Philipp and Fox, Roy and Goldberg, Ken and Gonzalez, Joseph and Jordan, Michael and Stoica, Ion},
  booktitle={International conference on machine learning},
  pages={3053--3062},
  year={2018},
  organization={PMLR}
}

26
@inproceedings{pan2022optimizing,
  title={Optimizing communication in deep reinforcement learning with xingtian},
  author={Pan, Lichen and Qian, Jun and Xia, Wei and Mao, Hangyu and Yao, Jun and Li, Pengze and Xiao, Zhen},
  booktitle={Proceedings of the 23rd ACM/IFIP International Middleware Conference},
  pages={255--268},
  year={2022}
}

27
@inproceedings{zhu2023msrl,
  title={$\{$MSRL$\}$: Distributed Reinforcement Learning with Dataflow Fragments},
  author={Zhu, Huanzhou and Zhao, Bo and Chen, Gang and Chen, Weifeng and Chen, Yijie and Shi, Liang and Yang, Yaodong and Pietzuch, Peter and Chen, Lei},
  booktitle={2023 USENIX Annual Technical Conference (USENIX ATC 23)},
  pages={977--993},
  year={2023}
}

28
@article{mei2023srl,
  title={SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores},
  author={Mei, Zhiyu and Fu, Wei and Gao, Jiaxuan and Wang, Guangju and Zhang, Huanchen and Wu, Yi},
  journal={arXiv preprint arXiv:2306.16688},
  year={2023}
}

29
@article{horgan2018distributed,
  title={Distributed prioritized experience replay},
  author={Horgan, Dan and Quan, John and Budden, David and Barth-Maron, Gabriel and Hessel, Matteo and Van Hasselt, Hado and Silver, David},
  journal={arXiv preprint arXiv:1803.00933},
  year={2018}
}

30
@article{schaul2015prioritized,
  title={Prioritized experience replay},
  author={Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  journal={arXiv preprint arXiv:1511.05952},
  year={2015}
}

31
@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

32
@article{espeholt2019seed,
  title={Seed rl: Scalable and efficient deep-rl with accelerated central inference},
  author={Espeholt, Lasse and Marinier, Rapha{\"e}l and Stanczyk, Piotr and Wang, Ke and Michalski, Marcin},
  journal={arXiv preprint arXiv:1910.06591},
  year={2019}
}

33
@inproceedings{petrenko2020sample,
  title={Sample factory: Egocentric 3d control from pixels at 100000 fps with asynchronous reinforcement learning},
  author={Petrenko, Aleksei and Huang, Zhehui and Kumar, Tushar and Sukhatme, Gaurav and Koltun, Vladlen},
  booktitle={International Conference on Machine Learning},
  pages={7652--7662},
  year={2020},
  organization={PMLR}
}

34
@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

35
@article{assran2019gossip,
  title={Gossip-based actor-learner architectures for deep reinforcement learning},
  author={Assran, Mahmoud and Romoff, Joshua and Ballas, Nicolas and Pineau, Joelle and Rabbat, Michael},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

36
@article{recht2011hogwild,
  title={Hogwild!: A lock-free approach to parallelizing stochastic gradient descent},
  author={Recht, Benjamin and Re, Christopher and Wright, Stephen and Niu, Feng},
  journal={Advances in neural information processing systems},
  volume={24},
  year={2011}
}

37
@INPROCEEDINGS{9428188,
  author={Liu, Sidun and Qiao, Peng and Dou, Yong and Li, Rongchun},
  booktitle={2021 IEEE International Conference on Multimedia and Expo (ICME)}, 
  title={Ddper: Decentralized Distributed Prioritized Experience Replay}, 
  year={2021},
  volume={},
  number={},
  pages={1-6},
  keywords={Training;Conferences;Reinforcement learning;Writing;Approximation algorithms;Reinforcement Learning;Distributed Machine Learning;Deep Q-Learning},
  doi={10.1109/ICME51207.2021.9428188}}

38
@INPROCEEDINGS{10077903,
  author={Qiao, Peng and He, Zhouyu and Li, Rongchun and Jiang, Jingfei and Dou, Yong and Li, Dongsheng},
  booktitle={2022 IEEE 28th International Conference on Parallel and Distributed Systems (ICPADS)}, 
  title={MLPs: Efficient Training of MiniGo on Large-scale Heterogeneous Computing System}, 
  year={2023},
  volume={},
  number={},
  pages={475-482},
  keywords={Training;Deep learning;Neural networks;Memory management;Reinforcement learning;Parallel processing;Programming;deep reinforcement learning;deep neural networks;MLPerf;heterogeneous architecture;large-scale parallel computing},
  doi={10.1109/ICPADS56603.2022.00068}}

39
@inproceedings{Liu2022HeterogeneousSL,
  title={Heterogeneous Skill Learning for Multi-agent Tasks},
  author={Yuntao Liu and Yuan Li and Xinhai Xu and Yong Dou and Donghong Liu},
  booktitle={Neural Information Processing Systems},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:258509530}
}

40
@inproceedings{zhao2023high,
  title={High-throughput Sampling, Communicating and Training for Reinforcement Learning Systems},
  author={Zhao, Laiping and Dai, Xinan and Zhao, Zhixin and Xin, Yusong and Hu, Yitao and Qian, Jun and Yao, Jun and Li, Keqiu},
  booktitle={2023 IEEE/ACM 31st International Symposium on Quality of Service (IWQoS)},
  pages={1--10},
  year={2023},
  organization={IEEE}
}

41
@article{ppo,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

42
@article{ddpg,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}

43
@article{asurvey,
author = {Liu, Zhihong and Xu, Xin and Qiao, Peng and Li, Dongsheng},
title = {Acceleration for Deep Reinforcement Learning using Parallel and Distributed Computing: A Survey},
year = {2024},
issue_date = {April 2025},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {57},
number = {4},
issn = {0360-0300},
url = {https://doi.org/10.1145/3703453},
doi = {10.1145/3703453},
abstract = {Deep reinforcement learning has led to dramatic breakthroughs in the field of artificial intelligence for the past few years. As the amount of rollout experience data and the size of neural networks for deep reinforcement learning have grown continuously, handling the training process and reducing the time consumption using parallel and distributed computing is becoming an urgent and essential desire. In this article, we perform a broad and thorough investigation on training acceleration methodologies for deep reinforcement learning based on parallel and distributed computing, providing a comprehensive survey in this field with state-of-the-art methods and pointers to core references. In particular, a taxonomy of literature is provided, along with a discussion of emerging topics and open issues. This incorporates learning system architectures, simulation parallelism, computing parallelism, distributed synchronization mechanisms, and deep evolutionary reinforcement learning. Furthermore, we compare 16 current open-source libraries and platforms with criteria of facilitating rapid development. Finally, we extrapolate future directions that deserve further research.},
journal = {ACM Comput. Surv.},
month = dec,
articleno = {91},
numpages = {35},
keywords = {Deep reinforcement learning, acceleration, parallel and distributed computing, large-scale}
}

44
@article{pala ,
author={Sun, Zhenglun and Qiao, Peng and Dou, Yong and Li, Qingqing and Li, Rongchun},
title={ PALA: Parallel Actor-Learner Architecture for Distributed Deep Reinforcement Learning},
organization={National University of Defense Technology},
journal={Chinese Journal of Computers},
year={2023},
volume={46},
number={2},
pages={229-243},
month={2},
}

45
@inproceedings{RiskQ,
 author = {Shen, Siqi and Ma, Chennan and Li, Chao and Liu, Weiquan and Fu, Yongquan and Mei, Songzhu and Liu, Xinwang and Wang, Cheng},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
 pages = {34791--34825},
 publisher = {Curran Associates, Inc.},
 title = {RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization},
 url = {https://proceedings.neurips.cc/paper_files/paper/2023/file/6d3040941a2d57ead4043556a70dd728-Paper-Conference.pdf},
 volume = {36},
 year = {2023}
}

46
@ARTICLE{10124081,
  author={Shen, Han and Zhang, Kaiqing and Hong, Mingyi and Chen, Tianyi},
  journal={IEEE Transactions on Signal Processing}, 
  title={Towards Understanding Asynchronous Advantage Actor-Critic: Convergence and Linear Speedup}, 
  year={2023},
  volume={71},
  number={},
  pages={2579-2594},
  keywords={Convergence;Complexity theory;Signal processing algorithms;Optimization;Training;Parallel processing;Markov processes;Reinforcement learning;policy gradient;actor critic;asynchronous parallel method},
  doi={10.1109/TSP.2023.3268475}}