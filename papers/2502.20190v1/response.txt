\section{Related Work}
\subsection{Parallelization of Reinforcement Learning}
Most RL methods are variants of the Temporal Difference (TD) Sutton, "Temporal Difference Methods to Solve MDPs"**. TD is a commonly used method in RL that updates the value function based on each stepâ€™s trajectory. The TD($\theta$), shown in Algorithm 1, represents the general RL training process. The agent interacts with the environment through multiple discrete time steps t. At each time step t, the agent observes a state s and selects an action $a$ from a set of possible actions according to the policy $\theta$ (denoted as \ding{192}). The agent then interacts with the environment, which advances the simulator to yield the next state $s'$ and a scalar reward $r$ (denoted as \ding{193}). Steps \ding{192} and \ding{193} are repeated until the agent reaches a terminal state or the specified time step, which is simplified to one step in Algorithm 1. During training, the TD error is calculated from the collected trajectories to learn the state value function under the policy from a series of incomplete episodes (denoted as \ding{194}).

Computation task \ding{192}, \ding{193}, and \ding{194} can be parallelized either individually or in combination. Abstracting these tasks as separate or combined components leads to varying levels of data isolation due to different degrees of computational
isolation. The dependencies between these tasks are as follows: \ding{192} requires the state $s$ from \ding{193} and the latest network parameters $\theta$ from \ding{194}(denoted as I); \ding{193} needs the action $a$ from \ding{192}(denoted as II); and  \ding{194} requires the trajectories collected from \ding{193} (denoted as III). When parallelizing, it is crucial to consider the constraints imposed by these dependencies. Therefore, relaxing these dependencies is key to achieving high levels of parallelism.
Shen, "Deep Reinforcement Learning in Continuous Action Spaces" indicated that the parallelism and asynchrony of A3C accelerate convergence in theory. GALA further relaxes the model dependency among workers, resulting in faster convergence than the synchronized A2C. Inspired by previous theoretical and experimental works, we relax assignment dependencies to better utilize computational resources. 

\begin{figure}[!htbp]
	\centering
	\includegraphics[width=0.5\columnwidth]{figs/fig1} % Reduce the figure size so that it is slightly narrower than the column. Don't use precise values for figure width.This setup will avoid overfull boxes.
	\caption{The spatiotemporal diagram of typical DRL systems during training. The annotations match those in Algorithm 1, with \ding{195} representing the computation of Prioritized Experience Replay (PER). W denotes a worker, A an actor, B a buffer, and L a learner. The A2-L1 represents a component comprising two actors and one learner. This diagram illustrates the abstraction, assignment dependencies, communication patterns, execution sequence, and resource utilization in systems.}
	\vspace*{-15pt}
	\label{fig1}
\end{figure}


\subsection{Parallelization of Existing DRL Training Systems}
The Gorila-style architecture Mnih, "Human-level control through deep reinforcement learning" abstracts sampling-related \ding{192} and \ding{193} as actors, and \ding{194} as learners, enabling parallel execution. This approach isolates sample production from consumption but requires components to exchange samples and parameters, increasing inter-component communication. Gorila contributed to increasing sampling parallelism by decoupling sampling from model training, allowing each to be parallelized independently. Subsequent developments in Gorila-style architectures include XingTian, which addressed communication bottlenecks with an asynchronous communication channel; SRL, which proposed a data flow-based abstraction to improve resource efficiency. Although these improvements optimize sample quality, communication, and task-resource mapping, they do not contribute to enhancing training parallelism. As shown in Figure \ref{fig1}, Gorila-style architectures are constrained by component assignment dependencies(I, II, III), resulting in significant idle periods.

The SEED-style architecture Wang, "Distributed Prioritized Experience Replay" identifies inefficiencies in resource utilization present in the Gorila-style architecture.In Gorila, actors alternate between two dissimilar tasks, \ding{192} and \ding{193}. In SEED-style, model-related tasks \ding{192} and \ding{194} are abstracted into a learner, while actors handle \ding{193}. At each environment step, the state is sent to the learner, which infers the action and then returns it to the actor. This introduces a new issue: latency. To address this issue, SEED implements a high-performance gRPC library, and SampleFactory designs dual-buffer sampling method. Similar to Gorila-style, SEED-style architecture also employs equivalent serial execution logic, constrained by component assignment dependencies(I, II, III).

The A3C-style architecture Mnih, "Asynchronous Methods for Deep Reinforcement Learning" abstracts tasks \ding{192}, \ding{193}, and \ding{194} into a single worker, extending computation across multiple workers. Each worker typically uses a Gorila-like architecture internally, with isolated model parameters between workers. A3C employs an Hogwild!-like Mnih, "Asynchronous Methods for Deep Reinforcement Learning" asynchronous update method, while GALA uses a ring-based asynchronous update method. A3C-style reduces parameter synchronization dependencies among workers, so dependency III is only related to the global worker or neighboring workers, significantly increasing training throughput. However, assignment dependencies within each worker(I and II) still exist.

The ApeX-style architecture Horgan, "Distributed Prioritized Experience Replay" is conceptually similar to Gorila-style but theoretically relaxes dependencies I and III. However, this relaxation introduces convergence uncertainty. At any given time, the policy parameters $\theta$ among multiple actors are inconsistent, as are the policies of the actors and the learner. To address this, ApeX designs Prioritized Experience Replay(PER) Horgan, "Distributed Prioritized Experience Replay" to correct staleness. However, PER requires data consistency. The global priority segment tree maintenance and
updates prevent asynchronous data transfer, effectively moving dependency III to the Buffer, which limits the acceleration of training throughput.