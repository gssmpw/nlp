\section{Preliminaries and Motivation}

\subsection{Preliminaries}
\label{sec:preliminaries}

\myparatightestn{Synthetic data} refers to ``fake'' data generated by models or software for various applications, including data augmentation, model training, and software testing \cite{lin2022data}. One approach involves \textbf{machine learning models}, ranging from simple statistical models like Gaussian mixtures to more advanced deep neural network-based generative models such as GANs \cite{goodfellow2020generative}, diffusion models \cite{sohl2015deep}, and auto-regressive models \cite{openai2023gpt4,liu2024distilled}.
The other approach relies on \textbf{simulators}. \textbf{In this paper, we broadly define simulators as non-neural-network data synthesizers with hard-coded, interpretable logic.} For example, given network configurations, ns-2 \cite{issariyakul2009introduction} can simulate a network and generate network packets. Similarly, given 3D models and lighting configurations, Blender \cite{blender} can render images and videos of objects.
These simulators are widely used across various domains, particularly in cases where the underlying data distribution is too complex for machine learning models to learn effectively.

\myparatightestn{DP synthetic data} requires the synthetic data to be \emph{close to a given private dataset}, while \emph{having a strict privacy guarantee called Differential Privacy (DP)} \cite{dwork2006calibrating}. Formally, a mechanism $\alg$ is $(\epsilon,\delta)$-DP if for any two neighboring datasets $\calD$ and $\calD'$ (i.e., $\calD'$ has one extra entry compared to $\calD$ or vice versa) and for any set $S$ of outputs of $\alg$, we have 
$\probof{\alg\bra{\calD}\in S}\leq e^{\epsilon}\probof{\alg\bra{\calD'}\in S} + \delta$. 
Smaller $\epsilon$ and $\delta$ imply stronger privacy guarantees. 
Current state-of-the-art DP image and text synthesis methods rely on machine learning models and typically require model training \cite{lin2020using,beaulieu2019privacy,dockhorn2022differentially,yin2022practical,yu2021differentially,he2022exploring,li2021large,ghalebikesabi2023differentially,yue2022synthetic,jordon2019pate,harder2022differentially,harder2021dp,vinaroz2022hermite,cao2021don}.

\myparatightestn{\privateevolution{} (\pe{})} \cite{lin2023differentially,xie2024differentially} is a recent training-free framework for DP data synthesis. \pe{} only requires \emph{inference access} to the foundation models. Therefore, unlike prior training-based methods, \pe{} can leverage the state-of-the-art models even if they are behind APIs (e.g., GPT-4) and is more computationally efficient. 

In more detail, \pe{} has achieved state-of-the-art performance on several \emph{image} and \emph{text} benchmarks \cite{lin2023differentially,xie2024differentially}.  
\emph{When using similar open-source pre-trained models}, \pe{} attains an \emph{image} quality score of FID $\leq 7.9$ on \cifar{} with a privacy cost of $\epsilon=0.67$,  
a significant improvement over the previous state-of-the-art, which required $\epsilon=32$ \cite{lin2023differentially}.  
Furthermore, \pe{} can be up to 66$\times$ more efficient than training-based methods on DP \emph{text} generation \cite{xie2024differentially}.  
\emph{By leveraging state-of-the-art models behind APIs---where training-based methods are not applicable---}\pe{} further enhances performance,  
outperforming all prior approaches in downstream \emph{text} classification accuracy on the \openreview{} dataset \cite{xie2024differentially}.  
Additionally, \pe{} can be applied in \emph{federated learning} to shift model training from devices to central servers in a differentially private and more efficient manner \cite{hou2024pre,zou2025contrastive}.  

\pe{} is versatile across data modalities, as long as suitable foundation models are available with two functions: (1) \randomsampleapiname{} that generates a random sample (e.g., a random bird image), and (2) \samplevariationapiname{} that generates slight modifications of the given sample (e.g., a similar bird image). \pe{} works by first calling \randomsampleapiname{}  to get an initial set of synthetic samples, and then iteratively refine this set by selecting the closest ones to the private samples (in a DP manner) and calling \samplevariationapiname{} to generate more of such samples. The full \pe{} algorithm from \citet{lin2023differentially} is attached in \cref{app:pe} for completeness.

\subsection{Motivation}

While \pe{} achieves state-of-the-art performance on several image and text benchmarks \cite{lin2023differentially,xie2024differentially,hou2024pre}, its performance significantly drops when there is a large distribution shift between the private data and the foundation modelâ€™s pre-trained data \cite{dpimagebench}. For instance, when using the \mnist{} dataset \cite{lecun1998mnist} (handwritten digits) as the private data, training a downstream digit classifier (10 classes) on DP synthetic data (with $\epsilon=1$) from \pe{}---using a foundation model pre-trained on \imagenet{}---yields an accuracy of only 27.9\%. Since relevant foundation models may not always be available for every domain, this limitation hinders \pe{}'s applicability in real-world scenarios. Extending \pe{} to leverage simulators could significantly expand its potential applications.

More broadly, as discussed in \cref{sec:preliminaries}, simulators cannot be substituted by foundation models in (non-DP) data synthesis across many domains. Unfortunately, current state-of-the-art DP synthetic data methods are deeply reliant on machine learning models (e.g., requiring model training) and cannot be applied to simulators. By extending \pe{} to work with simulators, we aim to unlock the potential of simulators in DP data synthesis.
