\section{Experiments}
\label{sec:exp}

\subsection{Experimental Setup}

\subsubsection{Datasets and Simulators}
\label{sec:datasets_and_simulators}

\myparatightestn{Datasets.} Following prior work \cite{dpimagebench}, we use two private datasets: \textbf{(1) \mnist{}} \cite{lecun1998mnist}, where the image class labels are digits `0'-`9', and \textbf{(2) \celeba{}} \cite{liu2015faceattributes}, where the image class labels are male and female. We aim at \emph{conditional generation} for these datasets (i.e., each generated image is associated with the class label).

\myparatightestn{Simulators.}
To demonstrate the general applicability of \simpe{}, we select three diverse simulators with very different implementations.


\noindent\textbf{(1) Text rendering program.} Generating images with readable text using foundation models is a known challenge \cite{betker2023improving}. Simulators can address this gap, as generating images with text through computer programs is straightforward. To illustrate this, we implement our own text rendering program, treating \mnist{} as the private dataset. Specifically, we use the Python PIL library to render digits as images.
\textbf{The categorical parameters include:}
\underline{(1) Font.} We use Google Fonts \cite{googlefonts}, which offers 3589 fonts in total.
\underline{(2) Text.} The text consists of digits `0' - `9'. %
\textbf{The numerical parameters include:}
\underline{(1) Font size}, ranging from 10 to 29.
\underline{(2) Stroke width}, ranging from 0 to 2.
\underline{(3) Digit rotation degree}, ranging from $-30\degree$ to $30\degree$. We set the feasible sets of these parameters to be large enough so that the random samples differ significantly from \mnist{} (see \cref{fig:mnist_simulator}).

\noindent\textbf{(2) Computer graphics-based renderer for face images.} Computer graphics-based rendering is widely used in real-world applications such as game development, cartoons, and movie production. This experiment aims to assess whether these advanced techniques can be adapted for DP synthetic image generation via \simpe{}. We use \celeba{} as the private dataset and a Blender-based face image renderer from \citet{bae2023digiface} as the API. Since the source code for their renderer is not publicly available, we apply our data-based algorithm from \cref{sec:method_data} on their released dataset of 1.2 million face images.
It is important to note that this renderer may not necessarily represent the state-of-the-art. As visualized in \cref{fig:celeba_simulator}, the generated faces exhibit various unnatural artifacts and appear less realistic than images produced by state-of-the-art generative models (e.g., \citet{rombach2022high}). Therefore, this experiment serves as a preliminary study, and the results could potentially improve with more advanced rendering techniques.

\noindent\textbf{(3) Rule-based avatar generator.} We further investigate whether \simpe{} remains effective when the simulator's data significantly differs from the private dataset. We use \celeba{} as the private dataset and a rule-based avatar generator \cite{pythonavatar} as the API. 
This simulator has 16 categorical parameters that control attributes of the avatar including eyes, noses, background colors, skin colors, etc.
As visualized in \cref{fig:celeba_avatar_simulator}, the generated avatars have a cartoon-like appearance and lack fine-grained details. This contrasts sharply with \celeba{} images, which consist of real human face photographs.

\myparatightestn{Class label information from the simulators.} For simulator 1, the target class label (i.e., the digit) is fully controlled by one parameter.
For simulators 2 and 3, the target class label (i.e., the gender) is not directly controlled by any parameter, but could potentially be obtained by an external image gender classifier.
One benefit of using domain-specific simulators is that we can potentially use the class label information to enhance data quality.
To get a more comprehensive understanding of \simpe{}, we consider two settings: \textbf{(1) Class label information is unavailable (abbreviated as ``\classunavail{}'').} We artificially make the problem more challenging by assuming that the class label information is \emph{not} available. Therefore, \simpe{} has to learn to synthesize images with the correct class by itself. \textbf{(2) Class label information is available  (abbreviated as ``\classavail{}'').} On \mnist{}, we further test how \simpe{} can be improved if the class label information is available. In this case, 
the \randomsampleapiname{} and \samplevariationapiname{} (\cref{eq:simulator_random_api,eq:simulator_variation_api}) are restricted to draw parameters from the corresponding class (i.e., the digit is set to the target class).



\subsubsection{Metrics and Evaluation Pipelines}

We follow the evaluation settings of DPImageBench \cite{dpimagebench}, a recent benchmark for DP image synthesis. Specifically, we use two metrics: \textbf{(1) FID} \cite{heusel2017gans} as a quality metric and \textbf{(2) the accuracy of downstream classifiers} as a utility metric.
Specifically, %
we use the conditional version of \pe{} (\cref{app:pe}), 
so that each generated images are associated with the class labels (i.e., `0'-`9' digits in \mnist{}, male vs. female in \celeba{}). 
These class labels are the targets for training the classifiers.
We employ a strict train-validation-test split and account for the privacy cost of classifier hyperparameter selection. Specifically, we divide the private dataset into disjoint training and validation sets. We then run \simpe{} on the training set to generate synthetic data. Next, we train three classifiers—ResNet \cite{he2016deep}, WideResNet \cite{zagoruyko2016wide}, and ResNeXt \cite{xie2017aggregated}—on the synthetic data and evaluate their accuracy on the validation set. Since the validation set is part of the private data, we use the Report Noisy Max algorithm \cite{dwork2014algorithmic} to select the best classifier checkpoint across all epochs of all three classifiers. Finally, we report the accuracy of this classifier on the test set.
This procedure ensures that the reported accuracy is not inflated due to train-test overlap or DP violations in classifier hyperparameter tuning.

Following \citet{dpimagebench}, we set DP parameter $\delta=1/(\numprisamples\cdot\log \numprisamples)$, where $\numprisamples$ is the number of samples in the private dataset, and $\epsilon=1$ or 10.



\subsubsection{Baselines}
We compare \simpe{} with 12 state-of-the-art DP image synthesizers reported in \citet{dpimagebench}, including DP-MERF \cite{dp-merf}, DP-NTK \cite{dp-ntk}, DP-Kernel \cite{dp-kernel}, GS-WGAN \cite{gs-wgan}, DP-GAN \cite{dpgan}, DPDM \cite{dpdm}, PDP-Diffusion \cite{dp-diffusion}, DP-LDM \cite{dpldm}, DP-LoRA \cite{dplora}, PrivImage \cite{li2023privimage}, and \pe{} with foundation models \cite{lin2023differentially}. Except for \pe{}, all other baselines require model training.
For \simpe{} with simulator-generated data, we additionally compare it against the two baselines introduced in \cref{sec:method_data}.

\textbf{It is important to note that this comparison is not intended to be entirely fair, as different methods leverage different prior knowledge.} For example, many of the baselines rely on pre-trained foundation models or public datasets from similar distributions, whereas \simpe{} does not. Conversely, \simpe{} utilizes simulators, which none of the baseline methods incorporate. \textbf{Since \simpe{} is the only approach that leverages simulators, it is more appropriate to consider it as a new evaluation setting or benchmark. The results of other methods serve as a reference point to contextualize this new paradigm among state-of-the-art approaches and to highlight directions for future research.}



\subsection{\simpe{} with Simulator Access}
\label{sec:exp_simulator}

In this section, we evaluate \simpe{} with a text rendering program on \mnist{} dataset. The results are shown in \cref{tab:acc_and_fid,fig:mnist,tab:mnist_know_digit,fig:mnist_know_digit}. The key takeaway messages are:

\input{tex_ICML2025/fig/mnist}
\input{tex_ICML2025/table/acc_and_fid}

\myparatightestn{\simpe{} effectively guides the simulator to generate high-quality samples.}
As shown in \cref{fig:mnist_simulator}, without any information from the private data or guidance from \simpe{}, the simulator initially produces poor-quality images with incorrect digit sizes, rotations, and stroke widths. These low-quality samples serve as the starting point for \simpe{} (via \randomsampleapiname{}). Through iterative refinement and private data voting, \simpe{} gradually optimizes the simulator parameters, ultimately generating high-quality \mnist{} samples, as illustrated in \cref{fig:mnist_simpe}.

Quantitative results in \cref{tab:acc_and_fid} further support this. Without private data guidance, the simulator naturally generates digits from incorrect classes, leading to a downstream classifier accuracy of only 11.6\%, close to random guessing. In contrast, \simpe{} significantly improves accuracy to approximately 90\%. Additionally, FID scores confirm that the images generated by \simpe{} more closely resemble real data.

\myparatightestn{\simpe{} can significantly improve the performance of \pe{}.} 
The \pe{} baseline \cite{lin2023differentially} uses a diffusion model pre-trained on \imagenet{}, which primarily contains natural object images (e.g., plants, animals, cars). Since \mnist{} differs significantly from such data, \pe{}, as a training-free method, struggles to generate meaningful \mnist{}-like images. Most \pe{}-generated images lack recognizable digits (see \citet{dpimagebench}), resulting in a classification accuracy of only $\sim 30\%$ (\cref{tab:classifier}). By leveraging a simulator better suited for this domain, \simpe{} achieves significantly better results, tripling the classification accuracy and reducing the FID score by 80\% at $\epsilon=10$.

\myparatightestn{\simpe{} achieves competitive results among state-of-the-art methods.}
When the foundation model or public data differs significantly from the private data, training-based baselines can still adapt the model to the private data distribution by updating its weights, whereas \pe{} cannot. This limitation accounts for the substantial performance gap between \pe{} and other methods. Specifically, \pe{} records the lowest classification accuracy among all 12 methods (\cref{tab:classifier}). By leveraging domain-specific simulators, \simpe{} significantly narrows this gap, achieving classification accuracy within 5.4\% and 4.2\% of the best-performing method for $\epsilon=1$ and $\epsilon=10$, respectively.


\myparatightestn{Class label information from the simulators can be helpful.} All the above experiments are based on the \classunavail{} setting, where the class label information from the simulator is assumed to be unknown. However, one key advantage of using simulators over foundation models for generating synthetic data is that simulators can provide various labels for free \cite{wood2021fake,bae2023digiface}. In our case, for \mnist{}, the simulators provide information on which digit the generated image represents.
Following the approach in \cref{sec:datasets_and_simulators}, we utilize this label information, and the results are presented in \cref{tab:mnist_know_digit,fig:mnist_know_digit}. We observe that with digit information, the simulator-generated data achieve significantly higher classification accuracy (92.2\%), although the FID remains low due to the generated digits exhibiting incorrect characteristics (\cref{fig:mnist_know_digit_simulator}). The fact that \simpe{} outperforms the simulator in both FID and classification accuracy across all settings suggests that \simpe{} effectively incorporates private data information to enhance both data fidelity and utility, even when compared to such a strong baseline. As expected, \simpe{} under \classavail{} matches or surpasses the results obtained in \classunavail{} across all settings, suggesting the usefulness of  leveraging class label information.


\input{tex_ICML2025/fig/mnist_know_digit}
\input{tex_ICML2025/table/mnist_know_digit}



\subsection{\simpe{} with Simulator-generated Data}
\input{tex_ICML2025/fig/celeba}

In this section, we evaluate \simpe{} using a generated dataset from a computer graphics-based renderer on the \celeba{} dataset. The results, presented in \cref{tab:acc_and_fid,fig:celeba}, highlight the following key takeaways:

\myparatightestn{\simpe{} effectively selects samples that better match the correct classes.}
Without any information from the private data, the simulator naturally generates images with incorrect class labels (\cref{fig:celeba_simulator}). Consequently, a downstream gender classifier trained on simulator-generated data can at best achieve a trivial accuracy 61.4\%---on the test set, the majority class (female) constitutes 61.4\%. Building upon this noisy data, \simpe{} iteratively refines the sample selection. Ultimately, \simpe{} selects samples that better align with the target classes (\cref{fig:celeba_simpe}), leading to an accuracy improvement of up to 21.1\% (\cref{tab:classifier}).

\myparatightestn{\simpe{} maintains the strong data quality of \pe{}.} As shown in \cref{tab:fid}, \simpe{} and \pe{} achieve similar FID scores. Unlike in the \mnist{} experiments (\cref{sec:exp_simulator}), where \simpe{} significantly improved over \pe{}, the lack of substantial improvement on \celeba{} can be attributed to two factors. First, on \celeba{}, \pe{} with foundation models already ranks 3rd among all methods in terms of FID, leaving little room for further gains. Second, in this experiment, \simpe{} is only provided with a fixed dataset generated from the simulator. As seen in \cref{fig:celeba}, the simulator-generated images exhibit noticeable differences from real \celeba{} images, such as faces appearing larger. Since \simpe{} in this setting can only select images without modifying them, it cannot correct such discrepancies. Having access to simulator code, as in \cref{sec:exp_simulator}, could potentially alleviate this issue, as \simpe{} could learn to modify the parameters that control the face size.
Another potential direction for improvement is a hybrid approach that enables \pe{} to leverage both foundation models and simulators, which we explore preliminarily in the next section.

\input{tex_ICML2025/fig/celeba_avatar}
\input{tex_ICML2025/table/celeba_avatar}

\subsection{\simpe{} with both Simulators and Foundation Models}




In this section, we first examine how \simpe{} performs with weak simulators. We again use the \celeba{} dataset as the private data, but this time, we switch to a rule-based cartoon avatar generator \cite{pythonavatar} as the simulator. As shown in \cref{fig:celeba_avatar_simulator}, the avatars generated by the simulator differ significantly from the real \celeba{} images.

\myparatightestn{\simpe{} with weak simulators still learns useful features.} From \cref{tab:celeba_avatar}, we observe that downstream classifiers trained on \simpe{} with weak simulators achieve poor classification accuracy. However, two interesting results emerge: \textbf{(1)} Despite the significant difference between avatars and real face images, \simpe{} still captures certain characteristics of the two classes correctly. Specifically, \simpe{} tends to generate faces with long hair for the female class and short hair for the male class (\cref{fig:celeba_avatar_simpe}). \textbf{(2)} Although the FID scores of \simpe{} are quite poor (\cref{tab:celeba_avatar}), they still outperform many baselines (\cref{tab:fid}). This can be explained by the fact that, as shown in \citet{dpimagebench}, when DP noise is high, the training of many baseline methods becomes unstable. %
This results in images with noisy patterns, non-face images, or significant mode collapse, particularly for DP-NTK, DP-Kernel, and GS-WGAN. In contrast, \simpe{} is training-free, and thus it avoids these issues.

Next, we explore the feasibility of using \pe{} with both foundation models and the weak avatar simulator (\cref{sec:method_simulator_and_model}). The results are shown in \cref{tab:celeba_avatar}.

\myparatightestn{\pe{} benefits from utilizing simulators and foundation models together.} We observe that using both simulators and foundation models yields the best results in terms of both FID and classification accuracy. This result is intuitive: the foundation model, pre-trained on the diverse \imagenet{} dataset, has a low probability of generating a face image through \randomsampleapiname{}. While avatars are quite different from \celeba{}, they retain the correct image layout, such as facial boundaries, eyes, nose, etc. Using these avatars as seed samples for variation allows the foundation model to focus on images closer to real faces, rather than random, unrelated patterns.

Unlike other state-of-the-art methods that are tied to a specific data synthesizer, this result suggests that \pe{} is a promising framework that can easily combine the strengths of multiple types of data synthesizers.

\input{tex_ICML2025/fig/metrics_vs_iterations}
\input{tex_ICML2025/table/celeba_data_selection_baseline}

\subsection{Validating the Design of \simpe{}}
In this section, we provide more experiments to understand and validate the design of \simpe{}.



\myparatightestn{How does \simpe{} with simulator-generated data compare to other data selection algorithms?}
In \cref{sec:method_data}, we discussed two simple alternative solutions for simulator data selection. The comparison is shown in \cref{tab:celeba_data_selection_baseline}. As we can see, \simpe{} with iterative data selection outperforms the baselines on most metrics, validating the intuition outlined in \cref{sec:method_data}. However, the clustering approach used in the second baseline still has merit, as it results in a better FID for $\epsilon=10$. This idea is orthogonal to the design of \simpe{} and could potentially be combined for further improvement. We leave this exploration to future work.

\myparatightestn{How does \simpe{}'s performance evolve across \pe{} iterations?} \cref{fig:metrics_vs_iterations} shows that both the FID and the downstream classifier's accuracy generally improve as \pe{} progresses. This confirms that \pe{}'s iterative data refinement process is effective when combined with simulators.




