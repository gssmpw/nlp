\section{Discussion}
We developed a theoretical connection between the relative Mahalanobis distance score for outlier detection and the outlier probability under a Gaussian DPMM with tied covariance.
This Bayesian nonparametric perpsective led us to propose hierarchical Gaussian DPMMs that allow each cluster a different covariance matrix, while still sharing statistical strength across classes via the prior.
We developed efficient EM algorithms to estimate the hyperparameters of the hierarchical models, and we studied their performance on synthetic data as well as the OpenOOD benchmarks.
We found that these models --- especially the coupled diagonal covariance model --- yielded improved performance on some benchmarks, especially the Near OOD benchmarks.

\paragraph{Limitations and Future Work}
Despite the hierarchical prior, we found that the full covariance Gaussian DPMMs were prone to overfitting.
Further work could explore low-rank plus diagonal covariance matrices, which would interpolate between the diagonal and full covariance models.
More generally, like RMDS and MDS, we assume that features are Gaussian distributed within each class.
The competitive performance on the OpenOOD Imagenet benchmark using ViT features suggests that this assumption is reasonable~\citep{yang2022openood,zhang23openood15}, but there is no guarantee.
Future work could consider fine-tuning the features or learning a nonlinear transformation to address this potential source of model misspecification, as in prototype networks~\citep{snell2017prototypical}.


Bayesian nonparametric approaches naturally extend to other closely related problems, like generalized category discovery~\cite{vaze22} and continual learning~\cite{van2022three}.
For example,
given a collection of data points, a DPMM may have sufficient evidence to allocate new classes for the out-of-distribution data.
More generally, casting OOD as inference in a generative model brings modeling choices to the fore.
Here, we focused on the challenge of modeling high-dimensional covariance matrices that may vary across classes, but there are several other ways in which the simple Gaussian DPMM could be improved.
For example, we could attempt to capture the nonstationarity inherent in the OOD setting by allowing the prior predictive distribution to drift from what was inferred based on the training data.
Such a model could afford greater robustness on OOD detection tasks.

\paragraph{Conclusion}
In summary, we find that Bayesian nonparametric methods with hierarchical priors are a promising approach for OOD detection.
If the features extracted from foundation models are reasonably well approximated as realizations of Gaussian DPMMs, the posterior inferences under such models can provide accurate estimates of outlier probability.
This probabilistic perspective not only casts widely used methods in a new light, it also leads to practical model improvements and enables several lines of future inquiry.
