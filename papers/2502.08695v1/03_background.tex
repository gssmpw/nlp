\section{Background}
\label{sec:background}
We start with background on Dirichlet process mixture models (DPMMs) and the special case of a Gaussian DPMM with tied covariance.

\subsection{Dirichlet process mixture models}
\label{sec:dpmm}

Dirichlet process mixture models~\citep{lo1984class} are Bayesian nonparametric models for clustering and density estimation that allow for a countably infinite number of clusters.
There is always some probability that a new data point could come from a cluster that has never been seen before --- i.e., that the new point is an \textit{outlier}.

Let $\cD = \{x_n, y_n\}_{n=1}^N$ denote a set of training data points $x_n \in \reals^D$ and labels $y_n \in [K]$.
Likewise, let $\cD_k = \{x_n: y_n=k\}$ denote the subset of points assigned to cluster $k$, and let $N_k = |\cD_k|$ denote the number of such points.
Now consider a new, unlabeled data point $x$. Under a DPMM, its corresponding label $y$ has probability,
\begin{align}
    \label{eq:outlier_prob}
    p(y = k \mid x, \cD)
    &\propto
    \begin{cases}
    N_k \, p(x \mid \cD_k) &\text{if } k\in[K] \\
    \alpha \, p(x) &\text{if } k = K+1,
    \end{cases}
\end{align}
where the hyperparameter $\alpha \in \reals_+$ specifies the concentration of the Dirichlet process prior.

The first case captures the probability that the new point belongs to one of the training clusters (that it is an \textit{inlier}).
That probability depends on two factors:
1) the number of training data points in that cluster since, intuitively, larger clusters are more likely;
2) the \textit{posterior predictive probability}, which is obtained by integrating over the posterior distribution of cluster parameters,
\begin{align}
    p(x \mid \cD_k)
    &= \int p(x \mid \theta_k) \, p(\theta_k \mid \cD_k) \dif \theta_k \\
    \nonumber
    &\propto \int p(x \mid \theta_k) \, \bigg[\prod_{x_n \in \cD_k} p(x_n \mid \theta_k) \bigg] \, p(\theta_k) \dif \theta_k.
\end{align}
The second case of eq.~\eqref{eq:outlier_prob} captures the probability that the new point is an outlier.
It depends on the concentration $\alpha$ and the \textit{prior predictive probability} obtained by integrating over the prior distribution of cluster parameters,
\begin{align}
    p(x) &= \int p(x \mid \theta_k) \, p(\theta_k) \dif \theta_k.
\end{align}
For many models of interest, the posterior and prior predictive distributions have closed forms.

\subsection{Gaussian DPMM with Tied Covariance}
\label{sec:tied-cov}

For example, consider a Gaussian DPMM in which each cluster is parameterized by a mean and covariance,~${\theta_k = (\mu_k, \Sigma_k)}$.
Assume a conjugate prior for the mean,~$\mu_k \sim \mathcal{N}(\mu_0, \Sigma_0)$.
For now, assume that all clusters share the same covariance matrix, which we express through an atomic prior,~$\Sigma_k \sim \delta_\Sigma$.
The hyperparameters of the prior are $\eta = (\mu_0, \Sigma_0, \Sigma)$.

Under this Gaussian DPMM, the conditional distribution of a new data point's label is,
\begin{align}
    \label{eq:gaussian_dpmm_outlier_unnormalized}
    p(y = k \mid x, \cD)
    &\!\propto\!
    \begin{cases}
    N_k \, \mathcal{N}(x \mid \mu_k', \Sigma_k' + \Sigma) &\!\!\!\text{if } k\in[K] \\
    \alpha \, \mathcal{N}(x \mid \mu_0, \Sigma_0 + \Sigma) &\!\!\!\text{if } k = K+1.
    \end{cases}
\end{align}
where
\begin{align}
    \nonumber \mu_k' &= \Sigma_k' \left(\Sigma_0^{-1} \mu_0 +  N_k \Sigma^{-1} \bar{x}_k\right), \\
    \label{eq:gaussian_tied_postpred}
    \Sigma_k' &= \left( \Sigma_0^{-1} + N_k \Sigma^{-1} \right)^{-1},
\end{align}
and $\bar{x}_k = \tfrac{1}{N_k} \sum_{x_n \in \cD_k} x_n$ is the mean of the data points assigned to cluster~$k$.

The relative probability of theses cases is an intuitive measure of how likely a point is to be an outlier.
Indeed, the next section shows that the outlier probabilities from this Bayesian nonparametric model are closely related to another common outlier detection score.
