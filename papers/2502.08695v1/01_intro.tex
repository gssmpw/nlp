\section{Introduction}
\label{sec:intro}
Machine learning systems inevitably face data that deviate from their training distributions.
Generally, this data is either sparsely labeled or wholly unsupervised.
Faced with such a dynamic environment, an intelligent system must accurately detect outliers and respond appropriately.
This capability is the subject of modern research on out-of-distribution~(OOD) detection~\citep{HendrycksD17}, anomaly detection~\cite{chandola2009anomaly}, and open-set recognition~\citep{scheirer12openset}.

Outlier detection is one of the oldest problems in statistics~\citep{anscombe1960rejection}, and there are well-established methods for tackling this canonical problem.
For example, Bayesian nonparametric methods offer a coherent, probabilistic framework for estimating the probability that a data point belongs to a new cluster~\citep{ferguson1973bayesian,antoniak74dpmm,lo1984class,Sethuraman94, maceachern1994estimating, neal00}.
Here, we use Dirichlet Process Mixture Models (DPMMs) to fit a generative model to the training data.
Under this model, OOD detection reduces to a straightforward computation of the probability that a data point belongs to a novel class.

Care must be taken with Bayesian nonparametric methods, however.
Modern machine learning systems are often built on foundation models that have been trained on massive datasets~\citep{dosovitskiy20vit,caron21dino,oquab23dinov2,darcet23needreg,chen20simclr,chen20simclrv2}.
These models yield feature embeddings of data points that can be used for several downstream tasks, but the embeddings are high-dimensional.
When fitting a Gaussian DPMM, for example, we must implicitly estimate the covariance of embeddings within and across classes, which presents both computational and statistical challenges.
We propose a hierarchical model that adaptively shares statistical strength across classes when estimating these high-dimensional covariance matrices.

Generative classifiers like these have been largely eschewed in favor of simpler distance metrics, like the relative Mahalanobis distance score~\citep[RMDS;][]{ren21rmds}.
Here, we show both theoretically and empirically that RMDS is a close approximation to the outlier probability under a Gaussian DPMM with tied covariance matrices, connecting this widely-used approach to inference in a Bayesian nonparametric model.
From this perspective, we propose hierarchical models that generalize RMDS by relaxing the assumption of equal covariance matrices across classes.

We find that hierarchical Gaussian DPMMs offer a well-grounded and practically competitive approach to OOD detection.
Section~\ref{sec:relatedwork} reviews related work, and Section~\ref{sec:background} covers important background on DPMMs.
We make a theoretical connection between RMDS and DPMMs in Section~\ref{sec:theory}.
This connection motivates our use of hierarchical models for estimating the high-dimensional covariance matrices in DPMMs --- including a novel ``coupled diagonal covariance'' model --- which we describe in \Cref{sec:models} and evaluate in~\Cref{sec:experiments}.
We compare these models to existing baselines on synthetic datasets as well as the OpenOOD benchmark to characterize the regimes where Bayesian nonparameteric yield improved OOD performance\footnote{The implementation for all DPMM models and experiments is available at \url{https://github.com/rwl93/bnp4ood}.}.
