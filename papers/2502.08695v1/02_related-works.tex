\section{Related Work}
\label{sec:relatedwork}
The OOD detection task has been widely studied and many solutions have been proposed.
For example, some approaches alter the architecture or objective of a classifier~\citep{TackJ20,HuangR21,wei2022mitigating,linderman23}, and others exploit auxiliary outlier datasets~\citep{HendrycksD19,zhang2021mixture}.
Our approach is related to a class of post-hoc methods including max softmax probability~\citep[MSP;][]{HendrycksD17}, temperature-scaled MSP~\citep{guo17tempscale}, ODIN~\citep{LiangS18}, energy-based OOD~\cite{LiuW20}, the Mahalanobis distance score~\citep[MDS;][]{lee18mds}, and the Relative MDS~\citep{ren21rmds}, which derive OOD scores from embeddings or activations of a pre-trained network.

Recently, \citet{zhang23openood15} proposed a set of Near and Far OOD benchmarks, as well as a leaderboard named OpenOOD to facilitate comparison across methods. The OpenOOD benchmarks found (1) that post-hoc methods are more scalable to large datasets, (2) there is no method that is best on all datasets, and (3) methods are sensitive to which model was used for embedding.
The best performing OpenOOD methods for vision transformer (ViT) feature embeddings are the MDS and RMDS.
The relative Mahalanobis distance score was inspired by earlier work by~\citet{ren2019likelihood} that addressed the poor performance of the OOD performance with density estimation methods.
\citet{sun2022out} propose to relax some of the assumptions of Mahalanobis distance methods by using the negative $k$-th nearest neighbor distances instead.
We will show that the relative Mahalanobis distance score (RMDS) is similar to scores derived from Bayesian nonparametric mixture models in \Cref{sec:theory}.

Bayesian nonparametric methods have previously been proposed for outlier detection and used in several applications. \citet{shotwell2011} proposed to detect outliers within datasets by partitioning data via a DPMM and identifying clusters containing a small number of samples as outliers.
\citet{varadarajan17} developed a method for detecting anomalous activity in video snippets by modeling object motion with DPMMs. Another line of work explored Dirichlet prior networks~\citep[DPN;][]{malinin2018predictive,malinin2019reverse} that explicitly model distributional uncertainty arising from dataset shift as a Dirichlet distribution over the categorical class probabilities.
More recently, \citet{Kim2024UnsupervisedOD} performed unsupervised anomaly detection through an ensemble of Gaussian DPMMs fit to random projections of a subset of datapoints.
Our work focuses on connecting DPMMs to post-hoc confidence scores and developing \textit{hierarchical} Gaussian DPMMs that share statistical strength across classes in order to estimate their high-dimensional covariance matrices.
