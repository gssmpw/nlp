\section{Related Work}
\label{sec:relatedwork}
The OOD detection task has been widely studied and many solutions have been proposed.
For example, some approaches alter the architecture or objective of a classifier Hsu, "Deep Isometric Learning" __ Chen et al., "Training Confidence-calibrated Classifiers for Detection" .
Our approach is related to a class of post-hoc methods including max softmax probability Liang, "Enhancing Theoretical Understandings of Adversarially Trained Neural Networks: Part I - Robust Generalization via Maximum Softmax Probability" __ temperature-scaled MSP Hendrycks et al., "Deep Anomaly Detection with Outlier Exposure" __ ODIN Lee et al., "A Simple Unified Framework for Dimensionality Reduction" __ energy-based OOD Dziugaite et al., "Training Confidence-calibrated Classifiers for Detection" __ the Mahalanobis distance score Liang, "Enhancing Theoretical Understandings of Adversarially Trained Neural Networks: Part I - Robust Generalization via Maximum Softmax Probability" __ and the Relative MDS Zhang et al., "Towards Scalable Out-of-Distribution Detection for High-Dimensional Data" .

Recently, Liu et al., "A Simple Framework for Constructing Transferrable Multiple-Source Domain Adaptation"  proposed a set of Near and Far OOD benchmarks, as well as a leaderboard named OpenOOD to facilitate comparison across methods. The OpenOOD benchmarks found (1) that post-hoc methods are more scalable to large datasets, (2) there is no method that is best on all datasets, and (3) methods are sensitive to which model was used for embedding.
The best performing OpenOOD methods for vision transformer (ViT) feature embeddings are the MDS Zhang et al., "Towards Scalable Out-of-Distribution Detection for High-Dimensional Data"  and RMDS.
The relative Mahalanobis distance score was inspired by earlier work by Lee, "A Simple Unified Framework for Dimensionality Reduction"  that addressed the poor performance of the OOD performance with density estimation methods.
Dziugaite et al., "Training Confidence-calibrated Classifiers for Detection"  propose to relax some of the assumptions of Mahalanobis distance methods by using the negative $k$-th nearest neighbor distances instead.
We will show that the relative Mahalanobis distance score (RMDS) is similar to scores derived from Bayesian nonparametric mixture models in \Cref{sec:theory}.

Bayesian nonparametric methods have previously been proposed for outlier detection and used in several applications. Chaudhuri et al., "On the Convergence Rates of Nonparametric Mixtures"  proposed to detect outliers within datasets by partitioning data via a DPMM and identifying clusters containing a small number of samples as outliers.
Dziugaite et al., "Training Confidence-calibrated Classifiers for Detection"  developed a method for detecting anomalous activity in video snippets by modeling object motion with DPMMs. Another line of work explored Dirichlet prior networks Oates et al., "Visual Object Segmentation via Dirichlet Process Mixture Models"  that explicitly model distributional uncertainty arising from dataset shift as a Dirichlet distribution over the categorical class probabilities.
More recently, Nguyen et al., "Unsupervised Anomaly Detection using Gaussian DPMMs"  performed unsupervised anomaly detection through an ensemble of Gaussian DPMMs fit to random projections of a subset of datapoints.
Our work focuses on connecting DPMMs to post-hoc confidence scores and developing \textit{hierarchical} Gaussian DPMMs that share statistical strength across classes in order to estimate their high-dimensional covariance matrices.