\section{Experiments}
\label{sec:experiments}

We experimentally tested these hierarchical Gaussian DPMMs on real and synthetic datasets.
First we used simulated datasets to build intuition for where hierarchical models improve performance.
Then we compared hierarchical Gaussian DPMMs to other widely-used OOD metrics on Imagenet-1K embeddings from a ViT,
and we studied performance versus dimensionality of the embeddings.

\input{EM_MCWonly}
\subsection{Synthetically generated dataset experiments}
To understand the regimes in which hierarchical models yield benefits, we
simulated $D=2$ dimensional data from full covariance models with varying
$\nu_0$ and $N_k$. When $\nu_0$ is small, covariances differ considerably from
one class to the next, and the assumptions of the tied model (and of RMDS) are
not well met. Conversely, as $\nu_0 \to \infty$, the prior concentrates on
$\Sigma_0$, and the covariances are effectively tied. \Cref{fig:synthpanel}A
and \ref{fig:synthpanel}B show simulated datasets from these two regimes. As
expected, \Cref{fig:synthpanel}C shows that hierarchical Gaussian DPMMs yielded
considerable improvements when $\nu_0$ was small, and the largest improvements
came from the full covariance model, which matched the data generating process.
Notably, the tied model matched the RMDS performance, as predicted in
\Cref{sec:theory}.

We then asked if the hierarchical model was strictly necessary or whether a
simpler model would suffice. For example, we considered an ``Independent RMDS''
based on Mahalanobis distances to the per-class covariance estimates,
$\hat{\Sigma}_k$, instead of the average covariance $\hat{\Sigma}$.
Intuitively, we expected the hierarchical models to perform best when there
were few data points per cluster relative to the dimensionality of the
embeddings; i.e., when $N_k/D$ is small. Indeed, \Cref{fig:synthpanel}D shows
that DPMMs offered substantial improvements in this regime, with diminishing
gains as $N_k$ increased.

Taken together, these analyses suggest that hierarchical Gaussian DPMMs should
yield benefits in regimes where covariances differ across classes and the
number of data points per class is relatively small.

\subsection{OpenOOD Performance on Imagenet-1K}

Next, we compared hierarchical Gaussian DPMMs to other widely-used outlier
detection methods on  the OpenOOD benchmark
datasets~\cite{yang2022openood,zhang23openood15}. The benchmark assesses how
well a method can distinguish Imagenet-1K clusters from other image datasets.
The datasets are grouped into \textit{Near} and \textit{Far}, where the Near
OOD datasets are more similar to Imagenet-1K. We used $D=768$ dimensional
embeddings from the ViT-B-16 model trained according to the DeIT
method~\citep{touvron2021training}, which are available in the Pytorch
\texttt{torchvision}~\citep{torchvision2016} package. We preprocessed the
embeddings as described in~\Cref{app:preprocessing}. As baselines, we
considered both MDS~\citep{lee18mds} and RMDS~\citep{ren21rmds}. We also
compare to maximum softmax probability (MSP)~\citep{HendrycksD17} and
temperature scaled MSP with $T=1000$ (Temp. Scale)~\citep{guo17tempscale},
which is the same as ODIN~\citep{LiangS18} but without input preprocessing. We
trained a single linear layer with gradient descent and supervised
cross-entropy loss for the MSP and ODIN methods. For all models, we measured
the accuracy of classifying which class an in-distribution test image came
from, as well as the AUROC score for outlier detection across the OpenOOD
datasets. For DPMMs, we computed AUROC scores using~$\widetilde{C}(x)$.

Table~\ref{tab:openood-main} shows that hierarchical models do offer improved
performance, but the results are nuanced. We found improvements primarily in
Near OOD settings, where the outliers are more similar to the in-distribution
clusters. In particular, the Coupled Diagonal model yields the highest
performance on the both Near OOD benchmarks. In Far OOD settings, the benefits
of estimating covariance matrices for each class were muted; there the
tied-covariance model performed best. We suspect that for Near OOD problems,
correctly identifying the shape of individual clusters is more important. In
all cases, RMDS and the tied-covariance model had nearly identical performance,
as expected. The Full Covariance model performed surprisingly poorly on these
benchmarks, and we suspected it was because of the high-dimensional embeddings.

\subsection{Performance vs. Dimensionality}

\input{autowhiten_study}

Finally, we asked how these methods compare as we vary the dimensionality of
the embeddings. We suspected that the Full Covariance model would perform
better in lower dimensions for two reasons. First, it has $\cO(KD^2)$
parameters compared to only $\cO(KD)$ in the diagonal models and $\cO(D^2)$ in
the tied model, so even with a hierarchical prior, the full model could still
overfit the data. This problem is exacerbated for classes that have fewer data
points than the feature dimension, in which case the prior has a strong effect
on the conditional distribution of the per-class covariance matrix and the
posterior predictive distributions. Second, we suspected that the inverse
Wishart prior distribution, which has only a scalar concentration
parameter~$\nu_0$, may be a poor prior for high-dimensional covariance
matrices.

To test this hypothesis, we swept the number of principal components retained
in preprocessing (see~\cref{app:preprocessing}). We found that
out-of-distribution detection of the full-covariance hierarchical model
plateaued for $D \geq 128$ dimensional embeddings
(\cref{fig:autowhiten-study}). By contrast, the diagonal and coupled diagonal
models performed considerably better, especially on Near OOD benchmarks. The
diagonal covariance model outperforms the tied model across all dimensions on
Near OOD detection, as well as in lower dimensions for Far OOD. However,
in-distribution classification accuracy plateaus around 256 dimensions.

Altogether, these analyses of synthetic and real datasets demonstrate that
hierarchical Gaussian DPMMs are advantageous for OOD detection, especially in
regimes where: \textit{i)}~covariance matrices differ across clusters;
\textit{ii)}~the number of data points per cluster is small compared to the
dimension; and \textit{iii)}~detection relies on fine-grained distinctions
between training data and Near OOD test points.
