\section{Hierarchical Gaussian DPMMs}
\label{sec:models}


RMDS has proven to be a highly effective outlier detection method, but it assumes that all clusters share the same covariance.
This assumption helps avoid overfitting the covariance matrices for each class~\citep{ren21rmds}, but it is not always warranted.
\Cref{fig:classwise-forstner} shows a histogram of differences between empirical covariance matrices $\hat{\Sigma}_k$ and $\hat{\Sigma}_{k'}$ for all pairs of classes $(k,k')$ in the Imagenet-1K dataset, as measured by the F\"orstner-Moonen distance~\citep{forstner_metric_2003}.

\input{samplecov-forstner-hist}
These pairwise distances are systematically larger than what we would expect under a null distribution where the true covariance matrices are the same for all classes, and the empirical estimates differ solely due to sampling variability.
Complete details of this analysis are provided in~\Cref{app:exploratory_details}.
This analysis suggests that the covariance matrices are significantly different across classes and motivates a more flexible approach.

The connection between RMDS and Gaussian DPMMs established above suggests a natural way of relaxing the tied-covariance assumption without sacrificing statistical power:
Instead of estimating covariance matrices independently, we could infer them jointly under a hierarchical Bayesian model~\citep{gelman1995bayesian}.
With such a model, we can estimate separate covariance matrices for each cluster, while sharing information via a prior.
By tuning the strength of the prior, we can obtain the tied covariance model in one limit and a fully independent model in the other.
Finally, we can estimate these hierarchical prior parameters using a simple expectation-maximization algorithm that runs in a matter of minutes, even with large, high-dimensional datasets.

\subsection{Full Covariance Model}
\label{sec:hierarchical-cov}

First, we propose a hierarchical Gaussian DPMM with full covariance matrices and a conjugate prior.
The cluster parameters, $\theta_k = (\mu_k, \Sigma_k)$, are drawn from a conjugate, normal-inverse Wishart (NIW) prior,
\begin{equation}
    p(\theta_k) = \mathrm{IW}\big(\Sigma_k \mid \nu_0, (\nu_0 - D - 1) \Sigma_0 \big)
    \times \cN\big(\mu_k \mid \mu_0, \kappa_0^{-1} \Sigma_k \big),
\end{equation}
where $\mathrm{IW}$ denotes the inverse Wishart density. Under this parameterization, $\E[\Sigma_k] = \Sigma_0$ for $\nu_0 > D + 1$.
The hyperparameters of the prior are~$\eta = (\nu_0, \kappa_0, \mu_0, \Sigma_0)$.

The most important hyperparameters are $\nu_0$ and $\Sigma_0$, as they specify the prior on covariance matrices.
As $\nu_0 \to \infty$, the prior concentrates around its mean and we recover a tied covariance model. For small values of $\nu_0$, the hierarchical model shares little strength across clusters, and the covariance estimates are effectively independent.

We propose a simple approach to estimate these hyperparameters in~\Cref{app:em-hdpmm}. Briefly, we use empirical Bayes estimates for the prior mean and covariance, setting $\mu_0 = \hat{\mu}_0$ and $\Sigma_0 = \hat{\Sigma}$. We derive an expectation-maximization~(EM) algorithm to optimize $\nu_0$ and $\kappa_0$. Thanks to the conjugacy of the model, the E-step and the M-step for $\kappa_0$ can be computed in closed form. We leverage a generalized Newton method~\citep{minka2000beyond} to update the concentration hyperparameter, $\nu_0$, effectively learning the strength of the prior to maximize the marginal likelihood of the data.

Finally, the prior and posterior predictive distributions are  multivariate Student's t distributions with closed-form densities.
The log density ratios derived from these predictive distributions form the basis of the DPMM scores, $\widetilde{C}(x)$.

\subsection{Diagonal Covariance Model}

Even with the hierarchical prior, we find that the full covariance model can still overfit to high-dimensional embeddings.
Thus, we also consider a simplified version of the hierarchical Gaussian DPMM with diagonal covariance matrices.
Here, the cluser parameters are $\theta_k = \{\mu_{k,d}, \sigma_{k,d}^2\}_{d=1}^D$, and the conjugate prior is,
\begin{equation}
    p(\theta_k) = \prod_{d=1}^D \chi^{-2}(\sigma_{k,d}^2 \mid \nu_{0,d}, \sigma_{0,d}^2)
    \times \cN(\mu_{k,d} \mid \mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2)
\end{equation}
where $\chi^{-2}$ is the scaled inverse chi-squared density.

In addition to having fewer parameters per cluster, another advantage of this model is that it allows for
different concentration hyperparameters for each dimension, $\nu_{0,d}$. We estimate the hyperparameters using a
procedure that closely parallels the full covariance model. Likewise, the prior
and posterior predictive densities reduce to products of scalar Student's t
densities, which are even more efficient to compute.
Complete details are in~\Cref{app:em-diag-hdpmm}.

\input{covariance-analysis}
\subsection{Coupled Diagonal Covariance Model}

The diagonal covariance model dramatically reduces the number of parameters per cluster, but it also makes a strong assumption about the per-class covariance matrices.
Specifically, it assumes the variances, $\sigma_{k,d}^2$, are conditionally independent across dimensions.
\Cref{fig:cov-analysis} suggests that this is not the case: the diagonals of the empirical covariance matrices, $\hat{\Sigma}_k$, tend to be systematically larger
or smaller than those of the average covariance matrix, $\hat{\Sigma}$.
This analysis suggests that $\sigma_{k,d}^2$ are not independent; rather, if $\sigma_{k,d}^2$ is larger than average, then $\sigma_{k,d'}^2$ is likely to be larger as well.

We propose a novel, \emph{coupled} diagonal covariance model to capture these effects. Based on the analysis above, we introduce a scale factor $\gamma_k \in \bbR_+$ that shrinks or amplifies the variances for class~$k$ compared to the average.
In this model, the cluster parameters are $\theta_k = (\gamma_k, \{\mu_{k,d}, \sigma_{k,d}^2\}_{d=1}^D)$, and the prior is,
\begin{equation}
    p(\theta_k) =
    \chi^2(\gamma_k \mid \alpha_0)
    \prod_{d=1}^D \bigg[\chi^{-2}(\sigma_{k,d}^2 \mid \nu_{0,d}, \gamma_k \sigma_{0,d}^2)
    \times \cN(\mu_{k,d} \mid \mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2) \bigg]
\end{equation}
where $\gamma_k$ scales the means of $\sigma_{k,d}^2$ for all dimensions $d$ in order to capture the correlations seen above.

\input{synth-panel}

Our procedure for hyperparameter estimation and computing DPMM scores is very similar to those described above.
The only complication is that with the $\gamma_k$, the posterior distribution no longer has a simple closed form.
However, for any fixed value of $\gamma_k$, the coupled model is a straightforward generalization of the diagonal model above.
Since $\gamma_k$ is a one-dimensional variable, we can use numerical quadrature to integrate over its possible values.
Likewise, we can estimate the hyperparameter $\alpha_0$ using a generalized Newton method, just like for the concentration parameter $\nu_0$.
See \Cref{app:em-coupled} for complete details of this model.
