\section{Preprocessing}
\label{app:preprocessing}

Before computing OOD scores, we first preprocessed the embeddings using PCA to
whiten and sort the dimensions in order of decreasing variance. We discarded
dimensions with near zero variance to ensure the empirical covariance matrices
were full rank. We scaled each dimension by the inverse square root of the
eigenvalues so that the transformed embeddings had identity covariance.
Finally, we rotated the embeddings using the eigenvectors of the average
covariance matrix, so that the average within-class covariance matrix is
diagonal.

More precisely, the preprocessing steps are as follows:
\begin{enumerate}
    \item Let $\{x_i\}_{i=1}^N$ denote the mean-centered embeddings.

    \item Let $\hat{\Sigma}_0 = U \Lambda U^\top$ denote the covariance of the
      centered embeddings and its eigendecomposition. Discard any dimensions
      with eigenvalues less than a threshold of approximately~$10^{-7}$, and
      then we project and scale the embeddings by,
    \begin{align}
        x_i' &\leftarrow \Lambda^{-\frac{1}{2}} U^T x_i,
    \end{align}
    so that the empirical covariance of $\{x_i'\}_{i=1}^n$ is the identity matrix.

    \item Compute the average within-class covariance $\hat{\Sigma} = \frac{1}{N} \sum_{i=1}^M (x_i' - \hat{\mu}_{y_i})(x_i' - \hat{\mu}_{y_i})^\top$, where $\hat{\mu}_k = \frac{1}{N_k} \sum_{i:y_i=k} x_i'$.

    \item Compute the eigendecomposition $\hat{\Sigma} = V S V^\top$, with
      eigenvalues $S = \mathrm{diag}(\sigma_1^2, \ldots, \sigma_D^2)$ sorted in
      increasing order of magnitude so that the first dimension has the
      smallest within-class covariance. The embeddings~$x_i'$ have unit
      variance in all dimensions, but along the dimension of the first
      eigenvector in~$V$, the average within-class covariance is smallest.

    \item Project the embeddings into this eigenbasis,
    \begin{align}
        z_i &\leftarrow V^\top x_i'.
    \end{align}
\end{enumerate}
After these preprocessing steps, the resulting embeddings $\{z_i\}_{i=1}^N$ are
zero mean ($\hat{\mu}_0=0$), their empirical covariance is the identity
($\hat{\Sigma}_0=I$), and the average within-class covariance is diagonal
($\hat{\Sigma}=\mathrm{diag}(\sigma_1^2, \ldots, \sigma_D^2)$). The empirical
within-class covariance matrix $\hat{\Sigma}_k$ for class~$k$ will \emph{not}
generally be diagonal, but this sequence of preprocessing steps is intended to
make them closer to diagonal on average.

Note that the relative Mahalanobis distance score is invariant to these linear
transformations. They simply render the embeddings more amenable to our
hierarchical models with diagonal covariance. We further test the effect of
these preprocessing steps via the ablation experiment described in
\Cref{app:ablation}.

\section{Further Details of Exploratory Analyses}
\label{app:exploratory_details}

First, we investigated the degree to which the sample covariance matrices
differ between the 1000 classes in the Imagenet-1K dataset.
In accordance with the OpenOOD benchmark, we used embeddings from the ViT-B-16
model trained according to the DeIT method~\citep{touvron2021training}, which
are~$D=768$ dimensional.
The embeddings are available in the Pytorch
\texttt{torchvision}~\citep{torchvision2016} package.
We preprocessed the embeddings as described in~\Cref{app:preprocessing}.
For this analysis, we only kept the top 128 PCs to speed computation.

To measure the distance between covariance matrices for two clusters, we used
the F\"orstner-Moonen (FM) metric~\citep{forstner_metric_2003},
\begin{align}
d(\mbSigma_1, \mbSigma_2) = \bigg[\sum_{i=1}^n (\log  \lambda_i (\mbSigma_1^{-1} \mbSigma_2))^{2}\bigg]^{\frac{1}{2}},
\end{align}
where $\lambda_i(\mbSigma_1^{-1} \mbSigma_2)$ is the $i$-th eigenvalue of $\mbSigma_1^{-1} \mbSigma_2$.
We computed the FM metric for all pair of empirical covariance matrices
$(\hat{\Sigma}_k, \hat{\Sigma}_{k'})$. We compared the distribution of FM
distances under the real data to distances between covariance matrices sampled
from the null model, in which~$\Sigma_k$ truly equals~$\hat{\Sigma}$ for
all~$k$, and the differences in the estimates~$\hat{\Sigma}_k$ arise solely
from sampling error. The corresponding null distribution is a Wishart
distribution, $\hat{\Sigma}_k \sim \mathrm{W}(\overline{N},
\hat{\Sigma}/\overline{N})$, where $\overline{N}$ is the average number of data
points per class.

\section{Compute Resources}
\label{app:compute}
The OOD experiments were performed on a cluster consisting of compute nodes
with 8 NVIDIA RTX A5000 GPUs. The
OpenOOD~\citep{yang2022openood,zhang23openood15} experiments on the Imagenet-1K
dataset~\citep{RussakovskyO15} utilized weights available from
Pytorch's~\citep{PYTORCH} torchvision~\citep{torchvision2016} models. The
compute across experiments was reduced by storing summary statistics of the
embeddings for the Gaussian models. The DPMM fitting and prediction was
performed on the CPU except for calculating the posterior and prior predictive
distributions for each sample.

\section{EM Algorithm for the Full Covariance Model}
\label{app:em-hdpmm}
Here we describe an expectation-maximization (EM) algorithm for estimating the hyperparameters of the hierarchical covariance model. Recall that under this model,
\begin{align*}
    \Sigma_k &\sim \mathrm{IW}(\nu_0, (\nu_0 - D - 1) \Sigma_0) \\
    \mu_k &\sim \mathrm{N}(\mu_0, \kappa_0^{-1} \Sigma_k)
\end{align*}
so that the prior hyperparameter $\Sigma_0$ specifies the mean of the per-class covariances, $\E[\Sigma_k] = \Sigma_0$. This hyperparameter should not be confused with $\hat{\Sigma}_0$ defined in the main text, which denoted the empirical estimate of the marginal covariance. Also note that this prior formulation requires $\nu_0 > D+1$.

First, we set the hyperparameters $\mu_0$ and $\Sigma_0$ using empirical Bayes estimates,
\begin{align}
    \mu_0 &= \hat{\mu}_0 = \frac{1}{N} \sum_{n=1}^N x_n \\
    \Sigma_0 &= \hat{\Sigma} = \frac{1}{N} \sum_{n=1}^N (x_n - \hat{\mu}_{y_n})(x_n - \hat{\mu}_{y_n})^\top
\end{align}
where $\hat{\mu}_{y_n} = \frac{1}{N_k} \sum_{n: y_n=k} x_n$.

To set the remaining hyperparameters, $\kappa_0$ and $\nu_0$, we use EM. The expected log likelihood is separable over these two hyperparameters,
\begin{align*}
    \cL(\nu_0, \kappa_0) &= \cL(\nu_0) + \cL(\kappa_0) \\
    \cL(\nu_0)
    &= \E\left[\sum_{k=1}^K \log \mathrm{IW}(\Sigma_k \mid \nu_0, (\nu_0 - D - 1)\Sigma_0) \right] \\
    \cL(\kappa_0)
    &= \E \left[ \sum_{k=1}^K \log \mathrm{N}(\mu_k \mid \mu_0, \kappa_0^{-1} \Sigma_k) \right]
\end{align*}
where the expectations are taken with respect to the posterior distribution over $\{\mu_k, \Sigma_k\}_{k=1}^K$.

\subsection{M-step for \texorpdfstring{$\nu_0$}{Î½â‚€}}
Expanding the first objective yields,
\begin{align*}
    \cL(\nu_0)
    &= \sum_{k=1}^K \E \left[ \log \left\{ \frac{|(\nu_0 - D - 1)\Sigma_0|^{\frac{\nu_0}{2}}}{2^{\frac{\nu_0 D}{2}} \Gamma_D(\frac{\nu_0}{2}) } |\Sigma_k|^{-\frac{\nu_0 + D + 1}{2}} e^{-\mathrm{Tr}(\tfrac{\nu_0 - D - 1}{2} \Sigma_0 \Sigma_k^{-1})} \right\}  \right] \\
    &= \sum_{k=1}^K  \tfrac{\nu_0 D}{2} \log \left(\tfrac{\nu_0 - D - 1}{2}\right) + \tfrac{\nu_0}{2} \log |\Sigma_0| - \log \Gamma_D(\tfrac{\nu_0}{2}) -\tfrac{\nu_0 + D + 1}{2} \E[\log |\Sigma_k|] - \tfrac{\nu_0 - D - 1}{2} \mathrm{Tr}(\Sigma_0 \E[\Sigma_k^{-1}]) \\
    &= \sum_{k=1}^K  \tfrac{\nu_0 D}{2} \log \left(\tfrac{\nu_0 - D - 1}{2}\right) + \tfrac{\nu_0}{2} \left[\log |\Sigma_0| - \E[\log |\Sigma_k|] - \mathrm{Tr}(\Sigma_0 \E[\Sigma_k^{-1}]) \right] - \log \Gamma_D(\tfrac{\nu_0}{2}).
\end{align*}
We can maximize this objective using a generalized Newton's method~\citep{minka2000beyond}.
We need the first and second derivatives of the objective,
\begin{align*}
    \cL'(\nu_0)
    &= \sum_{k=1}^K  \tfrac{D}{2} \left[ \log \left(\tfrac{\nu_0 - D - 1}{2}\right) + \tfrac{\nu_0}{\nu_0 - D - 1} \right] + \tfrac{1}{2} \left[\log |\Sigma_0| - \E[\log |\Sigma_k|] - \mathrm{Tr}(\Sigma_0 \E[\Sigma_k^{-1}]) \right] - \tfrac{1}{2} \psi_D(\tfrac{\nu_0}{2}) \\
    \cL''(\nu_0)
    &= \sum_{k=1}^K  \tfrac{D}{2} \left[\tfrac{1}{\nu_0 - D - 1} -  \tfrac{D+1}{(\nu_0 - D - 1)^2}\right] - \tfrac{1}{4} \psi_D^{(2)}(\tfrac{\nu_0}{2}).
\end{align*}
The idea is to lower bound the objective with a concave function of the form,
\begin{align*}
    g(\nu_0) = k + a \log \nu_0 + b \nu_0
\end{align*}
which has derivatives $g'(\nu_0) = \tfrac{a}{\nu_0} + b$ and $g''(\nu_0) = -\tfrac{a}{\nu_0^2}$. Matching derivatives implies,
\begin{align*}
    a &= -\nu_0^2 \cL''(\nu_0) \\
    b &= \cL'(\nu_0) - \tfrac{a}{\nu_0} \\
    k &= \cL(\nu_0) - a\log \nu_0 -b \nu_0.
\end{align*}
For $a > 0$ and $b<0$, the maximizer of the lower bound is obtained at
\begin{align}
    \nu_0^\star
    &= -\frac{a}{b} = \frac{\nu_0^2 \cL''(\nu_0)}{\cL'(\nu_0) + \nu_0 \cL''(\nu_0)}
\end{align}

\subsection{M-step for \texorpdfstring{$\kappa_0$}{Îºâ‚€}}
Expanding the second objective,
\begin{align*}
    \cL(\kappa_0) &= \sum_{k=1}^K \tfrac{D}{2} \log \kappa_0 - \tfrac{\kappa_0}{2} \E \left[(\mu_k - \mu_0)^\top \Sigma_k^{-1} (\mu_k - \mu_0) \right] + c.
\end{align*}
The maximum is obtained at,
\begin{align*}
    \kappa_0^\star &= \left(\frac{1}{KD} \sum_{k=1}^K \E \left[(\mu_k - \mu_0)^\top \Sigma_k^{-1} (\mu_k - \mu_0) \right]\right)^{-1}.
\end{align*}

\subsection{Computing the posterior expectations}
Under the conjugate prior, those posteriors are normal inverse Wishart distributions,
\begin{align*}
    \mu_k, \Sigma_k \mid \{x_n: y_n = k\}
    &\sim \mathrm{NIW}(\nu_k', \Sigma_k', \kappa_k', \mu_k') \\
    \nu_k' &= \nu_0 + N_k \\
    \kappa_k' &= \kappa_0 + N_k \\
    \mu_k' &= \frac{1}{\kappa_k'} \left(\kappa_0 \mu_0 + \sum_{n: y_n=k} x_n \right) \\
    \Sigma_k' &= (\nu_0 - D - 1) \Sigma_0 + \kappa_0 \mu_0 \mu_0^\top + \sum_{n:y_n=k} x_n x_n^\top - \kappa_k' \mu_k' \mu_k'^\top
\end{align*}
To evaluate the objectives above, we need the following expected sufficient statistics of the normal inverse Wishart distribution,
\begin{align*}
    \E[\Sigma_k^{-1}] &= \nu_k' \Sigma_k'^{-1} \\
    \E[\log |\Sigma_k|] &= \log |\Sigma_k'| - \psi_D(\tfrac{\nu_k'}{2}) - D \log 2 \\
    \E[(\mu_k - \mu_0)^\top \Sigma_k^{-1} (\mu_k - \mu_0)] &= \frac{1}{\kappa_k'} + (\mu_k' - \mu_0)^\top \E[\Sigma_k^{-1}] (\mu_k' - \mu_0)
\end{align*}

Since the tied covariance approach in RMDS already works quite well, we recommend initializing the EM iterations by setting $\nu_0 \approx \bar{N}_k$ and $\kappa_0 \approx 0$. That way, the covariances are strongly coupled across clusters and the means have an uninformative prior.

\subsection{Marginal Likelihood}
This EM algorithm maximizes the marginal likelihood,
\begin{align*}
    \log p(\{x_n, y_n\}_{n=1}^N)
    &= \sum_{k=1}^K \log p(\{x_n: y_n =k\})  \\
    &= \sum_{k=1}^K \log \int p(\{x_n: y_n=k\} \mid \mu_k, \Sigma_k) \, p(\mu_k, \Sigma_k) \dif \mu_k \dif \Sigma_k \\
    &= \sum_{k=1}^K \log \int \left[\prod_{n:y_n=k} \mathrm{N}(x_n \mid \mu_k, \Sigma_k) \right] \mathrm{NIW}(\mu_k, \Sigma_k \mid \nu_0, \Sigma_0, \kappa_0, \mu_0) \dif \mu_k \dif \Sigma_k \\
    &= \sum_{k=1}^K \log Z(\nu_k', \Sigma_k', \kappa_k', \mu_k') - \log Z(\nu_0, (\nu_0 - D - 1) \Sigma_0, \kappa_0, \mu_0) + c
\end{align*}
where
\begin{align*}
    \log Z(\nu, \Sigma, \kappa, \mu) = -\tfrac{D}{2} \log \kappa + \log \Gamma_D(\tfrac{\nu}{2}) + \tfrac{\nu D}{2}\log 2  - \tfrac{\nu}{2} \log |\Sigma|
\end{align*}
is the log normalizer of the normal inverse Wishart distribution, and $c$ is constant with respect to the hyperparameters being optimized (but it is data dependent).


\section{EM Algorithm for the Diagonal Covariance Model}
\label{app:em-diag-hdpmm}
Here we describe an expectation-maximization (EM) algorithm for estimating the hyperparameters of the hierarchical diagonal covariance model. Recall that under this model,
\begin{align*}
    x_{n,d} \mid y_n=k &\sim \mathrm{N}(\mu_{k,d}, \sigma_{k,d}^2)
\end{align*}
where
\begin{align*}
    \sigma_{k,d}^2 &\sim \chi^{-2}(\nu_{0,d}, \sigma_{0,d}^2) \\
    \mu_{k,d} &\sim \mathrm{N}(\mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2),
\end{align*}
for each dimension $d=1,\ldots,D$ independently.
Under the prior $\E[\sigma_{k,d}^2] = \frac{\nu_{0,d}}{\nu_{0,d} - 2} \sigma_{0,d}^2$, which is approximately $\sigma_{0,d}^2$ for large degrees of freedom $\nu_{0,d}$.

First, we set the hyperparameters $\mu_{0,d}$ and $\sigma_{0,d}^2$ using empirical Bayes estimates,
\begin{align}
    \mu_{0,d} &= \hat{\mu}_{0,d} = \frac{1}{N} \sum_{n=1}^N x_{n,d} \\
    \sigma_{0,d}^2 &= \hat{\sigma}_{d}^2 = \frac{1}{N} \sum_{n=1}^N (x_{n,d} - \hat{\mu}_{y_n,d})(x_{n,d} - \hat{\mu}_{y_n,d})^\top
\end{align}
where $\hat{\mu}_{y_n,d} = \frac{1}{N_k} \sum_{n: y_n=k} x_{n,d}$.

To set the remaining hyperparameters, $\kappa_{0,d}$ and $\nu_{0,d}$, we use EM. The expected log likelihood is separable over these two hyperparameters,
\begin{align*}
    \cL(\nu_{0,d}, \kappa_{0,d}) &= \cL(\nu_{0,d}) + \cL(\kappa_{0,d}) \\
    \cL(\nu_{0,d})
    &= \E\left[\sum_{k=1}^K \log \chi^{-2}(\sigma_{k,d}^2 \mid \nu_{0,d}, \sigma_{0,d}^2) \right] \\
    \cL(\kappa_{0,d})
    &= \E \left[ \sum_{k=1}^K \log \mathrm{N}(\mu_{k,d} \mid \mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2) \right]
\end{align*}
where the expectations are taken with respect to the posterior distribution over $\{\mu_{k,d}, \sigma_{k,d}\}_{k=1}^K$.

\subsection{M-step for \texorpdfstring{$\nu_{0,d}$}{Î½â‚€}}
Expanding the first objective yields,
\begin{align*}
    \cL(\nu_{0,d})
    &= \sum_{k=1}^K \E \left[ \log \left\{ \frac{(\tfrac{\nu_{0,d}}{2} \sigma_{0,d}^2)^{\tfrac{\nu_{0,d}}{2}}}{\Gamma(\frac{\nu_{0,d}}{2}) } (\sigma_{k,d}^2)^{-\frac{\nu_{0,d} + 2}{2}} e^{-\frac{\nu_{0,d} \sigma_{0,d}^2}{2 \sigma_{k,d}^2}} \right\}  \right] \\
    &= \sum_{k=1}^K  \tfrac{\nu_{0,d}}{2} \log \left(\tfrac{\nu_{0,d}}{2}\right) + \tfrac{\nu_{0,d}}{2} \log \sigma_{0,d}^2 - \log \Gamma(\tfrac{\nu_{0,d}}{2}) -\tfrac{\nu_{0,d} + 2}{2} \E[\log \sigma_{k,d}^2] - \tfrac{\nu_{0,d}}{2} \sigma_{0,d}^2 \E[\sigma_{k,d}^{-2}] \\
    &= \sum_{k=1}^K  \tfrac{\nu_{0,d}}{2} \log \left(\tfrac{\nu_{0,d}}{2}\right) + \tfrac{\nu_{0,d}}{2} \left[\log \sigma_{0,d}^2 - \E[\log \sigma_{k,d}^2] - \sigma_{0,d}^2 \E[\sigma_{k,d}^{-2}]) \right] - \log \Gamma(\tfrac{\nu_{0,d}}{2}) + c.
\end{align*}
We can maximize this objective using a generalized Newton's method~\citep{minka2000beyond}.
We need the first and second derivatives of the objective,
\begin{align*}
    \cL'(\nu_{0,d})
    &= \sum_{k=1}^K  \tfrac{1}{2} \left[ \log \left(\tfrac{\nu_{0,d}}{2}\right) + 1 \right] + \tfrac{1}{2} \left[\log \sigma_{0,d}^2 - \E[\log \sigma_{k,d}^2] - \sigma_{0,d}^2 \E[\sigma_{k,d}^{-2}]) \right] - \tfrac{1}{2} \psi(\tfrac{\nu_{0,d}}{2}) \\
    \cL''(\nu_{0,d})
    &= \sum_{k=1}^K  \tfrac{1}{2 \nu_{0,d}} - \tfrac{1}{4} \psi'(\tfrac{\nu_{0,d}}{2}).
\end{align*}
The idea is to lower bound the objective with a concave function of the form,
\begin{align*}
    g(\nu_{0,d}) = k + a \log \nu_{0,d} + b \nu_{0,d}
\end{align*}
which has derivatives $g'(\nu_{0,d}) = \tfrac{a}{\nu_{0,d}} + b$ and $g''(\nu_{0,d}) = -\tfrac{a}{\nu_{0,d}^2}$. Matching derivatives implies,
\begin{align*}
    a &= -\nu_{0,d}^2 \cL''(\nu_{0,d}) \\
    b &= \cL'(\nu_{0,d}) - \tfrac{a}{\nu_{0,d}} \\
    k &= \cL(\nu_{0,d}) - a\log \nu_{0,d} -b \nu_{0,d}.
\end{align*}
For $a > 0$ and $b<0$, the maximizer of the lower bound is obtained at
\begin{align}
    \nu_{0,d}^\star
    &= -\frac{a}{b} = \frac{\nu_{0,d}^2 \cL''(\nu_{0,d})}{\cL'(\nu_{0,d}) + \nu_{0,d} \cL''(\nu_{0,d})}
\end{align}

\subsection{M-step for \texorpdfstring{$\kappa_{0,d}$}{Îºâ‚€}}
Expanding the second objective,
\begin{align*}
    \cL(\kappa_{0,d}) &= \sum_{k=1}^K \tfrac{1}{2} \log \kappa_{0,d} - \tfrac{\kappa_{0,d}}{2} \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right] + c.
\end{align*}
The maximum is obtained at,
\begin{align*}
    \kappa_{0,d}^\star &= \left(\frac{1}{K} \sum_{k=1}^K \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right]\right)^{-1}.
\end{align*}

\subsection{Computing the posterior expectations}
Under the conjugate prior, those posteriors are normal inverse chi-squared distributions,
\begin{align*}
    \mu_{k,d}, \sigma_{k,d}^2 \mid \{x_n: y_n = k\}
    &\sim \mathrm{NIX}(\nu_{k,d}', \sigma_{k,d}'^2, \kappa_{k,d}', \mu_{k,d}') \\
    \nu_{k,d}' &= \nu_{0,d} + N_{k} \\
    \kappa_{k,d}' &= \kappa_{0,d} + N_{k} \\
    \mu_{k,d}' &= \frac{1}{\kappa_{k,d}'} \left(\kappa_0 \mu_{0,d} + \sum_{n: y_n=k} x_n \right) \\
    \sigma_{k,d}'^2 &= \frac{1}{\nu_{k,d}'} \left[\nu_{0,d} \sigma_{0,d}^2 + \kappa_{0,d} \mu_{0,d}^2 + \sum_{n:y_n=k} x_{n,d}^2 - \kappa_{k,d}' \mu_{k,d}'^2 \right]
\end{align*}
To evaluate the objectives above, we need the following expected sufficient statistics of the normal inverse chi-squared distribution,
\begin{align*}
    \E[\sigma_{k,d}^{-2}] &= \sigma_{k,d}'^{-2} \\
    \E[\log \sigma_{k,d}^2] &= \log \tfrac{\nu_{k,d}' \sigma_{k,d}'^2}{2} - \psi(\tfrac{\nu_{k,d}'}{2}) \\
    \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right] &= \frac{1}{\kappa_{k,d}'} +  \frac{(\mu_{k,d}' - \mu_{0,d})^2}{\sigma_{k,d}'^2}
\end{align*}

Since the tied covariance approach in RMDS already works quite well, we recommend initializing the EM iterations by setting $\nu_{0,d} \approx \bar{N}_k$ and $\kappa_{0,d} \approx 0$. That way, the covariances are strongly coupled across clusters and the means have an uninformative prior.

\subsection{Marginal Likelihood}
This EM algorithm maximizes the marginal likelihood,
\begin{align*}
    \log p(\{x_n, y_n\}_{n=1}^N)
    &= \sum_{k=1}^K \log p(\{x_n: y_n =k\})  \\
    &= \sum_{d=1}^D \sum_{k=1}^K \log \int p(\{x_{n,d}: y_n=k\} \mid \mu_{k,d}, \sigma_{k,d}^2) \, p(\mu_{k,d} \sigma_{k,d}^2) \dif \mu_{k,d} \dif \sigma_{k,d}^2 \\
    &= \sum_{d=1}^D \sum_{k=1}^K \log \int \left[\prod_{n:y_n=k} \mathrm{N}(x_{n,d} \mid \mu_{k,d}, \sigma_{k,d}^2) \right] \mathrm{NIX}(\mu_{k,d}, \sigma_{k,d}^2 \mid \nu_{0,d}, \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d}) \dif \mu_{k,d} \dif \sigma_{k,d}^2 \\
    &= \sum_{d=1}^D \left[ \sum_{k=1}^K \left(\log Z(\nu_{k,d}', \sigma_{k,d}'^2, \kappa_{k,d}', \mu_{k,d}') - \log Z(\nu_{0,d}, \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d})\right) + c \right]
\end{align*}
where
\begin{align*}
    \log Z(\nu, \sigma^2, \kappa, \mu) = -\tfrac{1}{2} \log \kappa + \log \Gamma(\tfrac{\nu}{2}) - \tfrac{\nu}{2}\log \tfrac{\nu \sigma^2}{2}
\end{align*}
is the log normalizer of the normal inverse chi-squared distribution, and $c=\log (2\pi)^{-N/2}$ is constant with respect to the hyperparameters being optimized.

\subsection{Predictive Distributions}
Under this model, the predictive distribution of a data point given its cluster assignment is,
\begin{align*}
    p(x \mid y=k, X_{\mathsf{tr}}, y_{\mathsf{tr}})
    &= \prod_{d=1}^D \int p(x_d \mid \mu_{k,d}, \sigma_{k,d}^2) \, p(\mu_{k,d}, \sigma_{k,d}^2 \mid X_{\mathsf{tr}}, y_{\mathsf{tr}}) \dif \mu_{k,d} \dif \sigma_{k,d}^2 \\
    &= \prod_{d=1}^D \int \mathrm{N}(x_d \mid \mu_{k,d}, \sigma_{k,d}^2) \, \mathrm{NIX}(\mu_{k,d}, \sigma_{k,d}^2 \mid \nu_{k,d}', \sigma_{k,d}'^2, \kappa_{k,d}', \mu_{k,d}') \dif \mu_{k,d} \dif \sigma_{k,d}^2 \\
    &= \prod_{d=1}^D \mathrm{St}(x_d \mid \nu_{k,d}', \mu_{k,d}', \tfrac{\kappa_{k,d}' + 1}{\kappa_{k,d}'} \sigma_{k,d}'^2),
\end{align*}
where $\mathrm{St}(x \mid \nu, \mu, \sigma^2)$ denotes the univariate Student's t distribution with $\nu$ degrees of freedom, location $\mu$, and scale $\sigma$.

Under this model, the prior predictive distribution is,
\begin{align*}
    p(x \mid y=K+1, X_{\mathsf{tr}}, y_{\mathsf{tr}})
    &= \prod_{d=1}^D \mathrm{St}(x_d \mid \nu_{0,d}, \mu_{0,d}, \tfrac{\kappa_{0,d} + 1}{\kappa_{0,d}} \sigma_{0,d}^2),
\end{align*}
which is approximately Gaussian, $\mathrm{N}(x_d \mid \mu_{0,d}, \sigma_{0,d}^2)$ when $\nu_{0,d}, \kappa_{0,d} \gg 1$.

\section{EM Algorithm for the Coupled Diagonal Covariance Model}
\label{app:em-coupled}
This model introduces a scale factor $\gamma_k \in \reals_+$ that is shared by all dimensions. The model is,
\begin{align*}
    \gamma_k &\sim \chi^2(\alpha_0) \\
    \sigma_{k,d} &\sim \chi^{-2}(\nu_{0,d}, \gamma_k \sigma_{0,d}^2) & \text{for } d &=1,\ldots D  \\
    \mu_{k,d} &\sim \mathrm{N}(\mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2) &  \text{for } d &=1,\ldots D  \\
\end{align*}
Since $\E[\gamma_k] = 1$, under the prior $\E[\sigma_{k,d}^2] = \tfrac{\nu_{0,d}}{\nu_{0,d}-2} \sigma_{0,d}^2$, which is approximately $\sigma_{0,d}^2$ for large $\nu_{0,d}$.

The hyperparameters of the model are $\eta = (\alpha_0, \{\nu_{0,d}, \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d}\})$. We set the hyperparameters $\mu_{0,d}$ and $\sigma_{0,d}^2$ using empirical Bayes estimates,
\begin{align}
    \mu_{0,d} &= \hat{\mu}_{0,d} = \frac{1}{N} \sum_{n=1}^N x_{n,d} \\
    \sigma_{0,d}^2 &= \hat{\sigma}_{d}^2 = \frac{1}{N} \sum_{n=1}^N (x_{n,d} - \hat{\mu}_{y_n,d})^2
\end{align}
where $\hat{\mu}_{k} = \frac{1}{N_k} \sum_{n: y_n=k} x_n$ and $N_k = \sum_n \bbI[y_n = k]$.

To set the remaining hyperparameters, we use EM.

\subsection{E-step}
Note that the posterior distribution factors as,
\begin{align*}
    p(\gamma_k, \{\mu_{k,d}, \sigma_{k,d}^2\}_{d=1}^D \mid X_k)
    &= p(\gamma_k \mid X_k) \, p(\{\mu_{k,d}, \sigma_{k,d}^2\}_{d=1}^D \mid \gamma_k, X_k) \\
    &= p(\gamma_k \mid X_k) \prod_{d=1}^D p(\mu_{k,d}, \sigma_{k,d}^2 \mid \gamma_k, X_k).
\end{align*}

The posterior distribution over $\gamma_k$ doesn't have a simple closed form, but since it's only one-dimensional, we can approximate it on a dense grid of points, $\{\gamma^{(p)}\}_{p=1}^P$. Conditioned on $\gamma_k = \gamma^{(p)}$, the distribution of $\mu_{k,d}$ and $\sigma_{k,d}^2$ is a normal inverse chi-squared. For each point,
\begin{align*}
    p(\mu_{k,d}, \sigma_{k,d}^2 \mid \gamma_k = \gamma^{(p)}, X_k)
    &=\mathrm{NIX}(\mu_{k,d}, \sigma_{k,d}^2 \mid \nu_{k,d}', \sigma_{k,p,d}'^2, \kappa_{k,d}', \mu_{k,d}') \\
    \nu_{k,d}' &= \nu_{0,d} + N_{k} \\
    \kappa_{k,d}' &= \kappa_{0,d} + N_{k} \\
    \mu_{k,d}' &= \frac{1}{\kappa_{k,d}'} \left(\kappa_0 \mu_{0,d} + \sum_{n: y_n=k} x_n \right) \\
    \sigma_{k,p,d}'^2 &= \frac{1}{\nu_{k,d}'} \left[\nu_{0,d} \gamma^{(p)} \sigma_{0,d}^2 + \kappa_{0,d} \mu_{0,d}^2 + \sum_{n:y_n=k} x_{n,d}^2 - \kappa_{k,d}' \mu_{k,d}'^2 \right]
\end{align*}
Note that this is practically the same as above, but with $\gamma_k$ scaling the prior for $\sigma_{k,d}^2$.
For any value of $\gamma_k$, the posterior probability is,
\begin{align*}
    p(\gamma_k = \gamma^{(p)} \mid X_k)
    &\propto p(\gamma_k = \gamma^{(p)}) \, p(X_k \mid \gamma_k = \gamma^{(p)}) \\
    &= p(\gamma_k = \gamma^{(p)}) \prod_{d=1}^D p(X_{k,d} \mid \gamma_k = \gamma^{(p)}) \\
    &= p(\gamma_k = \gamma^{(p)}) \prod_{d=1}^D \frac{Z(\nu_{k,d}', \sigma_{k,p,d}'^2, \kappa_{k,d}', \mu_{k,d}')}{Z(\nu_{0,d}, \gamma^{(p)} \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d})} \\
    &\triangleq \tilde{w}_{k,p}'.
\end{align*}
where we reused the marginal likelihood calculation from the hierarchical diagaonal DPMM above. Finally, denote the normalized posterior probabilities as,
\begin{align*}
    w_{k,p}' &= \frac{\tilde{w}_{k,p}'}{\sum_r \tilde{w}_{k,r}'}.
\end{align*}


\subsection{M-Step}
To set the hyperparameters, $\kappa_{0,d}$, $\nu_{0,d}$, and $\alpha_0$, we use EM. The expected log likelihood is separable over these two hyperparameters,
\begin{align*}
    \cL(\nu_{0,d}, \kappa_{0,d}, \alpha_0) &= \cL(\nu_{0,d}) + \cL(\kappa_{0,d}) + \cL(\alpha_0) \\
    \cL(\nu_{0,d})
    &= \E\left[\sum_{k=1}^K \log \chi^{-2}(\sigma_{k,d}^2 \mid \nu_{0,d}, \gamma_k \sigma_{0,d}^2) \right] \\
    \cL(\kappa_{0,d})
    &= \E \left[ \sum_{k=1}^K \log \mathrm{N}(\mu_{k,d} \mid \mu_{0,d}, \kappa_{0,d}^{-1} \sigma_{k,d}^2) \right] \\
    \cL(\alpha_0)
    &= \E \left[ \log \mathrm{Ga}(\gamma_k \mid \alpha_0, \alpha_0) \right]
\end{align*}
where the expectations are taken with respect to the posterior distribution over $\{\gamma_k, \{\mu_{k,d}, \sigma_{k,d}\}_{d=1}^D \}_{k=1}^K$.

\subsection{M-step for \texorpdfstring{$\nu_{0,d}$}{Î½â‚€}}
Expanding the first objective yields,
\begin{align*}
    \cL(\nu_{0,d})
    &= \sum_{k=1}^K \E \left[ \log \left\{ \frac{(\tfrac{\nu_{0,d}}{2} \gamma_k \sigma_{0,d}^2)^{\tfrac{\nu_{0,d}}{2}}}{\Gamma(\frac{\nu_{0,d}}{2}) } (\sigma_{k,d}^2)^{-\frac{\nu_{0,d} + 2}{2}} e^{-\frac{\nu_{0,d} \gamma_k \sigma_{0,d}^2}{2 \sigma_{k,d}^2}} \right\}  \right] \\
    &= \sum_{k=1}^K  \tfrac{\nu_{0,d}}{2} \log \left(\tfrac{\nu_{0,d}}{2}\right) + \tfrac{\nu_{0,d}}{2} \E[\log \gamma_k] + \tfrac{\nu_{0,d}}{2} \log \sigma_{0,d}^2 - \log \Gamma(\tfrac{\nu_{0,d}}{2}) -\tfrac{\nu_{0,d} + 2}{2} \E[\log \sigma_{k,d}^2] - \tfrac{\nu_{0,d}}{2} \sigma_{0,d}^2 \E[\gamma_k \sigma_{k,d}^{-2}] \\
    &= \sum_{k=1}^K  \tfrac{\nu_{0,d}}{2} \log \left(\tfrac{\nu_{0,d}}{2}\right) + \tfrac{\nu_{0,d}}{2} \left[\E[\log \gamma_k] + \log \sigma_{0,d}^2 - \E[\log \sigma_{k,d}^2] - \sigma_{0,d}^2 \E[\gamma_k \sigma_{k,d}^{-2}]) \right] - \log \Gamma(\tfrac{\nu_{0,d}}{2}) + c.
\end{align*}
We can maximize this objective using a generalized Newton's method~\citep{minka2000beyond}.
We need the first and second derivatives of the objective,
\begin{align*}
    \cL'(\nu_{0,d})
    &= \sum_{k=1}^K  \tfrac{1}{2} \left[\log \left(\tfrac{\nu_{0,d}}{2}\right) + 1 \right] + \tfrac{1}{2} \left[\E[\log \gamma_k] +  \log \sigma_{0,d}^2 - \E[\log \sigma_{k,d}^2] - \sigma_{0,d}^2 \E[\gamma_k \sigma_{k,d}^{-2}]) \right] - \tfrac{1}{2} \psi(\tfrac{\nu_{0,d}}{2}) \\
    \cL''(\nu_{0,d})
    &= \sum_{k=1}^K  \tfrac{1}{2 \nu_{0,d}} - \tfrac{1}{4} \psi'(\tfrac{\nu_{0,d}}{2}).
\end{align*}
The idea is to lower bound the objective with a concave function of the form,
\begin{align*}
    g(\nu_{0,d}) = k + a \log \nu_{0,d} + b \nu_{0,d}
\end{align*}
which has derivatives $g'(\nu_{0,d}) = \tfrac{a}{\nu_{0,d}} + b$ and $g''(\nu_{0,d}) = -\tfrac{a}{\nu_{0,d}^2}$. Matching derivatives implies,
\begin{align*}
    a &= -\nu_{0,d}^2 \cL''(\nu_{0,d}) \\
    b &= \cL'(\nu_{0,d}) - \tfrac{a}{\nu_{0,d}} \\
    k &= \cL(\nu_{0,d}) - a\log \nu_{0,d} -b \nu_{0,d}.
\end{align*}
For $a > 0$ and $b<0$, the maximizer of the lower bound is obtained at
\begin{align}
    \nu_{0,d}^\star
    &= -\frac{a}{b} = \frac{\nu_{0,d}^2 \cL''(\nu_{0,d})}{\cL'(\nu_{0,d}) + \nu_{0,d} \cL''(\nu_{0,d})}
\end{align}

\subsection{M-step for \texorpdfstring{$\kappa_{0,d}$}{Îºâ‚€}}
Expanding the second objective,
\begin{align*}
    \cL(\kappa_{0,d}) &= \sum_{k=1}^K \tfrac{1}{2} \log \kappa_{0,d} - \tfrac{\kappa_{0,d}}{2} \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right] + c.
\end{align*}
The maximum is obtained at,
\begin{align*}
    \kappa_{0,d}^\star &= \left(\frac{1}{K} \sum_{k=1}^K \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right]\right)^{-1}.
\end{align*}

\subsection{M-step for \texorpdfstring{$\alpha_0$}{Î±â‚€}}
Expanding the final objective,
\begin{align*}
    \cL(\alpha_0) &= \sum_{k=1}^K \alpha_0 \log \alpha_0 - \log \Gamma(\alpha_0) + \alpha_0 \E\left[ \log \gamma_k \right]- \alpha_0 \E\left[\gamma_k\right] + c.
\end{align*}
Its derivatives are,
\begin{align*}
    \cL'(\alpha_0) &= K\log \alpha_0 + K - K\psi(\alpha_0) + \sum_{k=1}^K \E\left[\log \gamma_k\right] - \E [\gamma_k] \\
    \cL''(\alpha_0) &= \frac{K}{\alpha_0} - K\psi'(\alpha_0)
\end{align*}
We can optimize $\alpha_0$ using the generalized Newton's method described in the M-step for $\nu_0$.

\subsection{Computing the posterior expectations}
To evaluate the objectives above, we need the following expected sufficient statistics of the normal inverse chi-squared distribution,
\begin{align*}
    \E[\gamma_k] &= \sum_p w_{k,p}' \gamma^{(p)} \\
    \E[\log \gamma_k] &= \sum_p w_{k,p}' \log \gamma^{(p)} \\
    \E[\gamma_k \sigma_{k,d}^{-2}] &= \E_{\gamma_k} [ \E_{\sigma_{k,d}^2 | \gamma_k} [\gamma_k \sigma_{k,d}^{-2}]]
    = \sum_p w_{k,p}' \gamma^{(p)} \sigma_{k,p,d}'^{-2} \\
    \E[\log \sigma_{k,d}^2] &=
    \E_{\gamma_k} [\E_{\sigma_{k,d}^2 | \gamma_k} [\log \sigma_{k,d}^2]]
    = \sum_p w_{k,p}' [\log \tfrac{\nu_{k,d}' \sigma_{k,p,d}'^2}{2} - \psi(\tfrac{\nu_{k,d}'}{2})] \\
    \E \left[\frac{(\mu_{k,d} - \mu_{0,d})^2}{\sigma_{k,d}^2}\right]
    &= \E_{\gamma_k} \left[\E_{\mu_{k,d}, \sigma_{k,d}^2 | \gamma_k} \left[\frac{(\mu_{k,d} -    \mu_{0,d})^2}{\sigma_{k,d}^2}\right] \right]
    = \sum_p w_{k,p}' \left[\frac{1}{\kappa_{k,d}'} +  \frac{(\mu_{k,d}' - \mu_{0,d})^2}{\sigma_{k,p,d}'^2} \right]
\end{align*}
where $\gamma^{(p)}$ are the centers of discretized posterior on $\gamma_k$ and $w_{k,p}$ are the corresponding weights.

Since the tied covariance approach in RMDS already works quite well, we recommend initializing the EM iterations by setting $\nu_{0,d} \approx \bar{N}_k$ and $\kappa_{0,d} \approx 0$. That way, the covariances are strongly coupled across clusters and the means have an uninformative prior.


\subsection{Computing the predictive distributions}
The prior predictive is,
\begin{multline*}
    p(x^\star; \eta_0) = \\
    \int  \left[\prod_{d=1}^D \int \mathrm{N}(x^\star_d \mid \mu_d, \sigma^2_d)
        \mathrm{NIX}(\mu_d, \sigma^2_d \mid \nu_{0,d}, \kappa_{0,d}, \mu_{0,d}, \gamma \sigma^2_{0,d}, \{x_n, y_n\}_{n=1}^N)
        \dif \mu_d \dif \sigma^2_d \right] \mathrm{Ga}(\gamma \mid \alpha_0, \alpha_0) \dif \gamma.
\end{multline*}
where $\eta_0 = \alpha_0, \{\mu_{0,d}, \sigma^2_{0,d}, \kappa_{0,d}, \nu_{0,d}\}_{d=1}^D$ are the model hyperparamters.

The $\gamma$ integral can be estimated by numerical integration over a dense grid of points,
\begin{align*}
p(x^\star; \eta_0)
&\approx \sum_{p=1}^P
    w_{0,p}
    \left[ \prod_{d=1}^D
    \int \mathrm{N}(x^\star_d \mid \mu_d, \sigma^2_d)
    \mathrm{NIX}(\mu_d, \sigma^2_d \mid \nu_{0,d}, \kappa_{0,d}, \mu_{0,d}, \gamma^{(p)} \sigma^2_{0,d}, \{x_n, y_n\}_{n=1}^N)
    \dif \mu_d \dif \sigma^2_d \right] \\
&= \sum_{p=1}^P
    w_{0,p} \,
    \left[ \prod_{d=1}^D
    \mathrm{St}(x^\star_d \mid \nu_{0,d}, \mu_{0,d},
    \tfrac{\kappa_{0,d}+1}{\kappa_{0,d}} \gamma^{(p)}\sigma^2_{0,d}) \right]
\end{align*}
where,
\begin{align*}
    w_{0, p} = \frac{\mathrm{Ga}(\gamma^{(p)} \mid \alpha_0, \alpha_0) \Delta\gamma^{(p)}}{\sum_r \mathrm{Ga}(\gamma^{(r)} \mid \alpha_0, \alpha_0) \Delta\gamma^{(r)}}.
\end{align*}
We renormalize the weights to ensure that the numerical integration satisfies that $\E_\gamma[1] = 1$.

In practice, we evaluate the prior \textit{log} predictive probability using a log-sum-exp,
\begin{align*}
   \log p(x^\star; \eta_0)
   \approx \mathrm{logsumexp}_p \left[ \log w_{0,p} + \sum_{d=1}^D \log \mathrm{St}(x_d^\star \mid \nu_{0,d}, \mu_{0,d}, \tfrac{\kappa_{0,d} + 1}{\kappa_{0,d}} \gamma^{(p)} \sigma_{0,d}^2) \right]
\end{align*}

By the same logic, the posterior log predictive is,
\begin{align*}
   \log p(x^\star \mid y^\star=k, \{x_n, y_n\}_{n=1}^N; \eta_0)
   \approx \mathrm{logsumexp}_p \left[ \log w_{k,p}' + \sum_{d=1}^D \log \mathrm{St}(x_d^\star \mid \nu_{k,d}', \mu_{k,d}', \tfrac{\kappa_{k,d}'+ 1}{\kappa_{k,d}'} \gamma^{(p)} \sigma_{k,p,d}'^2) \right]
\end{align*}

\subsection{Marginal Likelihood}
This EM algorithm maximizes the marginal likelihood,
\begin{align*}
    \log p(\{x_n, y_n\}_{n=1}^N)
    &= \sum_{k=1}^K \log p(\{x_n: y_n =k\})  \\
    &= \sum_{k=1}^K \log \int p(\{x_n: y_n=k\} \mid \{\mu_{k,d}, \sigma_{k,d}^2\}) \ p(\{\mu_{k,d} \sigma_{k,d}^2\} \mid \gamma_k) \, p(\gamma_k) \dif \mu_{k,d} \dif \sigma_{k,d}^2 \dif \gamma_k \\
    &= \sum_k \log \int \left[ \prod_{d=1}^D \int \left[ \prod_{n:y_n=k} p(x_{n,d} \mid \mu_{k,d}, \sigma_{k,d}^2) p(\mu_{k,d}, \sigma_{k,d}^2 \mid \gamma_k) \dif \mu_{k,d} \dif \sigma_{k,d}^2 \right] p(\gamma_k) \dif \gamma_k \right] \\
    &= \sum_k \log \int \prod_{d=1}^D \left[\frac{Z(\nu_{k,d}', \sigma_{k,p,d}'^2, \kappa_{k,d}', \mu_{k,d}')}{Z(\nu_{0,d}, \gamma_k \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d})} \right] p(\gamma_k) \dif \gamma_k + c \\
    &\approx \sum_k \log \sum_p w_{0,p} \prod_{d=1}^D \frac{Z(\nu_{k,d}', \sigma_{k,p,d}'^2, \kappa_{k,d}', \mu_{k,d}')}{Z(\nu_{0,d}, \gamma^{(p)} \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d})}  + c \\
    &= \sum_k \mathrm{logsumexp}_p \left[ \ell_{k,p} \right] + c
\end{align*}
where
\begin{align*}
    \ell_{k,p}
    &\triangleq \log w_{0,p} + \sum_{d=1}^D \left(\log Z(\nu_{k,d}', \sigma_{k,p,d}'^2, \kappa_{k,d}', \mu_{k,d}') - \log Z(\nu_{0,d}, \gamma^{(p)} \sigma_{0,d}^2, \kappa_{0,d}, \mu_{0,d}) \right)
\end{align*}
and $c = \tfrac{ND}{2} \log 2\pi$.
\clearpage

\section{Score Correlation between RMDS and the Tied Covariance Model}
\label{app:rmds_dpmm_corr}
We observe that the DPMM model with shared covariance and the RMDS~\cite{ren21rmds} are highly correlated, as illustrated in~\Cref{fig:mds-tied-hist}.
This empirical result supports the theoretical relationship derived in~\cref{prop:rmds_tied_dpmm}.
\input{mds-vs-tied}

\clearpage
\section{Ablation experiment}
\label{app:ablation}
\input{EM_Preprocessing}
