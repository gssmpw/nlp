\section{Theory: Connecting Relative Mahalanobis Distance and DPMMs}
\label{sec:theory}
Here we show that a widely used outlier detection method called the relative Mahalanobis distance score~\citep[RMDS;][]{ren21rmds} is closely related to the outlier probabilities obtained using a Gaussian DPMM with tied covariances. RMDS outputs a score, $C(x)$, where smaller values indicate that a data point $x$ is more likely to be an outlier.
The RMDS score of a new point $x$ is defined as follows,\footnote{We flip the sign of $\mathrm{RMD}_k(x)$ compared to~\citet{ren21rmds}, but account for it in the definition of $C(x)$ so that the resulting score is unchanged. Our presentation is in keeping with the definition of the MDS score~\citep{lee18mds}.}
\begin{align}
    \nonumber
    \mathrm{MD}_0(x) &= (x - \hat{\mu}_0)^\top \hat{\Sigma}_0^{-1} (x - \hat{\mu}_0) \\
    \nonumber
    \mathrm{MD}_k(x) &= (x - \hat{\mu}_k)^\top \hat{\Sigma}^{-1} (x - \hat{\mu}_k) \\
    \nonumber
    \mathrm{RMD}_k(x) &= \mathrm{MD}_0(x) - \mathrm{MD}_k(x)\\
    C(x) &= \max_k \; \mathrm{RMD}_k(x),
\end{align}
where $\mathrm{MD}_0(x)$ and $\mathrm{MD}_k(x)$ are squared Mahalanobis distances, $\hat{\mu}_0$ and $\hat{\Sigma}_0$ are the sample mean and covariance of the data, $\hat{\mu}_k$ is the sample mean of cluster $k$, and $\hat{\Sigma} = \tfrac{1}{N} \sum_{n=1}^N (x_n - \hat{\mu}_{y_n})(x_n - \hat{\mu}_{y_n})^\top$ is the sample within-class covariance.

\citet{ren21rmds} motivated this score in terms of log density ratios between a Gaussian distribution for each cluster and a Gaussian ``background'' model.
Specifically,
\begin{align}
    \label{eq:rmdk}
    \mathrm{RMD}_k(x) &= 2 \log \frac{\cN(x \mid \hat{\mu}_k, \hat{\Sigma})}{\cN(x \mid \hat{\mu}_0, \hat{\Sigma}_0)} + d,
\end{align}
where $d=\log |\hat{\Sigma}| - \log|\hat{\Sigma}_0|$ does not depend on $x$ or $k$.
Larger values of $\mathrm{RMD}_k(x)$ indicate that $x$ is more likely under cluster $k$ than under the background model.

The procedure for mapping $\mathrm{RMD}_k(x)$ values to the score~$C(x)$ is inherited from the Mahalanobis distance score~\citep[MDS;][]{lee18mds}. If the log density ratio is negative for all $k$, then the background model is more likely than \textit{all} of the existing clusters, and hence $x$ is likely to be an outlier.
Propositions~\ref{prop:rmds_dpmm} and~\ref{prop:rmds_tied_dpmm} show that a very similar computation is at work in the calculation of outlier probabilities for DPMMs.

First, we show that the \emph{inlier} probabilities under a general DPMM (not necessarily Gaussian) can be expressed in terms of a quantity analogous to $C(x)$.
\begin{proposition}
\label{prop:rmds_dpmm}
The \emph{inlier} probability of a general DPMM with concentration $\alpha$ can be expressed as follows,
\begin{align}
    p(y \in [K] \mid x, \cD)
    &= \sigma(\widetilde{C}(x) - \log \nicefrac{\alpha}{\overline{N}})
\end{align}
where $\sigma(u) = (1 + e^{-u})^{-1}$ is the logistic (sigmoid) function, ${\overline{N}=\tfrac{1}{K} \sum_k N_k}$ is the average cluster size,
and
\begin{align}
    \nonumber \widetilde{C}(x) &= \log \sum_{k=1}^K e^{\lambda_k + \log \nicefrac{N_k}{\overline{N}}} \\
    \label{eq:Ctilde}
    &= \mathrm{softmax}_k \left\{ \lambda_k + \log \nicefrac{N_k}{\overline{N}} \right\}, \\
    \lambda_k &= \log \frac{p(x \mid \cD_k)}{p(x)}.
\end{align}
Here, $\lambda_k$ is the log density ratio of the posterior predictive and prior predictive distributions from eq.~\eqref{eq:outlier_prob}.
\end{proposition}

\begin{proof}
The inlier probability is one minus the outlier probability.
Normalizing the outlier probability in eq.~\eqref{eq:outlier_prob} and rearranging, we can write the inlier probability as,
\begin{align}
    \nonumber p(y \in [K] \mid x, \cD) &=
    1 - \frac{\alpha p(x)}{\alpha p(x) + \sum_{k=1}^K N_k p(x \mid \cD_k)} \\
    \nonumber &=
    1 - \left( 1 +  \sum_{k=1}^K \frac{\nicefrac{N_k}{\overline{N}} \, p(x \mid \cD_k)}{\nicefrac{\alpha}{\overline{N}} \, p(x)} \right)^{-1} \\
    \nonumber &= 1 - \left(1 + e^{\widetilde{C}(x) - \log \nicefrac{\alpha}{\overline{N}}}\right)^{-1} \\
    &= \sigma(\widetilde{C}(x) - \log \nicefrac{\alpha}{\overline{N}})
\end{align}
where $\widetilde{C}(x)$ is defined in eq.~\eqref{eq:Ctilde} and the last line follows from the fact that $1 - \sigma(-u) = \sigma(u)$.
\end{proof}

This proposition says that the log-odds of data point $x$ belonging to an existing cluster is the difference of a \textit{DPMM score},~$\widetilde{C}(x)$, which is analogous to the relative Mahalanobis score, and a \textit{threshold},~$\log \nicefrac{\alpha}{\overline{N}}$, which is tuned by the hyperparameter $\alpha$.

Next, we show that in certain regimes, the DPMM score from a Gaussian DPMM with tied covariance is almost perfectly correlated with the RMDS.

\begin{proposition}
\label{prop:rmds_tied_dpmm}
If the average cluster covariance $\hat{\Sigma}$ is much smaller than the population covariance $\hat{\Sigma}_0$ and all clusters are of size $N_k = \overline{N} \gg 1$, then a Gaussan DPMM with tied covariance (c.f. Sec.~\ref{sec:tied-cov}) and hyperparameters $\eta = (\hat{\mu}_0, \hat{\Sigma}_0, \hat{\Sigma})$ will have,
\begin{align}
    \widetilde{C}(x) \approx \tfrac{1}{2} C(x) - d,
\end{align}
where $C(x)$ is the RMDS and $d$ is an additive constant, as defined in eq.~\eqref{eq:rmdk}.
\end{proposition}

\begin{proof}
In the regime where~${\hat{\Sigma} \ll \hat{\Sigma}_0}$, the posterior mean is approximately $\mu_k' \approx \hat{\mu}_k$, the
posterior variance is approximately $\Sigma_k' \approx N_k^{-1} \hat{\Sigma}$, and the prior predictive variance is approximately $\hat{\Sigma}_0 + \hat{\Sigma} \approx \hat{\Sigma}_0$. Furthermore, since $N_k = \overline{N} \gg 1$, it follows that $\Sigma_k' + \hat{\Sigma} \approx \tfrac{\overline{N} + 1}{\overline{N}} \hat{\Sigma} \approx \hat{\Sigma}$. Thus,
\begin{align}
    \nonumber
    \lambda_k
    &= \log \frac{\cN(x \mid \mu_k', \Sigma_k' + \hat{\Sigma})}{\cN(x \mid \hat{\mu}_0, \hat{\Sigma}_0 + \hat{\Sigma})} \\
    \nonumber
    &\approx \log \frac{\cN(x \mid \hat{\mu}_k, \hat{\Sigma})}{\cN(x \mid \hat{\mu}_0, \hat{\Sigma}_0)} \\
    &= \tfrac{1}{2}\mathrm{RMD}_k(x) - d,
\end{align}
Finally, since $N_k = \overline{N}$, and since the softmax is approximately equal to the maximum,
\begin{align}
    \nonumber
    \widetilde{C}(x)
    &= \mathrm{softmax}_k \{\lambda_k\} \\
    \nonumber
    & \approx \mathrm{softmax}_k \left\{\tfrac{1}{2} \mathrm{RMD}_k(x)  - d \right\} \\
    \nonumber
    & \approx \max_k \left\{\tfrac{1}{2} \mathrm{RMD}_k(x) -d \right\} \\
    &= \tfrac{1}{2} C(x) - d,
\end{align}
which completes the proof.
\end{proof}

This proposition establishes the close correspondence between the relative Mahalanobis distance score and the log-odds that a point is an inlier under a Gaussian DPMM with tied covariance.
We show that this correspondence holds in practice in the experiments below and in \Cref{app:rmds_dpmm_corr}.
This correspondence provides further support for using RMDS for outlier detection, beyond the original motivation in terms of log likelihood ratios.
However, from this perspective, we also recognize several natural generalizations of RMDS that could improve outlier detection through richer DPMMs.
We present three such generalizations below.
