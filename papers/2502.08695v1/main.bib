@String(PAMI  = {IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI)})
@String(IJCV  = {International Journal of Computer Vision (IJCV)})
@String(CVPR  = {Proceedings of the {IEEE/CVF} Conference on Computer Vision and Pattern Recognition (CVPR)})
@String(ICCV  = {Proceedings of the {I}nternational {C}onference on {C}omputer {V}ision (ICCV)})
@String(NeurIPS = {Advances in Neural Information Processing Systems (NeurIPS)})
@String(ICML  = {International Conference on Machine Learning (ICML)})
@String(ICLR  = {International Conference on Learning Representations (ICLR)})
@String(ICMLW = {International Conference on Machine Learning Workshops})
@String(TMLR  = {Transactions on Machine Learning Research (TMLR)})
@String(PR    = {Pattern Recognition})
@String(WACV  = {{IEEE/CVF} Winter Conference on Applications of Computer Vision, (WACV)})
@String(CSUR  = {ACM computing surveys (CSUR)})

@inproceedings{HendrycksD17,
  author = {Dan Hendrycks and Kevin Gimpel},
  title = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks},
  booktitle = "The Fifth " # ICLR,
  year = 2017,
}

@article{chandola2009anomaly,
  title={Anomaly detection: {A} survey},
  author={Chandola, Varun and Banerjee, Arindam and Kumar, Vipin},
  journal=CSUR,
  volume={41},
  number={3},
  pages={1--58},
  year={2009},
  publisher={ACM New York, NY, USA},
}

@article{scheirer12openset,
  title={Toward open set recognition},
  author={Scheirer, Walter J and de Rezende Rocha, Anderson and Sapkota, Archana and Boult, Terrance E},
  journal=PAMI,
  volume={35},
  number={7},
  pages={1757--1772},
  year={2012},
  publisher={IEEE},
}

@article{anscombe1960rejection,
  title={Rejection of outliers},
  author={Anscombe, Frank J},
  journal={Technometrics},
  volume={2},
  number={2},
  pages={123--146},
  year={1960},
  publisher={Taylor \& Francis}
}

@article{ferguson1973bayesian,
  title={A {B}ayesian analysis of some nonparametric problems},
  author={Ferguson, Thomas S},
  journal={The Annals of Statistics},
  pages={209--230},
  year={1973},
  volume = {1},
  number = {2},
  publisher = {Institute of Mathematical Statistics},
}

@article{antoniak74dpmm,
  title={Mixtures of {D}irichlet processes with applications to {B}ayesian nonparametric problems},
  author={Antoniak, Charles E},
  journal={The Annals of Statistics},
  pages={1152--1174},
  year={1974},
  publisher = {Institute of Mathematical Statistics},
  number = {6},
  volume = {2},
}

@article{lo1984class,
  title={On a class of {B}ayesian nonparametric estimates: {I}. {D}ensity estimates},
  author={Lo, Albert Y},
  journal={The Annals of Statistics},
  pages={351--357},
  year={1984},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  volume = {12},
}

@article{Sethuraman94,
 abstract = {In this paper we give a simple and new constructive definition of Dirichlet measures removing the restriction that the basic space should be Rk. We also give complete, self contained proofs of the three basic results for Dirichlet measures: 1. The Dirichlet measure is a probability measure on the space of all probability measures. 2. It gives probability one to the subset of discrete probability measures. 3. The posterior distribution is also a Dirichlet measure.},
 author = {Jayaram Sethuraman},
 journal = {Statistica Sinica},
 number = {2},
 pages = {639--650},
 publisher = {Institute of Statistical Science, Academia Sinica},
 title = {A CONSTRUCTIVE DEFINITION OF {D}IRICHLET PRIORS},
 volume = {4},
 year = {1994}
}

@article{maceachern1994estimating,
  title={Estimating normal means with a conjugate style {D}irichlet process prior},
  author={MacEachern, Steven N},
  journal={Communications in Statistics-Simulation and Computation},
  volume={23},
  number={3},
  pages={727--741},
  year={1994},
  publisher={Taylor \& Francis}
}

@article{neal00,
 abstract = {This article reviews Markov chain methods for sampling from the posterior distribution of a Dirichlet process mixture model and presents two new classes of methods. One new approach is to make Metropolis-Hastings updates of the indicators specifying which mixture component is associated with each observation, perhaps supplemented with a partial form of Gibbs sampling. The other new approach extends Gibbs sampling for these indicators by using a set of auxiliary parameters. These methods are simple to implement and are more efficient than previous ways of handling general Dirichlet process mixture models with non-conjugate priors.},
 author = {Radford M Neal},
 journal = {Journal of Computational and Graphical Statistics},
 number = {2},
 pages = {249--265},
 publisher = {[American Statistical Association, Taylor & Francis, Ltd., Institute of Mathematical Statistics, Interface Foundation of America]},
 title = {Markov Chain Sampling Methods for {D}irichlet Process Mixture Models},
 volume = {9},
 year = {2000}
}

@inproceedings{dosovitskiy20vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  booktitle= "The Ninth " # ICLR,
  year={2020}
}

@inproceedings{caron21dino,
  title={Emerging Properties in Self-Supervised Vision Transformers},
  author={Caron, Mathilde and Touvron, Hugo and Misra, Ishan and J\'egou, Herv\'e  and Mairal, Julien and Bojanowski, Piotr and Joulin, Armand},
  booktitle=ICCV,
  year={2021}
}

@article{oquab23dinov2,
title={{DINO}v2: Learning Robust Visual Features without Supervision},
author={Maxime Oquab and Timoth{\'e}e Darcet and Th{\'e}o Moutakanni and Huy V. Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel HAZIZA and Francisco Massa and Alaaeldin El-Nouby and Mido Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Herve Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
journal=TMLR,
year={2024},
note={Featured Certification}
}

@inproceedings{
darcet23needreg,
title={Vision Transformers Need Registers},
author={Timoth{\'e}e Darcet and Maxime Oquab and Julien Mairal and Piotr Bojanowski},
booktitle= "The Twelfth " # ICLR,
year={2024},
}

@inproceedings{chen20simclr,
  title={A simple framework for contrastive learning of visual representations},
  author={Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  booktitle="Proceedings of the 37th " # ICML,
  pages={1597--1607},
  year={2020},
  publisher={PMLR},
  volume = 	 {119},
}

@inproceedings{chen20simclrv2,
 author = {Chen, Ting and Kornblith, Simon and Swersky, Kevin and Norouzi, Mohammad and Hinton, Geoffrey E},
 booktitle = NeurIPS,
 pages = {22243--22255},
 publisher = {Curran Associates, Inc.},
 title = {Big Self-Supervised Models are Strong Semi-Supervised Learners},
 volume = {33},
 year = {2020}
}

@inproceedings{ren21rmds,
      title={A Simple Fix to {M}ahalanobis Distance for Improving Near-{OOD} Detection},
      author={Jie Ren and Stanislav Fort and Jeremiah Liu and Abhijit Guha Roy and Shreyas Padhy and Balaji Lakshminarayanan},
      year={2021},
      booktitle=ICMLW,
      series={Uncertainty \& Robustness in Deep Learning}
}

@inproceedings{TackJ20,
 author = {Tack, Jihoon and Mo, Sangwoo and Jeong, Jongheon and Shin, Jinwoo},
 booktitle = NeurIPS,
 pages = {11839--11852},
 publisher = {Curran Associates, Inc.},
 title = {{CSI}: {N}ovelty Detection via Contrastive Learning on Distributionally Shifted Instances},
 volume = {33},
 year = {2020}
}

@inproceedings{HuangR21,
  author = {Rui Huang and Yixuan Li},
  title = {{MOS:} {T}owards scaling out-of-distribution detection for large semantic space},
  booktitle = CVPR,
  pages = {8710--8719},
  year = 2021
}


@inproceedings{wei2022mitigating,
  title={Mitigating neural network overconfidence with logit normalization},
  author={Wei, Hongxin and Xie, Renchunzi and Cheng, Hao and Feng, Lei and An, Bo and Li, Yixuan},
  booktitle="Proceedings of the 39th " # ICML,
  pages={23631--23644},
  year={2022},
  organization={PMLR}
}

@inproceedings{linderman23,
  title = 	 {Fine-grain Inference on Out-of-Distribution Data with Hierarchical Classification},
  author =       {Linderman, Randolph and Zhang, Jingyang and Inkawhich, Nathan and Li, Hai and Chen, Yiran},
  booktitle = 	 {Proceedings of The 2nd Conference on Lifelong Learning Agents (CoLLAs)},
  pages = 	 {162--183},
  year = 	 {2023},
  volume = 	 {232},
  publisher =    {PMLR},
  abstract = 	 {Machine learning methods must be trusted to make appropriate decisions in real-world environments, even when faced with out-of-distribution (OOD) samples. Many current approaches simply aim to detect OOD examples and alert the user when an unrecognized input is given. However, when the OOD sample significantly overlaps with the training data, a binary anomaly detection is not interpretable or explainable, and provides little information to the user. We propose a new model for OOD detection that makes predictions at varying levels of granularity—as the inputs become more ambiguous, the model predictions become coarser and more conservative. Consider an animal classifier that encounters an unknown bird species and a car. Both cases are OOD, but the user gains more information if the classifier recognizes that its uncertainty over the particular species is too large and predicts “bird” instead of detecting it as OOD. Furthermore, we diagnose the classifier’s performance at each level of the hierarchy improving the explainability and interpretability of the model’s predictions. We demonstrate the effectiveness of hierarchical classifiers for both fine- and coarse-grained OOD tasks.}
}

@inproceedings{HendrycksD19,
  author    = {Dan Hendrycks and
               Mantas Mazeika and
               Thomas G. Dietterich},
  title     = {Deep Anomaly Detection with Outlier Exposure},
  booktitle = "The Seventh " # ICLR,
  year      = 2019,
}

@inproceedings{zhang2021mixture,
  author       = {Jingyang Zhang and
                  Nathan Inkawhich and
                  Randolph Linderman and
                  Yiran Chen and
                  Hai Li},
  title        = {Mixture Outlier Exposure: Towards Out-of-Distribution Detection in
                  Fine-grained Environments},
  booktitle    = WACV,
  pages        = {5520--5529},
  publisher    = {{IEEE}},
  year         = {2023},
}

@inproceedings{guo17tempscale,
  title = 	 {On Calibration of Modern Neural Networks},
  author =       {Chuan Guo and Geoff Pleiss and Yu Sun and Kilian Q. Weinberger},
  booktitle =  "Proceedings of the 34th " # ICML,
  pages = 	 {1321--1330},
  year = 	 {2017},
  volume = 	 {70},
  publisher =    {PMLR},
}


@inproceedings{LiangS18,
  author = {Shiyu Liang and Yixuan Li and R. Srikant},
  title={Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks},
  booktitle = "The Sixth " # ICLR,
  year = 2018
}

@inproceedings{LiuW20,
  author    = {Weitang Liu and
               Xiaoyun Wang and
               John D. Owens and
               Yixuan Li},
  title     = {Energy-based Out-of-distribution Detection},
  booktitle = NeurIPS,
  year      = 2020,
  volume    = {33},
  pages = {21464--21475},
  publisher = {Curran Associates, Inc.},
}

@inproceedings{lee18mds,
	author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
	booktitle = NeurIPS,
	publisher = {Curran Associates, Inc.},
	title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
	volume = {31},
	year = {2018},
    pages = {},
}

@article{
zhang23openood15,
title={OpenOOD v1.5: Enhanced Benchmark for Out-of-Distribution Detection},
author={Jingyang Zhang and Jingkang Yang and Pengyun Wang and Haoqi Wang and Yueqian Lin and Haoran Zhang and Yiyou Sun and Xuefeng Du and Yixuan Li and Ziwei Liu and Yiran Chen and Hai Li},
journal={Journal of Data-centric Machine Learning Research (DMLR)},
year={2024},
volume={2},
note={Dataset Certification},
}

@inproceedings{ren2019likelihood,
 author = {Ren, Jie and Liu, Peter J. and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
 booktitle = NeurIPS,
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Likelihood Ratios for Out-of-Distribution Detection},
 volume = {32},
 year = {2019}
}


@inproceedings{sun2022out,
  title = 	 {Out-of-Distribution Detection with Deep Nearest Neighbors},
  author =       {Sun, Yiyou and Ming, Yifei and Zhu, Xiaojin and Li, Yixuan},
  booktitle = "Proceedings of the 39th " # ICML,
  pages = 	 {20827--20840},
  year = 	 {2022},
  volume = 	 {162},
  publisher =    {PMLR},
  abstract = 	 {Out-of-distribution (OOD) detection is a critical task for deploying machine learning models in the open world. Distance-based methods have demonstrated promise, where testing samples are detected as OOD if they are relatively far away from in-distribution (ID) data. However, prior methods impose a strong distributional assumption of the underlying feature space, which may not always hold. In this paper, we explore the efficacy of non-parametric nearest-neighbor distance for OOD detection, which has been largely overlooked in the literature. Unlike prior works, our method does not impose any distributional assumption, hence providing stronger flexibility and generality. We demonstrate the effectiveness of nearest-neighbor-based OOD detection on several benchmarks and establish superior performance. Under the same model trained on ImageNet-1k, our method substantially reduces the false positive rate (FPR@TPR95) by 24.77% compared to a strong baseline SSD+, which uses a parametric approach Mahalanobis distance in detection. Code is available: https://github.com/deeplearning-wisc/knn-ood.}
}

@article{shotwell2011,
	author = {Matthew S. Shotwell and Elizabeth H. Slate},
	journal = {Bayesian Analysis},
	number = {4},
	pages = {665 -- 690},
	publisher = {International Society for Bayesian Analysis},
	title = {Bayesian Outlier Detection with {D}irichlet Process Mixtures},
	volume = {6},
	year = {2011},
}


@inproceedings{varadarajan17,
  author={Varadarajan, Jagannadan and Subramanian, Ramanathan and Ahuja, Narendra and Moulin, Pierre and Odobez, Jean-Marc},
  booktitle=WACV,
  title={Active Online Anomaly Detection Using {D}irichlet Process Mixture Model and {G}aussian Process Classification},
  year={2017},
  pages={615-623},
}

@inproceedings{malinin2018predictive,
  title={Predictive uncertainty estimation via prior networks},
  author={Malinin, Andrey and Gales, Mark},
  booktitle=NeurIPS,
  volume={31},
  year={2018},
  publisher = {Curran Associates, Inc.},
}

@inproceedings{malinin2019reverse,
  title={Reverse {KL}-divergence training of prior networks: {I}mproved uncertainty and adversarial robustness},
  author={Malinin, Andrey and Gales, Mark},
  booktitle=NeurIPS,
  volume={32},
  year={2019},
  publisher = {Curran Associates, Inc.},
}

@article{Kim2024UnsupervisedOD,
title = {Unsupervised outlier detection using random subspace and subsampling ensembles of Dirichlet process mixtures},
journal = PR,
volume = {156},
pages = {110846},
year = {2024},
author = {Dongwook Kim and Juyeon Park and Hee Cheol Chung and Seonghyun Jeong},
abstract = {Probabilistic mixture models are recognized as effective tools for unsupervised outlier detection owing to their interpretability and global characteristics. Among these, Dirichlet process mixture models stand out as a strong alternative to conventional finite mixture models for both clustering and outlier detection tasks. Unlike finite mixture models, Dirichlet process mixtures are infinite mixture models that automatically determine the number of mixture components based on the data. Despite their advantages, the adoption of Dirichlet process mixture models for unsupervised outlier detection has been limited by challenges related to computational inefficiency and sensitivity to outliers in the construction of outlier detectors. Additionally, Dirichlet process Gaussian mixtures struggle to effectively model non-Gaussian data with discrete or binary features. To address these challenges, we propose a novel outlier detection method that utilizes ensembles of Dirichlet process Gaussian mixtures. This unsupervised algorithm employs random subspace and subsampling ensembles to ensure efficient computation and improve the robustness of the outlier detector. The ensemble approach further improves the suitability of the proposed method for detecting outliers in non-Gaussian data. Furthermore, our method uses variational inference for Dirichlet process mixtures, which ensures both efficient and rapid computation. Empirical analyses using benchmark datasets demonstrate that our method outperforms existing approaches in unsupervised outlier detection.}
}

@incollection{forstner_metric_2003,
author="F{\"o}rstner, Wolfgang
and Moonen, Boudewijn",
title="A Metric for Covariance Matrices",
bookTitle="Geodesy-The Challenge of the 3rd Millennium",
year="2003",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="299--309",
abstract="The paper presents a metric for positive definite covariance matrices. It is a natural expression involving traces and joint eigenvalues of the matrices. It is shown to be the distance coming from a canonical invariant Riemannian metric on the space Sym+ (n, ℝ) of real symmetric positive definite matrices",
}

@book{gelman1995bayesian,
  title={Bayesian data analysis},
  author={Gelman, Andrew and Carlin, John B and Stern, Hal S and Rubin, Donald B},
  year={1995},
  publisher={Chapman and Hall/CRC}
}

@misc{minka2000beyond,
	abstract = {Newton's method for optimization is equivalent to iteratively maximizing a local quadratic approximation to the objective function. But some functions are not well-approximated by a quadratic, leading to slow convergence, and some have turning points where the curvature changes sign, leading to failure. The fix this, we can use a more appropriate choice of local approximation than quadratic, based on the type of function we are optimizing. This paper demonstrates three such generalized Newton rules. Like Newton's method, they only involve the first two derivatives of the function, yet converge faster and fail less often.},
	author = {Minka, Tom},
	month = {April},
	title = {Beyond {N}ewton's Method},
	year = {2000},
	bdsk-url-1 = {https://www.microsoft.com/en-us/research/publication/beyond-newtons-method/}}

@inproceedings{yang2022openood,
    title={Open{OOD}: Benchmarking Generalized Out-of-Distribution Detection},
    author={Jingkang Yang and Pengyun Wang and Dejian Zou and Zitang Zhou and Kunyuan Ding and WenXuan Peng and Haoqi Wang and Guangyao Chen and Bo Li and Yiyou Sun and Xuefeng Du and Kaiyang Zhou and Wayne Zhang and Dan Hendrycks and Yixuan Li and Ziwei Liu},
    booktitle=NeurIPS # " Datasets and Benchmarks Track",
    year={2022},
    publisher = {Curran Associates, Inc.},
}

@inproceedings{touvron2021training,
  title={Training data-efficient image transformers \& distillation through attention},
  author={Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and J{\'e}gou, Herv{\'e}},
  booktitle="Proceedings of the 38th " # ICML,
  pages={10347--10357},
  year={2021},
  organization={PMLR}
}

@misc{torchvision2016,
    title        = {TorchVision: PyTorch's Computer Vision library},
    author       = {{TorchVision maintainers and contributors}},
    year         = 2016,
    journal      = {GitHub repository},
    publisher    = {GitHub},
}


@inproceedings{snell2017prototypical,
  title={Prototypical networks for few-shot learning},
  author={Snell, Jake and Swersky, Kevin and Zemel, Richard},
  booktitle=NeurIPS,
  volume={30},
  year={2017},
  publisher = {Curran Associates, Inc.},
}

@inproceedings{vaze22,
  title={Generalized Category Discovery},
  author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
  booktitle=CVPR,
  year={2022},
  pages={7492--7501},
}

@article{van2022three,
  title={Three types of incremental learning},
  author={Van de Ven, Gido M and Tuytelaars, Tinne and Tolias, Andreas S},
  journal={Nature Machine Intelligence},
  volume={4},
  number={12},
  pages={1185--1197},
  year={2022},
  publisher={Nature Publishing Group UK London}
}

@article{RussakovskyO15,
  author = {Olga Russakovsky and
            Jia Deng and
            Hao Su and
            Jonathan Krause and
            Sanjeev Satheesh and
            Sean Ma and
            Zhiheng Huang and
            Andrej Karpathy and
            Aditya Khosla and
            Michael Bernstein and
            Alexander C. Berg and
            Li Fei-Fei},
  title = {ImageNet Large Scale Visual Recognition Challenge},
  journal = IJCV,
  volume=115,
  number=3,
  pages={211--252},
  year = 2015
}

@inproceedings{PYTORCH,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = NeurIPS,
volume={32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@inproceedings{bitterwolf2023ninco,
    title = 	 {In or Out? {F}ixing {I}mage{N}et Out-of-Distribution Detection Evaluation},
    author =       {Bitterwolf, Julian and M\"{u}ller, Maximilian and Hein, Matthias},
    booktitle= "Proceedings of the 40th " # ICML,
    year={2023},
    pages = {2471--2506},
    url={},
    volume = 	 {202},
    publisher =    {PMLR},
}

@inproceedings{vaze22ssb,
    title={Open-Set Recognition: A Good Closed-Set Classifier is All You Need},
    author={Sagar Vaze and Kai Han and Andrea Vedaldi and Andrew Zisserman},
    booktitle= "The Tenth " # ICLR,
    year={2022},
}

@book{textures,
    author={Kylberg, Gustaf},
    title={{The Kylberg Texture Dataset v. 1.0}},
    publisher={Centre for Image Analysis, Swedish University of Agricultural Sciences and Uppsala University, External report (Blue series) No. 35.},
    year={2011},
}

@inproceedings{inaturalist,
author = { Van Horn, Grant and Mac Aodha, Oisin and Song, Yang and Cui, Yin and Sun, Chen and Shepard, Alex and Adam, Hartwig and Perona, Pietro and Belongie, Serge },
booktitle = CVPR,
title = {The {iNaturalist} Species Classification and Detection Dataset},
year = {2018},
pages = {8769-8778},
}

@inproceedings{openimageo,
author = { Wang, Haoqi and Li, Zhizhong and Feng, Litong and Zhang, Wayne },
booktitle = CVPR,
title = {{ViM}: {O}ut-Of-Distribution with Virtual-logit Matching },
year = {2022},
pages = {4911-4920},
}
