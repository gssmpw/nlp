\section{Related Work}
\subsection{Cognitive Architecture - Foundational Model Integration}
Existing works typically explore how CAs can be integrated with large language models (LLMs). For example, Wu et al. investigate whether the Llama-2 13B model encodes features that can predict expert decisions in a decision making task by training a linear classifier on the Llama model's last contextual embeddings to predict ACT-R's expert decision when the model is given ACT-R's strings of decision making traces as input; they further examine whether ACT-R's knowledge can be injected into the Llama model by fine-tuning a Llama-classifier system on ACT-R's expert decisions \cite{wu_cognitive_2024}. Bajaj et al. enhance ACT-R's analogical reasoning capabilities by building a natural language processing pipeline to automatically extract key entities, relationships, and attributes \cite{bajaj_generating_2023}. Once these key elements have been extracted from unstructured text, an LLM is prompted to convert the unstructured text into a structured format based on its key elements. ACT-R can utilize the structured knowledge for reasoning tasks downstream, significantly reducing the need for manual knowledge engineering. While Bajaj et al. propose to use LLMs to transform unstructured text into structured knowledge, Kirk et al. explore ways in which LLMs can be leveraged as a knowledge source for the task knowledge needed for successful task planning downstream \cite{kirk_exploiting_2023}. They propose three approaches to knowledge extraction: indirect extraction in which the LLM's reponses are placed in a knowledge store that the cognitive agent accesses, direct extraction in which the agent directly queries the LLM and parses its output for structured knowledge, and direct knowledge encoding in which the LLM creates programs that are run as part of the cognitive agent's task pipeline. 

To the best of our knowledge, no existing work has explored the integration of a CA with a VLA model. Realizing such an integration requires a method to ``probe'' the VLA’s hidden representations for symbolic content. We next discuss relevant literature on foundational model probing, which informs our approach in extracting object and action states from OpenVLA.

\subsection{Foundational Model Probing}
Foundational models encode extensive knowledge derived from their internet-scale training data \cite{bommasani_opportunities_2022}. The increasing popularity of these models has drawn significant attention to the challenges of extracting and evaluating the knowledge they encode. One commonly used approach to evaluate LLMs is prompt-based probing in which an LLM is prompted to fill in the blanks in the prompt \cite{petroni_language_2019}. For example, Alivanistos et al. combine various prompting techniques to probe LLMs for possible objects of a triple where the subject and the relation are given \cite{alivanistos_prompting_2023}, Wang et al. develop a method to automatically search sentences to construct optimal and readable prompts for probing certain knowledge \cite{wang_readprompt_2023}, and Qi et al. propose a probing framework for multimodal LLMs that includes visual prompting, textual prompting, and extra knowledge prompting \cite{qi_what_2023}. While prompt-based probing is intuitive and easy to execute, it lacks the layer-specific precision offered by linear probing. Furthermore, prompt-based probing is not applicable to vision-language-action models as they do not output language tokens. Linear probing on the other hand involves training a linear classifier on top of each frozen layer of a foundational model. Each classifier is tasked with predicting specific knowledge based on the output features of the corresponding frozen layer. For example, Li et al. train semantic probes to predict object properties and relations as they evolve throughout a discourse \cite{li_implicit_2021}. Similarly, Chen et al. use linear probes to evaluate the Llama model family’s performance on higher-order tasks such as reasoning and calculation, comparing probe performance across layers and model sizes \cite{chen_is_2024}. In our work, we extract symbolic representations of state changes similar to the approach described in Li et al \cite{li_implicit_2021} and we evaluate probing accuracies across hidden layers similar to the approach used by Chen et al \cite{chen_is_2024}.