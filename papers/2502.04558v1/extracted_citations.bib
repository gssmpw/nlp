@misc{alivanistos_prompting_2023,
	title = {Prompting as Probing: Using Language Models for Knowledge Base Construction},
	url = {http://arxiv.org/abs/2208.11057},
	doi = {10.48550/arXiv.2208.11057},
	shorttitle = {Prompting as Probing},
	abstract = {Language Models ({LMs}) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. {LMs} are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present {ProP} (Prompting as Probing), which utilizes {GPT}-3, a large Language Model originally proposed by {OpenAI} in 2020, to perform the task of Knowledge Base Construction ({KBC}). {ProP} implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the {LM} must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the {LM}, that the size of the {LM} is a crucial factor, and that a dictionary of entity aliases improves the {LM} score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: {ProP} won track 2 of the {LM}-{KBC} competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/{HEmile}/iswc-challenge.},
	number = {{arXiv}:2208.11057},
	publisher = {{arXiv}},
	author = {Alivanistos, Dimitrios and Santamaría, Selene Báez and Cochez, Michael and Kalo, Jan-Christoph and Krieken, Emile van and Thanapalasingam, Thiviyan},
	urldate = {2024-12-15},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2208.11057 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/6Y6D4THG/Alivanistos et al. - 2023 - Prompting as Probing Using Language Models for Kn.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/3M9GMBEI/2208.html:text/html},
}

@article{bajaj_generating_2023,
	title = {Generating Chunks for Cognitive Architectures},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27683},
	doi = {10.1609/aaaiss.v2i1.27683},
	abstract = {Knowledge engineering is an important task for creating and maintaining a knowledge base for cognitive models. It involves acquiring, representing, and organizing knowledge in a form that computers can use to make decisions and solve problems. However, this process can be a bottleneck for designing and using cognitive models. Knowledge engineering is a time-consuming and resource-intensive task that requires subject matter experts to provide information about a domain. In addition, models can acquire knowledge but require significant mechanisms to structure that information in a structured format appropriate for general use. Given the knowledge engineering bottleneck, we propose a solution that relies on natural language processing to extract key entities, relationships, and attributes to automatically generate chunks encoded as triples or chunks from unstructured text. Once generated, the knowledge can be used to create or add to a knowledge base within cognitive architectures to reduce knowledge engineering and task-specific models.},
	pages = {246--252},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Bajaj, Goonmeet and Pearce, Kate and Kennedy, Sean and Larue, Othalia and Hough, Alexander and King, Jayde and Myers, Christopher and Parthasarathy, Srinivasan},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Knowledge Engineering},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/AWC8D5AB/Bajaj et al. - 2023 - Generating Chunks for Cognitive Architectures.pdf:application/pdf},
}

@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2024-12-15},
	date = {2022-07-12},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/YUUT4PRM/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/4AQ7XWQT/2108.html:text/html},
}

@misc{chen_is_2024,
	title = {Is Bigger and Deeper Always Better? Probing {LLaMA} Across Scales and Layers},
	url = {http://arxiv.org/abs/2312.04333},
	doi = {10.48550/arXiv.2312.04333},
	shorttitle = {Is Bigger and Deeper Always Better?},
	abstract = {This paper presents an in-depth analysis of Large Language Models ({LLMs}), focusing on {LLaMA}, a prominent open-source foundational model in natural language processing. Instead of assessing {LLaMA} through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of {LLaMA} lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.},
	number = {{arXiv}:2312.04333},
	publisher = {{arXiv}},
	author = {Chen, Nuo and Wu, Ning and Liang, Shining and Gong, Ming and Shou, Linjun and Zhang, Dongmei and Li, Jia},
	urldate = {2024-12-05},
	date = {2024-01-09},
	eprinttype = {arxiv},
	eprint = {2312.04333},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/4HWRGELD/Chen et al. - 2024 - Is Bigger and Deeper Always Better Probing LLaMA .pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/RMAELCDM/2312.html:text/html},
}

@misc{kirk_exploiting_2023,
	title = {Exploiting Language Models as a Source of Knowledge for Cognitive Agents},
	url = {http://arxiv.org/abs/2310.06846},
	doi = {10.48550/arXiv.2310.06846},
	abstract = {Large language models ({LLMs}) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.},
	number = {{arXiv}:2310.06846},
	publisher = {{arXiv}},
	author = {Kirk, James R. and Wray, Robert E. and Laird, John E.},
	urldate = {2024-09-04},
	date = {2023-09-05},
	eprinttype = {arxiv},
	eprint = {2310.06846 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, I.2.11},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/Y3HUCXRL/Kirk et al. - 2023 - Exploiting Language Models as a Source of Knowledg.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/5K654M6V/2310.html:text/html},
}

@inproceedings{li_implicit_2021,
	location = {Online},
	title = {Implicit Representations of Meaning in Neural Language Models},
	url = {https://aclanthology.org/2021.acl-long.143},
	doi = {10.18653/v1/2021.acl-long.143},
	abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In {BART} and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {1813--1827},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-10-01},
	date = {2021-08},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/R8A8I7NM/Li et al. - 2021 - Implicit Representations of Meaning in Neural Lang.pdf:application/pdf},
}

@misc{petroni_language_2019,
	title = {Language Models as Knowledge Bases?},
	url = {http://arxiv.org/abs/1909.01066},
	doi = {10.48550/arXiv.1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream {NLP} tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, {BERT} contains relational knowledge competitive with traditional {NLP} methods that have some access to oracle knowledge, (ii) {BERT} also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain {QA} systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/{LAMA}.},
	number = {{arXiv}:1909.01066},
	publisher = {{arXiv}},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	urldate = {2024-12-15},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.01066 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/2E6A5S5M/Petroni et al. - 2019 - Language Models as Knowledge Bases.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/J8ZCR5M3/1909.html:text/html},
}

@article{qi_what_2023,
	title = {What is the limitation of multimodal {LLMs}? A deeper look into multimodal {LLMs} through prompt probing},
	volume = {60},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457323002479},
	doi = {10.1016/j.ipm.2023.103510},
	shorttitle = {What is the limitation of multimodal {LLMs}?},
	abstract = {Large language models ({LLMs}) are believed to contain vast knowledge. Many works have extended {LLMs} to multimodal models and applied them to various multimodal downstream tasks with a unified model structure using prompt. Appropriate prompts can stimulate the knowledge capabilities of the model to solve different tasks. However, how the content of the prompts affects the model’s understanding of the information is still under-explored in the literature. We fill this gap by offering a systematic study on prompt probing for multimodal {LLMs}, examining various factors for their understanding of prompts. To achieve this goal, we propose a novel prompt probing framework that starts with the input and designs three types of input change strategies as templates for probing: visual prompt, text prompt and extra knowledge prompt. Our extensive experiments on the {VQA} dataset show that existing multimodal {LLMs} do not understand the input content but more simply fit the training data distribution. Current multimodal models are still very far from understanding prompts properly.},
	pages = {103510},
	number = {6},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Qi, Shuhan and Cao, Zhengying and Rao, Jun and Wang, Lei and Xiao, Jing and Wang, Xuan},
	urldate = {2024-12-15},
	date = {2023-11-01},
	keywords = {Large language model, Model probing, {MultiModal} {LLMs}, Prompt learning, Visual question answer, Zero-shot/few-shot},
	file = {ScienceDirect Snapshot:/Users/helenlu/Zotero/storage/X8MPEJPF/S0306457323002479.html:text/html},
}

@inproceedings{wang_readprompt_2023,
	location = {Singapore},
	title = {{ReadPrompt}: A Readable Prompting Method for Reliable Knowledge Probing},
	url = {https://aclanthology.org/2023.findings-emnlp.501},
	doi = {10.18653/v1/2023.findings-emnlp.501},
	shorttitle = {{ReadPrompt}},
	eventtitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	pages = {7468--7479},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zezhong and Ye, Luyao and Wang, Hongru and Kwan, Wai-Chung and Ho, David and Wong, Kam-Fai},
	urldate = {2024-12-15},
	date = {2023},
	langid = {english},
	file = {Wang et al. - 2023 - ReadPrompt A Readable Prompting Method for Reliab.pdf:/Users/helenlu/Zotero/storage/MDGKY9Y4/Wang et al. - 2023 - ReadPrompt A Readable Prompting Method for Reliab.pdf:application/pdf},
}

@misc{wu_cognitive_2024,
	title = {Cognitive {LLMs}: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making},
	url = {http://arxiv.org/abs/2408.09176},
	doi = {10.48550/arXiv.2408.09176},
	shorttitle = {Cognitive {LLMs}},
	abstract = {Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models ({LLMs}) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing {LLMs} with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use {LLMs} for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference -- reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce {LLM}-{ACTR}, a novel neuro-symbolic architecture that provides human-aligned and versatile decision-making by integrating the {ACT}-R Cognitive Architecture with {LLMs}. Our framework extracts and embeds knowledge of {ACT}-R's internal decision-making process as latent neural representations, injects this information into trainable {LLM} adapter layers, and fine-tunes the {LLMs} for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to {LLM}-only baselines that leverage chain-of-thought reasoning strategies.},
	number = {{arXiv}:2408.09176},
	publisher = {{arXiv}},
	author = {Wu, Siyu and Oltramari, Alessandro and Francis, Jonathan and Giles, C. Lee and Ritter, Frank E.},
	urldate = {2024-09-24},
	date = {2024-08-17},
	eprinttype = {arxiv},
	eprint = {2408.09176 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/XM7SN9WR/Wu et al. - 2024 - Cognitive LLMs Towards Integrating Cognitive Arch.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/LADD4RXZ/2408.html:text/html},
}

