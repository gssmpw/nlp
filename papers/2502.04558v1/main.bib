
@incollection{aldinhas_ferreira_overview_2019,
	location = {Cham},
	title = {An Overview of the Distributed Integrated Cognition Affect and Reflection {DIARC} Architecture},
	volume = {94},
	isbn = {978-3-319-97549-8 978-3-319-97550-4},
	url = {http://link.springer.com/10.1007/978-3-319-97550-4_11},
	abstract = {{DIARC} has been under development for over 15 years. Different from other cognitive architectures like {SOAR} or {ACT}-R, {DIARC} is an intrinsically component-based distributed architecture scheme that can be instantiated in many different ways. Moreover, {DIARC} has several distinguishing features, such as affect processing and deep natural language integration, is open-world and multi-agent enabled, and allows for “one-shot instruction-based learning” of new percepts, actions, concepts, rules, and norms.},
	pages = {165--193},
	booktitle = {Cognitive Architectures},
	publisher = {Springer International Publishing},
	author = {Scheutz, Matthias and Williams, Thomas and Krause, Evan and Oosterveld, Bradley and Sarathy, Vasanth and Frasca, Tyler},
	editor = {Aldinhas Ferreira, Maria Isabel and Silva Sequeira, João and Ventura, Rodrigo},
	urldate = {2023-12-19},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-319-97550-4_11},
	note = {Series Title: Intelligent Systems, Control and Automation: Science and Engineering},
}

@misc{morita_cognitive_2023,
	title = {Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative {AIs}: Trial on Model-Model Interactions in Tangram Naming Task},
	url = {http://arxiv.org/abs/2311.05851},
	doi = {10.48550/arXiv.2311.05851},
	shorttitle = {Cognitive Architecture Toward Common Ground Sharing Among Humans and Generative {AIs}},
	abstract = {For generative {AIs} to be trustworthy, establishing transparent common grounding with humans is essential. As a preparation toward human-model common grounding, this study examines the process of model-model common grounding. In this context, common ground is defined as a cognitive framework shared among agents in communication, enabling the connection of symbols exchanged between agents to the meanings inherent in each agent. This connection is facilitated by a shared cognitive framework among the agents involved. In this research, we focus on the tangram naming task ({TNT}) as a testbed to examine the common-ground-building process. Unlike previous models designed for this task, our approach employs generative {AIs} to visualize the internal processes of the model. In this task, the sender constructs a metaphorical image of an abstract figure within the model and generates a detailed description based on this image. The receiver interprets the generated description from the partner by constructing another image and reconstructing the original abstract figure. Preliminary results from the study show an improvement in task performance beyond the chance level, indicating the effect of the common cognitive framework implemented in the models. Additionally, we observed that incremental backpropagations leveraging successful communication cases for a component of the model led to a statistically significant increase in performance. These results provide valuable insights into the mechanisms of common grounding made by generative {AIs}, improving human communication with the evolving intelligent machines in our future society.},
	number = {{arXiv}:2311.05851},
	publisher = {{arXiv}},
	author = {Morita, Junya and Yui, Tatsuya and Amaya, Takeru and Higashinaka, Ryuichiro and Takeuchi, Yugo},
	urldate = {2024-09-04},
	date = {2023-11-09},
	eprinttype = {arxiv},
	eprint = {2311.05851 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/L9GA95W8/Morita et al. - 2023 - Cognitive Architecture Toward Common Ground Sharin.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/NJZ87ZEK/2311.html:text/html},
}

@article{mcdonald_exploring_2023,
	title = {Exploring the Path from Instructions to Rewards with Large Language Models in Instance-Based Learning},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27697},
	doi = {10.1609/aaaiss.v2i1.27697},
	abstract = {A prominent method to model human learning is through experiential learning, where decisions are influenced by the outcomes observed in previous actions. The decisions-from-experience approach often excludes other forms of learning in humans, such as learning from descriptive information. In humans, descriptive information can enhance learning by providing a denser signal, achieved through understanding the relationship between intermediate decisions and their future outcomes, instead of relying solely on observed outcomes. To account for experiential and descriptive information, we propose the use of large language models ({LLMs}) to convert descriptive information into dense signals that can be used by computational models that learn from experience. Building on past work in cognitive modeling, we utilize task instructions and prompt an {LLM} to define and quantify the critical actions an agent must take to succeed in the task. In an initial experiment, we test this approach using an Instance-Based Learning cognitive model of experiential decisions in a gridworld task. We demonstrate how the {LLM} can be prompted to provide a series of actions and relative values given the task instructions, then show how these values can be used in place of sparse outcome signals to improve the model’s learning of the task significantly.},
	pages = {334--339},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {{McDonald}, Chase and Malloy, Tyler and Nguyen, Thuy Ngoc and Gonzalez, Cleotilde},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Intrinsic Rewards},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/CNLILLXJ/McDonald et al. - 2023 - Exploring the Path from Instructions to Rewards wi.pdf:application/pdf},
}

@article{maher_grounding_2023,
	title = {The Grounding Problem: An Approach to the Integration of Cognitive and Generative Models},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27695},
	doi = {10.1609/aaaiss.v2i1.27695},
	shorttitle = {The Grounding Problem},
	abstract = {The integration of cognitive and neural {AI} paradigms is a promising direction for overcoming the limitations of current deep learning models, but how to effect this integration is an open question. We propose that the key to this challenge lies in addressing the question of grounding. We adopt a cognitive perspective on grounding, and identify five types of grounding that are relevant for {AI} systems. We discuss ways that grounding in both cognitive and neural {AI} systems can facilitate the integration of these two paradigms, illustrating with examples in the domains of computational creativity and education. Because grounding is not only a technical problem but also a social and ethical one, requiring the collaboration and participation of multiple stakeholders, prosecuting such a research program is both timely and challenging.},
	pages = {320--325},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Maher, Mary Lou and Ventura, Dan and Magerko, Brian},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Education},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/MC4JIUDT/Maher et al. - 2023 - The Grounding Problem An Approach to the Integrat.pdf:application/pdf},
}

@misc{kirk_exploiting_2023,
	title = {Exploiting Language Models as a Source of Knowledge for Cognitive Agents},
	url = {http://arxiv.org/abs/2310.06846},
	doi = {10.48550/arXiv.2310.06846},
	abstract = {Large language models ({LLMs}) provide capabilities far beyond sentence completion, including question answering, summarization, and natural-language inference. While many of these capabilities have potential application to cognitive systems, our research is exploiting language models as a source of task knowledge for cognitive agents, that is, agents realized via a cognitive architecture. We identify challenges and opportunities for using language models as an external knowledge source for cognitive systems and possible ways to improve the effectiveness of knowledge extraction by integrating extraction with cognitive architecture capabilities, highlighting with examples from our recent work in this area.},
	number = {{arXiv}:2310.06846},
	publisher = {{arXiv}},
	author = {Kirk, James R. and Wray, Robert E. and Laird, John E.},
	urldate = {2024-09-04},
	date = {2023-09-05},
	eprinttype = {arxiv},
	eprint = {2310.06846 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, I.2.7, I.2.11},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/Y3HUCXRL/Kirk et al. - 2023 - Exploiting Language Models as a Source of Knowledg.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/5K654M6V/2310.html:text/html},
}

@article{gupta_building_2023,
	title = {Building Intelligent Systems by Combining Machine Learning and Automated Commonsense Reasoning},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27687},
	doi = {10.1609/aaaiss.v2i1.27687},
	abstract = {We present an approach to building systems that emulate human-like intelligence. Our approach uses machine learning technology (including generative {AI} systems) to extract knowledge from pictures, text, etc., and represents it as (pre-defined) predicates. Next, we use the s({CASP}) automated commonsense reasoning system to check the consistency of this extracted knowledge and reason over it in a manner very similar to how a human would do it. We have used our approach for building systems for visual question answering, task-specific chatbots that can ``understand" human dialogs and interactively talk to them, and autonomous driving systems that rely on commonsense reasoning. Essentially, our approach emulates how humans process knowledge where they use sensing and pattern recognition to gain knowledge (Kahneman's System 1 thinking, akin to using a machine learning model), and then use reasoning to draw conclusions, generate response, or take actions (Kahneman's System 2 thinking, akin to automated reasoning).},
	pages = {272--276},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Gupta, Gopal and Zeng, Yankai and Rajasekaran, Abhiraman and Padalkar, Parth and Kimbrell, Keegan and Basu, Kinjal and Shakerin, Farahad and Salazar, Elmer and Arias, Joaquín},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Conversational Agent},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/VCYLXUH6/Gupta et al. - 2023 - Building Intelligent Systems by Combining Machine .pdf:application/pdf},
}

@article{choi_using_2023,
	title = {On Using Generative Models in a Cognitive Architecture for Embodied Agents},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27684},
	doi = {10.1609/aaaiss.v2i1.27684},
	abstract = {Recent popularity of generative models brought research on a variety of applications. We take a more architectural point of view, where we discuss ways in which generative {AI} techniques and cognitive architectures can benefit each other for a more capable overall integrated system. We use a cognitive architecture, {ICARUS}, as the framework for our discussion, but most of the discussed points should carry over to other architectures as well.},
	pages = {253--255},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Choi, Dongkyu},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Large Language Models},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/L7SDAITF/Choi - 2023 - On Using Generative Models in a Cognitive Architec.pdf:application/pdf},
}

@article{bajaj_generating_2023,
	title = {Generating Chunks for Cognitive Architectures},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27683},
	doi = {10.1609/aaaiss.v2i1.27683},
	abstract = {Knowledge engineering is an important task for creating and maintaining a knowledge base for cognitive models. It involves acquiring, representing, and organizing knowledge in a form that computers can use to make decisions and solve problems. However, this process can be a bottleneck for designing and using cognitive models. Knowledge engineering is a time-consuming and resource-intensive task that requires subject matter experts to provide information about a domain. In addition, models can acquire knowledge but require significant mechanisms to structure that information in a structured format appropriate for general use. Given the knowledge engineering bottleneck, we propose a solution that relies on natural language processing to extract key entities, relationships, and attributes to automatically generate chunks encoded as triples or chunks from unstructured text. Once generated, the knowledge can be used to create or add to a knowledge base within cognitive architectures to reduce knowledge engineering and task-specific models.},
	pages = {246--252},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Bajaj, Goonmeet and Pearce, Kate and Kennedy, Sean and Larue, Othalia and Hough, Alexander and King, Jayde and Myers, Christopher and Parthasarathy, Srinivasan},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Knowledge Engineering},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/AWC8D5AB/Bajaj et al. - 2023 - Generating Chunks for Cognitive Architectures.pdf:application/pdf},
}

@article{furlong_bridging_2023,
	title = {Bridging Cognitive Architectures and Generative Models with Vector Symbolic Algebras},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27686},
	doi = {10.1609/aaaiss.v2i1.27686},
	abstract = {Recent developments in generative models have demonstrated that with the right data set, techniques, computational infrastructure, and network architectures,  it is possible to generate seemingly intelligent outputs, without explicitly reckoning with underlying cognitive processes.  The ability to generate novel, plausible behaviour could be a boon to cognitive modellers.  However, insights for cognition are limited, given that generative models' blackbox nature does not provide readily interpretable hypotheses about underlying cognitive mechanisms. On the other hand, cognitive architectures make very strong hypotheses about the nature of cognition, explicitly describing the subjects and processes of reasoning. Unfortunately, the formal framings of cognitive architectures can make it difficult to generate novel or creative outputs. We propose to show that  cognitive architectures that rely on certain Vector Symbolic Algebras ({VSAs})  are, in fact, naturally understood as generative models. We discuss how memories of {VSA} representations of data form distributions, which are necessary for constructing distributions used in generative models. Finally, we discuss the strengths, challenges, and future directions for this line of work.},
	pages = {262--271},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Furlong, P. Michael and Eliasmith, Chris},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Generative Models},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/9J9IIJK7/Furlong and Eliasmith - 2023 - Bridging Cognitive Architectures and Generative Mo.pdf:application/pdf},
}

@article{wu_comparing_2023,
	title = {Comparing {LLMs} for Prompt-Enhanced {ACT}-R and Soar Model Development: A Case Study in Cognitive Simulation},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27710},
	doi = {10.1609/aaaiss.v2i1.27710},
	shorttitle = {Comparing {LLMs} for Prompt-Enhanced {ACT}-R and Soar Model Development},
	abstract = {This paper presents experiments on using {ChatGPT}4 and Google Bard to create {ACT}-R and Soar models. The study involves two simulated cognitive tasks, where {ChatGPT}4 and Google Bard (Large Language Models, {LLMs}) serve as conversational interfaces within the {ACT}-R and Soar framework development environments. The first task involves creating an intelligent driving model using {ACT}-R with motor and perceptual behavior and can further interact with an unmodified interface. The second task evaluates the development of educational skills using Soar. Prompts were designed to represent cognitive operations and actions, including providing context, asking perception-related questions, decision-making scenarios, and evaluating the system's responses, and they were iteratively refined based on model behavior evaluation. Results demonstrate the potential of using {LLMs} to serve as interactive interfaces to develop {ACT}-R and Soar models within a human-in-the-loop model development process. We documented the mistakes {LLMs} made during this integration and provided corresponding resolutions when adopting this modeling approach. Furthermore, we presented a framework of prompt patterns that maximizes {LLMs} interaction for artificial cognitive architectures.},
	pages = {422--427},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Wu, Siyu and Souza, Rodrigo F. and Ritter, Frank E. and Jr, Walter T. Lima},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {{LLMs}},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/Z39P7ZYQ/Wu et al. - 2023 - Comparing LLMs for Prompt-Enhanced ACT-R and Soar .pdf:application/pdf},
}

@misc{romero_synergistic_2023,
	title = {Synergistic Integration of Large Language Models and Cognitive Architectures for Robust {AI}: An Exploratory Analysis},
	url = {http://arxiv.org/abs/2308.09830},
	doi = {10.48550/arXiv.2308.09830},
	shorttitle = {Synergistic Integration of Large Language Models and Cognitive Architectures for Robust {AI}},
	abstract = {This paper explores the integration of two {AI} subdisciplines employed in the development of artificial agents that exhibit intelligent behavior: Large Language Models ({LLMs}) and Cognitive Architectures ({CAs}). We present three integration approaches, each grounded in theoretical models and supported by preliminary empirical evidence. The modular approach, which introduces four models with varying degrees of integration, makes use of chain-of-thought prompting, and draws inspiration from augmented {LLMs}, the Common Model of Cognition, and the simulation theory of cognition. The agency approach, motivated by the Society of Mind theory and the {LIDA} cognitive architecture, proposes the formation of agent collections that interact at micro and macro cognitive levels, driven by either {LLMs} or symbolic components. The neuro-symbolic approach, which takes inspiration from the {CLARION} cognitive architecture, proposes a model where bottom-up learning extracts symbolic representations from an {LLM} layer and top-down guidance utilizes symbolic representations to direct prompt engineering in the {LLM} layer. These approaches aim to harness the strengths of both {LLMs} and {CAs}, while mitigating their weaknesses, thereby advancing the development of more robust {AI} systems. We discuss the tradeoffs and challenges associated with each approach.},
	number = {{arXiv}:2308.09830},
	publisher = {{arXiv}},
	author = {Romero, Oscar J. and Zimmerman, John and Steinfeld, Aaron and Tomasic, Anthony},
	urldate = {2024-09-04},
	date = {2023-09-28},
	eprinttype = {arxiv},
	eprint = {2308.09830 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/V848TAHW/Romero et al. - 2023 - Synergistic Integration of Large Language Models a.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/S3P7PF8C/2308.html:text/html},
}

@article{raja_leveraging_2023,
	title = {Leveraging Conflict to Bridge Cognitive Reasoning and Generative Algorithms},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27705},
	doi = {10.1609/aaaiss.v2i1.27705},
	abstract = {Autonomous agents require the ability to identify and adapt to unexpected conditions. Real-world environments are rarely stationary, making it problematic for agents operating in such environments to learn efficient  policies. There is therefore a need for a general framework capable of detecting when an agent has encountered novel conditions, and determining how it should adjust its actions. In this position paper we propose a framework that couples cognitive reasoning and generative algorithms by leveraging conflict detection to adjust to unexpected dynamics. Specifically, we propose that a metacognitive conflict resolution mechanism is necessary; such a mechanism would balance the use of commonsense and deliberative reasoning to allow the agent to navigate novel conditions.},
	pages = {391--395},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Raja, Anita and Leshchenko, Alisa and Kim, Jihie},
	urldate = {2024-09-04},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Non-stationarity},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/ZYQ4LJEP/Raja et al. - 2023 - Leveraging Conflict to Bridge Cognitive Reasoning .pdf:application/pdf},
}

@misc{oruganti_automating_2023,
	title = {Automating Knowledge Acquisition for Content-Centric Cognitive Agents Using {LLMs}},
	url = {http://arxiv.org/abs/2312.16378},
	doi = {10.48550/arXiv.2312.16378},
	abstract = {The paper describes a system that uses large language model ({LLM}) technology to support the automatic learning of new entries in an intelligent agent's semantic lexicon. The process is bootstrapped by an existing non-toy lexicon and a natural language generator that converts formal, ontologically-grounded representations of meaning into natural language sentences. The learning method involves a sequence of {LLM} requests and includes an automatic quality control step. To date, this learning method has been applied to learning multiword expressions whose meanings are equivalent to those of transitive verbs in the agent's lexicon. The experiment demonstrates the benefits of a hybrid learning architecture that integrates knowledge-based methods and resources with both traditional data analytics and {LLMs}.},
	number = {{arXiv}:2312.16378},
	publisher = {{arXiv}},
	author = {Oruganti, Sanjay and Nirenburg, Sergei and English, Jesse and {McShane}, Marjorie},
	urldate = {2024-09-04},
	date = {2023-12-26},
	eprinttype = {arxiv},
	eprint = {2312.16378 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/WUV6NPLM/Oruganti et al. - 2023 - Automating Knowledge Acquisition for Content-Centr.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/Q7KABWJU/2312.html:text/html},
}

@misc{oltramari_enabling_2023,
	title = {Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems},
	url = {http://arxiv.org/abs/2311.07759},
	doi = {10.48550/arXiv.2311.07759},
	abstract = {High-level reasoning can be defined as the capability to generalize over knowledge acquired via experience, and to exhibit robust behavior in novel situations. Such form of reasoning is a basic skill in humans, who seamlessly use it in a broad spectrum of tasks, from language communication to decision making in complex situations. When it manifests itself in understanding and manipulating the everyday world of objects and their interactions, we talk about common sense or commonsense reasoning. State-of-the-art {AI} systems don't possess such capability: for instance, Large Language Models have recently become popular by demonstrating remarkable fluency in conversing with humans, but they still make trivial mistakes when probed for commonsense competence; on a different level, performance degradation outside training data prevents self-driving vehicles to safely adapt to unseen scenarios, a serious and unsolved problem that limits the adoption of such technology. In this paper we propose to enable high-level reasoning in {AI} systems by integrating cognitive architectures with external neuro-symbolic components. We illustrate a hybrid framework centered on {ACT}-R and we discuss the role of generative models in recent and future applications.},
	number = {{arXiv}:2311.07759},
	publisher = {{arXiv}},
	author = {Oltramari, Alessandro},
	urldate = {2024-09-04},
	date = {2023-11-13},
	eprinttype = {arxiv},
	eprint = {2311.07759 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/EQNQKDT8/Oltramari - 2023 - Enabling High-Level Machine Reasoning with Cogniti.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/TJNJQJMB/2311.html:text/html},
}

@misc{gonzalez-santamarta_integration_2024,
	title = {Integration of Large Language Models within Cognitive Architectures for Autonomous Robots},
	url = {http://arxiv.org/abs/2309.14945},
	doi = {10.48550/arXiv.2309.14945},
	abstract = {Symbolic reasoning systems have been used in cognitive architectures to provide inference and planning capabilities. However, defining domains and problems has proven difficult and prone to errors. Moreover, Large Language Models ({LLMs}) have emerged as tools to process natural language for different tasks. In this paper, we propose the use of {LLMs} to tackle these problems. This way, this paper proposes the integration of {LLMs} in the {ROS} 2-integrated cognitive architecture {MERLIN}2 for autonomous robots. Specifically, we present the design, development and deployment of how to leverage the reasoning capabilities of {LLMs} inside the deliberative processes of {MERLIN}2. As a result, the deliberative system is updated from a {PDDL}-based planner system to a natural language planning system. This proposal is evaluated quantitatively and qualitatively, measuring the impact of incorporating the {LLMs} in the cognitive architecture. Results show that a classical approach achieves better performance but the proposed solution provides an enhanced interaction through natural language.},
	number = {{arXiv}:2309.14945},
	publisher = {{arXiv}},
	author = {González-Santamarta, Miguel Á and Rodríguez-Lera, Francisco J. and Guerrero-Higueras, Ángel Manuel and Matellán-Olivera, Vicente},
	urldate = {2024-09-24},
	date = {2024-03-23},
	eprinttype = {arxiv},
	eprint = {2309.14945 [cs]},
	keywords = {Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/PQ4877TW/González-Santamarta et al. - 2024 - Integration of Large Language Models within Cognit.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/ARCR5GA9/2309.html:text/html},
}

@article{peller-konrad_memory_2023,
	title = {A Memory System of a Robot Cognitive Architecture and its Implementation in {ArmarX}},
	volume = {164},
	issn = {09218890},
	url = {http://arxiv.org/abs/2206.02241},
	doi = {10.1016/j.robot.2023.104415},
	abstract = {Cognitive agents such as humans and robots perceive their environment through an abundance of sensors producing streams of data that need to be processed to generate intelligent behavior. A key question of cognition-enabled and {AI}-driven robotics is how to organize and manage knowledge efficiently in a cognitive robot control architecture. We argue, that memory is a central active component of such architectures that mediates between semantic and sensorimotor representations, orchestrates the flow of data streams and events between different processes and provides the components of a cognitive architecture with data-driven services for the abstraction of semantics from sensorimotor data, the parametrization of symbolic plans for execution and prediction of action effects. Based on related work, and the experience gained in developing our {ARMAR} humanoid robot systems, we identified conceptual and technical requirements of a memory system as central component of cognitive robot control architecture that facilitate the realization of high-level cognitive abilities such as explaining, reasoning, prospection, simulation and augmentation. Conceptually, a memory should be active, support multi-modal data representations, associate knowledge, be introspective, and have an inherently episodic structure. Technically, the memory should support a distributed design, be access-efficient and capable of long-term data storage. We introduce the memory system for our cognitive robot control architecture and its implementation in the robot software framework {ArmarX}. We evaluate the efficiency of the memory system with respect to transfer speeds, compression, reproduction and prediction capabilities.},
	pages = {104415},
	journaltitle = {Robotics and Autonomous Systems},
	shortjournal = {Robotics and Autonomous Systems},
	author = {Peller-Konrad, Fabian and Kartmann, Rainer and Dreher, Christian R. G. and Meixner, Andre and Reister, Fabian and Grotz, Markus and Asfour, Tamim},
	urldate = {2024-09-24},
	date = {2023-06},
	eprinttype = {arxiv},
	eprint = {2206.02241 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/QXYMAPAP/Peller-Konrad et al. - 2023 - A Memory System of a Robot Cognitive Architecture .pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/UDFXRGD8/2206.html:text/html},
}

@misc{sun_can_2024,
	title = {Can A Cognitive Architecture Fundamentally Enhance {LLMs}? Or Vice Versa?},
	url = {http://arxiv.org/abs/2401.10444},
	doi = {10.48550/arXiv.2401.10444},
	shorttitle = {Can A Cognitive Architecture Fundamentally Enhance {LLMs}?},
	abstract = {The paper discusses what is needed to address the limitations of current {LLM}-centered {AI} systems. The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like. It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current {LLMs}. In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in {AI} and computing technology. Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for {AI} and for understanding the human mind.},
	number = {{arXiv}:2401.10444},
	publisher = {{arXiv}},
	author = {Sun, Ron},
	urldate = {2024-09-24},
	date = {2024-01-18},
	eprinttype = {arxiv},
	eprint = {2401.10444 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/TQSM9PZQ/Sun - 2024 - Can A Cognitive Architecture Fundamentally Enhance.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/9ZQUDB7Q/2401.html:text/html},
}

@misc{chen_analyzing_2024,
	title = {Analyzing the Roles of Language and Vision in Learning from Limited Data},
	url = {http://arxiv.org/abs/2403.19669},
	doi = {10.48550/arXiv.2403.19669},
	abstract = {Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models ({VLMs}) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a {VLM}'s performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoning.},
	number = {{arXiv}:2403.19669},
	publisher = {{arXiv}},
	author = {Chen, Allison and Sucholutsky, Ilia and Russakovsky, Olga and Griffiths, Thomas L.},
	urldate = {2024-09-24},
	date = {2024-05-10},
	eprinttype = {arxiv},
	eprint = {2403.19669 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/D489TXTP/Chen et al. - 2024 - Analyzing the Roles of Language and Vision in Lear.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/BWTAT9SW/2403.html:text/html},
}

@misc{wu_cognitive_2024,
	title = {Cognitive {LLMs}: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making},
	url = {http://arxiv.org/abs/2408.09176},
	doi = {10.48550/arXiv.2408.09176},
	shorttitle = {Cognitive {LLMs}},
	abstract = {Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models ({LLMs}) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing {LLMs} with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use {LLMs} for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference -- reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce {LLM}-{ACTR}, a novel neuro-symbolic architecture that provides human-aligned and versatile decision-making by integrating the {ACT}-R Cognitive Architecture with {LLMs}. Our framework extracts and embeds knowledge of {ACT}-R's internal decision-making process as latent neural representations, injects this information into trainable {LLM} adapter layers, and fine-tunes the {LLMs} for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to {LLM}-only baselines that leverage chain-of-thought reasoning strategies.},
	number = {{arXiv}:2408.09176},
	publisher = {{arXiv}},
	author = {Wu, Siyu and Oltramari, Alessandro and Francis, Jonathan and Giles, C. Lee and Ritter, Frank E.},
	urldate = {2024-09-24},
	date = {2024-08-17},
	eprinttype = {arxiv},
	eprint = {2408.09176 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Symbolic Computation},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/XM7SN9WR/Wu et al. - 2024 - Cognitive LLMs Towards Integrating Cognitive Arch.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/LADD4RXZ/2408.html:text/html},
}

@online{noauthor_icml_nodate,
	title = {{ICML} Poster Position: {LLMs} Can’t Plan, But Can Help Planning in {LLM}-Modulo Frameworks},
	url = {https://icml.cc/virtual/2024/poster/33965},
	urldate = {2024-09-24},
	file = {ICML Poster Position\: LLMs Can’t Plan, But Can Help Planning in LLM-Modulo Frameworks:/Users/helenlu/Zotero/storage/5G8ICUJI/33965.html:text/html},
}

@article{mitsopoulos_psychologically-valid_2023,
	title = {Psychologically-Valid Generative Agents: A Novel Approach to Agent-Based Modeling in Social Sciences},
	volume = {2},
	rights = {Copyright (c) 2023 Association for the Advancement of Artificial Intelligence},
	issn = {2994-4317},
	url = {https://ojs.aaai.org/index.php/AAAI-SS/article/view/27698},
	doi = {10.1609/aaaiss.v2i1.27698},
	shorttitle = {Psychologically-Valid Generative Agents},
	abstract = {Incorporating dynamic realistic human behaviors in population-scale computational models has been challenging.  While some efforts have leveraged behavioral theories from social science, validated theories specifically applicable to  Agent-based modeling remain limited. Existing approaches lack a comprehensive framework to model the situated, adaptive nature of human cognition and choice. To address these challenges, this paper proposes a novel framework, Psychologically-Valid Generative Agents. These agents consist of a Cognitive Architecture that provides data-driven and cognitively-constrained decision-making functionality, and a Large Language Model that generates human-like linguistic data. In addition, our framework benefits from Stance Detection, a Natural Language Processing technique, that allows highly personalize initialization of the agents, based on real-world data, within Agent-based modeling simulations. This combination provides a flexible yet structured approach to endogenously represent how people perceive, deliberate, and respond to social or other types of complex decision-making dynamics. Previous work has demonstrated promising results by using a subset of the components of our proposed architecture. Our approach has the potential to exhibit highly-realistic human behavior and can be used across a variety of domains (e.g., public health, group dynamics, social and psychological sciences, and financial markets).},
	pages = {340--348},
	number = {1},
	journaltitle = {Proceedings of the {AAAI} Symposium Series},
	author = {Mitsopoulos, Konstantinos and Bose, Ritwik and Mather, Brodie and Bhatia, Archna and Gluck, Kevin and Dorr, Bonnie and Lebiere, Christian and Pirolli, Peter},
	urldate = {2024-09-24},
	date = {2023},
	langid = {english},
	note = {Number: 1},
	keywords = {Natural Language Processing},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/8J6Q7U5X/Mitsopoulos et al. - 2023 - Psychologically-Valid Generative Agents A Novel A.pdf:application/pdf},
}

@misc{wang_towards_2024,
	title = {Towards Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation: An Empirical Study},
	url = {http://arxiv.org/abs/2409.12894},
	doi = {10.48550/arXiv.2409.12894},
	shorttitle = {Towards Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation},
	abstract = {Multi-modal foundation models and generative {AI} have demonstrated promising capabilities in applications across various domains. Recently, Vision-language-action ({VLA}) models have attracted much attention regarding their potential to advance robotic manipulation. Despite the end-to-end perception-control loop offered by the {VLA} models, there is a lack of comprehensive understanding of the capabilities of such models and an automated testing platform to reveal their robustness and reliability across different robotic manipulation scenarios. To address these challenges, in this work, we present {VLATest}, a testing framework that automatically generates diverse robotic manipulation scenes to assess the performance of {VLA} models from various perspectives. Large-scale experiments are considered, including eight {VLA} models, four types of manipulation tasks, and over 18,604 testing scenes. The experimental results show that existing {VAL} models still lack imperative robustness for practical applications. Specifically, the performance of {VLA} models can be significantly affected by several factors from the operation environments, such as camera poses, lighting conditions, and unseen objects. Our framework and the insights derived from the study are expected to pave the way for more advanced and reliable {VLA}-enabled robotic manipulation systems in practice.},
	number = {{arXiv}:2409.12894},
	publisher = {{arXiv}},
	author = {Wang, Zhijie and Zhou, Zhehua and Song, Jiayang and Huang, Yuheng and Shu, Zhan and Ma, Lei},
	urldate = {2024-09-28},
	date = {2024-09-19},
	eprinttype = {arxiv},
	eprint = {2409.12894 [cs]},
	keywords = {Computer Science - Robotics, Computer Science - Software Engineering},
	file = {arXiv Fulltext PDF:/Users/helenlu/Zotero/storage/HIYEJDQA/Wang et al. - 2024 - Towards Testing and Evaluating Vision-Language-Act.pdf:application/pdf;arXiv.org Snapshot:/Users/helenlu/Zotero/storage/Z4KTBI2L/2409.html:text/html},
}

@online{noauthor_open_nodate,
	title = {Open X-Embodiment: Robotic Learning Datasets and {RT}-X Models},
	url = {https://robotics-transformer-x.github.io/},
	shorttitle = {Open X-Embodiment},
	abstract = {Project page for Open X-Embodiment: Robotic Learning Datasets and {RT}-X Models.},
	urldate = {2024-10-01},
	file = {Snapshot:/Users/helenlu/Zotero/storage/4QM9NSNX/robotics-transformer-x.github.io.html:text/html},
}

@inproceedings{li_implicit_2021,
	location = {Online},
	title = {Implicit Representations of Meaning in Neural Language Models},
	url = {https://aclanthology.org/2021.acl-long.143},
	doi = {10.18653/v1/2021.acl-long.143},
	abstract = {Does the effectiveness of neural language models derive entirely from accurate modeling of surface word co-occurrence statistics, or do these models represent and reason about the world they describe? In {BART} and T5 transformer language models, we identify contextual word representations that function as *models of entities and situations* as they evolve throughout a discourse. These neural representations have functional similarities to linguistic models of dynamic semantics: they support a linear readout of each entity's current properties and relations, and can be manipulated with predictable effects on language generation. Our results indicate that prediction in pretrained neural language models is supported, at least in part, by dynamic representations of meaning and implicit simulation of entity state, and that this behavior can be learned with only text as training data.},
	eventtitle = {{ACL}-{IJCNLP} 2021},
	pages = {1813--1827},
	booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Li, Belinda Z. and Nye, Maxwell and Andreas, Jacob},
	editor = {Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto},
	urldate = {2024-10-01},
	date = {2021-08},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/R8A8I7NM/Li et al. - 2021 - Implicit Representations of Meaning in Neural Lang.pdf:application/pdf},
}

@online{kim_openvla_2024,
	title = {{OpenVLA}: An Open-Source Vision-Language-Action Model},
	url = {https://arxiv.org/abs/2406.09246v3},
	shorttitle = {{OpenVLA}},
	abstract = {Large policies pretrained on a combination of Internet-scale vision-language data and diverse robot demonstrations have the potential to change how we teach robots new skills: rather than training new behaviors from scratch, we can fine-tune such vision-language-action ({VLA}) models to obtain robust, generalizable policies for visuomotor control. Yet, widespread adoption of {VLAs} for robotics has been challenging as 1) existing {VLAs} are largely closed and inaccessible to the public, and 2) prior work fails to explore methods for efficiently fine-tuning {VLAs} for new tasks, a key component for adoption. Addressing these challenges, we introduce {OpenVLA}, a 7B-parameter open-source {VLA} trained on a diverse collection of 970k real-world robot demonstrations. {OpenVLA} builds on a Llama 2 language model combined with a visual encoder that fuses pretrained features from {DINOv}2 and {SigLIP}. As a product of the added data diversity and new model components, {OpenVLA} demonstrates strong results for generalist manipulation, outperforming closed models such as {RT}-2-X (55B) by 16.5\% in absolute task success rate across 29 tasks and multiple robot embodiments, with 7x fewer parameters. We further show that we can effectively fine-tune {OpenVLA} for new settings, with especially strong generalization results in multi-task environments involving multiple objects and strong language grounding abilities, and outperform expressive from-scratch imitation learning methods such as Diffusion Policy by 20.4\%. We also explore compute efficiency; as a separate contribution, we show that {OpenVLA} can be fine-tuned on consumer {GPUs} via modern low-rank adaptation methods and served efficiently via quantization without a hit to downstream success rate. Finally, we release model checkpoints, fine-tuning notebooks, and our {PyTorch} codebase with built-in support for training {VLAs} at scale on Open X-Embodiment datasets.},
	titleaddon = {{arXiv}.org},
	author = {Kim, Moo Jin and Pertsch, Karl and Karamcheti, Siddharth and Xiao, Ted and Balakrishna, Ashwin and Nair, Suraj and Rafailov, Rafael and Foster, Ethan and Lam, Grace and Sanketi, Pannag and Vuong, Quan and Kollar, Thomas and Burchfiel, Benjamin and Tedrake, Russ and Sadigh, Dorsa and Levine, Sergey and Liang, Percy and Finn, Chelsea},
	urldate = {2024-10-04},
	date = {2024-06-13},
	langid = {english},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/9336XWNC/Kim et al. - 2024 - OpenVLA An Open-Source Vision-Language-Action Mod.pdf:application/pdf},
}

@online{team_octo_2024,
	title = {Octo: An Open-Source Generalist Robot Policy},
	url = {https://arxiv.org/abs/2405.12213v2},
	shorttitle = {Octo},
	abstract = {Large policies pretrained on diverse robot datasets have the potential to transform robotic learning: instead of training new policies from scratch, such generalist robot policies may be finetuned with only a little in-domain data, yet generalize broadly. However, to be widely applicable across a range of robotic learning scenarios, environments, and tasks, such policies need to handle diverse sensors and action spaces, accommodate a variety of commonly used robotic platforms, and finetune readily and efficiently to new domains. In this work, we aim to lay the groundwork for developing open-source, widely applicable, generalist policies for robotic manipulation. As a first step, we introduce Octo, a large transformer-based policy trained on 800k trajectories from the Open X-Embodiment dataset, the largest robot manipulation dataset to date. It can be instructed via language commands or goal images and can be effectively finetuned to robot setups with new sensory inputs and action spaces within a few hours on standard consumer {GPUs}. In experiments across 9 robotic platforms, we demonstrate that Octo serves as a versatile policy initialization that can be effectively finetuned to new observation and action spaces. We also perform detailed ablations of design decisions for the Octo model, from architecture to training data, to guide future research on building generalist robot models.},
	titleaddon = {{arXiv}.org},
	author = {Team, Octo Model and Ghosh, Dibya and Walke, Homer and Pertsch, Karl and Black, Kevin and Mees, Oier and Dasari, Sudeep and Hejna, Joey and Kreiman, Tobias and Xu, Charles and Luo, Jianlan and Tan, You Liang and Chen, Lawrence Yunliang and Sanketi, Pannag and Vuong, Quan and Xiao, Ted and Sadigh, Dorsa and Finn, Chelsea and Levine, Sergey},
	urldate = {2024-10-04},
	date = {2024-05-20},
	langid = {english},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/SLLMJEKB/Team et al. - 2024 - Octo An Open-Source Generalist Robot Policy.pdf:application/pdf},
}

@inproceedings{dahlgren-lindstrom-etal-2020-probing,
	title = {Probing Multimodal Embeddings for Linguistic Properties: the Visual-Semantic Case},
	url = {http://arxiv.org/abs/2102.11115},
	doi = {10.18653/v1/2020.coling-main.64},
	shorttitle = {Probing Multimodal Embeddings for Linguistic Properties},
	abstract = {Semantic embeddings have advanced the state of the art for countless natural language processing tasks, and various extensions to multimodal domains, such as visual-semantic embeddings, have been proposed. While the power of visual-semantic embeddings comes from the distillation and enrichment of information through machine learning, their inner workings are poorly understood and there is a shortage of analysis tools. To address this problem, we generalize the notion of probing tasks to the visual-semantic case. To this end, we (i) discuss the formalization of probing tasks for embeddings of image-caption pairs, (ii) define three concrete probing tasks within our general framework, (iii) train classifiers to probe for those properties, and (iv) compare various state-of-the-art embeddings under the lens of the proposed probing tasks. Our experiments reveal an up to 12\% increase in accuracy on visual-semantic embeddings compared to the corresponding unimodal embeddings, which suggest that the text and image dimensions represented in the former do complement each other.},
	pages = {730--744},
	booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
	author = {Lindström, Adam Dahlgren and Bensch, Suna and Björklund, Johanna and Drewes, Frank},
	urldate = {2024-10-12},
	date = {2020},
	eprinttype = {arxiv},
	eprint = {2102.11115 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{pimentel_pareto_2020,
	location = {Online},
	title = {Pareto Probing: Trading Off Accuracy for Complexity},
	url = {https://www.aclweb.org/anthology/2020.emnlp-main.254},
	doi = {10.18653/v1/2020.emnlp-main.254},
	shorttitle = {Pareto Probing},
	abstract = {The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the {NLP} literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations—e.g., why should the non-contextual {fastText} representations encode more morpho-syntactic information than the contextual {BERT} representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations. Our code can be found at https://github. com/rycolab/pareto-probing.},
	eventtitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	pages = {3138--3153},
	booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pimentel, Tiago and Saphra, Naomi and Williams, Adina and Cotterell, Ryan},
	urldate = {2024-10-12},
	date = {2020},
	langid = {english},
	file = {Pimentel et al. - 2020 - Pareto Probing Trading Off Accuracy for Complexit.pdf:/Users/helenlu/Zotero/storage/YD34466X/Pimentel et al. - 2020 - Pareto Probing Trading Off Accuracy for Complexit.pdf:application/pdf},
}

@misc{xiao_robot_2023,
	title = {Robot Learning in the Era of Foundation Models: A Survey},
	url = {http://arxiv.org/abs/2311.14379},
	doi = {10.48550/arXiv.2311.14379},
	shorttitle = {Robot Learning in the Era of Foundation Models},
	abstract = {The proliferation of Large Language Models ({LLMs}) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence ({AI}). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and {AI} alignment, etc.},
	number = {{arXiv}:2311.14379},
	publisher = {{arXiv}},
	author = {Xiao, Xuan and Liu, Jiahang and Wang, Zhipeng and Zhou, Yanmin and Qi, Yong and Cheng, Qian and He, Bin and Jiang, Shuo},
	urldate = {2024-10-26},
	date = {2023-11-24},
	eprinttype = {arxiv},
	eprint = {2311.14379},
	keywords = {Computer Science - Robotics},
}

@misc{xiao_robot_2023-1,
	title = {Robot Learning in the Era of Foundation Models: A Survey},
	url = {http://arxiv.org/abs/2311.14379},
	doi = {10.48550/arXiv.2311.14379},
	shorttitle = {Robot Learning in the Era of Foundation Models},
	abstract = {The proliferation of Large Language Models ({LLMs}) has s fueled a shift in robot learning from automation towards general embodied Artificial Intelligence ({AI}). Adopting foundation models together with traditional learning methods to robot learning has increasingly gained recent interest research community and showed potential for real-life application. However, there are few literatures comprehensively reviewing the relatively new technologies combined with robotics. The purpose of this review is to systematically assess the state-of-the-art foundation model techniques in the robot learning and to identify future potential areas. Specifically, we first summarized the technical evolution of robot learning and identified the necessary preliminary preparations for foundation models including the simulators, datasets, foundation model framework. In addition, we focused on the following four mainstream areas of robot learning including manipulation, navigation, planning, and reasoning and demonstrated how the foundation model techniques can be adopted in the above scenarios. Furthermore, critical issues which are neglected in the current literatures including robot hardware and software decoupling, dynamic data, generalization performance with the presence of human, etc. were discussed. This review highlights the state-of-the-art progress of foundation models in robot learning and future research should focus on multimodal interaction especially dynamics data, exclusive foundation models for robots, and {AI} alignment, etc.},
	number = {{arXiv}:2311.14379},
	publisher = {{arXiv}},
	author = {Xiao, Xuan and Liu, Jiahang and Wang, Zhipeng and Zhou, Yanmin and Qi, Yong and Cheng, Qian and He, Bin and Jiang, Shuo},
	urldate = {2024-10-26},
	date = {2023-11-24},
	eprinttype = {arxiv},
	eprint = {2311.14379},
	keywords = {Computer Science - Robotics},
}

@misc{brohan_rt-2_2023,
	title = {{RT}-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control},
	url = {http://arxiv.org/abs/2307.15818},
	doi = {10.48550/arXiv.2307.15818},
	shorttitle = {{RT}-2},
	abstract = {We study how vision-language models trained on Internet-scale data can be incorporated directly into end-to-end robotic control to boost generalization and enable emergent semantic reasoning. Our goal is to enable a single end-to-end trained model to both learn to map robot observations to actions and enjoy the benefits of large-scale pretraining on language and vision-language data from the web. To this end, we propose to co-fine-tune state-of-the-art vision-language models on both robotic trajectory data and Internet-scale vision-language tasks, such as visual question answering. In contrast to other approaches, we propose a simple, general recipe to achieve this goal: in order to fit both natural language responses and robotic actions into the same format, we express the actions as text tokens and incorporate them directly into the training set of the model in the same way as natural language tokens. We refer to such category of models as vision-language-action models ({VLA}) and instantiate an example of such a model, which we call {RT}-2. Our extensive evaluation (6k evaluation trials) shows that our approach leads to performant robotic policies and enables {RT}-2 to obtain a range of emergent capabilities from Internet-scale training. This includes significantly improved generalization to novel objects, the ability to interpret commands not present in the robot training data (such as placing an object onto a particular number or icon), and the ability to perform rudimentary reasoning in response to user commands (such as picking up the smallest or largest object, or the one closest to another object). We further show that incorporating chain of thought reasoning allows {RT}-2 to perform multi-stage semantic reasoning, for example figuring out which object to pick up for use as an improvised hammer (a rock), or which type of drink is best suited for someone who is tired (an energy drink).},
	number = {{arXiv}:2307.15818},
	publisher = {{arXiv}},
	author = {Brohan, Anthony and Brown, Noah and Carbajal, Justice and Chebotar, Yevgen and Chen, Xi and Choromanski, Krzysztof and Ding, Tianli and Driess, Danny and Dubey, Avinava and Finn, Chelsea and Florence, Pete and Fu, Chuyuan and Arenas, Montse Gonzalez and Gopalakrishnan, Keerthana and Han, Kehang and Hausman, Karol and Herzog, Alexander and Hsu, Jasmine and Ichter, Brian and Irpan, Alex and Joshi, Nikhil and Julian, Ryan and Kalashnikov, Dmitry and Kuang, Yuheng and Leal, Isabel and Lee, Lisa and Lee, Tsang-Wei Edward and Levine, Sergey and Lu, Yao and Michalewski, Henryk and Mordatch, Igor and Pertsch, Karl and Rao, Kanishka and Reymann, Krista and Ryoo, Michael and Salazar, Grecia and Sanketi, Pannag and Sermanet, Pierre and Singh, Jaspiar and Singh, Anikait and Soricut, Radu and Tran, Huong and Vanhoucke, Vincent and Vuong, Quan and Wahid, Ayzaan and Welker, Stefan and Wohlhart, Paul and Wu, Jialin and Xia, Fei and Xiao, Ted and Xu, Peng and Xu, Sichun and Yu, Tianhe and Zitkovich, Brianna},
	urldate = {2024-11-23},
	date = {2023-07-28},
	eprinttype = {arxiv},
	eprint = {2307.15818},
	keywords = {Computer Science - Machine Learning, Computer Science - Robotics, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/NUWY6WFH/Brohan et al. - 2023 - RT-2 Vision-Language-Action Models Transfer Web K.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/9L8DR6T4/2307.html:text/html},
}

@misc{chen_is_2024,
	title = {Is Bigger and Deeper Always Better? Probing {LLaMA} Across Scales and Layers},
	url = {http://arxiv.org/abs/2312.04333},
	doi = {10.48550/arXiv.2312.04333},
	shorttitle = {Is Bigger and Deeper Always Better?},
	abstract = {This paper presents an in-depth analysis of Large Language Models ({LLMs}), focusing on {LLaMA}, a prominent open-source foundational model in natural language processing. Instead of assessing {LLaMA} through its generative output, we design multiple-choice tasks to probe its intrinsic understanding in high-order tasks such as reasoning and computation. We examine the model horizontally, comparing different sizes, and vertically, assessing different layers. We unveil several key and uncommon findings based on the designed probing tasks: (1) Horizontally, enlarging model sizes almost could not automatically impart additional knowledge or computational prowess. Instead, it can enhance reasoning abilities, especially in math problem solving, and helps reduce hallucinations, but only beyond certain size thresholds; (2) In vertical analysis, the lower layers of {LLaMA} lack substantial arithmetic and factual knowledge, showcasing logical thinking, multilingual and recognitive abilities, with top layers housing most computational power and real-world knowledge.},
	number = {{arXiv}:2312.04333},
	publisher = {{arXiv}},
	author = {Chen, Nuo and Wu, Ning and Liang, Shining and Gong, Ming and Shou, Linjun and Zhang, Dongmei and Li, Jia},
	urldate = {2024-12-05},
	date = {2024-01-09},
	eprinttype = {arxiv},
	eprint = {2312.04333},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/4HWRGELD/Chen et al. - 2024 - Is Bigger and Deeper Always Better Probing LLaMA .pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/RMAELCDM/2312.html:text/html},
}

@misc{sumers_cognitive_2024,
	title = {Cognitive Architectures for Language Agents},
	url = {http://arxiv.org/abs/2309.02427},
	doi = {10.48550/arXiv.2309.02427},
	abstract = {Recent efforts have augmented large language models ({LLMs}) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents ({CoALA}). {CoALA} describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use {CoALA} to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, {CoALA} contextualizes today's language agents within the broader history of {AI} and outlines a path towards language-based general intelligence.},
	number = {{arXiv}:2309.02427},
	publisher = {{arXiv}},
	author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
	urldate = {2024-12-14},
	date = {2024-03-15},
	eprinttype = {arxiv},
	eprint = {2309.02427 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Symbolic Computation},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/VMT66Y9P/Sumers et al. - 2024 - Cognitive Architectures for Language Agents.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/XQ562SWW/2309.html:text/html},
}

@incollection{aldinhas_ferreira_overview_2019-1,
	location = {Cham},
	title = {An Overview of the Distributed Integrated Cognition Affect and Reflection {DIARC} Architecture},
	volume = {94},
	isbn = {978-3-319-97549-8 978-3-319-97550-4},
	url = {http://link.springer.com/10.1007/978-3-319-97550-4_11},
	abstract = {{DIARC} has been under development for over 15 years. Different from other cognitive architectures like {SOAR} or {ACT}-R, {DIARC} is an intrinsically component-based distributed architecture scheme that can be instantiated in many different ways. Moreover, {DIARC} has several distinguishing features, such as affect processing and deep natural language integration, is open-world and multi-agent enabled, and allows for “one-shot instruction-based learning” of new percepts, actions, concepts, rules, and norms.},
	pages = {165--193},
	booktitle = {Cognitive Architectures},
	publisher = {Springer International Publishing},
	author = {Scheutz, Matthias and Williams, Thomas and Krause, Evan and Oosterveld, Bradley and Sarathy, Vasanth and Frasca, Tyler},
	editor = {Aldinhas Ferreira, Maria Isabel and Silva Sequeira, João and Ventura, Rodrigo},
	urldate = {2023-12-19},
	date = {2019},
	langid = {english},
	doi = {10.1007/978-3-319-97550-4_11},
	note = {Series Title: Intelligent Systems, Control and Automation: Science and Engineering},
	file = {Scheutz et al. - 2019 - An Overview of the Distributed Integrated Cognitio.pdf:/Users/helenlu/Zotero/storage/V45ZT5WE/Scheutz et al. - 2019 - An Overview of the Distributed Integrated Cognitio.pdf:application/pdf},
}

@incollection{gudivada_chapter_2016,
	title = {Chapter 1 - Cognitive Computing: Concepts, Architectures, Systems, and Applications},
	volume = {35},
	url = {https://www.sciencedirect.com/science/article/pii/S0169716116300451},
	series = {Cognitive Computing: Theory and Applications},
	shorttitle = {Chapter 1 - Cognitive Computing},
	abstract = {Cognitive computing is an emerging field ushered in by the synergistic confluence of cognitive science, data science, and an array of computing technologies. Cognitive science theories provide frameworks to describe various models of human cognition including how information is represented and processed by the brain. Data science provides processes and systems to extract knowledge from both structured and unstructured data. Cognitive computing employs the computing discipline's theories, methods, and tools to model human cognition. The recent advances in data science and computing disciplines—neuromorphic processors, big data, predictive modeling, machine learning, natural language understanding, and cloud computing—are accelerating advances in cognitive science and cognitive computing. The overarching goal of this chapter is to provide an interdisciplinary introduction to cognitive computing. The focus is on breadth to provide a unified view of the discipline. The chapter begins with an overview of cognitive science, data science, and cognitive computing. The principal technology enablers of cognitive computing are presented next. An overview of three major categories of cognitive architectures is presented, which is followed by a description of cognitive computing systems and their applications. Trends and future research directions in cognitive computing are discussed. The chapter concludes by listing various cognitive computing resources.},
	pages = {3--38},
	booktitle = {Handbook of Statistics},
	publisher = {Elsevier},
	author = {Gudivada, V. N.},
	editor = {Gudivada, Venkat N. and Raghavan, Vijay V. and Govindaraju, Venu and Rao, C. R.},
	urldate = {2024-12-14},
	date = {2016-01-01},
	doi = {10.1016/bs.host.2016.07.004},
	keywords = {Cognitive applications, Cognitive architectures, Cognitive computing, Cognitive computing systems, Cognitive models, Cognitive systems, Data science},
	file = {ScienceDirect Snapshot:/Users/helenlu/Zotero/storage/JLWHADVE/S0169716116300451.html:text/html},
}

@misc{liu_libero_2023,
	title = {{LIBERO}: Benchmarking Knowledge Transfer for Lifelong Robot Learning},
	url = {http://arxiv.org/abs/2306.03310},
	doi = {10.48550/arXiv.2306.03310},
	shorttitle = {{LIBERO}},
	abstract = {Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making ({LLDM}) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in {LLDM}, we introduce {LIBERO}, a novel benchmark of lifelong learning for robot manipulation. Specifically, {LIBERO} highlights five key research topics in {LLDM}: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for {LLDM}; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for {LLDM}. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent {LLDM}. Check the website at https://libero-project.github.io for the code and the datasets.},
	number = {{arXiv}:2306.03310},
	publisher = {{arXiv}},
	author = {Liu, Bo and Zhu, Yifeng and Gao, Chongkai and Feng, Yihao and Liu, Qiang and Zhu, Yuke and Stone, Peter},
	urldate = {2024-12-15},
	date = {2023-10-14},
	eprinttype = {arxiv},
	eprint = {2306.03310 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/V7G8Q8CH/Liu et al. - 2023 - LIBERO Benchmarking Knowledge Transfer for Lifelo.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/F5XE7XPB/2306.html:text/html},
}

@misc{tao_probing_2024,
	title = {Probing Multimodal Large Language Models for Global and Local Semantic Representations},
	url = {http://arxiv.org/abs/2402.17304},
	doi = {10.48550/arXiv.2402.17304},
	abstract = {The advancement of Multimodal Large Language Models ({MLLMs}) has greatly accelerated the development of applications in understanding integrated texts and images. Recent works leverage image-caption datasets to train {MLLMs}, achieving state-of-the-art performance on image-to-text tasks. However, there are few studies exploring which layers of {MLLMs} make the most effort to the global image information, which plays vital roles in multimodal comprehension and generation. In this study, we find that the intermediate layers of models can encode more global semantic information, whose representation vectors perform better on visual-language entailment tasks, rather than the topmost layers. We further probe models regarding local semantic representations through object recognition tasks. We find that the topmost layers may excessively focus on local information, leading to a diminished ability to encode global information. Our code and data are released via https://github.com/kobayashikanna01/probing\_MLLM\_rep.},
	number = {{arXiv}:2402.17304},
	publisher = {{arXiv}},
	author = {Tao, Mingxu and Huang, Quzhe and Xu, Kun and Chen, Liwei and Feng, Yansong and Zhao, Dongyan},
	urldate = {2024-12-15},
	date = {2024-11-21},
	eprinttype = {arxiv},
	eprint = {2402.17304 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/7X7P7FM6/Tao et al. - 2024 - Probing Multimodal Large Language Models for Globa.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/WHGF2UEG/2402.html:text/html},
}

@inproceedings{schiappa_probing_2024,
	location = {Seattle, {WA}, {USA}},
	title = {Probing Conceptual Understanding of Large Visual-Language Models},
	rights = {https://doi.org/10.15223/policy-029},
	isbn = {9798350365474},
	url = {https://ieeexplore.ieee.org/document/10678310/},
	doi = {10.1109/CVPRW63382.2024.00186},
	abstract = {{BridgeTower} and {ViLT}’s performance indicates that co-attention is a method that can improve relational understanding. (1) This would indicate that both modality specific attention and co-attention simultaneously improves relational understanding. (2) When the predicate is swapped to something that violates expectation, the drop in confidence, regardless of accuracy, indicates that their performance may not be due to an underlying conceptual map. (3) When the subject is swapped, all models show better performance compared to predicate swapping, indicating they are focusing on objects less-so than their relations to each other.},
	eventtitle = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	pages = {1797--1807},
	booktitle = {2024 {IEEE}/{CVF} Conference on Computer Vision and Pattern Recognition Workshops ({CVPRW})},
	publisher = {{IEEE}},
	author = {Schiappa, Madeline and Abdullah, Raiyaan and Azad, Shehreen and Claypoole, Jared and Cogswell, Michael and Divakaran, Ajay and Rawat, Yogesh},
	urldate = {2024-12-15},
	date = {2024-06-17},
	langid = {english},
	file = {Schiappa et al. - 2024 - Probing Conceptual Understanding of Large Visual-L.pdf:/Users/helenlu/Zotero/storage/UT639SFS/Schiappa et al. - 2024 - Probing Conceptual Understanding of Large Visual-L.pdf:application/pdf},
}

@misc{sepehri_mediconfusion_2024,
	title = {{MediConfusion}: Can you trust your {AI} radiologist? Probing the reliability of multimodal medical foundation models},
	url = {http://arxiv.org/abs/2409.15477},
	doi = {10.48550/arXiv.2409.15477},
	shorttitle = {{MediConfusion}},
	abstract = {Multimodal Large Language Models ({MLLMs}) have tremendous potential to improve the accuracy, availability, and cost-effectiveness of healthcare by providing automated solutions or serving as aids to medical professionals. Despite promising first steps in developing medical {MLLMs} in the past few years, their capabilities and limitations are not well-understood. Recently, many benchmark datasets have been proposed that test the general medical knowledge of such models across a variety of medical areas. However, the systematic failure modes and vulnerabilities of such models are severely underexplored with most medical benchmarks failing to expose the shortcomings of existing models in this safety-critical domain. In this paper, we introduce {MediConfusion}, a challenging medical Visual Question Answering ({VQA}) benchmark dataset, that probes the failure modes of medical {MLLMs} from a vision perspective. We reveal that state-of-the-art models are easily confused by image pairs that are otherwise visually dissimilar and clearly distinct for medical experts. Strikingly, all available models (open-source or proprietary) achieve performance below random guessing on {MediConfusion}, raising serious concerns about the reliability of existing medical {MLLMs} for healthcare deployment. We also extract common patterns of model failure that may help the design of a new generation of more trustworthy and reliable {MLLMs} in healthcare.},
	number = {{arXiv}:2409.15477},
	publisher = {{arXiv}},
	author = {Sepehri, Mohammad Shahab and Fabian, Zalan and Soltanolkotabi, Maryam and Soltanolkotabi, Mahdi},
	urldate = {2024-12-15},
	date = {2024-09-23},
	eprinttype = {arxiv},
	eprint = {2409.15477 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/YDK67QC3/Sepehri et al. - 2024 - MediConfusion Can you trust your AI radiologist .pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/3K42YUAL/2409.html:text/html},
}

@misc{bommasani_opportunities_2022,
	title = {On the Opportunities and Risks of Foundation Models},
	url = {http://arxiv.org/abs/2108.07258},
	doi = {10.48550/arXiv.2108.07258},
	abstract = {{AI} is undergoing a paradigm shift with the rise of models (e.g., {BERT}, {DALL}-E, {GPT}-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	number = {{arXiv}:2108.07258},
	publisher = {{arXiv}},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and Arx, Sydney von and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	urldate = {2024-12-15},
	date = {2022-07-12},
	eprinttype = {arxiv},
	eprint = {2108.07258 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computers and Society, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/YUUT4PRM/Bommasani et al. - 2022 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/4AQ7XWQT/2108.html:text/html},
}

@article{qi_what_2023,
	title = {What is the limitation of multimodal {LLMs}? A deeper look into multimodal {LLMs} through prompt probing},
	volume = {60},
	issn = {0306-4573},
	url = {https://www.sciencedirect.com/science/article/pii/S0306457323002479},
	doi = {10.1016/j.ipm.2023.103510},
	shorttitle = {What is the limitation of multimodal {LLMs}?},
	abstract = {Large language models ({LLMs}) are believed to contain vast knowledge. Many works have extended {LLMs} to multimodal models and applied them to various multimodal downstream tasks with a unified model structure using prompt. Appropriate prompts can stimulate the knowledge capabilities of the model to solve different tasks. However, how the content of the prompts affects the model’s understanding of the information is still under-explored in the literature. We fill this gap by offering a systematic study on prompt probing for multimodal {LLMs}, examining various factors for their understanding of prompts. To achieve this goal, we propose a novel prompt probing framework that starts with the input and designs three types of input change strategies as templates for probing: visual prompt, text prompt and extra knowledge prompt. Our extensive experiments on the {VQA} dataset show that existing multimodal {LLMs} do not understand the input content but more simply fit the training data distribution. Current multimodal models are still very far from understanding prompts properly.},
	pages = {103510},
	number = {6},
	journaltitle = {Information Processing \& Management},
	shortjournal = {Information Processing \& Management},
	author = {Qi, Shuhan and Cao, Zhengying and Rao, Jun and Wang, Lei and Xiao, Jing and Wang, Xuan},
	urldate = {2024-12-15},
	date = {2023-11-01},
	keywords = {Large language model, Model probing, {MultiModal} {LLMs}, Prompt learning, Visual question answer, Zero-shot/few-shot},
	file = {ScienceDirect Snapshot:/Users/helenlu/Zotero/storage/X8MPEJPF/S0306457323002479.html:text/html},
}

@misc{alivanistos_prompting_2023,
	title = {Prompting as Probing: Using Language Models for Knowledge Base Construction},
	url = {http://arxiv.org/abs/2208.11057},
	doi = {10.48550/arXiv.2208.11057},
	shorttitle = {Prompting as Probing},
	abstract = {Language Models ({LMs}) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. {LMs} are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present {ProP} (Prompting as Probing), which utilizes {GPT}-3, a large Language Model originally proposed by {OpenAI} in 2020, to perform the task of Knowledge Base Construction ({KBC}). {ProP} implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the {LM} must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the {LM}, that the size of the {LM} is a crucial factor, and that a dictionary of entity aliases improves the {LM} score. Our evaluation study indicates that these proposed techniques can substantially enhance the quality of the final predictions: {ProP} won track 2 of the {LM}-{KBC} competition, outperforming the baseline by 36.4 percentage points. Our implementation is available on https://github.com/{HEmile}/iswc-challenge.},
	number = {{arXiv}:2208.11057},
	publisher = {{arXiv}},
	author = {Alivanistos, Dimitrios and Santamaría, Selene Báez and Cochez, Michael and Kalo, Jan-Christoph and Krieken, Emile van and Thanapalasingam, Thiviyan},
	urldate = {2024-12-15},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2208.11057 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/6Y6D4THG/Alivanistos et al. - 2023 - Prompting as Probing Using Language Models for Kn.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/3M9GMBEI/2208.html:text/html},
}

@inproceedings{cao_can_2022,
	location = {Dublin, Ireland},
	title = {Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View},
	url = {https://aclanthology.org/2022.acl-long.398},
	doi = {10.18653/v1/2022.acl-long.398},
	shorttitle = {Can Prompt Probe Pretrained Language Models?},
	abstract = {Prompt-based probing has been widely used in evaluating the abilities of pretrained language models ({PLMs}). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying {PLMs} in real-world applications. To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention. This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models. Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.},
	eventtitle = {{ACL} 2022},
	pages = {5796--5808},
	booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
	publisher = {Association for Computational Linguistics},
	author = {Cao, Boxi and Lin, Hongyu and Han, Xianpei and Liu, Fangchao and Sun, Le},
	editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
	urldate = {2024-12-15},
	date = {2022-05},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/62ZR48JM/Cao et al. - 2022 - Can Prompt Probe Pretrained Language Models Under.pdf:application/pdf},
}

@inproceedings{wang_readprompt_2023,
	location = {Singapore},
	title = {{ReadPrompt}: A Readable Prompting Method for Reliable Knowledge Probing},
	url = {https://aclanthology.org/2023.findings-emnlp.501},
	doi = {10.18653/v1/2023.findings-emnlp.501},
	shorttitle = {{ReadPrompt}},
	eventtitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	pages = {7468--7479},
	booktitle = {Findings of the Association for Computational Linguistics: {EMNLP} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Zezhong and Ye, Luyao and Wang, Hongru and Kwan, Wai-Chung and Ho, David and Wong, Kam-Fai},
	urldate = {2024-12-15},
	date = {2023},
	langid = {english},
	file = {Wang et al. - 2023 - ReadPrompt A Readable Prompting Method for Reliab.pdf:/Users/helenlu/Zotero/storage/MDGKY9Y4/Wang et al. - 2023 - ReadPrompt A Readable Prompting Method for Reliab.pdf:application/pdf},
}

@misc{alampara_probing_2024,
	title = {Probing the limitations of multimodal language models for chemistry and materials research},
	url = {http://arxiv.org/abs/2411.16955},
	doi = {10.48550/arXiv.2411.16955},
	abstract = {Recent advancements in artificial intelligence have sparked interest in scientific assistants that could support researchers across the full spectrum of scientific workflows, from literature review to experimental design and data analysis. A key capability for such systems is the ability to process and reason about scientific information in both visual and textual forms - from interpreting spectroscopic data to understanding laboratory setups. Here, we introduce {MaCBench}, a comprehensive benchmark for evaluating how vision-language models handle real-world chemistry and materials science tasks across three core aspects: data extraction, experimental understanding, and results interpretation. Through a systematic evaluation of leading models, we find that while these systems show promising capabilities in basic perception tasks - achieving near-perfect performance in equipment identification and standardized data extraction - they exhibit fundamental limitations in spatial reasoning, cross-modal information synthesis, and multi-step logical inference. Our insights have important implications beyond chemistry and materials science, suggesting that developing reliable multimodal {AI} scientific assistants may require advances in curating suitable training data and approaches to training those models.},
	number = {{arXiv}:2411.16955},
	publisher = {{arXiv}},
	author = {Alampara, Nawaf and Schilling-Wilhelmi, Mara and Ríos-García, Martiño and Mandal, Indrajeet and Khetarpal, Pranav and Grover, Hargun Singh and Krishnan, N. M. Anoop and Jablonka, Kevin Maik},
	urldate = {2024-12-15},
	date = {2024-11-25},
	eprinttype = {arxiv},
	eprint = {2411.16955 [cs]},
	keywords = {Computer Science - Machine Learning, Condensed Matter - Materials Science},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/LQ3P82XA/Alampara et al. - 2024 - Probing the limitations of multimodal language mod.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/68WL4CAL/2411.html:text/html},
}

@misc{petroni_language_2019,
	title = {Language Models as Knowledge Bases?},
	url = {http://arxiv.org/abs/1909.01066},
	doi = {10.48550/arXiv.1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream {NLP} tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, {BERT} contains relational knowledge competitive with traditional {NLP} methods that have some access to oracle knowledge, (ii) {BERT} also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain {QA} systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/{LAMA}.},
	number = {{arXiv}:1909.01066},
	publisher = {{arXiv}},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	urldate = {2024-12-15},
	date = {2019-09-04},
	eprinttype = {arxiv},
	eprint = {1909.01066 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/2E6A5S5M/Petroni et al. - 2019 - Language Models as Knowledge Bases.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/J8ZCR5M3/1909.html:text/html},
}

@misc{touvron_llama_2023,
	title = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
	url = {http://arxiv.org/abs/2307.09288},
	doi = {10.48550/arXiv.2307.09288},
	shorttitle = {Llama 2},
	abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models ({LLMs}) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned {LLMs}, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of {LLMs}.},
	number = {{arXiv}:2307.09288},
	publisher = {{arXiv}},
	author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
	urldate = {2024-12-15},
	date = {2023-07-19},
	eprinttype = {arxiv},
	eprint = {2307.09288 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/3K74GTKH/Touvron et al. - 2023 - Llama 2 Open Foundation and Fine-Tuned Chat Model.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/GYRFTVDD/2307.html:text/html},
}

@misc{zhai_sigmoid_2023,
	title = {Sigmoid Loss for Language Image Pre-Training},
	url = {http://arxiv.org/abs/2303.15343},
	doi = {10.48550/arXiv.2303.15343},
	abstract = {We propose a simple pairwise Sigmoid loss for Language-Image Pre-training ({SigLIP}). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four {TPUv}4 chips, we train a {SigLiT} model that achieves 84.5\% {ImageNet} zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at https://github.com/google-research/big\_vision and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.},
	number = {{arXiv}:2303.15343},
	publisher = {{arXiv}},
	author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
	urldate = {2024-12-16},
	date = {2023-09-27},
	eprinttype = {arxiv},
	eprint = {2303.15343 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/9427WZVT/Zhai et al. - 2023 - Sigmoid Loss for Language Image Pre-Training.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/6ARLKEJG/2303.html:text/html},
}

@misc{oquab_dinov2_2024,
	title = {{DINOv}2: Learning Robust Visual Features without Supervision},
	url = {http://arxiv.org/abs/2304.07193},
	doi = {10.48550/arXiv.2304.07193},
	shorttitle = {{DINOv}2},
	abstract = {The recent breakthroughs in natural language processing for model pretraining on large quantities of data have opened the way for similar foundation models in computer vision. These models could greatly simplify the use of images in any system by producing all-purpose visual features, i.e., features that work across image distributions and tasks without finetuning. This work shows that existing pretraining methods, especially self-supervised methods, can produce such features if trained on enough curated data from diverse sources. We revisit existing approaches and combine different techniques to scale our pretraining in terms of data and model size. Most of the technical contributions aim at accelerating and stabilizing the training at scale. In terms of data, we propose an automatic pipeline to build a dedicated, diverse, and curated image dataset instead of uncurated data, as typically done in the self-supervised literature. In terms of models, we train a {ViT} model (Dosovitskiy et al., 2020) with 1B parameters and distill it into a series of smaller models that surpass the best available all-purpose features, {OpenCLIP} (Ilharco et al., 2021) on most of the benchmarks at image and pixel levels.},
	number = {{arXiv}:2304.07193},
	publisher = {{arXiv}},
	author = {Oquab, Maxime and Darcet, Timothée and Moutakanni, Théo and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat, Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu and Jegou, Hervé and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
	urldate = {2024-12-16},
	date = {2024-02-02},
	eprinttype = {arxiv},
	eprint = {2304.07193 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Full Text PDF:/Users/helenlu/Zotero/storage/U35FQXGZ/Oquab et al. - 2024 - DINOv2 Learning Robust Visual Features without Su.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/X3AMIXBU/2304.html:text/html},
}

@misc{wang_large_2024,
	title = {Large Language Models for Robotics: Opportunities, Challenges, and Perspectives},
	url = {http://arxiv.org/abs/2401.04334},
	doi = {10.48550/arXiv.2401.04334},
	shorttitle = {Large Language Models for Robotics},
	abstract = {Large language models ({LLMs}) have undergone significant expansion and have been increasingly integrated across various domains. Notably, in the realm of robot task planning, {LLMs} harness their advanced reasoning and language comprehension capabilities to formulate precise and efficient action plans based on natural language instructions. However, for embodied tasks, where robots interact with complex environments, text-only {LLMs} often face challenges due to a lack of compatibility with robotic visual perception. This study provides a comprehensive overview of the emerging integration of {LLMs} and multimodal {LLMs} into various robotic tasks. Additionally, we propose a framework that utilizes multimodal {GPT}-4V to enhance embodied task planning through the combination of natural language instructions and robot visual perceptions. Our results, based on diverse datasets, indicate that {GPT}-4V effectively enhances robot performance in embodied tasks. This extensive survey and evaluation of {LLMs} and multimodal {LLMs} across a variety of robotic tasks enriches the understanding of {LLM}-centric embodied intelligence and provides forward-looking insights toward bridging the gap in Human-Robot-Environment interaction.},
	number = {{arXiv}:2401.04334},
	publisher = {{arXiv}},
	author = {Wang, Jiaqi and Wu, Zihao and Li, Yiwei and Jiang, Hanqi and Shu, Peng and Shi, Enze and Hu, Huawen and Ma, Chong and Liu, Yiheng and Wang, Xuhui and Yao, Yincheng and Liu, Xuan and Zhao, Huaqin and Liu, Zhengliang and Dai, Haixing and Zhao, Lin and Ge, Bao and Li, Xiang and Liu, Tianming and Zhang, Shu},
	urldate = {2024-12-17},
	date = {2024-01-09},
	eprinttype = {arxiv},
	eprint = {2401.04334 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/58U4YDW9/Wang et al. - 2024 - Large Language Models for Robotics Opportunities,.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/ZMWZIPDK/2401.html:text/html},
}

@misc{wei_chain--thought_2023,
	title = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
	url = {http://arxiv.org/abs/2201.11903},
	doi = {10.48550/arXiv.2201.11903},
	abstract = {We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the {GSM}8K benchmark of math word problems, surpassing even finetuned {GPT}-3 with a verifier.},
	number = {{arXiv}:2201.11903},
	publisher = {{arXiv}},
	author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Ichter, Brian and Xia, Fei and Chi, Ed and Le, Quoc and Zhou, Denny},
	urldate = {2024-12-17},
	date = {2023-01-10},
	eprinttype = {arxiv},
	eprint = {2201.11903 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/I5DNICE2/Wei et al. - 2023 - Chain-of-Thought Prompting Elicits Reasoning in La.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/JI5JHNRR/2201.html:text/html},
}

@misc{wang_self-consistency_2023,
	title = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
	url = {http://arxiv.org/abs/2203.11171},
	doi = {10.48550/arXiv.2203.11171},
	abstract = {Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including {GSM}8K (+17.9\%), {SVAMP} (+11.0\%), {AQuA} (+12.2\%), {StrategyQA} (+6.4\%) and {ARC}-challenge (+3.9\%).},
	number = {{arXiv}:2203.11171},
	publisher = {{arXiv}},
	author = {Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
	urldate = {2024-12-17},
	date = {2023-03-07},
	eprinttype = {arxiv},
	eprint = {2203.11171 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/TY9KZSYV/Wang et al. - 2023 - Self-Consistency Improves Chain of Thought Reasoni.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/RLBLVL3Q/2203.html:text/html},
}

@misc{xu_creative_2023,
	title = {Creative Robot Tool Use with Large Language Models},
	url = {http://arxiv.org/abs/2310.13065},
	doi = {10.48550/arXiv.2310.13065},
	abstract = {Tool use is a hallmark of advanced intelligence, exemplified in both animal behavior and robotic capabilities. This paper investigates the feasibility of imbuing robots with the ability to creatively use tools in tasks that involve implicit physical constraints and long-term planning. Leveraging Large Language Models ({LLMs}), we develop {RoboTool}, a system that accepts natural language instructions and outputs executable code for controlling robots in both simulated and real-world environments. {RoboTool} incorporates four pivotal components: (i) an "Analyzer" that interprets natural language to discern key task-related concepts, (ii) a "Planner" that generates comprehensive strategies based on the language input and key concepts, (iii) a "Calculator" that computes parameters for each skill, and (iv) a "Coder" that translates these plans into executable Python code. Our results show that {RoboTool} can not only comprehend explicit or implicit physical constraints and environmental factors but also demonstrate creative tool use. Unlike traditional Task and Motion Planning ({TAMP}) methods that rely on explicit optimization, our {LLM}-based system offers a more flexible, efficient, and user-friendly solution for complex robotics tasks. Through extensive experiments, we validate that {RoboTool} is proficient in handling tasks that would otherwise be infeasible without the creative use of tools, thereby expanding the capabilities of robotic systems. Demos are available on our project page: https://creative-robotool.github.io/.},
	number = {{arXiv}:2310.13065},
	publisher = {{arXiv}},
	author = {Xu, Mengdi and Huang, Peide and Yu, Wenhao and Liu, Shiqi and Zhang, Xilun and Niu, Yaru and Zhang, Tingnan and Xia, Fei and Tan, Jie and Zhao, Ding},
	urldate = {2024-12-17},
	date = {2023-10-19},
	eprinttype = {arxiv},
	eprint = {2310.13065 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/UYVAABDU/Xu et al. - 2023 - Creative Robot Tool Use with Large Language Models.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/JVJND2GW/2310.html:text/html},
}

@misc{shukla_lgts_2023,
	title = {{LgTS}: Dynamic Task Sampling using {LLM}-generated sub-goals for Reinforcement Learning Agents},
	url = {http://arxiv.org/abs/2310.09454},
	doi = {10.48550/arXiv.2310.09454},
	shorttitle = {{LgTS}},
	abstract = {Recent advancements in reasoning abilities of Large Language Models ({LLM}) has promoted their usage in problems that require high-level planning for robots and artificial agents. However, current techniques that utilize {LLMs} for such planning tasks make certain key assumptions such as, access to datasets that permit finetuning, meticulously engineered prompts that only provide relevant and essential information to the {LLM}, and most importantly, a deterministic approach to allow execution of the {LLM} responses either in the form of existing policies or plan operators. In this work, we propose {LgTS} ({LLM}-guided Teacher-Student learning), a novel approach that explores the planning abilities of {LLMs} to provide a graphical representation of the sub-goals to a reinforcement learning ({RL}) agent that does not have access to the transition dynamics of the environment. The {RL} agent uses Teacher-Student learning algorithm to learn a set of successful policies for reaching the goal state from the start state while simultaneously minimizing the number of environmental interactions. Unlike previous methods that utilize {LLMs}, our approach does not assume access to a propreitary or a fine-tuned {LLM}, nor does it require pre-trained policies that achieve the sub-goals proposed by the {LLM}. Through experiments on a gridworld based {DoorKey} domain and a search-and-rescue inspired domain, we show that generating a graphical structure of sub-goals helps in learning policies for the {LLM} proposed sub-goals and the Teacher-Student learning algorithm minimizes the number of environment interactions when the transition dynamics are unknown.},
	number = {{arXiv}:2310.09454},
	publisher = {{arXiv}},
	author = {Shukla, Yash and Gao, Wenchang and Sarathy, Vasanth and Velasquez, Alvaro and Wright, Robert and Sinapov, Jivko},
	urldate = {2024-12-17},
	date = {2023-10-14},
	eprinttype = {arxiv},
	eprint = {2310.09454 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/helenlu/Zotero/storage/2ZUGTKQE/Shukla et al. - 2023 - LgTS Dynamic Task Sampling using LLM-generated su.pdf:application/pdf;Snapshot:/Users/helenlu/Zotero/storage/R9SCGXI6/2310.html:text/html},
}
