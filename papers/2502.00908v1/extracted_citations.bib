@inproceedings{congdon_merging_2018,
  address =       {Tokyo Japan},
  author =        {Congdon, Ben J. and Wang, Tuanfeng and
                   Steed, Anthony},
  booktitle =     {Proceedings of the 24th {ACM} {Symposium} on
                   {Virtual} {Reality} {Software} and {Technology}},
  month =         nov,
  pages =         {1--8},
  publisher =     {ACM},
  title =         {Merging environments for shared spaces in mixed
                   reality},
  year =          {2018},
  abstract =      {In virtual reality a real walking interface limits
                   the extent of a virtual environment to our local
                   walkable space. As local spaces are specific to each
                   user, sharing a virtual environment with others for
                   collaborative work or games becomes complicated. It
                   is not clear which user’s walkable space to prefer,
                   or whether that space will be navigable for both
                   users.},
  doi =           {10.1145/3281505.3281544},
  isbn =          {978-1-4503-6086-9},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3281505.3281544},
}

@article{ens_revisiting_2019,
  author =        {Ens, Barrett and Lanir, Joel and Tang, Anthony and
                   Bateman, Scott and Lee, Gun and
                   Piumsomboon, Thammathip and Billinghurst, Mark},
  journal =       {International Journal of Human-Computer Studies},
  month =         nov,
  pages =         {81--98},
  title =         {Revisiting collaboration through mixed reality: {The}
                   evolution of groupware},
  volume =        {131},
  year =          {2019},
  abstract =      {Collaborative Mixed Reality (MR) systems are at a
                   critical point in time as they are soon to become
                   more commonplace. However, MR technology has only
                   recently matured to the point where researchers can
                   focus deeply on the nuances of supporting
                   collaboration, rather than needing to focus on
                   creating the enabling technology. In parallel, but
                   largely independently, the ﬁeld of Computer
                   Supported Cooperative Work (CSCW) has focused on the
                   fundamental concerns that underlie human
                   communication and collaboration over the past 30-plus
                   years. Since MR research is now on the brink of
                   moving into the real world, we reﬂect on three
                   decades of collaborative MR research and try to
                   reconcile it with existing theory from CSCW, to help
                   position MR researchers to pursue fruitful directions
                   for their work. To do this, we review the history of
                   collaborative MR systems, investigating how the
                   common taxonomies and frameworks in CSCW and MR
                   research can be applied to existing work on
                   collaborative MR systems, exploring where they have
                   fallen behind, and look for new ways to describe
                   current trends. Through identifying emergent trends,
                   we suggest future directions for MR, and also ﬁnd
                   where CSCW researchers can explore new theory that
                   more fully represents the future of working, playing
                   and being with others.},
  doi =           {10.1016/j.ijhcs.2019.05.011},
  issn =          {10715819},
  language =      {en},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                  S1071581919300606},
}

@article{fink_re-locations_2022,
  author =        {Fink, Daniel Immanuel and Zagermann, Johannes and
                   Reiterer, Harald and Jetter, Hans-Christian},
  journal =       {Proceedings of the ACM on Human-Computer Interaction},
  month =         nov,
  number =        {ISS},
  pages =         {1--30},
  title =         {Re-locations: {Augmenting} {Personal} and {Shared}
                   {Workspaces} to {Support} {Remote} {Collaboration} in
                   {Incongruent} {Spaces}},
  volume =        {6},
  year =          {2022},
  abstract =      {Augmented reality (AR) can create the illusion of
                   being virtually co-located during remote
                   collaboration, e.g., by visualizing remote co-workers
                   as avatars. However, spatial awareness of each
                   other's activities is limited as physical spaces,
                   including the position of physical devices, are often
                   incongruent. Therefore, alignment methods are needed
                   to support activities on physical devices. In this
                   paper, we present the concept of Re-locations, a
                   method for enabling remote collaboration with
                   augmented reality in incongruent spaces. The idea of
                   the concept is to enrich remote collaboration
                   activities on multiple physical devices with
                   attributes of co-located collaboration such as
                   spatial awareness and spatial referencing by locally
                   relocating remote user representations to
                   user-defined workspaces. We evaluated the
                   Re-locations concept in an explorative user study
                   with dyads using an authentic, collaborative task.
                   Our findings indicate that Re-locations introduce
                   attributes of co-located collaboration like spatial
                   awareness and social presence. Based on our findings,
                   we provide implications for future research and
                   design of remote collaboration systems using AR.},
  doi =           {10.1145/3567709},
  issn =          {2573-0142},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3567709},
}

@inproceedings{gronbaek_partially_2023,
  address =       {Hamburg Germany},
  author =        {Grønbæk, Jens Emil Sloth and Pfeuffer, Ken and
                   Velloso, Eduardo and Astrup, Morten and
                   Pedersen, Melanie Isabel Sønderkær and
                   Kjær, Martin and Leiva, Germán and Gellersen, Hans},
  booktitle =     {Proceedings of the 2023 {CHI} {Conference} on {Human}
                   {Factors} in {Computing} {Systems}},
  month =         apr,
  pages =         {1--16},
  publisher =     {ACM},
  title =         {Partially {Blended} {Realities}: {Aligning}
                   {Dissimilar} {Spaces} for {Distributed} {Mixed}
                   {Reality} {Meetings}},
  year =          {2023},
  doi =           {10.1145/3544548.3581515},
  isbn =          {978-1-4503-9421-5},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3544548.3581515},
}

@article{herskovitz_xspace_2022,
  author =        {Herskovitz, Jaylin and Cheng, Yi Fei and Guo, Anhong and
                   Sample, Alanson P. and Nebeling, Michael},
  journal =       {Proceedings of the ACM on Human-Computer Interaction},
  month =         nov,
  number =        {ISS},
  pages =         {277--302},
  title =         {{XSpace}: {An} {Augmented} {Reality} {Toolkit} for
                   {Enabling} {Spatially}-{Aware} {Distributed}
                   {Collaboration}},
  volume =        {6},
  year =          {2022},
  abstract =      {Augmented Reality (AR) has the potential to leverage
                   environmental information to better facilitate
                   distributed collaboration, however, such applications
                   are difficult to develop. We present XSpace, a
                   toolkit for creating spatially-aware AR applications
                   for distributed collaboration. Based on a review of
                   existing applications and developer tools, we design
                   XSpace to support three methods for creating shared
                   virtual spaces, each emphasizing a different aspect:
                   shared objects, user perspectives, and environmental
                   meshes. XSpace implements these methods in a
                   developer toolkit, and also provides a set of
                   complimentary visual authoring tools to allow
                   developers to preview a variety of configurations for
                   a shared virtual space. We present five example
                   applications to illustrate that XSpace can support
                   the development of a rich set of collaborative AR
                   experiences that are difficult to produce with
                   current solutions. Through XSpace, we discuss
                   implications for future application design, including
                   user space customization and privacy and safety
                   concerns when sharing users’ environments. CCS
                   Concepts: • Human-centered computing → Interface
                   design prototyping; Mixed / augmented reality.},
  doi =           {10.1145/3567721},
  issn =          {2573-0142},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3567721},
}

@article{huang_surfshare_2023,
  author =        {Huang, Xincheng and Xiao, Robert},
  journal =       {Proceedings of the ACM on Interactive, Mobile,
                   Wearable and Ubiquitous Technologies},
  month =         dec,
  number =        {4},
  pages =         {1--24},
  title =         {{SurfShare}: {Lightweight} {Spatially} {Consistent}
                   {Physical} {Surface} and {Virtual} {Replica}
                   {Sharing} with {Head}-mounted {Mixed}-{Reality}},
  volume =        {7},
  year =          {2023},
  abstract =      {Shared Mixed Reality experiences allow two co-located
                   users to collaborate on both physical and digital
                   tasks with familiar social protocols. However,
                   extending the same to remote collaboration is limited
                   by cumbersome setups for aligning distinct physical
                   environments and the lack of access to remote
                   physical artifacts. We present SurfShare, a
                   general-purpose symmetric remote collaboration system
                   with mixed-reality head-mounted displays (HMDs). Our
                   system shares a spatially consistent physical-virtual
                   workspace between two remote users, anchored on a
                   physical plane in each environment (e.g., a desk or
                   wall). The video feed of each user’s physical
                   surface is overlaid virtually on the other side,
                   creating a shared view of the physical space. We
                   integrate the physical and virtual workspace through
                   virtual replication. Users can transmute physical
                   objects to the virtual space as virtual replicas. Our
                   system is lightweight, implemented using only the
                   capabilities of the headset, without requiring any
                   modifications to the environment (e.g. cameras or
                   motion tracking hardware). We discuss the design,
                   implementation, and interaction capabilities of our
                   prototype, and demonstrate the utility of SurfShare
                   through four example applications. In a user
                   experiment with a comprehensive prototyping task, we
                   found that SurfShare provides a physical-virtual
                   workspace that supports low-fi prototyping with
                   flexible proxemics and fluid collaboration dynamics.
                   CCS Concepts: • Human-centered computing → Mixed
                   / augmented reality.},
  doi =           {10.1145/3631418},
  issn =          {2474-9567},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3631418},
}

@inproceedings{kang_real-time_2023,
  address =       {Sydney, Australia},
  author =        {Kang, Jiho and Yang, Dongseok and Kim, Taehei and
                   Lee, Yewon and Lee, Sung-Hee},
  booktitle =     {2023 {IEEE} {International} {Symposium} on {Mixed}
                   and {Augmented} {Reality} ({ISMAR})},
  month =         oct,
  pages =         {885--893},
  publisher =     {IEEE},
  title =         {Real-time {Retargeting} of {Deictic} {Motion} to
                   {Virtual} {Avatars} for {Augmented} {Reality}
                   {Telepresence}},
  year =          {2023},
  doi =           {10.1109/ISMAR59233.2023.00104},
  isbn =          {9798350328387},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10316476/},
}

@inproceedings{keshavarzi_optimization_2020,
  author =        {Keshavarzi, Mohammad and Yang, Allen Y. and
                   Ko, Woojin and Caldas, Luisa},
  booktitle =     {2020 {IEEE} {Conference} on {Virtual} {Reality} and
                   {3D} {User} {Interfaces} ({VR})},
  month =         mar,
  note =          {arXiv:1910.05998 [cs]},
  pages =         {353--362},
  title =         {Optimization and {Manipulation} of {Contextual}
                   {Mutual} {Spaces} for {Multi}-{User} {Virtual} and
                   {Augmented} {Reality} {Interaction}},
  year =          {2020},
  abstract =      {Spatial computing experiences are physically
                   constrained by the geometry and semantics of the
                   local user environment. This limitation is elevated
                   in remote multi-user interaction scenarios, where
                   ﬁnding a common virtual ground physically
                   accessible for all participants becomes challenging.
                   Locating a common accessible virtual ground is
                   difﬁcult for the users themselves, particularly if
                   they are not aware of the spatial properties of other
                   participants. In this paper, we introduce a framework
                   to generate an optimal mutual virtual space for a
                   multi-user interaction setting where remote users’
                   room spaces can have different layout and sizes. The
                   framework further recommends movement of surrounding
                   furniture objects that expand the size of the mutual
                   space with minimal physical effort. Finally, we
                   demonstrate the performance of our solution on
                   real-world datasets and also a real HoloLens
                   application. Results show the proposed algorithm can
                   effectively discover optimal shareable space for
                   multiuser virtual interaction and hence facilitate
                   remote spatial computing communication in various
                   collaborative workﬂows.},
  doi =           {10.1109/VR46266.2020.00055},
  language =      {en},
  url =           {http://arxiv.org/abs/1910.05998},
}

@inproceedings{kim_adjusting_2021,
  address =       {Lisboa, Portugal},
  author =        {Kim, Dooyoung and Shin, Jae-eun and Lee, Jeongmi and
                   Woo, Woontack},
  booktitle =     {2021 {IEEE} {Virtual} {Reality} and {3D} {User}
                   {Interfaces} ({VR})},
  month =         mar,
  pages =         {653--660},
  publisher =     {IEEE},
  title =         {Adjusting {Relative} {Translation} {Gains}
                   {According} to {Space} {Size} in {Redirected}
                   {Walking} for {Mixed} {Reality} {Mutual} {Space}
                   {Generation}},
  year =          {2021},
  abstract =      {We propose the concept of relative translation gains,
                   a novel Redirected Walking (RDW) method to create a
                   mutual movable space between the Augmented Reality
                   (AR) host’s reference space and the Virtual Reality
                   (VR) client’s space. Previous RDW methods have
                   focused on maximizing the movable space at the
                   expense of aligning the coordinates between the AR
                   and VR side, and could only be applied to
                   collaborative scenarios involving sequential tasks.
                   Our method solves these problems by adjusting the
                   remote client’s walking speed for each axis of a VR
                   space to modify the movable area without coordinate
                   distortion. We estimate the relative translation gain
                   threshold, deﬁned as the extent to which the
                   walking speed can be altered without creating a
                   perceived difference in distance. In order to
                   reﬂect features of the reference space in
                   generating the mutual space, we then examine how
                   changing its size affects the threshold value. Our
                   study showed that for remote clients connected to the
                   larger reference space, relative translation gains
                   can be increased to utilize a VR space bigger than
                   their real space. Our method can be applied to create
                   optimal mutual spaces for a wider variety of
                   asymmetric Mixed Reality (MR) remote collaboration
                   systems.},
  doi =           {10.1109/VR50410.2021.00091},
  isbn =          {978-1-66541-838-6},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/9417781/},
}

@inproceedings{kim_object_2024,
  address =       {Orlando, FL, USA},
  author =        {Kim, Seonji and Kim, Dooyoung and Shin, Jae-Eun and
                   Woo, Woontack},
  booktitle =     {2024 {IEEE} {Conference} {Virtual} {Reality} and {3D}
                   {User} {Interfaces} ({VR})},
  month =         mar,
  pages =         {796--805},
  publisher =     {IEEE},
  title =         {Object {Cluster} {Registration} of {Dissimilar}
                   {Rooms} {Using} {Geometric} {Spatial} {Affordance}
                   {Graph} to {Generate} {Shared} {Virtual} {Spaces}},
  year =          {2024},
  abstract =      {We propose Object Cluster Registration (OCR) using
                   Geometric Spatial Affordance Graph (GSAG) to support
                   user interaction with multiple objects in a shared
                   space generated from two dissimilar rooms. Previous
                   research on generating a shared virtual space from
                   dissimilar real spaces has only reﬂected the
                   information of individual objects and aimed at
                   maximizing the area of the shared space. This led to
                   limited interactions relying on the singular
                   affordances of objects, neglecting to consider the
                   usability and effectiveness of the generated shared
                   spaces. The proposed OCR with GSAG, which considers
                   the relationship between objects based on facing
                   formation, extracts optimal object cluster pairs to
                   align dissimilar rooms in generating shared virtual
                   spaces. In an evaluation study involving 100
                   multi-cluster space pairs, applying OCR using GSAG
                   showed greater effectiveness in preserving object
                   correlations compared to cases where OCR was not
                   used. Furthermore, the size of the shared space did
                   not signiﬁcantly differ between the two methods.
                   This suggests that factoring in the relationship
                   between objects does not compromise the objective of
                   maximizing the shared virtual space. The proposed
                   method is expected to serve as a foundation for
                   generating shared virtual spaces that are more
                   user-oriented and efﬁcient by facilitating a wider
                   range of collaborative activities for remote users in
                   dissimilar real spaces with varied conﬁgurations.},
  doi =           {10.1109/VR58804.2024.00099},
  isbn =          {9798350374025},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10494135/},
}

@inproceedings{lehment_creating_2014,
  address =       {Munich, Germany},
  author =        {Lehment, Nicolas H. and Merget, Daniel and
                   Rigoll, Gerhard},
  booktitle =     {2014 {IEEE} {International} {Symposium} on {Mixed}
                   and {Augmented} {Reality} ({ISMAR})},
  month =         sep,
  pages =         {201--206},
  publisher =     {IEEE},
  title =         {Creating automatically aligned consensus realities
                   for {AR} videoconferencing},
  year =          {2014},
  abstract =      {This paper presents an AR videoconferencing approach
                   merging two remote rooms into a shared workspace.
                   Such bilateral AR telepresence inherently suffers
                   from breaks in immersion stemming from the different
                   physical layouts of participating spaces. As a
                   remedy, we develop an automatic alignment scheme
                   which ensures that participants share a maximum of
                   common features in their physical surroundings. The
                   system optimizes alignment with regard to initial
                   user position, free shared ﬂoor space, camera
                   positioning and other factors. Thus we can reduce
                   discrepancies between different room and furniture
                   layouts without actually modifying the rooms
                   themselves. A description and discussion of our
                   alignment scheme is given along with an exemplary
                   implementation on realworld datasets.},
  doi =           {10.1109/ISMAR.2014.6948428},
  isbn =          {978-1-4799-6184-9},
  language =      {en},
  url =           {http://ieeexplore.ieee.org/document/6948428/},
}

@article{marques_critical_2022,
  author =        {Marques, Bernardo and Teixeira, António and
                   Silva, Samuel and Alves, João and Dias, Paulo and
                   Santos, Beatriz Sousa},
  journal =       {Computers \& Graphics},
  month =         feb,
  pages =         {619--633},
  title =         {A critical analysis on remote collaboration mediated
                   by {Augmented} {Reality}: {Making} a case for
                   improved characterization and evaluation of the
                   collaborative process},
  volume =        {102},
  year =          {2022},
  abstract =      {Remote Collaboration mediated by Mixed and Augmented
                   Reality (MR/AR) shows great potential in scenarios
                   where physically distributed collaborators need to
                   establish a common ground to achieve a shared goal.
                   So far, most research efforts have been devoted to
                   creating the enabling technology, overcoming
                   engineering hurdles and proposing methods to support
                   its design and development. To contribute to more
                   in-depth knowledge on how remote collaboration occurs
                   through these technologies, it is paramount to
                   understand where the field stands and how
                   characterization and evaluation have been conducted.
                   In this vein, this work reports the results of a
                   literature review which shows that evaluation is
                   frequently performed in ad-hoc manners, i.e.,
                   disregarding adapting the evaluation methods to
                   collaborative AR. Most studies rely on single-user
                   methods, which are not suitable for collaborative
                   solutions, falling short of retrieving the necessary
                   amount of contextualized data for more comprehensive
                   evaluations. This suggests minimal support of
                   existing frameworks and a lack of theories and
                   guidelines to guide the characterization of the
                   collaborative process using AR. Then, a critical
                   analysis is presented in which we discuss the
                   maturity of the field and a roadmap of important
                   research actions is proposed, that may help address
                   how to improve the characterization and evaluation of
                   the collaboration process moving forward and, in
                   consequence, improve MR/AR based remote
                   collaboration.},
  doi =           {10.1016/j.cag.2021.08.006},
  issn =          {00978493},
  language =      {en},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                  S0097849321001709},
}

@article{marques_remote_2022,
  author =        {Marques, Bernardo and Silva, Samuel and Alves, João and
                   Rocha, António and Dias, Paulo and
                   Santos, Beatriz Sousa},
  journal =       {International Journal on Interactive Design and
                   Manufacturing (IJIDeM)},
  month =         mar,
  number =        {1},
  pages =         {419--438},
  title =         {Remote collaboration in maintenance contexts using
                   augmented reality: insights from a participatory
                   process},
  volume =        {16},
  year =          {2022},
  abstract =      {Problem solving in Industry 4.0 often requires
                   collaboration among remote team members, which face
                   increased complexity on their daily tasks and require
                   mechanisms with adaptive capabilities to share and
                   combine knowledge. Augmented Reality (AR) is one of
                   the most promising solutions, allowing taking
                   advantage from seamless integration of virtual and
                   real-world objects, which can be used to provide a
                   shared understanding of the task and context. In this
                   regard, most research works, so far, have been
                   devoted to explore and evolve the necessary
                   technology. However, it is now important to revisit
                   the subject of remote collaboration in relation with
                   AR to understand how much of the collaborative effort
                   can already be supported and identify gaps that
                   should inform further research. In line with this
                   mindset, we adopted a user-centered approach with
                   partners from the industry sector, including
                   participatory design and a focus group with domain
                   experts to probe how AR could provide solutions to
                   support their collaborative efforts. We focused on
                   using tangible artifacts in the form of storyboards
                   to create a shared understanding with target users in
                   remote collaboration. Afterwards, we identify a set
                   of requirements, which we materialize through the
                   design and creation of a collaborative prototype
                   based on sharing of enhanced AR annotations. Finally,
                   we present and discuss the results from a case study
                   on a maintenance context, which provides interesting
                   insights that can be applied to other remote
                   settings, thus facilitating the digitization of the
                   industry sector.},
  doi =           {10.1007/s12008-021-00798-6},
  issn =          {1955-2513, 1955-2505},
  language =      {en},
  url =           {https://link.springer.com/10.1007/s12008-021-00798-6},
}

@inproceedings{pejsa_room2room_2016,
  address =       {San Francisco California USA},
  author =        {Pejsa, Tomislav and Kantor, Julian and Benko, Hrvoje and
                   Ofek, Eyal and Wilson, Andrew},
  booktitle =     {Proceedings of the 19th {ACM} {Conference} on
                   {Computer}-{Supported} {Cooperative} {Work} \&
                   {Social} {Computing}},
  month =         feb,
  pages =         {1716--1725},
  publisher =     {ACM},
  title =         {{Room2Room}: {Enabling} {Life}-{Size} {Telepresence}
                   in a {Projected} {Augmented} {Reality} {Environment}},
  year =          {2016},
  abstract =      {Room2Room is a telepresence system that leverages
                   projected augmented reality to enable life-size,
                   co-present interaction between two remote
                   participants. Our solution recreates the experience
                   of a face-to-face conversation by performing 3D
                   capture of the local user with color + depth cameras
                   and projecting their life-size virtual copy into the
                   remote space. This creates an illusion of the remote
                   person’s physical presence in the local space, as
                   well as a shared understanding of verbal and
                   non-verbal cues (e.g., gaze, pointing.) In addition
                   to the technical details of two prototype
                   implementations, we contribute strategies for
                   projecting remote participants onto physically
                   plausible locations, such that they form a natural
                   and consistent conversational formation with the
                   local participant. We also present observations and
                   feedback from an evaluation with 7 pairs of
                   participants on the usability of our solution for
                   solving a collaborative, physical task.},
  doi =           {10.1145/2818048.2819965},
  isbn =          {978-1-4503-3592-8},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/2818048.2819965},
}

@article{yang_visual_2024,
  author =        {Yang, Dongseok and Kang, Jiho and Kim, Taehei and
                   Lee, Sung-Hee},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  pages =         {1--14},
  title =         {Visual {Guidance} for {User} {Placement} in
                   {Avatar}-{Mediated} {Telepresence} between
                   {Dissimilar} {Spaces}},
  year =          {2024},
  abstract =      {Rapid advances in technology gradually realize
                   immersive mixed-reality (MR) telepresence between
                   distant spaces. This paper presents a novel visual
                   guidance system for avatar-mediated telepresence,
                   directing users to optimal placements that facilitate
                   the clear transfer of gaze and pointing contexts
                   through remote avatars in dissimilar spaces, where
                   the spatial relationship between the remote avatar
                   and the interaction targets may differ from that of
                   the local user. Representing the spatial relationship
                   between the user/avatar and interaction targets with
                   angle-based interaction features, we assign
                   recommendation scores of sampled local placements as
                   their maximum feature similarity with remote
                   placements. These scores are visualized as
                   color-coded 2D sectors to inform the users of better
                   placements for interaction with selected targets. In
                   addition, virtual objects of the remote space are
                   overlapped with the local space for the user to
                   better understand the recommendations. We examine
                   whether the proposed score measure agrees with the
                   actual user perception of the partner’s interaction
                   context and find a score threshold for recommendation
                   through user experiments in virtual reality (VR). A
                   subsequent user study in VR investigates the
                   effectiveness and perceptual overload of different
                   combinations of visualizations. Finally, we conduct a
                   user study in an MR telepresence scenario to evaluate
                   the effectiveness of our method in real-world
                   applications.},
  doi =           {10.1109/TVCG.2024.3354256},
  issn =          {1077-2626, 1941-0506, 2160-9306},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10400945/},
}

@article{yoon_placement_2022,
  author =        {Yoon, Leonard and Yang, Dongseok and Kim, Jaehyun and
                   Chung, Choongho and Lee, Sung-Hee},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  month =         mar,
  number =        {3},
  pages =         {1619--1633},
  title =         {Placement {Retargeting} of {Virtual} {Avatars} to
                   {Dissimilar} {Indoor} {Environments}},
  volume =        {28},
  year =          {2022},
  abstract =      {Rapidly developing technologies are realizing a 3D
                   telepresence, in which geographically separated users
                   can interact with each other through their virtual
                   avatars. In this article, we present novel methods to
                   determine the avatar’s position in an indoor space
                   to preserve the semantics of the user’s position in
                   a dissimilar indoor space with different space
                   conﬁgurations and furniture layouts. To this end,
                   we ﬁrst perform a user survey on the preferred
                   avatar placements for various indoor conﬁgurations
                   and user placements, and identify a set of related
                   attributes, including interpersonal relation, visual
                   attention, pose, and spatial characteristics, and
                   quantify these attributes with a set of features. By
                   using the obtained dataset and identiﬁed features,
                   we train a neural network that predicts the
                   similarity between two placements. Next, we develop
                   an avatar placement method that preserves the
                   semantics of the placement of the remote user in a
                   different space as much as possible. We show the
                   effectiveness of our methods by implementing a
                   prototype AR-based telepresence system and user
                   evaluations.},
  doi =           {10.1109/TVCG.2020.3018458},
  issn =          {1077-2626, 1941-0506, 2160-9306},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/9173828/},
}

