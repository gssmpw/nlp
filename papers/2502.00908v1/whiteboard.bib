

@inproceedings{cherubini_lets_2007,
  address =       {San Jose California USA},
  author =        {Cherubini, Mauro and Venolia, Gina and DeLine, Rob and
                   Ko, Amy J.},
  booktitle =     {Proceedings of the {SIGCHI} {Conference} on {Human}
                   {Factors} in {Computing} {Systems}},
  month =         apr,
  pages =         {557--566},
  publisher =     {ACM},
  title =         {Let's go to the whiteboard: how and why software
                   developers use drawings},
  year =          {2007},
  abstract =      {Software developers are rooted in the written form of
                   their code, yet they often draw diagrams representing
                   their code. Unfortunately, we still know little about
                   how and why they create these diagrams, and so there
                   is little research to inform the design of visual
                   tools to support developers’ work. This paper
                   presents findings from semi-structured interviews
                   that have been validated with a structured survey.
                   Results show that most of the diagrams had a
                   transient nature because of the high cost of changing
                   whiteboard sketches to electronic renderings.
                   Diagrams that documented design decisions were often
                   externalized in these temporary drawings and then
                   subsequently lost. Current visualization tools and
                   the software development practices that we observed
                   do not solve these issues, but these results suggest
                   several directions for future research.},
  doi =           {10.1145/1240624.1240714},
  isbn =          {978-1-59593-593-9},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/1240624.1240714},
}

@article{walny_visual_2011,
  author =        {Walny, J. and Carpendale, S. and
                   Henry Riche, Nathalie and Venolia, G. and
                   Fawcett, P.},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  month =         dec,
  number =        {12},
  pages =         {2508--2517},
  title =         {Visual {Thinking} {In} {Action}: {Visualizations}
                   {As} {Used} {On} {Whiteboards}},
  volume =        {17},
  year =          {2011},
  abstract =      {While it is still most common for information
                   visualization researchers to develop new
                   visualizations from a data- or taskdriven
                   perspective, there is growing interest in
                   understanding the types of visualizations people
                   create by themselves for personal use. As part of
                   this recent direction, we have studied a large
                   collection of whiteboards in a research institution,
                   where people make active use of combinations of
                   words, diagrams and various types of visuals to help
                   them further their thought processes. Our goal is to
                   arrive at a better understanding of the nature of
                   visuals that are created spontaneously during
                   brainstorming, thinking, communicating, and general
                   problem solving on whiteboards. We use the
                   qualitative approaches of open coding, interviewing,
                   and afÀnity diagramming to explore the use of
                   recognizable and novel visuals, and the interplay
                   between visualization and diagrammatic elements with
                   words, numbers and labels. We discuss the potential
                   implications of our Àndings on information
                   visualization design.},
  doi =           {10.1109/TVCG.2011.251},
  issn =          {1077-2626},
  language =      {en},
  url =           {http://ieeexplore.ieee.org/document/6065018/},
}

@incollection{abascal_using_2015,
  address =       {Cham},
  author =        {Lucero, Andrés},
  booktitle =     {Human-{Computer} {Interaction} – {INTERACT} 2015},
  editor =        {Abascal, Julio and Barbosa, Simone and Fetter, Mirko and
                   Gross, Tom and Palanque, Philippe and
                   Winckler, Marco},
  note =          {Series Title: Lecture Notes in Computer Science},
  pages =         {231--248},
  publisher =     {Springer International Publishing},
  title =         {Using {Affinity} {Diagrams} to {Evaluate}
                   {Interactive} {Prototypes}},
  volume =        {9297},
  year =          {2015},
  abstract =      {Affinity diagramming is a technique used to
                   externalize, make sense of, and organize large
                   amounts of unstructured, far-ranging, and seemingly
                   dissimilar qualitative data. HCI and interaction
                   design practitioners have adopted and used affinity
                   diagrams for different purposes. This paper discusses
                   our particular use of affinity diagramming in
                   prototype evaluations. We reflect on a decade’s
                   experience using affinity diagramming across a number
                   of projects, both in industry and academia. Our
                   affinity diagramming process in interaction design
                   has been tailored and consists of four stages:
                   creating notes, clustering notes, walking the wall,
                   and documentation. We draw examples from eight
                   projects to illustrate our particular practices along
                   these four stages, as well as to ground the
                   discussion.},
  doi =           {10.1007/978-3-319-22668-2_19},
  isbn =          {978-3-319-22667-5 978-3-319-22668-2},
  language =      {en},
  url =           {http://link.springer.com/10.1007/978-3-319-22668-2_19},
}

@inproceedings{yoon_effect_2019,
  address =       {Osaka, Japan},
  author =        {Yoon, Boram and Kim, Hyung-il and Lee, Gun A. and
                   Billinghurst, Mark and Woo, Woontack},
  booktitle =     {2019 {IEEE} {Conference} on {Virtual} {Reality} and
                   {3D} {User} {Interfaces} ({VR})},
  month =         mar,
  pages =         {547--556},
  publisher =     {IEEE},
  title =         {The {Effect} of {Avatar} {Appearance} on {Social}
                   {Presence} in an {Augmented} {Reality} {Remote}
                   {Collaboration}},
  year =          {2019},
  abstract =      {This paper investigates the effect of avatar
                   appearance on Social Presence and users’ perception
                   in an Augmented Reality (AR) telepresence system.
                   Despite the development of various commercial 3D
                   telepresence systems, there has been little
                   evaluation and discussions about the appearance of
                   the collaborator’s avatars. We conducted two user
                   studies comparing the effect of avatar appearances
                   with three levels of body part visibility (head \&
                   hands, upper body, and whole body) and two different
                   character styles (realistic and cartoon-like) on
                   Social Presence while performing two different remote
                   collaboration tasks. We found that a realistic whole
                   body avatar was perceived as being the best for
                   remote collaboration, but an upper body or cartoon
                   style could be considered as a substitute depending
                   on the collaboration context. We discuss these
                   results and suggest guidelines for designing future
                   avatar-mediated AR remote collaboration systems.},
  doi =           {10.1109/VR.2019.8797719},
  isbn =          {978-1-72811-377-7},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/8797719/},
}

@article{aseeri_influence_2021,
  author =        {Aseeri, Sahar and Interrante, Victoria},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  month =         may,
  number =        {5},
  pages =         {2608--2617},
  title =         {The {Influence} of {Avatar} {Representation} on
                   {Interpersonal} {Communication} in {Virtual} {Social}
                   {Environments}},
  volume =        {27},
  year =          {2021},
  abstract =      {Current avatar representations used in immersive VR
                   applications lack features that may be important for
                   supporting natural behaviors and effective
                   communication among individuals. This study
                   investigates the impact of the visual and nonverbal
                   cues afforded by three different types of avatar
                   representations in the context of several cooperative
                   tasks. The avatar types we compared are No Avatar
                   (HMD and controllers only), Scanned Avatar (wearing
                   an HMD), and Real Avatar (video-see-through). The
                   subjective and objective measures we used to assess
                   the quality of interpersonal communication include
                   surveys of social presence, interpersonal trust,
                   communication satisfaction, and attention to
                   behavioral cues, plus two behavioral measures:
                   duration of mutual gaze and number of unique words
                   spoken. We found that participants reported higher
                   levels of trustworthiness in the Real Avatar
                   condition compared to the Scanned Avatar and No
                   Avatar conditions. They also reported a greater level
                   of attentional focus on facial expressions compared
                   to the No Avatar condition and spent more extended
                   time, for some tasks, attempting to engage in mutual
                   gaze behavior compared to the Scanned Avatar and No
                   Avatar conditions. In both the Real Avatar and
                   Scanned Avatar conditions, participants reported
                   higher levels of co-presence compared with the No
                   Avatar condition. In the Scanned Avatar condition,
                   compared with the Real Avatar and No Avatar
                   conditions, participants reported higher levels of
                   attention to body posture. Overall, our exit survey
                   revealed that a majority of participants (66.67\%)
                   reported a preference for the Real Avatar, compared
                   with 25.00\% for the Scanned Avatar and 8.33\% for
                   the No Avatar. These ﬁndings provide novel insight
                   into how a user’s experience in a social VR
                   scenario is affected by the type of avatar
                   representation provided.},
  doi =           {10.1109/TVCG.2021.3067783},
  issn =          {1077-2626, 1941-0506, 2160-9306},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/9382845/},
}

@inproceedings{congdon_merging_2018,
  address =       {Tokyo Japan},
  author =        {Congdon, Ben J. and Wang, Tuanfeng and
                   Steed, Anthony},
  booktitle =     {Proceedings of the 24th {ACM} {Symposium} on
                   {Virtual} {Reality} {Software} and {Technology}},
  month =         nov,
  pages =         {1--8},
  publisher =     {ACM},
  title =         {Merging environments for shared spaces in mixed
                   reality},
  year =          {2018},
  abstract =      {In virtual reality a real walking interface limits
                   the extent of a virtual environment to our local
                   walkable space. As local spaces are specific to each
                   user, sharing a virtual environment with others for
                   collaborative work or games becomes complicated. It
                   is not clear which user’s walkable space to prefer,
                   or whether that space will be navigable for both
                   users.},
  doi =           {10.1145/3281505.3281544},
  isbn =          {978-1-4503-6086-9},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3281505.3281544},
}

@inproceedings{keshavarzi_optimization_2020,
  author =        {Keshavarzi, Mohammad and Yang, Allen Y. and
                   Ko, Woojin and Caldas, Luisa},
  booktitle =     {2020 {IEEE} {Conference} on {Virtual} {Reality} and
                   {3D} {User} {Interfaces} ({VR})},
  month =         mar,
  note =          {arXiv:1910.05998 [cs]},
  pages =         {353--362},
  title =         {Optimization and {Manipulation} of {Contextual}
                   {Mutual} {Spaces} for {Multi}-{User} {Virtual} and
                   {Augmented} {Reality} {Interaction}},
  year =          {2020},
  abstract =      {Spatial computing experiences are physically
                   constrained by the geometry and semantics of the
                   local user environment. This limitation is elevated
                   in remote multi-user interaction scenarios, where
                   ﬁnding a common virtual ground physically
                   accessible for all participants becomes challenging.
                   Locating a common accessible virtual ground is
                   difﬁcult for the users themselves, particularly if
                   they are not aware of the spatial properties of other
                   participants. In this paper, we introduce a framework
                   to generate an optimal mutual virtual space for a
                   multi-user interaction setting where remote users’
                   room spaces can have different layout and sizes. The
                   framework further recommends movement of surrounding
                   furniture objects that expand the size of the mutual
                   space with minimal physical effort. Finally, we
                   demonstrate the performance of our solution on
                   real-world datasets and also a real HoloLens
                   application. Results show the proposed algorithm can
                   effectively discover optimal shareable space for
                   multiuser virtual interaction and hence facilitate
                   remote spatial computing communication in various
                   collaborative workﬂows.},
  doi =           {10.1109/VR46266.2020.00055},
  language =      {en},
  url =           {http://arxiv.org/abs/1910.05998},
}

@article{fink_re-locations_2022,
  author =        {Fink, Daniel Immanuel and Zagermann, Johannes and
                   Reiterer, Harald and Jetter, Hans-Christian},
  journal =       {Proceedings of the ACM on Human-Computer Interaction},
  month =         nov,
  number =        {ISS},
  pages =         {1--30},
  title =         {Re-locations: {Augmenting} {Personal} and {Shared}
                   {Workspaces} to {Support} {Remote} {Collaboration} in
                   {Incongruent} {Spaces}},
  volume =        {6},
  year =          {2022},
  abstract =      {Augmented reality (AR) can create the illusion of
                   being virtually co-located during remote
                   collaboration, e.g., by visualizing remote co-workers
                   as avatars. However, spatial awareness of each
                   other's activities is limited as physical spaces,
                   including the position of physical devices, are often
                   incongruent. Therefore, alignment methods are needed
                   to support activities on physical devices. In this
                   paper, we present the concept of Re-locations, a
                   method for enabling remote collaboration with
                   augmented reality in incongruent spaces. The idea of
                   the concept is to enrich remote collaboration
                   activities on multiple physical devices with
                   attributes of co-located collaboration such as
                   spatial awareness and spatial referencing by locally
                   relocating remote user representations to
                   user-defined workspaces. We evaluated the
                   Re-locations concept in an explorative user study
                   with dyads using an authentic, collaborative task.
                   Our findings indicate that Re-locations introduce
                   attributes of co-located collaboration like spatial
                   awareness and social presence. Based on our findings,
                   we provide implications for future research and
                   design of remote collaboration systems using AR.},
  doi =           {10.1145/3567709},
  issn =          {2573-0142},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3567709},
}

@article{ens_revisiting_2019,
  author =        {Ens, Barrett and Lanir, Joel and Tang, Anthony and
                   Bateman, Scott and Lee, Gun and
                   Piumsomboon, Thammathip and Billinghurst, Mark},
  journal =       {International Journal of Human-Computer Studies},
  month =         nov,
  pages =         {81--98},
  title =         {Revisiting collaboration through mixed reality: {The}
                   evolution of groupware},
  volume =        {131},
  year =          {2019},
  abstract =      {Collaborative Mixed Reality (MR) systems are at a
                   critical point in time as they are soon to become
                   more commonplace. However, MR technology has only
                   recently matured to the point where researchers can
                   focus deeply on the nuances of supporting
                   collaboration, rather than needing to focus on
                   creating the enabling technology. In parallel, but
                   largely independently, the ﬁeld of Computer
                   Supported Cooperative Work (CSCW) has focused on the
                   fundamental concerns that underlie human
                   communication and collaboration over the past 30-plus
                   years. Since MR research is now on the brink of
                   moving into the real world, we reﬂect on three
                   decades of collaborative MR research and try to
                   reconcile it with existing theory from CSCW, to help
                   position MR researchers to pursue fruitful directions
                   for their work. To do this, we review the history of
                   collaborative MR systems, investigating how the
                   common taxonomies and frameworks in CSCW and MR
                   research can be applied to existing work on
                   collaborative MR systems, exploring where they have
                   fallen behind, and look for new ways to describe
                   current trends. Through identifying emergent trends,
                   we suggest future directions for MR, and also ﬁnd
                   where CSCW researchers can explore new theory that
                   more fully represents the future of working, playing
                   and being with others.},
  doi =           {10.1016/j.ijhcs.2019.05.011},
  issn =          {10715819},
  language =      {en},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                  S1071581919300606},
}

@article{marques_remote_2022,
  author =        {Marques, Bernardo and Silva, Samuel and Alves, João and
                   Rocha, António and Dias, Paulo and
                   Santos, Beatriz Sousa},
  journal =       {International Journal on Interactive Design and
                   Manufacturing (IJIDeM)},
  month =         mar,
  number =        {1},
  pages =         {419--438},
  title =         {Remote collaboration in maintenance contexts using
                   augmented reality: insights from a participatory
                   process},
  volume =        {16},
  year =          {2022},
  abstract =      {Problem solving in Industry 4.0 often requires
                   collaboration among remote team members, which face
                   increased complexity on their daily tasks and require
                   mechanisms with adaptive capabilities to share and
                   combine knowledge. Augmented Reality (AR) is one of
                   the most promising solutions, allowing taking
                   advantage from seamless integration of virtual and
                   real-world objects, which can be used to provide a
                   shared understanding of the task and context. In this
                   regard, most research works, so far, have been
                   devoted to explore and evolve the necessary
                   technology. However, it is now important to revisit
                   the subject of remote collaboration in relation with
                   AR to understand how much of the collaborative effort
                   can already be supported and identify gaps that
                   should inform further research. In line with this
                   mindset, we adopted a user-centered approach with
                   partners from the industry sector, including
                   participatory design and a focus group with domain
                   experts to probe how AR could provide solutions to
                   support their collaborative efforts. We focused on
                   using tangible artifacts in the form of storyboards
                   to create a shared understanding with target users in
                   remote collaboration. Afterwards, we identify a set
                   of requirements, which we materialize through the
                   design and creation of a collaborative prototype
                   based on sharing of enhanced AR annotations. Finally,
                   we present and discuss the results from a case study
                   on a maintenance context, which provides interesting
                   insights that can be applied to other remote
                   settings, thus facilitating the digitization of the
                   industry sector.},
  doi =           {10.1007/s12008-021-00798-6},
  issn =          {1955-2513, 1955-2505},
  language =      {en},
  url =           {https://link.springer.com/10.1007/s12008-021-00798-6},
}

@article{marques_critical_2022,
  author =        {Marques, Bernardo and Teixeira, António and
                   Silva, Samuel and Alves, João and Dias, Paulo and
                   Santos, Beatriz Sousa},
  journal =       {Computers \& Graphics},
  month =         feb,
  pages =         {619--633},
  title =         {A critical analysis on remote collaboration mediated
                   by {Augmented} {Reality}: {Making} a case for
                   improved characterization and evaluation of the
                   collaborative process},
  volume =        {102},
  year =          {2022},
  abstract =      {Remote Collaboration mediated by Mixed and Augmented
                   Reality (MR/AR) shows great potential in scenarios
                   where physically distributed collaborators need to
                   establish a common ground to achieve a shared goal.
                   So far, most research efforts have been devoted to
                   creating the enabling technology, overcoming
                   engineering hurdles and proposing methods to support
                   its design and development. To contribute to more
                   in-depth knowledge on how remote collaboration occurs
                   through these technologies, it is paramount to
                   understand where the field stands and how
                   characterization and evaluation have been conducted.
                   In this vein, this work reports the results of a
                   literature review which shows that evaluation is
                   frequently performed in ad-hoc manners, i.e.,
                   disregarding adapting the evaluation methods to
                   collaborative AR. Most studies rely on single-user
                   methods, which are not suitable for collaborative
                   solutions, falling short of retrieving the necessary
                   amount of contextualized data for more comprehensive
                   evaluations. This suggests minimal support of
                   existing frameworks and a lack of theories and
                   guidelines to guide the characterization of the
                   collaborative process using AR. Then, a critical
                   analysis is presented in which we discuss the
                   maturity of the field and a roadmap of important
                   research actions is proposed, that may help address
                   how to improve the characterization and evaluation of
                   the collaboration process moving forward and, in
                   consequence, improve MR/AR based remote
                   collaboration.},
  doi =           {10.1016/j.cag.2021.08.006},
  issn =          {00978493},
  language =      {en},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                  S0097849321001709},
}

@inproceedings{pejsa_room2room_2016,
  address =       {San Francisco California USA},
  author =        {Pejsa, Tomislav and Kantor, Julian and Benko, Hrvoje and
                   Ofek, Eyal and Wilson, Andrew},
  booktitle =     {Proceedings of the 19th {ACM} {Conference} on
                   {Computer}-{Supported} {Cooperative} {Work} \&
                   {Social} {Computing}},
  month =         feb,
  pages =         {1716--1725},
  publisher =     {ACM},
  title =         {{Room2Room}: {Enabling} {Life}-{Size} {Telepresence}
                   in a {Projected} {Augmented} {Reality} {Environment}},
  year =          {2016},
  abstract =      {Room2Room is a telepresence system that leverages
                   projected augmented reality to enable life-size,
                   co-present interaction between two remote
                   participants. Our solution recreates the experience
                   of a face-to-face conversation by performing 3D
                   capture of the local user with color + depth cameras
                   and projecting their life-size virtual copy into the
                   remote space. This creates an illusion of the remote
                   person’s physical presence in the local space, as
                   well as a shared understanding of verbal and
                   non-verbal cues (e.g., gaze, pointing.) In addition
                   to the technical details of two prototype
                   implementations, we contribute strategies for
                   projecting remote participants onto physically
                   plausible locations, such that they form a natural
                   and consistent conversational formation with the
                   local participant. We also present observations and
                   feedback from an evaluation with 7 pairs of
                   participants on the usability of our solution for
                   solving a collaborative, physical task.},
  doi =           {10.1145/2818048.2819965},
  isbn =          {978-1-4503-3592-8},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/2818048.2819965},
}

@inproceedings{lehment_creating_2014,
  address =       {Munich, Germany},
  author =        {Lehment, Nicolas H. and Merget, Daniel and
                   Rigoll, Gerhard},
  booktitle =     {2014 {IEEE} {International} {Symposium} on {Mixed}
                   and {Augmented} {Reality} ({ISMAR})},
  month =         sep,
  pages =         {201--206},
  publisher =     {IEEE},
  title =         {Creating automatically aligned consensus realities
                   for {AR} videoconferencing},
  year =          {2014},
  abstract =      {This paper presents an AR videoconferencing approach
                   merging two remote rooms into a shared workspace.
                   Such bilateral AR telepresence inherently suffers
                   from breaks in immersion stemming from the different
                   physical layouts of participating spaces. As a
                   remedy, we develop an automatic alignment scheme
                   which ensures that participants share a maximum of
                   common features in their physical surroundings. The
                   system optimizes alignment with regard to initial
                   user position, free shared ﬂoor space, camera
                   positioning and other factors. Thus we can reduce
                   discrepancies between different room and furniture
                   layouts without actually modifying the rooms
                   themselves. A description and discussion of our
                   alignment scheme is given along with an exemplary
                   implementation on realworld datasets.},
  doi =           {10.1109/ISMAR.2014.6948428},
  isbn =          {978-1-4799-6184-9},
  language =      {en},
  url =           {http://ieeexplore.ieee.org/document/6948428/},
}

@inproceedings{kim_object_2024,
  address =       {Orlando, FL, USA},
  author =        {Kim, Seonji and Kim, Dooyoung and Shin, Jae-Eun and
                   Woo, Woontack},
  booktitle =     {2024 {IEEE} {Conference} {Virtual} {Reality} and {3D}
                   {User} {Interfaces} ({VR})},
  month =         mar,
  pages =         {796--805},
  publisher =     {IEEE},
  title =         {Object {Cluster} {Registration} of {Dissimilar}
                   {Rooms} {Using} {Geometric} {Spatial} {Affordance}
                   {Graph} to {Generate} {Shared} {Virtual} {Spaces}},
  year =          {2024},
  abstract =      {We propose Object Cluster Registration (OCR) using
                   Geometric Spatial Affordance Graph (GSAG) to support
                   user interaction with multiple objects in a shared
                   space generated from two dissimilar rooms. Previous
                   research on generating a shared virtual space from
                   dissimilar real spaces has only reﬂected the
                   information of individual objects and aimed at
                   maximizing the area of the shared space. This led to
                   limited interactions relying on the singular
                   affordances of objects, neglecting to consider the
                   usability and effectiveness of the generated shared
                   spaces. The proposed OCR with GSAG, which considers
                   the relationship between objects based on facing
                   formation, extracts optimal object cluster pairs to
                   align dissimilar rooms in generating shared virtual
                   spaces. In an evaluation study involving 100
                   multi-cluster space pairs, applying OCR using GSAG
                   showed greater effectiveness in preserving object
                   correlations compared to cases where OCR was not
                   used. Furthermore, the size of the shared space did
                   not signiﬁcantly differ between the two methods.
                   This suggests that factoring in the relationship
                   between objects does not compromise the objective of
                   maximizing the shared virtual space. The proposed
                   method is expected to serve as a foundation for
                   generating shared virtual spaces that are more
                   user-oriented and efﬁcient by facilitating a wider
                   range of collaborative activities for remote users in
                   dissimilar real spaces with varied conﬁgurations.},
  doi =           {10.1109/VR58804.2024.00099},
  isbn =          {9798350374025},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10494135/},
}

@inproceedings{kim_adjusting_2021,
  address =       {Lisboa, Portugal},
  author =        {Kim, Dooyoung and Shin, Jae-eun and Lee, Jeongmi and
                   Woo, Woontack},
  booktitle =     {2021 {IEEE} {Virtual} {Reality} and {3D} {User}
                   {Interfaces} ({VR})},
  month =         mar,
  pages =         {653--660},
  publisher =     {IEEE},
  title =         {Adjusting {Relative} {Translation} {Gains}
                   {According} to {Space} {Size} in {Redirected}
                   {Walking} for {Mixed} {Reality} {Mutual} {Space}
                   {Generation}},
  year =          {2021},
  abstract =      {We propose the concept of relative translation gains,
                   a novel Redirected Walking (RDW) method to create a
                   mutual movable space between the Augmented Reality
                   (AR) host’s reference space and the Virtual Reality
                   (VR) client’s space. Previous RDW methods have
                   focused on maximizing the movable space at the
                   expense of aligning the coordinates between the AR
                   and VR side, and could only be applied to
                   collaborative scenarios involving sequential tasks.
                   Our method solves these problems by adjusting the
                   remote client’s walking speed for each axis of a VR
                   space to modify the movable area without coordinate
                   distortion. We estimate the relative translation gain
                   threshold, deﬁned as the extent to which the
                   walking speed can be altered without creating a
                   perceived difference in distance. In order to
                   reﬂect features of the reference space in
                   generating the mutual space, we then examine how
                   changing its size affects the threshold value. Our
                   study showed that for remote clients connected to the
                   larger reference space, relative translation gains
                   can be increased to utilize a VR space bigger than
                   their real space. Our method can be applied to create
                   optimal mutual spaces for a wider variety of
                   asymmetric Mixed Reality (MR) remote collaboration
                   systems.},
  doi =           {10.1109/VR50410.2021.00091},
  isbn =          {978-1-66541-838-6},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/9417781/},
}

@inproceedings{kang_real-time_2023,
  address =       {Sydney, Australia},
  author =        {Kang, Jiho and Yang, Dongseok and Kim, Taehei and
                   Lee, Yewon and Lee, Sung-Hee},
  booktitle =     {2023 {IEEE} {International} {Symposium} on {Mixed}
                   and {Augmented} {Reality} ({ISMAR})},
  month =         oct,
  pages =         {885--893},
  publisher =     {IEEE},
  title =         {Real-time {Retargeting} of {Deictic} {Motion} to
                   {Virtual} {Avatars} for {Augmented} {Reality}
                   {Telepresence}},
  year =          {2023},
  doi =           {10.1109/ISMAR59233.2023.00104},
  isbn =          {9798350328387},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10316476/},
}

@article{yoon_placement_2022,
  author =        {Yoon, Leonard and Yang, Dongseok and Kim, Jaehyun and
                   Chung, Choongho and Lee, Sung-Hee},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  month =         mar,
  number =        {3},
  pages =         {1619--1633},
  title =         {Placement {Retargeting} of {Virtual} {Avatars} to
                   {Dissimilar} {Indoor} {Environments}},
  volume =        {28},
  year =          {2022},
  abstract =      {Rapidly developing technologies are realizing a 3D
                   telepresence, in which geographically separated users
                   can interact with each other through their virtual
                   avatars. In this article, we present novel methods to
                   determine the avatar’s position in an indoor space
                   to preserve the semantics of the user’s position in
                   a dissimilar indoor space with different space
                   conﬁgurations and furniture layouts. To this end,
                   we ﬁrst perform a user survey on the preferred
                   avatar placements for various indoor conﬁgurations
                   and user placements, and identify a set of related
                   attributes, including interpersonal relation, visual
                   attention, pose, and spatial characteristics, and
                   quantify these attributes with a set of features. By
                   using the obtained dataset and identiﬁed features,
                   we train a neural network that predicts the
                   similarity between two placements. Next, we develop
                   an avatar placement method that preserves the
                   semantics of the placement of the remote user in a
                   different space as much as possible. We show the
                   effectiveness of our methods by implementing a
                   prototype AR-based telepresence system and user
                   evaluations.},
  doi =           {10.1109/TVCG.2020.3018458},
  issn =          {1077-2626, 1941-0506, 2160-9306},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/9173828/},
}

@article{herskovitz_xspace_2022,
  author =        {Herskovitz, Jaylin and Cheng, Yi Fei and Guo, Anhong and
                   Sample, Alanson P. and Nebeling, Michael},
  journal =       {Proceedings of the ACM on Human-Computer Interaction},
  month =         nov,
  number =        {ISS},
  pages =         {277--302},
  title =         {{XSpace}: {An} {Augmented} {Reality} {Toolkit} for
                   {Enabling} {Spatially}-{Aware} {Distributed}
                   {Collaboration}},
  volume =        {6},
  year =          {2022},
  abstract =      {Augmented Reality (AR) has the potential to leverage
                   environmental information to better facilitate
                   distributed collaboration, however, such applications
                   are difficult to develop. We present XSpace, a
                   toolkit for creating spatially-aware AR applications
                   for distributed collaboration. Based on a review of
                   existing applications and developer tools, we design
                   XSpace to support three methods for creating shared
                   virtual spaces, each emphasizing a different aspect:
                   shared objects, user perspectives, and environmental
                   meshes. XSpace implements these methods in a
                   developer toolkit, and also provides a set of
                   complimentary visual authoring tools to allow
                   developers to preview a variety of configurations for
                   a shared virtual space. We present five example
                   applications to illustrate that XSpace can support
                   the development of a rich set of collaborative AR
                   experiences that are difficult to produce with
                   current solutions. Through XSpace, we discuss
                   implications for future application design, including
                   user space customization and privacy and safety
                   concerns when sharing users’ environments. CCS
                   Concepts: • Human-centered computing → Interface
                   design prototyping; Mixed / augmented reality.},
  doi =           {10.1145/3567721},
  issn =          {2573-0142},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3567721},
}

@inproceedings{gronbaek_partially_2023,
  address =       {Hamburg Germany},
  author =        {Grønbæk, Jens Emil Sloth and Pfeuffer, Ken and
                   Velloso, Eduardo and Astrup, Morten and
                   Pedersen, Melanie Isabel Sønderkær and
                   Kjær, Martin and Leiva, Germán and Gellersen, Hans},
  booktitle =     {Proceedings of the 2023 {CHI} {Conference} on {Human}
                   {Factors} in {Computing} {Systems}},
  month =         apr,
  pages =         {1--16},
  publisher =     {ACM},
  title =         {Partially {Blended} {Realities}: {Aligning}
                   {Dissimilar} {Spaces} for {Distributed} {Mixed}
                   {Reality} {Meetings}},
  year =          {2023},
  doi =           {10.1145/3544548.3581515},
  isbn =          {978-1-4503-9421-5},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3544548.3581515},
}

@article{huang_surfshare_2023,
  author =        {Huang, Xincheng and Xiao, Robert},
  journal =       {Proceedings of the ACM on Interactive, Mobile,
                   Wearable and Ubiquitous Technologies},
  month =         dec,
  number =        {4},
  pages =         {1--24},
  title =         {{SurfShare}: {Lightweight} {Spatially} {Consistent}
                   {Physical} {Surface} and {Virtual} {Replica}
                   {Sharing} with {Head}-mounted {Mixed}-{Reality}},
  volume =        {7},
  year =          {2023},
  abstract =      {Shared Mixed Reality experiences allow two co-located
                   users to collaborate on both physical and digital
                   tasks with familiar social protocols. However,
                   extending the same to remote collaboration is limited
                   by cumbersome setups for aligning distinct physical
                   environments and the lack of access to remote
                   physical artifacts. We present SurfShare, a
                   general-purpose symmetric remote collaboration system
                   with mixed-reality head-mounted displays (HMDs). Our
                   system shares a spatially consistent physical-virtual
                   workspace between two remote users, anchored on a
                   physical plane in each environment (e.g., a desk or
                   wall). The video feed of each user’s physical
                   surface is overlaid virtually on the other side,
                   creating a shared view of the physical space. We
                   integrate the physical and virtual workspace through
                   virtual replication. Users can transmute physical
                   objects to the virtual space as virtual replicas. Our
                   system is lightweight, implemented using only the
                   capabilities of the headset, without requiring any
                   modifications to the environment (e.g. cameras or
                   motion tracking hardware). We discuss the design,
                   implementation, and interaction capabilities of our
                   prototype, and demonstrate the utility of SurfShare
                   through four example applications. In a user
                   experiment with a comprehensive prototyping task, we
                   found that SurfShare provides a physical-virtual
                   workspace that supports low-fi prototyping with
                   flexible proxemics and fluid collaboration dynamics.
                   CCS Concepts: • Human-centered computing → Mixed
                   / augmented reality.},
  doi =           {10.1145/3631418},
  issn =          {2474-9567},
  language =      {en},
  url =           {https://dl.acm.org/doi/10.1145/3631418},
}

@article{yang_visual_2024,
  author =        {Yang, Dongseok and Kang, Jiho and Kim, Taehei and
                   Lee, Sung-Hee},
  journal =       {IEEE Transactions on Visualization and Computer
                   Graphics},
  pages =         {1--14},
  title =         {Visual {Guidance} for {User} {Placement} in
                   {Avatar}-{Mediated} {Telepresence} between
                   {Dissimilar} {Spaces}},
  year =          {2024},
  abstract =      {Rapid advances in technology gradually realize
                   immersive mixed-reality (MR) telepresence between
                   distant spaces. This paper presents a novel visual
                   guidance system for avatar-mediated telepresence,
                   directing users to optimal placements that facilitate
                   the clear transfer of gaze and pointing contexts
                   through remote avatars in dissimilar spaces, where
                   the spatial relationship between the remote avatar
                   and the interaction targets may differ from that of
                   the local user. Representing the spatial relationship
                   between the user/avatar and interaction targets with
                   angle-based interaction features, we assign
                   recommendation scores of sampled local placements as
                   their maximum feature similarity with remote
                   placements. These scores are visualized as
                   color-coded 2D sectors to inform the users of better
                   placements for interaction with selected targets. In
                   addition, virtual objects of the remote space are
                   overlapped with the local space for the user to
                   better understand the recommendations. We examine
                   whether the proposed score measure agrees with the
                   actual user perception of the partner’s interaction
                   context and find a score threshold for recommendation
                   through user experiments in virtual reality (VR). A
                   subsequent user study in VR investigates the
                   effectiveness and perceptual overload of different
                   combinations of visualizations. Finally, we conduct a
                   user study in an MR telepresence scenario to evaluate
                   the effectiveness of our method in real-world
                   applications.},
  doi =           {10.1109/TVCG.2024.3354256},
  issn =          {1077-2626, 1941-0506, 2160-9306},
  language =      {en},
  url =           {https://ieeexplore.ieee.org/document/10400945/},
}

@incollection{hart_development_1988,
  author =        {Hart, Sandra G. and Staveland, Lowell E.},
  booktitle =     {Advances in {Psychology}},
  pages =         {139--183},
  publisher =     {Elsevier},
  title =         {Development of {NASA}-{TLX} ({Task} {Load} {Index}):
                   {Results} of {Empirical} and {Theoretical}
                   {Research}},
  volume =        {52},
  year =          {1988},
  abstract =      {T h e results of a multi-year research program to
                   identiJy the Jactors asaoriaied with variations i n
                   subjective workload uizthin and betweerr different
                   types OJ tasks are reviewed. Subjecizve evalualions
                   oJ 10 utorkload-related factors were obtained J r o m
                   16 different urperzments. The ezperimental tasks
                   included simple cogn i t i w and manual control
                   tasks, complez laboratory and supervisory control
                   tasks, and aircraJi simulation. Task-, behavior-, and
                   subject-related correlates OJ subjeciive workload
                   ezperiences wried as a Junction oJ difficulty
                   manipulations within experiments, different sources
                   OJ workload between experiments. and individual
                   differences in workload definition. A
                   multi-dimensional rating scale is proposed in which
                   inJormation about the magnitude and sources oJ six
                   workload-related factors are combined io derive a
                   sensitzve and reliable estimate of workload.},
  doi =           {10.1016/S0166-4115(08)62386-9},
  isbn =          {978-0-444-70388-0},
  language =      {en},
  url =           {https://linkinghub.elsevier.com/retrieve/pii/
                  S0166411508623869},
}

@incollection{holzinger_construction_2008,
  address =       {Berlin, Heidelberg},
  author =        {Laugwitz, Bettina and Held, Theo and Schrepp, Martin},
  booktitle =     {{HCI} and {Usability} for {Education} and {Work}},
  editor =        {Holzinger, Andreas},
  note =          {Series Title: Lecture Notes in Computer Science},
  pages =         {63--76},
  publisher =     {Springer Berlin Heidelberg},
  title =         {Construction and {Evaluation} of a {User}
                   {Experience} {Questionnaire}},
  volume =        {5298},
  year =          {2008},
  abstract =      {An end-user questionnaire to measure user experience
                   quickly in a simple and immediate way while covering
                   a preferably comprehensive impression of the product
                   user experience was the goal of the reported
                   construction process. An empirical approach for the
                   item selection was used to ensure practical relevance
                   of items. Usability experts collected terms and
                   statements on user experience and usability,
                   including ‘hard’ as well as ‘soft’ aspects.
                   These statements were consolidated and transformed
                   into a first questionnaire version containing 80
                   bipolar items. It was used to measure the user
                   experience of software products in several empirical
                   studies. Data were subjected to a factor analysis
                   which resulted in the construction of a 26 item
                   questionnaire including the six factors
                   Attractiveness, Perspicuity, Efficiency,
                   Dependability, Stimulation, and Novelty. Studies
                   conducted for the original German questionnaire and
                   an English version indicate a satisfactory level of
                   reliability and construct validity.},
  doi =           {10.1007/978-3-540-89350-9_6},
  isbn =          {978-3-540-89349-3 978-3-540-89350-9},
  language =      {en},
  url =           {http://link.springer.com/10.1007/978-3-540-89350-9_6},
}

@article{brooke_sus_nodate,
  author =        {Brooke, John},
  title =         {{SUS} - {A} quick and dirty usability scale},
  abstract =      {Usability does not exist in any absolute sense; it
                   can only be defined with reference to particular
                   contexts. This, in turn, means that there are no
                   absolute measures of usability, since, if the
                   usability of an artefact is defined by the context in
                   which that artefact is used, measures of usability
                   must of necessity be defined by that context too.
                   Despite this, there is a need for broad general
                   measures which can be used to compare usability
                   across a range of contexts. In addition, there is a
                   need for “quick and dirty” methods to allow low
                   cost assessments of usability in industrial systems
                   evaluation. This chapter describes the System
                   Usability Scale (SUS) a reliable, low-cost usability
                   scale that can be used for global assessments of
                   systems usability.},
  language =      {en},
}

