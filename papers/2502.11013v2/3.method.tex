
% 加一节，怎么区分确定性和不确定性

\section{Methodology}


To achieve effective probabilistic predictions, we propose CoST that simultaneously leverages the advantages of both deterministic and probabilistic models. Our approach involves two stages. In the first stage, the deterministic model is pretrained to predict the conditional mean that captures the primary patterns. In the second stage, the parameters of the deterministic model are frozen, and the scale-aware diffusion model, constrained by a customized fluctuation scale, is jointly trained to model residual distributions that reflect random fluctuations.   
Figure~\ref{fig:model} illustrates an overview of our model.


\subsection{Mean-Residual Decomposition}

For urban spatiotemporal probabilistic prediction, current approaches typically employ a single probabilistic model to capture the full distribution of data, incorporating both the primary spatiotemporal patterns and the random fluctuations. However, it is challenging to model both of these distributions. Inspired by~\cite{mardani2023residual} and the Reynolds decomposition in fluid dynamics~\cite{pope2001turbulent}, we propose to decompose the target data \(\mathbf{x}^{ta}\) as follows:
\begin{equation}
 \mathbf{x}^{ta} = \underbrace{\mathbb{E}[\mathbf{x}^{ta}|\mathbf{x}^{co}]}_{\substack{:=\boldsymbol{\mu}(Deterministic)}} + \underbrace{(\mathbf{x}^{ta}-\mathbb{E}[\mathbf{x}^{ta}|\mathbf{x}^{co}])}_{\substack{:=\mathbf{r}(Probabilistic)}},
\end{equation}
where \(\boldsymbol{\mu}\) is the conditional mean representing the primary patterns, and \(\mathbf{r}\) is the residual representing the random variations. Our core idea is that if a deterministic model can accurately predict the conditional mean, that is, \(\boldsymbol{\mu}\approx\mathbb{E}_{\theta}[\mathbf{x}^{ta}|\mathbf{x}]\), then the probabilistic model only needs to focus on learning the simpler residual distribution, thus combining the strengths of both models to enhance the probabilistic prediction capability.









\subsection{Mean Prediction via Deterministic Model}

We require a deterministic model that can accurately predict the conditional mean to align with our research hypothesis, and also maintain high predictive efficiency to avoid additional increases in training and inference time. Therefore, we select the MLP-based STID model as our mean prediction module.
In the first stage of training, we pretrain the model for 50 epochs to effectively capture the primary spatiotemporal patterns. Specifically, we input historical conditional data \(\mathbf{x}^{co}\) into the STID model to obtain the conditional mean output \(\mathbb{E}_{\theta}[\mathbf{x}^{ta}|\mathbf{x}^{co}]\).

The STID model is pretrained by optimizing the following loss function:

\begin{equation}
\label{eq:loss2}
   \mathcal{L}_{2}  = \left\| \mathbb{E}_{\theta}[\mathbf{x}^{ta}|\mathbf{x}^{co}] - \mathbf{x}^{ta} \right\|_2^2 .
\end{equation}

\subsection{Residual Learning via Diffusion Model}
Diffusion models have achieved significant success in probabilistic modeling. In this work, we employ a diffusion model for probabilistic prediction, training it to learn the residual distribution:
\begin{equation}
\label{eq:one-setp-forward}
    \mathbf{r}_{ta}=\mathbf{x}^{ta}-\mathbb{E}_{\theta}[\mathbf{x}^{ta}|\mathbf{x}^{co}].
\end{equation}
Consequently, the target data \(\mathbf{x}^{ta}\) for diffusion models in Eqs.~\eqref{eq:one-setp-forward}, \eqref{eq:inference}, and \eqref{eq:loss1} is replaced by \(\mathbf{r}_{ta}\).
The residual distribution of urban spatiotemporal data is not independently and identically distributed (i.i.d.) nor does it follow a fixed distribution, such as \(\mathcal{N}(0, \mathbf{\sigma})\). Instead, it often exhibits complex spatiotemporal dependence and heterogeneity. So we consider both temporal residual learning and spatial residual learning. 




\subsubsection*{\textbf{Temporal Residual Learning.}} 
For urban spatiotemporal data, the residual distribution varies at different time points. For example, fluctuations are lower at night and higher during the day, with uncertainty varying between weekends and weekdays. To model this, we incorporate the timestamp information as the condition for the denoising process. Meanwhile, the residual distribution can also be affected by sudden weather changes or public events. To capture these real-time changes, we concatenate the context data $\mathbf{x}^{co}_0$ with noised target residual $\mathbf{r}^{ta}_n$ as the input. The noise is not added to $\mathbf{x}^{co}_0$ and $\mathbf{r}^{ta}_n$ during the diffusion training and inference.




\subsubsection*{\textbf{Spatial Residual Learning.}}
In areas with frequent traffic accidents, fluctuations tend to be more pronounced and may induce anomalous variations in adjacent regions, thus affecting their distributions.
For spatial dependence modeling, the model learns a spatial embedding for each location, following the STID approach. Additionally, we propose a scale-aware diffusion process to further distinguish the heterogeneity for different regions. In this section, we detail the calculation of \(\mathbf{Q}\) and how it is integrated into the scale-aware diffusion process.

\noindent\textbf{(i) Customized Fluctuation Scale.} Specifically, we apply the Fast Fourier Transform (FFT) to spatiotemporal sequences in the training set to quantify fluctuation levels in different regions and use the custom scale \(\mathbf{Q}\) as input to account for spatial differences in residual. Specifically, we first employ FFT to extract the fluctuation components for each region within the training set. The detailed steps are as follows:









\begin{equation}
    \begin{aligned}
    & \mathbf{A}_{\mathrm{k}} = \left| \text{FFT}(\mathbf{x})_\mathrm{k} \right|, \quad \mathbf{{\phi}}_{\mathrm{k}} = \mathbf{\phi} \left( \text{FFT}(\mathbf{x})_\mathrm{k} \right), \\
    & \mathbf{A}_{\text{max}}=\max_{\mathrm{k}\in\left\{1,\cdots,\left\lfloor\frac{\mathbf{L}}{2}\right\rfloor + 1\right\}}\mathbf{A}_{\mathrm{k}}, \\
    & \mathcal{K} = \left\{ \mathrm{k} \in \left\{ 1, \cdots, \left\lfloor \frac{{L}}{2} \right\rfloor + 1 \right\} : \mathbf{A}_{\mathrm{k}} < 0.1 \times \mathbf{A}_{\text{max}} \right\}, \\
    & \mathbf{x}_{\mathbf{r}}[i] = \sum_{\mathrm{k} \in \mathcal{K}} \mathbf{A}_{\mathrm{k}} \Big[ \cos \left( 2\pi \mathbf{f}_{\mathrm{k}} i + \mathbf{\phi}_{\mathrm{k}} \right) \\
    & \qquad \qquad + \cos \left( 2\pi \bar{\mathbf{f}}_{\mathrm{k}} i + \bar{\mathbf{\phi}}_{\mathrm{k}} \right) \Big],
    \end{aligned}
\end{equation}
where \(\mathbf{A}_{\mathrm{k}},\mathbf{\phi}_{\mathrm{k}}\) reprent the amplitude and phase of the $\mathrm{k}-$th frequency component. $L$ is the temporal length of the training set. \(\mathbf{A}_{\text{max}}\) is the maximum amplitude among the components, obtained using the $\max$ operator. $\mathcal{K}$ represents the set of indices for the selected residual components. \(\mathbf{f}_{\mathrm{k}}\) is the frequency of the \(\mathrm{k}\)-th component. $\bar{\mathbf{f}}_{\mathrm{k}}, \bar{\mathbf{\phi}}_{\mathrm{k}}$ represent the conjugate components. \(\mathbf{x}_{\mathbf{r}}\) ref to the extracted residual component of the training set. We then compute the variance $\sigma^2_k$ of the residual sequence for each location $k$ and expand it to match the shape as 
\(\mathbf{r}^{ta}_0 \in \mathbb{R}^{B \times K \times P}\) , where $B$ represents the batch size. And we can get the variance tensor \(\mathcal{M}\): 
\begin{equation}
\begin{aligned}
    &\mathcal{M}_{b,k,p}=\sigma_{k}^2,\\
&\forall b\in\{1,\cdots,B\}, \forall k\in\{1,\cdots,K\}, \forall p\in\{1,\cdots,P\}.
\end{aligned}
\end{equation}
The residual fluctuations are bidirectional, encompassing both positive and negative variations, so we generate a random sign tensor \(\mathbf{S}\in\mathbb{R}^{B\times K\times P}\) for \(\mathcal{M}\), where each element \(S_{b,k,p}\) of \(\mathbf{S}\) is sampled from a Bernoulli distribution with \(p = 0.5\). 
%That is, \(r_{b,k,p}\) takes the value $1$ with probability $0.5$ and $-1$ with probability $0.5$. 
The customized fluctuation scale \(\mathbf{Q}\) is then defined as:
\begin{equation}
\begin{aligned}
&\mathbf{Q}_{b,k,p}=S_{b,k,p}\times\mathcal{M}_{b,k,p},\\
&\forall b\in\{1,\cdots,B\}, \forall k\in\{1,\cdots,K\}, \forall p\in\{1,\cdots,P\}.
\end{aligned}
\end{equation}
Then \(\mathbf{Q}\) is used as the input of the denoising network. 





\noindent\textbf{(ii) Scale-aware Diffusion Process.}

The vanilla diffusion models typically model all regions as the same \(\mathcal{N}(0, I)\) distribution at the end of the diffusion process, failing to distinguish the differences among regions. To further model the differences of residual distribution across various regions, we adopt the technique proposed by~\cite{han2022card} to make the residual learning region-specific conditioned on \({\mathbf{Q}}\). Specially, we have calculated the customized fluctuation scale \({\mathbf{Q}}\), and We redefined the noise distribution at the endpoint of the diffusion process as follows:
\begin{equation}
    p(\mathbf{r}^{ta}_N)=\mathcal{N}({\mathbf{Q}},I),
\end{equation}
Accordingly, the Eq~\ref{eq:new one-step} in the forward process is rewritten as:
\begin{equation}
\label{eq:new one-step}
    \mathbf{r}_n^{ta} = \sqrt{\bar{\alpha}_n} \mathbf{r}_0^{ta}+(1-\sqrt{\bar{\alpha}_n})\mathbf{Q} + \sqrt{1 - \bar{\alpha}_n} \mathbf{\epsilon}, \quad \mathbf{\epsilon} \sim \mathcal{N}(0, I).
\end{equation}
And in the denoising process, we sample \(\mathbf{r}_N^{ta}\) from $\mathcal{N}({\mathbf{Q}},I)$, and denoise it use Eq~(\ref{eq:inference}), the computation of \(\mu_{\theta}(\mathbf{r}_n^{ta}, n| \mathbf{x}_0^{co})\) in Eq~\ref{eq:inference} is modified as:
\begin{equation}
\label{eq: mu}
    \mu_{\theta}(\mathbf{r}_n^{ta}, n| \mathbf{x}_0^{co})=\frac{1}{\sqrt{\bar{\alpha}_n}} \left( \mathbf{r}_n^{ta} - \frac{\beta_n}{\sqrt{1 - \bar{\alpha}_n}} \mathbf{\epsilon}_{\theta}(\mathbf{r}_n^{ta}, n| \mathbf{x}_0^{co}) \right)+(1-\frac{1}{\sqrt{\bar{\alpha}_n}})\mathbf{Q}.
\end{equation}
This approach enables the diffusion process to be governed by the \(\mathbf{Q}\) values at each region, leading to more effective utilization of the customized scale conditions.


\subsection{Training and Inference}
\input{Algorithm/train}
\input{Algorithm/infer}
\subsubsection*{\textbf{Training}}
Our training process is a two-stage procedure. We first pretrain the deterministic model STID to enable it to predict the conditional mean. Subsequently, we train the diffusion mode to learn the distribution of residuals, where the residuals are calculated as the difference between the true values and the conditional mean predicted by the pretrained STID model with frozen parameters. The detailed training procedure is presented in Algorithm~\ref{al: train}.
\subsubsection*{\textbf{Inference}}
The inference process of the model consists of two paths: one utilizes the pretrained STID model to predict the conditional mean, while the other uses the diffusion model to predict the residuals. The final sample is obtained by summing the results of both paths. This process is detailed in Algorithm~\ref{al: sampling}.