\section{Related Work}
\subsection{Urban Spatiotemporal Prediction}

Spatiotemporal prediction in urban environments~\cite{zhang2017deep, yuan2024unist, yu2017spatio} aims to model and forecast the dynamics of urban data, such as traffic flow, cellular network traffic, and energy consumption. Previously, researchers have employed statistical methods (e.g., ARIMA~\cite{bouznad2020trend}) and traditional machine learning models (e.g., SVM~\cite{ohashi2012wind}, GP~\cite{senanayake2016predicting}, and RF~\cite{hengl2018random}) for prediction. Recently, a variety of neural network-based models have been proposed for this task, which can be categorized into deterministic and probabilistic prediction. 

\subsection{Deterministic  Prediction}

Deterministic prediction of spatiotemporal data is a point estimation approach that assumes a deterministic prediction can be made given the same historical observations. These models are typically trained using loss functions such as MSE or MAE, ensuring that the model learns to predict the conditional mean \(\mathbb{E}[y|x]\) to capture the primary patterns. Existing deep learning-based deterministic forecasting models include MLP-based~\cite{shao2022spatial,qin2023spatio,zhang2023mlpst}, CNN-based~\cite{li2017diffusion,liu2018attentive,zhang2017deep}, and RNN-based~\cite{bai2019passenger,lin2020self,wang2017predrnn,wang2018predrnn++} architectures, which are commonly used for their computational efficiency. Additionally, GNN-based models~\cite{bai2019stg2seq,bai2019stg2seq,bai2020adaptive,geng2019spatiotemporal,jin2023spatio} have been developed to capture spatial dependencies in graph-structured data, while Transformer-based models~\cite{chen2022bidirectional,chen2021s2tnet,jiang2023pdformer,yu2020spatio} excel at capturing complex dynamic patterns. 

\subsection{Probabilistic  Prediction}


The core of probabilistic prediction lies in constructing a conditional probability distribution \( p(y|x) \), which quantifies the uncertainty of the prediction rather than merely providing a single deterministic value~\cite{yang2024survey,tashiro2021csdi}. Unlike deterministic prediction models, these models 
%focus on capturing the randomness of spatiotemporal data, such as measurement noise and environmental interference, and 
assess the reliability of the predictions through probabilistic representations like quantiles or confidence intervals. Early applications typically employ Bayesian networks. In recent years, with the development of generative models, approaches based on GAN~\cite{jin2022gan,saxena2019d,zhang2021satp}, VAE~\cite{chen2021learning,de2022vehicles,zhou2020variational}, and diffusion models~\cite{tashiro2021csdi,chai2024diffusion,lin2024diffusion} have made significant advancements, owing to their powerful data distribution modeling capabilities. However, deterministic models still dominate in related research, and probabilistic prediction models remain relatively underexplored. This study aims to leverage the strengths of both deterministic and probabilistic models to achieve efficient probabilistic predictions.



\section{Preliminary}
\subsection{Problem Formulation}
Urban spatiotemporal prediction aims to predict future variations based on historical observations. The urban spatiotemporal data is typically represented as a multi-dimensional tensor \(\mathbf{x} \in \mathbb{R}^{T \times K \times C}\), where \(T\) denotes the temporal dimension, \(K\) represents the spatial dimension, and \(C\) is the feature dimension. The format typically includes two types: grid-based data and graph-based data.


%\subsubsection*{\textbf{Grid Data.}} 

For grid-based data, the spatial dimension \(K\) can be expressed in a two-dimensional form as \(H\times W\), where \(H\) and \(W\) denote the height and width, respectively. 


%\subsubsection*{\textbf{Graph Data.}} 
For graph-based data, \(K\) represents the number of nodes in the graph structure. These nodes are located within a spatial topological structure denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E},\mathcal{A})$. Here, $\mathcal{V}$ is the set of all nodes. $\mathcal{E}$ is the set of edges connecting the nodes. In addition, $\mathcal{A}$ is the adjacency matrix of graph $\mathcal{G}$. Its elements $a_{ij}$ show if there's an edge between node $i$ and $j$ in $\mathcal{V}$, $a_{ij}=1$ when there's an edge and $a_{ij}=0$ otherwise.





\subsection{Diffusion-based Probabilistic Prediction}
The diffusion-based urban spatiotemporal model is a conditional probabilistic model, consisting of a forward process and a reverse process. In the forward process, noise is added incrementally to target data \(\mathbf{x}^{ta}_0\) according to a predefined noise schedule \(\{\beta_n\}_{t=1}^{N}\), gradually transforming the data distribution into a standard Gaussian distribution \(\mathcal{N}(\mathbf{0}, \mathbf{I})\). At any diffusion step, the corrupted target data can be computed using the one-step forward equation:
\begin{equation}
\label{eq:one-step-forward}
    \mathbf{x}_n^{ta} = \sqrt{\bar{\alpha}_n} \mathbf{x}_0^{ta} + \sqrt{1 - \bar{\alpha}_n} \mathbf{\epsilon}, \quad \mathbf{\epsilon} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}),
\end{equation}
where \(\bar{\alpha}_n=\prod_{i=1}^{n}\alpha_i\) and \(\alpha_n=1-\beta_n\). In the reverse process, we first sample \(\mathbf{x}^{ta}_N\) from the standard Gaussian distribution \(\mathcal{N}(\mathbf{0}, \mathbf{I})\) and denoise it through the following Markov process:
\begin{equation}
\label{eq:inference}
\begin{aligned}
    &p_{\theta}(\mathbf{x}_{0:N}^{ta}) := p(\mathbf{x}_N^{ta}) \prod_{n=1}^{N} p_{\theta}(\mathbf{x}_{n-1}^{ta} | \mathbf{x}_n^{ta}, \mathbf{x}_0^{co}), \\
    &p_{\theta}(\mathbf{x}_{n-1}^{ta} | \mathbf{x}_n^{ta}) := \mathcal{N}(\mathbf{x}_{n-1}^{ta}; \mu_{\theta}(\mathbf{x}_n^{ta}, n| \mathbf{x}_0^{co}), \Sigma_{\theta}(\mathbf{x}_n^{ta}, n)),\\
    &\mu_{\theta}(\mathbf{x}_n^{ta}, n| \mathbf{x}_0^{co})=\frac{1}{\sqrt{\bar{\alpha}_n}} \left( \mathbf{x}_n^{ta} - \frac{\beta_n}{\sqrt{1 - \bar{\alpha}_n}} \mathbf{\epsilon}_{\theta}(\mathbf{x}_n^{ta}, n| \mathbf{x}_0^{co}) \right)
\end{aligned}
\end{equation}
where the variance $\Sigma_{\theta}(\mathbf{x}_n^{ta}, n)=\frac{1 - \bar{\alpha}_{n-1}}{1 - \bar{\alpha}_n} \beta_n$, and $\mathbf{\epsilon}_{\theta}(\mathbf{x}_n^{ta}, n| \mathbf{x}_0^{co})$ is predicted by the denoising network which is trained by the loss function below:
\begin{equation}
\label{eq:loss1}
   \mathcal{L}(\theta)  = \mathbb{E}_{n, \mathbf{x}_0, \mathbf{\epsilon}} \left[ \left\| \mathbf{\epsilon} - \mathbf{\epsilon}_{\theta}(\mathbf{x}_n^{ta}, n| \mathbf{x}_0^{co}) \right\|_2^2 \right].
\end{equation}


\begin{figure*}[t]
    \centering
    \includegraphics[width=0.95\linewidth]{Figures/model.pdf}
    \caption{The overview of CoST. (a) illustrates the pretraining process of the deterministic prediction model. (b) demonstrates the computation of the  Customized Fluctuation Scale. (c) presents the overall framework of the mean-residual decomposition.}
    \label{fig:model}
\end{figure*}


\subsection{Evaluation of Probabilistic Prediction}


To comprehensively evaluate probabilistic prediction, it is essential to consider two key dimensions: 
\begin{itemize}[leftmargin=*]
    \item \textbf{Data Distribution}—the predicted distribution should align with the empirical data distribution.
    \item \textbf{Prediction Usability}—A reliable prediction interval has a high probability of containing the true future values, while a sharp interval provides a more precise estimate. 
\end{itemize}

Existing studies predominantly rely on Continuous Ranked Probability Score (CRPS)  to provide a measure of the distribution discrepancy, while deterministic metrics such as MAE and RMSE to capture point estimation errors. However, these metrics fail to systematically evaluate: \textbf{(i)} Whether the learned distribution accurately covers the data across different quantile intervals, particularly when dealing with multi-modal distributions; \textbf{(ii)} Whether the interval width appropriately reflects real uncertainty. To address these challenges, we introduce two more evaluation metrics, Quantile Interval Coverage Error (QICE) and Interval Score (IS).



\subsubsection*{\textbf{QICE}} QICE is first proposed in~\cite{han2022card} for regression tasks. This metric is intended to assess the mean absolute deviation between the proportion of true data points within each quantile interval (QI) and the ideal proportion. Specially, for each sample point \(y\), after obtaining a sufficient number of predicted values \(\hat{y}\), these predictions are divided into \(M_{\text{QIs}}\) equally-sized QIs. Then the QICE is computed according to the following formula:
\begin{equation}
\begin{aligned}
    &\text{QICE} := \frac{1}{M_{\text{QIs}}} \sum_{m=1}^{M_{\text{QIs}}} \left| r_m - \frac{1}{M_{\text{QIs}}} \right|,\\
    &r_m = \frac{1}{N} \sum_{n=1}^{N} \mathbb{1}_{y_n \geq \hat{y}_n^{\text{low}_m}} \cdot \mathbb{1}_{y_n \leq \hat{y}_n^{\text{high}_m}},
\end{aligned}
\end{equation}
where $\hat{y}_n^{\text{low}_m}$ and $\hat{y}_n^{\text{high}_m}$ represents the lower and upper bound of the $m-$th quantile interval for $y_n$. When the true distribution aligns with the predicted distribution, \(\frac{1}{M_{\text{QIs}}}\) of the observation data will fall into each QI, achieving an optimal QICE value of 0. Smaller values of QICE indicate a closer match between the learned distribution and the true distribution.


\subsubsection*{\textbf{IS}}
The IS is a scoring metric designed to evaluate the quality of probabilistic prediction intervals (PIs). It is defined and systematically studied in~\cite{gneiting2007strictly}. This metric jointly considers two critical aspects: the sharpness and coverage ability of the prediction interval. The computation of IS can be formulated as follows:
\begin{equation}
\begin{aligned}
&IS:= \frac{1}{N} 
\sum_{n=1}^{N}\left[(u_n^\alpha - l_n^\alpha) + \frac{2}{\alpha}(l_n^\alpha - y_n)\mathbb{1}_{y_n < l_n^\alpha} + \frac{2}{\alpha}(y_n - u_n^\alpha)\mathbb{1}_{y_n > u_n^\alpha}\right],
\end{aligned}
\end{equation}
where $u_n^\alpha$ and $l_n^\alpha$ are the upper and lower endpoints of the central $(1 - \alpha)\times100\%$ prediction interval for the $n$-th data point, which are the predictive quantiles at levels $1-\frac{\alpha}{2}$ and $\frac{\alpha}{2}$ respectively; $y_n$ is the actual value corresponding to the $n$-th data point. The final prediction results are rewarded for having a narrower prediction interval. If the observed value falls outside the prediction interval, a penalty is incurred, and the severity of the penalty is determined by the parameter $\alpha$. Thus, a lower IS indicates better performance. 






























