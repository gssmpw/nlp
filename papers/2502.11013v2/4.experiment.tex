\section{Performance Evaluations}
\input{Tables/short_term1}
\input{Tables/short-term2}


\subsection{Experimental Settings}


\subsubsection*{\textbf{Datasets.}}

To validate the effectiveness and generalizability of our method, we use seven real-world datasets across four categories, each with different spatiotemporal granularities, covering information on crowd flow (CrowdBJ and CrowdBM), cellular network traffic (CellularNJ and CellularSH), vehicle flow (TaxiBJ and BikeDC), and traffic speed (Los-Speed). All datasets are split into training, validation, and test sets in a 6:2:2 ratio, and all datasets are standardized during training. Detailed information on the datasets can be found in Table~\ref{tbl:append_data}.









\subsubsection*{\textbf{Baselines.}}
we select six of the most widely recognized and state-of-the-art models in the spatiotemporal domain as our baselines, including:
\begin{itemize}[leftmargin=*]
    \item \textbf{D3VAE~\cite{li2022generative}:} Aims at short-period and noisy time series forecasting. It combines generative modeling with a bidirectional variational auto-encoder, integrating diffusion, denoising, and disentanglement.
    \item \textbf{DiffSTG~\cite{wen2023diffstg}:} First applies diffusion models to spatiotemporal graph forecasting. By combining STGNNs and diffusion models, it reduces prediction errors and improves uncertainty modeling.
    \item \textbf{TimeGrad~\cite{rasul2021autoregressive}:} An autoregressive model based on diffusion models. It conducts probabilistic forecasting for multivariate time series and performs well on real-world datasets.
    \item \textbf{CSDI~\cite{tashiro2021csdi}:}Utilizes score-based diffusion models for time series imputation. It can leverage the correlations of observed values and also shows remarkable results on prediction tasks.
    \item \textbf{Dyffusion~\cite{ruhling2023dyffusion}:} A training method for diffusion models in probabilistic spatiotemporal forecasting. It combines data temporal dynamics with diffusion steps and performs well in complex dynamics forecasting.
    \item \textbf{NPdiff~\cite{sheng2025unveiling}:} A general noise prior framework for cellular traffic prediction. It uses the data dynamics to calculate noise prior for the denoising process and achieve effective
    performance.
\end{itemize}
Since the dataset contains some data with a grid structure, we construct adjacency matrixes based on the adjacency relationships to support baseline models that rely on graph structures. 


\subsubsection*{\textbf{Metrics.}} To evaluate the performance, we employed two deterministic metrics, MAE and RMSE, along with three probabilistic metrics: CRPS, QICE, IS. For QICE, we set the number of QIs, denoted as \(M_{\text{QIs}}\), to 10. For IS, we choose a confidence level of 90\%, that is, \(\alpha = 0.1\). 

\subsubsection*{\textbf{Experimental Configuration.}} 

In our experiment, for our model, we set the training maximum epoch for both the STID deterministic model and the diffusion model to 50, with early stopping based on patience of 5 for both models. During training, we set the initial learning rate to 0.001, and after 20 epochs, we adjust it to 4e-4. We use the Adam optimizer with a weight decay of 1e-6. For the diffusion model, we set the validation set sampling number to 3, and the average metric computed over these samples is used as the criterion for early stopping. For the baseline models,  we set the maximum training epoch to 100 and the early stopping patience also to 5. We set the number of samples to 50 for computing the experimental results presented in the paper. In terms of model architecture, we set the encoder layer number for the STID deterministic model to 4, with an embedding dimension of 32. In the denoising network of the diffusion model, we set the encoder layer number to 8, and the embedding dimension is 128. We set the maximum diffusion steps N for the diffusion model to 50, with a linear noise schedule from $\beta_1= 0.0001$ to the maximum $\beta_N= 0.5$.



\subsection{Short-Term Prediction}
\subsubsection*{\textbf{Setups.}} We define the short-term prediction task as a 12-step ahead prediction based on the previous 12 steps, following~\cite{sheng2025unveiling,wen2023diffstg}. Since the temporal granularity varies across datasets, the actual time duration corresponding to these 12 steps differs accordingly.

\subsubsection*{\textbf{Results of Probabilistic Metrics.}}
% \noindent\textbf{(i) Probabilistic Error Metrics.}
Table~\ref{tbl:short} presents the results of probabilistic metrics for selected datasets. Due to space constraints, the remaining results can be found in the Appendix Table~\ref{tbl:short1-app}. As shown in Table~\ref{tbl:short}, our model achieves the best performance across various types of datasets, including crowd flow, cellular network traffic, bike flow, taxi flow, and traffic speed. Compared to the best-performing baseline methods on each dataset, our model demonstrates an average improvement of 38\% in CRPS and QICE metrics, indicating its superior ability to accurately capture the true distribution characteristics. Moreover, our model achieves a 11\% improvement in the IS metric, suggesting that its prediction intervals not only maintain compactness but also exhibit higher coverage, thereby better reflecting the uncertainty of data. Although certain individual metrics may not reach the optimal level on specific datasets, our model consistently maintains performance comparable to the best methods. TimeGrad performs better on the Los-Speed dataset, likely due to its lower dynamic nature and smaller fluctuations, which suits autoregressive models better.

\input{Tables/long-term1}
\input{Tables/long-term2}

\subsubsection*{\textbf{Results of Deterministic Metrics.}}
We also conduct a comparative analysis of the deterministic error between the predicted sample mean and the observation values to assess the offset between the predicted center and the ground truth. Table ~\ref{tbl:short3} presents a subset of the results, with more detailed information available in Appendix Table~\ref{tbl:short1-app}. The experimental results demonstrate that our method consistently outperforms others across most datasets, achieving an average reduction of 7\% in MAE and 4.5\% in RMSE. This indicates that, with the support of the current advanced conditional mean prediction model, our approach can effectively capture the primary patterns compared to other probabilistic baseline models.



\subsection{Long-Term Prediction}
\subsubsection*{\textbf{Setups.}}
In addition to the 12-step prediction task which is the most typical setup for urban spatiotemporal prediction, we also conduct long-term prediction experiments on four. We define the long-term prediction task as a 64-step ahead prediction based on the previous 64 steps, following~\cite{yuan2024unist,jin2023time,nie2022time}.

\subsubsection*{\textbf{Results of Probabilistic Metrics.}} As shown in Table ~\ref{tbl:long1}, our model demonstrates exceptional long-term prediction capability. It is worth noting that our model is based on an MLP architecture. Although some individual metrics are slightly inferior to CSDI based on Transformer, which excels at modeling long-term dependencies, our model outperforms them overall. Furthermore, in terms of training efficiency and inference speed, it significantly outperforms it, as detailed in Section~\ref{sc:efficiency}.

\subsubsection*{\textbf{Results of Deterministic Error Metrics.}} As shown in Table~\ref{tbl:long2}, our model also significantly outperforms other models on deterministic metrics. Compared to the best baseline, our approach achieves an average reduction of 6.5\% in MAE and 3.45\% in RMSE.


\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/ablation.pdf}
    \caption{Ablation study on the CrowdBJ and CrowdBMcomparing variants in terms of (a) CRPS and (b) IS.}
    \label{fig:ablation}
    \vspace{-3mm}
\end{figure}


\subsection{Ablation Study}



we conduct an ablation on each proposed module in our model to further evaluate the influence. Specifically, we constructed three variant models by progressively removing key components from the original architecture:\textbf{(w/o c):} The scale-aware diffusion process module is removed; \textbf{(w/o q):} The customized fluctuation scale is excluded as a prior condition;\textbf{(w/o m):} The conditional mean prediction module is eliminated, allowing the model to rely solely on the diffusion model for training.
We conducted experiments on two datasets and visualize the results in terms of CRPS and IS metrics, as illustrated in Figure~\ref{fig:ablation}. The results demonstrate that incorporating the conditional mean prediction module significantly enhances model performance, indicating that it effectively captures the primary spatiotemporal pattern and alleviates the burden on the diffusion model to capture the full data distribution. Furthermore, incorporating the customized fluctuation Scale as a prior condition further improves prediction performance, suggesting that it provides valuable fluctuation information across different regions. Finally, the scale-aware diffusion process enables the model to better utilize this condition.



\subsection{Model Efficiency}
\label{sc:efficiency}
\input{Tables/efficiency}
We evaluated the training time of our model and all baseline models on the CellularSH dataset, as well as the time required to perform 50 sampling iterations on the test set. Notably, the pretraining time of the mean prediction module in our model is also included in the evaluation. The experimental results are summarized in Table~\ref{tbl:traintime}. As shown in Table~\ref{tbl:traintime}, our model demonstrates a significant advantage over existing probabilistic models in terms of training and inference efficiency. This efficiency is particularly valuable for real-world deployment scenarios. It is worth noting that while the CSDI model benefits from the Transformer architecture's capability to capture spatiotemporal dependencies, its training and sampling time is considerably higher than that of other models, posing challenges for time-sensitive tasks.



% 推理速度 和 训练速度 比较
% 抗噪声实验


\subsection{Analysis of Distribution Alignment}
For regions with complex distributions in the CellularSH dataset, we validate the model's ability to capture multi-modal characteristics by comparing the predicted and true distributions using Kernel Density Estimation (KDE). As shown in Figure~\ref{fig:KDE}, the real distribution exhibits significant spatiotemporal multi-modality: Figure~\ref{fig:KDE}(a) shows three peaks, which likely represent values at different time points and values under different states at the same time points. CoST successfully captures the three peaks, while CSDI only fits two, demonstrating superior capability in modeling multimodal distributions. In Figure~\ref{fig:KDE}(b), the difference between the two peaks reflects strong temporal dynamics in this region. Although both models predict two peaks, the peak spacing of CoST is more consistent with the true distribution. These advantages stem from the model's design: the diffusion model focuses on residuals, effectively capturing multimodal distribution features, while the deterministic model captures long-term trends and enhances the model's capacity to model primary dynamic patterns.


\begin{figure}[t]

    \centering
    \includegraphics[width=\linewidth]{Figures/KDE_MobileSH14.pdf}
    \vspace{-6mm}
    \caption{KDE plots of CellularSH dataset for different regions: (a) Location 182, (b) Location 520.}
    \label{fig:KDE}
    \vspace{-5mm}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/Case2_2.pdf}
    \vspace{-5mm}
    \caption{PIT analysis on the CellularSH dataset: (a) PIT histogram and (b) PIT empirical CDF.}
    \label{fig:PIT}
    \vspace{-3mm}
\end{figure}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/estimation_crowdbj.pdf}
    \vspace{-5mm}
    \caption{Visualizations of predictive uncertainty for both CSDI and our model on the CrowdBJ dataset. The shaded regions represent the 90\% confidence interval, derived from 50 independent sampling runs. The dashed lines denote the median of the predicted values for each model.}
    \label{fig:estimation}
\end{figure}
% 不确定性预测效果图
% PICP 的变化示意图（挑出来具体的样本）
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figures/Case2_1.pdf}
    \vspace{-3mm}
    \caption{PICP comparison between our model and CSDI on CrowdBM and CellularSH.}
    \vspace{-5mm}
    \label{fig:PICP}
\end{figure}



Additionally, we present the PIT (Probability Integral Transform) histogram in Figure~\ref{fig:PIT} (a) and the PIT empirical cumulative distribution function (CDF) in Figure~\ref{fig:PIT} (b) to visually reflect the alignment of the full distribution. Ideally, the true values' quantiles in the predictive distribution should follow a uniform distribution, corresponding to the dashed line in Figure~\ref{fig:PIT} (a). In the case of perfect calibration, the PIT CDF should closely resemble the yellow diagonal line. Clearly, our model outperforms CSDI.


\subsection{Analysis of Prediction Quality}

To intuitively demonstrate the effectiveness of our predictions, we visualize the prediction results on the CrowdBJ dataset, as shown in Figure~\ref{fig:estimation}. Here, we compare our model with the best baseline model, CSDI. On one hand, thanks to the deterministic model, our model is able to more accurately capture regular spatiotemporal patterns, as demonstrated in Figures~\ref{fig:estimation} (a, c, and f). On the other hand, the diffusion model, by focusing on modeling the residual distribution, better reflects the uncertainty in predictions across different samples, as illustrated in Figures~\ref{fig:estimation} (b, d, g, and h).

In addition to sample-specific analysis, we also perform a comparative analysis of the prediction intervals against the best baseline model, CSDI. We visualize the dynamic quantile errors of the CrowdBM and CellularSH prediction results in Figure~\ref{fig:PICP}. For each confidence level $\alpha$, we compute the corresponding quantile intervals (e.g., $\frac{\alpha}{2}$ and $1-\frac{\alpha}{2}$) and calculate the proportion of real values falling within this interval, known as the Prediction Interval Coverage Probability (PICP). The closer the curve is to the diagonal (black dashed line), the better the calibration of the prediction intervals. The results show that our model achieves superior prediction interval calibration.