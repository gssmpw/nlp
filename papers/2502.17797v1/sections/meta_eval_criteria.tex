\section{Meta evaluation of human evaluations}\label{sec:meta_eval_metrics}

% \input{tables/system_ranking}

This section outlines five criteria for analyzing the human annotation results: (1) inter-annotator agreement, (2) inter-translation consistency, (3) system-level ranking, (4) segment-level ranking, and (5) error distribution. Results are in \sectionref{sec:results_and_discussion}.

\noindent\textbf{Inter-annotator agreement} (IAA) Each segment pair is annotated by three annotators, allowing for calculating the IAA. For each segment translated by system \textit{a} and \textit{b}, each annotator's annotation is categorized as \textit{a} $>$ \textit{b}, \textit{a} $=$ \textit{b}, or \textit{a} $<$ \textit{b}.
% \footnote{The conversion is based on the segment MQM scores by each annotator for \psxsmqm~and \sxsmqm. For \sxsqr, the conversion is based on the point on the scale. \texttt{Much better} and \texttt{better} are collapsed to \texttt{better}; \texttt{Much worse} and \texttt{worse} are collapsed to \texttt{worse}.} 
Krippendorff's $\alpha$ \citep{krippendorff2018content} quantifies the IAA.\footnote{In the MQM settings, two segments tie if they have the same score.}

\noindent\textbf{Inter-translation consistency} When the same error occurs in translations from multiple systems of the same source input, annotators should mark it consistently with the same span, category, and severity. This consistency is crucial for fair system comparisons and training MQM-style automatic metrics \citep{juraska-etal-2023-metricx, fernandes-etal-2023-devil}. Inter-translation consistency quantifies the degree to which annotators achieve this uniformity across translations. The detailed process for calculating this consistency is provided in \appendixref{appendix:itc_pseudo_code}.

\noindent\textbf{Agreement in segment-level rankings} Pairwise ranking agreement (PRA) \citep{deutsch-etal-2023-ties}, defined in \eqref{eq:PRA} and \tableref{tab:pra_terms} (\appendixref{appendix:pra_terms}), measures the consistency between two annotation settings in ranking translation pairs by considering agreements, disagreements, and ties to evaluate alignment between evaluation methods.\footnote{The metric is termed \textit{pairwise accuracy} in \citet{deutsch-etal-2023-ties}. However, since there is no gold reference in this work, the metric is referred to as \textit{agreement}. The word \textit{ranking} is to emphasize that the metric pertains to rankings.}

% \prcomment{Without defining the terms, the equation is less useful. Consider moving the equation to the Appendix and replacing it with a prose description, such as ``the proportion of segment translation pairs where two metrics agree on which is better (or that they are tied).''}

% \vspace{-5pt}
% \begin{equation}
%   \resizebox{0.6\hsize}{!}{$
%   PRA = \frac{C + T_{\alpha\beta}}{C + D + T_\alpha + T_\beta + T_{\alpha\beta}}
%   $}
%   \label{eq:PRA}
% \end{equation}
% \vspace{-8pt}

\noindent\textbf{Agreement in system-level rankings} Systems in each pair in \tableref{tab:sys_pairs} are ranked pairwise based on scores calculated as detailed in \sectionref{sec:result_score_calc}. A random permutation test is applied to the segment-level scores of the paired systems to evaluate the statistical significance of observed differences.

\noindent\textbf{Error distribution} For the two MQM settings, one possibility is that they identify more/fewer errors of particular kinds. To explore this, the total number of target-side errors annotated across all 3 ratings was counted for each language pair and MQM setting by category and severity.%\footnote{It is possible that different annotators annotate the same error in a segment. For the error distribution calculation, such errors are not deduplicated.}

% The next section presents the results of the meta evaluation of the human annotations.