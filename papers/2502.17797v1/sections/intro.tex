\section{Introduction}\label{sec:intro}
% With the rapid advancement of more powerful language models, the quality of their generated outputs has significantly improved, often surpassing the capabilities of existing automatic evaluation metrics \citep{karpinska-iyyer-2023-large, pham2024suri}. As a result, human evaluation continues to play a vital role in assessing models' performance.
With the rapid improvement of large language models' capabilities, automatic evaluation metrics have struggled to reliably measure their quality \citep{karpinska-iyyer-2023-large, pham2024suri}. As a result, human evaluation continues to play a vital role in assessing models' performance.

Human annotations can be influenced by several difficult-to-control factors, such as annotators' proficiency and their relative leniency or stringency. Annotator proficiency can be managed by hiring experts \citep{karpinska-etal-2021-perils, krishna-etal-2023-longeval}. Varying degrees of leniency or stringency can be mitigated by carefully assigning tasks to annotators in a structured manner \citep{riley-etal-2024-finding}. 
%However, there can be other factors that affect human behavior, for example, annotation setups. 
However, other factors can affect rater behavior, such as the specific annotation task used to measure quality \cite{belz-kow-2010-comparing}.

% However, despite clear guidelines, annotators often apply their own interpretations of the standards, leading to inconsistencies and subjective variations both intra- and inter-annotators (\citealp{ACJ}; \sectionref{sec:inter-TC}).

This work investigates the influence of annotation settings on annotator behavior and results by using Chinese to English (\ZhEn) and English to German (\EnDe) machine translation (MT) as a case study. It examines three annotation settings: (1) the state-of-the-art point-wise MT annotation setup \textbf{MQM} \citep{lommel2014mqm, freitag-etal-2021-experts} where annotators see one translation at a time and identify errors with category and severity assignment of each, (2) \textbf{side-by-side} (\sxs) \textbf{MQM}, where annotators see two translations of the same input at a time and give fine-grained error annotations as MQM, (3) \textbf{\sxs~relative ranking} (RR), where annotators see two translations and decide which one is better, without error annotation. The latter two settings incorporate comparative judgment \citep{LCJ1927}, a pair-wise setting that allows annotators to make relative assessments between system outputs; in \sxsqr, it assists annotators in making comparisons, while in \sxsmqm~it helps them detect errors more easily, particularly those appearing in only one output. Comparative judgment has been shown to reduce subjectivity and enhance consistency in quality judgments \citep{karpinska-etal-2021-perils, de-Moira-2022, Jones-2024}. The three settings are illustrated in \figureref{fig:figure1}.

% \sxsmqm~and \sxsqr~incorporate comparative judgment \citep{LCJ1927} \prcomment{Does SxS MQM really qualify as comparative judgment? Does that term imply a ranking/preference judgment? Is "pairwise" what we're looking for?} that allows annotators to make relative assessments between system outputs and has been shown to reduce subjectivity and enhance consistency in quality judgments \citep{karpinska-etal-2021-perils, de-Moira-2022, Jones-2024}. The three settings are illustrated in \figureref{fig:figure1}.

\begin{figure*}
    \centering
    \includegraphics[scale=0.63]{images/fig1.pdf}
    \caption{Illustration of the three annotation settings studied in this work (\sectionref{sec:three_studied_settings}). The grey-highlighted text
    % \prcomment{The grey highlights are not really visible when printed, even in color}
    is the segment to be annotated within their context. In {\color{blue}\textbf{single-sided}} and {\color{yellow}\textbf{side-by-side}} MQM, annotators mark error spans and assign error category with severity. The score of a segment/document is determined by the category and severity of its error(s). In {\color{green}\textbf{side-by-side relative ranking}}, annotators read two translations and choose the (much) better side or decide if they tie, without labelling errors. The scoring scheme of each setting is in \sectionref{sec:result_score_calc}.
    }
    \label{fig:figure1}
\end{figure*}

% address and answer to the following questions:

% \begin{itemize}[label=-, left=0pt, itemsep=0pt]
%     \item \sectionref{sys_ranking}--\sectionref{sec:pairwise_ranking_agreement}: How well do MQM, \sxsmqm, and \sxsqr~agree on the system and segment rankings?
    
%     The three settings yield the same system rankings in most cases. When they diverge, the differences are not significant. At the segment level, the settings agree with each other moderately when they share a common feature (e.g., MQM or \sxs).
    
%     \item \sectionref{sec:inter-AA}: Do the \sxs~settings increase inter-annotator agreement?
    
%     \sxsmqm~slightly increase inter-annotator agreement compared to MQM. Annotators achieve fair agreement in all settings in terms of Krippendorff's $\alpha$.
    
%     \item \sectionref{sec:inter-TC}: Do \sxsmqm~increase inter-translation consistency? 
    
%     If an error occurs in multiple translations, annotators should mark them consistently. \sxsmqm~elicit a significantly higher inter-translation consistency even for translations that are not annotated \sxs.
    
%     \item \sectionref{sec:error_distr}--\sectionref{sec:error_cate_consistency}: Do MQM and \sxsmqm~return a similar error distribution?
    
%     \sxsmqm~has the potential to elicit more accuracy error annotation and less emphasis on fluency and style errors. 
% \end{itemize}

% The current study is the first to systematically investigate the effect of comparative judgment and detailed error detection on machine generated outputs. By evaluating the settings from various aspects, this work provides insights into how different annotation setups influence the evaluation of model outputs. Overall, if comprehensive error identification is key, detailed error detection like MQM is preferable as it comprehensively detects errors. For consistent and precise error annotation, a \sxs~error labelling setup like \sxsmqm~is recommended, though it may inadvertently steer annotators' focus onto more significant differences between annotated items. For system ranking alone, \sxsqr~performs similarly to the more complex methods but takes only a third of the time. These findings are expected to guide the design of future annotation frameworks by highlighting the trade-offs in accuracy, consistency, and subjectivity reduction when using comparative versus traditional evaluation methods.

This work meta-evaluates human annotation results from the studied annotation settings across five aspects: inter-annotator agreement, inter-translation error marking consistency, segment- and system-level quality rankings, and error distribution in MQM and \sxsmqm. Overall, our human annotation results reveal the following relative strength/weakness of each protocol. \textbf{Comparative judgment improves inter-annotator agreement}, especially in \sxsmqm, compared to point-wise MQM. \textbf{Comparative judgment boosts inter-translation error marking consistency}, with average increases of 38.05\% for systems that were shown side-by-side, and 19.5\% for system pairs that were not. For MT system ranking, \textbf{\sxsmqm~proved more reliable in identifying equal-quality translations} while \sxsqr~offered a cost-effective alternative 
with limited ability in capturing subtle quality differences due to the lack of error annotation.\footnote{We estimate that \sxsqr~reduces costs by approximately one-third compared to MQM.}
% \prcomment{This claim is contradictory: identifying equal-quality translations is presented as good, but higher tie rates is presented as bad}, 
% limiting its ability to capture subtle differences. 
In terms of error distribution, \textbf{\sxsmqm~can affect the distribution of error categories and severities}, as seen in the higher detection rate of accuracy errors in \ZhEn~compared to MQM. Overall, the contributions of this paper are:
% \prcomment{I don't think we can claim this; let's discuss in Chat}
% In terms of error distribution, \yscomment{\ZhEn~and \EnDe~behave differently, with fluency errors more prevalent in \EnDe~and \sxsmqm~highlighting more \ZhEn~accuracy errors. The higher prevalence of accuracy errors in \ZhEn~is likely due to the linguistic differences between Chinese and English, suggesting that \textbf{\sxsmqm~tends to amplify accuracy errors in typologically distant language pairs}.}
% \textbf{\sxsmqm~highlighted more major accuracy errors in \ZhEn}, 
% \prcomment{Should this be reframed as something like "error distributions were different between (pointwise) MQM and SxS MQM"?},
% likely due to linguistic differences between Chinese and English, while \EnDe~annotations showed a higher prevalence of fluency errors. 



\vspace{-3pt}
\begin{enumerate}[label=(\arabic*), itemsep=-0.2em]
    \item It examines the  point-wise MQM, \sxsmqm, and the simplified \sxsqr, offering a systematic investigation of comparative judgment in human annotation tasks in MT;
    
    \item It offers insights that \sxsmqm~provides more reliable and fine-grained annotations while \sxsqr~provides an efficient alternative when detailed error annotation is not required.
\end{enumerate}
\vspace{-6pt}

% The human evaluation data and code will be open-sourced for future research.% \footnote{\url{https://github.com/google-research/google-research/tree/master/anthea}}