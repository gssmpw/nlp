\begin{abstract}

Human evaluation is crucial for assessing rapidly evolving language models but is influenced by annotator proficiency and task design. This study explores the integration of comparative judgment into human annotation for machine translation (MT) and evaluates three annotation setups---point-wise Multidimensional Quality Metrics (MQM), side-by-side (\sxs) MQM, and its simplified version \sxs~relative ranking (RR). In MQM, annotators mark error spans with categories and severity levels. \sxsmqm~extends MQM to pairwise error annotation for two translations of the same input, while \sxsqr~focuses on selecting the better output without labeling errors.

Key findings are: (1) the \sxs~settings achieve higher inter-annotator agreement than MQM; (2) \sxsmqm~enhances inter-translation error marking consistency compared to MQM by, on average, 38.5\% for explicitly compared MT systems and 19.5\% for others; (3) all annotation settings return stable system rankings, with \sxsqr~offering a more efficient alternative to (\sxs) MQM;
% , albeit with higher tie rates and less granularity; 
(4) the \sxs~settings highlight subtle errors overlooked in MQM without altering absolute system evaluations.

To spur further research, we release the triply annotated datasets comprising 377 \ZhEn~and 104 \EnDe~annotation examples.\footnote{Data will be available at \url{https://github.com/google/wmt-mqm-human-evaluation/tree/main/generalMT2023}.}
% doing  annotations side-by-side does not affect the absolute evaluation of individual systems but can highlight errors that are hard to find in point-wise MQM.

\end{abstract}