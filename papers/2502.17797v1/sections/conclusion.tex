\section{Conclusion}

This study uses machine translation as a case study and examines the impact of MQM, \sxsmqm, and \sxsqr~on the annotation results from five aspects: inter-annotator agreement, inter-translation error annotation consistency, quality ranking at segment- and system-levels, and error distributions. 

Incorporating comparative judgment, \sxsmqm~and \sxsqr~achieved higher inter-annotator agreement. \sxsmqm~enhanced error marking consistency both for explicitly compared system pairs and across others. Concerning \sxsqr, although it does not provide detailed error annotations, it offers an efficient and reliable alternative for system ranking provided by \sxsmqm, with a trade-off in differentiating subtle quality differences due to higher tie rates.

The findings in the paper demonstrate the value of comparative judgment in improving annotation quality and efficiency, with \sxsmqm~and \sxsqr~serving as practical alternatives to MQM, tailored to different evaluation needs.