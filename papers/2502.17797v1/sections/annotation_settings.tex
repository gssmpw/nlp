\section{Annotation settings}\label{sec:annotation_settings}

This section introduces the core concepts and methodologies used in this study. It begins with key terminology in \sectionref{sec:term}, followed by the rationale for integrating comparative judgment into annotation tasks (\sectionref{sec:comparative_judgement}) and an overview of the MQM framework in \sectionref{sec:MQM}. The three annotation settings analyzed in this work--—MQM, \sxsmqm, and \sxsqr—--are then described in detail in \sectionref{sec:three_studied_settings}. Finally, \sectionref{sec:result_score_calc} explains how annotations are converted into numeric scores.

\subsection{Terminology}\label{sec:term}

Two terms in the current work are defined here following \citet{riley-etal-2024-finding}.  A \textbf{segment} is a unit of one or multiple sentences that is highlighted at a time for annotators to focus on for annotation (i.e., the grey-highlighted text in \figureref{fig:figure1}); a \textbf{document} is a sequence of input segments (e.g., an excerpt from an article).

% \begin{itemize}[label=-, left=0pt, itemsep=0pt]
%     \item \textbf{Segment} (seg): a unit of one or multiple sentences that is highlighted at a time for annotators to focus on for annotation;
    
%     \item \textbf{Document} (doc): A sequence of input segments (e.g., an excerpt from an article).
% \end{itemize}

\subsection{Side-by-side annotation}\label{sec:comparative_judgement}

Comparative judgment \citep{LCJ1927} is a psychometric method that presents two items side by side, asking which better satisfies a given criterion. It assumes that people are more reliable when comparing two items than evaluating them individually. Studies show that comparative judgment improves the consistency and accuracy of teachers' assessment of students' writing \citep{ACJ, de-Moira-2022, Jones-2024}. 

In natural language generation, the \sxs~annotation (more commonly called \textit{pair-wise annotation}) has been used for tasks like open-ended text generation \citep{wang-etal-2023-knn, krishna-etal-2023-longeval}, machine translation \citep{karpinska-iyyer-2023-large}, and long-form question answering \citep{xu-etal-2023-critical}. However, there lacks systematic study that compares this approach with point-wise approaches in terms of annotator behavior and annotation results.

\subsection{Multidimensional quality metrics}\label{sec:MQM}

MQM is an annotation framework proposed by \citet{lommel2014mqm} and refined by \citet{freitag-etal-2021-experts}. It is the state-of-the-art annotation setting currently used by WMT \citep{freitag-etal-2023-results}. It involves marking error spans and assigning error severity and category. In this work, annotators use the error hierarchy in \tableref{tab:error_hierarchy} (\appendixref{appendix:error_hierarchy}) for error categorization. All categories can be either \major~or \minor, except for \texttt{Non-translation} which is always \major.\footnote{\texttt{Major} errors significantly alter the meaning of the source text; \minor~errors are noticeable but do not significantly alter the source meaning.} Annotators are instructed that the more precise the error spans, the more informative the annotation.



\subsection{Studied annotation settings}\label{sec:three_studied_settings}

Three annotation settings, illustrated in \figureref{fig:figure1}, are studied and compared to each other: \psxsmqm, \sxsmqm, and \sxs~RR. Annotators evaluate system outputs segment by segment, with full access to the surrounding context.

\textbf{Single-sided MQM} (i.e., point-wise MQM; henceforth, MQM), as in \sectionref{sec:MQM}, involves annotators evaluating one translation at a time. 

% is the setting described in Section~\ref{sec:MQM}. Each annotator reviews a single translation at a time.%uses the within-subject setting proposed by \citet{riley-etal-2024-finding} where all system outputs of a given input are annotated by the same annotator(s). Although annotators see one translation of a document at a time, it controls the noise from annotators as the same degree of leniency/stringency is applied to all system outputs of a given input. \citet{riley-etal-2024-finding} shows that this within-subject \psxsmqm~yields high stability of system ranking.

\textbf{\sxsmqm}~is the same as MQM with one difference: two translations (each from a different system) are shown side-by-side instead of just one. The core annotation task is otherwise unchanged.
%builds upon the within-subject \psxsmqm~with the modification that two translations from two systems are shown side-by-side to annotators, along with the input. The core annotation task is the same as \psxsmqm.
% In principle, \sxsmqm~should enhance annotators' consistency of error span and category marking, compared to MQM, by enabling direct comparisons between the two systems. % No matter what interpretation an annotator has of the guidelines at the time of annotation, the interpretation is applied to both system outputs in the same way, unlike \psxsmqm~that still leaves room for variation. 

% \textbf{\sxsqr} uses the within-subject setup without requiring error marking and categorization. 

\textbf{\sxsqr} does not require detailed error annotation. Annotators view two translations of the same input text side-by-side and rate them on a five-point scale as \texttt{(much) better} on one side or \texttt{about the same} (see \figureref{fig:figure1}). This setting evaluates whether side-by-side annotation can provide reliable system rankings without detailed error annotation.

% \vspace{6pt}

% \begin{center}
% \begin{tikzpicture}[scale=0.75]

%     % Define the scale points
%     \foreach \i/\label in {0/1, 2/2, 4/3, 6/4, 8/5} {
%         \node[circle, draw, inner sep=2pt] at (\i,0) {};
%     }

%     % Define the labels below the points
%     \node[scale=0.7] at (0,-0.8) {\texttt{Much better}};
%     \node[scale=0.7] at (2,-0.8) {\texttt{Better}};
%     \node[scale=0.7] at (4,-0.8) {\texttt{About the same}};
%     \node[scale=0.7] at (6,-0.8) {\texttt{Better}};
%     \node[scale=0.7] at (8,-0.8) {\texttt{Much better}};

%     % Draw lines between the points
%     \draw (0.15,0) -- (1.85,0);
%     \draw (2.15,0) -- (3.85,0);
%     \draw (4.15,0) -- (5.85,0);
%     \draw (6.15,0) -- (7.85,0);

% \end{tikzpicture}
% \end{center}


\subsection{Score calculation}\label{sec:result_score_calc}

Expert annotations are converted into numeric scores for system and annotation setting comparison. For MQM and \sxsmqm, the segment score is the average of the scores assigned by each annotator, with each score determined by the error severity and category (\tableref{tab:weighting-scheme} in \appendixref{appendix:scoring_scheme}). The system score is obtained by averaging the segment scores. For MQM, lower scores are better, with an error-free segment receiving a score of 0. The scores are \textit{z}-normalized following \citet{riley-etal-2024-finding}.

The scoring of \sxsqr~follows MQM in that a lower score is better. In each rated pair, the \texttt{much worse} translation segment is penalized by 2 points, and the \texttt{worse} segment by 1 point. If both translations are of similar quality, no penalty is applied. As the MQM settings, the segment score is the average of the scores assigned by each annotator. The system score is averaged over the segment scores.\footnote{We z-normalized the (\sxs) MQM scores to account for variations in score ranges across different annotators, ensuring comparability and mitigating individual annotator biases. In contrast, the \sxsqr~scores were not z-normalized since they are inherently constrained within the range of -2 to 0.}