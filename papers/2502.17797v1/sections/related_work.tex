\section{Related work}

Human evaluation has long been the gold standard for assessing both MT performance and automatic MT evaluation metrics in the WMT conferences and numerous MT studies. Over the years, it has evolved through efforts to establish more reliable and replicable methods \citep{stanchev-etal-2020-towards}.

\noindent \textbf{Human evaluation in the WMT metrics shared task} transitioned from the 5-point scale on fluency and adequacy \citep{koehn-monz:2006:WMT} to relative ranking (RR) of 5 translation sentences or phrases \citep{callison-burch-etal-2007-meta} for a better inter-/intra-annotator agreement \citep{callison-burch-etal-2007-meta, callison-burch-etal-2008-meta}. WMT16 \citep{bojar-etal-2016-results} used direct assessment (DA) \citep{graham-etal-2013-continuous, graham-etal-2016-glitters, GRAHAM_BALDWIN_MOFFAT_ZOBEL_2017}, where annotators scored translations between 0 and 100. It returns reliable evaluation when each item receives 15 or more judgments. As MT systems advance, DA struggles and mistakenly ranks high-quality human translations below machine outputs \citep{freitag-etal-2021-results}.
% However, DA requires at least 15 judgments per annotation item to achieve reliable evaluation. Furthermore, 
To improve human evaluation quality, MQM \citep{lommel2014mqm, freitag-etal-2021-experts} was introduced into WMT21 \citep{freitag-etal-2021-results}. It emphasizes the inclusion of context \citep{mathur-etal-2020-results} and the use of experts to better capture subtle differences \citep{goto-etal-2014-crowdsourcing, toral-etal-2018-attaining, Lubli2020ASO}. 

\noindent \textbf{MQM is the state-of-the-art MT evaluation method}, but it has shortcomings---disagreement in marking error span boundaries, category, and severity \citep{lommel2014assessing}. By introducing \sxsmqm, the current work aims to address those issues. \citet{kocmi2024esa} aim to mitigate the impact of these issues through Error Span Annotation, a point-wise annotation setting where annotators first identify error spans (with severity) in a segment before assigning it an overall score. The segment-level scores are different from MQM in two ways: first, the scores are assigned to measure the amount of meaning preserved in translations (as in their Figure 1); second, the scores are not automatically calculated from errors, which may introduce subjectivity and latency.

\noindent \textbf{Pairwise evaluation offers a simpler and more intuitive approach to MT evaluation.} \citet{vilar-etal-2007-human} advocate for using binary instead of n-ary RR, as it is more intuitive and straightforward for annotators. Unlike \sxsqr, their method does not provide context and requires annotators to rank segments based on only adequacy and fluency. All possible system pairs are considered, with a full system ranking being obtained either by treating the task as a sorting problem or by applying the Bradley-Terry model \citep{Bradley1952RankAO, dras-2015-squibs}. Pairwise evaluation is also implemented in automatic MT evaluation, for example, by \citet{guzman-etal-2015-pairwise} and \citet{liu2024aligning}.

% draft in progree: \url{https://docs.google.com/document/d/1fqeZsMZnrWgpjGtA6o8l6o8aWZDcHqUFaosdHvZNC00/edit?usp=sharing}

% WMT has been putting efforts in finding the optimal human evaluation methodology.



% \sxsqr~is reminiscent of \citet{vilar-etal-2007-human}: pairwise comparison at the sentence level with no context, full ranking of systems


% DA --> MQM p3. \url{https://wrap.warwick.ac.uk/id/eprint/622/1/WRAP_Stewart_absolute_identification.pdf} judgement of the current stimuli is influenced by the previous one---sequential effects \citep{stewart2005absolute}

% People (experts and non experts) are better at finding the worse/better outputs in a side by side setting \citep{karpinska-etal-2021-perils}

% z-normalization is important if other suggestions are not followed (stability)

% \citet{liu2024aligninghumanjudgementrole}

% \citet{jang2022decreasingannotationburdenpairwise} pairwise in other domains

% \citet{guzman-etal-2015-pairwise}

% \citet{licht-etal-2022-consistent}

% \citet{dras-2015-squibs}

% \citet{freitag-etal-2021-experts}

% \citet{popovic-2021-agree}

% target only MT evaluation \url{https://aclanthology.org/W15-3059.pdf}

% yvette graham 

% ranking five different outputs

% wmt findings paper

% check lit to see if we need to change the name of sxs qr