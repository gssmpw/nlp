\section*{Limitations}

Practical considerations make it difficult to control all variables in human evaluation experiments. As an example, the single-sided MQM annotations were collected in 2023 with a different original goal, while the \sxsmqm~and \sxsqr~annotations were collected in 2024 for this project. Additionally, our annotators were engaged in multiple projects throughout 2024 and thus may have performed other annotation tasks in between items collected for this project.

Due to the time and cost involved in human evaluation, the current work is not able to test MQM, \sxsmqm, and \sxsqr~on a larger set of language pairs besides \ZhEn~and \EnDe. It is also impractical to exhaustively test all possible system pairs in the two tested language pairs. However, the current study still provides a strong foundation for understanding the trade-offs between detailed error detection and overall system ranking. Future work could expand this work to include more language pairs and a broader range of systems, further validating the generalizability of the results.

Future work can also test the annotation setups in other domains beyond MT to see how annotation settings influence evaluations in diverse NLP tasks. Additionally, researchers can investigate the impact of different annotation settings on annotator backgrounds. Although \sxsmqm~and \sxsqr~do not significantly increase the inter-annotator agreement of expert annotators, they might do so for crowd-sourced workers.

