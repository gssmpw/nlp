\section{Results and discussion}\label{sec:results_and_discussion}

This section presents the meta-evaluation of the human experiments using the metrics outlined in \sectionref{sec:meta_eval_metrics}, with results reported based on \textit{z}-normalized scores following \citet{riley-etal-2024-finding}. Segments annotated by a \ZhEn~outlier annotator are excluded.\footnote{The rationale for excluding the outlier annotator is provided in \appendixref{appendix:outlier_annotator}. After excluding the segments annotated by the outlier annotator, \ZhEn~has 16 documents with 220 segments and an average token count of 31.48 per segment.} The findings show that comparative judgment improves annotator agreement and consistency, maintains a reliable quality ranking, and facilitates accuracy error finding in \ZhEn.

\subsection{Inter-annotator agreement}\label{sec:annotator_agreement}

\textbf{\sxs~settings consistently yield higher agreement, particularly in \sxsmqm, in both \ZhEn~and \EnDe.} \tableref{tab:inter-AA} presents the Krippendorff's $\alpha$, indicating fair agreement among annotators in the three annotation protocols. The IAA does not exhibit a clear correlation with textual similarity between systems, as detailed in  \tableref{tab:inter-AA_expanded} (\appendixref{appendix:IAA_expanded}), an expanded version of \tableref{tab:inter-AA}.

\begin{table}[htbp]
    \fontsize{5}{6}\selectfont
    \centering
    % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
    \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{@{}rrrr@{}}
    \midrule
     & \textbf{\psxsmqm} & \textbf{\sxsmqm} $\uparrow$ & \textbf{\sxsqr} $\uparrow$ \\ \cmidrule{1-4}
    \ZhEn & 0.2178 & \textbf{0.2510} & 0.2380 \\
    \EnDe & 0.2345 & \textbf{0.3594} & 0.2402 \\\midrule
    \end{tabular}%
    }
    \caption{Krippendorff's $\alpha$ in three annotation settings. The annotators in each setting achieve a fair agreement.
    }
    \label{tab:inter-AA}
    \vspace{-5pt}
\end{table}

The results suggest that comparative judgment improves alignment among human annotators in evaluations. This is likely because MQM, as a pointwise approach, introduces more noise by preventing direct comparisons between translations. In contrast, the \sxs~settings allow for direct comparisons, reducing noise by minimizing inconsistent error marking (\sectionref{sec:itc}) and instances where shared mistakes are flagged for one system but overlooked for the other.

We hypothesize two reasons for \sxsmqm's higher agreement compared to \sxsqr. 

First, \sxsmqm~enables explicit error marking, reducing ambiguity and enhances the clarity of the decision-making process. In contrast, \sxsqr~requires annotators to simultaneously evaluate and weigh multiple aspects of two segments (e.g., accuracy and style). This increases cognitive load and introduces greater variability in their decisions.

Second, the increased cognitive load may cause annotators to be influenced by longer segments during comparative judgments. To test this, we ranked the segments by length, divided them into three equally sized groups, and computed Krippendorff's $\alpha$ for each group. The results in \tableref{tab:inter-AA-3-buckets} (\appendixref{appendix:IAA_expanded}) show that, in the \sxs~settings, the shortest segments achieve the highest agreement.

Overall, the improvement in segment-level agreement introduced by comparative judgment is valuable because segment-level evaluation is susceptible to noise \citep{freitag-etal-2023-results}, so mitigating that noise can improve reliability.
%Overall, the improvement introduced by comparative judgment is valuable for two key factors. First, the agreement is calculated at the segment level, a method susceptible to noise due to the inherent variability of individual segments \citep{freitag-etal-2023-results}. Second, our expert annotators consistently perform MQM ratings, ensuring high-quality annotations and a strong baseline agreement, leaving limited room for further enhancement. Given these constraints, the improvement gained from comparative judgement is notable.

\subsection{Inter-translation consistency}\label{sec:itc}

\input{tables/inter-translation_consistency_removed}

\textbf{\sxsmqm~demonstrates remarkable increases in inter-translation error marking consistency}, as shown in \tableref{tab:inter-TC_psxs_sxs_removed}.\footnote{The results without removing the \ZhEn~outlier annotator are in \tableref{tab:inter-TC_psxs_sxs}, which remains similar to \tableref{tab:inter-TC_psxs_sxs_removed}, meaning that, while the outlier annotator identified significantly more errors, they also exhibited improved inter-translation consistency.}
% \prcomment{This needs to be reworded; "albeit significantly more annotated errors" is missing something to parse properly}} 
% \prcomment{Should we just use Table 14 instead of 4? More generally, does it make sense to just remove the outlier rater \textbf{everywhere}?}
The upper half of \tableref{tab:inter-TC_psxs_sxs_removed} shows a substantial consistency increase, averaging $40.4\%$ for \ZhEn~and $35.7\%$ for \EnDe, when evaluating two systems together in \sxsmqm~(i.e., the pairs in \tableref{tab:sys_pairs}). This improvement persists, averaging $20.4\%$ for \ZhEn~and $18.6\%$ for \EnDe, even when two systems are not evaluated side-by-side, as shown in the lower table.\footnote{The \ZhEn~results are the average of 40 system pairs. For \EnDe, because GPT4-5shot is annotated twice in \sxsmqm, one of the annotated GPT4-5shot is excluded. Hence, the last two rows in \tableref{tab:inter-TC_psxs_sxs} are the averages of 31 pairs.} All increases extends beyond error spans into categories and severity.
% \prcomment{The improvement for non-compared systems is interesting; can we hypothesize why it is the case?}
The improvement in non-compared systems in \sxsmqm~may result from exposure to side-by-side comparisons, which potentially refine annotators' internal error detection standards as well as increase error awareness and cognitive anchoring.

The findings demonstrate that comparative judgment significantly enhances annotators' consistency in identifying error spans and assigning error severity and categories. The improvement is valuable for both gaining insights from annotations and training MQM-style automated metrics.

\subsection{Segment-level ranking agreement}\label{sec:seg_pra}

\textbf{\sxsmqm~and \sxsqr~show solid agreement with each other and are better at identifying equal-quality segments.} \tableref{tab:segment_PRA} presents the PRA results for every pair of settings in \ZhEn~and \EnDe. \tableref{tab:tie_rate} reports the tie rates for each pair of settings. The results provide three important insights. 

\begin{table}[t]
    \fontsize{5}{6}\selectfont
    \centering
    % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
    \resizebox{0.96\columnwidth}{!}{%
    \begin{tabular}{@{}rrrrr@{}}
    \midrule
    \textbf{$\alpha$ setting} & \textbf{$\beta$ setting} & \textbf{\ZhEn~PRA} $\uparrow$ & \textbf{\EnDe~PRA} $\uparrow$ & \textbf{Avg.}\\ \cmidrule{1-5}
    \psxsmqm          & \sxsqr           & 0.568   &  0.540 & 0.554 \\
    \sxsmqm           & \sxsqr           & \textbf{0.626}   &  0.629 & 0.628 \\
    \psxsmqm          & \sxsmqm          & 0.623   &  \textbf{0.646}  & 0.635 \\\midrule
    \end{tabular}%
    }
    \caption{Segment pairwise ranking agreement between every two annotation settings. Results are based on \textit{z}-scores with the \ZhEn~outlier annotator being excluded.
    }
    \label{tab:segment_PRA}
    \vspace{-5pt}
% https://docs.google.com/spreadsheets/d/1L32sSpspzRf28XHxoN3NJPl-LrAIM-DOk-B79d1jIyU/edit?usp=sharing
% /data/yixiao/human_eval_proj/code_pra/pra.py
\end{table}

First, MQM and \sxsqr~have the lowest agreement in both language pairs, largely due to the fundamental differences in their features: point-wise \textit{vs}.\ pairwise and detailed error annotation \textit{vs}.\ preference only. This shows that methodological divergence indeed impacts annotation outcomes.

Second, \sxsmqm~and \sxsqr~show solid agreement (\tableref{tab:segment_PRA}). With better IAA than MQM (\tableref{tab:inter-AA}) and lower cost than \sxsmqm, \sxsqr~is an appealing and efficient choice when detailed error annotation is not required.

Third, the MQM setting has the lowest tie rate (\tableref{tab:tie_rate}). This can be attributed to the fact that MQM lacks explicit comparisons between paired segments, which results in its low inter-translation consistency (\tableref{tab:inter-TC_psxs_sxs_removed}). As a result, MQM may misjudge segment pairs of equal quality, compromising the reliability of its annotation outcomes.

\begin{table}[htbp]
    \fontsize{5}{6}\selectfont
    \centering
    % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
    \resizebox{0.9\columnwidth}{!}{%
    \begin{tabular}{@{}rrrr@{}}
    \midrule
    \textbf{Language pair} & \textbf{MQM} & \textbf{\sxsmqm} & \textbf{\sxsqr} \\\cmidrule{1-4}
    \ZhEn & 7.36\% & 16.55\% & 18.55\% \\
    \EnDe & 6.92\% & 11.54\% & 16.15\% \\\midrule
    \end{tabular}%
    }
    \caption{
    Tie rate in three annotation settings. \sxsqr~has the highest tie rate in both language pairs.
    }
    \label{tab:tie_rate}
    % /data/yixiao/human_eval_proj/code_pra/pra.py
\vspace{-5pt}
\end{table}

% \yscomment{psxs mqm has the lowest tie rate, due to table 6. when two seg are actually tied, psxs is less likely to find out than sxs and qr do. <-- due to how mqm is designed}

\subsection{System-level ranking agreement}\label{sec:sys_level_agreement}

\input{tables/system_ranking_z_wo_56225225}

\textbf{MQM, \sxsmqm, and \sxsqr~demonstrate strong agreement in system-level rankings; \sxsqr's high tie rate may impact its reliability.} \tableref{tab:system_rankings} presents the system ranking results. 

For \ZhEn, all three settings yield identical system rankings. For \EnDe, GPT4-5shot was annotated twice in \sxsmqm, once with ONLINE-W and once with refA, and obtained stable scores. This suggests that \sxsmqm~does not compromise the absolute evaluation of individual systems. \sxsqr~on ONLINE-A and ONLINE-Y show a discrepancy with MQM and \sxsmqm; however, the difference is not statistically significant, indicated by the \textit{p}-value.

Further investigation into the discrepancy in \EnDe~\sxsqr~reveals that the high tie rate in \sxsqr~(\tableref{tab:tie_rate}) plays a key role. In three rounds of \sxsqr~annotations, ONLINE-Y and ONLINE-A tied in two. Across all annotations from three annotators, the tie outcome occurred 116 times (37.18\%) in \sxsmqm, compared to 17.31\% in MQM and 29.48\% with \sxsmqm.


% \yscomment{In three rounds of \sxsqr~annotations, ONLINE-Y and ONLINE-A tied in two. Across all annotations from three annotators, the tie outcome occurred 116 times (37.18\%), among which only 12 were shared with MQM and 25 with \sxsmqm.} \prcomment{How about just say ties appeared X\% more than in MQM and Y\% more than in SxS MQM?}

% , compared to 54 and 92 ties in MQM and \sxsmqm, respectively
% /data/yixiao/human_eval_proj/code_sys_ranking/ONLINE-Y_A_ties_per_sys_pair.py
% /data/yixiao/human_eval_proj/code_sys_ranking/ONLINE-Y_A_qr_tie_sxs_not_tie.py
% \prcomment{There are too many numbers embedded in this paragraph. Can we simplify/condense?} %This aligns with the observation in \sectionref{sec:seg_pra} that \sxsqr~tends to have a high tie rate, which offsets its reliability by the lack of explicit error marking, especially for the long and highly similar segments in \EnDe~ONLINE-Y and ONLINE-A.

These findings suggest several insights: (1) the coarse rating scale of \sxsqr~makes it difficult to detect nuanced quality differences; (2) while annotating error in MQM facilitates fine-grained distinctions, it also increases the risk of spurious differences due to rater noise; and (3) \sxsmqm~balances these trade-offs more effectively, as reflected in its higher inter-translation consistency.

\subsection{MQM error distribution}\label{sec:error_distr}

\textbf{\sxsmqm~highlights more major accuracy errors in \ZhEn, reflecting its ability in finding accuracy errors that may be neglected in MQM.} \figureref{fig:error_perc} illustrates the distribution of error category percentages for MQM and \sxsmqm.\footnote{Because GPT4-5shot is annotated twice in \EnDe~\sxsmqm, when counting error numbers, the GPT4-5shot errors in MQM are duplicated for a fair comparison between \EnDe~MQM and \sxsmqm.} 

% \input{tables/minor_major_error_perc}

\begin{figure*}[ht]
% https://colab.research.google.com/drive/1oLaOw6CoVY-4Bj_lJlbEAY6CuXkF5Gof?usp=sharing
    \centering
    \includegraphics[scale=0.4]{images/MQM_Distribution_Comparison_wo_56225225.pdf}
    \caption{Percentages of error categories in the MQM settings in \ZhEn~and \EnDe. The GPT4-5shot errors in \EnDe~MQM are doubled for a fair comparison with \EnDe~\sxsmqm. While the percentages in \EnDe~stay relatively stable, in \ZhEn, {\color{gglblue} \textbf{accuracy}} errors have a higher percentage in \sxsmqm~than in \psxsmqm.}
    \label{fig:error_perc}
    \vspace{-7pt}
\end{figure*}

\begin{figure*}
% /data/yixiao/human_eval_proj/code_error_distribution/count_error.py
    \centering
    \includegraphics[scale=0.48]{images/MQM_Error_Count_Comparison_wo_56225225.pdf}
    \caption{Number of errors in five categories in the MQM settings in \ZhEn~and \EnDe~of all three rounds of annotations. \texttt{Others} includes non-translation, locale convention, and other.
    }
    \label{fig:error_count}
    \vspace{-10pt}
\end{figure*}

While \EnDe~\sxsmqm~shows similar proportions to MQM, \ZhEn~\sxsmqm~shows a higher prevalence of major accuracy errors, further supported by the detailed counts in \figureref{fig:error_count}. To understand the source of major accuracy errors in \ZhEn~\sxsmqm, we examined whether annotators altered their category assignment of the same errors across the two annotation settings. The heatmaps in \figureref{fig:category_conversion} (\appendixref{appendix:error_distribution}) reveal category conversions, notably from Terminology and Style errors in MQM to Accuracy errors in \sxsmqm~for \ZhEn.

The conversions only partially explain the increase in major accuracy errors in \ZhEn. A review of 50 randomly sampled segments from the \ZhEn~ONLINE-A system\footnote{ONLINE-A shows the largest increase in accuracy errors when comparing \sxsmqm~to \psxsmqm.} revealed that many accuracy errors identified in \sxsmqm~were not annotated in \psxsmqm.

Overall, \EnDe~has more fluency errors in both MQM settings while \ZhEn~has a significant increase in accuracy errors in \sxsmqm. This may stem from English and German belonging to the same language family, making fluency the key challenge in translation, whereas the linguistic differences between Chinese and English make accuracy a greater challenge in \ZhEn. \sxsmqm~may further highlight accuracy errors in \ZhEn, especially when only one translation contains such an error.

% \subsection{Suggestions for future human evaluation}

% \yscomment{to be revised}


% Given the results, if comprehensive error identification is the priority, MQM is preferable because \sxsmqm~might inadvertently steer annotators toward focusing on differences that better differentiate systems (e.g., accuracy errors), potentially overlooking minor errors (e.g., fluency). However, \sxsmqm~provides significantly more consistent and reliable error annotation. Lastly, if one's primary goal is system ranking but not detailed error analysis, \sxsqr~offers performance on a par with more sophisticated methods while requiring considerably less time.
