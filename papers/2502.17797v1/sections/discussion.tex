\section{Discussion}

The results of this study elucidate the comparative effectiveness of three annotation settings on human annotators' assessments of machine translation systems. Three key dimensions are explored: system/segment-level rankings, inter/intra-annotator agreement, and annotators' behaviors within the different annotation frameworks.

Concerning the system-level rankings, all three annotation settings provided identical rankings for \ZhEn~with slight divergence on \EnDe. This underscores the robustness of the three annotation settings in that they are reliable in returning stable overall quality assessments. However, such assessment display certain noise at the segment level where the three annotation settings only moderately agree with each other.

Regarding consistency between and within annotators, 
the results reveal that \sxsmqm~slightly improves inter-annotator agreement over MQM but achieves a remarkable increase in inter-translation consistency. Such an improvement highlights the effectiveness of \sxsmqm~in fostering precise and consistent annotations regardless of whether two items are annotated \sxs. This promotes a more standardized annotation process.

Overall, if comprehensive error identification is the priority, MQM is preferable. For consistent and reliable error annotation and robust system rankings, \sxsmqm~is recommended although \sxsmqm~might inadvertently steer annotators toward focusing on differences that more clearly differentiate systems (e.g., accuracy errors), potentially overlooking minor errors (e.g., fluency). Lastly, if one's primary goal is system ranking without detailed error analysis, \sxsqr~offers a comparable performance to more sophisticated methods while requiring only a third of the time.

% \subsection{Suggestions for future human evaluation}

% \yscomment{to be revised}


% Given the results, if comprehensive error identification is the priority, MQM is preferable because \sxsmqm~might inadvertently steer annotators toward focusing on differences that better differentiate systems (e.g., accuracy errors), potentially overlooking minor errors (e.g., fluency). However, \sxsmqm~provides significantly more consistent and reliable error annotation. Lastly, if one's primary goal is system ranking but not detailed error analysis, \sxsqr~offers performance on a par with more sophisticated methods while requiring considerably less time.
