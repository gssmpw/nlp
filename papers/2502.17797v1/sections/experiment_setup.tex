\section{Experimental setup}\label{sec:exp_setup}

This section details the setup of human experiments, including the dataset, the criteria for selecting systems to be evaluated, and the process for assigning tasks among annotators.

\subsection{Dataset and language pairs}

The human annotation experiments are performed on the system outputs from the news domain in WMT2023 \citep{freitag-etal-2023-results}, covering two language pairs: Chinese to English (\ZhEn) and English to German (\EnDe). Basic statistics are in \tableref{tab:basic_stats}.

\input{tables/dataset_stats}

\subsection{System pairs}\label{sec:sys_pairs}

Due to the time and cost involved in human evaluation, pairwise comparisons of all MT systems in the \sxs~settings are impractical. Therefore, 5 system pairs are selected per language pair (\tableref{tab:sys_pairs}), which are drawn from the systems in the WMT 2023 General Machine Translation Task \citep{kocmi-etal-2023-findings}.%, with a focus on systems with similar quality, as distinguishing systems with smaller quality differences presents a greater challenge in practical scenarios. 
% , focusing on systems with similar quality, as distinguishing between systems with smaller quality differences presents a greater challenge in practical scenarios.

\input{tables/system_pairs}

Two features are considered when forming system pairs: text similarity and quality similarity. For text similarity, cross-BLEU \citep{papineni-etal-2002-bleu} is applied. For quality similarity, XCOMET-QE-Ensemble (XCOMET) \citep{xcomet, freitag-etal-2023-results} is used in tandem with a random permutation test.\footnote{\texttt{permutation\_test} from \texttt{scipy.stats}.} XCOMET provides segment scores for each system. Two systems are considered similar in quality if the permutation test of their segment scores returns $p > 0.05$. 
% \url{https://docs.scipy.org/doc/scipy-1.14.1/reference/generated/scipy.stats.permutation_test.html}

For each language pair, the top two systems identified by XCOMET form a pair; two pairs have high text similarity (cross-BLEU); and two have low text similarity. In all cases, systems with similar quality were selected, as distinguishing small quality differences presents a greater challenge in practical scenarios. The system pairs are listed in \tableref{tab:sys_pairs}.\footnote{For brevity, NLLB\_MBR\_BLEU is referred to as NLLB\_BLEU in this work.} Applying these criteria resulted in GPT4-5shot appearing twice in \EnDe. This has the benefit of allowing comparison between each instance of this system's evaluation in the side-by-side settings.%~to examine whether a system's MQM score varies with different comparison systems.

\subsection{Task assignment}\label{sec:task_assignment}

The annotation experiments are conducted by professional translators who regularly perform MQM annotation. Tasks are distributed approximately evenly among 8 annotators for \ZhEn~ and 10 for \EnDe. To mitigate rater bias, we use the within-subject setup of \citet{riley-etal-2024-finding}: for each input document, all system translations for that document are evaluated by the same set of 3 annotators. Additionally, each translation is evaluated by the same 3 annotators in all 3 annotation settings.
%each input document is evaluated by the same annotator across all settings. Three rounds of human annotation are conducted for each setting. In each round, task assignments are ``rotated'', with tasks assigned to annotator 1 in round 1 given to annotator 2 in round 2, and so on. The same task assignments are used across all three settings in each round.

% \subsection{Score calculation}\label{sec:result_score_calc}

% Annotations from the expert annotators are converted to numeric scores for system and annotation setting comparison. For both \psxsmqm~and \sxsmqm, the segment score is the average of the scores assigned by each annotator, with each score determined by the error severity and category. Gibberish segments score 25 points without further error annotation; major errors are penalized by 5 points, minor punctuation errors by 0.1, and other minor errors by 1. The system score is obtained by averaging the segment scores. MQM scores are the lower the better, with an error-free segment receiving a score of 0.

% \begin{table}[ht]
% \fontsize{6}{7}\selectfont
% \centering
% % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
% \resizebox{0.55\columnwidth}{!}{%
% \begin{tabular}{@{}p{0.55cm}p{1.6cm}p{0.7cm}@{}}
% \midrule
% \textbf{Severity}   & \textbf{Category} & \textbf{Weight} \\ \midrule
% \multirow{2}{*}{\major}  & Non-translation & 25  \\
%                          & Others   & 5 \\\cmidrule{1-3}
% \multirow{2}{*}{\minor}  & Fluency/Punctuation & 0.1  \\
%                          & Others   & 1 \\\midrule
% \end{tabular}%
% }
% \caption{MQM error span weighting scheme. Gibberish segments score 25 points, \major~errors 5 points, and \minor~errors 1 point, except for \minor~punctuation errors, which are weighted at 0.1 point each.
% }
% \label{tab:weighting-scheme}
% \end{table}

% The scoring of \sxsqr~follows MQM in that a lower score is better. In each rated pair, the \texttt{much worse} translation segment                is penalized by 2 points, and the \texttt{worse} segment by 1 point. If both translations are of similar quality, no penalty is applied. As the MQM settings, the segment score is the average of the scores assigned by each annotator. The system score is averaged over the segment scores.

% \begin{table}[ht]
% \fontsize{5}{6}\selectfont
% \centering
% % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
% \resizebox{0.55\columnwidth}{!}{%
% \begin{tabular}{@{}p{1.5cm}p{0.7cm}@{}}
% \midrule
% \textbf{Category} & \textbf{Weight} \\ \midrule
% Much worse & 2  \\
% Worse      & 1  \\
% About the same & 0 \\\midrule
% \end{tabular}%
% }
% \caption{\sxsqr~segment scoring scheme.
% }
% \label{tab:weighting-scheme-qr}
% \vspace{-10pt}
% \end{table}

