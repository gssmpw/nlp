\section{More details to metrics of meta evaluation}
\label{appendix:meta_eval_details}

\subsection{Pairwise ranking agreement}\label{appendix:pra_terms}

\eqref{eq:PRA} is used to calcualate agreement in segment-level rankings between the annotation settings and \tableref{tab:pra_terms} defines its terms. The ranking of two translations of the same source segment depends on the segment scores: segments with identical scores are tied, while differing scores dictate their ranking. PRA quantifies the frequency with which two evaluation settings agree on the ranking of each pair of segments from two systems.

% \vspace{-5pt}
\begin{equation}
  \resizebox{0.6\hsize}{!}{$
  PRA = \frac{C + T_{\alpha\beta}}{C + D + T_\alpha + T_\beta + T_{\alpha\beta}}
  $}
  \label{eq:PRA}
\end{equation}
\vspace{-3pt}

\begin{table}[ht]
% \fontsize{6}{7}\selectfont
\centering
% \renewcommand{\arraystretch}{0.8} % Reduce vertical space
\resizebox{0.85\columnwidth}{!}{%
\begin{tabular}{@{}ll@{}}
\midrule
\textbf{Symbol}    & \textbf{Description}                \\ \midrule
$\alpha$  & One of the annotation settings    \\
$\beta$   & One of the annotation settings that is not $\alpha$ \\
$C$       & The number of concordant pairs    \\
$D$       & The number of discordant pairs    \\
$T_{\alpha}$ & The number of pairs tied \textit{only} in $\alpha$ \\
$T_{\beta}$  & The number of pairs tied \textit{only} in $\beta$ \\
$T_{\alpha\beta}$ & The number of pairs tied in both $\alpha$ and $\beta$ \\\midrule
\end{tabular}%
}
\caption{Terms in \eqref{eq:PRA}. The annotation settings are \psxsmqm, \sxsmqm, and \sxsqr.
}
\label{tab:pra_terms}
\end{table}

\subsection{Inter-translation consistency}\label{appendix:itc_pseudo_code}

Using the example below, inter-translation consistency is meant to be the following: if an annotator labels ``arabica'' as a minor fluency error in \ref{ex:chatgpt}, they should do the same in \ref{ex:claude}.\footnote{The two translations share common spans highlighted in green, identified using the \texttt{get\_opcodes()} function from Python's \texttt{difflib} module.} For reasons of practicality and clarity, two systems are considered at a time for calculating the consistency.

\begin{enumerate}[label=(\alph*)]
    \item {\color{gglgreen}Brazil is the world's largest producer of \underline{\textbf{arabica}} beans,} a {\color{gglgreen}coffee} variety {\color{gglgreen}commonly used by baristas} to make {\color{gglgreen}coffee}.\label{ex:chatgpt}
    \vspace{-6pt}
    
    \item {\color{gglgreen}Brazil is the world's largest producer of \underline{\textbf{arabica}} beans,} which are the {\color{gglgreen}coffee} beans {\color{gglgreen}commonly used by baristas} in making {\color{gglgreen}coffee}.\label{ex:claude}
\end{enumerate}

Inter-translation consistency is calculated as follows: \textbf{Alignment of Tokens} the translations from two systems are tokenized, and the alignment between their tokens is computed using \texttt{get\_opcodes()} from \texttt{difflib}. This generates a list of operations (replace, delete, insert, equal) that align the tokens of the two translations; \textbf{Identification of Potential Common Errors} Errors annotated in each translation are compared based on their spans. If an error in one translation aligns with an ``equal'' operation in the token alignment, it is considered as a potential common error of the two compared translations. These errors are stored for further analysis; \textbf{Matching Errors Using a Criterion} A specified criterion (e.g., matching spans, categories, severities, or combinations thereof) is applied to the potential common errors. This determines how many errors are consistently marked across the two translations; \textbf{Calculation of Consistency} the consistency value is calculated as the percentage of errors that satisfy the criterion out of the total potential errors. 

The final inter-translation consistency is averaged over all raters per criterion.

% \begin{table*}[h]
% \fontsize{11}{12}\selectfont
% \centering
% % \renewcommand{\arraystretch}{0.8} % Reduce vertical space
% \resizebox{\textwidth}{!}{%
% \begin{tabular}{p{20cm}}
% \midrule
% \multicolumn{1}{c}{Step 1: Get \texttt{opcodes}}  \\ \midrule

% 1. Input: \texttt{sys1\_tgt\_tok\_lst} (a list of tokens of system 1) and \texttt{sys2\_tgt\_tok\_lst} (a list of tokens of system 2).

% 2. Use \texttt{get\_opcodes()} from \texttt{difflib} to compute \texttt{opcode\_lst}, the alignment operations between \texttt{sys1\_tgt\_tok\_lst} and \texttt{sys2\_tgt\_tok\_lst}.\newline

% Example output: [(`equal', 0, 39, 0, 39), (`replace', 39, 40, 39, 40), (`equal', 40, 52, 40, 52), (`delete', 52, 54, 52, 52), (`equal', 54, 68, 52, 66), (`delete', 68, 70, 66, 66), (`equal', 70, 82, 66, 78)] \\\midrule

% \multicolumn{1}{c}{Step 2: Find possible common errors}  \\ \midrule

% 1. Input: 

% \hspace{1cm}\texttt{sys1\_errors}: List of errors marked in \texttt{sys1\_tgt\_tok\_lst} (each error includes type, span, category, severity, and indices).

% \hspace{1cm}\texttt{sys2\_errors}: List of errors marked in \texttt{sys2\_tgt\_tok\_lst}.

% \hspace{1cm}\texttt{opcode\_lst}: Alignment operations from Step 1.

% 2. Initialize \texttt{all\_possible\_errors = []}.

% 3. For each error in \texttt{sys1\_errors}:

% \hspace{1cm}Check if the error start and end indices fall within any \textt{equal} operation in \texttt{opcode\_lst}

% \hspace{1cm}If yes, add the error to \texttt{all\_possible\_errors} with a label (\texttt{'sys1'})

% 4. Repeat the same process for \texttt{sys2\_errors}.\newline

% Example output: \texttt{all\_possible\_errors} = [
%     (`sys1', `target', [`human', ` ', `resources', ` ', `departments'], `accuracy', `major', 76, 81),
%     (`sys2', `target', [`human', ` ', `resources', ` ', `departments'], `terminology', `major', 72, 77)
% ] \\\midrule

% \multicolumn{1}{c}{Step 3: Count matched errors}  \\ \midrule

% 1. Input: 

% \hspace{1cm}\texttt{all\_possible\_errors}: List of errors from Step 2.

% \hspace{1cm}\texttt{criterion}: to determine if two errors match (span, span + category, span + severity, or span + category + severity).

% 2. Initialize \texttt{matched\_error\_count = 0}.

% 3. For each errors in \texttt{all\_possible\_errors} with \texttt{`sys1} or \texttt{`sys2'}:

% \hspace{1cm}Check if their counterpart is in \texttt{all\_possible\_errors} (without considering the error indices).

% \hspace{1cm}If yes, increment \texttt{matched\_error\_count}.\\\midrule

% \multicolumn{1}{c}{Step 4: Compute agreement percentages}  \\ \midrule






% \end{tabular}%
% }
% \caption{
% Steps of calculating pairwise inter-translation consistency.
% }
% \label{tab:irc_pseudo_code}
% \end{table*}

