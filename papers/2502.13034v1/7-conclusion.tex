\section{Conclusion}
In this position paper, we focused on the problem of multi-image V2L generation and connected the various tasks that are typically considered separate by the research community. We proposed a method for quantitatively exploring the current landscape of this problem, using which we uncovered relationships between task objectives and their corresponding datasets. Despite having different characteristics in terms of the objectives or the input-output data, we argued that all the tasks present a common set of challenges for developing models and for assessing their generated outputs. To understand the progress made over the years with regard to multi-image-to-text generation, we extensively reviewed the different modeling approaches and evaluation protocols pertaining to each of the tasks, and discussed them in a comprehensive and unified manner. As the problem of generating text conditioned on sequences of multiple temporally ordered images or video frames has various real-world applications, we underline the challenges and propose several concrete research directions informed by insights from linguistics, cognitive sciences, and natural language processing (NLP) aimed towards facilitating further advancements. We argue that leveraging these insights could also help the development of better VLMs, which are currently not immune from some of the highlighted limitations.