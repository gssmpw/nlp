\section{Dimensions of Variation}
\label{sec:tasks_and_analysis}

While our focus is on the multi-image-to-text-problem, different tasks within this problem space may have different characteristics. Two relevant dimensions are the complexity of the visual input and of the textual output. For example, some of these tasks require models to generate succinct answers and descriptions, and others require generation of long-form textual narratives intended to also complement the visual information. These dimensions of variation across tasks typically tend to be crucial factors in making design choices with regard to developing model architectures and learning procedures. In terms of the visual input, depending on the objective of the task, images within the input sequence of each data sample could be comparable to each other or vary drastically to the point of being completely heterogeneous. For instance, in the \color{xkcdVividBlue}CC \color{black}task, where the goal is to localize and describe changes between a pair of images obtained from real-time surveillance cameras or large-scale remote-sensing snapshots, we hypothesize that the similarity between input images would be generally high. On the other hand, in tasks such as \textit{Visual Storytelling} (\color{xkcdVividBlue}VST\color{black}) \cite{vist}, in which the input sequences typically depict an overarching narrative, we hypothesize low similarity between consecutive images within each data sample (<visual sequence, text> pair). Regarding textual output, we similarly posit that the consistency of consecutive sentences within each data sample could be high or low depending on the corresponding task objective.

To preliminarily test our intuitions, we quantitatively analyze a few datasets corresponding to each of these tasks. Specifically, for each data sample, we compute \textit{visual similarity} scores between CLIP \cite{clip} visual encoder embeddings of consecutive images in the input sequence and report the average score. In the same manner, we compute \textit{textual consistency} scores between CLIP text encoder embeddings of consecutive sentences in the corresponding ground-truth text of each data sample. For this study, we randomly select 100 instances per task from five datasets---\color{xkcdOrange}Spot-the-diff \color{black} \cite{cc_spot_the_diff} for \color{xkcdVividBlue}CC\color{black}, \color{xkcdOrange}VIST \color{black} \cite{vist} for \color{xkcdVividBlue}VST\color{black}, \color{xkcdOrange}Charades \color{black} \cite{vc_charades} for \color{xkcdVividBlue}VC\color{black}, \color{xkcdOrange}MSVD-QA \color{black} \cite{msvd_qa} for \color{xkcdVividBlue}VIDQA\color{black}, and \color{xkcdOrange}MAD-v1 \color{black} \cite{madv1} for \emph{Movie Auto Audio Description (Auto AD)} \cite{maad1} (\color{xkcdVividBlue}MAAD\color{black}).\footnote{Further details are provided in Appendix~\ref{appendix:a}.} Figure~\ref{fig:data_analysis} shows the distributions of similarity scores obtained for each of the tasks along the visual and textual dimensions. In terms of \textit{visual similarity}, we observe that \color{xkcdVividBlue}CC \color{black} and \color{xkcdVividBlue}VST \color{black} indeed obtain maximum and minimum scores respectively, with other tasks ranging in between, confirming our intuitions. In terms of \textit{textual consistency}, the differentiation is less evident. We observe that for the \color{xkcdVividBlue}MAAD \color{black} and \color{xkcdVividBlue}VC \color{black} tasks, consecutive sentences in the ground-truth text are relatively less consistent with each other. Using the average similarity scores across all data samples, we categorize the five tasks by placing them at different positions in the shared space between \emph{textual consistency} and \emph{visual similarity} (see Figure~\ref{fig:shared_space}). Besides the two axes considered for this analysis, we note that tasks could also be compared along various other dimensions. We posit that this kind of analysis would serve as a meaningful guide for making modeling and evaluation decisions both for current and for novel future tasks in the multi-image-to-text landscape.

\begin{figure}[hbtp]
  \centering
  \includegraphics[width=\columnwidth,keepaspectratio]{images/review_tasks.pdf}
  \caption{Tasks positioned in the visual-textual shared space using the similarity scores obtained for corresponding datasets. \underline{Interpretation:} a task in the top right corner of the plot is both inconsistent at the textual level and dissimilar at the visual level.}
  \label{fig:shared_space}
\end{figure}