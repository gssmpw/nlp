\begin{table}[htbp]
    \vspace{-17.5em}
    \centering
    \begin{tabularx}{\textwidth}{llll}
        \toprule
          \textbf{Model} & \textbf{Vision Encoder} & \textbf{Projector} & \textbf{Language Decoder} \\
        \midrule
          \multicolumn{4}{c}{\textit{Change Captioning}} \\
        \midrule
          CARD \cite{cc_card} & ResNet, Transformer$^\dag$ & \texttt{NA} & Transformer$^\dag$ \\
          SCORER+CBR \cite{cc_scorer} & ResNet, MH(S/X)A$^\dag$ & \texttt{NA} & Transformer$^\dag$ \\
          VARD-Trans \cite{cc_vard} & ResNet, Linear(s)$^\dag$ & \texttt{NA} & Transformer$^\dag$ \\
          DUDA \cite{cc_duda} & ResNet, RNN$^\dag$ & \texttt{NA} & RNN$^\dag$\\
        \midrule
          \multicolumn{4}{c}{\textit{Multi-image/Video Question Answering}} \\
        \midrule
          ViLA \cite{videoqa_vila} & ViT, Transformer$^\dag$ & Q-Former$^\dag$ & {Flan-T5 XL} \\
          LLaMA-VQA \cite{videoqa_llamavqa} & CLIP-ViT-L & Linear$^\dag$ & {LLAMA} \\
          SeViLA \cite{videoqa_sevila} & ViT & Q-Former$^\dag$ & {Flan-T5 XL} \\
          FrozenBiLM \cite{videoqa_fbilm} & CLIP-ViT-L & Linear$^\dag$ & {DeBERTa-V2-XL} \\
        \midrule
          \multicolumn{4}{c}{\textit{Video Captioning}} \\
        \midrule
          Vid2Seq \cite{vc_vid2seq} &  {CLIP ViT-L, Transformer$^\dag$} & \texttt{NA} & {T5-base} \\
          TextKG \cite{vc_textkg} & Transformer$^\dag$ & \texttt{NA} & Transformer$^\dag$ \\
          VTAR \cite{vc_vtar} & {InceptionResNetV2, C3D} & Transformer$^\dag$ & Transformer$^\dag$ \\
          ENC-DEC \cite{vc_task} & 3DCNN$^\dag$ & Attention$^\dag$ & RNN$^\dag$ \\
        \midrule
          \multicolumn{4}{c}{\textit{Movie Auto Audio Description}} \\
        \midrule
          MM-Narrator \cite{maad_mm_narrator} & CLIP-ViT-L & \texttt{NA} & {GPT-4} \\
          AutoAD-III \cite{maad3} & EVA-CLIP & Q-Former$^\dag$ & {LLAMA 2} \\
          AutoAD-II \cite{maad2} & {CLIP-ViT-L} & \texttt{NA} & {GPT-2, MHXA$^\dag$} \\
          AutoAD \cite{maad1} & CLIP-ViT-L & {Transformer$^\dag$} & {GPT-2} \\
        \midrule
          \multicolumn{4}{c}{\textit{Visual Storytelling}} \\
        \midrule
          MCSM+BART \cite{vist_kg1} & ResNet, RNN$^\dag$ & \texttt{NA} & {BART} \\
          TAPM \cite{vist_tapm} & ResNet, FRCNN & \texttt{NA} & {GPT-2} \\
          KG Story \cite{vist_kg2} & FRCNN & \texttt{NA} & Transformer$^\dag$ \\
          GLAC Net \cite{vist_glacnet} & ResNet, RNN$^\dag$ & \texttt{NA} & RNN$^\dag$ \\
        \bottomrule
    \end{tabularx}
  \caption{A selection of recent models proposed for each of the tasks considered in the paper. All the models reported are end-to-end and task-specific, i.e., trained or fine-tuned for the task. For each model, we report the underlying vision encoder and language decoder, as well as the projector module, when applicable (\texttt{NA}: Not Applicable). Components with $^\dag$ have been trained from scratch using only the datasets available for the corresponding task.}
  \label{tab:models}
\end{table}