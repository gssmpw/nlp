\section{Models Architectures}
\label{sec:models}

\begin{figure*}[htbp]
  \centering
  \includegraphics[width=\linewidth,keepaspectratio]{images/review_model_arch.pdf}
  \caption{Outline of the architecture common across modeling approaches for multi-image-to-text tasks.}
  \label{fig:model_arch}
\end{figure*}

Modeling approaches to multi-image-to-text tasks have evolved over time from being recurrent neural network (RNN)-based \cite{lstm} to being transformer-based \cite{transformer}. More recent models directly leverage pre-trained large (vision)-language models (LLMs/VLMs), often in a zero-shot manner. In this section, we discuss this evolution and summarize the various state-of-the-art model architectures proposed for the five multi-image-to-text tasks. Architectures proposed for these tasks primarily comprise three modules---a vision encoder, a language decoder, and an intermediate module (typically referred as the projector/adapter) for adapting visual information into contextualized representations for text generation. We describe the functionality of these modules and review the design principles common across all the tasks in the proposed approaches. Furthermore, we also discuss how off-the-shelf  pre-trained VLMs are currently being used to handle various multi-image-to-text tasks. Table~\ref{tab:models} outlines a summary of the models and details related to the selection procedure are provided in Appendix~\ref{appendix:b}.

\subsection{Vision Encoder}

The primary purpose of a vision encoder in vision-to-language tasks is to extract information from the input visual sequence and to optimally encode it into a contextual representation that guides language generation. To achieve this, encoders in the proposed models follow multiple steps, some of which are common across the five multi-image-to-text tasks. First, a pre-trained vision model is utilized for extracting feature representations of the raw input sequences of images/video frames. Earlier approaches used convolutional neural network (CNN)-based vision models such as ResNet \cite{resnet} or R3D \cite{resnet3d} that are primarily pre-trained on the object detection task using large amounts of image/video data. Most of the recent models across the tasks use transformer-based vision models pre-trained for various image-only and image-text alignment objectives, e.g., CLIP-ViT-L \cite{clip}.

We note that besides the primary input sequence of images/video frames, models proposed for some of the tasks, e.g., \color{xkcdVividBlue}MAAD\color{black}, utilize additional input data such as close-ups of characters in the movie clips (\textit{exemplars}) \cite{maad2}. Furthermore, the TAPM \cite{vist_tapm} model proposed for the \color{xkcdVividBlue}VST \color{black} task utilizes FasterRCNN \cite{faster-rcnn} for extracting such local character/object-level features alongside the global image-level features from ResNet. Following the extraction of visual features using pre-trained vision models, most vision encoders comprise an internal sequence-encoder component for learning relationships and dependencies between the individual image/frame-level features at different temporal positions. Some models implement this step either using RNNs or a transformer network with multi-head self-attention for learning temporal relationships and position-encoding mechanism for tracking the order of entities/events in the sequence.

Beyond these steps that are common across the tasks, vision-encoders may also contain additional task-specific steps for capturing the visual information in a way that suits the task's objective better. For instance, the ViLA model \cite{videoqa_vila} for the \color{xkcdVividBlue}VIDQA \color{black} task utilizes a learnable Frame-Sampler component to efficiently select a small subset of frames that are most likely to contain the relevant information needed to answer the question. Another example with a task-specific step is the MSCM+BART  model \cite{vist_kg2} for \color{xkcdVividBlue}VST\color{black}, in which the initial set of image objects/`concepts' are expanded using an external knowledge graph for generating diverse and informative stories.
Despite these task-specific steps, we found that the vision encoder module in the architectures proposed for the various multi-image-to-text tasks share a common set of components that are broadly outlined in Figure~\ref{fig:model_arch}.

\subsection{The Vision-to-Language Bridge}

Some V2L model architectures utilize an intermediate module that bridges the input and output modalities for effectively conditioning the text generation on the extracted visual features. Different models operationalize this module with different degrees of complexity. Earlier approaches for several multi-image-to-text tasks condition the text generation process by directly fusing vision encoder outputs with the language decoder input \cite{vist_glacnet}. Some architectures employ cross-attention mechanisms to focus on the relevant parts of the visual features at various temporal positions during decoding \cite{vc_task}. However, approaches that adopt pre-trained models---e.g., CLIP-ViT-L \cite{clip} as the visual model---tend to employ learnable intermediate layers for aligning and converting outputs of the vision encoder into a format that the language decoder can understand.

In some of the proposed models, this intermediate module is a single linear layer that transforms the visual features into a common shared space, which can be used by the language decoder \cite{videoqa_llamavqa,llava}. In other models, advanced transformer-based projectors such as a Q-Former \cite{blip2} are used for their ability to leverage cross-modal interactions effectively \cite{maad3}. In essence, Q-Former uses dynamic query vectors that are pre-trained to attend to both visual and textual representations, enhancing its ability to generalize and perform well (relative to a single linear layer) across different tasks. Besides these popular methods for adapting multimodal information, some approaches make use of graph neural networks for capturing relationships between objects in the images at different temporal positions and words in the corresponding sentences of the text \cite{vc_gnn}. While there is no definitive way to design this intermediate module, recent work has compared the two approaches, i.e., using cross-attention between modalities or using a multimodal projector for transforming vision encoder features into the language space, and found that the latter leads to a stable/improved performance of models \cite{what_matters}.

\subsection{Language Decoder}

After encoding and adapting the visual information, models employ a language decoder component for text generation. The decoder can either be learned from scratch or consist of a pre-trained language model with additional trainable task-specific layers. Figure~\ref{fig:model_arch} summarizes the different ways in which this step is operationalized across tasks in the proposed architectures. Earlier models learn an RNN by initializing it with the visual context embedding from the previous steps \cite{vc_task,vist_glacnet}. The decoder then typically follows a `teacher forcing' strategy during training to generate one word at a time autoregressively.

Subsequent models have replaced RNNs with the transformer architecture owing to its computation scalability and efficiency in handling long context-windows. Besides the initial word embedding layer and the position encoding step (for maintaining information about the input sequence token order), a transformer decoder is typically made up of multiple identical blocks. Each block comprises a multi-head self-attention layer for modeling intra-sentence relations (between the words) and a multi-head cross-attention layer for learning relationships between representations of each word and the outputs of the visual encoder/projector. For instance, in the \color{xkcdVividBlue}CC \color{black} task, this refers to conditioning each word in the caption on vision encoder outputs (denoted as `difference-representations').

Instead of training the decoder from scratch, some approaches use language models such as GPT-2 \cite{gpt2} and \textsc{LLAMA 2} \cite{llama2}, which are pre-trained on several text-only tasks such as question-answering and text classification/completion. The pre-trained language models are either used directly for generation by freezing their parameters \cite{maad1,maad2,maad3}, or by inserting and fine-tuning additional adaptive layers on top of them for ensuring relevance of the generated text to the downstream task of interest \cite{vist_tapm}. We also note that some models incorporate information from external knowledge bases/graphs into the decoder module to improve coherence and factuality of the generated text, e.g., TextKG \cite{vc_textkg} for the \color{xkcdVividBlue}VC \color{black} task and KG Story \cite{vist_kg2} for the \color{xkcdVividBlue}VST \color{black} task.

\subsection{Off-the-shelf Pre-trained VLMs}
\label{sec:4_4}

The standard model architecture we have discussed so far is also present in more powerful general-purpose foundation VLMs (pre-trained on several tasks using large amounts of data), which can be used directly for multi-image-to-text tasks. Their pre-training process typically happens in two stages---self-supervised alignment training and visual instruction tuning. During the first stage, only the parameters of the intermediate module connecting both unimodal backbones are updated (commonly using paired image-text data) utilizing a contrastive training objective.

In the second stage, models are instruction-tuned using multi-turn conversations obtained for visual data either through crowd-sourcing or by leveraging tools such as GPT-4 \cite{gpt4}. Contrary to task-specific modeling approaches, these pre-trained VLMs are simply prompted (typically in a zero-shot manner) using visual tokens accompanied by task-specific instructions. Some of the pre-trained VLMs that are used off-the-shelf for the multi-image-to-text tasks include: ViLA for \color{xkcdVividBlue}VIDQA\color{black}, mPLUG-2 for \color{xkcdVividBlue}VC\color{black}, VideoLLAMA for \color{xkcdVividBlue}MAAD\color{black}, and LLaVA-NeXT for \color{xkcdVividBlue}VST \color{black} \cite{videoqa_vila,mplug2,maad4,nytws}.