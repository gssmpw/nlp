\section{Evaluation}
\label{sec:eval}

Given all the similarities described above, it is not surprising that all multi-image-to-text tasks are also evaluated leveraging similar methods. These methods range from using traditional \textit{n}-gram matching metrics to obtaining human judgments and ratings to, more recently, using off-the-shelf pre-trained VLMs assessing the generated output. We broadly classify these evaluation methods into two main categories---automatic and human evaluation. In the following subsections, we discuss the several quantitative metrics and benchmarks widely used for each of the tasks, along with the rationales for relying on them.

\subsection{Automatic Evaluation}
\label{sec:5_1}

To computationally assess the quality of model-generated text along different aspects, several automatic metrics have been proposed. While some metrics rely on answers/text provided by human annotators, others are reference-free and assess model outputs independent of the ground-truth data. Besides computational metrics, the community has also relied on benchmark datasets designed to reveal various general capabilities of models.

\paragraph{Reference-based metrics.}
The five multi-image-to-text tasks we examined primarily assessed model-generated candidate text by comparing it to corresponding human-written references. Specifically, traditional metrics that were originally designed for evaluating machine translation and text summarization tasks---BLEU \cite{bleu}, METEOR \cite{meteor}, and ROUGE \cite{rouge}---are used to measure precision and recall of overlapping $\mathit{n}$-grams between the candidate and reference text. Usually, metrics such as CIDEr \cite{cider} and SPICE \cite{spice} that have been specifically developed for the evaluation of image and video captioning are also used in conjunction with the above three metrics.

All the above-mentioned metrics rely on direct raw text comparisons of ground-truth references and model outputs. As this ground-truth data might not be always available, embedding-level reference-based evaluation metrics such as WMD \cite{wmd}, BERTScore \cite{bertscore}, and ViLBERTScore \cite{vilbertscore} have been proposed. Recent work for \color{xkcdVividBlue}VC\color{black}, \color{xkcdVividBlue}VIDQA\color{black}, and \color{xkcdVividBlue}MAAD \color{black} tasks have incorporated these metrics to measure the similarity between candidate and reference embeddings, obtained by projecting corresponding text into a common pre-trained semantic space \cite{vc_swinbert,maad2,maad3}.

\paragraph{Reference-free metrics.}
Comparing model-generated text to ground-truth references, typically provided by crowd-workers or scraped from the internet has various limitations. Most reference-based metrics do not account for the visual modality upon which the generated text is conditioned. Furthermore, as shown in Figure~\ref{fig:shared_space}, text in the \color{xkcdVividBlue}VC\color{black}, \color{xkcdVividBlue}MAAD\color{black}, and \color{xkcdVividBlue}VST \color{black} tasks often complements the visual input by encompassing various abstract/creative concepts, and is not merely descriptive. This makes reference-based metrics generally inappropriate for accurately evaluating multi-image-to-text tasks.

For the reasons detailed above, various reference-free metrics such as MAUVE \cite{mauve} and UNION \cite{union} have been proposed for unimodal open-ended text generation tasks. However, for the visually-conditioned text generation tasks, this adoption/shift is still relatively recent, with a persistent emphasis on reference-based $\mathit{n}$-gram metrics to date. That said, various reference-free metrics have been recently proposed to assess different aspects of evaluation that are important for several tasks. For instance, metrics such as CLIPScore \cite{clipscore} and GROOViST \cite{groovist} have been developed for evaluating visual grounding---the degree of alignment between the generated text and the visual input---in \color{xkcdVividBlue}VC \color{black} and \color{xkcdVividBlue}VST \color{black} tasks. Similarly, the RoViST \cite{rovist} suite of metrics has been proposed to assess coherence, the extent of repetition, and visual grounding of the generated text. We note that there are also metrics such as CRITIC \cite{maad3} and CM (character matching) \cite{vist_cm} designed to evaluate task-specific aspects such as the accuracy of referencing to characters in the model outputs. Besides the above-mentioned metrics (tailored to measure specific features of the generated content), there is also an increasing adoption and reliance on using pre-trained LLMs and VLMs as judges \cite{llmeval2}. Essentially, these pre-trained general-purpose models are prompted to score or rate a model-generated response along any of the evaluation dimensions of interest, such as fluency or relevance (e.g.,\textit{ `How fluent is the generated text on a scale of 1 to 5?'}). However, the effectiveness and reliability of this approach is still debated \cite{llmeval1}.

\paragraph{Benchmarks.}
With the increase in scale, data, and extensive multi-step training processes it is difficult to fully understand the capabilities of models based only on their performance on held-out test splits of task-specific datasets. To address this limitation, numerous benchmark datasets have been proposed that focus on evaluating more fine-grained abilities of models. Some of the popular benchmarks proposed to test models trained for multi-image scenarios include: NLVR2 \cite{cc_nlvr2} which focuses on models' ability to understand the visual compositionality given a pair of images and corresponding textual description; ViLMA \cite{vc_vilma} and MVBench \cite{videoqa_mvbench} which focus on testing models' spatio-temporal reasoning capabilities (e.g., counting actions across frames of a video); Mementos \cite{mementos} which studies object and behavioral hallucinations, and their interconnectedness.

Despite continued progress to improve and update existing benchmark datasets to cover various edge cases, evaluation using benchmarks is not without its limitations. For instance, some benchmarks are constructed using data from test/validation splits of existing popular datasets in the community, leading to a potential contamination problem \cite{contaminated_benchmarks}. Moreover, recent modeling approaches typically tend to incorporate most existing benchmark datasets into their fine-tuning process to ensure the stability and generalization of models in real-world applications. When such approaches abstain from disclosing the data used for training the models, it generally undermines the process of automatic evaluation using benchmarks for comparing models against each other.

\subsection{Human Evaluation}

Given the current state of automatic evaluation, some multi-image-to-text tasks such as \color{xkcdVividBlue}VC \color{black}and \color{xkcdVividBlue}VST \color{black}rely on human evaluation to accurately determine the quality of the model-generated text. This process involves recruiting online crowd-workers who are native or proficient speakers of the target language. Depending on the type of data or variation of the task, annotators with expertise and familiarity with terminology relevant to the corresponding domain (e.g., medical or sports videos) might be preferred.

Participants of the evaluation study are provided with a set of task-specific rubrics/instructions along with representative examples required for judging the model outputs. They are asked to assess the overall quality of model-generated outputs either independently (per sample) \cite{groovist} or relative to outputs from other models \cite{rovist}. Alternatively, evaluators might be required to provide scores/ratings for various criteria ranging from broad (e.g., text conciseness, fluency, grammatical correctness) to specific (e.g., factuality, hallucinations, expressiveness). The obtained scores are usually compared pairwise to rank models appropriately.

Some pre-trained VLM frameworks such as LLaVA-RLHF \cite{llava_rlhf} leverage this qualitative feedback to optimize model parameters for learning to generate human-preferred text. Although human evaluation is still indispensable for several tasks, it is also expensive, time-consuming, and challenging. Defining clear evaluation protocols for ensuring the reliability and quality of human judgments is an active research area \cite{human_eval_protocols1,human_eval_protocols2}.