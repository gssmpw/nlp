\section{Introduction}

Over the years, research in natural language generation has demonstrated the importance of grounding language in the visual modality to improve understanding and reasoning capabilities of models \cite{mm_reasoning0,mm_reasoning1,mm_reasoning2}. Earlier work on visually conditioned language generation primarily focused on single-image-to-text tasks such as image captioning and visual question answering. However, many practical real-world vision-to-language (V2L) applications in several domains such as \textit{surveillance} and \textit{media content creation} require understanding and reasoning across multiple temporally ordered images or video frames. With the increase in availability of video and image sequence data, various tasks have been proposed over the past few years to develop and evaluate models that can generate text grounded in multiple images or frames of videos. While some proposed multi-image-to-text tasks such as \textit{Video Captioning} (\color{xkcdVividBlue}VC\color{black}) \cite{vc_task} and \textit{Multi-image/Video Question Answering} (\color{xkcdVividBlue}VIDQA\color{black}) \cite{videoqa,miqa} are reminiscent of popular single-image-to-text settings, other tasks such as \textit{Change Captioning} (\color{xkcdVividBlue}CC\color{black}) \cite{cc_spot_the_diff} are unique both in terms of their objectives and the type of input-output data.

\begin{table}[t]
    \centering
    \begin{tabular}{rcc}
        % \toprule
        \rowcolor{gray!25}
        \textsc{Task}    &   \textsc{Input}   &   \textsc{Output}\\
        \midrule[0.1pt]
        \textit{Video Captioning}    &   \color{xkcdSky}\faVideo\color{black}  &   Caption\\
        \midrule[0.1pt]
        \textit{Change Captioning}    &   \color{xkcdOlive}\faImages\color{black}  &   Caption\\ % \makecell{Caption\\differences}
        \midrule[0.1pt]
        \textit{Movie Auto AD}    &   \color{xkcdRust}\faFilm\color{black}  &   Story\\
        \midrule[0.1pt]
        \textit{Visual Storytelling}    &   \color{xkcdOlive}\faImages\color{black}\ \textit{or} \color{xkcdSky}\faVideo\color{black}  &   Story\\
        \midrule[0.1pt]
        \textit{Multi-image/Video QA}    &   \makecell{Question + \\ \color{xkcdOlive}\faImages\color{black}\ \textit{or} \color{xkcdSky}\faVideo\color{black}}  &   Answer\\
        % \bottomrule
    \end{tabular}
    \caption{Outline of multi-image-to-text tasks along with corresponding input and output data type---videos, image sequences, and movie clips are denoted using \color{xkcdSky}\faVideo\color{black},\ \color{xkcdOlive}\faImages\color{black},\ and \color{xkcdRust}\faFilm\color{black}\ respectively. }
    \label{tab:tasks_outline}
\end{table}

\begin{figure*}[ht]
    \centering
    \subcaptionbox{Between consecutive images or video frames of input.} %sequences.}
    {\includegraphics[width=0.49\textwidth]{images/review_tasks_v_similarities.pdf}}
    \hfill
    \subcaptionbox{Between consecutive sentences in ground-truth text.} %output.}
    {\includegraphics[width=0.49\textwidth]{images/review_tasks_l_similarities.pdf}}
    \caption{Similarity scores obtained for the tasks along the visual and textual dimensions. We exclude \textit{Video Question Answering} task from \emph{textual consistency} analysis due to the lack of multi-sentence datasets.}
    \label{fig:data_analysis}
\end{figure*}

Nevertheless, all multi-image-to-text tasks require models to reason along the temporal dimension of the visual input for generating the textual output. Therefore, in this position paper, we consider all multi-image-to-text tasks as instances of the broader problem of \textit{generating natural language output given a sequence of multiple temporally ordered images or video frames as input}. Table~\ref{tab:tasks_outline} outlines a set of tasks that we consider as good representatives of this problem and presents the differences between the corresponding input and output data type. For all the tasks we provide examples in Table~\ref{tab:review_tasks_examples}.

In summary, our main contributions are:
\begin{itemize}[noitemsep,topsep=0pt,]
    \item[\textbf{\romannumeral 1.}] We quantitatively characterize each of the multi-image V2L tasks along two dimensions based on the complexities of their input-output data. We argue that the degree to which they vary along these dimensions is dependent on the corresponding task definitions and objectives. 
    \item[\textbf{\romannumeral 2.}] We highlight and discuss a common set of challenges relevant for multi-image-to-text generation such as accurate tracking and grounding of entities, ensuring coherence between output text segments, \textit{inter alia}.
    \item[\textbf{\romannumeral 3.}] We comprehensively review the evolution of modeling approaches, learning procedures, and evaluation protocols and provide a unified overview, which we believe will be useful for facilitating further advancements.
    \item[\textbf{\romannumeral 4.}] Finally, we propose future research directions to improve the systems and methods used for tackling these tasks at different stages of the process.
\end{itemize}