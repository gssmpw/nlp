\section{Discussion}
\label{sec:future_directions}

As discussed above, the problem of generating text from a sequence of temporally ordered images or frames is a challenging one, and relevant to several downstream tasks and applications. Here, we reflect on some crucial aspects and outline various prospective research directions (RDs) and takeaways.

\paragraph{RD 1: Towards more naturalistic scenarios.}
Many of the multi-image-to-text tasks we consider in this work have real-world applications. For instance, solutions to the \color{xkcdVividBlue}CC \color{black} task can be used for assisted surveillance and for tracking changes in digital media assets \cite{cc_spot_the_diff}. In the \color{xkcdVividBlue}MAAD \color{black} task, models are required to generate descriptions that complement information in the original audio dialog/soundtrack, for improved accessibility to visually impaired users and for enhancing the visual experience of sighted users \cite{maad2}.

However, many day-to-day human-centered scenarios involve personalizing to various contexts or situations. We argue that existing multi-image-to-text tasks in their definitions and settings do not fully reflect this aspect. Tailoring model-generated descriptions/narrations to the perspective of end-users requires task settings in which models and humans can interact iteratively. Such settings would enable incorporation of human expectations and communicative contexts which typically tend to be dynamic in real-world applications. To this end, we advocate for variations of existing tasks where models can learn to contextualize and reason through interactions with other agents (humans or other models) for generating stories, descriptions, or answers. Furthermore, we also advocate for exploration of controlled task settings in which models are expected to generate text adhering to a specific style \cite{rd_task1} or point-of-view.

\paragraph{RD 2: Are general-purpose VLMs all we need?}
As discussed in Section~\ref{sec:4_4}, VLMs that are trained on various general-purpose datasets are increasingly being used for multi-image-to-text tasks through prompting. Powerful open-source models such as Molmo \cite{molmo}, and models optimized for multi-image scenarios such as Mantis \cite{mantis} are becoming increasingly available, suggesting that the trend of adopting them off-the-shelf for solving many V2L tasks is widespread. General-purpose VLMs learn abundant information through multitask pre-training and have a modular design, making them suitable for many downstream tasks. Their modularity also enables for seamless adaptation of VLMs to various novel domains (e.g., medical science) by updating only a small fraction of their parameters \cite{rd_model1}.

Despite the promising generalization of VLMs to certain tasks and domains, they have also been shown to be sensitive to prompts \cite{rd_model2_a,rd_model2_b} and biased towards the textual modality \cite{rd_model3}. To address these problems, recent work proposes various \textit{prompt engineering} techniques to facilitate inference-time adaptation of prompts to make them more suitable for the specific task of interest \cite{rd_model4,rd_model5}. On the other hand, task-specific model architectures consist of components designed to effectively address specialized aspects of the tasks, e.g., computing a \textit{difference representation} of the input image pair in \color{xkcdVividBlue}CC\color{black}. We advocate for modular modeling approaches that bring together efficient task-specific components and combine them with the powerful foundational VLMs. Furthermore, we argue that using graph-based architectures and memory-based modules would result in improved tracking of entity positions/relationships and enable models to assign saliency to memorable events in tasks like \color{xkcdVividBlue}VST \color{black} or \color{xkcdVividBlue}MAAD\color{black}.

\paragraph{RD 3: Improving and rethinking evaluation.}
In Section~\ref{sec:eval}, we discussed the various approaches for evaluating model outputs in multi-image-to-text tasks. While human evaluation is impractical for conducting large-scale assessments, existing automatic evaluation metrics are limited in terms of fully capturing the abilities of models. Increasingly various benchmarking datasets are being proposed to assess models along different axes important for grounding language in the visual input \cite{rd_eval2}. However, many benchmarks often suffer from the problem of \textit{visual content irrelevance}, which refers to models performing well on the benchmark datasets by primarily relying only on the language modality \cite{rd_eval1}. Furthermore, data leakage and contamination problems (see section~\ref{sec:5_1}) also hinder fair and accurate testing of model's skills using benchmarks.

While it is important to continue directing research efforts towards developing more extensive multi-image benchmarks such as ReMI \cite{rd_eval3}, we argue that the purpose of evaluation is to also provide insights that can be directly leveraged for improving model architectures and learning procedures. For single-image-to-text tasks, recent works have adapted interpretability methods that focus on understanding the behavior and internal representations of models \cite{rd_eval4,rd_eval5}. These methods can complement traditional evaluation techniques for enabling intra- and inter-model comparisons of behaviors and mechanisms for obtaining a holistic understanding. We strongly advocate for work that adapts various categories of interpretability methods for multi-image-to-text scenarios.