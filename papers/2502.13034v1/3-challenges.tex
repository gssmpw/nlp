\section{Common Challenges}
\label{sec:challenges}

Multi-image-to-text tasks present a unique set of challenges. Some of these challenges are task-specific, but most of them are common across the five tasks. In this section, we describe these challenges and discuss how they instantiate for each of the tasks.

\paragraph{Entity tracking.}
Identifying and tracking entities is an important requisite for accurately interpreting actions and relationships between them. This has been underlined by several works in both language understanding \cite{et_lm1,et_lm2} and computer vision domains \cite{et_vm1,et_vm2}. In multi-image V2L tasks, the availability of input signals from two modalities makes the aspect of disambiguating entities more challenging. For instance, in the \color{xkcdVividBlue}MAAD \color{black} or \color{xkcdVividBlue}VST \color{black} tasks where the visual input is heterogeneous, entities in the input image sequences/video clips tend to `disappear' in some of the images/frames at the intermediate temporal positions, while still being actively referenced in the textual input at the corresponding positions. \citet{groovist} have denoted such cases as being \textit{temporally misaligned}. To track entities accurately under such temporal misalignment, it is necessary to not only learn causal relations between people and objects in each of the modalities at corresponding positions, but also to obtain a cross-modal cross-temporal representation of all the relationships relevant to the scene/narrative. For the \color{xkcdVividBlue}CC\color{black}, \color{xkcdVividBlue}VC\color{black}, or \color{xkcdVividBlue}VIDQA \color{black} tasks, in which the input images/video frames are typically similar to each other, it is crucial to differentiate meaningful semantic entities and their changes from various distractions. While in \color{xkcdVividBlue}CC\color{black}, viewpoint changes or illuminations are considered as distractors and discarded, in the \color{xkcdVividBlue}VIDQA \color{black} task, entities relevant to answering the question need to be differentiated from others for accurate tracking. Effective tracking of entities would therefore require accounting for changes in appearance (including disappearance), capturing interactions, and correctly identifying occlusions.

\paragraph{Visual grounding.}
Humans acquire language understanding through perception and interaction with the environment \cite{vg0,lang_acquisition} and consequently this enables them to seamlessly ground language in visual data. Over the years, a great deal of work has been proposed to adapt the architecture and learning process of vision-language models for acquiring visual grounding \cite{vg1}. However, there are still significant challenges for achieving human-levels of grounding using computational models and this becomes more apparent when looked at from the perspective of multi-image-to-text tasks. For instance, in the \color{xkcdVividBlue}VST \color{black} or \color{xkcdVividBlue}MAAD \color{black} tasks, since language is typically `inconsistent' (see Figure~\ref{fig:shared_space}), it is also inadvertently under-specified semantically \cite{semantic_underspecification} (e.g., `\textit{The boy on the bridge was waving to the tourists near the waterfall. A photographer over \color{red}\textbf{there} \color{black} clicks...'}). Stories also tend to contain many abstract adverbs such as `\textit{often}' or `\textit{today}' and it has been shown that vision-language models struggle to disambiguate such under-specified text and accurately map phrases to regions in the image sequences/videos. Moreover, the amount of language informativeness---degree of information required for identifying the correct object \cite{vg2}---could be inadequate in tasks such as \color{xkcdVividBlue}VIDQA\color{black}, particularly with the presence of confounding entities in various frames (e.g., input video of a football match and a question: \textit{`What is the color of the card the referee is holding?'}). Also, when grounding objects, it has been shown that models often struggle to reliably capture spatial relationships \cite{vg3}. To summarize, beyond being merely descriptive, language in some multi-image-to-text tasks could also be complementary to the data in images/videos, making visual grounding challenging without access to relevant additional external knowledge.

\paragraph{Knowledge integration.}
For some multi-image-to-text tasks, models would be required to utilize additional information beyond what is available in the input data. In \color{xkcdVividBlue}VC \color{black} or \color{xkcdVividBlue}VIDQA \color{black} tasks pertaining to certain domains such as \emph{news}, the input video might not contain all the aspects needed to correctly describe its contents or answer questions about it \cite{vc_kg_task,videoqa_kg_task}. To address this, various approaches often rely on using large pre-trained general purpose models or external knowledge bases such as ConceptNet \cite{concept_net} to retrieve both factual and commonsense information. This method is commonly referred as retrieval-augmented generation (RAG) \cite{rag}. Besides being sources for missing information, external knowledge bases are also leveraged for enriching the generated text with social or cultural contexts. For instance, in \color{xkcdVividBlue}VST\color{black}, some approaches use recognized visual objects in the input to retrieve concepts from external knowledge graphs for generating more engaging and figurative stories (e.g., concept of `\textit{graduation ceremony}' following the detection of an `\textit{academic gown}' object).

From knowledge selection/retrieval stage to accurately representing and utilizing it during text generation, the process of integrating external knowledge has various challenges. Robust retrieval systems are required which can holistically extract the essence of image sequences/video frames, including the various entities and their interrelationships. Typically, the retrieved knowledge is concatenated with input representations which are then used for generating text either through fine-tuning \cite{kg_finetuning} or by prompting general-purpose VLMs \cite{kg_promptbased}. However, this approach might lead to models either over or under utilizing the retrieved knowledge potentially leading to incoherent text \cite{rag_issues}. To address this, we argue that fusion mechanisms which can effectively balance information from representations of both input data and the retrieved knowledge need to be developed. Furthermore, retrieving relevant knowledge from increasingly large databases could be computationally expensive, especially in the multi-image scenario. Ways to optimize retrieval components for improving efficiency is an active research area.

\paragraph{Textual coherence.}
Coherence is the property of text that refers to the ordering of its constituents (words/sentences) and the way in which they relate to each other \cite{coh1}. Coherent text should have a consistent logical structure in which the events, interactions, and relationships between various elements are ordered in a meaningful way. It is an important aspect of discourse and has been studied extensively in neural language generation \cite{coh2}. For several multi-image-to-text tasks, particularly the ones in which the output tends to be less `consistent' along the text axis in Figure~\ref{fig:shared_space}, it is challenging to ensure that multiple sentences in the generated output are locally coherent. In \color{xkcdVividBlue}MAAD \color{black}and \color{xkcdVividBlue}VST \color{black} tasks, where there are multiple characters and various interactions developing across the sequence of images/video frames, it is often difficult for models to balance between selecting the visual information and representing it in a cohesive/connected language \cite{coh3}. This challenge is more apparent in the \color{xkcdVividBlue}VST \color{black} task, in which models are expected to keep track of multiple things such as emotional arcs or motivations of the characters and the overarching narrative \cite{nytws}. There is increasing work in unimodal text-only storytelling suggesting how using concepts of narratology \cite{narrativity1,narrativity2} such as \citet{narrativity_genette}'s triangle can potentially aid models in generating stories with engaging and coherent structures. However, it still unclear how these theories can be applied to multimodal scenarios where the generated text needs to be consistent with image sequences/videos.

\paragraph{Theory of mind.}
Theory of Mind (ToM), which is considered the basis of human social cognition \cite{tom1}, is described as the ability to understand and make inferences about the mental states (e.g., beliefs, intentions, and desires) of other people or living beings. In the context of multi-image V2L tasks, ToM refers to the ability of models to go beyond merely recognizing objects/actions and to reason about the mental states of entities depicted in the image sequences/videos. Although most of the tasks we consider in this work, besides \color{xkcdVividBlue}VIDQA \color{black} \cite{videoqa_bdiqa}, do not explicitly require ToM abilities, the skill is still relevant for all the tasks and closely connected to the other challenges and abilities discussed so far. For instance, in tasks such as \color{xkcdVividBlue}VST\color{black}, to causally connect heterogeneous images in the input sequence, models need to be equipped with different reasoning abilities pertaining to emotions and social perceptions/intentions. This enables generation of stories that reflect actions and mental states of the characters beyond literal interpretation of the visual data.

Recently, several ToM benchmarks have been proposed to assess general-purpose VLMs \cite{tom2,tom3} along different aspects such as temporal localization of emotions, intentionality-understanding, and perspective-taking. However, these studies find that only models that are fine-tuned on curated ToM datasets exhibit any reasoning abilities, albeit not aligning with the well-established ToM theories/frameworks explaining human social cognition. Such curated data is scarcely available and it is unclear what alternative architectures or training objectives would enable models to obtain the ToM abilities required for multi-image V2L tasks.