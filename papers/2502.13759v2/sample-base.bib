
% Journals

% First the Full Name is given, then the abbreviation used in the AMS Math
% Reviews, with an indication if it could not be found there.
% Note the 2nd overwrites the 1st, so swap them if you want the full name.

 %{AMS}
 @String{AMSTrans = "American Mathematical Society Translations" }
 @String{AMSTrans = "Amer. Math. Soc. Transl." }
 @String{BullAMS = "Bulletin of the American Mathematical Society" }
 @String{BullAMS = "Bull. Amer. Math. Soc." }
 @String{ProcAMS = "Proceedings of the American Mathematical Society" }
 @String{ProcAMS = "Proc. Amer. Math. Soc." }
 @String{TransAMS = "Transactions of the American Mathematical Society" }
 @String{TransAMS = "Trans. Amer. Math. Soc." }

 %ACM
 @String{CACM = "Communications of the {ACM}" }
 @String{CACM = "Commun. {ACM}" }
 @String{CompServ = "Comput. Surveys" }
 @String{JACM = "J. ACM" }
 @String{ACMMathSoft = "{ACM} Transactions on Mathematical Software" }
 @String{ACMMathSoft = "{ACM} Trans. Math. Software" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newsletter" }
 @String{SIGNUM = "{ACM} {SIGNUM} Newslett." }

 @String{AmerSocio = "American Journal of Sociology" }
 @String{AmerStatAssoc = "Journal of the American Statistical Association" }
 @String{AmerStatAssoc = "J. Amer. Statist. Assoc." }
 @String{ApplMathComp = "Applied Mathematics and Computation" }
 @String{ApplMathComp = "Appl. Math. Comput." }
 @String{AmerMathMonthly = "American Mathematical Monthly" }
 @String{AmerMathMonthly = "Amer. Math. Monthly" }
 @String{BIT = "{BIT}" }
 @String{BritStatPsych = "British Journal of Mathematical and Statistical
          Psychology" }
 @String{BritStatPsych = "Brit. J. Math. Statist. Psych." }
 @String{CanMathBull = "Canadian Mathematical Bulletin" }
 @String{CanMathBull = "Canad. Math. Bull." }
 @String{CompApplMath = "Journal of Computational and Applied Mathematics" }
 @String{CompApplMath = "J. Comput. Appl. Math." }
 @String{CompPhys = "Journal of Computational Physics" }
 @String{CompPhys = "J. Comput. Phys." }
 @String{CompStruct = "Computers and Structures" }
 @String{CompStruct = "Comput. \& Structures" }
 @String{CompJour = "The Computer Journal" }
 @String{CompJour = "Comput. J." }
 @String{CompSysSci = "Journal of Computer and System Sciences" }
 @String{CompSysSci = "J. Comput. System Sci." }
 @String{Computing = "Computing" }
 @String{ContempMath = "Contemporary Mathematics" }
 @String{ContempMath = "Contemp. Math." }
 @String{Crelle = "Crelle's Journal" }
 @String{GiornaleMath = "Giornale di Mathematiche" }
 @String{GiornaleMath = "Giorn. Mat." } % didn't find in AMS MR., ibid.

 %IEEE
 @String{Computer = "{IEEE} Computer" }
 @String{IEEETransComp = "{IEEE} Transactions on Computers" }
 @String{IEEETransComp = "{IEEE} Trans. Comput." }
 @String{IEEETransAC = "{IEEE} Transactions on Automatic Control" }
 @String{IEEETransAC = "{IEEE} Trans. Automat. Control" }
 @String{IEEESpec = "{IEEE} Spectrum" } % didn't find in AMS MR
 @String{ProcIEEE = "Proceedings of the {IEEE}" }
 @String{ProcIEEE = "Proc. {IEEE}" } % didn't find in AMS MR
 @String{IEEETransAeroElec = "{IEEE} Transactions on Aerospace and Electronic
     Systems" }
 @String{IEEETransAeroElec = "{IEEE} Trans. Aerospace Electron. Systems" }

 @String{IMANumerAna = "{IMA} Journal of Numerical Analysis" }
 @String{IMANumerAna = "{IMA} J. Numer. Anal." }
 @String{InfProcLet = "Information Processing Letters" }
 @String{InfProcLet = "Inform. Process. Lett." }
 @String{InstMathApp = "Journal of the Institute of Mathematics and
     its Applications" }
 @String{InstMathApp = "J. Inst. Math. Appl." }
 @String{IntControl = "International Journal of Control" }
 @String{IntControl = "Internat. J. Control" }
 @String{IntNumerEng = "International Journal for Numerical Methods in
     Engineering" }
 @String{IntNumerEng = "Internat. J. Numer. Methods Engrg." }
 @String{IntSuper = "International Journal of Supercomputing Applications" }
 @String{IntSuper = "Internat. J. Supercomputing Applic." } % didn't find
%% in AMS MR
 @String{Kibernetika = "Kibernetika" }
 @String{JResNatBurStand = "Journal of Research of the National Bureau
     of Standards" }
 @String{JResNatBurStand = "J. Res. Nat. Bur. Standards" }
 @String{LinAlgApp = "Linear Algebra and its Applications" }
 @String{LinAlgApp = "Linear Algebra Appl." }
 @String{MathAnaAppl = "Journal of Mathematical Analysis and Applications" }
 @String{MathAnaAppl = "J. Math. Anal. Appl." }
 @String{MathAnnalen = "Mathematische Annalen" }
 @String{MathAnnalen = "Math. Ann." }
 @String{MathPhys = "Journal of Mathematical Physics" }
 @String{MathPhys = "J. Math. Phys." }
 @String{MathComp = "Mathematics of Computation" }
 @String{MathComp = "Math. Comp." }
 @String{MathScand = "Mathematica Scandinavica" }
 @String{MathScand = "Math. Scand." }
 @String{TablesAidsComp = "Mathematical Tables and Other Aids to Computation" }
 @String{TablesAidsComp = "Math. Tables Aids Comput." }
 @String{NumerMath = "Numerische Mathematik" }
 @String{NumerMath = "Numer. Math." }
 @String{PacificMath = "Pacific Journal of Mathematics" }
 @String{PacificMath = "Pacific J. Math." }
 @String{ParDistComp = "Journal of Parallel and Distributed Computing" }
 @String{ParDistComp = "J. Parallel and Distrib. Comput." } % didn't find
%% in AMS MR
 @String{ParComputing = "Parallel Computing" }
 @String{ParComputing = "Parallel Comput." }
 @String{PhilMag = "Philosophical Magazine" }
 @String{PhilMag = "Philos. Mag." }
 @String{ProcNAS = "Proceedings of the National Academy of Sciences
                    of the USA" }
 @String{ProcNAS = "Proc. Nat. Acad. Sci. U. S. A." }
 @String{Psychometrika = "Psychometrika" }
 @String{QuartMath = "Quarterly Journal of Mathematics, Oxford, Series (2)" }
 @String{QuartMath = "Quart. J. Math. Oxford Ser. (2)" }
 @String{QuartApplMath = "Quarterly of Applied Mathematics" }
 @String{QuartApplMath = "Quart. Appl. Math." }
 @String{RevueInstStat = "Review of the International Statisical Institute" }
 @String{RevueInstStat = "Rev. Inst. Internat. Statist." }

 %SIAM
 @String{JSIAM = "Journal of the Society for Industrial and Applied
     Mathematics" }
 @String{JSIAM = "J. Soc. Indust. Appl. Math." }
 @String{JSIAMB = "Journal of the Society for Industrial and Applied
     Mathematics, Series B, Numerical Analysis" }
 @String{JSIAMB = "J. Soc. Indust. Appl. Math. Ser. B Numer. Anal." }
 @String{SIAMAlgMeth = "{SIAM} Journal on Algebraic and Discrete Methods" }
 @String{SIAMAlgMeth = "{SIAM} J. Algebraic Discrete Methods" }
 @String{SIAMAppMath = "{SIAM} Journal on Applied Mathematics" }
 @String{SIAMAppMath = "{SIAM} J. Appl. Math." }
 @String{SIAMComp = "{SIAM} Journal on Computing" }
 @String{SIAMComp = "{SIAM} J. Comput." }
 @String{SIAMMatrix = "{SIAM} Journal on Matrix Analysis and Applications" }
 @String{SIAMMatrix = "{SIAM} J. Matrix Anal. Appl." }
 @String{SIAMNumAnal = "{SIAM} Journal on Numerical Analysis" }
 @String{SIAMNumAnal = "{SIAM} J. Numer. Anal." }
 @String{SIAMReview = "{SIAM} Review" }
 @String{SIAMReview = "{SIAM} Rev." }
 @String{SIAMSciStat = "{SIAM} Journal on Scientific and Statistical
     Computing" }
 @String{SIAMSciStat = "{SIAM} J. Sci. Statist. Comput." }

 @String{SoftPracExp = "Software Practice and Experience" }
 @String{SoftPracExp = "Software Prac. Experience" } % didn't find in AMS MR
 @String{StatScience = "Statistical Science" }
 @String{StatScience = "Statist. Sci." }
 @String{Techno = "Technometrics" }
 @String{USSRCompMathPhys = "{USSR} Computational Mathematics and Mathematical
     Physics" }
 @String{USSRCompMathPhys = "{U. S. S. R.} Comput. Math. and Math. Phys." }
 @String{VLSICompSys = "Journal of {VLSI} and Computer Systems" }
 @String{VLSICompSys = "J. {VLSI} Comput. Syst." }
 @String{ZAngewMathMech = "Zeitschrift fur Angewandte Mathematik und
     Mechanik" }
 @String{ZAngewMathMech = "Z. Angew. Math. Mech." }
 @String{ZAngewMathPhys = "Zeitschrift fur Angewandte Mathematik und Physik" }
 @String{ZAngewMathPhys = "Z. Angew. Math. Phys." }

% Publishers % ================================================= |

 @String{Academic = "Academic Press" }
 @String{ACMPress = "{ACM} Press" }
 @String{AdamHilger = "Adam Hilger" }
 @String{AddisonWesley = "Addison-Wesley" }
 @String{AllynBacon = "Allyn and Bacon" }
 @String{AMS = "American Mathematical Society" }
 @String{Birkhauser = "Birkha{\"u}ser" }
 @String{CambridgePress = "Cambridge University Press" }
 @String{Chelsea = "Chelsea" }
 @String{ClaredonPress = "Claredon Press" }
 @String{DoverPub = "Dover Publications" }
 @String{Eyolles = "Eyolles" }
 @String{HoltRinehartWinston = "Holt, Rinehart and Winston" }
 @String{Interscience = "Interscience" }
 @String{JohnsHopkinsPress = "The Johns Hopkins University Press" }
 @String{JohnWileySons = "John Wiley and Sons" }
 @String{Macmillan = "Macmillan" }
 @String{MathWorks = "The Math Works Inc." }
 @String{McGrawHill = "McGraw-Hill" }
 @String{NatBurStd = "National Bureau of Standards" }
 @String{NorthHolland = "North-Holland" }
 @String{OxfordPress = "Oxford University Press" }  %address Oxford or London?
 @String{PergamonPress = "Pergamon Press" }
 @String{PlenumPress = "Plenum Press" }
 @String{PrenticeHall = "Prentice-Hall" }
 @String{SIAMPub = "{SIAM} Publications" }
 @String{Springer = "Springer-Verlag" }
 @String{TexasPress = "University of Texas Press" }
 @String{VanNostrand = "Van Nostrand" }
 @String{WHFreeman = "W. H. Freeman and Co." }

%Entries

@article{1984:1040142,
 address = {New York, NY, USA},
 issn = {0146-4833},
 issue_date = {January/April 1984},
 journal = {SIGCOMM Comput. Commun. Rev.},
 key = {{$\!\!$}},
 number = {5-1},
 publisher = {ACM},
 volume = {13-14},
 year = {1984}
}

@inproceedings{2004:ITE:1009386.1010128,
 acmid = {1010128},
 address = {Washington, DC, USA},
 booktitle = {Proceedings of the IEEE International Conference on Web Services},
 doi = {http://dx.doi.org/10.1109/ICWS.2004.64},
 isbn = {0-7695-2167-3},
 key = {IEEE},
 pages = {21--22},
 publisher = {IEEE Computer Society},
 series = {ICWS '04},
 title = {IEEE TCSC Executive Committee},
 url = {http://dx.doi.org/10.1109/ICWS.2004.64},
 year = {2004}
}

@misc{2023xtuner,
 author = {XTuner Contributors},
 title = {{XTuner: A Toolkit for Efficiently Fine-tuning LLM}},
 year = {2023}
}

@article{6:1:1,
 author = {J. E. {Archer, Jr.} and R. Conway and F. B. Schneider},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {1--19},
 title = {User recovery and reversal in interactive systems},
 year = {1984}
}

@article{6:3:380,
 author = {E. Korach and D.  Rotem and N. Santoro},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {380--401},
 title = {Distributed algorithms for finding centers and medians in networks},
 year = {1984}
}

@article{7:1:137,
 author = {D. D. Dunlop and V. R. Basili},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {137--158},
 title = {Generalizing specifications for uniformly implemented loops},
 year = {1985}
}

@article{7:2:183,
 author = {J. Heering and P. Klint},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {183--213},
 title = {Towards monolingual programming environments},
 year = {1985}
}

@article{7:3:359,
 author = {F. Nielson},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {359--379},
 title = {Program transformations in a denotational setting},
 year = {1985}
}

@techreport{897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987}
}

@article{abbas2023semdedup,
 author = {Abbas, Amro and Tirumala, Kushal and Simig, D{\'a}niel and Ganguli, Surya and Morcos, Ari S},
 journal = {arXiv preprint arXiv:2303.09540},
 title = {SemDeDup: Data-efficient learning at web-scale through semantic deduplication},
 year = {2023}
}

@misc{abdin2024phi3,
 archivePrefix = {arXiv},
 author = {Marah Abdin and Sam Ade Jacobs and Ammar Ahmad Awan and Jyoti Aneja and Ahmed Awadallah et al.},
 eprint = {2404.14219},
 primaryClass = {cs.CL},
 title = {Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone},
 year = {2024}
}

@book{abelson-et-al:scheme,
 address = {Cambridge, Massachusetts},
 author = {Harold Abelson and Gerald~Jay Sussman and Julie Sussman},
 publisher = {MIT Press},
 title = {Structure and Interpretation of Computer Programs},
 year = {1985}
}

@article{achiam2023gpt,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {arXiv preprint arXiv:2303.08774},
 title = {GPT-4 Technical Report},
 year = {2023}
}

@inproceedings{Adya-01,
 author = {A. Adya and P. Bahl and J. Padhye and A.Wolman and L. Zhou},
 booktitle = {Proceedings of the IEEE 1st International Conference on Broadnets Networks (BroadNets'04)},
 pages = {210--217},
 title = {A multi-radio unification protocol for {IEEE} 802.11 wireless networks},
 year = {2004}
}

@inproceedings{agrawal2019nocaps,
 author = {Agrawal, Harsh and Desai, Karan and Wang, Yufei and Chen, Xinlei and Jain, Rishabh and Johnson, Mark and Batra, Dhruv and Parikh, Devi and Lee, Stefan and Anderson, Peter},
 booktitle = {ICCV},
 title = {Nocaps: Novel object captioning at scale},
 year = {2019}
}

@article{AI-caps,
 author = {Wu, Jiahong and Zheng, He and Zhao, Bo and Li, Yixin and Yan, Baoming and Liang, Rui and Wang, Wenjia and Zhou, Shipei and Lin, Guosen and Fu, Yanwei and others},
 journal = {arXiv:1711.06475},
 title = {Ai challenger: A large-scale dataset for going deeper in image understanding},
 year = {2017}
}

@misc{ai2024yi,
 archivePrefix = {arXiv},
 author = {01. AI and : and Alex Young and Bei Chen and Chao Li and Chengen Huang and Ge Zhang and Guanwei Zhang and Heng Li and Jiangcheng Zhu and Jianqun Chen and Jing Chang and Kaidong Yu and Peng Liu and Qiang Liu and Shawn Yue and Senbin Yang and Shiming Yang and Tao Yu and Wen Xie and Wenhao Huang and Xiaohui Hu and Xiaoyi Ren and Xinyao Niu and Pengcheng Nie and Yuchi Xu and Yudong Liu and Yue Wang and Yuxuan Cai and Zhenyu Gu and Zhiyuan Liu and Zonghong Dai},
 eprint = {2403.04652},
 primaryClass = {cs.CL},
 title = {Yi: Open Foundation Models by 01.AI},
 year = {2024}
}

@article{aiello2023jointly,
 author = {Aiello, Emanuele and Yu, Lili and Nie, Yixin and Aghajanyan, Armen and Oguz, Barlas},
 journal = {arXiv preprint arXiv:2309.15564},
 title = {{Jointly Training Large Autoregressive Multimodal Models}},
 year = {2023}
}

@article{aiello2023jointly,
 author = {Aiello, Emanuele and Yu, Lili and Nie, Yixin and Aghajanyan, Armen and Oguz, Barlas},
 journal = {arXiv:2309.15564},
 title = {Jointly training large autoregressive multimodal models},
 year = {2023}
}

@inproceedings{aishell-1,
 author = {Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
 booktitle = {O-COCOSDA},
 title = {Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline},
 year = {2017}
}

@article{aishell-2,
 author = {Du, Jiayu and Na, Xingyu and Liu, Xuechen and Bu, Hui},
 journal = {arXiv:1808.10583},
 title = {Aishell-2: Transforming mandarin asr research into industrial scale},
 year = {2018}
}

@article{akbari2021vatt,
 author = {Akbari, Hassan and Yuan, Liangzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
 journal = {Proc. of NeurIPS},
 pages = {24206--24221},
 title = {{Vatt: Transformers for multimodal self-supervised learning from raw video, audio and text}},
 year = {2021}
}

@article{Akyildiz-01,
 author = {I. F. Akyildiz and W. Su and Y. Sankarasubramaniam and E. Cayirci},
 journal = {Comm. ACM},
 pages = {393--422},
 title = {Wireless Sensor Networks: A Survey},
 year = {2002}
}

@article{Akyildiz-02,
 author = {I. F. Akyildiz and T. Melodia and K. R. Chowdhury},
 journal = {Computer Netw.},
 pages = {921--960},
 title = {A Survey on Wireless Multimedia Sensor Networks},
 year = {2007}
}

@article{akyurek2023rl4f,
 author = {Aky{\"u}rek, Afra Feyza and Aky{\"u}rek, Ekin and Madaan, Aman and Kalyan, Ashwin and Clark, Peter and Wijaya, Derry and Tandon, Niket},
 journal = {arXiv preprint arXiv:2305.08844},
 title = {{RL4F: Generating Natural Language Feedback with Reinforcement Learning for Repairing Model Outputs}},
 year = {2023}
}

@article{alayrac2022flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
 journal = {Proc. of NeurIPS},
 pages = {23716--23736},
 title = {Flamingo: a visual language model for few-shot learning},
 year = {2022}
}

@article{alayrac2022flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
 journal = {Proc. of NeurIPS},
 pages = {23716--23736},
 title = {{Flamingo: a visual language model for few-shot learning}},
 year = {2022}
}

@article{alayrac2022flamingo,
 author = {Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
 journal = {NeurIPS},
 title = {Flamingo: a visual language model for few-shot learning},
 year = {2022}
}

@misc{alpaca,
 author = {Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B},
 title = {Stanford alpaca: An instruction-following llama model},
 year = {2023}
}

@manual{Amsthm15,
 month = {April},
 note = {\url{http://www.ctan.org/pkg/amsthm}},
 organization = {American Mathematical Society},
 title = {Using the amsthm Package},
 year = {2015}
}

@inproceedings{anderson2018bottom,
 author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
 booktitle = {CVPR},
 title = {Bottom-up and top-down attention for image captioning and visual question answering},
 year = {2018}
}

@article{anil2023palm,
 author = {Anil, Rohan and Dai, Andrew M and Firat, Orhan and Johnson, Melvin and Lepikhin, Dmitry and Passos, Alexandre and Shakeri, Siamak and Taropa, Emanuel and Bailey, Paige and Chen, Zhifeng and others},
 journal = {arXiv preprint arXiv:2305.10403},
 title = {Palm 2 technical report},
 year = {2023}
}

@article{anthropic2024claude,
 author = {Anthropic, AI},
 journal = {Claude-3 Model Card},
 title = {The claude 3 model family: Opus, sonnet, haiku},
 year = {2024}
}

@inproceedings{antol2015vqa,
 author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Zitnick, C Lawrence and Parikh, Devi},
 booktitle = {ICCV},
 title = {Vqa: Visual question answering},
 year = {2015}
}

@inproceedings{asai2023retrieval,
 author = {Asai, Akari and Min, Sewon and Zhong, Zexuan and Chen, Danqi},
 booktitle = {Proc. of ACL},
 pages = {41--46},
 title = {{Retrieval-based language models and applications}},
 year = {2023}
}

@inproceedings{astruc2024openstreetview,
 author = {Astruc, Guillaume and Dufour, Nicolas and Siglidis, Ioannis and Aronssohn, Constantin and Bouia, Nacim and Fu, Stephanie and Loiseau, Romain and Nguyen, Van Nguyen and Raude, Charles and Vincent, Elliot and others},
 booktitle = {Proc. of CVPR},
 pages = {21967--21977},
 title = {OpenStreetView-5M: The Many Roads to Global Visual Geolocation},
 year = {2024}
}

@article{awadalla2023openflamingo,
 author = {Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
 journal = {arXiv preprint arXiv:2308.01390},
 title = {{Openflamingo: An open-source framework for training large autoregressive vision-language models}},
 year = {2023}
}

@article{awadalla2023openflamingo,
 author = {Awadalla, Anas and Gao, Irena and Gardner, Josh and Hessel, Jack and Hanafy, Yusuf and Zhu, Wanrong and Marathe, Kalyani and Bitton, Yonatan and Gadre, Samir and Sagawa, Shiori and others},
 journal = {arXiv:2308.01390},
 title = {Openflamingo: An open-source framework for training large autoregressive vision-language models},
 year = {2023}
}

@inproceedings{Bahl-02,
 author = {P. Bahl and R. Chancre and J. Dungeon},
 booktitle = {Proceeding of the 10th International Conference on Mobile Computing and Networking (MobiCom'04)},
 pages = {112--117},
 title = {{SSCH}: Slotted Seeded Channel Hopping for Capacity Improvement in {IEEE} 802.11 Ad-Hoc Wireless Networks},
 year = {2004}
}

@article{bai2023,
 author = {Bai, Yunpeng and Shang, Changjing and Li, Ying and Shen, Liang and Jin, Shangzhu and Shen, Qiang},
 journal = {Mathematics},
 pages = {3839},
 title = {{Transport Object Detection in Street View Imagery Using Decomposed Convolutional Neural Networks}},
 year = {2023}
}

@article{bai2023qwen,
 author = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
 journal = {arXiv preprint arXiv:2308.12966},
 title = {{Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond}},
 year = {2023}
}

@article{bai2023qwen,
 author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
 journal = {arXiv preprint arXiv:2309.16609},
 title = {{Qwen technical report}},
 year = {2023}
}

@article{bai2023qwen,
 author = {Bai, Jinze and Bai, Shuai and Yang, Shusheng and Wang, Shijie and Tan, Sinan and Wang, Peng and Lin, Junyang and Zhou, Chang and Zhou, Jingren},
 journal = {arXiv:2308.12966},
 title = {Qwen-vl: A frontier large vision-language model with versatile abilities},
 year = {2023}
}

@misc{bai2023qwenvl,
 archivePrefix = {arXiv},
 author = {Jinze Bai and Shuai Bai and Shusheng Yang and Shijie Wang and Sinan Tan and Peng Wang and Junyang Lin and Chang Zhou and Jingren Zhou},
 eprint = {2308.12966},
 primaryClass = {cs.CV},
 title = {Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond},
 year = {2023}
}

@article{bai2024hallucination,
 author = {Bai, Zechen and Wang, Pichao and Xiao, Tianjun and He, Tong and Han, Zongbo and Zhang, Zheng and Shou, Mike Zheng},
 journal = {arXiv preprint arXiv:2404.18930},
 title = {Hallucination of Multimodal Large Language Models: A Survey},
 year = {2024}
}

@misc{bai2024hallucinationmultimodallargelanguage,
 archivePrefix = {arXiv},
 author = {Zechen Bai and Pichao Wang and Tianjun Xiao and Tong He and Zongbo Han and Zheng Zhang and Mike Zheng Shou},
 eprint = {2404.18930},
 primaryClass = {cs.CV},
 title = {Hallucination of Multimodal Large Language Models: A Survey},
 url = {https://arxiv.org/abs/2404.18930},
 year = {2024}
}

@article{bai2024survey,
 author = {Bai, Tianyi and Liang, Hao and Wan, Binwang and Yang, Ling and Li, Bozhou and Wang, Yifan and Cui, Bin and He, Conghui and Yuan, Binhang and Zhang, Wentao},
 journal = {arXiv preprint arXiv:2405.16640},
 title = {A Survey of Multimodal Large Language Model from A Data-centric Perspective},
 year = {2024}
}

@inproceedings{bain2021frozen,
 author = {Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
 booktitle = {Proc. of ICCV},
 pages = {1728--1738},
 title = {Frozen in time: A joint video and image encoder for end-to-end retrieval},
 year = {2021}
}

@article{baize,
 author = {Xu, Canwen and Guo, Daya and Duan, Nan and McAuley, Julian},
 journal = {arXiv:2304.01196},
 title = {Baize: An open-source chat model with parameter-efficient tuning on self-chat data},
 year = {2023}
}

@inproceedings{bao2022analytic,
 author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
 booktitle = {Proc. of ICLR},
 title = {{Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models}},
 year = {2022}
}

@inproceedings{bao2022analytic,
 author = {Bao, Fan and Li, Chongxuan and Zhu, Jun and Zhang, Bo},
 booktitle = {Proc. of ICLR},
 title = {{Analytic-DPM: an Analytic Estimate of the Optimal Reverse Variance in Diffusion Probabilistic Models}},
 year = {2022}
}

@inproceedings{bgf:Lixto,
 author = {Robert Baumgartner and Georg Gottlob and Sergio Flesca},
 booktitle = {Proceedings of the 27th International Conference on Very Large Databases},
 pages = {119--128},
 title = {Visual Information Extraction with {Lixto}},
 year = {2001}
}

@inproceedings{biten2022latr,
 author = {Biten, Ali Furkan and Litman, Ron and Xie, Yusheng and Appalaraju, Srikar and Manmatha, R},
 booktitle = {Proc. of CVPR},
 pages = {16548--16558},
 title = {Latr: Layout-aware transformer for scene-text vqa},
 year = {2022}
}

@inproceedings{bithel2023evaluating,
 author = {Bithel, Shivangi and Bedathur, Srikanta},
 booktitle = {Proc. of SIGIR},
 pages = {1960--1965},
 title = {Evaluating Cross-modal generative models using retrieval task},
 year = {2023}
}

@book{book-minimal,
 author = {Donald E. Knuth},
 publisher = {Addison-Wesley},
 title = {Seminumerical Algorithms},
 year = {1981}
}

@article{bowman:reasoning,
 author = {Bowman, Mic and Debray, Saumya K. and Peterson, Larry L.},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {795-825},
 title = {Reasoning About Naming Systems},
 year = {1993}
}

@article{braams:babel,
 author = {Braams, Johannes},
 journal = {TUGboat},
 pages = {291-301},
 title = {Babel, a Multilingual Style-Option System for Use with LaTeX's Standard Document Styles},
 year = {1991}
}

@article{brachman-schmolze:kl-one,
 author = {Ronald~J. Brachman and James~G. Schmolze},
 journal = {Cognitive Science},
 pages = {171--216},
 title = {An overview of the {KL-ONE} knowledge representation system},
 year = {1985}
}

@inproceedings{brock2021high,
 author = {Brock, Andy and De, Soham and Smith, Samuel L and Simonyan, Karen},
 booktitle = {Proc. of ICML},
 pages = {1059--1071},
 title = {{High-performance large-scale image recognition without normalization}},
 year = {2021}
}

@article{brown2020language,
 author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
 journal = {Proc. of NeurIPS},
 pages = {1877--1901},
 title = {{Language models are few-shot learners}},
 year = {2020}
}

@inproceedings{bu2017aishell,
 author = {Bu, Hui and Du, Jiayu and Na, Xingyu and Wu, Bengu and Zheng, Hao},
 booktitle = {2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA)},
 pages = {1--5},
 title = {Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline},
 year = {2017}
}

@techreport{Buss:1987:VTB:897367,
 author = {Buss, Jonathan F. and Rosenberg, Arnold L. and Knott, Judson D.},
 title = {Vertex Types in Book-Embeddings},
 year = {1987}
}

@inproceedings{caba2015activitynet,
 author = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
 booktitle = {Proc. of CVPR},
 pages = {961--970},
 title = {{Activitynet: A large-scale video benchmark for human activity understanding}},
 year = {2015}
}

@inproceedings{caba2015activitynet,
 author = {Caba Heilbron, Fabian and Escorcia, Victor and Ghanem, Bernard and Carlos Niebles, Juan},
 booktitle = {CVPR},
 title = {Activitynet: A large-scale video benchmark for human activity understanding},
 year = {2015}
}

@inproceedings{CaFo,
 author = {Zhang, Renrui and Hu, Xiangfei and Li, Bohao and Huang, Siyuan and Deng, Hanqiu and Qiao, Yu and Gao, Peng and Li, Hongsheng},
 booktitle = {CVPR},
 title = {Prompt, generate, then cache: Cascade of foundation models makes strong few-shot learners},
 year = {2023}
}

@article{cai2023benchlmm,
 author = {Cai, Rizhao and Song, Zirui and Guan, Dayan and Chen, Zhenhao and Luo, Xing and Yi, Chenyu and Kot, Alex},
 journal = {arXiv preprint arXiv:2312.02896},
 title = {Benchlmm: Benchmarking cross-style visual capability of large multimodal models},
 year = {2023}
}

@article{campbell2019,
 author = {Campbell, Andrew and Both, Alan and Sun, Qian Chayn},
 journal = {Computers, Environment and Urban Systems},
 pages = {101350},
 title = {{Detecting and mapping traffic signs from Google Street View images using deep learning and GIS}},
 year = {2019}
}

@inproceedings{carion2020end,
 author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
 booktitle = {Proc. of ECCV},
 title = {End-to-end object detection with transformers},
 year = {2020}
}

@article{CAT,
 author = {Wang, Teng and Zhang, Jinrui and Fei, Junjie and Ge, Yixiao and Zheng, Hao and Tang, Yunlong and Li, Zhe and Gao, Mingqi and Zhao, Shanshan and Shan, Ying and others},
 journal = {arXiv:2305.02677},
 title = {Caption anything: Interactive image description with diverse multimodal controls},
 year = {2023}
}

@inproceedings{CC,
 author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
 booktitle = {ACL},
 title = {Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
 year = {2018}
}

@inproceedings{CC-12m,
 author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
 booktitle = {CVPR},
 title = {Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
 year = {2021}
}

@article{cepeda2023,
 author = {Vivanco Cepeda, Vicente and Nayak, Gaurav Kumar and Shah, Mubarak},
 journal = {Proc. of NeurIPS},
 pages = {},
 title = {{GeoCLIP: Clip-Inspired Alignment between Locations and Images for Effective Worldwide Geo-localization}},
 year = {2024}
}

@article{Cerspense,
 author = {Cerspense},
 title = {{Zeroscope: Diffusion-based text-to-video synthesis}},
 year = {2023}
}

@article{Cerspense,
 author = {Cerspense},
 title = {{Zeroscope: Diffusion-based text-to-video synthesis}},
 year = {2023}
}

@inproceedings{cha2024visually,
 author = {Cha, Sungguk and Lee, Jusung and Lee, Younghyun and Yang, Cheoljong},
 booktitle = {Proc. of ICASSP},
 pages = {5510--5514},
 title = {Visually Dehallucinative Instruction Generation},
 year = {2024}
}

@article{chalvatzaras2022,
 author = {Chalvatzaras, Athanasios and Pratikakis, Ioannis and Amanatiadis, Angelos A},
 journal = {IEEE Transactions on Intelligent Vehicles},
 pages = {1574-1596},
 title = {{A Survey on Map-Based Localization Techniques for Autonomous Vehicles}},
 year = {2022}
}

@article{chameleon,
 author = {Lu, Pan and Peng, Baolin and Cheng, Hao and Galley, Michel and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Gao, Jianfeng},
 journal = {arXiv:2304.09842},
 title = {Chameleon: Plug-and-play compositional reasoning with large language models},
 year = {2023}
}

@inproceedings{changpinyo2021conceptual,
 author = {Changpinyo, Soravit and Sharma, Piyush and Ding, Nan and Soricut, Radu},
 booktitle = {Proc. of CVPR},
 pages = {3558--3568},
 title = {Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts},
 year = {2021}
}

@article{chatbridge,
 author = {Zhao, Zijia and Guo, Longteng and Yue, Tongtian and Chen, Sihan and Shao, Shuai and Zhu, Xinxin and Yuan, Zehuan and Liu, Jing},
 journal = {arXiv:2305.16103},
 title = {ChatBridge: Bridging Modalities with Large Language Model as a Language Catalyst},
 year = {2023}
}

@article{chatcaptioner,
 author = {Zhu, Deyao and Chen, Jun and Haydarov, Kilichbek and Shen, Xiaoqian and Zhang, Wenxuan and Elhoseiny, Mohamed},
 journal = {arXiv:2303.06594},
 title = {Chatgpt asks, blip-2 answers: Automatic questioning towards enriched visual descriptions},
 year = {2023}
}

@article{chatgpt,
 author = {OpenAI},
 title = {{Open{AI}: Introducing {ChatGPT}}},
 year = {2022}
}

@techreport{chatgpt,
 author = {OpenAI},
 title = {ChatGPT: A Language Model for Conversational AI},
 year = {2023}
}

@article{chen2015microsoft,
 author = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 journal = {arXiv preprint arXiv:1504.00325},
 title = {Microsoft coco captions: Data collection and evaluation server},
 year = {2015}
}

@inproceedings{chen2020uniter,
 author = {Chen, Yen-Chun and Li, Linjie and Yu, Licheng and El Kholy, Ahmed and Ahmed, Faisal and Gan, Zhe and Cheng, Yu and Liu, Jingjing},
 booktitle = {Proc. of ECCV},
 title = {Uniter: Universal image-text representation learning},
 year = {2020}
}

@inproceedings{chen2021localizing,
 author = {Chen, Honglie and Xie, Weidi and Afouras, Triantafyllos and Nagrani, Arsha and Vedaldi, Andrea and Zisserman, Andrew},
 booktitle = {CVPR},
 title = {Localizing visual sounds the hard way},
 year = {2021}
}

@article{chen2022adaptformer,
 author = {Chen, Shoufa and Ge, Chongjian and Tong, Zhan and Wang, Jiangliu and Song, Yibing and Wang, Jue and Luo, Ping},
 journal = {Proc. of NeurIPS},
 pages = {16664--16678},
 title = {{Adaptformer: Adapting vision transformers for scalable visual recognition}},
 year = {2022}
}

@article{Chen2022beats,
 archivePrefix = {arXiv},
 author = {Sanyuan Chen and Yu Wu and Chengyi Wang and Shujie Liu and Daniel Tompkins and Zhuo Chen and Furu Wei},
 eprint = {2212.09058},
 title = {BEATs: Audio Pre-Training with Acoustic Tokenizers},
 year = {2022}
}

@article{chen2022pali,
 author = {Chen, Xi and Wang, Xiao and Changpinyo, Soravit and Piergiovanni, AJ and Padlewski, Piotr and Salz, Daniel and Goodman, Sebastian and Grycner, Adam and Mustafa, Basil and Beyer, Lucas and others},
 journal = {arXiv preprint arXiv:2209.06794},
 title = {Pali: A jointly-scaled multilingual language-image model},
 year = {2022}
}

@article{chen2023dress,
 author = {Chen, Yangyi and Sikka, Karan and Cogswell, Michael and Ji, Heng and Divakaran, Ajay},
 journal = {arXiv preprint arXiv:2311.10081},
 title = {{Dress: Instructing large vision-language models to align and interact with humans via natural language feedback}},
 year = {2023}
}

@article{chen2023internvl,
 author = {Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Muyan, Zhong and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
 journal = {arXiv preprint arXiv:2312.14238},
 title = {{Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks}},
 year = {2023}
}

@article{chen2023lion,
 author = {Chen, Gongwei and Shen, Leyang and Shao, Rui and Deng, Xiang and Nie, Liqiang},
 journal = {arXiv preprint arXiv:2311.11860},
 title = {{LION: Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge}},
 year = {2023}
}

@article{chen2023ll3da,
 author = {Chen, Sijin and Chen, Xin and Zhang, Chi and Li, Mingsheng and Yu, Gang and Fei, Hao and Zhu, Hongyuan and Fan, Jiayuan and Chen, Tao},
 journal = {arXiv:2311.18651},
 title = {LL3DA: Visual Interactive Instruction Tuning for Omni-3D Understanding, Reasoning, and Planning},
 year = {2023}
}

@article{chen2023minigpt,
 author = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechun and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2310.09478},
 title = {{Minigpt-v2: large language model as a unified interface for vision-language multi-task learning}},
 year = {2023}
}

@article{chen2023minigptv2,
 author = {Chen, Jun and Zhu, Deyao and Shen, Xiaoqian and Li, Xiang and Liu, Zechu and Zhang, Pengchuan and Krishnamoorthi, Raghuraman and Chandra, Vikas and Xiong, Yunyang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2310.09478},
 title = {MiniGPT-v2: large language model as a unified interface for vision-language multi-task learning},
 year = {2023}
}

@article{chen2023mitigating,
 author = {Chen, Zhiyang and Zhu, Yousong and Zhan, Yufei and Li, Zhaowen and Zhao, Chaoyang and Wang, Jinqiao and Tang, Ming},
 journal = {arXiv preprint arXiv:2311.16479},
 title = {Mitigating hallucination in visual language models with visual supervision},
 year = {2023}
}

@article{chen2023pali,
 author = {Chen, Xi and Djolonga, Josip and Padlewski, Piotr and Mustafa, Basil and Changpinyo, Soravit and Wu, Jialin and Ruiz, Carlos Riquelme and Goodman, Sebastian and Wang, Xiao and Tay, Yi and others},
 journal = {arXiv preprint arXiv:2305.18565},
 title = {{PaLI-X: On Scaling up a Multilingual Vision and Language Model}},
 year = {2023}
}

@article{chen2023sharegpt4v,
 author = {Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
 journal = {arXiv preprint arXiv:2311.12793},
 title = {Sharegpt4v: Improving large multi-modal models with better captions},
 year = {2023}
}

@article{chen2023sharegpt4v,
 author = {Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
 journal = {arXiv preprint arXiv:2311.12793},
 title = {{ShareGPT4V: Improving Large Multi-Modal Models with Better Captions}},
 year = {2023}
}

@article{chen2023sharegpt4v,
 author = {Chen, Lin and Li, Jisong and Dong, Xiaoyi and Zhang, Pan and He, Conghui and Wang, Jiaqi and Zhao, Feng and Lin, Dahua},
 journal = {arXiv:2311.12793},
 title = {Sharegpt4v: Improving large multi-modal models with better captions},
 year = {2023}
}

@article{chen2023shikra,
 author = {Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
 journal = {arXiv preprint arXiv:2306.15195},
 title = {Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic},
 year = {2023}
}

@article{chen2023shikra,
 author = {Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
 journal = {arXiv preprint arXiv:2306.15195},
 title = {{Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic}},
 year = {2023}
}

@article{chen2023shikra,
 author = {Chen, Keqin and Zhang, Zhao and Zeng, Weili and Zhang, Richong and Zhu, Feng and Zhao, Rui},
 journal = {arXiv:2306.15195},
 title = {Shikra: Unleashing Multimodal LLM's Referential Dialogue Magic}
}

@article{chen2023vlp,
 author = {Chen, Fei-Long and Zhang, Du-Zhen and Han, Ming-Lun and Chen, Xiu-Yi and Shi, Jing and Xu, Shuang and Xu, Bo},
 journal = {Machine Intelligence Research},
 pages = {38--56},
 title = {{Vlp: A survey on vision-language pre-training}},
 year = {2023}
}

@article{chen2023x,
 author = {Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo},
 journal = {arXiv preprint arXiv:2305.04160},
 title = {{X-llm: Bootstrapping advanced large language models by treating multi-modalities as foreign languages}},
 year = {2023}
}

@article{chen2024allava,
 author = {Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
 journal = {arXiv preprint arXiv:2402.11684},
 title = {ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model},
 year = {2024}
}

@article{chen2024allava,
 author = {Chen, Guiming Hardy and Chen, Shunian and Zhang, Ruifei and Chen, Junying and Wu, Xiangbo and Zhang, Zhiyi and Chen, Zhihong and Li, Jianquan and Wan, Xiang and Wang, Benyou},
 journal = {arXiv:2402.11684},
 title = {ALLaVA: Harnessing GPT4V-synthesized Data for A Lite Vision-Language Model},
 year = {2024}
}

@article{chen2024far,
 author = {Chen, Zhe and Wang, Weiyun and Tian, Hao and Ye, Shenglong and Gao, Zhangwei and Cui, Erfei and Tong, Wenwen and Hu, Kongzhi and Luo, Jiapeng and Ma, Zheng and others},
 journal = {arXiv preprint arXiv:2404.16821},
 title = {How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites},
 year = {2024}
}

@article{chen2024we,
 author = {Chen, Lin and Li, Jinsong and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Chen, Zehui and Duan, Haodong and Wang, Jiaqi and Qiao, Yu and Lin, Dahua and others},
 journal = {arXiv preprint arXiv:2403.20330},
 title = {Are We on the Right Way for Evaluating Large Vision-Language Models?},
 year = {2024}
}

@article{cheng2021,
 author = {Cheng, Bowen and Schwing, Alex and Kirillov, Alexander},
 journal = {NIPS},
 pages = {17864--17875},
 title = {{Per-Pixel Classification is Not All You Need for Semantic Segmentation}},
 year = {2021}
}

@article{cheng2022,
 author = {Cheng, Wenqing and Wen, Ruxue and Huang, Haojun and Miao, Wang and Wang, Chen},
 journal = {Neurocomputing},
 pages = {201-211},
 title = {{OPTDP: Towards optimal personalized trajectory differential privacy for trajectory data publishing}},
 year = {2022}
}

@inproceedings{cherti2023reproducible,
 author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
 booktitle = {Proc. of CVPR},
 pages = {2818--2829},
 title = {{Reproducible scaling laws for contrastive language-image learning}},
 year = {2023}
}

@inproceedings{cherti2023reproducible,
 author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
 booktitle = {CVPR},
 title = {Reproducible scaling laws for contrastive language-image learning},
 year = {2023}
}

@inproceedings{cherti2023reproducible,
 author = {Cherti, Mehdi and Beaumont, Romain and Wightman, Ross and Wortsman, Mitchell and Ilharco, Gabriel and Gordon, Cade and Schuhmann, Christoph and Schmidt, Ludwig and Jitsev, Jenia},
 booktitle = {Proc. of CVPR},
 pages = {2818--2829},
 title = {Reproducible scaling laws for contrastive language-image learning},
 year = {2023}
}

@misc{chinese-llava,
 author = {LinkSoul-AI.},
 title = {{Chinese-LLaVA}},
 year = {2023}
}

@inproceedings{cho2021unifying,
 author = {Cho, Jaemin and Lei, Jie and Tan, Hao and Bansal, Mohit},
 booktitle = {ICML},
 title = {Unifying vision-and-language tasks via text generation},
 year = {2021}
}

@article{chowdhery2023palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {Journal of Machine Learning Research},
 pages = {1--113},
 title = {Palm: Scaling language modeling with pathways},
 year = {2023}
}

@article{chowdhery2023palm,
 author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
 journal = {Journal of Machine Learning Research},
 pages = {1--113},
 title = {{Palm: Scaling language modeling with pathways}},
 year = {2023}
}

@article{chu2023mobilevlm,
 author = {Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
 journal = {arXiv preprint arXiv:2312.16886},
 title = {MobileVLM: A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices},
 year = {2023}
}

@article{chu2023mobilevlm,
 author = {Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
 journal = {arXiv preprint arXiv:2312.16886},
 title = {MobileVLM: A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices},
 year = {2023}
}

@article{chu2023mobilevlm,
 author = {Chu, Xiangxiang and Qiao, Limeng and Lin, Xinyang and Xu, Shuang and Yang, Yang and Hu, Yiming and Wei, Fei and Zhang, Xinyu and Zhang, Bo and Wei, Xiaolin and others},
 journal = {arXiv:2312.16886},
 title = {MobileVLM: A Fast, Reproducible and Strong Vision Language Assistant for Mobile Devices},
 year = {2023}
}

@article{chu2023qwen,
 author = {Chu, Yunfei and Xu, Jin and Zhou, Xiaohuan and Yang, Qian and Zhang, Shiliang and Yan, Zhijie and Zhou, Chang and Zhou, Jingren},
 journal = {arXiv preprint arXiv:2311.07919},
 title = {{Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models}},
 year = {2023}
}

@article{chu2024mobilevlm,
 author = {Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
 journal = {arXiv preprint arXiv:2402.03766},
 title = {MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
 year = {2024}
}

@article{chu2024mobilevlm,
 author = {Chu, Xiangxiang and Qiao, Limeng and Zhang, Xinyu and Xu, Shuang and Wei, Fei and Yang, Yang and Sun, Xiaofei and Hu, Yiming and Lin, Xinyang and Zhang, Bo and others},
 journal = {arXiv:2402.03766},
 title = {MobileVLM V2: Faster and Stronger Baseline for Vision Language Model},
 year = {2024}
}

@article{chung2022scaling,
 author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
 journal = {arXiv preprint arXiv:2210.11416},
 title = {{Scaling instruction-finetuned language models}},
 year = {2022}
}

@article{chung2022scaling,
 author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
 journal = {arXiv:2210.11416},
 title = {Scaling instruction-finetuned language models},
 year = {2022}
}

@inproceedings{clark2023,
 author = {Clark, Brandon and Kerrigan, Alec and Kulkarni, Parth Parag and Cepeda, Vicente Vivanco and Shah, Mubarak},
 booktitle = {CVPR},
 pages = {23182-23190},
 title = {{Where We Are and What We're Looking At: Query Based Worldwide Image Geo-Localization Using Hierarchies and Scenes}},
 year = {2023}
}

@inproceedings{clark2023we,
 author = {Clark, Brandon and Kerrigan, Alec and Kulkarni, Parth Parag and Cepeda, Vicente Vivanco and Shah, Mubarak},
 booktitle = {Proc. of CVPR},
 pages = {23182--23190},
 title = {Where we are and what we're looking at: Query based worldwide image geo-localization using hierarchies and scenes},
 year = {2023}
}

@inproceedings{clark:pct,
 author = {Malcolm Clark},
 booktitle = {TeX90 Conference Proceedings},
 pages = {84-89},
 title = {Post Congress Tristesse},
 year = {1991}
}

@phdthesis{Clarkson:1985:ACP:911891,
 author = {Clarkson, Kenneth Lee},
 title = {Algorithms for Closest-Point Problems (Computational Geometry)},
 year = {1985}
}

@inproceedings{clip,
 author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
 booktitle = {ICML},
 title = {Learning transferable visual models from natural language supervision},
 year = {2021}
}

@inproceedings{COCO,
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 booktitle = {Proc. of ECCV},
 title = {Microsoft coco: Common objects in context},
 year = {2014}
}

@article{cot-prompt-tuning,
 author = {Ge, Jiaxin and Luo, Hongyin and Qian, Siyuan and Gan, Yulu and Fu, Jie and Zhan, Shanghang},
 journal = {arXiv:2304.07919},
 title = {Chain of Thought Prompt Tuning in Vision Language Models},
 year = {2023}
}

@article{coyo,
 author = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
 title = {Coyo-700m: Image-text pair dataset},
 year = {2022}
}

@misc{CROSSBOW,
 key = {CROSSBOW},
 note = {http://www.xbow.com},
 title = {{XBOW} Sensor Motes Specifications},
 year = {2008}
}

@inproceedings{cui2024survey,
 author = {Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and others},
 booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
 pages = {958--979},
 title = {{A survey on multimodal large language models for autonomous driving}},
 year = {2024}
}

@article{Culler-01,
 author = {D. Culler and D. Estrin and M. Srivastava},
 journal = {IEEE Comput.},
 pages = {41--49},
 title = {Overview of Sensor Networks},
 year = {2004}
}

@proceedings{Czerwinski:2008:1358628,
 author = {},
 title = {CHI '08: CHI '08 extended abstracts on Human factors in computing systems},
 year = {2008}
}

@inproceedings{dai2023can,
 author = {Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
 booktitle = {Proc. of ACL Findings},
 pages = {4005-4019},
 title = {{Why Can GPT Learn In-Context? Language Models Implicitly Perform Gradient Descent as Meta-Optimizers}},
 year = {2023}
}

@article{dai2024instructblip,
 author = {Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale N and Hoi, Steven},
 journal = {Proc. of NeurIPS},
 title = {Instructblip: Towards general-purpose vision-language models with instruction tuning},
 year = {2024}
}

@article{dasgupta2022language,
 author = {Dasgupta, Ishita and Lampinen, Andrew K and Chan, Stephanie CY and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
 journal = {arXiv preprint arXiv:2207.07051},
 title = {Language models show human-like content effects on reasoning},
 year = {2022}
}

@misc{DataOptim,
 title = {DataOptim},
 url = {https://github.com/BAAI-DCAI/DataOptim},
 year = {2023}
}

@inproceedings{DBLP:conf/emnlp/ZhangLB23,
 author = {Hang Zhang and
Xin Li and
Lidong Bing},
 booktitle = {Proc. of EMNLP},
 pages = {543--553},
 title = {{Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for
Video Understanding}},
 year = {2023}
}

@inproceedings{DBLP:conf/emnlp/ZhangLZZWZQ23,
 author = {Dong Zhang and
Shimin Li and
Xin Zhang and
Jun Zhan and
Pengyu Wang and
Yaqian Zhou and
Xipeng Qiu},
 booktitle = {Proc. of EMNLP Findings},
 pages = {15757--15773},
 title = {{SpeechGPT: Empowering Large Language Models with Intrinsic Cross-Modal
Conversational Abilities}},
 year = {2023}
}

@inproceedings{DBLP:conf/icml/0008LSH23,
 author = {Junnan Li and
Dongxu Li and
Silvio Savarese and
Steven C. H. Hoi},
 booktitle = {Proc. of ICML},
 pages = {19730--19742},
 title = {{{BLIP-2:} Bootstrapping Language-Image Pre-training with Frozen Image
Encoders and Large Language Models}},
 year = {2023}
}

@inproceedings{DBLP:conf/icml/ChenW00T0CYW23,
 author = {Sanyuan Chen and
Yu Wu and
Chengyi Wang and
Shujie Liu and
Daniel Tompkins and
Zhuo Chen and
Wanxiang Che and
Xiangzhan Yu and
Furu Wei},
 booktitle = {Proc. of ICML},
 pages = {5178--5193},
 title = {{BEATs: Audio Pre-Training with Acoustic Tokenizers}},
 year = {2023}
}

@inproceedings{DBLP:conf/icml/RadfordKXBMS23,
 author = {Alec Radford and
Jong Wook Kim and
Tao Xu and
Greg Brockman and
Christine McLeavey and
Ilya Sutskever},
 booktitle = {Proc. of ICML},
 pages = {28492--28518},
 title = {{Robust Speech Recognition via Large-Scale Weak Supervision}},
 year = {2023}
}

@inproceedings{DBLP:conf/ijcai/DuLLZ22,
 author = {Yifan Du and
Zikang Liu and
Junyi Li and
Wayne Xin Zhao},
 booktitle = {Proc. of IJCAI},
 pages = {5436--5443},
 title = {{A Survey of Vision-Language Pre-Trained Models}},
 year = {2022}
}

@inproceedings{DBLP:conf/ijcai/LongCHY22,
 author = {Siqu Long and
Feiqi Cao and
Soyeon Caren Han and
Haiqin Yang},
 booktitle = {Proc. of IJCAI},
 pages = {5530--5537},
 title = {{Vision-and-Language Pretrained Models: {A} Survey}},
 year = {2022}
}

@inproceedings{DBLP:conf/ijcai/ZhangZJW022,
 author = {Duzhen Zhang and
Tielin Zhang and
Shuncheng Jia and
Qingyu Wang and
Bo Xu},
 booktitle = {Proc. of IJCAI},
 pages = {5670--5677},
 title = {{Recent Advances and New Frontiers in Spiking Neural Networks}},
 year = {2022}
}

@inproceedings{DBLP:conf/nips/0008BL022,
 author = {Min Zhao and
Fan Bao and
Chongxuan Li and
Jun Zhu},
 booktitle = {Proc. of NeurIPS},
 title = {{{EGSDE:} Unpaired Image-to-Image Translation via Energy-Guided Stochastic
Differential Equations}},
 year = {2022}
}

@inproceedings{DBLP:journals/corr/abs-2305-06500,
 author = {Wenliang Dai and
Junnan Li and
Dongxu Li and
Anthony Meng Huat Tiong and
Junqi Zhao and
Weisheng Wang and
Boyang Li and
Pascale Fung and
Steven C. H. Hoi},
 booktitle = {Proc. of NeurIPS},
 title = {{InstructBLIP: Towards General-purpose Vision-Language Models with
Instruction Tuning}},
 year = {2023}
}

@article{deshmukh2024pengi,
 author = {Deshmukh, Soham and Elizalde, Benjamin and Singh, Rita and Wang, Huaming},
 journal = {NeurIPS},
 title = {Pengi: An audio language model for audio tasks},
 year = {2024}
}

@article{detgpt,
 author = {Pi, Renjie and Gao, Jiahui and Diao, Shizhe and Pan, Rui and Dong, Hanze and Zhang, Jipeng and Yao, Lewei and Han, Jianhua and Xu, Hang and Zhang, Lingpeng Kong Tong},
 journal = {arXiv:2305.14167},
 title = {DetGPT: Detect What You Need via Reasoning},
 year = {2023}
}

@article{dettmers2023qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {arXiv preprint arXiv:2305.14314},
 title = {{Qlora: Efficient finetuning of quantized llms}},
 year = {2023}
}

@article{dettmers2023qlora,
 author = {Dettmers, Tim and Pagnoni, Artidoro and Holtzman, Ari and Zettlemoyer, Luke},
 journal = {arXiv:2305.14314},
 title = {Qlora: Efficient finetuning of quantized llms},
 year = {2023}
}

@incollection{Dijkstra:1979:GSC:1241515.1241518,
 author = {Dijkstra, E.},
 booktitle = {Classics in software engineering (incoll)},
 pages = {27--33},
 title = {Go to statement considered harmful},
 year = {1979}
}

@article{dino,
 author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel M and Shum, Heung-Yeung},
 journal = {arXiv:2203.03605},
 title = {Dino: Detr with improved denoising anchor boxes for end-to-end object detection},
 year = {2022}
}

@article{dinov2,
 author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
 journal = {arXiv:2304.07193},
 title = {Dinov2: Learning robust visual features without supervision},
 year = {2023}
}

@inproceedings{dong2020cif,
 author = {Dong, Linhao and Xu, Bo},
 booktitle = {Proc. of ICASSP},
 pages = {6079--6083},
 title = {{Cif: Continuous integrate-and-fire for end-to-end speech recognition}},
 year = {2020}
}

@article{dong2022survey,
 author = {Dong, Qingxiu and Li, Lei and Dai, Damai and Zheng, Ce and Wu, Zhiyong and Chang, Baobao and Sun, Xu and Xu, Jingjing and Sui, Zhifang},
 journal = {arXiv:2301.00234},
 title = {A Survey for In-context Learning},
 year = {2022}
}

@inproceedings{dong2023dreamllm,
 author = {Dong, Runpei and Han, Chunrui and Peng, Yuang and Qi, Zekun and Ge, Zheng and Yang, Jinrong and Zhao, Liang and Sun, Jianjian and Zhou, Hongyu and Wei, Haoran and others},
 booktitle = {Proc. of ICLR},
 title = {Dreamllm: Synergistic multimodal comprehension and creation},
 year = {2024}
}

@inproceedings{dong2023federated,
 author = {Dong, Jiahua and Zhang, Duzhen and Cong, Yang and Cong, Wei and Ding, Henghui and Dai, Dengxin},
 booktitle = {Proc. of CVPR},
 pages = {3934--3943},
 title = {{Federated Incremental Semantic Segmentation}},
 year = {2023}
}

@inproceedings{dong2023heterogeneous,
 author = {Dong, Jiahua and Liang, Wenqi and Cong, Yang and Sun, Gan},
 booktitle = {Proc. of ICCV},
 pages = {11742--11751},
 title = {Heterogeneous forgetting compensation for class-incremental learning},
 year = {2023}
}

@inproceedings{dosovitskiy2020image,
 author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
 booktitle = {ICLR},
 title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
 year = {2021}
}

@inproceedings{dosovitskiy2020image,
 author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
 booktitle = {Proc. of ICLR},
 title = {{An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale}},
 year = {2020}
}

@article{driess2023palm,
 author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
 journal = {arXiv preprint arXiv:2303.03378},
 title = {{Palm-e: An embodied multimodal language model}},
 year = {2023}
}

@inproceedings{driess2023palme,
 author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi S. M. and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and Huang, Wenlong and Chebotar, Yevgen and Sermanet, Pierre and Duckworth, Daniel and Levine, Sergey and Vanhoucke, Vincent and Hausman, Karol and Toussaint, Marc and Greff, Klaus and Zeng, Andy and Mordatch, Igor and Florence, Pete},
 booktitle = {arXiv preprint arXiv:2303.03378},
 title = {PaLM-E: An Embodied Multimodal Language Model},
 year = {2023}
}

@inproceedings{drossos2020clotho,
 author = {Drossos, Konstantinos and Lipping, Samuel and Virtanen, Tuomas},
 booktitle = {Proc. of ICASSP},
 title = {Clotho: An audio captioning dataset},
 year = {2020}
}

@article{du2018aishell,
 author = {Du, Jiayu and Na, Xingyu and Liu, Xuechen and Bu, Hui},
 journal = {arXiv preprint arXiv:1808.10583},
 title = {Aishell-2: Transforming mandarin asr research into industrial scale},
 year = {2018}
}

@inproceedings{du2022glm,
 author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
 booktitle = {Proc. of ACL},
 pages = {320--335},
 title = {GLM: General Language Model Pretraining with Autoregressive Blank Infilling},
 year = {2022}
}

@inproceedings{du2022glm,
 author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
 booktitle = {Proc. of ACL},
 pages = {320--335},
 title = {{GLM: General Language Model Pretraining with Autoregressive Blank Infilling}},
 year = {2022}
}

@article{du2023makes,
 author = {Du, Yifan and Guo, Hangyu and Zhou, Kun and Zhao, Wayne Xin and Wang, Jinpeng and Wang, Chuyuan and Cai, Mingchen and Song, Ruihua and Wen, Ji-Rong},
 journal = {arXiv:2311.01487},
 title = {What makes for good visual instructions? synthesizing complex visual reasoning instructions for visual instruction tuning},
 year = {2023}
}

@inproceedings{elizalde2023clap,
 author = {Elizalde, Benjamin and Deshmukh, Soham and Al Ismail, Mahmoud and Wang, Huaming},
 booktitle = {Proc. of ICASSP},
 title = {Clap learning audio concepts from natural language supervision},
 year = {2023}
}

@article{embodiedgpt,
 author = {Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
 journal = {arXiv:2305.15021},
 title = {EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought},
 year = {2023}
}

@inproceedings{emu,
 author = {Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
 booktitle = {ICLR},
 title = {Generative pretraining in multimodality},
 year = {2024}
}

@article{fang2021clip2video,
 author = {Fang, Han and Xiong, Pengfei and Xu, Luhui and Chen, Yu},
 journal = {arXiv preprint arXiv:2106.11097},
 title = {{Clip2video: Mastering video-text retrieval via image clip}},
 year = {2021}
}

@inproceedings{fang2023eva,
 author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
 booktitle = {Proc. of CVPR},
 pages = {19358--19369},
 title = {{Eva: Exploring the limits of masked visual representation learning at scale}},
 year = {2023}
}

@inproceedings{fang2023eva,
 author = {Fang, Yuxin and Wang, Wen and Xie, Binhui and Sun, Quan and Wu, Ledell and Wang, Xinggang and Huang, Tiejun and Wang, Xinlong and Cao, Yue},
 booktitle = {CVPR},
 title = {Eva: Exploring the limits of masked visual representation learning at scale},
 year = {2023}
}

@inproceedings{fang2024transformer,
 author = {Fang, Zhiyu and Lei, Shuai-Long and Zhu, Xiaobin and Yang, Chun and Zhang, Shi-Xue and Yin, Xu-Cheng and Qin, Jingyan},
 booktitle = {Proc. of SIGIR},
 pages = {70--79},
 title = {Transformer-based reasoning for learning evolutionary chain of events on temporal knowledge graph},
 year = {2024}
}

@manual{Fear05,
 author = {Simon Fear},
 title = {Publication quality tables in {\LaTeX}},
 year = {2005}
}

@article{fedus2022switch,
 author = {Fedus, William and Zoph, Barret and Shazeer, Noam},
 journal = {JMLR},
 title = {Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
 year = {2022}
}

@article{feng2023docpedia,
 author = {Feng, Hao and Liu, Qi and Liu, Hao and Zhou, Wengang and Li, Houqiang and Huang, Can},
 journal = {arXiv preprint arXiv:2311.11810},
 title = {{DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding}},
 year = {2023}
}

@inproceedings{few-shot-vqa,
 author = {Yang, Zhengyuan and Gan, Zhe and Wang, Jianfeng and Hu, Xiaowei and Lu, Yumao and Liu, Zicheng and Wang, Lijuan},
 booktitle = {Proc. of AAAI},
 title = {An empirical study of gpt-3 for few-shot knowledge-based vqa},
 year = {2022}
}

@article{firoozi2023foundation,
 author = {Firoozi, Roya and Tucker, Johnathan and Tian, Stephen and Majumdar, Anirudha and Sun, Jiankai and Liu, Weiyu and Zhu, Yuke and Song, Shuran and Kapoor, Ashish and Hausman, Karol and others},
 journal = {arXiv preprint arXiv:2312.07843},
 title = {{Foundation Models in Robotics: Applications, Challenges, and the Future}},
 year = {2023}
}

@article{fisher2016pizzagate,
 author = {Fisher, Marc and Cox, John Woodrow and Hermann, Peter},
 journal = {Washington Post},
 pages = {8410--8415},
 title = {Pizzagate: From rumor, to hashtag, to gunfire in DC},
 year = {2016}
}

@inproceedings{flickr30k,
 author = {Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
 booktitle = {ICCV},
 title = {Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
 year = {2015}
}

@article{frozen,
 author = {Tsimpoukelli, Maria and Menick, Jacob L and Cabi, Serkan and Eslami, SM and Vinyals, Oriol and Hill, Felix},
 journal = {NeurIPS},
 title = {Multimodal few-shot learning with frozen language models},
 year = {2021}
}

@inproceedings{fu2022adapterbias,
 author = {Fu, Chin-Lun and Chen, Zih-Ching and Lee, Yun-Ru and Lee, Hung-Yi},
 booktitle = {Proc. of ACL Findings},
 pages = {2608--2621},
 title = {{AdapterBias: Parameter-efficient Token-dependent Representation Shift for Adapters in NLP Tasks}},
 year = {2022}
}

@article{fu2023challenger,
 author = {Fu, Chaoyou and Zhang, Renrui and Lin, Haojia and Wang, Zihan and Gao, Timin and Luo, Yongdong and Huang, Yubo and Zhang, Zhengye and Qiu, Longtian and Ye, Gaoxiang and others},
 journal = {arXiv:2312.12436},
 title = {A challenger to gpt-4v? early explorations of gemini in visual expertise}
}

@article{fu2023gptscore,
 author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
 journal = {arXiv preprint arXiv:2302.04166},
 title = {Gptscore: Evaluate as you desire},
 year = {2023}
}

@article{fu2023mme,
 author = {Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and others},
 journal = {arXiv preprint arXiv:2306.13394},
 title = {Mme: A comprehensive evaluation benchmark for multimodal large language models},
 year = {2023}
}

@misc{fuyu-8b,
 author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
 title = {{Introducing our Multimodal Models}},
 year = {2023}
}

@misc{fuyu-8b,
 author = {Bavishi, Rohan and Elsen, Erich and Hawthorne, Curtis and Nye, Maxwell and Odena, Augustus and Somani, Arushi and  Ta\c{s}\i{}rlar, Sa\u{g}nak},
 title = {Introducing our Multimodal Models},
 year = {2023}
}

@article{gadre2023datacomp,
 author = {Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
 journal = {arXiv preprint arXiv:2304.14108},
 title = {DataComp: In search of the next generation of multimodal datasets},
 year = {2023}
}

@article{gallegos2023bias,
 author = {Gallegos, Isabel O and Rossi, Ryan A and Barrow, Joe and Tanjim, Md Mehrab and Kim, Sungchul and Dernoncourt, Franck and Yu, Tong and Zhang, Ruiyi and Ahmed, Nesreen K},
 journal = {arXiv preprint arXiv:2309.00770},
 title = {{Bias and fairness in large language models: A survey}},
 year = {2023}
}

@inproceedings{gan2017stylenet,
 author = {Gan, Chuang and Gan, Zhe and He, Xiaodong and Gao, Jianfeng and Deng, Li},
 booktitle = {CVPR},
 title = {Stylenet: Generating attractive visual captions with styles},
 year = {2017}
}

@inproceedings{gao2019dynamic,
 author = {Gao, Peng and Jiang, Zhengkai and You, Haoxuan and Lu, Pan and Hoi, Steven CH and Wang, Xiaogang and Li, Hongsheng},
 booktitle = {CVPR},
 title = {Dynamic fusion with intra-and inter-modality attention flow for visual question answering},
 year = {2019}
}

@article{gao2022pal,
 author = {Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
 journal = {arXiv:2211.10435},
 title = {PAL: Program-aided Language Models},
 year = {2022}
}

@article{gao2024sphinx,
 author = {Gao, Peng and Zhang, Renrui and Liu, Chris and Qiu, Longtian and Huang, Siyuan and Lin, Weifeng and Zhao, Shitian and Geng, Shijie and Lin, Ziyi and Jin, Peng and others},
 journal = {arXiv preprint arXiv:2402.05935},
 title = {SPHINX-X: Scaling Data and Parameters for a Family of Multi-modal Large Language Models},
 year = {2024}
}

@article{Ge2023Chain,
 author = {Jiaxin Ge and Hongyin Luo and Siyuan Qian and Yulu Gan and Jie Fu and Shanghang Zhang},
 title = {Chain of Thought Prompt Tuning in Vision Language Models},
 year = {2023}
}

@article{ge2023planting,
 author = {Ge, Yuying and Ge, Yixiao and Zeng, Ziyun and Wang, Xintao and Shan, Ying},
 journal = {arXiv preprint arXiv:2307.08041},
 title = {{Planting a seed of vision in large language model}},
 year = {2023}
}

@phdthesis{gerndt:89,
 author = {Michael Gerndt},
 title = {Automatic Parallelization for Distributed-Memory
Multiprocessing Systems},
 year = {1989}
}

@inproceedings{girdhar2023imagebind,
 author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
 booktitle = {Proc. of CVPR},
 pages = {15180--15190},
 title = {{Imagebind: One embedding space to bind them all}},
 year = {2023}
}

@inproceedings{girdhar2023imagebind,
 author = {Girdhar, Rohit and El-Nouby, Alaaeldin and Liu, Zhuang and Singh, Mannat and Alwala, Kalyan Vasudev and Joulin, Armand and Misra, Ishan},
 booktitle = {CVPR},
 title = {Imagebind: One embedding space to bind them all},
 year = {2023}
}

@article{gls:hypertrees,
 author = {Georg Gottlob and Nicola Leone and Francesco Scarcello},
 journal = {Journal of Computer and System Sciences},
 pages = {579--627},
 title = {Hypertree Decompositions and Tractable Queries},
 year = {2002}
}

@article{gong2023mindagent,
 author = {Gong, Ran and Huang, Qiuyuan and Ma, Xiaojian and Vo, Hoi and Durante, Zane and Noda, Yusuke and Zheng, Zilong and Zhu, Song-Chun and Terzopoulos, Demetri and Fei-Fei, Li and others},
 journal = {arXiv:2309.09971},
 title = {Mindagent: Emergent gaming interaction},
 year = {2023}
}

@article{gong2023multimodal,
 author = {Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
 journal = {arXiv preprint arXiv:2305.04790},
 title = {{Multimodal-gpt: A vision and language model for dialogue with humans}},
 year = {2023}
}

@article{goodfellow2013empirical,
 author = {Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
 journal = {arXiv preprint arXiv:1312.6211},
 title = {{An empirical investigation of catastrophic forgetting in gradient-based neural networks}},
 year = {2013}
}

@article{goodfellow2013empirical,
 author = {Goodfellow, Ian J and Mirza, Mehdi and Xiao, Da and Courville, Aaron and Bengio, Yoshua},
 journal = {arXiv:1312.6211},
 title = {An empirical investigation of catastrophic forgetting in gradient-based neural networks},
 year = {2013}
}

@book{Goossens:1999:LWC:553897,
 address = {Boston, MA, USA},
 author = {Goossens, Michel and Rahtz, S. P. and Moore, Ross and Sutor, Robert S.},
 publisher = {Addison-Wesley Longman Publishing Co., Inc.},
 title = {The  Latex Web Companion: Integrating TEX, HTML, and XML},
 year = {1999}
}

@article{gottlob:nonmon,
 author = {Georg Gottlob},
 journal = {Journal of Logic and Computation},
 pages = {397--425},
 title = {Complexity results for nonmonotonic logics},
 year = {1992}
}

@inproceedings{goyal2017making,
 author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
 booktitle = {Proc. of CVPR},
 pages = {6904--6913},
 title = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
 year = {2017}
}

@inproceedings{goyal2017making,
 author = {Goyal, Yash and Khot, Tejas and Summers-Stay, Douglas and Batra, Dhruv and Parikh, Devi},
 booktitle = {Proc. of CVPR},
 pages = {6904--6913},
 title = {Making the v in vqa matter: Elevating the role of image understanding in visual question answering},
 year = {2017}
}

@article{gpt-4o,
 author = {OpenAI},
 title = {GPT-4o System Card},
 year = {2024}
}

@article{gpt4tools,
 author = {Yang, Rui and Song, Lin and Li, Yanwei and Zhao, Sijie and Ge, Yixiao and Li, Xiu and Shan, Ying},
 journal = {arXiv:2305.18752},
 title = {GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction},
 year = {2023}
}

@article{gpt4v,
 author = {Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
 journal = {arXiv preprint arXiv:2303.08774},
 title = {{GPT-4 Technical Report}},
 year = {2023}
}

@misc{gpt4v,
 author = {OpenAI},
 title = {GPT-4V(ision) System Card},
 year = {2024}
}

@inproceedings{GQA2019,
 author = {Hudson, Drew A and Manning, Christopher D},
 booktitle = {Proc. of CVPR},
 pages = {6700--6709},
 title = {Gqa: A new dataset for real-world visual reasoning and compositional question answering},
 year = {2019}
}

@article{gu2022wukong,
 author = {Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Minzhe, Niu and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and others},
 journal = {Proc. of NeurIPS},
 pages = {26418--26431},
 title = {Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark},
 year = {2022}
}

@article{guan2023hallusionbench,
 author = {Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
 journal = {arXiv preprint arXiv:2310.14566},
 title = {Hallusionbench: An advanced diagnostic suite for entangled language hallucination \& visual illusion in large vision-language models},
 year = {2023}
}

@article{guan2023hallusionbench,
 author = {Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
 journal = {arXiv preprint arXiv:2310.14566},
 title = {HallusionBench: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models},
 year = {2023}
}

@article{gunasekar2023textbooks,
 author = {Gunasekar, Suriya and Zhang, Yi and Aneja, Jyoti and Mendes, Caio C{\'e}sar Teodoro and Del Giorno, Allie and Gopi, Sivakanth and Javaheripi, Mojan and Kauffmann, Piero and de Rosa, Gustavo and Saarikivi, Olli and others},
 journal = {arXiv preprint arXiv:2306.11644},
 title = {Textbooks Are All You Need},
 year = {2023}
}

@inproceedings{guo2023images,
 author = {Guo, Jiaxian and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Li, Boyang and Tao, Dacheng and Hoi, Steven},
 booktitle = {CVPR},
 title = {From images to textual prompts: Zero-shot visual question answering with frozen large language models},
 year = {2023}
}

@inproceedings{guo2023pitl,
 author = {Guo, Zixin and Wang, Tzu-Jui Julius and Pehlivan, Selen and Radman, Abduljalil and Laaksonen, Jorma},
 booktitle = {Proc. of SIGIR},
 pages = {2261--2265},
 title = {PiTL: Cross-modal Retrieval with Weakly-supervised Vision-language Pre-training via Prompting},
 year = {2023}
}

@inproceedings{gupta2019lvis,
 author = {Gupta, Agrim and Dollar, Piotr and Girshick, Ross},
 booktitle = {Proc. of CVPR},
 pages = {5356--5364},
 title = {Lvis: A dataset for large vocabulary instance segmentation},
 year = {2019}
}

@inproceedings{gurari2018vizwiz,
 author = {Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
 booktitle = {Proc. of CVPR},
 pages = {3608--3617},
 title = {Vizwiz grand challenge: Answering visual questions from blind people},
 year = {2018}
}

@article{haas2023learning,
 author = {Haas, Lukas and Alberti, Silas and Skreta, Michal},
 journal = {arXiv preprint arXiv:2302.00275},
 title = {Learning generalized zero-shot learners for open-domain image geolocalization},
 year = {2023}
}

@article{haas2023learning,
 author = {Haas, Lukas and Alberti, Silas and Skreta, Michal},
 journal = {arXiv preprint arXiv:2302.00275},
 title = {{Learning Generalized Zero-Shot Learners for Open-Domain Image Geolocalization}},
 year = {2023}
}

@article{haas2023learning,
 author = {Haas, Lukas and Alberti, Silas and Skreta, Michal},
 journal = {arXiv preprint arXiv:2302.00275},
 title = {Learning generalized zero-shot learners for open-domain image geolocalization},
 year = {2023}
}

@inproceedings{haas2024pigeon,
 author = {Haas, Lukas and Skreta, Michal and Alberti, Silas and Finn, Chelsea},
 booktitle = {Proc. of CVPR},
 pages = {12893--12902},
 title = {Pigeon: Predicting image geolocations},
 year = {2024}
}

@inproceedings{Hagerup1993,
 author = {Hagerup, Torben and Mehlhorn, Kurt and Munro, J. Ian},
 booktitle = {Proceedings of the 20th International Colloquium on Automata, Languages and Programming},
 pages = {253--264},
 title = {Maintaining Discrete Probability Distributions Optimally},
 year = {1993}
}

@inproceedings{hallusionbench,
 author = {Liu, Fuxiao and Guan, Tianrui and Li, Zongxia and Chen, Lichang and Yacoob, Yaser and Manocha, Dinesh and Zhou, Tianyi},
 booktitle = {CVPR},
 title = {Hallusionbench: You see what you think? or you think what you see? an image-context reasoning benchmark challenging for gpt-4v (ision), llava-1.5, and other multi-modality models},
 year = {2024}
}

@inproceedings{han2022improving,
 author = {Han, Minglun and Dong, Linhao and Liang, Zhenlin and Cai, Meng and Zhou, Shiyu and Ma, Zejun and Xu, Bo},
 booktitle = {Proc. of ICASSP},
 pages = {8532--8536},
 title = {{Improving end-to-end contextual speech recognition with fine-grained contextual knowledge selection}},
 year = {2022}
}

@article{han2023imagebind,
 author = {Han, Jiaming and Zhang, Renrui and Shao, Wenqi and Gao, Peng and Xu, Peng and Xiao, Han and Zhang, Kaipeng and Liu, Chris and Wen, Song and Guo, Ziyu and others},
 journal = {arXiv:2309.03905},
 title = {Imagebind-llm: Multi-modality instruction tuning},
 year = {2023}
}

@article{han2023knowledge,
 author = {Han, Minglun and Chen, Feilong and Shi, Jing and Xu, Shuang and Xu, Bo},
 journal = {arXiv preprint arXiv:2301.13003},
 title = {{Knowledge Transfer from Pre-trained Language Models to Cif-based Speech Recognizers via Hierarchical Distillation}},
 year = {2023}
}

@inproceedings{han2023mitigating,
 author = {Han, Xiao and Zhao, Xiangyu and Zhang, Liang and Wang, Wanyu},
 booktitle = {Proc. of KDD},
 pages = {673--684},
 title = {Mitigating action hysteresis in traffic signal control with traffic predictive reinforcement learning},
 year = {2023}
}

@article{han2024instinctive,
 author = {Han, Tianyang and Lian, Qing and Pan, Rui and Pi, Renjie and Zhang, Jipeng and Diao, Shizhe and Lin, Yong and Zhang, Tong},
 journal = {arXiv preprint arXiv:2402.03757},
 title = {The Instinctive Bias: Spurious Images lead to Hallucination in MLLMs},
 year = {2024}
}

@article{han2024skip,
 author = {Han, Zongbo and Bai, Zechen and Mei, Haiyang and Xu, Qianli and Zhang, Changqing and Shou, Mike Zheng},
 journal = {arXiv preprint arXiv:2402.01345},
 title = {Skip $\textbackslash n $: A simple method to reduce hallucination in Large Vision-Language Models},
 year = {2024}
}

@misc{Harvard-01,
 key = {Harvard CodeBlue},
 note = {http://www.eecs.harvard.edu/mdw/ proj/codeblue/},
 title = {{CodeBlue}: Sensor Networks for Medical Care},
 year = {2008}
}

@inproceedings{hays2008im2gps,
 author = {Hays, James and Efros, Alexei A},
 booktitle = {CVPR},
 pages = {1--8},
 title = {{IM2GPS: estimating geographic information from a single image}},
 year = {2008}
}

@inproceedings{he2016deep,
 author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
 booktitle = {Proc. of CVPR},
 pages = {770--778},
 title = {{Deep residual learning for image recognition}},
 year = {2016}
}

@article{he2020pathvqa,
 author = {He, Xuehai and Zhang, Yichen and Mou, Luntian and Xing, Eric and Xie, Pengtao},
 journal = {arXiv:2003.10286},
 title = {Pathvqa: 30000+ questions for medical visual question answering},
 year = {2020}
}

@inproceedings{he2021towards,
 author = {He, Junxian and Zhou, Chunting and Ma, Xuezhe and Berg-Kirkpatrick, Taylor and Neubig, Graham},
 booktitle = {Proc. of ICLR},
 title = {{Towards a Unified View of Parameter-Efficient Transfer Learning}},
 year = {2021}
}

@inproceedings{he2022masked,
 author = {He, Kaiming and Chen, Xinlei and Xie, Saining and Li, Yanghao and Doll{\'a}r, Piotr and Girshick, Ross},
 booktitle = {Proc. of CVPR},
 pages = {16000--16009},
 title = {{Masked autoencoders are scalable vision learners}},
 year = {2022}
}

@article{he2023continual,
 author = {He, Jinghan and Guo, Haiyun and Tang, Ming and Wang, Jinqiao},
 journal = {arXiv preprint arXiv:2311.16206},
 title = {{Continual instruction tuning for large multimodal models}},
 year = {2023}
}

@article{he2024efficient,
 author = {He, Muyang and Liu, Yexin and Wu, Boya and Yuan, Jianhao and Wang, Yueze and Huang, Tiejun and Zhao, Bo},
 journal = {arXiv preprint arXiv:2402.11530},
 title = {Efficient Multimodal Learning from Data-centric Perspective},
 year = {2024}
}

@article{herlihy:methodology,
 author = {Herlihy, Maurice},
 journal = {ACM Trans. Program. Lang. Syst.},
 pages = {745-770},
 title = {A Methodology for Implementing Highly Concurrent Data Objects},
 year = {1993}
}

@article{ho2020denoising,
 author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
 journal = {NeurIPS},
 title = {Denoising diffusion probabilistic models},
 year = {2020}
}

@incollection{Hoare:1972:CIN:1243380.1243382,
 author = {Hoare, C. A. R.},
 booktitle = {Structured programming (incoll)},
 pages = {83--174},
 title = {Chapter II: Notes on data structuring},
 year = {1972}
}

@article{hoffmann2022training,
 author = {Hoffmann, Jordan and Borgeaud, Sebastian and Mensch, Arthur and Buchatskaya, Elena and Cai, Trevor and Rutherford, Eliza and Casas, Diego de Las and Hendricks, Lisa Anne and Welbl, Johannes and Clark, Aidan and others},
 journal = {arXiv preprint arXiv:2203.15556},
 title = {{Training compute-optimal large language models}},
 year = {2022}
}

@article{hong20233d,
 author = {Hong, Yining and Zhen, Haoyu and Chen, Peihao and Zheng, Shuhong and Du, Yilun and Chen, Zhenfang and Gan, Chuang},
 journal = {NeurIPS},
 title = {3d-llm: Injecting the 3d world into large language models},
 year = {2023}
}

@article{hong2023cogagent,
 author = {Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
 journal = {arXiv preprint arXiv:2312.08914},
 title = {{Cogagent: A visual language model for gui agents}},
 year = {2023}
}

@article{hong2023cogagent,
 author = {Hong, Wenyi and Wang, Weihan and Lv, Qingsong and Xu, Jiazheng and Yu, Wenmeng and Ji, Junhui and Wang, Yan and Wang, Zihan and Dong, Yuxiao and Ding, Ming and others},
 journal = {arXiv:2312.08914},
 title = {Cogagent: A visual language model for gui agents},
 year = {2023}
}

@article{honovich2022instruction,
 author = {Honovich, Or and Shaham, Uri and Bowman, Samuel R and Levy, Omer},
 journal = {arXiv:2205.10782},
 title = {Instruction induction: From few examples to natural language task descriptions},
 year = {2022}
}

@article{honovich2022unnatural,
 author = {Honovich, Or and Scialom, Thomas and Levy, Omer and Schick, Timo},
 journal = {arXiv preprint arXiv:2212.09689},
 title = {{Unnatural instructions: Tuning language models with (almost) no human labor}},
 year = {2022}
}

@inproceedings{houlsby2019parameter,
 author = {Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrone, Bruna and De Laroussilhe, Quentin and Gesmundo, Andrea and Attariyan, Mona and Gelly, Sylvain},
 booktitle = {Proc. of ICML},
 pages = {2790--2799},
 title = {{Parameter-efficient transfer learning for NLP}},
 year = {2019}
}

@article{hsu2021hubert,
 author = {Hsu, Wei-Ning and Bolte, Benjamin and Tsai, Yao-Hung Hubert and Lakhotia, Kushal and Salakhutdinov, Ruslan and Mohamed, Abdelrahman},
 journal = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
 pages = {3451--3460},
 title = {{Hubert: Self-supervised speech representation learning by masked prediction of hidden units}},
 year = {2021}
}

@misc{hsu2021hubertselfsupervisedspeechrepresentation,
 archivePrefix = {arXiv},
 author = {Wei-Ning Hsu and Benjamin Bolte and Yao-Hung Hubert Tsai and Kushal Lakhotia and Ruslan Salakhutdinov and Abdelrahman Mohamed},
 eprint = {2106.07447},
 primaryClass = {cs.CL},
 title = {HuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units},
 url = {https://arxiv.org/abs/2106.07447},
 year = {2021}
}

@inproceedings{hu2021lora,
 author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
 booktitle = {ICLR},
 title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
 year = {2022}
}

@inproceedings{hu2021lora,
 author = {Edward J Hu and yelong shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Lu Wang and Weizhu Chen},
 booktitle = {Proc. of ICLR},
 title = {Lo{RA}: Low-Rank Adaptation of Large Language Models},
 year = {2022}
}

@inproceedings{hu2021lora,
 author = {Hu, Edward J and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
 booktitle = {Proc. of ICLR},
 title = {{LoRA: Low-Rank Adaptation of Large Language Models}},
 year = {2021}
}

@article{hu2021lora,
 author = {Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
 journal = {arXiv:2106.09685},
 title = {Lora: Low-rank adaptation of large language models},
 year = {2021}
}

@article{hu2023bliva,
 journal = {arXiv preprint arXiv:2308.09936},
 title = {{Bliva: A simple multimodal llm for better handling of text-rich visual questions}},
 year = {2023}
}

@article{hu2023large,
 author = {Hu, Jinyi and Yao, Yuan and Wang, Chongyi and Wang, Shan and Pan, Yinxu and Chen, Qianyu and Yu, Tianyu and Wu, Hanghao and Zhao, Yue and Zhang, Haoye and others},
 journal = {arXiv preprint arXiv:2308.12038},
 title = {{Large multilingual models pivot zero-shot multimodal learning across languages}},
 year = {2023}
}

@article{hu2023large,
 author = {Hu, Jinyi and Yao, Yuan and Wang, Chongyi and Wang, Shan and Pan, Yinxu and Chen, Qianyu and Yu, Tianyu and Wu, Hanghao and Zhao, Yue and Zhang, Haoye and others},
 journal = {arXiv:2308.12038},
 title = {Large multilingual models pivot zero-shot multimodal learning across languages},
 year = {2023}
}

@article{hu2023mplug,
 author = {Hu, Anwen and Shi, Yaya and Xu, Haiyang and Ye, Jiabo and Ye, Qinghao and Yan, Ming and Li, Chenliang and Qian, Qi and Zhang, Ji and Huang, Fei},
 journal = {arXiv preprint arXiv:2311.18248},
 title = {{mPLUG-PaperOwl: Scientific Diagram Analysis with the Multimodal Large Language Model}},
 year = {2023}
}

@article{hu2023mplug,
 author = {Hu, Anwen and Shi, Yaya and Xu, Haiyang and Ye, Jiabo and Ye, Qinghao and Yan, Ming and Li, Chenliang and Qian, Qi and Zhang, Ji and Huang, Fei},
 journal = {arXiv:2311.18248},
 title = {mplug-paperowl: Scientific diagram analysis with the multimodal large language model},
 year = {2023}
}

@article{hu2024mplug,
 author = {Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others},
 journal = {arXiv:2403.12895},
 title = {mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding},
 year = {2024}
}

@article{huang2023audiogpt,
 author = {Huang, Rongjie and Li, Mingze and Yang, Dongchao and Shi, Jiatong and Chang, Xuankai and Ye, Zhenhui and Wu, Yuning and Hong, Zhiqing and Huang, Jiawei and Liu, Jinglin and others},
 journal = {arXiv preprint arXiv:2304.12995},
 title = {{Audiogpt: Understanding and generating speech, music, sound, and talking head}},
 year = {2023}
}

@article{huang2023embodied,
 author = {Huang, Jiangyong and Yong, Silong and Ma, Xiaojian and Linghu, Xiongkun and Li, Puhao and Wang, Yan and Li, Qing and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
 journal = {arXiv:2311.12871},
 title = {An embodied generalist agent in 3d world},
 year = {2023}
}

@article{huang2023language,
 author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
 journal = {arXiv preprint arXiv:2302.14045},
 title = {{Language is not all you need: Aligning perception with language models}},
 year = {2023}
}

@article{huang2023visual,
 author = {Huang, Jiaxing and Zhang, Jingyi and Jiang, Kai and Qiu, Han and Lu, Shijian},
 journal = {arXiv preprint arXiv:2312.16602},
 title = {{Visual Instruction Tuning towards General-Purpose Multimodal Model: A Survey}},
 year = {2023}
}

@article{huang2024visual,
 author = {Huang, Wen and Liu, Hongbin and Guo, Minxin and Gong, Neil Zhenqiang},
 journal = {arXiv preprint arXiv:2402.14683},
 title = {Visual Hallucinations of Multi-modal Large Language Models},
 year = {2024}
}

@inproceedings{hudson2019gqa,
 author = {Hudson, Drew A and Manning, Christopher D},
 booktitle = {Proc. of CVPR},
 pages = {6700--6709},
 title = {Gqa: A new dataset for real-world visual reasoning and compositional question answering},
 year = {2019}
}

@article{HuggingGPT,
 author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
 journal = {arXiv:2303.17580},
 title = {Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface},
 year = {2023}
}

@article{idealgpt,
 author = {You, Haoxuan and Sun, Rui and Wang, Zhecan and Chen, Long and Wang, Gengyu and Ayyubi, Hammad A and Chang, Kai-Wei and Chang, Shih-Fu},
 journal = {arXiv:2305.14985},
 title = {IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models},
 year = {2023}
}

@misc{idefics,
 author = {IDEFICS},
 title = {Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model},
 year = {2023}
}

@misc{IDEFICS,
 author = {IDEFICS},
 title = {{Introducing IDEFICS: An Open Reproduction of State-of-the-Art Visual Language Model}},
 year = {2023}
}

@inproceedings{Im2GPS++YFCC4k+Im2GPS3k,
 author = {Vo, Nam and Jacobs, Nathan and Hays, James},
 booktitle = {ICCV},
 title = {Revisiting {IMG2GPS} in the deep learning era},
 year = {2017}
}

@misc{imp2024,
 author = {Shao, Zhenwei and Ouyang, Xuecheng and Yu, Zhou and Yu, Jun},
 title = {Imp: An Emprical Study of Multimodal Small Language Models},
 year = {2024}
}

@inproceedings{instructblip,
 author = {Wenliang Dai and Junnan Li and Dongxu Li and Anthony Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
 booktitle = {Proc. of NeurIPS},
 title = {Instruct{BLIP}: Towards General-purpose Vision-Language Models with Instruction Tuning},
 year = {2023}
}

@article{instructblip,
 author = {Dai, Wenliang and Li, Junnan and Li, Dongxu and Tiong, Anthony Meng Huat and Zhao, Junqi and Wang, Weisheng and Li, Boyang and Fung, Pascale and Hoi, Steven},
 journal = {arXiv:2305.06500},
 title = {Instructblip: Towards general-purpose vision-language models with instruction tuning},
 year = {2023}
}

@article{iyer2022opt,
 author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, Daniel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
 journal = {arXiv preprint arXiv:2212.12017},
 title = {{Opt-iml: Scaling language model instruction meta learning through the lens of generalization}},
 year = {2022}
}

@article{iyer2022opt,
 author = {Iyer, Srinivasan and Lin, Xi Victoria and Pasunuru, Ramakanth and Mihaylov, Todor and Simig, D{\'a}niel and Yu, Ping and Shuster, Kurt and Wang, Tianlu and Liu, Qing and Koura, Punit Singh and others},
 journal = {arXiv:2212.12017},
 title = {OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization},
 year = {2022}
}

@article{jain2023vcoder,
 author = {Jain, Jitesh and Yang, Jianwei and Shi, Humphrey},
 journal = {arXiv preprint arXiv:2312.14233},
 title = {{Vcoder: Versatile vision encoders for multimodal large language models}},
 year = {2023}
}

@article{jeong2023hijacking,
 author = {Jeong, Joonhyun},
 journal = {arXiv:2312.07553},
 title = {Hijacking Context in Large Multi-modal Models},
 year = {2023}
}

@inproceedings{jia2021scaling,
 author = {Jia, Chao and Yang, Yinfei and Xia, Ye and Chen, Yi-Ting and Parekh, Zarana and Pham, Hieu and Le, Quoc and Sung, Yun-Hsuan and Li, Zhen and Duerig, Tom},
 booktitle = {Proc. of ICML},
 pages = {4904--4916},
 title = {Scaling up visual and vision-language representation learning with noisy text supervision},
 year = {2021}
}

@inproceedings{jian2023bootstrapping,
 author = {Jian, Yiren and Gao, Chongyang and Vosoughi, Soroush},
 booktitle = {Proc. of NeurIPS},
 title = {{Bootstrapping Vision-Language Learning with Decoupled Language Pre-training}},
 year = {2023}
}

@article{jian2024bootstrapping,
 author = {Jian, Yiren and Gao, Chongyang and Vosoughi, Soroush},
 journal = {Proc. of NeurIPS},
 title = {Bootstrapping vision-language learning with decoupled language pre-training},
 year = {2024}
}

@article{jiang2023hallucination,
 author = {Jiang, Chaoya and Xu, Haiyang and Dong, Mengfan and Chen, Jiaxing and Ye, Wei and Yan, Ming and Ye, Qinghao and Zhang, Ji and Huang, Fei and Zhang, Shikun},
 journal = {arXiv:2312.06968},
 title = {Hallucination Augmented Contrastive Learning for Multimodal Large Language Model},
 year = {2023}
}

@article{jiang2024hal,
 author = {Jiang, Chaoya and Ye, Wei and Dong, Mengfan and Jia, Hongrui and Xu, Haiyang and Yan, Ming and Zhang, Ji and Zhang, Shikun},
 journal = {arXiv preprint arXiv:2402.15721},
 title = {Hal-Eval: A Universal and Fine-grained Hallucination Evaluation Framework for Large Vision Language Models},
 year = {2024}
}

@article{jiang2024mixtral,
 author = {Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Hanna, Emma Bou and Bressand, Florian and others},
 journal = {arXiv:2401.04088},
 title = {Mixtral of experts},
 year = {2024}
}

@inproceedings{jin2023unified,
 author = {Jin, Yang and Xu, Kun and Chen, Liwei and Liao, Chao and Tan, Jianchao and Chen, Bin and Lei, Chenyi and Liu, An and Song, Chengru and Lei, Xiaoqiang and others},
 booktitle = {Proc. of ICLR},
 title = {{Unified language-vision pretraining with dynamic discrete visual tokenization}},
 year = {2024}
}

@article{jin2024efficient,
 author = {Jin, Yizhang and Li, Jian and Liu, Yexin and Gu, Tianjun and Wu, Kai and Jiang, Zhengkai and He, Muyang and Zhao, Bo and Tan, Xin and Gan, Zhenye and others},
 journal = {arXiv preprint arXiv:2405.10739},
 title = {Efficient Multimodal Large Language Models: A Survey},
 year = {2024}
}

@article{jing2023faithscore,
 author = {Jing, Liqiang and Li, Ruosen and Chen, Yunmo and Jia, Mengzhao and Du, Xinya},
 journal = {arXiv:2311.01477},
 title = {FAITHSCORE: Evaluating Hallucinations in Large Vision-Language Models},
 year = {2023}
}

@incollection{KA:2001,
 author = {Kong, Wei-Chang},
 booktitle = {E-commerce and cultural values},
 pages = {51--74},
 title = {The implementation of electronic commerce in SMEs in Singapore (as Incoll)},
 year = {2001}
}

@inproceedings{kafle2018dvqa,
 author = {Kafle, Kushal and Price, Brian and Cohen, Scott and Kanan, Christopher},
 booktitle = {Proc. of CVPR},
 pages = {5648--5656},
 title = {Dvqa: Understanding data visualizations via question answering},
 year = {2018}
}

@inbook{KAGM:2001,
 author = {Kong, Wei-Chang},
 pages = {51--74},
 title = {E-commerce and cultural values},
 year = {2001}
}

@misc{kakaobrain2022coyo-700m,
 author = {Byeon, Minwoo and Park, Beomhee and Kim, Haecheon and Lee, Sungjun and Baek, Woonhyuk and Kim, Saehoon},
 title = {COYO-700M: Image-Text Pair Dataset},
 year = {2022}
}

@article{karamcheti2024prismatic,
 author = {Karamcheti, Siddharth and Nair, Suraj and Balakrishna, Ashwin and Liang, Percy and Kollar, Thomas and Sadigh, Dorsa},
 journal = {arXiv preprint arXiv:2402.07865},
 title = {Prismatic vlms: Investigating the design space of visually-conditioned language models},
 year = {2024}
}

@article{karimi2021compacter,
 author = {Karimi Mahabadi, Rabeeh and Henderson, James and Ruder, Sebastian},
 journal = {Proc. of NeurIPS},
 pages = {1022--1035},
 title = {{Compacter: Efficient low-rank hypercomplex adapter layers}},
 year = {2021}
}

@inproceedings{karpathy2015deep,
 author = {Karpathy, Andrej and Fei-Fei, Li},
 booktitle = {CVPR},
 title = {Deep visual-semantic alignments for generating image descriptions},
 year = {2015}
}

@article{kaul2024throne,
 author = {Kaul, Prannay and Li, Zhizhong and Yang, Hao and Dukler, Yonatan and Swaminathan, Ashwin and Taylor, CJ and Soatto, Stefano},
 journal = {arXiv preprint arXiv:2405.05256},
 title = {THRONE: An Object-based Hallucination Benchmark for the Free-form Generations of Large Vision-Language Models},
 year = {2024}
}

@inproceedings{kazemzadeh2014referitgame,
 author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
 booktitle = {Proc. of EMNLP},
 pages = {787--798},
 title = {Referitgame: Referring to objects in photographs of natural scenes},
 year = {2014}
}

@inproceedings{kazemzadeh2014referitgame,
 author = {Kazemzadeh, Sahar and Ordonez, Vicente and Matten, Mark and Berg, Tamara},
 booktitle = {Proc. of EMNLP},
 pages = {787--798},
 title = {Referitgame: Referring to objects in photographs of natural scenes},
 year = {2014}
}

@inproceedings{kenton2019,
 author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
 booktitle = {Proc. of NAACL},
 pages = {41714186},
 title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
 year = {2019}
}

@inproceedings{kenton2019bert,
 author = {Kenton, Jacob Devlin Ming-Wei Chang and Toutanova, Lee Kristina},
 booktitle = {Proc. of AACL},
 pages = {4171--4186},
 title = {{BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}},
 year = {2019}
}

@article{khan2024debunking,
 author = {Khan, Sohail Ahmed and Dierickx, Laurence and Furuly, Jan-Gunnar and Vold, Henrik Brattli and Tahseen, Rano and Linden, Carl-Gustav and Dang-Nguyen, Duc-Tien},
 journal = {Journal of the Association for Information Science and Technology},
 title = {Debunking war information disorder: A case study in assessing the use of multimedia verification tools},
 year = {2024}
}

@article{kiela2020hateful,
 author = {Kiela, Douwe and Firooz, Hamed and Mohan, Aravind and Goswami, Vedanuj and Singh, Amanpreet and Ringshia, Pratik and Testuggine, Davide},
 journal = {Proc. of NeurIPS},
 pages = {2611--2624},
 title = {The hateful memes challenge: Detecting hate speech in multimodal memes},
 year = {2020}
}

@article{kingma2013auto,
 author = {Kingma, Diederik P and Welling, Max},
 journal = {arXiv preprint arXiv:1312.6114},
 title = {{Auto-encoding variational bayes}},
 year = {2013}
}

@article{kingma2013auto,
 author = {Kingma, Diederik P and Welling, Max},
 journal = {arXiv preprint arXiv:1312.6114},
 title = {{Auto-encoding variational bayes}},
 year = {2013}
}

@article{kirillov2023segment,
 author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
 journal = {arXiv preprint arXiv:2304.02643},
 title = {{Segment anything}},
 year = {2023}
}

@article{Kirschmer:2010:AEI:1958016.1958018,
 author = {Kirschmer, Markus and Voight, John},
 journal = {SIAM J. Comput.},
 pages = {1714--1747},
 title = {Algorithmic Enumeration of Ideal Classes for Quaternion Orders},
 year = {2010}
}

@book{knuth:texbook,
 address = {Reading, MA.},
 author = {Donald E. Knuth},
 publisher = {Addison-Wesley},
 title = {The {\TeX{}book}},
 year = {1984}
}

@mastersthesis{ko94,
 author = {Jacob Kornerup},
 title = {Mapping Powerlists onto Hypercubes},
 year = {1994}
}

@inproceedings{koh2023generating,
 author = {Koh, Jing Yu and Fried, Daniel and Salakhutdinov, Ruslan},
 booktitle = {Proc. of NeurIPS},
 title = {{Generating images with multimodal language models}},
 year = {2023}
}

@inproceedings{koh2023grounding,
 author = {Koh, Jing Yu and Salakhutdinov, Ruslan and Fried, Daniel},
 booktitle = {Proc. of ICML},
 pages = {17283--17300},
 title = {{Grounding language models to images for multimodal inputs and outputs}},
 year = {2023}
}

@article{kojima2022large,
 author = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
 journal = {Proc. of NeurIPS},
 pages = {22199--22213},
 title = {Large language models are zero-shot reasoners},
 year = {2022}
}

@article{kong2020hifi,
 author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
 journal = {Proc. of NeurIPS},
 pages = {17022--17033},
 title = {{Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis}},
 year = {2020}
}

@incollection{Kong:2002:IEC:887006.887010,
 author = {Kong, Wei-Chang},
 booktitle = {E-commerce and cultural values (Incoll-w-text (chap 9) 'title')},
 pages = {51--74},
 title = {Chapter 9},
 year = {2002}
}

@incollection{Kong:2003:IEC:887006.887011,
 author = {Kong, Wei-Chang},
 booktitle = {E-commerce and cultural values},
 pages = {51--74},
 title = {The implementation of electronic commerce in SMEs in Singapore (Incoll)},
 year = {2003}
}

@inbook{Kong:2004:IEC:123456.887010,
 author = {Kong, Wei-Chang},
 pages = {51--74},
 title = {E-commerce and cultural values - (InBook-num-in-chap)},
 year = {2004}
}

@inbook{Kong:2005:IEC:887006.887010,
 author = {Kong, Wei-Chang},
 pages = {51--74},
 title = {E-commerce and cultural values (Inbook-text-in-chap)},
 year = {2005}
}

@inbook{Kong:2006:IEC:887006.887010,
 author = {Kong, Wei-Chang},
 pages = {51--74},
 title = {E-commerce and cultural values (Inbook-num chap)},
 year = {2006}
}

@inproceedings{kordopatis2021leveraging,
 author = {Kordopatis-Zilos, Giorgos and Galopoulos, Panagiotis and Papadopoulos, Symeon and Kompatsiaris, Ioannis},
 booktitle = {Proceedings of the 2021 International Conference on Multimedia Retrieval},
 pages = {155--163},
 title = {Leveraging efficientnet and contrastive learning for accurate global-scale location estimation},
 year = {2021}
}

@article{kosmos-1,
 author = {Huang, Shaohan and Dong, Li and Wang, Wenhui and Hao, Yaru and Singhal, Saksham and Ma, Shuming and Lv, Tengchao and Cui, Lei and Mohammed, Owais Khan and Liu, Qiang and others},
 journal = {arXiv:2302.14045},
 title = {Language is not all you need: Aligning perception with language models},
 year = {2023}
}

@article{krishna2017visual,
 author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
 journal = {International journal of computer vision},
 pages = {32--73},
 title = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
 year = {2017}
}

@article{krishna2017visual,
 author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
 journal = {International journal of computer vision},
 pages = {32--73},
 title = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
 year = {2017}
}

@article{lai2023lisa,
 author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
 journal = {arXiv preprint arXiv:2308.00692},
 title = {{Lisa: Reasoning segmentation via large language model}},
 year = {2023}
}

@article{lai2023lisa,
 author = {Lai, Xin and Tian, Zhuotao and Chen, Yukang and Li, Yanwei and Yuan, Yuhui and Liu, Shu and Jia, Jiaya},
 journal = {arXiv:2308.00692},
 title = {Lisa: Reasoning segmentation via large language model},
 year = {2023}
}

@article{LAION-400M,
 author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
 journal = {arXiv:2111.02114},
 title = {Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
 year = {2021}
}

@article{laion-5b,
 author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
 journal = {NeurIPS},
 title = {Laion-5b: An open large-scale dataset for training next generation image-text models},
 year = {2022}
}

@article{laion-coco,
 author = {Schuhmann, Christoph and Kpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
 journal = {https://laion.ai/blog/laion-coco/},
 title = {Laion coco: 600m synthetic captions from laion2b-en.},
 year = {2022}
}

@misc{LAION-GPT4V,
 howpublished = {\url{https://huggingface.co/datasets/laion/gpt4v-dataset}},
 title = {LAION-GPT4V}
}

@misc{laion2023gpt4v,
 author = {LAION},
 title = {Gpt-4v dataset},
 year = {2023}
}

@article{laioncoco,
 author = {Schuhmann, Christoph and Kpf, Andreas and Vencu, Richard and Coombes, Theo and Beaumont, Romain},
 title = {Laion coco: 600m synthetic captions from laion2b-en},
 year = {2022b}
}

@article{lamm,
 author = {Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},
 journal = {arXiv:2306.06687},
 title = {LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
 year = {2023}
}

@book{Lamport:LaTeX,
 address = {Reading, MA.},
 author = {Leslie Lamport},
 publisher = {Addison-Wesley},
 title = {\it {\LaTeX}: A Document Preparation System},
 year = {1986}
}

@article{larson2017benchmarking,
 author = {Larson, Martha and Soleymani, Mohammad and Gravier, Guillaume and Ionescu, Bogdan and Jones, Gareth JF},
 journal = {IEEE MultiMedia},
 pages = {93--96},
 title = {The benchmarking initiative for multimedia evaluation: MediaEval 2016},
 year = {2017}
}

@article{lau2018dataset,
 author = {Lau, Jason J and Gayen, Soumya and Ben Abacha, Asma and Demner-Fushman, Dina},
 journal = {Sci. Data},
 title = {A dataset of clinically generated visual questions and answers about radiology images},
 year = {2018}
}

@inproceedings{laurenccon2023obelics,
 author = {Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, Leo and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander M and Kiela, Douwe and others},
 booktitle = {Proc. of NeurIPS},
 title = {{OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents}},
 year = {2023}
}

@misc{laurencon2023obelics,
 archivePrefix = {arXiv},
 author = {Hugo Laurenon and Lucile Saulnier and Lo Tronchon and Stas Bekman and Amanpreet Singh and Anton Lozhkov and Thomas Wang and Siddharth Karamcheti and Alexander M. Rush and Douwe Kiela and Matthieu Cord and Victor Sanh},
 eprint = {2306.16527},
 primaryClass = {cs.IR},
 title = {OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents},
 year = {2023}
}

@misc{laurenon2024matters,
 archivePrefix = {arXiv},
 author = {Hugo Laurenon and Lo Tronchon and Matthieu Cord and Victor Sanh},
 eprint = {2405.02246},
 primaryClass = {cs.CV},
 title = {What matters when building vision-language models?},
 year = {2024}
}

@article{lavin,
 author = {Luo, Gen and Zhou, Yiyi and Ren, Tianhe and Chen, Shengxin and Sun, Xiaoshuai and Ji, Rongrong},
 journal = {arXiv:2305.15023},
 title = {Cheap and Quick: Efficient Vision-Language Instruction Tuning for Large Language Models},
 year = {2023}
}

@article{lee2023volcano,
 author = {Lee, Seongyun and Park, Sue Hyun and Jo, Yongrae and Seo, Minjoon},
 journal = {arXiv preprint arXiv:2311.07362},
 title = {Volcano: mitigating multimodal hallucination through self-feedback guided revision},
 year = {2023}
}

@incollection{Lee:1978:TQA:800025.1198348,
 author = {Lee, Jan},
 booktitle = {History of programming languages I (incoll)},
 pages = {68--71},
 title = {Transcript of question and answer session},
 year = {1981}
}

@inproceedings{leng2023mitigating,
 author = {Leng, Sicong and Zhang, Hang and Chen, Guanzheng and Li, Xin and Lu, Shijian and Miao, Chunyan and Bing, Lidong},
 booktitle = {CVPR},
 title = {Mitigating object hallucinations in large vision-language models through visual contrastive decoding},
 year = {2024}
}

@inproceedings{lester2021power,
 author = {Lester, Brian and Al-Rfou, Rami and Constant, Noah},
 booktitle = {Proc. of EMNLP},
 pages = {3045--3059},
 title = {{The Power of Scale for Parameter-Efficient Prompt Tuning}},
 year = {2021}
}

@inproceedings{levesque:belief,
 author = {Hector~J. Levesque},
 booktitle = {Proceedings of the Fourth National Conference on Artificial Intelligence},
 pages = {198--202},
 title = {A logic of implicit and explicit belief},
 year = {1984}
}

@article{levesque:functional-foundations,
 author = {Hector~J. Levesque},
 journal = {Artificial Intelligence},
 pages = {155--212},
 title = {Foundations of a functional approach to knowledge representation},
 year = {1984}
}

@inproceedings{li-etal-2023-evaluating,
 author = {Li, Yifan  and
Du, Yifan  and
Zhou, Kun  and
Wang, Jinpeng  and
Zhao, Xin  and
Wen, Ji-Rong},
 booktitle = {Proc. of EMNLP},
 pages = {292--305},
 title = {Evaluating Object Hallucination in Large Vision-Language Models},
 year = {2023}
}

@inproceedings{li2020oscar,
 author = {Li, Xiujun and Yin, Xi and Li, Chunyuan and Zhang, Pengchuan and Hu, Xiaowei and Zhang, Lei and Wang, Lijuan and Hu, Houdong and Dong, Li and Wei, Furu and others},
 booktitle = {Proc. of ECCV},
 pages = {121--137},
 title = {{Oscar: Object-semantics aligned pre-training for vision-language tasks}},
 year = {2020}
}
@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

@inproceedings{guan2024hallusionbench,
  title={Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion in large vision-language models},
  author={Guan, Tianrui and Liu, Fuxiao and Wu, Xiyang and Xian, Ruiqi and Li, Zongxia and Liu, Xiaoyu and Wang, Xijun and Chen, Lichang and Huang, Furong and Yacoob, Yaser and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14375--14385},
  year={2024}
}

@article{li2021align,
 author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
 journal = {Proc. of NeurIPS},
 pages = {9694--9705},
 title = {{Align before fuse: Vision and language representation learning with momentum distillation}},
 year = {2021}
}

@article{li2021align,
 author = {Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
 journal = {NeurIPS},
 title = {Align before fuse: Vision and language representation learning with momentum distillation},
 year = {2021}
}

@article{li2021caption,
 author = {Li, Yaochen and Wu, Chuan and Li, Ling and Liu, Yuehu and Zhu, Jihua},
 journal = {IEEE Transactions on Intelligent Transportation Systems},
 pages = {7805-7816},
 title = {{Caption Generation From Road Images for Traffic Scene Modeling}},
 year = {2021}
}

@inproceedings{li2021prefix,
 author = {Li, Xiang Lisa and Liang, Percy},
 booktitle = {Proc. of ACL},
 pages = {4582--4597},
 title = {{Prefix-Tuning: Optimizing Continuous Prompts for Generation}},
 year = {2021}
}

@inproceedings{li2022blip,
 author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
 booktitle = {Proc. of ICML},
 pages = {12888--12900},
 title = {{Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation}},
 year = {2022}
}

@inproceedings{li2022blip,
 author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
 booktitle = {ICML},
 title = {Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation},
 year = {2022}
}

@inproceedings{li2023blip,
 author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
 booktitle = {Proc. of ICML},
 pages = {19730--19742},
 title = {{BLIP}-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models},
 year = {2023}
}

@article{li2023blip,
 author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
 journal = {arXiv:2301.12597},
 title = {Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
 year = {2023}
}

@inproceedings{li2023blip,
 author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
 booktitle = {Proc. of ICML},
 pages = {19730--19742},
 title = {Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
 year = {2023}
}

@article{li2023evaluating,
 author = {Li, Yifan and Du, Yifan and Zhou, Kun and Wang, Jinpeng and Zhao, Wayne Xin and Wen, Ji-Rong},
 journal = {arXiv preprint arXiv:2305.10355},
 title = {Evaluating object hallucination in large vision-language models},
 year = {2023}
}

@article{li2023llama,
 author = {Li, Yanwei and Wang, Chengyao and Jia, Jiaya},
 journal = {arXiv preprint arXiv:2311.17043},
 title = {{LLaMA-VID: An Image is Worth 2 Tokens in Large Language Models}},
 year = {2023}
}

@article{li2023llava,
 author = {Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
 journal = {arXiv preprint arXiv:2306.00890},
 title = {{Llava-med: Training a large language-and-vision assistant for biomedicine in one day}},
 year = {2023}
}

@article{li2023llava,
 author = {Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
 journal = {arXiv:2306.00890},
 title = {Llava-med: Training a large language-and-vision assistant for biomedicine in one day},
 year = {2023}
}

@article{li2023m,
 author = {Li, Lei and Yin, Yuwei and Li, Shicheng and Chen, Liang and Wang, Peiyi and Ren, Shuhuai and Li, Mukai and Yang, Yazheng and Xu, Jingjing and Sun, Xu and others},
 journal = {arXiv preprint arXiv:2306.04387},
 title = {{M$^{3}$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning}},
 year = {2023}
}

@article{li2023mimic,
 author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
 journal = {arXiv preprint arXiv:2306.05425},
 title = {Mimic-it: Multi-modal in-context instruction tuning},
 year = {2023}
}

@article{li2023mimic,
 author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
 journal = {arXiv preprint arXiv:2306.05425},
 title = {Mimic-it: Multi-modal in-context instruction tuning},
 year = {2023}
}

@article{li2023mimic,
 author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Pu, Fanyi and Yang, Jingkang and Li, Chunyuan and Liu, Ziwei},
 journal = {arXiv:2306.05425},
 title = {Mimic-it: Multi-modal in-context instruction tuning},
 year = {2023}
}

@article{li2023monkey,
 author = {Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
 journal = {arXiv preprint arXiv:2311.06607},
 title = {{Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models}},
 year = {2023}
}

@article{li2023monkey,
 author = {Li, Zhang and Yang, Biao and Liu, Qiang and Ma, Zhiyin and Zhang, Shuo and Yang, Jingxu and Sun, Yabo and Liu, Yuliang and Bai, Xiang},
 journal = {arXiv:2311.06607},
 title = {Monkey: Image resolution and text label are important things for large multi-modal models},
 year = {2023}
}

@article{li2023otter,
 author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
 journal = {arXiv preprint arXiv:2305.03726},
 title = {{Otter: A multi-modal model with in-context instruction tuning}},
 year = {2023}
}

@article{li2023seed,
 author = {Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
 journal = {CVPR},
 title = {Seed-bench: Benchmarking multimodal llms with generative comprehension},
 year = {2024}
}

@article{li2023seed,
 author = {Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
 journal = {arXiv preprint arXiv:2307.16125},
 title = {Seed-bench: Benchmarking multimodal llms with generative comprehension},
 year = {2023}
}

@article{li2023silkie,
 author = {Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
 journal = {arXiv preprint arXiv:2312.10665},
 title = {{Silkie: Preference Distillation for Large Visual Language Models}},
 year = {2023}
}

@article{li2023silkie,
 author = {Li, Lei and Xie, Zhihui and Li, Mukai and Chen, Shunian and Wang, Peiyi and Chen, Liang and Yang, Yazheng and Wang, Benyou and Kong, Lingpeng},
 journal = {arXiv:2312.10665},
 title = {Silkie: Preference Distillation for Large Visual Language Models},
 year = {2023}
}

@article{li2023stablellava,
 author = {Li, Yanda and Zhang, Chi and Yu, Gang and Wang, Zhibin and Fu, Bin and Lin, Guosheng and Shen, Chunhua and Chen, Ling and Wei, Yunchao},
 journal = {arXiv preprint arXiv:2308.10253},
 title = {{Stablellava: Enhanced visual instruction tuning with synthesized image-dialogue data}},
 year = {2023}
}

@article{li2023textbooks,
 author = {Li, Yuanzhi and Bubeck, S{\'e}bastien and Eldan, Ronen and Del Giorno, Allie and Gunasekar, Suriya and Lee, Yin Tat},
 journal = {arXiv preprint arXiv:2309.05463},
 title = {Textbooks are all you need ii: phi-1.5 technical report},
 year = {2023}
}

@article{li2023videochat,
 author = {Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
 journal = {arXiv preprint arXiv:2305.06355},
 title = {{Videochat: Chat-centric video understanding}},
 year = {2023}
}

@article{li2024appagent,
 author = {Li, Yanda and Zhang, Chi and Yang, Wanqi and Fu, Bin and Cheng, Pei and Chen, Xin and Chen, Ling and Wei, Yunchao},
 journal = {arXiv preprint arXiv:2408.11824},
 title = {Appagent v2: Advanced agent for flexible mobile interactions},
 year = {2024}
}

@article{li2024mini,
 author = {Li, Yanwei and Zhang, Yuechen and Wang, Chengyao and Zhong, Zhisheng and Chen, Yixin and Chu, Ruihang and Liu, Shaoteng and Jia, Jiaya},
 journal = {arXiv preprint arXiv:2403.18814},
 title = {Mini-gemini: Mining the potential of multi-modality vision language models},
 year = {2024}
}

@misc{li2024monkey,
 archivePrefix = {arXiv},
 author = {Zhang Li and Biao Yang and Qiang Liu and Zhiyin Ma and Shuo Zhang and Jingxu Yang and Yabo Sun and Yuliang Liu and Xiang Bai},
 eprint = {2311.06607},
 primaryClass = {cs.CV},
 title = {Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models},
 year = {2024}
}

@article{li2024red,
 author = {Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi},
 journal = {arXiv preprint arXiv:2401.12915},
 title = {{Red teaming visual language models}},
 year = {2024}
}

@article{li2024red,
 author = {Li, Mukai and Li, Lei and Yin, Yuwei and Ahmed, Masood and Liu, Zhenguang and Liu, Qi},
 journal = {arXiv:2401.12915},
 title = {Red Teaming Visual Language Models},
 year = {2024}
}

@article{li2024survey,
 author = {Li, Lin and Chen, Guikun and Shi, Hanrong and Xiao, Jun and Chen, Long},
 journal = {arXiv preprint arXiv:2409.18142},
 title = {A Survey on Multimodal Benchmarks: In the Era of Large AI Models},
 year = {2024}
}

@inproceedings{ligeoreasoner,
 author = {Li, Ling and Ye, Yu and Jiang, Bingchuan and Zeng, Wei},
 booktitle = {Proc. of ICML},
 title = {GeoReasoner: Geo-localization with Reasoning in Street Views using a Large Vision-Language Model},
 year = {2024}
}

@inproceedings{lin2014microsoft,
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 booktitle = {Proc. of ECCV},
 pages = {740--755},
 title = {Microsoft coco: Common objects in context},
 year = {2014}
}

@inproceedings{lin2014microsoft,
 author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
 booktitle = {Proc. of ECCV},
 pages = {740--755},
 title = {Microsoft coco: Common objects in context},
 year = {2014}
}

@article{lin2022,
 author = {Lin, Jinliang and Zheng, Zhedong and Zhong, Zhun and Luo, Zhiming and Li, Shaozi and Yang, Yi and Sebe, Nicu},
 journal = {IEEE Transactions on Image Processing},
 pages = {3780-3792},
 title = {{Joint Representation Learning and Keypoint Detection for Cross-View Geo-Localization}},
 year = {2022}
}

@article{lin2023sphinx,
 author = {Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
 journal = {arXiv:2311.07575},
 title = {Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models},
 year = {2023}
}

@article{lin2023vila,
 author = {Lin, Ji and Yin, Hongxu and Ping, Wei and Lu, Yao and Molchanov, Pavlo and Tao, Andrew and Mao, Huizi and Kautz, Jan and Shoeybi, Mohammad and Han, Song},
 journal = {arXiv preprint arXiv:2312.07533},
 title = {Vila: On pre-training for visual language models},
 year = {2023}
}

@article{lin2023vila,
 author = {Lin, Ji and Yin, Hongxu and Ping, Wei and Lu, Yao and Molchanov, Pavlo and Tao, Andrew and Mao, Huizi and Kautz, Jan and Shoeybi, Mohammad and Han, Song},
 journal = {arXiv preprint arXiv:2312.07533},
 title = {{VILA: On Pre-training for Visual Language Models}},
 year = {2023}
}

@article{lin2024goat,
 author = {Lin, Hongzhan and Luo, Ziyang and Wang, Bo and Yang, Ruichao and Ma, Jing},
 journal = {arXiv preprint arXiv:2401.01523},
 title = {{GOAT-Bench: Safety Insights to Large Multimodal Models through Meme-Based Social Abuse}},
 year = {2024}
}

@article{lin2024moe,
 author = {Lin, Bin and Tang, Zhenyu and Ye, Yang and Cui, Jiaxi and Zhu, Bin and Jin, Peng and Zhang, Junwu and Ning, Munan and Yuan, Li},
 journal = {arXiv:2401.15947},
 title = {MoE-LLaVA: Mixture of Experts for Large Vision-Language Models},
 year = {2024}
}

@inproceedings{liu2019lending,
 author = {Liu, Liu and Li, Hongdong},
 booktitle = {Proc. of CVPR},
 pages = {5624--5633},
 title = {Lending orientation to neural networks for cross-view geo-localization},
 year = {2019}
}

@article{liu2021makes,
 author = {Liu, Jiachang and Shen, Dinghan and Zhang, Yizhe and Dolan, Bill and Carin, Lawrence and Chen, Weizhu},
 journal = {arXiv:2101.06804},
 title = {What Makes Good In-Context Examples for GPT-33 ?},
 year = {2021}
}

@article{liu2021p,
 author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng Lam and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
 journal = {arXiv preprint arXiv:2110.07602},
 title = {{P-tuning v2: Prompt tuning can be comparable to fine-tuning universally across scales and tasks}},
 year = {2021}
}

@inproceedings{liu2021slake,
 author = {Liu, Bo and Zhan, Li-Ming and Xu, Li and Ma, Lin and Yang, Yan and Wu, Xiao-Ming},
 booktitle = {ISBI},
 title = {Slake: A semantically-labeled knowledge-enhanced dataset for medical visual question answering},
 year = {2021}
}

@inproceedings{liu2021swin,
 author = {Liu, Ze and Lin, Yutong and Cao, Yue and Hu, Han and Wei, Yixuan and Zhang, Zheng and Lin, Stephen and Guo, Baining},
 booktitle = {Proc. of ICCV},
 pages = {10012--10022},
 title = {{Swin transformer: Hierarchical vision transformer using shifted windows}},
 year = {2021}
}

@inproceedings{liu2022p,
 author = {Liu, Xiao and Ji, Kaixuan and Fu, Yicheng and Tam, Weng and Du, Zhengxiao and Yang, Zhilin and Tang, Jie},
 booktitle = {Proc. of ACL},
 pages = {61--68},
 title = {{P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks}},
 year = {2022}
}

@article{liu2023controlllm,
 author = {Liu, Zhaoyang and Lai, Zeqiang and Gao, Zhangwei and Cui, Erfei and Li, Zhiheng and Zhu, Xizhou and Lu, Lewei and Chen, Qifeng and Qiao, Yu and Dai, Jifeng and others},
 journal = {arXiv preprint arXiv:2310.17796},
 title = {{Controlllm: Augment language models with tools by searching on graphs}},
 year = {2023}
}

@inproceedings{liu2023improved,
 author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
 booktitle = {NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following},
 title = {{Improved Baselines with Visual Instruction Tuning}},
 year = {2023}
}

@article{liu2023improved,
 author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
 journal = {arXiv:2310.03744},
 title = {Improved baselines with visual instruction tuning},
 year = {2023}
}

@misc{liu2023improvedllava,
 author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
 title = {Improved Baselines with Visual Instruction Tuning},
 year = {2023}
}

@article{liu2023llava,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 journal = {NIPS},
 title = {{Visual Instruction Tuning}},
 year = {2024}
}

@inproceedings{liu2023llava,
 author = {Haotian Liu and Chunyuan Li and Qingyang Wu and Yong Jae Lee},
 booktitle = {Proc. of NeurIPS},
 title = {Visual Instruction Tuning},
 year = {2023}
}

@inproceedings{liu2023llava,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 booktitle = {Proc. of NeurIPS},
 title = {{Visual Instruction Tuning}},
 year = {2023}
}

@article{liu2023llavaplus,
 author = {Liu, Shilong and Cheng, Hao and Liu, Haotian and Zhang, Hao and Li, Feng and Ren, Tianhe and Zou, Xueyan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
 journal = {arXiv preprint arXiv:2311.05437},
 title = {{Llava-plus: Learning to use tools for creating multimodal agents}},
 year = {2023}
}

@inproceedings{liu2023mitigating,
 author = {Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
 booktitle = {Proc. of ICLR},
 title = {Mitigating hallucination in large multi-modal models via robust instruction tuning},
 year = {2023}
}

@article{liu2023mmbench,
 author = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
 journal = {arXiv preprint arXiv:2307.06281},
 title = {Mmbench: Is your multi-modal model an all-around player?},
 year = {2023}
}

@article{liu2023visual,
 author = {Liu, Fangyu and Emerson, Guy and Collier, Nigel},
 journal = {TACL},
 pages = {635--651},
 title = {Visual spatial reasoning},
 year = {2023}
}

@misc{liu2024groundingdinomarryingdino,
 archivePrefix = {arXiv},
 author = {Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
 eprint = {2303.05499},
 primaryClass = {cs.CV},
 title = {Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection},
 url = {https://arxiv.org/abs/2303.05499},
 year = {2024}
}

@misc{liu2024llavanext,
 author = {Liu, Haotian and Li, Chunyuan and Li, Yuheng and Li, Bo and Zhang, Yuanhan and Shen, Sheng and Lee, Yong Jae},
 title = {LLaVA-NeXT: Improved reasoning, OCR, and world knowledge},
 year = {2024}
}

@inproceedings{liu2024mitigating,
 author = {Liu, Fuxiao and Lin, Kevin and Li, Linjie and Wang, Jianfeng and Yacoob, Yaser and Wang, Lijuan},
 booktitle = {ICLR},
 title = {Mitigating hallucination in large multi-modal models via robust instruction tuning},
 year = {2024}
}

@misc{liu2024mitigatinghallucinationlargemultimodal,
 archivePrefix = {arXiv},
 author = {Fuxiao Liu and Kevin Lin and Linjie Li and Jianfeng Wang and Yaser Yacoob and Lijuan Wang},
 eprint = {2306.14565},
 primaryClass = {cs.CV},
 title = {Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning},
 url = {https://arxiv.org/abs/2306.14565},
 year = {2024}
}

@article{liu2024phd,
 author = {Liu, Jiazhen and Fu, Yuhan and Xie, Ruobing and Xie, Runquan and Sun, Xingwu and Lian, Fengzong and Kang, Zhanhui and Li, Xirong},
 journal = {arXiv preprint arXiv:2403.11116},
 title = {PhD: A Prompted Visual Hallucination Evaluation Dataset},
 year = {2024}
}

@article{liu2024seeing,
 author = {Liu, Yexin and Liang, Zhengyang and Wang, Yueze and He, Muyang and Li, Jian and Zhao, Bo},
 journal = {arXiv preprint arXiv:2406.10638},
 title = {Seeing Clearly, Answering Incorrectly: A Multimodal Robustness Benchmark for Evaluating MLLMs on Leading Questions},
 year = {2024}
}

@article{liu2024sphinx,
 author = {Liu, Dongyang and Zhang, Renrui and Qiu, Longtian and Huang, Siyuan and Lin, Weifeng and Zhao, Shitian and Geng, Shijie and Lin, Ziyi and Jin, Peng and Zhang, Kaipeng and others},
 journal = {arXiv preprint arXiv:2402.05935},
 title = {Sphinx-x: Scaling data and parameters for a family of multi-modal large language models},
 year = {2024}
}

@article{liu2024survey,
 author = {Liu, Hanchao and Xue, Wenyuan and Chen, Yifei and Chen, Dapeng and Zhao, Xiutian and Wang, Ke and Hou, Liping and Li, Rongjun and Peng, Wei},
 journal = {arXiv preprint arXiv:2402.00253},
 title = {A survey on hallucination in large vision-language models},
 year = {2024}
}

@article{liu2024textmonkey,
 author = {Liu, Yuliang and Yang, Biao and Liu, Qiang and Li, Zhang and Ma, Zhiyin and Zhang, Shuo and Bai, Xiang},
 journal = {arXiv:2403.04473},
 title = {TextMonkey: An OCR-Free Large Multimodal Model for Understanding Document},
 year = {2024}
}

@article{liu2024visual,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 journal = {Proc. of NeurIPS},
 title = {Visual instruction tuning},
 year = {2024}
}

@inproceedings{liuaudioldm2023,
 author = {Haohe Liu and
Zehua Chen and
Yi Yuan and
Xinhao Mei and
Xubo Liu and
Danilo P. Mandic and
Wenwu Wang and
Mark D. Plumbley},
 booktitle = {Proc. of ICML},
 pages = {21450--21474},
 title = {{AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}},
 year = {2023}
}

@inproceedings{liuaudioldm2023,
 author = {Haohe Liu and
Zehua Chen and
Yi Yuan and
Xinhao Mei and
Xubo Liu and
Danilo P. Mandic and
Wenwu Wang and
Mark D. Plumbley},
 booktitle = {Proc. of ICML},
 pages = {21450--21474},
 title = {{AudioLDM: Text-to-Audio Generation with Latent Diffusion Models}},
 year = {2023}
}

@article{liuaudioldm22023,
 author = {Haohe Liu and
Qiao Tian and
Yi Yuan and
Xubo Liu and
Xinhao Mei and
Qiuqiang Kong and
Yuping Wang and
Wenwu Wang and
Yuxuan Wang and
Mark D. Plumbley},
 journal = {CoRR},
 title = {{AudioLDM 2: Learning Holistic Audio Generation with Self-supervised
Pretraining}},
 year = {2023}
}

@article{liuaudioldm22023,
 author = {Haohe Liu and
Qiao Tian and
Yi Yuan and
Xubo Liu and
Xinhao Mei and
Qiuqiang Kong and
Yuping Wang and
Wenwu Wang and
Yuxuan Wang and
Mark D. Plumbley},
 journal = {CoRR},
 title = {{AudioLDM 2: Learning Holistic Audio Generation with Self-supervised
Pretraining}},
 year = {2023}
}

@article{llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv:2302.13971},
 title = {Llama: Open and efficient foundation language models},
 year = {2023}
}

@article{llama-2,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {arXiv:2307.09288},
 title = {Llama 2: Open foundation and fine-tuned chat models},
 year = {2023}
}

@article{llama-adapter,
 author = {Zhang, Renrui and Han, Jiaming and Zhou, Aojun and Hu, Xiangfei and Yan, Shilin and Lu, Pan and Li, Hongsheng and Gao, Peng and Qiao, Yu},
 journal = {arXiv:2303.16199},
 title = {Llama-adapter: Efficient fine-tuning of language models with zero-init attention},
 year = {2023}
}

@article{llama-adapter-v2,
 author = {Gao, Peng and Han, Jiaming and Zhang, Renrui and Lin, Ziyi and Geng, Shijie and Zhou, Aojun and Zhang, Wei and Lu, Pan and He, Conghui and Yue, Xiangyu and others},
 journal = {arXiv:2304.15010},
 title = {Llama-adapter v2: Parameter-efficient visual instruction model},
 year = {2023}
}

@article{llava,
 author = {Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
 journal = {arXiv:2304.08485},
 title = {Visual instruction tuning},
 year = {2023}
}

@article{llava-med,
 author = {Li, Chunyuan and Wong, Cliff and Zhang, Sheng and Usuyama, Naoto and Liu, Haotian and Yang, Jianwei and Naumann, Tristan and Poon, Hoifung and Gao, Jianfeng},
 journal = {arXiv:2306.00890},
 title = {LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day},
 year = {2023}
}

@article{lu2021fantastically,
 author = {Lu, Yao and Bartolo, Max and Moore, Alastair and Riedel, Sebastian and Stenetorp, Pontus},
 journal = {arXiv:2104.08786},
 title = {Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity},
 year = {2021}
}

@inproceedings{lu2021iconqa,
 author = {Lu, Pan and Qiu, Liang and Chen, Jiaqi and Xia, Tony and Zhao, Yizhou and Zhang, Wei and Yu, Zhou and Liang, Xiaodan and Zhu, Song-Chun},
 booktitle = {Proc. of NeurIPS},
 title = {IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning},
 year = {2021}
}

@article{lu2022dynamic,
 author = {Lu, Pan and Qiu, Liang and Chang, Kai-Wei and Wu, Ying Nian and Zhu, Song-Chun and Rajpurohit, Tanmay and Clark, Peter and Kalyan, Ashwin},
 journal = {arXiv:2209.14610},
 title = {Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning},
 year = {2022}
}

@inproceedings{lu2022learn,
 author = {Lu, Pan and Mishra, Swaroop and Xia, Tony and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Ashwin Kalyan},
 booktitle = {Proc. of NeurIPS},
 title = {Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering},
 year = {2022}
}

@article{lu2022learn,
 author = {Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
 journal = {Proc. of NeurIPS},
 pages = {2507--2521},
 title = {Learn to explain: Multimodal reasoning via thought chains for science question answering},
 year = {2022}
}

@article{lu2023empirical,
 author = {Lu, Yadong and Li, Chunyuan and Liu, Haotian and Yang, Jianwei and Gao, Jianfeng and Shen, Yelong},
 journal = {arXiv:2309.09958},
 title = {An empirical study of scaling instruct-tuned large multimodal models},
 year = {2023}
}

@article{lu2023lyrics,
 author = {Lu, Junyu and Gan, Ruyi and Zhang, Dixiang and Wu, Xiaojun and Wu, Ziwei and Sun, Renliang and Zhang, Jiaxing and Zhang, Pingjian and Song, Yan},
 journal = {arXiv preprint arXiv:2312.05278},
 title = {{Lyrics: Boosting Fine-grained Language-Vision Alignment and Comprehension via Semantic-aware Visual Objects}},
 year = {2023}
}

@article{lu2023lyrics,
 author = {Lu, Junyu and Gan, Ruyi and Zhang, Dixiang and Wu, Xiaojun and Wu, Ziwei and Sun, Renliang and Zhang, Jiaxing and Zhang, Pingjian and Song, Yan},
 journal = {arXiv preprint arXiv:2312.05278},
 title = {Lyrics: Boosting fine-grained language-vision alignment and comprehension via semantic-aware visual objects},
 year = {2023}
}

@article{lu2023vim,
 author = {Lu, Yujie and Li, Xiujun and Wang, William Yang and Choi, Yejin},
 journal = {arXiv preprint arXiv:2311.17647},
 title = {{Vim: Probing multimodal large language models for visual embedded instruction following}},
 year = {2023}
}

@article{lu2024deepseek,
 author = {Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Sun, Yaofeng and others},
 journal = {arXiv preprint arXiv:2403.05525},
 title = {Deepseek-vl: towards real-world vision-language understanding},
 year = {2024}
}

@inproceedings{luo2022g3,
 author = {Luo, Grace and Biamby, Giscard and Darrell, Trevor and Fried, Daniel and Rohrbach, Anna},
 booktitle = {Proc. of EMNLP Findings},
 pages = {5841--5853},
 title = {G3: Geolocation via Guidebook Grounding},
 year = {2022}
}

@inproceedings{luo2022g3,
 author = {Luo, Grace and Biamby, Giscard and Darrell, Trevor and Fried, Daniel and Rohrbach, Anna},
 booktitle = {Proc. of EMNLP Findings},
 pages = {5841--5853},
 title = {G3: Geolocation via Guidebook Grounding},
 year = {2022}
}

@article{luo2023wizardcoder,
 author = {Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
 journal = {arXiv preprint arXiv:2306.08568},
 title = {{WizardCoder: Empowering Code Large Language Models with Evol-Instruct}},
 year = {2023}
}

@article{lv2023kosmos,
 author = {Lv, Tengchao and Huang, Yupan and Chen, Jingye and Cui, Lei and Ma, Shuming and Chang, Yaoyao and Huang, Shaohan and Wang, Wenhui and Dong, Li and Luo, Weiyao and others},
 journal = {arXiv preprint arXiv:2309.11419},
 title = {{Kosmos-2.5: A multimodal literate model}},
 year = {2023}
}

@article{m3it,
 author = {Lei Li and Yuwei Yin and Shicheng Li and Liang Chen and Peiyi Wang and Shuhuai Ren and Mukai Li and Yazheng Yang and Jingjing Xu and Xu Sun and Lingpeng Kong and Qi Liu},
 journal = {arXiv:2306.04387},
 title = {M$^3$IT: A Large-Scale Dataset towards Multi-Modal Multilingual Instruction Tuning},
 year = {2023}
}

@article{ma2023dolphins,
 author = {Ma, Yingzi and Cao, Yulong and Sun, Jiachen and Pavone, Marco and Xiao, Chaowei},
 journal = {arXiv preprint arXiv:2312.00438},
 title = {{Dolphins: Multimodal language model for driving}},
 year = {2023}
}

@article{maaz2023video,
 author = {Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
 journal = {arXiv preprint arXiv:2306.05424},
 title = {{Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models}},
 year = {2023}
}

@article{macaw-llm,
 author = {Chenyang Lyu and Minghao Wu and Longyue Wang and Xinting Huang and Bingshuai Liu and Zefeng Du and Shuming Shi and Zhaopeng Tu},
 journal = {arXiv:2306.09093},
 title = {Macaw-LLM: Multi-Modal Language Modeling with Image, Audio, Video, and Text Integration},
 year = {2023}
}

@inproceedings{mao2016generation,
 author = {Mao, Junhua and Huang, Jonathan and Toshev, Alexander and Camburu, Oana and Yuille, Alan L and Murphy, Kevin},
 booktitle = {Proc. of CVPR},
 pages = {11--20},
 title = {Generation and comprehension of unambiguous object descriptions},
 year = {2016}
}

@inproceedings{marino2019ok,
 author = {Marino, Kenneth and Rastegari, Mohammad and Farhadi, Ali and Mottaghi, Roozbeh},
 booktitle = {Proc. of CVPR},
 pages = {3195--3204},
 title = {Ok-vqa: A visual question answering benchmark requiring external knowledge},
 year = {2019}
}

@inproceedings{mathew2021docvqa,
 author = {Mathew, Minesh and Karatzas, Dimosthenis and Jawahar, CV},
 booktitle = {Proceedings of the IEEE/CVF winter conference on applications of computer vision},
 pages = {2200--2209},
 title = {Docvqa: A dataset for vqa on document images},
 year = {2021}
}

@inproceedings{mathews2016senticap,
 author = {Mathews, Alexander and Xie, Lexing and He, Xuming},
 booktitle = {Proc. of AAAI},
 title = {Senticap: Generating image descriptions with sentiments},
 year = {2016}
}

@inproceedings{mathvista,
 author = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
 booktitle = {ICLR},
 title = {Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
 year = {2024}
}

@misc{MC-LLaVA,
 title = {Multi-crop LLaVA-3b},
 url = {https://huggingface.co/visheratin/MC-LLaVA-3b},
 year = {2023}
}

@incollection{mccloskey1989catastrophic,
 author = {McCloskey, Michael and Cohen, Neal J},
 booktitle = {Psychology of learning and motivation},
 pages = {109--165},
 title = {{Catastrophic interference in connectionist networks: The sequential learning problem}},
 year = {1989}
}

@book{McCracken:1990:SSC:575315,
 address = {New York, NY, USA},
 author = {McCracken, Daniel D. and Golden, Donald G.},
 publisher = {John Wiley \& Sons, Inc.},
 title = {Simplified Structured COBOL with Microsoft/MicroFocus COBOL},
 year = {1990}
}

@article{mckinzie2024mm1,
 author = {McKinzie, Brandon and Gan, Zhe and Fauconnier, Jean-Philippe and Dodge, Sam and Zhang, Bowen and Dufter, Philipp and Shah, Dhruti and Du, Xianzhi and Peng, Futang and Weers, Floris and others},
 journal = {arXiv:2403.09611},
 title = {MM1: Methods, Analysis \& Insights from Multimodal LLM Pre-training},
 year = {2024}
}

@article{mei2023wavcaps,
 author = {Mei, Xinhao and Meng, Chutong and Liu, Haohe and Kong, Qiuqiang and Ko, Tom and Zhao, Chengqi and Plumbley, Mark D and Zou, Yuexian and Wang, Wenwu},
 journal = {arXiv preprint arXiv:2303.17395},
 title = {Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research},
 year = {2023}
}

@article{meta2024llama,
 author = {Meta, AI},
 journal = {Meta AI Blog. Retrieved December},
 pages = {2024},
 title = {Llama 3.2: Revolutionizing edge AI and vision with open, customizable models},
 year = {2024}
}

@misc{microsoft2024phi2,
 author = {Microsoft},
 title = {Phi-2: The surprising power of small language models},
 year = {2023}
}

@misc{minicpm2024,
 booktitle = {OpenBMB Blog},
 title = {MiniCPM: Unveiling the Potential of End-side Large Language Models},
 year = {2024}
}

@article{minigpt-4,
 author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
 journal = {arXiv:2304.10592},
 title = {Minigpt-4: Enhancing vision-language understanding with advanced large language models},
 year = {2023}
}

@article{mirowski2019streetlearn-navigation,
 author = {Mirowski, Piotr and Banki-Horvath, Andras and Anderson, Keith and Teplyashin, Denis and Hermann, Karl Moritz and Malinowski, Mateusz and Grimes, Matthew Koichi and Simonyan, Karen and Kavukcuoglu, Koray and Zisserman, Andrew and others},
 journal = {arXiv preprint arXiv:1903.01292},
 title = {The {S}treet{L}earn environment and dataset},
 year = {2019}
}

@inproceedings{mishra2019ocr,
 author = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
 booktitle = {Proc. of ICDAR},
 pages = {947--952},
 title = {Ocr-vqa: Visual question answering by reading text in images},
 year = {2019}
}

@inproceedings{mishra2019ocr,
 author = {Mishra, Anand and Shekhar, Shashank and Singh, Ajeet Kumar and Chakraborty, Anirban},
 booktitle = {Proc. of ICDAR},
 pages = {947--952},
 title = {Ocr-vqa: Visual question answering by reading text in images},
 year = {2019}
}

@article{mm-react,
 author = {Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
 journal = {arXiv:2303.11381},
 title = {Mm-react: Prompting chatgpt for multimodal reasoning and action},
 year = {2023}
}

@article{mm-vet,
 author = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
 journal = {arXiv:2308.02490},
 title = {Mm-vet: Evaluating large multimodal models for integrated capabilities},
 year = {2023}
}

@article{mmbench,
 author = {Yuan Liu and Haodong Duan and Yuanhan Zhang and Bo Li and Songyang Zhang and Wangbo Zhao and Yike Yuan and Jiaqi Wang and Conghui He and Ziwei Liu and Kai Chen and Dahua Lin},
 journal = {arXiv:2307.06281},
 title = {MMBench: Is Your Multi-modal Model an All-around Player?},
 year = {2023}
}

@article{mmbench,
 author = {Liu, Yuan and Duan, Haodong and Zhang, Yuanhan and Li, Bo and Zhang, Songyang and Zhao, Wangbo and Yuan, Yike and Wang, Jiaqi and He, Conghui and Liu, Ziwei and others},
 journal = {arXiv:2307.06281},
 title = {Mmbench: Is your multi-modal model an all-around player?},
 year = {2023}
}

@article{mme,
 author = {Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Yang, Jinrui and Zheng, Xiawu and Li, Ke and Sun, Xing and Ji, Rongrong},
 journal = {arXiv preprint arXiv:2306.13394},
 title = {MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
 year = {2023}
}

@article{mme,
 author = {Fu, Chaoyou and Chen, Peixian and Shen, Yunhang and Qin, Yulei and Zhang, Mengdan and Lin, Xu and Qiu, Zhenyu and Lin, Wei and Qiu, Zhenyu and Lin, Wei and others},
 journal = {arXiv:2306.13394},
 title = {MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models},
 year = {2023}
}

@article{mmmu,
 author = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
 journal = {arXiv:2311.16502},
 title = {Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
 year = {2023}
}

@article{moon2022imu2clip,
 author = {Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Dirafzoon, Alireza and Saraf, Aparajita and Bearman, Amy and Damavandi, Babak},
 journal = {arXiv preprint arXiv:2210.14395},
 title = {{IMU2CLIP: Multimodal Contrastive Learning for IMU Motion Sensors from Egocentric Videos and Text}},
 year = {2022}
}

@article{moon2023anymal,
 author = {Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and others},
 journal = {arXiv preprint arXiv:2309.16058},
 title = {{Anymal: An efficient and scalable any-modality augmented language model}},
 year = {2023}
}

@article{moon2023anymal,
 author = {Moon, Seungwhan and Madotto, Andrea and Lin, Zhaojiang and Nagarajan, Tushar and Smith, Matt and Jain, Shashank and Yeh, Chun-Fu and Murugesan, Prakash and Heidari, Peyman and Liu, Yue and others},
 journal = {arXiv:2309.16058},
 title = {Anymal: An efficient and scalable any-modality augmented language model},
 year = {2023}
}

@misc{moondream1,
 title = {moondream1},
 url = {https://huggingface.co/vikhyatk/moondream1},
 year = {2024}
}

@inproceedings{moor2023med,
 author = {Moor, Michael and Huang, Qian and Wu, Shirley and Yasunaga, Michihiro and Dalmia, Yash and Leskovec, Jure and Zakka, Cyril and Reis, Eduardo Pontes and Rajpurkar, Pranav},
 booktitle = {Proc. of ML4H},
 title = {Med-flamingo: a multimodal medical few-shot learner},
 year = {2023}
}

@article{mplug-owl,
 author = {Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
 journal = {arXiv:2304.14178},
 title = {mplug-owl: Modularization empowers large language models with multimodality},
 year = {2023}
}

@book{MR781536,
 address = {Berlin, Germany},
 author = {H{\"o}rmander, Lars},
 pages = {vii+352},
 publisher = {Springer-Verlag},
 title = {The analysis of linear partial differential operators. {IV}},
 year = {1985}
}

@book{MR781537,
 address = {Berlin, Germany},
 author = {H{\"o}rmander, Lars},
 pages = {viii+525},
 publisher = {Springer-Verlag},
 title = {The analysis of linear partial differential operators. {III}},
 year = {1985}
}

@inproceedings{msr-vtt,
 author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
 booktitle = {CVPR},
 title = {Msr-vtt: A large video description dataset for bridging video and language},
 year = {2016}
}

@inproceedings{mu2023embodiedgpt,
 author = {Mu, Yao and Zhang, Qinglong and Hu, Mengkang and Wang, Wenhai and Ding, Mingyu and Jin, Jun and Wang, Bin and Dai, Jifeng and Qiao, Yu and Luo, Ping},
 booktitle = {Proc. of NeurIPS},
 title = {{Embodiedgpt: Vision-language pre-training via embodied chain of thought}},
 year = {2023}
}

@book{Mullender:1993:DS:302430,
 address = {New York, NY, USA},
 editor = {Mullender, Sape},
 isbn = {0-201-62427-3},
 publisher = {ACM Press/Addison-Wesley Publishing Co.},
 title = {Distributed systems (2nd Ed.)},
 year = {1993}
}

@inproceedings{muller2018,
 author = {Mller-Budack, Eric and Pustu-Iren, Kader and Ewerth, Ralph},
 booktitle = {Proc. of ECCV},
 pages = {563-579},
 title = {{Geolocation Estimation of Photos using a Hierarchical Model and Scene Classification}},
 year = {2018}
}

@inproceedings{muller2018geolocation,
 author = {Muller-Budack, Eric and Pustu-Iren, Kader and Ewerth, Ralph},
 booktitle = {Proc. of ECCV},
 pages = {563--579},
 title = {Geolocation estimation of photos using a hierarchical model and scene classification},
 year = {2018}
}

@article{multiinstruct,
 author = {Xu, Zhiyang and Shen, Ying and Huang, Lifu},
 journal = {arXiv:2212.10773},
 title = {MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction Tuning},
 year = {2022}
}

@article{multimodal-cot,
 author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Zhao, Hai and Karypis, George and Smola, Alex},
 journal = {arXiv:2302.00923},
 title = {Multimodal chain-of-thought reasoning in language models},
 year = {2023}
}

@article{multimodal-gpt,
 author = {Gong, Tao and Lyu, Chengqi and Zhang, Shilong and Wang, Yudong and Zheng, Miao and Zhao, Qian and Liu, Kuikun and Zhang, Wenwei and Luo, Ping and Chen, Kai},
 journal = {arXiv:2305.04790},
 title = {Multimodal-gpt: A vision and language model for dialogue with humans},
 year = {2023}
}

@incollection{Mumford:1987:MES:54905.54911,
 author = {Mumford, E.},
 booktitle = {Critical issues in information systems research (incoll)},
 pages = {135--155},
 title = {Managerial expert systems and organizational change: some critical research issues},
 year = {1987}
}

@article{nakano2021webgpt,
 author = {Nakano, Reiichiro and Hilton, Jacob and Balaji, Suchir and Wu, Jeff and Ouyang, Long and Kim, Christina and Hesse, Christopher and Jain, Shantanu and Kosaraju, Vineet and Saunders, William and others},
 journal = {arXiv:2112.09332},
 title = {Webgpt: Browser-assisted question-answering with human feedback},
 year = {2021}
}

@inproceedings{Natarajan-01,
 author = {A. Natarajan and M. Motani and B. de Silva and K. Yap and K. C. Chua},
 booktitle = {Network Architectures},
 pages = {322--328},
 title = {Investigating Network Architectures for Body Sensor Networks},
 year = {2007}
}

@article{naveed2023comprehensive,
 author = {Naveed, Humza and Khan, Asad Ullah and Qiu, Shi and Saqib, Muhammad and Anwar, Saeed and Usman, Muhammad and Barnes, Nick and Mian, Ajmal},
 journal = {arXiv preprint arXiv:2307.06435},
 title = {{A comprehensive overview of large language models}},
 year = {2023}
}

@article{nebel:jair-2000,
 author = {Bernhard Nebel},
 journal = {Journal of Artificial Intelligence Research},
 pages = {271--315},
 title = {On the compilability and expressive power of propositional planning formalisms},
 year = {2000}
}

@article{nguyen2021survey,
 author = {Nguyen, Thi Tuyet Hai and Jatowt, Adam and Coustaty, Mickael and Doucet, Antoine},
 journal = {ACM Computing Surveys (CSUR)},
 pages = {1--37},
 title = {Survey of post-OCR processing approaches},
 year = {2021}
}

@inproceedings{ni2023vilas,
 author = {Ni, Ziyi and Han, Minglun and Chen, Feilong and Meng, Linghui and Shi, Jing and Lv, Pin and Xu, Bo},
 booktitle = {Proc. of ICASSP},
 title = {{VILAS: Exploring the Effects of Vision and Language Context in Automatic Speech Recognition}},
 year = {2024}
}

@article{ning2023video,
 author = {Ning, Munan and Zhu, Bin and Xie, Yujia and Lin, Bin and Cui, Jiaxi and Yuan, Lu and Chen, Dongdong and Yuan, Li},
 journal = {arXiv:2311.16103},
 title = {Video-Bench: A Comprehensive Benchmark and Toolkit for Evaluating Video-based Large Language Models},
 year = {2023}
}

@inproceedings{nn-UCF-GSV-Dataset-2014,
 author = {Zamir, A.R. and Shah, M.},
 journal = {Pattern Analysis and Machine Intelligence},
 title = {Image Geo-localization Based on Multiple Nearest Neighbor Feature Matching using Generalized Graphs},
 year = {2014}
}

@article{nye2021show,
 author = {Nye, Maxwell and Andreassen, Anders Johan and Gur-Ari, Guy and Michalewski, Henryk and Austin, Jacob and Bieber, David and Dohan, David and Lewkowycz, Aitor and Bosma, Maarten and Luan, David and others},
 journal = {arXiv:2112.00114},
 title = {Show your work: Scratchpads for intermediate computation with language models},
 year = {2021}
}

@misc{openai2023gpt4,
 archivePrefix = {arXiv},
 author = {OpenAI},
 eprint = {2303.08774},
 primaryClass = {cs.CL},
 title = {{GPT-4 Technical Report}},
 year = {2023}
}

@article{openai2023gpt4,
 author = {OpenAI},
 journal = {arXiv:2303.08774},
 title = {GPT-4 technical report},
 year = {2023}
}

@article{oquab2023dinov2,
 author = {Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni, Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and others},
 journal = {arXiv preprint arXiv:2304.07193},
 title = {{Dinov2: Learning robust visual features without supervision}},
 year = {2023}
}

@misc{oquab2023dinov2,
 author = {Oquab, Maxime and Darcet, Timothe and Moutakanni, Theo and Vo, Huy V. and Szafraniec, Marc and Khalidov, Vasil and Fernandez, Pierre and Haziza, Daniel and Massa, Francisco and El-Nouby, Alaaeldin and Howes, Russell and Huang, Po-Yao and Xu, Hu and Sharma, Vasu and Li, Shang-Wen and Galuba, Wojciech and Rabbat, Mike and Assran, Mido and Ballas, Nicolas and Synnaeve, Gabriel and Misra, Ishan and Jegou, Herve and Mairal, Julien and Labatut, Patrick and Joulin, Armand and Bojanowski, Piotr},
 journal = {arXiv:2304.07193},
 title = {DINOv2: Learning Robust Visual Features without Supervision},
 year = {2023}
}

@article{ordonez2011im2text,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 journal = {Proc. of NeurIPS},
 title = {Im2text: Describing images using 1 million captioned photographs},
 year = {2011}
}

@article{otter,
 author = {Li, Bo and Zhang, Yuanhan and Chen, Liangyu and Wang, Jinghao and Yang, Jingkang and Liu, Ziwei},
 journal = {arXiv:2305.03726},
 title = {Otter: A multi-modal model with in-context instruction tuning},
 year = {2023}
}

@article{ouyang2022training,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
 journal = {Proc. of NeurIPS},
 pages = {27730--27744},
 title = {{Training language models to follow instructions with human feedback}},
 year = {2022}
}

@article{ouyang2022training,
 author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
 journal = {NeurIPS},
 title = {Training language models to follow instructions with human feedback},
 year = {2022}
}

@article{palm-e,
 author = {Driess, Danny and Xia, Fei and Sajjadi, Mehdi SM and Lynch, Corey and Chowdhery, Aakanksha and Ichter, Brian and Wahid, Ayzaan and Tompson, Jonathan and Vuong, Quan and Yu, Tianhe and others},
 journal = {arXiv:2303.03378},
 title = {Palm-e: An embodied multimodal language model},
 year = {2023}
}

@article{pan2023kosmos,
 author = {Pan, Xichen and Dong, Li and Huang, Shaohan and Peng, Zhiliang and Chen, Wenhu and Wei, Furu},
 journal = {arXiv preprint arXiv:2310.02992},
 title = {{Kosmos-g: Generating images in context with multimodal large language models}},
 year = {2023}
}

@article{panagopoulou2023x,
 author = {Panagopoulou, Artemis and Xue, Le and Yu, Ning and Li, Junnan and Li, Dongxu and Joty, Shafiq and Xu, Ran and Savarese, Silvio and Xiong, Caiming and Niebles, Juan Carlos},
 journal = {arXiv preprint arXiv:2311.18799},
 title = {{X-InstructBLIP: A Framework for aligning X-Modal instruction-aware representations to LLMs and Emergent Cross-modal Reasoning}},
 year = {2023}
}

@article{pandagpt,
 author = {Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
 journal = {arXiv:2305.16355},
 title = {PandaGPT: One Model To Instruction-Follow Them All},
 year = {2023}
}

@article{parisi2022talm,
 author = {Parisi, Aaron and Zhao, Yao and Fiedel, Noah},
 journal = {arXiv:2205.12255},
 title = {Talm: Tool augmented language models},
 year = {2022}
}

@article{peng2023instruction,
 author = {Peng, Baolin and Li, Chunyuan and He, Pengcheng and Galley, Michel and Gao, Jianfeng},
 journal = {arXiv:2304.03277},
 title = {Instruction tuning with gpt-4},
 year = {2023}
}

@inproceedings{peng2023kosmos,
 author = {Zhiliang Peng and Wenhui Wang and Li Dong and Yaru Hao and Shaohan Huang and Shuming Ma and Qixiang Ye and Furu Wei},
 booktitle = {Proc. of ICLR},
 title = {Grounding Multimodal Large Language Models to the World},
 year = {2024}
}

@misc{persimmon-8b,
 author = {Elsen, Erich and Odena, Augustus and Nye, Maxwell and Ta\c{s}\i{}rlar, Sa\u{g}nak and Dao, Tri and Hawthorne, Curtis and Moparthi, Deepak and Somani, Arushi},
 title = {{Releasing {Persimmon-8B}}},
 year = {2023}
}

@misc{persimmon-8b,
 author = {Elsen, Erich and Odena, Augustus and Nye, Maxwell and Ta\c{s}\i{}rlar, Sa\u{g}nak and Dao, Tri and Hawthorne, Curtis and Moparthi, Deepak and Somani, Arushi},
 title = {{Releasing {Persimmon-8B}}},
 year = {2023}
}

@mastersthesis{Petrie:1986:NAD:12345,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986}
}

@techreport{Petrie:1986:NAD:899644,
 author = {Petrie, Charles J.},
 title = {New Algorithms for Dependency-Directed Backtracking (Master's thesis)},
 year = {1986}
}

@article{petryk2024aloha,
 author = {Petryk, Suzanne and Chan, David M and Kachinthaya, Anish and Zou, Haodi and Canny, John and Gonzalez, Joseph E and Darrell, Trevor},
 journal = {arXiv preprint arXiv:2404.02904},
 title = {ALOHa: A New Measure for Hallucination in Captioning Models},
 year = {2024}
}

@article{pi2023detgpt,
 author = {Pi, Renjie and Gao, Jiahui and Diao, Shizhe and Pan, Rui and Dong, Hanze and Zhang, Jipeng and Yao, Lewei and Han, Jianhua and Xu, Hang and Zhang, Lingpeng Kong Tong},
 journal = {arXiv preprint arXiv:2305.14167},
 title = {{DetGPT: Detect What You Need via Reasoning}},
 year = {2023}
}

@inbook{PlaNet,
 author = {Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
 booktitle = {Proc. of ECCV},
 pages = {3755},
 title = {PlaNet - Photo Geolocation with Convolutional Neural Networks},
 year = {2016}
}

@inproceedings{plummer2015flickr30k,
 author = {Plummer, Bryan A and Wang, Liwei and Cervantes, Chris M and Caicedo, Juan C and Hockenmaier, Julia and Lazebnik, Svetlana},
 booktitle = {Proc. of ICCV},
 pages = {2641--2649},
 title = {Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models},
 year = {2015}
}

@article{pmc-vqa,
 author = {Zhang, Xiaoman and Wu, Chaoyi and Zhao, Ziheng and Lin, Weixiong and Zhang, Ya and Wang, Yanfeng and Xie, Weidi},
 journal = {arXiv:2305.10415},
 title = {PMC-VQA: Visual Instruction Tuning for Medical Visual Question Answering},
 year = {2023}
}

@article{PointCLIP-V2,
 author = {Zhu, Xiangyang and Zhang, Renrui and He, Bowei and Zeng, Ziyao and Zhang, Shanghang and Gao, Peng},
 journal = {arXiv:2211.11682},
 title = {PointCLIP V2: Adapting CLIP for Powerful 3D Open-world Learning},
 year = {2022}
}

@inproceedings{pramanick2022,
 author = {Pramanick, Shraman and Nowara, Ewa M and Gleason, Joshua and Castillo, Carlos D and Chellappa, Rama},
 booktitle = {Proc. of ECCV},
 pages = {196-215},
 title = {{Where in the World is this Image? Transformer-based Geo-localization in the Wild}},
 year = {2022}
}

@inproceedings{pramanick2022world,
 author = {Pramanick, Shraman and Nowara, Ewa M and Gleason, Joshua and Castillo, Carlos D and Chellappa, Rama},
 booktitle = {Proc. of ECCV},
 pages = {196--215},
 title = {Where in the world is this image? transformer-based geo-localization in the wild},
 year = {2022}
}

@inproceedings{qi2019exploiting,
 author = {Qi, Peng and Cao, Juan and Yang, Tianyun and Guo, Junbo and Li, Jintao},
 booktitle = {Proc. of ICDM},
 pages = {518--527},
 title = {Exploiting multi-domain visual information for fake news detection},
 year = {2019}
}

@article{qian2024easy,
 author = {Qian, Yusu and Zhang, Haotian and Yang, Yinfei and Gan, Zhe},
 journal = {arXiv preprint arXiv:2402.13220},
 title = {How Easy is It to Fool Your Multimodal LLMs? An Empirical Analysis on Deceptive Prompts},
 year = {2024}
}

@inproceedings{qiao2022reasoning,
 author = {Qiao, Shuofei and Ou, Yixin and Zhang, Ningyu and Chen, Xiang and Yao, Yunzhi and Deng, Shumin and Tan, Chuanqi and Huang, Fei and Chen, Huajun},
 booktitle = {acl},
 pages = {53685393},
 title = {{Reasoning with Language Model Prompting: A Survey}},
 year = {2023}
}

@article{qiu2024valor,
 author = {Qiu, Haoyi and Hu, Wenbo and Dou, Zi-Yi and Peng, Nanyun},
 journal = {arXiv preprint arXiv:2404.13874},
 title = {VALOR-EVAL: Holistic Coverage and Faithfulness Evaluation of Large Vision-Language Models},
 year = {2024}
}

@article{Qwen,
 author = {Jinze Bai and
Shuai Bai and
Shusheng Yang and
Shijie Wang and
Sinan Tan and
Peng Wang and
Junyang Lin and
Chang Zhou and
Jingren Zhou},
 journal = {CoRR},
 title = {{Qwen-VL: {A} Frontier Large Vision-Language Model with Versatile Abilities}},
 year = {2023}
}

@article{qwen-lm,
 author = {Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
 journal = {arXiv:2309.16609},
 title = {Qwen technical report},
 year = {2023}
}

@article{radford2019language,
 author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
 title = {Language Models are Unsupervised Multitask Learners},
 year = {2019}
}

@inproceedings{radford2021learning,
 author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
 booktitle = {ICML},
 pages = {8748-8763},
 title = {{Learning Transferable Visual Models From Natural Language Supervision}},
 year = {2021}
}

@inproceedings{radford2021learning,
 author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
 booktitle = {Proc. of ICML},
 pages = {8748--8763},
 title = {{Learning transferable visual models from natural language supervision}},
 year = {2021}
}

@misc{radford2022robustspeechrecognitionlargescale,
 archivePrefix = {arXiv},
 author = {Alec Radford and Jong Wook Kim and Tao Xu and Greg Brockman and Christine McLeavey and Ilya Sutskever},
 eprint = {2212.04356},
 primaryClass = {eess.AS},
 title = {Robust Speech Recognition via Large-Scale Weak Supervision},
 url = {https://arxiv.org/abs/2212.04356},
 year = {2022}
}

@article{rafailov2023direct,
 author = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Ermon, Stefano and Manning, Christopher D and Finn, Chelsea},
 journal = {NeurIPS},
 title = {Direct preference optimization: Your language model is secretly a reward model},
 year = {2023}
}

@article{raffel2020exploring,
 author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
 journal = {The Journal of Machine Learning Research},
 pages = {5485--5551},
 title = {{Exploring the limits of transfer learning with a unified text-to-text transformer}},
 year = {2020}
}

@inproceedings{rao2023retrieval,
 author = {Rao, Jiahua and Shan, Zifei and Liu, Longpo and Zhou, Yao and Yang, Yuedong},
 booktitle = {Proc. of ACM MM},
 pages = {5399-5409},
 title = {{Retrieval-based Knowledge Augmented Vision Language Pre-training}},
 year = {2023}
}

@article{rasheed2023glamm,
 author = {Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M and Xing, Erix and Yang, Ming-Hsuan and Khan, Fahad S},
 journal = {arXiv preprint arXiv:2311.03356},
 title = {{Glamm: Pixel grounding large multimodal model}},
 year = {2023}
}

@article{rasheed2023glamm,
 author = {Rasheed, Hanoona and Maaz, Muhammad and Shaji, Sahal and Shaker, Abdelrahman and Khan, Salman and Cholakkal, Hisham and Anwer, Rao M and Xing, Erix and Yang, Ming-Hsuan and Khan, Fahad S},
 journal = {arXiv:2311.03356},
 title = {Glamm: Pixel grounding large multimodal model}
}

@article{rebuffi2017learning,
 author = {Rebuffi, Sylvestre-Alvise and Bilen, Hakan and Vedaldi, Andrea},
 journal = {Proc. of NeurIPS},
 title = {{Learning multiple visual domains with residual adapters}},
 year = {2017}
}

@inproceedings{reid:scribe,
 author = {Brian K. Reid},
 booktitle = {Proceedings of the 7th Annual Symposium on Principles of Programming Languages},
 pages = {24--31},
 title = {A high-level approach to computer document formatting},
 year = {1980}
}

@inproceedings{reimers2019,
 author = {Reimers, Nils and Gurevych, Iryna},
 booktitle = {Proc. of EMNLP},
 pages = {3980-3990},
 title = {{Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks}},
 year = {2019}
}

@article{ren2023pixellm,
 author = {Ren, Zhongwei and Huang, Zhicheng and Wei, Yunchao and Zhao, Yao and Fu, Dongmei and Feng, Jiashi and Jin, Xiaojie},
 journal = {arXiv preprint arXiv:2312.02228},
 title = {{PixelLM: Pixel Reasoning with Large Multimodal Model}},
 year = {2023}
}

@article{robins1995catastrophic,
 author = {Robins, Anthony},
 journal = {Connection Science},
 pages = {123--146},
 title = {{Catastrophic forgetting, rehearsal and pseudorehearsal}},
 year = {1995}
}

@inproceedings{rohrbach2018object,
 author = {Rohrbach, Anna and Hendricks, Lisa Anne and Burns, Kaylee and Darrell, Trevor and Saenko, Kate},
 booktitle = {EMNLP},
 title = {Object hallucination in image captioning},
 year = {2018}
}

@inproceedings{roller2012,
 author = {Roller, Stephen and Speriosu, Michael and Rallapalli, Sarat and Wing, Benjamin and Baldridge, Jason},
 booktitle = {Proc. of EMNLP},
 pages = {1500-1510},
 title = {{Supervised Text-based Geolocation Using Language Models on an Adaptive Grid}},
 year = {2012}
}

@inproceedings{rombach2022high,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
 booktitle = {Proc. of CVPR},
 pages = {10684--10695},
 title = {{High-resolution image synthesis with latent diffusion models}},
 year = {2022}
}

@inproceedings{rombach2022high,
 author = {Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik and Esser, Patrick and Ommer, Bj{\"o}rn},
 booktitle = {CVPR},
 title = {High-resolution image synthesis with latent diffusion models},
 year = {2022}
}

@inproceedings{ronneberger2015u,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 booktitle = {Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
 pages = {234--241},
 title = {{U-net: Convolutional networks for biomedical image segmentation}},
 year = {2015}
}

@inproceedings{ronneberger2015u,
 author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
 booktitle = {Medical Image Computing and Computer-Assisted Intervention--MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III 18},
 pages = {234--241},
 title = {{U-net: Convolutional networks for biomedical image segmentation}},
 year = {2015}
}

@article{Rose2023Visual,
 author = {Daniel Philip Rose and Vaishnavi Himakunthala and Andy Ouyang and Ryan He and Alex Mei and Yujie Lu and Michael Stephen Saxon and Chinmay Sonar and Diba Mirza and William Yang Wang},
 journal = {ArXiv},
 title = {Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings},
 year = {2023}
}

@article{ruan2022survey,
 author = {Ruan, Ludan and Jin, Qin},
 journal = {AI Open},
 pages = {1--13},
 title = {{Survey: Transformer based video-language pre-training}},
 year = {2022}
}

@article{rubenstein2023audiopalm,
 author = {Rubenstein, Paul K and Asawaroengchai, Chulayuth and Nguyen, Duc Dung and Bapna, Ankur and Borsos, Zal{\'a}n and Quitry, F{\'e}lix de Chaumont and Chen, Peter and Badawy, Dalia El and Han, Wei and Kharitonov, Eugene and others},
 journal = {arXiv preprint arXiv:2306.12925},
 title = {{AudioPaLM: A Large Language Model That Can Speak and Listen}},
 year = {2023}
}

@article{rubenstein2023audiopalm,
 author = {Rubenstein, Paul K and Asawaroengchai, Chulayuth and Nguyen, Duc Dung and Bapna, Ankur and Borsos, Zal{\'a}n and Quitry, F{\'e}lix de Chaumont and Chen, Peter and Badawy, Dalia El and Han, Wei and Kharitonov, Eugene and others},
 journal = {arXiv:2306.12925},
 title = {Audiopalm: A large language model that can speak and listen},
 year = {2023}
}

@article{SaeediJETC10,
 author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi and Zahra Sasanian},
 journal = {J. Emerg. Technol. Comput. Syst.},
 title = {Synthesis of Reversible Circuit Using Cycle-Based Approach},
 year = {2010}
}

@article{SaeediMEJ10,
 author = {Mehdi Saeedi and Morteza Saheb Zamani and Mehdi Sedighi},
 journal = {Microelectron. J.},
 pages = {185--194},
 title = {A library-based synthesis methodology for reversible logic},
 year = {2010}
}

@article{sajjadi2022object,
 author = {Sajjadi, Mehdi SM and Duckworth, Daniel and Mahendran, Aravindh and van Steenkiste, Sjoerd and Pavetic, Filip and Lucic, Mario and Guibas, Leonidas J and Greff, Klaus and Kipf, Thomas},
 journal = {Proc. of NeurIPS},
 pages = {9512--9524},
 title = {{Object scene representation transformer}},
 year = {2022}
}

@book{salas:calculus,
 address = {New York},
 author = {S.L. Salas and Einar Hille},
 publisher = {John Wiley and Sons},
 title = {Calculus: One and Several Variable},
 year = {1978}
}

@article{sam,
 author = {Kirillov, Alexander and Mintun, Eric and Ravi, Nikhila and Mao, Hanzi and Rolland, Chloe and Gustafson, Laura and Xiao, Tete and Whitehead, Spencer and Berg, Alexander C and Lo, Wan-Yen and others},
 journal = {arXiv:2304.02643},
 title = {Segment anything},
 year = {2023}
}

@article{sanh2021multitask,
 author = {Sanh, Victor and Webson, Albert and Raffel, Colin and Bach, Stephen H and Sutawika, Lintang and Alyafeai, Zaid and Chaffin, Antoine and Stiegler, Arnaud and Scao, Teven Le and Raja, Arun and others},
 journal = {arXiv:2110.08207},
 title = {Multitask prompted training enables zero-shot task generalization},
 year = {2021}
}

@article{SBU-Captions,
 author = {Ordonez, Vicente and Kulkarni, Girish and Berg, Tamara},
 journal = {NeurIPS},
 title = {Im2text: Describing images using 1 million captioned photographs},
 year = {2011}
}

@article{schick2023toolformer,
 author = {Schick, Timo and Dwivedi-Yu, Jane and Dess{\`\i}, Roberto and Raileanu, Roberta and Lomeli, Maria and Zettlemoyer, Luke and Cancedda, Nicola and Scialom, Thomas},
 journal = {arXiv:2302.04761},
 title = {Toolformer: Language models can teach themselves to use tools},
 year = {2023}
}

@article{schuhmann2021laion,
 author = {Schuhmann, Christoph and Vencu, Richard and Beaumont, Romain and Kaczmarczyk, Robert and Mullis, Clayton and Katta, Aarush and Coombes, Theo and Jitsev, Jenia and Komatsuzaki, Aran},
 journal = {arXiv preprint arXiv:2111.02114},
 title = {Laion-400m: Open dataset of clip-filtered 400 million image-text pairs},
 year = {2021}
}

@inproceedings{schuhmann2022laion,
 author = {Christoph Schuhmann and Romain Beaumont and Richard Vencu and Cade W Gordon and Ross Wightman and Mehdi Cherti and Theo Coombes and Aarush Katta and Clayton Mullis and Mitchell Wortsman and Patrick Schramowski and Srivatsa R Kundurthy and Katherine Crowson and Ludwig Schmidt and Robert Kaczmarczyk and Jenia Jitsev},
 booktitle = {Proc. of NeurIPS},
 title = {{LAION}-5B: An open large-scale dataset for training next generation image-text models},
 year = {2022}
}

@article{schuhmann2022laion,
 author = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
 journal = {Proc. of NeurIPS},
 pages = {25278--25294},
 title = {Laion-5b: An open large-scale dataset for training next generation image-text models},
 year = {2022}
}

@inproceedings{schwenk2022okvqa,
 author = {Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
 booktitle = {Proc. of ECCV},
 pages = {146--162},
 title = {A-okvqa: A benchmark for visual question answering using world knowledge},
 year = {2022}
}

@inproceedings{schwenk2022okvqa,
 author = {Schwenk, Dustin and Khandelwal, Apoorv and Clark, Christopher and Marino, Kenneth and Mottaghi, Roozbeh},
 booktitle = {Proc. of ECCV},
 pages = {146--162},
 title = {A-okvqa: A benchmark for visual question answering using world knowledge},
 year = {2022}
}

@article{scienceqa,
 author = {Lu, Pan and Mishra, Swaroop and Xia, Tanglin and Qiu, Liang and Chang, Kai-Wei and Zhu, Song-Chun and Tafjord, Oyvind and Clark, Peter and Kalyan, Ashwin},
 journal = {NeurIPS},
 title = {Learn to explain: Multimodal reasoning via thought chains for science question answering},
 year = {2022}
}

@inproceedings{seed-bench,
 author = {Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
 booktitle = {CVPR},
 title = {Seed-bench: Benchmarking multimodal llms with generative comprehension},
 year = {2024}
}

@article{seedbench,
 author = {Li, Bohao and Wang, Rui and Wang, Guangzhi and Ge, Yuying and Ge, Yixiao and Shan, Ying},
 journal = {arXiv preprint arXiv:2307.16125},
 title = {Seed-bench: Benchmarking multimodal llms with generative comprehension},
 year = {2023}
}

@inproceedings{seo2018,
 author = {Seo, Paul Hongsuck and Weyand, Tobias and Sim, Jack and Han, Bohyung},
 booktitle = {Proc. of ECCV},
 pages = {536-551},
 title = {{CPlaNet: Enhancing Image Geolocalization by Combinatorial Partitioning of Maps}},
 year = {2018}
}

@inproceedings{seo2018cplanet,
 author = {Seo, Paul Hongsuck and Weyand, Tobias and Sim, Jack and Han, Bohyung},
 booktitle = {Proc. of ECCV},
 pages = {536--551},
 title = {Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps},
 year = {2018}
}

@inproceedings{seo2018cplanet,
 author = {Seo, Paul Hongsuck and Weyand, Tobias and Sim, Jack and Han, Bohyung},
 booktitle = {Proc. of ECCV},
 pages = {536--551},
 title = {Cplanet: Enhancing image geolocalization by combinatorial partitioning of maps},
 year = {2018}
}

@article{shahgir2024illusionvqa,
 author = {Shahgir, Haz Sameen and Sayeed, Khondker Salman and Bhattacharjee, Abhik and Ahmad, Wasi Uddin and Dong, Yue and Shahriyar, Rifat},
 journal = {arXiv preprint arXiv:2403.15952},
 title = {IllusionVQA: A Challenging Optical Illusion Dataset for Vision Language Models},
 year = {2024}
}

@inproceedings{shao2019objects365,
 author = {Shao, Shuai and Li, Zeming and Zhang, Tianyuan and Peng, Chao and Yu, Gang and Zhang, Xiangyu and Li, Jing and Sun, Jian},
 booktitle = {Proc. of ICCV},
 pages = {8430--8439},
 title = {Objects365: A large-scale, high-quality dataset for object detection},
 year = {2019}
}

@inproceedings{shao2023,
 author = {Shao, Zhenwei and Yu, Zhou and Wang, Meng and Yu, Jun},
 booktitle = {CVPR},
 pages = {14974-14983},
 title = {{Prompting Large Language Models with Answer Heuristics for Knowledge-based Visual Question Answering}},
 year = {2023}
}

@inproceedings{sharma2018conceptual,
 author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
 booktitle = {Proc. of ACL},
 pages = {2556--2565},
 title = {Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
 year = {2018}
}

@inproceedings{sharma2018conceptual,
 author = {Sharma, Piyush and Ding, Nan and Goodman, Sebastian and Soricut, Radu},
 booktitle = {Proc. of ACL},
 pages = {2556--2565},
 title = {Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning},
 year = {2018}
}

@inproceedings{shayegani2023jailbreak,
 author = {Shayegani, Erfan and Dong, Yue and Abu-Ghazaleh, Nael},
 booktitle = {ICLR},
 title = {Jailbreak in pieces: Compositional adversarial attacks on multi-modal language models},
 year = {2023}
}

@inproceedings{shen2023aligning,
 author = {Shen, Yunhang and Fu, Chaoyou and Chen, Peixian and Zhang, Mengdan and Li, Ke and Sun, Xing and Wu, Yunsheng and Lin, Shaohui and Ji, Rongrong},
 booktitle = {CVPR},
 title = {Aligning and Prompting Everything All at Once for Universal Visual Perception},
 year = {2024}
}

@article{shen2023hugginggpt,
 author = {Shen, Yongliang and Song, Kaitao and Tan, Xu and Li, Dongsheng and Lu, Weiming and Zhuang, Yueting},
 journal = {arXiv preprint arXiv:2303.17580},
 title = {{Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface}},
 year = {2023}
}

@article{shen2023mixture,
 author = {Shen, Sheng and Hou, Le and Zhou, Yanqi and Du, Nan and Longpre, Shayne and Wei, Jason and Chung, Hyung Won and Zoph, Barret and Fedus, William and Chen, Xinyun and others},
 journal = {arXiv:2305.14705},
 title = {Mixture-of-experts meets instruction tuning: A winning combination for large language models},
 year = {2023}
}

@article{shen_2017_streetvizor,
 author = {Shen, Qiaomu and Zeng, Wei and Ye, Yu and Mueller Arisona, Stefan and Schubiger, Simon and Burkhard, Remo and Qu, Huamin},
 journal = {TVCG},
 pages = {1004-1013},
 title = {{StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views}},
 year = {2018}
}

@article{sheng2023towards,
 author = {Sheng, Dianmo and Chen, Dongdong and Tan, Zhentao and Liu, Qiankun and Chu, Qi and Bao, Jianmin and Gong, Tao and Liu, Bin and Xu, Shengwei and Yu, Nenghai},
 journal = {arXiv:2312.02520},
 title = {Towards More Unified In-context Visual Understanding},
 year = {2023}
}

@inproceedings{sidorov2020textcaps,
 author = {Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
 booktitle = {Proc. of ECCV},
 pages = {742--758},
 title = {Textcaps: a dataset for image captioning with reading comprehension},
 year = {2020}
}

@inproceedings{sidorov2020textcaps,
 author = {Sidorov, Oleksii and Hu, Ronghang and Rohrbach, Marcus and Singh, Amanpreet},
 booktitle = {Proc. of ECCV},
 pages = {742--758},
 title = {Textcaps: a dataset for image captioning with reading comprehension},
 year = {2020}
}

@inproceedings{singh2019towards,
 author = {Singh, Amanpreet and Natarajan, Vivek and Shah, Meet and Jiang, Yu and Chen, Xinlei and Batra, Dhruv and Parikh, Devi and Rohrbach, Marcus},
 booktitle = {Proc. of CVPR},
 pages = {8317--8326},
 title = {Towards vqa models that can read},
 year = {2019}
}

@article{sm,
 author = {Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
 journal = {arXiv:2204.00598},
 title = {Socratic models: Composing zero-shot multimodal reasoning with language},
 year = {2022}
}

@article{SMs,
 author = {Zeng, Andy and Wong, Adrian and Welker, Stefan and Choromanski, Krzysztof and Tombari, Federico and Purohit, Aveek and Ryoo, Michael and Sindhwani, Vikas and Lee, Johnny and Vanhoucke, Vincent and others},
 journal = {arXiv:2204.00598},
 title = {Socratic models: Composing zero-shot multimodal reasoning with language},
 year = {2022}
}

@inproceedings{song2021score,
 author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
 booktitle = {Proc. of ICLR},
 title = {{Score-Based Generative Modeling through Stochastic Differential Equations}},
 year = {2021}
}

@inproceedings{song2021score,
 author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
 booktitle = {Proc. of ICLR},
 title = {{Score-Based Generative Modeling through Stochastic Differential Equations}},
 year = {2021}
}

@article{song2023bridge,
 author = {Song, Shezheng and Li, Xiaopeng and Li, Shasha},
 journal = {arXiv preprint arXiv:2311.07594},
 title = {{How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model}},
 year = {2023}
}

@article{song2024mmac,
 author = {Song, Zirui and Li, Yaohang and Fang, Meng and Chen, Zhenhao and Shi, Zecheng and Huang, Yuan and Chen, Ling},
 journal = {arXiv preprint arXiv:2404.18074},
 title = {Mmac-copilot: Multi-modal agent collaboration operating system copilot},
 year = {2024}
}

@misc{stablelm2023,
 author = {{Stability AI}},
 title = {Introducing Stable LM 2},
 year = {2024}
}

@misc{step1v,
 author = {StepFun Research Team},
 title = {Step-1v: A hundred billion parameter multimodal large model.},
 year = {2024}
}

@article{stiennon2020learning,
 author = {Stiennon, Nisan and Ouyang, Long and Wu, Jeffrey and Ziegler, Daniel and Lowe, Ryan and Voss, Chelsea and Radford, Alec and Amodei, Dario and Christiano, Paul F},
 journal = {NeurIPS},
 title = {Learning to summarize with human feedback},
 year = {2020}
}

@article{su2023pandagpt,
 author = {Su, Yixuan and Lan, Tian and Li, Huayang and Xu, Jialu and Wang, Yan and Cai, Deng},
 journal = {arXiv preprint arXiv:2305.16355},
 title = {{Pandagpt: One model to instruction-follow them all}},
 year = {2023}
}

@article{sun2023aligning,
 author = {Sun, Zhiqing and Shen, Sheng and Cao, Shengcao and Liu, Haotian and Li, Chunyuan and Shen, Yikang and Gan, Chuang and Gui, Liang-Yan and Wang, Yu-Xiong and Yang, Yiming and others},
 journal = {arXiv:2309.14525},
 title = {Aligning large multimodal models with factually augmented rlhf},
 year = {2023}
}

@article{sun2023eva,
 author = {Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
 journal = {arXiv preprint arXiv:2303.15389},
 title = {Eva-clip: Improved training techniques for clip at scale},
 year = {2023}
}

@article{sun2023eva,
 author = {Sun, Quan and Fang, Yuxin and Wu, Ledell and Wang, Xinlong and Cao, Yue},
 journal = {arXiv:2303.15389},
 title = {Eva-clip: Improved training techniques for clip at scale},
 year = {2023}
}

@inproceedings{sun2023generative,
 author = {Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
 booktitle = {Proc. of ICLR},
 title = {{Generative pretraining in multimodality}},
 year = {2024}
}

@article{sun2023generative,
 author = {Sun, Quan and Yu, Qiying and Cui, Yufeng and Zhang, Fan and Zhang, Xiaosong and Wang, Yueze and Gao, Hongcheng and Liu, Jingjing and Huang, Tiejun and Wang, Xinlong},
 journal = {arXiv:2307.05222},
 title = {Generative pretraining in multimodality},
 year = {2023}
}

@article{suris2023vipergpt,
 author = {Sur{\'\i}s, D{\'\i}dac and Menon, Sachit and Vondrick, Carl},
 journal = {arXiv preprint arXiv:2303.08128},
 title = {{Vipergpt: Visual inference via python execution for reasoning}},
 year = {2023}
}

@article{tai2023link,
 author = {Tai, Yan and Fan, Weichen and Zhang, Zhao and Zhu, Feng and Zhao, Rui and Liu, Ziwei},
 journal = {arXiv:2308.07891},
 title = {Link-context learning for multimodal llms},
 year = {2023}
}

@inproceedings{tang2023codi,
 author = {Tang, Zineng and Yang, Ziyi and Zhu, Chenguang and Zeng, Michael and Bansal, Mohit},
 booktitle = {Proc. of NeurIPS},
 title = {{Any-to-Any Generation via Composable Diffusion}},
 year = {2023}
}

@article{tang2023codi2,
 author = {Tang, Zineng and Yang, Ziyi and Khademi, Mahmoud and Liu, Yang and Zhu, Chenguang and Bansal, Mohit},
 journal = {arXiv preprint arXiv:2311.18775},
 title = {{CoDi-2: In-Context, Interleaved, and Interactive Any-to-Any Generation}},
 year = {2023}
}

@article{tang2023salmonn,
 author = {Tang, Changli and Yu, Wenyi and Sun, Guangzhi and Chen, Xianzhao and Tan, Tian and Li, Wei and Lu, Lu and Ma, Zejun and Zhang, Chao},
 journal = {arXiv preprint arXiv:2310.13289},
 title = {{Salmonn: Towards generic hearing abilities for large language models}},
 year = {2023}
}

@inproceedings{tay2022ul2,
 author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Bahri, Dara and Schuster, Tal and Zheng, Steven and others},
 booktitle = {Proc. of ICLR},
 title = {{Ul2: Unifying language learning paradigms}},
 year = {2022}
}

@article{team2023gemini,
 author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
 journal = {arXiv preprint arXiv:2312.11805},
 title = {Gemini: a family of highly capable multimodal models},
 year = {2023}
}

@article{team2023gemini,
 author = {Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
 journal = {arXiv preprint arXiv:2312.11805},
 title = {{Gemini: a family of highly capable multimodal models}},
 year = {2023}
}

@misc{team2023internlm,
 author = {Team, InternLM},
 journal = {2023-01-06)[2023-09-27]. https://github. com/InternLM/InternLM},
 title = {{Internlm: A multilingual language model with progressively enhanced capabilities}},
 year = {2023}
}

@book{test,
 address = {Reading, MA},
 author = {Donald E. Knuth},
 publisher = {Addison-Wesley},
 title = {Seminumerical Algorithms},
 year = {1981}
}

@inproceedings{theiner2022,
 author = {Theiner, Jonas and M{\"u}ller-Budack, Eric and Ewerth, Ralph},
 booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
 pages = {750-760},
 title = {{Interpretable Semantic Photo Geolocation}},
 year = {2022}
}

@misc{tinyllava,
 author = {Zhou, Baichuan and Huang, Lei},
 title = {TinyLLaVA},
 year = {2024}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv preprint arXiv:2302.13971},
 title = {Llama: Open and efficient foundation language models},
 year = {2023}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv preprint arXiv:2302.13971},
 title = {{Llama: Open and efficient foundation language models}},
 year = {2023}
}

@article{touvron2023llama,
 author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
 journal = {arXiv preprint arXiv:2302.13971},
 title = {{Llama: Open and efficient foundation language models}},
 year = {2023}
}

@article{touvron2023llama2,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {arXiv preprint arXiv:2307.09288},
 title = {Llama 2: Open foundation and fine-tuned chat models},
 year = {2023}
}

@article{touvron2023llama2,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {arXiv preprint arXiv:2307.09288},
 title = {{Llama 2: Open foundation and fine-tuned chat models}},
 year = {2023}
}

@article{touvron2023llama2,
 author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
 journal = {arXiv preprint arXiv:2307.09288},
 title = {{Llama 2: Open foundation and fine-tuned chat models}},
 year = {2023}
}

@techreport{Tzamaloukas-01,
 author = {A. Tzamaloukas and J. J. Garcia-Luna-Aceves},
 title = {Channel-Hopping Multiple Access},
 year = {2000}
}

@article{udandarao2022sus,
 author = {Udandarao, Vishaal and Gupta, Ankush and Albanie, Samuel},
 journal = {arXiv:2211.16198},
 title = {Sus-x: Training-free name-only transfer of vision-language models},
 year = {2022}
}

@article{Ulip,
 author = {Salesforce},
 title = {{Ulip}},
 year = {2022}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 journal = {Proc. of NeurIPS},
 title = {Attention is all you need},
 year = {2017}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 journal = {Proc. of NeurIPS},
 title = {{Attention is all you need}},
 year = {2017}
}

@article{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
 journal = {NeurIPS},
 title = {Attention is all you need},
 year = {2017}
}

@article{vcot,
 author = {Rose, Daniel and Himakunthala, Vaishnavi and Ouyang, Andy and He, Ryan and Mei, Alex and Lu, Yujie and Saxon, Michael and Sonar, Chinmay and Mirza, Diba and Wang, William Yang},
 journal = {arXiv:2305.02317},
 title = {Visual Chain of Thought: Bridging Logical Gaps with Multimodal Infillings},
 year = {2023}
}

@inproceedings{vedantam2015cider,
 author = {Vedantam, Ramakrishna and Lawrence Zitnick, C and Parikh, Devi},
 booktitle = {CVPR},
 title = {Cider: Consensus-based image description evaluation},
 year = {2015}
}

@article{VG-Caps,
 author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
 journal = {IJCV},
 title = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
 year = {2017}
}

@misc{vicuna,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
 title = {Vicuna: An open-source chatbot impressing gpt-4 with 90\% chatgpt quality},
 year = {2023}
}

@misc{vicuna2023,
 author = {Chiang, Wei-Lin and Li, Zhuohan and Lin, Zi and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E. and Stoica, Ion and Xing, Eric P.},
 title = {{Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90\%* ChatGPT Quality}},
 year = {2023}
}

@article{video-chatgpt,
 author = {Maaz, Muhammad and Rasheed, Hanoona and Khan, Salman and Khan, Fahad Shahbaz},
 journal = {arXiv:2306.05424},
 title = {Video-ChatGPT: Towards Detailed Video Understanding via Large Vision and Language Models},
 year = {2023}
}

@article{video-llama,
 author = {Zhang, Hang and Li, Xin and Bing, Lidong},
 journal = {arXiv:2306.02858},
 title = {Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video Understanding},
 year = {2023}
}

@article{videochat,
 author = {Li, KunChang and He, Yinan and Wang, Yi and Li, Yizhuo and Wang, Wenhai and Luo, Ping and Wang, Yali and Wang, Limin and Qiao, Yu},
 journal = {arXiv:2305.06355},
 title = {Videochat: Chat-centric video understanding},
 year = {2023}
}

@article{vip,
 author = {Himakunthala, Vaishnavi and Ouyang, Andy and Rose, Daniel and He, Ryan and Mei, Alex and Lu, Yujie and Sonar, Chinmay and Saxon, Michael and Wang, William Yang},
 journal = {arXiv:2305.13903},
 title = {Let's Think Frame by Frame: Evaluating Video Chain of Thought with Video Infilling and Prediction},
 year = {2023}
}

@article{viscpm,
 author = {Jinyi Hu and Yuan Yao and Chongyi Wang and Shan Wang and Yinxu Pan and Qianyu Chen and Tianyu Yu and Hanghao Wu and Yue Zhao and Haoye Zhang and Xu Han and Yankai Lin and Jiao Xue and Dahai Li and Zhiyuan Liu and Maosong Sun},
 journal = {arXiv preprint arXiv:2308.12038},
 title = {Large Multilingual Models Pivot Zero-Shot Multimodal Learning across Languages},
 year = {2023}
}

@article{visionllm,
 author = {Wang, Wenhai and Chen, Zhe and Chen, Xiaokang and Wu, Jiannan and Zhu, Xizhou and Zeng, Gang and Luo, Ping and Lu, Tong and Zhou, Jie and Qiao, Yu and others},
 journal = {arXiv:2305.11175},
 title = {VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks},
 year = {2023}
}

@inproceedings{visprog,
 author = {Gupta, Tanmay and Kembhavi, Aniruddha},
 booktitle = {CVPR},
 title = {Visual programming: Compositional visual reasoning without training},
 year = {2023}
}

@article{visual-chatgpt,
 author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
 journal = {arXiv:2303.04671},
 title = {Visual chatgpt: Talking, drawing and editing with visual foundation models},
 year = {2023}
}

@article{vivanco2024geoclip,
 author = {Vivanco Cepeda, Vicente and Nayak, Gaurav Kumar and Shah, Mubarak},
 journal = {Proc. of NeurIPS},
 title = {Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization},
 year = {2024}
}

@inproceedings{vo2017,
 author = {Vo, Nam and Jacobs, Nathan and Hays, James},
 booktitle = {ICCV},
 pages = {2621-2630},
 title = {{Revisiting IM2GPS in the Deep Learning Era}},
 year = {2017}
}

@inproceedings{vo2017revisiting,
 author = {Vo, Nam and Jacobs, Nathan and Hays, James},
 booktitle = {Proc. of ICCV},
 pages = {2621--2630},
 title = {Revisiting im2gps in the deep learning era},
 year = {2017}
}

@article{wang2021simvlm,
 author = {Wang, Zirui and Yu, Jiahui and Yu, Adams Wei and Dai, Zihang and Tsvetkov, Yulia and Cao, Yuan},
 journal = {arXiv:2108.10904},
 title = {Simvlm: Simple visual language model pretraining with weak supervision},
 year = {2021}
}

@article{wang2022foundation,
 author = {Wang, Hongyu and Ma, Shuming and Huang, Shaohan and Dong, Li and Wang, Wenhui and Peng, Zhiliang and Wu, Yu and Bajaj, Payal and Singhal, Saksham and Benhaim, Alon and others},
 journal = {arXiv preprint arXiv:2210.06423},
 title = {{Foundation transformers}},
 year = {2022}
}

@article{wang2022image,
 author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
 journal = {arXiv preprint arXiv:2208.10442},
 title = {{Image as a foreign language: Beit pretraining for all vision and vision-language tasks}},
 year = {2022}
}

@misc{wang2022imageforeignlanguagebeit,
 archivePrefix = {arXiv},
 author = {Wenhui Wang and Hangbo Bao and Li Dong and Johan Bjorck and Zhiliang Peng and Qiang Liu and Kriti Aggarwal and Owais Khan Mohammed and Saksham Singhal and Subhojit Som and Furu Wei},
 eprint = {2208.10442},
 primaryClass = {cs.CV},
 title = {Image as a Foreign Language: BEiT Pretraining for All Vision and Vision-Language Tasks},
 url = {https://arxiv.org/abs/2208.10442},
 year = {2022}
}

@inproceedings{wang2022ofa,
 author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
 booktitle = {Proc. of ICML},
 pages = {23318--23340},
 title = {{Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework}},
 year = {2022}
}

@inproceedings{wang2022ofa,
 author = {Wang, Peng and Yang, An and Men, Rui and Lin, Junyang and Bai, Shuai and Li, Zhikang and Ma, Jianxin and Zhou, Chang and Zhou, Jingren and Yang, Hongxia},
 booktitle = {ICML},
 title = {Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework},
 year = {2022}
}

@article{wang2022self,
 author = {Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
 journal = {arXiv:2212.10560},
 title = {Self-Instruct: Aligning Language Model with Self Generated Instructions},
 year = {2022}
}

@article{wang2023all,
 author = {Wang, Weiyun and Shi, Min and Li, Qingyun and Wang, Wenhai and Huang, Zhenhang and Xing, Linjie and Chen, Zhe and Li, Hao and Zhu, Xizhou and Cao, Zhiguo and others},
 journal = {arXiv preprint arXiv:2308.01907},
 title = {{The all-seeing project: Towards panoptic visual recognition and understanding of the open world}},
 year = {2023}
}

@misc{wang2023cogvlm,
 archivePrefix = {arXiv},
 author = {Weihan Wang and Qingsong Lv and Wenmeng Yu and Wenyi Hong and Ji Qi and Yan Wang and Junhui Ji and Zhuoyi Yang and Lei Zhao and Xixuan Song and Jiazheng Xu and Bin Xu and Juanzi Li and Yuxiao Dong and Ming Ding and Jie Tang},
 eprint = {2311.03079},
 primaryClass = {cs.CV},
 title = {CogVLM: Visual Expert for Pretrained Language Models},
 year = {2023}
}

@article{wang2023cogvlm,
 author = {Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
 journal = {arXiv preprint arXiv:2311.03079},
 title = {{Cogvlm: Visual expert for pretrained language models}},
 year = {2023}
}

@article{wang2023cogvlm,
 author = {Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and Song, Xixuan and others},
 journal = {arXiv:2311.03079},
 title = {Cogvlm: Visual expert for pretrained language models},
 year = {2023}
}

@article{wang2023evaluation,
 author = {Wang, Junyang and Zhou, Yiyang and Xu, Guohai and Shi, Pengcheng and Zhao, Chenlin and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Zhang, Ji and Zhu, Jihua and others},
 journal = {arXiv:2308.15126},
 title = {Evaluation and analysis of hallucination in large vision-language models},
 year = {2023}
}

@inproceedings{wang2023image,
 author = {Wang, Wenhui and Bao, Hangbo and Dong, Li and Bjorck, Johan and Peng, Zhiliang and Liu, Qiang and Aggarwal, Kriti and Mohammed, Owais Khan and Singhal, Saksham and Som, Subhojit and others},
 booktitle = {Proc. of CVPR},
 pages = {19175--19186},
 title = {{Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks}},
 year = {2023}
}

@article{wang2023llm,
 author = {Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
 journal = {arXiv:2311.07397},
 title = {An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
 year = {2023}
}

@article{wang2023llm,
 author = {Wang, Junyang and Wang, Yuhang and Xu, Guohai and Zhang, Jing and Gu, Yukai and Jia, Haitao and Yan, Ming and Zhang, Ji and Sang, Jitao},
 journal = {arXiv preprint arXiv:2311.07397},
 title = {An llm-free multi-dimensional benchmark for mllms hallucination evaluation},
 year = {2023}
}

@article{wang2023see,
 author = {Wang, Junke and Meng, Lingchen and Weng, Zejia and He, Bo and Wu, Zuxuan and Jiang, Yu-Gang},
 journal = {arXiv:2311.07574},
 title = {To see is to believe: Prompting gpt-4v for better visual instruction tuning},
 year = {2023}
}

@article{wang2023vigc,
 author = {Wang, Bin and Wu, Fan and Han, Xiao and Peng, Jiahui and Zhong, Huaping and Zhang, Pan and Dong, Xiaoyi and Li, Weijia and Li, Wei and Wang, Jiaqi and others},
 journal = {arXiv:2308.12714},
 title = {Vigc: Visual instruction generation and correction},
 year = {2023}
}

@article{wang2024llm,
 author = {Wang, Jingwei and Zhu, Ziyue and Liu, Chunxiao and Li, Rong and Wu, Xin},
 journal = {PloS one},
 pages = {e0312240},
 title = {LLM-Enhanced multimodal detection of fake news},
 year = {2024}
}

@inproceedings{wang2024mitigating,
 author = {Wang, Lei and He, Jiabang and Li, Shenshen and Liu, Ning and Lim, Ee-Peng},
 booktitle = {International Conference on Multimedia Modeling},
 pages = {32--45},
 title = {Mitigating fine-grained hallucination by fine-tuning large vision-language models with caption rewrites},
 year = {2024}
}

@article{wang2024mobile,
 author = {Wang, Junyang and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Shen, Weizhou and Zhang, Ji and Huang, Fei and Sang, Jitao},
 journal = {arXiv:2401.16158},
 title = {Mobile-Agent: Autonomous multi-modal mobile device agent with visual perception},
 year = {2024}
}

@article{wang2024modaverse,
 author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
 journal = {arXiv preprint arXiv:2401.06395},
 title = {{ModaVerse: Efficiently Transforming Modalities with LLMs}},
 year = {2024}
}

@article{wang2024modaverse,
 author = {Wang, Xinyu and Zhuang, Bohan and Wu, Qi},
 journal = {arXiv:2401.06395},
 title = {ModaVerse: Efficiently Transforming Modalities with LLMs},
 year = {2024}
}

@article{wavcaps,
 author = {Mei, Xinhao and Meng, Chutong and Liu, Haohe and Kong, Qiuqiang and Ko, Tom and Zhao, Chengqi and Plumbley, Mark D and Zou, Yuexian and Wang, Wenwu},
 journal = {arXiv:2303.17395},
 title = {Wavcaps: A chatgpt-assisted weakly-labelled audio captioning dataset for audio-language multimodal research},
 year = {2023}
}

@inproceedings{webvid10m,
 author = {Bain, Max and Nagrani, Arsha and Varol, G{\"u}l and Zisserman, Andrew},
 booktitle = {ICCV},
 title = {Frozen in time: A joint video and image encoder for end-to-end retrieval},
 year = {2021}
}

@inproceedings{wei2021finetuned,
 author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
 booktitle = {Proc. of ICLR},
 title = {{Finetuned Language Models are Zero-Shot Learners}},
 year = {2021}
}

@article{wei2021finetuned,
 author = {Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
 journal = {arXiv:2109.01652},
 title = {Finetuned language models are zero-shot learners},
 year = {2021}
}

@article{wei2022,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
 journal = {NIPS},
 pages = {24824-24837},
 title = {{Chain-of-Thought Prompting Elicits Reasoning in Large Language Models}},
 year = {2022}
}

@article{wei2022chain,
 author = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
 journal = {Proc. of NeurIPS},
 pages = {24824--24837},
 title = {Chain-of-thought prompting elicits reasoning in large language models},
 year = {2022}
}

@article{wei2022emergent,
 author = {Wei, Jason and Tay, Yi and Bommasani, Rishi and Raffel, Colin and Zoph, Barret and Borgeaud, Sebastian and Yogatama, Dani and Bosma, Maarten and Zhou, Denny and Metzler, Donald and others},
 journal = {arXiv:2206.07682},
 title = {Emergent abilities of large language models},
 year = {2022}
}

@article{wei2023instructiongpt,
 author = {Wei, Lai and Jiang, Zihao and Huang, Weiran and Sun, Lichao},
 journal = {arXiv:2308.12067},
 title = {Instructiongpt-4: A 200-instruction paradigm for fine-tuning minigpt-4},
 year = {2023}
}

@article{wei2024small,
 author = {Wei, Haoran and Kong, Lingyu and Chen, Jinyue and Zhao, Liang and Ge, Zheng and Yu, En and Sun, Jianjian and Han, Chunrui and Zhang, Xiangyu},
 journal = {arXiv preprint arXiv:2401.12503},
 title = {Small Language Model Meets with Reinforced Vision Vocabulary},
 year = {2024}
}

@article{wen2023road,
 author = {Wen, Licheng and Yang, Xuemeng and Fu, Daocheng and Wang, Xiaofeng and Cai, Pinlong and Li, Xin and Ma, Tao and Li, Yingxuan and Xu, Linran and Shang, Dengke and others},
 journal = {arXiv:2311.05332},
 title = {On the road with GPT-4V (ision): Early explorations of visual-language model on autonomous driving}
}

@incollection{Wenzel:1992:TVA:146022.146089,
 author = {Wenzel, Elizabeth M.},
 booktitle = {Multimedia interface design (incoll)},
 pages = {257--288},
 title = {Three-dimensional virtual acoustic displays},
 year = {1992}
}

@inproceedings{weyand2016,
 author = {Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
 booktitle = {Proc. of ECCV},
 pages = {37-55},
 title = {{PlaNet - Photo Geolocation with Convolutional Neural Networks}},
 year = {2016}
}

@inproceedings{weyand2016planet,
 author = {Weyand, Tobias and Kostrikov, Ilya and Philbin, James},
 booktitle = {Proc. of ECCV},
 pages = {37--55},
 title = {Planet-photo geolocation with convolutional neural networks},
 year = {2016}
}

@inproceedings{workman2015wide,
 author = {Workman, Scott and Souvenir, Richard and Jacobs, Nathan},
 booktitle = {Proc. of ICCV},
 pages = {3961--3969},
 title = {Wide-area image geolocalization with aerial reference imagery},
 year = {2015}
}

@article{wu2017ai,
 author = {Wu, Jiahong and Zheng, He and Zhao, Bo and Li, Yixin and Yan, Baoming and Liang, Rui and Wang, Wenjia and Zhou, Shipei and Lin, Guosen and Fu, Yanwei and others},
 journal = {arXiv preprint arXiv:1711.06475},
 title = {Ai challenger: A large-scale dataset for going deeper in image understanding},
 year = {2017}
}

@inproceedings{wu2023large,
 author = {Wu, Yusong and Chen, Ke and Zhang, Tianyu and Hui, Yuchen and Berg-Kirkpatrick, Taylor and Dubnov, Shlomo},
 booktitle = {Proc. of ICASSP},
 pages = {1--5},
 title = {{Large-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation}},
 year = {2023}
}

@article{wu2023multimodal,
 author = {Wu, Jiayang and Gan, Wensheng and Chen, Zefeng and Wan, Shicheng and Yu, Philip S},
 journal = {arXiv preprint arXiv:2311.13165},
 title = {{Multimodal large language models: A survey}},
 year = {2023}
}

@article{wu2023next,
 author = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
 journal = {arXiv preprint arXiv:2309.05519},
 title = {{Next-gpt: Any-to-any multimodal llm}},
 year = {2023}
}

@article{wu2023next,
 author = {Wu, Shengqiong and Fei, Hao and Qu, Leigang and Ji, Wei and Chua, Tat-Seng},
 journal = {arXiv:2309.05519},
 title = {Next-gpt: Any-to-any multimodal llm},
 year = {2023}
}

@article{wu2023q,
 author = {Wu, Haoning and Zhang, Zicheng and Zhang, Erli and Chen, Chaofeng and Liao, Liang and Wang, Annan and Li, Chunyi and Sun, Wenxiu and Yan, Qiong and Zhai, Guangtao and others},
 journal = {arXiv preprint arXiv:2309.14181},
 title = {Q-bench: A benchmark for general-purpose foundation models on low-level vision},
 year = {2023}
}

@article{wu2023textit,
 author = {Wu, Penghao and Xie, Saining},
 journal = {arXiv:2312.14135},
 title = {V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs},
 year = {2023}
}

@article{Wu2023The,
 author = {Yifan Wu and Pengchuan Zhang and Wenhan Xiong and Barlas Ouz and James C. Gee and Yixin Nie},
 journal = {ArXiv},
 title = {The Role of Chain-of-Thought in Complex Vision-Language Reasoning Task},
 year = {2023}
}

@article{wu2023visual,
 author = {Wu, Chenfei and Yin, Shengming and Qi, Weizhen and Wang, Xiaodong and Tang, Zecheng and Duan, Nan},
 journal = {arXiv preprint arXiv:2303.04671},
 title = {{Visual chatgpt: Talking, drawing and editing with visual foundation models}},
 year = {2023}
}

@misc{wu2024largescalecontrastivelanguageaudiopretraining,
 archivePrefix = {arXiv},
 author = {Yusong Wu and Ke Chen and Tianyu Zhang and Yuchen Hui and Marianna Nezhurina and Taylor Berg-Kirkpatrick and Shlomo Dubnov},
 eprint = {2211.06687},
 primaryClass = {cs.SD},
 title = {Large-scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation},
 url = {https://arxiv.org/abs/2211.06687},
 year = {2024}
}

@article{wukong,
 author = {Gu, Jiaxi and Meng, Xiaojun and Lu, Guansong and Hou, Lu and Minzhe, Niu and Liang, Xiaodan and Yao, Lewei and Huang, Runhui and Zhang, Wei and Jiang, Xin and others},
 journal = {NeurIPS},
 title = {Wukong: A 100 million large-scale chinese cross-modal pre-training benchmark},
 year = {2022}
}

@article{x-llm,
 author = {Chen, Feilong and Han, Minglun and Zhao, Haozhi and Zhang, Qingyang and Shi, Jing and Xu, Shuang and Xu, Bo},
 journal = {arXiv:2305.04160},
 title = {X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages},
 year = {2023}
}

@article{xie2019visual,
 author = {Xie, Ning and Lai, Farley and Doran, Derek and Kadav, Asim},
 journal = {arXiv:1901.06706},
 title = {Visual entailment: A novel task for fine-grained image understanding},
 year = {2019}
}

@inproceedings{xie2024integrating,
 author = {Xie, Anbin and Zhu, Fuqing and Han, Jizhong and Hu, Songlin},
 booktitle = {2024 27th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
 pages = {1917--1922},
 title = {Integrating Open-domain Knowledge via Large Language Model for Multimodal Fake News Detection},
 year = {2024}
}

@misc{Xtuner,
 author = {XTuner Contributors},
 title = {Xtuner: A toolkit for efficiently fine-tuning llm.}
}

@inproceedings{xu2016msr,
 author = {Xu, Jun and Mei, Tao and Yao, Ting and Rui, Yong},
 booktitle = {Proc. of CVPR},
 pages = {5288--5296},
 title = {Msr-vtt: A large video description dataset for bridging video and language},
 year = {2016}
}

@article{xu2023lvlm,
 author = {Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
 journal = {arXiv preprint arXiv:2306.09265},
 title = {{LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models}},
 year = {2023}
}

@article{xu2023lvlm,
 author = {Xu, Peng and Shao, Wenqi and Zhang, Kaipeng and Gao, Peng and Liu, Shuo and Lei, Meng and Meng, Fanqing and Huang, Siyuan and Qiao, Yu and Luo, Ping},
 journal = {arXiv:2306.09265},
 title = {LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models},
 year = {2023}
}

@article{xu2023pointllm,
 author = {Xu, Runsen and Wang, Xiaolong and Wang, Tai and Chen, Yilun and Pang, Jiangmiao and Lin, Dahua},
 journal = {arXiv:2308.16911},
 title = {Pointllm: Empowering large language models to understand point clouds},
 year = {2023}
}

@inproceedings{xu2023wizardlm,
 author = {Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Lin, Qingwei and Jiang, Daxin},
 booktitle = {Proc. of ICLR},
 title = {WizardLM: Empowering large pre-trained language models to follow complex instructions},
 year = {2023}
}

@article{xu2024vision,
 author = {Xu, Zhiyang and Feng, Chao and Shao, Rulin and Ashby, Trevor and Shen, Ying and Jin, Di and Cheng, Yu and Wang, Qifan and Huang, Lifu},
 journal = {arXiv:2402.11690},
 title = {Vision-Flan: Scaling Human-Labeled Tasks in Visual Instruction Tuning},
 year = {2024}
}

@article{xuan2023pink,
 author = {Xuan, Shiyu and Guo, Qingpei and Yang, Ming and Zhang, Shiliang},
 journal = {arXiv:2310.00582},
 title = {Pink: Unveiling the power of referential comprehension for multi-modal llms},
 year = {2023}
}

@misc{xue2023ulip2,
 archivePrefix = {arXiv},
 author = {Le Xue and Ning Yu and Shu Zhang and Junnan Li and Roberto Martn-Martn and Jiajun Wu and Caiming Xiong and Ran Xu and Juan Carlos Niebles and Silvio Savarese},
 eprint = {2305.08275},
 primaryClass = {cs.CV},
 title = {ULIP-2: Towards Scalable Multimodal Pre-training for 3D Understanding},
 year = {2023}
}

@article{yan2021video,
 author = {Yan, Rui and Shou, Mike Zheng and Ge, Yixiao and Wang, Alex Jinpeng and Lin, Xudong and Cai, Guanyu and Tang, Jinhui},
 journal = {arXiv preprint arXiv:2112.01194},
 title = {{Video-text pre-training with learned regions}},
 year = {2021}
}

@article{yan2024vigor,
 author = {Yan, Siming and Bai, Min and Chen, Weifeng and Zhou, Xiong and Huang, Qixing and Li, Li Erran},
 journal = {arXiv preprint arXiv:2402.06118},
 title = {{ViGoR: Improving Visual Grounding of Large Vision Language Models with Fine-Grained Reward Modeling}},
 year = {2024}
}

@inproceedings{yang2022vision,
 author = {Yang, Jinyu and Duan, Jiali and Tran, Son and Xu, Yi and Chanda, Sampath and Chen, Liqun and Zeng, Belinda and Chilimbi, Trishul and Huang, Junzhou},
 booktitle = {Proc. of CVPR},
 pages = {15671--15680},
 title = {{Vision-language pre-training with triple contrastive learning}},
 year = {2022}
}

@article{yang2023appagent,
 author = {Yang, Zhao and Liu, Jiaxuan and Han, Yucheng and Chen, Xin and Huang, Zebiao and Fu, Bin and Yu, Gang},
 journal = {arXiv:2312.13771},
 title = {Appagent: Multimodal agents as smartphone users},
 year = {2023}
}

@article{yang2023dawn,
 author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
 journal = {arXiv:2309.17421},
 title = {The dawn of lmms: Preliminary explorations with gpt-4v (ision)}
}

@article{yang2023mm,
 author = {Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Azarnasab, Ehsan and Ahmed, Faisal and Liu, Zicheng and Liu, Ce and Zeng, Michael and Wang, Lijuan},
 journal = {arXiv preprint arXiv:2303.11381},
 title = {{Mm-react: Prompting chatgpt for multimodal reasoning and action}},
 year = {2023}
}

@article{yang2023teal,
 author = {Yang, Zhen and Zhang, Yingxue and Meng, Fandong and Zhou, Jie},
 journal = {arXiv preprint arXiv:2311.04589},
 title = {{TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models}},
 year = {2023}
}

@article{yao2023tree,
 author = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
 journal = {NIPS},
 title = {{Tree of Thoughts: Deliberate Problem Solving with Large Language Models}},
 year = {2024}
}

@article{ye2019,
 author = {Ye, Yu and Richards, Daniel and Lu, Yi and Song, Xiaoping and Zhuang, Yu and Zeng, Wei and Zhong, Teng},
 journal = {Landscape and Urban Planning},
 pages = {103434},
 title = {Measuring daily accessed street greenery: A human-scale approach for informing better urban planning practices},
 year = {2019}
}

@article{ye2023mplug,
 author = {Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Dan, Yuhao and Zhao, Chenlin and Xu, Guohai and Li, Chenliang and Tian, Junfeng and others},
 journal = {arXiv:2307.02499},
 title = {mplug-docowl: Modularized multimodal large language model for document understanding},
 year = {2023}
}

@article{ye2023mplug2,
 author = {Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
 journal = {arXiv preprint arXiv:2311.04257},
 title = {{mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration}},
 year = {2023}
}

@article{ye2023mplugdoc,
 author = {Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Dan, Yuhao and Zhao, Chenlin and Xu, Guohai and Li, Chenliang and Tian, Junfeng and others},
 journal = {arXiv preprint arXiv:2307.02499},
 title = {{mplug-docowl: Modularized multimodal large language model for document understanding}},
 year = {2023}
}

@article{ye2023mplugowl2,
 author = {Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei and Zhou, Jingren},
 journal = {arXiv preprint arXiv:2311.04257},
 title = {mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
 year = {2023}
}

@inproceedings{ye2023ureader,
 author = {Ye, Jiabo and Hu, Anwen and Xu, Haiyang and Ye, Qinghao and Yan, Ming and Xu, Guohai and Li, Chenliang and Tian, Junfeng and Qian, Qi and Zhang, Ji and others},
 booktitle = {EMNLP},
 title = {Ureader: Universal ocr-free visually-situated language understanding with multimodal large language model},
 year = {2023}
}

@article{ye_2019_visual,
 author = {Ye, Yu and Zeng, Wei and Shen, Qiaomu and Zhang, Xiaohu and Lu, Yi},
 journal = {Environment and Planning B: Urban Analytics and City Science},
 pages = {1439-1457},
 title = {The visual quality of streets: A human-centred continuous measurement based on machine learning algorithms and street view images},
 year = {2019}
}

@inproceedings{YFCC-Val26k-Interpretable,
 author = {Theiner, Jonas and M{\"u}ller-Budack, Eric and Ewerth, Ralph},
 booktitle = {WACV},
 title = {Interpretable semantic photo geolocation},
 year = {2022}
}

@misc{yfccwebsite,
 howpublished = {\url{{https://gitlab.com/jfolz/yfcc100m}}},
 note = {Accessed: 2023-10-10},
 title = {YFCC100m}
}

@article{yin2023lamm,
 author = {Yin, Zhenfei and Wang, Jiong and Cao, Jianjian and Shi, Zhelun and Liu, Dingning and Li, Mukai and Sheng, Lu and Bai, Lei and Huang, Xiaoshui and Wang, Zhiyong and others},
 journal = {arXiv preprint arXiv:2306.06687},
 title = {LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark},
 year = {2023}
}

@article{yin2023survey,
 author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Li, Ke and Sun, Xing and Xu, Tong and Chen, Enhong},
 journal = {arXiv preprint arXiv:2306.13549},
 title = {{A Survey on Multimodal Large Language Models}},
 year = {2023}
}

@article{yin2023woodpecker,
 author = {Yin, Shukang and Fu, Chaoyou and Zhao, Sirui and Xu, Tong and Wang, Hao and Sui, Dianbo and Shen, Yunhang and Li, Ke and Sun, Xing and Chen, Enhong},
 journal = {arXiv:2310.16045},
 title = {Woodpecker: Hallucination correction for multimodal large language models},
 year = {2023}
}

@article{ying2024mmt,
 author = {Ying, Kaining and Meng, Fanqing and Wang, Jin and Li, Zhiqian and Lin, Han and Yang, Yue and Zhang, Hao and Zhang, Wenbo and Lin, Yuqi and Liu, Shuo and others},
 journal = {arXiv preprint arXiv:2404.16006},
 title = {{MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI}},
 year = {2024}
}

@article{you2023ferret,
 author = {You, Haoxuan and Zhang, Haotian and Gan, Zhe and Du, Xianzhi and Zhang, Bowen and Wang, Zirui and Cao, Liangliang and Chang, Shih-Fu and Yang, Yinfei},
 journal = {arXiv:2310.07704},
 title = {Ferret: Refer and ground anything anywhere at any granularity},
 year = {2023}
}

@article{youku-mplug,
 author = {Haiyang Xu and Qinghao Ye and Xuan Wu and Ming Yan and Yuan Miao and Jiabo Ye and Guohai Xu and Anwen Hu and Yaya Shi and Guangwei Xu and Chenliang Li and Qi Qian and Maofei Que and Ji Zhang and Xiao Zeng and Fei Huang},
 journal = {arXiv:2306.04362},
 title = {Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language Dataset for Pre-training and Benchmarks},
 year = {2023}
}

@article{young2014image,
 author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
 journal = {TACL},
 pages = {67--78},
 title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
 year = {2014}
}

@article{young2014image,
 author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
 journal = {TACL},
 pages = {67--78},
 title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
 year = {2014}
}

@article{young2014image,
 author = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
 journal = {TACL},
 title = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
 year = {2014}
}

@inproceedings{yu2016modeling,
 author = {Yu, Licheng and Poirson, Patrick and Yang, Shan and Berg, Alexander C and Berg, Tamara L},
 booktitle = {Proc. of ECCV},
 pages = {69--85},
 title = {Modeling context in referring expressions},
 year = {2016}
}

@inproceedings{yu2019deep,
 author = {Yu, Zhou and Yu, Jun and Cui, Yuhao and Tao, Dacheng and Tian, Qi},
 booktitle = {CVPR},
 title = {Deep modular co-attention networks for visual question answering},
 year = {2019}
}

@inproceedings{yu2022point,
 author = {Yu, Xumin and Tang, Lulu and Rao, Yongming and Huang, Tiejun and Zhou, Jie and Lu, Jiwen},
 booktitle = {Proc. of CVPR},
 pages = {19313--19322},
 title = {{Point-bert: Pre-training 3d point cloud transformers with masked point modeling}},
 year = {2022}
}

@article{yu2023hallucidoctor,
 author = {Yu, Qifan and Li, Juncheng and Wei, Longhui and Pang, Liang and Ye, Wentao and Qin, Bosheng and Tang, Siliang and Tian, Qi and Zhuang, Yueting},
 journal = {arXiv preprint arXiv:2311.13614},
 title = {Hallucidoctor: Mitigating hallucinatory toxicity in visual instruction data},
 year = {2023}
}

@article{yu2023mm,
 author = {Yu, Weihao and Yang, Zhengyuan and Li, Linjie and Wang, Jianfeng and Lin, Kevin and Liu, Zicheng and Wang, Xinchao and Wang, Lijuan},
 journal = {arXiv preprint arXiv:2308.02490},
 title = {Mm-vet: Evaluating large multimodal models for integrated capabilities},
 year = {2023}
}

@article{yu2023rlhf,
 author = {Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
 journal = {arXiv preprint arXiv:2312.00849},
 title = {{Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback}},
 year = {2023}
}

@article{yu2023rlhf,
 author = {Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
 journal = {arXiv:2312.00849},
 title = {RLHF-V: Towards Trustworthy MLLMs via Behavior Alignment from Fine-grained Correctional Human Feedback},
 year = {2023}
}

@article{yu2023scaling,
 author = {Yu, Lili and Shi, Bowen and Pasunuru, Ramakanth and Muller, Benjamin and Golovneva, Olga and Wang, Tianlu and Babu, Arun and Tang, Binh and Karrer, Brian and Sheynin, Shelly and others},
 journal = {arXiv preprint arXiv:2309.02591},
 title = {{Scaling autoregressive multi-modal models: Pretraining and instruction tuning}},
 year = {2023}
}

@inproceedings{yu2024rlhf,
 author = {Yu, Tianyu and Yao, Yuan and Zhang, Haoye and He, Taiwen and Han, Yifeng and Cui, Ganqu and Hu, Jinyi and Liu, Zhiyuan and Zheng, Hai-Tao and Sun, Maosong and others},
 booktitle = {Proc. of CVPR},
 pages = {13807--13816},
 title = {Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional human feedback},
 year = {2024}
}

@article{yuan2023osprey,
 author = {Yuan, Yuqian and Li, Wentong and Liu, Jian and Tang, Dongqi and Luo, Xinjie and Qin, Chi and Zhang, Lei and Zhu, Jianke},
 journal = {arXiv preprint arXiv:2312.10032},
 title = {{Osprey: Pixel Understanding with Visual Instruction Tuning}},
 year = {2023}
}

@article{yuan2023osprey,
 author = {Yuan, Yuqian and Li, Wentong and Liu, Jian and Tang, Dongqi and Luo, Xinjie and Qin, Chi and Zhang, Lei and Zhu, Jianke},
 journal = {arXiv:2312.10032},
 title = {Osprey: Pixel Understanding with Visual Instruction Tuning}
}

@article{yuan2023tinygpt,
 author = {Yuan, Zhengqing and Li, Zhaoxu and Sun, Lichao},
 journal = {arXiv preprint arXiv:2312.16862},
 title = {TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones},
 year = {2023}
}

@article{yue2023mmmu,
 author = {Xiang Yue and Yuansheng Ni and Kai Zhang and Tianyu Zheng and Ruoqi Liu and Ge Zhang and Samuel Stevens and Dongfu Jiang and Weiming Ren and Yuxuan Sun and Cong Wei and Botao Yu and Ruibin Yuan and Renliang Sun and Ming Yin and Boyuan Zheng and Zhenzhu Yang and Yibo Liu and Wenhao Huang and Huan Sun and Yu Su and Wenhu Chen},
 journal = {arXiv preprint arXiv:2311.16502},
 title = {MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI},
 year = {2023}
}

@article{yue2024less,
 author = {Yue, Zihao and Zhang, Liang and Jin, Qin},
 journal = {arXiv preprint arXiv:2402.14545},
 title = {Less is More: Mitigating Multimodal Hallucination from an EOS Decision Perspective},
 year = {2024}
}

@article{zamir2014image,
 author = {Zamir, Amir Roshan and Shah, Mubarak},
 journal = {IEEE transactions on pattern analysis and machine intelligence},
 pages = {1546--1558},
 title = {Image geo-localization based on multiplenearest neighbor feature matching usinggeneralized graphs},
 year = {2014}
}

@inproceedings{zellers2019recognition,
 author = {Zellers, Rowan and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
 booktitle = {CVPR},
 title = {From recognition to cognition: Visual commonsense reasoning},
 year = {2019}
}

@inproceedings{zellers2022merlot,
 author = {Zellers, Rowan and Lu, Jiasen and Lu, Ximing and Yu, Youngjae and Zhao, Yanpeng and Salehi, Mohammadreza and Kusupati, Aditya and Hessel, Jack and Farhadi, Ali and Choi, Yejin},
 booktitle = {Proc. of CVPR},
 pages = {16375--16387},
 title = {{Merlot reserve: Neural script knowledge through vision and language and sound}},
 year = {2022}
}

@inproceedings{zeng2022glm,
 author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
 booktitle = {Proc. of ICLR},
 title = {{GLM-130B: An Open Bilingual Pre-trained Model}},
 year = {2022}
}

@inproceedings{zeng2022glm,
 author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and others},
 booktitle = {Proc. of ICLR},
 title = {{GLM-130B: An Open Bilingual Pre-trained Model}},
 year = {2022}
}

@inproceedings{zeng2022multi,
 author = {Zeng, Yan and Zhang, Xinsong and Li, Hang},
 booktitle = {Proc. of ICML},
 pages = {25994--26009},
 title = {{Multi-Grained Vision Language Pre-Training: Aligning Texts with Visual Concepts}},
 year = {2022}
}

@article{zeng2023matters,
 author = {Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
 journal = {arXiv preprint arXiv:2307.02469},
 title = {{What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?}},
 year = {2023}
}

@article{zeng2023matters,
 author = {Zeng, Yan and Zhang, Hanbo and Zheng, Jiani and Xia, Jiangnan and Wei, Guoqiang and Wei, Yang and Zhang, Yuchen and Kong, Tao},
 journal = {arXiv:2307.02469},
 title = {What Matters in Training a GPT4-Style Language Model with Multimodal Inputs?},
 year = {2023}
}

@article{zhai2023halle,
 author = {Zhai, Bohan and Yang, Shijia and Zhao, Xiangchen and Xu, Chenfeng and Shen, Sheng and Zhao, Dongdi and Keutzer, Kurt and Li, Manling and Yan, Tan and Fan, Xiangjun},
 journal = {arXiv:2310.01779},
 title = {HallE-Switch: Rethinking and Controlling Object Existence Hallucinations in Large Vision Language Models for Detailed Caption},
 year = {2023}
}

@inproceedings{zhai2023sigmoid,
 author = {Zhai, Xiaohua and Mustafa, Basil and Kolesnikov, Alexander and Beyer, Lucas},
 booktitle = {Proc. of ICCV},
 pages = {11975-11986},
 title = {Sigmoid Loss for Language Image Pre-Training},
 year = {2023}
}

@article{zhan2024anygpt,
 author = {Zhan, Jun and Dai, Junqi and Ye, Jiasheng and Zhou, Yunhua and Zhang, Dong and Liu, Zhigeng and Zhang, Xin and Yuan, Ruibin and Zhang, Ge and Li, Linyang and others},
 journal = {arXiv:2402.12226},
 title = {AnyGPT: Unified Multimodal LLM with Discrete Sequence Modeling},
 year = {2024}
}

@article{zhang2018,
 author = {Zhang, Fan and Zhang, Ding and Liu, Yu and Lin, Hui},
 journal = {Computers, Environment and Urban Systems},
 pages = {153-164},
 title = {Representing place locales using scene elements},
 year = {2018}
}

@inproceedings{zhang2020side,
 author = {Zhang, Jeffrey O and Sax, Alexander and Zamir, Amir and Guibas, Leonidas and Malik, Jitendra},
 booktitle = {Proc. of ECCV},
 pages = {698--714},
 title = {{Side-tuning: a baseline for network adaptation via additive side networks}},
 year = {2020}
}

@article{zhang2022automatic,
 author = {Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
 journal = {arXiv:2210.03493},
 title = {Automatic chain of thought prompting in large language models},
 year = {2022}
}

@inproceedings{zhang2022dino,
 author = {Zhang, Hao and Li, Feng and Liu, Shilong and Zhang, Lei and Su, Hang and Zhu, Jun and Ni, Lionel and Shum, Heung-Yeung},
 booktitle = {Proc. of ICLR},
 title = {{DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}},
 year = {2022}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {arXiv preprint arXiv:2205.01068},
 title = {{Opt: Open pre-trained transformer language models}},
 year = {2022}
}

@article{zhang2022opt,
 author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and others},
 journal = {arXiv preprint arXiv:2205.01068},
 title = {{Opt: Open pre-trained transformer language models}},
 year = {2022}
}

@inproceedings{zhang2023,
 author = {Zhang, Xiaohan and Li, Xingyu and Sultani, Waqas and Zhou, Yi and Wshah, Safwan},
 booktitle = {Proc. of AAAI},
 pages = {3480-3488},
 title = {{Cross-View Geo-Localization via Learning Disentangled Geometric Layout Correspondence}},
 year = {2023}
}

@inproceedings{zhang2023autostl,
 author = {Zhang, Zijian and Zhao, Xiangyu and Miao, Hao and Zhang, Chunxu and Zhao, Hongwei and Zhang, Junbo},
 booktitle = {Proc. of AAAI},
 pages = {4902--4910},
 title = {Autostl: Automated spatio-temporal multi-task learning},
 year = {2023}
}

@inproceedings{zhang2023continual,
 author = {Zhang, Duzhen and Cong, Wei and Dong, Jiahua and Yu, Yahan and Chen, Xiuyi and Zhang, Yonggang and Fang, Zhen},
 booktitle = {Proc. of EMNLP},
 title = {{Continual Named Entity Recognition without Catastrophic Forgetting}},
 year = {2023}
}

@inproceedings{zhang2023decomposing,
 author = {Zhang, Duzhen and Yu, Yahan and Chen, Feilong and Chen, Xiuyi},
 booktitle = {Proc. of SIGIR},
 pages = {1919--1923},
 title = {{Decomposing Logits Distillation for Incremental Named Entity Recognition}},
 year = {2023}
}

@article{zhang2023gpt4roi,
 author = {Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
 journal = {arXiv preprint arXiv:2307.03601},
 title = {{Gpt4roi: Instruction tuning large language model on region-of-interest}},
 year = {2023}
}

@article{zhang2023gpt4roi,
 author = {Zhang, Shilong and Sun, Peize and Chen, Shoufa and Xiao, Min and Shao, Wenqi and Zhang, Wenwei and Chen, Kai and Luo, Ping},
 journal = {arXiv:2307.03601},
 title = {Gpt4roi: Instruction tuning large language model on region-of-interest},
 year = {2023}
}

@article{zhang2023internlm,
 author = {Zhang, Pan and Wang, Xiaoyi Dong Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Ding, Shuangrui and Zhang, Songyang and Duan, Haodong and Yan, Hang and others},
 journal = {arXiv preprint arXiv:2309.15112},
 title = {{Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition}},
 year = {2023}
}

@article{zhang2023llavar,
 author = {Zhang, Yanzhe and Zhang, Ruiyi and Gu, Jiuxiang and Zhou, Yufan and Lipka, Nedim and Yang, Diyi and Sun, Tong},
 journal = {arXiv preprint arXiv:2306.17107},
 title = {{Llavar: Enhanced visual instruction tuning for text-rich image understanding}},
 year = {2023}
}

@inproceedings{zhang2023mlpst,
 author = {Zhang, Zijian and Huang, Ze and Hu, Zhiwei and Zhao, Xiangyu and Wang, Wanyu and Liu, Zitao and Zhang, Junbo and Qin, S Joe and Zhao, Hongwei},
 booktitle = {Proc. of CIKM},
 pages = {3381--3390},
 title = {MLPST: MLP is All You Need for Spatio-Temporal Prediction},
 year = {2023}
}

@article{Zhang2023Multimodal,
 author = {Zhuosheng Zhang and Aston Zhang and Mu Li and Hai Zhao and G. Karypis and Alexander J. Smola},
 journal = {Trans. Mach. Learn. Res.},
 title = {Multimodal Chain-of-Thought Reasoning in Language Models},
 year = {2023}
}

@inproceedings{zhang2023promptst,
 author = {Zhang, Zijian and Zhao, Xiangyu and Liu, Qidong and Zhang, Chunxu and Ma, Qian and Wang, Wanyu and Zhao, Hongwei and Wang, Yiqi and Liu, Zitao},
 booktitle = {Proc. of CIKM},
 pages = {3195--3205},
 title = {Promptst: Prompt-enhanced spatio-temporal multi-attribute prediction},
 year = {2023}
}

@article{zhang2023recognize,
 author = {Zhang, Youcai and Huang, Xinyu and Ma, Jinyu and Li, Zhaoyang and Luo, Zhaochuan and Xie, Yanchun and Qin, Yuzhuo and Luo, Tong and Li, Yaqian and Liu, Shilong and others},
 journal = {arXiv preprint arXiv:2306.03514},
 title = {{Recognize Anything: A Strong Image Tagging Model}},
 year = {2023}
}

@article{zhang2023speechgpt,
 author = {Zhang, Dong and Li, Shimin and Zhang, Xin and Zhan, Jun and Wang, Pengyu and Zhou, Yaqian and Qiu, Xipeng},
 journal = {arXiv:2305.11000},
 title = {Speechgpt: Empowering large language models with intrinsic cross-modal conversational abilities},
 year = {2023}
}

@inproceedings{zhang2023task,
 author = {Zhang, Duzhen and Li, Hongliu and Cong, Wei and Xu, Rongtao and Dong, Jiahua and Chen, Xiuyi},
 booktitle = {Proc. of CIKM},
 pages = {3319--3329},
 title = {{Task relation distillation and prototypical pseudo label for incremental named entity recognition}},
 year = {2023}
}

@article{zhang2023text,
 author = {Zhang, Hanqing and Song, Haolin and Li, Shaoyu and Zhou, Ming and Song, Dawei},
 journal = {ACM Computing Surveys},
 pages = {1-37},
 title = {{A Survey of Controllable Text Generation Using Transformer-based Pre-trained Language Models}},
 year = {2023}
}

@article{Zhang2024Improve,
 author = {Ruohong Zhang and Bowen Zhang and Yanghao Li and Haotian Zhang and Zhiqing Sun and Zhe Gan and Yinfei Yang and Ruoming Pang and Yiming Yang},
 title = {Improve Vision Language Model Chain-of-thought Reasoning},
 year = {2024}
}

@article{zhang2024mm,
 author = {Zhang, Duzhen and Yu, Yahan and Li, Chenxing and Dong, Jiahua and Su, Dan and Chu, Chenhui and Yu, Dong},
 journal = {arXiv preprint arXiv:2401.13601},
 title = {Mm-llms: Recent advances in multimodal large language models},
 year = {2024}
}

@article{zhang2024multimodal,
 author = {Zhang, Wenqi and Cheng, Zhenglin and He, Yuanyu and Wang, Mengna and Shen, Yongliang and Tan, Zeqi and Hou, Guiyang and He, Mingqian and Ma, Yanna and Lu, Weiming and others},
 journal = {arXiv preprint arXiv:2407.07053},
 title = {Multimodal self-instruct: Synthetic abstract image and visual reasoning instruction using language model},
 year = {2024}
}

@inproceedings{zhang2024sigir,
 author = {Zhang, Peng-Fei and Huang, Zi and Bai, Guangdong},
 booktitle = {Proc. of SIGIR},
 pages = {862871},
 title = {Universal Adversarial Perturbations for Vision-Language Pre-trained Models},
 year = {2024}
}

@article{zhang2024tinyllama,
 author = {Zhang, Peiyuan and Zeng, Guangtao and Wang, Tianduo and Lu, Wei},
 journal = {arXiv preprint arXiv:2401.02385},
 title = {Tinyllama: An open-source small language model},
 year = {2024}
}

@inproceedings{zhang2024universal,
 author = {Zhang, Peng-Fei and Huang, Zi and Bai, Guangdong},
 booktitle = {Proc. of SIGIR},
 pages = {862--871},
 title = {Universal adversarial perturbations for vision-language pre-trained models},
 year = {2024}
}

@inproceedings{zhao2015enquiring,
 author = {Zhao, Zhe and Resnick, Paul and Mei, Qiaozhu},
 booktitle = {Proc. of WWW},
 pages = {1395--1405},
 title = {Enquiring minds: Early detection of rumors in social media from enquiry posts},
 year = {2015}
}

@inproceedings{zhao2016exploring,
 author = {Zhao, Xiangyu and Xu, Tong and Liu, Qi and Guo, Hao},
 booktitle = {Proc. of DASFAA},
 pages = {396--411},
 title = {Exploring the choice under conflict for social event participation},
 year = {2016}
}

@inproceedings{zhao2017incorporating,
 author = {Zhao, Xiangyu and Xu, Tong and Fu, Yanjie and Chen, Enhong and Guo, Hao},
 booktitle = {Proc. of ICDM},
 pages = {1177--1182},
 title = {Incorporating spatio-temporal smoothness for air quality inference},
 year = {2017}
}

@inproceedings{zhao2017modeling,
 author = {Zhao, Xiangyu and Tang, Jiliang},
 booktitle = {Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},
 pages = {497--506},
 title = {Modeling temporal-spatial correlations for crime prediction},
 year = {2017}
}

@inproceedings{zhao2022multi,
 author = {Zhao, Xiangyu and Fan, Wenqi and Liu, Hui and Tang, Jiliang},
 booktitle = {Proc. of AAAI},
 pages = {4388--4396},
 title = {Multi-type urban crime prediction},
 year = {2022}
}

@article{zhao2023bubogpt,
 author = {Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi},
 journal = {arXiv preprint arXiv:2307.08581},
 title = {{Bubogpt: Enabling visual grounding in multi-modal llms}},
 year = {2023}
}

@article{zhao2023bubogpt,
 author = {Zhao, Yang and Lin, Zhijie and Zhou, Daquan and Huang, Zilong and Feng, Jiashi and Kang, Bingyi},
 journal = {arXiv:2307.08581},
 title = {Bubogpt: Enabling visual grounding in multi-modal llms},
 year = {2023}
}

@article{zhao2023chatspot,
 author = {Zhao, Liang and Yu, En and Ge, Zheng and Yang, Jinrong and Wei, Haoran and Zhou, Hongyu and Sun, Jianjian and Peng, Yuang and Dong, Runpei and Han, Chunrui and others},
 journal = {arXiv preprint arXiv:2307.09474},
 title = {{Chatspot: Bootstrapping multimodal llms via precise referring instruction tuning}},
 year = {2023}
}

@article{zhao2023evaluating,
 author = {Zhao, Yunqing and Pang, Tianyu and Du, Chao and Yang, Xiao and Li, Chongxuan and Cheung, Ngai-Man and Lin, Min},
 journal = {arXiv:2305.16934},
 title = {On Evaluating Adversarial Robustness of Large Vision-Language Models},
 year = {2023}
}

@inproceedings{zhao2023mamo,
 author = {Zhao, Zijia and Guo, Longteng and He, Xingjian and Shao, Shuai and Yuan, Zehuan and Liu, Jing},
 booktitle = {Proc. of SIGIR},
 pages = {1528--1538},
 title = {Mamo: Fine-grained vision-language representations learning with masked multimodal modeling},
 year = {2023}
}

@article{zhao2023mmicl,
 author = {Zhao, Haozhe and Cai, Zefan and Si, Shuzheng and Ma, Xiaojian and An, Kaikai and Chen, Liang and Liu, Zixuan and Wang, Sheng and Han, Wenjuan and Chang, Baobao},
 journal = {arXiv:2309.07915},
 title = {Mmicl: Empowering vision-language model with multi-modal in-context learning},
 year = {2023}
}

@article{zhao2023survey,
 author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
 journal = {arXiv preprint arXiv:2303.18223},
 title = {{A survey of large language models}},
 year = {2023}
}

@article{zhao2023survey,
 author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
 journal = {arXiv:2303.18223},
 title = {A survey of large language models},
 year = {2023}
}

@article{zhao2023svit,
 author = {Zhao, Bo and Wu, Boya and Huang, Tiejun},
 journal = {arXiv preprint arXiv:2307.04087},
 title = {Svit: Scaling up visual instruction tuning},
 year = {2023}
}

@article{zhao2023svit,
 author = {Zhao, Bo and Wu, Boya and Huang, Tiejun},
 journal = {arXiv preprint arXiv:2307.04087},
 title = {{Svit: Scaling up visual instruction tuning}},
 year = {2023}
}

@inproceedings{zhao2023tuning,
 author = {Zhao, Bingchen and Tu, Haoqin and Wei, Chen and Xie, Cihang},
 booktitle = {Proc. of ICLR},
 title = {{Tuning LayerNorm in Attention: Towards Efficient Multimodal LLM Finetuning}},
 year = {2024}
}

@article{zhao2024cobra,
 author = {Zhao, Han and Zhang, Min and Zhao, Wei and Ding, Pengxiang and Huang, Siteng and Wang, Donglin},
 journal = {arXiv preprint arXiv:2403.14520},
 title = {Cobra: Extending Mamba to Multi-Modal Large Language Model for Efficient Inference},
 year = {2024}
}

@inproceedings{zheng2023ddcot,
 author = {Zheng, Ge and Yang, Bin and Tang, Jiajin and Zhou, Hong-Yu and Yang, Sibei},
 booktitle = {NeurIPS},
 title = {DDCoT: Duty-Distinct Chain-of-Thought Prompting for Multimodal Reasoning in Language Models},
 year = {2023}
}

@article{zheng2023judging,
 author = {Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zi Lin and Zhuohan Li and Dacheng Li and Eric. P Xing and Hao Zhang and Joseph E. Gonzalez and Ion Stoica},
 journal = {arXiv:2306.05685},
 title = {Judging LLM-as-a-judge with MT-Bench and Chatbot Arena},
 year = {2023}
}

@article{zheng2023learn,
 author = {Zheng, Junhao and Qiu, Shengjie and Ma, Qianli},
 journal = {arXiv preprint arXiv:2312.07887},
 title = {{Learn or Recall? Revisiting Incremental Learning with Pre-trained Language Models}},
 year = {2023}
}

@article{zheng2023minigpt,
 author = {Zheng, Kaizhi and He, Xuehai and Wang, Xin Eric},
 journal = {arXiv preprint arXiv:2310.02239},
 title = {{Minigpt-5: Interleaved vision-and-language generation via generative vokens}},
 year = {2023}
}

@article{zheng2024beyond,
 author = {Zheng, Junhao and Ma, Qianli and Liu, Zhen and Wu, Binquan and Feng, Huawen},
 journal = {arXiv preprint arXiv:2401.09181},
 title = {{Beyond Anti-Forgetting: Multimodal Continual Instruction Tuning with Positive Forward Transfer}},
 year = {2024}
}

@book{Zhou-06,
 address = {Cambridge, MA},
 author = {G. Zhou and J. Lu and C.-Y. Wan and M. D. Yarvis and J. A. Stankovic},
 publisher = {MIT Press},
 title = {Body Sensor Networks},
 year = {2008}
}

@article{zhou2022large,
 author = {Zhou, Yongchao and Muresanu, Andrei Ioan and Han, Ziwen and Paster, Keiran and Pitis, Silviu and Chan, Harris and Ba, Jimmy},
 journal = {arXiv:2211.01910},
 title = {Large language models are human-level prompt engineers},
 year = {2022}
}

@article{zhou2022least,
 author = {Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Bousquet, Olivier and Le, Quoc and Chi, Ed},
 journal = {arXiv:2205.10625},
 title = {Least-to-most prompting enables complex reasoning in large language models},
 year = {2022}
}

@article{zhou2023analyzing,
 author = {Zhou, Yiyang and Cui, Chenhang and Yoon, Jaehong and Zhang, Linjun and Deng, Zhun and Finn, Chelsea and Bansal, Mohit and Yao, Huaxiu},
 journal = {arXiv:2310.00754},
 title = {Analyzing and mitigating object hallucination in large vision-language models},
 year = {2023}
}

@article{zhou2024tinyllava,
 author = {Zhou, Baichuan and Hu, Ying and Weng, Xi and Jia, Junlong and Luo, Jie and Liu, Xien and Wu, Ji and Huang, Lei},
 journal = {arXiv preprint arXiv:2402.14289},
 title = {TinyLLaVA: A Framework of Small-scale Large Multimodal Models},
 year = {2024}
}

@article{Zhou:2010:MMS:1721695.1721705,
 author = {Zhou, Gang and Wu, Yafeng and Yan, Ting and He, Tian and Huang, Chengdu and Stankovic, John A. and Abdelzaher, Tarek F.},
 journal = {ACM Trans. Embed. Comput. Syst.},
 pages = {39:1--39:41},
 title = {A multifrequency MAC specially designed for wireless sensor network applications},
 year = {2010}
}

@inproceedings{Zhou_2024,
 author = {Zhou, Zhongliang and Zhang, Jielu and Guan, Zihan and Hu, Mengxuan and Lao, Ni and Mu, Lan and Li, Sheng and Mai, Gengchen},
 booktitle = {Proc. of SIGIR},
 pages = {27492754},
 title = {Img2Loc: Revisiting Image Geolocalization using Multi-modality Foundation Models and Image-based Retrieval-Augmented Generation},
 year = {2024}
}

@inproceedings{zhu2016visual7w,
 author = {Zhu, Yuke and Groth, Oliver and Bernstein, Michael and Fei-Fei, Li},
 booktitle = {Proc. of CVPR},
 pages = {4995--5004},
 title = {Visual7w: Grounded question answering in images},
 year = {2016}
}

@inproceedings{zhu2022,
 author = {Zhu, Sijie and Shah, Mubarak and Chen, Chen},
 booktitle = {CVPR},
 pages = {1162-1171},
 title = {{TransGeo: Transformer Is All You Need for Cross-view Image Geo-localization}},
 year = {2022}
}

@article{zhu2023difftraj,
 author = {Zhu, Yuanshao and Ye, Yongchao and Zhang, Shiyao and Zhao, Xiangyu and Yu, James},
 journal = {Proc. of NeurIPS},
 pages = {65168--65188},
 title = {Difftraj: Generating gps trajectory with diffusion probabilistic model},
 year = {2023}
}

@inproceedings{zhu2023languagebind,
 author = {Zhu, Bin and Lin, Bin and Ning, Munan and Yan, Yang and Cui, Jiaxi and Wang, HongFa and Pang, Yatian and Jiang, Wenhao and Zhang, Junwu and Li, Zongwei and others},
 booktitle = {Proc. of ICLR},
 title = {{Languagebind: Extending video-language pretraining to n-modality by language-based semantic alignment}},
 year = {2024}
}

@inproceedings{zhu2023minigpt,
 author = {Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
 booktitle = {Proc. of ICLR},
 title = {Mini{GPT}-4: Enhancing Vision-Language Understanding with Advanced Large Language Models},
 year = {2024}
}

@article{zhu2023minigpt,
 author = {Zhu, Deyao and Chen, Jun and Shen, Xiaoqian and Li, Xiang and Elhoseiny, Mohamed},
 journal = {arXiv preprint arXiv:2304.10592},
 title = {{Minigpt-4: Enhancing vision-language understanding with advanced large language models}},
 year = {2023}
}

@article{zhu2023multimodal,
 author = {Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
 journal = {arXiv preprint arXiv:2304.06939},
 title = {Multimodal c4: An open, billion-scale corpus of images interleaved with text},
 year = {2023}
}

@article{zhu2023synmob,
 author = {Zhu, Yuanshao and Ye, Yongchao and Wu, Ying and Zhao, Xiangyu and Yu, James},
 journal = {Proc. of NeurIPS},
 pages = {22961--22977},
 title = {Synmob: Creating high-fidelity synthetic gps trajectory dataset for urban mobility analysis},
 year = {2023}
}

@article{zhu2024comprehensive,
 author = {Zhu, Minjie and Zhu, Yichen and Liu, Xin and Liu, Ning and Xu, Zhiyuan and Shen, Chaomin and Peng, Yaxin and Ou, Zhicai and Feng, Feifei and Tang, Jian},
 journal = {arXiv preprint arXiv:2403.06199},
 title = {A Comprehensive Overhaul of Multimodal Assistant with Small Language Models},
 year = {2024}
}

@inproceedings{zhu2024controltraj,
 author = {Zhu, Yuanshao and Yu, James Jianqiao and Zhao, Xiangyu and Liu, Qidong and Ye, Yongchao and Chen, Wei and Zhang, Zijian and Wei, Xuetao and Liang, Yuxuan},
 booktitle = {Proc. of KDD},
 pages = {4676--4687},
 title = {Controltraj: Controllable trajectory generation with topology-constrained diffusion model},
 year = {2024}
}

@article{zhu2024llava,
 author = {Zhu, Yichen and Zhu, Minjie and Liu, Ning and Ou, Zhicai and Mou, Xiaofeng and Tang, Jian},
 journal = {arXiv preprint arXiv:2401.02330},
 title = {LLaVA-phi: Efficient Multi-Modal Assistant with Small Language Model},
 year = {2024}
}

@article{zi2023delta,
 author = {Zi, Bojia and Qi, Xianbiao and Wang, Lingzhi and Wang, Jianan and Wong, Kam-Fai and Zhang, Lei},
 journal = {arXiv preprint arXiv:2309.02411},
 title = {{Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices}},
 year = {2023}
}

@article{ziegler2019fine,
 author = {Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey},
 journal = {arXiv:1909.08593},
 title = {Fine-tuning language models from human preferences},
 year = {2019}
}
