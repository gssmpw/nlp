%%
%% This is file `sample-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%%
%% The first command in your LaTeX source must be the \documentclass command.
% \documentclass[sigconf,authordraft,anonymous]{acmart}
% \documentclass[sigconf, screen,review,anonymous]{acmart}
\documentclass[sigconf, screen]{acmart}

\usepackage{amsmath,  algorithm, algorithmic, enumitem}
%% NOTE that a single column version may required for 
%% submission and peer review. This can be done by changing
%% the \doucmentclass[...]{acmart} in this template to 
%% \documentclass[manuscript,screen]{acmart}
%% 
%% To ensure 100% compatibility, please check the white list of
%% approved LaTeX packages to be used with the Master Article Template at
%% https://www.acm.org/publications/taps/whitelist-of-latex-packages 
%% before creating your document. The white list page provides 
%% information on how to submit additional LaTeX packages for 
%% review and adoption.
%% Fonts used in the template cannot be substituted; margin 
%% adjustments are not allowed.

%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2024}
% \acmYear{2024}
% \acmDOI{XXXXXXX.XXXXXXX}
\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
% \pagestyle{plain} % removes running headers 
\setcopyright{none}


% % These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation emai}{June 03--05,
%   2018}{Woodstock, NY}

%  Uncomment \acmBooktitle if th title of the proceedings is different
%  from ``Proceedings of ...''!

% \acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY} 
% \acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
% \acmSubmissionID{xxxx}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
% \citestyle{acmauthoryear}

%%
%% end of the preamble, start of the body of the document source.
\usepackage{comment}
\usepackage{caption} % 自定义图表标题的格式
\usepackage{subcaption} % 支持子图
\usepackage{tikz} % 绘图的核心包
\usepackage{pgfplots} % 支持高质量的绘图，如柱状图、散点图
\usepackage{fontawesome}
\usepackage{pifont} % Needed for \ding{55}
\usepackage{multirow}
\definecolor{ao}{rgb}{0.0, 0.5, 0.0} % Define the 'ao' color (green)
\definecolor{mediumgreen}{rgb}{0.56, 0.8, 0.56}
\usepackage{xcolor}
\definecolor{lightgreen}{rgb}{0.6, 1.0, 0.6}
\definecolor{darkgreen}{rgb}{0.0, 0.39, 0.0}
\definecolor{third}{HTML}{FFE5D9}
\definecolor{second}{HTML}{FFD7BA} 
\definecolor{best}{HTML}{FEC89A} 

\newcommand{\xmark}{\ding{55}}% % Define the cross symbol
\newcommand{\greencheck}{{\color{ao}\checkmark}} % Define the green checkmark
\newcommand{\redcross}{{\color{red}\xmark}} % Define the red cross


% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
% \setcopyright{acmlicensed}
% \copyrightyear{2018}
% \acmYear{2018}
% \acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
% \acmConference[Conference acronym 'XX]{Make sure to enter the correct
%   conference title from your rights confirmation email}{June 03--05,
%   2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Geolocation with Real Human Gameplay Data:\\ A Large-Scale Dataset and Human-Like Reasoning Framework}





\author{Zirui Song}
\affiliation{%
  \institution{MBZUAI}
  % \city{Abu Dhabi}
  \country{United Arab Emirates}
  }
% \email{zirui.song@mbzuai.ac.ae}

\author{Jingpu Yang, Yuan Huang}
\affiliation{%
  \institution{Northeastern University}
  % \city{Shenyang}
  \country{China}
  }
% \email{jingpu.yang@northeastern.edu}

% \author{Yuan Huang}
% \affiliation{%
%   \institution{Northeastern University}
%   % \city{Shenyang}
%   % \country{China}}
% }
% % \email{yuan.huang@northeastern.edu}

% \author{Mingxuan Cui}
% \affiliation{%
%   \institution{Northeastern University}
%   % \city{Shenyang}
%   % \country{China}}
% }
% \email{mingxuan.cui@northeastern.edu}

\author{Jonathan Tonglet}
\affiliation{%
  \institution{TU Darmstadt and KU Leuven}
  % \city{Darmstadt}
  \country{Germany and Belgium}
  }
% \email{jonathan.tonglet@tu-darmstadt.de}


\author{Zeyu Zhang}
\affiliation{%
  \institution{Australian National University}
  % \city{Canberra}
  \country{Australia}
  }
% \email{zeyu.zhang@anu.edu.au}

\author{Tao Cheng}
\affiliation{%
  \institution{University College London}
  % \city{London}
  \country{United Kingdom}
  }
% \email{tao.cheng@ucl.ac.uk}


\author{Meng Fang}
\affiliation{%
  \institution{University of Liverpool}
  % \city{Liverpool}
  \country{United Kingdom}
  }
% \email{meng.fang@liverpool.ac.uk}





\author{Iryna Gurevych, Xiuying Chen}
\affiliation{%
  \institution{MBZUAI}
  % \city{Darmstadt}
  \country{United Arab Emirates }
  }
% \email{iryna.gurevych@tu-darmstadt.de}




\newcommand{\dataset}[1]{\text{GeoComp}}
\newcommand{\eval}[1]{\text{GeoEval}}

%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
Geolocation, the task of identifying an image's location, requires complex reasoning and is crucial for navigation, monitoring, and cultural preservation. 
However, current methods often produce coarse, imprecise, and non-interpretable localization.
A major challenge lies in the quality and scale of existing geolocation datasets.
These datasets are typically small-scale and automatically constructed, leading to noisy data and inconsistent task difficulty, with images that either reveal answers too easily or lack sufficient clues for reliable inference.
To address these challenges, we introduce a comprehensive geolocation framework with three key components: \textbf{\textit{\dataset{}}}, a large-scale dataset; \textbf{\textit{GeoCoT}}, a novel reasoning method; and \textbf{\textit{GeoEval}}, an aspect-based metric designed to evaluate the correctness of the geolocation reasoning process.
At the core of this framework is \dataset{} (Geolocation Competition Dataset), a large-scale dataset collected from a geolocation game platform involving 740K users over two years. It comprises 25 million entries of metadata and 3.9 million geo-tagged locations spanning much of the globe, with each location annotated thousands to tens of thousands of times by human users. The dataset offers diverse difficulty levels for detailed analysis and highlights key gaps in current models.
Building on this dataset, we propose Geographical Chain-of-Thought (GeoCoT), a multi-step reasoning framework designed to enhance the reasoning capabilities of Large Vision Models (LVMs) in geolocation tasks.
GeoCoT improves performance by integrating contextual and spatial cues through a multi-step process that mimics human geolocation reasoning. 
Finally, we demonstrate that GeoCoT significantly boosts performance by up to 25\% on classic geolocation metrics and by 9\% in reasoning quality as measured by GeoEval:
% \faGithub   ~\href{https://anonymous.4open.science/r/Geocomp-82F4/README.md}{\textcolor{blue}{Github link}}.


\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010178.10010179.10010182</concept_id>
       <concept_desc>Computing methodologies~Natural language generation</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
   <concept>
       <concept_id>10010147.10010178.10010224.10010225</concept_id>
       <concept_desc>Computing methodologies~Computer vision tasks</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computing methodologies~Natural language generation}
\ccsdesc[500]{Computing methodologies~Computer vision tasks}
%
% Keywords. The author(s) should pick words that accurately describe
% the work being presented. Separate the keywords with commas.
\keywords{Geolocation, Reasoning, Chain of thought, Dataset}
% A "teaser" image appears between the author and affiliation
% information and the body of the document, and typically spans the
% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

% \received{20 February 2007}
% \received[revised]{12 March 2009}
% \received[accepted]{5 June 2009}

%
% This command processes the author and affiliation and title
% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}


\begin{table}[h]
\centering
% \resizebox{0.47\columnwidth}{!}{%@{}llccc@{}
\resizebox{\columnwidth}{!}{%@{}llccc@{}
\begin{tabular}{@{}llcccc@{}}
\toprule
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Size}}   & \textbf{Geographic} &   \multirow{2}{*}{\textbf{Source}}& \textbf{Open} &\textbf{Human}   \\ 
&&\textbf{Coverage}&&\textbf{Access}& \textbf{Annotation} \\
\hline
Google-WS-15k
~\cite{clark2023we}
& 15k & \textcolor{lightgreen}{Global}&Map Service &\redcross &\redcross  \\
GMCP~\cite{zamir2014image}
& 105K & Local & Map Service & \redcross &\redcross  \\
StreetCLIP~\cite{haas2023learning} 
& 1M & Unknown& Map Service& \redcross&\redcross  \\
Im2GPS
~\cite{hays2008im2gps}
& 237 &Local & Web-Scraped& \greencheck &\redcross  \\ 
Im2GPS3K
~\cite{Im2GPS++YFCC4k+Im2GPS3k}
& 2997 &Local & Web-Scraped& \greencheck &\redcross  \\ 
YFCC4K
~\cite{Im2GPS++YFCC4k+Im2GPS3k}    
& 4536 & Local& Web-Scraped& \greencheck &\redcross  \\
YFCC26K
~\cite{YFCC-Val26k-Interpretable}
& 26k & Local & Web-Scraped& \greencheck &\redcross  \\
MP-16
~\cite{larson2017benchmarking}
& 4.7M & Local& Web-Scraped & \greencheck & \redcross \\
OSV-5M~\cite{astruc2024openstreetview}    
& 5.1M & \textcolor{mediumgreen}{Global} & Map Service & \greencheck & \redcross  \\ \hline
\textbf{\dataset{}} & 3.3M & \textcolor{darkgreen}{\textbf{Global}} & Map Service  & \greencheck  & \greencheck \\
\bottomrule
\end{tabular}%
}
\caption{Comparison of Existing Geolocation Datasets and GeoComp. 
% GeoComp is the first to include real, rich player performance data. 
``Local'' refers to city- or region-specific data, while ``Global'' spans multiple continents.  
Darker \textcolor{ao}{green} shades indicate broader geographic coverage. }
\label{table:dataset}
\end{table}


Geolocation, the task of determining an image’s geographical location, is crucial for applications like crime tracking, navigation, fact-checking, and cultural exploration~\cite{cheng2022,chalvatzaras2022}.
It involves interpreting contextual clues within an image, such as architectural styles, road signs, natural landscapes, and cultural markers.
Inferring location from such diverse indicators demands advanced reasoning, making geolocation a challenging task for both artificial models and human experts~\cite{khan2024debunking}.


Significant effort has been devoted to solving the geolocation task, but often at a coarse level of granularity. 
For example, methods like Im2GPS3K~\cite{vo2017revisiting} and PlaNet~\cite{PlaNet} frame the task by dividing the globe into grid cells and training deep neural networks to predict the correct cell for a given image. 
Subsequent studies improve precision by retrieving the most visually similar image from a dataset and using its coordinates as the predicted location~\cite{zhu2022,muller2018}. 
The reason for this coarse granularity in many approaches is potentially due to the lack of high-quality datasets. 
For example, Im2GPS3K contains up to 35\% non-localizable images~\cite{astruc2024openstreetview}, while the YFC100M dataset includes irrelevant data such as indoor photos and food images, which provide little to no locational information~\cite{YFCC-Val26k-Interpretable}. 
Additionally, many datasets are limited in size, with Georeasoner~\cite{ligeoreasoner} featuring only 3K images, thereby restricting the robustness and generalizability of geolocation models. 
A comparison of these datasets is shown in Table~\ref{table:dataset}.


% \begin{figure}[tb]
%     \centering
%     \includegraphics[width=0.9\linewidth]{Images/fig1.pdf}
%     \caption{The gaming logic of our platform: Two players independently guess the location based on the same image and their own hints, with scores determined by the distance between their predictions and the ground truth location.}
%     \label{fig:intro}
% \end{figure}



To address the above obstacles, in this work, we leverage the contributions of hundreds of thousands of geolocation game enthusiasts who provide real user prediction annotations while playing the game. 
Specifically, we launched a free, public-benefit-oriented online geoguessing platform in June 2022, as shown in Figure~\ref{fig:Geo}(a).
A screenshot of the platform’s GUI is provided in Appendix A.
In each game, two players independently guess the location based on the same image and their own hints, with scores determined by the distance between their predictions and the ground-truth location.
The images are sourced from Google Maps, Baidu Maps, Tencent Maps, and Gaode Maps.
The platform offers multiple game modes, allowing users to either choose opponents or join random matchups.
As of December 17, 2024, this platform has 740,468 users, 3,954,397 locations as unique geolocation tasks, and 25,355,174 human response records.
We name the collected dataset \dataset{}.
This rich and valuable dataset of real human responses enables us to evaluate task difficulty and filter out unreasonable cases.
For instance, some tasks are too easy, such as when the name of a shopping mall in a city is clearly visible in the image, enabling most users to answer correctly.
On the other hand, some tasks are highly challenging, where only a few users spend considerable time before providing accurate answers. 
Additionally, there are unreasonable tasks that contain no identifiable hints, making them unsolvable for all users despite significant effort.

Unlike previous approaches that address this task with a coarse level of granularity, we conduct a comprehensive evaluation of recent advanced LVMs on GeoComp, where the models are required to reason and predict the exact city of a given location.
Our findings reveal that this task poses a significant challenge for existing LVM models.
To address this, we introduce a Geographical Chain of Thought (GeoCoT) approach, which automatically guides the reasoning process through multi-step analysis of geographical cues, such as landmarks, environmental features, and spatial relationships.
For the evaluation of the reasoning process, we propose a set of articulated evaluation metrics, named as GeoEval including comparison with ground truth reasoning data and intrinsic evaluation.
The results demonstrate that our GeoCoT paradigm significantly improves geolocation accuracy.
It not only helps break down complex tasks into manageable reasoning steps but also enhances the interpretability of the inference process.


Our work makes key contributions to geolocation. 
First, we present \dataset{}, a large-scale, human-annotated geolocation dataset with over 3.9 million location images with corresponding location labels, and 25 million human player annotations, featuring diverse geographic regions, languages, and environmental contexts.
These annotations identify high-difficulty geolocation cases and establish benchmarks to guide future advancements.
Second, we introduce the Geographical Chain of Thought (GeoCoT) framework, a multi-step reasoning approach that improves geolocation accuracy by leveraging geographical cues like landmarks, environmental features, and spatial relationships.  
Finally, through comprehensive evaluations involving human assessments and LLM inferences, we show that GeoCoT improves predictive performance by up to 25\% while enhancing interpretability.
% Collectively, these contributions offer valuable resources and methodologies that address existing challenges and propel the field of geolocation forward.


\begin{figure*}[htb] % 使用 figure* 环境跨栏插入
    \centering
    \includegraphics[width=0.9\textwidth]{Images/Geocom.pdf} 
    \caption{
    (a) The gaming logic of our platform: Two players independently guess the location based on the same image and their own hints, with scores determined by the distance between their predictions and the ground truth location.
(b) The global map shows spatial heterogeneity, with dense clusters in more urbanized regions like Europe and Asia, and sparse coverage in areas like Africa and Siberia.  
(c) The pie chart highlights the proportional geo-tagged locations distribution, led by North America and Asia.  
(d) Unlike previous datasets like OSV-5M, where a single country (e.g., America) dominates 25\% of the data, our dataset is balanced at country level.
}
    \label{fig:Geo}
\end{figure*}


\section{Related work}

\subsection{Image Geolocation Task}
\label{related1}
% Image Geolocalization is an important task in computer vision~\cite{zhu2023difftraj,zhu2023synmob,zhu2024controltraj}, 
% spatial data mining~\cite{zhao2017incorporating,zhao2016exploring,zhang2023promptst,han2023mitigating}, and

% GeoAI~\cite{zhao2017modeling,zhao2022multi,zhang2023mlpst,zhang2023autostl}. 

Image geolocation refers to determining the corresponding location of a given image, a crucial task in computer vision~\cite{zhu2023difftraj,zhu2023synmob,zhu2024controltraj}, spatial data mining~\cite{zhao2017incorporating,zhao2016exploring,zhang2023promptst,han2023mitigating}, and GeoAI~\cite{zhao2017modeling,zhao2022multi,zhang2023mlpst,zhang2023autostl}.
Previous research in image geolocalization could be primarily classified into two approaches: classification-based methods and retrieval-based methods.
(1) \textit{Classification-based methods} partition most regions of the Earth into multiple grid cells.
Models are trained to classify each image into the correct cell~\cite{clark2023, pramanick2022, muller2018, seo2018, weyand2016}. 
The center coordinates of each cell are used as the predicted values. 
However, due to the limited number of cells, the granularity of the predicted values is coarse, preventing precise predictions.
(2) \textit{Retrieval-based methods} establish a database of geographic images with GPS coordinates. 
For a given input image, these methods retrieve the most similar image from the dataset and use its coordinates as the predicted location~\cite{zhu2022,muller2018, zhang2023,workman2015wide,liu2019lending,Zhou_2024}. 
However, constructing a comprehensive global-level image database is clearly impractical.




\subsection{Geolocation Dataset}

Existing geolocation datasets primarily originate from web-scraped or street-view images that have not been human-validated, raising concerns about their quality for effectively evaluating geolocation capabilities. 
For instance, datasets derived from web scraping, such as YFCC100M~\cite{theiner2022} and Im2GPS3K~\cite{vo2017revisiting}, include a significant proportion of images depicting food, art, pets, and personal photographs. 
These types of images are often weakly localizable or entirely non-localizable~\cite{YFCC-Val26k-Interpretable}.
Street-view datasets also face limitations, such as restricted geographic coverage~\cite{astruc2024openstreetview}; for example, \cite{mirowski2019streetlearn-navigation}'s work includes data from only three cities in the United States.
% Online platforms typically associate a location with user-uploaded images, but the coordinates provided are often not strictly accurate. 
Furthermore, dataset collection processes often introduce biases. 
For instance, some commonly used platforms are inaccessible in certain countries, resulting in uneven geographic representation. 
Additionally, the difficulty of individual geolocation tasks varies widely within these datasets, but this aspect has not been comprehensively evaluated. 
For example, images taken at prominent landmarks are relatively easy to geolocate, while others offer no clear hints and are highly challenging~\cite{astruc2024openstreetview}.
These limitations undermine the reliability of current geolocation benchmarks.


% Street view datasets 通常视觉直观上表现出 strongly localizable, 这些图像一般包含了 rich geographical cues. 但是 existing street view datasets 通常有limited 覆盖范围，例如 ~\cite{mirowski2019streetlearn-navigation,nn-UCF-GSV-Dataset-2014}'s work only include 3 cities in US. 同时，现存的所有geolocation datasets, 都缺少大规模的人工验证，来可靠的验证其 localizable and reasoning value.

% From an intuitive perspective, 
% Moreover, all existing geolocation datasets lack extensive human validation, which is essential for reliably assessing their localizability and reasoning value.


\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.8\linewidth]{Images/player.pdf}
    \caption{Performance of game players of different levels in mainstream countries.
    Experts are defined as the top 15\% in performance scores, while beginners are those in the bottom 15\%.
    }
    \label{fig:player_score}
\end{figure*}

\subsection{Large Vision Language Models}

LLMs have exhibited extraordinary emergent abilities by scaling up data and model sizes, notably including instruction following \cite{li2023llama, dai2024instructblip}, in-context learning \cite{brown2020language}, and Chain of Thought (CoT) \cite{kojima2022large}.
Building on these emergent capabilities, significant research efforts have focused on enhancing cross-modality understanding and reasoning capabilities. 
Numerous studies have been conducted on various aspects of LVMs, encompassing structural design \cite{liu2024visual, cai2023benchlmm, liu2024sphinx}, data construction \cite{laion2023gpt4v, zhao2024cobra}, training strategies \cite{zhao2023mamo,mckinzie2024mm1, lu2024deepseek}, evaluations~\cite{bithel2023evaluating}, and the development of lightweight LVMs \cite{ zhu2024comprehensive}.
Additionally, the robust capabilities of LVMs have been applied to other fields, such as medical image understanding \cite{tinyllava, pmc-vqa,zhang2024universal} and document parsing \cite{ye2023mplug, liu2024textmonkey}. 
Furthermore, the development of multi-modal agents has advanced real-world applications, including embodied agents \cite{huang2023embodied, peng2023kosmos} and GUI agents \cite{yang2023appagent, li2024appagent,song2024mmac}.
However, the reasoning capabilities of LVMs in geolocation tasks remain underexplored. 
One of the primary reasons for this limitation is the lack of high-reasoning-value geographic data. 
% Our work addresses this gap by introducing a large-scale, human-validated dataset, offering a robust foundation for advancing LVMs’ exploration in geolocation tasks.




% \subsection{Vision Chain of Thought}
% The concept of Chain of Thought (CoT) was first introduced in the field of natural language processing, where explicit reasoning steps are modeled as intermediate representations to improve complex problem-solving~\cite{wei2022chain,fang2024transformer}. 
% CoT has demonstrated significant improvements in tasks such as arithmetic reasoning, commonsense inference, and multi-step question answering~\cite{dasgupta2022language}.
% Building on this foundation, recent research has adapted CoT reasoning to the realm of LVMs to enhance interpretability and robustness. 
% For instance, \cite{Wu2023The} proposed a Description then Decision strategy to improve task decomposition and step-by-step processing. 
% Furthermore, \cite{Ge2023Chain} advanced Vision CoT techniques to improve generalization in image classification tasks.
% In this work, we extend the CoT framework to the geolocation domain by introducing GeoCoT. 



\section{Data Overview}
In this section, we first describe the data collection platform, then present the statistical distributions with visualizations, and finally showcase the performance of human players on the dataset, a unique feature that sets our dataset apart.

\subsection{Geolocation Competition}

% Inspired by Geogussr\footnote{\url{https://www.geoguessr.com/}}, we create a free game website to 调动爱好者积极参与游戏. 并且记录爱好者的竞赛记录。在网站建立初期，我们通过对google street view 中 location 的随机采集获取了初始的数据库。由于google maps 在部分地区(China mainland)缺乏数据，我们同时也采取 Baidu Map, Gaode Map 的街景数据作为替代，以寻求尽可能全面的全球数据。得益于社区的运营，玩家的自发贡献也丰富了游戏location数据库。 我们提供了 xxxxx种竞赛模式，分别为xxxxx,以此尽可能调动积极性。网站的竞赛UI 如图所示，用户被要求在地图中标出，其所预测的点位。在游戏规则中，使用外部搜索引擎是不被允许的，为了尽可能避免用户cheating, 我们将每回合竞赛限制为 1 分钟/3分钟。



Inspired by geoguessr website, we developed a free geolocation game platform that tracks participants' competition histories.
Unlike most geolocation websites, including Geoguessr, which rely solely on samples from Google Street View, our platform integrates Baidu Maps, Tencent Maps, and Gaode Maps to address coverage gaps in regions like mainland China, ensuring broader global accessibility.
Users can choose specific opponents or engage in random matches. 
Each match consists of multiple questions, and each user is initially assigned a “vitality score.” 
Users mark their predicted location on a map, and the system evaluates accuracy based on the surface distance between the predicted point and the ground truth. 
Larger errors result in greater deductions from the user's vitality score. 
The user with the higher vitality score at the end of the match is declared the winner.
% To prevent cheating, external search engines are banned, and each round is time-limited.
To ensure predictions are human-generated rather than machine-generated, users must register with a phone number, enabling tracking of individual activities. 
Using this platform, we collected \dataset{}, a comprehensive dataset covering 1,000 days of user competition.


% recall 93.4\%

\subsection{Dataset Statistic}
% \textcolor{blue}{MF: need a figure to show data examples} 

% Together, we collect a total of 3,238,919 geo-tagged location, their corresponding geographic coordinates, and all user competition data. 
% Here, we visualize key dataset statistics.



Figure \ref{fig:Geo}(a) presents a global heatmap of geo-tagged locations density, highlighting significant spatial heterogeneity in our dataset.
High-density regions are concentrated in urbanized zones such as Europe, North America, and parts of Asia, whereas areas like Africa and Oceania are sparsely represented, often due to underdeveloped infrastructure or low population density. 
Figure \ref{fig:Geo}(b) provides an overview of the proportional distribution of geo-tagged locations counts across continents, offering a macroscopic view of the dataset's global spread.
In Figure \ref{fig:Geo}(c), we further illustrate the geo-tagged locations distribution by country. 
Notably, in contrast to datasets like OSV-5M~\cite{astruc2024openstreetview}, which suffers from severe imbalances—such as the U.S. alone accounting for up to 25\% of the total data—our dataset achieves a more balanced global distribution. 
No single country or continent dominates the dataset, ensuring a more equitable geographic representation and highlighting areas where further data collection efforts may be needed.



\subsection{Human Player Performance}
% \textcolor{blue}{MF: need a figure to show what is human player performance}
\begin{figure*}[htb]
    \centering
    \includegraphics[width=1\linewidth]{Images/Rethinking.pdf}
    \caption{Comparison of previous geolocation tasks and our proposed paradigm: while previous works focused on coarse-grained predictions limited by dataset quality, our generation and reasoning-based method enables fine-grained city-level predictions.}
    \label{fig:comp}
\end{figure*}

Our dataset not only includes image and location information but also rich human player performance data on the task. 
This label information serves not only as a valuable metric for evaluating the difficulty of different tasks but also as a benchmark for understanding human decision-making in geolocation challenges.
In this subsection, we analyze the performance score across players and countries, providing insights into how human players perform on a global scale and how their accuracy varies across different regions and task types. 
We use GeoGuessr's scoring formula to evaluate a user's performance on a single task:
\[
S = \textstyle \left\lfloor \exp\left(-\frac{d}{s_d}\right) \times 5000 \right\rfloor.
\]
Here, \( S \) is the user's score (0 to 5000), \( d \) is the geographic distance between the predicted and actual locations (in kilometers), and \( s_d \) is the maximum distance within the area divided by 10. 
The score decreases exponentially as \( d \) increases. 
For example, \( s_d \) is 1805 km globally (based on Earth's maximum distance of 18,050 km) and 615 km for China, reflecting smaller scales.
A perfect prediction (\( d = 0 \)) yields \( S = 5000 \). 
A player’s performance score is defined as the average score across all their tasks. 
Similarly, a country’s performance score is the average score across all tasks performed within that country.



\textbf{Player Performance Across Levels.}
The performance of game players across different levels, as illustrated in Figure~\ref{fig:player_score}, highlights significant gaps between beginners and experts in mainstream countries. 
Expert players, defined as the top 15\% of performers, consistently achieve much higher performance metrics compared to beginners, defined as the bottom 15\%, with noticeable gaps in countries \textcolor{black}{like Canada, China, and India.}
For example, \textcolor{black}{in Canada, experts perform nearly 10 times }better than beginners, underscoring the steep learning curve involved in mastering the game. 
This performance gap presents challenges for new players, as it emphasizes the level of skill, strategy, and game knowledge required to compete effectively at higher levels.

\textbf{Player Performance Across Countries.}  The player performance across countries, as shown in Figure~\ref{fig:player_score}, demonstrates significant variations influenced by three key factors: \textit{climate, geographic size, and language}. 
Players tend to perform well in countries such as Germany, France, and Japan. 
These nations are characterized by unique languages and relatively small geographic sizes. 
The presence of distinctive languages on urban street signs provides clear linguistic clues, enabling players to quickly identify the country. Additionally, the compactness of these countries allows for more precise guesses, resulting in higher scores.
In contrast, despite China's unique language, its vast size and diverse climates make pinpointing specific locations challenging, leading to lower scores. 
Similarly, large countries like the USA, China, and Canada face additional challenges due to their shared temperate climates and extensive territories, where players often confuse them due to similar vegetation and climate, reducing accuracy.


% ----version
% In contrast, despite China’s unique language offering clear hints to its identity, the vast geographic size and diverse climates of the country make it difficult for players to pinpoint specific locations. 
% Even if players correctly identify the country using linguistic clues, narrowing down the exact location within such a large territory proves challenging, leading to lower scores.
% For countries like the USA, China, and Canada, the challenges are further compounded by their extensive territories and shared temperate continental climates.
% In the absence of linguistic clues, players often confuse these nations due to similarities in vegetation and climate. This lack of distinguishing features contributes to lower guessing accuracy, which is reflected in the lower scores observed when identifying these countries.

\textbf{Player Performance Across Tasks.}
From Figure~\ref{fig:player_score}, we can also observe significant variations in player performance across different tasks. In certain countries, player performance is relatively low, while in others, it is notably higher. This highlights the diversity in task difficulty within our dataset, offering valuable insights for assessing and categorizing task complexity.



\begin{table*}[htb]
\centering
\caption{Comparison of Precision, Recall and F1 scores in country-level and city-level geolocation. The scores are represented as follows: \colorbox{best}{best}, \colorbox{second}{second}, and \colorbox{third}{third}. 
Numbers in \textbf{bold} mean that the improvement to the best baseline is statistically significant (a two-tailed paired t-test with p-value \textless 0.01).}
\begin{tabular}{lccccccccc}
\toprule
Model & \multicolumn{3}{c}{City} & \multicolumn{3}{c}{Country} & \multicolumn{3}{c}{Continent}\\
& Accuracy$\uparrow$ & Recall$\uparrow$ & F1$\uparrow$ & Accuracy$\uparrow$ & Recall$\uparrow$ & F1$\uparrow$ & Accuracy$\uparrow$ & Recall$\uparrow$ & F1$\uparrow$ \\
\midrule
LLaVA-1.6   & 0.002  & 0.001  & 0.002 & 0.041 & 0.019  & 0.028  & 0.175 & 0.067 & 0.056\\
LLama-3.2-Vision & 0.081 & 0.037 & 0.030 & \colorbox{best}{0.630} & \colorbox{second}{0.199} & \colorbox{second}{0.217} & \colorbox{second}{0.866} & \colorbox{second}{0.643} & \colorbox{third}{0.639}  \\
Qwen-VL   & 0.016  & 0.013  & 0.014  & 0.069  & 0.042  & 0.070  & 0.130 & 0.115  & 0.077\\
GeoCLIP & 0.018 & 0.007 & 0.008 & 0.550 & \colorbox{third}{0.197} & 0.204 & \colorbox{best}{0.872} & \colorbox{best}{0.746} & \colorbox{best}{0.731} \\
GeoReasoner & 0.018  & 0.014  & 0.012  & 0.092  & 0.053  & 0.085  & 0.208 & 0.161 & 0.144\\
GPT-4o & \colorbox{third}{0.092} & \colorbox{third}{0.048} & \colorbox{third}{0.044} & 0.615 & 0.188 & 0.208 & 0.807 & 0.468 & 0.487 \\
GPT-4o(CoT) & \colorbox{second}{0.094} & \colorbox{second}{0.052} & \colorbox{second}{0.042} & \colorbox{second}{0.623} & 0.194 & \colorbox{second}{0.212} & 0.819 & 0.430 & 0.449 \\
 GeoCoT  & \colorbox{best}{\textbf{0.118}} & \colorbox{best}{\textbf{0.089}} & \colorbox{best}{\textbf{0.086}} & \colorbox{best}{\textbf{0.640}} & \colorbox{best}{\textbf{0.260}} & \colorbox{best}{\textbf{0.291}} & \colorbox{third}{0.862} & \colorbox{third}{0.638} & \colorbox{second}{0.646}  \\
\toprule
\end{tabular}
\label{tab:main}
\end{table*}

\section{Geographic Chain of Thought}
In this section, we introduce GeoCoT, a novel chain-of-thought prompting framework for graph-based and geolocation tasks. 
Unlike standard CoT prompting which performs generic step-by-step reasoning, GeoCoT introduces a domain-specific, hierarchically structured reasoning process that mimics how humans localize geographic information from broad regions to fine-grained details.

\subsection{Rethinking Geolocation Task}

As discussed in \S~\ref{related1}, the geolocation task has traditionally relied on classification-based~\cite{clark2023, pramanick2022, weyand2016} and retrieval-based methods~\cite{zhu2022, zhang2023}, as shown in Figure~\ref{fig:comp}.
% There are also some works that combine these two paradigms together such as \cite{haas2024pigeon}.
While these approaches have advanced the field, they face significant limitations in precision and scalability, prompting a rethinking of the task.
% Classification-based methods divide the Earth into grid cells, predicting an image's cell and using its center as the location. 
% While simplifying the problem, this approach limits granularity, and increasing cell numbers adds computational complexity, hindering global use. 
% Retrieval-based methods rely on geographically tagged databases but face scalability issues and poor performance in underrepresented regions, limiting effectiveness.


Inspired by how humans gradually narrow down locations from broad to fine-grained observations~\cite{luo2022g3}, we propose a new geolocation paradigm: predicting geographic locations through a step-by-step reasoning process.
Unlike traditional approaches limited by grid-based classification and exhaustive databases, our model generates natural language reasoning, guiding it to the final predicted city. 
To implement this paradigm, we introduce GeoCoT (Geographic Chain-of-Thought), a framework designed for both interpretability and accuracy.


\subsection{GeoCoT Deisgn}
Our design of GeoCoT is inspired by how humans intuitively approach geolocation—progressing from broad to fine-grained analysis.
Rather than relying on generic step-by-step reasoning like standard CoT prompting, GeoCoT mimics the human process: starting with macro-level cues (e.g., climate, terrain), then narrowing down to country, city, and finally micro-level details to guide the model through interpretable geographic reasoning.



Concretely, GeoCoT operates in five sequential stages:
\textit{1. Continental or Climate Zone Identification.}
The process begins with identifying broad regions using natural features like mountains, vegetation, or soil, narrowing the scope to a continent or climate zone.
\textit{2. Country-Level Localization.}
Cultural markers, language on signs, and architectural styles are analyzed to refine predictions to the country level.
\textit{3. City-Level Refinement Using Infrastructure.}
Street elements, such as driving direction, bollards, and license plate colors, are used to locate specific cities or regions.
\textit{4. Landmark-Based Verification.}
Features like fire hydrants, guideposts, and street signs help validate and further refine the predicted location.
\textit{5. Fine-Grained Micro-Level Validation.}
Finally, subtle details such as sidewalk patterns and clothing styles confirm precise localization at a city or neighborhood level.
These five reasoning steps are formulated as a single, structured prompt and jointly fed into the LVM, which directly generates the final predicted location.
Detailed prompts can be found in Appendix B.

% By emulating human reasoning, GeoCoT surpasses traditional methods by avoiding coarse classification grids and exhaustive image databases, providing a scalable, interpretable, and accurate solution for geolocation tasks.
It is important to note that GeoCoT does not require any concrete knowledge about the specific features of locations. 
Instead, it offers reasoning tutorials designed to help LVMs identify geographic clues by leveraging their existing knowledge.




\section{Experiments}
In this section, we first introduce our experimental settings, then evaluate GeoCoT in terms of its general geolocalization ability, followed by a detailed evaluation of its reasoning process.



\subsection{Setting}
We selected 500 geo-tagged locations with high inferential value from the dataset to serve as a test set, using a stratified sampling method across continents to ensure balanced geographic distribution.
This number is larger than in previous works \cite{liu2023visual, guan2024hallusionbench}, which typically include only a few dozen case studies to examine the reasoning process.
We define "high inferential value" as locations with moderate difficulty—challenging enough to be correctly identified by experienced participants, but not so easy that beginners can do so effortlessly.
Specifically, we selected 20 mainstream countries across six continents as representative samples and extracted tasks with an average player score of around 3,000 out of 5,000 for annotation.
This test set has been publicly released on GitHub.
Our GeoCoT framework is implemented using GPT-4o.

\subsection{Baselines}
We compare our model, GeoCoT, against several strong baselines representing the latest advancements in geolocation on our dataset.
\textbf{\textit{General Open-Source VLMs}}:
LLaVA-1.6~\cite{liu2023improved} utilizes a fully connected vision-language connector, effectively bridging visual inputs with linguistic features to deliver strong results in geolocation tasks. 
Llama-3.2-vision~\cite{meta2024llama} demonstrates advanced multi-modal reasoning capabilities, making it a powerful open-source vision-language model.
Qwen-VL~\cite{qwen-lm}, leveraging vast datasets of billions of image-text pairs, achieves robust performance in geolocation through its strong visual and spatial semantic understanding.
\textit{\textbf{Baselines Targeting Geolocation Tasks}}:
GeoCLIP~\cite{vivanco2024geoclip}, inspired by CLIP, aligns images with GPS coordinates using a retrieval-based approach to enhance geolocation. GeoReasoner~\cite{ligeoreasoner} combines geospatial reasoning with visual-language alignment for state-of-the-art geolocation performance.
\textit{\textbf{Closed-Source VLMs}}:
GPT-4o~\cite{gpt-4o} excels in vision reasoning tasks with its advanced multi-modal capabilities. 
Furthermore, GPT-4o(CoT), following the setting of cot-zero-shot~\citep{wei2022chain}, leverages chain-of-thought reasoning to improve performance in complex scenarios.
All models are evaluated using the same input format and test set to ensure a fair comparison.

% Apart from our GeoComp dataset, we also test our model on traditional benchmark datasets to show its generalization ability, where we include the reported performance of some classic and close-source baselines:
% PlaNet~\cite{weyand2016planet} partitions the Earth into cells and trains on millions of images for classification-based geolocalization. CPlaNet~\cite{seo2018cplanet} improves this by intersecting coarse partitions for fine-grained classes. Translocator~\cite{pramanick2022world} uses a dual-branch transformer with RGB images and segmentation maps for robust planet-scale localization. 

\subsection{Overall Performance Evaluation of GeoCoT}
\label{sec:5.2}

In this subsection, we evaluate the city location prediction performance of our model in comparison with the latest LVM models. 
We evaluate geolocation performance from two aspects: first, location prediction compared with the ground truth at various levels; and second, the direct calculation of the Earth's surface distance.  
We present the location prediction performance in Table~\ref{tab:main}, evaluated across three levels: city, country, and continent.
Performance is measured using \textit{accuracy}, which calculates the proportion of correct predictions out of all predictions; \textit{recall}, which determines the proportion of true positive predictions out of all actual positive cases; and the \textit{F1} score, which balances precision and recall to provide their harmonic mean.  

The results reveal several key observations. 
First, open-source LVMs such as LLaMA-3.2-Vision achieve competitive performance, performing on par with GPT-4o and GPT-4o (CoT), demonstrating their effectiveness in location prediction tasks. 
Second, performance varies across different levels of granularity. 
While GPT-4o (CoT) ranks second at the city level, it underperforms at the country level, highlighting the importance of multi-level evaluation to fully assess a model’s geolocation reasoning ability. 
Finally, our model, GeoCoT, consistently achieves top performance across all nine metrics and three levels, demonstrating its robustness and adaptability in geolocation tasks.
Additionally, GeoCLIP surpasses GPT-4o at the continent level, which can be attributed to its pretraining on image-GPS pairs, making it particularly well-suited for coarse-grained geolocation tasks. 
Coarse-grained continent-level predictions typically require less detailed local knowledge and instead rely on broader geographic cues, such as climate, landscapes, and cultural markers. 
However, GeoCLIP performs poorly at finer granularities like country and city levels, suggesting that it lacks a strong capability for geographic reasoning beyond direct visual features.


\begin{table}[htb]
\centering
\caption{Accuracy of different models on geolocation tasks at various scales.
Numbers in \textbf{bold} mean that the improvement to the best baseline is statistically significant (a two-tailed paired t-test with p-value \textless 0.01).}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \text{Street} & \text{City} & \text{Country} \\ 
&\text{1km}&\text{25km}&\text{750km} \\ \hline
LLaVA-1.6 &0.006 &0.020 &0.082 \\
Llama-3.2-Vision &0.018 &0.104 &0.638 \\
Qwen-VL  &0.004 &0.014 &0.090 \\
%ISNs &0.000&0.000&0.102 \\ 
GeoCLIP&0.035&0.077&0.625  \\
GeoReasoner&0.010&0.020&0.128  \\
GPT-4o & 0.045 & 0.147 & 0.678 \\
GPT-4o(CoT) & 0.047 & 0.151 & 0.701 \\
GeoCoT&  \textbf{0.073}  & \textbf{0.157} & \textbf{0.711} \\ 

\bottomrule
\end{tabular}
\label{tab:model_accuracy}
\end{table}


Next, in Table~\ref{tab:model_accuracy}, we present the accuracy of each model by measuring the geographic distance between the predicted city and the ground truth. The metrics represent the proportion of predictions within three distance thresholds: Street (1 km), City (25 km), and Country (750 km). 
Higher values indicate better performance, with stricter thresholds assessing fine-grained localization and larger thresholds evaluating coarse-level accuracy.
The results show that GPT-4o and Llama-3.2-vision outperform the dedicated large-scale model GeoCLIP for geolocation, even under finer-grained evaluation settings. 
For example, at the street-level threshold, GPT-4o achieves 0.045 compared to GeoCLIP’s 0.035, and at the city-level threshold, GPT-4o scores 0.147, nearly double GeoCLIP’s 0.077.
Moreover, our proposed GeoCoT paradigm demonstrates even greater improvements.
At the street level, GeoCoT achieves 0.073, significantly outperforming both GeoCLIP (0.035) and GPT-4o (0.045). 
Similarly, at the city level, GeoCoT achieves 0.157, and at the country level (750 km), it achieves 0.711, the highest among all models. 
These results highlight GeoCoT’s strong performance and the potential of its reasoning framework for geolocation tasks.




\subsection{GeoEval: Reference-Based Evaluation of GeoCoT Reasoning}
\label{sec:5.4}
Beyond evaluating overall task performance, we focus on analyzing the reasoning process of GeoCoT, which emulates a human-like reasoning approach. 
To establish a reference for this evaluation, three gaming enthusiasts collaboratively constructed reasoning processes for the same 500 cases based on geo-tagged locations. We designated these as the reasoning ground truth (a human-annotated example can be found in Appendix C).
These GT annotations serve as a benchmark within our evaluation framework, GeoEval. 
The evaluation process utilizes (1) GPT-based assessment through GPTScore~\cite{fu2023gptscore} and (2) prompt-based scoring.


Our prompt-based scoring includes four dimensions ranging from 0-5, and the detailed prompts can be found in Github.
The first dimension is the \textit{\textbf{completeness of feature extraction (CE)}}, which evaluates whether all key clues provided in the GT are comprehensively covered and accurately described in the reasoning process. 
Comprehensive feature extraction ensures that reasoning outcomes are based on sufficient factual evidence, thereby enhancing their reliability and accuracy. 
The second dimension is the \textit{\textbf{accuracy of feature extraction (AE)}}, which measures whether the identified and described attributes or characteristics of the key information in the GT are correct.
Misidentified features can lead to reasoning outcomes that deviate from the facts, reducing the credibility of the results.
The third dimension is the \textit{\textbf{accuracy of reasoning and cue correspondence (AC)}}, which assesses whether the reasoning process derives reasonable conclusions based on the extracted cues and maintains consistency with the reasoning logic presented in the GT. Incorrect correspondence between cues and conclusions can result in outcomes that deviate from reality. 
The final dimension is the \textit{\textbf{logical coherence of reasoning (LC)}}, which evaluates the consistency, logical flow, and adherence to common sense within the reasoning chain. Logical errors compromise the reliability of the reasoning process and hinder the model’s ability to arrive at accurate conclusions.


\begin{table}[ht]
\centering
\small
\caption{Evaluation of GeoCoT's reasoning process using ground truth-based metrics within the GeoEval framework. 
Numbers in \textbf{bold} mean that the improvement to the best baseline is statistically significant (a two-tailed paired t-test with p-value \textless 0.01).}
\resizebox{0.48\textwidth}{!}{%

\begin{tabular}{lcccccc}
\toprule
Model & Similarity & \multicolumn{4}{c}{GeoEval} \\
& {\text{GPTScore}} & {\text{CE}} & \text{AE} & \text{AC} & \text{LC} \\
\midrule
LLaVA-1.6 & 0.478 & 1.262 & 1.271 & 1.446 & 1.490 \\
Llama-3.2-Vision & 0.566 & 2.203 & 2.386 & 2.558 & 2.721 \\
Qwen-VL & 0.371 & 1.231 & 1.255 & 1.453 & 1.484 \\
GeoReasoner & 0.424 & 1.421 & 1.533 & 1.719 & 2.038 \\
GPT-4o & 0.613 & 2.320 & 2.891 & 2.809 & 3.143 \\
GPT-4o(CoT) & 0.663 & 2.462 & 3.136 & 3.156 & 3.540 \\
GeoCoT & \textbf{0.728} & \textbf{2.690} & \textbf{3.538} & \textbf{3.696} & \textbf{3.945} \\
\bottomrule
\end{tabular}%
}
\label{tab:reasoning_eval}
\end{table}


The experimental results in Table~\ref{tab:reasoning_eval} highlight the significant advantages of GeoCoT compared to baseline models across all evaluation metrics.
GeoCoT achieves the highest GPTScore of 0.728, outperforming GPT-4o (CoT) (0.663) and -1.6 (0.478), demonstrating its superior alignment with human-constructed reasoning processes. 
In terms of feature extraction, GeoCoT achieves a CE score of 2.690 and an AE score of 3.538, significantly surpassing GPT-4o (CoT) and the dedicated GeoReasoner model. 
Furthermore, GeoCoT’s performance in reasoning accuracy and logical coherence is unmatched, with AC and LC scores of 3.696 and 3.945, compared to GPT-4o (CoT), which scores 3.156 and 3.540, and GeoReasoner, which lags behind at 1.719 and 2.038. 
These results clearly demonstrate that GeoCoT not only captures key information more comprehensively but also maintains a more accurate and logically coherent reasoning process compared to both reasoning-based models and traditional baselines like GeoReasoner.


\subsection{Intrinsic Evaluation of GeoCoT Reasoning}


\begin{figure*}[htb]
    \centering
    \includegraphics[width=0.95\linewidth]{Images/case_copy.pdf}
    \caption{Qualitative comparison of LLaVA, GPT4o, and GeoReasoner. Clues are shown in \textcolor[HTML]{2f6eba}{blue}, correct predictions in \textcolor[HTML]{628443}{green}, incorrect in \textcolor[HTML]{ea3323}{red}, and vague/uncertain guesses in \textcolor[HTML]{de8344}{orange}.}
    \label{fig:Case_study}
\end{figure*}

\label{sec:5.3}
%选平均分比较高的 5%，random 500个地点，下载全景图。 
% 1, （human eval）hallucination item (1) item exist (2) 事实错误 （3）Vision mismatched / 

We begin with a ground truth-based evaluation, comparing GeoCoT’s reasoning to human-authored processes to assess its alignment with established reasoning patterns.
To complement this, we conduct an intrinsic evaluation focused on hallucination errors—assessing logical consistency, coherence, and robustness without relying on external references.
Given the need for multimodal judgment, this evaluation is performed by human annotators.

Following previous work on assessing hallucinations in terms of objects, attributes, and relationships~\cite{li2023evaluating,sun2023aligning}, we evaluate the quality of synthetic data across three key dimensions: (1) \textbf{Object Hallucination (OH)}: assesses whether the synthetic data includes objects that do not exist in the image. Object Hallucination evaluates the extent to which synthetic data introduces fictional elements. 
(2) \textbf{Fact Hallucination (FH)} measures the accuracy of factual information within the synthetic data. Fact Hallucination occurs when the synthetic data contains facts, figures, or other information that is incorrect or not supported by the original data.   (3) \textbf{Attribution Hallucination (AH)} evaluates whether the synthetic data incorrectly attributes properties, characteristics, or relations to entities or objects. 
To quantify hallucinations, each detected error is counted as one instance in the corresponding dimension.
To evaluate these dimensions, we invited 2 human annotators with professional backgrounds in geographic reasoning and data validation to assess GPT-4o, GeoReasoner, and our proposed GeoCoT model. 
These three baselines provide textual reasoning processes across 1,500 evaluated cases. 
The results, shown in Table~\ref{tab:reasoning_value}, indicate the number of errors in each dimension, demonstrating that GeoCoT significantly reduces hallucination errors compared to the other models.
The inter-annotator agreement, measured by Cohen's Kappa, is 0.82 for OH, 0.79 for FH, and 0.85 for AH, indicating substantial agreement across all dimensions.
The correlation between the human evaluation scores and the automatic evaluation metrics in Table~\ref{tab:main} is –0.99 (p < 0.01), demonstrating a strong inverse relationship: as hallucination errors decrease, overall geolocation performance improves significantly.


\begin{table}[ht]
\centering
\caption{Hallucination Evaluation on Reasoning Data.}
\begin{tabular}{lccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & {OH} & {FH} & {AH} \\
               & Count$\downarrow$ 
               & Count$\downarrow$  
               & Count$\downarrow$  \\ \midrule
% LLaVA-1.5      &                     &                  &                                              \\
GeoReasoner    &237                     &151                                            
               &203                                                     \\
GPT-4o         &43                     &4                              
               &35                                                                 \\

GeoCoT         &5                     &1                                
               &18                                \\ 
\bottomrule
\end{tabular}
\label{tab:reasoning_value}
\end{table}



\begin{table}[htb]
\centering
\caption{Performance comparison of GeoCoT and state-of-the-art geolocation models on traditional benchmarks.
Numbers in \textbf{bold} mean that the improvement to the best baseline is statistically significant (a two-tailed paired t-test with p-value \textless 0.01).}
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multirow{3}{*}{\textbf{Model}} & &\textbf{Im2GPS} & &  &\textbf{Im2GPS3K} & \\
& \text{Street} & \text{City} & \text{Country} & Street & City & Country \\ 
&\text{1km}&\text{25km}&\text{750km} & 1km & 25km  & 750km  \\ \hline
LLaVA-1.6 &0.04 &0.18 & 0.39&0.03 &0.14 & 0.32 \\ 
Llama-3.2-Vision &0.09 &0.37 & 0.65&0.07 &0.27 & 0.52 \\
Qwen-VL &0.04 &0.21 & 0.37&0.04 &0.15 & 0.26 \\
GeoCLIP     & 0.17  & 0.41 & 0.77 & 0.13 &0.32&0.67  \\
GeoReasoner &0.05 &0.19 & 0.33&0.04 &0.15 & 0.26 \\
% PlaNet &0.08 &0.25 & 0.54&0.09 &0.25 & 0.48 \\ 
% CPlaNet& 0.17 &0.37 &0.62 &0.10 &0.27 &0.49  \\ 
% ISNs &0.17 &0.43 &0.67 &0.11 &0.28 &0.50\\ 
% Translocator & 0.20 &0.48 &0.76 &0.12 &0.31 &0.59\\
% GeoDecoder &0.22 &0.50 &0.80 & 0.13&0.34 &0.61 \\
GPT-4o & 0.13 & 0.47 & 0.74 & 0.14 & 0.40 & 0.66 \\
GPT-4o(CoT) & 0.16 & 0.49 & 0.77 & 0.14 & 0.45 & 0.69 \\
GeoCoT& \textbf{0.22} & \textbf{0.55} & \textbf{0.83} & \textbf{0.15} & \textbf{0.46} & \textbf{0.74} \\ 
\bottomrule
\end{tabular}
}
\label{tab:traditional}
\end{table}



\subsection{Case Study}


We present two examples in Figure~\ref{fig:Case_study} to analyze the performance of LLaVA, GPT4o, and GeoReasoner, highlighting the effectiveness of our GeoCoT approach. In the first example, struggles to provide a specific prediction, reflecting its reliance on general architectural cues and its tendency to consider broad regions such as the United Kingdom or France. GPT4o, despite identifying key features of the European landscape, incorrectly associates them with Germany, indicating limitations in handling specific regional markers. In contrast, GeoCoT accurately pinpoints the location in France by effectively integrating textual clues, architectural elements, and environmental context.

In the second example, GeoCoT correctly identifies the location as San Francisco, USA, by analyzing U.S. traffic standards, license plates, and local signage, demonstrating strong contextual reasoning. 
LLaVA-1.6 makes a broad prediction, covering the U.S., Australia, and the U.K., showing uncertainty from general cues. GPT-4o misidentifies the scene as Seattle, relying on architectural similarities but missing key details.
% GeoReasoner provides no specific location, highlighting its limited interpretation of distinguishing features. 
% This example underscores the need for precise context in geographic prediction tasks.

\subsection{Generalizability Evaluation}

Even though our dataset is more comprehensive and human-annotated, we are also interested in evaluating how our model performs on traditional geolocation datasets to provide a more thorough comparison.
Hence, we select two existing benchmark datasets, Im2GPS~\cite{hays2008im2gps} and Im2GPS3K~\cite{Im2GPS++YFCC4k+Im2GPS3k}, due to their popularity and widespread use in geolocation tasks as standard benchmarks for evaluating model performance.
Similarly, we use the center point coordinates of the city text address in GeoCoT's output and measure the distance between the output and the ground truth locations.

We present the performance results in Table~\ref{tab:traditional}.
We observe that state-of-the-art geolocation models, such as GeoCLIP, perform well on traditional geolocation tasks, surpassing GPT-4o and coming close to our model, GeoCoT.
However, this is in contrast to the results shown in Table~\ref{tab:main}, where GeoCLIP significantly underperforms GPT-4o on fine-grained city- and country-level geolocation tasks.
This discrepancy suggests that these baseline models may be overfitting to the specific datasets they were trained on, lacking the generalization ability required for more diverse or fine-grained geolocation challenges.
In contrast, our model consistently outperforms traditional methods across different granularity levels and datasets without any training, and thus does not suffer from overfitting.


\section{Conclusion}

In this work, we present the largest geolocation dataset to date, collected from a geolocation game platform with 740K users over two years.
The dataset comprises 25M entries of metadata, including 3M geo-tagged locations spanning most of the globe, each annotated thousands to tens of thousands of times by human users. This dataset enables diverse difficulty-level analysis and highlights the limitations of current LVMs.
We also introduce a generation-based reasoning solution for the geolocation task, where the LVM generates reasoning chains by leveraging clues from images and produces the final predicted location.
Using our GeoEval set of metrics, we demonstrate that our GeoCoT framework significantly outperforms state-of-the-art general and task-specific baselines on this dataset.

In future work, we plan to enhance model interpretability and robustness, explore multi-modal integration of text and visuals, and expand the dataset to better cover underrepresented regions for improved global coverage and fairness.



\section*{Data Ethics}
The creation and release of our dataset adhere to stringent ethical standards to ensure the privacy and well-being of all contributors. 
We have conducted rigorous anonymization of the dataset to protect user privacy. 
All personally identifiable information, such as usernames, email addresses, and IP addresses, has been permanently removed. 
Only non-identifiable behavioral data, such as prediction outcomes and timestamps, are retained. 
The dataset originates from user participation on our open-source geolocation game platform. 
Users were informed during the registration process that their activity data might be used for research purposes. 
This ensures transparency in data collection and maintains user trust. 
We have explicitly designed the dataset for research purposes, with the sole intention of advancing geolocation and related artificial intelligence technologies.
Importantly, our dataset does not include the images directly but instead provides links to images hosted on platforms such as Google Maps or Baidu Maps, which can be accessed through their official APIs.

We are committed to ensuring the responsible use of this dataset. Researchers accessing the data must agree to a data usage agreement that prohibits unethical or illegal use. 
% Furthermore, we welcome feedback from the research community and users. If any unforeseen ethical concerns arise, we are prepared to take immediate action, including revising or retracting the dataset.

% \appendix

% \section{Detail of GeoCoT}
% \label{details}
% We present the detailed prompt of our GeoCoT process below:



% \textit{$\bullet$ \textbf{Question1:} Are there prominent natural features, such as specific types of \textcolor{cyan}{vegetation}, \textcolor{cyan}{landforms} (e.g., \textcolor{orange}{mountains}, \textcolor{orange}{hills}, \textcolor{orange}{plains}), or \textcolor{cyan}{soil characteristics}, that provide clues about the geographical\textcolor{green}{region}?
%  \noindent $\bullet$ \textbf{Question2:} Are there any culturally, historically, or architecturally significant \textcolor{cyan}{landmarks}, \textcolor{cyan}{buildings}, or \textcolor{cyan}{structures}, or are there any \textcolor{cyan}{inscriptions} or \textcolor{cyan}{signs} in a specific
%     language or script that could help determine the \textcolor{green}{country} \textcolor{green}{or region}?
% \noindent $\bullet$  \textbf{Question3:} Are there distinctive road-related features, such as \textcolor{cyan}{traffic direction} (e.g., \textcolor{orange}{left-hand or right-hand driving}), specific types of \textcolor{cyan}{bollards}, unique utility \textcolor{cyan}{pole designs}, or \textcolor{cyan}{license plate}colors and styles, which \textcolor{green}{countries} are known to have these characteristics?
%  \noindent $\bullet$ \textbf{Question4:} Are there observable \textcolor{cyan}{urban} or \textcolor{cyan}{rural markers} (e.g., \textcolor{orange}{street signs}, \textcolor{orange}{fire hydrants guideposts}) , or other
%     \textcolor{cyan}{infrastructure} elements, that can provide more specific information about the \textcolor{green}{country or city}?
% \noindent $\bullet$  \textbf{Question5:} Are there identifiable patterns in \textcolor{cyan}{sidewalks} (e.g., \textcolor{orange}{tile shapes}, \textcolor{orange}{colors}, or \textcolor{orange}{arrangements}), \textcolor{cyan}{clothing styles} worn by people, or other culturally specific details that can help narrow down the \textcolor{green}{city or area}?}


% \textit{Let's think step by step. Based on the question I provided, locate the location of the picture as accurately as possible. Identify the continent, country, and city, and summarize it into a paragraph. 
% For example: the presence of tropical rainforests, palm trees, and red soil indicates a tropical climate... Signs in Thai, right-side traffic, and traditional Thai architecture further suggest it is in Thailand... Combining these clues, this image was likely taken in a city in \textcolor{red!70!black}{Bangkok, Thailand, Asia.}}

% Here, \textcolor{cyan}{cyan} highlights potential clues within the image to help the model infer geographic locations. \textcolor{green}{Green} defines the geographic scope inferred from the clues, such as a region, country, or city. \textcolor{orange}{Orange} provides detailed descriptions of the cyan clues, enhancing the model's understanding. \textcolor{red!70!black}{Red} specifies the expected output format, including city, country, and continent.
% % \textit{Let's think step by step. Based on the question I provided, locate the location of the picture as accurately as possible. Identify the continent, country, and city, and summarize it into a paragraph. For example: the presence of tropical rainforests, palm trees, and red soil indicates a tropical climate, most likely located in Southeast Asia. The signs are in Thai and the hydrant is a green circle, indicating that it is in Thailand. Right-side traffic, cylindrical bollards with blue markings, and license plates with black lettering on a white background meet Thai standards. Traditional Thai architecture, such as pitched roofs and wooden structures, further points to a specific location in Thailand. Square gray sidewalk tiles and non-motorized lanes marked with red asphalt are specific urban design features that help narrow down the location. Combining tropical vegetation, Thai-language road signs, traditional architecture, and specific urban design features, this image was most likely taken in a city in \textcolor{red!70!black}{Bangkok, Thailand, Asia.}}


% %%
% %% The acknowledgments section is defined using the "acks" environment
% %% (and NOT an unnumbered section). This ensures the proper
% %% identification of the section in the article metadata, and the
% %% consistent spelling of the heading.
% % \begin{acks}
% % To Robert, for the bagels and explaining CMYK and color spaces.
% % \end{acks}

% \section{Human Annotation Example}
% Below we show an example of human annotated ground truth to demonstrate the annotation process, criteria, and the reasoning behind the annotations, where clues are shown in \textcolor[HTML]{2f6eba}{blue}, correct predictions in \textcolor[HTML]{628443}{green}.

% \textit{The image shows a rural residential area with dense trees and expansive green lawns. The terrain is flat, and the \textcolor[HTML]{2f6eba}{soil is reddish-brown}, which matches the temperate climate of central Europe, particularly rural areas of France. The architectural style of the house is distinctive: a \textcolor[HTML]{2f6eba}{red-tiled sloped roof, yellow walls, and solar panels}, reflecting the region's focus on renewable energy, a common feature in French countryside homes. The \textcolor[HTML]{2f6eba}{red mailbox} at the gate is a hallmark of rural French residences. The design of the fences and modern \textcolor[HTML]{2f6eba}{gates aligns} with typical styles in the French countryside. The house design and surrounding natural environment suggest a rural European region. Based on the architectural style, natural landscape, and street elements, the image was most likely taken in \textcolor[HTML]{628443}{Aumont, France, Europe}.}



% \begin{figure}[htb]
%     \centering
%     \includegraphics[trim=9cm 3cm 9cm 1cm, clip, width=1.2\linewidth]{Images/example.pdf}
% \caption{A manually annotated example demonstrating the use of visual and architectural features for geographic inference. Key clues are highlighted in \textcolor[HTML]{4f71be}{blue}, while correct reasoning results are shown in \textcolor[HTML]{7eab55}{green}.}
%     \label{fig:human_example}
% \end{figure}

% In this section, we present examples of human annotation to demonstrate the annotation process, criteria, and the reasoning behind the annotations. Figure \ref{fig:human_example} illustrates a street-view image accompanied by manually annotated geographic information. The annotation process involves careful analysis of multiple visual and contextual cues, including architectural style, soil composition, vegetation characteristics, and street elements. Based on these features, annotators infer the most probable location of the image and provide a well-reasoned explanation to support their judgment. This approach ensures the reliability and interpretability of the annotation process, facilitating a better understanding of how human perception contributes to geographic inference.



%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.


\end{document}
\clearpage
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


\end{document}
\endinput
%%
%% End of file `sample-authordraft.tex'.
