\section{Related Work}
Preference alignment has gained notable attention, particularly in guiding LLMs to generate contents that align with the user's values or objectives. The primary approach to achieving this is through Reinforcement Learning with Human Feedback (RLHF) ____, and several successful applications have been reported for fine-tuning LLMs for different tasks ____, and extended to incorporate other feedback sources than humans ____. The most popular RL algorithm used for these methods is Proximal Policy Optimization (PPO) ____. Another alternative is to directly fine-tune LLMs with a preference dataset without RL, e.g., ____, and most notably Direct Preference Optimization (DPO) ____. 

Because user preferences for LLMs are likely multidimensional with trade-offs, e.g., helpfulness vs. harmlessness, multi-objective alignment methods have been proposed to produce Pareto-front aligned models with respect to multiple preference criteria. Notable recent work includes Rewarded Soup ____, multi-objective DPO ____, Rewards-in-Context ____, controllable preference optimization ____, and Panacea ____.

Note that the purpose of multi-objective alignment methods has a strong parallel with the purpose of multi-objective optimization methods ____ where the goal is to find models or solutions that constitute a high-quality Pareto front ____. Therefore, we were motivated to find inspirations from the techniques used in the latter domain such as the epsilon-constraint method ____ or non-dominated sorting ____.