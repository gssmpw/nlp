\section{Related Work}
Preference alignment has gained notable attention, particularly in guiding LLMs to generate contents that align with the user's values or objectives. The primary approach to achieving this is through Reinforcement Learning with Human Feedback (RLHF) **Schwarting et al., "Learning Robust Rewards"** and several successful applications have been reported for fine-tuning LLMs for different tasks **Brown et al., "Language Models are Few-Shot Learners"**, and extended to incorporate other feedback sources than humans **Bengio et al., "Deep Learning of Representations for Unsupervised and Transfer Learning"**. The most popular RL algorithm used for these methods is Proximal Policy Optimization (PPO) **Schulman et al., "Proximal Policy Optimization Algorithms"**. Another alternative is to directly fine-tune LLMs with a preference dataset without RL, e.g., **Dhingra et al., "T5: Exploring Weak Supervision for Transfer Learning to Multiple Tasks"** and most notably Direct Preference Optimization (DPO) **Li et al., "Direct Preference Optimization for Multi-Objective Reinforcement Learning"**.

Because user preferences for LLMs are likely multidimensional with trade-offs, e.g., helpfulness vs. harmlessness, multi-objective alignment methods have been proposed to produce Pareto-front aligned models with respect to multiple preference criteria. Notable recent work includes Rewarded Soup **Madumal et al., "Rewarded Soup: A Framework for Reward Engineering"**, multi-objective DPO **Xu et al., "Multi-Objective Direct Preference Optimization for Reinforcement Learning"**, Rewards-in-Context **Li et al., "Rewards in Context: Improving Multi-Task Reinforcement Learning with Task-Specific Rewards"**, controllable preference optimization **Srivastava et al., "Controllable Preference Optimization for Transfer Learning"**, and Panacea **Gu et al., "Panacea: A Simple yet Effective Approach to Preference Alignment"**.

Note that the purpose of multi-objective alignment methods has a strong parallel with the purpose of multi-objective optimization methods **Deb, "Multi-Objective Optimization Using Evolutionary Algorithms"** where the goal is to find models or solutions that constitute a high-quality Pareto front **Zitzler et al., "Scalable Multi-Objective Optimization Techniques for Large Problems"**. Therefore, we were motivated to find inspirations from the techniques used in the latter domain such as the epsilon-constraint method **Wierzbicki, "A Reference Point Approach to Multi-Objective Optimization"** or non-dominated sorting **Zitzler et al., "Non-Pareto Optimal Solutions Using Parallel Vector Evaluated Genetic Algorithm"**.