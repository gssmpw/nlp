\section{Related Work}
Preference alignment has gained notable attention, particularly in guiding LLMs to generate contents that align with the user's values or objectives. The primary approach to achieving this is through Reinforcement Learning with Human Feedback (RLHF) \cite{christiano2017deep}, and several successful applications have been reported for fine-tuning LLMs for different tasks \cite{ziegler2019fine, stiennon2020learning, ouyang2022training}, and extended to incorporate other feedback sources than humans \cite{leerlaif, liu2023rltf, jha2024rlsf, williams2024multi}. The most popular RL algorithm used for these methods is Proximal Policy Optimization (PPO) \cite{schulman2017proximal}. Another alternative is to directly fine-tune LLMs with a preference dataset without RL, e.g., \cite{hejna2023contrastive, azar2024general, ethayarajh2024kto}, and most notably Direct Preference Optimization (DPO) \cite{rafailov2024direct}. 

Because user preferences for LLMs are likely multidimensional with trade-offs, e.g., helpfulness vs. harmlessness, multi-objective alignment methods have been proposed to produce Pareto-front aligned models with respect to multiple preference criteria. Notable recent work includes Rewarded Soup \cite{rame2024rewarded}, multi-objective DPO \cite{zhou2024beyond}, Rewards-in-Context \cite{yang2024rewards}, controllable preference optimization \cite{guo2024controllable}, and Panacea \cite{zhong2024panacea}.

Note that the purpose of multi-objective alignment methods has a strong parallel with the purpose of multi-objective optimization methods \cite{deb2016multi} where the goal is to find models or solutions that constitute a high-quality Pareto front \cite{zitzler1998multiobjective}. Therefore, we were motivated to find inspirations from the techniques used in the latter domain such as the epsilon-constraint method \cite{haimes1971bicriterion} or non-dominated sorting \cite{deb2002fast}.