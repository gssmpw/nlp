\section{Related Work}
In absence of complete knowledge, the greedy strategy chooses the option that, at every step, makes the locally optimal choice. 
Greedy algorithms \cite{CLRS1990,Wang2023}, have been studied for a long time and although they have applications in multiple problems, like finding shortest paths, minimum spanning trees and in the Knapsack problem, less so have they been tested in game theory. The greedy strategy in this paper corresponds to the Randomized Greedy Algorithm proposed by Blum and Mansour in 2007 \cite{BM2007}, where they tie break between equal observations with uniform choices. Their framework tests these strategies in a repeated game as opposed to a one-shot game and calculates regret as a function of time (steps the player has taken in the repeated game). The greedy strategy that we are analysing in this paper, also closely relates to recent research published by Mahdian et al. \cite{MJK2022} where they measure the regret of the greedy strategy when used to make decisions from noisy observations. Their findings show that the regret incurred by the greedy algorithm can surpass the optimal regret by an unbounded margin, even in cases where the noises are unbiased. In our model, the value of the products is calculated from discrete ratings and the observations correspond to past ratings of the available products. In this context, we are able to precisely establish an upper bound on the regret that the greedy strategy could experience, considering a specific number of products and observations.

As mentioned above, the motivation to find an upper bound on the regret comes from Savage's 1951 work \cite{Savage1951} in decision theory, which develops the minimax regret criterion designed to minimize the worst-case regret. Similar to Wald's maximin model \cite{Wald1950}, this has often been utilized to model choices under uncertainty, however, it assumes knowledge of the underlying distribution. Ismail \cite{Ismail2020} proposes the optimin criterion which coincides with Wald's maximin criterion in zero-sum games.

The concepts of playing a greedy strategy and minimizing regret are studied together by Rogers \cite{Rogers2011}. In his work on Algorithmic Game Theory, he shows how algorithms can learn from past observations to reduce their regret. Analogous to how Jiao \cite{Jiao2021} compares pseudo and worst-case expected regret in a multi-armed bandit problem, we introduce a worst-case expected regret for the greedy strategy and show that it approaches 0 in the limit as observations increase. 

Commonly, the greedy strategy is not known to perform well \cite{BGY2004, Wang2023}, however, as Bayati et al. \cite{BHJK} and Jedor et al. \cite{JLP2021} show, in specific instances of the multi-armed bandit problem it performs reasonably well. Bayati et al. \cite{BHJK} show that in the multi-armed bandit problem, with high probability one of the arms on which the greedy strategy concentrates attention is likely to have a high mean reward.

For the multi-armed bandit problem, when uncertain about the opponent, Fu et al. \cite{FTYLWXWLXFY2022} propose methods for inferring the opponent's strategy and using a greedy strategy to make decisions based on that. 
Furthermore, a version of the multi-armed bandit problem with additional observations has been proposed by Yun et al. \cite{YPASY2018}, where the decision maker besides being able to select an arm to play, they are also able to observe outcomes of additional arms by paying certain costs of a given budget. In our setting there is no cost assigned to the observations, however, they also come from an unknown distribution. 

Empirical Risk Minimization (ERM) is a framework for optimizing decisions based on minimizing average loss over observed data, assuming it is drawn i.i.d. from a fixed distribution. Maurer and Pontil \cite{MP2009} extend this idea with sample variance penalization (SVP) to improve generalization by penalizing high-variance samples. While we use similar methods to prove our results, these approaches are not applicable to our framework; ERM minimizes the expected empirical loss, aiming for good average-case performance over the observed data, whereas the regret in our setup measures the shortfall relative to the best possible decision, focusing on worst-case performance.

Regret minimization is commonly studied in no-regret learning algorithms, as discussed by Roughgarden et al. \cite{TN16} and Avramopoulos et al. \cite{ARS2008}. Similarly to the work by Blum and Mansour however, that is measured in repeated settings with respect to the number of rounds. Lastly, although a method involving convex optimization techniques, Flores-Szwagrzak \cite{Flores2022} shows how an agent can utilize observations from an unknown distribution, to consistently learn and improve their payoff.