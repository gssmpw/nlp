@inproceedings{ARS2008,
author = {Avramopoulos, Ioannis and Rexford, Jennifer and Schapire, Robert},
year = {2008},
month = {01},
pages = {},
title = {From Optimization to Regret Minimization and Back Again.}
}

@article{BGY2004,
title = "When the greedy algorithm fails.",
author = "J{\o}rgen Bang-Jensen and G. Gutin and A. Yeo",
year = "2004",
doi = "10.1016/j.disopt.2004.03.007",
language = "English",
volume = "1",
pages = "121--127",
journal = "Discrete Optimization",
issn = "1572-5286",
publisher = "Elsevier",
}

@misc{BHJK,
      title={The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms}, 
      author={Mohsen Bayati and Nima Hamidi and Ramesh Johari and Khashayar Khosravi},
      year={2022},
      eprint={2002.10121},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inbook{BM2007, place={Cambridge}, title={Learning, Regret Minimization, and Equilibria}, booktitle={Algorithmic Game Theory}, publisher={Cambridge University Press}, author={Blum, Avrim and Mansour, Yishay}, editor={Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V.Editors}, year={2007}, pages={79–102}}

@book{CLRS1990,
  author       = {Thomas H. Cormen and
                  Charles E. Leiserson and
                  Ronald L. Rivest and
                  Clifford Stein},
  title        = {Greedy Algorithms IV-15; Introduction to Algorithms, 3rd Edition},
  publisher    = {{MIT} Press},
  year         = {2009},
  url          = {http://mitpress.mit.edu/books/introduction-algorithms},
  isbn         = {978-0-262-03384-8},
  timestamp    = {Mon, 17 Aug 2020 11:36:12 +0200},
  biburl       = {https://dblp.org/rec/books/daglib/0023376.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{FTYLWXWLXFY2022,
  title = 	 {Greedy when Sure and Conservative when Uncertain about the Opponents},
  author =       {Fu, Haobo and Tian, Ye and Yu, Hongxiang and Liu, Weiming and Wu, Shuang and Xiong, Jiechao and Wen, Ying and Li, Kai and Xing, Junliang and Fu, Qiang and Yang, Wei},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {6829--6848},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/fu22b/fu22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/fu22b.html},
  abstract = 	 {We develop a new approach, named Greedy when Sure and Conservative when Uncertain (GSCU), to competing online against unknown and nonstationary opponents. GSCU improves in four aspects: 1) introduces a novel way of learning opponent policy embeddings offline; 2) trains offline a single best response (conditional additionally on our opponent policy embedding) instead of a finite set of separate best responses against any opponent; 3) computes online a posterior of the current opponent policy embedding, without making the discrete and ineffective decision which type the current opponent belongs to; and 4) selects online between a real-time greedy policy and a fixed conservative policy via an adversarial bandit algorithm, gaining a theoretically better regret than adhering to either. Experimental studies on popular benchmarks demonstrate GSCU’s superiority over the state-of-the-art methods. The code is available online at \url{https://github.com/YeTianJHU/GSCU}.}
}

@techreport{Flores2022,
title = "Learning by Convex Combination",
abstract = "We study how an agent evaluates the optimality of an action when she only observes a sample of its outcomes, not the outcome distribution. We characterize a model where the agent assigns an ex-ante utility to the action and then, upon seeing the sample, “updates” her evaluation by taking a convex combination of this ex-ante utility and the average utility of the outcomes in the sample. The weight on the average utility in this convex combination increases with sample size. Asymptotically, actions are evaluated using their sample average utility. The model includes Bayesian benchmarks as special cases. More generally, it describes an agent that may learn imperfectly yet consistently with a rough intuitive understanding of the Law of Large Numbers; it also enables decision-theoretic deﬁnitions of important concepts in the descriptive study of probabilistic judgement.",
keywords = "Sample, Sample size, Learning, Uncertainty, Sample, Sample size, Learning, Uncertainty",
author = "Karol Szwagrzak",
year = "2022",
language = "English",
series = "Working Paper / Department of Economics. Copenhagen Business School",
publisher = "Copenhagen Business School [wp]",
number = "16-2022",
address = "Denmark",
type = "WorkingPaper",
institution = "Copenhagen Business School [wp]",
}

@inproceedings{Ismail2020, author = {Ismail, Mehmet S.}, title = {One for All, All for One---Von Neumann, Wald, Rawls, and Pareto}, year = {2020}, isbn = {9781450379755}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3391403.3399460}, doi = {10.1145/3391403.3399460}, abstract = {Applications of the maximin criterion extend beyond economics to computer science, politics, and statistics. However, the maximin criterion---be it von Neumann's, Wald's, or Rawls'---draws fierce criticism due to its extremely pessimistic stance. I propose a novel concept, dubbed the optimin criterion, which is based on (Pareto) optimizing the minimal payoffs of tacit agreements. The optimin criterion generalizes and unifies results in various fields: It not only coincides with (i) maximin strategies in zero-sum games, (ii) the core in cooperative games when the core is nonempty, though it exists even if the core is empty, but it also generalizes (iii) Nash equilibrium in n-person constant-sum games, (iv) stable matchings in matching models, and (v) competitive equilibrium.}, booktitle = {Proceedings of the 21st ACM Conference on Economics and Computation}, pages = {763–764}, numpages = {2}, keywords = {noncooperative games, maximin criterion, cooperative games}, location = {Virtual Event, Hungary}, series = {EC '20} }

@misc{JLP2021,
      title={Be Greedy in Multi-Armed Bandits}, 
      author={Matthieu Jedor and Jonathan Louëdec and Vianney Perchet},
      year={2021},
      eprint={2101.01086},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{Jiao2021,
      title={Analysis of finite-arm i.i.d.-reward bandit; Theory of Multi-armed Bandits and Reinforcement Learning}, 
      author={Jiantao Jiao},
      year={Department of Electrical Engineering and Computer Sciences at University of California Berkeley, United States, 2021},
}

@misc{MJK2022,
      title={Regret Minimization with Noisy Observations}, 
      author={Mohammad Mahdian and Jieming Mao and Kangning Wang},
      year={2022},
      eprint={2207.09435},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@article{MP2009,
      title={Empirical Bernstein Bounds and Sample Variance Penalization}, 
      author={Andreas Maurer and Massimiliano Pontil},
      year={2009},
      eprint={0907.3740},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/0907.3740}, 
}

@misc{Rogers2011,
      title={Algorithmic Game Theory}, 
      author={Ryan Rogers},
      year={University Of Pennsylvania, 2011},
}

@article{Savage1951,
	author = {Leonard J. Savage},
	journal = {Journal of the American Statistical Association},
	pages = {55--67},
	title = {The Theory of Statistical Decision},
	volume = {46},
	year = {1951}
}

@book{TN16, author = {Roughgarden, Tim}, title = {No-Regret Dynamics 230-242; Twenty Lectures on Algorithmic Game Theory}, year = {2016}, isbn = {131662479X}, publisher = {Cambridge University Press}, address = {USA}, edition = {1st}, abstract = {Computer science and economics have engaged in a lively interaction over the past fifteen years, resulting in the new field of algorithmic game theory. Many problems that are central to modern computer science, ranging from resource allocation in large networks to online advertising, involve interactions between multiple self-interested parties. Economics and game theory offer a host of useful models and definitions to reason about such problems. The flow of ideas also travels in the other direction, and concepts from computer science are increasingly important in economics. This book grew out of the author's Stanford University course on algorithmic game theory, and aims to give students and other newcomers a quick and accessible introduction to many of the most important concepts in the field. The book also includes case studies on online advertising, wireless spectrum auctions, kidney exchange, and network management.} }

@book{Wald1950,
	author = {Abraham Wald},
	editor = {},
	publisher = {Wiley: New York},
	title = {Statistical Decision Functions},
	year = {1950}
}

@article{Wang2023,
author = {Wang, Yizhun},
year = {2023},
month = {11},
pages = {233-239},
title = {Review on greedy algorithm},
volume = {14},
journal = {Theoretical and Natural Science},
doi = {10.54254/2753-8818/14/20241041}
}

@article{YPASY2018,
author = {Yun, Donggyu and Proutiere, Alexandre and Ahn, Sumyeong and Shin, Jinwoo and Yi, Yung},
title = {Multi-armed Bandit with Additional Observations},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3179416},
doi = {10.1145/3179416},
abstract = {We study multi-armed bandit (MAB) problems with additional observations, where in each round, the decision maker selects an arm to play and can also observe rewards of additional arms (within a given budget) by paying certain costs. In the case of stochastic rewards, we develop a new algorithm KL-UCB-AO which is asymptotically optimal when the time horizon grows large, by smartly identifying the optimal set of the arms to be explored using the given budget of additional observations. In the case of adversarial rewards, we propose H-INF, an algorithm with order-optimal regret. H-INF exploits a two-layered structure where in each layer, we run a known optimal MAB algorithm. Such a hierarchical structure facilitates the regret analysis of the algorithm, and in turn, yields order-optimal regret. We apply the framework of MAB with additional observations to the design of rate adaptation schemes in 802.11-like wireless systems, and to that of online advertisement systems. In both cases, we demonstrate that our algorithms leverage additional observations to significantly improve the system performance. We believe the techniques developed in this paper are of independent interest for other MAB problems, e.g., contextual or graph-structured MAB.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {apr},
articleno = {13},
numpages = {22},
keywords = {additional observations, budget; cost;, inf, kl-ucb, multi-armed bandit, online algorithm, reinforcement learning}
}

