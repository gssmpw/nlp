@string{anch-ie = {Angew.~Chem. Int.~Ed.}}
@string{cup     = {Cambridge University Press}}
@string{dtv     = {Deutscher Taschenbuch-Verlag}}
@string{hup     = {Harvard University Press}}
@string{jams    = {J.~Amer. Math. Soc.}}
@string{jchph   = {J.~Chem. Phys.}}
@string{jomch   = {J.~Organomet. Chem.}}
@string{pup     = {Princeton University Press}}

@incollection{westfahl:space,
  author       = {Westfahl, Gary},
  title        = {The True Frontier},
  subtitle     = {Confronting and Avoiding the Realities of Space in {American}
                  Science Fiction Films},
  pages        = {55-65},
  crossref     = {westfahl:frontier},
  langid       = {english},
  langidopts   = {variant=american},
  indextitle   = {True Frontier, The},
  annotation   = {A cross-referenced article from a \texttt{collection}. This is
                  an \texttt{incollection} entry with a \texttt{crossref}
                  field. Note the \texttt{subtitle} and \texttt{indextitle}
                  fields},
}

@set{set,
  entryset     = {herrmann,aksin,yoon},
  annotation   = {A \texttt{set} with three members.},
}

@set{stdmodel,
  entryset     = {glashow,weinberg,salam},
  annotation   = {A \texttt{set} with three members discussing the standard
                  model of particle physics.},
}

@article{aksin,
  author       = {Aks{\i}n, {\"O}zge and T{\"u}rkmen, Hayati and Artok, Levent
                  and {\c{C}}etinkaya, Bekir and Ni, Chaoying and
                  B{\"u}y{\"u}kg{\"u}ng{\"o}r, Orhan and {\"O}zkal, Erhan},
  title        = {Effect of immobilization on catalytic characteristics of
                  saturated {Pd-N}-heterocyclic carbenes in {Mizoroki-Heck}
                  reactions},
  journaltitle = jomch,
  date         = 2006,
  volume       = 691,
  number       = 13,
  pages        = {3027-3036},
  indextitle   = {Effect of immobilization on catalytic characteristics},
}

@inproceedings{Ismail2020, author = {Ismail, Mehmet S.}, title = {One for All, All for One---Von Neumann, Wald, Rawls, and Pareto}, year = {2020}, isbn = {9781450379755}, publisher = {Association for Computing Machinery}, address = {New York, NY, USA}, url = {https://doi.org/10.1145/3391403.3399460}, doi = {10.1145/3391403.3399460}, abstract = {Applications of the maximin criterion extend beyond economics to computer science, politics, and statistics. However, the maximin criterion---be it von Neumann's, Wald's, or Rawls'---draws fierce criticism due to its extremely pessimistic stance. I propose a novel concept, dubbed the optimin criterion, which is based on (Pareto) optimizing the minimal payoffs of tacit agreements. The optimin criterion generalizes and unifies results in various fields: It not only coincides with (i) maximin strategies in zero-sum games, (ii) the core in cooperative games when the core is nonempty, though it exists even if the core is empty, but it also generalizes (iii) Nash equilibrium in n-person constant-sum games, (iv) stable matchings in matching models, and (v) competitive equilibrium.}, booktitle = {Proceedings of the 21st ACM Conference on Economics and Computation}, pages = {763–764}, numpages = {2}, keywords = {noncooperative games, maximin criterion, cooperative games}, location = {Virtual Event, Hungary}, series = {EC '20} }

@book{Wald1950,
	author = {Abraham Wald},
	editor = {},
	publisher = {Wiley: New York},
	title = {Statistical Decision Functions},
	year = {1950}
}

@article{Savage1951,
	author = {Leonard J. Savage},
	journal = {Journal of the American Statistical Association},
	pages = {55--67},
	title = {The Theory of Statistical Decision},
	volume = {46},
	year = {1951}
}

@book{CLRS1990,
  author       = {Thomas H. Cormen and
                  Charles E. Leiserson and
                  Ronald L. Rivest and
                  Clifford Stein},
  title        = {Greedy Algorithms IV-15; Introduction to Algorithms, 3rd Edition},
  publisher    = {{MIT} Press},
  year         = {2009},
  url          = {http://mitpress.mit.edu/books/introduction-algorithms},
  isbn         = {978-0-262-03384-8},
  timestamp    = {Mon, 17 Aug 2020 11:36:12 +0200},
  biburl       = {https://dblp.org/rec/books/daglib/0023376.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Wang2023,
author = {Wang, Yizhun},
year = {2023},
month = {11},
pages = {233-239},
title = {Review on greedy algorithm},
volume = {14},
journal = {Theoretical and Natural Science},
doi = {10.54254/2753-8818/14/20241041}
}

@article{BGY2004,
title = "When the greedy algorithm fails.",
author = "J{\o}rgen Bang-Jensen and G. Gutin and A. Yeo",
year = "2004",
doi = "10.1016/j.disopt.2004.03.007",
language = "English",
volume = "1",
pages = "121--127",
journal = "Discrete Optimization",
issn = "1572-5286",
publisher = "Elsevier",
}

@inbook{BM2007, place={Cambridge}, title={Learning, Regret Minimization, and Equilibria}, booktitle={Algorithmic Game Theory}, publisher={Cambridge University Press}, author={Blum, Avrim and Mansour, Yishay}, editor={Nisan, Noam and Roughgarden, Tim and Tardos, Eva and Vazirani, Vijay V.Editors}, year={2007}, pages={79–102}}

@misc{MJK2022,
      title={Regret Minimization with Noisy Observations}, 
      author={Mohammad Mahdian and Jieming Mao and Kangning Wang},
      year={2022},
      eprint={2207.09435},
      archivePrefix={arXiv},
      primaryClass={cs.DS}
}

@misc{Rogers2011,
      title={Algorithmic Game Theory}, 
      author={Ryan Rogers},
      year={University Of Pennsylvania, 2011},
}

@misc{Jiao2021,
      title={Analysis of finite-arm i.i.d.-reward bandit; Theory of Multi-armed Bandits and Reinforcement Learning}, 
      author={Jiantao Jiao},
      year={Department of Electrical Engineering and Computer Sciences at University of California Berkeley, United States, 2021},
}


@InProceedings{FTYLWXWLXFY2022,
  title = 	 {Greedy when Sure and Conservative when Uncertain about the Opponents},
  author =       {Fu, Haobo and Tian, Ye and Yu, Hongxiang and Liu, Weiming and Wu, Shuang and Xiong, Jiechao and Wen, Ying and Li, Kai and Xing, Junliang and Fu, Qiang and Yang, Wei},
  booktitle = 	 {Proceedings of the 39th International Conference on Machine Learning},
  pages = 	 {6829--6848},
  year = 	 {2022},
  editor = 	 {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  volume = 	 {162},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {17--23 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v162/fu22b/fu22b.pdf},
  url = 	 {https://proceedings.mlr.press/v162/fu22b.html},
  abstract = 	 {We develop a new approach, named Greedy when Sure and Conservative when Uncertain (GSCU), to competing online against unknown and nonstationary opponents. GSCU improves in four aspects: 1) introduces a novel way of learning opponent policy embeddings offline; 2) trains offline a single best response (conditional additionally on our opponent policy embedding) instead of a finite set of separate best responses against any opponent; 3) computes online a posterior of the current opponent policy embedding, without making the discrete and ineffective decision which type the current opponent belongs to; and 4) selects online between a real-time greedy policy and a fixed conservative policy via an adversarial bandit algorithm, gaining a theoretically better regret than adhering to either. Experimental studies on popular benchmarks demonstrate GSCU’s superiority over the state-of-the-art methods. The code is available online at \url{https://github.com/YeTianJHU/GSCU}.}
}

@article{YPASY2018,
author = {Yun, Donggyu and Proutiere, Alexandre and Ahn, Sumyeong and Shin, Jinwoo and Yi, Yung},
title = {Multi-armed Bandit with Additional Observations},
year = {2018},
issue_date = {March 2018},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {2},
number = {1},
url = {https://doi.org/10.1145/3179416},
doi = {10.1145/3179416},
abstract = {We study multi-armed bandit (MAB) problems with additional observations, where in each round, the decision maker selects an arm to play and can also observe rewards of additional arms (within a given budget) by paying certain costs. In the case of stochastic rewards, we develop a new algorithm KL-UCB-AO which is asymptotically optimal when the time horizon grows large, by smartly identifying the optimal set of the arms to be explored using the given budget of additional observations. In the case of adversarial rewards, we propose H-INF, an algorithm with order-optimal regret. H-INF exploits a two-layered structure where in each layer, we run a known optimal MAB algorithm. Such a hierarchical structure facilitates the regret analysis of the algorithm, and in turn, yields order-optimal regret. We apply the framework of MAB with additional observations to the design of rate adaptation schemes in 802.11-like wireless systems, and to that of online advertisement systems. In both cases, we demonstrate that our algorithms leverage additional observations to significantly improve the system performance. We believe the techniques developed in this paper are of independent interest for other MAB problems, e.g., contextual or graph-structured MAB.},
journal = {Proc. ACM Meas. Anal. Comput. Syst.},
month = {apr},
articleno = {13},
numpages = {22},
keywords = {additional observations, budget; cost;, inf, kl-ucb, multi-armed bandit, online algorithm, reinforcement learning}
}

@misc{BHJK,
      title={The Unreasonable Effectiveness of Greedy Algorithms in Multi-Armed Bandit with Many Arms}, 
      author={Mohsen Bayati and Nima Hamidi and Ramesh Johari and Khashayar Khosravi},
      year={2022},
      eprint={2002.10121},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{JLP2021,
      title={Be Greedy in Multi-Armed Bandits}, 
      author={Matthieu Jedor and Jonathan Louëdec and Vianney Perchet},
      year={2021},
      eprint={2101.01086},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@book{TN16, author = {Roughgarden, Tim}, title = {No-Regret Dynamics 230-242; Twenty Lectures on Algorithmic Game Theory}, year = {2016}, isbn = {131662479X}, publisher = {Cambridge University Press}, address = {USA}, edition = {1st}, abstract = {Computer science and economics have engaged in a lively interaction over the past fifteen years, resulting in the new field of algorithmic game theory. Many problems that are central to modern computer science, ranging from resource allocation in large networks to online advertising, involve interactions between multiple self-interested parties. Economics and game theory offer a host of useful models and definitions to reason about such problems. The flow of ideas also travels in the other direction, and concepts from computer science are increasingly important in economics. This book grew out of the author's Stanford University course on algorithmic game theory, and aims to give students and other newcomers a quick and accessible introduction to many of the most important concepts in the field. The book also includes case studies on online advertising, wireless spectrum auctions, kidney exchange, and network management.} }

@inproceedings{ARS2008,
author = {Avramopoulos, Ioannis and Rexford, Jennifer and Schapire, Robert},
year = {2008},
month = {01},
pages = {},
title = {From Optimization to Regret Minimization and Back Again.}
}

@techreport{Flores2022,
title = "Learning by Convex Combination",
abstract = "We study how an agent evaluates the optimality of an action when she only observes a sample of its outcomes, not the outcome distribution. We characterize a model where the agent assigns an ex-ante utility to the action and then, upon seeing the sample, “updates” her evaluation by taking a convex combination of this ex-ante utility and the average utility of the outcomes in the sample. The weight on the average utility in this convex combination increases with sample size. Asymptotically, actions are evaluated using their sample average utility. The model includes Bayesian benchmarks as special cases. More generally, it describes an agent that may learn imperfectly yet consistently with a rough intuitive understanding of the Law of Large Numbers; it also enables decision-theoretic deﬁnitions of important concepts in the descriptive study of probabilistic judgement.",
keywords = "Sample, Sample size, Learning, Uncertainty, Sample, Sample size, Learning, Uncertainty",
author = "Karol Szwagrzak",
year = "2022",
language = "English",
series = "Working Paper / Department of Economics. Copenhagen Business School",
publisher = "Copenhagen Business School [wp]",
number = "16-2022",
address = "Denmark",
type = "WorkingPaper",
institution = "Copenhagen Business School [wp]",
}

@misc{YHLZM2022,
      title={Personalized Showcases: Generating Multi-Modal Explanations for Recommendations}, 
      author={An Yan and Zhankui He and Jiacheng Li and Tianyang Zhang and Julian McAuley},
      year={2023},
      eprint={2207.00422},
      archivePrefix={arXiv},
      primaryClass={cs.IR}
}

@inproceedings{HP2012, author = {Halpern, Joseph Y. and Pass, Rafael}, title = {Iterated regret minimization: a new solution concept}, year = {2009}, publisher = {Morgan Kaufmann Publishers Inc.}, address = {San Francisco, CA, USA}, abstract = {For some well-known games, such as the Traveler's Dilemma or the Centipede Game, traditional game-theoretic solution concepts--most notably Nash equilibrium--predict outcomes that are not consistent with empirical observations. We introduce a new solution concept, iterated regret minimization, which exhibits the same qualitative behavior as that observed in experiments in many games of interest, including Traveler's Dilemma, the Centipede Game, Nash bargaining, and Bertrand competition. As the name suggests, iterated regret minimization involves the iterated deletion of strategies that do not minimize regret.}, booktitle = {Proceedings of the 21st International Joint Conference on Artificial Intelligence}, pages = {153–158}, numpages = {6}, location = {Pasadena, California, USA}, series = {IJCAI'09} }

@article{AL2022,
  author  = {Daron Anderson and Douglas J. Leith},
  title   = {Expected Regret and Pseudo-Regret are Equivalent When the Optimal Arm is Unique},
  journal = {Journal of Machine Learning Research},
  year    = {2022},
  volume  = {23},
  number  = {293},
  pages   = {1--12},
  url     = {http://jmlr.org/papers/v23/20-1339.html}
}
@article{Kjeldsen2001,
title = "John von Neumann's conception of the minimax theorem: a journey through different mathematical contexts",
author = "Kjeldsen, {Tinne Hoff}",
year = "2001",
language = "English",
volume = "56",
pages = "39--68",
journal = "Archive for History of Exact Sciences",
issn = "0003-9519",
publisher = "Springer",
}

@article{ACF2002,
author = {Auer, Peter and Cesa-Bianchi, Nicolò and Fischer, Paul},
year = {2002},
month = {05},
pages = {235-256},
title = {Finite-time Analysis of the Multiarmed Bandit Problem},
volume = {47},
journal = {Machine Learning},
doi = {10.1023/A:1013689704352}
}
@article{RRKO2017,
  author       = {Daniel Russo and
                  Benjamin Van Roy and
                  Abbas Kazerouni and
                  Ian Osband},
  title        = {A Tutorial on Thompson Sampling},
  journal      = {CoRR},
  volume       = {abs/1707.02038},
  year         = {2017},
  url          = {http://arxiv.org/abs/1707.02038},
  eprinttype    = {arXiv},
  eprint       = {1707.02038},
  timestamp    = {Mon, 13 Aug 2018 16:46:49 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/0001RKO17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{MP2009,
      title={Empirical Bernstein Bounds and Sample Variance Penalization}, 
      author={Andreas Maurer and Massimiliano Pontil},
      year={2009},
      eprint={0907.3740},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/0907.3740}, 
}