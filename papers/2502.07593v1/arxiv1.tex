\newif\ifdraft
\drafttrue
%\draftfalse
\documentclass{article}
\usepackage{booktabs} % For formal tables
\usepackage[ruled]{algorithm2e} % For algorithms
\renewcommand{\algorithmcfname}{ALGORITHM}
\SetAlFnt{\small}
\SetAlCapFnt{\small}
\SetAlCapNameFnt{\small}
\SetAlCapHSkip{0pt}
\IncMargin{-\parindent}


%%% Load required packages here (note that many are included already).

\usepackage{balance} % for balancing columns on the final page

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


% Choose a citation style by commenting/uncommenting the appropriate line:
%\setcitestyle{acmnumeric}
%\setcitestyle{authoryear}

\usepackage{wrapfig}
\usepackage{amsmath,amsthm,amsfonts,mathtools}
\usepackage[toc,page]{appendix}
\usepackage{multicol}
\usepackage{color}
\usepackage{wrapfig}
\usepackage{ulem}
\definecolor{darkred}{rgb}{0.5,0,0}
\definecolor{lightblue}{rgb}{0,0.4,0.8}
\definecolor{darkgreen}{rgb}{0,0.5,0}
\usepackage[colorlinks=false, pdfborder={0 0 0}]{hyperref}
\usepackage{comment}
\usepackage[T1]{fontenc}
\usepackage{charter}
\usepackage{xfrac}
\usepackage{xspace}
\usepackage{xargs}
\usepackage{bbm}
\usepackage{longtable}
\usepackage{abstract}
\usepackage{lscape}
\usepackage{breqn}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage{dsfont}
\usepackage{float}

\renewcommand{\Pr}[1]{\mathds{P}\hspace{-0.1cm}\left[#1\right]}
\renewcommand{\epsilon}{\varepsilon}

\renewcommand{\arraystretch}{1.3}


\newcommand{\E}[1]{\mathds{E}\left[#1\right]}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{numresult}[theorem]{Numerical Result}
\newtheorem{simresult}[theorem]{Simulation Result}
\newtheorem{Definition}{Definition}
\newtheorem{remark}{Remark}
\newtheorem{corollary}{Corollary}
\newtheorem{example}{Example}
\newcommand{\fnote}[1]{{\ifdraft\color{teal} F: #1\fi}}
\newcommand{\mnote}[1]{{\ifdraft\color{brown} M: #1\fi}}
\newcommand{\knote}[1]{{\ifdraft\color{blue} K: #1\fi}}

\title{Decision-Making Under Complete Uncertainty: You Will Regret Not Being Greedy}

\author{%
	Kristijan Atanasov\textsuperscript{1}, Mehmet Ismail\textsuperscript{2} and Frederik Mallmann-Trenn\textsuperscript{1} 
}

\date{\footnotesize\textsuperscript{\textbf{1}}Department of Informatics, King's College London\\ \textsuperscript{\textbf{2}}Department of Political Economy, King's College London}

\begin{document}
\maketitle

\begin{abstract}
\noindent In this paper, we propose a probabilistic game-theoretic model to study the properties of the worst-case regret of the greedy strategy under complete (Knightian) uncertainty. In a game between a decision-maker (DM) and an adversarial agent (Nature), the DM observes a realization of product ratings for each product. Upon observation, the DM chooses a strategy, which is a function from the set of observations to the set of products. We study the theoretical properties, including the worst-case regret of the greedy strategy that chooses the product with the highest observed average rating. We prove that, with respect to the worst-case regret, the greedy strategy is optimal and that, in the limit, the regret of the greedy strategy converges to zero. We validate the model on data collected from Google reviews for restaurants, showing that the greedy strategy not only performs according to the theoretical findings but also outperforms the uniform strategy and the Thompson Sampling algorithm.
\end{abstract}


\setcounter{tocdepth}{2}
\tableofcontents


\section{Introduction} \label{section:intro_section}

% {\color{gray}Our research explores a two-player zero-sum game, where the first player serves as the decision maker (DM), and Nature acts as the opposing player \cite{Ismail2020}. The primary challenge for the decision maker is to make a selection from a set of products, where each product is associated with a value and historical (customer) ratings, akin to the review systems found on online platforms such as Amazon.  Although the decision maker has access to previous customer ratings of the products, they do not know their ground-truth value, i.e., the underlying distribution from which these products are drawn. 
% The goal for the DM is to choose the best product in a Savagian \cite{Savage1951} / Waldian \cite{Wald1950} sense: the reward for the DM is assessed w.r.t. to the worst-case underlying distribution of reviews.


% }
Suppose you are tasked with selecting a restaurant, guided by the establishment's star ratings on platforms such as Yelp or Google Maps. How big is the regret if you simply pick the restaurant with the highest average rating?
To answer this question, we introduce a game-theoretical model to study  the worst-case regret of the greedy strategy under complete (Knightian) uncertainty. In a game against Nature, the decision-maker (DM) observes a realization of a stochastic matrix consisting of product ratings for each product. Upon observation, the DM chooses a strategy, which is a function from the set of observations to the set of products (corresponding to restaurants in our example).

More concretely, the DM observes sampled ratings (e.g., reviews on Google Maps in our example) and chooses a strategy to pick a product (restaurant). Nature chooses a state, that is, a stochastic matrix of probability distributions over the product ratings, which is unobservable to the DM. 
We allow for any state and any strategy, meaning that both the DM and Nature have infinitely many (pure) strategies.

The basic structure of our framework dates back to Abraham Wald's maximin model \cite{Wald1950} and Leonard Savage's minimax regret model \cite{Savage1951}, both of which are based on worst-case principles. 
Wald's maximin criterion prescribes choosing the strategy that maximizes the minimum payoff, while Savage's criterion prescribes choosing the strategy that minimizes the maximum regret. The connection to our setting is that the DM tries to minimize the regret against the worst-case probability distribution.

In this paper, we study arguably the two most natural strategies the DM can employ: the \textit{greedy} strategy and the \textit{uniform} strategy. In the former case, the DM picks the product with the better average rating and breaks ties uniformly at random. The uniform strategy simply picks a product uniformly at random. We measure how these strategies perform in different scenarios, focusing on the worst-case. We track two metrics, the payoff that the DM receives from the game and the regret, which is defined as the difference between the expected payoff of the best possible alternative and the DM's product choice.
\subsection{Results Overview}

We outline six main findings. We show the maximum regret of the greedy strategy for different numbers of observations and explore how well this strategy works using real-life data from Google reviews of restaurants.

\begin{enumerate}
\item For two products and two ratings, we show that after seeing 1 observation (for each product) the greedy strategy has an expected regret of at most $\frac{1}{8}$ regardless of the underlying rating distributions (Proposition ~\ref{proposition:regretm1}). In other words, making a decision based only on 1 previous observation, the greedy strategy can extract at least $\frac{7}{8}$ of the value difference between the worst and the best product.

\item We further show that, with respect to the worst-case regret, the greedy strategy is optimal and guarantees the lowest possible regret  (Proposition~\ref{proposition:greedy_optimal}). 

\item We provide a method to calculate the worst-case regret of the greedy strategy as a function of the number of observations. When the number of observations increases, the maximum regret decreases, which we show numerically 
(Numerical Result~\ref{numresult:num}).

\item In the limit, given enough observations, we prove that the greedy strategy converges to a regret of 0. Additionally, given the number of products and ratings,
we calculate the sufficient number of observations required for the greedy strategy to achieve a regret of 0 for any specified probability (Theorem~\ref{theorem:theoremlimit}).

\item We show that the regret of the Thompson Sampling algorithm is higher than the regret of the greedy strategy, even as the number of observations goes to infinity. We further show the exact regret of Thompson Sampling, given an observation matrix (Proposition~\ref{proposition:regret_ts}).

\item We test the proposed model on data collected from Google reviews for restaurants \cite{YHLZM2022}. With ratings between 1 and 5, we  sample reviews uniformly at random to serve as initial observations from a dataset of 1.5 million observations and benchmark the regret of the greedy strategy on the subset of observations compared to when all observations are visible---treating this as the ground-truth. We show that the greedy strategy performs extremely well in comparison to the uniform strategy and similar to Numerical Result~\ref{numresult:num}. Additionally, greedy's 
regret decreases as observations increase (Simulation Result~\ref{theorem:simresult1}). We compare our model to the Thompson Sampling algorithm \cite{RRKO2017}, and observe that the greedy strategy consistently outperforms Thompson Sampling.
\end{enumerate}

\subsubsection{Industry Use-Cases}
In addition to the model test using Google reviews discussed in this paper, the model has a range of practical applications. For instance, in manufacturing and quality control, such as in the pharmaceutical, electronics, or food production industries, products are often produced in batches, each with its own quality distribution. This model can help quality control agents decide from which batch to select items for further processing or shipment based on observed quality data from different batches. For example, in pharmaceuticals, it can tell which batches to test more thoroughly to ensure safety. Knowing the number of observations (samples) tested and the sampling method (such as the greedy strategy), the model allows to bound the maximum regret of not detecting defective samples. Additionally, it can help determine the minimum number of samples to be tested to catch defects with high probability.


\subsection{Related Work}
In absence of complete knowledge, the greedy strategy chooses the option that, at every step, makes the locally optimal choice. 
Greedy algorithms \cite{CLRS1990,Wang2023}, have been studied for a long time and although they have applications in multiple problems, like finding shortest paths, minimum spanning trees and in the Knapsack problem, less so have they been tested in game theory. The greedy strategy in this paper corresponds to the Randomized Greedy Algorithm proposed by Blum and Mansour in 2007 \cite{BM2007}, where they tie break between equal observations with uniform choices. Their framework tests these strategies in a repeated game as opposed to a one-shot game and calculates regret as a function of time (steps the player has taken in the repeated game). The greedy strategy that we are analysing in this paper, also closely relates to recent research published by Mahdian et al. \cite{MJK2022} where they measure the regret of the greedy strategy when used to make decisions from noisy observations. Their findings show that the regret incurred by the greedy algorithm can surpass the optimal regret by an unbounded margin, even in cases where the noises are unbiased. In our model, the value of the products is calculated from discrete ratings and the observations correspond to past ratings of the available products. In this context, we are able to precisely establish an upper bound on the regret that the greedy strategy could experience, considering a specific number of products and observations.

As mentioned above, the motivation to find an upper bound on the regret comes from Savage's 1951 work \cite{Savage1951} in decision theory, which develops the minimax regret criterion designed to minimize the worst-case regret. Similar to Wald's maximin model \cite{Wald1950}, this has often been utilized to model choices under uncertainty, however, it assumes knowledge of the underlying distribution. Ismail \cite{Ismail2020} proposes the optimin criterion which coincides with Wald's maximin criterion in zero-sum games.

The concepts of playing a greedy strategy and minimizing regret are studied together by Rogers \cite{Rogers2011}. In his work on Algorithmic Game Theory, he shows how algorithms can learn from past observations to reduce their regret. Analogous to how Jiao \cite{Jiao2021} compares pseudo and worst-case expected regret in a multi-armed bandit problem, we introduce a worst-case expected regret for the greedy strategy and show that it approaches 0 in the limit as observations increase. 

Commonly, the greedy strategy is not known to perform well \cite{BGY2004, Wang2023}, however, as Bayati et al. \cite{BHJK} and Jedor et al. \cite{JLP2021} show, in specific instances of the multi-armed bandit problem it performs reasonably well. Bayati et al. \cite{BHJK} show that in the multi-armed bandit problem, with high probability one of the arms on which the greedy strategy concentrates attention is likely to have a high mean reward.

For the multi-armed bandit problem, when uncertain about the opponent, Fu et al. \cite{FTYLWXWLXFY2022} propose methods for inferring the opponent's strategy and using a greedy strategy to make decisions based on that. 
Furthermore, a version of the multi-armed bandit problem with additional observations has been proposed by Yun et al. \cite{YPASY2018}, where the decision maker besides being able to select an arm to play, they are also able to observe outcomes of additional arms by paying certain costs of a given budget. In our setting there is no cost assigned to the observations, however, they also come from an unknown distribution. 

Empirical Risk Minimization (ERM) is a framework for optimizing decisions based on minimizing average loss over observed data, assuming it is drawn i.i.d. from a fixed distribution. Maurer and Pontil \cite{MP2009} extend this idea with sample variance penalization (SVP) to improve generalization by penalizing high-variance samples. While we use similar methods to prove our results, these approaches are not applicable to our framework; ERM minimizes the expected empirical loss, aiming for good average-case performance over the observed data, whereas the regret in our setup measures the shortfall relative to the best possible decision, focusing on worst-case performance.

Regret minimization is commonly studied in no-regret learning algorithms, as discussed by Roughgarden et al. \cite{TN16} and Avramopoulos et al. \cite{ARS2008}. Similarly to the work by Blum and Mansour however, that is measured in repeated settings with respect to the number of rounds. Lastly, although a method involving convex optimization techniques, Flores-Szwagrzak \cite{Flores2022} shows how an agent can utilize observations from an unknown distribution, to consistently learn and improve their payoff.




\section{The Model} 
Let $G=(\Sigma_1, \mathcal{S}, \pi)$ be a two-player zero-sum game against Nature, with player one called the Decision Maker (DM), and player two called Nature. In what follows, we define the notation and the terminology used in this paper. 

\noindent Let $n_d \text{ and }n_r\in \mathbb{N}$.
We use
$D = \{1,2,...,n_d\}$ to denote the set of \textit{products}  and $R=\{1,2,...,n_r\}$ to denote the set of discrete \textit{ratings}, where we assume that  $n_r \geq 2$. Nature decides the ratings for each product $d$ with a probability distribution unknown to the DM. The probability distributions for each product and rating are defined in a state (stochastic matrix) $S$. Similarly, an observation matrix $B$ reflects the samples from $S$. The decision-maker picks a strategy $\sigma_1 \in \Sigma_1$ that selects a product based on the observed ratings in $B$ which are sampled from an unknown stochastic-matrix $S \in \mathcal{S}$, with the aim to maximize its expected rating. We use $\pi$ as the payoff function that determines the payoff in the game.


\subsection{States} \label{section:model_states}
Since each product $d$ has a probability distribution for its ratings, the set of products $D$ naturally induces a column-stochastic matrix $S \in [0,1]^{n_r \times n_d}$ called a \textit{state}:
\[
S = 
\begin{bmatrix}
s_{1,1} & s_{1,2} &\cdots & s_{1, n_d}\\
\vdots & \vdots & \ddots & \vdots \\
s_{n_r, 1} & s_{n_r, 2} & \cdots &  s_{n_r, n_d}
\end{bmatrix}. 
\]
Each element $s_{r,d}$ of a state gives the probability that a product $d \in D$ receives a rating $r \in R$, hence, for all $d \in D$, $\sum_{r \in R} s_{r,d}=1$. Let $\mathcal{S}=\{S \in [0,1]^{n_r \times n_d} ~|~ \text{where for all } d \in D, \sum_{r \in R}s_{r,d}=1\}$ be the set of all possible states. \newline The following is an example of a state with two products as the matrix columns and two ratings as its rows:
\[
\tilde{S}_1=
\begin{bmatrix} 
0.7 & 0.4\\
0.3 & 0.6
\end{bmatrix}.
\]
In this state, there is a 0.7 probability that product 1 receives a rating of 1 and a 0.3 probability that it receives a rating of 2. In addition, product 2 has a 0.4 probability of receiving a rating of 1 and 0.6 probability of receiving a rating of 2.

\subsection{Observations}
\label{sec:observations}
For each product, the DM observes the number of times each rating the product receives. However, the DM does not observe the state chosen by Nature. In this paper, we only allow an equal number of observations $m$ across all products.\footnote{In Section \ref{sec:future_work} we propose future work for the case when the number of observations per product is different.} Let $B \in \mathbb{N}^{n_r \times n_d}$ denote an \textit{observation matrix}, where each element $b_{r,d}$ of the matrix represents the number of observations for product $d$ receiving rating $r$:
\[
B = 
\begin{bmatrix}
b_{1,1} & b_{1,2} &\cdots & b_{1, n_d}\\
\vdots & \vdots & \ddots & \vdots \\
b_{n_r, 1} & b_{n_r, 2} & \cdots &  b_{n_r, n_d}
\end{bmatrix}. 
\]
We assume that these observations are mutually independent. We consider the set $\mathcal{B}_m$ of possible observations, defined by
\[
\mathcal{B}_m =
\{ B \in \mathbb{N}^{n_r \times n_d} \; | \text{ where for all } d\in D, \sum_{r\in R} b_{r,d} = m   \},
\]
where $m \in \mathbb{N}$ denotes the total number of observations across all ratings for any product. For example when $m=1$, then there are four possible observation matrices $\mathcal{B}_1=\{\tilde{B}_1, \tilde{B}_2, \tilde{B}_3, \tilde{B}_4\}$:
\begin{align*}
\label{eq:B}
\tilde{B}_1 = \begin{bmatrix} 
1 & 1\\
0 & 0
\end{bmatrix}\text{, }
\tilde{B}_2 = \begin{bmatrix} 
1 & 0\\
0 & 1
\end{bmatrix} \text{, }
\tilde{B}_3 = \begin{bmatrix} 
0 & 1\\
1 & 0
\end{bmatrix} \text{and }
\tilde{B}_4 = \begin{bmatrix} 
0 & 0\\
1 & 1
\end{bmatrix}.
\end{align*}
\newline
Denote by $B_{\cdot,d}$ the observation vector corresponding to product $d$, and by $\Pr{B_{\cdot,d} | S}$ the conditional probability  of observing each rating given state $S$.
For a given product $d$ and state $S$, its rating $r$ is drawn from a multinomial distribution characterized by a number of trials $m$ and a vector of probabilities $p=(p_{r_1}, p_{r_2},...,p_{r_n})$, where $p_i = S_{r_i, d}$. Let $Z$ denote a random variable which follows a multinomial distribution with the parameters $m$ and $p$. Observe that with $Z$, the probability of an observation vector per product $d$, $B_{\cdot, d}$ occurring given a state $S$ is:


\begin{align*}
\Pr{B_{\cdot,d} | S}=P(Z_1=b_{1, d}, Z_2=b_{2,d}, ...,Z_{n_r} = b_{n_r, d})= \\ = \frac{m!}{\prod_{r}b_{r,d}!}\prod_{r}S_{r,d}^{b_{r,d}}\in [0,1] \text{ for } r\in R.
\end{align*} \newline
By the independence of these distributions, the probability of observing matrix $B$ given $S$ is

\begin{align*}
\Pr{B|S} = \prod_{d \in D}\Pr{B_{\cdot,d} | S} =\frac{(m!)^{n_d}}{\prod_{r,d}b_{r,d}!}\prod_{r,d} S_{r,d}^{b_{r,d}} \in [0,1] \text{ for } r\in R,d\in D.
\end{align*} The first term consists of the multinomial coefficent, and the second term is the probability that a given product and rating combination is observed $b_{r,d}$ times.
\subsubsection*{Illustrative example}\label{sec:illustrative_ex}

Consider the following observation matrix for 2 products and 2 ratings:
\[ 
\tilde{B} = \begin{bmatrix} 
1 & 0\\
2 & 3
\end{bmatrix}.
\]
This matrix shows that, for product 1, the DM observed rating 1 once and rating 2 twice. For product 2, the DM observed rating 2 three times.

\noindent To illustrate our framework intuitively, consider a situation where the DM makes three `draws' (observations) from two urns (products), each containing balls marked 1 or 2 (ratings). These draws follow the probability distributions in a given state. For the example of $\tilde{S}_1$, we would obtain the number of observations given in matrix $\tilde{B}$ when the DM draws, with replacement, one ball marked 1 with a probability of 0.7 and two balls marked 2, each with a probability of 0.3, from urn 1. From urn 2, she draws three balls marked 2, each with a probability of 0.6.

\noindent For product 1 in our illustrative example,
$
\Pr{\tilde{B}_{\cdot,1} | \tilde{S}_1} = \binom{3}{1} \cdot  0.7 \cdot (0.3)^2 = 3 \cdot 0.063 = 0.189.
$
This is because the probability of `drawing' a rating of one is 0.7 and 2 ratings of two is $(0.3)^2$. We multiply this by the Binomial coefficient $\binom{3}{1}$ because there are three different combinations in which one can draw 1 rating of one and 2 ratings of two. Similarly
$
\Pr{\tilde{B}_{\cdot,2} | \tilde{S}_{1}} = \binom{3}{0}  \cdot 0.216 = 0.216.
$
Therefore, the probability of observing $\tilde{B}$ in state $\tilde{S}_1$ is
$
\Pr{\tilde{B}|\tilde{S}_{1}} = \prod_{d \in \{1,2\}}\Pr{\tilde{B}_{\cdot,d} | \tilde{S}_{1}} = 0.040824.
$\newline 

Table~\ref{tab:prob_obs} presents the probability of each observation matrix for $m=1$ occurring for $\tilde{S}_1$.

\begin{table}[h]
\centering
\begin{tabular}{l|cc}
 & $\Pr{B|\tilde{S}_1}$                          \\ \hline
$\tilde{B}_1$      & 0.28\\ 
$\tilde{B}_2$      & 0.42 \\ 
$\tilde{B}_3$      & 0.12  \\ 
$\tilde{B}_4$      & 0.18  \\ 
\end{tabular}
\caption{Probability of each observation $B \in \mathcal{B}_1$ occurring with $\tilde{S}_1$ }
\label{tab:prob_obs}
\end{table}

\subsection{Strategies}
A \textit{strategy} for the DM is a function $\sigma_1:\mathcal{B}_m \rightarrow \mathcal{D}$ where $\mathcal{D}=\{p:D\rightarrow [0,1] ~|~ \sum_{d \in D}p(d)=1\}$ which assigns a probability distribution over the set of products $D$ for every observation matrix $B \in \mathcal{B}_m$. For a given observation matrix $B$, let $\Sigma_1$ be the set of all probability distributions over $D$:
\[\Sigma_1=\{\sigma_1(B) \in [0,1]^{n_d} ~|~ \sum_{d \in D}\sigma_1(B)(d)=1\}.\] \newline
Nature's strategy is to pick any state $S \in \mathcal{S}$, such that it maximizes the DM's regret.

\subsection{Payoff}
 
The expected value, $V_S:D \rightarrow \mathbb{R}$, of a product given state $S$ is defined as follows
\[
V_S(d) = \mathbb{E}[d|S] = \sum_{r \in R} r\cdot s_{r,d}.
\]
For the $\tilde{S}_1$ state, the values of the products are the following, $V_{\tilde{S}_1}(1)=1.3$ and $V_{\tilde{S}_1}(2)=1.6$; where we used the first and second column of $\tilde{S}_1$ to compute $V_{\tilde{S}_1}(1)$ and $V_{\tilde{S}_1}(2)$ respectively.

Let $\pi : \mathcal{B}_m \times D\rightarrow \mathbb{R}$ denote the DM's \textit{expected payoff function}. For a given strategy $\sigma_1$ and a state $S$, the payoff for the DM is:
\[\pi(\sigma_1,S) = \sum_{B \in \mathcal{B}_m}\Pr{B|S}\sum_{d \in D}\sigma_1(B)(d)V_S(d).\]
\subsection{Regret}
We now introduce the notation for \textit{regret}, which is inspired from the notation used in the multi-armed bandit literature \cite{Jiao2021}. For a given state of Nature $S$, the (expected) regret for the DM not selecting the highest value product in that state is
\[
\bar{\gamma}(\sigma_1, S) = \max_{d \in D} V_S(d) - \pi(\sigma_1, S).
\] Since the regret is measured only for the DM, it is non-negative.
Note that, compared to the multi-armed bandit problem, in our setup, there is only one round (the DM makes only one choice), with one optimal choice to compare to (the highest value product), which makes the pseudo regret equal to the expected regret. Similarly, Anderson and Leith \cite{AL2022} show that in the multi-armed bandit problem, the expected regret and the pseudo-regret are equivalent when the optimal arm is unique.


\begin{Definition}
For a strategy $\sigma_1$, we define its worst-case regret, $\gamma(\sigma_1)$, as follows:
\[
\gamma(\sigma_1) = \max_{S \in \mathcal{S}} \left[ \max_{d \in D} V_S(d) - \pi(\sigma_1, S) \right].
\]
\end{Definition}

\noindent In words, the regret of strategy $\sigma_1$ is the difference in expected payoff between $\sigma_1$ and the highest valued product at some state $S \in \mathcal{S}$ such that this difference is maximized.




\section{Strategies for the Decision Maker}
\label{section:greedy_strategy}
In this section we define strategies for the DM that select products based on the available observations, without knowing the state distribution.

The \textit{uniform strategy}, denoted as $\sigma_u$, selects each product $d \in D$ with equal probability, $\sigma_u(B) \sim \text{Uniform}(D)$, regardless of the observation matrix $B$. We use the uniform strategy as a benchmark to compare against the greedy strategy. The payoff of the uniform strategy is equal to the simple mean of the expected value of the products, for example $\pi(\sigma_u, \tilde{S}_1)=1.45$. And the regret is the difference of the highest-valued product and the uniform's strategy payoff: $V_{\tilde{S}_1}(2)-\pi(\sigma_u, \tilde{S}_1)=0.15$. \newline

The \textit{greedy strategy} selects the highest rated products from the available observations.
Let $V_B:D \rightarrow [1,n_r]$ be the function that calculates the observations weighted value of a product given an observation matrix $B$: 
\[
V_B(d) =  \sum_{r \in R} \frac{rb_{r,d}}{m}.
\]
Let $d^*\in \arg \max_{d\in D} V_B(d)$ be a product with the highest observed rating given $B$. The choice of products with the \textit{greedy strategy} is defined as follows: 
\[
\sigma_g(B)\sim \text{Uniform}\left( \arg \max_{d\in D} V_B(d) \right).
\]

\noindent In words, if there is a unique $d^*$ , then the greedy strategy chooses $d^*$ with probability 1. If there are multiple maximizers, then the greedy strategy is the uniform distribution over the set of maximizers in $\arg \max_{d\in D} V_B(d)$. 
Table~\ref{tab:greedy_ex} presents a simple example of a greedy strategy choice of products.
\begin{table}[h]
\centering
\begin{tabular}{l|l|l}
$\sigma_g$ & $1$& $2$ \\ \hline
$\tilde{B}_1$ & 0.5 & 0.5  \\ 
$\tilde{B}_2$ & 0 & 1  \\ 
$\tilde{B}_3$ & 1 & 0  \\ 
$\tilde{B}_4$ & 0.5 & 0.5  \\ 
\end{tabular}
\quad \quad \quad

\caption{The greedy strategy $\sigma_g$ for observations in $\mathcal{B}_1$}
\label{tab:greedy_ex}
\end{table}

\noindent Given $\tilde{B}_1$ or $\tilde{B}_4$, the greedy strategy uniformly chooses product 1 and product 2. This is due to the identical ratings observed for both products, and hence the greedy strategy cannot determine which product is better given $\tilde{B}_4$, or conversely, which is worse given $\tilde{B}_1$. In contrast, for $\tilde{B}_2$, product 2 has a rating of 2 and product 1 has a rating of 1. Therefore, the greedy strategy chooses product 2 with a probability of 1 given $\tilde{B}_2$, and similarly, it chooses product 1 with a probability of 1 given $\tilde{B}_3$. \newline
Consider the computed probabilities of each observation matrix $B \in \mathcal{B}_1$ given in Table \ref{tab:prob_obs}, as well as the greedy strategy product choices for the same observation matrices given in Table \ref{tab:greedy_ex}. For instance having observed $\tilde{B}_1$ from $\tilde{S}_1$, greedy plays both products uniformly and it's probability weighted outcome is $\Pr{\tilde{B}_1|\tilde{S}_1}\left(\sigma_g(\tilde{B}_1)(1)V_{\tilde{S}_1}(1) +\sigma_g(\tilde{B}_1)(2)V_{\tilde{S}_1}(2)\right)=0.28\left(\frac{1.3}{2} + \frac{1.6}{2}\right)=0.406$. Computing this across all observation matrices $B \in \mathcal{B}_1$, we obtain the expected payoff of the greedy strategy: $\pi(\sigma_g, \tilde{S}_1)=1.495$. And its regret is the payoff difference of the highest-rated product and the greedy strategy payoff, in this case $V_{\tilde{S}_1}(2)-\pi(\sigma_g, \tilde{S}_1)=1.6-1.495=0.105$.

\subsection{Upper Confidence Bound Algorithm}
Auer et al. \cite{ACF2002} introduce the Upper Confidence Bound (UCB) algorithm for the multi-armed bandit problem that balances exploration and exploitation by selecting products that maximize an upper bound on the estimated payoff. The upper bound is the observed value of a product given an observation matrix $V_B$ plus a term for uncertainty $c$. The uncertainty term $c=\sqrt{\frac{2\log{({n_d}^2m)}}{m}}$ is designed for cases where products have different number of observations, such that it decreases the more observations a product has, eventually selecting less-explored products.
\[\sigma_{UCB}(B) \sim \text{Uniform}\left(\arg \max_{d \in D} V_B(d) + \sqrt{\frac{2\log{({n_d}^2m)}}{m}} \right)\]
In our case for a fixed number of observations $m$ across all products, the term $c$ would be the same for any product, resulting in $\sigma_{UCB}=\sigma_g$. For a fixed number of observations $m$, since everything we find for the greedy strategy applies for UCB as well, we will not study it here.


\subsection{Thompson Sampling}

Thompson Sampling is a Bayesian algorithm for the multi-armed bandit problem that, like UCB, balances between exploration and exploitation \cite{RRKO2017}. It does this by sampling once from the posterior distribution of each product's observed ratings and choosing the product with the highest sampled value. In our setup that would be the Dirichlet distribution\footnote{Beta distribution if there are only two ratings.} per product $d$, $\text{Dir}(\alpha^{(d)})$ where $\alpha^{(d)} = B_{\cdot, d}$.\footnote{The Beta distribution is defined only when its parameters $\alpha$ and $\beta$ are positive. For practical implementations when we have zero observations of a particular rating for a product, those can be set to small positive values.} Let $Y_d\sim\text{Dir}(\alpha^{(d)}) \text{ for } d \in D$, be random variables following the Dirichlet distribution for each product, Thompson Sampling then selects from the best samples:
\[\sigma_{TS}(B)\sim \text{Uniform}\left(\arg \max_{d \in D} Y_d\right).\] 
Consider the following observation matrix $\tilde{B}_5 =\begin{bmatrix} 
7 & 5\\
2 & 4
\end{bmatrix}$, in this case, the Thompson Sampling algorithm would have two random variables $Y_1 \sim \text{Beta}(2,7)$ and $Y_2 \sim \text{Beta}(4,5)$ for product one and two respectively. It would play the product which drawn sample has a higher rating --- $\text{Uniform}(\arg \max(Y_1, Y_2))$. 

\noindent Let $\tilde{B}_5$ be sampled from $\tilde{S}_1$ where $V_{\tilde{S}_1}(2) > V_{\tilde{S}_1}(1)$. As it follows from the regret calculation for the Thompson Sampling algorithm in Section \ref{sec:gt_1}, the algorithm incurs regret only when it samples a higher rating for the lower valued product or $P(Y_1>Y_2)$, thus, the regret for $\tilde{B}_5$ is: \[\Pr{\tilde{B}_5 | \tilde{S}_1}P(Y_1>Y_2)(V_{\tilde{S}_1}(2)-V_{\tilde{S}_1}(1))\approx0.0019.\]




\section{Maximum Regret for the Greedy Strategy}
In this section we evaluate the maximum regret for the greedy strategy as number of observations increases for each product. Starting with only one observation, and ending with a proof that greedy's regret reaches 0 with number of observations in the limit. Additionally, we prove that the greedy strategy is optimal with respect to the worst-case regret and that no other strategy $\sigma_1 \in \Sigma_1$ gives lower regret. Lastly, we show that the greedy strategy outperforms the Thompson Sampling algorithm even as the number of observations increases to infinity.

\subsection{With one observation per product} \label{section:regret_m1}
Since we measure the maximum regret across any possible state, without loss of generality we can pick a single generic state, assuming that it maximizes the regret. For $p_1, p_2 \in [0,1]$, define
\[S'=
\begin{bmatrix} 
p_1 & p_2\\
1 - p_1 & 1 - p_2
\end{bmatrix}.
\]
The following proposition shows that that for two products, that also have two ratings and with one observation in the game, the worst-case regret for the greedy strategy is $\frac{1}{8}$.
\begin{proposition}\label{proposition:regretm1}
For $n_d=2$, $n_r=2$ and $m=1$, the worst-case regret of the greedy strategy is $\frac{1}{8}$.
\end{proposition}
\begin{proof} 
Given $S'$ the value of each product is: $V_{S'}(1)=2-p_1$ and $V_{S'}(2)=2-p_2$.
In Table~\ref{tab:greedy_p_prob} below, we compute the probability of each observation $B \in \mathcal{B}_1$ occurring, given the state $S'$.

\begin{table}[H]
\centering
\begin{tabular}{l|l}

 &  $\Pr{B|S'}$ \\ \hline
$\tilde{B}_1$ & $p_1p_2$ \\ \hline
$\tilde{B}_2$ &   $p_1(1-p_2)$ \\ \hline
$\tilde{B}_3$ & $(1-p_1)p_2$ \\ \hline
$\tilde{B}_4$ &   $(1-p_1)(1-p_2)$ \\ 
\end{tabular}
\caption{Probability of each observation $B \in \mathcal{B}_1$ with $S'$}
\label{tab:greedy_p_prob}
\end{table}

\noindent Without loss of generality assume $p_1 < p_2$, the first product has higher value and it is always best to pick product one with value $2-p_1$. Thus, the regret of choosing the second product is:
$2-p_1 - (2-p_2) = p_2 - p_1$.
\newline \newline
The payoff of the greedy strategy in this setting is:
\[\pi(\sigma_g, S') = 2-\frac{p_1+p_2}{2}+\frac{(p_1-p_2)^2}{2}.\]

\noindent And the regret $\bar{\gamma}$:
\[\bar{\gamma}(\sigma_g, S') = (2-p_1)-\pi(\sigma_g, S') = -\frac{p_1}{2} + \frac{p_2}{2} - \frac{(p_1-p_2)^2}{2}.\]
We calculate what is the maximum regret that $\bar{\gamma}(\sigma_g, S')$ can have:
\[ \frac{\partial}{\partial p_1}\bar{\gamma}(\sigma_g, S') = -p_1 + p_2 - \frac{1}{2} \text{ and } \frac{\partial}{\partial p_2}\bar{\gamma}(\sigma_g, S') = p_1 - p_2 + \frac{1}{2}.\]

\[\text{From }\frac{\partial}{\partial p_1}\bar{\gamma}(\sigma_g, S')=0 \text{ and } \frac{\partial}{\partial p_2}\bar{\gamma}(\sigma_g, S')=0, \text{ it follows that }p_2 = p_1 + \frac{1}{2}.\]
The second partial derivatives test for $\bar{\gamma}(\sigma_g, S')$ is inconclusive, because the determinant of its Hessian is 0. However, the sign of $\frac{\partial^2}{\partial p_1^2}\bar{\gamma}(\sigma_g, S')=\frac{\partial^2}{\partial p_2^2}\bar{\gamma}(\sigma_g, S')=-1$ gives concavity along both axes. Additionally, from the first derivative, we know that the set of critical points is a line $p_2 = p_1 + \frac{1}{2}$, we also know that the minimum regret for the greedy strategy is 0 whenever $p_1=p_2$. Knowing the above, we calculate $\gamma(\sigma_g)$ at $p_2 = p_1 + \frac{1}{2}$
\[ \gamma(\sigma_g) = -\frac{p_1}{2} + \frac{p_1+\frac{1}{2}}{2} - \frac{(p_1 - (p_1 + \frac{1}{2}))^2}{2}= \frac{1}{8}.\]
For two products, two ratings, after one observation and any number of states, the regret that the greedy strategy can get is at most $\frac{1}{8}$.

In comparison, the worst-case regret for the uniform strategy for $n_d=n_r=2$ and any $m \in \mathbb{N}$ is $\frac{1}{2}$, and that occurs anytime $p_1=1$ and $p_2=0$ or $p_1=0$ and $p_2=1$.
\end{proof}


\subsection{Optimality of the Greedy Strategy (regret lower bound)}

{ In Section \ref{section:regret_m1} we show that the worst-case regret of the greedy strategy is $\frac{1}{8}$. Here we show that for the same setting, the greedy strategy is optimal and $\frac{1}{8}$ is the lowest worst-case regret any strategy can achieve for any state of Nature.

\begin{proposition}\label{proposition:greedy_optimal}
For $n_d=2$, $n_r=2, m=1$ and for any state of Nature, no strategy of the DM $\sigma_1 \in \Sigma_1$ can have a worst-case regret strictly lower than $\frac{1}{8}$.
\end{proposition}

\begin{proof} We prove this by contradiction, for the sake of the contradiction consider there is a fixed strategy for the DM $\sigma_1 \in \Sigma_1$ with $\gamma(\sigma_1)<\frac{1}{8}$. Consider the two states $S_1, S_2 \in \mathcal{S}$:
\[S_1 = \begin{bmatrix} 0.5 & 0 \\
0.5 & 1 \\ \end{bmatrix} \text{ and } 
S_2 = \begin{bmatrix} 0 & 0.5 \\
1 & 0.5 \\ \end{bmatrix}.\]

\noindent Consider the regret incurred for the observation matrix $\tilde{B}_4$, recall that this is the observation matrix where both products have exactly 1 two star rating. The probability of $\tilde{B}_4$ occurring is, $\Pr{\tilde{B}_4|S_1}=\Pr{\tilde{B}_4|S_2}=\frac{1}{2}$. The value difference between the two products is $V_{S_1}(2)-V_{S_1}(1)=V_{S_2}(1)-V_{S_2}(2)=\frac{1}{2}$. For simplicity, let $p=\sigma_1(\tilde{B}_4)(1)$ and $(1-p) =\sigma_1(\tilde{B}_4)(2)$, i.e., the probabilities with which a strategy $\sigma_1$ selects product one and two respectively. Note that, $\sigma_1$ has an arbitrary but fixed value of $p$. We show that, regardless of the chosen value of $p$, either $S_1$ or $S_2$ will incur a worst-case regret larger than $\frac18$.

\noindent With $\tilde{B}_4$, for $S_1$ and $S_2$ the regret reduces to:
\[\bar{\gamma}(\sigma_1, S_1)=p\Pr{\tilde{B}_4|S_1}\left(V_{S_1}(2)-V_{S_1}(1)\right)=\frac{p}{4} \text{ and}\]
\[\bar{\gamma}(\sigma_1, S_2)=(1-p)\Pr{\tilde{B}_4|S_2}\left(V_{S_2}(1)-V_{S_2}(2)\right)=\frac{(1-p)}{4} \text{ respectively.}\]

\noindent For the fixed strategy $\sigma_1$, by assumption the regret is smaller than $\frac18$ and thus:
\[\text{for } S_1 \text{, } \bar{\gamma}(\sigma_1, S_1)<\frac{1}{8} \text{ or } \frac{p}{4}<\frac{1}{8} \rightarrow p < \frac{1}{2}\text{, and}\]
\[\text{for } S_2 \text{, }\bar{\gamma}(\sigma_1, S_2)<\frac{1}{8} \text{ or }\frac{(1-p)}{4}<\frac{1}{8}\rightarrow p > \frac{1}{2}.\]
This contradiction yields the claim. 
\end{proof}

\begin{corollary}[Optimality of greedy]
Proposition \ref{proposition:greedy_optimal} shows that the lowest worst-case regret achievable by any strategy $\sigma_1 \in \Sigma_1$ is $\frac{1}{8}$. Furthermore, Proposition \ref{proposition:regretm1} shows that $\frac{1}{8}$ is the maximum regret for the greedy strategy across all possible states of Nature, $S \in \mathcal{S}$. Consequently, we conclude that the greedy strategy is optimal with respect to the worst-case regret.
\end{corollary}
}


\subsection{With $2\leq m \leq 20$ observations per product }\label{section:regret_m12}
Here we increase the number of observations up to $m=20$, but still consider two products and two ratings. In order to calculate what is the maximum regret with the greedy strategy as the observations increase, we express $\gamma$ with respect to $p_1$ and $p_2$ from the $S'$ state in Section \ref{section:regret_m1}, and numerically solve for its maximum.
\begin{numresult}\label{numresult:num}
For $n_d=n_r=2$ and for any state $S \in \mathcal{S}$, Figure~\ref{fig:numresult} presents a numerical bound for the worst-case regret of the greedy strategy as $m$ increases up to 20.
\end{numresult}


\begin{figure}[H]
\caption{Maximum regret of the greedy strategy choosing between two products as number of observations increase up to 20}
\label{fig:numresult}
\centering
\includegraphics[scale=0.4]{max_greedy_regret.png}
\end{figure}
\noindent From Figure \ref{fig:numresult}, we notice that the regret is steadily decreasing, after 10 observations the maximum regret would have decreased at least three fold from $\frac{1}{8}$ down to $\approx\frac{1}{24}$.




%%%%%%%%%%%%%%%%%
\subsection{The number of observations per product goes to infinity}
In this section, we demonstrate that in the limit, for a game with any number of products and any number of ratings, as the number of observations increases, the regret of the greedy strategy approaches zero. Additionally, for any set of states, we give a concrete bound on the number $m$ of observations that are sufficient for greedy to give 0 regret for any given probability bounded away from 1.
\begin{theorem}\label{theorem:theoremlimit}
For any $\delta\in(0,1/2]$, any state $S \in \mathcal{S}$, there exists an $m\in \mathbb{N}$ such that if we have at least $m$ observations per product, then the greedy strategy has a regret of 0 with probability $1-\delta$. Additionally, $\gamma(\sigma_g) \rightarrow 0$ as $m \rightarrow \infty$.
\end{theorem}
\begin{proof}
Since we measure the maximum regret across all states, without loss of generality we can pick a single state $S^*$ assuming that its products $\{d_1,d_2,\cdots,d_{n_d}\}$ are picked to yield largest regret. Without loss of generality we also sort the products in the state, with the highest value product at the start.
With $V_{S^*}(d_1) > V_{S^*}(d_i) \; \forall \; i \in \{1,2,\cdots,n_d\}$, the first product has the highest value and it is always best to pick product 1. The maximum regret is then $V_{S^*}(d_1) - V_{S^*}(d_{n_d}) \leq (n_r-1)$.
We introduce a simpler version of the regret calculation for the greedy strategy, which accounts for all cases where greedy does not pick product 1:


\begin{align*}\bar{\gamma}(\sigma_g, S^*) = \\ =\sum_{B \in \mathcal{B}_m}\Pr{B|S^*}\sum_{i=1}^{n_d} \sigma_g(B)(i)\left(V_{S^*}(d_1) - V_{S^*}(d_i)\right)\leq V_{S^*}(d_1) - V_{S^*}(d_{n_d}) \leq  \\ \leq n_r-1,\end{align*}
$\sigma_g(B)(i)$ greedy picks product $i$, only when it yields more observations of higher rating than any other product $i'$. For the regret to be 0, greedy has to pick product 1 with probability $1 - \delta$, and the rest of the products with probability $\delta$. It is sufficient then to show that for any $B$, $(1-\sigma_g(B)(1))\leq\delta$.
\newline Let $Y_i$ for  $i \in \{1,2, \cdots, n_d\}$ be random variables, of the sum of the observed product values per product $i$. For the products with largest and second largest value respectively, we have:
\[\mathbb{E}[Y_1] = mV_{S^*}(d_1) \text{ and } \mathbb{E}[Y_2] = mV_{S^*}(d_2)\]
We further calculate the half distance $l$ between $Y_1$ and $Y_2$:
\[l = \frac{\mathbb{E}[Y_1] - \mathbb{E}[Y_2]}{2} = \frac{mV_{S^*}(d_1) - mV_{S^*}(d_2)}{2} = \frac{m(V_{S^*}(d_1)-V_{S^*}(d_2))}{2}.\]
Knowing that
$V_{S^*}(d_1)\geq V_{S^*}(d_2) \geq \cdots \geq V_{S^*}(d_{n_d})$:
\begin{eqnarray*}
(1-\sigma_g(B)(1)) \leq P(Y_1 \leq \mathbb{E}[Y_1] - l \lor Y_2 \geq \mathbb{E}[Y_2] + l \lor \cdots \lor Y_{n_d} \geq \\ \geq \mathbb{E}[Y_{n_d}] + l ) \leq P(Y_1 \leq \mathbb{E}[Y_1] - l) + (n_d-1)P(Y_2 \geq \mathbb{E}[Y_2] + l) \leq \delta.
\end{eqnarray*}
Given $Y_1$ and $Y_2$ are independent, using Hoeffding's inequality:
\begin{align*}
P(Y_2 \geq \mathbb{E}[Y_2] + l) = P(Y_1 \leq \mathbb{E}[Y_1] - l) <\\<\exp\left(-\frac{2l^2}{m(n_r-1)^2}\right) <\exp \left(\frac{-m(V_{S^*}(d_1)-V_{S^*}(d_2))^2}{2(n_r-1)^2} \right)\leq \frac{\delta}{n_d},
\end{align*}
thus,
\[(1-\sigma_g(B)(1)) \leq n_d\exp \left(\frac{-m(V_{S^*}(d_1)-V_{S^*}(d_2))^2}{2(n_r-1)^2} \right).\]
As the observations increase to infinity, greedy's regret approaches 0:
\[\lim_{{m \to \infty}} n_d \exp \left(\frac{-m(V_{S^*}(d_1)-V_{S^*}(d_2))^2}{2(n_r-1)^2} \right) = 0,\]
where we used that $n_r > 1$.
We can derive the minimum number of observations $m$ required, to achieve 0 regret with desired probability $(1-\delta)$:
\[\Pr{\bar{\gamma}(\sigma_g, S^*) = 0 | m \geq \frac{2(n_r-1)^2\ln\left(\frac{n_d}{\delta}\right)}{(V_{S^*}(d_1)-V_{S^*}(d_2))^2}} = (1-\delta). \]
\end{proof}

\subsection{The Greedy Strategy outperforms the Thompson Sampling algorithm} \label{sec:gt_1}
In this section, for two products that also have two ratings and for any number of observations, we calculate the expected regret of the Thompson Sampling algorithm. Additionally, we show that as the number of observations increases to infinity, the Thompson Sampling algorithm incurs regret greater than the regret of the greedy strategy.\footnote{This proposition can be generalized to any number of ratings by replacing the Beta distribution with the Dirichlet distribution. Similarly to the proof of Theorem~\ref{theorem:theoremlimit}, this can also be extended to any number of products by taking a union bound across the set of products. }\newline

\noindent Consider the state $S'$ given in Section \ref{section:regret_m1}, where without loss of generality we assume that $p_1<p_2$, i.e., the first product has higher value and the regret of choosing product two is: $p_2-p_1$. For $a+b=c+d=m$, let $B$ be an observation matrix sampled from $S'$:
\[B=\begin{bmatrix} 
a & c\\
b & d
\end{bmatrix}.\]
From Thompson Sampling, let $X \sim \text{Beta}(b,a)$ and $Y \sim \text{Beta}(d,c)$, be two independent Beta-distributed random variables. From $p_1<p_2$, the algorithm incurs regret only when it selects product two, thus we need to calculate the probability of drawing a better sample from $Y$ than $X$, $P(X<Y)$:


\begin{align*}
P(X<Y)=  \int_0^1\int_0^y\frac{ x^{b-1}(1-x)^{a-1}}{B(b,a)}\frac{y^{d-1}(1-y)^{c-1}}{B(d,c)}\,dx \,dy = \\ =\frac{1}{B(b,a)B(d,c)}\int_0^1 y^{d-1}(1-y)^{c-1}B_y(b,a)\,dy,
\end{align*}
where $B$ not to be confused with an observation matrix, refers to the Beta function and $B_y(b,a)=\int_0^y\frac{x^{b-1}(1-x)^{a-1}}{B(b,a)} \,dx$ the incomplete Beta function. \newline

\noindent The regret of the Thompson Sampling algorithm then is: 
\[\bar{\gamma}(\sigma_{TS}, S') = \sum_{B \in \mathcal{B}_m}P[B|S']P(X<Y)(p_2-p_1).\]

\begin{proposition}\label{proposition:regret_ts}
For $n_d=2$, $n_r=2$ and $m\rightarrow \infty$, the regret of the Thompson Sampling algorithm is larger than the regret of the greedy strategy.
\end{proposition}

\begin{proof}


Let $Z_1$ and $Z_2$ be independent random variables of the sum of rating two observations for product 1 and 2 respectively, again sampled from $S'$ with $p_1<p_2$. For each product, the expected number of rating 2 observations is:
\[\mathbb{E}[Z_1]=m(1-p_1) \text{ and } \mathbb{E}[Z_2]=m(1-p_2).\]
We further calculate the half-distance $l$ between $Z_1$ and $Z_2$:
\[l=\frac{\mathbb{E}[Z_1] - \mathbb{E}[Z_2]}{2}=\frac{m(p_2-p_1)}{2}.\]
Given $Z_1$ and $Z_2$ are independent, using Hoeffding's inequality:


\begin{align*} P(Z_2\geq \mathbb{E}[Z_2]+l)=P(Z_1 \leq \mathbb{E}[Z_1]-l) < \exp\left(-\frac{2l^2}{m}\right) < \\ < \exp \left(- \frac{m^2(p_2-p_1)^2}{2m} \right) < \exp\left(- \frac{m(p_2-p_1)^2}{2} \right).
\end{align*}
As observations increase to infinity, the probability of observing more rating two observations for product 2 than product 1 approaches 0:
\[\lim_{m \rightarrow \infty}  2\exp \left(- \frac{m(p_2 -p_1)^2}{2} \right)=0.\]
Knowing the above, we consider all observation matrices as: 
\[B'=\begin{bmatrix} 
a & c\\
b & d
\end{bmatrix} \text{ where } a+b=c+d=m \text{ and } b>d.\]

\noindent As the number of observations goes to infinity, the probability of observing $B'$ approaches 1, $\lim_{m \rightarrow \infty}P[B'|S']=1$; thus the regret for Thompson Sampling is:
\[\lim_{m\rightarrow \infty}\bar{\gamma}(\sigma_{TS}, S')=P(X<Y)(p_2-p_1).\]
For some $a,b,c,d >0$, $P(X<Y)>0$, thus the Thompson Sampling algorithm incurs some regret even as the number of observations goes to infinity.
Whereas with $V_{B'}(1)>V_{B'}(2)$, greedy selects product 1 with probability 1 and receives 0 regret.
\end{proof}





\section{Empirical Results for the Greedy Strategy on Google Reviews Data for Restaurants}
\label{sec:Google}

In this section we test the discussed strategies in the context of selecting restaurants, using data extracted from Google reviews \cite{YHLZM2022}. The goal is to pick a restaurant that has high consumer satisfaction, regardless of its cuisine. From this experiment we confirm our theoretical findings, and show that greedy performs well when the states of Nature are unknown. The data has 1.5 million reviews for 64,000 different restaurants and follows the standard online review pattern, 1 review per guest with ratings $R = \{1,2,3,4,5\}$. It is important to note however, that the average rating from this data is 4.45 with a standard deviation of 1. Given the overall number of reviews is high, we assume that Nature has picked its strategy $S \in \mathcal{S}$, and that all of the reviews (observations) for each restaurant that we have in the data are as a result of that state $S$. Having that in mind, we compute the value of the products as the average of all available observations (reviews) and we benchmark the greedy strategy and the Thompson Sampling algorithm against that as the ground-truth. The experiment goes as follows:
\begin{enumerate}
\item Calculate the restaurant's value, by taking the average across all available observations for each restaurant
\item Draw uniformly at random without replacement $m$ different observations (reviews) for each restaurant
\item Run the greedy strategy and Thompson Sampling algorithm on the sampled observations and choose which restaurants they pick
\item Calculate the regret between the best restaurant from step 1 and the payoff from the restaurants that step three selects
\end{enumerate}

\begin{simresult}\label{theorem:simresult1}
For $n_d$ and $m$ in $\{1,2,3,4,5,6,7,8,9,10\}$ and $n_r=5$, we test the greedy strategy in an experiment with Google reviews for 500 simulations. We measure the average regret of the greedy strategy from all simulations, and observe that it decreases as observations increase and that it outperforms the uniform strategy and the Thompson Sampling algorithm, both as depicted in Figure~\ref{fig:simuresult}.
\end{simresult}

\begin{figure}[H]
\centering
\caption{Regret of the proposed strategies when tested on Google reviews data for restaurants. The regret of the greedy strategy is barely visible as it is below all other strategies.}
\includegraphics[scale=0.27]{greedy_regret_surface.png}
\label{fig:simuresult}
\end{figure}
\noindent From Figure ~\ref{fig:simuresult}, we observe that the greedy strategy quickly reaches regret $\leq 0.1$ as the observations increase, even when choosing between a larger set of products. For enhanced clarity, tabulated results for Figure \ref{fig:simuresult} can be found in Appendix \ref{section:appendix_tabulated_google}.

\section{Future Work} \label{sec:future_work}
Our current study assumes an equal number of observations per product $m$. As shown, the greedy strategy performs well in this framework; however, this changes when we have different number of observations per product. Consider the following observation matrix, \[\tilde{B}_p = \begin{bmatrix} 
0 & 1\\
1 & 10^6
\end{bmatrix}.\] In this case, the greedy strategy would calculate a higher observed value with respect to the observations of the first product, despite the second product having a million observations with a rating of two. When the number of observations per product is \emph{different}, the strategies have two goals: pick the best product, but also assess the confidence of the samples based on the number of observations, thus, the strategy outcome can be very different. Methods discussed in this paper as the UCB and Thompson Sampling algorithms already adjust for different number of samples (observations) and the different number of observation cases can be studied within this framework; Initial results show that the greedy strategy outperforms both UCB and the Thompson Sampling algorithm with respect to the worst-case regret. 

On the lower bound side, some additional work that we have done shows that the worst-case regret does not always monotonically decrease with different number of observations, even if more observations are given. Given this non-monotonicity, and that this setting requires substantially different proof techniques, we leave this case for future research. \newline \newline
Future work could also consider extending the theoretical analysis to settings with larger numbers of products and ratings beyond the binary case. While most of our proofs are limited to two products and two ratings, and our experimental results demonstrate applicability to up to 10 products and 5 ratings, it is obvious that regret increases as the number of products and ratings grows. Numerical analysis for such setups is computationally expensive, but the analytical methods we provide can be extended to more general setups.





\newpage
\bibliographystyle{plain} % We choose the "plain" reference style
\bibliography{refs} % Entries are in the refs.bib file

\newpage
\appendix


\section{Tabulated Results for Google Reviews Experiments}\label{section:appendix_tabulated_google}
Tabulated results are provided in this section to complement the result from Figure \ref{fig:simuresult}. Note, row indices are number of observations, and column indices are number of products and per the Google reviews standard the number of reviews for all experiments is five.
\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
$m,n_d$     & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     & 10    \\ \hline
1     & 0.195 & 0.256 & 0.304 & 0.366 & 0.395 & 0.404 & 0.423 & 0.432 & 0.448 \\ \hline
2        & 0.128 & 0.167 & 0.194 & 0.221 & 0.258 & 0.254 & 0.290 & 0.295 & 0.314 \\ \hline
3       & 0.074 & 0.117 & 0.136 & 0.155 & 0.170 & 0.195 & 0.216 & 0.213 & 0.234 \\ \hline
4        & 0.064 & 0.095 & 0.118 & 0.119 & 0.137 & 0.155 & 0.158 & 0.168 & 0.172 \\ \hline
5        & 0.051 & 0.081 & 0.103 & 0.094 & 0.119 & 0.131 & 0.139 & 0.137 & 0.140 \\ \hline
6       & 0.033 & 0.074 & 0.074 & 0.096 & 0.105 & 0.110 & 0.123 & 0.123 & 0.120 \\ \hline
7       & 0.036 & 0.054 & 0.063 & 0.086 & 0.087 & 0.097 & 0.104 & 0.107 & 0.127 \\ \hline
8       & 0.036 & 0.055 & 0.055 & 0.074 & 0.080 & 0.086 & 0.091 & 0.087 & 0.096 \\ \hline
9       & 0.028 & 0.046 & 0.056 & 0.065 & 0.071 & 0.076 & 0.079 & 0.084 & 0.093 \\ \hline
10      & 0.030 & 0.042 & 0.052 & 0.061 & 0.057 & 0.071 & 0.077 & 0.081 & 0.089 \\ \hline
\end{tabular}
\caption{Regret of the greedy strategy when tested on the Google reviews data for restaurants}
\end{table}

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
$m,n_d$ & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     & 10    \\ \hline
1       & 0.191 & 0.261 & 0.293 & 0.366 & 0.405 & 0.411 & 0.430 & 0.437 & 0.450 \\ \hline
2       & 0.134 & 0.179 & 0.210 & 0.230 & 0.260 & 0.261 & 0.289 & 0.292 & 0.312 \\ \hline
3       & 0.099 & 0.130 & 0.158 & 0.171 & 0.178 & 0.190 & 0.221 & 0.220 & 0.232 \\ \hline
4       & 0.089 & 0.115 & 0.146 & 0.137 & 0.140 & 0.169 & 0.168 & 0.180 & 0.177 \\ \hline
5       & 0.070 & 0.112 & 0.121 & 0.116 & 0.146 & 0.144 & 0.153 & 0.141 & 0.146 \\ \hline
6       & 0.060 & 0.101 & 0.100 & 0.121 & 0.124 & 0.126 & 0.139 & 0.142 & 0.123 \\ \hline
7       & 0.053 & 0.089 & 0.093 & 0.112 & 0.113 & 0.126 & 0.115 & 0.139 & 0.143 \\ \hline
8       & 0.059 & 0.083 & 0.087 & 0.099 & 0.108 & 0.115 & 0.115 & 0.113 & 0.114 \\ \hline
9       & 0.049 & 0.066 & 0.085 & 0.098 & 0.094 & 0.103 & 0.100 & 0.113 & 0.111 \\ \hline
10      & 0.052 & 0.071 & 0.073 & 0.082 & 0.095 & 0.091 & 0.109 & 0.106 & 0.095 \\ \hline
\end{tabular}
\caption{Regret of the Thompson Sampling algorithm when tested on the Google reviews data for restaurants}
\end{table}

\begin{table}[h]
\begin{tabular}{|l|l|l|l|l|l|l|l|l|l|}
\hline
$m,n_d$ & 2     & 3     & 4     & 5     & 6     & 7     & 8     & 9     & 10    \\ \hline
1       & 0.389 & 0.543 & 0.613 & 0.671 & 0.672 & 0.682 & 0.715 & 0.736 & 0.748 \\ \hline
2       & 0.368 & 0.515 & 0.557 & 0.598 & 0.643 & 0.639 & 0.673 & 0.670 & 0.693 \\ \hline
3       & 0.332 & 0.459 & 0.535 & 0.553 & 0.583 & 0.620 & 0.620 & 0.645 & 0.671 \\ \hline
4       & 0.343 & 0.420 & 0.483 & 0.507 & 0.551 & 0.574 & 0.603 & 0.596 & 0.615 \\ \hline
5       & 0.314 & 0.411 & 0.485 & 0.502 & 0.520 & 0.525 & 0.540 & 0.567 & 0.578 \\ \hline
6       & 0.281 & 0.362 & 0.436 & 0.467 & 0.460 & 0.502 & 0.519 & 0.518 & 0.539 \\ \hline
7       & 0.272 & 0.347 & 0.398 & 0.419 & 0.450 & 0.469 & 0.495 & 0.505 & 0.500 \\ \hline
8       & 0.244 & 0.322 & 0.365 & 0.402 & 0.427 & 0.450 & 0.468 & 0.468 & 0.471 \\ \hline
9       & 0.243 & 0.317 & 0.343 & 0.388 & 0.389 & 0.428 & 0.448 & 0.443 & 0.456 \\ \hline
10      & 0.237 & 0.320 & 0.326 & 0.362 & 0.379 & 0.409 & 0.410 & 0.425 & 0.426 \\ \hline
\end{tabular}
\caption{Regret of the uniform strategy when tested on the Google reviews data for restaurants}
\end{table}

\end{document}

