\section*{APPENDICES}\label{sec:appendices}
\subsection*{Toolchain Setup}
\label{app:setup}
In this appendix, we describe how to set up the toolchain described in this work. Toolchain's respository on Github~\cite{repo} also includes a README-file with more details on the setup instructions. This setup is based on our previous work~\cite{hoelscher:2023}, in which we made the toolchain more accessible by integrating it with an IDE.

To work with the presented toolchain, we recommend setting up the environment
the same way we do. For this, Visual Studio Code~\cite{vscode}, its extensions for working with remote
development container~\cite{vscodeextensions} as well as Docker~\cite{docker} are required. Please see their respective documentation for setup instructions or the repository's README-file. Alternatively, you can use your own environment, as long as Docker is supported.

To begin with, go ahead and download the repository. After downloading, go into the repository's root directory and execute~\cref{lst:submodule}
\begin{lstlisting}[caption={Submodule initialization},label=lst:submodule,language=bash]
$ git submodule init
$ git submodule update
\end{lstlisting}
This will set up the source code for gem5, NVMain, Unikraft, and the benchmarks.

Now start Visual Studio Code and open up the repository's directory. A prompt should pop up,
asking, if you want to reopen the directory in a development container. Go ahead and do so. If not asked, the directory can be reopened in a container via the blue "Remote Window" icon in the bottom left of
Visual Studio Code.

Before using the toolchain, it needs to be compiled. For this, we have provided an example script
(\texttt{ExampleBuild.sh}) within the repository that you can use. It provides some general
commands that enable building and executing the toolchain. In its current version, it compiles the
toolchain with our bit flip simulation extension enabled (see section~\cref{subsec:tudo}). More
information on the compiled simulation environment can be found within the example script. Please
modify it to your liking or use your own build commands. However, in the context of this appendix, move into
the repository's root directory and run~\cref{lst:build}
\begin{lstlisting}[caption={Build gem5 and NVMain},label=lst:build,language=bash]
$ ./ExampleBuild fb
\end{lstlisting}
The command builds gem5 and NVMain. If at any time, you want to use the provided benchmark
applications, you first need to build them with~\cref{lst:benchmarks}
\begin{lstlisting}[caption={Build benchmarks},label=lst:benchmarks,language=bash]
$ ./ExampleBuild ba BENCHMARK_NAME
\end{lstlisting}

To start a simulation run~\cref{lst:simulation} with any application, e.g., one of the benchmarks.
\begin{lstlisting}[caption={Run simulation},label=lst:simulation,language=bash]
$ ./ExampleBuild e APPLICATION_NAME
\end{lstlisting}

Following these instructions puts the repository in a usable state that has our bit-flip simulation
extension enabled, i.e., you can directly start to follow section~\cref{subsec:tudo}. In case you
want to enable other extensions, please provide the respective flags when building gem5 and NVMain.
For reference, see~\cref{lst:noflags} compared to~\cref{lst:flags}.
\begin{lstlisting}[caption={Build without flags},label=lst:noflags,language=bash]
$ python3 `which scons` -j 8 EXTRAS=../nvmain ./build/ARM/gem5.fast
\end{lstlisting}
\begin{lstlisting}[caption={Build with the \texttt{CDNCcim} flag for X86 architecture, without the NVMain extension},label=lst:flags,language=bash]
$ python3 `which scons` CDNCcim=1 -j 8 EXTRAS=../nvmain ./build/X86/gem5.fast
\end{lstlisting}
We currently provide the following flags for the features we have extended the toolchain with:
\begin{itemize}
    \item \texttt{tu\_dortmund}: Bit flip simulation
	\item \texttt{CDNCcim}: Compute in Memory for NVM
	\item \texttt{hybrid\_cache}: Heterogeneous cache extension
\end{itemize}

While building (see~\cref{lst:noflags}), you may notice that the target binary is \texttt{gem5.fast} in the directory \texttt{ARM}. Specifying the binary type (\texttt{fast}, \texttt{debug}, \texttt{opt}) works solely by defining the target binary. For example, to include debug information, use \texttt{gem5.debug} instead. The same applies to the ISA, e.g., switching from ARM to X86, for both of which the toolchain has been tested~\footnote{
Due to an addressing issue when integrating NVMain with the latest gem5 version, the flag \texttt{X86} needs to be provided when targeting X86 architectures. With ARM, the flag can be ignored. Other ISAs are currently not handled.
}. For details on build versions, refer to gem5's official documentation~\cite{gem5:documentation}. The example script builds the \texttt{.fast}~-~version for ARM as shown in~\cref{lst:noflags}.
You might also need to build the binary for X86 target and m5 library, use the following command:
\begin{lstlisting}[caption={Build X86 target and m5 library},label=lst:gem5_x86,language=bash]
$ python3 `which scons` -j 8 EXTRAS=../nvmain ./build/X86/gem5.fast
$ python3 `which scons` -j 8 EXTRAS=gem5/util/nvmain -C gem5/util/m5 build/x86/out/m5
\end{lstlisting}



From this point, you can follow the case studies presented in this work. They all assume that the repository was set up using the instructions above. 
If a different repository state is required, the case study will provide the necessary steps. 
For more details on the overall toolchain setup, we refer the readers to the repository's README file and its components' respective documentation.
 
\subsection*{Main memory simulation, hybrid setup and new operations}
\label{app:tudr:main_memory}
Once you have set up the \textit{NVM\_Simulation} repository, switch to a different branch using the following command:
\begin{lstlisting}[caption={Build gem5 and m5 library},language=bash]
$ git checkout Tutorial-NVM
\end{lstlisting}
Then,  compile the \texttt{gem5.fast} binary for the X86 architecture by running the commands in Listing ~\cref{lst:gem5_x86}. After this, you are ready to proceed to run the different experiments.

\subsubsection*{Timing and scheduling policies}
\label{app:tudr:scheduling}
The first task runs a matrix multiplication in gem5. Compile the sample \texttt{mm.c} using the following command:
\begin{lstlisting}[language=bash]
$ gcc -O3 -static NVM_tutorial/mm.c -o NVM_tutorial/mm.bin
\end{lstlisting}

To obtain statistics for a specific code region, i.e., excluding the program's initialization or finalization phases and only focusing on the matmul computation kernel, the following m5 calls were be placed around the \emph{region of interest} (RoI):   

\begin{lstlisting}[language=C]
#include <gem5/m5ops.h>
...
m5_reset_stats(0, 0);
/*** Region of Interest ***/
m5_dump_stats(0, 0);
... 
\end{lstlisting}
This also requires the compile command to be updated to include the m5 library, as shown below:
\begin{lstlisting}[language=bash]
$ gcc -O3 -static NVM_tutorial/mm_m5.c -I gem5/include/ -lm5 -Lgem5/util/m5/build/x86/out -o NVM_tutorial/mm_m5.bin
\end{lstlisting}

To run this application in gem5 using NVM memory, use the following command, as mentioned in ~\cref{subsec:nvm-dram-methods}:

\begin{lstlisting}[language=bash]
$ gem5/build/X86/gem5.fast --outdir=FCFS example/se.py  --cpu-type=DerivO3CPU --caches --mem-type=NVMainMemory --nvmain- config=nvmain/Config/PCM_ISSCC_2012_4GB.config --cmd NVM_tutorial/mm_m5.bin &> FCFS/nvmain_stats.txt
\end{lstlisting}


With this command line, Gem5 records the statistics in the output directory (i.e., FCFS/stats.txt) for the first chunk of the RoI between the markers "Begin" and "End". The NVMain output is redirected (FCFS/nvmain\_stats.txt) and is divided into two chunks, starting with \texttt{i0.defaultMemory} and \texttt{i1.defaultMemory...}, where the first chunk corresponds to the RoI.  

As described in \cref{subsec:nvm-dram-methods}, you can change the memory controller in the NVMain configuration file to \texttt{FCFS, FRFCFS, FRFCFS-WQF} and re-run the above command to notice the different on the result (in the generated stats file). 
 
\subsubsection*{Hybrid main memory simulation}
\label{app:tudr:hybrid}
The previous task uses gem5+NVMain to simulate an NVM memory. To simulate a hybrid memory, open the gem5 system configuration file using:
\begin{lstlisting}[language=bash]
$ cd sims/tudr/nv-gem5
$ nano configs/deprecated/example/hybrid_example.py
\end{lstlisting}
In this file, one can configure the memory types and specify the number of channels for the system.
\begin{lstlisting}[language=Python]
### hybrid_example.py (Line 130)
args.mem_channels = 1 # single mem_ctrl
args.external_memory_system = 0
args.hybrid_channel = True
args.mem_type = "DDR4_2400_16x4"
args.nvm_type = "NVM_2400_1x64"
\end{lstlisting}

\begin{lstlisting}[language=Python]
### configs/common/MemConfig.py (Line 246)
# mem_ctrl.dram = dram_intf
### configs/common/MemConfig.py (Line 267)
mem_ctrls.append(nvm_intf.controller())

### hybrid_example.py (Line 130)
args.hybrid_channel = False # for separate mem_ctrls ...
system.mem_ctrls[0].dram.addr_mapping = "RoRaBaCoCh"
system.mem_ctrls[1].dram.addr_mapping = "RoRaBaCoCh"
\end{lstlisting}

To test this configuration, use the following commands to run the STREAM benchmark~\cite{stream_bench} on the gem5 simulator:

\begin{lstlisting}[language=bash]
$ gem5/build/X86/gem5.fast gem5/configs/deprecated/example/hybrid_example.py NVM_tutorial/stream_8k.bin
$ gem5/build/X86/gem5.fast gem5/configs/deprecated/example/hybrid_example.py NVM_tutorial/stream_8k_P.bin
\end{lstlisting}


\subsubsection*{Trace-based simulation}
\label{app:tudr:traceSim}
To view the NVMain trace format and a sample trace, run the following command:
\begin{lstlisting}[language=bash]
$ cat nvmain/Tests/Traces/hello_world.nvt
\end{lstlisting}

Use the following command to compile NVMain and simulate this trace file. 
\begin{lstlisting}[language=bash]
$ cd nvmain
$ python3 `which scons` --build-type=fast
$ ./nvmain.fast Config/PCM_ISSCC_2012_4GB.config Tests/Traces/hello_world.nvt 1000000
\end{lstlisting}

\subsubsection*{Adding new operations}
\label{app:tudr:adding_ops}
Use the following commands to test a trace file with rowclone requests.
\begin{lstlisting}[language=bash]
$ ./nvmain.fast Config/Config/2D_DRAM_example.config Tests/Traces/row_clone.nvt 1000
\end{lstlisting}
You can also view the trace file \texttt{Tests/Traces/row\_clone.nvt} to see the rowclone requests.  

\begin{lstlisting}[language=bash]
$ cat nvmain/Tests/Traces/row_clone.nvt
\end{lstlisting}

In the generated output, similar to the \texttt{read, write} requests statistics, you can also see the \texttt{rowclone} statistics. 

\subsection*{Access Tracing and Data Processing Using Trace Writers}
\label{app:tudo}
In this appendix, we give more insight on details that exceed the scope of this appendix's respective case study.

\subsubsection*{Additional Trace Writer Exercises}
\label{app:tudo:exercises}
Complementary to the respective case study's step-by-step instructions, in the toolchain's repository, there are also exercises that teach how to add a new trace writer to NVMain Each exercise comes with some base code and an example solution. To follow these exercises, go into the repository's submodule that is located at \texttt{repository\_root/simulator/nvmain} and checkout one of the following branches:
\begin{itemize}
    \item Trace-Writer-Tutorial-Ex1: Base for exercise 1 - Setting up a trace writer skeleton.
    \item Trace-Writer-Tutorial-Ex1-Solution: Example solution for exercise 1.
    \item Trace-Writer-Tutorial-Ex2: Base for exercise 2 - Put actual data to the trace.
    \item Trace-Writer-Tutorial-Ex2-Solution: Example solution for exercise 2.
\end{itemize}
Everything that we provide for the exercise can be found under\\
\texttt{repository\_root/simulator/nvmain/traceWriter/ESWeekTraceWriter/},\\
including the instructions for the exercise.

\subsection*{Heterogeneous Cache Simulations}\label{app:fau}
In this appendix, we provide a concrete example on how to run a gem5 simulation with hybrid caches enabled, as shown in \cref{sec:nvm-cache}.
As part of the repository, we provide a specific configuration named \texttt{fs\_hy.py}, where the instantiated caches are objects of the \texttt{HybridCache} class.
To perform a simulation with hybrid caches and a non-volatile main memory simulated in NVMain, the following command can be used:
\begin{lstlisting}[language=bash]
$ ./build/ARM/gem5.opt configs/example/fs_hy.py --bare-metal --kernel=path_to_unikraft_kernel --cpu-type=O3CPU --sys-clock=240MHz --cpu-clock=480MHz --machine-type=VExpress_GEM5_V2 --dtb-filename=system/arm/dt/armv8_gem5_v2_1cpu.dtb --mem-size=4GB --mem-type=NVMainMemory --nvmain-config=../nvmain/Config/PCM_ISSCC_2012_4GB.config --caches --l1d_size=32kB --l1i_size=32kB --l1d_assoc=4 --l1i_assoc=2 --l1d_nv_block_ratio=50
\end{lstlisting}
Whereas, the \texttt{l1d\_size} or \texttt{l1d\_assoc} are part of gem5's default \texttt{fs.py} configuration to set the size and associativity of the L1 data cache, we have added the \texttt{l1d\_nv\_block\_ratio} parameter.
This parameter can be set to any desired value between 0 and 100, setting the percentage of L1 data cache lines per set that are treated as being implemented in NVM technology, as explained in \cref{sec:nvm-cache}.
Vice versa, for the L1 instruction cache, these settings can be changed via the prefix \texttt{l1i} (instead of \texttt{l1d} in front of the parameter name.
\begin{lstlisting}[language=Python]
class L1Cache(HybridCache):
    ...
    data_read_latency = 2
    data_write_latency = 8
    vol_read_energy = 0.009
    non_vol_read_energy = 0.007
    vol_write_energy = 0.009
    non_vol_write_energy = 0.056
\end{lstlisting}
In addition to the latency settings, the class definitions in \texttt{configs/common/HybridCaches.py} also contains parameters for the dynamic energy consumption regarding read- and write accesses to the volatile and non-volatile cache section, respectively.
For experimental reasons, the parameters in this file can be set to any desired value.
\par
After performing a simulation, by default, the resulting statistics are dumped in \texttt{m5out/stats.txt}.
Here, among many other statistics, the number of CPU cycles, as well as the number of accesses to each cache section and the resulting dynamic energy consumption, according to the previously set energy parameters, are dumped as shown in the following:
\begin{lstlisting}
system.cpu.numCycles                ... # Number of cpu cycles simulated (Cycle)
...
system.cpu.dcache.noOfNonVolReads   ... # Number of reads to non-vol cache blocks (Count)
system.cpu.dcache.noOfVolReads      ... # Number of reads to vol cache blocks (Count)
system.cpu.dcache.noOfNonVolWrites  ... # Number of writes to non-vol cache blocks (Count)
system.cpu.dcache.noOfVolWrites     ... # Number of writes to vol cache blocks (Count)
system.cpu.dcache.dynEnergy         ... # Dynamic energy caused by cache accesses (nJ)
\end{lstlisting}
Using this statistic dump and the above mentioned command to run the simulation with different \texttt{l1d\_nv\_block\_ratio} settings, we can generate plots as shown in our case study.
\subsection*{Compute-in-Memory (CiM)}
This appendix provides code samples (\cref{kit/lst1}, \cref{kit/lst2}, and \cref{kit/lst3}) for implementing CiM application-level code and converting C++ code to CiM code with the help of the CiM API, as discussed in \cref{sec:nvm-cim}.
Additionally, in the following, it provides toolchain setup steps and utilizing CiM extention.

\begin{lstlisting}[language={[ANSI]C++}, label={kit/lst1}, caption={Simple vector-wise NAND operation implementation in a user application.},emph={cimModule},emphstyle=\color{cyan!50!blue}, emph={[2]CimModule},emphstyle={[2]\color{cyan}},
% numbers=right,numbersep=10pt,linewidth=0.9\linewidth
]
// Including CiM API
#include "cim_api.hpp"      
// ...
// Instantiating CiM object
CimModule cimModule;

// initializing CPU side arrays
uint8_t a[CimModule.ROW_SIZE] {0};
uint8_t b[CimModule.ROW_SIZE] {0};
uint8_t c[CimModule.ROW_SIZE] {0};

// initializing CiM rows
cimModule.copy_to_cim(0, (void *)&a[0]); // row0 <- a...
cimModule.copy_to_cim(1, (void *)&b[0]); // row1 <- b...
cimModule.copy_to_cim(2, (void *)&c[0]); // row2 <- c...

// Performing NAND operation
cimModule.AND({0,2});   // SA outputs = row0 & row2
cimModule.NOT_COND(1);   // row1 = ~SA outputs

// Copying results back to CPU side array
cimModule.copy_to_cim((void *)&b[0], 1); // b... <- row1
\end{lstlisting}

\begin{lstlisting}[language={[ANSI]C++}, label={kit/lst2}, caption={An example of using a ternary operator inside a for loop in C++.},emph={cimModule},emphstyle=\color{cyan!50!blue}, emph={[2]CimModule},emphstyle={[2]\color{cyan}},
% numbers=right,numbersep=10pt,linewidth=0.9\linewidth
]
for(size_t i=0; i< CimModule.ROW_SIZE; i++)
    a[i] = (b[i]==0x12) ? c[i] : d[i];
\end{lstlisting}
\begin{lstlisting}[language={[ANSI]C++}, label={kit/lst3}, caption={Converting the ternary operator to CiM code, along with unrolling the loop.},emph={cimModule},emphstyle=\color{cyan!50!blue}, emph={[2]CimModule},emphstyle={[2]\color{cyan}},
% numbers=right,numbersep=10pt,linewidth=0.9\linewidth
]
cimModule.copy_to_cim(0, (void *)&b[0]);            // row0 <- b...
cimModule.copy_to_cim(1, (void *)&CONSTx12[0]);     // row1 <- 0x12...
cimModule.copy_to_cim(2, (void *)&c[0]);            // row2 <- c...
cimModule.copy_to_cim(3, (void *)&d[0]);            // row3 <- d...
cimModule.copy_to_cim(4, (void *)&a[0]);            // row4 <- a...

cimModule.XOR({0,1})                    // (b[i]==0x12)
cimModule.NOT_COND(5, false, true);     // row5 <- 0xff if result is 0, otherwise 0x00
cimModule.NOT_COND(6, 5, true, false);  // row6 <- ~row5

cimModule.AND({2,5})                    // row7 <- row2 & row5
cimModule.COPY(7)

cimModule.AND({3,6})                    // row8 <- row3 & row6
cimModule.COPY(8)

cimModule.OR({7,8})                     // a <- row7 | row8
cimModule.COPY(4)
\end{lstlisting}
\subsubsection*{Minimal CiM Implementation from Scratch}
To understand how to implement and run full-system-based CiM simulations with gem5, and to build the required materials such as device drivers, please clone the toolchain's repository on GitHub~\cite{repo} using the `\texttt{-{}-branch cdnc}` flag. Then, open it in Visual Studio Code's development container mode. This will automatically download and initialize the necessary repositories.
The \texttt{cdnc} folder within the repository contains video tutorials on creating a custom disk image and transferring the necessary files into it using \texttt{QEMU}. Additionally, the \texttt{README} file includes a detailed step-by-step guide for running the provided examples.

To test and execute the provided examples and disk image, open Visual Studio Code, navigate to the `Terminal' tab, and select `Run Task'. Choose the task \texttt{Run skip-Steps1and2.sh} to download the required materials. Once the process is complete, select the \texttt{Run step3-testingWithGem5.sh} task to compile and execute gem5. If the setup is successful, you can access the gem5 guest terminal and run the examples as described in the mentioned \texttt{README} file.

\subsubsection*{Using the Provided CiM Extension}
To use the CiM extension, refer to the instructions in the Toolchain Setup appendix. After compiling gem5 with the appropriate flags (e.g., \cref{lst:flags}), navigate to the \texttt{simulator/gem5/tests/test-progs/CDNCcim} folder. This folder contains the application codes provided, as well as scripts to build and run the examples.