\subsection{DRAM and NVMs-Based Main Memory Simulation}
\label{sec:nvm-dram}

\subsubsection{Overview}
As stated in Section~\ref{sec:Introduction}, NVMs have the potential to replace conventional SRAM and DRAM technologies but require careful design decisions to mitigate the impact of their challenges, e.g., non-reliability, expensive write operations, on the running application's accuracy, overall performance and energy consumption. Various optimizations have been proposed to achieve this including: novel architectural designs, memory controllers, and efficient scheduling strategies~\cite{5416645, 1555815}.

Nonetheless, choosing the right memory technology and determining the best set of optimizations for a specific application domain is non-trivial and requires system-level simulations to validate and refine design and optimization decisions. To this end, this case study provides a step-by-step introduction to the memory subsystem design, focusing on understanding its key components and comparing DRAM and NVM technologies using NVMain. As explained in Section~\ref{subsec:toolchain}, the modular structure and various simulation modes of NVMain enable quick prototyping and evaluation of memory systems, whether for specific application needs or to enhance overall system performance.

\subsubsection{Objective}
\label{subsubsec:tudr:objective}
This case study aims to familiarize readers with memory subsystem simulation by guiding them through the various components of NVMain. Specifically, it introduces readers to exploring architectural features, reordering memory requests to improve row buffer hits, and leveraging write and read queues to optimize NVMs' access latency. Since we cannot cover all parts of the memory subsystem, we intentionally select different experiments related to memory system modeling and optimizations, providing insights that enable readers to make tailored modifications of their choice. The case study also includes an introduction to configuring and simulating hybrid main memory systems, i.e., combining DRAM and NVM technologies. Finally, we show how to implement new memory operations exemplified by RowClone, a technique that enhances the functionality of modern memory systems for efficient bulk copies in memory.




\subsubsection{Methodology}
\label{subsec:nvm-dram-methods}
Before delving into the different experiments,  let us first introduce key simulation parameters, including memory timings and scheduling policies.

~\\\noindent \textbf{Timing in DRAM and Non-Volatile Memories:}\\
\textit{Single-Bank Timing:}
In DRAM, a read cycle requires restoring data (tRAS = activation + restoration time) and starting precharge before closing a row, as reads destroy data in the capacitor. The read cycle time (tRC) includes activation (tRCD), restoration, and precharge (tRP). In contrast, read operation in NVMs is non-destructive, so restoration is not needed. The read cycle for NVMs is dominated by tRCD + tBURST. 
\textit{Inter-Bank Timing:} To manage power and limit peak current, tFAW (four activation window) restricts four activations within a set time, while tRRD (Row-to-Row Activation Delay) spaces out activations to prevent excessive current. These rules apply within and across bank groups. In NVMs, high write currents and long write times can overlap between writes and activations, managed using parameters like tWWD (Write-to-Write Delay), tWAD (Write-to-Activation Delay), and tAWD (Activation-to-Write Delay).


~\\\noindent \textbf{Scheduling Policies:}\\
Memory controller's scheduling policies significantly affect the overall performance. Below are some commonly used policies implemented in NVMain:\\
\textit{First-Come-First-Served (FCFS):}
In single-bank timing, requests are processed in their arrival order without optimizing for delays caused by row activation or precharge. This hurts inter-bank parallelism, as idle banks must wait for pending requests in busy banks.\\
\textit{First-Ready First-Come-First-Served (FR-FCFS):}
In single-bank timing, FR-FCFS reorders requests to maximize row-buffer hits; leading to a reduced number of row activations and precharges that reduce the access latency. For inter-bank timing, parallelism is maximized by overlapping requests across banks, enabling some banks to process read/write requests while others handle activations or precharges.\\
\textit{First-Ready First-Come-First-Served with Write Queue Flush (FR-FCFS-WQF):}
In single-bank timing, writes are batched when the write queue fills, minimizing write-to-read penalties. For inter-bank timing, write batching is coordinated across banks to prevent stalling reads, improving efficiency and reducing delays.




In the following, we outline instructions to guide readers to conduct experiments and achieve the desired objective (see Section~\cref{subsubsec:tudr:objective}). The are total four tasks in this case study that are organized into two categories: (i) full-system simulation using gem5 and NVMain, (ii) Stand-alone trace-driving simulation using NVMain. The first two tasks focus on exploring the performance trade-offs of memory controller scheduling policies and hybrid memory setups, and the last two tasks cover trace-based simulations and the implementation of new operations in NVMain. All file paths used in these instructions as well as the current working directory are relative to \texttt{root/simulator/}, with root being the toolchain's repository's root directory. 

~\\\noindent \textbf{Performance Comparison of Scheduling Policies:}\\
The impact of different scheduling policies on performance is illustrated by compiling the matrix multiplication (MM) application (see Appendix~\ref{app:tudr:scheduling}) and executing it on gem5 with a desired configuration. The scheduling policy (FCFS, FRFCFS, or FRFCFS-WQF) can be configured in the configuration file, \texttt{nvmain/Config/PCM\_ISSCC\_2012\_4GB.config}, before running any simulation. For instance, changing the memory controller policy from the \texttt{FCFS} to \texttt{FRFCFS} or \texttt{FRFCFS-WQF} and examining statistics such as the number of row buffer hits and misses and the access latencies (\texttt{rb\_hits, rb\_miss, averageLatency and averageTotalLatency}), provides insight into performance differences.
\texttt{averageLatency} reflects the memory module's internal service time, excluding system-level queuing delays, while \texttt{averageTotalLatency} represents end-to-end latency.  
For \texttt{FCFS}, \texttt{rb\_hit} and \texttt{rb\_miss} are both zero, with \texttt{averageTotalLatency} in the millisecond range.  
With \texttt{FRFCFS}, row buffer hits appear but remain low and \texttt{averageTotalLatency} is reduced to $\sim6\mu s$.  
\texttt{FRFCFS-WQF} introduces separate read/write queues, significantly increasing \texttt{rb\_hits} beyond misses. While queue latency rises, \texttt{averageLatency} improves, indicating less read/write operations on NVM cells.  













~\\\noindent \textbf{Hybrid Memory Simulation:}\\
Hybrid main memories that combine DRAM and NVM have been widely explored in research and industry. gem5 supports the simulation and evaluation of configurations integrating DRAM and NVM as main memory. These configurations leverage NVM's high density and persistence alongside DRAM's speed, enabling the design of architectures that enhance memory capacity and efficiency while addressing data retention challenges.

The \texttt{hybrid\_example.py} file provides an example configuration of such a hybrid memory system.
To test this configuration, Appendix~\cref{app:tudr:hybrid} presents some commands to run the STREAM benchmark~\cite{stream_bench}.
In the \texttt{stream.c} file, lines \texttt{181–196} demonstrate how to allocate arrays in the data segment as pinned memory statically. The memory address ranges are defined in line \texttt{89} of \texttt{hybrid\_example.py}.
Again, we refer to the commands in Appendix~\cref{app:tudr:hybrid} to compile and simulate the stream kernel with the desired pinned memory size.


The performance results of the hybrid memory configuration on the STREAM benchmark demonstrate improvements in both latency and bandwidth compared to a DRAM-only system. The hybrid setup balances the strengths of DRAM’s high-speed access with the cost-effectiveness and persistence of NVM. 
Further system-level optimizations, such as more advanced memory scheduling policies and fine-tuned data placement strategies, could further leverage the potential of hybrid memory systems for diverse application scenarios.
Section~\ref{sec:nvm-cache} provides another use case of hybrid memory simulation for hybrid caches.

~\\\noindent \textbf{Trace-Driven Simulation:}\\
NVMain also supports trace input files (see Figure~\ref{fig:nvmain}), which must follow the specified format (\myCircled{1}) in Figure~\ref{fig:nvmain_changes}.
To simulate a trace file on NVMain (either in \texttt{fast}, \texttt{debug}, or \texttt{prof} mode), the configuration file, the trace file, and the number of simulation cycles should be specified.
Appendix~\cref{app:tudr:traceSim} illustrates the specific commands to view an example trace file and run it using NVMain. The trace-driven simulation specifically focuses on the memory subsystem (and not on the full system) and enables using NVMain with other simulators/frameworks, i.e., the input trace does not necessarily need to come from gem5. It also provides more flexibility and control over the simulation which is why we will use it in the next task to introduce new memory operations. 
The integrated full-system simulation with gem5 would require extending a few classes in the memory model of gem5 to make it work. For instance, if the new operation is neither a read nor a write, a new type should be specified; if the operation has to be exposed to the programmer, a new instruction or pseudo-instruction should be introduced.
Additionally, changes would be required to the cache coherence protocol to ensure data coherence in CPU caches. 

~\\\noindent \textbf{Adding a New Operation:}\\
NVMain can be extended to support new operations and simulate tailored memory system designs, e.g., for CiM (Compute-in-Memory). For instance, it has been demonstrated by previous research that bulk data copying and initialization can be done within the main memory to eliminate data transfer over the external bus.
For DRAM, the RowClone \cite{kit/rowclone} mechanism can copy an entire row from a source address to a destination address via back-to-back \emph{activation}. Similarly, in-memory mirroring \cite{rowMirroring} performs copy directly within the resistive crossbar memories without the need to read the data.

To implement RowClone in NVMain, multiple classes need to be modified, as summarized in \myCircled{2} of Figure~\ref{fig:nvmain_changes}. 
Copying a source row (\texttt{src}) to a destination row (\texttt{dst}) falls into one of these two cases: both rows are within the same subarray, in which case the RowClone (\texttt{RC}) is performed using Fast Parallel Mode (FPM); or \texttt{src} and \texttt{dst} reside in different banks or different subarrays within the same bank, hence accomplished by the Pipelined Serial Mode (PSM)~\cite{kit/rowclone}. 
In the provided code (details in Appendix~\ref{app:tudr:adding_ops}), the RowClone functionality is limited to FPM. The code parses memory commands whose opcode is \texttt{RC} and contains two addresses, as highlighted in \myCircled{1}. 
To support FPM, the subarray model accepts back-to-back \texttt{ACTIVATE}s if they belong to the same subarray, otherwise, it drops the second \texttt{ACTIVATE}.
Appendix~\ref{app:tudr:adding_ops} presents commands to compile NVMain as a stand-alone tool and simulate \texttt{RC} operations from a trace file.
Table~\ref{fig:rc_validation} presents the memory latency and energy improvements achieved by FPM RowClone for bulk copying and zeroing 4KB of data. The baseline for comparison is the same operation performed by the CPU (details in Appendix~\ref{app:tudr:adding_ops}). FPM~\cite{ambit} refers to the estimations provided in the original paper, while FPM (NVmain) refers to our validation results from this case study. 

\begin{figure}[ht!]
    \centering
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{TUD-figures/nvmain_changes.png} 
        \caption{NVMain classes to modify}
        \label{fig:nvmain_changes}
    \end{subfigure}
    \hfill
    
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
\begin{tabular}{clll}
\hline
\multicolumn{1}{l}{}                                                                                             & \textbf{Copy}   & \textbf{Zero} &        \\ \hline
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Memory\\ Energy\\ (µJ)\end{tabular}} & Baseline                         & 3.6        & 2.0  \\
                                                                                & FPM~\cite{ambit}                  & 0.04        & 0.04 \\
                                                                                & FPM (NVMain)                     &  0.04       &   0.05\\ \hline
                                                                                
\multirow{3}{*}{\begin{tabular}[c]{@{}c@{}}Latency\\ (ns)\end{tabular}}         & Baseline                         & 1046        & 546 \\
                                                                                & FPM~\cite{ambit}                  & 73        & 73  \\
                                                                                & FPM (NVMain)                     &  90       &  90 \\ \hline
\end{tabular}
        \caption{Latency and energy validation}
        \label{fig:rc_validation}
    \end{subfigure}

    \caption{Overview of the modifications made to NVMain to support RowClone and its validation}
    \label{fig:combined}
\end{figure}

\begin{insightbox}

This case study provided insights into customizing NVMain, both stand-alone and with gem5, covering main memory simulations based on DRAM, NVM, and hybrid DRAM-NVM configurations, and new memory operations. Specifically, the RowClone operation was demonstrated as an example, highlighting its potential for efficient in-memory bulk copies.
\end{insightbox}