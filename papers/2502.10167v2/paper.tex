\PassOptionsToPackage{table}{xcolor} 

\documentclass[manuscript,screen]{acmart}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{todonotes}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{listings}
\usepackage{tcolorbox}
\usepackage{hyperref}
\usepackage{tabularx}
\usepackage[capitalise]{cleveref}
\crefformat{section}{#2\S#1#3} 
\crefformat{subsection}{#2\S#1#3}
\crefformat{subsubsection}{#2\S#1#3}
\crefformat{figure}{#2Fig. #1#3}
\crefformat{table}{#2Table #1#3}

\usepackage[labelformat=simple]{subcaption}
\renewcommand\thesubfigure{(\alph{subfigure})}  
\usepackage{multirow}
\usepackage{siunitx}
\usepackage{makecell}
\usepackage{pgf}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{url}
\setuptodonotes{inline}

\usepackage{tikz}   
\newcommand{\CRicon}[1]{
	\tikz[baseline=(char.base)]{
		\node[shape=circle, rounded corners=3pt, inner sep=0pt,
		top color=black, bottom color=black, text=white,
		font=\bfseries] (char)
		{#1};
	}
}

\usepackage{circledsteps}
\newcommand\myCircled[2][]{\ifmmode
\Circled[fill color=black,inner color=white,#1]{\mathsf{#2}}
\else
\Circled[fill color=black,inner color=white,#1]{\sffamily#2}
\fi
}



\geometry{a4paper, margin=1in}
\setlength{\parskip}{1em}
\setlength{\parindent}{0em}


\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{SPP 2377 Collaboration}
\fancyhead[R]{\thepage}
\fancyfoot[C]{Draft}


\newtcolorbox{insightbox}{
    colback=purple!10, colframe=purple!80,
    title=Insight:, fonttitle=\bfseries, sharp corners,
    top=2mm, bottom=2mm, left=1mm, right=1mm
}


\lstset{
    basicstyle=\ttfamily\footnotesize,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red}
}

\newcommand\lk[1]{{\color{magenta!100}{\textbf{Lokesh}:} #1}}
\newcommand\yc[1]{{\color{purple!60}(\textbf{Tony}: #1)}}
\newcommand\ts[1]{{\color{green!60!black}(\textbf{Tristan}: #1)}}
\newcommand\nw[1]{{\color{blue!30!black}(\textbf{Nils FAU}: #1)}}
\newcommand\nils[1]{{\color{blue!80!blue}(\textbf{Start Nils}: #1 :\textbf{EndNils})}}
\newcommand{\ali}[1]{{\itshape\color{teal}\textbf{ALI:~}#1}}
\newcommand{\jp}[1]{{\color{olive}\textbf{Joao:~}#1}}
\newcommand{\as}[1]{{\color{red}\textbf{Asif:~}#1}}
\definecolor{asparagus}{rgb}{0.53, 0.66, 0.42}
\newcommand{\jc}[1]{{\color{asparagus}\textbf{Jeronimo:~}#1}}


\newcommand\bg[1]{{\color{red!60}(Background $\rightarrow$#1$\leftarrow$)}}
\newcommand\cs[1]{{\color{green!60}(Case Study $\rightarrow$#1$\leftarrow$)}}

\author{Yun-Chih Chen}
\author{Tristan Seidl}
\author{Nils Hölscher}
\author{Christian Hakert}
\author{Minh Duy Truong }
\author{Jian-Jia Chen}
\authornote{These authors contributed equally to this work.}
\affiliation{
  \institution{TU Dortmund University}
  \city{Dortmund}
  \country{Germany}
}
\email{{yunchih.chen, tristan.seidl, nils.hoelscher, minhduy.truong, jian-jia.chen}@tu-dortmund.de}

\author{João Paulo C. de Lima\textsuperscript{$\dagger$}}
\author{Asif Ali Khan}
\author{Jeronimo Castrillon\textsuperscript{$\dagger$}}
\authornotemark[1]
\affiliation{
  \institution{Technische Universität Dresden, ScaDS.AI\textsuperscript{$\dagger$}}
  \city{Dresden}
  \country{Germany}
}
\email{{joao.lima, asif_ali.khan, jeronimo.castrillon}@tu-dresden.de}

\author{Ali Nezhadi}
\author{Lokesh Siddhu}
\author{Hassan Nassar}
\author{Mahta Mayahinia}
\author{Mehdi Baradaran Tahoori}
\author{Jörg Henkel}
\authornotemark[1]
\affiliation{
  \institution{Karlsruhe Institute of Technology (KIT)}
  \city{Karlsruhe}
  \country{Germany}
}
\email{{ali.nezhadi, lokesh.siddhu, hassan.nassar, mahta.mayahinia, mehdi.tahoori,henkel}@kit.edu}

\author{Nils Wilbert}
\author{Stefan Wildermann}
\author{Jürgen Teich}
\authornotemark[1]
\affiliation{
  \institution{Friedrich-Alexander-Universität Erlangen-Nürnberg}
  \city{Erlangen}
  \country{Germany}
}
\email{{nils.wilbert, stefan.wildermann, juergen.teich}@fau.de}

\begin{document}


\title{Modeling and Simulating Emerging Memory Technologies: A Tutorial}
\maketitle
\renewcommand{\shortauthors}{Y. Chen et al.}
\authornote{These authors contributed equally to this work.}

\section*{Abstract}
Non-volatile Memory (NVM) technologies present a promising alternative to traditional volatile memories such as SRAM and DRAM. Due to the limited availability of real NVM devices, simulators play a crucial role in architectural exploration and hardware-software co-design. This tutorial presents a simulation toolchain through four detailed case studies, showcasing its applicability to various domains of system design, including hybrid main-memory and cache, compute-in-memory, and wear-leveling design. These case studies provide the reader with practical insights on customizing the toolchain for their specific research needs. The source code is open-sourced.



\section{Introduction}\label{sec:Introduction}
As semiconductor technology advances, energy efficiency is becoming a significant barrier to scalability.
This challenge has resulted in the development of heterogeneous computing devices and the \textit{Dark Silicon} phenomenon, in which portions of a chip are selectively turned off to save energy.
Non-volatile memory (NVM) technologies are a promising solution because they provide energy-efficient alternatives to traditional volatile memory.
Historically confined to storage applications such as SSDs and read-only instruction storage in embedded systems, new NVM technologies with access latencies approaching those of DRAM \cite{pcm:2024, 10145822} have opened the door for their use as main memory with a moderate performance penalty.

However, diverse application contexts expose different design requirements.
On the one hand, small and remote devices (e.g., edge servers at traffic lights or base stations in hard-to-access locations) may operate for extended durations with limited maintenance. In these settings, high power consumption, frequent dirty data writebacks, and costly data restoration upon wake-up are especially problematic for current memory technologies, including DRAM, SSD, and eFlash~\cite{8351201}.
On the other hand, data-center workloads continue to grow in scale and complexity, pushing SRAM and DRAM to their limits in terms of performance, bandwidth, density, and energy usage. 
This has driven the evolution of memory technology toward greater heterogeneity and specialization.
NVMs, with their longer retention times, are well-positioned to play an important role in this transition, particularly as an adaptable, energy-efficient memory option for a broader spectrum of applications, ranging from power-constrained wearable devices up to large-scale enterprise systems.


However, to fully realize NVMs' potential in mainstream computing, extensive research in computer architecture is required to address their challenges
, including limited endurance, higher fault rates, and expensive write operations.
These characteristics necessitate adaptive hardware-software co-design solutions to improve performance and longevity.
Effective co-design strategies must align software requirements with hardware limitations using techniques such as wear leveling, error correction, and vertical integration, ensuring that both software and hardware work together to maximize NVMs' capabilities.

Recognizing the importance of addressing these challenges as part of a broader effort to advance memory technologies, the German Research Foundation (DFG) launched SPP 2377 in 2022.
This 6-year national priority program brings together 23 principal investigators from 13 institutions and is subdivided into 14 research projects spanning topics such as computer architecture, databases, compilers, and operating systems.
The program investigates a wide range of issues related to emerging memory technologies, including commercially available ones, such as FRAM, Intel Optane DIMMs, and UPMEM\cite{devaux2019true}, as well as experimental NVMs that are still in development.


This paper is based on a 3-hour tutorial session delivered at the ESWEEK 2024 conference. 
It describes 
a highly configurable NVM simulation toolchain, developed in a collaborative effort within the SPP 2377. 
At its core, the toolchain builds atop gem5~\cite{binkert:2011} and NVMain (using NVMain2.0~\cite{poremba:2015}). 
With this, we support further advancements in the field by making our extensions accessible to the broader research community.

The toolchain enables the research community to do architectural exploration and study the behavior of target applications before the NVM devices become commercially available. 
This paper introduces the reader to the basic operation of the toolchain and provides a hands-on guide for the usage of our extensions, presented through a series of case studies that demonstrate the types of architectural exploration the toolchain enables.
We focus on four NVM research issues to illustrate the toolchain’s capabilities.
The first case study focuses on hybrid main memory (\cref{sec:nvm-dram}).
The second case study focuses on trace generation and trace-based wear-out analysis, which allows designers to examine memory access patterns and assess the behavior of target applications (\cref{subsec:tudo}).
The third case study explores architectural trade-offs in NVM-SRAM hybrid cache designs (\cref{sec:nvm-cache})
Finally, the fourth case study demonstrates the use of NVM in Compute-in-Memory (CiM) architectures (\cref{sec:nvm-cim}). 

\section{Background}\label{sec:Background}


NVM allows devices to wake up, operate, and power down without transferring data between memory and storage. NVM also significantly reduces standby power consumption compared to SRAM.  Lastly, most NVMs allow storing multiple bits per memory cell, thus increasing memory density.
From wearables and implantables to IoT and edge devices, NVM has the potential to enable persistent low power operation.
For example, a cell phone could extend its battery life by incorporating a low-power NVM-based co-processor alongside its high-performance processor.
The co-processor could monitor inputs, such as audio commands, while the phone is in sleep mode, waking the main processor only when necessary.

NVM can drive further advancements in scaling advanced data center SoCs, such as the ARM Neoverse series~\cite{ArmN1}, which feature hundreds of cores per chip. These SoCs operate at the edge of physical limitations, making power efficiency and effective cooling critical to maintaining performance without thermal throttling.  Research on advanced sub-1nm process nodes reveals that, depending on operating temperature, SRAM-based L2 caches can make up 25\% to 50\% of data center-grade SoCs' overall power consumption \cite{thermal_soc:24}.  On the other hand, NVM offers an energy-efficient, area-efficient alternative to SRAM, with the potential to significantly reduce the leakage current \cite{10750212}.

\subsection{NVM Technologies}
\label{subsec:materials}
There are many different types of NVM technologies.
Some rather mature NVM technologies include: Phase Change Memory (PCM), Resistive Random-Access Memory (ReRAM), Ferroelectric RAM (FRAM), and Spin-Transfer Torque RAM (STT-RAM).
Unlike volatile memory, which stores data using electric charge, many NVM technologies represent data through resistance levels, but each technology uses different materials and distinct physical principles to alter the state of the memory cell. 
PCM uses heat to change the material state between crystalline and amorphous, while ReRAM moves ions within a dielectric material. On the other hand, STT-RAM encodes data through the magnetic orientation of magnetic tunnel junctions (MTJs).


When PCM was introduced to the market as Intel Optane Persistent Memory in 2019, it was unable to compete with DRAM due to cost concerns. However, recent developments in material science research have revived interest in its potential, especially for processing-in-memory architectures, because of its exceptional endurance (i.e., $2 \cdot 10^8$ cycles) and quick access times (i.e., 40 ns) \cite{pcm:2024}.
ReRAM as a replacement for SRAM can address the power and cooling issue for ultra-dense 3D ICs with tightly coupled logic and memory \cite{rram:23}. Notably, recent fabrication of ReRAM-based AI accelerators has shown significant energy savings by eliminating the need to load weights from off-chip memory \cite{rram:22}.
FRAM, with its fast read/write speeds of around 50\,ns, low power consumption, and high switching durability (i.e., $10^{13}$ cycles), has been widely adopted in RFID tags and microcontrollers \cite{fram:24}.

For a detailed comparison of NVM material and architectural exploration for trade-offs in access latency, area, power efficiency, and endurance, we advise taking a look into Pentecost~et~al.'s comprehensive survey \cite{9773239}.

\subsection{Commercial Maturity of NVM}
\label{sec:commercial_maturity}
Commercially, NVMs have 
been in mass production for years. 
For example, Samsung began STT-MRAM's mass production in 2019.  8Mb of this NVM is used in Sony’s GPS receiver chip as part of Huawei GT2 smartwatch \cite{10145822}.  The NVM enables a remarkable two-week battery life, far exceeding the few days of a volatile-memory-based smartwatch. In the data center market, Intel’s release of Optane Persistent Memory (Optane DCPMM) in 2019 brought significant attention to in-memory data structures for persistent memory. However, the discontinuation of Optane in 2022 due to low profitability reflects a broader issue: NVM’s pricing remains uncompetitive with mature memory technologies. As shown in \cref{table:price_per_bit}, data from DigiKey as of November 2024 \cite{digikey} reveals that NVM capacities are still significantly more expensive than DRAM or NAND flash. While this cost disparity discouraged some researchers from exploring NVM further, NVM is finding success in specialized markets where its unique capabilities align closely with application needs.




For example, NXP plans to integrate NVM into automotive microcontrollers, while Samsung and Sony are utilizing NVM as a frame-buffer memory in advanced image sensors. Avalanche Technology has proposed deploying an 8 Gb MRAM package in low earth orbit for space applications. In these specialized domains with well-defined software behavior, NVM provides an opportunity for tight hardware-software co-design. Unlike general-purpose computing, where hardware is designed to accommodate any software, these domains enable tailoring of NVM’s capabilities to specific applications.

While NVM alone might be expensive, pairing NVM with mature memory technologies proves cost-effective. IBM, e.g., integrated Everspin’s 1 Gb STT-MRAM chips into the IBM FlashCore Module, an enterprise-grade solid-state drive, to achieve high reliability.  Another promising use case is an SRAM and STT-MRAM hybrid last-level cache, as we explore in \cref{sec:nvm-cache}.
The low profitability of NVM also stems from misconceptions about its role. Treating NVM as a drop-in replacement for DRAM imposes stringent requirements—such as long retention times, errorless access, and low latency—that drive up costs unnecessarily. Many applications do not require these demanding features, but aligning application requirements with NVM capabilities requires tight hardware-software co-design. This process, however, demands extensive testing and iteration. Simulators play a critical role in this, offering a cost-effective and flexible way to explore hardware-software co-design strategies, optimize NVM integration, and adapt its capabilities to meet application-specific needs.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|c|c|c|}
        \hline
        \textbf{Memory Type} & \textbf{Price (USD)} & \textbf{Capacity} & \textbf{Price per gigabit (USD)} \\ \hline
        Infineon FRAM        & 50                  & 16 Mbit           & $3125 $ \\ \hline
        Infineon ReRAM        & 10                  & 512 Mbit          & $19.53$ \\ \hline
        Everspin STT-MRAM    & 100                 & 1 Gbit            & $100$ \\ \hline
        Micron DRAM          & 108                 & 256 Gbit          & $0.42 $ \\ \hline
        Kioxia NAND Flash    & 132                 & 8 Tbit            & $0.0165$ \\ \hline
    \end{tabular}
    \caption{Price, capacity, and price per bit for various memory types (source: Digikey).}
    \label{table:price_per_bit}
\end{table}

\subsection{NVM Simulation}
\label{subsec:nvm-simulation}
To effectively adapt application-specific needs to a target NVM material’s unique characteristics (e.g., endurance limitations and variable access latencies), researchers must iteratively explore co-design strategies that align hardware capabilities with software requirements.  
A significant barrier to advancing NVM research is the lack of accessible experimentation platforms. Academic researchers face the difficulties of limited access to real NVM devices, which are often proprietary, while memory manufacturers encounter high costs associated with evaluating software behavior on physical NVM chips, given their relatively slow access times and destructive write operations.



To overcome these limitations, software-based simulations offer a cost-effective means of exploring the design space. They enable detailed modeling of system components and interactions, facilitating iterative co-design between hardware and software. Simulations allow researchers to test architectural parameters, analyze memory behavior, and develop optimization techniques without the constraints of physical hardware.
An example, among many, is recent work that investigates the effect of replacement policies on the lifetime of NVM caches~\cite{escuin_hpca23}. 

There are two primary simulation methods widely used in NVM research:
(a) \textit{Cycle-accurate full-system simulations} provide precise modeling of all system components and produce results that closely approximate running on real device behavior. While this precision captures intricate interactions such as cache and TLB behaviors, it comes at the cost of significantly longer execution times.
(b) \textit{Trace-based simulations} run on pre-produced logs of memory accesses.  It offers a faster alternative to cycle-accurate approaches, making them suitable for tasks like analyzing memory usage patterns or evaluating wear-leveling algorithms. However, their accuracy depends heavily on the quality and comprehensiveness of the traces used.


\subsection{Studied Toolchains}
\label{subsec:toolchain}
In this work, we focus on cycle-accurate full-system simulation for NVM research and offer a tutorial on a toolchain that consists of gem5 and NVMain. The toolchain is widely used in the research domain of emerging memory technologies~\cite{dwm-nvmain:19, hakert:2020:base, hakert:2022, hoelscher:2022, khan2019rtsim, khan2023downshift, asif:18, Wilbert:2024a, Wilbert:24b}. 
The tutorial is composed of four case studies that showcase our extensions to the toolchain, aiming to familiarize the reader with a powerful utensil for their NVM research.

gem5~\cite{binkert:2011} is a widely used cycle-accurate full-system simulator for computer architecture research. It models critical side effects, including cache set behavior, TLB misses and hits, and internal memory device states such as buffers and row hits.
NVMain~\cite{poremba:2015} extends gem5 to model NVM technologies alongside DRAM and SRAM. \cref{fig:nvmain}
\begin{figure}[t]
    \includegraphics[width=0.8\columnwidth]{TUD-figures/nvmain.png}
    \caption{
        Overview of the NVMain flow. Adapted from~\cite{poremba:2015,khan2019rtsim}.
    }
    \label{fig:nvmain}
\end{figure}
shows an overview of NVMain.
It can be configured to emulate the timing and energy parameters corresponding to different NVM technologies, as well as to support detailed investigations of memory behavior.
Its subarray modeling captures core memory operations, such as the varied access latency of multi-level cell (MLC).  It also performs access accounting, which reflects memory wearing and enables studies of endurance enhancement.
NVMain also offers fault injection to analyze the impact of memory defects and failures.



To ease evaluation, the toolchain comes with a set of benchmarks that offer diverse memory access patterns. This set was assembled throughout our previous work~\cite{hakert:2020:base, hakert:2020:softwear, hakert:2022, hoelscher:2023} and it includes benchmarks from MiBench~\cite{guthaus:2001} as well as from Olden~\cite{rogers:1995}. These benchmarks were previously~\cite{hoelscher:2023} ported to Unikraft~\cite{kuenzer:2021}, which we added to the toolchain in~\cite{hakert:2020:split}. Unikraft is a unikernel kit that bundles the required kernel functionality with the application into a single binary. This minimizes binary size by excluding unnecessary services and consolidates all functionality into a single address space, where the latter can simplify memory access analysis. Unikraft is highly compatible with Linux, which makes it simple to port Linux applications to Unikraft.

































\section{Case Studies}
The four case studies presented in this tutorial paper are as follows: 

\begin{enumerate}
    \item \textbf{Memory Subsystem Modeling:} 
     This case study introduces the configuration (e.g., timing and scheduling) and simulation of DRAM and NVM-based main memories, and the implementation of custom memory operations in NVMain, such as RowClone for in-memory bulk copying.  By the end of this case study, readers will be equipped with the knowledge to evaluate and refine memory system designs, enabling them to make informed decisions tailored to specific application needs.

    \item \textbf{Trace Writer:}  This case study introduces the trace-writing capabilities of NVMain2.0, providing step-by-step guidance on integrating and customizing trace writers for diverse use cases. The reader will learn how to set up a basic trace writer, integrate it into the toolchain’s build process, and log essential memory access information. Additionally, this case study explores how to analyze logs for tracking NVM wear-out and examining memory access patterns. By the end, the reader will be equipped with the knowledge to extend NVMain2.0’s trace-writing functionality for specific research or development needs.
 
    \item \textbf{NVM-SRAM Hybrid Caches:} This case study presents an architectural exploration of hybrid caches consisting of SRAM and STT-RAM cells. The reader will learn how to configure asymmetric read/write latencies for different memory materials, how to vary the non-volatility ratio, and how to evaluate their energy and performance impact, demonstrated through an image processing task (read-intensive) and a merge sort algorithm (write-intensive). The case study enables exploration of novel cache replacement policies and evaluating the trade-offs of hybrid memory systems specific to applications of interest.
    
    \item \textbf{Compute-in-Memory (CiM):} 
    This case study details the step-by-step modifications required to integrate Compute-in-Memory (CiM) functionality into gem5, including extending memory controllers, implementing CiM-specific operations, and adapting simulation parameters for accurate modeling.
     The resulting CiM-enabled system is evaluated with several real-world applications.
    The reader gains insights into the architectural differences between CiM and traditional Processing-in-Memory (PIM), the trade-offs of different CPU-CiM communication methods, and the physical placement of CiM components within memory modules. 
    
    
\end{enumerate}

For setting up the toolchain, please see the setup instructions provided in appendix~\cref{app:setup}.

\input{case-studies/TUDresden-case-study}
\input{case-studies/TUDortmund-case-study}
\input{case-studies/FAU-case-study}
\input{case-studies/KIT-case-study}

\section{Conclusion}
In this collaborative work from the SPP 2377 priority program, we present a tutorial that demonstrates extensions to a toolchain based on gem5 and NVMain. We show how to use the toolchain for architectural exploration and development of co-design strategies for emerging memory technologies. Four case studies were discussed: hybrid main memory, trace-based wear-out analysis, heterogeneous cache design, and compute-in-memory architectures. These case studies not only demonstrate the toolchain's versatility and depth, but also provide researchers with the knowledge they need to extend and customize it for their specific applications. We also provide open access to the source code and setup instructions in the hope of increasing the reproducibility of research in NVM co-design.

\bibliographystyle{plain}
\bibliography{references}


\input{Appendices}

\end{document}
