\section{Preliminaries}

\subsection{Some Linear Operators}
\paragraph{Notation.} We switch from $\{0,1\}$ notation to $\{\pm 1\}$ notation to facilitate later Fourier analysis.
%Regard the set $\pbra{\{\pm1\}^{n}}^k$ as isomorphic to $\{\pm1\}^{nk}$. That is, 
We will regard elements of $\{\pm1\}^{nk}$ as tuples $(X^1,\dots,X^k)$, where each $X^i\in \{\pm1\}^n$. If $X^i \in \{\pm1\}^n$, let $X^i_S$ denote the vector $X^i$ restricted to indices in $S$, so that if $S=\{a_1,\dots,a_{|S|}\}$ with $a_1<\dots<a_{|S|}$ then $X^i_S=(X^i_{a_1},\dots,X^i_{a_{|S|}})$. For two vectors $x,y\in \{\pm1\}^n$, define $\Delta(x,y)=\{a\in[n]: x_a\neq y_a\}$. Unless otherwise specified, $\log$ is $\log_2$. For positive integers $a\leq b$ denote $[a,b]=\{a,a+1,\cdots, b-1,b\}$ and let $[a]=[1,a]$. Let $\mathbf{1}$ be the all 1s vector, with length determined by context. When $L$ is a self-adjoint operator let $\lambda_2(L)$ be its second-smallest distinct eigenvalue. If $g\in\mathfrak{S}_{\{0,1\}^{\gamma}}$ is a permutation and $S\subseteq [n]$ has $|S|=\gamma$ then define $g^S\in\mathfrak{S}_{\{0,1\}^n}$ by setting $(g^Sx)_S=g(x_S)$, where coordinates are interpreted to be in lexicographic order, and $(g^Sx)_{[n]\setminus S}=x_{[n]\setminus S}$.

When $\mathcal U$ is a set, we let $\R^{\mathcal U}$ denote the vector space of all functions $\mathcal U\to \R$. We equip this space with the inner product $\left\langle \cdot,\cdot\right\rangle$ given by
\begin{align*}
    \left\langle f,g\right\rangle =& \sum_{x\in \mathcal U}f(x)g(x)
\end{align*}
for $f,g:\mathcal U\to \R$. Also define the norm by $\norm{f}_2=\sqrt{\left\langle f,f\right\rangle}$.

\paragraph{Random walk operators.} Suppose $R$ is a random walk operator corresponding to drawing the next state from the distribution $\mcD_X$, where $X$ is the current state. For $S\subseteq \mathcal{U}$ we will often write $\Pr_{\bm{Y}\sim \mcD_x}[\bm{Y}\in S]$ as $\Pr[X \to_{T_\mcD} S]$. When $S=\{Y\}$ is a singleton we use write $\Pr[X \to_{T_\mcD} S]=\Pr[X \to_{T_\mcD} Y]$.
\begin{fact}\label[fact]{fact:onesteptransition}
    For $X, Y \in \{\pm1\}^{nk}$, $\ip{e_X}{Re_Y} = \Pr[X \to_{R} Y]$.
\end{fact}

This above fact is especially useful as it can be extended across sequential distributions:
\begin{fact}\label[fact]{fact:twosteptransition}
    For $X, Y \in \{\pm1\}^{nk}$, $\ip{e_X}{R_1R_2e_Y} = \Pr[X \to_{R_2R_1} Y]$.
\end{fact}
It is worth pointing out the order of the operators on the bottom is written interpreting $R_1$ and $R_2$ as matrices. The order of application effectively flips under the composition here from how it is written above, so it is important to keep track of orientation.

Throughout the proof of \Cref{thm:1D main}, we use the following notation:
\begin{definition}\label{def:the random walk operator}
    Let $n$ be fixed. Define the distribution $\mathcal{D}^{m,S,k}_X$ to be the law of $\bm{Y}$, where to draw $\bm{Y}$ first one draws $\bm{\sigma'}\in \mathfrak{S}_{\{\pm1\}^{|S|}}$ and then defines $\bm{Y} = \pbra{(\bm{\sigma'}x^1_S,x^1_{\overline S}),\dots,(\bm{\sigma'}x^k_S,x^k_{\overline S})}$. The following is the corresponding random walk operator.
    \begin{align*}
        (R_{n,S,k}f)(X)=& \Ex_{\mathbf{Y} \sim\mathcal{D}^{m,S,k}_{X}}\sbra{f(\mathbf{Y})}.
    \end{align*}
\end{definition}



\begin{fact}\label{fact:self-adjoint}
    For any $n,S,k$ we have that $R_{n,S,k}$ is self-adjoint.
\end{fact}

\begin{fact}\label{fact:uniform is stationary}
    For any $S\subseteq[n]$ we have $R_{n,S,k}\mathbf{1}=\mathbf{1}$.
\end{fact}

In some contexts it is easier to work with the Laplacians of these operators:
\begin{definition}
    For any operator $R$ on a vector space $V$ let $L(R)$ denote its Laplacian $\Id_V-R$. 
\end{definition}

\subsection{Fourier Analysis of Boolean Functions}
\begin{definition}
    Let $S_1,\dots,S_k\subseteq[n]$. Then define the function $\chi_{S_1,\dots,S_k}$ by defining for $X\in\{\pm1\}^{nk}$,
    \begin{align*}
        \chi_{S_1,\dots,S_k}(X)=&\prod_{i\in [k]}\prod_{a\in S_i}X^i_a.
    \end{align*}
\end{definition}

\begin{fact}[\cite{ODonnell2014}]\label{fact:fourier characters}
    The functions $\chi_{S_1,\dots,S_k}$ for $S_1,\dots,S_k\subseteq[n]$ form an orthogonal basis of $\R^{\{\pm1\}^{nk}}$.
\end{fact}

\subsection{Comparison Bounds}
Recall that our main idea is to approximate one random walk by a simpler random walk. The following \Cref{lem:TV distance bound} formalizes this linear algebraically.
\begin{lemma}\label{lem:TV distance bound}
    Suppose $A$ and $B$ are self-adjoint random walk matrices on a domain $\mathcal{U}$. Let $\mathrm{Supp}\subseteq\mathcal{U}$. Then for any $f,g:\mathcal{U}\to\R$ with $\mathrm{Supp}(f)=\mathrm{Supp}(g)=\mathrm{Supp}$ we have
    \begin{align*}
        \abs{\left\langle f, (A-B)g\right\rangle}
        &\leq \sqrt{\sum_{X\in \mathrm{Supp}}f(X)^2\sum_{Y\in \mathrm{Supp}}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}
         \\
         &\qquad\cdot\sqrt{\sum_{X\in \mathrm{Supp}}g(X)^2\sum_{Y\in \mathrm{Supp}}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}.
    \end{align*}    
    In particular, when $f=g$ we get
    \begin{align*}
        \abs{\left\langle f, (A-B)f\right\rangle}\leq \sum_{X\in \mathrm{Supp}(f)}f(X)^2\sum_{Y\in \mathrm{Supp}(f)}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}},
    \end{align*}
\end{lemma}
\begin{proof}
    We directly compute
\begin{align*}
        &\abs{\left\langle f,(A-B)g\right\rangle}\\
        =&\abs{\sum_{X\in \mcU}f(X)\pbra{(Ag)(X)-(Bg)(X)}}\\
        =&\abs{\sum_{X\in \mcU}{f(X)\sum_{Y\in \calU}g(Y)\pbra{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}}\\
        \leq & \sum_{X,Y\in \mathrm{Supp}}\abs{f({X})}\abs{g( Y)}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}\tag{Support of $f$, $g$}\\
        \leq & \sqrt{\sum_{X,Y\in \mathrm{Supp}}f(X)^2\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}\sqrt{\sum_{X,Y\in \mathrm{Supp}}g(Y)^2\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}\\
        = & \sqrt{\sum_{X,Y\in \mathrm{Supp}}f(X)^2\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}\sqrt{\sum_{X,Y\in \mathrm{Supp}}g(Y)^2\abs{\Pr[Y \to_A X]-\Pr[Y \to_B X]}}\tag{Self-adjointness of $A$ and $B$}\\
        = & \sqrt{\sum_{X\in \mathrm{Supp}}f(X)^2\sum_{Y\in \mathrm{Supp}}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}\\
        &\qquad\cdot\sqrt{\sum_{X\in \mathrm{Supp}}g(X)^2\sum_{Y\in \mathrm{Supp}}\abs{\Pr\sbra{X\to_A Y}-\Pr\sbra{X\to_B Y}}}.
    \end{align*}
\end{proof}


This \Cref{lem:TV distance bound} will be frequently used in conjunction with the following bound relating sampling with replacement and sampling without replacement:
\begin{fact}\label{fact:k^2}
    If $k\leq \sqrt{N}$ then 
    \begin{align*}
        \prod_{i=0}^{k-1}\frac{N}{N-i}\leq 1+\frac{k^2}{N}.
    \end{align*}
\end{fact}
\begin{proof}
    We prove by induction on $i$. If $i=0$ then the result is trivially true. Now assume that $\prod_{i=0}^{k-2}\frac{N}{N-i}\leq 1+\frac{(k-1)^2}{N}$. Then
    \begin{align*}
        &\prod_{i=0}^{k-1}\frac{N}{N-i}
        \leq \pbra{1+\frac{(k-1)^2}{N}}\pbra{1+\frac{k-1}{N-k+1}}\\
        &\leq \pbra{1+\frac{(k-1)^2}{N}}\pbra{1+\frac{k}N}
        \leq \pbra{1+\frac{k^2}{N}}.
    \end{align*}
    The inequalities follow because $k\leq \sqrt{N}$.
\end{proof}


\subsection{Escape Probabilities}
Another idea that is key to our spectral gap proofs is to bound the contribution to spectra of ``badly-behaved regions" of the state spaces of random walks by showing that these sets exhibit probable escape. This \Cref{lem:escape probs} formalizes this linear algebraically.
\begin{lemma}\label{lem:escape probs}
    Suppose $A$ is a random walk matrix on a domain $\mathcal{U}$ such that the uniform distribution on $\mathcal{U}$ is a stationary distribution for $A$. Let $f,g:\mathcal{U}\to\R$ be such that for any $X\in \mathrm{Supp}(f)$,    
    \begin{align*}
        \Pr\sbra{X\to_{A}\mathrm{Supp}(g)}&\leq \epsilon.
    \end{align*}
    Then
    \begin{align*}
         \abs{\left\langle f,Ag\right\rangle} \leq \sqrt\epsilon\norm{f}_2\norm{g}_2.
    \end{align*}
\end{lemma}
\begin{proof}
    We directly compute:
    \begin{align*}
        &\abs{\left\langle f,Ag\right\rangle}\\
        =&\;\abs{\sum_{X\in\mathcal{U}}{f(X)\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{g(\mathbf{Y})\mathbf{1}\sbra{\mathbf{Y}\in \mathrm{Supp}(g)}}}}\\
        \leq&\;{\norm{f}_2\sqrt{\sum_{X\in\mathcal{U}}{\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}^2\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{g(\mathbf{Y})\mathbf{1}\sbra{\mathbf{Y}\in \mathrm{Supp}(g)}}^2}}}\tag{Cauchy-Schwarz}\\
        \leq&\;{\norm{f}_2\sqrt{\sum_{X \in\mathcal{U}}{\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}\pbra{\sqrt{\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{g(\mathbf{Y})^2}\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{\mathbf{1}\sbra{\mathbf{Y}\in \mathrm{Supp}(g)}^2}}}^2}}}\tag{Cauchy-Schwarz}\\
        =&\;{\norm{f}_2\sqrt{\sum_{X \in\mathcal{U}}{\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{g(\mathbf{Y})^2}\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{\mathbf{1}\sbra{\mathbf{Y}\in \mathrm{Supp}(g)}}}}}\\
        =&\; \norm{f}_2\sqrt{\sum_{X\in\mathcal{U}}{\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}\Ex_{\mathbf{Y}\sim\mathcal{D}_{X}}\sbra{g(\mathbf{Y})^2}}\Pr\sbra{X\to_{A}\mathrm{Supp}(g)}}.
    \end{align*}
    Now, consider each possible value that $X$ may take. If $X\in\mathrm{Supp}(f)$ then by the assumption, we have that $\Pr\sbra{X\to_{A}\mathrm{Supp}(g)}\leq \epsilon$. Otherwise if $X\not\in\mathrm{Supp}(f)$ we have that $\mathbf{1}\sbra{X\in\mathrm{Supp}(f)}=0$. In either case, we have $\mathbf{1}\sbra{X\in \mathrm{Supp}(f)}\Pr\sbra{X\to_{A}\mathrm{Supp}(g)}\leq \epsilon$. Continuing with our calculation, we find that:
    \begin{align*}
        &\abs{\left\langle f,Ag\right\rangle }
        \leq\norm{f}_2\abs{ \sqrt{\abs{\mcU}\Ex_{\mathbf{X} \sim\mathcal{U}}\sbra{\epsilon\Ex_{\mathbf{Y}\sim\mathcal{D}_{\mathbf{X}}}\sbra{g(\mathbf{Y})^2}}} } =\sqrt\epsilon\norm{f}_2\abs{\sqrt{\abs{\mcU}\Ex_{\mathbf{X} \sim \mathcal{U}}\sbra{\Ex_{\mathbf{Y} \sim \mathcal{D}_{\mathbf{X}}}\sbra{g(\mathbf{Y})^2}}}}.
    \end{align*}
    At this point, we note that sampling $\mathbf{X}$ uniformly at random and then sampling $\mathbf{Y}\sim\mathcal{D}_{\mathbf{X}}$ gives the same distribution as sampling $\mathbf{Y}$ uniformly by stationarity of the uniform distribution. This completes the proof by the following:
    \begin{align*}
        \abs{\left\langle f,Ag\right\rangle}\leq& \sqrt\epsilon\norm{f}_2\abs{\sqrt{\abs{\mcU}\Ex_{\mathbf{X} \in\mathcal{U}}\sbra{g(\mathbf{X})^2}}}=\sqrt\epsilon\norm{f}_2\norm{g}_2.
    \end{align*}
\end{proof}
