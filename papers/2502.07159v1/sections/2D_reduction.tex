\section{Reduction from Two-Dimensional to One-Dimensional Constructions: Proof of \Cref{thm:2D to 1D reduction}}\label{sec:proof of 2D to 1D reduction}

\subsection{Definitions}\label{sec:definitions}
To prove \Cref{thm:2D to 1D reduction} we introduce some new notation required for dealing with bit arrays on a higher-dimensional lattice architecture. 
\subsubsection{Bit Arrays}
We regard an element $x\in \{\pm1\}^{n}$ as a function $x : \sbra{\sqrt{n}\,} \times \sbra{\sqrt{n}\,} \to \{\pm1\}$. 
Similarly, we regard an element $X \in \{\pm1\}^{nk}$ as a function $X : \sbra{\sqrt{n}\,} \times \sbra{\sqrt{n}\,} \times \sbra{k} \to \{\pm1\}$. 
For $X \in \{\pm1\}^{nk}$, and $i,j \in \sbra{\sqrt{n}\,}$, and $\ell \in [k]$, we use the notation:
\begin{itemize}
    \item $X^\ell_{i, j} = X(i, j, \ell) \in \{\pm1\}$
    \item $X^\ell = X \mid_{\sbra{\sqrt{n}\,} \times \sbra{\sqrt{n}\,} \times \{\ell\}} \in \{\pm1\}^{n}$
    \item $X^\ell_{i, \cdot} = X \mid_{\{i\} \times \sbra{\sqrt{n}\,} \times \{\ell\}} \in \{\pm1\}^{\sqrt{n}}$
    \item $X^\ell_{\cdot, j} = X \mid_{\sbra{\sqrt{n}\,} \times \{j\} \times \{\ell\}} \in \{\pm1\}^{\sqrt{n}}$
    \item $X_{i, \cdot} = X \mid_{\{i\} \times \sbra{\sqrt{n}\,} \times [k]} \in \{\pm1\}^{\sqrt{n}k}$
    \item $X_{\cdot, j} = X \mid_{\sbra{\sqrt{n}\,} \times \{j\} \times [k]} \in \{\pm1\}^{\sqrt{n}k}$
\end{itemize}
We will use $\sD_n^{(k)}$ to denote the set of all $X \in \{\pm1\}^{nk}$ such that $X^i \ne X^j$ when $i \ne j$. Unless otherwise specified, $\sD$ refers to $\sD_{n}^{(k)}$.

\subsubsection{Color Classes}
\label{sec:colorclasses}
We partition $\{\pm1\}^{nk}$ into ``color classes'' via the following relation. Let $R^{(\sqrt{n})}$ be a tuple of $\sqrt{n}$ equivalence relations on $[k]$. That is, there is one equivalence relation for each row in $[\sqrt{n}]$. Then we define:
\begin{equation*}
        B_{R^{(\sqrt{n})}} = \left\{ X \in \{\pm1\}^{nk} : \forall i \in \sqrt{n}, X_{i, \cdot}^\ell = X_{i, \cdot}^m \text{ if and only if } \ell \,R_i\, m \right\}.
\end{equation*}

Informally, $X^{\ell}_{i, \cdot}$ and $X^m_{i, \cdot}$ share a color if $X^{\ell}_{i, \cdot} = X^m_{i, \cdot}$. This relation induces a coloring on the rows of $X$. We then say that $X$ and $Y$ are colored the same if all of their rows are colored the same. Since $R^{(\sqrt{n})}$ is an equivalence relation itself, the sets $\{B_{R^{(\sqrt{n})}}\}_{R^{(\sqrt{n})} \in \sR^{\otimes \sqrt{n}}}$ partition $\{\pm1\}^{nk}$ for $\sR$ the set of equivalence relations on $[k]$. Additionally, each $X$ has a unique color class we will denote as $B(X)$. We will also use $\sB$ to denote the set of color classes. This partition is useful in part due to its size.
\begin{fact}
    \label{fact:numofcolorclasses}
    There are $\leq k^{k\sqrt{n}}$ color classes, that is, $\abs{\sB} \leq k^{k\sqrt{n}}$.
\end{fact}

\begin{proof}
    We can count each color class by identifying the partition of each row, of which there are $\sqrt{n}$. Each row consists of $k$ elements, so we can overcount the number as putting the $k$ elements into $k$ partitions, $k^k$.
\end{proof}

We will define a simpler partition that will facilitate much of our analysis:
\begin{align*}
    &B_\text{safe} := \cbra{X \in \sD : \forall \ell \neq  m \in [k], i \in [\sqrt{n}], X^\ell_{i, \cdot} \neq X^m_{i, \cdot}},\\
    &B_\text{coll} := \sD \setminus B_\text{safe},\\
    &B_{=0} := \{\pm1\}^{nk} \setminus \sD.
\end{align*}
That is, $B_\text{safe}$ is the color class determined by the $\sqrt{n}$-wise product of the identity relation. $B_\text{coll}$ then consists of all other color classes within $\sD$, whereas $B_{=0}$ consists of all elements outside of $\sD$. We will often use the following result on the size of $B_\text{coll}$:
\begin{fact}
\label[fact]{fact:colorclasssizes}
    $\frac{\abs{B_{\mathrm{coll}}}}{\abs{\sD}} \leq \frac{2\sqrt{n}k^2}{2^{\sqrt{n}}}$.
\end{fact}

\begin{proof}
We may write:
\begin{equation*}
    \frac{\abs{B_{\mathrm{coll}}}}{\abs{\sD}} = \frac{\abs{B_{\mathrm{coll}}}}{\abs{\{\pm1\}^{nk}}} \cdot \frac{\abs{\{\pm1\}^{nk}}}{\abs{\sD}}.
\end{equation*}
The first can be viewed as the probability of sampling an element of $B_{\mathrm{coll}}$ when sampling from $\{\pm1\}^{nk}$. The process of sampling from $\{\pm1\}^{nk}$ can be seen as sampling $\sqrt{n}k$ rows from $\{\pm1\}^{\sqrt{n}}$. Under this view, a simple union bound tells us that there are at most $\sqrt{n}k^2$ possible ``collisions'' that would induce a non-distinct color class, allowing us to bound the probability by $\frac{\sqrt{n}k^2}{2^{\sqrt{n}}}$.

For the other term, we will prove simply that $\frac{\abs{B_{=0}}}{\abs{\{\pm1\}^{nk}}} \leq \frac{1}{2}$. Note that our analysis above actually bounds the probability $X \sim \{\pm1\}^{nk}$ is not in $B_{\mathrm{safe}}$, which is more than sufficient for this. \end{proof}


\subsubsection{Distributions}
\label{subsec:distributions}
Because in this section we are dealing with permutations of $n$-bit strings where the bits lie on a higher-dimensional lattice, it will be convenient to introduce completely new notation that will supplant the random walk operators $R_{n,S,k}$ from before.

If $\pi \in \mfS_{\{\pm1\}^{n}}$, then let $\pi^{\otimes k} \in \mfS_{\{\pm1\}^{nk}}$ be such that $\pi^{\otimes k}(X)^\ell = \pi(X^\ell)$ for all $X \in \{\pm1\}^{nk}$ and $\ell \in [k]$. 

\begin{itemize}
    \item Let $\mcB$ be a distribution on $\mfS_{\{\pm1\}^{\sqrt{n}}}$.
    
    \item Let $\mcP_R$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcP_R$ is sampled as follows: 
          Sample $\sigma_i \sim \mcB$ independently for each $i \in \sbra{\sqrt{n}\,}$ and define $\pi$ such that $\pi(x)_{i, \cdot} = \sigma_i(x_{i, \cdot})$ for all $x \in \{\pm1\}^{n}$ and all $i \in \sbra{\sqrt{n}\,}$. 
    \item Let $\mcP_C$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcP_C$ is sampled as follows: 
          Sample $\sigma_i \sim \mcB$ independently for each $i \in \sbra{\sqrt{n}\,}$ and define $\pi$ such that $\pi(x)_{\cdot, i} = \sigma_i(x_{\cdot, i})$ for all $x \in \{\pm1\}^{n}$ and all $i \in \sbra{\sqrt{n}\,}$. 
    \item Let $\mcP^0 = \mcP_R$. For all $t \ge 1$, let $\mcP^t$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcP^t$ is sampled as follows:
          Sample $\sigma_1 \sim \mcP^{t-1}$, $\sigma_2 \sim \mcP_C$, and $\sigma_3 \sim \mcP_R$ and define $\pi$ such that $\pi(x) = (\sigma_3 \circ \sigma_2 \circ \sigma_1)(x)$ for all $x \in \{\pm1\}^{n}$. It is worth noting that this construction is exactly that of our circuit model above.
    \item Let $\mcG_R$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcG_R$ is sampled as follows:
          Sample $\sigma_i \sim \mcU(\mfS_{\{\pm1\}^{\sqrt{n}}})$ independently for each $i \in \sbra{\sqrt{n}\,}$ and define $\pi$ such that $\pi(x)_{i, \cdot} = \sigma_i(x_{i,\cdot})$ for all $x \in \{\pm1\}^{n}$. 
    \item Let $\mcG_C$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcG_C$ is sampled as follows:
          Sample $\sigma_i \sim \mcU(\mfS_{\{\pm1\}^{\sqrt{n}}})$ independently for each $i \in \sbra{\sqrt{n}\,}$ and define $\pi$ such that $\pi(x)_{\cdot, i} = \sigma_i(x_{\cdot, i})$ for all $x \in \{\pm1\}^{n}$. 
    \item Let $\mcG^0 = \mcG_R$. For all $t \ge 1$, let $\mcG^t$ be a distribution on $\mfS_{\{\pm1\}^{n}}$ such that $\pi \sim \mcG^t$ is sampled as follows:
          Sample $\sigma_1 \sim \mcG^{t-1}$, $\sigma_2 \sim \mcG_C$, and $\sigma_3 \sim \mcG_R$ and define $\pi$ such that $\pi(x) = (\sigma_3 \circ \sigma_2 \circ \sigma_1)(x)$ for all $x \in \{\pm1\}^{n}$.
    \item Let $\mcG$ be $\mcU(\mfS_{\{\pm1\}^{n}})$, i.e. the uniform distribution on $\mfS_{\{\pm1\}^{n}}$.
    \item If $\mcD$ is a distribution on $\mfS_{\{\pm1\}^{n}}$ and $X \in \{\pm1\}^{nk}$, let $\mcD^{(k)}_X$ be a distribution on $\{\pm1\}^{nk}$ such that $Y \sim \mcD^{(k)}_X$ is sampled as follows:
          Sample $\pi \sim \mcD$ and define $Y$ such that $Y = \pi^{\otimes k}(X)$. 
          Note that if $X \in \sD$, then $\mcG^{(k)}_X$ is $\mcU\pbra{\sD}$. If the superscript is understood from the context of $X$ to be $k$, we will often drop it and just write $\mcD_X$.
\end{itemize}


It will be helpful to think of the distribution $\mcD$ as defining a Markov chain on $\{\pm1\}^{nk}$ for any choice $k \geq 1$. More specifically, $\mcD^{(k)}_X$ can be thought of as specifying the transition probabilities out of $X$ in the corresponding Markov chain on $\{\pm1\}^{nk}$. The transition operators corresponding to these Markov chains are given by:
\begin{align*}
    (T_{\mcD}f)(X)=\Ex_{\mathbf{Y} \sim \mathcal{D}_X^{(k)}}\sbra{f(\mathbf{Y})} = \sum_{Y \in \{\pm1\}^{nk}} \Pr_{\bm{Y}\sim\mcD_{X}^{(k)} }[\bm{Y}=Y] \cdot f(Y).
\end{align*}
When the value of $k$ is not clear from context, we will write $T_{\mcD^{(k)}}$ for this operator.



\subsection{Linear Algebra}
Recall that $\mcD_X$ can be thought of as the distribution after a 1-step random walk from $X$ according to a permutation drawn from $\mcD$.
\begin{fact}
    \label[fact]{fact:selfadjoint}
    $T_{\mcG_R}$, $T_{\mcG_C}$, and $T_{\mcG}$ are self-adjoint w.r.t our inner product.
\end{fact}
\begin{proof}
We will state the proof for $T_{\mcG_R}$, the reasoning for the others being symmetric. Let $f, g : \{\pm1\}^{nk} \to \R$.
\begin{align*}
    \langle f, T_{\mcG_R} g \rangle &= \sum_{X \in \{\pm1\}^{nk}} f(X)(T_{\mcG_R}g)(X)\\
    &= \sum_{X \in \{\pm1\}^{nk}} f(X) \sum_{Y \in \{\pm1\}^{nk}} \Pr[X \to_{T_{\mcG_R}} Y] \cdot g(Y)\\
    &= \sum_{X \in \{\pm1\}^{nk}}\sum_{Y \in \{\pm1\}^{nk}} f(X) \cdot g(Y) \cdot \Pr[X \to_{T_{\mcG_R}} Y].
\end{align*}
Note the symmetry in above: we are done if we prove that $\Pr[X \to_{T_{\mcG_R}} Y] = \Pr[Y \to_{T_{\mcG_R}} X]$. This fact is quite observable from the definition of $\mcG_R$, since for any $\sigma \in \mfS_{\{\pm1\}^{n}}$ the probability it is drawn from $\mcG_R$ is the same as the probability of drawing $\sigma^{-1}$. 
\end{proof}

\begin{fact}
    \label[fact]{fact:TGabsorbs}
    Let $\mcD$ be a distribution on $\mfS_{\{\pm1\}^{n}}$. Then:
    \begin{equation*}
        T_\mcD T_\mcG = T_\mcG = T_\mcG T_\mcD.
    \end{equation*}
\end{fact}

\begin{fact}
    \label{fact:differenceofproducts}
    Let $U_1, ..., U_s$ and $W_1,..., W_s$ be operators. Then we have:
    \begin{equation*}
        \prod_{i= 1}^s U_i - \prod_{i=1}^s W_i = \sum_{i=1}^s \prod_{j=1}^{i-1} U_j \cdot \pbra{U_i - W_i} \cdot \prod_{j = i+1}^s W_j.
    \end{equation*}
\end{fact}
\begin{proof}
We prove this by induction on $s$. The base case $s=1$ is immediate. For the inductive hypothesis note that:
\begin{align*}
    \prod_{i= 1}^{s+1} U_i - \prod_{i=1}^{s+1} W_i &= \pbra{\prod_{i= 1}^s U_i - \prod_{i=1}^s W_i}W_{s+1} + \prod_{i=1}^s U_i \pbra{U_{s+1} - W_{s+1}}\\
    &= \sum_{i=1}^s \prod_{j=1}^{i-1} U_j \cdot \pbra{U_i - W_i} \cdot \prod_{j = i+1}^{s+1} W_j + \prod_{i=1}^s U_i \pbra{U_{s+1} - W_{s+1}} \tag{Induction}\\
    &= \sum_{i=1}^{s+1} \prod_{j=1}^{i-1} U_j \cdot \pbra{U_i - W_i} \cdot \prod_{j = i+1}^{s+1} W_j.
\end{align*}
This completes the induction and the proof.
\end{proof}



\subsection{Proof of \Cref{thm:2D to 1D reduction}}
\label{sec:2D to 1D proof}

In this section we will prove \Cref{thm:2D to 1D reduction}, reducing the result to a spectral norm bound to be proved in \Cref{sec:spectralproof}. Our key insight will be that given the distribution and operator framework outlined in the previous section, we can now restate \Cref{thm:2D to 1D reduction} in a more ``palatable'' way by translating statements about the TV distance between distributions as quantities of their corresponding operators. More concretely we have the following:
\begin{claim}
    \label{tvdistancetolinearform}
    For $X \in \sD$, $d_{\mathrm{TV}}((\mcD_1)_X, (\mcD_2)_X) = \frac{1}{2} \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcD_1}-T_{\mcD_2}) e_Y}}$
\end{claim}

\begin{proof}We directly compute:
    \begin{align*}
        d_{\textrm{TV}}((\mcD_1)_X, (\mcD_2)_X) 
        &= \frac{1}{2} \sum_{Y \in \{\pm1\}^{nk}} \abs{\Pr[X \to_{T_{\mcD_1}} Y] - \Pr[X \to_{T_{\mcD_2}} Y]}\\
        &= \frac{1}{2} \sum_{Y \in \{\pm1\}^{nk}} \abs{\ip{e_X}{T_{\mcD_1} e_Y} - \ip{e_X}{T_{\mcD_2} e_Y}}\\
        &= \frac{1}{2} \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcD_1}-T_{\mcD_2}) e_Y}}.
    \end{align*}
    Here we used \Cref{fact:colorclasssizes} and that under a permutation elements in $\sD$ only map to $\sD$. 
\end{proof}

We apply this to prove \Cref{thm:2D to 1D reduction}:
\begin{theorem}[\Cref{thm:2D to 1D reduction} restated]
\label{thm:2D to 1D reduction technical}
    Given $k \leq 2^{\sqrt{n}/500}$, then for any $t \geq 500\pbra{k \log k + \frac{\log(1/\varepsilon)}{\sqrt{n}}}$, the following holds. Let $\mcP^t$ be the distribution on $\mathcal{S}_{\{\pm1\}^{n}}$ defined from our circuit model with a circuit family computing an $\frac{\varepsilon}{2\sqrt{n} \cdot (2t+1)}$-approximate $k$-wise independent distribution on $\mfS_{\{\pm1\}^{\sqrt{n}}}$ as the base. Then $\mcP^t$ is $\varepsilon$-approximate $k$-wise independent. That is, for all $X \in \sD$ we have $d_{\textrm{TV}}\pbra{\mcP^t_X, \mcG_X} \leq \varepsilon$ when $n$ is large enough.
\end{theorem}


\begin{proof}
We will bound the two quantities arising from the following application of the triangle inequality:
\begin{equation*}
    d_{\textrm{TV}}\pbra{\mcP^t_X, \mcG_X} \leq d_{\textrm{TV}}\pbra{\mcP^t_X, \mcG^t_X} + d_{\textrm{TV}}\pbra{\mcG^t_X, \mcG_X}.
\end{equation*}
Here we introduce the intermediate distribution $\mcG^t$ (defined in \Cref{subsec:distributions}) which will facilitate our analysis. The two steps are then to bound each of the latter terms separately by $\frac{\varepsilon}{2}$.

\begin{lemma}
    \label{lem:reduction}
    Assume the hypotheses of \Cref{thm:2D to 1D reduction}. Then for any $X \in \sD$:
\begin{equation*}
    \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcP^t}-T_{\mcG^t}) e_Y}} \leq  \max_{f \in \mcF} \abs{\ip{e_X}{(T_{\mcP^t}-T_{\mcG^t}) f}} \leq \frac\varepsilon2
\end{equation*}
where $\mcF$ is the set of functions $f : \{\pm1\}^{nk} \to [-1, 1]$ with $\mathrm{Supp}(f) \subseteq \sD$.
\end{lemma}

This lemma is a just a quantitative result capturing the intuition that if we replace all of the black-box $1$D pseudorandom permutation circuits in our design with truly random permutations, the TV distance does not change much.

\begin{lemma}
    \label{lem:maintrick}
    Assume the hypotheses of \Cref{thm:2D to 1D reduction}. Then for any $X \in \sD$:
\begin{equation*}
    \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcG^t}-T_{\mcG}) e_Y}} \leq \frac\varepsilon2.
\end{equation*}
\end{lemma}

This lemma claims that random columns and random row permutations approximate truly random permutations in low depth $t = \poly(k)$. In light of \Cref{tvdistancetolinearform}, we have shown that
\begin{equation*}
    d_{\textrm{TV}}\pbra{\mcP^t_X, \mcG^t_X} + d_{\textrm{TV}}\pbra{\mcG^t_X, \mcG_X} \leq \varepsilon,
\end{equation*}
which finishes the proof of \Cref{thm:2D to 1D reduction technical}, given the two lemmas.
\end{proof}


\subsection{Proof of \Cref{lem:reduction}}
\label{subsec:reduction}

In this proof, we roughly want to decompose the operators of our circuit as 1) sequential pieces corresponding to each layer of the circuit and 2) for each layer, since the pieces act independently on either the columns or rows, we can represent them as tensor products of simpler operators acting only on one row or column. Since these individual pieces are assumed to be approximately $k$-wise independent, we just show that this is preserved through tensorization and sequential application. 

Recall that $T_{\mcP^t} = T_{\mcP_R}\pbra{T_{\mcP_C}T_{\mcP_R}}^t$ and $T_{\mcG^t} = T_{\mcG_R}\pbra{T_{\mcG_C}T_{\mcG_R}}^t$, that is, they are products of $2t-1$ operators corresponding to the sequential pieces in the circuit. We can then rewrite the difference of the two operators as a telescoping sum following \Cref{fact:differenceofproducts}. For clarity let $T_{\mcP^{(i)}}$ denote the $i$th operator in the product $T_{\mcP^t}$ and likewise for $T_{\mcG^t}$.
\begin{equation*}
    T_{\mcP_R}\pbra{T_{\mcP_C}T_{\mcP_R}}^t - T_{\mcG_R}\pbra{T_{\mcG_C}T_{\mcG_R}}^t = \sum_{i=1}^{2t+1} \prod_{j=1}^{i-1} T_{\mcP^{(j)}} \cdot \pbra{T_{\mcP^{(i)}} - T_{\mcG^{(i)}}} \cdot \prod_{j = i+1}^s T_{\mcG^{(j)}}
\end{equation*}
To simplify this sum, we will make use of the following claim:

\begin{claim}\label{claim:sandwiching}
    Let $\mcD_1, \mcD_2$ be distributions on $\mfS_{\{\pm1\}^{n}}$ and $A$ an operator on the space $\R^{\{\pm1\}^{nk}}$. Let $X \in \sD$ and $f \in \mcF$. Then there exists some $X^* \in \sD$ and $f^* \in \mcF$ s.t.:
    \begin{equation*}
        \abs{\ip{e_X}{T_{\mcD_1}AT_{\mcD_2} f}} \leq \abs{\ip{e_{X^*}}{Af^*}}.
    \end{equation*}
\end{claim}
\begin{proof} Observe:
\begin{align*}
    \abs{\ip{e_X}{T_{\mcD_1}AT_{\mcD_2} f}} &= \abs{T_{\mcD_1}AT_{\mcD_2} f(X)}\\
    &= \abs{\sum_{Y \in \{\pm1\}^{nk}} \Pr[X \to_{T_{\mcD_1}} Y] \cdot AT_{\mcD_2} f(Y)}\\
    &= \abs{\sum_{Y \in \sD} \Pr[X \to_{T_{\mcD_1}} Y] \cdot \ip{e_Y}{AT_{\mcD_2} f}}\\
    &\leq \max_{X^* \in \sD} \abs{\ip{e_{X^*}}{AT_{\mcD_2} f}}.
\end{align*}
In the last line we are using triangle inequality. For the second part of the proof, it suffices to claim $T_{\mcD_2}f \in \mcF$. To see this just note:
\begin{equation*}
    T_{\mcD_2}f(Z) = \sum_{Y \in \mathrm{Supp}(\mcD_2(Z)))} \Pr[Z \to_{T_{\mcD_2}} Y] \cdot f(Y)
\end{equation*}
Since $f$ maps to $[-1, 1]$ which is a convex set, $T_{\mcD_2}f$ maps to $[-1, 1]$ as well. Additionally, for any $Z \notin \sD$, the support of $(\mcD_2)_Z$ cannot intersect $\sD$ so the term is 0, thus $T_{\mcD_2}f$ is supported on a subset of $\sD$ so is in $\mcF$. \end{proof}


\begin{claim}
    \begin{equation*}
        \max_{X \in \sD, f \in \mcF} \abs{\ip{e_X}{\pbra{T_{\mcP^t} - T_{\mcG^t}} f}} \leq \pbra{2t+1} \cdot \max_{X \in \sD, f \in \mcF} \abs{\ip{e_X}{\pbra{T_{\mcP_R} - T_{\mcG_R}} f}}.
    \end{equation*}
    Noting importantly that the latter term is interchangeable for $T_{\mcP_R} - T_{\mcG_R}$ and $T_{\mcP_C} - T_{\mcG_C}$. 
\end{claim}

\begin{proof}We directly compute:
    \begin{align*}
        \max_{X \in \sD, f \in \mcF} \abs{\ip{e_X}{\pbra{T_{\mcP^t} - T_{\mcG^t}} f}}
        &= \max_{X \in \sD, f \in \mcF} \abs{\ip{e_X}{\pbra{\sum_{i=1}^{2t+1} \prod_{j=1}^{i-1} T_{\mcP^{(j)}} \cdot \pbra{T_{\mcP^{(i)}} - T_{\mcG^{(i)}}} \cdot \prod_{j = i+1}^s T_{\mcG^{(j)}}} f}} \tag{\Cref{fact:differenceofproducts}}\\
        &\leq \max_{X \in \sD, f \in \mcF} \sum_{i=1}^{2t+1} \abs{\ip{e_X}{\pbra{ \prod_{j=1}^{i-1} T_{\mcP^{(j)}} \cdot \pbra{T_{\mcP^{(i)}} - T_{\mcG^{(i)}}} \cdot \prod_{j = i+1}^s T_{\mcG^{(j)}}} f}}\\
        &\leq \max_{X \in \sD, f \in \mcF} \sum_{i=1}^{2t+1} \abs{\ip{e_X}{\pbra{T_{\mcP^{(i)}} - T_{\mcG^{(i)}}} f}} \tag{\Cref{claim:sandwiching}}\\
        &\leq (2t+1) \cdot \max_{X \in \sD, f \in \mcF} \abs{\ip{e_X}{\pbra{T_{\mcP_R} - T_{\mcG_R}} f}}.
    \end{align*}
    In the last line we are using that $T_{\mcP_R}$ and $T_{\mcP_C}$ are symmetric, and likewise for $T_{\mcG_R}$ and $T_{\mcG_C}$.
\end{proof}

It is worth pointing out at this point we could convert back to the total variation distance making the above statement:
\begin{equation*}
    d_{\textrm{TV}}\pbra{\mcP^t_X, \mcG^t_X} \leq (2t+1) \cdot d_{\textrm{TV}}\pbra{(\mcP_R)_X, (\mcG_R)_X}.
\end{equation*}
We have in essence reduced the TV distance bound on our sequential circuit to just a single layer. Our next move will be to reduce the distance further to the individual parallel gates making up each layer, which is what we assumed black box is $\varepsilon'$-approximate $k$-wise independent. Towards this end we write:
\begin{equation*}
    \sum_{Y \in \sD} \abs{\Pr[X \to_{T_{\mcP_R}} Y] - \Pr[X \to_{T_{\mcG_R}} Y]} = \sum_{Y \in \sD} \abs{\prod_{i=1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_\mcB} Y_{i ,\cdot}] - \prod_{i=1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{i ,\cdot}]}.
\end{equation*}
We denote here $\mcG_{\sqrt{n}} = \mcU(\mfS_{\{\pm1\}^{\sqrt{n}}})$. The key fact here is that the operators correspond to product distributions on individual rows. We can again utilize \Cref{fact:differenceofproducts} to simplify the difference of products:
\begin{align*}
    &\sum_{Y \in \sD} \abs{\sum_{j = 1}^{\sqrt{n}} \prod_{i=1}^{j-1}\Pr[X_{i, \cdot} \to_{T_\mcB} Y_{i ,\cdot}] \cdot \pbra{\Pr[X_{j, \cdot} \to_{T_\mcB} Y_{j ,\cdot}] - \Pr[X_{j, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{j ,\cdot}]} \cdot \prod_{i=j+1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{i ,\cdot}]}\\
    \leq&  \sum_{j = 1}^{\sqrt{n}} \sum_{Y \in \sD} \abs{\prod_{i=1}^{j-1}\Pr[X_{i, \cdot} \to_{T_\mcB} Y_{i ,\cdot}] \cdot \pbra{\Pr[X_{j, \cdot} \to_{T_\mcB} Y_{j ,\cdot}] - \Pr[X_{j, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{j ,\cdot}]} \cdot \prod_{i=j+1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{i ,\cdot}]}\\
    =& \sum_{j = 1}^{\sqrt{n}} \sum_{y \in \{\pm1\}^{\sqrt{n}k}} \abs{\Pr[X_{j, \cdot} \to_{T_\mcB} y] - \Pr[X_{j, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} y]} \sum_{\substack{Y \in \sD\\ Y_{j, \cdot} = y}} \prod_{i=1}^{j-1}\Pr[X_{i, \cdot} \to_{T_\mcB} Y_{i ,\cdot}] \prod_{i=j+1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{i ,\cdot}]\\
    \leq& \sum_{j = 1}^{\sqrt{n}} \sum_{y \in \{\pm1\}^{\sqrt{n}k}} \abs{\Pr[X_{j, \cdot} \to_{T_\mcB} y] - \Pr[X_{j, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} y]} \sum_{\substack{Y \in \{\pm1\}^{nk}\\ Y_{j, \cdot} = y}} \prod_{i=1}^{j-1}\Pr[X_{i, \cdot} \to_{T_\mcB} Y_{i ,\cdot}] \prod_{i=j+1}^{\sqrt{n}}\Pr[X_{i, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} Y_{i ,\cdot}]\\
    =& \sum_{j = 1}^{\sqrt{n}} \sum_{y \in \{\pm1\}^{\sqrt{n}k}} \abs{\Pr[X_{j, \cdot} \to_{T_\mcB} y] - \Pr[X_{j, \cdot} \to_{T_{\mcG_{\sqrt{n}}}} y]}.
\end{align*}
Here we are partitioning the sum over $Y$ into its fixed row $Y_{j, \cdot}$. The large sum of products we get is just the probability the ``free'' rows map to different elements of $\{\pm1\}^{\sqrt{n}k}$, which marginalizes to 1 when we sum over the entire region. We are nearly done, as the term now looks very close to that which shows up in the definition of $\varepsilon$-approximate $k$-wise independent. The only difference is that we sum over the entire set of rows $\{\pm1\}^{\sqrt{n}k}$, whereas in the definition of $\varepsilon$-approximate $k$-wise independence, the sum is over ``distinct'' $k$-tuples. Distinct $k$-tuples of ``grids'' $X$ may share rows $X_{j, \cdot}$ that are not distinctly colored. Nonetheless, our result still follows from the definition of $\varepsilon$-approximate $k$-wise independence.

Using \Cref{kwiseimplies} below, we compute
\begin{equation*}
    \max_{f \in \mcF} \abs{\ip{e_X}{(T_{\mcP^t}-T_{\mcG^t}) f}} \leq (2t+1) \cdot \sum_{j=1}^{\sqrt{n}} \frac{\varepsilon}{2\sqrt{n} \cdot (2t+1)} \leq \frac\varepsilon2.
\end{equation*}
This completes the proof of \Cref{lem:reduction}.

\begin{lemma}
    \label{kwiseimplies}
    For every $x \in \{\pm1\}^{\sqrt{n}k}$ we have:
    \begin{equation*}
        \sum_{y \in \{\pm1\}^{\sqrt{n}k}} \abs{\Pr[x \to_{T_\mcB} y] - \Pr[x \to_{T_{\mcG_{\sqrt{n}}}} y]} \leq \frac{2\varepsilon}{\sqrt{n} \cdot (2t+1)}.
    \end{equation*}
\end{lemma}


\begin{proof}[Proof of \Cref{kwiseimplies}]
For $x$ corresponding to a distinct $k$-tuple, this reduces to the term in $\varepsilon'$-approximate $k$-wise independence, for which $\mcB$ is assumed to fulfill. The proof is then a matter of showing that ``$\varepsilon$-approximate $k$-wise independence'' implies ``$\varepsilon$-approximate $\tau$-wise independence'' for $\tau < k$. For this, we will need a notion of color class for elements in $\{\pm1\}^{\sqrt{n}k}$ analogous to the one defined in \Cref{sec:colorclasses}. We will define for an equivalence relation $R$ on $[k]$:
\begin{equation*}
    B_R = \left\{x \in \{\pm1\}^{\sqrt{n}k} \mid x^i = x^j \iff i R j\right\}.
\end{equation*}
That is, we think of $x$ as a $k$-tuple of rows and take the corresponding coloring.

First note that we only need to consider terms such that $y \in B(x)$, as if they are not colored the same then the transition probability under any permutation becomes 0. Now assume $x$ is $\tau$-colored for $\tau < k$ and $B = B(x)$. Let $T$ be the set of indices corresponding to the first instances of a color appearing in $x$. For example, if $x$ was colored with $k-1$ colors, with the first and last elements of the $k$-tuple colored the same, then $T$ would be $[k-1]$. Importantly, $T$ is the same across the color class $B(x)$ and $\abs{T} = \tau$. We will then create a function $\varphi_B : \{\pm1\}^{\sqrt{n}k} \to \{\pm1\}^{\sqrt{n}\tau}$ that projects out the indices outside of $T$. As a result we have for all $y \in B$, $\varphi_B(y) \in \sD_{\sqrt{n}}^{(\tau)}$, the set of distinct tuples in $\{\pm1\}^{\sqrt{n}\tau}$ and moreover the image of $\varphi_B$ under $B$ is entirely $\sD_{\sqrt{n}}^{(\tau)}$. The key observation is then that $[k] \setminus T$, the indices not in $T$, can be ignored across transitions since they are completely fixed:
\begin{align*}
    \sum_{y \in B(x)} \abs{\Pr[x \to_{T_\mcB} y] - \Pr[x \to_{T_{\mcG_{\sqrt{n}}}} y]} =& \sum_{y \in B} \abs{\Pr[\varphi_B(x) \to_{T_\mcB} \varphi_B(y)] - \Pr[\varphi_B(x) \to_{T_{\mcG_{\sqrt{n}}}} \varphi_B(y)]}\\
    =& \sum_{\varphi_B(y) \in \sD_{\sqrt{n}}^{(\tau)}} \abs{\Pr[\varphi_B(x) \to_{T_\mcB} \varphi_B(y)] - \Pr[\varphi_B(x) \to_{T_{\mcG_{\sqrt{n}}}} \varphi_B(y)]}.
\end{align*}

Note the end formula above has no dependence on the fixed indices $[k] \setminus T$. This allows us to pretend they are distinct, writing the above sum over elements of $\sD$ instead:
\begin{align*}
    &\sum_{y \in B(x)} \abs{\Pr[x \to_{T_\mcB} y] - \Pr[x \to_{T_{\mcG_{\sqrt{n}}}} y]}\\
    =& \sum_{\varphi_B(y) \in \sD_{\sqrt{n}}^{(\tau)}} \abs{\sum_{y_{[k] \setminus T} \in \sD_{\sqrt{n}}^{(k-\tau)}}\Pr[(\varphi_B(x), \cdot) \to_{T_\mcB} (\varphi_B(y), y_{[k] \setminus T})] - \Pr[\varphi_B(x) \to_{T_{\mcG_{\sqrt{n}}}} (\varphi_B(y), y_{[k] \setminus T})]}\\
    \leq& \sum_{\substack{\varphi_B(y) \in \sD_{\sqrt{n}}^{(\tau)} \\ y_{[k] \setminus T} \in \sD_{\sqrt{n}}^{(k-\tau)}}} \abs{\Pr[(\varphi_B(x), \cdot) \to_{T_\mcB} (\varphi_B(y), y_{[k] \setminus T})] - \Pr[\varphi_B(x) \to_{T_{\mcG_{\sqrt{n}}}} (\varphi_B(y), y_{[k] \setminus T})]}\\
    =& \sum_{y \in \sD^{(k)}_{\sqrt{n}}} \abs{\Pr[(\varphi_B(x), \cdot) \to_{T_\mcB} (\varphi_B(y), y_{[k] \setminus T})] - \Pr[\varphi_B(x) \to_{T_{\mcG_{\sqrt{n}}}} (\varphi_B(y), y_{[k] \setminus T})]}.
\end{align*}

In this last line we can choose $(\varphi_B(x), \cdot)$ to be from $\sD^{(k)}_{\sqrt{n}}$ so any $y$ outside of this class contributes nothing to the sum. Appealing to the approximate $k$-wise independence of $\mcB$ finishes the proof.
\end{proof}

\subsection{Proof of \Cref{lem:maintrick}}
\label{subsec:inductiontrick}

The following lemma will help us achieve the bound in \Cref{lem:maintrick}.
\begin{lemma}
\label[lemma]{lem:offdiagonalmoment}
    Assume the hypothesis of \Cref{lem:maintrick}. Then for any $Y \in\sD$, we have $\abs{\ip{e_X}{(T_{\mcG^t}-T_{\mcG}) e_Y}} \leq \frac{t+1}{2^{\sqrt{n}(t-1)/128}} \cdot \frac{1}{\abs{B(Y)}}$.
\end{lemma}

To see why the lemma is sufficient, observe:
\begin{equation*}
    \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcG^t}-T_{\mcG}) e_Y}} 
    \leq \frac{t+1}{2^{\sqrt{n}(t-1)/128}} \sum_{Y \in \sD} \frac{1}{\abs{B(Y)}} 
    \leq \frac{t+1}{2^{\sqrt{n}(t-1)/128}}\sum_{B \in \sB} \sum_{Y \in B} \frac{1}{\abs{B}} \leq \frac{\abs{\sB} \cdot (t+1)}{2^{\sqrt{n}(t-1)/128}}.
\end{equation*}
Here we partition the sum based on color classes, and note that each color class contributes a total of 1 to the sum. We can use \Cref{fact:numofcolorclasses} bounding the number of color classes and the fact that $\frac{t+1}{2^{t-1}}$ very quickly to write:
\begin{equation*}
    \sum_{Y \in \sD} \abs{\ip{e_X}{(T_{\mcG^t}-T_{\mcG}) e_Y}} 
    \leq \frac{k^{k\sqrt{n}} \cdot (t+1)}{2^{\sqrt{n}(t-1)/128}} \leq \frac{k^{k\sqrt{n}}}{2^{\pbra{\sqrt{n}/128-1} (t-1)}} \leq \frac{\varepsilon}{2}.
\end{equation*}

This is then bounded by $\frac\varepsilon2$ for $t \geq  \frac{\sqrt{n}k\log_2 k + \log_2 2/\varepsilon}{\sqrt{n}/128-1}+1$. When $n$ is large enough, the bound holds when $t\geq 500\pbra{k \log_2 k + \frac{\log_2 1/\varepsilon}{\sqrt{n}}}$.

\begin{proof}[Proof of \Cref{lem:offdiagonalmoment}.]
Recall that $T_{\mcG^t} = T_{\mcG_R}(T_{\mcG_C}T_{\mcG_R})^t$. We can then write $T_{\mcG^t}-T_{\mcG} = (T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R} - T_\mcG)$ and prove the claim by induction on $t$. Consider first when $t = 0$.
\begin{equation*}
    \abs{\ip{e_X}{(T_{\mcG_R}-T_{\mcG}) e_Y}} = \abs{\Pr[X \to_{T_{\mcG_R}} Y] - \Pr[X \to_{T_{\mcG}} Y]} = \abs{\Pr[Y \to_{T_{\mcG_R}} X] - \Pr[Y \to_{T_{\mcG}} X]}.
\end{equation*}
We use the self-adjointness of the two operators here. Observe that under the action of $T_{\mcG_R}$, $Y$ goes to a uniform element of $B(Y)$, and under $T_\mcG$ goes to a uniform element of $\sD$. Thus, the quantity is either $\frac{1}{\abs{B(Y)}} - \frac{1}{\abs{\sD}}$ or $\frac{1}{\abs{\sD}}$. Either way it is below $\frac{1}{\abs{B(Y)}}$. For the induction step, we assume the lemma for fixed $t \geq 0$. Then we compute
\begin{align*}
    \abs{\ip{e_X}{(T_{\mcG_R}T_{\mcG_C})^{t+1}(T_{\mcG_R}-T_{\mcG}) e_Y}} &= \abs{\ip{e_X}{(T_{\mcG_R}T_{\mcG_C})(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}\\
    &= \abs{T_{\mcG_R}\pbra{T_{\mcG_C}(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}(X)}\\
    &= \abs{\sum_{Z \in \sD} \Pr[X \to_{T_{\mcG_R}} Z]\pbra{T_{\mcG_C}(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}(Z)}\\
    &= \abs{\sum_{Z \in \sD} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z]\pbra{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}(Z)}\\
    &= \abs{\sum_{Z \in \sD} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z] \ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}.
\end{align*}
We have managed to write the $(t+1)$ case inner product as a convex combination of the case with $t$. However, if we try to apply the induction hypothesis here we will make no progress. Instead, we will break the sum up and handle only one half with induction. The other half we will bound ``from scratch'', and it is here we will make progress. Recall that we may partition $\sD$ into two regions, $B_\text{safe}$ and $B_\text{coll}$:
\begin{align*}
    \abs{\ip{e_X}{(T_{\mcG_R}T_{\mcG_C})^{t+1}(T_{\mcG_R}-T_{\mcG}) e_Y}} \leq& \abs{\sum_{Z \in B_\text{safe}} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z] \ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}\\
    &\;\;+ \abs{\sum_{Z \in B_\text{coll}} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z] \ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}\\
    \leq& \abs{\sum_{Z \in B_\text{safe}} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z] \ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}\\
    &\;\;+ \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} B_{\text{coll}}] \cdot \frac{t+1}{2^{\sqrt{n}(t-1)/128}} \cdot \frac{1}{\abs{B(Y)}}.
\end{align*}
In the last line we used the inductive hypothesis. We will then show that the first term is smaller than is demanded by the induction due to a straightforward spectral norm argument. The second term is small because the probability of ``collision'', or that a walk transitions to $B_{\mathrm{coll}}$, is small. More specifically we will need the following two lemmas which we will prove in \Cref{sec:spectralproof}.
\begin{lemma}\label{lem:spectralnorm}
    Assuming $k \leq 2^{\sqrt{n}/500}$ and $n$ large enough, $\norm{T_{\mcG_R}T_{\mcG_C}T_{\mcG_R}-T_{\mcG}}_{\mathrm{op}} \leq \frac{1}{2^{\sqrt{n}/128}}$.
\end{lemma}

\begin{lemma}
\label{lem:lowcollprob}
    Assuming $k \leq 2^{\sqrt{n}/500}$, for all $X \in \sD$, $\Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} B_\text{coll}] \leq \frac{1}{2^{\sqrt{n}/128}}$.
\end{lemma}
To use \Cref{lem:spectralnorm} we write for $Z \in B_{\mathrm{safe}}$:
\begin{align*}
    \abs{\ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}} =& \abs{\ip{T_{\mcG_R}e_Z}{(T_{\mcG_R}T_{\mcG_C}T_{\mcG_R}-T_{\mcG})^t T_{\mcG_R} e_Y}} \\
    \leq& \norm{T_{\mcG_R}T_{\mcG_C}T_{\mcG_R}-T_{\mcG}}{2}^t \norm{T_{\mcG_R}e_Z}_{2} \norm{T_{\mcG_R}e_Y}_{2}\\
    \leq& \frac{1}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}^{1/2}\abs{B_\text{safe}}^{1/2}}\\
    \leq& \frac{1}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}}.
\end{align*}
The first step uses the self-adjointness of $T_{\mcG_R}$, the fact that $T_{\mcG_R}^2 = T_{\mcG_R}$, and \Cref{fact:TGabsorbs}. The inequality is an application of Cauchy-Schwarz and submultiplicativity of the operator norm. The second to last step uses \Cref{lem:spectralnorm} and \Cref{TGR eU 2 norm} below, and the last step uses \Cref{fact:colorclasssizes}, namely that $B_{\mathrm{safe}}$ is larger than every other color class given our choice of $k$ and large enough $n$.

\begin{claim}\label{TGR eU 2 norm}
    For arbitrary $U \in \{\pm1\}^{nk}$:
\begin{equation*}
    \norm{T_{\mcG_R}e_U}_{2} = \frac{1}{\abs{B(U)}^{1/2}}.
\end{equation*}
\end{claim}

Continuing from the equation above we have:
\begin{align*}
    \abs{\ip{e_X}{(T_{\mcG_R}T_{\mcG_C})^{t+1}(T_{\mcG_R}-T_{\mcG}) e_Y}} \leq& \abs{\sum_{Z \in B_\text{safe}} \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} Z] \ip{e_Z}{(T_{\mcG_R}T_{\mcG_C})^t(T_{\mcG_R}-T_{\mcG}) e_Y}}\\
    &\;\;+ \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} B_{\text{coll}}] \cdot \frac{t+1}{2^{\sqrt{n}(t-1)/128}} \cdot \frac{1}{\abs{B(Y)}}\\
    \leq& \frac{1}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}} \tag{\Cref{lem:spectralnorm}}\\
    &\;\;+ \Pr[X \to_{T_{\mcG_C}T_{\mcG_R}} B_{\text{coll}}] \cdot \frac{t+1}{2^{\sqrt{n}(t-1)/128}} \cdot \frac{1}{\abs{B(Y)}}\\
    \leq& \frac{1}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}} + \frac{t+1}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}} \tag{\Cref{lem:lowcollprob}}\\
    \leq& \frac{t+2}{2^{\sqrt{n}t/128}} \cdot \frac{1}{\abs{B(Y)}}.
\end{align*}
This completes the induction. We finish by proving the claim above:

\begin{proof}[Proof of \Cref{TGR eU 2 norm}]
    Observe:
    \begin{align*}
        &\norm{T_{\mcG_R}e_U}_{2} = \sqrt{\sum_{W \in B(U)} \pbra{T_{\mcG_R}e_U(W)}^2} = \sqrt{\sum_{W \in B(U)} \Pr[W \to_{\mcG_R} U]^2} \\
        =& \sqrt{\sum_{W \in B(U)} \pbra{\frac{1}{\abs{B(U)}}}^2} = \frac{1}{\abs{B(U)}^{1/2}}.\qedhere
    \end{align*}
\end{proof}
This completes the proof of \Cref{lem:offdiagonalmoment}.
\end{proof} 
