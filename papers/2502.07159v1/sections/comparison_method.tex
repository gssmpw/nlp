\section{Comparison Method}\label{appendix:comparison}


\begin{theorem}[\cite{wilmer2009markov}, Theorem 13.23]
    \label{thm:wilmer comparison}
    Let $\wt{P}$ and $P$ be transition matrices for two ergodic Markov chains on the same state space $V$. Assume that for each $(x,y)\in V^2$ there exists a random path 
    \begin{align*}
        \bm{\Delta}(x,y)=\pbra{(x,\bm{u}_1),(\bm{u}_1,\bm{u}_2),(\bm{u}_2,\bm{u}_3),\dots,(\bm{u}_{\ell},y)}.
    \end{align*}
    Then we have that
    \begin{align*}
        \lambda_2(L)\geq \pbra{\max_{v\in V}\frac{\pi(v)}{\widetilde{\pi}(v)}}A(\bm\Delta)\lambda_2(\wt{L}).
    \end{align*}
    where the comparison constant of $\bm{\Delta}$ is defined to be
    \begin{align*}
        A(\bm{\Delta}) \coloneq \max_{\substack{(a,b)\in V^2 \\\wt{P}(a,b)>0}} \cbra{\frac{1}{\wt{\pi}(x)\wt{P}(a,b)}\sum_{(x,y)\in V^2}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(x,y)}\cdot |\bm{\Delta}(x,y)|}\cdot  \pi(x)\cdot P(x, y)}.
    \end{align*}
    Here $\pi$ and $\wt{\pi}$ are the (unique) stationary distributions for $P$ and $\wt{P}$, respectively, and $\mathbf{1}_{(a, b) \in \calP}$ is the indicator variable which captures whether $(a,b)$ appears in the sequence $\calP$.
\end{theorem}

\Cref{lem:comparison schreier} is a direct consequence of the following \Cref{cor:comparison random walks}, with symmetry applied.

\begin{corollary}
\label{cor:comparison random walks}
    Let $\wt{P}$ and $P$ be transition matrices for random walks on undirected (multi)graphs $\wt{G}=(V,\wt{E})$ and $G=(V,E)$, respectively. The graph $G$ is $d$-regular, and $\wt{G}$ is $\wt{d}$-regular. Assume that for each $e\in E$ there exists a random path
    \begin{align*}
        \bm{\Gamma}(e)=(\wt{\bm{e}}_1,\dots,\wt{\bm{e}}_{T_e}),
    \end{align*}
    where $(\wt{\bm{e}}_1,\dots,\wt{\bm{e}}_{T_e})$\footnote{Here $T_e$ is a (deterministic) quantity determined by the edge $e$.} is drawn from a distribution on sequences of edges connecting the endpoints of $E$. Then we have for any $f:V\to\R$ that
    \begin{align*}
        \lambda_2(L)\geq \pbra{\max_{v\in V}\frac{\pi(v)}{\widetilde{\pi}(v)}}B(\bm{\Gamma})\lambda_2(\wt{L}).
    \end{align*}
    where the \emph{multigraph comparison constant} is defined as the maximum congestion over all edges $\tilde{e} \in \wt{E}$ connecting vertices $a,b\in V$,
    \begin{align*}
        B(\bm{\Gamma}) \coloneq \max_{\wt{e}\in \wt{E}} \cbra{\frac{\wt{d}}{d}\sum_{e\in E}\Ex_{\bm{\Gamma}}\sbra{\mathbf{1}_{\wt{e} \in \bm{\Gamma}(e)}\cdot |\bm{\Gamma}(e)|}}.\footnotemark
    \end{align*}
    Here $\pi$ and $\wt\pi$ are the (unique) stationary distributions for $P$ and $\wt{P}$, respectively.
    \footnotetext{If $\calP$ is a sequence then $|\calP|$ is its length.}
\end{corollary}
\begin{proof}
    We construct a (randomized) map $\bm\Delta$ as in \Cref{lem:comparison schreier} from the map $\bm\Gamma$ as follows. For each $(x,y)\in V^2$, if $x$ and $y$ are not connected by an edge in $E$ then set $\bm\Delta(x,y)=()$ (the sequence of length 0). Otherwise select a random edge $\bm{e}$ from $(E)_{x,y}$ and let $\pbra{\bm{\wt{e}}_1,\dots,\bm{\wt{e}}_{{T_e}}}$ be the random path $\bm\Gamma(\bm{e})$. For each $i\in [T_e]$ let $(\bm{u}_i,\bm{v}_i)$ be the vertices connected by $\wt{\bm{e}}_i$. Then set 
    \begin{align*}
        \bm\Delta(x,y)=\pbra{(\bm{u}_1,\bm{v}_1),\dots, (\bm{u}_{\ell},\bm{v}_{T_e})}=\pbra{(x,\bm{v}_1),\dots, (\bm{u}_{T_e},y)}.
    \end{align*}
    The comparison constant of $\bm{\Delta}$ is
    $$A(\bm\Delta)=\max_{(a,b)\in V^2} \cbra{\frac{1}{\wt{\pi}(x)\wt{P}(a,b)}\sum_{(x,y)\in V^2}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(x,y)}\cdot |\bm{\Delta}(x,y)|}\cdot  \pi_{\mathsf{ref}}(x)\cdot P(x, y)}.$$
    Since both Markov chains have the same stationary distribution over the same state space, the $\pi$ terms cancel, so the above is equal to
    \begin{align*}
        &\max_{(a,b)\in V^2} \cbra{\frac{1}{\wt{P}(a,b)}\sum_{(x,y)\in V^2}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(x,y)}\cdot |\bm{\Delta}(x,y)|}\cdot  P(x, y)}\\
        =&\max_{(a,b)\in V^2} \cbra{\frac{\wt{d}}{\abs{\wt{E}_{a,b}}}\sum_{(x,y)\in V^2}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(x,y)}\cdot |\bm{\Delta}(x,y)|}\cdot  \frac{\abs{E_{x,y}}}{d}}.
    \end{align*}
    Let us now start translating from pairs of vertices to edges of the multigraph. Each pair of vertices $(x, y)$ has $\left|E_{x,y}\right|$ edges connecting them, we can change the summation from pairs of vertices to edges $e \in E$. Recall that $u(e), v(e)$ are the endpoints of edge $e$. Continuing the calculation, the above is equal to 
    \begin{align*}
        &\max_{(a,b)\in V^2} \cbra{\frac{\wt{d}}{d\abs{\wt{E}_{a,b}}}\sum_{e\in E}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(u(e),v(e))}\cdot |\bm{\Delta}(u(e),v(e))|} }\\
        =&\max_{(a,b)\in V^2} \cbra{\frac{\wt{d}}{d}\sum_{e\in E} \frac{1}{\abs{\wt{E}_{a,b}}}\Ex_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(u(e),v(e))}\cdot |\bm{\Delta}(u(e),v(e))|}}\\
        =&\max_{(a,b)\in V^2} \cbra{\frac{\wt{d}}{d}\sum_{e\in E} \frac{1}{\abs{\wt{E}_{a,b}}}|\bm{\Gamma}(e)|\Pr_{\bm{\Delta}}\sbra{\mathbf{1}_{(a, b) \in \bm{\Delta}(u(e),v(e))}}}.
    \end{align*}
    The last equality is because $|\bm\Delta(u(e),v(e))|=|\bm\Gamma(e)|$ with certainty, and $|\bm\Gamma(e)|$ is a deterministic quantity that only depends on $e$.

    The sum of probabilities that $\wt{e}\in (\wt{E})_{a,b}$ appears in the sequence $\bm\Gamma(e)$, over all such $\wt{e}$, is equal to the probability that $(a,b)$ appears in $\bm\Delta(u(e),v(e))$. By averaging, we have that the probability that $\wt{e}\in \wt{E}_{a,b}$ appears in $\bm\Gamma(e)$, where $\wt{e}$ maximizes this quantity, is at least $\frac1{|\wt{E}_{a,b}|}$ times the appearance probability of $(a,b)$. This results in
    \begin{align*}
        A(\bm\Delta)\leq&\max_{\wt{e}\in \wt{E}} \cbra{\frac{\wt{d}}{d}\sum_{e\in E}\Ex_{\bm{\Gamma}}\sbra{\mathbf{1}_{\wt{e} \in \bm{\Gamma}(e)}\cdot |\bm{\Gamma}(e)|} }=B(\bm\Gamma).
    \end{align*}
    Applying \Cref{thm:wilmer comparison} with this new map $\bm\Delta$ completes the proof.
\end{proof}
