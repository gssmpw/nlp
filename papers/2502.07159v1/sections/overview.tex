\section{Proof Overview}\label{sec:overview}

\subsection{One-Dimensional Circuits}
Recall that the main technical content behind the proof of \Cref{thm:1D main} is to exhibit a spectral gap for the random walk matrix induced by applying a single brickwork layer of fixed random gates. The first step to exhibit this spectral gap is to show that a \textit{single} random geometrically local gate also exhibits a large spectral gap, and the result about a single brickwork layer follows by the detectability lemma of Aharonov et al.~\cite{aharonov2009detectability}. 

To bound the spectral gap induced by a single random geometrically local gate, we note that the corresponding Laplacian is of the form 
\begin{align*}
    L &= \Ex_{\ba\in\{1,\dots,n-2\}}\sbra{L_{\ba}},
\end{align*}
where each $L_{\ba}$ corresponds to the application of a single random gate to the bits $\ba,\ba+1,\ba+2$. This type of operator can also be viewed as the Hamiltonian of a physical system consisting of $n$ particles on a one-dimensional lattice with nearest-neighbor interactions. To analyze the spectral gap of this operator, we use tools from quantum many-body physics.

In particular, a result of Nachtergaele~\cite{nachtergaele1996spectral} shows how to reduce exhibiting a spectral gap for this $n$-particle system to exhibiting a spectral gap for a smaller system. Previous results allow us to bound the spectral gap for this smaller system, but the main technical content in the proof of \Cref{thm:1D main} is to show that the hypotheses of this result of \cite{nachtergaele1996spectral} are satisfied by the operators we are interested in.

As alluded to before, showing that these hypotheses are satisfied involves proving statements such as 
\begin{align*}
    (\bm{\pi}_T\bm{\pi}_S(x^1),\dots,\bm{\pi}_T\bm{\pi}_S(x^k)) \approx \bm{F}_T\bm{F}_S(x^1,\dots,x^k).
\end{align*}
Recall that to show comparisons of the above form, the plan is to compare the processes of sampling with replacement and sampling without replacement in the setting where not too many elements are being resampled, relative to the total number of elements. To see why this comparison is relevant, note that applying a random permutation to each element in a $k$-tuple of strings is almost equivalent to sampling $k$ elements from a set of strings with replacement. 

Combinatorially, directly analyzing probabilities allows us to bound the distance between resampling with replacement and resampling without replacement. To relate this combinatorial bound to a linear-algebraic quantity, we use \Cref{lem:TV distance bound}.

An issue arises when we try to apply this comparison: the comparison does not hold on a small subset of $k$-tuples of $n$-bit strings. However, we show that this set exhibits good expansion already, or that the probability of a random walk escaping from this region is already large. Formally, \Cref{lem:escape probs} relates the escape probabilities from a region of the Markov chain to the quadratic form induced by its transition operator.



\subsection{Two-Dimensional Circuits}

The proof of \Cref{thm:2D main} proceeds by instantiating the lattice construction in \Cref{thm:2D to 1D reduction} with the one-dimensional construction in \Cref{thm:1D main}. We now overview the proof of \Cref{thm:2D to 1D reduction}.

Recall that in the 1D case we related our circuit construction to an induced Markov chain on the set of $k$-tuples of $n$-bit strings and exhibited a spectral gap for this Markov chain to deduce fast mixing. We will start by showing why this argument does not suffice for a sublinear-in-$n$ dependence on $n$ in the mixing time.

Recall that the standard spectral argument states that given spectral gap $\lambda$ for a reversible Markov chain on state space $S$ the time to mix is bounded above by
\begin{align*}
    \frac{1}{\lambda}\cdot(\log{\abs{S}} + \log(1/\varepsilon)).
\end{align*}
However, note that the state space $S$ in our context consists of the set of $k$-tuples of distinct $n$-bit strings, and therefore $\log(\abs{|S|}) \approx nk$. Therefore, since $\lambda$ is always bounded above by 1, the best mixing time one could hope for via this naive analysis would already have a linear dependence in $n$.

Therefore, we need to develop a more refined argument, which deals more directly with total variation distance. We now describe this approach. Recall that our 2D circuit construction involves alternating layers of row and column-oriented 1D circuits. As such, the Markov operator $T_P$ for the circuit can be expressed as $T_P = T_{P_R}(T_{P_C}T_{P_R})^t$ where $T_{P_R}$ and $T_{P_C}$ are the induced operators corresponding to individual row and column-oriented layers. 

We can now express the TV distance between relevant distributions as follows. Let $X \in \{\pm 1\}^{nk}$ be a $k$-tuple of unique $n$-bit strings (we will call the set of such $k$-tuples $\sD$), which corresponds to a single state in our Markov chain. Then the TV distance between the uniform distribution on $\sD$ and the output of a random circuit from our construction on $X$ can be expressed as
\begin{align*}
    \sum_{Y \in \sD} \abs{\Pr[X \to_{T_P} Y] - \frac{1}{\abs{\sD}}} = \sum_{Y \in \sD} \abs{\langle e_X, (T_P - T_G) e_Y \rangle}.
\end{align*}
where $\Pr[X \to_{T_P} Y]$ specifies the probability that our sampled circuit outputs $Y$ on the inputs in $X$ and $e_X$ represents the point mass on $X$ vector. This linear algebraic view turns out to be useful in providing bounds, since it allows us to use spectral techniques.

First, we introduce as an intermediate for analysis a \textit{fully idealized} row-column operator $T_{G_R}(T_{G_C}T_{G_R})^t$ which is analogous to our circuit operator $T_{P_R}(T_{P_C}T_{P_R})$ but with each 1D circuit replaced by a fully random permutation. By the approximate $k$-wise independence of the row and column permutations, replacing $T_P$ by this operator does not change the TV distance much.

We then look at the quantity
\begin{align*}
    \sum_{Y\in \sD}\abs{\langle e_X,(T_{G_R}\pbra{T_{G_C}T_{G_R}}^t-T_G)e_Y\rangle }.
\end{align*}
Each term in this sum can be bound using spectral techniques. Here, we reuse the idea of providing an orthogonal decomposition of the space $\R^{\{\pm1\}^{nk}}$ by partitioning $\{\pm1\}^{nk}$ and doing casework on how these random permutations act on particular tuples in $\{\pm1\}^{nk}$. For example, if a $k$-tuple of grids $X$ has that all rows across the tuple are distinct, then the action of $G_R$ on $X$ is actually very close to the action of $G$ already. To see this, observe that $G_R$ applies a uniform permutation on each row, which can be seen as sampling distinct rows across the tuple. Conversely, $G$ samples distinct grids. Compare this to the operators $H_R$ and $H$, which do the same sampling completely uniformly. While $G_R$ and $G$ are different, $H_R$ and $H$ are exactly the same. In the regime $k \ll \sqrt{n}$, the classic birthday bound tells us that these processes then look very similar.

These regions where $G_R$ looks like $G$ end up being quite large. However, there exist small regions of the graph on which $G_R$ and $G$ act very differently, and indeed it is this fact that causes the operator norm of $T_{G_R} - T_G$ to be large. For example, it can be the case that $X$ is such that all but one row is completely uniform on all elements in a $k$-tuple of $\sqrt{n}$-by-$\sqrt{n}$ grids. Then $G_R X$ must have the same property.

However, in such regions, $G_C$ must then act somewhat similarly to a completely random permutation $G$. More specifically, we will be able to show that by applying $G_C$ in between applications of $G_R$, we are able to ``escape'' the bad regions where $G_R$ does not look like $G$ and show that the end result operator is comparable. We end up with a spectral bound along the lines of:
\begin{equation*}
    \norm{T_{G_C}T_{G_R} - T_G}_{2} \ll \frac{1}{2^{\sqrt{n}}}.
\end{equation*}
Powering (which corresponds to repeating the construction sequentially) allows us to improve this bound exponentially. There is one slight problem that arises here: black box converting this to the statement on the TV distance as above suffers from a blow up on the order $2^{nk}$, similar to the reason we cannot apply a naive spectral norm bound to mixing time argument in the first place.

We get around this by observing that in our setting the Markov chain is actually ``warm-started'' by the first application of $G_R$, in that, $\norm{T_{G_R}e_X}_{2}$ is small already. This does not quite work as is: the bad regions of $\{\pm1\}^{nk}$ still have that $\norm{T_{G_R}e_X}_{2}$ is too large. We supplement by showing that in this case $G_C$ helps us escape the bad region with good probability. This argument breaks in our favor: viewed this way the warm-start brings the blow up from $2^{nk}$ to $2^{\sqrt{n} \cdot \widetilde{O}(k)}$.

See \Cref{sec:proof of 2D to 1D reduction} for the full proof of \Cref{thm:2D to 1D reduction}.
