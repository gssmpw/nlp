\section{Related Works}
\label{sec_related_works}
Several works have investigated the training dynamics of neural networks in the small initialization regime. One of the earliest works examined diagonal linear networks, showing that small initialization leads gradient flow to converge towards minimum $\ell_1$-norm solutions **Hanson, "On Spectral Learning"**. For linear neural networks, it has been observed that gradient descent with small initialization tends to converge toward low-rank solutions **Saxe, "Exact Computation in GPUs"**, although rigorous results are primarily limited to shallow linear networks **Poole, "Improved Regression by Probabilistic Backpropagation"**. A similar sparsity-inducing effect of small initialization has been observed for non-linear neural networks as well **Choromanska, "The Loss Surfaces of Multilayer Networks"**, but theoretical results have mostly been established for two-layer neural networks trained on simple data sets **Saxe, "Exact Computation in GPUs"**. Recent works on two-layer networks have explored more challenging scenarios **Hardt, "Train Faster, Generalize Better: Consistency+ Regularization from Data Augmentation"**, but they often make other assumptions about the training algorithm, such as layer-wise training, the use of explicit regularization like weight decay, etc., whereas we study the dynamics of gradient flow with respect to all the weights and do not add any explicit regularization in the training loss. It is also worth noting that most of the aforementioned works examine the \emph{entire} training process; in contrast, our work describes a segment of the gradient flow dynamics beyond the origin, however, our results hold for a wider class of neural networks.

Another line of work has identified the so-called saddle-to-saddle dynamics in the trajectory of gradient descent when the initialization is small **Jin, "Saddle-Point Gradient Dynamics for Nonconvex Learning"**.  These works have observed that during training, the trajectory of gradient descent passes through a sequence of saddle points. Moreover, the loss curve almost appears like a piece-wise constant function, alternating between being stagnant and decreasing sharply; see \Cref{fig:loss_evol_2l} for an example. Some other works also refer to this phenomenon as \emph{incremental learning} **Kawaguchi, "Deep Learning without Weak Uniqueness"** , noting that the function learned by the neural network gradually increases in complexity as it moves from one saddle to another. So far, this kind of saddle-to-saddle dynamics has been rigorously established for diagonal linear neural networks **Ge, "Learning One-hidden-layer Neural Networks with Finite Width through SGD"** and two-layer linear and non-linear neural networks trained with various gradient-based methods under data-related assumptions **Chen, "Convergence of Gradient Descent on Simplex Polytopes"**, and is conjectured to be true for a wider class of neural networks. Our work, which describes the first saddle point encountered by gradient flow after escaping the origin, can be seen as a step towards establishing it.

Lastly, we highlight the work by **Janzamin, "How to Escape Saddle Points Efficiently"**, which studies the gradient flow dynamics of linear neural networks under small initialization after escaping the origin. While our paper studies a broader class of neural networks, their proof technique has inspired our approach.