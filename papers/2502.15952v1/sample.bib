@article{luo_condense,
	author  = {Tao Luo and Zhi-Qin John Xu and Zheng Ma and Yaoyu Zhang},
	title   = {Phase Diagram for Two-layer {R}e{LU} Neural Networks at Infinite-width Limit},
	journal = {Journal of Machine Learning Research},
	year    = {2021},
	volume  = {22},
	number  = {71}
}

@inproceedings{mei_mean,
	title={Mean-field theory of two-layers neural networks: {D}imension-free bounds and kernel limit},
	author={Mei, Song and Misiakiewicz, Theodor and Montanari, Andrea},
	booktitle={Conference on learning theory},
	year={2019}
}

@article{lojasiewicz_convg,
	title={Sur les trajectoires du gradient dâ€™une fonction analytique},
	author={Lojasiewicz, Stanislaw},
	journal={Seminari di geometria},
	volume={1983},
	year={1982}
}

@article{absil_convg,
	title={Convergence of the Iterates of Descent Methods for Analytic Cost Functions},
	author={Absil, PA and Mahony, R and Andrews, B},
	journal={SIAM Journal on Optimization},
	year={2005},
	publisher={Society for Industrial and Applied Mathematics}
}

@article{lee2_esc_saddle,
	title={First-order methods almost always avoid strict saddle points},
	author={Lee, Jason D and Panageas, Ioannis and Piliouras, Georgios and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
	journal={Mathematical programming},
	year={2019}
}

@inproceedings{lee1_esc_saddle,
	title={Gradient descent only converges to minimizers},
	author={Lee, Jason D and Simchowitz, Max and Jordan, Michael I and Recht, Benjamin},
	booktitle={Conference on learning theory},
	year={2016}
}

@inproceedings{abbe_sgd,
	title={{SGD} learning on neural networks: {L}eap complexity and saddle-to-saddle dynamics},
	author={Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
	booktitle={The Thirty Sixth Annual Conference on Learning Theory},
	year={2023}
}

@inproceedings{
	mousavi_sgd,
	title={Neural Networks Efficiently Learn Low-Dimensional Representations with {SGD}},
	author={Alireza Mousavi-Hosseini and Sejun Park and Manuela Girotti and Ioannis Mitliagkas and Murat A Erdogdu},
	booktitle={The Eleventh International Conference on Learning Representations },
	year={2023}
}

@article{montanari_saddle,
	title={Learning time-scales in two-layers neural networks},
	author={Berthier, Rapha{\"e}l and Montanari, Andrea and Zhou, Kangjie},
	journal={arXiv preprint arXiv:2303.00055},
	year={2023}
}

@article{abbe_inc,
	title={Transformers learn through gradual rank increase},
	author={Abbe, Emmanuel and Bengio, Samy and Boix-Adsera, Enric and Littwin, Etai and Susskind, Joshua},
	journal={Advances in Neural Information Processing Systems},
	year={2024}
}

@article{gidel_incr,
	title={Implicit regularization of discrete gradient dynamics in linear neural networks},
	author={Gidel, Gauthier and Bach, Francis and Lacoste-Julien, Simon},
	journal={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{
	gissin_incr,
	title={The Implicit Bias of Depth: {H}ow Incremental Learning Drives Generalization},
	author={Daniel Gissin and Shai Shalev-Shwartz and Amit Daniely},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@article{
	kumar_dc,
	title={Directional Convergence Near Small Initializations and Saddles in Two-Homogeneous Neural Networks},
	author={Akshay Kumar and Jarvis Haupt},
	journal={Transactions on Machine Learning Research},
	year={2024}
}

@article{lecun_deep,
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	journal = {Nature},
	number = 7553,
	pages = 436,
	publisher = {Nature Publishing Group},
	title = {Deep learning},
	volume = 521,
	year = 2015
}

@InProceedings{lee_saddle,
	title = 	 {Understanding Incremental Learning of Gradient Descent: {A} Fine-grained Analysis of Matrix Sensing},
	author =       {Jin, Jikai and Li, Zhiyuan and Lyu, Kaifeng and Du, Simon Shaolei and Lee, Jason D.},
	booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
	year = 	 {2023}
}

@inproceedings{lyu_resolving,
	title={Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: {G}reedy Low-Rank Learning},
	author={Li, Zhiyuan and Luo, Yuping and Lyu, Kaifeng},
	booktitle={International Conference on Learning Representations},
	year={2021}
}

@inproceedings{ba_hd,
	title={High-dimensional Asymptotics of Feature Learning: {H}ow One Gradient Step Improves the Representation},
	author={Jimmy Ba and Murat A Erdogdu and Taiji Suzuki and Zhichao Wang and Denny Wu and Greg Yang},
	booktitle={Advances in Neural Information Processing Systems},
	year={2022}
}

@inproceedings{damian_reps,
	title={Neural networks can learn representations with gradient descent},
	author={Damian, Alexandru and Lee, Jason and Soltanolkotabi, Mahdi},
	booktitle={Conference on Learning Theory},
	year={2022}
}

@inproceedings{gf_orth,
	title={Gradient flow dynamics of shallow {R}e{LU} networks for square loss and orthogonal inputs},
	author={Etienne Boursier and Loucas Pillaud-Vivien and Nicolas Flammarion},
	booktitle={Advances in Neural Information Processing Systems},
	year={2022}
}

@inproceedings{wang_saddle,
	title={Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of {R}e{LU} Networks},
	author={Mingze Wang and Chao Ma},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023}
}

@article{maennel_quant,
	title={Gradient descent quantizes {R}e{LU} network features},
	author={Maennel, Hartmut and Bousquet, Olivier and Gelly, Sylvain},
	journal={arXiv preprint arXiv:1803.08367},
	year={2018}
}

@inproceedings{lyu_simp,
	author = {Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Gradient Descent on Two-layer Nets: {M}argin Maximization and Simplicity Bias},
	year = {2021}
}

@InProceedings{chizat_inf,
	title = 	 {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
	author =       {Chizat, L\'ena\"ic  and Bach, Francis},
	booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
	year = 	 {2020}
}


@InProceedings{abbe_msp,
	title = 	 {The merged-staircase property: a necessary and nearly sufficient condition for {SGD} learning of sparse functions on two-layer neural networks},
	author =       {Abbe, Emmanuel and Adsera, Enric Boix and Misiakiewicz, Theodor},
	booktitle = 	 {Proceedings of Thirty Fifth Conference on Learning Theory},
	year = 	 {2022}
}


@inproceedings{arora_exact,
	author = {Arora, Sanjeev and Du, Simon S and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Russ R and Wang, Ruosong},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {On Exact Computation with an Infinitely Wide Neural Net},
	year = {2019}
}

@misc{early_dc,
	title={Early Directional Convergence in Deep Homogeneous Neural Networks for Small Initializations}, 
	author={Akshay Kumar and Jarvis Haupt},
	year={2024},
	eprint={2403.08121},
	archivePrefix={arXiv},
	primaryClass={cs.LG}
}

@inproceedings{ntk,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Clement},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
	year = {2018}
}

@article{jacot_sd,
	title={Saddle-to-saddle dynamics in deep linear networks: {S}mall initialization training, symmetry, and sparsity},
	author={Jacot, Arthur and Ged, Fran{\c{c}}ois and {\c{S}}im{\c{s}}ek, Berfin and Hongler, Cl{\'e}ment and Gabriel, Franck},
	journal={arXiv preprint arXiv:2106.15933},
	year={2021}
}

@inproceedings{chizat_lazy,
	author = {Chizat, L\'{e}na\"{\i}c and Oyallon, Edouard and Bach, Francis},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {On Lazy Training in Differentiable Programming},
	year = {2019}
}

@inproceedings{dc_three_layer,
	author = {Zhou, Hanxu and Qixuan, Zhou and Jin, Zhenyuan and Luo, Tao and Zhang, Yaoyu and Xu, Zhi-Qin},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Empirical Phase Diagram for Three-layer Neural Networks with Infinite Width},
	year = {2022}
}


@article{geiger_feature,
	title={Disentangling feature and lazy training in deep neural networks},
	author={Geiger, Mario and Spigler, Stefano and Jacot, Arthur and Wyart, Matthieu},
	journal={Journal of Statistical Mechanics: Theory and Experiment},
	year={2020},
	publisher={IOP Publishing}
}

@article{dln_sparse,
	title={Implicit regularization for optimal sparse recovery},
	author={Vaskevicius, Tomas and Kanade, Varun and Rebeschini, Patrick},
	journal={Advances in Neural Information Processing Systems},
	year={2019}
}

@inproceedings{pesme_sd,
	title={Saddle-to-Saddle Dynamics in Diagonal Linear Networks},
	author={Scott Pesme and Nicolas Flammarion},
	booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
	year={2023}
}

@InProceedings{srebro_ib,
	title = 	 {Kernel and Rich Regimes in Overparametrized Models},
	author =       {Woodworth, Blake and Gunasekar, Suriya and Lee, Jason D. and Moroshko, Edward and Savarese, Pedro and Golan, Itay and Soudry, Daniel and Srebro, Nathan},
	booktitle = 	 {Proceedings of Thirty Third Conference on Learning Theory},
	pages = 	 {3635--3673},
	year = 	 {2020}
}

@inproceedings{guna_mtx_fct,
	author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {},
	title = {Implicit Regularization in Matrix Factorization},
	year = {2017}
}

@inproceedings{
	Lyu_ib,
	title={Gradient Descent Maximizes the Margin of Homogeneous Neural Networks},
	author={Kaifeng Lyu and Jian Li},
	booktitle={International Conference on Learning Representations},
	year={2020}
}

@inproceedings{
	atanasov_align,
	title={Neural Networks as Kernel Learners: {T}he Silent Alignment Effect},
	author={Alexander Atanasov and Blake Bordelon and Cengiz Pehlevan},
	booktitle={International Conference on Learning Representations},
	year={2022}
}

@inproceedings{ji_matus_align,
	author = {Ji, Ziwei and Telgarsky, Matus},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Directional convergence and alignment in deep learning},
	year = {2020}
}

@inproceedings{cohen_mtx_fct,
	author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Implicit Regularization in Deep Matrix Factorization},
	year = {2019}
}

@inproceedings{ji_gradient,
	title={Gradient descent aligns the layers of deep linear networks},
	author={Ziwei Ji and Matus Telgarsky},
	booktitle={International Conference on Learning Representations},
	year={2019}
}

@article{soudry_ib, 
	author = {Soudry, Daniel and Hoffer, Elad and Nacson, Mor Shpigel and Gunasekar, Suriya and Srebro, Nathan}, 
	title = {The Implicit Bias of Gradient Descent on Separable Data}, 
	year = {2018}, 
	issue_date = {January 2018}, 
	volume = {19}, 
	number = {1}, 
	journal = {J. Mach. Learn. Res.}, 
	month = jan
}

@inproceedings{mahdi_ib,
	author = {St\"{o}ger, Dominik and Soltanolkotabi, Mahdi},
	booktitle = {Advances in Neural Information Processing Systems},
	title = {Small random initialization is akin to spectral learning: {O}ptimization and generalization guarantees for overparameterized low-rank matrix reconstruction},
	year = {2021}
}

@InProceedings{fl_yang,
	title = 	 {Tensor Programs {IV}: {F}eature Learning in Infinite-Width Neural Networks},
	author =       {Yang, Greg and Hu, Edward J.},
	booktitle = 	 {Proceedings of the 38th International Conference on Machine Learning},
	year = 	 {2021}
}




@Article{chow:68,
  author = 	 {C. K. Chow and C. N. Liu},
  title = 	 {Approximating discrete probability distributions with dependence trees},
  journal = 	 {IEEE Transactions on Information Theory},
  year = 	 {1968},
  volume = 	 {IT-14},
  number = 	 {3},
  pages = 	 {462--467}}


@Book{pearl:88,
  author = 	 {Judea Pearl},
  title = 	 {Probabilistic {R}easoning in {I}ntelligent {S}ystems: 
		  {N}etworks of {P}lausible {I}nference},
  publisher = 	 {Morgan Kaufman Publishers},
  year = 	 {1988},
  address = 	 {San Mateo, CA}
}

