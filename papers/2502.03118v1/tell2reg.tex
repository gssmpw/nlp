% Template for ISBI paper; to be used with:
%          spconf.sty  - ICASSP/ICIP LaTeX style file, and
%          IEEEbib.bst - IEEE bibliography style file.
% --------------------------------------------------------------------------
\documentclass{article}
\usepackage{spconf,amsmath,graphicx, amsfonts}
\usepackage{algorithm}
\usepackage{algpseudocode}
% It's fine to compress itemized lists if you used them in the
% manuscript
\usepackage{enumitem}
% \setlist{nosep, leftmargin=14pt}
\usepackage{hyperref, lmodern}
\usepackage{url}
\usepackage{mwe} % to get dummy images
\usepackage{multirow}
\usepackage{titlesec}
% \titlespacing*{\section}{0pt}{*1}{*0.5}
% \titlespacing*{\subsection}{0pt}{*0.5}{*0.5}
% \usepackage{authblk}
% Example definitions.
% --------------------
\def\x{{\mathbf x}}
\def\L{{\cal L}}

% Title.
% ------
\title{Tell2Reg: Establishing spatial correspondence between images by the same language prompts}
%

\name{
\parbox{\linewidth}{\centering
Wen Yan$^{1,2}$, Qianye Yang$^{1,2}$, Shiqi Huang$^{1,2}$, Yipei Wang$^{1,2}$, Shonit Punwani$^{5}$, Mark Emberton$^{6}$, Vasilis Stavrinides$^{3,4,7}$, Yipeng Hu$^{1,2}$, Dean Barratt$^{1,2}$}}

\address{
\parbox{\linewidth}{\centering
\{$^{1}$UCL Hawkes Institute; 
$^2$Department of Medical Physics and Biomedical Engineering;
$^3$Cancer Institute;
$^4$Urology Department, UCL Hospital;
$^{5}$Centre for Medical Imaging, Division of Medicine;
$^{6}$Division of Surgery and Interventional Science\}, University College London\\
$^7$ Radiology Department, Imperial College Healthcare
}}

\begin{document}
%\ninept
%
\maketitle
\begin{abstract}
Spatial correspondence can be represented by pairs of segmented regions, such that the image registration networks aim to segment corresponding regions rather than predicting displacement fields or transformation parameters. In this work, we show that such a corresponding region pair can be predicted by the same language prompt on two different images using the pre-trained large multimodal models based on GroundingDINO and SAM. This enables a fully automated and training-free registration algorithm, potentially generalisable to a wide range of image registration tasks. In this paper, we present experimental results using one of the challenging tasks, registering inter-subject prostate MR images, which involves both highly variable intensity and morphology between patients. Tell2Reg is training-free, eliminating the need for costly and time-consuming data curation and labelling that was previously required for this registration task. This approach outperforms unsupervised learning-based registration methods tested, and has a performance comparable to weakly-supervised methods. Additional qualitative results are also presented to suggest that, for the first time, there is a potential correlation between language semantics and spatial correspondence, including the spatial invariance in language-prompted regions and the difference in language prompts between the obtained local and global correspondences. Code is available at \url{https://github.com/yanwenCi/Tell2Reg.git}.
% Existing learning-based image registration networks typically output pixel or feature correspondences by training models to predict transformation parameters or displacement fields. However, these models are task-specific and require retraining to generalize to new tasks, different modalities and/or varying imaging protocols. This paper proposes a novel registration method that leverages object-based region correspondences (ROIs) to achieve image registration. The proposed approach is training-free, utilizing pre-trained models such as GroundingDINO and SAM to generate corresponding ROIs, enabling a text-ROI-image workflow for registration. This method capitalizes on the generalization power of foundation models, making it adaptable across diverse tasks. Evaluation of prostate MRI images demonstrates that the proposed method outperforms state-of-the-art registration techniques.

\end{abstract}
%
\begin{keywords}
Segment anything model, GroundingDINO, language models, image registration
\end{keywords}
%
\section{Introduction}
\label{sec:intro}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\linewidth]{figs/samregister-abstract-2.png}
    \caption{A brief illustration of how Tell2Reg framework uses text prompts to generate corresponding ROIs from fixed and moving images.}
    \label{fig:abstract}
\end{figure}
 
MR imaging has increasingly been used for diagnosing prostate cancer, planning targeted biopsy and other treatment procedures. Registering images from different patients is an interesting research topic, which may enable propagating procedural plans from reference images to new patients, constructing MR-based lower-pelvic atlases and other similar population studies~\cite{harris2015consensus}. Among the sequences that are useful for prostate cancer diagnosis, T2-weighted sequences contain the richest soft tissue contrast and are often used in these registration tasks, which is a focus of the previous work as well as this study \footnote{The potential need, albeit comparably minor, of inter-sequence registration has been a research topic of its own~\cite{xu2016evaluation} and is not discussed further in this paper.}. Registering inter-subject lower-pelvic MR images is challenging due to significant variability in intensity and morphology between subjects, unlike the intra-subject registration tasks~\cite{yang2021morphological}. This task usually requires large training datasets and benefits from segmentation labels and advanced training strategies~\cite{9925717}. Classical iterative algorithms has been shown inferior to learning-based approaches in this application~\cite{9925717}.
Existing image registration methods often use deep learning models to learn pixel or feature correspondences between moving (to be aligned) and fixed (reference) images~\cite{balakrishnan2019voxelmorph,evan2022keymorph,chen2022transmorph}. These models typically optimize a similarity metric that measures how well the warped moving image aligns with the fixed image, based on intensity differences, such as cross correlation and mutual information. %Models like VoxelMorph~\cite{balakrishnan2019voxelmorph}, KeyMorph~\cite{evan2022keymorph}, and TransMorph~\cite{chen2022transmorph} have been developed for this purpose. 
Recent work also incorporated large language models for the same optimisation goal~\cite{ma2024large, chen2024spatially}, to predict a deformation field or a parametric transformation. 
%However, these methods are domain-specific and cannot transfer to other domains without retraining or fine-tuning. 
Following the discussion in a previous work~\cite{huang2024one} that connects region-level correspondence and the ability to segment a wide range of objects, with or without anatomical significance, this paper describes a novel approach to establish spatial correspondence by multimodal approaches with language prompts.
Notably, segment anything model (SAM)~\cite{kirillov2023segment} has %set a new milestone in object detection, which can detect any object in the image with a simple point box prompt without training.
%This also inspired new attempts to use SAM 
been developed together with language models, such as CLIP~\cite{hafner2021clip} and Bert~\cite{kenton2019bert}, to generate regions of interest (ROIs) from text prompts. GroundingDINO~\cite{liu2023grounding} models have merged the multi-level language model with object detection, generating ROIs from text prompts via contrastive learning.  

We hypothesize that the image registration task does not need to optimise pixel-level correspondence during model training; instead, it can be conceptualized as detecting the same regions from different images, therefore leveraging capabilities of segmentation models such as SAM without retraining. Further, by utilising pre-trained multimodal systems, these corresponding regions ought to be directly predicted by prompting identical text in the proposed ``Tell2Reg'' approach, illustrated in Fig.\ref{fig:abstract}. The proposed image registration does not require any training or finetuning, outperforming the tested state-of-the-art learning-based registration methods.
%Post-processing to refine these corresponding regions may further improve If the corresponding regions need  These ROIs can then be used to learn the correspondence between images, 


\section{Method}
\label{sec:method}
The above assertion - segmenting regions on both fixed and moving images using identical text prompts yields corresponding ROIs - represents an ideal scenario rather than a guaranteed outcome. 
%In practice, while using the same text prompts may enhance consistency in segmentation, several factors can affect the accuracy of the derived ROIs. Consequently, 
With practical constraints such as the availability of medical-image-finetuned foundation models, selecting ROIs may require further prompt engineering and/or post-processing to ensure that the segmented ROIs are corresponding pairs. 
%Our experiments show that the SAM model does not need finetuning or retraining to adapt to medical image tasks, even though the model is trained on natural images.   
This section describes a specific algorithm, utilising pre-trained SAM and GroundingDINO, outlined in Fig.~\ref{fig:net}.

 
\subsection{Text to corresponding ROIs}
\label{sec:roi}
%groundingdino+sam
We start our discussion with a theorem \cite{huang2024one} stating that the registration task can be framed as a correspondence learning problem between regions. Let the fixed and moving images be  $I^{\textit{fix}}$ and  $I^{\textit{mov}}$, respectively. Establishing spatial correspondence between the two images can be achieved by identifying \( K \) pairs of corresponding ROIs, denoted as \( \{(R_k^{\textit{fix}}, R_k^{\textit{mov}})\}_{k=1}^K \). In the limiting case where each ROI reduces to a single pixel, the ROI-based correspondence becomes equivalent to a pixel-wise one, as governed by a dense displacement field (DDF). 

We use GroundingDINO~\cite{liu2023grounding} to generate bounding boxes ${B}^{\textit{fix}}=f_{dino}(I^{\textit{fix}}, p)$ and ${B}^{\textit{mov}}=f_{dino}(I^{\textit{mov}}, p)$ for fixed and moving images with same text prompt $p$, respectively, where $p$ can be any text, examples are given in Table~\ref{tab:prompts}.
%where image $\textit{im}$ can be either the fixed $\textit{fix}$ or moving $\textit{mov}$ images, $\textit{im} \in \{\textit{fix}, \textit{mov}\}$. 
Ideally, bounding boxes generated from identical text prompts should correspond precisely between the fixed and moving images; however, due to the limits of the GroundingDINO model on prostate data,  which could lead to false positive correspondence, by detecting regions that appear similar based on texture or intensity but are not corresponding regions of registration interest.
%allows multiple objects corresponding to the same prompts. 
%This is particularly challenging when processing medical images, where the model lacks prior knowledge of anatomical structures. despite the fact that a pair of ROIs that indicate spatial correspondence need not contain known anatomical significance~\cite{huang2024one}, more specifically, the generated ROIs are not corresponding to identical anatomical structures, such as prostate, rectum or bladder.
%However, we argue that anatomically significant objects are unnecessary for the registration task as long as the yielded ROIs are consistent between the fixed and moving images. 


\begin{figure}[htb]
\centering
\includegraphics[width=0.88\linewidth]{figs/samregister-net-5.png}
\caption{Tell2Reg framework using pre-trained GroundingDINO and SAM. The ROI-to-dense transformation is an optional step for comparing with other methods.}
\label{fig:net}
\end{figure}

Before inputting BBox $B=\{B^{fix}, B^{mov}\}$ into the SAM model, we filter out those that are too small or too large, for reducing computation costs and potential outliers.
We then use the remaining bounding boxes as box prompts to segment ROIs in both fixed and moving images, to obtain the segmented ROIs for fixed and moving: \({R}^{\textit{fix}}=f_{sam}(I^{\textit{fix}}, {B}^{\textit{fix}})\) and \({R}^{\textit{mov}}=f_{sam}(I^{\textit{mov}}, {B}^{\textit{mov}})\)  
resulting in $\{M^{\textit{fix}}\}_{k=1}^{K^{\textit{fix}}}$ and $\{M^{\textit{mov}}\}_{k=1}^{K^{\textit{mov}}}$ as the binary masks of the ROIs in the fixed and moving images, respectively.
%We assume that the Grounding DINO model can detect same objects in the images with the same text prompts, regardless of the antomical significance. However, Unlike natural images, which often contain distinct objects and well-defined boundaries, medical images may exhibit irregular shapes and indistinct edges, complicating the task of object recognition. Thus, model may not be able to distinguish the objects effecively. 

%We establish correspondence between the ROIs in the fixed and moving images by selecting the minimal distance between prototype ROIs, ensuring accurate alignment. These matched ROIs are then used as box prompts to segment the objects of interest within the SAM model. 

\subsection{ROI correspondence refinement}
\label{ssec:reg}
To establish robust correspondence between these ROIs, we propose examining the similarity between ROI prototypes from the same or different text prompts. The ROI prototypes are generated by averaging the ROIs in the fixed and moving images. Given two sets of ROIs, \(\{{R}^{\textit{fix}}\}_{k=1}^{K^{\textit{fix}}}\) and \(\{{R}^{\textit{mov}}\}^{K^{\textit{mov}}}_{k=1}\), where $K^{\textit{mov}}$ may not be equal to $K^{\textit{fix}}$, the proposed matching process finds the correspondence ROIs by minimizing the $L^2$ distance between the ROI prototypes, detailed in Algorithm~\ref{alg:correspondence}, yielding $K^{\textit{cor}}$ pairs of matched 
ROIs $\{(R_k^{\textit{fix}}, R_k^{\textit{mov}})\}_{k=1}^{K^{\textit{cor}}}$. It is interesting to discuss that, when the specificity of the multimodal segmentation system (Sec.~\ref{sec:roi}) improves, this refining step may render itself unnecessary, while the differences to the previous work~\cite{huang2024one} include better-selected ROI sets (due to the correspondence-informing text prompts) and thus a different similarity function.
These ROIs $\{(R_k^{\textit{fix}}, R_k^{\textit{mov}})\}_{k=1}^{K^{\textit{cor}}}$ represent the region-level correspondence between the fixed and moving images.

\begin{algorithm}[!htb]
\label{alg:correspondence}
  \caption{ROI correspondence calculation}
  \begin{algorithmic}[1]
  \State \textbf{Input}: Image $I^{\textit{fix}}, I^{\textit{mov}}$, ROI masks $\{{M}_k^{\textit{fix}}\}_{k=1}^{K^{\textit{fix}}}$, $\{{M}_k^{\textit{mov}}\}_{k=1}^{K^{\textit{mov}}}$, Pretrained SAM model $f_{sam}$

  \State \textbf{Output}: Paired ROIs $\{R^{\textit{fix}}_k, R^{\textit{mov}}_k\}_{k=1}^{K^{\textit{cor}}}$ 
  
  \State \textbf{Step 1: Compute ROI embeddings}
  
  \For{each mask ${M}_i^{fix}$}
    \State Compute image embeddings ${E^{\textit{fix}}}$ by $f_{sam}(I^{\textit{fix}})$.
    \State Resize mask ${M}_i^{fix}$ to size of $E^{\textit{fix}}$ as $M_i^{'fix}$:
      \State Multiply the embedding by the mask to isolate ROI features:
      \(
      {E}_{roi}^{fix} = {E}^{fix} \cdot {M}_i^{'fix}
      \)
      \State Compute prototypes within ROI for both fix and moving:
      \(
      {e}_i^{\textit{fix}} = \frac{1}{|{M}_i'|} \sum_{x,y} {E}_{roi}^{fix}[x,y]
      \).
      % \State Store prototypes $\mathbf{e}_i$
\EndFor
\For {each mask $M_j^{mov}$}
    \State Do 5-8 for moving embedding, yielding \({e}_j^{\textit{mov}}\).
    \EndFor
  
  \State \textbf{Step 2: Compute Similarity Matrix}
  
  \For{each pair of embeddings $({e}_i^{\textit{fix}}, {e}_j^{\textit{mov}})$}
      \State Compute cosine similarity:
      \(
      S[i, j] = \frac{{e}_i^{\textit{fix}} \cdot {e}_j^{\textit{mov}}}{\|{e}_i^{\textit{fix}}\| \|{e}_j^{\textit{mov}}\|}
      \)
  \EndFor
   
  \State \textbf{Step 3: Find best correspondence}
  \State Paired ROIs: $\{R^\textit{{fix}}_k, R^{\textit{mov}}_k\}_{k=1}^{K^{\textit{cor}}}
=\arg\max\limits_{{(i,j)}}(S[i,j])$
  
  \State \textbf{Return}: Paired ROIs $\{R^\textit{{fix}}_k, R^{\textit{mov}}_k\}_{k=1}^{K^{\textit{cor}}}$

  \end{algorithmic}
  \end{algorithm}

  
\subsection{Optional dense transformation}
\label{ssec:roi2dense}
When useful, the region-level correspondence (represented by the ROI pairs, Sec.~\ref{ssec:reg}) can be converted to voxel-level dense correspondence (represented by a DDF), by iteratively minimising a region-specific alignment error $\mathcal{L}_{roi}$ and a $L^2$ regularization term $\mathcal{L}_{reg}$:
$\mathcal{L} = \mathbb{E}_{k}[\mathcal{L}_{roi}({R}_k^{fix}, \mathcal{T}({R}_k^{mov}, \Theta))] + \lambda \mathcal{L}_{reg}(\Theta)$, 
where $\mathbb{E}_{k}[\cdot]$ is the mathematical expectation, $\Theta$ is the parameters of the transformation model (here, the DDF), and $\lambda$ is the regularization weight. The region-specific alignment error $\mathcal{L}_{roi}$ is an equally weighted Dice and MSE loss.



\section{Experiments and Results}
\label{sec:results}
\subsection{Dataset and implementation details}
\label{ssec:data}
The five hundred and forty-two pairs T2 pelvis mpMR images were acquired from 850 prostate cancer patients, part of several clinical trials conducted at University College London Hospital. The images were resampled to 1mm isotropic resolution, sized $200 \times 200 \times 96$. Prostate segmentation masks labelled by experts are readily available for evaluation and ablation studies. The data were split to training, validation and test sets by 365, 90 and 87 pairs respectively.
 As our proposed method is totally training-free, we only use 87 pairs test data, while the other methods use all datasets. The proposed model was implemented with Pytorch, using pre-trained models~\cite{liu2023grounding,kirillov2023segment}, described in Sec.~\ref{sec:roi}.  Dice and target registration error (TRE) of centroids are used to evaluate the registration performance. The detection ratio describes the proportion of prostate ROI correspondences detected using a specific text prompt: ${N^{ROI^{cor}}_{prostate}}/{N_{prostate}}$, where $N^{ROI^{cor}}_{prostate}$ is detected corresponding prostate ROIs, and $N_{prostate}$ is the total number of prostate in ground-truth. 
%, yeilding 850 $\times$ 96 pairs of images.
\begin{figure*}[!htb]
\centering  
\includegraphics[width=0.88\linewidth]{figs/visual.png}
\caption{Visualization of registration results. The bounding boxes were generated by GroundingDINO, and the coloured masks were produced by SAM. The binary masks highlight the selected corresponding regions of interest (ROIs) in the fixed and moving images alongside the warped masks to show the optional spatial alignment results. 
The first two groups show corresponded ROIs, and the third group shows bad example of mismatched ROIs.
}\label{fig:visual}
\end{figure*}
\subsection{Comparison experiments}
We compare the proposed method with the state-of-the-art registration methods, including VoxelMorph~\cite{balakrishnan2019voxelmorph}, KeyMorph~\cite{evan2022keymorph} and TransMorph~\cite{chen2022transmorph}, listed in Table~\ref{tab:results}. All the compared methods also required substantial training data and computation costs for training. The results showed that Tell2Reg outperforms unsupervised methods. As the only exception, weakly supervised TransMorph, which requires prostate segmentation labels, led to a higher average Dice without statistical significance, than that from Tell2Reg ($p=0.060$). It is also arguable that the TREs may be more indicative of registration performance, in many applications. 
Fig.~\ref{fig:visual} shows the visualization of the proposed Tell2Reg outputs. Prior work~\cite{li2022few} showed that an atlas from similar inter-subject registration improved pathology-indicating statistics (p=0.009). The significant improvement reported here highlights strong clinical potential to explore.
%including two good cases and one bad case. The good cases show that the proposed method can detect the corresponding ROIs and align the images well. The bad case shows that the model fails to detect the corresponding ROIs, leading to misalignment.

\begin{table}[!htb]
\caption{Comparison between the proposed and SOTA registration methods, which employs unsupervised or weakly supervised training using prostate masks, whereas our method is training-free.}
\label{tab:results}
\resizebox{\linewidth}{!}{
\begin{tabular}{c|cc|cc}
\hline
 & \multicolumn{2}{c|}{Unsupervised} & \multicolumn{2}{c}{Weakly Supervised} \\
Methods & Dice & TRE(mm) &  Dice & TRE(mm)\\
\hline
Before register & 0.68$\pm$0.21 & 11.12$\pm$2.22 & / & / \\
VoxelMorph & 0.69$\pm$0.11 & 8.06$\pm$1.67 & 0.78$\pm$0.08 &4.23$\pm$3.19 \\
KeyMorph &  0.73$\pm$0.09 & 5.96$\pm$2.82 &0.84$\pm$0.04 & 3.32$\pm$ 1.88 \\
TransMorph & 0.68$\pm$0.10 & 7.24$\pm$3.72 &0.85$\pm$0.04 & 4.25$\pm$1.90   \\
Tell2Reg &0.84$\pm0.06$ & 3.09$\pm$2.25&/&/ \\ \hline
\end{tabular}}
\end{table}




\begin{table}[!bt]
    \caption{Effect of text prompts on prostate ROI detection and correspondence. The 1st row shows the \textit{detection ratio} of prostate ROIs with different prompts. Remaining rows report the number of ROIs detected in the fixed and moving images and the count of corresponding ROIs between the two images.}
    \label{tab:prompts}
    \centering
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccccc}
    \hline
        Detected ROIs        &\textit{``hole''}&\textit
{``head''}  & \textit{``prostate''} & \text{``dog''} & \textit{``correspond''} & \textit{``middle''}\\\hline
         Prostate ROI& 35\% &11\% &25\% & 19\% & 25\% & 23\%\\
         \hline
         ROIs Moving& 3279 & 1094&  1128 & 600& 427 & 452\\
         ROIs Fixed & 3651 & 1300& 1184& 678& 464 & 553\\
         Corresponding &1700&706& 715 &342 & 231 &271\\ \hline
    \end{tabular}}
\end{table}


% \footnotetext{$\textit{detection ratio}={N^{ROI^{cor}}_{prostate}}/{N_{prostate}}$, where $N^{ROI^{cor}}_{prostate}$ is detected corresponding prostate ROIs, and $N_{prostate}$ is the total number of prostate in ground-truth.}

\subsection{Text prompts comparison}

%\indent\textbf{Effect of text prompts}: 
We first test the registration performance by changing the text prompts and how they are sampled. We summarise our experience as follows:
First, for a specific registration task, the performance is more stable with a fixed set of prompts, predefined empirically and qualitatively with a small set of independent data, compared with those based on randomly sampled (individually or in-pairs) text prompts. However, we are open to explore automatic text prompt refinement in future; 
Second, descriptive phrases generally lead to less corresponding ROIs than concrete or abstract nouns do, as those used in this study. 
Third, perhaps as expected, words and concepts related more closely to natural images (the models were trained with), such as \textit{``cat"}, \textit{``dog"}, \textit{``car"} and \textit{``tree"}, produced fewer sensible ROIs, compared with those found in medical images or more abstract, such as \textit{``hole"},  \textit{``prostate"} and \textit{``middle"}. However, how much each category contributed to useful correspondence remains unclear and an interesting open research question.
%We found that the MR image is sensitive to specific text prompts, like  \textit{``hole"},  \textit{``prostate"} and \textit{``middle"}, but less sensitive to names of natural objects, like \textit{``cat"}, \textit{``dog"}, \textit{``car"} and \textit{``tree"}. This is because the MR image contains more abstract objects, which are not easy to detect by the model but can be detected by analogical descriptions. 
Table~\ref{tab:prompts} summarises a list of text prompts used to produce quantitative results reported in this paper, together with ratios between individual text-prompted ROIs overlapping with the prostate gland, as a reference example. The quantities of prompted ROIs and corresponding pairs after refinement are also summarised in the same table. 

% \subsection{Clinical significance}
% In addition to the substantially reduced computational cost, due to its training-free nature, our proposed methods may enable registration with tasks without any or sufficient training cases. Previous work demonstrated that an atlas constructed using the same inter-subject registration~\cite{li2022few} led to improved pathology-indicating statistics (p=0.009). 

\section{Discussion and Conclusion}
\label{sec:conclusion}
In this paper, we propose a novel registration method which learns the correspondence between regions, by utilising the existing SAM-based object detection model with text prompts. This approach does not require retraining, and shows better generalisability, compared with methods that even require substantial training and training data. Our future work will focus on exploring prostate-specific multimodel foundation models, developing automated prompt engineering, and implementing refinement strategies to enhance the robustness, adaptability, and efficiency of the proposed method. 
% Future work may investigate medical-image-specific multimodal models as well as prompt engineering techniques, for potentially further performance improvement.



\section*{Acknowledgments}
\label{sec:acknowledgments}

This work is supported by the International Alliance for Cancer Early Detection, an alliance between Cancer Research UK [C28070/A30912; C73666/A31378; EDDAMC-2021/100011], Canary Center at Stanford University, the University of Cambridge, OHSU Knight Cancer Institute, University College London and the University of Manchester.

\section*{Compliance with Ethical Standards}
All trial patients gave written consents, with ethics approved as part of the respective trial protocols~\cite{hamid_smarttarget}.
% References should be produced using the bibtex program from suitable
% BiBTeX files (here: strings, refs, manuals). The IEEEbib.bst bibliography
% style file from IEEE produces unsorted bibliography list.
% ------------------------------------------------------------------------- 
\bibliographystyle{IEEEbib}
\bibliography{strings,refs}

\end{document}
