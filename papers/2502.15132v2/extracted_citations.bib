@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@inproceedings{chu2024navigate,
  title={Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1173--1203},
  year={2024}
}

@article{dong2022survey,
  title={A Survey on In-context Learning},
  author={Dong, Li and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@inproceedings{hou2023towards,
  title={Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models},
  author={Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut, Antoine and Sachan, Mrinmaya},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4902--4919},
  year={2023}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{liu2024dag,
  title={DAG-aware Transformer for Causal Effect Estimation},
  author={Liu, Manqing and Bellamy, David R and Beam, Andrew L},
  journal={arXiv preprint arXiv:2410.10044},
  year={2024}
}

@article{nye2021show,
  title={{Show Your Work}: Scratchpads for Intermediate Computation},
  author={Nye, Max and Andreassen, Anders Johan and Abolafia, David A and Chollet, Fran{\c{c}}ois and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{prabhakar2024deciphering,
  title={Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning},
  author={Prabhakar, Akshara and Griffiths, Thomas L and McCoy, R Thomas},
  journal={arXiv preprint arXiv:2407.01687},
  year={2024}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{wang2022rationale,
  title={Rationale-Augmented Ensembles in Language Models},
  author={Wang, Xinyi and Wei, Jason and others},
  journal={arXiv preprint arXiv:2202.XXXX}, 
  year={2022}
}

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Tay, Yi and Davies, Mark and others},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@inproceedings{yang2024context,
  title={An In-Context Learning Theoretic Analysis of Chain-of-Thought},
  author={Yang, Chenxiao and Li, Zhiyuan and Wipf, David},
  year={2024},
  booktitle={ICML 2024 Workshop on In-Context Learning}
}

@inproceedings{zhou2024mystery,
  title={The mystery of in-context learning: A comprehensive survey on interpretation and analysis},
  author={Zhou, Yuxiang and Li, Jiazheng and Xiang, Yanzheng and Yan, Hanqi and Gui, Lin and He, Yulan},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={14365--14378},
  year={2024}
}

