[
  {
    "index": 0,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others",
        "title": "Language Models are Few-Shot Learners"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "garg2022can",
        "author": "Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory",
        "title": "What can transformers learn in-context? a case study of simple function classes"
      },
      {
        "key": "akyurek2024incontext",
        "author": "Ekin Aky{\\\"u}rek and Bailin Wang and Yoon Kim and Jacob Andreas",
        "title": "In-Context Language Learning: Architectures and Algorithms"
      },
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "bai2023transformers",
        "author": "Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei",
        "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "dong2022survey",
        "author": "Dong, Li and others",
        "title": "A Survey on In-context Learning"
      },
      {
        "key": "zhou2024mystery",
        "author": "Zhou, Yuxiang and Li, Jiazheng and Xiang, Yanzheng and Yan, Hanqi and Gui, Lin and He, Yulan",
        "title": "The mystery of in-context learning: A comprehensive survey on interpretation and analysis"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "wei2022chain",
        "author": "Wei, Jason and Tay, Yi and Davies, Mark and others",
        "title": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"
      },
      {
        "key": "nye2021show",
        "author": "Nye, Max and Andreassen, Anders Johan and Abolafia, David A and Chollet, Fran{\\c{c}}ois and Irving, Geoffrey",
        "title": "{Show Your Work}: Scratchpads for Intermediate Computation"
      },
      {
        "key": "kojima2022large",
        "author": "Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke",
        "title": "Large Language Models are Zero-Shot Reasoners"
      },
      {
        "key": "chu2024navigate",
        "author": "Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting",
        "title": "Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "wang2022rationale",
        "author": "Wang, Xinyi and Wei, Jason and others",
        "title": "Rationale-Augmented Ensembles in Language Models"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "liu2024dag",
        "author": "Liu, Manqing and Bellamy, David R and Beam, Andrew L",
        "title": "DAG-aware Transformer for Causal Effect Estimation"
      },
      {
        "key": "prabhakar2024deciphering",
        "author": "Prabhakar, Akshara and Griffiths, Thomas L and McCoy, R Thomas",
        "title": "Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "garg2022can",
        "author": "Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory",
        "title": "What can transformers learn in-context? a case study of simple function classes"
      },
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "bai2023transformers",
        "author": "Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei",
        "title": "Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "edelman2024the",
        "author": "Ezra Edelman and Nikolaos Tsilivis and Benjamin L. Edelman and eran malach and Surbhi Goel",
        "title": "The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "li2023dissecting",
        "author": "Yingcong Li and Kartik Sreenivasan and Angeliki Giannou and Dimitris Papailiopoulos and Samet Oymak",
        "title": "Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "hou2023towards",
        "author": "Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut, Antoine and Sachan, Mrinmaya",
        "title": "Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "yang2024context",
        "author": "Yang, Chenxiao and Li, Zhiyuan and Wipf, David",
        "title": "An In-Context Learning Theoretic Analysis of Chain-of-Thought"
      },
      {
        "key": "prabhakar2024deciphering",
        "author": "Prabhakar, Akshara and Griffiths, Thomas L and McCoy, R Thomas",
        "title": "Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning"
      }
    ]
  }
]