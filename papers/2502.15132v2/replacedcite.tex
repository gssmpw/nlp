\section{Related Work}
\paragraph{In-Context Learning.}
Initially popularized by GPT-3 ____, in-context learning has garnered extensive attention for its surprising ability to generalize with just a few example prompts. Many investigations center on how transformers might implicitly perform gradient descent or implement other adaptation mechanisms in their hidden activations ____. See ____ for surveys on the topic. However, these analyses often assume real-valued examples and very simple data distributions, leaving room to explore richer compositional structures that can align with natural language tasks.

\paragraph{Chain-of-Thought.}
CoT prompting ____ has emerged as an effective technique for eliciting more interpretable (and sometimes more accurate) intermediate reasoning from large language models. Despite empirical successes, debate persists as to whether models truly learn a generalized reasoning algorithm or simply latch onto superficial features ____. While some efforts ____ systematically study CoT's potential, they often rely on limited or handcrafted tasks that do not fully capture the complexity of multi-step compositional processes.

\paragraph{Synthetic Tasks for Controlled Model Analysis.}
Synthetic tasks provide controlled environments that enable precise interventions, ablation studies, and theoretical insights into the model behavior and training dynamics ____. However, existing synthetic settings generally remain numeric and follow overly restrictive Markovian assumptions ____ (e.g., a single parent for each token). Our proposed \coticl~extends these efforts by decoupling the causal structure from token-processing functions. We leverage directed acyclic graphs (DAGs) to control the branching factor in the chain generation, and MLPs for varied levels of token transformations. This design grants extensive configurability, encompassing vocabulary size, multi-input example length, chain length, DAG sparsity, MLP depth, activations and more.

\paragraph{Chain-of-Thought \& Compositional Reasoning.}
Recent studies focus on dissecting CoT to assess how compositionality might emerge from ICL. For instance, ____ examines how CoT might be effectively disentangled into filtering and learning components in the prompt. Furthermore, our work can be treated as a generalization of the \texttt{MechanisticProbe} approach by  ____ where the filtering and reasoning tree construction process is not limited by the availability of natural language datasets.  While these and related efforts ____ represent significant progress toward understanding emergent reasoning, their experimental setups typically do not offer the level of systematic and fine-grained complexity that \coticl~enables.

\bigskip


\vspace{-5mm}