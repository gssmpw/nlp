@article{firooz2025360brew,
  title={360Brew: A Decoder-only Foundation Model for Personalized Ranking and Recommendation},
  author={Firooz, Hamed and Sanjabi, Maziar and Englhardt, Adrian and Gupta, Aman and Levine, Ben and Olgiati, Dre and Polatkan, Gungor and Melnychuk, Iuliia and Ramgopal, Karthik and Talanine, Kirill and others},
  journal={arXiv preprint arXiv:2501.16450},
  year={2025}
}

@inproceedings{li2023transformers,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023},
  organization={PMLR}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@inproceedings{
bai2023transformers,
title={Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection},
author={Yu Bai and Fan Chen and Huan Wang and Caiming Xiong and Song Mei},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@inproceedings{wu2024how,
title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
author={Jingfeng Wu and Difan Zou and Zixiang Chen and Vladimir Braverman and Quanquan Gu and Peter Bartlett},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@article{zhang2024context,
  title={In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization},
  author={Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2402.14951},
  year={2024}
}

@inproceedings{
oko2024pretrained,
title={Pretrained Transformer Efficiently Learns Low-Dimensional Target Functions In-Context},
author={Kazusato Oko and Yujin Song and Taiji Suzuki and Denny Wu},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}

@inproceedings{
li2023dissecting,
title={Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning},
author={Yingcong Li and Kartik Sreenivasan and Angeliki Giannou and Dimitris Papailiopoulos and Samet Oymak},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@article{radfordimproving,
  title={Improving Language Understanding by Generative Pre-Training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya}
}

@inproceedings{
edelman2024the,
title={The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains},
author={Ezra Edelman and Nikolaos Tsilivis and Benjamin L. Edelman and eran malach and Surbhi Goel},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}

@article{dubey2024llama,
  title={The llama 3 herd of models},
  author={Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Yang, Amy and Fan, Angela and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}


######################

@article{brown2020language,
  title={Language Models are Few-Shot Learners},
  author={Brown, Tom B and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{wei2022chain,
  title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author={Wei, Jason and Tay, Yi and Davies, Mark and others},
  journal={arXiv preprint arXiv:2201.11903},
  year={2022}
}

@article{nye2021show,
  title={{Show Your Work}: Scratchpads for Intermediate Computation},
  author={Nye, Max and Andreassen, Anders Johan and Abolafia, David A and Chollet, Fran{\c{c}}ois and Irving, Geoffrey},
  journal={arXiv preprint arXiv:2112.00114},
  year={2021}
}

@article{kojima2022large,
  title={Large Language Models are Zero-Shot Reasoners},
  author={Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and Matsuo, Yutaka and Iwasawa, Yusuke},
  journal={arXiv preprint arXiv:2205.11916},
  year={2022}
}

@article{dong2022survey,
  title={A Survey on In-context Learning},
  author={Dong, Li and others},
  journal={arXiv preprint arXiv:2301.00234},
  year={2022}
}


@article{wang2022rationale,
  title={Rationale-Augmented Ensembles in Language Models},
  author={Wang, Xinyi and Wei, Jason and others},
  journal={arXiv preprint arXiv:2202.XXXX}, 
  year={2022}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{zha2023dissecting,
  title={Dissecting Chain-of-Thought: Compositionality through In-Context Filtering and Learning},
  author={Zha, Shengding and Wang, Xinyi and others},
  journal={arXiv preprint arXiv:2305.18869},
  year={2023}
}

% If you have additional references, update them here:
@article{olsson2022context,
  title={In-context learning and induction heads},
  author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
  journal={arXiv preprint arXiv:2209.11895},
  year={2022}
}

@article{liu2024dag,
  title={DAG-aware Transformer for Causal Effect Estimation},
  author={Liu, Manqing and Bellamy, David R and Beam, Andrew L},
  journal={arXiv preprint arXiv:2410.10044},
  year={2024}
}


@inproceedings{yang2024context,
  title={An In-Context Learning Theoretic Analysis of Chain-of-Thought},
  author={Yang, Chenxiao and Li, Zhiyuan and Wipf, David},
  year={2024},
  booktitle={ICML 2024 Workshop on In-Context Learning}
}



@article{prabhakar2024deciphering,
  title={Deciphering the factors influencing the efficacy of chain-of-thought: Probability, memorization, and noisy reasoning},
  author={Prabhakar, Akshara and Griffiths, Thomas L and McCoy, R Thomas},
  journal={arXiv preprint arXiv:2407.01687},
  year={2024}
}

@article{alstott2014powerlaw,
  title={powerlaw: a Python package for analysis of heavy-tailed distributions},
  author={Alstott, Jeff and Bullmore, Ed and Plenz, Dietmar},
  journal={PloS one},
  volume={9},
  number={1},
  pages={e85777},
  year={2014},
  publisher={Public Library of Science San Francisco, USA}
}

@article{hsu2024liger,
  title={Liger kernel: Efficient triton kernels for llm training},
  author={Hsu, Pin-Lun and Dai, Yun and Kothapalli, Vignesh and Song, Qingquan and Tang, Shao and Zhu, Siyu and Shimizu, Steven and Sahni, Shivam and Ning, Haowen and Chen, Yanning},
  journal={arXiv preprint arXiv:2410.10989},
  year={2024}
}

@inproceedings{
akyurek2024incontext,
title={In-Context Language Learning: Architectures and Algorithms},
author={Ekin Aky{\"u}rek and Bailin Wang and Yoon Kim and Jacob Andreas},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
}

@article{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={45614--45650},
  year={2023}
}

@inproceedings{feng2023towards,
  title={Towards revealing the mystery behind chain of thought: a theoretical perspective},
  author={Feng, Guhao and Zhang, Bohang and Gu, Yuntian and Ye, Haotian and He, Di and Wang, Liwei},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={70757--70798},
  year={2023}
}

@inproceedings{guo2024how,
title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
author={Tianyu Guo and Wei Hu and Song Mei and Huan Wang and Caiming Xiong and Silvio Savarese and Yu Bai},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}

@inproceedings{deutch2024context,
  title={In-context Learning and Gradient Descent Revisited},
  author={Deutch, Gilad and Magar, Nadav and Natan, Tomer and Dar, Guy},
  booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)},
  pages={1017--1028},
  year={2024}
}

@inproceedings{xie2022an,
title={An Explanation of In-context Learning as Implicit Bayesian Inference},
author={Sang Michael Xie and Aditi Raghunathan and Percy Liang and Tengyu Ma},
booktitle={International Conference on Learning Representations},
year={2022},
}

@inproceedings{
panwar2024incontext,
title={In-Context Learning through the Bayesian Prism},
author={Madhur Panwar and Kabir Ahuja and Navin Goyal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{merrill2024the,
title={The Expressive Power of Transformers with Chain of Thought},
author={William Merrill and Ashish Sabharwal},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
}

@inproceedings{prystawski2023think,
  title={Why think step by step? reasoning emerges from the locality of experience},
  author={Prystawski, Ben and Li, Michael Y and Goodman, Noah D},
  booktitle={Proceedings of the 37th International Conference on Neural Information Processing Systems},
  pages={70926--70947},
  year={2023}
}

@inproceedings{hou2023towards,
  title={Towards a Mechanistic Interpretation of Multi-Step Reasoning Capabilities of Language Models},
  author={Hou, Yifan and Li, Jiaoda and Fei, Yu and Stolfo, Alessandro and Zhou, Wangchunshu and Zeng, Guangtao and Bosselut, Antoine and Sachan, Mrinmaya},
  booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
  pages={4902--4919},
  year={2023}
}


@inproceedings{kim2023the,
title={The CoT Collection: Improving Zero-shot and Few-shot Learning of Language Models via Chain-of-Thought Fine-Tuning},
author={Seungone Kim and Se June Joo and Doyoung Kim and Joel Jang and Seonghyeon Ye and Jamin Shin and Minjoon Seo},
booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
year={2023},
}

@inproceedings{Vaswani2017AttentionIA,
  title={Attention is All you Need},
  author={Ashish Vaswani and Noam M. Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
  booktitle={Neural Information Processing Systems},
  year={2017},
}

@inproceedings{zhou2024mystery,
  title={The mystery of in-context learning: A comprehensive survey on interpretation and analysis},
  author={Zhou, Yuxiang and Li, Jiazheng and Xiang, Yanzheng and Yan, Hanqi and Gui, Lin and He, Yulan},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={14365--14378},
  year={2024}
}

@inproceedings{chu2024navigate,
  title={Navigate through enigmatic labyrinth a survey of chain of thought reasoning: Advances, frontiers and future},
  author={Chu, Zheng and Chen, Jingchang and Chen, Qianglong and Yu, Weijiang and He, Tao and Wang, Haotian and Peng, Weihua and Liu, Ming and Qin, Bing and Liu, Ting},
  booktitle={Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={1173--1203},
  year={2024}
}

@article{paszke2019pytorch,
  title={Pytorch: An imperative style, high-performance deep learning library},
  author={Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and others},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{zhu2013angles,
  title={Angles between subspaces and their tangents},
  author={Zhu, Peizhen and Knyazev, Andrew V},
  journal={Journal of Numerical Mathematics},
  volume={21},
  number={4},
  pages={325--340},
  year={2013},
  publisher={De Gruyter}
}

@inproceedings{
nichani2024how,
title={How Transformers Learn Causal Structure with Gradient Descent},
author={Eshaan Nichani and Alex Damian and Jason D. Lee},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
}

@inproceedings{
bietti2023birth,
title={Birth of a Transformer: A Memory Viewpoint},
author={Alberto Bietti and Vivien Cabannes and Diane Bouchacourt and Herve Jegou and Leon Bottou},
booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
year={2023},
}

@inproceedings{kwon2023efficient,
  title={Efficient memory management for large language model serving with pagedattention},
  author={Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
  booktitle={Proceedings of the 29th Symposium on Operating Systems Principles},
  pages={611--626},
  year={2023}
}

@inproceedings{
zheng2024sglang,
title={{SGL}ang: Efficient Execution of Structured Language Model Programs},
author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
}