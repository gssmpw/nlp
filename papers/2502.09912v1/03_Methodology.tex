\section{Methodology}
\label{methodology}
We conducted a between-subject experiment with 87 participants to test our hypotheses, assessing the perceived closeness among team members based on the experimental conditions and exploring other factors related to team behavior. 

\subsection{Participants}
The study was approved by the University of Notre Dame's Institutional Review Board (IRB) under protocol number 23-10-8142. We recruited participants through direct outreach by the research team, Notre Dame Psychology Department's SONA system (an online platform used to recruit participants for experiments), and social media channels. 


\subsection{Task Description}
All participants were randomly assigned to teams of three people to perform a collaborative task. Since we aimed to test preferences toward incumbents versus newcomers, we randomly asked two participants to meet first and complete an ice-breaker exercise (Appendix \ref{appendix:ice-breaker}). These two participants spent ten minutes getting to know each other. After that, the third member was introduced to the existing two participants, and the research assistant (RA) explained the task to them.

We asked the three participants to complete a task adapted from the ``hidden profile'' paradigm \cite{stasser1985pooling}, which requires all participants to review candidates and select the best one for a managerial position. The information about the candidates is deliberately distributed unequally among team members to create a ``hidden profile,'' where no single individual has all the necessary information to make the optimal choice. This task requires all team members to share their unique pieces of information and integrate them to identify the most suitable candidate. If only a few members shared information or dominated the group decision-making process, the team would not pick the best candidate. As such, this task can reveal biases in group decision-making, such as members dominating the conversation, low participation, and absence of a collective discussion. Hidden profile tasks have been employed in many laboratory experiments to test information sharing between group members when making decisions \cite{Goyal2014,mentis2009,mennecke1997using}. 

In our study, we requested participants to collectively decide on three candidates for the presidency of a university's new satellite campus. Each participant received a copy of the candidates' resumes, each with different information about the candidates' positive and negative attributes. Since each candidate's attributes varied in each resume's version, the participants needed to share and discuss the differing pieces of information provided in their respective copies. One candidate was appropriate to be hired if all participants shared their versions of the candidates' resumes. The task for this study was carefully designed to resemble real-world decision-making scenarios, such as hiring or promotions, and to make them relatable and meaningful to the participants. We instructed participants not to share the provided copies of the resumes with their teammates. The RA also monitored participants' progress by watching them through a video camera in the IP condition and observing the VR meeting room on a computer in the VR condition. 

We chose this task and the candidates' resumes based on three criteria: (a) that the task's content was currently relevant in organizational contexts, (b) that the task did not require extensive training to be completed, and (c) that the participants were likely to engage with the task since they will not know their teammates beforehand. The resumes are available in the Supplementary Materials.

 \begin{figure*}[!htb]
\centering
    \begin{subfigure}[b]{.48\textwidth}
        \includegraphics[trim={0 2.5cm 0 3cm},clip,width=\textwidth]{figures/InPerson.png}
        \Description{In-Person.}
        \caption{In-Person}
        \label{figure:in-person-session}
    \end{subfigure}\qquad
    \begin{subfigure}[b]{.47\textwidth}
        \includegraphics[width=\textwidth]{figures/VRSession.png}
        \Description{Virtual Reality (VR)}
        \caption{Virtual Reality (VR)}
        \label{figure:vr-session}
    \end{subfigure}
\caption{Experimental Conditions. On the left, participants are working in person. On the right, participants are working using Meta Horizon Workrooms. In both conditions, participants randomly sat in one of the chairs available.}
\Description{Experimental Conditions. On the left, participants are working on the task In-Person. On the right, participants are working on the task using Meta Horizon Workrooms. Participants were randomly assigned to a seat in both conditions.}
\label{fig:experimental-conditions}
\end{figure*} 

\subsection{Conditions}
The teams were randomly assigned to perform this task \textit{In-Person (IP)} or using \textit{Virtual Reality (VR)}. Meeting IP is the most natural communication medium and a ``gold standard'' to compare communication technology \cite{harrison2020framing}. We situated participants in an experimental room with a web camera to record their conversations. Two of the three participants were together in a physical room, while the third participant remained in a separate room. After the two participants completed the ice-breaker activity, the third participant joined them in the same room. Participants randomly sat in one of the three chairs. Each participant had a paper copy of their respective versions of the candidates' resumes, and we asked them not to display the papers to the other participants. All participants sat together at a table, with an omnidirectional microphone to record their discussion and final candidate choice (Figure \ref{figure:in-person-session}).

In the VR condition, all team members were located in different physical rooms with a Meta Oculus Quest 2 headset. The devices had activated hand tracking, enabling users to interact with the application directly using their hands. The RA helped participants wear the headset and learn to control the interface. They had five minutes to create their virtual avatars, and we instructed them to create avatars that looked similar to themselves. After the participants created their avatars, the RA started a session on \textit{Meta Horizon Workrooms} and placed the first two participants in a group workspace to complete the icebreaker exercise. At the same time, the third participant waited for them in a private virtual room. The RA added the third participant to the workspace after 10 minutes. In the virtual meeting room, each participant had a virtual monitor in front of them with their respective versions of the candidates' resumes. Participants could not see each other's monitors. They sat together at a virtual table, and the application randomly assigned them to a seat. Participants could use body movements and hands while talking to others. We cast the group workspace (Figure \ref{figure:vr-session}) and recorded audio of participants' conversations to analyze their decisions and identify their final candidate choice. 

We selected Meta Horizon Workrooms for this experiment for three reasons. First, it provided an accessible, high-quality virtual workspace that simulates real-world collaboration scenarios with multiple concurrent users and low latency. Second, it could connect computer monitors to each participant's headset using the Remote Desktop feature, allowing us to display the three different versions of the candidates' resumes. Third, this application enabled users to create their avatars quickly using Meta Avatars.   

\subsection{Procedure}
Participants attended our experimental sessions at our research laboratory. An RA was responsible for conducting the experiment and guiding the participants. At the beginning of the session, the RA explained the purpose of the study to the participants and answered their questions. The RAs emphasized that their participation was voluntary, that they were compensated, and that their responses were confidential. This stage lasted five minutes. The RAs provided a consent form to the participants and gave them time to read and ask questions. All participants had to consent to release their collected data to participate in this study; otherwise, the session would not have been conducted. We compensated participants recruited from the SONA pool with extra credit, while participants recruited through direct outreach and social media were compensated with \$20 electronic gift cards. Each session lasted 30 minutes on average.

The RA guided the participants to the check-in stations and assigned them nicknames so they could be recognized later for the surveys. The incumbents were nicknamed ``Cat'' and ``Dog,'' while the newcomer's nickname was ``Mouse.'' Each station had a label with the assigned nickname.

Participants completed a pre-treatment survey on Qualtrics, which included questions about their demographics (i.e., gender, ethnicity, race, and age), activity information (i.e., highest education level achieved, and current employment status), social self-efficacy (i.e., expectations to collaborate and form relationships while working together on a task), computational proficiency (i.e., how comfortable participants were using computers, phones, and VR). Since the study was conducted in the U.S., we also asked whether participants identified as Hispanic/Latino. Additionally, we ensured that participants did not know each other before the experiment. Participants sat at separate desks, divided by portable curtains, to prevent them from seeing each other's responses.

After completing the initial survey, the RA led the participants to the rooms according to the experimental conditions. Two participants completed the ice-breaker task while a third waited. After the ice-breaker task was completed, the third participant joined the other two participants for the hidden-profile task. We asked participants to introduce themselves again before starting the task. We gave the participants ten minutes to complete the task and select one candidate. 

The RA returned to the participants after the task was completed and relocated them to the check-in stations. Each participant completed a post-treatment survey on Qualtrics to assess their experiences working with the team. The participants had to evaluate their experience with the team and their teammates. To ensure that the participants recognized the correct team member in each question, we framed the questions about the incumbent as \textit{``For the partner you met first...''} and the ones regarding the newcomer as \textit{``For the partner you met later...''}. The third member had a different version of the survey and had to answer the questions based on the incumbents' nicknames. After participants had completed this final survey, RAs confirmed their completion, provided compensation, and conducted a brief debriefing of the experiment.

\subsection{Measurements}
We validated participants' responses to ensure they were reliable by calculating Cronbach's ($\alpha$) alpha. This test provided values above 0.75 for all scales, confirming their internal validity and allowing us to move on to the next steps of our data analysis. We provide the citations, reliability scores, and example items in Table \ref{tab:final_survey}.

\subsubsection{Dependent Variables}
\paragraph{Closeness}
We asked participants how closely they perceived each team member in their respective environment. Participants evaluated their closeness with each team member using a 7-point Likert scale using the items tested by \cite{Gachter2015}, which included questions from the `Inclusion of the Other in the Self' (IOS) Scale, the `Social Connectedness Scale,' and the `We Scale.' After checking that the items showed high-reliability levels ($\alpha=.90$), we averaged these items into a single score per participant. We defined this average as the closeness score ($C_{i \rightarrow j}$) to assess participant $i$'s closeness feelings to participant $j$. This score measured how close the participants felt to one another after completing the task.

\paragraph{Incumbents' Familiarity Bias}
Using participants' assessments of their relationships, we measured how different each incumbent felt connected with their incumbent compared to their newcomer. We operationalized the familiarity bias of each incumbent $i_1$ as the difference between their perceived closeness to the other incumbent $i_2$ (i.e., $C_{i_1 \rightarrow i_2}$) and their perceived closeness to the newcomer $n$ (i.e., $C_{i_1 \rightarrow n}$). This score ranged from -1 (i.e., feeling connected with the newcomer only) to 1 (i.e., feeling connected with the incumbent only). To normalize these closeness scores between participants and account for individual differences in rating scales, we calculated the relative difference from the absolute change between the closeness to $i_2$ and the closeness to $n$, and divided by the closeness to $i_2$ (Equation \ref{eq:familiarity-bias}). We calculated this familiarity score for each incumbent, giving us two scores per team (i.e., $i_1 \rightarrow i_2$ and $i_2 \rightarrow i_1$). 

%($FB_{t}$)
\begin{equation}
FB_{i_1} = \frac{\left(C_{i_{1} \rightarrow i_{2}} - C_{i_{1} \rightarrow n}\right)}{C_{i_{1} \rightarrow i_{2}}}
\label{eq:familiarity-bias}
%\caption{Familiarity bias score for the incumbent $i_1$.}
\end{equation}

\subsubsection{Independent Variables}
%Our independent variables aimed to measure the effect of the experimental conditions, participants' perceptions of each other, their communication, and support provided by the environment on the dependent variables. 

\paragraph{Experimental Conditions} We used a dummy variable to represent the experimental condition, where zero represented the In-Person sessions, and 1 represented the VR sessions. 

\paragraph{Perceived Similarity} We asked participants how similar they perceived themselves to their teammates. We used the items from \cite{brucks2022virtual}, including questions about their similarities with each member. 

\paragraph{Gender Homophily} We included a `same-gender' variable to verify that participants felt closer to each other because of gender homophily. 

\paragraph{Support provided by the Environment} To assess to what extent the environment supported participants to work on the task, we adapted questions from the \textit{Work Environment Satisfaction} scale \cite{tenorio2020syncmeet}. This scale includes questions measuring how participants feel about working in each environment, such as ``I could utilize all my skills and abilities to solve this task in this environment.''

\paragraph{Communication Metrics} Participants assessed how effective the communication with their partner was using the \textit{Communication Effectiveness Index} (CETI) \cite{lomas1989communicative}. This scale evaluates participants' ability to pay attention, communicate, respond, and interact. We also measured participants' communication using the \textit{Information Exchange} scale \cite{subramaniam2005influence}. This scale presents three statements asking whether the partner and the participant shared information, if they learned from each other, and if they exchanged ideas. 

\paragraph{Usability}
For the VR participants, we verified that using the headset and the workplace application was not an impediment to working on the collaborative task. Participants evaluated the system's usability using the SUS scale \cite{brooke1996sus}, consisting of 10 Likert-scale items with responses ranging from 1 (``Strongly disagree'') to 5 (``Strongly Agree''). SUS scores are calculated on a scale from 0 to 100, with higher scores indicating better usability.

\input{tables/final_survey}

\subsubsection{Open-ended Questions}
Lastly, we included questions in the final survey to know more about the participants' impression of their teammates and working with them. We asked them, `Do you feel closer to one partner than another?' and `What were the most significant obstacles to relating with your partners?'

\subsection{Quantitative Analysis}
Using the participants' responses, we conducted the following statistical analyses using R 4.4.0 \cite{R2024}.  We first conducted Welch $t$-tests to check significant differences between participants' closeness scores based on the experimental conditions and relationships. We used this test since the samples in the experimental conditions had different sizes and variances. We also conducted a two-way ANOVA to test whether the experimental condition and the type of relationship (i.e., ``incumbent $\rightarrow$ incumbent,'' ``incumbent $\rightarrow$ newcomer,'' ``newcomer $\rightarrow$ incumbent'') had an effect on their perceived closeness. 

To test whether VR had a significant effect on incumbents' closeness to newcomers (H1) and newcomers' closeness to incumbents (H2), we conducted a mixed-effects linear regression analysis to estimate the factors that most explained their perceived closeness. Each observation was the evaluation of a participant on another. We modeled the sessions as random effects since each session had three participants. We used their reported closeness scores ($C_{i}$) as the variable to estimate and added the independent variables to our model. We included two interaction terms to verify whether the type of relationship moderated the effect of the experimental condition on perceived closeness. We used the package \texttt{lme4} \cite{Bates2015} to create these mixed-effects models.

We then conducted a Welch \textit{t}-test to check whether incumbents' familiarity bias differed significantly based on the conditions (H3). We also conducted a mixed-effects linear regression model to estimate the factors that most explained these biases and used sessions as random effects since each session had two incumbents. Given that the familiarity bias was operationalized between incumbents and newcomers, we also computed the relative change between the communication and perceived similarities among them.

Lastly, we conducted mixed-effects mediation models to examine whether participants' perceived similarity influenced the effect of using VR on perceived closeness (H4). We created different models to account for all the observations, as well as the directionality of the relationships (i.e., newcomers to incumbents, and incumbents to newcomers). We also modeled sessions as random effects since each participant evaluated two teammates. We created these models using the \texttt{mediation} package \cite{Tingley2014}, which allows moderation models with nested data structures.

For all the mixed-effects regression models, we verified the normality assumptions of these models and checked that multicollinearity was not an issue by calculating the models' covariates Variance Inflation Factors (VIF).

\subsection{Qualitative Analysis}
To gain deeper insights into participants' experiences, we conducted a qualitative analysis of open-ended survey responses, capturing their perspectives on challenges faced during the experiment and perceptions of newcomers and incumbents.

Using an iterative inductive approach \cite{saldana2021coding}, the first author conducted initial open coding of 10 responses (five incumbents and five newcomers), identifying 40 codes. Additional coding by the second and third authors expanded the code set, which was later consolidated into a 20-code codebook through affinity mapping \cite{nielsen2024affinity}. After coding 43 more responses collaboratively, the codebook was refined to 14 codes during team discussions. The finalized codebook was then shared among four researchers, who independently coded overlapping batches of responses (24 by the first researcher and 21 by each of the others). Collaborative discussions reconciled differences and highlighted key insights, with responses addressing multiple non-mutually exclusive codes.

The first and last authors conducted a thematic analysis \cite{braun2006using}, organizing codes into overarching themes and compiling relevant data for each. Drawing on the studies covered in Section \ref{literature_review}, they employed a deductive coding approach, iteratively refining themes over three collaborative meetings. This process clarified theme definitions and resulted in four main themes that provided insights into the experiences of both newcomers and incumbents.






 



