@ARTICLE{Ayers2023-fe,
  title    = "Comparing physician and artificial intelligence chatbot responses
              to patient questions posted to a public social media forum",
  author   = "Ayers, John W and Poliak, Adam and Dredze, Mark and Leas, Eric C
              and Zhu, Zechariah and Kelley, Jessica B and Faix, Dennis J and
              Goodman, Aaron M and Longhurst, Christopher A and Hogarth,
              Michael and Smith, Davey M",
  abstract = "Importance: The rapid expansion of virtual health care has caused
              a surge in patient messages concomitant with more work and
              burnout among health care professionals. Artificial intelligence
              (AI) assistants could potentially aid in creating answers to
              patient questions by drafting responses that could be reviewed by
              clinicians. Objective: To evaluate the ability of an AI chatbot
              assistant (ChatGPT), released in November 2022, to provide
              quality and empathetic responses to patient questions. Design,
              Setting, and Participants: In this cross-sectional study, a
              public and nonidentifiable database of questions from a public
              social media forum (Reddit's r/AskDocs) was used to randomly draw
              195 exchanges from October 2022 where a verified physician
              responded to a public question. Chatbot responses were generated
              by entering the original question into a fresh session (without
              prior questions having been asked in the session) on December 22
              and 23, 2022. The original question along with anonymized and
              randomly ordered physician and chatbot responses were evaluated
              in triplicate by a team of licensed health care professionals.
              Evaluators chose ``which response was better'' and judged both
              ``the quality of information provided'' (very poor, poor,
              acceptable, good, or very good) and ``the empathy or bedside
              manner provided'' (not empathetic, slightly empathetic,
              moderately empathetic, empathetic, and very empathetic). Mean
              outcomes were ordered on a 1 to 5 scale and compared between
              chatbot and physicians. Results: Of the 195 questions and
              responses, evaluators preferred chatbot responses to physician
              responses in 78.6\% (95\% CI, 75.0\%-81.8\%) of the 585
              evaluations. Mean (IQR) physician responses were significantly
              shorter than chatbot responses (52 [17-62] words vs 211 [168-245]
              words; t = 25.4; P < .001). Chatbot responses were rated of
              significantly higher quality than physician responses (t = 13.3;
              P < .001). The proportion of responses rated as good or very good
              quality ($\geq$ 4), for instance, was higher for chatbot than
              physicians (chatbot: 78.5\%, 95\% CI, 72.3\%-84.1\%; physicians:
              22.1\%, 95\% CI, 16.4\%-28.2\%;). This amounted to 3.6 times
              higher prevalence of good or very good quality responses for the
              chatbot. Chatbot responses were also rated significantly more
              empathetic than physician responses (t = 18.9; P < .001). The
              proportion of responses rated empathetic or very empathetic
              ($\geq$4) was higher for chatbot than for physicians (physicians:
              4.6\%, 95\% CI, 2.1\%-7.7\%; chatbot: 45.1\%, 95\% CI,
              38.5\%-51.8\%; physicians: 4.6\%, 95\% CI, 2.1\%-7.7\%). This
              amounted to 9.8 times higher prevalence of empathetic or very
              empathetic responses for the chatbot. Conclusions: In this
              cross-sectional study, a chatbot generated quality and empathetic
              responses to patient questions posed in an online forum. Further
              exploration of this technology is warranted in clinical settings,
              such as using chatbot to draft responses that physicians could
              then edit. Randomized trials could assess further if using AI
              assistants might improve responses, lower clinician burnout, and
              improve patient outcomes.",
  journal  = "JAMA Intern. Med.",
  volume   =  183,
  number   =  6,
  pages    = "589--596",
  month    =  jun,
  year     =  2023,
  language = "en",
  doi="10.1001/jamainternmed.2023.1838",
}

@ARTICLE{Brynjolfsson2025-ne,
  title     = "Generative {AI} at work",
  author    = "Brynjolfsson, Erik and Li, Danielle and Raymond, Lindsey",
  abstract  = "Abstract We study the staggered introduction of a generative
               AI--based conversational assistant using data from 5,172
               customer-support agents. Access to AI assistance increases
               worker productivity, as measured by issues resolved per hour, by
               15\% on average, with substantial heterogeneity across workers.
               The effects vary significantly across different agents. Less
               experienced and lower-skilled workers improve both the speed and
               quality of their output, while the most experienced and
               highest-skilled workers see small gains in speed and small
               declines in quality. We also find evidence that AI assistance
               facilitates worker learning and improves English fluency,
               particularly among international agents. While AI systems
               improve with more training data, we find that the gains from AI
               adoption are largest for moderately rare problems, where human
               agents have less baseline experience but the system still has
               adequate training data. Finally, we provide evidence that AI
               assistance improves the experience of work along several
               dimensions: customers are more polite and less likely to ask to
               speak to a manager.",
  journal   = "Q. J. Econ.",
  publisher = "Oxford University Press (OUP)",
  month     =  feb,
  year      =  2025,
  language  = "en",
  doi = "10.1093/qje/qjae044"
}

@ARTICLE{Escalante2023-jp,
  title     = "{AI-generated} feedback on writing: insights into efficacy and
               {ENL} student preference",
  author    = "Escalante, Juan and Pack, Austin and Barrett, Alex",
  abstract  = "AbstractThe question of how generative AI tools, such as large
               language models and chatbots, can be leveraged ethically and
               effectively in education is ongoing. Given the critical role
               that writing plays in learning and assessment within educational
               institutions, it is of growing importance for educators to make
               thoughtful and informed decisions as to how and in what capacity
               generative AI tools should be leveraged to assist in the
               development of students' writing skills. This paper reports on
               two longitudinal studies. Study 1 examined learning outcomes of
               48 university English as a new language (ENL) learners in a
               six-week long repeated measures quasi experimental design where
               the experimental group received writing feedback generated from
               ChatGPT (GPT-4) and the control group received feedback from
               their human tutor. Study 2 analyzed the perceptions of a
               different group of 43 ENLs who received feedback from both
               ChatGPT and their tutor. Results of study 1 showed no difference
               in learning outcomes between the two groups. Study 2 results
               revealed a near even split in preference for AI-generated or
               human-generated feedback, with clear advantages to both forms of
               feedback apparent from the data. The main implication of these
               studies is that the use of AI-generated feedback can likely be
               incorporated into ENL essay evaluation without affecting
               learning outcomes, although we recommend a blended approach that
               utilizes the strengths of both forms of feedback. The main
               contribution of this paper is in addressing generative AI as an
               automatic essay evaluator while incorporating learner
               perspectives.",
  journal   = "Int. J. Educ. Technol. High. Educ.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  20,
  number    =  1,
  month     =  oct,
  year      =  2023,
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en",
  doi = "10.1186/s41239-023-00425-2"
}

@ARTICLE{Hayawi2024-ri,
  title     = "The imitation game: Detecting human and {AI-generated} texts in
               the era of {ChatGPT} and {BARD}",
  author    = "Hayawi, Kadhim and Shahriar, Sakib and Mathew, Sujith Samuel",
  abstract  = "The potential of artificial intelligence (AI)-based large
               language models (LLMs) holds considerable promise in
               revolutionising education, research and practice. However,
               distinguishing between human-written and AI-generated text has
               become a significant task. This article presents a comparative
               study, introducing a novel dataset of human-written and
               LLM-generated texts in different genres: essays, stories, poetry
               and Python code. We employ several machine learning models to
               classify the texts. Results demonstrate the efficacy of these
               models in discerning between human and AI-generated text,
               despite the dataset's limited sample size. However, the task
               becomes more challenging when classifying GPT-generated text,
               particularly in story writing. The results indicate that the
               models exhibit superior performance in binary classification
               tasks, such as distinguishing human-generated text from a
               specific LLM, compared with the more complex multiclass tasks
               that involve discerning among human-generated and multiple LLMs.
               Our findings provide insightful implications for AI text
               detection, while our dataset paves the way for future research
               in this evolving area.",
  journal   = "J. Inf. Sci.",
  publisher = "SAGE Publications",
  month     =  feb,
  year      =  2024,
  language  = "en",
  doi = "10.1177/01655515241227531"
}

@ARTICLE{Karinshak2023-fg,
  title     = "Working with {AI} to persuade: Examining a large language
               model's ability to generate pro-vaccination messages",
  author    = "Karinshak, Elise and Liu, Sunny Xun and Park, Joon Sung and
               Hancock, Jeffrey T",
  abstract  = "Artificial Intelligence (AI) is a transformative force in
               communication and messaging strategy, with potential to disrupt
               traditional approaches. Large language models (LLMs), a form of
               AI, are capable of generating high-quality, humanlike text. We
               investigate the persuasive quality of AI-generated messages to
               understand how AI could impact public health messaging.
               Specifically, through a series of studies designed to
               characterize and evaluate generative AI in developing public
               health messages, we analyze COVID-19 pro-vaccination messages
               generated by GPT-3, a state-of-the-art instantiation of a large
               language model. Study 1 is a systematic evaluation of GPT-3's
               ability to generate pro-vaccination messages. Study 2 then
               observed peoples' perceptions of curated GPT-3-generated
               messages compared to human-authored messages released by the CDC
               (Centers for Disease Control and Prevention), finding that GPT-3
               messages were perceived as more effective, stronger arguments,
               and evoked more positive attitudes than CDC messages. Finally,
               Study 3 assessed the role of source labels on perceived quality,
               finding that while participants preferred AI-generated messages,
               they expressed dispreference for messages that were labeled as
               AI-generated. The results suggest that, with human supervision,
               AI can be used to create effective public health messages, but
               that individuals prefer their public health messages to come
               from human institutions rather than AI sources. We propose best
               practices for assessing generative outputs of large language
               models in future social science research and ways health
               professionals can use AI systems to augment public health
               messaging.",
  journal   = "Proc. ACM Hum. Comput. Interact.",
  publisher = "Association for Computing Machinery (ACM)",
  volume    =  7,
  number    = "CSCW1",
  pages     = "1--29",
  month     =  apr,
  year      =  2023,
  language  = "en",
  doi = "10.1145/3579592"
}

@ARTICLE{Munoz-Ortiz2024-hp,
  title     = "Contrasting linguistic patterns in human and {LLM-generated}
               news text",
  author    = "Mu{\~n}oz-Ortiz, Alberto and G{\'o}mez-Rodr{\'\i}guez, Carlos
               and Vilares, David",
  abstract  = "We conduct a quantitative analysis contrasting human-written
               English news text with comparable large language model (LLM)
               output from six different LLMs that cover three different
               families and four sizes in total. Our analysis spans several
               measurable linguistic dimensions, including morphological,
               syntactic, psychometric, and sociolinguistic aspects. The
               results reveal various measurable differences between human and
               AI-generated texts. Human texts exhibit more scattered sentence
               length distributions, more variety of vocabulary, a distinct use
               of dependency and constituent types, shorter constituents, and
               more optimized dependency distances. Humans tend to exhibit
               stronger negative emotions (such as fear and disgust) and less
               joy compared to text generated by LLMs, with the toxicity of
               these models increasing as their size grows. LLM outputs use
               more numbers, symbols and auxiliaries (suggesting objective
               language) than human texts, as well as more pronouns. The sexist
               bias prevalent in human text is also expressed by LLMs, and even
               magnified in all of them but one. Differences between LLMs and
               humans are larger than between LLMs.",
  journal   = "Artif. Intell. Rev.",
  publisher = "Springer Science and Business Media LLC",
  volume    =  57,
  number    =  10,
  pages     = "265",
  month     =  aug,
  year      =  2024,
  keywords  = "Computational linguistics; Large language models; Linguistic
               biases; Machine-generated text",
  copyright = "https://creativecommons.org/licenses/by/4.0",
  language  = "en",
  doi = "10.1007/s10462-024-10903-2"
}

@ARTICLE{Nitu2024-fx,
  title     = "Beyond lexical boundaries: {LLM-generated} text detection for
               Romanian digital libraries",
  author    = "Nitu, Melania and Dascalu, Mihai",
  abstract  = "Machine-generated content reshapes the landscape of digital
               information; hence, ensuring the authenticity of texts within
               digital libraries has become a paramount concern. This work
               introduces a corpus of approximately 60 k Romanian documents,
               including human-written samples as well as generated texts using
               six distinct Large Language Models (LLMs) and three different
               generation methods. Our robust experimental dataset covers five
               domains, namely books, news, legal, medical, and scientific
               publications. The exploratory text analysis revealed differences
               between human-authored and artificially generated texts,
               exposing the intricacies of lexical diversity and textual
               complexity. Since Romanian is a less-resourced language
               requiring dedicated detectors on which out-of-the-box solutions
               do not work, this paper introduces two techniques for discerning
               machine-generated texts. The first method leverages a
               Transformer-based model to categorize texts as human or
               machine-generated, while the second method extracts and examines
               linguistic features, such as identifying the top textual
               complexity indices via Kruskal--Wallis mean rank and computes
               burstiness, which are further fed into a machine-learning model
               leveraging an extreme gradient-boosting decision tree. The
               methods show competitive performance, with the first technique's
               results outperforming the second one in two out of five domains,
               reaching an F1 score of 0.96. Our study also includes a text
               similarity analysis between human-authored and artificially
               generated texts, coupled with a SHAP analysis to understand
               which linguistic features contribute more to the classifier's
               decision.",
  journal   = "Future Internet",
  publisher = "MDPI AG",
  volume    =  16,
  number    =  2,
  pages     = "41",
  month     =  jan,
  year      =  2024,
  copyright = "https://creativecommons.org/licenses/by/4.0/",
  language  = "en",
  doi = "10.3390/fi16020041"
}

@ARTICLE{Zhang2023-jz,
  title     = "Human favoritism, not {AI} aversion: People's perceptions (and
               bias) toward generative {AI}, human experts, and {human--GAI}
               collaboration in persuasive content generation",
  author    = "Zhang, Yunhao and Gosline, Ren{\'e}e",
  abstract  = "Abstract With the wide availability of large language models and
               generative AI, there are four primary paradigms for human--AI
               collaboration: human-only, AI-only (ChatGPT-4), augmented human
               (where a human makes the final decision with AI output as a
               reference), or augmented AI (where the AI makes the final
               decision with human output as a reference). In partnership with
               one of the world's leading consulting firms, we enlisted
               professional content creators and ChatGPT-4 to create
               advertising content for products and persuasive content for
               campaigns following the aforementioned paradigms. First, we find
               that, contrary to the expectations of some of the existing
               algorithm aversion literature on conventional predictive AI, the
               content generated by generative AI and augmented AI is perceived
               as of higher quality than that produced by human experts and
               augmented human experts. Second, revealing the source of content
               production reduces---but does not reverse---the perceived
               quality gap between human- and AI-generated content. This bias
               in evaluation is predominantly driven by human favoritism rather
               than AI aversion: Knowing that the same content is created by a
               human expert increases its (reported) perceived quality, but
               knowing that AI is involved in the creation process does not
               affect its perceived quality. Further analysis suggests this
               bias is not due to a `quality prime' as knowing the content they
               are about to evaluate comes from competent creators (e.g.,
               industry professionals and state-of-the-art AI) without knowing
               exactly that the creator of each piece of content does not
               increase participants' perceived quality.",
  journal   = "Judgm. Decis. Mak.",
  publisher = "Cambridge University Press (CUP)",
  volume    =  18,
  number    = "e41",
  year      =  2023,
  language  = "en",
  doi = "10.1017/jdm.2023.37"
}

@ARTICLE{Ollivier2023-hx,
  title   = "A deeper dive into {ChatGPT}: history, use and future perspectives
             for orthopaedic research",
  author  = "Ollivier, Matthieu and Pareek, Ayoosh and Dahmen, Jari and Kayaalp, Enes and
             Winkler, Phillip  and Hirschmann, Michael and Karlsson, Jon",
  journal = "Knee Surgery, Sports Traumatology, Arthroscopy",
  volume  =  31,
  number  =  4,
  pages   = "1190--1192",
  year    =  2023,
  doi = "10.1007/s00167-023-07372-5"
}

@ARTICLE{Shi2024-fr,
  title   = "Red taming language model detectors with language models",
  author  = "Shi, Zhouxing and Wang, Yihan and Yin, Fan and Chen, Xiangning and Chang, Kai-Wei and
             Hsieh, Cho-Jui",
  journal = "Transactions of the Association for Computational Linguistics",
  volume  =  12,
  pages   = "174--189",
  year    =  2024,
  doi = "10.1162/tacl_a_00639"
}

@ARTICLE{Elkhatat2023-an,
  title   = "Evaluating the efficacy of {AI} content detection tools in
             differentiating between human and {AI-generated} text",
  author  = "Elkhatat, Ahmed and Elsaid, Khaled and Almeer, Saeed",
  journal = "International Journal for Educational Integrity",
  volume  =  19,
  number  =  1,
  year    =  2023,
  doi = "10.1007/s40979-023-00140-5"
}

@ARTICLE{Weber-Wulff2023-lo,
  title   = "Testing of detection tools for {AI-generated} text",
  author  = "Weber-Wulff, Debora and Anohina-Naumeca, Alla and Bjelobaba, Sonja and
             Folt{\'y}nek, Tom\'{a}\v{s} and Guerrero-Dib, Jean and Popoola, Olumide and \v{S}igut, Petr and Waddington, Lorna",
  journal = "International Journal for Educational Integrity",
  volume  =  19,
  number  =  1,
  year    =  2023,
  doi = "10.1007/s40979-023-00146-z"
}

@ARTICLE{Menz2024-jp,
  title   = "Gender Representation of Health Care Professionals in Large
             Language {Model-Generated} Stories",
  author  = "Menz, Bradley and Pharm (Hons), B. and Kuderer, Nicole and Chin-Yee, Benjamin and Logan, Jessica and Rowland, Andrew and Sorich, Michael and Hopkins, Ashley",
  journal = "JAMA network open",
  volume  =  7,
  number  =  9,
  pages   = "e2434997--e2434997",
  year    =  2024,
  doi = "10.1001/jamanetworkopen.2024.34997"
}

@ARTICLE{Cevik2024-jd,
  title   = "Assessment of the bias of artificial intelligence generated images
             and large language models on their depiction of a surgeon",
  author  = "Cevik, Jevan and Lim, Bryan and Seth, Ishith and Sofiadellis, Foti and Ross, Richard J and Cuomo, Roberto and Rozen, Warren M and others",
  journal = "ANZ Journal of Surgery",
  volume  =  94,
  number  =  3,
  pages   = "287--294",
  year    =  2024,
  doi = "10.1111/ans.18792"
}

@ARTICLE{Motoki2024-iq,
  title   = "More human than human: measuring {ChatGPT} political bias",
  author  = "Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor",
  journal = "Public Choice",
  volume  =  198,
  number  =  1,
  pages   = "3--23",
  year    =  2024,
  doi = "10.1007/s11127-023-01097-2"
}

@article{Rozado2024-nv,
  title={The political preferences of llms},
  author={Rozado, David},
  journal={arXiv preprint arXiv:2402.01789},
  year={2024}
}

@ARTICLE{Acerbi2023-xw,
  title   = "Large language models show human-like content biases in
             transmission chain experiments",
  author  = "Acerbi, Alberto and Stubbersfield, Joseph M",
  journal = "Proceedings of the National Academy of Sciences",
  volume  =  120,
  number  =  44,
  year    =  2023,
  doi = "10.1073/pnas.2313790120"
}

@ARTICLE{Fang2024-oe,
  title   = "Bias of {AI-generated} content: an examination of news produced by
             large language models",
  author  = "Fang, Xiao and Che, Shangkun and Mao, Minjia and Zhang, Hongzhe and Zhao, Ming and Zhao, Xiaohang",
  journal = "Scientific Reports",
  volume  =  14,
  number  =  1,
  year    =  2024,
  doi = "10.1038/s41598-024-55686-2"
}

@ARTICLE{Oleary2024-ak,
  title   = "Do large language models bias human evaluations?",
  author  = "O’Leary, Daniel E",
  journal = "IEEE Intelligent Systems",
  volume  =  39,
  pages   = "83--87",
  year    =  2024,
  doi = "10.1109/MIS.2024.3415208"
}

@ARTICLE{Boutadjine2024-oz,
  title   = "Human vs. Machine: A Comparative Study on the Detection of
             {AI-Generated} Content",
  author  = "Boutadjine, Amal and Harrag, Fouzi and Shaalan, Khaled",
  journal = "ACM Transactions on Asian and Low-Resource Language Information
             Processing",
  year    =  2024,
  doi = "10.1145/3708889"
}

@ARTICLE{Vaccaro2024-oy,
  title   = "When combinations of humans and {AI} are useful: A systematic
             review and meta-analysis",
  author  = "Vaccaro, Michelle and Almaatouq, Abdullah and Malone, Thomas",
  journal = "Nature Human Behaviour",
  pages   = "1--11",
  year    =  2024,
  doi = "10.1038/s41562-024-02024-1"
}

@ARTICLE{Chen2024-qy,
  title   = "Consumer attitudes toward {AI-generated} ads: Appeal types,
             self-efficacy and {AI's} social role",
  author  = "Chen, Yaqi and Wang, Haizhong and Hill, Sally Rao and Li, Binglian",
  journal = "Journal of Business Research",
  volume  =  185,
  year    =  2024,
  doi = "10.1016/j.jbusres.2024.114867"
}

@article{costello2024durably,
  title="Durably reducing conspiracy beliefs through dialogues with AI",
  author="Costello, Thomas H and Pennycook, Gordon and Rand, David G",
  journal="Science",
  volume=385,
  number=6714,
  pages="eadq1814",
  year=2024,
  publisher="American Association for the Advancement of Science",
  doi="10.1126/science.adq1814"
}

@article{sedgwick2015understanding,
  title="Understanding the Hawthorne effect",
  author="Sedgwick, Philip and Greenwood, Nan",
  journal="Bmj",
  volume=351,
  year=2015,
  publisher="British Medical Journal Publishing Group",
  doi="10.1136/bmj.h4672"
}