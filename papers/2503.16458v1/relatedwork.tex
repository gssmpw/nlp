\section{Literature review}
\subsection{Human and LLM generated texts}
Recent research has systematically examined the distinctions between human-generated and AI-generated texts, revealing measurable differences in sentence structure, emotion expression, and other linguistic features \citep{Munoz-Ortiz2024-hp,Nitu2024-fx}. Early studies demonstrated that traditional machine learning classifiers could effectively differentiate between human and AI-generated content; however, the advent of advanced large language models (LLMs) has significantly complicated this task \citep{Hayawi2024-ri}. In fact, AI-generated texts have at times matched or even exceeded human-written texts in specific applications, such as persuasive messaging \citep{Karinshak2023-fg} and providing writing feedback in education \citep{Escalante2023-jp}. 
The increasing sophistication of LLMs has led to a significant convergence between AI-generated text and human-written content, rendering the distinction between the two increasingly challenging \citep{Ollivier2023-hx, Hayawi2024-ri}. As these models evolve, they produce text that not only mimics human writing styles but also adheres to the nuances of language, context, and coherence that characterize authentic human communication. Evaluations of existing LLM-generated text detectors have reported inconsistencies \citep{Weber-Wulff2023-lo} and high false positive rates when these systems are applied to human-authored texts \citep{Elkhatat2023-an}.
In addition, the objectivity of AI-generated content is also questionable. The literature has revealed inherent biases within outputs produced by LLMs. Studies have documented significant gender and racial biases, notably in depictions of healthcare professionals and surgeons, where male representations are frequently favored \citep{Menz2024-jp, Cevik2024-jd}. Political bias has also been observed, with certain platforms such as ChatGPT exhibiting a tendency toward left-leaning perspectives \citep{Motoki2024-iq, Rozado2024-nv}. Moreover, LLMs tend to manifest human-like content biases, as demonstrated by transmission experiments \citep{Acerbi2023-xw} and linguistic analyses \citep{Fang2024-oe}. 

\subsection{Perception of LLM-generated content}
Comparative studies reveal that human evaluators often struggle to reliably differentiate between AI-generated and human-authored content \citep{Boutadjine2024-oz}. \citet{Zhang2023-jz} demonstrated that generative and augmented AI content is frequently perceived as superior to that produced by human experts, even when humans utilize AI tools. However, disclosing the source of content narrows the perceived quality gap, suggesting a bias favoring human contributions over AI. Participants rated content more favorably when attributed to human experts, whereas awareness of AI involvement had minimal impact on perceptions. \citet{Ayers2023-fe} examined the ability of an AI chatbot (ChatGPT) to deliver quality and empathetic responses to patient questions compared to physicians. Their findings revealed that chatbot responses were preferred in the majority of evaluations, rated higher in quality, and deemed more empathetic than those of physicians. Notably, chatbot-generated texts were also significantly longer than physician responses. \citet{Karinshak2023-fg} highlighted that large language models (LLMs), particularly GPT-3, can produce high-quality persuasive content; however, individuals tend to prefer public health messages originating from human institutions rather than AI sources. 
Some studies highlight the nuanced perceptions and preferences surrounding AI-generated content across various domains. For example, \citet{Escalante2023-jp} compared human tutor feedback with AI-generated feedback in educational settings, revealing mixed results. While face-to-face interactions with tutors enhanced student engagement, AI-generated feedback was favored for its clarity and specificity. The research by \citet{Chen2024-qy} demonstrates that consumers prefer AI-generated ads with agentic appeals, while favoring human-created ads with communal appeals. 
These findings underscore that contextual factors and the awareness of text origin play a critical role in shaping user preferences.