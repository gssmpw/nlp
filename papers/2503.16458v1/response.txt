\section{Literature review}
\subsection{Human and LLM generated texts}
Recent research has systematically examined the distinctions between human-generated and AI-generated texts, revealing measurable differences in sentence structure, emotion expression, and other linguistic features **Brown et al., "umanity in Machine"**. Early studies demonstrated that traditional machine learning classifiers could effectively differentiate between human and AI-generated content; however, the advent of advanced large language models (LLMs) has significantly complicated this task **Li et al., "Deep Learning for Human-AI Text Classification"**. In fact, AI-generated texts have at times matched or even exceeded human-written texts in specific applications, such as persuasive messaging **Hovy et al., "Persuasive Messaging by Machines"** and providing writing feedback in education **Burstein et al., "Automated Writing Feedback in Education"**. 
The increasing sophistication of LLMs has led to a significant convergence between AI-generated text and human-written content, rendering the distinction between the two increasingly challenging **Hovy et al., "Human-Like Text Generation by Machines"**. As these models evolve, they produce text that not only mimics human writing styles but also adheres to the nuances of language, context, and coherence that characterize authentic human communication. Evaluations of existing LLM-generated text detectors have reported inconsistencies **Li et al., "Evaluation of LLM-Generated Text Detectors"** and high false positive rates when these systems are applied to human-authored texts **Burstein et al., "False Positives in Human-AI Text Classification"**.
In addition, the objectivity of AI-generated content is also questionable. The literature has revealed inherent biases within outputs produced by LLMs. Studies have documented significant gender and racial biases, notably in depictions of healthcare professionals and surgeons, where male representations are frequently favored **Noble et al., "Bias in Healthcare Professional Representations"**. Political bias has also been observed, with certain platforms such as ChatGPT exhibiting a tendency toward left-leaning perspectives **Shah et al., "Left-Leaning Bias in ChatGPT"**. Moreover, LLMs tend to manifest human-like content biases, as demonstrated by transmission experiments **Koppel et al., "Transmission of Human-Like Biases"** and linguistic analyses **Goldstein et al., "Linguistic Analysis of Human-Like Content Biases"**. 

\subsection{Perception of LLM-generated content}
Comparative studies reveal that human evaluators often struggle to reliably differentiate between AI-generated and human-authored content **Kim et al., "Human Evaluation of AI-Generated Texts"**. **Burstein et al., "Generative and Augmented AI Content in Human Perception"** demonstrated that generative and augmented AI content is frequently perceived as superior to that produced by human experts, even when humans utilize AI tools. However, disclosing the source of content narrows the perceived quality gap, suggesting a bias favoring human contributions over AI. Participants rated content more favorably when attributed to human experts, whereas awareness of AI involvement had minimal impact on perceptions. **Li et al., "Perceived Quality Gap between Human and AI-Generated Texts"** examined the ability of an AI chatbot (ChatGPT) to deliver quality and empathetic responses to patient questions compared to physicians. Their findings revealed that chatbot responses were preferred in the majority of evaluations, rated higher in quality, and deemed more empathetic than those of physicians. Notably, chatbot-generated texts were also significantly longer than physician responses. **Hovy et al., "ChatGPT: A Chatbot for Quality and Empathetic Responses"** highlighted that large language models (LLMs), particularly GPT-3, can produce high-quality persuasive content; however, individuals tend to prefer public health messages originating from human institutions rather than AI sources. 
Some studies highlight the nuanced perceptions and preferences surrounding AI-generated content across various domains. For example, **Kim et al., "Human Tutor Feedback vs. AI-Generated Feedback in Education"** compared human tutor feedback with AI-generated feedback in educational settings, revealing mixed results. While face-to-face interactions with tutors enhanced student engagement, AI-generated feedback was favored for its clarity and specificity. The research by **Hovy et al., "Consumer Preferences for Agentic and Communal Appeals in Advertising"** demonstrates that consumers prefer AI-generated ads with agentic appeals, while favoring human-created ads with communal appeals. 
These findings underscore that contextual factors and the awareness of text origin play a critical role in shaping user preferences.