\section{Conclusion}
We proposed Syntriever, a training framework for retrieval systems using LLM synthesis. 
In the distillation stage, Syntriever synthesizes various types of passages including augmented queries, relevant and plausibly irrelevant passages. Relevant passages are clustered in the embedding space using modified soft nearest-neighbor loss. In the alignment stage, the retriever is continually trained based on the preference feedback of LLMs on the retrieved passages. We propose a preference modeling called partial Plackett-Luce ranking to learn LLM preferences while maintaining the similarity relation among embeddings learned during the distillation stage. Experiments show that Syntriever achieves significant performance gains over baselines on benchmark datasets from various domains.

\section{Limitations}
Although Syntriever achieves performance gains compared to baseline retrievers on various benchmark datasets, it requires LLM inferences to generate synthetic data and alignment feedback. This may incur additional costs compared to other methods which only perform a fine-tuning of text encoders. 
However, the cost of proprietary black-box LLMs has become increasingly cheaper and affordable. Moreover, weaker but cheaper LLMs become increasingly capable of teaching student models~\cite{bansal2024smaller}. Thus, we believe that the Syntriever framework is widely applicable to retrieval systems in practice.

\section{Acknowledgement}
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MSIT) (RS-2022-NR070834), and by the Institute of Information \& Communications Technology Planning \& Evaluation (IITP)-ICT Creative Consilience Program grant funded by the Korea government (MSIT) (IITP-2025-RS-2020-II201819).