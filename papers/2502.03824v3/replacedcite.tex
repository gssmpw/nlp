\section{Related Work}
\paragraph{Neural Information Retrieval.}
Neural information retrieval is a key element of retrieval-augmented generation (RAG) ____ which is a retrieve-and-read approach for open domain question answering tasks____. 
Lexical retrieval methods such as BM-25____ have been mostly used prior to neural retrievals, which however had difficulties with capturing semantic information at scale. Thus, dense passage retrievers using text encoders____ have been actively explored____. 
RocketQA____ is a multi-step training framework for a retrieval system consisting of a retriever and a re-ranker which typically is a cross-encoder to estimate the ranking among retrieved passages. RocketQA further utilizes the re-ranker to sample hard negatives from top-retrieved passages. Meanwhile, Syntriever does not use separate re-rankers, but continually trains the retriever for its alignment with the ranking preference of LLMs.
Unsupervised learning for retrieval ____ was proposed to train sentence encoders by contrastive learning using a large collection of text-pair datasets. Subsequently, a hybrid retrieval method which combines lexical, dense, and multi-vector retrievers has been proposed____. RePlug____ proposed a knowledge distillation for retrievers using KL divergence associated with the prediction probabilities of relevant documents from LLMs which, however, are available only from outdated APIs. 

\paragraph{Training with Synthetic Data.}
Tiny-stories____ first proposed training small language models using synthetic data generated by GPT-4____. Motivated by ____, Phi____ proposed filtering of code data based on the \emph{educational value} through the prompting of GPT-4.
The next version of Phi-series____ generated high-quality synthetic data from judiciously selected topics in order to distill GPT-4's knowledge into small LLMs. They demonstrated that distillation through synthetic data of high educational value can boost the performances of small LLMs. ____ proposed to train a Mistral-7B model ____ by synthetically generating query-document pairs by prompting GPT-4 for various text embedding tasks. 
____ proposed distillation synthetic data where they fine-tune the student LLM using the output answers from teacher models
based on rationales____. The aforementioned methods have demonstrated that student models can efficiently learn from the synthetic data generated by teacher models.