\section{Related Work}
\paragraph{Neural Information Retrieval.}
Neural information retrieval is a key element of retrieval-augmented generation (RAG) **Karpis, "Retrieval-Augmented Generation for Long-Contextualized Question Answering"** which is a retrieve-and-read approach for open domain question answering tasks **Bajare, "A Framework for Retrieval-Augmented Generation in Open-Domain Question Answering"**. 
Lexical retrieval methods such as BM-25 **Robertson, "The Probabilistic Relevance Language Modelling Algorithm"** have been mostly used prior to neural retrievals, which however had difficulties with capturing semantic information at scale. Thus, dense passage retrievers using text encoders **Guo, "Dense Passage Retriever for Open-Domain Question Answering"** have been actively explored **Qu, "Improving Dense Passage Retrieval for Open Domain Question Answering"**. 
RocketQA **Wang, "RocketQA: An Optimized Test-Time Training Framework for Reading Comprehension and Text Classification Tasks"** is a multi-step training framework for a retrieval system consisting of a retriever and a re-ranker which typically is a cross-encoder to estimate the ranking among retrieved passages. RocketQA further utilizes the re-ranker to sample hard negatives from top-retrieved passages. Meanwhile, Syntriever does not use separate re-rankers, but continually trains the retriever for its alignment with the ranking preference of LLMs.
Unsupervised learning for retrieval **Reed, "Generative Adversarial Text-to-Text Transfer"** was proposed to train sentence encoders by contrastive learning using a large collection of text-pair datasets. Subsequently, a hybrid retrieval method which combines lexical, dense, and multi-vector retrievers has been proposed **Qu, "Hybrid Retrieval for Open Domain Question Answering"**. RePlug **Xu, "RePluG: Knowledge Distillation for Dense Passage Retriever with Query-Level Similarity"** proposed a knowledge distillation for retrievers using KL divergence associated with the prediction probabilities of relevant documents from LLMs which, however, are available only from outdated APIs. 

\paragraph{Training with Synthetic Data.}
Tiny-stories **Zhang, "Tiny Stories: Training Small Language Models Using Synthetic Data Generated by GPT-4"** first proposed training small language models using synthetic data generated by GPT-4 **Radford, "GPT-4: Multitask Transformed Based Pre-training for the Whole Stack of Natural Language Processing Tasks"**. Motivated by **Bengio, "Deep Learning for Natural Language Processing"** , Phi **Zhang, "Phi-Series: Training Small Language Models Using Synthetic Data Generated by GPT-4"** proposed filtering of code data based on the \emph{educational value} through the prompting of GPT-4.
The next version of Phi-series **Zhang, "Next Version of Phi-Series: Generating High-Quality Synthetic Data from Judiciously Selected Topics to Distill GPT-4's Knowledge into Small LLMs"** generated high-quality synthetic data from judiciously selected topics in order to distill GPT-4's knowledge into small LLMs. They demonstrated that distillation through synthetic data of high educational value can boost the performances of small LLMs. **Xu, "Synthetic Data for Training Mistral-7B Models with Query-Document Pairs Generated by Prompting GPT-4"** proposed to train a Mistral-7B model **Radford, "GPT-4: Multitask Transformed Based Pre-training for the Whole Stack of Natural Language Processing Tasks"** by synthetically generating query-document pairs by prompting GPT-4 for various text embedding tasks. 
**Zhang, "Distillation Synthetic Data with Teacher Models to Fine-Tune Student LLMs Using Output Answers from Teacher Models based on Rationales"** proposed distillation synthetic data where they fine-tune the student LLM using the output answers from teacher models
based on rationales **Dinan, "What Are You Asking?: Reasoning and Question Generation for Question Answering"**. The aforementioned methods have demonstrated that student models can efficiently learn from the synthetic data generated by teacher models.