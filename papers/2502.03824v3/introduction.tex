\section{Introduction}
Large Language Models~(LLMs) have become a core technology in various NLP applications such as chatbots~\cite{achiam2023gpt, team2023gemini} and coding assistants~\cite{roziere2023code, guo2024deepseek}. 
It is essential that the knowledge of LLMs is complemented by up-to-date information from external sources. To this end, retrieval-augmented generations~(RAG) have been proposed and actively explored for various knowledge-intensive NLP tasks  \cite{lewis2020retrieval, guu2020retrieval, lazaridou2022internet}. RAG enhances the LLM performance without fine-tuning by incorporating external knowledge into LLMs through search and   alleviates problems such as hallucination  ~\cite{welleck2020neural}, i.e., plausible but non-factual information generated by LLMs. 

The retrieval of documents relevant to a given query is a key task of the RAG system.  Dense retrieval methods \cite{karpukhin2020dense,gao2022unsupervised} are widely used to capture semantic relationships between queries and documents, in which text encoders are trained to learn dense embeddings of queries and passages for their semantic matching.
The encoders can be pre-trained in an unsupervised manner by using large-scale text pairs sampled from sentences and their contexts \cite{lee2019latent, izacard2021unsupervised}, and then be fine-tuned on the annotated datasets for retrieval tasks \cite{wang2022text, chen2024bge}.
Meanwhile, recent LLMs have exhibited remarkable generalization abilities in many NLP tasks, including information retrieval. 
In this paper, we explore how the vast knowledge of LLMs can be effectively utilized in training retrievers.
Recently, RePlug~\cite{shi2024replug} has been proposed for distilling the LLMs' knowledge into small retrievers. RePlug calculates the relevance scores of $k$ retrieved passages given a query, from which a likelihood over $k$ passages is computed. The retriever is trained to minimize the KL divergence between this likelihood and the LLM's likelihood over passages based on its probability of predicting the ground truth answer. However, prediction probabilities are mostly unavailable as the output in the latest \emph{black-box} 
LLMs~\cite{achiam2023gpt, team2023gemini}. Thus, we consider the distillation of LLM's knowledge into retrievers when only the synthetically generated texts are available as the output from LLMs. 


\noindent\textbf{Contribution.} 
We propose Syntriever, a framework to train/fine-tune retriever models based on synthetic data so as to distill the knowledge of black-box LLMs into retrievers effectively. We propose a two-stage framework: in the first stage, called \emph{distillation stage}, we fine-tune the retriever with LLM-generated synthetic data; in the second stage, called \emph{alignment stage}, we align the retriever with the preference of LLMs. 
In the distillation stage, Syntriever exploits synthetically augmented queries using chain-of-thoughts~\cite{wei2022chain}, synthetic positive and hard-negative passages, as well as self-verification to deal with hallucination. The retriever is then trained by modified Soft Nearest-Neighbor loss \cite{frosst2019analyzing} to cluster multiple relevant passages together in the embedding space. In the alignment stage, we continually fine-tune the retriever trained from the distillation stage, where the goal is to align the retriever with  LLM preferences.
The retriever fetches top-$K$ passages from which a set of passage pairs is sampled and provided to LLMs for preference feedback. In particular, we propose a preference modeling called \emph{partial Plackett-Luce ranking} to learn LLM preferences with a regularization effect such that the aligned model does not deviate excessively from the distilled model.
We evaluated the performance of Syntriever in various domains of benchmark datasets for the retrieval tasks from BeIR~\cite{thakur2021beir}. 
Syntriever achieves superior performances on all benchmark datasets, by up to 18.6\% in nDCG@10, compared to the prior state-of-the-art. Moreover, we show that the Syntriever framework can be combined with diverse base retrievers and LLMs, leading to a significant increase in retrieval accuracy. 
