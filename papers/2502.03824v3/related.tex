\section{Related Work}

\paragraph{Neural Information Retrieval.}
Neural information retrieval is a key element of retrieval-augmented generation (RAG) \cite{lewis2020retrieval} which is a retrieve-and-read approach for open domain question answering tasks~\cite{chen2017reading}. 
Lexical retrieval methods such as BM-25~\cite{robertson2009probabilistic} have been mostly used prior to neural retrievals, which however had difficulties with capturing semantic information at scale. Thus, dense passage retrievers using text encoders~\cite{devlin2018bert} have been actively explored~\cite{karpukhin2020dense, gao2022unsupervised, xiong2021approximate}. 
RocketQA~\cite{qu2021rocketqa} is a multi-step training framework for a retrieval system consisting of a retriever and a re-ranker which typically is a cross-encoder to estimate the ranking among retrieved passages. RocketQA further utilizes the re-ranker to sample hard negatives from top-retrieved passages. Meanwhile, Syntriever does not use separate re-rankers, but continually trains the retriever for its alignment with the ranking preference of LLMs.
Unsupervised learning for retrieval \cite{izacard2021unsupervised, wang2022text} was proposed to train sentence encoders by contrastive learning using a large collection of text-pair datasets. Subsequently, a hybrid retrieval method which combines lexical, dense, and multi-vector retrievers has been proposed~\cite{chen2024bge}. RePlug~\cite{shi2024replug} proposed a knowledge distillation for retrievers using KL divergence associated with the prediction probabilities of relevant documents from LLMs which, however, are available only from outdated APIs. 

\paragraph{Training with Synthetic Data.}
Tiny-stories~\cite{eldan2023tinystories} first proposed training small language models using synthetic data generated by GPT-4~\cite{achiam2023gpt}. Motivated by \cite{eldan2023tinystories}, Phi~\cite{gunasekar2023textbooks} proposed filtering of code data based on the \emph{educational value} through the prompting of GPT-4.
The next version of Phi-series~\cite{li2023textbooks, abdin2024phi} generated high-quality synthetic data from judiciously selected topics in order to distill GPT-4's knowledge into small LLMs. They demonstrated that distillation through synthetic data of high educational value can boost the performances of small LLMs. \cite{wang2023improving} proposed to train a Mistral-7B model \cite{jiang2023mistral} by synthetically generating query-document pairs by prompting GPT-4 for various text embedding tasks. 
\cite{yu2024distilling} proposed distillation synthetic data where they fine-tune the student LLM using the output answers from teacher models
based on rationales~\cite{wei2022chain, deng2023rephrase}. The aforementioned methods have demonstrated that student models can efficiently learn from the synthetic data generated by teacher models. 