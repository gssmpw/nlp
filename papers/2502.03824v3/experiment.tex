\section{Experiment}
\subsection{Experimental Settings}

\textbf{Datasets.} Experiments are conducted on retrieval benchmark datasets from various domains in BeIR~\cite{thakur2021beir}. We evaluate the performance of retrievers in two benchmark settings as follows.
\begin{itemize}
    \item \textbf{Supervised Fine-Tuning.} The models are evaluated on BeIR benchmark datasets which contain the training datasets. For each benchmark dataset, every model is fine-tuned on its training dataset, and we report in-domain evaluation results on that benchmark dataset.

    \item \textbf{Zero-shot Transfer.} The models are evaluated on out-of-domain datasets from BeIR benckmark. The zero-shot setting is similar to previous work \cite{izacard2021unsupervised,wang2022text}: the models can be first fine-tuned on large retrieval datasets such as MSMARCO~\cite{nguyen2016ms} and NQ~\cite{kwiatkowski2019natural} for generic knowledge, and then are evaluated on unseen datasets.
\end{itemize}
We use Normalised Discounted Cumulative Gain (nDCG@$K$) as the default performance metric.

\noindent\textbf{Baselines.} We experiment with lexical retriever BM-25~\cite{robertson2009probabilistic}, semantic retrievers DPR~\cite{karpukhin2020dense}, SBERT~\cite{reimers2019sentence}, CoCondenser~\cite{gao2022unsupervised}, RocketQA~\cite{ren2021rocketqav2}, 
Contriever~\cite{izacard2021unsupervised}, E5~\cite{wang2022text}, English model of BGE-M3-EN~\cite{chen2024bge}, Nomic-embed~\cite{nussbaum2402nomic}. 

\noindent\textbf{Settings of Syntriever.} Syntriever uses pre-trained E5 \cite{wang2022text} as the base encoder $E$. In the settings of supervised fine-tuning, Syntriever is trained with synthetic data generated from each training dataset. In the settings of the zero-shot transfer, Syntriever is first trained on synthetic data based on training datasets of MSMARCO and NQ, and then is evaluated on out-of-domain datasets from BeIR benchmarks. This is a similar setting as Contriever \cite{izacard2021unsupervised}, E5 \cite{wang2022text}, etc. To minimize the effects of model sizes on performance, we set the size of Syntriever and all baseline models to (approximately) 125M. In alignment stage of Syntriever, we set $K=5$ by default, and set $N={K\choose 2}=10$. Detailed hyperparameters are in Appendix~\ref{appendix:hyperparameters}.

\begin{table}[t!]
\begin{adjustbox}{width=.5\textwidth,center}
\begin{tabular}{|l|l|l|l|l|l|l|l|}
\hline
\textbf{$q^\mathrm{cot}$} & \textbf{$p^{+,-}$} & \textbf{$c^+ \succ c^-$} & \textbf{MSMARCO} & \textbf{HotpotQA} & \textbf{FiQA} & \textbf{SciFact} & \textbf{NFCorpus} \\ \hline
{\color{red}\ding{55}} & {\color{red}\ding{55}} & {\color{red}\ding{55}} & 42.4 & 63.4 & 37.9 & 73.7 & 35.8 \\ \hline
{\color{blue}\ding{52}} & {\color{red}\ding{55}} & {\color{red}\ding{55}} & 44.6 & 64.3 & 38.5 & 76.7 & 40.2 \\ \hline
{\color{red}\ding{55}} & {\color{blue}\ding{52}} & {\color{red}\ding{55}} & 45.7 & 64.7 & 39.3 & 77.3 & 40.8 \\ \hline
{\color{blue}\ding{52}} & {\color{blue}\ding{52}} & {\color{red}\ding{55}} & 46.2 & 65.3 & 40.2 & 78.9 & 41.4 \\ \hline
{\color{red}\ding{55}} & {\color{red}\ding{55}} & {\color{blue}\ding{52}} & 45.8 & 67.3 & 39.1 & 75.5 & 37.3 \\ \hline
{\color{blue}\ding{52}} & {\color{blue}\ding{52}} & {\color{blue}\ding{52}} & \textbf{50.3} & \textbf{70.2} & \textbf{41.9} & \textbf{80.5} & \textbf{43.3} \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Ablation study (in nDCG@10). The first three columns represent the following components: synthesized query with CoT $(q^\mathrm{cot})$, synthetic positives and hard-negatives $(p^{+,-})$, alignment $(c^+ \succ c^-)$.} 
\label{tab:ablation_component}
\end{table}

%\section{Experimental Results}\label{sec:results} 
\subsection{Main Results}
We first present the supervised fine-tuning results on seven datasets for the retrieval task  which are BeIR benchmarks with training datasets. The results are shown in Table~\ref{tab:main_results}.
Compared to the second-best models, Syntriever improves the retrieval performances by: 18.6\% on MSMARCO, 5.9\% on HotpotQA, 2.5\% on FiQA, 1.8\% on SciFact, 8.3\% on NFCorpus, and 4\% on NQ. The base encoder for Syntriever is a pre-trained E5; still, Syntriever achieves performance gain over E5 by: 18.6\% on MSMARCO, 10.7\% on HotpotQA, 10.6\% on FiQA, 9.2\% on SciFact, 20.9\% on NFCorpus, 5.6\% on Fever and 5.7\% on NQ. This shows that Syntriever can successfully distill LLMs' capability into small retrievers and improve their performance by a large margin. Overall, Syntriever shows robust performances on datasets both in generalized and specialized domains. Our results show that small LMs can efficiently learn from the teacher model through synthetic data and can be successfully aligned through feedback, even without access to the output probability of black-box LLMs. 

Next, we present zero-shot transfer results.
Table~\ref{tab:main_zeroshot} shows that Syntriever achieves the best performances on 8, and the second best on 1, out of 15 datasets. Note that the performances of Syntriever on MSMARCO and NQ are in-domain results, whereas other baselines, e.g., Contriever \cite{izacard2021unsupervised}, E5 \cite{wang2022text}, etc., are reported also to be trained on MSMARCO and/or NQ. In particular, although Syntriever shares the same base model as E5, it improves the retrieval accuracy on 11 datasets. This is perhaps because LLM-generated synthetic data and alignment feedback improve the generalization capabilities of the retriever on unseen data.

\subsection{Ablation study}
We conduct an ablation study on Syntriever. We add or remove model components, and the effects on the performance are shown in Table \ref{tab:ablation_component}.
The results show that both synthesized query $(q^\text{cot})$ and passages $(p^+,p^-)$ in the distillation stage improve the retrieval performances. Overall, the distillation stage achieves an average gain of 8.2\% over the base retriever. Results show that the retriever successfully learns from the parametric knowledge of LLMs during the distillation stage. Also, the alignment component $(c^+\succ c^-)$ in Table~\ref{tab:ablation_component} is shown to achieve performance gains of up to 8.8\%. Our results show that the alignment component is significant for retrieval performance, considering that nDCG@K is sensitive to the fine-grained ranking of relevant passages.

\begin{table}[t!]
\begin{adjustbox}{width=.45\textwidth,center}
\begin{tabular}{l|l|lll}
\hline
\multirow{2}{*}{\textbf{Dataset}} & \multirow{2}{*}{\textbf{Metric}} & \multicolumn{3}{c}{\textbf{Base encoder}}                                  \\ \cline{3-5} 
                                  &                                  & \multicolumn{1}{c|}{ColBERT}  & \multicolumn{1}{c|}{SBERT}        & Contriever   \\ \hline
\multirow{4}{*}{HotpotQA}         & nDCG@1                           & \multicolumn{1}{l|}{65.3 \textbf{(+10.3})} & \multicolumn{1}{c|}{63.2 \textbf{(+10.3)}}  & 73.3 \textbf{(+10.5)} \\
                                  & nDCG@3                           & \multicolumn{1}{l|}{58.2 \textbf{(+9.7)}} & \multicolumn{1}{l|}{57.0 \textbf{(+9.5)}} & 68.2 \textbf{(+9.5)}  \\
                                  & nDCG@5                           & \multicolumn{1}{l|}{60.7 \textbf{(+8.5)}} & \multicolumn{1}{l|}{59.5 \textbf{(+9.8)}} & 70.4 \textbf{(+9.7)}  \\
                                  & nDCG@10                          & \multicolumn{1}{l|}{62.8 \textbf{(+9.1)}} & \multicolumn{1}{l|}{61.8 \textbf{(+11.1)}} & 72.1 \textbf{(+10.9)} \\ \hline
\multirow{4}{*}{FiQA}             & nDCG@1                           & \multicolumn{1}{l|}{30.1 \textbf{(+3.8)}} & \multicolumn{1}{l|}{26.7 \textbf{(+5.2)}}  & 32.1 \textbf{(+5.1)}  \\ 
                                  & nDCG@3                           & \multicolumn{1}{l|}{27.8 \textbf{(+3.2)}} & \multicolumn{1}{l|}{25.1 \textbf{(+4.6)}}  & 30.4 \textbf{(+4.7})  \\
                                  & nDCG@5                           & \multicolumn{1}{l|}{30.5 \textbf{(+2.8)}} & \multicolumn{1}{l|}{26.1 \textbf{(+4.5)}}  & 31.9 \textbf{(+4.3)}  \\
                                  & nDCG@10                          & \multicolumn{1}{l|}{33.5 \textbf{(+2.9)}} & \multicolumn{1}{l|}{25.8 \textbf{(+4.3)}}  & 35.2 \textbf{(+4.6)}  \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Performance gains of Syntriever with different base encoders.}
\label{tab:ablation_retriever}
\end{table}

\subsection{Performances with different encoders}
Syntriever is a framework for training encoders for retrieval, and thus can be combined with different sentence encoders. We experiment with various well-known encoders, e.g., ColBERT, SBERT, and Contriever, as the base encoders for Syntriever. Syntriever improves the performance by a large margin in all three retrieval models. The performance improvement is particularly high in nDCG@1 which concerns retrieving the exact passage relevant to the query. 
This is because the alignment stage in Syntriever helps the retriever with a fine-grained ranking of highly relevant passages. Overall, the results show that Syntriever is generally applicable to, and improves the performances of, various retrievers.

\begin{table}[t!]
\centering

\begin{adjustbox}{width=.4\textwidth}
\begin{tabular}{l|l|l|l}
\hline
\textbf{Method}  & \textbf{HotpotQA}  & \textbf{FiQA}  & \textbf{SciFact} \\ \hline
\multirow{1}{*}{w/o Self-verification} & 67.4  & 40.8 & 79.7             \\
\multirow{1}{*}{w/ Self-verification}  & 
\textbf{70.2} & \textbf{41.9} & \textbf{80.5}                 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Effectiveness of re-labeling hallucination passages. Results are in nDCG@10.}
\label{tab:ablation_verification}
\end{table}

\subsection{Effects of Re-labeling Hallucination Passages}\label{relabel}
Table~\ref{tab:ablation_verification} shows that LLM self-verification and re-labeling are effective for the synthetic training by Syntriever. The performance improvement of self-verification on HotpotQA is relatively greater than other datasets. We found that, approximately 15\% of synthetic positive passages were relabeled as hallucinations in the case of HotpotQA, whereas the proportion was about a few percent in other datasets. This indicates that the performance improvement through relabeling is likely higher for HotpotQA. In conclusion, removing hallucinations (and even \emph{re-using} them as hard-negatives as in Syntriever) through self-verification is important for data synthesis, which is the case for most tasks utilizing LLMs \cite{weng2023large}.


\begin{table}[t!]
\centering
    \begin{adjustbox}{width=.37\textwidth,center}
    \begin{tabular}{l|l|l|l}
\hline
\textbf{Dataset}          & \textbf{Metric} & \textbf{GPT-4o mini} & \textbf{GPT-4o} \\ \hline
\multirow{4}{*}{SciFact} & nDCG@1          &    65.7                                &      \textbf{66.7}                         \\
                          & nDCG@3          &   73.0                                 &        \textbf{75.0}                       \\ 
                          & nDCG@5          &   76.7                                &        \textbf{79.1}                      \\ 
                          & nDCG@10         &   76.3                                &         \textbf{78.9}                       \\ \hline
\multirow{4}{*}{NFCorpus}     & nDCG@1          &                         \textbf{46.0}      &        45.0    \\ 
                          & nDCG@3          &        \textbf{42.3}                            &        41.2    \\ 
                          & nDCG@5          &        \textbf{42.1}                            &        41.1    \\  
                          & nDCG@10         &        \textbf{42.6}                            &        41.4    \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Comparison of models  trained only up to \textbf{the distillation stage} using synthetic data from GPT-4o mini vs. GPT-4o.}
\label{tab:ablation_llm_stage1}
\end{table}

\begin{table}[t!]
    \centering
    \begin{adjustbox}{width=.37\textwidth,center}
    \begin{tabular}{l|l|l|l}
\hline
\textbf{Dataset}          & \textbf{Metric} & \textbf{GPT-4o mini} & \textbf{GPT-4o} \\ \hline
\multirow{4}{*}{HotpotQA} & nDCG@1          &     64.5                               &           \textbf{68.3}                    \\
                          & nDCG@3          &   63.5                                 &    \textbf{71.2}                           \\ 
                          & nDCG@5          &     66.3                              &        \textbf{71.4}                       \\ 
                          & nDCG@10         &     68.6                              &    \textbf{70.2}                            \\ \hline
\multirow{4}{*}{FiQA}     & nDCG@1          &        41.8                            &          \textbf{42.1}                     \\ 
                          & nDCG@3          &     39.7                               &      \textbf{40.4}                         \\ 
                          & nDCG@5          &       41.3                             &          \textbf{41.5}                     \\  
                          & nDCG@10         &      40.7                              &       \textbf{41.9}                        \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Comparion of models trained by preference feedback from GPT-4o mini vs. GPT-4o. Both models are trained with GPT-4o in the distillation stage.}
\label{tab:ablation_llm_stage2}
\end{table}

\subsection{Weaker but Cheaper LLMs can be effective}
We examine how the LLM capabilities affect distillation and alignment performances. We consider two LLMs: GPT-4o vs. GPT-4o-mini, where GPT-4o is the larger and more capable model. First, we compare the distillation capabilities of two LLMs. Table~\ref{tab:ablation_llm_stage1} shows the comparison, where Syntriever is trained only up to the distillation stage. Interestingly, the distillation performance of GPT-4o-mini is better than GPT-4o on NFCorpus. Considering the datasets concern different knowledge domains (SciFact: scientific, NFCorpus: medical), smaller models may be better at teaching than larger ones in certain domains. Our results interestingly coincide with recent findings that weaker models may be better at teaching than stronger models in domains like math problem solving \cite{bansal2024smaller}.
Next, we examine the alignment capabilities of LLMs. For a fair comparison, two models are first trained by GPT-4o in the distillation stage, and then trained by different LLMs in the alignment stage. Table~\ref{tab:ablation_llm_stage2} shows that the larger model (GPT-4o) is better at alignment. It is challenging to rank top-$K$ passages retrieved by a distilled retriever, requiring a deep understanding of various contexts, and thus larger models may be more favored for the task. Overall, smaller models appear to be quite competitive, i.e., the performance gap is small or even better in some domains. Thus, our prospect is that distillation/alignment through small models will become an increasingly good alternative, especially under a fixed compute budget  \cite{bansal2024smaller}.


%\section{Additional Experiments}\label{appendix:additional_experiments}
\begin{table}[t!]
\centering
\begin{adjustbox}{width=.5\textwidth,center}
\begin{tabular}{l|l|l|l}
\hline
\textbf{Method}  & \textbf{FiQA}  & \textbf{NFCorpus} & \textbf{SciFact} \\ \hline
\multirow{1}{*}{Partial Plackett Luce}  & \textbf{41.9} & \textbf{43.3} & \textbf{80.5}\\ 
\multirow{1}{*}{Bradley Terry}  & 35.7 & 36.1  & 79.8              \\ \hline

\end{tabular}
\end{adjustbox}
\caption{Comparison of preference modeling. Results are in nDCG@10.}
\label{tab:ablation_loss}
\end{table}

\subsection{Comparison of Preference Modeling Methods}

Table~\ref{tab:ablation_loss} compares the preference modeling methods for alignment:  Bradley-Terry~(BT) \cite{bradley1952rank} and partial Plackett-Luce~(PL) ranking model. While BT and partial PL models achieve similar performances on SciFact, BT model shows poor performances on FiQA and NFCorpus. The following is a possible explanation. The search results on SciFact tend to be highly accurate, and most of top-$K$ passages are likely to contain (partly) relevant context. By contrast, top-$K$ passages on FiQA and NFCorpus which are more challenging datasets, will tend to be only marginally relevant to the given query. The partial PL performs preference ranking while keeping those marginally relevant passages away from highly irrelevant (in-batch) passages. Without such regularization of keeping marginally positive samples away from in-batch negatives, which was done during the distillation stage, BT model may cause the retriever to \emph{forget} the knowledge learned during the distillation stage. This may cause performance drops on FiQA and NFCorpus as shown in Table~\ref{tab:ablation_loss}. Thus, we conclude that the proposed partial ranking is crucial for the alignment performance. 

\begin{table}[t!]
\begin{adjustbox}{width=.5\textwidth,center}
\begin{tabular}{l|l|l|l|l|l}
\hline
\textbf{Dataset}          & \textbf{Metric}  &  
\textbf{w/o Alignment} &  $K=3$ & $K=5$ & $K=10$ \\ \hline
\multirow{4}{*}{FiQA}     & nDCG@1  &    38.7   &  
    40.9    &       42.1         &    \textbf{42.3}     \\  
                          & nDCG@3  &    38.3   &     39.7          &     40.4           &            \textbf{41.5}     \\ 
                          & nDCG@5  &    38.5   &      40.9         &             41.8   &           \textbf{42.7}      \\  
                          & nDCG@10 &    37.9   &     41.7          &             41.9   &           \textbf{42.5}      \\ \hline
\multirow{4}{*}{SciFact}     & nDCG@1  &    80.2   &      81.0         &         \textbf{81.3}       &         80.0        \\  
                          & nDCG@3  &    78.0   &     78.6          &        79.0        &        \textbf{79.8}         \\ 
                          & nDCG@5  &   78.8    &     79.7          &        \textbf{79.8}        &         79.2        \\  
                          & nDCG@10 &   78.9    &     80.0          &       80.5         &        \textbf{80.6}         \\ \hline
\end{tabular}
\end{adjustbox}
\caption{Effect of varying $K$ in  top-$K$ retrieved passages for preference alignment.}
\label{tab:ablation_topk}
\end{table}

\begin{table}[t!]
\begin{adjustbox}{width=.5\textwidth,center}
\begin{tabular}{l|l|l|l|l}
\hline
\textbf{Dataset} & Default &  $K=3$ & $K=5$ & $K=10$ \\ \hline
FiQA & 41.9 & 41.7 & 41.8 & \textbf{42.3} \\
SciFact & \textbf{80.5} & 80.0 & 80.3 & 79.8 \\
NFCorpus & 43.3 & 42.4 & 42.6 & \textbf{43.8} \\\hline
\end{tabular}
\end{adjustbox}
\caption{Effect of varying $K$ in  top-$K$ retrieved passages and the number $N$ of sampled pairs for comparison in alignment.  We set $N=K$ for this experiment. By default, Syntriever uses $K=5$ and $N=\frac{K(K-1)}{2}=10$.  The evaluation metric is nDCG@10.}
\label{tab:ablation_k_n}
\end{table}

\subsection{Effects of the number of retrieved passages during alignment} 
We examine the effect of the number $K$ in the top-$K$ passage retrieved during the alignment process. Table~\ref{tab:ablation_topk} shows the results with varying $K$, where we sample all the possible pairs, or $N={K \choose 2}$, for comparison. The performance improves with increasing $K$, up to 12\% in FiQA and 5.1\% in SciFact. Also, results show that the larger $K$, the better the performance. In addition, using a larger number of passages is particularly effective when the overall retriever accuracy is low, since it is more likely to retrieve relevant context in top-$K$-ranked passages for large $K$. However, large $K$ may incur high computational costs if $N={K \choose 2}$, and thus there is a trade-off between performance and computational overheads. In this paper, we chose $K=5$ as a good trade-off point.

In addition, we experiment with the numbers of passage pairs to be sampled for comparison ($N$) with varying $K$. Previously in Table \ref{tab:ablation_topk}, we set $N={K\choose 2}=K(K-1)/2$. Here we provide the experiments with a smaller $N$  given by $N=K$. The results are shown in   Table~\ref{tab:ablation_k_n}. Overall, if we compare Table \ref{tab:ablation_topk} and \ref{tab:ablation_k_n}, the performance seems to slightly degrade for smaller $N$. As previously, for challenging datasets such as FiQA and NFCorpus, the performance seems to gradually improve with increasing $K$, again because more retrieved passages lead to a higher chance of including relevant passages in top-$K$. At the same time, increasing $K$ seems to exhibit diminishing returns on the performance. Overall, the default setting of Syntriever $(K=5, N=10)$ appears to be a reasonable choice in terms of a balance between complexity and performance. 

\subsection{Quality of Synthetic Positives} In general, it is difficult to accurately quantify the ratio of hallucination in the synthetic passage. The passage may not have direct clues to the answers, but may contain partial information from which the answer can be deduced. How relevant a passage should be to the query so that the passage is classified as positive? This is very hard to quantify, and thus measuring the quality of synthetic passages is difficult as well.

We performed experiments to indirectly measure the quality of synthetic positives as follows. We asked GPT-4o that whether the true answer can be directly derived from synthetic positive passages (after self-verification). We asked the same question, but in this time whether the answer can be derived from the ground-truth passages provided by the dataset. The results are shown in the table below.

\begin{table}[t!]
    \centering
    \begin{adjustbox}{width=.5\textwidth,center}
    \begin{tabular}{c|c|c}
        Passages & Can derive answer& Cannot derive answer\\\hline
        Synthetic positive & 88\% & 12\% \\
        Ground truth & 84\% & 16\%
    \end{tabular}
    \end{adjustbox}
    \caption{Results of GPT-4o about whether each passage can answer ground truth. We randomly select 1000 samples in each passage set.}
    \label{tab:self_verifiacition}
\end{table}

Interestingly, GPT-4o states that only 84\% of the ground truth passages have direct clues to the true answer. This is because, a significant portion of the "ground truth" passages of the  HotpotQA dataset do not contain direct clues to the true answer, but only indirect clues or partial information. By contrast, GPT-4o stated that 88\% of synthetic positives after self-verification contain direct contexts to the true answer. Thus, we conclude that synthetic positives after self-verification are of fairly high quality.