
%\section{Problem Definition}
%\section{Background and  Design Considerations}
\section{Design Considerations}
\label{sect:background}

\comments{
\begin{table}[h]
	\centering
		%\resizebox{1.02\columnwidth}{!}{ 
		\begin{small}
		\begin{tabular}{l l}
			\hline
			Symbol & Def. \\
                \hline
                %$\mathcal{D}$ & A set of documents in a dataset\\
                %$N$ & Total number of documents in a dataset \\
                $Q$ & A query \\
                $d_i$ & A document \\
                $\mathcal{D}^{+}$, $\mathcal{D}^{-}$ & Positive/negative document subsets corresponding to $Q$  \\
                $|.|$ &  The size of a set\\
		$\Theta$& Parameters of a scoring model\\
		$S(Q,d_i, \Theta)$ & The rank score of a document for a query using a model\\
%			& based on a model with parameters $\Theta$. \\
                $p_i$ & Teacher's top one probability of document $d_i$,\\
			& standing for  $P(d_i|Q, \mathcal{D}^+, \mathcal{D}^-, \Theta$) \\
                $q_i$ & Student's top one probability of document $d_i$\\
                $\pi(.)$ & The rank position of a document for a query\\
                $\beta_i$ & Exponent weight bias for negative  document $d_i$\\
                $\gamma, \alpha$ & CKL hyper-parameters\\
                \hline
		\end{tabular}
		\end{small}
	%}
	\caption{Table of Symbols}

	\label{tab:data}
\end{table}
}


{\bf Problem definition.}
Given query $q$ for searching a collection of $D$ text documents:   $ \{d_i\}^D_{i=1}$, 
retrieval  obtains the top $k$ relevant documents from this collection based on
the similarity between query $q$ and a candidate document  $d_i$ using  their
representation vectors.
Sparse retrieval uses  the dot product of their lexical or learned representation vectors as the similarity function
and implements its search efficiently using an inverted index. 
%\begin{equation}
%\label{eq:sparseretrieval}
$L(q) \cdot  L(d_i)$
%\end{equation}
%L(q,d_i ) = 
where $L(.)$ is a a vector of weighted term tokens for a document or a 
query~\cite{Robertson2009BM25, Dai2020deepct, Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL}. 
\comments{
For BM25 based lexical representation~\cite{Robertson2009BM25}, 
$L(.)$ treats each document or a query as a bag of terms
and term weights  are scaled based on   frequency of terms within a bag and  across all bags involved.
For a learned sparse representation~\cite{Dai2020deepct, Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL}, 
a document or a query is encoded using  a learned neural model, which
produces a bag which may contain original  and  expanded terms.
The encoding functions for a query and document can be different. Without the loss of generality, we assume they are the same in this paper.
}
Dense retrieval with a single vector representation computes the following rank score
%\begin{equation}
%\label{eq:denseretrieval}
$R(q) \cdot  R(d_i)$
%\end{equation}
%D(q,d_i) 
where $R(.)$ is a dense representation vector  of a fixed size~\cite{Karpukhin2020DPR}.
%In a single-vector dense  representation, each  representation vector of a document or a query is a dense vector. 


  %revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
%The weights of these terms evaluated in this paper use the SPLADE model as discussed below while other options are possible.
%(e.g., a bag of words or a neural encoder). 
%The implementation of such a dot product for a sparse retriever uses the inverted index data structure.
\comments{
%{\bf Learned term weights with SPLADE}.
For each document, the SPLADE term weighting revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
score $w_j$ of $j$-th token term in BERT WordPiece vocabulary as
%\begin{equation}
%\label{eq:splade}
$w_j = \sum_{i \in d}  log (1 + ReLU ( H(h_i)^T E_j + b_j)) $
%\end{equation}
where document $d$ consists of a sequence of BERT last layer's embeddings $(h_1, h_2, $
$\cdots, h_n)$.
$E_j$ is the BERT input embedding of the $j$-th token and $b_j$ is a token level bias.
$H(.)$ is a linear layer with activation and layer normalization. 
% (In SPLADE GeLU activation is used). 

{\bf Combining the results of a dense retriever and a sparse retriever.}
}
Following the work of~\cite{kuzi2020leveraging, 2022LinearInterpolationJimLin, Gao2020ComplementingLR}, 
linear interpolation is used  to  ensemble  dense and sparse retrieval scores. 
%Let $S$ be the set of top results obtained by a sparse retriever, and $D$ be
%be the set of results obtained by dense retrieval. 
%For any document $d_i \in S \cup D$, its  
The fused rank score for document $d_i$ in responding  query $q$ is:
%\begin{equation}
%\label{eq:fusion}
$\alpha L(q) \cdot  L(d_i) +
(1-\alpha) R(q) \cdot  R(d_i) $
%\end{equation}
where $\alpha$ is a coefficient in a range between 0 and 1.
\comments{
Notice that   the two ranked lists $S$ and $D$ are usually not fully overlapped and  some documents only exist in one list. While 
different  treatments on these documents affect fusion performance,
we take an option to  impute the score of such a document score   using $\beta * score (top \  k+1) $  in the missed  ranklist where $\beta$
is a scaling factor.
%While there are options to We consider a few options
For example,  we  impute the lexical scores of documents expanded by dense retrieval
as the top-$(k+1)$ lexical  score scaled by a factor. We use $\beta=0.95$  in our evaluation.
 
%\item Remove the documents when they only exist in one ranked list
%    \item  Take the document and compute the score for only one ranked list
}
Our optimization goal is to select a small number of dense vectors to fuse after sparse retrieval 
while maintaining a relevance competitive to the full dense retrieval  which searches  the entire document set. 
\comments{
In this way, limited dense retrieval that augments sparse retrieval is fast with minimum space consumption  on a CPU server. 
The ensembling method used in our evaluation is 
%The final ranking of the top $K$ documents is 
the reciprocal rank fusion (RRF) following Cormack et al.~\cite{Cormack2009RRF}, considering ranking positions of each candidate 
generated by a retriever and by a re-ranker. 
We use RRF
% above reciprocal rank fusion 
instead of linear interpolation as it has a similar performance with a simplified combination of two scoring models. 
%We choose RRF because it is easily applicable to multiple ranked lists ignoring the score distribution differences. 
\begin{equation}
     %RRF(Q,d) =  \sum_{r \in {rt, rr,}} \frac{1}{k_r + r(d)},
     RRF(Q,d) =  
\frac{1}{c_1 + \pi_1(d)} + 
\frac{1}{c_{2} + \pi_2(d)},
     \label{eq:score}
\end{equation}
where $c_{1}$ and $c_{2}$ are fixed smoothing constants. $\pi_2(d)$ is the rank position based  a re-ranker formula
while $\pi_1(d)$ is the rank position based on a retriever.
}

