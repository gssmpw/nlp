

\section{Introduction and Related Work}

\label{sect:intro}
Dense and sparse retrievers are two main categories of retrieval techniques for text  document search.
%It can be time-consuming to compute query-document similarities for the whole corpus on a CPU-only platform. The cost is often mitigated through approximate nearest neighbor search techniques. However, this reduction in latency comes with a tradeoff in relevance effectiveness. 
\comments{
Traditional sparse retrieval models, such as BM25, use lexical text matching and run
%to retrieve results from the collection 
efficiently by traversing  an inverted index on an inexpensive  CPU platform.
Recently, sparse retrieval studies have exploited deep learning 
models~\cite{Dai2020deepct,Mallia2021deepimpact,Lin2021unicoil,2021NAACL-Gao-COIL, Formal2021SPLADE, shen2023lexmae} to learn lexical representations of documents for better matching.
Dense retrieval 
exploits a dual encoder architecture to produce single document representations, 
e.g. ~\cite{gao-2021-condenser,2021SIGIR-Zhan-ADORE-dense,Ren2021RocketQAv2, 
Lin2021tctcolbert, Santhanam2021ColBERTv2,  Wang2022SimLM,  Liu2022RetroMAE}.

%For instance, AWS EC2 charges 
%High cost can be seen from AWS EC2 which charges a GPU instance one or two magnitute more than a strong CPU instance.  
%23x more for a GPU instance than CPU one for RepLLaMA retrieval.

%and SPLADE~\cite{Formal2021SPLADE, Formal2021SPLADEV2} has demonstrated the strong relevance performance among these models. 
}
\comments{
Recent studies have found that combining  sparse and dense retrieval scores  can further boost
retrieval recall and relevance~\cite{Lin2021unicoil,2022LinearInterpolationJimLin,kuzi2020leveraging},
suggest  that both categories  of retrievers tend to capture  different relevant signals. 
These studies  make an assumption that  a search platform has to run two retrievers separately and a dense retriever.
%delivers   its maximum effectiveness, and the impact of compression and IVF clustering is not considered. 
There is an imbalance of computing platform requirements for running  these two retrievers.

dense retrieval typically requires GPU support unless significant  
%We study the use of dense retrieval results to argument sparse retrieval. 
Sparse retrieval with an inverted index has the advantage that a low-cost CPU server can run the online inference  without GPU.
A judicious tradeoff consideration of retrieval  effectiveness and efficiency is critical on  a low-cost CPU-friendly search platform. 
The importance of CPU-friendly search is recognized 
in neural ranking studies~\cite{Yang2021WSDM-BECR,2022WWWforwardIndex,2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
For a large-scale  dataset (e.g. with billions of documents), a practical search system often employs  a multi-stage 
search architecture, which divides the dataset into many partitions so that  a fast first-stage  retrieval 
method  can  search these partitions in parallel.  Such a system  prefers low-cost
CPU-only servers for first-stage retrieval, and both time and memory space complexity of retrieval should be considered 
so that  each server can host as many documents as possible
to reduce the total number of servers to support a large data size and high query traffic. 
%Thus the challenge for retrieval on such an architecture needs to consider both time and space complexity. 
% small search index needs to be as small as possible.
%and the in-memory footprint size of 
%server  needs to host documents in memory as many as possible 
%given a large number  of data partitions involved for parallel search.
%Each server employs multiple CPU cores and is replicated with many copies  for dealing high traffic of  concurrent 
%search requests, and thus 
}
%which run  efficiently  on a single CPU without hardware accelerators  GPU
%which re  retaining strong relevance effectiveness
%%Another development in learned sparse retrieval is that
%sparse retrieval with a learned representation
\comments{
There are several recent efficiency studies advancing the
inverted index implementation for sparse retrieval with learned neural 
representations~\cite{Lassance2022SPLADE-efficient,mallia2022faster,qiao2023optimizing,2023SIGIR-Qiao, 2023SIGIR-SPLADE-pruning}.
which accelerates learned sparse retrieval with BM25-guided traversal or index pruning.

For example, Row 3 of Table~\ref{tab:costbudget} shows the latency, memory space cost, and  MRR@10  of
the latest efficient SPLADE retriever for searching 8.8 million 
MS MARCO passages~\cite{2023SIGIR-Qiao} on a CPU-only server.
% with a single thread on  a 2.25GHz AMD EPYC 7742  CPU server.
Row 2 is performance of a BM25-based retriever after document expansion. 
%There is a tradeoff  in relevance effectiveness if BM25 guidance or threshold-based pruning  is too aggressive.
%and dense retrieval can boost if it does not add much overhead. 
}
As shown in previous literature~\cite{Lin2021unicoil,2022LinearInterpolationJimLin, kuzi2020leveraging}, combining  sparse and dense retrieval scores with linear interpolation
can boost search relevance. 
%The  above studies has not considered to exploit the relationship of sparse and dense retrieval results for selective fusion. 
It is important to optimize for efficiency when combining the above two retrieval approaches on a low-cost CPU-only platform.
% Efficiency optimization in  combining the above  two retrieval approaches on a low-cost CPU-only platform is important.
This  is desirable in a large-scale  search system  which employs  a multi-stage search architecture, and 
runs partitioned first-stage retrieval in parallel  on a massive number of inexpensive CPU-only
machines. 
It is also critical for search on personal devices such as phones with limited computing/memory resources or battery use constraints.
%Dense retrieval is expensive, typically requiring GPUs. 


Dense retrieval can be accelerated   with  approximate nearest neighbor (ANN) search using 
partial IVF cluster search~\cite{johnson2019billion, 2021Facebook-DrBoost-Lewis} or proximity-graph-based navigation (e.g. HNSW)~\cite{2020TPAMI-HNSW}.
%2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
However, there is a  significant
relevance tradeoff for a  reduced search cost in these efforts. For example,
%Table~\ref{tab:costbudget} shows that  while 
RetroMAE~\cite{Liu2022RetroMAE}, a state-of-the-art  dense retriever,  is slow on CPU without compression, the use of
5\% IVF cluster search reduces CPU time substantially,  but there is a 7.5\% drop in MRR@10.
The use  of  OPQ quantization~\cite{Ge_2013_CVPR}, implemented in FAISS~\cite{johnson2019billion},
 further reduces memory space  to 1.2GB but causes an extra 4.6\% MRR@10 drop.
Notice that the idea of cluster-based retrieval was  explored for traditional information retrieval and 
selective search~\cite{ Can2004EfficiencyAE, 2008ACMTrans-Altingovde,2017ECIR-Hafizoglu}.

\comments{
\begin{table}[htbp]
        \centering
        \caption{Latency, relevance, and space  tradeoff  in a dense model for passage retrieval with MS MARCO Dev set}
         \resizebox{0.65\columnwidth}{!}{ 
                \begin{tabular}{ r | r  rr}
                        \hline\hline
Dense methods  & Latency & Memory & MRR@10\\ 
\hline
%BM25/DocT5Query & 9.2ms & 1.2GB & 0.261\\  
%SPLADE/HT1 & 31.2ms & 3.9GB & 0.396 \\ 
RetroMAE/Uncompressed & 1677ms & 27.2GB & 0.416 \\ 
%RetroMAE/IVF SimLM perhaps 3\% & 49.6ms  & 27.2GB & 0.389 \\ 
RetroMAE/IVF 5\% & 107ms  & 27.2GB & 0.387 \\ 
%SimLM /OPQ             & 225-1579ms & 0.57-2.3GB & $<$ 40.2 \\
RetroMAE /IVF 5\% OPQ  & 38ms & 1.2GB  & 0.370 \\ 
%RetroMAE SimLM? /IVF  OPQ  & 38-339ms & 1.2-2.3GB  & 0.37-0.401\\ 
                        \hline\hline
\end{tabular}
}

 \vspace*{-6mm}
\label{tab:costbudget}
\end{table}

}

%Quantization~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ} is another orthorgonal cost reduction technique.

LADR~\cite{2023SIGIR-LADR}, as a follow-up study of  GAR~\cite{2022CIKM-MacAvaneyGraphReRank},
has investigated partial dense retrieval by  
%pioneering in
using the results of a sparse retriever as a seed to select embeddings
%That takes an approach of augmenting sparse retrieval results using graph-based navigation based on
based on a document-to-document proximity graph. This strategy  follows  the previous graph-based ANN approaches~\cite{2020TPAMI-HNSW,2021KDDgraphANN}. 
Using  sparse retrieval results as a seed  allows quick narrowing of  the search scope at a low CPU cost. 
While LADR and HNSW have demonstrated their high efficiency  to search within a time budget,
the use  of a document-wise similarity graph adds a significant online space requirement  on a low-cost computing platform. 
%document-level  proximity graph  construction is  expensive for a large dataset during offline processing. 
For example, the proximity graph  can take an extra 4.3GB of memory space~\cite{2023SIGIR-LADR} for 
%an 
8.8M  MS MARCO passages. 
% to keep top 128 top neighbors per document.  
%The above studies focus on  a standalone dense search task given seed candidates, and our work
%exploits  the overlap degree  of sparse and dense retrieval, which yields additional  optimization benefits.


Another limitation of LADR and  HNSW is the assumption of in-memory access to proximity graphs and dense vectors. 
% PARKER's suggestion:
For large datasets or higher-dimension embedding vectors, some or all of the embeddings and proximity graphs will have to 
be stored on disk, especially when such  applications desire uncompressed embeddings for  better  relevance. 
For example, the embedding dimension of the recent RepLLaMA dense retriever~\cite{ma2023finetuning} is 4096,
based on LLaMA-2~\cite{touvron2023llama}, which leads to 145GB storage space for MS MARCO passages.
It is possible that more advanced dense models may be developed to take advantages of  
large language models in the future with  a higher  embedding dimensionality (e.g. up to 12,288~\cite{NEURIPS2020_GPT3}) for 
better relevance. 
For large on-disk search, random access of dense embeddings and/or a document-level proximity graph can incur substantial 
fine-grained I/O overhead.
DiskANN~\cite{NEURIPS2019_DiskANN} and SPANN~\cite{chen2021spann} are
two on-disk ANN search algorithms for general data applications which do not  leverage sparse retrieval.
They have not been studied in the context of text document retrieval
where the importance of semantic text matching presents a unique challenge and requires new design considerations.

One recently published contemporary work, CDFS~\cite{2024SIGIR-CDFS-Yang}, addresses the above problem. 
One weakness of CDFS is that it uses probabilistic thresholding to select  dense clusters  based on a strong assumption that 
the order statistics of query document ranking is independently and identically distributed. 
This assumption is only true when the query document similarity scores for relevant and irrelevant documents follow the same distribution, 
which is rare because they are usually distributed differently.
% to replace: 
%When the dense embeddings or its proximity graph of a large dataset cannot fit into memory, this approach that access can suffer 
%large random fine-grained I/O access overhead. 
%Nevertheless, random access of dense embeddings and/or graph nodes, as necessitated by document-level proximity graph navigation, can incur substantial
%fine-grained I/O overhead.  


This paper proposes a lightweight approach called CluSD 
(Cluster-based Selective Dense retrieval) guided by sparse retrieval results. 
Unlike CDFS, CluSD does not follow any statistical distribution assumption. 
%and  combine its results  with  sparse retrieval. 
%It exploits the relationship  of sparse and dense retrieval results to conduct  selective fusion. 
%To accomplish that, 
CluSD limits query-document similarity computations through a two-stage cluster selection algorithm via an LSTM model, which  
exploits inter-cluster distances and the overlapping degree between top sparse retrieval results and dense clusters.
%CluSD leverages IVF clustering and does not require an extra document similarity graph. 
%and then make a decision to determine which document clusters should be selected  to conduct dense retrieval computation.
%The extra memory space overhead required to assist CluSD is negligible, and it  works well when embedding vectors are highly compressed.
%  and  the extra memory required for hosting dense embeddings can be limited  after compression. 
When dense clusters are not available in memory, CluSD selects and loads a limited number of clusters with efficient disk block I/O.
Thus, it can handle a large dataset with high-dimension embeddings that cannot fit in memory, with less I/O overhead compared to a
graph-based ANN approach.  % TODO: second comma? split into two sentences? come back later Parker
\comments{
This paper evaluates  the effectiveness of CluSD  under time and space constraints on a CPU-only server in searching 
the MS MARCO and BEIR  datasets  when they fit or do not fit into memory.
%We compare against other baseline methods, including in a case in which the dataset does not fit into memory.
Our goal is to show that CluSD reaches state-of-the-art relevance levels at a much lower space and/or time cost including I/O if needed.

}

%0.419 by RocketQAv2~\cite{Ren2021RocketQAv2}, 
%which are the two latest state-of-the-art retrieval-reranker pipelines using cross-encoder based rerankers.
%Re-ranking in  recent dense retrieval work SimLM~\cite{Wang2022SimLM} has  used an expensive cross-encoder to
%achieve impressive MRR score 0.437 
%Re-ranking in RocketQAv2~\cite{Ren2021RocketQAv2} achieves 0.419 with a similar strategy.  



%Our evaluation also compares DiskANN~\cite{NEURIPS2019_DiskANN,2023Web-Filtered-DiskANN} and SPANN~\cite{chen2021spann}
%that conduct on-disk  ANN search, and these studies are targeted for general ANN search without considering the leverage of
%sparse retrieval.
%and  shows that CluSD can effectively identify a small number of document clusters to 
%for  a stronger relevance  even though such a sparse retriever is fast with a relevance tradeoff. 
 
%provides a flexibility and reduces  the relevance requirement of sparse retrieval.
%Namely protentially we can choose a relatively fast sparse retrival with the slightly lower accuracy/recall,
%and  let dense retrivsal make up the loss. Our design needs to  adapt to  sparse retrieval quality and plans

%CSDR in improving the effectiveness of sparse retrieval with limited dense  retrieval
%traversal for low-cost search.  Our emprial findings suggest that SRF  can consistently outperform other baselines on a single CPU server 
%without GPUs 
% and achieve a strong tradeoff of effectiveness and efficiency compared to an exhausitve dense retrieval search.


%from pretrained language models to provide effective semantic text matching between a query and a document.

