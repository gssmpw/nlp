%\section{SRF with Query-Specific Cluster Skipping }
%\section{CluSD: Selective Dense Retrieval}

  
\section{Selective Dense Cluster Retrieval}
\label{sec:clusd}
\comments{
It is common practice to interpolate the results from a sparse system and a dense system. We propose two directions to improve the latency of such system by avoiding the unnecessary dense evaluation on query level and per query level. We envision a hybrid system where sparse retrieval runs first and partial query and documents are evaluated by the dense retrieval system. 

The proposed CluSD method contains two components  during runtime inference to 
decide if  dense retrieval augmentation  can be skipped for a query and which  top dense document clusters should be selected  during supplemental retrieval.
}

\subsection{Selection of dense clusters}
%Per-query inference latency depends on the number of document evaluated and how per query-document evaluate is approximated. We do not touch per-query-document approximation in this paper and focus on reducing the number of documents evaluated. We cluster document using kmeans as IVF and would like to intelligently select a few most promising clusters to retrieve candidate results. 
\comments{
\begin{figure}[h!]
    \centering
    \includegraphics[scale=1.0]{lstm.png}
    \caption{LSTM prediction for each query}
    \label{fig:lstm}
\end{figure}
}
As discussed earlier, documents with embedding vectors are clustered based on their similarity to the cluster centroids. 
\comments{
Our experiments take an advantage of the FAISS library which  provides such a support such as k-mean clustering.
Once a query is determined by CluSD to trigger  partial dense retrieval that selects and visits a subset of dense clusters. 
% online inference  will  visit the dense clusters one by one in
%an incresing order of   their centroid distance to the input query.
%A decision procedure is invoked to decide if a cluster to be visited should be skipped or not. 
}
Our objective is to examine the features of each cluster and  incrementally decide if this cluster should  be visited or not.

\begin{figure}[htbp]
    \includegraphics[scale=0.75]{dense_separse_feature_lout.drawio.pdf}
    \vspace*{-1mm}
    \caption{Illustration of CluSD and its features.}
    \label{fig:features}
     \vspace*{-5mm}
\end{figure}

{\bf LSTM design.}
%Because this decision procedure uses the document scoring information  of each cluster visted 
We adopt a simple LSTM model based on  a recurrent neural network~\cite{hochreiter1997long}, 
taking a sequence  of clusters as an input along with their features.
Assume there are $N$ document clusters grouped using dense document embeddings, and 
it is time consuming for the LSTM model to examine all clusters. Thus
 we truncate the sequence of clusters at maximum length $n$ and then take a sequence of top $n$ clusters as the input to this LSTM. 
There are  $n$ nodes corresponding to these clusters in the LSTM model.
We will discuss how to truncate with a prioritization shortly below.



\comments{
The LSTM model outputs 1 for each cluster node if this cluster should be visited to perform the corresponding dense retrieval.
The goal of this procedure  is to visit  dense clusters as few as possible while still delivering a competitive fusion performance.
The number of predicted clusters are different across queries and the overall budget is controlled by the prediction threshold.
}
%  a few clusters as possible to recover the top results from the original dense retrieval. 
%The label of each query-cluster is 1 when the cluster contains the top k results of the query in the exact full search of dense retrieval.  
%The input is the a sequence of cluster nodes in the order of the centroid-query distance. 
%The feature of each input include: statistics on query-centroid distance, centroid-to-centroid distances, distribution of sparse top results across clusters. 
%The model will predict for each query which clusters to choose. 
%The input features of our proposed LSTM model are listed below.
%These clusters are sorted by the the similarity distance of the given query with the  centroid of each cluster.



Given the above  $n$ top clusters,
 the LSTM computation flow goes from the left most cluster called $c_1$ to the right most $c_{n}$.
Assume the current cluster is $c_i$,  
the LSTM computation outputs a predicted value $f(c_i)$ where $0\leq f(c_i) \leq 1$.
If $f(c_i) \ge \Theta$, where $\Theta$ is a predefined prediction threshold, then cluster  $c_i$ should be visited. In practice, the strictness on selecting a cluster to evaluate is controlled by $\Theta$ and it can be determined based on the overall latency requirement. 

\comments{
When the dense vectors of all documents are grouped into $N$ clusters, 
we limit our cluster search scope into top $n$ clusters in terms of query and cluster centroid similarities. 
we limit $n=128$ in our evaluation out of 1000 clusters.
%Query-specific information derived from  dense clusters:
}

As shown in Figure~\ref{fig:features}, the feature input vector for the current cluster  $c_i$ for the above LSTM computation to produce $f(c_i)$
%$f(q, c_i)$  
is composed of:
\[
[ sim(q,c_i),   
\{ AvgDist(sim(c_i, c_l) \text{ for } c_l \in A_j) \}^6_{j=1}, 
\]
\[ 
     \{  |c_i  \cap  B_j|    \}^{6}_{j=1},  
\{  AvgScore (c_i  \cap  B_j)    \}^{6}_{j=1} 
]. 
\] 
\begin{enumerate}[leftmargin=*]
    \item $sim(q,c_i)$: The similarity distance of this query $q$ with the centroid of cluster $c_i$.  
  %\item  $\{ sim(c_i, c_j) \}^{n-1}_{j=0}$: The distance vector between the centroid of $c_i$ to  all other clusters.
%the top  clusters that have been visited (or skipped).  We limit this number as $K=128$.
%other top k centroids for this query (k=128)??
  
\item  \{  $AvgDist(sim(c_i, c_l)) \text{for } c_l \in A_j \}^6_{j=1}$: 
The mean distance between this centroid and the centroids in each of six position bins $A_j$.
%These six bins  are formed by partitioning all clusters into consecutive position bins.
%For example, $A_1$ contains clusters from Postions  1 to 5, Positions 6-10, 
%$A_2$  contains clusters from Postions  6 to 10, and so on.   Bin $A_6$ contains the rest of uncovered clusters.  
These six position bins  are formed by cluster centriods. 
After all the $N$ centroids are ordered based on a criterion discussed shortly, we partition the sequence of sorted clusters into consecutive position bins.
These bins respectively contain clusters in Positions  1-5, Positions 6-10, 
Positions 11-20, Positions 21-50, Positions 51-100, and the rest of N clusters. 
The purpose of this feature group is to capture the distances of a cluster to previously examined clusters and unexamined clusters following the LSTM flow. 
When the mean distance  between cluster $c_i$  and clusters in a previously-examined  bin is close, if  many clusters in that bin have been selected, cluster 
$c_i$ may have a good  chance to  be selected.

%Since the clusters are processed from a lower position to a higher position one by  one,
%these features represent the distance to the groups 
    
    \item $ \{  |c_i  \cap  B_j|    \}^{6}_{j=1} $ and $\{  AvgScore (c_i  \cap  B_j)    \}^{6}_{j=1} $: 
The number of documents and average sparse ranking score from this  cluster contained in each of   the six bins $B_j$  formed by
top $k$ sparse retrival results. In our evaluation, we limit the retrieval depth to be $k$ where $k = 1000$.
Top $k$ sparse results are divided into six bins:
top 10, top 11 to 25, top 26-50, top 51-100, top 101-200, top 201-top 500, and 
top 501 to k documents.  These bins are called $B_j$ where $1\leq j \leq 6$. 
The purpose of this feature group is to capture the overlap degree  of a cluster with the top results of sparse retrieval.
A cluster that contains many top sparse results may be selected.
    %\item $\{  AvgScore (c_i  \cap  B_j)    \}^{6}_{j=1} $: 

% bin 
%top 10/25/50/100/200/500/1000 documents  in the sparse retrival results.
\end{enumerate}



%Section~\ref{sect:evaluation} provides more details on the effectiveness 
 

\textbf{Re-order candidate clusters for prioritization and pruning.} 
The inference time of an LSTM model is proportional to its input length ($n$) in terms of
the number of clusters to be examined. We denote $n$ as the  input length  to our LSTM  throughout the rest of this paper.
Table ~\ref{tab:LSTMclustertime} shows that as the number of clusters to be examined
varies from 32 to 512, the LSTM inference time increases from  2.8ms to 33.9ms.
As we would like to make a quick decision given the overall  time budget,
we devise another strategy to prune clusters that may not provide a meaningful boosting
of relevance quality. 

% dependent on the length of the sequence input. 

\begin{table}[h]
	\centering
		\resizebox{0.68\columnwidth}{!}{ 
%		\begin{small}
		\begin{tabular}{r | rrrrr}
			\hline\hline
			  \#Clusters examined & 512 & 256 & 128 & 64 & 32 \\
\hline
            Time (ms) & 33.9 & 16.2 & 6.2 & 3.8 & 2.8\\
                \hline\hline
		\end{tabular}
%		\end{small}
	}
	\caption{LSTM inference time with different input length.}
	\label{tab:LSTMclustertime}
 \vspace{-7mm} 
\end{table}

\comments{
\begin{figure}[htbp]

    \includegraphics[scale=0.27]{dist.pdf}
    \caption{Distribution of Good Documents
}
    \label{fig:dist}
%\vspace{-1em} 
\end{figure}
}

To prune clusters, one may prioritize a few top clusters sorted by the query-centroid distance.
We find this does not work well in the context of augmenting sparse retrieval results.  
Table~\ref{tab:dist} shows the distribution of top 10 documents from the full dense retrieval  
in embedding clusters under the different query percentile when searching an MS MARCO passage dataset using queries from its  Dev set. 
Row 2 of Table~\ref{tab:dist} shows the number of top clusters needed to cover  top 10 
documents from the full dense retrieval  when clusters are sorted by sorted by  the query-centroid distance.
This row shows that to cover top 10 dense results, $n$ needs  to be larger than 175.
%On the other hand, Row 2 is  the number of lusters decided by the LSTM to visit  under the different query percentiles. 
%At most  66 clusters are  selected  by the LSTM model for all queries.
%We further propose to skip unnecessary clusters. 
\begin{table}[h]
 \vspace*{-1mm}
	\centering
		\resizebox{0.78\columnwidth}{!}{ 
%		\begin{small}
		\begin{tabular}{r |r  r r r r}
			\hline\hline
			  Query percentile & 10\% & 50\% & 75\% & 90\% & 95\% \\
            \hline
%            \#clusters selected by  LSTM & 1 & 4 & 14 & 40 & 66\\
            %Relevant doc coverage & 1 & 2 & 7 & 29 & 69\\
             Top 10 dense result coverage & 1 & 6 & 25 & 92 & 175\\
                \hline\hline
		\end{tabular}
%		\end{small}
	}
	\caption{ Distribution of  top 10 dense results in sorted clusters 
% \% of Good Documents Among Clusters Sorted by Query-Centroid Distance
}
 \vspace*{-5mm}
	\label{tab:dist}
\end{table}

%provides an explanation.  evidence on the effectivness of the above LSTM model.
%After searching an MS MARCO passage dataset using queries from the MAMARCO dev set, 
%under the different query percentile shown in Row 2 is  the number of clusters decided by the LSTM to visit.
%Row 3 is  the number of clusters needed to 
%For a lot of queries, the top 5 or even top 10 clusters do not contain enough information, for example, around 10\% of queries need more than 92 top clusters to recover the top results from the full dense retrieval. It demonstrates the necessity of doing cluster selection and skipping.  
%On the other hand, there is a reasonable distribution resemblance between  Row 2 and  Row 3, which shows the LSTM model provides a reasonable guidance on where relevance results are.
%%While it does not exactly follow the distribution of top 10 results of sparse retrieval, the feature 
%From  Table~\ref{tab:dist}, we observe that
%the distribution of top 10 results from a sparse are contained is not necessarily concentrated in the top  clusters.
Since our ablation  analysis finds that Group  (3) of the above feature list
is important for the accuracy of LSTM to make a sound cluster selection, we resort to this feature group in making 
a preliminary selection of  necessary clusters. 
%With the above design consideration,  we propose to use the following strategy to 
%sort the candidate clusters based their feature vectors from Group  (3).
Specifically we sort the candidate clusters based on the overlap degree of a cluster with the six top sparse retrival result bins. 
If there is a tie between two clusters, then we sort them by their centroid distance to the query.
%We first use a simple heuristic to reorder the clusters, refered as Top K heuristic. 
%We order the clusters based the six features then 
%followed by q-c-orders. 
Then, the sorted  top $n$ candidate cluster list is used as the input sequence to the LSTM model. 
%We will stop the LSTM computation of  this list
%after visiting $\tau$ clusters and when the predicted $f(c_i)$  values of a few consecutive clusters are all below a threshold. 
%For our evaluation in MS MARCO and BEIR datasets, $\tau=20$. 
In our evaluation, we find that a relatively small number such as $n=32$ is a solid choice  to yield competitive results  after the above cluster sorting and prioritization.
%As a result, the LSTM computation for cluster selection can be completed within  1.8 milliseconds. 
%The number of clusters selected varies from a few to 20 on average, which yields a quick argumentation with dense cluster results.


%Features marked with Hexgos 1 and 4 are used for a boosting tree to make a decision if cluster selection should be triggered or not.
\comments{
\subsection{Query filtering}
%When computing the embedding of each query token or the whole  original query, we use 
%the denser retriever's encoder. 


We also exploit the possibility to skip dense retrieval completely.
Modifying the above LSTM-based decision mechanism does not yield a satisfied result.
We sort to add a classifier with  boosted decision trees with supervised learning
to make an earlier  determination before triggering the LSTM computation.
}
\comments{

a query needs an argumentation from dense retrieval 
%A challenge is to identify features and infer based on such features with supervised learning.
Given that sparse retrieval is conducted first,
our idea is to predict the quality of sparse retrieval and its relative strength  compared to unexecuted
dense retrieval based on quality of query token expansion by the sparse model,
the statistical behavior of  sparse retrieval results, and
the relationship between dense document embedding summary and sparse retrieval scoring structure.

}


\comments{
\begin{table}[ht]       
    \centering          
%    \resizebox{1.05\columnwidth}{!}{

\begin{small}                        

    \begin{tabular}{l |l}
        \hline
    Query    & "how long nyquil kicks in"\\
    \hline
 Query Expanded&["in", 36],  ["time", 74], ["years", 71], ["long", 97], \\
 & ["total", 60],\textcolor{red}{["move", 23]}, ["effect", 22], \textcolor{red}{["weapon", 21]}, \\
 & \textcolor{red}{["jump", 24]}, ["kick", 100], ["ny", 111], ["timing", 34], \\
 & ["kicks", 76], ["\#\#quil", 139]\\
    \hline  
Relevance Doc &["\#\#quil", 410.1], ["ny", 301.0], ["long", 216.7], \\
& ["time", 134.7], ["years", 131.5], ["total", 87.4],\\
& ["effect", 34.4], ["timing", 17.4]\\
\hline
Top 5 Doc & ["\#\#quil", 392.6], ["ny", 295.7], ["kick", 236.1], \\
& ["long", 198.3], ["time", 141.2], ["years", 116.9], \\ 
& ["kicks", 101.8], ["total", 86.3],\\
& ["in", 35.3], ["effect", 24.1], ["timing", 23.6],\\
& \textcolor{red}{["jump", 10.0], ["move", 6.0]}\\
        \hline
    \end{tabular}
\end{small}                        
%}     

\caption{An example of weak query expansion in SPLADE} 
\label{tab:queryencoder}
\end{table} 
}

\comments{
\begin{table*}[ht]       
    \centering          
%    \resizebox{1.05\columnwidth}{!}{

\begin{small}                        

    \begin{tabular}{l  l l}
        \hline\hline
    Query   &  \multicolumn{2}{l}{ "why do we push others away"}\\
    \cline{2-3}
&  \multicolumn{2}{l}{('push', 105), ('away', 104), ('because', 97), ('others', 94), ('people', 44), ('why', 44), ('purpose', 43), ('cause', 42), (',', 40), ('we', 38),} \\
&\multicolumn{2}{l}{ ('other', 38), ('effect', 33), ('them', 29), \boldred{('responsibility', 28),} ('help', 20), \boldred{('move', 19), ('severe', 17),}  ('step', 17), ('do', 16),...}\\
%& & ('things', 15)ï¼Œ ('fear', 15), ('method', 10), ('to', 6), ('language', 5), ('anger', 5), ('when', 2), ('feeling', 2), ('\#\#ness', 1), ('dangerous', 1)]\\
    \hline  \hline
   Doc Relevant? & \ rank & Text and Expanded Tokens \\
\hline
\comments{
Yes & 17 & From a psychological standpoint, pushing away the people you love the most is a very basic and common, defense mechanism.\\
&& As the relationship develops, people become inundated with their own fears and insecurities that they will not be accepted\\
&& and therefore hurt by their loved one.\\
\cline{3-3}
& & [('push', 198), ('away', 196), ('fear', 127), ('people', 104), ('effect', 81), ('because', 70), ('severe', 67), (',', 61), ('method', 61),
\\
& &  ('when', 59), ('anger', 58), ('dangerous', 55), ('feeling', 53), ('purpose', 53), ('responsibility', 53), ('move', 44), ('cause', 43),... \\
%& & ('them', 37), ('things', 31), ('\#\#ness', 27), ('step', 23), ('why', 13), ('help', 9), ('to', 3), ('do', 1), ('language', 0)]\\
\specialrule{.1em}{.05em}{.05em} 
}
No & 10 & Different types of push factors can be seen further below. A push factor is a flaw or distress that drives a person away from \\
&&  a certain place. A pull factor is something concerning the country to which a person migrates. It is generally a benefit that  \\
&&attracts people to  a certain place. Push and pull factors are usually considered as north and south poles on a magnet.\\
\cline{3-3}
&& [('push', 283), ('away', 162), \boldred{('move', 80),} \boldred{('severe', 74)}, ('people', 70), ('because', 68), ('effect', 58), ('cause', 44), ('purpose', 43),\\
& & \boldred{('responsibility', 41),} (',', 36), ('method', 35), ('\#\#ness', 33), ('step', 23), ('them', 22), ('to', 18), ('when', 13), ...\\ 
%& &  ('things', 11), ('anger', 9), ('dangerous', 8), ('fear', 3), ('do', 0)]\\
\specialrule{.1em}{.05em}{.05em} 
No & 8& Most of the universe is rushing away from us. It\textbackslash{}'s not that we\textbackslash{}'re particularly repellent; it\textbackslash{}'s just that the universe is\\
&&  expanding, pushing most other galaxies away. Light from distant galaxies travels toward us through this \\
&& expanding space, which stretches their light to longer, or redder, wavelengths.\\
\cline{3-3}
&& [('away', 184), ('push', 158), ('other', 111), \boldred{('move', 103)}, ('we', 101), ('effect', 83), (',', 73), ('because', 66), ('method', 53),\\
&&  ('others', 39), ('cause', 39), ('severe', 39), ('them', 36), ('do', 29), ('things', 29), ('dangerous', 29), ('feeling', 28), ... \\
% & & ('when', 27), ('help', 19), ('purpose', 13), ('to', 4), ('why', 2)]\\
         \hline\hline
    \end{tabular}
\end{small}                        
%}     

\caption{An example of weak SPLADE query expansion that boosts  irrelevant documents  } 
\label{tab:queryencoder}
\end{table*} 

Our consideration 
%is that the necessity of invoking a dense retrieval model after running a sparse model depends on 
%\begin{enumerate}
%the relative strength of results scored by this sparse retriever. If a
    is that  if a chosen
sparse retriever has  well-performed, the benefit of addition of dense scoring  is limited. 
Notice that a learned neural sparse retriever expands the original query with a transformer-based encoder
We found that the quality of query encoding with expansion in SPLADE impacts the quality of sparse retrieval. 
%and we form features using both the original query  and the expanded query.
When  there is a low similarity value  between the original query and the tokens of 
its expanded  query, the confidence for  strong sparse retrieval can be low.
Table~\ref{tab:queryencoder} gives such an example and lists an original query and its expanded query tokens
sorted by token weights.  Expanded query words  in red color
not highly relevant to the original query and they are weighted high in two irrelevant documents, which boosts their ranking.  
Thus    excessive presence of un-similar  query tokens  after expansion  can misrank irrelevant documents. 
Another consideration is that the alignment of sparse retrieval ranking  with the 
query-cluster based similarity can shed light  on the necessity of  dense retrieval.
When top sparse retrieval results are concentrated in a few top  ordered dense clusters, dense retrieval may bear
a resemblance  in  ranking.


With the above consideration  in mind, we use the  following categories of features
to train a boosted decision tree model.

}
% with these featuers an input by using the xgboost package. 
\comments{
To evaluate the performance of the sparse system, we design some features to describe the quality of sparse encoder. 
For example, the length of expanded query, distribution of token impact scores and their corresponding matching scores with top k documents.

To evaluate the discrepancy between the performance of sparse and dense system. We encode the original query with the dense retrieval encode and each expanded token and generate distributional features around the similarity between them. 


%We take a supervised approach to the construction of a query filter that classifies  a query after sparse retrieval 
%needs an argumentation from  dense retrieval or not. We consider the following two categories of the feautres.
}
\comments{
\begin{itemize}[leftmargin=*]
\item Quality of query expansion by the sparse retriever model.
    For example, the query-level matching score, define as the similarity  between the CLS embeddings of the orginal query and the expanded query.
    Another one is the token-based matching histogram vector between the expanded query and the original query. 
For each query token in the expanded query, we evaluate the matching score in terms of similarity
 between its token embedding and  the oringal query embedding. 
We map  these values into five bins with  ranges <0.5, 0.5-0.6, 0.6-0.7, 0.7-0.8, >0.8.
Then derive a matching histgram count  vector for each bin.
}

\comments{
{\bf WILL DELETE THIS LIST.}
\begin{enumerate}
    \item The length of the original query and the expanded query in terms of  the  number of words or tokens, respectively. 
    \item The token-based matching  vector between the expanded query and the original query. 
For each query token in the expanded query, we evaluate the matching score in terms of similarity
 between its token embedding and  the original query embedding. 
We map  these values into five bins with  ranges <0.5, 0.5-0.6, 0.6-0.7, 0.7-0.8, >0.8.
Then derive a matching count vector based on this bin mapping???
    \item The query-level matching score, define as the similarity  between the CLS embeddings of the original query and the expanded query.
    \item The sum of matching scores within each of above token similarity bin, divided by
the  query-level matching score. 
%The token groups are the tokens with similarities in each bin.
    \item The sum of matching scores of tokens not in NLTK English corpus, divided by  the query-level matching score. 
    \item The number of overlapping tokens between expanded and original query / number of all unique queries in both expanded and original query.
    \item number of  tokens only in expanded query / number of all unique queries in both expanded and original query.
    \item overall/mean/max of token weight of overlapping tokens between expanded and original query in the expanded query.
    \item max/min/mean of token weights of expanded query tokens.
\end{enumerate}

}
%\item
\comments{
Ranking characteristics of documents scored by sparse retrieval.
We compute  the mean sparse retrieval score of top 10-20/20-30/30-40/40-50/50-100/100-400/400-700/700-1000 documents.

%Sparse  retrival score distribution:
\begin{enumerate}
    \item Sparse retrieval scores of top 10 results
    \item The mean sparse retrieval score of top 10-20/20-30/30-40/40-50/50-100/100-400/400-700/700-1000 documents.
\end{enumerate}
%Sparse and Dense relational features:
\item Overlapping degree of sparse retrieval with dense clusters:
\begin{enumerate}
    \item The number of dense clusters containing top 5/10/20/50/100/1000 results from sparse system
\end{enumerate}
}

%\end{itemize}


%\subsection{Partial Document Evaluation}
\subsection{ Online inference and model training}

\begin{figure}[htbp]
    \includegraphics[scale=0.75]{sparse_dense_flow_lout.drawio.pdf}
    \caption{The flow of cluster-based selective dense retrieval}  
 \vspace*{-1mm}
    \label{fig:flow}

\end{figure}
As shown in Figure~\ref{fig:flow}, the online inference algorithm is as follows:

1) Conduct sparse retrieval to obtain top $k$ results.

%\item Condcut the query filter  to determine if the cluster-based dense retrieval is necessary.If needed, 
2) Sort clusters based on the overlap degree of dense clusters and top sparse retrieval results and their  centroid-to-query  distances.

3) Call the LSTM to choose   dense clusters. 

4) Expand top $k$ sparse retrieval results to include the document vectors of these dense clusters and
merge their sparse and dense scores with a linear interpolation.  

%During the above linear interpolation, only top $k$ of sparese retrieval contains a lexical score,
%and thus we  impute the lexical scores of other documents expanded by cluster-based selective dense retrieval 
%as the bottom document score in the top $k$ sparse retrieval list scalled by a factor. We use 0.95 in our evaluation.
\comments{
Notice that  the two ranked lists from two retrievers are usually not fully overlapped and  some documents only exist in one list. While
different  treatments on these documents affect fusion performance,
we resort  to  impute the score of such a document using  the top $k+1$ score   in the missed rank list scaled by 
a factor.
%While there are options to We consider a few options
For example,  we  impute the lexical scores of new documents added  by dense retrieval
using  the scaled lexical score of  top $(k+1)$ in the sparse retrieval list. We use scale $0.95$  in our evaluation.
}
%\item Remove the documents when they only exist in one ranked list
%    \item  Take the document and compute the score for only one ranked list

%When we have access to a relatively strong sparse retrieval system and a dense retrieval system, it is feasible to selectively choose queries that requires evaluation of both systems. We hypothesize that the sparse system is economical and we can afford to run it for every query. The problem boils down to dynamically select queries to evaluate by dense retrieval.

{\bf Online computing and space  cost for augmentation.} We only consider cost of  augmenting sparse retrieval results with dense retrieval during online inference. 
The space cost of dense retrieval is $O(D)$  for a corpus with $D$ vectors.
For MS MARCOS, the cost is around 27GB without compression. Quantization can  reduce that  0.5GB  to 1GB or less based on codebook parameters.
Then this cost becomes a fraction of sparse retrieval index cost.  
Excluding dense embeddings, the extra space overhead  for CluSD is to maintain  the similarity among centroids of $N$ clusters, which is  $O(N^2)$.
This cost is  negligible compared  to the embedding space with $N << D$.  In our evaluation, 
%we use  $N=8192$ or  $4096$.
the inter-cluster similar graph for MS MARCO passages with $N=8192$ takes about 32MB with quantization.

The time  cost of CluSD is dominated by the cost of cluster selection and  dense similarity computation of selected dense clusters.
The former is about $O(n)$ while the later is
proportional to the number of clusters selected, which is capped by $O(\frac{D}{N} n)$. 
In  our evaluation of MS MARCO and BEIR, the actual number of clusters selected and visited is relatively small.
%Given the short time added by the pruned LSTM model, 
The overall latency overhead introduced by CluSD is fairly small. 





{\bf Training of cluster selection.} 
We assume that the training data  contains
a set of queries and relevance-labeled documents,  that allows the computation of a ranking metric such as MRR or NDCG.
For example, from MS MARCO training set we randomly sample 5000 training instances, where each training instance contains a query and  one or more  relevant documents. 
%Then we run a sparse retriever this instance, and seperately run a fusion with a dense retriever. If the result of sparse retriever results merged with dense results  exceeds the standalone sparse retrival,  we label consider training instance as an positive example for dense retrieval, otherwise consider this as a negative example. The converted training isntances are used for training In this way, we convert a set of training instances for ranking as a set of training instances for fusion.
To train the LSTM for cluster selection, 
for each training instance, we mark the positive and negative examples for  top clusters.
If a cluster contains one of top 10 sparse retrieval results, we mark this cluster should be visited.
Otherwise this cluster should not be visited.

 
\comments{
We convert We define the labels as 
\begin{equation}
       y(q)= 
\begin{cases}
    1,& \text{if } U(s(q), d(q)) > U(s(q))\\
    0,              & \text{otherwise,}
\end{cases}
\end{equation}
where $U(.)$ is a evaluation metric we choose, for example, NDCG@10 or MRR. $s(q)$ is the scoring of a sparse system of query $q$ and $d(q)$ represents the scoring of a dense system.
}

\comments{
\subsection{How to combine two ranked list}
There are two commonly used method to combine the sparse and dense ranked lists:
\begin{enumerate}
    \item linear combination
    \item reciprocal rank fusion
\end{enumerate}
For linear combination, the final ranking score of a document is the weighted sum of the ranking score of two systems respectively. The weight on each system can be determined through grid search on a left-out training set or just use 1:1 combination. In this case, rescaling of the two ranked lists are necessary as the it largely affects the impact of a ranked list to the merged ranked list. 

On the other hand, reciprocal rank fusion alleviate the scoring distribution difference by only considering the ranking positions when merging. 
  Another consideration is on the missing values. The two ranked lists are usually not fully overlap. Some documents only exists in one ranked lists. Thus 
the treatment on these documents affects model performance. We consider a few options
\begin{enumerate}
    \item Remove the documents when they only exist in one ranked list
    \item  Take the document and compute the score for only one ranked list
    \item Impute the missing document with rank K+1 or score as 0.95 * min(score) in the ranked list.
\end{enumerate}

}


