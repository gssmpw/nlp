@article{Hinton2015Distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Geoffrey E. Hinton and Oriol Vinyals and Jeffrey Dean},
  journal={ArXiv},
  year={2015},
  volume={abs/1503.02531}
}

@article{Wang2022SimLM,
  title={{SimLM}: Pre-training with Representation Bottleneck for Dense Passage Retrieval},
  author={Liang Wang and Nan Yang and Xiaolong Huang and Binxing Jiao and Linjun Yang and Daxin Jiang and Rangan Majumder and Furu Wei},
  journal={ACL},
  year={2023}
}
  %volume={ArXiv abs/2207.02578}

@inproceedings{Shi2021ProKT,
  title={Follow Your Path: a Progressive Method for Knowledge Distillation},
  author={Wenxian Shi and Yuxuan Song and Hao Zhou and Bohan Li and Lei Li},
  booktitle={ECML/PKDD},
  year={2021}
}

@inproceedings{Zhou2021MetaDistil,
  title={{BERT} Learns to Teach: Knowledge Distillation with Meta Learning},
  author={Wangchunshu Zhou and Canwen Xu and Julian McAuley},
  booktitle={ACL},
  year={2021}
}

@article{Zeng2022Curriculum,
  title={Curriculum Learning for Dense Retrieval Distillation},
  author={Hansi Zeng and Hamed Zamani and Vishwa Vinay},
  journal={SIGIR},
  year={2022}
}

@article{Zhang2022AR2,
  title={Adversarial Retriever-Ranker for dense text retrieval},
  author={Hang Zhang and Yeyun Gong and Yelong Shen and Jiancheng Lv and Nan Duan and Weizhu Chen},
  journal={ICLR},
  year={2022}
}

@article{Xiao2022Distill-VQ,
  title={{Distill-VQ:} Learning Retrieval Oriented Vector Quantization By Distilling Knowledge from Dense Embeddings},
  author={Shitao Xiao and Zheng Liu and Weihao Han and Jianjin Zhang and Defu Lian and Yeyun Gong and Qi Chen and Fan Yang and Hao Sun and Yingxia Shao and Denvy Deng and Qi Zhang and Xing Xie},
  journal={SIGIR},
  year={2022}
}
