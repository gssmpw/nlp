%\subsection{CluSD with other sparse models and quantization methods }
\label{sect:evalsparse}
{\bf CluSD with other sparse models and quantization methods}.
%We explore the model performance when we interpolate different sparse model with SimLM with cluster selection in 
Table~\ref{tab:sparse} examines CluSD's fusion of SimLM
with  other three sparse models.
Last column lists in-memory dense retrieval portion of latency.
% under  different efficiency/relevance tradeoffs.
For each sparse  model, CluSD boosts overall relevance.
%These sparse models accomplish  different relevance  scores: 
%CluSD fused with  the   fastest BM25T5 model delivers the lowest MRR number while its fusion  with the slowest LexMAE delivers
%the highest MRR@10 number.
%The final relevance outcome of using two SPLADE models is better  than that for BM25 retriever.
%That is partially caused by dependence of LSTM training where  the positive clusters are identified by the overlap of top 10 SPLADE results with these  clusters.
As  CluSD's cluster selection  relies on  the overlap feature  of top sparse results with dense clusters,
the guidance from the fastest BM25 results is less accurate than other  sparse models. On the other hand, with a guidance from 
the slowest but stronger LexMAE retrieval, CluSD yields the highest MRR@10, comparable to full search. 
%the quality of top sparse results still has an impact on the final outcome.  
%consists of the distribution of sparse top results into clusters. 
%When the sparse model doesn't identify good top candidates, the input features to the LSTM model becomes more noisy which affect performance. 
\comments{
From left to right, the model performance degrades. 
We can observe that when sparse model performance degrades, 
the performance of S + CluSD degrades as well on the flat index. 
For example, on Splade-HT1-fast, the relevance is around the same as S $+$ D on MRR@10, and 0.6\% degradation on recall@1K. 
When we use BM25-T5 model, the degradation is 1.7\% and 1.5\% respectively on MRR@10 and recall@1K. 
This indicates that when the Splade model model does not perform well, CluSD becomes less effective. 
It is expected because the input to the LSTM model consists of the distribution of sparse top results into clusters. 
When the sparse model does not identify good top candidates, the input features to the LSTM model become more noisy which affects performance.
}

\comments{
\begin{table}[htbp]
	\centering
		\resizebox{\columnwidth}{!}{%
		\begin{tabular}{ r l l  |ll |ll |r }
			\hline\hline
                Sparse model& \multicolumn{2}{c|}{\bf{Splade-HT1-fast}} & \multicolumn{2}{c|}{\bf{Splade\_effi-HT3}}   & \multicolumn{2}{c|}{\bf{BM25-T5}} & Extra\\ 
%\cline{1-7}
%                Latency & \multicolumn{2}{c|}{22.4ms} & \multicolumn{2}{c|}{12.4 ms}   & \multicolumn{2}{c|}{9.2 ms} & Latency\\ 
\cline{1-7}
			& {MRR} & {R@1K}  & {MRR@10} & {R@1K}   & {MRR} & R@1K  & latency \\
   \hline
   \multicolumn{8}{c}{\bf{Uncompressed SimLM embeddings with  27.2 GB space}}\\
			\hline
			S & 0.385$^\dag$& 0.949$^\dag$& 0.380$^\dag$& 0.944$^\dag$& 0.259$^\dag$& 0.935$^\dag$& -\\
                $\blacktriangle$  S $+$ D & 0.416 & 0.986 & 0.413 & 0.984 & 0.415 & 0.985  & +1674.1\\
                {\textbf{S $+$ CluSD}} & 0.415 & 0.981$^\dag$& 0.411 & 0.979$^\dag$& 0.408& 0.970$^\dag$& +12.1 \\
                \hline
                \multicolumn{8}{c}{\bf{OPQ m=128. SimLM embedding space  1.1 GB}}\\
%, Report S+*
\hline
                S $+$ D-IVFOPQ & 0.384$^\dag$& 0.981$^\dag$& 0.381$^\dag$& 0.978$^\dag$& 0.369$^\dag$& 0.975$^\dag$& +21.9 \\
                %S $+$ D=HNSWOPQ & & & & & & &  \\
                \bf{S $+$ CluSD} & 0.413& 0.984& 0.412& 0.984& 0.402$^\dag$& 0.975$^\dag$& +9.8  \\
                %\hline
                %\multicolumn{8}{c}{OPQ M=64. Embedding space  0.6 GB}\\
%, Report S+*
                % S $+$ IVFOPQ-5\% & 0.389 & 0.981 & 0.389 & 0.980 & 0.370 & 0.975 & 23.7 \\
                %S $+$ HNSWOPQ & & & & & & &  \\
                %S $+$ \bf{CS} & 0.402& 0.981& 0.402& 0.981& 0.380& 0.973& 10.2 \\
            \hline\hline
		\end{tabular}
		}
	\caption{CluSD performance  under  other  sparse models}

\vspace*{-5mm}
	\label{tab:sparse}
\end{table}
% For all three sparse models, we use the same dense model configuration so for each row we only report one latency number. 
}

\comment{
\begin{table}[htbp]
	\centering
		\resizebox{\columnwidth}{!}{%
		\begin{tabular}{ r r r r r  r r r}
			\hline\hline
			& \multicolumn{2}{c}{MSMARCO Dev}& \multicolumn{2}{c}{DL19}& 
\multicolumn{2}{c}{DL20} \\
			& \% n& MRR@10 & R@1K & NDCG@10 & R@1K &  NDCG@10 & R@1K\\
   			\hline
 D& -- & 0.416& 0.988& 0.720& 0.744& 0.703&0.755\\
            $\blacktriangle$  S $+$ D & -- & 0.425 & 0.988 &  0.740& 0.816&   0.731& 0.819\\
            \bf S $+$ \bf{CluSD} & 0.5 &  0.426 & 0.987 &  0.744&  0.820& 0.734&  0.818\\
            \hline
            \multicolumn{8}{c}{OPQ M=128 setting, Storage 1.1GB, Report S+*}\\
            \hline
            IVFOPQ &10 & 0.404 & 0.987 & 0.713 & 0.818 & 0.722 & 0.817 \\
            & 5 & 0.394 & 0.987 & 0.687 & 0.817 & 0.706 & 0.817 \\
            & 2 & 0.374 & 0.986 & 0.656 & 0.815 & 0.700 & 0.817 \\
            HNSWOPQ & & & & & & \\
            \bf{CS} & 0.5 & 0.417& 0.986& 0.742 & 0.818 & 0.735  &0.815 \\
            \hline
            \multicolumn{8}{c}{OPQ M=64, Storage 0.6 GB, Report S+*}\\
            \hline
            IVFOPQ & 10 & 0.393 & 0.986 & 0.676 & 0.814 & 0.730 & 0.818 \\
            & 5 & 0.384 & 0.985 & 0.659 & 0.814 & 0.717 & 0.817 \\
            & 2 & 0.368 & 0.985 & 0.643 & 0.814 & 0.707 & 0.816 \\
            HNSWOPQ & & & & & & & \\
            \bf{CS} & 0.5  & 0.403& 0.987& 0.729 & 0.812 &0.724 & 0.814 \\           
			\hline\hline
		\end{tabular}
		}
	\caption{RetroMAE
%For quantization methods on top of ColBERT, we also report \% degradation from ColBERT.
}
	\label{tab:retromae}
\end{table}
}

%Added for KDD 2024
 \vspace*{-5mm}
\begin{table}[htbp]
	\centering
    \caption{ CluSD with  LexMAE, uniCOIL \& BM25  sparse models}
		\resizebox{0.75\columnwidth}{!}{%
		\begin{tabular}{ r |l l  |ll |ll |r }
			\hline\hline
                Sparse model& \multicolumn{2}{c|}{\bf{LexMAE}} & \multicolumn{2}{c|}{\bf{uniCOIL}}   & \multicolumn{2}{c|}{\bf{BM25T5}} & \bf{Dense}\\ 
%\cline{1-7}
                % & \multicolumn{2}{c|}{} & \multicolumn{2}{c|}{}   & \multicolumn{2}{c|}{} & Latency\\ 
\cline{1-7}
			& {MRR} & {R@1K}  & {MRR} & {R@1K}   & {MRR} & R@1K  & \bf{latency}\\
   \hline
   \multicolumn{8}{c}{\bf{Uncompressed SimLM  embeddings  with 27.2 GB space}}\\
			\hline
			S & 0.425& 0.988& 0.347$^\dag$ & 0.957$^\dag$& 0.259$^\dag$ &  0.935$^\dag$& -\\
                $\blacktriangle$  S $+$ D &    0.430& 0.990& 0.413 & 0.985& 0.415& 0.985& 1674ms\\
                S $+$ Rerank& 0.432& 0.988& 0.415& 0.957$^\dag$& 0.409& 0.935$^\dag$& 21.9ms\\
                %S $+$ LADR& 0.430& 0.990& 0.414& 0.983& 0.412& 0.983& 21.9ms\\
                {\textbf{S $+$ CluSD}} & 0.431& 0.989& 0.414 &  0.980&  0.410&  0.980& 12.1ms\\
                \hline
                \multicolumn{8}{c}{\bf{OPQ m=128. SimLM embedding space  1.1 GB}}\\
%, Report S+*
\hline
                S $+$ D-IVFOPQ & 0.383$^\dag$& 0.988& 0.354$^\dag$& 0.961$^\dag$& 0.369$^\dag$& 0.975$^\dag$& 21.9ms\\
                %S $+$ D=HNSWOPQ & & & & & & &  \\
                \bf{S $+$ CluSD} & 0.429& 0.989& 0.404 & 0.980&  0.402$^\dag$& 0.975$^\dag$& 9.8ms\\
                %\hline
                %\multicolumn{8}{c}{OPQ M=64. Embedding space  0.6 GB}\\
%, Report S+*
                % S $+$ IVFOPQ-5\% & 0.389 & 0.981 & 0.389 & 0.980 & 0.370 & 0.975 & 23.7 \\
                %S $+$ HNSWOPQ & & & & & & &  \\
                %S $+$ \bf{CS} & 0.402& 0.981& 0.402& 0.981& 0.380& 0.973& 10.2 \\
            \hline\hline
		\end{tabular}
		}
	
\vspace*{-6mm}
	\label{tab:sparse}
\end{table}
