%\vspace*{-5mm}
%\subsection{Fast search with minimized space overhead}
\begin{table*}[htbp]
	\centering
		\resizebox{1.98\columnwidth}{!}{
		\begin{tabular}{ r r l |l l ll l |llll l|r}
			\hline\hline
			 & & & \multicolumn{2}{c}{ 
{\bf MSMARCO Dev} }& {\bf DL19}& {\bf DL20} & {\bf BEIR} 
& \multicolumn{2}{c} { {\bf MSMARCO Dev} }& {\bf DL19}& {\bf DL20} & {\bf BEIR} 
& {\bf  Latency}    \\
			& &\% D& {MRR@10}& R@1k& {NDCG@10}&  {NDCG@10}& {NDCG@10}& {MRR@10}& {R@1k}& {NDCG@10}&{NDCG@10}& {NDCG@10}& ms \\
    \hline
 1. & S={\bf SPLADE-HT1} & --& 0.396& 0.980 & 0.732   & 0.721    & 0.500 & -- & --& --& --& --&31.2  \\
 & & & \multicolumn{5}{c|}{D=\textbf{SimLM}}& \multicolumn{5}{c|}{D=\textbf{RetroMAE}}&\\
   \hline
   &&& \multicolumn{10}{c}{\bf{Uncompressed flat setting. Embedding space  27.2GB}}\\
     \hline
          2. &   D  & 100 & 0.411$^\dag$ & 0.985$^\dag$& 0.714 &  0.697  & 0.429 & 0.416$^\dag$& 0.988& 0.720&0.703& 0.482 & 1674.1 \\
            3. &   $\blacktriangle$ S $+$ D   &100 & 0.424 & 0.989 & 0.740 & 0.726  & 0.518 & 0.425& 0.988& 0.740&0731& 0.520~&  1705.3 \\
           4. &    \bf S $+$ CluSD &0.3& 0.425 & 0.987 & 0.744 & 0.724  & 0.516 & 0.426& 0.987& 0.744&0.734 & 0.518 & 44.4 \\
              \hline
            &&& \multicolumn{10}{c}{\bf{OPQ $m=128$. Embedding space  1.1GB}}\\
              \hline
            5. &   $\blacktriangle$  S $+$ D-OPQ &100 & 0.420 & 0.988 & 0.735 &  0.719  & 0.508 & 0.416 & 0.988 & 0.737 & 0.732 & 0.515& 600.1 \\
            6. &  S $+$ D-IVFOPQ &10& 0.414$^\dag$& 0.987 & 0.726 & 0.734  & 0.505 & 0.404$^\dag$& 0.987& 0.713&0.722& 0.513 &  126.4 \\
           7. &   &5& 0.407$^\dag$& 0.987 & 0.717 & 0.733  & 0.499  & 0.394$^\dag$& 0.987& 0.687&0.817&0.507&  80.0 \\
           8. &   &2& 0.392$^\dag$& 0.986$^\dag$& 0.691 & 0.738  & 0.467 & 0.374$^\dag$& 0.986$^\dag$& 0.656&0.815& 0.499 & 52.5 \\
           9. &   \textbf{S $+$ CluSD}  &0.3& 0.423 & 0.987 & 0.739& 0.725 &0.514 & 0.417& 0.986& 0.742&0.735& 0.514  & 42.6  \\
             \hline
           & & & \multicolumn{10}{c}{\bf{OPQ $m=64$. Embedding space 0.6GB}}\\
            \hline
           10. & $\blacktriangle$  S $+$ D-OPQ  &100 & 0.409 & 0.986 & 0.718 &  0.721  &0.501 & 0.402 & 0.986 & 0.717 &  0.719 & 0.508 & 321.6 \\
           11. &   S $+$ D-IVFOPQ &10& 0.405 & 0.986 & 0.716 & 0.734 & 0.502 & 0.393$^\dag$& 0.986& 0.676&0.730& 0.505& 75.6 \\
           12. &   &5& 0.397$^\dag$& 0.986 & 0.704 & 0.733  & 0.497 & 0.384$^\dag$& 0.985& 0.659&0.717& 0.500& 55.0 \\ 
            13. &  &2& 0.383$^\dag$& 0.985 & 0.680 & 0.738 & 0.487 & 0.368$^\dag$& 0.985 & 0.643&0.707& 0.493 & 42.4 \\
            14. &  \textbf{S $+$ CluSD}  &0.3& 0.414 & 0.986 & 0.743& 0.726 & 0.511 & 0.403 & 0.987& 0.729&0.724 & 0.506 & 40.8 \\
              \hline\hline
		\end{tabular}
		}
	\caption{
CluSD vs. IVF cluster search with and without quantization.
%Sparse retrieval argumentation with minimum extra space overhead.
%no  with compressed or uncompressed dense emebddings S=SPLADE-HT1, D=SimLM or RetroMAE. 
%Model relevance and latency under compressed or uncompressed dense emebddings S=SPLADE-HT1, D=SimLM or RetroMAE. 
%Row  ``S $+$ D'' marked with $\blacktriangle$ indicates the oracle setting of sparse model interpolated with full dense retrieval. 
For MSMARCO Dev set, $^\dag$ is tagged when statistically significant drop is observed from the oracle $\blacktriangle$ at 95\% confidence level. 
}
\vspace*{-5mm}
	\label{tab:mainspace}
\end{table*}

\subsection{CluSD vs. IVF  cluster search}
\label{sect:eval0space}


	
Table~\ref{tab:mainspace} compares CluSD with other cluster-based selective retrievals when fusing with sparse retrieval results
in searching the MS MARCO and BEIR datasets with and without compression.  
SPLADE-HT1 is  the sparse model, and two dense models used are SimLM and RetroMAE.
%We compare our models on uncompressed embeddings, as well as two quantization configurations.
%There are the uncompressed setting and  two quantization  configurations. 
%The sparse model used is SPLADE-HT1 while the dense models are SimLM and RetroMAE.
%IVFOPQ is compared against CluSD as it selects only top clusters and does not incur extra space overhead. 
%The top portion of this table reports the performance when compression is not used, which captures a peak possible performance.
We also compare against IVFOPQ, which uses a percentage of top dense clusters sorted by the query-centroid distance.
%IVFOPQ is compared, which use some percentage of top dense clusters sorted by query to centroid clusters.
%against CluSD as it selects only top clusters and does not incur extra space overhead.
We include dense retrieval with OPQ quantization  and  IVFOPQ from FAISS.
OPQ quantization  is configured with  the number of codebooks as $m=$ 128 or 64.
We report performance of IVF clustering using the top 10\%, 5\%, or 2\% clusters respectively
as a standard approach which performs   limited top cluster retrieval.
CluSD execution is also based on the the same OPQ or  IVFOPQ index from  FAISS, but clusters are  selected  by our two step algorithm.

Notation ``S+D'' in Table~\ref{tab:mainspace} (and later Table~\ref{tab:maintime})
reports the fused performance of a sparse model and a dense model with linear interpolation.
The column marked with  ``\%D'' means the percentage of document embeddings  evaluated.
The last column,  ``Latency'' is the average single-query time using SPLADE plus SimLM for MS MARCO Dev set.
We only report SimLM latency because the latency of SimLM and RetroMAE are similar.
%When    prefix `+' is marked for a latency entry, it indicates  the extra milliseconds is spent for full or selective dense retrieval without counting   sparse retrieval time.

CluSD uses setting  $N=8192$ and 
$n=32$ 
% Tables~\ref{tab:mainspace} and~\ref{tab:maintime} use
which results in  selection of 22.3 clusters on average per query
and allows CluSD to meet  the latency requirement.
%The main space cost for CluSD is to maintain the inter-cluster similar graph, which 
%takes about 32MB with quantization for $N=8192$. 
%Switching a quantization different  from OPQ  is might result in an improvement in relevance, but it is orthogonal to our proposal.
%The reported CluSD performance on the flat index uses the exact same IVF index.
% for CluSD uses on average 20-25 (0.25 - 0.3\%) clusters per query.
%For re-ranking, we report the results of re-ranking top 1000 results from the sparse model using the dense model.
%The mean latency and 99 percentile latency of CluSD are 13.2 ms and 18.1 ms for searching uncompressed flat index, respectively;
%11.4 ms and 14.6ms for searching OPQ m=128 index;  9.6 ms and 11.1ms for searching OPQ m=128 index.





\comments{
For MS MARCO Dev set, the result of a method is marked with  tag $^\dag$ when
statistically significant drop is observed compared to  an oracle baseline
tagged with $\blacktriangle$ at 5\% confidence level.  This oracle
is sparse retrieval fused with full dense retrieval marked as ``S+D'', which typically delivers the highest relevance
using all uncompressed or quantized dense embeddings ($m=128$ or $m=64$).


%In realistic settings, a flat dense index is usually not affordable when corpus size is not small. Certain compression technique need to be used. 
The top portion of this table reports the performance of  SimLM dense model marked as ``D'' in Row 2.
In Row 1, ``S'' denotes sparse retrieval using  a variation of SPLADE with hard thresholding called HT1~\cite{2023SIGIR-Qiao}.
Sparse results merged with full  dense retrieval  results  are marked in Row 3, ``S $+$ D''.  
Our pipeline is reported in Row 4,``S $+$ CluSD''. 
The second and third portions of this table reports
the search performance   with  two compression configurations using OPQ implemented in FAISS:
1) ``m128'' that compresses dense embeddings from about 27GB into 1.1GB with 128 quantization codebooks.
2) ``m64'' that  yields 0.6GB compressed embeddings with 64 codebooks. 
%Setting where the storage for dense index is 1.1GB and 0.6GB. 
Under these two configurations, we report the Sparse model interpolated with dense retrieval under
IVFOPQ or OPQ using the default FAISS setting.


%In the first section of the table, we include the full dense retrieval SimLM (denoted as D in the table), 
%and full sparse retrieval SPLADE-HT1 (denoted as S in the table), 
%and 
%We then report dense retrieval with two compression configurations, using OPQ 
%with m=128 and m=64 respectively, resulting in an index 
%with 1.1GB and 0.6GB. These compressed configuration result in a 96\% and 97.8\% reduction in storage compared to flat index.
% and compare them against our pipeline.
}

Table ~\ref{tab:mainspace} Row 6 and 11
 show that selective IVF cluster search using  query-centroid distances 
can be under 130ms  when the percentage of clusters searched  drops from 100\% to 10\%. But the relevance drop is significant.  
Moreover, CluSD is as  fast as IVFOPQ  top 2\% search at m=64 (Row 14 vs. Row 13) and 2x fast at m=128 while
its relevance is much higher (Row 9 vs. Row 8).
%CluSD  performs  closely as the oracle at each compression setting, which
There is no statistically significant difference between CluSD and the oracle at each compression setting, which 
indicates CluSD effectively minimizes visitation of unnecessary  dense clusters. 

 



\comments{
 with full dense retrieval can reach a high relevance in all datasets and thus are marked as an oracle sign with or without compression.   
1. Almost no performance degradation when do S+CluSD.  Sometimes S+CluSD achieves slightly higher performance than S+D. It could be due to the fact that when we limit the search scope a lot of noisy documents are not considered by the dense model.

2. Under m=128 setting the performance degradation is acceptable especially on DL19 and 20. 

3. SimLM performs better under compression. This could be due to the fact that SimLM trained embedding is normalized while RetroMAE is not.

4. If we compare S+D-IVFOPQ with S+CluSD, given the same IVFPQ index, S+D-IVFOPQ represents the case where we use default order, even if we use 50x more clusters, the performance is not as good as CluSD. 
}




\subsection{CluSD vs. graph navigation and re-ranking} 
\label{sect:evaltimebudget}
Table~\ref{tab:maintime} compares  CluSD with  re-ranking and selective dense retrieval based on proximity graph navigation, which includes HNSW and LADR.
Since LADR has a variety of configurations, we use configurations under two time budgets.
We do not consider memory space restrictions in this evaluation as proximity graph takes a substantial amount of extra space.
We also compare  a simple re-ranking method,  denoted as ``D-rerank'' or ``rrk'',
that fetches the dense scores of the top-1000 sparse retrieval results to re-rank with interpolation.
Uncompressed dense embeddings  are used to exclude the relevance impact by quantization.
The last two columns report the total latency time in milliseconds including sparse retrieval when needed,
and data space cost. %Extra memory space on the top of sparse retrieval is marked with prefix `+'. 
``MRR'' means  MRR@10 and ``NDCG'' means  NDCG@10.
%The extra space for quantized inter-cluster distances for CluSD takes about 32MB. 

%As discussed in Section~\ref{sect:design}, the overall latency budget is set as 50ms and 25ms respectively. 
We allow some time for budget relaxation since it is difficult to tune parameters of all baselines to meet the constraints exactly. 
%For IVF, we use 1.5$\%$ and 3$\%$ top clusters for ``S+IVF'' and ``IVF''.
%  but we allow some variation for some baselines because it is difficult to find parameters that exactly control the latency under this budget. 
%In this case, we choose the dense configuration such that the overall latency is within 70 or 30 milliseconds. 
For HNSW under 50ms time budget, we set its expansion factor parameter (ef) as 1024, 
2048 for S$+$D-HNSW and D-HNSW. 
Under 25ms time constraint, the expansion factor (ef) is  512 and  1024 respectively. 
%We include option ``S+Re-rank" which uses the top 1,000 sparse retrieval results and only compute dense scores of these top results, and merge the score with an interpolation.
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint.
For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as 50 to meet the latency budget.
%Table~\ref{tab:maintime}.
\comments{
Under time budget 50ms, 
model  SPLADE-HT1 is used and it has a latency of 31.2 ms in sparse retrieval.  
Under  time budget 25ms, model SPLADE-effi-HT3 is used and it is more aggressive in  efficiency 
with a 12.4ms latency at the cost of a relevance degradation. 

%using the dense model only, We also include sparse retriever 
%IVFOPQ, IVF, OPQ and HNSW using the dense model only, We also include sparse retriever 
%with reranking top 1000 results using the dense model. We also include the results of document evaluation using LADR algorithm. 
Note that when ``S$+$'' is in the row, it means we use the sparse model and the reported results are interpolated. 

}
%[DISCUSS TABLE 4 RESULTS.]
The takeaways from this table are:

\textbullet\ 
%While IVF does not add extra space cost, the relevance  of IVF top cluster search under the time budget performs  worse than others.
Simple re-ranking ``D-rerank'' achieves good MRR@10 but under-performs on recall.
When the recall of a sparse retriever is relatively low, it cannot boost further as seen in the case of using BM25T5 in Table~\ref{tab:sparse} with R@1k 0.935.

\textbullet\ HNSW and LADR perform comparably, at the cost of extra online memory overhead  and extra complexity to compute offline in advance.
HNSW requires extra 9.2GB and LADR requires extra 4.3GB to store graph data in memory. 
When we  try to reduce their graph size by decreasing  the number of neighbors for HNSW and LADR, their  MRR@10 and/or  recall@1000 numbers drop visibly. For example, 
using the default 32 neighbors for the HNSW graph results in 2.4GB graph space, however  MRR@10 drops from 0.407 to 0.400 under the 25ms time budget. 

\textbullet\ CluSD incurs negligible extra space cost other than hosting embeddings, compared to HNSW and LADR.
For both time budget=50ms and 25ms,  CluSD has a higher MRR@10 while its DL19 and DL20 numbers are comparable to HNSW and LADR.


\textbullet\ 
Even when we relax the latency requirements for the baselines, CluSD still maintains its advantages in different datasets. 
For instance, if we double the time budget to around  110ms
for 	``S+D-HNSW'' after  SPLADE-HT1 retrieval, its MRR@10 for MS MARCO Dev is 0.422, which is still below S$+$CluSD with 0.426 in 46.3ms.




 

\begin{table}[htbp]
	\centering
 \resizebox{0.95\columnwidth}{!}{
		\begin{tabular}{r |llll|rr}
			\hline\hline
			& \multicolumn{2}{|c}{\bf{MSMARCO Dev}}& \bf{DL19}& 
\bf{DL20}& \bf{Latency}& \bf{Space}\\
			 & {MRR}& {R@1k}& \smaller{NDCG} &   \smaller{NDCG} & Total(ms)& GB\\
			\hline
        \multicolumn{7}{c}{\bf{Time Budget = 50 ms, S=SPLADE-HT1}}\\
        \hline
            D=SimLM & 0.411 & 0.985$^\dag$& 0.714 &  0.697 & 1674.1 & 27.2 \\
          S& 0.396$^\dag$& 0.980$^\dag$& 0.732   & 0.721    &   31.2 & 3.9 \\
           S $+$ D-rerank &  0.421 & 0.980$^\dag$& 0.745 & 0.728 & 34.6 & 31.1 \\
             $\blacktriangle$  S $+$ D  & 0.424 & 0.989 & 0.740 & 0.726 & 1705.0 & 31.1 \\
              \hline
              % D-IVF & 0.398$^\dag$& 0.935$^\dag$& 0.690 & 0.693 & 81.4 & $+$0.0\\
%            D-IVF & 0.389$^\dag$& 0.907$^\dag$& 0.671 & 0.676 & 49.5 & $+\sim$0\\
%            S $+$ D-IVF & 0.393$^\dag$& 0.987  & 0.678 & 0.715 & 56.5 & $+\sim$0\\
            %D-OPQ& 0.314$^\dag$& 0.957$^\dag$& 0.619 & 0.608 & 109.1 & 0.3 \\
           % D-IVFOPQ & 0.386$^\dag$& 0.929$^\dag$& 0.680 & 0.697 & 48.8 & 1.2\\
            % D-HNSW & 0.409$^\dag$& 0.978$^\dag$& 0.669 & 0.695 & 54.4 & 40.3 \\
            %S $+$ rerank-D &  0.412 & 0.980    &0.715  & 0.811 & 0.698 & 0.815 & $+$2.3 \\
            S $+$ D-LADR & 0.422 & 0.984$^\dag$& 0.743&0.728& 43.6& 35.4\\
%           7.1 &  S $+$ D-LADR & 0.423 & 0.987$^\dag$& 0.745 &0.728& 53.0 & $+$4.3\\
           % S $+$ D-IVF & 0.398$^\dag$& 0.987  & 0.674 & 0.720 & 64.7 & $+$0.0\\
           %S $+$ D-IVFOPQ & 0.392$^\dag$& 0.987 & 0.656 & 0.700 &  52.5 & 4.0\\
           S $+$ D-HNSW & 0.420 & 0.987 & 0.718 & 0.723 & 59.6 & 40.3 \\
            \bf S $+$ CluSD & 0.426 & 0.987 & 0.744 & 0.724 &46.3& 31.1\\
       \hline       
            \multicolumn{7}{c}{\bf{Time Budget = 25 ms, S=SPLADE-effi-HT3}}\\
            \hline
            S& 0.380$^\dag$& 0.944$^\dag$& 0.721 &  0.726 & 12.4 & 3.0 \\
             S + D-rerank & 0.406 & 0.944$^\dag$& 0.728 & 0.738 & 15.1 & 30.2 \\
            $\blacktriangle$  S + D & 0.413 & 0.984 & 0.721 &0.729 & 1654.2& 30.2  \\
            \hline
            %D-HNSW & 0.406 & 0.971$^\dag$& 0.672 & 0.694 & 28.4 & 39.4\\
             S $+$ D-HNSW & 0.407 & 0.984 & 0.706 & 0.724 & 28.7 & 39.4\\
            S $+$ D-LADR & 0.404$^\dag$& 0.984 & 0.728 &  0.736 & 24.2 & 34.5\\
%           16.1 & S $+$ D-LADR & 0.409 & 0.987 & 0.744 &  0.728  & 41.1 & $+$4.3\\
            \bf S $+$ CluSD & 0.412 & 0.984 & 0.733 & 0.728 & 25.4 & 30.2\\
			\hline\hline
		\end{tabular}  }
	\caption{CluSD vs. graph navigation and re-ranking 
% under time budget but  no memory constraints 
}
% \vspace*{-10mm}
	\label{tab:maintime}
\end{table}
%For pipelines with a time budget, we report the additional overhead in the Storage (GB) column. 




