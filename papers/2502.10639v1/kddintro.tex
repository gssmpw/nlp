

\section{Introduction}

\label{sect:intro}
Dense and sparse retrievers are two main categories of retrieval techniques for text  document search.
%It can be time-consuming to compute query-document similarities for the whole corpus on a CPU-only platform. The cost is often mitigated through approximate nearest neighbor search techniques. However, this reduction in latency comes with a tradeoff in relevance effectiveness. 
Traditional sparse retrieval models, such as BM25, use lexical text matching and run
%to retrieve results from the collection 
efficiently by traversing  an inverted index on an inexpensive  CPU platform.
Recently, sparse retrieval studies have exploited deep learning models~\cite{Dai2020deepct,
Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL, Formal2021SPLADE, shen2023lexmae} to learn lexical representations of documents for better matching.
Dense retrieval 
exploits a dual encoder architecture to produce single document representations, 
e.g. ~\cite{gao-2021-condenser,2021SIGIR-Zhan-ADORE-dense,Ren2021RocketQAv2, 
Lin2021tctcolbert, Santhanam2021ColBERTv2,  Wang2022SimLM,  Liu2022RetroMAE}.
This paper is focused on the efficiency optimization to combine the above  two retrieval approaches on a low-cost CPU-only platform.
This  is important in a large-scale  search system  which employs  a multi-stage search architecture, and 
runs partitioned first-stage retrieval in parallel  on a massive number of inexpensive machines.  

%and SPLADE~\cite{Formal2021SPLADE, Formal2021SPLADEV2} has demonstrated the strong relevance performance among these models. 
\comments{
Recent studies have found that combining  sparse and dense retrieval scores  can further boost
retrieval recall and relevance~\cite{Lin2021unicoil,2022LinearInterpolationJimLin,kuzi2020leveraging},
suggest  that both categories  of retrievers tend to capture  different relevant signals. 
These studies  make an assumption that  a search platform has to run two retrievers separately and a dense retriever.
%delivers   its maximum effectiveness, and the impact of compression and IVF clustering is not considered. 
There is an imbalance of computing platform requirements for running  these two retrievers.

dense retrieval typically requires GPU support unless significant  
%We study the use of dense retrieval results to argument sparse retrieval. 
Sparse retrieval with an inverted index has the advantage that a low-cost CPU server can run the online inference  without GPU.
A judicious tradeoff consideration of retrieval  effectiveness and efficiency is critical on  a low-cost CPU-friendly search platform. 
The importance of CPU-friendly search is recognized 
in neural ranking studies~\cite{Yang2021WSDM-BECR,2022WWWforwardIndex,2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
For a large-scale  dataset (e.g. with billions of documents), a practical search system often employs  a multi-stage 
search architecture, which divides the dataset into many partitions so that  a fast first-stage  retrieval 
method  can  search these partitions in parallel.  Such a system  prefers low-cost
CPU-only servers for first-stage retrieval, and both time and memory space complexity of retrieval should be considered 
so that  each server can host as many documents as possible
to reduce the total number of servers to support a large data size and high query traffic. 
%Thus the challenge for retrieval on such an architecture needs to consider both time and space complexity. 
% small search index needs to be as small as possible.
%and the in-memory footprint size of 
%server  needs to host documents in memory as many as possible 
%given a large number  of data partitions involved for parallel search.
%Each server employs multiple CPU cores and is replicated with many copies  for dealing high traffic of  concurrent 
%search requests, and thus 
}

%which run  efficiently  on a single CPU without hardware accelerators  GPU
%which re  retaining strong relevance effectiveness
%%Another development in learned sparse retrieval is that
%sparse retrieval with a learned representation
\comments{
There are several recent efficiency studies advancing the
inverted index implementation for sparse retrieval with learned neural 
representations~\cite{Lassance2022SPLADE-efficient,mallia2022faster,qiao2023optimizing,2023SIGIR-Qiao, 2023SIGIR-SPLADE-pruning}.
which accelerates learned sparse retrieval with BM25-guided traversal or index pruning.

For example, Row 3 of Table~\ref{tab:costbudget} shows the latency, memory space cost, and  MRR@10  of
the latest efficient SPLADE retriever for searching 8.8 million 
MS MARCO passages~\cite{2023SIGIR-Qiao} on a CPU-only server.
% with a single thread on  a 2.25GHz AMD EPYC 7742  CPU server.
Row 2 is performance of a BM25-based retriever after document expansion. 
%There is a tradeoff  in relevance effectiveness if BM25 guidance or threshold-based pruning  is too aggressive.
%and dense retrieval can boost if it does not add much overhead. 
}

As shown in previous literature~\cite{Lin2021unicoil,2022LinearInterpolationJimLin, kuzi2020leveraging}, combining  sparse and dense retrieval scores with linear interpolation
can boost relevance. 
However, none of above studies has considered to exploit the relationship of sparse and dense retrieval results for selective fusion. 
Dense retrieval is expensive, typically requiring GPUs. 
The cost of dense retrieval can be greatly reduced  by using  approximate nearest neighbor (ANN) search with 
partial IVF cluster search~\cite{johnson2019billion, 2021Facebook-DrBoost-Lewis} and proximity graph based navigation~\cite{2020TPAMI-HNSW}.
%2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
However, there is a  relevance tradeoff for a  reduced search cost in these efforts.
Table~\ref{tab:costbudget} shows that  
while RetroMAE~\cite{Liu2022RetroMAE}, a state-of-the-art  dense retriever,  is slow on CPU without compression, the use of
5\% IVF cluster search reduces CPU time substantially  but there is a  7.5\% MRR@10 drop.
The use  of  OPQ quantization further reduces memory space  to 1.2GB but causes an extra 4.6\% MRR@10 drop.

\begin{table}[htbp]
        \centering      
         \resizebox{0.82\columnwidth}{!}{ 
                \begin{tabular}{ r | r  rr}
                        \hline\hline
Dense methods  & Latency & Memory & MRR@10\\ 
\hline
%BM25/DocT5Query & 9.2ms & 1.2GB & 0.261\\  
%SPLADE/HT1 & 31.2ms & 3.9GB & 0.396 \\ 
RetroMAE/Uncompressed & 1677ms & 27.2GB & 0.416 \\ 
%RetroMAE/IVF SimLM perhaps 3\% & 49.6ms  & 27.2GB & 0.389 \\ 
RetroMAE/IVF 5\% & 107ms  & 27.2GB & 0.387 \\ 
%SimLM /OPQ             & 225-1579ms & 0.57-2.3GB & $<$ 40.2 \\
RetroMAE /IVF 5\% OPQ  & 38ms & 1.2GB  & 0.370 \\ 
%RetroMAE SimLM? /IVF  OPQ  & 38-339ms & 1.2-2.3GB  & 0.37-0.401\\ 
                        \hline\hline
\end{tabular}
}
\caption{Latency, relevance, and space  tradeoff  in a dense model for passage retrieval with MS MARCO Dev set}
 \vspace*{-7mm}
\label{tab:costbudget}
\end{table}


%Quantization~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ} is another orthorgonal cost reduction technique.

LADR~\cite{2023SIGIR-LADR}, as a follow-up study of  GAR~\cite{2022CIKM-MacAvaneyGraphReRank},
has studied partial dense retrieval by  
taking  the results of a sparse retriever as a seed to select embeddings
%That takes an approach of augmenting sparse retrieval results using graph-based navigation based on
based on a document-to-document proximity graph. This strategy  follows  the previous graph-based ANN approach~\cite{2020TPAMI-HNSW,2021KDDgraphANN}. 
While such a graph navigation approach has demonstrated its  efficiency  with a time budget, 
use  of a document-wise similarity graph adds a significant online space requirement  on a low-cost computing platform. 
%document-level  proximity graph  construction is  expensive for a large dataset during offline processing. 
For example, the proximity graph  can take an extra 4.3GB memory space~\cite{2023SIGIR-LADR} for an 8.8M  MS MARCO passage dataset.
% to keep top 128 top neighbors per document.  
Another weakness is that like HSNW, it assumes proximity graphs and dense vectors are  available from memory. 
% PARKER's suggestion:
For large datasets or higher-dimension embedding vectors, some or all of the embeddings and proximity graphs will have to be stored on disk,
especially when such  applications desire uncompressed embeddings for  a higher relevance effectiveness. 
%For example, recent RepLLaMA model~\cite{ma2023finetuning}, based on LLaMA-2~\cite{touvron2023llama}, requires 145GB storage space for 
%MARCO passage embeddings of dimension 4096.
% these 
Further, the random access of dense embeddings and/or graph nodes, as necessitated by document-level proximity graph navigation, can incur substantial 
fine-grained I/O overhead.
% to replace: 
%When the dense embeddings or its proximity graph of a large dataset cannot fit into memory, this approach that access can suffer 
%large random fine-grained I/O access overhead. 
%Nevertheless, random access of dense embeddings and/or graph nodes, as necessitated by document-level proximity graph navigation, can incur substantial
%fine-grained I/O overhead.  

%In contrast, cluster-based structuring captures  similar documents from  statically-formed  groups and enables faster  block-level I/O.
%DiskANN  and its variant study on-disk graph navigation for ANN search~\cite{NEURIPS2019_DiskANN,2023Web-Filtered-DiskANN},
%and SPANN is an effort on cluster-based ANN search. They are targeted for a stand-alone and geneal ANN problem while our 
%work takes an advange of sparse retrieval for better relevance  in the context of text document search.

%None of above studies has considered to exploit the relationship  of sparse and dense retrieval results for selective fusion. 

\comments{
That is true also with the  navigation of  a  proximity graph based on inter-document similarity distances called HNSW~\cite{2020TPAMI-HNSW}. 
The proximity graph approach to guide dense retrieval is revisited recently in GAR~\cite{2022CIKM-MacAvaneyGraphReRank}, which is furthered improved by
%LADR Kulkarni  et al.~\cite{2023SIGIR-LADR} improves HNSW.
LADR~\cite{2023SIGIR-LADR}.
%~\cite{2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
The weakness of  the  proximity graph approach is that its graph  construction requires a quadratic computing complexity, which is expensive
for a large dataset and the online hosting of such a graph also adds a significant space requirement  on a low-cost
computing platform. For example, LADR takes an extra 4.3GB memory space for 8.8M  passages in the MS MARCO dataset to keep top 128 
top neighbors per document. 
%Similarily HNSW also incurs a high space cost for graph connections.  
%In comparison, cluster-based selective dense retrieval can achieve in a comparable time complexity
%while adding  insignificant extra  space  overhead.
}



  
%with a latency controled within 4 to 8 milliseconds on a single CPU, but with a significant tradeoff in relevance effectiveness.
%For example, MRR@10 for MS MARCO Dev set drops from RetroMAE has dropped from  0.416 to 0.356.
%This paper does not impose 4-8 millisecond latency, but rather looks for a solution that can leverage both dense and sparse retrieval
%and relax the latency constraint in some degree while maximizing its relevance effectiveness as much possible.
%a range below or around  100 milliseconds 

%The response time  for such a system for an interactive service needs to be fast enough a
%and thus search needs to be completed within a few hundred milliseconds. 

%which only requires triggering of dense embedding  computation for a relaively small subset of documents.
%The proposed scheme called cluster-based selective dense  retrieval


This paper proposes a lightweight approach called CluSD to 
conduct Cluster-based Selective Dense retrieval at a low CPU-time and space cost after sparse retrieval. 
%and  combine its results  with  sparse retrieval. 
%It exploits the relationship  of sparse and dense retrieval results to conduct  selective fusion. 
%To accomplish that, 
CluSD limits query-document similarity computations through a two-stage cluster selection algorithm via an LSTM model, which  
exploits inter-cluster distances and the overlapping degree between top sparse retrieval results and dense clusters.
%CluSD leverages IVF clustering and does not require an extra document similarity graph. 
%and then make a decision to determine which document clusters should be selected  to conduct dense retrieval computation.
%The extra memory space overhead required to assist CluSD is negligible, and it  works well when embedding vectors are highly compressed.
%  and  the extra memory required for hosting dense embeddings can be limited  after compression. 
When dense clusters are not available in memory, CluSD selects and loads a limited number of clusters with efficient disk block I/O.
Thus, it can handle a large dataset with high-dimension embeddings that cannot fit in memory, with less I/O overhead compared to a
graph-based ANN approach.  % TODO: second comma? split into two sentences? come back later Parker



%0.419 by RocketQAv2~\cite{Ren2021RocketQAv2}, 
%which are the two latest state-of-the-art retrieval-reranker pipelines using cross-encoder based rerankers.
%Re-ranking in  recent dense retrieval work SimLM~\cite{Wang2022SimLM} has  used an expensive cross-encoder to
%achieve impressive MRR score 0.437 
%Re-ranking in RocketQAv2~\cite{Ren2021RocketQAv2} achieves 0.419 with a similar strategy.  






%0.419 by RocketQAv2~\cite{Ren2021RocketQAv2}, 
%which are the two latest state-of-the-art retrieval-reranker pipelines using cross-encoder based rerankers.
%Re-ranking in  recent dense retrieval work SimLM~\cite{Wang2022SimLM} has  used an expensive cross-encoder to
%achieve impressive MRR score 0.437 
%Re-ranking in RocketQAv2~\cite{Ren2021RocketQAv2} achieves 0.419 with a similar strategy.  







%0.419 by RocketQAv2~\cite{Ren2021RocketQAv2}, 
%which are the two latest state-of-the-art retrieval-reranker pipelines using cross-encoder based rerankers.
%Re-ranking in  recent dense retrieval work SimLM~\cite{Wang2022SimLM} has  used an expensive cross-encoder to
%achieve impressive MRR score 0.437 
%Re-ranking in RocketQAv2~\cite{Ren2021RocketQAv2} achieves 0.419 with a similar strategy.  





This paper evaluates  the effectiveness of CluSD  under time and space constraints on a CPU-only server, 
using the standard text retrieval benchmarks (MS MARCO and BEIR  datasets).
We compare against other baseline methods, including in a case in which the dataset does not fit into memory.
Our goal is to show that CluSD reaches state-of-the-art relevance levels at a much lower space and/or time cost.
We also compare our method to DiskANN~\cite{NEURIPS2019_DiskANN} and SPANN~\cite{chen2021spann},
two general-purpose on-disk ANN search algorithms that are not designed to leverage sparse retrieval.
%Our evaluation also compares DiskANN~\cite{NEURIPS2019_DiskANN,2023Web-Filtered-DiskANN} and SPANN~\cite{chen2021spann}
%that conduct on-disk  ANN search, and these studies are targeted for general ANN search without considering the leverage of
%sparse retrieval.
%and  shows that CluSD can effectively identify a small number of document clusters to 
%for  a stronger relevance  even though such a sparse retriever is fast with a relevance tradeoff. 
 
%provides a flexibility and reduces  the relevance requirement of sparse retrieval.
%Namely protentially we can choose a relatively fast sparse retrival with the slightly lower accuracy/recall,
%and  let dense retrivsal make up the loss. Our design needs to  adapt to  sparse retrieval quality and plans

%CSDR in improving the effectiveness of sparse retrieval with limited dense  retrieval
%traversal for low-cost search.  Our emprial findings suggest that SRF  can consistently outperform other baselines on a single CPU server 
%without GPUs 
% and achieve a strong tradeoff of effectiveness and efficiency compared to an exhausitve dense retrieval search.


%from pretrained language models to provide effective semantic text matching between a query and a document.

