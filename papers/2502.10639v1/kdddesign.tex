
\comments{
Sparse retrieval with a learned representation  is much slower than BM25
        Guided sparse retrieval can speedup, but there is a trade in  effectiveness
        Augmenting sparse retrieval with dense retrieval through fusion can make up the loss
        Selective dense  retrieval can make up the loss argument sparse retiieval more effective with limited extra computing cost.

Points to market: Option 2

                New dense retrieval algorithms have been developed every year that beat new performance records.
                New fast  sparse retrieval algorithms are developed.
                Fusion of  sparse retrieval and dense retrieval is helpful, but slow on a CPU server, dominating by dense computation.
                There is a clear redudance of computation as both retrievers scan the same dataset with different representation
                        for push for the same top documents.
                How to avoid redudant computation?
                        1) Select dense retrieval. We donot need to scan all dense documents. We can take advanrage of
                clusters of dense documents
                        and only  need to visit top clusters which are similar to a query.
                        2) The use of dense retrieval reduces the relevance reqirement of sparse retrieval. We can use a fast sparse
retrieval BM25-guided sparse retrieval or effient SPLADE version, which quickly retrieves top results. Selective dense retrieval
                can augment  a weaker but faster sparse retriever by only adding limited  extra computing cost
to  improve the recall and relevance metric.

                Challenges:
                        1) How to accomplish  selective clusetrs.
                        Which clusters  can make most contributions
                        2) What queries definitely need dense retrieval boosting?

}
%\section{Design Considerations}
%label{sect:design}
\section{Selective Dense Cluster Retrieval}
\label{sec:clusd}


\comments{
The availability of a dense retriever model, possibly  trained separately by a different language model,
offers opportunities to re-confirm and adjust the ranking of top quality documents in  the initial
candidate pool and improve the  recall by introducing more diversified
candidates.  On the other hand, there is a clear redundancy of computation as both retrievers visit the same dataset 
with different representation, possibly  pursing the  same subset of top documents.
}
 
%{\bf Yingrui, please add a table or figure which shows the distribution of training
%queries if possible, otherise the Dev set of MS MARCOS,  that has the various level of MRR increase.
%I remeber you show us some data you collected during summer or spring.} 
%Table~\ref{tab:fusionanalysis} shows 



%We found that among 33\% of queries in the MSMARCO training set, there is no increase in terms of the MRR@10 when dense retrieval results are added to sparse retrieval results. Namely there are many queries that do not actually benefit from the use of dense retrieval after sparse retrieval.
% two retrievers, and explicitly skipping some of fusion can avoid the unnecessary computing cost.
%Motivated by the above data, 

%Our design considerations are:

%of selective dense retrieval  
%has  the following considerations.
%\begin{itemize} [leftmargin=*]
\comments{
\item  {\bf Partial dense retrieval after  full sparse retrieval.}

%and also there are several techniques developed recently to accelerate learned sparse retrieval,
}

%\item {Inter-cluster vs. inter-document similarity.}
\comments{
{\bf Cluster-based  dense retrieval.}
After the  fusion option is determined for a  query,
there is an option to follow LADR~\cite{2023SIGIR-LADR} and select documents that are similar to
the top sparse retrieval results  using a similarity graph.
These similar documents form candidates  for partial dense retrieval and  rest of un-selected
documents in a dataset can be skipped.  
Maintaining a large inter-document similarity graph online is quite expensive from
offline  data collection, online storage space, and   inference time.


The experience with the learned sparse models such as SPLADE~\cite{Formal2021SPLADE, Formal2021SPLADEV2} showed that
learned sparse representations from  BERT can be very effective.
For example, without optimization in language model pretraining, SPLADE's MRR performance is on par to
the dense retriever by RocketQA~\ref{Ren2021RocketQAv2}. 
Thus there is an opportunity to skip some of scoring computation in dense or sparse retrieval.
Since it is not easy to recognize and separate part of an inverted index  that contains high scoring documents,
we follow the approach of LADR~\cite{2023SIGIR-LADR} and GAR~\cite{2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}
run a sparse retriever completely first and steer partial dense  retrieval. 
}

%Previous studies  with the learned sparse models~\cite{Mallia2021deepimpact, 
%Lin2021unicoil,2021NAACL-Gao-COIL, Formal2021SPLADE, Formal_etal_SIGIR2022_splade++, shen2023lexmae
%Formal2021SPLADE, Formal2021SPLADEV2} showed that
%the relevance resutls from learned sparse representations from  BERT can be  fairly strong.
We aim to identify the opportunity to skip a substantial portion of dense retrieval computation 
by exploiting the potential overlap of sparse and  dense retrieval results.
We take the IVF clustering approach~\cite{johnson2019billion} that  places similar documents within each cluster. 
However, unlike IVF cluster search, our selection goes beyond the similarity between the query and cluster centroids to select top clusters.
Instead, our work explicitly exploits the overlap of sparse retrieval results with the potential dense clusters for better efficiency and relevance.   

\comments{
As discussed in Section~\ref{sect:intro}, we do not take a document-to-document similarity graph approach~\cite{2020TPAMI-HNSW, 
2022CIKM-MacAvaneyGraphReRank,  2023SIGIR-LADR} to avoid both the large space requirement of hosting such a graph and the high cost of random fine-grained
disk I/O to 
either the dense embedding set or the graph when one of them is hosted on the disk.
}

%and also there are several techniques developed recently to accelerate learned sparse retrieval,
\comments{
There are two ways  to select documents for dense retrieval.
One way is to take a graph navigation approach such as HNSW~\cite{2020TPAMI-HNSW}, GAR~\cite{2022CIKM-MacAvaneyGraphReRank} and LADR~\cite{2023SIGIR-LADR}, 
exploiting  document-to-document similarity links during inference time.
Another way is to follow a clustering  approach to group  documents based on their embedding similarity at offline 
time~\cite{johnson2019billion} and select a small set of  clusters.
%~\cite{2020TPAMI-HNSW,2022CIKM-MacAvaneyGraphReRank, 2023SIGIR-LADR}.
% is viewed as  a modern instantiation
%of the clustering hypothesis to revisit these ideas in a new context.  
We opt to take  the clustering based approach with two reasons.
1) Maintaing a large document-to-document similarity graph with a qudradical complexity is not scalable for  a large  data collection size. 
%As illustrated in  Table~\ref{tab:costbudget}, compression with embedding  quantization is necessary to meet the space constraint.
%due to the memory usage of stroing  embedding vectors. 
%For 8.8M MS MACRO passages, that takes  about 27GB space.
%Fortunately the quantization techniques can significantly compress this space into 1-2GB or even 0.5GB depending on the configuration. 
%Our feature design does not opt for  inter-document similarity since it is space-consuming, but  plans to exploit cluster-based summary information, 
%which  only adds  insignificant  space overhead.  
%\end{itemize}
%, and set a limit on the number of dense clusters used.
2) Embedding vectors of a large dataset with a high dimension per vector may not fit into the memory.
Cluster-based on selection exploits the block-level disk I/O for lower access overhead and higher bandwidth. 
In comparison, random access of selected documents in a graph based approach can suffer higher I/O overhead. 

}
\comments{
We intend to exploit coarse grain cluster-level similarity to reduce the complexity in selective dense retrieval with two reasons.
%The challenge is to how to select clusters  which can make most contributions.
%We opt to take a cluster-level skipping approach with two reasons.
1) Documents can be clustered in advance during offline processing 
%Documents within a cluster similar to a query can provide a large variety of strong candidates to supplement sparse retrieval results.  
2) Cluster-level computation allows  aggregation of  similarity computation for query-document dot products as a fast matrix operation 
to fully  utilize on-chip CPU caches. 
%In comparsion, there is a less chance for online inference to fully benefit from  CPU cache based on an irregular  graph structure.
%During offline processing, it is computationally expensive to drive document-level similarity for all pairs for documents in a large datset.
A research problem  is how to adaptively identify a subset of clusters  which yield sufficient relevance benefits.
% benefitial fusion if  sparse retrieval scoring may be sufficient. In addition to the influence of sparse retrieval results,
% a dynamically-visited dense cluster may provide sufficient high quality documents, reducing the necessarity of visiting
%the remaining dense clusters.   
%Thus we will develop a dynamic feature-driven decision scheme based the results of sparse retrieval.
%and incremtentally navigated  dense document clusters  as input. 
}
\comments{
Another consideration is that there are various relevance and efficiency tradeoff in selecting  a sparse retrieval 
model~\cite{Lassance2022SPLADE-efficient,mallia2022faster,Qiao20222GT,2023SIGIR-Qiao,2023SIGIR-SPLADE-pruning}.
For example, BM25 guided-sparse retrieval can greatly speedup, but lose some recall or precision
 when this guidance is weighted aggressively.
The use of dense retrieval provides a flexibility and reduces  the relevance requirement of sparse retrieval.
Namely potentially we can choose a relatively fast sparse retrieval with the slightly lower accuracy/recall,
and  let dense retrieval make up the loss. Our design
needs to  adapt to  sparse retrieval quality and plans
We plan to exploit features that reveal  the overlapping degree of top sparse retrieval results with  dense clusters
to guide the selection in addition to query-cluster distances. 
}

\comments{
\item  {\bf Query filtering for skipping dense retrieval}
We want an online system to make an earlier  determination if a query needs an argumentation from dense retrieval with supervised learning.
%A challenge is to identify features and infer based on such features with supervised learning.
Given that sparse retrieval is conducted first,
our idea is to predict the quality of sparse retrieval and its relative strength  compared to unexecuted
dense retrieval based on quality of query token expansion by the sparse model,
the statistical behavior of  sparse retrieval results, and
the relationship between dense document embedding summary and sparse retrieval scoring structure.




{\bf Latency budget.}
%Our heuristic-based fusion selection may suffer from an effectiveness tradeoff
%as full dense retrieval execution likely offers the maximum relevance benefits.
For search latency, our goal is to minimize the relevance effectiveness loss to a small degree while still accomplishing  a fast response time
acceptable to the interactive search system requirement.
In a search system running as an interactive  web service,  a query with multi-stage processing
needs to be completed within a few hundred milliseconds.
%Considering the multi-stage search nature  and other service overhead involved, 
A first-stage retriever operation  for a data partition needs to be completed within several tens of milliseconds without query caching (e.g. 200ms). 
Our targeted time budget to augment sparse retrieval cannot be too high     compared to  sparse retrieval cost.
We will evaluate the latency budget in two settings: around 50ms  and 25ms, including the  sparse retrieval time.
%For the example of Table~\ref{tab:costbudget},
%The studies from~\cite{Lassance2022SPLADE-efficientmallia2022GTI,Qiao20222GT,2023SIGIR-Qiao},
If sparse retrieval can complete  in 30 ms on an  average CPU and the total latency
target is around 50ms,  the cost of dense retrieval is targeted under around  20ms.
% At most the total sparse and dense retrieval cannot be more than 100ms.
%Accuracy of above query-specific cluster skipping while retaining a competiitve relevance is the key to accomplish this latency goal. 
%Notice that the latency target in the  LADR paper~\cite{2023SIGIR-LADR} is in a 4ms-8ms range
%and 
%This latency budget choice is reasonable because for 
%Also our evaluation is conducted in a low-end CPU server and a faster CPU can further accelerate. 
%CPU latency reported in~\cite{2022WWWforwardIndex} is at 11

}
\comments{
For the memory space cost,  our target budget for  dense retrieval augmentation is controlled in  the same level or some fraction of  sparse retrieval cost.
As illustrated in  Table~\ref{tab:costbudget}, compression with embedding  quantization is necessary to meet the space constraint.
%due to the memory usage of stroing  embedding vectors. 
%For 8.8M MS MACRO passages, that takes  about 27GB space.
%Fortunately the quantization techniques can significantly compress this space into 1-2GB or even 0.5GB depending on the configuration. 
Our feature design does not opt for  
inter-document similarity since it is space-consuming,
but  plans to exploit cluster-based summary information, 
which  only adds  insignificant  space overhead.  
%, and set a limit on the number of dense clusters used.
}

