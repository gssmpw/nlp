\subsection{ Search with on-disk data}
\label{sect:evaldisk}
%{\bf CDFS vs. re-ranking and nearest-neighbor search baselines with in-memory or on-disk data.}

Table~\ref{tab:disktime} examines the performance  of CluSD fused with SPLADE when MS MARCO passage 
embeddings  cannot fit in memory and is stored on an SSD disk.
We use  $N=65,000$ for CluSD to reduce the cluster size and the extra space for quantized inter-cluster distances takes about 40MB. 
We also compare other selective dense retrieval baselines using  reranking or proximity graph navigation.
%The mean response time (MRT) and 99 percentile time (P99) include the I/O cost while their cost breakdown for I/O and CPU time is also listed. 
The mean response time (MRT) and 99\textsuperscript{th} percentile time (P99) only
include the total dense retrieval time, while their cost breakdown for I/O and CPU time is also listed. 
All times reported are in milliseconds. 
Column ``MRR@10'' means  MRR@10 for the Dev set. 
Column ``\#Doc'' is the number of embeddings fetched for performing full or partial dense retrieval.
%A small table on the top specifies the data location of configured retrieval algorithms.
The reranking method simply fetches top-$k$ embeddings  from the disk for fusion. 
LADR is designed for in-memory search~\cite{2023SIGIR-LADR} and we run it by  assuming  memory can sufficiently host the proximity  graph while letting LADR access  embeddings from the disk. 
Excluding the I/O cost portion of LADR in this table, the in-memory search cost of LADR is marked in the CPU time column.
We test two configurations of LADR, its default with 128 neighbors, 200 seed documents, and a search depth of 50,
as well as a faster configuration, selected such that its CPU time is similar to CluSD. 
LADR\textsubscript{fast} uses 128 neigbhors, 50 seed documents, and a depth of 50.
DiskANN~\cite{NEURIPS2019_DiskANN} assumes the graph and original embeddings are on disk  while the memory  hosts
compressed embeddings for quick guidance.  SPANN~\cite{chen2021spann} searches disk data in a cluster-based manner based on query-centroid distances.
Both DiskANN and SPANN  are designed for on-disk search from scratch without fusing with others, and thus we simply fuse their outcome with SPLADE results.
HNSW~\cite{2020TPAMI-HNSW} is not included because its in-memory relevance  is similar as DiskANN while it is not designed to be on disk. 

\comments{
As the size of dense embeddings or the number of documents in a dataset increases, it is likely that dense embeddings are too large to be fully stored in memory. For instance, RepLLaMA requires 135.6GB for the 8.8M documents in MS MARCO; BERT-based dense embeddings for a 1B document dataset would require 2.8 TB. This problem is exacerbated when a large navigation graph is also required to be loaded in memory. We compare CluSD with several baselines when the embeddings are stored on disk and must be loaded to memory before ranking. We memory-map the dense embeddings and load them on-demand during our algorithm. We measure both the disk access time (I/O) and the associated computation required by each method (Overhead). Our results are shown in Table \ref{tab:disktime}. 
% TODO (above): Latency figures given in the table only represent the dense portion of retrieval. 
% TODO (above): We compare each method with RepLLaMA and SimLM embeddings for WHY DO WE DO THIS?
}

Reranking is similar to CluSD in terms of  mean latency while having a much slower 99\textsuperscript{th} percentile latency, lower MRR@10 and  recall@1000.% in this case.
CPU-time of reranking is short but its total time is dominated by 1,000 random I/O operations for fetching top embeddings. 
%and reaches about  715MB/second I/O speed. 
Our setting stores embeddings in clusters grouped by their similarity on disk, thus
random I/O operations in reranking  benefits from spatial locality in clustered data, because top results tend to 
be similar. Meanwhile  its  99\textsuperscript{th} percentile latency is 5.1x slower when  spatial locality cannot be well exploited or  is 
less available. 
The column ``Worst'' lists  the estimated worst-case latency when documents are not clustered on the disk. 
We find that  each  I/O operation has about a 0.3ms queueing and other software overhead on our tested PCIe SSD.
Thus, more fine-grained operations in reranking yields more overhead.
In comparison, CluSD issues  much fewer  I/O operations to fetch 4,423 embeddings per query on average in a block I/O manner.
Its 99\textsuperscript{th} percentile latency is 2x faster than re-ranking.

CluSD outperforms LADR with higher MRR@10 and a lower latency for the two tested configurations.
% taking much less and  99\textsuperscript{th} percentile latency.   
The above I/O spatial locality  explanation for reranking is also applicable to LADR.  
CluSD outperforms  DiskANN in relevance and latency.
% which is designed for more general nearest neighbor search. 
CluSD is comparable to SPANN in relevance  but is 6.98x faster in the mean latency.

% while CDFS, which fetches clusters of consecutive embeddings, utilizes block I/O that incurs less access overhead. 



\comments{
We find that the disk access time required by CluSD is significantly less than the time required by LADR or a simple rerank. We suggest that SSD random seeks have significant overhead compared to consecutively reading. Thus, reading many documents consecutively from a cluster is quick, and the number of random seeks is limited to the number of clusters accessed. During other methods, it is likely that the number of random seeks is equal to the number of documents accessed. A simple least-squares model for our RepLLaMA results suggests that each random seek takes 167 us, whereas the time to read one document is only 11.8 us.
}


\comments{
We find that the disk access time required by CluSD is significantly less than the time required by LADR or a simple rerank. We suggest that SSD random seeks have significant overhead compared to consecutively reading. Thus, reading many documents consecutively from a cluster is quick, and the number of random seeks are limited to the number of clusters accessed. During other methods, it is likely that the number of random seeks is equal to the number of documents accessed. A simple least-squares model for our RepLLaMA results suggests that each random seek takes 167 us, whereas the time to read one document is only 11.8 us.
}

% CluSD is with XXXX configuration and XXXX average clusters per query. Reranking reranks the top-1000 documents from bm25 + docT5query. LADR_S are results from a "small" version of adaptive LADR, with k=128, n=8, c=200. LADR_L are results from a "large" version of proactive LADR, with k=128, n=200 that retrieves approximately ten times more documents than LADR_S






%Added for KDD 2024


\begin{table}[htbp]
        \centering
 \resizebox{1.0\columnwidth}{!}{
                \begin{tabular}{r |lll|lll|rr}
                        \hline\hline
                        & \multicolumn{2}{c}{\bf{Relevance}}& \bf{\#Docs} &  \multicolumn{5}{c}{\bf{Dense Latency \& Breakdown (ms)}}\\
            & MRR@10 & R@1k & fetched&  MRT & P99 & Worst&I/O  & CPU  \\
        \hline
        Rerank& 0.421 & 0.980 & 1000.0 &  18.8& 95.70 & 454.9  &16.9& 1.9\\
        %S+LADR & 0.422 & 0.984 & 82.99 & 28.86 & 8521.6 & 111.85 &  \\
% Above: Default configuration
        LADR\textsubscript{fast} & 0.419 & 0.980 & 2431.0 & 42.3 & 191.5 & 1105.9 &31.3 & 11.0\\ % ladr n=50, c=50, k=128
        LADR\textsubscript{default} & 0.422 & 0.984 & 8521.6 &  119.3 & 349.8 & 3876.5 & 91.4 & 27.9\\
% Above: LADR with time budget of CPU time ~= CluSD CPU time ~= 6-7ms
      %  D-ANN-Wentai & 0.398 & 0.970 & 271.4 & 5.24 & -- & 276.6 & 321.2\\
   %     SPANN-Wentai & 0.405 & 0.965 & -- & -- & -- & 123.6 & 159.6 \\
           DiskANN& 0.417 & 0.984 & -- &  276.5 & 312.2 & -- &272.3& 5.2\\ 
        SPANN& 0.420 & 0.989 & -- &  123.6 & 159.6  & -- &-- & -- \\
%Did Yingrui adds fusion and adds this on Wentai's result?  Yes  S+DiskANN & 0.417 & 0.984 & -- & -- & -- & 276.5 & 312.2\\ov
       % S+SPANN* & 0.424 & 0.985 & 6.54 & 6.20 & 3484.9 & +12.74 & 84.73\\
% Above: Parker selected clusters from top-16 SPLADE results -> thus relevance is too strong
        %CluSD& 0.425 & 0.986  & 6.19 & 6.48 & 4423.51 & 12.67 & 42.77\\
    %CluSD& 0.425 & 0.986  & 4423.5&  17.7& 47.8 & 178.1 &6.2& 11.0\\
    CluSD& 0.425 & 0.986  & 4423.5&  17.7& 47.8 & -- &6.2& 11.0\\
                \hline\hline
                \end{tabular}  }
        \caption{Search when uncompressed embeddings and/or the proximity graph are on disk for SPLADE + SimLM}
% Embedding space = 25.9 GB. Here we only report the latency of the dense retrieval part.
\vspace*{-10mm}
        \label{tab:disktime}
\end{table}
                                     
