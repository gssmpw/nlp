\section{Evaluations}

Our evaluation answers  the following research questions around our proposed techniques and the correspond subsection are marked.

\textbf{RQ1} (Section~\ref{sect:eval0space}): 
How does CluSD compare against other low-cost baselines when space is constrained?
%extra space overhead is minimized by augmentation under different compression configurations? 

\textbf{RQ2} (Section~\ref{sect:evaltimebudget}): 
How does CluSD  perform under a retrieval latency time budget when there is no space restriction?

\textbf{RQ3} (Section~\ref{sect:evalbeir}): 
How does CluSD  perform in searching  out-of-domain text content?
Zero-shot retrieval performance is important since a large dataset can contain documents from many domains. 


\textbf{RQ4} (Section~\ref{sect:evalLSTM}): 
Is  LSTM with cluster pruning preferred compared to other design options?
Are  features designed for LSTM useful?

\textbf{RQ5} 
(Section~\ref{sect:clusterno}): 
How does the number of clusters used in dataset partitioning 
affect the relevance and latency of CluSD? 

\textbf{RQ6} 
(Section~\ref{sect:threshold}):
How does the choice of selection threshold value in LSTM affect the average number of clusters selected?
%Does the cluster selection model dynamically select different number of clusters for different queries? How the selection differs for different datasets?

\textbf{RQ7}
(Section~\ref{sect:evalsparse}):
How does CluSD perform in augmenting  different sparse retrievers with different efficiency and relevance characteristics?


\comments{
To answer \textbf{RQ1},  we compare our proposed approach on the in domain test set, MSMARCO passage dev, DL 19, 20 with other commonly used techniques under storage (Table~\ref{tab:mainspace}) and latency constraints (Tabel~\ref{tab:maintime}).  
For \textbf{RQ2}, as our cluster selection technique requires training, we would like to understand how the approach would work on our domain dataset. In Table~\ref{tab:beir}, we use our cluster selection model trained on MS MARCO datasets on the 13 datasets from BEIR on a zero-shot manner and observe promising performance on all 13 out-domain datasets. 
For \textbf{RQ3} and \textbf{RQ4}, We compare the different cluster selection techniques, including heuristic approaches, a point-wise prediction model xgboost and a vanilla RNN prediction model against the proposed LSTM model (Table~\ref{tab:cluster_feat}).
To answer \textbf{RQ5}, we take two scenarios where the number of clusters is 4096, and 8192. Then we present the relevance in MRR@10, recall@1000, and the latency in terms of mean and 99 percentile time under three settings: when the flat index is used without compression, when we use OPQ to compress with m=128 or 64. Results are shown in Figure~\ref{fig:ncluster}.
For \textbf{RQ6}, we first plot the distribution of the number of clusters selected for each query under different model prediction thresholds. We also plot the average number of clusters for MSMARCO dev set and BEIR in Figure~\ref{fig:thresholds}.
For \textbf{RQ7}, we report the performance using three weaker sparse retrievers in Table~\ref{tab:sparse}. 
}

\subsection{Datasets and measures}
Our evaluation uses the MS MARCO datasets for full passage ranking~\cite{Campos2016MSMARCO,Craswell2020OverviewOT}. 
MS MARCO contains  8.8 million passages and about 1.1  judgment label per query. 
%The average number of WordPiece~\cite{wordpiece} tokens per passage is 67.5.
The development (Dev) query set contains  6980 test queries. %with about 1.1  judgment label per query. 
The test sets in  TREC deep learning (DL)  2019 and 2020 tracks 
provide 43 and 54 queries, respectively, with many judgment labels per query.
%which provide 54 and 43 test queries, repsectively,  
Following the common practice for the Dev set of MS MARCO,
%official leader-board standard, 
we report mean reciprocal rank at 10 (MRR@10) and recall at 1000 (recall@1K) 
which is the percentage of relevant-labeled results appeared in the final top-1000 results.
%we report mean reciprocal rank (MRR@10) for relevance instead of
%Normalized discounted cumulative gain (NDCG)~\cite{NDCG}  is not used because such a set has about 1.1 judgment label per query, which is too sparse to use NDCG.
For TREC DL test sets which have many judgment labels per query, we report the commonly used NDCG@10 score.
% when space allowed, we also report the recall@1k. 

%If space allows,
%In addition, we also report recall at 50 (recall@50) 
%as if we use large language model as a reranker, it is common practice to take less than 100 documents from retrieval model for evaluation.
%For TREC DL test sets which have many judgment labels per query, we report the commonly used NDCG@10 score.
%When space allows, we also report the recall ratio at 1000 

We also we use a collection called BEIR which contains the 13 publicly available datasets~\cite{thakur2021beir} 
for evaluating zero-shot retrieval performance. 
BEIR is a heterogeneous benchmark containing a variety of IR tasks, 
including search tasks (e.g.  question-answering, bio-medical IR, and entity retrieval)  
and semantic relatedness tasks (e.g.  duplicate question retrieval). 
The size of these data sets ranges from 3,633 to 5.4M.
%Their average query length ranges from 5.39 to 192.98, 
%and  their average document length ranges from 11.44 to 634.79. 
%Each dataset has binary or three-level relevance labels.
We report average NDCG@10 for the BEIR datasets. 

\subsection{Models and parameters}
For sparse retrieval, we adopt 3 SPLADE-based  models and one BM25 based model with different relevance/efficiency  tradeoffs:
1) SPLADE-HT1 is based on   a recent SPLADE model optimized by  hybrid thresholding~\cite{2023SIGIR-Qiao}
and BM25-guided traversal~\cite{mallia2022faster,qiao2023optimizing} with index space 3.9GB
and 31.2 ms retrieval latency. 2) SPLADE-HT1-fast is a variation of SPLADE-HT1 with a different parameter setting, which has 
3.9GB index space and 22.4 ms latency. 
3) SPLADE-effi-HT3 is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} with 3GB space
and  12.4ms latency due to use of hybrid thresholding and BM25 guided traversal techniques. 
4) BM25-T5 applies  the BM25 lexical model on expanded documents using DocT5Query~\cite{Cheriton2019doct5query}. 
BM25-T5 uses 1.2GB index space with latency 9.2ms. 
For dense retrieval, we adopt the recently published   dense models
SimLM~\cite{Wang2022SimLM} and RetroMAE~\cite{Liu2022RetroMAE}. 
We use their   checkpoints from the Huggingface library to generate document and query embeddings 
and  use  K-means in the FAISS library~\cite{johnson2019billion} to derive dense embedding clusters.
Our evaluation implementation uses C++ and Python and the
related code will be released to the public after publication of this paper. 
\comments{
%Note that SimLM model's embeddings are normalized while RetroMAE's embeddings are not. 
For sparse retrieval, we use two fast versions of SPLADE and a SPLADE-efficient model, as well as BM25-T5. 
The implementation of SPLADE models follows 2GTI optimization~\cite{qiao2023optimizing}. %its official release~\cite{spladeRepo}. 
SPLADE-HT1 is the SPLADE model trained with hybrid thresholding scheme~\cite{2023SIGIR-Qiao} using $\lambda_{t}=1$, 
SPLADE-effi-HT3
is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} trained with hybrid thresholding scheme using $\lambda_{t}=3$. Both of these two models are using the "accurate" configuration in 2GTI during retrieval, while SPLADE-HT1-fast uses the "fast" configuration.
The sparse retrieval code for Splade and BM25-T5 uses the code from PISA~\cite{mallia2019pisa}
with some optimization~\cite{Lassance2022SPLADE-efficient,2023SIGIR-Qiao}.
}

To train the CluSD model, we sample 5000 queries from the MS MARCO training set 
%and use the LSTM unit in Pyytorch. 
and the hidden dimension for LSTM is 32. The models are trained for 150 epochs. 
%The input documents are sorted based on the distance between the centroids the documents belong to and  the query. 
For sparse and dense model interpolation, we use min-max normalization to rescale the top results per query. 
%Then we use linear combination with equal weights to combine two ranked lists. 
For  BM25-T5, the interpolation  weights are  0.05 and 0.95 for sparse  and  dense scores, respectively. 
For SPLADE, they are  0.5 and  0.5, respectively. 
%In terms of missing document in either ranked list, as the smallest value from each normalized list is 0, there is no missing imputation needed as we 
%can view the missing  documents taking the value of the worst document in the ranked list automatically. 
For CluSD, the default setting of number of clusters is $N=8192$, and the input sequence length to the LSTM model is truncated at $n=32$.  
To report the mean single-query latency and 99 percentile latency in milliseconds,
we  run test queries multiple times  from MS MARCO Dev set using a single thread on  a 2.25GHz AMD EPYC 7742  CPU server. 

\subsection{Baselines and notations}
%We select two groups of baselines under space and time constraints respectively.

Table~\ref{tab:mainspace} compares  baselines  with memory space constraint. 
We include dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
OPQ quantization  is configured with  the number of codebooks as $m=$ 128 or 64.  
When IVF clustering is used,  we report its performance  using top 10\%, 5\%, 2\% clusters respectively
as a standard approach which performs   limited top cluster retrieval. 
CluSD execution is also based on the the same OPQ or  IVFOPQ index from  FAISS but clusters are  selected  by our LSTM model. 
%Switching a quantization different  from OPQ  is might result in an improvement in relevance, but it is orthogonal to our proposal. 
We use notation ``S+D'' to report the interpolated performance of a sparse model and a dense model in Table~\ref{tab:mainspace} and Table~\ref{tab:sparse}.

Table~\ref{tab:maintime} compares  baselines  without  memory space constraint. 
For proximity graph-based dense retrieval which incurs additional graph space overhead,
we include  HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR} as shown in Table~\ref{tab:maintime}. 
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint. 
%For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as 50 to meet the latency requirements in 
%Table~\ref{tab:maintime}.
%Uncompressed dense embeddings (marked as "flat") are used
%to exclude the impact due to quantization.
We also compare  a simple re-ranking method,  denoted as ``D-rerank'' or ``rrk'', 
that fetches the dense scores of top 1000 of sparse retrieval to re-rank with interpolation. 
%We use the same clustering setting for IVF and CluSD. 

%The reported CluSD performance on the flat index uses the exact same IVF index. 
The configuration we selected to meet the latency requirement for CluSD uses on average 20-25 (0.25 - 0.3\%) clusters per query. 
%For re-ranking, we report the results of re-ranking top 1000 results from the sparse model using the dense model.  

For MS MARCO Dev set, the result of a method is marked with  tag $^\dag$ when 
statistically significant drop is observed compared to  an oracle baseline
tagged with $\blacktriangle$ at 5\% confidence level.  This oracle
is sparse retrieval fused with full dense retrieval marked as ``S+D'', which typically delivers the highest relevance  
using all uncompressed or quantized dense embeddings ($m=128$ or $m=64$).
