\subsection{CluSD with other quantization models}
\label{sect:evalcompress}

% parker's rewrite
%CluSD is efficient and effective when combined with a variety of quantization methods.
Tables~\ref{tab:mainspace}, \ref{tab:beir},
and \ref{tab:RepLLaMA}  use the OPQ quantization method~\cite{Ge_2013_CVPR}  implemented in FAISS.
%We  the performance of CluSD with three quantization methods on the MS MARCO Passage Dev set, 
%OPQ~\cite{Ge_2013_CVPR}, JPQ~\cite{2021CIKM-JPQ-Zhan}, and DistillVQ~\cite{Xiao2022Distill-VQ}.
Table~\ref{tab:compress} shows the use of two other recent quantization methods 
called DistillVQ~\cite{Xiao2022Distill-VQ} and JPQ~\cite{2021CIKM-JPQ-Zhan} 
with in-memory CluSD for MS MARCO Passage Dev set. Latency column includes sparse retrieval time for last two rows where fusion is conducted. 
%We use the OPQ implementation from FAISS with results in Table \ref{tab:sparse}.
For JPQ, we use its released model and compression checkpoints and follow its default setting with $m=96$. 
For DistillVQ, we use its released code to train compression on the RetroMAE model using its default  value $m=128$.
This DistillVQ setting  yields 1.38GB storage space for RetroMAE embeddings while JPQ yields  0.95GB for its dense model. 
%The sparse SPLADE-HT1 model used  has a 3.9GB index.  [alreadly mentioned before]
We set IVF to selectively search 2\% of clusters. 

%This is consistent with our findings with OPQ; thus we conclude that CluSD is effective under many different quantization methods.
% below: old
%The above  evaluation uses the OPQ quantization method~\cite{Ge_2013_CVPR}  implemented in FAISS.



%DistillVQ~\cite{Xiao2022Distill-VQ} and JPQ~\cite{2021CIKM-JPQ-Zhan} using  their released code. 
%and  the recent  quantization
%methods~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}
%also have  a significant relevance drop pattern when the compression ratio increases.
%A limitation of the current  CluSD work is that the training of the LSTM model has a dependence on the sparse retrieval model used.
%While  CluSD can  boost the relevance of a weaker sparse retriever model such as BM25, the augmented relevance still has not reached a level
%comparable  to the one where CluSD is coupled with a stronger sparse retriever. There is room for improvement in future work.
%%We will revise the paper to discuss and compare [52, 57, 59] in more details and will include extra evaluation results. 

%This evaluation uses  the  code and checkpoints released from DistillVQ and JPQ authors,
%and follows their default setting  with $m=128$ for DistillVQ and $m=96$ for  JPQ.
%DistillVQ for RetroMAE has  1.38GB embeddings while  JPQ dense model has  0.95GB embeddings. 
%The sparse SPLADE model has 3.9GB index.  IVF setting selectively searches   2\% clusters. 

%Table ~\ref{tab:compress} shows that 
%under the same quantization (DistillVQ or JPQ), CluSD outperforms FAISS IVF in relevance and latency significantly 
%before and after fusion.
%This is   consistent with our findings with OPQ and thus CluSD can work effectively with different  quantization methods. 
%The comparison in ~\cite{Xiao2022Distill-VQ}
%shows the quantization with another quantization method called RepCONC~\cite{2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}
%has   relevance similar to JPQ  and DistillVQ outperforms them.  Thus we did not directly evaluate RepCONC. 

\comments{
\begin{verbatim}
DistillVQ	MRR@10	Recall@1K	Latency (ms)
IVF	0.365	0.899	20.8
CluSD	0.393	0.977	9.7
SPLADE+ IVF	0.392	0.987	52.0
SPLADE+CluSD	0.417	0.987	40.9

JPQ	MRR@10	Recall@1K	Latency (ms)
IVF	0.326	0.917	21.3
CluSD	0.341	0.969	11.2
SPLADE+IVF	0.379	0.985	52.5
SPLADE+CluSD	0.392	0.985	42.4
\end{verbatim}
}
\begin{table}[htbp]
        \centering
                \resizebox{\columnwidth}{!} {%
                \begin{tabular}{ r |l l l |ll l}
                        \hline\hline
                Quantization & \multicolumn{3}{c|}{\bf{DistillVQ}} & \multicolumn{3}{c}{\bf{JPQ}}   \\
%\cline{1-6}
                        \hline
S=SPLADE-HT1                        & {MRR@10} & {R@1K}  & {Latency } & {MRR@10} & {R@1K}    & {Latency}\\
                        \hline
                        IVF (2\%)  & 0.365$^\dag$ &	0.899$^\dag$ &	20.8 ms
& 0.326$^\dag$	&0.917$^\dag$ &	21.3 ms\\
%CluSD &0.393&	0.977&	9.7ms &0.341&	0.969&	11.2ms\\
S+ IVF (2\%)	&0.392$^\dag$ &	0.987&	52.0 ms
& 0.379$^\dag$ & 	0.985& 	52.5 ms\\
S+CluSD&	 0.417 &	0.987 &	40.9 ms
 &0.392 &	0.985 &	42.4 ms\\
\hline           
\hline           
                \end{tabular}
                }
        \caption{ Use of CluSD with   DistillVQ and JPQ quantization}
\vspace*{-5mm}  
        \label{tab:compress}
\end{table}     

Table ~\ref{tab:compress} shows that 
%use of approximated embedding with the recent quantization methods still reduces relevance visibly 
%compared to uncompressed embeddings shown in Table~\ref{tab:mainspace}, and CluSD can boost relevance substantially.
under the same quantization (DistillVQ or JPQ), CluSD significantly outperforms FAISS IVF in relevance and latency 
before and after fusion.  This is consistent with our findings with OPQ.
Although DistillVQ outperforms OPQ for standalone dense retrieval,
% (0.365 vs. 0.347 MRR@10),
relevance of S+CluSD with OPQ for RetroMAE shown in  Table~\ref{tab:mainspace} is about the same  as S+CluSD with DistillVQ
(same 0.417 MRR@10;  0.986 vs. 0.987 R@1K).  
While switching to a different quantization affects compression and  relevance, 
the result shows that CluSD can work effectively and  outperform other baselines under the same quantization. 
We do not directly evaluate with RepCONC~\cite{2022WSDM-Zhan-RepCONC}
because ~\cite{Xiao2022Distill-VQ} shows that it has similar relevance to JPQ
and is outperformed by DistillVQ.

