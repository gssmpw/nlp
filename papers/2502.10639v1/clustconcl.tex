%\section{Limitations and Future Work}
\section{Concluding Remarks}

The contribution of this work is a cluster-based dense retrieval selection scheme  
to augment sparse retrieval with minimal space overhead and a small amount of extra latency  time.
CluSD optimizes an LSTM based model with cluster prioritization and pruning
to effectively identify  a small number of clusters that can supplement and boost  sparse retrieval. 
With this lightweight low-cost design,  
CluSD can deliver a strong in-domain and out-of-domain relevance  efficiently when searching  MS MARCO and BEIR datasets
on a CPU only server. %even though the sparse retriever used has its relevance tradeoff under a time efficiency constraint.
For example, CluSD delivers 0.423 MRR@10  for searching 8.8M MS MARCO passages 
with 11.4ms  latency and  1.13GB space mainly for compressed dense embeddings, to augment sparse retrieval under
a fraction of its  cost. The same  CluSD version  reaches  0.514 average NDCG@10 for BEIR with compression.

Compared to re-ranking approach, CluSD outperforms for recall in MS MARCO while having a marginal improvement for MRR@10 numbers 
while  CluSDâ€™s average NDCG number is 7.2\% and 4\% higher than re-ranking with RetroMAE and SimLM, respectively for BEIR.
%It is attractive to run such an algorithm on a first-stage retrieval low-cost machine
%to host  as many documents as possible and 
%that can  significantly reduce the overall infrastructure cost when scaling  data size and  search traffic. 

\comments{
 While IVF does not add extra space cost,
the relevance  of IVF top cluster search under the time budget performs  worse than others.
Simple re-ranking ``D-rerank'' achieves good MRR@10 but under-performs on recall. 
When the recall of a sparse retriever is relatively low, it cannot boost further as seen in the case of $m=64$.
}

Compared to a  graph navigation based approach, the main advantage of CluSD is its capability in delivering 
a similar or higher relevance under the same time budget with negligible extra space overhead. 
HNSW and LADR perform comparably, at the cost of extra online memory overhead  and extra complexity to compute offline in advance.
HNSW requires extra 9.2GB and LADR requires extra 4.3GB to store graph data in memory.
When we  try to reduce their graph size by decreasing  the number of neighbors for HNSW and LADR, their  MRR@10 and/or  recall@1000 numbers drop visibly. For example,
using the default 32 neighbors for the HNSW graph results in 2.4GB graph space, however  MRR@10 drops from 0.407 to 0.4 under the 25ms time budget.



\comments{
Our evaluation uses OPQ in FAISS to compress embeddings. 
Switching to a different quantization affects the compression ratio and  the relevance outcome.  
However, we expect that our findings remain similar because  the recent  quantization 
methods~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}
also have  a significant relevance drop pattern when the compression ratio increases.
%A limitation of the current  CluSD work is that the training of the LSTM model has a dependence on the sparse retrieval model used. 
While  CluSD can  boost the relevance of a weaker sparse retriever model such as BM25, the augmented relevance still has not reached a level
comparable  to the one where CluSD is coupled with a stronger sparse retriever. There is room for improvement in future work.
}
%the dependency on a training dataset. Unlike rule-based or unsupervised pipelines, our pipeline requires labeled data on at least one search task. However, the number of queries with labels is not large and a trained model is robust on never-seen corpus and tasks. 
%
%Another limitation is the dependency on the sparse retrieval model performance. CluSD benefit from a good sparse retrieval top results. When using a BM25 based retrieval, the quality of the clusters selected degrades slightly. 
