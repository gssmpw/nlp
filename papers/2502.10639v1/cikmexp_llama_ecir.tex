%\subsection{CluSD with LLaMA-2 based dense retrieval}
{\bf CluSD with LLaMA-2 based dense retrieval}.
%\label{sect:llama}
Table~\ref{tab:RepLLaMA}
demonstrates that CluSD effectively supports selective RepLLaMA dense retrieval~\cite{ma2023finetuning} 
to take  advantage of   LLMs on CPUs. 
The ``Latency'' column includes sparse retrieval CPU time when ``S+'' is marked in the method name. 
S=SPLADE-HT1.
%This LLaMA-2~\cite{touvron2023llama} based model  
RepLLaMA  uses 145GB embedding space with 4K dimensionality.
% for MS MARCO passages. 
All rows except the last row  conduct in-memory search assuming data fits into memory.
Row 1 shows  it takes 7.9 second CPU time to conduct in-memory full RepLLaMA  retrieval. 
Row 2  shows performance of RepLLaMA using OPQ quantization but without IVF selective search.
Row 3  shows performance of RepLLaMA using OPQ and  IVF selective search.
Use  of these optimization techniques leads to  a significant relevance degradation. 
As a reference, Row 4 lists the fusion of SPLADE and  full dense retrieval. 
%Rows 5 and 6 are CluSD with SPLADE-HT1 retrieval without or with disk data.
%The latency includes sparse retrieval. 
CluSD visits  a small number of clusters from the 60,000 dense RepLLaMA clusters. 
CluSD  with SPLADE-HT1 takes about 39ms for in-memory search shown in  
Row 6, and  60ms with on-disk search shown in Row 8.  
%The latency time includes both sparse and dense retrieval on a CPU server. 
RepLLaMA inference requires five Nvidia 32GB V100 GPUs  to host data and take 37ms GPU time. 
Thus, CluSD  can deliver 1 to 2 orders of magnitude of improvement in either total latency or infrastructure cost.
Rows 5 and 7 show the performance of CDFS, which is similar to CluSD for in-memory search, but 23.3\%
slower than  CluSD for on-disk search while achieving a similar relevance.
% worse, but without making the assumption of statistical distribution.





\comments{
\begin{verbatim}
MRR@10	R@1K	CPU Latency	Space
(1) RepLLaMA	0.412	0.990	7.9s	145GB
(2) RepLLaMA-ivfopq	0.365	0.915	97ms	6.1GB
(3) S + CluSD	0.424	0.986	60ms	149GB
(4) S + RepLLaMA	0.426	0.994	7.9s	149GB
(5) RepLLaMA-OPQ	0.384	0.990	666ms	2.4GB

\end{verbatim}
}

\begin{table}[htbp]
\vspace*{-1em}
\small
        \centering
        \caption{Selective fusion with  dense RepLLaMA and  sparse SPLADE} 
                \begin{tabular}{r r |l l|l l}
                        \hline\hline 
     &Method    & MRR@10 & R@1K & Latency & Space \\
                        \hline
1.  & RepLLaMA     & 0.412$^\dag$& 	0.990& 	7.9 s& 	145 GB\\
2.   &RepLLaMA-OPQ& 	0.384$^\dag$& 	0.990& 	666 ms& 	2.4 GB \\
3.  & RepLLaMA-IVFOPQ & 	0.365$^\dag$ & 	0.915$^\dag$ & 	97 ms & 	6.1 GB\\
4.   & S + RepLLaMA  & 	0.426& 	0.994& 	7.9 s& 	149 GB\\
5.   & S+CDFS & 0.425	& 0.987& 	41 ms& 	149 GB\\
6.   & S+CluSD & 0.424	& 0.986& 	39 ms& 	149 GB\\
7.   & S+CDFS (on-disk)& 	0.425	& 0.987& 	74 ms& 	149 GB \\
8.   & $\blacktriangle$ S+CluSD (on-disk)& 	0.424	& 0.986& 	60 ms& 	149 GB\\
                \hline\hline
                \end{tabular}  
        
 \vspace*{-5mm}
        \label{tab:RepLLaMA}
\end{table}




For the BEIR datasets,
RepLLaMA achieves an average NDCG of 0.561. 
While significantly faster than RepLLaMA, CluSD achieves an average NDCG of 0.541 while CDFS reaches 0.554.
However, CluSD's LSTM model for RepLLaMA was used in a zero-shot manner after being trained using SimLM on MS MARCO passage.
In the future we expect that further improvement in CluSD is possible if its LSTM is trained using RepLLaMA.
%data and then applied to search MS MARCO Dev and BEIR datasets and we expect that further improvement is possible in the future.
%(a 3.6\% NDCG degradation) 

