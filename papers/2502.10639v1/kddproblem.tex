\section{Background and Related Work }
%\section{Problem Definition}
\label{sect:background}

\comments{
\begin{table}[h]
	\centering
		%\resizebox{1.02\columnwidth}{!}{ 
		\begin{small}
		\begin{tabular}{l l}
			\hline
			Symbol & Def. \\
                \hline
                %$\mathcal{D}$ & A set of documents in a dataset\\
                %$N$ & Total number of documents in a dataset \\
                $Q$ & A query \\
                $d_i$ & A document \\
                $\mathcal{D}^{+}$, $\mathcal{D}^{-}$ & Positive/negative document subsets corresponding to $Q$  \\
                $|.|$ &  The size of a set\\
		$\Theta$& Parameters of a scoring model\\
		$S(Q,d_i, \Theta)$ & The rank score of a document for a query using a model\\
%			& based on a model with parameters $\Theta$. \\
                $p_i$ & Teacher's top one probability of document $d_i$,\\
			& standing for  $P(d_i|Q, \mathcal{D}^+, \mathcal{D}^-, \Theta$) \\
                $q_i$ & Student's top one probability of document $d_i$\\
                $\pi(.)$ & The rank position of a document for a query\\
                $\beta_i$ & Exponent weight bias for negative  document $d_i$\\
                $\gamma, \alpha$ & CKL hyper-parameters\\
                \hline
		\end{tabular}
		\end{small}
	%}
	\caption{Table of Symbols}

	\label{tab:data}
\end{table}
}

\textbf{Problem definition}
Given query $q$ for searching a collection of $D$ text documents:   $ \{d_i\}^D_{i=1}$, 
retrieval  obtains the top $k$ relevant documents from this collection based on
the similarity between query $q$ and document  $d_i$ using  their
representation vectors.

With a lexical representation, each  representation vector of a document or a query is sparse.
Sparse retrieval uses  the dot product of their lexical representation vectors as the similarity function
and implements its search efficiently using an inverted index. 
\begin{equation}
\label{eq:sparseretrieval}
L(q) \cdot  L(d_i)
\end{equation}
%L(q,d_i ) = 
where $L(.)$ is a lexical representation   which is a vector of weighted term tokens for a document or a query.
For BM25 based lexical representation~\cite{Robertson2009BM25}, 
$L(.)$ treats each document or a query as a bag of terms.
% Q: do we want to keep referring to everything as bag of terms?
%and term weights  are scaled based on   frequency of terms within a bag and  across all bags involved.
Term weights are scaled based on the frequency of terms within a document and across all documents in the collection.
For a learned sparse representation~\cite{Dai2020deepct, Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL}, 
a document or a query is encoded using  a trained neural ranking model, which
%produces a bag which may contain original  and  expanded terms.
produces modified representations of document or query which may contain both original and expanded terms.
The encoding functions for a query and document can be different. Without the loss of generality, we assume they are the same in this paper.

With a single-vector dense  representation, each  representation vector of a document or a query is a dense vector. 
Dense retrieval computes the following rank score
\begin{equation}
\label{eq:denseretrieval}
R(q) \cdot  R(d_i)
\end{equation}
%D(q,d_i) 
where $R(.)$ is a dense representation vector  of a fixed size~\cite{Karpukhin2020DPR}.


  %revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
%The weights of these terms evaluated in this paper use the SPLADE model as discussed below while other options are possible.
%(e.g., a bag of words or a neural encoder). 
%The implementation of such a dot product for a sparse retriever uses the inverted index data structure.
\comments{
%{\bf Learned term weights with SPLADE}.
For each document, the SPLADE term weighting revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
score $w_j$ of $j$-th token term in BERT WordPiece vocabulary.
\begin{equation}
\label{eq:splade}
w_j = \sum_{i \in d}  log (1 + ReLU ( H(h_i)^T E_j + b_j)) 
\end{equation}
where document $d$ consists of a sequence of BERT last layer's embeddings $(h_1, h_2, $
$\cdots, h_n)$.
$E_j$ is the BERT input embedding of the $j$-th token and $b_j$ is a token level bias.
$H(.)$ is a linear layer with activation and layer normalization. 
% (In SPLADE GeLU activation is used). 

{\bf Combining the results of a dense retriever and a sparse retriever.}
}
Following the work of~\cite{kuzi2020leveraging, 2022LinearInterpolationJimLin, Gao2020ComplementingLR}, 
linear interpolation is used  to  ensemble  dense and sparse retrieval scores. 
%Let $S$ be the set of top results obtained by a sparse retriever, and $D$ be
%be the set of results obtained by dense retrieval. 
%For any document $d_i \in S \cup D$, its  
The fused rank score for document $d_i$ in responding  query $q$ is:
\begin{equation}
\label{eq:fusion}
\alpha L(q) \cdot  L(d_i) +
(1-\alpha) R(q) \cdot  R(d_i) 
\end{equation}
where $\alpha$ is a coefficient in a range between 0 and 1.

\comments{
Notice that   the two ranked lists $S$ and $D$ are usually not fully overlapped and  some documents only exist in one list. While 
different  treatments on these documents affect fusion performance,
we take an option to  impute the score of such a document score   using $\beta * score (top \  k+1) $  in the missed  ranklist where $\beta$
is a scaling factor.
%While there are options to We consider a few options
For example,  we  impute the lexical scores of documents expanded by dense retrieval
as the top-$(k+1)$ lexical  score scaled by a factor. We use $\beta=0.95$  in our evaluation.
 
%\item Remove the documents when they only exist in one ranked list
%    \item  Take the document and compute the score for only one ranked list
}


Our goal is to maintain a relevance competitive to full dense retrieval, while minimizing the time and memory overhead of cluster-based dense retrieval.
We propose leveraging sparse retrieval to minimize the scope of dense clusters to visit.
% Parker: below is awkard, and a bit of a run-on sentence. Above I reworked it, but it's still not great
%Our optimization goal is to minimize the scope of cluster-based dense retrieval and extra memory space overhead by leveraging sparse retrieval 
%while maintaining a relevance competitive to full dense retrieval.  
%In this way, limited dense retrieval that augments sparse retrieval is fast with minimum space consumption  on a CPU server. 
\comments{
The ensembling method used in our evaluation is 
%The final ranking of the top $K$ documents is 
the reciprocal rank fusion (RRF) following Cormack et al.~\cite{Cormack2009RRF}, considering ranking positions of each candidate 
generated by a retriever and by a re-ranker. 
We use RRF
% above reciprocal rank fusion 
instead of linear interpolation as it has a similar performance with a simplified combination of two scoring models. 
%We choose RRF because it is easily applicable to multiple ranked lists ignoring the score distribution differences. 
\begin{equation}
     %RRF(Q,d) =  \sum_{r \in {rt, rr,}} \frac{1}{k_r + r(d)},
     RRF(Q,d) =  
\frac{1}{c_1 + \pi_1(d)} + 
\frac{1}{c_{2} + \pi_2(d)},
     \label{eq:score}
\end{equation}
where $c_{1}$ and $c_{2}$ are fixed smoothing constants. $\pi_2(d)$ is the rank position based  a re-ranker formula
while $\pi_1(d)$ is the rank position based on a retriever.
}

