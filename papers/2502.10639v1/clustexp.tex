\section{Evaluations}


\comments{
Our evaluation answers  the following research questions around our proposed techniques and the correspond subsection are marked.

\textbf{RQ1} (Section~\ref{sect:eval0space}): 
How does CluSD compare against other low-cost baselines when space is constrained?
%extra space overhead is minimized by augmentation under different compression configurations? 

\textbf{RQ2} (Section~\ref{sect:evaltimebudget}): 
How does CluSD  perform under a retrieval latency time budget when there is no space restriction?

\textbf{RQ3} (Section~\ref{sect:evalbeir}): 
How does CluSD  perform in searching  out-of-domain text content?
Zero-shot retrieval performance is important since a large dataset can contain documents from many domains. 


\textbf{RQ4} (Section~\ref{sect:evalLSTM}): 
Is  LSTM with cluster pruning preferred compared to other design options?
Are  features designed for LSTM useful?

\textbf{RQ5} 
(Section~\ref{sect:clusterno}): 
How does the number of clusters used in dataset partitioning 
affect the relevance and latency of CluSD? 

\textbf{RQ6} 
(Section~\ref{sect:threshold}):
How does the choice of selection threshold value in LSTM affect the average number of clusters selected?
%Does the cluster selection model dynamically select different number of clusters for different queries? How the selection differs for different datasets?

\textbf{RQ7}
(Section~\ref{sect:evalsparse}):
How does CluSD perform in augmenting  different sparse retrievers with different efficiency and relevance characteristics?


}

{\bf Datasets and measures.}
Our evaluation uses the MS MARCO datasets with 8.8 million passages for full passage ranking~\cite{Campos2016MSMARCO,Craswell2020OverviewOT}. 
%MS MARCO contains  8.8 million passages and about 1.1  judgment label per query. 
The test query sets are the Dev set with 6980 queries and TREC deep learning (DL)  2019/2020 tracks with 43 and 54 queries.
%The average number of WordPiece~\cite{wordpiece} tokens per passage is 67.5.
%The development (Dev) query set contains  6980 test queries. %with about 1.1  judgment label per query. 
%The test sets in  TREC deep learning (DL)  2019 and 2020 tracks 
%provide 43 and 54 queries, respectively, with many judgment labels per query.
%which provide 54 and 43 test queries, repsectively,  
Following the common practice for Dev, 
%official leader-board standard, 
we report mean reciprocal rank at 10 (MRR@10) and recall at 1000 (recall@1K) 
which is the percentage of relevant-labeled results appeared in the final top-1000 results.
%we report mean reciprocal rank (MRR@10) for relevance instead of
%Normalized discounted cumulative gain (NDCG)~\cite{NDCG}  is not used because such a set has about 1.1 judgment label per query, which is too sparse to use NDCG.
For DL19 and DL20 sets, we report NDCG@10.
%which have many judgment labels per query, we report the commonly used NDCG@10 score.
% when space allowed, we also report the recall@1k. 
%If space allows,
%In addition, we also report recall at 50 (recall@50) 
%as if we use large language model as a reranker, it is common practice to take less than 100 documents from retrieval model for evaluation.
%For TREC DL test sets which have many judgment labels per query, we report the commonly used NDCG@10 score.
%When space allows, we also report the recall ratio at 1000 
We report average NDCG@10 for the 13 publically available BEIR datasets~\cite{thakur2021beir} 
to evaluate  zero-shot retrieval performance. 
\comments{
BEIR is a heterogeneous benchmark containing a variety of IR tasks, 
including search tasks (e.g.  question-answering, bio-medical IR, and entity retrieval)  
and semantic relatedness tasks (e.g.  duplicate question retrieval). 
The size of these data sets ranges from 3,633 to 5.4M.
%Their average query length ranges from 5.39 to 192.98, 
%and  their average document length ranges from 11.44 to 634.79. 
%Each dataset has binary or three-level relevance labels.
}

{\bf Models and parameters.}
For sparse retrieval, we adopt 3 SPLADE-based  models and one BM25 based model with different relevance and efficiency  tradeoffs:
1) SPLADE-HT1 is based on   a recent SPLADE model optimized by  hybrid thresholding~\cite{2023SIGIR-Qiao}
and BM25-guided traversal~\cite{mallia2022faster,qiao2023optimizing} with index space 3.9GB
and 31.2 ms retrieval latency. 2) SPLADE-HT1-fast is a variation of SPLADE-HT1 with a different parameter setting, which has 
3.9GB index space and 22.4 ms latency. 
3) SPLADE-effi-HT3 is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} with 3GB space
and  12.4ms latency due to use of hybrid thresholding and BM25 guided traversal techniques. 
4) BM25-T5 applies  the BM25 lexical model on expanded documents using DocT5Query~\cite{Cheriton2019doct5query}. 
BM25-T5 uses 1.2GB index space with latency 9.2ms. 
For dense retrieval, we adopt the recently published   dense models
SimLM~\cite{Wang2022SimLM} and RetroMAE~\cite{Liu2022RetroMAE}. 
We use their   checkpoints from the Huggingface library to generate document and query embeddings 
and  use  K-means in the FAISS library~\cite{johnson2019billion} to derive dense embedding clusters.
Our evaluation implementation uses C++ and Python and the
related code will be released to the public after publication of this paper. 
\comments{
%Note that SimLM model's embeddings are normalized while RetroMAE's embeddings are not. 
For sparse retrieval, we use two fast versions of SPLADE and a SPLADE-efficient model, as well as BM25-T5. 
The implementation of SPLADE models follows 2GTI optimization~\cite{qiao2023optimizing}. %its official release~\cite{spladeRepo}. 
SPLADE-HT1 is the SPLADE model trained with hybrid thresholding scheme~\cite{2023SIGIR-Qiao} using $\lambda_{t}=1$, 
SPLADE-effi-HT3
is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} trained with hybrid thresholding scheme using $\lambda_{t}=3$. Both of these two models are using the "accurate" configuration in 2GTI during retrieval, while SPLADE-HT1-fast uses the "fast" configuration.
The sparse retrieval code for Splade and BM25-T5 uses the code from PISA~\cite{mallia2019pisa}
with some optimization~\cite{Lassance2022SPLADE-efficient,2023SIGIR-Qiao}.
}

To train the CluSD model, we sample 5000 queries from the MS MARCO training set 
%and use the LSTM unit in Pyytorch. 
and the hidden dimension for LSTM is 32. The models are trained for 150 epochs. 
%The input documents are sorted based on the distance between the centroids the documents belong to and  the query. 
For sparse and dense model interpolation, we use min-max normalization to rescale the top results per query. 
%Then we use linear combination with equal weights to combine two ranked lists. 
For  BM25-T5, the interpolation  weights are  0.05 and 0.95 for sparse  and  dense scores, respectively. 
For SPLADE, they are  0.5 and  0.5, respectively. 
%In terms of missing document in either ranked list, as the smallest value from each normalized list is 0, there is no missing imputation needed as we 
%can view the missing  documents taking the value of the worst document in the ranked list automatically. 
For CluSD, the default setting of number of clusters is $N=8192$, and the input sequence length to the LSTM model is truncated at $n=32$.  
To report the mean single-query latency and 99 percentile latency in milliseconds,
we  run test queries multiple times  from MS MARCO Dev set using a single thread on  a 2.25GHz AMD EPYC 7742  CPU server. 

%{\bf Baselines and notations.}
%We select two groups of baselines under space and time constraints respectively.
%\label{sect:eval0space}

\begin{table*}[htbp]
	\centering
		\resizebox{1.9\columnwidth}{!}{
		\begin{tabular}{ r r l |l l ll l |llll l|r}
			\hline\hline
			 & & & \multicolumn{2}{c}{ 
{\bf MSMARCO Dev} }& {\bf DL19}& {\bf DL20} & {\bf BEIR} 
& \multicolumn{2}{c} { {\bf MSMARCO Dev} }& {\bf DL19}& {\bf DL20} & {\bf BEIR} 
& {\bf  Latency}    \\
			& &\% D& {MRR@10}& R@1k& {NDCG@10}&  {NDCG@10}& {NDCG@10}& {MRR@10}& {R@1k}& {NDCG@10}&{NDCG@10}& {NDCG@10}& ms \\
    \hline
 1. & S={\bf SPLADE-HT1} & --& 0.396& 0.980 & 0.732   & 0.721    & 0.500 & -- & --& --& --& --&31.2  \\
 & & & \multicolumn{5}{c|}{D=\textbf{SimLM}}& \multicolumn{5}{c|}{D=\textbf{RetroMAE}}&\\
   \hline
   &&& \multicolumn{10}{c}{\bf{Uncompressed flat setting. Embedding space  27.2GB}}\\
     \hline
          2. &   D  & 100 & 0.411$^\dag$ & 0.985$^\dag$& 0.714 &  0.697  & 0.429 & 0.416$^\dag$& 0.988& 0.720&0.703& 0.482 & 1674.1 \\
            3. &   $\blacktriangle$ S $+$ D   &100 & 0.424 & 0.989 & 0.740 & 0.726  & 0.518 & 0.425& 0.988& 0.740&0731& 0.520~& $+$1674.1 \\
           4. &    \bf S $+$ CluSD &0.3& 0.425 & 0.987 & 0.744 & 0.724  & 0.516 & 0.426& 0.987& 0.744&0.734 & 0.518 & $+$13.2 \\
              \hline
            &&& \multicolumn{10}{c}{\bf{OPQ $m=128$. Embedding space  1.1GB}}\\
              \hline
            5. &   $\blacktriangle$  S $+$ D-OPQ &100 & 0.420 & 0.988 & 0.735 &  0.719  & 0.508 & 0.416 & 0.988 & 0.737 & 0.732 & 0.515& $+$568.9 \\
            6. &  S $+$ D-IVFOPQ &10& 0.414$^\dag$& 0.987 & 0.726 & 0.734  & 0.505 & 0.404$^\dag$& 0.987& 0.713&0.722& 0.513 &  $+$95.2 \\
           7. &   &5& 0.407$^\dag$& 0.987 & 0.717 & 0.733  & 0.499  & 0.394$^\dag$& 0.987& 0.687&0.817&0.507&$+$ 48.8 \\
           8. &   &2& 0.392$^\dag$& 0.986$^\dag$& 0.691 & 0.738  & 0.467 & 0.374$^\dag$& 0.986$^\dag$& 0.656&0.815& 0.499&$+$21.3 \\
           9. &   \textbf{S $+$ CluSD}  &0.3& 0.423 & 0.987 & 0.739& 0.725 &0.514 & 0.417& 0.986& 0.742&0.735& 0.514  & $+$11.4  \\
             \hline
           & & & \multicolumn{10}{c}{\bf{OPQ $m=64$. Embedding space 0.6GB}}\\
            \hline
           10. & $\blacktriangle$  S $+$ D-OPQ  &100 & 0.409 & 0.986 & 0.718 &  0.721  &0.501 & 0.402 & 0.986 & 0.717 &  0.719 & 0.508 &$+$ 290.4 \\
           11. &   S $+$ D-IVFOPQ &10& 0.405 & 0.986 & 0.716 & 0.734 & 0.502 & 0.393$^\dag$& 0.986& 0.676&0.730& 0.505& $+$44.4 \\
           12. &   &5& 0.397$^\dag$& 0.986 & 0.704 & 0.733  & 0.497 & 0.384$^\dag$& 0.985& 0.659&0.717& 0.500& $+$23.8 \\ 
            13. &  &2& 0.383$^\dag$& 0.985 & 0.680 & 0.738 & 0.487 & 0.368$^\dag$& 0.985 & 0.643&0.707& 0.493 & $+$11.2 \\
            14. &  \textbf{S $+$ CluSD}  &0.3& 0.414 & 0.986 & 0.743& 0.726 & 0.511 & 0.403 & 0.987& 0.729&0.724 & 0.506 & $+$9.6 \\
              \hline\hline
		\end{tabular}
		}
	\caption{
Sparse retrieval argumentation with minimum extra space overhead.
%no  with compressed or uncompressed dense emebddings S=SPLADE-HT1, D=SimLM or RetroMAE. 
%Model relevance and latency under compressed or uncompressed dense emebddings S=SPLADE-HT1, D=SimLM or RetroMAE. 
%Row  ``S $+$ D'' marked with $\blacktriangle$ indicates the oracle setting of sparse model interpolated with full dense retrieval. 
For MSMARCO Dev set, $^\dag$ is tagged when statistically significant drop is observed from the oracle $\blacktriangle$ at 5\% confidence level. 
}
\vspace*{-5mm}
	\label{tab:mainspace}
\end{table*}
	%\caption{Model relevance and latency under different configurations with storage constraints. S=SPLADE-HT1, D=SimLM or RetroMAE. The row  ``S $+$ D'' marked with $\blacktriangle$ indicates the oracle setting of sparse model interpolated with full flat dense search model. For MSMARCO Dev set, $^\dag$ is marked when statistically significant drop is observed from the oracle $\blacktriangle$ at 5\% confidence level.



{\bf Fast search with minimized space overhead}
Table~\ref{tab:mainspace} 
Table~\ref{tab:mainspace} compares   CluSD and the baselines in searching MS MARCO and BEIR datasets with memory space constraint. 
It uses two quantization  configurations and the uncompressed setting. 
The sparse model used is SPLADE-HT1 while the dense models are SimLM and RetroMAE.
%The top portion of this table reports the performance when compression is not used, which captures a peak possible performance.
IVFOPQ is compared, which use some percentage of top dense clusters sorted by query to centroid clusters.
%against CluSD as it selects only top clusters and does not incur extra space overhead. 
We include dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
OPQ quantization  is configured with  the number of codebooks as $m=$ 128 or 64.  
We report performance of IVF clustering using top 10\%, 5\%, 2\% clusters respectively
as a standard approach which performs   limited top cluster retrieval. 
CluSD execution is also based on the the same OPQ or  IVFOPQ index from  FAISS but clusters are  selected  by our LSTM model. 


Notation ``S+D'' in Table~\ref{tab:mainspace} (and Table~\ref{tab:sparse} later) 
reports the fused performance of a sparse model and a dense model with linear interpolation. 
Column marked with  ``\%D'' means the percentage of dense embeddings  evaluated. 
Last column on ``Latency'' is the average single-query time using SimLM for MS MARCO Dev set.  
We only report SimLM latency because the latency of SimLM and RetroMAE are similar. 
When 	prefix `+' is marked for a latency entry, it indicates  the extra milliseconds is spent 
for full or selective dense retrieval without counting   sparse retrieval time.
CluSD uses setting $n=32$
% Tables~\ref{tab:mainspace} and~\ref{tab:maintime} use 
which results in  selection of 22.3 clusters on average per query 
and allows CluSD to meet  the latency requirement.
%Switching a quantization different  from OPQ  is might result in an improvement in relevance, but it is orthogonal to our proposal. 
%The reported CluSD performance on the flat index uses the exact same IVF index. 
% for CluSD uses on average 20-25 (0.25 - 0.3\%) clusters per query. 
%For re-ranking, we report the results of re-ranking top 1000 results from the sparse model using the dense model.  
The mean latency and 99 percentile latency of CluSD are 13.2 ms and 18.1 ms for searching uncompressed flat index, respectively; 
11.4 ms and 14.6ms for searching OPQ m=128 index;  9.6 ms and 11.1ms for searching OPQ m=128 index.

For MS MARCO Dev set, the result of a method is marked with  tag $^\dag$ when 
statistically significant drop is observed compared to  an oracle baseline
tagged with $\blacktriangle$ at 5\% confidence level.  This oracle
is sparse retrieval fused with full dense retrieval marked as ``S+D'', which typically delivers the highest relevance  
using all uncompressed or quantized dense embeddings ($m=128$ or $m=64$).

Table ~\ref{tab:mainspace} 
shows that
selective dense  to augment sparse retrieval by top IVF clusters sorted by query-centroid distances 
can be fast below  100ms  when the percentage of clusters searched  drops from 100\% to 10\%. But the relevance drop is significant. 
Moreover, CluSD is as  fast  as IVFOPQ  top 2\% search at m=64 and 2x fast at m=128 while
its relevance is much higher.
CluSD  performs  closely as the oracle at each compression setting, which  
indicates CluSD effectively minimizes visitation of unnecessary  dense clusters. 

 

{\bf Search with no memory space constraints}.
%\label{sect:evaltimebudget}
Table~\ref{tab:maintime} compares  CluSD with  several baselines under two time budgets when memory space restriction is removed.
%Uncompressed dense embeddings  are used to exclude the relevance impact by quantization.
The last two columns report the total latency time in milliseconds including sparse retrieval when needed,
and data space cost. Extra memory space on the top of sparse retrieval is marked with prefix `+'. 
``MRR'' means  MRR@10 and ``NDCG'' means  NDCG@10.
The extra space for quantized inter-cluster distances for CluSD takes about 32MB. 
%Table~\ref{tab:maintime} compares  baselines  without  memory space constraint. 
For proximity graph-based dense retrieval which incurs additional graph space overhead,
we include  HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR} as shown in Table~\ref{tab:maintime}. 
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint. 
%For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as 50 to meet the latency requirements in 
%Table~\ref{tab:maintime}.
%Uncompressed dense embeddings (marked as "flat") are used
%to exclude the impact due to quantization.
We also compare  a simple re-ranking method,  denoted as ``D-rerank'' or ``rrk'', 
that fetches the dense scores of top 1000 of sparse retrieval to re-rank with interpolation. 
%We use the same clustering setting for IVF and CluSD. 




%As discussed in Section~\ref{sect:design}, the overall latency budget is set as 50ms and 25ms respectively. 
We allow some time for budget relaxation since it is difficult to tune parameters of all baselines to meet the constraints exactly. 
For IVF, we use 1.5$\%$ and 3$\%$ top clusters for ``S+IVF'' and ``IVF''.
%  but we allow some variation for some baselines because it is difficult to find parameters that exactly control the latency under this budget. 
%In this case, we choose the dense configuration such that the overall latency is within 70 or 30 milliseconds. 
For HNSW under 50ms time budget, we set its expansion factor parameter (ef) as 1024, 
2048 for S$+$D-HNSW and D-HNSW. 
Under 30ms time constraint, the expansion factor (ef) is  512 and  1024 respectively. 
%We include option ``S+Re-rank" which uses the top 1,000 sparse retrieval results and only compute dense scores of these top results, and merge the score with an interpolation.
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint.
For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as  50 to meet the latency budget.
%Table~\ref{tab:maintime}.
\comments{
Under time budget 50ms, 
model  SPLADE-HT1 is used and it has a latency of 31.2 ms in sparse retrieval.  
Under  time budget 25ms, model SPLADE-effi-HT3 is used and it is more aggressive in  efficiency 
with a 12.4ms latency at the cost of a relevance degradation. 

%using the dense model only, We also include sparse retriever 
%IVFOPQ, IVF, OPQ and HNSW using the dense model only, We also include sparse retriever 
%with reranking top 1000 results using the dense model. We also include the results of document evaluation using LADR algorithm. 
Note that when ``S$+$'' is in the row, it means we use the sparse model and the reported results are interpolated. 

}
%[DISCUSS TABLE 4 RESULTS.]
\comments{
The takeaways from this table are:

\textbullet\ While IVF does not add extra space cost,
the relevance  of IVF top cluster search under the time budget performs  worse than others.
Simple re-ranking ``D-rerank'' achieves good MRR@10 but under-performs on recall.
When the recall of a sparse retriever is relatively low, it cannot boost further as seen in the case of $m=64$.

\textbullet\ HNSW and LADR perform comparably, at the cost of extra online memory overhead  and extra complexity to compute offline in advance.
HNSW requires extra 9.2GB and LADR requires extra 4.3GB to store graph data in memory. 
When we  try to reduce their graph size by decreasing  the number of neighbors for HNSW and LADR, their  MRR@10 and/or  recall@1000 numbers drop visibly. For example, 
using the default 32 neighbors for the HNSW graph results in 2.4GB graph space, however  MRR@10 drops from 0.407 to 0.4 under the 25ms time budget. 

\textbullet\ CluSD incurs negligible extra space cost other than hosting embeddings, compared to HNSW and LADR.
For $m=128$,  CluSD has a higher MRR@10 while its DL19 and DL20 numbers are higher than HNSW, but comparable as LADR.
CluSD has more  advantages in relevance when $m=64$. 

\textbullet\ The time budget setting is discussed in Section~\ref{sect:design}.
Even when we relax the latency requirements for the baselines, CluSD still maintains its advantages in different datasets. 
For instance, if we double the time budget to around  110ms
for 	``S+D-HNSW'' after  SPLADE-HT1 retrieval, its MRR@10 for MS MARCO Dev is 0.422, which is still below S$+$CluSD with 46.3ms.


}

 

\begin{table}[htbp]
	\centering
 \resizebox{0.9\columnwidth}{!}{
		\begin{tabular}{r |llll|rr}
			\hline\hline
			& \multicolumn{2}{|c}{\bf{MSMARCO Dev}}& \bf{DL19}& 
\bf{DL20}& \bf{Latency}& \bf{Space}\\
			 & {MRR}& {R@1k}& \smaller{NDCG} &   \smaller{NDCG} & Total& GB\\
			\hline
        \multicolumn{7}{c}{\bf{Time Budget = 50 ms, S=SPLADE-HT1}}\\
        \hline
            D=SimLM & 0.411 & 0.985$^\dag$& 0.714 &  0.697 & 1674.1 & 27.2 \\
          S& 0.396$^\dag$& 0.980$^\dag$& 0.732   & 0.721    &   31.2 & 3.9 \\
           S $+$ D-rerank &  0.421 & 0.980$^\dag$& 0.745 & 0.728 & 34.6 & 31.1 \\
             $\blacktriangle$  S $+$ D  & 0.424 & 0.989 & 0.740 & 0.726 & 1705.0 & 31.1 \\
              \hline
              % D-IVF & 0.398$^\dag$& 0.935$^\dag$& 0.690 & 0.693 & 81.4 & $+$0.0\\
            D-IVF & 0.389$^\dag$& 0.907$^\dag$& 0.671 & 0.676 & 49.5 & $+\sim$0\\
            S $+$ D-IVF & 0.393$^\dag$& 0.987  & 0.678 & 0.715 & 56.5 & $+\sim$0\\
            %D-OPQ& 0.314$^\dag$& 0.957$^\dag$& 0.619 & 0.608 & 109.1 & 0.3 \\
           % D-IVFOPQ & 0.386$^\dag$& 0.929$^\dag$& 0.680 & 0.697 & 48.8 & 1.2\\
             D-HNSW & 0.409$^\dag$& 0.978$^\dag$& 0.669 & 0.695 & 54.4 & $+$9.2\\
            %S $+$ rerank-D &  0.412 & 0.980    &0.715  & 0.811 & 0.698 & 0.815 & $+$2.3 \\
            S $+$ D-LADR & 0.422& 0.984$^\dag$& 0.743&0.728& 43.6& $+$4.3\\
%           7.1 &  S $+$ D-LADR & 0.423 & 0.987$^\dag$& 0.745 &0.728& 53.0 & $+$4.3\\
           % S $+$ D-IVF & 0.398$^\dag$& 0.987  & 0.674 & 0.720 & 64.7 & $+$0.0\\
           %S $+$ D-IVFOPQ & 0.392$^\dag$& 0.987 & 0.656 & 0.700 &  52.5 & 4.0\\
           S $+$ D-HNSW & 0.420 & 0.987 & 0.718 & 0.723 & 59.6 & $+$9.2\\
            \bf S $+$ CluSD & 0.426 & 0.987 & 0.744 & 0.724 &46.3& $+$0.03\\
       \hline       
            \multicolumn{7}{c}{\bf{Time Budget = 25 ms, S=SPLADE-effi-HT3}}\\
            \hline
            S& 0.380$^\dag$& 0.944$^\dag$& 0.721 &  0.726 & 12.4 & 3.0 \\
             S + D-rerank & 0.406 & 0.944$^\dag$& 0.728 & 0.738 & 15.1 & 30.2 \\
            $\blacktriangle$  S + D & 0.413 & 0.984 & 0.721 &0.729 & 1654.2& 30.2  \\
            \hline
            D-HNSW & 0.406 & 0.971$^\dag$& 0.672 & 0.694 & 28.4 & $+$9.2\\
             S $+$ D-HNSW & 0.407 & 0.984 & 0.706 & 0.724 & 28.7 & $+$9.2\\
            S $+$ D-LADR & 0.404$^\dag$& 0.984 & 0.728 &  0.736 & 24.2 & $+$4.3\\
%           16.1 & S $+$ D-LADR & 0.409 & 0.987 & 0.744 &  0.728  & 41.1 & $+$4.3\\
            \bf S $+$ CluSD & 0.412 & 0.984 & 0.733 & 0.728 & 25.4 & $+$0.03\\
			\hline\hline
		\end{tabular}  }
	\caption{Search under time budget but  no memory constraints 
}
  \vspace*{-10mm}
	\label{tab:maintime}
\end{table}
%For pipelines with a time budget, we report the additional overhead in the Storage (GB) column. 




