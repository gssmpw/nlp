%\section{SRF with Query-Specific Cluster Skipping }
%\section{CluSD: Selective Dense Retrieval}

  
%\section{Selective Dense Cluster Retrieval}
%%%%%%%%%\label{sec:clusd}

\comments{
It is common practice to interpolate the results from a sparse system and a dense system. We propose two directions to improve the latency of such system by avoiding the unnecessary dense evaluation on query level and per query level. We envision a hybrid system where sparse retrieval runs first and partial query and documents are evaluated by the dense retrieval system. 

The proposed CluSD method contains two components  during runtime inference to 
decide if  dense retrieval augmentation  can be skipped for a query and which  top dense document clusters should be selected  during supplemental retrieval.
}

%Per-query inference latency depends on the number of document evaluated and how per query-document evaluate is approximated. We do not touch per-query-document approximation in this paper and focus on reducing the number of documents evaluated. We cluster document using kmeans as IVF and would like to intelligently select a few most promising clusters to retrieve candidate results. 
\comments{
\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.9]{lstm.png}
    \caption{LSTM prediction for each query}
    \label{fig:lstm}
\end{figure}
}
\comments{
As discussed earlier, documents with embedding vectors are clustered based on their similarity to the cluster centroids. 
Our experiments take an advantage of the FAISS library which  provides such a support such as k-mean clustering.
Once a query is determined by CluSD to trigger  partial dense retrieval that selects and visits a subset of dense clusters. 
% online inference  will  visit the dense clusters one by one in
%an incresing order of   their centroid distance to the input query.
%A decision procedure is invoked to decide if a cluster to be visited should be skipped or not. 
Our objective is to examine the features of each cluster and  incrementally decide if this cluster should  be visited or not.
This section will first describe the overall flow of CluSD in online inference for selective fusion of sparse and dense retrieval,
 then present an LSTM-based  cluster selection algorithm. 
}


\subsection{ Online inference}
\label{sect:online}
\begin{figure}[htbp]
    \includegraphics[scale=0.79]{sparse_dense_flow_lout.drawio.pdf}
    \caption{The flow of cluster-based selective dense retrieval}  
 \vspace*{-5mm}
    \label{fig:flow}

\end{figure}





%Step 3. Call the LSTM selection (Section 4.1)  to choose a small number of dense clusters. That is about 25 in our MS MARCO evaluation.





The online inference algorithm of CluSD has the following steps:

\textbf{Step 1}:   Conduct sparse retrieval to obtain top-$k$ results.

%\item Conduct the query filter  to determine if the cluster-based dense retrieval is necessary.If needed, 
%Step 2:  Sort clusters based on the overlap degree of dense clusters and top sparse retrieval results and their  centroid-to-query  distances.

\textbf{Step 2}:  Conduct a two-stage selection process. 
Stage I selects the top-$n$ clusters from  the given $N$ dense embedding clusters, discussed in detail in Section~\ref{sect:stage1}.
%The  key  features collected include the overlap degree of dense clusters and top sparse retrieval results and their centroid-to-query distances.
Stage II  applies an LSTM model to the $n$ top clusters
to choose  a limited number of dense clusters to evaluate, discussed in Section~\ref{sect:lstm}. 

\textbf{Step 3}: Expand the top-$k$ sparse retrieval results to include the documents vectors of these dense clusters and
merge their sparse and dense scores with linear interpolation.  

%During the above linear interpolation, only top $k$ of sparese retrieval contains a lexical score,
%and thus we  impute the lexical scores of other documents expanded by cluster-based selective dense retrieval 
%as the bottom document score in the top $k$ sparse retrieval list scalled by a factor. We use 0.95 in our evaluation.
\comments{
Notice that  the two ranked lists from two retrievers are usually not fully overlapped and  some documents only exist in one list. While
different  treatments on these documents affect fusion performance,
we resort  to imputing the score of such a document using  the top $k+1$ score   in the missed rank list scaled by 
a factor.
%While there are options to We consider a few options
For example,  we  impute the lexical scores of new documents added  by dense retrieval
using  the scaled lexical score of  top $(k+1)$ in the sparse retrieval list. We use scale $0.95$  in our evaluation.
}
%\item Remove the documents when they only exist in one ranked list
%    \item  Take the document and compute the score for only one ranked list

%When we have access to a relatively strong sparse retrieval system and a dense retrieval system, it is feasible to selectively choose queries that requires evaluation of both systems. We hypothesize that the sparse system is economical and we can afford to run it for every query. The problem boils down to dynamically select queries to evaluate by dense retrieval.

{\bf Time  and space  cost for CluSD.} 
%We only consider the cost of  online partial dense retrieval. 
The embedding space is $O(D)$  for a corpus with $D$ vectors.
For MS MARCO, this  cost is around 27GB without compression. Quantization can reduce the total size to about  0.5GB  to 1GB, depending on codebook parameters.
Then this cost becomes a fraction of sparse retrieval index cost.  
Excluding embeddings, the extra space overhead for CluSD is to maintain  the similarity among centroids of $N$ clusters.
We only maintain top $m$ cluster neighbors for each cluster in terms of their centroid similarity. 
For our evaluation, $m=128$ and this reduces the extra space overhead as $O(N)$, which
is negligible compared  to the embedding space with $N << D$. 
For example,  with $N=8192$,  the inter-cluster graph for MS MARCO passages takes about 5MB with quantization.
%In our evaluation, %we use  $N=8192$ or  $4096$.
%the inter-cluster similar graph for MS MARCO passages with $N=8192$ takes about 32MB with quantization.

Excluding sparse retrieval cost, the time  cost of 
CluSD is dominated by the cost of cluster selection and  dense similarity computation of selected dense clusters.
The former is about $O(n)$ while the latter is
proportional to the number of clusters selected, which is capped by $O(\frac{D}{N} n)$. 
In  our evaluation of MS MARCO and BEIR, $n$ is chosen to be 32, and
the actual number of clusters selected and visited is about 22.3.
%Given the short time added by the pruned LSTM model, 
Thus, the overall latency overhead introduced by CluSD is fairly small. 


\subsection{ Stage I: Selection of  top $n$ candidate clusters} 
\label{sect:stage1}
\comments{
As discussed earlier,  we control 
the input length  of the above  LSTM model  to be  $n$  as 
the number of top candidates clusters to be examined, and then the inference 
}

Given  $N$ clusters grouped using dense document embeddings, 
we devise a strategy below to conduct a preliminary selection that
prunes low-scoring clusters that contain  only less relevant documents.
%prunes low-scoring clusters that may not provide a meaningful boosting of relevance quality,
%and  choose  top $n$ candidate clusters for Stage II for further refinement.
The two-stage approach is motivated by the fact that the LSTM model used in the second stage is time consuming to examine all clusters. 
%Thus, we truncate the sequence of clusters to maximum length $n$ and then take a sequence of top $n$ clusters as the input to this LSTM. 
Thus, we choose the top-$n$ candidate clusters first.
\comments{
There are  $n$ nodes corresponding to these clusters in the LSTM model.
We will discuss how to truncate with a prioritization shortly below.
%Because this decision procedure uses the document scoring information  of each cluster visted 
}
\comments{
Table ~\ref{tab:LSTMclustertime} shows that the number of clusters to be examined
varies from 32 to 512, the LSTM inference time increases from  2.8ms to 33.9ms.
As we would like to make a quick decision given the overall  time budget,
we devise another strategy to prune clusters that may not provide a meaningful boosting
of relevance quality. 

% dependent on the length of the sequence input. 

\begin{table}[h]
	\centering
		\resizebox{0.68\columnwidth}{!}{ 
%		\begin{small}
		\begin{tabular}{r | rrrrr}
			\hline\hline
			  \#Clusters examined & 512 & 256 & 128 & 64 & 32 \\
\hline
            Time (ms) & 33.9 & 16.2 & 6.2 & 3.8 & 2.8\\
                \hline\hline
		\end{tabular}
%		\end{small}
	}
	\caption{LSTM inference time with different input lengths.}
	\label{tab:LSTMclustertime}
 \vspace{-4mm} 
\end{table}

\comments{
\begin{figure}[htbp]

    \includegraphics[scale=0.27]{dist.pdf}
    \caption{Distribution of Good Documents }
    \label{fig:dist}
%\vspace{-1em} 
\end{figure}
}
}

To prioritize clusters, one idea is to adopt their query-centroid distance used
in IVF cluster selection.  We find this does not work well for CluSD.
For example, if we rank clusters based on the query-centroid distance for MS MARCO passages, 
around 10\% of top-10 dense results reside in clusters ranked beyond the 175th cluster, which means we would have to select 175 clusters in order to recover 90\% of top-10 documents. 
  
\comments{
Table~\ref{tab:dist} shows the distribution of top 10 documents from the full dense retrieval  
in embedding clusters under the different query percentile when searching an MS MARCO passage dataset using queries from its  Dev set. 
Row 2 of Table~\ref{tab:dist} shows the number of top clusters needed to cover  top 10 
documents from the full dense retrieval  when clusters are sorted by sorted by  the query-centroid distance.
This row shows that to cover top 10 dense results, $n$ needs  to be larger than 175.
%On the other hand, Row 2 is  the number of lusters decided by the LSTM to visit  under the different query percentiles. 
%At most  66 clusters are  selected  by the LSTM model for all queries.
%We further propose to skip unnecessary clusters. 
\begin{table}[h]
 \vspace*{-1mm}
	\centering
		\resizebox{0.78\columnwidth}{!}{ 
%		\begin{small}
		\begin{tabular}{r |r  r r r r}
			\hline\hline
			  Query percentile & 10\% & 50\% & 75\% & 90\% & 95\% \\
            \hline
%            \#clusters selected by  LSTM & 1 & 4 & 14 & 40 & 66\\
            %Relevant doc coverage & 1 & 2 & 7 & 29 & 69\\
             Top 10 dense result coverage & 1 & 6 & 25 & 92 & 175\\
                \hline\hline
		\end{tabular}
%		\end{small}
	}
	\caption{ Distribution of  top 10 dense results in sorted clusters 
% \% of Good Documents Among Clusters Sorted by Query-Centroid Distance
}
 \vspace*{-5mm}
	\label{tab:dist}
\end{table}
}


%provides an explanation.  evidence on the effectivness of the above LSTM model.
%After searching an MS MARCO passage dataset using queries from the MAMARCO dev set, 
%under the different query percentile shown in Row 2 is  the number of clusters decided by the LSTM to visit.
%Row 3 is  the number of clusters needed to 
%For a lot of queries, the top 5 or even top 10 clusters do not contain enough information, for example, around 10\% of queries need more than 92 top clusters to recover the top results from the full dense retrieval. It demonstrates the necessity of doing cluster selection and skipping.  
%On the other hand, there is a reasonable distribution resemblance between  Row 2 and  Row 3, which shows the LSTM model provides a reasonable guidance on where relevance results are.
%%While it does not exactly follow the distribution of top 10 results of sparse retrieval, the feature 
%From  Table~\ref{tab:dist}, we observe that
%the distribution of top 10 results from a sparse are contained is not necessarily concentrated in the top  clusters.
%With the above design consideration,  we propose to use the following strategy to 
%sort the candidate clusters based their feature vectors from Group  (3).


Our method   for the preliminary selection at this stage is to rely on
the degree of overlap between dense clusters and the top sparse retrieval results. 
Specifically, we divide $k$ top sparse retrieval results into $v$ top sparse result bins denoted $B_1, \cdots, B_v$,  
as depicted in Figure~\ref{fig:LSTMfeaturegroup}(b).
We  assign each embedding cluster $C_i$ with a priority vector $(P(C_i, B_1), \cdots, P(C_i, B_v))$ where
\[
P(C_i, B_j)=  |C_i  \cap  B_j|.
\]
Namely, $P(C_i, B_j)$ is the number of documents in top sparse bin $B_j$ included in cluster $C_i$.
A larger $P(C_i, B_j)$ value indicates a higher priority for $C_i$ with respect to sparse result position bin $B_j$.
In our evaluation with  retrieval depth $k = 1000$ for the MS MARCO and BEIR datasets,
we use $v=6$ result bins, divided into the top-10 results, top 11-25, top 26-50, top 51-100, top 101-200, top 201-500, and top 501 to $k$ documents.
%top $k$ sparse results are divided into $v$ result bins where $v=6$, and these bins are top 10, top 11 to 25,  and so on.
%top 26-50, top 51-100, top 101-200, top 201-top 500, and 
%top 501 to k documents.  
    %\item $\{  AvgScore (c_i  \cap  B_j)    \}^{6}_{j=1} $: 
% bin 
%top 10/25/50/100/200/500/1000 documents  in the sparse retrival results.

Then, we sort clusters based on the lexicographic ordering of cluster priority vectors $(P(C_i, B_1), \cdots, P(C_i, B_v))$.
%$(P_1, \cdots, P_v)$. A higher value for each dimension entry represents a higher priority and is prefered.
If there is a tie between two clusters, then we rank them by their centroid distance to the query.
%We first use a simple heuristic to reorder the clusters, refered as Top K heuristic. 
%We order the clusters based the six features then 
%followed by q-c-orders. 
%Then, the  top $n$ ranked candidate cluster list is used as the input sequence to the LSTM model. 
The top-$n$ ranked candidate clusters will be  the  input for the LSTM model described below.
%We will stop the LSTM computation of  this list
%after visiting $\tau$ clusters and when the predicted $f(c_i)$  values of a few consecutive clusters are all below a threshold. 
%For our evaluation in MS MARCO and BEIR datasets, $\tau=20$. 

\comments{
Section~\ref{sect:evalLSTM} studies the effectiveness of features used in Stage II, and explains our choice of features for preliminary cluster selection. 
For the choice of $n$,
our evaluation finds  that a relatively small number of candidate clusters, such as $n=32$, yields competitive results  for our
evaluations when $k=1000$.
%As a result, the LSTM computation for cluster selection can be completed within  1.8 milliseconds. 
%The number of clusters selected varies from a few to 20 on average, which yields a quick argumentation with dense cluster results.
%Features marked with Hexgos 1 and 4 are used for a boosting tree to make a decision if cluster selection should be triggered or not.

}
\subsection{Stage II: Selection with LSTM}
\label{sect:lstm}

%{\bf LSTM design.}

\begin{figure}[htbp]
    \includegraphics[scale=0.9]{dense_separse_feature_lout.drawio.pdf}
    \vspace*{-1mm}
    \caption{Illustration of CluSD and its features.}
    \label{fig:features}
%     \vspace*{-5mm}
\end{figure}


\comments{
The LSTM model outputs 1 for each cluster node if this cluster should be visited to perform the corresponding dense retrieval.
The goal of this procedure  is to visit  dense clusters as few as possible while still delivering a competitive fusion performance.
The number of predicted clusters are different across queries and the overall budget is controlled by the prediction threshold.
}
%  a few clusters as possible to recover the top results from the original dense retrieval. 
%The label of each query-cluster is 1 when the cluster contains the top k results of the query in the exact full search of dense retrieval.  
%The input is the a sequence of cluster nodes in the order of the centroid-query distance. 
%The feature of each input include: statistics on query-centroid distance, centroid-to-centroid distances, distribution of sparse top results across clusters. 
%The model will predict for each query which clusters to choose. 
%The input features of our proposed LSTM model are listed below.
%These clusters are sorted by the the similarity distance of the given query with the  centroid of each cluster.

To select which dense clusters we visit, we adopt a simple LSTM model~\cite{hochreiter1997long}
which takes a sequence of clusters as input along with their features. 
The time complexity of the LSTM model is limited by $O(n)$,
where the $n$ top clusters are  given from  Stage I.
%We denote $n$ as the  input length  to our LSTM  throughout the rest of this paper.

The LSTM sequentially visits each cluster, denoted $C_1$ to $C_{n}$. 
For each cluster $C_i$, the LSTM predicts a score $f(C_i)$ such that $0\leq f(C_i) \leq 1$.
If $f(C_i) \ge \Theta$, where $\Theta$ is a set prediction threshold, then cluster  $C_i$ should be visited. 
$\Theta$ controls the tradeoff between efficiency and relevance; in practice, $\Theta$ can be tuned based on the overall latency requirement.
The evaluation in Section~\ref{sect:threshold} demonstrates the impact of varying this threshold on the number of clusters selected.

%In practice, the strictness on selecting a cluster to evaluate is controlled by $\Theta$ and it can be determined based on the overall latency requirement. 

\comments{
When the dense vectors of all documents are grouped into $N$ clusters, 
we limit our cluster search scope to top $n$ clusters in terms of query and cluster centroid similarities. 
we limit $n=128$ in our evaluation out of 1000 clusters.
%Query-specific information derived from  dense clusters:
}

As depicted in Figure~\ref{fig:features}, the feature input vector for the current cluster  $C_i$ for the above LSTM computation to produce $f(C_i)$
%$f(q, c_i)$  
is composed of the following three groups of features:


\comments{
\[ sim(q,c_i),   
\{ AvgDist(sim(c_i, c_l) \mbox{ for } c_l \in A_j) \}^u_{j=1}, 
\]
\[ 
     \{  |c_i  \cap  B_j|    \}^{v}_{j=1},  
\{  AvgScore (c_i  \cap  B_j)    \}^{v}_{j=1}. 
\] 
}
%\begin{enumerate}[leftmargin=*]
%KDD style does not recognize leftmargin
\begin{itemize}
\item \textbf{Query-cluster similarity:} $sim(q,c_i)$.
The similarity distance of this query $q$ with the centroid $c_i$ of cluster $C_i$.  
%\item $sim(q,c_i)$: The similarity distance of this query $q$ with the centroid $c_i$ of cluster $C_i$.  
%\item  $\{ sim(c_i, c_j) \}^{n-1}_{j=0}$: The distance vector between the centroid $c_i$ of cluster $C_i$ to  all other clusters.
%the top  clusters that have been visited (or skipped).  We limit this number as $K=128$.
%other top k centroids for this query (k=128)??
  

%\item  $\{ AvgDist(sim(c_i, c_l)) \mbox{ for } c_l \in A_j \}^u_{j=1}$: 

\item \textbf{Inter-cluster similarity:} $AvgDist(C_i, Aj)$ for $1 \leq j \le u$.

%\item  $AvgDist(C_i, Aj)$ for $1 \leq j \le u$.
As depicted in   Figure~\ref{fig:LSTMfeaturegroup}(a),
given the $n$ sorted clusters derived from Stage I, we uniformly partition  these $n$ consecutive  clusters into $u$ consecutive cluster bins:
$\{ A_1, A_2, \cdots, A_u\}$. Then we define
\[
AvgDist(C_i, Aj) = \frac{1}{|A_j|} \sum_{ c_l \in A_j}
 sim(c_i, c_l) 
\]
where  $c_l$ is the centroid representing cluster $C_l$ in bin $A_j$.
The above formula computes the mean distance between centroid $c_i$ and the centroid of each cluster within  Bin $A_j$.


%These six bins  are formed by partitioning all clusters into consecutive position bins.
%For example, $A_1$ contains clusters from Postions  1 to 5, Positions 6-10, 
%$A_2$  contains clusters from Postions  6 to 10, and so on.   Bin $A_6$ contains the rest of uncovered clusters.  
%After all the $N$ centroids are ordered based on a criterion discussed shortly, we partition the sequence of sorted clusters into $u$ consecutive 
%cluster bins.

The purpose of this feature group is to capture the distances of a cluster to previously examined clusters and unexamined clusters 
following the LSTM flow. 
When the mean distance  between cluster $c_i$  and clusters in a previously-examined  bin is close, if  
many clusters in that bin have been selected, cluster 
$c_i$ may have a good  chance to  be selected.
In our evaluation with MS MARCO and BEIR datasets, we found that $u=6$ is appropriate.
% these 6 cluster bins respectively contain clusters in Positions  1-5, Positions 6-10, Positions 11-20, Positions 21-50, Positions 51-100, and the rest of N clusters. 
%Since the clusters are processed from a lower position to a higher position one by  one,
%these features represent the distance to the groups 
As discussed earlier in Section~\ref{sect:online}, to reduce the extra space cost, 
we only maintain top-$m$ cluster neighbors of each cluster where $m$ is not large  (128 in our evaluation).
    
%\item $ \{  |c_i  \cap  B_j|    \}^{v}_{j=1} $ and $\{  AvgScore (c_i  \cap  B_j)    \}^{v}_{j=1} $: 

\item \textbf{Cluster overlap:} $P(C_i, B_j)$ and $Q(C_i,   B_j)$     for $1 \leq j \le v$.
%\item $P(C_i, B_j)$ and $Q(C_i,   B_j)$     for $1 \leq j \le v$.

As shown in   Figure~\ref{fig:LSTMfeaturegroup}(b),
this group of features captures the position-weighted and score-weighted overlap degree of this embedding cluster $C_i$ with the top sparse results.
Stage I in Section~\ref{sect:stage1} divided the top-$k$ sparse retrieval results into $v$ position bins
$B_1, \cdots B_v$ and defined   $P (C_i, B_j)$ as  the count-based overlap measure.
Now, we define a score-weighted overlap degree $Q (C_i,   B_j)$     
as the average sparse rank score of documents that  are in both $C_i$ and sparse position bin $B_j$.  
Namely,
\[
Q (C_i,   B_j) = \frac{ \sum_{d \in C_i \cap B_j} SparseRankScore(d)} { |C_i  \cap  B_j|}.    
\]


%With a high score from this feature, a cluster that contains many top sparse results may be selected.
Clusters with high $Q$ scores for the top bins contain many top sparse results and may be selected.

%In our evaluation with  retrieval depth $k = 1000$ for MS MARCO and BEIR datasets,
%top $k$ sparse results are divided into $v$ bins where $v=6$, and 
%these bints are top 10, top 11 to 25, top 26-50, top 51-100, top 101-200, top 201-top 500, and 
%top 501 to k documents.  

    %\item $\{  AvgScore (c_i  \cap  B_j)    \}^{6}_{j=1} $: 

% bin 
%top 10/25/50/100/200/500/1000 documents  in the sparse retrival results.

\end{itemize}

In total, we extract $1+u+2v$ features for each top-$n$ cluster.



\begin{figure}[htbp]
    \includegraphics[scale=0.85]{LSTM\_feature.drawio.pdf}
    \vspace*{-3mm}
    \caption{Cluster distance   and result  overlap features } 
\vspace*{-1mm}
    \label{fig:LSTMfeaturegroup}
\end{figure}

%Section~\ref{sect:evaluation} provides more details on the effectiveness 
 

\comments{
\subsection{Query filtering}
%When computing the embedding of each query token or the whole  original query, we use 
%the denser retriever's encoder. 


We also exploit the possibility to skip dense retrieval completely.
Modifying the above LSTM-based decision mechanism does not yield a satisfactory result.
We sort to add a classifier with  boosted decision trees with supervised learning
to make an earlier  determination before triggering the LSTM computation.
}
\comments{

a query needs an argumentation from dense retrieval 
%A challenge is to identify features and infer based on such features with supervised learning.
Given that sparse retrieval is conducted first,
our idea is to predict the quality of sparse retrieval and its relative strength  compared to unexecuted
dense retrieval based on quality of query token expansion by the sparse model,
the statistical behavior of  sparse retrieval results, and
the relationship between dense document embedding summary and sparse retrieval scoring structure.

}


\comments{
\begin{table}[ht]       
    \centering          
%    \resizebox{1.05\columnwidth}{!}{

\begin{small}                        

    \begin{tabular}{l |l}
        \hline
    Query    & "how long nyquil kicks in"\\
    \hline
 Query Expanded&["in", 36],  ["time", 74], ["years", 71], ["long", 97], \\
 & ["total", 60],\textcolor{red}{["move", 23]}, ["effect", 22], \textcolor{red}{["weapon", 21]}, \\
 & \textcolor{red}{["jump", 24]}, ["kick", 100], ["ny", 111], ["timing", 34], \\
 & ["kicks", 76], ["\#\#quil", 139]\\
    \hline  
Relevance Doc &["\#\#quil", 410.1], ["ny", 301.0], ["long", 216.7], \\
& ["time", 134.7], ["years", 131.5], ["total", 87.4],\\
& ["effect", 34.4], ["timing", 17.4]\\
\hline
Top 5 Doc & ["\#\#quil", 392.6], ["ny", 295.7], ["kick", 236.1], \\
& ["long", 198.3], ["time", 141.2], ["years", 116.9], \\ 
& ["kicks", 101.8], ["total", 86.3],\\
& ["in", 35.3], ["effect", 24.1], ["timing", 23.6],\\
& \textcolor{red}{["jump", 10.0], ["move", 6.0]}\\
        \hline
    \end{tabular}
\end{small}                        
%}     

\caption{An example of weak query expansion in SPLADE} 
\label{tab:queryencoder}
\end{table} 
}

\comments{
\begin{table*}[ht]       
    \centering          
%    \resizebox{1.05\columnwidth}{!}{

\begin{small}                        

    \begin{tabular}{l  l l}
        \hline\hline
    Query   &  \multicolumn{2}{l}{ "why do we push others away"}\\
    \cline{2-3}
&  \multicolumn{2}{l}{('push', 105), ('away', 104), ('because', 97), ('others', 94), ('people', 44), ('why', 44), ('purpose', 43), ('cause', 42), (',', 40), ('we', 38),} \\
&\multicolumn{2}{l}{ ('other', 38), ('effect', 33), ('them', 29), \boldred{('responsibility', 28),} ('help', 20), \boldred{('move', 19), ('severe', 17),}  ('step', 17), ('do', 16),...}\\
%& & ('things', 15)， ('fear', 15), ('method', 10), ('to', 6), ('language', 5), ('anger', 5), ('when', 2), ('feeling', 2), ('\#\#ness', 1), ('dangerous', 1)]\\
    \hline  \hline
   Doc Relevant? & \ rank & Text and Expanded Tokens \\
\hline
\comments{
Yes & 17 & From a psychological standpoint, pushing away the people you love the most is a very basic and common, defense mechanism.\\
&& As the relationship develops, people become inundated with their own fears and insecurities that they will not be accepted\\
&& and therefore hurt by their loved one.\\
\cline{3-3}
& & [('push', 198), ('away', 196), ('fear', 127), ('people', 104), ('effect', 81), ('because', 70), ('severe', 67), (',', 61), ('method', 61),
\\
& &  ('when', 59), ('anger', 58), ('dangerous', 55), ('feeling', 53), ('purpose', 53), ('responsibility', 53), ('move', 44), ('cause', 43),... \\
%& & ('them', 37), ('things', 31), ('\#\#ness', 27), ('step', 23), ('why', 13), ('help', 9), ('to', 3), ('do', 1), ('language', 0)]\\
\specialrule{.1em}{.05em}{.05em} 
}
No & 10 & Different types of push factors can be seen further below. A push factor is a flaw or distress that drives a person away from \\
&&  a certain place. A pull factor is something concerning the country to which a person migrates. It is generally a benefit that  \\
&&attracts people to  a certain place. Push and pull factors are usually considered as north and south poles on a magnet.\\
\cline{3-3}
&& [('push', 283), ('away', 162), \boldred{('move', 80),} \boldred{('severe', 74)}, ('people', 70), ('because', 68), ('effect', 58), ('cause', 44), ('purpose', 43),\\
& & \boldred{('responsibility', 41),} (',', 36), ('method', 35), ('\#\#ness', 33), ('step', 23), ('them', 22), ('to', 18), ('when', 13), ...\\ 
%& &  ('things', 11), ('anger', 9), ('dangerous', 8), ('fear', 3), ('do', 0)]\\
\specialrule{.1em}{.05em}{.05em} 
No & 8& Most of the universe is rushing away from us. It\textbackslash{}'s not that we\textbackslash{}'re particularly repellent; it\textbackslash{}'s just that the universe is\\
&&  expanding, pushing most other galaxies away. Light from distant galaxies travels toward us through this \\
&& expanding space, which stretches their light to longer, or redder, wavelengths.\\
\cline{3-3}
&& [('away', 184), ('push', 158), ('other', 111), \boldred{('move', 103)}, ('we', 101), ('effect', 83), (',', 73), ('because', 66), ('method', 53),\\
&&  ('others', 39), ('cause', 39), ('severe', 39), ('them', 36), ('do', 29), ('things', 29), ('dangerous', 29), ('feeling', 28), ... \\
% & & ('when', 27), ('help', 19), ('purpose', 13), ('to', 4), ('why', 2)]\\
         \hline\hline
    \end{tabular}
\end{small}                        
%}     

\caption{An example of weak SPLADE query expansion that boosts  irrelevant documents  } 
\label{tab:queryencoder}
\end{table*} 

Our consideration 
%is that the necessity of invoking a dense retrieval model after running a sparse model depends on 
%\begin{enumerate}
%the relative strength of results scored by this sparse retriever. If a
    is that  if a chosen
sparse retriever has  well-performed, the benefit of addition of dense scoring  is limited. 
Notice that a learned neural sparse retriever expands the original query with a transformer-based encoder
We found that the quality of query encoding with expansion in SPLADE impacts the quality of sparse retrieval. 
%and we form features using both the original query  and the expanded query.
When  there is a low similarity value  between the original query and the tokens of 
its expanded  query, the confidence for  strong sparse retrieval can be low.
Table~\ref{tab:queryencoder} gives such an example and lists an original query and its expanded query tokens
sorted by token weights.  Expanded query words  in red color
not highly relevant to the original query and they are weighted high in two irrelevant documents, which boosts their ranking.  
Thus    excessive presence of un-similar  query tokens  after expansion  can misrank irrelevant documents. 
Another consideration is that the alignment of sparse retrieval ranking  with the 
query-cluster based similarity can shed light  on the necessity of  dense retrieval.
When top sparse retrieval results are concentrated in a few top  ordered dense clusters, dense retrieval may bear
a resemblance  in  ranking.


With the above consideration  in mind, we use the  following categories of features
to train a boosted decision tree model.

}
% with these featuers an input by using the xgboost package. 
\comments{
To evaluate the performance of the sparse system, we design some features to describe the quality of sparse encoder. 
For example, the length of expanded query, distribution of token impact scores and their corresponding matching scores with top k documents.

To evaluate the discrepancy between the performance of sparse and dense system. We encode the original query with the dense retrieval encode and each expanded token and generate distributional features around the similarity between them. 


%We take a supervised approach to the construction of a query filter that classifies  a query after sparse retrieval 
%needs an argumentation from  dense retrieval or not. We consider the following two categories of the feautres.
}
\comments{
\begin{itemize}[leftmargin=*]
\item Quality of query expansion by the sparse retriever model.
    For example, the query-level matching score, define as the similarity  between the CLS embeddings of the orginal query and the expanded query.
    Another one is the token-based matching histogram vector between the expanded query and the original query. 
For each query token in the expanded query, we evaluate the matching score in terms of similarity
 between its token embedding and  the oringal query embedding. 
We map  these values into five bins with  ranges <0.5, 0.5-0.6, 0.6-0.7, 0.7-0.8, >0.8.
Then derive a matching histgram count  vector for each bin.
}

\comments{
{\bf WILL DELETE THIS LIST.}
\begin{enumerate}
    \item The length of the original query and the expanded query in terms of  the  number of words or tokens, respectively. 
    \item The token-based matching  vector between the expanded query and the original query. 
For each query token in the expanded query, we evaluate the matching score in terms of similarity
 between its token embedding and  the original query embedding. 
We map  these values into five bins with  ranges <0.5, 0.5-0.6, 0.6-0.7, 0.7-0.8, >0.8.
Then derive a matching count vector based on this bin mapping???
    \item The query-level matching score, define as the similarity  between the CLS embeddings of the original query and the expanded query.
    \item The sum of matching scores within each of above token similarity bin, divided by
the  query-level matching score. 
%The token groups are the tokens with similarities in each bin.
    \item The sum of matching scores of tokens not in NLTK English corpus, divided by  the query-level matching score. 
    \item The number of overlapping tokens between expanded and original query / number of all unique queries in both expanded and original query.
    \item number of  tokens only in expanded query / number of all unique queries in both expanded and original query.
    \item overall/mean/max of token weight of overlapping tokens between expanded and original query in the expanded query.
    \item max/min/mean of token weights of expanded query tokens.
\end{enumerate}

}
%\item
\comments{
Ranking characteristics of documents scored by sparse retrieval.
We compute  the mean sparse retrieval score of top 10-20/20-30/30-40/40-50/50-100/100-400/400-700/700-1000 documents.

%Sparse  retrieval score distribution:
\begin{enumerate}
    \item Sparse retrieval scores of top 10 results
    \item The mean sparse retrieval score of top 10-20/20-30/30-40/40-50/50-100/100-400/400-700/700-1000 documents.
\end{enumerate}
%Sparse and Dense relational features:
\item Overlapping degree of sparse retrieval with dense clusters:
\begin{enumerate}
    \item The number of dense clusters containing top 5/10/20/50/100/1000 results from the sparse system
\end{enumerate}
}

%\end{itemize}


%\subsection{Partial Document Evaluation}
{\bf Training of LSTM.} 
We assume that the training data  contains
a set of queries and relevance-labeled documents,  that allows the computation of a ranking metric such as MRR or NDCG.
For example, from MS MARCO training set we randomly sample 5000 training instances, where each training instance contains a query and  one or more  relevant documents. 
%Then we run a sparse retriever this instance, and seperately run a fusion with a dense retriever. If the result of sparse retriever results merged with dense results  exceeds the standalone sparse retrival,  we label consider training instance as an positive example for dense retrieval, otherwise consider this as a negative example. The converted training isntances are used for training In this way, we convert a set of training instances for ranking as a set of training instances for fusion.
To train the LSTM for cluster selection, 
for each training instance, we mark the positive and negative examples for  top cluster selection.
If a cluster contains one of top-10 dense retrieval results, we mark this cluster as positive otherwise negative.
This approximately indicates that such an embedding cluster should be visited for a fusion.

 
\comments{
We convert We define the labels as 
\begin{equation}
       y(q)= 
\begin{cases}
    1,& \text{if } U(s(q), d(q)) > U(s(q))\\
    0,              & \text{otherwise,}
\end{cases}
\end{equation}
where $U(.)$ is a evaluation metric we choose, for example, NDCG@10 or MRR. $s(q)$ is the scoring of a sparse system of query $q$ and $d(q)$ represents the scoring of a dense system.
}

\comments{
\subsection{How to combine two ranked lists}
There are two commonly used methods to combine the sparse and dense ranked lists:
\begin{enumerate}
    \item linear combination
    \item reciprocal rank fusion
\end{enumerate}
For linear combination, the final ranking score of a document is the weighted sum of the ranking score of two systems respectively. The weight on each system can be determined through grid search on a left-out training set or just use 1:1 combination. In this case, rescaling of the two ranked lists is necessary as it largely affects the impact of a ranked list on the merged ranked list. 

On the other hand, reciprocal rank fusion alleviates the scoring distribution difference by only considering the ranking positions when merging. 
  Another consideration is the missing values. The two ranked lists usually do not fully overlap. Some documents only exist in one ranked list. Thus 
the treatment on these documents affects model performance. We consider a few options
\begin{enumerate}
    \item Remove the documents when they only exist in one ranked list
    \item  Take the document and compute the score for only one ranked list
    \item Impute the missing document with rank K+1 or score as 0.95 * min(score) in the ranked list.
\end{enumerate}

}


