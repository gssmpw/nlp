\subsection{Design options for LSTM}
\label{sect:evalLSTM}

% In this section, we discuss \textbf{RQ3} and \textbf{RQ4}.
Table~\ref{tab:cluster_feat} investigates the following five design options 
for cluster selection in CluSD. 
\begin{enumerate}[leftmargin=*]
    \item TopByDistance: Choose top clusters sorted by the distance of their  centroids to  the  query embedding.
% as implemented in the faiss IVF index.
    \item TopByOverlap:  Choose top  clusters sorted  by Group (3) features discussed in Section~\ref{sec:clusd}.
Namely use the overlap degree of clusters with top result bins of sparse retrieval.  

% select  the centroids based on the number of top10/25/100 sparse retrieval results in the clusters. This is the purning technique we propose in the method section.
    \item XGBoost: Use the boosting tree XGBoost model to detect if a cluster should be visited.
The features used are the same ones discussed in Section~\ref{sec:clusd}. This is a pointwise approach as we view query-cluster pairs as independent samples and the decision on one clusters doesn't affect other clusters for the same query.
% excluding inter-cluster distances.
%for each query-centroid pair to predict whether the centroid should be evaluted. This is a pointwise approach.
    \item RNN: Use a vanilla RNN model to predict. The feature and sequence setup is the same as our LSTM model. 
    \item Unpruned LSTM: Sort clusters  by the distance of their  centroids to  the  query embedding and feed into the LSTM model.
    \item Pruned LSTM: That is the final version used by CluSD with  cluster pruning.
\end{enumerate}
To have a fair comparison, this table assumes on average either 3 or 5 clusters are targeted by CluSD.
%We compare the performance of different strategies to select dense clusters.  
For XGBoost, RNN or LSTM, we set their prediction threshold so that 
the average number of clusters is close to the targeted number.
The last column of this table is the time spent by cluster selection.
%chosen depends  on the model prediction thresholds. 
%where top 3 or 5 clusters are selected. 
%In order to compare under same budget, we set the thresholds where on average around 3 or 5 clusters are used averaged over all queries. 
Table~\ref{tab:cluster_feat} shows that  the RNN or LSTM based prediction achieves much higher MRR@10 and recall compared to 
XGBoost. 
%point-wise tree based model. 
%It is reasonable as the tree model doesn't take  into account of incremental the inter-cluster relationships.
%Row  marked ``LSTM+prune'' of Table~\ref{tab:cluster_feat}, we 
Candidate pruning before using LSTM in CluSD can significantly 
decrease  the decision time by reducing the length of input cluster sequence, and can  actually lead a visible relevance increase in all cases.

 

\comments{
%\textbf{Effect of Model Options}
For k-means and topk heuristics, we report the performance where top 3 or 5 clusters are selected. For XGBoost, RNN or LSTM, the number of clusters chosen is dependent on the model prediction thresholds. In order to compare under same budget, we set the thresholds where on average around 3 or 5 clusters are used averaged over all queries. 
As we can see in Table~\ref{tab:cluster_feat}, RNN or LSTM based prediction achieves much higher MRR and Recall compared to point-wise tree based model. It is reasonable as the tree based model doesn't take into account the relationships between centroids when making predictions. 
}


%\textbf{Impact of feature groups}.  
The last two rows of Table~\ref{tab:cluster_feat} show
the {\bf positive impact  of feature groups}  used in the LSTM model.
That is done by 
removing  the inter-cluster distance feature group marked as ``w/o inter-cluster dist'' 
or by removing  the overlap degree of top sparse results and each cluster marked as  
`` w/o S-C overlap''.
The results indicate that the overlap degree is important for  cluster selection
while the inter-cluster feature group is also useful. 

%  vs. Dense'' has a visible drop in relevance, indicating that the features we use are useful. Especially the sparse vs. dense feature, model performance has a significant drop without the feature group.

\begin{table}
\resizebox{1.0\columnwidth}{!}{
  \begin{tabular}{ r r r r r r }
			\hline\hline
			 Avg \#clusters targeted  & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{5} & Time \\
			& MRR@10 & R@1k& MRR@10 & R@1k & ms\\
                   \hline
                 \multicolumn{6}{l}{\it{Model options}}\\
%                 Kmeans 
TopByDistance
& 0.297 & 0.655 & 0.331 & 0.740  & 0 \\
%                 TopK Heuristic
TopByOverlap
& 0.384 & 0.867 & 0.401 & 0.917 & 0.2 \\
                 XGBoost& 0.398 & 0.888 & 0.404 & 0.929 & 192 \\
                 RNN & 0.404 & 0.908 & 0.406 & 0.938 & 2.8 \\
                 %RNN+prune & 0.401 & 0.919 & 0.411 & 0.961 & 2.8 \\
                 Unpruned LSTM & 0.406 & 0.923 & 0.406 &0.943 & 2.8\\
            \textbf{Pruned LSTM}  &  0.408 & 0.943 & 0.410 & 0.953 & 2.8 \\
              \hline
             \multicolumn{5}{l}{\it{Feature group removal}}\\
             w/o inter-cluster  dist& 0.402 & 0.915 & 0.406 & 0.941 & --\\
             %w/o Centroid Distance& 0.389 & 0.898 & 0.410 & 0.959 & --\\
             %w/o Sparse vs. Dense& 0.378 & 0.866 & 0.404 & 0.943 & --\\
             w/o  S-C overlap& 0.289 & 0.638 & 0.325 & 0.730 & --\\
             %$-$ Query vs. Centroid & 0.405 & 0.925 & 0.406 & 0.941 & --\\
              \hline\hline
		\end{tabular}
}
	\caption{Design options for  CluSD and the impact of  features }
	 \vspace*{-5mm}
        \label{tab:cluster_feat}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.4]{ncluster2.pdf}
    \caption{CluSD performance and latency with respect to a different average  number of visited clusters}
     \vspace*{-4mm}
    \label{fig:ncluster}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.45]{threshold.pdf}
    \caption{Left: Distribution  of \#visited clusters under different selection threshold $\Theta$.
Right: Mean \#visited clusters on MSMARCO Dev set and BEIR datasets.}
     %\vspace*{-5mm}
    \label{fig:thresholds}
\end{figure}

\subsection{Impact of  cluster partitioning size}
\label{sect:clusterno}

%The cluster selection technique is built on top of a group of defined clusters. It is important to understand how the cluster selection works and a good rule of thumb to 
%determine the model prediction threshold given different number of clusters. 
CluSD assumes the embeddings are  pre-partitioned into $N$ clusters.  Figure~\ref{fig:ncluster} shows the impact of CluSD when
$N$ is  4096 and  8192.
% we plot three scenarios where the total number of clusters is 1024, 4096 and 8192 respectively. 
We use $n=128$ in this figure to depict the impact of selecting different numbers of clusters on average in a wider range.
A  solid line refers to the use of uncompressed dense embeddings while  a dash-dot line refers to the quantization with OPQ M=128, 
and a dotted line refers to the quantization  with OPQ m=64. X-axis is the average number of clusters selected by CluSD when $n=128$.
We vary the selection threshold parameter $\Theta$ in CluSD to achieve the different average number of clusters selected. 

Figure~\ref{fig:ncluster} shows that with both $N$,
%MRR@10 reaches above 0.420 when above    $<$ 10 clusters 
MRR@10 reaches above 0.420 when above 10 clusters 
are selected.
As more clusters are selected,  the recall ratio does  increase. 
%Note that when total number of clusters increases, the percentage of documents search will be different given a fixed number of clusters are selected. 
%Given a fixed number of selected clusters,  the latency impact is different with different $n$ values.
%This result in difference in average latency. 
Looking at  a fixed average number of selected clusters, 
different $N$ values imply different latencies.
For example, when 10 clusters are selected on average, 
CluSD  scans through 0.1\% and  0.2\% of embeddings corresponding to N=4096 and 8192, respectively. 
%As we can see in the bottom two sub-figures, 
When $N= 8192$, its latency in milliseconds is smaller  with a  smaller size per cluster
as the CluSD time complexity is proportional  to  the size per cluster and the number of clusters selected.
With $N$ becomes much bigger,  the sorting overhead of clusters in pruning optimization starts to offset or outweigh latency benefits
of small clusters. 
% while the overall CluSD time is still  
\comments{
In this case, given a 20 
%(35 ms for 99\%) 
millisecond mean latency constraint, 
CluSD  with flat index can achieve 0.425 and 0.988 in MRR@10 and recall@10 by visiting  about 50 clusters (0.6\% of documents), 
which is the same performance compared to the full index search. 
This is observation is logical, as for each cluster, there are possible only tens or a few hundred relatively important documents to be evaluated for 
a given query. Majority of the documents in the clusters are not worth exploring. As the size of each clusters decreases, we are able to see more top important documents. 
}

Lastly, as CluSD makes a query-specific decision to dynamically select  clusters, each query has  a  different number of clusters visited
given a fixed threshold value and query latency can vary. 
The bottom right subfigure shows that the 99th percentile latency is around 2x longer than the mean latency, which is reasonable by
looking at the previous retrieval latency studies (e.g. ~\cite{mallia2019pisa, mackenzie2021anytime}). 




\subsection{Impact of  selection threshold $\Theta$}
\label{sect:threshold}
\comments{
\textbf{RQ6}: does the cluster selection model dynamically select different number of clusters for different queries? How the selection differs for different datasets?
For the first question, we plot out the distribution of number of clusters selected for each query as a boxplot in 
}
The left portion of Figure~\ref{fig:thresholds} shows
the average number of SimLM dense clusters selected by CluSD for MS MARCO Dev set under a different value of threshold $\Theta$ when
the input sequence length $n$ is 32.
%Model prediction threshold refers to the threshold which we use to determine the selection of a cluster. 
The threshold can be determined based on the time budget. 
%As we can see, there is a variety in the number of clusters selected. 
While the median number of clusters selected is  10 clusters, 75th percentile is around 18 clusters. 

The right portion of Figure~\ref{fig:thresholds} 
shows  the mean number of clusters selected for MS MARCO Dev set and averaged over 
the zero-shot BEIR datasets.
This subfigure shows that for a fixed $\Theta$ value, CluSD  chooses a fewer number of clusters for a
BEIR dataset compared to the MS MARCO Dev set.  
%This aligns with our expectation that  
This is because
SimLM underperforms SPLADE on the BEIR datasets in general. Thus, it is unnecessary to evaluate a lot of clusters. 
CluSD adaptively selects fewer clusters to conduct selective dense retrieval.

