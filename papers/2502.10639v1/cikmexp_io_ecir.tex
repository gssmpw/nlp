%\subsection{ CluSD vs. Graph-based  and re-ranking baselines}
%\label{sect:evaldisk}

{\bf  CluSD vs. baselines with in-memory or on-disk data}.
%{\bf CDFS vs. re-ranking and nearest-neighbor search baselines with in-memory or on-disk data.}
Table~\ref{tab:disktime} examines the performance  of CluSD fused with SPLADE when MS MARCO passage 
embeddings cannot fit in memory and are stored on an SSD disk.
We use  $N=65,000$ for CluSD, and the extra space for quantized inter-cluster distances takes about 40MB. 
This setting here is larger than the $N=8192$ setting in Table~\ref{tab:compare}  and we choose that
because a smaller   cluster size gives a flexibility in  reducing the disk I/O size when 
the total number of clusters selected by CluSD is controlled.
We also compare other selective dense retrieval baselines using  reranking or proximity graph navigation.
%The mean response time (MRT) and 99 percentile time (P99) include the I/O cost while their cost breakdown for I/O and CPU time is also listed. 
The mean response time (MRT) and 99\textsuperscript{th} percentile (P99) include
the total sparse and dense retrieval time for methods noted ``S+*'', otherwise only the dense retrieval time.
The MRT cost breakdown for I/O and CPU time is also listed. 
Times reported are in milliseconds. 
``\%D'' is the approximate  percentage of document embeddings evaluated based on the number of clusters fetched from the disk.
%Column ``\#Doc'' is the number of embeddings fetched for performing full or partial dense retrieval.
%MRR stands for MRR@10.
%R@1K stands for Recall@1K.
%A small table on the top specifies the data location of configured retrieval algorithms.

Reranking simply fetches top-$k$ embeddings  from the disk for fusion. 
LADR is designed for in-memory search~\cite{2023SIGIR-LADR} and we run it by  assuming  memory can sufficiently host the proximity  graph while letting LADR access  embeddings from the disk. 
Excluding the I/O cost portion of LADR in this table, the in-memory search cost of LADR is marked in the ``CPU'' column.
We test two configurations of LADR, its default with 128 neighbors, 200 seed documents, and a search depth of 50,
as well as a faster configuration, selected such that its CPU time is similar to CluSD. 
LADR\textsubscript{fast} uses 128 neighbors, 50 seed documents, and a depth of 50.
DiskANN~\cite{NEURIPS2019_DiskANN} assumes the graph and original embeddings are on disk  while the memory  hosts
compressed embeddings for quick guidance.  SPANN~\cite{chen2021spann} searches disk data in a cluster-based manner based on query-centroid distances.
Both DiskANN and SPANN  are designed for on-disk search from scratch without fusing with others, and thus we simply fuse their outcome with SPLADE results.
HNSW~\cite{2020TPAMI-HNSW} is not included because its in-memory relevance  is similar as DiskANN while it is not designed to be on disk. 

\comments{
As the size of dense embeddings or the number of documents in a dataset increases, it is likely that dense embeddings are too large to be fully stored in memory. For instance, RepLLaMA requires 135.6GB for the 8.8M documents in MS MARCO; BERT-based dense embeddings for a 1B document dataset would require 2.8 TB. This problem is exacerbated when a large navigation graph is also required to be loaded in memory. We compare CluSD with several baselines when the embeddings are stored on disk and must be loaded to memory before ranking. We memory-map the dense embeddings and load them on-demand during our algorithm. We measure both the disk access time (I/O) and the associated computation required by each method (Overhead). Our results are shown in Table \ref{tab:disktime}. 
% TODO (above): Latency figures given in the table only represent the dense portion of retrieval. 
% TODO (above): We compare each method with RepLLaMA and SimLM embeddings for WHY DO WE DO THIS?
}


\comments{
We find that the disk access time required by CluSD is significantly less than the time required by LADR or a simple rerank. We suggest that SSD random seeks have significant overhead compared to consecutively reading. Thus, reading many documents consecutively from a cluster is quick, and the number of random seeks is limited to the number of clusters accessed. During other methods, it is likely that the number of random seeks is equal to the number of documents accessed. A simple least-squares model for our RepLLaMA results suggests that each random seek takes 167 us, whereas the time to read one document is only 11.8 us.
}


\comments{
We find that the disk access time required by CluSD is significantly less than the time required by LADR or a simple rerank. We suggest that SSD random seeks have significant overhead compared to consecutively reading. Thus, reading many documents consecutively from a cluster is quick, and the number of random seeks are limited to the number of clusters accessed. During other methods, it is likely that the number of random seeks is equal to the number of documents accessed. A simple least-squares model for our RepLLaMA results suggests that each random seek takes 167 us, whereas the time to read one document is only 11.8 us.
}

% CluSD is with XXXX configuration and XXXX average clusters per query. Reranking reranks the top-1000 documents from bm25 + docT5query. LADR_S are results from a "small" version of adaptive LADR, with k=128, n=8, c=200. LADR_L are results from a "large" version of proactive LADR, with k=128, n=200 that retrieves approximately ten times more documents than LADR_S






%Added for KDD 2024

\comments{
\begin{table}[htbp]
        \centering
 \resizebox{1.0\columnwidth}{!}{
                \begin{tabular}{r |lll|ll|rr}
                        \hline\hline
                        & \multicolumn{2}{c}{\bf{Relevance}}& \bf{\#Docs} &  \multicolumn{4}{c}{\bf{Dense Latency \& Breakdown (ms)}}\\
            & MRR@10 & R@1K & fetched&  MRT & P99 & I/O & CPU  \\
        \hline
        Rerank& 0.421 & 0.980 & 1000.0 & 127.5 & 186.1 & 125.2 & 2.3\\
        LADR\textsubscript{fast} & 0.419 & 0.980 & 2431.0 & 314.9 & 1013 & 1003 & 10.3\\ % ladr n=50, c=50, k=128
        LADR\textsubscript{default} & 0.422 & 0.984 & 8521.6 & -- & -- & -- & --\\ % ladr seed=200, depth=50, k=128
        % ABOVE: NEW NUMBERS WITH REAL UNCACHED DATA
        % BELOW: OLD NUMBERS WITH NoClust DATA
        %Rerank& 0.421 & 0.980 & 1000.0 &  18.8& 95.70 & 454.9  &16.9& 1.9\\
        %LADR\textsubscript{fast} & 0.419 & 0.980 & 2431.0 & 42.3 & 191.5 & 1106 &31.3 & 11.0\\ % ladr n=50, c=50, k=128
        %LADR\textsubscript{default} & 0.422 & 0.984 & 8521.6 &  119.3 & 349.8 & 3877 & 91.4 & 27.9\\ % ladr seed=200, depth=50, k=128
        % Above: LADR with time budget of CPU time ~= CluSD CPU time ~= 6-7ms
        % ========================
      %  D-ANN-Wentai & 0.398 & 0.970 & 271.4 & 5.24 & -- & 276.6 & 321.2\\
   %     SPANN-Wentai & 0.405 & 0.965 & -- & -- & -- & 123.6 & 159.6 \\
         %S+ DiskANN& 0.417 & 0.984 & -- &  276.5 & 321 & -- &272.3& 5.2\\ 
         S+ DiskANN& 0.417 & 0.984 & -- &  276.5 & 321 &272& 5.2\\ 
        DiskANN & 0.398 & 0.970 & --    & 276.5 & 321 & 272 & 5.2\\
        S+SPANN& 0.420 & 0.989 & -- &  123.6 & 159.6  &-- & -- \\
        SPANN& 0.396 & 0.965 & --    & 123.6 & 159.6  & -- & --\\
%Did Yingrui adds fusion and adds this on Wentai's result?  Yes  S+DiskANN & 0.417 & 0.984 & -- & -- & -- & 276.5 & 312.2\\ov
       % S+SPANN* & 0.424 & 0.985 & 6.54 & 6.20 & 3484.9 & +12.74 & 84.73\\
% Above: Parker selected clusters from top-16 SPLADE results -> thus relevance is too strong
        %CluSD& 0.425 & 0.986  & 6.19 & 6.48 & 4423.51 & 12.67 & 42.77\\
    %CluSD& 0.425 & 0.986  & 4423.5&  17.7& 47.8 & 178.1 &6.2& 11.0\\
    % ========================
    S+CluSD& 0.425 & 0.986  & 4423.5& 24.4 & 63.2 & 17.5 & 6.9\\
    % ABOVE: NEW NUMBERS WITH REAL UNCACHED DATA
    % BELOW: OLD NUMBERS WITH NoClust DATA
    %S+CluSD& 0.425 & 0.986  & 4423.5&  17.7& 47.8 & -- &6.2& 11.0\\
                \hline\hline
                \end{tabular}  }
        \caption{Search when uncompressed embeddings and/or the proximity graph are on disk for SPLADE + SimLM}
% Embedding space = 25.9 GB. Here we only report the latency of the dense retrieval part.
%\vspace*{-5mm}
        \label{tab:disktime}
\end{table}

From Parker
LADR default without SPLADE
MRT = 643.7 ms
P99 = 2818 ms
CPU = 30.1 ms
IO = 2788 ms
}
 %\vspace*{-5mm}
 \comments{
% BELOW: new table that improves formatting
\begin{table}[htbp]
        \caption{Search when uncompressed embeddings and/or the proximity graph are on disk for SPLADE + SimLM}
        \centering
 \resizebox{0.8\columnwidth}{!}{
                \begin{tabular}{r |ll|ll|rrr}
                        \hline\hline
                        & \multicolumn{2}{c}{\bf{Relevance}} &  \multicolumn{2}{c}{\bf{Latency (ms)}} & \multicolumn{3}{c}{\bf{Breakdown (ms)}}\\
S=SPLADE-HT1            & MRR & R@1K &  MRT & P99 & I/O  & CPU  & \# Docs  \\
        \hline
        %Rerank& 0.421 & 0.980 & 127.5 & 186.1 & 125.2 & 2.3 & 1000.0\\
        S+Rerank& 0.421 & 0.980$^\dag$ & 158.7 & 217.3 & 125.2 & 33.5 & 1000.0\\
       % LADR\textsubscript{fast} & 0.419 & 0.980 & 314.9 & 1013 & 1003 & 10.3 & 2431.0\\ % ladr n=50, c=50, k=128
        S+LADR\textsubscript{fast} & 0.419 & 0.980$^\dag$ & 346.1 & 1013 & 1003 & 41.5 & 2431.0\\ % ladr n=50, c=50, k=128
        %LADR\textsubscript{default} & 0.422 & 0.984 & 643.7 & 2818 & 2788 & 30.1 & 8521.6 \\ % ladr seed=200, depth=50, k=128
        S+LADR\textsubscript{default} & 0.422 & 0.984 & 674.9 & 2849 & 2788 & 61.3 & 8521.6 \\ % ladr seed=200, depth=50, k=128
        DiskANN & 0.398$^\dag$ & 0.970$^\dag$ & 276.5 & 321 & 272 & 5.2 & --    \\
        S+ DiskANN& 0.417$^\dag$ & 0.984 &  307.7 & 352.2 &272& 36.4 & -- \\ 
        SPANN& 0.396$^\dag$ & 0.965$^\dag$ & 123.6 & 159.6  & -- & -- & --    \\
        S+SPANN& 0.420 & 0.989 &  154.8 & 190.8  &-- & -- & -- \\
        %old without sparse time S+CluSD& 0.425 & 0.986 & 24.4 & 63.2 & 17.5 & 6.9 & 4423.5\\
        S+CluSD& 0.425 & 0.986 & 55.6 & 94.4 & 17.5 & 38.1 & 4423.5\\
                \hline\hline
                \end{tabular}  }

% Embedding space = 25.9 GB. Here we report dense retrieval
        \label{tab:disktime}
\end{table}
 \vspace*{-6mm}
 }
% changing table to use %D instead of #Doc. Also matching column formatting with Table 1
 \begin{table}[htbp]
 
        %\caption{Search when uncompressed embeddings and/or the proximity graph are on disk for SPLADE + SimLM}
        \caption{Search when uncompressed embeddings are on disk for SPLADE + SimLM}
        \centering
 \resizebox{0.8\columnwidth}{!}{
                \begin{tabular}{r|c|ll|rr|rr}
                        \hline\hline
                        & & \multicolumn{2}{c}{\bf{Relevance}} &  \multicolumn{2}{c}{\bf{Latency (ms)}} & \multicolumn{2}{c}{\bf{Breakdown}}\\
S=SPLADE-HT1            & \%D & MRR@10 & R@1K &  MRT & P99 & I/O  & CPU  \\
        \hline
        %Rerank& 0.421 & 0.980 & 127.5 & 186.1 & 125.2 & 2.3 & 1000.0\\
        S+Rerank & 0.01 & 0.421 & 0.980$^\dag$ & 158.7 & 217.3 & 125.2 & 33.5 \\
       % LADR\textsubscript{fast} & 0.419 & 0.980 & 314.9 & 1013 & 1003 & 10.3 & 2431.0\\ % ladr n=50, c=50, k=128
        S+LADR\textsubscript{fast} & 0.03 & 0.419 & 0.980$^\dag$ & 346.1 & 1013 & 1003 & 41.5 \\ % ladr n=50, c=50, k=128
        %LADR\textsubscript{default} & 0.422 & 0.984 & 643.7 & 2818 & 2788 & 30.1 & 8521.6 \\ % ladr seed=200, depth=50, k=128
        S+LADR\textsubscript{default} & 0.10 & 0.422 & 0.984 & 674.9 & 2849 & 2788 & 61.3 \\ % ladr seed=200, depth=50, k=128
        DiskANN & -- & 0.398$^\dag$ & 0.970$^\dag$ & 276.5 & 321 & 272 & 5.2    \\
        S+ DiskANN & -- & 0.417$^\dag$ & 0.984 &  307.7 & 352.2 &272& 36.4  \\ 
        SPANN & -- & 0.396$^\dag$ & 0.965$^\dag$ & 123.6 & 159.6  & -- & --    \\
        S+SPANN & -- & 0.420 & 0.989 &  154.8 & 190.8  &-- & -- \\
        %old without sparse time S+CluSD& 0.425 & 0.986 & 24.4 & 63.2 & 17.5 & 6.9 & 4423.5\\
       $\blacktriangle$ S+CluSD & 0.05 & 0.425 & 0.986 & 55.6 & 94.4 & 17.5 & 38.1 \\
                \hline\hline
                \end{tabular}  }

% Embedding space = 25.9 GB. Here we report dense retrieval
        \label{tab:disktime}
           \vspace*{-5mm}
\end{table}
 %\vspace*{-6mm}
 
CluSD significantly outperforms all other methods by leveraging block I/O.
CluSD is 2.2x faster than the next fastest system (SPANN), while having a noticeably higher relevance.
DiskANN performs similarly to SPANN in terms of relevance, but is 4.97x slower than CluSD.
Reranking has a lower MRR@10 and recall@1k while being 2.85x slower.
Both variants of LADR have a lower MRR@10 than CluSD, but are 6.2x and 12.1x slower on average for LADR\textsubscript{fast} and LADR\textsubscript{default} respectively.
We find that  each  I/O operation has about a 0.15ms queueing and other software overhead on our tested PCIe SSD.
%We find that  each  I/O operation has about a 0.3ms queueing and other software overhead on our tested PCIe SSD.
Thus, more fine-grained operations in reranking yields more overhead.
Proximity graph methods such as LADR rely on large amounts of fine-grained I/O operations and suffer significantly in terms of latency when embeddings are stored on disk.
In comparison, CluSD takes advantage of block I/O operations
because it fetches documents by clusters  and thus CluSD issues fewer I/O requests to search more documents in less time.

%Reranking is similar to CluSD in terms of  mean latency while having a 2x slower 99\textsuperscript{th} percentile latency, lower MRR@10 and  
%recall@1K. % in this case.
%The CPU-time of reranking is 
%short but its total time is 
%dominated by 1,000 random I/O operations for fetching top embeddings. 
%and reaches about  715MB/second I/O speed. 

%Our setting stores embeddings in clusters grouped by their similarity on disk, thus
%random I/O operations in reranking  benefits from spatial locality in clustered data, because top results tend to be similar. 

%and reranking is 25.7x slower in the mean latency. 
%We find that  each  I/O operation has about a 0.3ms queueing and other software overhead on our tested PCIe SSD.
%Thus, more fine-grained operations in reranking yields more overhead.
%In comparison, CluSD issues  much fewer  I/O operations to fetch 4,423 embeddings per query on average in a block I/O manner.
%Its 99\textsuperscript{th} percentile latency is 2x faster than re-ranking.


                                     
%CluSD outperforms LADR with higher MRR@10 and up-to 6.9x faster  for the two tested configurations.
% taking much less and  99\textsuperscript{th} percentile latency.   
%The above I/O spatial locality  explanation for reranking is also applicable to LADR.  
%CluSD outperforms  DiskANN and SPANN in relevance and is about 15.6x and 7x faster in mean latency, respectively. 
% which is designed for more general nearest neighbor search. 
%CluSD is comparable to SPANN in relevance  but is 6.98x faster in the mean latency.

% while CDFS, which fetches clusters of consecutive embeddings, utilizes block I/O that incurs less access overhead. 

