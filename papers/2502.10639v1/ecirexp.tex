\section{Evaluations}

\comments{
Our evaluation answers  the following research questions:
% and the corresponding subsections are marked:
\textbf{RQ1} (Sect.~\ref{sect:eval0space}): 
How does CluSD perform when searching in-memory datasets?
The baselines to compare include  IVF-based selective dense retrieval, 
graph navigation based selective retrieval, and reranking.
We also provide details on searching  out-of-domain text content.
%extra space overhead is minimized by augmentation under different compression configurations? 
%\textbf{RQ2} (Section~\ref{sect:evaltimebudget}): 
%  under a retrieval latency time budget when there is no space restriction?
%\textbf{RQ3} (Section~\ref{sect:evalbeir}): 
%Zero-shot retrieval performance is important since a large dataset can contain documents from many domains. 
\textbf{RQ2} (Sect.~\ref{sect:evaldisk}): 
How does  CluSD perform on-disk search when there is not enough memory to host  the dataset?
\textbf{RQ3} (Sect.~\ref{sect:evalsparse}): 
Does CluSD work well with  different sparse retrievers?
\textbf{RQ4} (Sect.~\ref{sect:evalcompress}): 
Does CluSD work  well using different quantization methods?
\textbf{RQ5} (Sect.~\ref{sect:evalLSTM}): 
Is  LSTM 
%with cluster pruning 
preferred compared to other design options?
Are  features designed for LSTM useful?
%\textbf{RQ6} (Section~\ref{sect:clusterno}): 
\textbf{RQ6} (Sect.~\ref{sect:clusterno}): 
How does the number of clusters used affect the relevance and latency of CluSD? 
%\textbf{RQ7} (Sect.~\ref{sect:threshold}):
How does the choice of selection threshold in LSTM affect the average number of clusters selected?
%Does the cluster selection model dynamically select different number of clusters for different queries? How the selection differs for different datasets?
}


\comments{
To answer \textbf{RQ1},  we compare our proposed approach on the in domain test set, MSMARCO passage dev, DL 19, 20 with other commonly used techniques under storage (Table~\ref{tab:mainspace}) and latency constraints (Tabel~\ref{tab:maintime}).  
For \textbf{RQ2}, as our cluster selection technique requires training, we would like to understand how the approach would work on our domain dataset. In Table~\ref{tab:beir}, we use our cluster selection model trained on MS MARCO datasets on the 13 datasets from BEIR on a zero-shot manner and observe promising performance on all 13 out-domain datasets. 
For \textbf{RQ3} and \textbf{RQ4}, We compare the different cluster selection techniques, including heuristic approaches, a point-wise prediction model xgboost and a vanilla RNN prediction model against the proposed LSTM model (Table~\ref{tab:cluster_feat}).
To answer \textbf{RQ5}, we take two scenarios where the number of clusters is 4096, and 8192. Then we present the relevance in MRR@10, recall@1000, and the latency in terms of mean and 99 percentile time under three settings: when the flat index is used without compression, when we use OPQ to compress with m=128 or 64. Results are shown in Figure~\ref{fig:ncluster}.
For \textbf{RQ6}, we first plot the distribution of the number of clusters selected for each query under different model prediction thresholds. We also plot the average number of clusters for MSMARCO dev set and BEIR in Figure~\ref{fig:thresholds}.
For \textbf{RQ7}, we report the performance using three weaker sparse retrievers in Table~\ref{tab:sparse}. 
}

%\subsection{Settings }
{\bf Datasets and measures}.
Our evaluation uses the MS MARCO dataset with 8.8 million passages~\cite{Campos2016MSMARCO,Craswell2020OverviewOT} 
for training and in-domain search,
%The test query sets are the Dev set with 6980 queries and TREC deep learning (DL)  2019/2020 tracks with 43 and 54 queries.
%Following the common practice, we report mean reciprocal rank at 10 (MRR@10) and recall at 1000 (recall@1K)
%which is the percentage of relevant-labeled results appeared in the final top-1000 results.
%For DL19 and DL20 sets, we report NDCG@10.
%report average NDCG@10 for 
and the  13 publicly available BEIR datasets~\cite{thakur2021beir} 
for zero-shot retrieval. 
%BEIR is a heterogeneous benchmark containing a variety of IR tasks, 
%including search tasks (e.g.  question-answering, bio-medical IR, and entity retrieval)  
%and semantic relatedness tasks (e.g.  duplicate question retrieval). 
The size of BEIR data sets ranges from 3,633 to 5.4M documents.
%Their average query length ranges from 5.39 to 192.98, 
%and  their average document length ranges from 11.44 to 634.79. 
%Each dataset has binary or three-level relevance labels.
%The above metrics follow the common practice in the information retrieval literature.
To report the mean latency and 99 percentile latency, 
we test queries multiple times using a single thread on  a 2.25GHz AMD EPYC 7742  CPU server with PCIe SSD.
%The reason to use a single thread is that each CPU server in a busy search site handles many queries in parallel, and thus we only take
%one core resource with one thread from this server and leave other cores  for other queries.
%That is the standard practice in many of the previous efficiency studies in the IR literature.

{\bf Models and parameters}.
%For sparse retrieval, we adopt two variations of 
For sparse retrieval, we use
SPLADE~\cite{Formal2021SPLADE,Formal_etal_SIGIR2022_splade++,Lassance2022SPLADE-efficient} 
with hybrid thresholding~\cite{2023SIGIR-Qiao} and  BM25-guided traversal~\cite{mallia2022faster,qiao2023optimizing}:
% and one BM25 based model with different relevance and efficiency  tradeoffs:
1) SPLADE-HT1 with index space 3.9GB and 31.2 ms retrieval latency.
%2) SPLADE-HT1-fast is a variation of SPLADE-HT1 with a different parameter setting, which has 3.9GB index space and 22.4 ms latency. 
%2) SPLADE-effi-HT3 with  3GB space and  12.4ms latency. 
We also test on three other sparse models:
% uniCOIL~\cite{Lin2021unicoil,2021NAACL-Gao-COIL}, and LexMAE~\cite{shen2023lexmae}.
%2) DeepImpact~\cite{Mallia2021deepimpact};
2) uniCOIL~\cite{ Lin2021unicoil,2021NAACL-Gao-COIL} with 1.2GB index and 35.5ms latency;
3) LexMAE~\cite{shen2023lexmae} with 3.7GB index and 180ms latency;
4) BM25-T5 that applies  BM25 with  DocT5Query document expansion~\cite{Cheriton2019doct5query}.  
BM25-T5 has a 1.2GB index and 9.2ms latency. 
%Their index space varies from 1.2GB to  3.7GB.
% with 1.2GB index space.
% with latency 9.2ms.
%5) BM25 after T5-based document expansion~\cite{Cheriton2019doct5query}. 
%They represent the state-of-the-art sparse retrievers.
% with the best relevance performance evaluated on MS MARCO.
For dense retrieval, we adopt the recent dense models
SimLM~\cite{Wang2022SimLM}, RetroMAE~\cite{Liu2022RetroMAE}, and
RepLLaMA~\cite{ma2023finetuning}. 
%We use their checkpoints from the Huggingface library to generate document and query embeddings 
%and  
We use  K-means in the FAISS library~\cite{johnson2019billion} to derive dense embedding clusters.
RetroMAE-2~\cite{liu-etal-2023-retromae} is not used because its checkpoint is not released.


Our evaluation implementation uses C++ and Python.
% and the related code will be released to the public after publication of this paper. 
\comments{
%Note that SimLM model's embeddings are normalized while RetroMAE's embeddings are not. 
For sparse retrieval, we use two fast versions of SPLADE and a SPLADE-efficient model, as well as BM25-T5. 
The implementation of SPLADE models follows 2GTI optimization~\cite{qiao2023optimizing}. %its official release~\cite{spladeRepo}. 
SPLADE-HT1 is the SPLADE model trained with hybrid thresholding scheme~\cite{2023SIGIR-Qiao} using $\lambda_{t}=1$, 
SPLADE-effi-HT3
is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} trained with hybrid thresholding scheme using $\lambda_{t}=3$. Both of these two models use the "accurate" configuration in 2GTI during retrieval, while SPLADE-HT1-fast uses the "fast" configuration.
The sparse retrieval code for Splade and BM25-T5 uses the code from PISA~\cite{mallia2019pisa}
with some optimization~\cite{Lassance2022SPLADE-efficient,2023SIGIR-Qiao}.
}
To train the CluSD model, we sample 5000 queries from the MS MARCO training set 
%and use the LSTM unit in Pyytorch. 
and the hidden dimension for LSTM is 32. The models are trained for 150 epochs. 
For sparse and dense model interpolation, we use min-max normalization to rescale the top results per query. 
%Then we use linear combination with equal weights to combine two ranked lists. 
For  BM25-T5, the interpolation  weights are  0.05 and 0.95 for sparse  and  dense scores, respectively. 
For other sparse retrieval models, they are  0.5 and  0.5.  
The results are marked with tag $^\dag$ when 
statistically significant drop is observed compared to the baseline marked with $\blacktriangle$ at 95\% confidence level. 
%In terms of missing document in either ranked list, as the smallest value from each normalized list is 0, there is no missing imputation needed as we 
%can view the missing  documents taking the value of the worst document in the ranked list automatically. 

%For CluSD, the default setting of number of clusters is $N=8192$, and the input sequence length to the Stage II LSTM model is $n=32$.  

{\bf Time budget.}
%For search latency, our goal is to minimize the relevance effectiveness loss to a small degree while still accomplishing  a fast response time
%acceptable to the interactive search system requirement.
We mainly use a time budget of about 50ms including the sparse retrieval time for the average latency,
and the use of this budget allows us to choose the algorithm parameters accordingly.
% and finding for 25ms time budget is similar.
We choose this budget  because
%We explain the reason as follows.
in a search system running as an interactive web service,  a query with multi-stage processing
needs to be completed within a few hundred milliseconds.
%Considering the multi-stage search nature  and other service overhead involved,
A first-stage retriever operation  for a data partition needs to be completed within several tens of milliseconds without query caching.
%Our targeted time budget to conduct dense retrieval cannot be too high     compared to  sparse retrieval cost.
%For example, if sparse retrieval can complete  in 30 ms on an  average CPU and the total latency
%target is around 50ms,  the cost of dense retrieval is targeted under around  20ms.
 \vspace*{-6mm}
\comments{
{\bf Baselines.}
We compare CluSD with two groups of baselines under space and time constraints respectively.
The first group compared in Section~\ref{sect:eval0space} 
is to fuse  sparse retrieval with cluster-based selective dense retrieval.
That includes  dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
The second group compared in Section~\ref{sect:evaltimebudget} 
is to fuse  sparse retrieval with reranking and graph-based selective dense retrieval such as HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR}. 
This group involves random access of embeddings and/or graph nodes, which  can incur high I/O cost when data is not on disk.


{\bf Statistical significance.}
For MS MARCO Dev set, the result of a method is marked with  tag $^\dag$ when 
statistically significant drop is observed compared to  a baseline at 95\% confidence level.  In Table~\ref{tab:mainspace}, \ref{tab:maintime} and \ref{tab:sparse}, this baseline 
is sparse retrieval fused with full dense retrieval marked as ``S+D'', which typically delivers the highest relevance  
using all uncompressed or quantized dense embeddings. In Table~\ref{tab:disktime}, \ref{tab:RepLLaMA} and \ref{tab:compress}, the baseline is our proposed method ``S+CluSD''. 


}
\comments{
{\bf Baselines and notations}
%We select two groups of baselines under space and time constraints respectively.
Table~\ref{tab:mainspace} compares  baselines  with memory space constraint. 
We include dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
OPQ quantization  is configured with  the number of codebooks as $m=$ 128 or 64.  
When IVF clustering is used,  we report its performance  using top 10\%, 5\%, 2\% clusters respectively
as a standard approach which performs   limited top cluster retrieval. 
CluSD execution is also based on the the same OPQ or  IVFOPQ index from  FAISS but clusters are  selected  by our LSTM model. 
%Switching a quantization different  from OPQ  is might result in an improvement in relevance, but it is orthogonal to our proposal. 
We use notation ``S+D'' to report the interpolated performance of a sparse model and a dense model in Table~\ref{tab:mainspace} and Table~\ref{tab:sparse}.

Table~\ref{tab:maintime} compares  baselines  without  memory space constraint. 
For proximity graph-based dense retrieval which incurs additional graph space overhead,
we include  HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR} as shown in Table~\ref{tab:maintime}. 
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint. 
%For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as 50 to meet the latency requirements in 
%Table~\ref{tab:maintime}.
%Uncompressed dense embeddings (marked as "flat") are used
%to exclude the impact due to quantization.
We also compare  a simple reranking method,  denoted as ``D-rerank'' or ``rrk'', 
that fetches the dense scores of top 1000 of sparse retrieval to rerank with interpolation. 
%We use the same clustering setting for IVF and CluSD. 

%The reported CluSD performance on the flat index uses the exact same IVF index. 
The configuration we selected to meet the latency requirement for CluSD uses on average 20-25 (0.25 - 0.3\%) clusters per query. 
%For re-ranking, we report the results of re-ranking top 1000 results from the sparse model using the dense model.  

}

