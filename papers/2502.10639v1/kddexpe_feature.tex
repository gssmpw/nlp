\subsection{Design options for CluSD}
\label{sect:evalLSTM}

% In this section, we discuss \textbf{RQ3} and \textbf{RQ4}.
Table~\ref{tab:cluster_feat} investigates the following design options 
for cluster selection in CluSD. 
For Stage I selection, we explore two options:
\begin{enumerate}
%[leftmargin=*]
    \item SortByDist: Choose top clusters sorted by the distance of their  centroids to  the  query embedding.
% as implemented in the faiss IVF index.
    \item SortByOverlap:  Choose top  clusters sorted  by $P(C_i, B_j)$ features discussed in Section~\ref{sec:clusd}.
Namely use the overlap degree of clusters with top result bins of sparse retrieval. 
\end{enumerate}
For Stage II selection, we explore three options:
% select  the centroids based on the number of top10/25/100 sparse retrieval results in the clusters. This is the purning technique we propose in the method section.
 \begin{enumerate}
    \item XGBoost: Use the boosting tree XGBoost model to detect if a cluster should be visited.
The features used are the same ones discussed in Section~\ref{sec:clusd}. This is a pointwise approach as we view query-cluster pairs as independent samples and the decision on one cluster doesn't affect other clusters for the same query.
% excluding inter-cluster distances.
%for each query-centroid pair to predict whether the centroid should be evaluted. This is a pointwise approach.
    \item RNN: Use a vanilla RNN model to predict. The feature and sequence setup is the same as our LSTM model. 
    \item LSTM: the model we used in CluSD.
\end{enumerate}
    We also assess  the impact  of removing a  feature group  used CluSD:
\begin{enumerate}
\item w/o inter-cluster dist: removing  the inter-cluster distance feature group;
\item w/o S-C overlap: removing  the overlap-degree feature of top sparse results with each cluster.
\end{enumerate}


The last row marked ``Default'' is the final version used by CluSD with SortByOverlap and LSTM.
To have a fair comparison, this table assumes on average either 3 or 5 clusters are targeted by CluSD.
%We compare the performance of different strategies to select dense clusters.  
For XGBoost, RNN or LSTM, we set their prediction threshold so that 
the average number of clusters is close to the targeted number.
The last column of this table is the time spent to select clusters. 
%chosen depends  on the model prediction thresholds. 
%where top 3 or 5 clusters are selected. 
%In order to compare under same budget, we set the thresholds where on average around 3 or 5 clusters are used averaged over all queries. 
Table~\ref{tab:cluster_feat} shows that  the RNN or LSTM based prediction achieves  higher MRR@10 and recall compared to 
XGBoost. 
%point-wise tree based model. 
%It is reasonable as the tree model doesn't take  into account of incremental the inter-cluster relationships.
%Row  marked ``LSTM+prune'' of Table~\ref{tab:cluster_feat}, we 
%SortbyOverlap in Stage I in CluSD can significantly  decrease  the decision time by reducing the length of input cluster sequence, and can  actually lead a visible relevance increase.
Feature exploration indicates that the overlap degree is important for  cluster selection
while the inter-cluster distance feature group is also useful. 


 

\comments{
%\textbf{Effect of Model Options}
For k-means and topk heuristics, we report the performance where top 3 or 5 clusters are selected. For XGBoost, RNN or LSTM, the number of clusters chosen is dependent on the model prediction thresholds. In order to compare under the same budget, we set the thresholds where on average around 3 or 5 clusters are used averaged over all queries. 
As we can see in Table~\ref{tab:cluster_feat}, RNN or LSTM based prediction achieves much higher MRR and Recall compared to point-wise tree based model. It is reasonable as the tree based model doesn't take into account the relationships between centroids when making predictions. 
}


%\textbf{Impact of feature groups}.  

%  vs. Dense'' has a visible drop in relevance, indicating that the features we use are useful. Especially the sparse vs. dense feature, model performance has a significant drop without the feature group.

\begin{table}
\resizebox{1.0\columnwidth}{!}{
  \begin{tabular}{ r |r r| r r| r }
			\hline\hline
			 Avg \#clusters targeted  & \multicolumn{2}{c}{3} & \multicolumn{2}{c}{5} & Time \\
			& MRR@10 & R@1k& MRR@10 & R@1k & ms\\
                   \hline
                 %RNN+prune & 0.401 & 0.919 & 0.411 & 0.961 & 2.8 \\
                 \multicolumn{6}{l}{\it{Stage II is removed. Stages I only options}}\\
                 SortByDist & 0.297 & 0.655 & 0.331 & 0.740  & 0 \\
                $\blacktriangle$ SortByOverlap & 0.384 & 0.867 & 0.401 & 0.917 & 0.2 \\
                   \hline
                \multicolumn{6}{l}{\it{Stage I=SortByDist;  Stage II model options}}\\
                 XGBoost& 0.398 & 0.888 & 0.404 & 0.929 & 192 \\
                 RNN & 0.404 & 0.908 & 0.406 & 0.938 & 2.8 \\
                 LSTM & 0.406 & 0.923 & 0.406 &0.943 & 2.8\\
                   \hline
             \multicolumn{5}{l}{\it{ Stage II=SortByOverlap;  Stage II LSTM feature group options}}\\
             w/o inter-cluster  dist& 0.402 & 0.915 & 0.406 & 0.941 & --\\
             %w/o Centroid Distance& 0.389 & 0.898 & 0.410 & 0.959 & --\\
             %w/o Sparse vs. Dense& 0.378 & 0.866 & 0.404 & 0.943 & --\\
             w/o  S-C overlap& 0.289 & 0.638 & 0.325 & 0.730 & --\\
             \hline 
             $\blacktriangle$  Default   &  0.408 & 0.943 & 0.410 & 0.953 & 2.8 \\
             %$-$ Query vs. Centroid & 0.405 & 0.925 & 0.406 & 0.941 & --\\
              \hline\hline
		\end{tabular}
}
	\caption{Design options for  CluSD. $\blacktriangle$ is the default choice.}
	 \vspace*{-5mm}
        \label{tab:cluster_feat}
\end{table}

\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.42]{ncluster2.pdf}
    \caption{CluSD performance and latency with respect to a different average  number of selected clusters}
    % \vspace*{-4mm}
    \label{fig:ncluster}
\end{figure}


\begin{figure}[h!]
    \centering
    \includegraphics[scale=0.48]{threshold.pdf}
    \caption{Left: Distribution  of \#visited clusters under different selection threshold $\Theta$.
Right: Mean \#visited clusters on MSMARCO Dev set and BEIR datasets.}
     \vspace*{-5mm}
    \label{fig:thresholds}
\end{figure}

\subsection{Impact of  cluster partitioning size}
\label{sect:clusterno}

%The cluster selection technique is built on top of a group of defined clusters. It is important to understand how the cluster selection works and a good rule of thumb to 
%determine the model prediction threshold given different number of clusters. 
CluSD assumes the embeddings are pre-partitioned into $N$ clusters.  Figure~\ref{fig:ncluster} shows the impact of CluSD when
$N$ is  4096 and  8192.
% we plot three scenarios where the total number of clusters is 1024, 4096 and 8192 respectively. 
For stage I, we use $n=128$ in this figure to depict the impact of selecting different numbers of clusters on average in a wider range.
A  solid line refers to the use of uncompressed dense embeddings while  a dash-dot line refers to the quantization with OPQ m=128, 
and a dotted line refers to the quantization  with OPQ m=64. X-axis is the average number of clusters selected by CluSD.
We vary the selection threshold parameter $\Theta$ in CluSD to achieve the different average number of clusters selected. 

Figure~\ref{fig:ncluster} shows that with both $N$,
%MRR@10 reaches above 0.420 when above    $<$ 10 clusters 
MRR@10 reaches above 0.420 when above 10 clusters 
are selected.
As more clusters are selected,  the recall ratio does  increase. 
%Note that when total number of clusters increases, the percentage of documents search will be different given a fixed number of clusters are selected. 
%Given a fixed number of selected clusters,  the latency impact is different with different $n$ values.
%This result in difference in average latency. 
Looking at  a fixed average number of selected clusters, 
different $N$ values imply different latency numbers.
For example, when 10 clusters are selected on average, 
CluSD  scans through 0.1\% and  0.2\% of embeddings corresponding to N=4096 and 8192, respectively. 
%As we can see in the bottom two sub-figures, 
When $N= 8192$, its latency in milliseconds is smaller  with a  smaller size per cluster
as the CluSD time complexity is proportional  to  the size per cluster and the number of clusters selected.
%With $N$ becomes much bigger,  the overhead of clusters in stage II optimization starts to offset or outweigh latency benefits of small clusters. 
% while the overall CluSD time is still  
\comments{
In this case, given a 20 
%(35 ms for 99\%) 
millisecond mean latency constraint, 
CluSD  with flat index can achieve 0.425 and 0.988 in MRR@10 and recall@10 by visiting  about 50 clusters (0.6\% of documents), 
which is the same performance compared to the full index search. 
This observation is logical, as for each cluster, there are possibly only tens or a few hundred relatively important documents to be evaluated for 
a given query. Majority of the documents in the clusters are not worth exploring. As the size of each clusters decreases, we are able to see more top important documents. 
}

Lastly, as CluSD makes a query-specific decision to dynamically select  clusters, each query has  a  different number of clusters visited
given a fixed threshold value and query latency can vary. 
The bottom right subfigure shows that the 99\textsuperscript{th}
percentile latency is around 2x longer than the mean latency, which is reasonable by
looking at the previous retrieval latency studies (e.g. ~\cite{mallia2019pisa, mackenzie2021anytime}). 




\subsection{Impact of  selection threshold $\Theta$}
\label{sect:threshold}
\comments{
\textbf{RQ6}: does the cluster selection model dynamically select different numbers of clusters for different queries? How the selection differs for different datasets?
For the first question, we plot out the distribution of number of clusters selected for each query as a boxplot in 
}
The left portion of Figure~\ref{fig:thresholds} shows
the average number of SimLM dense clusters selected by CluSD for MS MARCO Dev set under different threshold $\Theta$ values  when
the input sequence length $n$ for Stage II LSTM is 32.
%Model prediction threshold refers to the threshold which we use to determine the selection of a cluster. 
A threshold can be chosen for CluSD to follow a  time budget. 
%As we can see, there is a variety in the number of clusters selected. 
%While the median number of clusters selected is  10 clusters, the 75th percentile is around 18 clusters. 
In our experiments,  the default setting of
CluSD is to use threshold 0.02  for MS MARCO passages and the average number of clusters selected by CluSD using this threshold is 22.3.

The right portion of Figure~\ref{fig:thresholds} 
shows  the mean number of clusters selected for MS MARCO Dev set and averaged over 
the zero-shot BEIR datasets.
This subfigure shows that for a fixed $\Theta$ value, CluSD  adaptively chooses a fewer number of clusters for a
BEIR dataset compared to the MS MARCO Dev set.  
%This aligns with our expectation that  
This is intuitive as
SimLM underperforms SPLADE on the BEIR datasets in general. Thus, it is unnecessary to evaluate a lot of clusters. 
%CluSD adaptively selects fewer clusters for BEIR% to conduct selective dense retrieval. 

