%\section{Limitations and Future Work}
%\vspace*{-2mm}
\section{Concluding Remarks}

We have demonstrated that CluSD, a lightweight cluster-based partial dense retrieval method,
can deliver fast response time and competitive relevance on affordable computing platforms,
without the need of an expensive document-wise proximity graph.
%The main contribution of this work is  to  show  how 
%a lightweight cluster-based partial dense retrieval 
%with competitive  relevance can be made possible
%to deliver fast response time on affordable computing platforms, without the need of an expensive  document-wise  proximity graph.
%CluSD carries with minimal space overhead compared to the orginal dense retrieval
%and incurrs a small amount of CPU latency time due to the limited dense computation.
\comments{
CluSD uses  an LSTM-based  model and  leverages sparse retrieval results to effectively identify  a small number of 
clusters that can supplement and boost the relevance of partial dense  retrieval.  
With this lightweight low-cost design,  
CluSD is suitable for a CPU-only low-cost server while delivering  a fairly strong  in-domain and out-of-domain relevance.  
For example, CluSD delivers 0.423 MRR@10  for searching 8.8M MS MARCO passages 
with 11.4ms  latency and  1.13GB space mainly for compressed dense embeddings, to augment sparse retrieval under
a fraction of its  cost. The same  CluSD version  reaches  0.514 average NDCG@10 for BEIR with compression.
That represents state-of-the-art relevance performance for a retriever that  runs on a CPU-only server. 
}
Guided by sparse retrieval, CluSD significantly outperforms previous cluster-based partial dense retrieval with IVF 
search in relevance under the same time budget.
For example, CluSD delivers 0.431 MRR@10  for searching 8.8M MS MARCO passages with SimLM and LexMAE 
on its Dev test set, while taking only tens of milliseconds on a low-end CPU server,
%and reaches  0.516 average NDCG@10 for BEIR.  
and reaches an average NDCG@10 of 0.541 for BEIR with RepLLaMA and SPLADE.  
The MRR@10 achieved is close to that of RankT5 (0.434)~\cite{2023SIGIR-ZhuangRankT5},
which is an expensive cross-encoder.
Thus, CluSD can deliver a state-of-the-art relevance performance for a retriever that runs efficiently on 
a CPU-only server. 
%The othe recent efficient retrievers reported 0.426  for MS MARCO Dev, and 0.48 for BEIR by LexMAE.
%Its MRR@10 number is close to one reported recently using an expensive cross-encoder RankT5 which reaches 0.434~\cite{2023SIGIR-ZhuangRankT5}. 
%ClusD's BEIR performance is close to  that  of  Promptagator++~\cite{dai2023promptagator}.  Both  RankT5 and Promptagtor++h employ an expensive cross-encoder.
%Noted that user search experience is highly impacted by the ranking order and its relevance thus a relatively small percentage point increase  
%in metrics for the standard public test benchmarks used in this paper is viewed to be hard and valuable in the IR research literature.

%For example,
%Table ~\ref{tab:mainspace} shows CluSD is as fast as IVFOPQ top 2\% search at m=64 and 2x fast at m=128 while its MRR@10  is 11.5\% and 9.5\% higher 
%that only use query-centroid distances to select clusters. 

Under the same time budget, CluSD delivers similar or higher relevance than methods that rely upon
a document-level proximity graph, such as LADR and HNSW, with negligible extra space overhead.
Further, CluSD is up-to 12.1x faster by avoiding fine-grained document-level disk I/O when embeddings do not fit in memory.
When compared to general on-disk ANN search methods DiskANN and SPANN~\cite{NEURIPS2019_DiskANN,2023Web-Filtered-DiskANN},
CluSD is 2.2x and 4.97x faster respectively, while achieving better relevance
% for text document search 
by leveraging sparse retrieval.
Thus, compared to graph navigation approaches, CluSD is lightweight, with both low space overhead and low I/O cost.
%Compared to LADR and HNSW, our evaluation demonstrates that without a document-level proximity graph, CluSD can 
%deliver a similar or higher 
%relevance under the same time budget
%with negligible extra space overhead when embeddings are available in memory  and  up-to 6.9x   faster by avoiding fine-grained document-level disk I/O
%when embeddings do not fit in  memory.  
%Thus the lightweight with low-space overhead  and low-I/O cost is the main advantage of CluSD compared to a graph navigation approach.
\comments{
For example, Table ~\ref{tab:maintime} shows
CluSD has 2\% higher MRR@10  than LADR and 1.2\% higher than HNSW under a time budget of 25ms.
%It is attractive to run such an algorithm on a first-stage retrieval low-cost machine
%that can significantly reduce the overall infrastructure cost when scaling  data size and search traffic. 
}
%Compared to several on-disk nearest-search efforts  with DiskANN and SPANN~\cite{NEURIPS2019_DiskANN,2023Web-Filtered-DiskANN},
%like HNSW, they are targeted for a general ANN problem while our
%work takes advantage of sparse retrieval with 15.6x and 7x faster  latency and better relevance in the context of text document search.


CluSD is also stronger and more versatile than reranking. 
Simple reranking does surprisingly well if sparse results are strong. But as stated in~\cite{2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR},
when the sparse retriever doesn’t achieve high relevance, reranking is too 
restrictive and recall@1K does not increase as shown in Table 5. 
\comments{
For example, after 
sparse retrieval with SPLADE-effi-HT3 with 0.944 recall@1K, reranking does not increase it while CluSD increases it to 0.984.
While reranking restricts candidate documents to the top sparse results, CluSD adds more diversified results from selected dense clusters. 
}
CluSD’s advantage becomes more significant in zero-shot retrieval as shown in Table 6 with 13 BEIR datasets. CluSD’s average NDCG number is 
7.2\% higher than reranking with RetroMAE. With SimLM, CluSD is 4\% higher than reranking.
In addition, when embeddings are not available in memory, reranking is 2.85x slower in mean latency.

%becomes very slow with a large number of document-level random I/O operations. 

CluSD works well  with LLaMA-2 based RepLLaMA dense  model for the tested datasets.
With more dense models  developed in the future using large language models of high dimensionality for better relevance,
our study  sheds a light on how to take advantages of such models on low-cost CPU servers, 
especially when their embeddings cannot fit into memory and are hosted on disk.

Our evaluation has not used ColBERT with a multi-vector representation because 
ColBERTv2's MRR@10~\cite{Santhanam2021ColBERTv2}  for MS MARCO is 0.397, which is lower than that the single-vector dense models studied in this paper. 
CluSD techniques are  orthogonal to ColBERTv2 and could be used with ColBERTv2 after small modifications.

\comments{
Our evaluation uses OPQ in FAISS. 
Switching to a different quantization affects the compression ratio and  the relevance outcome. 
However, we expect that our findings remain similar and  the recent  quantization 
methods~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}
also have  a significant relevance drop pattern when the compression ratio increases.
%A limitation of the current  CluSD work is that the training of the LSTM model has a dependence on the sparse retrieval model used. 
%While  CluSD can  boost the relevance of a weaker sparse retriever model such as BM25, the augmented relevance still has not reached a level
%comparable  to the one where CluSD is coupled with a stronger sparse retriever. There is room for improvement in future work.

}

%the dependency on a training dataset. Unlike rule-based or unsupervised pipelines, our pipeline requires labeled data on at least one search task. However, the number of queries with labels is not large and a trained model is robust on never-seen corpus and tasks. 
%
%Another limitation is the dependency on the sparse retrieval model performance. CluSD benefit from a good sparse retrieval top results. When using a BM25 based retrieval, the quality of the clusters selected degrades slightly. 
