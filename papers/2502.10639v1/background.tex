%\section{Related Work}
%\label{sect:background}

{\bf Sparse and dense retrieval}. 
%A traditional method for first-stage retrieval uses 
%BM25 based additive ranking with a bag-of-words sparse document representation~\cite{Robertson2009BM25}
%and fast retrieval can be designed  using the inverted index data structure to exploit this
%sparse representation.
To improve traditional BM25 lexical sparse retrieval, 
%Meanwhile, an advancement in sparse retrieval with inverted indices is to 
%adopt learned sparse representations with neural term weights, which has
%yielded in  a significant relevance gain. 
%severl studies  have developed  
learned sparse representations with neural term weights have been studied in 
DeepCT~\cite{Dai2020deepct},
DeepImpact~\cite{Mallia2021deepimpact}, uniCOIL~\cite{Lin2021unicoil,2021NAACL-Gao-COIL},
SPLADE~\cite{Formal2021SPLADE, Formal2021SPLADEV2}. 
Pre-training optimization of a language model for SPLADE-like retrieval 
is studied in LexMAE~\cite{shen2023lexmae}. 
%following the pretraining optimization in SimLM~\cite{Wang2022SimLM}
and RetroMAE~\cite{Liu2022RetroMAE}.
\comments{
The  evaluation of our work for the first-stage retrieval
centers on the SPLADE family of sparse retrieval~\cite{Formal2021SPLADE, Formal2021SPLADEV2} 
because it can deliver a strong relevance with maintaining low space and time costs during inference.
For instance, SPLADE++~\cite{Formal2022SPLADE++} 
delivers 0.38 MRR@10 for the MS MARCO passage Dev set and it is possible to improve this score 
to 0.394 as shown in Section~\ref{sect:evaluation}.  
}
We use SPLADE in our evaluation because it has been well studied by  
a number of papers  with various efficiency optimization options as discussed below
and LexMAE has not made its checkpoints available for evaluation. 
Besides, using a sparse model like LexMAE with better language model pretraining is orthogonal and complementary to our study.
% not sure if the last sentence is needed

Initially there was a concern
that online traversal  of  a learned  inverted index is significantly slower than a BM25 based index,
the recent efficiency studies such as guided traversal and index pruning with sparsification 
show that the  latency of learned sparse retrieval  can be 
reduced dramatically~\cite{Lassance2022SPLADE-efficient,mallia2022faster,qiao2023optimizing,2023SIGIR-SPLADE-pruning}.
%In~\cite{Qiao20222GT}, the model retrieves top 1000 documents in 43ms with 0.394 MRR@10, 25ms with 0.388 MRR@10, or 
%15ms with 0.379 MRR@10, under different index sizes respectively.

% should be approximate search or approximate nearest neighbor search
\comments{
This performance score is very close to 0.395 produced by
AR2 dense retrieval with approximate search which requires 8 NVIDIA Tesla A100 GPUs.
AR2 dense retrieval with approximate search using faiss 
IndexFlatIP~\footnote{https://faiss.ai/cpp_api/struct/structfaiss_1_1IndexFlatIP.html} 
which uses a flat index with exhaustive search.  
%Qiao20222GT.
%takes about a few hundred milliseconds without GPA support and can be further reduced to  up-to 80ms with a small relevance 
%tradeoff~\cite{Lassance2022SPLADE-efficient,AskYifan}.
Meanwhile, avoiding exhaustive exact search in  dense retrieval with approximate search comes with a 
tradeoff of  a relevance drop~\cite{2021Facebook-DrBoost-Lewis, johnson2019billion, Xiao2022Distill-VQ}.
For instance, %AR2 dense retrieval with exact search yields an MRR@10 of 0.395 while 
with Distill-VQ~\cite{Xiao2022Distill-VQ} , a recent 
joint IVF and PQ based ANN search framework, AR2 has an MRR@10 of 0.361 as opposed to 0.395.
%, which still needs GPU support
%because a dense retriever has a significant relerence loss when using faster but 

Notice that for a large-scale search system containing billions of documents, 
the first-stage fast retrieval is typically executed in parallel across many index partitions hosted on
a large number of  machines. In such cases, efficient retrieval with inexpensive hardware could be preferred. 

}

Dense retrieval is an alternative approach for the first-stage search with a dual encoder architecture
%For example, TCT-ColBERT~\cite{Lin2021tctcolbert} and  JPQ~\cite{2021CIKM-JPQ-Zhan}
(e.g. ~\cite{xiong2021-ANCE,gao-2021-condenser}).
Since the representation  for a document and a query is a dense vector,
%Efficiency studies for dense retrieval have been conducted on
fast dense retrieval for a large dataset requires
approximate nearest neighbor (ANN) search~\cite{johnson2019billion} 
%exhaustive exact search~\cite{2021Facebook-DrBoost-Lewis}  
%and ~\cite{Xiao2022Distill-VQ}, 
or  a more expensive GPU server.
%and quantization~\citep{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ},
%applying dense retrieval on  a large dataset requires requires an approximate nereast neighbor search a more expensive GPU 
%, which is more suitable for a GPU-based platform for fast processing.
There are numerous studies recently to enhance  dense retrieval with knowledge distillation
and  improved  language model pretraining~\cite{Lin2021tctcolbert, Santhanam2021ColBERTv2,  Ren2021RocketQAv2,
Wang2022SimLM,  Liu2022RetroMAE}.

\comments{
Retrieval that uses a dense representation of documents and a dual-encoder (or also called bi-encoder) architecture 
outperforms BM25 significantly in relevance at the cost of relatively more expensive similarity computation
%For example, TCT-ColBERT~\cite{Lin2021tctcolbert} and  JPQ~\cite{2021CIKM-JPQ-Zhan}
(e.g. ~\cite{Lin2021tctcolbert, 2021CIKM-JPQ-Zhan, xiong2021-ANCE,gao-2021-condenser,2021SIGIR-Zhan-ADORE-dense,Ren2021RocketQAv2}). 
Efficiency studies for dense retrieval have been conducted on
approximate nearest neighbor (ANN) search~\cite{johnson2019billion} to avoid
exhaustive exact search~\cite{2021Facebook-DrBoost-Lewis}  and  quantization~\citep{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}, 
%Meanwhile, avoiding exhaustive exact search in  dense retrieval with approximate search 
and these optimizations come with a tradeoff of  a relevance drop.
}

{\bf Combining  dense and sparse retrieval.}
%The earlier IR studies use the term fusion to combine rankers, e.g.  the reciprocal rank fusion (RRF)~\cite{Cormack2009RRF}.
The neural IR studies  have  considered combining the traditional lexical ranking signals and neural semantic matching signals~\cite{Mitra2017LearningTM,Shao2019PrivacyawareDR,Gao2020ComplementingLR,Luan2020SparseDA,Yang2021WSDM-BECR}. 
It is found that combining  sparse and dense retrieval scores  
can further boost recall and/or other relevance metrics~\cite{Lin2021unicoil,2022LinearInterpolationJimLin, kuzi2020leveraging}, which
suggest  that both categories  of retrievers tend to capture  different relevant signals. However, these studies do not address slow  latency  of
dense retrieval on a CPU-only server.
% and do not consider impact of data compression  in dense retrieval. 
%The focus of this paper follows ~\cite{Lin2021unicoil,2022LinearInterpolationJimLin,kuzi2020leveraging} to 
%to combine the ranking results of two retrievers with a linear interpolation,
%
\comments{
We adopt 
a simple fusion wit linear combination ~\cite{Lin2021unicoil,2022LinearInterpolationJimLin,kuzi2020leveraging}, 
which allows the proposed techniques to  easily adopt the latest dense or sparse retriever models.
Our query filtering work is motivated by
the dense and sparse retriever selection study~\cite{2021CIKM-PredictDenseSparse}
when to use BM25 retriever or a  dense retriever.
The difference is our work always assumes sparse retrieval is conducted first, and our selection focuses on how to choose part of dense clusters.
Another difference is that we exploit the characteristics of a learned lexical representation in query encoding and expansion in making a query-level decision.
% filtering exploits the SPLADE encoding as SPLADE is our primary targeted sparse retrieval, even 
%our work is applicable to other sparse retrivers such as BM25. 
}

{\bf Re-ranking.} 
There  are many studies that exploit advanced re-ranking in a multi-stage search pipeline with modern
neural IR techniques(e.g. ~\cite{MacAvaney2019SIGIR-cedr, MacAvaney2020SIGIR-prettr, 2023SIGIR-ZhuangRankT5,Yang2022CQ,Zhuang2021TILDEv2,2022WWWforwardIndex}),
possibly leveraging more expense GPU platforms to conduct complex scoring.
These  re-ranking pipelines perform very well but are inherently limited by the recall of the first-stage retrieval
as pointed out in  ~\cite{2022CIKM-MacAvaneyGraphReRank, 2023SIGIR-LADR}. 
A  focus of this work is to enhance the relevance of retrieval with CPU-friendly low-cost optimization by expanding the candidate pool of  
fast sparse retrieval using selective  dense retrieval at a modest amount of extra CPU cost and without incurring significant  space cost.  

{\bf Cluster based retrieval and proximity graph navigation based on inter-document similarity.}
%with lexical matching
There has been a large body of studies on cluster-based document retrieval  in traditional IR
throughout the last few decades (e.g.  ~\cite{liu2004cluster,2022SIGIR-KurlandClusterRank,kurland2008opposite}).
In most of cases, it has been applied to re-rank an initial retrieved list by creating clusters of similar  documents in this list. Then the clusters are ranked and the cluster ranking may be further transformed to document ranking.  
%{\bf Proximity graph navagation based on inter-document similarity.}
Search with  inter-document similarity is  also studied~\cite{2006NAACL-docExpan}.
The above studies follow traditional IR   with  lexical signals. 
Advance in neural representations introduces new opportunities and challenges, as studied in  
GAR~\cite{2022CIKM-MacAvaneyGraphReRank} and LADR~\cite{2023SIGIR-LADR} following
HNSW~\cite{2020TPAMI-HNSW} for proximity graph based search with inter-document similarity.
%~\cite{2020TPAMI-HNSW,2022CIKM-MacAvaneyGraphReRank, 2023SIGIR-LADR}.
% is viewed as  a modern instantiation 
%of the clustering hypothesis to revisit these ideas in a new context.  

Motivated by such studies, our work emphasizes on fast selective dense retrieval to augment sparse retrieval effectively
without introducing substantial memory space overhead  by exploiting  inter-cluster and query-cluster similarity at a coarse grain level. 
%Result overlapping degree of sparse retrieval with the dense clusters also plays a key role in the low-cost selection  of dense clusters.

{\bf  Dense retrieval with approximate nearest neighbor search and IVF clustering.}
Faster dense retrieval can be accomplished by approximate nearest neighbor search~\cite{2021Facebook-DrBoost-Lewis, johnson2019billion} with 
a small number of top clusters  (e.g. IVF-based), which can run efficiently on a CPU-only  server.
Memory space cost of dense retrieval can be reduced significantly by 
embedding  quantization (e.g.  ~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ}).
Using these techniques has a tradeoff with a significant relevance drop as shown in these previous studies. 
Our evaluation confirms  that sparse retrieval aided with top IVF-based dense clusters does not yield a competitive relevance 
under a tight time budget.  


There is also a line of work on cluster-based earlier termination studies for nearest neighbor search over a large   high dimensional 
feature vector collection~\cite{2020SIGMOD-ANNadaptive,2023USENIX-ANNcluster}.
The above techniques are targeted for generic data applications and  are not investigated in the context of text document  retrieval where
semantic  relevance presents   its unique ranking challenge and  
requires new design considerations and model structuring.  Another difference is that in our context,
dense cluster selection is conducted after sparse retrieval, which demands optimization that  exploits the interaction  between
these two stages.


