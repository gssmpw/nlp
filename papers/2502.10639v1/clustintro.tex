

\section{Introduction and Related Work}



\comments{
Sparse and dense retrievers are two main categories of retrieval techniques for text  document search.
Traditional sparse retrieval models, such as BM25, use lexical text matching and run
%to retrieve results from the collection 
efficiently by traversing  an inverted index on an inexpensive  CPU platform.
Recently sparse retrieval studies have exploited learned neural representations~\cite{Dai2020deepct,
Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL, Formal2021SPLADE, Formal2021SPLADEV2} for better semantic matching. 
%and SPLADE~\cite{Formal2021SPLADE, Formal2021SPLADEV2} has demonstrated the strong relevance performance among these models. 
The second category of document retrieval 
exploits a pre-trained language model with a dual encoder architecture to produce a dense document representation, 
e.g. ~\cite{gao-2021-condenser,2021SIGIR-Zhan-ADORE-dense,Ren2021RocketQAv2, 
%e.g. ~\cite{Lin2021tctcolbert, 2021CIKM-JPQ-Zhan, xiong2021-ANCE, gao-2021-condenser,2021SIGIR-Zhan-ADORE-dense,Ren2021RocketQAv2}. 
%There are numerious studies recently to enhance  dense retrieval with knowledge distillation
%and  improved  langage model pretraining~\cite
Lin2021tctcolbert, Santhanam2021ColBERTv2,  Wang2022SimLM,  Liu2022RetroMAE}.
}

Recent studies have found that combining  sparse and dense retrieval scores  can further boost
retrieval recall and relevance~\cite{Lin2021unicoil,2022LinearInterpolationJimLin,kuzi2020leveraging},
suggest  that both categories  of retrievers tend to capture  different relevant signals. 
These studies  make an assumption that  a search platform has to run two retrievers separately and a dense retriever.
%delivers   its maximum effectiveness, and the impact of compression and IVF clustering is not considered. 
There is an imbalance of computing platform requirement for running  these two retrievers.
dense retrieval typically requires GPU support unless significant  
%We study the use of dense retrieval results to argument sparse retrieval. 
Sparse retrieval with an inverted index has an advantage that a low-cost CPU server can run the online inference  without GPU.
This paper is targted at a low-cost CPU-friendly retrieval design.
%A judicious tradeoff consideration of retrieval  effectiveness and efficiency is critical on  a low-cost CPU-friendly search platform. 
%The importance of CPU-friendly search is recognized 
%in neural ranking studies~\cite{Yang2021WSDM-BECR,2022WWWforwardIndex,2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
For a large-scale  dataset (e.g. with billions of documents), a practical search system often employs  a multi-stage 
search architecture, which divides the dataset into many partitions so that  a fast first-stage  retrieval 
method  can  search these partitions in parallel.  Such a system  prefers low-cost
CPU-only servers for first-stage retrieval, and both time and memory space complexity of retrieval should be considered 
so that  each server can host as many documents as possible
to reduce the total number of servers to support a large data size and high query traffic. 
%Thus the challenge for retrieval on such an architecture needs to consider both time and space complexity. 
% small search index needs to be as small as possible.
%and the in-memory footprint size of 
%server  needs to host documents in memory as many as possible 
%given a large number  of data partitions involved for parallel search.
%Each server employs multiple CPU cores and is replicated with many copies  for dealing high traffic of  concurrent 
%search requests, and thus 


\comments{
There are recent efficiency studies 
for 
%advancing the inverted index implementation for 
fast sparse retrieval with learned neural 
representations~\cite{Lassance2022SPLADE-efficient,mallia2022faster,qiao2023optimizing,2023SIGIR-Qiao, 2023SIGIR-SPLADE-pruning},
%which run  efficiently  on a single CPU without hardware accelerators  GPU
%which re  retaining strong relevance effectiveness
%%Another development in learned sparse retrieval is that
%sparse retrieval with a learned representation
which accelerates learned sparse retrieval with BM25-guided traversal or index pruning.
For example, Row 3 of Table~\ref{tab:costbudget} shows the latency, memory space cost, and  MRR@10  of
a latest efficient SPLADE retriever for searching 8.8 million 
MS MARCO passages~\cite{2023SIGIR-Qiao} on a CPU only server.
% with a single thread on  a 2.25GHz AMD EPYC 7742  CPU server.
Row 2 is performance of a BM25 based retriever after document expansion. 
%There is a tradeoff  in relevance effectiveness if BM25 guidance or threshold-based pruning  is too aggressive.
%and dense retrieval can boost if it does not add much overhead. 
}

%There are recent efficiency studies on
%fast  sparse retrieval with learned
%representations~\cite{Lassance2022SPLADE-efficient,mallia2022faster,qiao2023optimizing,2023SIGIR-Qiao, 2023SIGIR-SPLADE-pruning}.
Dense retrieval can be accelerated  by approximate nearest neighbor  search with 
IVF clustering~\cite{johnson2019billion, 2021Facebook-DrBoost-Lewis}, quantization~\cite{2021CIKM-JPQ-Zhan,  2022WSDM-Zhan-RepCONC, Xiao2022Distill-VQ},
%and hierachical navigable  small world graphs (HNSW)
and  proximity graph based navigation~\cite{2020TPAMI-HNSW,2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
Limited IVF cluster search and quantization can lead to  a significant  relevance drop while 
%For example, RetroMAE~\cite{Liu2022RetroMAE} dense retrieval without compression is slow, use of
%IVF clustering and OPQ quantization substantially accelerates but leads to a significant drop  of   MRR@10 score when searching up-to 20\%  
%of top dense clusters. 
%There is also a cost-relevance tradeoff  with proximity graph based search  using inter-document similarity distances~\cite{2020TPAMI-HNSW, 
%2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}.  
%Construction of a large document-level  graph   is  expensive
%during offline processing. 
online hosting of a document similarity graph  for graph navigation adds a large space cost.
%For example, the proximity graph  can take extra 4.3GB memory space~\cite{2023SIGIR-LADR} for 8.8M  MS MARCO passages. 
None of above studies has considered to exploit the selective fusion of dense and sparse  retrievers. 
\comments{
That is true also with the  navigation of  a  proximity graph based on inter-document similarity distances called HNSW~\cite{2020TPAMI-HNSW}. 
The proximity graph approach to guide dense retrieval is revisited recently in GAR~\cite{2022CIKM-MacAvaneyGraphReRank}, which is furthered improved by
%LADR Kulkarni  et al.~\cite{2023SIGIR-LADR} improves HNSW.
LADR~\cite{2023SIGIR-LADR}.
%~\cite{2022CIKM-MacAvaneyGraphReRank,2023SIGIR-LADR}. 
The weakness of  the  proximity graph approach is that its graph  construction requires a quadratic computing complexity, which is expensive
for a large dataset and the online hosting of such a graph also adds a significant space requirement  on a low-cost
computing platform. For example, LADR takes extra 4.3GB memory space for 8.8M  passages in the MS MARCO dataset to keep top 128 
top neighbors per document. 
%Similarily HNSW also incurs a high space cost for graph connections.  
%In comparison, cluster-based selective dense retrieval can achieve in a comparable time complexity
%while adding  insignificant extra  space  overhead.

\begin{table}[htbp]
        \centering      
         \resizebox{0.82\columnwidth}{!}{ 
                \begin{tabular}{ r | r  rr}
                        \hline\hline
Sparse or dense methods  & Latency & Memory & MRR@10\\ 
\hline
BM25/DocT5Query & 9.2ms & 1.2GB & 0.261\\  
SPLADE/HT1 & 31.2ms & 3.9GB & 0.396 \\ 
RetroMAE/Uncompressed & 1677ms & 27.2GB & 0.419 \\ 
%SimLM /OPQ             & 225-1579ms & 0.57-2.3GB & $<$ 40.2 \\
RetroMAE /IVFOPQ  & 38-339ms & 1.2-2.3GB  & 0.37-0.401\\ 
                        \hline\hline
\end{tabular}
}
\caption{Latency and space cost of sampled sparse and dense models for MS MARCO passages with Dev query  set}
 \vspace*{-5mm}
\label{tab:costbudget}
\end{table}



  
%with a latency controled within 4 to 8 milliseconds on a single CPU, but with a significant tradeoff in relevance effectiveness.
%For example, MRR@10 for MS MARCO Dev set drops from RetroMAE has dropped from  0.416 to 0.356.
%This paper does not impose 4-8 millisecond latency, but rather looks for a solution that can leverage both dense and sparse retrieval
%and relax the latency constraint in some degree while maximizing its relevance effectiveness as much possible.
%a range below or around  100 milliseconds 

%The response time  for such a system for an interactive service needs to be fast enough a
%and thus search needs to be completed within a few hundred milliseconds. 

%which only requires triggering of dense embedding  computation for a relaively small subset of documents.
%The proposed scheme called cluster-based selective dense  retrieval

}

This paper proposes  a selective fusion of dense and sparse retrieval results 
%Because sparse retrieval is fast on CPU-only servers, this paper 
%augmenting sparse retrieval with 
called CluSD to argument  sparse retrieval with limited cluster-based dense retrieval results.
CluSD includes an LSTM based model with cluster prioritization and pruning to trigger limited dense computation. 
CluSD exploits inter-cluster and query-to-cluster distances and   the overlapping degree between top sparse retrieval results and dense clusters
to optimize cluster selection. 
%and then make a decision to determine which document clusters should be selected  to conduct dense retrieval computation.
The extra memory space overhead required to assist CluSD is negligible, and it  
works well when embedding vectors are highly compressed.
%  and  the extra memory required for hosting dense embeddings can be limited  after compression. 

\comments{
This paper evaluates  the usefulness   of CluSD  under a time and space constraint on a CPU only server when
 searching MS MARCO and BEIR  datasets  and provides a comparison with other baselines.
The evaluation  shows that CluSD can effectively identify a small number of document clusters to augment sparse retrieval results efficiently 
for  a stronger relevance  even though such a  sparse retriever is fast with  a relevance tradeoff. 
} 
%provides a flexibility and reduces  the relevance reqirement of sparse retrieval.
%Namely protentially we can choose a relatively fast sparse retrival with the slightly lower accuracy/recall,
%and  let dense retrivsal make up the loss. Our design needs to  adapt to  sparse retrieval quality and plans

%CSDR in improving the effectiveness of sparse retrieval with limited dense  retrieval
%traversal for low-cost search.  Our emprial findings suggest that SRF  can consistently outperform other baselines on a single CPU server 
%without GPUs 
% and achieve a strong tradeoff of effectiveness and efficiency compared to an exhausitve dense retrieval search.


%from pretrained language models to provide effective semantic text matching between a query and a document.





%0.419 by RocketQAv2~\cite{Ren2021RocketQAv2}, 
%which are the two latest state-of-the-art retrieval-reranker pipelines using cross-encoder based rerankers.
%Re-ranking in  recent dense retrieval work SimLM~\cite{Wang2022SimLM} has  used an expensive cross-encoder to
%achieve impressive MRR score 0.437 
%Re-ranking in RocketQAv2~\cite{Ren2021RocketQAv2} achieves 0.419 with a similar strategy.  



\comments{Modern search engines for text documents
typically employ two-stage ranking design. 
The first retrieval stage extracts top candidate
documents matching a query from a large search index with a fast ranking method. 
The second stage uses a more complex algorithm to 
re-rank top results thoroughly.
Recently neural ranking techniques from transformer-based architectures
have achieved impressive relevance scores 
for both retrieval and top $k$ document re-ranking. 
However, many recent work treats retrieval and reranking stage as two independent tasks and optimize them separately. Only a few works take into consideration the interplay of a retriever and a reranker. RocketQAv2 trains a cross encoder reranker and dual encoder retriever jointly with a strong reranker as teacher and train the retriever by dynamically distilling from reranker. Leonhardt (2022)\footnote{https://arxiv.org/pdf/2110.06051.pdf} uses a linear combination of the semantic similarity scores from the re-ranking phase and lexical matching scores from the retrieval phase with respect to the query for query evaluation.  

These methods either uses the reranker as a teacher to train the re-retriever, or consider the shallow combination of the results from two stages. In these training strategies, both the retriever and reranker are trained towards the same goal. In real application, as the two stages work jointly, either model does not need to perform perfectly by itself. Instead, by taking a more integrated view of the two stages in training and inference, the joint performance of the two stage models will be more efficient and effective. 

Moreover, these approaches typically requires a reranker (such as a cross-encoder in the case of RocketQA). In text retrieval applications, efficient rerankers with low online inference latency are often favored. In recent years, the late interaction reranking model ColBERT, TILDE, BECR provide reasonable performance with low query latency. We demonstrate that pipelines with such efficient models can be improved using our training strategy. 


In our work, we train the efficient reranker and retriever iteratively to improve the integrated performance of the two stage pipeline through a process termed dynamic integrated training.
 To achieve optimal performance under such setting, when training the reranker, we upweight the poorly ranked samples in the joint pipeline through a soft-hard loss. In the meanwhile we gradually update the training set with the top combined ranked results from pipeline during training. With these shifts, the model will automatically be geared towards the weak point of the joint ranking results of the pipeline. In addition, we employ the top documents retrieved by the retriever to provide additional ranking signal in both training and evaluation in the reranking stage. 


During inference, the retriever would return top documents and their corresponding rank order to formulate a pseudo query. The reranker then matches both the original query and the pseudo query with top documents to get two ranking scores for each query-document pair. As the reranker is trained as a booster, the final ranking order from the pipeline naturally becomes the combination of the retrieval and reranking ranked lists. The final top-k ranking scores is determined by the reciprocal ranked fusion~\footnote{https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf} of ranks given by both the retrieval and reranking models.

We find that the propose integrated strategy enjoys several advantages. First, under same model setting and training condition, the dynamic integrated training strategy improves the relevance of the two stage pipeline under both scenarios where reranker is an efficient one or reranker strongly outperforms retriever on MS MARCO passage dataset and TREC19 and TREC20 datasets. This indicates that the integrated training strategy is generally applicable.  Secondly, as the two models learn slightly different goals, the joint performance of the two is less likely to overfit. Indeed, we observe a good zero-shot performance of the pipeline on the BEIR dataset. Lastly, we find that our training strategy can be applicable to the currently popular sparse and dense retrieval pipeline where two retrieval models jointly determine the final ranking list. 

}
