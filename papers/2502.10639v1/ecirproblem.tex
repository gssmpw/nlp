\vspace*{-1em}
%\section{Background}
\section{Selective Dense Cluster Retrieval}
%\section{Problem Definition}
\label{sect:background}

\comments{
\begin{table}[h]
	\centering
		%\resizebox{1.02\columnwidth}{!}{ 
		\begin{small}
		\begin{tabular}{l l}
			\hline
			Symbol & Def. \\
                \hline
                %$\mathcal{D}$ & A set of documents in a dataset\\
                %$N$ & Total number of documents in a dataset \\
                $Q$ & A query \\
                $d_i$ & A document \\
                $\mathcal{D}^{+}$, $\mathcal{D}^{-}$ & Positive/negative document subsets corresponding to $Q$  \\
                $|.|$ &  The size of a set\\
		$\Theta$& Parameters of a scoring model\\
		$S(Q,d_i, \Theta)$ & The rank score of a document for a query using a model\\
%			& based on a model with parameters $\Theta$. \\
                $p_i$ & Teacher's top one probability of document $d_i$,\\
			& standing for  $P(d_i|Q, \mathcal{D}^+, \mathcal{D}^-, \Theta$) \\
                $q_i$ & Student's top one probability of document $d_i$\\
                $\pi(.)$ & The rank position of a document for a query\\
                $\beta_i$ & Exponent weight bias for negative  document $d_i$\\
                $\gamma, \alpha$ & CKL hyper-parameters\\
                \hline
		\end{tabular}
		\end{small}
	%}
	\caption{Table of Symbols}

	\label{tab:data}
\end{table}
}

\textbf{Problem definition.}
Given query $q$ for searching a collection of $D$ text documents:   $ \{d_i\}^D_{i=1}$, 
retrieval  obtains the top $k$ relevant documents from this collection based on
the similarity between query $q$ and document  $d_i$ using  their
representation vectors.
With a lexical representation, each  representation vector of a document or a query is sparse.
Sparse retrieval uses  the dot product of their lexical representation vectors as the similarity function
and implements its search efficiently using an inverted index: 
%\begin{equation}
%\label{eq:sparseretrieval}
$L(q) \cdot  L(d_i)$
%\end{equation}
%L(q,d_i ) = 
where $L(.)$ is a lexical representation   which is a vector of weighted term tokens for a document or a query.
For BM25-based lexical representation~\cite{Robertson2009BM25}, 
$L(.)$ treats each document or a query as a bag of terms.
% Q: do we want to keep referring to everything as bag of terms?
%and term weights  are scaled based on   frequency of terms within a bag and  across all bags involved.
Term weights are scaled based on the frequency of terms within a document and across all documents in the collection.
For a learned sparse representation~\cite{Dai2020deepct, Mallia2021deepimpact, Lin2021unicoil,2021NAACL-Gao-COIL}, 
a document or a query is encoded using  a trained neural ranking model, which
%produces a bag which may contain original  and  expanded terms.
produces modified representations of document or query containing both original and expanded terms.
The encoding functions for a query and document can be different. Without the loss of generality, we assume they are the same in this paper.
With a single-vector dense  representation, each  representation vector of a document or a query is a dense vector. 
Dense retrieval computes the following rank score: 
%\begin{equation}
%\label{eq:denseretrieval}
$R(q) \cdot  R(d_i)$
%\end{equation}
%D(q,d_i) 
where $R(.)$ is a dense representation vector  of a fixed size~\cite{Karpukhin2020DPR}.


  %revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
%The weights of these terms evaluated in this paper use the SPLADE model as discussed below while other options are possible.
%(e.g., a bag of words or a neural encoder). 
%The implementation of such a dot product for a sparse retriever uses the inverted index data structure.
\comments{
%{\bf Learned term weights with SPLADE}.
For each document, the SPLADE term weighting revises the SparTerm model~\cite{Bai2020SparTerm} to predict the weight 
score $w_j$ of $j$-th token term in BERT WordPiece vocabulary.
\begin{equation}
\label{eq:splade}
w_j = \sum_{i \in d}  log (1 + ReLU ( H(h_i)^T E_j + b_j)) 
\end{equation}
where document $d$ consists of a sequence of BERT last layer's embeddings $(h_1, h_2, $
$\cdots, h_n)$.
$E_j$ is the BERT input embedding of the $j$-th token and $b_j$ is a token level bias.
$H(.)$ is a linear layer with activation and layer normalization. 
% (In SPLADE GeLU activation is used). 

{\bf Combining the results of a dense retriever and a sparse retriever.}
}
Following the work of~\cite{kuzi2020leveraging, 2022LinearInterpolationJimLin, Gao2020ComplementingLR}, 
linear interpolation is used  to  ensemble  dense and sparse retrieval scores. 
%Let $S$ be the set of top results obtained by a sparse retriever, and $D$ be
%be the set of results obtained by dense retrieval. 
%For any document $d_i \in S \cup D$, its  
The fused rank score for document $d_i$ in responding  query $q$ is:
%\begin{equation}
%\label{eq:fusion}
$\alpha L(q) \cdot  L(d_i) +
(1-\alpha) R(q) \cdot  R(d_i) 
$
%\end{equation}
where $\alpha$ is a coefficient in a range between 0 and 1.
Our goal is to maintain a relevance competitive to full dense retrieval, while minimizing the time and memory overhead of cluster-based dense retrieval,
guided by sparse retrieval results.
%to minimize the scope of dense clusters to visit.
% Parker: below is awkard, and a bit of a run-on sentence. Above I reworked it, but it's still not great
%Our optimization goal is to minimize the scope of cluster-based dense retrieval and extra memory space overhead by leveraging sparse retrieval 
%while maintaining a relevance competitive to full dense retrieval.  
%In this way, limited dense retrieval that augments sparse retrieval is fast with minimum space consumption  on a CPU server. 
\comments{
The ensembling method used in our evaluation is 
%The final ranking of the top $K$ documents is 
the reciprocal rank fusion (RRF) following Cormack et al.~\cite{Cormack2009RRF}, considering ranking positions of each candidate 
generated by a retriever and by a re-ranker. 
We use RRF
% above reciprocal rank fusion 
instead of linear interpolation as it has a similar performance with a simplified combination of two scoring models. 
%We choose RRF because it is easily applicable to multiple ranked lists ignoring the score distribution differences. 
\begin{equation}
     %RRF(Q,d) =  \sum_{r \in {rt, rr,}} \frac{1}{k_r + r(d)},
     RRF(Q,d) =  
\frac{1}{c_1 + \pi_1(d)} + 
\frac{1}{c_{2} + \pi_2(d)},
     \label{eq:score}
\end{equation}
where $c_{1}$ and $c_{2}$ are fixed smoothing constants. $\pi_2(d)$ is the rank position based  a re-ranker formula
while $\pi_1(d)$ is the rank position based on a retriever.
}


\label{sec:clusd}

{\bf Design considerations.}
We aim to identify the opportunity to skip a substantial portion of dense retrieval
%by exploiting the potential overlap of sparse and  dense retrieval results.
by taking the IVF clustering approach~\cite{johnson2019billion} that  groups similar documents.
This  helps CluSD to recognize similar relevant documents during inference while avoiding the
need of a document-level proximity graph. The number of clusters needs to be as large as possible (meaning a small size per cluster) to avoid unnecessary I/O and computing cost. That imposes a significant challenge for accurate and fast selection of dense clusters.
%Thus unlike IVF cluster search, 
%our selection goes beyond using the similarity between the query and cluster centroids to choose top clusters.  
To address this  our work proposes a two-stage LSTM-based 
selection algorithm that exploits the overlap of sparse retrieval results 
with the potential dense clusters.
% for better efficiency and relevance.
