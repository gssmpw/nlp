\section{Evaluations}

\comments{
Our evaluation answers  the following research questions around our proposed techniques and the correspond subsection are marked.

\textbf{RQ1} (Section~\ref{sect:eval0space}): 
How does CluSD compare against other low-cost baselines when space is constrained?
%extra space overhead is minimized by augmentation under different compression configurations? 

\textbf{RQ2} (Section~\ref{sect:evaltimebudget}): 
How does CluSD  perform under a retrieval latency time budget when there is no space restriction?

\textbf{RQ3} (Section~\ref{sect:evalbeir}): 
How does CluSD  perform in searching  out-of-domain text content?
Zero-shot retrieval performance is important since a large dataset can contain documents from many domains. 


\textbf{RQ4} (Section~\ref{sect:evalLSTM}): 
Is  LSTM with cluster pruning preferred compared to other design options?
Are  features designed for LSTM useful?

\textbf{RQ5} 
(Section~\ref{sect:clusterno}): 
How does the number of clusters used in dataset partitioning 
affect the relevance and latency of CluSD? 

\textbf{RQ6} 
(Section~\ref{sect:threshold}):
How does the choice of selection threshold value in LSTM affect the average number of clusters selected?
%Does the cluster selection model dynamically select different number of clusters for different queries? How the selection differs for different datasets?

\textbf{RQ7}
(Section~\ref{sect:evalsparse}):
How does CluSD perform in augmenting  different sparse retrievers with different efficiency and relevance characteristics?
}

\comments{
To answer \textbf{RQ1},  we compare our proposed approach on the in domain test set, MSMARCO passage dev, DL 19, 20 with other commonly used techniques under storage (Table~\ref{tab:mainspace}) and latency constraints (Tabel~\ref{tab:maintime}).  
For \textbf{RQ2}, as our cluster selection technique requires training, we would like to understand how the approach would work on our domain dataset. In Table~\ref{tab:beir}, we use our cluster selection model trained on MS MARCO datasets on the 13 datasets from BEIR on a zero-shot manner and observe promising performance on all 13 out-domain datasets. 
For \textbf{RQ3} and \textbf{RQ4}, We compare the different cluster selection techniques, including heuristic approaches, a point-wise prediction model xgboost and a vanilla RNN prediction model against the proposed LSTM model (Table~\ref{tab:cluster_feat}).
To answer \textbf{RQ5}, we take two scenarios where the number of clusters is 4096, and 8192. Then we present the relevance in MRR@10, recall@1000, and the latency in terms of mean and 99 percentile time under three settings: when the flat index is used without compression, when we use OPQ to compress with m=128 or 64. Results are shown in Figure~\ref{fig:ncluster}.
For \textbf{RQ6}, we first plot the distribution of the number of clusters selected for each query under different model prediction thresholds. We also plot the average number of clusters for MSMARCO dev set and BEIR in Figure~\ref{fig:thresholds}.
For \textbf{RQ7}, we report the performance using three weaker sparse retrievers in Table~\ref{tab:sparse}. 
}

\subsection{Settings }
{\bf Datasets and measures}.
Our evaluation uses the MS MARCO dataset with 8.8 million passages for full passage ranking~\cite{Campos2016MSMARCO,Craswell2020OverviewOT}.
The test query sets are the Dev set with 6980 queries and TREC deep learning (DL)  2019/2020 tracks with 43 and 54 queries.
We report mean reciprocal rank at 10 (MRR@10) and recall at 1000 (recall@1K)
which is the percentage of relevant-labeled results appeared in the final top-1000 results.
For DL19 and DL20 sets, we report NDCG@10.
%We report average NDCG@10 for the 13 publically available BEIR datasets~\cite{thakur2021beir}
%to evaluate  zero-shot retrieval performance.  
We also report average NDCG@10 for 
the  BEIR  collection containing the 13 publicly available datasets~\cite{thakur2021beir} 
for evaluating zero-shot retrieval performance. 
%BEIR is a heterogeneous benchmark containing a variety of IR tasks, 
%including search tasks (e.g.  question-answering, bio-medical IR, and entity retrieval)  
%and semantic relatedness tasks (e.g.  duplicate question retrieval). 
The size of these data sets ranges from 3,633 to 5.4M.
%Their average query length ranges from 5.39 to 192.98, 
%and  their average document length ranges from 11.44 to 634.79. 
%Each dataset has binary or three-level relevance labels.
The above metrics follow the common practice in the information retrieval literature.

{\bf Models and parameters}.
For sparse retrieval, we adopt two variations of 
SPLADE~\cite{Formal2021SPLADE, Formal_etal_SIGIR2022_splade++,Lassance2022SPLADE-efficient} 
based on hybrid thresholding~\cite{2023SIGIR-Qiao} and  BM25-guided traversal~\cite{mallia2022faster,qiao2023optimizing}:
% and one BM25 based model with different relevance and efficiency  tradeoffs:
1) SPLADE-HT1 with index space 3.9GB and 31.2 ms retrieval latency; 
%2) SPLADE-HT1-fast is a variation of SPLADE-HT1 with a different parameter setting, which has 3.9GB index space and 22.4 ms latency. 
2) SPLADE-effi-HT3 with  3GB space and  12.4ms latency. 
We also test on three other sparse models:
% uniCOIL~\cite{Lin2021unicoil,2021NAACL-Gao-COIL}, and LexMAE~\cite{shen2023lexmae}.
%2) DeepImpact~\cite{Mallia2021deepimpact};
3) uniCOIL~\cite{ Lin2021unicoil,2021NAACL-Gao-COIL} with 1.2GB index and 35.5ms latency;
4) LexMAE~\cite{shen2023lexmae} with 3.7GB index and 180ms latency;
5) BM25-T5 that applies  BM25 with  DocT5Query document expansion~\cite{Cheriton2019doct5query}.  
BM25-T5 has 1.2GB index and  9.2ms latency. 
%Their index space varies from 1.2GB to  3.7GB.
% with 1.2GB index space.
% with latency 9.2ms.
%5) BM25 after T5-based document expansion~\cite{Cheriton2019doct5query}. 
They represent the state-of-the-art sparse retrievers.
% with the best relevance performance evaluated on MS MARCO.

For dense retrieval, we adopt the recent dense models
SimLM~\cite{Wang2022SimLM} and RetroMAE~\cite{Liu2022RetroMAE}. 
We use their checkpoints from the Huggingface library to generate document and query embeddings 
and  use  K-means in the FAISS library~\cite{johnson2019billion} to derive dense embedding clusters.
RetroMAE-2~\cite{liu-etal-2023-retromae} is not used because its checkpoint is not released.


Our evaluation implementation uses C++ and Python and the
related code will be released to the public after publication of this paper. 
\comments{
%Note that SimLM model's embeddings are normalized while RetroMAE's embeddings are not. 
For sparse retrieval, we use two fast versions of SPLADE and a SPLADE-efficient model, as well as BM25-T5. 
The implementation of SPLADE models follows 2GTI optimization~\cite{qiao2023optimizing}. %its official release~\cite{spladeRepo}. 
SPLADE-HT1 is the SPLADE model trained with hybrid thresholding scheme~\cite{2023SIGIR-Qiao} using $\lambda_{t}=1$, 
SPLADE-effi-HT3
is the SPLADE-efficient model~\cite{Lassance2022SPLADE-efficient} trained with hybrid thresholding scheme using $\lambda_{t}=3$. Both of these two models use the "accurate" configuration in 2GTI during retrieval, while SPLADE-HT1-fast uses the "fast" configuration.
The sparse retrieval code for Splade and BM25-T5 uses the code from PISA~\cite{mallia2019pisa}
with some optimization~\cite{Lassance2022SPLADE-efficient,2023SIGIR-Qiao}.
}
To train the CluSD model, we sample 5000 queries from the MS MARCO training set 
%and use the LSTM unit in Pyytorch. 
and the hidden dimension for LSTM is 32. The models are trained for 150 epochs. 
For sparse and dense model interpolation, we use min-max normalization to rescale the top results per query. 
%Then we use linear combination with equal weights to combine two ranked lists. 
For  BM25-T5, the interpolation  weights are  0.05 and 0.95 for sparse  and  dense scores, respectively. 
For other learned sparse retrieval models, they are  0.5 and  0.5, respectively. 
%In terms of missing document in either ranked list, as the smallest value from each normalized list is 0, there is no missing imputation needed as we 
%can view the missing  documents taking the value of the worst document in the ranked list automatically. 

%For CluSD, the default setting of number of clusters is $N=8192$, and the input sequence length to the Stage II LSTM model is $n=32$.  
To report the mean latency and 99 percentile latency, 
we  run test queries multiple times  from MS MARCO Dev set using a single thread on  a 2.25GHz AMD EPYC 7742  CPU server with PCIe SSD.
The reason to use a single thread is that each CPU server in a busy search site handles many queries in parallel, and thus we only take
one core resource with one thread from this server and leave other cores  for other queries.
That is the standard practice in many of the previous efficiency studies in the IR literature.

{\bf Time budget.}
%For search latency, our goal is to minimize the relevance effectiveness loss to a small degree while still accomplishing  a fast response time
%acceptable to the interactive search system requirement.
We use  two time budget settings: around 50ms  and 25ms, including the  sparse retrieval time.
We explain the reason as follows.
In a search system running as an interactive  web service,  a query with multi-stage processing
needs to be completed within a few hundred milliseconds.
%Considering the multi-stage search nature  and other service overhead involved,
A first-stage retriever operation  for a data partition needs to be completed within several tens of milliseconds without query caching.
Our targeted time budget to conduct dense retrieval cannot be too high     compared to  sparse retrieval cost.
%For the example of Table~\ref{tab:costbudget},
%The studies from~\cite{Lassance2022SPLADE-efficientmallia2022GTI,Qiao20222GT,2023SIGIR-Qiao},
For example, if sparse retrieval can complete  in 30 ms on an  average CPU and the total latency
target is around 50ms,  the cost of dense retrieval is targeted under around  20ms.
% At most the total sparse and dense retrieval cannot be more than 100ms.
%Accuracy of above query-specific cluster skipping while retaining a competiitve relevance is the key to accomplish this latency goal.
%Notice that the latency target in the  LADR paper~\cite{2023SIGIR-LADR} is in a 4ms-8ms range
%and
%This latency budget choice is reasonable because for
%Also our evaluation is conducted in a low-end CPU server and a faster CPU can further accelerate.
%CPU latency reported in~\cite{2022WWWforwardIndex} is at 11

\comments{
{\bf Baselines.}
We compare CluSD with two groups of baselines under space and time constraints respectively.
The first group compared in Section~\ref{sect:eval0space} 
is to fuse  sparse retrieval with cluster-based selective dense retrieval.
That includes  dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
The second group compared in Section~\ref{sect:evaltimebudget} 
is to fuse  sparse retrieval with re-ranking and graph-based selective dense retrieval such as HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR}. 
This group involves random access of embeddings and/or graph nodes, which  can incur high I/O cost when data is not on disk.
}


{\bf Statistical significance.}
For MS MARCO Dev set, the result of a method is marked with  tag $^\dag$ when 
statistically significant drop is observed compared to  an oracle baseline
tagged with $\blacktriangle$ at 95\% confidence level.  This oracle
is sparse retrieval fused with full dense retrieval marked as ``S+D'', which typically delivers the highest relevance  
using all uncompressed or quantized dense embeddings.


\comments{
{\bf Baselines and notations}
%We select two groups of baselines under space and time constraints respectively.
Table~\ref{tab:mainspace} compares  baselines  with memory space constraint. 
We include dense retrieval with quantization option OPQ and  IVFOPQ with IVF clustering available in FAISS.
OPQ quantization  is configured with  the number of codebooks as $m=$ 128 or 64.  
When IVF clustering is used,  we report its performance  using top 10\%, 5\%, 2\% clusters respectively
as a standard approach which performs   limited top cluster retrieval. 
CluSD execution is also based on the the same OPQ or  IVFOPQ index from  FAISS but clusters are  selected  by our LSTM model. 
%Switching a quantization different  from OPQ  is might result in an improvement in relevance, but it is orthogonal to our proposal. 
We use notation ``S+D'' to report the interpolated performance of a sparse model and a dense model in Table~\ref{tab:mainspace} and Table~\ref{tab:sparse}.

Table~\ref{tab:maintime} compares  baselines  without  memory space constraint. 
For proximity graph-based dense retrieval which incurs additional graph space overhead,
we include  HNSW~\cite{2020TPAMI-HNSW} and  LADR~\cite{2023SIGIR-LADR} as shown in Table~\ref{tab:maintime}. 
%For HNSW, we fix the number of neighbors to be 128 and vary the expansion factor(ef) to meet the latency constraint. 
%For LADR, we use its default setting with seed = 200, number of neighbor = 128 and use the exploration depth as 50 to meet the latency requirements in 
%Table~\ref{tab:maintime}.
%Uncompressed dense embeddings (marked as "flat") are used
%to exclude the impact due to quantization.
We also compare  a simple re-ranking method,  denoted as ``D-rerank'' or ``rrk'', 
that fetches the dense scores of top 1000 of sparse retrieval to re-rank with interpolation. 
%We use the same clustering setting for IVF and CluSD. 

%The reported CluSD performance on the flat index uses the exact same IVF index. 
The configuration we selected to meet the latency requirement for CluSD uses on average 20-25 (0.25 - 0.3\%) clusters per query. 
%For re-ranking, we report the results of re-ranking top 1000 results from the sparse model using the dense model.  

}

