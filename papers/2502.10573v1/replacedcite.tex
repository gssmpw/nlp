\section{Related Work}
Over the past decade, predicting the next activity in business process management (BPM) has attracted attention for improving organizational efficiency and supporting better decision-making. Numerous studies have focused on predicting the next activity in ongoing cases. Early approaches primarily relied on traditional machine learning techniques Models like Decision Trees and Random Forests were first applied because of their explanatory simplicity____. However, with increasing complexity in event logs, deep learning techniques were increasingly researched to better respond to complexities inherent in temporal and sequential information.

In the initial application of deep learning in BPM, RNNs were extensively employed to forecast the next activity ____. Although RNNs showed promising performance, they failed to remember the earlier context in lengthy
sequences, and thus their performance on sequence prediction tasks was limited ____. To amend this limitation,____ explored the use of CNNs stacked inception modules as alternatives to RNNs like LSTMs in next-activity prediction of process mining. Their CNN model performs better than RNNs in accuracy and computational expense, with an average accuracy improvement of 12.17\% and halving the training time on real-world data ____. The findings demonstrate that CNNs are a strong alternative for sequential data tasks, and future work includes predicting the following activities and execution times using advanced inception modules____. Building on these findings, other researchers further explored LSTMs for next-activity prediction. For example, in ____, LSTMs were applied to event logs to enhance predictions in BPM, with a particular focus on real-world applications and anomaly detection. 

The potential of LSTM-based models was further advanced with methods like Data-Aware Explainable Next Activity Prediction (DENAP)____. DENAP combines LSTM networks with Layer-Wise Relevance Propagation (LRP) to provide accurate predictions (80â€“97\%) alongside interpretability.

Transformer models have only recently been discovered as powerful tools for next activity prediction. Transformers leverage self-attention mechanisms to avoid the limitations of traditional RNNs and LSTMs in capturing long-range dependencies. For instance, The Multi-View information Fusion Method (MiFTMA) employs transformers with multi-view which more precisely capture long-term dependencies compared to the baseline methods____. Similarly, the Multi-Task Learning Guided Transformer Network (MTLFormer) combines transformers' self-attention with multi-task parallel training. This approach reduces complexity but improves accuracy in long-distance predictions, where relevant information may be spread across distant input parts____.
In____, authors proposed ProcessTransformer, a transformer-based model capable of learning high-level representations from event logs with minimal preprocessing. The approach surpasses 80\% accuracy in next-activity prediction across nine datasets, improving traditional baselines by capturing long-range dependencies without the need for recurrence. Remarkably, ProcessTransformer demonstrates strong performance even with no context and attribute-aware model.

However, existing transformer-based models (\textit{i.e.,} MiFTMA and ProcessTransformer) employ a sliding window approach, segmenting traces into fixed-length k-prefixes for processing. With this approach, they failed to consider long-term process behavior, and they mostly relied on the last recent behavior of the process. To address this, our proposed DAW-Transformer prepares sequences for each attribute using an extended sliding window, providing a dynamic mechanism to effectively capture and incorporate long sequences into the model. By incorporating all historical events, our method ensures a more comprehensive understanding of the process, improving predictive accuracy.