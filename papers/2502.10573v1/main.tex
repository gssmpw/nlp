\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{mathptmx}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{makecell}
\onehalfspacing
\usepackage{multibib}
\usepackage{graphicx,xcolor,comment}
\usepackage{pgfplots}
\pgfplotsset{compat=1.7}
\usepackage{dingbat}
\usepackage{multirow}
\usepackage{natbib}
\usepackage{lscape}
\usepackage{fancyhdr}
\usepackage{authblk}
\usepackage{multicol}
\usepackage{adjustbox}
\usepackage{geometry}
\usepackage{caption}
\usepackage{rotating} 
\usepackage{float}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{csquotes}
\usepackage{subcaption}
\usepackage{graphicx}  
\usepackage{listingsutf8}
\usepackage{hyperref}
\usepackage{cleveref}

\captionsetup{
  font={footnotesize},
  aboveskip=1pt,
  belowskip=1pt
}
\newgeometry{left=0.8in,right=0.8in} 

\providecommand{\keywords}[1]{
  \small	
  \textbf{\textit{Keywords: }} #1
}

\title{\vspace{-2cm} \textbf{An Innovative Next Activity Prediction Approach Using Process Entropy and DAW-Transformer}}

\author[1]{Hadi Zare}
\author[2]{Mostafa Abbasi}
\author[1]{Maryam Ahang}
\author[1,2]{Homayoun Najjaran\thanks{Corresponding author: \texttt{najjaran@uvic.ca}}}

\affil[1]{\textit{Department of Electrical and Computer Engineering, Faculty of Engineering and Computer Science, University of Victoria, Victoria, Canada}}
\affil[2]{\textit{Department of Mechanical Engineering, Faculty of Engineering and Computer Science, University of Victoria, Victoria, Canada}}





\lstset{breaklines=true}
% \newcites{References}{\small{Introduction and other stuff}}
% \newcites{pi}{\small{PROCESS IMPROVEMENT}}
% \newcites{pe}{\small{PROCESS ENHANCEMENT}}
% \newcites{pepi}{\small{BOTH IMPROVEMENT AND ENHANCEMENT}}

\begin{document}

\maketitle


\begin{abstract}

\textbf{Purpose} - In Business Process Management (BPM), accurate prediction of the next activities is vital for operational efficiency and decision-making. Current Artificial Intelligence (AI)/Machine Learning (ML) models struggle with the complexity and evolving nature of business process event logs, balancing accuracy and interpretability. This paper proposes an entropy-driven model selection approach and DAW-Transformer, which stands for Dynamic Attribute-Aware Transformer, to integrate all attributes with a dynamic window for better accuracy.

%This paper aims to address these limitations by developing a dynamic model selection approach based on process entropy.

%Purpose – In Business Process Management (BPM), accurate next-activity prediction is vital for efficiency and decision-making. AI/ML models struggle with complex, evolving event logs, balancing accuracy and interpretability. This paper proposes an entropy-based model selection approach, leveraging DAW-Transformer to integrate all attributes with a dynamic window for better accuracy.


\textbf{Design/methodology/approach} - This paper introduces a novel next-activity prediction approach that uses process entropy to assess the complexity of event logs and dynamically select the most suitable ML model. A new transformer-based architecture with multi-head attention and dynamic windowing mechanism, DAW-Transformer, is proposed to capture long-range dependencies and utilize all relevant event log attributes. Experiments were conducted on six public datasets, and the performance was evaluated with process entropy.

\textbf{Finding} - The results demonstrate the effectiveness of the approach across these publicly available datasets. DAW-Transformer achieved superior performance, especially on high-entropy datasets such as Sepsis exceeding Limited window Multi-Transformers by 4.69\% and a benchmark CNN-LSTM-SAtt model by 3.07\%. For low-entropy datasets like Road Traffic Fine, simpler, more interpretable algorithms like Random Forest performed nearly as well as the more complex DAW-Transformer and offered better handling of imbalanced data and improved explainability.


%Further analysis showed Random Forest also demonstrated better handling of imbalanced data and improved explainability on the Road Traffic Fine dataset.

%Further analysis showed Random Forest achieved a significantly lower confusion matrix entropy (0.12) than DAW-Transformer (0.35) on the Road Traffic Fine dataset, highlighting its ability to handle imbalanced data and improve explainability.

\textbf{Originality/ value} - This work's novelty lies in the proposed DAW-Transformer, with a dynamic window and considering all relevant attributes. Also, entropy-driven selection methods offer a robust, accurate, and interpretable solution for next-activity prediction.

%its dynamic model selection based on process entropy, balancing prediction performance and interpretability.


%This work offers a novel contribution by dynamically adapting model selection based on process entropy, effectively balancing predictive performance with interpretability.  The proposed DAW-Transformer model and the entropy-based model selection approach provide a robust solution for next-activity prediction in BPM, addressing the limitations of existing methods and offering improved accuracy and explainability.

%In Business Process Management (BPM), the accurate prediction of the next activities is vital for enhancing operational efficiency and enabling well-informed decision-making.  Various AI and machine learning (ML)-based methods have been introduced, gaining significant attention due to their performance and capabilities. Notably, interpretability and accuracy emerge as the two key attributes required for ensuring effective prediction models. However, the inherent complexity of business process event logs introduces significant challenges. Processes often evolve, requiring models that adapt to shifting behavioral patterns. Moreover, the diverse flexibility of processes within organizations underscores the inadequacy of a one-size-fits-all prediction model. This paper introduces a next activity prediction approach that uniquely incorporates process entropy to dynamically evaluate dataset complexity and select the most effective ML-based prediction model. Unlike existing methods, this approach balances interpretability and performance by tailoring model selection to the behavioral characteristics of event logs, offering a robust solution for next-activity prediction. This study employs a widely used entropy metric to assess the flexibility of individual processes, effectively capturing variability across diverse datasets. However, a notable limitation of the existing techniques is their inability to capture long-range dependencies between activities or to fully represent all critical characteristics of an event log, which often constitute its most important features. To address these shortcomings, we improved the existing transformer architecture and proposed DAW-Transformer, a multi-head attention-based model. This novel approach integrates all relevant event log attributes and incorporates a dynamic windowing mechanism to enhance predictive capabilities.

%We conducted experiments on six publicly available datasets to evaluate performance in relation to process entropy. Additionally, we benchmarked the performance of the proposed DAW-Transformer against existing approaches. The results demonstrate that the DAW-Transformer model achieves superior performance, particularly in cases with higher entropy (e.g., Sepsis). The DAW-Transformer achieved an accuracy of 70.14\%, surpassing conventional Transformers by 4.69\%, attributed to its dynamic windowing mechanism and rich input features, and delivering a 3.07\% improvement over CNN-LSTM-SAtt, the benchmark deep learning model in the literature. In contrast, datasets with low process entropy, such as Road Traffic Fine, are better suited to more interpretable algorithms like Random Forest and Decision Tree, which achieve accuracies of 99.71\% and 99.69\%, respectively - results nearly identical to those obtained with the DAW-Transformer (92.36\%). Furthermore, an examination of the confusion matrix reveals that Random Forest attains a lower entropy value of 0.12 Compared to the 0.35 of DAW-Transformer, signifying a notable enhancement of 191\% which means the lower scattering in the matrix. This improvement demonstrates the robustness of our method in handling imbalanced distributions without requiring extensive pre-processing, while also enhancing explainability. Furthermore, it underscores the reliability of our approach in using process entropy as a key factor in identifying the most suitable methods that strike a balance between performance and interpretability.

\textbf{Keywords} Next activity prediction, Business process management, Machine learning, Process entropy, Transformer

\textbf{Paper type} Research paper
\end{abstract}
%\keywords{Next activity prediction, Business process management, Machine learning, Process entropy, Transformer}

%\textbf{Paper type} Research paper or Technical paper
\section{Introduction}

Process mining is a business process management (BPM) technique that offers information extracted from event logs to help improve organizational operations and also service performance. ~\citep{burattin2015process, turner2012process}. 
One critical application of process mining is predicting the next activity, which provides precise execution insights for ongoing or incomplete process instances~\citep{dentamaro2023next}. Predicting the most likely subsequent steps in a process allows proactive resource allocation, optimization of workflow, and defect detection early, assuring smooth execution and the achievement of the proper execution of the process goals according to ~\citep{sun2024next}. These capabilities are particularly critical in healthcare and manufacturing where anticipating the following steps can significantly improve operational efficiency and outcomes. Also, In customer service, understanding customer expectations is essential for delivering high-quality services and gaining a competitive advantage ~\citep{kim2001rationalizing}. Predicting the next activity accurately is crucial in customer service, as it enables businesses to align service processes with customer needs, improve responsiveness, and enhance overall satisfaction.

Most businesses rely on event logs and the result of process mining to support decision-making and identify bottlenecks~\citep{rivera2022multi}. In this context, various machine learning approaches, such as Decision Trees, Random Forests, Long Short-Term Memory (LSTM) networks, Convolutional Neural Networks (CNNs), and others, have been explored for the next activity prediction. Among these, deep learning methods have gained significant attention because of their ability to model complex patterns and sequences~\citep{abbasi2025forlaps}.

Deep learning is widely adopted for sequence modeling tasks because it can autonomously learn complex data representations~\citep{sun2024next,lecun2015deep}. Using layered neural networks, deep learning is able to detect patterns and correlations in large datasets so that data can be analyzed at multiple levels of abstraction~\citep{lecun2015deep}. These capabilities make it particularly suitable for the prediction of the following activity in BPM, where algorithms such as Recurrent Neural Networks (RNNs) and LSTM networks have shown considerable success~\citep{musa2023prediction, di2019activity}. These models are trained in event logs, learning relationships between activities, and forecasting subsequent process steps~\citep{wang2023mitfm}. However, most of the current models have a strong focus on activity sequences~\citep{wang2023mitfm}, overlooking the rich contextual information that significantly influences process outcomes. This limitation reduces their ability to fully capture the context of a process.

On the other hand, transformer architecture greatly improves sequence-to-sequence modeling~\citep{vaswani2017attention}, offering a reliable solution for this task in process mining. Transformers utilize an encoder-decoder model with multi-head self-attention, which makes it possible to incorporate multi-view information and effectively capture long-distance dependencies through scaled dot-product attention ~\citep{vaswani2017attention, wang2023mitfm}. Transformers have recorded exceptional performance in machine translation, revolutionizing natural language processing and giving rise to many advanced
models. This renders Transformers central to modern AI~\citep{vaswani2017attention}. 

To address these issues, we propose the DAW-Transformer, a multi-head transformer-based method that integrates multiple attributes of event logs. By leveraging a self-attention mechanism, the DAW-Transformer delivers high-accuracy predictions, particularly for long sequences~\citep{vaswani2017attention}. It incorporates all relevant event log attributes, ensuring no loss of information and enabling more precise predictions of subsequent activities. Additionally, the model employs a dynamic window mechanism, allowing it to effectively incorporate long sequences into the process models.

%To address this challenge, we propose the \textbf{DAW-Transformer}, a multi-head transformer model that integrates multiple attributes from event logs. By leveraging a self-attention mechanism, the DAW-Transformer achieves high accuracy in long-sequence predictions~\citep{vaswani2017attention}.
In addition, selecting the most appropriate predictive model for a dataset remains a critical challenge in Business Process Management (BPM). The ideal model must trade-off between accuracy, efficiency, and resource usage. Interpretability is also a key consideration, as clear algorithms foster trust in decision-making. Traditional models such as Decision Trees and Random Forests are often more interpretable, offering explainability over more complex deep learning models like Transformers and CNNs~\citep{kumar2024transparency}. As a result, companies tend to favor explainable models for applications where transparency is crucial.

The proposed research addresses key gaps in next-activity prediction. One major gap is the lack of a systematic method to identify the most efficient and accurate predictive model based on dataset complexity, coupled with the absence of a proper metric to guide decision-making. Current approaches often rely on trial and error, leading to wasted time and resources. Another gap is the underutilization of event attributes in following activity prediction tasks, as well as the absence of an appropriate approach to capture these attributes, which limits the contextual understanding of processes.


The rest of this paper is organized as follows: Section 2 describes related work on the next activity prediction. Section 3 provides preliminaries for understanding our approaches. Section 4 introduces the proposed methods and methodology, and Section 5 discusses experiments. Sections 6 and 7 cover the results, discussion, and conclusions. Lastly, Section 8 provides acknowledgments.

\section{Related Work}

Over the past decade, predicting the next activity in business process management (BPM) has attracted attention for improving organizational efficiency and supporting better decision-making. Numerous studies have focused on predicting the next activity in ongoing cases. Early approaches primarily relied on traditional machine learning techniques Models like Decision Trees and Random Forests were first applied because of their explanatory simplicity~\citep{breiman2001random, song2015decision}. However, with increasing complexity in event logs, deep learning techniques were increasingly researched to better respond to complexities inherent in temporal and sequential information.

In the initial application of deep learning in BPM, RNNs were extensively employed to forecast the next activity \citep{abbasi2024review}. Although RNNs showed promising performance, they failed to remember the earlier context in lengthy
sequences, and thus their performance on sequence prediction tasks was limited ~\citep{wang2023mitfm}. To amend this limitation,~\cite{di2019activity} explored the use of CNNs stacked inception modules as alternatives to RNNs like LSTMs in next-activity prediction of process mining. Their CNN model performs better than RNNs in accuracy and computational expense, with an average accuracy improvement of 12.17\% and halving the training time on real-world data ~\citep{di2019activity}. The findings demonstrate that CNNs are a strong alternative for sequential data tasks, and future work includes predicting the following activities and execution times using advanced inception modules~\citep{di2019activity}. Building on these findings, other researchers further explored LSTMs for next-activity prediction. For example, in ~\citep{musa2023prediction}, LSTMs were applied to event logs to enhance predictions in BPM, with a particular focus on real-world applications and anomaly detection. 

The potential of LSTM-based models was further advanced with methods like Data-Aware Explainable Next Activity Prediction (DENAP)~\citep{aversano2023data}. DENAP combines LSTM networks with Layer-Wise Relevance Propagation (LRP) to provide accurate predictions (80–97\%) alongside interpretability.

Transformer models have only recently been discovered as powerful tools for next activity prediction. Transformers leverage self-attention mechanisms to avoid the limitations of traditional RNNs and LSTMs in capturing long-range dependencies. For instance, The Multi-View information Fusion Method (MiFTMA) employs transformers with multi-view which more precisely capture long-term dependencies compared to the baseline methods~\citep{wang2023mitfm}. Similarly, the Multi-Task Learning Guided Transformer Network (MTLFormer) combines transformers' self-attention with multi-task parallel training. This approach reduces complexity but improves accuracy in long-distance predictions, where relevant information may be spread across distant input parts~\citep{wang2023mtlformer}.
In~\citep{bukhsh2021processtransformer}, authors proposed ProcessTransformer, a transformer-based model capable of learning high-level representations from event logs with minimal preprocessing. The approach surpasses 80\% accuracy in next-activity prediction across nine datasets, improving traditional baselines by capturing long-range dependencies without the need for recurrence. Remarkably, ProcessTransformer demonstrates strong performance even with no context and attribute-aware model.

However, existing transformer-based models (\textit{i.e.,} MiFTMA and ProcessTransformer) employ a sliding window approach, segmenting traces into fixed-length k-prefixes for processing. With this approach, they failed to consider long-term process behavior, and they mostly relied on the last recent behavior of the process. To address this, our proposed DAW-Transformer prepares sequences for each attribute using an extended sliding window, providing a dynamic mechanism to effectively capture and incorporate long sequences into the model. By incorporating all historical events, our method ensures a more comprehensive understanding of the process, improving predictive accuracy.

\section{Preliminaries}
This section introduces foundational concepts relevant to process entropy and process mining.
\subsection{Event log}
Event logs record data about various event types and their timestamps, typically collected during the operation of modern industrial systems and machines~\citep{huang2021deep}. These logs are valuable for analyzing and anticipating critical events, enabling proactive responses that improve system efficiency and reliability~\citep{huang2021deep}. Each event log consists of three main components: \textbf{CaseID}, \textbf{Activities}, and \textbf{Timestamps}.


\subsection{Process Entropy}
The entropy of business process models is a measure of quantifying the uncertainty of process execution ~\citep{jung2008measuring}. Systems characterized by high variability and uncertainty struggle to execute precise planning and scheduling, leading to the wastage of human and system resources. In information theory, information uncertainty is typically quantified by information entropy, commonly known as Shannon’s entropy ~\citep{jung2008measuring}. It is defined as:
\begin{equation}
H(X) = - \sum_{i=1}^n p(x_i) \log(p(x_i))
\end{equation}
In this expression, \( X \) represents a discrete random variable that can assume possible values \( x_1, x_2, \ldots, x_n \) with corresponding probabilities \( p(x_1), p(x_2), \ldots, p(x_n) \). For \( 1 \leq i \leq n \), the probabilities satisfy \( p(x_i) \geq 0 \) and $\sum_{i=1}^n p(x_i) = 1$.

% \subsection{CNN-LSTM/ BiLSTM}
% CNN-LSTM is a hybrid deep learning architecture combining Convolutional Neural Networks (CNN) with Long Short-Term Memory (LSTM) networks. The CNN extracts spatial features from input data, acting as an encoder, while the LSTM models temporal relationships within sequences as a decoder~\citep{muhammad2022cnn}.  BiLSTM further enhances temporal modelling by capturing relationships in both forward and backward directions~\citep{li2024improved}.

% \subsection{XGBoost}
% XGBoost is a robust tree-boosting algorithm widely used for high-performance machine learning tasks. It introduces key innovations such as a sparsity-aware algorithm, a weighted quantile sketch for approximate tree learning, and out-of-core computation for large datasets. Additionally, it optimizes cache access, compression, and sharding while providing built-in support for missing values and parallel processing~\citep{chen2016xgboost}.

% \subsection{Decision tree}
% The decision tree method is an innovative tool widely used for classification, prediction, and data analysis in different areas. Its key strengths include simplifying complex relationships, being easy to interpret, handling missing data without imputation or transformation, and robustness to outliers. However, decision trees can suffer from overfitting and underfitting, especially with small datasets~\citep{song2015decision}.
 

% \subsection{Random Forest}
% Random forests have an ensemble learning algorithm that employs multiple decision trees on independently sampled random data and subsets of features to enhance accuracy in classification and regression tasks. Overfitting and robustness in dealing with noisy datasets is avoided through averaging or voting between the trees. The most important factors in accuracy include individual tree strength and diversity between them, and these are attained through a random feature selection ~\citep{breiman2001random}.


\section{Methodology}
This section presents the details of the DAW-Transformer and the proposed next activity prediction. This method in this study aim to predict the next activity in the most efficient and interpretable way that considers all the attributes over time. In this section, we will first provide a detailed discussion of the DAW-Transformer, covering input data preprocessing, the multi-head attention transformer, and evaluation details. We will then analyze the evaluation results, and their confusion matrices, and compare process entropy with model accuracy. This comparison helps determine the most suitable machine learning model for each specific dataset.

% \subsection{DAW-Transformer}
The DAW-Transformer integrates multiple attributes from event logs, allowing for model training over all relevant data perspectives. Existing works focused on using just sequence of activities or timestamp-realted attributes. Unlike traditional approaches that rely solely on activities, in contrast,  this one utilizes all important features to improve prediction accuracy.

\subsection{Data Preprocessing}
Each event log consists of several attributes such as activity, timestamp, case ID, and context-specific features (\textit{e.g.,} resource). During preprocessing, categorical attributes are encoded, and all data is standardized to ensure compatibility with machine learning algorithms. 

In event logs, it is possible to identify multiple cases. A case is characterized as a sequence of events, commonly termed a ``trace" in the literature. Let T denote a trace, represented as \( T = \langle e_1, e_2, e_3, \ldots, e_n \rangle \), where each \( e_i \) signifies a recorded event. Each event \( e_i \) is linked to multiple attributes, the quantity of which may differ based on the particular event log under examination~\citep{bolt2017finding}.

For each case, activities with the exact case number were extracted to create unique sequences, which were prepared for processing in the models. The datasets were split into 80\% training and 20\% validation for model optimization, with the predictive accuracy of the selected models evaluated on a separate test dataset for the next activity prediction.

To enable sequences with variable lengths, padding is performed by inserting appropriate values (\textit{e.g.,} zero values) in shorter sequences, effectively bringing them to a level with the longest sequence in terms of length. This helps in having uniform dimensions for model input and enables effective learning for all cases.

%Each event within the dataset comprises multiple attributes, including activity, timestamp, case ID, and context-specific features such as age and location. These attributes carry varying levels of importance in predicting the next activity. All relevant attributes are carefully prepared for the model to ensure no critical data is overlooked and to enhance prediction accuracy.

%The preprocessing begins by cleaning data. In this phase, every dataset is checked for Null value, data type, etc. Then categorical attributes are encoded and scaled to numerical features to standardize the data. This standardization step is crucial to ensure optimal performance of the machine learning algorithms. 

%Event logs are then grouped by case ID, resulting in a set of unique sequences for each case. Each sequence comprises a series of activities, each associated with its corresponding attributes. For each step within a sequence, the model extracts features from all preceding steps. The subsequent activity in the sequence serves as the target variable for the model to predict. These extracted features and their corresponding target activities are stored in separate lists for subsequent model training.

%To address the varying lengths of these sequences, padding is applied. The shortest sequences are padded with appropriate values (e.g., zeros) to match the length of the longest sequence in the dataset. This padding step ensures consistent input dimensions for the model and facilitates efficient learning across all cases.

%The training process begins with the first activity and its associated attributes, with the second activity serving as the initial target. In subsequent steps, the model leverages information from all preceding activities and their attributes to predict the next activity within the sequence.

\subsection{Multi-Feature Embedding and Position Encoding}
This component aims to comprehensively represent each sequence by embedding categorical and numerical features while incorporating positional encoding to capture the order of events. Embedding is crucial for the model to understand the relationships between categorical and numerical features and their temporal evolution. Positional encoding is significant as the attention mechanism lacks awareness of the sequence order. By assigning a specific position to each, the model can naturally understand the progression and order of sequence and in turn, have a deeper grasp of temporal dependencies~\citep{vaswani2017attention}.

\subsection{Multi-Transformer}
The transformer encoder block is central to this model, enabling the integration of numerical data for prediction. This block begins by applying multi-head self-attention, which captures the relationships within each sequence. The attention mechanism is computed as follows: 
\begin{equation}
text{Attention}(Q, K, V) = \text{softmax}\left(\frac{Q K^T}{\sqrt{d_k}}\right) V 
\end{equation}

$V$, with $Q$, $K$, and $V$ representing queries, keys, and values, respectively, and $d_k$ the key dimension~\citep{vaswani2017attention}. A residual connection and layer normalization stabilize learning and enhance performance.

Next, a feed-forward network (FFN) introduces non-linearity and transforms the data using the following equation:
\begin{equation}
\text{FFN}(x) = \text{ReLU}(x W_1 + b_1) W_2 + b_2 
\end{equation}
The embeddings are then transformed, flattened, and concatenated with additional numerical features to enhance the input representation ~\citep{vaswani2017attention}.

Finally, a dense output layer with a softmax activation function generates a probability distribution over the prediction classes. This approach effectively integrates sequential and numerical information, ensuring comprehensive and accurate predictions.

%\[
%\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}  %~\citep{burattin2015process}
%\]

\subsection{Process Entropy and Next Activity Prediction }

This work formally assesses activity sequence datasets' process entropy, and in doing so, provides a quantitative measure of process complexity. The algorithm begins with loading a dataset and case organization into traces, with each representing a sequence of specific activities.

Next, the transition probabilities are computed by dividing the frequency of each transition by the total number of transitions. Using these probabilities, the process entropy is calculated as the sum of the negative product of each transition probability and its logarithm, considering only non-zero probabilities. The entropy calculation is a tool for gauging uncertainty and unpredictability in a specific process, and through it, one can gain important insights about its variance and complexity.
% \[
% H(X) = - \sum_{i=1}^n p(x_i) \log(p(x_i))~\citep{shannon1948mathematical}
% \]
\Cref{fig:framework} describes an entropy-driven method for the next activity prediction. In this process entropy stands out as a key consideration in choosing an effective prediction model. By calculating the entropy of a given dataset's activity sequences, the method quantifies the uncertainty and complexity inherent in the process. Datasets with high entropy, indicative of complex and unpredictable behaviors, will be managed by the DAW-Transformer model, known for its accuracy in handling such complex patterns. On the other hand, datasets with low entropy, characterized by their simple and predictable sequence, will be handled through less complex models such as Decision Trees and Random Forest. 
%These models have both interpretability and performance capabilities similar to that of the Transformer model. 
With this adaptive approach, we intend to ensure that the selected prediction model aligns with the specific characteristics of the dataset, optimizing both accuracy and interpretability in the next activity prediction task.
\vspace{10pt}
\begin{figure*}[htbp]
    \centering
 
    \hspace*{0pt}
    \vspace{10pt}
    \includegraphics[width=0.9\textwidth]{Images/Framework.jpg}
    \caption{\centering
    Entropy-Driven Next Activity Prediction: High-entropy datasets use DAW-Transformer for accuracy, while low-entropy datasets leverage Decision Trees for interpretability and comparable performance.}
    \label{fig:framework}
    %\vspace{10pt} % Adjust this value to add space below the figure if needed
\end{figure*}

% Methodology should be given in this section~\cite{3}. Figures are shown in Fig. \ref{fig1}.
% \lipsum[1]

% \begin{algorithm}
%   \KwData{this text}
%   \KwResult{how to write algorithm with \LaTeX2e}
%   initialization\;
%   \While{not at the end of this document}{
%     read current\;
%     \eIf{understand}{
%       go to next section\;
%       current section becomes this one\;
%       }{
%       go back to the beginning of the current section\;
%       }
%     }
%   \caption{How to write algorithms}
% \end{algorithm}

% \lipsum[1] Algorithm 1 as shown below:

\section{Experiments}
This section presents an experimental evaluation of the DAW-Transformer model, comparing its performance with that of CNN-LSTM, CNN-BiLSTM, XGBoost, Decision Trees, and Random Forest. Additionally, the datasets are analyzed based on their process entropy to determine the most suitable model for each dataset. 

 \textbf{Dataset} - Six publicly available datasets, widely used in process mining, were selected for this study. The properties of each dataset are detailed in \textbf{Table~\ref{tab:properties_of_each_dataset_with_entropy}} below.
 
\textbf{Sepsis:} This real-world event log includes events of sepsis cases from a hospital, documented by the ERP (Enterprise Resource Planning) system. Each case in the log represents a patient's journey through the hospital.
\footnote{https://doi.org/10.4121/uuid:915d2bfb-7e84-49ad-a286-dc35f063a460}
%\footnote{[\href{https://doi.org/10.4121/uuid:915d2bfb-7e84-49ad-a286-dc35f063a460}{Sepsis Link}]}

\textbf{Helpdesk:} This dataset comprises events from the ticket management process of the help desk of an Italian software company. Each case in the log commences with a new ticket entry into the ticket management system and concludes with the issue's resolution and the ticket's closing.
\footnote{https://doi.org/10.17632/39bp3vv62t.1}
%\footnote{[\href{https://doi.org/10.17632/39bp3vv62t.1}{Helpdesk Link}]}

\textbf{Road Traffic Fine:} A real-world event log from an information system that manages road traffic violations.
\footnote{https://doi.org/10.4121/uuid:270fd440-1057-4fb9-89a9-b699b47990f5}
%\footnote{[\href{https://doi.org/10.4121/uuid:270fd440-1057-4fb9-89a9-b699b47990f5}{Road Traffic Fine Link}]}

\textbf{BPI Challenge 2020\_Prepaid Travel Costs:} This file includes events associated with prepaid travel expenses for the parent item.
\footnote{https://doi.org/10.4121/uuid:5d2fe5e1-f91f-4a3b-ad9b-9e4126870165}
%\footnote{[\href{https://doi.org/10.4121/uuid:5d2fe5e1-f91f-4a3b-ad9b-9e4126870165}{BPI Challenge 2020\_Prepaid Travel Costs Link}]}

\textbf{BPI Challenge 2020\_Request For Payment:} This dataset includes Payment request events of the travel expense claims.
\footnote{https://doi.org/10.4121/uuid:895b26fb-6f25-46eb-9e48-0dca26fcd030}
%\footnote{[\href{https://doi.org/10.4121/uuid:895b26fb-6f25-46eb-9e48-0dca26fcd030}{BPI Challenge 2020\_Request For Payment Link}]}


\textbf{BPI\_ 2017\_O:} This event log concerns a Dutch financial institution's loan application procedure. All offers made for an accepted application are included in the data in the event log. 
% \footnote{[\href{https://data.4tu.nl/articles/_/12705737/2}{BPI\_ 2017\_O Link}]}
\footnote{https://data.4tu.nl/articles/\_/12705737/2}


\begin{table*}[ht]
    \centering
    \caption{Properties of each dataset and process entropy.}
    \label{tab:properties_of_each_dataset_with_entropy}
    \resizebox{\textwidth}{!}{
    \begin{tabular}{lcccccc}
        \toprule
        \textbf{Dataset} & \textbf{Cases} & \textbf{Events} & \textbf{Activities} & \textbf{\makecell{Avg.\\ Length of Traces}} & \textbf{\makecell{Original\\Process Entropy}} &        \textbf{\makecell{Normalized\\Process Entropy}} \\
        \midrule
        Helpdesk & 3804 & 13710 & 9 & 3.6 & 2.6 & \textbf{0.51} \\
        Road Traffic Fine & 150370 & 561470 & 11 & 3.73 & 2.96 & \textbf{0.58} \\
        BPI\_2020\_Request for Payment & 6886 & 36795 & 18 & 5.34 & 3.21 & \textbf{0.63} \\
        BPI\_2017\_O & 31509 & 193849 & 8 & 6.15 & 3.24 & \textbf{0.64} \\
        BPI\_2020\_Prepaid Travel Cost & 2092 & 18017 & 18 & 8.61 & 3.64 & \textbf{0.72} \\
        Sepsis & 781 & 9165 & 15 & 11.73 & 5.07 & \textbf{1} \\
        \bottomrule
    \end{tabular}
    }
\end{table*}


Based on the original process entropy values in Table~\ref{tab:properties_of_each_dataset_with_entropy}, the Sepsis dataset (5.07) is classified as high entropy. In contrast, the Helpdesk (2.6) and Road Traffic Fine (2.96) datasets fall into the low entropy. The remaining datasets (between 3 and 5) are classified as medium entropy. The normalized process entropy column provides a relative comparison, with Sepsis normalized to 1 and others scaled accordingly.

\section{Results and Discussion}

Various ML methods are employed alongside the proposed DAW-Transformer. For the Sepsis dataset, a CNN-BiLSTM model was used, with carefully tuned hyperparameters to enhance performance, as shown in~\Cref{tab:model_hyperparameters}. Key parameters included an initial filter size of 64 for the first convolutional layer, which progressively increased to 256 in subsequent layers, facilitating the extraction of complex features. To counteract overfitting, dropout values of 0.4 and 0.5 were added. To extract temporal relations in the sequence, a 400-unit bidirectional LSTM layer was added. The Adam optimizer with a learning rate of 0.001 was employed, and the model was trained over 300 epochs with a batch size of 32; early stopping and a learning rate scheduler were used to refine the training process and enhance generalization. In ~\Cref{tab:hyperparameters} hyperparameters for the DAW Transformer Model on the Sepsis Dataset are shown.
 \begin{table}[ht]
    \centering
    \caption{Hyperparameters for DAW Transformer Model on sepsis dataset.}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{Hyperparameter}                       & \textbf{Value} \\ \midrule
        Embedding dimension (embed\_dim)            & 256            \\
        Number of Transformer heads (num\_heads)     & 8              \\
        Feed-forward dimension (ff\_dim)             & 256            \\
        Optimizer                                     & Adam           \\
        Batch size                                    & 2              \\
        Number of epochs                              & 50             \\
        Validation split                              & 0.2            \\ \bottomrule
    \end{tabular}
    \label{tab:hyperparameters}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Hyperparameters for CNN-LSTM model on sepsis dataset.}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}l|l|l|l|l|l@{}}
        \toprule
        \textbf{Hyperparameter}        & \textbf{Value} & \textbf{Hyperparameter}        & \textbf{Value} & \textbf{Hyperparameter}        & \textbf{Value} \\ \midrule
        Filters in 1st Conv Layer      & 64             & Optimizer                      & Adam           & Dropout rate after 1st Conv    & 0.4            \\
        Kernel size in 1st Conv Layer  & 3              & Learning rate                  & 0.001          & Filters in 2nd Conv Layer      & 128            \\
        Pool size in 1st Max Pooling   & 1              & Batch size                     & 32             & Kernel size in 2nd Conv Layer  & 3              \\
        Dropout rate after 1st Conv    & 0.4            & Number of epochs               & 300            & Pool size in 2nd Max Pooling   & 1              \\
        Filters in 2nd Conv Layer      & 128            & Validation split               & 0.2            & Dropout rate after 2nd Conv    & 0.5            \\
        Kernel size in 2nd Conv Layer  & 3              & Early stopping patience        & 30             & Filters in 3rd Conv Layer      & 256            \\
        Pool size in 2nd Max Pooling   & 1              & Learning rate scheduler patience & 10            & Kernel size in 3rd Conv Layer  & 3              \\
        Dropout rate after 2nd Conv    & 0.5            & Learning rate scheduler factor & 0.5            & Pool size in 3rd Max Pooling   & 1              \\
        Filters in 3rd Conv Layer      & 256            & Learning rate scheduler min lr & 1e-6           & Dropout rate after 3rd Conv    & 0.5            \\
        Units in LSTM Layer            & 400            & Output Layer activation        & Softmax        & Units in Dense Layer           & 100            \\
        L2 Regularization in Dense     & 0.02           & Dropout rate after Dense Layer & 0.6            &                                &                \\ \bottomrule
    \end{tabular}%
    }
    \label{tab:model_hyperparameters}
\end{table}

The evaluation results, summarized in~\Cref{table:accuracy_comparison}, highlight the effectiveness of different models across various datasets. It illustrates how these models handle dataset features, including complexity and variation. Additionally, the results indicate that considering the entropy of each dataset can have diverse impacts. In other words, Sepsis is a complex dataset that varies with patient conditions, and it is challenging for less complex models to work effectively with it. With its ability to capture intricate long-range dependencies and complex interactions between various patient factors, the DAW-Transformer demonstrated superior performance, achieving an accuracy of 70.14\% . In contrast, simpler models like Random Forest and Decision Tree struggled to effectively model the intricate dynamics of the Sepsis data, achieving lower accuracies of 59.81\% and 58.86\%, respectively. This indicates the crucial role of advanced architectures in effectively handling the challenges posed by high-entropy datasets.




\begin{table*}[htbp]
\centering
\caption{Model accuracy across datasets.}
\label{table:accuracy_comparison}
\resizebox{\textwidth}{!}{%
\begin{tabular}{@{}lcccccccc@{}}
\toprule
\textbf{Dataset} & \textbf{CNN-LSTM} & \textbf{CNN-BiLSTM} & \textbf{\makecell{DAW\\-Transformer}} & \textbf{\makecell{Multi-Transformer\\limited window}} & \textbf{XGBoost} & \textbf{Random Forest} & \textbf{Decision Tree} \\ \midrule
Helpdesk & 94.15\% & 94.25\% & 94.10\% & 60.89\% & \textbf{94.35\%} & 94.33\% & 94.30\% \\
Road Traffic Fine & 92.24\% & 92.25\% & 92.36\% & 81.37\% & 92.28\% & \textbf{99.71\%} & 99.69\% \\
BPI\_2020\_Request\_for\_payment & 94.96\% & 94.98\% & 96.09\% & \textbf{97.18\%} & 96.02\% & 96.08\% & 96.09\% \\
BPI\_2017\_O & 96.80\% & 96.84\% & 96.90\% & 70.98\% & 96.17\% & \textbf{96.93\%} & 96.92\% \\
BPI\_2020\_Prepaid\_travel\_cost & 91.50\% & 91.97\% & 92.38\% & 87.87\% & 91.90\% & 92.45\% & \textbf{92.55\%} \\
Sepsis & 60.31\% & 60.63\% & \textbf{70.14\%} & 65.45\% & 62.12\% & 59.81\% & 58.86\% \\ \bottomrule
\end{tabular}%
}
\end{table*}


In contrast to high-entropy datasets, for low-entropy datasets such as Road Traffic Fine, traditional ML models like Random Forest (99.71\%) and Decision Tree (99.69\%) performed on par with the DAW-Transformer (92.36\%). Simpler models can be competitive or even superior on low-entropy datasets. In this situation, it is better to use simple and more interpretable models which are transparent and faster and also need fewer sources. This finding supports the entropy-driven next activity prediction strategy, which tailors model selection based on dataset entropy to balance performance and interpretability.

For a better understanding of this method, confusion matrices for one high-entropy dataset (Sepsis) and one low-entropy dataset (Road Traffic Fine) are shown in Figure \ref{fig:side_by_side_figures} and Figure \ref{fig:side_by_side_figures2}. In Figure \ref{fig:side_by_side_figures}, the confusion matrices for the Sepsis dataset highlight the comparison between the DAW Transformer and Random Forest methods. These matrices have been normalized based on each row in the True label. The DAW Transformer confusion matrix exhibits a higher density along the diagonal and fewer false predictions outside it, indicating better classification performance. In contrast, the Random Forest confusion matrix (Figure \ref{fig:second_figure}) shows more false predictions, reflecting a lower classification accuracy. This is further supported by the calculated confusion matrix entropy value of 1.30 for the Random Forest and 0.92 for the DAW Transformer, a 41\% improvement. Having a lower value for entropy for the DAW Transformer signifies less scattering and a concentrated distribution of predictions regarding actual labels. These results clearly demonstrate the superior performance of the DAW Transformer, particularly for high-entropy datasets, and it is effective in dealing with challenging classification issues.

Figure \ref{fig:side_by_side_figures2} presents the normalized confusion matrices for the Road Traffic Fine dataset, comparing the performance of the DAW Transformer and Random Forest classifiers. This dataset is both low-entropy and imbalanced, with the class distribution shown in Table~\ref{tab:class_distribution}. To correct the class imbalance, under-sampling was performed such that all classes have an equivalent number of examples to the least represented one \citep{vijay2023deep}. Before under-sampling, the confusion matrix for the DAW Transformer (Figure \ref{fig:first_without_sampling_figure_Traffic}) exhibits significant misclassifications, with most predictions concentrated in a single class. After under-sampling (Figure \ref{fig:confusion_matrix_second_under_sampling_Traffic}), the confusion matrix becomes more diagonal, indicating improved classification performance.

\begin{table}[ht]
\centering
\caption{Class distribution of original road traffic fine.}
\small
\begin{tabular}{p{6cm}r p{6cm}r}
\toprule
\textbf{Classes} & \textbf{Count} & \textbf{Classes} & \textbf{Count} \\
\midrule
Create Fine                          & 150,370 & Send for Credit Collection           & 59,013 \\
Send Fine                            & 103,987 & Insert Date Appeal to Prefecture     & 4,188  \\
Insert Fine Notification             & 79,860  & Send Appeal to Prefecture            & 4,141  \\
Add penalty                          & 79,860  & Receive Result Appeal from Prefecture & 999    \\
Payment                              & 77,601  & Notify Result Appeal to Offender      & 896    \\
                                     &         & Appeal to Judge                       & 555    \\
\bottomrule
\end{tabular}
\label{tab:class_distribution}
\end{table}
%In contrast, the DAW Transformer matrix (Figure \ref{fig:confusion_matrix_first_Traffic}) reveals more off-diagonal elements, highlighting its higher rate of false predictions and comparatively lower accuracy.
In contrast, the Random Forest classifier was trained on the original dataset without any sampling. Its confusion matrix (Figure \ref{fig:second_figure_Traffic}) shows a higher density along the diagonal, signifying better classification accuracy and fewer misclassifications. Additionally, the confusion matrix entropy values further validate these observations: the DAW Transformer (with under-sampling) yields an entropy of 0.35, whereas Random Forest achieves a lower entropy of 0.12—a 191\% improvement—demonstrating the superior performance of the Random Forest model. Under-sampling reduces the majority class by removing instances and is highly likely to lose useful information, while other methods, such as random forest, are more reliable \citep{more2017review}.

According to the results, Random Forest outperforms DAW-Transformer when the datasets have low entropy. Random Forest adeptly combines numerous decision trees to discern prevailing patterns in low-entropy data while decreasing the risk of overfitting. Also, Random Forest is computation-
extremely inexpensive and interpretable, making it a practical choice for datasets with more straightforward trends, such as
Road Traffic Fine. Experimental proof, such as reduced confusion matrix entropy values and enhanced accuracy in confusion matrices, confirms the superior performance of Random Forest.

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/Confusion_M_Sepsis_Random_Forest_3.png}
        \caption{Matrix for sepsis dataset using Random Forest.}
        \label{fig:second_figure}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Images/Confusion_M_Sepsis_ALLFusion_Transformer_3.png}
        \caption{Matrix for sepsis dataset using DAW Transformer.}
        \label{fig:confusion_matrix_first}
    \end{subfigure}
    \vspace{10pt} % Adjusted spacing
    \caption{\centering
        Confusion matrix for the high-entropy sepsis dataset, 
        demonstrating improved performance with the DAW Transformer model.
    }
    \label{fig:side_by_side_figures}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%222222

\begin{figure*}[ht!]
    \centering
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Confusion_Matrix_Trafic_Fine_Transformer_Normal_without_sampling.png}
        \caption{Matrix for road traffic fine dataset using DAW-Transformer without under-sampling.}
        \label{fig:first_without_sampling_figure_Traffic}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Confusion_Matrix_Trafic_Fine_Transformer_Normal_under_sampling_summery.png}
        \caption{Matrix for road traffic fine dataset using DAW-Transformer with under-sampling.}
        \label{fig:confusion_matrix_second_under_sampling_Traffic}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.325\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Images/Confusion_Matrix_Trafic_Fine_RandomForest_Normal-summery.png}
        \caption{Matrix for road traffic fine dataset using Random Forest without under-sampling.}
        \label{fig:second_figure_Traffic}
    \end{subfigure}
    \centering
    \\[10pt]
    \caption{\centering
        Confusion matrix for the low-entropy road traffic fine dataset,
        demonstrating improved performance with the Random Forest model.
    }
    \label{fig:side_by_side_figures2}
\end{figure*}




% \begin{figure*}[h!]
%     \centering
%     %\vspace{10pt} % Adjust this value to increase/decrease the space before the caption
%     \hspace*{-50pt} % Adjust this value to move the image to the left
%     \vspace{10pt}
%     \includegraphics[width=0.7\textwidth]{Images/Confusion_M_Sepsis_DAW_Transformer_2.png}
%     \label{fig:confusion_matrix}
%     \vspace{10pt} % Adjust this value to add space below the figure if needed
%     \caption{Confusion Matrix of True Next Activity vs. Predicted Next Activity for Sepsis Dataset}
% \end{figure*}


% \begin{figure*}[h!]
%     \centering
%     %\vspace{10pt} % Adjust this value to increase/decrease the space before the caption
%     \hspace*{-50pt} % Adjust this value to move the image to the left
%     \vspace{10pt}
%     \includegraphics[width=0.9\textwidth]{Images/Confusion_Matrix_Trafic_Fine.png}
%     \label{fig:confusion_matrix}
%     \vspace{10pt} % Adjust this value to add space below the figure if needed
%     \caption{Confusion Matrix of True Next Activity vs. Predicted Next Activity for Traffic Fine Dataset_____ This data set has a low entropy, and ``Add penalty" is the most frequent activity. To solve this problem, we did under-sampling on the data set and trained it on the new data set.}
% \end{figure*}



Our entropy-driven next activity prediction, which is based on an evaluation of the dataset complexity and uncertainty using the process entropy, showed a clearly visible relation of accuracy to entropy levels. High entropy indicates greater complexity and variability, as shown in \textbf{Table~\ref{tab:properties_of_each_dataset_with_entropy}}. For low-entropy datasets such as Helpdesk, Road Traffic Fine, BPI\_2020\_Prepaid Travel Cost, BPI\_2020\_Request for Payment, and BPI\_2017\_O, the accuracy across different algorithms was uniformly high. More straightforward and interpretable models, such as Decision Trees and Random Forests, were more suitable in these cases due to their transparency and lower computational cost. Moreover, their interpretability ensures efficient decision-making for stakeholders by providing clear and actionable insights. 


These results validate the applicability of our proposed method in real-world BPM scenarios, enabling dynamic model selection based on dataset characteristics to achieve a balance between accuracy and interpretability, effectively addressing diverse operational needs.

%\textbf{Limitations}: This study is subject to certain limitations. The evaluation was conducted on a specific set of datasets, and the findings may not be generalizable to all possible process scenarios. Further research is warranted to investigate the applicability of these findings across a wider range of domains and process types.


%%Conversely, for high-entropy datasets like Sepsis, the DAW-Transformer outperformed other models by a significant margin, achieving an accuracy of 70.14\%, while CNN-BiLSTM and Random Forest achieved 60.63\% and 59.81\%, respectively. This highlights its capacity to handle complex, attribute-rich datasets effectively. These results validate PANAP's approach to aligning model complexity with dataset entropy, optimizing predictive performance and interpretability based on dataset characteristics.

%Future work could focus on optimizing the DAW-Transformer to reduce the computational overhead and explore hybrid approaches that combine the interpretability of traditional algorithms with the predictive power of the transformer.

%\documentclass[twocolumn]{article}
%\usepackage{booktabs}
%\usepackage{graphicx} % Required for \resizebox

%\begin{document}

%\usepackage{graphicx} % Add this to your preamble



%\usepackage{graphicx}
%\usepackage{caption}

\section{Conclusions}
This paper presents the entropy-driven approach for optimizing next-activity prediction in business process management (BPM) by leveraging process entropy to guide ML-based model selection. This method addresses the trade-off between predictive performance and interpretability by aligning model complexity with the inherent uncertainty of the process.

For high-entropy datasets, the DAW-Transformer, a powerful multi-head attention-based model, effectively integrated all relevant event log attributes to enhance prediction accuracy with a dynamic windows which consider all the eventlog for each case. Experimental evaluations on six public datasets confirmed its effectiveness, particularly for high-entropy datasets like the Sepsis dataset, where it achieved a 70.14\% accuracy— a 9.51\% improvement over CNN-BiLSTM, a 4.69\% over Limited window Multi-Transformers, and a 3.07\% improvement over the literature's best deep learning model (CNN-LSTM-SAtt).%~\cite{1}.

In datasets characterized by low entropy, such as the Road Traffic Fine dataset, traditional machine learning models, including Random Forest (accuracy: 99.71\%) and Decision Tree (accuracy: 99.69\%), either outperformed or achieved comparable performance to deep learning models, with the DAW-Transformer yielding an accuracy of 92.36\%. The subsequent experiment focused on handling imbalanced low-entropy datasets highlighted that, prior to performing under-sampling, the DAW-Transformer faced challenges related to class imbalance, resulting in several misclassifications. Following the application of under-sampling, classification performance improved; however, Random Forest still exhibited a lower confusion matrix entropy (0.12) compared to the DAW-Transformer (0.35), demonstrating a 191\% improvement. This demonstrates that while under-sampling can be a convenient approach for balancing classes, it may not always be the most effective solution, as it risks discarding valuable information. In contrast, methods such as Random Forest are more adept at handling imbalanced distributions, and preserving important data while improving model performance. These results highlight how well interpretable models can be accurate while still being transparent for informed decision-making in less complex settings.

%By combining predictive accuracy, interpretability, and process awareness, PANAP offers a robust and adaptable solution for next-activity prediction in diverse BPM contexts. This work advances predictive modelling in BPM and provides practical insights for resource allocation and process optimization. Future research could extend the Process-Aware Next Activity Prediction to evaluate its effectiveness on more complex and dynamic datasets.

% By providing a principled approach to model selection, our method has the potential to significantly improve the effectiveness and efficiency of business process management in various industries. This research advances predictive modeling in BPM and provides valuable insights for informed decision-making, resource allocation, and process optimization.

This approach necessitates a comprehensive understanding of entropy and requires users to evaluate trade-offs among entropy, accuracy, cost, and resource utilization, thereby restricting its accessibility to experts. To mitigate this limitation, future research should investigate the development of a more autonomous and automated framework that accounts for various parameters, such as dataset characteristics, computational costs, and resource constraints, thereby minimizing the need for expert intervention.

%Additionally, the entropy-driven model selection may not be suitable for highly sensitive fields, such as medical treatment due to the priority of accuracy over time, cost, and resources. %In that case, our proposed method can guaranty high accuracy for the next activity prediction task.

%Future research directions include extending the entropy-driven approach for the next activity prediction to assess accuracy and efficiency before investing in and implementing complex algorithms such as transformers. With this method, businesses can understand their potential results before committing time, effort, and resources. 

\section{Acknowledgment}
We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant No. RGPIN-2023-05408.


\bibliographystyle{agsm.bst}

\bibliography{main}


\end{document}


