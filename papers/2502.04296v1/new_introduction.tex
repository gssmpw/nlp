\section{Introduction}
\label{sec:intro}
Scaling robot learning is bottlenecked by i) large amounts of high-quality data for training; ii) real-time and high-fidelity evaluation. This is in stark contrast with other domains such as natural language processing \cite{radford2019language,openai2023gpt4} and computer vision \cite{kirillov2023segment,videoworldsimulators2024} where abundant training data are available on the Internet and evaluation can be simply performed online with held-out data.

One compelling solution to these bottlenecks is generative modeling \cite{videoworldsimulators2024,1X_Technologies_1X_World_Model_2024}, which aims to learn the full dynamics and simulate the world. For training, it can produce an infinite amount of in-distribution data; for evaluation, it can potentially generate reasonable physical interactions without real-world deployment. However, building useful generative models is nontrivial. It has to be \emph{general} to handle various setups across embodiments, domains, and tasks rather than specific settings \cite{bruce2024genie,valevski2024diffusionmodelsrealtimegame,alonso2024diffusion}; it's also desirable to be \emph{efficient}, so as to deal with the real-time interactions from the policies~\cite{zhu2024irasim,yang2023learning} necessary for robotic applications.


\begin{figure}
\centering
\includegraphics[width=\linewidth]{figures/framework_figure_xinlei2.pdf}
\caption{\label{fig:frame}
\textbf{Action-Video Dynamics Model from Heterogeneous Robot Interactions.} \ourshort utilizes heterogeneous datasets comprising over 3 million trajectories (videos) from 40 distinct embodiments to pre-train a full dynamics model with next-set-of-token predictions using masked autoregression. After pre-training, the resulting action-video dynamics model is versatile, supporting applications such as video simulation, policy evaluation, synthetic data generation, and direct adoption as an imitation policy.}
\end{figure}

Building a general solution across embodiments, domains, and tasks at scale requires handling the \textit{action heterogeneity} in robotics. In particular, different robots use different action spaces, action frequencies, and action horizons for their specific tasks. 
To this end, we build on the recent idea that aligns such heterogeneity into the shared latent space \cite{wang2024scaling,doshi2024scaling,team2024octo} of action-video dynamics generation. To maximize the generality of the framework (\cref{fig:frame}), the network is modularized such that after pre-training, any new embodiment only requires training a small action encoder (``stem'') and action decoder (``head'') from scratch.




Beyond pure policy learning~\cite{wang2024scaling}, our full-dynamics setting also requires modeling of observations, which are typically formatted as sequences of images, or videos. While state-of-the-art, diffusion-based video modeling \cite{zhu2024irasim,videoworldsimulators2024,rigter2024avid} has achieved impressive visual quality, such frameworks are inefficient for real-time applications like robotics due to their need for extensive iterations over the entire sequence at each generation step. In contrast, we build on recent advances in autoregressive modeling for vision \cite{li2024autoregressive,liu2024mardini,tian2024visual}, which offer a more efficient alternative while maintaining high generation quality. In particular, we leverage masked autoregression \cite{chang2022maskgit,li2024autoregressive} for action-video dynamics and explore two variants to trade off speed \vs fidelity for videos: the discrete variant which generates vector-quantized (VQ) tokens at a high speed, and the continuous one which better preserves visual fidelity.


With these insights on Heterogeneity (\texttt{H}) on actions and Masked Autoregression (\texttt{MA}) on video dynamics, we introduce \ourshort, a masked autoregression framework for \textit{action-video dynamics} (AVD) over heterogeneous robotic data. The versatility of this unmasking architecture handles several core problems in robotics, including world models and end-to-end policies (\cref{fig:dynamics}). These problems can be framed as dynamic models using sequence modeling, enabling the joint generation of observations and actions \cite{chen2021decision, radosavovic2024humanoid}. 



\ourshort is shown to scale across embodiments, trajectories, and model sizes in the heterogeneous pre-training mixtures. In particular, we study the scaling behaviors measured by visual fidelity and action controllability. The model is pre-trained with over 3 million trajectories (videos) with action labels and a single \ourshort can generate video across a wide range of 40 embodiment datasets from 2-DoF action space to 28-DoF action space. 


The main use case of \ourshort is the first high-fidelity and real-time video robotic simulator (\href{https://liruiw.github.io/hma/hma_demo}{example}). Our generation achieves better visual fidelity and controllability than the previous state-of-the-art \cite{zhu2024irasim} with \textbf{15$\times$ faster} inference latency, enabling real-time interactions \cite{lynch2023interactive}. Because \ourshort autoregressively models the video and action sequences in interaction data, it can also be used to generate trajectories over 100 frames (over 30 seconds) stably in various robotic applications. On simulation benchmarks \cite{robomimic2021}, \ourshort is used to evaluate policies as a high-fidelity simulator and to generate 90\% synthetic trajectory data for improving policy performance to match ground truth data. The learned dynamic models can also be used as an imitation policy to predict actions. We hope that \ourshort can shed
some light on learning action-video dynamic models in a unified framework from heterogeneous data. 


