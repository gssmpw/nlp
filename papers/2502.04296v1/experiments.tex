

\section{Pre-Training Experiments\label{sec:experiment}}

We first discuss the implementation details below for training \ourshort, including datasets, models, and evaluation metrics.
\begin{figure*}[t]
\centering
\includegraphics[width=.95\linewidth]{figures/video_generation.pdf}
\caption{\label{fig:video_results}\textbf{Pre-trained Video Model Generation.} We show that a single unified \ourshort model can generate realistic (left 3 columns) and diverse (right 3 columns) videos across multiple embodiment datasets with heterogeneous action spaces. Each group shows three generated frames from a single sequence.}
\end{figure*}

\begin{figure}
    \centering    \includegraphics[width=\linewidth]{figures/ablation_plot.pdf}
    \caption{\textbf{Ablation on Pre-training Settings and  Architecture.} Under the pre-training setting with VQ tokens, we ablate the video generation performance (visual fidelity measured by perplexity and controllability measured by controllability). (a) We find action-conditioned models outperform passive video models. (b) We compare different action conditioning architectures in the masked autoregression framework. The purple color denotes the best model that we use by default. }
    \label{fig:pretrain_ablation}
\end{figure}


\paragraph{Datasets.} We use multiple domains of embodiment data related to robotic manipulations. In the largest dataset mixture, the 35 real robot datasets contain primarily the Open-X embodiment dataset \cite{o2023open}, and 3 human video datasets contain ego-centric hand motions \cite{grauman2024ego,damen2020epic,damen2022rescaling}  and the 2 simulation datasets contain standard benchmark \cite{robomimic2021,yu2020meta}. We follow the same dataset processing as in previous works \cite{wang2024scaling} for using 2D hand detections as action label proxies.  The full dataset has over 2.5 billion frames and 3 million trajectories. Note that these datasets with action labels usually have different action dimensions and action frequencies.  During training, we apply inverse exponential probability for sampling the dataset of each batch sample. We preprocess videos in these datasets into tokens with fixed resolution ($256\times 256$) using the pre-trained Stable Video Diffusion VAE \cite{blattmann2023stable} with $8\times8$ spatial downsampling for implicit diffusion models and the 1XGPT tokenizer \cite{1X_Technologies_1X_World_Model_2024}, which is a fine-tuned OPEN-MAGVIT2 \cite{luo2024open} tokenizer, with $16\times16$ downsampling for explicit cross-entropy losses. We unify all datasets into 2Hz by adaptively choosing action strides.  For example, a 10Hz dataset will have an action stride of 5 and shrinking into 2Hz action chunks. Under 2Hz frequencies, the context length is in total 12 frames (6 seconds) with 4 frames (2 seconds) as prompts and 8 frames (4 seconds) as predictions, which are sufficient for most closed-loop interactive applications. We choose these hyperparameters for prototyping, but technically any image resolution or video frequencies can be used. The continuous objective also does not require a tokenizer and can directly operate on pixel space.



\paragraph{Model.} Under the standard setup for ablations,  we use the discrete model and VQ tokens with forward dynamics objective (with cross-entropy loss) for the simplicity of training. We use a 32-layer transformer model with dimension $d=256$. The model is trained with 8 V-100 GPUs and batch size 64 for around 60,000 iterations and 2 epochs. We train larger models with up to 64 GPUs.

\paragraph{Metrics.} We measure the performance of the video models via several metrics on the held-out validation datasets with a maximum of 500 examples per dataset and present the average statistics across datasets. When we use auto-regressive models with explicit cross-entropy loss on tokenized outputs, we measure the training objectives for \textit{fidelity} via validation perplexity, which is directly correlated with validation loss and visual fidelity metrics, such as PSNR, SSIM. We use $\Delta$PSNR \cite{bruce2024genie} to measure \textit{controllability}, which is the average difference of PSNR on the last frame computed from ground truth action sequence with PSNR computed from 5 random action sequences. Intuitively, if $\Delta$PSNR is small, then the model predictions are less affected by actions and therefore the video model is not controllable.

\begin{figure*}
    \centering
    \includegraphics[width=\linewidth]{figures/scale_plot.pdf}
    \caption{\textbf{Experiments on Scaling Behaviors of \ourshort.} We observe positive trends in the scaling performance of heterogeneous video models across axes including the number of datasets, number of trajectories, and model sizes. The evaluation metrics on fidelity (perplexity) and controllability ($\Delta$PSNR) are averaged across validation datasets.  }
    \label{fig:pretrain_scaling}
\end{figure*}


\subsection{Action-Video Dynamics under Heterogeneity\label{sec:hma_pretrain}}

In this experiment section, we investigate how well heterogeneous pre-training works for world models. We choose the discrete loss objective in \ourshort for these experiments. In \cref{fig:video_results}, we showcase the generation of our single unified pre-train model \ourshort to handle heterogeneous datasets. 


\paragraph{Dynamic Model Settings.} In this section, we measure the performance of different dynamics model settings, including passive video generation, forward dynamics (action-conditioned video generation), and full dynamics which have an auxiliary task of action predictions. Shown in \cref{fig:pretrain_ablation} (a), low-level action conditioning in the forward dynamics model can improve video generation results. Intuitively, passive video sequences without actions contain less information on the causal physical relationships of these videos and are sometimes hard (or even ambiguous) to capture fine-grained details related to motions. We find full dynamics training outperforms passive dynamics but does not outperform forward dynamics generation. We hypothesize that these results stem from the limitations of VQ tokens, which were originally designed for visual generation tasks rather than for action prediction.


\paragraph{Action Architecture Ablation.} We compare several architectural designs in conditioning action information for video generation. Specifically, we ablate architecture variants including i) modulation, ii) token concatenation, iii) feature addition, and iv) token cross-attention following \cite{peebles2023scalable}. In \cref{fig:pretrain_ablation} (b), we show that the modulation method can outperform other methods.  In particular, token concatenation along the sequence dimension for each frame does not have enough expressiveness and compute on action-conditioning, compared to per-layer modulation. 


\subsection{Scaling Behaviors of \ourshort}
We experiment with the scaling performance of \ourshort along multiple axis. In \cref{fig:pretrain_scaling}, we scale across the number of videos per dataset, scale across the number of datasets, and scale across model sizes. Under the same validation datasets, we observe consistent gains from scaling measured by perplexity and $\Delta$PSNR.  

\paragraph{Scaling Embodiments.} Scaling the number of datasets (\cref{fig:pretrain_scaling} Left) from 5 to 40 datasets strictly increases the dataset heterogeneity and creates more model parameters to handle such action space differences among embodiments. Yet we see that the video prediction performance does not degrade or become unstable. This gives positive signals for joint heterogeneous training with diverse embodiment video datasets. For this experiment, we evaluate the models on the first random subset of 5 datasets to be consistent. 

\paragraph{Scaling Data.} Using 40 datasets, we scale the number of trajectories (videos) per dataset from 10 to $10^6$, equivalent to training on 8 thousand to 3 million trajectories. In  \cref{fig:pretrain_scaling} middle, the slight plateau of the trajectory scaling performance after $10^5$ total trajectories is likely due to the dataset quantity imbalance, as the additional trajectories all come from the largest few datasets. 

\paragraph{Scaling Model.} Under the largest dataset with the maximum number of datasets (40) and maximum trajectories (3 million), we study model scaling of \ourshort. In  \cref{fig:pretrain_scaling} right, we scale the hidden dimension of the Transformer while keeping other parameters constant. This changes the model parameters from 3 million to 400 million (sparse) parameters. We qualitatively observe improved visual qualities and controllability when we increase the model capacity.


\section{Post-Training Applications\label{sec:post}}


After pre-training, we finetune \ourshort to evaluate its performance with limited data and ground truth controllability metrics. In the real-robot action simulator setting,  we demonstrate its speed advantages over the previous state-of-the-art interactive simulator by 15$\times$. We also post-train the model to various downstream robotics use cases including policy evaluation, policy data generation, and dynamic models as policies.

\paragraph{Datasets.} We focus on Robomimic \cite{robomimic2021}, a simulation benchmark with 200 trajectories per task, and Language Table \cite{lynch2023interactive}, a real-world benchmark with 442k trajectories, to study the applications of \ourshort model in simulation and policy learning. Simulation benchmarks are suitable because we can evaluate the ground truth controllability and benchmark against policy methods and policy evaluation. Language Table is suitable for its sheer amounts of trajectory data and focused domain. We use held-out videos to evaluate \ourshort on these two datasets. Since we need to apply interactions in post-training, we use an action stride of 1 and maintain the original action frequencies in the dataset.

\paragraph{Model.} Following the training protocol of \cref{sec:experiment}, we finetune the pre-trained \ourshort with layer dimension 256 on each dataset for 10 epochs. The interactive simulation setting requires the forward dynamics formulation of our model. In \cref{tab:size_fps}, we compare different model sizes of \ourshort with the previous state-of-the-art open-source model IRASim \cite{zhu2024irasim} which uses the DiT \cite{peebles2023scalable} model and action modulation architecture and outperforms VDM and LVDM \cite{he2022latent,
ho2022video} in robotics. Notably, IRASim is not optimized for single-frame interactions, as the default setup predicts video sequences based on action sequences. On \cref{tab:size_fps}, we observe a 15$\times$ model speed improvement (4.44 v.s. 0.28 FPS) even compared to their amortized result, thanks to masked autoregression. Using the diffusion heads, the model can still be faster while maintaining high visual fidelity.

\paragraph{Metrics.} For real-world data, we use model-based metrics such as PSNR \cite{hore2010image} and SSIM \cite{wang2004image}, learning-based metrics such as LPIPS \cite{zhang2018perceptual}, FID \cite{heusel2017gans}, and FVD \cite{unterthiner2018towards} for visual fidelity measurement; and $\Delta$PSNR \cite{bruce2024genie} for controllability. For simulation tasks, since we can access ground truth observations in any state through rendering, we directly compare ground truth against the learned model to measure controllability rather than using proxy $\Delta$PSNR. In particular, we apply perturbations to the action sequences and denote the metrics as PSNR$^*$ and perplexity$^*$ which measure the average sensitivity of the dynamics model to small action perturbations.

\begin{table}[t]
\centering
\small
\tablestyle{5.5pt}{1.2}
\begin{tabular}{lccl}
\textbf{Model}       & \textbf{Method}       & \textbf{Parameters} (M) & \textbf{FPS $\uparrow$}   \\ 
\hline
IRASim-XL   & DiT &  679            & 0.28 \\
IRASim-XL, amortized  & DiT & 679            & 0.58  \\ 

\hline

\ourshort-Base  & MaskGIT &  44             &  22.72 \\
\ourshort-XL & MaskGIT &  679         & 4.38 \\

\ourshort-Base     & MAR &  96          &  4.44   \\
\ourshort-XL    & MAR & 741         &  2.01  \\
\hline
\end{tabular}
\caption{\textbf{Inference Speed.} We measure the per-frame inference speed across 16 frames for various model sizes. The Base model has a model size of around 30M and the XL model has a similar model size as IRASim-XL. The models all use 32-block transformers where the base model has dimensions 256 and the XL models have dimensions 768. Our fastest model of the same size is more than 15$\times$ faster than \cite{zhu2024irasim} because \ourshort does not pass through the full Transformer multiple times (with diffusion modeling) to generate each frame. MAR incurs more parameters than MaskGIT \cite{chang2022maskgit} because of the diffusion heads \cite{li2024autoregressive}.  The amortized result for \cite{zhu2024irasim} comes from averaging over multiple frames. The speeds are all measured on the same hardware setup with RTX-4080 GPU. }
\label{tab:size_fps}
\end{table}

\begin{table}[t]
\centering
\small
\tablestyle{4pt}{1.2}
\begin{tabular}{l|cccccc}
& {\bf PSNR $\uparrow$} & {\bf SSIM $\uparrow$} & {\bf $\Delta$PSNR $\uparrow$} & {\bf LPIPS $\downarrow$} & {\bf FID $\downarrow$} & {\bf FVD $\downarrow$} \\ 
\hline
IRASim & 25.41 & 0.82 & 5.78 & 0.08  & 23.22 & 152.20\\
\ourshort &28.19 & 0.83 & 6.06  &    0.07 & 33.56 & 111.52
\end{tabular}
\caption{\textbf{Comparison with IRASim.} In Language Table Benchmark \cite{lynch2023interactive}, we show that a pre-trained \ourshort-based model (diffusion) is able to achieve better visual qualities and controllability than IRASim while maintaining faster speed and requiring less compute. The results are computed over 200 held-out trajectories.}
\label{table:compare_withirasim}
\end{table}

\subsection{Toward Real-Time Simulation with \ourshort}
\label{exp:finetune_real}
In this section, we illustrate how to use \ourshort as a real-time learned simulation model in robotics and compare it to IRASim \cite{zhu2024irasim} for controllability and visual fidelity.  First, in \cref{tab:finetune_real}, we observe better visual fidelity and controllability by using a pre-trained \ourshort model with discrete image tokens. In  \cref{fig:tokenizer}, we qualitatively compare different tokenizers along with training objectives. We find soft tokens to contain richer visual information in general, but take more space and compute to store and genereate.


In \cref{table:compare_withirasim}, our MAR model achieves better visual fidelity and controllability compared to IRASim \cite{zhu2024irasim} despite using less than 1/8th of the parameters (\cref{tab:size_fps}).   When using a similar number of parameters, \ourshort is significantly faster than the amortized version of \cite{zhu2024irasim}. Our fastest discrete model can run up to 22Hz, which allows for real-time interactions. The speedup compared to full-sequence diffusion comes from the autoregression process and the architecture, which only requires computation on the diffusion heads rather than the full Transformer. Compared to full-sequence diffusion models, autoregressive models learn to predict one frame/action at a time, which is natural for robotic interactions. Qualitatively, we also find interacting with our learned simulators to be more reactive and consistent than \cite{zhu2024irasim}, with only small compounded errors in generation quality when running long autoregressive inferences (e.g. \cref{fig:controllability_comparison}) Given the scaling performance (\eg  \cref{fig:pretrain_scaling}) in \cref{sec:experiment}, we expect training longer with more compute resources to further improve the generation quality. 

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/qualitative_comparison.pdf}
\caption{\label{fig:tokenizer}\textbf{Qualitative Comparisons Between Tokenizers and Models.} Despite longer convergence time, diffusion-based methods (\cref{eq:continuous}) on soft tokens generate better visual quality than on VQ tokens (\cref{eq:discrete}), qualitatively and measured by PSNR.}
\vspace{-2mm}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figures/controllability_figure.pdf}
\caption{\label{fig:controllability_comparison}\textbf{Video Controllability.}  \ourshort can follow user action inputs to generate physically plausible object permanence (top row) and block pushing interactions (bottom row). These video predictions are both at out-of-distribution settings and at a much longer horizon than training (over 100 frames). }
\vspace{-2mm}
\end{figure}

In \cref{fig:controllability_comparison}, we build a simple user interface to test the performance of \ourshort under an out-of-distribution setting and find stable simulation quality. Under real-world compute and inference speed constraints for robotic policy and simulation use cases,  \ourshort with masked autoregression is more suitable than full-sequence diffusion, which can be of order faster to reach real-time speed. 
This showcases the potential of \ourshort for policy evaluation and synthetic data generation in the real world, which can save effort in evaluation and collecting data when scaling robot learning. 

\begin{table}[t]
\centering
\small
\tablestyle{7pt}{1.2}
\begin{tabular}{l|cccc}
& {\bf PSNR $\uparrow$} & {\bf Perplexity $\downarrow$} & {\bf $\Delta$ PSNR $\uparrow$} & {\bf LPIPS $\downarrow$}   \\ 
\hline
\ourshort  & 21.01 &  305.87 & 0.01 &  0.19 \\
\ourshort$^{+}$ &  22.04  & 189.83 & 0.06 &   0.17
\end{tabular}
\caption{\textbf{Real World Finetuning.}  \ourshort$^{+}$ denotes finetuned model based on pre-trained checkpoints while \ourshort trains from scratch on the finetuning data. This experiment uses the discrete loss baseline. }
\label{tab:finetune_real}
\vspace{-5pt}
\end{table}


\subsection{Evaluating \ourshort with Simulator}
In this section, we evaluate the controllability and visual fidelity of \ourshort against the ground truth in simulation. In \cref{table:sim}, we observe that pre-trained models can help with the performance compared to training from scratch. Moreover, through the PSNR$^*$   and Perplexity$^*$ metrics with ground truth, we observe better robustness and less video divergence with pre-training on large amounts of heterogeneous data.   In \cref{fig:sim_app}, we show some interaction examples with the learned video models compared to the ground truth simulation. The experiments show that the learned simulator is reactive despite being finetuned on only 200 trajectories. 

\begin{figure}[t]
    \centering    
    \includegraphics[width=\linewidth]{figures/policy_evaluation.pdf}
\caption{\textbf{Policy Evaluation with \ourshort.} By learning the action-video dynamics over both successful and failed examples, \ourshort can be used to evaluate policies, similar to a traditional simulator \cite{todorov2012mujoco}. The autoregressive horizon at inference time is 10 times more than the training time horizon. }
    \label{fig:sim_app}
\end{figure}

\begin{table}[t]
\centering
\small
\tablestyle{7pt}{1.2}
\begin{tabular}{l|cccc}
& 
{\bf PSNR$\uparrow$} &  {\bf Perplexity$\downarrow$} & {\bf PSNR$^*$$\uparrow$} & {\bf Perplexity$^*$$\downarrow$}    \\ 
\hline
\ourshort& 24.17  & 20.69 & 19.19 & 1193.70\\
\ourshort$^{+}$ & 25.11 &11.82 & 20.20 & 103.01 
\end{tabular}
\caption{\textbf{Simulation Transfer Learning.} We show that  pre-trained \ourshort can help with fine-tuning using cross-entropy losses and diffusion losses jointly. where \ourshort$^{+}$ denotes the finetuned model based on pre-trained checkpoints.}
\label{table:sim}
\end{table}

\subsection{\ourshort for Policy Evaluation}
\label{exp:finetune_sim_evaluation}

In this section, we show that a well-trained video model can be used as a simulator for evaluating policy performance. Note that since a policy can be flawed, a simulator must simulate both succeeded and failed examples, which might come from broader sets of interaction data than high-quality demonstrations, as shown in \cref{fig:sim_app}. Note that learned models for policy evaluation can be used with any particular policy. Specifically, a Diffusion Policy (DP) \cite{chi2023diffusion} is trained on Robomimic Lift \cite{robomimic2021} task with 200 trajectories for 10k iterations with an action horizon of 16. In \cref{table:eval_policy}, we present the evaluation results of four baseline policies, each trained with the same data but differing in training iterations before full convergence. We train \ourshort on the perturbed datasets generated earlier to evaluate groundtruth PSNR.

We evaluate each policy using 50 runs and observe a correlation of evaluation statistics between policies evaluated in the Mujoco Simulator \cite{todorov2012mujoco} and policies evaluated in the learned simulators. 
Note that since rewards and success criteria rely on the ground truth states, we use manual human annotation to determine the evaluation results using learned simulators. This can be done in batches and extending to using foundation models for reward labeling can be future works. There are still failure cases in capturing the physical details of dropping and catching the boxes. But we also see many highly realistic interactions with only limited amounts of data. Notably, even though the training horizon is only 12 steps, the autoregressive horizon of \ourshort at inference time can generalize to over 100 steps.



We also report the time taken to evaluate policies in both ground truth and learned simulators. Each episode in the simulation has a maximum of 100 timesteps and the average simulation time is 32 seconds. This is still slower compared to 9 seconds in the Mujoco simulator, but the potential of simulating deformable, granular objects in cluttered realistic scenes for policy evaluation is promising. 




\begin{table}[t]
\centering
\small
\tablestyle{8pt}{1.2}
\begin{tabular}{l|ccccc|}
\textbf{Policy Evaluator} & 1 & 2 & 3 & 4  \\ 
\hline
{Ground Truth Simulator} & 0.38 & 0.52 & 0.70  & 1.00   \\ 
{\ourshort Simulator} & 0.43 & 0.56 & 0.66 & 0.73  
\end{tabular}
\caption{\textbf{Policy Evaluation Results Across 4 Different Policies.} We observed positive correlations of  the evaluation results  for 4 different policies bewteen the ground truth and learned simulators. The Pearson ratio between evaluations is 0.95.  }
\label{table:eval_policy}
\end{table}


\subsection{Synthetic Data Generation}
We investigate generating synthetic data for policy training using \ourshort. We leverage a dataset of 100 action trajectories to generate synthetic data (video, action) pairs based on a trained \ourshort. 

We evaluate the quality of the generated data by adding different amounts of them to 10 original trajectories. We then train baseline DP policies for 10k iterations and evaluate for 50 trials.
In \cref{table:synthetic_data}, we show policy improvements by training on 90\% of synthetic data in the low-data regime, to the extent that matches the full original data with 100 trajectories. This also leverages \ourshort's ability to generate long-horizon videos without much degradation.  

We conduct similar experiments in the Language Table \cite{lynch2023interactive} benchmark. Due to no access to real robots, we measure the data quality 
by validation learning losses. We use 10 out of 100 
trajectories from the original benchmark. We then replay and render-augment it with \ourshort from 10 to 90 trajectories with image observations generated by our systems. The training pipeline has language and image inputs and we measure the MSE losses. Using a small diffusion policies (17M), we show an improved BC policy performance trained with generated data from HMA.



We plan to explore removing the dependence on action trajectories and initial states. We can also scale up training and generating more unseen trajectories in complex real-world settings. We explored using dynamics models for policies but did not achieve surprising results. The hypothesis is that the perception used to 
predict actions is still not unified with the image generation 
objectives with VQ codes.  One underexplored capability   of autoregression in robotic policies is that a single frame can allow autoregressive generation of longer action sequences beyond the training context, in contrast to full sequence generation policies such as ACT \cite{zhao2023learning} and DP \cite{chi2023diffusion}. 

 
\begin{table}[t]
\centering
\small
\tablestyle{4pt}{1.2}
\begin{tabular}{l|ccccc}
& +0 & +10 & +50  & +90 & original \\ 
\hline
{Success in \cite{robomimic2021}} & 82\% &  90\% & 96\%& 100\% & 100\%   \\
{Validation Loss in \cite{lynch2023interactive}} &  1.72  & 1.16   & 1.09  &  0.88 & 0.87 
\end{tabular}
\caption{\textbf{Synthetic Data for Policy Learning.} We evaluate the quality of generated synthetic data by adding different numbers of generated video trajectories in \cite{robomimic2021} and \cite{lynch2023interactive}, from 10 to 100, to a fixed subset (10 trajectories) of the original data (100 trajectories). We then conduct policy training and evaluation and report the Robomimic success rates (top row) and Language Table validation losses (bottom row).  }
\label{table:synthetic_data}
\end{table}
