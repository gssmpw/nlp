\section{Heterogeneous Masked Autoregression}
\label{sec:method}
\begin{figure}
    \centering    \includegraphics[width=\linewidth]{figures/masking_framework_figure.pdf}
    \caption{\textbf{Dynamics Model.} Masked autoregression in the dynamics model generalizes multiple problem settings including policy learning, forward and passive dynamics, and full dynamics. }
    \label{fig:dynamics}
\end{figure}

\subsection{Dynamic Models}
\label{subsec:dynamics}
Our objective is to learn the dynamics model $f$ on visual observation history $\mathcal{O}_{\text{history}}=\{o_{t-N_{\text{past}}},...,o_{t-1}\}$ and action history $\mathcal{A}_{\text{history}}=\{a_{t-N_{\text{past}}},...,a_{t-1}\}$ to predict the future observations $\mathcal{O}_{\text{future}}=\{o_{t},...,o_{t+N_{\text{future}}}\}$  and actions $\mathcal{A}_{\text{future}}=\{a_{t},...,a_{t+N_{\text{future}}}\}$ where $N_{\text{past}}$ and $N_{\text{future}}$ are hyper-parameters. Using the masking objective in the next section, the model is trained to predict a masked component in this function given other components. In \cref{fig:dynamics}, multiple core problems in decision-making and robotics \cite{bertsekas1995neuro,murray2017mathematical} can be viewed as a subset of this problem: 
full dynamics: $f_{\text{full-dynamics}}  (\mathcal{O}_{\text{history}},\mathcal{A}_{\text{history}})\mapsto (\mathcal{O}_{\text{future}},\mathcal{A}_{\text{future}})$, passive video predictions $f_{\text{passive}}  (\mathcal{O}_{\text{history}})\mapsto \mathcal{O}_{\text{future}}$, 
forward dynamics: $f_{\text{forward-dynamics}}(\mathcal{O}_{\text{history}},\mathcal{A}_{\text{history}})\mapsto \mathcal{O}_{\text{future}}$,  and policy models: $f_{\text{policy}}(\mathcal{O}_{\text{history}},\mathcal{A}_{\text{history}})\mapsto \mathcal{A}_{\text{future}}$ are all subsets of such sequential and generative models. The objective can be applied to datasets with missing labels such as pure video datasets and policy datasets, and can even be extended to inverse dynamics $f_{\text{inverse-dynamics}}  (\mathcal{O}_{\text{history}},\mathcal{O}_{\text{future}})\mapsto \mathcal{A}_{\text{future}}$ if non-causal models are used. 
 

\subsection{Action Heterogeneity}
A world model that understands the physical world should be able to simulate various forms of embodiments and actions. For instance, humans can intuitively understand how other creatures' physical actions would impact the world. Action heterogeneity in robotics includes different action spaces, action frequencies, and action dimensions. For example, a dynamics model with 30Hz frequencies and a 50-DoF joint space for a humanoid is different from one with 5Hz frequencies and a 6-DoF end effector space for a Franka Arm. Since the videos can be unified as a sequence of images of fixed resolutions, we decouple observation and actions to handle the heterogeneity in actions. One particular approach that handles such action heterogeneity without explicit tokenization processing step is HPT~\cite{wang2024scaling}. Specifically, for each domain, we consider 2Hz dynamics frequencies and 12 frame context (6 seconds wallclock time) to balance generation length and compute efficiency, but technically any action frequencies can be used. We chunk the action sequences into a fixed 2Hz to ensure consistency across datasets. For action prediction objectives, we will use different decoder heads to predict actions for different embodiments.


\subsection{Masked Autoregression}
Following the success of language models and image/video models, we use masked autoregression (MAR) \cite{1X_Technologies_1X_World_Model_2024,chang2022maskgit} to generate video predictions and action predictions based on previous video observations and action sequences. Specifically, masked autoregression models the joint distribution as the conditional distribution based on previous generations. It uses the masked autoencoding objective with a random order of tokens at training time. Then it predicts the next-set-of-tokens and
gradually unmasks at inference time \cite{li2024autoregressive}, which is natural in robotic interactions. The joint dynamics probability distribution can be decomposed as:

\begin{align}
p(\mathcal{O}_{\text{history}},\mathcal{O}_{\text{future}},\mathcal{A}_{\text{history}},\mathcal{A}_{\text{future}})=p(X_1,...,X_K) \nonumber \\
   =\Pi_{k=1}^K p(X_k|X_1,...,X_{k-1}),
\end{align}
where $X_k$ can be any (causally) valid masked set of observations and actions. Therefore, it can unify the different dynamic settings mentioned in \cref{subsec:dynamics}. We use a neural network $f_\theta$ parameterized by $\theta$ to learn such a distribution.  Notably, both images and actions are continuous signals, and multiple loss functions can be applied to this. One simple choice is to tokenize the images into discrete tokens and use the raw actions. We then train the network with cross-entropy (CE) loss on image tokens while applying mean-squared-error (MSE) regression losses to the actions. For simplicity, let ${X}=(\hat{o}, \hat{a})$ generated by $\theta$ and $({o}, {a})$ denote the ground truth, the discrete loss on the video vector-quantized (VQ) tokens can be written as:
\begin{equation}
\label{eq:discrete}
    \mathcal{L}_{\text{VQ}}(X;\theta)=MSE(a,\hat{a})+CE(o,\hat{o}).
\end{equation}


Alternatively, we can use a \textbf{separate} denoising diffusion objective that learns to reconstruct $X$ based on a continuous latent $z$, which we also call ``soft tokens'' ~\cite{li2024autoregressive}: 
\begin{equation}
\label{eq:continuous}
    \mathcal{L}_{\text{soft}}(X;\theta)=\lVert \epsilon_a - f(a|t,z) \rVert^2  +\lVert \epsilon_o - f(o|t,z) \rVert^2,
\end{equation}
where $\epsilon_a,\epsilon_o$ are noise vectors sampled from $\mathcal{N}(0,\mathbf{I})$ and $t$ is the timestep of the noise schedule. Note that $z,t$ are separate for action $a$ and video $o$ in practice.

Different from previous video models that use either diffusion \cite{zhu2024irasim,alonso2024diffusion} or autoregression \cite{liu2024mardini,bruce2024genie}, this method combines the efficiency and expressiveness of both approaches to model the joint dynamic models.



\subsection{Model Architecture}
The overall architecture, denoted as  Heterogeneous Masked Autoregression (\ourshort), builds on the generality across embodiments with action heterogeneity, and the efficiency across dynamic settings and discrete/continuous observation tokens with masked autoregression.

In \cref{fig:network}, the network architecture of \ourshort follows the heterogeneous pre-training in HPT \cite{wang2024scaling}, in which we create multiple modules of action inputs (``stem'') and action outputs (``head'') and share the core spatial-temporal transformer (``trunk'') as the dynamic models for pre-training and transferring. The spatial attention runs bi-directionally with the masked video and action tokens and the unmasked tokens, and the temporal attention is causal in predicting tokens in future steps.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/network_figure_xinlei.pdf}
    \caption{\textbf{Network Architecture.} The \ourshort model architecture maps low-level video and action sequences across different embodiments into a shared latent space. For actions, embodiment projectors are activated based on the training sample. The spatial-temporal Transformer produces the output video and action tokens for future frames.   }
    \label{fig:network}
\end{figure}

This model architecture explicitly handles the heterogeneity in action spaces in robotics \cite{wang2024scaling,shazeer2017outrageously}. Specifically, each domain has its own action encoder which has an MLP layer that maps normalized actions of certain horizons into features. A decoder can be a 3-layer diffusion MLP that is trained to regress on the continuous features and/or actions.

Notably, the ability to predict a controllable and high-fidelity future given past observations requires dedicated information streams from the actions. We use modulation \cite{peebles2023scalable} layers for each domain in every Transformer block \cite{shazeer2017outrageously}. To generalize the video model to predict both video and action in the full dynamics task, we also use the token concatenation method for fusing video and action tokens.

\subsection{Training and Inference}

In practice, each frame in the video has 256 tokens and 64 repeated action tokens. The masking has a minimum ratio and follows a cosine schedule to mask more tokens at later steps in the training horizon. We  use Maximal Update Parametrization \cite{yang2022tensor} for scaling with bigger models.  We find a Xavier initialization with gain 0.1 for the action projectors to be useful for training stability.  We use different mask tokens and different decoders for action and video tokens. We ablate with other variants such as cross-attention layers in the Section \ref{sec:experiment}.

At inference time, we append masked tokens to the video sequence and masked tokens to the action sequence up to the maximum horizon whenever needed. The full inference process for the diffusion-based \ourshort contains three nested autoregression procedures across $T$ timesteps in the video time dimension, $M$ unmasking steps in the image patch dimension, and  $N$ diffusion steps for the continuous token generation. Since each step in the nested iterations is fast, the generation process is both high-quality and efficient. For diffusion head, we use DDIM with per-step clipping \cite{song2020denoising} to train with $N_{\text{train}}=1000$ timesteps and test with $N_{\text{test}}=100$ steps. As in standard diffusion works \cite{peebles2023scalable}, we use patch size 2 to reduce context length. This means a $2\times2$ ``patch" of tokens is replaced with a single token, We find $M=2$ unmasking \cite{chang2022maskgit,li2024autoregressive} iterations with random unmasking order  to be sufficient to generate high-quality videos at inference time and iterations across $T$ are not necessary. 

 
