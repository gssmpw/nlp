\section{Related Works}
\label{sec:related_works}

\paragraph{World Models.}
 World models \cite{ha2018world}, or dynamic models \cite{bertsekas1995neuro}, are computer programs that evolve based on agents' behaviors. Different from simulator software, learned dynamic models predict future states or reward functions based on past observations and then apply to model-based reinforcement settings \cite{zhang2021autoregressive,chen2021decision,hansen2023td} or robotics \cite{byravan2017se3,li2018learning,seo2023masked}. Notably, since ground truth states are often unavailable in decision-making, dynamic models need to handle high-dimensional video data and low-dimensional grounded physical actions. Full-sequence diffusion and autoregressive models are two primary approaches for generative tasks across languages, images, and videos~\cite{li2024autoregressive,ye2024latent,kondratyuk2023videopoet,chang2022maskgit,ho2022video}. In particular, 1xGPT~\cite{1X_Technologies_1X_World_Model_2024} uses masked autoregression for video generation and MAR~\cite{li2024autoregressive} applies diffusion losses with masked autoregression for image generation, and Diffuser \cite{janner2022planning} uses diffusion models to jointly model the full state and action sequences in planning. Our work focuses on masked autoregression over full-sequence diffusion \cite{bruce2024genie,valevski2024diffusionmodelsrealtimegame,alonso2024diffusion}, aiming to create efficient and interactive world models.
 
\paragraph{Steering Video Models with Actions.}
Video models \cite{videoworldsimulators2024} can be applied to a wide range of video data including curated videos, human videos, synthetic videos, and robot videos. Interactive video models usually rely on language instructions \cite{yang2023learning}, latent actions \cite{bruce2024genie}, or sketches to guide the video or image models.  However, video controllability is still an issue when \textbf{fine-grained details and low-level motion controls} are  used as prompts \cite{zhu2024irasim,rigter2024avid,alonso2024diffusion}. In particular, IRASim \cite{zhu2024irasim} applies DiT \cite{peebles2023scalable} to several robotic datasets and demonstrates high-quality video simulation qualities. Unlike previous works that apply such models to a single task or embodiment or use natural languages as actions, our work investigates action-conditioned video models across heterogeneous embodied settings and their scaling behaviors.  

\paragraph{Visual Generative Models for Robotics.}
Visual generative models have been explored extensively for robotic applications such as policy learning, planning, and synthetic data generation. Synthesized goals and subgoals \cite{du2023video,nair2018visual} have been used to improve end-to-end manipulation policies. Video predictions such as visual foresight \cite{finn2017deep} have been used to guide policy executions. Video language planning \cite{du2023video} achieves long-horizon planning tasks through model-predictive control and search. In the context of robotics, diffusion methods have been used to augment images 
 \cite{chen2023genaug,yu2023scaling} and 3D generative methods have been used to generate synthetic data from novel views \cite{zhou2023nerf}. In this work, we use learned dynamic models for policy evaluation and synthetic data generation in policy learning.  Moreover, the dynamics model can act as policies for action predictions.
