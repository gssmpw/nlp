\section{Related work}
The study of random matrices with nonlinear dependencies was initiated by El Karoui~\cite{elkaroui} and Cheng and Singer~\cite{cheng} in the context of \emph{random inner-product kernel matrices}, where the nonlinearity is applied to the sample covariance matrix, formally \( f (X^\top X)\), with \(X\) being a rectangular matrix with i.i.d.\ entries. In the case of Gaussian entries and linear-width regime, the bulk eigenvalues asymptotically follow the free convolution of the semicircle and Marchenko-Pastur distributions~\cite{cheng,fan2019}. More recently,~\cite{lu2023} extended these results to the polynomial growth regime, and~\cite{dubova2023} generalized them to cases where \(X\) has i.i.d.\ entries with finite moments, demonstrating the universality of this phenomenon.

Instead of applying a nonlinearity to the sample covariance matrix, one can also consider the sample covariance matrix of a nonlinearity. This is the focus of the present article, which studies random matrices of the form \(YY^\top\), where \(Y = f(WX)\) is the so-called \emph{random features matrix}. This model is crucial for understanding the training dynamics and generalization properties of two-layer feed-forward neural networks. Specifically, the expected training loss and generalization error are closely linked to the spectral properties of these matrices in high dimensions. From a mathematical perspective, characterizing the asymptotic spectrum of the random matrix \(YY^\top\) is challenging due to the nonlinear dependencies introduced by the activation function, which make the analysis significantly more complex compared to linear random matrix ensembles. The global law of the conjugate kernel was first studied by Pennington and Worah~\cite{pennington2017} in the setting where \(W\) and \(X\) have i.i.d.\ centered Gaussian entries. This work was later extended by Benigni and P\'{e}ch\'{e}~\cite{benigni2021} to matrices with sub-Gaussian tails and real analytic activation functions. P\'{e}ch\'{e}~\cite{peche2019} further showed that the nonlinear random matrix \(YY^\top\) is asymptotically equivalent to a \emph{Gaussian linear model}, where the asymptotic effect of the nonlinearity is captured by a linear combination of the involved matrices and an additional independent Gaussian matrix. Building on this line of work, the second author in collaboration with Schr\"{o}der~\cite{schroder2021} computed the asymptotic spectral density of the random feature model in the practically important case with additive bias, i.e., \(Y = f(WX+B)\), where \(B\) is an independent rank-one rectangular Gaussian random matrix. This work employed the resolvent method and cumulant expansion, rather than the moment method used in earlier works~\cite{pennington2017,benigni2021}. Recently, Speicher and Wendel~\cite{speicher2024} computed the cumulants of a broader class of nonlinear random matrices, where the nonlinearity is applied to symmetric orthogonally invariant random matrices, and showed that a Gaussian equivalence principle holds. Dabo and Male~\cite{dabo2024} further generalized the model by considering random matrices with variance profiles, namely matrices where the variance of the entries varies from one variable to another. They showed that the model is asymptotically \emph{traffic-equivalent} to an information-plus-noise type sample covariance matrix, consistent with previous results~\cite{peche2019}. In parallel to~\cite{pennington2017}, Louart, Liao, and Couillet~\cite{couillet2018} initiated another line of research on the model \(f(WX) f(WX)^\top\), focusing on the case where \(X\) is deterministic, \(W\) is a random (with entries given by functions of standard Gaussian random variables), and \(f \) is a Lipschitz activation function. Using concentration inequalities, they derived a deterministic equivalent for the expectation of the resolvent and showed that the eigenvalue distribution aligns with that of a standard sample covariance matrix. Generalizations of this result were explored further in~\cite{chouard2023}.

In this article, we study conjugate kernel random matrices with light-tailed inputs and heavy-tailed weights. Linear models of symmetric matrices with independent heavy-tailed entries have been extensively analyzed in~\cite{zakarevich2006,benarous2008,bordenave2011,bordenave2011bis}. These matrices fall outside the Wigner universality class. Specifically, while the empirical measure of their eigenvalues converges, the limiting distribution is not the semicircular law. Instead, it is a probability measure with unbounded support. Depending on the model, this limit can exhibit atoms \cite{salez2015}, as in the case of adjacency matrices of Erd\"os-R\'enyi graphs, or have a smooth density, such as when the entries follow an \(\alpha\)-stable distribution \cite{Belinschi2009}. The eigenvalue fluctuations resemble those of independent random variables~\cite{BGM}. However, the local spectral fluctuations remain largely unknown, except in the case of \(\alpha\)-stable entries, where certain regimes exhibit fluctuations similar to the Gaussian Orthogonal Ensemble~\cite{bordenave2013,bordenave2017,aggarwal2021}. In contrast, the behavior of conjugate kernel matrices with heavy-tailed weights is even less understood. In this work, the empirical spectral measure of these models has light tails, in fact all finite moments, although we conjecture that the limiting distribution is not compactly supported. For the eigenvalue fluctuations, we conjecture that they follow the usual scaling of the central limit theorem which agrees with our rough bounds on the covariance derived in Section~\ref{sec:cov}.