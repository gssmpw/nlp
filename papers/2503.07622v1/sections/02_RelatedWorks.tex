\section{RELATED WORKS}

Research has shown that users display common instinctive social signals during robot errors, distinguishing these situations from error-free scenarios. These signals include gaze behaviour \cite{kontogiorgos_embodiment_2020, peacock_gaze_2022, kontogiorgos_systematic_2021, ramtin_hri_2025}, facial expressions \cite{mirnig_impact_2015, stiber_modeling_2022, kontogiorgos_systematic_2021, wachowiak_time_2024}, verbalisation \cite{kontogiorgos_embodiment_2020, mirnig_impact_2015, kontogiorgos_systematic_2021}, and body movements \cite{mirnig_impact_2015, trung_head_2017, wachowiak_time_2024}. For example,  Peacock et al. \cite{peacock_gaze_2022} observed that gaze initially increases in motion during failures and then stabilises as users address the issue. Stiber et al. \cite{stiber_using_2023} identified heightened activity in facial muscles, such as smiling and brow lowering, during robot errors. Similarly, Kontogiorgos et al. \cite{kontogiorgos_systematic_2021, kontogiorgos_embodiment_2020} reported increased spoken words, longer utterances, and more gaze shifts toward the robot, reflecting greater user engagement during failures.

Several studies have explored machine-learning approaches to detect failures in human-robot interactions using various behavioural and physiological cues. For example, Peacock et al. \cite{peacock_gaze_2022} trained logistic regression models on gaze dynamics to detect failures, achieving accurate detection a few seconds after the errors occurred. Similarly, Kontogiorgos et al. \cite{kontogiorgos_systematic_2021, kontogiorgos_behavioural_2020} developed machine learning models, including XGBoost and Random Forest, that utilised multimodal behaviours—such as linguistic, facial, and acoustic features—to achieve high accuracy in distinguishing failure scenarios from non-failure scenarios in a verbal guidance scenario. Separately, Stiber et al. \cite{stiber_modeling_2022} trained Multi-Layer Perceptron (MLP) models using action units (AUs) derived from facial reactions, showing that AUs are effective if users provide timely and observable responses to robot errors. Since not all users exhibit clear facial or verbal reactions to failures, this highlights a gap in the literature. This research aims to address this gap by designing classifier models based on user gaze during a collaborative task, evaluating their performance relative to the time elapsed after a robot failure, and assessing their effectiveness in real-time settings.