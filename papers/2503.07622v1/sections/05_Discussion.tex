\section{DISCUSSION and CONCLUSION}  
This study highlights the importance of user gaze dynamics in detecting robot failures during collaborative tasks. The results demonstrate that gaze-based machine learning classifiers can identify robot errors with high accuracy when the models are trained by labelling each pick-and-place action as either a failure or a non-failure, and tested similarly while reducing the duration of failure periods. However, since the exact moment of a robot failure is unknown, the robot needs to repeatedly analyse user gaze at regular intervals to determine whether a failure has occurred. For this purpose, we tested time intervals of 3, 5, and 10 seconds.
Although this real-time approach does not achieve the same level of accuracy as the previous method, it allows robots to continuously monitor for EFs or DFs. The models were more effective at distinguishing between NF and EF than between NF and DF. Among the classifiers, Random Forest achieved the highest accuracy, but its recall of failures was lower than others. For higher recall rates, SVM may be a better option as it detects the highest number of failures. 

Additionally, we analysed the percentage of users whose failures were correctly detected within each 5-second interval after a failure began. For EF, this percentage was approximately 70\%, while for DF, the results varied across classifiers, with CatBoost reaching around 60\%. Unlike user facial expressions in response to failures, as studied in \cite{stiber_using_2023}, gaze reactions do not exhibit specific characteristics like reaction time and reaction duration. As shown in the results, varying the duration of failure periods or using different 5-second intervals yielded consistent performance across models.

Despite these promising results, several limitations remain. In some cases, the robot's actions, such as placing its piece, overlapped with participants planning their next move, which could affect model accuracy. Furthermore, the study relied solely on gaze behaviour as an error indicator. Integrating multimodal cues, such as facial expressions, body movements, and speech, could enhance detection accuracy and robustness.

In conclusion, leveraging user gaze dynamics for robot error detection represents a significant step toward improving the reliability and trustworthiness of collaborative robots. This approach has the potential to enhance human-robot collaboration by enabling robots to proactively detect and recover from errors in real-time.

