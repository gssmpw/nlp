\section{Methodology}

\subsection{Tasks Description}

The experiment consisted of four collaborative tasks where a participant and a robot worked together to solve Tangram puzzles. In each task, participants were tasked with creating a unique shape using Tangram pieces. To avoid any influence of task difficulty on participant perception or behaviour, puzzles of comparable difficulty were carefully selected (See \cite{ramtin_hri_2025} for more details on the experimental setup).

Each Tangram puzzle consisted of seven pieces. The robot was responsible for placing four pieces (two small triangles, a square, and a parallelogram), while the participant placed the remaining three pieces (a medium triangle and two large triangles). The Tangram pieces were 3D-printed. The silhouettes of the puzzles were printed in black on A2-sized white paper, which was fixed to the table. Both the participant and the robot placed their pieces into the Tangram figure. Participants were instructed to move their pieces only after the robot had completed its action, with the robot always beginning by placing the first piece.

The robot's pieces were positioned near its workspace, adjacent to the paper.
%, as illustrated in Figure \ref{fig:Robot}. 
For each puzzle, the robot determined the placement and orientation of its pieces before initiating movement. The Tiago robot, programmed using ROS1, utilized the `tf` library to map the pose of each piece to the coordinate frame of its robotic arm. It then employed inverse kinematics to precisely position its arm for accurate placement of each piece.




% \begin{figure}[h]
%   \centering
%   \includegraphics[width=0.9\linewidth]{Photos/AoIs2.pdf} 
%   \caption{Different areas of interest in the experiment.
% }
%   \label{fig:Robot}
% \end{figure}






\subsection{Robot Failures}

We designed the robot to deliberately fail during each task, with failures categorized into two types: Executional Failure (EF) and Decisional Failure (DF), representing common technical issues robots may face in interactions.

\begin{itemize}
    \item \textbf{EF:} The robot pauses for 15 seconds after grasping an object, holding it in its end effector during the pause. After the delay, it resumes the task and completes the pick-and-place action.
    \item \textbf{DF:} The robot picks up an object but mistakenly moves to the location intended for a different object. It places the object incorrectly, pauses for 5 seconds, and then corrects the mistake by lifting the object and placing it in the correct location.
\end{itemize}

During both failure and non-failure events, the robot followed the same pick-and-place procedure, with failures differing only in task duration: EFs added a 15-second pause, while DFs added 16.5 seconds due to incorrect placement and correction.

To avoid timing biases, the robot's malfunctions were programmed to occur at different points in each task. Specifically, failures could occur either at the beginning of the collaboration, while the robot was placing the first piece, or toward the end of the interaction, while the robot was placing the third piece.




\subsection{Experiment}

A total of 26 participants (16 females, 9 males, and 1 non-binary; aged 18–34) were recruited from a university platform. They provided informed consent, received gift vouchers as compensation, and were debriefed after the experiment.

Participants were guided by an experimenter who explained the tasks and intervened as needed to ensure safety or trigger the robot’s responses. Participants wore eye-tracking glasses to record their gaze data.



Each participant completed four puzzles, with each puzzle lasting approximately 3 minutes, followed by a short break between puzzles. The study was conducted in a controlled laboratory setting with participants who had no prior experience with robotics. In each puzzle, the robot was responsible for placing four pieces, correctly placing three and making a failure with one. This failure was pre-programmed to vary by type (Executional or Decisional) and timing (Early or Late). These combinations were counterbalanced using a four-condition Latin-Square design, ensuring balanced exposure across conditions and minimizing timing effects.




\begin{table}[h!]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{ c|c c c c c}

\textbf{Participant ID}  & \textbf{Puzzle 1} & \textbf{Puzzle 2} & \textbf{Puzzle 3} & \textbf{Puzzle 4} \\ \hline
1  & EF (Early) & EF (Late) & DF (Late) & DF (Early) \\ 
2  & EF (Late) & DF (Early) & EF (Early) & DF (Late)\\ 
3  & DF (Early) & DF (Late) & EF (Late) & EF (Early)\\ 
4  & DF (Late) & EF (Early) & DF (Early) & EF (Late)\\
5  & EF (Early) & EF (Late) & DF (Late) & DF (Early)\\
...  &  ... &  ... & ... & ...\\ 
14  & EF (Early) & EF (Late) & DF (Late) & DF (Early) \\ 
...  &  ... &  ... & ... & ... \\
\end{tabular}

}
\caption{Order of failure type with their timing across puzzles}
\label{table:participant_data}
\end{table}



\subsection{Measures}\label{Measures}

% For each puzzle and each piece, we recorded the robot’s current actions—such as moving above the target object and lowering to pick it up—along with information about whether a failure occurred and the type of failure.

% Gaze data was collected as participants collaborated with the Tiago robot to solve the puzzles, with particular focus on the gaze data during the robot's turn. Data collection was facilitated by Neon Eye Tracking Glasses from Pupil Labs, which recorded the participant's field of view image frames and the x and y coordinates of their gaze within those frames at a rate of 30Hz.

% To identify the participant's areas of interest (AOIs), ArUco markers were placed around these areas, including the robot's body (face and torso), the Tangram figure, the end effector, the robot's pieces, and the participant's pieces.

For each puzzle and piece, the robot's actions (e.g., moving or picking up objects), failure occurrences, and failure types were recorded, along with participant gaze data collected using Neon Eye Tracking Glasses. For more details on the methodology, refer to Paper \cite{ramtin_hri_2025}.

Using the gaze data, we calculated several metrics, including the average rate of gaze shifts towards all AOIs, the average rate of gaze shifts towards the robot's body, the average duration of gaze directed at the end effector, the probability of gazing at each AOI, transition entropy, and stationary entropy. For successful pickups and placements, metrics were calculated from when the robot began picking up the object until it placed the piece. For EFs, metrics covered the 15-second failure period, while for DFs, they spanned from the robot's movement towards the incorrect location to completing the placement and pausing for 5 seconds.


Using gaze behaviour metrics, machine learning models were trained to classify each type of failure against a no-failure condition in a binary manner. Each participant contributed 12 data points for non-failure conditions, 2 data points for EFs, and 2 data points for DFs. Five classifiers were employed: Random Forest, configured with 100 decision trees; AdaBoost, with 100 boosting iterations; XGBoost, performing 100 boosting rounds with a learning rate of 0.01; Support Vector Machine (SVM), using a linear kernel; and CatBoost, configured with 100 iterations, a learning rate of 0.1, and a tree depth of 6. Classifiers were implemented using Scikit-learn, XGBoost, and CatBoost libraries.






To address data imbalance and prevent bias, we applied SMOTE normalization with k-neighbors set to 2. The trained models were evaluated using two approaches. First, we assessed their performance in distinguishing failure events from non-failure events based on the first \textit{n} seconds of a failure. Second, to evaluate real-time failure detection, we analysed the models' performance in identifying failure types by segmenting the eye-tracking data into intervals of 3, 5, and 10 seconds, using a sliding window of 1 second. For both evaluation methods, we employed leave-one-out cross-validation, where models were trained on data from 25 participants and tested on the data from the remaining participant.

The primary goal of the classifiers was to achieve high accuracy while minimising false negatives, as failing to detect a failure event is critical in this context. Given this aim, we focus on reporting only the accuracy and recall metrics.
