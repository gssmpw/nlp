\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{comment}
\usepackage{hyperref}
\usepackage{threeparttable}
\usepackage[font=footnotesize]{caption}
\usepackage{afterpage}
\usepackage{float}
\usepackage{subcaption} % Modern replacement for subfigure

\usepackage{balance}
%\usepackage[moderate,tracking=normal]{savetrees}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

%\title{Investigating Human Gaze Dynamics in Response to Robotic Failure in Human-Robot Collaboration
%}
\title{Real-Time Detection of Robot Failures Using Gaze Dynamics in Collaborative Tasks
}


    
\author{\IEEEauthorblockN{Ramtin Tabatabaei}
\IEEEauthorblockA{\textit{The University of Melbourne} \\
Melbourne, Australia \\
stabatabaeim@student.unimelb.edu.au}
\and

\IEEEauthorblockN{Vassilis Kostakos}
\IEEEauthorblockA{\textit{The University of Melbourne} \\
Melbourne, Australia \\
vassilis.kostakos@unimelb.edu.au}
\and

\IEEEauthorblockN{Wafa Johal}
\IEEEauthorblockA{\textit{The University of Melbourne} \\
Melbourne, Australia \\
wafa.johal@unimelb.edu.au}}



% \author{\IEEEauthorblockN{Anonymous Author(s)}
% }



\maketitle

\begin{abstract}

Detecting robot failures during collaborative tasks is crucial for maintaining trust in human-robot interactions. This study investigates user gaze behaviour as an indicator of robot failures, utilising machine learning models to distinguish between non-failure and two types of failures: executional and decisional. Eye-tracking data were collected from 26 participants collaborating with a robot on Tangram puzzle-solving tasks. Gaze metrics, such as average gaze shift rates and the probability of gazing at specific areas of interest, were used to train machine learning classifiers, including Random Forest, AdaBoost, XGBoost, SVM, and CatBoost.
The results show that Random Forest achieved 90\% accuracy for detecting executional failures and 80\% for decisional failures using the first 5 seconds of failure data. Real-time failure detection was evaluated by segmenting gaze data into intervals of 3, 5, and 10 seconds. These findings highlight the potential of gaze dynamics for real-time error detection in human-robot collaboration.





%Results show that Random Forest achieved ~90% accuracy for execution failures and ~80% for decisional failures using the first 5 seconds of failure data. Real-time detection was evaluated by segmenting gaze data into intervals of 3, 5, and 10 seconds. The findings highlight the potential of gaze dynamics for real-time error detection in human-robot collaboration.

\end{abstract}


\begin{IEEEkeywords}
Robot Failures, Gaze Dynamics, Human-Robot Collaboration, Machine Learning Classifiers
\end{IEEEkeywords}


\input{sections/01_Introduction}
\input{sections/02_RelatedWorks}


\input{sections/03_Methodology}



% \input{sections/04_2_Expriment}
% \input{sections/04_3_Measures}


\input{sections/04_Results}

\input{sections/05_Discussion}

% \input{sections/07_Conclusion}




\section*{Acknowledgement}
This research is partially supported by the Australian Research Council Discovery Early Career Research Award (Grant No. DE210100858)

\newpage

\bibliographystyle{ieeetr}
\balance
\bibliography{references,hri2025}



\end{document}
