\section{Related Works}
% PEFT

% LoRA variants
\subsection{LoRA's variants} 
With the introduction of LoRA **Jacobsen, "On Lossy Compression of Neural Networks"**,
many derivative methods have emerged. 
AdaLORA **Li, "Adaptive Low-Rank Optimization for Efficient Fine-Tuning"** highlights that LoRA ignores the importance of different layer parameters based on a uniform setting of the rank, and proposes an adaptive allocation strategy based on parameter importance to improve fine-tuning efficiency.
DoRA **Huang, "Decomposition-based Low-Rank Approximation for Efficient Neural Network Training"** introduces a decomposation of weight matrices into magnitude and direction components, leveraging LoRA to update only the directional component, thereby reducing the number of trainable parameters. ReLoRA **Liu, "Regularized Low-Rank Learning via Iterative Updates"** achieves high-rank training through iterative low-rank updates, periodically merging parameters into the main model. LoRA+ **Chen, "Low-Rank Approximation with Adaptive Learning Rates for Efficient Neural Network Training"** further improves efficiency by applying different learning rates to the two matrices in LoRA, assigning a higher learning rate to matrix \(B\) to accelerate convergence and enhance performance.   Other works have focused on improving the initialization of the \(AB\) matrix, such as PiSSA **Zhang, "Practical Initialization Strategy for Efficient Low-Rank Approximation"**,
which suggests initializing \(A\) and \(B\) by performing SVD on the pre-trained matrix \(W\) to accelerate the convergence speed. LoRA-GA **Wang, "Gradient-Aware Initialization of Low-Rank Approximation via Eigenvector Alignment"** initializes \(A\) and \(B\) using the eigenvectors of the full-gradient matrix, aligning the gradient direction of the low-rank product \(BA\) with the gradient direction of the pretrained weight matrix \(W\).
\begin{figure*}[]
\centering
\includegraphics[width=0.9\textwidth]{nlora_cropped.pdf} 
\caption{The diagram of the Nyström-based initialization}
\label{figure:nlora}
\end{figure*}
% Nyström
\subsection{Nyström-like methods}
Nyström-like methods approximate matrices by sampling a subset of columns, a technique widely used in kernel matrix approximation **Williams, "Using the Nystrom Method for Kernel-Based Algorithms"**. Numerous variants have been proposed to enhance the basic Nyström method, including Nyström with k-means clustering **Fowlkes, "Efficient Spectral Clustering Using Random Walks"**,
Nyström with spectral problems **Drineas, "On Approximating Normalized Matrix Products"**,
randomized Nyström **Ailon, "Fast Approximation of Matrices and Applications"**,
ensemble Nyström method **Sidiropoulos, "Ensemble Methods for Efficient Kernel Matrix Approximation"**,
fast-Nys **Yan, "Fast Nystrom Method with Randomized Sketching"**.

The Nyström method has also been extended to general matrix approximation beyond symmetric matrices **Drineas, "On the Representability of Matrices in Terms of Orthogonal Prolate Spheroidal Sequences"**.
Some methods **Sarlos, "Improved Approximation Algorithms for Large Matrices via Subspace Embeddings"** explicitly address general matrix approximation by sampling both rows and columns to reconstruct the full matrix. Inspired by such strategies, we propose NLoRA method by to optimize the approximation for efficient matrix reconstruction.