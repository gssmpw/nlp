\documentclass[preprint,12pt]{elsarticle}

\usepackage{subcaption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{svg}
\usepackage{slashbox}
\usepackage{hyperref}
\usepackage{orcidlink}
\DeclareMathOperator{\Tr}{Tr}
\def\vect{\mathbf}
\def\matr{\mathbf}
\def\mrm{\mathrm}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\journal{Elsevier Image and Vision Computing}

\begin{document}

\begin{frontmatter}

%% Title, authors and addresses

%% use the tnoteref command within \title for footnotes;
%% use the tnotetext command for theassociated footnote;
%% use the fnref command within \author or \affiliation for footnotes;
%% use the fntext command for theassociated footnote;
%% use the corref command within \author for corresponding author footnotes;
%% use the cortext command for theassociated footnote;
%% use the ead command for the email address,
%% and the form \ead[url] for the home page:
%% \title{Title\tnoteref{label1}}
%% \tnotetext[label1]{}
%% \author{Name\corref{cor1}\fnref{label2}}
%% \ead{email address}
%% \ead[url]{home page}
%% \fntext[label2]{}
%% \cortext[cor1]{}
%% \affiliation{organization={},
%%             addressline={},
%%             city={},
%%             postcode={},
%%             state={},
%%             country={}}
%% \fntext[label3]{}
\fntext[contrib]{These authors contributed equally to this work}

\title{Hybrid Deepfake Image Detection: A Comprehensive Dataset-Driven Approach Integrating Convolutional and Attention Mechanisms with Frequency Domain Features}

\author[inst1]{Kafi Anan\fnref{contrib}}
\author[inst1]{Anindya Bhattacharjee\fnref{contrib}}
\author[inst1]{Ashir Intesher\fnref{contrib}}
\author[inst1]{Kaidul Islam\fnref{contrib}}
\author[inst1]{Abrar Assaeem Fuad\fnref{contrib}}
\author[inst1,inst2]{Utsab Saha\,\orcidlink{0000-0003-2106-8648}}
\author[inst1]{Hafiz Imtiaz\,\orcidlink{0000-0002-2042-5941}}


\affiliation[inst1]{organization={Department of Electrical and Electronic Engineering},%Department and Organization
            addressline={BUET}, 
            city={Dhaka},
            postcode={1205}, 
            % state={State One},
            country={Bangladesh}}


\affiliation[inst2]{organization={School of Data and Sciences},%Department and Organization
            addressline={BRAC University}, 
            city={Dhaka},
            postcode={1212}, 
            % state={State Two},
            country={Bangladesh}}


%% Abstract
\begin{abstract}
Effective deepfake detection tools are becoming increasingly essential over the last few years due to the growing usage of deepfakes in unethical practices. There exists a diverse range of deepfake generation techniques, which makes it challenging to develop an accurate universal detection mechanism. The 2025 Signal Processing Cup (\textit{DFWild-Cup} competition) provided a diverse dataset of deepfake images, which are generated from multiple deepfake image generators, for training machine learning model(s) to emphasize the generalization of deepfake detection. To this end, we proposed an ensemble-based approach that employs three different neural network architectures: a ResNet-34-based architecture, a data-efficient image transformer (DeiT), and an XceptionNet with Wavelet Transform to capture both local and global features of deepfakes. We visualize the specific regions that these models focus for classification using Grad-CAM, and empirically demonstrate the effectiveness of these models in grouping real and fake images into cohesive clusters using t-SNE plots. Individually, the ResNet-34 architecture has achieved 88.9\% accuracy, whereas the Xception network and the DeiT architecture have achieved 87.76\% and 89.32\% accuracy, respectively. With these networks, our weighted ensemble model achieves an excellent accuracy of 93.23\% on the validation dataset of the SP Cup 2025 competition. Finally, the confusion matrix and an Area Under the ROC curve of 97.44\% further confirm the stability of our proposed method.
% \textcolor{magenta}{We used a t-SNE plot to demonstrate that our models effectively distinguish between real and fake images, grouping them into cohesive clusters. For visualization, we also included Grad-CAM views to highlight which specific regions our models focus on for classification. Finally, the confusion matrix and an Area Under the ROC curve of 97.44\% further confirm the stability of our proposed method.}
\end{abstract}

% %%Graphical abstract
% \begin{graphicalabstract}
% %\includegraphics{grabs}
% \end{graphicalabstract}

% %%Research highlights
% \begin{highlights}
% \item Research highlight 1
% \item Research highlight 2
% \end{highlights}

%% Keywords
\begin{keyword}
Deepfake \sep ResNet-34 \sep Transformer \sep DeiT \sep Wavelet \sep XceptionNet

\end{keyword}

\end{frontmatter}

%% Add \usepackage{lineno} before \begin{document} and uncomment 
%% following line to enable line numbers
%% \linenumbers

%% main text
%%
\section{Introduction}
Deepfakes are media that are edited or created by generative Artificial Intelligence (AI), which do not exist or have not occurred in reality. Although deepfakes have existed for a long time, it took experts, and often entire studios, to generate \emph{convincing} fake media. Due to the surge of machine learning and generative AI tools, the generation of deepfakes has never been easier and more accessible. Such ease of access has raised serious societal concerns since these are being used to manipulate public opinion, commit fraud, create non-consensual pornography, and attack individuals through defamation or identity threats~\cite{doi:10.1089/cyber.2020.0272}. Deepfakes are also being used to gain malicious political advantage worldwide~\cite{westerlund2019emergence}. Although deepfake generation tools are widely accessible, detection tools are not nearly as effective and accurate. More specifically, the existing detection methods only work well when the test image resembles the features of the images of the training dataset of the detector. That is, the major issues in the existing detection approaches are the lack of diversity in training datasets, and the inability to be up to date with new fake image generation techniques~\cite{kaur2024deepfake}.

To address the research gap, the \textit{DFWild-Cup} competition called for novel approaches that can achieve accurate deepfake detection and provided a diverse dataset (consisting of samples from eight different standard datasets) to help train a generalized deepfake detection algorithm. Such a diverse dataset poses unique challenges to formulating accurate image detection methods. In our proposed solution for deepfake image detection, we have employed a weighted ensemble approach, which utilized three different neural network architectures with different strengths: i) a ResNet-34-based architecture with squeeze and excitation block, ii) a data-efficient image transformer (DeiT) model, and iii) an Xception network. More specifically, we consolidate the outputs from these architectures and employ a weighted ensemble mechanism to get the final result. We note that the ResNet-34-based architecture, due to its residual connections, allows learning complex hierarchical features with deeper networks by addressing the problem of vanishing gradients~\cite{he2016deep}. Moreover, the squeeze and excitation (SE) block incorporated in the ResNet-34-based architecture provides channel-wise attention with very little computational expense~\cite{hu2018squeeze}. Besides, the self-attention mechanism and the distillation token introduced in the DeiT model allow learning long-range dependencies across the entire image. More specifically, the ResNet model excels at capturing local spatial relationships of an image, while the transformer model can capture global features of an image through the self-attention mechanism~\cite{khan2022transformers}. Finally, the Xception model, combined with wavelet transform as a preprocessing technique, is effective in detecting fine-grained artifacts in deepfake images. We have utilized the 'Haar' wavelets due to their notable ability to detect abrupt transitions and edges in the data~\cite{porwik2004haar}. A weighted ensemble mechanism of these three networks is used as the final solution, which has enabled us to achieve superior performance compared to the models operating alone.\\

\noindent\textbf{Related Works. }The advancement in deepfake research has led to the creation of nearly perfect image manipulations that are undetectable to the human eye. The combination of multiple deep learning architectures has been shown to perform well on deepfake detection. Gowda and Thillaiarasu~\cite{gowda2022investigation} showed ensemble techniques of Convolutional Neural Network (CNN) models (ResNeXt and Xception) achieve higher accuracy on deepfake detection. Wodajo and Atnafu~\cite{wodajo2021deepfake} combined VGG-16 for feature extraction and Vision Transformer (ViT) for classification, showing promising results on the DFDC dataset. Wolter et al.~\cite{wolter2022wavelet} proposed a wavelet-packet-based analysis for detecting GAN-generated images, highlighting spatial frequency differences in synthetic content and achieving competitive results using combined architectures of wavelets and CNNs. Ricker et al.~\cite{ricker2022towards} proposed the usage of frequency domain features extracted via DCT, and achieved high accuracy rates (97.7\% for GAN and 73\% for diffusion models), highlighting the utility of frequency-domain artifacts. Patel et al. developed an enhanced deep CNN (D-CNN) architecture for deepfake detection, achieving reasonable accuracy and high generalization capability~\cite{patel2023improved}. Patch-Forensics, a technique analyzing smaller patches to detect local image artifacts, achieves 100\% average precision on StyleGAN-generated images using an Xception Block 2 classifier~\cite{frank2020leveraging}. However, Abdullah et al.~\cite{abdullah2024analysis} highlighted two major limitations of the existing networks arguing that the state-of-the-art models often lack control over content and image quality, making them ineffective at detecting high-quality deepfakes if trained on low-quality fake images, and they also lack adversarial evaluation, where an attacker can exploit knowledge of the defense. Works on adversarial perturbation attacks to fool deep neural network (DNN) based detectors are a challenge in deepfake detection~\cite{hussain2021adversarial, gandhi2020adversarial}. Training on diverse and adversarially perturbed datasets can increase the robustness of a detection architecture~\cite{nguyen2022deep}. Lin et al. proposed a CNN-based method for deepfake detection that combines multi-scale convolution with a vision transformer~\cite{lin2023deepfake}. Their approach features a multi-scale module incorporating dilation convolution and depth-wise separable convolution. This design aims to capture more facial details and identify tampering artifacts at various scales~\cite{lin2023deepfake}. Note that the ViT has gained significant popularity in recent years for deepfake image detection~\cite{heo2023deepfake, usmani2024efficient}. 

Researchers are not only relying on complex network architectures but also exploring other approaches, such as the Frequency Enhanced Self-Blending Images (FSBI) method for deepfake detection, which utilizes the Discrete Wavelet Transform (DWT)~\cite{hasanaath2025fsbi}. Zhu et al. introduced a learnable Image Decomposition Module (IDM) that uses a recomposition operation to highlight illumination inconsistencies~\cite{zhu2024deepfake}. It features a multi-level enhancement technique for effective feature recomposition and is trained in the logarithmic domain, with an extensible design~\cite{zhu2024deepfake}. To address challenges with low-quality datasets and inadequate detection performance, Cheng et al. have developed MSIDSnet~\cite{cheng2024deepfake}. This framework utilizes a multi-scale fusion (MSF) module to capture forged facial features and an interactive dual-stream (IDS) module to enhance feature integration across frequency and spatial domains~\cite{cheng2024deepfake}. Another recent detection model by Byeon et al. uses a spatial-frequency joint dual-stream convolutional neural network with learnable frequency-domain filtering kernels~\cite{byeon2024deep}.

\section{Proposed Methodology: Weighted Ensemble of Neural Network Models}
\subsection{Description of the Dataset}
As mentioned before, the \textit{DFWild-Cup} competition has provided a diverse dataset. The dataset contains images from eight publicly available standard datasets designed for the DeepfakeBench evaluation~\cite{DeepfakeBench_YAN_NEURIPS2023}: Celeb-DF-v1~\cite{li2020celeb}, Celeb-DF-v2~\cite{li2020celeb}, FaceForensics++~\cite{rossler2019faceforensics++}, DeepfakeDetection~\cite{Dufour_Gully_2019}, FaceShifter~\cite{li2019faceshifter}, UADFV~\cite{li2018ictu}, Deepfake Detection Challenge Preview~\cite{dolhansky2019dee}, and Deepfake Detection Challenge~\cite{dolhansky2020deepfake}. To ensure that the detector performance is not biased by specific pre-processing techniques, all datasets underwent uniform pre-processing. Additionally, any information that would identify the origin of the dataset has been eliminated, and the file names have been anonymized. For training, there are a total of 42,690 real images and 219,470 fake images. The validation dataset consists of 1,548 real images and 1,524 fake images. It is evident that there is a significant class imbalance present in the training data as the ratio of real and fake images is approximately 5:1.
\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{Figures/training-sessions.pdf}
\caption{Training procedure including class imbalance handling}
\label{figure_1}
\end{figure}

\subsection{Data Preprocessing}
\subsubsection{Class Balancing for Model Training}
% We have employed a weighted ensemble approach, which utilized three diverse neural network architectures: a ResNet-34-based architecture with squeeze and excitation (SE) block, a data-efficient image transformer model, and an XceptionNet model. 
Since the number of fake images is approximately five times the number of real images, we divided the fake images $\{\mathcal{F}\}$ into five disjoint subsets $\{\mathcal{F}_1, \ldots, \mathcal{F}_5\}$. Each of the three aforementioned models, denoted as \( M_1, M_2, M_3 \), was trained in five stages with these disjoint subsets of fake images. At each stage \( i \), the model is trained using the entire real image set \( \mathcal{R} \) and one subset of fake images \( \mathcal{F}_i \), such that the training dataset of stage $i$ is given by:
\begin{equation}
    \mathcal{D}_i = \mathcal{R} \cup \mathcal{F}_i, \quad i \in \{1,2,3,4,5\}.
\end{equation}
We emphasize that the same set of real images $\{\mathcal{R}\}$ are treated as one class during each stage. The training of each model is initialized with pre-trained weights \( \theta_0 \), obtained from an ImageNet~\cite{deng2009imagenet}-trained version of the corresponding architecture. The model parameters at each stage are updated iteratively using the dataset \( \mathcal{D}_i \), following the optimization process:
\begin{equation}
    \theta_i = \arg\min_{\theta} \sum_{(x,y) \in \mathcal{D}_i} \mathcal{L}(M(x; \theta), y),
\end{equation}
where \( \mathcal{L} \) represents the chosen loss function. After training on \( \mathcal{D}_i \), the final weights \( \theta_i \) from each stage are used to warm-start the next stage:
\begin{equation}
    \theta_{i+1} \leftarrow \theta_i, \quad \text{for } i \in \{1,2,3,4\}.
\end{equation}
This process continues until the model has been trained on all subsets of fake images, ensuring that the model learns from diverse variations of the fake images while preserving knowledge of real images. Figure \ref{figure_1} illustrates the sequential learning procedure.


\subsubsection{Data Augmentation}
As mentioned before, we utilized the same set of real images across five training stages to address the class imbalance. However, this evidently increases the chance of overfitting. To mitigate overfitting and enhance the model's generalization performance, we implemented four data augmentation techniques: random horizontal flips, random rotations, color jittering, and random resized cropping. The random horizontal flip method flips the training images along their vertical axis with a probability of 50\%. The Random rotation method augments images by randomly rotating them within $0^o-10^o$. The color jitter method randomly alters the brightness, contrast, saturation, and hue of images within a specified range. To this end, a range of 0.2 for brightness, contrast, saturation, and hue provided the best results in our experiments. Finally, the random resized crop was used as a data augmentation technique that applies both random cropping and resizing to create varied versions of training images. In our case, it produced $224\times 224$ images, while randomly scaling the crop area between 80\% and 120\% of the original image size. These augmentations enhanced the diversity of the training dataset, reduced overfitting, and improved the model’s ability to generalize across a wider range of deepfake detection scenarios. It should be noted that such augmentations were used only in training the ResNet-34 and the DeiT architectures. For the XceptionNet, we employed wavelet transform-based feature extraction from the raw images, and data augmentation techniques would result in incorrect feature representation in the spectral domain.

\subsubsection{Data Resizing and Normalization}
All three models were pre-trained using the ImageNet dataset. Since the requirements of input image dimensions are different on the ResNet-34 architecture and the Xception network, we have resized the input images accordingly to the specific dimensions for respective architectures. Additionally, the training images were normalized using the mean and standard deviation of the ImageNet dataset~\cite{deng2009imagenet}.



\subsection{Wavelet Feature Extraction for XceptionNet}
The wavelet transform is shown to be effective for image feature extraction as it decomposes images into multi-resolution frequency components~\cite{mallat1989theory}. The ability to decompose an image into different frequency sub-bands while preserving spatial relationships makes it particularly effective at identifying edges, textures, and subtle patterns of an image~\cite{guo2022review}. Using 2D Wavelet transform, an image can be decomposed into \emph{approximate} and \emph{detail} coefficients~\cite{porwik2004haar}. As such, we performed a single-level 2D Discrete Wavelet Transform (2D-DWT) using Haar wavelets to produce the approximate and detailed coefficients of the images and used the computed coefficients as features. 

\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{Figures/wavelet_flow_final.pdf}
\caption{Row and Column-wise filtering and downsampling to generate approximation and detail images through Haar 2D-DWT}
\label{figure_wavelet}
\end{figure}

The 2D-DWT is applied to an image \( I(x, y) \) to decompose it into frequency sub-bands while preserving spatial information. Given an image \( I \in \mathbb{R}^{M \times N} \), a single-level 2D-DWT using Haar wavelets generates four coefficient matrices: the approximate coefficients and three sets of detail coefficients (horizontal, vertical, and diagonal). The wavelet decomposition is performed using separable filtering along rows and columns. The Haar wavelet uses two filters:
\begin{itemize}
    \item \textbf{Low-pass filter (\(\phi\))}: Coefficients \( \left[\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right] \)
    \item \textbf{High-pass filter (\(\psi\))}: Coefficients \( \left[-\frac{1}{\sqrt{2}}, \frac{1}{\sqrt{2}}\right] \)
\end{itemize}
As shown in Figure~\ref{figure_wavelet}, the sub-bands are computed as follows:
\begin{enumerate}
    \item Row-wise filtering and downsampling:
    \begin{align*}
        L_{\text{row}} &= \left( I \ast \phi \right) \downarrow_{2,\text{cols}} \\
        H_{\text{row}} &= \left( I \ast \psi \right) \downarrow_{2,\text{cols}}
    \end{align*}
    
    \item Column-wise filtering and downsampling:
    \begin{align*}
        A &= \left( L_{\text{row}} \ast \phi \right) \downarrow_{2,\text{rows}} \quad \text{(Approximation, LL)} \\
        V &= \left( L_{\text{row}} \ast \psi \right) \downarrow_{2,\text{rows}} \quad \text{(Vertical Details, LH)} \\
        H &= \left( H_{\text{row}} \ast \phi \right) \downarrow_{2,\text{rows}} \quad \text{(Horizontal Details, HL)} \\
        D &= \left( H_{\text{row}} \ast \psi \right) \downarrow_{2,\text{rows}} \quad \text{(Diagonal Details, HH)},
    \end{align*}
\end{enumerate}
where \( \ast \) indicates convolution; \( \downarrow_{2,\text{cols}} \), and \( \downarrow_{2,\text{rows}} \) indicate downsampling the columns and rows by 2, respectively; \( A \) denotes low-frequency approximation coefficients; and \( H, V, D \) denote high-frequency horizontal, vertical, and diagonal detail coefficients, respectively. For an RGB image \( I_r, I_g, I_b \) consisting of three color channels, we apply the above transformation independently to each color channel:
\begin{equation}
    A_c, H_c, V_c, D_c = \text{DWT}(I_c), \quad c \in \{r, g, b\},
\end{equation}
where \( A_c, H_c, V_c, D_c \) are the approximate and detail coefficients for each color channel \( c \). The final feature representation is formed by concatenating the coefficients across all three channels:
\begin{equation}
    F = \text{concat}((A_r, A_g, A_b), (H_r, H_g, H_b), (V_r, V_g, V_b), (D_r, D_g, D_b)).
\end{equation}
To prepare the wavelet-domain features for classification, the coefficients are scaled to an 8-bit integer range and resized to match the input dimensions of the XceptionNet model. In Figure~\ref{figure_3}, we showed two such feature images with the approximate and detailed coefficients of two sample images from the dataset.



%%%%%%%%%%%%%%%%%%%%%%%%%
% \textcolor{red}{Need to edit this as the process is already described mathematically}
% In order to apply 2D-DWT to the RGB images of our competition dataset, we separated each image into three color channels, and computed single-level 2D DWT coefficients using the Haar wavelets for each of the channels. Note that a single-level 2D DWT generates four coefficient arrays -- one array for approximate coefficients, and three arrays for horizontal, vertical, and diagonal detail coefficients. These arrays are half the size of the original image. Recall that we have used the Haar wavelets as they can capture the sharp transitions present in an image and are computationally efficient~\cite{guo2022review}. Next, we concatenated these 4 arrays from the RGB channels to form a \textit{feature image}. 

 % As the feature images are used as inputs to our XceptionNet model, we scaled the coefficients to 8-bit integers and performed resizing and other adjustments similar to the other two models. 

\begin{figure}[t]
\centering
\includegraphics[scale=0.6]{Figures/output_colored.png}
\caption{Real and Fake feature images after wavelet transform. The top left, top right, bottom left, and bottom right portions indicate the approximate, horizontal detail, vertical detail, and diagonal detail coefficients of the original image, respectively. Different colormaps are applied to the horizontal, vertical, and diagonal detail coefficients, and their contrasts are increased for better visibility.}
\label{figure_3}
\end{figure}

\subsection{Details of the Model Architectures}
Recall that our proposed solution utilizes three neural network models for a weighted ensemble approach for our final prediction. We chose each of these models to focus on different aspects of a deepfake image so that the ensemble approach provides better accuracy than individual models operating alone. The details of the models in our proposed solution are presented below.

\subsubsection{ResNet-34 with SE Block}
We implemented a ResNet-34 model with SE block for the detection of deepfake images. Note that, we investigated with different Convolutional Neural Network (CNN)-based architectures -- shallow CNNs exhibited high bias, whereas deep CNNs caused vanishing gradients. The residual mapping of the ResNet-34 architecture addresses the issue of vanishing gradients, while the capacity of the ResNet-34 architecture is sufficient for capturing the intricate features for deepfake classification.

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.42]{Figures/Residual_block.pdf}
\caption{Building block of residual learning}
\label{figure_4}
\end{figure}

Note that residual learning works by reformulating the target function $\text{H}(x)$ as $\text{F}(x) = \text{H}(x) - x$, where $x$ is the input~\cite{he2016deep}. Instead of directly learning $\text{H}(x)$, the network learns the residual $\text{F}(x)$. The final output is computed as $\text{F}(x) + x$, combining the learned residual with the input via a skip connection. This mechanism allows each layer to focus on learning the residual adjustments needed to improve upon the input. The building block of residual mapping is shown in Figure~\ref{figure_4}. 
Our ResNet-34-based architecture is implemented with two simple principles following the traditional structure of this model: i) When maintaining the same output feature map size, each layer will consistently have the same number of filters. ii) Additionally, when the feature map size is halved, the number of filters is effectively doubled. Based on these principles, shortcut connections are implemented on a generic CNN architecture to obtain the desired residual network.\\

\begin{figure}[htbp]
\centering
\includegraphics[scale=0.45]{Figures/Figure_ResNet.pdf}
\caption{ResNet-34 architecture with Squeeze and Excitation}
\label{figure_5}
\end{figure}

\noindent\textbf{Squeeze and Excitation (SE) Block.} The SE block is introduced as a lightweight attention mechanism to adaptively recalibrate each feature channel's importance~\cite{hu2018squeeze}. The SE block is placed after the final sequential layer (layer 4) of the ResNet-34 model to enhance feature representation by modeling channel-wise dependencies. It is responsible for two main operations:
\begin{itemize}
    \item Squeeze: Global spatial information is aggregated using global average pooling to generate channel-wise descriptors. Given an input feature map $X \in \mathbb{R}^{H \times W \times C}$, the squeezed feature vector $z \in \mathbb{R}^{C}$ is obtained by squeezing function $F_{sq}(.)$ as:
    \begin{equation}
        z_c = \frac{1}{H W} \sum_{i=1}^{H} \sum_{j=1}^{W} X_c(i,j), \quad c = 1, 2, ..., C,
    \end{equation}
    where $X_c(i,j)$ represents the activation at spatial location $(i,j)$ in channel $c$.
    \item Excitation: The descriptors generated from the Squeeze block are passed through a lightweight two-layer fully connected network with a bottleneck structure to model non-linear channel independencies. The squeezed feature vector is passed through a gating mechanism with two fully connected layers and a non-linearity (typically ReLU and Sigmoid) to learn channel-wise dependencies:
\begin{equation}
    s = \sigma(W_2 \delta(W_1 z)),
\end{equation}
where $W_1 \in \mathbb{R}^{\frac{C}{r} \times C}$ and $W_2 \in \mathbb{R}^{C \times \frac{C}{r}}$ are the weights of the two fully connected layers, $\delta(\cdot)$ represents the ReLU activation function, and $\sigma(\cdot)$ is the Sigmoid function that outputs a channel-wise scaling factor $s \in \mathbb{R}^{C}$. The final output is obtained by rescaling the input feature maps $X$ using the channel-wise attention weights:
\begin{equation}
    \tilde{X}_c = s_c X_c, \quad c = 1, 2, ..., C 
\end{equation}
This step selectively enhances informative channels while suppressing less important ones. We denoted this operation as a function $F_{ex}(.)$, as depicted in Fig. \ref{figure_5}.
\end{itemize}
The details of the ResNet-34 architecture including the output shapes of each layer and the number of parameters are shown in Table \ref{tab:resnet34_se}.

\begin{table}[t]
\centering
\caption{Details of the ResNet-34 Architecture with SE Block}
\begin{tabular}{lll}
\hline
\textbf{Layer (type)} & \textbf{Output Shape} & Param \#\\ \hline
Conv2d, BatchNorm2d, ReLU & [64, 112, 112] & 9,536 \\ 
MaxPool2d & [64, 56, 56] & 0 \\ 
BasicBlock-1 $\times$ 3 & [64, 56, 56] & 221,952 \\ 
\quad|Conv2d & [64, 56, 56] & 36,864 \\
\quad|BatchNorm2d + ReLU & [64, 56, 56] & 128 \\
\quad|Conv2d & [64, 56, 56] & 36,864 \\
\quad|BatchNorm2d + ReLU & [64, 56, 56] & 128 \\
BasicBlock-2 $\times$ 4 & [128, 28, 28] & 1,116,416 \\ 
BasicBlock-3 $\times$ 6 & [256, 14, 14] & 6,822,400 \\ 
BasicBlock-4 $\times$ 3 & [512, 7, 7] & 13,114,368 \\ 
SEBlock (Global Avg Pool + FC) & [512, 7, 7] & 33,312 \\ 
AdaptiveAvgPool2d & [512, 1, 1] & 0 \\ 
Linear & [2] & 1,026 \\ \hline
\multicolumn{2}{c} {Total Parameters} & 21,319,010 \\
\multicolumn{2}{c} {Trainable Parameters} & 8,204,642 \\ 
\multicolumn{2}{c} {Non-Trainable Parameters} & {13,114,368}
\\ \hline
\end{tabular}
\label{tab:resnet34_se}
\end{table}

\subsubsection{Data Efficient Image Transformer (DeiT) Model }
Unlike conventional Recurrent Neural Networks (RNNs) that process sequential data step by step, transformers process all inputs simultaneously, relying on a self-attention mechanism. This simultaneous processing allows transformers to learn long-range dependencies and global contexts of data~\cite{khan2022transformers}. It has been shown that the data-efficient image transformer, which is based on the backbone of the vision transformer, works well for cases where the training data is limited~\cite{touvron2021training}. Here, an image is divided into a grid of smaller patches as a sequence of input tokens ($N$ number of $16 \times 16$ patches), and each patch is associated with a vector through linear embedding. After adding positional encoding for each patch, the vectors are fed into the transformer encoder, where the self-attention mechanism helps to learn the relationship among all the patches~\cite{dosovitskiy2020image}. Note that the attention mechanism is based on a trainable associative memory of (key, value) pairs. A query vector $\mathbf{Q}$ is matched against key vectors $\{\mathbf{K}\}$ using dot products, scaled by $\sqrt{d}$ and normalized via a softmax function to produce weights. These weights are used to compute a weighted sum of value vectors $\{\mathbf{V}\}$, producing an output matrix as:
\begin{equation*}
    \text{attention}\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \text{softmax}\left(\frac{\mathbf{Q} \mathbf{K}^\top}{\sqrt d}\right)\mathbf{V}.
\end{equation*}
In self-attention, the queries, keys, and values are derived from the same input sequence via linear transformations~\cite{vaswani2017attention}. Multi-head self-attention (MSA) applies $h$ separate attention functions (heads) in parallel. Each head provides a sequence of outputs, which are concatenated and projected back to the original dimension. This allows the model to attend to different parts of the input sequence simultaneously.

\begin{figure}[t]
\centering
\includegraphics[scale=0.45]{Figures/Figure_5_V_1.pdf}
\caption{Data Efficient Image Transformer (DeiT) architecture}
\label{figure_6}
\end{figure}

Note that CNN models primarily capture local contexts and eventually may capture global contexts through hierarchical learning. To this end, DeiT is an effective alternative to traditional CNNs in learning global features~\cite{khan2022transformers}. It uses knowledge distillation techniques and optimization strategies to improve generalization. DeiT learns from a pre-trained CNN model (that is, the RegNetY-16GF model~\cite{radosavovic2020designing}) and uses a distillation token during training, allowing it to learn faster and more efficiently with less data~\cite{touvron2021training}.

The DeiT architecture introduces a new hard distillation mechanism where they treat the hard decisions of the teacher model $y_t$ as true labels along with the actual true labels, $y$. If $Z_t$ are the logits of the teacher model and $Z_s$ are the logits of the student model, then the hard decision of the teacher model is defined as
\begin{equation}
y_t = \text{argmax}_c (Z_t(c))
\end{equation}
Here $c$ is the class index. Using this prediction, the final loss function for hard distillation is treated as
\begin{equation}
L^{\text{hard dist}}_\text{global} = \frac{1}{2}L_{CE}(\text{softmax}(Z_s), y) + \frac{1}{2}L_{CE}(\text{softmax}(Z_s), y_t)
\end{equation}
The first term of the equation can be thought of as $L_\text{cross-entropy}$ and the second term as $L_\text{teacher}$ depicted in Figure~\ref{figure_6}

As the competition dataset lacks enough samples to generalize a ViT, we chose DeiT to capture global contexts effectively, despite it being computationally costly. There exist different DeiT model variations based on the number of heads and embedding resolutions. In our proposed solution, we have implemented the DeiT-B model, which uses the ViT-B model as its backbone. It has 768 embedding dimensions with 12 heads containing a total of 86 million parameters~\cite{touvron2021training}. The DeiT model architecture is shown in Figure \ref{figure_6} and the number of parameters and the output shapes of each layer are shown in Table \ref{tab:DeiT}.

\begin{table}[t]
\centering
\caption{Details of the Data Efficient Image Transformer (DeiT) Model}
\begin{tabular}{lll}

\hline
\textbf{Layer (type)} & \textbf{Output Shape} & \textbf{Param \#} \\ \hline
Conv2d & [764, 14, 14] & 590,592\\ 
Patch Embed & [196, 768] & 0\\ 
Dropout & [197, 768] & 0\\
Identity-1 & [197, 768] & 0\\
LayerNorm $\times$ 25 & [197, 768] & 38,400\\ 
Linear-1 $\times$ 12 & [197, 2304] & 21,261,312\\
Identity-2 & [12, 197, 64] & 0\\
Linear-2 $\times$ 12 & [197, 768] & 35,407,872\\ 
Attention $\times$ 12 & [197, 768] & 0\\
Linear-3 $\times$ 12 & [197, 3072] & 28,348,416\\ 
GELU $\times$ 12 & [197, 3072] & 0 \\ 
Dropout & [197, 3072] & 0 \\
Mlp $\times$ 12 & [197, 768] & 0\\
Identity and Dropout & [768] & 0\\
Linear-4 & [2] & 1,538 \\
Vision Transformer & [2] & 0\\ \hline
\multicolumn{2}{c} {Total Parameters} & 85,648,130 \\
\multicolumn{2}{c} {Trainable Parameters} & 85,648,130 \\ 
\multicolumn{2}{c} {Non-Trainable Parameters} & 0
\\ \hline
\end{tabular}
\label{tab:DeiT}
\end{table}

\subsubsection{XceptionNet Model}
XceptionNet is based on the concept of depthwise separable convolutions, which enhance computational efficiency while maintaining accuracy. In depthwise convolution, a single convolutional filter is applied to each input channel. Following this, pointwise convolution is used to combine the outputs of the depthwise convolution into a linear form~\cite{chollet2017xception}. XceptionNet features a fully convolutional architecture for simplicity and efficiency, leverages residual connections to improve gradient flow, and employs deep feature extraction.
% \begin{figure}[htbp]
% \centering
% \includegraphics[scale=0.34]{Figures/Xception_architecture.pdf}
% \caption{XceptionNet architecture \textcolor{red}{(need landscape mode)}}
% \label{figure_8}
% \end{figure}
\begin{figure}[htbp] 
    \centering
    % First row
    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Entry_flow.pdf}
        \caption{Entry Flow}
        \label{fig:figure7a}
    \end{subfigure}
    
    \begin{subfigure}{0.4\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Middle_flow.pdf}
        \caption{Middle Flow}
        \label{fig:figure7b}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.5\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/Exit_flow.pdf}
        \caption{Exit Flow}
        \label{fig:figure7c}
    \end{subfigure}
    \caption{Details of XceptionNet architecture}
    \label{fig:figure_7}
\end{figure}

As shown in Figure~\ref{fig:figure_7}, our implemented XceptionNet has 36 convolutional layers, structured into 14 modules with residual connections, except for the first and last modules \cite{chollet2017xception}. It has three primary sections—Entry Flow, Middle Flow, and Exit Flow. The Entry Flow layer consists of convolutional layers followed by max pooling and is responsible for reducing spatial dimensions and extracting low-level features. The Middle Flow layer contains multiple depthwise convolution blocks with residual connection, focusing on high-level features. The Exit Flow layer includes more depthwise separable convolutions and reduces spatial dimensions further. We modified the fully connected layer from the original architecture to match our binary classification problem. The summary of the output shapes and number of parameters of each block and layer is shown in Table \ref{tab:Xception}.

\begin{table}[hbtp]
\centering
\caption{Details of the XceptionNet Model}
\begin{tabular}{@{}lll@{}}
\hline
\textbf{Layer (type)} & \textbf{Output Shape} & \textbf{Param \#}  \\ \hline
Conv2d & [32, 149, 149] & 864 \\
BatchNorm2d + ReLU  & [32, 149, 149] & 64 \\
Conv2d & [64, 147, 147] & 18,432 \\
BatchNorm2d + ReLU & [64, 147, 147] & 128 \\
Block-1 & [128, 74, 74] & 35,264 \\
\quad|SeparableConv2d & [128, 147, 147] &  8,768 \\
\quad|BatchNorm2d + ReLU& [128, 147, 147] &  256 \\
\quad|SeparableConv2d & [128, 147, 147] &  17,536 \\
\quad|BatchNorm2d & [128, 147, 147] &  256 \\
\quad|MaxPool2d & [128, 74, 74] &  0 \\ 
\quad|Conv2d & [128, 74, 74] &  8,192 \\ 
\quad|BatchNorm2d & [128, 74, 74] &  256 \\ 
Block-2 & [256, 37, 37] & 136,064 \\
Block-3 & [728, 19, 19] & 915,944 \\
Block-4 $\times$ 8 & [728, 19, 19] & 12,911,808 \\
Block-5 & [1024, 10, 10] & 2,039.584 \\
SeparableConv2d & [1536, 10, 10] & 1,582,080 \\
BatchNorm2d + ReLU & [1536, 10, 10] & 3072 \\
SeparableConv2d & [2048, 10, 10] & 3,159,552 \\
BatchNorm2d + ReLU & [2048, 10, 10] & 4096 \\
SelectAdaptivePool2d & [2048] & 0 \\
Linear & [2] & 4098 \\
\\\hline
\multicolumn{2}{c}{Total Parameters} & 20,811,050\\
\multicolumn{2}{c}{Trainable Parameters} & 20,811,050\\
\multicolumn{2}{c}{Non-Trainable Parameters} & 0\\ \hline
\end{tabular}
\label{tab:Xception}
\end{table}

\subsubsection{Weighted Ensemble Mechanism}
Finally, we implemented a weighted ensemble mechanism to combine the predictions of the aforementioned three models. The associated weights for the ResNet-34 with SE, DeiT, and Xception with wavelet transform are 0.36, 0.45, and 0.19, respectively, based on our parameter search. The final architecture is shown in Figure \ref{figure_9}, and the total number of trainable and non-trainable parameters in the final architecture is shown in Table \ref{Final parameters}. 
\begin{figure}[htbp]
\centering
\includegraphics[scale=0.45]{Figures/Figure_7_V1.pdf}
\caption{The proposed weighted ensemble mechanism}
\label{figure_9}
\end{figure}


\begin{table}[htbp]
    \centering
    \caption{Number of parameters in the final weighted ensemble mechanism}
    \label{Final parameters}
    \begin{tabular}{lccc}
        \hline
        \textbf{Model} & \textbf{Total Parameters} & \textbf{Trainable} & \textbf{Non-trainable} \\
        \hline
        ResNet & 21,319,010 & 8,204,642 & 13,114,368 \\
        DeiT & 85,648,130 & 85,648,130 & 0 \\
        Xception &  20,811,050 & 20,811,050 & 0 \\
        \hline
        Total & 127,778,990 & 114,664,622 & 13,114,368 \\
        \hline
    \end{tabular}
\end{table}

\section{Experimental Results}
\begin{figure}[htbp] 
    \centering
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/confusion_matrix.pdf}
        \caption{}
        \label{confusion matrix}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/roc_curve.pdf}
        \caption{}
        \label{ROC curve}
    \end{subfigure}
    \caption{(a) Confusion matrix and (b) ROC curve for validation set}
    \label{fig:Ensemble_valid_metrics}
\end{figure}

In this section, we demonstrate the performance of our proposed solution on the competition dataset. We note that investigation and experimentation with different architectures helped us make informed decisions to reach our proposed solution. More specifically, we started experimenting with simpler CNN models. As mentioned before, the performance of CNN models, such as the VGG-19, indicated insufficient capacity and indicated the necessity of deeper networks. Additionally, residual networks, such as the ResNet-50, raised overfitting issues. These results motivated us to choose a ResNet-34-based architecture to strike a balance between high-bias and high-variance regimes. We investigated the performance of the convolutional block attention module (CBAM) and SE and opted for SE block based on the generalization performance on the given dataset. Lastly, we investigated the effect of different handcrafted feature extraction methods on the performance and opted for wavelet feature extraction for our XceptionNet model. 

\begin{figure}[htbp] 
    \centering
    % First row
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_ResNet-34SE_untrained.pdf}
        \caption{SEResNet (Untrained)}
        \label{fig:allfigures_tSNE1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_DeiT_untrained.pdf}
        \caption{DeiT (Untrained)}
        \label{ffig:allfigures_tSNE3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_Xception_untrained.pdf}
        \caption{Xception (Untrained)}
        \label{fig:allfigures_tSNE5}
    \end{subfigure}

    % Second row
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_ResNet-34SE_trained.pdf}
        \caption{SEResNet (Trained)}
        \label{fig:allfigures_tSNE2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_DeiT_trained.pdf}
        \caption{DeiT (Trained)}
        \label{fig:allfigures_tSNE4}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/t_SNE_Xception_trained.pdf}
        \caption{Xception (Trained)}
        \label{fig:allfigures_tSNE6}
    \end{subfigure}

    \caption{t-SNE plot of three models before and after training for randomly selected 4000 real and fake images using perplexity = 40}
    \label{fig:allfigures_tSNE}
\end{figure}

In essence, gradually increasing the complexity of networks helped us understand the shortcomings of different architectures for our specified task and led us to choose the model architectures based on their generalization capability with a relatively small dataset. To further reduce the variance, we employed a weighted ensemble of ResNet-34 with SE, DeiT, and XceptionNet to generate the final results. We present a comparative analysis of the results of different model variations and configurations that guided us to our final design decisions in Table. \ref{table_5}. We argue that these results serve as empirical evidence of our choice of neural network architectures, and the weighted ensemble approach.\\


% \begin{table}[htbp]
%     \centering
%     \caption{Comparative Analysis of Different Model Variations}
%     \label{table_5}
%     \begin{tabular}{clc}
%         \hline
%         \textbf{No.} & \textbf{Model} & \textbf{Validation accuracy} \\
%         \hline
%         1 & Xception-Net~\cite{chollet2017xception} & 61.48\% \\
%         2 & VGG-19~\cite{simonyan2014very} & 62.25\% \\
%         3 & ResNet-50~\cite{he2016deep} & 69.00\% \\
%         4 & DCT with ResNet-34 & 71.48\% \\
%         5 & ResNet-34~\cite{he2016deep} & 80.32\% \\
%         6 & ResNet-34 with CBAM & 85.19\% \\
%         7 & Xception-Net with Wavelet Transform & 87.76\%\\
%         8 & ResNet-34 with SE & 88.90\% \\
%         9 & Data Efficient Image Transformer~\cite{touvron2021training} & 89.32\% \\
%         10 & Maj. Voting (ResNet, DeiT, Xception) & 91.34\%\\
%         11 &  Maj. Voting (ResNet, DeiT) & 91.80\%\\
%         12 & \textbf{Weighted ensem. (proposed) (ResNet, DeiT, Xception)}  & \textbf{93.23}\%\\
%         \hline
%     \end{tabular}
% \end{table}
\begin{table}[t]
    \centering
    \scriptsize % Reduce font size
    \caption{Comparative Analysis of Different Model Variations}
    \label{table_5}
    \resizebox{\textwidth}{!}{ % Automatically scale to fit text width
    \begin{tabular}{p{6.5cm} c} % Adjust column width
        \hline
        \textbf{Model} & \textbf{Validation Acc.} \\
        \hline
        Xception-Net~\cite{chollet2017xception} & 61.48\% \\
        VGG-19~\cite{simonyan2014very} & 62.25\% \\
        ResNet-50~\cite{he2016deep} & 69.00\% \\
        DCT with ResNet-34 & 71.48\% \\
        ResNet-34~\cite{he2016deep} & 80.32\% \\
        ResNet-34 with CBAM & 85.19\% \\
        Xception-Net with Wavelet Transform & 87.76\% \\
        ResNet-34 with SE & 88.90\% \\
        DeiT~\cite{touvron2021training} & 89.32\% \\
        Maj. Voting (ResNet, DeiT, Xception) & 91.34\% \\
        Weighted Ensem. (ResNet, DeiT) & 91.80\% \\
        \textbf{Weighted Ensem. (ResNet, DeiT \& Xception) (Proposed)} & \textbf{93.23\%} \\
        \hline
    \end{tabular}
    }
\end{table}



\noindent\textbf{Hyperparameters. }For the training of our models, we have used the ADAM optimizer. We have used accuracy, precision, recall, and F1 score as our evaluation metrics. Since the deepfake detection problem is a binary classification problem, we used binary categorical cross-entropy as our loss function. Using the validation set to observe the accuracy of the proposed architecture, each image took approximately 0.03515625 sec on a Tesla P100 GPU. The hyperparameters used for training are presented in Table \ref{tab:Hyperparameters}.


\begin{table}[t]
    \centering
    \caption{Hyperparameters for model training}
    \label{tab:Hyperparameters}
    \begin{tabular}{lc}
        \hline
        \textbf{Hyperparameters} & \textbf{Details} \\
        \hline
        Batch size & 32 \\
        Number of Epochs & -- \\
        \quad - ResNet-34 with SE & 10 \\
        \quad - DeiT & 5 \\
        \quad - XceptionNet & 10 \\
        Learning rate & -- \\
        \quad - Learning rate scheduler & ReducedLROnPlateau \\
        \quad - Initial learning rate & 0.0001\\
        \quad - Reduction factor & 0.1\\
        \quad - Patience & 3\\
        \hline
    \end{tabular}
\end{table}
%The initial learning rate was set to be 0.0001 and the learning rate scheduler reduced the learning rate by a factor of 0.1, if the validation loss did not increase for 3 epochs. For each training session, the batch size was 32.    \\



We have utilized the \texttt{NumPy} and \texttt{Pandas} libraries for preprocessing and data manipulation tasks, and PyTorch~\cite{paszke2019pytorch} for model implementation, training, and validation. More specifically, we used \texttt{torch} for core functionality, \texttt{torch.nn} for neural networks, \texttt{torch.optim} for optimization, and utilities like \texttt{torch.utils.data.Dataset}, \texttt{DataLoader}, and \texttt{random\_split}. From \texttt{TorchVision}, we used \texttt{transforms} for image transformations and \texttt{models} for pre-trained architectures. For performance metrics, we applied \texttt{Scikit-learn} (\texttt{precision\_score}, \texttt{recall\_score}, and \texttt{f1\_score}). Progress tracking was managed with \texttt{TQDM}, and \texttt{TorchSummary} provided model summaries. Image processing utilities include \texttt{OpenCV}, while \texttt{PyWavelets} was utilized for wavelet transformations. Additionally, \texttt{Timm} was used to access advanced pre-trained models, and the \texttt{Random} module was applied for generating randomness.\\

%%%%%%%%%%%%%%%%%%% GradCAM View %%%%%%%%%%%%%%%%%%%%
\begin{figure}[htbp] 
    \centering
    % First row
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ResNet_predicted_fake_1.pdf}
        \caption{SEResNet}
        \label{fig:gradCAMfigure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/DeiT_predicted_fake_1.pdf}
        \caption{DeiT}
        \label{fig:gradCAMfigure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/xception_predicted_fake_2.pdf}
        \caption{Xception}
        \label{fig:gradCAMfigure5}
    \end{subfigure}

    % Second row
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ResNet_predicted_fake_2.pdf}
        \caption{SEResNet}
        \label{fig:gradCAMfigure2}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/DeiT_predicted_fake_2.pdf}
        \caption{DeiT}
        \label{fig:gradCAMfigure4}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/xception_predicted_fake_1.pdf}
        \caption{Xception}
        \label{fig:gradCAMfigure6}
    \end{subfigure}

    \caption{Importance map shown by Grad-CAM for different models}
    \label{fig:allfigures_GradCam}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%% GradCam False Case %%%%%%%%%%%%%%
\begin{figure}[htbp] 
    \centering
    % First row
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/ResNet_grad_cam_real_img_predicted_fake.pdf}
        \caption{SEResNet}
        \label{fig:wrong_prediction_figure1}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/DeiT_grad_cam_predicted_real.pdf}
        \caption{DeiT}
        \label{fig:wrong_prediction_figure3}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.31\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Figures/xception_gradcam_real_img_predicted_fake.pdf}
        \caption{Xception}
        \label{fig:wrong_prediction_figure5}
    \end{subfigure}
    \caption{Grad-CAM view of a few failed classifications of our proposed model}
    \label{fig:gradcam_failed_case}
\end{figure}


\noindent\textbf{Result Analysis. }As mentioned before, we evaluated our weighted ensemble model for multiple evaluation metrics, including accuracy, precision, recall, F1-score, area under receiver operating characteristic (AU-ROC) curve, equal error rate, and average precision. While calculating these metrics, fake images were considered as class 0 and real images as class 1. The performance of our model on these metrics is presented in Table \ref{tab:final results} and the confusion matrix and the ROC curve are shown in Figure \ref{fig:Ensemble_valid_metrics}. It is evident from the evaluation indices that our proposed weighted ensemble architecture performs very well on the competition dataset, which underlines the generalization capability of the proposed approach.
\begin{table}[t]
    \centering
    \caption{Performance of the Final Ensemble Architecture combining ResNet-34, DeiT, and XceptionNet}
    \label{tab:final results}
    \begin{tabular}{lc}
        \hline
        \textbf{Evaluation Metrics} & \textbf{Performance}\\
        \hline
        Accuracy & 93.23\% \\
        Precision & 0.9515 \\
        Recall &  0.9121 \\
        F1 score & 0.9314 \\
        Area under ROC & 0.9769 \\
        Equal Error Rate & 6.33\% \\
        Average Precision & 0.9744 \\
        \hline
    \end{tabular}
\end{table}
To investigate the performance of our ensemble model, we intend to visualize i) the regions of the images that the three models focus for classification, and ii) how the three models distinguish real and fake images before and after training. To that end, we note that the Grad-CAM visualizations show how different models focus on different facial regions of the same image for deepfake detection. In Figure~\ref{fig:allfigures_GradCam}, it can be seen that the ResNet-34 and Xception models focus on localized facial features, whereas DeiT displays diffuse attention patterns indicating a focus on global features. Each of the models focuses on different parts of the image, capturing unique and complementary information. When these diverse insights are combined into an ensemble, the overall prediction becomes more accurate, robust, and better at generalizing to new data. Additionally, we present the Grad-CAM view of some of the wrong predictions by the three models in Figure~\ref{fig:gradcam_failed_case}, showing the focus of irrelevant regions, leading to mis-classifications.

Next, we note that high-dimensional data can be visualized by projecting onto a two-dimensional space using the non-linear dimensionality reduction technique known as t-SNE (t-Distributed Stochastic Neighbor Embedding)~\cite{van2008visualizing}. By maintaining the local neighborhoods of data points, it shows the differences and similarities of data points in the learned feature space. As such, the t-SNE visualizations (Fig. \ref{fig:allfigures_tSNE}) offer valuable insights into the effectiveness of our models. For both real and fake images, we observe a single cohesive cluster for all three models. Notably, the cluster of fake images generated by two of our models, SEResNet and XceptionNet, is compact, despite these images being produced using various deepfake generation methods. The plots show that the training has effectively enhanced the ability to distinguish between real and fake image features for all three models. This, in-turn, highlights the generalization capability of our proposed ensemble model. After training, all three backbone models—SEResNet, DeiT, and XceptionNet—produced well-separated embeddings for real and fake images, as illustrated in Fig. \ref{fig:allfigures_tSNE2}, \ref{fig:allfigures_tSNE4}, and \ref{fig:allfigures_tSNE6}. 

\section{Conclusion}
In this work, we have proposed a weighted ensemble approach that takes advantage of the strengths of a ResNet-34-based model, a DeiT model, and an XceptionNet model for the accurate classification of deepfake images. The ResNet-34 model enabled extracting local features from an image, while the DeiT model specialized in capturing global features. Additionally, XceptionNet, combined with wavelet transform, effectively identified frequency-dependent artifacts. We emphasize that detecting deepfake images that are generated with a range of generative AI approaches is a challenging task. Our proposed model architectures captured the intricate properties of the input images to successfully distinguish between real and fake images and produced excellent results. Through t-SNE plots, we demonstrated that our model is capable of separating real and fake images into separable clusters. Additionally, we included a Grad-CAM visualizations to map the focus-regions of images to illustrate how our models analyze the images and which portions contribute to distinguishing between real and fake images. An interesting future work could be the incorporation of lightweight architectures and reduce the computational overhead, as well as an increase in the model's adaptability to newer and sophisticated deepfake generation methods. 
% combines the tested and analyzed multiple deep-learning models to detect deepfake images. After trying many different architectures, we found that combining three models(ResNet-34 with SE, DeiT, and Xception-Net) using an ensemble mechanism worked best. This combination was good because it could identify complex patterns, generalize, and achieve higher accuracy.  However, such an ensemble technique requires high computational overhead.   

% \section*{Acknowledgments}
% The authors would like to express their sincere gratitude towards the Department of Electrical and Electronic Engineering (EEE) of Bangladesh University of Engineering and Technology (BUET) for providing support for research. The authors would also like to express their gratitude to the organizers of SP Cup 2025 \cite{Spcup25} and IEEE for launching a competition that addresses an important issue in our time.


\bibliographystyle{elsarticle-num} 
 \bibliography{references}

%% else use the following coding to input the bibitems directly in the
%% TeX file.

% \begin{thebibliography}{00}

% %% \bibitem{label}
% %% Text of bibliographic item

% \bibitem{}

% \end{thebibliography}
\end{document}
\endinput
%%
%% End of file `elsarticle-template-num.tex'.
