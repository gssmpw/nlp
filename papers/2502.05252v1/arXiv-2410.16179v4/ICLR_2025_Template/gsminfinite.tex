\section{\sysb} 
\label{gsminfinite} 

In this section, we present key techniques that enable the synthetic dataset to be diverse in operations in \cref{reverseproblem}, LLM-understandable, and enable the evaluation to be free from non-reasoning factors in \cref{attachlanguage}. Then, we present synthetic problem generators capable of generating grade-school math questions with arbitrary reasoning difficulty and context length. Thus, we generate a suite of benchmarks called \sysb detailed in \cref{fullbenchmark}. 

\subsection{Challenge 1: How to Generate Implicit $-$ and $\div$ Operations?} 
\label{reverseproblem} 
\textbf{Firstly, we review why the abstract-instance construct can only generate ``+" and $\times$ but not ``$-$" and $\div$.} 
The key limitation of \citet{ye2024physicslanguagemodels21} abstract parameters and instance parameter design is that it is only able to generate problems with solutions with the ``forward" and constructive ordering. Shown in Figure \ref{mediumhard} (a) and (b), the design dictates that the specific and detailed variables should be defined before a more abstract variable. For example, ``the number of Lions in Zoo" and ``the number of Monkeys in Zoo" have to be defined before ``Total Animal in Zoo" is defined. 
The ``forward" ordering leads to the inability to generate implicit '-' operations for 2-entity problems and implicit ``$\div$" operations for 3-entity problems that require the more abstract variables, e.g. ``Total Animal in Zoo", to be defined before a more specific variable, e.g. ``the number of Monkeys in Zoo". 

\textbf{To generate all four kinds of implicit operations, we introduce a ``reverse mode" to generate the computation graph.} 
Essentially, the graph construction still continues as before: starting with specific detailed variables and growing to incorporate more abstract variables. When it completes and we know all the values of nodes in the graph, we then randomly mask out specific initial low-level variables and force the solution to traverse in the reverse direction as in the "forward" ordering. 
We present the illustration of data generation in Figures \ref{mediumhard} (a) and (b) for the 2-entity and 3-entity, respectively. 
% Now, we have a problem generator capable of generating all the possible operators appearing in GSM-8K. 
However, for 3-entity problems, it can result in quadratic equations leading to multiple possible solutions. We develop some techniques that effectively reduce the probability of the situation. Details are presented in \cref{sometips} of the Appendix. 
\label{reverse_problem} 

\subsection{Challenge 2: How to Ensure LLM-understandable Problem Generation?} 
\label{attachlanguage} 
\begin{wrapfigure}{r}{0.35\textwidth} 
    \includegraphics[width=0.35\textwidth]{figures/illustrationofarea} 
    \caption{Area Under Curve Metrics is Used to Compare between LLM Performance.} 
    \label{areacurve} 
\end{wrapfigure} 
Mapping computation graphs to natural language is critical for evaluating LLMs' reasoning capabilities. \textbf{To automate this process, we develop inter-swappable templates that enhance linguistic diversity while maintaining clarity.} Several key considerations inform our design. 

First, Certain syntactic forms, such as possessive constructions (e.g., A’s B), are straightforward to encode but can mislead LLMs due to their deviation from natural language. For example, South Zoo’s Penguin is restructured as Penguin in South Zoo, and South Zoo’s Adult Penguin’s Average Number of Newborn Children becomes Average Number of Newborn Children per Adult Penguin in South Zoo. Through extended trial and error, variable names with different entity numbers are \textbf{presented naturally} in all prepared templates. 

Second, the number of constraints applied to the random graph generation process is as little as possible, forcing templates to \textbf{enforce unit consistency} across two-entity and three-entity variables to enable assignment between these two. For instance, ``The average number of animal children per penguin in South Zoo" must share a unit with ``The number of penguins in South Zoo" to allow variable assignments.

Third, to ensure real-world knowledge doesn't confuse the LLM's decision, \textbf{we avoid specific real-world locations, people's names, and festival names} from appearing in the template. 
Based on these constricts, we propose three different templates that meet real-world templates: children-animal-zoo, teachers-school-district, and awards-movies-festival. We present in Appendix \ref{templates} an ablation study showing that three templates are consistent in overall performance with only minor fluctuations when evaluated using Llama-3.1-8B-Instruct. At each op, equal problems are tested for both constructive ordering (forward) and reverse ordering (reverse). 

\subsection{Benchmark Details} 
\label{fullbenchmark} 
\begin{table*}[ht] 
\centering 
\footnotesize 
% \caption{Performance Metrics for Various Models Across Subtasks and Detailed Statistics on Hard Subtask} 
\caption{18 selected models are evaluated on \sysb zero-noise benchmarks using Area-Under-Curve (AUC), which is computed by taking the Riemann Sum of accuracy versus op count from 2 to when the model accuracy drops below 5\%. We also present detailed statics of the first op number for the model to have an accuracy lower than 50\%, 10\%, and the average accuracy of the first 30 ops settings. Besides, we also highlight the \textcolor{yellow}{reasoning models}, \textcolor{green}{linear attention hybrid models}, and \textcolor{blue}{SSM hybrid models}. Due to space constraint, ``Mistral-Large-Instruct-2411" is shortened as ``Mistral-Large";``Claude-3.5-Sonnet" and ``Claude-3.5-Haiku" has version number 20241022; ``GPT-4o-2024-11-20" is shortened as ``GPT-4o" and ``GPT-4o-mini-2024-07-18" is shortened as ``GPT-4o-mini".} 
\begin{tabular}{@{}lccc|ccc|c@{}}
\toprule
\multirow{2}{*}{\textbf{Models}} & \multicolumn{3}{c|}{\textbf{Three Subtasks}} & \multicolumn{3}{c|}{\textbf{Detailed Statistics on Hard Subtask}} & \textbf{Score} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{Symbolic} & \textbf{Medium} & \textbf{Hard} & \textbf{1st$<$50\% op} & \textbf{1st$<$10\% op} & \textbf{Avg. Acc op$\leq$30} & \textbf{Avg.$\uparrow$} \\
\midrule
\rowcolor{yellow!20} 
DeepSeek-R1 & 7280.0 & 9750.85 & 8573.8 & 100 & $>$130 & 0.9427 & 8534.88 \\ 
\rowcolor{yellow!20} 
GPT-o3-mini & 6690.0 & 8335.66 & 5769.96 & 70 & 110 & 0.9423 & 6931.88 \\
\rowcolor{yellow!20} 
GPT-o1-mini & 5060.0 & 6054.91 & 3738.43 & 50 & 90 & 0.8397 & 4951.11 \\
DeepSeek-V3 & 4310.0 & 4100.81 & 2407.86 & 24 & 55 & 0.6669 & 3606.22 \\
\rowcolor{yellow!20} 
QwQ-32B-preview & 3530.0 & 3205.75 & 1846.19 & 21 & 50 & 0.5403 & 2860.65 \\
Gemini-1.5-Pro-002 & 2547.0 & 3659.59 & 2318.28 & 26 & 45 & 0.6924 & 2841.62 \\
Claude-3.5-Sonnet & 2161.0 & 3281.8 & 2115.79 & 26 & 40 & 0.6758 & 2519.53 \\
Mistral-Large & 2332.5 & 2879.92 & 2310.49 & 24 & 50 & 0.6645 & 2507.64 \\
Qwen2.5-72B-Instruct & 2048.0 & 2496.81 & 2016.38 & 21 & 40 & 0.5433 & 2187.06 \\
GPT-4o & 2379.0 & 2457.37 & 1451.54 & 18 & 30 & 0.5064 & 2095.97 \\
Gemini-1.5-Flash-002 & 1970.0 & 1478.75 & 1274.25 & 13 & 30 & 0.4460 & 1574.33 \\
Llama3.1-70B-Instruct & 1769.0 & 1650.25 & 1205.25 & 15 & 30 & 0.4314 & 1541.50 \\
\rowcolor{palegreen!20} 
MiniMax-Text-01 & 1618.5 & 1712.64 & 1178.51 & 14 & 30 & 0.4213 & 1503.22 \\
% llama-3.1-405b-instruct & 1557.0 & 1321.54 & 950.0 & 11 & 20 & 0.3409 & 1276.18 \\ 
GPT-4o-mini & 1389.0 & 1406.5 & 913.89 & 12 & 22 & 0.3094 & 1236.46 \\
Claude-3.5-Haiku & 897.0 & 1053.16 & 784.34 & 10 & 22 & 0.2910 & 911.50 \\
Qwen2.5-7B-Instruct & 786.95 & 886.75 & 618.5 & 7 & 19 & 0.2257 & 764.07 \\
Llama3.1-8B-Instruct & 462.0 & 786.5 & 606.5 & 6 & 17 & 0.2186 & 618.30 \\
\rowcolor{skyblue!20} 
Jamba-1.5-Large & 856.0 & 485.13 & 466.4 & 6 & 26 & 0.1828 & 602.51 \\
\bottomrule
\end{tabular} 
\label{zerocontextleaderb} 
\end{table*} 

With the synthetic problem generators detailed in Section \ref{gsminfinite}, we then use them to generate problems to build a suite of reasoning tasks with increasing complexity. For the brevity of reference, we refer to the generated problems with only explicit operations as ``Easy", the generated problems with 2-entity variables at maximum as ``Medium", and the generated problems with 3-entity variables at maximum as "Hard. 

Ideally, when evaluating an LLM, we want to evaluate all difficulty levels, from the most basic logic complexity to when it completely fails to solve any problem. For the Easy subset of problems, it usually leads to large operation counts for powerful LLMs. 
However, although complexity-wise not challenging, LLMs trained with internal COT tend to generate very long arguments, saturating their API output generation limit (4K for many models). Thus, we observe a sudden decay in accuracy in large ops, not because of LLMs' ability bottlenecks, but because of the above-mentioned nuance. Thus, we make a tweak to its problem: Instead of asking the LLM to find the value of one variable, we ask the LLM to find all the variables that have some value specified, effectively increasing the difficulty of the problem. 

\begin{wraptable}{r}{0.5\textwidth} 
\centering 
\caption{10 selected models are evaluated on \sysb Long Context benchmarks using Average AUC of Symbolic, Medium, and Hard. We evaluated models on 8K, 16K, and 32K context. Although our pipeline is capable of generating longer problems, the resource required to go further for larger models beyond our acceptance, while smaller models effectively has completely failed.} 
\footnotesize 
\begin{tabular}{@{}lcccc@{}}
\toprule
\textbf{Model} & \textbf{8K} & \textbf{16K} & \textbf{32K} & \textbf{Avg.$\uparrow$} \\
\midrule
Gemini-1.5-Pro-002 & 1182.43 & 896.31 & 812.96 & 963.9 \\
Qwen2.5-72B-Instruct & 927.33 & 681.53 & 563.65 & 724.17 \\
Mistral-Large & 914.49 & 563.73 & 319.21 & 599.14 \\
DeepSeek-V3 & 935.10 & 477.02 & 313.66 & 575.2 \\
Gemini-1.5-Flash-002 & 673.88  & 476.72 & 377.38 & 509.3 \\
Llama3.1-70B-Instruct & 479.00 & 394.50 & 355.5 & 409.67 \\
MiniMax-Text-01 & 481.32  & 359.56 & 325.95 & 388.94 \\
GPT-4o-mini & 401.00  & 337.81 & 275.63 & 338.15 \\
Qwen2.5-7B-Instruct & 248.00  & 211.50 & 196.17 & 218.56 \\
Llama3.1-8B-Instruct & 183.67  & 149.50 & 109.45 & 147.54 \\

\bottomrule
\end{tabular} 
\label{longcontextleaderb} 
\vspace{-5mm} 
\end{wraptable} 

For the modified Easy subset, we keep the generated problem in the most basic form: symbolic assignment. The typical problem statement then becomes "v1235 equals v1468 plus 1." Since the modified problem is not easier compared to Medium and Hard, we now call it "Symbolic". For Medium and Hard, we used all three templates and mixed the generated problems together to ensure diversity. For reporting LLM performance, we use \textbf{Area Under Curve (AUC)}, as shown in \cref{areacurve}, which is computing a Riemann sum over the LLM's performance in accuracy versus the number of operations from 2 to when its performance is lower than 5\%. 

We prepare zero-noise, 8K, 16K, and 32K in the benchmarks. The existing generation pipeline is capable of generating in $>$ 16M context, but the smaller 70B level models effectively failed in the 32K context already, while evaluating larger ones brings cost beyond our acceptance.
