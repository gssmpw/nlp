\section{Evaluation} 

In this section, we present comprehensive evaluations of various LLMs on \sysb. Specifically, the section is organized as follows: 
\begin{itemize}[leftmargin=*, itemsep=0pt, parsep=0pt, topsep=0pt] 
    \item \cref{leaderboardtwo} presents the complete comparison of LLMs on both zero-noise tasks and long-context tasks. Besides the main leaderboard, we further share four interesting findings. 
    \item \cref{sigmoid} reveals that the sigmoid function generally fits the LLM performance degradation to the increasing ops well. 
    \item \cref{forwardreverseeval} shows that LLMs generally and consistently perform forward problems better than reverse problems. (Defined in \ref{reverse_problem}) 
    \item \cref{longdegradation} discuss that various LLM performance decay over longer context and further ablation of the noise. 
    % \item \cref{resam} presents that on \sysb, exponentially increasing inference compute leads to a linear increase in AUC. 
    \item \cref{resam} shows that on \sysb, exponential increase in inference compute yields linear AUC gains. 
\end{itemize} 

\subsection{Leaderboard} 
\label{leaderboardtwo} 

\begin{wrapfigure}{r}{0.6\textwidth} 
    \centering
    \includegraphics[width=0.6\textwidth]{figures/srepeated} 
    \caption{(a) shows repeated sampling on zero-context Hard task with Qwen-2.5-7B-Instruct; (b) shows the AUC to repeated sampling number of trials. We show that for repeated sampling, exponentially increasing inference compute only leads to a linear increase in AUC improvement.} 
    \vspace{-7mm} 
    \label{aucrepeated} 
\end{wrapfigure} 

We evaluated 18 powerful LLMs on zero-noise problems, resulting in Table \ref{zerocontextleaderb}, while 10 models are evaluated on the long context as shown in Table \ref{longcontextleaderb}. 
From Table \ref{zerocontextleaderb}, we can see that the score separates these LLMs into clear groups. Reasoning models (R1 and o1-mini) are significantly ahead of the rest of non-reasoning LLMs. On the other hand, models with hybrid architecture aren't performing strong on the zero-noise pure reasoning benchmarks. MiniMax-Text-01 and Jamba both severely underperform compared to 70 B-level models. Also, similar things can be discussed when comparing 70B and 7B level models. In Table \ref{longcontextleaderb}, we see that the models show a very different decay pattern, while Gemini-1.5-Pro is significantly ahead of the rest of the models. 
Reasoning model evaluation remains too costly or slow for current long-context leaderboard participation. 

\begin{figure*}
    \includegraphics[width=\textwidth]{figures/runnings} 
     \vspace{-6mm}
    \caption{(a) shows three different LLMs' behavior on \sysb benchmark zero-context Medium Forward, GPT o1-mini, Qwen2.5-72B-Instruct, and Qwen2.5-7B-Instruct. These models are drastically different in reasoning ability but have performance be modeled by sigmoid well, all with $R^2$ $>$0.98. (b) shows the gap between forward-thinking and reverse-thinking problems from Mistral Large on zero-context Hard. reverse-thinking problems are significantly harder and can be approximated by the sigmoid function that is essentially left-ward shifting from the forward sigmoid function. (c) and (d) presents RAG and corresponding LLMs' performance on different noises. Other than ours, RAG even improves performance.} 
    \label{xprmnt} 
    \vspace{-2mm}
\end{figure*} 

Later on, we examine the behavior of LLMs on \sysb and summarize four interesting findings uniquely enabled by our method of problem generation. 
\vspace{-3mm} 

\subsection{LLM Performance Degradation Can be Modeled Using Sigmoid Function} 
\label{sigmoid} 

Our construction of \sysb enables precise measurement of LLM performance across fine-grained difficulty levels. For each subtask, we observe a clear trend: LLM accuracy declines as the number of required operations increases. Surprisingly, most models exhibit a sigmoid-like performance decay, as shown in \ref{xprmnt} for forward problems (see Section \ref{reverse_problem} for definitions). Under the Medium subtask, LLM performance aligns remarkably well with a sigmoid function curve, with RÂ² $>$ 0.98. The pattern is intuitive: at low operation counts, accuracy remains near 1.0; as complexity increases, performance first decays gradually, then drops sharply toward zero, where LLMs effectively fail to solve the problems. The score eventually stabilizes near zero. Interestingly, while LLMs vary in overall capability, they follow the same trend, differing primarily in the decay rate of the sigmoid function. 

\subsection{Reverse Problems are  Harder to Solve for LLMs} 
\label{forwardreverseeval} 

The generator of \sysb can generate both the "forward" and the "reverse" problems, which can be compared separately. Most LLMs perform worse in reverse problems than forward ones, shown in Figure \ref{xprmnt}(b) using Mistral-Large~\citep{jiang2023mistral}. For the Medium, only Jamba-1.5-Large out of a total of 18 LLMs evaluated do reverse problems better than forward problems, and the average difference in AUC is 604. 
For the Hard subtask, 5 out of 18 models have larger reverse problems AUC, Deepseek R1, Gemini-1.5-Pro-002, Qwen2.5-7B-Instruct~\cite{qwen2025qwen25technicalreport}, 4o-mini, and Jamba-1.5-Large~\citep{team2024jamba}. 
The average difference in AUC is 154. A detailed breakdown is listed in Appendix \ref{forwardreverse}. Besides, LLM performance on reverse problems can also be modeled by a sigmoid mapping.  Five more LLM plots are presented in Appendix \ref{forwardreverse}. 

\subsection{Long-context Degradation and Noise Ablation} 
\label{longdegradation} 
We evaluate LLM performance across increasing context lengths (0, 8K, 16K, 32K) and observe a consistent decline in performance as context length increases. Notably, models exhibit different decay patterns. 
% Gemini 1.5 Pro degrades slowly at 8K despite a lower y-intercept, while Llama-3.1-70B-Instruct shows no such trend on the Hard subtask. 
We present results for 10 models across 3 subtasks, each with four curves representing different context lengths. All 30 plots are in Appendix \ref{picutrall}.

We conduct an ablation study on three noise types: \sysb (ours), LLM-generated, and random. For LLM-generated noise, we prompt GPT-4o to create a fake documentary-style commentary on random problems, occasionally introducing nonsensical variable mentions. For random noise, we follow Hsieh et al. (2024), using generic statements like "The sky is blue. The tree is green." We evaluate Llama3.1-70B-Instruct and a RAG system under all three noise types in an 8K context. Interestingly, RAG outperforms long-context LLMs on LLM-generated and random noise, effectively filtering irrelevant content. However, it fails to distinguish \sysb noise from essential problem statements.

\subsection{Limitations of Repeated Sampling} 
\label{resam} 
The construction of \sysb allows us to study techniques for Inference Scaling as well. Specifically, we study repeated sampling \citep{brown2024large,snell2024scalingllmtesttimecompute} and its effectiveness under different reasoning complexity. We study both Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct with the best-of-N settings similar to \citet{brown2024largelanguagemonkeysscaling}. Interestingly, we find that repeated sampling seems to boost the performance the most for smaller op count subsets, and the benefit of repeated sampling diminishes gradually for larger op count subsets, Qwen2.5-7B-Instruct behavior is plotted in Figure \ref{aucrepeated}(a) with different repeated trial settings. 

Surprisingly, suppose we calculate the AUC score under every curve corresponding to each number of repeated trial settings. In that case, we find that the increment in the AUC score from two consecutive settings is close. If we plot the AUC score versus the number of repeated trial settings and take the log scale of the repeated trial N, the graph is linear, as shown in Figure \ref{aucrepeated}(b). The R-squared is greater than 0.99 for both Qwen2.5-72B-Instruct and Llama3.1-8B-Instruct, where Llama3.1-8B-Instruct. \textbf{Therefore, \sysb helps reveal that Repeated Sampling leads to linear AUC Improvement from exponentially increasing computation cost.} 
