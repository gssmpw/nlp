\appendix 
\section*{Appendix Table of Contents}
We present the following table of contents to better traverse the Appendix. 

\begin{itemize}
    \item \hyperref[extrelatedworks]{Related Work}
    \begin{itemize}
        \item \hyperref[longcontextmodels]{Long-context Language Models}
        \item \hyperref[longcontextbenchmarks]{Long context benchmarks and tasks}
        \item \hyperref[reasoninglimitations]{Limitation of Existing Reasoning Tasks}
        \item \hyperref[synthesizeddatasets]{Synthesized Datasets for Long-context}
    \end{itemize} 
    \item \hyperref[limitation]{Benchmark Limitation} 
    \item \hyperref[setupr]{Detailed Experiment Setup}
    \item \hyperref[rag_full_result]{Full Result of RAG Experiments}
    \item \hyperref[illustrative_problems]{Illustrative Problems}
    \item \hyperref[forwardreverse]{Forward and Reverse Problems Breakdown}
    \item \hyperref[templates]{Ablation Study of Task Templates}
    \item \hyperref[picutrall]{Long-Context Degradation of Models}
\end{itemize} 

\section{Related Work} 
\label{extrelatedworks} 
\subsection{Long-context Language Models} 
\label{longcontextmodels} 
Various works related to the Long-context Language Model have been proposed. Flash attention\citep{dao2022flashattention}, Flash attention2\citep{dao2023flashattention2}, Ring attention\citep{liu2023ringattentionblockwisetransformers}, and Tree attention\citep{shyam2024treeattentiontopologyawaredecoding} significantly reduced the memory footprint and communication overhead for processing long context in engineering level across multiple nodes. Architectural level innovations such as sparse attentions represented by sliding window attention\citep{beltagy2020longformerlongdocumenttransformer}, are also widely used to reduce the overhead caused by the increasing sequence length. New training strategies, such as gradually extending the training context length in the final stages of pretraining have been applied to support a long context window\citep{dubey2024llama3herdmodels}.
\subsection{Long context benchmarks and tasks}
\label{longcontextbenchmarks} 
There have been quite a few works benchmarking long-context language models. Existing comprehensive benchmarks like $\infty$bench\citep{zhang2024inftybenchextendinglongcontext} cover realistic tasks including document QA, summary, and synthetic tasks including information retrieval, expression calculation, extending the context length in the benchmark to over 200k tokens. $\infty$bench\citep{zhang2024inftybenchextendinglongcontext} does have mathematical reasoning tasks, however the most relevant math.calc part seems to be too difficult for SOTA models to work out. Synthetic tasks often offer more control and are less affected by parametric knowledge in comparison with realistic tasks. One comprehensive synthetic benchmark is RULER\citep{hsieh2024ruler}, a synthetic benchmark with tasks including retrieval, variable tracking and so on, offering some controls over context length and task complexity. Experiments with various complexities were done, but it does not provide a quantitative analysis of complexity and context length on the correctness of the task, let alone isolate two separate patterns of performance decay. Other benchmarks usually focus on simple retrieval\citep{niah, liu2023lostmiddlelanguagemodels}, fact reasoning\citep{kuratov2024babilongtestinglimitsllms}, the impact of long context on natural language reasoning\citep{levy2024tasktokensimpactinput} and other real-world knowledge involved tasks. 
\subsection{Limitation of Existing Reasoning Tasks} 
\label{reasoninglimitations} 
Popular reasoning benchmarks are loose collections of human-made problems that naturally suffer from the following limitations. 
Firstly, the difficulty of problems within the same benchmark varies widely. 
We analyzed all 1.3K test problems in the GSM8K dataset, we plot the histogram in the number of operations in Figure \ref{gsm8kvariability}(a). The problem varied from 2 to over 12 following a skewed bell shape curve. 
This lack of fine-grained control makes it challenging to systematically evaluate models across incremental difficulty levels. 
Also, notice that the total number of problems is less than 10 for op $\geq$ 9, too little for stable evaluation. The lack of problem quantity on human-curated datasets eliminates the possibility of filtering out problems of each fine-grained difficulty level. 
Secondly, there is a significant difficulty gap between the benchmarks: GSM-8K focuses on middle school problems, MATH \citep{hendrycks2021measuringmathematicalproblemsolving} and AIME targets prospective university students, and Frontier Math challenges top-tier math graduate students. 
% Many LLMs achieve near-perfect scores on the easier benchmarks or completely fail on the harder ones. 
% This discontinuity in difficulty hinders the precise quantitative modeling of LLM performance decay. (3) Most of existing problems have very short input prompts, while there is a lack of reasoning benchmark with long context and high information density. 
It is difficult to quantitatively determine the difference in problem difficulty between GSM-8K problems and MATH problems since MATH uses operations such as taking power or roots absent in GSM-8K. 
Similarly, it is not possible to determine the difference in complexity from MATH to Frontier Math. 
It is difficult to quantitatively model LLMs' performance degradation with the continuously increasing difficulty of the problem. 
Third, most of the existing problems have very short input prompts. On average, GSM-8K test set problems have a length of 59.96 tokens, while MATH test set problems have 67.37 tokens when using Llama 3.1 tokenizer. We have seen from \ref{lcllm} that the addition of irrelevant noise cannot meaningfully evaluate the ability to reason in a long context of LLMs. 
\subsection{Synthesized Datasets for long-context} 
\label{synthesizeddatasets} 
Synthesized tasks are simple to build and absolutely deterministic, data contamination safe, but highly effective in evaluating certain aspects of LLM performance. Its use in long-context benchmarks is profound. Needle-in-the-haystack \cite{kamradt2023needle}, a pioneering long-context synthesized task, now becomes the go-to task for evaluating LLM long-context retrieval ability. On the other hand, LLM reasoning benchmarks also see recent efforts in synthesized tasks. \cite{mirzadeh2024gsm} recently proposes to use build synthesized dataset upon GSM8K \cite{cobbe2021training} to study the robustness of LLM reasoning. \textbf{Part of our work draws a strong inspiration from a series of works (\cite{ye2024physicslanguagemodels21}, \cite{ye2024physicslanguagemodels22}) which systematically studies the intricacies of decoder transformers in solving grade-school level problems.} Following their footsteps, we carefully redesign the process of generating the problems so current LLMs can solve without training, and together with thoughtful steps in noise addition, we effectively construct reasoning benchmarks for the long-context community. 

\section{Benchmark Limitation} 
\label{limitation} 
Despite our careful construction and attention to LLM understandability, there are still several important limitations to the existing benchmark \sysb. First, our current benchmark only targets operations contained in the grades school math, so only +, $-$, $\times$, $\div$ are included. We are aware that Math \citep{hendrycks2021measuringmathematicalproblemsolving} includes more complex operators, which we are looking at incorporating into the next version of the benchmark. The key limitation it brings to our benchmark is shown in \cref{figureone}(a), to generate complex problems with high reasoning complexity, our generator needs to generate problems that are much longer than hard mathematic benchmarks, despite our limitless quantity and complexity scalability. Second, another key limitation of synthetic benchmarks is the lack of diversity in natural language. Natural Language reasoning tasks usually contain multiple ways to assign a variable value or to describe a relationship between multiple variables, which is extremely difficult to program in software without the LLMs in the loop. We deliberately aim to keep LLMs out of the loop for better scalability and counter the diversity by incorporating more than one template into the problem generation. Third, we implement multiple checks to ensure all of the essential steps in the core graph have positive values and have a number range of less than four figures, but for the current noise generation, we don't implement these checks, since they are redundant to the problems. However, we do notice that when processing through the noise, the LLMs sometimes are confused by the irregular noise variable values. In later version of the benchmark, we will address this issue and make sure the entire graph is consistent. 

\section{Detailed Experiment Setup} 
\subsection{RAG Experiment Setup} 
\label{setupr} 
The RAG system contains two components, the retriever and the decoder. For the retriever, we use all-mpnet-v2-base. For the decoder, we use Llama-3.1-70B-Instruct. The context retrieval budget for all problems is 2048. We employ two different RAG methods: passive and active RAGs. Passive RAG calls the retriever once before the generation of the decoder. The retriever computes the semantic similarity or distance between each chunk of context and the query sentence. These chunks in context are then ranked from closest (most semantic similar) to furthest (least semantic similar), and depending on the retrieved context, top-k chunks are retrieved. For our study, we used the L2 distance between context chunk embeddings and query embeddings. The decoder then takes the retrieved chunks as input and then outputs its response to the query. 

We use two types of RAG systems: Passive RAG that only calls the retriever once at the beginning to retrieve relevant context or Interactive RAG \cite{jiang2023activeretrievalaugmentedgeneration} in which the decoder decides when to retrieve, how many retrievals are needed, and generate a query for each retrieval. For the latter one, we restrict the decoder generation with only the latest retrieval content and its past generation. 

On the other hand, active RAG shows strong performance \cite{jiang2023activeretrievalaugmentedgeneration} especially for common sense reasoning tasks. In addition to the steps in passive RAGs, the decoder is allowed to initiate additional calls to the retriever to retrieve more context by generating new queries. We follow the state-of-the-art active RAG method FLARE \cite{jiang2023activeretrievalaugmentedgeneration}, which allows for 10 rounds of query, but restricts the LLM to only see its current round of retrieved context and its past rounds of generation to generate its full response. 


\subsection{Repeated Sampling Experiment Setup}

\subsubsection{Procedure}
\begin{enumerate}
    \item \textbf{Oversampling Phase}
    \begin{itemize}
        \item Generate 256 samples per task with temperature $T=1.0$
        \item Use fixed random seeds for reproducibility
    \end{itemize}
    
    \item \textbf{Accuracy Calculation}
    \begin{itemize}
        \item Compute per-task empirical accuracy:
        \begin{equation}
            p_{\text{task}} = \frac{\text{\# Correct Samples}}{256}
        \end{equation}
        
        \item Estimate accuracy for N samples:
        \begin{equation}
            \text{Acc}_{\text{task}} = 1 - (1 - p_{\text{task}})^{N}
        \end{equation}
    \end{itemize}
    
    \item \textbf{Aggregation}
    \begin{itemize}
        \item Average results across 80 tasks:
        \begin{equation}
            \text{Final Accuracy} = \frac{1}{80} \sum_{i=1}^{80} \text{Acc}_{\text{task}_i}
        \end{equation}
    \end{itemize}
\end{enumerate}

\subsubsection{Rationale}
\begin{itemize}
    \item \textbf{Oversampling}: 256 samples reduces variance in estimating $p_{\text{task}}$ compared to using 128 samples directly
    \item \textbf{Probability Formula}: Models cumulative success probability:
    \begin{equation*}
        P(\geq\text{1 correct in }k\text{ trials}) = 1 - (1 - p)^k
    \end{equation*}
    \item \textbf{Task Count}: 80 tasks per op provide stable statistics while remaining computationally feasible
\end{itemize}

\subsection{Some Small Implementation Tips to Avoid Quadratic (or higher-order) Equations and Multiple Possible Solutions} 
\label{sometips} 
The quadratic equation gives rise to the following particular situation. For solving reverse mode questions, one easy way is to assign the variable asked as X, perform symbolic operations carrying with X, and go through the entire computation graph to close the circle and formulate an equation. Using that equation would easily solve for X. However, sometimes the query variable to too deep in the chain, and it might assign both variables Y and Z, which are later multiplied. Then, two linear expressions are multiplied to get quadratic operations, and as op becomes larger, it can be more severe than quadratic. 

To alleviate the situation, we found that the following technique is empirically helpful. When generating the problem statement, we first locate the variable that has the largest number of ``entities" in the variable name and make sure that that one is the end of the topology sort list during the forward mode generation. Then, when switching to reverse mode, we locate the last possible initialized variable, so as close to the end of the sort list as possible, and assign this variable as a query, effectively reducing the chance of its dependents multiplying each other. The method can reduce the chance of quadratic or higher-order equations happening. However, if we still encounter the cases even when the above procedure is in place, we discard the generated example and ask the generator to generate again. 

\newpage 

\section{Full Result of RAG experiments}
\label{rag_full_result}
Here, we present the full result of the RAG experiment for further analysis. We retrieve 2048 tokens for each RAG retrieval.
\subsection{RULER}
% First part of the table
\begin{longtable}{l| c c c c c c c c |c}
\hline
\textbf{Models} & \textbf{s1} & \textbf{s2} & \textbf{s3} & \textbf{mk1} & \textbf{mk2} & \textbf{mk3} & \textbf{mv} & \textbf{mq} & \textbf{Context Length} \\ \hline
\endhead

Llama3.1-70B-Instruct & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 100 & 8k \\ 
OnePass RAG & 100 & 100 & 100 & 100 & 100 & 96 & 98.5 & 100 & 8k \\ 
Interactive RAG & 100 & 100 & 100 & 100 & 100 & 98 & 99 & 99 & 8k \\ 
Llama3.1-70B-Instruct & 100 & 100 & 100 & 100 & 98 & 100 & 98 & 100 & 32k \\ 
OnePass RAG & 100 & 100 & 100 & 98 & 100 & 72 & 99.5 & 97.5 & 32k \\ 
Interactive RAG & 100 & 100 & 100 & 98 & 96 & 96 & 98.5 & 98.5 & 32k \\ 
Llama3.1-70B-Instruct & 100 & 100 & 100 & 98 & 96 & 100 & 89 & 98 & 64k \\ 
OnePass RAG & 100 & 100 & 100 & 100 & 100 & 56 & 95.5 & 100 & 64k \\
Interactive RAG & 100 & 100 & 100 & 100 & 96 & 98 & 99 & 100 & 64k \\ \hline

\caption{RAG vs Model (RULER NIAH)}
\label{tab:rag_performance_ruler_part1}
\end{longtable}

% Second part of the table
\begin{longtable}{l|c c c c c|c}
\hline
\textbf{Models} & \textbf{vt} & \textbf{cwe} & \textbf{fwe} & \textbf{qa1} & \textbf{qa2} & \textbf{Context Length} \\ \hline
\endhead

Llama3.1-70B-Instruct & 100 & 100 & 96.67 & 84 & 74 & 8k \\ 
OnePass RAG & 82.4 & 14.8 & 97.33 & 86 & 86 & 8k \\ 
Interactive RAG & 98.4 & 31.2 & 79.33 & 80 & 68 & 8k \\ 
Llama3.1-70B-Instruct & 100 & 95.2 & 97.33 & 80 & 66 & 32k \\ 
OnePass RAG & 86 & 5.2 & 92 & 84 & 74 & 32k \\ 
Interactive RAG & 98 & 7.6 & 80 & 78 & 64 & 32k \\ 
Llama3.1-70B-Instruct & 100 & 6.2 & 95.33 & 72 & 62 & 64k \\ 
OnePass RAG & 78.4 & 1.2 & 88.67 & 82 & 74 & 64k \\ 
Interactive RAG & 98.8 & 2.6 & 72 & 78 & 56 & 64k \\ \hline

\caption{RAG vs Model (RULER other subsets)}
\label{tab:rag_performance_ruler_part2}
\end{longtable}
% \vspace{2mm}
\textbf{Comments}: s1-3 = niah\_single\_1-3, mk1-3 = niah\_multikey\_1-3, mv = niah\_multivalue, mq = niah\_multiquery

\subsection{LongBench V2}
\begin{longtable}{l|c|c c c c}
\hline
\textbf{Tasks} & \textbf{Overall} & \textbf{Easy} & \textbf{Hard} & \textbf{Short} & \textbf{Long} \\
\hline
\endhead
Llama3.1-70B-Instruct & 30 & 33.3 & 28.1 & 44.7 & 21 \\
OnePassRAG  & 25 & 33.3 & 20.3 & 23.7 & 25.8 \\
InteractiveRAG  & 33 & 36.1 & 31.2 & 34.2 & 32.3 \\
\hline
\caption{RAG vs Model (LongBench V2)}
\label{tab:rag_performance_longbenchv2}
\end{longtable}

\subsection{LongBench}

\begin{longtable}{l|c c c}
\hline
\textbf{Tasks} & \textbf{passage\_count} & \textbf{hotpot-qa} & \textbf{samsum} \\
\hline \endhead
Llama3.1-70B-Instruct & 36.0,36.0,32.0 & 58.87,71.22,76.44 & 28.88,35.95,41.48 \\
OnePassRAG  & 0.0,0.0,0.0 & 65.04,61.59,63.05 & 31.63,23.28,26.84 \\
InteractiveRAG  & 27.0,14.0,6.0 & 61.86,51.0,55.94 & 24.83,20.21,23.81 \\
\hline
\caption{RAG vs Model (LongBench) - Part 1}
\label{tab:longbench_part1}
\end{longtable}

\begin{longtable}{l|c c c}
\hline
\textbf{Tasks} & \textbf{multi-news} & \textbf{multifieldqa\_en} & \textbf{gov\_report} \\
\hline \endhead
Llama3.1-70B-Instruct & 27.71,24.81,23.17 & 57.31,51.83,64.98 & 34.94,34.97,31.82 \\
OnePassRAG  & 26.85,22.72,20.49 & 51.69,48.66,55.85 & 32.6,30.67,27.32 \\
InteractiveRAG  & 24.64,19.99,18.87 & 47.71,42.98,58.45 & 29.91,27.02,25.34 \\
\hline
\caption{RAG vs Model (LongBench) - Part 2}
\label{tab:longbench_part2}
\end{longtable}

\begin{longtable}{l|c c c}
\hline
\textbf{Tasks} & \textbf{qasper} & \textbf{passage\_retrieval\_en} & \textbf{2wikimqa} \\
\hline \endhead
Llama3.1-70B-Instruct & 50.3,46.5,25.89 & 100.0,100.0,100.0 & 74.93,64.37,59.6 \\
OnePassRAG  & 45.73,43.5,35.3 & 72.0,72.0,79.0 & 67.97,59.64,48.4 \\
InteractiveRAG  & 43.8,37.9,32.84 & 91.0,90.0,85.33 & 46.04,43.46,38.8 \\
\hline
\caption{RAG vs Model (LongBench) - Part 3}
\label{tab:longbench_part3}
\end{longtable}

\begin{longtable}{l|c c c c}
\hline
\textbf{Tasks} & \textbf{triviaqa} & \textbf{trec} & \textbf{lcc} & \textbf{repobench-p} \\
\hline \endhead
Llama3.1-70B-Instruct & 82.0,93.6,94.0 & 48.0,12.0,12.0 & 50.14,55.0,50.04 & 29.98,27.82,26.84 \\
OnePassRAG  & 92.13,89.46,90.97 & 47.0,56.0,53.0 & 19.92,14.5,18.36 & 34.76,33.62,28.0 \\
InteractiveRAG  & 88.11,92.32,91.5 & 56.0,57.0,52.0 & 24.26,23.26,22.57 & 14.97,17.15,16.56 \\
\hline
\caption{RAG vs Model (LongBench) - Part 4}
\label{tab:longbench_part4}
\end{longtable}
\textbf{Comments}: The 3 data separated by commas are subsets of 0-4k, 4-8k,8k+ respectively


\subsection{LOFT}
\begin{longtable}{l|c c c c c c c}
\hline
\textbf{Tasks} & \textbf{ArguAna} & \textbf{FEVER} & \textbf{FIQA} & \textbf{MS MARCO} & \textbf{NQ} & \textbf{Quora} & \textbf{SciFact} \\
\hline \endhead 
Llama3.1-70B-Instruct & 0.06 & 0.78 & 0.37 & 0.67 & 0.84 & 0.62 & 0.59 \\
OnePassRAG  & 0.64 & 0.88 & 0.45 & 0.77 & 0.86 & 0.62 & 0.64 \\
InteractiveRAG  & 0.42 & 0.73 & 0.53 & 0.69 & 0.76 & 0.83 & 0.87 \\
\hline
\caption{RAG vs Model (LOFT) - Part 1}
\end{longtable}

\begin{longtable}{l|c c c c c c}
\hline
\textbf{Tasks} & \textbf{Touché-2020} & \textbf{HotPotQA} & \textbf{MuSiQue} & \textbf{QAMPARI} & \textbf{QUEST}  \\
\hline \endhead
Llama3.1-70B-Instruct & 0.4411 & 0.37 & 0.2 & 0.024 & 0.07166 \\
OnePassRAG  & 0.2529 & 0.455 & 0.2383 & 0.1559 & 0.1899  \\
InteractiveRAG  & 0.79 & 0.29 & 0.13 & 0.1539 & 0.2983 \\
\hline
\caption{RAG vs Model (LOFT) - Part 2}
\label{tab:your_label}
\end{longtable}
\textbf{Comments}: Due to LOFT's limited prompt availability (no official releases for 32k/1M contexts), we conducted experiments solely on 128k context lengths. We observed that LOFT’s document ID retrieval tasks inherently require document identifiers that aren’t captured in standard RAG-retrieved chunks. To address this, we implemented a lightweight modification: appending document ID tags to each context chunk during retrieval. This allows the RAG system to infer the correct document ID when retrieving relevant content, resolving the task-specific limitation without altering core RAG functionality. 
\newpage

\section{Illustrative Problems} 
\label{illustrative_problems} 
This section presents one representative problem from each subset (Symbolic, Medium, and Hard) defined in the appendix. These examples illustrate the variations within the benchmark. see Table \ref{tab:illustrative_problems}

\begin{table}[h]
\caption{Illustrative Problems from Each Subset}
\label{tab:illustrative_problems} 
\begin{tabular}{llll}
\toprule
\multicolumn{4}{l}{Three example problems one for each subtask} \\ 
\midrule
Problem & \multicolumn{3}{p{0.8\textwidth}}{\vspace{-0.5\baselineskip} % Adjust vertical spacing as needed 
% Problem & \multicolumn{3}{p{0.8\textwidth}}{ 
\begin{itemize}
\item \textbf{Symbolic (op=5):} $<$context$>$\textbackslash nassign V705804 = V437110 + 1. assign V986916 = V705804. assign V873548 = 6. assign V684196 = V873548. assign V437110 = V873548.\textbackslash n $<$/context$>$ \textbackslash n\textbackslash nThe context contains relationships between variables. These relationships are independent mathematical equations that are all satisfied simultaneously.\textbackslash n Using only these relationships, determine which variables (if any) from which values can be derived are equal to 7.\textbackslash nShow your step-by-step reasoning and calculations, and then conclude your final answer in a sentence. \textbf{Answer}: V705804,V986916.
\item \textbf{Medium (op=5):} Problem: The number of adult owl in Bundle Ranch equals 2 times the number of adult eagle in Bundle Ranch. The number of adult eagle in Hamilton Farm equals the difference between the total number of adult animals in Bundle Ranch and the number of adult eagle in Bundle Ranch. The number of adult owl in Hamilton Farm equals 4 times the number of adult owl in Bundle Ranch. The number of adult eagle in Bundle Ranch equals 3.  Question: What is the total number of adult animals in Bundle Ranch?
\textbf{Answer}: 9.
\item \textbf{Hard (op=5):} The average number of newborn children per adult blue jay in Bundle Ranch equals 2. The number of adult parrot in Bundle Ranch equals 2. The number of adult blue jay in Bundle Ranch equals 2 times the average number of newborn children per adult blue jay in Bundle Ranch. The number of adult eagle in Bundle Ranch equals 2 times the average number of newborn children per adult blue jay in Bundle Ranch. The number of adult parrot in South Zoo equals 4 times the sum of the average number of newborn children per adult eagle in Hamilton Farm, the number of adult eagle in Hamilton Farm, and the average number of newborn children per adult eagle in Hamilton Farm. The average number of newborn children per adult eagle in Hamilton Farm equals the number of adult eagle in Bundle Ranch. The number of adult eagle in Hamilton Farm equals 3. The average number of newborn children per adult parrot in Bundle Ranch equals the total number of adult animals in Hamilton Farm. The number of adult eagle in South Zoo equals 1. The average number of newborn children per adult parrot in South Zoo equals the average number of newborn children per adult parrot in Bundle Ranch. The average number of newborn children per adult eagle in Bundle Ranch equals 3 plus the average number of newborn children per adult parrot in Bundle Ranch. The average number of newborn children per adult eagle in South Zoo equals the sum of the number of adult blue jay in Bundle Ranch, the average number of newborn children per adult blue jay in Bundle Ranch, the average number of newborn children per adult parrot in Bundle Ranch, and the number of adult parrot in Bundle Ranch.  Question: What is the average number of newborn children per adult eagle in Bundle Ranch?
\textbf{Answer}: 6.
\end{itemize}
}\\ 
\bottomrule
\end{tabular}
\end{table}

\newpage
\section{Forward and Reverse Problems Breakdown} 
\label{forwardreverse} 

\begin{longtable}{|l|c|c|c|} 
\hline
\textbf{Models} & \textbf{Forward Problem} & \textbf{Reverse Problem} & \textbf{Forward AUC - Reverse AUC} \\ \hline
Llama3.1-70B-Instruct & 2100.625000 & 1283.750000 & 816.875000 \\ \hline
GPT-4o-mini & 1529.725400 & 1267.579000 & 262.146400 \\ \hline
\rowcolor{yellow!20} 
Jamba-1.5-Large & 390.380000 & 624.980000 & -234.600000 \\ \hline
GPT-4o & 3073.997375 & 1952.816875 & 1121.180500 \\ \hline
Mistral-Large & 3468.234100 & 2431.732450 & 1036.501650 \\ \hline
Llama3.1-8B-Instruct & 1030.000000 & 563.125000 & 466.875000 \\ \hline
Claude-3.5-Sonnet & 3653.830050 & 3158.657850 & 495.172200 \\ \hline
Qwen2.5-72B-Instruct & 2889.375000 & 2141.250000 & 748.125000 \\ \hline
Qwen2.5-7B-Instruct & 995.625000 & 833.125000 & 162.500000 \\ \hline
o1-mini & 6517.510550 & 5592.307100 & 925.203450 \\ \hline
Gemini-1.5-Flash-002 & 1889.375000 & 1153.750000 & 735.625000 \\ \hline
Claude-3.5-Haiku & 1234.620000 & 873.100000 & 361.520000 \\ \hline
Llama-3.1-405B-Instruct & 1781.400000 & 981.250000 & 800.150000 \\ \hline
DeepSeek-V3 & 4613.125000 & 3713.125000 & 900.000000 \\ \hline
Gemini-1.5-Pro-002 & 4204.564075 & 3160.574950 & 1043.989125 \\ \hline
DeepSeek-R1 & 9764.950000 & 9750.950000 & 14.000000 \\ \hline
MiniMax-Text-01 & 2148.071300 & 1539.415650 & 608.655650 \\ \hline
QwQ-32B-Preview & 3530.000000 & 2846.250000 & 683.750000 \\ \hline 
\caption{Medium Difference in AUC in Forward Problems and Reverse Problems} 
\end{longtable} 

\begin{longtable}{|l|c|c|c|}
\hline
\textbf{Models} & \textbf{Forward Problem} & \textbf{Reverse Problem} & \textbf{Forward AUC - Reverse AUC} \\ \hline
Claude-3.5-Haiku & 819.240000 & 776.900000 & 42.340000 \\ \hline
Llama3.1-70B-Instruct & 1314.375000 & 1098.750000 & 215.625000 \\ \hline
Gemini-1.5-Flash-002 & 1341.250000 & 1219.375000 & 121.875000 \\ \hline
MiniMax-Text-01 & 1360.555000 & 1034.625000 & 325.930000 \\ \hline
\rowcolor{yellow!20}
DeepSeek-R1 & 8444.500000 & 8756.950000 & -312.450000 \\ \hline
o1-mini & 3831.381000 & 3645.474200 & 185.906800 \\ \hline
\rowcolor{yellow!20}
Gemini-1.5-Pro-002 & 2255.732025 & 2444.270375 & -188.538350 \\ \hline
DeepSeek-V3 & 2725.085000 & 2109.560000 & 615.525000 \\ \hline
\rowcolor{yellow!20}
Qwen2.5-7B-Instruct & 625.625000 & 630.625000 & -5.000000 \\ \hline
GPT-4o & 1592.280000 & 1311.560000 & 280.720000 \\ \hline
Llama3.1-8B-Instruct & 759.375000 & 460.625000 & 298.750000 \\ \hline
Qwen2.5-72B-Instruct & 2196.875000 & 1895.000000 & 301.875000 \\ \hline
Claude-3.5-Sonnet & 2242.309950 & 1999.998100 & 242.311850 \\ \hline
\rowcolor{yellow!20}
GPT-4o-mini & 858.400000 & 873.310000 & -14.910000 \\ \hline
QwQ-32B-Preview & 1878.750000 & 1855.625000 & 23.125000 \\ \hline
Mistral-Large & 2570.940500 & 2018.469000 & 552.471500 \\ \hline
Llama-3.1-405B-Instruct & 1215.000000 & 743.750000 & 471.250000 \\ \hline
\rowcolor{yellow!20}
Jamba-1.5-Large & 274.980000 & 699.990000 & -425.010000 \\ \hline 
\caption{Hard Difference in AUC in Forward Problems and Reverse Problems} 
\end{longtable} 

\begin{figure}[h]
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fwd-rev/deepseek.png}
    \caption{DeepSeek}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fwd-rev/llama-405b.png}
    \caption{LLaMA-405B}
  \end{subfigure}
  \hfill
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fwd-rev/mistral.png}
    \caption{Mistral}
  \end{subfigure}
  \caption{Comparison of different models}
  \label{fig:models}
\end{figure} 

\begin{figure}[h] 
  \centering
  \begin{subfigure}[b]{0.3\textwidth}
    \includegraphics[width=\textwidth]{fwd-rev/minimax.png} 
  \end{subfigure} 
  \hspace{0.05\textwidth} % Adjust spacing for symmetry
  \begin{subfigure}[b]{0.3\textwidth} 
    \includegraphics[width=\textwidth]{fwd-rev/qwen-72b.png} 
  \end{subfigure} 
  \caption{Comparison between forward and reverse}
  \label{fig:forwardreverse} 
\end{figure} 

\newpage

\section{Ablation Study of Task Templates}
\label{templates} 
\begin{figure}[h!]
  \includegraphics[width=1.0\textwidth]{figures/Presentation12.pdf} 
  \caption{Comparison between different task templates}
  \label{languagetemplates} 
\end{figure} 

We have three different real-world templates ready. We show that they offer consistent scores with Llama-3.1-8B-Instruct, with slight variables in specific operations. We also show three problem examples. 



\newpage

\section{Long-Context Degradation of Models} 
\label{picutrall} 
In this section, we provide the accuracy decay curves of all LLMs tested across zero-context, 8K, 16K and 32K for further analysis. We selected the first 30 reasoning steps to truncate the data for comparison purposes. 

\begin{figure}[h!]
  \centering
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_DeepSeek-V3_symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_DeepSeek-V3_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_DeepSeek-V3_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Gemini-1.5-Pro-002_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Gemini-1.5-Pro-002_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Gemini-1.5-Pro-002_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Mistral-Large-2411_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Mistral-Large-2411_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Mistral-Large-2411_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Qwen2.5-72B-Instruct_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Qwen2.5-72B-Instruct_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Qwen2.5-72B-Instruct_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Gemini-1.5-Flash-002_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Gemini-1.5-Flash-002_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Gemini-1.5-Flash-002_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Meta-Llama-3.1-70B-Instruct_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Meta-Llama-3.1-70B-Instruct_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Meta-Llama-3.1-70B-Instruct_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_MiniMax-Text-01_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_MiniMax-Text-01_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_MiniMax-Text-01_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_GPT-4o-mini-2024-07-18_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_GPT-4o-mini-2024-07-18_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_GPT-4o-mini-2024-07-18_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Qwen2.5-7B-Instruct_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Qwen2.5-7B-Instruct_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Qwen2.5-7B-Instruct_Hard.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/symbolic/plot_multicontext_Meta-Llama-3.1-8B-Instruct_Symbolic.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/medium/plot_multicontext_Meta-Llama-3.1-8B-Instruct_Medium.png}
  \end{subfigure}
  \hfill
  \begin{subfigure}{0.17\textwidth}
    \includegraphics[width=\textwidth]{appendix/hard/plot_multicontext_Meta-Llama-3.1-8B-Instruct_Hard.png}
  \end{subfigure}
  \caption{Accuracy decay with context length for different models}
\end{figure} 
