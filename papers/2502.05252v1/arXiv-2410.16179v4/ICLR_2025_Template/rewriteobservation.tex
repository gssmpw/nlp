\section{Computational Graphs} 

\begin{figure*}
    \includegraphics[width=\textwidth]{figures/computationalgraph} 
    \caption{Computational Graphs Illustration. (a) shows a simple computational graph, where every node and edge can be straightforwardly converted to a statement, the other way conversion works as well. (b) breaks down the essence of implicit operations and the abstract parameter constructions; here, an assumption provided in the system prompt is to assume the animals not mentioned in question don't exist; essentially, problems in natural language omit the two red arrows in the computational graph. (c) provides an example problem when all variables that appeared are ``two-entity" variables, where only implicit addition/subtraction can be generated. (d) contrasts (c) and shows an example that with additional ``three-entity" variables, the computation graph can also generate implicit multiplication/division. Both (c) and (d) also illustrate the design of reverse mode that specifically aims to generate implicit subtraction and division. (Figure best viewed in color)} 
    \label{mediumhard} 
\end{figure*} 

In this section, we detailed the construction of computational graphs, the key construct that enables \sysb generation of infinite quantities of arbitrary context length and reasoning complexity. Specifically, we explain in detail the potential mapping between reasoning problems and computational graphs in \Cref{sec:graphtoreasoning}, and in \Cref{sec:noisecontruction}, we propose to generate indistinguishable noise by strategically extending the computational graph. 

\subsection{Graph Construction to Build Reasoning Problems} 
\label{sec:graphtoreasoning}
After carefully studying GSM-8K problems, we draw the following crucial observations, which allow us to map a randomly generated computational graph to grade-school-level math reasoning problems that cover all possible operations and relationship types. 

\textbf{Mapping Explicit Ops to Computational Graphs - } From Every operation used in the GSM-8K is one of the four ``+”, ``$-$”, ``×”, and ``÷”. Consider the following example when operations are presented explicitly, ``Eggs cost twice as much as tomatoes, while tomatoes cost 1 dollar each.” These statements mention operations (``plus”, ``more”, ``times", etc.) can easily be abstracted out as a computational graph with variables, 
``dollar per egg” and ``dollar per tomato”, as nodes. There are two edges one pointing from "dollar per tomato" to "dollar per egg", while another one from a constant 2 to "dollar per tomato". Another similar example is shown in \cref{mediumhard} (a). 
Therefore, randomly generating a computational graph with different topology of edge connections will
lead to a new reasoning problem once the natural language
context are attached to the nodes of the graph. 

\textbf{Generating Implicit + using Computational Graphs - } On the other hand, the operations can also be presented implicitly hidden in natural language hierarchies. ``Mary earns 20 dollars in the morning, while she earns 25 dollars in the afternoon. How much total she earned that day?” Although the problem doesn’t explicitly mention addition, the solution has to sum up 20 and 25 to get 45. The reason is that natural language assumes a working day consists of morning and afternoon. Similarly, all four operations can be hidden in natural language hierarchies. Inspired by \citet{ye2024physicslanguagemodels21},
we adopt its construct of ``Abstract Parameters” and ``Instance Parameters” to construct computational graphs that facilitate the generation of problem statements containing the hidden operations. Essentially, the newly added constructs can be thought of as adding the ``total money” as a new node to the computational graph, which has two edges coming in, one from node ”Morning money” and the other one from node ``Afternoon money”. But when generating the problem, we omit the description of two edges pointing to the node ``total money on Friday”. We also illustrate it in \cref{mediumhard}(b). Following \cite{ye2024physicslanguagemodels21}, we connect the specific instance parameters to the abstract parameters using red edges. To reiterate, these edges are omitted, forcing the LLMs to use commonsense and inducing implicit operations. 

\textbf{Generating Implicit $\times$ using Computational Graphs -} every variables in the above-mentioned problem with the implicit ``+” operation are ”two-entity variables”. ``Morning Money” contains two entities, ``Morning” and ``Money”, where in the context,
``Money” is an attribute of ``Morning”. Same with ``After-
noon Money”. In fact, out of all the examples we manually examined in GSM-8K, the minimum number of entities in the variable name is two. However, if every variable in the problem are two-entity" variables, only generate implicit operations of + and - will be induced, but not $\times$ and $\div$. For a problem to contain implicit operations $\times$, problems must contain variables with more than two entities in its name. For example, ``Mary works 8 hours on Friday. Her hourly rate on
Friday is 10 dollars. How much she will earn on Friday
in total?” The variable ”money per hour” contains ``money”, ``hour”, and ``Friday” three entities, where
``money” is an attribute of ``hour”, while ``hour” is also an attribute of ``Friday”. We further contrast the above scenario in \cref{mediumhard}(c) and (d), we see that if every variable is two-entity or ``Animal" in ``Location", generating $\times$ isn't straightforward, but once we add in the third entity, or an attribute of animal, generation of $\times$ becomes natural. 

\textbf{Scaling up Reasoning Complexity and maintaining control with Computational Graph -} Our computational graph generator employs the abstract parameter construct to generate implicit operations and three-entity variables to represent multiplication operations. 
% To maximize diversity in reasoning paths, we impose minimal restrictions on graph generation. 
During the generation process of a problem, specifically, a query node is first sampled from the graph. The corresponding topological sort list—ending with the query—ensures the shortest solution path, serving as a measure of the problem's reasoning complexity. 
This approach enables the generation of a vast number of synthetic graphs. Furthermore, adjusting the number of variables provides a coarse control over complexity, which is refined through precise filtering to produce well-defined subsets of graphs that meet specific operation constraints.

\subsection{Noise Construction Using Computational Graphs} 
\label{sec:noisecontruction} 

\begin{wrapfigure}{r}{0.35\textwidth} 
    \includegraphics[width=0.35\textwidth]{figures/noisy.pdf} 
    \caption{The Illustration of Noise as Extension of Core Computational Graph.} 
    \label{noisytwo} 
\end{wrapfigure} 

\textbf{Spider Topology -} We observe that we can view noise as extending the computational graph to incorporate fake and unnecessary parameters and operators. However, two critical questions emerge. First, how to extend the computational graph without contaminating the original graph's solution and contaminations? We found that edges have to point outwards from the nodes in the original graph to the newly added noise nodes, essentially preventing the noise nodes from contributing to the core graph. Second, how to maximize the chance that RAG cannot retrieve the essential graph? It turns out interconnecting edges between newly added noise nodes won't contribute help detering RAG's retriever. We find out a simple trick works well: ensuring the majority of the added edges connect core nodes and the noise nodes contribute to a semantically close noise. We call this design Spider Topology as shown in Figure \ref{mediumhard}(c). 

We evaluated the resulting noise using two RAG systems presented in \ref{lcllm}. The results are shown in Figures \ref{mediumandhard}(c) and (d). Llama3.1-70B-Instruct achieves drastically stronger performance than the two RAG systems on 8K 2-entity problems and 3-entity problems. We also carried out the same study before on our data set setting 2, in Figure \ref{mediumandhard}(b). \textbf{We found that the RAG retriever now completely cannot distinguish which essential chunks from noise chunks, showing a clear contrast with vt tasks of the same context length in (a).}  
