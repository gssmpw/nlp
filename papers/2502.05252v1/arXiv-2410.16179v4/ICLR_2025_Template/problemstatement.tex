\section{Related Work and Problem Statement} 
Despite the wide popularity of some existing long-context benchmarks, this section reveals and elaborates on the three key limitations: low complexity, detectable noise, and low resource or limited quantity of test examples. These three limitations make it extremely challenging to measure long-context LLMs' progress toward advanced intellectual agents using existing benchmarks. 

\textbf{Low Complexity.} A significant portion of long-context evaluation datasets, including RULER \citep{hsieh2024rulerwhatsrealcontext}, LongBench \citep{bai2024longbenchbilingualmultitaskbenchmark}, LongBench v2 \citep{bai2025longbenchv2deeperunderstanding}, and LOFT \citep{lee2024longcontextlanguagemodelssubsume}, primarily assess retrieval and summarization rather than complex reasoning. As shown in Figure \ref{passiveplusinteractive}, our experiments demonstrate that RAG systems achieve competitive results with Llama3.1-70B-Instruct across these datasets. Notably, RAG outperforms LLMs in retrieval-focused tasks (e.g., RULER, LOFT) and performs comparably in text summarization and QA (most LongBench tasks), as well as structured reasoning problems such as variable tracking (RULER-vt) and code completion (LongBench-repobench-p). RAG methods provide a strong baseline while being substantially more efficient.

\begin{wrapfigure}{r}{0.5\textwidth} 
    \includegraphics[width=0.5\textwidth]{figures/gsm} 
    \caption{(a) presents a conservative estimate for each problem difficulty in GSM-8K 1.3K test set. We evaluate the difficulty of the problems by the number of operations needed to get to the final answer. The op count ranges from 2 to 12, while most are around 3-4. (b) shows the Llama3.1-8B-Instruct performance across different semantics hierarchies, revealing the hidden reasoning difficulty innate in natural language.} 
    \label{gsm8kvariability} 
    % \vspace{-3mm} 
\end{wrapfigure} 

\textbf{Detectable Noise.} Many long-context benchmarks artificially extend short-context tasks by injecting extraneous text that does not contribute to solving the problem, allowing retrieval-based models to filter out noise effectively. In RULER's variable-tracing task with an 8192-token context, Llama3.1-70B-Instruct achieves 100\%, while OnePassRAG and InteractiveRAG reach 82.4\% and 98.4\%, respectively, despite using only a 2048-token retrieval budget. A detailed breakdown in Figure \ref{mediumandhard} (a) reveals that retrievers consistently identify and prioritize relevant information while disregarding injected noise. These findings indicate that existing long-context benchmarks do not adequately justify the need for expensive long-context LLMs, as RAG systems can effectively mitigate the impact of noise and achieve similar performance. 

\textbf{Low Resource.} Many high-quality reasoning tasks (Math and coding) heavily rely on human annotation and labeling and have test examples in limited quantity. Here we use GSM-8K \citep{cobbe2021trainingverifierssolvemath} as an example, shown in \cref{gsm8kvariability}(a). It is infeasible to extract subsets of examples with exact op at 8 for precise LLM evaluation due to the limited number of available casesâ€”only 26 in total, with even fewer satisfying op $\geq$ 8. This scarcity makes meaningful evaluation impractical. Also, DafnyBench \citep{loughridge2024dafnybenchbenchmarkformalsoftware}, a high-quality long-context coding benchmark only contains 782 verified and deduped examples. 

\textbf{Problem Statement -} \textit{How can we develop a benchmark that contains sufficient problems at every fine-grained level of reasoning difficulty, from easy retrieval tasks to infinitely hard challenges, while providing infinitely customizable context length with high information density?} 
\label{lcllm} 

\begin{figure*}
    \includegraphics[width=\textwidth]{figures/ourlonger} 
    \caption{RAG performance on our proposed long-context benchmarks. (a) studies retriever's behavior on the first 100 chunks of a random problem in vt from RULER with 8192 context length. The chunks that need to be retrieved to solve the problem are labeled in coral, while the noise is in blue. The chunks have retriever scores ranked from large (semantically far) to small (semantically close). Retriever locates the essential chunks with high precision, classifying all necessary chunks with the right side of the spectrum; (b) contrasts vt with our long-context benchmarks, showing that the retriever cannot locate precisely which chunk to retrieve. (c) and (d) display the performance of two RAG systems on our benchmark medium and hard tasks. (Figure best viewed in color)} 
    \label{mediumandhard} 
\end{figure*} 
