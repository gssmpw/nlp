\section{Introduction} 
\label{submission} 

\begin{wrapfigure}{r}{0.5\textwidth} 
    \includegraphics[width=\linewidth]{figures/headding} 
     \vspace{-5mm}
    \caption{Evaluation of 10 powerful LLMs on \sysb, comparing API generation cost (horizontal axis) with zero-context reasoning ability (vertical axis). Bubble size represents reasoning performance at a 16K context length.} 
    \vspace{-5mm} 
\end{wrapfigure} 

\begin{figure*}[t] 
    \centering
    \includegraphics[width=\textwidth]{figures/front.pdf} 
    \vspace{-5mm}
    \caption{(a) We position existing benchmarks across the Reasoning complexity versus context length plot. Reasoning datasets are usually of very short context. Existing long context benchmarks are usually low in reasoning complexity. Our task can cover any context length that the user so chooses and can generate infinite reasoning complexity. However, for high reasoning complexity, our task needs to use a longer context for problems. Our task is shown in Red. (b) A simplified example of our dataset-building process. We first generate an interconnected computational graph, and we then based on the graph, attach real-world context to it to formulate the problem statements. (c) Shows Qwen2.5-72B-Instruct Score decay across zero-context, 8K, 16K, and 32K.} 
    \label{figureone} 
    \vspace{-5mm}
\end{figure*} 

Recently, state-of-the-art long-context LLMs \citep{team2024gemini, minimax2025minimax01scalingfoundationmodels} have achieved astonishing performance in a tremendously long context, where \citet{team2024gemini} achieves near-perfect performance in 10M multimodal retrieval and long document QA. 
However, for long-context LLMs to contribute to cutting-edge mathematical and scientific discoveries or function as autonomous agents, they must be capable of processing dense, complex information and reason through multi-step tasks. 
For instance, Sir Andrew Wiles' proof \citep{wiles1995modular} of Fermat's Last Theorem in 1995 spans more than 88K highly compact tokens with deep logical connections, making context-level RAG~\citep{lewis2021retrievalaugmentedgenerationknowledgeintensivenlp} \textbf{insufficient} and highlighting the need for long-context LLMs. 
Therefore, it is crucial to benchmark and facilitate long-context LLMs for complex reasoning and high-density information processing.

Although widely used, current long-context benchmarks do not fully capture the true potential of long-context LLMs~\citep{yu2024defense,li2024long,li2024retrieval}, making it challenging to measure their progress toward advanced intellectual agents. 
It is mainly due to the following three reasons: (1) \textbf{Low Complexity.} Many long-context benchmarks, such as LongBench \citep{bai2025longbenchv2deeperunderstanding, bai2024longbenchbilingualmultitaskbenchmark} and most tasks in RULER \citep{hsieh2024rulerwhatsrealcontext}, focus on retrieval or summarization, which involve low reasoning complexity. 
Similar to~\citep{yu2024defenserageralongcontext}, we found that simple context-level RAG achieves on-par or even better results than long-context LLMs (shown in~\cref{fig:3}). 
(2) \textbf{Detectable Noise.} Many tasks are innately short-context but are bloated into longer context through semantically irrelevant filler text (\citet{kuratov2024babilongtestinglimitsllms} and variable-tracing in \citet{hsieh2024rulerwhatsrealcontext}), which is easily distinguished by a retriever of context-level RAG. 
(3) \textbf{Low Resource.} While complex long-context tasks exist, such as long code completion \citep{loughridge2024dafnybenchbenchmarkformalsoftware}, they lack sufficient high-quality examples with adequate and verified annotation and labeling. This scarcity limits test diversity and fine-grained difficulty assessment, reducing their effectiveness in model evaluation. 

Ideally, a long-context reasoning benchmark should (1) offer controllable and \textbf{scalable complexity}, (2) incorporate \textbf{hard-to-distinguish noise}, and (3) support \textbf{infinite data} generation for continuous and adaptable evaluation. Inspired by \citet{delÃ©tang2023neuralnetworkschomskyhierarchy, ye2024physicslanguagemodels21}, we model reasoning problems as computational graphs attached with language semantics. By adjusting their structure and complexity, we gain fine-grained control over reasoning difficulty and enable infinite scaling. Instead of inserting semantically irrelevant filler text, noise is introduced as additional nodes and edges upon the core graph, strategically connected to existing nodes without affecting to the necessary reasoning steps for solving the tasks. This design enables the generation of arbitrarily long test examples, from which context-level RAG finds it hard to differentiate relevant information from noise. 

\begin{figure*}
    \includegraphics[width=\textwidth]{figures/pfront} 
    \caption{Study of Llama3.1-70B-Instruct with Passive RAG (referred to as OnePassRAG) and Active RAG (referred to as InteractiveRAG) on popular long-context benchmarks: RULER (at 64K context length), LongBench ($>$8K), LongBenchV2, and LOFT (128K context length). RAG is under the 2048 retrieved token budget, and the decoder used for the RAG is Llama-3.1-70B-Instruct. RAGs generally have robust performance, on par with the corresponding LLMs, showing that previous long-context benchmarks are either too simple in reasoning complexity or contain detectable noise.} 
    \label{passiveplusinteractive} 
    \label{fig:3}
\end{figure*} 

However, several technical challenges must be addressed to construct a practical benchmark. First, ensuring diverse reasoning patterns and operations - ideally providing a comprehensive coverage of GSM-8K - is crucial to enable fair and thorough evaluation. Second, computational graphs must be effectively translated into natural language that is both human- and LLM-readable, eliminating ambiguity from memorization and ensuring a clear focus on reasoning. 

We introduce \sysb, a long-context benchmarking framework that scales and controls reasoning complexity and noise through fine-grained manipulation of computational graphs, enabling their translation into diverse, human- and LLM-readable problems (Figure \ref{figureone}(b)). Specifically, 
\begin{itemize}[itemsep=0.0pt, topsep=0pt, leftmargin=*]
    \item in Section~\ref{sec:graphtoreasoning}, ~\ref{sec:noisecontruction}, we discuss how to control and scale reasoning complexity by manipulating computational graphs and how to insert noise. 
    \item Section~\ref{reverseproblem} discusses methods to ensure comprehensive coverage of reasoning patterns appeared in GSM-8K. 
    \item Section~\ref{attachlanguage} shows ways enabling the computational graphs to LLM-understandable natural language mapping. 
\end{itemize}

Figure \ref{figureone}(a) shows where our benchmark positions among existing benchmarks, demonstrating the effectiveness of \sysb for long-context reasoning evaluations.

We conduct a comprehensive evaluation of 17 state-of-the-art LLMs on zero-noise problems and 10 LLMs on various noise-injected tasks using \sysb. 
We inclusively covered a wide range of models, including both popular closed-source and open-source options, conventional transformers and hybrid architectures, models of varying sizes, and both reasoning and non-reasoning LLMs. 
In zero-noise settings, recent reasoning-optimized LLMs demonstrate substantial improvements over their non-reasoning counterparts. Notably, Deepseek-R1 \citep{deepseekai2025deepseekr1incentivizingreasoningcapability} achieves an average AUC score nearly four times higher than previous SOTA models. However, in noise-injected scenarios, LLMs exhibit varying degrees of performance degradation. Our analysis reveals several key observations:
  

\begin{itemize}[itemsep=0.0pt, topsep=0pt, leftmargin=*]
    \item \textbf{Decay with reasoning complexity:} LLM performance follows a strikingly consistent sigmoid or exponential decay as problem difficulty increases, highlighting fundamental limitations in scaling reasoning capabilities.  
     \item \textbf{Decay with noise:} Performance degradation intensifies as context length increases within the same difficulty level, while longer context brings sharper degradation of the LLM performance. 
    \item \textbf{LLM thinking patterns:} Models consistently perform better on forward-thinking tasks than on backward-thinking ones, suggesting a fundamental asymmetry in their reasoning strategies.  
    \item \textbf{Influence of Repeated sampling:} Repeated sampling~\citep{brown2024large} on \sysb reveals a clear pattern: performance improves linearly with increased inference steps but at an exponentially growing computation cost, underscoring inefficiencies in current LLM inference strategies.  
\end{itemize} 
