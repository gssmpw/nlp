\section{Background and Related Work}
\label{sec:background}




\subsection{Small Language Model}


Small language models (SLMs) are characterized by their reduced parameter count and computational requirements compared to larger LLMs. These models can be general-purpose or specialized, and this paper focuses on the evaluation of pre-trained general-purpose SLMs, which will be referred to simply as SLMs throughout the remainder of the paper. Structurally, SLMs and LLMs share a common architecture, consisting of stacked decoder-only layers from the transformer framework~\cite{attention_all_you_need}, generating outputs in an autoregressive manner. SLMs typically have fewer decoder layers, attention heads, and smaller hidden dimensions, and they are trained on smaller, high-quality datasets~\cite{lu2024smalllanguagemodelssurvey}. While there is no clear definition for the parameter count that qualifies a model as an SLM, models that can operate on consumer-grade devices are generally considered to fall within this category. This paper focuses on models smaller than 8GB in FP16 precision, corresponding to fewer than 4B parameters.




% 文本总结评估是什么主要涉及哪些方面，常用的方法有哪些，以及现有的evaluation集中在传统方法和LLM。
\subsection{Text Summarization and Its Evaluation}

% Text summarization evaluation involves assessing the quality of automatically generated summaries from aspects such as relevance, coherence, and factual consistency, ensuring they capture the necessary information from the original text. Existing summarization evaluation methods can be broadly categorized into three types: human evaluation, reference-based evaluation, and LLM evaluation. 

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/evaluation_method_compare.pdf}
    \caption{Comparison of text summarization evaluation. }
    \label{fig:evaluation_compare}
\end{figure}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{ARR_version/fig/workflow3.pdf}
%     \caption{SLM news summary evaluation workflow. }
%     \label{fig:workflow}
% \end{figure}

Text summarization is the process of compressing a large volume of text into a shorter version while retaining its main ideas and essential information. Traditional text summarization models usually take a single news article as input and output a summary. However, SLMs require both a prompt and an article to generate a summary. 

% The bottom of Figure~\ref{fig:workflow} illustrates the summary generation with SLMs.


The evaluation methods of text summarization can be categorized into three categories, as shown in Figure~\ref{fig:evaluation_compare}: human evaluation, reference-based evaluation, and LLM evaluation. 


\noindent \textbf{Human evaluation} is an intuitive and effective method, involving the hiring of annotators to score the generated text. For instance, \citet{huang-etal-2020-achieved} employed human evaluation to evaluate 10 representative summarization models, revealing that extractive summary methods often performed better in human evaluation. The Summeval benchmark~\cite{fabbri2021summeval} utilized human evaluation methods to evaluate 23 traditional models and found Pegasus~\cite{pegasus} performed best. \citet{dead_summarization} used human evaluation to assess LLMs and found that they were more favored by evaluators. \citet{zhang2024benchmarking} also used human evaluation to evaluate the LLMs, discovering that instruction tuning had a greater impact on performance than model size. INSTRUSUM~\cite{liu2023benchmarking} evaluated instruction controllable summarization of LLMs by annotators and found them still unsatisfactory in summarizing text based on complex instructions.


\noindent \textbf{Reference-based evaluation} is a commonly used method. It scores generated text by calculating its similarity to reference texts using various metrics, and many such metrics have been proposed~\cite{lin-2004-rouge,papineni-etal-2002-bleu,sellam-etal-2020-bleurt,bert-score,bartscore}. In addition to human evaluation, \citet{huang-etal-2020-achieved} and ~\citet{fabbri2021summeval} also used many reference-based metrics to evaluated models and found metrics like Rouge and BertScore are very close to human evaluations. \citet{google-2023-benchmarking} used the Rouge metric to measure text similarity. Tiny Titans~\cite{tiny_titan} assessed the performance of small models in meeting summarization and found FLAN-T5 performed best.


\noindent \textbf{LLM evaluation} is explored by recent studies~\cite{goyal2022news,kendeer,gao2023human-like,liu2023benchmarking,wang2023chatgpt}. This approach involves guiding LLMs with prompts and providing both the summary and the original article for scoring. However, \citet{not-yet} indicated that LLMs may exhibit biases when evaluating different models, warranting further research to ensure fairness.

In conclusion, existing evaluations primarily evaluate the news summarization capabilities of traditional models or LLMs, leaving a gap in the evaluation of SLMs.
% This paper aims to address this with reference-based evaluation methods.





% The evaluation of text summarization models has been a significant area of research, addressing various dimensions such as relevance, coherence, and factual consistency~\cite{huang-etal-2020-achieved,fabbri2021summeval,dead_summarization,zhang2024benchmarking,liu2023benchmarking}. For instance, \citet{huang-etal-2020-achieved} employed ROUGE and human evaluation to assess 10 representative summarization models, revealing that extractive summarizers often performed better in human assessments. The Summeval benchmark~\cite{fabbri2021summeval} analyzed existing metrics and utilized human evaluation methods to evaluate 23 models, providing a comprehensive overview of summarization performance. Additionally, \citet{dead_summarization} examined the performance of LLMs in text summarization, noting a preference for these models among evaluators, which has implications for the perceived effectiveness of LLMs over SLMs. In a study by \citet{zhang2024benchmarking}, human evaluation indicated that instruction tuning significantly influenced performance, more so than model size, suggesting that the methodology of training can be as crucial as the model architecture itself. The INSTRUSUM benchmark~\cite{liu2023benchmarking} focused on instruction-controllable summarization of LLMs, highlighting ongoing challenges in summarizing text based on complex instructions.

% Despite the promising capabilities of SLMs, comprehensive evaluations of their text summarization performance remain limited. The Tiny Titans study~\cite{tiny_titan} assessed SLMs in the context of meeting summarization but primarily focused on older models, excluding newer iterations such as Phi3 and Llama3. Furthermore, its evaluation relied solely on the ROUGE metric, which may not capture the full spectrum of summarization quality. While \citet{lu2024smalllanguagemodelssurvey} provided a broader evaluation of SLMs across various tasks, news summarization was notably absent. This paper aims to fill this gap by providing a thorough evaluation of SLM performance specifically in news summarization, introducing innovative methodologies that highlight the unique strengths of SLMs in this domain. 

% To justify our choice of SLMs, we selected models based on their performance in prior studies, their architectural innovations, and their applicability to resource-constrained environments. Our findings suggest that SLMs not only match but can sometimes exceed the performance of LLMs in specific summarization tasks, thereby offering a compelling alternative for practical applications. By addressing the limitations of previous studies, particularly the lack of focus on news summarization and the reliance on outdated models, this work contributes to a deeper understanding of SLM capabilities and their potential in real-world applications. Furthermore, we contextualize our findings within the broader landscape of summarization techniques, discussing the implications of using SLMs compared to LLMs and traditional summarization methods, particularly in scenarios where computational resources are limited. This discussion underscores the relevance of our research in advancing the field of NLP and highlights the critical need for further exploration of SLMs in diverse summarization contexts, thereby establishing a clear connection between the identified gaps in the literature and our research objectives.

