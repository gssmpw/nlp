\section{Challenges in Existing Summary Evaluation Method}
\label{sec:appendix}
In this section, we will supplement the detailed analysis of existing evaluation methods and explain why we use the reference-based evaluation method.

\subsection{Human evaluation}
Human evaluation remains the most intuitive and effective method for text summarization evaluation~\cite{fabbri2021summeval,pu2023summarization,liu2023benchmarking,zhang2024benchmarking}. Annotators generally score generated summaries on aspects such as relevance and coherence (scale 1-5) based on the source text, and the final score is the average across all samples. Despite its reliability, human evaluation faces three main challenges for large-scale use: (1) Time-consuming—even evaluating 100 samples for each model can take weeks~\cite{zhang2024benchmarking}; (2) Low reproducibility—scores may vary due to different annotators; (3) High cost—only expert annotators are suitable, but hiring them is costly~\cite{fabbri2021summeval}. As shown in Figure~\ref{fig:kd_heatmap_relevance}, experts demonstrate significantly higher agreement than crowd-sourced annotators.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/kendall_correlation_heatmap_relevance.pdf}
    \caption{Pairwise system-level Kendall’s tau correlation on relevance for different annotators in SummEval benchmark. "CS" is the abbreviation for crowd-sourced. Darker colors indicate better consistency. }
    \label{fig:kd_heatmap_relevance}
\end{figure}



\subsection{Reference-based Evaluation}
\label{sec:reference eva}
Reference-based methods are among the earliest for evaluating text summarization~\cite{papineni-etal-2002-bleu}. They are more cost-effective and faster than human evaluation, using statistical metrics or small models to compute the similarity between generated and reference summaries. Higher similarity indicates better quality. There are two main categories: one measures textual overlap (e.g., ROUGE~\cite{lin-2004-rouge}, BLEU~\cite{papineni-etal-2002-bleu}); the other uses language models like BERT~\cite{devlin-etal-2019-bert} to compute semantic similarity (e.g., BertScore~\cite{bert-score}, BLEURT~\cite{sellam-etal-2020-bleurt}). However, the reference quality affects the evaluation reliability heavily~\cite{how_well,zhang2024benchmarking}. Heuristic methods used to create references in many datasets often lead to poor-quality summaries. For instance, XSum~\cite{xsum} uses only the first sentence of a news article as the reference, resulting in 54\% of summaries missing key information~\cite{how_well}. Moreover, since different datasets use varied reference generation methods, evaluation reliability fluctuates. Table~\ref{tab:compare_kt_corr} illustrates the impact of reference quality on reference-based methods. The correlation with human evaluation varies significantly across datasets when using the original reference for each dataset. It highlights the need for a unified, high-quality reference generation method.


\subsection{LLM evaluation}
Recent studies~\cite{goyal2022news,kendeer,gao2023human-like,liu2023benchmarking,wang2023chatgpt} have explored using LLMs for human-like summarization evaluation to achieve better alignment with human judgment. This approach involves guiding LLMs with prompts and providing both the summary and the original article for scoring. However, while LLM-based evaluation is reproducible and promising, it shares cost and efficiency challenges with human evaluation. Using commercial LLM APIs for large datasets can be expensive, and deploying open-source LLMs (e.g., 70B models) locally requires costly GPUs like the Nvidia A100 for reasonable speed. Additionally, studies~\cite{not-yet} indicate that LLMs may exhibit biases when evaluating different models, warranting further research to ensure fairness.

Overall, existing evaluation methods face issues related to financial cost, reproducibility, and effectiveness. Among them, reference-based methods are the most cost-effective and efficient. Therefore, we consider the LLM-augmented reference-based evaluation method as our metrics.


% \section{Experimental Setup for LLM-Generated Reference Verification}

% We use prompt 2 in Figure~\ref{fig:prompt} as the input of the LLM. For all datasets, we consistently use two sentences to summarize the article. We also perform post-processing on the output of the LLM. We filter out some prompting words, such as "The news summary is:", and retain only complete sentences to form the final reference summaries. And the reference summaries are generated by Qwen1.5-72B-Chat~\cite{qwen1} and Llama2-70B-Chat~\cite{touvron2023llama2openfoundation} with vLLM framework~\cite{vllm}. All models do not restrict output length, and the temperature was 0. In Table~\ref{tab:compare_kt_corr}, BertScore exhibits the best performance among the evaluated metrics. The correlation between BertScore and human evaluation in terms of relevance is approaching that observed among different expert evaluations.



\section{Model Details}
In this section, we describe all benchmarked SLMs in our experiments.

\textbf{LiteLlama~\cite{huggingface2024litelama}:} LiteLlama is an open-source reproduction of Llama2~\cite{touvron2023llama2openfoundation}. It has 460M parameters and is trained on 1T tokens from the RedPajama dataset~\cite{together2023redpajama}.

\textbf{Bloom-560M~\cite{bloom}:} Bloom-560M is part of the BLOOM (BigScience Large Open-science Open-access Multilingual) family of models developed by the BigScience project. It has 560M parameters and is trained on 1.5T pre-processed text.

\textbf{TinyLlama~\cite{zhang2024tinyllama}:} TinyLlama has the same architecture and tokenizer as Llama 2. It has 1.1B parameters and is trained on 3T tokens. We use the chat finetuned version.

\textbf{GPT-Neo series~\cite{gpt-neo}:} GPT-Neo Series are open-source language models developed by EleutherAI to reproduce GPT-3 architecture. It has two versions with 1.3B and 2.7B parameters. They are trained on 380B and 420B tokens, respectively, on the Pile dataset~\cite{gao2020pile,biderman2022datasheet_pile}.

\textbf{Qwen2 series~\cite{qwen}:} Qwen2 comprises a series of language models pre-trained on multilingual datasets. And some models use instruction tuning to align with human preferences. We select models with no more than 7B parameters for benchmarking, including 0.5B, 0.5B-Ins, 1.5B, 1.5B-Ins, 7B, and 7B-Ins. Models with the "Ins" suffix indicate the instruction tuning version.

\textbf{InterLM2 series~\cite{cai2024internlm2}:} The InternLM2 series includes models with various parameter sizes. They all support long contexts of up to 200,000 characters. We select the 1.8B version and the 1.8B-Chat version for evaluation.

\textbf{Gemma-1.1~\cite{team2024gemma}:} Gemma-1.1 is developed by Google and is trained using a novel RLHF method. We select the 2B instruction tuning version for benchmarking.

\textbf{MiniCPM~\cite{hu2024minicpm}:} MiniCPM is an end-size language model with 2.7B parameters. It employs various post-training methods to align with human preferences. We select the supervised fine-tuning (SFT) version for benchmarking.

\textbf{Llama3.2 series~\cite{llama3.2}:} In the Llama 3.2 series, Meta introduced two SLMs, 1B and 3B, designed for edge devices. While Meta highlighted their capability to summarize social media messages, their performance in news summarization has yet to be evaluated.


\textbf{Phi series~\cite{microsoft2023phi2,abdin2024phi3}:} Phi series are some SLMs developed by Microsoft. They are trained on high-quality datasets. We select Phi-2(2.7B parameters) and Phi-3-Mini(3.8B parameters, 4K context length) for evaluation. 

\textbf{Brio~\cite{brio}: } Brio is one of the state-of-the-art specialized language models designed for text summarization, which leverages reinforcement learning to optimize the selection of sentences, ensuring high-quality, informative, and coherent summaries. It only has 406M parameters, and we select the version trained on the CNN/DM dataset.

\textbf{Pegasus-Large~\cite{pegasus}:} Pegasus-Large also is a specialized language model designed for abstractive text summarization, utilizing gap-sentence generation pre-training to achieve exceptional performance on summarization tasks. It has 568M parameters.