\section{Introduction}


Automatic text summarization is a fundamental challenge in Natural Language Processing (NLP) that plays a crucial role in various applications, including search engine optimization~\cite{2019soe}, financial forecasting~\cite{fintech}, and public sentiment analysis~\cite{twitter_news}. The ability to distill large volumes of information into concise summaries is particularly important in today's fast-paced digital environment, where users often seek quick insights from news articles. Recent advancements in large language models (LLMs) such as GPT-3~\cite{NEURIPS2020_gpt3}, OPT~\cite{opt}, and Llama2~\cite{touvron2023llama2openfoundation} have significantly enhanced the quality and coherence of generated summaries compared to traditional models~\cite{luhn1958automatic, dong-etal-2018-banditsum, zhang-etal-2018-neural, t5, genest-lapalme-2012-fully}. However, the hundreds of billions of parameters in these LLMs require substantial computational and storage resources. For example, deploying the Llama2-70B model at FP16 precision typically necessitates two Nvidia A100 GPUs with 80GB of memory each. This means that only organizations with significant computing power can offer LLM services, raising concerns about service stability and data privacy.


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/model_parameter_score.pdf}
    \caption{Variation of SLMs in size and BertScore for news summarization. }
    \label{fig:intro}
\end{figure}




As a result, general-purpose small language models (SLMs) have emerged as a potential alternative, as shown in Figure~\ref{fig:intro}. They share the same decoder-only architecture as LLMs but have fewer than 4B parameters~\footnote{Currently, there is no clear definition of SLMs, but they generally have fewer than 4 billion parameters.}~\cite{azure2024phi,qwen,abdin2024phi3,llama3.2}. Same as the LLMs, they generate specified content based on prompts. SLMs are designed to run efficiently on edge devices like smartphones and personal computers, making them more accessible. This accessibility makes SLMs an attractive option for applications requiring quick, stable, and low-cost solutions that also protect user privacy, such as summarizing news articles for mobile users or enhancing data processing privacy in sensitive environments.

Despite the promising capabilities of SLMs, their specific performance in the domain of news summarization remains underexplored. Previous studies, such as Tiny Titans~\cite{tiny_titan}, have evaluated specifically fine-tuned SLMs in meeting summarization. The adopted ROUGE metric~\cite{lin-2004-rouge} only reflects the quality in terms of relevance. In addition, broader evaluations of SLMs across various tasks have been evaluated but omitted news summarization~\cite{lu2024smalllanguagemodelssurvey}, highlighting the gap in understanding SLM in this area.

This paper aims to address these gaps by conducting a comprehensive evaluation of pre-trained general-purpose SLMs in news summarization, a process that abstracts news articles into shorter versions while preserving key information. We seek to answer the following research questions:
% \begin{itemize}
%     \item How do different SLMs compare to each other, and how do they compare to LLMs?
%     \item What is the impact of prompt complexity on the performance of SLMs?
%     \item How does instruction tuning affect the summarization capabilities of different SLMs?
% \end{itemize}


First, \textbf{How do different SLMs compare to each other, and how do they compare to LLMs?} We benchmark 19 SLMs on 2,000 news samples\footnote{The codes and results are available at \url{https://github.com/Xtra-Computing/SLM_Summary_Benchmark}}. Our findings reveal that the news summarization capabilities of different SLMs vary significantly as shown in Figure~\ref{fig:intro}. And the best-performing SLMs can generate news summaries comparable in quality to those produced by LLMs while producing shorter summaries. Notably, Phi3-Mini and Llama3.2-3B-Ins emerge as the top performers.


Second, \textbf{What is the impact of prompt complexity on the performance of SLMs?} We compare the impact of prompts with varying levels of detail on the quality of the summaries. Our findings indicate that prompt engineering has a limited impact on enhancing summarization abilities, with even complex prompts potentially degrading model performance. Thus, simple prompts suffice in practical applications.

Third, \textbf{How does instruction tuning affect the summarization capabilities of different SLMs?} We find that the effects of instruction tuning on SLMs vary significantly. While the Llama3.2 series models show substantial improvements after instruction tuning, which aligns with existing works~\cite{zhang2024benchmarking}, models like Qwen2 and InternLM2 exhibit minimal changes.




% The rest of the paper is organized as follows: Section~\ref{sec:background} covers the background and related work, Section~\ref{sec:method} introduces our reference summary generation, Section~\ref{sec:setup} illustrates the benchmark design, Section~\ref{sec:benchmark} presents the benchmarking results of SLMs, Section~\ref{sec:factor} analyzes the influencing factors on SLM summarization, Section~\ref{sec:Verification} verifies our evaluation method, and Section~\ref{sec:conclusion} concludes with a summary of findings and future work.

The rest of the paper is organized as follows: Section~\ref{sec:background} covers the background and related work, Section~\ref{sec:method} introduces reference summary generation, Section~\ref{sec:setup} illustrates the benchmark design, Section~\ref{sec:benchmark} presents presents the benchmarking results of SLMs, Section~\ref{sec:factor} analyzes the influencing factors on SLM summarization, and Section~\ref{sec:conclusion} concludes our findings.