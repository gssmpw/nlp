%实验部分介绍数据集，如何生成的数据集，为什么选择500个
%这种测评方法的具体实现，数据清洗？prompt设计
%验证这种方法的结果和llm做测评是相媲美的。
%选择了那些tiny LLM进行测评，给出测评结果
%llm测评和我们测评的相关性。
%tiny llm的能力和34b，70b相比如何？
%每B的得分值
%与人类写手得分相比
%some example 给出具体的例子
%平均长度
%多少分算好？60分以下，65分以下，70分以上
%小模型上有什么问题？
%参数从多少到多少提升最大？模型越大性能越强，但到了一定程度提升不明显了
%instruct对性能的影响
%提出的方法的robust，不同prompt，不同LLM
%加入summary level？
%gpt-4o的测评？不用啊，本身就是测的和大模型的相似度
%data相关？为什么更好？因为训练数据方面么？
%template的影响？
%影响summarization能力的因素有哪些
%输出太长
%给出例子说明bertscore的有效性，相比原始reference更好
%instruct 对长度的影响
%小模型生成总结长度更短
%解释指标的含义，为什么要衡量总结的长短
%稳定性
%常见存储16GB？
%原始新闻的质量？给出例子
%为什么采用这种比较方法，因为不受reference长度影响结果
% 所有figure to fig.


\section{Benchmark Design}\label{sec:setup}


In this section, we first introduce the dataset for news summarization evaluation, followed by the SLM details, evaluation metrics, and reference summary generation.

% \subsection{Experimental Setup}
\subsection{News Article Selection }

All experiments are conducted on the following four datasets: CNN/DM~\cite{cnndm}, XSum~\cite{xsum}, Newsroom~\cite{newsroom}, and BBC2024. The first three datasets have been widely used for verifying model text summarization~\cite{goyal2022news,google-2023-benchmarking,zhang2024benchmarking}. However, their older data may overlap with the training datasets of some models. To address this, we create BBC2024, featuring news articles from January to March 2024 on the BBC website~\footnote{\url{http://dracos.co.uk/made/bbc-news-archive/}}. Based on prior research, 500 samples per dataset are sufficient to differentiate the models~\cite{google-2023-benchmarking}. We randomly select 500 samples from each of the four test sets and construct 2000 samples in total. Each sample contains 500 to 1500 tokens according to the Qwen1.5-72B-Chat tokenizer.



\subsection{Model Selection}
We select 21 popular models with parameter sizes not exceeding 4 billion for benchmarking, including 19 SLMs and 2 models specifically designed for text summarization, Pegasus-Large and Brio, which have been fine-tuned on text summarization datasets, detailed in Table~\ref{tab:model_list}.

% \textbf{LiteLlama~\cite{huggingface2024litelama}:} LiteLlama is an open-source reproduction of Llama2~\cite{touvron2023llama2openfoundation}. It has 460M parameters and is trained on 1T tokens from the RedPajama dataset~\cite{together2023redpajama}.

% \textbf{Bloom-560M~\cite{bloom}:} Bloom-560M is part of the BLOOM (BigScience Large Open-science Open-access Multilingual) family of models developed by the BigScience project. It has 560M parameters and is trained on 1.5T pre-processed text.

% \textbf{TinyLlama~\cite{zhang2024tinyllama}:} TinyLlama is an SLM which has the same architecture and tokenizer as Llama 2. It has 1.1B parameters and is trained on 3T tokens. We use the chat finetuned version.

% \textbf{GPT-Neo series~\cite{gpt-neo}:} GPT-Neo Series are open-source language models developed by EleutherAI to reproduce GPT-3 architecture. It has two versions with 1.3B and 2.7B parameters. They are trained on 380B and 420B tokens, respectively, on the Pile dataset~\cite{gao2020pile,biderman2022datasheet_pile}.

% \textbf{Qwen2 series~\cite{qwen}:} Qwen2 comprises a series of language models pre-trained on multilingual datasets. And some models use instruction tuning to align with human preferences. We select models with no more than 7B parameters for benchmarking, including 0.5B, 0.5B-Ins, 1.5B, 1.5B-Ins, 7B, and 7B-Ins. Models with the "Ins" suffix indicate the instruction tuning version.

% \textbf{InterLM2 series~\cite{cai2024internlm2}:} The InternLM2 series includes models with various parameter sizes. They all support long contexts of up to 200,000 characters. We select the 1.8B version and the 1.8B-Chat version for evaluation.

% \textbf{Gemma-1.1~\cite{team2024gemma}:} Gemma-1.1 is developed by Google and is trained using a novel RLHF method. We select the 2B instruction tuning version for benchmarking.

% \textbf{MiniCPM~\cite{hu2024minicpm}:} MiniCPM is an end-size language model with 2.7B parameters. It employs various post-training methods to align with human preferences. We select the supervised tuning (SFT) version for benchmarking.

% \textbf{Phi series~\cite{microsoft2023phi2,abdin2024phi3}:} Phi series are some SLMs developed by Microsoft. They are trained on high-quality datasets. We select Phi-2(2.7B parameters) and Phi-3-Mini(3.8B parameters, 4K context length) for evaluation. 

% \textbf{Brio~\cite{brio}: } Brio is one of the state-of-the-art specialized language models designed for text summarization, which leverages reinforcement learning to optimize the selection of sentences, ensuring high-quality, informative, and coherent summaries. It only has 406M parameters, and we select the version trained on the CNN/DM dataset.

% \textbf{Pegasus-Large~\cite{pegasus}:} Pegasus-Large also is a specialized language model designed for abstractive text summarization, utilizing gap-sentence generation pre-training to achieve exceptional performance on summarization tasks. It has 568M parameters.


%model details

% Meanwhile, to compare the performance of tiny LLMs with larger LLMs, we also consider the Llama3-70B. 

All models are sourced from the Hugging Face\footnote{\url{https://huggingface.co}} and tested based on the lm-evaluation-harness tool~\cite{eval-harness}. For general-purpose SLMs, we use the prompt from the huggingface hallucination leaderboard\footnote{\url{https://huggingface.co/spaces/hallucinations-leaderboard/leaderboard}} (Prompt 1 in Figure~\ref{fig:prompt}). To ensure reproducibility, we employ the greedy decoding strategy, generating until a newline character or <EOS> token is reached. Outputs are post-processed to remove prompt words and incomplete sentences. Given the 2048-token limit of some models, we only evaluate the zero-shot approach.

\begin{table}[]
\centering
\begin{adjustbox}{max width=1\columnwidth}
% \resizebox{\columnwidth}{!}{
\begin{tabular}{lccc}
\hline
Model          & Parameters & \begin{tabular}[c]{@{}c@{}}Instruction \\ Tuning\end{tabular} & Reference                                                              \\ \hline
Brio $^{\ast}$          & 406M         & $\checkmark$                                                  & \cite{brio} \\
LiteLlama      & 460M         & $\times$                                                      & \cite{huggingface2024litelama}       \\
Qwen2-0.5B     & 500M         & $\times$                                                      & \cite{qwen}              \\
Qwen2-0.5B-Ins & 500M         & $\checkmark$                                                  & \cite{qwen}     \\
Bloom-560M          & 560M         & $\times$                                                      & \cite{bloom}        \\
Pegasus-Large$^{\ast}$  & 568M         & $\checkmark$                                                  & \cite{pegasus}         \\
TinyLlama      & 1.1B         & $\checkmark$                                                  & \cite{zhang2024tinyllama} \\
Llama3.2-1B    & 1.2B         & $\times$                                                      & \cite{llama3.2} \\
Llama3.2-1B-Ins& 1.2B         & $\checkmark$                                                  & \cite{llama3.2} \\
GPT-Neo-1.3B   & 1.3B         & $\times$                                                      & \cite{gpt-neo}      \\
Qwen2-1.5B     & 1.5B         & $\times$                                                      & \cite{qwen}              \\
Qwen2-1.5B-Ins & 1.5B         & $\checkmark$                                                  & \cite{qwen}     \\
InternLM2-1.8B & 1.8B         & $\times$                                                      & \cite{cai2024internlm2}     \\
InternLM2-1.8B-Chat & 1.8B    & $\checkmark$                                                  & \cite{cai2024internlm2}     \\
MiniCPM        & 2.4B         & $\checkmark$                                                  & \cite{hu2024minicpm}  \\
Gemma-1.1      & 2.5B         & $\checkmark$                                                  & \cite{team2024gemma}       \\
GPT-Neo-2.7B   & 2.7B         & $\times$                                                      & \cite{gpt-neo}       \\
Phi-2          & 2.7B         & $\times$                                                      & \cite{microsoft2023phi2}             \\
Llama3.2-3B    & 3.2B         & $\times$                                                      & \cite{llama3.2} \\
Llama3.2-3B-Ins& 3.2B         & $\checkmark$                                                  & \cite{llama3.2} \\
Phi-3-Mini     & 3.8B         & $\checkmark$                                                  & \cite{abdin2024phi3}   \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{Models for news summarization benchmark. Models marked with $^{\ast}$ have been fine-tuned on text summarization datasets.} }
\label{tab:model_list}
\end{table}


% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{latex/fig/benchmark_prompt.pdf}
%     \caption{The benchmark prompt template for news summary generation. }
%     \label{fig:benchmark_prompt}
% \end{figure}


\subsection{Evaluation Metric }

We evaluate the summary quality in relevance, coherence, factual consistency, and text compression using BertScore~\cite{bert-score}, HHEM-2.1-Open~\cite{HHEM-2.1-Open}, and summary length.

% As mentioned earlier, BertScore aligns well with human preferences for relevance and coherence when using high-quality references. It measures how well the summary captures key information (relevance) and maintains logical flow (coherence). 

BertScore is a robust semantic similarity metric designed to assess the quality of the generated text and achieves the best correlation in Table~\ref{tab:compare_kt_corr}. It compares the embeddings of candidate and reference sentences through contextualized word representations from BERT~\cite{devlin-etal-2019-bert}. It can measure how well the summary captures key information (relevance) and maintains logical flow (coherence). We use the F1 score of BertScore\footnote{\url{https://huggingface.co/microsoft/deberta-xlarge-mnli}}, ranging from 0 to 100, where higher values indicate better quality.

HHEM-2.1-Open is a hallucination detection model which outperforms GPT-3.5-Turbo and even GPT-4. It can evaluate whether news summaries are factually consistent with the original article. It is based on a fine-tuned T5 model to flag hallucinations with a score. When the score falls below 0.5, it indicates a summary is inconsistent with the source. We report the percentage of summaries deemed factually consistent; higher values indicate better performance.

Since BertScore is insensitive to summary length, we also track the average summary length to assess text compression. With similar BertScore results, shorter summaries indicate better text compression ability.


\subsection{Reference Summary Generation}
To mitigate the impact of occasional low-quality reference summaries on evaluation results, as well as the tendency for models within the same series to score higher due to possible similar output content, we utilize two LLMs, Qwen1.5-72B-Chat and Llama2-70B-Chat, to generate two sets of reference summaries. We then average the scores during the SLM evaluation. We use Prompt 2 from Figure~\ref{fig:prompt} as the prompt template and apply a greedy strategy to generate summaries based on the vLLM inference framework~\cite{vllm}.



% Figure 5 shows the average length of the generated summaries for each dataset.






% \subsubsection{Parameter Setup}

% \subsection{Evaluation Results}

% \begin{table*}[]
% \caption{}
% \label{tab:my-table}
% \begin{adjustbox}{max width=2.05\columnwidth}
% \begin{tabular}{lcccccccccccc}
% \hline
% Model                & \multicolumn{3}{c}{XSum}   & \multicolumn{3}{c}{Newsroom} & \multicolumn{3}{c}{CNN/DM} & \multicolumn{3}{c}{BBC2024} \\
% \multicolumn{1}{c}{} & Qwen1.5 & Llama2 & Average & Qwen1.5  & Llama2  & Average & Qwen1.5 & Llama2 & Average & Qwen1.5  & Llama2 & Average \\ \hline
% LiteLama      & 38.68 & 39.24 & 38.96 & 38.02 & 38.70 & 38.36 & 40.58 & 41.38 & 40.98 & 38.05 & 38.86 & 38.45 \\
% Qwen2-0.5     & 69.57 & 70.71 & 70.14 & 65.94 & 66.92 & 66.43 & 69.50 & 70.48 & 69.99 & 68.23 & 69.67 & 68.95 \\
% Qwen2-0.5-Ins & 61.67 & 62.35 & 62.01 & 60.17 & 60.91 & 60.54 & 63.34 & 63.98 & 63.66 & 60.72 & 61.47 & 61.09 \\
% TinyLlama     & 68.07 & 69.16 & 68.61 & 64.70 & 65.63 & 65.16 & 68.66 & 69.23 & 68.95 & 66.93 & 68.07 & 67.50 \\
% GPT-Neo       & 59.67 & 59.55 & 59.61 & 52.54 & 53.11 & 52.83 & 58.37 & 58.81 & 58.59 & 58.16 & 58.18 & 58.17 \\
% Qwen2-1.5     & 72.07 & 72.65 & 72.36 & 69.12 & 69.86 & 69.49 & 71.30 & 72.05 & 71.67 & 71.08 & 71.99 & 71.53 \\
% Qwen2-1.5-Ins & 71.52 & 72.36 & 71.94 & 69.43 & 69.56 & 69.50 & 71.72 & 72.39 & 72.06 & 70.83 & 71.78 & 71.30 \\
% InternLM2-1.8 & 68.64 & 69.20 & 68.92 & 64.27 & 65.00 & 64.64 & 68.81 & 69.49 & 69.15 & 67.88 & 68.74 & 68.31 \\
% InternLM2-1.8-chat   & 67.77   & 68.82  & 68.30   & 64.99    & 65.57   & 65.28   & 68.78   & 69.38  & 69.08   & 68.32    & 69.62  & 68.97   \\
% Gemma-1.1     & 70.80 & 72.27 & 71.53 & 69.42 & 70.30 & 69.86 & 71.88 & 72.83 & 72.35 & 70.60 & 71.67 & 71.13 \\
% MiniCPM       & 65.59 & 65.77 & 65.68 & 62.48 & 63.13 & 62.80 & 66.82 & 67.51 & 67.17 & 65.54 & 65.96 & 65.75 \\
% Phi-2         & 70.95 & 72.03 & 71.49 & 68.15 & 69.14 & 68.64 & 71.19 & 72.21 & 71.70 & 70.38 & 71.50 & 70.94 \\
% Phi-3-mini    & 74.31 & 74.65 & 74.48 & 72.07 & 72.64 & 72.36 & 74.54 & 75.24 & 74.89 & 73.63 & 74.05 & 73.84 \\
% ChatGLM3      & 73.51 & 74.08 & 73.79 & 70.77 & 71.11 & 70.94 & 73.66 & 74.27 & 73.97 & 72.57 & 73.41 & 72.99 \\
% Mistral       & 51.73 & 51.98 &       & 57.13 & 57.45 &       & 51.19 & 52.10 &       & 59.85 & 59.47 &       \\
% Mistral-Ins   & 74.51 & 75.33 & 74.92 & 71.18 & 71.81 & 71.49 & 74.29 & 74.81 & 74.55 & 73.90 & 74.32 & 74.11 \\
% Qwen2-7B      & 75.49 & 75.73 & 75.61 & 73.65 & 73.73 & 73.69 & 74.94 & 75.90 & 75.42 & 74.89 & 75.27 & 75.08 \\
% Qwen2-7B-Ins  & 75.04 & 75.50 & 75.27 & 73.12 & 73.01 & 73.07 & 75.14 & 74.28 & 74.71 & 74.24 & 74.71 & 74.48 \\
% Llama3-instruct      & 77.39   & 76.77  & 77.08   & 75.47    & 74.67   & 75.07   & 77.63   & 76.95  & 77.29   & 76.79    & 76.06  & 76.42   \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table*}



\section{Evaluation Results }
\label{sec:benchmark}
% In this section, we first evaluate the news summarization. Then we compare them with larger LLMs and give model recommendations with different model sizes.



%根据分数划分结果

\subsection{Relevance and Coherence Evaluation}


\begin{table*}[]
\begin{center}
\resizebox{2\columnwidth}{!}{
% \begin{adjustbox}{max width=2.05\columnwidth}
\begin{tabular}{llccccccccr}
\hline
Score Range                  & Model         & \multicolumn{2}{c}{XSum} & \multicolumn{2}{c}{Newsroom} & \multicolumn{2}{c}{CNN/DM} & \multicolumn{2}{c}{BBC2024} & Average \\
                    &                    & Qwen1.5 & Llama2 & Qwen1.5 & Llama2 & Qwen1.5 & Llama2 & Qwen1.5 & Llama2 & \multicolumn{1}{l}{} \\ \hline
\multirow{4}{*}{$< 60$} & LiteLlama           & 38.68   & 39.24  & 38.02   & 38.70  & 40.58   & 41.38  & 38.05   & 38.86  & 39.19                \\
                    & Bloom-560M              & 48.23   & 48.43  & 42.73   & 43.28  & 49.70   & 50.04  & 47.56   & 48.01  & 47.25                \\
                    & GPT-Neo-1.3B            & 59.67   & 59.55  & 52.54   & 53.11  & 58.37   & 58.81  & 58.16   & 58.18  & 57.30                \\ 
                    & GPT-Neo-2.7B            & 60.11   & 60.15  & 53.65   & 54.14  & 59.15   & 59.97  & 59.29   & 59.50  & 58.25                \\ \hline
\multirow{10}{*}{$60 \sim 70$}
                    & Llama3.2-1B       &62.44    &62.33   &55.25    &55.86   &60.32    &60.67  &62.17&62.33      &60.17    \\
                    & Pegasus-Large      &61.39    &61.44   &61.26    &61.64   &61.94    &62.26  &63.12&63.47      &62.07    \\
                    & Llama3.2-3B       &66.14    &65.84   &59.93    &60.68   &64.19    &64.51  &66.62&66.57      &64.31    \\
                    & MiniCPM            & 65.59   & 65.77  & 62.48   & 63.13  & 66.82   & 67.51  & 65.54   & 65.96  & 65.35                \\
                    & TinyLlama          & 68.07   & 69.16  & 64.70   & 65.63  & 68.66   & 69.23  & 66.93   & 68.07  & 67.56                \\
                    & InternLM2-1.8B      & 68.64   & 69.20  & 64.27   & 65.00  & 68.81   & 69.49  & 67.88   & 68.74  & 67.75                \\
                    & InternLM2-1.8B-Chat & 67.77   & 68.82  & 64.99   & 65.57  & 68.78   & 69.38  & 68.32   & 69.62  & 67.91                \\
                    & Qwen2-0.5B-Ins & 69.30       & 70.01      & 65.85   & 66.65   &69.31   & 70.06 & 67.54 & 68.64        & 68.42   \\
                    & Qwen2-0.5B          & 69.57   & 70.71  & 65.94   & 66.92  & 69.50   & 70.48  & 68.23   & 69.67  & 68.88                \\
                    &Brio   & 70.67  & 70.71  &68.43  & 68.41   &69.86  &70.34  &70.39  & 70.21  & 69.88  \\ \hline
\multirow{7}{*}{$> 70$} & Phi-2              & 70.95   & 72.03  & 68.15   & 69.14  & 71.19   & 72.21  & 70.38   & 71.50  & 70.69                \\
                    & Gemma-1.1          & 70.80   & 72.27  & 69.42   & 70.30  & 71.88   & 72.83  & 70.60   & 71.67  & 71.22                \\
                    & Qwen2-1.5B          & 72.03   & 72.90  & 69.19   & 69.91  & 71.33   & 72.42  & 71.08   & 72.18  & 71.38                \\
                    & Qwen2-1.5B-Ins      & 72.21   & 73.06  & 69.49   & 69.86  & 71.64   & 71.95  & 71.01   & 71.97  & 71.40                \\
                    & Llama3.2-1B-Ins     & 72.24   & 73.27  & 70.54   & 70.98  & 72.43   & 73.61  & 71.68   & 72.55  & 72.16                \\
                    & Phi-3-Mini         & \textbf{74.67}   & {75.14}  & {72.42}   & {72.32}  & \textbf{74.86}   & {74.88}  & {74.08}   & {73.72}  & {74.01} \\  
                    & Llama3.2-3B-Ins     & 74.40   & \textbf{75.33}  & \textbf{72.81}   & \textbf{73.41}  & 74.54   & \textbf{75.29}  & \textbf{74.51}   & \textbf{74.95}  & \textbf{74.41}   \\ \hline
                  
\end{tabular}}
\end{center}
% \end{adjustbox}
\caption{\upshape {BertScore of SLMs on four text summarization datasets. Qwen1.5-72B-Chat and Llama2-70B-Chat are used to generate reference summaries.}}
\label{tab:bertscore_res}
\end{table*}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/bertscore_example5.pdf}
    \caption{ Example summaries from SLMs and LLMs. The bold part is the same as the reference, the underline indicates irrelevant content, and the red indicates incorrect content. Summaries with BertScore above 70, such as those from Llama3.2-3B-Ins, demonstrate similar quality to LLMs. }
    \label{fig:bertscore_example}
\end{figure}

Table~\ref{tab:bertscore_res} shows the average BertScore results of SLMs. All models demonstrate consistent performance across datasets. Due to the significant score differences, we categorize the models into three approximate ranges, using two specialized summarization models, Pegasus-Large and Brio, as reference points.


The first range includes scores below 60. \textbf{Models scoring below 60 struggle to effectively summarize articles.} LiteLlama, Bloom, and GPT-Neo series fall into this range. In terms of relevance, these models often miss key points; in terms of coherence, these models sometimes produce repetitive outputs, leading to overly long summaries. Figure~\ref{fig:bertscore_example} shows an example from LiteLlama, it scores only 40.81, deviating significantly from the news.


The second range covers scores between 60 and 70. \textbf{Models in this range produce useful summaries but occasionally lack key points.} Models like TinyLlama and Qwen-0.5B series fall into this range. In relevance, these models generally relate to the original content but may omit crucial details. In coherence, the sentences can be well-organized. Fig~\ref{fig:bertscore_example} shows examples from Qwen2-0.5B. Although its summary is relevant and coherent to the news article, it omits the final score.


The third range includes scores above 70. \textbf{Models in this range produce summaries comparable to LLMs, with occasional inconsistencies.} They effectively capture key points and maintain coherent structure. Phi3-Mini and Llama3.2-3B-Ins stand out in this group. As shown in Figure~\ref{fig:bertscore_example}, Llama3.2-3B-Ins generates a concise, accurate summary, correctly capturing the match outcome even though the source news lacks a direct score description.







\subsection{Factual Consistency Evaluation}


% \begin{table}[]
% \caption{\upshape{Factual consistency rate of SLMs on news summarization datasets.}}
% \label{tab:fact}
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lccccc}
% \hline
% Model          & XSum  & Newsroom & CNN/DM & BBC2024 & Average \\ \hline
% Qwen2-1.5B     & 95.39 & 92.86    & 97.65  & 93.27   & 94.79   \\
% Qwen2-1.5B-Ins & 94.77 & 95.28    & 98.51  & 94.57   & 95.78   \\
% Gemma1.1       & 95.35 & 96.46    & 98.60  & \textbf{94.68}   & \textbf{96.27}   \\
% Phi2           & 93.00 & 95.46    & 97.41  & 93.43   & 94.82   \\
% Phi3-Mini      & \textbf{96.29} & \textbf{97.09}    & \textbf{99.14}  & 92.27   & 96.20   \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}




During summary generation, SLMs may produce false information due to hallucinations. Therefore, we report the factual consistency rate in Table~\ref{tab:fact}. Models with high BertScore generally have better factual consistency as shown in Figre~\ref{fig:bertscore_example}. SLMs that perform well in fact consistency include Phi3-Mini (96.7\%) and Qwen2-1.5B-Ins (96.5\%), maintaining over 95\% consistency across all datasets. The traditional model, Pegasus-large, outperforms SLMs in fact consistency, which achieves an exceptional 99.9\% average. This is because it often copies sentences from the original text, ensuring alignment but sometimes missing key information. 


% This table also includes LLM results, showing minimal gaps between LLMs and top SLMs, with Phi3-Mini and Qwen2-1.5B even outperforming Llama3-70B-Ins on multiple datasets.


% fig3中加上fact分数





\subsection{Summary Length Evaluation}



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/hist_summary_length.pdf}
    \caption{Average summary length comparison. SLMs with high BertScore generate 50-70 word summaries.}
    \label{fig:summary_length}
\end{figure}




% Average summary length changes with BertScore. The summary length first decreases, then increases, and finally remains stable. 
Figure~\ref{fig:summary_length} shows the average summary lengths on 4 datasets produced by SLMs, which vary significantly across models. For scores below 60, summaries typically exceed 100 words due to redundant content and poor summarization. In the 60–70 range, length varies widely; e.g., the Llama3.2 series includes too many unnecessary details, while TinyLlama omits key information, leading to shorter summaries. Models with scores above 70 generate more consistent summaries, averaging around 50 to 70 words in length. For the top-performing models, Phi3-Mini and Llama3.2 series, Phi3-Mini generates more concise summaries while maintaining similar relevance.




\begin{table}[]
\begin{adjustbox}{max width=1\columnwidth}
\begin{tabular}{lccccc}
\hline
Model              & XSUM &Newsroom & CNN/DM & BBC2024 & Average \\ \hline
LiteLama           & 60.6 & 63.4    & 68.8   & 63.6    & 64.1    \\
Bloom-560M         & 83.6 & 77.4                          & 87.0   & 83.4    & 82.9    \\
GPT-Neo-1.3B       & 91.2 & 76.8                          & 92.4   & 92.0    & 88.1    \\
GPT-Neo-2.7B       & 83.2 & 67.0                          & 87.6   & 84.0    & 80.5    \\
Llama3.2-1B        & 97.6 & 85.8                         & 97.6  & 95.8      & 94.2    \\
Pegasus-Large      & 99.6 & 100.0                         & 100.0  & 100.0   & \textbf{99.9}    \\
Llama3.2-3B        & 98.8 & 85.6                         & 98.8    & 97.4    & 95.2    \\
MiniCPM            & 85.8 & 72.8                          & 93.6   & 89.6    & 85.5    \\
TinyLlama          & 95.4 & 92.4                          & 98.4   & 98.0    & 96.1    \\
InternLM2-1.8B     & 91.8 & 87.4                          & 96.0   & 94.8    & 92.5    \\
InternLM2-1.8B-Chat & 88.6 & 84.6                          & 93.8   & 90.8    & 89.5    \\
Qwen2-0.5B-Ins     & 87.2 & 86.0                          & 93.2   & 89.6    & 89.0    \\
Qwen2-0.5B         & 93.0 & 92.8                          & 96.8   & 95.2    & 94.5    \\
Brio               & 94.8 & 89.0                          & 97.8   & 95.4    & 94.3    \\
Phi2               & 95.2 & 93.4                          & 97.8   & 96.8    & 95.8    \\
Gemma-1.1          & 96.6 & 93.6                          & 97.8   & 94.6    & 95.7    \\
Qwen2-1.5B         & 95.6 & 96.2                          & 97.4   & 96.6    & 96.5    \\
Qwen2-1.5B-Ins     & 91.8 & 91.2                          & 94.4   & 92.4    & 92.5    \\
Llama3.2-1B-Ins    & 92.2 & 88.0                          & 95.4   & 94.0    & 92.4    \\
Phi3-Mini          & 96.0 & 94.8                          & 98.2   & 97.6    & \textbf{96.7}    \\ 
Llama3.2-3B-Ins    & 93.2 & 93.2                          & 96.6   & 95.6    & 94.7    \\ \hline
% ChatGLM3           & 96.0 & 95.2                          & 99.2   & 96.0    & 96.6    \\
% Mistral-7B-Ins     & 92.6 & 88.0                          & 96.8   & 93.0    & 92.6    \\
% Qwen2-7B-Ins       & 94.0 & 95.0                          & 98.6   & 95.6    & 95.8    \\
% Llama3-70B-Ins     & 94.6 & 94.8                          & 97.6   & 95.8    & 95.7    \\
% Qwen2-72B-Ins      & 97.0 & 97.6                          & 98.8   & 98.4    & 98.0    \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{Factual consistency rate (\%) on news datasets.}}
\label{tab:fact}
\end{table}

% Summary length reflects the model's ability to compress information. Although TinyLlama produces the shortest summaries, its low BertScore suggests it misses critical details. Therefore, we consider text compression only when BertScore exceeds 70. Among these, Gemma-1.1 (43 words) and Qwen2-1.5-Ins (41 words) demonstrate the best compression performance.

%总结
\subsection{Comparison with LLMs}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\linewidth]{latex/fig/vsLLM_bertscore.pdf}
%     \caption{ Bertscore between two models. }
%     \label{fig:vsllm_bertscore}
% \end{figure}



% \begin{table}[]
% \centering
% % \resizebox{\columnwidth}{!}{
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lcccc}
% \hline
% Model &
%   \begin{tabular}[c]{@{}c@{}}Minimum\\ length\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Maximum\\ length\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Average\\ length\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Standard\\ Deviation\end{tabular} \\ \hline
% Phi2           & 17 & 219 & 51.00 & 38.27 \\
% Gemma-1.1      & 1  & 72  & 41.84 & 8.68  \\
% Qwen2-1.5B-Ins & 16 & 206 & 40.97 & 17.85 \\
% Phi3-Mini      & 25 & 182 & 50.69 & 13.09 \\
% ChatGLM3       & 15 & 139 & 59.31 & 19.33 \\
% Mistral-7B-Ins & 26 & 191 & 59.62 & 27.85 \\
% Qwen2-7B-Ins   & 33 & 169 & 58.57& 16.06 \\
% Llama3-70B-Ins & 51 & 95  & 71.06 & 7.31  \\
% Qwen2-72B-Ins  & 31 & 254 & 85.75 & 21.67 \\ \hline
% \end{tabular}
% \end{adjustbox}
% \caption{\upshape{Summary length comparison of SLMs and LLMs on BBC2024 dataset.}}
% \label{tab:vs_llm_length}
% \end{table}

To further evaluate the SLM performance in news summarization, we compare the well-performing SLMs with LLMs, including ChatGLM3~\cite{glm2024chatglm}, Mistral-7B-Ins~\cite{jiang2023mistral}, Llam3-70B-Ins~\cite{llama3modelcard}, and Qwen2 series, all in instruction-tuned versions. As the models show similar performance across various reference summaries and datasets, we use Llama2-70B-Chat to generate references and present the average results on BBC2024 in Table~\ref{tab:vs_llm_bertscore}. 


We highlight the top two metrics within each category. As can be seen, the summarization quality produced by the SLMs is on par with that of the LLMs. The difference in BertScore and factual consistency is minimal, with SLMs occasionally performing better. Notably, when BertScore scores are similar, smaller models tend to generate shorter summaries that facilitate quicker reading and comprehension of news outlines. 

\begin{table}[]
\centering
% \resizebox{\columnwidth}{!}{
\begin{adjustbox}{max width=1\columnwidth}
\begin{tabular}{lccc}
\hline
Model & BertScore & \begin{tabular}[c]{@{}c@{}}Factual \\ consistency\end{tabular} & \begin{tabular}[c]{@{}c@{}}Summary\\ length\end{tabular} \\ \hline
Qwen2-1.5B-Ins  & 71.97 & 92.4 & \textbf{41} \\
Phi3-Mini       & 73.72 & \textbf{97.6} & \textbf{51} \\
Llama3.2-3B-Ins & \textbf{74.95} & 95.6 & 70 \\
ChatGLM3        & 73.41 & 96.0 & 59 \\
Mistral-7B-Ins  & 74.32 & 93.0 & 60 \\
Qwen2-7B-Ins    & 74.71 & 95.6 & 59 \\
Llama3-70B-Ins  & \textbf{76.06} & 95.8 & 71 \\
Qwen2-72B-Ins   & 73.78 & \textbf{98.4} & 86 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{Average metric comparison of SLMs and LLMs on BBC2024 dataset.}}
\label{tab:vs_llm_bertscore}
\end{table}


\begin{table}[]
\centering
\begin{adjustbox}{max width=1\columnwidth}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\hline
Model & \multicolumn{1}{r}{Prompt 1} & \multicolumn{1}{r}{Prompt 2} & \multicolumn{1}{r}{Prompt 3} \\ \hline
Qwen2-0.5B-Ins  & 68.64 & 60.31 & 62.46 \\
Qwen2-1.5B-Ins  & 71.97 & 69.74 & 72.30 \\
Llama3.2-1B-Ins & 72.55 & 72.16 & 67.53 \\
Phi3-Mini       & 73.72 & 73.27 & 73.83 \\
Llama3.2-3B-Ins & 74.95 & 74.94 & 75.12 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{BertScore comparison with different prompts.} }
\label{tab:diff_prompt_bertscore}
\end{table}

% Although SLM trails behind LLM in both BertScore and stability, the difference is minimal. Llama3-70B-Ins achieves the highest average BertScore of 76.06, reflecting its superior summary quality, and also has the lowest standard deviation (5.12), indicating consistent performance. Although Phi3-Mini is an SLM, its scores and stability are comparable to LLMs. Its BertScore surpasses that of the ChatGLM3, and its standard deviation is lower than 7B LLMs like Qwen2-7B-Ins and Mistral-7B-Ins.


% Table~\ref{tab:vs_llm_length} shows summary length statistics. SLMs generally produce more concise summaries than LLMs, especially those around 70B parameters. Although SLMs occasionally generate longer summaries, their average length is typically shorter due to the use of simpler sentences and fewer descriptive words. In Figure~\ref{fig:bertscore_example}, while both Phi3-Mini and Llama2-70B-Chat summarize match results and rankings, Llama2-70B-Chat uses more descriptive language for additional details. In terms of summary length stability, there is little difference between SLMs and LLMs. Models like Gemma-1.1 and Llama3-70B-Ins show consistent summary lengths, while Qwen2-72B-Ins and Phi2 exhibit greater variability.


\subsection{Human Evaluation}


To further validate the effectiveness of our metrics, we performed a small-scale human evaluation, referencing existing studies~\cite{fabbri2021summeval}. Specifically, six highly educated annotators evaluated the relevance of news summaries generated by five SLMs based on the original news articles (scored on a 1-5 scale, with higher scores indicating greater relevance). We randomly selected 20 news instances from the BBC2024 dataset as our evaluation set. As shown in Table~\ref{tab:small_human_eva}, the average scores of each model demonstrate a strong alignment between BertScore and human evaluations, achieving a Kendall correlation coefficient of 1.

\begin{table*}[]
\begin{tabular}{lrrrrrrrr}
\hline
Model & \multicolumn{1}{l}{Ann. 1} & \multicolumn{1}{l}{Ann. 2} & \multicolumn{1}{l}{Ann. 3} & \multicolumn{1}{l}{Ann. 4} & \multicolumn{1}{l}{Ann. 5} & \multicolumn{1}{l}{Ann. 6} & \multicolumn{1}{l}{Average} & \multicolumn{1}{l}{BertScore} \\ \hline
Bloom-560M & 1.95 & 1.25 & 1.43 & 1.60 & 1.00 & 1.00 & 1.37 & 46.30 \\
Llama3.2-3B & 3.15 & 2.10 & 2.25 & 2.50 & 1.35 & 1.15 & 2.08 & 66.36 \\
Qwen2-0.5B & 4.05 & 2.15 & 3.28 & 2.70 & 3.60 & 3.25 & 3.17 & 69.85 \\
Phi3-Mini & 4.45 & 4.60 & 4.00 & 3.45 & 4.60 & 4.25 & 4.20 & 73.14 \\
Llama3.2-3B-Ins & 4.65 & 4.45 & 4.35 & 3.65 & 4.00 & 4.10 & 4.23 & 75.15 \\ \hline
\end{tabular}
\caption{Humance evaluation in relevance on 5 SLMs.}
\label{tab:small_human_eva}
\end{table*}



\subsection{Overall Evaluation and Model Selection}
SLMs exhibit considerable variability in text summarization performance, with larger and newer models generally showing stronger capabilities. Among them, Phi3-Mini and Llama3.2-3B-Ins perform the best, matching the 70B LLM in relevance, coherence, and factual consistency, while producing shorter summaries. All of these comparisons suggest that SLMs can effectively replace LLMs on edge devices for news summarization.

Although we evaluate 19 SLMs, there is considerable variation in their parameter sizes. To optimize deployment on edge devices, we provide model selection recommendations based on size. For models under 1B parameters, Brio and Qwen2-0.5B are the top choices overall. General-purpose language models in this range offer no clear advantage over those specialized in text summarization. For models between 1B and 2B parameters, Llama3.2-1B-Ins demonstrates a clear edge. For models above 2B parameters, Llama3.2-3B-Ins and Phi3-Mini outperform others across all criteria.

% \begin{table}[]
% \caption{\upshape{Overall SLM performance evaluation for different model sizes}}
% \label{tab:by_size}
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{ccccc}
% \hline
% Parameters & Model & BertScore($\uparrow$) & Faithfulness($\uparrow$) & Length($\downarrow$) \\ \hline
% \multirow{6}{*}{$< 1B$} & Brio                & \textbf{69.88} & 94.3          & \textbf{53.70} \\
%                   & LiteLama            & 39.19          & 64.1          & 186.88         \\
%                   & Bloom-560M          & 47.25          & 82.9          & 198.60         \\
%                   & Qwen2-0.5B          & \textbf{68.88} & \textbf{94.5} & \textbf{44.03} \\
%                   & Qwen2-0.5B-Ins      & 68.42          & 89.0          & 61.38          \\
%                   & Pegasus-Large       & 62.07          & \textbf{99.9} & 97.37          \\ \hline
% \multirow{6}{*}{$1B \sim 2B$} & TinyLlama           & 67.56          & \textbf{96.1} & \textbf{36.80} \\
%                   & GPT-Neo-1.3B        & 57.30          & 88.1          & 169.60         \\
%                   & Qwen2-1.5B          & \textbf{71.38} & \textbf{96.5} & 59.10          \\
%                   & Qwen2-1.5B-Ins      & \textbf{71.40} & 92.5          & \textbf{41.20} \\
%                   & InternLM2-1.8B      & 67.75          & 92.5          & 45.29          \\
%                   & InternLM2-1.8B-Chat & 67.91          & 89.5          & 43.58          \\ \hline
% \multirow{5}{*}{$> 2B$} & MiniCPM             & 65.35          & 85.5          & 105.90         \\
%                   & Gemma-1.1           & \textbf{71.22} & \textbf{95.7} & \textbf{42.78} \\
%                   & GPT-Neo-2.7B        & 58.25          & 80.5          & 121.55         \\
%                   & Phi-2               & 70.69          & 95.8          & 52.72          \\
%                   & Phi-3-Mini          & \textbf{74.01} & \textbf{96.7} & \textbf{51.88} \\\hline
% \end{tabular}
% \end{adjustbox}
% \end{table}



% \begin{table*}[]
% % \caption{System-level Kendall's tau correlation coefficients with different reference summary settings.}
% \begin{center}
% % \begin{adjustbox}{max width=2.05\columnwidth}
% \resizebox{2\columnwidth}{!}{
% \begin{tabular}{cccccccccc}
% \hline
% \multirow{2}{*}{Metric} &
%   \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Reference\\ type\end{tabular}} &
%   \multicolumn{4}{c}{Relevance evaluation} &
%   \multicolumn{4}{c}{Coherence evaluation} \\
%  &
%    &
%   \begin{tabular}[c]{@{}c@{}}Bench\\ -CNN/DM\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Bench\\ -XSum\end{tabular} &
%   SummEval &
%   Average &
%   \begin{tabular}[c]{@{}c@{}}Bench\\ -CNN/DM\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Bench\\ -XSum\end{tabular} &
%   SummEval &
%   Average \\ \hline
% \multirow{3}{*}{RougeL}   & Original    & 0.6732 & 0.2647 & 0.3714 & 0.4364          & 0.4248 & 0.6765 & 0.1238 & 0.4084          \\
%                            & Qwen1.5-72B & 0.7778 & 0.7059 & 0.4476 & \textbf{0.6438} & 0.5556 & 0.5000 & 0.2381 & \textbf{0.4312} \\
%                            & llama2-70B  & 0.7778 & 0.6324 & 0.4095 & 0.6066          & 0.5294 & 0.3676 & 0.2000 & 0.3657          \\ \hline
% \multirow{3}{*}{BertScore} & Original    & 0.6209 & 0.2206 & 0.4476 & 0.4297          & 0.4771 & 0.6912 & 0.4286 & 0.5323          \\
%                            & Qwen1.5-72B & 0.7516 & 0.6324 & 0.8095 & 0.7312          & 0.5817 & 0.6029 & 0.6000 & \textbf{0.5949} \\
%                            & llama2-70B  & 0.7516 & 0.6618 & 0.8286 & \textbf{0.7413} & 0.5294 & 0.6029 & 0.6190 & 0.5838          \\ \hline
% \multirow{3}{*}{BLEURT}    & Original    & 0.5425 & 0.2647 & 0.181  & 0.3294          & 0.3203 & 0.6176 & 0.2381 & 0.3929          \\
%                            & Qwen1.5-72B & 0.5817 & 0.7206 & 0.4476 & 0.5833          & 0.4902 & 0.4853 & 0.5429 & 0.5061          \\
%                            & llama2-70B  & 0.5556 & 0.7059 & 0.5429 & \textbf{0.6015} & 0.5425 & 0.4706 & 0.6762 & \textbf{0.5631} \\ \hline
% \end{tabular}}
% % \end{adjustbox}
% \end{center}
% \caption{{System-level Kendall's tau correlation coefficients between reference-based and human evaluations under different reference summary settings. "Original" is the original heuristic reference from the dataset; others are generated by LLMs. References generated by LLMs improve the effectiveness of reference-based evaluations.}}
% \label{tab:compare_kt_corr}
% \end{table*}

\section{Influencing Factor Analysis}
\label{sec:factor}
% In this section, we further select some well-performing SLMs to explore factors influencing the quality of text summarization, including prompt design and instruction tuning. 


\subsection{Prompt Design}
Prompt engineering has demonstrated significant power in many tasks, helping to improve the output quality of LLMs~\cite{zhao2021calibrate,surveypromptengineering}. Therefore, we use different prompt templates shown in Figure~\ref{fig:prompt} to analyze the impact of prompt engineering on BertScore scores, factual consistency, and summary length. The instructions become more detailed from Prompt 1 to Prompt 3.



\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/diff_prompt_example2.pdf}
    \caption{Examples summaries generated by Llama3.2-3B-Ins with different prompts. The bold parts show the same content, suggesting that the prompt design has a limited improvement in summary quality.}
    \label{fig:diff_prompt_example}
\end{figure}




Table~\ref{tab:diff_prompt_bertscore} compares the BertScore. The small score differences among prompts suggest that detailed descriptions do not significantly improve summary relevance and coherence and may even negatively affect some models. This suggests that \textbf{simple instructions are more suitable for SLMs, and overly complex prompts may degrade the performance}. For instance, the BertScore of Qwen2-0.5B-Ins drops significantly. The decline is likely due to prompt noise, where the model struggles to balance multiple instructions, resulting in less coherent summaries. Additionally, Figure~\ref{fig:diff_prompt_example} shows an example of Llama3.2-3B-Ins with different prompts. All summaries capture the key information, with only slight differences in details.



Table~\ref{tab:diff_prompt_fact} provides a comparison of factual consistency rates for various models using three different prompts. Many models, such as Qwen2-0.5B-Ins, exhibit a significant drop in factual consistency when using prompt 3. This further demonstrates that models with very small parameters are not well-suited for complex prompt design. The summary length variations across different prompts in Table~\ref{tab:diff_prompt_length} also indicate the limitations of prompt design. Some models fail to generate summaries of appropriate length given complex prompts.





\begin{table}[]
\centering
\begin{adjustbox}{max width=1\columnwidth}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\hline
Model & \multicolumn{1}{r}{Prompt 1} & \multicolumn{1}{r}{Prompt 2} & \multicolumn{1}{r}{Prompt 3} \\ \hline
Qwen2-0.5B-Ins  & 89.6 & 89.8  & 78.2 \\
Qwen2-1.5B-Ins  & 92.4 & 90.8  & 89.0 \\
Llama3.2-1B-Ins & 94.0 & 94.8  & 91.6 \\
Phi3-Mini       & 97.6 & 96.4  & 97.2 \\
Llama3.2-3B-Ins & 95.6 & 96.2  & 94.6 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{Factual consistency rate (\%)  comparison with different prompts.} }
\label{tab:diff_prompt_fact}
\end{table}


\begin{table}[]
\centering
\begin{adjustbox}{max width=1\columnwidth}
% \resizebox{\linewidth}{!}{
\begin{tabular}{lccc}
\hline
Model & \multicolumn{1}{r}{Prompt 1} & \multicolumn{1}{r}{Prompt 2} & \multicolumn{1}{r}{Prompt 3} \\ \hline
Qwen2-0.5B-Ins  & 65 & 150 & 151 \\
Qwen2-1.5B-Ins  & 41 & 99  & 53 \\
Llama3.2-1B-Ins & 63 & 60  & 58 \\
Phi3-Mini       & 51 & 57  & 65  \\
Llama3.2-3B-Ins & 70 & 67  & 61 \\ \hline
\end{tabular}
\end{adjustbox}
\caption{\upshape{Summary length with different prompts. All prompts require models to summarize in two sentences.} }
\label{tab:diff_prompt_length}
\end{table}



Overall, the improvement of prompt design on the quality of SLM summaries is limited. For SLMs that already perform well with simple prompts, they can capture the key points without the need for complex prompts. In contrast, SLMs that perform poorly may struggle with the logical relationships in complex prompts, leading to lengthy summaries. For practical applications, we recommend using simple and clear prompts when deploying SLMs for news summarization.










% Models that already perform well do not benefit from prompt adjustments, while models with insufficient capabilities are not improved by prompt design and may even produce lower-quality summaries.






\subsection{Instruction Tuning}
Instruction tuning plays a crucial role in training LLMs by enhancing their ability to follow specific instructions. And some existing studies claim that instruction-tuned language models have stronger summarization capabilities~\cite{goyal2022news,zhang2024benchmarking}. However, in our extensive evaluations, with the exception of the Llama3.2 series, models perform very similarly before and after instruction tuning. In fact, models without instruction tuning often exhibit higher factual consistency. In the sample analysis, we find that the Llama3.2 models without instruction tuning include too many details, while instruction tuning helps it produce more general summaries. However, this trend is not observed in other models like Qwen2 and InternLM2 series, which is different from the conclusions of previous work~\cite{zhang2024benchmarking}. We leave the deep study of instruction tuning on summarization ability as a future research direction.


% \section{LLM-Generated Reference Verification}
% \label{sec:Verification}
% In this section, we verify whether the LLM-generated reference summaries can improve the effectiveness of reference-based evaluation on three human evaluation datasets~\cite{zhang2024benchmarking,fabbri2021summeval}. These datasets include summaries generated by various models as well as human ratings. We compare the consistency with human evaluations using different reference summaries and show results in Table~\ref{tab:compare_kt_corr}. As can be seen, LLM-generated summaries significantly improve consistency than original references. BertScore shows the best performance, which validates the effectiveness of using LLM-generated reference summaries in our evaluations.



% method on three human evaluation datasets~\cite{zhang2024benchmarking,fabbri2021summeval} and show results in Table~\ref{tab:compare_kt_corr}. We use Prompt 2 in Figure~\ref{fig:prompt} as the LLM input. The output is post-processed to remove introductory phrases and retain only complete sentences for the final summaries. For the LLM model, we select Qwen1.5-72B-Chat~\cite{qwen1} and Llama2-70B Chat~\cite{touvron2023llama2openfoundation}. As can be seen, LLM-generated summaries significantly improve consistency with human evaluation, particularly for relevance.  According to previous work~\cite{kendeer}, a Kendall coefficient above 0.3 indicates a strong correlation, making BertScore a suitable metric for the following SLM summarization benchmark.






% T 检验？
% instruction tuning更容易停止，更短
% 指令调优效果在不同模型上不同，以及可能有负面作用，这个是需要进一步研究的东西
%prompt design 对表现不好的模型有帮助么？


% Additionally, the impact of instruction tuning on summary length did not display a consistent trend. It is worth noting that after instruction tuning, the summaries generated by Qwen2-0.5B-Ins increased significantly in length.

% We also explore whether instruction tuning can enhance the model ability to control summary length. We ask the models to generate summaries of a specified length. Specifically, based on the prompt 1 template, we specify the word count or the sentence count for the generated summaries. 
% Tables~\ref{tab:summary_len_words} and~\ref{tab:summary_len_sentence} show the results, and model names with the suffix "Ins" or "Chat" are instruction-tuned versions. we find that instruction tuning does not significantly help in controlling summary length. Except for Phi3-Mini, the instruction-tuned models perform similarly to the original versions in limiting summary words. Under different summary length constraints, only Phi3-Mini meets the requirements well. When considering the word limit, although the summary length increases with the word limit increasing, most models output a length that exceeds the word limit. Only Phi3-Mini generates summaries that match it closely. When sentence count is limited, Phi3-Mini produces summaries with each sentence containing 20-30 words, effectively adhering to the specified number of sentences. Phi2 and the Qwen2 series perform well only when the number of sentences is specified. Regardless of whether they have undergone instruction tuning, they are unable to effectively limit the number of words. 

% Although instruction tuning has limited effects on controlling summary length, it helps models better terminate their output. During testing, we find that the instruction-tuned versions more accurately adhere to the specified token (such as <eos>) limit, effectively stopping output. In contrast, for the non-instruction-tuned versions, we had to detect the newline character to halt the output, which sometimes resulted in truncating the generated content.
% how to stop

% \begin{table}[]
% \caption{\upshape{Summary length under different prompt word limits.}}
% \label{tab:summary_len_words}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lcccc}
% \hline
% Model          & 50 words & 60 words & 70 words & 80 words \\ \hline
% Qwen2-1.5B-Ins & 75.01    & 71.99    & 79.19    & 84.40    \\
% Qwen2-1.5B     & 84.01    & 86.84    & 98.64    & 105.57   \\
% InternLM2-1.8B-Chat & 91.59 & 93.62  & 98.76  & 106.14  \\
% InternLM2-1.8B & 77.47 & 101.86 & 110.29  & 116.47 \\
% Phi2           & 115.68   & 137.78   & 148.54   & 157.19  \\
% Phi3-Mini      & 49.78    & 56.31    & 67.78    & 76.32    \\
% Qwen2-7B-Ins   & 78.57    & 86.00    & 91.00    & 99.07    \\
% Qwen2-7B       & 71.78    & 86.81    & 87.32    & 91.54    \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}

% \begin{table}[]
% \caption{\upshape{Summary length under different prompt sentence limits.}}
% \label{tab:summary_len_sentence}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lcccc}
% \hline
% Model &
%   \begin{tabular}[c]{@{}c@{}}One\\ sentence\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Two\\ sentences\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Three\\ sentences\end{tabular} &
%   \begin{tabular}[c]{@{}c@{}}Four\\ sentences\end{tabular} \\ \hline
% Qwen2-1.5B-Ins & 31.61 & 40.90 & 61.80 & 75.17 \\
% Qwen2-1.5B     & 38.08 & 58.89 & 78.95 & 93.78 \\
% InternLM2-1.8B-Chat & 44.48 & 57.69 & 75.75 & 95.10 \\
% InternLM2-1.8B & 23.45 & 41.17 & 66.01 & 78.90 \\
% Phi2           & 35.50 & 60.00 & 69.96 & 97.09 \\
% Phi3-Mini      & 35.25 & 50.69 & 78.39 & 97.49 \\
% Qwen2-7B-Ins   & 44.92 & 59.80 & 80.32 & 96.35 \\
% Qwen2-7B       & 42.38 & 59.44 & 74.21 & 86.58 \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}







% \subsection{Prompt Design}

% \begin{table}[]
% \caption{\upshape{BertScore comparison with different prompts.} }
% \label{tab:diff_prompt_bertscore}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{lrrr}
% \hline
% Model          & Prompt 1 & Prompt 2 & Prompt 3 \\ \hline
% Qwen2-0.5B-Ins & 68.64   & 68.09   & 68.30   \\
% Qwen2-0.5B     & 69.67   & 69.58   & 69.67   \\
% Qwen2-1.5B-Ins & 71.78   & 72.02   & 71.70   \\
% Qwen2-1.5B     & 71.99   & 72.12   & 72.22   \\
% Phi2           & 71.50   & 70.97   & 71.87   \\
% Phi3-Mini      & 73.72   & 74.13   & 74.49   \\
% Qwen2-7B-Ins   & 74.71   & 73.40   & 73.88   \\
% Qwen2-7B       & 75.27   & 75.37   & 74.99   \\ \hline
% \end{tabular}
% \end{adjustbox}

% \end{table}




% \begin{table}[]
% \caption{\upshape{BertScore and summary length comparison with different prompts}}
% \label{tab:diff_detail_prompt}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lrrrrrr}
% \hline
% Model          & \multicolumn{3}{c}{BertScore}  & \multicolumn{3}{c}{Summary Length} \\
%                & Prompt 1 & Prompt 2 & Prompt 3 & Prompt 1   & Prompt 2  & Prompt 3  \\ \hline
% Qwen2-0.5B-Ins & 61.47    & 68.09    & 68.30    & 140.75     & 51.52     & 64.79     \\
% Qwen2-0.5B     & 69.67    & 69.58    & 69.67    & 40.84      & 30.93     & 37.90     \\
% Qwen2-1.5B-Ins & 71.78    & 72.02    & 71.70    & 53.36      & 42.68     & 41.38     \\
% Qwen2-1.5B     & 72.18    & 72.12    & 72.22    & 58.89      & 48.64     & 56.98     \\
% Phi2           & 71.50    & 70.97    & 71.87    & 60.00      & 100.88    & 72.554    \\
% Phi3-Mini      & 73.72    & 74.13    & 74.49    & 50.69      & 58.27     & 58.66     \\
% Qwen2-7B-Ins   & 74.71    & 73.40    & 73.88    & 58.57      & 64.19     & 63.74     \\
% Qwen2-7B       & 75.27    & 75.37    & 74.99    & 56.41      & 50.78     & 51.64     \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}


% Prompt engineering has demonstrated significant power in many tasks, helping to improve the output quality of LLMs~\cite{zhao2021calibrate,surveypromptengineering}. Therefore, we use different prompt templates shown in Fig~\ref{fig:prompt} to analyze the impact of prompt engineering on BertScore scores, factual consistency, and summary length. From Prompt 1 to Prompt 3, the instructions become increasingly detailed. 
% % Tables~\ref{tab:diff_detail_prompt} present the results.

% Table~\ref{tab:diff_prompt_bertscore} compares the BertScore for various models using three different prompts. The changes across different prompts are minimal, indicating that more detailed prompt words cannot significantly improve the relevance and coherence of the summary. This consistency is beneficial for practical applications where prompts may vary, ensuring stable and reliable text generation performance. Fig~\ref{fig:diff_prompt_example} shows some examples of Qwen-0.5B-Ins with different prompts. All summaries can capture the key information, and prompts 2 and 3 result in only minor changes in connective words. 

% Table~\ref{tab:diff_prompt_fact} provides a comparison of factual consistency rates for various models using three different prompts. Although most models show slight variations in factual consistency rates, they show consistent changes under some prompts. On most language models, using prompt 2 improves fact consistency by about 2\% compared with prompt 1. The BBC2024 test dataset includes 500 instances. A 2\% improvement means improving the consistency of facts on 10 summaries.

% However, prompt design has a significant impact on summary length, especially for SLMs. Although they are all asked to summarize the article in two sentences, some SLMs exhibit significant fluctuations, as shown in Table~\ref{tab:diff_prompt_len}. Only Phi3-Mini can maintain the stable summary length. Prompts affect different models in varying ways. For the Qwen2 series, although prompt 2 is more detailed than prompt 1, it generates shorter summaries. However, for the Phi series, summaries generated using prompt 2 are longer than those generated using prompt 1. Fig~\ref{fig:diff_prompt_example} shows examples of summaries generated by Qwen2-0.5B-Ins under different prompts. While all summaries capture the key points, the summary generated with prompt 1 is significantly longer due to more details.



% \begin{table}[]
% \caption{\upshape{Factual consistency rate (\%) with different prompts.} }
% \label{tab:diff_prompt_fact}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% % \resizebox{\linewidth}{!}{
% \begin{tabular}{lrrr}
% \hline
% Model          & Prompt 1 & Prompt 2 & Prompt 3 \\ \hline
% Qwen2-0.5B-Ins & 89.6    & 88.8   & 88.2   \\
% Qwen2-0.5B     & 95.2    & 97.4   & 96.0   \\
% Qwen2-1.5B-Ins & 92.4    & 94.0   & 91.8   \\
% Qwen2-1.5B     & 96.6    & 97.6   & 97.6   \\
% Phi2           & 96.8    & 99.0   & 97.2   \\
% Phi3-Mini      & 97.6    & 98.2   & 98.6   \\
% Qwen2-7B-Ins   & 95.6    & 96.0   & 95.0   \\
% Qwen2-7B       & 96.8    & 97.4   & 96.0    \\ \hline
% \end{tabular}
% \end{adjustbox}

% \end{table}


% \begin{table}[]
% \caption{\upshape{Summary length comparison with different prompts.}}
% \label{tab:diff_prompt_len}
% \centering
% \begin{adjustbox}{max width=1\columnwidth}
% \begin{tabular}{lrrr}
% \hline
% Model          & Prompt 1 & Prompt 2 & Prompt 3 \\ \hline
% Qwen2-0.5B-Ins & 64.80  & 51.52   & 64.79   \\
% Qwen2-0.5B     & 40.84   & 30.93   & 37.90   \\
% Qwen2-1.5B-Ins & 53.36   & 42.68   & 41.38   \\
% Qwen2-1.5B     & 58.89   & 48.64   & 56.98   \\
% Phi2           & 60.00   & 100.88  & 72.55    \\
% Phi3-Mini      & 53.87   & 58.27   & 58.66   \\
% Qwen2-7B-Ins   & 58.57   & 64.19   & 63.74   \\
% Qwen2-7B       & 56.41   & 50.78   & 51.64   \\ \hline
% \end{tabular}
% \end{adjustbox}
% \end{table}




% \subsection{Model Size and Training Data}
% To ensure that model size is the only variable, we can focus on models from the same series, such as the Qwen2 series and GPT-Neo series. As can be seen in Table~\ref{tab:bertscore_res} and~\ref{tab:diff_prompt_bertscore}, it is evident that increasing the model size improves the performance on BertScore, but the effect diminishes with larger models, which is consistent with existing studies~\cite{zhang2024benchmarking}. For instance, Qwen2-7B-Ins scores on the BBC2024 dataset even surpass those of Qwen2-72B-Ins. In terms of factual consistency, having more parameters does not necessarily lead to improvement. For example, as shown in Table~\ref{tab:fact}, the summaries generated by GPT-Neo-1.3B are more aligned with the original articles than those generated by GPT-Neo-2.7B by around 8\%.

% Besides model size, high-quality training data is also crucial. When the model sizes are similar, there is also a large gap between different model series. For example, Qwen2-0.5B and Bloom-560M have a BertScore difference of nearly 30 points. Some models with
% fewer parameters even score higher. Although Qwen2-1.5B has fewer parameters than Phi2 and Gemma-1.1, it scores higher average BertScore. And Phi3-Mini even outperforms some 70B models on factual consistency. We attribute this to the differences in training data and training methods. Thus, both model size and the quality of training data are crucial for the text summarization capability.

% \subsection{Findings}
% Among all the factors influencing the summarization capabilities of SLMs, we find that model size and the training data quality are crucial for ensuring the relevance, coherence, and factual consistency of summaries. Prompt design and instruction tuning have a limited impact on most SLMs, which maintain consistent summary quality across different prompts. Additionally, instruction tuning only marginally improves the control of output text length.

\section{Conclusion}
\label{sec:conclusion}
This paper presents the comprehensive evaluation of SLMs for news summarization, comparing 19 models across diverse datasets. Our results demonstrate that top-performing models like Phi3-Mini and Llama3.2-3B-Ins can match the performance of larger 70B LLMs while producing shorter, more concise summaries. These findings highlight the potential of SLMs for real-world applications, particularly in resource-constrained environments. Further exploration shows that simple prompts are more effective for SLM summarization, while instruction tuning provides inconsistent benefits, necessitating further research.




% we comprehensively evaluate 17 SLMs on their text summarization performance. In terms of evaluation metrics, to improve the evaluation results more aligned with human preferences, we use high-quality reference summaries generated by LLMs to replace the original reference summaries in reference-based evaluation methods. In the evaluation experiments, we find that there is significant variability among SLMs in text summarization, with Phi3-Mini exhibiting the best performance among them. Compared with LLMs, while state-of-the-art SLMs still lag behind LLMs in summary relevance and coherence, the difference is minimal, with LLMs typically including more detailed content. There is no significant difference in factual consistency. Furthermore, we find that model size and training data remain the most critical factors influencing summary quality, with prompt design and instruction tuning having limited impact.


\section*{Limitations}

Although using LLM-generated summaries improved the reliability of reference-based summarization evaluation methods, it may introduce bias. For instance, if the LLM fails to accurately summarize the news, an SLM that produces similar content might receive an undeservedly high score. To mitigate this potential bias, we use summaries generated by multiple LLMs as references and calculate the average scores. Furthermore, although BertScore demonstrates a high level of consistency with human evaluations in coherence, it is not specifically designed to evaluate coherence. In future work, we plan to explore the use of more accurate metrics for coherence evaluation. Due to the input length limits of most SLMs, we only considered news articles with a maximum of 1,500 tokens. In the future, we will explore how to effectively use SLMs to summarize much longer news articles. Additionally, we do not consider quantized models in this study. In future work, we will also incorporate an analysis of the performance of quantized models into our evaluation.
% \subsection{Text Order}