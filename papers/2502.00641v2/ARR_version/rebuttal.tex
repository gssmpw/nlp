\documentclass[a4paper]{article}
\usepackage{geometry}
  \geometry{scale=.8}% use 80% of the page - adjust as desired
\usepackage[compact]{titlesec}
\usepackage{kantlipsum}

% 包导入
\usepackage{amsmath}  % 数学公式
\usepackage{graphicx} % 插入图片
\usepackage{hyperref} % 链接
\usepackage{markdown} % 导入markdown包
\usepackage{xcolor}
\usepackage{float}
% 文档开始
\begin{document}

% 标题
\title{Rebuttal}
% \author{Your Name}
% \date{\today}

% 生成标题
\maketitle

\vspace{5mm}
\hrule height 0.5mm width \textwidth
\vspace{5mm}
\section{Reviewer 1 (score: 4)}
\subsection{\textcolor{red}{W1: BertScore should not be a metric to measure Coherence.}}
% 间接反应coherence
% We acknowledge that BERTScore is primarily designed to evaluate the relevance between generated and reference texts. However, given the absence of widely adopted metrics specifically tailored for coherence evaluation, we employed it as an indirect measure of coherence. This choice is supported by two key reasons: First, as noted in your referenced paper, relevance and coherence often exhibit a strong correlation, making relevance metrics a practical proxy for coherence. Second, as shown in Table 1 of our paper, BERTScore achieved the highest correlation with human coherence ratings among the evaluated metrics, further supporting its suitability as an indirect indicator in our study.


We appreciate your comment about the appropriateness of BERTScore for coherence evaluation. While we acknowledge that BERTScore is primarily designed to measure relevance, our analysis shows a strong correlation between BERTScore and human coherence evaluation, which is presented in \textbf{Table 1 in our paper}. This suggests that BERTScore can serve as a practical proxy for coherence in the absence of widely adopted metrics specifically designed for coherence evaluation. In future work, we plan to explore metrics tailored for coherence evaluation.



\subsection{\textcolor{red}{W2: Lack of human evaluation}}

% \textcolor{blue}{In this paper, we focus on automatic metrics for three primary reasons. First, BERTScore, particularly in relevance evaluation, strongly correlates with expert judgments (Table 1 and Figure 7), making it a reliable and efficient alternative for our study. Second, automatic metrics like BERTScore facilitate easy benchmarking and comparison of new models, ensuring scalability in evaluation. Lastly, as shown in Figure 7 in our paper, non-expert annotators exhibit low correlation in their evaluation, while high-quality manual evaluations require expert annotators, which are costly and impractical for continuous comparison and integration of new models. }

{We chose to rely on automatic metrics like BERTScore due to their strong correlation with expert evaluations and their scalability for benchmarking a large number of samples (2,000 samples). Table 1 and Figure 7 in our paper demonstrate that automatic metrics, such as BERTScore, exhibit much higher consistency with expert annotators compared to general crowd-sourced annotators. }

{And in the following Table 1, we refer to work [1] for a small-scale human evaluation of the relevance aspect of five SLMs on 20 randomly selected BBC2024 news articles. The results show a strong alignment between BERTScore and human evaluations, with \textbf{a Kendall correlation coefficient of 0.99}.}


\begin{table}[h]
\centering
\caption{Humance evaluation in relevance on 5 SLMs.}
\label{tab:my-table}
\begin{tabular}{lrrrrrrrr}
\hline
Model           & Ann. 1 & Ann. 2 & Ann. 3 & Ann. 4 & Ann. 5 & Ann. 6 & Average & BERTScore \\ \hline
Bloom-560M      & 1.95   & 1.25   & 1.43   & 1.60   & 1.00   & 1.00   & 1.37    & 46.30     \\
Llama3.2-3B     & 3.15   & 2.10   & 2.25   & 2.50   & 1.35   & 1.15   & 2.08    & 66.36     \\
Qwen2-0.5B      & 4.05   & 2.15   & 3.28   & 2.70   & 3.60   & 3.25   & 3.17    & 69.85     \\
Phi3-Mini       & 4.45   & 4.60   & 4.00   & 3.45   & 4.60   & 4.25   & 4.20    & 73.14     \\
Llama3.2-3B-Ins & 4.65   & 4.45   & 4.35   & 3.65   & 4.00   & 4.10   & 4.23    & 75.15     \\ \hline
\end{tabular}
\end{table}

[1] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.



\subsection{\textcolor{red}{Q1: Why did the authors generate new summaries and didn't use the human summaries they already generated?}}

% As existing human-generated reference summary datasets contain only a limited number of samples (e.g., the Summeval dataset includes just 100 news articles), they are insufficient for evaluating SLMs on a larger scale. To address this limitation and ensure robust benchmarking, we utilized LLMs to generate 2,000 high-quality reference summaries, providing a more comprehensive dataset for evaluating SLM performance.


Recent work [1] suggests that 500 samples per dataset are sufficient to differentiate models, but existing human-generated datasets, such as SummEval [2] which contains only 109 articles, are too small for evaluating 19 SLMs. To address this, we generated 2,000 high-quality LLM summaries for consistent, large-scale benchmarking, supported by findings [2] and our results (\textbf{Table 1 in our paper}) showing LLM summaries align closely with human preferences and are well-suited for evaluation.

[1]Maynez, Joshua, et al. "Benchmarking Large Language Model Capabilities for Conditional Generation." Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (2023): 9194–9213.

[2] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.

\subsection{\textcolor{red}{Q2: Are human evaluation datasets (line 243) created without using the reference summaries?}}

Yes, human evaluation datasets are created without reference summaries. All human evaluations are based solely on the source articles and the summaries generated by their benchmarked models, ensuring an unbiased assessment from the reference summaries.

\vspace{5mm}
\hrule height 0.5mm width \textwidth
\vspace{5mm}


\section{Reviewer 2 (score: 2.5)}
\subsection{\textcolor{red}{W1: The authors should avoid using length to compare compression rates directly, as BERTScore tends to favor shorter summaries even when quality is comparable, potentially biasing the evaluation. The authors shouldn't use the length to compare the compression rate directly since the samples are not of the same quality.}}

% We agree that BERTScore can exhibit a slight bias toward shorter summaries. But we also think it is reasonable to evaluate summary quality, as concise summaries facilitate quicker reading and comprehension for users. And this bias is not obvious. For instance, as demonstrated in our paper, TinyLlama generates the shortest summaries yet does not rank among the best-performing SLMs. Additionally, since BERTScore does not directly reflect summary length, summary length can help understand the compression ratio of text. 

% We acknowledge that BERTScore may exhibit a slight bias toward shorter summaries. To investigate this, in Table 1, we experiment on the BBC2024 dataset to examine the effect of summary length. As outlined in your referenced paper, we control the summary quality by randomizing article-summary pairings. A noticeable bias in BERTScore only becomes evident when the difference in summary length exceeds 100 words. Consequently, when differences in summary length are relatively small, their impact on BERTScore is minimal. For example, for SLMs with a BERTScore exceeding 70, the variations in summary length are minor. Thus, using summary length to evaluate the compression ability of these summaries is reasonable.
We acknowledge that BERTScore may exhibit a slight bias toward shorter summaries. To investigate this, in Table 1, we experiment on the BBC2024 dataset to examine the effect of summary length. As outlined in your referenced paper, we control the summary quality by randomizing article-summary pairings. A noticeable bias in BERTScore only becomes evident when the difference in summary length exceeds 100 words. And BERTScore does not exhibit a clear trend when summary lengths range between 40 and 70 words. For example, when SLMs achieve a BERTScore above 70, their lengths typically range from 40 to 70, these inspire us to use length as a metric for evaluating compression ability within a narrow score range.

\begin{table}[h]
\centering 
\caption{Effect of summary length on BERTScore at the same summary quality.}
\begin{tabular}{lcc}
\hline
Model           & BERTScore & Length \\ \hline
Qwen2-0.5B      & 52.89     & 41  \\
TinyLlama       & 52.47     & 46  \\
Phi3-Mini       & 53.71     & 51  \\
Llama3.2-1B-Ins & 53.73     & 63     \\
Llama3.2-3B-Ins & 53.81     & 70     \\
Llama3.2-3B     & 50.46     & 171    \\
Llama3.2-1B     & 48.36     & 184    \\ \hline
\end{tabular}
\end{table}



\subsection{\textcolor{red}{W2: The authors do not control for mean summary length across SLMs, making it unclear whether lower BERTScores stem from length bias or poor quality.}}

Considering practical scenarios, we only limit the length to two sentences in the prompt without controlling the exact length in words, as we treat summary length as an integral aspect of summary quality. Excessively lengthy summaries often indicate the model struggles to distill key information, justifying lower BERTScores. As discussed in Section 5.1 of our paper, low BERTScores for such summaries are typically due to issues like empty responses, copying paragraphs, or repetition.





% 空输出
\subsection{\textcolor{red}{W3: No human evaluation and other automatic metrics might be useful}}

{We chose to use automatic metrics like BERTScore due to their strong correlation with expert evaluations and their scalability for benchmarking a large dataset of 2,000 samples. As shown in Table 1 and Figure 7 of our paper, automatic metrics such as BERTScore demonstrate significantly higher consistency with expert annotators compared to crowd-sourced evaluators.}

{In the following Table 2, we refer to work [1] for a small-scale human evaluation of the relevance aspect of five SLMs on 20 randomly selected BBC2024 news articles. The results show a strong alignment between BERTScore and human evaluations, with \textbf{a Kendall correlation coefficient of 0.99}.}

\begin{table}[h]
\centering
\caption{Humance evaluation in relevance on 5 SLMs.}
\label{tab:my-table}
\begin{tabular}{lrrrrrrrr}
\hline
Model           & Ann. 1 & Ann. 2 & Ann. 3 & Ann. 4 & Ann. 5 & Ann. 6 & Average & BERTScore \\ \hline
Bloom-560M      & 1.95   & 1.25   & 1.43   & 1.60   & 1.00   & 1.00   & 1.37    & 46.30     \\
Llama3.2-3B     & 3.15   & 2.10   & 2.25   & 2.50   & 1.35   & 1.15   & 2.08    & 66.36     \\
Qwen2-0.5B      & 4.05   & 2.15   & 3.28   & 2.70   & 3.60   & 3.25   & 3.17    & 69.85     \\
Phi3-Mini       & 4.45   & 4.60   & 4.00   & 3.45   & 4.60   & 4.25   & 4.20    & 73.14     \\
Llama3.2-3B-Ins & 4.65   & 4.45   & 4.35   & 3.65   & 4.00   & 4.10   & 4.23    & 75.15     \\ \hline
\end{tabular}
\end{table}
[1] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.


{In addition to BERTScore, we report RougeL results in Table 3 below, which show a strong correlation (Kendall’s tau = 0.8667) with BERTScore rankings. so in the paper, we only report BERTScore as the main ranking reference.}



\begin{table}[h]
\centering 
\caption{RougeL on the BBC2024 dataset. Qwen1.5-72B and Llama2-70B are used to generate reference summaries.}
\begin{tabular}{lccc}
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Qwen1.5} & \multicolumn{1}{l}{Llama2} & \multicolumn{1}{l}{Average} \\ \hline
LiteLlama            & 9.39                        & 10.48                      & 9.94                        \\
Bloom-560M           & 13.78                       & 15.02                      & 14.40                       \\
GPTNeo-1.3B          & 19.75                       & 20.82                      & 20.28                       \\
GPT-Neo-2.7B         & 18.91                       & 20.04                      & 19.47                       \\
Llama3.2-1B          & 21.85                       & 23.47                      & 22.66                       \\
Pegasus-Large        & 23.45                       & 25.11                      & 24.28                       \\
Llama3.2-3B          & 24.73                       & 26.42                      & 25.57                       \\
MiniCPM              & 26.25                       & 28.05                      & 27.15                       \\
TinyLlama            & 25.83                       & 10.48                      & 18.15                       \\
InternLM2-1.8B       & 26.05                       & 28.17                      & 27.11                       \\
InternLM2-1.8B-Chat  & 28.44                       & 32.22                      & 30.33                       \\
Qwen2-0.5B-Ins       & 27.31                       & 30.75                      & 29.03                       \\
Qwen2-0.5B           & 27.73                       & 31.61                      & 29.67                       \\
Brio                 & 30.90                       & 31.54                      & 31.22                       \\
Phi-2                & 31.35                       & 35.15                      & 33.25                       \\
Gemma-1.1            & 31.77                       & 34.52                      & 33.14                       \\
Qwen2-1.5B           & 33.01                       & 37.13                      & 35.07                       \\
Qwen2-1.5B-Ins       & 31.63                       & 34.28                      & 32.95                       \\
Llama3.2-1B-Ins      & 32.41                       & 35.99                      & 34.20                       \\
Phi-3-Mini           & 36.25                       & 36.25                      & 36.25                       \\
Llama3.2-3B-Ins      & 36.56                       & 40.08                      & 38.32    \\          
\hline
\end{tabular}
\end{table}



\subsection{\textcolor{red}{W4: If they contain the dataset used for summarization evaluation?}}

Table 4 provides a list of the models used in our paper, along with their corresponding training datasets. For datasets that are not open source, we indicate them as "unknown." Apart from two traditional summarization models, pegasus-large and brio, which are trained on text summarization datasets, the other models are either not trained on specific summarization datasets or their training details are not specified.



\begin{table}[h]
\centering
\caption{Pre-training dataset statistics}
\label{tab:my-table}
\begin{tabular}{lcc}
\hline
Model               & Pre-training dataset                            & Summarization dataset \\ \hline
LiteLlama           & RedPajama                                       & No                    \\
Bloom-560M          & ROOTS, WuDaoCorpora                             & No                    \\
GPTNeo-1.3B         & Pile                                            & No                    \\
GPT-Neo-2.7B        & Pile                                            & No                    \\
Llama3.2-1B         & unknown (9T tokens)                             & unknown               \\
Pegasus-Large       & C4 and HugeNews                                 & Yes                   \\
Llama3.2-3B         & unknown (9T tokens)                             & unknown               \\
MiniCPM             & ShareGPT and unknown datasets                   & unknown               \\
TinyLlama           & SlimPajama, StarCoder, UltraChat, UltraFeedback & No                    \\
InternLM2-1.8B      & unknown                                         & unknown               \\
InternLM2-1.8B-Chat & unknown                                         & unknown               \\
Qwen2-0.5B-Ins      & unknown (7T tokens)                             & unknown               \\
Qwen2-0.5B          & unknown (7T tokens)                             & unknown               \\
Brio                & CNN/DM                                          & Yes                   \\
Phi-2               & unknown (1.4T tokens)                           & unknown               \\
Gemma-1.1           & unknown (6T tokens)                             & unknown               \\
Qwen2-1.5B          & unknown (7T tokens)                             & unknown               \\
Qwen2-1.5B-Ins      & unknown (7T tokens)                             & unknown               \\
Llama3.2-1B-Ins     & unknown (9T tokens)                             & unknown               \\
Phi-3-Mini          & unknown (3.3T tokens)                           & unknown               \\
Llama3.2-3B-Ins     & unknown (9T tokens)                             & unknown               \\ \hline
\end{tabular}
\end{table}

\hrule height 0.5mm width \textwidth


\section{Reviewer 3 (score: 3)}

\subsection{\textcolor{red}{ limited novelty}}
% Our first novelty lies in using LLMs to generate reference summaries, improving the alignment between automated metrics and human evaluations. As shown in Table 1 and Figure 7 of the paper, LLM-generated summaries greatly enhance the correlation between metrics like BERTScore and expert judgments, surpassing the correlation with crowdsourced annotators.

Our novelty lies in evaluating the characteristics of SLMs for summarization tasks, which is a benchmark reference for people who want to use SLMs in their work. As for the technical novelty, we design an automatic evaluation solution that uses LLMs for reference generation that aligns the preference with human evaluations.

\subsection{\textcolor{red}{more evaluation metrics could be reported}}

We provide the RougeL scores on the BBC2024 dataset in the following Table 1. The results of RougeL and BERTScore show high similarity, with a Kendall correlation coefficient of 0.8667, so in the paper, we only report BERTScore as the main ranking reference.
% We will add in the results to the revised version.



\begin{table}[h]
\centering 
\caption{RougeL on the BBC2024 dataset. Qwen1.5-72B and Llama2-70B are used to generate reference summaries.}
\begin{tabular}{lccc}
\hline
\multicolumn{1}{l}{} & \multicolumn{1}{l}{Qwen1.5} & \multicolumn{1}{l}{Llama2} & \multicolumn{1}{l}{Average} \\ \hline
LiteLlama            & 9.39                        & 10.48                      & 9.94                        \\
Bloom-560M           & 13.78                       & 15.02                      & 14.40                       \\
GPTNeo-1.3B          & 19.75                       & 20.82                      & 20.28                       \\
GPT-Neo-2.7B         & 18.91                       & 20.04                      & 19.47                       \\
Llama3.2-1B          & 21.85                       & 23.47                      & 22.66                       \\
Pegasus-Large        & 23.45                       & 25.11                      & 24.28                       \\
Llama3.2-3B          & 24.73                       & 26.42                      & 25.57                       \\
MiniCPM              & 26.25                       & 28.05                      & 27.15                       \\
TinyLlama            & 25.83                       & 10.48                      & 18.15                       \\
InternLM2-1.8B       & 26.05                       & 28.17                      & 27.11                       \\
InternLM2-1.8B-Chat  & 28.44                       & 32.22                      & 30.33                       \\
Qwen2-0.5B-Ins       & 27.31                       & 30.75                      & 29.03                       \\
Qwen2-0.5B           & 27.73                       & 31.61                      & 29.67                       \\
Brio                 & 30.90                       & 31.54                      & 31.22                       \\
Phi-2                & 31.35                       & 35.15                      & 33.25                       \\
Gemma-1.1            & 31.77                       & 34.52                      & 33.14                       \\
Qwen2-1.5B           & 33.01                       & 37.13                      & 35.07                       \\
Qwen2-1.5B-Ins       & 31.63                       & 34.28                      & 32.95                       \\
Llama3.2-1B-Ins      & 32.41                       & 35.99                      & 34.20                       \\
Phi-3-Mini           & 36.25                       & 36.25                      & 36.25                       \\
Llama3.2-3B-Ins      & 36.56                       & 40.08                      & 38.32    \\          
\hline
\end{tabular}
\end{table}




\subsection{\textcolor{red}{Missing elaboration of datasets involved in Table 1, such as the dataset sizes.}}
Table 2 provides details of the human evaluation datasets used in Table 1 of the paper. These datasets involve human annotators evaluating summaries generated by various models on a scale of 1 to 5, with higher scores indicating better performance across different aspects.


\begin{table}[h]
\centering
\caption{Human evaluation dataset details.}
\label{tab:my-table}
\begin{tabular}{lrrrr}
\hline
Dataset       & {Article size} & Evaluation method   & Evaluation model Size & Source \\ \hline
Bench- CNN/DM & {100}          & 1 to 5 Likert scale & 18                    & 1      \\
Bench- XSum   & {100}          & 1 to 5 Likert scale & 18                    & 1      \\
SummEval      & {100}          & 1 to 5 Likert scale & 16                    & 2      \\ \hline
\end{tabular}
\end{table}

[1] Zhang, Tianyi, et al. "Benchmarking large language models for news summarization." Transactions of the Association for Computational Linguistics 12 (2024): 39-57.

[2] Fabbri, Alexander R., et al. "Summeval: Re-evaluating summarization evaluation." Transactions of the Association for Computational Linguistics 9 (2021): 391-409.



\subsection{\textcolor{red}{Will the LLM-generated reference summaries be public?}}
We will release the detailed results and LLM-generated reference summaries. Per the response policy, we cannot provide a URL. We will add the link in the revised version of our paper.


\subsection{\textcolor{red}{Typos}}
Thank you, we will correct all identified typos in the revised version.

\end{document}