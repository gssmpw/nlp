\section{Introduction}
% \cite{zhang2024benchmarking} modify the figure to fig.

% 大模型

% text summarization for database or search engine optimization
% SEO
% Frontiers: Supporting Content Marketing with Natural Language Generation
% Text Summarization Techniques for Meta Description Generation in Process of Search Engine Optimization
% Design of optimal search engine using text summarization through artificial intelligence techniques
% https://wordlift.io/blog/en/text-summarization-in-seo/
% https://marketbrew.ai/a/text-summarization-seo#how-does-text-summarization-impact-the-readability-and-user-experience-of-a-website
%  Web-page classification through summarization. 
% DeepTitle -- Leveraging BERT to generate Search Engine Optimized Headlines
% Searching web documents using a summarization approach

% database
% Automatic Text Summarization Methods: A Comprehensive Review
% Querying a summary of database
% Text summarization in the biomedical domain: A systematic review of recent research
% https://www.frase.io/blog/20-applications-of-automatic-summarization-in-the-enterprise/

% IR
% Application of Text Summarization techniques to the Geographical Information Retrieval task

%金融预测
%搜索引擎优化
%

% Automatic text summarization is a classic problem in the field of natural language processing (NLP) with a wide range of applications~\cite{tam2015webtablesumm,2019soe,sharma2022database_summ}.

Automatic text summarization is a classic problem in the field of Natural Language Processing (NLP), with wide applications in areas such as search engine optimization~\cite{2019soe}, financial forecasting\cite{fintech}, and public sentiment analysis~\cite{twitter_news}. And the advent of large language models (LLMs) like GPT-3~\cite{NEURIPS2020_gpt3}, OPT~\cite{opt}, and Llama2~\cite{touvron2023llama2openfoundation} has revolutionized the field of text summarization. LLMs not only improve the summarization quality but also generate more flexible and coherent summaries than traditional text summarization models~\cite{luhn1958automatic, dong-etal-2018-banditsum, zhang-etal-2018-neural, t5, genest-lapalme-2012-fully}. \citet{zhang2024benchmarking} indicated that human evaluators show an equal preference for summaries generated by freelance writers and those produced by LLMs.


However, the hundreds of billions of parameters in LLMs require substantial computational and storage resources. For example, to efficiently deploy the Llama2-70B model at FP16 precision, we typically require two Nvidia A100 GPUs with 80GB of memory each. This means that only organizations with significant computing power can offer LLM services, raising concerns about service stability and data privacy. As a result, small language models (SLMs) have emerged as a potential alternative. They typically use a decoder-only architecture and have text generation capabilities similar to LLMs but with fewer than 4B parameters~\cite{azure2024phi,qwen,cai2024internlm2,abdin2024phi3,llama3.2}. They are designed to run efficiently on edge devices like smartphones and personal computers, making them more accessible.



% Despite their smaller size, SLMs have already achieved competitive performance by leveraging advanced training techniques and high-quality training data.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/introduction_example.pdf}
    \caption{Comparison of news summaries by Phi3-Mini and LLMs. Phi3-Mini is comparable to LLMs with fewer words, highlighting the potential of SLMs.  }
    \label{fig:intro}
\end{figure}



% Using SLM instead of LLM for news summarization on the edge devices is a low-cost method that also protects user privacy. 

Due to the high demands for efficiency and data privacy in areas like financial forecasting and sentiment analysis, using SLMs for text summarization on the edge is quicker, safer, and more stable than relying on cloud-based LLMs. However, there is currently a lack of comprehensive works evaluating the capabilities of SLMs in text summarization. Titans~\cite{tiny_titan} evaluated the capabilities of SLMs in meeting summarization, but it did not include general-purpose SLMs, and the ROUGE metric~\cite{lin-2004-rouge} they used only reflects the quality in terms of relevance and is heavily influenced by the reference quality~\cite{zhang2024benchmarking}. \citet{lu2024smalllanguagemodelssurvey} conducted a broader evaluation of SLMs across various tasks but did not include text summarization, leaving a gap in understanding SLMs in this area.



% However, while several studies have demonstrated the strong text generation capabilities of SLMs, their specific abilities in the domain of news summarization remain underexplored. Tiny Titans~\cite{tiny_titan} evaluated the capabilities of SLMs in meeting summarization, but it did not include general-purpose SLMs, and the ROUGE metric~\cite{lin-2004-rouge} they used is heavily influenced by the reference quality and does not cover all aspects of the summary evaluation~\cite{zhang2024benchmarking}. \citet{lu2024smalllanguagemodelssurvey} conducted a broader evaluation of SLMs across various tasks but did not include news summarization, leaving a gap in understanding SLM in this area.


In this paper, we aim to conduct a comprehensive evaluation and analysis of SLMs for news summarization, a process that compresses news articles into shorter versions while preserving key information as shown in Figure~\ref{fig:intro}. We first benchmark 19 SLMs on 2,000 news samples, evaluating their performance based on relevance, coherence, factual consistency, and summary length. Despite notable differences among SLMs, we find that the best-performing ones can generate news summaries comparable in quality to LLMs, while producing shorter summaries. This makes SLMs a possible choice for real-time summarization tasks. And among all the SLMs we evaluated, Phi3-Mini and Llama3.2-3B-Ins perform the best. Furthermore, we explore the impact of prompt design and instruction tuning on the news summarization capabilities of SLMs.

% We also investigate the factors influencing the news summarization capabilities. Our findings indicate that prompt engineering has a limited impact on enhancing the summarization abilities, with even complex prompts potentially degrading model performance. Therefore, simple prompts are enough in practical applications. Furthermore, the effects of instruction tuning on SLMs vary significantly. The quality of summaries generated by the Llama3.2 series models shows a significant improvement after instruction tuning, which aligns with \citet{zhang2024benchmarking}'s conclusions. However, the summary quality of the Qwen2 and InternLM2 series models shows almost no change after instruction tuning. 


In summary, our contributions are as follows: 
\begin{itemize}
  
    \item We benchmark 19 SLMs on 2,000 news articles, finding that models like Phi3-Mini and Llama3.2-3B-Ins achieve performance comparable to 70B LLMs, with the advantage of producing shorter summaries. 
    
    \item Our experiments reveal that simple prompts outperform complex ones in SLM summarization, suggesting practical implications for prompt design.

    \item We find that instruction tuning has inconsistent effects on SLM performance, with significant improvements in models like Llama3.2 but minimal gains in others like Qwen2 and InternLM2.
    
\end{itemize}

The remainder of the paper is organized as follows: Section~\ref{sec:background} covers the background and related work, Section~\ref{sec:method} details the methodology employed in our evaluation, Section~\ref{sec:setup} introduces our experimental setup, Section~\ref{sec:benchmark} presents the benchmarking results of SLMs, Section~\ref{sec:factor} analyzes the influencing factors on SLM summarization, Section~\ref{sec:Verification} verifies our evaluation method, and Section~\ref{sec:conclusion} concludes with a summary of findings and future work.








% When evaluating the text summarization capabilities of LLMs, many existing works~\cite{laskar-etal-2023-systematic,goyal2022news} have found that while traditional models like Pegasus~\cite{pegasus} and Brio~\cite{brio} outperform LLMs on reference-based metrics such as ROUGE~\cite{lin-2004-rouge}, human evaluators often prefer the summaries generated by LLMs. Additionally, other studies~\cite{fabbri2021summeval,zhang2024benchmarking} have shown that reference-based metrics typically have a low correlation with human evaluations because of low-quality reference summaries. Although we can directly use human evaluation, the high financial cost prevents us from applying it to large-scale models and datasets. Existing research~\cite{fabbri2021summeval} also found significant differences in the quality of human annotators, leading to substantial fluctuations in inter-annotator agreement. Recently, some works~\cite{goyal2022news,kendeer,gao2023human-like,liu2023benchmarking,wang2023chatgpt} have explored using LLMs as evaluators and found they correlate highly with the human evaluation. However, this approach also faces the high cost of LLM API usage~\cite{laskar-etal-2023-systematic} and fair issues~\cite{not-yet}.


% Therefore, this paper aims to evaluate the text summarization capabilities of SLMs using low-cost and effective metrics and to analyze the factors influencing the text summarization performance of SLMs. First, in order to improve the consistency between reference-based evaluation and human evaluation, we use LLMs to generate high-quality reference summaries. Our experiments demonstrate that high-quality references can increase the Kendall correlation coefficient with human evaluations from 0.43 to 0.74 in summary relevance, closing to the consistency between different human evaluators. Then, We evaluate several SLMs, including TinyLlama~\cite{zhang2024tinyllama}, the GPT-Neo series~\cite{gpt-neo-paper}, the Qwen2 series~\cite{qwen}, the Phi series~\cite{microsoft2023phi2,abdin2024phi3}, and others, from four perspectives: relevance, coherence, factual consistency, and summary length. Phi3-Mini can perform well across all dimensions, sometimes surpassing 7B LLMs. Finally, we study the impact of prompt design, instruction tuning, model size, and training data on the text summarization capabilities of SLMs. Among these factors, model size and the quality of training data had the most significant influence, while the effects of the other factors were relatively limited.

