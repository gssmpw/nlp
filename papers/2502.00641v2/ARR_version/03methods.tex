% \section{Evaluation Method}
% In this section, we first analyze the shortcomings of existing news summarization evaluation methods. Subsequently, we illustrate the new news summarization evaluation method used in this paper.


%1. Why use system level corr?
%2. different human different score?
%3. why this method? compared with llm evaluation is still well
%加一个coherence的图
% \section{Text Summarization with SLMs}
% \section{Methodology}
% \label{sec:method}

% In this section, we outline the process of news summarization with general-purpose SLMs and introduce reference summary generation for evaluation.






% \subsection{News Summarization with SLMs}



\section{LLM-Augmented Reference-Based Evaluation}\label{sec:method}


% Reference-based evaluation methods are widely used for evaluating text summarization~\cite{papineni-etal-2002-bleu,fabbri2021summeval,google-2023-benchmarking,tiny_titan}. These methods typically rely on statistical metrics (e.g., ROUGE~\cite{lin-2004-rouge}, BLEU~\cite{papineni-etal-2002-bleu}) or language models (e.g., BertScore~\cite{bert-score}, BLEURT~\cite{sellam-etal-2020-bleurt}) to measure the similarity between generated and reference summaries, with higher similarity indicating better quality. However, existing research~\cite{how_well,fabbri2021summeval} indicates that the poor quality of reference summaries in current datasets significantly weakens the correlation between these evaluation methods and human preferences. Moreover, \citet{zhang2024benchmarking} found that using summaries generated by freelance writers as references can improve alignment with human preferences. Numerous studies have shown that LLM-generated summaries are comparable in quality to those written by freelance writers and are often preferred for their style~\cite{goyal2022news,dead_summarization}. Therefore, we use LLM-generated summaries as high-quality reference summaries for the evaluation of SLMs. Figure~\ref{fig:workflow} illustrates how LLM-generated summaries are used as references to evaluate SLMs. The LLM generates summaries autoregressively, while the SLM-generated summaries are scored based on selected metrics.


% LLM-generated reference summaries not only match the quality of those created by freelance writers but also offer two distinct advantages. First, LLMs produce summaries faster than freelance writers. We can use LLMs to collect hundreds of summaries within a few hours, which is beneficial for large-scale news evaluations. In contrast, a freelance writer typically takes about 15 minutes to complete a single summary~\cite{zhang2024benchmarking}. Second, the financial cost of generating reference summaries with LLMs is lower. Hiring a freelance writer to summarize a news article costs around four dollars~\cite{zhang2024benchmarking}. In Section~\ref{sec:Verification}, we verify that LLM-generated reference summaries significantly improve reference-based evaluation.


As the reference-based method offers higher efficiency, better reproducibility, and lower cost compared to human and LLM evaluation~\cite{fabbri2021summeval,zhang2024benchmarking}, we use the reference-based method for large-scale SLM evaluation. However, some existing research~\cite{how_well,fabbri2021summeval} indicates that the poor quality of heuristic reference summaries in current datasets weakens their correlation with human preferences. Moreover, \citet{zhang2024benchmarking} find that using high-quality summaries as references can significantly improve alignment with human preferences. Thus, we use LLM-generated summaries as references in our SLM evaluation instead of relying on original dataset references. 
% Figure~\ref{fig:workflow} illustrates how LLM-generated summaries serve as references for evaluating SLMs.

LLM-generated reference summaries offer two key advantages.
First, they have higher quality than original heuristic summaries; \citet{zhang2024benchmarking} find LLMs averaged 4.5 on a five-point scale, while original references scored only 3.6. Additionally, \citet{dead_summarization} find that LLM-generated summaries are preferred to those written by freelance authors by up to 84\% of the time. 
Second, LLMs produce high-quality summaries quickly, generating hundreds in a few hours, which benefits large-scale news evaluations. In contrast, a freelance writer typically takes about 15 minutes for a single summary~\cite{zhang2024benchmarking}. 
% \end{itemize}



% Section~\ref{sec:Verification} further verifies that LLM-generated references enhance reference-based evaluation.

% \section{LLM-Generated Reference Verification}
% \label{sec:Verification}
We also verify whether the LLM-generated reference summaries can improve the effectiveness of reference-based evaluation on three human evaluation datasets~\cite{zhang2024benchmarking,fabbri2021summeval} in Table~\ref{tab:compare_kt_corr}. These datasets include summaries generated by various models as well as human ratings. We compare the correlation of three popular similarity metrics with human scoring using different references. BertScore and BLEURT calculate similarity based on fine-tuned language models, while RougeL measures similarity through textual overlap. We use prompt 2 in Figure~\ref{fig:prompt} as the input of the LLM. For all datasets, we consistently use two sentences to summarize the article. LLM-generated summaries significantly improve consistency than original references. BertScore performs best in both the relevance and coherence aspects.

\begin{table*}[]
% \caption{System-level Kendall's tau correlation coefficients with different reference summary settings.}
\begin{center}
% \begin{adjustbox}{max width=2.05\columnwidth}
\resizebox{2\columnwidth}{!}{
\begin{tabular}{cccccccccc}
\hline
\multirow{2}{*}{Metric} &
  \multirow{2}{*}{\begin{tabular}[c]{@{}c@{}}Reference\\ type\end{tabular}} &
  \multicolumn{4}{c}{Relevance evaluation} &
  \multicolumn{4}{c}{Coherence evaluation} \\
 &
   &
  \begin{tabular}[c]{@{}c@{}}Bench\\ -CNN/DM\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Bench\\ -XSum\end{tabular} &
  SummEval &
  Average &
  \begin{tabular}[c]{@{}c@{}}Bench\\ -CNN/DM\end{tabular} &
  \begin{tabular}[c]{@{}c@{}}Bench\\ -XSum\end{tabular} &
  SummEval &
  Average \\ \hline
\multirow{3}{*}{RougeL}   & Original    & 0.6732 & 0.2647 & 0.3714 & 0.4364          & 0.4248 & 0.6765 & 0.1238 & 0.4084          \\
                           & Qwen1.5-72B & 0.7778 & 0.7059 & 0.4476 & \textbf{0.6438} & 0.5556 & 0.5000 & 0.2381 & \textbf{0.4312} \\
                           & llama2-70B  & 0.7778 & 0.6324 & 0.4095 & 0.6066          & 0.5294 & 0.3676 & 0.2000 & 0.3657          \\ \hline
\multirow{3}{*}{BertScore} & Original    & 0.6209 & 0.2206 & 0.4476 & 0.4297          & 0.4771 & 0.6912 & 0.4286 & 0.5323          \\
                           & Qwen1.5-72B & 0.7516 & 0.6324 & 0.8095 & 0.7312          & 0.5817 & 0.6029 & 0.6000 & \textbf{0.5949} \\
                           & llama2-70B  & 0.7516 & 0.6618 & 0.8286 & \textbf{0.7413} & 0.5294 & 0.6029 & 0.6190 & 0.5838          \\ \hline
\multirow{3}{*}{BLEURT}    & Original    & 0.5425 & 0.2647 & 0.181  & 0.3294          & 0.3203 & 0.6176 & 0.2381 & 0.3929          \\
                           & Qwen1.5-72B & 0.5817 & 0.7206 & 0.4476 & 0.5833          & 0.4902 & 0.4853 & 0.5429 & 0.5061          \\
                           & llama2-70B  & 0.5556 & 0.7059 & 0.5429 & \textbf{0.6015} & 0.5425 & 0.4706 & 0.6762 & \textbf{0.5631} \\ \hline
\end{tabular}}
% \end{adjustbox}
\end{center}
\caption{{System-level Kendall's tau correlation coefficients between reference-based and human evaluations under different reference summary settings. "Original" is the original heuristic reference from the dataset; others are generated by LLMs. References generated by LLMs improve the effectiveness of reference-based evaluations.}}
\label{tab:compare_kt_corr}
\end{table*}






\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ARR_version/fig/prompt2.pdf}
    \caption{ The prompt templates for the language model to generate summaries. }
    \label{fig:prompt}
\end{figure}



