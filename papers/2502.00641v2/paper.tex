% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage{enumitem}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}

\usepackage{multirow}
\usepackage{adjustbox}
\usepackage{amssymb}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{Evaluating Small Language Models for News Summarization: Implications and Factors Influencing Performance}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{Borui Xu \\
  Shandong University \\
  \texttt{boruixu@mail.sdu.edu.cn} \\\And
  Yao Chen\thanks{Corresponding authors} \\
  National University of Singapore \\
  \texttt{yaochen@nus.edu.sg} \\
  \And
  Zeyi Wen \\
  HKUST(GZ) \& HKUST \\
  \texttt{wenzeyi@ust.hk}
  \AND
  Weiguo Liu\textsuperscript{*} \\
  Shandong University \\
  \texttt{weiguo.liu@sdu.edu.cn}
  \And
  Bingsheng He \\
  National University of Singapore \\
  \texttt{hebs@comp.nus.edu.sg}}

% \author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
% \\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
% \\
% \\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
% \\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
% }

\begin{document}
\maketitle
\begin{abstract}

% While large language models (LLMs) excel at text summarization, significant computing and memory resources limit their widespread application. A more accessible alternative is using small language models (SLMs), which have fewer parameters and can run on consumer devices like mobile phones and PCs. However, the abilities of SLMs in text summarization, as well as the gaps and characteristics compared to LLMs, require further study.
% Large language models (LLMs) deliver superior summarization quality but demand high computational resources, limiting their practical use. Small language models (SLMs) offer a more accessible alternative, capable of efficient real-time summarization on edge devices. However, their summarization capabilities, and how they compare to LLMs, require further investigation. In this paper, we present a comprehensive evaluation of SLMs for news summarization, focusing on relevance, coherence, factual consistency, and summary length. First, we evaluate 19 SLMs on 2,000 news samples and find that, despite significant variations in SLM summarization capabilities, the top-performing SLMs like Phi3-Mini and Llama3.2-3B-Ins exhibit performance comparable to that of 70B LLMs and generate more concise summaries. Then our further exploration reveals that SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in the summary quality. Additionally, we find that instruction tuning does not consistently enhance the news summarization capabilities of SLMs.

The increasing demand for efficient summarization tools in resource-constrained environments highlights the need for effective solutions. While large language models (LLMs) deliver superior summarization quality, their high computational resource requirements limit practical use applications. In contrast, small language models (SLMs) present a more accessible alternative, capable of real-time summarization on edge devices. However, their summarization capabilities and comparative performance against LLMs remain underexplored. This paper addresses this gap by presenting a comprehensive evaluation of 19 SLMs for news summarization across 2,000 news samples, focusing on relevance, coherence, factual consistency, and summary length. Our findings reveal significant variations in SLM performance, with top-performing models such as Phi3-Mini and Llama3.2-3B-Ins achieving results comparable to those of 70B LLMs while generating more concise summaries. Notably, SLMs are better suited for simple prompts, as overly complex prompts may lead to a decline in summary quality. Additionally, our analysis indicates that instruction tuning does not consistently enhance the news summarization capabilities of SLMs. This research not only contributes to the understanding of SLMs but also provides practical insights for researchers seeking efficient summarization solutions that balance performance and resource use.



\end{abstract}

%生成式两个字？
% what is the conclusion?
% what is the insight?
%2000 instances? more than previous works?
%limitation: reference bias? traditional math model?幻觉检测？缺少对数据质量对SLMs影响的进一步探索


%contributation
%我们使用LLM增强了基于reference评估与人类偏好的一致性
%不同SLM之间的差异巨大，一般3B左右的模型可以取得较好的结果，phi3目前最好
%在文本总结领域，LLM和SOTA SLM的差距很小（远小于其他任务？），使用SLM在终端设备上进行文本总结是可能的。
%和之前结论的不同，instruct不能更好summary
%不同数据集上保持稳定
%随着SLM的发展，summary的提升越来越小？

% 现有指标都只有用rougeL

%conclusion
% 我们认为现在一些小模型已经适合在端侧直接进行新闻文本的总结，其中相关性和连贯性已经很好，幻觉问题需要解决，相比新闻总结能力，提高它们在端侧的推理速度更加重要？
%要不要添加一些选择建议
%细化比较了instruct的影响，格式的影响
%和LLM稳定性比较之前工作没比较过
%细化和llm的比较
%效率对比？
%之前论文也没做过指令遵从的比较，有一个做过，但比较难，我们想简单的比如控制字数。这也是以后发展的一个方向
%SLM在relevance和coherence方面已经很好，未来更应该考虑fact的问题

% \input{ARR_version/01introduction}
% \input{ARR_version/02background}
% \input{ARR_version/03methods}
% \input{ARR_version/04experiment}

% title issue (done)
% abstract (done)
% sec1 figure 1(done) 
% sec1 SLM intorduction (done)
% figure 2 (done)
% sec3.2 provide some detailed number (done)

\input{ARR_version/01_introduction_paper_debugger}
\input{ARR_version/02_barkground_paper_debugger}
\input{ARR_version/03methods}
\input{ARR_version/04experiment}

\section*{Acknowledgments}
This work is funded by the National Key R\&D Program of China (No. 2019YFA0709400), the NSFC Project (No. 62306256), the Guangzhou Science and Technology Development Projects (No. 2023A03J0143 and No. 2024A04J4458), and the Guangzhou Municipality Big Data Intelligence Key Lab (NO. 2023A03J0012). This work is also supported by the National Research Foundation, Singapore under its Industry Alignment Fund – Pre-positioning (IAF-PP) Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore. Borui's work was done when he was a visiting student at the National University of Singapore.


% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\input{ARR_version/05appendix}



\end{document}
