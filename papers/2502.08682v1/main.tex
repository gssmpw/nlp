\documentclass[sigconf, balance=false, authorversion, nonacm]{acmart}

\usepackage[utf8]{inputenc}
\usepackage{lipsum}
\usepackage[group-separator={,}]{siunitx}
\usepackage{tikz}
\usetikzlibrary{matrix, positioning, fit, plotmarks, patterns, tikzmark, patterns.meta}
\usepackage{pgfplots}
\usepgfplotslibrary{statistics,groupplots}
\usepackage{colortbl}
\usepackage{subfig}
\usepackage{makecell}
\usepackage{amsmath}
\usepackage{xspace}
\usepackage{extdash}
\usepackage{pifont}
\usepackage{graphicx}

% ASM listings
%\usepackage[newfloat,finalizecache,cachedir=minted-cache]{minted}

\def\mystrut{\vphantom{hg}}
\def\titlestrut{\vphantom{hg}}

\pgfplotscreateplotcyclelist{embedding list}{
  red\\
  blue\\
  black\\
  orange\\
  violet\\
  teal\\
}

\pgfplotsset{
  compat=1.17,
  cycle list name=embedding list,
}

\title[On the Role of Pre-trained Embeddings in Binary Code
Analysis]{On the Role of Pre-trained Embeddings \\ in Binary Code
  Analysis\vspace{0.3cm}}
\author{Alwin Maier} \affiliation{ \institution{Max Planck
    Institute for \\ Solar System Research} \country{Germany} }
\author{Felix Weißberg} \affiliation{ \institution{Technische Universität Berlin}
  \country{Germany} \vspace{0.8cm} } % KR: Let's reserve two lines for later.
\author{Konrad Rieck} \affiliation{ \institution{Technische Universität Berlin\\ \& BIFOLD}
  \country{Germany} \email{\phantom{rieck@tu-berlin.de}} }

\settopmatter{printacmref=false, printccs=false, printfolios=false}

\setcopyright{cc}
\setcctype{by-sa}

\makeatletter
\renewcommand{\@copyrightpermission}{%
  {\footnotesize  
  This is a preprint of the paper published in the proceedings of the 19th ACM Asia Conference on Computer and Communications Security (AsiaCCS). The final version is available at \href{https://doi.org/10.1145/3634737.3657029}{https://doi.org/10.1145/3634737.3657029}
  } \\}
\makeatother 

% Keywords
\keywords{Transfer learning, Binary code analysis}

% CCS Concepts (https://dl.acm.org/ccs#)
\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10010147.10010257.10010258</concept_id>
       <concept_desc>Computing methodologies~Learning paradigms</concept_desc>
       <concept_significance>300</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}

\ccsdesc[300]{Computing methodologies~Learning paradigms}

\mathchardef\mhyphen="2D
\newcommand{\insn}{I}
\newcommand{\fun}{F}
\newcommand{\seq}{S}
\newcommand{\iset}{\mathcal{I}}
\newcommand{\sset}{\mathcal{S}}
\newcommand{\funset}{\mathcal{F}}
\newcommand{\cfgs}{\mathcal{C}}
\newcommand{\emb}{\Phi}
\newcommand{\embi}{\Psi}
\newcommand{\embf}{\Omega}
\newcommand{\mnem}{M}
\newcommand{\op}{O}
\newcommand{\dataset}{\mathcal{D}}
\newcommand{\task}{\mathcal{T}}
\newcommand{\model}{\mathcal{M}}

\newcommand{\palmtree}{\textsl{PalmTree}\xspace}
\newcommand{\wtov}{\textsl{Word2Vec}\xspace}
\newcommand{\itov}{\textsl{Instruction2Vec}\xspace}
\newcommand{\itovx}{\textsl{Instr.2Vec}\xspace}
\newcommand{\atov}{\textsl{Asm2Vec}\xspace}
\newcommand{\etoe}{\textsl{end\Hyphdash*to\Hyphdash*end}\xspace}
\newcommand{\rand}{\textsl{random}\xspace}
\newcommand{\ada}{\textsl{ada-002}\xspace}
\newcommand{\Gemini}{\textsl{Gemini}\xspace}
\newcommand{\SAFE}{\textsl{SAFE}\xspace}

\newcommand{\gcc}{\textsl{GCC}\xspace}
\newcommand{\clang}{\textsl{CLang}\xspace}
\newcommand{\debian}{\textsl{Debian}\xspace}

% Circled text
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=1.0pt] (char) {\small{#1}};}}

\newcommand{\traindata}{\ensuremath{\mathcal{T}}}

\newcommand{\performance}{\mathcal{A}}
\newcommand{\rocauc}{\mathcal{B}}

\begin{document}

\begin{abstract}
  Deep learning has enabled remarkable progress in binary code
  analysis. In particular, pre-trained embeddings of assembly code
  have become a gold standard for solving analysis tasks, such as
  measuring code similarity or recognizing functions. These
  embeddings are capable of learning a vector representation from
  unlabeled code. In contrast to natural language processing, however,
  label information is not scarce for many tasks in binary code
  analysis. For example, labeled training data for function
  boundaries, optimization levels, and argument types can be easily
  derived from debug information provided by a compiler. Consequently,
  the main motivation of embeddings does not transfer directly to
  binary code analysis.

  In this paper, we explore the role of pre-trained embeddings from a
  critical perspective. To this end, we systematically evaluate recent
  embeddings for assembly code on five downstream tasks using a corpus
  of 1.2 million functions from the Debian distribution. We observe
  that several embeddings perform similarly when sufficient labeled
  data is available, and that differences reported in prior work are
  hardly noticeable. Surprisingly, we find that end-to-end learning
  \emph{without} pre-training performs best on average, which calls
  into question the need for specialized embeddings. By varying the
  amount of labeled data, we eventually derive guidelines for when
  embeddings offer advantages and when end-to-end learning is
  preferable for binary code analysis.
\end{abstract}

\maketitle

\section{Introduction}

Deep learning has been a driving force behind several advances in
computer security. In particular, the ability of neural networks to
distill information from highly complex data, such as assembly code,
has led to a number of learning-based methods for binary code
analysis. These methods allow, for example, to locate function
boundaries~\citep{PeiGuaWil+21, AlvSon19, ShiSonMoa15}, differentiate
optimization levels~\citep{PizIno21, CheShiLi+18}, assess code
similarity~\citep{XuLiuFenYin+17, ZuoLiYouLuo+19, MasAntPet+19,
  RedLuoZen19}, reconstruct arguments~\citep{ChuSheSaxLia+17,
  JinPeiWonLin+22, HeIvaTsaRay+18}, and detect aliases in memory
\citep{GuoMuXin19}.  While the approaches differ in the architecture
of the neural networks used, most share a key component: an
\mbox{\emph{embedding}}. This learned vector representation originates
from the area of natural language processing and provides geometric
access to the data's structure and semantics, forming a versatile
basis for solving different learning tasks.

Over recent years, several methods have emerged for crafting
embeddings tailored to binary code analysis, including
\atov~\citep{DinFunCha19}, \itov~\citep{LeeKwoChoLimBaePar19}, and
\palmtree~\citep{LiQuYin21}. Additionally, dedicated approaches such
as \Gemini~\citep{XuLiuFeng+17} and \SAFE~\citep{MasLunPet+19} have
been specifically designed for generating function embeddings used to
detect similar functions.
%
The underlying rationale for these embeddings lies in their
\emph{pre-training} on large collections of unlabeled code, which
allows for encoding general characteristics the data and learning a
versatile representations for various downstream tasks.
%
Over time, these embeddings have become a gold standard for applying
deep learning to binary code analysis~\citep{AhnAhnKooPae+22,
  BisBarLaz+22, PeiGuaBroChe+21, JinPeiWonLin+22, YuRuiQiy20,
  ZuoLiYouLuo+19}.

Although natural language processing bears
%
similarities with
code analysis, the availability of labeled data differs
%
between the two domains. For natural language text, tedious manual
labeling is often unavoidable to create examples for supervised
learning, rendering pre-trained embeddings indispensable.
%
In contrast, for many tasks of binary code analysis, labeled data can
be easily generated from debug information provided by a compiler. For
example, labels for function boundaries, optimization levels, and
argument types can be extracted during the compilation process and
enable constructing large-scale training sets with label information
automatically.
%
As a result, the necessity of pre-trained embeddings in natural
language processing does not naturally apply to tasks in binary code
analysis, where end-to-end learning is often possible.

In this paper, we investigate the role of pre-trained embeddings
for binary code from a critical perspective. For this investigation,
we construct a labeled evaluation corpus of 1.2 million functions from
the Debian distribution, totaling about 129 million x86
instructions. This corpus allows us to systematically evaluate the
capabilities and limitations of five widely used embeddings for
assembly code, namely \wtov~\cite{MikSutCheCorDea13},
\atov~\citep{DinFunCha19}, \itov~\citep{LeeKwoChoLimBaePar19}, and
\palmtree~\citep{LiQuYin21}. In particular, we evaluate the
performance of each embedding in different configurations on five
common downstream tasks of binary code analysis: compiler detection,
optimization level identification, function argument prediction,
argument type reconstruction and code similarity detection.

Our results provide a new view on pre-trained embeddings in binary
code analysis: First, we observe that the embeddings hardly differ in
performance if sufficient training data is available,
%
so that differences discussed in prior work are not
noticeable~\citep{LiQuYin21}. Even a random instruction embedding
provides a reasonable performance in our
experiments. Second, we surprisingly find that end-to-end learning
\emph{without} a pre-trained embedding yields the best performance on
average. Contrary to our intuition, pre-training does not generally
unlock additional information, and it provides no advantage in binary
code analysis when sufficient labels are available.

Our work should not be interpreted as a general criticism of
pre-trained embeddings: By reducing the amount of labeled
data, we can also demonstrate the utility of this technique
in our evaluation. In particular, PalmTree~\citep{LiQuYin21} provides
the best overall performance when labeled training data becomes
scarce. We can derive guidelines to help practitioners decide
whether or not to use pre-training.
%
Consequently, our work adds a new facet to research on deep learning
for binary code analysis, indicating that benefits from other domains
do not necessarily carry over and need be critically reflected.
%
In summary, we make the following contributions:
%
\vspace{0.2cm}
\begin{enumerate}\setlength{\itemsep}{3pt}
  
\item \emph{Critical evaluation of pre-trained embeddings.} We present
  a systematic evaluation of embeddings for binary code analysis with
  varying training data and embedding dimensions while also
  considering computational expenses.
  
\item \emph{Large-scale evaluation corpus.} We provide researchers
  with an open corpus of 1.2~million labeled functions and 129~million
  x86~instructions from the Debian distribution for five
  downstream tasks (\url{https://github.com/a0x77n/orbit-dataset}).
  
\item \emph{Recommendations for binary code analysis.} We derive
  guidelines to study the performance of pre-trained embeddings on
  assembly code and decide when to rely on conventional end-to-end
  learning instead.

\end{enumerate}

\paragraph{Roadmap.} We briefly review the background of pre-training
and embeddings for assembly code in
Section~\ref{sec:pre-training}. Our benchmark corpus of labeled code
for five downstream tasks is then introduced in
Section~\ref{sec:benchmark} and the corresponding experiments in
Section~\ref{sec:experiments}. We discuss our findings on pre-trained
embeddings and derive recommendations in
Section~\ref{sec:results}. Finally, Section~\ref{sec:conclusion}
concludes the paper.

\section{A Primer on Pre-Training}
\label{sec:pre-training}

Let us start by introducing some background on the concept of
pre-training and embeddings for assembly code, before critically
reflecting on their role in binary code analysis.

\subsection{Training with a Headstart}
\label{sec:pre-training-vs}

When humans learn a new task, such as a playing a music instrument,
they usually do not start from scratch but are able to built upon
prior knowledge. For example, someone who played the violin is
probably faster in learning to play the cello than someone without
prior experience. This insight into the human learning process has
also been employed in the machine-learning domain and is commonly
referred to as transfer learning.

To be more specific, a learning model trained for a certain task can
help create a second model for a different task.
%
The first task is commonly referred to as \emph{pre-training task} and
the second as \emph{downstream task}.  The rationale underlying this
transfer is that the second model may perform better when building on
the knowledge of the first one.  Since this transfer learning
typically revolves around improving the performance of downstream
task, we refer to the pre-training as task-agnostic (with respect to
the downstream task), while the downstream training is task-specific.

Pre-training is considered especially beneficial in cases where high
quality labelled data is expensive to create but unlabelled data is
widely available. In natural language processing and computer vision,
for example, vast amounts of unlabeled data are available on the
Internet and can be employed for pre-training.
%
Since the pre-training task and the downstream task do not need to
share the same training objective, it becomes possible to use
unsupervised learning for pre-training on a large unlabeled dataset
and perform supervised learning on a smaller labeled one.
%
A well-known example of this strategy is the unsupervised learning of
input representations. In this case, the pre-training task learns a
vector representation of the data, denoted as \emph{embedding}, which
serves as input for the subsequent downstream tasks, as shown in
Figure~\ref{fig:overview}(a).

\begin{figure}[htbp]
  \centering \subfloat[Pre-training and downstream task]
  {\includegraphics[width=0.98\columnwidth]{figures/orbit-overview1.pdf}}
  %\vspace{0.25cm}

  \subfloat[End-to-end learning]%
  {\includegraphics[width=0.98\columnwidth]{figures/orbit-overview2.pdf}}

  \caption{Schematic comparison of pre-training task with downstream
    task and conventional end-to-end learning.\vspace{0.2cm}}
  \label{fig:overview}
\end{figure}


This type of pre-training has been shown to be effective for several
learning tasks in several domains. For example, in computer vision,
pre-training has been employed for object
recognition~\cite{DonJiaVin14} and semantic
segmentation~\cite{LinCheCoh17}. Similarly, in natural language
processing, embeddings have been successfully applied for question
answering~\cite{DevChaLeeTou19}, machine
translation~\cite{EduOttAul18}, and text
summarization~\cite{AghGupShr21}.
%
Naturally, these advances have spawned a research in security aimed at
improving binary code analysis. The rationale of this work has been to
create \emph{embeddings} for assembly code that provide a versatile
representation, simplifying downstream tasks, such as function
recognition and type inference.

Pre-training contrasts with so-called \emph{\etoe learning}. In this
setup, the representation of the data is learned along with the task
at hand, as shown in Figure~\ref{fig:overview}(b). The previously
separate tasks now focus on the same objective and work with the same
labeled data. That is, the learned representation becomes
task-specific and is only suitable for the particular downstream
task. Unlabeled data cannot be used in this setting. Consequently,
\etoe learning requires access to a sufficient amount of labeled data.

\subsection{Instruction Embeddings}
\label{sec:instr-embedd}

We proceed to investigate the different embeddings that have been
proposed for binary code analysis. To this end, we first give a formal
definition and then provide a recap of recent approaches for embedding
assembly code.

Simply put, embeddings are functions mapping data of the input domain
to a vector representation. This vector can then be used for
subsequent machine-learning tasks. The main reason behind using such a
representation is, that it does not simply encode a word as a
low-dimensional vector but is also able to express relationships among
different words. For example, synonyms may be mapped to vectors within
close proximity, while opposites are a mapped far apart from each
other. Since these are appealing characteristics for machine learning,
embeddings are widely used for natural language tasks
\cite{DevChaLeeTou19, EduOttAul18, AghGupShr21} and recently for
binary code analysis, where instructions take up the role of words.

\begin{figure}[h]
  %\vspace{0.15cm}
  \includegraphics[trim={4.7cm 21.35cm 8cm 4.5cm},clip,width=0.95\columnwidth]{figures/asm.pdf}
  \vspace{-0.25cm}
  \caption{\vspace{-0.3cm}Example of x86-64 assembly instructions.\vspace{0.1cm}}
  \label{fig:asm}
\end{figure}

Before introducing these embeddings, we first need to formally define
what an instruction is. In particular, we refer to an instruction as
the disassembled representation of a machine code operation for a
given architecture. Generally, an instruction consists of an operation
type denoted as \emph{mnemonic} ($\mnem$) and a group of
\emph{operands} \mbox{($\op_1$, \dots, $\op_n$)}. Typically, the
operand of an instruction can have one of the following types:
\begin{enumerate}
  \setlength{\itemsep}{2pt} % Mehr Luft zum Lesen ;-)
\item \emph{Register.} The operand is a processor \emph{register},
  where each register has a unique identifier.

\item \emph{Immediate value.} The operand is an address value or a
  numerical constant.
  
\item \emph{Memory access.} The operand is a memory access expression
  which usually consists of several components.

\end{enumerate}
While the specific structure of the operands and the instruction
depends on the architecture and the syntax flavor, the general
structure remains the same. To give an example for a specific case,
Figure~\ref{fig:asm} shows a sequence of five instructions for the
x86-64 architecture in Intel syntax. For the instruction in line
three, the mnemonic is highlighted in \tikzmark{red}red and the
operands in \tikzmark{yellow}yellow. The first operand is a register
operand, while the second one is a memory access expression. In this
specific case, \circled{1}, \circled{2}, \circled{3} and \circled{4}
point to the components of the memory access: the \emph{base
  register}, \emph{index register}, \emph{scale} and
\emph{displacement value}, respectively. Although these instructions
actually describe program semantics, it is easy to see that they can
be interpreted as words (token) or phrases, similar to natural
language text.

Embeddings for binary code differ in how they characterize the content
of the instructions. While most embeddings build on tokenization
schemes for operations and operands that are architecture independent,
some approaches utilize specific knowledge about instruction semantics
and execution behavior. The embeddings considered in our work fall
into the first category, with the exception of \itov, which
incorporates knowledge about the structure of x86-specific memory
access expressions.

To describe common instruction embeddings jointly, we phrase the
previous description more formally. To this end, let $\iset$ be the
set of all
instructions. The embedding method can then intuitively be described
as a function $\embi$ mapping an instruction $\insn \in \iset$ to a
real valued vector of dimension $d$:
\[
  \embi: \iset \longrightarrow \mathbb{R}^d
\]
Although sufficient for most cases, this definition is not able to
capture methods which create context-dependent embeddings, such as
\palmtree. That is, for a sequence of instructions
$\seq = (\insn_1, \dots, \insn_m)$, the embedding for one instruction
also depends on other instructions within the same sequence. To also
capture such embedding methods we extend our definition to sequences
of instructions.  Let $\sset$ be the set of all instruction sequences
$\sset = \bigcup_i \iset^i$.  This allows for the following embedding
function definition
\[
  \emb: \sset \longrightarrow \bigcup_i \mathbb{R}^{d \times i}
\]
where a whole instruction sequence is mapped to a matrix in which each
column represents the embedding vector of the respective instruction.
%
With this definition it is possible to map the same instruction to
different vector representations dependent on the surrounding
instructions.

The remainder of this section introduces the six instruction
embeddings used in our evaluation.

\paragraph{(a) \wtov} This embedding \cite{MasLunPet+19} is a variant
of the classic embedding introduced by \citet{MikSutCheCorDea13},
which has been originally designed for natural language text.  To
learn a vector representation of assembly instructions, one can
directly transfer this idea to binary code by considering instructions
sequences as sentences and instructions themselves as words.
%
The resulting embedding is based on the word embedding function $\phi$
of the original model and yields the function
\begin{equation*}
  \embi(\insn) = 
  \begin{cases}
    \phi(\insn) \text{, if $\insn$ is in the vocabulary} \\
    0^d \text{ otherwise.}
  \end{cases}
\end{equation*}
That means out-of-vocabulary (OOV) tokens are mapped to the
zero~vector.  For the considered \wtov based embeddings approach, full
instructions are considered as a single token and no further
tokenization is conducted. However, immediate values greater than
\num{5000} are normalized, i.e. replaced by the word \texttt{IMM}.
%
In line with \citet{MasLunPet+19}, our implementation uses the
skip-gram algorithm~\cite{MikCheCorDea13} with negative sampling
\cite{MikSutCheCorDea13}. A similar instruction embedding based on a
Word2Vec model has also been used by \citet{ChuSheSaxLia17} for
learning function type signatures.

\paragraph{(b) \itov}

The \itov embedding \cite{LeeCho+17} utilizes a classic model as
well. As opposed to \wtov, it is trained over individual instruction
components gathered from instruction sequences. Based on the
component-based embedding $\phi$ that maps a component to a $m$
dimensional space, an embedding for instructions is assembled. The
embedding vector of an instruction $\insn$ can be divided into 9~slots
of size $m$. Slot~1 contains the embedding vector of the
operation~($\mnem$), slot~2 up to slot~5 contains the embedding of the
first operand~($\op_1$), and slot~6 to slot~9 contains the embedding
of the second operand~($\op_2$). If an operand is absent, the
respective slots are set to $0^m$.
%
The layout of the slots of the operands depends on the type of the
respective operand:
%
\begin{enumerate}
\item $\op_i$ is a register $x$: $\textrm{slot}_{1 + 4*i + 1} = \phi(x)$
\item $\op_i$ is a immediate $x$: $\textrm{slot}_{1 + 4*i + 2} = (0, \dots 0, x)$
\item $\op_i$ is a memory access $(a, b, c, d)$:\\
  $\textrm{slot}_{1 + 4*i + 1} = \phi(a)$, where $a$ is the base register,\\
  $\textrm{slot}_{1 + 4*i + 2} = (0, \dots, 0, d)$, where $d$ is the displacement value,\\
  $\textrm{slot}_{1 + 4*i + 3} = \phi(b)$, where $b$ is the index register,\\
  $\textrm{slot}_{1 + 4*i + 4} = \phi(c)$, where $c$ is the scale
\end{enumerate}

The final embedding vector of $\insn$ is obtained by concatenation of
the nine slots:
$\embi(\insn) = \mathrm{slot}_1 \mathbin\Vert \dots \mathbin\Vert
\mathrm{slot}_9$. Hence, the \itov embedding has the dimension
constraint $d = 9m$. As can already be seen by the way in which the
embeddings are created, \itov uses a finer granularity compared to the
\wtov approach. That is, the instruction is broken down into mnemonics
and operands, including registers, immediates, and memory access
elements:
For example, the third instruction of Figure~\ref{fig:asm} would be
tokenized into \texttt{ADD RAX RSP RCX 2 0x8}.

\paragraph{(c) Asm2Vec} \citet{DinFunCha19} introduces Asm2Vec
\cite{DinFunCha19}, an embedding technique for binary functions based
on the document embedding method PV-DM~\cite{LeMik14}. Similar to the
\itov approach, Asm2Vec utilizes an embedding function $\phi$, mapping
instruction components to an $m$-dimensional vector. To enhance the
embedding, the instructions's syntactic structure is considered:
\[
  \embi(\insn) = \phi(\mnem) \mathbin\Vert \frac{1}{N} \sum_{n=1}^N
  \phi(\op_n)
\]
Each instruction embedding is defined as the concatenation of the
mnemonic and the averaged embeddings of the operands. The tokenization
in \atov is identical to \itov and the embedding dimension $d$
is constrained to even numbers: $d = 2m$. Asm2Vec provides embeddings
for functions and intermediate embeddings for instructions
We investigate both types in our evaluation.

\paragraph{(d) \palmtree} This embedding \cite{LiQuYin21} is the most
recent assembly language model build on top of BERT
\cite{DevChaLeeTou19}. Since BERT is based on the transformer
architecture \cite{ShaParUsz+17} its models are capable of producing
different embeddings for the same word dependent on the word's
context. \palmtree can do the same for instructions. Hence, different
from previous approaches, this embedding is context-dependent. Also
different from the previous approaches, \palmtree interprets
instructions as sentences and instruction tokens as words. Compared to
the other approaches, \palmtree uses an even more fine grained
tokenization which considers every syntactic element of the
instruction. For example, the third instruction in
Figure~\ref{fig:asm} is tokenized into \texttt{ADD RAX , [ RSP + ( RCX
  * 2 ) + 0x8 ]}.

To train the language model, two tasks from the BERT approach are
reused, namely, masked language modeling and an adapted version of
next sentence prediction. Also a third task is taken into account
which builds on the clearly documented semantics of instructions
(i. e. source and destination are known) and aims at predicting
def-use relationships between two instructions. The \palmtree
embedding function $\emb_{\text{PalmTree}}$ maps a sequence of
instructions to a real valued matrix by performing a mean pooling of
the hidden states of the second last layer of the transformer
encoder. This gives the instruction-sequence embedding function
\[
  \emb(\seq) = \emb_{\text{PalmTree}}(\seq)
\]
and a context-dependent embedding representations for the $i$-th
instruction of this sequence by $\emb_{\text{PalmTree}}(\seq)_i$.

\paragraph{(e) Random} As a baseline, we also consider a random
embedding. This embedding is conceptually trivial and just uses a
random projection to map instructions to real-valued vectors. It can
be expressed as $\embi(\insn) = r_\insn$ with $r_\insn$ being
uniformly sampled from $\mathbb{R}^d$ for each instruction. Note that
this embedding can be considered a lower bound for the utility of
embeddings, as it randomly represents instructions in a vector space
without any information inferred from real-word binary code.

\paragraph{(f) End-to-end} The conceptual antagonist to these
pre-trained embeddings is \etoe learning. In this case, a so-called
embedding layer is added to the neural network used as learning
model. This layer executes the mapping $\embi$ from tokens to vectors
before the data is forwarded to subsequent layers in the
model. Technically, $\embi$ is defined by an embedding matrix which
can be thought of as a lookup table. Different from pre-trained
embeddings, the entries of the embedding matrix are trainable
parameters of the neural network, realizing a task-specific
representation.

For our implementation of an embedding layer in end-to-end learning,
we process each function using \atov's tokenization scheme to split
instructions into tokens first. Based on these tokens we adapt a
vocabulary with \num{2048} features consisting of the \num{2046} most
frequent tokens and two special tokens, one used for masking and the
other represents the out-of-vocabulary (OOV) token. The OOV token
combines the less frequent tokens. Subsequently all tokens of a
function are encoded using a unique number for each vocabulary token,
resulting in a sequence of numbers which are streamed into the
embedding layer. The embedding layer turns each number into a
$d$-dimensional embedding vector.

\subsection{Function embeddings}
\label{sec:function-embeddings}

In contrast to embeddings at the instruction level, function
embeddings aim to generate a comprehensive vector representation of an
entire binary function. Before we present different embeddings for
binary functions, we formally describe our notion of a binary function
and define the embedding function.

A binary function is represented by its control-flow graph and
comprising basic blocks, i.e., continuous sequences of disassembled
instructions interconnected by directed edges that signify the flow of
control. Formally, let
$C = (\mathcal{V}, \mathcal{N}: \mathcal{V} \to \{x: x \subset
\mathcal{V}\})$ be a control-flow graph with a set of basic blocks
$\mathcal{V}$, and a function $\mathcal{N}$ that maps each basic block
to its successors.% according to the control flow.
%
We can then define an embedding function using the set of all control
flow graphs $\mathcal{C}$
as % $\cfgs$ the set of all control flow graphs, and consider
\[
  \embf: \mathcal{C} \longrightarrow \mathbb{R}^d
\]
that maps an arbitrary control-flow graph $C$ of a binary function to
a $d$-dimensional vector. This embedding function outputs a singular
vector representation, effectively condensing the intricate structure
of a binary function into a point within a continuous vector space,
enabling streamlined comparison and analysis of functions in binary
code.

In the following, we present four function embedding methods, two of
which leverage instruction embeddings, resulting in a total of 15
different embeddings for our evaluation.

\paragraph{(a) Asm2Vec} As highlighted in the preceding section, \atov
constitutes an embedding technique for binary functions, drawing
inspiration from the PV-DM model rooted in Word2Vec principles. In
PV-DM, the model learns to predict a word based on both context words
and a paragraph vector. Throughout the training of \atov, the vector
representations of instruction components and entire functions undergo
multiple updates. Following training, the embedding of an unknown
function $C$ is derived by applying the model through gradient
descent, allowing for the inference of vector representations
$\embf(C)$.

\paragraph{(b) Gemini} \Gemini, introduced by \citet{XuLiuFenYin+17},
employs a neural network to detect similar binary functions, building
upon the \textsl{Structure2Vec} algorithm \cite{DaiDaiSong16}. In
\Gemini, basic block-level features are iteratively aggregated into
function-level features based on the binary function's control
flow. While the original \Gemini implementation relies on manually
selected features \cite[see][]{FengZhou+16, XuLiuFeng+17} for each
basic block, we further incorporate pre-trained instruction embeddings
as well as the \etoe and \rand embedding types to generate diverse
initial representations for basic blocks.

Given the instruction sequence of a basic block
$v = (\insn_1, \dots, \insn_m) \in \mathcal{V}$ and the
instruction-sequence embedding function $\emb$, the initial
representation is computed as
\[
  x_v^\emb = \frac{1}{m} \sum_{i=1}^m \emb(v)_{\ast,i},
\]
where $\emb(v)_{\ast,i}$ is the $i$-th column and the embedding vector
of $\insn_i$. In total, we compute six different vector
representations $\embf(C)$ for each binary function
$C = (\mathcal{V}, \mathcal{N})$ using the same iterative approach of
\textsl{Structure2Vec} architecture.

\paragraph{(c) SAFE} Another method for detecting similar functions is
\SAFE \cite{MasLunPet+19}, which relies on a self-attentive neural
network. Unlike \atov and \Gemini, \SAFE operates without the need for
a control-flow graph. Instead, it treats functions as flat linear
sequences of instructions, employing a recurrent neural network. The
instruction sequence undergoes initial embedding using the \wtov
technique before being fed into the neural network. Additionally, we
incorporate the previously introduced instruction embeddings. In
total, we generate five distinct embedding types, aligning with the
network architecture proposed by \citeauthor{MasLunPet+19}. For a
given function $C = (\mathcal{V}, \mathcal{N})$ and the
instruction-sequence embedding function $\emb$, the embedding vector
$\embf(C)$ by running the embedded instruction sequence through the
neural network.

\paragraph{(d) Ada-002} Finally, we consider a large language model
for embedding. OpenAI's \emph{text-embedding-ada-002} represents their
latest model, succeeding task-specific embeddings and offering a
unified
representation\footnote{\url{https://openai.com/blog/new-and-improved-embedding-model}}. Designed
for versatile applications, including text similarity, it features a
maximum context length of \num{8192} tokens and an embedding dimension
of \num{1536}. While not exclusively tailored to binary code, it draws
strength from extensive training on a diverse corpus, making it a
state-of-the-art, all-purpose embedding.

\section{An Embedding Benchmark}
\label{sec:benchmark}

The cornerstone of our evaluation is a vast open corpus of labeled
assembly code designed for various downstream tasks. Prior to delving
into the assessment of the embeddings under consideration, we provide
an introduction to these tasks (Section~\ref{sec:downstr-tasks}) and
outline our pipeline for generating large-scale datasets automatically
to train embeddings and learn models
(Section~\ref{sec:dataset-generation}).

\subsection{Downstream Tasks}
\label{sec:downstr-tasks}

\begin{table*}
  \centering
  \caption{Number of classes and model size for each downstream
    task. The right column shows the number of additional weights for
    the embedding in the end-to-end learning setup.}
  \label{tab:tasks}
  \input{tables/weights.tex}
\end{table*}

For our evaluation, we select five common downstream tasks by which we
measure the performance of the different embeddings.  Each task
addresses a different challenge in binary
analysis~\citep{PizIno21,CheShiLi+18,ChuSheSaxLia17,DinFunCha19,XuLiuFeng+17,MasLunPet+19}.
Note that we are not proposing new solutions for these tasks but
rather recreate existing experiments to investigate the role of
pre-trained embeddings. % in the tasks.
%
Table~\ref{tab:tasks} offers a summary of the five downstream tasks,
presenting the number of trainable parameters for the learning model
and the parameters exclusive to the embedding layer in our end-to-end
approach, along with their corresponding dimensions. Notably, Task~T5,
focused on detecting similar code, does not require an additional
learning model as it relies solely on the embedding vectors. It's
worth highlighting the substantial variation in the number of model
parameters across tasks, providing an opportunity for experimentation
with learning tasks of varying complexities in the domain of binary
code analysis.

\paragraph{Compiler and optimization options} The first two tasks deal
with the detection of compilers (Task~T1) and the identification of
optimization options (Task~T2).
Both tasks have been previously investigated by \citet{PizIno21} and
\citet{CheShiLi+18}.  Our implementation is based on the network
introduced by \citeauthor{PizIno21}.  They propose a shallow network
as learning model that consists of a single LSTM layer with a terminal
dense output layer.  The LSTM layer has an output dimension of
\num{256} and performs the actual learning. 
To handle the different classes of each task, we use
\num{2} output nodes for the compiler detection (\gcc and \clang) and
\num{4} nodes for the identification of optimization options
(\verb|O0|, \verb|O1|, \verb|O2|, \verb|O3|).
Different from the approach by \citet{PizIno21} we use disassembled
instructions as input as opposed to the raw bytes of machine code
instructions.

\paragraph{Function type signatures} The other two downstream tasks
deal with the recovery of function type signatures.  We choose the
implementation published by \citet{ChuSheSaxLia17} and reuse their
network architecture.  According to their method, we create two tasks
for this problem.  The first task predicts the number of function
parameters (Task~T3) and the second task aims at determining the data
type of the first parameter (Task~T4), such as \texttt{int} or
\texttt{char *}.

The learning model consists of three sequential GRU~layers configured
with dropouts to avoid overfitting. The final layer has \num{10} nodes
for the prediction of the number of arguments and \num{7} nodes for
the prediction of the argument types \citep[see][]{ChuSheSaxLia17}.

\paragraph{Function Similarity} In addition to assessing the efficacy
of function embeddings, we explore two distinct approaches for
learning embeddings for function similarity detection. The first
approach is based on \Gemini \cite{XuLiuFenYin+17}, which leverages a
graph neural network. Another method for identifying similar binary
functions is \SAFE~\cite{MasLunPet+19}, which is founded on a
self-attentive neural network architecture. We re-implement the
original \Gemini network and adjust parameters for \SAFE to align with
the complexity of \Gemini.

While this task doesn't inherently necessitate a learning model, as
embedding vectors can be directly compared using cosine similarity, we
train different embedding models using \Gemini and \SAFE, integrating
pre-trained instruction embeddings.

\subsection{Mining Debian Packages}
\label{sec:dataset-generation}

\begin{figure*}
  \centering \input{tikz/overview.tikz}
  \caption{Dataset creation pipeline. A function corpus is build from
    all available training packages. Based on this corpus the
    embeddings are trained. Using the pre-trained embeddings, each
    \debian package is transformed into a dataset file containing the
    embedded functions and corresponding labels.}
  \label{fig:dataset-generation}
\end{figure*}

For the training of instruction embeddings with unlabeled data and the
automatic construction of a labeled dataset for training the
downstream tasks, we deploy two data processing pipelines. The first
pipeline processes \debian binary packages and emits the pre-trained
embeddings. The second pipeline uses \debian packages as well, but
also requires an embedding model to generate the labeled dataset used
to train and evaluate the downstream tasks. Both processes are
depicted in Figure~\ref{fig:dataset-generation}.

We manually build each package from source for the x86-64 architecture
using eight compilation combinations.  Each compilation combinations
uses a different compiler (\gcc or \clang) and one of four
optimization levels (\verb|O0| to \verb|O3|).  Note that building a
single \debian source package can produce multiple binary packages for
the same compilation combination.

In total, we extract \num{1293205} functions consisting of
\num{129487277} instructions from \num{480} binary \debian packages.
More details are provided in Table \ref{tab:dataset}, where we list
the number of binary packages (with and without variants due to
compilation combinations), functions, and instructions for each source
package. Since the dataset generation processes binary \debian
packages our dataset can be expanded by adding more packages. It is
also possible to create datasets for different architectures.

\begin{table}
  \caption{Our dataset is based on 480 binary \debian packages compiled
    from 8 source packages.}
  \centering \input{tables/ds1}
  \label{tab:dataset}
\end{table}

\paragraph{Training and test splits} We split the dataset in training
and test data at the level of \debian source packages. Consequently,
all functions from a single source package will be placed either in
the training set or test set. We never spread functions from one
source package over training and test data. We believe that this is an
important detail as functions can be shared not only between the
binaries of a packages, but also between binary packages of a single
source package.  While functions may also be shared across source
packages, we argue that these cases are rare and do not necessarily
lead to overfitting. Instead, they reflect scenarios in which source
code is simply reused, for example, through the bad practice of
copying and pasting code snippets.
%
Our data split procedure is different from the approaches used by
\citeauthor{ChuSheSaxLia17} who divide each open source project into
training and test data or \citeauthor{PizIno21} who randomly partition
all functions of the whole dataset into training and test data. Our
procedure aims at reducing the risk that information from the training
data leak to the test data.

Table~\ref{tab:dataset} also shows which packages we use for training
and which we use for testing. The processing of training as well as
test packages are displayed in Figure~\ref{fig:dataset-generation}.

\paragraph{Unsupervised data} For pre-training embeddings, we create a
generic function corpus. To do so, we iterate over the binary \debian
packages selected for training and hand them over to the corpus
generator. The corpus generator creates a indefinitely repeating
stream of disassembled and formatted instruction sequences on-the-fly,
matching the requirements of the target embedding type. The streaming
approach allows for processing large amounts of data without storing
them in memory. Moreover, by working directly with \debian packages
the corpus can be easily extended.
%
This token stream is then used as unsupervised training data for the
\wtov, \itov, and \atov embeddings.
%
It is important to note that packages selected for testing are not
used for pre-training embeddings. This will prevent patterns from test
packages to leak into the training process.

In particular, we extract all ELF~binaries from each package.  Next,
we extract the opcodes of functions from the ELF~binaries.  Here, we
leverage the DWARF~\cite{manual:dwarf} debugging information to
identify every function and to pinpoint the address range of the
functions to extract the corresponding opcodes from the text segment
of the ELF binary. For this purpose, we use the python library
pyelftools\footnote{\url{https://github.com/eliben/pyelftools}} to
parse the text segment as well as the debugging information.  Last,
the opcodes are disassembled with the Capstone
engine\footnote{\url{https://www.capstone-engine.org/}} to produces a
sequence of tokens for each function. Since each embedding type
requires its specific instruction formatting and tokenization, we
implement different tokenization schemes for the \wtov, \itov, and
\atov embeddings.
%
Figure~\ref{fig:dataset-generation} shows where the corpus generator
fits into the pipeline.

\paragraph{Embedding Pre-training}

To pre-train the embedding models, we leverage the function corpus
extracted from Debian packages, as illustrated in
Figure~\ref{fig:dataset-generation}. In this process, we make use of
the gensim\footnote{\url{https://radimrehurek.com/gensim/}} Python
library, incorporating a module that implements the Word2Vec
algorithm. The Word2Vec algorithm is applied to both the \wtov and
\itov embedding types, as both are grounded in a Word2Vec model,
albeit with distinct encodings for each instruction. In the case of
\atov, we extend gensim to support the \atov embedding, as detailed in
Section~\ref{sec:instr-embedd} and
Section~\ref{sec:function-embeddings}. The training of instruction
embeddings marks the concluding step in the pipeline depicted in
Figure~\ref{fig:dataset-generation}. Subsequently, these resulting
embedding models are then reused to construct supervised datasets,
facilitating the training and evaluation of various tasks, as well as
the development of embedding models based on \Gemini and \SAFE.

\paragraph{Supervised data}

To create a labeled dataset for training the downstream tasks and
learning function embeddings we use the pre-trained embedding models
to encode the instruction sequences of functions and the debug
information to obtain the labels.  We create an independent record
file for each binary package and embedding. These dataset files
contain the encoded functions and the corresponding labels for all
four downstream tasks. Each file can then be loaded by
TensorFlow\footnote{\url{https://www.tensorflow.org/}} and fits
seamlessly into TensorFlow's data processing pipeline.

To train embedding models based on the network architectures of
\Gemini and \SAFE, labeled function pairs are required. While this
presents a limitation for \Gemini and \SAFE, both approaches can
incorporate pre-trained embeddings, allowing them to still benefit
from unsupervised data in theory. All embedding models undergo
training using a twin-network \cite{BroGuyLeC+93}.  The dataset used
for training the twin-networks is derived from the same Debian
packages used to pre-train the instruction embeddings and \atov.  This
involves a specialized corpus generator sampling both positive
function pairs (comprising the same functions with different
compilation configurations) and negative function pairs (consisting of
different functions from arbitrary packages).

All datasets used in our evaluation are available for other
researchers: \url{https://github.com/a0x77n/orbit-dataset}

\section{Experiments}
\label{sec:experiments}

Equipped with a large corpus of labeled code for different downstream
tasks, we are finally ready to compare the strengths and weaknesses of
the considered embeddings. To this end, we design five basic
experiments, each addressing a different aspect of pre-trained
embeddings.

\paragraph{Experimental instances.}

For every experiment, we evaluate multiple \emph{instances} of the
downstream tasks. In this context, an instance is defined by the
learning model of the task (T1-T4), the deployed embedding and the
available supervised training data. For example, the learning model
for task~T1 paired with a \wtov embedding and all training data from
Table~\ref{tab:dataset} is one instance of the compiler identification
task. The performance of each instance is then evaluated using the
test data from Table~\ref{tab:dataset}. With this experimental setup,
we can directly evaluate the capabilities of the embeddings, simply by
comparing the instances considered in one experiment.

\paragraph{Performance measures.}

As an evaluation criterion, we uee the \emph{accuracy} metric for
tasks T1-T4, aligning with its prominent usage as the primary measure
in the original publications~\cite{PizIno21, ChuSheSaxLia17}. Task T5
employs the \emph{area under the ROC curve}, a widely recognized
metric for assessing binary classifiers, providing a comprehensive
view of the trade-off between true-positive and false-positive
rates. The \emph{ROC curve} (receiver operating characteristic curve)
\citep{fawcett2006} visually represents the performance of a
classification model or detection system across various classification
thresholds, plotting the true positive rate against the false positive
rate.

Given a downstream task, the accuracy $\performance_F$ with respect to
a set of labeled test functions $F$ is given by the ratio of correct
classified functions $f \in F$ to the total number of functions in
$F$. The \emph{general accuracy} $\performance$ of an instance is the
performance over all test functions from Table~\ref{tab:dataset}.
%
Analogously, we define the accuracy of a binary package. For this
definition, we need a formal notion of a \emph{compilation
  combination} $c$, which is an element of
$C = \{ \text{\gcc}, \text{\clang} \} \times \{ \verb|O0|, \verb|O1|,
\verb|O2|, \verb|O3| \}$.  Now, given an instance of a downstream
task, the \emph{package accuracy} for the package~$p$, denoted as
$\performance_p$, is defined as the average accuracy over all
compilation combinations of $p$, that is,
$\frac{1}{8} \sum_{c \in C} \performance_{F(p_c)}$, where $F(p_c)$
denotes the functions in $p$ build with compilation combination $c$.%
We make use of these notions of general and package accuracy for the
different instances in our evaluation.

\paragraph{Experimental setup.}

Table~\ref{tab:dataset} gives an overview of the datasets
%and the \debian packages
that are used for training and testing.
%in the five experiments.
Each instance is evaluated on \num{132529}~functions from
\num{240}~binary \debian packages each.  The functions used for
training are determined by the instance, consistently sourced from
packages within the training set.  In total, \num{1160676}~functions
from \num{240}~packages are available for training. Notably, while we
vary the amount of supervised training data in some experiments, the
number of unsupervised training samples remains constant
throughout. This means that pre-trained embeddings are consistently
trained using the \emph{entire} set of training data.

Whenever we consider an instance involving \etoe learning, we add an
additional dropout layer after the embedding layer to counteract
overfitting. This layer randomly drops \qty{20}{\percent} of its
inputs.  The reason for this extension is that unlike pre-trained
embeddings, the size of the labeled data used to train instances with
\etoe learning varies, making them prone to overfitting when the
labeled data is limited.

We proceed with the presentation of the five distinct experiments,
each delineated in Sections~\ref{sec:experiments:baseline} to
\ref{sec:experiments:exp4}. Within each section, the purpose of the
experiment is elucidated alongside comprehensive details regarding its
execution. It is worth noting that the reader has the flexibility to
peruse these sections in any order they prefer. Furthermore, to
facilitate navigation and comprehension, each section closes with a
forward reference to the corresponding results expounded upon in
Section~\ref{sec:results}. Additionally, for convenient reference, an
overarching overview of all experiments is encapsulated in
Table~\ref{tab:experiments}, providing a succinct summary for quick
comprehension and navigation.

\begin{table}[htbp]
  \centering
  \caption{Overview of the experiments. The right columns list the
    corresponding sections for the description of the experiments and
    results.}
  \label{tab:experiments}
  \begin{tabular}{lrrrrr}
    \toprule
    Experiment & \#\,Shards & Dimension & Seq.~len. & Exp. & Res. \\
    \midrule
    Baseline & 1 & 128 (126\footnotemark) & 512 & \ref{sec:experiments:baseline} & \ref{sec:results:baseline} \\
    Experiment 1 & varies & 128 (126\footnotemark[5]) & 512 & \ref{sec:experiments:exp1} & \ref{sec:results:exp1} \\
    Experiment 2 & varies & 128 (126\footnotemark[5]) & 512 & \ref{sec:experiments:exp2} & \ref{sec:results:exp2} \\
    Experiment 3 & 1 & varies & 512 & \ref{sec:experiments:exp3} & \ref{sec:results:exp3} \\
    Experiment 4 & 1 & 512 & varies & \ref{sec:experiments:exp4} & \ref{sec:appendix:exp4} \\
    \bottomrule
  \end{tabular}
\end{table}
\footnotetext{The dimension of the \itov embedding is 126.}

\subsection{Baseline Experiment}
\label{sec:experiments:baseline}

To begin, we introduce a general baseline experiment. That is, we
design an experiment to test our implementations of the various
embeddings and downstream tasks for correctness.  We create five
different instances for each combination of downstream task and
embedding type and train them on all training data.  Each instance is
based on a different random seed in order to detect possible
variations. We further use this experiment to revise the peculiarities
of the downstream task and discuss the natural limits on the
classification performance and identify variations in the test data.
%
The results are presented in Section~\ref{sec:results:baseline} and
complemented in Appendix~\ref{sec:appendix:baseline}.

\subsection{Experiment 1: Size of Labeled Data}
\label{sec:experiments:exp1}

The objective of the first experiment is to investigate the effect of
the amount of labeled data on the accuracy of the employed
embeddings. Moreover, we consider the changes in training time.  The
rationale behind this experiment is the following: Unlike pre-trained
embeddings, an end-to-end embedding is build from scratch and can only
be trained with supervised data.

For each combination of downstream application and embedding type, we
create nine different groups of instances. The groups differ in the
amount of labeled data used for training. In contrast, the instances
within one group are trained with the same number of functions, but
the function sets are disjoint.
%
On that account, we partition the training data $T$ into $n$ disjoint
subsets $t_i^n, i=1, \dots, n$, i.e., $T = \bigcup_{i=1}^n t_i^n$.  We
refer to each set $t_i^n$ as a \emph{shard} (of the training data).
For instance, if $n = 1$ the same data that is used for pre-training
the embeddings is used to train the instances.  As $n$ gets larger
less supervised data is available for training the instances, e.g.,
for $n=2^8$ less than \qty{1}{\percent} of the data is used.
%
Doing so, we mimic scenarios where more unlabeled then labeled data is
available.
%
We conduct this experiment for $n = 2^i, i = 0, \dots 8$.  In total,
we train \num{511} ($\sum_{i=0}^8 2^i$) instances distributed over
nine instance groups for all \num{24} combinations of downstream task
and embedding type.

Note that the pre-trained embeddings remain unaffected by the value of
$n$.  They are consistently trained on the entire training set, while
only the learning model of the downstream task is trained on reduced
data, that is, it is trained on a single shard of the training data.
In contrast, experimental instances that use \etoe learning have only
this single shard to infer all weights for the embedding layer as well
as the learning model. Hence, the \etoe approaches face a significant
penalty for larger values of $n$.  Our primary findings of this
experiment are elaborated on in Section~\ref{sec:results:exp1}, while
Appendix~\ref{sec:appendix:exp1} offers supplementary insights.

\subsection{Experiment 2: Computational Expense}
\label{sec:experiments:exp2}

In the second experiment, we investigate the computational overhead of
\etoe learning, assuming pre-trained models are readily available and
thus only \etoe-learning incurs an additional run-time overhead. 
Using the setup from the previous section, we focus on the required
epochs, that is, the number of training cycles needed for each instance.

Recall that we work with groups of instances that exist for each task
and embedding pair, where  group is defined by the size of the
labeled data.  For example, one group uses the entire training data
while for another group it is split into multiple shards.  In this
case, the first group will contain a single instance and the
other group contains instances for each shard.  We compute the
\emph{mean number of epochs} for each group by averaging the number of
required epochs of its instances. We present the results in Section~\ref{sec:results:exp2}.

\subsection{Experiment 3: Embedding Dimension}
\label{sec:experiments:exp3}

The goal of the next experiment is to analyze the impact of the
embedding dimension on the general accuracy of an instance.  In case
of an end-to-end approach the embedding dimension is an easy to change
parameter of the network architecture.  It can be tuned like any other
hyper-parameter. By contrast, the dimension of a pre-trained embedding
is fixed.  If one wants to deploy a pre-trained embedding for some
application, the user needs to pick the embedding dimension in
advance.  In other cases, a particular pre-trained embedding might
only exist for a specific dimension. For that reason, it is worthwhile
to determine if it is possible to identify a recommended value for a
default embedding dimension that yields strong results across
different downstream tasks.

We create several pre-trained embeddings with
different embedding dimensions. We pick dimensions of the range from
\num{1} to \num{256}. Starting with the smallest possible dimension
for each embedding we subsequently increase the dimension in each
step. Some embeddings come with a restriction in regard to
the dimension: The embedding dimension of \itov is a multiple
of \num{9} and the smallest possible dimension of \atov is \num{2}.
\palmtree has a fixed dimensionality of \num{128}.
Again, all instances are trained on the entire dataset.
%
The results are summarized in Section~\ref{sec:results:exp2}.

\subsection{Experiment 4: Sequence Length}
\label{sec:experiments:exp4}

In the last experiment, we seek to find a viable sequence length for
the downstream tasks. We aim at determining a good
compromise between training time and the general accuracy of an
instance.  This trade-off enables us to train and evaluate thousands
of instances. To this end, we truncate each function sequence at
various positions and observe the general accuracy of the instances.
Starting with only the first instruction of each function, we double
the sequence length subsequently.  We stop at a maximum of \num{256}
instructions per function.  Figure~\ref{fig:function-length} shows
a bar plot of the distribution of function lengths.
% in our corpus.
Most functions are rather short. Only few functions are longer than \num{256} instructions. These functions are accumulated in the last bar of the
plots.
%
We use the \palmtree embedding for this experiment, as it is the only
context-dependent one.
%
We report the result in Section~\ref{sec:appendix:exp4} of the
appendix.

\section{Results and Discussion}
\label{sec:results}

We will now delve into the outcomes of the five experiments outlined
in Sections~\ref{sec:results:baseline} to \ref{sec:results:exp3}.  The
setup for each specific experiment can be found in
Section~\ref{sec:experiments}, where their objectives and designs are
described. Supplementary findings are also provided in the appendix
for further reference.

\subsection{Baseline Experiment}
\label{sec:results:baseline}

\begin{table}
  \caption{General accuracy of the baseline experiment. The second
    value (\textpm) reflects the minimal standard deviation observed for
    different random seeds, underscoring the stability of the results
    over all instances.}
  \centering \input{tables/accuracy}
  \label{tab:baseline}
\end{table}

The outcome of the baseline experiment is summarized in
Table~\ref{tab:baseline} and part of
Figure~\ref{fig:limited-data-function-similarity}.
Table~\ref{tab:baseline} lists the general accuracy of experimental
instances that are trained on the entire dataset, that is, the same
amount of data is available to pre-training and \etoe learning. Our
results are in line with the original publications and show that our
re-implementations are correct.
\mbox{\cite{PizIno21, LiQuYin21}}.

In Figure~\ref{fig:limited-data-function-similarity}, it can be
observed that when utilizing \qty{100}{\percent} of the training data,
the area under the ROC curve ranges from \num{0.79} to \num{0.92} for
\Gemini and \num{0.79} to \num{0.90} for \SAFE. It is essential to
highlight that our dataset differs from the evaluation data previously
employed for function similarity \citep{DinFunCha19, MasAntPet+19,
  XuLiuFenYin+17} rendering direct comparisons challenging. Despite
these discrepancies, discernible patterns emerge: models using \wtov
and \palmtree demonstrate optimal performance, while the \etoe
approach yields comparable results to \atov. Conversely, \ada exhibits
the least favorable performance in this experiment.

Apart from the reproduced performance,
we observe \emph{no} notable differences between the embeddings.
This result is particularly striking for the \rand embedding that performs
as well as the specialized instruction embeddings.  We conclude that
the embedding type is not as important when sufficient labeled data is
available for the downstream task.
%
Furthermore, we observe that the results are stable across five runs
with different random seeds. This is quantified by the standard
deviation provided in Table~\ref{tab:baseline}.  The deviation is at
most \num{0.01}.  This suggests that the used optimization algorithms
are well calibrated and the networks capacities are adequate for the
data basis. As a result, the training algorithm is able to find model
weights that produce stable results for each downstream task.

\subsection{Experiment 1: Size of Labeled Data}
\label{sec:results:exp1}

\begin{figure*}
  \centering \input{tikz/limited-data.tikz}
  \caption{General accuracy of the considered embeddings in relation
    to the available labeled training data (shards)}
  \label{fig:limited-data}
\end{figure*}

\begin{figure}
  \centering \input{tikz/limited-data-function-similarity.tikz}
  \caption{General accuracy of the considered embeddings in relation to the available labeled training data (shards).\vspace*{-0.35cm}}
  \label{fig:limited-data-function-similarity}
  %\vspace{-0.5cm}
\end{figure}

The preceding results suggest that the amount of labeled data
available is a key factor in the success of \etoe learning. Therefore,
we turn to the first experiment in which this quantity is varied. The
corresponding results for tasks~T1-T4 are shown in
Figure~\ref{fig:limited-data}, and
Figure~\ref{fig:limited-data-function-similarity} shows the results
for task~T5.

The plots show the performance of the embeddings trained with
different amounts of labeled data. This amount is defined by the
number of shards used to partition the training data. For example, if
8 shards have been generated from the training data, only
$\frac{1}{8}$ or \qty{12.5}{\percent} of the labeled data is available
for supervised training of the learning models or, in the case of
task~T5, the embedding models.

Expectedly, the overall performance improves with fewer shards; that
is, the more supervised training data is available, the better the
prediction.  This trend can be observed for all embedding types and
downstream tasks. At a certain point, however, this improvement
saturates, so that increasing the amount of labeled data does not
improve the performance. We conclude that the size of our evaluation
corpus is sufficient for the downstream tasks and further extensions
of it would not change the results significantly.

When we investigate the left part of the plots, that is, instances
with limited labeled data, we observe different yet subtle performance
differences. In the case of tasks~T1 and T2, the pre-trained
embeddings \wtov, \atov, and \palmtree lead to better results when
less labeled data is used. The \etoe approach requires more labeled
data to attain a comparable performance. This confirms the expectation
one has for pre-trained embeddings: Since a vector representation for
the input is already learned, the neural network has fewer degrees of
freedom and thus can better generalize even if only a few labeled
examples are given.
%
This observation is also confirmed by the performance of the \rand
embedding. Despite its simplicity, it also provides reasonable
accuracy, suggesting that it is not the embedding that matters, but
rather the reduced number of trainable weights of the networks.

Our findings for tasks~T3 and T4 are the other way around. For both
tasks, the pre-trained embeddings \wtov, \atov, and \palmtree show no
benefit compared to the \etoe approach.  Even with less data
available, the \etoe approach gives better results. For both tasks,
pre-training negatively affects the performance, and we cannot
identify a general advantage over conventional learning. This is
further contradicted by the results of the \rand embedding, which
works just as well, even when trained on limited training data.

The function embeddings employed in task T5 exhibit a slightly
different trend: when utilizing minimal data, no discernible
differences can be observed. However, with an increased volume of
available data, \Gemini models with \wtov and \palmtree, as well as
the manual selected features of the original \Gemini implementation,
perform better than the \etoe-based models and \atov, which have a
similar performance. Such differences cannot be identified for \SAFE
embedding models. In this context, the \etoe embedding type and all
pre-trained embeddings exhibit similar results, whereas the
performance of the \rand embedding is comparatively weaker.
Surprisingly, the performance of \itov shifts from being subpar in the
context of \Gemini to displaying a remarkably competent performance
with the \SAFE model. In both scenarios, the \ada embedding fails to
achieve competitive results.
%
Notably, in this task, the \rand embedding finally reveals its
limitations. In the case of \Gemini, this embedding type fails to
derive any benefit from a larger pool of supervised training data.

We must conclude that for the considered downstream tasks,
pre-training is not necessarily beneficial. When we examine the
average performance on the right-hand side of
Figure~\ref{fig:limited-data}, \etoe learning and \palmtree provide
the best results, regardless of the amount of labeled data
available. These results suggest that when developing a learning-based
method for binary code analysis, experimenting with \etoe learning is
clearly a sound and likely successful design decision.

\paragraph{Minimizing the labeled data}
In addition, Table~\ref{tab:accuracy_drop} shows the general
performance averaged over all instances that are trained with
256~shards.  In this case, each shard contains only
approximately~\num{4500} labeled training samples. Further, the table
shows the margin to the baseline experiment with
approximately~\num{1160676} labeled samples. Considering task~T2, the
accuracy of the \etoe approach drops by \num{0.08} while the
pre-trained approaches drop by \num{0.04} (\wtov and \palmtree),
\num{0.6} (\itov), and \num{0.05} (\atov) only. Similar observations
can be made for task~T1. As already observed, the tasks~T3 and T4
behave differently, i.e., all embedding types lose accuracy just about
equally. The limitation of labeled data has the most impact on task~T3
with a loss of up to \num{0.30}. Task~T4, however, is barely affected,
which is attracting our attention. We show in the appendix that a
different metric gives a different picture in this case.

\begin{table}[tbp]
  \caption{General accuracy of the considered embeddings with a
    minimum of labeled data. The second value (\textdownarrow)
    displayed represents the margin to the accuracy on the
    full dataset.}
  \centering \input{tables/accuracy_shards256}
  \label{tab:accuracy_drop}
\end{table}

\subsection{Experiment 2: Computational Expense}
\label{sec:results:exp2}

\begin{figure*}
  \centering
  \input{tikz/epochs-chart.tikz}
  \caption{Training time of the considered embeddings in relation to
    the supervised data size.}
  \label{fig:train-time}
\end{figure*}

\begin{figure}
  \centering
  \input{tikz/epochs-chart-function-similarity.tikz}
  \caption{Number of epochs needed to train embeddings
    for code similarity in relation to the supervised data size.}
  \label{fig:train-time-function-similarity}
  \vspace{-0.15cm}
\end{figure}

While we've noted advantages of \etoe learning in binary code analysis,
they do come with a prize: Figure~\ref{fig:train-time} shows the training time for
tasks T1-T4.
It stands out that the \etoe approach needs
the most epochs for training.
When training on more labeled data, we notice a significant drop in the
number of epochs.
Especially for tasks~T3 and T4,
where the \etoe learner has a comparable training time once the full
dataset is used.
%
The epochs needed for instances with pre-trained embeddings
remain relatively constant.
We conclude that pre-trained embeddings have acquired relevant
knowledge about the instructions, which helps training instances for
each downstream application with fewer epochs.

An alternative perspective is presented for task~T5 in
Figure~\ref{fig:train-time-function-similarity}. Here, we maintain
a consistent number of training samples per epoch.
regardless of the available data. Consequently, training the embedding
models for \Gemini and \SAFE with a larger volume of supervised data
necessitates more epochs to cycle through all samples. Our
observations indicate a general trend: the training algorithm
effectively reduces the loss over more epochs when more data is
available and concludes earlier with less data, yielding less
favorable results, as depicted in
Figure~\ref{fig:limited-data-function-similarity}.
%
Among the different embedding models, those based on \palmtree
demonstrate the fastest training time while delivering one of the best
results. Notably, the \Gemini model with the \itov embedding trains
slowly, contrasting with its relatively short training time for
\SAFE. Unlike tasks T1-T4, the training time of \etoe-based models is
not as significantly impacted negatively; however, it is still
noticeably slower.

We can conclude here that learning-based methods for code analysis can
profit from a pre-trained embedding as the training process becomes
faster. However, this is only relevant in scenarios where a complex
network architecture is involved and the training time per epoch is
high or the hardware resources are limited. Note that the prediction
time is not affected, which is much more relevant in practice. We want
to stress that we do not address the training time of the pre-trained
embeddings, since at least in theory those embeddings are only trained
once, but used for many
tasks.

\subsection{Experiment 3: Embedding Dimension}
\label{sec:results:exp3}

It remains to investigate the role of the embedding dimension in our
analysis, focusing specifically on tasks T1-T4 and the instruction
embeddings.  The results of this experiment are depicted in
Figure~\ref{fig:dimension-chart} in the appendix. The plot shows the
shift of the general accuracy of instances trained with different
dimensions.  Recall that the lowest dimension of \atov is \num{2} and
\itov's embedding dimension is a multiple of \num{9}. The \palmtree
embedding, that we use as an off-the-shelf embedding, has a fixed
dimension of \num{128}.

This experiment shows that the \etoe approach adapts better to lower
embedding dimensions. On average, instances with an \etoe embedding
attain the best performance already with 16 dimensions.  The
pre-trained embedding types tend to require higher dimensionality. As
a general rule, a dimension of \num{128} suffices for our tasks. This
confirms the observation made by \citeauthor{LiQuYin21}

In summary, we have seen that an \etoe embedding can get good results
out of lower embedding dimensions. All pre-trained embedding types
require a higher dimensionality to yield equal results. Our
explanation is that pre-trained embeddings are more universal while an
\etoe embedding is always tailored towards the downstream task. Hence,
an \etoe embedding does not require the same complexity as pre-trained
embeddings.

\subsection{Discussion}
\label{sec:discussion}

Our experiments shed new light on the role of pre-trained embeddings
in binary code analysis.
%
We show that an \etoe approach can compete with pre-trained embeddings
if sufficient labeled data is available. If training time is not a
constraint, \etoe learning can obtain the same performance even with
smaller embedding dimension.  For pre-trained embeddings to be
beneficial, the labeled data must be orders of magnitude smaller than
the unlabeled data. This raises the question of whether these
embeddings are actually relevant in practice, since the learning model
of the downstream task requires labeled data in any case.

Moreover, our evaluation reveals that the embedding process is not
crucial for the downstream task, as the success of the \rand embedding
striking illustrates. We credit this finding to task-agnostic
relations among instructions that are not as advanced as the
connections between words in natural language. Without a given
analysis task, an instruction embedding cannot carry the same amount
of information as a natural word embedding. This makes instruction
embeddings less useful and also explains why the differences between
our pre-trained embeddings are negligible.

Overall, our empirical analysis demonstrates that a general benefit of
pre-training does not exist for binary code analysis in practice. This
is contradictory to previous research in our domain
\citep[e.g.,][]{LiQuYin21}, and the common strategy to favor
pre-trained embeddings over \etoe learning. Interestingly, research in
other fields also arrived at this observation \cite{WanKhaMa20} and
questioned the role of pre-training if sufficient labeled information
is available.
%
Thus, our analysis refutes the intuition that pre-training is a
generally beneficial and therefore mandatory step in designing methods
for binary analysis.

Based on our observations, we work out the following recommendations
for deep learning in binary code analysis.

\vspace{0.2cm}
\begin{enumerate}
\setlength{\itemsep}{4pt}

\item[R1] Our first recommendation is to always try \etoe learning, as
  it produces tailored embeddings for the task at hand and is easy to
  implement. Consequently, \etoe learning should always be considered
  as the baseline approach. %for other embedding types.

\item[R2] When there is little labeled data or the size of the
  unlabeled data is orders of magnitude larger, the use of a
  pre-trained embedding is indicated. We recommend the use of the
  \palmtree model as it performs best in our evaluation.

\item[R3] We also recommend to differentiate between training and
  inference time. Only if the training time for a model is
  constrained, pre-trained embeddings are clearly preferable over slow
  \etoe learning.

\item[R4] Our last recommendation is to always publish pre-trained
  embeddings. If researchers need to re-train them anyway, their
  benefit vanishes and again \etoe learning becomes an viable
  alternative.

\end{enumerate}
%\vspace{0.15cm}

Our recommendations are to be understood as guidelines and must be
adapted to the respective downstream task. Nevertheless, we argue that
pre-training is not a ``silver bullet'' in our field and conventional
learning concepts must not be neglected from the start when developing
new approaches for binary code analysis.

\section{Conclusion}
\label{sec:conclusion}

This paper investigates the role of pre-training on binary code
analysis. To this end, we compare four pre-trained embeddings, an
end-to-end approach, and an random instruction embedding under
different downstream tasks. Our results show that binary analysis
tasks with sufficiently labeled data do not benefit from pre-trained
embeddings. Instead, conventional \etoe learning provides the best
performance on average. Only if the labeled data is artificially
reduced, we can observe an advantage of pre-training.

Our results have consequences for applying deep learning in other
tasks of binary code analysis. First, an \etoe setup is typically easy
to deploy along with the learning model. Hence, if labeled data is
available, it should be the first option when developing a new
approach. Second, when labeled data is scarce or the training time is
constrained, pre-trained embeddings are a reasonable solution. For
example, we could show that \palmtree often provides good results when
we capped the available labeled data.
%for the downstream tasks.

Overall, our study highlights an interesting aspect of
interdisciplinary research. The benefits of embeddings in one research
area can only be transferred to another if the experimental setup is
limited by similar constraints. Once these constraints are lifted from
a setup, the performance improvement may disappear. Therefore, we
recommend that practitioners develop learning-based approaches
judiciously and, when in doubt, prefer well-known learning concepts
over the latest inventions.

\section*{Acknowledgements}
This work was funded by Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy -- EXC 2092 CASA -- (390781972), the European Research Council (ERC) under the consolidator grant MALFOY (101043410), and the German Federal Ministry of Education and Research under the grant BIFOLD24B.
\clearpage

\bibliographystyle{ACM-Reference-Format}
\bibliography{sec, orbit}

\appendix

\section{Supplementary Results}

\subsection{Baseline Experiment}
\label{sec:appendix:baseline}

\begin{figure}[htbp]
  \centering \input{tikz/opt-level-labeling.tikz}
  \caption{Overlap of functions compiled with different optimization
    levels. The arrows indicate the reading direction.}
  \label{fig:option-labeling}
\end{figure}

\paragraph{A closer look at optimization levels}
The results for task~T2 are worse compared to the other
applications. We find that this decrease is not due to a more complex
downstream task, but due to an inherent problem in determining
optimization levels. To understand this problem, we first recall that
the optimization \texttt{O3} activates the same flags as \texttt{O2}
and a few others. This means that the flags enabled by \texttt{O2} are
a subset of the flags enabled by \texttt{O3}. Unfortunately, not every
function provides sufficient complexity to benefit from the unique
flags of \texttt{O3}. As a result, both options may produce the same
assembly instructions for some functions, making it impossible to
recover to the original optimization level. The same is true for the
other optimization level. However, the option \texttt{O2} adds many
more flags in addition to those added by \texttt{O1} making the
differentiation possible in most cases.


Figure~\ref{fig:option-labeling} shows the overlap between the
different optimization levels in our evaluation corpus. About
\qty{70}{\percent} of the functions compiled with option \texttt{O2}
could be compiled with option \texttt{O3} just as well and vice versa.
Less prevalent are functions compiled with option \texttt{O1} that
could also be compiled with options \texttt{O2} and \texttt{O3}.
Interestingly, the same issue also exists for the compiler-detection
task.  However, in this case it is far less pronounced and the overlap
of the labels is not that common as can be seen in
Figure~\ref{fig:compiler-labeling}.  As a consequence, performance
values as obtained for the other downstream tasks are impossible to
achieve in this setting. Considering this observation, all embedding
types actually produce results close to the optimum for the tasks~T1
and T2.

\begin{figure}[htbp]
  \centering \input{tikz/compiler-labeling.tikz}
  \caption{Overlap of functions produced with different compilers. The
    arrows indicate the reading direction.}
  \label{fig:compiler-labeling}
\end{figure}

\begin{figure*}
  \centering
  \input{tikz/boxplot3.tikz}
  \caption{Package accuracy of instances trained on the full dataset
    presented as boxplots for all considered instruction embeddings
    across all binary packages. Outliers are visually highlighted as
    horizontal lines for easy identification.}
  \label{fig:boxplot}
\end{figure*}

\paragraph{Analyzing the package performance}
In addition to the previous observations, we want to check whether
there are significant variations in prediction performance for
different binary packages. For that purpose, we first compute the
package accuracy of each instance.  Second, we compute statistical
characteristics for each downstream task and embedding type.

We depict the results as box-and-whiskers plots in
Figure~\ref{fig:boxplot}. Since we used five different seeds and our
dataset contains \num{30}~different binary packages for testing
(without taking account of different compilation combinations), each
plot is based on \num{150} values. The boxes cover the interquartile
range ($\text{IQR}$) from the first to the third quartile. That means
each box encompasses the mean accuracy values of \qty{50}{\percent} of
the binary packages. The plot shows that the prediction accuracy on
\qty{50}{\percent} of the binary packages differs by less than
\qty{5}{\percent} for tasks T1 and T2 and less than \qty{10}{\percent}
for tasks T3 and T4.
%
The whiskers extend no more than $1.5 \cdot \text{IQR}$.

Overall, we observe that the majority of the packages fall within a
normal range of packet accuracy. Nevertheless, there are some outliers
that allow for better or worse performance on some of the downstream
tasks. We investigate these outliers by identifying the individual
Debian packages. We find that these packages are outliers for all
considered embedding, indicating again that the choice of an embedding
is less relevant to the underlying learning task when sufficient
labeled data is available.

\subsection{Experiment 1: Size of Labeled Data}
\label{sec:appendix:exp1}

\begin{figure*}
  \centering
  \input{tikz/boxplot_shards256}
  \caption{Package accuracy of instances trained with a minimum of
    labeled data presented as boxplots for all considered instruction
    embeddings across all binary packages. Outliers are visually
    highlighted as horizontal lines for easy identification.}
  \label{fig:boxplot_shards256}
\end{figure*}

To check for variations in the general performance of instances
trained on shards, we again use box-and-whiskers plots. Each plot is
based on 256~different instances, each trained on a different data
shard. We show the results of this experiment in
Figure~\ref{fig:boxplot_shards256}. While some variations exist, most
instances produce similar results. We reason that the functions from
some shards are not representative for the test data and no viable
instance can be trained from those functions alone.

\begin{figure*}
  \centering \input{tikz/dimension-chart.tikz}
  \caption{ General accuracy of the considered embeddings in relation
    to the embedding dimension.}
  \label{fig:dimension-chart}
\end{figure*}

\subsection{Experiment 4: Sequence length}
\label{sec:appendix:exp4}

\begin{figure*}
  \centering \input{tikz/hist.tikz}
  \caption{Distribution of function lengths. The red bar accumulates
    all functions with more than 128 instructions.}
  \label{fig:function-length}
\end{figure*}

\begin{figure}
    \centering
    \input{tikz/sequence-length-chart.tikz}
    \caption{General accuracy for the \palmtree embedding in relation
      to the sequence length.}
    \label{fig:seq-len}
\end{figure}

Figure~\ref{fig:seq-len} shows the results of this experiment. It
shows the general accuracy of instances trained with the \palmtree
instruction embedding model in relation to the sequence length. That
is, the maximum number of instructions of each function used to train
the instances. The plot clearly shows that a certain number of
instructions is needed. This comes at no surprise, however, the plot
also shows that after processing about \num{16} instructions the
performance starts to plateaus and only little gain can be
observed. We observe no further improvement after \num{128}
instructions. Further, Figure~\ref{fig:function-length} shows that
only few functions are longer than \num{256} instructions. Because of
that, we limit the instruction sequences in the other experiments to
this threshold.

However, that such short sequences are sufficient raises questions, at
least for tasks~T3 and T4, since we expect that necessary information
is missing in such short sequences. For that reason we measure the
general accuracy with a different metric, the balanced accuracy, which
takes the label imbalance into account. The results are also shown in
Figure~\ref{fig:seq-len} and gives a different picture. The reason is,
that now the underrepresented type categories have more weight. The
classes of tasks~T1 and T2 are more balanced and, hence, the
performance is not affected.

\begin{table}
  \caption{General balanced accuracy of the considered embeddings
    witha minimum of labeled data. The second value (\textdownarrow)
    displayed represents the margin to the balanced accuracy achieved
    on the full dataset.}
  \label{tab:bal_accuracy_drop}
  \centering \input{tables/accuracy_shards256_bal}
\end{table}

Table~\ref{tab:bal_accuracy_drop} serves the same purpose as
Table~\ref{tab:accuracy_drop}, but shows the performance based on the
balanced accuracy. The average performance is noticeable
lower. Moreover, the performance drop, that is, the margin to the
performance obtained on the full dataset, is significant.

\end{document}
