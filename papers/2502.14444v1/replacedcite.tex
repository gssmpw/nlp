\section{Literature Review}
\subsection{Jiang, Z., et al.’s Compression-Based Classification Algorithm}

Compression-based classification algorithms can identify similarities between two texts by examining patterns and repetitions within the data. These algorithms often employ techniques such as dictionary-based encoding, which replaces frequently occurring patterns with shorter representations. For example, if two texts share common phrases or sequences, the algorithm will efficiently encode these similarities, thereby using less space. This process naturally highlights the similarities between texts, as repeated patterns or sequences are compressed similarly. The ability to capture these similarities depends on the algorithm's design, with some algorithms being more proficient at recognizing and leveraging these patterns for effective compression ____.

In the 2022 paper "Less is More: Parameter-Free Text Classification with Gzip" by Jiang, Z., et al., the authors introduced an innovative approach that serves as a straightforward, efficient, and versatile alternative to the commonly used deep neural networks (DNNs) for text classification tasks. This method integrates a lossless compression algorithm with a k-nearest-neighbor (k-NN) classifier, effectively bypassing the need for any pre-processing or training stages that are typically required in traditional classification methods. The proposed approach leverages the inherent ability of the Gzip compression algorithm to identify and exploit patterns within the text, which are then utilized by the k-NN classifier to perform the classification. This combination allows for a parameter-free classification process, simplifying the overall workflow and reducing computational overhead.

The k-nearest neighbors (kNN) algorithm has a computational complexity of  \(O(n^2)\), which means that its efficiency significantly decreases as the dataset size grows. This is due to the substantial computational resources needed to process larger datasets. As a result, the speed of kNN becomes a major bottleneck when working with very large datasets, limiting its practical usefulness and overall performance. This issue is especially pronounced in high-dimensional data or large-scale applications, where the computation time can become excessively long ____.

\subsection{News Article Categorization}

News organizations and various online platforms encounter the complex task of efficiently organizing and classifying large volumes of news articles to ensure that users can easily access information that aligns with their interests. This classification process involves categorizing news articles into various fields such as technology, politics, sports, entertainment, business, and more ____. As the volume of news published online increases, it is crucial to categorize articles to help people easily locate the information they need. With news coming from both print and digital sources, effective methods for organizing and understanding it are essential. Storing unclassified news does not serve much purpose ____. To address this issue, a system that monitors news in specific areas and automatically organizes it into relevant categories is essential. News classification involves extracting key information from articles and categorizing them accordingly ____.

Text classification involves categorizing text into predefined groups based on its content. This process assigns specific labels to text documents to identify their subject matter or intent ____. It is often applied in commercial domains, including spam filtering, decision making, information extraction from raw data, and other uses. It is important for different aspects because it eliminates the need for manual data classification, which is both costly and time consuming ____. Additionally, it is a core task that has greatly benefited from advancements in neural networks. However, these networks often require substantial computational resources, involving millions of parameters and large datasets with labeled data. This makes their use, optimization, and adaptation to out-of-distribution cases costly in practical applications ____.

\subsection{Gzip Compressor}

Originally developed in the mid-1990s, GZIP is based on the DEFLATE compression algorithm, which combines LZ77 and Huffman coding techniques to minimize file sizes. Although GZIP has been a standard for compression since its inception, its design is constrained by a 32 KB sliding window—a limitation stemming from the memory constraints of the time. As a result, GZIP is less efficient on modern hardware, which can easily access much larger amounts of memory ____. ____ emphasized several limitations of the Gzip compression algorithm, one of which is its fixed sliding window size. This limitation restricts the algorithm's ability to effectively identify and match patterns that are distant from each other within large documents. As a result, Gzip's performance in compressing large files can be suboptimal when dealing with data that contains patterns or repeated sequences spread over long distances, as the fixed window size is unable to capture these distant patterns efficiently.

In text classification, Gzip identifies similarities by analyzing the extent of compression achievable when combining different texts, forming the basis for metrics like the Normalized Compression Distance (NCD). It is a versatile metric designed to identify all types of similarities between signals that other distance measures usually detect individually. The NCD values range from 0 to 1, where 0 signifies maximum similarity and 1 indicates no similarity ____.

\subsection{Unigram}

The essential preprocessing step for text is tokenization, which determines the level of granularity at which textual data is analyzed and generated. Tokenization involves breaking a text stream into smaller pieces called tokens. Traditionally, most NLP models have used words as their primary units of analysis. However, more recent methods have begun decomposing text into smaller units, such as character n-grams or even underlying bytes. UnigramLM, like WordPiece, uses language models to determine the best way to break words into smaller units. However, UnigramLM starts with a much larger vocabulary and then gradually reduces it using a probabilistic approach. This approach allows for multiple possible ways to break words, and it can even randomly choose different ways based on their probabilities. While not often used alone, UnigramLM is a key part of the SentencePiece toolkit ____. 

A unigram is a specific type of n-gram, where n equals 1, and it refers to a sequence consisting of a single adjacent element extracted from a set of tokens or string elements. These elements can be letters, syllables, or complete words, depending on the application or the level of granularity desired. In simpler terms, a unigram represents an isolated unit, which could be a single character, syllable, or word, that is considered independently of its surrounding context. The concept of unigrams is widely utilized in various fields, including linguistics, speech recognition, and cryptography, where the frequency of unigrams plays a crucial role. In these domains, unigram frequencies help analyze and process large amounts of text by capturing the occurrence of individual words or symbols. This analysis calculates the probability of a word appearing based on previous words, aiding in language models, speech transcription, and cryptographic systems ____.