\section{Related Work}
\subsection{Diffusion Models for Graphs}  
Diffusion models have gained significant traction in graph generation tasks. Early approaches, such as \citet{ScorebasedGraph}, introduced score-based methods that applied Gaussian perturbations to continuous adjacency matrices, ensuring permutation invariance in generated graphs. Building on this, \citet{ScorebasedGraphDSS} extended the framework to incorporate both node attributes and edges using Stochastic Differential Equations (SDEs). However, these models relied on continuous Gaussian noise, which is inherently misaligned with the discrete nature of graphs.  

To address this issue, \citet{DiffusionGraphBenefitDiscrete} introduced a discrete diffusion model tailored for unattributed graphs, demonstrating the advantages of discrete noise over continuous perturbations in graph generation. Among the most advanced diffusion-based graph generation models, DiGress~\citep{DiGressDDGraph} employs a discrete diffusion process, where noise is introduced by iteratively modifying edges and altering node categories. \citet{SaGessSamplingGraph} further enhanced DiGress by proposing a divide-and-conquer sampling framework, which improves scalability by generating graphs at the subgraph level. Another notable approach, Latent Graph Diffusion (LGD)~\citep{LGD}, first encodes graphs into a latent space using an autoencoder and then applies continuous noise in this transformed space.  

Despite these advancements, most works focus primarily on designing diffusion noise processes, while the choice of backbone architectures—which determine how well the model captures graph structure—remains largely overlooked. These models predominantly employ graph transformers, yet their expressivity in capturing fine-grained substructure distributions is rarely analyzed in depth.

\subsection{Expressivity of Graph Neural Networks}  
The expressivity of Graph Neural Networks (GNNs)—which defines the range of functions a model can learn—is crucial for capturing complex graph distributions. Traditional GNNs, particularly those based on Message Passing Neural Networks (MPNNs)~\citep{MPNN}, update node representations by aggregating information from their neighbors. However, these architectures struggle to capture higher-order dependencies, limiting their ability to model complex graph structures accurately.  

To overcome these limitations, High-Order GNNs (HOGNNs)~\citep{NGNN, SSWL, PPGN} extend message passing by generating tuple-based representations, enabling richer structural encoding. An alternative perspective on GNN expressivity involves analyzing the graph polynomial bases that a model can approximate~\citep{GPoly}. This perspective aligns with subgraph counting, as graph polynomial functions can effectively encode structural motifs in a graph.  

Enhanced GNN architectures, capable of accurately estimating complex graph polynomials, can significantly improve score function modeling in diffusion models. This is particularly critical for preserving key substructures in generated graphs, ensuring high-quality and structurally consistent outputs. In this work, we take a graph polynomial decomposition approach to analyze the expressivity required for diffusion models to accurately capture substructure distributions. By establishing a direct link between score function estimation and GNN expressivity, we provide insights into how backbone architectures influence the fidelity of generated graphs.