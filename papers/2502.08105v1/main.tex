%%%% ijcai25.tex


\PassOptionsToPackage{dvipsnames,table}{xcolor}

% These are the instructions for authors for IJCAI-25.

\documentclass{article}
\typeout{IJCAI--25 Instructions for Authors}
\pdfpagewidth=8.5in
\pdfpageheight=11in

% The file ijcai25.sty is a copy from ijcai22.sty
% The file ijcai22.sty is NOT the same as previous years'
\usepackage{ijcai25}

% Use the postscript times font!
\usepackage{times}
\usepackage{soul}
\usepackage{url}
\usepackage[hidelinks]{hyperref}
\usepackage[utf8]{inputenc}
\usepackage[small]{caption}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{booktabs}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage[switch]{lineno}
\usepackage{enumitem}
\usepackage{multirow}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage[edges]{forest} 
\usepackage{xcolor}
% \usepackage[x11names,table]{xcolor}
\usepackage{tikz}
\usetikzlibrary{arrows,shapes,positioning,shadows,trees}

\newtheorem{definition}{Definition}


% Comment out this line in the camera-ready submission
% \linenumbers

\urlstyle{same}

% the following package is optional:
%\usepackage{latexsym}

% See https://www.overleaf.com/learn/latex/theorems_and_proofs
% for a nice explanation of how to define new theorems, but keep
% in mind that the amsthm package is already included in this
% template and that you must *not* alter the styling.
\newtheorem{example}{Example}
\newtheorem{theorem}{Theorem}

% Following comment is from ijcai97-submit.tex:
% The preparation of these files was supported by Schlumberger Palo Alto
% Research, AT\&T Bell Laboratories, and Morgan Kaufmann Publishers.
% Shirley Jowell, of Morgan Kaufmann Publishers, and Peter F.
% Patel-Schneider, of AT\&T Bell Laboratories collaborated on their
% preparation.

% These instructions can be modified and used in other conferences as long
% as credit to the authors and supporting agencies is retained, this notice
% is not changed, and further modification or reuse is not restricted.
% Neither Shirley Jowell nor Peter F. Patel-Schneider can be listed as
% contacts for providing assistance without their prior permission.

% To use for other conferences, change references to files and the
% conference appropriate and use other authors, contacts, publishers, and
% organizations.
% Also change the deadline and address for returning papers and the length and
% page charge instructions.
% Put where the files are available in the appropriate places.


% PDF Info Is REQUIRED.

% Please leave this \pdfinfo block untouched both for the submission and
% Camera Ready Copy. Do not include Title and Author information in the pdfinfo section
\pdfinfo{
/TemplateVersion (IJCAI.2025.0)
}

\title{Out-of-Distribution Detection on Graphs: A Survey}


% Single author syntax
\author{
    Author Name
    \affiliations
    Affiliation
    \emails
    email@example.com
}





% Multiple author syntax (remove the single-author syntax above and the \iffalse ... \fi here)
% \iffalse
\author{
Tingyi Cai$^{1}$
\and
Yunliang Jiang$^{1,2}$\and
Yixin Liu$^{3}$\and
Ming Li$^{1,4}$\and
Changqin Huang$^1$\And
Shirui Pan$^3$
\\
\affiliations
$^1$Zhejiang Normal University, China\\
$^2$Huzhou University, China\\
$^3$Griffith University, Australia\\
$^4$Zhejiang Institute of Optoelectronics, China\\
\emails
tingyicai@zjnu.edu.cn,
jyl2022@zjnu.cn,
yixin.liu@griffith.edu.au,
mingli@zjnu.edu.cn,
cqhuang@zju.edu.cn,
s.pan@griffth.edu.au
}
% \fi

\begin{document}

\maketitle

\begin{abstract}
    Graph machine learning has witnessed rapid growth, driving advancements across diverse domains. However, the in-distribution assumption, where training and testing data share the same distribution, often breaks in real-world scenarios, leading to degraded model performance under distribution shifts. This challenge has catalyzed interest in graph out-of-distribution (GOOD) detection, which focuses on identifying graph data that deviates from the distribution seen during training, thereby enhancing model robustness.
    In this paper, we provide a rigorous definition of GOOD detection and systematically categorize existing methods into four types: enhancement-based, reconstruction-based, information propagation-based, and classification-based approaches. We analyze the principles and mechanisms of each approach and clarify the distinctions between GOOD detection and related fields, such as graph anomaly detection, outlier detection, and GOOD generalization. Beyond methodology, we discuss practical applications and theoretical foundations, highlighting the unique challenges posed by graph data. Finally, we discuss the primary challenges and propose future directions to advance this emerging field. 
    The repository of this survey is available at \url{https://github.com/ca1man-2022/Awesome-GOOD-Detection}.
\end{abstract}

\section{Introduction}

In recent years, graph machine learning (GML) has become a powerful tool for processing graph-structured data, with wide applications in domains such as social networks, molecular biology, and transportation systems~\cite{wu2021survey}. Among the various GML techniques, graph neural networks (GNNs) have emerged as the de facto solution, achieving impressive performance across a wide range of tasks. GNNs leverage the relationships between nodes to capture complex dependencies in graph data, making them particularly effective for predictive tasks. 
% Graph neural networks (GNNs) have emerged as a powerful tool for processing graph-structured data, which is prevalent in domains such as social networks, molecular biology, and transportation systems~\cite{wu2021survey}. By effectively leveraging the relationships among nodes, GNNs enable powerful predictive capabilities in these applications. 
However, real-world scenarios often introduce challenges where testing data differs significantly from training data, a situation referred to as out-of-distribution (OOD)~\cite{PGR-MOOD}. These distribution shifts can lead to dangerously overconfident predictions, posing significant risks in safety-critical applications such as medical diagnosis and autonomous driving.  
For example, a medical diagnosis system might misclassify a novel disease, while an autonomous driving system could fail to detect an unexpected obstacle, both potentially leading to catastrophic outcomes.

\begin{figure}[!t]
    \centering
    \subfigure[Annual publication trend.]{
    \label{fig:tongji}
    \includegraphics[width=0.40\linewidth]{img/new-year.png}
    }
    \subfigure[GOOD detection for autonomous driving based on traffic graphs.]{
    \label{motivation-driving}
    \includegraphics[width=0.54\linewidth]{img/motivation2.pdf}
    }
    \subfigure[GOOD detection for drug discovery.]{
    \label{motivation-drug}
    \includegraphics[width=\linewidth]{img/motivation1.png}
    }
    \caption{Applications and rising interest in GOOD detection. This figure illustrates the diverse applications of GOOD detection, including drug discovery and robust AI systems, and highlights the increasing number of publications in this field over recent years, emphasizing its growing significance in both academia and industry.}
    \label{fig:motivation}
\end{figure}

% To improve the generalizability of graph machine learning models, \textbf{graph out-of-distribution (GOOD) detection} aims to distinguish in-distribution (IND) graphs or nodes from OOD graphs or nodes, identifying and rejecting OOD samples.
% As illustrated in Figure~\ref{fig:motivation}, the necessity of GOOD detection can be understood through two key motivations. In drug discovery (Figure~\ref{motivation-drug}), detecting previously unseen molecular structures is essential for identifying potential therapeutic candidates. Without GOOD detection, GNNs trained only on known molecular data may overlook novel compounds with breakthrough potential. In contrast, in autonomous driving (Figure~\ref{motivation-driving}), GOOD detection can be applied to traffic graph models, where detecting previously unseen or anomalous road networks, intersections, or vehicle behaviors helps prevent incorrect system predictions and enhances safety. These examples highlight the dual objectives of GOOD detection: enabling the discovery of new entities and ensuring robust safety in open-world applications.

To improve the generalizability of graph machine learning models, \textbf{graph out-of-distribution (GOOD) detection} aims to distinguish in-distribution (IND) graphs or nodes from OOD graphs or nodes, identifying and rejecting OOD samples.
The significance of GOOD detection is evident in its ability to address two critical challenges. Firstly, GOOD detection \textbf{enables the discovery of new entities}. Taking drug discovery~(Figure~\ref{motivation-drug}) as an example, detecting previously unseen molecular structures is essential for identifying potential therapeutic candidates. Without GOOD detection, GNNs trained only on known molecular data may overlook novel compounds with breakthrough potential. Meanwhile, GOOD detection also \textbf{ensures robust safety in open-world applications}. For instance, in autonomous driving (Figure~\ref{motivation-driving}), GOOD detection can be applied to traffic graph models, where detecting previously unseen or anomalous road networks, intersections, or vehicle behaviors helps prevent incorrect system predictions and enhances safety. 


% To address this challenge, \textbf{graph out-of-distribution (GOOD) detection} has emerged as a crucial research direction, aiming to enable GNNs to distinguish and reject OOD samples while maintaining high accuracy for in-distribution (IND) data. Unlike image data, graph-structured data introduces unique challenges due to its intricate relational structure among nodes. Although recent studies have proposed various methods to tackle this problem, the field remains fragmented, lacking a systematic framework and comprehensive understanding. The statistics of papers in the GOOD field are shown in Figure~\ref{fig:tongji}.
Different from OOD detection tasks on images or text, OOD detection on graphs poses unique challenges. Graph-structured data is inherently complex, characterized by diverse topologies and dynamic relationships between nodes. This complexity makes distinguishing between IND and OOD samples particularly challenging, especially in scenarios involving sparse or noisy graph structures. To address the unique challenges, GOOD detection has emerged as a rapidly growing research area, as demonstrated by the surge in related publications depicted in Figure~\ref{fig:tongji}. Despite this growing interest, the research field of GOOD detection remains fragmented, lacking a unified framework or systematic understanding. This highlights the pressing need for a comprehensive survey to consolidate existing knowledge and guide future research directions. While existing surveys on OOD detection in machine learning~\cite{yang2024survey,ju2024survey} provide valuable insights, they overlook the unique challenges posed by graph data and rarely delve into detection tasks specifically designed for such data. 


% Although recent studies have proposed various approaches to address these issues, the field remains fragmented, lacking a unified framework or systematic understanding.
% Figure~\ref{fig:tongji} shows the rapid growth in research on GOOD detection, highlighting the need for a systematic survey to consolidate the field.


% Figure~\ref{fig:tongji} shows the rapid growth in research on GOOD detection and the diversity of methodological approaches, highlighting the need for a systematic survey to consolidate the field.
% While existing surveys have explored OOD detection in general machine learning contexts~\cite{ju2024survey,yu2023mind,wu2024survey,yang2024survey,salehi2022survey,wang2024uncertainty}, they rarely address the specific challenges posed by graph data or focus on the detection task within this domain. This paper aims to fill this gap by providing the first systematic and comprehensive survey on GOOD detection. 
% Existing surveys on OOD detection in machine learning~\cite{yang2024survey,ju2024survey} provide valuable insights but rarely consider the specific challenges associated with graph data. Moreover, they seldom focus on detection tasks tailored to graph-structured data. To address this gap, this paper presents the first systematic and comprehensive survey on GOOD detection, establishing a foundational framework for this emerging area.

% To fill the gap, this paper presents the first systematic and comprehensive survey on GOOD detection, establishing a foundational framework for this emerging area. 
% This survey provides a comprehensive overview of existing methods for GOOD detection, systematically categorizing them into four distinct approaches, i.e., enhancement-based, reconstruction-based, information propagation, and classification-based approaches. For each category, we analyze the fundamental principles, representative techniques, and their practical applications. Additionally, we delineate the distinctions and connections between GOOD detection and related problems, such as GOOD generalization, graph anomaly detection, and outlier detection, clarifying prevalent misconceptions in the literature. Beyond summarizing current progress, this survey identifies pressing challenges in the field, highlights promising directions for future research, and proposes key strategies for developing more robust and efficient GOOD detection frameworks.

To fill the gap, this paper presents the first systematic survey on GOOD detection, laying a foundational framework for this emerging field. Specifically, the core contents and contributions of this survey are four-fold:
\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Systematic Taxonomy.} We present a comprehensive and fine-grained taxonomy that classifies existing GOOD detection methods into four categories: enhancement-based, reconstruction-based, information propagation-based, and classification-based approaches.
    \item \textbf{Up-to-Date Review.} We conduct a comprehensive review of representative GOOD detection methods, covering over 30 works published between 2020 and 2025. 
    \item \textbf{Insightful Discussions.} We analyze the principles, techniques, and concept distinctions of GOOD detection, clarifying its scope relative to related fields and addressing common misconceptions.
    \item \textbf{Future Directions.} We identify the key challenges and suggest potential research directions to drive future advancements and innovation in this field.
\end{itemize}

% \begin{figure}
%     \centering
%     \subfigure[Number of papers per year.]{\includegraphics[width=0.49\linewidth]{img/year.png}}
%     \subfigure[Categorization distribution.]{\includegraphics[width=0.49\linewidth]{img/category.png}}
%     \caption{Statistics of existing studies on graph OOD detection.}
%     \label{fig:tongji}
% \end{figure}


\section{Foundations of GOOD Detection}
In this section, we first formalize the problem of GOOD detection, followed by a high-level categorization of existing methods to provide a structured overview of the field.

\subsection{Problem Definition}
\label{Problem}

A graph can be denoted as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathbf{A}, \mathbf{X})$, where $\mathcal{V}=\{v_1, v_2, \cdots, v_n\}$ is the set of vertices, $\mathcal{E}$ is the set of edges, and $\mathbf{A} \in \{0,1\}^{n\times n}$ is the adjacency matrix where $a_{i,j}$ indicates whether nodes $v_i$ and $v_j$ are connected. The feature matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ encodes the features of each node, with $d$ as the feature dimension. For node-level tasks, a dataset $\mathcal{D}$ is the entire graph $\mathcal{G}$, with each node $v_i$ as a sample. For graph-level tasks, a dataset is a collection of graphs, i.e., $\mathcal{D}=\{\mathcal{G}_1,\cdots, \mathcal{G}_N\}$, with each graph $\mathcal{G}_i$ as a sample. 

% a sample can be denoted as $s_i=(v_i,y_i)$, where $y_i$ is the label for node $v_i$. For graph-level tasks, a sample can be written as $s_i=(\mathcal{G}_i,y_i)$, where $y_i$ is the label for graph $\mathcal{G}_i$.

\begin{definition}
    \textnormal{(GOOD Detection)} 
    GOOD detection focuses on distinguishing in-distribution (IND) data, drawn from a known distribution $D_{\text{in}}$, from out-of-distribution (OOD) data, sampled from an unknown distribution $D_{\text{out}}$. Specifically, the goal is to classify samples from $D_{\text{in}}$ while rejecting samples from $D_{\text{out}}$. Given a dataset $\mathcal{D}$, the task is defined as follows:

\begin{equation}
    \hat{y}_i = \begin{cases} 
    f(\mathcal{D}, s_i), & \text{if } {s}_i \in D_{\text{in}}, \\ 
    \text{reject}, & \text{if } {s}_i \in D_{\text{out}}, 
    \end{cases}
\end{equation}
where $\mathbf{s}_i$ represents a sample in a dataset $\mathcal{D}$, corresponding to a node $v_i$ for node-level tasks or a graph $\mathcal{G}_i$ for graph-level tasks, and $f(\cdot)$ is a classifier that predicts the label for IND samples while rejecting OOD samples.
\end{definition}

\begin{definition}
    \textnormal{(Loss Optimization for GOOD Detection)}
    \textit{The optimization objective for GOOD detection, seeking to balance  accurate classification of IND samples and effective rejection of OOD samples,  can be expressed as:}
    \begin{align}
        \mathcal{L}_{\text{GOOD}} =& \mathbb{E}_{s_i \sim D_{\text{in}}} \left[\ell_{\text{cls}}(f_\theta(\mathcal{D}, s_i), y_i) \right] \\ \nonumber
        +& \lambda \mathbb{E}_{s_i \sim D_{\text{out}}} \left[\ell_{\text{reject}}(f_\theta(\mathcal{D}, s_i)) \right],
    \end{align}
    where $y_i$ is the corresponding label of sample $s_i$ (i.e., node $v_i$ or graph $\mathcal{G}_i$), $\ell_{\text{cls}}(\cdot)$ measures classification error for IND samples using ground-truth labels $y_i$, $\ell_{\text{reject}}(\cdot)$ penalizes incorrect predictions for OOD samples, and $\lambda$ controls the trade-off between two terms.
\end{definition}

% Graph-structured data, represented as $\mathcal{G}=(\mathcal{V}, \mathcal{E}, \mathbf{A}, \mathbf{X})$, plays a pivotal role in many real-world applications. Here, $\mathcal{V}=\{v_1, v_2, \cdots, v_n\}$ denotes the set of vertices, $\mathcal{E}$ is the set of edges, and $\mathbf{A} \in \{0,1\}^{n\times n}$ is the adjacency matrix, where $a_{i,j}$ indicates whether nodes $v_i$ and $v_j$ are connected. The feature matrix $\mathbf{X} \in \mathbb{R}^{n\times d}$ encodes features of nodes (for node-level tasks) or graphs (for graph-level tasks), with $d$ as the feature dimension. 

% \paragraph{Definition 1} (GOOD Detection).
% \textit{GOOD detection focuses on distinguishing in-distribution (IND) data, drawn from a known distribution $D_{\text{in}}$, from out-of-distribution (OOD) data, sampled from an unknown distribution $D_{\text{out}}$. Specifically, the goal is to classify data from $D_{\text{in}}$ while rejecting data from $D_{\text{out}}$. Given a dataset $\mathcal{D}$, the task is defined as follows:}
% \begin{equation}
%     \hat{y}_i = \begin{cases} 
%     f(\mathbf{x}_i, \mathcal{G}), & \text{if } \mathbf{x}_i \in D_{\text{in}}, \\ 
%     \text{reject}, & \text{if } \mathbf{x}_i \in D_{\text{out}}, 
%     \end{cases}
% \end{equation}
% \textit{where $\mathbf{x}_i$ is the feature of a sample, and $f(\cdot)$ is a classifier that predicts the label for IND samples while rejecting OOD samples. Depending on the task granularity:}
% \begin{itemize}
%     \item \textit{In node-level tasks, each sample corresponds to a node $v_i$, and the dataset $\mathcal{D}$ is the collection of all nodes $\mathcal{V}$ within a single graph.}
%     \item \textit{In graph-level tasks, $\mathbf{x}_i$ represents the feature vector of a graph $\mathcal{G}_i$, and the dataset $\mathcal{D}$ is a collection of multiple graphs.} 
% \end{itemize}

% \paragraph{Definition 2}(Loss Optimization for GOOD Detection).
% \textit{The optimization objective for GOOD detection seeks to balance two goals: accurate classification of IND samples and effective rejection of OOD samples. The overall loss can be expressed as:}
% \begin{align}
%     \mathcal{L}_{\text{GOOD}} =& \mathbb{E}_{(\mathbf{x}_i, \mathcal{G}) \sim D_{\text{in}}} \left[\ell(f_\theta(\mathbf{x}_i, \mathcal{G}), y_i) \right] \\ \nonumber
%     +& \lambda \mathbb{E}_{(\mathbf{x}_i, \mathcal{G}) \sim D_{\text{out}}} \left[\ell_{\text{reject}}(f_\theta(\mathbf{x}_i, \mathcal{G})) \right],
% \end{align}
% \textit{where $y_i$ is its corresponding label. The granularity of $\mathbf{x}_i$ and $y_i$ depends on the task:}
% \begin{itemize}
%     \item \textit{In node-level tasks, $\mathbf{x}_i$ corresponds to a node $v_i$, and $y_i$ is its node-level label.}
%     \item \textit{In graph-level tasks, $\mathbf{x}_i$ represents a graph $\mathcal{G}_i$, and $y_i$ denotes the graph-level label of $\mathcal{G}_i$.}
% \end{itemize}
% \textit{In this formulation, $\mathbf{Y}$ denotes the set of node- or graph-level labels, $\ell$ measures classification error for IND samples using ground-truth labels $\mathbf{Y}$, $\ell_{\text{reject}}$ penalizes incorrect predictions for OOD samples, and $\lambda$ controls the trade-off between these terms.}

\subsection{Taxonomy of Approaches}
\label{taxonomy}
To provide an in-depth understanding of GOOD detection methods, we construct a systematic taxonomy that categorizes existing approaches into four groups, i.e., enhancement-based, reconstruction-based, information propagation-based, and classification-based methods. A more fine-grained taxonomy is presented in Figure~\ref{fig:taxonomy}, with a detailed illustration and discrimination provided as follows.
% As shown in Figure~\ref{fig:taxonomy}, our taxonomy captures the diversity of approaches employed to improve robustness, utilize generative models, leverage information flow, and estimate prediction uncertainty.



\begin{figure*}
\resizebox{\linewidth}{!}{
\centering
\begin{forest}
for tree={
    % font=\scriptsize, 
    grow=east,
    minimum size=1.2em, 
    align=left,
    s sep+=2pt, 
    l sep+=2pt, 
    anchor=west,
    edge={thick}, 
    parent anchor=east,
    child anchor=west,
    rounded corners,
    draw
}
[\parbox{1.5cm}{\centering \textbf{GOOD Detection}}, fill=gray!30
    [\parbox{5.8cm}{\centering Classification-based Methods (\S~\ref{Classification})}, fill=yellow!20
        [\parbox{3.5cm}{\centering Boundary Optimization Classification}, fill=blue!10
            [\parbox{12cm}{EL~\cite{EL}, AAGOD~\cite{guo2023AAGOD}, UGNN~\cite{UGNN}}, fill=blue!10]
        ]
        [\parbox{3.5cm}{\centering Probabilistic Evidence Classification}, fill=blue!10
            [\parbox{9cm}{GKDE~\cite{GKDE}, S-BGCN-T-K~\cite{S-BGCN-T-K}}, fill=blue!10]
        ]
    ]
    [\parbox{5.8cm}{\centering Information Propagation-based Methods (\S~\ref{Propagation})}, fill=yellow!20
        [\parbox{3.5cm}{\centering Uncertainty Propagation}, fill=green!10
            [\parbox{11cm}{\centering GPN~\cite{GPN}, NGC~\cite{NGC}, GNSD~\cite{GNSD}}, fill=green!10]
        ]
        [\parbox{3.5cm}{\centering Energy Propagation}, fill=green!10
            [\parbox{12cm}{\centering GNNSafe~\cite{wu2023gnnsafe}, TopoOOD~\cite{TopoOOD}, NODESAFE~\cite{NODESAFE}, EnergyDef~\cite{EnergyDef2024kdd}, ML-GOOD~\cite{cai2025mlgood}, EDBD~\cite{anonymous2025EDBD}}, fill=green!10]
        ]
        [\parbox{3.5cm}{\centering Feature Propagation}, fill=green!10
            [\parbox{12cm}{\centering OODAT~\cite{song2022OODGAT}, LMN~\cite{LMN}, Relation~\cite{Relation}, Open-WRF~\cite{Open-WRF}, GERDQ~\cite{GERDQ}, GRASP~\cite{GRASP}}, fill=green!10]
        ]
    ]
    [\parbox{5.8cm}{\centering Reconstruction-based Methods (\S~\ref{Reconstruction})}, fill=yellow!20
        [\parbox{13cm}{\centering OpenWGL~\cite{OpenWGL}, OSSC~\cite{OSSC}, PGR-MOOD~\cite{PGR-MOOD}}, fill=red!10]
    ]
    [\parbox{5.8cm}{\centering Enhancement-based Methods (\S~\ref{Enhancement})}, fill=yellow!20
        [\parbox{3.5cm}{\centering Model Enhancement}, fill=orange!20
            [\parbox{12cm}{GraphDE~\cite{li2022GraphDE}, GPN-CE-GD~\cite{GPN-CE-GD}, GOLD~\cite{anonymous2025GOLD}, DeGEM~\cite{anonymous2025DeGEM}}, fill=orange!20]
        ]
        [\parbox{3.5cm}{\centering Data Enhancement}, fill=orange!20
            [\parbox{12cm}{GOOD-D~\cite{GOOD-D}, GOODAT~\cite{wang2024GOODAT}, SMUG~\cite{SMUG}, SGOOD~\cite{SGOOD}, HGOE~\cite{HGOE2024mm}}, fill=orange!20]
        ]
    ]
]
\end{forest}
}
\caption{A taxonomy of GOOD detection approaches.}
\label{fig:taxonomy}
\end{figure*}

\noindent\textbf{Enhancement-based methods} aim to improve the sensitivity of GNN models to OOD samples through both model enhancement and data enhancement strategies, as illustrated in Figure~\ref{fig:enha}. These methods incorporate advanced techniques into the model architecture and training to boost robustness and adaptability to unseen data.
The key technologies employed in these methods include:
\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Data Enhancement}: This approach increases the diversity of training data to improve the model's ability to distinguish between IND and OOD samples, often through structural or feature augmentation.
    % , and few-shot learning.
    % , which increase the variability of the data and help the model generalize better.
    \item \textbf{Model Enhancement}: This approach modifies the GNN architecture or training strategy to improve sensitivity to OOD samples, typically using techniques like regularization, adversarial training, or attention mechanisms.
\end{itemize}


\noindent\textbf{Reconstruction-based methods} detect OOD samples by using generative models to learn the distribution of the training data and reconstruct the data samples. Then, the reconstruction quality can be used to assess the likelihood of each sample belonging to IND or OOD. 
% If a sample can be well reconstructed, it is likely to belong to the IND data; otherwise, it is considered an OOD sample. 
Figure~\ref{fig:reco} illustrates the general pipeline of these methods with three key steps:
\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Reconstruction Model Training}: The generative model (e.g., variational autoencoder or diffusion models) is trained to learn and reconstruct graph distributions. 
    \item \textbf{Data Reconstruction}: The graph samples are reconstructed during testing using diffusive denoising or latent space sampling.
    \item \textbf{Similarity Measurement}: The difference between original and reconstructed samples is measured using metrics such as cosine similarity or Fused Gromov-Wasserstein distance to indicate OOD scores.
\end{itemize}


\noindent\textbf{Information propagation-based methods}
leverage graph structures to aggregate essential information, which captures the pivot characteristics for OOD detection from its neighbors. Figure~\ref{fig3:propagation} illustrates the pipeline of these methods with the following key steps:

\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Information Propagation}: The model uses the message-passing mechanism to propagate information across the graph, such as node features, energy functions, or uncertainty measures. This allows the model to capture the influence of neighboring nodes and identify OOD samples.
    \item \textbf{OOD Decision}: Classify each node as IND or OOD based on propagated information, using threshold or discriminative functions.
\end{itemize}

\noindent\textbf{Classification-based methods} leverage the predictive power of classification models to identify samples that do not conform to the training data distribution. In the context of OOD detection for graph data, classifiers are trained to recognize whether a sample belongs to known categories or unseen classes. These methods typically involve the following key steps, as shown in Figure \ref{fig4:classification}:

\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Feature Representing}: Initially, features representing both the structure and attributes of the graph are extracted. These features are designed specifically to capture the patterns that can distinguish IND samples from potential OOD samples.
    \item \textbf{Model Training}: A classification model is trained using the extracted features and known labels to distinguish among different classes of samples.
    \item \textbf{Uncertainty Quantification}: The uncertainty is estimated according to the prediction of models, e.g., the confidence of predicted probability distributions. %Assign uncertainty estimates to the predictions of models, such as through predictive probability distributions to gauge the model's confidence in its predictions.
\end{itemize}



\section{Enhancement-based Methods}
\label{Enhancement}

\begin{figure}
    \centering
	\includegraphics[width=0.72\linewidth]{img/enhan-based.png}
	\caption{General framework for the enhancement-based methods.}
	\label{fig:enha}
\end{figure}

Enhancement-based methods aim to improve the OOD detection capability of models by enhancing the model architecture and learning pipeline. Specifically, we categorize these methods into two subcategories based on the primary focus of their enhancement strategies: (1) \textit{data enhancement}, which augments the input graph data, and (2) \textit{model enhancement}, which improves the architecture design or training process of the GNN itself to enhance OOD detection performance.

\subsection{Data Enhancement}
Data augmentation methods enhance the diversity and complexity of training data, enabling models to better capture the underlying patterns and structures of IND samples. By learning more generalized representations through data augmentation, the model improves its ability to differentiate between IND and OOD samples.

% GOOD-D~\cite{GOOD-D} generates unperturbed structural views by combining node features and graph topology, enabling the model to learn diverse representations. Unlike methods focused on data perturbation, GOOD-D relies on structural consistency to enhance detection.
% In contrast to structural view generation of GOOD-D, GOODAT~\cite{wang2024GOODAT} further utilizes the information bottleneck theory by focusing on OOD differentiation capabilities during the testing phase.
% SMUG~\cite{SMUG} proposes a graph few-shot learning (GFSL) framework for detecting OOD samples in graph classification tasks. The method introduces a novel ``sand-mixing" strategy, which artificially mixes known categories with simulated OOD samples during meta-training. This strategy allows the model to recognize OOD samples in the absence of real OOD data.
% HGOE~\cite{HGOE2024mm} leverages both cross-domain external outliers and internally generated outliers within IND subgroups to improve the robustness and accuracy of GOOD detection.
% SGOOD~\cite{SGOOD} constructs a substructure supergraph and employs advanced encoding techniques and substructure-preserving transformations to enhance graph-level OOD detection. By using Mahalanobis distance for anomaly scoring, it ensures robust differentiation between IND and OOD samples.
Structural augmentation modifies the graph structure to create diverse training scenarios. For instance, GOOD-D~\cite{GOOD-D} generates unperturbed structural views by combining node features and graph topology, enhancing the model's robustness without altering the original structure. Similarly, SGOOD~\cite{SGOOD} constructs a substructure supergraph with substructure-preserving transformations to improve graph-level OOD detection. Additionally, HGOE~\cite{HGOE2024mm} leverages both external and internal outliers within IND subgroups to improve the robustness and accuracy of OOD detection.

Feature augmentation enhances the node and graph features to improve the model's generalization ability. SMUG~\cite{SMUG} introduces a ``sand-mixing" strategy during meta-training, which combines known categories with simulated OOD samples, enabling the model to recognize OOD data without real examples. Similarly, GOODAT~\cite{wang2024GOODAT} incorporates the information bottleneck theory to enhance OOD differentiation during testing, further improving the model's sensitivity to OOD samples.

% Limitation
\noindent\textbf{Discussion.} 
Data enhancement methods expand the training space and refine the learned representations, thereby improving the model's ability to generalize and detect OOD samples. However, their computational demands and scalability challenges remain critical issues, underscoring the need for efficient and scalable augmentation strategies in future research.

\subsection{Model Enhancement}
Different from data enhancement approaches, model enhancement methods modify GNN architectures or training strategies to improve OOD detection. 

For example, GraphDE~\cite{li2022GraphDE} and GPN-CE-GD~\cite{GPN-CE-GD} both focus on adapting the training process to better handle OOD shifts. GraphDE introduces a unified probabilistic framework leveraging variational inference to model data distribution shifts, enabling dynamic adaptation during training. GPN-CE-GD integrates distance-based regularization into the loss function, clustering OOD nodes in the latent space while improving uncertainty quantification.
In contrast to these methods, GOLD~\cite{anonymous2025GOLD} enhances model robustness using implicit adversarial training. It leverages a generative model to create pseudo-OOD instances, simulating OOD exposure without the need for real OOD data.
% DeGEM~\cite{anonymous2025DeGEM}

% Limitation
\noindent\textbf{Discussion.} 
While model enhancement methods achieve ideal detection performance, their reliance on complex training pipelines and probabilistic modeling may limit scalability and ease of implementation.
How to simplifying these processes while maintaining robustness remains a challenge.

% \subsection{Feature Enhancement}
% Feature enhancement methods refine graph representations by employing graph attention mechanisms or advanced graph convolution techniques. These approaches enable the model to capture richer, more discriminative features, enhancing GOOD detection performance.
% Representative work include:
% SGOOD~\cite{SGOOD} constructs a substructure supergraph, employing advanced encoding techniques and substructure-preserving transformations to enhance graph-level OOD detection. Its use of Mahalanobis distance for anomaly scoring ensures robust differentiation between IND and OOD samples.
% \textbf{SGOOD}~\cite{SGOOD} proposes a graph-level OOD detection method, which improves the recognition of OOD graphs by enhancing the substructure features of the graph. The method first constructs a substructure super graph for each graph, and utilizes a GNN to bilaterally encode nodes and substructures in order to learn a rich graph representation. Then, the model's recognition of the substructure patterns of the graph is further enhanced by three substructure-preserving graph enhancement techniques-substructure dropping, super graph sampling, and substructure substitution. SGOOD combines classification loss and contrastive loss for training, aiming to enable the model to improve its accuracy in detecting abnormal graphs while maintaining its ability to classify normal graphs. In the testing phase, SGOOD evaluates the anomaly scores of the test graphs using the Mahalanobis distance, thus effectively distinguishing between IND and OOD samples.


\section{Reconstruction-based Methods}
\label{Reconstruction}

Reconstruction-based methods are based on the idea that generation models trained on IND data often struggle to reconstruct OOD data. The key advantage of these methods lies in using the generative capacity of the models to learn and capture the distributional characteristics of the data.

\begin{figure}
    \centering
	\includegraphics[width=0.96\linewidth]{img/recon-based.png}
	\caption{General framework for the reconstruction-based methods.}
	\label{fig:reco}
\end{figure}

Due to its light-weight architecture, variational graph autoencoder (VGAE) is an effective selection of the generation models. As a representative method, 
OpenWGL~\cite{OpenWGL} employs a constrained VGAE to model node uncertainty. This approach distinguishes itself by introducing label loss and class uncertainty loss, ensuring that the model not only learns the features of IND data but also remains sensitive to the presence of OOD data. The label loss minimizes the cross-entropy loss of labeled data, whereas the class uncertainty loss maximizes entropy to enhance the distinguishability to unseen nodes. %This dual loss mechanism is critical for ensuring accurate OOD detection in real-world settings where new categories may emerge.
In contrast, OSSC~\cite{OSSC} adopts a dynamic variational autoencoder (DVAE) integrated with a graph convolutional network (GCN), enabling the model to handle both structural and temporal changes in graph data. By learning a sequence of potential distributions for each node, OSSC captures the evolution of node attributes and graph topology.

Apart from VGAE, diffusion models are also desirable for graph generation. 
The representative method PGR-MOOD~\cite{PGR-MOOD} utilizes an auxiliary diffusion modeling framework to detect OOD molecules. The detection process compares the similarity between input molecules and their reconstructed counterparts, providing a novel perspective for graph-level OOD detection.


\section{Information Propagation-based Methods}
\label{Propagation}

Message passing is a crucial mechanism for capturing node dependencies by propagating information along the graph structure. In the context of GOOD detection, information propagation-based methods leverage this mechanism to facilitate the communication of relevant structural and feature information, helping distinguish IND from OOD data. 
% to capture node dependencies and graph structure. This enables them to identify OOD instances effectively, as similar nodes behave similarly during message passing, helping distinguish IND from OOD data. 
The information propagation-based methods can be divided into three types: (1) \textit{feature propagation}, (2) \textit{energy propagation}, and (3) \textit{uncertainty propagation}.

\subsection{Feature Propagation}
Feature propagation methods focus on propagating node features through the graph to improve GOOD detection.

As a pioneering work, LMN~\cite{LMN} implements OOD detection in end-to-end semi-supervised node classification. The method introduces latent variables through a variational framework to indicate whether a node is within the distribution, and attenuates the effect of OOD nodes through message passing mechanism.
OODGAT~\cite{song2022OODGAT}, GERDQ~\cite{GERDQ}, and GRASP~\cite{GRASP} all refine feature propagation by considering graph structure and node relationships. OODGAT explicitly separates IND and OOD nodes through an attention mechanism during feature propagation. GERDQ adjusts edge weights via deep Q-learning to mitigate the impact of OOD nodes on the model, while GRASP proposes an edge augmentation strategy to optimize the ratio of intra-edges to inter-edges. 
Relation~\cite{Relation} takes a different approach within this category by constructing a relational graph to capture the complex interactions between nodes, helping to differentiate between IND and OOD data.
Open-WRF~\cite{Open-WRF} is a weakly supervised correlated feedback method that reduces sensitivity to OOD detection thresholds by combining graph neighborhood information to identify OOD categories.

\begin{figure}
    \centering
	\includegraphics[width=0.75\linewidth]{img/prop-based.png}
	\caption{General framework for the information propagation-based methods.}
	\label{fig3:propagation}
    \vspace{-5mm}
\end{figure}

\subsection{Energy Propagation}
Energy propagation methods rely on energy-based models (EBM) to calculate OOD scores, which are then propagated through graph structure to optimize detection performance. 

For instance, GNNSafe~\cite{wu2023gnnsafe} uses an energy function directly extracted from a GNN trained with standard classification loss as the OOD discriminator and applies energy-based belief propagation to improve detection consensus.
Building on GNNSafe, NODESAFE~\cite{NODESAFE} mitigates the impact of extreme fraction values during aggregation, introducing optimization terms to bound negative energy scores and reduce logarithmic bias, thus stabilizing results and enhancing robustness.
In contrast, TopoOOD~\cite{TopoOOD} focuses on neighborhood context, quantifying clutter through k-hop Dirichlet energy. This local perspective provides a confidence score based on topological disorder, complementing GNNSafe’s global approach.
EnergyDef~\cite{EnergyDef2024kdd} leverages EBM within the GNN framework to estimate node densities for OOD detection. By incorporating Langevin dynamics and structural sampling, it enhances detection accuracy under limited annotation scenarios.
% ML-GOOD~\cite{cai2025mlgood}
% EDBD~\cite{anonymous2025EDBD}


\subsection{Uncertainty Propagation}
Uncertainty propagation methods aim to spread prediction uncertainty across the graph structure for GOOD detection.

As a representative method, GPN~\cite{GPN} quantifies prediction uncertainty in semi-supervised node classification by updating predictions using a Bayesian posterior, ensuring reliable uncertainty estimates.
Similarly, NGC~\cite{NGC} leverages graph structure and label propagation to correct noise in IND data and identify OOD samples, offering a unified framework for detecting noisy data in an open-world setting.
Furthermore, GNSD~\cite{GNSD} estimates uncertainty by linking GNNs to stochastic partial differential equations, using drift networks for accurate predictions and stochastic drive networks to model uncertainty propagation across the graph.

\paragraph{Discussion.} Information propagation-based methods assume consistent feature distributions within graph communities, making them effective in homophilic graphs. However, they struggle in heterophilic or noisy graphs, where errors are amplified. This highlights the need for solutions that address heterophily, noise resilience, and dynamic cluster adaptation to improve scalability and robustness.


\section{Classification-based Methods}
\label{Classification}

Classification-based methods use the output of classifiers to identify samples that deviate from the training data distribution. These methods apply statistical techniques to the model prediction to detect errors and OOD samples. These methods can be categorized into two groups: (1) \textit{probabilistic evidence classification} and (2) \textit{boundary optimization classification}.
\begin{figure}
    \centering
	\includegraphics[width=0.75\linewidth]{img/class-based.png}
	\caption{General framework for the classification-based methods.}
	\label{fig4:classification}
\end{figure}

\subsection{Probabilistic Evidence Classification}
This type of methods focuses on quantifying the probabilistic outputs of classification models or leveraging evidence-based probabilistic reasoning to detect OOD samples.

For instance, GKDE~\cite{GKDE} proposes a GNN-based multi-source uncertainty framework to accurately predict node-level Dirichlet distributions and detect OOD nodes by collecting evidence from the labels of training nodes. 
Differently, S-BGCN-T-K~\cite{S-BGCN-T-K} is based on the theory of subjective logic and provides two uncertainty measures, vacuity and dissonance. The uncertainty that the sample belongs to the OOD is measured by parameterizing the model output through the Dirichlet distribution.

\subsection{Boundary Optimization Classification}
This type of methods enhances the ability to distinguish OOD samples via optimizing the decision boundary of classifiers.

AAGOD~\cite{guo2023AAGOD} extends pre-trained GNNs for OOD detection by using an adaptive amplifier generator to augment key patterns in the adjacency matrix, improving the model’s ability to detect OOD instances.
In contrast, EL~\cite{EL} proposes the Encouraging Loss function, rewarding correctly classified samples to refine the decision boundary.
While both methods optimize the boundary to improve OOD detection, AAGOD focuses on data augmentation, whereas EL enhances the training process through loss function modification.
UGNN~\cite{UGNN} constructs semantic prototypes of known and unknown classes using unlabeled data from the subgraph perspective and uses a posteriori prototype assignments inferred by Sinkhorn-Knopp algorithm.




\label{datasets}

\begin{table}
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \begin{tabular}{cp{2cm}ccc}
        \toprule
         Types & Datasets & \# Nodes & \# Edges & \# Classes \\
        % \textbf{Types}   & \textbf{Datasets}         & \textbf{\# Nodes} & \textbf{\# Edges}   & \textbf{\# Classes} \\ 
        \midrule
        \multirow{5}{*}{\parbox{2.5cm}{Citation Networks}}          
        & Cora~\shortcite{cora}             & 2,708   & 5,429      & 7          \\
        & CoraML~\shortcite{pubmed}        & 2,995   & 8,416      & 7          \\
        & Citeseer~\shortcite{pubmed}       & 4,230   & 10,674     & 6          \\
        & PubMed  ~\shortcite{pubmed}       & 19,717  & 88,648     & 3          \\
        & DBLP~\shortcite{DBLP}            & 40,672   & 288,270    & 137        \\ \midrule
        \multirow{4}{*}{\parbox{2cm}{Co-Purchase Datasets} }
        & Amazon-Computers~\shortcite{shchur2018pitfalls} & 13,752 & 491,722 & 10    \\
        & Amazon-Photo~\shortcite{shchur2018pitfalls}    & 7,650  & 238,162 & 8      \\
        & Amazon-Clothing~\shortcite{mcauley2015inferring} & 24,919 & 91,680  & 77     \\
        & Amazon-Electronics~\shortcite{mcauley2015inferring}  & 42,318  & 43,556 & 167\\
        \midrule
        \multirow{2}{*}{\parbox{2cm}{Coauthor Datasets}}          
        & Coauthor-CS~\shortcite{shchur2018pitfalls}   & 18,333   & 163,788    & 15         \\
        & Coauthor-Physics~\shortcite{shchur2018pitfalls} & 34,493   & 495,924    & 5          \\ \midrule
        
        \multirow{1}{*}{\parbox{2.2cm}{Social Networks}}
        & \cellcolor[HTML]{F1F1C1}Twitch~\shortcite{rozemberczki2021twitch}  &\cellcolor[HTML]{F1F1C1}1,912-9,498  &\cellcolor[HTML]{F1F1C1}31,299-153,138 &\cellcolor[HTML]{F1F1C1}2 \\
        \midrule
        \multirow{3}{*}{\parbox{2cm}{Large Datasets}}             
        & Arxiv~\shortcite{OGB}     & 169,343  & 2,315,598  & 40         \\
        & Proteins~\shortcite{OGB}   & 132,534  & 21,446,852 & 112        \\ 
        & Reddit~\shortcite{reddit} & 232,965  & 114,615,892 & 41\\
        \bottomrule
    \end{tabular}
    }
    \caption{Single-domain datasets for node-level GOOD detection (highlighted multi-graph datasets).}
    \label{tb:single-data}
\end{table}

\section{Benchmark Datasets}

This section summarizes the commonly used datasets and standard benchmark for fair evaluation and comparison of existing GOOD detection approaches.

\subsection{Datasets for GOOD Detection}



% the classification criteria and applicable scenarios for both \textit{node-level} and \textit{graph-level} tasks using single-domain and cross-domain datasets.

\paragraph{Node-Level Tasks.}
Node-level GOOD detection focuses on identifying unknown nodes within a graph, with commonly used datasets presented in Table~\ref{tb:single-data}. The definition of OOD samples usually follows these strategies:
\begin{itemize}[leftmargin=0.3cm]
    \item \textbf{Class-based partitioning}, where nodes from specific classes are treated as OOD samples, e.g. in reference networks we can specify specific categories as IND/OOD data.
    \item \textbf{Domain-based partitioning,} where OOD nodes can be identified using domain information (e.g., time or region). Multi-graph datasets like Twitch, which divide data by country or language, exhibit domain shifts that can define OOD nodes based on varying interaction patterns.
    \item \textbf{Structure/Feature perturbation} that generates OOD nodes by perturbing the graph structure, features, or labels, making them significantly different from IND nodes.
\end{itemize}

\paragraph{Graph-Level Tasks.}
Graph-level GOOD detection aims to identify graphs that differ significantly from the training distribution. OOD partitioning is usually based on cross-domain dataset pair partitioning. As shown in Table~\ref{tb:cross-data}, two datasets that belong to the same field and have moderate domain shifts form a dataset pair for evaluation.


\begin{table}[]
    \centering
    \small
    \begin{tabular}{lll}
        \toprule
        Types & IND Datasets & OOD Datasets
        \\ \midrule
        \multirow{6}{*}{Molecule Datasets} & BZR          & COX2         \\
                                           & PTC-MR       & MUTAG        \\
                                           & AIDS         & DHFR         \\
                                           & Tox21        & SIDER        \\
                                           & FreeSolv     & ToxCast      \\
                                           & BBBP         & BACE         \\  \midrule
        \multirow{3}{*}{Bioinformatics Datasets}  & ENZYMES      & PROTEIN  \\
        & ClinTox      & LIPO         \\ 
        & \cellcolor[HTML]{C4EEEF}HumLoc      & \cellcolor[HTML]{C4EEEF}EukLoc       \\
        \midrule
        \multirow{2}{*}{Social Networks}                    
        & IMDB-M       & IMDB-B       \\
        & Esol         & MUV          \\ \bottomrule
    \end{tabular}
    \caption{Cross-domain dataset pairs for node-level and graph-level GOOD detection (blue highlighted node-level datasets).}
    \label{tb:cross-data}
\end{table}

% \subsection{Splits}

% \subsection{Evaluation Metric}

% Commonly used evaluation indexes for OOD detection are FPR, AUROC, AUPR.
% \paragraph{False Positive Rate (FPR).} The FPR is the proportion of negative instances that are incorrectly classified as positive. Specifically, FPR95 refers to the FPR at the point where the True Positive Rate (TPR) is at 95\%. The TPR, also known as recall or sensitivity, is the proportion of actual positives that are correctly identified.
% \begin{equation}
%     FPR  = \frac{FP}{FP+TN},
% \end{equation}
% where $FP$ is the number of false positives, $TN$ is the number of true negatives.

% \paragraph{Area Under the Receiver Operating Characteristic Curve (AUROC).} The AUROC is a performance measurement for the ability of a binary classifier to distinguish between classes. The ROC curve is created by plotting the true positive rate against the false positive rate at various threshold settings, and the AUROC represents the classifier's ability to discriminate between the two classes.
% The AUROC is typically calculated using numerical integration or trapezoidal rule.

% \paragraph{Area Under the Precision-Recall Curve (AUPR).} The AUPR is a performance metric that is particularly useful when dealing with imbalanced datasets where positive instances are rare. Precision is the ratio of true positives to the total predicted positives, while recall (or sensitivity) is the ratio of true positives to the total actual positives.
% Like AUROC, the AUPR is computed using numerical methods, typically by calculating the area under the curve generated by plotting precision against recall at different threshold levels.

% \subsection{Evaluation Metric}
% Common evaluation metrics for GOOD detection include False Positive Rate (FPR), Area Under the Receiver Operating Characteristic Curve (AUROC), and Area Under the Precision-Recall Curve (AUPR).

% \paragraph{False Positive Rate (FPR).} 
% FPR measures the proportion of negative instances incorrectly classified as positive. FPR95 is the FPR when the True Positive Rate (TPR) reaches 95\%. It is calculated as:
% % The False Positive Rate (FPR) measures the proportion of negative instances incorrectly classified as positive. Specifically, FPR95 refers to the FPR at the point where the True Positive Rate (TPR), also known as recall or sensitivity, reaches 95\%. The FPR is calculated as:
% \begin{equation}
%     FPR  = \frac{FP}{FP+TN},
% \end{equation}
% where $FP$ is the number of false positives and $TN$ is the number of true negatives.
% % A lower FPR indicates better performance, as fewer negative samples are incorrectly classified as positive.
% Lower FPR indicates better performance.

% \paragraph{Area Under the Receiver Operating Characteristic Curve (AUROC).} 
% AUROC evaluates a ability of classifier to distinguish between classes. It is calculated by plotting the True Positive Rate (TPR) against the False Positive Rate (FPR) across various threshold settings. The area under the curve (AUC) represents the classifier's discrimination power, with a higher AUC indicating better performance in distinguishing between IND and OOD data.

% \paragraph{Area Under the Precision-Recall Curve (AUPR).} 
% % The AUPR is particularly useful in imbalanced datasets where positive instances (typically OOD samples) are rare. Precision is the ratio of true positives to the total predicted positives, and recall (or sensitivity) is the ratio of true positives to the total actual positives. AUPR is computed by plotting precision against recall at various threshold levels and calculating the area under this curve. It provides a more informative evaluation when the dataset has a high class imbalance, as is common in OOD detection tasks.
% AUPR is useful for imbalanced datasets, where OOD samples are rare. It is calculated by plotting precision vs. recall, and a higher AUPR indicates better performance, especially in class-imbalanced tasks.


\subsection{Standard Benchmarks}

Apart from separate datasets, there are two well-established standard benchmarks for the evaluation of GOOD detection. 

DrugOOD benchmark~\cite{ji2023drugood} focuses on the OOD problem in AI-assisted drug discovery, which arises when data distributions differ between training and testing. It provides a platform for OOD dataset curation and evaluation, with a focus on drug-target binding affinity prediction.
% The open-source Python package automates this process, enabling researchers to customize datasets according to their specific needs. 
UB-GOLD benchmark~\cite{wang2024unifying} bridges the gap between graph-level anomaly detection and graph-level OOD detection, both of which are integral to GOOD detection tasks. UB-GOLD offers a unified evaluation framework, enabling comparisons across these closely related domains and providing valuable insights into the performance of methods in detecting graph-level OOD samples.
% \begin{itemize}
%     \item Unified Evaluation: The benchmark unifies GLAD and GLOD under the concept of generalized graph-level OOD detection, offering a holistic approach to assessing model performance.
%     \item Datasets: UB-GOLD includes 35 datasets spanning four practical anomaly and OOD detection scenarios, providing a diverse set of challenges for evaluation.
%     \item Methods: The benchmark facilitates the comparison of 16 representative GLAD/GLOD methods, offering a broad perspective on the current state-of-the-art.
% \end{itemize}


% 泛化
% GOOD introduces a systematic benchmark tailored for graph OOD method evaluation. This benchmark addresses the gap in the existing literature by providing a comprehensive suite of datasets and metrics designed to assess the generalization capabilities of GNNs across different distribution shifts.

% \begin{itemize}
%     \item Datasets: GOOD includes 11 datasets with 17 domain selections, encompassing a variety of graph prediction tasks, such as graph classification, graph regression, and node classification.
%     \item Splits: The benchmark features 51 different splits when combined with covariate, concept, and no shifts, allowing for a nuanced evaluation of model performance under various distributional changes.
%     \item Performance Metrics: GOOD utilizes standard metrics such as accuracy, AUROC, and AUPR to measure model effectiveness in both IND and OOD settings.
% \end{itemize}


% The benchmark presented in \cite{wang2024unifying} aims to bridge the gap between graph-level anomaly detection (GLAD) and graph-level OOD detection (GLOD). UB-GOLD provides a unified framework to evaluate methods across these two domains.
% \begin{itemize}
%     \item Unified Evaluation: The benchmark unifies GLAD and GLOD under the concept of generalized graph-level OOD detection, offering a holistic approach to assessing model performance.
%     \item Datasets: UB-GOLD includes thirty-five datasets spanning four practical anomaly and OOD detection scenarios, providing a diverse set of challenges for evaluation.
%     \item Methods: The benchmark facilitates the comparison of 16 representative GLAD/GLOD methods, offering a broad perspective on the current state-of-the-art.
% \end{itemize}


% \begin{table*}
% \centering
% % \small
% \resizebox{\linewidth}{!}{
% \begin{tabular}{cccccccl}
% \toprule
% \textbf{Methods} & \textbf{Category}  & \textbf{Venue}  & \textbf{Time} 
% &  \textbf{Evaluation Metrics}  &  \textbf{Key Words}  & \textbf{Task Type} & \textbf{Code Links}
% \\ \midrule
% GKDE             & Classification    & NeurIPS                      & 2020          
% &AUROC, AUPR & Dirichlet Distribution & Node-Level & https://github.com/zxj32/uncertainty-GNN\\
% OpenWGL          & Reconstruction    & ICDM                         & 2020   
% &ACC, F1 & Graph Autoencoder  & Node-Level & NA\\
% GPN              & Propagation       & NeurIPS                      & 2021       
% &ACC, AUROC & Bayesian Posterior  & Node-Level & https://github.com/stadlmax/Graph-Posterior-Network\\
% NGC              & Propagation     & ICCV                          & 2021    
% & ACC & Contrastive Leaning  & Graph-Level & NA  \\
% S-BGCN-T-K       & Classification    & CIKM                         & 2021     
% & AUROC, AUPR  & Knowledge Distillation  & Node-Level & NA   \\
% OODGAT           & Propagation       & KDD                          & 2022 
% &ACC, AUROC, FPR95, F1  & Semi-Supervised Learning   & Node-Level & https://github.com/SongYYYY/KDD22-OODGAT  \\
% LMN              & Propagation       & IJCAI                        & 2022 
% &ACC & Semi-Supervised Learning  & Node-Level & NA       \\
% GraphDE          & Enhancement       & NeurIPS                      & 2022   
% &AUROC, AUPR, FPR95 & Generative Model & Graph-Level & https://github.com/Emiyalzn/GraphDE    \\
% OSSC             & Reconstruction    & ICDM                         & 2022  
% &ACC, AUC, F1 & Graph Autoencoder & Node-Level & https://github.com/qinzhang11/OSSC\\
% EL               & Classification    & AAAI                         & 2022     
% &ACC & Self-Supervised Learning & Node\&Graph-Level & https://github.com/lancopku/well-classified-examples-are-underestimated\\
% AAGOD            & Classification    & KDD                          & 2023   
% &AUC, AUPR, FPR95& Self-Supervised Learning & Graph-Level & https://github.com/BUPT-GAMMA/AAGOD\\
% GNNSafe          & Propagation       & ICLR                         & 2023 
% &ACC, AUROC, AUPR, FPR95 & Energy-Based Model & Node-Level & https://github.com/qitianwu/GraphOOD-GNNSafe \\
% GOOD-D           & Enhancement       & WSDM                         & 2023     
% & AUROC & Contrastive Leaning & Graph-Level & https://github.com/yixinliu233/G-OOD-D\\
% Relation         & Propagation       & NeurIPS                      & 2023     
% &AP, TNR95&  Semi-Supervised Learning & Graph-Level & https://github.com/snu-mllab/Neural-Relation-Graph\\
% GPN-CE-GD        & Propagation       & NeurIPS                      & 2023   
% &ACC, AUROC, AUPR & Bayesian Posterior & Node-Level & https://github.com/neoques/Graph-Posterior-Network\\
% Open-WRF         & Propagation       & IJCNN                        & 2023    
% &ACC, AUROC & Weakly-Supervised Learning & Node\&Graph-Level & https://github.com/Bobowner/Open-World-LGL\\
% UGNN             & Classification    & TKDE                         & 2023     
% &ACC & Semi-Supervised Learning & Graph-Level & NA\\
% GERDQ            & Propagation       & CIKM                         & 2023  
% &ACC, ECE & Deep Q-Learning & Node-Level & https://github.com/DamoSWL/Calibration-of-GNN-with-OOD-nodes\\
% SGOOD            & Enhancement       & CIKM                         & 2024        
% &AUROC, AUPR, FPR95 & Self-Supervised Learning & Graph-Level & https://github.com/TommyDzh/SGOOD \\
% GOODAT           & Enhancement       & AAAI                         & 2024      
% &AUROC & Information Bottleneck & Graph-Level & NA\\
% TopoOOD          & Propagation       & ICML                         & 2024  
% &ACC, AUROC, AUPR, FPR95 & Dirichlet Energy & Node-Level & NA\\
% NODESAFE         & Propagation       & ICML                         & 2024      
% &ACC, AUROC, AUPR, FPR95 & Energy-Based Model & Node-Level  & https://github.com/ShenzhiYang2000/NODESAFE\\
% GNSD             & Propagation       & ICML                         & 2024      
% &ACC, AUROC, AUPR, FPR95 & Diffusion Model & Node-Level & NA\\
% PGR-MOOD         & Reconstruction    & KDD                          & 2024         
% &AUROC, AUPR, FPR95& Diffusion Model & Graph-Level & https://github.com/se7esx/PGR-MOOD\\
% EnergyDef        & Propagation       & KDD                          & 2024     
% &AUROC, AUPR, FPR95 & Energy-Based Model & Node-Level & https://github.com/KellyGong/EnergyDef\\
% SMUG             & Enhancement       & WWW                          & 2024   
% &AUROC & Few-Shot Learning & Node-Level & NA\\
% HGOE             & Enhancement       & MM                           & 2024     
% &AUC & Outlier Exposure & Graph-Level & NA\\ 
% HGIF             & Enhancement       & MM                           & 2024  
% &AUC, F1 & Contrastive Leaning & Node-Level & https://github.com/Ling-Fei-Ren/HGIF\\ 
% GRASP            & Propagation       & NeurIPS                      & 2024 
% &AUROC, FPR95    & Graph Augmentation   & Node-Level & https://github.com/longfei-ma/GRASP\\
% ML-GOOD          & Propagation       & AAAI                     & 2025 
% &ACC, AUROC, AUPR, FPR95    & Energy-Based Model & Node-Level  & https://github.com/ca1man-2022/ML-GOOD\\
% GOLD            & Enhancement       & ICLR                      & 2025 
% &AUROC, FPR95    & Generative Model & Node-Level  & NA\\
% DeGEM            & Enhancement       & ICLR                       & 2025 
% &AUROC, FPR95    & Energy-Based Model  & Node-Level & NA\\
% EDBD           & Propagation       & ICLR                       & 2025 
% &AUROC, FPR95    & Energy-Based Model & Node-Level  & NA\\
% \bottomrule
% \end{tabular}
% }
% \caption{A summary of GOOD detection approaches.}
% \label{tb:summary}
% \end{table*}


\section{Discussions and Future Directions}
\label{Discussion}

In this section, we provide a comprehensive and insightful discussion of GOOD detection problem. We begin by distinguishing GOOD detection from related tasks like anomaly and outlier detection (\S~\ref{distinguishing}) and then exploring its connections to GOOD generalization (\S~\ref{GOOD-G}). Practical applications and theoretical foundations are examined to highlight the interplay between real-world utility and robust theory (\S~\ref{practical}). Finally, we propose promising directions for future research, including graph foundation models with OOD awareness, to guide advancements in this field (\S~\ref{future}).


\subsection{GOOD Detection v.s. Graph Anomaly and Outlier Detection}
\label{distinguishing}
Graph OOD (GOOD) detection shares common goals with graph anomaly detection (GAD)~\cite{wang2024unifying} and outlier detection (OD)~\cite{yang2024survey}, specifically identifying samples that deviate from the common distribution. However, their focus differs, making it important to distinguish them for clearer definition of GOOD detection.

% Both GAD and OD focus on identifying data points that exhibit abnormal behavior or deviate significantly from typical patterns. In GAD, the focus is often on detecting anomalous nodes, edges, or subgraphs within a graph, typically based on features or structural irregularities. On the other hand, OD is concerned with finding outliers in any data distribution, whether graph-based or not. Despite these differences, GOOD detection shares the objective of identifying when a graph or its components deviate from the expected distribution.

% The key distinction lies in \textbf{how these methods define the distributional shift}. In OOD detection, the shift refers to a mismatch between the distribution of the test data and the training data—specifically when the model encounters data from an entirely different distribution, which can be semantic (label-based) or structural (graph-based). GAD and OD typically focus on the detection of individual outliers or anomalous patterns, but they do not inherently address the broad distributional shifts that occur in OOD detection.
While GAD aims to detect anomalous nodes, edges, or subgraphs, and OD focuses on outliers in any data distribution (graph-based or not), GOOD detection identifies when a graph or its components deviate from the expected distribution. The key distinction lies in \textbf{how distributional shifts are defined}. In GOOD detection, the shift refers to mismatches between test and training distributions, either semantic (label-based) or structural (graph-based). GAD and OD typically detect outliers or anomalies but do not inherently address the broader distributional shifts central to GOOD detection.

By harmonizing the definitions of anomalies and distributions, the techniques used in GAD and OD can often be adapted for GOOD detection. Several methods~\cite{GDN,GRADATE,BWGNN}, while initially described in the context of GAD, could be classified under GOOD detection due to their objectives and experimental setups.
% ,iGAD}.
% If we harmonize the definitions of anomalies and distributions across these methods, their underlying techniques can often be generalized to each other. For instance, methods developed for GAD and OD can potentially be adapted for GOOD detection, as they also aim to detect discrepancies between test and training distributions. Notably, several methods have been proposed that could be classified under graph OOD detection. These works~\cite{GDN,GRADATE,BWGNN,iGAD,GLocalKD,GLADC} describe their approaches in the context of graph anomaly detection but can be considered as part of the broader GOOD detection category in terms of their objectives and experimental setups.

\subsection{GOOD Detection v.s. GOOD Generalization}
\label{GOOD-G}
GOOD generalization~\cite{gui2022good} focuses on model performance across OOD data, ensuring robustness despite changes in data distribution. While GOOD detection targets \textbf{semantic shifts}, GOOD generalization is concerned with mitigating \textbf{covariate shifts}, adapting to changes in feature distributions while maintaining prediction accuracy.
% GOOD generalization~\cite{gui2022good} focuses on the model's ability to generalize from the training set to the test set, particularly in the presence of OOD data. The aim is to enable the model to maintain prediction accuracy when faced with OOD data. While GOOD detection is closely related to \textbf{semantic shift}, focusing on whether the labels or features of the data align with known semantic distributions, GOOD generalization is more concerned with mitigating \textbf{covariate shift}. This shift addresses the challenge of adapting to changes in feature distributions while ensuring good generalization performance across OOD scenarios.
% Despite these differences, both tasks share the ultimate goal of enhancing the model's robustness to OOD scenarios.

% However, these tasks often present conflicting goals: 
Despite their differences, both tasks aim to improve robustness to OOD scenarios. However, they often present conflicting goals: GOOD detection requires high sensitivity to identify OOD samples accurately, while GOOD generalization focuses on maintaining performance across varying GOOD scenarios, sometimes sacrificing detection accuracy for broader adaptability. To address this conflict, a promising approach is the development of \textbf{graph foundation models with OOD awareness}. Such models would integrate both detection and generalization capabilities, enabling it to dynamically adjust to varying graph distributions while preserving high accuracy in both tasks. This approach may involve multi-task learning frameworks, adaptive training methods, and robust inference strategies that balance the trade-offs between detection and generalization.

\subsection{Practical Applications and Theoretical Foundations}
\label{practical}
GOOD detection goes beyond binary or multi-class classification to tasks like graph classification, node classification, link prediction, graph regression, and community detection.
For example, GRADATE~\cite{GRADATE} applies GOOD detection to graph classification and BWGNN~\cite{BWGNN} enhances link prediction by incorporating GOOD detection to ensure robustness under distribution shifts.
In robust graph representation learning, GOOD detection helps learn stable embeddings despite changes in graph structures or features. Methods like FocusedCleaner~\cite{tkde2024FocusedCleaner} filter adverse factors, which is key for anomaly detection and recommendation systems.

Theoretical foundations are also advancing.
Bazhenov et al.~\cite{bazhenov2024evaluating} propose a framework for inducing distributional shifts in graph learning, highlighting trade-offs between performance and separation ability.
Du et al.~\cite{du2024when} provide a formal insight into how IND labels affect OOD detection, with provable error bounds.
    

\subsection{Future Directions}
\label{future}
% We summarize existing GOOD detection methods in Table~\ref{tb:summary}.
Beyond the development of \textbf{Graph Foundation Models with OOD Awareness} discussed in \S~\ref{GOOD-G}, in this subsection, we highlight more key challenges and promising research directions for advancing GOOD detection.

\noindent\textbf{LLM-Powered GOOD Detection.} Large Language Models (LLMs) offer significant potential for enhancing GOOD detection by combining semantic understanding with graph structure. This integration can improve OOD-aware embeddings and knowledge graph completion, especially in multi-modal or heterogeneous graph settings. The generalizability of LLMs with limited labels opens the door for semi-supervised or few-shot GOOD detection, addressing the challenge of sparse or costly labels in real-world applications.

\noindent\textbf{Explainability.} In GOOD detection, explainability is crucial for trust and accountability, especially in high-risk areas like fraud detection, healthcare, and autonomous driving. An explainable model that highlights the specific graph structures or features driving OOD predictions can increase confidence in its decisions and make it easier for developers to identify and fix issues. Moreover, transparency in decision-making aids in debugging and optimizing the model, ensuring consistent and accurate performance across diverse real-world applications.


\noindent\textbf{Advanced Benchmarks and Metrics.} Current benchmarks focus narrowly on tasks like node or graph classification, often overlooking diverse distributional shifts such as density, locality, or temporal dynamics. Expanding the benchmarks and introducing novel metrics from new perspectives (e.g., robustness under structural perturbations) is crucial for fair and comprehensive evaluation of GOOD detection methods.

\noindent\textbf{Scalability and Complexity.} Scalability remains a major hurdle for many GOOD detection methods, which rely on complex pipelines and probabilistic models. Developing more efficient algorithms, simplified models, and leveraging parallel or distributed computing will be critical to maintaining performance in large-scale, dynamic graph settings.


\section{Conclusion}
\label{Conclusion}

In this paper, we provide a clear definition of GOOD detection, highlighting the distinctions between often-confused tasks. We also categorize and review existing methods, summarizing key experimental setups, such as data partitioning approaches for GOOD detection. Based on this, we discuss the primary challenges in the field and propose future research directions. We hope that our work serves as a solid foundation for future research, encouraging more scholars to tackle the complex challenges in GOOD detection.


% \appendix

% \section*{Acknowledgments}


%% The file named.bst is a bibliography style file for BibTeX 0.99c
\bibliographystyle{named}
\bibliography{main}

\end{document}

