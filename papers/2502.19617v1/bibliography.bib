@String { ar     = {Annual Review of Control, Robot., and Autom. Syst.} }
@String { arso   = {{IEEE} Wksp. on Advanced Robot. and its Social Impacts} }
@String { corl   = {Conf. on Robot Learning} }
@String { human  = {IEEE-RAS Intl. Conf. on Humanoid Robots} }
@String { icra   = {{IEEE} Intl. Conf. Robot. Autom.} }
@String { ijhr   = {Intl. J. of Humanoid Robot.} }
@String { ijra   = {{IEEE} J. Robot. Autom.} }
@String { ijrr   = {Intl. J. of Robotics Research} }
@String { iral   = {{IEEE} Robot. Autom. Letters} }
@String { iros   = {{IEEE/RSJ} Intl. Conf. on Intell. Robots and Syst.} }
@String { isrr   = {Intl. Symp. on Robotics Research} }
@String { jfr    = {J. of Field Robotics} }
@String { nips   = {Advances in Neural Information Processing Systems} }
@String { ram    = {{IEEE} Robot. Autom. Magazine} }
@String { rss    = {Robotics: Science and Syst.} }
@String { rsstmp = {Robotics: Science and Syst. Wksp. on Task and Motion Planning} }
@String { siap   = {{SIAM} J. Appl. Math.} }
@String { springer = {Springer Tracts in Advanced Robot.}}
@String { tro    = {{IEEE} Trans. Robot.} }
@String { troa   = {{IEEE} Trans. Robot. Autom.} }
@String { tsmc   = {{IEEE} Trans. on Syst., Man, and Cybernetics} }
@String { wafr   = {Intl. Wksp. on the Algorithmic Foundations of Robotics} }
@String { icml   = {Intl. Conf. on Machine Learning} }
@String { iclr   = {Intl. Conf. on Learning Representations} }
@String { icaps  = {Intl. Conf. on Automated Planning and Scheduling} }
@String { cvpr  = {{IEEE} Conf. on Computer Vision and Pattern Recognition} }
@String { trm  = {{IEEE/ASME} Trans. on Mechatronics} }
@String { pami  = {{IEEE} Trans. on Pattern Analysis and Machine Intelligence} }
@String { iccv  = {{IEEE/CVF} Intl. Conf. on Computer Vision} }




@inproceedings{Andriluka2014,
  title={2d human pose estimation: New benchmark and state of the art analysis},
  author={Andriluka, Mykhaylo and Pishchulin, Leonid and Gehler, Peter and Schiele, Bernt},
  booktitle=cvpr,
  pages={3686--3693},
  year={2014}
}
@article{Zhang2000,
   abstract = {We propose a flexible new technique to easily calibrate a camera. It only requires the camera to observe a planar pattern shown at a few (at least two) different orientations. Either the camera or the planar pattern can be freely moved. The motion need not be known. Radial lens distortion is modeled. The proposed procedure consists of a closed-form solution, followed by a nonlinear refinement based on the maximum likelihood criterion. Both computer simulation and real data have been used to test the proposed technique and very good results have been obtained. Compared with classical techniques which use expensive equipment such as two or three orthogonal planes, the proposed technique is easy to use and flexible. It advances 3D computer vision one more step from laboratory environments to real world use. The corresponding software is available from the author's Web page. © 2000 IEEE.},
   author = {Zhengyou Zhang},
   doi = {10.1109/34.888718},
   issn = {01628828},
   issue = {11},
   journal = pami,
   keywords = {2d pattern,Absolute conic,Calibration from planes,Camera calibration,Closed-form solution,Flexible plane-based calibration,Flexible setup,Lens distortion,Maximum likelihood estimation,Projective mapping},
   pages = {1330-1334},
   title = {A flexible new technique for camera calibration},
   volume = {22},
   year = {2000},
}
@article{Hutchinson1996,
   abstract = {This article provides a tutorial introduction to visual servo control of robotic manipulators. Since the topic spans many disciplines our goal is limited to providing a basic conceptual framework. We begin by reviewing the prerequisite topics from robotics and computer vision, including a brief review of coordinate transformations, velocity representation, and a description of the geometric aspects of the image formation process. We then present a taxonomy of visual servo control systems. The two major classes of systems, position-based and image-based systems, are then discussed in detail. Since any visual servo system must be capable of tracking image features in a sequence of images, we also include an overview of feature-based and correlation-based methods for tracking. We conclude the tutorial with a number of observations on the current directions of the research field of visual servo control. © 1996 IEEE.},
   author = {Seth Hutchinson and Gregory D. Hager and Peter I. Corke},
   doi = {10.1109/70.538972},
   issn = {1042296X},
   issue = {5},
   journal = tro,
   pages = {651-670},
   title = {A tutorial on visual servo control},
   volume = {12},
   year = {1996},
}
@webpage{3dRecon,
   journal = {OpenCV},
   title = {Camera Calibration and 3D Reconstruction},
   url = {https://docs.opencv.org/4.x/d9/d0c/group__calib3d.html},
}
@inproceedings{Lee2020,
  title={Camera-to-robot pose estimation from a single image},
  author={Lee, Timothy E and Tremblay, Jonathan and To, Thang and Cheng, Jia and Mosier, Terry and Kroemer, Oliver and Fox, Dieter and Birchfield, Stan},
  booktitle=icra,
  pages={9426--9432},
  year={2020}
}
@inproceedings{Toshev2014,
  title={Deeppose: Human pose estimation via deep neural networks},
  author={Toshev, Alexander and Szegedy, Christian},
  booktitle=cvpr,
  pages={1653--1660},
  year={2014}
}

@article{Chang2021,
   abstract = {Camera calibration error, vision latency, nonlinear dynamics, and so on present a major challenge for designing the control scheme for a visual servoing system. Although many approaches on visual servoing have been proposed, surprisingly, only a few of them have taken into account system dynamics in the control design of a visual servoing system. In addition, the depth information of feature points is essential in the image-based visual servoing architecture. As a result, to cope with the aforementioned problems, this article proposes a Kalman filter-based depth and velocity estimator and a modified image-based dynamic visual servoing architecture that takes into consideration the system dynamics in its control design. In particular, the Kalman filter is exploited to deal with the problems caused by vision latency and image noise so as to facilitate the estimation of the joint velocity of the robot using image information only. Moreover, in the modified image-based dynamic visual servoing architecture, the computed torque control scheme is used to compensate for system dynamics and the Kalman filter is used to provide accurate depth information of the feature points. Results of visual servoing experiments conducted on a two-degree of freedom planar robot verify the effectiveness of the proposed approach.},
   author = {Ting Yu Chang and Wei Che Chang and Ming Yang Cheng and Shih Sian Yang},
   doi = {10.1177/17298814211016674},
   issn = {17298814},
   issue = {3},
   journal = {Intl. J. of Advanced Robotic Syst.},
   keywords = {Depth and velocity estimator,Kalman filter,modified image-based dynamic visual servoing,virtual visual servoing},
   publisher = {SAGE Publications Inc.},
   title = {Dynamic visual servoing with Kalman filter-based depth and velocity estimator},
   volume = {18},
   year = {2021},
}
@article{Jagersand1997,
   abstract = {In this paper we present an experimental evaluation of adaptive and non-adaptive visual servoing in 3, 6 and 12 degrees of freedom (DOF), comparing it to traditional joint feedback control. While the purpose of experiments in most other work has been to show that the particular algorithm presented indeed also works in practice, we do not focus on the algorithm, but rather on properties important to visual servoing in general. Our main results are: positioning of a 6 axis PUMA 762 arm is up to 5 times more precise under visual control, than under joint control. Positioning of a Utah/MIT dextrous hand is better under visual control than under joint control by a factor of 2. We also found that a trust-region-based adaptive visual feedback controller is very robust. For m tracked visual features the algorithm can successfully estimate online the m×3 (m≥3) image Jacobian (J) without any prior information, while carrying out a 3 DOF manipulation task. For 6 and higher DOF manipulation, a rough initial estimate of J is beneficial. We also verified that redundant visual information is valuable. Errors due to imprecise tracking and goal specification were reduced as the number of visual features, m, was increased. Furthermore highly redundant systems allow us to detect outliers in the feature vector, and deal with partial occlusion.},
   author = {Martin Jagersand and Olac Fuentes and Randal Nelson},
   doi = {10.1109/robot.1997.606723},
   isbn = {0780336127},
   issn = {10504729},
   issue = {April},
   journal = icra,
   pages = {2874-2880},
   title = {Experimental evaluation of uncalibrated visual servoing for precision manipulation},
   volume = {4},
   year = {1997},
}
@article{Ren2017,
   abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [1] and Fast R-CNN [2] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features - using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model [3], our detection system has a frame rate of 5 fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available.},
   author = {Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun},
   doi = {10.1109/TPAMI.2016.2577031},
   issn = {01628828},
   issue = {6},
   journal = pami,
   keywords = {Object detection,convolutional neural network,region proposal},
   month = {6},
   pages = {1137-1149},
   pmid = {27295650},
   publisher = {IEEE Computer Society},
   title = {Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks},
   volume = {39},
   year = {2017},
}
@web\_page{P2021,
   author = {Alex P},
   journal = {medium.com},
   title = {How to Train a Custom Keypoint Detection Model with PyTorch},
   url = {https://medium.com/@alexppppp/how-to-train-a-custom-keypoint-detection-model-with-pytorch-d9af90e111da},
   year = {2021},
}
@web\_page{Patil2021,
   author = {Chetan Patil and Vikas Gupta},
   journal = {learnopencv.com},
   title = {Human Pose Estimation using Keypoint RCNN in PyTorch},
   url = {https://learnopencv.com/human-pose-estimation-using-keypoint-rcnn-in-pytorch/},
   year = {2021},
}
@inproceedings{Lin2014,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={Computer Vision--ECCV, Zurich, Switzerland, Proceedings, Part V 13},
  pages={740--755},
  year={2014},
  organization={Springer}
}
@inproceedings{he2017,
  title={Mask r-cnn},
  author={He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  booktitle=iccv,
  pages={2961--2969},
  year={2017}
}

@article{Cao2021,
   abstract = {Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.},
   author = {Zhe Cao and Gines Hidalgo and Tomas Simon and Shih En Wei and Yaser Sheikh},
   doi = {10.1109/TPAMI.2019.2929257},
   issn = {19393539},
   issue = {1},
   journal = pami,
   keywords = {2D foot keypoint estimation,2D human pose estimation,multiple person,part affinity fields,real-time},
   month = {1},
   pages = {172-186},
   pmid = {31331883},
   title = {OpenPose: Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields},
   volume = {43},
   year = {2021},
}
@article{Lu2022,
   abstract = {Keypoint detection is an essential building block for many robotic applications like motion capture and pose estimation. Historically, keypoints are detected using uniquely engineered markers such as checkerboards or fiducials. More recently, deep learning methods have been explored as they have the ability to detect user-defined keypoints in a marker-less manner. However, different manually selected keypoints can have uneven performance when it comes to detection and localization. An example of this can be found on symmetric robotic tools where DNN detectors cannot solve the correspondence problem correctly. In this work, we propose a new and autonomous way to define the keypoint locations that overcomes these challenges. The approach involves finding the optimal set of keypoints on robotic manipulators for robust visual detection and localization. Using a robotic simulator as a medium, our algorithm utilizes synthetic data for DNN training, and the proposed algorithm is used to optimize the selection of keypoints through an iterative approach. The results show that when using the optimized keypoints, the detection performance of the DNNs improved significantly. We further use the optimized keypoints for real robotic applications by using domain randomization to bridge the reality gap between the simulator and the physical world. The physical world experiments show how the proposed method can be applied to the wide-breadth of robotic applications that require visual feedback, such as camera-to-robot calibration, robotic tool tracking, and end-effector pose estimation. As a way to encourage further research in this topic, we establish the 'Robot Pose' dataset, comprising calibration and tracking problems and ground truth data, available online 1.},
   author = {Jingpei Lu and Florian Richter and Michael C. Yip},
   doi = {10.1109/LRA.2022.3151981},
   issn = {23773766},
   issue = {2},
   journal = iral,
   keywords = {Visual tracking,deep learning for visual perception,perception for grasping and manipulation},
   month = {4},
   pages = {4622-4629},
   title = {Pose Estimation for Robot Manipulators via Keypoint Optimization and Sim-to-Real Transfer},
   volume = {7},
   year = {2022},
}
@inproceedings{Shademan2010,
  title={Robust jacobian estimation for uncalibrated visual servoing},
  author={Shademan, Azad and Farahmand, Amir-Massoud and J{\"a}gersand, Martin},
  booktitle=icra,
  pages={5564--5569},
  year={2010}
}
@book{Chaumette2016,
   author = {Francois Chaumette and Seth Hutchinson and Peter Corke},
   pages = {841-866},
   title = {Springer Handbook Of Robotics},
   volume = {Springer},
   year = {2016},
}
@inproceedings{Hosoda1994,
   abstract = {In this paper, we propose a-versatile visual servoing control scheme with a Jacobian matrix estimator. The Jacobian matrix estimator does not need a priori knowledge of the kinematic structure and parameters of the robot system, such as camera and link parameters. The proposed visual servoing control scheme ensures the convergence of the image-features to desired trajectories, by using the estimated Jacobian matrix, which is proved by the Lyapunov stability theory. To show the effectiveness of the proposed scheme, simulation and experimental results are presented.},
   author = {Koh Hosoda and Minoru Asada},
   doi = {10.1109/IROS.1994.407392},
   isbn = {0780319338},
   issn = {21530866},
   journal = iros,
   pages = {186-193},
   title = {Versatile visual servoing without knowledge of true Jacobian},
   volume = {1},
   year = {1994},
}
@book{Craig2005,
   author = {John J Craig},
   edition = {3},
   pages = {62-100},
   publisher = {Pearson Education International},
   title = {Introduction to Robotics Mechanics and Control},
   year = {2005},
}
@article{Wang2022,
   abstract = {Head pose estimation (HPE) is a key step in computation and quantification of 3D facial features and has a significant impact on the precision and accuracy of measurements. High-precision HPE is the basis for standardized facial data collection and analysis. The Camper's plane is the standard (baseline) plane commonly used by anthropologists for head and face research, but there is no research on automatic positioning of the Camper's plane using color and depth cameras. This paper presents a high-accuracy method for Camper's plane localization and HPE based on multi-view RGBD depth sensors. The 3D facial point clouds acquired by the multi-view RGBD depth sensors are aligned to obtain a complete 3D face. Keypoint RCNN is used for facial keypoint detection to obtain facial landmarks. A method is proposed to build a general face datum model based on a self-built dataset. The head pose is estimated by applying rigid body transformation to an individual 3D face and the general 3D face model. In order to verify the accuracy of Camper's plane localization and HPE, 102 cases of 3D facial data and experiments were collected and conducted. The tragus and nasal alar points are localized to within 7 pixels (about 0.83 cm) and the average accuracy of the three dimensions of Camper's plane identified is 0.87°, 0.64° and 0.47° respectively. The average accuracies of the three dimensions of HPE were 1.17°, 0.90° and 0.97. The experiment results demonstrate the effectiveness of the method for Camper's plane localization and HPE.},
   author = {Huaqiang Wang and Lu Huang and Kang Yu and Tingting Song and Fengen Yuan and Hao Yang and Haiying Zhang},
   doi = {10.1109/ACCESS.2022.3227572},
   issn = {21693536},
   journal = {IEEE Access},
   keywords = {3-D point cloud,Head pose estimation,camper's plane,depth sensor,multi-view},
   pages = {131722-131734},
   title = {Camper's Plane Localization and Head Pose Estimation Based on Multi-View RGBD Sensors},
   volume = {10},
   year = {2022}
}
@article{Deng2022,
   abstract = {The wide application of industrial robots has greatly improved assembly efficiency and reliability. However, determining how to efficiently teach a robot to perform assembly manipulation trajectories from demonstration videos is a challenging issue. This paper proposes a method integrating deep learning, image processing, and an iteration model to predict the real assembly manipulation trajectory of a human hand from a video without specific depth information. First, a pose estimation network, Keypoint-RCNN, is used to accurately estimate hand pose in the two-dimensional (2-D) image of each frame in a video. Second, image processing is applied to map the 2-D hand pose estimated by the neural network with the real 3-D assembly space. An iteration model based on the trust region algorithm is proposed to solve for the quaternions and translation vectors of two frames. All the quaternions and translation vectors form the predicted assembly manipulation trajectories. Finally, a UR3 robot is used to imitate the assembly operation based on the predicted manipulation trajectories. The results show that the robot could successfully imitate various operations based on the predicted manipulation trajectories.},
   author = {Xinjian Deng and Jianhua Liu and Honghui Gong and Hao Gong and Jiayu Huang},
   doi = {10.1109/TII.2022.3224966},
   issn = {19410050},
   journal = {{IEEE} Trans. on Industrial Informatics},
   keywords = {Industrial robot,image processing,intelligent assembly,learning from demonstration},
   title = {A Human-Robot Collaboration Method Using a Pose Estimation Network for Robot Learning of Assembly Manipulation Trajectories from Demonstration Videos},
   year = {2022}
}
@article{Lai2020,
abstract = {In soft robotics, developing an effective way of robot-environment interaction is a challenging task due to the soft nature of the material that makes the manipulator. This paper demonstrates a vision-based approach to configure a two-segment soft continuum robot manipulator into an user-defined configuration and interact with unknown objects on plane. The soft robot manipulator actuated by cable-driven mechanism, is composed of two cascade continuum segments which are made from poly-dimethyl-siloxane (PDMS). The overall robot configuration can be determined in a point-wise manner on image plane provided by an eye-to-hand system. One can define the end-effectors' location on the visual system to re-shape the manipulator. The visual servoing fashion allows the robot to optimize its posture to its best fit without developing any complicated model. Experiments on prototype indicate that the proposed model-free approach can be well employed, even when the manipulator is bearing a payload. By adaptively adjusting manipulator's stiffness to a quasi-deadlock status, the payload capacity is up to nearly 6 times of the manipulator's mass itself.},
author = {Lai, Jiewen and Huang, Kaicheng and Lu, Bo and Chu, Henry K.},
doi = {10.1109/AIM43001.2020.9158975},
file = {:home/abhinav/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lai et al. - 2020 - Toward vision-based adaptive configuring of a bidirectional two-segment soft continuum manipulator.pdf:pdf},
isbn = {9781728167947},
journal = {{IEEE/ASME} Intl. Conf. on Adv. Intell. Mechatronics, AIM},
mendeley-groups = {Control/VBC},
pages = {934--939},
title = {{Toward vision-based adaptive configuring of a bidirectional two-segment soft continuum manipulator}},
volume = {July},
year = {2020}
}
@inproceedings{Calli2016,
abstract = {In this paper, a method is proposed for visionbased within-hand precision manipulation with underactuated grippers. The method combines the advantages of adaptive underactuation with the robustness of visual servoing algorithms by employing simple action sets in actuator space, called precision manipulation primitives (PMPs). It is shown that, with this approach, reliable precision manipulation is possible even without joint and force sensors by using only minimal gripper kinematics information. An adaptation method is also utilized in the vision loop to enhance the system's transient performance. The proposed methods are analyzed with experiments using various target objects and reference signals. The results indicate that underactuated hands, even with minimalistic sensing and control via visual servoing, can provide a simple and inexpensive solution to allow low-fidelity precision manipulation.},
author = {Calli, Berk and Dollar, Aaron M.},
booktitle = iros,
doi = {10.1109/IROS.2016.7759173},
file = {:home/abhinav/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Calli, Dollar - 2016 - Vision-based precision manipulation with underactuated hands Simple and effective solutions for dexterity.pdf:pdf},
isbn = {9781509037629},
issn = {21530866},
mendeley-groups = {Control/VBC},
pages = {1012--1018},
title = {{Vision-based precision manipulation with underactuated hands: Simple and effective solutions for dexterity}},
url = {},
volume = {Nov},
year = {2016}
}
@Inbook{Imambi2021,
author="Imambi, Sagar
and Prakash, Kolla Bhanu
and Kanagachidambaresan, G. R.",
editor="Prakash, Kolla Bhanu
and Kanagachidambaresan, G. R.",
title="PyTorch",
bookTitle="Programming with TensorFlow: Solution for Edge Computing Applications",
year="2021",
publisher="Springer International Publishing",
address="Cham",
pages="87--104",
abstract="PyTorch is a library for Python programs that encourages deep learning programs. With this receptiveness and convenience found in (Deep Learning for Computer Vision: Expert techniques to train advanced neural networks using TensorFlow and Keras. [Authors: RajalingappaaShanmugamani]), PyTorch makes it useful in developing deep neural networks. It has an expansive scope and is applied for various applications. As Python is for programming, PyTorch is both a magnificent prologue to profound learning just as an instrument usable in proficient real-world applications.",
isbn="978-3-030-57077-4",
doi="10.1007/978-3-030-57077-4_10"
}
@book{ayyadevara2020modern,
  title={Modern Computer Vision with PyTorch: Explore deep learning concepts and implement over 50 real-world image applications},
  author={Ayyadevara, V.K. and Reddy, Y.},
  isbn={9781839216534},
  year={2020},
  publisher={Packt Publishing}
}
  @inproceedings{guler2018densepose,
  title={Densepose: Dense human pose estimation in the wild},
  author={ G{\"u}ler, R{\i}za Alp and Neverova, Natalia and Kokkinos, Iasonas},
  booktitle=cvpr,
  pages={7297--7306},
  year={2018}
}
@inproceedings{cuevas2018hybrid,
  title={Hybrid multi-camera visual servoing to moving target},
  author={Cuevas-Velasquez, Hanz and Li, Nanbo and Tylecek, Radim and Saval-Calvo, Marcelo and Fisher, Robert B},
  booktitle=iros,
  pages={1132--1137},
  year={2018}
}
@inproceedings{ardon2018reaching,
  title={Reaching and grasping of objects by humanoid robots through visual servoing},
  author={Ard{\'o}n, Paola and Dragone, Mauro and Erden, Mustafa Suphi},
  booktitle={Haptics: Science, Technology, and Applications: 11th Intl. Conf., EuroHaptics, Proceedings, Part II 11},
  pages={353--365},
  year={2018},
  organization={Springer}
}
@InProceedings{Suvorov_2022_WACV,
    author    = {Suvorov, Roman and Logacheva, Elizaveta and Mashikhin, Anton and Remizova, Anastasia and Ashukha, Arsenii and Silvestrov, Aleksei and Kong, Naejin and Goka, Harshith and Park, Kiwoong and Lempitsky, Victor},
    title     = {Resolution-Robust Large Mask Inpainting With Fourier Convolutions},
    booktitle = {{IEEE/CVF} Winter Conf. on Applications of Computer Vision},
    month     = {January},
    year      = {2022},
    pages     = {2149-2159}
}
@article{wu2019detectron2,
  title={Detectron2},
  author={Wu, Yuxin and Kirillov, Alexander and Massa, Francisco and Lo, Wan-Yen and Girshick, Ross},
  year={2019}
}
@InProceedings{Romero_2022_CVPR,
    author    = {Romero, Andr\'es and Castillo, Angela and Abril-Nova, Jose and Timofte, Radu and Das, Ritwik and Hira, Sanchit and Pan, Zhihong and Zhang, Min and Li, Baopu and He, Dongliang and Lin, Tianwei and Li, Fu and Wu, Chengyue and Liu, Xianming and Wang, Xinying and Yu, Yi and Yang, Jie and Li, Rengang and Zhao, Yaqian and Guo, Zhenhua and Fan, Baoyu and Li, Xiaochuan and Zhang, Runze and Lu, Zeyu and Huang, Junqin and Wu, Gang and Jiang, Junjun and Cai, Jiayin and Li, Changlin and Tao, Xin and Tai, Yu-Wing and Zhou, Xiaoqiang and Huang, Huaibo},
    title     = {NTIRE 2022 Image Inpainting Challenge: Report},
    booktitle = cvpr,
    month     = {June},
    year      = {2022},
    pages     = {1150-1182}
}
@article{zhao2021large,
  title={Large scale image completion via co-modulated generative adversarial networks},
  author={Zhao, Shengyu and Cui, Jonathan and Sheng, Yilun and Dong, Yue and Liang, Xiao and Chang, Eric I and Xu, Yan},
  journal={arXiv preprint arXiv:2103.10428},
  year={2021}
}
@inproceedings{patzold2022online,
  title={Online marker-free extrinsic camera calibration using person keypoint detections},
  author={P{\"a}tzold, Bastian and Bultmann, Simon and Behnke, Sven},
  booktitle={DAGM German Conf. on Pattern Recognition},
  pages={300--316},
  year={2022},
  organization={Springer}
}
@inproceedings{bohg2014robot,
  title={Robot arm pose estimation through pixel-wise part classification},
  author={Bohg, Jeannette and Romero, Javier and Herzog, Alexander and Schaal, Stefan},
  booktitle=icra,
  pages={3143--3150},
  year={2014}
}
@inproceedings{chatterjee2023keypoints,
  title={Keypoints-Based Adaptive Visual Servoing for Control of Robotic Manipulators in Configuration Space},
  author={Chatterjee, Sreejani and Karade, Abhay C and Gandhi, Abhinav and Calli, Berk},
  booktitle=iros,
  pages={6387--6394},
  year={2023}
}
@inproceedings{lambrecht2021optimizing,
  title={Optimizing keypoint-based single-shot camera-to-robot pose estimation through shape segmentation},
  author={Lambrecht, Jens and Grosenick, Philipp and Meusel, Marvin},
  booktitle=icra,
  pages={13843--13849},
  year={2021}
}
@article{liang2019vision,
  title={A vision-based marker-less pose estimation system for articulated construction robots},
  author={Liang, Ci-Jyun and Lundeen, Kurt M and McGee, Wes and Menassa, Carol C and Lee, SangHyun and Kamat, Vineet R},
  journal={Autom. in Construction},
  volume={104},
  pages={80--94},
  year={2019},
  publisher={Elsevier}
}
@article{hashimoto2003review,
  title={A review on vision-based control of robot manipulators},
  author={Hashimoto, Koichi},
  journal={Adv. Robot.},
  volume={17},
  number={10},
  pages={969--991},
  year={2003},
  publisher={Taylor and Francis Ltd.}
}
@article{hutchinson1996tutorial,
  title={A tutorial on visual servo control},
  author={Hutchinson, Seth and Hager, Gregory D and Corke, Peter I},
  journal=tro,
  volume={12},
  number={5},
  pages={651--670},
  year={1996}
}
@article{zhang2021towards,
  title={Towards high performance human keypoint detection},
  author={Zhang, Jing and Chen, Zhe and Tao, Dacheng},
  journal={Intl. J. of Computer Vision},
  volume={129},
  number={9},
  pages={2639--2662},
  year={2021},
  publisher={Springer}
}
@article{kulshreshtha2022feature,
  title={Feature refinement to improve high resolution image inpainting},
  author={Kulshreshtha, Prakhar and Pugh, Brian and Jiddi, Salma},
  journal={arXiv preprint arXiv:2206.13644},
  year={2022}
}
@inproceedings{yu2019free,
  title={Free-form image inpainting with gated convolution},
  author={Yu, Jiahui and Lin, Zhe and Yang, Jimei and Shen, Xiaohui and Lu, Xin and Huang, Thomas S},
  booktitle=iccv,
  pages={4471--4480},
  year={2019}
}
@article{nazeri2019edgeconnect,
  title={Edgeconnect: Generative image inpainting with adversarial edge learning},
  author={Nazeri, Kamyar and Ng, Eric and Joseph, Tony and Qureshi, Faisal Z and Ebrahimi, Mehran},
  journal={arXiv preprint arXiv:1901.00212},
  year={2019}
}
@article{zhu2021image,
  title={Image inpainting by end-to-end cascaded refinement with mask awareness},
  author={Zhu, Manyu and He, Dongliang and Li, Xin and Li, Chao and Li, Fu and Liu, Xiao and Ding, Errui and Zhang, Zhaoxiang},
  journal={{IEEE} Trans. on Img. Processing},
  volume={30},
  pages={4855--4866},
  year={2021}
}
@inproceedings{yi2020contextual,
  title={Contextual residual aggregation for ultra high-resolution image inpainting},
  author={Yi, Zili and Tang, Qiang and Azizi, Shekoofeh and Jang, Daesik and Xu, Zhan},
  booktitle=cvpr,
  pages={7508--7517},
  year={2020}
}
@inproceedings{bohlin2000path,
  title={Path planning using lazy PRM},
  author={Bohlin, Robert and Kavraki, Lydia E},
  booktitle=icra,
  volume={1},
  pages={521--528},
  year={2000}
}
@inproceedings{gandhi2022skeleton,
  title={Skeleton-based adaptive visual servoing for control of robotic manipulators in configuration space},
  author={Gandhi, Abhinav and Chatterjee, Sreejani and Calli, Berk},
  booktitle=iros,
  pages={2182--2189},
  year={2022}
}
@inproceedings{lee2011obstacle,
  title={Obstacle avoidance using image-based visual servoing integrated with nonlinear model predictive control},
  author={Lee, Daewon and Lim, Hyon and Kim, H Jin},
  booktitle={{IEEE} Conf. on Decision and Control and European Control Conference},
  pages={5689--5694},
  year={2011}
}
@inproceedings{mezouar2000path,
  title={Path planning in image space for robust visual servoing},
  author={Mezouar, Youcef and Chaumette, Fran{\c{c}}ois},
  booktitle=icra,
  volume={3},
  pages={2759--2764},
  year={2000}
}

@article{levine2016rl,
  author  = {Sergey Levine and Chelsea Finn and Trevor Darrell and Pieter Abbeel},
  title   = {End-to-End Training of Deep Visuomotor Policies},
  journal = {J. of Machine Learning Research},
  year    = {2016},
  volume  = {17},
  number  = {39},
  pages   = {1--40},
  url     = {http://jmlr.org/papers/v17/15-522.html}
}

i@inproceedings{hafner2019learning,
  title={Learning latent dynamics for planning from pixels},
  author={Hafner, Danijar and Lillicrap, Timothy and Fischer, Ian and Villegas, Ruben and Ha, David and Lee, Honglak and Davidson, James},
  booktitle=icml,
  pages={2555--2565},
  year={2019},
  organization={PMLR}
}

@article{ichter2019robot,
  title={Robot motion planning in learned latent spaces},
  author={Ichter, Brian and Pavone, Marco},
  journal=iral,
  volume={4},
  number={3},
  pages={2407--2414},
  year={2019}
}

@article{kavraki1996probabilistic,
  title={Probabilistic roadmaps for path planning in high-dimensional configuration spaces},
  author={Kavraki, Lydia E and Svestka, Petr and Latombe, J-C and Overmars, Mark H},
  journal=tro,
  volume={12},
  number={4},
  pages={566--580},
  year={1996}
}

@article{orthey2024review,
author = {Orthey, Andreas and Chamzas, Constantinos and Kavraki, Lydia E.},
title = {Sampling-Based Motion Planning: A Comparative Review},
journal = ar,
volume = {7},
number = {1},
year = {2024},
doi = {10.1146/annurev-control-061623-094742},
}

@article{schulman2014motion,
  title={Motion planning with sequential convex optimization and convex collision checking},
  author={Schulman, John and Duan, Yan and Ho, Jonathan and Lee, Alex and Awwal, Ibrahim and Bradlow, Henry and Pan, Jia and Patil, Sachin and Goldberg, Ken and Abbeel, Pieter},
  journal=ijrr,
  volume={33},
  number={9},
  pages={1251--1270},
  year={2014},
  publisher={SAGE}
}

@inproceedings{kuffner2000rrt,
  title={RRT-connect: An efficient approach to single-query path planning},
  author={Kuffner, James J and LaValle, Steven M},
  booktitle=icra,
  volume={2},
  pages={995--1001},
  year={2000}
}

@article{likhachev2003ara,
  title={ARA*: Anytime A* with provable bounds on sub-optimality},
  author={Likhachev, Maxim and Gordon, Geoffrey J and Thrun, Sebastian},
  journal=nips,
  volume={16},
  year={2003}
}
@inproceedings{chatterjee2024utilizing,
  title={Utilizing Inpainting for Training Keypoint Detection Algorithms Towards Markerless Visual Servoing},
  author={Chatterjee, Sreejani and Doan, Duc and Calli, Berk},
  booktitle=icra,
  pages={3086--3092},
  year={2024}
}
@article{liu2020survey,
  title={A survey on underactuated robotic systems: bio-inspiration, trajectory planning and control},
  author={Liu, Pengcheng and Huda, M Nazmul and Sun, Li and Yu, Hongnian},
  journal={Mechatronics},
  volume={72},
  pages={102443},
  year={2020},
  publisher={Elsevier}
}
@inproceedings{chavdarov2019design,
  title={Design and control of an educational redundant 3D printed robot},
  author={Chavdarov, Ivan and Nikolov, Valentin and Naydenov, Bozhidar and Boiadjiev, George},
  booktitle={{IEEE} Intl. Conf. on Software, Telecommunications and Computer Networks},
  pages={1--6},
  year={2019}
}
@article{adzeman2020kinematic,
  title={Kinematic modeling of a low cost 4 DOF robot arm system},
  author={Adzeman, Muhammad Ashraff Mohd and Zaman, Mohd Hairi Mohd and Nasir, Muhammad Fikri and Faisal, Mohd and Ibrahim, Seri Mastura Mustaza},
  journal={Intl. J. of Emerging Trends in Engineering Research},
  volume={8},
  number={10},
  year={2020}
}
@article{hussein2015review,
  title={A review on vision-based control of flexible manipulators},
  author={Hussein, Mustafa Turki},
  journal={Adv. Robot.},
  volume={29},
  number={24},
  pages={1575--1585},
  year={2015},
  publisher={Taylor \& Francis}
}
@article{wang2018adaptive,
  title={Adaptive visual servoing of contour features},
  author={Wang, Hesheng and Yang, Bohan and Wang, Jingchuan and Liang, Xinwu and Chen, Weidong and Liu, Yun-Hui},
  journal=trm,
  volume={23},
  number={2},
  pages={811--822},
  year={2018}
}
@article{navarro2017fourier,
  title={Fourier-based shape servoing: A new feedback method to actively deform soft objects into desired 2-D image contours},
  author={Navarro-Alarcon, David and Liu, Yun-Hui},
  journal=tro,
  volume={34},
  number={1},
  pages={272--279},
  year={2017}
}
@inproceedings{gandhi2023shape,
  title={Shape Control of Variable Length Continuum Robots Using Clothoid-Based Visual Servoing},
  author={Gandhi, Abhinav and Chiang, Shou-Shan and Onal, Cagdas D and Calli, Berk},
  booktitle=iros,
  pages={6379--6386},
  year={2023}
}
@inproceedings{liu2021model,
  title={A model-free deep reinforcement learning approach for robotic manipulators path planning},
  author={Liu, Wenxing and Niu, Hanlin and Mahyuddin, Muhammad Nasiruddin and Herrmann, Guido and Carrasco, Joaquin},
  booktitle={{IEEE} Intl. Conf. on Control, Autom. and Syst.},
  pages={512--517},
  year={2021}
}
@article{zhao2024survey,
  title={A survey of optimization-based task and motion planning: from classical to learning approaches},
  author={Zhao, Zhigen and Cheng, Shuo and Ding, Yan and Zhou, Ziyi and Zhang, Shiqi and Xu, Danfei and Zhao, Ye},
  journal=trm,
  year={2024},
  publisher={IEEE}
}
@inproceedings{quintero2021robust,
  title={Robust optimization-based motion planning for high-DOF robots under sensing uncertainty},
  author={Quintero-Pena, Carlos and Kyrillidis, Anastasios and Kavraki, Lydia E},
  booktitle=icra,
  pages={9724--9730},
  year={2021},
  organization={IEEE}
}
@article{kingston2018sampling,
  title={Sampling-based methods for motion planning with constraints},
  author={Kingston, Zachary and Moll, Mark and Kavraki, Lydia E},
  journal=ar,
  volume={1},
  number={1},
  pages={159--185},
  year={2018}
}
@inproceedings{cohen2010search,
  title={Search-based planning for manipulation with motion primitives},
  author={Cohen, Benjamin J and Chitta, Sachin and Likhachev, Maxim},
  booktitle=icra,
  pages={2902--2908},
  year={2010}
}
@article{park2020trajectory,
  title={Trajectory planning with collision avoidance for redundant robots using jacobian and artificial potential field-based real-time inverse kinematics},
  author={Park, Sun-Oh and Lee, Min Cheol and Kim, Jaehyung},
  journal={Intl. J. on Control, Autom. and Syst.},
  volume={18},
  number={8},
  pages={2095--2107},
  year={2020},
  publisher={Springer}
}
@article{xu2024recent,
  title={Recent advances in Rapidly-exploring Random Tree: A Review},
  author={Xu, Tong},
  journal={Heliyon},
  year={2024},
  publisher={Elsevier}
}
@inproceedings{zhou2021robotic,
  title={Robotic arm motion planning based on residual reinforcement learning},
  author={Zhou, Dongxu and Jia, Ruiqing and Yao, Haifeng and Xie, Mingzuo},
  booktitle={{IEEE} Intl. Conf. on Computer and Autom. Engineering},
  pages={89--94},
  year={2021}
}
@article{luo2018orisnake,
  title={OriSnake: Design, fabrication, and experimental analysis of a 3-D origami snake robot},
  author={Luo, Ming and Yan, Ruibo and Wan, Zhenyu and Qin, Yun and Santoso, Junius and Skorina, Erik H and Onal, Cagdas D},
  journal=iral,
  volume={3},
  number={3},
  pages={1993--1999},
  year={2018}
}
@article{onal2014origami,
  title={Origami-inspired printed robots},
  author={Onal, Cagdas D and Tolley, Michael T and Wood, Robert J and Rus, Daniela},
  journal=trm,
  volume={20},
  number={5},
  pages={2214--2221},
  year={2014}
}
@INPROCEEDINGS{556169,
  author={Zeller, M. and Sharma, R. and Schulten, K.},
  booktitle={{IEEE} Intl. Symp. on Intell. Control}, 
  title={Vision-based motion planning of a pneumatic robot using a topology representing neural network}, 
  year={1996},
  volume={},
  number={},
  pages={7-12},
  keywords={Motion planning;Robot vision systems;Network topology;Phase change materials;Robot sensing systems;Orbital robotics;Robot motion;Motion control;Uncertainty;Neural networks},
  doi={10.1109/ISIC.1996.556169}}
