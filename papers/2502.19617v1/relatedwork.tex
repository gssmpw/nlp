\section{Related Work}
Motion planning is a core problem in robotics that has been extensively studied over the decades. It is generally categorized into three main types: optimization-based \cite{schulman2014motion, zhao2024survey}, sampling-based \cite{orthey2024review, kingston2018sampling}, and search-based planners \cite{likhachev2003ara, cohen2010search}. Recently, Jacobian-based motion planning \cite{park2020trajectory} has also gained traction for obstacle avoidance tasks. While all methods have found widespread success in different applications, all of these rely on having an explicit geometric model of the robot to design feasible paths. In this work we extend the principles of sampling-based planning of the Probabilistic Roadmap (\prm) approach \cite{kavraki1996probabilistic}, to provide a method for motion planning where a robot model is not available. 

Model-free planning, especially without prior knowledge of the robot's geometry, presents unique challenges. Reinforcement learning (RL) has been explored extensively in the recent decades to address such challenges. For instance, \cite{liu2021model} proposed an RL-based framework for generating jerk-free, smooth trajectories. While effective, this method depends on carefully crafted reward functions and extensive datasets generated in simulation, with no real-world validation. Similarly, \cite{zhou2021robotic} introduced a hybrid approach combining RRT*-based trajectories with PPO reinforcement learning for policy refinement. However, this method heavily relies on model-based elements like precomputed trajectories and supervised learning for initial policy design, with experiments confined to simulated environments. In contrast, our approach eliminates reliance on explicit or precomputed models and trajectories by leveraging visual keypoints to construct roadmaps directly in image space, requiring only a small dataset collected from a real robot. This makes our method more adaptable to scenarios without precise geometric models.

Among related works, the approaches proposed in \cite{ichter2019robot} and \cite{556169} are the most closely aligned with ours. In \cite{ichter2019robot}, the authors use sampling-based planning directly from images by leveraging learned forward propagation models, custom distance metrics, and collision checkers. While effective, their method requires extensive simulation-based training, which introduces a significant sim-to-real gap. In contrast, our method uses a limited dataset collected entirely from real robots and constructs a configuration-space roadmap without relying on simulation. Similarly, \cite{556169} combines image features with a robot model to train a neural network for planning a path. Unlike their approach, our method eliminates the need for a robot model, relying solely on image features for motion planning.

Planning a path for image based visual servoing without a-priori knowledge of the robot's model is rarely delved into in the literature. For instance, \cite{mezouar2000path} modeled a path planner for visual servoing to bridge gaps between initial and target positions which are much further apart in configuration space, without addressing collision avoidance. In \cite{lee2011obstacle} authors achieve obstacle avoidance in pose based visual servoing and hence still needed explicit robot model instead of only visual feedback.