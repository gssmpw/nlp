

\documentclass[letterpaper, 10 pt, conference]{ieeetran}  % Comment this line out if you need a4paper



\IEEEoverridecommandlockouts                             
\usepackage[noend]{algpseudocode}
\usepackage{algorithm}
\usepackage{amssymb}
\usepackage{bbm}
\usepackage{dsfont}
\usepackage[left=1.69cm,right=1.69cm,top=2.12cm,bottom=1.52cm]{geometry}


\newcommand{\algorithmautorefname}{Alg.}

\usepackage{xspace}
\usepackage{xcolor}
\definecolor{softgreen}{RGB}{34,139,34} % A visually appealing forest green
\usepackage[colorlinks=true, linkcolor=softgreen, citecolor=softgreen, urlcolor=softgreen]{hyperref}

\renewcommand{\figureautorefname}{Fig.} % Changes "Figure" to "Fig."
\renewcommand{\equationautorefname}{Eq.} % Changes "Equation" to "Eq."
\usepackage{array}


\newcommand{\R}{\ensuremath{\mathbb{R}}\xspace}
\newcommand{\X}{\ensuremath{\mathcal{X}}\xspace}
\newcommand{\T}{\ensuremath{\mathcal{T}}\xspace}
\newcommand{\Xg}{\X_G} 
\newcommand{\xs}{\ensuremath{\x_\start}} 
\newcommand{\U}{\ensuremath{\mathcal{U}}\xspace}
\newcommand{\uu}{u}
\newcommand{\Xfree}{\ensuremath{\X_{\text{free}}}\xspace}
\newcommand{\Xobs}{\ensuremath{\X_{\text{obs}}}\xspace}
\newcommand{\K}{\ensuremath{I}\xspace}
\newcommand{\Kspace}{\K-space\xspace}
\newcommand{\key}{\ensuremath{\mathbb{K}}\xspace}
\newcommand{\skey}{\ensuremath{\mathds{k}}\xspace}
\newcommand{\keys}{\ensuremath{\key_\text{start}}\xspace}
\newcommand{\keyg}{\ensuremath{\key_\text{goal}}\xspace}
\newcommand{\Kobs}{\ensuremath{\K_{\text{obs}}}\xspace}
\newcommand{\Kfree}{\ensuremath{\K_{\text{free}}}\xspace}
\newcommand{\x}{x}
\newcommand{\xr}{\x_{rand}}
\newcommand{\Kf}{{\ensuremath{\K_\free}}\xspace}
\newcommand{\Ko}{{\ensuremath{\K_\text{obs}}}\xspace}
\renewcommand{\xi}{\x_{I}}
\newcommand{\prm}{\textsc{prm}\xspace}
\newcommand{\lprm}{\textsc{lazy-prm}\xspace}
\newcommand{\euclid}{image distance}
\newcommand{\learned}{learned}

\newcommand{\start}{\text{start}}
\newcommand{\source}{\text{src}}
\newcommand{\destination}{\text{dst}}
\newcommand{\goal}{\text{goal}}
\newcommand{\free}{\text{free}}
\newcommand{\dof}{\textsc{dof}\xspace}

\newcommand{\im}{\ensuremath{Im}\xspace}
\newcommand{\IM}{\ensuremath{\mathcal{IM}}\xspace}


% Configuration Space
\newcommand{\C}{\X}
\newcommand{\Cs}{\X}
\newcommand{\Cf}{\Xfree}
\newcommand{\Co}{\Xobs}

\usepackage[textsize=small]{todonotes}
\setlength{\marginparwidth}{1.2cm}

\usepackage{graphics} 
\usepackage{listings}
\usepackage{comment}
\usepackage{epsfig} 

\usepackage{amsmath} 
\usepackage{cite}
\usepackage{subcaption}
\usepackage{xcolor}
 \usepackage{url}
\definecolor{gg}{RGB}{0, 155, 85} 
\definecolor{json-key}{rgb}{0.13,0.55,0.13}
\definecolor{json-value}{rgb}{0.25,0.25,0.25}
\definecolor{json-string}{rgb}{0.9,0.3,0.3}
\usepackage{caption}
\captionsetup[figure]{font=small, labelfont=small, labelsep=period}
\captionsetup[table]{font=small, labelfont=small, labelsep=period}

\lstdefinelanguage{json}{
  basicstyle=\ttfamily,
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=,
    numbersep=5pt,
    breaklines=true,
    frame=lines,
    backgroundcolor=\color{white!10},
    captionpos=b
}

\newcommand\centerImage[2][]%
  {\raisebox
    {-\dimexpr0.5\height}%
    [\dimexpr0.5\height+1mm]%
    [\dimexpr0.5\height+1mm]%
    {\includegraphics[#1]{#2}}%
  }


\title{\LARGE \bf  Image-Based Roadmaps for Vision-Only Planning and Control of Robotic Manipulators}

\author{Sreejani Chatterjee$^{1}$, Abhinav Gandhi $^{1}$, Berk Calli$^{1}$ and Constantinos Chamzas $^{1}$ % stops a space
% \thanks{*This paper was supported in part by the National Science Foundation under grant
% IIS-1900953 and CMMI-1928506.}% <-this % stops a space
\thanks{$^{1}$S Chatterjee, $^{1}$A Gandhi,  $^{1}$C Chamzas and $^{1}$B Calli are with Department of Robotics Engineering,
        Worcester Polytechnic Institute, Worcester, MA 01609, USA
        {\tt\small schatterjee@wpi.edu, agandhi@wpi.edu}}%
}

\begin{document}
\maketitle
\thispagestyle{empty}
\pagestyle{empty}


\begin{abstract}

This work presents a motion planning framework for robotic manipulators that computes collision-free paths directly in image space. The generated paths can then be tracked using vision-based control, eliminating the need for an explicit robot model or proprioceptive sensing.
At the core of our approach is the construction of a roadmap entirely in image space. To achieve this, we explicitly define sampling, nearest-neighbor selection, and collision checking based on visual features rather than geometric models. We first collect a set of image-space samples by moving the robot within its workspace, capturing keypoints along its body at different configurations. These samples serve as nodes in the roadmap, which we construct using either learned or predefined distance metrics.
At runtime, the roadmap generates collision-free paths directly in image space, removing the need for a robot model or joint encoders. We validate our approach through an experimental study in which a robotic arm follows planned paths using an adaptive vision-based control scheme to avoid obstacles. The results show that paths generated with the learned-distance roadmap achieved 100\% success in control convergence, whereas the predefined image-space distance roadmap enabled faster transient responses but had a lower success rate in convergence.
 
\end{abstract}

\section{Introduction}

Vision-based control techniques~\cite{hutchinson1996tutorial, hashimoto2003review}, offer significant advantages for robotic manipulators in unstructured and cluttered environments by enabling closed-loop control using task-relevant visual information. These techniques also enhance robustness against model inaccuracies, beneficial for robots with complex or variable dynamics \cite{Calli2016, cuevas2018hybrid, ardon2018reaching}. This strategy is especially useful for robots that are difficult to model accurately, e.g. soft robots \cite{Lai2020, luo2018orisnake}, under-actuated robots \cite{liu2020survey, gandhi2023shape}, 3D printed robots \cite{chavdarov2019design, onal2014origami}, and robots with inexpensive hardware \cite{adzeman2020kinematic}. Furthermore, model-free visual servoing, which learns robot-feature motion models during control, reduces reliance on a priori knowledge of the robot model \cite{wang2018adaptive, navarro2017fourier}.

The goal of our research is to push the boundaries of purely vision-based control and motion planning for robotic manipulators, by decreasing reliance on explicit robot modeling or proprioceptive sensing. In doing so, we strive to use natural visual features along the robot's body in image space, without attaching any external markers. While the works in \cite{gandhi2022skeleton,chatterjee2023keypoints, chatterjee2024utilizing} provided algorithms to track natural keypoints and use them to achieve vision-based model-free control with decent transient responses, these algorithms are only designed to run in obstacle-free space and do not provide motion planning capability to avoid obstacles in the scene. 

\begin{figure}[t]
        \centering
    \includegraphics[width=0.65\columnwidth]{images/fig_1_with_obs_v4.png}
    \caption{
    The robot follows a collision-free path using visual keypoints (colored) planned with the proposed method. 
    The set of keypoints are tracked in order:
    green, magenta, blue, and red. The robot avoids the yellow obstacle while moving between the specified start and goal.  }
    \label{intro-image}
    \vspace{-1em}
\end{figure}

In this work, we tackle the problem of collision free path planning for purely vision-based robot control: we develop a planning and control scheme that does not rely on a robot model or on-board sensors (e.g. joint encoders) in runtime. As such, this strategy is especially useful for (but not limited to) soft/underactuated/inexpensive robots that are hard to model and may not have reliable proprioceptive sensing.

Toward this goal, we propose a novel motion planning formulation that operates only with visual information. We propose a vision-only motion planning methodology using roadmaps \cite{kavraki1996probabilistic}.  We investigated two ways of generating roadmaps using natural features on a robot: 1) directly utilizing the Euclidean distance between keypoints in image space as a distance metric, 2) estimating the joint displacements from image features and utilizing it as a distance metric. For the latter, we first used an automated data collection pipeline to annotate natural keypoints' placement in image space along the robot's body as the robot moves across various configurations.

This dataset was used to develop a simple neural network that approximates joint displacements based on keypoint locations in image space. These distance metrics were integrated into the roadmap construction. Once the roadmaps are constructed, polygon-based collision checking and A* search are employed to ensure collision-free paths. These two approaches have different implications for vision-based robot control. In a nutshell, we observed that utilizing estimated joint distances in the roadmap results in smoother and more accurate tracking of the generated path, allowing the robot to stay in its defined workspace and avoid obstacles. In contrast, roadmaps based on Euclidean distances in image space can offer faster transient responses, albeit with potentially less reliable tracking. Our experiments demonstrate these aspects both in the presence and absence of obstacles.
 
\section{Related Work}

Motion planning is a core problem in robotics that has been extensively studied over the decades. It is generally categorized into three main types: optimization-based \cite{schulman2014motion, zhao2024survey}, sampling-based \cite{orthey2024review, kingston2018sampling}, and search-based planners \cite{likhachev2003ara, cohen2010search}. Recently, Jacobian-based motion planning \cite{park2020trajectory} has also gained traction for obstacle avoidance tasks. While all methods have found widespread success in different applications, all of these rely on having an explicit geometric model of the robot to design feasible paths. In this work we extend the principles of sampling-based planning of the Probabilistic Roadmap (\prm) approach \cite{kavraki1996probabilistic}, to provide a method for motion planning where a robot model is not available. 

Model-free planning, especially without prior knowledge of the robot's geometry, presents unique challenges. Reinforcement learning (RL) has been explored extensively in the recent decades to address such challenges. For instance, \cite{liu2021model} proposed an RL-based framework for generating jerk-free, smooth trajectories. While effective, this method depends on carefully crafted reward functions and extensive datasets generated in simulation, with no real-world validation. Similarly, \cite{zhou2021robotic} introduced a hybrid approach combining RRT*-based trajectories with PPO reinforcement learning for policy refinement. However, this method heavily relies on model-based elements like precomputed trajectories and supervised learning for initial policy design, with experiments confined to simulated environments. In contrast, our approach eliminates reliance on explicit or precomputed models and trajectories by leveraging visual keypoints to construct roadmaps directly in image space, requiring only a small dataset collected from a real robot. This makes our method more adaptable to scenarios without precise geometric models.

Among related works, the approaches proposed in \cite{ichter2019robot} and \cite{556169} are the most closely aligned with ours. In \cite{ichter2019robot}, the authors use sampling-based planning directly from images by leveraging learned forward propagation models, custom distance metrics, and collision checkers. While effective, their method requires extensive simulation-based training, which introduces a significant sim-to-real gap. In contrast, our method uses a limited dataset collected entirely from real robots and constructs a configuration-space roadmap without relying on simulation. Similarly, \cite{556169} combines image features with a robot model to train a neural network for planning a path. Unlike their approach, our method eliminates the need for a robot model, relying solely on image features for motion planning.

Planning a path for image based visual servoing without a-priori knowledge of the robot's model is rarely delved into in the literature. For instance, \cite{mezouar2000path} modeled a path planner for visual servoing to bridge gaps between initial and target positions which are much further apart in configuration space, without addressing collision avoidance. In \cite{lee2011obstacle} authors achieve obstacle avoidance in pose based visual servoing and hence still needed explicit robot model instead of only visual feedback.

\section{Preliminaries and Problem Statement}
In this section we describe the motion planning problem, a brief review of sampling-based methods focusing on probabilistic roadmaps, and finally we introduce the problem statement that we are trying to solve.  

\begin{figure}[t]
      \centering
      \includegraphics[width=0.5\textwidth]{images/config_space_v7.pdf}
      \vspace{-3.85em}
      \caption{The images depict a vision-based motion planning problem. The left image shows the discretized representation of \K  highlighted in gray, where each configuration is sampled as an image frame, resulting in a finite set of \key. The gray region in the right image highlights the same discretized representation of \Kfree, avoiding the obstacle in yellow. \Kfree ensures a collision-free path from the start configuration \keys to the goal configuration \keyg}
      \label{config_space}
\end{figure}

\subsection{Model-based Motion Planning}
\label{ssec:prob-state}

Let $\x \in \X$ denote the state and state-space, and $ \uu \in \U$ the control and control-space of a robot system \cite{orthey2024review}.
The system dynamics equations $f$ of the robot system can be written as 
\begin{equation} \label{eq:motion}
\x(\T) = \x(0)+\int_0^{\T} f(x(t),u(t))dt \end{equation}
If the system dynamics impose constraints on the allowable paths the system is called a non-holonomic system.  Now, Let $\Co \subset \C $ denote the invalid state space, which is the set of states that violates the robots kinematic constraints or collides with the workspace $\mathcal{W} \subseteq \mathbb{R}^3$. e.g.,
\begin{equation}
\Co =\{\x \in \C | R(\x) \bigcap \mathcal{W} \neq \emptyset \}
\end{equation}
where $R(\x) \subseteq \mathbb{R}^3$ is the set of all the points of the robot in the workspace at state \x \footnote{This equation only encodes collisions but joint/kinematic constraints can be encoded in a similar manner}. Then let, $\Cf = \C \setminus \Co$ denote the collision free state-space. Also, let $x_\start \in \Cf $ and \mbox{$X_\goal \subseteq \Cf$} denote the start state and goal regions respectively.

\textit{Motion Planning Problem:}
Given the motion planning tuple $(\X, R, \mathcal{W}, \U, f,  x_\start, \Xg)$ find a time $\T$ and a set of controls $u: [0,\T] \rightarrow \U$ such that the motion described by \autoref{eq:motion} satisfies $ x(0) = \xs $, $ \x(\T) \in \Xg$ and $x(t) \in \Xfree$.   

\subsection{Vision-Only Motion Planning} 
\label{ssec:prob-state-vis}

The problem we are considering in this setting is departing from the above motion planning problem as the geometric model $R(x)$, the dynamics $f$ and the workspace $\mathcal{W}$ is not directly available. Instead the only information available is the space of admissible controls \U and an image $\im \in \IM$.

We will describe the new problem statement by explicitly defining equivalent concepts of obstacles, robot states, and state dynamics directly in the image space.
We assume that we are given a point tracking function that maps a given image to $N$ fixed pixel points on the robot's body.  
This vector of pixel points is denoted as an image state $\key \in \K $.
Where $\K \subset \mathbb{R}^{N \times 2}$ denotes the space of all image states.
In \autoref{intro-image} we can see $4$ different robot configurations in image space. Each configuration is represented by a set of $5$ same-colored circles. Each of these circles on the robot's body in the image is a pixel point and is denoted as a keypoint or $\skey$. The kinematic chain or shape formed by the vector of $5$ circles represents one image state \key.

Given a set of image obstacles $\mathcal{IO} \subset \im$ we define: 
\begin{equation}
\Kobs =\{\key \in \K | RI(\key) \bigcap \mathcal{IO} \neq \emptyset \}
\end{equation}
where $RI(\key) \subseteq \im $ is the set of pixels that the robot occupies at image state \key . Similarly to before, we define $\Kfree  = \K \setminus \Kobs$, $\key_\start$ and $\K_g$. 

We define the unknown system dynamics function as:     
\begin{equation} \label{eq:motion_image}
\key(\T) = \key(0)+\int_0^{\T} g(\key(t),u(t))dt
\end{equation}

Here we note that even if the underlying function f is fully-integrable (holonomic-system) if $dim(\K)> dim(\X)$ the equivalent system dynamics equations $g$ will be non-holonomic as there will be paths in the higher dimensional image-space \K that cannot be followed by the robot. In our approach we consider this issue, and propose a way to produce paths that approximately satisfy these constraints. In the first image of \autoref{config_space}, the grey region illustrates the discretized representation of \K, as each configuration is captured as an image frame, resulting in a finite set of \key sampled at a specific frame rate. The second image of \autoref{config_space} highlights similar discretized representation of \Kfree.

 Given the above definitions, let us define the vision-only motion planning problem. 

\textit{Vision-Only Motion Planning Problem:}
Given the tuple $(\K, RI, \U, \mathcal{IO}, \im, \key_{\start}, \K_g)$ find a time $\T$ and a set of controls $u: [0,\T] \rightarrow \U$ such that the motion described by $g$ satisfies $\key(0) = \key_{\start}$, $ \key(\T) \in \K_g$ and $\key(t) \in \Kfree$.  
%If we had the model of the robot, then we can compute all these different functions and succesfully find a path from start to goal that avoids the obstacles. However since we don't have the model of the robot, we would need to find ways to compute the  and distance for nearest neighbor structure.  
\section{Methodology}
\label{ssec:methodology}

Since the system dynamics equation is unknown, we cannot directly plan in the control space \U. Instead, we will plan a path $\key_0, \key_1 \ldots, \key_n$ directly in the \Kspace and then control the robot to follow the path with a vision-only controller\cite{gandhi2022skeleton}.

To compute the path, which is the main contribution of this work, we propose to use probabilistic road-map planner (\prm) by adapting its subroutines to operate directly in \K-space. Specifically, we opted to adapt the \lprm~\cite{bohlin2000path} planner due to its compatibility with our particular requirements. However, any sampling-based planner which relies on the the same subroutines could be used. 

\begin{algorithm}[H]
   \caption{Build-Lazy-PRM} 
   \label{alg:build-lazy-prm}
    \begin{algorithmic}[1] 
     \Procedure{Build-Lazy-PRM}{N, k}  
        \State {$G$} $\gets$ INIT()   
        \While{$G$.size()$\leq$ N}%$i = 1 ,\ldots, N$} 
          %\State \Comment{generate collision free sample}
           \State $\key_{new} \gets $ {\color{red}{\textsc{sample}}}($\K$) \label{sample} 
           \State $G$.addNode($\key_{new}$) \label{roadmap}
        \EndWhile
       \For{each $\key \in  G$.nodes()} 
           \State $\mathcal{N}(\key) \gets $ {\color{red}{\textsc{K-nearest}}}($\key$, {$G$}) \label{dist} 
           \For{each $\key_{near} \in \mathcal{N}(\key_{new})$} 
            \State $e \gets$ ($\key, \key_{near})$
            \If{$e \notin G$.edges()} \label{collision}
                \State  $G$.addEdge($e$) 
            \EndIf 
          \EndFor 
        \EndFor 
    	\State \Return $G$ 
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
    \vspace{-0em}

Similar to \prm, \lprm operates in two distinct phases, the building-phase \autoref{alg:build-lazy-prm} and the query-phase \autoref{alg:query-lazy-prm}. In the building-phase, a roadmap is built  without performing any collision checking. In the query-phase, the roadmap is utilized to find a potential path for a given start and goal. The key difference between \lprm and \prm lies in their approach to collision checking. While \prm verifies the collision status of all edges in the roadmap during the building phase, \lprm reverses the order. It first performs the search query and only checks collisions for the found path. If any edges are found to be in collision, the roadmap is updated, and another search query is executed until a valid path is discovered.

We selected probabilistic roadmap method for its efficiency in multi-query scenarios, where a single precomputed graph can handle multiple start and goal configurations, and used the lazy version, as we only have very few varying obstacles between queries. 
Additionally, its ability to operate in high-dimensional spaces makes it particularly well-suited for handling non-traditional representations of configuration space, such as the visual keypoints in \K-space used in our approach. We first describe how \lprm works and then we describe how we modified the necessary subroutines to make it work in \Kspace. 

The building-phase (\autoref{alg:build-lazy-prm}) works with the following procedure. First, in line \ref{sample}, a sample is generated and added in graph $G$. Then the k nearest nearest neighbors are found by using a distance defined in \K (line \ref{dist}) and are connected with edges. This continues until $N$ nodes are in the graph $G$. 


\begin{algorithm}[H]
   \caption{Query-Lazy-PRM} 
   \label{alg:query-lazy-prm}
    \begin{algorithmic}[1] 
     \Procedure{Lazy-Query-PRM}{\keys, \keyg, $G$}   
       \State G.add(\keys) \Comment{Add start, and goal to the Graph}
       \State G.add(\keyg)
       \State Edges, $\gets$ \textsc{Search-Graph} $G$(\keyg, \keys)
       \For{each $e \in  G$.edges()} 
           \If{{\color{red}{\textsc{coll\_free}}}($e$)} \label{collision}
           \For{each $\key_{near} \in \mathcal{N}(x_{new})$} 
            \State $e \gets$ ($\key, \key_{near})$
            \If{{\color{red}{\textsc{coll\_free}}}($e$) and $e \notin G$.edges()} \label{collision}
                \State  $G$.addEdge($e$) 
            \EndIf 
          \EndFor 
    	\State \Return $Path$ 
        \EndIf 
     \EndFor 
    \EndProcedure
    \end{algorithmic}
    \end{algorithm}
    \vspace{-0.75em}



During the query-phase (\autoref{alg:query-lazy-prm} a new motion planning problem is solved. Given a $\key_\start$ and $\key_\goal $ they are added in the  graph, and connected with their nearest-neighbors. Then a graph search algorithm e.g., A* is used to find a path. If edges of the path are in-collision they are updated accordingly in the roadmap, and the process repeats until a collision free-path is found. 
The three operations \textsc{sample},  \textsc{k-nearest},  \textsc{coll\_free},
for \autoref{alg:build-lazy-prm} in lines \ref{sample}, \ref{dist} and \autoref{alg:query-lazy-prm}  typically require a model for the robot.

\begin{figure*}[ht!]
      \centering
      \includegraphics[width=0.8\textwidth]{images/graph_flow_v8.pdf}
          \caption{Overview of the roadmap creation process for motion planning. In this image the purple circles are nodes in image space represented by image state in each frame. Using a distance metric, we connect the nodes to create a roadmap. As observed two different metrics produce different edges on the graph. After an A* search for path finding between a pair of start (\keys) and goal (\keyg) configurations and collision check if required, paths are found for both roadmaps. The green lines denote the final path after collision check. As observed, the \textbf{learned} distance roadmap has a clearly more optimized path than the \textbf{image space} distance roadmap}
      \label{graph}
      \vspace{-1.4em}
\end{figure*}


\textsc{sample}: This function typically samples the configuration space uniformly. However, in the vision-only setting, we can't directly sample in \Kspace as we don't have the model of the robot. Since $dim(\K) > dim(\X)$ the keypoint vectors (image state \key) that correspond to actual configurations of the robot, will lie in a lower-dimensional (equal to $dim(\X)$) manifold in \Kspace. Thus 
randomly sampling \Kspace will have $0$ probability of sampling a valid configuration that lies on the valid manifold \cite{kingston2018sampling}. We describe how to address this issues in \autoref{ssec:dataset}, by collecting and storing valid samples directly from the real robot.  

\textsc{k-nearest}: This function usually relies on a distance defined in \C  and finds the nearest configurations that can be connected. However, since our representation in \Kspace is now a non-holonomic system, defining this distance is very challenging, as a straight line path in \Kspace defined by a simple Euclidean distance, might not be accurately followable by a controller. To mitigate this, we describe a learning-based approach to estimate the unknown joint-distance, in \autoref{ssec:dist_metric}.


\textsc{coll\_free}: This function checks if there is a collision for an edge in \C. Again this typically requires the model $R(x)$ for the robot. In the vision only case, we propose simple yet successful method to do collision checking with an $RI(x)$ directly in \Kspace \autoref{ssec:line-check}. 

In the next section we describe our proposed method, for each, of the aforementioned subroutines. \autoref{graph} visually describes all the steps of the visual motion planning framework.

\subsection{\textsc{sample}: Executed Trajectories as Proxy Samples} 
\label{ssec:dataset}
In the absence of a robot model, we represent each image state $\key \in \K$ by identifying and annotating keypoints (\skey)
 on the robotic arm using an automated data collection pipeline described in \cite{chatterjee2023keypoints, chatterjee2024utilizing}. Each \ensuremath{\key_n} is composed of a set of $N$ keypoints \skey, where each \skey represents a pixel point of a specific location on the robot's body in image space. For instance, if \( N = 5 \), a keypoint vector or image state \ensuremath{\key_n} is represented by a vector of {\ensuremath{\skey_1}, \ensuremath{\skey_2}, \ensuremath{\skey_3}, \ensuremath{\skey_4}, \ensuremath{\skey_5}}, where each \skey is a pixel point in the $n\textsuperscript{th}$ image frame. An example vector of such keypoints is shown in \autoref{config_space} as \keys or \keyg

 To systematically explore the robot's visible workspace in the image space and ensure comprehensive coverage, we compute velocities for each joint \( j \) using:

\begin{equation}
\label{comp_vel}
v_j = \min\left(\frac{M_j}{\ensuremath{res} \cdot t}, v_{\text{max}}\right),
\end{equation}

where, \( M_j \) is the motion range or difference of limits of joint \( j \), \ensuremath{res}, is the number of discrete  steps used to traverse \( M_j \), \( t \) is the duration allocated to complete each step, and \( v_{\text{max}} \) is the maximum allowable velocity for joint \( j \).
By dividing the motion range of each joint into uniform increments based on \ensuremath{res}, we ensure that the robot systematically explores all possible configurations within its workspace. The resulting dataset of image states \key is thus evenly distributed across the image space \K, enabling robust coverage and accurate representation of the robot's motion capabilities. 

Each configuration \ensuremath{\key_n} is computed using the following transformation:

\begin{equation}
\label{2d_eq}
\ensuremath{\key_n} = K \cdot T_{cw} \cdot \ensuremath{x_n}
\end{equation}
where $K$ is the camera intrinsic matrix, $T_{cw}$ is the camera extrinsics matrix derived from calibration processes described in \cite{Lee2020, Zhang2000},  and \ensuremath{x_n} is the 3D configuration robot in workspace. 
This transformation projects the 3D configuration \ensuremath{x_n}, into their corresponding 2D (pixel) projection in the image described in \cite{3dRecon}, resulting in a set of \skey for each image state \ensuremath{\key_n} The process captures the robot's motion across its visible workspace and creates a comprehensive dataset of \key, representing close to all feasible configurations in image space. Dataset of \ensuremath{\key} can also be collected by following the process describe in \cite{chatterjee2024utilizing}. 

The \textbf{Keypoint Dataset Samples} section of \autoref{graph}, represents this part of the workflow. The grey area in the left image of \autoref{config_space} illustrates how \key are distributed within image space with each frame consisting of a set of \skey or keypoints. This collection process yields a large dataset of \key that can be used to enable efficient roadmap construction for vision-based motion planning.

\subsection{\textsc{k-nearest}: Learned and Image-Space Metrics}
\label{ssec:dist_metric}
To search for the K-nearest neighbors of each image state sample we employ the following two distance metrics:

\begin{itemize}
\item \textit{Learned distance},  where the distance is learned by a neural network, trained to predict joint displacements between two image states. The input to the network is a pair of image states $\key_1$ and $\key_2$ and and the output is an estimated joint displacement required to transition between them:
\begin{equation}
\label{cust-eq}
\text{dist}_{learned}() \leftarrow \text{NN}(\key_1, \key_2)
\end{equation}
\item \textit{Distance in Image space}, where we simply calculate the Euclidean distance between $\key_1$ and $\key_2$ in image space:
\begin{equation}
\label{euc-eq}
\text{dist}_{image}() \leftarrow || \key_1 - \key_2 ||_2
\end{equation}
\end{itemize}
Each metric influences the graph's structure by determining the nearest neighbors and defining the edges in the roadmap. The learned distance prioritizes image states with minimal estimated joint displacement, while the image space distance favors image states that are closer in the image. These differences impact the connectivity of the roadmap, as illustrated in the \textbf{Connected Roadmap} and \textbf{Connect Start and Goal} sections of \autoref{graph}.

\subsubsection{Dataset generation for network model}
\label{ssec:approx-joint}
While collecting the dataset of image states \key in \autoref{ssec:dataset}, we recorded the velocity (\autoref{comp_vel}) applied to transition between consecutive frames. Using this data, we created a new dataset that includes pairs of consecutive image states (\ensuremath{\key_1}, \ensuremath{\key_2}) and their estimated joint displacements. This is calculated by multiplying the recorded velocity with the frame rate at which \key was captured, as described in \autoref{2d_eq}.

 Please note here, only pairs of \key captured in consecutive image frames are included in this data generation process.  However, for constructing the graph $G$, it is essential to compute distances between arbitrary \key pairs in \K-space. To achieve this, a neural network is employed to predict the joint displacement across different pairs of image-states \key.
 
To enhance diversity, the dataset is augmented by combining frame sequences where the first frame's image state (\(\key_{\text{start}}\)) and the last frame's image state (\(\key_{\text{end}}\)) act as boundaries, with total joint displacements calculated as sum of displacements across intermediate frames. This approach ensures diverse transitions, enabling the neural network to accurately estimate joint displacements for any image state pair.

\subsubsection{Neural Network for Learned Distance Metric}
\label{ssec:reg-model}
To derive \autoref{cust-eq}, we design a simple neural network using the aforementioned dataset to learn a distance more similar to the joint space distance. The model takes concatenated arrays of the starting image state (\(\key_{\text{start}}\)) and the subsequent image state (\(\key_{\text{next}}\)) as input and predicts the estimated joint displacement.

\subsection{\textsc{coll\_free}:Image-Based Collision Checking}
\label{ssec:line-check} 

To check for collisions in our model-free system, we employ an image-based polygon collision-checking framework. Polygons are formed by connecting pairs of consecutive keypoints ((\(\skey_{n-1}\)), (\(\skey_{n}\))) in one image state (\(\key_1\)) with their corresponding keypoints in the neighboring image state (\(\key_2\)). In \autoref{line-seg}, the polygons created by (\(\skey_1\), \(\skey_2\)),  (\(\skey_2\), \(\skey_3\)), (\(\skey_3\), \(\skey_4\)) and (\(\skey_4\), \(\skey_5\)) of the start image state (\(\key_1\)) in green and its neighboring image state (\(\key_2\)) in red are examples of such polygons. 

Each polygon is then checked for intersections with the obstacle boundaries defined by a safety margin which we consider to account for controller uncertainty. This polygon-based approach ensures a thorough and robust collision-checking process, effectively handling obstacles of any size. In \autoref{line-seg} illustrates an example where the obstacle, enclosed by the red safety margin, intersects with the polygons defined by ([\(\skey_2\), \(\skey_3\)],[\(\skey_3\), \(\skey_4\)] and [\(\skey_4\), \(\skey_5\)]), demonstrating the effectiveness of this method.
The collision checking process and \textbf{A*} search is shown in  \textbf{A* Search with Lazy Collision Checking} of \autoref{graph}.  

\begin{figure}[t]
      \centering
      \includegraphics[width=0.4\textwidth]{images/poly_check_comb_paper.pdf}
      % \vspace{-3.0em}
      \caption{Illustration of polygon-based collision checking for vision-only motion planning. In this image $4$ polygons (bordered with light blue) are defined by connecting consecutive keypoint pairs (\(\skey_1\), \(\skey_2\)),  (\(\skey_2\), \(\skey_3\)), (\(\skey_3\), \(\skey_4\)) and (\(\skey_4\), \(\skey_5\)) of the image state (\(\key_1\)) in green and its neighboring image state (\(\key_2\)) in red. The yellow obstacle enclosed by the purple safety margin is situated right on the polygons formed by ([\(\skey_2\), \(\skey_3\)],[\(\skey_3\), \(\skey_4\)] and [\(\skey_4\), \(\skey_5\)]), rendering these two configurations ineligible for a collision-free path.}
      \vspace{-0.5em}
      \label{line-seg}
\end{figure}

\subsection{Adaptive Visual Servoing}
This work builds on the adaptive visual servoing method described in \cite{gandhi2022skeleton}, using a roadmap of collision-free sequence of goal image states. At each goal, vector of keypoints (\skey) in image state (\key), is tracked as visual features as described in \cite{chatterjee2023keypoints}. The controller moves the arm minimizing the feature error, computed as the difference between the current and the target \key. The Jacobian matrix, estimated online via least-square optimization of recent joint velocities and keypoints vector over a moving window, eliminates the need to read joint position from  encoder. This makes the control pipeline completely model-free. To improve accuracy, we reset the Jacobian estimation window at each new target keeping the estimate unbiased and relevant to the current goal. Since the goals may not be evenly spaced in the image for different experiments, we use a saturation limit on the feature error to prevent sudden spikes in velocity ensuring stable motion and protecting the motor from damage.

\begin{figure}[t]
      \centering
      \includegraphics[width=0.9\columnwidth]{images/histogram_slide.pdf}
      \caption{Histograms of actual joint displacements along the edges for the three roadmaps. The \textbf{Learned} roadmap closely aligns with the distribution of the Joint Space roadmap, showing only slight deviations and indicating reasonable accuracy in the predicted joint displacements. In contrast, the \textbf{Image space} roadmap demonstrates a wider spread and larger joint displacements, reflecting its lack of alignment between image-space proximity and joint-space movements. This misalignment may reduce its efficiency in generating paths suitable for precise control.}
      \vspace{-1.5em}
      \label{histogram}
\end{figure}

\section{Experiments and Observations:}
We experimentally assesed the performance of the proposed vision only motion planning framework on a Franka Emika Panda Arm.

\subsection{Experimental details}

\subsubsection{Generating samples} We generated the required image state samples by collecting a large dataset created by actuating the planar joints (Joints $2,4,6$) of the Franka arm, using velocities computed from \autoref{comp_vel} as explained in \autoref{ssec:dataset}, covering the robot's planar workspace.

\subsubsection{ Generating the roadmap} The collected samples \key were used as nodes to generate roadmaps using the different proposed distances described in \autoref{ssec:dist_metric}. The roadmap generated by the image-space distance is coined as \textbf{Image Space} roadmap and the one generated using the learned distance is coined as \textbf{Learned} roadmap. For benchmarking purposes we also use the actual joint distance to create the Joint Space roadmap. The Joint Space roadmap used actual joint displacements from encoders solely for benchmarking, maintaining the proposed model-free framework. The k-neighbor value for all approaches was set to $25$.
 
\subsubsection{Obstacle Representation and Path Planning} In the experimental setup, obstacles were modeled as virtual yellow rectangles with a safety margin, as shown in \autoref{line-seg}. Paths were generated offline for various start (\keys) and goal (\keyg) incorporating the obstacle avoidance logic from \autoref{ssec:line-check}. These paths were later used in adaptive visual servoing experiments

\subsubsection{Control Experiment Set-up} 
 The real-time control experiments used an Intel Realsense D435i camera in an eye-to-hand setup for visual feedback. The Panda arm followed the planned paths in $16$ obstacle-free and $10$ obstacle-avoidance experiments. The performance of each proposed roadmap was evaluated by comparing the joint position changes between intermediate image states to those of the Joint Space roadmap. The controllerâ€™s ability to guide the arm along collision-free paths was evaluated for efficiency and effectiveness.

\subsection{Roadmap and Path Planning Experiments}
In this section, we analyze the joint displacements along the edges of the three roadmaps to evaluate their efficiency. Path planning experiments were conducted in both collision-free and obstacle-avoidance scenarios to compare the joint distances covered by the paths generated from each roadmap.

\subsubsection{Distribution of Joint Distances of Edges for Different Roadmaps}
\autoref{histogram} shows the histograms of joint displacements along the edges of the three roadmaps. The \textbf{Learned} roadmap closely aligns with the joint space roadmap with minor deviations, demonstrating reasonable accuracy in estimated joint displacements. In contrast, the \textbf{Image Space} roadmap exhibits a broader spread and larger joint displacements. This suggests that the Learned roadmap is more effective in accurately capturing transitions and generating paths that minimize joint movements.

\subsubsection{Average Joint Distances for Planned Collision-Free Paths}
\label{random-rm}
We randomly selected $100$ pairs of \keys and \keyg from the roadmaps for path planning without obstacles. The average joint distances over $100$ trials, summarized in \autoref{random_trials}, show that the \textbf{learned} roadmap yields results closer to the joint space while the \textbf{image space} roadmap results in significantly larger joint displacements. These findings suggest that the learned roadmap possibly generates more efficient path, better suited for successful control convergence.  

\begin{table}[t]
\caption{Average Joint Distances (Radians) for Collision-Free Paths}
\label{random_trials}
\centering
\resizebox{0.5\textwidth}{!}{%
\begin{tabular}{|m{2cm}||m{2cm}||m{2cm}||m{2cm}||}
\hline
& \multicolumn{1}{m{1cm}||}{\centering \textbf{Joint Space}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{Learned}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{Image} \\ \centering \textbf{Space}} \\
\hline
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{Mean} \\ \centering \textbf{(radians)}}  & \multicolumn{1}{m{1cm}||} {\centering $\textbf{1.74}$} & \multicolumn{1}{m{1cm}||}{\centering $\textbf{2.19}$} & \multicolumn{1}{m{1cm}|}{\centering $\textbf{3.06}$} \\
\hline
\end{tabular}
} 
\vspace{-0.75em}
\end{table}

\subsubsection{Planned paths for Control Experiments}

\label{ssec:path_planning}
We generated planned paths for two scenarios: $16$ start and goal pairs without checking for collision and $10$ pairs with collision avoidance. Paths were computed using the three roadmaps: Joint Space, Learned, and Image Space. These precomputed paths were used in the control experiments described in \autoref{all_control_exps}. 

For each planned path, we calculated metrics including the average number of waypoints (intermediate image states), joint distances between waypoints, total joint distances for the entire path, and Euclidean distances between image states (keypoint distances) both between waypoints and across the entire path. 

As observed in \autoref{exps_free_and_obs} the \textbf{learned} roadmap consistently resulted in fewer waypoints and shorter joint distances compared to the \textbf{image space} roadmap, which prioritizes minimizing keypoints distances in image space but incurs higher joint displacements.

Notably, the joint distances required to traverse $1000$ pixels in image space were much higher for the Image space roadmap than for the Learned roadmap. This suggests that reliance on image space proximity may lead to less efficient joint-space paths.

\begin{table}[t]
\caption{Comparison of Joint Distances and Keypoints Distance in Image Space over Experiments}
\label{exps_free_and_obs}
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{|m{2cm}||m{1cm}|m{1cm}|m{1cm}||m{1cm}|m{1cm}|m{1cm}|}
\hline
& \multicolumn{3}{c||}{\textbf{Without Collision-check}} & \multicolumn{3}{c|}{\textbf{With Collision-check}} \\
\hline
\textbf{Roadmaps} & \textbf{Joint Space} & \textbf{Learned} & \textbf{Image Space} & \textbf{Joint Space} & \textbf{Learned} & \textbf{Image Space} \\
\hline
\textbf{Number of Experiments} & \textbf{16} & \textbf{16} & \textbf{16} & \textbf{10} & \textbf{10} & \textbf{10} \\
\hline
\textbf{Avg No. of Waypoints} & \textbf{8} & \textbf{8} & \textbf{14} & \textbf{11} & \textbf{13} & \textbf{15} \\
\hline
\textbf{Avg. Joint Distances (radians) b/w Waypoints} & \textbf{0.25} & \textbf{0.3} & \textbf{0.32} & \textbf{0.28} & \textbf{0.28} & \textbf{0.35} \\
\hline
\textbf{Avg. Keypoints Distances (pixels) b/w Waypoints} & \textbf{167.99} & \textbf{174.76} & \textbf{84.11} & \textbf{139.53} & \textbf{129.13} & \textbf{89.36} \\
\hline
\textbf{Avg. Joint Distances (radians) over Entire Path} & \textbf{1.92} & \textbf{2.27} & \textbf{4.29} & \textbf{3.04} & \textbf{3.46} & \textbf{5.36} \\
\hline
\textbf{Avg. Keypoints Distances (pixels) over Entire Path} & \textbf{1222.42} & \textbf{1278.92} & \textbf{1157.00} & \textbf{1532.72} & \textbf{1569.37} & \textbf{1361.43} \\
\hline
\textbf{Joint Distance (radians) Traversed To Move 1000 pixels in image space} & \textbf{1.6} & \textbf{1.77} & \textbf{3.79} & \textbf{1.98} & \textbf{2.19} & \textbf{3.9} \\
\hline
\end{tabular}
} 
\vspace{-1.85em}
\end{table}
We have two theories from the above observations:

\begin{itemize}
    \item The larger number of waypoints in the image space roadmap arises from its Euclidean distance-based edge weights, causing A* to prioritize shorter pixel distances and select more intermediate nodes. In contrast, the learned roadmap, with joint displacement-based weights, produces more direct paths with fewer waypoints.
    \item The image states \key which are close in image space may be much further away in joint space as highlighted in \autoref{exps_free_and_obs}. This behavior may lead to increased overall joint movement where non-holonomy may exist in the joint space for the covered joint displacements. This may reduce control accuracy and hinder convergence to the target image state.
\end{itemize}


\subsection{Control Experiments}
\label{all_control_exps}
The adaptive visual servoing experiments\footnote{Planning and Control Experiments videos are available at \href{https://drive.google.com/file/d/1eOoP0dVFz85q4usiLzjlPdA5PTBA3UKU/view?usp=drive_link}{this link}. The details of how to use the link is in the supplementary ReadMe file} used precomputed paths from  \autoref{ssec:path_planning}, with control gains optimized to minimize rise and settling times while keeping overshoot within 5\%, by careful tuning.

In \autoref{performance_data}, the overall control metrics highlight that the image space roadmap succeeded in only $69.2$\% of cases, while the learned roadmap achieved a $100$\% success rate. However, the image space roadmap, when successful, showed faster transient responses compared to the Learned roadmap. \autoref{exps_success_failure} uses identical metrics as in \autoref{exps_free_and_obs}, categorizing experiments into successful and failed cases

Failures in the image space roadmap were characterized by large joint displacements relative to smaller image space distances, as noted earlier in \autoref{ssec:path_planning}. Optimal execution of the references generated by using image space distances requires all the keypoints to follow straight paths. This is not possible for the robot due to the non-holonomic constraints of its kinematics (if defined directly in image space). As a result, the robot deviates from the planned path. While the robot reaches the reference locations in most cases, the deviations cause it to go out of its workspace (due to joint limits) in some others. This effect is especially visible when obstacles exist in the workspace since the robot needs to travel near the edges of its workspace to avoid them. When estimated joint distances are used the resulting trajectories are more suitable to the robot's kinematics, which prevents such failures at large. 


\begin{table}[t]
\caption{Performance results for the control experiments with and without obstacle}
\label{performance_data}
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{|m{1cm}||m{1cm}||m{1cm}||m{1cm}||m{1cm}||m{1cm}||m{1cm}|}
\hline
& \multicolumn{3}{c|||}{\textbf{Without Collision-check}} & \multicolumn{3}{c|}{\textbf{With Collision-check}} \\
\hline
 \multicolumn{1}{|m{2cm}||}{\centering \textbf{Performance} \\ \centering \textbf{Metrics}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{Joint} \\ \centering \textbf{Space}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{Learned}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{Image} \\ \centering \textbf{Space}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{Joint} \\ \textbf{Space}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{Learned}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{Image} \\ \centering \textbf{Space}} \\
\hline
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{Successful} \\ \textbf{Experiments}} & \multicolumn{1}{m{1cm}||} {\centering \textbf{16/16}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{16/16}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{13/16}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{10/10}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{10/10}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{5/10}} \\
\hline
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{System} \\ \textbf{Rise time (s)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{74.92}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{97.53}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{94.75}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{105.38}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{125.38}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{90.46}} \\
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{System} \\ \textbf{Settling time (s)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{94.37}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{118.62}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{101.99}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{118.18}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{155.14}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{96.44}} \\
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{End effector} \\ \textbf{Rise time (s)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{74.86}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{97.53}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{94.39}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{105.02}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{125.38}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{90.46}} \\
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{End effector} \\ \textbf{Settling time (s)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{94.37}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{114.02}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{99.81}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{118.18}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{147.14}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{94.22}} \\
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{Overshoot (\%)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{1.94}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{2.61}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{1.89}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{1.93}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{2.36}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{1.35}} \\
\hline
\multicolumn{1}{|m{2cm}||}{\centering \textbf{Execution} \\ \textbf{time (s)}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{125.17}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{148.92}} & \multicolumn{1}{m{1cm}|||}{\centering \textbf{140.78}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{146.02}} & \multicolumn{1}{m{1cm}||}{\centering \textbf{201.68}} & \multicolumn{1}{m{1cm}|}{\centering \textbf{124.70}} \\
\hline
\end{tabular}
} 
\end{table}

\begin{table}[t]
\caption{Comparison of Joint Distances and Keypoints Distance in Image Space over Experiments for Successful and Failed Experiments}
\label{exps_success_failure}
\centering
\resizebox{0.45\textwidth}{!}{%
\begin{tabular}{|m{2cm}||m{1cm}|m{1cm}|m{1cm}||m{1cm}|m{1cm}|m{1cm}|}
\hline
& \multicolumn{3}{c||}{\textbf{Successful}} & \multicolumn{3}{c|}{\textbf{Failed (Image Space)}} \\
\hline
\textbf{Roadmaps} & \textbf{Joint Space} & \textbf{Learned} & \textbf{Image Space} & \textbf{Joint Space} & \textbf{Learned} & \textbf{Image Space} \\
\hline
\textbf{Number of Experiments} & \textbf{18} & \textbf{18} & \textbf{18} & \textbf{8} & \textbf{8} & \textbf{8} \\
\hline
\textbf{Avg No. of Waypoints} & \textbf{8} & \textbf{8} & \textbf{14} & \textbf{11} & \textbf{13} & \textbf{15} \\
\hline
\textbf{Avg. Joint Distances (radians) b/w Waypoints} & \textbf{0.25} & \textbf{0.3} & \textbf{0.32} & \textbf{0.27} & \textbf{0.29} & \textbf{0.35} \\
\hline
\textbf{Avg. Keypoints Distances (pixels) b/w Waypoints} & \textbf{166.77} & \textbf{163.41} & \textbf{86.03} & \textbf{135.17} & \textbf{143.24} & \textbf{86.35} \\
\hline
\textbf{Avg. Joint Distances (radians) over Entire Path} & \textbf{2.10} & \textbf{2.37} & \textbf{4.42} & \textbf{2.92} & \textbf{3.52} & \textbf{5.33} \\
\hline
\textbf{Avg. Keypoints Distances (pixels) over Entire Path} & \textbf{1307.02} & \textbf{1257.02} & \textbf{1210.93} & \textbf{1419.93} & \textbf{1691.26} & \textbf{1291.2} \\
\hline
\textbf{Joint Distance (radians) Traversed To Move 1000 pixels in image space} & \textbf{1.6} & \textbf{1.9} & \textbf{3.71} & \textbf{2.1} & \textbf{2.1} & \textbf{4.12} \\
\hline
\end{tabular}
} 
\vspace{-1.05em}

\end{table}


To summarize, both the \textbf{image space} and \textbf{learned} roadmaps exhibit unique benefits. The image space roadmap provides faster execution when successful but struggles in reliability and path convergence. The \textbf{learned} roadmap, using joint displacement-based distances, avoids non-holonomic constraints and ensures robustness, particularly in complex paths. The choice between the two ultimately depends on the specific application context.

\section{Conclusion and Future Work}
In conclusion, this work introduced a novel framework for collision-free motion planning of robotic manipulators that relied solely on visual features, eliminating the need for explicit robot models or encoder feedback. 

The \textbf{learned} roadmap offered smoother, more reliable transitions, and due to its joint displacement-based distance definition, the paths it generated maintained joint-space holonomy when it existed. In contrast, the paths produced by the \textbf{image space} roadmap sometimes failed to maintain joint-space holonomy, even when holonomy existed in the image space. However, the \textbf{image space} roadmap provided faster transient responses and simplicity, making it advantageous for applications where speed and computational efficiency were prioritized.

Future work will explore extending this approach to out-of-plane motion and incorporating real-world obstacles to develop a fully integrated control and manipulation pipeline.

\bibliographystyle{IEEEtran}
\input{root.bbl}
\end{document}
