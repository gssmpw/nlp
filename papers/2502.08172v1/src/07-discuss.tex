\section{DISCUSSION}
\subsection{Discussions on Intention Classification and Extraction}


\major{Our framework divides the code refinement task into two steps: \textbf{intention analysis} and \textbf{intention-guided refinement}. This separation allows for improvements in effectiveness by enhancing both steps individually. Ideally, we aim to extract intentions as concretely as possible, such as Explicit and Reversion Intentions, which simplify the following refinement process. However, when the intention is less concrete (e.g., for General Suggestions), the refinement process involves more understanding difficulties, making performance improvements less significant.}

\major{
Our results also indicate that inaccurate intention extraction can degrade performance compared to an end-to-end refinement approach (see RQ3). This highlights why we only extract high-level intentions for General Suggestions, as it is challenging to ensure fully accurate and concrete intention extraction in these cases. Extracting incorrect intentions can lead to misguided refinements, which we aim to avoid.
}

\major{
In the future, refining the categories of General Intentions could further enhance the refinement process. For example, if the categories are more concrete, a rule-based method can be easily designed, or weaker models may better understand and refine the code. An ideal scenario would be that we have a complete classification of intention categories, where each category is sufficiently concrete to allow the use of reliable rule-based methods or even very weak models for effective refinement.
Explicit Suggestions and Reversion Suggestions are prime examples of such concrete categories.}



\subsection{Threats to Validity}
\noindent \textit{Model Threats:} We only tested the 7B versions of open-source code models, DeepSeek7B and CodeQwen7B, due to the resource limit. However, based on the performance of general open-source models like GPT4o and DeepSeekV2, our Intention-based framework performs excellently in code refinement tasks compared to other prompt techniques.
\major{Another potential threat is the length of the prompt templates. While longer prompts can pose input challenges for some models, the longest prompt template used in our study (the RAG prompt) contains only 141 tokens. The fields for each case (e.g., \texttt{OriginalCode}, \texttt{ReviewLine}, \texttt{Intention}) typically remain below 200 tokens. Even when including three-shot examples, the total input length remains under 1,000 tokens. Therefore, the prompt length is unlikely to affect the validity of our results.}

 \noindent \textit{Data Threats:} We only selected CodeReviewer dataset because other datasets lack some data fields and do not provide the link to the original data, making it impossible to use the Intention framework. However, the CodeReviewer dataset has been used in many papers and is recognized as a relatively complete and objective dataset. In the future, we will also try to collect more comprehensive and higher-quality datasets.
\major{We also acknowledge the potential risk of data leakage, particularly when using LLMs. While it is challenging to entirely rule out the possibility of data leakage within LLMs, our experimental results demonstrate that utilizing the Intention framework consistently yields better outcomes compared to LLMs not using it. This indicates that even in scenarios where data leakage may occur, the framework's design and methodology provide a significant performance advantage.}

 \noindent \textit{Efficiency Threats:} Our framework involves multiple LLM calls for classification and code generation, potentially making it slightly less efficient than other prompt strategies. However, the model’s classification response speed is relatively fast, and for Explicit Code Suggestions and Reversion Suggestions, we only need one step LLM call. Therefore, the overall impact on efficiency is not significant.

% 模型的THREATS：我们只测试了7B版本的开源代码模型，deepseekcoder和code-qwen，对于更大参数量的代码模型是否会有其他性质，我们没有计算资源去测试。不过从开源通用模型的效果来看，如gpt4o和deepseek，我们的Intention框架相较于其他prompt技术可以很出色的完成code Refinement任务

% 数据的THREATS：我只选择了一个测试数据集，这是因为其他数据集没有提供原始数据难以完成数据补全，也就没法使用Intention框架的。不过这个测试集已经在很多论文中使用，是公认的比较完整客观的数据集。以后我们也会尝试收集更全面，质量更好的数据集。

% 效率的THREATS：因为我们框架调用了多次LLM进行分类和生成代码，整体效率可能会略低于其他prompt策略。不过用模型分类的响应速度比较快，而且对于Explicit code Suggestion，和Reversion Suggestion我们只需要通过一次LLM。所以整体效率影响不是很大。