\section{Introduction}

% Code review是什么。很重要。 耗费很多人力，需要自动化。

% code refine是什么。 已有一些工作证明了，LLM技术在coderefine任务中的效果不错。然而总体效果仍不理想。

% 有两方面原因，第一是输入数据不全，第二是prompt设计过于简单。

% 输入方面：传统的输入只是old code和review comment。但这是一个复杂的流程，developer是可以感知到当前项目的所有信息的，其中两个至关重要的信息是last diff和review line。对于一个review质疑上次commit的必要性，是需要revert到上次修改前的状态。而有些review只是简单的说删除这一行，这是需要review所指的具体哪一行的信息的。

% prompt方面：随着LLM prompt技术的发展，有更多的prompt技术在NLP领域和软工领域都有很好的效果。如问答，数学求解，代码生成，测试用例生成等。包括了single agent的 cot， fewshot， self-gen等，还包括了multi-agent的xxx，yyy等。这些技术会对code refine有什么效果，这些也是关键问题。

% 基于以上原因，我们先做了一个empirical study。尝试了在多输入信息的情况下，用现有个prompt技术的效果。得到以下结论
% 1. 对于不同的任务类型，需要的信息不同
% 2. 对于不同的任务类型，需要的prompt策略也不同

% 基于以上发现，我们设计了一种新的multi-agent的检测框架。首先将任务分类，然后对不同类型可以设计不同的输入，以及不同promnpt策略。最后，使用多轮检测的MA技术，提高了整体EM值。

% 我们的主要贡献有：
% 1. 重新定了一种coderefine任务。增加了新的输入信息，更加贴近真实场景，更加实用
% 2. 调研了现有prompt技术在
% 3. 提出一种framework，提高了coderefine效果

\section{Background}

\subsection{Code refine}
% 形式化定义, input: original code, last diff, review line, review comment
% output: new code


\subsection{Prompt Engineering}
% prompt 技巧包括：COT， one-shot, fewshot
% Single angent 和 Multi agent
% In 0-shot prompting, φ directly yields x
% In 0-shot CoT, φ supplements x with a general instruction, such as “[x] think step by step”.
% In few-shot CoT, φ supplements x with several labeled exemplars, {(xi,ri,ai)}K
% i=1, such as “[x1] [r1] [a1]...[xK ] [rK ] [aK ] [x]”.

\section{Empirical Exploration}
% 经验研究的目标是，分析现有prompt engineer方法在全量code refine任务上的效果。

% The empirical study aims to investigate LLM 在 code review 任务中的效果，有哪些优点和不足。为了达成这个目标，我们设计了以下RQ
% 数据收集流程：
% 1. 可以获得的信息：original code, last diff, review line, review comment等。现有数据集的情况。
% 2. 数据质量问题，即自动化处理的边界（观察review和commit之间，一对多，多对一的情况）
% 3. 数据分类：

% RQ1 现有prompt工程方法的效果
% 1 单轮prompt的技巧：COT, fewshot, retrival-fewshot，self-gen
% 2 multi-agent方法：6*6

% 结论：
% 1. 不同类型的任务，需要的信息不同，需要不同的prompt方法
% 3. 检测者能力很弱，难以利用多轮对话提高效果


\section{Method}

\section{Experimental Setup}

% RQ2 框架的效果
% 1 按类别设计的prompt的效果
% 2 按类别设计checker的效果
% 3 分类的效果
% 4 总体效果/每一轮的效果


\section{Experimental result}

\subsection{dataset}
\subsection{result}

\section{related work}

% 有研究表明，generating review comment任务往往只能生成一些简单的review comment，如replace operator，remove exception, rename等，平均comment长度在6个words左右。而对于一些复杂的任务，如refactoring，performance optimizations等，特别是那些长度多于14个单词的review comment，生成效果很差。

% 为了使得codereview自动化，许多工作主要围绕以下两个任务，生成评论和代码修改。生成评论是xxxx，代码修改是xxxxx。
% 最早，由Tufano实现了一个基于NMT的自动化，处理java code review的问题。后来Tu还做了xxxx等工作
% 后来，微软实现了预训练模型，可以深刻理解代码片段
% 总体而言，他们对于代码修改任务的输入还只是comment+oldcode。一方面这受限于当时的模型输入长度，另一方面

% codereview包括生成评论和代码修复两部分工作。comment生成任务是很难自动化的，一方面这是由于现有模型的能力不足。现有模型只有能力完成一些添加/移出异常，更改变量名等简单任务。对于真正设计核心的逻辑的修改毫无办法，最后一些已经上线到github的自动化comment生成的工具，被developer采纳的概率也不高。另一方面，这个问题应该前移到代码生成，或者代码检查任务中。完整的代码结构，项目信息可以加强模型对当前代码的理解能力，可以生成更好的comment。另外，developer可以立即得到comment，可以即时修改反馈。
% 我们的研究重点在于代码修复部分。代码修复是指xxxx。而代码修复的自动化包括了LLM和pretrained-model两种。
% 一些研究表明，相比pretainedmodel, LLM在代码修复中有很好的效果。主要有两个原因，第一是数据质量比较低，依次作为训练集会影响效果。第二是code refine任务需要理解当前代码内容以及review的意图，这中复杂的理解能力是LLM更擅长的。