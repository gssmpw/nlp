\section{related work}
Code refinement is a critical core component in the code review process, and numerous scholars have conducted research on the automation of code refinement. Tufano M.~\cite{tufano2019learning} were the pioneers in proposing the use of Neural Machine Translation (NMT) to learn the automated modification of Java methods based on review comments. Subsequently, Tufano R.~\cite{tufano2021towards} and Thongtanunam~\cite{thongtanunam2022autotransform} employed transformer~\cite{vaswani2017attention} models to train and enhance the original task’s performance. 
Tufano R.~\cite{tufano2022using} and Li~\cite{li2022automating} further advanced this field by using the Text-To-Text Transfer Transformer (T5) model~\cite{raffel2020exploring} and CodeT5 model~\cite{wang2021codet5}, pre-training code review-related tasks to enable the model to comprehend the meaning of the code and review comments. These approaches yielded significant improvements in downstream code refinement tasks.

% Compared to pre-trained models, large models exhibit significant advantages in understanding instructions and generating code. 
With the rise of LLMs, many researchers have attempted to leverage them in software engineering \cite{ma2024specgen, ma2024speceval, kong2024contrastrepair, guo2024ft2ra, xia2023keep}. In particular, Guo~\cite{guo2024exploring} explored using ChatGPT for code refinement tasks, uncovering some prompt design techniques.
% as well as the strengths and weaknesses of Large Language Models (LLMs) in this context. 
Tufano R.~\cite{tufano2024code} manually analyzed over 2,000 code refinement examples, evaluating three code review models~\cite{hong2022commentfinder, li2022automating, tufano2022using} and comparing their performance to ChatGPT. This analysis revealed that ChatGPT is highly competitive compared to previous methods. 
Pornprasit~\cite{pornprasit2024fine} experimented with various prompt strategies for LLMs in code refinement. They also fine-tuned ChatGPT using an API~\cite{ChatGPTblog}, enhancing the effectiveness of LLMs in this task.
% Pornprasit~\cite{pornprasit2024fine} experimented with various prompt strategies for LLM in code refinement, including zero-shot, few-shot, and persona-based approaches. Additionally, Pornprasit fine-tuned ChatGPT using an API~\cite{ChatGPTblog}, which further improved the effectiveness of LLMs in this task.

Some studies have also involved classifying code refinement tasks.
Tufano~\cite{tufano2024code} categorizes code refinement based on the type of task and examines the performance of pre-trained models on different task types. 
Kononenko~\cite{kononenko2016code} studied the time and effort required by programmers for different types of code review tasks. Bacchelli~\cite{bacchelli2013expectations} investigated the categories of code review tasks, focusing on developer motivation and response speed. Pascarella~\cite{pascarella2018information} explored the information needed for different types of code refinement tasks, but their classification method is more oriented toward human understanding rather than guiding model modifications.

% 另外，将code Refinement分类，也有很多研究工作。Tufano是从任务的类型角度来分类，查看pre-trained模型在不同任务类型上的效果。Kononenko[kononenko2016code]研究了不同类别的code review任务所消耗程序员的时间和精力。Bacchelli[bacchelli2013expectations]调研了不同code review任务类别，开发者的motivation和响应速度。Pascarella[pascarella2018information]研究了不同类别的code Refinement任务的所需信息，不过他们的分类方式更偏重于让人类理解，而不是指导模型进行修改。


% Code Refinement自动化任务：code Refinement任务作为code review流程中的关键核心任务，有很多学者进行过code Refinement自动化相关研究。最早由Tufano M.等人[tufano2019learning]提出使用NMT去学习根据review comment自动化的修改java method。而后Tufano R.等人[tufano2021towards],Thongtanunam等人[thongtanunam2022autotransform]分别使用transformer模型训练并提升了原任务的效果。Tufano R.[tufano2022using]进一步使用Text-To-Text Transfer Transformer (T5)模型[raffel2020exploring]，allowing the model to work with raw source code by keeping under control the vocabulary size, 解决了之前工作需要将变量名做化简代替的问题。而后，Li等人[li2022automating]首先使用了pre-trained模型，设计了 Diff Tag Prediction，Denoising Objective，Review Comment Generation等三个预训练任务，在CodeT5模型[wang2021codet5]的基础上预训练code review相关任务，让模型理解代码的含义，以及代码和review comment的对应关系。并在下游code Refinement任务上取得了很好的效果。

% 大模型for code Refinement任务：相较于预训练模型，大模型在理解指令，和生成代码方面具明显优势。随着大模型的兴起，很多研究者也尝试用大模型解决code Refinement任务。Guo[guo2024exploring]尝试用ChatGPT处理code Refinement任务，发现了一些prompt设计技巧，以及LLM在此任务上的优势和不足。Tufano R.[tufano2024code]手工分析了2000多个code Refinement例子，评估了三种code review模型[hong2022commentfinder, li2022automating, tufano2022using]并与ChatGPT的效果对比，发现ChatGPT相较于之前的方法具有很强的竞争力，并且通过引导模型先去思考问题类型，再做修改的COT方法，可以进一步提升修复效果。Pornprasit[pornprasit2024fine]尝试了大模型解决code Refinement问题时的多种prompt策略，包括zeroshot，fewshot和Persona，并且用API的方式finetune了ChatGPT，可以进一步提升大模型的效果。