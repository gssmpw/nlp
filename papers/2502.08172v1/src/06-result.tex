\section{experimental results}

\subsection{RQ1: Intention Extraction Accuracy}

\begin{table}[!t]
\caption{\major{Results of intention accuracy.}}
% \vspace{-4mm}
\footnotesize
\centering
\label{tb:rq1-1}
\resizebox{.4\textwidth}{!}{
\begin{tabular}{ccccc}
\hline
           & Explicit & Reversion        & General          & All clean data   \\ \hline
\#Samples  & 175      & 308              & 854              & 1337             \\
\major{GPT3.5}     & -        & \major{84.42\%}          & \major{49.18\%}          & \major{63.95\%}          \\
GPT4o      & -        & \textbf{99.03\%} & \textbf{66.86\%} & \textbf{78.61\%} \\
\major{DeepSeekV2} & -        & \major{95.45\%}          & \major{62.88\%}          & \major{75.24\%}          \\
\major{DeepSeek7B} & -        & \major{80.19\%}          & \major{46.72\%}          & \major{61.41\%}          \\
\major{CodeQwen7B} & -        & \major{82.14\%}          & \major{43.91\%}          & \major{60.06\%}          \\ \hline
\end{tabular}
}
\vspace{-4mm}
\end{table}




\major{
We manually annotated the data to select valid samples, with 1,337 out of a total of 2,000 identified as valid. To ensure annotation quality, we enlisted two senior PhD candidates specializing in code learning to conduct the manual labeling. Disagreements between the annotators were observed in 249 instances, and inter-rater reliability was assessed using Cohen’s Kappa coefficient, which yielded a value of 0.719, indicating acceptable agreement.
Subsequently, we used various models to generate intentions using our framework, and then manually assessed each generated intention for its correctness. Notably, since the Explicit category is classified using predefined rules, we just measure the accuracy of the reversion intention and general intention. 
% As shown in Table~\ref{tb:rq1-1}, 1,337 cases out of the total 2,000 samples were identified as valid. We used different models to predict the intentions and then manually checked the accuracy of these predictions.
% To ensure annotation quality, we invited two senior PhD candidates specializing in code learning to manually label the data. There were 249 instances of disagreement between the annotators, and the inter-rater reliability was evaluated using Cohen's Kappa coefficient, which yielded a value of 0.719, indicating acceptable agreement.
}

% As shown in Table~\ref{tb:rq1-1}, there are 1,337 clean cases, accounting for 66.9\% of the total 2,000 samples. 
\major{As shown in Table~\ref{tb:rq1-1}, we observe that GPT-4o and DeepSeekV2 are significantly more effective than the other three models. Among these, Reversion Intention is relatively easier to extract, with all models achieving at least 80\% accuracy, while GPT-4o achieves an impressive 99.03\%.}


It is noteworthy that 20\% of Reversion Suggestions overlap with General Suggestions, representing a dual classification challenge. In these instances, predicting the intention as either category is considered correct. 
For General Suggestions cases, the highest accuracy is 66.86\%, indicating that this category presents more significant challenges for the model to comprehend. Unlike the other categories, General Suggestions lack specific patterns or structured cues, making it more difficult for the model to pinpoint the exact intention behind the suggestions. This lower accuracy highlights the complexity and ambiguity inherent in general suggestions, which often require a deeper understanding of the context and the underlying reasoning behind the recommendations.



% Among these clean cases, the intention can be correctly predicted in 1,051 cases, which is 78.61\% of the clean samples. This result highlights the ability of the Intention-based Framework to effectively identify and predict intentions in a significant majority of the clean data. Overall, intention correctly predicts 1,371 cases, accounting for 68.6\% of the total samples, demonstrating its overall reliability across different types of suggestions.

% For Explicit Code Suggestions cases, the accuracy of intention prediction is 100\%. This high accuracy is achieved by employing regular expressions to determine whether a case belongs to this category. The rule-based prediction method ensures that each explicit suggestion is captured without errors, reflecting the structured and well-defined nature of these suggestions. Such a robust approach leaves no room for mistakes, making it a perfect classification for these cases.


% In the case of Reversion Suggestions, the accuracy is 99.03\%, reflecting a strong performance with a minimal margin for error. 





% The overall performance of the Intention-based Framework illustrates its potential in accurately predicting intentions across various suggestion types. The ability to achieve 100\% accuracy in Explicit Code Suggestions and near-perfect results in Reversion Suggestions reflects its robustness and precision in structured scenarios. However, the challenges faced in General Suggestions indicate areas for improvement, inviting further exploration into more sophisticated methods for handling complex and ambiguous cases. 

% As shown in Table~\ref{tb:rq1-1}, there are 1,337 clean cases, accounting for 66.9\% of the total 2,000 samples. Among these clean cases, the intention can be correctly predicted in 1,051 cases, which is 78.61\% of the clean samples. Overall, intention correctly predicts 1,371 cases, accounting for 68.6\% of the total samples.

% For Explicit Code Suggestions cases, the accuracy of intention prediction is 100\%, as we use regular expressions to determine whether a case belongs to this category. This rule-based prediction method is error-free.
% For Reversion Suggestions cases, the accuracy is 99.03\%. Notably, 20\% of Reversion Suggestions are also General Suggestions. For these dual classification tasks, predicting the Intention as either category is considered correct.
% For General Suggestions cases, the accuracy is only 66.86\%, indicating that this category is more challenging for the model to understand.

% Overall, for valid samples, the intention extraction accuracy is nearly 80\%, which is very effective. This suggests that using intention to replace review comments in the subsequent task is highly feasible.

% While observing the intention, we found that some cases, although correct, require significant effort to understand the reviewer's intent, even for humans. However, intention can help clarify the task clearly.

% **Case 1:** The review comment, “Any reason not to fold the `currentComponent` assignment into that?”, might be interpreted as asking the developer to comment on the `options.diffed = ()` part of the code, explaining why it is assigned this way. However, considering the previous modifications, the reviewer's real intent is to inquire about the reason for adding the code, implying a suggestion to possibly revert the last modification. Intention can directly discern this meaning and make the correct understanding.

% Additionally, we found during manual labeling that code refinement is a flexible task, often resulting in multiple correct answers for a single task.

% **Case 2:** The review comment, “set the scheduledExecutorService to null?”, can be interpreted in two ways: replacing `this.getScheduledExecutorService().shutdown();` with `this.scheduledExecutorService = null;`, or adding a line to set the service to null after the previous shutdown step. Although the intention and revised code might differ in their representation, both modifications are reasonable to an ordinary programmer.

\vspace{5pt}\noindent \fbox{
\parbox{0.95\linewidth}{\textbf{Answers to RQ1}: \major{GPT-4o achieves the highest accuracy in intention extraction, with DeepSeekV2 performing competitively (with 3\% less). General intentions, however, are more challenging to extract due to the inherent complexity and the often ambiguous nature in the review comments.}}
}


% 从表1可以看出，合理的case共1337个，占样本总数2000个的66.9%。而合理的case中，Intention可以预测正确的有1051个，占合理样本中的78.6%。对总体样本而言，Intention共预测对了1371个，占总体样本的68.6%。接下来具体分析每一个类别的正确率：Explicit code Suggestion类别的Intention预测准确率是100%，这是因为我们是通过正则表达式来判断是否属于第一类，这种rule-based的预测方法不会出错。Reversion Suggestion类别的Intention预测量是99.03%，值得注意的是，Reversion Suggestion有20%的数据同时也是General Suggestion的。也就是说，对于这些数据，如果理解成为要求退回上次修改是正确的Intention，如果按照Intention具体的执行change xxx code to xxx，也是可以正确修复的。最后，对于General Suggestion类别，Intention正确率只有66.86%，说明这一类理解比较困难。总体而言，对于合理样本，Intention的提取正确率接近80%具有非常好的效果，这说明使用Intention在接下来生成revised code步骤中代替review comment，具有较高可行性。

% 我们在观察Intention时，发现了一些case虽然是正确的，但是从即使人来做，也需要很大的努力才能理解其reviewer的意图。然而，Intention可以帮助人很明确的理解任务。
% 在case1中，如果通过review comment：“Any reason not to fold the `currentComponent` assignment into that?”理解，可能是需要developer对options.diffed = () 这部分代码进行注释，解释为什么要在这样赋值。然而综合考虑上一次的修改情况，review的真实意图是询问这次添加代码的原因，隐含了建议是可能要退回上次修改。而Intention可以直接洞察这种含义，做出正确的理解。

% 另外我们在人工标注时发现code Refinement是很灵活的任务，经常会出现一个任务存在多种正确回答的现象。
% 在case2中，review comment：“set the scheduledExecutorService to null?”，可以理解成把this.getScheduledExecutorService().shutdown();代码替换成this.scheduledExecutorService = null;。也可以理解成在前一步shutdown之后，再添加一行代码将service设为null。即使Intention和revised code表现结果不同，但是普通程序员看来，这两种修改都是合理的。

% 总之，Intention对合理数据的理解正确率高达78.61%，可以作为理解程序的一个可靠方式。


% \begin{table}[!t]
% \caption{Comparative Results with Pre-trained Models.}
% % \vspace{-4mm}
% \footnotesize
% \label{tb:rq2-2}
% \resizebox{.49\textwidth}{!}{
% \begin{tabular}{ccccccccc}
% \hline
%    & \multicolumn{5}{c}{Intention-based   Framework}           &  & \multicolumn{2}{c}{Pre-trained Model} \\ \cline{2-6} \cline{8-9} 
%    & GPT3.5 & GPT4o & DeepSeekV2     & DeepSeek7B & CodeQwen7B &  & CodeReviewer          & T5CR          \\ \hline
% EM & 53.40  & \textbf{64.77} & \major{64.25} & 49.29      & 48.47      &  & 52.13                 & 18.84         \\ \hline
% \end{tabular}
% }
% \vspace{-4mm}
% \end{table}



\begin{table*}[!t]
\caption{\major{Comparative results between our intention-based framework and LLM-based baselines.}}
% \vspace{-4mm}
\footnotesize
\label{tb:rq2-1}
% \vspace{-2mm}
\resizebox{.99\textwidth}{!}{
\begin{tabular}{ccccccccccccccccccccccccc|cc}
\hline
           & \multicolumn{3}{c}{Simple Prompt} &  & \multicolumn{3}{c}{Simple COT} &  & \multicolumn{3}{c}{Tufuno COT} &  & \multicolumn{3}{c}{Random fewshot} &  & \multicolumn{3}{c}{RAG} &  & \multicolumn{3}{c}{Self-generated} &  & \multicolumn{2}{c}{Intention   Framework with RAG} \\ \cline{2-4} \cline{6-8} \cline{10-12} \cline{14-16} \cline{18-20} \cline{22-24} \cline{26-27} 
           & Basic     & P-A       & Comp.     &  & Basic    & P-A      & Comp.    &  & Basic    & P-A      & Comp.    &  & Basic      & P-A       & Comp.     &  & Basic  & P-A    & Comp. &  & Basic      & P-A       & Comp.     &  & Same             & \major{GPT4o Inte.}            \\ \hline
GPT3.5     & 38.00     & 38.29     & 36.42     &  & 34.85    & 36.87    & 32.76    &  & 19.97    & 17.20    & 11.74    &  & 45.25      & 48.62     & 32.39     &  & 46.45  & 49.89  & 33.96 &  & 31.86      & 25.88     & 27.97     &  & 53.40                & \major{\textbf{58.86}}              \\
GPT4o      & 41.14     & 46.60     & 29.54     &  & 39.72    & 43.68    & 26.25    &  & 20.12    & 24.98    & 13.76    &  & 49.21      & 52.88     & 35.00     &  & 49.59  & 54.08  & 36.95 &  & 51.01      & 56.84     & 34.41     &  & 64.77                & \major{\textbf{64.77}}              \\
DeepSeekV2 & 34.63     & 38.07     & 27.60     &  & 41.29    & 46.37    & 31.49    &  & 24.31    & 27.60    & 17.43    &  & 46.45      & 52.66     & 42.41     &  & 48.09  & 55.35  & 45.62 &  & 44.95      & 53.03     & 38.37     &  & 64.25                & \major{\textbf{68.06}}              \\
DeepSeek7B & 28.50     & 34.48     & 24.38     &  & 29.92    & 36.28    & 24.01    &  & 5.76     & 6.24     & 3.60     &  & 33.28      & 39.49     & 25.58     &  & 36.05  & 41.59  & 25.88 &  & 7.26       & 8.68      & 5.01      &  & 49.29                & \major{\textbf{56.40}}              \\
CodeQwen7B & 26.10     & 33.13     & 19.22     &  & 25.06    & 31.04    & 16.98    &  & 2.85     & 3.29     & 1.97     &  & 35.75      & 40.99     & 17.80     &  & 35.83  & 44.05  & 20.12 &  & 13.69      & 19.67     & 7.93      &  & 48.47                & \major{\textbf{54.75}}              \\ \hline
\end{tabular}
}
\vspace{-4mm}
\end{table*}

\subsection{RQ2: Intention-based Framework Effectiveness}\label{sec:rq2}

\major{Table~\ref{tb:rq2-1} presents a comparison between our method and the LLM-based baselines. For the LLM baselines, we made extensive efforts to optimize their performance by setting multiple configurations, including six different prompts and three types of inputs (i.e., basic inputs, position-aware code refinement, and comprehensive code refinement; details are provided in Section~\ref{sec:background}).
For our method, we show the results of the RAG-based approach and the results with other prompts are shown in Table~\ref{tb:rq3-1}. Note that we present two types of results for our method: (1) using the same model for both intention extraction and code refinement (Column \textit{Same}) and (2) extracting accurate intentions with GPT-4o and generating revised code using other LLMs based on these intentions (Column \textit{GPT4o Inte.}).}



% We then compare the performance of other prompt methods based on large models.
From Table~\ref{tb:rq2-1}, we can observe that using the Intention-based method improved the performance of all models compared to all baselines. The model with the highest improvement was DeepSeekV2, which saw an increase from 55.35\% to 64.25\%, a 9 percentage points improvement. The model with the least improvement was GPT3.5, which increased from 49.89\% to 53.40\%, a mere 3 percentage points improvement.  We provide several representative examples to demonstrate the advantages of the Intention-based framework, which are available on our website~\cite{IntentionWebsite}.
% 我们展示了一些有代表性的例子以说明Intention-based framework的优势，相较于其他baseline方法，做到了理解任务再完成任务。限于文章篇幅，我们将例子放到网站中。

\noindent
\major{
\textbf{From the Perspective of Intention Quality}:
Comparing the results of our method using intentions generated by the model itself and those generated by GPT4o, it is evident that intention quality plays a critical role. While using the intentions generated by the same model (Column \textit{Same}) generally improves performance compared to LLM baselines, leveraging higher-quality intentions (Column \textit{GPT4o Inte.}) further enhances the results. This is due to the fact that some models, as shown in Table~\ref{tb:rq1-1}, are not as effective as GPT4o in extracting accurate intentions. Surprisingly, we also observed that DeepSeekV2 outperformed GPT4o (68.06 VS 64.77) when provided with the same intentions (from GPT4o). This demonstrates that, while DeepSeekV2 may not excel in intention extraction, it has superior code refinement capabilities when given accurate intentions. The results underscore the strength of our framework, which allows for separate optimization of the agents responsible for intention understanding and code refinement under a given intent. A similar trend is observed for other prompts, as shown in Table~\ref{tb:rq3-1}.
}

\noindent
\textbf{From the Perspective of Input Complexity:} By comparing the baseline results, we found that across all six prompt strategies, the Position-Aware Code Refinement task performed the best, followed by the Basic Code Refinement task, and lastly the Comprehensive Code Refinement task. Although the Comprehensive Code Refinement task provided the most information, including all necessary data, all models and prompt methods struggled to effectively utilize this information. This may be due to the complexity of understanding \texttt{LastCodeDiffHunk} and the potential for overly long prompts to reduce model focus, impairing comprehension of key information. Incomplete input information reduces model effectiveness, while excessive input information hampers task understanding and reduces effectiveness. The Position-Aware Code Refinement task, which includes \texttt{OriginalCode}, \texttt{ReviewComment}, and \texttt{ReviewLine}, balances the completeness of information with the model's processing capability (5 percentage points better than Basic Code Refinement and about 10 percentage points better than Comprehensive Code Refinement in general). 

% We observed that under prompt strategies like Simple Prompt, Simple COT, Random few-shot, and RAG, the Position-Aware Code Refinement task generally performed about 5\% better than Basic Code Refinement and about 10\% better than Comprehensive Code Refinement.

\noindent
\textbf{From the Perspective of Prompt Strategy:} 
% discrepancies between our implementation and the described methodology in the paper, as we did not have access to the original code. Additionally, the prompt design lacked detailed execution steps, failing to enhance model performance effectively.
Except for the DeepSeekV2 model, other models showed only slight improvements with the Simple COT Prompt method over the Simple Prompt, and even showed declines for GPT3.5 and GPT4o in the Basic task. This indicates that for most models like GPT3.5, GPT4o, DeepSeek7B, and CodeQwen7B, Simple COT Prompt without clear step-by-step guidance does not significantly enhance code refinement tasks. However, for the DeepSeek model, using COT significantly improved accuracy in Basic, Position-Aware, and Comprehensive tasks by up to 8 percentage points, demonstrating DeepSeekV2's strong reasoning capabilities. 
% The Tufano COT method performed significantly worse than others. This may be due to that the prompt design lacks detailed steps, failing to enhance model performance effectively.

Consistent with our intuition, the Random Few-Shot Prompt outperformed the Simple COT Prompt and Simple Prompt. Furthermore, RAG demonstrated superior performance compared to the Random Few-Shot Prompt across almost all models and tasks. \major{This indicates that retrieving more similar examples is highly beneficial, as it provides refinement guidance that aligns closely with both the format and the content of the task.
Therefore, we recommend that users intending to use the RAG prompt for code review tasks construct the retrieval database using a relevant historical code review dataset. Relevance can be assessed from various perspectives, including project similarity, task similarity, author alignment, dataset quality, and fine-grained intention matching. Specifically, retrieved data from the same or similar project, addressing similar tasks, or authored by the same individuals or the same group is likely to offer better guidance for new code refinement tasks, as these examples share meaningful similarities and contextual relevance.}


For Self-generation Prompt, compared to the RAG method, it showed a slight improvement for the GPT4o model, a slight decline for the DeepSeekV2 model, and a significant decline for the other three models. Notably, GPT4o and DeepSeekV2, the two largest models with the best overall performance in other prompt methods, benefited from Self-generation. This suggests that Self-generation Prompt is more effective for models with large parameter sizes and strong reasoning capabilities. 



Last, we compared our method with pre-trained models, specifically CodeReviewer~\cite{li2022automating} and T5CR~\cite{tufano2022using}, which achieved 52.14\% and 18.84\% accuracy, respectively. As expected, our intention-based method demonstrates superior effectiveness, achieving significantly higher performance due to the use of LLMs and the incorporation of intention extraction (e.g., 64.77\% with GPT4o).


% We also compared our method with that of the pre-trained models, i.e., CodeReviewer and T5CR, which achieve 52.14\% and 18..84\%, respectively. It is expected that our intention-based method is more effective due to the use of LLMs and the intention extraction. (e.g., 64.77\% with GPT4o).

% It is expected that 
% According to Table~\ref{tb:rq2-2}, we observe that all five models perform better with the Intention-based framework than with the pre-trained models. Among them, the GPT4o model achieves the best performance, reaching 64.77\%, which is 12.64 percentage points higher than the CodeReviewer model (52.13\%). 

\vspace{5pt}\noindent \fbox{
\parbox{0.95\linewidth}{\textbf{Answers to RQ2}: The results show that the Intention-based framework outperforms the existing baselines, including both pre-trained models and LLMs with diverse prompts. 
\major{High-quality intentions play a critical role in achieving effective code refinement.}
% Specifically, for the DeepSeekV2 model, the framework achieves an Exact Match (EM) accuracy of 68\%, which is 13\% higher than the baseline. 
% Additionally, we observed that among these baseline methods, using large models for the code refinement task yields the best performance for the Position-Aware Code Refinement task, and the most effective prompt strategy is RAG.
}
}


\begin{table}[!t]
\centering
\caption{\major{Results of our method with different prompts.}}
\footnotesize
\label{tb:rq3-1}
\resizebox{.49\textwidth}{!}{
\begin{tabular}{cccclccc}
\hline
           & \multicolumn{3}{c}{Our Method (Same)}                    &  & \multicolumn{3}{c}{\major{Our Method (GPT4o   Inte.)}} \\ \cline{2-4} \cline{6-8} 
           & Simple Prompt  & RAG            & Self-generated &  & \major{Simple Prompt}  & \major{RAG}             & \major{Self-generated} \\ \hline
GPT3.5     & 53.03          & \textbf{53.40} & 43.38          &  & \major{57.14}          & \major{\textbf{58.86}}  & \major{46.60}          \\
GPT4o      & 64.10          & 64.77          & \textbf{65.97} &  & \major{64.10}          & \major{64.77}           & \major{\textbf{65.97}} \\
DeepSeekV2 & 60.88          & \textbf{64.25} & 61.03          &  & \major{61.03}          & \major{\textbf{68.06}}  & \major{64.10}          \\
DeepSeek7B & \textbf{50.04} & 49.29          & 26.55          &  & \major{54.45}          & \major{\textbf{56.40}}  & \major{27.75}          \\
CodeQwen7B & 46.00          & \textbf{48.47} & 29.32          &  & \major{50.11}          & \major{\textbf{54.75}}  & \major{27.45}          \\ \hline
\end{tabular}
}
\vspace{-4mm}
\end{table}

% 从表2中，我们首先可以看到，所有模型，使用Intention-based方法对比所有baseline均有提升。提升最高的模型是deepseek,从55.35%提升到了68.06%提升了13%。提升最少的模型是GPT3.5，从49.89%提升到53.03%只提升了3%。

% 我们也从baseline的结果，得到更多的结论。

% 从任务角度观察：我们通过对比baseline的结果发现：在所有6种不同的策略下，Position-Aware code Refinement任务都是最优的，其次是Basic code Refinement，最后是comprehensive code Refinement。虽然comprehensive code Refinement任务提供了最多的信息，提供了全部的数据所需要的信息，但是所有模型，所有prompt方法都不能很好的理解这些信息。这一方面可能是由于code diff是一种难以理解的结构，另一方面可能是因为过长的prompt会降低模型的注意力，反而降低了对关键有效信息的理解。
% 输入信息不全会降低模型的效果；输入信息太多会影响模型对任务的理解，也会导致降低模型的效果。Position-Aware code Refinement任务输入是original code，review comment和review line信息，很好的平衡了信息完整程度和模型对信息的处理能力。我们观察到simple prompt，simple cot，Random fewshot，RAG等prompt策略之下，大部分情况Position-Aware code Refinement任务效果比Bacis code Refinement的效果高5%左右，比comprehensive code Refinement效果高10%左右。

% 从prompt策略角度观察：Tufuno COT的效果远低于其他，这一方面是因为我们是根据她论文描述而写的，没有找到她的代码实现，可能跟他的实现有出入。另一方面是因为prompt设计时也没有详细执行步骤，未能有效提高模型效果。

% 另外可以观察到，除了deepseek模型以外，其他模型在simple COT方法的效果只是略优于simple prompt，甚至对于Basic任务中，GPT3.5和GPT4o的效果还有下降。这说明对于GPT3.5，GPT4o，deepseekcode，code-qwen等模型，没有明确步骤指导的COT对code refinement任务提升不大。而对于deepseek模型，在Basic，P-Aware，Comprehensive等三个任务上，使用COT可以显著提升准确率，最多8%。这说明deepseek具有较强的推理能力。

% 与我们的直觉一致的，random fewshot的效果好于simple cot和simple prompt。而RAG的效果也好于random fewshot。这对几乎对所有模型和所有任务都成立。

% 最后，我们尝试了self-generation方法。相比于RAG方法，我们发现只有对GPT4o模型效果有小幅提升，deepseek模型小幅下降，其他三个模型效果都大幅下降。恰巧GPT4o和deepseek是参数量最大的两个模型，也是在其他prompt方法综合效果最好的两个模型。这说明只有对参数规模较大，自身推理能力较强的模型，self-generation方法才能充分发挥效果。

% 总之，我们在5个模型，3种类型的code Refinement任务上充分比较了现有的6种prompt方法的效果。发现对模型的输入信息不能过多也不能过少，要平衡信息完整性和模型理解能力，效果最好的任务是Position-Aware。还发现了对于一般的模型RAG是最优的prompt策略，而对于参数量大推理能力强的模型，self-generation也是很好的prompt策略。而我们提出的Intention框架，充分利用了所有信息，效果比目前所有的prompt策略都要高3%-13%。

% % 设置表格中所有文本颜色为蓝色
% \begingroup
% \color{blue}  % 将以下内容颜色设置为蓝色
\begin{table*}[!t]
\caption{\major{Ablation results on different intention-based components.}}
\footnotesize
\label{tb:rq3-3}
\resizebox{.99\textwidth}{!}{
\major{
\begin{tabular}{cccccccccccccccc}
\hline
           & \multicolumn{3}{c}{Our Method (with Intention)}         &  & \multicolumn{3}{c}{w/o Explicit  Intention} &  & \multicolumn{3}{c}{w/o Reversion   Intention} &  & \multicolumn{3}{c}{w/o General Intention} \\ \cline{2-4} \cline{6-8} \cline{10-12} \cline{14-16} 
           & Simple Prompt & RAG            & Self-generated &  & Simple Prompt    & RAG             & Self-generated   &  & Simple Prompt    & RAG             & Self-generated    &  & Simple Prompt         & RAG                 & Self-generated        \\ \hline
GPT3.5     & 57.14         & \textbf{58.86} & 46.60          &  & 56.25(-0.9)      & 56.39(-2.47)    & 42.63(-3.96)     &  & 49.59(-7.55)     & 51.6(-7.26)     & 35.53(-11.07)     &  & 51.53(-5.61)          & 58.41(-0.45)        & 44.05(-2.54)          \\
GPT4o      & 64.10         & 64.77          & \textbf{65.97} &  & 64.03(-0.07)     & 63.65(-1.12)    & 65.45(-0.52)     &  & 59.61(-4.49)     & 60.21(-4.56)    & 61.41(-4.56)      &  & 57.37(-6.73)          & 62.3(-2.47)         & 65.75(-0.22)          \\
DeepSeekV2 & 61.03         & \textbf{68.06} & 64.10          &  & 60.58(-0.45)     & 65.52(-2.54)    & 61.41(-2.69)     &  & 55.27(-5.76)     & 61.85(-6.21)    & 56.32(-7.78)      &  & 52.35(-8.68)          & 63.95(-4.11)        & 63.05(-1.05)          \\
DeepSeek7B & 54.45         & \textbf{56.40} & 27.75          &  & 53.18(-1.27)     & 53.48(-2.92)    & 22.59(-5.16)     &  & 45.85(-8.6)      & 44.2(-12.19)    & 8.97(-18.77)      &  & 48.62(-5.83)          & 52.51(-3.89)        & 29.39(+1.65)          \\
CodeQwen7B & 50.11         & \textbf{54.75} & 27.45          &  & 46.97(-3.14)     & 52.81(-1.94)    & 19.75(-7.7)      &  & 38.0(-12.12)     & 46.6(-8.15)     & 6.96(-20.49)      &  & 47.8(-2.32)           & 54.75(0.00)          & 39.72(+12.27)         \\ \hline
\end{tabular}
}
}
\end{table*}
% \endgroup


\subsection{RQ3: Ablation Study on Different Intention Categories}

\major{Table~\ref{tb:rq3-3} presents the ablation study results. The first column shows the performance with all intention-handling components included, while the subsequent columns illustrate the effects of removing intention-handling for each category (i.e., removing each agent in Fig.~\ref{fig:framework}). For example, removing the Explicit Intention component means that code reviews with explicit suggestions are instead handled by Agent 2 and Agent 3. Similarly, removing the Reversion Intention redirects code reviews with reversion suggestions to Agent 1 and Agent 3. For Agent 3, the general intention extraction is removed, and LLMs are used to handle these cases directly.
The results indicate that removing any of these intention components leads to performance drops across almost all scenarios, demonstrating the importance of each component. Notably, removing the Reversion Intention results in significant declines in EM scores across all models and prompt methods, with reductions ranging from 5 to 15 percentage points, highlighting its critical role in improving this category of code reviews.
}

\major{An exception is observed with the Self-generated prompt under the condition of ``w/o General Intention'', where performance increases slightly. This is likely because Self-generated prompts tend to perform worse with LLMs that have weaker reasoning capabilities, as discussed in RQ2. However, the Simple Prompt shows a significant drop, indicating that the intention-based method provides substantial improvements when using very basic prompts. In contrast, RAG-based and Self-generated prompts exhibit relatively smaller performance declines. These advanced prompts compensate for the models' limitations in implicitly understanding intentions, even when intentions are not explicitly provided.}

% We conducted ablation experiments on the three agents used in the framework, and the results are presented in Table 6. First, analyzing the impact of Agent1 on the Exact Match (EM) scores, we observe that removing Agent1 causes a decline in the EM values for all models and all prompt methods. Notably, weaker models such as GPT-3.5, CodeQwen7B, and DeepSeek7B exhibit a significant drop of approximately 6 percentage points, whereas stronger models like GPT-4o and DeepSeekV2 experience a smaller decrease of 1-3 percentage points. 
% Next, examining the impact of Agent2, we find that its removal leads to a substantial decline in the EM scores across all models and prompt methods, with reductions ranging from 5 to 15 percentage points. 
% Finally, considering the influence of Agent3, we observe that it provides a notable improvement for stronger models when using the Simple Prompt approach (e.g., GPT-4o and DeepSeekV2 improve by 6.8 and 8.5 percentage points, respectively) and a slight improvement when using the RAG method (e.g., GPT-4o and DeepSeekV2 improve by 2.4 and 0.3 percentage points, respectively). However, for weaker models, Agent3 only shows a slight benefit for Simple Prompt (e.g., GPT-3.5 and DeepSeek7B), while for the RAG method, its influence is negligible or even detrimental in most cases. This outcome can be attributed to Agent3's ability to extract intentions that help the models better understand the reviewers' goals under the Simple Prompt setting. In contrast, the RAG method compensates for the models' inherent deficiencies in interpreting intentions, reducing the added value of Agent3. 
% Further analysis reveals that weaker models occasionally produce incorrect intention extraction in Agent3. When replacing the extracted intentions with those generated by GPT-4o, all models' performance improves, especially for the RAG method, where all models see an improvement of more than 4 percentage points.
% These findings highlight that Agent1 and Agent2 significantly enhance the results, while Agent3 offers improvements primarily for stronger models. However, for weaker models, particularly under the RAG method, Agent3 provides limited or no benefit.}


\begin{table*}[!t]
\caption{\major{Improvement results with different intention-based components.}}
\footnotesize
\label{tb:rq3-2}
\resizebox{.99\textwidth}{!}{
\begin{tabular}{cccccccccccc}
\hline
           & \multicolumn{3}{c}{Simple Prompt}           &  & \multicolumn{3}{c}{RAG}                     &  & \multicolumn{3}{c}{Self-generated}          \\ \cline{2-4} \cline{6-8} \cline{10-12} 
           & Explicit & Reversion & General (\major{Same/GPT4o}) &  & Explicit & Reversion & General (\major{Same/GPT4o}) &  & Explicit & Reversion & General (\major{Same/GPT4o}) \\ \hline
GPT3.5     & 29.71    & 62.01     & 2.86/\major{12.78}           &  & 8.57     & 57.47     & -9.57/\major{2.64}           &  & 38.29    & 70.13     & -1.29/\major{5.73}           \\
GPT4o      & 6.86     & 74.35     & 13.22/\major{13.22}          &  & 2.29     & 68.83     & 4.85/\major{4.85}            &  & 4.00     & 71.10      & 0.44/\major{0.44}            \\
DeepSeekV2 & 21.14    & 76.95     & 16.74/\major{16.45}          &  & 7.43     & 67.53     & 0.59/\major{7.49}            &  & 13.14    & 67.53     & -3.96/\major{1.47}           \\
DeepSeek7B & 25.71    & 65.91     & 2.71/\major{11.75}           &  & 14.29    & 60.71     & -6.14/\major{8.52}           &  & 38.86    & 73.38     & -5.43/\major{-3.23}          \\
CodeQwen7B & 29.71    & 67.21     & -3.43/\major{6.75}           &  & 24.00    & 57.14     & -12.00/\major{0.00}          &  & 50.29    & 68.51     & -19.86/\major{-22.91}        \\ \hline
\end{tabular}
}
\vspace{-4mm}
% \vspace{-4mm}
\end{table*}

% We experimented with different models under the Intention framework using three distinct prompt strategies: Simple Prompt, RAG, and Self-generated Prompt. The results are presented in Table~\ref{tb:rq3-1}. We can observe that DeepSeek7B achieved the best results with the Simple Prompt strategy, GPT4o achieved the best results with the Self-generated Prompt strategy, and the other three models achieved high accuracy with the RAG Prompt strategy.

% Comparing the Self-generated Prompt with the RAG Prompt, the results align with our previous analysis in RQ2: the larger the model's parameters and the stronger its reasoning capabilities, the better it performs with the Self-generated Prompt. For example, GPT4o and DeepSeekV2 achieved 65.97\% and 64.10\% accuracy, respectively. Conversely, other models perform much worse with the Self-generated Prompt compared to other prompt strategies.

% Notably, even with the Simple Prompt method, all models still achieved very high accuracy. Compared to the Simple Prompt result in Table~\ref{tb:rq2-1}, the accuracy of the five models increased by 15 to 23 percentage points. This indicates that the Intention framework makes it easier for models to understand reviewer's requirements.

\major{To further understand the improvements that are caused by our three intention-based components, we calculate the EM improvements on the test samples that are handled by each component, i.e., the EM difference compared to the best performance of the LLM baselines (see Table~\ref{tb:rq2-1}). Note that, due to that the intention quality is important, similar to RQ2, we show both results using intentions extracted by the same model and GPT4o. 
% The results are shown in Table~\ref{tb:rq3-2}.
}

% Next, we compare the improvement of each category using the Intention-based framework with Simple Prompt, RAG, and Self-Generated Prompt strategies against the ordinary tasks. For better demonstration of the Intention-based framework's effect, we chose the Position-Aware task, which performed the best in RQ2, as the control group.

As shown in Table~\ref{tb:rq3-2}, the Intention-based framework demonstrates improvements in most cases. Notably, the most significant improvement is observed in the Reversion Suggestion category, with accuracy increases ranging from 57 to 76 percentage
points. This substantial gain is attributed to the rule-based refinement strategy employed for Reversion Suggestions, which ensures correct refinement when the given sample is accurately classified into this category during the intention analysis.

For the Explicit Code Suggestion category, our framework also shows improvement. For relatively weaker models like GPT3.5, DeepSeek7B, and CodeQwen7B, the improvement is particularly notable, often exceeding 20 percentage points, with a maximum increase of 50 percentage points. For stronger models like GPT4o and DeepSeekV2, it still provides slight improvements. 
\major{This is because explicit code suggestions are relatively easier for stronger models to handle, as the explicit intentions are more straightforward for them to extract. However, weaker models struggle to effectively extract these intentions and refine code, resulting in more significant improvements when using our method.}
% This is due to the repair rules used in the framework, which can correct errors generated by the model.
\major{For tasks in the General Suggestion category, we observed that our method improves results when high-quality intention extraction (e.g., using GPT4o) is employed. However, when using the same model for both intention extraction and code refinement, the results decrease in some cases. This is because weaker models may extract incorrect intentions, which not only fail to assist the refinement process but can also lead to the generation of incorrect code.}

\major{Additionally, with the Simple Prompt strategy, the overall improvement is notable since the original model performs poorly, leaving substantial room for improvement. On the other hand, for RAG-based and Self-generated prompts, the overall performance is already enhanced, so the use of low-quality intentions (extracted by the same model) can more easily degrade performance.}

\major{
Furthermore, we found that the improvement in the General Suggestion category (even with high-quality intentions) is lower compared to Explicit Suggestion and Reversion Suggestion categories. This is because intentions in General Suggestions remain relatively high-level and less concrete, making it more challenging for the model to execute precise refinements compared to the other two categories.}

% the framework provides slight improvements for the stronger models, GPT4o and DeepSeekV2. Specifically, for the RAG prompt method, the improvements are 4.85 and 7.49 percentage points, respectively. This could be attributed to the repair strategy within the framework or more effective indexing after intention extraction, leading to more relevant task retrieval. However, for the three weaker models, GPT3.5, DeepSeek7B, and CodeQwen7B, the framework shows almost no improvement for these tasks, and in some cases, the performance decreases. Particularly with the RAG method, the performance drops by 6 to 12 percentage points. This might be because weaker models extract less accurate intentions, and using these intentions for indexing can lead to retrieving irrelevant examples, thereby interfering with the results.





\vspace{5pt}\noindent \fbox{
\parbox{0.95\linewidth}{\textbf{Answers to RQ3}: \major{The results demonstrate that all three components contribute to performance improvement. However, their effectiveness varies. The Reversion Intention component achieves the largest improvement (over 50 percentage points) due to its rule-based code refinement, which ensures high accuracy. In contrast, the General Intention component shows relatively less improvement, as its intentions are less concrete compared to Explicit and Reversion Intentions, introducing larger uncertainty into the refinement process. Moreover, performance can decrease if the General Intention is not accurately extracted.}
}
}

% The results show that using the Intention-based framework improves performance for all models. 对Reversion Suggestion类型的任务提升50%以上。对Explicit Code Suggestion类型任务提升20%以上。如果使用比较强的模型，如GPT4o和DeepSeekV2，对General Suggestion tasks也有小幅提升4.85\% and 7.49\%。
% Strong models show improvements across Explicit Code Suggestion, Reversion Suggestion, and General Suggestion tasks. Weaker models only show improvements in the first two tasks, with performance decreasing in the third task. 


% 我们尝试了不同模型在Intention框架下，使用simple prompt，rag，self-generation三种不同的prompt策略。结果如表3所示，我们可以观察到，code-deepseek在simple prompt策略下取得最好结果。GPT4o在self-generation下取得最好结果，其余三个模型都在RAG的prompt策略下取得高的准确率。

% 对比self-generation方法与RAG方法，与之前在RQ2中分析的结果一致，模型参数量越大推理能力越强，在self-generation方法的效果越好，例如GPT4o和deepseek效果分别为65.97%和64.10%。反之，一般的模型在self-generation上的效果相较于其他prompt策略会落后很多。

% 另外值得注意的是，即便只是使用最简单的simple prompt方法，所有模型仍可取得非常高的准确率。相较于表2中的simple prompt方法，五个模型分别提升了15%-23%的正确率。这说明Intention的框架使模型更加容易理解任务的需求。

% 而后我们再比较一下相较于普通任务中的simple prompt，RAG，self-generation策略，使用Intention框架后对每一类别的提升。为了更好的展示Intention的效果，对照组我们选择了在RQ2中表现最好的Position-Aware任务。

% 如表4所示，几乎大部分情况，使用Intention框架都有提升。特别的，对于Reversion Suggestion任务提升最为明显，均提升57%-76%之间。这是因为Intention框架中对Reversion Suggestion的修复策略是基于规则的，如果能在extraction Intention过程中正确分类，识别出任务属于这一类Intention，那么后续修复就保证修复正确。
% 其次，Explicit code Suggestion类别，使用Intention框架也均有提升。对于相对弱一些的模型，如GPT3.5，deepseekcoder，code-qwen，提升尤为明显，大部分情况可以提升20%以上，最高提升有50%。而对于能力较强GPT4o和deepseek，Intention框架也可以小幅提升。这是因为Intention框架中使用了repair规则，可以修复模型生成中的错误。
% 最后，对于general的任务，我们发现对于能力较强的模型GPT4o和deepseek，Intention框架仍可以进行小幅提升。特别的，对于效果最好的RAG prompt方法，提升分别为4.85%和7.49%。这可能要归功于Intention框架中的repair策略，或者是Intention提取后使得索引更有效，可以索引到更相近的任务。然而，对于能力较弱的3个模型，GPT35，deepseekcoder，codeqwen，general任务中Intention方法就几乎没有提升，反而效果会下降，特别是RAG方法上，效果下降最明显，下降了6%-12%。这可能是因为弱模型本身生成Intention的正确率就不高，再继续用Intention做索引会索引到不相关的例子，反而干扰了结果。

% 总之，无论是能力强的模型还是能力弱的模型使用Intention框架效果都会有提升。能力强的模型在Explicit code Suggestion，Reversion Suggestion，general Suggestion三个任务上都有提升。而能力弱的模型只在前两个任务上有提升，第三个任务上会有下降。

\subsection{RQ4: Intention-Based Dataset Cleaning}



\begin{table}[!t]
\centering
\caption{Effectiveness on data cleansing.}
\scriptsize
\label{tb:rq4}
\begin{tabular}{ccccccc}
\hline
          & TP   & FP  & TN  & FN  & Accuracy & Precision \\ \hline
Intention-based & 1076 & 110 & 553 & 261 & 81.45\%  & 90.73\%   \\
Comment-based    & 1201 & 312 & 351 & 136 & 77.60\%  & 79.38\%   \\ \hline
\end{tabular}
\vspace{-4mm}
\end{table}

We further investigated the role of intention in data cleaning. As shown in Table~\ref{tb:rq4}, compared to directly using the review comment to verify whether the code modification meets the reviewer’s requirements, using Intention to verify the code modification proved to be more effective. The accuracy increased from 77.60\% to 81.45\%, and the precision increased from 79.38\% to 90.73\%.

% The improvement in True Positives (TP) and the reduction in False Positives (FP) also highlights the effectiveness of the Intention-based approach. Specifically, the True Positives for the Intention-based method are 1076, compared to 1201 for the Comment-based method, showing a more conservative approach that reduces unnecessary inclusions. The False Positives dropped significantly from 312 to 110, indicating a more precise identification of valid code modifications. Similarly, True Negatives (TN) rose from 351 to 553, and False Negatives (FN) increased slightly from 136 to 261, showing that while some valid changes were missed, the overall precision improved.

Such improvements in accuracy and precision underscore the significance of utilizing intention as a guiding principle for code verification. Since dataset construction places a higher emphasis on data quality, the 12\% improvement in precision is significant for enhancing data quality. This increase ensures that the cleansed data is not only more accurate but also more reliable for downstream applications and analysis.

Overall, the intention-based approach demonstrates a more balanced and effective methodology for ensuring that code modifications align closely with the original reviewer's intentions, resulting in cleaner and more precise datasets. This shift toward a more intention-driven process marks a substantial advancement in the field of data cleansing, providing developers and data scientists with a more robust tool for maintaining code integrity and quality.

\vspace{5pt}\noindent \fbox{
\parbox{0.95\linewidth}{\textbf{Answers to RQ4}: The results indicate that intention-based cleansing is more effective for code refinement data cleansing than comment-based methods, achieving an accuracy of 81\% and a precision of 91\%.
}
}

% 我们进一步研究Intention在筛选数据中的作用。结果如表5所示，相较于直接用review comment检验代码修改是否符合要求，使用Intention来检验代码修改是否符合要求的效果更好。准确率从78%提升到了81%，而精确率从79%提升到了91%。因为构造数据集更在意数据质量，精确率提升的12%对数据质量的提升意义很重要。