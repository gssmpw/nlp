\section{Experimental Setup}
To evaluate the effectiveness of the proposed approach, we design the following research questions.
\begin{itemize}[leftmargin=*,topsep=2pt]
    \item \major{RQ1: How accurate are LLMs in extracting intentions?}
    \item RQ2: To what extent is the Intention-based framework effective in code refinement?
    \item RQ3: To what extent does the framework improve performance across different intention categories?
    \item RQ4: To what extent is intention-based dataset cleaning effective?
\end{itemize}





% Our objective is to systematically evaluate the effectiveness of the Intention-based Code Refinement Framework in addressing the Comprehensive Code Refinement task. The research questions include:

% 1. To What Extent is the Intention Extraction Accurate?

% 2. To What Extent is the Intention-based Framework Effective in Code Refinement?

% 3. To What Extent Does the Framework Improve Performance Across Different Intention Categories?

% 4. To What Extent is Intention-Based Dataset Cleaning Effective?
 
% 我们的目标是系统的评估Intention-based code refinement framework解决Comprehensive Code Refinement任务的有效性。研究问题包括：首先，需要先调研Extracting Intention这个步骤中取得Intention的正确率。其次，验证整体框架的有效性，即评估模型通过框架解决code Refinement任务的正确率，对比不使用本框架的方法是否有提升。再次，调研不同Intention类别的数据，使用框架可以提升多少效果。最后，研究使用Intention作为数据质量判别的方法，对清洗数据集的效果。

\subsection{RQ1: Intention Extraction Accuracy}
The accuracy of the intention understanding is important for code refinement. 
% The concept of intention is a novel proposition we introduced, which currently lacks an existing benchmark. 
To measure the accuracy, we constructed a test dataset from the existing code review dataset~\cite{li2022automating}. We randomly select 2,000 samples for manual annotation to assess the accuracy of different LLMs in intention extraction. 
% Correct intention extraction implies that the intention accurately summarizes the reviewer’s suggested solution, allowing the developer to refine the code without having to focus on the review comments but solely relying on the intention.


% As presented in Fig.~\ref{fig:framework}, three general categories are designed including explicit code suggestion, reversion suggestion and general suggestion. Followed by the rule-based intention analyzer, we use GPT4o to classify the categories. 
As shown in Fig.~\ref{fig:framework}, for reversion intention and general intention, we use the LLM to extract the intention. To check the accuracy of the extracted intention, we invited two PhD candidates to conduct manual annotations. In cases of differing results, the annotators reached a consensus through discussion to finalize the annotations. During the annotation process, we not only assessed the correctness of intention extraction but also filtered out invalid data. Specifically, cases where the revised code was unrelated to the review comments were excluded to ensure that only relevant and high-quality data were used in subsequent research questions.

% We categorized the selected samples based on the actual \textit{Intentions} into three categories: Explicit Code Suggestion, Reversion Suggestion, and General Suggestion. The following criteria define how to determine the correctness of the framework’s intention extraction:

% 1. Explicit Code Suggestion: If the framework correctly classifies the intention as an Explicit Code Suggestion, the extracted intention is deemed correct.

% 2. Reversion Suggestion: There are two scenarios where the framework's extraction is considered correct for this category:

%    - The framework directly categorizes the intention as a Reversion Suggestion.

%    - The framework classifies the intention as a General Suggestion, but following the suggested fixes achieves the same outcome as the Revised Code.

% 3. General Suggestion: For data categorized under General Suggestion, the framework must classify the intention as a General Suggestion, and the proposed solution must be consistent with the original review comments to be deemed correct.

% Given the extensive nature of manual annotations, we focused solely on evaluating the results of the GPT-4o model in the intention extraction process. Two Ph.D. candidates in software engineering conducted the annotations. 

% Intention的正确性代表了模型真正理解了任务的要求，是做好后续步骤的前提。Intention的概念是我们首次提出，目前还没有benchmark，我们使用codereview数据集的测试数据，随机选取2000个，而后用人工标记的方式来判断Intention的正确率。所谓Intention的正确就是指Intention概括总结了reviewer的建议方案。developer可以不再关注ReviewComment，只根据Intention就能正确修复代码。
% 我们按照真实的Intention把数据分成Explicit Code Suggestion，Reversion Suggestion和General Suggestion这三类。接下来定义如何判断framework提取的Intention是否正确。对于Explicit Code Suggestion类别的数据，只要framework正确将Intention分成Explicit Code Suggestion类别，就认为提取的Intention是正确的。对于Reversion Suggestion类别的数据，framework提取的Intention有两种情况是正确的。一种是framework直接将Intention分成Reversion Suggestion类别。另一种是framework将Intention分类成General Suggestion类别，并且按照修复建议，达到的效果和Revised Code是一致的。对于General Suggestion类别的数据，需要framework将Intention分类到General Suggestion，并且修复的方案和原始ReviewComment是一致的，才算是framework提取的Intention是正确的。
% 由于涉及大量人工标注，我们仅评估了GPT-4o模型提取Intention的结果。共有两个软件工程的PHD进行标注，出现不同结果时，由两人商议再最终确认。在标注过程中，我们不仅标注了Intention提取的正确性，也标注了原始数据的合理性。对于revised code与review comment无关的不合理的数据进行过滤，保证在后续的RQ中，只使用合理的数据。

\begin{figure}[!t]
\centering
\includegraphics[width=0.85\linewidth]{fig/prompt2.pdf}
\vspace{-2mm}
\caption{The used prompt format for different tasks.}
\label{fig:prompt2}
\vspace{-4mm}
\end{figure}


\subsection{RQ2: Intention-based Framework Effectiveness}
To evaluate the effectiveness of our proposed intention-based code refinement, we select two state-of-the-art baselines for comparison. 

% To What Extent is the Intention-Based Framework Effective in Code Refinement?
% In this study, we compare the results of our framework with other code refinement methods. Our selected baselines include two categories: pre-trained models and large model prompt techniques.

% For pre-trained models, we have chosen CodeReviewer~\cite{li2022automating} and T5CR~\cite{tufano2022using} as our baselines.

\noindent \textbf{CodeReviewer~\cite{li2022automating}:} 
It designed three pre-training objectives, i.e., Diff Tag Prediction, Denoising Objective, and Review Comment Generation, to pre-train the model based on CodeT5~\cite{codet5} for code review activities. Several downstream tasks, including code change quality estimation, code review generation and code refinement, are selected to evaluate the effectiveness of the proposed models. 


\noindent \textbf{T5CR~\cite{tufano2022using}:} It utilized two datasets including the official Stack Overflow dump (i.e., SOD) and CodeSearchNet (i.e., CSN) to pre-train the code review model based on T5 architecture. A tokenizer, i.e., SentencePiece~\cite{kudo2018sentencepiece}, is adopted to tokenize the source code, and the input sequence's maximum length is increased to 512 for training. 

Apart from these baselines, we also comprehensively evaluate the effectiveness of different LLMs and prompt strategies. In particular, we select three closed-source LLMs i.e., GPT-4o-2024-05-13 (GPT4o)~\cite{achiam2023gpt}, GPT-3.5-turbo-0125 (GPT3.5)~\cite{ouyang2022training}, DeepSeek-Coder-V2-0724 (DeepSeekV2)~\cite{zhu2024deepseek} and two open-source large models: CodeQwen1.5-7B-Chat (CodeQwen7B)~\cite{bai2023qwen} and Deepseek-coder-6.7b-instruct (DeepSeek7B)~\cite{guo2024deepseek} for evaluation.
\major{Note that, since different models have varying capabilities in extracting intentions, we present results using both the intentions extracted by the model itself and those extracted by a high-quality model (i.e., GPT-4o). The use of GPT-4o intentions allows us to evaluate whether providing the correct intention can enhance performance in code refinement tasks.}


% The T5~\cite{raffel2020exploring} model serves as the foundational model, pre-trained on a dataset comprising 1.5 million Java-English corresponding pairs through the Masked Language Model Task. In this experiment, we employed the same pre-trained model and also fine-tuned it using the $CodeReview_{train}$ datasets.

% Since our framework, like prompt techniques, is model-agnostic, we conducted a comprehensive and detailed comparison of five models and six common prompt strategies under three code refinement tasks as baselines.

% The models include three closed-source large models: gpt-4o-2024-05-13 (Gpt4o)~\cite{achiam2023gpt}, gpt-3.5-turbo-0125 (Gpt3.5)~\cite{ouyang2022training}, and DeepSeek-Coder-V2-0724 (DeepSeekV2)~\cite{zhu2024deepseek} and two open-source large models: CodeQwen1.5-7B-Chat (CodeQwen7B)~\cite{bai2023qwen} and Deepseek-coder-6.7b-instruct (DeepSeek7B)~\cite{guo2024deepseek}.

% The three tasks include (see Section 3): Basic Code Refinement, Position-aware Code Refinement, and Comprehensive Code Refinement. This allows for a thorough comparison of the effectiveness of various refinements under different input scenarios.
For LLMs, we selected different prompting strategies:

\noindent \textbf{Simple Prompt:} As shown in Fig.~\ref{fig:prompt2}, the Simple Prompt first describes the scenario, then introduces the input information in the task, and finally requests the generation of revised code based on the provided information. This is a concise and effective prompt design used in Guo et al.~\cite{guo2024exploring}.

\noindent \textbf{Simple COT Prompt:} This prompt builds upon the simple prompt by adding the phrase ``Let's think step by step." This technique is employed in model reasoning~\cite{wei2022chain,wang2022self}.

\noindent \textbf{Tufano COT Prompt:} It is introduced in Tufano et al.~\cite{tufano2024code}, which first requires determining which of the following six categories the modification belongs to before completing the modification and then utilizes LLM for the generation.

% The six modification categories are:

% - Changes have been required to refactor the code to improve its quality;

% - Changes have been required since tests for this code must be written;

% - Changes have been required to better align this code to good object-oriented design principles;

% - Changes have been required to fix one or more bugs;

% - Changes have been required to improve the logging of its execution;

% - Changes have been required for other reasons not listed above.

\noindent \textbf{Random Few-shot Prompt:} For each case, three data examples are randomly selected as examples for the prompt as few-shot (excluding self-examples). The data fields provided as examples depend on the task's input fields, such as \texttt{OriginalCode}, \texttt{ReviewComment}, \texttt{ReviewLine}, \texttt{RevisedCode} in Comprehensive Code Refinement, and only \texttt{OriginalCode}, \texttt{ReviewComment}, \texttt{RevisedCode} in Basic Code Refinement. After providing examples as prompt hints, they are concatenated with the simple prompt.

% \noindent \textbf{RAG Prompt:} Similar to Random Fewshot, but the example selection is based on the most similar data retrieved using the BM25~\cite{robertson2009probabilistic} method. We use the \texttt{ReviewComment} as the key for retrieval, with the complete data as the value stored in the retrieval set. For each case, its \texttt{ReviewComment} is used to retrieve three nearest neighbors (excluding itself). After retrieval, the prompt construction method is the same as Random Fewshot.

\noindent \major{\textbf{RAG Prompt:} 
To compare with the Random Few-Shot Prompt, we design a Retrieval-Augmented Generation (RAG) prompting method as an alternative approach to few-shot prompting, leveraging relevant example selection. The retrieval database is constructed from the dataset used in RQ1, specifically the 2,000 randomly selected samples from the CodeReview dataset. However, only 1,337 of these samples are included in the database, as the remaining 663 cases exhibit low-quality refinements that do not align well with the review comments.
We selected these samples for two reasons: 1) The CodeReview dataset uniquely provides the complete data required for this study, including \texttt{OriginalCode}, \texttt{ReviewComment}, \texttt{ReviewLine}, \texttt{RevisedCode}, and \texttt{LastCodeDiffHunk}. 2) The quality of the retrieval dataset is crucial, and these 2,000 samples have been manually analyzed in RQ1, ensuring their reliability.
During testing, for each test case, we retrieve three samples to construct a 3-shot prompt. If the test data is included in the retrieval results, it is excluded and replaced with another sample. To enhance contextual relevance, we use BM25 to select semantically similar examples, ensuring that the retrieved examples closely align with the test data's context..}

\noindent \textbf{Self-generated Prompt:} This prompt technique addresses mathematical and reasoning problems by utilizing the model's own understanding ability~\cite{yasunaga2023large}. It first generates several examples and then uses these examples to inspire the model to answer the original question.

% The format of these prompts is provided on our website~\cite{IntentionWebsite}. 

\noindent \textbf{Evaluation Metrics:} To fully automate the code refinement task, we prioritize the exact match (EM) between the model's output and the actual revised code. While metrics like BLEU and Code-BLEU can reflect proximity to the ground truth, they do not effectively gauge the degree to which the model's output aids programmers in code refinement. Particularly in simple tasks, if the model's output is not completely correct, it may be less beneficial for programmers to use the model's output for refinement than to directly use the review comment. Therefore, our evaluation metric only includes the EM value.

% Dataset: The data we used is the manually labeled valid data from RQ1.

% In this experiment, our framework utilizes the RAG prompt method. By comparing its performance with these baselines, we can assess whether our framework surpasses existing methods.


% To What Extent is the Intention-based Framework Effective in Code Refinement? 我们将framework的结果与其他Code Refinement方法进行比较。
% 我们选用的baseline包括预训练模型和大模型的prompt技术两种。
% 预训练模型，我们选择的baseline是CodeReviewer和T5CR。
% \textbf{CodeReviewer:} The model is initialized using the weight parameters of CodeT5 ~\cite{codet5}. Subsequently, the pre-training is carried out with three objectives: Diff Tag Prediction, Denoising Objective, and Review Comment Generation. In this experiment, we employed the same pre-trained CodeReviewer model and fine-tuned it using the $CodeReview_{train}$ datasets.
% T5CR：使用的T5作为基础模型，在一份1.5million个的，关于java和英语对应关系的数据集上，进行Masked Language Model Task任务预训练。In this experiment, we employed the same pre-trained model and fine-tuned it using the $CodeReview_{train}$ datasets.
% 因为我们的框架和prompt技术一样，都是模型无关的。所以我们全面而详细的对比5种模型，6种常见的prompt，在三种code Refinement任务下的效果作为baseline。
% 模型我们采用了3个闭源大模型，gpt-4o-2024-05-13，gpt-3.5-turbo-0125，DeepSeek-Coder-V2-0724，和2个开源大模型CodeQwen1.5-7B-Chat，deepseek-coder-6.7b-instruct。
% 三种任务包括（见第三节）：Basic code Refinement，Position-aware code Refinement和Comprehensive code Refinement等三种任务。这样充分对比不同输入信息的情况下，哪种修复的效果最好。
% Prompt方法包括了Simple Prompt，Simple COT，Tufuno COT，Random Fewshot，RAG Prompt, Self-generated Prompt等6种常见的prompt策略。
% Simple Prompt：如图3所示，Simple Prompt首先描述场景，然后介绍任务中的输入信息，最后要求根据提供的信息，生成revised code。是Guo的文章中使用的，简洁有效的Prompt设计。
% Simple COT Prompt: 在simple prompt的基础上，加上“Let's think step by step.”。这是在多个推理任务中使用的prompt方法。
% Tufano COT Prompt：在Tufano论文中提到的一种COT技术，首先要求判断修改类型属于以下六个类别的哪一种，然后在完成修改。六种修改类别为：
% Changes have been required to refactor the code to improve its quality;
% Changes have been required since tests for this code must be written;
% Changes have been required to better align this code to good object-oriented design principles;
% Changes have been required to fix one or more bugs;
% Changes have been required to improve the logging of its execution;
% Changes have been required for other reasons not listed above.
% Random Fewshot Prompt：用有效数据为样例选取数据集。对每个case，随机选取3个数据作为examples提供给prompt作为fewshot（已排除掉自己给自己做example的情况）。例子提供的数据字段依照任务本身输入字段，如Comprehensive Code Refinement里包含Initial code，original code，review comment，review line，revised code等5个字段。而Basic Code Refinement只包含original code，review comment，revised code三个字段。提供例子作为prompt提示后，再拼接上simple prompt。
% RAG Prompt：与random fewshot类似，只不是选取example时，使用的是用BM25方法检索到的最相似的数据。我们使用review comment作为检索的key，完整的数据作为value，存储再检索集中。对每个case，使用其review comment去检索相近的3个邻居（排除掉自身）。检索到example之后，后面构造prompt的方法与random fewshot相同。
% Self-generated Prompt：这是解决数学和推理问题一种prompt方法，利用模型自身对问题的理解能力，先生成若干个例子，然后再根据这些例子来启发模型，回答最初的问题。
% 详细的prompt设计我们已上传到网站。
% 评价指标：为了完全自动化code Refinement任务，我们更关模型输出结果与真实RevisedCode是否完全匹配，即EM。BLEU与Code-BLEU等指标虽然可以反映与真实值的接近程度，但是对code Refinement任务不能很有效的反映模型输出结果对程序员的帮助程度。特别是对于简单的任务，如果模型输出结果不是完全正确，那么程序员利用模型输出的结果再去做Refinement，可能不如直接利用ReviewComment来做Refinement。所以，我们这次的评价指标只选择了EM值。
% 数据集：我们所用的数据是RQ1中人工标记valid的数据
% 在这个实验中，我们框架选用RAG的prompt方法。通过与这些baseline的效果做对比，可以看到我们的framework是否可以超过现有的方法。

\subsection{RQ3: Improvement Across Different Intention Categories}

% To What Extent Does the Framework Improve Performance Across Different Intention Categories?

In this experiment, we mainly evaluated the effects of different prompting strategies and three prompting strategies: Simple Prompt, RAG, and Self-generated Prompt. We aim to understand how much our framework improves performance for each of Intention types compared to methods that do not use the framework.

\major{Additionally, we conducted ablation experiments on the Intention Extraction component of our framework, specifically evaluating the impact of the three agents (Agent1, Agent2, and Agent3). For each agent, we tested the Exact Match (EM) values by removing the respective agent and comparing the results. This analysis allows us to quantify the individual contribution of each agent to the overall performance of the framework and to identify which components are most critical for improving the accuracy of intention-based predictions.}

% The model, data, and experimental settings are the same as those used in RQ2.

% To What Extent Does the Framework Improve Performance Across Different Intention Categories? 
% 在这个实验中，我们对比了Simple Prompt，RAG和Self-generated Prompt等三种方法，在使用和不使用框架的效果。特别的，对于每一种Intention类型都进行了详细的对比。我们希望了解，相较于不使用框架的方法，我们的框架对于3种不同的Intention类型的数据，效果分别提升了多少？
% 模型，数据等实验设置与RQ2相同

\subsection{RQ4: Intention-Based Dataset Cleaning}

Lastly, we aim to explore the capability of using intention to enhance data quality. A significant challenge in code refinement data quality is the inconsistency between the content expressed in the review comment and the modifications made in the revised code. Currently, no practical method exists for aligning the review comment and revised code at the semantic level. We will attempt to use intention to determine whether the modifications in the revised code meet the reviewer's requirements.

The Intention Method we designed is as follows: we provide the model with information about the Intention, Original Code, Review Line, and Revised Code and use GPT4o as a classification model to determine whether the modifications in the Revised Code meet the Intention requirements.

We also compared this with a common method that does not use the intention: we provide the model with information about the Review Comment, Original Code, Review Line, and Revised Code and use GPT4o as a classification model to determine whether the revised code meets the requirements of the review comment.

% By systematically evaluating these methods, we aim to demonstrate how intention can potentially improve the alignment between review comments and revised code, thereby enhancing the overall data quality in code refinement tasks.

% 最后，我们要探索，使用Intention来提升数据质量的能力。review comment所表达内容与revised code所修改的内容不一致，是影响code Refinement数据质量的一个难题，目前还没有很好的方法完成review comment和revised code语义级别的对齐。
% 我们将尝试使用Intention来判断revised code的修改是否符合要求。
% 我们设计的方法是：告诉模型Intention，OriginalCode，ReviewLine和RevisedCode等信息，使用Gpt4o作为分类模型，让模型判断RevisedCode的修改是否符合Intention要求。
% 我们对比了不使用Intention的方法：告诉模型ReviewComment，OriginalCode，ReviewLine和RevisedCode，使用Gpt4o作为分类模型，让模型判断RevisedCode是否符合ReviewComment的要求。
