\section{Results}\label{sec:results}

We focus our analysis on the L2 processor design and performance.
For the evaluations, HITNet \cite{hitnet} has been chosen as a representative of the latest stereo depth algorithms.
For comparison with the L2 processor, we evaluate HITNet on the Jetson Orin Nano, which is a representative off-the-shelf mobile compute system.
A public implementation of HITNet is used \cite{tinyhitnet}, and it is compiled on a per-ROI size basis using ONNX and TensorRT. This approach provides optimistic estimates for the device's performance.
To measure system power, we utilize Jetson Stats and isolate the power consumption of the GPU and CPU for comparison.

Our system simulator is made of multiple parts. 
Firstly, we implement a counter-based architecture model to estimate the L2 processor performance.
The dynamic and static energy of PE, SCU, \ca{and} SRAM I/O are estimated based on post-APR simulation using the TSMC 28nm PDK and 16-bit operations.
This L2 simulator also accounts for DRAM I/O \cite{lpddr4x, lpddr5_est} and NoC \cite{ansa} energy and latency.
% In our energy estimation, we take into account all parts of the system including DRAM I/O \cite{lpddr4x, lpddr5_est}, NoC \cite{ansa}, sensor \cite{gs_cis1}, uTSV, and MIPI interface \cite{gomez_distributed_2022}.
We base the L1 energy and latency on \cite{marsellus} and estimate the energy required for object detection on images of size $384\times1280$.
We also evaluate system level sensor \cite{gs_cis1}, uTSV, and MIPI interface \cite{gomez_distributed_2022} energy.
This complete simulator is used to conduct design space sweeps of the \projname{} system running HITNet for stereo depth and TinyYOLOv3 for object detection, according to Section ~\ref{sec:mapping}.

\subsection{Ablation Studies}

% \begin{figure}
%     \centering
%     \includegraphics[width=0.8\linewidth]{Figures/ResultAxisAblation_2024_07_04.pdf}
%     \caption{Binning with each design space axis disabled.}
%     \label{fig:result_axis_ablation}
% \end{figure}

% \textbf{Design Space Axes}: In Fig.~\ref{fig:result_axis_ablation}, we evaluate binnings generated by disabling each of the design axes from Section ~\ref{subsec:phase1}, and compare these to the ones generated with all axes enabled.
% For example, disabling the processor utilization axis forces all bins to use one uniform processor utilization, i.e., the entire L2 processor without power gating.
% By comparing the uniform processor utilization curve with the independent bins curve, one can recognize the significance of disabling the processor utilization axis.
% From the comparisons in Fig.~\ref{fig:result_axis_ablation}, we can see that the design-time ROI axis has the least impact on energy.
% The DRAM modes axis is situationally important.
% Large processors have sufficient on-chip SRAM and do not need DRAM I/O caching.
% On the other hand, for very small processors, other mapping inefficiencies dominate the energy for average ROI sizes.
% However, medium-sized processors rely on DRAM caching to support large ROIs, but suffer up to $3.15\times$ inefficiency by using it for average ROIs.
% Finally, the processor utilization axis is the most significant axis across processor sizes, showing that the ability to operate efficiently on small and large ROI sizes by tuning processor utilization within the same silicon is critical for this system design.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/ResultBinCountAblation_2024_07_04.pdf}
    \caption{Effect of number of bins. Increasing bin count results in marginal gains, with highest energy savings on intermediate sized processors.}
    \label{fig:result_bincount_ablation}
\end{figure}

\textbf{Bin Count}: In Fig.~\ref{fig:result_bincount_ablation}, we evaluate the results using different number of bins, which corresponds to the number of runtime ROI intervals.
Moving from one to two bins, the processor can use different mappings for large and small ROI sizes, optimized for different objectives.
This results in a significant gain in energy.
However, as the number of bins increases beyond 2 bins, the marginal gain per additional bin diminishes and varies slightly across processor areas.
In some cases, adding an extra bin can still be beneficial to better adapt to the specific ROI distribution being used.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/ResultDistributionCompare_2024_07_04.pdf}
    \caption{Results of different ROI distributions. The pareto curve of an ROI distribution is dictated primarily by the ROI mean, though variance also plays a minor role.}
    \label{fig:result_distribution_compare}
\end{figure}

\textbf{ROI Distributions}: We compare results for multiple ROI probability distributions to evaluate the generality of our design methodology, as seen in Fig.~\ref{fig:result_distribution_compare}.
Interestingly, the average energy required to run the various ROI distributions is ordered in the same sequence as their mean ROI size, as reported in Fig.~\ref{fig:roi_distribution}.
This suggests that our design method minimizes nonlinear overheads caused by the variable ROI sizes.
Even high variance bimodal distributions, such as ``Vegetables'' \cite{epic_kitchens}, can be efficiently handled by adequately parameterized ROI binnings on \projname{}.

\subsection{Design Benchmarking}

Next, we compare designs generated by this methodology, with existing edge system and baseline ASIC designs.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/ResultJetsonCompare_2024_07_04.pdf}
    \caption{Energy consumption of Jetson Orin Nano running different ROI sizes and \projname{} systems optimized for different ROI distributions. \projname{} achieves superior granularity in energy and lower overall energy.}
    \label{fig:jetson_compare}
    % Distribution: KITTI
    %    Config: ((4, 16), 16, 4)
    %    SRAM Dictionary: {'core_v0': 164864.0, 'core_v1': 123648.0, 'vmm_vec': 61632.0, 'vmm_mat': 248112.0}
    % Distribution: Aubergine
    %    Config: ((8, 16), 6, 2)
    %    SRAM Dictionary: {'core_v0': 245760.0, 'core_v1': 491520.0, 'vmm_vec': 256000.0, 'vmm_mat': 1024000.0}
    % Distribution: Olive
    %    Config: ((4, 16), 6, 2)
    %    SRAM Dictionary: {'core_v0': 1310720.0, 'core_v1': 491520.0, 'vmm_vec': 109568.0, 'vmm_mat': 1382400.0}
    % Distribution: Chopping Board
    %    Config: ((16, 16), 4, 1)
    %    SRAM Dictionary: {'core_v0': 370944.0, 'core_v1': 491520.0, 'vmm_vec': 1966080.0, 'vmm_mat': 4748928.0}
    % Distribution: Left Hand
    %    Config: ((16, 16), 4, 1)
    %    SRAM Dictionary: {'core_v0': 513280.0, 'core_v1': 491520.0, 'vmm_vec': 1966080.0, 'vmm_mat': 4799664.0}
    % Distribution: Pan
    %    Config: ((16, 16), 4, 1)
    %    SRAM Dictionary: {'core_v0': 368640.0, 'core_v1': 491520.0, 'vmm_vec': 1966080.0, 'vmm_mat': 4748928.0}
    % Distribution: Vegetable
    %    Config: ((8, 16), 4, 1)
    %    SRAM Dictionary: {'core_v0': 1966080.0, 'core_v1': 491520.0, 'vmm_vec': 61440.0, 'vmm_mat': 4055040.0}
\end{figure}

\textbf{Comparison with Jetson Orin Nano}: 
In Fig.~\ref{fig:jetson_compare}, we compare the per-ROI energy of the Jetson Orin Nano with \projname{} systems optimized for different ROI distributions.
The \projname{} designs demonstrate two prominent advantages.
Firstly, a \projname{} design can be optimized based on the ROI distribution, whereas the Jetson Orin Nano requires statically compiled binaries for each ROI size in the distribution, which makes it less practical. 
Secondly, 
\projname{} processor dynamically scales performance and energy according to the size of ROI being processed.
In contrast, the Jetson Orin Nano appears to suffer from a coarse-grained reconfigurability of its tensor cores, resulting in an energy pattern characterized by stair steps\ca{; compute latency also suffers, and the Jetson Orin Nano does not exceed 15 FPS operation.}
\projname{}, on the other hand, uses tiles, PEs and SCUs to enable fine-grained optimization.

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/ResultBinningBreakdown_2024_07_11_KITTI.NORM.pdf}
    \caption{Breakdown of \projname{} L2 processor energy by ROI size. Dashed lines represent the boundaries of mapping bins.}
    \label{fig:binning_breakdown}
    % 29 January 2025
    % 4 Tiles, 16 PEs / Tile, 1 Core / Tile
    % 16 Vectors, 4 Long / PE
    % 90kB Vector1 SRAM / Core
    % 480kB Vector2 SRAM / Core
    % 18kB Vector SRAM / PE
    % 240kB Matrix SRAM / PE
\end{figure}

\textbf{Energy Breakdown by ROI Size}: We analyze the breakdown of energy by ROI size in a \projname{} L2 processor, as illustrated in Fig.~\ref{fig:binning_breakdown}.
The energy consumption is primarily influenced by static power and DRAM access, with their proportions varying accordingly.
For very small ROIs, static power draw is dominant. 
For extremely large ROIs, the energy is dominated by DRAM I/O.
DRAM I/O is used to minimize the L2 SRAM and reduce static power for small ROIs.
This insight underscores the challenge of optimizing energy across ROI distribution in the L2 processor design. 

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{Figures/Piechart_2024_07_11.pdf}
    \caption{Comparison of energy in baseline design (no ROI exploitation) with \projname{} design, assuming object detection runs every 5 frames and KITTI ROI distribution.}
    \label{fig:baseline_compare}
    % 29 January 2025
    % Naive Pipeline:
    %   8 Tiles, 16 PEs / Tile, 1 Core / Tile
    %   16 Vectors, 4 Long / PE
    %   960kB Vector1 SRAM / Core
    %   240kB Vector2 SRAM / Core
    %   16kB Vector SRAM / PE
    %   600kB Matrix SRAM / PE
    % ROI Sparsity Exploitation:
    %   4 Tiles, 16 PEs / Tile, 1 Core / Tile
    %   16 Vectors, 4 Long / PE
    %   480kB Vector1 SRAM / Core
    %   241.5kB Vector2 SRAM / Core
    %   288kB Vector SRAM / PE
    %   487.5kB Matrix SRAM / PE
\end{figure}

\textbf{Comparison with Baseline System}: In Fig.~\ref{fig:baseline_compare}, we compare a baseline system with no ROI or temporal sparsity with a \projname{} system.
In this comparison, we optimize both processors to have an area under 100 mm\textsuperscript{2} and a frame rate of at least 30 FPS.
The exploitation of ROI requires object detection and object tracking.
Energy savings are limited by these two costs, which do not scale with the ROI.
Nevertheless, \projname{} still achieves \textbf{$4.35\times$} per-inference energy savings by effectively leveraging ROI-based processing.
