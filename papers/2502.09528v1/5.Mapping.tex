\section{\projname{} L2 Processor Mapping}\label{sec:mapping}

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.9\linewidth]{Figures/Binning_2024_07_04.pdf}
%     \caption{We perform a multidimensional design space sweep along each axis, and evaluate these mappings across the range of ROI sizes in the target application (Phase \#1). From here, optimal binning can be computed by multiphase annealing over this cost volume, respecting the target ROI probability distribution and area and latency constraints (Phase \#2). Note that the plots have been reduced from 4D to 2D for the purpose of simplifying visualization.}
%     \label{fig:binning_explanation}
% \end{figure*}

% To efficiently support a broad range of ROI sizes, we need to navigate the vast design space of possible mappings onto the L2 processor. 
\je{To leverage the energy savings made possible by our algorithm and architecture, we must do a good job of mapping compute onto the processor.}
\je{However, there are two key challenges that must be overcome to achieve this.}
\je{The first is the vast space of possible mappings that are possible; to find low energy, low latency mappings within this space, we must either reduce the dimensionality of this space, or develop algorithms to efficiently traverse it.}
\je{The variable ROI-size in this application adds the second challenge of providing mapping support across this range of possible ROIs.}
\je{Purely offline solutions to this problem are impractical, due to the storage requirements for supporting the full ROI range; and purely online solutions are equally impractical, due to the complexity of generating these mappings.}

\subsection{Single-ROI Mapping}\label{subsec:single_roi}

\je{We first consider the space of mappings for algorithms running on a fixed ROI size.}
\je{To traverse the large space of possible mappings, we first generate a set of higher level mapping descriptors, which can be used to generate low-level control signals for the processor.}

\textbf{DRAM I/O Options}:
\je{The off-chip DRAM storage in the \projname{} system can be used to store intermediate activations in neural networks.}
\je{This helps to address two levels of memory utilization variance.}
\je{Within a single frame, different activations in the network can have vastly different sizes; to enable processing on form-factor limited processors, it is beneficial to not rely on local SRAMs to store these activations.}
\je{Across multiple frames, different ROI sizes result in similarly variable activation sizes.}
% The variations in ROI result in vastly varying activation sizes from frame to frame. 
% Furthermore, for a single ROI, each layer of the model requires significantly different intermediate activation sizes.
DRAM provides a means to trade off dynamic energy and latency with SRAM utilization for handling these large ROIs and layers: firstly, by choosing which activations are stored in DRAM; and secondly, by choosing how these activations are accessed.
Activations may be streamed directly from DRAM to local buffers to reduce SRAM utilization. 
Alternatively, activations can be buffered in SRAM to reduce energy and latency.

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/DRAMModes_2024_07_10.pdf}
    \caption{By successively buffering or streaming large activations from DRAM, a progression of \textit{DRAM Modes} are formed which reduce SRAM utilization.}
    \label{fig:dram_modes}
\end{figure}

\je{To choose which activations to store in DRAM and which to store in SRAM, we consider the minimal amount of off-device storage necessary to reduce an algorithms peak SRAM utilization, as seen in Figure ~\ref{fig:dram_modes}.}
\je{Iterating on this process, we generate a sequence of \textit{DRAM Modes}, or sets of activations to be streamed or buffered in DRAM.}
\je{By forming this sequence, we reduce the problem of determining DRAM I/O for a mapping from making a per-activation ternary choice, to selecting selecting the appropriate DRAM mode to fit within a given architecture's memory budget.}
\je{In practice, a combination of dataflow selection and DRAM I/O can be used to reduce SRAM utilization; therefore, we generate mappings for a few DRAM modes, and evaluate each on latency, energy, and memory utilization when generating mappings.}
% The DRAM option is used to reduce peak SRAM utilization.
% This utilization is usually dominated by large intermediate activations, which can instead be streamed from DRAM.
% By applying DRAM I/O to the ranked list of largest activations, we generate a sequence of activation sets to be streamed or loaded to and from DRAM, as shown in Fig.~\ref{fig:dram_modes}.
% We refer to these sets of activations as \textit{DRAM Modes}. 
% Indexing these DRAM Modes simplifies the selection of DRAM I/O to a single parameter.

\textbf{Dataflow Options}: The \projname{} L2 processor supports per-layer dataflows for both PEs and SCUs.
These dataflows include Weight and Input Stationary, which define the patterns of data reuse for weights and activations\je{;} and Channel First and Last, which divide input channel accumulation either spatially or temporally.
While the choice between Weight and Input Stationary is conventionally based on data reuse, the relative sizes of activations and weights for variably sized ROIs must also be considered.
Data movement in the NoC must also be managed. Different dataflows affect the location where activations are produced, thereby impacting the efficiency of data movement between PEs.

\je{Prior works \cite{ansa} have used greedy algorithms to assign per-layer dataflows to a network.}
\je{While this method is useful for minimizing latency and dynamic energy; we find it is insufficient to satisfy SRAM utilization constraints, as minimizing peak memory utilization requires global optimization.}
\je{Therefore, we use an augmented greedy algorithm to assign per-layer dataflows, which consists of a preliminary greedy assignment to minimize dynamic energy, and a subsequent optimization step which identifies dataflow choices that exceed memory useage limits.}
% In every elementary step in the above procedure, a greedy algorithm is employed to assign layer-wise dataflow options.
% However, relying solely on the greedy algorithm may lead to suboptimal solutions, particularly when operating within a fixed SRAM budget.
% To address this, we further augment this greedy algorithm by implementing an SRAM utilization refinement step, which identifies layers in the network exceeding a fixed SRAM allocation, and fine-tunes the mapping to bring these layers within the SRAM budget.

\je{\textbf{Tile Shutoff}:}
\je{We also consider turning off tiles, PEs, or SCUs within a processor to be an aspect of mapping.}
\je{Partial processor shutoff can be used on a per-frame basis to conform the compute capacity of the processor to the current task.}
\je{This allows dynamic energy efficiency to be sacrificed in order to reduce static power draw for the duration of processing a given frame.}
\je{Like with DRAM Modes, we limit our evaluation to a small number of processor sub-configurations to simplify the mapping design space.}

% \subsection{Indexing Mapping Design Space}\label{subsec:phase1}

% To efficiently describe the mapping design space, we identify three orthogonal axes to index the design space. 
% We then sweep these axes in relation to the ROI sizes to generate mappings. 
% Finally, we analyze key metrics of these mappings, including runtime latency, SRAM utilization, and dynamic energy.

% \textbf{Design-Time ROI Axis}: The choice of per-layer dataflow impacts both SRAM utilization and dynamic energy consumption.
% Tradeoffs between SRAM utilization (and by proxy, static energy) and dynamic energy must be made to minimize total energy.
% To simplify layer dataflow selection, we describe a series of mappings with different tradeoffs between SRAM utilization and dynamic energy, indexed by \textit{design-time ROI} sizes.
% For a given processor, we first find the absolute minimum SRAM needed to support the largest expected ROI size.
% Next, we apply our augmented greedy mapping algorithm using this SRAM budget.
% For much smaller ROIs, this SRAM budget has minimal impact on per-layer dataflow selection, as the small activations do not approach the SRAM budget.
% However, for larger ROIs, compromises need to be made to meet the SRAM budget.
% This sequence of network dataflow selections is indexed by a single design-time ROI axis.
% As we sweep through the design-time ROI axis, the corresponding dataflow selections
% are applied to various \textit{runtime ROI sizes} to form low-level mappings, realizing variable tradeoffs between mapping dynamic energy and static energy.

% \textbf{Processor Utilization Axis}:
% We may wish to utilize the entire L2 processor when processing large ROIs, but power gate all but a small portion when processing small ROIs to save energy.
% We thus enumerate the possible compute subconfigurations, ordered in terms of compute capacity to produce the third axis.

% \subsection{Binning Runtime ROIs}\label{subsec:phase2}
\subsection{Multiple-ROI Binning}\label{subsec:multi_roi}

% \je{\textbf{Mapping Descriptors}:}
\je{We consider the combination of a DRAM Mode, a dataflow assignment for each network layer, and a processor sub-configuration to constitute a \textit{mapping descriptor}.}
\je{From this mapping descriptor, a \textit{low level mapping} may be trivially generated by assigning compute operations and activation storage to the PEs, SCUs, and SRAMs within a processor.}
\je{While generating a low-level mapping requires knowledge of the ROI and intermediate feature sizes, a mapping descriptor is agnostic of this detail.}
\je{For HITNet, storing a mapping descriptor takes on the order of 100s of Bytes of memory.}

\je{To support the full range of possible ROI sizes efficiently, we propose to divide this range of sizes into a finite number of intervals, and assigning a separate high level mapping descriptor to each interval.}
\je{Low level mapping can then be performed at runtime with minimal overhead.}
\je{This scheme allows for the mapping descriptors to be optimized offline while still enabling runtime ROI variability.}
\je{Additionally, using separate mapping descriptors for different ROI size intervals allows for mappings to be customized for each size; for example, larger ROIs may use mappings that focus more heavily on reducing SRAM utilization, while mappings for smaller ROIs can focus entirely on maximizing energy efficiency.}
\je{We combine optimized mapping design with architecture architectural design exploration.}
\je{In this way, we are able to determine the optimal architecture design for different ROI probability distributions.}

% We propose ROI range binning to process the continuous range of ROI sizes efficiently.
% A binning divides the range of runtime ROI sizes into $n$ intervals.
% The ROIs within an interval are then mapped according to a single point in the mapping design space.
% This phase also determines the allocation of system SRAM allocation based on the usage of SRAM within each bin.

% By representing a bin as the coordinates of the interval boundaries and 
% the coordinates of the chosen mapping in the mapping design space, we can use simulated annealing to optimize binning.
% This representation can be visualized geometrically, as seen in Fig.~\ref{fig:binning_explanation}, with a bin represented by a straight line in the mapping design space.
% However, as the dimensionality of this representation increases with the number of intervals, we face the challenge of simulated annealing getting stuck in local minima.
% To address this, we use \textit{multiphase binning annealing}.
% This process first optimizes a single interval; then splits this interval in two sub-intervals and optimizes each; and the process repeats as needed.

% \subsection{Sweeping Processor Configurations}\label{subsec:phase3}

% \textbf{Design Space Exploration}: To systematically and efficiently navigate this vast and complex mapping design space, we propose a three-phase design space exploration.
% In \textbf{Phase 1} (Sec. ~\ref{subsec:phase1}), we use three orthogonal axes to traverse the design space of mappings for a set of runtime ROIs onto a processor of known compute capacity.
% In \textbf{Phase 1} (Section~\ref{subsec:phase1}), we use three orthogonal axes to generate high level descriptions of mappings for different ROI sizes on a processor of known compute capacity.
% In \textbf{Phase 2} (Section~\ref{subsec:phase2}), we partition the range of ROI sizes into intervals, or ``bins''.
% This enables runtime mapping, where each runtime ROI within a particular bin is assigned the same mapping.
% Each interval is then assigned a high level mapping descriptor offline; mappings for ROIs within an interval can then be quickly generated at runtime.
% Finally, in \textbf{Phase 3} (Section~\ref{subsec:phase3}), we perform Phases 1 and 2 for many processor configurations, evaluating core area, average energy, and latency. 
% This process yields a pareto-optimal curve of potential L2 processor configurations and ROI binnings.

% The design so far has assumed fixed processor compute resources in terms of the number of PEs and SCUs.
% We expand the design space sweep over a range of potential L2 processor designs to perform a complete design space exploration.
% This sweep allows us to evaluate tradeoffs between fine and coarse-grained compute structures in the processor, as well as to determine many candidate processors and optimal binnings for them.
