% \section{\projname{} L2 Processor Architecture}\label{sec:architecture}
\section{System and Architecture Design}\label{sec:architecture}

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/System_2024_06_03.pdf}
    \caption{\projname{} system design.}
    \label{fig:steroi_compare}
\end{figure*}

\textbf{System Design}:
The system-level design of \projname{} is depicted in Fig.~\ref{fig:steroi_compare}. %, while the processing pipeline is illustrated in Fig.~\ref{fig:alg_design}.
\je{The design features stereo sensors and a custom accelerator integrated into an SoC, which is a typical design for AR platforms \cite{aria}.}
% \projname{} uses a detect-and-track strategy for energy-efficient ROI extraction.
% Since the compute complexity of object tracking is significantly less than object detection, \projname{} employs only periodic object detection to detect the objects (ROIs) and uses lightweight object tracking to efficiently follow these ROIs.
% The large difference in computing demands between object tracking and object detection means that we can achieve energy savings even when object detection is run frequently.
\je{Furthermore, each sensor is co-packaged with a lightweight L1 processor, based on \cite{siracusa}}.
\je{These processors are responsible for handling the object tracking algorithms used to extract ROIs; in this way, they enable saving energy when transmitting ROIs from the L1 processor to the SoC and the accelerator (heretoafter referred to as the \textit{L2 Processor}.}
% \projname{} incorporates a small level-1 (L1) processor co-packaged with each image sensor in the AR/VR system for efficient object tracking using correlation filters \cite{correlation_filter, vota}. 
% This approach significantly reduces the need for costly object detection, which is performed only occasionally using the level-2 (L2) processor.

\begin{figure}
    \centering
    \includegraphics[width=0.8\linewidth]{Figures/Architecture_2024_07_04.pdf}
    \caption{\projname{} L2 processor architecture. The Special Compute Unit (SCU) accelerates non-parameterized compute patterns.}
    %The design is flexible to permit efficient adaptation to dynamic ROIs.}
    \label{fig:arch_sketch}
\end{figure}

\je{\textbf{Accelerator Architecture}:}
The L2 processor is responsible for object detection and ROI-based stereo depth processing %, which is the most significant challenge.
The \je{accelerator} uses a parameterized hierarchical \je{architecture}, pictured in Fig.~\ref{fig:arch_sketch}, which draws inspiration from ANSA \cite{ansa}.
The architecture is composed of tiles, with each tile consisting of a collection of PEs. 
Each PE is responsible for executing convolutions and activation functions through a Vector-Matrix Multiplier (VMM), local SRAMs, and a shuffle buffer for depthwise convolutions.
Flexibility is achieved through both the hierarchical structure, allowing for dynamic reconfiguration of compute resources for different compute tasks\je{;} and the compute units themselves, which support multiple dataflows to enable optimization at the mapping level.
\je{Power gating is further used to capture further energy savings by disabling unused resources on a per-frame basis.}
We have expanded on ANSA to accommodate the diverse sizes of ROIs while maintaining energy efficiency. 

\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{Figures/Operations_2024_04_16.pdf}
    \caption{Conventional CNN operation counts (e.g. convolution MACs) and stereo depth specific operations in stereo depth networks in recent years on 384$\times$1280 images.}
    \label{fig:ops_compare}
\end{figure}

\textbf{Special Compute Unit (SCU)}: 
% Non-parameterized operations specific to stereo depth must be supported efficiently and with low latency to prevent them from becoming bottlenecks.
Stereo depth networks consist of both CNN layers and stereo-depth-specific non-parameterized layers, as illustrated in Fig.~\ref{fig:ops_compare}.
% Non-parameterized operations specific to stereo depth must be supported efficiently and with low latency to enable real time, high frame rate operation.
These operations, seen in many networks \cite{stereonet, hitnet, tiefenrausch}, are used for processing disparity estimates.
\je{They use operations and data broadcast patterns for which conventional linear algebra engines are ill-suited, such as vector L1 norm and list argmin.}
\je{While not the majority of operations in any network, they constitute a sufficient portion of the network to necessitate hardware support to enable low latency, high framerate processing.}
Supporting these operations directly in PEs would increase their complexity and footprint, and reduce efficiency for both CNN and special operations.
Therefore, the \projname{} L2 processor uses both PEs for CNN compute, and special compute units (SCUs) for special operations.

To design an efficient SCU, we first observe that the cost volume processing in \cite{stereonet, hitnet, tiefenrausch} employs a limited set of compute primitives: vector-vector difference, L1 norm, sequence minimum, and argmin.
We propose a SCU pipeline with bypass options to handle these operations as well as their compositions.
The resulting SCU design captures various variants of cost volume processing for each stereo depth network, along with the warp and aggregate operations from \cite{hitnet}, and the maxpool operation in \cite{redmon_yolov3_2018}. 
\ca{A single SCU is allocated per Tile; the correct balance of SCUs to PEs is then realized through design space exploration over the mapping design.}

\textbf{Mutipacket NoC Routing}: 
The \projname{} L2 processor uses hierarchically arranged mesh NoCs connecting tiles globally and PEs locally.
These NoCs allow for fine-grained partitioning of compute tasks. 
However, this flexibility also costs redundant data movements.
To mitigate this cost, the \projname{} L2 processor uses a multipacket NoC. %, illustrated in Fig.~\ref{fig:multipacket}.
A multipacket consists of a data packet and a list of destination nodes. 
When a NoC node receives a multipacket, it forwards the data to the remaining destination nodes in the network.
To minimize the overhead of this scheme, we use simple Direction Order Routing (DOR) \cite{eecs570_dor}.
With DOR, each data packet is sent over a given link only once.
This approach reduces data movement while maintaining high flexibility in compute unit allocation.

% \begin{figure}
%     \centering
%     \includegraphics[width=0.6\linewidth]{Figures/Multipacket_2024_07_04.pdf}
%     \caption{Multipackets address multiple destination nodes with a single packet.}
%     %This allows efficient transfer through bottlenecks, such as DRAM I/Os or tile-tile links.}
%     \label{fig:multipacket}
% \end{figure}