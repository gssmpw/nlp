\begin{figure*}[t]
    \centering
    \includegraphics[width=0.92\textwidth]{figures/Fig4.pdf}
    \caption{Qualitative results of experiments comparing (a) predicted occupancy against ground truth for corresponding multi-camera input, (b) predicted occupancy for the area covered by one camera, comparing input from one camera versus input from all cameras, and (c) better resolution in occupancy map with increasing grid size.}
    \label{fig:fig4}
\vspace{-0.2in}
\end{figure*}

% \newpage


\section{EXPERIMENTAL EVALUATION}
\label{experiments}

\subsection{Experiment Setup and Model Training}

We establish a late fusion baseline and compare it with adapted early fusion methods: MVDeTr, MVDet, GMVD. We also augment each early fusion method with the background (denoted as ``+BG''). These models were trained on $16$ scenes of the generated dataset and evaluated on four unseen scenes, two from Town 10 and two from other towns. Each model was trained on one single NVIDIA A100 GPU.

The occupancy map size was set to 480$\times$480 by default, with a resolution of $0.31\,\text{m}$ $\times$ $0.31\,\text{m}$ per grid cell. All early fusion models were trained with a batch size of $2$ for $8$ epochs at a learning rate of $0.005$. The late fusion baseline is trained at a higher learning rate of $0.05$ and a batch size of $1$, which generated the best result in hyperparameter tuning.

\subsection{Evaluation Metric}
\vspace{-0.02in}
For all experiments, intersection over union (IoU), also known as the Jaccard index~\cite{costa2021further}, is used to quantitatively measure the model's performance by calculating the overlap between the predictions and the ground truth occupancy. 
% If the IoU value is $1$, the prediction and ground truth are perfectly matched. If the value is $0$, there is no intersection.
The predicted probabilities are threshold at $0.5$ to obtain the predicted binary occupancy map $M$, which is compared with the ground truth binary occupancy map $O$ to obtain the IoU,

\begin{equation}
    \text{IoU} = \frac{\sum_{x,y}MO}{\sum_{x,y}M + \sum_{x,y}O - \sum_{x,y}MO}.
\end{equation}
% \vspace{-0.1in}

\subsection{Performance Evaluation and Model Comparison}

As shown in \tableautorefname~\ref{tab:model_results}, the late fusion model, serving as the baseline, performs the worst with an IoU of 0.20388, indicating its limitations in capturing the necessary spatial relationships. In comparison, all early fusion models significantly outperform the baseline, demonstrating the effectiveness of early fusion techniques. Particularly, the simple average pooling method adapted from GMVD has marginally better results than MVDet and MVDeTr. Furthermore, adding background information to these models consistently boosts the performance. Here, (MVDeTr + BG) achieved the best IoU of 0.67759, which is a 43\% improvement, followed by (MVDet + BG) and (GMVD + BG).  This highlights the crucial role of background information in generalization to new unseen scenes. An example qualitative result is shown in Fig. \ref{fig:fig4} (a) with (MVDeTr + BG).

\begin{table}[h]
\centering
\begin{tabular}{rccc}
\hline
\textbf{Model} & \textbf{IoU (Model)} & \textbf{IoU (Model + BG)} \\ \hline
Early Fusion (GMVD) & \textbf{0.49454} & 0.60671 \\
Early Fusion (MVDet) & 0.48755 & 0.66986 \\ 
Early Fusion (MVDeTr) & 0.473734 & \textbf{0.67759} \\
\hline
Late Fusion & 0.20388 & -\\

\hline
\end{tabular}
\caption{Performance results of proposed models}
\label{tab:model_results}
\vspace{-0.2in}
\end{table}

\begin{figure*}[!th]
    \centering
    \includegraphics[width=0.94\linewidth]{figures/Fig5.pdf}
    \caption{Real-world data collection. The top row shows the pipeline to get BEV occupancy from aerial images using oriented bounding box detections. The bottom row presents camera setup and predicted BEV occupancy with and without fine-tuning.}
    \label{fig:fig5}
\vspace{-0.2in}
\end{figure*}
\subsection{Ablation Study: Impact of Multi-camera Input}

The impact of multi-camera input on the performance was assessed by evaluating the trained models over the area covered by a single camera with input from that camera against input from all four cameras, as shown in Fig.~\ref{fig:fig4} (b). The results are shown in \tableautorefname~\ref{tab:view_results}. The results clearly show that using images from multiple cameras can help the models predict the occupancy map better, which validates the need for multi-camera input for better information fusion.

\begin{table}[h]
\centering
\begin{tabular}{rcc} \hline
\textbf{Model}  & \textbf{Camera 1 Only }   & \textbf{All Cameras} \\ \hline
GMVD        & 0.4146                       & 0.4984         \\ 
GMVD + BG   & 0.1739                       & 0.6076         \\ 
MVDet       & 0.2644                        & 0.4932          \\
MVDet + BG  & 0.3139                       & 0.7319          \\
MVDeTr      & 0.2438                      & 0.5031          \\
MVDeTr + BG & \textbf{0.5047}             & \textbf{0.7544}          \\
\hline
\end{tabular}
\caption{Performance comparison of single-camera vs. multi-camera. IOU results are only computed under the area covered by camera 1 for fair comparison. See Fig.~\ref{fig:fig4} (b) for an example.}
\label{tab:view_results}
\vspace{-0.1in}
\end{table}


\subsection{Ablation Study: Impact of Different Occupancy Map Sizes}

We further explored the impact of varying BEV occupancy map resolution and size on the model's performance by training additional early fusion models with (MVDeTr + BG). Examples of the effects of the different occupancy map sizes on ground truth occupancy is shown in Fig. \ref{fig:fig4} (c). The results are shown in \tableautorefname~\ref{tab:grid_results}.

\begin{table}[h!]

\centering
% \begin{tabular}{@{}lc@{}}
% \hline
% \textbf{Model}    & \textbf{IoU} \\ \hline \hline
% 240$\times$240 & 0.5168             \\
% 320$\times$320 & 0.5353             \\
% 480$\times$480 & 0.5802             \\
% 640$\times$640 & 0.5748             \\ \hline \hline
% \end{tabular}
\begin{tabular}{c|c|c|c|c}
\hline
\textbf{Occ. map size} & 240$\times$240 & 320$\times$320 & 480$\times$480 & 640$\times$640 \\ \hline
\textbf{MVDeTr + BG}      & 0.5480  & 0.5659  & \textbf{0.6776}  & 0.6220  \\ \hline
\end{tabular}
\caption{Impact of the BEV grid resolution.}
\label{tab:grid_results}
\vspace{-0.2in}
\end{table}

From the table, the performance peaks at the grid size of 480$\times$480. On the one hand, we believe that the rasterization error limits the performance at low BEV grid resolution, as illustrated in Fig.~\ref{fig:fig4} (c). On the other hand, we speculate that the model struggles with precision but has good recall at high BEV grid resolution. One reason is that vehicles are usually not boxy around the corners, and another reason may be the fact that the chassis of vehicles are lifted above the ground by the suspension system, which makes it difficult for the neural network to precisely find the exact boundaries of the 3D bounding boxes projected on the ground from the image features. Additionally, the gains in performance with higher grid resolution come at the cost of increased computational requirements, which is an important consideration for deploying such models in real-world scenarios.

\subsection{Real-World Evaluation}

We assessed the ability of our models trained on synthetic data to generalize to a real-world scene. With the assistance of the Maricopa County Department of Transportation in Arizona, we obtained synchronized videos from a system of four infrastructure-based traffic monitoring cameras \cite{lu2021carom} deployed at an intersection. We also flew a drone and developed a vehicle localization pipeline \cite{lu2023carom} from aerial videos, as shown in Fig.\ref{fig:fig5}.

We selected three and a half minutes of video taken at $30$ fps, collected simultaneously from the four infrastructure cameras and the drone to evaluate the real-world performance of our proposed framework. This data was then time-synchronized, yielding $6400$ data samples, where each data sample contains four synchronized images and the reference measurement from the drone. The occupancy grid map size and image size were kept consistent with our synthetic dataset, but the grid cell resolution was reduced to $0.5\,\text{m}\times 0.5\,\text{m}$ because the infrastructure-based cameras cover a larger area. Our drone-based method has a vehicle location accuracy of $0.1\,\text{m}-0.3\,\text{m}$ \cite{lu2023carom}, which is approximately half the size of one cell in our BEV grid space. Hence, it is adequate to be used as the ground truth.

The early fusion methods with background information were evaluated in both zero-shot and few-shot fine-tuning settings with 24 samples. Two fine-tuning strategies were designed to test different aspects of the model's performance. In the first setting, the backbone and bottleneck were frozen while the aggregation layer and the head were fine-tuned. This approach is intuitive but it does not address the sim-to-real gap in the backbone features. In the second setting, the aggregation layer and head were frozen while the backbone and bottleneck were fine-tuned. This is the typical case in practice and it is designed to study the domain adaptation capability as well as the limitation of the aggregation layers.


\begin{table}[h!]
\centering
\resizebox{\columnwidth}{!}{%
\begin{tabular}{rccc}
\hline
\textbf{Model} &
  \textbf{Zero Shot} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Few Shot \\ (Frozen Backbone)\end{tabular}} &
  \textbf{\begin{tabular}[c]{@{}c@{}}Few Shot \\ (Frozen Head \& Agg.)\end{tabular}} \\ \hline
GMVD + BG   & 0.0282  & 0.28408 & \textbf{0.67013} \\
MVDet + BG  & 0.1180  & \textbf{0.64481} & 0.64348 \\
MVDeTr + BG & \textbf{0.19141} & 0.64316 & 0.66936 \\
\hline
\end{tabular}%
}
\caption{Performance evaluation of Models in real-world under zero-shot and few-shot fine-tuning settings.}
\label{tab:few-shot-table}
\vspace{-0.2in}
\end{table}

% The simulation-to-real-world gap becomes apparent upon evaluating the MVDeTr model without fine-tuning on real-world data. This performance limitation can be attributed to the lower confidence in occupancy prediction and a few instances of false positives, i.e., unseen background features in the real world that the model mistakes as objects. To address this, we fine-tuned the model using different numbers of images and evaluated it on the unseen $5400\times4$ images. The fine-tuning significantly improved model performance from an IoU of $0.059$ to an IoU of $0.7115$. However, the performance gain starts to diminish after $100$ samples, suggesting that while more samples could benefit the performance, that model can generalize to the real-world scene on fine-tuning with just $100$ samples, i.e., $<~1\%$ of the training data. This indicates that the model has sufficient foundational knowledge from training on synthetic data to generalize on real-world data with just a small amount of fine-tuning work.

% The results demonstrate that all models exhibit limited zero-shot generalization from synthetic to real-world data, with MVDeTr + BG achieving the highest IoU score of 0.19141 followed by MVDet + BG with IoU of 0.1180. GMVD performs poorly in this zero-shot setting with IoU of 0.0282. However, fine-tuning in the few-shot setting significantly improves performance across all models. Notably, freezing the backbone while fine-tuning the aggregation layer and head leads to substantial gains, particularly for MVDet + BG and MVDeTr + BG, which achieve IoU scores around 0.64. This gain is not reflecting with GMVD model which achieves an IoU of 0.2840 in this setting. Fine-tuning the backbone while freezing the aggregation layer and head yields comparable or higher performance across all models with GMVD achieving the best performance with IoU of 0.67013. One possible explanation for improved performance of GMVD can be explained by the lack of learnable parameter in aggregation layer as it uses average pooling for aggregating the feature maps, which puts a heavy reliance of the backbone to extract better feature maps. Since MvDeTr and MvDet have learnable parameters in aggregation this is split between the backbone and the aggregation layer.  The results highlight the models’ ability to generalize from synthetic to real-world data when fine-tuned on a small set of real-world samples. The consistent performance improvements in the few-shot settings indicate that both fine-tuning strategies are effective, while setting with frozen aggregation and head can be beneficial in scenarios where a backbone pre-trained on other tasks can be reused, reducing the need for retraining the aggregation layer. 

The results indicate that all models show limited zero-shot generalization performance to real-world data, while both few-shot fine-tuning strategies improve performance significantly with only 24 data samples. Particularly, when fine-tuning the backbone and bottleneck while freezing the aggregation layer and head, the simple method with average pooling in the aggregation layer (GMVD) achieves marginally better results than the other two methods with lots of learnable parameters in the BEV space, placing greater reliance on the backbone for domain adaptation. In contrast, MVDeTr and MVDet distribute this burden between the backbone and the aggregation layer. These findings highlight the models' capability to generalize from synthetic to real-world data with fine-tuning. We speculate that it might be sufficient to fine-tune the backbone in other simpler tasks to address the sim-to-real gap, such as semantic segmentation in the image domain with a single camera, and this will be explored in our future work.

% \begin{figure}[t]
%     \centering
%     \includegraphics[width=0.70\linewidth]{figures/finetuning_results.png}
%     \caption{Evaluation on real-world data}
%     \label{fig:finetune_results}
%     % \vspace{-0.15in}
% \end{figure}

% \begin{table}[h]
% \centering
% \begin{tabular}{cccc} \hline
% \textbf{Model}  & \textbf{Zero Shot }   & \textbf{Freeze Backbone} & \textbf{Freeze Rest}\\

% \hline
% GMVD + BG   & 0.0282  & 0.2841 & 0.6701   \\ 
% MVDet + BG  & 0.1180  & 0.64481 & 0.6435     \\
% MVDeTr + BG & 0.1914  & 0.6432  & 0.6694     \\
% \hline
% \end{tabular}
% \caption{Evaluation on Real world data}
% \label{tab:real_world_results}
% \vspace{-0.2in}
% \end{table}