
\section{Multi-camera BEV Occupancy Dataset}
\label{sec:datagen}

To the best of our knowledge, there is no existing open dataset for multi-camera road occupancy detection for traffic monitoring on the roadside or from the road infrastructure. In this paper, we utilized CARLA~\cite{dosovitskiy2017carla} to generate a synthetic dataset. Town, 10 of the simulator, was chosen as the primary focus due to its larger urban area, numerous intersections, wide varieties of traffic scenes in an urban environment, flexible locations for pedestrians and vehicle generation, and close resemblance of real-world traffic patterns. Twelve traffic scenes were used from this town with four cameras at each scene. Additionally, we also included six other towns in the simulator for data collection, which further improves the diversity in traffic scenarios. Some of the six towns had distinct visual features, such as light-colored asphalt, which can negatively affect model performance on real-world data with dark road surfaces. Eight scenes were used from these six towns with the same camera settings. For each scene, we consulted road transportation experts from the Institute of Automated Mobility \footnote{\url{https://www.azcommerce.com/iam/}} and strategically placed the four cameras to monitor road traffic (mainly for vehicles) within a region of interest. Specifically, the cameras were positioned at a height varying from $5\,\text{m to }\,8\,\text{m}$, covering a distance roughly from $50\,\text{m} - 150\,\text{m}$. Each camera had a resolution of $1920\times1080$ and a field of view of $90^{\circ}$ capturing data at $20\,\text{ frames per second}$. The degree of camera overlap varies across the scenes to evaluate the system's ability to aggregate information from multiple views and ensure consistent detection. Examples are shown in Fig. \ref{fig:fig2}.
% The overlapping views provided by this setup are crucial for validating the use of multiview.


% varying landscape, location and configs

% The behavior of pedestrians and vehicles was generated by CARLA's default controller with randomly configured paths and movement speeds. Each scene features various number of vehicles $(70-90)$ and pedestrians $(20-30)$. The pedestrians were spawned within a distance of $50\,\text{m}$ from the cameras, while vehicles were spawned at the nearest $n$-vehicle spawn locations predefined by the simulator. These settings created diverse and dynamic traffic conditions that closely mimic the real world. 
For each simulated vehicle and pedestrian in CARLA, its position $(x, y)$, dimension $(l, w)$, orientation $(\theta)$, and the $2D$ bounding boxes on each camera view are stored.
% This can be set manually or based on the area covered by the cameras in the real world.
To generate the ground-truth BEV road occupancy at a traffic scene, we first specify a reference origin $(x_0,y_0)$ and determine the total area in the BEV space covered by all four cameras. Next, we use a predefined occupancy grid size (e.g., $480\times480$) and the cell's metric resolution $(\Delta x, \Delta y)$ to define the BEV grid boundary based on the actual camera coverage. This step is critical for maintaining consistent cell resolution in meters across the dataset so that our models can be generalized to a new traffic scene. Finally, we use the calculated cell resolution and map size to transform each vehicle's position $(x, y)$ and dimension $(l, w)$ from the world reference frame to our BEV space position $(g_x, g_y)$ and dimensions $(g_l, g_w)$ as shown in the following equations. Finally, we generate ground-truth binary occupancy by rasterizing the objects in the BEV grid space. An example is shown in Fig. \ref{fig:fig1}.

\begin{align}
\label{eq:grid_generation}
    (g_x, g_y) &= (\frac{x - x_0}{\Delta x}, \frac{y - y_0}{\Delta y}) &
    (g_l, g_w) &= (\frac{l}{\Delta x}, \frac{w}{\Delta y})    
\end{align}

\begin{figure}[t]
    \centering
    \includegraphics[width=1\linewidth]{figures/Fig2.pdf}
    \caption{Samples from the generated multi-camera occupancy dataset showcasing diverse scenes and perspectives.}
    \label{fig:fig2}
\vspace{-0.4in}
\end{figure}

This procedure is conducted for each image, and the generated data consists of $500$ images from each camera at each scene. Our dataset has 8 three-way intersections, 6 four-way intersections, and 6 road segments, which ensures a balanced representation of urban environments with different traffic dynamics. In total, the dataset has $160K$ traffic participants with $70K$ vehicle instances and $90K$ pedestrian instances.  

\section{Multi-Camera BEV Occupancy Detection}
\label{sec:baseline_methodologies}

This section details the proposed framework for BEV occupancy detection using multi-camera inputs for traffic monitoring. We first describe the framework's inputs and outputs, followed by an explanation of the late fusion approach and three early fusion methods. Moreover, we demonstrate that incorporating static background images from the cameras can further enhance model performance and generalization across different scenes.

\subsection{Input and Output}

The proposed framework takes images from four static cameras and their calibration as input. These images are processed to generate two BEV occupancy grid maps: one for vehicles and the other for pedestrians. Each occupancy grid map represents a BEV representation of the scene, where the area is divided into discrete cells, and each cell in the map indicates a probability of whether it is occupied. Pedestrians are typically represented by Gaussian centered at a single cell, while vehicles are represented by multiple cells due to their larger size. This occupancy representation provides a comprehensive view of the environment without focusing on individual object detections.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.98\linewidth]{figures/Fig3.pdf}
    \caption{An overview of implemented models (top) the baseline late fusion model predicts per-camera occupancy maps, fused via mean aggregation, (middle) the early fusion model fuses projected features using a neural network, (bottom) the integration of background information is depicted.}
    \label{fig:fig3}
\vspace{-0.25in}
\end{figure}
\subsection{Late Fusion Approach}

A late fusion approach is implemented where images from each camera are processed independently. As shown in \figureautorefname~\ref{fig:fig3}~(top), each image passes through a ResNet-18~\cite{resnet} backbone, where the last three stridden convolutions are replaced by dilated convolutions to enhance spatial resolution. This step extracts features from each view, which are then transformed using a homography projection onto the ground plane of the BEV space. A final convolution layer predicts the BEV occupancy for the area covered by each camera, which is then fused using mean aggregation. The model shares weights across all camera views to ensure consistency, and the independent predictions are combined to generate the final occupancy map.

\subsection{Early Fusion Approaches}

In addition to the late fusion approach, we developed three early fusion methods with a similar architecture as shown in \figureautorefname~\ref{fig:fig3}~(middle). All three methods utilize the same ResNet-18 backbone and generate the two occupancy maps in the same format. However, a bottleneck layer is used to reduce the total number of feature maps, and each early fusion method employs a unique feature aggregation technique in the BEV space, shown as the aggregation layer in the figure. They are adapted from prior BEV human detection works as follows.

\textit{MVDet} \cite{hou2020multiview} utilizes three layers of dilated fully convolutional layers on the projected feature maps to use a relatively large receptive field to consider the ground plane neighbors jointly. We adapt these convolutional layers as the aggregation layer.

\textit{MVDeTr} \cite{hou2021multiview} uses a deformable transformer on a projected feature map obtained from multi-camera images, where each point in the resultant feature map is obtained by applying an attention mechanism on a fixed set of reference points across the projected feature maps of all views for each attention head. We adapted this transformer in our BEV space as the aggregation layer.

\textit{GMVD} \cite{vora2023bringing} uses average pooling for spatial aggregation of the projected feature maps. We adapted the same average pooling method as the aggregation layer.

\subsection{Improved Generalization via Background Integration}

To enhance generalization across different scenes, we integrate static background images from each camera view into the framework as illustrated in \figureautorefname~\ref{fig:fig3}~(bottom). These static background images are passed through the same ResNet-18 backbone used for processing the current images. In simulation, they are just static images when no traffic participants are spawned. In practice, they can be computed through an online rolling background subtraction method. The backbone generates a feature map of shape $(C, H, W)$ for each of the $N$ current images and their corresponding $N$ background images, resulting in a combined $2N$ feature map of shape $(2N, C, H, W)$. The features from the current and background images are then concatenated along the channel dimension, transforming the feature map from $(2N, C, H, W)$ to $(N, 2C, H, W)$. This concatenated map is subsequently passed through a bottleneck layer consisting of a $1\times1$ convolution, which reduces the feature dimensions to $(N, Câ€™, H, W)$. Each output channel from the bottleneck layer is a linear combination of the $2C$ channels corresponding to the current image and background features \cite{lin2013network}. The resulting features are then fed into the aggregation layers of the early fusion methods, producing two BEV occupancy maps for vehicles and pedestrians. Additionally, background features can be cached and reused to reduce computational overhead during inference, which can significantly speed up the process.
