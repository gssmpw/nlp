\section{INTRODUCTION}

The growing number of vehicles has led to many road transportation problems, such as traffic congestion, parking difficulties, environmental impacts, accidents, and safety of pedestrians~\cite{rodrigue2020geography}. Such problems have driven a focus on developing and deploying intelligent transportation systems (ITS). One of the critical technologies in ITS is roadside perception, which converts the raw sensor data to traffic condition information that can help traffic operation decision-making~\cite {xiong2012intelligent}. A core component of such a roadside perception system is $3D$ traffic participant detection, which traditionally uses $3D$ bounding boxes to estimate the objectâ€™s $3D$ position, dimension, and orientation in a world reference frame~\cite{arnold2020cooperative}.

However, such object-based traffic participant representation significantly increases the complexity of the detection algorithm. Particularly, it requires a large dataset with manually annotated traffic participants in 3D space, which is time-consuming and labor-intensive to obtain. Also, it is difficult and often unnecessary to identify individual objects in certain cases, such as a cluster of pedestrians or a trailer towed by a truck. On the contrary, it is usually sufficient to know that an area on the road is occupied by such objects. 

Existing works \cite{ye2022rope3d}\cite{yang2023bevheight}\cite{Agrawal2024SemiAutomaticAO} that focus on $3D$ perception from a single view are inherently vulnerable to impairments, including occlusion, restricted perception horizon, and limited coverage. Even if multiple monocular cameras are used in a network, fusing detected objects from different views is error-prone when the individual object detection algorithms running on each view generate conflicting results due to their performance limitation. Additionally, in real-world deployment, traffic monitoring cameras may be mounted at different heights with different pitch angles and coverage, and hence, it is difficult for such object detection algorithms to generalize to different situations.

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{figures/Fig1.pdf}
    \caption{An overview of BEV road occupancy detection using multiple roadside cameras.}
    \label{fig:fig1}
\vspace{-0.25in}
\end{figure}

To address these challenges, we propose a Bird's-Eye-View (BEV) road occupancy detection framework to unify multiple roadside traffic monitoring cameras. In our proposed framework, the image features from each view are transformed into the BEV space and fused together, which increases the overall perception horizon and overcomes occlusion, as shown in Fig. \ref{fig:fig1} (left). Moreover, a traffic scene is represented by an orthographic top view where the region of interest is divided into equidistant cells (i.e., BEV occupancy grid map \cite{bieder2021improving}\cite{schreiber2021dynamic}), with each grid cell encoding a probability on whether it is occupied or not, as shown in Fig. \ref{fig:fig1} (right). While objects on the image may vary in scale due to each camera's image resolution and field-of-view, the BEV occupancy is scale invariant. Moreover, when an object is observed in different views, the fused BEV features can better capture its pose and shape. Additionally, other than road occupancy, such BEV features can be used to predict lane occupancy \cite{kaniarasu2020goal}, traffic flow \cite{kashinath2021review}, and potential collision \cite{yu2018space}. The proposed framework is also compatible with real-world implementation architecture in ITS \cite{arnold2020cooperative}, where multiple cameras are typically linked to a central control box on the roadside, resembling the control system of multiple traffic lights at an intersection.


% To fully utilize these multiple views, the data from these views needs to be aggregated, this can be achieved through early or late fusion of the views. Early fusion involves combining images from different cameras before processing, which allows for extracting depth information from multiple views. Late fusion involves independently processing images from each camera and combining the detection results. Late fusion methods are less computationally intensive but require effective strategies to resolve conflicting information from different views. The evaluation of various methods in (\cite{chavdarova2018wildtrack}) showcases that an early fusion method outperforms late fusion methods and has been used in this work.

% While multiview occupancy detection is explored for people tracking and autonomous driving, its use in ITS is limited by the lack of multiview dataset. Given the ambiguous mounting position of infrastructure cameras due to differing urban landscapes and installation constraints, it is imperative for such dataset to include diverse camera configurations and locations for developing methods adaptable to different settings without extensive retraining \cite{vora2023bringing}. This, compounded by resource limitation, difficulties in camera calibration, the time required for annotation, and privacy concerns, makes gathering multiview real-world infrastructure data from varying locations challenging. 

% Lastly, by adopting a BEV-based approach, we propose a simplified yet effective method for understanding traffic dynamics and vehicle positioning while allowing many ITS applications. This highlights the need for a multiview-based BEV occupancy detection dataset for ITS.

In summary, we make the following contributions:

\begin{itemize}
    \item We propose a BEV road occupancy detection framework using multiple roadside cameras, implement a late fusion baseline and adapt three early fusion models. We further improve these early fusion models by incorporating backgrounds achieving a 43\% boost in performance.
    
    \item Motivated by the lack of multi-camera BEV occupancy detection dataset, we create a synthetic dataset using the CARLA simulator \cite{dosovitskiy2017carla}. It covers diverse traffic scenes encompassing multiple cameras with varying viewing angles and positions. Our evaluation and ablation studies reveal insights into how the use of multiple cameras and occupancy map resolution impact performance.

    \item To address the practicality and the simulation-to-real-world gap, we develop a pipeline for obtaining real-world data and demonstrate the generalization capabilities of our models using zero-shot and few-shot fine-tuning approaches.
    
\end{itemize}


% The rest of the paper is organized as follows. Section~\ref{sec:related_works} surveys existing literature, setting the stage for our contributions. Section~\ref{sec:datagen} introduces the approach for synthetic generation of the proposed multi-camera occupancy detection dataset. Section~\ref{sec:baseline_methodologies} details the proposed multi-camera road occupancy detection framework for traffic monitoring. Section~\ref{experiments} evaluates the models trained on the proposed dataset, investigates the impact of multi-camera setups and grid resolution on performance, and reports the results of zero-shot and few-shot fine-tuning using real-world data. Finally, Section~\ref{conclusion} concludes the findings and suggests future works.

%The paper is organized as follows: Section~\ref{sec:related_works} reviews the related works. Section~\ref{sec:datagen} presents our approach for generating the multi-camera occupancy detection dataset. Section~\ref{sec:baseline_methodologies} outlines the proposed multi-camera road occupancy detection framework for traffic monitoring. Section~\ref{experiments} evaluates our models, examines the impact of multi-camera setups and occupancy map resolution, and reports results from zero-shot and few-shot fine-tuning with real-world data. Finally, Section~\ref{conclusion} summarizes our findings and concludes this paper.