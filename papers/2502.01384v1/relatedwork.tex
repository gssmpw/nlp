\section{Related works}
\paragraph{Inference-time techniques.}
Inference-time techniques are simple yet effective as they require no fine-tuning or training when reward functions are available. Recent studies \cite{singhal2025general,ma2025inference} showed that they can achieve competitive performance by scaling computational resources. Although inference-time techniques offer distinct advantages, they typically result in longer inference times compared to fine-tuned models. The key considerations for these techniques include computational efficiency and differentiability of the reward \cite{uehara2025reward}.

\paragraph{Policy gradients algorithms.}
Policy gradient algorithms are a key class of reinforcement learning methods that optimize parameterized policies by directly maximizing expected returns. Modern implementations include Proximal Policy Optimization \cite{schulman2017proximal} or Group Relative Policy Optimization \cite{shao2024deepseekmath}. These algorithms are highly sensitive to policy design since the architecture impacts expressiveness, optimization stability, and exploration.

\paragraph{Fine-tuning diffusion models with Reinforcement Learning.}
In the case of continuous diffusion models, fine-tuning via policy gradients has been proposed \cite{fan2024reinforcement,li2024learning,black2023training,ren2024diffusion}. In a more recent study, \cite{marion2024implicit} implements \texttt{REINFORCE} algorithm~\cite{williams1992simple} for continuous diffusion models in a single-loop algorithm, avoiding nested optimization. However, extending these approaches to discrete diffusion models is more challenging. This work adapts these studies to the discrete case and extends them to general policy gradient algorithms.