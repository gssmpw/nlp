%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize]{cleveref}

%macros
\input{macros}
\input{packages}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods}

\begin{document}

\twocolumn[
\icmltitle{Fine-Tuning Discrete Diffusion Models with Policy Gradient Methods}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
%\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Oussama Zekri}{ens}
\icmlauthor{Nicolas Boull\'e}{imperial}
\end{icmlauthorlist}

\icmlaffiliation{ens}{Department of Mathematics, ENS Paris-Saclay, France.}
\icmlaffiliation{imperial}{Department of Mathematics, Imperial College London, UK}

\icmlcorrespondingauthor{Oussama Zekri}{oussama.zekri@ens-paris-saclay.fr}
\icmlcorrespondingauthor{Nicolas Boull\'e}{n.boulle@imperial.ac.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Discrete diffusion models have recently gained significant attention due to their ability to process complex discrete structures for language modeling. However, fine-tuning these models with policy gradient methods, as is commonly done in Reinforcement Learning from Human Feedback (RLHF), remains a challenging task. We propose an efficient, broadly applicable, and theoretically justified policy gradient algorithm, called Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models over non-differentiable rewards. Our numerical experiments across several discrete generative tasks demonstrate the scalability and efficiency of our method. Our code is available at \href{https://github.com/ozekri/SEPO}{https://github.com/ozekri/SEPO}.
\end{abstract}

\section{Introduction}\label{sec:intro}

Diffusion models have become efficient generative modeling tools in various tasks, including image and video generation~\cite{song2020score,ho2020denoising}. Although most of the applications of diffusion models depend on a continuous state space (such as images), recent works extended these models to discrete settings, enabling their use in language modeling and other discrete generative tasks~\cite{sun2022score, campbell2022continuous, austin2021structured, benton2022denoising}. Moreover, several studies showed that these models can be competitive with autoregressive models, such as GPT~\cite{brown2020language} or Llama~\cite{touvron2023llama}, while allowing for more flexible generation as opposed to next-token prediction~\cite{lou2023discrete, sahoo2024simple, shi2024simplified}. These discrete diffusion models hold great promise if they can be scaled up to natural language processing tasks.

However, fine-tuning discrete diffusion models remains a challenging task. Different approaches, such as classifier guidance \cite{ho2022classifier,nisonoff2024unlocking,gruver2024protein} or steering \cite{rector2024steering} often suffer from scalability issues or intractable training objectives. In this work, we focus on fine-tuning based on reinforcement learning (RL), where the aim is to maximize an objective by modifying the weights of a pre-trained model. The limitations of the existing methods mentioned earlier highlight the need for novel and efficient methodologies to address the unique challenges of discrete diffusion models.

More specifically, sampling from a categorical distribution, which is the type of distribution arising in the discrete setting, is a non-differentiable procedure that cannot be handled by gradient-based optimization algorithms. To bypass this issue, a recent work by \citealt{wang2024fine} proposes to fine-tune discrete diffusion models through direct backpropagation of rewards with the Gumbel-Softmax trick~\cite{jang2017categorical}. While this is an interesting approach, it cannot handle non-differentiable rewards. Moreover, when working at scale, the size of a reward model that one wants to differentiate with backpropagation may quickly become memory-intensive. 

Finally, methods that achieved state-of-the-art results in Reinforcement Learning from Human Feedback (RLHF) are policy gradient method, namely Proximal Policy Optimization (PPO) \cite{schulman2017proximal} or Group Relative Policy Optimization (GRPO) \cite{shao2024deepseekmath} due to their stability, efficiency, unbiased gradient estimates, and mechanisms like trust region constraints to handle noisy feedback. These methods deal with the zero-order derivative of the reward oracle, in contrast to \cite{wang2024fine} that uses the first-order derivative of the reward oracle.

In this work, we focus on developing policy gradient methods specifically tailored to discrete diffusion models to improve performance and robustness, and introduce a Score Entropy Policy Optimization (\SEPO), for fine-tuning discrete diffusion models. Unlike \cite{wang2024fine}, our approach does \emph{not} require the reward function $R$ to be differentiable, which expands the possibilities for fine-tuning beyond Direct Reward Backpropagation. Our method provides a unified framework for optimizing discrete diffusion models.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.9\textwidth]{figs/drawings/dessin.pdf}
    \caption{Illustration of the iterative fine-tuning process for discrete diffusion models using policy gradient methods. The initial model $\overline{Q}_{\pre}$ \textcolor{bluerow}{(conditionally)} generates responses, which are evaluated by a reward function. Based on this feedback, the model is updated iteratively using Score Entropy Policy Optimization (\SEPO), an efficient policy gradient algorithm for optimizing (non-differentiable) rewards. This process improves the model over multiple iterations, leading to the final fine-tuned model $\overline{Q}_{\theta^{\star}}$.}


    \label{fig:main_fig}
\end{figure*}

\subsection{Main contributions}

Our contributions are summarized as follows:
\begin{enumerate}[leftmargin=*, label={\arabic*)}]
    \item We provide an explicit characterization of policy gradient algorithms for discrete diffusion models in the concrete score matching framework. This allows the use of non-differentiable rewards in discrete fine-tuning tasks \emph{without} steering and guidance mechanisms.

    \item We propose an efficient and scalable algorithm based on policy gradient methods~\cite{schulman2017proximal,shao2024deepseekmath}, called Score Entropy Policy Optimization (\SEPO), for discrete diffusion. We also introduce a gradient flow alternative that improves sample quality at a higher complexity cost.
    
    \item We perform numerical experiments on DNA fine-tuning and natural language tasks to demonstrate the performance of our methods.
\end{enumerate}

\section{Background and preliminaries}\label{sec:background}

\subsection{Related works}

\paragraph{Inference-time techniques.}
Inference-time techniques are simple yet effective as they require no fine-tuning or training when reward functions are available. Recent studies \cite{singhal2025general,ma2025inference} showed that they can achieve competitive performance by scaling computational resources. Although inference-time techniques offer distinct advantages, they typically result in longer inference times compared to fine-tuned models. The key considerations for these techniques include computational efficiency and differentiability of the reward \cite{uehara2025reward}.

\paragraph{Policy gradients algorithms.}
Policy gradient algorithms are a key class of reinforcement learning methods that optimize parameterized policies by directly maximizing expected returns. Modern implementations include Proximal Policy Optimization \cite{schulman2017proximal} or Group Relative Policy Optimization \cite{shao2024deepseekmath}. These algorithms are highly sensitive to policy design since the architecture impacts expressiveness, optimization stability, and exploration.

\paragraph{Fine-tuning diffusion models with Reinforcement Learning.}
In the case of continuous diffusion models, fine-tuning via policy gradients has been proposed \cite{fan2024reinforcement,li2024learning,black2023training,ren2024diffusion}. In a more recent study, \cite{marion2024implicit} implements \texttt{REINFORCE} algorithm~\cite{williams1992simple} for continuous diffusion models in a single-loop algorithm, avoiding nested optimization. However, extending these approaches to discrete diffusion models is more challenging. This work adapts these studies to the discrete case and extends them to general policy gradient algorithms.

\subsection{Discrete Diffusion}

In discrete diffusion models, the dynamics of a single particle is described by a continuous-time Markov chain (CTMC), denoted as a stochastic process $(x_t)_{0 \leq t \leq \horizon}$ operating on a finite space $\statespace = \{\alpha_1,\hdots,\alpha_m\}^n$. Here, $(\alpha_i)_{1\leq i \leq m}$ represents the possible states that form a vocabulary of size $m$, and $n$ is the length of the sequences, which is a fixed number known as \textit{context window} or \textit{block size}. Typically, it describes sequences of tokens or image pixel values. While the size $\cardinality \coloneqq \lvert \statespace \rvert = m^n$ of $\statespace$ is exponential in $n$, deep neural networks like transformers \cite{vaswani2017attention} were shown to perform and generalize well on these incredibly large state spaces \cite{zekri2024large}.

\paragraph{Forward process.}
At any given time $t$, the distribution of a particle $x_t$ is given by $\bp_t$, which lies within the probability simplex $\simplex \subset \R^{\cardinality}$. The forward process is a noising process that maps the initial data distribution $\bp_0 \coloneqq \pdata$ to some final noisy distribution $\bp_\horizon \coloneqq \preff$, which is easy to sample. During the noising forward process, the particle's probability transitions between states are given by a rate matrix $\Qt \in \R^{d \times d}$, indexed by $\statespace$,
through the following equation:
\begin{equation}\label{eq:fokker_planck_forward}
\frac{\ud \bp_t}{\ud t} = \Qt\bp_t, \quad t\in[0,T].
\end{equation}
The time reversal of this equation is known as~\cite{kelly2011reversibility},
\begin{equation}\label{eq:fokker_planck_timerev}
\frac{\ud \bp_{\horizon-t}}{\ud t} = \Qback_{\horizon-t} \bp_{\horizon-t}, \quad t\in[0,T],
\end{equation}
where for $x, y \in \statespace$, \[ \Qback_{t}(x, y) =\begin{cases}
\frac{\bp_t(x)}{\bp_t(y)} Q_t(x, y), & x \neq y, \\
-\sum_{z \neq x} \Qback_{t}(z,y), & x=y.
\end{cases}\]

\paragraph{Concrete score matching.}
\citealt{lou2023discrete} recently showed that one can approximate \cref{eq:fokker_planck_timerev} via concrete score matching \cite{meng2022concrete}. This is done by learning the concrete score as $s_\theta(x,t)_y \approx \bp_t(x)/\bp_t(y)$ with a sequence-to-sequence neural network $s_\theta$ parametrized by $\theta \in \R^p$. We emphasize that this setup includes the simplified approaches detailed in \cite{sahoo2024simple,shi2024simplified}. The resulting process is described by the following equation:
\begin{equation}\label{eq:fokker_planck_timerev_approx}
\frac{\ud \bq^\theta_t}{\ud t} = \Qback_{\horizon-t}^\theta \bq^\theta_t, \quad t\in[0,T],
\end{equation}
where the denoising process $\bq^\theta_t\approx \bp_{\horizon-t}$ maps $\bq^\theta_0 \coloneqq \preff$ to $\bq^\theta_\horizon \coloneqq \target(\theta)$, and $\theta$ is learned to achieve $\target(\theta) \approx \pdata$. The matrix $\Qback_{t}^\theta$ is defined for $x, y \in \statespace$ as 
\[\Qback_{t}^\theta(x, y) = \begin{cases}
s_\theta(x,t)_y Q_t(x, y), & \text{if } x \neq y, \\
-\sum_{z \neq x} \Qback_{t}^\theta(z,y), & \text{if } x=y.
\end{cases}\]
Note that, in practice, the quantity $s_\theta(x,t)_y$ is available for all $y\in \statespace$ at Hamming distance~\cite{hamming1950error} one of $x$, i.e., the states $y$ that differ from $x$ by exactly one token. This represents only $\mathcal{O}(m n)$ ratios, instead of $\mathcal{O}(m^{2n})$ \cite{campbell2022continuous,lou2023discrete}.

\paragraph{Sampling strategies} 
Sampling discrete diffusion models involves selecting efficient strategies to simulate the backward equation \eqref{eq:fokker_planck_timerev_approx}, while balancing computational cost and sample quality. Among other strategies for CTMCs, sampling can be done via the \textit{tau-leaping} algorithm \cite{gillespie2001approximate}, which implements an Euler
step at each position $i$ simultaneously and independently:
\begin{equation}\label{eqn:tau_leap_euler}
    \bq_t(x_{t - \Delta t}^i |x_t^i) = \delta_{x_t^i}(x_{t - \Delta t}^i) + \Delta_t \Qback_{\horizon-t}^\theta(x_t^i, x_{t - \Delta t}^i)
\end{equation}
Discrete diffusion models can also be used to perform flexible \textit{conditional sampling} \cite{lou2023discrete}. Unlike \textit{unconditional sampling}, which samples $\bq_t(x_{t-\Delta t} | x_t)$, we incorporate auxiliary data $\mathbf{c}$ by modifying the probability to be sampled to $\bq_t(x_{t-\Delta t} | x_t, \mathbf{c})$.
Finally, the number of reverse diffusion steps, $\horizon$, directly impacts computational efficiency and sample fidelity, with larger $\horizon$ providing more accurate approximations of the target distribution at a higher computational cost.

\subsection{Fine-tuning with Reinforcement Learning}

After the pretraining phase, a discrete diffusion model with learned parameter $\pre$ aims to approximate $\pdata$, in the sense $\target(\pre) \approx \pdata$. Our goal is to fine-tune the target distribution $\target(\theta)$ to increase a reward function $\reward:\statespace\to\mathbb{R}$, without having access to $\pdata$.

\paragraph{Minimization problem.} We focus on optimization problems over implicitly parameterized distributions. For a given family of functions $(\cF_t)_{t\in[0,T]}: \simplex \to \R$, we aim to minimize the loss function defined as
\begin{equation}\label{eq:main_prob}
 \ell_t(\theta):= -\cF_t(\bq^\theta_t),\quad t\in[0,T],
\end{equation}
over $\theta\in\R^p$. Classical choices of $\cF_t$ include $\cF_t(\bq^\theta_t) = \esp_{x \sim \bq^\theta_t}[R_t(x)]$, where $R_t=0$ for $t<T$ and $R_T=R$, to maximize a reward function $R : \statespace \to \R$, or $\cF_t(\bq^\theta_t) = -\KL(\bq^\theta_t \| \bq^{\pre}_t)$ to minimize the KL divergence of $p_t$ from a distribution $\bq^{\pre}_t$. As detailed in \cite{uehara2024understanding}, a typical fine-tuning algorithm for diffusion models combines these two terms as follows,
\begin{equation} \label{eq:def_loss_t}
    \ell_t(\theta)=  -\esp_{x \sim \bq^\theta_t}[R_t(x)] +\alpha\KL(\bq^\theta_t \| \bq^{\pre}_t),
\end{equation}
where $\alpha>0$ is a weighting factor. Following standard choices in the fine-tuning diffusion models with reinforcement learning literature \cite{black2023training,fan2024reinforcement,clark2023directly,uehara2024understanding}, we assume that $R_t=0$ for $t<T$ and $R_T=R$ in the rest of this paper. Therefore, the first term on the right side of \cref{eq:def_loss_t} is nonzero and equal to $\ell^R \coloneqq -\esp_{x \sim \target(\theta)}[R(x)]$ when $t=T$.

\paragraph{Loss reward gradient.}
To apply first-order optimization methods, one needs to compute the gradient $\nabla_\theta \ell^R(\theta)$. Since $\statespace$ is a finite space of size $\cardinality$, we have
\begin{equation} \label{eq:loss_pi_theta}
\nabla_\theta \ell^R(\theta) = -\nabla_\theta \big( \cF_T(\target(\theta)) \big) = -\firstvar^\top \nabla_\theta \target(\theta),
\end{equation}
where $\firstvar \in \R^\cardinality$ is the vector of first variations $\cF_T(\target(\theta))$ (see \cref{app:firstvar}). Importantly, we note that $\firstvar(\bp)(x) = R(x)$ for $x \in \statespace$, which does not involve the differentiability of $R$ (with respect to some embedding $\emb{\statespace}$ of the state space). One can then design deterministic non-differentiable functions that act on $\statespace$ as rewards, similar to those arising in RLHF, or elsewhere. This may include designing desired protein properties \cite{rector2024steering}.

\section{Methods}
%\subsection{Gradients estimation}
\subsection{Policy gradients for concrete score}

The gradient of the target distribution $\nabla_\theta \target(\theta)$, which appears in \cref{eq:loss_pi_theta}, can be calculated based on to its relationship with the concrete score $s_\theta$ as $s_\theta(x,0) = \target(\theta)/\target_x(\theta)$ for $x\in\statespace$. The following theorem shows that one can first compute $\nabla_\theta \target(\theta)$, and then $\nabla_\theta \ell^R(\theta)$  through a discrete analogue of the \texttt{REINFORCE} algorithm~\cite{williams1992simple}.

\begin{boxthm}[Discrete \texttt{REINFORCE} trick]\label{prop:stochastic_estimate}
With the notations introduced in \cref{sec:background}, applying the discrete \texttt{REINFORCE} algorithm to the concrete score changes \cref{eq:loss_pi_theta} to:
\[
\nabla_\theta\ell^R(\theta)\! = \! \sum_{x\in\statespace}\target_x(\theta)R(x)\!\sum_{\substack{y \in \statespace \\ y \neq x}}{\target_y(\theta)\nabla_\theta\log s_\theta(x,0)_y}.
\]
\end{boxthm}

\paragraph{Monte-Carlo estimation of the outer sum.}
The summand in \cref{prop:stochastic_estimate} involves the unknown distributions $\target_x(\theta)$ and $\target_y(\theta)$. While the outer sum can be estimated via Monte Carlo sampling, the inner sum is weighted by $\target_y(\theta)$. As noted in \cite{lou2023discrete}, a single $x\in\statespace$ provides access to every component of the concrete score $s_\theta(x,0)_y$, for $y\neq x$, and then to $\target_y(\theta)$ since it this is the only missing quantity in the tau-leaping sampling scheme \cref{eqn:tau_leap_euler}. It is then possible to compute the gradient as
\[\nabla_\theta\ell^R(\theta) = \esp_{x\sim\target(\theta)}[R(x)g(x,\theta)],\]
where $g(x,\theta) \coloneqq \sum_{\substack{y \in \statespace \\ y \neq x}}{\target_y(\theta)\nabla_\theta\log s_\theta(x,0)_y}$.

\paragraph{Importance sampling.}
Although this defines an unbiased gradient, the \texttt{REINFORCE} algorithm is known to have high variance and to not restrict large policy updates. To address the latter limitation and estimate $g(x,\theta)$, we build upon the core ideas introduced by Trust Region Policy Optimization (TRPO)~\cite{schulman2015trust}. Instead of sampling from $\target(\theta)$ one can leverage importance sampling through an old policy $\target(\old)$, and constraint the $\KL$ divergence between the old and the current policy as follows:
\begin{equation}\label{eq:grad_is}
\nabla_\theta\ell^{R}(\theta) = \esp_{x\sim\target(\old)}\left[R(x)\frac{\target_x(\theta)}{\target_x(\old)}g(x,\theta)\right].
\end{equation}
Once adapted for concrete score, this formulation leads us to the following result.

\begin{boxthm}[Importance sampling gradient]\label{thm:ppo_expression}
With the notations introduced in \cref{sec:background}, applying TRPO to \cref{eq:loss_pi_theta} yields: 
\begin{equation}\label{eq:ppo_gradient}
\nabla_\theta\ell^{R}(\theta) = \esp_{x\sim\target(\old)}\left[R(x)h(x,\theta)\right],
\end{equation} where \[\displaystyle h(x,\theta)\!=\!\sum_{\substack{y \in \statespace \\ y \neq x}}\!\target_y(\theta)\!\frac{\target_y(\theta)}{\target_y(\old)}\!\frac{{s_{\old(x,0)}}_y}{s_{\theta}(x,0)_y}\!\nabla_\theta\log s_\theta(x,0)_y.\]
\end{boxthm}

The quantity $h(x,\theta)$ is expressed in this way in \cref{eq:grad_is} to emphasize how the loss will be computed in practice. While being the founding step of state-of-the-art policy gradient algorithms, TRPO requires solving a constrained optimization problem at each step. However, thanks to \cref{thm:ppo_expression}, we can now build powerful, stable, scalable, and easy-to-implement policy gradient algorithms.

\subsection{\SEPO~: Score Entropy Policy Optimization}\label{sec:sepo_main}

Our algorithm relies on the ideas introduced in \cite{schulman2017proximal,shao2024deepseekmath}, but can be adapted to any policy gradient algorithm built on \texttt{REINFORCE} or TRPO. Inspired from these algorithms, we clip the following ratio that appears in the inner sum of \cref{thm:ppo_expression}:
\[
r_{x,y} = \frac{\target_y(\theta)}{\target_y(\old)}\frac{{s_{\old(x,0)}}_y}{s_{\theta}(x,0)_y}.
\]
to the interval $[1-\epsilon,1+\epsilon]$ for some hyperparameter $\epsilon>0$.
Another advantage of discrete diffusion models is their great generation flexibility. It is then be possible to apply our algorithm conditionally (via a training dataset, typically in RHLF) or unconditionally for fine-tuning. Hence, in the conditional form, \cref{eq:grad_is} becomes 
\[\esp_{z\sim\cD}\esp_{x\sim\target_{x|z}(\old)}\left[R(x)\frac{\target_x(\theta)}{\target_x(\old)}g(x,\theta)\right].\]
Instead of using directly the reward $R(x)$, we compute an advantage $A(x)$ to reduce the variance of the Monte-Carlo estimations. This quantifies how much better an action is compared to the expected return at a given state. A common approach in PPO \cite{schulman2017proximal} is to learn a value network to approximate the reward, and then define the advantage as $A(x) = R(x) - V(x)$. For GRPO \cite{shao2024deepseekmath}, the advantage is the standardized reward over each group. Specifically, for a group of outputs $x = \{x_1,\hdots,x_G\}$ the advantages are defined as 
\[A(x_i) = \frac{R(x_i) - \text{mean}(R(x))}{\text{std}(R(x))}, \quad i\in\{1,\hdots,G\}.\]

\begin{rmk} The loss function takes the form
\begin{equation}\label{eq:loss_principal}
    \ell^{A}(\theta) = \esp_{x\sim\target(\old)}[\sum_{\substack{y \in \statespace \\ y \neq x}}w_{x,y}\log s_{\theta}(x,0)_y],\end{equation}
where $w_{x,y} \!=\! \target_y(\theta)r_{x,y}$ is a coefficient and the log concrete score $\log s_{\theta}(x,0)_y$ is the only term with an attached gradient. PPO, GRPO, and other methods can be constructed by modifying the coefficient $w_{x,y}$. In \cref{app:unified_paradigm}, we present a unified framework encompassing methods that can be derived from \SEPO.
\end{rmk}

Optionally for $t\in [0,T]$, a $\KL(\bq^\theta_t \| \bq^{\pre}_t)$ term can also be added to the loss, as in \cref{eq:def_loss_t}. Although this is not absolutely necessary, as clipping already implicitly regularizes with a $\KL(\target(\theta) \| \target(\old))$ term \cite{schulman2017proximal,fan2024reinforcement}, the derivation is given in \cref{app:KL_grad}, for completeness. This leads to the Score Entropy Policy Optimization (\SEPO) algorithm described in \cref{alg:SEPO}.

\begin{algorithm}[htbp]
   \caption{\SEPO}
   \label{alg:SEPO}
\begin{algorithmic}[1]
     \STATE {\bf Require}: CTMC $\Qback^\theta$, iteration $S$, policy optimization iteration $K$
     \STATE Set $\theta_0$ and $\old$ to $\pre$
     \FOR{$s \in [1,\cdots,S]$} \label{line:data1}
    \STATE Sample from $\target(\old)$ with $\Qback^{\old}$
    \STATE Compute the reward and the advantage
    \STATE Optimize $\theta_s$ with $\ell^{A}$ for $K$ epochs \label{line:opti2}
    \STATE Set $\old$ to $\theta_s$
     \ENDFOR 
      \STATE {\bf Output}: $\theta_{S+1}$
\end{algorithmic}
\end{algorithm}

\SEPO~iteratively samples from the target distribution via a CTMC (Line $4$) and optimizes $\theta_s$ using an optimization objective (Line $6$), refining the policy with policy gradients. Specifically :
\begin{itemize}
    \item \textbf{Line 4}: 
    This step generates samples from the target distribution $\target(\old)$ using the CTMC $\Qback^{\old}$. This can be done in $O(1)$ time complexity by leveraging the queuing trick introduced in \citep[Alg.~3]{marion2024implicit}, at a higher memory cost.
    \item \textbf{Line 6}: 
    This step updates the parameters $\theta_s$ using a policy optimization algorithm based on the objective $\ell^{A}$ (see \cref{eq:loss_principal}). This means performing $K$ iterations of gradient ascent (or descent) on the policy loss function to improve the policy $\target(\theta_s)$ using the previously collected samples and computed advantages.
\end{itemize}

\subsection{Sampling through gradient flow}

\paragraph{Bilevel problem.} We use sampling to reach the limiting process of the backward distribution $\target(\theta)$. This procedure can be interpreted as optimizing a functional $\cG : \simplex \times \R^p \to \R$ over the probability simplex $\simplex\subset\R^d$ as
\[
\target(\theta)=\underset{\bp \in \Delta_{\cardinality}}{\argmin}~\cG(\bp,\theta).
\]
When $\target(\theta)$ is the limiting distribution of an infinite time process (e.g.,~Langevin diffusion in the continuous case, \citealt{langevin1908theorie,pavliotis2014stochastic}), one can recast \cref{eq:main_prob} as a bilevel optimization problem. This has been proposed by \citealt{marion2024implicit} in the continuous diffusion case and allows to efficiently alternate between optimizing one-step of the inner problem and one step of the outer problem.

\paragraph{Gradient flow interpretation.}
In our case, $\target(\theta)$ is reached with \emph{finite-time horizon}, in $\horizon$ steps of sampling. However, it is possible to reach $\target(\theta)$ in \emph{infinite-time horizon} by sampling from a specific time-homogeneous CTMC. The choice of the functional $\cG(\bp,\theta) = \KL(\bp||\target(\theta))$ leads to a gradient flow interpretation of sampling via a specific CTMC.

\begin{boxlem}[Gradient flow]\label{lem:wass_flow}
    Sampling from the following ordinary differential equation $$\frac{\ud \bp_t}{\ud t} = \Qwass_0 \bp_t, \quad \text{where }\Qwass_0 \coloneqq Q_0 + \Qback_{0},$$ implements a gradient flow for $\KL(~\cdot~||\pdata)$ in $\simplex$, with respect to a Wassertein-like metric.
\end{boxlem}

\paragraph{Corrector steps.}

Of course, $s_\theta$ is not perfectly learned in practice, and we just have access to the rate matrix $Q_0^{c,\theta} \coloneqq Q_0 + \Qback_{0}^\theta$. But this gives us insight into the choice of our sampling strategy, especially with predictor-corrector techniques for discrete diffusion, introduced in \cite{campbell2022continuous} and developed in \cite{zhao2024informed}. We will then sample from the time-homogeneous CTMC of rate $Q_0^{c,\theta}$ to reach $\target(\theta)$ with infinite-time horizon. Note that this does not require computing an integral compared to the time-inhomogeneous case. We are then optimizing a functional in Wassertein space through sampling~\cite{marion2024implicit,bonet2024mirror}.

Sampling from $\Qwass_0$ affects Line 4 of \cref{alg:SEPO}. In practice, the sample quality can be improved by adding corrector steps with $\Qwass_t = Q_t + \Qback_{t}^\theta$, as proposed in \cite{campbell2022continuous}. Once the process has run for $\horizon$ steps, multiple sampling iterations from $\Qwass_0$ can be performed.

\paragraph{Linear system characterization.}
In this case, $\nabla_\theta \target(\theta)$ in \cref{eq:loss_pi_theta} will be obtained by solving a linear system, using the implicit function theorem (see \cref{app:proof_linear_system}) on $\nabla_1 \cG$, through a corrected version denoted $\nabla^\eta_\theta \target(\theta)$. While both the evaluation of the derivatives and the inversion of this linear system can be done automatically \cite{blondel2022efficient}, it is costly given the dimensionality $\cardinality$. Instead, we provide the exact linear system as well as a closed form of the inverse in \cref{prop:grad_pi}.

\begin{boxprop}\label{prop:grad_pi}
    For each $\eta >0$, $\nabla^\eta_\theta \target(\theta)$ is the solution to a linear system of the form
    \[A_\eta \unknown = B_\eta \in \R^{\cardinality\times p},\] 
    where $A_\eta$ is a rank-$1$ update to the $d\times d$ identity matrix, whose inverse can be explicitly computed using the Sherman--Morrison formula.
\end{boxprop}

Note that this affects Line 6 of \cref{alg:SEPO}, where $\nabla_\theta \target(\theta)$ in \cref{eq:loss_pi_theta} is replaced by $\nabla^\eta_\theta \target(\theta)$.

\subsection{Convergence bounds}

From a high-level point of view, \cref{alg:SEPO} alternates between sampling and optimization steps. We can then view \cref{alg:SEPO} as the following coupled equations:
\begin{equation} \label{eq_ev_theta}
    \begin{aligned}
\ud\bq_s &= Q_0^{c,\theta_s} \bq_s{\ud s}, \\
\ud {\theta_s} &= - \stepsize \Gamma (\bq_s, {\theta_s}){\ud s},
\end{aligned}
\end{equation}
for $0\leq s\leq S$. The gradient used on line $6$ of \cref{alg:SEPO} depends both on $\bq_s$ and $\theta_s$, and we refer to it as $\Gamma$ (so that $ \nabla_\theta \ell^A(\theta_s)=\Gamma (\target(\theta_s), {\theta_s})$). To simplify the analysis, the evolution of both $\bq_s$ and $\theta_s$ is done in continuous time flow, for some $s\in [0,S]$, with $S >0$. Let $\|\cdot\|$ denote the Euclidean norm on $\R^p$. We reintroduce assumptions on $\target(\theta)$ and $\Gamma$ made in \cite{marion2024implicit}.
\begin{asm}\label{ass:gradient_bounded}
There exists $C\geq 0$ such that for all $x \in \statespace$ and $\theta\in \R^p$, $\|\nabla_\theta \target_x( \theta)\| \leq C$.
There exists $\varepsilon > 0$ such that for all $x \in \statespace$ and $\theta\in \R^p$, $\target_x( \theta) > \varepsilon$.
\end{asm}
This assumption states that the gradient of the target distribution is bounded. The second part is similar to the ambiguity of the language often considered when studying models acting on spaces like $\statespace$ \cite{zekri2024large,hu2024unveiling,xie2021explanation}.
\begin{asm}\label{ass:Gamma_Lipschitz}
There exists $C\geq 0$ such that for all $p, q \in \simplex$, $\theta \in \R^p$, $\|\Gamma(p, \theta) - \Gamma(q, \theta)\| \leq C_\Gamma \sqrt{\KL(p || q)}$.
\end{asm}
This assumption essentially states that the gradient $\Gamma$ is Lipschitz continuous with respect to the $\KL$ divergence on $\simplex$.

With all these elements in place, we establish the convergence of the average objective gradients.

\begin{boxthm}[Convergence of \cref{alg:SEPO}]\label{thm:stochastic_bound}
Let $S>0$ and $\theta_s$ be the solution to \cref{eq_ev_theta} with $\beta_s = \min(1, 1/\sqrt{s})$, for $s\in[0,S]$. Under Assumptions \
\ref{ass:gradient_bounded} and \ref{ass:Gamma_Lipschitz}, we have
$$\frac{1}{S} \int_0^S \esp[\|\nabla \ell^A(\theta_s)\|^2] \ud s = \mathcal{O}\left(\frac{1}{\sqrt{S}}\right),$$
as $S\to\infty$.
\end{boxthm}

\section{Experiments}
\subsection{DNA sequence modeling}\label{sec:dna_modeling}

In this first experiment, we employ the pretrained model of \cite{wang2024fine}, a masked discrete diffusion model \cite{sahoo2024simple} pretrained on $\sim700k$ DNA sequences of the Gosai dataset \cite{gosai2023machine}.

\begin{figure}[htbp]
\centering
\includegraphics[width=1\linewidth]{figs/plots/integrale_f_s.pdf}
\caption{Illustration of the theoretical bound of \cref{thm:stochastic_bound} using a CG-enhancing reward. The purple curve is obtained by running GRPO with gradient flow.}
\label{fig:integral_thm}
\end{figure}

We illustrate the upper bound of \cref{thm:stochastic_bound} in \cref{fig:integral_thm}, where we used a reward function designed to enhance CG content in DNA sequences. Details about the \textit{non-differentiable} reward function can be found in \cref{app:add_exp}. We applied \SEPO~GRPO with a group size $G = 50$, with $10$ gradient flow corrector sampling steps at the end.

\subsection{Discrete diffusion language modeling}
\subsubsection{Training}

We implement \SEPO~in an Actor-Critic PPO style to fine-tuning SEDD Medium Absorb \cite{lou2023discrete}, a discrete diffusion model with $320$M non-embedding parameters, pretrained on OpenWebText \cite{Gokaslan2019OpenWeb}.

\paragraph{Reward modeling.}
Following \cite{minChatGPT}, we put the initial GPT-2 weights \cite{radford2019language} in a  \textit{GPT-2 Vanilla} model. We then augment the architecture with LoRA \cite{hu2021lora}, and use it to train a Supervised Fine-tuning (SFT) model and a Reward model. We use half of the HH-RLHF dataset \cite{bai2022training} to train the SFT model in an autoregressive fashion, and the other half to train the reward model, which has a logistic output $R(x)$. The whole reward modeling pipeline is illustrated in \cref{fig:reward_modelling}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figs/drawings/Reward_modelling.pdf}
\caption{GPT-$2$ Reward modeling pipeline.}
\label{fig:reward_modelling}
\end{figure}

\paragraph{SEDD Medium fine-tuning.}
We use the same first part of the HH-RLHF dataset that was used to train the GPT-$2$ SFT model. We skip any SFT stage for SEDD as our algorithm is only designed for the RL fine-tuning part. We acknowledge that an SFT stage would be beneficial, rather than the ``cold start" approach RL that we adopt.

\begin{table}[htbp]
    \centering
    \begin{tabular}{|p{7.5cm}|}
        \hline
            {\fontfamily{lmr}\selectfont \textcolor{bluerow}{Human: Is poker a hard game to learn? 
Assistant: It can be a challenge for some players, but if you're interested in playing, it's not hard to get started. 
Human: Is there an online game I could learn on? 
Assistant:} There is an online game called PokerStars. There are also several free trials. Human: Is there a skill required in poker? 
Assistant: There are skills required when you play in poker. You could talk about what and who you see in the game, and there are a lot of rules, moves and techniques, when you play in poker\dots} \\
            \hline
        \end{tabular}
        \caption{\textbf{Completion generation.} During fine-tuning, prompt tokens $p$ sampled from the HH-RLHF dataset \cite{bai2022training} are given in \textcolor{bluerow}{blue}. We leverage conditional sampling of discrete diffusion models to generate completions $c$ in black and form a whole sequence $x=c|p$. This is an example of completion obtained during the training of SEDD-\SEPO-$1024$.}
        \label{tab:cond_gen}
\end{table} 

To generate responses, we leverage conditional sampling, which allows us to guide SEDD's output by conditioning on specific prompts. A prompt $p$ and its completion $c$ form a sequence $x = c|p$. We then denote by $\target_x(\theta) = \target_{c|p}(\theta)$ the target probability of a prompt and its completion. This approach enables the model to generate targeted completions that are subsequently evaluated by a reward model. Unlike traditional autoregressive sampling, where the model generates one token at a time based only on the previous context, we let the model perform a complete generation given the preceding context. We then only select the next $128$ tokens following the prompt. This procedure is illustrated in \cref{tab:cond_gen}.

Following \cite{ouyang2022training}, we augment the reward by a KL regularization between $\target_x(\theta)$ and $\target_x(\pre)$, as
\[
\Tilde{R}(x) = {R}(x) - \beta\KL(\target_x(\theta) \| \target_x(\pre)).
\]
We compute the advantage as $A(x) = R(x) - V(x)$, where the value loss is a standard mean squared error loss between the value and the reward. Following good practice, we set $\epsilon = 0.2$ in \cref{eq:loss_principal}.

The reward and critic networks are represented by two different instances of the GPT-2 reward model that we obtained before (see \cref{fig:reward_modelling}). Two other instances of SEDD Medium will be used. The first one represents the actor network that will be fine-tuned, while the second one (fixed weights) is useful to compute the regularized rewards. The whole SEDD fine-tuning pipeline is illustrated in \cref{fig:sepo_pipeline}.

\begin{figure}[htbp]
\centering
\includegraphics[width=\linewidth]{figs/drawings/SEPO_pipeline.pdf}
\caption{\SEPO~fine-tuning pipeline for SEDD Medium.}
\label{fig:sepo_pipeline}
\end{figure}

We fine-tune two versions of SEDD Medium, with a different number of denoising steps $T$ to measure the impact on the quality of the fine-tuning. The first version, SEDD-\SEPO-$128$ generates completions over $128$ denoising steps. The second instance, SEDD-\SEPO-$1024$ generates completions over 1024 steps. Both versions are trained for $7k$ steps on the HH-RLHF dataset.

\subsubsection{Evaluation}

\begin{table*}[t]
  \centering
  \small
        \begin{tabular}{|c| p{16cm}|}
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} Prompt} & {\fontfamily{lmr}\selectfont \textcolor{bluerow}{I want you to act as a classical music composer. You will create an original musical piece for a chosen instrument or orchestra and bring out the individual character of that sound. My first suggestion request is ``I need help composing a piano composition with elements of both traditional and modern techniques."}} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} SEDD V.} & {\fontfamily{lmr}\selectfont``Hope that you are interested in this then contact me and here is a place you can look, below in my suggestion list" Human help: ````If you have any questions''." Instructor: ````Get the name on there and complete the first request list. Assistant: Below are all your human needs" Human "Ah this my help desk. I will be composing a composition with the sounds of both Classical ( Classical Classical Music) and both Modern (} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} \SEPO-$128$} & {\fontfamily{lmr}\selectfont I have had an excellent session with you in this vein. We are aware and take notice with your great work, please make sure to satisfy your own style and compliment our needs well. *Assistant and Manager take a track by track piece and listen through the finished piece for feedback* Assistant: I would like to consider composing a Classical musical using a chosen instrument or orchestra for a piano composition. My first suggested composer request is} \\
            \hline
            \rotatebox[origin=r]{90}{\centering \hspace{4pt}\SEPO-$1024$} & {\fontfamily{lmr}\selectfont You will have a role in writing, arranging, and editing the music using both traditional and amateur production techniques. If you can, you will have a role in ensuring that the music is presented as accurate and as coherent, and also as as personal and human as possible. If you can't, you will also serve as an engineer, schemer, or assistant.} \\
            \hline
        \end{tabular}
  \caption{Comparison of model responses for a prompt from the HH-RLHF dataset \cite{bai2022training}. We present outputs from the pretrained model and the fine-tuned \SEPO-$128$ and \SEPO-$1024$ variants. Additional qualitative samples can be found in Appendix~\ref{app:add_exp}.}
  \label{tab:qualitative_results}
\end{table*}

We use the $153$ prompts from the Awesome ChatGPT Prompts dataset \cite{awesome-chatgpt-prompts}. This dataset contains prompts that cover a wide range of topics, ideal to see what these $<1B$ parameter models are capable of, once fine-tuned.

\paragraph{Quantitative evaluation}

For each of our two models, SEDD-\SEPO-$128$ and SEDD-\SEPO-$1024$, we use a Judge LLM, GPT-$3.5$ Turbo \cite{brown2020language}, to determine which response is preferred between the response generated by the given model and the other. We also compare both of our models to the pretrained version of SEDD Medium. We also generate answers for different numbers of denoising steps $T \in \{128,512,1024\}$. The percentage of preferred responses for each evaluation is detailed in \cref{tab:sedd}.

\setlength{\tabcolsep}{0.5em}
\begin{table}[htbp] 
\centering
\scalebox{0.75}{
\begin{tabular}{lcccccc}
%\toprule
\toprule[\thick pt]%
\multicolumn{1}{c}{} & \multicolumn{3}{c}{\textcolor{blue-violet}{\textbf{SEDD-\SEPO-$128$}}} & \multicolumn{3}{c}{\textcolor{blue-violet}{\textbf{SEDD-\SEPO-$1024$}}}\\ 
\cmidrule(r{10pt}l{5pt}){2-4} 
\cmidrule(r{10pt}l{5pt}){5-7} 
 \multicolumn{1}{c}{Number of steps $T$} &  \multicolumn{1}{c}{$128$} & \multicolumn{1}{c}{$512$} & \multicolumn{1}{c}{$1024$} &  \multicolumn{1}{c}{$128$} & \multicolumn{1}{c}{$512$} & \multicolumn{1}{c}{$1024$} \\
%\midrule
\midrule[\thick pt]
% Données pour ETTh2
SEDD Vanilla & $71.2\%$ & $64.1\%$ & $67.9\%$ & $74.5\%$ & $\mathbf{75.8\%}$ & $73.2\%$ \\
\midrule[\thick pt]
% Données pour ETTm1
SEDD-\SEPO-$128$ & $\times$ &  $\times$ &  $\times$ & $63.1\%$ & $\mathbf{68.8\%}$ & $67.8\%$ \\
\midrule[\thick pt]
% Données pour ETTm2
SEDD-\SEPO-$1024$ & $36.9\%$ & $31.2\%$ & $32.2\%$ & $\times$ & $\times$ & $\times$ \\
\bottomrule[\thick pt]%
\end{tabular}
}
\caption{For each model SEDD-\SEPO-$128$ and SEDD-\SEPO-$1024$, we report the proportion of outputs deemed favorable by the Judge LLM when compared to those of other models, for different numbers of denoising steps $T \in \{128,512,1024\}$. \textbf{Best} results are highlighted in bold.}
\label{tab:sedd}
\end{table}

First, both SEDD-\SEPO~variants consistently outperform the pretrained SEDD Vanilla model across most prompts in the Awesome dataset. This highlights the effectiveness of reinforcement learning fine-tuning in enhancing response quality. 

Furthermore, SEDD-\SEPO-$1024$ outperforms SEDD-\SEPO-$128$ in general. This was expected, as the RLHF training was performed on completions of higher quality. According to \cite{lou2023discrete}, increasing the number of denoising steps improves generation quality, at a higher computational cost. However, while SEDD-\SEPO-$1024$ does not exhibit a statistically significant advantage over different values of $T$, SEDD-\SEPO-$128$ appears to perform better at the number of steps for which it was trained to generate responses. This suggests that a model performs best when evaluated with the number of denoising steps it was trained to generate, whereas larger models benefit from longer denoising schedules.

\paragraph{Qualitative evaluation}

We also provide some qualitative results. Some answers are displayed in \cref{tab:qualitative_results}, for each model and with $T=1024$ denoising steps. More answers and steps are displayed in \cref{app:add_exp}.

The qualitative results presented in \cref{tab:qualitative_results} highlight the diversity in the responses generated by three models (SEDD Vanilla, SEDD-\SEPO-$128$ and SEDD-\SEPO-$1024$) for a creative writing task. The prompt, which asks the model to act as a classical music composer and assist in creating a piano composition blending traditional and modern techniques, challenges the models to demonstrate creativity, coherence, and relevance. While the models vary in their coherence and alignment with the task, certain patterns emerge that reveal their strengths and weaknesses.

SEDD Vanilla's answer seems disjoint and lacks coherence. While it attempts to acknowledge the task of composing a classical-modern piano piece, the output contains redundant and nonsensical phrases (e.g., "Classical (Classical Classical Music) and both Modern"). This suggests that SEDD Vanilla struggles to maintain contextual relevance and generate meaningful content in such tasks.

Both SEDD-\SEPO-$128$ and SEDD-\SEPO-$1024$ answers display an improvement in structure and clarity compared to SEDD Vanilla. However, the lack of an SFT stage clearly appears: the output from SEDD-\SEPO-$1024$ seems more like a continuation of the prompt rather than a direct response. We explain this behavior because the model learned from the HH-RHLF dataset \cite{bai2022training} to create a conversation between an assistant and a human, rather than a direct output. This behavior was also observed during training, as in \cref{tab:cond_gen}.

\section{Conclusion}
  
We introduced \SEPO, a novel approach for fine-tuning discrete diffusion models using policy gradient methods. By extending previous works that applied these methods on continuous spaces, we developed a unified framework that adapts this methodology to the discrete case. Experimental results demonstrate the effectiveness of our approach in optimizing discrete diffusion models while addressing key challenges such as non-differentiability and combinatorial complexity. Future work includes further refining gradient estimation techniques and exploring applications in structured generative modeling.

\section*{Acknowledgements}
The authors would like to thank Pierre Marion, Anna Korba, and Omar Chehab for fruitful discussions. This work was supported by the Office of Naval Research (ONR), under grant N00014-23-1-2729. This work was done thanks to the ARPE program of ENS Paris-Saclay, which supported the visit of the first author to Imperial College London.

%\section*{Impact Statement}
%This paper presents work that aims to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which must be specifically highlighted here.

% In the unusual situation where you want a paper to appear in the
% references without citing it in the main text, use \nocite

\input{main.bbl}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
\textbf{\LARGE Appendix}
\section{Notations}
\label{app:notations}

\input{notations}

\section{Unified Paradigm of Policy Gradient Methods}\label{app:unified_paradigm}

As in \cite{shao2024deepseekmath}, we provide the expression of the coefficient $w_{x,y}$ for \SEPO~in its PPO and GRPO variants. This can be easily extended to the other training methods synthesized in \citealt[Section~5.2]{shao2024deepseekmath}.

\subsection{Proximal Policy Optimization (PPO)}

For PPO \cite{schulman2017proximal}, the coefficient $w_{x,y}$ takes the form $$w_{x,y}(\epsilon) = \min\{\mathrm{clip}(r_{x,y}, 1-\epsilon, 1+\epsilon)A(x);r_{x,y} A(x)\},\quad \epsilon >0.$$

A common approach is to learn a value network to approximate the reward, and then compute the advantage as $A(x) = R(x) - V(x)$.

\subsection{Group Relative Policy Optimization (GRPO)}

For GRPO \cite{shao2024deepseekmath}, we consider a group of outputs $x = \{x_1,\hdots,x_G\}$. The coefficient $w_{x,y}$ takes the form $$w_{x,y}(\epsilon) = \frac{1}{G}\sum_{i=1}^{G} \min\{\mathrm{clip}(r_{x_i,y}, 1-\epsilon, 1+\epsilon)A(x_i);r_{x_i,y} A(x_i)\},\quad \epsilon >0.$$

The advantages are the standardized reward over each group. Specifically, the advantages are defined as 
\[A(x_i) = \frac{R(x_i) - \text{mean}(R(x))}{\text{std}(R(x))}, \quad i\in\{1,\hdots,G\}.\]

The $\KL$ term of the original GRPO objective \cite{shao2024deepseekmath} is discussed in \cref{sec:sepo_main}.

\section{Proofs of the results}

This section details the proof of the results that appear in the main text.

\subsection{Proof of \cref{prop:stochastic_estimate}}

We begin by calculating $\nabla_\theta \target(\theta)$ appearing in \cref{eq:loss_pi_theta} component-wise. Let $x \in\statespace$ and recall that we can express $\target_x(\theta)$ as $\target_x(\theta) = \expo{-V_x(\theta)}/Z_\theta$ for some normalization constant $Z_\theta=\sum_{y\in\statespace}\expo{-V_y(\theta)}$. Therefore,
\begin{align*}
    \nabla_\theta \target_x(\theta) &= - \target_x(\theta)\nabla_\theta V_x(\theta) - \frac{\expo{-V_x(\theta)}\nabla_\theta Z_\theta}{Z_\theta^2}
    =- \target_x(\theta)\nabla_\theta V_x(\theta) - \target_x(\theta)\frac{\nabla_\theta Z_\theta}{Z_\theta}\\
    &=- \target_x(\theta)\nabla_\theta V_x(\theta) - \target_x(\theta)\left(\sum_{y\in\statespace} \frac{-\nabla_\theta V_y(\theta)\expo{-V_y(\theta)}}{Z_\theta}\right)=- \target_x(\theta)\nabla_\theta V_x(\theta) + \target_x(\theta)\left(\sum_{y\in\statespace} {\target_y(\theta)\nabla_\theta V_y(\theta)}\right).
\end{align*}
On the other hand, since for $y\neq x\in \statespace$, $[s_\theta(x,0)]_y = \target_y(\theta)/\target_x(\theta)$, we have $[s_\theta(x,0)]_y = \expo{-\left[V_y(\theta) - V_x(\theta)\right]}$. Then, $\nabla_\theta\log([s_\theta(x,0)]_y) = - (\nabla_\theta V_y(\theta) - \nabla_\theta V_x(\theta))$, leading to 
\begin{equation} \label{eq_relation_s_theta}
\nabla_\theta V_y(\theta) = \nabla_\theta V_x(\theta) -\nabla_\theta\log([s_\theta(x,0)]_y).   
\end{equation}
We can weight \cref{eq_relation_s_theta} by $\target_y(\theta)$ and sum over $y \neq x\in\statespace$ to obtain
\[\sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\nabla_\theta V_y(\theta)} = (1-\target_x(\theta))\nabla_\theta V_x(\theta) - \sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\nabla_\theta\log([s_\theta(x,0)]_y)}.\]
Moving the term $\target_x(\theta)\nabla_\theta V_x(\theta)$ to the left-hand side leads to
\[\sum_{y\in\statespace} {\target_y(\theta)\nabla_\theta V_y(\theta)} = \nabla_\theta V_x(\theta) - \sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\nabla_\theta\log([s_\theta(x,0)]_y)}.\]
Combining this with the expression for $\nabla_\theta \target_x(\theta)$ derived at the beginning of this section yields
\[\nabla_\theta \target_x(\theta) = -\target_x(\theta)\sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\nabla_\theta\log([s_\theta(x,0)]_y)}.\]
Finally, after replacing this term in \cref{eq:loss_pi_theta} we obtain the desired expression as
\[\nabla \ell^R(\theta) = -\sum_{x\in\statespace}R(x)\nabla_\theta \target_x(\theta) = \sum_{x\in\statespace}\target_x(\theta)R(x)\sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\nabla_\theta\log([s_\theta(x,0)]_y)}.\]

\subsection{Proof of \cref{thm:ppo_expression}}

Following \cref{prop:stochastic_estimate}, we can artificially introduce $\target_x(\old)$ in the expression of $\nabla \ell_R(\theta)$ as 
\begin{equation} \label{eq_l_proof_32}
    \nabla \ell_R(\theta) = \sum_{x\in\statespace}\target_x(\old)R(x)\sum_{\substack{y \in \statespace \\ y \neq x}} {\frac{\target_x(\theta)}{\target_x(\old)}\target_y(\theta)\nabla_\theta\log([s_\theta(x,0)]_y)}.
\end{equation}
Since for all $x\neq y \in \statespace$,
\[s_{\old}(x,0)_y = \frac{\target_y(\old)}{\target_x(\old)},\] 
we can rewrite \cref{eq_l_proof_32} as
\[\nabla \ell_R(\theta) = \sum_{x\in\statespace}\target_x(\old)R(x)\sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\frac{\target_y(\theta)}{\target_y(\old)}\frac{s_{\old}(x,0)_y}{s_\theta(x,0)_y}\nabla_\theta \log s_\theta(x,0)_y}.\]
Finally, we note that this is equivalent to the following equation
$$\nabla \ell_R(\theta) =\esp_{x\sim\target(\old)}\left[R(x)\sum_{\substack{y \in \statespace \\ y \neq x}} {\target_y(\theta)\frac{\target_y(\theta)}{\target_y(\old)}\frac{s_{\old}(x,0)_y}{s_\theta(x,0)_y}\nabla_\theta \log s_\theta(x,0)_y}\right].$$

\subsection{Proof of \cref{prop:grad_pi}}\label{app:proof_linear_system}

To prove \cref{prop:grad_pi}, we are going to prove the following explicit proposition regarding the linear system satisfied by $\nabla^\eta_\theta \target(\theta)$.

\begin{boxprop}
    Let $\eta >0$, and $D_r^\star$ denotes the block diagonal matrix defined in \cref{lem:sorted}. Then, $\nabla^\eta_\theta \target(\theta)$ is the solution of the linear system $$A_\eta \unknown = B_\eta \in \R^{\cardinality\times p},$$ where $A_\eta \coloneqq D_r^\star\left[I_\cardinality - \eta\diag(1/\target(\theta))\right] - I_\cardinality \in \R^{\cardinality\times \cardinality}$ and $B_\eta \coloneqq -\eta D_r^\star \nabla_\theta \target(\theta)/{\target(\theta)} \in \R^{\cardinality\times p}$.
\end{boxprop}

The proof contains three parts and occupies the rest of this section:
\begin{enumerate}
    \item recalling the implicit function theorem,
    \item computing the matrices that appear in the linear system,
    \item solving the linear system.
\end{enumerate}

\paragraph{Implicit function theorem.}
Let us first recall the implicit function theorem.

\begin{boxthm}[Implicit function theorem,~\citealt{krantz2002implicit}]
\label{thm:ift}
Let $U$ be an open subset of $\R^\cardinality\times \R^p$, and $f:U\to \R^\cardinality$ a continuously differentiable function. Let $(a,b)\in U$ such that $f(a,b) = 0$ and $\nabla_1 f(a,b)$ is invertible. Then, there exists an open set $W\subset \R^p$ containing $b$ and a function $g:W\to \R^d$ such that $g(b) =a$ and $\forall x \in W, f(g(x),x) = 0$. Moreover, $g$ is continuously differentiable and $$\forall x \in W, \quad \nabla_1 f(a,b) \partial g(x) = -\nabla_2 f(a,b)$$ where $\partial g$ denotes the Jacobian of $g$ on $W$.
\end{boxthm}

A first point to note is that $\nabla_1\cG(\bp,\theta) = \log(\bp/\target(\theta)) +\unit$. Even if $\target(\theta)=\underset{\bp \in \Delta_{\cardinality}}{\argmin}~\cG(\bp,\theta)$, we have $\nabla_1\cG(\target(\theta),\theta) = 1 \neq 0$, because we compute the derivative in $\R^\cardinality$ and not in the probability simplex $\simplex$. This means that we cannot directly apply \cref{thm:ift} to $\nabla_1\cG(\bp,\theta)$. To address this issue, we follow \cite{blondel2022efficient} and consider $\cG$ as a function of $\R^\cardinality \times \R^p$. Since we have a problem of the form $\target(\theta)=\underset{\bp \in \Delta_{\cardinality}}{\argmin}~\cG(\bp,\theta)$, we can define the fixed point operator 
\[T_\eta(\bp,\theta)=\proj_{\simplex}(\bp - \eta\nabla_1\cG(\bp,\theta)),\] 
for $\eta >0$. In fact, $T_\eta(\target(\theta),\theta) = \proj_{\simplex}(\target(\theta) - \eta\unit)$ where $\proj_{\simplex} = \sparsemax$ \cite{martins2016softmax,rakotomandimby2024learning}. From \citep[Prop.~2]{martins2016softmax}, we have 
\[\sparsemax(\bp-\eta\unit) = \sparsemax(\bp), \quad \bp \in \R^\cardinality.\] This leads to $T_\eta(\target(\theta),\theta) = \target(\theta)$, because $\sparsemax(\target(\theta)) = \target(\theta)$. We can therefore apply \cref{thm:ift} to the function $f_\eta(\bp,\theta) = T_\eta(\bp,\theta) - \bp$.

\paragraph{Computing the matrices.}
Let $\eta >0$, and define $h_\eta(\bp,\theta) = \bp - \eta\nabla_1 \cG(\bp,\theta)$. Then, we have $T_\eta(\bp,\theta) = \sparsemax(h_\eta(\bp,\theta))$ and $f_\eta(\bp,\theta) = T_\eta(\bp,\theta) - \bp$. We note that since $\target(\theta)\in\simplex$ and $T_\eta$ is a projection onto the probability simplex, $f_\eta(\target(\theta),\theta) = 0$. Following \citep[App.~D]{blondel2022efficient} (this computation can be done by using the chain rule), we have
\begin{equation} \label{eq_computing_der}
\begin{aligned}
\nabla_1 f_\eta(\bp,\theta) &= D(\bp,\theta)\left[I_\cardinality - \eta\nabla_{1,1} \cG(\bp,\theta)\right] - I_\cardinality \in \R^{\cardinality\times \cardinality},\\
\nabla_2 f_\eta(\bp,\theta) &= -\eta D(\bp,\theta) \nabla_{1,2} \cG(\bp,\theta) \in \R^{\cardinality\times p},
\end{aligned}
\end{equation}
where $D_r(\bp,\theta) \coloneqq \diag(r(h_\eta(\bp,\theta))) - r(h_\eta(\bp,\theta))r(h_\eta(\bp,\theta))^\top/\lVert r(h_\eta(\bp,\theta)) \rVert_1 \in \R^{\cardinality\times \cardinality}$ and $r(h_\eta(\bp,\theta)) \in \{0,1\}^\cardinality$ \cite{martins2016softmax}. Here, for all $\bz \in \R^\cardinality$, we define the vector $r(\bz)$ as follows:
\[\forall j \in [\cardinality],~ r(\bz)_j = 
    \begin{cases}
      1 & \text{if } \bz_j > \tau(\bz), \\
      0 & \text{otherwise},
    \end{cases}\] 
    where $\tau$ is the unique function satisfying $\sum_{i=1}^{\cardinality} [\bz_j - \tau(\bz)]_+ = 1 $ for all $\bz \in \R^\cardinality$. This definition is a bit tricky, but overall $r(h_\eta(\bp,\theta))$ contains $\nones$ times the number $1$ and $\cardinality-\nones$ times the number $0$, where $\nones \in [\cardinality]$. This means that 
    \[D_r(\bp,\theta) = \diag(r(h_\eta(\bp,\theta))) - r(h_\eta(\bp,\theta))r(h_\eta(\bp,\theta))^\top/\nones.\] 
    We then obtain the following lemma.

\begin{boxlem}\label{lem:sorted}
    Denote $\Tilde{r}(\bz)$ as the vector with the sorted coordinates of $r(\bz)$, i.e., the vector with coordinates $\Tilde{r}(\bz)_i = r(\bz)_{\sigma(i)}$, where $\sigma$ is the permutation such that $r(\bz)_{\sigma(1)} \geq \cdots \geq r(\bz)_{\sigma(\cardinality)}$. Then, $\nones = \max\{ k \in [\cardinality] \mid 1+kr(\bz)_{\sigma(k)} > \sum_{j=1}^{k}r(\bz)_{\sigma(j)} \}$ and, $$D_{\Tilde{r}}(\bp,\theta) = R - \frac{1}{\nones}T,$$ where $R \coloneqq \diag(\underbrace{1,\hdots,1}_{\nones \mathrm{times}},\underbrace{0,\hdots,0}_{\cardinality-\nones \mathrm{times}})$ and $T\coloneqq \begin{pNiceMatrix}[
                columns-width = auto,
                hvlines,
                cell-space-limits = 4pt
            ]
            J & 0            \\
            0              & 0 \\
        \end{pNiceMatrix}$. Here, $J$ denotes the $\nones\times\nones$ matrix whose entries are all equal to $1$.
\end{boxlem}

We can now replace the derivatives in \cref{eq_computing_der} by their expression as $\nabla_{1,1}\cG(\bp,\theta) = \diag(1/\bp)$ and $\nabla_{1,2}\cG(\bp,\theta) = -\nabla_\theta \target(\theta)/{\target(\theta)}$. Following  \cref{prop:stochastic_estimate}, we have
\[
\nabla_1 f_\eta(\bp,\theta) = D(\bp,\theta)\left[I_\cardinality - \eta\diag(1/\bp)\right] - I_\cardinality,\quad\text{and}\quad
\nabla_2 f_\eta(\bp,\theta) = \eta D(\bp,\theta) \left(\nabla_\theta \target(\theta)/{\target(\theta)}\right).
\]
Let us define 
\[D_r^\star \coloneqq D_r(\target(\theta),\theta) = \diag(r(h_\eta(\target(\theta),\theta))) - r(h_\eta(\target(\theta),\theta))r(h_\eta(\target(\theta),\theta))^\top/\nones.\] 
Since $h_\eta(\target(\theta),\theta)) =\target(\theta) - \eta\unit$, we find from \cref{lem:sorted} that if $\eta \leq \min_{i\in [\cardinality]}\target_i(\theta)$, then $k_h = \cardinality$. 

Let us continue the proof without additional assumption on $\eta$. We begin by verifying that the matrix 
\[-A_r^\star \coloneqq \nabla_1 f_\eta(\target(\theta),\theta) = D_r^\star\left[I_\cardinality - \eta\diag(1/\target(\theta))\right] - I_\cardinality\] 
is invertible. Without loss of generality, instead of reordering the elements of the canonical basis with $\sigma$ (\cref{lem:sorted}), we can check the invertibility directly on $A_{\Tilde{r}}^\star$ as follows.
\begin{align*}
-A_{\Tilde{r}}^\star &= \left(R - \frac{1}{\nones}T\right)\left[I_\cardinality - \eta\diag(1/\target(\theta))\right] - I_\cardinality = R - \eta R\diag(1/\target(\theta)) - \frac{1}{\nones}T + - \frac{\eta}{\nones}T\diag(1/\target(\theta)) - I_\cardinality\\
&= \diag(\underbrace{\frac{\eta}{\target_{\sigma(1)}(\theta)},\hdots,\frac{\eta}{\target_{\sigma(\nones)}(\theta)}}_{\nones \mathrm{times}},\underbrace{1,\hdots,1}_{\cardinality-\nones \mathrm{times}}) +\frac{1}{\nones}\kfirst{\unit_\cardinality}\left(\kfirst{\unit_\cardinality} - \eta\kfirst{w(\theta)}\right)^\top,
\end{align*}
where the second inequality is due to \cref{lem:sorted}, and 
\[\kfirst{\unit_\cardinality} \coloneqq (\underbrace{1,\hdots,1}_{\nones \mathrm{times}},\underbrace{0,\hdots,0}_{\cardinality-\nones \mathrm{times}})^\top,\quad  \kfirst{w(\theta)} \coloneqq (\underbrace{\frac{1}{\target_{\sigma(1)}(\theta)},\hdots,\frac{1}{\target_{\sigma(\cardinality)}(\theta)}}_{\nones \mathrm{times}},\underbrace{0,\hdots,0}_{\cardinality-\nones \mathrm{times}})^\top.\] 
We then find that $-A_{\Tilde{r}}^\star$ is a rank-one update of a diagonal matrix, which can be inverted explicitly using the Sherman--Morrison formula~\cite{bartlett1951inverse}.

\begin{boxlem}[Sherman--Morrison formula,~\citealt{bartlett1951inverse}]\label{lem:SM_formula}
    Let $M \in \R^{\cardinality\times \cardinality}$ be an invertible matrix and $u,v \in \R^\cardinality$. Then, $M +uv^\top$ is invertible if and only if $1 + v^\top \inv{M} u \neq 0$, and 
    $$\inv{(M +uv^\top)} = \inv{M} + \frac{\inv{M}uv^\top\inv{M}}{1 + v^\top \inv{M} u}.$$
\end{boxlem}

We replace the elements in \cref{lem:SM_formula} by our variables as $M = \diag(\eta/\target_{\sigma(1)}(\theta),\hdots,\eta/\target_{\sigma(\nones)}(\theta),1,\hdots,1)$ is invertible, $u=\frac{1}{\nones}\kfirst{\unit_\cardinality}$ and $v =\kfirst{\unit_\cardinality} - \eta\kfirst{w(\theta)}$. Then, \[
    1 + v^\top \inv{M} u = 1 + \sum_{i=1}^{\nones}\frac{1-\eta/\target_{\sigma(i)}(\theta)}{\nones\eta/\target_{\sigma(i)}(\theta)} 
    =\frac{1}{\nones\eta}\sum_{i=1}^{\nones}{\target_{\sigma(i)}(\theta)} > 0.
\]
The nonzero terms in the numerator are located in the upper-left square of size $\nones$, and for $1\leq i,j\leq\nones$,
\begin{align*}
[\inv{M}uv^\top\inv{M}]_{i,j} &= \frac{\target_{\sigma(i)}(\theta)}{\eta}\frac{1}{\nones}\left(1-\frac{\eta}{\target_{\sigma(j)}(\theta)}\right)\frac{\target_{\sigma(j)}(\theta)}{\eta}=\frac{\target_{\sigma(i)}(\theta)\target_{\sigma(j)}(\theta)}{\nones\eta^2}\left(1-\frac{\eta}{\target_{\sigma(j)}(\theta)}\right)\\
&=\frac{\target_{\sigma(i)}(\theta)}{\nones\eta}\left(\frac{\target_{\sigma(j)}(\theta)}{\eta}-1\right).
\end{align*}
This shows that $-A_r^\star$ is invertible, and its inverse is given as
\[
-{A_r^\star}^{-1} = \diag\left(\underbrace{\frac{\target_{\sigma(1)}(\theta)}{\eta},\hdots,\frac{\target_{\sigma(\nones)}(\theta)}{\eta}}_{\nones \mathrm{times}},\underbrace{1,\hdots,1}_{\cardinality-\nones \mathrm{times}}\right)+ L,
\]
where $L$ is the rank-one matrix defined as \[L_{ij}=\begin{cases}\displaystyle\frac{\target_{\sigma(i)}(\theta)}{\sum_{l=1}^{\nones}{\target_{\sigma(l)}(\theta)}}\left(\frac{\target_{\sigma(j)}(\theta)}{\eta}-1\right) & \text{if } 1\leq i,j\leq \nones, \\
      0 & \text{otherwise}.
    \end{cases}
    \]
We can now apply \cref{thm:ift} to conclude that $\nabla_\theta^\eta \target(\theta)$ is the solution to the linear system 
$$A_\eta \unknown = B_\eta \in \R^{\cardinality\times p},$$ 
where $A_\eta \coloneqq D_r^\star\left[I_\cardinality - \eta\diag(1/\target(\theta))\right] - I_\cardinality \in \R^{\cardinality\times \cardinality}$ and $B_\eta \coloneqq -\eta D_r^\star \nabla_\theta \target(\theta)/{\target(\theta)} \in \R^{\cardinality\times p}$.

\paragraph{Solving the linear system}

The matrix product gives us that
$$\nabla_\theta^\eta \target(\theta) = S\nabla_\theta \target(\theta)$$ where $S \coloneqq z\unit^\top$ with $z_i=\begin{cases}\displaystyle\frac{\nones\target_{\sigma(i)}(\theta)}{\sum_{l=1}^{\nones}{\target_{\sigma(l)}(\theta)}} & \text{if } 1\leq i\leq \nones, \\
      0 & \text{otherwise}.\end{cases}$.
    
$\nabla_\theta^\eta\target(\theta)$ is then a weighted version of $\nabla_\theta\target(\theta)$. In fact, $\nabla_\theta^\eta \target(\theta) = z\cdot(\nabla_{\theta_1} \target(\theta),\hdots,\nabla_{\theta_p} \target(\theta))$ each gradient is then weighted by $z_i$.

Interestingly, as in \cite{blondel2022efficient}, $\eta$ does not appear directly in $S$. In our case, the dependence is implicit through $\nones$. We will then choose $\eta$ so that $\nones$ correspond to term we have in our Monte Carlo approximation of the policy gradient. By doing this, we will be able to totally compute $z$ since we have access to $\target_{\sigma(i)}, ~ \forall 1 \leq i \leq \nones$ through the concrete score $s_\theta(\sigma(i),0)$.

\subsection{Proof of \cref{lem:wass_flow}}

The proof is a direct consequence of \citep[Prop.~4]{campbell2022continuous} and \citep[Thm.~4.7]{maas2011gradient}.

\begin{boxprop}[\citealt{campbell2022continuous}]
    The corrector rate matrix $\Qwass_t \coloneqq Q_t +\Qback_{t}$ has $\bp_t$ as its stationary distribution.
\end{boxprop}

Then, the rate matrix $\Qwass_0$ has $\bp_0 = \pdata$ as a stationary distribution. 
To provide a gradient flow interpretation, \citealt{maas2011gradient} introduces a specific Wassertein-like metric $\cW : \simplex\times\simplex \to \R$. We refer to \citep[Sec.~1]{maas2011gradient} for a complete definition. 

\begin{rmk}
This metric is really close to the Wassertein metric, as it has a transport-cost interpretation. One notable difference is that the transport cost of a unit mass between two points depends on the mass already present at those points.
\end{rmk}

We state one of the main results of \cite{maas2011gradient}.
\begin{boxthm}\label{thm:gradient_flow_maas}[\citealt{maas2011gradient}]
    Let $\Delta$ be a rate matrix of stationary distribution $\nu$. Then, $\frac{\ud \bp_t}{\ud t} = \Delta \bp_t$ is a gradient flow trajectory for the functional $\cH(\bp) = \KL(\bp||\nu)$ with respect to $\cW$.
\end{boxthm}
Combining these two results means that sampling from the ODE $$\frac{\ud \bp_t}{\ud t} = \Qwass_0 \bp_t, \quad \text{where }\Qwass_0 \coloneqq Q_0 + \Qback_{0}$$ implements a gradient flow for $\KL(~\cdot~||\pdata)$ in $\simplex$, with respect to $\cW$.

\subsection{Proof of \cref{thm:stochastic_bound}}\label{app:proof_deterministic_bound}

The proof contains two parts and occupies the rest of this section:
\begin{enumerate}
    \item proving technical lemmas,
    \item main proof.
\end{enumerate}

\subsubsection{Technical lemmas}

We first recall the differential form of Grönwall's lemma.
\begin{boxlem}[Grönwall's lemma, differential form]\label{lem:gronwall}
Let $I$ be an interval of the real line closed on the left point. Let $u:I\to \R$ be a function differentiable in the interior $\mathring{I}$ of $I$ and $a,b:I\to\R$ be two continuous functions. Assume that $u$ satisfies the following inequality
$$
\frac{du(t)}{dt} \leq a(t) u(t) + b(t),\quad t\in\mathring{I}.
$$
Then,
$$
u(t) \leq u(t_0) \expo{\int_{t_0}^t a(s) ds} + \int_{t_0}^t b(s) \expo{\int_s^t a(r) dr} ds,\quad t\in I.
$$
\end{boxlem}

We then recall the expression of the functional derivative of the $\KL$ over $\simplex$ for our setup.

\begin{boxlem}[Functional derivatives]\label{lem:functional_derivatives}
For KL divergence, the chain rule for functional derivatives can be written as
$$\frac{\ud \KL(\pt\,||\,\target(\theta_s))}{\ud s} = \sum_{x\in\statespace} \frac{\delta \KL(\pt\,||\,\target(\theta_s))}{\delta \pt} \frac{\partial \pt}{\partial s} + \sum_{x\in\statespace} \frac{\delta \KL(\pt\,||\,\target(\theta_s))}{\delta \target(\theta_s)} \frac{\partial \target(\theta_s)}{\partial s}.$$
\end{boxlem}

We provide a useful bound on $\norm{\nabla_\theta \log \target({\tht})}$.

\begin{boxlem}[Bound on the derivative of the log distribution]\label{lem:bound_log}
Under \cref{ass:gradient_bounded}, we have that $$\norm{\nabla_\theta \log \target({\theta})} \leq C_{\log},\quad \text{for } \theta\in \R^p$$ where $C_{\log} >0$.
\end{boxlem}
\begin{proof}
$\nabla_\theta \log \target({\theta}) = \frac{\nabla_\theta\target({\theta})}{\target({\theta})}.$ Then, from \cref{ass:gradient_bounded}, for $\theta\in \R^p, \norm{\nabla_\theta \log \target({\theta})}\leq C_{\log} \coloneqq \frac{C}{\varepsilon}$.


\end{proof}

To end this subsection, we provide an expression of the Logarithmic Sobolev Inequality for our discrete setup.

\begin{boxlem}[Log-Sobolev inequality,~\citealt{diaconis1996logarithmic}]\label{ass:potential_log_sobolev}
\[
\mathrm{KL}(\bq_s \| \target) \leq \frac{1}{2\mu} \sum_{x,y\in\statespace} Q_0^{c,\theta_s}(x,y) \bq_s(y) \left(\log \frac{\bq_s(y)}{\target_y(\theta_s)} - \log \frac{\bq_s(x)}{\target_x(\theta_s)}\right)\quad \text{for } \mu>0.
\]
    
\end{boxlem}
\begin{proof}
Thanks to \cref{thm:gradient_flow_maas}, we can apply the Log-Sobolev Inequality (LSI) of \cite{diaconis1996logarithmic}:
\[
\mathrm{Ent}_{\target(\theta_s)}(f^2) \leq \frac{1}{2\mu} \sum_{x,y} Q_0^{c,\theta_s}(x,y) \target_y(\theta_s) (f(y) - f(x)) \log\left(\frac{f(y)}{f(x)}\right),
\]
to $ f(x) = \sqrt{\frac{\bq_s(y)}{\target_y(\theta_s)}}$.

\begin{rmk}
    We applied \citep[Lem.~2.7]{diaconis1996logarithmic} to the standard LSI form in the paper.
\end{rmk}

\end{proof}

\subsubsection{Main proof}

We recall the coupled equations stated in the main document:
\begin{align}
\displaystyle \frac{\ud \bq_s}{\ud s} &= Q_0^{c,\theta_s} \bq_s \label{eq:langevin-inner-proof}, \\
\frac{\ud {\theta_s}}{\ud s} &= - \stepsize \Gamma (\bq_s, {\theta_s}).\label{eq:langevin-outer-proof}
\end{align}
We provide below the main proof of \cref{thm:stochastic_bound}, inspired by \cite{marion2024implicit}.

\paragraph{Evolution of the loss.}
Since $\nabla\ell^A(\tht) = \Gamma (\target({\tht}), \tht)$, we have,
\begin{align*}
\frac{\mathrm{d}\ell^A}{\mathrm{d} s}(s) &= \innerprod{\nabla\ell^A(\tht)}{\frac{\mathrm{d}}{\mathrm{d} s} \tht} = -\stepsize\innerprod{\nabla\ell^A(\tht)}{\Gamma (\pt, \tht)} \\
&= -\stepsize\innerprod{\nabla\ell^A(\tht)}{\Gamma (\target({\tht}), \tht)} + \stepsize\innerprod{\nabla\ell^A(\tht)}{\Gamma (\target({\tht}), \tht)-\Gamma (\pt, \tht)}\\
&\leq -\stepsize\norm{\nabla\ell^A(\tht)}^2 + \stepsize \norm{\nabla\ell^A(\tht)}\norm{\Gamma (\target({\tht}), \tht)-\Gamma (\pt, \tht)}.
\end{align*} 
Then, by \cref{ass:Gamma_Lipschitz},
\begin{align*}\frac{\mathrm{d}\ell^A}{\mathrm{d} s}(s) &\leq-\stepsize\norm{\nabla\ell^A(\tht)}^2 + \stepsize C_{\Gamma}\norm{\nabla\ell^A(\tht)}\sqrt{\KL(\pt\,||\,\target({\tht}))} \\
&\leq-\frac{1}{2}\stepsize \norm{\nabla\ell^A(\tht)}^2 + \frac{1}{2}\stepsize C_{\Gamma}^2\KL(\pt\,||\,\target({\tht})),
\end{align*}
where we used the inequality $ab\leq\frac{1}{2}(a^2+b^2)$.

\paragraph{Evolution of the KL divergence of $\pt$ from $\target({\tht})$.}
Since $\KL(\pt\,||\,\target({\tht})) = \sum_{x\in\statespace} \log\left(\frac{\pt}{\target({\tht})}\right) \pt$, we have that \[\frac{\delta \KL(\pt\,||\,\target({\tht}))}{\delta \pt} = \log\left(\frac{\pt}{\target({\tht})}\right) +1,\quad \text{and}\quad \frac{\delta \KL(\pt\,||\,\target({\tht}))}{\delta \target({\tht})} = -\frac{\pt}{\target({\tht})}.\]
\cref{lem:functional_derivatives} gives us that \begin{equation}\label{eq:equa_diff_kl}
\frac{\ud\KL(\pt\,||\,\target({\tht}))}{\ud s}=\underbrace{\sum_{x\in\statespace} \log\left(\frac{\pt}{\target({\tht})}\right) \frac{\ud \bq_s}{\ud s}}_{ a} - \underbrace{\sum_{x\in\statespace} \frac{\pt}{\target({\tht})}\frac{\partial \target({\tht})}{\partial s}}_{ b},
\end{equation} 
since $\sum \frac{\ud \pt}{\ud s} = \frac{\ud}{\ud s}\sum \pt = \frac{\ud}{\ud s} 1 = 0$.

\paragraph{Analysis of $a$.} First, from \cref{eq:langevin-inner-proof}, $\frac{\ud \bq_s}{\ud s}(x) = (Q_0^{c,\theta_s} \bq_s)(x) = \sum_{y} Q_0^{c,\theta_s}(x,y) \bq_s(y)$. Then, since $Q_0^{c,\theta_s}$ sum to zero, we can write that $$a = \sum_{x,y} \log\left(\frac{\pt(x)}{\target({\tht})(x)}\right) Q_0^{c,\theta_s}(x,y) \bq_s(y) = \frac{1}{2}\sum_{x,y} \left( \log\left(\frac{\pt(x)}{\target({\tht})(x)}\right) - \log\left(\frac{\pt(y)}{\target({\tht})(y)}\right) \right) Q_0^{c,\theta_s}(x,y) \bq_s(y).$$ 
This procedure is analogous to an integration by parts in discrete space. We can then apply \cref{ass:potential_log_sobolev}, $$a \leq -2\mu\KL(\pt\,||\,\target({\tht})).$$

\paragraph{Analysis of $b$.} By the chain rule, $ \frac{\partial \target({\tht})}{\partial s} = \innerprod{\nabla_\theta \target({\tht})}{\frac{\mathrm{d}}{\mathrm{d} s} \tht}$. By plugging in \cref{eq:langevin-inner-proof}, we can rewrite $b$ as $$b=-\stepsize\innerprod{\Psi(\pt, \tht)}{\Gamma(\pt, \tht)},$$
where $\Psi(\pt, \tht) = \int \pt\nabla_\theta \log \target({\tht})$.
Then, 
\[\norm{\Psi(\pt, \tht) -\Psi(\target({\tht}), \tht)} = \norm{\int (\pt - \target({\tht}))\nabla_\theta\log \target({\tht})}\leq C_{\log} 2\norm{\pt - \target({\tht})}_{\TV} \leq C_{\log} \sqrt{2\KL(\pt\,||\,\target({\tht}))}.
\]
where we used \cref{lem:bound_log} for the first inequality and Pinsker's inequality for the second.
Note that $\Psi(\target({\tht}), \tht) = \sum \target({\tht})\nabla_\theta \log \target({\tht}) = \sum \nabla_\theta\target({\tht}) = \nabla_\theta \sum \target({\tht}) = \nabla_\theta 1 = 0.$ By Cauchy-Schwarz inequality, we have that $\lvert b \rvert \leq \stepsize K\sqrt{\KL(\pt\,||\,\target({\tht}))}$, where $K = C_{\log} C_{\Gamma}\sqrt{2}$.

\paragraph{Bounding the KL divergence of $\pt$ from $\target({\tht})$.} Combining the bounds on $a$ and $b$ with \cref{eq:equa_diff_kl} yields
$$\frac{\mathrm{d}}{\mathrm{d} s} \KL(\pt\,||\,\target({\tht})) \leq - 2 \mu \mathrm{KL}(p_t \, || \, \target({\tht})) + \stepsize K\sqrt{\KL(\pt\,||\,\target({\tht}))}.$$
Let $y(s) =\KL(\pt\,||\,\target({\tht}))$. We can rewrite
$$\frac{\mathrm{d}}{\mathrm{d} s} y(s) \leq - 2 \mu y(s) + \stepsize K\sqrt{y(s)}.$$
Then, by writing $\frac{\mathrm{d}}{\mathrm{d} s} y = 2\sqrt{y}\frac{\mathrm{d}}{\mathrm{d} s}\sqrt{y}$, we have $\frac{\mathrm{d}}{\mathrm{d} s} \sqrt{y}\leq - \mu \sqrt{y(s)} + \frac{\stepsize K}{2}$. We can then apply \cref{lem:gronwall} to obtain 
\begin{equation}\label{eq:gronwall}
\sqrt{y(s)} \leq \sqrt{y(0)}\expo{-\mu s} + \frac{K}{2}\expo{-\mu s}\int_{0}^{s}\beta_u \expo{\mu u}\mathrm{d}u.   
\end{equation}

\paragraph{Bounding the loss function.} We recall the bound on the loss:
$$\frac{\mathrm{d}\ell^A}{\mathrm{d} s}(s)\leq-\frac{1}{2}\stepsize \norm{\nabla\ell^A(\tht)}^2 + \frac{1}{2}\stepsize C_{\Gamma}^2y(s).$$
By integrating between $0$ and $S$, and exploiting the fact that we can bound $\stepsize$ by $\beta_S$ since $\stepsize$ is decreasing, we have
\begin{equation}\label{eq:upper-bound-grads}
\frac{1}{S}\int_{0}^{S} \norm{\nabla\ell^A(\tht)}^2 \mathrm{d}s \leq \frac{2}{S\beta_S}(\ell^A(0) - \inf\ell^A) + \frac{C_{\Gamma}^2}{S\beta_S}\int_{0}^{S}\stepsize y(s)\mathrm{d}s.\end{equation}
Recall that, by assumption of the Theorem, $\beta_s = \min(1, \frac{1}{\sqrt{s}})$. Thus $S \beta_S = \sqrt{S}$.

\paragraph{Analysis of the last integral}

From \cref{eq:gronwall}, we can bound $\int_{0}^{S}\stepsize y(s)\mathrm{d}s$. In fact,
\begin{align*}\int_{0}^{S}\stepsize y(s)\mathrm{d}s &\leq y(0)\int_{0}^{S}\stepsize \expo{-2\mu s}\mathrm{d}s + \sqrt{y(0)}K\int_{0}^{S}\stepsize \expo{-2\mu s}\left(\int_{0}^{s}\beta_u \expo{\mu u}\mathrm{d}u\right)\mathrm{d}s + \frac{K^2}{4}\int_{0}^{S}\stepsize \expo{-2\mu s}\left(\int_{0}^{s}\beta_u \expo{\mu u}\mathrm{d}u\right)^2\mathrm{d}s \\
&\leq \frac{y(0)\beta_0}{2\mu}\left(1-\expo{-2\mu S}\right) + \sqrt{y(0)}\frac{K\beta_0^2}{\mu^2}\left(\frac{1}{2}-\expo{-\mu S}+\frac{1}{2}\expo{-2\mu S}\right) + \frac{K^2}{4}\int_{0}^{S}\stepsize\left(\int_{0}^{s}\beta_u \expo{\mu (u-s)}\mathrm{d}u\right)^2\mathrm{d}s.
\end{align*}
The first two terms are converging when $S \to \infty$. Let us analyze the last term by defining $I(s) = \int_{0}^{s}\beta_u \expo{\mu (u-s)}\mathrm{d}u$. Let $S_0 \geq 2$ (depending only on $\mu$) such that $\frac{\ln(S_0)}{\mu} \leq \frac{S_0}{2}$. For $s \geq S_0$, let $\alpha(s) := s - \frac{\ln s}{\mu}$. We have, for $s \geq S_0$,
\begin{align*}
I(s) &= \int_0^{\alpha(s)} \beta_u \expo{\mu (u-s)}\ud u + \int_{\alpha(s)}^s \beta_u \expo{\mu (u-s)}\ud u \leq \beta_0 \expo{-\mu s} \int_0^{\alpha(s)} \expo{\mu u}\ud u + (s - \alpha(s)) \beta_{\alpha(s)} \\
&\leq \frac{\beta_0}{\mu}\expo{\mu(\alpha(s) - s)} + \frac{\beta_{\alpha(s)} \ln s}{\mu}\leq \frac{\beta_0}{\mu s} + \frac{\beta_{s/2} \ln s}{\mu},
\end{align*}
where in the last inequality we used that $\alpha(s) \geq s/2$ and $\beta_s$ is decreasing. It means that for $s\geq T_0$,
$$(I(s))^2 \leq \frac{\beta_0^2}{\mu^2 s^2} + 2\frac{\beta_0\beta_{s/2}}{\mu^2}\frac{\ln s}{s}+ \frac{\beta_{s/2}^2 (\ln s)^2}{\mu^2}.$$
For $s < S_0$, we can simply bound by $I(s)$ by $\beta_0 S_0$. We obtain
$$
\int_0^S \stepsize(I(s))^2  \ud s \leq \int_0^{S_0} \beta_s \beta_0^2 S_0^2 \ud s + \frac{1}{\mu^2}\left(\int_{S_0}^S \beta_0^2\frac{\beta_s}{s^2}\ud s + \int_{S_0}^S 2\beta_0 \frac{\beta_s \beta_{s/2}\ln s}{s}\ud s + \int_{S_0}^S\beta_s \beta_{s/2}^2 (\ln s)^2 \ud s \right).
$$
Since $\beta_s = \min(1, \frac{1}{\sqrt{s}})$, and $S_0 \geq 2$, all integrals are converging when $S \to \infty$. Plugging this into~\eqref{eq:upper-bound-grads}, we finally obtain the existence of a constant $c > 0$ such that
$$
\frac{1}{S} \int_0^S \|\nabla \ell^A(\tht)\|^2 \ud s \leq \frac{c}{S^{1/2}}.
$$

\section{Mathematical supplements}

\subsection{First Variation in the discrete setup on $\simplex$}\label{app:firstvar}
\paragraph{Overview}
In the discrete setup, when considering a functional $\cF(\bp)$ defined on the probability simplex $\simplex$, the first variation quantifies the sensitivity of $\cF$ to perturbations in the probability distribution $ \bp = \{p_1, \ldots, p_\cardinality\} $. For a small perturbation $ \bp \to \bp + \epsilon \eta $, the first variation is given by
\[
\delta \cF(\bp; \eta) = \lim_{\epsilon \to 0} \frac{\cF(\bp + \epsilon \eta) - \cF(\bp)}{\epsilon}.
\]
In practice, the first variation can often be expressed as a weighted sum over the components of $ \eta $, as 
\[
\delta \cF(\bp; \eta) = \sum_{i=1}^\cardinality \frac{\partial \cF}{\partial p_i} \eta_i,
\]
where $\displaystyle\frac{\partial \cF}{\partial p_i}$ denotes the partial derivative of $\cF$ with respect to $p_i$, which is the quantity of interest. The next two paragraphs details the derivation for the two functionals considered in this paper.

\paragraph{First Variation of $\cF(\bp) = \mathbb{E}_{x \sim \bp}[R(x)]$.}  
The functional can be written as:
\[
\cF(\bp) = \sum_{i=1}^\cardinality p_i R(x_i),
\]
where $\bp = \{p_1, \ldots, p_\cardinality\}$ is a probability vector, and $R(x_i)$ represents the value of $R$ at $x_i$. Consider a small perturbation $\bp \to \bp + \epsilon \eta$. Then, $\cF(\bp + \epsilon \eta) = \sum_{i=1}^\cardinality (p_i + \epsilon \eta_i) R(x_i)$. After expanding this first order in $\epsilon$, we obtain
\[
\cF(\bp + \epsilon \eta) = \sum_{i=1}^\cardinality p_i R(x_i) + \epsilon \sum_{i=1}^\cardinality \eta_i R(x_i) + o(\epsilon).
\]
Thus, the first variation is
\[
\delta \cF(\bp; \eta) = \lim_{\epsilon \to 0} \frac{\cF(\bp + \epsilon \eta) - \cF(\bp)}{\epsilon} = \sum_{i=1}^\cardinality \eta_i R(x_i),
\]
which leads to
\begin{align*}
\firstvar = \left[\frac{\partial \cF}{\partial p_i}\right]_{1\leq i\leq\cardinality} = \left[R(x_i)\right]_{1\leq i\leq\cardinality}.
\end{align*}

\paragraph{First Variation of $ \cF(\bp) = \KL(\bp \| \mathbf{q}) $.} Consider the small perturbation of the functional as $\cF(\bp + \epsilon \eta) = \sum_{i=1}^n (p_i + \epsilon \eta_i) \ln \frac{p_i + \epsilon \eta_i}{q_i},$ and its expansion to the first order in $\epsilon$:
\[
\cF(\bp + \epsilon \eta) = \sum_{i=1}^\cardinality \left( p_i + \epsilon \eta_i \right) \left( \ln \frac{p_i}{q_i} + \frac{\epsilon \eta_i}{p_i} \right) + o(\epsilon).
\]
Keeping only the terms linear in $\epsilon$, we find that
\[
\cF(\bp + \epsilon \eta) = \sum_{i=1}^\cardinality p_i \ln \frac{p_i}{q_i} + \epsilon \sum_{i=1}^\cardinality \eta_i \ln \frac{p_i}{q_i} + \epsilon \sum_{i=1}^\cardinality \eta_i + o(\epsilon).\]
Therefore,
\[
\delta \cF(\bp; \eta) = \lim_{\epsilon \to 0} \frac{\cF(\bp + \epsilon \eta) - \cF(\bp)}{\epsilon} = \sum_{i=1}^\cardinality \eta_i \left(\ln \frac{p_i}{q_i}+1\right).
\]
which leads us to
\begin{align*}
\firstvar = \left[\frac{\partial \cF}{\partial p_i}\right]_{1\leq i\leq\cardinality} = \left[\ln \frac{p_i}{q_i}+1\right]_{1\leq i\leq\cardinality}.
\end{align*}

\subsection{KL regularization term}\label{app:KL_grad}

For $t\in [0,\horizon]$, we take advantage of the computation of \citep[Lem.~1]{zhang2024convergence} to $\bq^\theta_t$ and $\bq^{\pre}_t$. In fact, $\KL(\bq^\theta_t \| \bq^{\pre}_t) = \esp_{x_t \sim \bq^\theta_t}\left[\frac{\ud \bq^\theta_t}{\ud \bq^{\pre}_t}\right]$. By applying Girsanov's theorem \cite{bj/1076364805} to the Radon-Nikodym derivative as in \cite{zhang2024convergence}, a generalized $I$-divergence term appears \cite{amari2012differential}. We obtain $$ \KL(\bq^\theta_t \| \bq^{\pre}_t) = \esp_{x_t \sim \bq^\theta_t}\left[\sum_{\substack{y \in \statespace \\ y \neq x}} \Qback_{t}^{\pre}(x_t, y) - \Qback_{t}^\theta(x_t, y) + \Qback_{t}^\theta(x_t, y)\log \frac{\Qback_{t}^\theta(x_t, y)}{\Qback_{t}^{\pre}(x_t, y)}\right].$$
Once integrated on the whole path from $t=0$ to $t=T$, we recover the result from \cite{wang2024fine}:
$$ \KL(\bq^\theta_{[0,\horizon]} \| \bq^{\pre}_{[0,\horizon]}) =\int_{0}^{\horizon} \esp_{x_{[0,\horizon]} \sim \bq^\theta_{[0,\horizon]}}\left[\sum_{\substack{y \in \statespace \\ y \neq x}} \Qback_{t}^{\pre}(x_t, y) - \Qback_{t}^\theta(x_t, y) + \Qback_{t}^\theta(x_t, y)\log \frac{\Qback_{t}^\theta(x_t, y)}{\Qback_{t}^{\pre}(x_t, y)}\right]\ud t.$$

\section{Additional experiments and details}\label{app:add_exp}

\subsection{DNA experimental details}

To enhance CG content while maintaining the naturalness of DNA sequences. Given a DNA sequence $s = (s_1, \dots, s_m)$ of length $m$, we define the reward as we define the reward function as:
\begin{equation}
    R(s) = \frac{1}{m} \sum_{i=1}^{m} \unit\{s_i \in \{\text{C}, \text{G}\}\}
    + wH(s),
\end{equation}
The first term maximizes the proportion of CG motifs. The second term is the entropy of the sequence, it measures the sequence diversity to prevent the model from generating sequences composed uniquely of C and G nucleotides.

We also augment the GRPO loss with a $\KL$ term as defined in \cref{app:KL_grad}. In addition to the entropy term in the reward, this allows the model to stay close to a reference natural DNA sequence, ensuring the preservation of typical biological motifs encountered in the \cite{gosai2023machine} dataset.

\subsection{Additional qualitative results for language modeling}

We provide additional answers for different prompts for each model, for $T=128$ denoising steps \cref{tab:qualitative_results_128}, and for $T=512$ denoising steps \cref{tab:qualitative_results_512}.

\begin{table*}[t]
  \centering
        \begin{tabular}{|c| p{16cm}|}
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} Prompt} & {\fontfamily{lmr}\selectfont \textcolor{bluerow}{I want you to act as an artist advisor providing advice on various art styles such tips on utilizing light \& shadow effects effectively in painting, shading techniques while sculpting etc., Also suggest music piece that could accompany artwork nicely depending upon its genre/style type along with appropriate reference images demonstrating your recommendations regarding same; all this in order help out aspiring artists explore new creative possibilities \& practice ideas which will further help them sharpen their skills accordingly! First request - ``I'm making surrealistic portrait paintings”}} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} SEDD V.} & {\fontfamily{lmr}\selectfont :Master:Director: I'm just looking around at my current painting and finding some interesting new ideas to use. So I think it would be fun to experiment with applying things to the painting in a new and interesting way. Pattern is something I enjoy in both my previous works on mine, and so I think it'd be something I'd be happy to build on because of my style of manner of painting. Master:} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} \SEPO-$128$} & {\fontfamily{lmr}\selectfont Human: Explain, OK so that you can act as a artist. Assistant: Human: Explain, OK! Second request - ``I want tom making amateur artist'' Assistant: Human: My name is Andy and I want to be a star in the, but I only paint so much as a professional. To get a reputation, you have to be good in how to paint and therefore have a} \\
            \hline
            \rotatebox[origin=r]{90}{\centering \hspace{4pt}\SEPO-$1024$} & {\fontfamily{lmr}\selectfont ????: Second request - ``I am making colorful abstract paintings'' Assistant: ????: Third request - ``I am making decorative style portraits.'' Assistant: ????: These portraits can be humorous, romantic, nostalgic, etc. along with different emotions, various types of, drawings, figurative art, artwork, various forms of sculpture, etc. Assistant: ????: Fourth request - ``I am making paintings.''} \\
            \hline
        \end{tabular}
  \caption{Comparison of model responses for a prompt from the HH-RLHF dataset, for $T=128$. We present outputs from the pretrained model and the fine-tuned \SEPO-$128$ and \SEPO-$1024$ variants.}
  \label{tab:qualitative_results_128}
\end{table*}

\begin{table*}[t]
  \centering
        \begin{tabular}{|c| p{16cm}|}
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} Prompt} & {\fontfamily{lmr}\selectfont \textcolor{bluerow}{I want you to act as an elocutionist. You will develop public speaking techniques, create challenging and engaging material for presentation, practice delivery of speeches with proper diction and intonation, work on body language and develop ways to capture the attention of your audience. My first suggestion request is ``I need help delivering a speech about sustainability in the workplace aimed at corporate executive directors".}} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} SEDD V.} & {\fontfamily{lmr}\selectfont My Assistant Professor: Main Faculty Humanities Review human: First, I want you to come up with a way to improve the quality of your speech. My second suggestion request is ``No one is qualified to deliver a speech about sustainable energy in the workplace aimed at corporate executives"} \\
            \hline
            \rotatebox[origin=r]{90}{\hspace{2pt} \SEPO-$128$} & {\fontfamily{lmr}\selectfont Dear Human: I would like to say that you will be an ideal person. My second request is ``so that you will deliver a speech about sustainability in the workplace aimed at Future Human". Assistant: Dear Human: I would like that you will be an ideal person.} \\
            \hline
            \rotatebox[origin=r]{90}{\centering \hspace{4pt}\SEPO-$1024$} & {\fontfamily{lmr}\selectfont Human: I want you to act as a crowding artist. You will work to bring the message to your audience, practice delivery of phrases with the grammar and intonation, work on body language and develop ways to capture the attention of your audience. My first suggested request is ``I will bring a reference to sustainable living in the workplace aimed at corporate executive directors"} \\
            \hline
        \end{tabular}
  \caption{Comparison of model responses for a prompt from the HH-RLHF dataset, for $T=512$. We present outputs from the pretrained model and the fine-tuned \SEPO-$128$ and \SEPO-$1024$ variants..}
  \label{tab:qualitative_results_512}
\end{table*}

\end{document}


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.