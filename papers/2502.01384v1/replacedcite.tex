\section{Related works}
\paragraph{Inference-time techniques.}
Inference-time techniques are simple yet effective as they require no fine-tuning or training when reward functions are available. Recent studies ____ showed that they can achieve competitive performance by scaling computational resources. Although inference-time techniques offer distinct advantages, they typically result in longer inference times compared to fine-tuned models. The key considerations for these techniques include computational efficiency and differentiability of the reward ____.

\paragraph{Policy gradients algorithms.}
Policy gradient algorithms are a key class of reinforcement learning methods that optimize parameterized policies by directly maximizing expected returns. Modern implementations include Proximal Policy Optimization ____ or Group Relative Policy Optimization ____. These algorithms are highly sensitive to policy design since the architecture impacts expressiveness, optimization stability, and exploration.

\paragraph{Fine-tuning diffusion models with Reinforcement Learning.}
In the case of continuous diffusion models, fine-tuning via policy gradients has been proposed ____. In a more recent study, ____ implements \texttt{REINFORCE} algorithm____ for continuous diffusion models in a single-loop algorithm, avoiding nested optimization. However, extending these approaches to discrete diffusion models is more challenging. This work adapts these studies to the discrete case and extends them to general policy gradient algorithms.