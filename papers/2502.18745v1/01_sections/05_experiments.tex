\subsection{Implementation Details}
\label{sec:implementation_details}
\noindent \textbf{Data preparation.}
All experiments are carried out on the Extended PaintNet dataset introduced in Sec.~\ref{sec:ext_paintnet_dataset}.
%
Input point clouds $\bO$ are derived by sampling 5120 points from the surface of the available meshes through Poisson Disk sampling~\cite{Cook_Stochastic_1986}.
%
We further down-sample ground-truth paths so that adjacent poses are approximately 5cm apart, avoiding to deal with needlessly dense waypoints.
Each waypoint is represented as a 6D vector with position described by $(x,y,z)$ coordinates, and orientation encoded as a 3D unit vector, rather than Euler angles. This simplification is permitted by our conic spray gun model, which is symmetric and invariant to rotations around the approach axis.
%
Consequently, ground-truth Euler angles are converted into 3D unit vectors (2-DoF), indicating where the gun nozzle is pointing.
%
When computing Euclidean distances in Eq.~(\ref{eq:pscd}) we rescale and adjust the relative importance of position and orientation components in the 6D vector by weighting the latter by $0.25$. 
%
Point clouds and path poses in the dataset are finally transformed so that each sample is centered around the origin, and down-scaled by a dataset-global factor.
%
We create training and test sets by dividing each category in the dataset according to an 80\%/20\% split. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=0.75\linewidth]{02_images/baselines/baselines_v7_noSegmentWise.pdf}
    \caption{Overview of the novel baselines implemented for comparison with \ours in the OCMG problem. All baselines share the same PointNet++ (PN++) encoder architecture for extracting object features.
    }
    \label{fig:baselines_overview}
    \vspace{-6pt}
\end{figure*}
%%%

\noindent \textbf{Architecture.} We employ an encoder module based on PointNet++~\cite{Qi_Pointnet++_2017} as the feature extractor mapping the input point cloud to a latent space of dimensionality 1024.
The encoder is initialized with pre-trained weights from an auxiliary shape classification task on ModelNet~\cite{wu_modelnet_2014}.
Segment and mask predictions are generated in parallel from the object features through separate dedicated 3-layer MLPs decoders with hidden sizes (1024,1024) and output sizes $6{\times}\lambda{\times} K$ and $K{\times} N$---respectively for segments and masks.
Confidence scores $\hat{c}^i$ for each path mask are learned through an additional linear output layer of size $N$, followed by a sigmoid activation.
In line with~\cite{tiboni2023paintnet}, we set $\lambda{=}4$ throughout all our experiments.

\noindent \textbf{Training details.}
We minimize our loss function $\mL$ using the Adam optimizer with learning rate $10^{-3}$ over $4800$ epochs.
Notably, we find it beneficial to activate the path mask loss $\mL_{mask}$ only after $3200$ epochs, so that masks are learned only when the predicted segments are sufficiently close to the ground truth.
We implement the AP2S curriculum described in Sec.~\ref{sec:segments_prediction} by scaling $w^b_p$ and $w^b_s$ by factors of $0.1$ and $10$, respectively, at epochs 1000 and 2000.
The learning rate is halved five times at intervals of 800 epochs throughout the training process.
Training takes approximately $6$ hours on a single NVIDIA RTX-4080 GPU for a training set of $800$ objects and a batch size of $64$.
Each forward pass on 5120 input points takes approximately 100ms on the same GPU, while postprocessing requires 100-500ms on a single CPU core, depending on the number and length of output paths.

\subsection{Baselines}
The OCMG problem as described in Sec.~\ref{sec:introduction} and formalized in Sec.~\ref{sec:method}, is introduced in this work for the first time. The overview of previous literature in Sec.~\ref{sec:related_work} highlighted that none of the existing methods are suitable for tackling OCMG tasks via Deep Learning. To provide a thorough benchmark comparison of \ours against alternative approaches, we devise a new set of baselines as ad-hoc adaptations of existing works designed for different problem settings.
%
A schematic overview of all baselines is depicted in Fig.~\ref{fig:baselines_overview}.
We provide a detailed description of each baseline below.

\noindent\textbf{Path-wise.}
This model outputs a set of complete paths at once, where each path is a $6 {\cdot} T$-dimensional vector.
It predicts a predefined maximum number of paths, and identifies which paths to retain at inference time through learned confidence scores.
The model also predicts an ``end-of-path'' probability for each individual path waypoint, thus the length of each path can vary.
%
This approach is inspired by the object detection literature, with paths treated analogously to bounding boxes. In particular, we adopt the same logic of one-shot set prediction as in DETR~\cite{carion2020detr}, but we design the network architecture with MLP layers as in \ours, instead of DETR's query-based modules.
%%
Overall, this implementation extends the \emph{Multi-Path Regression} baseline introduced in~\cite{tiboni2023paintnet}, and reflects the attempt to directly predict long-horizon paths as opposed to breaking down the problem into segment predictions.

\noindent \textbf{Autoregressive.}
%
This baseline allows comparing one-shot prediction methods---such as \ours and Path-wise---to an alternative autoregressive strategy for OCMG.
%
We design a method for predicting the next pose and a termination probability given a history of previously generated poses, separately for each path. 
It consists of a first model trained to predict a set of Start of Path (SoP) poses given the object features (first stage). This is followed by the autoregressive model conditioned on the predicted SoP, the object features, and the $10$ most recent predictions (second stage).
%%
The training procedure for the first stage is analogous to the Path-wise baseline, \ie, the SoP model learns confidence probabilities for filtering out SoPs in excess.
We then use teacher forcing~\cite{teacherforcing} to train the autoregressive model and inject noise into the input history to mitigate compounding errors.
Interestingly, we observe that predicting a sequence of $\lambda$ output poses---\ie, segments---at each autoregressive step yields smoother output paths.
Thus, we adopt this approach for this baseline.
Finally, we find that jointly training the SoP and autoregressive models at the same time
leads to training instabilities and does not improve predictive performance.

\noindent \textbf{Point-wise.}
Inspired by shape completion methods~\cite{Yuan_Pcn_2018}, the approach proposed in \cite{tiboni2023paintnet} directly predicts all output waypoints as a set of unordered 6D poses, employing the standard symmetric Chamfer Distance~\cite{Fan_Point_2017}.
We integrate this approach with the mask predictions pipeline of \ours in order to recognize how output waypoints are organized into separate paths.
Notably, this implementation can be seen as an edge case of \ours with fully point-wise loss terms $w^f_p{=}1,w^f_s{=}0,w^b_p{=}1,w^b_s{=}0$.
%
However, unlike \ours, this baseline is unable to concatenate output poses into long-horizon paths through postprocessing, rendering it practically inapplicable to a real-world scenario.
%%%%%%%%%%%%%%%%%%%%%%%%%



\subsection{Evaluation Metrics}
\label{sec:eval_metrics}
We introduce a collection of metrics for OCMG to assess the performance of different methods in predicting unstructured paths. In the following we use $h{=}1,\ldots,H$ as an index on the test object samples $\bO_h$. 

\noindent \textbf{Point-wise Chamfer Distance (PCD)~\cite{Fan_Point_2017}.} It compares the predicted and ground-truth paths as two clouds of 6D poses by computing $\frac{1}{H}\sum_{h=1}^H PCD(\bPhat_h, \bP_h)$, where $PCD(\bPhat, \bP) {=} d_{ACD}(\bPhat ,\bP) + d_{ACD}( \bP,\bPhat)$. This metric accounts for the accuracy of all end-effector positions and orientations, while disregarding the order among poses. Lower is better.

\noindent \textbf{Accuracy of Number of Paths (Acc-NoP).} It measures the fraction
%proportion 
of objects across the test set for which the predicted number of output paths matches the true number, \ie, $\frac{1}{H}\sum_{h=1}^H\mathbbm{1}_{\hat{n}(\bO_h)=n(\bO_h)}$. Higher is better.

\noindent \textbf{Mean Absolute Error of Number of Paths (MAE-NoP).} This metric measures the average deviation of the predicted number of paths from the true number. It is obtained by 
computing the mean absolute error $\frac{1}{H}\sum_{h=1}^H\vert \hat{n}(\bO_h) - n(\bO_h)\vert$ 
across objects in the test set\footnote{Both MAE-NoP and Acc-NoP are based on established metrics for ordinal regression problems in machine learning (\eg, see Sec.~4.1.3 in~\cite{7161338}).}. 
Lower is better.

\noindent \textbf{Paint Coverage (PC).}
Although not explicitly optimized at training time, we aim to assess the percentage of object surface covered by the predicted paths when executed in a spray painting simulator.
%
To do so, we follow the same approach as in~\cite{tiboni2023paintnet}.
First, the $10_{th}$ percentile of the ground truth paint thickness distribution across mesh faces is selected as the relative paint thickness threshold for measuring coverage. 
%
Next, we execute the predicted paths and evaluate the percentage of faces whose paint thickness is larger than such threshold.
We then average the percentages across all instances of the test set. 
Note that the relative thickness threshold makes the metric independent of the specific spray gun model parameters used during simulation (\eg, paint flux), thus renders it suitable for benchmarking purposes.
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/qualitatives/qualitatives_segments_v5.pdf}
    \caption{
        Main qualitative results: the raw network predictions of all baselines are shown for a representative test sample of each object Category. Points displayed with the same color belong to the same path. Point orientations are not visible.
    }
    \label{fig:qualitatives_segments_prediction}
    % \vspace{-6pt}
\end{figure*}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/intermediate_results/intermediate_results_v2.pdf}
    \caption{
    The network is trained in a coarse-to-fine manner via the Asymmetric Point-to-Segment curriculum:
    first, output waypoints are positioned across the surface; then, local structure is promoted.
    Segments associated to the same path mask are shown with the same color.
    }
    \label{fig:intermediate_results}
    % \vspace{-6pt}
\end{figure*}

\input{03_tables/quantitative_all_metrics}


\subsection{Results: Network Predictions}
\label{sec:results_network_predictons}
We evaluate the performance of models trained individually on each category, while keeping the same hyperparameters.
This allows us to study the robustness of our design choices to variations in data distributions.
The categories are considered in order of growing complexity of object shapes and path patterns: Cuboids, Windows, Shelves and Containers. 

\medskip
\noindent \textbf{Main qualitative results.}
We report the qualitative results of raw network predictions on a subset of test instances in Fig.~\ref{fig:qualitatives_segments_prediction}.
The results from the \emph{Path-wise} baseline indicate that one-shot methods inspired by object detection~\cite{redmon2016yolo,carion2020detr,cheng2021maskformer} provide a strong basis for addressing unstructured output paths.
Although promising, the major limitation of this approach stems from the explicit computation of the loss between high-dimensional paths: the curse of dimensionality yields inaccurate predictions over long-time horizons. This can be noted by observing the degenerate raster patterns for Cuboids and Containers. 

The \emph{Autoregressive} approach aims at counteracting the described  phenomenon, by reducing the dimensionality of the network output at the cost of additional forward passes.
However, this baseline exhibits significant compounding errors, often predicting the wrong number of straight passes when attempting to match the long raster paths of the Cuboids category.
These findings highlight the importance of coping with the long-horizon nature of output paths alongside the other challenges of OCMG tasks.

Both \emph{Point-wise} and \ours approach the task by focusing on local path patterns and generating sets of poses or path segments, respectively. 
This novel paradigm shows significantly stronger generalization capabilities across all object categories.
Still, the Point-wise baseline provides predictions that fail to capture detailed path structures---\eg, see the top face of the Cuboid, and the inner paths of the Shelf.
%
\ours successfully tackles this problem by introducing (1) segment-wise Chamfer Distance terms, and (2) the Asymmetric Point-to-Segment Curriculum (AP2S). Notice that both enable \ours's state-of-the-art performance: the paths obtained by \ours without AP2S exhibit errors similar to those of the Point-wise baseline. In other words, incorporating both point-wise and segment-wise terms in $\mathcal{L}_{p2s}$ yields better performance than minimizing either one alone (see Appendix~\ref{appendix:pointtosegment_ablation} for a thorough ablation of our loss function).

Finally, in Fig.~\ref{fig:intermediate_results} we present an illustration of \ours's network predictions as the training progresses. The picture highlights the effect of the asymmetric curriculum: the model initially learns to generate sparse individual poses close to the target object surface; then, local structure is gradually promoted and smooth segment predictions are obtained.

\medskip
\noindent \textbf{Main quantitative results.}
The quantitative results of our experimental analysis are presented in Tab.~\ref{tab:quantitative_all_metrics}, considering all the metrics defined in Sec.~\ref{sec:eval_metrics}.
%
It can be observed that the novel mask predictions paradigm is the most effective across all object categories.
The Path-wise and Autoregressive baselines attempt to learn a confidence score for each path, but underperform in terms of NoP metrics.
%
Instead, Point-wise and \ours predict non-mutually exclusive masks over their own predictions to distinguish among different paths, a strategy that leads to significantly better NoP metrics at inference time.
Notably, these findings align with those on the predicted number of instances from the Panoptic Segmentation literature, where MaskFormer~\cite{cheng2021maskformer} improves over DETR~\cite{carion2020detr}.

Finally, we analyze the results on the Containers category in greater detail. Here, the models are trained on only 70 data samples of heterogeneous object shapes, and tested on 18 unseen instances.
%
This is a challenging setting often encountered in real-world industrial scenarios where the training set size is limited, while strong generalization capabilities are required.
%
All methods exhibit low performance across the evaluated metrics, with high variance across independent training repetitions.
Although Point-wise achieves PCD scores comparable to our method, we observe that the former leads to globally sparse predictions that lack local consistency (cf. Fig.~\ref{fig:qualitatives_segments_prediction}).
We attribute this behavior to the properties of the Chamfer Distance, which is well-known for its insensitivity to mismatched local density~\cite{densityawarecd}.
%
Yet, \ours outperforms the other baselines in terms of PCD also on the Containers category.
%
Generalization in such data-scarce conditions can be further improved by leveraging pre-trained models or incorporating additional data across different categories, as discussed in Sec.~\ref{sec:results_generalization}.
%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Postprocessing.}
The postprocessing step described in Sec.~\ref{sec:postprocessing} aims at preparing the generated paths for execution on robotic systems.
Its practical effect on concatenating and smoothing the predicted segments can be observed in Fig.~\ref{fig:example_postprocess}.
We remark that the choice of the segments' length $\lambda$ can affect the postprocessing, potentially leading to unfeasible paths.
%

\begin{figure}[] % tb
    \centering
    \includegraphics[width=\linewidth]{02_images/example_postprocessing/example_postprocess_v3.pdf}
    \vspace{-28pt}
    \caption{Final paths when postprocessing is applied to segment predictions on Cuboids and Windows.}
    \vspace{-6pt}
    \label{fig:example_postprocess}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{02_images/closeup/closeup_v4.pdf}
    \caption{
    Close up \ours results after postprocessing: both pose locations and orientations (red arrows) are effectively learned.
    }
    \label{fig:closeup}
    \vspace{-6pt}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Results: Generalization capabilities}
\label{sec:results_generalization}
By formalizing OCMG as a supervised learning task we are able to address key challenges commonly encountered in real-world industrial scenarios, such as the need to adapt to new data as it becomes available.
To this end, data-driven approaches offer several advantages over ad-hoc heuristics or traditional optimization-based methods. 
For example, models can be efficiently re-trained without requiring process re-engineering, and it is possible to leverage pre-trained models to improve performance in cases of limited data or time constraints. In the following, we analyze how different training configurations influence the generalization capabilities of our method.

\noindent \textbf{Sensitivity to training set size.}
We investigate the performance trend of \ours when scaling up the amount of available training data. Specifically, we focus on the Cuboids category and report our findings for a model trained on 300, 1000, or 3000 samples.
The results in Fig.~\ref{fig:n_samples_scaling} show that training on 300 Cuboids yields high PCD (\ie, high error) and paths with visible irregularities. These issues are largely mitigated when considering 1000 training samples and further improve when increasing the training set size to 3000. 

\begin{figure}[tb]
\centering
\begin{subfigure}[b]{\linewidth}
   \centering
   \includegraphics[width=0.55\linewidth]{02_images/scaling/scaling_v2.pdf}
   \vspace{-4pt}
   \caption{}
   \label{fig:Ng1} 
   % \vspace{6pt}
\end{subfigure}
% \vspace{12pt}
\begin{subfigure}[b]{\linewidth}
    \centering
   \includegraphics[width=\linewidth]{02_images/scaling/stroke_qualitatives_v4_withpostprocess.pdf}
   % \vspace{-15pt}
   \caption{}
   \label{fig:Ng2}
\end{subfigure}

\caption{(a) Generalization performance for varying amounts of training data, on the Cuboids category (5 seeds, average training time is displayed as text). For each seed, a new model is trained on different number of samples and tested on the same, fixed test set. (b) A representative predicted path at test time is depicted for each of the three configurations, before (top) and after (bottom) postprocessing.}
\label{fig:n_samples_scaling}
% \vspace{-6pt}
\end{figure}

\begin{figure}[] % tb
    \centering
    \includegraphics[width=\linewidth]{02_images/jointtraining/joint_training_v3_withGT.pdf}
    \caption{Performance comparison on two test samples between a Containers-specific model vs.~a model trained on all categories combined (joint-category). Quantitative metrics refer to average over all 18 container test samples.}
    \label{fig:jointtraining}
    % \vspace{-6pt}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent \textbf{Joint-category training.}
We extend the aforementioned analysis by studying the effect of jointly training \ours on multiple object categories with the goal of answering the following question:~\emph{``Does augmenting the training set with data from a different distribution but same task yield higher predictive performance?''}
We illustrate the results in Fig.~\ref{fig:jointtraining}, where the basic model trained only on 70 Containers is compared to a model jointly trained  on Cuboids, Windows, Shelves, and Containers.
Notably, the latter achieves significantly lower PCD on unseen Containers. It is qualitatively evident that the Containers-specific model is unable to preserve segments structure for novel shapes, showing more scattered predictions over the object surface. 
We conclude that experiencing higher shape and path diversity during training improves the model's generalization ability.


\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{02_images/generalization/fewshot_v2.pdf}
    \caption{
    Few-shot: a model jointly pre-trained on Cuboids, Shelves, and Windows generalizes better when finetuned on a subset of Containers. Results on test set after (left) 600 and (right) 1200 training epochs; 10 repetitions.
    }
    % \vspace{-12pt}
    \label{fig:few_shot}
\end{figure}

\begin{figure}[]
    \centering
    \includegraphics[width=\linewidth]{02_images/generalization/convSpeed.pdf}
    \caption{
    Convergence speed: a model jointly pre-trained on Cuboids, Shelves, and Windows leads to faster convergence when finetuned on Containers. Training with (left) 50\% and (right) 100\% of available containers; 10 repetitions.
    }
    % \vspace{-6pt}
    \label{fig:conv_speed}
\end{figure}

\noindent \textbf{Finetuning on a novel category.}
Finally, we consider a knowledge transfer scenario in which \ours pre-trained on Cuboids, Shelves, and Windows is subsequently finetuned on the Containers category.
We compare the resulting performance with that obtained by \ours in the vanilla setting which leverages the task of shape classification on ModelNet as pre-training for backbone initialization. In particular, we inspect how performances vary across data and time constraints. The results in Fig.~\ref{fig:few_shot} show that exploiting pre-trained models on task-specific data provides an advantage over the vanilla strategy in the few-shot scenario with scarce availability of training data.
Similar conclusions can be drawn from the results in Fig.~\ref{fig:conv_speed}, where we analyze the convergence speed of the two approaches in terms of training epochs.

These findings provide a clear indication of the potential of supervised learning for future OCMG applications.
Indeed, the real-time inference capabilities of these models combined with an increasing amount of data can drastically reduce robot programming times.

\subsection{Results: Spray Painting in Simulation}
\label{sec:results_spray_painting}
\ours and the proposed baselines aim at solving the spray painting task purely from expert data, \ie achieving high paint coverage despite not being explicitly optimized for a task-specific objective.
%
To evaluate their capabilities on the downstream task, we run a spray painting simulation by placing the gun nozzle at each 6D waypoint predicted by the network (as displayed in Fig.~\ref{fig:qualitatives_segments_prediction}), at discrete time steps.
%
By doing so, the resulting paint coverage is invariant under permutation of the predicted waypoints and their arrangement into separate paths.
Thus, we can fairly compare all the methods regardless of their point, segment, or path output structure. 
We execute the evaluation using a proprietary painting simulator, although similar tools would be equally suitable~\cite{Andulkar_Incremental_2015}.

The results in Tab.~\ref{tab:quantitative_coverage} show two main trends: \ours achieves (1) substantially higher paint coverage on average, and (2) exhibits the lowest variance across test samples on all object categories.
Notably, over $99\%$ of the object meshes are covered for test instances of Cuboids, Windows, and Shelves when executing \ours's predictions.

Results on the Containers category provide further insights into the performance of \ours.
In line with the PCD results in Tab.~\ref{tab:quantitative_all_metrics}, \ours attains a significant improvement in terms of paint coverage compared to the Path-wise and Autoregressive baselines.
We report qualitative results for a representative Container object in Fig.~\ref{fig:qualitatives_pc_containers}.
%
Here, it is evident that Path-wise and Autoregressive predictions display issues in managing high dimensional paths or diverge due to compounding errors---as mentioned in Sec.~\ref{sec:results_network_predictons} and highlighted in Fig.~\ref{fig:qualitatives_segments_prediction}. 
As a result, large portions of the surface remain uncovered. 

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/qualitatives/qualitative_pc_coverage_v4.pdf}
    \vspace{-16pt}
    \caption{
    Qualitative paint coverage results in simulation on a representative test instance of the Containers category.
    }
    \label{fig:qualitatives_pc_containers}
    % \vspace{-6pt}
\end{figure*}

\input{03_tables/quantitative_coverage_stdevs}

\subsection{Results: Spray Painting Validation in the Real World}
\label{sec:real_world_exps}
We conclude our experimental evaluation by demonstrating the successful application of \ours to a real-world industrial robotic system on the production line. 
%
Our validation aims to answer the following questions:
\begin{itemize}
    \item \emph{Are paths predicted by \ours kinematically and dynamically feasible for direct execution on a real robot?}
    \item \emph{Does near-complete coverage in simulation translate to near-complete coverage of real objects?}
\end{itemize}
%
To this end, two representative object instances are drawn from the test set of Cuboids and Windows and hand-crafted by experts with their nominal dimensions.
Note that, by doing so, the object point-cloud is readily available from our dataset and no domain shift between synthetic and real data is experienced by the network at inference time.

We design the experiment for powder coating, a widely adopted spray painting technique where paint is applied electrostatically to the object and subsequently cured under heat.
We refer the reader to Fig.~\ref{fig:realworld_validation_results} for an illustration of the full real-world validation pipeline.

\noindent \textbf{Inference and execution.}
We deploy the same category-specific models whose results have been discussed in Sec.~\ref{sec:results_network_predictons}  
%
to predict output segments for the Cuboid and Window instances displayed in Fig.~\ref{fig:realworld_validation_results} (left).
Predicted segments are then automatically concatenated through our postprocessing step to generate output paths.
%
The entire path generation pipeline only takes $200ms$ for each input object, requires a single forward pass of the network, and results in six paths for the Cuboid (total length of 83 meters) and 14 paths for the Window (total length of 17 meters).
%
The generated paths can be then executed on real robots by employing any approach of choice, such as tracking waypoints in Cartesian space.
%
In our setting, we apply the Ramer–Douglas–Peucker algorithm---with thresholds of $1cm$ for translational coordinates and $15deg$ for rotational coordinates---and interpolate the subsampled waypoints through smooth junctions that preserve acceleration limits while targeting a desired velocity.
In particular, we set the target velocity to $25cm/s$.
%
A dedicated software automatically detects and manages reachability issues: in our physical setup it occasionally applied minor adjustments to the orientation normals of the paths (\eg, for the four inner and four outer edges of the window).

Furthermore, motivated by the symmetries of the considered objects, we avoid painting the back side of the window and cuboid without loss of generality.
We execute paths in a random permutation, and reset the robot to a predefined starting configuration between each path execution.
%
Both the generated and the ground-truth paths are then executed on a real
Efort GR-680 6-DoF specialized painting robot, with their final outcome depicted on the right side of Fig.~\ref{fig:realworld_validation_results}.


\noindent \textbf{Results and discussion.}
All paths generated by \ours were executed successfully on real hardware.
Paths were found to be dynamically feasible for direct execution, and no collisions occurred at any time.
%
Importantly, the final spray painting outcome achieved with \ours trajectories proved indistinguishable from that of ground-truth expert trajectories across both objects (cf. Fig.~\ref{fig:realworld_validation_results}).
Remind that \ours includes no explicit paint optimization subroutine, rendering the final result a ground-breaking demonstration of its potential for addressing OCMG tasks through a pure data-driven perspective.
%
Notably, we observe slight uneven paint deposition across both ground truth and predicted paths on the real Cuboid (see the vertical stripe trend in Fig.~\ref{fig:realworld_validation_results}).
This effect can be mitigated by domain experts with further application-specific trajectory optimization techniques~\cite{gleeson2022generating}, or manual tuning of the target velocity for different paint types.
%
Nevertheless, our real-world validation experiment demonstrates the effectiveness of \ours in quickly generating expert-level path patterns for previously unseen object instances, while requiring no domain knowledge and remaining applicable to a variety of object-centric motion generation tasks.

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/realworld_validation/realworld_validation_v5.pdf}
    \vspace{-12pt}
    \caption{
    Real-world validation of \ours on two test objects. A set of long-horizon paths is inferred given the object point clouds through a single forward pass ($100ms$) and a postprocessing step ($100ms$). Then, paths are checked for kinematic and dynamic feasibility in simulation and later executed on the real setup.
    The final paint result on the real objects is effectively equivalent to that produced by ground truth paths.
    }
    \label{fig:realworld_validation_results}
    % \vspace{-12pt}
\end{figure*}