In this paper, we address the core problem of robot motion generation conditioned on free-form 3D objects. Particularly, we formalize the Object-Centric Motion Generation (OCMG) problem setting, aiming to unify several robotic applications under a common framework for the prediction of long-horizon, unstructured paths.
To tackle this problem, we introduce \ours, a novel deep learning method capable of inferring smooth and accurate paths directly from expert data.
%
In simple terms, \ours breaks down motion generation into the joint prediction of local path segments and probability masks, followed by a postprocessing step that concatenates the segments predicted within the same mask.
Our approach demonstrates generalization across both convex and concave 3D objects (Cuboids, Windows, Shelves), after being trained for only 6 hours on 800 samples on a single NVIDIA GeForce RTX 4080 GPU.
We validate \ours in the context of robotic spray painting, demonstrating its ability to achieve near-complete paint coverage on previously unseen objects, both in simulation and in real-world experiments.
Our method remains task-agnostic and makes minimal assumptions on both the object geometry and the output path patterns, paving the way for future applications of deep learning to OCMG tasks beyond spray painting---such as welding, sanding, cleaning, or visual inspection.
In fact, our findings showcase the potential of data-driven methods to achieve solutions to OCMG tasks that are scalable, generalizable, and cheap to deploy: models can (1) be pre-trained with data from similar tasks, (2) boosted in performance when more samples become available, (3) leverage heuristics-based demonstrations, and (4) be deployed at 10Hz on large objects.
Remarkably, we believe this approach can shine even for high-precision tasks when used in combination with task-specific trajectory optimization techniques that leverage \ours to promptly generate good starting solutions for novel objects.
%%%%%%%%%%
Furthermore, \ours enables the generation of unstructured paths from data modalities beyond 3D objects by replacing the feature encoder, opening new directions of work for applications such as agricultural coverage path planning from aerial images and multi-UAV search-and-rescue missions.

\noindent\textbf{Limitations.}  Our current pipeline relies on a simple postprocessing strategy to concatenate predicted segments, which is not able to recover from erroneous predictions and may lead to inaccurate paths in such cases. Learning-based modules for concatenation will be investigated in future work to be robust to misplaced segment predictions.
We also reckon that the choice of the segment length $\lambda{=}4$ may not work equally well across different tasks and objects.
This hyperparameter should be tuned accordingly to learn accurate isolated patterns that generalize well across the object surface.
%
Alternatively, tuning the granularity of the sampled waypoints from the expert trajectories to achieve the desired degree of sparsity in the predicted poses would serve a similar purpose.
Finally, our practical implementation simplifies gun orientations to unit vectors (from 3-DoF to 2-DoF), making the sole assumption that the task is invariant to gun rotations around the approach axis.
%
Future work can extend the pipeline to the prediction of full orientation representations---\eg, Euler angles---to tackle tasks where the additional degree of freedom is necessary, such as motion generation for object grasping.