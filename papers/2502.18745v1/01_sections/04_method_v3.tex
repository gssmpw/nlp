\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/notation_helper/notation_helper_v2.pdf}
    \vspace{-16pt}
    \caption{Schematic illustration of a data sample $(\bO, \bY)$ describing input and output of the OCMG problem. The output paths are unstructured as they vary in number and length depending on the input object and can be executed in arbitrary order.}
    \label{fig:notation_helper}
\end{figure}


\subsection{Problem statement}
We formalize the OCMG problem as finding a mapping from a 3D object point cloud to a set of unstructured output paths (see Fig.~\ref{fig:notation_helper}). 
Given expert demonstrations to learn from, we aim to generate accurate paths for previously unseen objects. 

Let $\bO$ represent the object geometry as a point cloud consisting of an arbitrary number of points in 3D space.
%
Each object $\bO$ is associated with a ground truth set of paths $\bY {=} \{\by^i \}_{i=1}^{n(\bO)}$, with object-dependent cardinality $n(\bO)$. 
Every path $\by^i = (\bp^i_1, \dots, \bp^i_T) \in \mathcal{Y} \subseteq \mathbb{R}^{6 \cdot T}$ is encoded as a sequence of 6D poses $\bp \in \mathbb{R}^6$. For simplicity, we consider the dimension $T$ fixed, with shorter paths zero-padded to reach that maximum length.  

Under this formulation, we consider the problem of finding a function $f : 2^{\mathbb{R}^{3}} \rightarrow 2^{\mathcal{Y}}$ mapping the set of points $\bO$ describing the object geometry to the set of desired paths $\bY$\footnote{For a set \( \mathcal{S} \), the notation \( 2^{\mathcal{S}} \) here denotes the powerset of \( \mathcal{S} \), \ie, the set of all subsets of \( \mathcal{S} \).}.
To do so, we parametrize $f$ using a deep neural network, and train it through empirical risk minimization~\cite{vapnik1991principleserm}.
%
Specifically, we minimize a loss function $\mathcal{L}(\hat{\bY}, \bY)$, which quantifies the discrepancy between the predicted paths $\hat{\bY}=f(\bO)$ and the ground truth paths $\bY$ on the training data, using gradient descent to optimize the network parameters.

%
We highlight that this formulation does not make task-specific assumptions related to the spray painting problem, making our contribution applicable to a broad range of object-centric motion generation tasks (\eg, welding or cleaning).
%
%

\subsection{Method Overview}
We tackle object-centric motion generation with a tailored deep learning model that copes with unstructured input---3D point clouds---and unstructured output paths.
More precisely, instead of directly predicting a set of paths, we decompose the problem into the prediction of unordered path segments, \ie, short sequences of 6D end-effector poses (or \emph{waypoints}).
%
In addition, we concurrently predict a set of probability masks that identify which segments belong to the same path.
We denote our method as \emph{\ours}, and emphasize that all required path segments and masks are predicted by our network in parallel, with a single forward pass.
Such approach induces our model to learn end-to-end global embeddings of the input object that allow for both local (segments) and global (masks) planning decisions in one step.
Overall, by designing a joint segment and mask prediction pipeline, we conveniently simplify the problem of dealing with unstructured paths and address 

\begin{itemize}[leftmargin=*,itemsep=2pt]
\item \textbf{long-horizon paths:} we do not impose constraints on the shape or length of each path.
Diverse path configurations can be handled just as effectively, as exemplified in Fig.~\ref{fig:temporal_correlation}.
%
\item \textbf{unordered paths:} within a set prediction framework, the order of the predicted path segments is irrelevant. Therefore, concatenating segments that belong to the same path naturally yields a set of output paths that are invariant by permutation.
%
\item \textbf{variable length and number of paths:} we predict a conservatively large number of path segments, allowing for the generation of redundant overlapping segments that can be easily filtered out.
\end{itemize}

Our method takes inspiration from the Panoptic Segmentation (PanSeg) literature~\cite{carion2020detr,cheng2021maskformer}, where a variable number of class instances shall be predicted given a static environment with global information (an RGB image).
Notably, works in the field of PanSeg shifted towards one-shot predictors over the years as opposed to multiple-stage or autoregressive approaches.
Similarly, we depart from sequential methods for OCMG, as we aim to reach real-time inference capabilities and avoid compounding errors on long-horizon predictions.


\begin{figure}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/temporal_correlation/fig_global_v3.pdf}
    \vspace{-8pt}
    \caption{Example of two arbitrary configurations of ground truth paths $\bY$ on an L-shaped 2D object.
    Notice how \ours can easily manage both cases by breaking down the learning problem into the prediction of path-agnostic segments and their associated path masks.
    }
    \label{fig:temporal_correlation}
    \vspace{-6pt}
\end{figure}

Throughout this work, we demonstrate that \ours is capable of predicting a large number of long-horizon paths with a single forward pass, that implicitly incorporate task-specific requirements without domain knowledge.
We remark that ad-hoc trajectory optimization methods~\cite{gleeson2022generating} may still be applied to impose task-specific kinodynamic constraints (\eg  reachability and collision avoidance), which we consider subsequent and complementary to the scope of this work. 
Here, we directly interpolate the predicted sequence of 6D waypoints for execution on a real robot, resulting in feasible trajectories out of the box.

In the following part of this section we describe each step of our method in detail, breaking it down into segment predictions (Sec.~\ref{sec:segments_prediction}), mask predictions (Sec.~\ref{sec:masks_prediction}), and postprocessing (Sec.~\ref{sec:postprocessing}).
%

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/method/method_v9_withNotation.pdf}
    \caption{Overview of the training pipeline of our method (\ours). Global features are learned from a point cloud representation of the input object, and used to concurrently predict path segments and path masks, in a single forward pass.
    }
    \label{fig:method}
    \vspace{-6pt}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\subsection{Segment Predictions}
\label{sec:segments_prediction}

Let $\bS$ be the set of ground truth path segments $\bS {=} \{\bs^j\}_{j=1}^{k(\bY)}$ of an object $\bO$ and its associated paths $\bY$.
We define a segment as a sequence of $\lambda \in \mathbb{N}^+$ poses obtained from a path $\by^i \in \bY$.  
Namely, $\bs^j = (\bp^i_t, \dots, \bp^i_{t+\lambda-1}) \in \mathbb{R}^{6 \cdot\lambda}$ for some timestep $t{=}1, \dots , T-\lambda$.
We derive all segments in \(\bS\) by striding along all paths with a step of $\lambda-1$, \ie considering an overlap of one pose between consecutive segments.
Notice that the total number of resulting segments $k(\bY)$ depends on the number of paths and their lengths.

%
We design our model to take the object point cloud $\bO$ as input, and predict a set of path segments $\bShat{=}\{\bshat^j\}_{j=1}^{K}$ that approximate the true segments $\bS$. 
We adopt the PointNet++ architecture~\cite{Qi_Pointnet++_2017} as basic backbone for global feature extraction from $\bO$, followed by a fully connected 3-layer decoder that jointly outputs all path segments. 
A fixed number of segments $K = \max k(\bY)$ is predicted to ensure all ground truth segments are recovered for all objects.

Let $\bP$ be the set of unordered ground truth waypoints, 
formally described as 
$\{\bp^i_t {\in} \mathbb{R}^6 \ | \ i {\in} [1, \dots, n(\bO)], \ t{\in} [1, \dots, T] \}$
---equivalent to the set of segments for $\lambda{=}1$. Analogously, we define  
$\bPhat{=}\{ \bshat^j_t {\in} \mathbb{R}^6 \ | \ j {\in} [1, \dots, K], \ t {\in} [1, \dots, \lambda] \}$
as the set of individual predicted waypoints, obtained by interpreting $\bShat$ as an unordered collection of waypoints.

\ours is trained with a novel loss function aimed at driving the prediction of path segments $\bShat$ close to the ground truth segments $\bS$ by means of Euclidean distances in $\mathbb{R}^{6\cdot\lambda}$ space.
To do this, our loss includes auxiliary point-wise terms that penalize prediction errors in the lower dimensional space $\mathbb{R}^{6}$, by disregarding how waypoints are arranged into segments.
Overall, our \emph{Point-to-Segment Chamfer Distance} (P2S-CD) is defined as:
\begin{equation}
\label{eq:pscd}
\begin{split}
    \mathcal{L}_{p2s}(\bPhat ,\bP,\bShat ,\bS) & = \\
    & w^{f}_{p} \cdot d_{ACD}(\bPhat , \bP) + w^{f}_{s} \cdot d_{ACD}(\bShat , \bS) \ + \\
    & \underbrace{\raisebox{10pt}{$w^{b}_{p} \cdot d_{ACD}( \bP,\bPhat)$}}_{\text{Point-wise}} \raisebox{10pt}{$+$} \underbrace{\raisebox{10pt}{$w^{b}_{s} \cdot d_{ACD}(\bS,\bShat)$~.}}_{\text{Segment-wise}}
\end{split}
\end{equation} 
Here, the \emph{Asymmetric Chamfer Distance} (ACD) \emph{from} set $A$ \emph{to} set $B$ is: 
\begin{equation}
    d_{ACD}(A, B) = \frac{1}{|A|} \sum _{a\in A}\min_{b\in B} \| a-b\| _{2}^{2}~,
\end{equation}
and the parameters $w^f_p, w^f_s, w^b_p, w^b_s \in \mathbb{R}^{+}$ weight the computation of the four ACD terms ($f$: forward, $b$: backward, $p$: point-wise, $s$: segment-wise). 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Based on this formulation, we propose an auxiliary point-to-segment curriculum that (1) first focuses on matching waypoints in some random permutation (point-wise terms), and then (2) gradually promotes local structure by computing distances among sequences of poses (segment-wise terms).
%
To achieve this, we start the training with a dominant point-wise term ($w^b_p {\gg} w^b_s$) and progressively converge to equal point-wise and segment-wise contributions ($w^b_p{=}1, w^b_s{=}1$).
Notably, this procedure is asymmetric and only applied on the backward terms: this ensures that predictions are pulled sufficiently close to the ground truth poses, which in turn enables effective global coverage of the desired poses, and limits the generation of clusters of poses~\cite{densityawarecd}.
%
On the other hand, we keep the forward weights fixed to $w^f_p{=}0$, $w^f_s{=}1$ throughout the training, promoting the generation of segments that are smooth and locally accurate.
%
We refer to this process as the \emph{Asymmetric Point-to-Segment} (AP2S) curriculum, and report a schematic illustration of our overall loss function in Fig.~\ref{fig:apscurriculum}.

\begin{figure}[tb]
    \centering
    \includegraphics[width=\linewidth]{02_images/asymmCurrChamferDistance/asymm_curr_CD_v7_blu.pdf}
    \caption{
    Illustration of our Asymmetric Point-to-Segment curriculum for segment predictions ($\lambda{=}3$). The parameters $w^b_p,w^b_s$ weighting the backward point-wise and segment-wise ACD terms vary during training.}
    \vspace{-6pt }
    \label{fig:apscurriculum}
\end{figure}

\begin{figure*}[!t]
    \centering
    \includegraphics[width=\linewidth]{02_images/postprocessing/postprocessing_v2.pdf}
    \caption{
    Postprocessing: concatenation of the set of predicted segments belonging to the same path mask.
    In step (1) the figure depicts raw network predictions with $\lambda{=}4$, with separate segments differentiated by color. Step (2) shows the effect of segment filtering. In step (3) and (4), where the path is identified and further refined, the ordered sequence of waypoints is shown with a color gradient.}
    \label{fig:postprocessing}
    % \vspace{-12pt}
\end{figure*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Mask Predictions}
\label{sec:masks_prediction}

Our model concurrently predicts a set of probability masks over predicted segments, indicating which segments belong to the same path. 
Defining a supervised learning objective for this task is not trivial, as it requires comparing predicted masks on generated segments with ground truth masks on generated segments. However, the latter do not exist.
%
A reasonable choice is to match each predicted segment with the closest ground truth segment---as already proposed in Sec.\ref{sec:segments_prediction}---and construct target masks accordingly.
We refer to this strategy as \emph{nearest-neighbour label association}, and use it to project masks over ground truth segments onto predicted segments.
In particular, let $\bM{=}\{\bm^i\}_{i=1}^{n(\bO)}$ be the set of \emph{target path masks} for some input pair $(\bO,\bY)$.
Each element $\bm^i \in \{0,1\}^K$ encodes the set of predicted segments that belong to path $i$, that is
%
\begin{equation}
\begin{split}
    & \bm^{i}_{j} =\begin{cases}
1 & \mathrm{if\ } \mathrm{NN}(\bshat^j) \ \mathrm{belongs \ to \ path} \ i\\
0 & \mathrm{otherwise},
\end{cases} \\
& \forall \ j = 1, \dots, K
\end{split}
\end{equation}
where $\mathrm{NN}(\bshat) {=} \argmin_{\bs \in \bS} \| \bs-\bshat \|^2$. 
%
Our model is designed to predict a set of $N\geq n(\bO)$ probability masks $\bMhat{=}\{ \bmhat^{i} \}_{i=1}^{N}$, where $N{=}\max n(\bO)$ is the maximum number of paths across objects $\bO$ in the training set. 
All masks are predicted in parallel via a 3-layer MLP decoder from the global features of the object, followed by a sigmoid activation.
Note that, as in~\cite{cheng2021maskformer}, we do not force mask predictions to be mutually exclusive for a given segment $\bshat^j$. Hence we avoid using a softmax activation.

We drive the predicted path masks $\bMhat$ towards the target path masks $\bM$ through the Binary Cross Entropy (BCE) loss:
\begin{equation}
\label{eq:bce_loss}
    \mathcal{L}_{bce}(\bmhat, \bm) = - \sum_{j=1}^{K}\Bigl[\bm_j\cdot\log(\bmhat_j) + (1-\bm_j)\cdot\log(1-\bmhat_j)\Bigl].
\end{equation}
Importantly, we must consider the prediction of permutation invariant masks to cope with a set of unordered paths.
Therefore, we assign each predicted path mask $\bmhat$ to a target mask $\bm$ by finding a bijection $ \sigma : \bMhat \rightarrow \bM$.
%
Similarly to~\cite{carion2020detr,cheng2021maskformer}, we do this by solving a bipartite matching problem between the two sets, where the assignment costs are computed using $\mathcal{L}_{bce}$.
%
Particularly, we pad the target masks with a ``no path'' token $\varnothing$ to allow one-to-one matching.
Ultimately, the training loss for mask prediction is as follows:
\begin{equation}
\begin{split}
    % \mathds{1}_{4+3}
    \mathcal{L}_{mask}(\bMhat, \bM) = \\ \sum_{i=1}^{N}
    \Bigl[ 
        % \mathcal{L}_{bce}(c^i, \mathds{1}_{\bm^i \neq \varnothing})
        & c^{\sigma(i)}\cdot\log(\hat{c}^i) + (1-c^{\sigma(i)})\cdot\log(1-\hat{c}^i) \\
        & + c^{\sigma(i)} \cdot
         \mathcal{L}_{bce}(\bmhat^i, \bm^{\sigma(i)})
    \Bigl]~,
\end{split}
\end{equation}
where $\hat{c}^i \in [0,1]$ is a learned confidence score for each mask, and $c^i{=}\mathds{1}_{\bm^i \neq \varnothing}$ indicates the true masks.
%%%%%%%%%%%%%%%%%%%%%%%%
Overall, we train our model to minimize $\mL = \mL_{p2s} + \mL_{mask}$. 
We display a schematic description of our training pipeline in Fig.~\ref{fig:method}.

At inference time, we derive the groups of segments belonging to the same path by assigning each predicted segment $\bshat^j$ to one of the $N$ predicted masks. Formally, the assignment of segment $\bshat^j$ occurs as follows:
\begin{equation}
\label{eq:path_mask_assignment}
    \begin{split}
        \underset{i \in [1, \dots, N]}{\argmax} \ \bmhat^i_j \ \ \text{s.t.} \ \ \hat{c}^i \geq 0.5~.
    \end{split}
\end{equation}
In other words, segments are assigned to the path mask with the highest predicted probability, discarding path masks that are predicted as ``no path''. Assigning all predicted segments according to Eq.~(\ref{eq:path_mask_assignment}) results in a final number of $\hat{n}(\bO)\leq N$ paths predicted by our model given the input object, ideally equal to $n(\bO)$.


%%%%%%%%%%%%%%%%%%%%%%
\subsection{Postprocessing: Segment Concatenation}
\label{sec:postprocessing}
%
A final postprocessing step is applied to concatenate the subset of predicted segments $\bShat^i \subseteq \bShat$ that are assigned to the same path mask $\bmhat^i$, and produce an ordered sequence of 6D waypoints, \ie the executable path.

Viable solutions may include solving an open Traveling Salesman Problem (TSP) among segments or employing learning-based approaches for ranking. Here, we adopt a simple and effective concatenation strategy based on segment proximity and alignment. 

First, predicted segments in excess are removed by discarding segment pairs whose distance falls below a predefined threshold, proceeding in ascending order of pairwise distances\footnote{Note that our loss $\mathcal{L}_{p2s}$ implicitly promotes overlapping if the number of predicted segments $K$ is higher than the ground truth segments $k(\bY)$.}. 
%
Then, we find an optimal path that connects the retained segments. Consider the set of segments in $\bShat^i$ as nodes of a directed graph. An edge among two segments is weighted based on the proximity in space and orientation between the starting and ending poses of the two segments, as well as the similarity in segment directions.
More formally, the assigned cost to the edge from $\bshat^j$ to $\bshat^k$ is 
\begin{equation}
\label{eq:cost}
    C(\bshat^j,\bshat^k) = \| \bshat^j_\lambda - \bshat^k_1 \|_2^2 + w_v \cdot \| (\bshat^j_\lambda - \bshat^j_{\lambda-1}) - (\bshat^k_2 - \bshat^k_1)  \|_2^2~.
\end{equation}
Here, $w_v \in \mathbb{R}^+$ is a trade-off weight between the two terms, and the segments' subscripts specify the index of a particular pose among the $\lambda$ poses that make up each segment.

We find the optimal concatenation by employing the Edmonds' algorithm~\cite{edmonds1967optimum} to the $k$-nearest neighbor graph of segments $\bShat^i$ constructed using $C(\cdot, \cdot)$ and $k{=}5$.
Ultimately, we extract the longest path from the resulting Directed Acyclic Graph (DAG) and obtain the final \emph{ordered} sequence of predicted segments.
At this point, filtering techniques such as interpolation, upsampling, and smoothing may be conveniently applied.
The same process is repeated independently for all predicted path masks.
For clarity, we illustrate each step of the postprocessing in Fig.~\ref{fig:postprocessing}.