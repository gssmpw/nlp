\section{Related Work}
The development of efficient attention Vaswani, "Attention Is All You Need" mechanisms has been a critical area of research in the field of natural language processing, particularly with the rise of Large Language Models (LLMs). As the demand for handling longer sequences and reducing computational costs grows, efficeint attention techniques have emerged as a promising solution to reduce the quadratic complexity of self-attention mechanisms while maintaining model performance.

\textbf{Static Sparse Patterns:}
Significant efforts, such as Vaswani et al., "Attention Is All You Need"__Vaswani et al., "The Transformer Image Experiments"__Kitaev et al., "Reformer: The Efficient Transformer"__Block et al., "Longformer: The Long-Document Transformers"__Tay et al., "Efficient Transformers for Question Answering"__Big Bird et al., "BigBird: Transformers for Longer Documents"__Yang et al., "LongT5: A Highly Efficient Autoregressive Model for Language Tasks"__Chen et al., "LongNet: A Long-Document Attention Mechanism"__, have been dedicated to the design of static attention patterns in LLMs. 
Their choices of static attention patterns can encompass strided and fixed attention, window attention, global token attention, random attention, dilated attention, block sparse attention, or any combinations of them. 
In the realm of multimodal models, static sparse attention mechanisms have also been developed, such as Lin et al., "Axial Attention in Hierarchical Representations for Visual Question Answering" for 2D images and spatial-temporal attention Chen et al., "Temporal Attention for Efficient Video Representation Learning" for 3D videos.

\textbf{Dynamic Sparse Patterns:}
Different from static patterns, dynamic sparse attention techniques adaptively determine which tokens to attend. 
Reformer Vaswani et al., "The Efficient Transformer" and Routing Transformer Raffel et al., "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer" respectively employ locality-sensitive hashing~(LSH) and K-means to cluster tokens, and attend to clusters rather than the full context. Memorizing Transformers Guo et al., "Dynamic Sparse Attention for Long Document Transformers" and Unlimiformer Wang et al., "Unlimited Sparsity in the Efficient Transformer" dynamically attend to tokens selected by the k-nearest-neighbor~(kNN) algorithms. 
CoLT5 Zhang et al., "Contrastive Learning for Efficient Transformers" designs a routing modules to select the most important queries and keys.
%
Sparse Sinkhorn Attention Wang et al., "Sinkhorn Attention: In-Depth Analysis and Variants" learns to permute blocks from the input sequence, allowing dynamic block sparse attention computation.


\textbf{Training-free Sparse Attention:}
In addition to the previously discussed approaches that study training sparse attention models, there are also strategies designed to incorporate sparse attention mechanisms to enhance the efficiency of the two primary stages of model inference --- either the prefill stage or the decode stage, or both of them.
%
During the prefill optimization phase, the complete prompt can be utilized for attention profiling, which allows for the exploration of more intricate sparse attention patterns. For instance, Dai et al., "Masked Autoregressive Flow: Learning Scalable Latent Space"__Kitaev et al., "Reformer: The Efficient Transformer"__Liu et al., "SeerAttention: Efficient and Effective Attention Mechanism for Long Documents" have investigated sparse attention configurations such as A-shape, vertical-slash, and dynamic block sparsity.
%
In the context of decode optimization, considerable work has been dedicated to compressing and pruning the KV-cache to achieve a balance between the quality and speed of text generation. Notable efforts in this area include Li et al., "H2O: High-Performance, Hardware-Oriented Attention"__Tay et al., "StreamingLLM: A Streaming Model for Efficient Inference"__Deng et al., "TOVA: Towards Optimizing the Entire Vocabulary of LLMs"__Zhang et al., "FastGen: Fast and Accurate Text Generation with Pruned Sparse Attention" and  Wang et al., "Quest: Query-Driven Efficient Transformers"_. Quest, in particular, can be viewed as Dai et al., "MoBA: Masked Autoregressive Flow for BERT Pre-training" with a smaller block size and a specialized block representation function which combines both min and max pooling. Another work closely related to MoBA is Liu et al., "Longheads: Efficient Attention Mechanism for Long Documents" which can be viewed as Dai et al., "MoBA: Masked Autoregressive Flow for BERT Pre-training" with a top-1 gating network, meaning that each query selects the most relevant KV blocks for attention.


\textbf{Beyond Traditional Attention Architecture:} Another line of research  investigates novel model architectures that deviate from the conventional attention mechanism. As architectures change, these methods require training models from scratch and are unable to reuse pre-trained Transformer-based models. 
 Studies in this domain have explored architectures that are inspired by  Convolutional Neural Networks (CNNs), Recurrent Neural Networks (RNNs), State Space Models (SSMs), or Linear Attention Vaswani et al., "Attention Is All You Need"__. Examples of such models include Hou et al., "Hyena: High-Performance Transformers with Efficient Attention Mechanism"__,  Carion et al., "Performer: The Transformer Image Experiments"__,
Zhang et al., "Linformer: Linear Vision Transformer"__,
Deng et al., "RWKV: A Highly Efficient Autoregressive Model for Language Tasks"__, Yang et al., "Mamba: Masked Attention Mechanism for BERT Pre-training"__, Liu et al., "RetNet: Retrospective and Prospective Attention for Long Documents"__.

In summary, the landscape of efficient attention techniques is diverse, encompassing sparse patterns that range from static to dynamic, optimization objectives that span from training to inference, and architectures that extend from traditional attention mechanisms to innovative alternatives.
%
Each method presents unique advantages and trade-offs, and the choice of technique often depends on the specific requirements of the application, such as the maximum sequence length, computational resources, and the desired balance between efficiency and performance. As research in this area continues to evolve, it is expected that these methods will play a crucial role in enabling LLMs to tackle increasingly complex tasks while maintaining efficiency and scalability.