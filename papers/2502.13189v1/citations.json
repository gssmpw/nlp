[
  {
    "index": 0,
    "papers": [
      {
        "key": "tay2020efficient",
        "author": "Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald",
        "title": "Efficient transformers: A survey. arXiv"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "child2019generating",
        "author": "Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya",
        "title": "Generating long sequences with sparse transformers"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "guo2019star",
        "author": "Guo, Qipeng and Qiu, Xipeng and Liu, Pengfei and Shao, Yunfan and Xue, Xiangyang and Zhang, Zheng",
        "title": "Star-transformer"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "qiu2019blockwise",
        "author": "Qiu, Jiezhong and Ma, Hao and Levy, Omer and Yih, Scott Wen-tau and Wang, Sinong and Tang, Jie",
        "title": "Blockwise self-attention for long document understanding"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "beltagy2020longformer",
        "author": "Beltagy, Iz and Peters, Matthew E and Cohan, Arman",
        "title": "Longformer: The long-document transformer"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "gupta2020gmat",
        "author": "Gupta, Ankit and Berant, Jonathan",
        "title": "Gmat: Global memory augmentation for transformers"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "ainslie2020etc",
        "author": "Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li",
        "title": "ETC: Encoding long and structured inputs in transformers"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "zaheer2020big",
        "author": "Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others",
        "title": "Big bird: Transformers for longer sequences"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "guo2021longt5",
        "author": "Guo, Mandy and Ainslie, Joshua and Uthus, David and Ontanon, Santiago and Ni, Jianmo and Sung, Yun-Hsuan and Yang, Yinfei",
        "title": "LongT5: Efficient text-to-text transformer for long sequences"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ding2023longnet",
        "author": "Ding, Jiayu and Ma, Shuming and Dong, Li and Zhang, Xingxing and Huang, Shaohan and Wang, Wenhui and Zheng, Nanning and Wei, Furu",
        "title": "Longnet: Scaling transformers to 1,000,000,000 tokens"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ho2019axial",
        "author": "Ho, Jonathan and Kalchbrenner, Nal and Weissenborn, Dirk and Salimans, Tim",
        "title": "Axial attention in multidimensional transformers"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "opensora",
        "author": "Zangwei Zheng and Xiangyu Peng and Tianji Yang and Chenhui Shen and Shenggui Li and Hongxin Liu and Yukun Zhou and Tianyi Li and Yang You",
        "title": "Open-Sora: Democratizing Efficient Video Production for All"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "kitaev2020reformer",
        "author": "Kitaev, Nikita and Kaiser, {\\L}ukasz and Levskaya, Anselm",
        "title": "Reformer: The efficient transformer"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "roy2021efficient",
        "author": "Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David",
        "title": "Efficient content-based sparse attention with routing transformers"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "wu2022memorizing",
        "author": "Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian",
        "title": "Memorizing transformers"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "bertsch2024unlimiformer",
        "author": "Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew",
        "title": "Unlimiformer: Long-range transformers with unlimited length input"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "ainslie2023colt5",
        "author": "Ainslie, Joshua and Lei, Tao and de Jong, Michiel and Onta{\\~n}{\\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and Lee-Thorp, James and Tay, Yi and others",
        "title": "Colt5: Faster long-range transformers with conditional computation"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "tay2020sparse",
        "author": "Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan, Da-Cheng",
        "title": "Sparse sinkhorn attention"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "fu2024moa",
        "author": "Fu, Tianyu and Huang, Haofeng and Ning, Xuefei and Zhang, Genghan and Chen, Boju and Wu, Tianqi and Wang, Hongyi and Huang, Zixiao and Li, Shiyao and Yan, Shengen and others",
        "title": "Moa: Mixture of sparse attention for automatic large language model compression"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "jiang2024minference",
        "author": "Jiang, Huiqiang and Li, Yucheng and Zhang, Chengruidong and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Han, Zhenhua and Abdi, Amir H and Li, Dongsheng and Lin, Chin-Yew and others",
        "title": "Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "gao2024seerattention",
        "author": "Gao, Yizhao and Zeng, Zhichen and Du, Dayou and Cao, Shijie and So, Hayden Kwok-Hay and Cao, Ting and Yang, Fan and Yang, Mao",
        "title": "SeerAttention: Learning Intrinsic Sparse Attention in Your LLMs"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "zhang2024h2o",
        "author": "Zhang, Zhenyu and Sheng, Ying and Zhou, Tianyi and Chen, Tianlong and Zheng, Lianmin and Cai, Ruisi and Song, Zhao and Tian, Yuandong and R{\\'e}, Christopher and Barrett, Clark and others",
        "title": "H2o: Heavy-hitter oracle for efficient generative inference of large language models"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "xiao2023efficient",
        "author": "Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike",
        "title": "Efficient streaming language models with attention sinks"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "oren2024tova",
        "author": "Oren, Matanel and Hassid, Michael and Adi, Yossi and Schwartz, Roy",
        "title": "Transformers are multi-state rnns"
      }
    ]
  },
  {
    "index": 24,
    "papers": [
      {
        "key": "ge2023fastgen",
        "author": "Ge, Suyu and Zhang, Yunan and Liu, Liyuan and Zhang, Minjia and Han, Jiawei and Gao, Jianfeng",
        "title": "Model tells you what to discard: Adaptive kv cache compression for llms"
      }
    ]
  },
  {
    "index": 25,
    "papers": [
      {
        "key": "tang2024quest",
        "author": "Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song",
        "title": "Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference"
      }
    ]
  },
  {
    "index": 26,
    "papers": [
      {
        "key": "lu2024longheads",
        "author": "Lu, Yi and Zhou, Xin and He, Wei and Zhao, Jun and Ji, Tao and Gui, Tao and Zhang, Qi and Huang, Xuanjing",
        "title": "LongHeads: Multi-Head Attention is Secretly a Long Context Processor"
      }
    ]
  },
  {
    "index": 27,
    "papers": [
      {
        "key": "katharopoulos2020transformers",
        "author": "Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\\c{c}}ois",
        "title": "Transformers are rnns: Fast autoregressive transformers with linear attention"
      }
    ]
  },
  {
    "index": 28,
    "papers": [
      {
        "key": "poli2023hyena",
        "author": "Poli, Michael and Massaroli, Stefano and Nguyen, Eric and Fu, Daniel Y and Dao, Tri and Baccus, Stephen and Bengio, Yoshua and Ermon, Stefano and R{\\'e}, Christopher",
        "title": "Hyena hierarchy: Towards larger convolutional language models"
      }
    ]
  },
  {
    "index": 29,
    "papers": [
      {
        "key": "choromanski2020rethinking",
        "author": "Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and others",
        "title": "Rethinking attention with performers"
      }
    ]
  },
  {
    "index": 30,
    "papers": [
      {
        "key": "wang2020linformer",
        "author": "Wang, Sinong and Li, Belinda Z and Khabsa, Madian and Fang, Han and Ma, Hao",
        "title": "Linformer: Self-attention with linear complexity"
      }
    ]
  },
  {
    "index": 31,
    "papers": [
      {
        "key": "peng2023rwkv",
        "author": "Peng, Bo and Alcaide, Eric and Anthony, Quentin and Albalak, Alon and Arcadinho, Samuel and Biderman, Stella and Cao, Huanqi and Cheng, Xin and Chung, Michael and Grella, Matteo and others",
        "title": "Rwkv: Reinventing rnns for the transformer era"
      }
    ]
  },
  {
    "index": 32,
    "papers": [
      {
        "key": "gu2023mamba",
        "author": "Gu, Albert and Dao, Tri",
        "title": "Mamba: Linear-time sequence modeling with selective state spaces"
      }
    ]
  },
  {
    "index": 33,
    "papers": [
      {
        "key": "sun2023retentive",
        "author": "Sun, Yutao and Dong, Li and Huang, Shaohan and Ma, Shuming and Xia, Yuqing and Xue, Jilong and Wang, Jianyong and Wei, Furu",
        "title": "Retentive network: A successor to transformer for large language models"
      }
    ]
  }
]