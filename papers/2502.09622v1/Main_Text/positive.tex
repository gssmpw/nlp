
\section{MDMs Can Generate Low-Perplexity Sentence More Efficiently}
\label{sec:positive}

Although parallel sampling (\cref{eq:parallel_sample}) in MDM inference introduces a known distributional mismatch with respect to ground-truth data, the practical implications of this divergence in sampling quality remain insufficiently explored. The inherent sparsity and ambiguity of natural language suggest that such mismatches may not necessarily manifest as perceptible inconsistencies in the generated text. In this section, we adopt the $n$-gram language model as an analytical framework to rigorously investigate both the efficiency and quality of sampling in MDMs. Despite their simplicity, $n$-gram models, which capture local dependencies in language, have long served as foundational tools for a broad range of NLP tasks, including text generation, machine translation, and speech recognition. Notably, recent studies have demonstrated that $n$-gram models can achieve comparable performance with modern large language models across diverse datasets, highlighting their continued relevance in NLP research. Within this framework, we show that MDMs are capable of efficiently generating $n$-gram languages while preserving high output quality.

\textbf{$n$-Gram Language.} The $n$-gram language model provides a statistical framework for modeling natural language by estimating the likelihood of a token conditioned on its preceding \(n-1\) tokens. Formally, let $\gV$ be the vocabulary and $q$ be the $n$-gram model, given a sequence $\vx = (x_1, x_2, \dots, x_L)\in\gV^L$, the $n$-gram model approximates the probability of the sequence as
\begin{equation}
    q(\vx) = \prod_{i=1}^L q(x_i \mid x_{\max\{1,i-n+1\}}, \dots, x_{i-1}),
\end{equation}
where $q(x_i \mid x_{\max\{1,i-n+1\}}, \dots, x_{i-1})$ denotes the conditional probability of token $x_i$ given its preceding \(n-1\) tokens. 
%

To establish the main theoretical results, we begin with the following assumption:

\begin{assumption}[Learning with Small Error]
\label{ass:perfect_learning}
    Let $q$ denote the $n$-gram language model with vocabulary $\gV$, and let $p_\mathbf{\theta}$ represent the reverse model trained to approximate the reverse process of the $n$-gram language under a masking schedule $\alpha_t$. Assume there exists $\delta > 0$ such that the KL divergence between $p_\mathbf{\theta}$ and the reverse process distribution of the $n$-gram language is bounded by $\delta$, i.e.,
    \begin{equation*}
        \DKL{q_{0|t}(x_0^i \mid \vx_t)}{p_\mathbf{\theta}(x_0^i \mid \vx_t)} < \delta, \quad \forall\ t \text{ and } \vx_t.
    \end{equation*}
\end{assumption}

It is worth noting that $p_\mathbf{\theta}(x_0^i \mid \vx_t) = q_{0|t}(x_0^i \mid \vx_t)$ represents the optimal solution to the ELBO loss during training. \cref{ass:perfect_learning} implies that the MDM model is well-trained and approximates the ground-truth distribution with only a small error.

During MDM inference, the time interval $[0, 1]$ is discretized into $T$ steps, where $t_i = \frac{i}{T},\ i \in [T]$, and the reverse process is solved sequentially. The following theorem establishes that the distribution generated by the reverse process, even with a small number of sampling steps, achieves near-optimal perplexity. Consequently, MDMs exhibit high efficiency in generating $n$-gram languages.

\begin{theorem}[Perplexity Bounds for $n$-Gram Language Generation]
\label{thm:acceleration_ngram}
    For any $n$-gram language $q$ and any $\epsilon > 0$, let $p_\mathsf{\theta}$ denote the reverse model. Under \cref{ass:perfect_learning}, there exists a masking schedule $\alpha_t$ such that, with $T = O\big( (\frac{2\log|\gV|}{\epsilon})^{(n-1)} \big)$ sampling steps, the perplexity of the MDM is upper-bounded by:
    \begin{equation}
        \begin{gathered}
            \operatorname{PPL}(p_\mathsf{\theta}) \leq \operatorname{PPL}(q) (1 + \delta + \epsilon), \\
            \text{where} \quad \operatorname{PPL}(p) = 2^{\mathbb{E}_{\vx \sim q} \frac{\log (p(\vx))}{|\vx|}}.
        \end{gathered}
    \end{equation}
\end{theorem}

\begin{remark}
For a given data distribution $q$, the perplexity of a language model $p$ achieves its global minimum when $p = q$.
\end{remark}

\cref{thm:acceleration_ngram} demonstrates that MDMs can efficiently generate $n$-gram languages with high fidelity. To ensure a perplexity gap of at most $\epsilon$ during sampling, the number of required sampling steps is bounded by $O\big( \big(\frac{2 \log |\gV|}{\epsilon}\big)^{(n-1)} \big)$. This bound is independent of the sequence length $L$ and scales polynomially with respect to the logarithm of the vocabulary size $|\gV|$ and the inverse of $\epsilon$, with a polynomial degree of \(n-1\).

\textbf{Practical Insights.} Perplexity is a widely adopted metric for evaluating the quality of text generation models. Models with lower perplexity are typically better at producing coherent, fluent, and contextually appropriate text. In \cref{thm:acceleration_ngram}, we establish that MDMs can achieve near-optimal perplexity, underscoring their ability to generate high-quality text while maintaining sampling efficiency. Recent work has shown that $n$-gram models trained on one trillion tokens with a dynamic $n$ can achieve perplexity levels comparable to large language models across diverse datasets. For instance, when the median $n$ is approximately 7, the required number of sampling steps is significantly smaller than the sequence length $L$, even as $L$ increases. MDMs capitalize on this property by employing parallel sampling, which generates multiple tokens simultaneously at each step. Consequently, the number of sampling steps required by MDMs remains independent of $L$, offering substantial efficiency gains over autoregressive models, which scale linearly with $L$. This efficiency enables MDMs to handle long-sequence generation tasks with efficiency while maintaining high-quality outputs. 