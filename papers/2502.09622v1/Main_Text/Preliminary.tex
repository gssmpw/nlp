\section{Masked Diffusion Language Model}
\label{sec:mdm}

Without loss of generality, we study the sequence generation task where the sequence length is upper bounded by $L$. Let $\gV$ denote the vocabulary. The MDM \citep{lou2024discrete,shi2024simplified,gong2024scaling,sahoo2024simple} extends the vocabulary $\gV$ by introducing a special mask token $\mask$. The forward diffusion process progressively transforms an initial sequence $\vx_0 = (x_0^1, x_0^2, \dots, x_0^L) \in \gV^L$ into a fully masked sequence $\vx_1 = (\mask, \mask, \dots, \mask)$ by independently masking each token according to a predefined schedule. Conversely, the reverse process defines a generative model that reconstructs a sequence by iteratively modifying a fully/partially masked sequence. Below, we formally define both the forward and reverse processes.

\subsection{Forward Process}
Given a sequence $\vx_0$ and a masking schedule $\alpha_t$, the distribution of the sequence $\vx_t$ at time $t\in[0,1]$ is expressed as:
\begin{equation}
    \begin{gathered}
        q_{t|0}(\vx_t|\vx_0) = \prod_{i=0}^{L-1} q_{t|0}(x_t^i|x_0^i), \\
        \text{where} \quad q_{t|0}(x_t^i|x_0^i) =
        \begin{cases}
        \alpha_t, & x_t^i = x_0^i, \\
        1-\alpha_t, & x_t^i = \mask.
        \end{cases}
    \end{gathered}
\end{equation}
The masking schedule $\alpha_t$ is designed such that $\alpha_0 = 1$, ensuring that the sequence remains unmasked at the start of the process. Similar to the continuous diffusion methods \citep{Ho2020Denoising, song2021denoising, Karras2022Elucidating}, we set $\alpha_1 = 0$ (or a value approaching zero), ensuring the sequence is fully masked at the end of the forward process.

\subsection{Reverse Process}
The reverse process reconstructs a sequence from a masked version by reversing the forward dynamics. Given the sequence at time $t$ and the original sequence $\vx_0$, the conditional distribution of the sequence at time $s<t$, is defined as:
\begin{equation*}
    q_{s|t,0}(x_s^i|\vx_t, \vx_0) = \frac{1-\alpha_s}{1-\alpha_t} \delta_{x_t^i}(x_s^i) + \frac{\alpha_s - \alpha_t}{1-\alpha_t} \delta_{x_0^i}(x_s^i),
\end{equation*}
where $\delta_{x}(y)$ is the Kronecker delta function. Marginalizing over $\mathbf{x}_0$ yields the true reverse process $q(\mathbf{x}_{s}|\mathbf{x}_t)$: 
\begin{equation}
\label{eq:rev_proc}
    \begin{gathered}
        q_{s|t}(\vx_s|\vx_t) = \prod_{i=0}^{L-1} q_{s|t}(x_s^i|\vx_t), \quad
        \text{where} \\ q_{s|t}(x_s^i|\vx_t) =
        \begin{cases}
        1, & x_t^i \neq \mask, x_s^i = x_t^i, \\
        \frac{1-\alpha_s}{1-\alpha_t}, & x_t^i = \mask , x_s^i = \mask, \\
        \frac{\alpha_s - \alpha_t}{1-\alpha_t} q_{0|t}(x_s^i|\vx_t), & x_t^i = \mask , x_s^i \neq \mask, \\
        0, & \text{otherwise.}
        \end{cases}
    \end{gathered}
\end{equation}
In MDM, a parameterized reverse model $p_\theta$ is often employed to approximate the distribution $q_{0|t}(x_s^i|\vx_t)$. This model is trained by minimizing the evidence lower bound (ELBO) \citep{lou2024discrete,shi2024simplified,gong2024scaling,sahoo2024simple} on the negative log-likelihood of the data distribution $q_0$.

\textbf{Inference.}  
Inference within the MDM framework entails discretizing the reverse process to iteratively reconstruct sequences from a fully masked sequence. Let $T$ denote the number of sampling steps. Starting with a fully masked sequence, the denoising process proceeds via $q_{s|t}(\vx_s \mid \vx_t)$, where $s = \frac{i}{T}$ and $t = \frac{i+1}{T}$. At each step, the model first samples $\vx_0$ from the conditional distribution $p_\theta(\vx_0 \mid \vx_t)$, followed by masking specific tokens according to $q(\vx_s \mid \vx_t, \vx_0)$. 

In practice, the reverse model is parameterized using a factorized denoising model, where the conditional distribution $p_\theta(\vx_0 \mid \vx_t)$ is expressed as:
\begin{equation}
\label{eq:parallel_sample}
    p_\theta(\vx_0 \mid \vx_t) = \prod_{i=1}^L p_\theta(x_0^i \mid \vx_t).
\end{equation}
Here, each token is predicted independently using $p_\theta(x_0^i \mid \vx_t)$, allowing for efficient parallel sampling. However, this factorized approach imposes a significant limitation: it disregards interdependencies between tokens within the sequence. As a result, the factorized model $p_\theta(\vx_0 \mid \vx_t)$ cannot exactly match the true reverse distribution $q(\vx_0 \mid \vx_t)$ \citep{xu2024energy}. In this work, we analyze the conditions under which this sampling method achieves a favorable balance between efficiency and the quality of the generated sequences.