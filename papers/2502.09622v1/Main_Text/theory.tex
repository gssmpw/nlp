\vspace{-5pt}
\section{Theoretical Analysis}
\label{sec:theory}
In image generation, the primary goal is typically to produce visually appealing and seamless images \citep{heusel2017gans}. Language generation is more task-specific. Depending on the application, the users may prefer fluent outputs, as in article writing, or precise and accurate reasonings, as in problem-solving tasks. In this section, we explore the sampling efficiency of MDMs in addressing various language tasks with respect to different evaluation metrics. 

\subsection{Notations and Problem Setting}
\label{sec:problem_setting}

Our investigation employs the hidden Markov model (HMM) framework to analyze natural language generation. This section establishes the formal notation and problem setting that underlies our subsequent analysis.

HMMs \citep{eddy1996hidden} provide a probabilistic foundation for modeling sequential data with latent structures, where observed sequences are generated by an underlying sequence of unobservable hidden states. Formally, an HMM $\gH=(\gS,\gV,\mA,\mB)$ is characterized by the following components: a finite set of hidden states $\gS = \{s_1, s_2, \dots, s_N\}$, an observable vocabulary $\gV$, a state transition probability matrix $\mA \in \mathbb{R}^{N \times N}$, an emission probability matrix $\mB \in \mathbb{R}^{N \times |\gV|}$, and an initial state distribution $\boldsymbol{\pi} \in \mathbb{R}^N$. Given a sequence of observations $\vx = (x_1, x_2, \dots, x_L) \in \gV^L$ and a sequence of hidden states $\vs = (s_1, s_2, \dots, s_L) \in \gS^L$, the generative process of an HMM is governed by the following probabilistic relations:
\begin{equation*}
    \begin{gathered}
           \Pr(s_1) = \boldsymbol{\pi}_{s_1}, \quad \Pr(x_i \mid s_i) = \mB_{s_i, x_i}, \\
           \Pr(s_i \mid s_{1:i-1}) = \Pr(s_i \mid s_{i-1}) = \mA_{s_{i-1}, s_i}.
    \end{gathered}
\end{equation*}
This formulation enables HMMs to capture both the sequential dependencies among hidden states and their probabilistic relationships with observed data. In the field of NLP, HMMs serve as the fundamental statistical tools to model natural language \citep{eddy1996hidden,marti2001using}. A notable special case of HMM is the $n$-gram language model \citep{brown1992class}, which estimates the probability of a token given its preceding \(n-1\) tokens. Despite their simplicity, $n$-gram models are foundational tools in NLP tasks \citep{brown1992class,de2010improved}. Moreover, \citet{liu2024infini} suggests that scaling up $n$-gram models can also achieve performance comparable to modern large language models.

Formally, we aim to address the following question: If MDMs have the capability to approximate a target HMM model, what are the computational costs, and do MDMs offer advantages over auto-regressive models? To evaluate the approximation quality of MDMs, we adopt two widely used metrics: \textit{TER} and \textit{SER}, which quantify different aspects of a model's performance.

\textbf{Token Error Rate.} In practice, perplexity is one of the most widely used metrics for evaluating token-level errors in language models. It quantifies the uncertainty of a model in predicting the next token in a sequence and serves as a standard measure for assessing the quality of text generation. In this paper, we define the TER by perplexity. Models with lower TER are generally considered more effective at generating fluent and coherent text. Formally, given a ground-truth language model $q$ and an evaluated model $p$, the TER is computed as:
\begin{equation}
    \operatorname{TER}(p) = 2^{\mathbb{E}_{\vx \sim q} \left[ -\frac{\log (p(\vx))}{|\vx|} \right]}.
\end{equation}

\textbf{Sequence Error Rate.} The SER evaluates the correctness of an entire sequence rather than individual tokens. Let $q$ represent a target language defined over a vocabulary $\gV$, and let $\gL_q = \{\vx \in \gV^* \mid q(\vx) > 0\}$ denote the support set of distribution $q$. For a generative model $p$, the SER is defined as:
\begin{equation}
\label{eq:SER_def}
    \operatorname{SER}(p) = 1 - \sum_{\vx \in \gL_q} p(\vx).
\end{equation}
This metric quantifies the probability that the model generates sequences falling outside the support set of the ground-truth distribution.

Compared to TER, SER imposes a stricter evaluation criterion by requiring the correctness of entire sequences. This makes SER particularly well-suited for tasks that demand logical consistency or reasoning, where the correctness of the complete reasoning chain is crucial. 
\subsection{MDMs Can Generate Low-TER Sentences Efficiently}
\label{sec:positive}

%Although parallel sampling (\cref{eq:parallel_sample}) in MDM inference introduces a known distributional mismatch with respect to ground-truth data \citep{xu2024energy}, the practical implications of this divergence in sampling quality remain insufficiently explored. The inherent sparsity and ambiguity of natural language suggest that such mismatches may not necessarily manifest as perceptible inconsistencies in the generated text. 
%During inference, MDMs predict multiple tokens independently at each step. As a result, it is conventional wisdom that this parallel sampling enhances the efficiency of generation. However, there are limited works examining the impact of parallel sampling on the generating quality. 
In this subsection, we rigorously examine the efficiency of sampling in MDMs, demonstrating that MDMs are capable of efficiently generating sentences with near-optimal TER. To establish the main theoretical results, we assume that the MDMs have enough expressive power and begin with the following assumption:

\begin{assumption}[Learning with Small Error]
\label{ass:perfect_learning}
    Let $q$ denote the target language model with vocabulary $\gV$, and let $p_\mathbf{\theta}$ represent the reverse model trained to approximate the reverse process generating the target language under a masking schedule $\alpha_t$. Assume there exists $\epsilon_\text{learning} > 0$ such that the KL divergence between $p_\mathbf{\theta}$ and the reverse process distribution generating the language $q$ is bounded by $\epsilon_\text{learning}$, i.e.,
    \begin{equation*}
        \DKL{q_{0|t}(x_0^i \mid \vx_t)}{p_\mathbf{\theta}(x_0^i \mid \vx_t)} < \epsilon_\text{learning}, \quad \forall\ t \text{ and } \vx_t.
    \end{equation*}
\end{assumption}

It is worth noting that $p_\mathbf{\theta}(x_0^i \mid \vx_t) = q_{0|t}(x_0^i \mid \vx_t)$ represents the optimal solution to the ELBO loss during training. \cref{ass:perfect_learning} implies that the MDM model is well-trained and approximates the ground-truth distribution with only a small error.

During MDM inference, the time interval $[0, 1]$ is discretized into $N$ steps, where $t_i = \frac{i}{N},\ i \in [N]$, and iteratively reconstruct sequences from a fully masked sequence. The following theorem shows that the sequence distribution generated by the reverse process, even with a small number of sampling steps, can achieve near-optimal TER. Consequently, MDMs exhibit high efficiency in generating $n$-gram language.
\begin{theorem}[TER Bounds for $n$-Gram Language Generation]
\label{thm:acceleration_ngram}
    For any $n$-gram language $q$ and any $\epsilon > 0$, let $p_\mathsf{\theta}$ denote the reverse model and $L$ denote the sequence length. The distribution over sequences generated by $p_\mathsf{\theta}$ is denoted as $p$. For any $L>O\big( \frac{n-1}{\epsilon^{n+0.5}}\big)$, under \cref{ass:perfect_learning}, there exists a masking schedule $\alpha_t$ such that, with $N = O\big( \frac{n-1}{\epsilon^n}\big)$ sampling steps, the TER of the MDM is upper-bounded by:
    \begin{equation}
        \begin{gathered}
            \log\operatorname{TER}(p) \leq \log\operatorname{TER}(q) + \epsilon_\text{learning} + 4\epsilon\log |\gV|. \\
        \end{gathered}
    \end{equation}
\end{theorem}

The proof of this theorem is presented in \cref{app:positive}.

\cref{thm:acceleration_ngram} demonstrates that MDMs can efficiently generate sentences with high fidelity. It is notable that for a given data distribution $q$, the TER of a language model $p$ achieves its global minimum when $p = q$. To ensure a gap of at most $\epsilon$ with the optimal TER during sampling, the number of required sampling steps is bounded by $O\big( \frac{n-1}{\epsilon^n}\big)$. 

The above results suggest that to achieve near-optimal TER, MDMs require only a number of sampling steps that is independent of the sequence length $L$.
In each sampling step, the neural network model, i.e., a Transformer, is executed once. Therefore, informally, the neural network execution count is constant for MDM. This offers substantial efficiency gains over auto-regressive models, where the model must be executed $L$ times, once for each token in the sequence. Such efficiency enables MDMs to handle long-sequence generation tasks effectively while maintaining high-quality outputs.

\subsection{MDMs Cannot Generate Low-SER Sentences with A Low Cost}
\label{sec:negative}

In this subsection, we examine the SER of sampling in MDMs and highlight a fundamental limitation of MDMs in generating logically rigorous language. We begin by establishing that, with sufficient sampling steps, the MDMs have the capability to approximate a target HMM model with perfect SER. 


\begin{theorem}[Accurate Generation of HMM with Sufficient Steps]
\label{thm:pos_hmm}
    Let $q$ denote any HMM, and let $p_\mathsf{\theta}$ represent the reverse model under an arbitrary masking schedule, where $L$ is the sequence length. Let $p$ denote the distribution over sequences generated by $p_\mathsf{\theta}$. Under \cref{ass:perfect_learning} with a learning error $\epsilon_\text{learning} < O(\frac{\delta}{L})$, and given a sufficient number of reverse steps, the sequence error rate $\operatorname{SER}(p)$ of the generated text satisfies 
    \[
    \operatorname{SER}(p) \leq  \delta.
    \]
\end{theorem}

The complete proof of \cref{thm:pos_hmm} is detailed in \cref{app:proof_hmm_pos}. While this result establishes the theoretical capability of MDMs to achieve low SER, we still need to estimate the computational cost to achieve it. The following theorem provides a negative result for this problem.


\begin{theorem}[SER Bound for HMM Generation]
\label{thm:negative}
    There exists an HMM $q$ over a vocabulary of size $16$ that satisfies the following conditions: for any reverse model $p_\mathsf{\theta}$ under \cref{ass:perfect_learning} with $\eps_\mathrm{learning}<\frac{1}{128}$, and any masking schedule $\alpha_t$, let $p$ denote the distribution over sequences generated by $p_\mathsf{\theta}$. There exists a constant $C$ such that if the number of sampling steps satisfies $N = CL$, where $L$ is the sequence length, the SER of the generated text is lower-bounded by:
    \begin{equation*}
        \operatorname{SER}(p) > \frac{1}{2}.
    \end{equation*}
\end{theorem}



The proof is presented in \cref{app:proof_neg}.

\cref{thm:negative} shows that to generate sequences with low SER, the number of sampling steps in MDMs must scale at least linearly with the sequence length $L$, indicating that the number of neural network executions is comparable between MDMs and autoregressive models. However, this scaling law of MDMs typically leads to much higher computational costs compared to autoregressive models. For instance, in the case of Transformer-based architectures, each execution step in MDMs involves a quadratic computational complexity in terms of $L$, as opposed to the linear complexity of auto-regressive Transformer models in each generation step (through reusing the stored KV caches). Consequently, in accuracy-critical applications, MDMs offer no computational efficiency advantage over auto-regressive models. 

Furthermore, some prior works \citep{sahoo2024simple,ou2024your} have proposed efficient sampling strategies that reuse cached outputs without requiring additional forward passes through the network when no token is modified from $\mask$ at a given step. Nevertheless, our theoretical results remain applicable to these sampling strategies, as discussed in \cref{app:ddpm_cache}.


\textbf{Do TER and SER Conflict?} The above results reveal that MDMs can efficiently generate low-TER sentences but may incur higher costs when evaluating the generation under SER. One might think these results are contradictory. Note that several previous works have already shown that TER (a.k.a perplexity) may not reflect a model's true performance in solving several long-sequence understanding tasks~\citep{Huang2022OnTL,hu2024perplexityreflectlargelanguage,luden2024beyond}. Thus, it is natural to arrive at different conclusions depending on the metric used. 

Moreover, many practical scenarios have shown that the choice of evaluation metric significantly influences the conclusion of other problems. For instance, while the community has previously focused on the emergence phenomenon, recent works by \citet{wei2022emergent} and \citet{schaeffer2024emergent} demonstrate that this phenomenon may stem from the use of non-smooth evaluation metrics. Our work further reveals that conclusions regarding the efficiency of MDMs depend heavily on the evaluation metric employed. Specifically, MDMs excel in applications where fluency is prioritized. In contrast, for reasoning-intensive tasks that demand highly accurate trajectories, MDMs may fail to offer a significant efficiency advantage over auto-regressive models.
%例如之前人们关注涌现现象，发现突然模型就学会了，但最近也有文章表明涌现现象的根源是用了non-smooth 的metric。
%我们的工作揭示

