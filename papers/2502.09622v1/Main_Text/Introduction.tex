\section{Introduction}
\label{sec:intro}

Diffusion models \citep{Ho2020Denoising,Song2020ScoreBased} have emerged as a powerful paradigm in generative modeling, establishing state-of-the-art performance in image synthesis \citep{Karras2022Elucidating,song2021denoising}. Their extension to discrete domains has opened new possibilities for generating sequences, such as natural language \citep{Campbell2022ACT,Dieleman2022Continuous,Zheng2023ARD,lou2024discrete,campbell2024generative,lovelace2024diffusion} and biological sequences \citep{rastogi2022semi,vignac2022digress,sun2023difusco,avdeyev2023dirichlet}. Among various discrete diffusion architectures, masked diffusion models (MDMs) \citep{shi2024simplified,sahoo2024simple,ou2024your}—which generate sequences by iteratively converting masks to tokens—have emerged as a prominent approach and demonstrated competitive performance across diverse language modeling tasks.


%line9改成这部分内容 和ar的token by token对比，diffusion天然具备一次并行生成多个token的特点，因此人们普遍认为diffusion是ar一个efficient的替代品。但是in this paper，我们argue当考虑efficiency的时候，我们需要同时考虑生成的质量，即，我们应该关心，生成同样质量的内容，diffusion是否会比ar效率更高呢，这个问题可能有多个答案，比如，如果生成同样质量的内容，diffusion只需要excute更少次数的neural network model，那么它有加速效果，但是如果它针对一些困难任务需要调用的次数相当甚至超过ar，那么diffusion并不是一个好的选择
While auto-regressive models generate sequences token-by-token, discrete diffusion models can generate multiple tokens simultaneously during each step (reverse process). Therefore, it is natural to hypothesize that this parallel sampling improves generation efficiency. However, we argue that reaching a conclusion requires considering both computational cost and generation quality. Specifically, we pose the following question: do discrete diffusion models achieve superior efficiency when the generated content meets an acceptable quality standard? There may be multiple answers to this question. If diffusion models require fewer neural network executions while maintaining quality, they can offer better acceleration. Conversely, if their execution count is comparable to or exceeds that of auto-regressive models, diffusion language models may not be a better choice.

%line 13后面加上我们选择那些metric有什么意义
To answer the above question, we leverage two complementary metrics to evaluate the efficiency of MDMs in language modeling. The first metric, \textit{token error rate} (TER), quantifies token-level accuracy, which correlates with the fluency of the generated text. In practice, \textit{perplexity} is a widely used metric for measuring token-level errors of language models \citep{jelinek1977perplexity,Devlin2019BERT}; thus, we define the metric of TER by perplexity in this paper. The second metric, \textit{sequence error rate} (SER), evaluates the correctness of an entire sequence, which is crucial for reasoning tasks requiring logically correct sequences. We provide a natural definition of SER that reflects the correctness of the whole sequence. Together, these metrics enable a comprehensive evaluation of the efficiency of MDMs under both token-level and sequence-level metrics. We first provide a positive theoretical result regarding TER. We prove that under mild conditions, MDMs can achieve near-optimal TER with sampling steps regardless of the sequence length $L$. Compared to the auto-regressive model, which must be executed $L$ times to generate the sequence, MDMs demonstrate substantial efficiency gains, especially when the generation length is long.


However, we show that this efficiency advantage diminishes when SER is considered. We theoretically prove that to achieve a low SER, the number of required sampling steps for MDMs scales at least linearly with sequence length. Intuitively, this limitation arises from the fact that SER, as a metric for the entire sequence, requires the generated sequence to be free of any error in the whole sequence, which forces MDMs to sample only a small number of tokens per step to mitigate such inconsistencies. As a result, the number of required sampling steps can be significant. It is notable that each MDM sampling step usually incurs a higher computational cost than an auto-regressive step under the same architecture, thus MDMs offer no efficiency advantage under this metric. 




Finally, we validate our theoretical findings through comprehensive experiments. Our experiments examine MDMs trained on formal languages, including $n$-gram languages and Hidden Markov Models (HMMs), systematically analyzing the relationship between performance and efficiency under both TER and SER metrics. Additional experiments on natural language tasks, including TER evaluation on text generation and SER assessment on the GSM8k dataset \citep{cobbe2021gsm8k}, corroborate our theoretical predictions: while achieving low SER necessitates substantial sampling steps, relatively few steps suffice for TER. These results provide practical guidance for deploying diffusion language models across different applications.

% Discrete diffusion models offer several advantages over traditional auto-regressive approaches, including enhanced flexibility, computational efficiency, and controllability. 
%In principle, discrete diffusion models achieve efficiency through parallel sampling, where multiple tokens are sampled simultaneously at each step of the reverse process \citep{Li2022DiffusionLM,lou2024discrete,xu2024energy}. However, this parallel sampling ignores the interdependencies between tokens within a sequence, leading to a discrepancy between the generated distribution and data distribution. While this mismatch is a well-recognized challenge in the community, its precise impact on the quality of the generated sequences remains underexplored. 

\section{Related Work}

\textbf{Discrete Diffusion Models. }  
The auto-regressive paradigm has achieved significant success in language modeling \citep{dai2019transformer,floridi2020gpt,achiam2023gpt}. However, its left-to-right, token-by-token generation approach is not without limitations. Notably, it faces challenges such as restricted controllability \citep{zhang2023tractable} and inefficiencies in inference speed \citep{leviathan2023fast}. To overcome these drawbacks, inspired by the success of diffusion models in image generation \citep{SohlDickstein2015Deep,song2021denoising,Karras2022Elucidating} researchers have adapted these techniques for NLP tasks \citep{austin2021structured,He2022DiffusionBERT,chen2022analog,Meng2022Concrete,ye2023diffusion,Gulrajani2023LikelihoodBased,zhang2024language}. Discrete diffusion models, in particular, have shown promising results, achieving comparable performance with auto-regressive models across a range of NLP benchmarks.  

Discrete diffusion models can be categorized based on the initialization strategy of the reverse process: (1) reverse processes that begin with masked sequences and (2) reverse processes that start with sequences of tokens sampled randomly from the vocabulary. The first category, termed \emph{masked diffusion models} (MDMs), includes models such as SEDD Absorb \citep{lou2024discrete} and its streamlined variants in subsequent works \citep{sahoo2024simple,zhao2024improving,shi2024simplified,ou2024your,zheng2024masked}. The second category encompasses models like SEDD Uniform \citep{lou2024discrete}, as well as extensions introduced in follow-up studies \citep{campbell2024generative}. Notably, \citet{gat2024discrete,davis2024fisher} and \citet{campbell2024generative} further extend flow-matching to the discrete domain, with differing initialization strategies: the former employs masked sequences, while the latter utilizes a customized distribution for the reverse process.


\textbf{Masked Diffusion Models. }  
Among the two primary classes of discrete diffusion models, MDMs have consistently demonstrated superior performance and scalability \citep{lou2024discrete,campbell2024generative}. For instance, in \citet{lou2024discrete}, the masked variant of SEDD significantly outperforms its uniform counterpart across a range of benchmarks. Similarly, \citet{campbell2024generative} reports that the masked variant achieves better results in most language tasks. Furthermore, recent advancements have successfully scaled MDMs to over 1 billion parameters \citep{gat2024discrete,nie2024scaling,gong2024scaling,shi2024simplified}, underscoring their robustness and adaptability to large-scale NLP models. In this paper, we focus on MDMs, and our theoretical contributions can be applied to all MDMs, including the masked variant of discrete flow matching.


\textbf{Various Metrics in NLP Tasks.}  
Evaluation metrics in NLP tasks are inherently tied to the specific objectives and requirements of their respective domains. For general language modeling tasks, perplexity \citep{jelinek1977perplexity,Devlin2019BERT} remains the metric of choice due to its ability to capture a model's predictive performance effectively. However, domain-specific tasks often demand more specialized evaluation criteria. For instance, in machine translation \citep{bahdanau2014neural,wu2016google}, the BLEU score is widely regarded as a standard measure of translation quality \citep{papineni2002bleu}, while text generation tasks \citep{sutskever2014sequence} frequently rely on metrics such as ROUGE to assess output fidelity \citep{lin2004rouge}. Similarly, tasks requiring reasoning \citep{wei2022chain}, such as mathematics \citep{bubeck2023sparks} or code generation \citep{roziere2023code,ouyang2023llm}, commonly adopt accuracy as an intuitive and straightforward measure of success.
\vspace{-2pt}

