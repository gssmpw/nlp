
\section{Experiments}
\vspace{5pt}
\label{sec:experiments}
\begin{figure*}[h]
    \centering
    \begin{minipage}{1\textwidth}
    \centering
        \includegraphics[width=0.98\linewidth,height=0.3\linewidth]{Figures/ngram-ppl.pdf}
    \end{minipage}
    \begin{minipage}{1\textwidth}
    \centering
        \includegraphics[width=0.98\linewidth,height=0.3\linewidth]{Figures/ngram-acc.pdf}
    \end{minipage}
    \vspace{-15pt}
    \caption{Sampling Efficiency and Quality of MDMs on Formal Languages: The above subfigure illustrates generative perplexity of generated sequences versus the number of sampling steps for $n$-gram languages ($n \in {2, 3, 4}$) and HMM. The y-axis represents the generative perplexity, and the x-axis represents the sampling steps, with the last point indicating the performance of auto-regressive models. The figure below shows the SER of generated sequences versus the number of sampling steps for the same formal languages. The y-axis represents the SER, while the x-axis is the same as the above figure. The number above each bar indicates the speedup of MDMs under different sampling steps compared to the auto-regressive models. }
    \label{fig:synth_data}
    \vspace{-15pt}
\end{figure*}


We conducted a series of experiments to empirically validate the theoretical findings, focusing on evaluating the sampling quality and computational efficiency of MDMs under diverse metrics. The results reveal that while MDMs effectively generate low-TER sequences, but achieving low-SER demands substantial computational resources. We will first introduce our experimental settings and then present the experimental results.



\subsection{Experimental Setup}



\looseness=-1\textbf{Tasks and Datasets.}  
First, we evaluated MDMs on a variety of formal languages, including $n$-gram languages ($n \in \{2, 3, 4\}$) and HMMs. For each formal language, parameters such as transition matrices, observation matrices, and initial distributions were generated through random sampling. Detailed descriptions of the parameter generation process, along with illustrative examples of the resulting sequences, are provided in \cref{app:data}. Using these formal languages, we constructed datasets comprising $1{,}000{,}000$ samples, of which $990{,}000$ were allocated for training and $10{,}000$ for validation. When using the formal language models to generate the dataset, we set the max length of 512.

\begin{figure*}[!t]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/uncond.png}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{Figures/gsm8k.png}
    \end{minipage}
    \vspace{-10pt}
    \caption{Evaluation on Language Tasks: The left subfigure illustrates the text generation quality of MDLM-OWT across different sampling steps, with GPT2-medium as baseline. The y-axis represents the average generative perplexity of 2000 generated texts, and the x-axis indicates the number of sampling steps. The numbers above indicate the speedup of MDLM-OWT under different sampling steps compared to GPT2-medium. The right subfigure shows the accuracy of MDM on the GSM8K benchmark at different sampling steps, with  Qwen-Math-1.5B as baseline. The y-axis indicates accuracy, and the x-axis represents the number of sampling steps.}
    \label{fig:language_data}
    \vspace{-15pt}
\end{figure*}

%先放上来看看效果

\textbf{Model Training.}  
We adopted transformer-based architectures as the backbone models due to their scalability and expressiveness in sequence modeling tasks. Comprehensive architectural details, including the number of layers, hidden dimensions, and positional encoding schemes, are provided in \cref{tab:model_config} in \cref{app:train}. The training process followed the framework proposed by \citet{sahoo2024simple}, with additional training configurations detailed in \cref{tab:training_config}. Models were trained for 20 epochs, and their convergence was monitored using the validation set. Perplexity was used as the primary convergence metric, and the trained models achieved optimal perplexity values consistent with the ground-truth language models that generated the datasets.

\looseness=-1\textbf{Evaluation Metrics.}  To assess the quality of generated sequences, we used TER and SER as the primary evaluation metrics, in alignment with our theoretical framework. Computational efficiency was evaluated based on the number of sampling steps. Following prior work \citep{lou2024discrete,xu2024energy}, generative perplexity was employed as the TER metric to evaluate the sample qualities under different sampling steps. We compute the generative perplexity using the ground-truth model to evaluate the likelihood of sequences generated by MDMs, which were subsequently converted into perplexity scores. SER was computed directly using its definition in \cref{eq:SER_def}, leveraging ground-truth models for evaluation. For sequence generation, we utilized the \verb|ddpm_cache| sampler proposed in prior work \citep{sahoo2024simple}, ensuring efficient sampling. Computational efficiency was measured by the number of sampling steps, and we further discuss the influence of \verb|ddpm_cache| under different sampling steps in \cref{app:ddpm_cache}. Furthermore, we also test the true speedup of MDMs under different sampling steps compared to the auto-regressive models in \cref{fig:synth_data}, the detailed testing settings are listed in \cref{app:train}. To ensure robust evaluation, we generated $2000$ sequences for each setting and computed both TER and SER over these samples.


To compare MDMs with auto-regressive models, we trained auto-regressive models with identical architectures and model sizes on the same datasets generated by the formal languages. These models were evaluated under the same metrics, serving as a baseline for performance comparison. The training configurations are provided in \cref{tab:training_config_AR}.


\subsection{Experiment Results}

The experiment results are presented in \cref{fig:synth_data}. The upper subfigure shows the generative perplexity across different formal languages with the number of sampling steps varying. The x-axis represents the number of sampling steps, ranging from 8 to 2048, while the y-axis measures the generative perplexity, where lower values indicate higher text fluency and token-level accuracy. The performance of auto-regressive models is marked as the final point on the x-axis for comparison. As shown in the figure, MDMs achieve near-optimal generative perplexity with relatively few sampling steps. To achieve a perplexity similar to the auto-regressive model, MDMs only require about 64 steps and demonstrate 1.57 times speedup compared to auto-regressive models. This demonstrates that MDMs are highly efficient at generating fluent sequences even with a small number of sampling steps. As the number of sampling steps increases, the performance of MDMs approaches that of auto-regressive models, converging to a similar level of generative perplexity.

The lower subfigure evaluates the relationship between the number of sampling steps and the SER, which measures the correctness of an entire sequence. The x-axis again represents the number of sampling steps, with the performance of auto-regressive models included as a baseline, and the y-axis measures the SER, where lower values indicate higher sequence-level accuracy. Compared to the upper subfigure, this subfigure reveals a slower improvement in SER as the number of sampling steps increases. For these formal languages, achieving low SER requires significantly more sampling steps. Moreover, even when the number of sampling steps reaches 2048, there remains a gap in SER between MDMs and auto-regressive models. These results demonstrate that auto-regressive models maintain a clear advantage in SER, as their token-by-token generation achieves zero SER across these tasks.

\Cref{fig:synth_data} highlights the trade-off between efficiency and accuracy for MDMs empirically. While MDMs excel in generating fluent outputs with low TER, they require substantially more sampling steps to achieve low SER, particularly for reasoning-intensive tasks that demand sequence-level correctness. These experimental results further reinforce our theoretical findings.


\section{Preliminary Experiments on Large Models}

We further conducted an extensive set of experiments on language tasks using open-source MDMs. First, we evaluated the quality of text generation by measuring the generative perplexity of MDMs using MDLM-OWT \citep{sahoo2024simple}, a diffusion language model trained on OpenWebText \citep{Gokaslan2019OpenWeb}. For a fair comparison, we evaluated GPT2-medium \citep{radford2019language}, which is similar in size. Second, we explored the mathematical reasoning ability of MDMs on the GSM8K dataset \citep{cobbe2021gsm8k}. Given that small models typically exhibit poor reasoning performance, we used a fine-tuned diffusion language model with 1.1B non-embedding parameters proposed by \citet{nie2024scaling}, and compared it against model with a similar number of parameters. While the generative perplexity represents the metric of TER, mathematical reasoning is more concerned with the correctness of the entire generated sequence, thus, the GSM8K accuracy is partially consistent with the negative sequence error rate $-\SER$.


\textbf{Text Generation.}  
For text generation, we use MDLM-OWT, which has a context length of 1024 and size similar to GPT2-medium and is trained on OWT dataset \citep{Gokaslan2019OpenWeb}. Since our goal is to compare the acceleration of MDMs relative to auto-regressive models and examine the effect of the number of steps on text generation quality, the absolute size and capability of the model are less important. Following the approach in the original work, we used the \verb|ddpm_cache| sampler and the GPT2 tokenizer. For the number of sampling steps ranging from 4 to 2048, we generated 2000 samples of length 1024 and evaluated the generative perplexity using GPT2-large. To compare MDMs with auto-regressive models, we took GPT2-medium as baseline and computed its generative perplexity in the same manner.

\looseness=-1 The experiment result is shown in the left subfigure of \cref{fig:language_data}, which illustrates the text generation quality of MDLM-OWT across different sampling steps, with GPT2-medium as the baseline. The the x-axis represents the number of sampling steps, and the y-axis represents the average generative perplexity of 2000 generated texts, where lower generative perplexity indicates higher fluency and, consequently, a lower TER. The numbers above indicate the speedup of MDLM-OWT under different sampling steps compared to GPT2-medium. As is shown in the figure, MDLM-OWT matches the generative perplexity of GPT2-medium with only 32 steps, where there is a 2.28x speedup, and the perplexity continues to decline and converge as the number of sampling steps increases. This demonstrates that MDMs can generate texts efficiently while ensuring a high fluency, which illustrates the potential of MDMs for basic language generation tasks at a larger scale.

%The left figure illustrates the unconditional text generation quality of MDLM-OWT across different sampling steps, with GPT2-medium as the baseline. The y-axis represents the average generative perplexity of 2000 generated texts, and the x-axis indicates the number of sampling steps. The right figure shows the accuracy of MDM on the GSM8K benchmark at different sampling steps, with Qwen1.5-1.8B, Qwen2-1.5B and Qwen-Math-1.5B as the baselines. The y-axis indicates accuracy, and the x-axis represents the number of sampling steps.

\textbf{Mathematical Reasoning.}  
For mathematical reasoning, we used the MDM provided by \citet{nie2024scaling}, which was fine-tuned on GSM8K using a model trained on SlimPajama \citep{cerebras2023slimpajama} for $3.3\times 10^{21}$ training FLOPs with 1.1B non-embedding parameters. This is so far the first MDM to be fine-tuned on mathematical reasoning tasks. % 存疑，如果不是的话就删掉这句
We generated answers with a maximum length of 256 for the number of sampling steps ranging from 1 to 256. 
Since there are very few models fine-tuned on GSM8K at the same scale, we took Qwen2-Math-1.5B \citep{yang2024qwen2} as our baseline. We evaluated its performance following the widely used Language Model Evaluation Harness framework \citep{eval-harness}, 
%8-shot performance as suggested in \citet{qwen}, 
and counted the average length of the generated answers, which partly reflects the efficiency of the model.
% Since there are very few models fine-tuned on GSM8K at the same scale, we take the GSM8K accuracy of Qwen-1.8B, Qwen2-1.5B and Qwen2-Math-1.5B reported in \citet{qwen, yang2024qwen2} as our baseline.

The experiment results are presented in the right subfigure of \cref{fig:language_data}. For all tested step numbers, the average length of generated answers for MDM is around 30, while for the baseline model, the average length is about 105. Unlike text generation, MDM does not show a significant advantage over auto-regressive models for mathematical reasoning tasks. The accuracy of the MDM decreases sharply as the number of steps falls below the sequence length, and it shows only a slight improvement as the number of samples exceeds the sequence length. While the latter may be due to the limitations of the MDM we used, the former likely caused by insufficient sampling, which leads to a high sequence error rate. It is worth noting that the experimental setup for the MDM differs from that of the baseline models, so the baseline accuracy is provided only for reference.

\textbf{Summary.}  \cref{fig:language_data} illustrates the performance of MDMs on language tasks and and its dependence on sampling steps. For text generation, MDLM-OWT achieved similar performance to GPT2-medium with few sampling steps, demonstrating efficiency in generating fluent text. On the contrary, MDMs showed no significant advantage on the GSM8K accuracy, with performance declining rapidly when the number of steps fell below the sequence length. These results highlight MDMs’ ability in text generation, but suggest challenges in reasoning-relevant tasks.


\section{Conclusion and Limitations}

\textbf{Conclusion. } This paper provides a rigorous theoretical and empirical analysis of the efficiency of MDMs under various metrics. We demonstrate that MDMs can achieve near-optimal TER with a fixed number of sampling steps, regardless of sequence length, making them highly efficient for tasks emphasizing fluency. However, when evaluated using SER, MDMs require sampling steps that scale linearly with sequence length, negating their efficiency advantage over auto-regressive models. These findings highlight the trade-off between efficiency and accuracy, depending on the evaluation metric. Experimental results further reinforce our theoretical results across formal and natural language tasks, offering practical guidance for deploying MDMs. While MDMs demonstrate efficiency advantages in applications prioritizing fluency, they may fall short in reasoning-intensive tasks requiring high accuracy compared to auto-regressive models.

\textbf{Limitations. } 
Our study focuses on formal languages modeled using HMM, which, while foundational, still differs from modern language models. Extending this analysis to more advanced language models remains an important direction for future work. Additionally, we primarily analyze Masked Diffusion Models, but the broader family of diffusion-based language models, including variants like SEDD-unform \citep{lou2024discrete}, requires further investigation. In summary, while our work establishes a theoretical understanding of MDMs, further exploration is needed to 
generalize our findings to real-world settings
% bridge the gap with modern language models
and to systematically analyze other diffusion approaches.


\section*{Impact Statement}
This paper presents work whose goal is to advance the field of generative models. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here.