
\section{Experiment Details}
\label{app:exp_detail}

In this section, we will present the details of the experiments.
\vspace{-10pt}
\subsection{Data Generation}
\label{app:data}
We evaluate the MDMs in a variety of formal languages, including $n$-gram languages and HMMs. For each formal language, parameters are generated through random sampling, we present the sampling algorithm in \cref{alg:hmm4ngram} and \cref{alg:hmm4hmm}. It is notable that to add some deterministic to the language model in the evaluation of SER, we add the parameter of \texttt{thres} to prune the tail probabilities, making sure the language model only generates the correct sequence. For the evaluation of TER, we set the \texttt{thres} to be $0$, for the well definition of generative perplexity. The detailed parameters to generate the formal languages are listed in \cref{tab:language-params}.

\begin{algorithm}[H]
\caption{Generate $n$-gram Language Model}
\label{alg:hmm4ngram}
\textbf{Input}: \\
\quad $n$: number of grams \\
\quad $\text{vocab\_size}$: size of vocabulary \\
\quad $\text{temp}$: temperature (controls randomness, higher indicates more randomness) \\
\quad $\text{thres}$: threshold for pruning small probabilities \\
\textbf{Output}: $n$-gram language model with parameters: \\
\quad $T$: transition probability matrix ($\text{vocab\_size}^{n-1} \times \text{vocab\_size}$) \\
\quad $\text{Init\_dist}$: initial state distribution
\begin{algorithmic}[1]

\STATE $\text{Init\_dist} \gets \text{rand}(\text{hidden\_states\_num})$
\STATE $\text{Init\_dist} \gets \text{Init\_dist} / \sum(\text{Init\_dist})$

\STATE $T \gets \text{randn}(\text{vocab\_size}^{n-1}, \text{vocab\_size}) \times \text{randomness}$ 
\STATE $T \gets \softmax(T)$
\IF{$\text{thres} > 0$}
    \STATE $T[\text{where}(T < \text{thres})] \gets 0$
    \STATE $T \gets T / \text{rowsum}(T)$
\ENDIF

\RETURN $T$ and Init\_dist

\end{algorithmic}
\end{algorithm}

\vspace{-20pt}
\begin{table}[H]
\caption{Generation Parameters for Different Language Models}
\label{tab:language-params}
\begin{center}
\begin{tabular}{lcccc}
\hline
\textbf{Parameter} & \textbf{2-gram} & \textbf{3-gram} & \textbf{4-gram} & \textbf{HMM} \\
\hline
vocabulary size & 8 & 8 & 8 & 8 \\
Hidden States ($n$) & N/A & N/A & N/A & 32 \\
Temperature & 2 & 2 & 2 & 3.2 \\
Threshold & 0.008 & 0.008 & 0.005 & 0.003 \\
\hline
\end{tabular}
\end{center}
\end{table}


\begin{algorithm}[H]
\caption{Generate Hidden Markov Model}
\label{alg:hmm4hmm}
\textbf{Input}: \\
\quad $n$: number of hidden states \\
\quad $\text{vocab\_size}$: size of vocabulary \\
\quad $\text{randomness}$: temperature parameter to control probability distributions \\
\quad $\text{thres}$: threshold for pruning small transition probabilities \\
\textbf{Output}: HMM with parameters: \\
\quad $A$: state transition matrix ($n \times n$) \\
\quad $B$: emission probability matrix ($n \times (\text{vocab\_size}+1)$) \\
\quad $\text{Init\_dist}$: initial state distribution ($n$-dimensional)
\begin{algorithmic}[1]
\STATE $\text{hidden\_states\_num} \gets n$
\STATE $\text{Init\_dist} \gets \text{rand}(\text{hidden\_states\_num})$
\STATE $\text{Init\_dist} \gets \text{Init\_dist} / \sum(\text{Init\_dist})$

\STATE $A \gets \text{randn}(\text{hidden\_states\_num}, \text{hidden\_states\_num}) \times \text{randomness}$
\STATE $A \gets \softmax(A)$

\IF{$\text{thres} > 0$}
    \STATE $A[\text{where}(A < \text{thres})] \gets 0$
    \STATE $A \gets A / \text{rowsum}(A)$
\ENDIF

\STATE $B \gets \text{randn}(\text{hidden\_states\_num}, \text{vocab\_size}) \times \text{randomness} \times 2.5$
\STATE $B \gets \softmax(B)$
\STATE $B[\text{where}(B < 0.05)] \gets 0$
\STATE $B \gets B / \text{rowsum}(B)$

\STATE $B \gets \text{concat}(B, \text{ones}(\text{hidden\_states\_num}, 1) / \text{hidden\_states\_num})$

\RETURN $A, B, \text{and Init\_dist}$
\end{algorithmic}
\end{algorithm}




\subsection{Model Training and Testing}
\label{app:train}
In our experiments of formal languages, all training was conducted on NVIDIA A100 GPUs. The model architectures and train configurations are listed in \cref{tab:model_config} and \cref{tab:training_config}. The training configuration of the auto-regressive model is listed in \cref{tab:training_config_AR}.
\begin{table}[H]
    \centering
    \begin{tabular}{ll}
      \toprule
    {\centering \textbf{Model Configuration} }\\
    \midrule
        Hidden Size & 512 \\
        Sequence Length & 512 \\
        Number of Layers & 8* \\
        Attention Heads & 8 \\
        \bottomrule
        \multicolumn{2}{l}{*For the 4-gram model, the number of layers is 10.}
    \end{tabular}

    \caption{Model Configuration for the Formal Language Tasks}
    \label{tab:model_config}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
      \toprule
    {\centering \textbf{Training Configuration for MDMs} }\\
    \midrule
        Epochs & 20\\
        Learning Rate & 3e-4 \\
        Optimizer & AdamW \\
        $\beta_1$ & 0.9 \\
        $\beta_2$ & 0.999 \\
        Learning Rate Scheduler & Cosine Scheduler with Warmup \\
        Warmup Ratio & 0.1 \\
        \bottomrule
    \end{tabular}
    \caption{Training Configuration for MDMs on the Formal Language Tasks}
    \label{tab:training_config}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
      \toprule
    {\centering \textbf{Training Configuration for Auto-regressive Models} }\\
    \midrule
        Epochs & 20\\
        Learning Rate & 3e-4 \\
        Optimizer & AdamW \\
        $\beta_1$ & 0.9 \\
        $\beta_2$ & 0.999 \\
        Learning Rate Scheduler & Cosine Scheduler with Warmup \\
        Warmup Ratio & 0.1 \\
        \bottomrule
    \end{tabular}
    \caption{Training Configuration for Auto-regressive Models on the Formal Language Tasks}
    \label{tab:training_config_AR}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{ll}
      \toprule
    {\centering \textbf{Speedup Testing Settings} }\\
    \midrule
        GPU & Nvidia RTX 4090\\
        Batch Size & 16 \\
        Sequence Length & 512\\
        Testing Model Configuration & In \cref{tab:model_config}\\
        \bottomrule
    \end{tabular}
    \caption{Setting for the experiments to test the speedup of MDMs under different sampling steps compare to auto-regressive models.}
    \label{tab:speed_up}
\end{table}
