
\section{Proof for \cref{thm:pos_hmm} and \cref{thm:negative}}
\subsection{Proof for \cref{thm:pos_hmm}}
\label{app:proof_hmm_pos}
In this section, we aim to derive the upper bound for the $\SER$ of generated sequences with sufficient reverse steps. First, we argue that, given a making schedule $\alpha_t$, with sufficient steps, the probability of sampling multiple locations in the sequence at the same time can be very low.

\begin{lemma}[Low Probability of Simultaneous Sampling with Sufficient Steps]
\label{lemma:prob_mul_suff}
    Given a sequence of length $L$ and a masking schedule $\alpha_t$. For any $\eps>0$, there exists $N_0$, such that for any $N\geq N_0$, with $N$ reverse steps, the probability $p_\mathrm{mul}$ of sampling multiple locations in the sequence at the same time satisfies:
    $$p_\mathrm{mul}<\eps.$$
\end{lemma}

\begin{proof}
    By \cref{lemma:bound_sep_new_rev}, we know that the probability of a location being sampled at time step $t_i=\frac{N-i}{N}$ is:
    $$\delta_i=\alpha_{t_i}-\alpha_{t_{i-1}}=\alpha_\frac{N-i}{N}-\alpha_\frac{N-i+1}{N}.$$
    Since all the locations are sampled independently, for two distinct locations $i\neq j$ in the sequence, the probability that $i$ and $j$ are sampled simultaneously is:
    $$p_{i,j}=\sum_{i=1}^{N}\delta_i^2.$$
    Summing up $p_{i,j}$, the probability of having two locations a=sampled simultaneously can be bounded by:
    $$p_\mathrm{mul}\leq \frac{L(L-1)}{2}\cdot\sum_{i=1}^{N}\delta_i^2$$
    Since $\alpha_t$ is continuous on $[0,1]$, we know that it is uniformly continuous. Therefore, for any $\eps>0$, there exists $N_0>0$ that satisfies:
    $$|\alpha_x-\alpha_y|<\frac{2\eps}{L(L-1)}, \quad\forall x,y\in [0,1], |x-y|<\frac{1}{N_0}.$$
    In this case, for $N>N_0$, we know that:
    $$|\delta_i|=|\alpha_\frac{N-i}{N}-\alpha_\frac{N-i+1}{N}|<\frac{2\eps}{L(L-1)},\quad\forall i\in [N].$$
    Combining with the fact that $\sum_{i=1}^N\delta_i=1$, we can obtain:
    $$p_\mathrm{mul}\leq \frac{L(L-1)}{2}\cdot\sum_{i=1}^{N}\delta_i\cdot\max_{j\in [N]}\delta_j<\eps.$$
\end{proof}

Next, we consider the $\SER$ increase due to the learning error. Specifically, we only investigate the case where all the locations are sampled at different steps.

\begin{lemma}[Accurate Step-by-Step Generation with Low Learning Error]
\label{lemma:acc_gen}
    Let $q$ denote any HMM, and let $p_\mathsf{\theta}$ represent the reverse model under an arbitrary masking schedule, where $L$ is the sequence length. Let $p$ denote the distribution over sequences generated by $p_\mathsf{\theta}$. Under \cref{ass:perfect_learning} with a learning error $\epsilon_\text{learning} < \frac{\delta}{L},\ \delta>0$, and given an instance of reverse process $\tau=(\gM_1,\gM_2,\cdots,\gM_N)$ with $|\gM_i|\leq 1$, let $p_\mathrm{acc}$ denote the probability of generating a valid sequence. Then $p_\mathrm{acc}$ satisfies:
    $$p_\mathrm{acc}\geq e^{-\delta}.$$
\end{lemma}

\begin{proof}
    Since $|\gM_i|\leq 1$, we only need to consider the steps where one token is sampled. Let $\Tilde{\vx}_t$ denote the previously sampled tokens, and $\Tilde{x}_t$ denote the token sampled at the current step. If $\Tilde{\vx}_t$ is can later form a valid sequence, let $\gX_t$ denote the set of valid choices for $\Tilde{x}_t$. In other words, if $\Tilde{x}_t\in \gX_t$, then the combination of $\Tilde{\vx}_t$ and $\Tilde{x}_t$ is can later form a valid sequence, or more intuitively:
    $$q_{0|t}(\Tilde{x}_t\mid\Tilde{\vx}_t)>0.$$
    Under \cref{ass:perfect_learning}, we know that:
    $$\DKL{q_{0|t}(x_t \mid \Tilde{\vx}_t)}{p_\mathbf{\theta}(x_t \mid \Tilde{\vx}_t)} < \epsilon_\text{learning}.$$
    Since it is assumed that $0\log 0=0$, we have:
    $$\sum_{x_t\in\gX_t}q_{0|t}(x_t \mid \Tilde{\vx}_t)\log\frac{q_{0|t}(x_t \mid \Tilde{\vx}_t)}{p_\theta(x_t \mid \Tilde{\vx}_t)}<\eps_\text{learning}.$$
    Equivalently, we have:
    $$-\eps_\text{learning}<\sum_{x_t\in\gX_t}q_{0|t}(x_t \mid \Tilde{\vx}_t)\log\frac{p_\theta(x_t \mid \Tilde{\vx}_t)}{q_{0|t}(x_t \mid \Tilde{\vx}_t)}.$$
    Due to the concavity of $\log x$, by Jensen's Inequality, we can obtain:
    $$\sum_{x_t\in\gX_t}q_{0|t}(x_t \mid \Tilde{\vx}_t)\log\frac{p_\theta(x_t \mid \Tilde{\vx}_t)}{q_{0|t}(x_t \mid \Tilde{\vx}_t)}\leq \log\left(\sum_{x_t\in\gX_t}q_{0|t}(x_t \mid \Tilde{\vx}_t)\cdot \frac{p_\theta(x_t \mid \Tilde{\vx}_t)}{q_{0|t}(x_t \mid \Tilde{\vx}_t)}\right) =\log\sum_{x_t\in\gX_t}p_\theta(x_t \mid \Tilde{\vx}_t).$$
    Therefore, the probability that each step remains valid satisfies:
    $$\sum_{x_t\in\gX_t}p_\theta(x_t \mid \Tilde{\vx}_t)\geq e^{-\eps_\text{learning}}\geq e^{-\frac{\delta}{L}}.$$
    Since there are $L$ locations in the sequence, the probability of generating a valid sequence is bounded by:
    $$p_\mathrm{acc}\geq (e^{-\frac{\delta}{L}})^L=e^{-\delta}.$$
\end{proof}

Combining the above lemmas, we can derive the upper bound of $\SER$ by taking sufficient reverse steps and small learning error.

\begin{theorem}[Accurate Generation of HMM with Sufficient Steps]
    Let $q$ denote any HMM, and let $p_\mathsf{\theta}$ represent the reverse model under an arbitrary masking schedule, where $L$ is the sequence length. Let $p$ denote the distribution over sequences generated by $p_\mathsf{\theta}$. Under \cref{ass:perfect_learning} with a learning error $\epsilon_\text{learning} < O(\frac{\delta}{L})$, and given a sufficient number of reverse steps, the sequence error rate $\operatorname{SER}(p)$ of the generated text satisfies 
    \[
    \operatorname{SER}(p) \leq  \delta.
    \]
\end{theorem}

\begin{proof}
    For $\delta>0$, we know that:
    $$1-\delta<c.$$
    By \cref{lemma:prob_mul_suff}, given the masking schedule $\alpha_t$, there exists $N_0$, for $N>N_0$ and $N$ reverse steps, the probability of sampling multiple locations in the sequence at the same time is bounded by:
    $$p_\mathrm{mul}<1-\frac{1-\delta}{e^{-\delta}}.$$
    In other words, the probability of sampling all the locations at different steps is at least $\frac{1-\delta}{e^{-\delta}}$. By \cref{lemma:acc_gen}, for each reverse process which satisfies that all the locations are sampled at different steps, the probability of generating a valid sequence is lower bounded by:
    $$p_\mathrm{acc}\geq e^{-\delta}.$$
    Therefore, the sequence error rate $\SER$ satisfies:
    $$\SER(p)\leq 1-\frac{1-\delta}{e^{-\delta}}\cdot e^{-\delta}= \delta.$$
\end{proof}

\subsection{Proof for \cref{thm:negative}}
\label{app:proof_neg}
In the section, we aim to find an example (\cref{exa:interval}) with high sequence error rate. To present this example, we begin with a special class of languages defined under the interval setting:

\begin{definition}[Interval Setting]
\label{def:interval_setting}
Consider a sequence of length $L$, which is divided equally into $M$ intervals $\gI_1,\gI_2,\cdots,\gI_M$, each of length $l=\frac{L}{M}\geq 2$. Given a masking schedule $\alpha_t$, an instance of reverse process $\tau=(\gM_1,\gM_2,\cdots,\gM_N)$ is defined by \cref{def:ins_rev}. For any two locations within different intervals, their corresponding tokens are independent from each other. In other words, let $\Tilde{\vx}_i^{(j)}$ denote the new tokens in $\gM_i\cap\gI_j$, $\Tilde{\vx}_{<i}^{(j)}$ denote the previously sampled tokens in $\gM_{<i}\cap\gI_j$, and $p$ denote the distribution over sequences generated by the reverse model with reverse process $\tau$, then for time step $t_i=\frac{N-i}{N}$:
$$p(\Tilde{\vx}_i^{(j)}|\Tilde{\vx}_{<i})=p(\Tilde{\vx}_i^{(j)}|\Tilde{\vx}_{<i}^{(j)}).$$
In this case, we have:
$$p(\vx)=\prod_{j=1}^{M}p(\vx^{(j)})=\prod_{j=1}^{M}\prod_{i=1}^{N}p(\Tilde{\vx}_i^{(j)}|\Tilde{\vx}_{<i}^{(j)}).$$
We denote the above setting as $\operatorname{Inter}(L,l,\alpha_t)$.
\end{definition}

% \begin{definition}[Interval Setting]
% \label{def:interval_setting}
% Consider a sequence of length $L$, which is divided equally into $N$ intervals, each of length $l=L/N$. At each step, one or two previously unselected positions, denoted $i$ or $(i,j)$ with $ i\neq j$, are randomly chosen from the sequence. Random variables are then sampled according to
% $$X_i \sim q_i(X_{I_i}), \quad \text{and independently, } X_j \sim q_j(X_{I_j}),$$
% where $X_{I_i}$ represents the set of previously sampled random variables within the interval of $i$, and similarly for $X_{I_j}$.

% Let $M$ be the total number of steps in which two positions are sampled simultaneously. The overall setting is denoted as $\operatorname{Inter}(L,N,M)$.
% \end{definition}

Under the interval setting defined above, we can control the probability of sampling simultaneously in the same interval.

\begin{lemma}[Simultaneous Sampling Probability for an Interval]
\label{lemma:simul_prob_inter}
    Consider the interval setting $\operatorname{Inter}(L,l,\alpha_t)$. For each interval $\gI_j$ of length $l$, let $h_j$ denote the probability that all the locations in $\gI_j$ are sampled in different time steps. Then, 
    % for $\alpha_t$ satisfying $\max\delta_i\leq \frac{2}{l}$, 
    $h_j$ can be bounded by:
    % $$h_j\leq \left(1-\frac{1}{N}\right)^{l-1},$$
    % and for all $\alpha_t$:
    $$h_j\leq 1-\frac{1}{N}.$$
\end{lemma}

\begin{proof}
    Let $\delta_i=\alpha_{t_i}-\alpha_{t_{i-1}}$. Similar to \cref{lemma:bound_sep_new_rev}, we know that $\delta_i$ is the probability of a location being sampled at time step $t_i$. Take the first location in $|\gI_j|$, denote it as $X_1$, and let $X_2,\cdots,X_l$ denote the rest $l-1$ locations in $\gI_j$. If $X_1$ is sampled at step $t_i$, then $X_2,\cdots,X_l$ must be sampled at time steps other than $t_i$. Therefore, $h_j$ can be bounded by:
    $$h_j\leq \sum_{i=1}^{N}\delta_i(1-\delta_i)^{l-1}\leq\sum_{i=1}^{N}\delta_i(1-\delta_i).$$
    Let $f(\delta)=\delta(1-\delta)$. Note that we have:
    $$f''(\delta)=-2\leq 0,$$
    which indicates that $f(\delta)$ is concave. Using Jensen's Inequality, we can obtain:
    $$h_j\leq \sum_{i=1}^{N}f(\delta_i)\leq Nf\left(\frac{1}{N}\right)=1-\frac{1}{N}.$$
    
    % $$h_j\leq \sum_{i=1}^{N}\delta_i(1-\delta_i)^{l-1}.$$
    % Let $f(\delta)=\delta(1-\delta)^{l-1}$. Note that for $l\delta\leq 2$, we have:
    % $$f''(\delta)=-(l-1)(2-l\delta)(1-\delta)^{l-3}\leq 0,$$
    % which indicates that $f(\delta)$ is concave. Using Jensen's Inequality, we can obtain:
    % $$h_j\leq \sum_{i=1}^{N}f(\delta_i)\leq Nf\left(\frac{1}{N}\right)=\left(1-\frac{1}{N}\right)^{l-1}.$$
    
    % In the other hand, for any $\alpha_t$, note that:
    % $$h_j\leq \sum_{i=1}^{N}\delta_i(1-\delta_i)^{l-1}\leq \sum_{i=1}^{N}\delta_i(1-\delta_i).$$
    % Using similar tricks as before, we can get:
    % $$h_j\leq 1-\frac{1}{N}.$$

\end{proof}

Using the above lemma, if we assume that sampling simultaneously in one interval increases $\SER$, then we can derive an lower bound for $\SER(p)$.

\begin{lemma}[$\SER$ bound for Interval Setting]
\label{lemma:acc_inter}
    Consider the interval setting $\operatorname{Inter}(L,l,\alpha_t)$. Assume that sampling simultaneously in the same interval introduces an error with probability at least $p_0$, and other actions do not reduce error. In other words, if two locations in an interval are both sampled at step $t_i$, then there is a probability of $p_e$ that the sequence will not be accurate afterwards. In this case, let $p$ denote the distribution over sequences of length $L$ generated by the reverse model with masking schedule $\alpha_t$ and $N$ reverse steps. We have the following bound for $\SER$: 
    % if $\max\delta_i\leq \frac{2}{l}, N\geq (l-1)(l-2)$ and $l\geq 2$, we have:
    % $$\operatorname{ACC}(p)\leq \left(1-\frac{(l-2)p_e}{N}\right)^{L/l}.$$
    % In other cases:
    $$\SER(p)\geq 1-\left(1-\frac{p_e}{N}\right)^{L/l}.$$
\end{lemma}

\begin{proof}
    By \cref{lemma:simul_prob_inter}, we can obtain that for each interval $\gI_j$, the probability $p_\textrm{error}^{(j)}$ of generating an error in $\gI_j$ is lower-bounded by:
    $$p_\textrm{error}^{(j)}\geq p_e(1-h_j)\geq\frac{p_e}{N}.$$
    % $$\begin{cases}
    %     p_\textrm{error}^{(j)}\geq p_e\left(1-(1-\frac{1}{N})^{l-1}\right), &\max\delta_i\leq \frac{2}{l},\\
    %     p_\textrm{error}^{(j)}\geq \frac{p_e}{N}, &\textit{otherwise}.
    % \end{cases}$$
    % Expanding the formula, we have:
    % \begin{align*}
    %     1-(1-\frac{1}{N})^{l-1}&=\left(1-(1-\frac{1}{N})\right)\cdot \left(1+(1-\frac{1}{N})+\cdots+(1-\frac{1}{N})^{l-2}\right)\\
    %     &\geq \frac{l-1}{N}\left(1-\frac{1}{N}\right)^{l-2}\\
    %     &\geq \frac{l-1}{N}\left(1-\frac{l-2}{N}\right).
    % \end{align*}
    % If $N\geq (l-1)(l-2)$, we know that:
    % $$1-(1-\frac{1}{N})^{l-1}\geq \frac{l-2}{N}$$
    Due to the independence between different intervals, the accuracy $\SER(p)$ can be calculated as:
    $$\SER(p)=1-\prod_{j=1}^{M}(1-p_\textrm{error}^{(j)}).$$
    Therefore, we have the bound:
    $$\SER(p)\geq 1-\left(1-\frac{p_e}{N}\right)^{L/l}.$$
    % Therefore, for $\max\delta_i\leq \frac{2}{l}, N\geq (l-1)(l-2)$ and $l\geq 2$, we know that:
    % $$\operatorname{ACC}(p)\leq \left(1-\frac{(l-2)p_e}{N}\right)^{L/l}.$$
    % And in other cases, we have:
    % $$\operatorname{ACC}(p)\leq \left(1-\frac{p_e}{N}\right)^{L/l}.$$
\end{proof}

% \begin{lemma}[Interval Sampling Probability Bound]
% \label{lemma:interval_bound}
% Consider the setting $\operatorname{Inter}(L,N,M)$. Assume that there are $k$ unsampled positions remaining, and two positions are sampled simultaneously. Let $p(k)$ denote the probability that both sampled positions belong to the same interval. Then $p(k)$ satisfies
% $$p(k)\geq \max\left\{0,\frac{k-N}{(k-1)N}\right\}.$$
% \end{lemma}

% \begin{proof}
% The probability \( p(k) \) of selecting two positions within the same interval is given by:
% \[
% p(k) = \frac{1}{k(k-1)} \sum_{i=1}^N u_i(u_i-1),
% \]
% where \( u_i \) is the number of unsampled positions in the \( i \)-th interval, and \( \sum_{i=1}^N u_i = k \).
% By the AM-GM Inequality, we have:
% \begin{align*}
%     p(k) = \frac{\sum_{i=1}^N u_i^2-k}{k(k-1)}\geq \frac{(\sum_{i=1}^N u_i)^2/N-k}{k(k-1)}=\frac{k-N}{(k-1)N}.
% \end{align*}
% To ensure non-negativity, we take:
% $$p(k) \geq \max\left\{0, \frac{k-N}{(k-1)N}\right\}.$$
% \end{proof}

% \begin{lemma}[Interval Sampling Lemma for Masked Condition]
% \label{lemma:interval_mask}
% Consider the setting $\operatorname{Inter}(L,N,M)$, where $N$ is divisible by 2. Assume that sampling simultaneously in the same interval introduces an error with probability $p_0$, and other sampling processes do not introduce or reduce error. Denote the total number of errors as $n_e$, then its expectation satisfies
% $$\E[n_e]\geq \left(\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\ln\frac{2M}{N}\right)p_0.$$
% \end{lemma}

% \begin{proof}
% Assume that these $M$ simultaneous samplings of two positions occur when there are $k_1,k_2,\cdots,k_M$ unselected positions, and introduce $e_1,\cdots,e_M$ errors, respectively. Using the fact that $n_e=\sum_{i=1}^{M}e_i$, we know that:
% \begin{align*}
%     \E[e_i]&=p_0\cdot p(k_i),\\
%     \E[n_e]&=\E\left[\sum_{i=1}^{M}e_i\right]=\sum_{i=1}^{M}\E[e_i]=p_0\cdot\left(\sum_{i=1}^{M}p(k_i)\right).
% \end{align*}

% By Lemma \ref{lemma:interval_bound}, we have:
% $$\sum_{i=1}^{M}p(k_i)\geq\sum_{i=1}^{M}\max\left\{0, \frac{k_i-N}{(k_i-1)N}\right\}.$$
% Since the function $\max\left\{0, \frac{k-N}{(k-1)N}\right\}$ is increasing with respect to k:
% \begin{align*}
%     \sum_{i=1}^{M}p(k_i)&\geq\sum_{i=1}^{M}\max\left\{0, \frac{2i-N}{(2i-1)N}\right\}\\
%     &=\sum_{i=\frac{N}{2}+1}^{M}\left(\frac{1}{N}-\frac{N-1}{N}\cdot\frac{1}{2i-1}\right)\\
%     &=\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\cdot\left[\frac{1}{N+1}+\frac{1}{N+3}+\cdots+\frac{1}{2M-1}\right]\\
%     &\geq\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\cdot\frac{1}{2}\int_{N}^{2M}\frac{\mathrm{d}x}{x}\\
%     &=\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\ln\frac{2M}{N}.
% \end{align*}
% The second inequality is due to the concavity of $\frac{1}{x}$. Substituting back, we can get:
% $$\E[n_e]\geq \left(\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\ln\frac{2M}{N}\right)p_0.$$
% \end{proof}

% \begin{corollary}\label{cor:inter_estimate}
% Consider the setting $\operatorname{Inter}(L,N,M)$, where $N$ is divisible by 2. For $p$ and $n_e$ as defined above, if $M\geq\left(\frac{2}{p_0}+4\ln 2-1\right)N$, then 
% $$\E[n_e]\geq 1.$$
% \end{corollary}

% \begin{proof}
% By Lemma \ref{lemma:interval_mask},
% $$\E[n_e]\geq \left(\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\ln\frac{2M}{N}\right)p_0.$$

% Let $M=cN$, We only need prove that for $c\geq\frac{2}{p_0}+4\ln 2-1$,
% $$c-\frac{N-1}{N}\ln(2c)\geq \frac{1}{p_0}+\frac{1}{2}.$$

% Let $f(c)=c-\ln c$. For $c\geq 2$, derivative $f'(c)=1-\frac{1}{c}\geq\frac{1}{2}$, so we have
% $$f(c)=f(2)+\int_{2}^{c}f'(x)\mathrm{d}x\geq 2-\ln 2+\frac{c-2}{2}=1-\ln 2+\frac{c}{2}.$$

% Therefore, for $c\geq\frac{2}{p_0}+4\ln 2-1\geq 2$,
% $$c-\frac{N-1}{N}\ln(2c)\geq c-\ln(2c)\geq 1-2\ln 2+\frac{c}{2}\geq\frac{1}{p_0}+\frac{1}{2}.$$
% \end{proof}

% \begin{remark}
% If $l$ is chosen to be sufficiently large (or equivalently, $N/L$ is sufficiently large), then $M=\left\lceil\left(\frac{2}{p_0}+4\ln 2-1\right)N\right\rceil$ naturally satisfies the condition $M\leq 2N$. Specifically, we need $l=O(\frac{1}{p_0})$.
% \end{remark}

To show that the above setting is reasonable and achievable, we give the following example, which is later shown to be the example we are looking for.

\begin{example}
\label{exa:interval}
Consider a sequence of length $L$, which is divided equally into $
M$ intervals, each of length $l=L/M$. Denote the $k$-th interval as $\gI_k=[1+(k-1)l,\ kl]$. The tokens $x_i,\ 1\leq i\leq L$ in the sequence satisfy the following rules:
\begin{itemize}
    \item Each $x_i$ takes values in the set $\gA=\{a_1,\cdots,a_{2^{l-1}}\}$. For each $a_j\in\gA$, there corresponds a vector $v_j=(v_{j,1},\cdots,v_{j,l-1})\in\{0,1\}^{l-1}$, where $(v_{j,1}\cdots v_{j,l-1})_2$ is the binary expression for $j-1$. Thus, each random variable $x_i$ corresponds to a random vector $(v_1^{(i)},\cdots,v_{l-1}^{(i)})$, where $v_j^{(i)}\in\{0,1\}$ for $j=1,\cdots l-1$.
    \item For $i\in \gI_k$ and $j\in \gI_s$, if $k\neq s$, then $x_i$ and $x_j$ are independent.
    \item For $i, j\in \gI_k$ such that $i<j$, let $i'=i-(s-1)l$ and $j'=j-(s-1)l$. Then, $x_i$ and $x_j$ are the $i'$-th and $j'$-th elements in interval $\gI_k$, respectively. The corresponding binary components satisfy $v_{j'-1}^{(i)}=v_{i'}^{(j)}\sim \operatorname{Bernoulli}(\frac{1}{2})$, which is independent of all other $v_t^{(s)}$.
\end{itemize}
In this setup, each interval $\gI_k$ contains $\frac{l(l-1)}{2}$ pairs of mutually independent random variables. Given an arbitrary masking schedule $\alpha_t$, this setting is consistent with \cref{def:interval_setting}. Let $q$ denote the data distribution described above.

Under \cref{ass:perfect_learning}, we only need to examine the case where $\vx_t$ has no error. By \cref{lemma:pinsker}, we know that:
$$\left\lVert q_{0|t}(x_0^i \mid \vx_t)-p_{\theta}(x_0^i \mid \vx_t)\right\rVert_1\leq \sqrt{2\DKL{q_{0|t}(x_0^i \mid \vx_t)}{p_\mathbf{\theta}(x_0^i \mid \vx_t)}} \leq \sqrt{2\eps_\textit{learning}}.$$ 
Let $\gM$ denote the set of previously sampled locations. For $q$ and any unsampled location in interval $\gI$, all of the potential tokens $x$ at this location which is consistent with $\vx_t$ have the same probability:
$$q(x | \vx_t)=\frac{1}{2^{l-1-|\gM\cap\gI|}}.$$

If two locations $x_i,x_j$ within the same interval $\gI$ are sampled simultaneously, ignoring the possible inconsistency with previously sampled tokens (since error can not be reduced), the independence of the random variable pairs implies that the probability of generating an error is lower-bounded by:
$$p_e\geq (\frac{1}{2}+e_1)(\frac{1}{2}+e_2)+(\frac{1}{2}+e_3)(\frac{1}{2}+e_4)$$
where $\frac{1}{2}$ implies the probability (for $q$) of letting $v_{i'}^{(j)}$ or $v_{j'-1}^{(i)}$ to be $0$ or $1$, and $e_1,e_2,e_3,e_4$ satisfies:
\begin{align*}
    |e_1|+|e_3|&=\left\lVert q_{0|t}(x_0^i \mid \vx_t)-p_{\theta}(x_0^i \mid \vx_t)\right\rVert_1\\
    |e_2|+|e_4|&=\left\lVert q_{0|t}(x_0^j \mid \vx_t)-p_{\theta}(x_0^j \mid \vx_t)\right\rVert_1
\end{align*}
Thus, we know that:
$$p_e\geq \frac{1}{2}-(|e_1|+|e_2|+|e_3|+|e_4|)\geq \frac{1}{2}-2\sqrt{2\eps_\textit{learning}}.$$

In other words, this is consistent with the setting \cref{lemma:acc_inter}, with an error probability $p_e=\frac{1}{2}-2\sqrt{2\eps_\textit{learning}}$.

% Furthermore, if two locations within the same interval are sampled simultaneously, the independence of the random variable pairs implies that there is a probability of $2\times (\frac{1}{2})^2=\frac{1}{2}$ to generate an inconsistent situation. Specifically, this would occur if $v_{j'-1}^{(i)}\neq v_{i'}^{(j)}$ for the sampled variables $x_i$ and $x_j$. This scenario can be viewed as an error, and is consistent with the assumption in \cref{lemma:acc_inter}, where the error probability is $p_0=\frac{1}{2}$.
\end{example}

Although the example above seems a bit tricky, it can actually be modified into the form of an HMM, a commonly considered structure for generative models.

\begin{note}[HMM Form of \cref{exa:interval}]
\label{note:hmm_eg}
The setting described in \cref{exa:interval} can be alternatively modeled as a Hidden Markov Model (HMM), where the observation space is $\gO=\gA$, and the state space is $\gS=\{(i,A^{(i)})|A^{(i)}\in\R^{(l-1)\times(l-1)},i=1,\cdots,l\}$. Here, $i$ represents the current position within the interval, and $A^{(i)}$ is an upper triangular matrix with entries taking values of 0 or 1. For $j\leq i$, the $j$-th row of $A^{(i)}$ encodes the values sampled by the variable pairs formed between the $j$-th position and all its subsequent positions in the interval. For $j>i$, the $j$-th row of $A^{(i)}$ is set to 0.

Given the current state $s=(i,A^{(i)})$, the state transition and emission process can be describe as follows:
\begin{itemize}
    \item The observation $o_i$ corresponds to the $i-1$-th column and the $i$-th row of the matrix $A^{(i)}$, where the values of variable pairs relevant to the $i$-th position within the interval are encoded. Specifically, we know that $o_i\in\gA$ corresponds to a vector $v_i=(v_{i,1},\cdots,v_{i,l-1})$, where $$v_{i,j}=\begin{cases}
        A^{(i)}_{j,i-1}, &j<i,\\
        A^{(i)}_{i,j}, &j\geq i.
    \end{cases}$$
    \item If $i<l$, the next state is $s'=(i,A^{(i+1)})$, where the first $i$ rows of $A^{(i+1)}$ is the same as $A^{(i)}$, and $A^{(i+1)}_{i+1,j}\sim\operatorname{Bernoulli}(\frac{1}{2}) \text{ i.i.d.}$ for $j=i+1,\cdots,l-1$, with the remaining entries set to 0.
    \item If $i=l$, the next state resets to $s'=(1,A^{(1)})$, where the entries in the first row are independently sampled from $\operatorname{Bernoulli}(\frac{1}{2})$, and other entries are set to 0.
\end{itemize}
The size of the observation space is given by $|\gO|=|\gA|=2^{l-1}$. The size of the state space is computed as: $$|\gS|=\sum_{i=1}^{l}2^{(2l-i-1)i/2}\leq l\cdot 2^{l(l-1)/2}.$$
\end{note}

The above Note gives the HMM form of \cref{exa:interval}. In fact, with appropriate adjustments, it can be further modified into an n-gram language. Using the HMM defined above, we can prove \cref{thm:negative}.

\begin{theorem}[SER Bound for HMM Generation]
    There exists an HMM $q$ over a vocabulary of size $16$ that satisfies the following conditions: for any reverse model $p_\mathsf{\theta}$ under \cref{ass:perfect_learning} with $\eps_\mathrm{learning}<\frac{1}{128}$, and any masking schedule $\alpha_t$, let $p$ denote the distribution over sequences generated by $p_\mathsf{\theta}$. There exists a constant $C$ such that if the number of sampling steps satisfies $N = CL$, where $L$ is the sequence length, the SER of the generated text is lower-bounded by:
    \begin{equation*}
        \operatorname{SER}(p) > \frac{1}{2}.
    \end{equation*}
\end{theorem}

\begin{proof}
    Take the HMM described in \cref{note:hmm_eg}, and set $l=5$, $N=CL$. The vocabulary is the observation space $\gO$ which satisfies $|\gO|=2^{l-1}$. By \cref{lemma:acc_inter}, for any masking schedule $\alpha_t$, we have:
    $$\SER(p)\geq 1-\left(1-\frac{p_e}{N}\right)^{L/l}.$$
    As illustrated in \cref{exa:interval}:
    $$p_e=\frac{1}{2}-2\sqrt{2\eps_\textit{learning}}.$$
    Therefore, take $N=CL$, and let $y=\frac{CL}{p_e}$, we have:
    $$\SER(p)\geq 1-\left[\left(1-\frac{1}{y}\right)^y\right]^\frac{p_e}{Cl}.$$
    Since $(1-\frac{1}{y})^y$ is decreasing, and apparently $y\geq \frac{Cl}{p_e}$, we know that:
    $$\SER(p)\geq \frac{p_e}{Cl}.$$
    Let $C=\frac{2p_e}{l+1}$, we can get the upper bound:
    $$\SER(p)>\frac{1}{2}.$$
    In this way:
    $$C=\frac{2p_e}{l+1}=\frac{\frac{1}{2}-2\sqrt{2\eps_\textit{learning}}}{6}\geq \frac{1}{24}=O(1).$$
\end{proof}


% \begin{remark}
% In the case of Example \ref{exa:interval}, we can further estimate the expectation
% $$\E[n_e]\geq \frac{1}{2}\left(\frac{M}{N}-\frac{1}{2}-\frac{N-1}{N}\ln\frac{2M}{N}\right).$$

% Using tighter estimations, we can get the following result: For $M\geq 5N$, we have $\E[n_e]\geq 1$. Note that this only requires $l\geq 10$.
% \end{remark}
