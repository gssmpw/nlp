\vspace{-10pt}
\section{Extend to Efficient Sampling Strategies}
\vspace{-5pt}
\label{app:ddpm_cache}

In \citet{sahoo2024simple} and \citet{ou2024your}, an efficient sampling strategy \verb|ddpm_cache| is proposed, which can reduce the sampling time by a constant order of magnitude. Specifically, this sampler is approximately 3-4 times faster than previously used samplers when the number of sampling steps is large. In this section, we discuss the influence of \verb|ddpm_cache| on our conclusions under different sampling steps.

First, we briefly introduce the principles of \verb|ddpm_cache|. It utilizes the observation that if no locations are sampled at a given step, the sequence remains unchanged. Consequently, when the reverse model is not conditioned on time, the cached value computed during the first time this sequence went through the reverse model can be reused, instead of going through the reverse model again.

This sampling strategy does not affect our main theorems, as they are based solely on the sampled locations at each step, while unsampled locations are not considered. As for the evaluation metrics for computational efficiency in our experiments, we break it down into the following two cases:
\begin{enumerate}[nosep] 
    \item When the number of sampling steps is much smaller than the sequence length, which is the primary scenario we focus on, the expectation of steps where no new locations are sampled is relatively low, resulting in a computational cost that is nearly linear with respect to the number of sampling steps.
    \vspace{5pt}
    \item As the number of sampling steps becomes larger, the computational cost is mainly dependent on the number of valid steps where at least one location is sampled. As a matter of fact, the expectation of the number of valid steps increases as the number of sampling steps increases, and the maximum number of valid steps is equal to the number of sampling steps. In this case, the MDMs offer no computational advantage over auto-regressive models.
\end{enumerate}
Based on the above conclusions, we can find that for tasks requiring a low TER, using \verb|ddpm_cache| can further accelerate the generation of MDMs, suggesting high efficiency. Conversely, for tasks that demand a low SER, we have shown that the number of sampling steps need to be large enough, such that MDMs can not generate with low cost even when using \verb|ddpm_cache|. Therefore, we extend our findings to MDMs with efficient sampling strategies.