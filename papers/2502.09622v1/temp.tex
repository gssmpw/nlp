% \begin{lemma}[Upper Bound for Multi-tokens Sampling]
% \label{lemma:kl_mul_token_sample}
%     Let $\mX = (X_1, X_2, \ldots, X_k) \in [N]^k$ be a random vector following the distribution $q$, where each component $X_i$ follows the marginal distribution $q_i$. Define $\Tilde{\mX} = (\Tilde{X}_1, \Tilde{X}_2, \ldots, \Tilde{X}_k) \sim p$ as another random vector, where the components $\Tilde{X}_i$ are sampled independently according to $p_i$. Let $\delta=\max_{i}\{\DKL{q_i}{p_i}\}$, then, the KL divergence between $p$ and $q$ satisfies the inequality:
%     \[
%     \DKL{q}{p} \leq (k-1)\log N+k\delta.
%     \]
    
% \end{lemma}

% \begin{proof}

% Using the chain rule for probabilities, the KL divergence can be written as:
% \[
% \DKL{q}{p} = \mathbb{E}_q\left[\sum_{i=1}^k \log\!\biggl(\frac{q_i(x_i \mid \mathbf{x}_{<i})}{p_i(x_i)}\biggr)\right],
% \]
% where \(\mathbf{x}_{<i} = (x_1, \ldots, x_{i-1})\). For \(i = 1\), there are no preceding variables, so:
% \[
% \mathbb{E}_q\left[\log\!\biggl(\frac{q_1(x_1)}{p_1(x_1)}\biggr)\right] = \DKL{q_1}{p_1}.
% \]

% For \(i > 1\), we bound:
% \[
% \mathbb{E}_q\left[\log\!\biggl(\frac{q_i(x_i \mid \mathbf{x}_{<i})}{p_i(x_i)}\biggr)\right]
% \leq \mathbb{E}_q\left[\log\!\biggl(\frac{1}{p_i(x_i)}\biggr)\right].
% \]
% Decomposing \(\mathbb{E}_q\left[\log\!\bigl(1/p_i(x_i)\bigr)\right]\), we get:
% \[
% \mathbb{E}_q\left[\log\!\biggl(\frac{1}{p_i(x_i)}\biggr)\right]
% = \mathbb{E}_q\left[\log\!\biggl(\frac{q_i(x_i)}{p_i(x_i)}\biggr)\right] + \mathbb{E}_q\left[\log\!\biggl(\frac{1}{q_i(x_i)}\biggr)\right].
% \]
% The first term is \(\DKL{q_i}{p_i}\), and the second term is \(-\mathbb{E}_q\bigl[\log q_i(x_i)\bigr]\), which represents the entropy of \(q_i\). Since the entropy of any distribution over \([N]\) is at most \(\log N\), we have:
% \[
% -\mathbb{E}_q\bigl[\log q_i(x_i)\bigr] \leq \log N.
% \]
% Thus:
% \[
% \mathbb{E}_q\left[\log\!\biggl(\frac{q_i(x_i \mid \mathbf{x}_{<i})}{p_i(x_i)}\biggr)\right]
% \leq \DKL{q_i}{p_i} + \log N.
% \]

% Summing over all \(i = 1, \ldots, k\), we obtain:
% \[
% \DKL{q}{p} = \sum_{i=1}^k \mathbb{E}_q\left[\log\!\biggl(\frac{q_i(x_i \mid \mathbf{x}_{<i})}{p_i(x_i)}\biggr)\right]
% \leq \DKL{q_1}{p_1} + \sum_{i=2}^k \bigl(\DKL{q_i}{p_i} + \log N\bigr).
% \]

% Reorganizing, we have:
% \[
% \DKL{q}{p} \leq \sum_{i=1}^k \DKL{q_i}{p_i} + (k-1)\log N.
% \]
% Since \(\DKL{q_i}{p_i} \leq \delta\) for all \(i\), the total sum of marginal KL divergences is bounded by \(k\delta\). Therefore:
% \[
% \DKL{q}{p} \leq k\delta + (k-1)\log N.
% \]

% This completes the proof. 

% \end{proof}

% \begin{lemma}[Chernoff Bound]
% \label{lemma:chernoff}
% Let \( X_1, \ldots, X_n \) be independent random variables taking values in \(\{0, 1\}\). Define \( X = \sum_{i=1}^n X_i \) as the sum of these independent random variables, and let \(\mu = \mathbb{E}[X]\) denote the expected value of \( X \). Then, the following probabilistic bounds hold:
% \begin{equation*}
%     \begin{aligned}
%         & \Pr(X \geq (1 + \delta)\mu) \leq e^{-\frac{\delta^2\mu}{2 + \delta}}, \quad &\text{for } \delta \geq 0,\\
%         & \Pr(X \leq (1 - \delta)\mu) \leq e^{-\frac{\delta^2\mu}{2}}, \quad &\text{for \ } 0 < \delta < 1.
%     \end{aligned}
% \end{equation*}
% \end{lemma}

% \begin{lemma}[Pinsker's Inequality]
% \label{lemma:pinsker}
%     Let $p$ and $q$ be two probability distributions. Then, the total variation distance between these distributions satisfies:
%     $$D_\mathrm{TV}(p,q)\leq \sqrt{\frac{1}{2}\DKL{p}{q}}.$$
%     Specifically, since $D_\mathrm{TV}(p,q)=\frac{1}{2}\left\lVert p-q\right\rVert_1$, the following inequality holds:
%     $$\left\lVert p-q\right\rVert_1\leq \sqrt{2\DKL{p}{q}}.$$
% \end{lemma}

% \subsection{Lemmas for the Instance of Reverse Process}