\section{Related Work}
~\label{sec:relatedwork}

\paragraph{Work on Range Searching} Some earlier work addresses range retrieval in high dimensions from a theoretical standpoint~\cite{chazelle2008approximate}, or with only minimal experiments that do not extend to the large embeddings used today~\cite{wang2013pltree}. Theory results on nearest neighbor search using locality sensitive hashing (LSH) also implicitly apply to range search, since their approximation guarantees are usually in the form of finding neighbors within a $(1+\epsilon)$ radius of the distance to the true top-$k$ result~\cite{indyk1998towards}.

On the practical side, Meta AI released a range retrieval dataset aimed towards detecting misinformation~\cite{simsearchnet}, which was used as one of the competition datasets in the NeurIPS 2021 Big ANN Benchmarks Challenge~\cite{simhadri2021results}, but competitors solved this task using naive adaptations of top-$k$ search rather than novel range searching algorithms. A recent work by Szilvasy, Mazare, and Douze~\cite{szilvasy2024vector} addresses the combined task of retrieving range search results with one metric and then re-ranking them using a more sophisticated model. They address the question of designing a range search metric (RSM) specifically for \textit{bulk} search---that is, range search over a group of queries with a fixed computational budget---and benchmark its results on an image search application. They use an inverted file (IVF) index, a common data structure for similarity search~\cite{douze2024faiss}, to serve their database queries. 

The Starling system~\cite{wang2024starling} is an SSD-resident graph-based ANNS index optimized for I/O efficiency. Their work includes benchmarks on range search metrics, and they use an algorithm similar to the doubling beam search algorithm we present in Section~\ref{sec:algorithms}. However, the focus of their paper is not on comprehensive benchmarking of range search, and since their implementation is a disk-resident index and ours is fully in-memory, our experiments are not directly comparable to theirs. 

\paragraph{Other Related Work} Some variants on the high-level ideas behind our range search algorithms---that is, early termination of queries with few results, and extensions of beam search for queries with many results---have been used before in the context of nearest neighbor search. A work by Li et al.~\cite{li2020improving} uses a machine learning model to predict when a query can terminate early and still maintain high accuracy. Another work~\cite{xu2021twostage} uses graph search with two phases (essentially, an initial cheaper phase and a later, more compute intensive phase for selected queries to gather additional candidates).