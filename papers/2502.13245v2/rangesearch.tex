\section{Characteristics of Range Search Datasets}\label{sec:rangedata}

In this section, we take eight existing embedding datasets from top-$k$ retrieval benchmarks and show how to use them for range search---namely, by finding a radius which yields a suitable distribution. We evaluate the characteristics of each dataset; alongside, we also evaluate the one existing range search benchmark. 

For a dataset to be meaningful as a range search benchmark, the same distance $\epsilon$ must be indicative of a match for each query point. While this is not necessarily the case for all embedding datasets, it is not directly observable without full access to the source material, and even with that access would be incredibly costly to determine. Thus, we use the characteristics of existing benchmarks to determine whether a dataset is suitable, and select a radius if so. Real-world query sets tend to follow a power-law distribution, where most points have no matches, while a handful of points have thousands of matches~\cite{szilvasy2024vector,simsearchnet}. To be useful as a range search benchmark, a query set should roughly follow this distribution in the number of matches for each query. 

Another important aspect of suitability for range search is whether there exists a choice of radius that is ``robust'' in the sense that small perturbations of the radius do not result in a wildly different match distribution. Our later experiments show that datasets that are more robust by this measure tend to perform better over the baseline measures than ones which are more easily perturbed.

See Figure~\ref{fig:datasets} for a description of each dataset used in our benchmarks.

\begin{figure*}
\small
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Dataset & Metric & Dimension & \begin{tabular}{c} Query \\ Size \end{tabular}  & \begin{tabular}{c} Selected \\ Radius \end{tabular} & Source  \\
\hline
BIGANN & L2 & 128 (uint8)& 10000 & 10000 & SIFT descriptors of images~\cite{jegou2010product} \\
\hline
DEEP & L2 & 96 (float)& 10000 & .02 & Image embeddings from the last fully-connected  layer of the GoogLeNet model~\cite{Deep1B}  \\
\hline
MSTuring & L2 & 100 (float) & 100000 & .3  & Bing queries encoded by Turing AGI v5~\cite{msturing}\\
\hline
GIST & L2 & 968 (float) & 10000 & .5 & GIST descriptors of images~\cite{jegou2010product}  \\
\hline
SSNPP & L2 & 200 (uint8) & 100000 & 96237 & SimSearchNet++ encodings of images~\cite{simsearchnet}\\
\hline
OpenAI & L2 & 1536 (float) & 10000 & .2 & ArXiv articles embedded using the OpenAI  text-embedding-ada-002 model~\cite{openai}   \\
\hline
Text2Image & IP & 200 (float) & 100000 & -.6 &  Image embeddings produced by  the Se-ResNext-101 model~\cite{Deep1B}      \\
\hline
Wikipedia & IP & 768 (float) & 5000 & -10.5 & Wikipedia articles embedded from the  cohere.ai multilingual 22-12 model~\cite{wikipedia}   \\
\hline
MSMARCO & IP & 768 (float) & 9374 & -62 & Clicked document-query pairs from the ClueWeb22 website corpus~\cite{msmarco} \\
\hline
\end{tabular}
\caption{Descriptions of each dataset used in our experiments, along with the radius used for range search. Every dataset is publicly available~\cite{bigann,siftgist}. The SSNPP dataset is a range search benchmark, while the other datasets were originally published as top-$k$ benchmarks.}\label{fig:datasets}
\end{figure*}

\subsection{Density of Matches by Dataset}
We begin our investigation by computing a measure of density of each dataset, in order to both find a suitable radius and determine whether the choice of radius is ``robust" in the sense that small changes to the radius do not result in drastically more points in each ball around a query point. To do this, we take each dataset and vary the radius, recording what we refer to as the ``percent captured'' for each radius---that is, for each query point $q$ and radius $r$, what fraction of the dataset is captured in the ball of radius $r$ around $q$?

\begin{figure}
	\centering
	\includegraphics[scale=.28]{figures/all_datasets_legend.pdf}

		\includegraphics[scale=.24]{figures/all_euclidean.pdf}
		\includegraphics[scale=.24]{figures/all_mips.pdf}

	\caption{Plots of radius versus percent captured for each dataset: Euclidean datasets on the left and inner product datasets on the right. Radius is normalized to enable displaying multiple datasets on one plot. Note that inner product values can be less than zero, so negative ranges of radius are present on the right-hand figure.}
	\label{fig:pctcaptured}
\end{figure}

This information is presented for each dataset in Figure~\ref{fig:pctcaptured}, which shows Euclidean and inner product datasets separately and plots the percent captured from the largest radius achieving 0\% capture to the smallest radius achieving 100\% capture. As shown in the figure, there is wide variation in each dataset's response to perturbations in the radius. Datasets BIGANN, GIST, DEEP, Wikipedia, and MSMARCO are the most robust to perturbations in the interesting range (for million size datasets, the applicable range is around $10^{-6}$ to $10^{-5}$, as most query points will have no results). In these datasets, it is thus less likely that there will be significant numbers of points near the boundary of the ball of radius $r$, and thus less work to distinguish points that are just inside the boundary from those which are just outside the boundary. We will see this insight at work in the experimental results in Section~\ref{sec:experiments}.

Using the insights from this experiment, we choose an appropriate radius for each dataset. The real (non-normalized) choices for each radius are shown in Figure~\ref{fig:datasets}.

\subsection{Frequency Distribution of Matches}

Given a choice of radius for each dataset, we now evaluate the frequency distribution of the number of range results for each query point. The distributions are found in Figure~\ref{fig:freqdistr}. We observe a general pattern of most queries having no results and a few large outliers, which is more pronounced in BIGANN, DEEP, MSTuring, and Text2Image, and less pronounced in SSNPP, OpenAI, Wikipedia, and MSMARCO. GIST is unusual among the datasets in having many extremely large outliers. For BIGANN, SSNPP, and Wikipedia, we also show the frequency distribution for a larger version of the same base dataset. Since these are effectively larger samples from the same distribution, we observe an increase in density of range results. 

\begin{figure}
	\small
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		Dataset & $0$ & $\leq 10^1$ & $\leq 10^2$ & $\leq 10^3$ & $\leq 10^4$ & $\leq 10^5$  \\
		\hline
		BIGANN-1M & 9728 & 143& 84  & 45 & 0 & 0 \\
		\hline
		BIGANN-10M & 9590 & 173 & 99 & 93 & 45 & 0 \\
		\hline
		BIGANN-100M & 9413 & 212 & 136 & 101 & 93 & 45 \\
		\hline
		DEEP-1M & 9923 & 56 & 19  & 2 & 0 & 0 \\
		\hline
		MSTuring-1M & 95716 & 2443 & 20 & 21 & 0 & 0 \\
		\hline
		GIST-1M & 8487 & 830 & 160  & 143 & 134  & 246 \\
		\hline
		SSNPP-1M & 97422 & 2424 &  254 & 0 & 0 & 0 \\
		\hline
		SSNPP-10M & 91575 & 7310 & 954  & 161 & 0 & 0 \\
		\hline
		SSNPP-100M & 80795 & 13719 & 4357  & 971 & 158  & 0\\
		\hline
		OpenAI-1M & 7030 & 2564 & 372 & 34 & 0 & 0\\
		\hline
		Text2Image-1M & 99327 & 669 & 4 & 0 & 0 & 0 \\
		\hline
		Wikipedia-1M & 4445 & 482 & 73 & 0 & 0 & 0 \\
		\hline
		Wikipedia-10M & 3385 &  1328 & 277 & 10 & 0 & 0 \\
		\hline
		MSMARCO-1M & 7022 & 2199 & 152  & 3 & 0 & 0 \\
		\hline
	\end{tabular}
	\caption{Table showing the distribution of result sizes for each dataset. Note that not all datasets have the same number of query points; see Figure~\ref{fig:datasets} for the number of queries and corresponding radius for each dataset. No data point had more than $10^5$ results.}\label{fig:freqdistr}
\end{figure}





