\section{Experimental Results}\label{sec:experiments}

In this section, we conduct an experimental evaluation of our algorithms on the nine datasets covered in Section~\ref{sec:rangedata}. In addition to plotting the QPS and precision tradeoffs for each dataset, we evaluate the time breakdown of each algorithm and draw conclusions about the reasons for their effectiveness. 

\paragraph{Experimental Setup and Code} Our algorithms were implemented as extensions of the ParlayANN~\cite{parlayann} library, which is implemented in C++ using ParlayLib~\cite{parlay} for fork-join parallelism as well as standard building blocks such as sorting and filtering. The code will be made available after the anonymous review period. All experiments were conducted on an Azure Standard\_L32s\_v3 virtual machine, with a 3rd Generation Intel® Xeon® Platinum 8370C (Ice Lake) processor in a hyper-threaded configuration. It has 32 vCPUs available to the user and a 256 GB main memory. Experiments use all 32 vCPUs unless otherwise stated.

To produce a curve of QPS versus average precision, the starting beamwidth is varied on a number of searches and the Pareto frontier is reported.

\begin{figure*}
	\centering
	\includegraphics[scale=.35]{figures/legends/eight_datasets_legend.pdf}
	\includegraphics[scale=.35]{figures/legends/threealg_legend.pdf} \\
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/BIGANN-1M_ap.pdf}
		\caption{BIGANN-1M}\label{fig:bigannap}
	\end{subfigure} 
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/DEEP-1M_ap.pdf}
		\caption{DEEP-1M}\label{fig:deepap}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/MSTuring-1M_ap.pdf}
		\caption{MSTuring-1M}\label{fig:msturingap}
	\end{subfigure}\hfil
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/GIST-1M_ap.pdf}
		\caption{GIST-1M}\label{fig:gistap}
	\end{subfigure} 
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/OpenAI-1M_ap.pdf}
		\caption{OpenAI-1M}\label{fig:openaiap}
	\end{subfigure}\hfil
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/Text2Image-1M_ap.pdf}
		\caption{Text2Image-1M}\label{fig:t2iap}
	\end{subfigure} 
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/Wikipedia-1M_ap.pdf}
		\caption{Wikipedia-1M}\label{fig:wikiap}
	\end{subfigure}
	\begin{subfigure}{.24\textwidth}
		\includegraphics[scale=.24]{figures/qps_recall/MSMARCO-1M_ap.pdf}
		\caption{MSMARCO-1M}\label{fig:msmarcoap}
	\end{subfigure}
	\caption{Average precision vs QPS for eight datasets and three range search algorithms. For GIST-1M, the lines for doubling search and greedy search are very short due to even the smallest of initial beam sizes producing recall in the .999 range.}
	\label{fig:apgraphseight}
\end{figure*}

\begin{figure}
	\includegraphics[scale=.2]{figures/legends/threealg_twocol_legend.pdf} 
	\includegraphics[scale=.2]{figures/legends/scaling_legend.pdf} \\
	\includegraphics[scale=.17]{figures/qps_recall/ssnpp-1M_ap.pdf}
	\includegraphics[scale=.17]{figures/qps_recall/ssnpp-10M_ap.pdf}
	\includegraphics[scale=.17]{figures/qps_recall/ssnpp-100M_ap.pdf} \\
	\includegraphics[scale=.17]{figures/qps_recall/bigann-1M_ap.pdf}
	\includegraphics[scale=.17]{figures/qps_recall/bigann-10M_ap.pdf}
	\includegraphics[scale=.17]{figures/qps_recall/bigann-100M_ap.pdf} \\
	\caption{Average precision vs QPS for the SSNPP and BIGANN datasets. From left to right, slices of size 1 million, 10 million and 100 million.}
	\label{fig:sizescaling}
\end{figure}

\begin{figure*}
		\includegraphics[scale=.3]{figures/bar_charts/legend.pdf}
	\begin{subfigure}{.32\textwidth}
		\includegraphics[scale=.32]{figures/bar_charts/bigann-1M_ap.pdf}
		\caption{BIGANN-1M}\label{fig:bigannbar}
	\end{subfigure}
	\begin{subfigure}{.32\textwidth}
		\includegraphics[scale=.32]{figures/bar_charts/msmarco-1M_ap.pdf}
		\caption{MSMARCO-1M}\label{fig:msmarcobar}
	\end{subfigure}
	\begin{subfigure}{.32\textwidth}
		\includegraphics[scale=.32]{figures/bar_charts/gist-1M_ap.pdf}
		\caption{GIST-1M}\label{fig:gistbar}
	\end{subfigure}
	\caption{Figures breaking down the cost in seconds (single threaded time) of each type of search for three datasets for selected average precision. The label at the top of each column indicates the beam width of the initial search. Each collection from left to right shows: the beam search baseline, beam search followed by greedy search, beam search with early stopping followed by greedy search, beam search followed by doubling beam search, and beam search with early stopping followed by doubling beam search. Note that in some cases, the time spent on the second phase of search is so short that it is not visible. GIST-1M has only one selected recall, as every search setting yields .999 recall within a .001 tolerance; the beam search baseline is not present for GIST, since it cannot achieve the desired recall (see Figure~\ref{fig:gistap}).}\label{fig:barchart}
\end{figure*}

\begin{figure*}
	\centering
	\includegraphics[scale=.35]{figures/legends/earlystopping_legend.pdf}
	\includegraphics[scale=.35]{figures/legends/greedy_legend.pdf} \\
	\includegraphics[scale=.19]{figures/greedy_earlystopping/BIGANN-1M_greedy_ap.pdf}
	\includegraphics[scale=.19]{figures/greedy_earlystopping/DEEP-1M_greedy_ap.pdf}
	\includegraphics[scale=.19]{figures/greedy_earlystopping/ssnpp-1M_greedy_ap.pdf}
	\includegraphics[scale=.19]{figures/greedy_earlystopping/wikipedia-1M_greedy_ap.pdf}
	\includegraphics[scale=.19]{figures/greedy_earlystopping/msmarco-1M_greedy_ap.pdf}
	\caption{Average precision vs QPS for five selected datasets, showing greedy search without (solid line) and with (dashed line) early stopping.}
	\label{fig:earlystoppinggreedy}
\end{figure*}

\paragraph{QPS and Precision Tradeoffs} We begin our experimental evaluation by plotting queries per second (QPS) versus precision for all nine datasets. Data on all datasets except SSNPP can be found in Figure~\ref{fig:apgraphseight}; SSNPP is shown in Figure~\ref{fig:sizescaling}, which illustrates the effects of size scaling on the algorithms using datasets SSNPP and BIGANN. Overall, our algorithms generate improvement over the naive baseline on every dataset, but we find significant diversity in both the magnitude of the improvement and the relative ordering of the two algorithms presented in Section~\ref{sec:algorithms}. We find that DEEP, BIGANN, and MSTuring experience massive improvement over the baseline (close to 100x improvement in throughput in some cases), with the doubling beam search slightly faster than the greedy search on the 1 million scale, but significantly slower at the 100 million scale. OpenAI also sees large improvement in the realm of one order of magnitude, with greedy search and beam search having virtually identical performance. Wikipedia and MSMarco experience decent improvement with the greedy search somewhat outperforming doubling beam search. GIST achieves such significant improvement over the baseline due to its outlier status with respect to the frequency distribution of its matches, as it is the only dataset with hundreds of points with more than 10000 range results. Greedy search also significantly outperforms doubling beam search for GIST. Text2Image sees modest improvement below 90\% recall. SSNPP, which we investigate for three different sizes, first shows little to no improvement at the 1 million scale, then some improvement at the 10 million scale, then significant improvement at size 100 million.

\paragraph{Scaling} Figure~\ref{fig:sizescaling} examines the performance of our algorithms on two datasets at three different scales: 1 million, 10 million, and 100 million. Similarly to existing range search benchmarks, we use the same radius for each data scale, so each order of magnitude effectively represents a more dense space, and the average number of results per query point increases. Unsurprisingly, the advantage of our algorithms increases significantly over the baseline, which is only capable of returning up to about 1000 points per query before becoming hopelessly inefficient. Interestingly, we also see that the greedy search seems to have a significant advantage over the doubling beam search as the density of the dataset increases (this is also true of GIST-1M, which has some extraordinarily dense outliers). This is likely because in denser graphs, the greedy search is particularly adept at efficiently finding all points within the radius, which are likely to be in a dense connected cluster in the graph structure. Additional data on size scaling can be found in Appendix~\ref{apdx}.

\paragraph{Analyzing Time Spent on Algorithm Components} To aid in our analysis, we investigate the time spent on different phases of each algorithm in Figure~\ref{fig:barchart}. Each algorithm consists of an initial beam search phase---either with or without the early stopping criteria in Algorithm~\ref{alg:earlystopping}---and then a following phase of either greedy search or successive calls to doubling beam search. We show this visual breakdown for three datasets at selected fixed average precision: BIGANN-1M, MSMARCO-1M, and GIST-1M. 

The time breakdowns provide significant insight into the results in Figures~\ref{fig:apgraphseight} and~\ref{fig:sizescaling}. First, note that for all datasets, the initial beam search phase is a sizable portion of the overall cost of computation. This is because the initial beam search takes place on all points, but further searches only take place on points with one or more valid range result (since the distances used are exact, a point with zero results will never pass on to the second round of computation). 

\paragraph{Greedy Search vs Doubling Search} Next, let us examine the differences between doubling search and greedy search. Doubling search spends a significantly longer portion of time in the second phase of search compared to greedy search, but is capable of achieving higher recall from a smaller starting beam. The characteristics of each individual dataset seem to determine which approach provides the best tradeoff between QPS and average precision. Greedy search requires a larger initial beam to achieve a particular recall value, but the second round of search is extremely cheap as it only visits points within the radius. On the other hand, doubling beam search requires a smaller initial beam, but spends more time in the second phase of search, which performs comparatively more distance comparisons to points which are not valid range results. It also has some overheads related to repeated calls to the beam search function. Based on these factors and the experimental results, it appears that doubling search dominates greedy search when a) a small initial beam is sufficient for doubling search to achieve high recall, and b) a fairly small number of rounds of doubling search are required. These conditions are met with datasets BIGANN-1M (and DEEP-1M, although its time breakdown is not pictured), but GIST-1M requires too many rounds of doubling, and MSMARCO-1M requires too high an initial beam. We also see that in the scaling studies in Figure~\ref{fig:sizescaling}, the greedy search overtakes the doubling search as the density of matches increases, also likely because many rounds of doubling search become necessary.

\paragraph{Effects of Early Stopping} Finally, we consider the benefit of early stopping. Early stopping clearly decreases the time taken for a fixed beam; for example, in Figure~\ref{fig:bigannbar}, early stopping beam search with beam 95 takes less than half the time compared to beam search with beam 95 without early stopping. On the other hand, as illustrated in Figure~\ref{fig:msmarcobar}, early stopping can make the search significantly less accurate due to points with valid range results being terminated too early. The effects of early stopping on selected datasets are shown in Figure~\ref{fig:earlystoppinggreedy} and expanded on in Appendix~\ref{apdx}; overall, early stopping is beneficial on datasets where significant separation exists between the distributions of points with no results and points with one or more results, and not beneficial otherwise.

\paragraph{Comparison with Top-$k$ Search} We add a final note on comparison with top-$k$ search. To investigate this, we ran top-10 searches on OpenAI-1M and MSTuring-1M, and measured the QPS at 90\% 10@10 recall. For OpenAI-1M, we found the QPS at 90\% recall was around 5000, compared to around 10000 QPS for range search at 90\% precision. For MSTuring-1M, the QPS at 90\% recall for top-10 search was about 30000, compared to over 100000 for range search using our algorithms. This suggests that range benchmarking may actually be thought of as an easier problem than top-$k$ search.






