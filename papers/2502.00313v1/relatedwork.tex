\section{Related Work}
\paragraph{Theories of Human Preferences and Distributive Fairness.}
Different allocation principles, such as \textit{inequality aversion} (aka equitability) \citep{F&S00,B&O00}, Rawlsian maximin (RMM) \citep{RawlsTOJ}, welfare maximization, and combinations of multiple principles \citep{C&R02}, prove to be effective in characterizing human behavior in a variety of settings such as ultimatum and dictator games, and income distribution scenarios \citep{Frohlich87,KRITIKOS2001,E&S04}. In the presence of information about the identity of individuals and groups, it is observed that the needs and merit of recipients influence allocation decisions \citep{Konow03,Overlaet1991,Gaertner200}. For subjectively valued goods, i.e. those for which utility is non-transferrable, studies on both procedural justice (fair mechanisms) \citep{KYROPOULOU202228,Schnieder04,dupuis2011simpler} and distributive justice (fair outcomes) \citep{H&P07,HP10,Gates2020} reveal that people's perception of fairness is dependent on multiple aspects. What people consider fair has also been explored in a variety of real-world scenarios such as the division of inheritance \citep{Pratt90}, rent \citep{gal2016rent}, food donations \citep{LeeFR17,LeeWBAI19}, and conflicted territories \citep{BramsSpratly,BramsDavid,Massoud02}. 

\paragraph{LLMs as Social and Economic Agents.}
A growing line of research focuses on whether LLMs can substitute for human decision-makers \citep{raman2024rationality,zhang2024llm, scherrer2024evaluating} or simulate human behavior \citep{CharnessLLM2023,gui2023challenge,goli2023can} in social and economic contexts. LLMs display human-like traits such as generosity and a concern for fairness, in ultimatum \citep{ross2024llm,MeiTuring24,AherICML} and single-round dictator games \citep{horton2023large}, as well as reciprocity and social learning in multi-round dictator games \citep{leng2023llm}. 
Newer versions of LLMs (such as GPT-4 and Llama3), like humans, are less cooperative as compared to older versions (GPT-3.5 and Llama2) in repeated prisoner's dilemma games \citep{akata2023playing,guo2023gpt,fontana2024nicer}. Additionally, LLMs demonstrate an ability to negotiate in buyer-seller pricing scenarios \citep{bianchi2024well} as well as collude to their benefit as pricing agents in oligopoly settings \citep{fish2024algorithmic}. 

\paragraph{Rationality and Reasoning in LLMs.} Prior research shows that LLMs struggle with complex reasoning \citep{Chang2024Survey,hadi2023survey} and rely on surface-level patterns in training data \citep{Dziri2023Faith,mondorf2024beyond}. They are also unable to identify causal relationships between economic events \citep{guo2024econnli,quan2024econlogicqa}, 
or choose optimal actions in strategic games involving a larger space of possible actions \citep{gandhi2023strategic}. %
LLMs are more capable than humans in terms of maximizing utility in ultimatum and gambling games \citep{ross2024llm} as well as in budgeting settings \citep{ChenRevealedPreference2023}; some  (e.g. GPT-4) can ensure desirable outcomes in auction games \citep{chen2023put} and show reasonable performance in bargaining games \citep{hua2024game}. 
Yet, it is unclear whether they can serve as \textit{fair preference aggregators}, a task involving more complex multi-step reasoning and evaluation of possibilities. %


\paragraph{Social choice theory and generative AI.} 
Recent works have emphasized the need for value alignment to aggregate the often conflicting preferences of human annotators during the fine-tuning stage \citep{conitzer2024social}, leveraging the power of generative AI to augment or extrapolate preferences in social choice theory \citep{fish2024generative}, and adopting an axiomatic approach to AI alignment \citep{ge2024axioms}.
Our work focuses on evaluating the alignment of popular LLMs with human perceptions of fairness, and uncovering their fairness characteristics.
In the fine-tuning process that is guided by preference aggregation in social choice theory \cite{conitzer2024social,fish2024generative}, the extent to which these models represent human and social values remains understudied.