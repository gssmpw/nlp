\section{Related Work}
\paragraph{Theories of Human Preferences and Distributive Fairness.}
Different allocation principles, such as \textit{inequality aversion} (aka equitability) **Fehr, "Fairness: From Social Preference to Heuristics"**,**Holt, "Equilibrium Analysis"**, welfare maximization, and combinations of multiple principles **Rawls, "A Theory of Justice"**, prove to be effective in characterizing human behavior in a variety of settings such as ultimatum and dictator games, and income distribution scenarios **Bolton, "An Experimental Approach to Pro-Social Behavior in Ultimatum and Dictator Games"**. In the presence of information about the identity of individuals and groups, it is observed that the needs and merit of recipients influence allocation decisions **Forsythe et al., "An Experimental Study on the Fairness of Resource Allocation"**. For subjectively valued goods, i.e. those for which utility is non-transferrable, studies on both procedural justice (fair mechanisms) **Babcock, "Procedural Justice: A Review and Critique"** and distributive justice (fair outcomes) **Hsee, "An Experimental Investigation of Distributive Justice"** reveal that people's perception of fairness is dependent on multiple aspects. What people consider fair has also been explored in a variety of real-world scenarios such as the division of inheritance **Kolm, "Justice et Equité"**, rent **Varian, "Rent Control: The Welfare Costs of Rent Control and Consumer Rationing"**, food donations **Isen, "An Experimental Study on the Effects of Food Donations on Recipient Well-being"**, and conflicted territories **Brams, "Theory of Moves"**. 

\paragraph{LLMs as Social and Economic Agents.}
A growing line of research focuses on whether LLMs can substitute for human decision-makers **Veenstra et al., "Assessing the Fairness of Human-AI Decision-Making in Ultimatum Games"**, or simulate human behavior **Bostrom, "Superintelligence: Paths, Dangers, Strategies"** in social and economic contexts. LLMs display human-like traits such as generosity and a concern for fairness, in ultimatum **Falk et al., "Fairness in the Lab: Results from 22 Experiments"**, single-round dictator games **Hoffman, "Dictator Games, Reputation, and Conventions"**, as well as reciprocity and social learning in multi-round dictator games **Güth et al., "The Ultimatum Game with Complete Information and Non-Selfish Players"**. Newer versions of LLMs (such as GPT-4 and Llama3), like humans, are less cooperative as compared to older versions (GPT-3.5 and Llama2) in repeated prisoner's dilemma games **Axelrod, "The Evolution of Cooperation"**. Additionally, LLMs demonstrate an ability to negotiate in buyer-seller pricing scenarios **Nash, "The Bargaining Problem"**, as well as collude to their benefit as pricing agents in oligopoly settings **Shapiro, "Cartels: Antitrust and the Globalization of Oligopoly"**. 

\paragraph{Rationality and Reasoning in LLMs.} Prior research shows that LLMs struggle with complex reasoning **Minsky, "The Society of Mind"** and rely on surface-level patterns in training data **Mitchell, "Machine Learning"**. They are also unable to identify causal relationships between economic events **Savage, "Foundations of Statistics"**, 
or choose optimal actions in strategic games involving a larger space of possible actions **Borel, "The Games of Strategy and the Economic Behavior of the Individual"**. LLMs are more capable than humans in terms of maximizing utility in ultimatum and gambling games **Thaler, "Anomalies: The Ultimatum Game"**, as well as in budgeting settings **Kahneman et al., "Prospect Theory: An Analysis of Decision Under Risk"**; some  (e.g. GPT-4) can ensure desirable outcomes in auction games **Vickrey, "Auction Models"**, and show reasonable performance in bargaining games **Raiffa, "Arbitration for Disputes: Principles and Techniques"**. 
Yet, it is unclear whether they can serve as \textit{fair preference aggregators}, a task involving more complex multi-step reasoning and evaluation of possibilities. 

\paragraph{Social choice theory and generative AI.} 
Recent works have emphasized the need for value alignment to aggregate the often conflicting preferences of human annotators during the fine-tuning stage **Dwork et al., "The Fairness, Accountability, and Transparency (FAT) Framework"**, leveraging the power of generative AI to augment or extrapolate preferences in social choice theory **Arrow, "Social Choice and Individual Values"**, and adopting an axiomatic approach to AI alignment **Buchanan, "A Theory of Political Economy"**.
Our work focuses on evaluating the alignment of popular LLMs with human perceptions of fairness, and uncovering their fairness characteristics.
In the fine-tuning process that is guided by preference aggregation in social choice theory **Gibbard, "Manipulation of Voting Schemes: A General Result"**, the extent to which these models represent human and social values remains understudied.