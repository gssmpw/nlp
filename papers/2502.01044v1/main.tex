\documentclass[AMA,Times1COL]{WileyNJDv5pdflatex}
% ---- pdfLaTeX 用に追加するとよい設定: ----
\pdfoutput=1  % This line should be uncommented when uploaded to arXiv.or
\usepackage[T1]{fontenc}         % フォントエンコーディング
% ---------------------------------------

\usepackage{subcaption}

\articletype{Research Article}%

\received{Date Month Year}
\revised{Date Month Year}
\accepted{Date Month Year}
%\journal{Journal}
\volume{00}
\copyyear{2025}
\startpage{1}

\raggedbottom



\begin{document}

\title{Nonlinear receding-horizon differential game for drone racing along a three-dimensional path}


\author[1]{Kijin Sung}
\author[1]{Kenta Hoshino}
\author[2]{Akihiko Honda}
\author[2]{Takeya Shima}
\author[1]{Toshiyuki Ohtsuka}

\authormark{Nonlinear receding-horizon differential game for drone racing along a three-dimensional path}

\address[1]{\orgdiv{Graduate School of Informatics}, \orgname{Kyoto University},\orgaddress{\state{Kyoto}, \country{Japan}}} 
\address[2]{\orgdiv{Advanced Technology R\&D Center}, \orgname{Mitsubishi Electric Corporation},\orgaddress{\state{Hyogo}, \country{Japan}}}

\corres{Toshiyuki Ohtsuka, Department of Informatics, Graduate School of Informatics, Kyoto University, Kyoto 606-8501, Japan. Email: ohtsuka@i.kyoto-u.ac.jp}

% ORCID
% Kenta Hoshino https://orcid.org/0000-0003-0562-2733
% Toshiyuki Ohtsuka https://orcid.org/0000-0003-3554-8933



%\fundingInfo{Text}
%\JELinfo{ejlje}

\abstract[Abstract]{Drone racing involves high-speed navigation of three-dimensional paths, posing a substantial challenge in control engineering. 
This study presents a game-theoretic control framework, the nonlinear receding-horizon differential game (NRHDG), designed for competitive drone racing. 
NRHDG enhances robustness in adversarial settings by predicting and countering an opponent's worst-case behavior in real time. It extends standard nonlinear model predictive control (NMPC), which otherwise assumes a fixed opponent model. 
First, we develop a novel path-following formulation based on projection point dynamics, eliminating the need for costly distance minimization. 
Second, we propose a potential function that allows each drone to switch between overtaking and obstructing maneuvers based on real-time race situations. 
Third, we establish a new performance metric to evaluate NRHDG with NMPC under race scenarios.
Simulation results demonstrate that NRHDG outperforms NMPC in terms of both overtaking efficiency and obstructing capabilities. }



\keywords{drone control, path following, differential game, model predictive control}

% \jnlcitation{\cname{%
% \author{Taylor M.},
% \author{Lauritzen P},
% \author{Erath C}, and
% \author{Mittal R}}.
% \ctitle{On simplifying ‘incremental remap’-based transport schemes.} \cjournal{\it J Comput Phys.} \cvol{2021;00(00):1--18}.}


\maketitle

% \renewcommand\thefootnote{}
% \footnotetext{\textbf{Abbreviations:} ANA, anti-nuclear antibodies; APC, antigen-presenting cells; IRF, interferon regulatory factor.}

\renewcommand\thefootnote{\fnsymbol{footnote}}
\setcounter{footnote}{1}



\section{Introduction} \label{sec:intro} 

Drone racing is an emerging field in robotics that requires drones to navigate three-dimensional paths at high speeds with precision.\cite{race0} 
These races pose significant control challenges, requiring trajectory adjustments under nonlinear dynamics. 
Although recent advances in trajectory optimization have enabled precise gate passages and energy-efficient maneuvers,\cite{race1} existing methods have predominantly focused on single-drone scenarios. 
Similarly, learning-based approaches can navigate moving gates;\cite{race2} however, they often overlook the multi-agent competitive interactions. 

To address these competitive interactions, several studies\cite{Spica2020,Wang2020,race3} have formulated drone racing as receding-horizon differential games (RHDG). 
Unlike standard model predictive control (MPC), 
RHDG predicts the opponent's worst-case actions over a finite horizon, thus optimizing the ego drone's strategy accordingly. 
In particular, the nonlinear receding-horizon differential game (NRHDG) \cite{race3} handles fully nonlinear dynamics, rather than relying on simplified kinematic models.\cite{Spica2020,Wang2020} 
However, this approach still assumes fixed roles (i.e., overtaking or obstructing) for each drone, limiting the ability to switch strategies during a race. 
Moreover, relying on explicit path features, such as curvature and torsion, can increase computational costs and lead to potential numerical singularities.

In this study, we enhance the conventional NRHDG framework to facilitate dynamic role-switching and reduce reliance on cumbersome path information. 
Furthermore, we introduce a novel path-following model that integrates seamlessly with the drone's state equation without iterative minimization, and we devise a potential function that allows each drone to flexibly move between overtaking and obstructing modes in real time. 
Finally, we introduce a systematic performance metric to evaluate NRHDG with standard nonlinear MPC (NMPC). Simulation results confirm that NRHDG outperforms NMPC in both overtaking efficiency and obstructing performance along a three-dimensional path.

The main contributions of this work can be summarized as follows.
\begin{enumerate}
    \item We derive a unified dynamical model for projection onto a three-dimensional path. Unlike existing formulations requiring distance minimization or frame-specific parameters, our model embeds directly into the drone's state equation and applies to any smooth curve of arbitrary dimension.
    \item We propose a custom potential function for NRHDG that enables real-time switching between overtaking and obstructing strategies, adapting to each drone's relative position.
    \item We introduce a performance metric for evaluating the effectiveness of overtaking and obstructing in drone racing. Numerical simulations demonstrate that NRHDG outperforms NMPC.
\end{enumerate}

The first contribution of the above offers a unified and computationally efficient framework for \emph{path-following control},\cite{Sujit2014,Rubi2020,Hung2023} one of the practically important problems in control engineering. 
Conventional path-following control methods involve iterative searches or approximations of an orthogonal projection (a \emph{projection point}) of the vehicle's position to the path \cite{Hanson1995,Brito2019,Wang2022,Santos2023,Romero2022} or are limited to two-dimensional paths.\cite{Altafini2002,Okajima2007} 
Other methods define reference points on the path arbitrarily by geometric relationships \cite{Amundsen2023,Reinhardt2023,Xu2024,Degorre2024} such as \emph{carrot chasing} or \emph{timing laws} \cite{Faulwasser2009,Faulwasser2016} as additional degrees of freedom. 
Furthermore, there are different settings of path-following control based on implicit function representations of paths \cite{Hladio2013,Chen2021,Itani2024} or vector fields.\cite{Shivam2021} However, arbitrary reference points do not represent the actual distance from the path, and implicit function representations and vector fields are often difficult to construct and apply in general. 
In contrast, the proposed dynamical model of the projection point provides a computationally efficient and unified formulation for path-following control, making it particularly suited for dynamic and complex environments such as drone racing.

The remainder of this paper is organized as follows. In Section 2, we derive dynamical models of a drone and projection of the drone's position onto a three-dimensional path. 
In Section 3, we give an overview of NMPC and NRHDG. 
In Section 4, we propose objective functions for NMPC and NRHDG, including a potential function for role-switching. 
In Section 5, we introduce a performance metric to evaluate controllers and perform numerical simulations of races along a three-dimensional path, demonstrating the advantages of NRHDG over NMPC. Finally, in Section 6, we state the conclusions of this study and discuss future work. 




\section{Modeling} \label{sec:model} 

\subsection{Dynamics of drone} \label{subsec:model_drone}

In this section, we consider a quadrotor-type drone whose dynamical model is derived from reference.\cite{Nonami2020} 
The inertial frame is denoted by $(e^i_1, e^i_2, e^i_3)$, and the body frame by $(e^b_1, e^b_2, e^b_3)$ with origin at the drone's center of mass, as shown in Figure \ref{fig:model}. 
Attitude is parameterized by the quaternion\cite{Choset2005} $q= (q_{0}\ q_{1}\ q_{2}\ q_{3})^{\rm T} \in \mathbb{R}^{4}$. 
The rotation matrix $Q(q) \in \mathbb{R}^{3 \times 3}$ from the body frame to the inertial frame is formed as follows:
\begin{equation}
Q(q) = 
\left(\begin{array}{ccc}
    q_{0}^{2}+q_{1}^{2}-q_{2}^{2}-q_{3}^{2} & 2({q}_{1}{q}_{2}-{q}_{0}{q}_{3})& 2({q}_{0}{q}_{2}+{q}_{1}{q}_{3}) \\
    2(q_{1}q_{2}+q_{0}q_{3})& q_{0}^{2}-q_{1}^{2}+q_{2}^{2}-q_{3}^{2} &   2(q_{2}q_{3}-q_{0}q_{1})\\
    2(q_{1}q_{3}-q_{0}q_{2})& 2(q_{0}q_{1}+q_{2}q_{3})& q_{0}^{2}-q_{1}^{2}-q_{2}^{2}+q_{3}^{2} 
\end{array}\right).
\end{equation}

\begin{figure}[tbp]
  \centering
  \includegraphics[height=0.2\textheight]{model_new.png}
  \caption{Overview of the inertial frame and body frame.} \label{fig:model}
\end{figure}

We assume that the external forces acting on the drone consist only of gravity in the $e^{i}_3$ direction and rotor thrusts in the $e^{b}_{3}$ direction, and each rotor generates a reaction torque that is proportional to its thrust. 
Then, the Newton-Euler equations describing translational motion in the inertial frame and rotational motion in the body frame for the drone are expressed as follows. 
\begin{equation}
  \left\{
    \begin{alignedat} {1}
      & m \ddot{p}_d = Q_3(q) e_a^{\mathrm{T}} u_{d} - mg e^{i}_3 \\
      & J \dot{\omega} + \omega \times J \omega = T u_d,
    \end{alignedat}
  \right. \label{eq:N-E}
\end{equation}
where $p_d = (x ~ y ~ z)^{\mathrm{T}} \in \mathbb{R}^{3}$ represents the drone's position in the inertial frame, $\omega = (\omega_{1} ~ \omega_{2} ~ \omega_{3})^{\mathrm{T}} \in \mathbb{R}^{3}$ is the angular velocity vector in the body frame, and $u_d = (F_1 ~ F_2 ~ F_3 ~ F_4)^{\mathrm{T}} \in \mathbb{R}^{4}$ is the vector of rotor thrusts. Symbols $m$ and $J \in \mathbb{R}^{3 \times 3}$ are the mass and inertia matrix of the drone, respectively, $Q_3(q)$ denotes the third column of $Q(q)$, $e_a = (1 \ 1 \ 1 \ 1)^{\mathrm{T}}$, and $T \in \mathbb{R}^{3 \times 4}$ is a matrix 
\begin{equation}
    T = \left(
    \begin{array}{cccc}
        0 & l & 0 & -l \\
        -l & 0 & l & 0 \\
        k & -k & k & -k \\
    \end{array}
    \right),
\end{equation}
with $l$ the distance from the center of mass to each rotor, and $k$ the proportion constant relating thrust to reaction toque. 
%
The time derivative of the quaternion $\dot{q}$ is given by
\begin{equation}
  \dot{q} = \frac{1}{2} \varOmega(\omega) q \label{eq:qdot},
\end{equation}
where $\varOmega(\omega) \in \mathbb{R}^{4 \times 4}$ is the following skew-symmetric matrix
\begin{equation}
  \varOmega(\omega) = \left(
    \begin{array}{cccc}
      0 & -\omega_{1} & -\omega_{2} & -\omega_{3} \\
      \omega_{1} & 0 & \omega_{3} & -\omega_{2} \\
      \omega_{2} & -\omega_{3} &0 & \omega_{1} \\
      \omega_{3} & \omega_{2} & -\omega_{1} & 0
    \end{array}
  \right).
\end{equation}
%
We now define the 13-dimensional state vector $x_d=( p_d^{\textrm{T}} \ \dot{p}_d^{\textrm{T}} \ \omega^{\textrm{T}} \ q^{\textrm{T}} ) ^{\textrm{T}}$. Combining Eqs.\ (\ref{eq:N-E}) and (\ref{eq:qdot}), the state equation for the drone is given as follows: 
\begin{equation}
\dfrac{d}{dt}
   \left(
    \begin{array}{c}
      p_d \\ \dot{p}_d \\ \omega \\ q
    \end{array}
  \right)
= \left(
    \begin{array}{c}
      \dot{p}_d \\
      \frac{1}{m}Q_3(q) e_a^{\mathrm{T}} u_d - ge^{i}_{3} \\ -J^{-1}(\omega \times J\omega - T u_d) \\
      \frac{1}{2} \varOmega(\omega) q
    \end{array}
  \right) \label{eq:stateeq_drone}.
\end{equation}


\subsection{Dynamics of projection point and arc length}

Subsequently, in this section, we analyze the relationship between the drone's position and the path it should follow. 
By formulating the dynamics of a projection point and its associated arc length, we establish a foundation for efficient path-following control. 
We assume that the path is given as a curve parameterized by a \emph{path parameter} $\theta$ over an interval $\Theta \subset \mathbb{R}$. 
That is, the path is represented as the image $r(\Theta) \subset \mathbb{R}^3$ of a mapping $r : \Theta  \to \mathbb{R}^3$. 
We assume $r$ is twice differentiable and the tangent vector $dr(\theta)/d\theta$ does not vanish for any $\theta \in \Theta$, which guarantees that $r(\theta)$ does not move backwards when $\theta$ increases. 
However, we do not assume that the mapping $r$ is one-to-one globally, allowing the path to go through a point multiple times. 
The path parameter $\theta$ is not necessarily the arc length of the path, which allows us to have explicit representations of various paths. 

We define the distance $d(p_d)$ from the drone's position $p_d \in \mathbb{R}^3$ to the path $r(\Theta)$ by the minimum distance as 
\begin{equation}
    d(p_d) = \inf_{\theta \in \Theta} \| r(\theta) - p_d \| . 
\end{equation}
If this infimum is attained at $\theta_d$ in the interior of $\Theta$, then the stationary condition 
\begin{equation}
     (r(\theta_d) - p_d)^{\mathrm{T}} \dfrac{dr(\theta_d)}{d\theta} = 0 \label{eq:proj}
\end{equation}
holds. 
For $\theta_d$ satisfying (\ref{eq:proj}), we call a point $p_p = r(\theta_d)$ a \emph{projection point} of $p_d$ because (\ref{eq:proj}) implies that $p_p$ is an orthogonal projection of $p_d$ onto the path, as shown in Figure \ref{fig:path}. 
We also define the \emph{signed arc length} $s(\theta_0,\theta_1)$ from $r(\theta_0)$ to $r(\theta_1)$ along the path as 
\begin{equation}
    s(\theta_0,\theta_1) = \int_{\theta_0}^{\theta_1} \left\| \dfrac{dr(\theta)}{d\theta} \right\| d\theta 
\end{equation}
for $\theta_0, \theta_1 \in \Theta$, which is negative when $\theta_1 < \theta_0$.
If the trajectory $p_d(t)$ of the drone is differentiable in time $t \in [0,t_f]$ $(t_f > 0)$, we can derive ordinary differential equations for $\theta_d(t)$ and the corresponding arc length $\sigma(t) = s(\theta_d(0),\theta_d(t))$. 

\begin{figure}[tbp]
  \centering
  \includegraphics[height=0.23\textheight]{path_drone-crop.pdf}
  \caption{Relationship between drone and path.} \label{fig:path}
\end{figure}

\begin{theorem}\label{thm:proj_ode}
Suppose the path $r: \Theta \to \mathbb{R}^3$ is twice differentiable with $dr(\theta)/d\theta \neq 0$ for any $\theta \in \Theta$, and the trajectory $p_d: [0,t_f] \to \mathbb{R}^3$ of the drone is differentiable for any $t \in [0,t_f]$. 
If $\theta_d(t)$ is a solution of a differential equation  
\begin{equation}
    \dot{\theta}_d(t) = \dfrac{\dot{p}_{d}^{\mathrm{T}}(t) \dfrac{dr(\theta_d(t))}{d\theta}}{\left\| \dfrac{dr(\theta_d(t))}{d\theta} \right\|^2 + (r(\theta_d(t)) - p_d(t))^{\mathrm{T}}\dfrac{d^2 r(\theta_d(t))}{d\theta^2} } 
 \label{eq:thetadot}
\end{equation}
with its initial value $\theta_d(0)$ satisfying (\ref{eq:proj}), and 
\begin{equation}
    \left\| \dfrac{dr(\theta_d(t))}{d\theta} \right\|^2 + (r(\theta_d(t)) - p_d(t))^{\mathrm{T}} \dfrac{d^2 r(\theta_d(t))}{d\theta^2} > 0  \label{eq:thetadot_denom}
\end{equation}
holds for any $t \in [0,t_f]$, then $p_p(t) = r(\theta_d(t))$ is a projection point with a local minimum distance from the drone's position $p_d(t)$ for all $t \in [0,t_f]$. 
%
Moreover, the arc length $\sigma(t) = s(\theta_d(0),\theta_d(t))$ of the projection point $p_p(t)$ along the path satisfies a differential equation
\begin{equation}
    \dot{\sigma}(t) = \left\| \dfrac{dr(\theta_d(t))}{d\theta} \right\| \dot{\theta}_d(t)   \label{eq:sdot}
\end{equation}
for all $t \in [0,t_f]$ with the initial condition $\sigma(0) = 0$. 
\end{theorem}

\begin{proof}
We obtain (\ref{eq:thetadot}) by differentiating (\ref{eq:proj}) with respect to time. 
In other words, (\ref{eq:thetadot}) implies that the time derivative of the left-hand side of (\ref{eq:proj}) is identically zero. 
Since the left-hand side of (\ref{eq:proj}) is equal to zero at $t=0$ by the assumption on the initial value $\theta_d(0)$, it remains zero for all $t \in [0,t_f]$ provided that (\ref{eq:thetadot}) holds. 
Moreover, (\ref{eq:thetadot_denom}) implies that the second-order derivative of $\| r(\theta) - p_d \|^2$ with respect to $\theta$ is always positive for $\theta = \theta_d(t)$ and $p_d(t)$ for all $t \in [0,t_f]$. 
Therefore,  (\ref{eq:proj}) and (\ref{eq:thetadot_denom}) imply that $\theta_d(t)$ is a local minimizer of $\| r(\theta) - p_d \|^2$ for each $t \in [0,t_f]$. 
Note that the sufficient conditions for a local minimizer are valid even when $\theta_d$ is at the boundary of $\Theta$.\cite{Nocedal1999} 
Finally, by differentiating $\sigma(t) = s(\theta_d(0),\theta_d(t))$ with respect to time, we obtain (\ref{eq:sdot}) and observe that $\sigma(0) = 0$ by definition. 
\end{proof}

\begin{remark}
Theorem \ref{thm:proj_ode} ensures that the path parameter $\theta_d(t)$ and the arc length $\sigma(t)$ of the projection point $p_p(t)$ can be computed dynamically by integrating differential equations (\ref{eq:thetadot}) and (\ref{eq:sdot}), without requiring iterative search or approximation in conventional methods.\cite{Hanson1995,Brito2019,Wang2022,Santos2023,Romero2022} 
This formulation simplifies path-following control by eliminating computational overhead while maintaining generality across three-dimensional paths.
It also does not involve specific frames along the path, such as the Frenet-Serret frame, and is applicable to any dimension, in contrast to the other existing methods.\cite{Altafini2002,Okajima2007} 
\end{remark}

\begin{remark}
The singularity in the differential equations (\ref{eq:thetadot}) and (\ref{eq:sdot}) occurs when the denominator of (\ref{eq:thetadot}) vanishes, which is avoided if 
\begin{equation}
     \| r(\theta_d(t)) - p_d(t) \| \left \| \dfrac{d^2 r(\theta_d(t))}{d\theta^2} \right\| < \left\| \dfrac{dr(\theta_d(t))}{d\theta} \right\|^2   \label{eq:nonsingular_cond}
\end{equation}
holds, that is, if the deviation or the curvature of the path is sufficiently small. 
Moreover, Theorem \ref{thm:proj_ode} also guarantees the local minimum distance of the projection point if the singularity does not occur, which is the best possible guarantee without global search. 
\end{remark}

\begin{remark}
If the path $r$ is defined to satisfy $\| dr(\theta)/d\theta \| = 1$, then $\dot{\sigma} = \dot{\theta}_d$ and $\sigma(t) = \theta_d(t) - \theta_d(0)$ hold, and (\ref{eq:sdot}) can be omitted. 
However, it is often difficult to obtain such a representation with the particular property explicitly. Therefore, (\ref{eq:sdot}) is useful in practical applications. 
\end{remark}


\subsection{Augmented state equation for path following}

To incorporate path-following dynamics into the drone's control framework, we augment the state vector $x_d$ of the drone with the path parameter $\theta_d$ and the arc length $\sigma$. 
This extension enables seamless integration of path-following errors into the control design without requiring additional optimization steps. 
Specifically, since the right-hand sides of the differential equations Eqs.\ (\ref{eq:thetadot}) and (\ref{eq:sdot}) depend on $\theta_d(t)$, $p_d(t)$, and $\dot{p}_d(t)$, we can integrate them into the state equation of the drone. 
Subsequently, by augmenting the state vector as $\bar{x}_d = ( p_d^{\textrm{T}} \ \dot{p}_d^{\textrm{T}} \ \omega^{\textrm{T}} \ q^{\textrm{T}} \ \theta_d \ \sigma ) ^{\textrm{T}} \in \mathbb{R}^{15}$, we obtain the augmented state equation for path following as follows: 
\begin{equation}
\dfrac{d}{dt}
   \left(
    \begin{array}{c}
      p_d \\ \dot{p}_d \\ \omega \\ q \\ \theta_d \\[2mm] \sigma
    \end{array}
  \right)
  = \left(
    \begin{array}{c}
      \dot{p}_d \\
      \frac{1}{m}Q_3(q) e_a^{\mathrm{T}} u_d - ge^{i}_{3} \\ -J^{-1}(\omega \times J\omega - T u_d) \\
      \frac{1}{2}\varOmega(\omega) q \\
      \frac{\dot{p}_{d}^{\mathrm{T}} (dr(\theta_d)/d\theta)}{\left\| dr(\theta_d)/d\theta \right\|^2 + (r(\theta_d) - p_d)^{\mathrm{T}} (d^2 r(\theta_d)/d\theta^2) }  \\
      \left\| \frac{r(\theta_d)}{d\theta} \right\| \frac{\dot{p}_{d}^{\mathrm{T}} (dr(\theta_d)/d\theta)}{\left\| dr(\theta_d)/d\theta \right\|^2 + (r(\theta_d) - p_d)^{\mathrm{T}} (d^2 r(\theta_d)/d\theta^2) }
    \end{array}
  \right) \label{eq:stateeq_pathF}.
\end{equation}
Equation (\ref{eq:stateeq_pathF}) defines the augmented state equation for the drone, including the dynamics of the projection point ($\theta_d$) and the associated arc length ($\sigma$). 
The additional terms allow the control system to directly account for path-following errors in real-time decision-making.




\section{Control methods} \label{sec:control} 

In competitive drone racing, the control system must address both the drone's nonlinear dynamics and potential adversarial behavior from an opponent. 
This section discusses two suitable approaches, which are NMPC and NRHDG. 

\subsection{Nonlinear model predictive control}

In this section, we first review NMPC, a widely used real-time optimization-based control approach for nonlinear systems, in a continuous-time setting.\cite{jitsuyou}
We consider a dynamical system
\begin{equation}
	\dot{x}_M(t) = f_M(x_M(t), u(t), t), 
\end{equation}
where \(x_M(t)\) is the state and \(u(t)\) is the control input. 
To determine the control input $u(t)$ at each time instant, NMPC solves a finite-horizon optimal control problem (OCP) to minimize the following objective function with a receding horizon $[t,t+T]$ $(T>0)$. 
\begin{equation}
	J_M[u] = \varphi_M(x_M(t+T)) + \int^{t+T}_t L_M(x_M(\tau), u(\tau),\tau)d\tau, \label{eq:JJ}
\end{equation}
where $x_M(\tau)$ and $u(\tau)$ $(t \leq \tau \leq t+T)$ represent the predicted state and control input over the horizon, respectively. They may not necessarily coincide with the actual system's state and control input in the future. 

At each time \(t\), the initial value of the optimal control input minimizing the objective function (\ref{eq:JJ}) over the horizon $[t,t+T]$ is used as the actual control input for that time. 
Subsequently, NMPC defines a state feedback control law because the control input $u(t)$ depends on the current state $x_M(t)$ of the system that is used as the initial state in the OCP over $[t,t+T]$. 
NMPC can handle various control problems if the OCP is solved numerically in real time. 
However, it cannot handle multi-agent interactions in competitive scenarios such as drone racing. 
This limitation arises from the need to predict the opponent's behavior, which is inherently uncertain. Therefore, NMPC requires simplifying assumptions about the opponent's future trajectory, reducing its effectiveness in adversarial environments.


\subsection{Nonlinear receding-horizon differential game}

To explicitly model an adversarial opponent, we employ NRHDG, an extension of NMPC incorporating a differential game problem (DGP).\cite{dg0} 
In what follows, we limit our discussion to a two-player zero-sum DGP, where one player's gain is exactly the other player's loss, making it a suitable model for competitive scenarios. 
We consider a dynamical system described by a state equation
\begin{eqnarray}
	\label{nonlinearsys2}
	\dot{x}_D(t) = f_D(x_D(t), u(t), v(t), t),
\end{eqnarray}
where $x_D(t)$ is the combined state of two players, \(u(t)\) is the strategy (control input) for player $U$, and \(v(t)\) is the strategy for player $V$. 
One player $U$ aims to minimize some objective function, while the other $V$ aims to maximize it. 
We assume that both players know the current state of the game and the state equation governing the game.

In NRHDG, we consider an objective function with a receding horizon as follows:
\begin{equation}
	\label{rhdgef}
	J_D[u,v] = \varphi_D(x_D(t+T)) + \int^{t+T}_t L_D(x_D(\tau), u(\tau), v(\tau),\tau)d\tau,
\end{equation}
which models the zero-sum nature of the game. 
If there exists a pair of strategies $(u^0,v^0)$ such that
\begin{equation}
J_D[u^0,v] \leq J_D[u^0,v^0] \leq J_D[u,v^0]
\end{equation}
holds for any strategies $u$ and $v$, $(u^0,v^0)$ is called a saddle-point solution. 
The saddle-point solution $(u^0, v^0)$ ensures that both players adopt optimal strategies, balancing minimization by player $U$ and maximization by player $V$.
In particular, the player $U$ can achieve a lower value of the objective function if $V$ chooses a different strategy from $v^0$. 
Therefore, $U$ can use the initial value of a saddle-point solution $u^0$ for the DGP as the actual input to the system regardless of the control input of $V$, which also defines a state feedback control law for $U$. 
This property makes NRHDG suitable for dynamic problems in competitive scenarios such as drone racing. 
Although the necessary conditions for a saddle-point solution of a DGP are identical to the stationary conditions for an OCP,\cite{dg0} numerical solution methods for NMPC are not necessarily applicable to NRHDG if they are tailored for minimization, for instance, involving line searches. 
However, there are some methods that are applicable to both NMPC and NRHDG.\cite{race3,dg1,Nagata2023} 




\section{Design of objective functions for competitive drone racing} \label{sec:design} 

\subsection{Control objectives}

This section describes the construction of the objective functions for NMPC and NRHDG that balance the following two key racing objectives. 
\begin{enumerate}
    \item Path following: Ensure that the drone progresses efficiently along the predefined three-dimensional path, minimizing deviations while allowing flexibility for dynamic maneuvers. This flexibility avoids strict convergence to the path, which can hinder competitive behaviors such as overtaking or obstructing. 
    \item Overtaking and obstructing: Enable the ego drone to dynamically switch between overtaking a preceding opponent and obstructing a following opponent, depending on the race scenario. These behaviors are achieved while ensuring collision avoidance and maintaining competitive efficiency. 
\end{enumerate}
We first design an objective function for NMPC to achieve the above objectives. Subsequently, we modify it to define an objective function for NRHDG. 


\subsection{NMPC objective function}

\subsubsection{Path-following term}

Consider the augmented drone state $\bar{x}_d$ in Section \ref{sec:model}, which includes position $p_d$, velocity $\dot{p}_d$, angular velocity $\omega$, quaternion $q$, path parameter $\theta_d$, and arc length $\sigma$. 
We define a stage cost for path following as follows: 
\begin{equation}
     L_{PF}(\bar{x}_d,u_d) = \sum_{i=1}^{3} a_{i}(p_{di} - r_{i}(\theta_d) )^{2}+\sum_{i=1}^{3}a_{i+3}\omega_{i}^{2} - a_{7} \sigma + \sum_{i=1}^{4} b(u_{di} - u_{ref})^{2},
     \label{eq:pathL}
\end{equation}
where \(u_{ref} = mg/4 \) represents the reference input when the drone is in a hovering state. 
The parameters \(a_i \ (i = 1, \dots ,7)\) represent the state weights, while \(b\) serves as the input weight. 
The stage cost in (\ref{eq:pathL}) balances multiple objectives for path following: The first term penalizes deviations from the desired path. The second term penalizes the drone's angular velocity to suppress excessive attitude motion. The third term maximizes the drone's progress along the path. The final term suppresses large deviations in the control inputs from the reference input. 
A corresponding terminal cost 
\begin{equation}
    \varphi_{PF}(\bar{x}_d)  = \sum_{i=1}^{3}a_{i}(p_{di} - r_{i}(\theta_d) )^{2}+\sum_{i=1}^{3}a_{i+3}\omega_{i}^{2} - a_{7} \sigma .
    \label{eq:pathphi1}
\end{equation}
ensures the terminal state aligns with the path-following objective. 
Since the path parameter $\theta_d$ and the arc length $\sigma$ of the projection point $p_p = r(\theta_d)$ are embedded in the state equation as state variables, the stage and terminal costs do not involve any optimization problem nor complicated coordinate transformation to determine the projection point for a wide class of three-dimensional paths. 


\subsubsection{Overtaking and obstructing term} \label{subsec:objective}

To enable the ego drone to overtake or obstruct an opponent, 
we introduce a potential function that depends on both the ego-drone's state $\bar{x}_d$ and the opponent's predicted state. 
For NMPC, we assume that the opponent moves at a constant speed in parallel to the path (Figure \ref{fig:MPCpredict}). 
Specifically, the ego drone predicts the position $p_{op}$ and path parameter $\theta_{op}$ of the opponent by a simplified state equation 
\begin{equation}
\dfrac{d}{dt} \left(
    \begin{array}{c}
      p_{op} \\
      \theta_{op} \\ 
    \end{array}
  \right) =
  \left( \begin{array}{c}
      \lambda \frac{d{r}(\theta_{op})}{d\theta} \\
      \lambda \\ 
    \end{array}
  \right) ,\label{eq:stateeq_MPC}
\end{equation}
where $\lambda$ denotes the constant speed of the opponent. 
We denote the state vector of the simplified prediction model as $x_{op} = (p^{\textrm{T}}_{op} \ \theta_{op})^{\textrm{T}} \in \mathbb{R}^4$. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{NMPC_prediction-crop.pdf}
    \caption{Prediction of opponent's motion in NMPC}
    \label{fig:MPCpredict}
\end{figure}

Here, we define a potential function for overtaking and obstructing in NMPC as follows. 
\begin{align}
    G_{O}(\bar{x}_d,p_{op},\theta_{op}) &= \exp\left(-\left(\frac{\theta_{\Delta}-\delta_{1}}{\alpha}\right)^2\right) \tanh(\theta_{\Delta}-\delta_{2}) \frac{\beta}{1 + \gamma R^2}
    \label{eq:poten}, \\
    \theta_{\Delta} &= \theta_{op} - \theta_d, \quad R = \| (p_{op} - r(\theta_{op})) - (p_d - r(\theta_d)) \| , 
\end{align}
where $\theta_{\Delta}$ represents the difference of the path parameters between the ego drone and the opponent, $R$ represents the difference in the deviations of the two drones from the path (Figure \ref{fig:dev_diff}), and $\alpha$, $\beta$, $\gamma$, $\delta_{1}$, and $\delta_{2}$ are constants. 
The shape of this potential function is shown on the $\theta_{\Delta}$-$R$ plane in Figure \ref{fig:poten}, where the origin corresponds to the location of the ego drone. 
The potential function $G_O$ in (\ref{eq:poten}) enables adaptive overtaking and obstructing behaviors based on the relative position of the ego drone to its opponent. 
When $\theta_{\Delta}$ is positive, the ego drone follows the opponent and should avoid and overtake the opponent. 
Therefore, we define the potential function such that it has its maximum at $R=0$ when $\theta_{\Delta}$ is positive. 
Here, we use $R$, the difference in the deviations of the two drones from the path, rather than the distance $\| p_{op} - p_d \|$, because $R$ does not depend on the distance of the two drones along the path. 
That is, the ego drone does not slow down to keep the distance from the opponent along the path when it avoids and overtakes the opponent. 
However, when $\theta_{\Delta}$ is negative, the ego drone precedes the opponent and should obstruct the opponent not to be overtaken. 
To induce obstructing behavior, the potential function has its minimum at $R=0$ when $\theta_{\Delta}$ is negative. 
Since $R$ is independent of the distance of the two drones along the path, the ego drone maintains its speed when obstructing the opponent. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.5\linewidth]{deviation_difference-crop.pdf}
    \caption{Difference of deviations of two drones.}
    \label{fig:dev_diff}
\end{figure}

\begin{figure}[tbp]
  \centering
  \includegraphics[width=0.5\linewidth]{potential-crop.pdf}
  \caption{Potential function around ego drone.} \label{fig:poten}
\end{figure}


\subsubsection{Overall NMPC cost} \label{subsec:NMPCformulation}

In this section, we formulate NMPC for drone racing and define the overall objective function. The ego drone predicts its future motions and the opponent using state equations (\ref{eq:stateeq_pathF}) and (\ref{eq:stateeq_MPC}), and the state vector of the entire system is defined as $x_M =(\bar{x}_d^\textrm{T} \ x_{op}^\textrm{T})^\textrm{T} \in \mathbb{R}^{19}$. 
Subsequently, we define the stage cost and terminal cost of NMPC by combining the objective functions for path following and overtaking and obstructing, which enables the ego drone to balance path following with competitive behaviors, as follows.
\begin{align}
     &L_{M}({x}_M,u_d) = L_{PF}(\bar{x}_d,u_d) + G_{O}(\bar{x}_d,p_{op},\theta_{op}),   \label{eq:raceL}\\
     &\varphi_{M}({x}_M)  = \varphi_{PF}(\bar{x}_d) + G_{O}(\bar{x}_d,p_{op},\theta_{op}).     \label{eq:racephi}
\end{align}
These costs determine behaviors of the ego drone depending on the race scenario represented by the states of the two drones. 
In particular, since the path parameters $\theta_d$ and $\theta_{op}$ are components of the state vectors determined by the state equations (\ref{eq:stateeq_pathF}) and (\ref{eq:stateeq_MPC}), there is no need for additional optimization or moving frames to determine the projection point along the path, which makes the proposed formulation advantageous over conventional formulations of path-following control. 


\subsection{NRHDG objective function} \label{subsec:NRHDG}

In NRHDG, the ego drone minimizes an objective function (\ref{rhdgef}) while assuming that the opponent maximizes the same objective function. 
Furthermore, the ego drone assumes that the opponent is also governed by the same augmented state equation (\ref{eq:stateeq_pathF}). 
Unlike NMPC, which assumes a fixed prediction model for the opponent, NRHDG explicitly models the opponent's strategy as a dynamic, adversarial behavior. By incorporating a zero-sum game framework, NRHDG enables the ego drone to optimize its performance against the opponent that actively counters its actions.
We denote the state vector and the input vector of the opponent by $\bar{x}_{op}$ and $u_{op}$, respectively, which consist of the opponent's variables corresponding to those of $\bar{x}_d$ and $u_d$. 
We also denote the state vector of the entire system as $x_D = ( \bar{x}_d^\textrm{T} \ \bar{x}_{op}^\textrm{T} )^\textrm{T}$. 
We now define the stage cost and the terminal cost for NRHDG as 
\begin{align}
    L_D(x_D,u_d,u_{op}) &= L_{PF}(\bar{x}_d,u_d) + G_{O}(\bar{x}_d,p_{op},\theta_{op}) - L_{PF}(\bar{x}_{op},u_{op}) - G_{O}(\bar{x}_{op},p_d,\theta_d) , \label{eq:L_D}\\
    \varphi_D(x_D) &= \varphi_{PF}(\bar{x}_d) + G_{O}(\bar{x}_d,p_{op},\theta_{op}) - \varphi_{PF}(\bar{x}_{op}) - G_{O}(\bar{x}_{op},p_d,\theta_d) . \label{eq:phi_D}
\end{align}
The stage cost $L_D$ in (\ref{eq:L_D}) includes terms for both the ego drone and the opponent, reflecting the adversarial nature of the interaction. 
The terminal cost $\varphi_D$ in (\ref{eq:phi_D}) evaluates the terminal state of the game, ensuring that each drone's strategy aligns with the race objectives. These costs create a zero-sum strategic behaviors where the ego drone minimizes its cost while maximizing the opponent's. 
At each time $t$, the ego drone determines its control input $u_d$ by solving the NRHDG problem subject to the 30-dimensional state equation for $x_D$. 




\section{Performance evaluation in competitive scenarios} 

\subsection{Performance metric}

This section assesses the effectiveness of the proposed NRHDG compared to the baseline NMPC in competitive drone racing scenarios. 
We focus on the following two key aspects. 
\begin{enumerate}
    \item Overtaking performance: The ability of the following drone to maximize its progress while overtaking the preceding drone. 
    \item Obstructing performance: The ability of the preceding drone to minimize the following drone's progress while being overtaken.
\end{enumerate}
These metrics reflect both offensive (overtaking) and defensive (obstructing) capabilities of the controllers in adversarial races. 
For notation, we label NMPC as controller $M$ and NRHDG as controller $D$. 
In a race denoted by $\text{Race}(B,A)$, the drone using controller $B$ starts ahead, while the drone using controller $A$ starts behind. 

To evaluate the overtaking performance, we compare each pair of two races, $\text{Race}(A,M)$ and $\text{Race}(A,D)$, for $A \in \{ M, D \}$. 
This means that we set the leading controller to be the same in both races and compare the progress of the drone starting from the rear position. 
To compare controllers fairly, the race settings, such as the initial lead and weight coefficients in the objective functions, are set to be the same between both races. 
Moreover, the race settings are chosen so that the controller starting from the rear position eventually overtakes the opponent starting ahead in all races, which enables us to make a quantitative comparison between different controllers in different races. 
The controller achieving more progress while overtaking against the same controller $A$ has better overtaking performance. 
Here, we measure the progress of a drone by its path parameter and define a symbol, $\text{Prog}_{\text{rear}}(B, A)$, to represent the progress of controller $A$ at a certain time in $\text{Race}(B,A)$.
A larger value of $\text{Prog}_{\text{rear}}(B,A)$ indicates better overtaking performance by the rear-position drone (controller $A$). 
That is, if the relationship $\text{Prog}_{\text{rear}}(B, A_1) > \text{Prog}_{\text{rear}}(B, A_2)$ holds most of the time in both races, $A_1$ makes its progress more effectively than $A_2$ and has better overtaking performance than $A_2$. 
Since NRHDG (controller $D$) explicitly models the opponent's behavior and optimizes against the worst-case scenarios, it will outperform NMPC (controller $M$). Therefore, we can expect NRHDG to have better overtaking performance than NMPC as follows. 
\begin{align}
     \text{Prog}_{\text{rear}}(A,D) > \text{Prog}_{\text{rear}}(A,M), \quad A \in \{M,D\}, \label{eq:ADM} 
\end{align}
which means NRHDG (controller $D$) achieves more progress than NMPC (controller $M$) when overtaking the same controller $A$.

A similar discussion can be made for obstructing performance. 
To evaluate obstructing performance, we compare each pair of two races, $\text{Race}(M,A)$ and $\text{Race}(D,A)$ for $A \in \{ M, D \}$. 
This implies that we set the controller starting from the rear position to the same in two races and observe the difference between the two races. 
Specifically, the controller that more effectively slows the progress of the rear-position drone $A$ has better obstructing performance. 
Therefore, a lower value of $\text{Prog}_{\text{rear}}(B,A)$ indicates better obstructing performance of the front-position drone (controller $B$). 
That is, if the relationship $\text{Prog}_{\text{rear}}(B_1, A) < \text{Prog}_{\text{rear}}(B_2, A)$ holds most of the time in the two races, $B_1$ obstructs the progress of $A$ more effectively than $B_2$ and has better obstructing performance than $B_2$. 
Therefore, the following relationship should hold between NRHDG (controller $D$) and NMPC (controller $M$). 
\begin{align}
    \text{Prog}_{\text{rear}}(D,A) < \text{Prog}_{\text{rear}}(M,A), \quad A \in \{M,D\}, \label{eq:DMA} 
\end{align}
which means NRHDG obstructs the progress of controller $A$ better than NMPC. 

Subsequently, the relationships in (\ref{eq:ADM}) and (\ref{eq:DMA}) imply  
\begin{align}
    \text{Prog}_{\text{rear}}(D,M) &< \text{Prog}_{\text{rear}}(D,D) < \text{Prog}_{\text{rear}}(M,D) ,  \label{eq:prog_D} \\
    \text{Prog}_{\text{rear}}(D,M) &< \text{Prog}_{\text{rear}}(M,M) < \text{Prog}_{\text{rear}}(M,D) ,  \label{eq:prog_M} 
\end{align}
where the first and second inequalities in (\ref{eq:prog_D}) are obtained from (\ref{eq:ADM}) and (\ref{eq:DMA}), respectively, with $A = D$, and those in (\ref{eq:prog_M}) are obtained from (\ref{eq:DMA}) and (\ref{eq:ADM}), respectively, with $A = M$. 
Finally, (\ref{eq:prog_D}) and (\ref{eq:prog_M}) imply 
\begin{align}
    \text{Prog}_{\text{rear}}(D,M) < \text{Prog}_{\text{rear}}(M,D) \label{eq:prog_DMMD}. 
\end{align}
If the relationships in (\ref{eq:ADM}) and (\ref{eq:DMA}) or (\ref{eq:prog_D})--(\ref{eq:prog_DMMD}) hold in numerical simulations, NRHDG is more suitable for drone racing than NMPC. 


\subsection{Race setup}

We simulate races on a three-dimensional path
\begin{align}
  r(\theta) =  \left(
    \begin{array}{ccc}
      6\text{sin} \theta & 3\text{sin} 2\theta & 6\text{sin} \frac{\theta}{2} 
    \end{array}
  \right)^\textrm{T}, \label{eq:course1}
\end{align}
depicted in Figure \ref{fig:course}. 
This path tests the controllers' ability to handle complex three-dimensional environments with varying curvature and torsion. 
The sinusoidal path includes sharp turns and gradual slopes, challenging the drones' path-following and maneuvering capabilities. 
The rear drone starts at $r(0)$, and the front drone starts at $r(1)$. 
The physical parameters of the drones in the simulation are shown in Table \ref{tab:param}, which are based on the Parrot MamboFly platform.\cite{mambo} The weight coefficients and constants in the objective functions of NMPC and NRHDG are shown in Table \ref{tab:pathomomi}.
We assigned different input weights to represent a speed advantage for the rear drone ($b=20$) and a slower response for the front drone ($b=40$). 

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\linewidth]{path-crop.pdf}\\
 \caption{Overview of path.}\label{fig:course}
\end{figure}

%%% tab:Parameters of MamboFly
\begin{table}[tbp]
  \centering
  \caption{Parameters of MamboFly.}
    \begin{tabular}{lll} \toprule
    Variable & Meaning & Value \\ \midrule
    $m$ & Mass of the aircraft & 0.063 kg \\ 
    $g$ & Gravitational acceleration & 9.81 m/$\mathrm{s^{2}}$ \\ 
    $l$ & Distance from the center of mass to the rotor & 0.0624 m \\ 
    $J_{xx}$ & Moment of inertia around the roll axis &
    $5.82857 \times 10^{-5}$ $\mathrm{kg \cdot m^{2}}$ \\ 
    $J_{yy}$ & Moment of inertia around the pitch axis & $7.16914 \times 10^{-5}$ $\mathrm{kg \cdot m^2}$ \\ 
    $J_{zz}$ & Moment of inertia around the yaw axis & $1 \times 10^{-4}$ $\mathrm{kg \cdot m^2}$ \\ 
    $k$ & Proportional constant between reaction torque and thrust & 0.0024 m \\ \bottomrule
    \end{tabular}
  \label{tab:param}
\end{table}

\begin{table}[tbp]
  \centering
  \caption{Parameters in objective function.}
  \begin{tabular}{ll} \toprule
    Parameter & Value\\ \midrule
    $a_i \ (i=1, \ 2 , \ 3)$ & 1 \\ 
    $a_i \ (i=4, \ 5 , \ 6)$ & 0.1 \\ 
    $a_7 \  $ & 0.5 \\ 
    $\alpha $ & 1 \\  
    $\beta  $ & 4 \\ 
    $\gamma $ & 5 \\ 
    $\delta_{1} $ & $-0.5$ \\ 
    $\delta_{2} $ & $-1$ \\ 
    $\lambda$ & $1$ \\ \bottomrule
  \end{tabular}
  \label{tab:pathomomi}
\end{table}

We implemented NRHDG and NMPC with a continuation-based real-time optimization algorithm, C/GMRES,\cite{Ohtsuka2004cgmres} and its automatic code generation tool, AutoGenU for Jupyter.\cite{Katayama2020autogen}\footnote{\url{https://ohtsukalab.github.io/autogenu-jupyter}}
The C/GMRES finds a stationary solution to an optimal control problem without any line searches and is also applicable directly to NRHDG problems. 
AutoGenU for Jupyter generates C++ code and a Python package for updating the solution with C/GMRES. Then, those codes for NRHDG and NMPC can be used together for simulation of drone racing. 
We conducted numerical simulations on a PC (CPU: Core i9-12900 2.4 GHz, RAM: 16 GB, OS: Ubuntu 22.04.2 LTS on WSL2, hosted by Windows 11 Pro) to demonstrate the feasibility of real-time implementation. 
The simulation ran for 20 s, with a horizon length $T$ of 0.4 s and a control cycle of 1 ms. 
The average computation times per update were 0.8 ms for NRHDG and 0.5 ms for NMPC, both of which are within the control cycle. 


\subsection{Simulation results}

\subsubsection{Time histories and example overtaking scenario}

Figure \ref{fig:timehis} shows a sample time history from $\text{Race}(D,M)$, where controller $D$ (NRHDG) starts ahead and controller $M$ (NMPC) starts behind. 
The dashed vertical line indicates the moment $M$ manages to overtake $D$. 
The altitudes ($z$) of the two drones oscillate as they attempt to obstruct or overtake one another. 
Figure \ref{fig:racePic} shows a snapshot of $\text{Race}(D,M)$ around the overtaking time. 
The blue drone (NRHDG) predicts its future trajectory and that of the opponent, which are shown as the blue trajectories, while the red trajectories represent the predictions by the red drone (NMPC). 
NRHDG generates its blue trajectory moving toward the front of the red drone, while NMPC predicts the blue drone to move in parallel to the path. 
Moreover, NRHDG predicts the red drone to move in parallel to the path, while NMPC generates a larger avoidance motion than NRHDG's prediction. 
This indicates that NRHDG generates a less conservative prediction regarding the opponent's future behavior compared to NMPC.  

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{timeplot-crop.pdf}
    \caption{Time history of $\text{Race}(D,M)$.}
    \label{fig:timehis}
\end{figure}
%%%% Different line styles should be used for different controllers. %%%%
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{racePic2.png}
    \caption{Snapshot of $\text{Race}(D,M)$, where the blue drone (NRHDG) is obstructing the red drone (NMPC).}
    \label{fig:racePic}
\end{figure}

\subsubsection{Comparisons across multiple races}

\paragraph{Overtaking performance}  

First, we evaluate the overtaking performance by comparing two races: $\text{Race}(A,D)$ and $\text{Race}(A,M)$ with $A \in \{D,M\}$. 
The comparison of $\text{Race}(A,D)$ and $\text{Race}(A,M)$ for $A=D$ is shown in Figure \ref{fig:overtakeD} in terms of the progress of the drone starting from the rear position.
Two vertical lines indicate the overtaking times in the two races. After overtaking, the two drones do not interact. 
Therefore, we focus on the plots from the start time to the overtaking time. 
In Figure \ref{fig:overtakeD}, the figure on the left shows the plots of $\text{Prog}_{\text{rear}}(A,D)$ and $\text{Prog}_{\text{rear}}(A,M)$, and the figure on the right shows their difference, $\text{Prog}_{\text{rear}}(A,D)- \text{Prog}_{\text{rear}}(A,M)$. As can be seen in the figure on the right, $\text{Prog}_{\text{rear}}(A,D)$ exceeds $\text{Prog}_{\text{rear}}(A,M)$ between the start time and the overtaking time. That is, NRHDG makes more progress than NMPC while overtaking the same opponent $A=D$. 
The comparison of $\text{Race}(A,D)$ and $\text{Race}(A,M)$ for $A=M$ is shown in Figure \ref{fig:overtakeM}. As shown in the figure, NRHDG still makes more progress than NMPC while overtaking the same opponent. 
These observations validate the relationship in (\ref{eq:ADM}) and imply that NRHDG has better overtaking performance than NMPC, as expected. 

\begin{figure}[htbp]
  \begin{minipage}[b]{\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{overtakeD-crop.pdf}
    \subcaption{Comparison of $\text{Race}(A,D)$ and $\text{Race}(A,M)$ for $A=D$.}
    \label{fig:overtakeD}
  \end{minipage} \\
  \begin{minipage}[b]{\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{overtakeM-crop.pdf}
    \subcaption{Comparison of $\text{Race}(A,D)$ and $\text{Race}(A,M)$ for $A=M$.}
    \label{fig:overtakeM}
  \end{minipage}
  \caption{Comparison of $\text{Race}(A,D)$ and $\text{Race}(A,M)$.}
\end{figure}

\paragraph{Obstructing performance}
In a similar manner to the overtaking performance evaluation, we only need to consider the graph from the start time to the overtaking time. 
The comparison of $\text{Race}(D,A)$ and $\text{Race}(M,A)$ for $A=D$ is shown in Figure \ref{fig:observeD}. The graph does not show the progress of the obstructing drones but shows the progress of drones (controller $A$) starting from the rear position and obstructed by the preceding drones. 
In Figure \ref{fig:observeD}, the figure on the left shows the plots of $ \text{Prog}_{\text{rear}}(D,A)$ and $\text{Prog}_{\text{rear}}(M,A)$, and the figure on the right shows their difference, $\text{Prog}_{\text{rear}}(D,A)- \text{Prog}_{\text{rear}}(M,A)$.
As can be seen in the figure, $\text{Prog}_{\text{rear}}(D,A)$ lags behind $\text{Prog}_{\text{rear}}(M,A)$ for most of the time between the start time and the overtaking time, except for the beginning of the race.
This result shows NRHDG obstructs the opponent more effectively than NMPC. 
The comparison of $\text{Race}(D,A)$ and $\text{Race}(M,A)$ for $A=M$ is shown in Figure \ref{fig:observeM}. The figure shows that $\text{Prog}_{\text{rear}}(D,A)$ lags behind $\text{Prog}_{\text{rear}}(M,A)$ until the overtaking time. 
These results validate the relationship (\ref{eq:DMA}) and show that NRHDG has better obstructing performance than NMPC. 
Hence, from the numerical simulations, we conclude that NRHDG is better suited than NMPC for competitive drone racing scenarios. 

\begin{figure}[htbp]
  \begin{minipage}[b]{\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{observeD-crop.pdf}
    \subcaption{Comparison of $\text{Race}(D,A)$ and $\text{Race}(M,A)$ for $A=D$.}
    \label{fig:observeD}
  \end{minipage} \\
  \begin{minipage}[b]{\linewidth}
    \centering
    \includegraphics[width=0.8\linewidth]{observeM-crop.pdf}
    \subcaption{Comparison of $\text{Race}(D,A)$ and $\text{Race}(M,A)$ for $A=M$.}
    \label{fig:observeM}
  \end{minipage}
  \caption{Comparison of $\text{Race}(D,A)$ and $\text{Race}(M,A)$.}
\end{figure}




\section{Conclusions} 

This study presents NRHDG, a game-theoretic control method for competitive drone racing, addressing both path-following control and adversarial interactions. 
Building on a unified path-following formulation via projection-point dynamics, our approach eliminates the need for iterative distance minimization and its subsequent approximation. The proposed potential function further allows drones to adaptively balance overtaking and obstructing behaviors, while a new performance metric systematically evaluates overtaking and obstructing capabilities. Numerical simulations confirmed that NRHDG outperforms a baseline NMPC in both offensive and defensive maneuvers across a challenging three-dimensional race path. 
Beyond drone racing, the developed principles and techniques have potential applications in other domains requiring dynamic multi-agent interactions. 
Potential use cases include autonomous vehicle coordination, robotic swarm navigation, and air traffic management. 
These applications highlight the broader significance of NRHDG in advancing control methodologies for competitive and dynamic systems.

Future work includes adapting NRHDG to more complex racing environments with even more complex paths or gates. 
Another possible extension is a race with three or more drones, for which a multi-player non-zero-sum game framework is necessary. 
Addressing uncertainties in drone dynamics and opponent strategies will also be critical for real-world implementation. This includes developing robust methods to handle unknown disturbances, such as wind or sensor noise, and designing predictive models that account for stochastic behavior in opponents. 




%\backmatter
% \bmsection*{Author contributions}

% This is an author contribution text. This is an author contribution text. This is an author contribution text. This is an author contribution text. This is an author contribution text.

%\bmsection*{Acknowledgments}


%\bmsection*{Financial disclosure}
%None reported.

\bmsection*{Conflict of interest}
The authors declare no potential conflict of interests.

%\bibliography{wileyNJD-AMA}
\bibliography{drone_racing}

% \bmsection*{Supporting information}

% Additional supporting information may be found in the
% online version of the article at the publisher’s website.


%\nocite{*}% Show all bib entries - both cited and uncited; comment this line to view only cited bib entries;


% \bmsection*{Author Biography}

% \begin{biography}{\includegraphics[width=76pt,height=76pt,draft]{empty}}{
% {\textbf{Author Name.} Please check with the journal's author guidelines whether
% author biographies are required. They are usually only included for
% review-type articles, and typically require photos and brief
% biographies for each author.}}
% \end{biography}


\end{document}
