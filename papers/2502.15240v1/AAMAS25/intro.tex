\section{Introduction}

% Two examples --- one used in simulation and another one (preferebly real worls example)
\gan{2 Pages}

\nocite*

In a classical stochastic Multi-armed Bandit (MAB) problem, a central decision maker takes an action or, equivalently,  selects an arm from a fixed set of arms at each of \( T \) time steps. Each arm pull yields a random reward from an unknown distribution. The objective is to develop a strategy for selecting arms that minimizes the regret; difference between the cumulative rewards of the best possible arm-pulling strategy in hindsight and the cumulative expected rewards achieved by the algorithm's policy.


\gan{PARA 2: MAMAB motivating examples}


% \himanshu{Sir, should we include "Capitalistic Policies with Communistic Elements for the Underprivileged" example? But this gives different direction than the current broadcast one.}
\gan{Lets avoid statements with philosophical innuendos}

 We study an interesting variant of stochastic MAB problem, first proposed by \citet{Hossain2020FairAF} and known as  Multi-agent Multi Armed Bandits (MA-MAB). In the MA-MAB setting  an arm pull generates a vector-valued reward whose each entry is independently sampled from a fixed but unknown distribution denoting reward obtained by corresponding agent. When there is a single agent, this setting reduces to a classical stochastic MAB setting. 
 % \piyushi{In Nisarg Shah's paper, stochastic MAB was motivated by citing papers that stress that winner-takes-all allocations can be considered unfair to the items in many applications and can lead to undesirable long-term dynamics. Should we similarly motivate why we work with stochastic MAB?}
 % \gan{added this later}

 
The  MA-MAB setting captures several interesting real-world applications. Consider, for instance,  the problem of distributing a fixed monthly/yearly budget, say one unit, among $k$ different projects. There are \( n \) beneficiaries (or agents) who each experience varying levels of benefit from the different projects.    % For instance, some agents may be relatively indifferent between the projects, whereas others have clear favourites. 
Each agent $i \in [n]$ receives a reward sampled independently from distribution $\mathcal{D}(\mu_{i,j})$ when the algorithm pulls an arm $j$ (or equivalently, selects project $j$) where $\mu_{i,j}$ denotes the mean reward for agent $i$ from arm $j$. The randomness in the reward received by agents may arise from uncertainty in the assessment of the value of the project by individual agents and randomness in the aggregation/reporting step. Given  a distribution  $\pi \in \Delta_{m}$ over the  set $[m]$ of arms,  the total expected reward to agent $i$ is given by $\sum_{j \in [m]} \mu_{i,j} \pi_j$.    


Consider another example where a networked TV channel must decide which movie/program to telecast on  a given time slot. The different movie/program genres are the arms, whereas the  population group (based on age group, demographics, etc.) are the agents. The reward represents the preferences of the corresponding agent, a.k.a. age group.  The decision-maker's problem here is to telecast the most liked program that, at the same time, caters to the preferences of a diverse population.  We will return to this example in Section \ref{sec: simulation}. 
  

%Each agent \( i \in [n] \) receives a reward sampled independently from the distribution \(\mathcal{D}(\mu_{i,j})\) when the algorithm selects project \( j \in [m] \). This reward variability can stem from uncertainties in how each agent values the project and variations in how rewards are aggregated or reported. Given a distribution \(\pi \in \Delta_{m}\), the total expected reward for agent \( i \) is given by \(\sum_{j \in [m]} \mu_{i,j} \pi_j\).

It is easy to see that when the goal is to maximize social welfare  \footnote{ Defined as the sum of cumulative expected reward.},  the resulting arm pull  strategy/allocation strategy  might become skewed. For example, consider a MA-MAB instance with \( n \) agents and \( m=2 \)  arms with  reward distributions such that \(\mu_{1,1} =1\) and \(\mu_{1,2} =0\), and \(\mu_{i,1} =0\) and \(\mu_{i,2} = 1/n\) for \(i >  1\). In this case, a social welfare maximizing  policy  would allocate the entire budget to the first project. Similarly,  select the movie genre most preferred by the entire population in the second example. However, this policy benefits only the first agent, leaving the vast majority of \(n-1\) agents without any reward.  Such winner-takes-all allocations can be considered unfair in many applications and can lead to undesirable long-term dynamics leading to mistrust towards the algorithm \cite{Hossain2020FairAF}.     %This approach can lead to situations where a certain section  of population does not benefit from the allocation. %The decision maker must  balance providing minimum opportunities for all stakeholders while maximizing overall profit. %For example, in the context of online recommendations, incentivizing small businesses by offering enough exposure to guarantee certain minimum rewards may not be enough to stimulate economic growth, as it could overlook the needs of larger businesses and corporations that contribute more to the social welfare significantly more than medium-scale corporations.
%The policymaker faces the  challenge of maximizing overall profit by  favouring the most rewarding arm, while also ensuring that individual agents are each guaranteed a certain minimum reward (i.e., receive guaranteed minimum opportunities).  
The  MA-MAB framework with fairness constraints facilitates the simultaneous optimization of both individual rewards and overall societal welfare.







    %For example, consider a scenario where agents represent businesses and policies represent tax incentive schemes. Each agent aims to maximize utility, such as profit and growth, while policymakers seek to ensure fairness and economic prosperity for all.     
%    By leveraging the MAMAB framework, policymakers can design policies that provide minimum support for small businesses while also incentivizing larger entrepreneurs, thus fostering a robust and inclusive economy.

    %Our proposed problem formulation aligns with this agenda by focusing on simultaneously providing minimum incentives for all agents and maximizing overall social welfare, ensuring a balanced and equitable approach to policy design in dynamic economic environments.


%\gan{Para 4: On minimum reward guarantee, why minimium guarantee makes sense in given examples }

\gan{para: introduce the fairness notion with intuitive explanation} 

 Consider a thought experiment where a single agent dictates the arm-pulling policy. \footnote{Alternatively, consider a scenario where there is only one agent, i.e., \( n = 1 \).}  This dictatorial agent would choose to pull her most rewarding arm at every time step. However, this policy completely disregards the preferences of other agents and thus fails to ensure any minimum reward for them. 
%In the stochastic rewards setting without fairness constraints, the most rewarding arm—defined as the one with the highest expected sum of rewards—is frequently selected by any (asymptotically) optimal learning strategy, such as those from the UCB family of algorithms. As a result, agents who do not favor the most rewarding arm are not guaranteed to receive a certain minimum level of reward.
\iffalse 
The fairness notion studied in this paper ensures the minimum reward guarantee to each agent in terms of a fraction of the maximum possible expected reward to that agent. Consider, as a thought experiment,  that a particular agent dictates the arm-pulling policy. \footnote{Equivalently, consider that there is only one agent affected by the policy i.e. $n =1$.}  The dictator agent would prefer his/her most rewarding arm to be pulled at each time instant.  However, such a policy completely ignores the preferences of other agents and consequently may not ensure any minimum reward guarantee to them. 

Under the stochastic rewards setting, and without fairness constraints,  the most rewarding arm, a.k.a. an arm with the largest expected sum of rewards,  is pulled often by  any  (asymptotically) optimal learning strategy (say from a UCB family of algorithms).  That is, the agents who do not prefer the most rewarding arm are not guaranteed to receive a certain minimum reward. 
\fi 
%\paragraph{Minimum reward guarantee fairness: }{ %In  this paper, we consider  minimum reward guarantee to each agent as an additional, explicit constraint.  In particular, each agent $i$ is guarantees at least a certain fraction $c_i \in [0,1]$ of maximum possible reward. In the above budget allocation example,  the policies $ \pi_1 = (1/4, 3/4)$ and $\pi_2 = (3/4, 1/4)$ both achieve  $1/4$ th of maximum possible reward to each agent,  however,  $\pi_2$ has a significantly large social welfare over $\pi_1$ \footnote{For large values of $n$,  the expected reward from  policy $\pi_1$ is approximately $5/4$ whereas the same by $\pi_2$ is  approximately $3n_1/4 $. }. It is easy to see that when all the mean rewards are known the social welfare maximization under minimum reward guarantee constraints can be posed as a linear program (see Section \toref for details).  The objective of this paper is to design algorithms that efficiently trade off social welfare guarantee with  minimum rewards guarantee violations  in a MAB  setting with apriori unknown reward distributions.  
In this paper, we address the problem of ensuring a minimum reward guarantee for each agent as an explicit constraint. Specifically, each agent \( i \) is guaranteed at least a certain fraction \( C_i \in [0,1] \) of the maximum possible reward they could receive. %The goal of this paper is to develop algorithms that effectively balance the trade-off between maximizing social welfare and minimizing violations of the minimum reward guarantees in a Multi-armed Bandit (MAB) setting with unknown reward distributions. %For example, in the budget allocation scenario mentioned, the policies \( \pi_1 = (1/n^2, (n+1)/n^2, \cdots (n+1)/n^2) \) and \( \pi_2 = (1 - (n-1)/n^2, 1/n^2, 1/n^2, \cdots 1/n^2) \) both ensure that each agent receives at least \( 1/n^2 \) of their maximum possible reward. However, \( \pi_2 \) provides a significantly large  overall social welfare compared to \( \pi_1 \). In particular, $SW(\pi_1)/ SW(\pi_2) \rightarrow \infty$ as $n \rightarrow \infty$.  %For a large number of agents, the expected reward under policy \( \pi_1 \) is approximately \( 5/4 \), while for policy \( \pi_2 \), it is approximately \( 3n_1/4 \). 
%}

%When all the mean rewards are known, maximizing social welfare while satisfying minimum reward guarantee constraints can be formulated as a linear program (refer to Section \toref for details). 

% \gan{Para: exploration exploitation dilemma--- best arm discovery in the context of fairness}
%A classical stochastic MAB setting faces .   

\input{AAMAS25/Rel-work}

%\gan{Para 5: On social welfare maximization objective}

\gan{Para 6: Main results of the paper}
\subsection{Main Results and Organization of the Paper} We propose a novel formulation for the multi-agent multi-armed bandits (MA-MAB) problem to maximize social welfare obtained from the rewards while also guaranteeing each agent a specified fraction of their maximum possible reward. In Section~\ref{sec:settings_prelim}, we formally define the problem and provide sufficient conditions under which a fair MA-MAB instance is guaranteed to have a feasible solution. Then, in Section~\ref{sec:warmup}, we consider an MA-MAB instance with 2 arms and $n > 1$ agents and show that a simple {\sc Explore-First} algorithm achieves a simultaneous regret bound of $\tilde{O}(T^{2/3})$ for both fairness and social welfare. 

In Section~\ref{sec:UCB}, we propose the main algorithm of this paper, \ouralgo\ ,  and show that it achieves the regret guarantee of $\tilde{O}(\sqrt{T})$ for the social welfare regret and $\tilde{O}(T^{3/4})$ for the fairness regret. In Section~\ref{subsec:lowerBound}, we prove lower bounds of $\Omega (\sqrt{T})$ for both social welfare regret and fairness regret. These lower bounds hold independently for the regret notions. We then provide a dual formulation based heuristic algorithm in Section~\ref{sec:dual} that achieves a better regret performance on the simulated data and real-world datasets (Section~\ref{sec: simulation}).   The main results of the paper are summarized in  Table~\ref{tab:your_label}. 

%We derive sufficient conditions for the feasibility of this problem. We define notions of social welfare regret and fairness regret 
%and derive an instant-independent $\Omega(\sqrt{T})$ lower bounds for the regrets
%propose algorithms that achieve sublinear regrets. We begin with the Explore-First algorithm for the case of 2-arms. Our analysis characterizes the optimal policy and leads us to $\tilde{O}(T^{2/3})$ regret bounds for both social welfare and fairness. With the proposed \ouralgo, we obtain the social welfare regret of $\tilde{O}(\sqrt{T})$ and the fairness regret of $\tilde{O}(T^{3/4})$ The main results of the paper are summarized in  Table~\ref{tab:your_label}. 
% \begin{table*}[ht!]
%   \centering
%   \begin{tabular}{|c|c|c| p{ 60 mm}|}
%     \hline
%       & Social Welfare Regret  & Fairness  Regret & Remark\\ \hline
%      Lower Bound   & $\Omega(\sqrt{T})$  & $\Omega(\sqrt{T})$ &  The lower bounds hold individually for SW regret and fairness regret.     \\ \hline
%     \textsc{ExploreFirst}   & $\tilde{O} \big (\frac{n}{ \sqrt{a}} T^{2/3} \sqrt{\log(T)} \big )$     & $\tilde{O} \big (\frac{n}{ \sqrt{a}} T^{2/3} \sqrt{\log(T)} \big ) $ & Time Horizon aware, Only for 2 arms, only for $c_i \leq 1/2$.    \\ \hline
%     \ouralgo   & $\tilde{O}(n\sqrt{mT\log(mn/\delta)})$  &   $\tilde{O}(T^{3/4})$  & Time Horizon aware, any number of finite arms. Optimal (upto logaritmic factor) social welfare regret    \\ \hline
%   \end{tabular}
%   \caption{Key findings of the paper. }
%   \label{tab:your_label}
% \end{table*}

\begin{table*}[ht!]
  \centering
  \begin{tabular}{@{}lcc  p{72mm}@{}}
    \toprule
      & Social Welfare Regret  & Fairness  Regret & \multicolumn{1}{c}{Remark}\\ \toprule
     Lower Bound (Sec~\ref{subsec:lowerBound})  & $\Omega(\sqrt{T})$  & $\Omega(\sqrt{T})$ &  The lower bounds hold individually for social welfare regret and fairness regret.     \\ \midrule
    \textsc{Explore-First}   (Sec~\ref{sec:warmup}) & $\tilde{O}(T^{2/3})$     & $\tilde{O}(T^{2/3})$ & For two arms, $C_i=c\leq  0.5\ \forall i\in [n]$.\\ \midrule
    \ouralgo \ (Sec~\ref{sec:UCB})  & $\tilde{O}(\sqrt{T})$  &   $\tilde{O}(T^{3/4})$  & For any finite number of arms. Optimal (up to logarithmic factor) social welfare regret.  \\ \bottomrule
  \end{tabular}
  \caption{Key findings of the paper. \piyushi{May be we should remove time horizon aware and in discussion write about doubling trick.}}
  \label{tab:your_label}
\end{table*}

