\onecolumn
\appendix 
\section{Missing Proofs From Sections~\ref{sec:settings_prelim} and~\ref{sec:warmup}}
\characterization* 
\begin{proof}
\noindent 
We prove the existence of fair policy in two cases separately. 
\newline
\noindent \textbf{Case 1 ($\sum_{i \in [n]} C_i \leq 1$):}   Consider without loss of generality that $C_i > 0$ for at-least one agent $i$.\footnote{For otherwise  if $\sum_{i}C_i =0$,  then for every policy $\pi \in \Delta_m$ we have $A_i\pi \geq 0$ for all $i \in [n]$ and hence every policy is feasible. 
} In particular, $\sum_{i} C_i > 0$. Let $j_i$ denote the smallest index $j \in [m]$ such that $j  \in \arg\max_{j' \in [m]} A_{i,j'}$. Consider the following policy. 
\begin{equation}
    \pi_j = \frac{\sum_{i=1}^n \mathds{1}(j = j_i) C_i}{\sum_{i=1}^n C_i}
    \label{eq:feasiblePolicy}
\end{equation}
First, observe that the policy given in Equation~\ref{eq:feasiblePolicy} is feasible i.e. is a valid probability distribution. It is easy to see that $\pi_j \geq  0$ for all $j$ and further 
\begin{align*}
    \sum_{j=1}^m \pi_j = \frac{\sum_{j=1}^m \sum_{i=1}^n \mathds{1}(j = j_i) C_i}{\sum_{i = 1}^n C_i} = \frac{ \sum_{i=1}^n C_i \sum_{j=1}^m \mathds{1}(j = j_i)}{\sum_{i = 1}^n C_i} = 1.   
\end{align*}
We now show that the policy obtained in Equation~\ref{eq:feasiblePolicy} satisfies fairness constraints. 
\begin{align*}
    A_i \pi  &= \sum_{j=1}^m A_{i,j}\pi_j =  A_{i,j_i}\pi_{j_i} + \sum_{j \neq j_i}^m A_{i,j}\pi_j   \geq  A_{i,j_i}\frac{C_{i}}{\sum_{i=1}^n C_i} \stackrel{(1)}{\geq} A_{i j_i} C_i =  C_i \Amax_i,     
\end{align*}
where inequality (1) above uses that $\sum_{i\in [n]}C_i\in (0, 1]$.
\newline
\textbf{Case 2 ($C_{\max} \leq \frac{1}{\min(n,m)}$): }    If $n \leq m $ then we have $ C_{\max} \leq \frac{1}{n}$ which implies that $\sum_{i=1}^n C_i \leq 1$. This is exactly the Case 1 discussed above. 

For $m < n$,  consider a policy $\pi = [1/m , 1/m, \cdots , 1/m]$ and note that 
\begin{align*}
    A_i \pi = 1/m \sum_{j =1}^m A_{i,j} \geq \frac{A_{i, j_i }}{m}  \stackrel{(2)}{\geq} C_{\max} A_{i, j_i } \geq C_i A_{i, j_i } = C_i \Amax_i,    
\end{align*}
where inequality (2) above uses that $C_{\max}\leq \frac{1}{m}$.
This completes the proof of the lemma. 
 \end{proof} 

\propOne*

\begin{proof}
Let us write the fairness constraints explicitly. Recall from the definition of $n_1$ that 
 for all $i \in [n_1]$  we have 
\begin{align*}
A_{i,1}x + (1-x) A_{i,2} \geq C_i    A_{i,1}   
\iff   x \geq \frac{C_i - \frac{A_{i,2}}{A_{i,1}}}{ 1 - \frac{A_{i,2}}{A_{i,1}}}. 
\end{align*}
Similarly for $i \in [n] \setminus [n_1]$ we have 
\begin{align*}
A_{i,1}x + (1-x) A_{i,2} & \geq C_i    A_{i,2}  
\iff   x  \leq \frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}}
\end{align*}
A policy $[x, 1-x]$ is feasible iff $\exists x \in [0,1]$ such that $ $ 

$$ \max \Bigg( 0,  \max_{i \in [n_1]}\frac{C_i - \frac{A_{i,2}}{A_{i,1}}}{ 1 - \frac{A_{i,2}}{A_{i,1}}} \Bigg) \leq x \leq \min \Bigg( 1,  \min_{i \in [n] \setminus [n_1]}\frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}} \Bigg). $$  The optimality of the policy $x^* = \min \Bigg( 1,  \min_{i \in [n] \setminus [n_1]}\frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}} \Bigg)$ follows from the fact that arm 1 is the optimal arm.  
\end{proof}

\section{Fairness and Social Welfare regret guarantees of \textsc{Explore-First} Algorithm}

Let $\mathcal{I}$ be a MA-MAB instance with $n \geq 1 $ agents and 2 arms. We begin with social welfare regret guarantee for \textsc{Explore-First}. We will first prove useful supporting results. We begin by stating  Chernoff's tail bounds, a well known result in probability theory. 

\begin{lemma}\label{lemma:chernoff-ltb}
[Chernoff's Lower Tail Bound] Let $X = \frac{1}{k}\sum_{i=1}^k X_i$ be the average of $k$ I.I.D. non-negative $\sigma$-sub-Gaussian random variables and let $ \mu=\mathbb{E}[X_i]$. Then for any $\delta\in [0, 1]$,
$$\mathbb{P}(X\leq (1-\delta) \mu)\leq \exp{\left(-\left(\frac{ \mu}{\sigma^2}\right)k\delta^2 \mu/2\right)}.$$
\end{lemma}

\begin{proof}
For any $\lambda\in \mathbb{R}; ~\lambda<0$,
\begin{align*}
\mathbb{P}(X\leq (1-\delta) \mu) = \mathbb{P}(e^{\lambda X}\geq e^{(1-\delta)\lambda \mu}) \stackrel{(M.I.)}{=} \frac{\mathbb{E}[e^{\lambda (X- \mu)}]}{e^{-\delta\lambda \mu}}\stackrel{(I.I.D.)}{=}\Pi_{i=1}^k \frac{\mathbb{E}[e^{\lambda/k(X_i- \mu)}]}{e^{-\delta\lambda \mu}}\leq  \frac{\Pi_{i=1}^s e^{\frac{\sigma^2 \lambda^2}{2k^2}}}{e^{-\delta\lambda \mu}}=e^{\frac{\sigma^2\lambda^2}{2k}+\delta\lambda \mu},
\end{align*}
where the second equality uses the Markov's inequality and the last inequality follows from the definition of $\sigma$-sub-Gaussian random variables. On choosing $\lambda = -\frac{k\delta  \mu}{\sigma^2}$ that gives the tightest upper-bound on the RHS, we obtain the final result.
\end{proof}

\begin{lemma}\label{lemma:chernoff-utb}
[Chernoff's Upper Tail Bound] Let $X = \frac{1}{k}\sum_{i=1}^k X_i$ be the average of $s$ I.I.D. non-negative $\sigma$-sub-Gaussian random variables and let $ \mu=\mathbb{E}[X_i]$. Then for any $\delta\in [0, 1]$,
$$\mathbb{P}(X\geq (1+\delta) \mu)\leq \exp{\left(-\left(\frac{ \mu}{\sigma^2}\right)k\delta^2 \mu/2\right)}.$$
\end{lemma}

\begin{proof}
For any $\lambda\in \mathbb{R}; ~\lambda>0$,
\begin{align*}
    \mathbb{P}(X\geq (1+\delta) \mu) = \mathbb{P}(e^{\lambda X}\geq e^{(1+\delta)\lambda \mu}) \stackrel{(M.I.)}{=} \frac{\mathbb{E}[e^{\lambda (X- \mu)}]}{e^{\delta\lambda \mu}}\stackrel{(I.I.D.)}{=}\Pi_{i=1}^k \frac{\mathbb{E}[e^{\lambda/k(X_i- \mu)}]}{e^{\delta\lambda \mu}}\leq  \frac{\Pi_{i=1}^s e^{\frac{\sigma^2 \lambda^2}{2k^2}}}{e^{\delta\lambda \mu}}=e^{\frac{\sigma^2\lambda^2}{2k}-\delta\lambda \mu},
\end{align*}
where the second equality uses the Markov's inequality and the last inequality follows from the definition of $\sigma$-sub-Gaussian random variables. On choosing $\lambda = \frac{k\delta  \mu}{\sigma^2}$ that gives the tightest upper-bound on the RHS, we obtain the final result.
\end{proof}
We also prove another useful lemma which will be used in bounding the regrets of \textsc{Explore-First}. We present the proofs with $C_i=c\ \forall i\in [n]$. We will use that both arms are equally pulled up to $2t'$. We consider $[n]\setminus [n_1]$ to be non-empty as otherwise, we have a trivial optimal policy. 
\begin{restatable}{lemma}{lemOne}
\label{lem:lemOne}
%Let $x^t$ be the probability that the \textsc{Explore-First} algorithm pulls arm $1$ for $t >  2 t'$ and let $\delta \in (0,1)$. 
Let  $a_{\min} : = \min_{i \in[n],j \in [m]} A_{i,j} > 0$ and $x^t$ be the \textsc{Explore-First} policy.  Then,  with  probability atleast $ 1 - 2 \exp(-t'\delta^2 a_{\min}^2/(2\sigma^2))$,  we have, 
$$ x^* - x^t \leq \frac{2c\delta}{(1+\delta)(1-c)}.$$  
\end{restatable}
\input{AAMAS25/Proof_Lemma7}

\begin{manualtheorem}{2}[Part (1): Social Welfare Regret of \textsc{Explore-First}] For any feasible MA-MAB instance $\mathcal{I}$ with $T$ sufficiently large and $C_i=c \forall i\in [n]$,     expected social welfare regret of \textsc{Explore-First} algorithm is upper-bounded by $O\left(\frac{n}{a_{\min} } T^{2/3} \sqrt{\log(T)}\right)$. Here $a_{min}:= \min_{i \in[n],j \in [m]} A_{i,j}$.  
\end{manualtheorem}
% \ExploreFirstFairnessRegret*
\begin{proof}
The social welfare regret for 2-arm MA-MAB can be written as 
$$\mathcal{R}_\textsc{SW}(T) =  \sum_{t=1}^T (x^* -x^t) \Delta. 
$$

Here, $\Delta: = \sum_{i=1}^n A_{i,1} - \sum_{i=1}^n A_{i,2} $ ($0<\Delta \leq n$) denote the difference between the social welfare generated by optimal  arm 1 and suboptimal arm 2. The expectation is taken over the instance, and also randomness in the policy.  



\iffalse 
\begin{restatable}{lemma}{ChernoffUpper}[Chernoff's Upper Tail Bound]
Let $X = \frac{1}{n} \sum_{i=1}^n X_i$ be the sample average  of $n$ iid random variables and let $a = \mathbb{E}[X_i]$. Then for any $\delta \in [0,1]$,  
\begin{align*}
\mathbb{P}(X \geq (1 + \delta) a) \leq \exp(- n \delta^2 a/3).  
\end{align*}
\label{lem:ChernoffUpper}
\end{restatable}    

\begin{restatable}{lemma}{ChernoffLower}[Chernoff's Lower Tail Bound]
Let $X = \frac{1}{n}\sum_{i=1}^n X_i$ be the average of $n$ iid random variables and let $a = \mathbb{E}[X_i]$. Then for any $\delta \in [0,1]$,  
\begin{align*}
\mathbb{P}(X \leq (1 - \delta) a) \leq \exp(- n \delta^2 a/2). 
\end{align*}
\label{lem:ChernoffLower}
\end{restatable}
\fi 




\iffalse 
\begin{theorem}
The EXPLORE-FIRST algorithm achieves the expected social welfare regret of $O\left(\frac{n}{(a/\sigma)}T^{2/3}\sqrt{\log(T)}\right)$.
\end{theorem}
\begin{proof}
\begin{align*}
    \mathcal{R}_{sw} &= \sum_{t=1}^T(x^*-x^t)\Delta\\
    &\leq n\left(2t' + (T-2t')\left[\left(1-2e^{-\left(\frac{ a^2}{\sigma^2}\right)2t'\delta^2/2}\right)\frac{2\delta c}{(1+\delta)(1-c)} + 2e^{-\left(\frac{ a^2}{\sigma^2}\right)2t'\delta^2/2} \right]\right)\\
    &\textup{Choose }2t'=T^{2/3} \textup{ and }\delta = \left(\frac{\sigma}{ a}\right)\sqrt{\frac{2\log{T}}{3T^{2/3}}} \textup{ with }T \textup{ large enough such that }\delta\in(0, 1).\\
    &\leq n\left( T^{2/3} + T\frac{2c\sqrt{\log{(T)}}}{(1-c)\left(a/\sigma\right)T^{1/3} } + 2T^{2/3}\right)\\
    &=nT^{2/3}\left(3+\frac{2\sqrt{2\log(T)}}{(a/\sigma)} \right)  \ (\because c\leq 1/2).
\end{align*}
    
\end{proof}
\fi 

Let $\widehat A$ denote the sample estimate of $A$ after the exploration phase i.e. first $ 2t' :=\lfloor T^\alpha \rfloor$ rounds. \footnote{We assume that $\lfloor T^\alpha \rfloor$ is even number. However, otherwise we can consider $\lfloor T^\alpha \rfloor$ +1 with additional regret of 1. This does not affect our regret guarantee in order terms.     }  %We will also write $E_{\delta} $ to denote an event $ \Big \{ \frac{\widehat{A}_{i,1} }{ \widehat{A}_{i,1}}  \leq  \frac{A_{i,1}}{A_{i,2}} - \frac{2\delta c}{1+ \delta } \Big \}$ for some $\delta > 0$.  
Furthermore, let the arm pull distribution of \textsc{Explore-First} algorithm  be denoted by $[x^t, 1-x^t]$ and $[x^*, 1-x^*]$ be  optimal feasible policy. 

% From Chernoff's upper tail bound (Lemma~\ref{lemma:chernoff-utb}) we have with probability atmost $\exp\left(- \Big(\frac{A_{i,2}}{\sigma^2} \Big)n \delta^2 A_{i,2}/2\right)$ that   
% \begin{equation}
% \widehat{A}_{i,2} \geq (1+ \delta) A_{i,2}.  
% \end{equation}

%  Similarly,  from Chernoff's lower tail bound (Lemma~\ref{lemma:chernoff-ltb}) we have with probability  atmost $\exp\left(-\Big(\frac{A_{i,1}}{\sigma^2} \Big) n\delta^2 A_{i,1}/2\right)$ that  
% \begin{equation}
% \widehat{A}_{i,1} \leq (1-\delta) A_{i,1}.  
% \end{equation}

Recall that the \textsc{Explore-First} algorithm pulls arm 1 with probability $1/2$ till first $2t'$ round and with probability $x^t$ for all $t\geq 2t'$. We now bound the expected social welfare regret.
\begin{align*}
\mathbb{E} \left[\mathcal{R}_\textsc{SW}(T)\right] & =  \sum_{t=1}^T \mathbb{E}[(x^* -x^t)  \Delta]\\ 
& = \sum_{t=1}^{2t'} (x^* - 1/2) \Delta + \sum_{t=2t' + 1}^{T} (\mathbb{E}[x^* -x^t]) \Delta \\
& \leq 2nt' + \left( \sum_{t=2t' + 1}^{T} \frac{2c\delta}{(1+\delta)(1-c)}  \mathbb{P}\Big(x^* - x^t \le \frac{2c\delta}{(1+\delta)(1-c)} \Big)  +  \sum_{t=2t' + 1}^{T}  \mathbb{P}\Big(x^* - x^t >  \frac{2c\delta}{(1+\delta)(1-c)} \Big)      \right) \Delta \tag{From Lemma~\ref{lem:lemOne}}\\  
& \leq 2nt' + n (T - 2t') \Big[ \left(1 - 2e^{-t'\delta^2a_{\min}^2/2\sigma^2}\right) \frac{2\delta c}{(1+\delta)(1-c)} +  2e^{-t' \delta^2  a_{\min}^2/2\sigma^2} \Big] \tag{From Lemma~\ref{lem:lemOne}}
\intertext{Choose $2t' = T^{2/3}$ and $\delta = \left(\frac{2\sigma}{ a_{\textup{min}}}\right)\sqrt{\frac{\log{T}}{3T^{2/3}}}$ with $T$ sufficiently large for $\delta\in (0, 1)$ to get,}  
\mathbb{E}\left[\mathcal{R}_\textsc{SW}(T)\right] & \leq n \left( T^{2/3} +  \frac{4cT^{2/3}\sqrt{\log{T}}}{(1-c)(a_{\min}/\sigma)} + T^{2/3} \right) \\ 
&\leq n \left( T^{2/3} +  \frac{4T^{2/3}\sqrt{\log{T}}}{(a_{\min}/\sigma)} + T^{2/3} \right) \tag{with the sufficient condition for feasibility, $c\in (0, 1/2]$}\\
&\leq 6\frac{n\sigma}{a_{\min}}T^{2/3}\sqrt{\log{(T)}}
\end{align*}
This proves the claim. Our proof needs $c<1$, which is justified because $c=1$ would simplify the corresponding fairness constraint to be $ \langle A_i,\pi\rangle = \Amax_i$, which in turn fixes certain coordinates of $\pi$ making the problem simpler.
\end{proof}



% Notice that  both the arms are pulled for equal number of rounds till $2t' \leq T$ many rounds in a round robin fashion. Furthermore, let $i_1$ be an arm such that $ i_1 \in \arg\min_{i \in[n]\setminus [n_1]} \frac{1}{1 - \frac{A_{i,1}}{A_{i,2}}}$ and let $i_2 \in \arg\min_{i : \widehat A_{i,1} < \widehat A_{i,2}}  \frac{1}{1 - \frac{\widehat A_{i,1}}{\widehat A_{i,2}}}$.  \footnote{If $\{ i : \widehat A_{i,1} < \widehat A_{i,2}\} = \emptyset$, pull the arm maximizing social welfare i.e. arm $1$. }  

% \piyushi{The following initial upper-bound for $x^*-x^t$ holds in the case $x^*<1$ so we can shift it inside Case1. The problem however is that the event $E_\delta$ is defined for $i_2$ but $x^*<1\implies \frac{A_{i_1, 1}}{A_{i_1, 2}}<c$, need not imply $\frac{A_{i_2, 1}}{A_{i_2, 2}}<c$ as $\frac{A_{i_1, 1}}{A_{i_1, 2}} \leq \frac{A_{i_2, 1}}{A_{i_2, 2}}$.}


% When  $i_2 \neq i_1$   we have  
% $x^* - x^t \leq (1-c) \left[ \frac{1}{1 - \frac{A_{i_1,1}}{A_{i_1,2}}} -   \frac{1}{1 - \frac{\widehat{A}_{i_2,1}}{\widehat{A}_{i_2,2}}} \right]  \leq (1-c) \left[ \frac{1}{1 - \frac{A_{i_2,1}}{A_{i_2,2}}} -   \frac{1}{1 - \frac{\widehat{A}_{i_2,1}}{\widehat{A}_{i_2,2}}} \right].$ 
% That is, the regret is upper bounded when we consider  $i_1=i_2$. Hence, throughout the proof we let, $i := i_1 = i_2 $. 


% We consider following two cases;  
% \newline
% \textbf{Case 1 ($x^* <1$)}: First consider the case $x^* < 1$, that is, $\frac{A_{i,1}}{A_{i,2}} \leq c$,   and let $\delta \in (0,1)$. % and let  $E_{\delta} = \{ \frac{\widehat{a}_{i,1}}{\widehat{a}_{i,2}}  \leq  \frac{A_{i,1}}{A_{i,2}}  -   \frac{2\delta}{1 + \delta} c \} $   %\frac{A_{i,1}}{A_{i,2}}  \}$ be an event. We have the following 

% %That is, with probability atmost $ {2 \exp(- n\delta^2 A_{i,1}/2)}$ 
% \begin{claim}
% Let $E_{\delta} = \Big \{ \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}} \leq  \frac{ A_{i,1}}{A_{i,2}}  - \frac{2\delta c }{ 1+ \delta} \Big \}$ for some $\delta \in (0,1)$ and assume that $\frac{A_{i,1}}{A_{i,2}} \leq  c$. Then,  
% $$ \mathbb{P}(E_{\delta} \cap \{ \widehat{A}_{i,2} \leq (1+ \delta) A_{i,2}\}) \leq  \exp(-t'\delta^2 A_{i,1}/2).$$ % and, 
%     %\item $\mathbb{P}(E_{\delta} \cap \{ \widehat{A}_{i,2} \leq  (1+\delta) A_{i,2} \} \cap \{ \widehat{A}_{i,1} >   (1 - \delta) A_{i,1} \} ) = 0. $$
% \label{lem:Two}
% \end{claim}
% \begin{proof}
% Observe that since $\frac{A_{i,1}}{A_{i,2}} \leq c$, the occurrence of event $E_{\delta}$ implies 
% \begin{align*}
% \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}} & \leq   \frac{A_{i,1}}{A_{i,2}} - \frac{2\delta }{1+\delta } \frac{A_{i,1}}{A_{i,2}} \\
% & = \frac{(1-\delta)A_{i,1}}{(1+\delta)A_{i,2}}
% \end{align*}
% This, with the fact that  $\widehat{A}_{i,2} \leq (1+\delta) A_{i,2}$, gives 
% \begin{align*}
% \widehat{A}_{i,1} & \leq (1-\delta) A_{i,1} \frac{\widehat{A}_{i,2}}{(1+\delta) A_{i,2}} \\
% \implies \widehat{A}_{i,1} & \leq (1-\delta) A_{i,1}
% \end{align*}
% Hence $E_{\delta} \cap \{ \widehat{A}_{i,2} \leq (1+ \delta) A_{i,2}\}  \subseteq   \{ \widehat{A}_{i,2} \leq (1+ \delta) A_{i,2}\} \cap \{ \widehat{A}_{i,1} <  (1 -  \delta) A_{i,1}\} $. This gives us 
% \begin{align*}
% \mathbb{P}(E_{\delta} & \cap \{ \widehat{A}_{i,2} \leq (1+ \delta) A_{i,2}\}) \\  & \leq  \mathbb{P}( \{ \widehat{A}_{i,2} \leq (1+ \delta) a_{i,2}\} \cap \{ \widehat{A}_{i,1} <  (1 -  \delta) A_{i,1}\}) \\ 
% &  =   \mathbb{P}( \{ \widehat{A}_{i,2} \leq (1+ \delta) A_{i,2}\}) \mathbb{P}( \{ \widehat{A}_{i,1} <  (1 -  \delta) A_{i,1}\}) \tag{as $\widehat{A}_{i,1}$ and $\widehat{A}_{i,2}$ are independent random variables}\\ 
% & \leq \exp\left(- \left( \frac{A_{i,1}}{\sigma^2} \right)t'\delta^2  A_{i,1}/2)\right) \tag{from Eq.~\toref}%\\ 
% %& \leq  \exp(-n\delta^2 A_{i,1}/2) 
% \end{align*}
% %\textcolor{red}{Q: can dependency on $a$ be removed? A: Yes, this dependency can be removed, we can discuss this in next meeting }
% \iffalse 
% To prove the second part of the claim, we show that $A := \{ \widehat{a}_{i,2} \leq  (1+\delta) a_{i,2} \} \cap \{ \widehat{a}_{i,1} >   (1 - \delta) a_{i,1} \} \subseteq \overline{E}_{\delta}$. This follows from the fact that $A  \implies 
%  \frac{\widehat{a}_{i,1}}{\widehat{a}_{i,2}} \geq \frac{\widehat{a}_{i,1}}{\widehat{a}_{i,2}} - \frac{2\delta}{1+ \delta} \frac{ a_{i,1}}{a_{i,2}}$. Since, $\frac{ a_{i,1}}{a_{i,2}} \leq c $, we have $A \implies \overline{E}_{\delta}$. Which shows that $A \cap E =\emptyset$, proving the stated result.       
%  \fi 
% \end{proof}

% \begin{claim}
% Let $a_{\min} : = \min_{i,j} A_{i,j}$ and $\delta \in (0,1)$ then 
% $$\mathbb{P}(E_{\delta}) \leq 2 \exp(-t'a_{\min}^2 \delta^2/2{\sigma^2} ).$$ 
% \label{Lem:EDeltaUpper}
% \end{claim}
% \begin{proof}
% \begin{align*}
% \mathbb{P}(E_{\delta}) &= \mathbb{P}(E_{\delta} \cap \{ \widehat{A}_{i,2} > (1+\delta) A_{i,2} \})   + \mathbb{P}(E_{\delta} \cap \{ \widehat{A}_{i,2} \leq  (1+\delta) A_{i,2} \})  \\
% %& \leq \underbrace{\mathbb{P}( \{ \widehat{a}_{i,1}  <  (1 -\delta) a_{i,1} \} \cap \{ \widehat{a}_{i,2} > (1+\delta) a_{i,2} \})}_{\text{Claim } \ref{lem:Two}} + \mathbb{P}(E_{\delta} \cap \{ \widehat{a}_{i,2} \leq  (1+\delta) a_{i,2} \} \cap \{ \widehat{a}_{i,1} \leq  (1 - \delta) a_{i,1} \} )  \\ &  \ \ + \underbrace{\mathbb{P}(E_{\delta} \cap \{ \widehat{a}_{i,2} \leq  (1+\delta) a_{i,2} \} \cap \{ \widehat{a}_{i,1} >   (1 - \delta) a_{i,1} \} )}_{ \text{Claim}  \ \ref{lem:Two} } \\
% %& \leq \exp(- n\delta^2 (a_{i,1}/2 + a_{i,2}/3)) + \exp(- n\delta^2 a_{i,1}/2) + 0 \\ 
% & \leq    \exp(- t'\delta^2 A_{i,2}^2/2 \sigma^2) + \exp(- t'\delta^2 A_{i,1}^2/2\sigma^2) \tag{from Eq.~\toref  and Lemma \toref } \\ 
% & \leq  2\exp(- t'\delta^2 a_{\min}^2 /2\sigma^2)
% \end{align*}
% \end{proof}

% We have,   
% \begin{align*}
%     x^* - x^t  &\leq  (1-c) \left(  \frac{1}{1 - \frac{A_{i,1}}{A_{i,2}}} - \frac{1}{1 - \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}}} \right) \\ 
%     & \leq (1-c) \left(  \frac{1}{1 - \frac{A_{i,1}}{A_{i,2}}} - \frac{1}{1 - \frac{ A_{i,1}}{A_{i,2}} + \frac{2\delta}{1+ \delta} c} \right) \tag{with probability  $\mathbb{P}(\overline{E}_{\delta}) \geq 1 -  2 \exp(-t'\delta^2 a_{\min}^2/2\sigma^2)  $}\\ 
%     & \leq \frac{2\delta c}{(1+\delta) (1-c)}
% \end{align*}


% \textbf{Case 2  ($x^* =1$)}: From the expression for $x^*$, we have that $x^*=1\implies \frac{A_{i, 1}}{A_{i, 2}}\geq c$. In this case, a nonzero  SW regret is incurred only if $x^t < 1$ i.e., $\frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}} \leq c$. So we have  
% \begin{align*}
% x^* - x^t \leq \frac{c - \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}}}{1 - \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}}}
% \end{align*}
% \piyushi{Issue: $E_\delta$ has the last term as $2\delta c/(1+\delta)$. Here, we can not replace $c$ by $\frac{A_{i2, 1}}{A_{i2, 1}}$.}
% Observe that the RHS is decreasing function in  $\frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}}$. From Claim~\ref{Lem:EDeltaUpper} above,  we have with probability  atleast $1 - 2 \exp(-t'\delta^2 a_{\min}^2/2\sigma^2)$,  
% \begin{align*}
% x^* - x^t & \leq \frac{c - \frac{A_{i,1}}{A_{i,2}} + \frac{2\delta}{1+\delta} \frac{A_{i,1}}{A_{i,2}}}{1 - \frac{A_{i,1}}{A_{i,2}} + \frac{2\delta}{1+\delta} \frac{A_{i,1}}{A_{i,2}}} \\
% & = \frac{c-\frac{1-\delta}{1+\delta}\frac{A_{i, 1}}{{A_{i, 2}}}}{1-\frac{1-\delta}{1+\delta}\frac{A_{i, 1}}{{A_{i, 2}}}}\\
% & \leq \frac{c(1+\delta) - (1-\delta)c }{(1+\delta) + (1-\delta)c} \left(\ \because \textup{the RHS is a decreasing function in }\frac{A_{i, 1}}{A_{i, 2}}\right) \\
% & \leq  \frac{2c\delta}{(1+\delta)(1-c)}
% \end{align*}

% This completes the proof of the lemma.
% % \textcolor{red}{We can optimize the results above further...I also think we dont need two different cases above... proof needs some polishing}
% \end{proof}
% \section{RE-WRITING LEMMA7 PROOF}
% \input{AAMAS25/Proof_Lemma7}

Next, we bound the fairness regret of \textsc{Explore-First} algorithm. We begin with a crucial lemma.
\begin{lemma}\label{fairness-reform}
The Fairness regret for the case with two arms simplifies as \begin{equation}
\mathcal{R}_\textsc{FR}(T) \leq \sum_{t=1}^T\sum_{i=1}^n \left|(A_{i,1}-A_{i,2})\cdot (x^*-x^t)\right|_+.
\end{equation}
\end{lemma}
\begin{proof}
This proof uses the characterization of the feasible policy presented in Proposition (\ref{prop:One}), restated below.
\begin{equation}
    \max \Bigg( 0,  \max_{i \in [n_1]}\frac{C_i - \frac{A_{i,2}}{A_{i,1}}}{ 1 - \frac{A_{i,2}}{A_{i,1}}} \Bigg) \leq x \leq \min \Bigg( 1, \min_{i \in [n] \setminus [n_1]} \frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}} \Bigg).
    \label{feasible-x}
\end{equation}

We first simplify $C_i\Amax_i-A_i^\top \pi^t$. Let $e^i\in \{0, 1\}^m$  be such that $e^i_j=1$, for $j\in\argmax_{j\in[m]}\{A_{i,j}\}$ having the least index.

We have, 
\begin{align}
    C_i\Amax_i-\langle A_i, \pi^t\rangle &=A_{i,1}e^i_1C_i+A_{i,2}(1-e^i_1)C_i - A_{i,1}x^t - A_{i,2}(1-x^t) \nonumber \\
    &=A_{i,1}\left( C_ie^i_1-x^t \right)-A_{i,2}\left(C_ie^i_1-x^t+1-C_i\right) \nonumber \\
    &= \left(A_{i,1}-A_{i,2}\right).\left( C_ie^i_1-x^t \right)-A_{i,2}(1-C_i)\nonumber \\
    & = (A_{i,1}-A_{i,2})\left( (C_ie^i_1-x^t)-\frac{A_{i,2}}{A_{i,1}-A_{i,2}}(1-C_i)\right)\label{simplify}
\end{align}
An implicit assumption is that $A_{i, 1}\neq A_{i,2}\forall i\in [n]$. This is without loss of generality because when $A_{i, 1}= A_{i,2}$, agent $i$ does not contribute to the fairness regret.
We now separately analyze the case for $i\in[n_1]$ and $i\in[n]\setminus [n_1]$.
\newline
\textbf{Case}: For $i\in[n_1]$ i.e. for $\{i\in [n]\ | A_{i,1}>A_{i,2}\}$. For such $i$'s, $e^i_1=1$, with which (\ref{simplify}) simplifies to the following.
\begin{align}
C_i\Amax_i-\langle A_i, \pi^t\rangle & = (A_{i,1}-A_{i,2})\left( (C_i-x^t)-\frac{A_{i,2}}{A_{i,1}-A_{i,2}}(1-C_i)\right)\nonumber \\
&=(A_{i,1}-A_{i,2})\left( \frac{C_iA_{i,1}-C_iA_{i,2}-A_{i,2}+C_iA_{i,2}}{A_{i,1}-A_{i,2}}-x^t \right)\nonumber \\
&=(A_{i,1}-A_{i,2})\left( \frac{C_iA_{i,1}-A_{i,2}}{A_{i,1}-A_{i,2}}-x^t \right)\nonumber \\
&=(A_{i,1}-A_{i,2})\left( \frac{C_i-\frac{A_{i,2}}{A_{i,1}}}{1-\frac{A_{i,2}}{A_{i,1}}}-x^t \right)\nonumber \\
& \leq (A_{i,1}-A_{i,2})\cdot (x^*-x^t) \quad \tag{From (\ref{feasible-x})}. \label{FR-i1}
\end{align}
\newline
\textbf{Case}: $i\in[n]\setminus [n_1]$ i.e. for $\{i\in [n]\ | A_{i,1}<A_{i,2}\}$. For such $i$'s, $e^i_1=0$, with which (\ref{simplify}) simplifies to the following.
\begin{align}\label{FR-i2}
C_i\Amax_i-\langle A_i, \pi^t\rangle & = (A_{i,1}-A_{i,2})\left( -x^t-\frac{A_{i,2}}{A_{i,1}-A_{i,2}}(1-C_i)\right)\nonumber \\
& = (A_{i,1}-A_{i,2})\left( -x^t + \frac{1-C_i}{1-\frac{A_{i,1}}{A_{i,2}}} \right)\nonumber \\
&\leq (A_{i,1}-A_{i,2})\cdot (-x^t+x^*) \quad \tag{From (\ref{feasible-x}), and using that $(A_{i,1}-A_{i,2})<0$} 
\end{align}
Combining the above two cases, we have that $C_i\Amax_i-\langle A_i, \pi^t\rangle\leq (A_{i,1}-A_{i,2})(x^*-x^t)$ which proves $\mathcal{R}_{\textsc{FR}}(T)=\sum_{t=1}^T\sum_{i=1}^n|C_i\Amax_i-\langle A_i, \pi^t\rangle|_+\leq \sum_{t=1}^T\sum_{i=1}^n(A_{i,1}-A_{i,2})(x^*-x^t)$.    
\end{proof}

\input{AAMAS25/fairness-regret_ef}

\section{Fairness and Social Welfare Regret Guarantees of \textsc{RewardFairUCB}}
% \begin{lemma}[Hoeffding's Inequality]
% Let $X_1, X_2, \cdots, X_s$ be independent $\sigma^2$-sub-gaussian random variables. Then,
% \begin{equation}
%     \mathbb{P}\left( \bigg| \frac{1}{s} \sum_{i=1}^s ( X_i - \mathbb{E}(X_i) )\bigg| \geq \varepsilon \right) \leq   2\exp\left({- \frac{ s \varepsilon^2}{2\sigma^2} }\right) \ \ \ \text{ for all }  \varepsilon > 0.
%     \end{equation}
% \end{lemma}

We first bound the social welfare regret of \ouralgo . We begin with an important lemma from \cite{wang2021fairness}.   
  \begin{restatable}{lemma}{MartingaleLemma}[Lemma A.4.2 in \cite{wang2021fairness}]\label{lemma:wang21}
For any $\delta \in (0,1)$, with probability $1 - \delta/2$ we have 
\begin{equation}
    \Big |\sum_{t=\tau+1}^T \mathbb{E}_{j \sim \pi^t} \sqrt{1/N_j^t} - \sum_{t=\tau+1}^T \sqrt{1/N_{j_t}^{t}} \Big | \leq \sqrt{2T\log(4/\delta)}
\end{equation}
where $N_j^t$ represents the number of times arm $j$ has been selected up to time $t$, and $j_t$ denotes the arm actually chosen at time $t$.
\end{restatable}
\begin{proof} We re-write the proof of [Lemma A.4.2 in \cite{wang2021fairness}].
Consider the sequence $\{Z^t\}_{t=\tau+1}^T$ with 
$
    Z^t := \sqrt{\frac{1}{N_{j_t}^t}} - \mathbb{E}_{j \sim \pi^t} \sqrt{\frac{1}{N_j^t}}
$. This sequence is a martingale difference sequence. Additionally, for any $t > \tau$, we have
\[
    |Z^t| = \left|\sqrt{\frac{1}{N_{j_t}^t}} - \mathbb{E}_{j \sim \pi^t} \sqrt{\frac{1}{N_j^t}}\right| \leq 1,
\]
since the square root function is Lipschitz with a constant of 1, and the counts $N_j^t$ are non-negative.

Thus, we can apply Azuma-Hoeffding's inequality to bound the sum of the martingale difference sequence:
\[
    \mathbb{P} \left( \Bigg| \sum_{t=\tau+1}^T Z^t \Bigg| \geq \epsilon \right) \leq 2 \exp\left( -\frac{\epsilon^2}{2(T-\tau)} \right).
\]
Setting $\epsilon = \sqrt{2(T-\tau) \log(4/\delta)}$ gives that with probability at least $1 - \delta/2$,
\[
    \Bigg| \sum_{t=\tau+1}^T Z^t \Bigg| \leq \sqrt{2T \log\left(\frac{4}{\delta}\right)}.
\]
This concludes the proof.
\gan{@Himanshu: Write the proof of the lemma}
\end{proof}


\rewardRegretUCB*
 \begin{proof}
\begin{align}
    \mathcal{R}_\textsc{SW}(T) &= \sum_{t=1}^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] \nonumber \\ 
&\leq  \underbrace{\sum_{t=1}^{ m\lceil \sqrt{T} \rceil} \sum_{i=1}^n \langle A_i, \pi^* - \pi^t \rangle}_{\textup{R1}}   + \underbrace{ \sum_{t= m\lceil \sqrt{T } \rceil + 1 }^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] }_{\textup{R2}} 
\end{align}
We bound terms R1 and R2 separately. We begin with an upper bound for R1. 
\begin{align*}
 \textup{R1} &=  \sum_{t=1}^{ m\lceil \sqrt{T} \rceil} \sum_{i=1}^n \langle A_i, \pi^* - \pi^t \rangle 
 \\ &=  \sum_{j =1}^m\sum_{t \leq (m\lceil \sqrt{T} \rceil \land j^t =j) } \  \sum_{i=1}^n   A_{i,j} ( \pi_j^* - \pi^t_j )  \tag{where $j^t$ denotes the arm chosen at timestep $t$}\\
&\leq \sum_{j =1}^m\sum_{t \leq (m\lceil \sqrt{T} \rceil \land j^t =j) }\  \sum_{i=1}^n   A_{i,j}  \pi_j^*    \\ 
&\leq  \sqrt{T}  \sum_{j =1}^m \Big (\sum_{i=1}^n   A_{i,j} \Big )  \pi_j^* 
\\ & = \sqrt{T} \left\langle \sum_{i=1}^n   A_i, \pi^*  \right\rangle   \\ 
&\leq  \sqrt{T}  \left\| \sum_{i=1}^n   A_{i}\right\|_2  \|\pi^*\|_2    \\ 
&\leq \sqrt{T} n\sqrt{m}\sqrt{m}  = mn\sqrt{T}.
\end{align*}
Next, we give an upper bound on R2. The proof uses the notation $\epsilon^t_{j}=\frac{\epsilon}{\sqrt{N_j^t}}=\sigma \sqrt{\frac{2\log{(8mn/\delta)}}{N_j^t}}$. 
\begin{align*}
   \textup{R2} &= \sum_{t= m\lceil \sqrt{T } \rceil + 1 }^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] \\ 
   & \leq \sum_{t= m\lceil \sqrt{T } \rceil + 1 }^T \big[ SW_{\pi^t}(\overline{A}^t)  - SW_{\pi^t}(A)  \big] \ \tag{w.p. at least $1-\delta/2$; From Lemma \ref{lem:equivalence}}\\  
%\intertext{ The above inequality holds  with probability atleast $1-\delta$ +Lemma \ref{lem:equivalence}.}
  & \leq  \sum_{t= m\lceil \sqrt{T } \rceil +1}^T \sum_{i=1}^n \langle \overline{A}_i^t - A_i,  \pi^t \rangle \\ 
%\end{align*} % From lemma %Using Hoeffding's inequality with $\delta' = \delta/2mn $ for each $A_{i,j}$ then applying  union bound gives  the following upper bound with probability atleast $1-\delta$.
%\begin{align*}
    & \leq 2n \sum_{t=m+1}^T \mathbb{E}_{j \sim \pi^t} \left[\varepsilon^{t}_{j}\right]   \\
    &=   2 n \varepsilon \sum_{t=m+ 1}^T \mathbb{E}_{j \sim \pi^t}\left[\sqrt{1/N_j^t}\right] \\ 
& \leq   2n \varepsilon \left( \sum_{t=1}^T \sqrt{1/N_{j^t}^{t}} + \sqrt{2T\log(2/\delta)} \right) \ \tag{w.p. at least $1-\delta/2$; From Lemma \ref{lemma:wang21}, where $j^t$ denotes the arm chosen at $t$}\\
& \leq  2n\sigma \sqrt{2\log(8mn/\delta)}  ( \sqrt{T} + \sqrt{2T\log(2/\delta)}) \tag{$\because N^t_{j}\geq \sqrt{T} \ \forall j \in [m]$} \\
&\leq 2n\sigma \left(\sqrt{2T\log{(8mn\sigma)}} + \sqrt{2T}\log{(8mn/\delta)} \right)\\
&\leq 4n\sigma  \sqrt{2T}\log{(8mn/\delta)}
\end{align*}
We have the following bound on the expected regret
\begin{align*}
(1-\delta)(R_1+R_2)+\delta T \leq (1-\delta)(4n\sigma \sqrt{2T}\log{(8mn/\delta)} + mn\sqrt{T})  + \delta T\leq 4n\sigma \sqrt{2T}\log{(8mn/\delta)} + mn\sqrt{T} + \delta T.
\end{align*}
We choose $\delta=\frac{4\sqrt{2}n\sigma }{\sqrt{T}}$ (with $T$ large enough to have $\delta<1$) to get the tightest upper bound on the RHS. This proves the stated upper-bound on the social welfare regret of \ouralgo.

\end{proof}

\input{AAMAS25/ucb_fr}
\section{Lower Bounds}
\lowerRegret*
\begin{proof}
In addition to the instance described in Sec~\ref{subsec:lowerBound} for proving the lower bound on the social welfare, we observe that our MA-MAB problem for a single agent ($n=1$) and $C$ as the zero matrix reduces to the Nash Social Welfare based MA-MAB problem \cite{Hossain2020FairAF} with $n=1$. The lower bound of $\Omega(\sqrt{mT})$ for social welfare regret then follows from Proposition (2) in Appendix A of \citep{Hossain2020FairAF}.

    We now discuss the lower bound for the fairness regret. We observe that it is enough to show the lower bound for $m=2$ case as we can easily construct instances of the problem with more than 2 arms that reduce to the case with $m=2$, e.g. by choosing an $A$ matrix with $m-2$ columns as zeros. The proof of lower bound for the fairness regret in the 2-arm case proceeds as follows. From Lemma~\ref{fairness-reform}, we have that $\mathcal{R}_{FR}(T) \leq \sum_{t=1}^T\sum_{i=1}^n \left|(A_{i,1}-A_{i,2}).(x^*-x^t)\right|_+$. To prove the lower bound, we construct an instance where this inequality is tight. We first recall an inequality from the proof of characterization of the optimal feasible policy ($[x, 1-x]$) in Lemma~\ref{prop:One}.
    \begin{align}\label{charac-for-lb}
    \max \Bigg( 0,  \max_{i \in [n_1]}\frac{C_i - \frac{A_{i,2}}{A_{i,1}}}{ 1 - \frac{A_{i,2}}{A_{i,1}}} \Bigg) \leq x \leq \min \Bigg( 1,  \min_{i \in [n] \setminus [n_1]}\frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}} \Bigg). 
    \end{align}
    Also, following the steps in the proof of Lemma~\ref{fairness-reform}, we have the following.
    \begin{align}\label{FR-lb}
        \mathcal{R}_{FR}(T) &= \sum_{t=1}^T\left( \sum_{i\in [n_1]}|C_i\Amax_i - \langle A_i, \pi^t \rangle|_+ + \sum_{i\in [n]\setminus [n_1]}|C_i\Amax_i - \langle A_i, \pi^t \rangle|_+\right) \nonumber \\
        &= \sum_{t=1}^T\left(\sum_{i\in [n_1]}
        \left| (A_{i, 1}-A_{i, 2})\left(\frac{C_i-\frac{A_{i, 2}}{A_{i, 1}}}{1-\frac{A_{i, 2}}{A_{i, 1}}}-x^t \right)\right|_+
        +
        \sum_{i\in [n]\setminus [n_1]} \left|(A_{i, 1}-A_{i, 2})\left( \frac{1-C_i}{1-\frac{A_{i, 1}}{A_{i, 2}}} - x^t \right)\right|_+ \right)
    \end{align}
    Consider an instance where $\frac{C_i-\frac{A_{i, 2}}{A_{i, 1}}}{1-\frac{A_{i, 2}}{A_{i, 1}}} = l(>0) \ \forall i\in [n_1]$ and $\frac{1-C_i}{1-\frac{A_{i, 1}}{A_{i, 2}}}=u(<1) \ \forall i\in [n]\setminus [n_1]$ and $l=u$. In this case, $x^*=l=u$ and the fairness regret (Eq.~\ref{FR-lb}) reduces to the following.
    \begin{align*}
        \mathcal{R}_\textsc{FR}(T) &= \sum_{t=1}^T\sum_{i=1}^n \left|(A_{i,1}-A_{i,2})\cdot (x^*-x^t)\right|_+ \\
        &\geq   \left|\sum_{t=1}^T\sum_{i=1}^n (A_{i,1}-A_{i,2})\cdot (x^*-x^t) \right|_+ \ \tag{Triangular inequality}\\
        &= \left| \mathcal{R}_\textsc{SW}(T) \right|_+ \tag{From Eq.~\ref{eq:eqnClosedFormSW}}\\
        &\geq   \Omega(\sqrt{T})  \ \tag{From the lower bound for social welfare}.
    \end{align*}
    Finally, we present an instance where the fairness regret inequality is tight. Let $\underline{i}\in \argmax_{i\in [n_1]}\left(\frac{C_i-\frac{A_{i, 2}}{A_{i, 1}}}{1-\frac{A_{i, 2}}{{A_{i, 1}}}} \right)$ and $\overline{i}\in \left(\argmin_{i\in [n]\setminus [n_1]} \frac{1 - C_i}{ 1 - \frac{A_{i,1}}{A_{i,2}}}\right)$ be the least indices. Consider an instance with $C_{\underline{i}} = 1-C_{\overline{i}}$ and an $A$ matrix such that $A_{\underline{i}, 2}=0$ and $A_{\overline{i}, 1}=0$, e.g. $A=\begin{bmatrix}
    1 & 0\\
    0 & 1\\
    1 & 0
    \end{bmatrix}$. For this instance, any optimal feasible policy $[x^*, 1-x^*]$ must satisfy $C_{\underline{i}}\leq x^* \leq 1-C_{\overline{i}} \implies x^* = C_{\underline{i}}$.

\end{proof}
\input{AAMAS25/simulation_Appendix}
