\input{AAMAS25/dual} 
\section{Experimental Evaluation}
\label{sec: simulation}

\begin{figure*}[ht!]
\centering
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/oct8-ef.jpg}  
    \caption{Explore-Exploit tradeoff with {\sc Explore-First}.
    % : The tradeoff between Social Welfare regret and Fairness regret is shown as a function of exploration parameter $\alpha$ (displayed as legends). We can see that $\alpha=0.67$ obtains the optimal trade-off supporting our theoretical result.
    }
    \label{sim1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/SW-sim.jpg}  
    \caption{Social welfare regrets vs timesteps.}
    \label{sim2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/FR-sim.jpg}  
    \caption{Fairness regrets vs timesteps.}
    \label{sim3}
\end{subfigure} 
    \caption{Experimental results on simulated data ($n=4, m=3$). $C_i$ is $0.3\ \forall i \in [n]$.}
    \label{fig:rewardfair_ucb-sim}
    \Description{Experimental results on simulated data.}
\end{figure*} 

\begin{figure*}[ht!]
\centering
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/oct8-ef-mlens.jpg}  
    \caption{Explore-Exploit tradeoff with {\sc Explore-First}.
    % : The tradeoff between Social Welfare regret and Fairness regret is shown as a function of exploration parameter $\alpha$ (displayed as legends). We can see that $\alpha=0.67$ obtains the optimal trade-off supporting our theoretical result.
    }
    \label{rw1}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/SW-mlens_TMP.jpg}  
    \caption{Social welfare regrets vs timesteps.}
    \label{rw2}
\end{subfigure}
\begin{subfigure}{.33\textwidth}
    \centering
    \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/FR-mlens_TMP.jpg}  
    \caption{Fairness regrets vs timesteps.}
    \label{rw3}
\end{subfigure}
    \caption{Experimental results on MovieLens real-world data ($n=6039, m=18$). $C_i$ is $1/m \ \forall i \in [n]$.}
    \label{fig:rewardfair_ucb}
    \Description{Experimental results on MovieLens data.}
\end{figure*} 

% \begin{figure*}[ht!]
%     \centering
%     \begin{subfigure}{0.49\textwidth}
%         \centering
%         \includegraphics[width=\linewidth]{AAMAS25/Empirical_Piyushi/New/oct8-ef-mlens.jpg}
%         \caption{Explore-Exploit Tradeoff}  
%         \label{fig:explore_exploit_tradeoff}
%     \end{subfigure}
%     \begin{subfigure}{0.49\textwidth} 
%         \centering
%         \includegraphics[width=\linewidth]{AAMAS25/plots_movie_lens/Real_world_data_UCB.png}
%         \caption{RewardFairUCB Performance}
%         \label{fig:rewardfair_ucb}
%     \end{subfigure}
%     \caption{Performance of algorithms on the MovieLens 1M dataset. (a) Tradeoff between exploration and exploitation. (b) Sublinear trend of social welfare and fairness regret using RewardFairUCB.} 
% \end{figure*}


% \gan{@Piyushi can you add the empirical results we have so far here?}
We now empirically validate the sublinear regret guarantees of the proposed algorithms and the efficacy of \ouralgo. 
\subsection{Common Experimental Setup}
The distribution $\mathcal{D}(\mu_{i,j})$ is taken $\textup{Ber}(\mu_{i, j})$ i.e. when an arm $j'\in [m]$ is sampled, agent $i\in [n]$ obtains a reward drawn from Bernoulli with parameter $\mu_{i, j}$. We plot the average regrets after simulating with 100 runs for different realizations of randomly sampled rewards. Complementing our analysis for \textsc{Explore-First} in the 2-arm case, we empirically find that the same regret guarantees for $m>2$ hold with same optimal exploration parameter valued, i.e.,  $\alpha$ = 0.67. For this, we replace Step (6) of \textsc{Explore-First} algorithm with the solution of the linear program P1 obtained with empirical reward estimates $\hat{A}$. The CVXPY \citep{diamond2016cvxpy} library is used to solve the linear programs wherever required in our algorithms. More results with different $A$ matrices are presented in the Appendix.
\subsection{Experiments on Simulated Data} 
Figure~\ref{fig:rewardfair_ucb-sim} shows results with a mean reward matrix $A$ of size (4, 3), i.e. $n=4, \ m=3$. The stopping time $T=10^5$ and $C_i=c=0.3\ \forall i\in [n]$. We first empirically show the trade-off between exploration and exploitation by plotting the social welfare regret and fairness regret of \textsc{Explore-First} algorithm on varying the exploration parameter $\alpha$. Figure~\ref{sim1} shows the plots comparing the social welfare regret and the fairness regret on varying the exploration parameter $\alpha$ from $\{0.1, 0.2, \cdots, 1.0, 0.67\}$, marked in the legend. The plotted regret values are after normalizing by $T$. The empirically observed best choice for obtaining low regrets for both social welfare and fairness is $\alpha=0.67$ which closely matches the theoretically optimal value of $\alpha=2/3$ derived for the 2-arm case in Sec~\ref{sec:warmup}. 

Figures~\ref{sim2} and~\ref{sim3} respectively compare the social welfare regrets and the fairness regrets of \ouralgo \ with the \textsc{Explore-First} baseline (with $\alpha=0.67$) and the dual heuristic (Sec~\ref{sec:dual}). Figure~\ref{sim2} shows that \ouralgo\ not only obtains a sub-linear regret but also outperforms the baselines and heuristics. We can also see sublinear regrets obtained by \textsc{Explore-First}, supporting our theoretical claim derived for the 2-arm case. Figure~\ref{sim3} demonstrates sublinear fairness regret of \ouralgo. While the \textsc{Explore-First} baseline and the dual-based heuristic obtain a lower fairness regret, they incur an excess social welfare regret. \ouralgo\ achieves optimal social welfare performance while maintaining a sublinear fairness regret.

\subsection{Experiments on Real-World Data}
Figure~\ref{fig:rewardfair_ucb} shows the performance of our algorithm on real-world data, MovieLens 1M \cite{10.1145/2827872}. MovieLens comprises ratings given by users to different movies. We obtain a user-genre matrix with the average rating that users assign to each movie genre. This matrix is normalized to have each entry in $[0, 1]$ and serves as the mean reward matrix $A$. For the movies associated with multiple genres, their contribution to each genre was divided equally. 

The $A$ matrix for this experiment is of size (6039, 18), i.e. $n=6039, \ m=18$. The stopping time $T=10^5$ and $C_i=c=1/m\ \forall i\in [n]$. We first empirically show the trade-off between social welfare regret and fairness regret of \textsc{Explore-First} algorithm on varying the exploration parameter $\alpha$.

We begin by empirically illustrating the effect of exploration and exploitation trade-off, controlled by the exploration parameter $\alpha$, on the social welfare regret and fairness regret of the \textsc{Explore-First}. Figure~\ref{rw1} shows the two regrets (normalized by $T$) with $\alpha$ values ranging from $\{0.1, 0.2, \cdots , 1.0, 0.67\}$ as marked in the legend. The empirically determined optimal that minimizes both social welfare regret and the fairness regret is $\alpha=0.67$ which closely matches the theoretically optimal $\alpha=2/3$ derived for the 2-arm case in Sec~\ref{sec:warmup}. Figures~\ref{rw2} and~\ref{rw3} compare the social welfare and fairness regrets of \ouralgo\ with the baseline algorithm \textsc{Explore-First} and the dual-heuristic. Figures~\ref{rw2} and~\ref{rw3} empirically demonstrate that \ouralgo\ obtains a sublinear regret for both social welfare and fairness. Although the \textsc{Explore-First} baseline and the dual-based heuristic achieve lower fairness regret, they lead to a higher social welfare regret. \ouralgo\ performs optimally in terms of social welfare and still obtains a sublinear fairness regret. The empirical results with \textsc{Explore-First} also support our theoretically sublinear regret claim that was derived for the 2-arm case. 
% \himanshu{Do we need to add more details for the Movie lens dataset and also the plot for Explore First should we add? There are more details of the experiment like the matrix dimension, dataset details}
% \gan{Yes, Himanshu. Finish it by today.}
% To implement our algorithm on the real-world dataset, we used the to demonstrate the efficacy of our approach. The dataset includes user ratings for various movies, and we transformed it to create a user-genre matrix, which is crucial for our experiment. Each entry in this matrix represents the average normalized rating that users have assigned to each movie genre. To construct this matrix, each userâ€™s rating was normalized between 0 and 1, and for movies associated with multiple genres, the contribution to each genre was divided equally.


\iffalse 

\begin{figure*}
\centering
\begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_0_100000.jpg}  
    \caption{}
    \label{SUBFIGURE LABEL 2}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_6_100000.jpg}  
    \caption{}
    \label{SUBFIGURE LABEL 1}
\end{subfigure}
\begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_21_100000.jpg}  
    \caption{}
    \label{SUBFIGURE LABEL 3}
\end{subfigure}

\begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=.95\linewidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_21_100000.jpg}  
    \caption{}
    \label{SUBFIGURE LABEL 3}
\end{subfigure}
\caption{FIGURE CAPTION}
\label{FIGURE LABEL}
\end{figure*}

\iffalse 
\begin{figure*}[h!]
    \centering
    \includegraphics[width=0.3\textwidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_0_100000.jpg}
    \includegraphics[width=0.3\textwidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_6_100000.jpg}
        \includegraphics[width=0.3\textwidth]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/TRADEOFF/0.5_21_100000.jpg}
    \caption{Explore-First for 2-arm case: The tradeoff between Social Welfare regret and Fairness regret is shown as a function of exploration parameter $\alpha$ (marked with different colors). $C_i$ is 0.5, and different plots correspond to different mean reward matrix $A$. The values reported are averaged across 100 runs with different random seeds. We can see that $\alpha=0.67$ obtains the optimal trade-off supporting our theoretical claim.}
    \label{fig:EF2}
\end{figure*}
\fi 

Figure (\ref{fig:EF-reg}) shows the regrets as a function of $T$ when $\alpha=2/3$. The regrets grow sub-linearly with a rate $\tilde{O}(T^{2/3})$.

\begin{figure}[h!]
    % \centering
    \includegraphics[scale=0.25]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/REGRET/0.5_0_100000.jpg}
    \includegraphics[scale=0.25]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/REGRET/0.5_6_100000.jpg}\\
    \includegraphics[scale=0.25]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/REGRET/0.5_21_100000.jpg}
    \includegraphics[scale=0.25]{AAMAS25/Empirical_Piyushi/EF_Oct1/EF2/REGRET/0.5_23_100000.jpg}
    \caption{Explore-First for 2-arm case: The plots show the Social Welfare Regret (SWR) and Fairness Regret (FR) as a function of $t$ on varying the mean reward matrix $A$. The values reported are averaged across 100 runs with different random seeds. The exploration hyperparameter $\alpha$ is set to the optimal value, i.e.$0.67$, and $C_i$ is 0.5. We can see that both the fairness and social welfare regrets are of the order $\tilde{O}(T^{2/3})$.}
    \label{fig:EF-reg}
\end{figure}

\subsection{Results with RewardFairUCB algorithm}
Figure (\ref{fig:ucb}) shows the simulation results with the stopping time $T=10^5$. The results validate our theoretical bounds of $\tilde{O}(\sqrt{T})$ for the social welfare regret and $\tilde{O}(T^{3/4})$ for the fairness regret. More results with different $A$ matrices are presented in the Appendix.

\begin{figure}[h!]
\includegraphics[scale=0.25]{Empirical_Piyushi/UCB_Oct1/0.5_0.jpg}
\includegraphics[scale=0.25]{Empirical_Piyushi/UCB_Oct1/0.5_6.jpg}\\
\includegraphics[scale=0.25]{Empirical_Piyushi/UCB_Oct1/0.5_19.jpg}
\includegraphics[scale=0.25]{Empirical_Piyushi/UCB_Oct1/0.5_23.jpg}
\vspace{-0.1in}
\caption[short]{Results of the proposed RewardFairUCB Algorithm (\ref{algUCB}): The plots show the Social Welfare Regret (SWR) and Fairness Regret (FR) as a function of $t$ on varying the mean reward matrix $A$. The values reported are averaged across 100 runs with different random seeds. $C_i$ is 0.5. We can see that the social welfare regret is of the order $\tilde{O}(T^{3/4})$ and the fairness regret is of the order $\tilde{O}(T^{1/2})$.}\label{fig:ucb}
\end{figure}

\subsection{Results with Dual-Inspired Algorithm}
In Figure (\ref{fig:dual}), we present results with Algorithm \ref{alg-dual} with stopping time as $T=10^5$. We empirically observe the social welfare regret and the fairness regret grow $\tilde{O}(\sqrt{T})$.

\begin{figure}[h!]
\includegraphics[scale=0.25]{Empirical_Piyushi/Dual_Plots_Oct1/Lambda_once/0.5_0.jpg}
\includegraphics[scale=0.25]{Empirical_Piyushi/Dual_Plots_Oct1/Lambda_once/0.5_6.jpg}\\
\includegraphics[scale=0.25]{Empirical_Piyushi/Dual_Plots_Oct1/Lambda_once/0.5_19.jpg}
\includegraphics[scale=0.25]{Empirical_Piyushi/Dual_Plots_Oct1/Lambda_once/0.5_23.jpg}
\vspace{-0.1in}
\caption[short]{Results of the proposed Dual-Inspired Algorithm (\ref{alg-dual}): The plots show the Social Welfare Regret (SWR) and Fairness Regret (FR) as a function of $t$ on varying the mean reward matrix $A$. The values reported are averaged across 100 runs with different random seeds. $C_i$ is 0.5. We can see that both the fairness and social welfare regrets are of the order $\tilde{O}(T^{1/2})$.}\label{fig:dual}
\end{figure}
\fi 