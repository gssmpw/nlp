\section{Setting and Preliminaries}\label{sec:settings_prelim}

\gan{Para: Notation and basic setting }
We write $[n]$ to denote the set $\{1, 2, \cdots , n \}$. Further, we will assume that the set of agents ($[n]$) and the set of arms  ($[m]$) are both finite sets. 
%\himanshu{ For reference, we use the following notation conventions throughout the paper: \\
%- The notation \(\{..\}^t\) denotes the time step. \\
%- The notation \(\{..\}^\star\) is reserved for indicating the maximum operation, while \(*\) represents the optimal policy. \\
%- Additionally, we predominantly employ capital letters for notation.}
Let $\mathcal{D}(\mu_{i,j})$ denote a  probability distribution with finite  mean $\mu_{i,j}$. Further, let the random variable  $X_{i,j}$ denote a random reward obtained by agent $i$ from arm $j$, that is  $X_{i,j} \sim \mathcal{D}(\mu_{i,j})$. Finally, we  $X_i$  to denote a vector of size $m$ with $i^{\rm{th}}$ entry  $ X_{i,j}$.

 
               A fair Multi-Agent Multi-Armed Bandit (MA-MAB)  instance $\mathcal{I}$ is denoted by a tuple $ \langle A, C, T \rangle $ where,  
\begin{itemize}
    \item $A$ denotes an $n \times m$ non-negative matrix with each entry $A_{i,j} := \mu_{i,j}$. Note that $A$ is fixed but unknown to the algorithm. We will assume, without loss of generality \footnote{ It is easy to see that if all the  entries  are divided by the largest row entry, the optimal strategy does not change.}  that $A_{i,j} \in [0,1]$ for all $i \in [n]$ and $j \in [m]$.  Further, define   $\Amax$ to be a matrix in $[0,1]^n$ whose  $i^{\rm{th}}$ entry represents the maximum possible expected reward  to agent $i$. $\Amax_i = \max_{j \in [m]} A_{i,j}$.  
    \item $C $ denotes $n \times n$ non-negative diagonal matrix. The entry $C_i : = C_{i,i} \in [0,1]$ specifies the fraction of maximum possible rewards to be guaranteed to agent $i$. Note that $C$ is a pre-defined constant and does not change with time. 
    \item $T$ denotes the stopping time of the algorithm. We assume that $T$ is known a priori. However,  all our results can be extended to unknown time horizon setting using a doubling trick (see \cite{besson2018doubling}) with an additional constant multiple factor increase in the regret.  
\end{itemize}

We now formally define the notion of minimum-reward guarantee fairness regret for a given MA-MAB instance $\mathcal{I}$. We begin by first defining the notion of minimum-reward guarantee.   
%  by learning the mean rewards corresponding to a single arm elads to asymptotically highest reward to a particular arm. This leads to logarithmic regret in the fairness. 

\begin{definition}[Minimum Reward Fairness Guarantee]
Let $\mathcal{I} = \langle A, C, T \rangle$ be  a MA-MAB instance and let  $\Amax$ be the vector of maximum values from the corresponding row of $A$. We say  that a policy $\pi$ satisfies minimum reward fairness guarantee for    $\mathcal{I}$,  if 
\begin{equation}
A \pi \geq C   \Amax .
\end{equation}  
\end{definition}

Throughout the paper, we  will call a policy $\pi$ fair if it satisfies minimum fairness guarantees for any fair MA-MAB instance $\mathcal{I}$. 



We observe that   there may not always exist a fair policy. Consider, an example with 2 agents and 2 arms with $A_{1,1} = A_{2,2} = 1$ and $A_{1,2} = A_{2,2} = 0$ and $C_{1} = C_{2}  = c\in [0, 1] $. It is straightforward to see that  no policy $\pi$ satisfies the minimum reward fairness guarantee for   instances with $c>0.5$.  However, a fair policy always exists for $c \leq 0.5$. In particular, $\pi = [0.5, 0.5]$ is one such  fair policy. 

In our first result of the paper, we provide sufficient conditions that guarantee the existence of fair policy for a given instance $\mathcal{I}$

% \piyushi{Theorem 1 should come after Definition 1.}
\begin{restatable}{theorem}{characterization}
\label{thm:characterization}
A fair MA-MAB instance $\mathcal{I}$ admits a fair policy if at least one of the below conditions is satisfied 
\begin{enumerate}
    \item $\sum_{i \in [n]} C_i \leq 1$, 
    \item $ C_{\max} := \max_{i\in [n]} C_i \leq \frac{1}{\min(n,m)}$. 
\end{enumerate}
\end{restatable}

For the first condition,  observe that the policy  $   \pi_j = \frac{\sum_{i=1}^n \mathds{1}(j = j_i) C_i}{\sum_{i=1}^n C_i}$ where $j_i$ being the arm  with largest reward to agent $i$ is a fair policy \footnote{If $\sum_{i} C_i = 0$, then every policy $\pi \in \Delta_m$ is feasible. }. Under the second condition, uniform arm pull policy i.e., $ \pi_j = [1/m, \cdots, 1/m]$ is feasible.    A formal proof of Theorem~\ref{thm:characterization} is given in the Appendix. 

\himanshu{ Sir, should we talk about the geometric interpretation of the feasibility region of $\pi$ we discussed when we were thinking from policy perspective instead of social welfare perspective}

%The problem, becomes challenging when multiple agents obtain stochastically generated rewards and each agent is guaranteed a certain minimum expected reward as a fraction of maximum possible reward. 

%We call the arm that gives maximum reward to agent $i$ as the most preferred arm of $i$. 

    Under a learning setting where  the algorithm is not privy to $A$,  the algorithm must learn the policy ($\pi^t$) from the history of past pulls  and observed rewards, denoted by $\mathcal{H}^t$. More specifically,  for a given time instance $t$,  an arm pull strategy $\pi^t$ is a mapping,  $\pi^t: \mathcal{H}^t \rightarrow \Delta_{m}$.  The minimum-reward fairness regret of the policy $\pi:= (\pi^t)_{t\geq 1}$ is defined  the cumulative \emph{positive} difference between promised expected reward and the expected reward under the policy $\pi^t$.   

\iffalse 
The objective is to maximize the expected social welfare subject to fairness guarantee. \begin{align}
    \max_{\pi \in \Delta([k])}  & \sum_{ i\in [n]} \langle A_i, \pi \rangle \nonumber \\ 
    \text{subject to} & \nonumber \\ 
    A\pi &\geq C \cdot  \Amax \label{opt_problem}
\end{align}
Here, $C$ is a diagonal matrix with $C_{i,i} = C_i \leq \frac{1}{\min(n,k)}$. 
%\textcolor{red}{We can think of $C_{i,i} = C_i$ (each agent is guaranteed different fraction of max possible reward) in a more general setting but that's for later...}
\fi 


\begin{definition}[Minimum-reward Fairness Regret]
Given a MA-MAB instance $\mathcal{I}$ and a policy $\pi$, the minimum-reward fairness regret of $\pi$ on instance $\mathcal{I}$ over $T$ time instances is given as  
\begin{equation}
\mathcal{R}_{\textsc{fr}}^\pi(T) = \sum_{t=1}^T \sum_{i=1}^n | 
\underbrace{C_i \Amax_i}_{\text{I}} - \underbrace{\mathbb{E}_{\pi^t} [X_i^t]}_{\text{II}} |_+ \ ,
\end{equation}
where \( |\cdot|_+ \equiv \max\{\cdot, 0\} \).
\end{definition}


The term labelled \textbf{I} indicates the minimum rewards as a fraction of the maximum possible expected reward that agent \(i\) is guaranteed, while the term labelled \textbf{II} represents the expected reward that agent \(i\) receives under the policy \(\pi^t\) at time \(t\). The use of the expression \( |\cdot|_+ \) allows us to capture the scenario where, if the reward received by the agent exceeds the minimum required to satisfy the fairness constraints, the fairness regret incurred is zero. Therefore, the total fairness regret is accumulated across all agents up to time \(T\), reflecting the extent to which the agents' reward under policy $\pi^t$ deviates from their minimum guarantees. 






\subsection{Social Welfare Maximization with Minimum-reward Fairness Guarantee}
\label{sec:regret}

Let $\textsc{SW}_{\pi}(T) := \sum_{t=1}^T \sum_{ i\in [n]} \langle A_i, \pi^t \rangle$ represent the total expected social welfare achieved by the policy $\pi = (\pi^t)_{t \geq 1}$ over the time horizon,  $T$. %For a given strategy $\pi^t \in \Delta_m$, the expected reward for arm $i$, denoted as $r_i(t)$, is given by $r_i(t) = \sum_{j=1}^m \pi_j^t \mu_{i,j}$. %Also, note that $ \textsc{SW}_{\pi}(T) = \sum_{t = 1}^T \sum_{i=1}^n r_i(t)$.
\begin{figure}[H]
    \centering
% \begin{tcolorbox}[colback=white,colframe=black, width=\columnwidth]
\textbf{P1}
\begin{align}
\text{Maximize}_{ \pi = (\pi_1, \pi_2, \cdots, \pi_T) } \quad & \textsc{SW}_\pi(T)  \label{eq:POne} \\
\text{subject to} \quad & A \pi^t  \geq C \cdot  \Amax \ \ \  \forall t \in [T] \nonumber 
\end{align}
% \end{tcolorbox}
\Description{The original optimization problem.}
\end{figure}
It is easy to see that the optimal fair policy $\pi^* $ pulls each arm with the same probability in each round, i.e., $\pi^*_i = \pi^t_i$ for all $t$ since  matrices $A$ and $C$ are fixed.  

We further assume that the conditions in Theorem~\ref{thm:characterization} are satisfied, i.e., $\pi^*$ is well defined. The reward regret is defined as the cumulative loss in social welfare by not following the policy  $\pi^*$ at each time instant. 


\paragraph{Connection with Nash Social Welfare:} The Nash Social Welfare (NSW) objective is known to satisfy fairness guarantees in resource allocation scenarios (see \cite{Hossain2020FairAF, caragiannis2019unreasonable} and references therein). However, it falls short in accommodating user-defined fairness requirements. Additionally, the primary aim of NSW is not to maximize social welfare, which is the central objective of our work. Interestingly, our proposed formulation reveals an equivalence with the Nash product. Specifically, 

\begin{align*}
P1 \equiv \argmax_{\pi\in\Delta_m}\ \underbrace{\underbrace{\Pi_{i=1}^n\left( e^{\langle A_i, \pi\rangle} \right)}_{\textup{Nash product for rewards}} \underbrace{\Pi_{i=1}^n\left( \mathbbm{1}_{\langle A_i,\pi\rangle-C_i\Amax_i\geq 0} \right) }_{\textup{Nash product for fairness}}}_{\textup{Nash product for rewards and fairness}}.
\end{align*}

In this formulation, the Nash product for fairness reaches its maximum value of one only when the fairness guarantees are met for all agents. Provided that a feasible policy exists, the Nash product for rewards in our formulation can be interpreted as an NSW configuration, where the agents' rewards are exponentiated. 

Next,  we define the social welfare regret as the additional loss incurred by the algorithm as compared to the optimal fair policy in hindsight.


\begin{definition}[Social Welfare Regret]
Let  $\pi^*$ be an optimal policy (solution of problem P1) for a given MA-MAB instance $\mathcal{I}$.  Further, let  $\pi = (\pi^t)_{t\geq 1}$ be an arm pull strategy. The social welfare regret of $\pi$ on instance $\mathcal{I}$ over time horizon $T$ is defined as  
\begin{equation}
\mathcal{R}_\textsc{SW}^\pi (T) =  T \cdot  \textsc{SW}(\pi^*) - \sum_{t=1}^T \textsc{SW}(\pi^t)  
\end{equation}
\end{definition}

We drop the superscript in the notation of fairness regret and social welfare regret  whenever the arm-pull strategy  $\pi$ is clear from the context. Note that the expected cumulative SW regret could well  be negative, in which case the policy $\pi$ generates more social welfare than an optimal fair policy at the cost of fairness regret. %Next, we provide sufficient conditions for a feasible  solution to exist for optimization problem P1. 







