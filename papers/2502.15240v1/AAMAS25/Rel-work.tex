\subsection{Related Work}
\label{sec:relWork}
\gan{@Piyushi: can you give a try at this?}
The stochastic multi-armed bandits (MAB) problem has been extensively studied with the goal of designing algorithms that optimally trade-off exploration and exploitation for maximizing the expected cumulative reward \cite{Lattimore2020BanditA, slivkins2024introductionmultiarmedbandits, bubeck2012regretanalysisstochasticnonstochastic}. The multi-agent multi-armed bandits (MA-MAB) variant \cite{Liu2009DistributedLI} involves multiple agents simultaneously solving a given instance of the MAB problem. Such a setting often demands providing reward fairness guarantees to each agent, besides maximizing the sum of expected cumulative rewards obtained by the agents. Several works have emerged focusing on fairness for the MAB problem \cite{JMLR:v22:20-704,wang2021fairness,sinha2023textttbanditq,baudry2024, porat21, liu2017calibrated, patil2022mitigating,krishna25pmean}. However, these approaches do not generalize to provide reward guarantees for different agents involved in the MA-MAB setup. Moreover, these formulations either focus on guaranteeing a certain fraction of arm pulls \cite{JMLR:v22:20-704,sinha2023textttbanditq, porat21},  constrain the deviation of the policy to a specific closed-form optimal policy \cite{wang2021fairness,baudry2024} or focus on meritocratic criteria in online resource allocation setting \cite{patil2022mitigating, liu2017calibrated}. 
% Recently, \citep{Hossain2020FairAF} studied algorithms for Nash Social Welfare based fairness notion in the MA-MAB setting. 

%We consider a minimum reward guarantee as a fairness constraint to be satisfied simultaneously for each agent. The minimum reward guarantee is specified in terms of the fraction of the maximum expected   reward  for a particular agent. Given a vector of fairness requirement, the objective is to maximize the social welfare  subject to minimum rewards of at least a given value for   every agent. 

The closest work to ours is by     \citet{Hossain2020FairAF} who proposed learning a policy over the $m$-arms that maximizes the Nash Social Welfare (NSW) involving the $n$ agents. \citet{Jones_Nguyen_Nguyen_2023} proposed a more efficient algorithm for the NSW-based MA-MAB problem and recently \citet{Zhang2024NoRegretLF} tried improving the corresponding regret bounds. While NSW objective is known to satisfy desirable fairness and welfare properties (see \cite{NSW, caragiannis2019unreasonable} for details), the fairness guarantees in NSW are implicit and cannot be specified externally. This may not always be desired. Consider a case with 2 agents with $\mu_{1, 1}=\mu_{2,2}=1$ and $\mu_{1, 2}=\mu_{2, 1}=0$ where the agents demand at least one-third and two-thirds of their maximum possible reward. Maximizing the NSW results in the policy $(1/2, 1/2)$ which does not satisfy the specified reward requirement for the   second agent. In contrast, our proposed MA-MAB formulation finds a policy that respects the reward allocations demanded by each agent, whenever it is possible to do so.


