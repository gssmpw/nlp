%\section{Results}
%\label{sec:preliminaries}
\iffalse 
\begin{definition}[Proportionality]
$A \pi \geq  \frac{1}{\min(n,k)} A \mathds{1}$. 

Here $\mathds{1}$ is all-ones vector.
\end{definition}
\fi 


Throughout the paper, we will consider that the reward functions $X_{i,j}$'s are  sub-gaussian  random variables with finite and positive mean. 

\begin{definition}[Sub-gaussian Rewards]
 We call $X$  a sub-gaussian random variable if  there is a positive constant $\sigma$ such that for every $\lambda \in \mathbb{R}$,  we have 
\begin{equation}
    \mathbb{E}\left[\exp{\left(\lambda(X-\mathbb{E}[X])\right)}\right]\leq \exp(\lambda^2\sigma^2/2).
\end{equation}
\end{definition}

Sub-gaussian random variables encompass a diverse range of distributions, including Bernoulli random variables. More generally, any random variable bounded in $[a, b]$ is $\sigma$-sub-gaussian with $\sigma=\frac{(b-a)}{2}$. The sub-gaussian property ensures that the probability of extreme reward values is minimized, which contributes to better reward guarantees. This characteristic is particularly advantageous for designing and analyzing learning algorithms, allowing us to consider a more general class of reward distributions.


 



%We provide intitive explanation here. It is easy to see that when the $c_{\text{max}}$ value is small, the uniform arm pull strategy when $m \leq  n$ and the startegy that pulls each agents most preferred arm for a minimum specified probability when $m > n$ is a fair policy, establishing feasibility. For the first condition, 




\iffalse 
\begin{definition}[Equitability]
textcolor{red}{Let us di}
\end{definition}

\begin{definition}[Nash Social Welfare]
\end{definition}


\begin{proposition}
For any given matrix $A$, the optimal max fair policy with $c = \frac{1}{\min(n,k)}$  (solution to Problem \ref{opt_problem}),  can be found in polynomial time. 
\label{propOne}
\end{proposition}
We prove Proposition \ref{propOne} by giving a polynomial time  algorithm that finds a solution to Problem \ref{opt_problem}. 

\begin{algorithm}[H]
\SetAlgoLined

  \KwIn{ $A \in \mathbb{R}_{+}^{n \times k}$ }
\caption{\textsc{OptimalPolicy}}
\label{algOne}
{\bf Initialize:} $ \Amax  , B \leftarrow A/\Amax , C = \mathbf{0}^k   $\;
\For{\texttt{each column $\ell$ of $B$}}{ 
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
- $C_{\ell} = \sum_{i: b_{i\ell} =1} a_{i,\ell} $ 
}

Let $\ell^* = \arg \max_{\ell \in [k]} C_\ell$ be the largest entry in $C$. \;
Construct the policy $\pi$ as follows 
\begin{center}  
$\pi_\ell =
\begin{cases}
 0 &       C_\ell = 0 \\ 
 1/\min(n,k)  &   \ell \neq \ell^*  \text{ and }  C_\ell > 0  \\  
 1 - \frac{|\{\ell\neq \ell^* | C_\ell > 0 \}|}{\min(n,k)}  & \ell = \ell^*
\end{cases}$  \end{center}
 \textbf{Return {$\pi$}}

 \end{algorithm}
 
\begin{claim}
The policy $\pi$ returned by Algorithm \ref{algOne} is fair. 
\end{claim}
\begin{proof}[Proof outline]
We first show that $\pi$ returned by Algorithm \ref{algOne} is a valid probability distribution over $\Delta_k$. The claim is straightforward when $\min(k,n) = k$ i.e.,  $k\leq n$. Let us assume WLTG that $k>n$. In this case,  let there be $m$  nonzero entries in $C$. Observe that $1 \leq m \leq  n < k$. The policy returned by algorithm \ref{algOne} satisfies $\pi_{\ell^*} = 1 - \frac{m-1}{n} = \frac{n+1 - m}{n} \in [0,1]$. Furthermore, $ \sum_{\ell}\pi_\ell = (m-1) \frac{1}{n} + \frac{n+1 -m}{n} = 1 $. 

Let, for contradiction,  there exist and  agent $i \in [n]$ such that $(A\pi)_i  < \frac{ \Amax_i}{n}$. Furthermore, let $\ell_i \in [k]$ be such that $a_{i, \ell_i} = \max_{\ell \in [k]} a_{i, \ell}$.  \footnote{If there are more than one, consider any one arbitrarily.} This implies $b_{i,\ell_i} =1$ and hence $C_{\ell_i} >0$. If $\ell_i \neq \ell^*$ we have by construction $\pi_{\ell_i} = \frac{1}{\min(n,k)}$. This gives 
$$(A\pi)_{i} = \sum_{\ell \in [k]} a_{i,\ell} \pi_{\ell} \geq a_{i,\ell_i} \pi_{\ell_i} = \frac{\Amax_{i}}{\min(n,k)}.  $$ A contradiction. On the other hand, if $ \ell_i = \ell^*$, consider the following two cases. 
\\ \\ 
\noindent {\bf Case 1 ($k \leq n$):} In this case $ \pi_{\ell_i} =\pi_\ell^*  \geq 1 - \frac{k-1}{k} = 1/k $.  \\ \\ 
\noindent \textbf{Case 2 ($k > n$):} Let there are $m$  nonzero entries in $C$. Observe that $1 \leq m \leq  n < k$. Hence $\pi_{\ell_i} = \pi_{\ell^*} = 1 - \frac{m-1}{n} \geq   \frac{1}{n}  = \frac{1}{\min(n,k)}$. 

In both the above cases we have,  $(A\pi)_i \geq \frac{\Amax_{i}}{\min(n,k)} $. A contradiction.
\end{proof}
\begin{claim}
The algorithm terminates in $O(??)$ time.  
\end{claim}
\begin{proof}
\textcolor{red}{TODO}
\end{proof}
\begin{lemma}
For any given $c > \frac{1}{\min(n,k)}$, there exists a fair { \sc MAMAB}  instance $\mathcal{I}$ such that the optimization problem \ref{opt_problem} is infeasible. 
\end{lemma}
\begin{proof}
\textcolor{red}{TODO}
\end{proof}

\begin{proof}[Proof of Proposition \ref{propOne}]
\textcolor{red}{@Himanshu: Complete the proof by contradiction. Essentially the argument is that if $\pi$ is not $\pi$ then the social welfare will strictly decrease as the probability weight to arms that generate less reward has to increase. Check carefully the tie-breaking case.   }
\end{proof}
\fi 

%\section{The Relation between Welfare maximizing and Fairness regret minimizing strategies}
%\textcolor{red}{TODO: see if SW maximizing policy can guarantee some minimum fairness level; if not give worst case examples. Also vice, versa.}
\section{Warmup:  Two Arms Case}\label{sec:warmup}
%Our goal is to design a policy $(\pi^t)_{t\geq 1}$ such that it simultaneously achieves optimal welfare regret guarantee and zero asymptotic fairness regret guarantee. That is $\lim_{T \rightarrow \infty} \mathcal{R}_{fr}(T) =0 $. 

In this section, we consider a simple MA-MAB setup with 2 arms and $n$ agents. This setup allows us to write the optimal fair policy in tractable mathematical form. We also provide our first algorithm, {\sc Explore-First}. 


Consider the MA-MAB instance with $2$ arms and $n $ agents.
Index the agents such that the first $ n_1$ agents prefer arm 1 and the next $n - n_1$ agents prefer arm 2. Note that when $n_1 = n$ (when $n_1 = 0$), we have that all the agents prefer arm 1 (arm 2) and the optimal fair policy, in this case, is straightforward: pull arm $1$ (or arm 2 respectively) with probability 1. Hence, without loss of generality, let $0 <   n_1 < n$. That is,  $ \Amax_i  = A_{i,1} (\geq A_{i,2})$ for all $i \in [n_1]$, $ \Amax_i  = A_{i,2} (> A_{i,1})$ for all $i \notin [n_1]$.  Further assume without loss of generality that arm $1$ is an optimal arm i.e. $\sum_{i \in [n]} A_{i,1} \geq \sum_{i \in [n]} A_{i,2} $ and let $[x^*, 1-x^*]$ be the optimal arm pulling policy. 

To characterize the optimal arm pulling strategy in the two-arms case,  first observe the following property of the optimal fair policy. An optimal fair policy pulls a sub-optimal arm (arm 2) with a nonzero  probability i.e. $1-x^* > 0$ only  when the minimum reward  fairness guarantee  is violated for some agent.   
With this intuitive understanding,  we now characterize the optimal policy $[x^*, 1-x^*]$. 

Let $\Delta:=\sum_{i=1}^n (A_{i,1} - A_{i,2}) >0$. The regret of the policy $\pi$  can be written in terms of $\Delta$ as follows  
\begin{equation}
    \mathcal{R}_\textsc{SW}(T) = \sum_{t=1}^T [x^* - x^t] \Delta. 
    \label{eq:eqnClosedFormSW}
\end{equation}
Here, $x^t$ is the probability of pulling arm $1$ at time $t$. 


\begin{restatable}{lemma}{propOne} The optimal feasible policy of a  fair MA-MAB instance  $\mathcal{I}$ with two arms is given by 
\label{prop:One}
    %Let $A$ be the reward matrix and $c := (c_1, c_2, \cdots c_n) \geq  \boldsymbol{0}$ be the fairness vector such that $c_i \leq 1/2$ for all $i \in [n]$. Then the optimal policy is given by 
    \begin{equation}
    x^* = \min  \Bigg ( 1, \min_{i \in [n] \setminus [n_1]}\frac{ 1- C_i}{ 1- \frac{A_{i,1}}{A_{i,2}}} \Bigg ).
    \end{equation}
\end{restatable}


%It is clear that the policy $x = 1/2 $ is a feasible policy, however, it may not be an optimal policy. To see this, note that the social welfare of the policy $x=1/2$ is given by $\frac{1}{2} \sum_{i=1}^n a_{i,1} + \frac{1}{2} \sum_{i=1}^n a_{i,2} $ whereas the social welfare of the optimal policy is given by $x^* \sum_{i=1}^n a_{i,1} + (1-x^*)  \sum_{i=1}^n a_{i,2}$. The difference in social welfare is given as $(x^* -1/2) (\sum_{i=1}^n a_{i,1} - \sum_{i=1}^n a_{i,2} )$. This is a non-negative value since by assumption we have that arm 1 is optimal $i.e.$,  $\sum_{i=1}^n a_{i,1} > \sum_{i=1}^n a_{i,2} $ and  $x^* \geq  1/2$. 



The proof of Lemma~\ref{prop:One} is given in the Appendix. We are now ready to present our first algorithm that achieves a  sublinear regret guarantee. 

\begin{algorithm}
\caption{\textsc{Explore-First}}
\label{algOne-EF}
\begin{algorithmic}[1]
\STATE \textbf{Require:} $T, C$.
\FOR{$t = 1,2, \cdots, \lfloor T^{\alpha} \rfloor $}  
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline 
\STATE Pull arm $i = t\mod(2)+1$. % +1 AS WE HAVE INDEXED ARMS AS 1, 2.
\ENDFOR 
 \STATE Compute the estimated reward matrix $\widehat{A}$ of the rewards observed so far.
\STATE Compute $x'=\min \Bigg ( 1, \min_{ i: \widehat{A}_{i,2} > \widehat{A}_{i,1} }\frac{1 - C_i}{1 - \frac{\widehat{A}_{i,1}}{\widehat{A}_{i,2}} }\Bigg ).$
\FOR{$t = \lfloor T^{\alpha} \rfloor + 1, \lfloor T^{\alpha} \rfloor + 2, \cdots,  T $}  
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
\STATE  Pull arm $1$ with probability $x'$   and   arm $2$ 
with probability $1-x'$.     
\ENDFOR 
 \end{algorithmic}
\end{algorithm}
%\subsection{\textsc{ExploreFirst} algorithm for general case}
%\gan{I dont think we have a proof for this. Atleast we can show the empirical results for this. }

\iffalse 
\begin{algorithm}[H]
\SetAlgoLined
  \KwIn{ $T, c$ }
\caption{\textsc{Explore-First}}
\label{algOne-EF}
%{\bf Initialize:} $ \Amax ,    $\;
\For{$t = 1,2, \cdots \lfloor T^{\alpha} \rfloor $}{ 
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline 
- Pull arm $1$ with probability $1/2$  
}
%- Let $U = (u_{i,j})_{i \in [n], j\in [2]}$ and $L = (u_{i,j})_{i \in [n], j\in [2]}$ \\
Compute the estimated reward matrix $\widehat{A}$ of the rewards observed so far \\
Compute the probability$(x^t)=\min_{ i: \widehat{a}_{i,2} > \widehat{a}_{i,1} } \Bigg ( 1, \frac{1 - c_i}{1 - \frac{\widehat{a}_{i,1}}{\widehat{a}_{i,2}} }\Bigg )$\\
\For{$t = \lfloor T^{\alpha} \rfloor + 1, \lfloor T^{\alpha} \rfloor + 2, \cdots  T $}{ 
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
- Pull arm $1$ with probability $x^t$  
} 
 \end{algorithm}
 \fi 
 
 \iffalse 
\textcolor{red}{TODO: }
 \begin{itemize}
     \item Simulate with different values of $\alpha$ and plot the following
     \begin{itemize}
         \item SW regret for different values of $\alpha$, $T$, and $c$
         \item Fairness regret regret for different values of $\alpha$, $T$, and $c$
         \item The tradeoff and asymptotic optimality 
     \end{itemize}
 \end{itemize}
 \fi 
 
 \subsection{ Regret Analysis of Explore-First Algorithm }
 

\iffalse 
\begin{lemma}
    Under the conditions of Claim \ref{lem:Two} above, we have
    \begin{equation}
    \mathbb{P}( \{ \widehat{a}_{i,2} \geq (1+ \delta) a_{i,2}\} \cap \{ \widehat{a}_{i,1} <  (1+ \delta) a_{i,1}\}) \leq 2 \exp(-n\delta^2 a_{i,1}/2). \end{equation}    \label{lem:Three}
\end{lemma}

\begin{proof}
\textcolor{red}{@Himanshu: complete this proof. }
\end{proof}
\fi 


The \textsc{Explore-First} algorithm addresses the exploration-exploitation tradeoff effectively by delineating the exploration phase from the exploitation phase. During the exploration phase, the algorithm employs a round-robin strategy to pull each arm for $\lfloor T^{\alpha} \rfloor$ rounds. This approach ensures that each arm is sampled sufficiently, yielding more accurate estimates of each arm's reward. However, this phase does not prioritize the arm with the highest reward and does not guarantee immediate rewards for the agents.

In the subsequent exploitation phase, the algorithm utilizes these reward estimates to solve an optimization problem P1. For the specific case of two arms, a closed-form solution is given in Line 6. The optimal fair policy derived from this solution is then used to determine the arm pulls for the remaining rounds.

It is important to note that the regrets associated with both social welfare and fairness are influenced by the choice of the parameter $\alpha$. Specifically, the regret incurred during the exploration phase is proportional to $T^{\alpha}$ in both cases. Thus, a larger value of $\alpha$ results in higher regret due to the increased duration of the exploration phase. Conversely, if $\alpha$ is too small, the estimates of the arm rewards may not be sufficiently accurate, leading to suboptimal decisions in the exploitation phase and, consequently, higher regret. This tradeoff highlights the importance of carefully choosing $\alpha$ to obtain a  balance between accurate reward estimation and minimizing regret.




\noindent




%We now form the lower bound  so we need to find the lower bound on this ratio (using Chernoff's tail bounds above); replace the lower bound to get appropriate result. 


\iffalse 
\begin{restatable}{theorem}{ExploreFirstRegret}
\label{thm:ExploreFirstRegret}
The  \textsc{Explore-First} algorithm achieves expected social welfare regret of $\tilde{O} \big (\frac{n}{ \sqrt{a}} T^{2/3} \sqrt{\log(T)} \big )  $. 
\end{restatable}\fi 

\begin{restatable}{theorem}{ExploreFirstFairnessRegret} [Informal]
\label{thm:ExploreFirstRegret}
The  \textsc{Explore-First} algorithm achieves, 

\begin{enumerate}
    \item expected social welfare  regret of $O \Big(\frac{n}{a_{\min}} T^{2/3} \sqrt{\log(T)} \Big)  $,  and 
    \item expected fairness regret of $O \Big(\frac{n}{a_{\min}} T^{2/3} \sqrt{\log(T)} \Big)  $,\\
    where $a_{\min}=\min_{i,j}A_{i,j}>0$. 
\end{enumerate} 
\end{restatable}
Detailed proof of Theorem~\ref{thm:ExploreFirstRegret} is given in the Appendix. It is easy to see that the \textsc{Explore-First} algorithm is inadequate for both fairness and social welfare. Firstly, observe that the algorithm fails to collect information gathered during the exploit phase and, thus, ceases learning after the exploration phase. This impacts both social welfare and fairness regret guarantees, as inaccurate estimates can result in suboptimal policies. Next, we propose a UCB-based policy that provides a better tradeoff in terms of social welfare regret and fairness regret. 

\iffalse  
 This implies that for any agent $i$ we have 
\begin{align*}
\frac{1-c}{1- \frac{a_{i,1} -  \varepsilon }{a_{i,2}  +  \varepsilon} } \leq \frac{1- c} { 1 - \frac{\widehat{a}_{i,1}}{\widehat{a}_{i,2}} } \leq \frac{1-c}{ 1 - \frac{a_{i,1} + \varepsilon }{a_{i,2}  -  \varepsilon}}
\end{align*}
 
 \section{$\epsilon$-Greedy algorithms}
\begin{algorithm}[H]
\SetAlgoLined

  \KwIn{ $c$ }
\caption{\textsc{EpsilonGreedy}}
\label{algOne-EG}
%{\bf Initialize:} $ \Amax ,    $\;
\For{$t \leq t' $ where $t'$ is the first time such that  reward is non-zero for some arm }{ 
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
- Pull  arm  with  probability $1/2$\;
}
- Let $\widehat{a}_1$ and $\widehat{a}_2$ be empirical reward vectors of arm $1$ and 2 respectively \;

%- Let $U = (u_{i,j})_{i \in [n], j\in [2]}$ and $L = (u_{i,j})_{i \in [n], j\in [2]}$
\For{$t > t'$}{ 
- Let  $\widehat{\pi}$ be the solution of LP (equation \ref{opt_problem}) with $\widehat{A} = (\widehat{a}_1, \widehat{a}_2)$\; 
%- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
- Pull arm $1$ with probability $ \varepsilon^t \cdot  \widehat{\pi} + \frac{(1- \varepsilon^t)}{2} $\;
- Let $i_t$ be the arm pulled at time $t$\; 
- Update $\widehat{a}_{j,i} = \frac{S_{j,i} +   \mathds{1}(i=i_t) \cdot X_{j,i}  }{N_{i_t} }  $ and $N_{j,i} = N_{j,i} + \mathds{1}(i =i_t)$  for all $j \in [n]$ for all $ i \in [m]$. 
} 
 \end{algorithm}


\begin{itemize}
    \item First consider a simple case $\varepsilon^t = 1/t$.
    \item Also run this with $\frac{1}{\sqrt{t}}$ and $\frac{1}{\sqrt[3]{t}}$. 
\end{itemize}
\fi 
\section{ The Proposed Algorithm and Analysis}\label{sec:UCB}
\gan{Write the algorithm details}
At each time step $t$, our proposed algorithm \ouralgo\  (refer to Algorithm~\ref{algUCB}) keeps an Upper Confidence Bound (UCB) estimate and a Lower Confidence Bound (LCB) for every arm-agent pair  $(i,j)$. During the initial $t'$ rounds (Lines 2-7), the \ouralgo\  performs exploration, i.e. pulls the arms in a round-robin manner. In the following exploitation phase (i.e., $t \geq t'$), the algorithm keeps UCB and LCB estimates for each arm-agent combination. The UCB index is utilized to provide an optimistic estimate of social welfare, while both UCB and LCB indices are used to assess the fairness requirements to determine the arm-pulling strategy as given in problem P2 below.
\begin{figure}[ht!]
    \centering
% \begin{tcolorbox}[colback=white,colframe=black, width=\columnwidth]
\centering 
\textbf{P2}
\begin{align}\label{P2}
\text{Maximize}_{ \pi \in \Delta_{m} } \quad & \sum_{i=1}^n \langle \overline{A}_i, \pi\rangle \\
\text{subject to} \quad & \overline{A} \pi  \geq C \cdot \underline{\Amax} \nonumber
\end{align}
% \end{tcolorbox}
\Description{Our reformulated UCB-LCB-based optimization problem.}
\end{figure}

\piyushi{May be we should (re)-write what $\overline{A}, \underline{A}$ means.} While using the UCB index to estimate rewards is common in literature and used in virtually all UCB-based algorithms, the use of LCB to estimate fairness constraints is not common.  We employ the LCB estimate to ease the fairness constraints in P2, ensuring that the below two properties hold with high probability,
\begin{enumerate}
    \item   the social welfare guarantees remain intact,  and   
    \item  the fairness constraints are met.
\end{enumerate}  
Our proof crucially uses the above two properties of the solution obtained by solving the linear program P2. In particular,  we show the optimal solutions of P2 exhibit similar  social welfare  with a  small loss in fairness guarantee   in comparison with the solution of P1. 

We  begin our analysis with a standard result in probability theory. 
\iffalse
\begin{lemma}[Hoeffding's Inequality~\tocite ]
Let $X_1, X_2, \cdots, X_n$ be independent bounded
random variables with $X_i  \in  [a, b]$  for all $i$, where $ - \infty <  a  \leq  b  <  \infty $. Then
\begin{equation}
    \mathbb{P}\Big( \frac{1}{n} \sum_{i=1}^n ( X_i - \mathbb{E}(X_i) ) \geq \varepsilon \Big ) \leq   \exp^{- \frac{2 n \varepsilon^2}{(b-a)^2} } \ \ \ \text{ for all }  \varepsilon > 0.
    \end{equation}
    Similarly, 
    \begin{equation}
    \mathbb{P}\Big( \frac{1}{n} \sum_{i=1}^n ( X_i - \mathbb{E}(X_i) ) \leq   - \varepsilon \Big) \leq   \exp^{- \frac{2 n \varepsilon^2}{(b-a)^2} } \ \ \ \text{ for all }  \varepsilon > 0.
    \end{equation}
\end{lemma}
\fi
\begin{lemma}[Hoeffdingâ€™s inequality for sub-Gaussian random variables]\label{lem:hoeffding}
   Let \( Z_1, Z_2, \dots, Z_k \) be independent sub-Gaussian random variables, each with sub-Gaussian parameter \( \sigma \) and let $S_k = \frac{1}{k} \sum_{s=1}^k Z_s$.  Then  for all \( \varepsilon > 0 \), we have 
\[
P\left( \left| S_k - \mathbb{E}[S_k] \right| > \varepsilon \right) \leq 2 \exp \left( - \frac{ k \varepsilon^2}{2  \sigma^2} \right).
\]
Alternatively, for any \( \delta \in (0, 1] \), with probability at least \( 1 - \delta \),
\[
\left| S_k - \mathbb{E}[S_k] \right| \leq \sigma \sqrt{\frac{2 \log\left( \frac{2}{\delta} \right)}{k}}. 
\]
 
\end{lemma}

\noindent We now prove an important technical lemma. 


\begin{restatable}{lemma}{ImpLem} Let $\pi^*$ be an optimal feasible solution of P1 and for any $t \geq t'$,   $\pi^t$ be an optimal solution of  P2 with $\overline{A}:= \overline{A}^t$. Then with probability at-least $1-  1/\sqrt{T}$, we have  \[  SW_{\pi^t} (\overline{A}) \geq SW_{\pi^*}( A ).  \]
\label{lem:equivalence}
\end{restatable}

\begin{proof}
Let $\varepsilon_{i,j}^t = \sigma \sqrt{\frac{2 \log(4mn\sqrt{T})}{N_j^t}} $ (See Line 9 of \ouralgo\ algorithm) and define   
 $ \delta'_{i,j} := \exp\left(\frac{-N_j^t \varepsilon^2}{2\sigma^2}\right)  = \frac{1}{4mn\sqrt{T}}$. Further,  let $\delta = 1/\sqrt{T}$.  Note that $\delta'_{i,j}$ is the  probability that $\overline{A}_{i,j}^t = \widehat{A}_{i,j}^t + \varepsilon_{i,j}^t \leq A_{i,j}$ at some time $t \geq t'$. 
 
 By symmetry of tail bounds  around the mean value  given by Hoeffding's inequality (Lemma~\ref{lem:hoeffding}) we have that  $ \delta'_{i,j} $ is also the  probability that $A_{i,j} \geq \underline{A}_{i,j}^t $.    
 
Note that arms are pulled in a round-robin fashion in the exploration phase of the \ouralgo\ algorithm. This implies that   each arm is pulled for the same number of rounds, i.e., $N_j^{t'}$ is the same for all $j \in [m]$.   Hence, we have that $\delta' := \delta'_{i,j} = \frac{1}{4mn\sqrt{T}} $. %Also note that $\delta' = \delta/2$ %For a given $\delta$, let $ \delta' = \frac{1}{2mn}$. 
 
 We prove the stated claim by showing that  with probability at least $1- \frac{1}{\sqrt{T}}$,  every feasible policy $\pi$ of P1 is also  a feasible policy of P2. %That is $  A_i^T \pi \geq c \cdot \Amax_i$ for all $i\in [n]$. 
 
 Fix $i \in [n]$. Let $k  \in  \arg\max_{j \in [m]} A_{i,j}$ and $ k^t \in   \arg\max_{j \in [m]} \underline{A}_{i,j}^t $ be the least indexed arms with maximum value in the $i^{\textup{th}}$ row of matrices $A$ and $\underline{A}$ respectively. 

 We have   
\begin{align}\label{eqn:sw}
\langle \overline{A}_i,  \pi \rangle &  \underbrace{\geq}_{w.p. \geq 1-m \delta'} \langle  A_i, \pi  \rangle      \geq  C_i \cdot A_{i,k} \geq C_i \cdot  A_{i, k^t} \nonumber  \\  & \underbrace{\geq}_{w.p. \geq 1-m\delta'} C_i \cdot (\widehat{A}_{i,k^t} - \varepsilon_{k^t}^t) = C_i \cdot \underline{A}_{i,k^t}. 
\end{align}
The first inequality (from left) in Equation~\ref{eqn:sw} above  follows from Hoeffding's inequality (Lemma~\ref{lem:hoeffding}), the second inequality follows from the feasibility of $\pi$ for P1, the third inequality follows from the definition of $k^t$ and the last inequality again follows from Hoeffding's inequality (Lemma~\ref{lem:hoeffding}). The first and last inequalities each hold with probability at least $(1-m\delta')$. Hence we have with probability at-least $1-2m\delta'$  that $\langle \overline{A}_i,  \pi \rangle \geq  C_i \cdot \underline{A}_{i,k^t}$.

Using union bound, we have  with probability at-least $1 - 2nm\delta'$ that $\langle \overline{A}_i,  \pi \rangle \geq  C_i \cdot \underline{A}_{i,k^t} $ for all $i\in [n]$; i.e.   every  feasible solution $\pi$ of P1 is also a feasible solution of P2. In particular, $\pi^*$ is a feasible solution of P2 with probability at least $1 - \delta/2$. This, along with the definition of $\pi^t$  gives 
\begin{align*}
SW_{\pi^t} (\overline{A}) \underbrace{\geq}_{w.p. \geq 1- \delta/2 }  SW_{\pi^*} (\overline{A}) \underbrace{\geq}_{w.p. \geq 1- \delta/2 } SW_{\pi^*} (A)
\end{align*}

This proved the stated claim.   
\end{proof}
 Lemma~\ref{lem:equivalence} implies that by easing the fairness constraints, it is possible to increase social welfare.  %Our next lemma shows that the after initial exploration phase, the optimal randomized  policy $\pi^t$  
\iffalse 
\begin{proof}
 We prove the stated claim by showing that with probability at least $1-\delta$, every feasible policy $\pi$ of P1 is a feasible policy of P2. That is $  A_i^T \pi \geq c \cdot \Amax_i$ for all $i\in [n]$. 
 
 Fix $i \in [n]$. Let $k := \arg\max_{j \in [m]} A_{i,j}$ and $ k^t :=  \arg\max_{j \in [m]} \underline{A}_{i,j}^t $. 
 
 Let $\delta_0 = \delta/m$. Then using Hoeffding's Inequality (Eq~\toref) with  $\varepsilon_{i,j}^t = \sqrt{\frac{\log(m n /\delta_0)}{2N_{j,t}}}$, we have  with probability at-least $1 - \frac{\delta_0}{mn}$ that 
\begin{align*}
\widehat{A}_{i,j} - A_{i,j} & \geq - \varepsilon_{i,j}^t 
 \end{align*}
 Using union bound, we have with probability at least $1 - \delta/n$ 
\begin{equation}
 \sum_{j =1}^m ( \widehat{A}_{i,j} - A_{i,j}    +  \varepsilon_{i,j}^t ) \pi_j  \geq 0   
 \implies \overline{A}_i^T \pi  \geq A_i^T \pi    
\end{equation}
We have   
\[  \langle \overline{A}_i,  \pi \rangle  \geq \langle  A_i, \pi  \rangle     \geq  c . A_{i,k} \geq c.  A_{i, k^t}     \geq c.(\widehat{A}_{i,k^t} - \varepsilon_{k^t}^t) = c. \underline{A}_{i,k^t}  \] 
The second inequality follows from the feasibility of $\pi$ for P1, the third inequality follows from the definition of $k_i$, and the last inequality follows from Hoeffding's inequality. The first and last inequalities each hold with probability at least $(1-\delta)$. Applying union bound, we conclude the proof of the lemma.   
\end{proof}
 \fi 
 \gan{get this proof from appendix to main...}



\begin{algorithm}[tb]
\caption{\ouralgo}
\label{algUCB}
\begin{algorithmic}[1] %[1] enables line numbers
\STATE \textbf{Require:} $T, n, m , C, N_{j}^t=0 \ \forall j \in [m].$
\STATE $t'=m\lceil \sqrt{T} \rceil, \ t=1$, $\widehat{A}=\mathbf{0}_{m\times n}$.
\FOR{$t\leq t'$}
\STATE Pull arm $j'=t \textup{ mod } m + 1$.
\STATE $\forall i \in [n]$, observe reward $X_{i, j'}^t\sim \mathcal{D}(\mu_{i, j'})$.
\STATE $\forall i \in[n], \forall j \in[m],$ $\widehat{A}_{i,j} = 
    \begin{cases}
     \widehat{A}_{i,j} & \text{ if }  j\neq j'  \\
      \frac{(N_j^{t-1}) \widehat{A}_{i,j} + X^t_{i,j}  }{N_j^t}  & \text{ if } j =j'. \\ 
      \end{cases}$
\STATE $N_{j'}^ t = N_{j'}^ {t-1}+1.$
\ENDFOR
\STATE Compute the confidence matrix $\mathcal{E}$ with entries $$\epsilon_{i,j}^t=\sigma \sqrt{\frac{2\log{(8mnT)}}{N_j^ t}} \ \forall i\in [n], j\in [m].$$ 
\FOR{$t\leq T$}
\STATE Compute $\overline{A} = \widehat{A} + \mathcal{E}$ and $\underline{A} = \widehat{A} - \mathcal{E}$.
\STATE $\forall i \in [n]$, compute $\underline{\Amax}_i = \max_{j\in [m]}\underline{A}_{i,j}$ (break ties arbitrarily).
\STATE Solve \textbf{P2} and 
     let $\pi'$ be the solution of this LP. 
\STATE Sample $j' \sim \pi'$.
\STATE $\forall i \in [n]$, observe reward $X_{i, j'}^t\sim \mathcal{D}(\mu_{i,j'})$.
\STATE $N_{j'}^ t = N_{j'}^ {t-1}+1.$
\STATE  $\forall i \in[n], \forall j \in[m]$, $\widehat{A}_{i,j} = 
    \begin{cases}
     \widehat{A}_{i,j} & \text{ if }  j\neq j'  \\
      \frac{(N_j^{t-1}) \widehat{A}_{i,j} + X_{i,j}^t  }{N_j^t}  & \text{ if } j =j'. \\ 
      \end{cases}$
\STATE Update entries of $\mathcal{E}$.
\ENDFOR
\end{algorithmic}
\end{algorithm}
\himanshu{ Algorithm 2, line 6, its calculating the estimated reward upto time $t' $ but in Algorithm 1, we wrote one line ie line 5. Should we change any one for consistancy}

% \begin{algorithm}
% \caption{\ouralgo} 
% \label{algUCB}
% \begin{algorithmic}
% \Require $T, n \geq 1, m \geq 1, C, \mathcal{E} \in \mathbb{R_+}^{n\times m}, \forall j \in [m], N_{j,0}=0$ 
% \For{$t \leq \lceil  t' \rceil $  } \State  Pull  arm $j' =  t \mod m$ 
% \State  For $j=j'$ and for all $i\in [n]$, observe reward $r_{i,j} \sim \mathcal{D}(A_{i,j}) $
% \State  $\forall j \in [m]$, \ \ $N_{j,t} =  N_{j,t-1} +  \mathds{1}(j = j')$ 
% \EndFor

% \State  -  $ \forall i \in [n], \forall j \in [m] \ \ \ \widehat{a}_{i,j} = \frac{   r_{i,j}}{N_{j,t}}$  
% %- $\varepsilon_j$\;
% %- Let $U = (u_{i,j})_{i \in [n], j\in [2]}$ and $L = (u_{i,j})_{i \in [n], j\in [2]}$
% \State - Let $\widehat{A}$  be an $n\times m$ empirical rewards matrix with entries $\widehat{a}_{i,j}$ and $\mathcal{E}$ be $n\times m$ confidence matrix with entries $\varepsilon_{j}$ for all $i\in [n], j \in [m]$

% \For{$t > t' $}  
% \State \begin{enumerate}
% \item 
%   Calculate $\overline{A}  = \widehat{A} + \mathcal{E}$ and $\underline{A} = \widehat{A} - \mathcal{E} $
% \item $\forall i\in [n]$ compute $\underline{a}_{i}^* = \max_{j\in [m]} \underline{A}_{i,j}$ (break ties arbitrarily)
%     \item Solve \textbf{P2} and 
%      let $\pi'$ be the solution of above LP. \item  Sample $j' \sim \pi'$
%      \item $\forall i\in [n]$, observe reward $ r_{i,j'} \sim \mathcal{D}(A_{i,j'})$ 

%     \item $\forall j \in [m]$, \ \ $N_{j,t} =  N_{j,t-1} +  \mathds{1}(j = j')$ 
%     \item 
%     $
%     \forall i \in[n], \forall j \in[m], \newline \widehat{a}_{i,j} = 
%     \begin{cases}
%      \widehat{a}_{i,j} & \text{ if }  j\neq j'  \\
%       \frac{(N_{j,t}-1) \widehat{a}_{i,j} + r_{i,j}  }{N_{j,t}}  & \text{ if } j =j' \\ 
%       \end{cases}  $  % $ \forall i \in [n], \forall j \in [m] \ \ \  = \frac{\sum_{\ell =1}^t  \mathds{1}(x_{\ell} = j) r_{i,j,t}}{N_{j,t}}$ 
%     \item $\forall i \in [n]$ , $ \ \varepsilon_{i,j'}^t = \frac{\alpha}{\sqrt{N_{j',t}}}$\;
% \end{enumerate}
% \EndFor 
  
% \end{algorithmic}
% \end{algorithm}
% \iffalse 
% \begin{algorithm}[H]


% \KwIn{ $T, n \geq 1, m \geq 1, c \in [0,\frac{1}{\min(n,m)}), \mathcal{E} \in \mathbb{R_+}^{n\times m}, \forall j \in [m], N_{j,0}=0$ }
% \caption{\ouralgo}
% \label{algUCB}
% %{\bf Initialize:} $ \Amax ,    $\;
% \For{$t \leq \lceil  t' \rceil $  }{ 
% - Pull  arm $j' =  t \mod m$\;\newline
% - For $j=j'$ and for all $i\in [n]$, observe reward $r_{i,j} \sim \mathcal{D}(A_{i,j}) $\;\newline
% - $\forall j \in [m]$, \ \ $N_{j,t} =  N_{j,t-1} +  \mathds{1}(j = j')$ 
% }

% % - $\forall j \in [m] \ \ N_j =1$ \;
% - $ \forall i \in [n], \forall j \in [m] \ \ \ \widehat{a}_{i,j} = \frac{   r_{i,j}}{N_{j,t}}$ \; 
% %- $\varepsilon_j$\;
% %- Let $U = (u_{i,j})_{i \in [n], j\in [2]}$ and $L = (u_{i,j})_{i \in [n], j\in [2]}$
% \newline
% - Let $\widehat{A}$  be an $n\times m$ empirical rewards matrix with entries $\widehat{a}_{i,j}$ and $\mathcal{E}$ be $n\times m$ confidence matrix with entries $\varepsilon_{j}$ for all $i\in [n], j \in [m]$\;

% \For{$t > t' $}{ 
% \begin{enumerate}
% \item 
%   Calculate $\overline{A}  = \widehat{A} + \mathcal{E}$ and $\underline{A} = \widehat{A} - \mathcal{E} $
% \item $\forall i\in [n]$ compute $\underline{a}_{i}^* = \max_{j\in [m]} \underline{A}_{i,j}$ (break ties arbitrarily)
%     \item Solve \begin{align*} \arg&\max_{\pi \in \Delta_m} \sum_{i=1}^n\overline{A}_i^T\pi  \\ \text{subject to} \ \ \ &\overline{A}_{i}^T\pi \geq c \underline{a}_{i}^* \end{align*} 
%      Let $\pi'$ be the solution of above LP. \item  Sample $j' \sim \pi'$
%      \item $\forall i\in [n]$, observe reward $ r_{i,j'} \sim \mathcal{D}(A_{i,j'})$ 

%     \item $\forall j \in [m]$, \ \ $N_{j,t} =  N_{j,t-1} +  \mathds{1}(j = j')$ 
%     \item 
%     $
%     \forall i \in[n], \forall j \in[m], \ \widehat{a}_{i,j} = 
%     \begin{cases}
%      \widehat{a}_{i,j} & \text{ if }  j\neq j'  \\
%       \frac{(N_{j,t}-1) \widehat{a}_{i,j} + r_{i,j}  }{N_{j,t}}  & \text{ if } j =j' \\ 
%       \end{cases}  $  % $ \forall i \in [n], \forall j \in [m] \ \ \  = \frac{\sum_{\ell =1}^t  \mathds{1}(x_{\ell} = j) r_{i,j,t}}{N_{j,t}}$ 
%     \item $\forall i \in [n]$ , $ \ \varepsilon_{i,j'}^t = \frac{\alpha}{\sqrt{N_{j',t}}}$\;
% \end{enumerate}

% %- Let  $\widehat{\pi}$ be the solution of LP (equation \ref{opt_problem}) with $\widehat{A} = (\widehat{a}_1, \widehat{a}_2)$\; 
% %- $M_{\ell} = |\{ i: b_{i\ell} =1 \}| $ %Count the number of $1$'s in column $\ell$ \newline \\  
% %- Pull arm $1$ with probability $ \varepsilon^t \cdot  \widehat{\pi} + \frac{(1- \varepsilon^t)}{2} $\;
% %- Let $i_t$ be the arm pulled at time $t$\; 
% %- Update $\widehat{a}_{j,i} = \frac{S_{j,i} +   \mathds{1}(i=i_t) \cdot X_{j,i}  }{N_{i_t} }  $ and $N_{j,i} = N_{j,i} + \mathds{1}(i =i_t)$  for all $j \in [n]$ for all $ i \in [m]$. 
% } 
%  \end{algorithm}
%  \fi 
%\subsection{Regret Analysis of \ouralgo}
%Let $ \widehat{A}^t \in \math$ 
\begin{restatable}{theorem}{rewardRegretUCB}
\label{thm:rewardRegretUCB}
For any feasible MA-MAB instance $\mathcal{I}$ with   $T\geq 32n^2\sigma^2$,     expected social welfare regret of \ouralgo \ is upper-bounded by $$ 4n \sqrt{2T}\left(\sigma\log{(2m^2T)} +m + \sigma \right).  $$
    
   % For any $\delta \in (0,1)$, the reward regret of \ouralgo is bounded by $O(n\sqrt{mT\log(mn/\delta)})$ with probability atleast $1-\delta$. 
\end{restatable}

\gan{give  outline of the proof with technical details }
The detailed proof of~Theorem \ref{thm:rewardRegretUCB} is provided in the Appendix. We provide a high-level overview of the proof here.  First, we break down the regret into two components: R1 and R2, representing the regret from the exploration phase and exploitation phase, respectively. The component R1 encompasses the regret over the initial exploration phase of  $m\lceil \sqrt{T} \rceil$ rounds. Following this, using      Lemma~\ref{lem:equivalence} we  argue that the social welfare obtained by solving P2 is at-least that of the original problem with high probability. By applying Hoeffding's inequality and the union bound to the aggregated expected values, we show that R2 is capped by $\tilde{O}(T^{1/2})$. 
 We emphasize here that the social welfare regret  of \ouralgo\ is asymptotically optimal (refer to Section~\ref{subsec:lowerBound} for the lower bound). 

We now give the fairness regret guarantee of \ouralgo\ algorithm. 


\begin{restatable}{theorem}{fairnessRegretUCB}
\label{thm:fairnesRegretUCB}


For any feasible MA-MAB instance $\mathcal{I}$ with   $T\geq 32n^2\sigma^2$,     expected fairness regret of \ouralgo \ is upper-bounded by
$$ 6n(\textstyle \max_{i\in [n]} C_i)\sigma T^{3/4}\log{(2m^2T)} + mnO(\sqrt{T}). $$
    % The fairness regret of  \ouralgo\ is upper bounded by $6n\sigma T^{3/4}\log(2m^2T)+n\sqrt{T}(m+4\sqrt{2}\sigma)$. 
\end{restatable}
%\fairnessRegretUCB*

It is worth noting that while the social welfare regret guarantee of \ouralgo\ is much stronger, this comes at the cost of higher  fairness  regret. We demonstrate this trade-off in  Section~\ref{sec: simulation} with simulations. 
\iffalse 
\begin{proof}
We are now ready to prove the statement of the theorem. From the definition of fairness, we have, 
\begin{align*}
 &\mathcal{R}_{fr}(T)  =  \sum_{t=1}^T \sum_{i=1}^n \big | c A_{i, k_i}  -   \langle A_i, \pi^t \rangle  \big|_{+} \\ 
& = \sum_{t=1}^{\lceil \sqrt{T m} \rceil } \sum_{i=1}^n \big | c A_{i, k_i}  -   \langle A_i, \pi^t \rangle  \big|_{+}  \\ &  \ \ \ +  \sum_{t= \lceil \sqrt{T m} \rceil + 1}^T \sum_{i=1}^n \big | c A_{i, k_i}  -   \langle A_i, \pi^t \rangle  \big|_{+} \\
& \leq \sum_{t=1}^{\lceil \sqrt{T m} \rceil } \sum_{i=1}^n \big | c A_{i, k_i}  -   \langle A_i, \pi^t \rangle \big|_{+} \\   \ \ &  \ \ \ + \sum_{t= \lceil \sqrt{T m} \rceil +1 }^T \sum_{i=1}^n \big | c A_{i, k_i}  -    \langle \overline{A}_i^t   -   2\varepsilon^{t},  \pi^t \rangle |_+ \tag{wp $ \geq 1-\delta$}\\ 
\end{align*}
\begin{align*}
&\leq cn\sqrt{mT} + \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \sum_{i=1}^n \big | c A_{i, k_i}  -   \langle \overline{A}_i^t, \pi^t \rangle |_+   \\ \ \ \ &   \ \ \ +   2 \varepsilon \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \mathbb{E}_{j \sim \pi^t} \big [ \sqrt{1/N_{j,t}} \big ] \\ 
&\leq  cn\sqrt{mT} + \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \sum_{i=1}^n c \big  |  A_{i, k_i}  -   \underline{A}_{i, k_i^t}^t|_+   \\  &  \  \ \ +   2 \varepsilon \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \mathbb{E}_{j \sim \pi^t} \Big [\sqrt{1/N_{j,t}} \Big ] \tag{as $\pi^t$ is feasible, $ \langle \overline{A}_i^t, \pi^t \rangle \geq c \underline{A}_{i, k_i^t}^t $}  \\ 
\end{align*}
\begin{align*}
& \leq cn\sqrt{mT} + c  \sum_{t= \lceil \sqrt{T m} \rceil +1 }^T \sum_{i=1}^n |A_{i, k_i} - \underline{A}_{i, k_i}^t|_{+}  \\ & \ \ \ +  2 \varepsilon \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \mathbb{E}_{j \sim \pi^t} \Big [\sqrt{1/N_{j,t}} \Big ] %2n \sqrt{2 \log(2mn/\delta)}  ( \sqrt{T m} + \sqrt{2T\log(1/\delta)}) 
\tag{follows from the fact that $k_i^t \in \arg\max_{j \in [m]} \underline{A}_{i,j}^t$} \\ 
& \leq cn \sqrt{mT} + c  \sum_{t=\lceil \sqrt{T m} \rceil + 1}^T \sum_{i=1}^n \varepsilon_{k_i}^t \\ \ \ \ & +  2 \varepsilon \sum_{t=\lceil \sqrt{T m} \rceil +1}^T \mathbb{E}_{j \sim \pi^t} \Big [\sqrt{1/N_{j,t}} \Big ] \tag{with probability atleast $1- \delta/m$}\\
\end{align*}
\begin{align*}
%\intertext{\textcolor{red}{TODO}}
& \leq   cn \sqrt{mT }  + 2 c n \varepsilon  \sum_{t=  \lceil \sqrt{T m} \rceil + 1 }^T \frac{1}{\sqrt{N_{k_i, t}}}  \\ & \ \ \ +  2n \sqrt{2 \log(2mn/\delta)}  ( \sqrt{T m} + \sqrt{2T\log(1/\delta)})\\
& \leq   cn \sqrt{mT }  + 2 c n \varepsilon  \sum_{t=  1}^T \frac{1}{\sqrt{ \sqrt{T m} }}  \\ & \ \ \ +  2n \sqrt{2 \log(2mn/\delta)}  ( \sqrt{T m} + \sqrt{2T\log(1/\delta)})\\
& =   cn \sqrt{mT }  + 2 c n \varepsilon  \frac{T^{3/4}}{\sqrt{ m }}  \\ & \ \ \  +  2n \sqrt{2 \log(2mn/\delta)}  ( \sqrt{T m} + \sqrt{2T\log(1/\delta)})
\end{align*} 
\end{proof}
\fi 
\gan{Need two paragraphs with proof intuition in the main. Detailed proof may go in the appendix. Most of this part is handwavy ...need to make it more concrete}
Next, we show the lower bound on fairness and social welfare regret guarantees of  MA-MAB problems, 

\section{Regret Lower Bounds}
\label{subsec:lowerBound}
%In this section, we show that the MA-MAB instance admits a lower bound of $\Omega(\sqrt{T})$ on both social welfare and fairness regret guarantees. 
\iffalse 
\begin{definition}[Instance-Independent Simultaneous Regret Bound]
    We say that an MA-MAB instance admits an instance-independent regret bound of $(T^\alpha, T^\beta)$ if for any feasible instance $\mathcal{I} 
    % \in \mathbb{R}^{n \times m }
    $
    , we have that 
    \begin{enumerate}
        \item $\mathcal{R}_{SW}(T)  = \Omega(T^\alpha)$, and 
        \item $\mathcal{R}_{FR}(T)  = \Omega(T^\beta)$.
    \end{enumerate}
\end{definition}
\fi 
In this section, we  prove that every algorithm must suffer an \emph{instance-independent regret} \footnote{ A regret guarantee is called instance independent if it  holds for every feasible MA-MAB instance  $\mathcal{I}$.  That is,  for all values of fairness constraints matrix $C$, time horizon $T$, and mean rewards matrix $A$ provided that  $\mathcal{I}$ admits a feasible policy.}  of $\Omega(\sqrt{T})$  in both fairness and social welfare.     
\begin{restatable}{theorem}{lowerRegret}
\label{thm:lowerBound}
An instance-independent social welfare and fairness regrets of MA-MAB problem is lower bounded by $(\Omega(\sqrt{T}), \Omega(\sqrt{T}))$. 
\end{restatable}
\begin{proof}[Proof Outline]
The proof of Theorem \ref{thm:lowerBound} is given in the Appendix. 
 We provide an intuition here. To show the lower bound on social welfare, consider a class of instances where each row is a non-negative  multiple of the first row i.e. $A_i = \beta_i A_1$ for some $\beta_i \geq 0$ and $C$ as a zero matrix. As every agent has the same preferences over arms, the problem of maximizing social welfare now is reduced to the problem of identifying an arm $j$ with the highest $\sum_{i \in [n]} A_{i,j}$. This is equivalent to finding an arm  $j$ with largest $\Big( 1 +  \sum_{i\neq 1} \beta_i \Big) \cdot A_{1,j}$. This problem is the same as the classical stochastic MAB problem with $m$ arms and reward distributions with the mean reward of arm $j$ as $\Big( 1 +  \sum_{i\neq 1} \beta_i \Big) \cdot A_{1,j}$.   We use the $\Omega(\sqrt{T})$ instance-independent regret  lower bound \cite[Theorem 5.1]{Auer02}  for  classical stochastic  bandits to lower bound the social welfare regret.    

To lower-bound the fairness regret, we construct a MA-MAB instance with $m=2$  arms and $n=2$ agents with $A$ being the Identity matrix. For any values of $C_1 > 0 $  and $ C_2 =1- C_1$ satisfying the conditions in Theorem~\ref{thm:characterization}, the fairness criteria is satisfied if and only if $ x^* = C_1$. Since $ A_{1,1} - A_{1,2} = - (A_{2,1} - A_{2,2}) = 1$ we have that the fairness regret can be written (using Eq. \ref{eq:eqnClosedFormSW}) as 
$$\mathcal{R}_{\textsc{FR}}(T) = \sum_{t=1}^T |C_1 - x^t|_+ \geq | \sum_{t=1}^T  C_1 -  \sum_{t=1}^T x^t|_+ \geq \sum_{t=1}^T  C_1 -  \sum_{t=1}^T x^t. $$  Now consider a stochastic MAB setup with two arms and rewards $1 $ \piyushi{I think this should be 1} and $0$ respectively. We again use a lower bound of $\Omega(\sqrt{T})$ for the stochastic MAB setting with this instance to obtain a lower bound on the fairness regret for the MA-MAB setting.     
\end{proof}

It is worth noting that the lower bounds presented in Theorem~\ref{thm:lowerBound} invoke different stochastic MAB instances and, therefore, may not hold simultaneously. The \ouralgo\ algorithm proposed in this paper is asymptotically optimal up to a logarithmic factor; however, the same is not true for the fairness regret.   

In the next section, we present a heuristic algorithm that provides better empirical performance for fairness. However, the empirical performance for the social welfare of this heuristic algorithm is worse than that of \ouralgo.   

\iffalse 
\subsubsection{SW Regret with $T^\alpha$}
\begin{proof}
\begin{align}
    \mathcal{R}_{sw}(T) &= \sum_{t=1}^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] \nonumber \\ 
&\leq  \underbrace{\sum_{t=1}^{ \lceil T^\alpha \rceil} \sum_{i=1}^n \langle A_i, \pi^* - \pi^t \rangle}_{\textup{R1}}  + \underbrace{ \sum_{t= \lceil T^\alpha \rceil + 1 }^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] }_{\textup{R2}} 
\end{align}
We bound terms R1 and R2 separately. We begin with an upper bound for R1. 
\begin{align*}
 \textup{R1} &=  \sum_{t=1}^{ \lceil T^\alpha \rceil} \sum_{i=1}^n \langle A_i, \pi^* - \pi^t \rangle \\ 
%  &=  \sum_{j =1}^m\ \sum_{t \leq \lceil T^\alpha \rceil  }  \ \sum_{i=1}^n   A_{i,j} ( \pi_j^* - \mathds{1}[j_t =j] )  \\
&\leq \sum_{t \leq \lceil T^\alpha \rceil } \sum_{j =1}^m  \sum_{i=1}^n   A_{i,j}  \pi_j^*    \\ 
&\leq  \frac{T^\alpha + 1}{m}   \sum_{j =1}^m \Big (\sum_{i=1}^n   A_{i,j} \Big )  \pi_j^* \\ 
& = \frac{T^\alpha + 1}{m} \Big\langle \sum_{i=1}^n   A_i, \pi^*  \Big\rangle   \\ 
&\leq  \frac{T^\alpha + 1}{m}  \left\| \sum_{i=1}^n   A_{i} \right\|_2  \left\|\pi^*\right\|_2    \\ 
&\leq \frac{T^\alpha + 1}{m} n\sqrt{m}   = n T^\alpha /\sqrt{m} \\
\end{align*}

Next, we give an upper bound on R2. 
\begin{align*}
   \textup{R2} &= \sum_{t= \lceil T^\alpha \rceil + 1 }^T \big[ SW_{\pi^*}(A)  - SW_{\pi^t}(A)  \big] \\ 
   & \underset{ w.p.\ 1-\delta}{\leq} \sum_{t= \lceil T^\alpha \rceil + 1 }^T \big[ SW_{\pi^t}(\overline{A}^t)  - SW_{\pi^t}(A)  \big] \ \textup{(From Lemma \ref{lem:equivalence})}\\  
  & \leq  \sum_{t= \lceil T^\alpha \rceil +1}^T \sum_{i=1}^n \Big\langle \overline{A}_i^t - A_i,  \pi^t \Big\rangle \\ 
    & \leq 2n \sum_{t= m +1}^T \mathbb{E}_{j \sim \pi^t} [(\varepsilon_{t})_j] \ \textup{(Using } \lceil T^\alpha \rceil >m) \\
    &=   2 n \varepsilon \sum_{t= m +1}^T \mathbb{E}_{j \sim \pi^t}[\sqrt{1/N_{j,t}}] \\ 
& \underset{w.p.  1-\delta}{\leq}   2n \varepsilon \left ( \sum_{t= m +1}^T \sqrt{1/N_{j_t,t}} + \sqrt{2T\log(1/\delta)} \right) \\
& \leq  2n \sqrt{2 \log(2mn/\delta)}  \left( \sqrt{T m} + \sqrt{2T\log(1/\delta)}\right)  
\end{align*}
\end{proof}
\fi 