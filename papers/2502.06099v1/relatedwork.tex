\section{Related Works}
This section provides an overview of intrusion detection methods for CAVs and Federated Learning, highlighting resource limitations in cross-device FL and the role of Transfer Learning in addressing these challenges.

\subsection{CAV-based IDS and Federated Learning}
Intrusion Detection Systems are critical for addressing the growing security challenges in both external and in-vehicle networks of Connected and Autonomous vehicles. Recent studies have demonstrated the effectiveness of integrating deep learning methods into IDS to detect and mitigate cyber threats \cite{s21144736}. These approaches employ diverse network architectures with varying capabilities \cite{lansky2021deep, aleesa2020review, guha2019one}, offering insights into potential solutions but often neglecting privacy and scalability concerns in distributed environments such as CAVs.

CAV systems face a variety of threats, including network-based attacks (e.g., denial-of-service), physical tampering (e.g., sensor spoofing), software vulnerabilities (e.g., malware injection), data integrity issues (e.g., data tampering or exfiltration), and privacy breaches. These threats create a demand for robust IDS solutions that can handle the complexity and diversity of CAV networks. Federated Learning has emerged as a promising tool for addressing these challenges by enabling collaborative IDS model training without sharing sensitive data \cite{chellapandi2023survey, mcmahan2017communication, olagunju2024privacy}. This decentralized approach enables real-time adaptation to emerging threats, making it particularly suited for CAV environments.

For example, Li et al. \cite{li2020deepfed} proposed DeepFed to detect cyber threats in industrial cyber-physical systems, while Nguyen et al. \cite{nguyen2019diot} developed an anomaly detection-based IDS using FL to autonomously detect compromised IoT devices. Despite these advancements, existing FL-based IDS approaches often overlook resource constraints in edge devices, such as limited memory, computation, and energy. These limitations restrict the scalability and practical deployment of FL-based IDS in real-world CAV environments, necessitating frameworks that optimize resource efficiency.

\subsection{Resource Limitations}
While FL provides privacy-preserving and scalable model training, its implementation is often hampered by resource constraints on edge devices \cite{bonawitz2019towards}. Edge devices typically have limited computational power, memory, energy, and bandwidth,  which must be carefully managed in FL frameworks. Existing FL research addresses these constraints through techniques such as server/client-side hyperparameter tuning for improved inference efficiency \cite{cai2019once, kumar2017resource}, bottleneck reduction \cite{ashraf2020novel}, and model compression \cite{aleesa2020review}. Although effective for individual devices, these methods often fail to consider the collaborative nature of FL, where devices must participate in iterative training cycles.

This gap underscores the need for frameworks that balance resource efficiency with scalability while preserving model accuracy. Addressing these limitations requires innovative approaches that reduce computational and memory overhead without sacrificing the collaborative benefits of FL. Transfer Learning offers a promising direction for achieving this balance, as discussed in the next subsection.

\subsection{Transfer Learning}
Transfer Learning leverages knowledge from a pre-trained model on a source domain and applies it to a target domain. This involves adapting the model through fine-tuning or feature extraction using a smaller dataset relevant to the target task \cite{iman2023review, shin2016deep}. Although feature extraction is memory-efficient, it often sacrifices accuracy by not retaining hidden layer outputs. However, fine-tuning the entire network improves accuracy but increases memory demands, making it unsuitable for resource-constrained devices \cite{cai2020tinytl}.

The aforementioned methods address some resource limitations in Federated Learning, such as memory management and computation efficiency, but they often fail to balance these optimizations with the collaborative and iterative nature of FL training. Furthermore, while individual techniques such as model compression or hyperparameter tuning enhance device-level performance, they do not fully account for the scalability and resource-sharing demands of FL frameworks in real-world distributed environments like CAV networks.