\section{Related Work}

In this section we focus on two topics that bear similarities to the proposed model-centric information access paradigm, (i) previous retrieval paradigms in which multiple collections/models were combined to produce the final ranking of resources upon a user's query, namely distributed and federated retrieval and meta-search, and (ii) Agentic AI as an emerging research topic where AI agents plan the solutions to complex problems engaging a large number of specialized expert models in the execution of their plans.

\subsection{Distributed Information Retrieval, Federated Search, and Meta-Search}

Multi-LLM retrieval shares conceptual similarities with distributed information retrieval (DIR)~\cite{Callan1995, Callan2000, Crestani2013}, federated search (FS)~\cite{Shokouhi2009}, and meta-search~\cite{Glover2000metasearch, Chen2001Metasearch}, as all involve querying and integrating results from multiple independent sources. However, while these traditional approaches rely on structured document collections with predefined ranking mechanisms, multi-LLM retrieval must dynamically infer and assess expertise, making it a fundamentally different and more complex problem.

Meta-search engines issue queries to multiple independent search engines and aggregate their results using fusion techniques such as reciprocal rank weighting or probabilistic rank merging~\cite{Glover2000metasearch}. Similarly, federated search queries multiple independent databases and merges the retrieved results into a unified ranking. However, unlike these methods, which rely on external document rankings, expert LLM retrieval must actively determine which models to engage first and then synthesize their responses. Moreover, meta-search systems depend on external ranking functions provided by the underlying search engines, while multi-LLM retrieval lacks predefined ranking mechanisms, requiring alternative strategies such as confidence-based scoring, response quality estimation, and adaptive model selection. Additionally, whereas meta-search/federated search operates on pre-indexed documents, multi-LLM retrieval generates responses dynamically, introducing further challenges in answer aggregation, conflict resolution, and provenance tracking~\cite{Glover2001metasearcharchitecture}. Finally, result merging in meta-search typically involves normalizing and fusing ranked lists, whereas multi-LLM retrieval synthesizes knowledge across models, demanding approaches beyond traditional rank fusion~\cite{Shokouhi2009}.

DIR shares similarities with multi-LLM retrieval in that queries are processed across multiple autonomous document collections, with ranking determined by each collection's estimated ability to satisfy the information need~\cite{Callan1995}. The DIR process consists of three key steps: (i) \textit{resource description}, where each collection is characterized using high-level metadata; (ii) \textit{resource selection}, where a subset of collections is chosen based on estimated query relevance~\cite{Callan1995, Callan2000}; and (iii) \textit{results merging}, where retrieved documents from selected sources are integrated into a unified ranked list~\cite{Crestani2013}.

Despite these similarities, a key distinction is that DIR relies on pre-existing document collections, where relevance can be inferred through established statistical measures. In contrast, multi-LLM retrieval does not operate over a fixed set of indexed documents but rather dynamically selects and queries LLMs that generate responses on demand. Unlike DIR, where source selection is driven by stable collection statistics, multi-LLM retrieval must infer expertise dynamically, since LLMs lack structured metadata and predefined ranking signals. This lack of explicit document collections and structured retrieval units makes expertise assessment and reliability estimation significantly more challenging in multi-LLM environments.

\subsection{Agentic AI and Multi-Agent Collaboration}

An agentic information access ecosystem also bears similarities with agentic AI~\cite{acharya2025agentic}. Agentic AI is a fast-progressing field where AI systems are composed of autonomous, interacting agents that can plan, adapt, and collaborate to achieve complex goals with minimal human intervention. These agents can independently perceive environments, make decisions, and take actions, often coordinating with other agents or tools to optimize outcomes. Key challenges in agentic AI include coordination and orchestration, ensuring that multiple agents effectively divide tasks and avoid conflicts; expertise inference, dynamically selecting the best agent for a given task without predefined metadata; trust and robustness, preventing biases, adversarial manipulation, or errors from propagating across the system; scalability, as increasing the number of agents introduces computational and communication bottlenecks; and evaluation, since traditional AI benchmarks do not capture the complexities of multi-agent interactions, making it difficult to assess decision quality, efficiency, and emergent behaviors.

Despite these overlaps, the primary focus of agentic AI is on planning and execution, where AI agents strategically break down tasks, select relevant tools or models, and iteratively refine their outputs~\cite{li2023camelcommunicativeagentsmind}. In contrast, agentic information access focuses on retrieving and synthesizing knowledge, where the challenge lies not in executing multi-step plans but in identifying the most relevant information sources and integrating their responses effectively.

Second, most agentic AI systems are constrained to a limited number of agents, typically orchestrating a handful of models or API calls to accomplish a goal~\cite{hong2024metagptmetaprogrammingmultiagent, wu2023autogenenablingnextgenllm}. In contrast, agentic information access envisions a large-scale information ecosystem, where potentially thousands or even millions of domain-specialized LLMs must be efficiently queried, ranked, and synthesized. This distinction raises scalability challenges that agentic AI frameworks have yet to address, such as query budget optimization, redundancy elimination, and cost-aware model selection.

A third fundamental difference lies in the availability and structure of metadata. In current agentic AI frameworks, each tool, API, or model is associated with structured or semi-structured metadata, such as a description of its capabilities, and supported inputs/outputs~\cite{xu2023toolbench, li2023apibank}. This metadata provides a prior for model selection, simplifying the decision-making process. For instance, Hugging Face model cards ~\cite{Mitchell2019}\footnote{\url{https://huggingface.co/docs/hub/en/model-cards}} provide descriptive summaries that allow an AI agent to infer whether a model is suited for a given task. However, in multi-LLM retrieval, no standardized metadata can be assumed for all models, making expertise identification significantly harder. Unlike APIs that perform heterogeneous tasks -- such as image segmentation, text-to-speech conversion, or protein folding -- the LLMs in a multi-LLM retrieval system are more homogeneous, all designed to generate textual responses. This lack of clear differentiation complicates expert selection, as distinguishing models requires indirect signals, such as previous retrieval effectiveness, self-reported confidence, or trust-based reputation scores.