\section{An Evaluation Framework for Multi-LLM Ranking}

In this section, we introduce a framework for evaluating agents in their ability to rank multiple LLMs based on their expertise. Our approach simplifies the broader problem of generating optimal answers while minimizing query costs, allowing us to focus specifically on the ranking aspect. These simplifications enable the construction of a reusable and standardized evaluation infrastructure.\looseness=-1

\subsection{Problem Definition}

Let $l \in L$ represent an LLM expert model within a collection of expert LLMs, $L$. Given a user query $q$, the ultimate objective is to generate an optimal answer $a$ while minimizing the computational and financial costs of querying models in $L$.

To facilitate a controlled evaluation, we decompose the problem into two stages: (i) expertise identification and (ii) answer generation. The former is about determining which LLMs are most relevant to a query before invoking them for answer generation, while the latter concerns selecting and querying the most relevant LLMs to produce a response. To isolate the ranking problem, we simplify the task as follows:  
\begin{itemize}
    \item Upon receiving a user query $q$, the user agent must rank all LLMs based on their belief about which LLMs are most relevant to answering $q$.
    \item Prior to real-time querying, the agent is provided with a predefined budget $b$, allowing it to query a subset of LLMs across a set of preparatory queries $\{q\}$.
    \item The challenge is to develop a strategy that maximizes information acquisition about each LLM while minimizing the number of queries issued, ultimately leading to better ranking accuracy for an unseen query $q'$.
\end{itemize}

When an agent queries an LLM $l$ with a query $q$, the model returns:  
\begin{itemize}
    \item An answer $a$,
    \item Metadata $m$, such as information about the model (e.g., ``model cards''), probabilities estimated during auto-regressive generation (used to calculate metrics like perplexity\footnote{\url{https://huggingface.co/spaces/evaluate-metric/perplexity}}~\cite{jelinek1977perplexity}~), logits from the final neural network layer (which could be leveraged for uncertainty estimation), and others.
\end{itemize}

This formulation remains a simplification of the full problem, which ideally would integrate expertise identification with answer generation -- allowing the agent to simultaneously answer user queries while updating its belief on LLM expertise, and allow unrestricted querying during expertise identification -- permitting the agent to use arbitrary query strategies rather than a predefined set of queries. However, by constraining the problem in this way, we enable the construction of a reusable test collection, consisting of standardized query-response metadata.

\subsection{Reusable Test Collection for Evaluation}
\label{sec:test_collection}
A well-structured test collection facilitates systematic evaluation and reproducibility. We propose structuring the dataset as follows:

\subsubsection{Training Data}  

A training set provides access to LLM responses for a range of preparatory queries, allowing agents to learn expertise patterns. Each training tuple consists of:
\[
(q_i, a_i^j, m_i^j)
\]
where $q_i$ is a user query, $a_i^j$ is the answer from LLM $l_j$, $m_i^j$ represents metadata provided by $l_j$, such as confidence scores, logits, or retrieval information. This data allows ranking models to develop heuristics from past responses and model-specific characteristics.

\subsubsection{Test Data}

The test set evaluates the agent's ability to rank LLMs effectively. It can be structured in two ways:

\begin{enumerate}
    \item Answer-Based Evaluation: Comparing LLM-generated answers against ground-truth responses:
    \[
    (q_i, a_i^G)
    \]
    where $a_i^G$ is the correct reference answer for query $q_i$.

    \item Relevance-Based Evaluation: Measuring the relevance of each LLM for a given query:
    \[
    (q_i, qrels_i)
    \]
    where $qrels_i$ defines the relevance scores of all LLMs for query $q_i$.
\end{enumerate}
Relevance judgments can be obtained via human annotations or automatic evaluation metrics (e.g., BLEU, ROUGE, METEOR).

This evaluation framework enables the systematic benchmarking of LLM ranking models by providing a structured methodology for expertise identification and selection. While the framework is simplified for ranking-focused evaluation, it establishes a foundation for future extensions where expertise learning and answer generation can be jointly optimized. By constructing a reusable test collection, we facilitate the development of scalable, adaptable ranking methodologies for multi-LLM retrieval systems.

\subsection{Simulating Thousands of Expert LLMs}
\label{sec:simulation}

\begin{figure}[t]
        \centering
        \includegraphics[width=1\linewidth]{Figures/mLLM_figure2.pdf}
        \caption{Using RAG-based LLMs to obtain expert models. Each expert consists of a shared LLM and retriever but is restricted to a distinct document collection, specifically structured KBs,  which define its domain expertise.}
        \label{fig:simulation}
\end{figure}

As previously mentioned, we do not provide direct access to LLMs but rather their responses. This raises the question: how can we obtain responses from thousands of domain-specific LLMs?
In practice, such a collection of LLMs, covering a wide variety of domains such as sports, finance, medicine, music, and cinema, does not yet exist, and it must be developed by us. We simulate the setup by leveraging a RAG-based approach, where each simulated expert consists of:  
\begin{itemize}
    \item A (shared) underlying LLM, responsible for generating responses. Note that instead of a single LLM it is possible to employ a small number of different LLMs, to diversify the language in the output. This is to address that sometimes, when only one LLM sits at the bottleneck, responses start to look templated in terms of writing style, and one may want to avoid that.
    \item A dedicated retriever, responsible for fetching relevant documents. Similarly to the previous point, one may also want to diversify the retrievers according to domain.
    \item A unique document collection, defining the domain expertise of the LLM.
\end{itemize}
Thus, an LLM configured with a cinema-related document collection acts as a cinema expert, while another with access to financial reports functions as a finance expert. 
This document-controlled retrieval ensures that each simulated expert has access only to domain-specific knowledge, allowing us to model the behavior of real-world expert LLMs. Our simulated setup is illustrated in Figure~\ref{fig:simulation}.

One of the key challenges in simulating expert LLMs is ensuring that they do not leak general knowledge from their base models. Ideally, an LLM should generate responses exclusively based on the retrieved documents and not rely on information it has learned during its pre-training phase. In practice, however, it is impossible to separate an LLM's language generation skills from its parametric memory.
%
To mitigate this issue, several strategies can be employed. In this work, we start simple and recommend (i) prompt engineering and (ii) query filtering. With prompt engineering, the model can be explicitly instructed to generate responses only based on the provided retrieved content. However, research has shown that this method is not always reliable, as LLMs sometimes disregard instructions and incorporate pre-trained knowledge into their responses. Query filtering is a more robust approach by which we filter user queries to ensure that the correct answers are absent from the LLM's pre-existing knowledge. This guarantees that any response must be derived purely from retrieval. One way to implement this is to compare base LLM responses against ground truth answers. If the base model fails to generate the correct answer without retrieval, but relevant documents are available, then we can further analyze whether the model's retrieved response logically follows (entails) the relevant documents. Queries in which the model's generated response is not entailed by the documents or differs significantly from the ground truth are ideal candidates for testing.

While an ideal evaluation setup would involve constructing a dedicated test collection with thousands of specialized document collections corresponding to different expert domains, this is computationally expensive and impractical at scale. Instead, a more feasible alternative is to reuse existing large-scale document collections.\looseness=-1

For this approach to work, the chosen collection must meet the following criteria:
\begin{itemize}
    \item It should be large enough to allow the construction of sub-collections representing different areas of expertise.
    \item It should include a large set of queries, for which either relevant documents are identified or ground truth answers have been collected.
\end{itemize}
Given such a dataset, a document clustering approach can be used to partition the full collection into thousands of clusters, each representing a distinct area of expertise.

For each simulated expert model, retrieval is performed only within its assigned cluster to obtain relevant documents. The retrieved documents are then used as input prompts to generate responses, ensuring that each expert model is constrained to its predefined domain. This method, however, presents a computational bottleneck, as it requires building thousands of individual search indices, one for each cluster. A more efficient alternative is to construct a single global index for the entire document collection. Then, upon receiving a query:
\begin{enumerate}
    \item Retrieve the top-k documents using the global index.
    \item Identify which clusters these documents belong to.
    \item For each cluster represented in the top-k ranking, provide the top-c documents to the LLM for response generation.
    \item For clusters that are not represented in the top-k ranking, a random document from the cluster can be sampled, since it is likely that all documents in the cluster are irrelevant to the query.
\end{enumerate}
This approach maintains domain specialization while significantly reducing computational complexity, making it a practical solution for scaling the simulation of thousands of expert LLMs.

\subsubsection{Implementation Details}

\begin{figure}[t!]
\begin{subfigure}{0.49\columnwidth}
  \centering
  \resizebox{1\columnwidth}{!}{\includegraphics[width=1.\columnwidth]{Figures/wordcloud/cluster_100.pdf}}
  \caption{Expertise in elections.}
  \label{fig:cluster_1}
\end{subfigure}
\begin{subfigure}{0.49\columnwidth}
  \centering
  \resizebox{1\columnwidth}{!}{\includegraphics[width=1.\columnwidth]{Figures/wordcloud/cluster_274.pdf}}
  \caption{Expertise in business.}
  \label{fig:cluster_2}
\end{subfigure}
\begin{subfigure}{0.49\columnwidth}
  \centering
  \resizebox{1\columnwidth}{!}{\includegraphics[width=1.\columnwidth]{Figures/wordcloud/cluster_1488.pdf}}
  \caption{Expertise in urban planning.}
  \label{fig:cluster_3}
\end{subfigure}
\begin{subfigure}{.49\columnwidth}
  \centering
  \resizebox{1\columnwidth}{!}{\includegraphics[width=1.\columnwidth]{Figures/wordcloud/cluster_1535.pdf}}
  \caption{Expertise in video.}
  \label{fig:cluster_4}
\end{subfigure}
\caption{Word clouds of 4 randomly chosen clusters, each simulating an LLM expert.}
\label{fig:word_cloud}
\end{figure}

\paragraph{Clustering} For effective creation of areas of expertise, a large document collection is necessary to ensure that the cardinality of each cluster is large. Existing collections that meet our standards are ClueWeb09~\cite{ClueWeb09}, containing approximately 504 million web pages, and Clueweb22~\cite{overwijk2022clueweb2210billionweb}, containing roughly 1 billion. 

To simulate the experts on ClueWeb09, we cluster its corpus with K-Means~\cite{Hartigan1979}, which ensures disjoint clusters with no overlap. Additionally, to transform the raw text to feature vectors, an efficient and scalable vectorizer with very low memory usage and streaming capability is essential for fast addition of new experts and practical constraints such as memory consumption. Instead of using a TF-IDF~\cite{SALTON1988513,7754750} vectorizer requiring in-memory vocabulary and mapping which renders it infeasible for very large collections on a resource-constrained infrastructure, a hashing vectorizer~\cite{Ramos1999,NIPS2017_f0f6ba4b,engproc2023046005} is used. A hashing vectorizer maps each text document to a fixed sized feature vector by applying a hash function to the term frequency of each token. The hashing vectorizer is stateless and can be easily used for addition of new expert knowledge. 1000-dimensional hash vectors of the collection were used for creating 2000 clusters for our document collections. Additionally, clusters with too many (generic) or too few (specific) documents were discarded. In Figure ~\ref{fig:word_cloud} we present a method by which we qualitatively estimate the expertise of a cluster; we sample random documents and assess their expertise based on their vocabulary. While this qualitative approach is rough, it makes for efficient human evaluation, given the scale of the problem.

Even though the Clueweb22 corpus is only twice as large as ClueWeb09, performing the same type of clustering using K-means and hashing on a billion documents is computationally prohibitive. Thankfully, the creators of Clueweb22 have tagged its web pages with topics. Leveraging this, we form approximately 20000 experts by performing topic-based clustering with a simple deterministic union-find algorithm.\looseness=-1

\paragraph{Question Answering} An essential component while building a data collection described in Section \ref{sec:test_collection} and the simulation process described in Section \ref{sec:simulation} is to ensure that ground truth answers are available for correctness and reliability of the question and answer pairs. To do so, we utilize QUASAR-T~\cite{dhingra2017quasardatasetsquestionanswering} as the annotated question answering dataset which is built over ClueWeb09 document collection. QUASAR-T contains a training set with $37012$ question-answer pairs, a dev set with $3139$ and a test set with $3000$. Each context document associated with a question-answer pair belongs to one of the clusters built in the clustering step, and is also associated with the selected cluster. Next, we built a global index using all the documents in the collection. Alternatively, $K$ indices can be built with the positive/relevant documents from Quasar-T and negative documents (which is not associated with a question-answer pair) from the same cluster. Each expert LLM uses the top-k relevant documents retrieved from the global/local index to simulate an expert by means of RAG.

\subsection{Discussion}

Our proposed evaluation framework has its own set of caveats. For instance, how does one create and query $K$ indices, when $K$ is large? What is the trade-off between having a single shared index between $K$ experts, and $K$ separate indices? However, dealing with these caveats is far more preferable to the alternative of storing and querying $K$ LLMs. For this reason, we believe that the simulation proposed in this section is a viable start to evaluating the research challenges proposed in Section~\ref{sec:challenges}.
