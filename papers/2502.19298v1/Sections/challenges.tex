\section{Challenges in Multi-LLM Information Retrieval}
\label{sec:challenges}

The transition from document-centric to model-centric IR introduces a range of fundamental challenges. Unlike conventional search engines that retrieve and rank documents based on well-established data structures and ranking functions, an LLM-based system must dynamically select, query, and synthesize responses from a diverse set of expert models. This raises critical questions:
\begin{itemize}
    \item[RQ1] Which LLMs should be queried for a given information need? 
    \item[RQ2] How should their responses be aggregated into a coherent answer? 
    \item[RQ3] How do we ensure robustness against bias or adversarial manipulation? 
    \item[RQ4] How do we evaluate a model-centric retrieval system?
\end{itemize}

In this section, we explore these challenges in detail, starting with the problem of selecting relevant LLM experts, followed by multi-LLM answer aggregation, issues of robustness, and the development of novel evaluation frameworks.

\subsection{Selecting Relevant LLM Experts}
Selecting the right LLMs for a given query is a fundamental challenge. Unlike traditional IR, where search engines rely on well-established ranking functions over documents, model-centric search requires identifying and querying a subset of expert models, balancing accuracy, diversity, cost, and efficiency.

\subsubsection{Defining LLM Expertise}

A fundamental challenge in an LLM-centric retrieval system is determining what makes an LLM an expert on a given topic. Unlike traditional document retrieval, where relevance can be inferred from topical similarity measures, expertise in LLMs is more elusive. Should an LLM be considered an expert based on the data it was trained on, the domains it was fine-tuned for, or its demonstrated ability to provide high-quality answers? In many cases, LLMs operate as black boxes, offering little transparency about their training data or the methods by which they generate responses. This lack of visibility complicates the process of verifying their domain competence. Additionally, even if an LLM is trained on relevant material, does that necessarily mean it can generate accurate, contextually appropriate responses? Expertise is not just about knowledge but also about reasoning and the ability to contextualize information correctly. Given this, we need a systematic way to rank LLM expertise dynamically, rather than relying on static, pre-determined categorizations. This leads to the following research question:

\begin{itemize}
    \item[\textbf{RQ1.1}] How can we define and measure the expertise of an LLM?
\end{itemize}

One possible approach is to infer expertise empirically by evaluating an LLM's responses on benchmark datasets covering different domains. However, this method is limited to the domains covered by the benchmark. Further, there is no guarantee that all LLMs can be evaluated against the same benchmarks. Alternatively, reputation-based models could be developed, where an LLM's past performance on user queries informs its future ranking for similar queries. But this raises additional concerns: how do we assess the performance of an LLM? Do we need human judges, automatic methods to assess performance, e.g. new query performance prediction methods, or new uncertainty quantification methods that correlate well with accuracy? How do we handle new or emerging LLMs that lack historical performance data? And how can we ensure that performance metrics are robust against adversarial manipulation, where LLM providers game the system to appear more authoritative than they actually are? %Moreover, even if we could measure expertise reliably, we still need to address the question of how to surface the most relevant LLM for a given user query -- should expertise be binary (expert vs. non-expert), or should it be a graded ranking based on confidence, domain overlap, and response quality? 
Addressing these challenges requires a fundamental rethink of how expertise is defined, measured, and updated in a rapidly evolving LLM ecosystem.

\subsubsection{Efficient Querying}

Once relevant LLM experts have been identified, the next challenge is determining how many should be queried to maximize response quality while minimizing cost and computational overhead. Unlike traditional search engines, where retrieving more documents often improves recall without significant additional expense, querying more LLMs incurs non-trivial costs in computation, latency, and financial resources. Each LLM query consumes processing power and may be subject to usage fees, making an exhaustive search across all available experts infeasible. Instead, an efficient expert selection strategy must strike a balance: querying too few LLMs may result in incomplete or biased answers, while querying too many can lead to unnecessary redundancy, slower response times, and increased costs. This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ1.2}] How many LLMs should be queried once we identify a ranking of expertise for a given query?
\end{itemize}

An ideal system would dynamically adjust the number of experts consulted based on the complexity of the query and the expected information gain from additional models. If a simple factual question can be confidently answered by a single high-reputation LLM, there is little benefit in polling multiple experts. However, for ambiguous, multi-faceted, or domain-spanning queries, aggregating responses from multiple LLMs might be necessary to construct a well-rounded answer. The system must therefore develop predictive mechanisms to estimate, before execution, how much additional querying will improve the response. 

\subsubsection{Cost and Query Budgeting: How Do We Minimize Computational Overhead?}

Another consideration is how to prioritize and schedule LLM queries. Should the system query all selected LLMs in parallel to minimize latency, or should it use an adaptive querying strategy, where it starts with a small number of high-confidence experts and only expands the search if the responses are inconsistent or incomplete? 

\begin{itemize}
    \item [\textbf{RQ1.3}] How can we minimize the computational overhead of querying LLMs?
\end{itemize}

A practical approach is to implement a tiered querying strategy based on cost and expected value. Low-cost or freely available LLMs could be queried first, providing an initial response, with more expensive models accessed only if additional expertise is needed. This strategy is analogous to progressive search techniques in IR, where retrieval starts with a small, efficient subset before expanding if necessary. However, to implement such a system effectively, the system must be able to predict the expected cost and utility of querying different LLMs in real time. This requires building cost-aware meta-indices, which estimate an LLM's likely expertise, confidence, and query cost before actually invoking it.

A more adaptive solution would involve reinforcement learning or multi-armed bandit approaches that dynamically optimize the cost-performance trade-off. The system could learn over time which LLMs provide the most useful answers for specific query types, gradually refining its querying strategy to minimize cost without sacrificing accuracy. Additionally, cost modeling could incorporate user preferences and query criticality -- for instance, users might accept slightly less accurate responses for routine queries but demand higher-quality results for complex, high-stakes questions, justifying higher costs.

Ultimately, achieving cost-effective querying in an LLM-based IR system requires a multi-layered optimization approach: (i) predictive cost modeling to estimate the utility of each LLM before querying it, (ii) adaptive, tiered querying strategies that escalate complexity only when needed, and (iii) learning-based approaches to refine expert selection dynamically. Without such mechanisms, the financial and computational burden of large-scale LLM-based retrieval would be prohibitive, limiting the viability of this new paradigm.\looseness=-1

\subsubsection{Temporal and Contextual Adaptation: Can LLM Expertise Change Over Time?}
Expertise is not static. What is considered authoritative today may become outdated tomorrow. In an LLM-centric retrieval system, this challenge is even more pronounced because LLMs do not inherently update their knowledge in real time. Unlike traditional search engines, which can index new documents as they become available, LLMs rely on their training data, which may be months or even years old. This creates a fundamental issue: how do we ensure that LLMs selected as experts remain relevant as real-world facts change? If an LLM trained on medical knowledge from 2022 is queried about a treatment breakthrough from 2024, its response may be inaccurate, yet the system may still consider it an "expert" based on its past performance. This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ1.4}] How can we update the expertise of LLMs in a dynamic fashion?
\end{itemize}

One approach to mitigating this problem is periodic re-evaluation of LLM expertise. Just as human experts require continuous education to stay updated, LLMs could be assessed on evolving benchmark datasets that incorporate the latest knowledge. However, this requires an infrastructure capable of continuously testing LLMs against new ground-truth information, a challenging and resource-intensive goal. 

\subsection{Multi-LLM Answer Aggregation}
Once multiple LLM experts are selected and queried, the next challenge is how to synthesize their responses into a coherent, accurate, and useful answer. Unlike traditional search, where ranked lists of documents allow users to extract information themselves, an LLM-centric retrieval system must integrate multiple expert outputs -- some of which may complement each other, while others may conflict. Below are the key open subtopics in multi-LLM answer aggregation.

\subsubsection{Response Merging}

A naive approach might be to concatenate all responses, but this often results in redundancy, verbosity, or inconsistencies. Instead, should aggregation rely on abstractive summarization techniques, where responses are fused into a more concise and cohesive format? Alternatively, should users be presented with a ranked list of expert opinions, preserving individual perspectives? Another consideration is context-dependent synthesis: should the method of merging depend on the type of question asked, with factual queries receiving a single integrated response while opinion-based or exploratory queries maintain multiple viewpoints? The challenge, then, is to design aggregation methods that minimize redundancy while ensuring completeness, clarity, and informativeness. This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ2.1}] How can we develop methods that optimally aggregate different answers account for the query and user needs?
\end{itemize}

\subsubsection{Conflict Resolution}
As different LLM experts are consulted, it is inevitable that some will provide conflicting answers, especially on ambiguous or evolving topics. Unlike traditional search, where users can compare multiple sources themselves, LLM aggregation systems must resolve inconsistencies automatically or present contradictions transparently. Should models be ranked based on confidence scores, with higher-confidence responses prioritized in synthesis? Should conflicting responses trigger a secondary verification process, where additional LLMs are queried as tie-breakers? Moreover, does presenting multiple perspectives side by side, rather than forcing a single consensus, better align with the needs of users searching for nuanced or controversial topics? The system must strike a balance: when should conflicts be merged into a neutral response, and when should disagreements be explicitly surfaced? Without robust mechanisms for handling contradictions, users may receive misleading or oversimplified information, undermining trust in the system. This leads to the following research question:

\begin{itemize}
    \item[\textbf{RQ2.2}] How do we present conflicting answers to users?
\end{itemize}

\subsubsection{Confidence Estimation}
Trustworthiness is a key concern in multi-LLM aggregation, as users need a way to assess the reliability of synthesized responses. Since different LLMs vary in training data, reasoning capabilities, and update frequency, some responses may be significantly more reliable than others. How should the system evaluate an LLM's confidence in its own response? Should models be required to self-report confidence scores, or should an external mechanism infer reliability? Could meta-evaluation models analyze linguistic patterns, factual consistency, or source attribution to predict response trustworthiness? Additionally, if multiple LLMs agree on an answer, should their collective agreement serve as an implicit confidence boost, akin to ensemble methods in machine learning? A key research challenge is to develop a framework where users can gauge not just what is being said, but how reliable the information is, ensuring that low-confidence or speculative responses are not treated as authoritative. This leads to the following research question:

\begin{itemize}
    \item[\textbf{RQ2.3}] How do we quantify the reliability of an answer in a way that we can compare multiple heterogeneous models?
\end{itemize}

\subsubsection{Attribution and Source Transparency}
One of the fundamental challenges in aggregating responses is ensuring transparency regarding where information comes from. Users of traditional search engines can inspect the provenance of retrieved documents, but in an LLM-centric retrieval system, the origin of synthesized information is often obscured. Should responses include explicit citations to the LLMs that contributed to the final answer? If so, how should attribution be handled when multiple models generate similar but slightly different responses? Furthermore, proprietary LLMs may refuse to disclose their sources, making it difficult to assess their credibility. Would a trust registry for LLMs, where models are scored based on their past citations and accuracy, be a viable solution? Transparency in attribution is not just about trust -- it also ensures accountability, preventing the spread of unverifiable or misleading information. Without clear provenance, users may struggle to distinguish authoritative knowledge from speculative or hallucinated responses. This leads to the following research question:

\begin{itemize}
    \item[\textbf{RQ2.4}] How do we attribute answers to the underlying models that provided them?
\end{itemize}

\subsection{Bias, Trust, and Adversarial Manipulation in Multi-LLM Retrieval}
In an LLM-centric retrieval system, trust and bias mitigation are fundamental challenges, especially when adversarial actors seek to manipulate perceived expertise for strategic gain. Unlike traditional search engines, where documents and sources can be manually verified, an LLM-based ecosystem relies on dynamic model selection, which opens the door for adversarial exploitation. Malicious entities could develop LLMs that appear authoritative, only to later inject biased, misleading, or harmful responses into the information ecosystem. This presents a novel and pressing research challenge: how can we ensure that expert selection remains unbiased, resistant to manipulation, and aligned with factual accuracy?

\subsubsection{Preventing the Domination of Certain LLMs in Expert Selection}
A major risk in multi-LLM retrieval is the monopolization of expert selection by a small set of dominant models. If the aggregation system frequently favors the same LLMs, users may receive an overly narrow perspective, reinforcing existing biases. Further, if user feedback is incorporated into the belief model for expertise selection, these biases can be amplified over time, undermining the quality and diversity of ranking. This raises the question: 

\begin{itemize}
    \item [\textbf{RQ3.1}] How do we ensure that expert selection is diverse and representative?
\end{itemize} 

One approach is to introduce diversification constraints, ensuring that responses incorporate perspectives from multiple LLMs, even if some models have lower confidence scores. Alternatively, should the system cap the frequency with which a specific LLM is selected per query type to prevent overrepresentation? However, these solutions introduce trade-offs: if an LLM is truly the most reliable expert, artificially limiting its influence could reduce overall answer quality. Another concern is gaming of the expert ranking system -- should model providers be able to pay for increased visibility, and if so, how do we prevent economic incentives from skewing expert selection? Without safeguards, an open-market approach could lead to a winner-takes-all scenario, where the most financially backed LLMs dominate, drowning out smaller but potentially more accurate models.

\subsubsection{Adversarial Attack Models}
In an open ecosystem where LLMs compete to be recognized as experts, malicious actors could design adversarial models that exploit trust mechanisms. 
%
A deceptive LLM might start by providing reliable, high-quality answers, gradually building a strong reputation within the system. However, once it achieves expert status, it could subtly introduce misinformation, steering responses in ways that serve political, economic, or ideological interests. Another potential attack vector is citation gaming, where multiple adversarial LLMs artificially reinforce each other's credibility by citing one another in their responses. This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ3.2}] What is an effective adversarial attack in a model-centric retrieval?
\end{itemize}

\subsubsection{Adversarial Attack Detection}

Detecting such adversarial behavior is an urgent challenge. One possible defense is delayed adversarial detection, where models are continuously evaluated over time for sudden shifts in behavior. Additionally, should trust scores decay over time if inconsistencies or contradictions are detected? Addressing these risks requires robust adversarial detection systems, perhaps borrowing techniques from fraud detection, anomaly detection, and adversarial machine learning. Without proactive safeguards, LLM-driven search could become an easy target for misinformation campaigns, with major societal implications. This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ3.3}] How can we robustify ranking algorithms against adversarial attacks?
\end{itemize}

To maintain trust in multi-LLM retrieval, a reputation framework must be developed to evaluate expert reliability over time. But what metrics should determine trust? Should trust scores be assigned based on historical accuracy? If so, how do we fairly measure accuracy in subjective domains like law, ethics, or politics? Another approach could involve user feedback, where responses are rated for correctness and usefulness. However, how do we prevent coordinated attacks, where adversarial actors manipulate ratings to boost or suppress certain LLMs? 

\subsection{Developing an Evaluation Framework for Multi-LLM Retrieval}

The evaluation of IR systems has traditionally relied on test collections consisting of queries, a fixed document corpus, and relevance judgments~\cite{Voorhees2001TREC, Robertson2008evaluation}. This structured approach has enabled the development of standard retrieval metrics, which assess how well a system retrieves and ranks documents. However, multi-LLM retrieval fundamentally alters the nature of retrieval units, shifting from static documents to generated responses that synthesize information from multiple models. This shift necessitates a re-examination of evaluation frameworks, as traditional IR methodologies do not fully capture the challenges introduced by generative responses, expertise selection, and response aggregation.

\subsubsection{Computational Challenges in Developing a Million LLMs}
Building a multi-LLM retrieval ecosystem at the scale of millions of specialized models introduces significant computational challenges that go beyond traditional machine learning infrastructures. Unlike current AI deployment paradigms, where a small number of general-purpose LLMs are trained and fine-tuned on diverse datasets, a system with millions of domain-specific LLMs requires an entirely new approach to training, and deployment.

A fundamental challenge lies in model differentiation and specialization. Training millions of LLMs with distinct expertise requires efficient knowledge partitioning, ensuring that each model acquires specialized domain knowledge while avoiding unnecessary redundancy. Traditional fine-tuning strategies, which involve adapting a base model to a specific dataset, do not scale well when training millions of models, as each instance would demand separate computational resources, training data pipelines, and continuous updates. Instead, novel parameter-efficient tuning techniques such as Low-Rank Adaptation (LoRA)\cite{Hu2021LoRA} and Mixture of Experts (MoE) architectures\cite{Shazeer2017MoE} may offer scalable alternatives by allowing models to share a common backbone while activating only domain-specific components during inference. A different scalable solution could involve architectures that integrate retrieval-augmented generation (RAG)~\cite{Lewis2020RAG} to allow LLMs to fetch information from a specialized data store. The hardware and storage infrastructure required to support a million LLMs is another critical bottleneck. Unlike traditional search engines, where documents are stored in an index, a multi-LLM system requires active model storage, retrieval, and execution.  This leads to the following research question:

\begin{itemize}
    \item [\textbf{RQ4.1}] How can we build the necessary infrastructure to support a large number of specialized LLMs?
\end{itemize}

\subsubsection{Redefining Test Collections for Multi-LLM Retrieval}
In docu-\\-ment-centric IR, test collections are built around a static corpus, allowing queries to be evaluated based on a predefined set of relevance judgments. In contrast, multi-LLM retrieval does not involve retrieving pre-existing documents but instead generates responses dynamically. This shift raises fundamental questions about how to construct evaluation datasets. Should test collections contain human-written reference responses, similar to QA and summarization benchmarks like SQuAD, MS MARCO, or Natural Questions~\cite{Rajpurkar2016SQuAD, Bajaj2018MSMARCO}? Alternatively, should multi-LLM retrieval be evaluated based on human comparative judgments, where different model outputs are rated and ranked? This leads to the following research question:\looseness=-1

\begin{itemize}
    \item [\textbf{RQ4.2}] How can we build static and reusable test collections to assess the LLM expertise and the quality of the response aggregation?
\end{itemize}

\subsubsection{New Metrics for Evaluating Multi-LLM Responses}
The transition from document ranking to generative response synthesis requires new evaluation metrics that go beyond traditional IR effectiveness measures. Precision, recall, and NDCG assume a fixed relevance scale based on pre-existing documents, making them unsuitable for evaluating generated responses that may vary significantly in wording and structure. One possible direction is to adapt existing NLP evaluation metrics such as BLEU~\cite{Papineni2002BLEU}, ROUGE~\cite{Lin2004ROUGE}, and BERTScore~\cite{Zhang2019BERTScore} to assess content overlap between generated and reference responses. However, these metrics primarily measure lexical or embedding similarity, with already known issues in generative model evaluation. This leads to the following research question:\looseness=-1

\begin{itemize}
    \item[\textbf{RQ4.3}] What metrics allow us to compare different user agents that search, retrieve, query and aggregate answers against multiple LLMs? 
\end{itemize}

