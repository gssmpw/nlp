\section{Introduction}

\begin{figure*}[t]
        \centering
        \includegraphics[width=0.95\linewidth]{Figures/mLLM_figure.pdf}
        \caption{An illustration of a framework for querying both domain-specialized LLMs (knowledge agents) and user-specific LLMs (user agents). Users interact with the system through personalized user agents, which not only track past queries and responses to refine retrieval but also share specialized user knowledge with others. A belief model on expertise determines which knowledge agents ($K_1, K_2, \dots, K_M$) and user agents are experts on specific topics and should be queried to generate the most relevant answers. The system optimizes for cost and latency, ensuring efficient and accurate responses while minimizing unnecessary queries. This architecture enables the dynamic orchestration of expert LLMs, moving beyond document retrieval toward a multi-expert synthesis of knowledge.}
    \label{fig:metaLLM}
\end{figure*}

The rapid advancement of generative artificial intelligence has significantly reshaped the field of information retrieval (IR)~\cite{zhu2023large,allan-2024-future,white2025information}. Traditionally, a user-provided query prompts an IR system to align the query's intent with a pre-existing corpus of textual resources, returning a ranked list of documents from which the user then extracts relevant information. In this paper, we envision a future beyond traditional information access, in which information is served to users through large language models (LLMs)~\cite{openai2022chatgpt, wei2022emergentabilitieslargelanguage}. As generative AI models increasingly act as primary sources of information, this shift necessitates a new paradigm. This paradigm must address the challenges of selecting the most suitable LLMs to query and synthesizing knowledge across multiple expert models.

This future is unfolding at a fast pace~\cite{caramancion2024large}\footnote{\url{https://kafkai.com/en/blog/ai-impact-on-search-engines/}}, driven by the proliferation of domain-specialized LLMs tailored to increasingly narrow fields of expertise. We are moving toward a world where millions of such expert LLMs will exist, each trained on distinct knowledge domains, some overlapping, others highly specialized. This shift is already underway, with companies like BloombergGPT~\cite{wu2023bloomberggpt}, Thomson Reuters\footnote{\url{https://legalsolutions.thomsonreuters.co.uk/en/c/practical-law/now-with-generative-ai.html}}, Elsevier\footnote{\url{https://www.elsevier.com/products/scopus/scopus-ai}} developing LLMs on proprietary material. This trend is set to accelerate as more content providers, institutions, and organizations build models fine-tuned to their unique datasets.
As this paradigm evolves, a user's information needs will increasingly be met not by a single system, but by querying and synthesizing responses from multiple expert LLMs, requiring careful orchestration to extract the most accurate and relevant insights.

Beyond expert LLMs, we envision a future where users themselves become ``experts'' through their search behavior. Personalized digital assistant LLMs will continuously read, process, and synthesize everything a user has searched for and engaged with, leading to the creation of millions of individualized user-LLMs, each evolving to reflect a specific user's knowledge, expertise, and interests.

In this evolving digital ecosystem, an agent will be necessary to orchestrate queries and synthesize responses from domain-expert LLMs. However, querying all available LLMs is prohibitively expensive and inefficient. Thus, a new type of search engine is required that, given a user's question, intelligently retrieves only the most relevant expert LLMs. This agent must dynamically determine which expert models to consult, balancing the need for accuracy with cost efficiency. Upon receiving a user's query, it should strategically decide which LLMs to engage, ensuring the response is both comprehensive and cost-effective while minimizing redundant computations. 

This new paradigm presents several fundamental challenges. First, selecting relevant LLM experts by querying all available models is computationally expensive and inefficient, requiring intelligent mechanisms to identify the most relevant models based on expertise, context, and cost. Second, multi-LLM answer aggregation introduces new complexities, as responses from different expert LLMs must be synthesized, reconciled, and ranked to provide coherent and contextually accurate answers. Third, bias, trust, and adversarial manipulation in multi-LLM retrieval become pressing concerns, as LLM-generated responses may reflect underlying biases, selectively omit critical information, or be manipulated by adversarial actors. Finally, these challenges require a novel evaluation framework and the infrastructure necessary to study. Addressing these issues will be crucial in shaping the next generation of model-centric\footnote{In this work we use model-centric and LLM-centric interchangeably.} IR.

To address these challenges, we propose a framework for orchestrating domain-specialized LLMs (knowledge agents) and personalized user LLMs (user agents). As illustrated in Figure \ref{fig:metaLLM}, this framework structures IR as a dynamic interaction between users and a distributed network of expert models.
Users interact with the system through personalized user agents, which refine retrieval strategies based on past queries, responses, and feedback. These agents maintain a belief model on expertise, dynamically assessing which knowledge agents (domain-specialized LLMs) and user agents (peers with relevant expertise) are most suitable for responding to a given query. This belief model is continuously updated based on user interactions, ensuring that information synthesis prioritizes the most reliable and contextually relevant sources.
Each knowledge agent specializes in a distinct domain, accessing structured knowledge bases (KBs) or proprietary datasets. Some knowledge agents overlap in expertise, while others are highly specialized, creating a network of interoperable expert models that can be selectively queried based on the belief model's assessment. Additionally, user agents share specialized user knowledge, allowing insights from personalized experiences to propagate through the system.
To optimize cost and latency, the framework incorporates an adaptive query mechanism, reducing redundant queries and minimizing computational overhead while maintaining response quality. Instead of blindly querying all potential sources, the system selectively engages only the most relevant knowledge and user agents, balancing efficiency with accuracy.

To ground our discussion, we first examine previous and current IR paradigms relevant to the  multi-LLM retrieval paradigm. We then explore each of the key challenges in detail, analyzing the complexities of selecting relevant LLM experts, aggregating responses across multiple models, ensuring trust and robustness. Finally, we introduce a simplified evaluation framework for ranking relevant LLM experts as a starting point.