\documentclass[sigconf,natbib=true,nonacm]{acmart}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2018}
\acmYear{2018}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference acronym 'XX]{Make sure to enter the correct
  conference title from your rights confirmation email}{June 03--05,
  2018}{Woodstock, NY}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}

\usepackage{caption}
\usepackage{subcaption}

\begin{document}

\title{Agent-centric Information Access}

\author{Evangelos Kanoulas, Panagiotis Eustratiadis, Yongkang Li, Yougang Lyu, Vaishali Pal, \\ Gabrielle Poerwawinata, Jingfen Qiao, Zihan Wang}
\affiliation{
  \institution{University of Amsterdam} 
  \country{The Netherlands}
}
\email{{ e.kanoulas, p.efstratiadis, y.li7, y.lyu, v.pal, g.poerwawinata, j.qiao, z.wang2 } @uva.nl}

\renewcommand{\shortauthors}{Kanoulas et al.}

\begin{abstract}
As large language models (LLMs) become more specialized, we envision a future where millions of expert LLMs exist, each trained on proprietary data and excelling in specific domains. In such a system, answering a query requires selecting a small subset of relevant models, querying them efficiently, and synthesizing their responses. This paper introduces a framework for agent-centric information access, where LLMs function as knowledge agents that are dynamically ranked and queried based on their demonstrated expertise. Unlike traditional document retrieval, this approach requires inferring expertise on the fly, rather than relying on static metadata or predefined model descriptions. This shift introduces several challenges, including efficient expert selection, cost-effective querying, response aggregation across multiple models, and robustness against adversarial manipulation. To address these issues, we propose a scalable evaluation framework that leverages retrieval-augmented generation and clustering techniques to construct and assess thousands of specialized models, with the potential to scale toward millions.

\end{abstract}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle

\input{Sections/intro}
\input{Sections/related}
\input{Sections/challenges}
\input{Sections/evaluation}

\section{Conclusion}

The emergence of multi-LLM retrieval marks a significant departure from traditional information retrieval, shifting from document ranking to expert model selection and response synthesis. This evolution challenges long-established principles of retrieval effectiveness, and evaluation, demanding new methodologies that balance cost and accuracy. As we envision a future with millions of specialized LLMs, the necessity for scalable ranking mechanisms, robust expertise identification, and adaptive retrieval strategies becomes paramount.

The shift to multi-LLM retrieval presents several fundamental challenges. First, LLM expertise identification is complex, as models vary in domain specialization, reliability, and transparency, requiring new ranking mechanisms. Second, query efficiency and cost management are critical, as querying multiple LLMs is computationally expensive, necessitating budget-aware strategies that balance retrieval effectiveness with cost constraints. Third, multi-LLM response aggregation introduces challenges in synthesizing coherent, accurate, and unbiased answers from multiple models, especially when responses conflict. Fourth, trust, bias, and adversarial manipulation remain major concerns. Finally, evaluation frameworks must evolve, as traditional test collections and document relevance judgments are no longer sufficient. Our proposed evaluation framework lays the groundwork for systematically addressing these questions, yet it is only the beginning. The methodologies outlined here will require continuous refinement, driven by empirical research and large-scale experimental validation. 

%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
% \begin{acks}
% To Robert, for the bagels and explaining CMYK and color spaces.
% \end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{references}


%%
%% If your work has an appendix, this is the place to put it.
% \appendix

\end{document}
\endinput
%%
%% End of file `sample-sigconf.tex'.
