\documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{multirow}
\usepackage{float}
\usepackage{subfig}
\usepackage{hyperref}
\usepackage{bm}
\usepackage{amsmath}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{amsthm,amsmath,amssymb}\usepackage{mathrsfs}
% \usepackage[pdftex]{graphicx}
\usepackage{epstopdf}
\usepackage[table]{xcolor}
\usepackage{amsmath,amssymb,amsthm}
\makeatletter
\renewcommand{\maketag@@@}[1]{\hbox{\m@th\normalsize\normalfont#1}}%
\makeatother
\hyphenation{op-tical net-works semi-conduc-tor IEEE-Xplore}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\usepackage{balance}
\newtheorem{theorem}{Theorem}
\begin{document}
\title{
% Matching and Phrase Grounding of Medical Images and Text through Multi-Granularity Alignment
Progressive Local Alignment for Medical Multimodal Pre-training}
\author{Huimin Yan, Xian Yang, Liang Bai, Jiye Liang~\IEEEmembership{Fellow,~IEEE}

\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem
Huimin Yan, Liang Bai and Jiye Liang are with Institute of Intelligent Information Processing, Shanxi University, Taiyuan, 030006, China (Corresponding author: Liang Bai)\protect\\
Email: yanhm0925@163.com, bailiang@sxu.edu.cn, ljy@sxu.edu.cn

Xian Yang is  with Alliance Manchester Business School, The University of Manchester, Manchester, M13 9PL, UK\protect\\
Email: xian.yang@manchester.ac.uk
}
\thanks{}
}

\markboth{}%
{How to Use the IEEEtran \LaTeX \ Templates}

\maketitle

\begin{abstract}
Local alignment between medical images and text is essential for accurate diagnosis, though it remains challenging due to the absence of natural local pairings and the limitations of rigid region recognition methods. Traditional approaches rely on hard boundaries, which introduce uncertainty, whereas medical imaging demands flexible soft region recognition to handle irregular structures. To overcome these challenges, we propose the Progressive Local Alignment Network (PLAN), which designs a novel contrastive learning-based approach for local alignment to establish meaningful word-pixel relationships and introduces a progressive learning strategy to iteratively refine these relationships, enhancing alignment precision and robustness. By combining these techniques, PLAN effectively improves soft region recognition while suppressing noise interference. Extensive experiments on multiple medical datasets demonstrate that PLAN surpasses state-of-the-art methods in phrase grounding, image-text retrieval, object detection, and zero-shot classification, setting a new benchmark for medical image-text alignment.
\end{abstract}

\begin{IEEEkeywords}
Vision and language model, Medical Multimodal, Local alignment
\end{IEEEkeywords}


\section{Introduction}
\IEEEPARstart{M}{edical} image-text pretraining aims to bridge visual data (e.g., X-rays) and textual reports (e.g., radiology findings) \cite{VLPNeurIPS2023,VLPCVPR2024} to enhance the performance of tasks such as disease detection, phrase grounding, and image-text retrieval \cite{wan2024med,chen2024medblip}. The representation of multimodal data can be categorized into global and local levels \cite{icml2024medicalmultimedia, medicml2023}. Global representations provide an overall summary of the entire image or report, whereas local representations focus on specific lesion regions or critical clinical details. Fine-grained local representations are crucial for accurately identifying subtle variations in medical images and interpreting key textual features, which not only improves disease diagnosis accuracy but also facilitates precise lesion localization \cite{yan2023representation, koh2023grounding}.

Although local alignment is of great importance in medical multimodal learning, its implementation faces significant challenges. Traditional manually annotating local regions is time-consuming and costly, relying heavily on the expertise of radiologists, which limits scalability \cite{gao2024training}. Another major issue is the lack of natural local pairing information in medical data \cite{medicml2023}. This makes the effectiveness of local alignment heavily dependent on two key factors: first, the quality of local image regions, especially the accurate localization of lesion regions; and second, the quality of the representation of relationships between words and regions. Furthermore, local alignment in medical applications typically involves soft regions with irregular and ambiguous boundaries. However, existing region recognition methods, such as MPMA \cite{MPMA} and GLIP \cite{GLIP}, primarily rely on hard regions defined by explicit bounding boxes, making them unsuitable for handling the inherent uncertainty in medical images, as shown in Figure \ref{fig:motivation}(a).


\begin{figure}[h]
	\centering	\includegraphics[scale=.8]{motivion2.pdf}
	\caption{(a) Challenges in local alignment, including the absence of word-region annotations and reliance on hard region-based alignment. (b) Radiologists iteratively refine their focus—PLAN emulates this process through soft region alignment.}
	\label{fig:motivation}
\end{figure}




To tackle this challenge, we introduce the Progressive Local Alignment Network (PLAN), which formulates word-region relationship learning as two interconnected subproblems. The first involves \textit{local alignment via contrastive learning}, where relationships between words and pixels are established, allowing word-region associations to be inferred. The second is a \textit{progressive learning strategy}, inspired by radiologists’ diagnostic reasoning, which iteratively refines local alignment relationships, as illustrated in Figure \ref{fig:motivation}(b). By integrating these strategies, PLAN effectively captures fine-grained associations between medical images and text, improving both the interpretability of the model and the quality of the alignment, even under unsupervised conditions.
Our contributions are as follows.
\begin{itemize}
    \item We propose a novel local alignment approach based on contrastive learning  to capture high-quality word-region associations by integrating word importance and spatial relevance, resulting in a more precise and semantically enriched alignment.
    \item Inspired by radiologists' diagnostic reasoning, we propose a progressive learning strategy that iteratively enhances local alignment between medical images and reports.
    \item Extensive experiments on multiple medical datasets demonstrate that PLAN sets a new benchmark in tasks such as phrase grounding and image-text retrieval, achieving state-of-the-art performance.
\end{itemize}



\section{Related Work}
Methods like CLIP \cite{CLIP} excel in learning joint image-text representations through aligning matched pairs and contrasting unmatched pairs, achieving strong performance in coarse-grained alignment. However, CLIP struggles with capturing the detailed region-to-text correspondences needed for phrase grounding and region-specific diagnostic tasks. To address this, recent vision-language pretraining studies have introduced fine-grained alignment mechanisms. For example, PyramidCLIP \cite{PyramidCLIP} uses a multi-level semantic pyramid to optimize both intra- and cross-layer alignment, while FILIP \cite{FILIP} employs token-level contrastive learning through a cross-modal post-interaction mechanism. These advancements underscore the increasing focus on fine-grained alignment in vision-language tasks.

Fine-grained alignment is crucial for linking lesion regions in medical images to corresponding textual descriptions, enhancing clinical decision support. Various methods attempt to bridge global and fine-grained alignment. MPMA \cite{MPMA} combines global contrastive learning with patch-token alignment but struggles with incomplete or noisy alignment due to fixed patch sizes. GLoRIA \cite{GLoRIA} uses token-level alignment with cross-attention but is hindered by irrelevant tokens or regions dominating the process. GLIP \cite{GLIP} treats detection as localization using pseudo-labels from teacher-student models but relies heavily on bounding box annotations, limiting scalability in medical applications. MedCLIP \cite{MedCLIP} aligns at the disease level via semantic similarity matrices but often misses subtle lesion-text relationships. Similarly, BioViL \cite{BioViL} and REFERS \cite{REFERS} leverage global contrastive and region-aware objectives but lack iterative refinement for improving local alignment dynamically.
 
\section{Method}
\begin{figure*}[!htbp]
	\centering
		\includegraphics[scale=.6]{modelf2.pdf}
	\caption{The overall framework of PLAN. 
\( \boldsymbol{R}_t \) denotes the remaining keywords in the medical report at iteration \( t \); \( \mathbf{S}_t \) represents the similarity matrix computed in the current iteration; \( \widetilde{\mathbf{S}}_{t-1} \) retains alignment information from the previous iteration; and \( \widetilde{\mathbf{S}}_{t} \) serves as the updated similarity matrix, integrating prior alignment knowledge with the current iteration.}
	\label{PLAN}
\end{figure*}


\subsection{Preliminary}
We adopt a CLIP-style pretraining paradigm to align medical images with their corresponding textual reports. This approach projects both modalities into a shared latent space, facilitating the capture of rich semantic relationships between them. 
The training dataset is defined as $
\mathcal{D}_{\text{train}} = \{ (\mathbf{I}_n, \mathbf{R}_n) \mid n = 1, 2, \ldots, N \}$,
where $\mathbf{I}_n \in \mathbb{R}^{H \times W \times C}$ represents the $n$-th medical image with height $H$, width $W$, and $C$ channels, while $\mathbf{R}_n \in \mathbb{R}^{L}$ denotes its corresponding textual report containing $L$ words.

To extract informative features from both modalities, we employ an image encoder $E_{\text{image}}$ (e.g., ResNet \cite{resnet50}) and a text encoder $E_{\text{text}}$ (e.g., BioClinical-BERT \cite{BioClinicalBERT}). These encoders generate both global and local feature representations:
\begin{equation}
E_{\text{image}}: \mathbf{I}_n \to (\mathbf{I}_n^g, \mathbf{I}_n^l), \quad E_{\text{text}}: \mathbf{R}_n \to (\mathbf{R}_n^g, \mathbf{R}_n^l),
\end{equation}
where $\mathbf{I}_n^g \in \mathbb{R}^D$ and $\mathbf{R}_n^g \in \mathbb{R}^D$ are global feature vectors that capture high-level semantic representations of the entire image and text, respectively. $\mathbf{I}_n^l \in \mathbb{R}^{HW \times D}$ and $\mathbf{R}_n^l \in \mathbb{R}^{L \times D}$ are local features that encode fine-grained  details. 
The embedding dimension $D$ ensures compatibility between the extracted image and text features in the latent space.

The global alignment mechanism aligns the global features $\mathbf{I}_n^g$ and $\mathbf{R}_n^g$ through a contrastive loss. The loss for aligning images to text is defined as:
\begin{equation}
\mathcal{L}_g^{R \leftarrow I} = - \frac{1}{B} \sum_{n=1}^B \log \frac{\exp (\mathbf{I}_n^g \cdot \mathbf{R}_n^g / \tau_1)}{\sum_{k=1}^B \exp (\mathbf{I}_n^g \cdot \mathbf{R}_k^g / \tau_1)},
\label{eq1}
\end{equation}
where $B$ is the batch size and $\tau_1$ is a temperature parameter that controls the sharpness of the similarity distribution. The dot operator $\cdot$ refers to the inner product of vectors. Similarly, the loss for aligning text to images, $\mathcal{L}_g^{I \leftarrow R}$, is defined symmetrically to $\mathcal{L}_g^{R \leftarrow I}$. The total global alignment loss is the sum of these two components:
\begin{equation}
\mathcal{L}_g = \mathcal{L}_g^{R \leftarrow I} + \mathcal{L}_g^{I \leftarrow R}.
\label{Global_loss}
\end{equation}
In medical image-text analysis, local alignment is essential as traditional global methods fail to meet domain-specific requirements. Local contrastive learning faces challenges as it depends on high-quality local regions and their alignment with text. However, the lack of natural local relationships in medical data hinders this process.

To address these challenges, we propose PLAN, as illustrated in Figure \ref{PLAN}, which comprises two key components: \textbf{\textit{ a local alignment  approach via contrastive learning}} for capturing meaningful word-region relationships, and \textbf{\textit{ a progressive  learning strategy}} that iteratively refines these relationships to enhance alignment precision and robustness.



\subsection{Local Alignment via Contrastive Learning}
\label{sec:process}
Achieving fine-grained local alignment in medical multimodal tasks is inherently challenging due to the absence of natural pairings between textual phrases and local image regions. This unpaired nature makes the quality of local regions and the relationships between regions and words critical to the success of contrastive learning. To address this challenge, we decompose local contrastive learning into two sub-problems: (1) constructing a word-pixel similarity matrix $\mathbf{S}$, which captures relationships between textual words and image pixels, and (2) deriving high-quality local regions and leveraging these regions for robust local contrastive learning. By focusing on these two aspects, PLAN transforms the unpaired local alignment problem into a solvable learning framework.


\textbf{Learning a Word-Pixel Similarity Matrix.} 
A learnable similarity matrix $\mathbf{S}$ is designed to quantify the relationships between text words and image pixels. It is achieved using a learnable transformation $g(\cdot)$, which maps image features into a compatible space for textual comparison:
\begin{equation}
\mathbf{S}_n = \mathbf{R}_n^l (g(\mathbf{I}_n^l))^\top,
\label{S_prior}
\end{equation}
where $\mathbf{R}_n^l \in \mathbb{R}^{L \times D}$ denotes the local textual features of the report, 
and $\mathbf{I}_n^l \in \mathbb{R}^{HW \times D}$ represents the local image features. 
For brevity, we omit the sample index \( n \) in \( \mathbf{S}_n \) and its derived terms in subsequent equations, referring to \( \mathbf{S}_n \) simply as \( \mathbf{S} \).

To improve the reliability of the similarity matrix and establish robust word-pixel relationships, the key challenge is mitigating noise and redundancy while preserving meaningful associations. To address this, we refine the similarity matrix \( \mathbf{S} \) by systematically modeling the importance of words and pixels, integrating their co-dependencies, and refining the learned relationships.

We begin by independently processing the row and column dimensions of \( \mathbf{S} \), ensuring modality-specific feature selection. Row-wise processing identifies key pixel regions relevant to text, while column-wise processing extracts word associations aligned with visual features. To quantify the relative significance of words and pixels, we define the importance vectors \( \boldsymbol{\gamma}_{\text{word}}^{\text{init}} \in \mathbb{R}^{L} \) and \( \boldsymbol{\gamma}_{\text{pix}}^{\text{init}} \in \mathbb{R}^{HW} \), capturing the initial word and pixel importance, respectively:
\begin{equation}
\begin{aligned}
    \boldsymbol{\gamma}_{\text{word}}^{\text{init}}(i) &= \frac{1}{HW} \sum_{i=1}^{HW} \mathbf{S}(i, j) + \alpha \Delta_{\text{word}}(i),\\
    \boldsymbol{\gamma}_{\text{pix}}^{\text{init}}(j) &= \frac{1}{L} \sum_{j=1}^{L} \mathbf{S}(i, j) + \beta\Delta_{\text{pix}}(j),
\end{aligned}
\end{equation}
where \( \Delta_{\text{word}}(i) \) and \( \Delta_{\text{pix}}(j) \) are the standard deviations of similarity scores for words and pixels, respectively. The learnable parameters \( \alpha \) and \( \beta \) regulate the contribution of variance terms, allowing adaptive modality balancing.

Since meaningful relationships emerge from the joint association of words and pixels rather than their isolated analysis, we introduce a co-importance matrix \( \boldsymbol{\Phi} \in \mathbb{R}^{L \times HW} \), where each entry \( \boldsymbol{\Phi}(i, j) \) represents the significance of the association between word \( r \) and pixel \( c \):
\begin{equation}
\boldsymbol{\Phi}(i, j) = \text{softmax}_{i,j} \left(  \boldsymbol{\gamma}_{\text{word}}^{\text{init}}(i) +\boldsymbol{\gamma}_{\text{pix}}^{\text{init}}(j)  \right).
\end{equation}
Using \( \boldsymbol{\Phi} \), the importance values of words and pixels are refined by incorporating their learned associations:
\begin{equation}
\begin{small}
\begin{aligned}
 \boldsymbol{\gamma}_{\text{word}}^{\text{refine}}(i) = \sum_{
 j=1}^{HW} \boldsymbol{\Phi}(i, j)  \mathbf{S}(i, j), \quad
  \boldsymbol{\gamma}_{\text{pix}}^{\text{refine}}(j) = \sum_{i=1}^{L} \boldsymbol{\Phi}(i, j) \mathbf{S}(i, j).  
\end{aligned}  
\end{small}
\label{word_imp}
\end{equation}

To achieve a balanced contribution from both modalities,  we further refine the co-importance matrix:
\begin{equation}
\boldsymbol{\Phi}^{\text{refine}}(i,j) = \text{softmax}_{i,j} \left( 
w_{\text{word}} \boldsymbol{\gamma}_{\text{word}}^{\text{refine}}(i) +
w_{\text{pix}} \boldsymbol{\gamma}_{\text{pix}}^{\text{refine}}(j)  
\right),
\end{equation}
where the weighting factors for words and pixels are  as:
\begin{equation}
w_{\text{word}} = \frac{1}{\mathcal{Z}} \sum_{i=1}^L \boldsymbol{\gamma}_{\text{word}}^{\text{refine}}(i),
\quad
w_{\text{pix}} = \frac{1}{\mathcal{Z}} \sum_{j=1}^{HW} \boldsymbol{\gamma}_{\text{pix}}^{\text{refine}}(j),
\end{equation}
where $\mathcal{Z}$ is the normalization term.
% Here, the normalization term is defined as:
% \begin{equation}
% \mathcal{Z} = \sum_{r=1}^L \boldsymbol{\gamma}_{\text{word}}^{\text{refine}}(r) + \sum_{c=1}^{HW} \boldsymbol{\gamma}_{\text{pix}}^{\text{refine}}(c) +  \varepsilon,
% \end{equation}
% where $\varepsilon$ is a small perturbation term to prevent numerical instability.
This calibration ensures a well-balanced fusion of word and pixel importance, preserving meaningful cross-modal associations while mitigating redundancy.

Leveraging the refined co-importance matrix, the  word-pixel similarity matrix is updated as:
\begin{equation}
\mathbf{S}^{\Phi} = \boldsymbol{\Phi}^{\text{refine}} \circ \mathbf{S},
\end{equation}
where $\circ$ refers to the element-wise product of two matrices.

\textbf{Contrastive Learning for Word-Region Alignment.}
To refine word-pixel alignment, we utilize the similarity matrix \( \mathbf{S}^{\Phi} \) to associate each word with its most relevant image regions, generating fine-grained, semantically enriched regional representations.  
The refined local image features \( \hat{\mathbf{I}}_n^l \in \mathbb{R}^{L \times H \times W \times D} \) are obtained via broadcasted element-wise multiplication between the similarity matrix \( \mathbf{S}^{\Phi} \) and the local image features \( \mathbf{I}_n^l \):
\begin{equation} 
\hat{\mathbf{I}}_n^l = \mathbf{S}^{\Phi} \otimes \mathbf{I}_n^l,  
\end{equation}
where \( \otimes \) denotes broadcasted element-wise multiplication, allowing each word to selectively attend to spatial regions by reweighting the corresponding image features. This ensures structured word-region mapping while preserving spatial coherence.

Since not all words contribute equally to alignment, we refine the word importance vector \( \boldsymbol{\gamma}_{\text{word}}^{\text{refine}}(i) \) in Eq.(\ref{word_imp}) using the refined co-importance matrix \( \boldsymbol{\Phi}^{\text{refine}} \). The updated importance scores guide Gumbel-Softmax sampling to extract the most informative words, forming the keyword representation \( \hat{\mathbf{R}}_n^l \in \mathbb{R}^{L' \times D} \), where \( L' \) is the number of selected critical keywords.

To enhance local alignment, we introduce a contrastive loss that enforces consistency between the selected text features \( \hat{\mathbf{R}}_n^l \) and their corresponding image region features \( \hat{\mathbf{I}}_n^l \). The local alignment loss, which quantifies the alignment between keywords and pixel regions, is formulated as:
\begin{equation}
\begin{aligned}
\mathcal{L}_l^{I \leftarrow R} = 
- \frac{1}{BL'} \sum_{n=1}^B \sum_{i=1}^{L'} \log \frac{\exp (\hat{\mathbf{I}}_{ni}^l \cdot \hat{\mathbf{R}}_{ni}^l / \tau_2)}{\sum_{k =1} ^{L^2} \exp (\hat{\mathbf{I}}_{ni}^l \cdot \mathbf{R}_{nk}^l / \tau_2)},
\end{aligned}
\label{eq15}
\end{equation}
where \( \tau_2 \) is a temperature parameter that controls the sharpness of similarity scores. 

The denominator encompasses all possible combinations of word-region pairs, including both positive and negative samples.
Given that there are $L$ words and each has a corresponding region, the total number of similarity scores in the matrix is $L^2$.
After selecting $L'$ keywords as positive pairs, the remaining $L^2-L'$ pairs contribute to the negative samples.

In Eq.(\ref{eq15}), \( \hat{\mathbf{I}}_{ni}^l \) refers to the local image feature associated with the \( i \)-th word, which is extracted from the corresponding region in \( \hat{\mathbf{I}}_n^l \) (of dimension \( H \times W \times D \)). 
To obtain a compact representation, we apply average pooling over the spatial dimensions, yielding a \( 1 \times 1 \times D \) feature vector, followed by a linear projection to obtain a final \( 1 \times D \) embedding.
The representations \( \mathbf{R}_{nk}^l \) and \( \hat{\mathbf{R}}_{ni}^l \) correspond to the embeddings of the \( k \)-th and \( i \)-th words, extracted from the rows of \( \mathbf{R}_{n}^l \) and \( \hat{\mathbf{R}}_{n}^l \), respectively. 


Similarly, the local alignment loss for associating pixel regions with selected keywords, \( \mathcal{L}_l^{R \leftarrow I} \), is computed in an analogous manner. 
The total local alignment loss is then formulated as:
\begin{equation}
\mathcal{L}_l = \mathcal{L}_l^{R \leftarrow I} + \mathcal{L}_l^{I \leftarrow R}.
\label{local_loss}
\end{equation}


The output of this local alignment process is a refined similarity matrix \( \widetilde{\mathbf{S}} \in \mathbb{R}^{L' \times HW} \), constructed by extracting the rows of \( \mathbf{S} \) that correspond to the selected keyword indices.


\subsection{Progressive Local Alignment}

\begin{figure}[h]
	\centering	\includegraphics[scale=.5]{SubGraph222.pdf}
	\caption{Progressive learning Strategy.}
	\label{fig:progessive}
\end{figure}




We propose a progressive learning strategy, as illustrated in Figure \ref{fig:progessive}, which incrementally refines soft region representations and improves alignment precision for more accurate semantic matching.  
This approach is inspired by the iterative diagnostic process of radiologists, who progressively narrow their focus from broad observations to specific lesion regions.


To model this iterative process, we introduce a time index \( t \) to denote the similarity matrix computed at each iteration, \( \widetilde{\mathbf{S}}_t \), and iteratively refine the alignment using both historical trends and current observations.
The update rule for the similarity matrix at each step \( t \) is defined as:
\begin{equation}\label{eq20}
\widetilde{\mathbf{S}}_{t} = f_{\text{LA}} \left( \delta \cdot \mathbf{S}_{t} + (1 - \delta) \cdot \widetilde{\mathbf{S}}_{t-1} \right),
\end{equation}
where \( \widetilde{\mathbf{S}}_{t-1} \) encodes alignment information from the previous iteration, \( \mathbf{S}_t \) represents the similarity matrix computed in the current iteration based on Eq.(\ref{S_prior})  using the  features ${\mathbf{R}}_i^l$ and ${\mathbf{I}}_i^l$. \( \delta \in (0, 1) \) is a smoothing factor balancing the influence of historical and current information.
\( f_{\text{LA}} \) refers to the process detailed in Section~\ref{sec:process}.

This refinement process is inspired by Bayesian updating, where prior knowledge is iteratively combined with observed evidence to produce a posterior matrix \( \widetilde{\mathbf{S}}_t \). By progressively integrating historical trends and current observations, this method ensures smooth and robust updates across iterations, mimicking the iterative focus-narrowing process employed by radiologists. Empirical results validate its stability and effectiveness in improving alignment quality.


To provide a theoretical foundation for this iterative mechanism, we analyze its convergence properties. 
Specifically, as \( t \to \infty \), the iterative refinement process stabilizes to a similarity matrix \( \mathbf{S}_\infty \), defined as:

\begin{theorem}[Convergence]
\label{thm:convergence}
Let \( f_{\text{LA}}: \mathbb{R}^{L \times HW} \to \mathbb{R}^{L' \times HW} \) be a row-removal operator that projects matrices to a lower-dimensional space by deleting rows. Given a smoothing factor \( \delta \in (0, 1) \) and bounded projected observations \( \|f_{\text{LA}}(\mathbf{S}_t)\|_F \leq M \) (where \( \|\cdot\|_F \) denotes the Frobenius norm), the similarity matrix update rule given by Eq.~(\ref{eq20}) converges to a stable matrix:
\begin{equation}
\mathbf{S}_{\infty} = \sum_{k=0}^\infty \delta (1 - \delta)^k f_{\text{LA}}(\mathbf{S}_{t-k}).
\end{equation}
\end{theorem}



\subsection*{Detailed Proof}
\paragraph{Recursive Expansion and Linearity Preservation}  
Since \( f_{\text{LA}} \) is a row-removal operation, it can be represented as a linear projection matrix \( \mathbf{P} \in \mathbb{R}^{k \times m} \) (where each row contains exactly one "1" and others are "0"). For any matrix \( \mathbf{X} \in \mathbb{R}^{m \times n} \):
\begin{equation}
f_{\text{LA}}(\mathbf{X}) = \mathbf{P} \mathbf{X}.
\end{equation}
Substituting into the update rule:
\begin{equation}
\widetilde{\mathbf{S}}_{t-1} = \delta \mathbf{P} \mathbf{S}_{t-1} + (1-\delta) \mathbf{P} \widetilde{\mathbf{S}}_{t-2}.
\end{equation}
Substituting back into \( \widetilde{\mathbf{S}}_{t} \):
\begin{equation}
\begin{aligned}
\widetilde{\mathbf{S}}_{t} &= \delta \mathbf{P} \mathbf{S}_t + (1-\delta) \left[ \delta \mathbf{P} \mathbf{S}_{t-1} + (1-\delta) \mathbf{P} \widetilde{\mathbf{S}}_{t-2} \right] \\
&= \delta \mathbf{P} \mathbf{S}_t + \delta (1-\delta) \mathbf{P} \mathbf{S}_{t-1} + (1-\delta)^2 \mathbf{P} \widetilde{\mathbf{S}}_{t-2}.
\end{aligned}
\end{equation}
Continuing this expansion to the initial iteration \( t=0 \):
\begin{equation}
\widetilde{\mathbf{S}}_{t} = \sum_{k=0}^{t} \delta (1-\delta)^k \mathbf{P} \mathbf{S}_{t-k} + (1-\delta)^{t+1} \mathbf{P} \widetilde{\mathbf{S}}_{0}.
\end{equation}

\paragraph{Convergence of Projected Series}  
For the infinite series \( \sum_{k=0}^{\infty} \delta (1-\delta)^k \mathbf{P} \mathbf{S}_{t-k} \), the boundedness condition \( \|\mathbf{P} \mathbf{S}_{t}\|_F \leq M \) implies:
\begin{equation}
\begin{aligned}
\Big\| \sum_{k=0}^\infty \delta (1-\delta)^k \mathbf{P} \mathbf{S}_{t-k} \Big\|_F &\leq \sum_{k=0}^\infty \delta (1-\delta)^k \|\mathbf{P} \mathbf{S}_{t-k}\|_F \\&\leq M \sum_{k=0}^\infty \delta (1-\delta)^k.
\end{aligned}
\end{equation}
Evaluating the geometric series:
\begin{equation}
\sum_{k=0}^\infty \delta (1-\delta)^k = \delta \cdot \frac{1}{1 - (1-\delta)} = 1.
\end{equation}
Thus, the series converges absolutely with:
\begin{equation}
\Big\| \sum_{k=0}^\infty \delta (1-\delta)^k \mathbf{P} \mathbf{S}_{t-k} \Big\|_F \leq M.
\end{equation}

\paragraph{Exponential Decay of Initial Term}  
The norm of the initial term decays as:
\begin{equation}
\| (1-\delta)^{t+1} \mathbf{P} \widetilde{\mathbf{S}}_{0} \|_F \leq (1-\delta)^{t+1} \|\mathbf{P}\|_F \|\widetilde{\mathbf{S}}_{0}\|_F.
\end{equation}
Since \( \|\mathbf{P}\|_F = \sqrt{k} \) (Frobenius norm of the projection matrix) and \( 0 < \delta < 1 \), we have \( (1-\delta)^{t+1} \to 0 \) as \( t \to \infty \).

\paragraph{Combined Convergence}  
Combining all results, as \( t \to \infty \):
\begin{equation}
\widetilde{\mathbf{S}}_{t} = \underbrace{\sum_{k=0}^{t} \delta (1-\delta)^k \mathbf{P} \mathbf{S}_{t-k}}_{\text{Converges to } \mathbf{S}_{\infty}} + \underbrace{(1-\delta)^{t+1} \mathbf{P} \widetilde{\mathbf{S}}_{0}}_{\text{Vanishes}} \to \mathbf{S}_{\infty}.
\end{equation}
Hence, the similarity matrix converges to:
\begin{equation}
\mathbf{S}_{\infty} = \sum_{k=0}^\infty \delta (1 - \delta)^k f_{\text{LA}}(\mathbf{S}_{t-k}).
\end{equation}

This completes the proof of Theorem~\ref{thm:convergence}. \(\hfill \square\)

The detailed proof demonstrate that the iterative refinement process reduces dependence on initial conditions over time. The final matrix \( \mathbf{S}_\infty \) reflects accurate and robust associations between textual keywords and pixel regions.


The overall learning objective, \( \mathcal{L} \), is formulated as the average alignment loss over all iterations:
\begin{equation}\label{eq21}
\mathcal{L} = \frac{1}{T} \sum_{t=1}^T \mathcal{L}_{\text{align}}^t,
\end{equation}
where \( T \) is the total number of iterations. The alignment loss at each step is given by:
\begin{equation}
\mathcal{L}_{\text{align}}^t = \mathcal{L}_g^t + \mathcal{L}_l^t,
\end{equation}
where \( \mathcal{L}_g^t \) and \( \mathcal{L}_l^t \) are the global and local alignment losses, respectively, as defined in Eq.~(\ref{Global_loss}) and Eq.~(\ref{local_loss}). Note that for brevity, the iteration index \( t \) was omitted in the previous equations.
Optimizing this objective function enables the model to dynamically focus on lesion regions in medical images and the key semantic elements of diseases in medical reports.









\section{Experiments}



\subsection{Experimental Settings}
\subsubsection{Datasets}

We utilized several datasets to pre-train and evaluate our model. Table \ref{tab:dataset} summarizes the datasets employed in our experiments, along with their respective annotations and applications in various tasks. The datasets cover a range of multimodal alignment and downstream tasks, including phrase grounding, object detection, image-text retrieval, and zero-shot classification. The annotations and task-specific usage of each dataset ensure a comprehensive evaluation of the proposed PLAN model across multiple dimensions.

\begin{table*}[!t]
\renewcommand{\arraystretch}{1.2} % 调整行间距（可选）
\caption{The usage of the dataset employed by the model in downstream tasks.}
\label{tab:dataset}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l l l c c c c}
\hline
Dataset        & Size/Images & Annotations                     & Phrase Grounding & Object Detection & Image-Text Retrieval & Zero-shot Classification \\
\hline
MS-CXR         & 206         & Bounding boxes, radiology phrase & \checkmark       &                  &                      &                          \\
MIMIC-5×200    & 1,000       & radiology text, 5 diseases       &                  &                  & \checkmark           & \checkmark               \\
COVID          & 5,162       & COVID-19/normal labels           &                  &                  &                      & \checkmark               \\
RSNA Pneumonia & 12,024      & Pneumonia/normal labels          &                  & \checkmark       &                      & \checkmark               \\
\hline
\end{tabular}%
}
\end{table*}



\textbf{MIMIC-CXR}: In this study, we pre-trained our model using the publicly available MIMIC-CXR dataset, which includes 377,110 chest X-ray images and 227,835 reports from 65,379 patients. Each report, authored by radiologists, covers sections like Examination, Indication, Findings, and Impression. To focus on key information, we used only the "Findings" and "Impression" sections as input. We employed the preprocessed JPG version of MIMIC-CXR 2.0.0 and divided the dataset into two subsets following the strategy proposed in MedCLIP: one subset with 354,615 images and 201,063 reports for model pre-training, and another subset, MIMIC-5x200, dedicated to model performance evaluation. Further details on MIMIC-5x200 will be provided in subsequent sections.

\textbf{MS-CXR}: The MS-CXR dataset is a valuable resource for evaluating the performance of phrase grounding in deep learning models. The dataset includes image bounding box labels annotated with radiology text descriptions for eight types of chest diseases, which have been verified by boardcertified radiologists. By utilizing the bounding box labels and radiology text descriptions, our model can accurately ground the specific phrases or words in the text to the corresponding regions in the image. The MS-CXR dataset can serve as a benchmark for evaluating the performance of our model in phrase grounding.

\textbf{MIMIC-5x200}: The original dataset contains multi-label images, where each image may represent multiple diseases. This poses a challenge for retrieval tasks, as similarity-based matching between input images and target text can result in category confusion due to overlapping labels. To address this, we followed GLoRIA and sampled a multi-class subset from MIMIC-CXR, named MIMIC-5x200, designed for zero-shot classification and image-text retrieval. This dataset consists of images and sentences describing five disease conditions: atelectasis, cardiomegaly, consolidation, edema, and pleural effusion.

\textbf{RSNA Pneumonia}: The RSNA Pneumonia dataset is a binary classification dataset consisting of chest X-ray images labeled as pneumonia or normal. Each image is annotated with a pneumonia classification label and bounding box labels for detection tasks. To create a balanced image classification subset, we performed random sampling from the dataset. Following the approach used in MedCLIP \cite{MedCLIP}, we split the sampled data into two subsets: one containing 8,486 images for training the classification head and the other containing 3,538 images for testing.

\textbf{COVID}: Chest X-ray images have been used as a supplementary tool for diagnosing COVID-19, particularly in regions with limited diagnostic testing. The COVID dataset provides a valuable resource for developing and evaluating models that use chest X-ray images to detect COVID-19. We created a binary dataset by sampling COVID-19 and normal images from the COVID database. Following the method used in MedCLIP, we split the sampled dataset into two subsets: one with 2,162 images for training the classification head and another with 3,000 images for testing.

\subsubsection{Baselines}
We compared several state-of-the-art medical image-text pretraining methods, including MedCLIP \cite{MedCLIP}, MGCA\cite{MGCA}, MedKLIP\cite{MedKLIP}, CLEFT\cite{CLEFT}, MAVL\cite{MAVL}, PRIOR\cite{PRIOR}, and CARZero\cite{CARZero}. Since MedCLIP and CLEFT were pretrained on CheXpert dataset, we retrained their models on the MIMIC-CXR dataset for fair comparison. 


\subsubsection{Implementation Details}
Our model was pretrained on the MIMIC-CXR dataset for 10 epochs with a batch size of 48 on a single NVIDIA A100 GPU. Parameter updates were performed using the Adam optimizer (weight decay: 1×10-6), with an initial learning rate of 1×10-5, dynamically adjusted based on the ReduceLROnPlateau strategy. During image preprocessing, images were resized to 256×256 and randomly cropped to 224×224 to enhance model robustness.

\subsection{Experimental Results: Multimodal Alignment}

\begin{table*}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{The performance of the Phrase Grounding task is evaluated on the MS-CXR dataset using Contrast-to-Noise Ratio (CNR) scores across eight disease categories.}
\label{table1}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{l|cccccccc}
\hline
Model      & Atelectasis    & Cardiomegaly   & Consolidation  & Lung Opacity   & Edema          & Pneumonia      & Pneumothorax   & Pleural Effusion \\ \hline
MedCLIP    & 0.56$\pm$0.06  & 0.53$\pm$0.03  & 0.58$\pm$0.02  & 0.56$\pm$0.08  & 0.44$\pm$0.06  & 0.62$\pm$0.03  & 0.47$\pm$0.02  & 0.51$\pm$0.08   \\
MGCA       & 0.63$\pm$0.05  & 0.57$\pm$0.02  & 0.65$\pm$0.05  & 0.56$\pm$0.08  & 0.52$\pm$0.06  & 0.83$\pm$0.15  & 0.52$\pm$0.02  & 0.82$\pm$0.13   \\
MedKLIP    & 0.56$\pm$0.02  & 0.53$\pm$0.07  & 0.59$\pm$0.01  & 0.59$\pm$0.11  & 0.49$\pm$0.04  & 0.68$\pm$0.13  & 0.59$\pm$0.01  & 0.64$\pm$0.07   \\
PRIOR      & 0.65$\pm$0.08  & 0.56$\pm$0.01  & 0.59$\pm$0.04  & 0.66$\pm$0.09  & 0.55$\pm$0.03  & 0.67$\pm$0.06  & 0.66$\pm$0.10  & 0.82$\pm$0.09   \\
CLEFT      & 0.69$\pm$0.10  & 0.57$\pm$0.02  & 0.65$\pm$0.02  & 0.57$\pm$0.08  & 0.50$\pm$0.04  & 0.73$\pm$0.10  & 0.60$\pm$0.12  & 0.80$\pm$0.10   \\
MAVL       & 0.58$\pm$0.08  & 0.54$\pm$0.01  & 0.55$\pm$0.05  & 0.64$\pm$0.08  & 0.51$\pm$0.02  & 0.71$\pm$0.09  & 0.60$\pm$0.07  & 0.76$\pm$0.13   \\
CARZero    & 0.68$\pm$0.09  & 0.52$\pm$0.07  & 0.69$\pm$0.09  & 0.56$\pm$0.02  & 0.52$\pm$0.03  & 0.84$\pm$0.12  & 0.63$\pm$0.05  & 0.97$\pm$0.12   \\

\rowcolor[gray]{0.9}PLAN (Our)   & \textbf{0.83}$\pm$0.06 & \textbf{0.61}$\pm$0.01 & \textbf{0.81}$\pm$0.02 & \textbf{0.72}$\pm$0.07 & \textbf{0.73}$\pm$0.08 & \textbf{0.85}$\pm$0.05 & \textbf{0.72}$\pm$0.08 & \textbf{1.02}$\pm$0.10 \\ \hline
\end{tabular}%
}
\end{table*}





\begin{figure*}[!htbp]
	\centering
		\includegraphics[scale=.6]{MS_CXR.pdf}
	\caption{Visualization of attention maps generated by PLAN on MS-CXR dataset. The red boxes indicate the ground truth for disease localization, while the heatmaps highlight regions with higher activation weights with disease-related phrases in the text.}
	\label{PhaseG}
\end{figure*}

\begin{figure*}[!htbp]
	\centering
		\includegraphics[scale=.35]{MS_CXR_G.pdf}
	\caption{Visualization of attention map in PLAN on MS-CXR. The red boxes indicate the corresponding ground truth of grounding. Highlighted pixels represent higher activation weights correlating specific words with regions in the image.}
	\label{Attention}
\end{figure*}





To evaluate PLAN’s alignment capabilities, we performed two experiments: phrase grounding for fine-grained alignment and image-text retrieval for coarse-grained alignment. These tasks demonstrate PLAN's ability to capture both detailed and global semantic correspondences.

\subsubsection{Phrase Grounding}
Phrase grounding associates textual phrases (e.g., disease descriptions or anatomical terms) with corresponding regions in medical images, offering precise diagnostic insights and enhancing model interpretability.
Table \ref{table1} presents the phrase grounding results on the MS-CXR dataset. Using the Contrast-to-Noise Ratio (CNR) \cite{hendrick2008signal} as the evaluation metric, PLAN achieved the highest CNR across eight disease categories, outperforming MGCA and PRIOR. Heatmaps generated with Grad-CAM (Figure \ref{PhaseG}) further illustrate PLAN’s ability to accurately localize lesion sites and align disease-related phrases with image regions. Additional comparisons with baseline methods can be found in Figure \ref{Attention}. These findings highlight PLAN’s superior precision and interpretability in phrase grounding tasks.



\subsubsection{Image-Text Retrieval}
To assess the matching effectiveness between medical images and textual descriptions, we conducted experiments on the MIMIC-$\rm{5\times200}$ dataset. Our evaluation involves selecting a query image and calculating the cosine similarity with candidate sentences using representations of the \texttt{[cls]} token. Precision@K was used to measure the quality of retrieved sentences, where higher precision reflects better alignment between retrieved reports and the query image’s disease category.
Table \ref{Table2} presents the results for image-text retrieval. PRIOR, which incorporates fine-grained local information, achieved strong performance; however, PLAN outperformed all baseline methods, demonstrating the effectiveness of iterative refinement in learning robust alignments.
% \captionsetup[table]{skip=1pt}
\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.2} % 调整表格行间距
\caption{The Image-Text Retrieval task is compared to state-of-the-art methods on the MIMIC-$\rm{5\times200}$ dataset, and Precision scores are reported.}
\label{Table2}
\footnotesize
\begin{tabular}{l|cccc} \hline
Model      & Prec@1 & Prec@2 & Prec@5 & Prec@10 \\ \hline
MedCLIP    & 44.40  & 43.45  & 44.50  & 45.58  \\
MGCA       & 50.06  & 49.77  & 49.05  & 47.53  \\
MedKLIP    & 50.00  & 50.00  & 49.42  & 50.00  \\
PRIOR      & 53.19  & 51.72  & 51.89  & 41.69  \\
CLEFT      & 52.75  & 50.31  & 48.75  & 49.81  \\
MAVL       & 50.00  & 50.62  & 51.06  & 49.97  \\
CARZero    & 50.00  & 50.00  & 51.65  & 47.38  \\

\rowcolor[gray]{0.9}PLAN (Our) & \textbf{55.88} & \textbf{54.08} & \textbf{54.05} & \textbf{50.56} \\ \hline
\end{tabular}
\end{table}



\subsection{Experimental Results: Downstream Tasks} 
Beyond alignment, the performance of PLAN’s image encoder is critical for downstream tasks, where the learned image features are applied to object detection and zero-shot classification. Object detection tests the model’s ability to detect lesion regions and distinguish between disease categories at a granular level. Zero-shot classification, on the other hand, evaluates the generalization capability of the image encoder to classify unseen categories without additional fine-tuning. These tasks provide a holistic assessment of PLAN’s effectiveness in learning robust and transferable visual representations.



\textbf{Object Detection.} The RSNA Pneumonia dataset, containing binary labels for pneumonia and normal cases, was used to evaluate the performance of image-text pretraining models in object detection tasks. We used three metrics: Intersection over Union (IoU), Dice coefficient (Dice), and accuracy (ACC). IoU measures pixel-level overlap between predicted and ground-truth regions, Dice assesses similarity in predicted and ground-truth pixels, and ACC evaluates overall classification accuracy. Higher values across these metrics indicate better model performance \cite{vluasceanu2024selecting}.

\begin{table}[t]
\caption{The Object Detection task is compared to state-of-the-art methods on the RSNA Pneumonia dataset.}
\label{Table:ObjectDetection}
\centering
\renewcommand{\arraystretch}{1.2} % 调整行距
\begin{tabular}{l|ccc}
\hline
Model    & IoU   & Dice   & ACC   \\
\hline
MedCLIP  & 24.96 & 34.67  & 85.42 \\
MGCA     & 35.52 & 48.65  & 86.70 \\
MedKLIP  & 31.39 & 46.05  & 86.14 \\
PRIOR    & 33.06 & 45.86  & 85.44 \\
CLEFT    & 34.90 & 47.87  & 88.15 \\
MAVL     & 35.44 & 51.02  & 88.84 \\
CARZero  & 36.12 & 49.48  & 88.22 \\

\rowcolor[gray]{0.9}PLAN (Our) & \textbf{37.92} & \textbf{51.28} & \textbf{89.63} \\
\hline
\end{tabular}
\end{table}



Table \ref{Table:ObjectDetection} shows the object detection results. PLAN outperformed MedKLIP and MAVL by leveraging iterative optimization of local feature alignment between reports and lesion regions. This iterative refinement led to improved accuracy and detection performance, highlighting the robustness of PLAN in fine-grained object detection tasks.



\begin{table}[!t]
\centering
\renewcommand{\arraystretch}{1.2} % 调整行间距（可选）
\caption{Accuracy scores of the Zero-shot Classification task, evaluated on the MIMIC-$\rm{5\times200}$, COVID, and RSNA Pneumonia datasets, are compared across state-of-the-art methods.}
\label{Table:Classification}
\begin{tabular}{l|ccc}
\hline
Model    & MIMIC-$\rm{5\times200}$ & COVID  & RSNA \\
\hline
MedCLIP  & 0.5250 & 0.7770 & 0.7990 \\
MGCA     & 0.7487 & 0.8687 & 0.7920 \\
MedKLIP  & 0.5194 & 0.8326 & 0.7465 \\
PRIOR    & 0.7683 & 0.8627 & 0.7473 \\
CLEFT    & 0.7547 & 0.8418 & 0.7825 \\
MAVL     & 0.7460 & 0.8306 & 0.7814 \\
CARZero  & 0.7606 & 0.8680 & 0.7855 \\

\rowcolor[gray]{0.9}PLAN(Our) & \textbf{0.7789} & \textbf{0.8820} & \textbf{0.8005} \\
\hline
\end{tabular}
\end{table}





\textbf{Zero-Shot Classification.} To assess the adaptability of the learned image encoder, we evaluated its performance on zero-shot classification tasks, where the model classifies images without additional fine-tuning. A classification head was added to the image encoder and trained using cross-entropy loss, enabling the model to make predictions solely based on visual features.

Table \ref{Table:Classification} presents classification results on MIMIC-$\rm{5\times200}$, COVID, and RSNA Pneumonia datasets. PLAN consistently outperformed all baseline methods, demonstrating strong generalization across diverse datasets. These results highlight PLAN’s ability to effectively capture transferable image representations, further validating its potential as a robust image-text pretraining model for medical applications.

\begin{table*}[!t]
\renewcommand{\arraystretch}{1.3}
\caption{Results of the Phrase Grounding task on the MS-CXR dataset under different loss configurations.}
\label{tab:loss_grounding}
\centering
\begin{tabular}{l | c c c c c c c c}
\hline
   & Atelectasis & Cardiomegaly & Consolidation & Lung Opacity & Edema & Pneumonia & Pneumothorax & Pleural Effusion \\
\hline
$\mathcal{L}_g$    & 0.70\scriptsize{$\pm 0.04$} & 0.57\scriptsize{$\pm 0.03$} & 0.76\scriptsize{$\pm 0.03$} & 0.55\scriptsize{$\pm 0.04$} & 0.68\scriptsize{$\pm 0.04$} & 0.69\scriptsize{$\pm 0.11$} & 0.60\scriptsize{$\pm 0.07$} & 0.76\scriptsize{$\pm 0.06$} \\

$\mathcal{L}_g'$  & 0.64\scriptsize{$\pm 0.04$} & 0.58\scriptsize{$\pm 0.02$} & 0.72\scriptsize{$\pm 0.04$} & 0.62\scriptsize{$\pm 0.12$} & 0.69\scriptsize{$\pm 0.07$} & 0.75\scriptsize{$\pm 0.21$} & 0.67\scriptsize{$\pm 0.05$} & 0.97\scriptsize{$\pm 0.18$} \\

\rowcolor[gray]{0.9}$\mathcal{L}_g + \mathcal{L}_l$ & \textbf{0.83}\scriptsize{$\pm 0.06$} & \textbf{0.61}\scriptsize{$\pm 0.01$} & \textbf{0.81}\scriptsize{$\pm 0.02$} & \textbf{0.72}\scriptsize{$\pm 0.07$} & \textbf{0.73}\scriptsize{$\pm 0.08$} & \textbf{0.85}\scriptsize{$\pm 0.05$} & \textbf{0.72}\scriptsize{$\pm 0.08$} & \textbf{1.02}\scriptsize{$\pm 0.10$} \\
\hline
\end{tabular}
\end{table*}





\begin{figure}[h]
	\centering	\includegraphics[scale=.9]{iter.pdf}
	\caption{Performance of PLAN on the phrase grounding task using the MS-CXR dataset under different iteration numbers $T$.}
	\label{fig:iter}
\end{figure}



\subsection{Ablation Study}

To better understand the performance of PLAN, we conducted a series of ablation experiments focusing on the impact of different loss designs and the number of iterations on key tasks.



\textbf{Impact of Loss Functions.} We evaluated three loss configurations for PLAN on two tasks: image-text retrieval using the MIMIC-$\rm{5 \times 200}$ dataset and object detection using the RSNA Pneumonia dataset. The three loss configurations included: (1) global contrastive loss ($\mathcal{L}_g$), applied only to global image and text embeddings to establish a coarse-grained alignment; (2) global loss combined with a local loss based on only the initial separation of pixel-word pairs ($\mathcal{L}_g'$); and (3) global loss combined with joint local loss ($\mathcal{L}_g + \mathcal{L}_l$), which incorporates iterative joint pixel-word refinement for fine-grained alignment.
Table \ref{tab:loss_grounding} summarizes the results. Models incorporating joint local loss ($\mathcal{L}_g + \mathcal{L}_l$) consistently outperformed those using only global loss or individual local loss across all metrics. 
Specifically, this configuration achieves the best CNR scores in phrase grounding. These results highlight the critical role of iterative joint pixel-word refinement in enhancing both global and local alignments.

% \subsubsection{Impact of Iteration Numbers on Local Alignment}
\textbf{Impact of Iteration Numbers on Local Alignment.} To evaluate the impact of iteration numbers on phrase grounding performance, we conducted experiments with PLAN using $T$ ranging from 1 to 5 iterations on the MS-CXR dataset. 
Figure \ref{fig:iter} reports the results. 
With $T=1$ and $2$, the model struggled to establish precise correspondences between lesion regions and textual descriptions, resulting in lower CNR scores. The performance improved significantly with $T=3$, achieving the highest average CNR values across eight disease categories. However, when $T=4$ and $5$, performance declined slightly. This can be attributed to over-refinement, where fewer critical words were retained, potentially excluding relevant terms such as those indicating severity or location.
Based on these results, $T=3$ was selected as the optimal iteration number for PLAN, as it strikes a balance between retaining critical keywords and maintaining sufficient semantic coverage to enhance localization accuracy.


Additional qualitative results, including further visualization examples of text-to-image alignment and iterative refinement, can be found in Figure \ref{table:cxr-results}. These examples further illustrate the progressive improvement in localizing lesion regions and associating them with key phrases, reinforcing the effectiveness of PLAN’s iterative refinement strategy.

\begin{figure*}[ht]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|>{\centering\arraybackslash}m{1cm}|>{\centering\arraybackslash}m{5cm}|>{\centering\arraybackslash}m{5cm}|>{\centering\arraybackslash}m{4cm}|>{\centering\arraybackslash}m{4cm}|}
\hline
\textbf{ID} & \textbf{Original} &\textbf{First Iteration} &\textbf{Second Iteration}&\textbf{Third iteration}
\\ \hline
\multirow{2}{*}{\centering eg.1}&
  \includegraphics[width=3cm]{image1_1.png} &
  \includegraphics[width=3cm]{image1_2.png} &
  \includegraphics[width=3cm]{image1_3.png} &
  \includegraphics[width=3cm]{image1_4.png} 
\\ \cline{2-5}
  &Increasing bibasilar opacities which could be seen with lower airway inflammation or infection, although developing bronchopneumonia is not entirely excluded. Mild anterior wedge compression deformity of a vertebral body at the thoracolumbar junction, likely L1; although probably chronic, potentially increased somewhat. &
  increasing bibasilar could with lower airway inflammation infection developing bronchopneumonia not entirely excluded mild anterior wedge compression deformity of vertebral body thoracolumbar junction likely probably chronic increased somewhat. &
  increasing bibasilar could lower airway inflammation infection bronchopneumonia not excluded compression deformity vertebral body thoracolumbar junction chronic somewhat. &
  increasing bibasilar lower airway inflammation bronchopneumonia not excluded deformity vertebral thoracolumbar junction chronic.
\\ \hline
\multirow{2}{*}{\centering eg.2}&
  \includegraphics[width=3cm]{image2_1.png} &
  \includegraphics[width=3cm]{image2_2.png} &
  \includegraphics[width=3cm]{image2_3.png} &
  \includegraphics[width=3cm]{image2_4.png} 
\\ \cline{2-5}
  &There is a right-sided pleural effusion that blunts the right-sided lateral pleural sinus but extends into the posterior pleural spaces, occupying the area posterior to the stent prosthesis along the right posterior chest wall. The amount of pleural effusion has increased in comparison with the preceding AP single view chest examination of. The left-sided hemithorax demonstrates unchanged findings with regard to pulmonary vasculature and absence of any new acute infiltrates. &
  a right-sided pleural effusion blunts right-sided lateral pleural extends posterior pleural spaces posterior the stent prosthesis right posterior chest wall amount pleural effusion increased in comparison single view chest examination left-sided hemithorax demonstrates unchanged regard pulmonary vasculature absence any acute infiltrates. &
  right-sided pleural effusion blunts lateral pleural extends posterior spaces posterior prosthesis wall pleural effusion increased single view chest left-sided hemithorax demonstrates unchanged pulmonary vasculature absence acute infiltrates. &
  right-sided pleural effusion blunts pleural extends posterior spaces prosthesis wall pleural effusion increased view chest left-sided hemithorax unchanged pulmonary vasculature absence infiltrates.
\\ \hline
\multirow{2}{*}{\centering eg.3}&
  \includegraphics[width=3cm]{image3_1.png} &
  \includegraphics[width=3cm]{image3_2.png} &
  \includegraphics[width=3cm]{image3_3.png} &
  \includegraphics[width=3cm]{image3_4.png} 
\\ \cline{2-5}
  &Two right basilar chest tubes remain in place. There is a stable small right apical pneumothorax. Contiguous patchy airspace disease at the right base is also seen and stable. There is a small residual right pleural effusion vs. pleural thickening, unchanged. Left lung is clear. No evidence of pulmonary edema. Heart remains enlarged. Mediastinal contours are unchanged. &
  two right basilar chest tubes remain there stable small right apical pneumothorax contiguous patchy airspace disease right also seen stable a small residual right pleural effusion Pleural thickening unchanged left lung clear no evidence pulmonary edema heart remains enlarged mediastinal contours are unchanged. &
  two chest tubes remain small right apical pneumothorax patchy airspace disease right stable residual right pleural effusion pleural thickening unchanged left lung clear no evidence pulmonary edema heart enlarged mediastinal contours unchanged. &
  two tubes right apical pneumothorax patchy disease pleural effusion pleural thickening unchanged left lung clear no edema heart enlarged mediastinal contours unchanged.
\\ \hline
\end{tabular}%
}
\label{table:cxr-results}
    \caption{Iteration Effect Analysis. Visualization of text-to-image alignment and its refinement over multiple iterations on the MS-CXR dataset. Each example demonstrates the progressive improvement in localizing lesion regions and associating them with key phrases as iterations increase. The alignment becomes more accurate and precise, reflecting the effectiveness of PLAN’s iterative refinement strategy.}
\label{table:cxr-results}
\end{figure*}




\section{Conclusion}
Achieving precise local alignment between medical images and text is critical for effective interpretation, yet existing approaches are hindered by rigid region constraints and the absence of natural local pairings. In response, we introduce PLAN, which decouples the learning of word-pixel relationships from word-region associations while integrating a progressive contrastive learning mechanism. This framework enhances the adaptability of soft region recognition, particularly for irregular anatomical structures, and effectively mitigates noise. By iteratively refining alignment with contextual cues, PLAN captures more accurate and interpretable word-region correspondences. Extensive evaluations across multiple datasets confirm its superiority, consistently outperforming state-of-the-art methods in phrase grounding, image-text retrieval, and zero-shot classification, reinforcing its effectiveness in fine-grained medical image-text alignment.


\bibliographystyle{IEEEtran}
\bibliography{refs}


\end{document}