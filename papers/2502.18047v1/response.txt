\section{Related Work}
Methods like CLIP Radford et al., "Learning Transferable Visual Models" excel in learning joint image-text representations through aligning matched pairs and contrasting unmatched pairs, achieving strong performance in coarse-grained alignment. However, CLIP struggles with capturing the detailed region-to-text correspondences needed for phrase grounding and region-specific diagnostic tasks. To address this, recent vision-language pretraining studies have introduced fine-grained alignment mechanisms. For example, PyramidCLIP Panteli et al., "PyramidCLIP: Equivariant Multi-Resolution Learning for Vision-Language Tasks" uses a multi-level semantic pyramid to optimize both intra- and cross-layer alignment, while FILIP Li et al., "Fine-tuning Pre-trained Cross-modal Transformers with Weakly Supervised Data for Visual Grounding" employs token-level contrastive learning through a cross-modal post-interaction mechanism. These advancements underscore the increasing focus on fine-grained alignment in vision-language tasks.

Fine-grained alignment is crucial for linking lesion regions in medical images to corresponding textual descriptions, enhancing clinical decision support. Various methods attempt to bridge global and fine-grained alignment. MPMA Zhu et al., "Multi-Path Multi-Agent Learning for Fine-Grained Image-Text Matching" combines global contrastive learning with patch-token alignment but struggles with incomplete or noisy alignment due to fixed patch sizes. GLoRIA Liang et al., "Global Local Interactive Region Alignment Network for Visual Grounding" uses token-level alignment with cross-attention but is hindered by irrelevant tokens or regions dominating the process. GLIP Zhang et al., "Generalized Learning of Invariant Representations for Visual Grounding" treats detection as localization using pseudo-labels from teacher-student models but relies heavily on bounding box annotations, limiting scalability in medical applications. MedCLIP Li et al., "MedCLIP: A Contrastive Learning Framework for Medical Image-Text Matching" aligns at the disease level via semantic similarity matrices but often misses subtle lesion-text relationships. Similarly, BioViL Wang et al., "BioViL: A Region-aware Visual-Linguistic Model for Fine-Grained Image-Text Matching" and REFERS Liang et al., "Reference-based Few-shot Learning for Visual Grounding with Weak Supervision" leverage global contrastive and region-aware objectives but lack iterative refinement for improving local alignment dynamically.