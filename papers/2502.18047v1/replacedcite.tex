\section{Related Work}
Methods like CLIP ____ excel in learning joint image-text representations through aligning matched pairs and contrasting unmatched pairs, achieving strong performance in coarse-grained alignment. However, CLIP struggles with capturing the detailed region-to-text correspondences needed for phrase grounding and region-specific diagnostic tasks. To address this, recent vision-language pretraining studies have introduced fine-grained alignment mechanisms. For example, PyramidCLIP ____ uses a multi-level semantic pyramid to optimize both intra- and cross-layer alignment, while FILIP ____ employs token-level contrastive learning through a cross-modal post-interaction mechanism. These advancements underscore the increasing focus on fine-grained alignment in vision-language tasks.

Fine-grained alignment is crucial for linking lesion regions in medical images to corresponding textual descriptions, enhancing clinical decision support. Various methods attempt to bridge global and fine-grained alignment. MPMA ____ combines global contrastive learning with patch-token alignment but struggles with incomplete or noisy alignment due to fixed patch sizes. GLoRIA ____ uses token-level alignment with cross-attention but is hindered by irrelevant tokens or regions dominating the process. GLIP ____ treats detection as localization using pseudo-labels from teacher-student models but relies heavily on bounding box annotations, limiting scalability in medical applications. MedCLIP ____ aligns at the disease level via semantic similarity matrices but often misses subtle lesion-text relationships. Similarly, BioViL ____ and REFERS ____ leverage global contrastive and region-aware objectives but lack iterative refinement for improving local alignment dynamically.