\section{Related Work}
Methods like CLIP \cite{CLIP} excel in learning joint image-text representations through aligning matched pairs and contrasting unmatched pairs, achieving strong performance in coarse-grained alignment. However, CLIP struggles with capturing the detailed region-to-text correspondences needed for phrase grounding and region-specific diagnostic tasks. To address this, recent vision-language pretraining studies have introduced fine-grained alignment mechanisms. For example, PyramidCLIP \cite{PyramidCLIP} uses a multi-level semantic pyramid to optimize both intra- and cross-layer alignment, while FILIP \cite{FILIP} employs token-level contrastive learning through a cross-modal post-interaction mechanism. These advancements underscore the increasing focus on fine-grained alignment in vision-language tasks.

Fine-grained alignment is crucial for linking lesion regions in medical images to corresponding textual descriptions, enhancing clinical decision support. Various methods attempt to bridge global and fine-grained alignment. MPMA \cite{MPMA} combines global contrastive learning with patch-token alignment but struggles with incomplete or noisy alignment due to fixed patch sizes. GLoRIA \cite{GLoRIA} uses token-level alignment with cross-attention but is hindered by irrelevant tokens or regions dominating the process. GLIP \cite{GLIP} treats detection as localization using pseudo-labels from teacher-student models but relies heavily on bounding box annotations, limiting scalability in medical applications. MedCLIP \cite{MedCLIP} aligns at the disease level via semantic similarity matrices but often misses subtle lesion-text relationships. Similarly, BioViL \cite{BioViL} and REFERS \cite{REFERS} leverage global contrastive and region-aware objectives but lack iterative refinement for improving local alignment dynamically.