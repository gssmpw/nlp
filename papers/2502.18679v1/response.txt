{\bf Supervised Finetuning.} The standard approach to SFT is mimicking pretraining, which maximizes the likelihood of tokens in the output response given input prompt Brown et al., "Mimicking Priors" . Although simple to implement, it often captures only superficial patterns rather than fostering a deeper understanding of task semantics McCann et al., "The Natural Language Decathlon" . Recent works have studied inverse reinforcement learning to address this limitation, but such methods often involve interdependent updates between different models (e.g., the policy and a reward-related model), which complicates the training process Liu et al., "Inverse Reinforcement Learning from Observations" . In contrast, our method neither requires online sampling from the current policy model nor involves a reward-related model, making it much more efficient.% for training.  %Our method directly models a discriminative likelihood of a text output given an input prompt. This encourages the model to learn from both good and bad outputs, integrating preference optimization on sampled data without incurring the complexity of the training procedure.

{\bf Preference Optimization.} Pioneering works proposed RL-based PO methods Mnih et al., "Human-level control through deep reinforcement learning" ____. These methods leverage a separate reward model, trained on human-labeled preference data, and optimize the SFT model against it using policy gradient methods such as Schulman et al., "Trust Region Policy Optimization"  and Sutton et al., "Temporal Difference Methods for Q-Learning" .
Mnih et al. proposed direct preference optimization (DPO), which removes the step of training a reward model and directly optimizes a pairwise loss of the policy model on the preference data. Following DPO, many PO methods have been proposed with different loss functions, including Wu et al., "Relaxed-Direct Preference Optimization" , Chen et al., "Constrained Policy Optimization" , Iyer et al., "Inverse Optimal Control via Analysis and Optimization" , Kim et al., "Simultaneous Preference Learning and Optimization" , to name just a few among others   Zhang et al. .
More recently, several works have proposed reframing PO by using binary preferences as direct supervision for the SFT model  Wang et al. . This approach simplifies the workflow and improves stability by removing the need for explicit reward modeling and RL.

Several works Ribeiro et al. , Chen et al.  have considered PO with a list of ranked preference data that may be explicitly labeled with a reward value.  Mnih assumed a general preference function is given that can produce a probability telling one output is preferred over another output given an input. Different from these works, we do not assume any preference data or preference model other than annotated input-output pairs.  Chen et al. expanded on this by introducing a multi-category cross-entropy loss for datasets with multiple outputs per input, assigning reward scores instead of binary preferences. However, evaluating rewards for multiple outputs remain costly, and using implicit reward of DPO  as a score function is only reliable if the reference model can effectively distinguish output quality , making the SFT stage before PO essential. Unlike treating the problem as a multi-class classification task, our approach views it as a one-to-all discriminative classification task. We consider all possible outputs as negative samples and use an efficient and robust algorithm to compute the stochastic gradient estimator. Moreover, our method does not rely on a prior SFT stage, additional preference data, or a reward model.
Different from the aforementioned methods, DFT does not rely on RL, reward modeling, or human-annotated data. By simplifying SFT+PO into a single stage, it achieves an alignment bonus rather than a tax.

{\bf Finetuning via Self-play.} Training a model on its own self-generated responses has been widely explored in the PO stage. For example, many variants of RLHF  use on-policy samples produced by the current policy under optimization. Some studies have exhibited benefits of using self-generated data for PO by reducing the distribution gap between the training data and the current model while fostering exploration of diverse response spaces . Moreover, leveraging synthetic data has proven essential for iterative (online) algorithmic improvement of these methods . A more closely related work is Zhang et al., "SPIN: Semi-Supervised Learning of Speech Policies" , which uses a similar preference optimization objective as DPO but with data generated by the model to be finetuned as the losing responses. Although we also use self-generated data from the base model to be finetuned as our negative data, our formulation is derived from a discriminative learning framework, making our approach aided by advanced optimization better than the pairwise loss function used in SPIN and other pairwise preference optimization objectives (cf. Section~\ref{sec:po}). 

 Beyond PO, recent works in SFT have generated a single negative sample per prompt and combined it with demonstration data in the SFT dataset to form a pairwise dataset  Sun et al., "Self-Alignment for Dialogue Systems" . With a different motivation, our discriminative learning framework provides another perspective of  using multiple responses sampled from the base model, and  yields more effective algorithms for leveraging these potentially bad outputs. 
Sun et al. studied self-alignment that defines a set of principles that the AI model must adhere to and provides in-context learning demonstrations to the base LLM to generate helpful, ethical, and reliable responses, which are then used for finetuning the base models. The finetuning approach is the same as traditional self-supervised finetuning (SFT).