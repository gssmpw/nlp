%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{tcolorbox}
% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{fancybox}

% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
% \usepackage{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
\usepackage[preprint]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}

% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}

\newcommand{\ilgee}[1]{\textcolor{blue}{[Ilgee: #1]}}
% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Discriminative Finetuning of Generative Language Models without Reward Models and Preference Data}



%%%%%%%%%%%%%%%%%%%% Added package%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{makecell}
\usepackage{algorithm,algorithmic}
\usepackage{paralist,amsmath, amssymb,bm}
\usepackage{multirow}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{cor}{Corollary}
\newtheorem{asm}{Assumption}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{fancybox}
\usepackage{mathtools}
\usepackage{xcolor,colortbl}
\usepackage{subcaption}

%%%% mathcal
\def \S {\mathbf{S}}
\def \L {\mathcal{L}}
\def \A {\mathcal{A}}
\def \X {\mathcal{X}}
\def \O {\mathcal{O}}
\def \Y {\mathcal{Y}}
\def \Ab {\bar{\A}}
\def \R {\mathbb{R}}
\def \Kt {\widetilde{K}}
\def \k {\mathbf{k}}
\def \w {\mathbf{w}}
\def \v {\mathbf{v}}
\def \t {\mathbf{t}}
\def \x {\mathbf{x}}
\def \Se {\mathcal{S}}
\def \E {\mathrm{E}}
\def \Rh {\widehat{R}}
\def \x {\mathbf{x}}
\def \p {\mathbf{p}}
\def \a {\mathbf{a}}
\def \diag {\mbox{diag}}
\def \b {\mathbf{b}}
\def \e {\mathbf{e}}
\def \ba {\boldsymbol{\alpha}}
\def \c {\mathbf{c}}
\def \tr {\mbox{tr}}
\def \d {\mathbf{d}}
\def \db {\bar\mathbf{d}}
\def \1 {\mathbf{1}}

\def \z {\mathbf{z}}
\def \s {\mathbf{s}}
\def \bh {\widehat{\b}}
\def \y {\mathbf{y}}
\def \u {\mathbf{u}}
\def \uh {\widehat{\u}}
\def \H {\mathcal{H}}
\def \g {\mathbf{g}}
\def \F {\mathcal{F}}
\def \I {\mathbb{I}}
\def \P {\mathcal{P}}
\def \Q {\mathcal{Q}}
\def \xh {\widehat{\x}}
\def \wh {\widehat{\w}}
\def \lambdah {\widehat{\lambda}}

\def \ah {\widehat{\a}}
\def \Rc {\mathcal R}
\def \Sigmah {\widehat\Sigma}

\def \Bh {\widehat B}
\def \Ah {\widehat A}
\def \Uh {\widehat U}
\def \Ut {\widetilde U}
\def \B {\mathcalB}
\def \C {\mathbf C}
\def \U {\mathbf U}
\def \Kh {\widehat K}
\def \fh {\widehat f}
\def \yh {\widehat\y}
\def \Xh {\widehat{X}}
\def \Fh {\widehat{F}}

\def \m {\mathbf{m}}
\def \y {\mathbf{y}}
\def \E {\mathrm{E}}
\def \x {\mathbf{x}}
\def \g {\nabla{g}}
\def \D {\mathcal{D}}
\def \z {\mathbf{z}}
\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \Z {\mathcal{Z}}
\def \Pc {\mathcal{P}}
\def \w {\mathbf{w}}
\def \s {\mathbf{s}}
\def \r {\mathbf{r}}
\def \R {\mathbb{R}}
\def \S {\mathcal{S}}
\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \Q {\mathcal{Q}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \q {\mathbf{q}}
\def \v {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}

\def \I {\mathbb{I}}
\def \xt {\widetilde{\x}}
\def \yt {\widetilde{\y}}
\def \hrho {\hat{\rho}}

\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
\def \B {\mathcal{B}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \G {\mathcal {G}}
\def \bh {\widehat{b}}
\def \wh {\widehat{\w}}
\def \Dth {\widehat{\Delta}}
\def \vb {\bar{\mathbf v}}
\def \zt {\widetilde{\z}}
\def \zh {\widehat{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\widehat{\g}}
\def \vh {\widehat{\v}}
\def \Sh {\widehat{S}}
\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{L}}
\def \t {\mathbf{t}}
\def \xh {\widehat{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\m}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \X {\mathcal{X}}
\def \nb {\widehat{\nabla}}
\def \K {\mathcal{K}}
\def \P {\mathbb{P}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
\def \Rt {\mathcal{R}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \zu {\underline{\z}}
\def \vect {\text{vec}}
\def \E {\mathbb{E}}
\def \bftau {\boldsymbol{\tau}}
\def\Tau{{\rm T}}
\def\bw{\mathbf{w}}
\def \start {\textsc{Start}}

% \usepackage{setspace}
% \AtBeginDocument{%
%   \addtolength\abovedisplayskip{-0.3\baselineskip}%
%   \addtolength\belowdisplayskip{-0.3\baselineskip}%
%   \addtolength\abovedisplayshortskip{-0.3\baselineskip}%
%   \addtolength\belowdisplayshortskip{-0.3\baselineskip}%
% }
% %\looseness-3
\usepackage{titlesec}
\titlespacing*{\section}{0pt}{-0.1\baselineskip}{-0.15\baselineskip}
\titlespacing*{\subsection}{0pt}{-0.1\baselineskip}{-0.15\baselineskip}
\titlespacing*{\subsubsection}{0pt}{-0.1\baselineskip}{-0.15\baselineskip}

\newcommand{\pp}{p(\y | \x)}

\usepackage{comment}
\usepackage{pifont}

\usepackage{changepage}

\begin{document}

\twocolumn[
\icmltitle{Discriminative Finetuning of Generative Large Language Models \\without  Reward Models and Preference Data}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Siqi Guo}{tamu}
\icmlauthor{Ilgee Hong}{gatech}
\icmlauthor{Vicente Balmaseda}{tamu}
% \icmlauthor{Changlong Yu}{amazon}
% \icmlauthor{Liang Qiu}{amazon}
% \icmlauthor{Xin Liu}{amazon}
% \icmlauthor{Haoming Jiang}{amazon}
\icmlauthor{Tuo Zhao}{gatech}
\icmlauthor{Tianbao Yang}{tamu}
\end{icmlauthorlist}

\icmlaffiliation{tamu}{Department of Computer Science and Engineering, Texas A\&M University, College Station, USA}
\icmlaffiliation{gatech}{H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology, Atlanta, USA}
% \icmlaffiliation{amazon}{Amazon.com Inc, CA, USA}

\icmlcorrespondingauthor{Tianbao Yang}{tianbao-yang@tamu.edu}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.


\setlength{\textfloatsep}{2pt}% Remove \textfloatsep
\setlength\abovedisplayskip{4pt}
\setlength\belowdisplayskip{4pt}

\begin{abstract}
Supervised fine-tuning (SFT) followed by preference optimization (PO) denoted by SFT\textrightarrow PO has become the standard for improving pretrained large language models (LLMs), with PO demonstrating significant performance gains. However, PO methods rely on either human-labeled preference data or a strong reward model to generate preference data. \textit{Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT\textrightarrow  PO?}
We address this question by introducing Discriminative Fine-Tuning (DFT), a novel approach that eliminates the need for preference data. Unlike SFT, which employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that that increases the probability of positive answers while suppressing potentially negative ones, shifting from token prediction to data prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT\textrightarrow PO. The code can be found at \href{https://github.com/PenGuln/DFT/}{\texttt{https://github.com/PenGuln/DFT}}.

%Supervised finetuning (SFT) followed by preference optimization (PO)  has become a norm for improving a pretrained large language model (LLM). Extensive studies have demonstrated that PO can greatly improve the performance of a finetuned LLM by SFT. Hence, tremendous studies have proposed better methods for PO, yielding a variety of PO methods. However, PO-based approaches require either  human-labeled preference data or a strong reward model to generate preference data.  \textit{Can we finetune LLMs without using preference data and reward model to achieve competitive performance as SFT + PO?  }

%We answer this question by proposing a discriminative finetuning (DFT) approach of LLMs. The fundamental reason that PO is needed after SFT is that the current SFT approach does not model the negative data because it is a generative approach rather a discriminative approach. In traditional classification problems, discriminative approaches dominate generative approaches by pushing up the score of positive classes and pushing down the score of negative classes. We generalize this notion to finetuning LLMs based on supervised data so as to  push up the score of positive answer and push down the score of negative answers. Our contributions are three-fold: (i)  We propose a discriminative probabilistic approach for learning a generative model by explicitly model the discriminative likelihood of an answer out of all text outputs given a text input. (ii) We propose efficient algorithms to maximize the discriminative log-likelihood of true answers of inputs. (iii) We conduct extensive experiments to verify the effectiveness of our approach comparing with SFT and SFT+PO approaches. 

%  In this draft, we describe a new approach for finetuning generative large language models. State-of-the-art approaches for finetuning LLMs use the same loss function as pretraining, which is the negative log-likelihood of output texts given the input prompts defined by a generative model. A question remains {\it how to train the generative model with a discriminative approach?} The benefit of using a discriminative approach is that it directly model the negative data in the objective, which not only pushes the likelihood of positive data to to be large but also pushes the likelihood of negative data to be small. It could bring us several benefits including (i) faster convergence than maximizing the likelihood of positive data only; (ii) mitigates the burden or completely removes the process of collecting human preference data for alignment. 
\end{abstract}

\section{Introduction}
Fine-tuning large language models (LLMs) has become an essential step to adapt pretrained models to specific tasks, significantly improving their performance and practical utility \citep{wei2022finetuned, ouyang2022training, touvron2023llama, openai2024o1, guo2025deepseek}. While pretraining enables LLMs to acquire vast amounts of general knowledge, fine-tuning tailors the model to exhibit desirable behaviors, %align with user preferences, 
and excel in specialized domains. As LLMs become integral to applications like conversational AI, content generation, and decision-making, developing effective and efficient fine-tuning methods remains a critical challenge.

The current standard for fine-tuning LLMs typically involves supervised fine-tuning (SFT) followed by preference optimization (PO), including techniques such as reinforcement learning with human feedback (RLHF) \citep{learn2020summ,ouyang2022training, bai2022training, rafailov2023direct}. In this approach, SFT first aligns the model with supervised data, and PO further refines the model using preference data labeled by humans or a reward model, which simulates human preferences. This two-stage process has achieved significant success, particularly in improving human alignment and response quality. However, PO methods often require extensive human annotations or the construction of robust reward models, both of which are resource-intensive and may limit scalability and applicability in highly specialized areas. 
%\begin{shadowbox}
%\begin{minipage}{0.9\linewidth}
\\This raises an intriguing question: \textit{Can we fine-tune LLMs without preference data or reward models while achieving competitive performance to SFT\textrightarrow  PO?}\\
%\end{minipage}
%\end{shadowbox}
To address this question, we propose \underline{\textbf{D}}iscriminative \underline{\textbf{F}}ine-\underline{\textbf{T}}uning (DFT), a novel alternative to SFT\textrightarrow PO that eliminates the need for preference data and reward models. Unlike SFT, which uses a generative approach and overlooks negative examples, DFT adopts a discriminative notion, explicitly discriminating good from ``bad'' outputs generated by the base model to be finetuned. We formalize this approach by introducing a discriminative probabilistic framework that models the {\bf discriminative likelihood} of an answer among all possible outputs for a given input. This stands in stark contrast to SFT, which uses a generative probabilistic framework to model only the generative likelihood of individual tokens in the answers. To implement this framework, we develop efficient algorithms to optimize the discriminative likelihood of good answers, ensuring both scalability and practicality. Due to its strong discriminative capability, DFT delivers competitive or superior performance compared to SFT\textrightarrow PO, while eliminating the need for preference data and reward models. 

Our main contributions are summarized as follows:
\begin{itemize}
%\vspace*{-0.15in}
\item {\bf A novel discriminative framework:} We introduce a probabilistic framework that explicitly models the discriminative likelihood of an answer among all possible outputs, in contrast to the generative likelihood approach used in SFT.
%\vspace*{-0.05in}
\item {\bf Efficient optimization algorithms:} We propose scalable and practical methods to maximize the discriminative likelihood of good answers, ensuring effective fine-tuning of LLMs.
%\vspace*{-0.05in}
\item {\bf Extensive empirical validation:} We conduct extensive experiments to demonstrate that DFT consistently outperforms standard SFT  and achieves comparable results than preference optimization methods that rely on explicit preference datasets.
%\item Resource efficiency: By eliminating the need for preference data and reward models, DFT provides a more resource-efficient and accessible alternative for fine-tuning large language models.
\end{itemize}
%\vspace*{-0.1in}
These contributions establish DFT as a new paradigm for enhancing pretrained language models, offering both theoretical and practical advancements in the field.

%Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from natural language understanding to complex reasoning. To adapt these models for specific applications or to improve their performance on particular tasks, supervised fine-tuning (SFT) has emerged as the predominant approach. In the standard SFT framework, models are fine-tuned by maximizing the likelihood of desired outputs (positive examples) given corresponding input prompts. While this approach has proven effective, it suffers from a fundamental limitation: it focuses solely on increasing the probability of desired responses without explicitly considering what makes other responses undesirable.

%This one-sided optimization in SFT has led to the development of preference optimization methods, most notably Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO). By leveraging the preference dataset, these approaches explicitly model both winning and losing responses. However, preference optimization methods typically require collecting extensive preference datasets, where annotators or reward models must compare and rank different model outputs. This data collection process is both resource-intensive and time-consuming. Moreover, the requirement for explicit preference annotations means these methods cannot directly leverage the wealth of existing SFT datasets.

%To address these limitations, we introduce Discriminative Fine-Tuning (DFT), a novel approach that combines the strengths of generative and discriminative learning paradigms. Unlike traditional SFT which only maximizes the likelihood of positive examples, DFT explicitly incorporates both positive and negative examples in its training objective. The key insight behind DFT is to treat the fine-tuning process as a discriminative task where the model must learn to distinguish between desired and undesired outputs while maintaining its generative capabilities.

%In this paper, we show that DFT provides several key advantages:
%\begin{itemize}
%    \item Better final performance across various tasks without requiring additional annotated data.
%    \item Naturally align with preferences.
%    \item Minimal length exploitation.
%\end{itemize}
% Our theoretical analysis demonstrates that DFT's objective function has strong connections to both preference optimization and contrastive learning, providing a unified framework for understanding how discriminative signals can enhance generative language models. 


\section{Related Work}
{\bf Supervised Finetuning.} The standard approach to SFT is mimicking pretraining, which maximizes the likelihood of tokens in the output response given input prompt \citep{ouyang2022training, wei2022finetuned,xu2023wizardlm,wang2023self,zhang2023instruction,li2024self}. Although simple to implement, it often captures only superficial patterns rather than fostering a deeper understanding of task semantics \citep{kung2023models, zhang2023instruction, gudibande2024the}. Recent works have studied inverse reinforcement learning to address this limitation, but such methods often involve interdependent updates between different models (e.g., the policy and a reward-related model), which complicates the training process \cite{li2024getting, wulfmeier2024imitating}. In contrast, our method neither requires online sampling from the current policy model nor involves a reward-related model, making it much more efficient.% for training.  %Our method directly models a discriminative likelihood of a text output given an input prompt. This encourages the model to learn from both good and bad outputs, integrating preference optimization on sampled data without incurring the complexity of the training procedure.

{\bf Preference Optimization.} Pioneering works proposed RL-based PO methods \cite{christiano2017deep,ziegler2019fine,learn2020summ,ouyang2022training,bai2022training}. These methods leverage a separate reward model, trained on human-labeled preference data, and optimize the SFT model against it using policy gradient methods such as PPO \cite{schulman2017proximal} and REINFORCE \cite{williams1992simple}.
\citet{rafailov2023direct} proposed direct preference optimization (DPO), which removes the step of training a reward model and directly optimizes a pairwise loss of the policy model on the preference data. Following DPO, many PO methods have been proposed with different loss functions, including R-DPO~\cite{Park2024DisentanglingLF}, CPO~\cite{xu2024contrastive}, IPO~\cite{ipo_2022}, SimPO~\cite{meng2024simpo}, KTO~\cite{ethayarajhmodel}, to name just a few among others~\cite{zhao2023calibrating,jung2024binaryclassifieroptimizationlarge}.  
%More recently, several works have proposed reframing PO by using binary preferences as direct supervision for the SFT model \cite{zhao2023calibrating, rafailov2023direct, ipo_2022, ethayarajhmodel, meng2024simpo}. This approach simplifies the workflow and improves stability by removing the need for explicit reward modeling and RL.  

Several works~\cite{chen2024noise,yuan2023rrhf,10.1609/aaai.v38i17.29865} have considered PO with a list of ranked preference data that may be explicitly labeled with a reward value.  \citet{rosset2024direct} assumed a general preference function is given that can produce a probability telling one output is preferred over another output given an input. Different from these works, we do not assume any preference data or preference model other than annotated input-output pairs. %\citet{chen2024noise} expanded on this by introducing a multi-category cross-entropy loss for datasets with multiple outputs per input, assigning reward scores instead of binary preferences. However, evaluating rewards for multiple outputs remain costly, and using implicit reward of DPO \cite{rafailov2023direct} as a score function is only reliable if the reference model can effectively distinguish output quality \cite{chen2024preference, DBLP:journals/corr/abs-2407-13709}, making the SFT stage before PO essential. Unlike treating the problem as a multi-class classification task, our approach views it as a one-to-all discriminative classification task. We consider all possible outputs as negative samples and use an efficient and robust algorithm to compute the stochastic gradient estimator. Moreover, our method does not rely on a prior SFT stage, additional preference data, or a reward model.
%Different from the aforementioned methods, DFT does not rely on RL, reward modeling, or human-annotated data. By simplifying SFT+PO into a single stage, it achieves an alignment bonus rather than a tax.

{\bf Finetuning via Self-play.} Training a model on its own self-generated responses has been widely explored in the PO stage. For example, many variants of RLHF \citep{christiano2017deep,ziegler2019fine,learn2020summ,ouyang2022training, bai2022training, li2023remax, chan2024dense, ji2024self} use on-policy samples produced by the current policy under optimization. Some studies have exhibited benefits of using self-generated data for PO by reducing the distribution gap between the training data and the current model while fostering exploration of diverse response spaces \citep{xu2024is,tajwar2024preference, tang2024understanding}. Moreover, leveraging synthetic data has proven essential for iterative (online) algorithmic improvement of these methods \citep{xu2023some, guo2024direct, yuanself, chen2024bootstrapping, dong2024rlhf}. A more closely related work is SPIN~\citep{wu2024self}, which uses a similar preference optimization objective as DPO but with data generated by the model to be finetuned as the losing responses. Although we also use self-generated data from the base model to be finetuned as our negative data, our formulation is derived from a discriminative learning framework, making our approach aided by advanced optimization better than the pairwise loss function used in SPIN and other pairwise preference optimization objectives (cf. Section~\ref{sec:po}). 

 %Beyond PO, recent works in SFT have generated a single negative sample per prompt and combined it with demonstration data in the SFT dataset to form a pairwise dataset \citep{chen2024self, li2024getting}. With a different motivation, our discriminative learning framework provides another perspective of  using multiple responses sampled from the base model, and  yields more effective algorithms for leveraging these potentially bad outputs. 
%Sun et al. studied self-alignment that defines a set of principles that the AI model must adhere to and provides in-context learning demonstrations to the base LLM to generate helpful, ethical, and reliable responses, which are then used for finetuning the base models. The finetuning approach is the same as traditional self-supervised finetuning (SFT).




\section{Preliminaries: SFT}
%We first discuss  SFT for finetuning a pretrained LLM. 
For SFT, we are given a set of data $\D=\{(\x_i, \y_i), i=1, \ldots, n\}$, where $\x_i$ is an input prompt and $\y_i$ is a labeled output answer.  Both the input $\x$ and output $\y$ are expressed as a sequence of tokens from a vocabulary of size $K$ denoted by $\mathcal V=\{v_1, \ldots, v_K\}$. We let $\x=(x_{[1]}, \ldots, x_{[k]})$ %denote the sequence of tokens of the input $\x$ and let
and $\y= (y_{[1]}, \ldots, y_{[m']})$, % denote the sequence of tokens of the output $\y$, 
where $x_{[i]}\in\mathcal V, y_{[j]}\in\mathcal V, \forall i, j$. %=1,\ldots, m, j=1, \ldots, m'$.  

%The current approach of SFT is mimicking the pretraining method of LLMs.  In particular, LLMs are pretrained by considering the ``next-token prediction" task, i.e., maximizing the log-likelihood of tokens in input data.  In particular, the probability of $\x$ is modeled by $P_g(\x) = \prod_{j=1}^{m}p_g(x_j|x_{1}, \ldots, x_{j-1})$, where the conditional probability of $p_g(x_j|x_1,\ldots, x_{j-1})$ is modeled by a Transformer:  
%\begin{align*}
%p_g& (x_j|x_1,\ldots, x_{j-1}) \\
%& = \frac{\exp(h(\w; x_1, \ldots, x_{j-1})^{\top}W_{x_j})}{\sum_{k=1}^K\exp(h(\w; x_1, \ldots, x_{j-1})^{\top}W_{k})},
%\end{align*}
%where $W_1,\ldots, W_K$ denotes the token embedding vectors of that in $\mathcal V$, $h(\w;  x_1, \ldots, x_{j-1})$ denotes the representation of the input sequence of tokens produced by a transformer network parameterized by $\w$. We abuse the notation $W_{x_j}$ to denote the embedding vector of the token $x_j$.  We let  $\theta=(\w, W)$ to denote all parameters of the LLM. %are learned by minimizing the negative log-likelihood over a set of data. % $\D=\{\x_1, \ldots, \x_n\}$ with $\x_i = (t^i_1, \ldots, t^i_{m_i})$:
%\begin{align}\label{eq:llm}
%\hspace*{-0.1in}\min_{\theta}-\frac{1}{n}\sum_{i=1}^n[\log p_g(\x_i):=\sum_{j=1}^{m_i}\log p_g(t^i_{j}|t^i_{1}, \ldots, t^i_{j-1})].
%\end{align}
%To the best of our knowledge, the temperature-scaled softmax loss has not been used in training LLMs. We will introduce our approach in next section. 

SFT considers the next-token prediction in the output $\y$ given an input $\x$. For an input-output pair $(\x, \y)$, it models the conditional probability of $\y$ given $\x$ by $P_g(\y|\x)= \prod_{j=1}^{m'} p_g(y_j|\x, y_1, \ldots, y_{j-1})$. The token conditional probability $p_g(x_j|x_1,\ldots, x_{j-1})$ is modeled by a Transformer: %\citep{NIPS2017_3f5ee243}:  
\begin{align*}
p_g& (x_j|x_1,\ldots, x_{j-1}) \\
& = \frac{\exp(h(\w; x_1, \ldots, x_{j-1})^{\top}W_{x_j})}{\sum_{k=1}^K\exp(h(\w; x_1, \ldots, x_{j-1})^{\top}W_{k})},
\end{align*}
where $W_1,\ldots, W_K$ denotes the token embedding vectors of that in $\mathcal V$, $h(\w;  x_1, \ldots, x_{j-1})$ denotes the representation of the input sequence of tokens produced by a transformer network parameterized by $\w$. %We abuse the notation $W_{x_j}$ to denote the embedding vector of the token $x_j$. 
We let  $\theta=(\w, W)$ to denote all parameters of the LLM. 



By minimizing the negative log-likelihood  of all $\y_1, \ldots, \y_n$, SFT solves the following problem from a pretrained model: 
\begin{align}\label{eqn:sft}
\min_{\theta} -\frac{1}{n}\sum_{i=1}^n \log P_g(\y_i|\x_i).
\end{align}
In order to differentiate from our approach, we refer to $P_g(\y|\x)$ as the {\bf generative likelihood}, as it decomposes the likelihood of generating $\y$ given $\x$ into the product of likelihood of generating each token in $\y$. 

\section{DFT: Discriminative Finetuning}
In order to motivate our approach, let us first examine the limitation of SFT. Our goal of finetuning LLMs is to ensure that LLMs generate good answers more likely than bad answers. However, SFT only has one-sided optimization power by maximizing the likelihood of tokens in the good output $\y$ given $\x$ and their preceding tokens. It does not necessarily guarantee that the likelihood of tokens in the bad answer is low. Let us consider a simple example: 
%\vspace*{-0.05in}
\begin{tcolorbox}[colback=gray!5!white,colframe=gray!75!black,title=Motivation Example]
($\x$) \text{What is the bigger number between 9.11 and 9.9?} \\
($\y$) \text{The bigger number between 9.11 and 9.9 is 9.9.}\\
($\y'$) \text{The bigger number between 9.11 and 9.9 is 9.11.} 
\end{tcolorbox}
%\vspace*{-0.05in}
The good answer $\y$ and the bad answer $\y'$ only differ in the last token. The likelihood of all preceding tokens are the same. Even though the likelihood of the last token ``9'' in $\y$ conditioned on preceding tokens is increased during the finetuning with this data, the likelihood of the token ``11'' as the last one might still be high, making generating the bad answer $\y'$ likely. 

To address this issue, the current mainstream approach is to finetune the model further using  PO on preference data. If humans label the two answers such that $\y\succ \y'$, the model might be able to push the likelihood of $\y'$ given $\x$ smaller than that of $\y$ given $\x$. As a result, the good answer $\y$ will be generated more likely than the bad answer $\y'$.

However, traditional supervised learning methods never use human preference data. For example, in image classification, training data $(\x, y)$ denote an input image and its true class label $y\in\{1,\ldots, K\}$. We do not need the preference optimization step on preference data saying that a dog class is preferred to a cat class for an image of a dog.  So what is the difference between traditional supervised learning and supervised finetuning of LLMs that makes SFT not enough? The answer lies in the fact that {\it traditional supervised learning methods are usually {\bf discriminative approaches}, while the standard SFT method is not discriminative.} 


Below, we introduce our discriminative finetuning (DFT) framework of LLMs. A discriminative approach aims to push the ``score" of the true output to be higher than that of other possibly wrong outputs. In this paper, we examine a classical approach through {\bf discriminative probabilistic model. }
%In this section, we present a discriminative approach. The idea originates from ``predicting an instance given another instance of our unified statistical X-risk framework". In particular, we model the following probability $\Pr(\y|\x)$. Instead of using a generative approach to define this probability, we use a discriminative approach. 
To this end, we introduce a parameterized scoring function $s_{\theta}(\y, \x)\in\R$, which measures the fitness of $\y$ given $\x$. This is similar to the prediction score in traditional supervised learning. We will discuss shortly how to set the scoring function for learning an LLM. 
In a  {discriminative probabilistic model}, we model the conditional probability $P_d(\y|\x)$ of one output $\y$ out of the space of all possible texts denoted by $\Y$.  In particular, we define  
\begin{align}\label{eq:prob}
P_d(\y|\x) = \frac{\exp(s_{\theta}(\y, \x)/\tau)}{\sum_{\y'\in\Y}\exp(s_{\theta}(\y', \x)/\tau)}, \forall \y\in\Y,
\end{align}
where $\tau>0$ is a temperature hyperparameter. Then, given a set of training data $\D=\{(\x_1, \y_1), \ldots, (\x_n, \y_n)\}$, we learn $\theta$ by maximizing the log-likelihood of observed data, i.e., 
\begin{align}\label{eq:dftv1}
&\min\nolimits_{\theta}\quad F(\theta)\\
%\vspace*{-0.15in}
&\text{where}\quad F(\theta) : = -\frac{1}{n}\sum_{i=1}^n\tau \log P_d(\y_i|\x_i) = \notag\\
& -\frac{1}{n}\sum_{i=1}^n s_{\theta}(\y_i, \x_i) + \frac{\tau }{n}\sum_{i=1}^n\log \bigg[\sum_{\y'\in\Y}\exp(\frac{s_{\theta}(\y', \x_i)}{\tau})\bigg],\notag
\end{align}
where scaling the negative log-likelihood by $\tau$ is for increasing the numerical stability, which does not change the optimal solution.  

DFT marks a paradigm shift from “token” prediction to “data” prediction. To differentiate from the generative likelihood $P_g(\y|\x)$, we refer to $P_d(\y|\x)$ in~(\ref{eq:prob}) as {\bf discriminative likelihood} of $\y$ given $\x$.  By maximizing the discriminative log-likelihood of the training data, we not only increase the score of the true output \(\mathbf{y}_i\) for each input \(\mathbf{x}_i\), corresponding to the numerator of the discriminative likelihood, but also decrease the scores of other potentially bad answers in \(\mathcal{Y}\), which correspond to the denominator of the discriminative likelihood. 

%Below, we will discuss the choices of the scoring function and how to solve the optimization problem. 

\subsection{The Scoring Function}
The above framework might remind one of the contrastive representation learning~\cite{radford2021learning,wang2025on}. However, we cannot directly borrow the same idea of contrastive representation learning to design the scoring function. In particular,  contrastive representation learning uses an encoder network $e(\x)$ to induce an embedding of any input text $\x$, and computes the scoring function by using the cosine similarity between $e(\x)$ and $e(\y)$. However, this representation model $e(\cdot)$ is of no use for generative tasks of LLMs. %One may have to use a sampling approach (e.g., MCMC) based on the discriminative likelihood to generate data, which would be very expensive. 


To circumvent this issue, we define the scoring function based on the generative log-likelihood $\log P_g(\y|\x)$, as it measures the likeliness of generating $\y$ given $\x$. For a good model, we expect that a high value of the generative log-likelihood $\log P_g(\y|\x)$ would indicate a high fitness score of $\y$ to answer $\x$. With such correspondence, the above discriminative learning framework would increase the chance of generating a good output $\y$ given $\x$ and decrease the chance of generating possibly bad outputs given $\x$. We will examine two simple settings of the scoring function. 

\noindent{\bf Setting 1:} $s_{\theta}(\y, \x) = \log P_g(\y|\x)$. Plugging this into~(\ref{eq:dftv1}) results in the following  objective: 
\begin{align}\label{eqn:dft1}
&\min_{\theta}  -\frac{1}{n}\sum_{i=1}^n \log P_g(\y_i|\x_i) \nonumber\\ & + \tau \frac{1}{n}\sum_{i=1}^n\log \bigg(\sum\nolimits_{\y'\in\Y}\exp\bigg(\frac{\log P_g(\y'|\x_i)}{\tau}\bigg)\bigg).
\end{align}
Comparing the above objective of DFT to that of SFT in~(\ref{eqn:sft}),  we can see that the first term in~(\ref{eqn:dft1}) is exactly the same as the objective of SFT. The difference lies in the second term that penalizes the possibly bad outputs in $\Y$ for each $\x_i$, trying to decrease their generative log-likelihood.

\noindent{\bf Setting 2:} For the second setting, we use length normalized generative log-likelihood as the scoring function, e.g.,  $s_{\theta}(\y, \x) =\frac{1}{|\y|} \log P_g(\y|\x)$, where $|\y|$ denotes the number of tokens in $\y$. This will allow us to compare DFT with some PO approaches using length normalized reward~\cite{meng2024simpo}. As a result, the problem becomes: 
\begin{align}\label{eqn:dft1n}
\min_{\theta} & -\frac{1}{n}\sum_{i=1}^n \frac{1}{|\y_i|}\log P_g(\y_i|\x_i) \nonumber\\ & + \tau \frac{1}{n}\sum_{i=1}^n\log \bigg(\sum_{\y'\in\Y}\exp\bigg(\frac{\log P_g(\y'|\x_i)}{|\y'|\tau}\bigg)\bigg).
\end{align}

\subsection{The Optimization Algorithm}
Although our DFT formulation~(\ref{eq:prob}) and~(\ref{eq:dftv1}) is nearly identical to that of the traditional discriminative probabilistic approach for classification (e.g., logistic regression), the key challenge lies in solving the optimization problem in~(\ref{eq:dftv1}), particularly in handling the second term of \(F(\theta)\), where \(\mathcal{Y}\) encompasses all possible texts. We address this challenge by advanced optimization techniques. In particular, we cast the second term into an instance of finite-sum coupled compositional optimization framework (FCCO)~\cite{wang2022finite}. The idea is to write the second term of \(F(\theta)\) into the form of $\frac{1}{n}\sum_{i=1}^nf(\E_{\zeta}g_i(\theta; 
\zeta))$, where $\zeta$ is some random variable.   To this end, we introduce a {\bf sampling distribution $P_i(\cdot)$}, which is specified later. Then we define 
\begin{align*}
 g_i(\theta): &= \sum_{\y'\in\Y}\exp(\frac{s_{\theta}(\y', \x_i)}{\tau}) % \tau\log \bigg(\sum_{\y\in\Y}p^0_g(\y|\x)\frac{p_g(\y|\x_i)^{1/\tau}}{p^0_g(\y|\x)}\bigg) =
 = \E_{\y'\sim P_i(\cdot)}\frac{\exp(\frac{s_{\theta}(\y', \x_i)}{\tau})}{P_i(\y')}.
\end{align*}
The objective becomes: 
\begin{align}\label{eq:dftv1_final}
\min_{\theta} & -\frac{1}{n}\sum_{i=1}^n s_{\theta}(\y_i, \x_i) \nonumber\\ & + \frac{1}{n}\sum_{i=1}^n  \tau \log \bigg(\E_{\y'\sim P_i(\cdot)}\frac{\exp(s_{\theta}(\y', \x_i)/\tau)}{P_i(\y')}\bigg).
\end{align}
Next, we discuss three components of our algorithm for solving the above problem. 

{\bf Sampling Distributions.} We need three properties of these sampling distributions: (1) it is easy to sample data from them; (2) it is possible to compute the probability value of a sample $\y'$; (3) the sampled outputs $\y'\sim P_i(\cdot)$ are likely to be bad outputs in answering $\x_i$. To this end, we let $P_i(\cdot) = P^0_g(\cdot|\bar\x_i)$, where $P^0_g$ corresponds to the base LLM $\theta_0$ to be finetuned, and $\bar\x_i$ is an augmented text of $\x_i$ including some system prompts to facilitate the generation of bad outputs. We explore this in our experiments. %induce potentially bad answers. In our experiments, we simply add an adversarial prompt like ``You are an unhelpful assistant." to the input $\x_i$.

{\bf Key Updates.} Computing a stochastic gradient estimator for the first term is the same as in SFT. %The difference is how to compute a stochastic gradient estimator for the second term. 
The challenge is how to estimate the gradient of  $\tau\log(g_i(\theta_t))$ in the second term using random samples. Its gradient is given by $\frac{\tau}{g_i(\theta_t)}\nabla g_i(\theta_t)$. Although $\nabla g_i(\theta_t)$ can be simply estimated using an unbiased stochastic gradient, estimating $\frac{\tau}{g_i(\theta_t)}$ cannot simply use an unbiased stochastic estimator of $g_i(\theta_t)$ since it is a non-linear function $g_i(\theta_t)$, which will yield a biased estimator.  Following~\citet{wang2022finite}, we maintain and update $n$ moving average estimators $\{u_1, \ldots, u_n\}$ to track  $g_i(\theta)$ for each $\x_i$. In particular, at the $t$-th iteration given a solution $\theta_t$, we first sample a mini-batch of data $\S_t\subset \{\x_1,\dotsc,\x_n\}$. For each data $\x_i\in\S_t$, we sample one or multiple outputs $\y'\sim P^0_g(\cdot|\bar\x_i)$, e.g., by generating them through feeding $\bar\x_i$ as the input prompt to the base LLM $P^0_g$. We denote these outputs as $\B_{i,t}^0 = \{\y'_{i,t,1}, \ldots, \y'_{i, t, B}\}$. Then we update $u_{i,t+1}$ by:
\begin{align}\label{eqn:u}
u_{i,t+1} = (1-\gamma) u_{i, t} +  \gamma \frac{1}{B}\sum_{\y'\in\B_{i,t}^0}\frac{\exp(\frac{s_{\theta_t}(\y', \x_i)}{\tau})}{P^0_g(\y'|\bar\x_i)},
\end{align}
where $\gamma\in(0,1)$.  With $u_{i,t}$, the gradient of $\tau\log(g_i(\theta_t))$ can be estimated by $\frac{\tau}{u_{i,t+1}}\nabla \widehat g_i(\theta_t)$, where \[
\nabla \widehat g_i(\theta_t) = \frac{1}{B}\sum_{\y'\in\B_{i,t}^0}\frac{\exp(\frac{s_{\theta_t}(\y', \x_i)}{\tau})\nabla s_{\theta_t}(\y', \x_i)}{\tau P^0_g(\y'|\bar\x_i)}
\]
denotes a mini-batch estimator of $\nabla g_i(\theta_t)$. We emphasize that the moving average estimator~(\ref{eqn:u}) is critical to calculating an accurate gradient estimator of the objective. In our experiments, we show $\gamma=1$ (i.e., simply using a mini-batch estimator of $g_i(\theta_t)$) will yield much worse performance. 

Thus, we compute an estimator of the gradient $\nabla F(\theta_t)$ by: 
\begin{align}\label{eqn:G}
&G_t  = - \frac{1}{|\S_t|}\sum_{\x_i\in\S_t} \nabla s_{\theta_t}(\y_i, \x_i)+ \nonumber\\
&\hspace{-0.15in} \frac{1}{|\S_t|}\sum_{\x_i\in\S_t}\frac{1}{u_{i,t+1}B}\sum_{\y'\in\B_{i,t}^0}\frac{\exp(\frac{s_{\theta_{t}}(\y', \x_i)}{\tau})\nabla s_{\theta_t}(\y', \x_i)}{P^0_g(\y'|\bar\x_i)}.
\end{align}
Finally, we can update the model parameter $\theta_{t+1}$ following the momentum-based methods (e.g., Adam, Adam-W). This method has a provable convergence guarantee for solving~(\ref{eq:dftv1}) following~\citet{wang2022finite}. Our optimization method is summarized in Algorithm~\ref{alg:DFT}. 

%\vspace{-0.05in}
\begin{algorithm}[t]
\caption{The DFT Algorithm}
\label{alg:DFT}
\begin{algorithmic}[1] 
\STATE Initialize $\theta_0$ as the base LLM, and $\u_0=\mathbf 1$
\FOR{$t=0,1,\dotsc,T-1$}
\STATE Sample a mini-batch $\S_t\subset \{\x_1,\dotsc,\x_n\}$
\FOR{each $\x_i\in\S_t$} 
\STATE Sample a mini-batch $\B^0_{i,t}$ from $P_g^0(\cdot|\bar\x_i)$ via an offline pool
\STATE Update $u_{i,t+1}$ according to~(\ref{eqn:u})
\ENDFOR
\STATE Compute a gradient estimator $G_t$ according to~(\ref{eqn:G})  
\STATE Update $\theta_{t+1}$ using Adam-W
\ENDFOR
\end{algorithmic}
\end{algorithm}
%%\vspace{-0.1in}

\textbf{Efficient Implementation.} There are several implementation issues of Algorithm~\ref{alg:DFT} that are worth discussing. 

The first issue is the Step 5, which sample outputs from the sampling model $P^0_g(\cdot|\bar\x_i)$. This could increase the training time if it is done online. However, since the sampling model is fixed, we can generate these data offline for all $\x_i, i=1,\ldots, n$. This could dramatically reduce our training time. In our experiments, we generate $m=E \times B$ number of outputs for each data $\x_i$ from the sampling model and sample $B$ outputs from this pool in Step 5 without replacement, where $E$ is the number of epochs. 

Another issue is the numerical stability when calculating the stochastic gradient estimator in Step 8 (c.f. (\ref{eqn:G})). Take $s_\theta(\y, \x) = \log P_g(\y|\x)$ as an example. Then, $\exp(s_{\theta_t}(\y', \x_i)/\tau) = P_g(\y'|\x)^{1/\tau}$, which could be a very small value as we are trying to decrease the generative likelihood of generated outputs $\y'$. As a result, the value of estimators $u_{i,t}$ can be extremely small, e.g., $10^{-x}$ where $x$ is a large number, causing some numerical issues.  This issue is tackled by maintaining and updating $\{\log u_1, \ldots, \log u_n\}$ instead of $\{u_1, \ldots, u_n\}$.  Specifically, we denote by $w_{i, t, \y'}=\frac{\exp(\frac{s_{\theta_t}(\y', \x_i)}{\tau})}{P^0_g(\y'|\bar\x_i)}$. Then  (\ref{eqn:u}) can be reformulated to:
\begin{align*}
    % u_{i,t+1} = & (1-\gamma)u_{i,t} + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i, \y'} \\ 
    \exp(\log u_{i,t+1}) = \exp(\log (1 - \gamma) + \log u_{i,t}) \\ +  \exp(\log \gamma +  \log \frac{1}{B}\sum\nolimits_{\y'\in\B_{i,t}^0} w_{i,t,\y'}). 
\end{align*}% where we save $\b_t = \log \u_t$ instead of saving $\u_t$ directly.
For simplicity, let $b_{i,t} = \log (1 - \gamma) + \log u_{i,t}$ and $w_{i,t} = \log \gamma +  \log \frac{1}{B}\sum_{\y'\in\B_{i,t}^0} w_{i,t, \y'}$, we have
\begin{align}
    \exp(\log u_{i,t+1}) = & \exp(b_{i,t}) + \exp(w_{i,t}) \nonumber %\\ 
                     %   = & \exp(b_{i,t}) + \exp(w_{i,t} - b_{i,t} + b_{i,t}) \nonumber \\
                    %    = & \exp(b_{i,t})(1 + \exp(w_{i,t} - b_{i,t})).
\end{align}
If $w_{i,t}<b_{i,t}$, we let 
\begin{align*}
    \exp(\log u_{i,t+1}) %= & \exp(b_{i,t}) + \exp(w_{i,t}) \nonumber \\ 
 = & \exp(b_{i,t})(1 + \exp(w_{i,t} - b_{i,t}));
\end{align*}
otherwise, we let  %But $\exp(w_{i,t} - b_{i,t})$ can be still large if $w_{i,t} > b_{i,t}$. Thus, we update $\log u_{i,t+1}$ by (\ref{eq:merge_exp1}) only if $w_{i,t} < b_{i,t}$, otherwise, $\log u_{i,t+1}$ is updated by another equivalent formula:
\begin{align*}
    \exp(\log u_{i,t+1}) = \exp(w_{i,t})(1 + \exp(b_{i,t} - w_{i,t})). 
\end{align*}
Combining these two cases, we have the following: 
\begin{align}\label{eq:merge_exp}
    &\exp(\log u_{i,t+1}) \\
    & = \exp(\max\{b_{i,t}, w_{i,t}\})(1 + \exp(-|b_{i,t} - w_{i,t}|))\notag\\
    & = \exp(\max\{b_{i,t}, w_{i,t}\})\sigma^{-1}(|b_{i,t} - w_{i,t}|)\notag,
\end{align}
where $\sigma(\cdot)$ denotes the sigmoid function. Taking the log on both sides gives the update for $\log u_{i,t+1}$. To summarize, we maintain and update $\bar u_{i,t} = \log u_{i,t}$ as following: 
\begin{equation}\label{eqn:ub}
\begin{aligned}
    b_{i,t} & = \log (1 - \gamma) + \bar u_{i,t}\\
    w_{i,t}  & = \log \gamma +  \log \frac{1}{B}\sum\nolimits_{\y'\in\B_{i,t}^0} w_{i,t, \y'}\\
    \bar u_{i, t+1} &  = \max\{b_{i,t}, w_{i,t}\} - \log \sigma(|b_{i,t} - w_{i,t}|). 
\end{aligned}
\end{equation}
Then $G_t$ is calculated using $\exp(\bar u_{i,t+1})$ in place of $u_{i, t+1}$. 
% \begin{algorithm}[ht]
% \caption{DFT}
% \label{alg:LLM2}
% \begin{algorithmic}[1] 
% \STATE Initialize $\w, \u$
% \FOR{$t=0,1,\dotsc,T-1$}
% \STATE Sample a batch $\S_t\subset \{1,\dotsc,n\}$, $|\S_t| = S$ 
% \FOR{each $\x_i\in\S_t$} 
% \STATE Sample size-$B$ mini-batches $\B_t^{(i)}$ from $p^0(\y|\x_i)$
% \STATE For each $\y \in \B_t^{(i)}$, compute $d_{\x_i,\y}=\frac{1}{\tau} \log p^t_g(\y|\x_i) - \log p^0(\y | \x_i)$,\\$w_{\x_i,\y} = \exp(d_{\x_i,\y} - \max_{\y} \{d_{\x_i,\y}\})$
% \STATE Compute $w_t' = \log \gamma + \log(\frac{1}{B}\sum_{\y\in\B_t^{(i)}}w_{\x_i,\y}) + \max_{\y} \{d_{\x_i,\y}\}$, $b_t' = \log(1 - \gamma) + b_t^{(i)}$
% \STATE Update $b_{t+1}^{(i)} = \max\{b_t', w_t'\} - \log \sigma(|b_t' - w_t'|)$
% \ENDFOR
% \STATE Compute $L_t =\frac{1}{S}\sum_{i\in\S_t}\left( -\log p_g^t(\y_i | \x_i)  + \frac{1}{B}\sum_{\y\in\B_t^{(i)}} \exp(d_{\x_i,\y}-b_{t+1}^{(i)}) \log p_g^t(\y | \x_i) \right)$ 
% \STATE Compute $G_t = \nabla L_t$
% \STATE Update $\w_{t+1}$ using Adam-W
% \ENDFOR
% \end{algorithmic}
% \end{algorithm}
\section{DFT2: An Approximation Approach}

Compared with SFT, DFT  has an extra cost of computing $P_g^0(\y'|\bar\x_i)$ at each forward step during training. We can further reduce this cost by using an approximation. The idea is simple by just using the length-normalized generative log-likelihood as the scoring function  $s_\theta(\y,\x)=\frac{1}{|\y|}\log P_g(\y|\x)$ and dropping $P^0_g(\y'|\bar\x_i)$ in the update of $u_{i,t+1}$ and the gradient estimator $G_t$. Below, we explain this approximation from two perspectives. 


We first explain the approximation via approximating $\sum_{\y'\in\Y}\exp(s_\theta(\y', \x))$ by using the data generated by the base LLM model. Let   $\S^0_i=\{\y'_{i,1}, \ldots, \y'_{i,m}\}$ denote a set of outputs sampled for each data $\x_i$ following the base model $P^0_g(\cdot|\bar\x_i)$. Considering that the base LLM has already been trained significantly on a large corpus, hence $\exp(s_{\theta}(\y', \x)/\tau)$ for $\y'\sim P_g^0(\cdot|\bar\x_i)$ would be much larger than a random $\y'$  in $\Y$. This is verified by Figure~\ref{fig:s_distrib}. Hence, we approximate $\sum_{\y'\in\Y}\exp(s_\theta(\y', \x)/\tau)\approx \sum_{\y'\in\S^0_i}\exp(s_\theta(\y', \x)/\tau)$.  The second explanation is drawn from an observation made in~\citet{meng2024simpo}. They observed that the samples from a LLM have roughly the same values of $s_\theta(\y',\x)=\frac{1}{|\y'|}\log P_g(\y'|\x)$. Hence,  we can approximate $g_i(\theta)$ by 
\begin{align*}
g_i(\theta)& \approx \frac{1}{m}\sum_{\y'\in\S^0_i} \frac{\exp(\frac{s_{\theta}(\y', \x)}{\tau})}{P^0_g(\y'|\bar\x_i)} \propto \frac{1}{m}\sum_{\y'\in\S^0_i} \exp(\frac{s_{\theta}(\y', \x)}{\tau}),
\end{align*}
where the second step is justified by that $\exp(s_{\theta}(\y', \x)/\tau)$ are approximately the same, hence the weighting by $1/{P^0_g(\y'|\bar\x_i)}$ becomes insignificant. 


\begin{algorithm}[t]
\caption{The DFT2 Algorithm}
\label{alg:DFT2}
\begin{algorithmic}[1] 
\STATE Initialize $\theta_0$ as the base LLM, and $\u_0=\mathbf 1$
\FOR{$t=0,1,\dotsc,T-1$}
\STATE Sample a mini-batch batch $\S_t\subset \{\x_1,\dotsc,\x_n\}$
\FOR{each $\x_i\in\S_t$} 
\STATE Sample a mini-batch $\B^0_{i,t}$ from $\S^0_{i}$
\STATE Update $\bar u_{i,t+1}$ according to~(\ref{eqn:u2}) and (\ref{eqn:ub})
\ENDFOR
\STATE Compute a gradient estimator $G_t$ according to~(\ref{eqn:G2})  
\STATE Update $\theta_{t+1}$ using Adam-W
\ENDFOR
\end{algorithmic}
\end{algorithm}






With either approximation, we end up with the following optimization problem: 
\begin{align}\label{eq:dftv2}
&\min_{\theta}  -\frac{1}{n}\sum_{i=1}^n s_\theta(\y_i,\x_i) \nonumber\\ & + \frac{1}{n}\sum_{i=1}^n  \tau \log \bigg(\frac{1}{m}\sum\nolimits_{\y'\in\S^0_i}\exp(s_{\theta}(\y', \x)/\tau)\bigg).
\end{align}
We solve the above problem using the same optimization technique, except for the change on  $u_{i,t+1}$ and $G_t$: 
\begin{align}
&u_{i,t+1} = (1-\gamma) u_{i, t} + \frac{ \gamma }{B}\sum_{\y'\in\B_{i,t}^0}\exp(\frac{s_{\theta_t}(\y', \x_i)}{\tau})\label{eqn:u2}\\
&G_t  = - \frac{1}{|\S_t|}\sum_{\x_i\in\S_t} \nabla s_{\theta_t}(\y_i, \x_i)+ \label{eqn:G2}\\
&\frac{1}{|\S_t|}\sum_{\x_i\in\S_t}\frac{1}{u_{i,t+1}B}\sum_{\y'\in\B_{i,t}^0}\exp(\frac{s_{\theta_{t}}(\y', \x_i)}{\tau})\nabla s_{\theta_t}(\y', \x_i).\nonumber
\end{align}
We refer to the algorithm for solving~(\ref{eq:dftv2}) similar to Algorithm~\ref{alg:DFT} with the above updates of $u_{i,t+1}$ and $G_t$ as DFT2. 




{\bf Computational Costs:} As shown in Figure~\ref{fig:runtime}, DFT2 has a dramatic reduction in computation costs compared with DFT, as it does not need to load the sampling model $P_g^0$ into the memory and compute $P_g^0(\y'|\x_i)$, which DFT requires. Compared with SFT, DFT2 has additional costs for computing $\nabla s_{\theta_t}(\y', \x_i), y'\in\B^0_i$. Nevertheless, such costs appear in the preference optimization step of existing approaches.



\section{Comparison with PO and Self-play}\label{sec:po}
Let us compare DFT2 with preference optimization (PO) approaches. A standard setting of PO is to finetune a LLM based on a set of preference data $\{(\x_i, \y_{i}, \y'_i)\}_{i=1}^n$, where $\y_i$ denotes a winning response to $\x_i$ and $\y'_i$ denotes a losing response, labeled either by a human or a reward model. Most PO approaches can be cast into the following pairwise loss minimization problem: 
\begin{align*}
\min_{\theta}\frac{1}{n}\sum_{i=1}^n\ell(r_\theta(\y_i, \x_i), r_{\theta}(\y'_i, \x_i)),
\end{align*}
where $r_\theta(\y, \x)$ denotes some reward function. This framework can be easily extended to incorporate multiple losing responses in the preference data $\{(\x_i, \y_{i}, \y'_{i1}, \y'_{i1}, \ldots, \y'_{im})\}_{i=1}^n$, by solving the following problem: 
\begin{align}
\min_{\theta}\frac{1}{n}\sum_{i=1}^n\frac{1}{m}\sum_{j=1}^m\ell(r_\theta(\y_i, \x_i), r_{\theta}(\y'_{ij}, \x_i)).
\end{align}
For example, DPO uses a reward function $r_\theta(\y, \x) =  \beta \log \frac{P_g(\y|\x)}{P^{0}_g(\y|\x)}$ and a logistic loss function $\ell(r_\theta(\y, \x) ,  r_{\theta}(\y', \x)) = -\log \sigma(r_\theta(\y, \x)- r_{\theta}(\y', \x))$. Self-play finetuning (SPIN) uses the same objective as DPO except for that $\y'$ is generated by the base LLM. SimPO uses a reward function $r_\theta(\y, \x) =\frac{\beta}{|\y|}\log P_g(\y|\x)$  and adds a margin parameter $\gamma$ to the logistic loss function $\ell(r_\theta(\y_i, \x_i) ,  r_{\theta}(\y'_i, \x_i))= -\log \sigma(r_\theta(\y_i, \x_i)-   r_{\theta}(\y'_i, \x_i) - \gamma)$. 

\begin{figure}[t]
% %\vspace*{-0.1in}
\begin{center}
\centerline{\includegraphics[width=0.8\columnwidth]{figs/s_histogram_e1.pdf}}
%\vspace*{-0.1in}
\caption{Distribution of $s_\theta(\y',\x)$ computed with the model checkpoint after one epoch of training on UltraFeedback data, comparing two groups of $\y'$. Random $\y'$ samples are generated using a very high temperature, while the ``Sampling" group represents $\y'$ generated with temperature less than 1 as used in our method. }
%\vspace*{-0.1in}
\label{fig:s_distrib}
\end{center}
\end{figure}
{\bf Similarity:} From the perspective of PO, we can regard the sampled data $\y'\in\S^0_i$ in DFT2  as potentially losing response to $\x_i$ and the given output $\y_i$ as the winning response. Hence, the objective of DFT2 integrates both the power of SFT and PO.

{\bf Difference:} To understand the difference between the objective of DFT2 and the pairwise loss used in PO approaches, we can rewrite~(\ref{eq:dftv2}) as: 
\begin{align*}
\min_{\theta} & \frac{1}{n}\sum_{i=1}^n  \tau \log \bigg(\frac{1}{m}\sum_{\y'\in\S^0_i}\exp(\frac{s_{\theta}(\y', \x) - s_\theta(\y_i, \x_i)}{\tau})\bigg).
\end{align*}
It can be seen that the difference between DFT2 and PO is that they use different losses for each data $\x_i$.  In particular, DFT2 uses the log-sum-exp loss function $\tau\log (\frac{1}{m}\sum_{\y'\in\S^0_i}\exp(\frac{s_{\theta}(\y', \x) - s_\theta(\y_i, \x_i)}{\tau}))$ for each data $\x_i$, while PO uses an averaged loss $\frac{1}{m}\sum_{j=1}^m\ell(r_\theta(\y_i, \x_i), r_{\theta}(\y'_i, \x_i))$ for each data. Like the cross-entropy loss for classification or the contrastive loss for self-supervised representation learning, the log-sum-exp loss has the property of giving higher weights to a potentially bad output $\y'$ with a larger score $s_\theta(\y', \x)$ in the gradient computation. In contrast, the averaged loss in PO does not enjoy this property. 


\iffalse
In practice, we find that the denominator in \eqref{eq:prob} is dominated by those $\y$ sampled from the reference LLM $p_g^0$. Hence, we can rewrite the \eqref{eq:prob} into
\begin{align}\label{eq:prob2}
\Pr(\y|\x) = \frac{\exp(s(\y, \x)/\tau)}{\E_{\y\sim p^0_g(\y|\x)}\exp(s(\y, \x)/\tau)}.
\end{align}
% To imitate the overhead, we propose a new objective.
% \begin{align*}
% \min_{\w} \E_{\x,\y,\y'\sim p_g^0(\cdot|\x)} \left[l(s(\x,\y) - s(\x,\y'))\right]
% \end{align*}
% where $l(\cdot)$ is a loss function that is both monotonically decreasing and convex. Here, we use a linear function $l(x) = -x$. 
% \begin{align}
% \min_{\w} -\frac{1}{n}\sum_{i=1}^n s(\x_i, \y_i) + \frac{1}{n}\sum_{i=1}^n\E_{\y' \sim p^0_g(\y|\x_i)} s(\x_i, \y')
% \end{align}
Given a dataset $\D=\{(\x_i, \y_i)\}_{i=1}^n$, we construct synthetic responses $\Y_i=\{\y_{ij}'\}_{j=1}^m$ for each prompt $\x_i$. The objective will be
% \begin{align}
% \min_{\w} -\frac{1}{n}\sum_{i=1}^n s(\x_i,\y_i) + \frac{1}{n}\sum_{i=1}^n \frac{1}{m}\sum_{j=1}^m s(\x_i, \y_{ij}')
% \end{align}
% We consider applying distributionally robust optimization (DRO) for the second term:
% \begin{align}\label{eq:dftv2}
% & \min_{\w} -\frac{1}{n}\sum_{i=1}^n s(\x_i, \y_i) \\ 
% & + \frac{1}{n}\sum_{i=1}^n \max_{\boldsymbol{\pi}_i \in \Delta_m}\left\{\sum_{j=1}^m \boldsymbol{\pi}_i^{(j)} s(\x_i, \y_{ij}') - \tau D_\mathrm{KL}\left(\boldsymbol{\pi}_i \vert\vert \mathbf{1}/m\right) \right\}
% \end{align}
% where $\Delta_m\subset\R^m$ is the probability simplex and $\mathbf{1}/m$ represents the uniform distribution on $\Y_i$. Due to the KKT condition, the DRO problem in \eqref{eq:dftv2} is equivalent to 
\begin{align}\label{eq:dftv2}
\min_{\w} - & \frac{1}{n}\sum_{i=1}^n s(\y_i, \x_i) \\ 
 + & \frac{1}{n}\sum_{i=1}^n \tau \log \left(\sum_{j=1}^m \exp\left(s(\y_{ij}', \x_i) / \tau \right) \right).
\end{align}
% \ilgee{$s(\x_i,\y_i)->s(\y_i,\x_i)$ for consistency?}
This optimization problem can be addressed by SogCLR as well. We present the algorithm in~\ref{alg:DFT2}.
\fi 



% \section{Multi-Instance Sampling}

% Given a dataset $\D=\{(\x_i, \y_i)\}_{i=1}^n$ and $q$ LLMs, we construct synthetic responses $\Y_i=\{\y_{ij}^l\}_{j=1}^m$ for each prompt $\x_i$ and model $l=1,\ldots,q$. The multi-instance sampling object is:

% \begin{align}\label{eq:mis}
% & \min_{\w} -\frac{1}{n}\sum_{i=1}^n s(\x_i,\y_i) \\ 
% & + \frac{1}{n}\sum_{i=1}^n \tau \log \left(\sum_{j=1}^m\sum_{l=1}^q  w^l(\y_{ij}^l)\frac{\exp\left(s(\x_i, \y_{ij}^l) / \tau \right)}{p_0^l(\y_{ij}^l | \x_i)} \right)
% \end{align}

% where $p_0^l(\y|\x)$ denotes the probability of generating response $\y$ given prompt $\x$ using the $l$-th LLM, and
% \begin{align}\label{eq:mis_weight}
% w^l(\y_{ij}^l) = \frac{p_0^l(\y_{ij}^l | \x_i)}{\sum_{k=1}^q p_0^k(\y_{ij}^l | \x_i)}
% \end{align}

% Substitute \eqref{eq:mis_weight} into \eqref{eq:mis}, we will obtain
% \begin{align}
% \min_{\w} -\frac{1}{n}\sum_{i=1}^n s(\x_i,\y_i) + \frac{1}{n}\sum_{i=1}^n \tau \log \left(\sum_{j=1}^m\sum_{l=1}^q  \frac{\exp\left(s(\x_i, \y_{ij}^l) / \tau \right)}{\sum_{k=1}^q p_0^k(\y_{ij}^l | \x_i)} \right)    
% \end{align}


\section{Experiments} \label{experiment}
\textbf{Setup}: We evaluate our proposed DFT framework under two distinct training settings. First, we focus on improving the mathematical reasoning capability of a base LLM by using DFT on the MetaMathQA dataset \citep{yu2024metamath}, which contains 395K samples generated through bootstrapped mathematical questions with augmented reasoning paths. In this setting, we set $B=4$. Second, we fine-tune a base LLM using DFT on the UltraFeedback (UF) dataset \citep{cui2023ultrafeedback}, comprising 61K samples. UF is originally used as a preference dataset, where each data pairs a winning response and a losing response. For SFT and DFT, we regard the winning responses $\y_w$ as the ground-truth and discard all losing responses. In this setting, we set $B=2$ and generate $\y'$ by adding an adversarial prompt like ``You are an unhelpful assistant." to the input $\x_i$ in a chat template (cf. Appendix~\ref{sec:temp}) and use it as input to the base LLM for generation. For both settings, we use \href{https://huggingface.co/mistralai/Mistral-7B-v0.1}{Mistral-7B-v0.1} as our base model. More details of implementation and hyper-parameter tuning are described in Appendix~\ref{app:A}.  %Ablation studies about different methods of augmenting $\x$ are provided in Section~\ref{sec:ab}. 

\textbf{Evaluation Benchmarks.} For the first training setting, we evaluate our methods on two widely adopted benchmarks: GSM8K \citep{cobbe2021gsm8k} and MATH \citep{hendrycksmath2021}. We use zero-shot test accuracy as our evaluation metric to assess the model's true reasoning capabilities. For the second training setting, we evaluate on seven diverse benchmarks from the Huggingface Open Leaderboard, including MMLU \citep{hendrycks2021measuring}, TruthfulQA \citep{lin-etal-2022-truthfulqa}, HellaSwag \citep{zellers-etal-2019-hellaswag}, WinoGrande \citep{sakaguchi2019winogrande}, GSM8K \citep{cobbe2021gsm8k}, ARC \citep{allenai:arc}, and IFEval \citep{zhou2023instruction}. We follow the few-shot evaluation protocol from \citet{chen2024self}. For IFEval, we report the prompt-level strict accuracy.  In addition, we also consider evaluation using GPT4-as-a-judge on AlpacaEval2 \cite{dubois2024lengthcontrolled}. %, where we use the length-controlled win rate (LC) as our evaluation metric. 

\subsection{Results}
Table \ref{tab:math} shows the performance of DFT(2) on improving mathematical reasoning capabilities. Table~\ref{tab:sp} and Table~\ref{tab:po} compare DFT(2) with self-play methods and SFT\textrightarrow PO methods, respectively, for the second training setting. We describe our observations below. 
%Tables \ref{tab:math} and \ref{tab:sp} present results comparing DFT and DFT2 against baseline methods across both training settings.


\textbf{Observation 1: DFT variants improve standard SFT.} Both DFT and DFT2 surpass MetaMath-Mistral-7B trained by SFT, achieving state-of-the-art performance among 7B-parameter models on GSM8K (79.15\%) and MATH (28.62\%). Similarly, for general language tasks (Table \ref{tab:sp}), DFT improves SFT across almost all benchmarks except for MMLU, on which both methods are competitive. In addition, both DFT and DFT2 outperform SFT on average.  
%As shown in Table~\ref{tab:sp}, DFT consistently achieves superior or comparable results across multiple commonsense reasoning benchmarks, particularly excelling in tasks requiring complex reasoning (ARC) and instruction following (IFEval).

\textbf{Observation 2: DFT variants consistently outperform PO methods on self-play data.} In Table~\ref{tab:sp}, we compare DFT(2) with PO approaches using self-play data that is generated by the base model as negative data, including SPIN, SimPO, KTO, and SimPO-SFT, where the last one just combines the SimPO loss and the SFT loss similar to~\citet{xu2024contrastive}. Comparing with SimPO-SFT allows us to verify the advantage of our objective over SimPO loss. For these baselines, we use the same generated $\y'$ as in DFT as their negative data and finetune the same base model. The results in Table~\ref{tab:sp}  show that these PO approaches on self-play data can not improve SFT.  This is different from the observation in~\citet{chen2024self,xu2024contrastive}, as their experiments are for finetuning an SFT model. Comparing DFT(2) with SPIN and SimPO can justify the effectiveness of our objectives and the optimization algorithm. % In addition, we also observe that using $B=2$ sampled $\y'$ at each iteration is better than using $B=1$. 


\begin{table}[t]
%%\vspace{-0.2in}
\caption{Testing accuracy on GSM8K and MATH}
%\vspace{-0.2in}
\label{tab:math}
\begin{center}
\begin{sc}
\begin{tabular}{l|ccc}
\hline
Method  & GSM8K & MATH  \\
\hline
%LLaMA-2-7B          & 14.6 & 2.5    \\
%\hline 
RFT-7B              & 50.3 & --      \\
Qwen-7B             & 51.6 & --     \\
Mistral-7B-base (8 shots)         & 52.2 & 13.1   \\
\hline 
WizardMath-7B       & 54.9 & 10.7   \\
LLaMA-2-70B         & 56.8 & 13.5   \\
\hline
MetaMath-7B         & 66.5 & 19.8   \\
MetaMath-Mistral-7B & 77.7 & 28.2   \\
\hline
DFT (Mistral-7B-base) & \textbf{79.15} & 28.34 \\
%DFT-nl (Mistral-7B-base)   & 78.54 & \textbf{28.42}   \\
DFT2 (Mistral-7B-base)  & 78.77 & \textbf{28.62}  \\
\hline 
\end{tabular}
\end{sc}
\end{center}
\end{table}

\begin{table*}[htb!]
\caption{Comparison between DFT, SFT, and PO methods on self-play data in the second training setting. % on downstream tasks. We use the UF dataset to construct the input-output data, where the output for each input is the winning response.  The results show performance across $7$ different benchmarks and their average.
}
%\vspace*{-0.15in}
\label{tab:sp}
\begin{center}
\begin{sc}
\resizebox{\textwidth}{!}{\begin{tabular}{lccccccc|>{\columncolor[gray]{0.8}}c}
\toprule
 Method   & MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k  & ARC &  IFEval & Avg.  \\ \hline
% \multicolumn{8}{c}{Mistral}   \\ \hline
% Mistral 7b & & & 62.56 &  42.60 & 83.53 & 78.30 & 38.67 &    61.18 & 61.14 & \\ \hline
SFT & 62.18 & 50.04 & 83.59 & 78.06 & 45.26 & 63.65 & 49.72 & 61.79 \\ \hline
%\multicolumn{9}{c}{$B = 1$}   \\ \hline
%SPIN  & 62.16 & 50.23 & 83.67 & 78.06 & 46.10 & 62.03 & 19.59 & 57.41  \\
%SimPO & 62.29 & 50.75 & 83.84 & 78.06 & 2.88 & 61.69 & 19.41 & 51.27 \\
%SimPO+SFT & 62.53 & 48.83 & 83.55 & 77.82 & 43.44 & 61.60 & 42.70 & 60.07   \\
%DFT   & 61.76 & 50.89 & 83.95 & 77.98 & 46.70 & 64.42 & 50.65 & 62.33 \\
%DFT-ln & 62.30 & 55.23 & 83.13 & 77.82 & 45.26 & 64.93 & 64.78 & 12.20 \\
%DFT2  & 61.92 & 52.87 & 83.02 & 78.14 & 44.81 & 64.68 & 51.57 & 62.43 \\ \hline
%\multicolumn{9}{c}{$B = 2$}   \\ \hline
SPIN & 61.99 & 49.91 & 83.75 & 77.90 & 46.02 & 61.95 & 23.11 & 57.80 \\
SimPO & 62.39 & 52.08 & 83.89 & 78.14 & 2.58 & 61.86 & 18.85 & 51.40    \\ 
SimPO-SFT & 62.28 & 49.59 & 83.46 & 77.90 & 42.53 & 61.52 & 43.62 & 60.13  \\
KTO & 61.59 & 49.32 & 82.88 & 79.24 & 43.97 & 61.60 & 38.08 & 59.53 \\
\hline
DFT & 61.69 & 52.23 & 83.95 & 78.37 & 48.22 & 64.25 & 51.20 & 62.84   \\
%DFT-ln & 61.94 & 53.93 & 83.25 & 78.22 & 44.28 & 65.02 & 64.44 & 11.41\\
DFT2 & 61.66 & 54.14 & 83.20 & 77.82 & 45.49 & 64.42 & 51.20 & 62.56    \\ \bottomrule
%\multicolumn{9}{c}{$B = 4$}   \\ \hline
%SPIN &   61.94 & 49.60  & 83.70 & 77.98 & 46.02 & 62.03 & 63.54 &  1.14 \\
%SIMPO & 62.31 & 51.39 & 83.83 & 77.90 & 4.47 & 61.86 & 56.96 & 0.81   \\ 
%SIMPO-CPO & 62.46 & 49.47 & 83.47 & 78.06 & 42.15 & 61.69 & 62.88 & 8.88  \\
%DFT & 61.57 & 51.88 & 83.70 & 78.22 & 47.38 & 64.42 & 64.53 & 12.32  \\
%DFT-ln & 62.14 & 52.76 & 82.79 & 78.85 & 44.35 & 65.44 & 64.39 & 12.18 \\
%DFT2 & 61.78 & 53.67 & 83.35 & 77.90 & 45.64 & 65.02 & 64.56 & 11.69   \\
%\hline
\end{tabular}}
\end{sc}
\end{center}
%\vspace*{0.1in}
%\end{table*}
%\begin{table*}[htb!]
\caption{Comparison between DFT and SFT\textrightarrow PO approaches on preference data in the second training setting.}
%\vspace*{-0.15in}
\label{tab:po}
\begin{center}
\begin{sc}
\resizebox{\textwidth}{!}{\begin{tabular}{l|l|ccccccc|>{\columncolor[gray]{0.8}}c}
\toprule
Method &Data& MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k & ARC &  IFEval & {Avg.} \\
\hline 
\hline
SFT\textrightarrow DPO&UC \textrightarrow UF & 57.49 & 53.15 & 83.60 & 77.43 & 30.55 & 61.52 & 39.93 & 57.67   \\
SFT\textrightarrow SimPO &UC \textrightarrow UF & 58.33 & 50.67 & 83.39 & 76.95 & 33.36 & 61.86 & 40.48 & 57.86   \\
% ORPO & \\
SFT\textrightarrow RRHF & UC \textrightarrow UF & 56.40 & 43.70 & 80.37 & 77.51 & 0.45 & 52.99 & 37.52 & 49.85 \\
SFT\textrightarrow R-DPO &UC \textrightarrow UF & 58.29 & 46.10 & 84.11 & 76.40 & 28.43 & 61.26 & 38.63 & 56.17 \\
SFT\textrightarrow CPO & UC \textrightarrow UF & 58.05 & 47.10 & 80.73 & 77.11 & 35.86 & 57.17 &  40.67 & 56.67 \\
SFT\textrightarrow IPO &UC \textrightarrow UF & 59.10 & 45.45 & 83.14 & 77.43 & 34.12 & 60.24 & 42.88 & 57.48 \\
SFT\textrightarrow KTO &UC \textrightarrow UF & 59.72 & 56.65 & 84.92 & 78.14 & 40.49 & 63.57 & 43.62 & 61.01 \\ \hline
SFT\textrightarrow KTO & UF \textrightarrow UF & 62.15 & 54.53 & 84.77 & 77.98 & 45.41 & 64.51 & {51.94} & {63.04} \\
SFT\textrightarrow DPO & UF \textrightarrow UF & 62.28 & 55.67 & 84.79 & 78.22 & 47.54 & {64.68} & 47.69 & 62.98   \\
SFT\textrightarrow SimPO & UF \textrightarrow UF & 61.20 & 59.36 & 85.18 & 77.03 & 44.12 & 63.74 & 43.81 & 62.06 \\
%KTO & UF & 62.68 & 50.95 & 85.01 & 78.45 & 46.17 & 64.33 & 47.50 & 62.16 \\ 
\hline
DFT &UF ($\x, \y_w$) & 61.69 & 52.23 & 83.95 & 78.37 & 48.22 & 64.25 & 51.20 & 62.84   \\
DFT2 &UF ($\x, \y_w$) & 61.66 & 54.14 & 83.20 & 77.82 & 45.49 & 64.42 & 51.20 & 62.56  \\
\bottomrule
\end{tabular}}
\end{sc}
\end{center}
%\vspace*{-0.2in}
\end{table*}

\textbf{Observation 3: DFT is competitive with SFT\textrightarrow PO approaches.} We compare with state-of-the-art PO methods including DPO, SimPO, RRHF, R-DPO, CPO, IPO, and KTO after SFT in Table~\ref{tab:po}. It is notable that the existing training pipeline for PO-based methods first trains an SFT model on the UltraChat-200k (UC) dataset \cite{ding2023enhancing} and then applies PO on the UF preference dataset. However, this training pipeline gives worse performance than SFT on UF winning data. It is probably because the winning responses in the UF dataset give better ground-truth than the data in UC. To mitigate this issue, we implement three methods SFT\textrightarrow DPO, SFT\textrightarrow SimPO and SFT\textrightarrow KTO  using an improved pipeline (UF\textrightarrow UF) that first performs SFT on UF winning data and then applies PO on the UF preference data.

Our results in Table~\ref{tab:po} show that DFT variants significantly outperform all methods using the standard UC\textrightarrow UF pipeline. When compared to the improved UF\textrightarrow UF pipeline,  DFT enjoys competitive performance with state-of-the-art PO methods including DPO, SimPO and KTO.  This is particularly noteworthy as DFT achieves these results in a single training stage, directly fine-tuning the pretrained Mistral-7B-v0.1 model without the preference data.

{\bf AlpacaEval2 Results.} Finally, we briefly discuss the GPT4-as-a-judge evaluation on instruction following by reporting the length-controlled winning rate (LC) on AlapacaEval2. The comparison between DFT(2) with SFT and PO-based methods on self-play data is shown in Figure~\ref{fig:alpaca2}, which demonstrates DFT(2) outperforms these baselines. The comparison between DFT(2) and SFT\textrightarrow PO approaches on preference data is shown in Appendix~\ref{app:alpaca}, which shows that DFT(2) is competitive with some PO approaches using the preference data, such as KTO, but worse than SimPO and DPO. However, DFT(2) have much shorter lengths for the outputs with an average length of 1359. In contrast,  KTO, DPO, SimPO have average lengths of 1449, 1477, 1868, respectively. The GPT4-as-a-judge evaluation tends to favor outputs that are longer.     %This is not too much surprising as the UF preference data is constructed using GPT-4 as a reward model. %Interestingly, we observe that DFT models tend to generate shorter output than those finetuned by PO-based approaches. 
Nevertheless, DFT has competitive if not better performance on verifiable instruction following benchmark IFEval (cf. Table~\ref{tab:po}). 

\begin{figure}[h]
\centering
\begin{subfigure}{0.47\linewidth}
\includegraphics[width=\linewidth]{figs/alpaca2.pdf}
\caption{AlpacaEval2.}
\label{fig:alpaca2}
\end{subfigure}
\hfill
\begin{subfigure}{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/gamma.pdf}
%\vspace*{-0.25in}
\caption{Average accuracy.}
\label{fig:gamma}
\end{subfigure}
\caption{(a) AlpacaEval2 LC win rate of DFT, SFT, and PO methods based on self-play data. (b) Average testing accuracy of DFT under different $\gamma$ values.}
\end{figure}

% While preference optimization methods show higher win rate in some cases, it's crucial to note that these methods are trained on GPT-4 annotated preference dataset UltraFeedback. Since the AlpacaEval2 uses GPT-4 Turbo as the evaluator, this alignment between training data and evaluation criteria likely contributes to their higher win rate.

%\section{Analyses}
\subsection{Ablation Studies}\label{sec:ab}
We present some results to illustrate the advantage of our optimization using moving-average estimators $u$, and the effect of the number of generated samples $B$ in each iteration. %, and the impact of the prompts for sampling negative data.    %(  the  investigate the effect of initialization value $\mathbf{u}_0$, moving average coefficient $\gamma$, and prompts for synthetic data generation. %Our analysis demonstrates: 

{\noindent\bf The advantage of using moving-average estimators $u$.} In the second training setting, we train DFT  using different values of $\gamma$ ranging from 0.8 to 1.0. The value of $1.0$ corresponds to using the mini-batch estimator of $g_i(\theta)$ for estimating the gradient.  The average testing accuracy on the seven benchmarks of Huggingface Open Leaderboard is  shown in Figure~\ref{fig:gamma} with more detailed results reported in Table~\ref{tab:gamma} in Appendix~\ref{sec:gamma}. It shows that using a moving-average estimator with $\gamma\in [0.80, 0.95]$  significantly improves the performance compared to not using the moving-average estimators corresponding to $\gamma = 1.0$, justifying the effectiveness of our optimization algorithm.  %, we observe a substantial degradation in performance across all tasks, especially the GSM8K and IFEval.

{\noindent\bf The effect of $B$.} We compare different values of $B=1, 2, 4$ in the second training setting on UF data. The results of DFT(2) and other PO-based approaches using the self-play data are shown in Appendix~\ref{app:batch}. The results show that increasing $B$ from 1 to $2$ improves the performance, especially on AlpacalEval2. However, further increasing it to $B=4$ decreases the performance. We suspect that this is probably due to overfitting, and expect more training data will accommodate a larger $B$, e.g., DFT on the larger MetaMathQA dataset with $B=4$ is better than $B=2$. 



\section{Conclusion}
In this paper, we have proposed a discriminative probabilistic framework for finetuning a pretrained large language model without using any preference data or a reward model. Efficient and effective optimization algorithms are developed. Extensive experiments have demonstrated the effectiveness of the proposed methods compared with the standard supervised finetuning method and the existing preference optimization methods. 

\section*{Impact Statements}
This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.

\bibliography{refs}
\bibliographystyle{icml2025}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn

\section{Implementation Details}\label{app:A}

Our implementations are based on alignment-handbook\footnote{\href{https://github.com/huggingface/alignment-handbook}{\texttt{https://github.com/huggingface/alignment-handbook}}} and TRL \citep{vonwerra2022trl} training framework. Below we provide comprehensive details about our experimental setup and implementation choices.

\subsection{Hyper-parameters Tuning}

{\bf Setting 1.} For both DFT and DFT2, we follow \citet{yu2024metamath}, and set the batch size to 128, max sequence length to 512, and number of epochs to 3. We tune the learning rate in $\{5e-7, 8e-7, 2e-6\}$. For DFT, we tune the $\tau$ in $\{0.8, 0.9, 1.0\}$ and for DFT2, we tune the $\tau$ in $\{0.1,0.2,0.3\}$. Details of the chosen hyper-parameters are summarized in Table \ref{tab:hyperparams_setting1}.

\begin{table}[h]
\caption{Hyper-parameters for DFT methods under setting 1.}
\label{tab:hyperparams_setting1}
\begin{center}
\begin{tabular}{l|ccccc}
\hline
Hyperparameters & DFT & DFT2 \\
\hline
$\tau$  & $1.0$ & $0.1$ \\
$\gamma$ & \multicolumn{2}{c}{$0.95$} \\
Batch Size & \multicolumn{2}{c}{128} \\
max sequence length & \multicolumn{2}{c}{512}  \\
Learning Rate & \multicolumn{2}{c}{8e-7}  \\
LR Scheduler & \multicolumn{2}{c}{Cosine}  \\
Warmup Ratio & \multicolumn{2}{c}{0.1}  \\
Optimizer & \multicolumn{2}{c}{AdamW} \\
Epochs & \multicolumn{2}{c}{3} \\
\hline
\end{tabular}
\end{center}
\end{table}

{\bf Setting 2.} We compare DFT variants against several baseline methods: SFT, SPIN, KTO, SimPO, and SimPO-SFT. For all methods, we use a batch size of 128, a maximum sequence length of 1024, and a training duration of 2 epochs. We perform learning rate tuning across $\{3e-7, 5e-7, 8e-7, 2e-6\}$, except for KTO where we use a larger learning rate of 5e-6. For DFT variants, we tune $\tau$ in $\{0.8, 0.9, 1.0\}$ for DFT and $\tau$ in $\{0.1,0.2,0.3,0.4\}$ for DFT2, with $\gamma$ in $\{0.80, 0.85, 0.9, 0.95\}$ for both variants. The most effective values of $\tau$ are found to be $1.0$ for DFT and $0.3$ for DFT2. For the baseline methods, we tune their respective hyperparameters: $\beta$ in $\{0.01, 0.05, 0.1\}$ for SPIN; $\beta$ in $\{6, 8, 10, 12\}$ with a gamma-beta-ratio of $0.5$ for SimPO; and $\beta$ in $\{6, 8, 10, 12\}$ with a gamma-beta-ratio of $0.5$ and combining weight of $1$ for SimPO-SFT. For KTO, we set $\lambda_U=1$, and tune $\beta \in \{0.01,0.05,0.1\}$, $\lambda_D \in [B, 1.5B]$.

Details of the chosen hyperparameters are summarized in Table~\ref{tab:hyperparams_setting2}.

\begin{table}[h]
\caption{Hyper-parameters for DFT methods under setting 2.}
\label{tab:hyperparams_setting2}
\begin{center}
\begin{tabular}{l|ccc}
\hline
Hyperparameters & DFT & DFT2  \\
\hline
$\tau$  & 1.0 & 0.3 \\
$\gamma$ & 0.85 & 0.90 \\
Batch Size & \multicolumn{2}{c}{128} \\
max sequence length & \multicolumn{2}{c}{1024}  \\
Learning Rate & \multicolumn{2}{c}{2e-6}  \\
LR Scheduler & \multicolumn{2}{c}{Cosine}  \\
Warmup Ratio & \multicolumn{2}{c}{0.1}  \\
Optimizer & \multicolumn{2}{c}{AdamW} \\
Epochs & \multicolumn{2}{c}{2} \\
\hline
\end{tabular}
\end{center}
\end{table}

\subsection{Implemenation of SFT\textrightarrow PO}
For SFT\textrightarrow PO methods under the UC\textrightarrow UF pipeline, we use the released checkpoints produced by SimPO~\cite{meng2024simpo}, where models are first trained using SFT on UltraChat-200k for 1 epoch and then undergo preference optimization on UltraFeedback for 1 epoch. For SFT\textrightarrow PO methods under the UF\textrightarrow UF pipeline, we first train the SFT model on UltraFeedback with a learning rate of 2e-6 for 2 epochs, then conduct preference optimization on UltraFeedback for 1 epoch.

\subsection{Training Costs}
We compare the training efficiency of different methods, with results shown in Figure \ref{fig:runtime}.

\begin{figure}[h]
\begin{center}
\centerline{\includegraphics[width=0.45\linewidth]{figs/runtime3}}
\caption{Training time per-iteration of DFT, DFT2 ($B=2$) and SFT and PO methods for finetuning Mistral-7b-base model on UltraFeedback dataset.}
\label{fig:runtime}
\end{center}
\end{figure}

\begin{figure}[h]
%\vspace*{-0.1in}
\centering
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{figs/alpaca_po_len.pdf}
\caption{Average generation length}
\label{fig:alpaca_po_len}
\end{subfigure}
\hfill
\begin{subfigure}{0.45\linewidth}
\includegraphics[width=\linewidth]{figs/alpaca_po.pdf}
\caption{LC win rate}
\label{fig:alpaca_po}
\end{subfigure}
\caption{(a) AlpacaEval2 average generation length of DFT and SFT\textrightarrow PO approaches. (b) AlpacaEval2 LC win rate of DFT and SFT\textrightarrow PO approaches. SIMPO*, KTO*, and DPO* denote training under the UF\textrightarrow UF pipeline.}
\end{figure}

% \subsection{Initialization of $\mathbf{u}_0$}

% We find Setting $\mathbf{u}_0 > 0$ is crucial to prevent the collapse of likelihood scores for negative samples. As shown in xxx, we observe that $\mathbf{u}_0 = 0$ will push $\log P_g(\y|\x)$ to $-\infty$ in a few steps. This issue can be addressed by initializing $\mathbf{u}_0$ to a value larger than $0$. We use $\mathbf{u}_0=1$ in our implementation.

% Following are some mathematical explanation: When $\mathbf{u}_0 = c$, the average weight $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{(1 - \gamma)c + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}$. If $c = 0$, we have the average weight $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{\gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}} = \frac{1}{\gamma}$. If $c > 0$, $w_{\x_i,\y}$ decreases very fast and soon become negligible comparing with $c$ in practice, then we have $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{(1 - \gamma)c}$. It means a smaller weight will be assigned to those samples with smaller $w_{\x_i,\y}$, which mitigates the over-fitting.

\section{Additional Evaluation Results}

\subsection{AlpacaEval2 Results}\label{app:alpaca}
Figure~\ref{fig:alpaca_po_len} and Figure~\ref{fig:alpaca_po} presents a detailed comparison between DFT variants and SFT\textrightarrow PO approaches on AlpacaEval2. The results demonstrate that DFT variants perform competitively to some PO methods such as SFT\textrightarrow KTO. They also tend to generate shorter output than models finetuned by most PO-based approaches. It is interesting to see that SFT\textrightarrow SimPO trained using UC\textrightarrow UF pipeline generates the longest outputs and it has the highest  AlpacaEval2 score. However, its results on benchmark datasets in Table~\ref{tab:po} is much worse than our method. 

 \subsection{ DFT with different $\gamma$}\label{sec:gamma}
As shown in Table~\ref{tab:gamma}, $\gamma=1.0$ leads to a significant performance degradation across all tasks, with particularly severe drops in GSM8K and IFEval. But the performance across this range $\gamma \in (0.8, 0.9)$ is consistently good.
 
 \begin{table*}[h]
 \caption{Results of DFT with different $\gamma$ values for finetuning on UF winning data.}
 \label{tab:gamma}
 \begin{center}
 \begin{sc}
 \begin{small}
 \begin{tabular}{lcccccccc}
 \hline 
  Method   & MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k  & ARC & IFEval &Avg. \\ \hline
 DFT w/ $\gamma=0.8$  & 61.42 & 51.08 & 83.90 & 78.14 & 48.22 & 64.51 & 52.50 & 62.82  \\
 DFT w/ $\gamma=0.85$  & 61.69 & 52.23 & 83.95 & 78.37 & 48.22 & 64.25 & 51.20 & 62.84  \\
 DFT w/ $\gamma=0.9$  & 61.75 & 52.19 & 83.88 & 78.37 & 47.84 & 64.33 & 50.46 & 62.69  \\
 DFT w/ $\gamma=0.95$ & 61.82 & 50.78 & 83.93 & 78.30 & 46.47 & 64.42 & 51.94 & 62.52 \\
 DFT w/ $\gamma=1$ & 59.46 & 45.38 & 80.54 & 76.64 & 22.14 & 63.05 & 19.96 & 52.45  \\ \hline

 \end{tabular}
 \end{small}
 \end{sc}
 \end{center}
 \end{table*}

\subsection{Detailed Analysis of $B$ Effect}\label{app:batch}
We conducted extensive experiments varying the number of negative samples ($B$) used during training. Table~\ref{tab:B} presents comprehensive results comparing DFT, SFT, and PO methods across different values of $B$ (1, 2, and 4). Figure~\ref{fig:alpaca_b} shows the AlpacaEval2 results. We can see that $B=2$ has a dramatic improvement over $B=1$, especially on AlpacaEval2 evaluation. However, increasing $B$ to 4 will decrease the performance.  

\begin{table*}[htb!]
\caption{Comparison between DFT, SFT and PO methods for fine-tuning Mistral-7B base model on self-play data across $B \in \{1,2,4\}$. The results show performance across $7$ different benchmarks and their average.}
\label{tab:B}
\begin{center}
\begin{sc}
\resizebox{\textwidth}{!}{\begin{tabular}{lccccccc|>{\columncolor[gray]{0.8}}c}
\toprule
 Method   & MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k  & ARC &  IFEval & Avg.  \\ \hline
% \multicolumn{8}{c}{Mistral}   \\ \hline
% Mistral 7b & & & 62.56 &  42.60 & 83.53 & 78.30 & 38.67 &    61.18 & 61.14 & \\ \hline
SFT & 62.18 & 50.04 & 83.59 & 78.06 & 45.26 & 63.65 & 49.72 & 61.79 \\ \hline
\multicolumn{9}{c}{$B = 1$}   \\ \hline
SPIN  & 62.16 & 50.23 & 83.67 & 78.06 & 46.10 & 62.03 & 19.59 & 57.41  \\
SimPO & 62.29 & 50.75 & 83.84 & 78.06 & 2.88 & 61.69 & 19.41 & 51.27 \\
SimPO-SFT & 62.53 & 48.83 & 83.55 & 77.82 & 43.44 & 61.60 & 42.70 & 60.07   \\
KTO &  61.27 & 50.28 & 83.30 & 78.53 & 40.79 & 63.74 & 42.51 & 60.06 \\
DFT   & 61.76 & 50.89 & 83.95 & 77.98 & 46.70 & 64.42 & 50.65 & 62.33 \\
% DFT-ln & 62.30 & 55.23 & 83.13 & 77.82 & 45.26 & 64.93 & 64.78 & 12.20 \\
DFT2  & 61.92 & 52.87 & 83.02 & 78.14 & 44.81 & 64.68 & 51.57 & 62.43 \\ \hline
\multicolumn{9}{c}{$B = 2$}   \\ \hline
SPIN & 61.99 & 49.91 & 83.75 & 77.90 & 46.02 & 61.95 & 23.11 & 57.80 \\
SimPO & 62.39 & 52.08 & 83.89 & 78.14 & 2.58 & 61.86 & 18.85 & 51.40    \\ 
SimPO-SFT & 62.28 & 49.59 & 83.46 & 77.90 & 42.53 & 61.52 & 43.62 & 60.13  \\
KTO & 61.59 & 49.32 & 82.88 & 79.24 & 43.97 & 61.60 & 38.08 & 59.53 \\
DFT & 61.69 & 52.23 & 83.95 & 78.37 & 48.22 & 64.25 & 51.20 & 62.84   \\
%DFT-ln & 61.94 & 53.93 & 83.25 & 78.22 & 44.28 & 65.02 & 64.44 & 11.41\\
DFT2 & 61.66 & 54.14 & 83.20 & 77.82 & 45.49 & 64.42 & 51.20 & 62.56    \\ \bottomrule
\multicolumn{9}{c}{$B = 4$}   \\ \hline
SPIN & 61.94 & 49.60  & 83.70 & 77.98 & 46.02 & 62.03 & 20.70 & 57.42  \\
SimPO & 62.31 & 51.39 & 83.83 & 77.90 & 4.47 & 61.86 & 19.04 & 51.54   \\ 
SimPO-SFT & 62.46 & 49.47 & 83.47 & 78.06 & 42.15 & 61.69 & 39.93 & 59.60  \\
KTO & 60.99 & 49.87 & 82.06 & 78.61 & 41.62 & 61.86 & 40.11 & 59.30  \\
DFT & 61.52 & 51.80 & 83.97 & 78.53 & 46.32 & 64.51 & 50.65 & 62.47   \\
% DFT-ln & 62.14 & 52.76 & 82.79 & 78.85 & 44.35 & 65.44 & 64.39 & 12.18 \\
DFT2 & 61.78 & 53.67 & 83.35 & 77.90 & 45.64 & 65.02 & 48.80 & 62.31    \\
\hline
\end{tabular}}
\end{sc}
\end{center}
\end{table*}

\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figs/alpaca_b.pdf}}
\caption{Comparison of different methods (DFT, SFT and PO methods) on AlpacaEval2 LC win rate across $B \in \{1,2,4\}$.}
\label{fig:alpaca_b}
\end{center}
\end{figure}

\begin{figure}[htb]
\begin{center}
\centerline{\includegraphics[width=0.4\columnwidth]{figs/prompt.pdf}}
\caption{Average performance scores across different prompting strategies for generating negative samples. ``CT" indicates whether chat template formatting was used, and ``sys msg" refers to system messages. Results show that using chat templates with adversarial (``Bad") system messages achieves the best performance (62.84\%), while beneficial (``Good") system messages yield a lower score (62.35\%).}
\label{fig:prompt}
\end{center}
\end{figure}


\subsection{Impact of Prompting Strategies}\label{sec:temp}
We investigated four different prompting strategies for generating the self-play data: (1) direct prompting without any special formatting;
(2) structuring prompts using the same chat template as during fine-tuning; (3) in addition to (2), structuring prompts with beneficial system messages. (4) in addition to (2), structuring prompts with deliberately adversarial system messages. For setting (3), we follow the generation scripts of UltraFeedback. For setting (4), we add ``You are an unhelpful assistant." to the system prompt. Examples of these prompts are given in the box at the end of the appendix.  For all settings, we sample the negatives with the following parameters: a temperature of $0.7$, a top-p of $1.0$, a top-k of $50$, and a max-tokens of $320$. 

Figure~\ref{fig:prompt} illustrates the comparative performance of these strategies. Our analysis reveals that using structured prompts with adversarial system messages tends to generate more challenging negative samples, leading to better performance. Below we provide an example of how each prompting strategy is implemented:

% \begin{table*}[htb!]
% \caption{Results of DFT with different prompting strategies in generating the self-play data. CT denotes whether to apply a chat template.}
% \label{tab:prompt}
% \begin{center}
% \begin{sc}
% \begin{small}
% \begin{tabular}{lccccccccc}
% \hline 
% CT & sys message & MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k  & ARC & IFEval &Avg. \\ \hline
% $\times$ & -- & 62.19 & 52.21 & 83.91 & 78.06 & 45.64 & 64.85 & 49.54 & 62.34 \\
% $\surd$ & -- & 61.67 & 51.16 & 83.88 & 77.74 & 46.40 & 64.68 & 51.57 & 62.44 \\
% $\surd$ & Good & 61.74 & 52.23 & 83.96 & 78.61 & 44.43 & 64.08 & 51.39 & 62.35  \\
% $\surd$ & Bad & 61.69 & 52.23 & 83.95 & 78.37 & 48.22 & 64.25 & 51.20 & 62.84\\ \hline
% \end{tabular}
% \end{small}
% \end{sc}
% \end{center}
% \end{table*}

\begin{tcolorbox}[title=Examples of Different Prompting Strategies, colback=white, colframe=gray]
\textbf{Raw Prompt Message:}
\begin{verbatim}
{"content": "Which animal has two hands, a hyrax or a dog?", "role": "user"}
\end{verbatim}

\textbf{Direct Prompting:} 
\begin{verbatim}
Which animal has two hands, a hyrax or a dog?
\end{verbatim}

\textbf{Chat Template:}
\begin{verbatim}
<|user|>
Which animal has two hands, a hyrax or a dog?</s>
<|assistant|>
\end{verbatim}

\textbf{Chat Template + Good System Messages:}
\begin{verbatim}
<|system|>
The assistant should answer truthfully and be faithful to factual knowledge 
as well as given contexts, never making up any new facts that aren't true 
or cannot be grounded in the instruction.</s>
<|user|>
Which animal has two hands, a hyrax or a dog?</s>
<|assistant|>
\end{verbatim}

\textbf{Chat Template + Bad System Messages:}
\begin{verbatim}
<|system|>
You are an unhelpful assistant.</s>
<|user|>
Which animal has two hands, a hyrax or a dog?</s>
<|assistant|>
\end{verbatim}
\end{tcolorbox}

\end{document}

