[
  {
    "index": 0,
    "papers": [
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "wei2022finetuned",
        "author": "Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le",
        "title": "Finetuned Language Models are Zero-Shot Learners"
      },
      {
        "key": "xu2023wizardlm",
        "author": "Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin",
        "title": "Wizardlm: Empowering large language models to follow complex instructions"
      },
      {
        "key": "wang2023self",
        "author": "Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh",
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions"
      },
      {
        "key": "zhang2023instruction",
        "author": "Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others",
        "title": "Instruction tuning for large language models: A survey"
      },
      {
        "key": "li2024self",
        "author": "Li, Xian and Yu, Ping and Zhou, Chunting and Schick, Timo and Levy, Omer and Zettlemoyer, Luke and Weston, Jason E and Lewis, Mike",
        "title": "Self-Alignment with Instruction Backtranslation"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "kung2023models",
        "author": "Kung, Po-Nien and Peng, Nanyun",
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning"
      },
      {
        "key": "zhang2023instruction",
        "author": "Zhang, Shengyu and Dong, Linfeng and Li, Xiaoya and Zhang, Sen and Sun, Xiaofei and Wang, Shuhe and Li, Jiwei and Hu, Runyi and Zhang, Tianwei and Wu, Fei and others",
        "title": "Instruction tuning for large language models: A survey"
      },
      {
        "key": "gudibande2024the",
        "author": "Arnav Gudibande and Eric Wallace and Charlie Victor Snell and Xinyang Geng and Hao Liu and Pieter Abbeel and Sergey Levine and Dawn Song",
        "title": "The False Promise of Imitating Proprietary Language Models"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "li2024getting",
        "author": "Jiaxiang Li and Siliang Zeng and Hoi To Wai and Chenliang Li and Alfredo Garcia and Mingyi Hong",
        "title": "Getting More Juice Out of the {SFT} Data: Reward Learning from Human Demonstration Improves {SFT} for {LLM} Alignment"
      },
      {
        "key": "wulfmeier2024imitating",
        "author": "Markus Wulfmeier and Michael Bloesch and Nino Vieillard and Arun Ahuja and Jorg Bornschein and Sandy Huang and Artem Sokolov and Matt Barnes and Guillaume Desjardins and Alex Bewley and Sarah Maria Elisabeth Bechtle and Jost Tobias Springenberg and Nikola Momchev and Olivier Bachem and Matthieu Geist and Martin Riedmiller",
        "title": "Imitating Language via Scalable Inverse Reinforcement Learning"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "learn2020summ",
        "author": "Nisan Stiennon and\nLong Ouyang and\nJeff Wu and\nDaniel M. Ziegler and\nRyan Lowe and\nChelsea Voss and\nAlec Radford and\nDario Amodei and\nPaul F. Christiano",
        "title": "Learning to summarize from human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "schulman2017proximal",
        "author": "Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg",
        "title": "Proximal policy optimization algorithms"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "williams1992simple",
        "author": "Williams, Ronald J",
        "title": "Simple statistical gradient-following algorithms for connectionist reinforcement learning"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "Park2024DisentanglingLF",
        "author": "Ryan Park and Rafael Rafailov and Stefano Ermon and Chelsea Finn",
        "title": "Disentangling Length from Quality in Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "xu2024contrastive",
        "author": "Haoran Xu and Amr Sharaf and Yunmo Chen and Weiting Tan and Lingfeng Shen and Benjamin Van Durme and Kenton Murray and Young Jin Kim",
        "title": "Contrastive Preference Optimization: Pushing the Boundaries of {LLM} Performance in Machine Translation"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "ipo_2022",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ethayarajhmodel",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Model Alignment as Prospect Theoretic Optimization"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhao2023calibrating",
        "author": "Yao Zhao and Mikhail Khalman and Rishabh Joshi and Shashi Narayan and Mohammad Saleh and Peter J Liu",
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"
      },
      {
        "key": "jung2024binaryclassifieroptimizationlarge",
        "author": "Seungjae Jung and Gunsoo Han and Daniel Wontae Nam and Kyoung-Woon On",
        "title": "Binary Classifier Optimization for Large Language Model Alignment"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "zhao2023calibrating",
        "author": "Yao Zhao and Mikhail Khalman and Rishabh Joshi and Shashi Narayan and Mohammad Saleh and Peter J Liu",
        "title": "Calibrating Sequence likelihood Improves Conditional Language Generation"
      },
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      },
      {
        "key": "ipo_2022",
        "author": "Azar, Mohammad Gheshlaghi and Guo, Zhaohan Daniel and Piot, Bilal and Munos, Remi and Rowland, Mark and Valko, Michal and Calandriello, Daniele",
        "title": "A general theoretical paradigm to understand learning from human preferences"
      },
      {
        "key": "ethayarajhmodel",
        "author": "Ethayarajh, Kawin and Xu, Winnie and Muennighoff, Niklas and Jurafsky, Dan and Kiela, Douwe",
        "title": "Model Alignment as Prospect Theoretic Optimization"
      },
      {
        "key": "meng2024simpo",
        "author": "Meng, Yu and Xia, Mengzhou and Chen, Danqi",
        "title": "SimPO: Simple Preference Optimization with a Reference-Free Reward"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "chen2024noise",
        "author": "Chen, Huayu and He, Guande and Yuan, Lifan and Cui, Ganqu and Su, Hang and Zhu, Jun",
        "title": "Noise contrastive alignment of language models with explicit rewards"
      },
      {
        "key": "yuan2023rrhf",
        "author": "Hongyi Yuan and Zheng Yuan and Chuanqi Tan and Wei Wang and Songfang Huang and Fei Huang",
        "title": "{RRHF}: Rank Responses to Align Language Models with Human Feedback"
      },
      {
        "key": "10.1609/aaai.v38i17.29865",
        "author": "Song, Feifan and Yu, Bowen and Li, Minghao and Yu, Haiyang and Huang, Fei and Li, Yongbin and Wang, Houfeng",
        "title": "Preference ranking optimization for human alignment"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "rosset2024direct",
        "author": "Rosset, Corby and Cheng, Ching-An and Mitra, Arindam and Santacroce, Michael and Awadallah, Ahmed and Xie, Tengyang",
        "title": "Direct nash optimization: Teaching language models to self-improve with general preferences"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "chen2024noise",
        "author": "Chen, Huayu and He, Guande and Yuan, Lifan and Cui, Ganqu and Su, Hang and Zhu, Jun",
        "title": "Noise contrastive alignment of language models with explicit rewards"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "rafailov2023direct",
        "author": "Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea",
        "title": "Direct preference optimization: Your language model is secretly a reward model"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "chen2024preference",
        "author": "Angelica Chen and Sadhika Malladi and Lily H Zhang and Xinyi Chen and Qiuyi Zhang and Rajesh Ranganath and Kyunghyun Cho",
        "title": "Preference Learning Algorithms Do Not Learn Preference Rankings"
      },
      {
        "key": "DBLP:journals/corr/abs-2407-13709",
        "author": "Yixin Liu and Pengfei Liu and Arman Cohan",
        "title": "Understanding Reference Policies in Direct Preference Optimization"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "christiano2017deep",
        "author": "Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario",
        "title": "Deep reinforcement learning from human preferences"
      },
      {
        "key": "ziegler2019fine",
        "author": "Ziegler, Daniel M and Stiennon, Nisan and Wu, Jeffrey and Brown, Tom B and Radford, Alec and Amodei, Dario and Christiano, Paul and Irving, Geoffrey",
        "title": "Fine-tuning language models from human preferences"
      },
      {
        "key": "learn2020summ",
        "author": "Nisan Stiennon and\nLong Ouyang and\nJeff Wu and\nDaniel M. Ziegler and\nRyan Lowe and\nChelsea Voss and\nAlec Radford and\nDario Amodei and\nPaul F. Christiano",
        "title": "Learning to summarize from human feedback"
      },
      {
        "key": "ouyang2022training",
        "author": "Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others",
        "title": "Training language models to follow instructions with human feedback"
      },
      {
        "key": "bai2022training",
        "author": "Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others",
        "title": "Training a helpful and harmless assistant with reinforcement learning from human feedback"
      },
      {
        "key": "li2023remax",
        "author": "Li, Ziniu and Xu, Tian and Zhang, Yushun and Lin, Zhihang and Yu, Yang and Sun, Ruoyu and Luo, Zhi-Quan",
        "title": "Remax: A simple, effective, and efficient reinforcement learning method for aligning large language models"
      },
      {
        "key": "chan2024dense",
        "author": "Chan, Alex J and Sun, Hao and Holt, Samuel and van der Schaar, Mihaela",
        "title": "Dense reward for free in reinforcement learning from human feedback"
      },
      {
        "key": "ji2024self",
        "author": "Ji, Xiang and Kulkarni, Sanjeev and Wang, Mengdi and Xie, Tengyang",
        "title": "Self-Play with Adversarial Critic: Provable and Scalable Offline Alignment for Language Models"
      }
    ]
  },
  {
    "index": 20,
    "papers": [
      {
        "key": "xu2024is",
        "author": "Shusheng Xu and Wei Fu and Jiaxuan Gao and Wenjie Ye and Weilin Liu and Zhiyu Mei and Guangju Wang and Chao Yu and Yi Wu",
        "title": "Is {DPO} Superior to {PPO} for {LLM} Alignment? A Comprehensive Study"
      },
      {
        "key": "tajwar2024preference",
        "author": "Tajwar, Fahim and Singh, Anikait and Sharma, Archit and Rafailov, Rafael and Schneider, Jeff and Xie, Tengyang and Ermon, Stefano and Finn, Chelsea and Kumar, Aviral",
        "title": "Preference fine-tuning of llms should leverage suboptimal, on-policy data"
      },
      {
        "key": "tang2024understanding",
        "author": "Tang, Yunhao and Guo, Daniel Zhaohan and Zheng, Zeyu and Calandriello, Daniele and Cao, Yuan and Tarassov, Eugene and Munos, R{\\'e}mi and Pires, Bernardo {\\'A}vila and Valko, Michal and Cheng, Yong and others",
        "title": "Understanding the performance gap between online and offline alignment algorithms"
      }
    ]
  },
  {
    "index": 21,
    "papers": [
      {
        "key": "xu2023some",
        "author": "Xu, Jing and Lee, Andrew and Sukhbaatar, Sainbayar and Weston, Jason",
        "title": "Some things are more cringe than others: Preference optimization with the pairwise cringe loss"
      },
      {
        "key": "guo2024direct",
        "author": "Guo, Shangmin and Zhang, Biao and Liu, Tianlin and Liu, Tianqi and Khalman, Misha and Llinares, Felipe and Rame, Alexandre and Mesnard, Thomas and Zhao, Yao and Piot, Bilal and others",
        "title": "Direct language model alignment from online ai feedback"
      },
      {
        "key": "yuanself",
        "author": "Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Li, Xian and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason E",
        "title": "Self-Rewarding Language Models"
      },
      {
        "key": "chen2024bootstrapping",
        "author": "Chen, Changyu and Liu, Zichen and Du, Chao and Pang, Tianyu and Liu, Qian and Sinha, Arunesh and Varakantham, Pradeep and Lin, Min",
        "title": "Bootstrapping Language Models with DPO Implicit Rewards"
      },
      {
        "key": "dong2024rlhf",
        "author": "Dong, Hanze and Xiong, Wei and Pang, Bo and Wang, Haoxiang and Zhao, Han and Zhou, Yingbo and Jiang, Nan and Sahoo, Doyen and Xiong, Caiming and Zhang, Tong",
        "title": "Rlhf workflow: From reward modeling to online rlhf"
      }
    ]
  },
  {
    "index": 22,
    "papers": [
      {
        "key": "wu2024self",
        "author": "Wu, Yue and Sun, Zhiqing and Yuan, Huizhuo and Ji, Kaixuan and Yang, Yiming and Gu, Quanquan",
        "title": "Self-play preference optimization for language model alignment"
      }
    ]
  },
  {
    "index": 23,
    "papers": [
      {
        "key": "chen2024self",
        "author": "Chen, Zixiang and Deng, Yihe and Yuan, Huizhuo and Ji, Kaixuan and Gu, Quanquan",
        "title": "Self-play fine-tuning converts weak language models to strong language models"
      },
      {
        "key": "li2024getting",
        "author": "Jiaxiang Li and Siliang Zeng and Hoi To Wai and Chenliang Li and Alfredo Garcia and Mingyi Hong",
        "title": "Getting More Juice Out of the {SFT} Data: Reward Learning from Human Demonstration Improves {SFT} for {LLM} Alignment"
      }
    ]
  }
]