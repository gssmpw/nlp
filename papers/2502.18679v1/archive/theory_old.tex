\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%%%%%%%%%%%%%%%%%% Added package%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{algorithm,algorithmic}
\usepackage{paralist,amsmath, amssymb,bm}
\usepackage{multirow}
\usepackage{ntheorem}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}
\newtheorem{cor}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{asm}{Assumption}
\usepackage{enumitem}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{fancybox}
\usepackage{mathtools}

%%%% mathcal
\def \S {\mathbf{S}}
\def \L {\mathcal{L}}
\def \A {\mathcal{A}}
\def \X {\mathcal{X}}
\def \O {\mathcal{O}}
\def \Y {\mathcal{Y}}
\def \Ab {\bar{\A}}
\def \R {\mathbb{R}}
\def \Kt {\widetilde{K}}
\def \k {\mathbf{k}}
\def \w {\mathbf{w}}
\def \v {\mathbf{v}}
\def \t {\mathbf{t}}
\def \x {\mathbf{x}}
\def \Se {\mathcal{S}}
\def \E {\mathrm{E}}
\def \Rh {\widehat{R}}
\def \x {\mathbf{x}}
\def \p {\mathbf{p}}
\def \a {\mathbf{a}}
\def \diag {\mbox{diag}}
\def \b {\mathbf{b}}
\def \e {\mathbf{e}}
\def \ba {\boldsymbol{\alpha}}
\def \c {\mathbf{c}}
\def \tr {\mbox{tr}}
\def \d {\mathbf{d}}
\def \db {\bar\mathbf{d}}
\def \1 {\mathbf{1}}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def \z {\mathbf{z}}
\def \s {\mathbf{s}}
\def \bh {\widehat{\b}}
\def \y {\mathbf{y}}
\def \u {\mathbf{u}}
\def \uh {\widehat{\u}}
\def \H {\mathcal{H}}
\def \g {\mathbf{g}}
\def \F {\mathcal{F}}
\def \I {\mathbb{I}}
\def \P {\mathcal{P}}
\def \Q {\mathcal{Q}}
\def \xh {\widehat{\x}}
\def \wh {\widehat{\w}}
\def \lambdah {\widehat{\lambda}}

\def \ah {\widehat{\a}}
\def \Rc {\mathcal R}
\def \Sigmah {\widehat\Sigma}

\def \Bh {\widehat B}
\def \Ah {\widehat A}
\def \Uh {\widehat U}
\def \Ut {\widetilde U}
\def \B {\mathcalB}
\def \C {\mathbf C}
\def \U {\mathbf U}
\def \Kh {\widehat K}
\def \fh {\widehat f}
\def \yh {\widehat\y}
\def \Xh {\widehat{X}}
\def \Fh {\widehat{F}}

\def \m {\mathbf{m}}
\def \y {\mathbf{y}}
\def \E {\mathrm{E}}
\def \x {\mathbf{x}}
\def \g {\nabla{g}}
\def \D {\mathcal{D}}
\def \z {\mathbf{z}}
\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \Z {\mathcal{Z}}
\def \Pc {\mathcal{P}}
\def \w {\mathbf{w}}
\def \s {\mathbf{s}}
\def \r {\mathbf{r}}
\def \R {\mathbb{R}}
\def \S {\mathcal{S}}
\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \Q {\mathcal{Q}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \q {\mathbf{q}}
\def \v {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}

\def \I {\mathbb{I}}
\def \xt {\widetilde{\x}}
\def \yt {\widetilde{\y}}
\def \hrho {\hat{\rho}}

\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
\def \B {\mathcal{B}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \G {\mathcal {G}}
\def \bh {\widehat{b}}
\def \wh {\widehat{\w}}
\def \Dth {\widehat{\Delta}}
\def \vb {\bar{\mathbf v}}
\def \zt {\widetilde{\z}}
\def \zh {\widehat{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\widehat{\g}}
\def \vh {\widehat{\v}}
\def \Sh {\widehat{S}}
\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{L}}
\def \t {\mathbf{t}}
\def \xh {\widehat{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\m}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \X {\mathcal{X}}
\def \nb {\widehat{\nabla}}
\def \K {\mathcal{K}}
\def \P {\mathbb{P}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
\def \Rt {\mathcal{R}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \zu {\underline{\z}}
\def \vect {\text{vec}}
\def \E {\mathbb{E}}
\def \bftau {\boldsymbol{\tau}}
\def\Tau{{\rm T}}
\def\bw{\mathbf{w}}
\def \start {\textsc{Start}}

\usepackage[colorinlistoftodos,bordercolor=green,backgroundcolor=white,linecolor=green,textsize=scriptsize]{todonotes}
\newcommand{\bokun}[1]{\todo[inline]{\footnotesize \textbf{\color{red!40} } #1}}



\title{Finetuning Generative Language Models with A Discriminative Approach}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Tianbao Yang \\
  % Department of Computer Science and Engineering \\
  Texas A\&M University \\
   College Station, USA \\
  \texttt{tianbao-yang@tamu.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}


\begin{document}


\maketitle


\section{Connection to Previous Works}

\subsection{A Distributionally Robust Generalization of SPIN}

We abuse the notation $p$ to represent a probability distribution or its density function. The true risk of SPIN~\cite{chen2024self} can be written as
\begin{align}\label{eq:spin_risk}
% \L_{\text{SPIN}}(\theta) = \E_{\x\sim q(\cdot),\y\sim p_{\mathrm{data}}(\cdot\mid \x),\y'\sim p_{\mathrm{ref}}(\cdot\mid \x)} \left[\ell\left(\lambda \log\frac{p_\theta(\y\mid \x)}{p_{\mathrm{ref}}(\y\mid \x)} - \lambda \log\frac{p_\theta(\y'\mid \x)}{p_{\mathrm{ref}}(\y'\mid \x)}\right)\right],
& \L(\theta) = \E_{(\x,\y)}\left[L(\theta;\x,\y)\right],\quad L(\theta;\x,\y) = \E_{\y'\sim p_{\mathrm{ref}}(\cdot\mid \x)}\left[\phi(\theta;\x,\y,\y')\right],\\\nonumber
& \phi(\theta;\x,\y,\y') = \ell\left(\lambda \log\frac{p_\theta(\y\mid \x)}{p_{\mathrm{ref}}(\y\mid \x)} - \lambda \log\frac{p_\theta(\y'\mid \x)}{p_{\mathrm{ref}}(\y'\mid \x)}\right),
\end{align}
where $\ell(\cdot)$ is a monotonically decreasing and convex penalty function, %$q(\cdot)$ is the marginal density function of the prompt $\x$, $p_{\mathrm{data}}(\cdot\mid \x)$ is the \emph{unknown} groundtruth density of the response $\y$ conditioned on a prompt $\x$, 
$p_\theta$ is the generative LLM parametrized by $\theta$, $p_{\mathrm{ref}}$ is a weak reference LLM, and $\lambda$ controls the strength of the KL regularization term between $p_\theta$ and $p_{\mathrm{ref}}$. In \cite{chen2024self}, the authors choose $p_{\mathrm{ref}} = p_{\theta_0}$ (where $\theta_0$ is the supervised fine-tuned model and the initial model of fine-tuning).

In the fine-tuning task, there is a dataset of $n$ high-quality prompt-response pairs $\{(\x_i,\y_i)\}_{i=1}^n$. 
% where $\x_i\sim q(\cdot)$, $\y_i\sim p_{\mathrm{data}}(\cdot\mid \x_i)$. 
For each prompt $\x_i$,  we can generate a set of $m$ synthetic responses $\S_i=\{\y_{ij}'\}_{j=1}^m$ by the reference model, where $\y_{ij}'\sim p_{\mathrm{ref}}(\cdot\mid \x_i)$. Note that large $m$ leads to smaller variance of estimation. Thus, the empirical objective of SPIN becomes:
\begin{align}\label{eq:emp_spin}
& \hat{\L}(\theta) = \frac{1}{n}\sum_{i=1}^n \hat{L}(\theta;\x_i,\y_i),\quad \hat{L}(\theta;\x_i,\y_i) = \frac{1}{m}\sum_{j=1}^m \phi(\theta;\x_i,\y_i,\y_{ij}'),%\\\label{eq:pairwise_loss}
%& \hat{\phi}(\theta;\x_i,\y_i,\y_{ij}')\coloneqq  \ell\left(\lambda \log\frac{p_\theta(\y_i\mid \x_i)}{p_{\mathrm{ref}}(\y_i\mid \x_i)} - \lambda \log\frac{p_\theta(\y_{ij}'\mid \x_i)}{p_{\mathrm{ref}}(\y_{ij}'\mid \x_i)}\right).
\end{align}
Borrowing the idea of distributional robust optimization (DRO), we can generalize the empirical SPIN loss $\hat{\L}$ to the following:
\begin{align}\nonumber
& \hat{\L}_{\text{KL-pen}}(\theta) = \frac{1}{n}\sum_{i=1}^n \hat{L}_{\text{KL-pen}}(\theta;\x_i,\y_i),\\\label{eq:dro_emp_spin_max_subprob}
& \hat{L}_{\text{KL-pen}}(\theta;\x_i,\y_i) \coloneqq  \max_{\boldsymbol{\pi}_i\in\Delta_m} \left\{\sum_{j=1}^m \pi_{ij} \phi(\theta;\x_i,\y_i,\y_{ij}') - \tau D_\mathrm{KL}\left(\boldsymbol{\pi}_i\vert\vert \mathbf{1}/m\right)\right\},
\end{align}
where $\tau\geq 0$, $\Delta_m\subset \R^m$ is  probability simplex, and $\mathbf{1}/m$ denotes the uniform distribution on $[m]$. The maximization subproblem in \eqref{eq:dro_emp_spin_max_subprob} has the following closed-form solution. We can show that \eqref{eq:dro_emp_spin_max_subprob} is equivalent to the following expression.
\begin{align*}
\hat{L}_{\text{KL-pen}}(\theta;\x_i,\y_i) = \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\phi(\theta;\x_i,\y_i,\y_{ij}')/\tau\right)\right).
\end{align*}
Then, we can derive the following empirical objective function of DRO-SPIN
\begin{align}\label{eq:dro_emp_spin}
\hat{\L}_{\text{KL-pen}}(\theta) = \frac{1}{n}\sum_{i=1}^n \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\phi(\theta;\x_i,\y_i,\y_{ij}')/\tau\right)\right).
\end{align}
For the objective in \eqref{eq:dro_emp_spin}, we have the following remarks:
\begin{itemize}
    \item When $\tau\rightarrow\infty$, DRO-SPIN in \eqref{eq:dro_emp_spin} becomes the vanilla SPIN in \eqref{eq:emp_spin}.
    \item When $\tau<\infty$, DRO-SPIN puts more weights on the ``hard negatives'' $\y_{ij}'$ with high degrees of belief that $\y_{ij}'$ originates from $p_{\mathrm{data}}$. 
\end{itemize}

\subsection{Our Approach and Its Connection to DRO-SPIN}

The true risk of our negative likelihood minimization approach can be written as
\begin{align*}
\L(\theta) & = -\E_{\x\sim q(\cdot),\y\sim p_{\mathrm{data}}(\cdot\mid \x)}\left[\log \frac{\exp\left(\log p_\theta(\r\mid \x)/\tau\right)}{\int_{\Y}\exp\left(\log p_\theta(\r\mid \x)/\tau\right)d\r }\right],\\
& = \E_{\x\sim q(\cdot),\y\sim p_{\mathrm{data}}(\cdot\mid \x)}\left[-\log p_\theta(\y\mid\x) + \tau \log \left(\int_{\Y}\exp\left(\log p_\theta(\r\mid \x)/\tau\right)d\r \right)\right].
\end{align*}
Given the dataset of $n$ high-quality prompt-response pairs $\{(\x_i,\y_i)\}_{i=1}^n$, we can define our ``semi-empirical'' risk
\begin{align*}
\dot{\L}(\theta) & = -\frac{1}{n}\sum_{i=1}^n \log p_\theta(\y_i\mid\x_i) + \tau \frac{1}{n}\sum_{i=1}^n \log \left(\int_{\Y}\exp\left(\log p_\theta(\r\mid \x_i)/\tau\right)d\r \right).
\end{align*}
The ``semi-empirical'' risk $\dot{\L}(\theta)$ is not tractable due to the challenge in estimating the partition function $\int_{\Y}\exp\left(\log p_\theta(\r\mid \x_i)/\tau\right)d\r$. We tackle this challenge by exploiting the Importance Sampling (IS) based Monte-Carlo approach with the initial (weak) LLM $p_{\theta_0}(\cdot\mid \x)$. Note that
\begin{align*}
\int_{\Y}\exp\left(\log p_\theta(\r\mid \x_i)/\tau\right)d\r = \int_{\Y} p_\theta(\r\mid \x_i)^{1/\tau} d\r = \E_{\y'\sim p_{\theta_0}(\cdot\mid \x_i)} \left[\frac{p_\theta(\r\mid \x_i)^{1/\tau}}{p_{\theta_0}(\y'\mid \x_i)}\right].
\end{align*}
We generate a set of $m$ synthetic responses $\S_i=\{\y_{ij}'\}_{j=1}^m$ for each prompt $\x_i$ by the reference model, where $\y_{ij}'\sim p_{\theta_0}(\cdot\mid \x_i)$. Then, our empirical risk can be defined as
\begin{align}\label{eq:emp_x}
\hat{\L}(\theta) & = -\frac{1}{n}\sum_{i=1}^n \log p_\theta(\y_i\mid\x_i) + \tau \frac{1}{n}\sum_{i=1}^n \log \left(\frac{1}{m}\sum_{j=1}^m\frac{p_\theta(\y_{ij}'\mid \x_i)^{1/\tau}}{p_{\theta_0}(\y_{ij}'\mid \x_i)}\right).
\end{align}

\begin{prop}
When $\lambda= 1$, minimizing our empirical risk $\hat{\L}(\theta)$ in \eqref{eq:emp_x} is equivalent to minimizing the empirical risk in \eqref{eq:dro_emp_spin} of DRO-SPIN with correlation loss $\ell(t) = 1-t$ and $p_{\text{ref}} = p_{\theta_0}^\tau$. 
\end{prop}
\begin{proof}
When $\ell(t) = 1-t$, the empirical objective function of DRO-SPIN can be written as
\begin{align*}
& \hat{\L}_{\text{KL-pen}}(\theta)   = 1 + \frac{1}{n}\sum_{i=1}^n \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\frac{\lambda}{\tau} \log\frac{p_\theta(\y_{ij}'\mid \x_i)}{p_{\mathrm{ref}}(\y_{ij}'\mid \x_i)} - \frac{\lambda}{\tau} \log\frac{p_\theta(\y_i\mid \x_i)}{p_{\mathrm{ref}}(\y_i\mid \x_i)}\right)\right)\\
& = 1 + \frac{1}{n}\sum_{i=1}^n \tau \log \left(\exp\left(-\frac{\lambda}{\tau} \log\frac{p_\theta(\y_i\mid \x_i)}{p_{\theta_0}(\y_i\mid \x_i)^\tau}\right)\left(\frac{1}{m}\sum_{j=1}^m \exp\left(\frac{\lambda}{\tau} \log\frac{p_\theta(\y_{ij}'\mid \x_i)}{p_{\theta_0}(\y_{ij}'\mid \x_i)^\tau}\right)\right)\right)\\
& =1 - \lambda \frac{1}{n}\sum_{i=1}^n \log p_\theta(\y_i\mid \x_i) + \lambda \frac{1}{n}\sum_{i=1}^n \log p_{\theta_0}(\y_i\mid \x_i) + \tau \frac{1}{n}\sum_{i=1}^n \log \left(\frac{1}{m}\sum_{j=1}^m \frac{p_\theta(\y_{ij}'\mid \x_i)^{\lambda/\tau}}{p_{\theta_0}(\y_{ij}'\mid \x_i)^\lambda}\right).
\end{align*}
Note that the term $1+\lambda \frac{1}{n}\sum_{i=1}^n \log p_{\mathrm{ref}}(\y_i\mid \x_i)^\tau$ does not depend on $\theta$. Thus, $\min_{\theta} \hat{\L}_{\text{KL-pen}}(\theta)$ is equivalent to $\min_{\theta} \hat{\L}(\theta)$ when $\lambda= 1$.
\end{proof}

% \bokun{Consider the online inner problem. Replace the summation from 1 to $m$ with expectation.}

% \bokun{Show the benefit over SPIN through the generalization bounds of the DRO formulation and ERM?}

% \section{Variance Regularization and Excess Risk Bound}

% \bokun{1. SPIN's true risk and (constrained) DRO-SPIN's true risk;\\2. Justification of convexity, NTK, Linear regime (See \href{https://proceedings.neurips.cc/paper_files/paper/2023/file/d28077e5ff52034cd35b4aa15320caea-Paper-Conference.pdf}{this paper});\\3. Decomposition: from true risk to semi-empirical \& from semi-empirical to empirical;\\4. Bounds for SPIN and DRO-SPIN.}

% First, the true risk of DRO-SPIN with KL uncertainty set constraint can be written as 
% \begin{align*}
% & \L_{\text{KL}}(\theta) = \E_{(\x,\y)}\left[L_{\text{KL}}(\theta;\x,\y)\right],\\
% & L_{\text{KL}}(\theta;\x,\y) = \sup_{p(\cdot\mid \x)}\left\{\E_{\y'\sim p(\cdot\mid \x)}\left[\phi(\theta;\x,\y,\y')\right] : D_\mathrm{KL}(p\vert\vert \hat{p}_m) \leq \frac{\rho}{m}\right\},
% \end{align*}
% where $\hat{p}_m(\cdot\mid \x)$ is the empirical measure over $\{\y_j'(\x)\}_{j=1}^m$. \textcolor{red}{This makes no sense because we need a sample $\{\y_j'(\x)\}_{j=1}^m$ for each $\x$.}

% Now we consider the semi-empirical risk of DRO-SPIN with KL uncertainty set constraint.
% \begin{align}\label{eq:semi_risk_dro_spin}
% & \dot{\L}_{\text{KL}}(\theta) =\frac{1}{n}\sum_{i=1}^n L_{\text{KL}}(\theta;\x_i,\y_i),\\\nonumber
% & L_{\text{KL}}(\theta;\x_i,\y_i) = \sup_{p_i}\left\{\E_{\y'\sim p_i}\left[\phi(\theta;\x_i,\y_i,\y')\right] : D_\mathrm{KL}(p_i\vert\vert \hat{p}_i^m) \leq \frac{\rho}{m}\right\},
% \end{align}
% where $p_i$ denotes the conditional density $p(\cdot\mid \x_i)$ and $\hat{p}_i^m$ is the empirical measure over $\{\y_{ij}'\}_{j=1}^m$, $\y_{ij}'\stackrel{\text{i.i.d.}}{\sim} p_{\mathrm{ref}}(\cdot\mid \x_i)$. As a comparison, we can define semi-empirical risk of SPIN as
% \begin{align}\label{eq:semi_risk_spin}
% \dot{\L}(\theta) =  \frac{1}{n}\sum_{i=1}^n L(\theta;\x_i,\y_i),\quad L(\theta;\x_i,\y_i) = \E_{\y'\sim p_{\mathrm{ref}}(\cdot\mid \x_i)}\left[\phi(\theta;\x_i,\y_i,\y')\right]
% \end{align}

% Suppose that (i) the domain $\Theta$ of $\theta$ is compact; (ii) $\phi(\theta;\x,\y,\y')$ is $M$-Lipschitz continuous to $\theta$; (iii) $\E_{\y'\sim p_{\mathrm{ref}}(\cdot\mid \x_i)}\left[|\phi(\theta;\x_i,\y_i,\y')|^2\right] < \infty$. Then, Theorem 2 in \cite{duchi2021statistics} implies
% \begin{align*}
% \frac{1}{n}\sum_{i=1}^n \sup_{p_i:D_{\mathrm{KL}}(p_i\vert\vert \hat{p}_i^m) \leq \frac{\rho}{m}}\E_{\y'\sim p_i}\left[\phi(\theta;\x_i,\y_i,\y')\right]  = \hat{\L}(\theta) + \frac{C_1}{n}\sum_{i=1}^n \sqrt{\frac{\rho}{m}\mathrm{Var}_{\hat{p}_i^m}(\phi(\theta;\x_i,\y_i,\y'))} + \epsilon_m(\theta),
% \end{align*}
% where $C_1>0$, $\hat{p}_i^m$ is the empirical measure over $\{\y_{ij}'\}_{j=1}^m$, $\y_{ij}'\stackrel{\text{i.i.d.}}{\sim} p_{\mathrm{ref}}(\cdot\mid \x_i)$, $\hat{\L}(\theta) = \frac{1}{n}\sum_{i=1}^n \frac{1}{m}\sum_{j=1}^m \phi(\theta;\x_i,\y_i,\y_{ij}')$, $\sup_\theta \sqrt{m}\epsilon_m(\theta) \stackrel{p_i^*}{\rightarrow} 0$. 


\section{``Consistency''}
We define the empirical robustly regularized risk
\begin{align}\label{eq:emp_rob_risk}
\hat{\L}_{\text{KL}}(\theta) = \frac{1}{n}\sum_{i=1}^n \hat{L}_{\text{KL}}(\theta;\x_i,\y_i),\quad \hat{L}_{\text{KL}}(\theta;\x_i,\y_i) = \sup_{p_i}\left\{\E_{\y'\sim p_i} [\phi(\x_i,\y_i,\y')]:D_{\mathrm{KL}}(p_i\vert\vert\hat{p}_i^m)\leq \frac{\rho_i}{m}\right\}.
\end{align}
Let $\hat{p}_i^m$ be the empirical measure over $\{\y_{ij}'\}_{j=1}^m$. We want to bound
\begin{align*}
& \sup_\theta \left\{\hat{\L}_{\text{KL}}(\theta) - \L(\theta)\right\}\\
& \leq \sup_\theta \left\{\hat{\L}_{\text{KL}}(\theta) - \hat{\L}(\theta)\right\} + \sup_\theta\left\{\hat{\L}(\theta) - \L(\theta)\right\}\\
& \leq \frac{1}{n}\sum_{i=1}^n \sup_\theta \sup_{p_i:p_i\ll \hat{p}_i^m} \left\{\left|\E_{\y'\sim p_i}[\phi(\x_i,\y_i,\y')] - \E_{\y'\sim p_{\mathrm{ref}}}[\phi(\theta;\x_i,\y_i,\y')]\right|:D_{\mathrm{KL}}(p_i\vert\vert\hat{p}_i^m)\leq \frac{\rho_i}{m}\right\}\\
& \quad\quad + \sup_\theta\left|\frac{1}{n}\sum_{i=1}^n L(\theta;\x_i,\y_i) - \E_{(\x,\y)}[L(\theta;\x,\y)]\right|.
\end{align*}

\begin{asm}
Let $\Theta$ be convex, compact and $\phi(\theta;\x_i,\y_i,\y')$ be continuous in $\theta$ for some $\x_i\in\X,\y_i\in\Y$ and almost all $\y'\in \Y$. There exists $C(\y'):\Y\rightarrow\R_+$ such that $|\phi(\theta;\x_i,\y_i,\y')|\leq Z(\y')$ for all $\theta\in\Theta$ and $\E[Z(\y')^{1+\epsilon}] < \infty$ for some $\epsilon>0$.
\end{asm}

\begin{asm}
Let $L(\theta;\x,\y)$ be continuous in $\theta$ for almost all $\x,\y\in \X\times\Y$. There exists $C(\x,\y):\X\times\Y\rightarrow\R_+$ such that $|L(\theta;\x,\y)|\leq C(\x,\y)$ for all $\theta\in\Theta$ and $\E[C(\x,\y)] < \infty$.
\end{asm}

Thus, $\bar{\H}=\{L(\theta;\cdot):\theta\in\Theta\}$ is a Glivenko-Cantelli class (See e.g. the definition in \cite{bartlett2008glivenko}). Let $q$ be the product measure corresponding to the real data on $\X\times\Y$, $\hat{q}_n$ be the empirical measure over the sample $\{(\x_i,\y_i)\}_{i=1}^n$, and $\hat{q}_n^*$ is the outer measure of $\hat{q}_n$.
\begin{align}\label{eq:gc_1}
& \sup_\theta\left|\frac{1}{n}\sum_{i=1}^n L(\theta;\x_i,\y_i) - \E_{(\x,\y)}[L(\theta;\x,\y)]\right| \stackrel{\hat{q}_n^*}{\rightarrow}0.
\end{align}
Moreover, $\H_i=\{\phi(\theta;\x_i,\y_i,\cdot):\theta\in\Theta\}$ is Glivenko-Cantelli. Theorem 7 in \cite{duchi2021statistics} implies that
\begin{align}\label{eq:gc_2}
\sup_\theta \sup_{p_i:p_i\ll \hat{p}_i^m} \left\{\left|\E_{\y'\sim p_i}[\phi(\x_i,\y_i,\y')] - \E_{\y'\sim p_{\mathrm{ref}}(\cdot\mid \x_i)}[\phi(\theta;\x_i,\y_i,\y')]\right|:D_{\mathrm{KL}}(p_i\vert\vert\hat{p}_i^m)\leq \frac{\rho_i}{m}\right\} \stackrel{p_i^*}{\rightarrow}0.
\end{align}
Combining \eqref{eq:gc_1} and \eqref{eq:gc_2}, we can conclude that
\begin{align}\label{eq:uniform_converge}
\sup_\theta \left\{\hat{\L}_{\text{KL}}(\theta) - \L(\theta)\right\} \leq \sup_\theta \varepsilon^n(\theta) + \frac{1}{n}\sum_{i=1}^n \sup_\theta \varepsilon_i^m(\theta),
\end{align}
where $\sup_\theta \varepsilon^n(\theta) \stackrel{\hat{q}_n^*}{\rightarrow} 0$ and $\sup_\theta\sup_{p_i:p_i\ll \hat{p}_i^m} \varepsilon_i^m(\theta) \stackrel{p_i^*}{\rightarrow} 0$.

\begin{asm}\label{asm:linearized}
Suppose that $p_\theta(\y\mid \x)$ is linear in $\theta$ throughout the fine-tuning process for any $\x,\y$.   
\end{asm}

As shown in previous work, during fine-tuning, the first-order approximation $p_\theta(\y\mid \x)\approx p_{\theta_0}(\y\mid \x) + (\theta - \theta_0)^\top \nabla_\theta p_{\theta_0}(\y\mid \x)$ is valid since parameter evolution is minimal~\cite{deshpande2021linearized,ortiz2024task}. Under Assumption~\ref{asm:linearized}, $\phi(\theta;\x,\y,\y')$ is convex in $\theta$ such that $\L(\theta)$ in \eqref{eq:spin_risk} is convex in $\theta$.

Since $\L(\theta)$ is convex on a compact $\Theta$, the infimum $\inf_\theta L(\theta)$ is attained on a convex subset $S^*\subset \Theta$. For any $\theta^*\in S^*$, \eqref{eq:uniform_converge} implies that
\begin{align}\label{eq:consistency}
\inf_\theta \hat{\L}_{\text{KL}}(\theta) - \L(\theta^*) & \leq \hat{\L}_{\text{KL}}(\theta^*) - \L(\theta^*) \leq \sup_\theta \varepsilon^n(\theta) + \frac{1}{n}\sum_{i=1}^n \sup_\theta \varepsilon_i^m(\theta).
\end{align}

\section{Generalization Bounds}

\subsection{SPIN}

\begin{asm}\label{eq:bounded_loss}
Suppose that $\phi(\theta;\x,\y,\y')$ is uniformly bounded, i.e., there exists $C>0$ such that $\phi(\theta;\x,\y,\y') \in [0, M]$ for any $\theta\in\Theta$ and any $\x,\y,\y'$.    
\end{asm}

We define $\hat{\theta}_\text{SPIN}=\argmin_{\theta\in\Theta} \hat{\L}(\theta)$, where the empirical SPIN risk $\hat{\L}$ is defined in \eqref{eq:emp_spin}. Suppose that there exists $\theta^*\in\Theta$ such that $\L(\theta^*) = \inf_{\theta\in\Theta}\L(\theta)$. We further define the ``semi-empirical risks''
\begin{align*}
& \dot{\L}(\theta) = \frac{1}{n}\sum_{i=1}^n L(\theta;\x_i,\y_i) = \frac{1}{n}\sum_{i=1}^n \E_{\y'\sim p_\text{ref}(\cdot\mid \x_i)}[\phi(\theta;\x_i,\y_i,\y')].
\end{align*}

\begin{asm}\label{asm:bounded_var}
Suppose that $\mathrm{Var}[L(\theta^*;\x,\y)] = \E_{(\x,\y)}[(L(\theta^*;\x,\y) - \L(\theta^*))^2] \leq \sigma^2$ for $\L(\theta^*) = \inf_\theta \L(\theta)$ and some $\sigma_*^2>0$. Moreover, for each $i\in[n]$, $\mathrm{Var}[\phi(\theta^*;\x_i,\y_i,\y')] = \E_{\y'}[(\phi(\theta^*;\x_i,\y_i,\y') - L(\theta^*;\x_i,\y_i))^2] \leq \sigma_{*,i}^2$ for some $\sigma_{*,i}^2>0$.
\end{asm}


The excess risk bound $\L(\hat{\theta}_\text{SPIN}) -  \L(\theta^*)$ of SPIN can be decomposed as
\begin{align}\nonumber
& \L(\hat{\theta}_\text{SPIN}) -  \L(\theta^*)  = \L(\hat{\theta}_\text{SPIN}) - \hat{\L}(\hat{\theta}_\text{SPIN}) + \hat{\L}(\hat{\theta}_\text{SPIN}) -  \L(\theta^*)\\\nonumber
& \leq \L(\hat{\theta}_\text{SPIN}) - \hat{\L}(\hat{\theta}_\text{SPIN}) + \hat{\L}(\theta^*) -  \L(\theta^*)\\\label{eq:spin_decomp}
& = \underbrace{\L(\hat{\theta}_\text{SPIN}) - \dot{\L}(\hat{\theta}_\text{SPIN})}_{\text{I.}} + \underbrace{\dot{\L}(\hat{\theta}_\text{SPIN}) - \hat{\L}(\hat{\theta}_\text{SPIN})}_{\text{II.}} + \underbrace{\hat{\L}(\theta^*) -  \dot{\L}(\theta^*)}_{\text{III.}} + \underbrace{\dot{\L}(\theta^*) -  \L(\theta^*)}_{\text{IV.}},
\end{align}
where the inequality utilizes $\hat{\L}(\theta^*) \geq \hat{\L}(\hat{\theta}_\text{SPIN})$. Note that
\begin{align*}
\text{I.} & = \L(\hat{\theta}_\text{SPIN}) - \dot{\L}(\hat{\theta}_\text{SPIN}) = \E_{(\x,\y)}[L(\hat{\theta}_\text{SPIN};\x,\y)] - \frac{1}{n}\sum_{i=1}^n L(\hat{\theta}_\text{SPIN};\x_i,\y_i).
\end{align*}
Due to the first inequality in Theorem 5.1 in Boucheron et al.~\cite{boucheron2005theory} and Sauer's Lemma, we can obtain that: For any $\delta\in(0,1)$, with probability at least $1-\delta$, any $\theta\in\Theta$ satisfies
\begin{align}\label{eq:I_bound}
\L(\theta)  - \dot{\L}(\theta)\leq  2\sqrt{\dot{\L}(\theta) \frac{d_{\bar{\H}}\log (n+1) + \log\frac{4}{\delta}}{n}} + 4\frac{2d_{\bar{\H}}\log (n+1) + \log\frac{4}{\delta}}{n},
\end{align}
where $d_{\bar{\H}}$ is the VC dimension of the hypothesis family $\bar{\H}=\{L(\theta;\cdot):\theta\in\Theta\}$. Next, we notice that the II. term in \eqref{eq:spin_decomp} can be written as
\begin{align*}
\text{II.} & =\dot{\L}(\hat{\theta}_\text{SPIN}) - \hat{\L}(\hat{\theta}_\text{SPIN}) = \frac{1}{n}\sum_{i=1}^n \left(\E_{\y'\sim p_\text{ref}(\cdot\mid \x_i)}[\phi(\theta;\x_i,\y_i,\y')] - \frac{1}{m}\sum_{j=1}^m \phi(\theta;\x_i,\y_i,\y_{ij}')\right).
\end{align*}
We define $\hat{L}(\theta;\x_i,\y_i)=\frac{1}{m}\sum_{j=1}^m \phi(\theta;\x_i,\y_i,\y_{ij}')$. Similarly, for each $i\in[n]$ and any $\delta\in(0,1)$, with probability at least $1-\delta$, any $\theta\in\Theta$ satisfies
\begin{align*}
& L(\theta;\x_i,\y_i) - \hat{L}(\theta;\x_i,\y_i)
\leq  2\sqrt{\hat{L}(\theta;\x_i,\y_i) \frac{2d_{\H_i}\log (m+1) + \log\frac{4}{\delta}}{m}} + 4\frac{2d_{\H_i}\log (m+1) + \log\frac{4}{\delta}}{m},
\end{align*}
where $\H_i=\{\phi(\theta;\x_i,\y_i,\cdot):\theta\in\Theta\}$. The union bound leads to: For any $\delta\in(0,1)$, with probability at least $1-\delta$, any $\theta\in\Theta$ satisfies
\begin{align}\label{eq:II_bound}
\dot{\L}(\theta) - \hat{\L}(\theta) \leq \frac{1}{n}\sum_{i=1}^n \left(2\sqrt{\hat{L}(\theta;\x_i,\y_i) \frac{2d_{\H_i}\log (m+1) + \log\frac{4n}{\delta}}{m}} + 4\frac{2d_{\H_i}\log (m+1) + \log\frac{4n}{\delta}}{m}\right).
\end{align}
Then, we apply Bennett's inequality (e.g. Theorem 3 in \cite{Maurer2009EmpiricalBB}): Each of the following holds with probability at least $1-\delta$ for $\theta=\theta^*$ and any $\delta\in(0,1)$:
\begin{align}\label{eq:III_bound}
& \hat{\L}(\theta^*) -  \dot{\L}(\theta^*) \leq \frac{1}{n}\sum_{i=1}^n \sqrt{2\sigma_{*,i}^2\frac{\log\frac{n}{\delta}}{m}}  + \frac{\log \frac{n}{\delta}}{3m},\\\label{eq:IV_bound}
&  \dot{\L}(\theta^*) - \L(\theta^*)\leq \sqrt{2\sigma_*^2\frac{\log\frac{1}{\delta}}{n}} + \frac{\log \frac{1}{\delta}}{3n}.
\end{align}
Note that $\dot{\L}(\hat{\theta}_\text{SPIN}) \in [0,M]$ and $\hat{L}(\hat{\theta}_\text{SPIN};\x_i,\y_i) \in [0, M]$. Thus, we can obtain the following excess risk bound: For any $\delta\in(0,1)$, it is satisfied with at least probability $1-\delta$ and some constant $C>0$ that
\begin{align}\nonumber
 \L(\hat{\theta}_\text{SPIN}) -  \L(\theta^*) 
& \leq C\frac{\max\left\{\sqrt{M\left(d_{\bar{\H}}\log (n+1) + \log\frac{1}{\delta}\right)}, \sqrt{\sigma_*^2\log\frac{1}{\delta}}\right\}}{\sqrt{n}}\\\nonumber
& + C\frac{\max\left\{\frac{1}{n}\sum_{i=1}^n\sqrt{M\left(d_{\H_i}\log (m+1) + \log\frac{n}{\delta}\right)}, \frac{1}{n}\sum_{i=1}^n \sqrt{\sigma_{*,i}^2\log\frac{n}{\delta}}\right\}}{\sqrt{m}}\\\label{eq:excess_spin}
& + C\frac{d_{\bar{\H}}\log (n+1) + \log\frac{1}{\delta}}{n} + C\frac{\frac{1}{n}\sum_{i=1}^nd_{\H_i}\log (m+1) + \log\frac{1}{\delta}}{m}.
\end{align}

\subsection{Constrained DRO SPIN}


\begin{asm}
Let $\Theta$ be compact. Besides, there exists $G:\X\times \Y\times \Y\rightarrow \R_+$ such that $\phi(\theta;\x,\y,\y')$ is $G(\x,\y,\y')$-Lipschitz continuous and $\E[G(\x,\y,\y')^2] < \infty$. 
\end{asm}


We define $\hat{\theta}_\text{KL}=\argmin_{\theta\in\Theta} \hat{\L}_\text{KL}(\theta)$, where the empirical robustly regularized risk $\hat{\L}_\text{KL}$ is defined in \eqref{eq:emp_rob_risk}. For any $\theta$, we consider the decomposition
\begin{align*}
& \L(\theta) = (\L(\theta) - \dot{\L}(\theta)) + \dot{\L}(\theta).
\end{align*}
% We can derive a uniform convergence result that depends on $\frac{V_n \log n}{\sqrt{n}}$ for the A. term (We need it to be uniform because we will plug in $\hat{\theta}_\text{KL}$, which is changing w.r.t. the sample).

For the first term $\L(\theta) - \dot{\L}(\theta)$ on the R.H.S., note that \eqref{eq:I_bound} still holds with probability at least $1-\delta$, for any $\delta\in(0,1)$ and $\theta\in\Theta$. Next, we apply Lemma C.1 in~\cite{duchi2019variance} (or Theorem 6 in~\cite{Maurer2009EmpiricalBB}) and Corollary 3.1 in~\cite{duchi2019variance}  to the $\dot{\L}(\theta)$ term. For some $\epsilon>0$, $m\geq \frac{8M^2}{t}$, $t_i\geq \log 12$, and $t_i=\log \frac{d_{\H_i}}{\delta} + (d_{\H_i}-1)\log \frac{8M e}{\epsilon}$, with probability at least $1-\delta$, we have
\begin{align*}
L(\theta;\x_i,\y_i) - \hat{L}(\theta;\x_i,\y_i) \leq 3\sqrt{\frac{2\widehat{\text{Var}}_i^m(\theta) t_i}{m}} + \frac{15Mt_i}{m} + 2\left(1+2\sqrt{\frac{2t_i}{m}}\right) \epsilon,\quad \forall \theta\in\Theta, 
\end{align*}
where $\widehat{\text{Var}}_i^m(\theta)=\frac{1}{m-1}\sum_{j=1}^m \left(\phi(\theta;\x_i,\y_i,\y_{ij}') - \frac{1}{m}\sum_{j=1}^m \phi(\theta;\x_i,\y_i,\y_{ij}')\right)^2$ is the sample variance on $\{\y_{ij}'\}_{j=1}^m$. Through the union bound, we can derive the following result. Denote that $\bar{t} = \frac{1}{n}\sum_{i=1}^n t_i = \frac{1}{n}\sum_{i=1}^n \left(\log \frac{d_{\H_i}}{\delta} + (d_{\H_i}-1)\log \frac{8M e}{\epsilon}\right)$. For some $\epsilon>0$, $m\geq \frac{8M^2}{t}$, $t_i\geq \log 12$, and $t_i=\log \frac{n d_{\H_i}}{\delta} + (d_{\H_i}-1)\log \frac{8M e}{\epsilon}$, with probability at least $1-\delta$, we have
\begin{align*}
\dot{\L}(\theta) - \hat{\L}(\theta) \leq \frac{3}{n}\sum_{i=1}^n \sqrt{\frac{2\widehat{\text{Var}}_i^m(\theta) t_i}{m}} + \frac{15M\bar{t}}{m} + 2\left(1+2\sqrt{\frac{2\bar{t}}{m}}\right) \epsilon,\quad \forall \theta\in\Theta,
\end{align*}
where we use $\frac{1}{n}\sum_{i=1}^n \sqrt{t_i} \leq \sqrt{\bar{t}}$. Since the KL-divergence satisfies Assumption A in~\cite{duchi2021statistics}, Theorem 2~\cite{duchi2021statistics} leads to the following expansion for $\hat{L}_{\text{KL}}(\theta;\x_i,\y_i)$ defined in \eqref{eq:emp_rob_risk}.
\begin{align*}
\hat{\L}_{\text{KL}}(\theta) = \frac{1}{n}\sum_{i=1}^n\hat{L}_{\text{KL}}(\theta;\x_i,\y_i) = \hat{\L}(\theta) + \frac{1}{n}\sum_{i=1}^n\sqrt{\frac{\rho_i}{m}\widehat{\text{Var}}_i^m(\theta) } + \frac{1}{n}\sum_{i=1}^n\varepsilon_i^m(\theta),\quad \forall \theta\in\Theta,
\end{align*}
where $\sup_\theta \varepsilon_i^m(\theta) = o_{p_i^*}(\frac{1}{\sqrt{m}})$, i.e., $\limsup_n \P_i^*(|\sup_\theta\varepsilon_i^m(\theta)|\geq \frac{c}{\sqrt{m}})=0$ holds for all $c>0$. \textcolor{red}{Set $\rho_i \geq 18t_i $ for each $i\in[n]$}. Thus, for any $\theta\in \Theta$, with probability at least $1-\delta$, 
\begin{align}\label{eq:dot_L}
\dot{\L}(\theta) & \leq  \hat{\L}_{\text{KL}}(\theta) + \frac{15M\bar{t}}{m} + 2\left(1+2\sqrt{\frac{2\bar{t}}{m}}\right) \epsilon + \frac{1}{n}\sum_{i=1}^n \xi_i^m(\theta),\quad \forall \theta\in\Theta,
\end{align}
where $\xi_i^m(\theta) = - \varepsilon_i^m(\theta) = o_{p_i^*}(\frac{1}{\sqrt{m}})$. 
Since $\hat{\theta}_\text{KL} = \argmin_\theta \hat{\L}_{\text{KL}}(\theta)$, with probability at least $1-\delta$,
\begin{align*}
\dot{\L}(\hat{\theta}_\text{KL}) \leq  \hat{\L}_{\text{KL}}(\theta) + \frac{15M\bar{t}}{m} + 2\left(1+2\sqrt{\frac{2\bar{t}}{m}}\right) \epsilon + \frac{1}{n}\sum_{i=1}^n \sup_\theta \xi_i^m(\theta),\quad \forall \theta\in\Theta.
\end{align*}
Utilizing Theorem 2 in~\cite{duchi2021statistics} and plugging in $\theta=\theta^*$ further leads to: With probability at least $1-\delta$, 
\begin{align*}
\dot{\L}(\hat{\theta}_\text{KL}) &\leq  \hat{\L}(\theta^*) + \frac{15M\bar{t}}{m} + 2\left(1+2\sqrt{\frac{2\bar{t}}{m}}\right) \epsilon + \frac{1}{n}\sum_{i=1}^n (\sup_\theta \xi_i^m(\theta) + \sup_\theta \varepsilon_i^m(\theta))  \\
& \quad\quad+ \frac{1}{n}\sum_{i=1}^n\sqrt{\frac{\rho_i}{m}\widehat{\text{Var}}_i^m(\theta^*)}.
\end{align*}
Define $\text{Var}_i(\theta) = \E_{\y'}[\phi(\theta;\x_i,\y_i,\y')^2] - L(\theta;\x_i,\y_i)^2$. Due to Lemma A.1 in \cite{duchi2019variance} and $\rho_i\geq 12 t_i$, with probability at least $1-\delta$, for any fixed $\theta=\theta^*$ we have
\begin{align*}
\dot{\L}(\hat{\theta}_\text{KL}) &\leq  \hat{\L}(\theta^*) + \frac{15M\bar{t}}{m} + 2\left(1+2\sqrt{\frac{2\bar{t}}{m}}\right) \epsilon + \frac{1}{n}\sum_{i=1}^n (\sup_\theta \xi_i^m(\theta) + \sup_\theta \varepsilon_i^m(\theta))  \\
& \quad\quad+ \frac{1}{n}\sum_{i=1}^n\sqrt{\frac{\rho_i}{m}\text{Var}_i(\theta^*)} +  \frac{1}{n}\sum_{i=1}^n\frac{\sqrt{2\rho_i M^2 \log \frac{1}{\delta}}}{m}.
\end{align*}
According to Assumption~\ref{asm:bounded_var}, we have $\text{Var}_i(\theta^*) \leq \sigma_{*,i}^2$. Same as \eqref{eq:III_bound} and \eqref{eq:IV_bound}, with probability at least $1-\delta$, we have
\begin{align*}
\hat{\L}(\theta^*) \leq  \L(\theta^*) + \frac{1}{n}\sum_{i=1}^n \sqrt{2\sigma_{*,i}^2\frac{\log\frac{n}{\delta}}{m}}  + \frac{\log \frac{n}{\delta}}{3m} + \sqrt{2\sigma_*^2\frac{\log\frac{1}{\delta}}{n}} + \frac{\log \frac{1}{\delta}}{3n}.
\end{align*}
We define $\bar{\rho} = \frac{1}{n}\sum_{i=1}^n \rho_i$. With a probability of at least $1-\delta$ and $\epsilon=\frac{1}{m}$, we have
\begin{align}\nonumber
\dot{\L}(\hat{\theta}_\text{KL}) &\leq  \L(\theta^*) + \frac{5M\bar{\rho}}{4m} + 2\left(1+2\sqrt{\frac{\bar{\rho}}{6m}}\right) \frac{1}{m} + \frac{1}{n}\sum_{i=1}^n (\sup_\theta \xi_i^m(\theta) + \sup_\theta \varepsilon_i^m(\theta))  \\\nonumber
& \quad\quad+ \frac{1}{n}\sum_{i=1}^n\sqrt{\frac{\rho_i}{m}\sigma_{*,i}^2} +  \frac{1}{n}\sum_{i=1}^n\frac{\sqrt{2\rho_i M^2 \log \frac{1}{\delta}}}{m}\\\label{eq:excess_dro}
& \quad\quad + \frac{1}{n}\sum_{i=1}^n \sqrt{2\sigma_{*,i}^2\frac{\log\frac{n}{\delta}}{m}}  + \frac{\log \frac{n}{\delta}}{3m} + \sqrt{2\sigma_*^2\frac{\log\frac{1}{\delta}}{n}} + \frac{\log \frac{1}{\delta}}{3n}.
\end{align}

% Since $\theta_\text{KL}^* = \min_\theta \hat{\L}_\text{KL}(\theta)$ and the bound for A. is uniform over $\Theta$, we can derive
% \begin{align*}
% \L(\theta^*_\text{KL})\leq \hat{\L}_\text{KL}(\theta) + \dotsc
% \end{align*}
% Then, utilize variance expansion to transform $\hat{\L}_\text{KL}(\theta)$ back to $\hat{\L}(\theta) + \dotsc$. Next, bound $\hat{\L}(\theta) = \hat{\L}(\theta) - \L(\theta) + \L(\theta)$ by uniform convergence (applying the second inequality in Theorem 5.1~\cite{boucheron2005theory} and bound the empirical variance by variance as in Lemma A.1~\cite{duchi2019variance}).



\bibliographystyle{unsrt}
\bibliography{refs}
\end{document}
