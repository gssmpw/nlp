\documentclass{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2023


% ready for submission
% \usepackage{neurips_2023}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
    % \usepackage[preprint]{neurips_2023}


% to compile a camera-ready version, add the [final] option, e.g.:
    \usepackage[final]{neurips_2023}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2023}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

%%%%%%%%%%%%%%%%%%%% Added package%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{makecell}
\usepackage{algorithm,algorithmic}
\usepackage{paralist,amsmath, amssymb,bm}
\usepackage{multirow}
\usepackage{ntheorem}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{lemma}{Lemma}
\newtheorem*{proof}{Proof}
\newtheorem{cor}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{asm}{Assumption}
\usepackage{enumitem}
\usepackage{textcase}
\usepackage{booktabs}
\usepackage{fancybox}
\usepackage{mathtools}

%%%% mathcal
\def \S {\mathbf{S}}
\def \L {\mathcal{L}}
\def \A {\mathcal{A}}
\def \X {\mathcal{X}}
\def \O {\mathcal{O}}
\def \Y {\mathcal{Y}}
\def \Ab {\bar{\A}}
\def \R {\mathbb{R}}
\def \Kt {\widetilde{K}}
\def \k {\mathbf{k}}
\def \w {\mathbf{w}}
\def \v {\mathbf{v}}
\def \t {\mathbf{t}}
\def \x {\mathbf{x}}
\def \Se {\mathcal{S}}
\def \E {\mathrm{E}}
\def \Rh {\widehat{R}}
\def \x {\mathbf{x}}
\def \p {\mathbf{p}}
\def \a {\mathbf{a}}
\def \diag {\mbox{diag}}
\def \b {\mathbf{b}}
\def \e {\mathbf{e}}
\def \ba {\boldsymbol{\alpha}}
\def \c {\mathbf{c}}
\def \tr {\mbox{tr}}
\def \d {\mathbf{d}}
\def \db {\bar\mathbf{d}}
\def \1 {\mathbf{1}}

\def \z {\mathbf{z}}
\def \s {\mathbf{s}}
\def \bh {\widehat{\b}}
\def \y {\mathbf{y}}
\def \u {\mathbf{u}}
\def \uh {\widehat{\u}}
\def \H {\mathcal{H}}
\def \g {\mathbf{g}}
\def \F {\mathcal{F}}
\def \I {\mathbb{I}}
\def \P {\mathcal{P}}
\def \Q {\mathcal{Q}}
\def \xh {\widehat{\x}}
\def \wh {\widehat{\w}}
\def \lambdah {\widehat{\lambda}}

\def \ah {\widehat{\a}}
\def \Rc {\mathcal R}
\def \Sigmah {\widehat\Sigma}

\def \Bh {\widehat B}
\def \Ah {\widehat A}
\def \Uh {\widehat U}
\def \Ut {\widetilde U}
\def \B {\mathcalB}
\def \C {\mathbf C}
\def \U {\mathbf U}
\def \Kh {\widehat K}
\def \fh {\widehat f}
\def \yh {\widehat\y}
\def \Xh {\widehat{X}}
\def \Fh {\widehat{F}}

\def \m {\mathbf{m}}
\def \y {\mathbf{y}}
\def \E {\mathrm{E}}
\def \x {\mathbf{x}}
\def \g {\nabla{g}}
\def \D {\mathcal{D}}
\def \z {\mathbf{z}}
\def \u {\mathbf{u}}
\def \H {\mathcal{H}}
\def \Z {\mathcal{Z}}
\def \Pc {\mathcal{P}}
\def \w {\mathbf{w}}
\def \s {\mathbf{s}}
\def \r {\mathbf{r}}
\def \R {\mathbb{R}}
\def \S {\mathcal{S}}
\def \regret {\mbox{regret}}
\def \Uh {\widehat{U}}
\def \Q {\mathcal{Q}}
\def \W {\mathcal{W}}
\def \N {\mathcal{N}}
\def \A {\mathcal{A}}
\def \q {\mathbf{q}}
\def \v {\mathbf{v}}
\def \M {\mathcal{M}}
\def \c {\mathbf{c}}
\def \ph {\widehat{p}}
\def \d {\mathbf{d}}
\def \p {\mathbf{p}}
\def \q {\mathbf{q}}
\def \db {\bar{\d}}
\def \dbb {\bar{d}}

\def \I {\mathbb{I}}
\def \xt {\widetilde{\x}}
\def \yt {\widetilde{\y}}
\def \hrho {\hat{\rho}}

\def \f {\mathbf{f}}
\def \a {\mathbf{a}}
\def \b {\mathbf{b}}
\def \ft {\widetilde{\f}}
\def \bt {\widetilde{\b}}
\def \h {\mathbf{h}}
\def \B {\mathcal{B}}
\def \bts {\widetilde{b}}
\def \fts {\widetilde{f}}
\def \Gh {\widehat{G}}
\def \G {\mathcal {G}}
\def \bh {\widehat{b}}
\def \wh {\widehat{\w}}
\def \Dth {\widehat{\Delta}}
\def \vb {\bar{\mathbf v}}
\def \zt {\widetilde{\z}}
\def \zh {\widehat{\z}}
\def \zts {\widetilde{z}}
\def \s {\mathbf{s}}
\def \gh {\widehat{\g}}
\def \vh {\widehat{\v}}
\def \Sh {\widehat{S}}
\def \rhoh {\widehat{\rho}}
\def \hh {\widehat{\h}}
\def \C {\mathcal{C}}
\def \V {\mathcal{L}}
\def \t {\mathbf{t}}
\def \xh {\widehat{\x}}
\def \Ut {\widetilde{U}}
\def \wt {\m}
\def \Th {\widehat{T}}
\def \Ot {\tilde{\mathcal{O}}}
\def \X {\mathcal{X}}
\def \nb {\widehat{\nabla}}
\def \K {\mathcal{K}}
\def \P {\mathbb{P}}
\def \T {\mathcal{T}}
\def \F {\mathcal{F}}
\def \ft{\widetilde{f}}
\def \Rt {\mathcal{R}}
\def \Rb {\bar{\Rt}}
\def \wb {\bar{\w}}
\def \zu {\underline{\z}}
\def \vect {\text{vec}}
\def \E {\mathbb{E}}
\def \bftau {\boldsymbol{\tau}}
\def\Tau{{\rm T}}
\def\bw{\mathbf{w}}
\def \start {\textsc{Start}}

\usepackage{comment}

\usepackage{tcolorbox}
\tcbuselibrary{minted,breakable,xparse,skins}

\definecolor{bg}{gray}{0.95}
\DeclareTCBListing{mintedbox}{O{}m!O{}}{%
  breakable=true,
  listing engine=minted,
  listing only,
  minted language=#2,
  minted style=default,
  minted options={%
    linenos,
    gobble=0,
    breaklines=true,
    breakafter=,,
    fontsize=\small,
    numbersep=8pt,
    #1},
  boxsep=0pt,
  left skip=0pt,
  right skip=0pt,
  left=25pt,
  right=0pt,
  top=3pt,
  bottom=3pt,
  arc=5pt,
  leftrule=0pt,
  rightrule=0pt,
  bottomrule=2pt,
  toprule=2pt,
  colback=bg,
  colframe=orange!70,
  enhanced,
  overlay={%
    \begin{tcbclipinterior}
    \fill[orange!20!white] (frame.south west) rectangle ([xshift=20pt]frame.north west);
    \end{tcbclipinterior}},
  #3}

\title{ \textsc{DFT}: Finetuning Generative Language Models with A Discriminative Approach}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Tianbao Yang \\
  % Department of Computer Science and Engineering \\
  Texas A\&M University \\
   College Station, USA \\
  \texttt{tianbao-yang@tamu.edu} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\usepackage{enumitem,amssymb}
\newlist{todolist}{itemize}{3}
\setlist[todolist]{label=$\square$}
\usepackage{pifont}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%
\newcommand{\done}{\rlap{$\square$}{\raisebox{2pt}{\large\hspace{1pt}\cmark}}%
\hspace{-2.5pt}}
\newcommand{\wontfix}{\rlap{$\square$}{\large\hspace{1pt}\xmark}}
\usepackage{changepage}

\begin{document}


\maketitle

% Add SPIN
% Figure out why SPIN doesn't work
% Finetune on orca-math if possible
\section{Results}
\subsection{GSM8k}
Finetune llama3 8B on the GSM8k training set using SFT and DFT. The learning rate is tuned in  $\{2 \times 10^{-6}, 5 \times 10^{-6}\}$, $\tau$ is tuned in $\{10, 1.1, 1.0, 0.9, 0.8, 0.1\}$. We uses fixed $6$ epochs, $\gamma = 0.95$ and $\mathbf{u}_0 = 0$. We find $\tau=0.9$ and a learning rate of $5 \times 10^{-6}$ yields the best performance. Then using the best hyperparameters, we increase the number of negative samples and show the results in the Table~\ref{tabel:1}.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
Method                            & GSM8k (0) & GSM8K (5) \\ \hline
Baseline                           & 0            & 49.12        \\
SFT                       & 60.80        & 63.68        \\
DFT 1 Negative & 62.77        & 63.61        \\
DFT 2 Negatives & 63.99        & 61.94        \\
DFT 4 Negatives & 63.84        & 63.68        \\ \hline
\end{tabular}
\label{tabel:1}
\caption{The GSM8K evaluation results. (n) stands for n shots prompting.}
\end{table}


\subsection{Alpaca}

Finetune llama 7b on the 52K alpaca training set using SFT and DFT. We use the same setting as Stanford Alpaca project~\cite{alpaca}. For the extra hyperparameters in our method, we use $\gamma = 0.95$, $\mathbf{u}_0 = 0$ and roughly tuned $\tau \in \{0.9, 1.0, 1.2\}$. Finally, we report the AlpacaEval 2~\cite{alpaca_eval} results.

\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
Method      & LC win rate & win rate \\ \hline
SFT         & 5.88        & 2.59     \\
DFT w/ $\tau=1.0$ & 7.51        & 3.22     \\
\hline
\end{tabular}
\end{table}

\subsection{Zephyr}
We finetune Mistral-7B-v0.1 using the chosen output of the UltraFeedback dataset and generate the reference responses using the prompts from the dataset. In the SFT setup, we use the zephyr-sft-full as the reference model and use this model to generate reference responses. We use 2 epochs in the base setup and 1 in the SFT setup to have a fair comparison. Finally, we report downstream task evaluation and AlpacaEval results.

% Add SFT on UF
% Tune tau, show results in table
% Tune u
% More negatives
% Add other POs
% different base models
% Use Odd similarity
% Add much details of experiment. Add more analysis and curves.
% Eval harmless, train on harmless dataset if possible
% Add margin
% Check HH-RLHF
% Show why SPIN doesnâ€™t work on GSM8k
% SFT vs discriminate experiment
% small tau
% include hard negatives

\begin{table}[ht]
 \begin{adjustwidth}{-2cm}{}
\begin{tabular}{lccccccccc}
\hline
 Method&data    & MMLU & TruthfulQA & HellaSwag & Winogrande & GSM8k  & ARC\_c & Avg.  & LC \\ \hline
% \multicolumn{8}{c}{Mistral}   \\ \hline
Mistral & & 62.56 &  42.60 & 83.53 & 78.30 & 38.67 &    61.18 & 61.14 & \\
% zephyr-SFT &UC                      & 58.85 & 40.40  & 80.94      & 76.09      & 32.68 & 57.34     & 57.71 & 10.24 \\ 
% zephyr $\beta$ &UC+UF & 59.76 & 55.14  & 84.40      & 78.14      & 32.98 & 63.05     & 62.24 & 12.62 \\
%SFT + DPO  &UC + UF                & 57.49 & 53.15  & 83.47      & 76.87      & 31.24 & 61.86     & 60.68 \\
%SFT + RDPO &UC + UF                & 58.29 & 46.10  & 84.11      & 76.40      & 28.43 & 61.26     & 59.10 \\ 
%SFT + KTO &UC + UF                 & 59.72 & 56.65  & 84.92      & 78.14      & 40.49 & 63.57     & 63.91 \\ 
%SFT + SimPO &UC + UF               & 58.33 & 50.67  & 83.39      & 76.95      & 33.36 & 61.86     & 60.76 \\ 
\hline
SFT & UF &  61.92 &   49.99 & 83.74 &  78.22 & 45.19 & 63.82 & 63.81 & 8.88 \\ 
SPIN& UF & 62.65 & 44.14  & 83.06      & 78.77      & 39.12 & 61.26     & 61.50 & 4.50 \\ 
% SIMPO $(3\times 10^{-7}, 2, 0.3)$ & UF & 42.91 & 56.42 & 62.92 & 66.47 & 78.22 & 85.22 & 65.36 & 9.05\\ 
% SIMPO $(3\times 10^{-7}, 2, 0.5)$& UF & 40.41 & 57.61 & 62.38 & 66.81 & 78.30 & 85.52 & 65.17 & 14.07 \\
SIMPO $(3\times 10^{-7}, 2, 0.7)$& UF &   62.46 & 63.95 & 85.44 & 78.53 & 35.86  &   67.75  &  65.67 & 14.06 \\ \hline
DFT v1 & UF & 61.70 & 51.77  & 83.63      & 78.06      & 45.19 & 63.57     & 63.99 & 11.50 \\
% DFT v2 $(0.1, 1)$ & UF& 62.25 &  52.04 & 83.36 &  77.27 & 44.35 & 63.91 & 63.86 & 12.94 \\
% DFT v2 $(0.2, 1)$ & UF&  62.45 & 51.30 & 83.63 & 77.58 & 46.63 & 64.68 & 64.38 & 8.55 \\ 
% DFT v2 $(0.3, 1)$ & UF& 62.48  &  53.42 & 83.50   &  77.74 & 42.46 & 64.76 & 64.06 & 12.96 \\ 
% DFT v2 $(0.4, 1)$ & UF& 61.92 & 52.87 & 83.02 & 78.14 & 44.81 &  64.68 & 64.24 & 11.12\\ \hline
% DFT v2 $(0.2, 2)$ & UF& 62.04 & 52.52 &  83.33  & 77.35 & 46.40 & 64.42 & 64.34 & 9.73 \\
% DFT v2 $(0.1, 4)$ & UF& 62.23 &  50.99 & 83.56 & 77.03 & 43.06 & 63.65 & 63.42 & 12.63 \\
% DFT v2 $(0.2, 4)$ & UF& 61.99 & 52.86 & 83.22 & 77.58 & 45.19 & 63.99 & 64.14 & 15.76 \\
DFT v2 $(0.3, 4)$ & UF& 61.78 & 53.67 & 83.35 & 77.90 & 45.64 & 65.02 & 64.56 & 16.08 \\ 
% DFT v2 $(0.4, 4)$ & UF& 61.52 & 52.52 & 82.92 & 77.98 & 43.29 & 64.85 & 63.85 & 13.33\\ 

\hline
%\multicolumn{8}{c}{Zephyr SFT}                                                      \\ \hline
%SFT + SPIN &UC + UF & 58.99 & 39.70  & 81.84      & 76.56      & 33.97 & 58.19     & 58.21 \\ 
%SFT + DFT & UC + UF & 58.66 & 40.31  & 81.81      & 76.80      & 34.50 & 59.30     & 58.56 \\ 
\end{tabular}
\end{adjustwidth}
\end{table}

\begin{table}[ht]
\centering
\begin{tabular}{lccc}
\hline
Method      & LC win rate & win rate & Length\\ \hline
zephyr-SFT          & 8.4       & 6.2   & 914   \\
zephyr $\beta$      &  13.2     & 11.0  & 1444 \\
DPO                 &  15.1     & 12.5  & 1477 \\
SimPO               & 21.5      & 20.8  & 1868 \\ \hline
DFT w/ LN $(0.2, 4)$ & 13.3     & 9.4   & 1230   \\
\hline
\end{tabular}
\end{table}

\section{TODO}
\subsection{Starting POint}

\subsection{Compare to SFT}
\begin{itemize}
    \item Dataset: Orca-Math-200k. https://huggingface.co/datasets/microsoft/orca-math-word-problems-200k. 

    Pipeline: three epochs, use the checkpoint at the end of the last epoch as the reference model.
    
    Evaluation metric: the same in the orca-math paper: GSM8k and other math benchmarks beyond GSM8k.
    
    \item Dataset tulu-v2-sft-mixture-326k. https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture. Finetune Llama 2 and compare with SFT. 

    Pipeline: two epochs of SFT.
    
    Evaluation metric: MT-Bench and AlpacaEval 2.
\end{itemize}

% \subsection{Compare to SPIN}
% \begin{enumerate}
%     \item use zephyr-sft as the base model.
%     \item use zephyr (without the SFT stage) as the base model. We can also show that DFT is better than SFT here, by comparing to zephyr-sft.
% \end{enumerate}
% 
% Other settings are the same as SPIN.

\subsection{Compare to SimPO}
Use llama3-8b as the base model, funetune on Ultrafeedback-61k using our method, and compare with llama3-8b-Instruct and other results from SimPO paper.

Evaluation metric: AlpacaEval 2, Arena-Hard and MT-bench.

\subsection{Referene-Free DFT (with Length Normalization)}

% \begin{figure}[h]
%     \centering
%     \includegraphics[width=0.5\textwidth]{avg_logps.png}
%     \caption{Mistral 7b}
%     \label{fig:1}
% \end{figure}

We consider the following distributionally robust optimization (DRO) problem under some conditions.
\begin{align}\nonumber
& \min_\w \frac{1}{n}\sum_{i=1}^n \hat{L}(\w;\x_i,\y_i),\quad \hat{L}(\w;\x_i,\y_i) \coloneqq  \max_{\boldsymbol{\pi}_i\in\Delta_m} \left\{\sum_{j=1}^m \boldsymbol{\pi}_i^{(j)} \phi(\w;\x_i,\y_i,\y_{ij}') - \tau D_\mathrm{KL}\left(\boldsymbol{\pi}_i\vert\vert \mathbf{1}/m\right)\right\},\\\label{eq:dro}
& \phi(\w;\x,\y,\y') = \ell \left(\frac{\lambda}{|\y|} \log p_g(\y\mid \x) - \frac{\lambda}{|\y'|} \log p_g(\y'\mid \x) - \gamma \right),
\end{align}
where $\ell(\cdot) = \log(1+\exp(-\cdot))$, $\Delta_m\subset\R^m$ is the probability simplex and $\mathbf{1}/m$ represents the uniform distribution on $\S_i$. Due to the KKT condition, the DRO problem in \eqref{eq:dro} is equivalent to 
\begin{align}\label{eq:dro_equiv}
& \min_\w \frac{1}{n}\sum_{i=1}^n \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\phi(\w;\x_i,\y_i,\y_{ij}')/\tau\right)\right) \\
& \min_\w \frac{1}{n}\sum_{i=1}^n \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\frac{1}{\tau}\log \left(1 + \exp\left(- \frac{\lambda}{|\y_i|} \log p_g(\y_i\mid \x) + \frac{\lambda}{|\y_{ij}'|} \log p_g(\y_{ij}'\mid \x) + \gamma\right) \right)\right)\right) \\
& \min_\w -\frac{1}{n}\sum_{i=1}^n \frac{\lambda}{|\y_i|} \log p_g(\y_i\mid \x) \\
& + \frac{1}{n}\sum_{i=1}^n \tau \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\frac{1}{\tau}\log \left(\exp\left(\frac{\lambda}{|\y_i|} \log p_g(\y_i\mid \x) - \gamma \right) + \exp\left(\frac{\lambda}{|\y_{ij}'|} \log p_g(\y_{ij}'\mid \x) \right) \right)\right)\right).
\end{align}

% DFT with Length Normalization:

% \begin{align}\label{eqn:dis}
% \min_{\w} -\frac{1}{n}\sum_{i=1}^n \frac{1}{|\y_i|}\log p_g(\y_i|\x_i) +  \frac{1}{n}\sum_{i=1}^n  \tau \log \bigg(\E_{\y\sim p^0_g(\y|\x_i)}\frac{p_g(\y|\x_i)^{\frac{1}{\tau |\y|}}}{p^0_g(\y|\x_i)}\bigg)
% \end{align}

Another option:

\begin{align}\label{eqn:dis}
\min_{\w} -\frac{1}{n}\sum_{i=1}^n \frac{1}{|\y_i|}\log p_g(\y_i|\x_i) +  \frac{1}{n}\sum_{i=1}^n  \tau \log \bigg(\E_{\y\sim p^0_g(\y|\x_i)}\frac{p_g(\y|\x_i)^{\frac{1}{\tau |\y|}}}{p^0_g(\y|\x_i)^{\frac{1}{|\y|}}}\bigg)
\end{align}

If the generation outputs is not length biased, then $\frac{1}{|\y|}\log p^0_g(\y|\x_i)$ is nearly a constant and $p^0_g(\y|\x_i)^{\frac{1}{|\y|}}$ can be removed from the expectation. Hence, we obtain a referene-free DFT objective:

replace $\frac{\tau}{\lambda}$ by $\tau'$, we have
\begin{align}
& \min_\w -\frac{1}{n}\sum_{i=1}^n \frac{1}{|\y_i|} \log p_g(\y_i\mid \x) \\
& + \frac{1}{n}\sum_{i=1}^n \tau' \log \left(\frac{1}{m}\sum_{j=1}^m \exp\left(\frac{1}{\tau'\lambda}\log \left(\exp\left(\frac{\lambda}{|\y_i|} \log p_g(\y_i\mid \x) - \gamma \right) + \exp\left(\frac{\lambda}{|\y_{ij}'|} \log p_g(\y_{ij}'\mid \x) \right) \right)\right)\right).
\end{align}


When $\gamma \rightarrow \infty$, we obtain the following objective:

\begin{align}\label{eqn:dis}
\min_{\w} -\frac{1}{n}\sum_{i=1}^n \frac{1}{|\y_i|}\log p_g(\y_i|\x_i) +  \frac{1}{n}\sum_{i=1}^n  \tau' \log \bigg(\E_{\y\sim p^0_g(\y|\x_i)}p_g(\y|\x_i)^{\frac{1}{\tau' |\y|}}\bigg).
\end{align}




\subsection{Different Choice of Fitness Score}
\begin{enumerate}
    \item Perplexity
    \item ORPO
\end{enumerate}

\subsection{Generation Hyper-parameters}
Through the experiments, we find the generation hyper-parameters can impact the value of $\log p_0$. And $\log p_g$ will start from $\log p_0$. The different start values of $\log p_g$ will impact the training speed of the negatives. 

We also find SPIN does not perform well in both vllm generated data and long generation sequence length. This may be due to the smaller $\log p_0$ in vllm generated data than hf generated data. And longer generation sequence will also lead to smaller $\log p_0$.

Potential hyper-parameters:
\begin{enumerate}
    \item top\_k
    \item top\_p
    \item max\_seq\_length
\end{enumerate}


\section{Numerical Stability}
\begin{algorithm}[ht]
\caption{DFT}
\label{alg:LLM}
\begin{algorithmic}[1] 
\STATE Initialize $\w, \u$
\FOR{$t=0,1,\dotsc,T-1$}
\STATE Sample a batch $\S_t\subset \{1,\dotsc,n\}$, $|\S_t| = S$ \label{lst:line:dual_start}
\FOR{each $\x_i\in\S_t$} 
\STATE Sample size-$B$ mini-batches $\B_t^{(i)}$ from $p^0(\y|\x_i)$
\STATE For each $\y \in \B_t^{(i)}$, compute $d_{\x_i,\y}=\frac{1}{\tau} \log p^t_g(\y|\x_i) - \log p^0(\y | \x_i)$, $w_{\x_i,\y} = \exp(d_{\x_i,\y})$
\STATE Update $u_{t+1}^{(i)} = (1-\gamma)u_t^{(i)} + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}$
\ENDFOR
\STATE Compute $L_t =\frac{1}{S}\sum_{i\in\S_t}\left( -\log p_g^t(\y_i | \x_i)  + \frac{1}{B(u_{t+1}^{(i)} + \epsilon)}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y} \log p_g^t(\y | \x_i) \right)$  \label{lst:line:primal_start}
\STATE Compute $G_t = \nabla L_t$
\STATE Update $\w_{t+1}$ using Adam-W
\ENDFOR
\end{algorithmic}
\end{algorithm}
We have the original algorithm \ref{alg:LLM}. In practice, as we push the loglikelihood of negatives down, the value of $\u_t$ is negligible compared to $\epsilon$, and the second term in the loss function will be $0$, which makes DFT become SFT. To address the issue, we propose a division-free algorithm \ref{alg:LLM2}

The moving average is reformulated to
\begin{align}
    u_{t+1}^{(i)} = & (1-\gamma)u_t^{(i)} + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y} \\ 
    \exp(b_{t+1}^{(i)}) = & \exp(\log (1 - \gamma) + b_t^{(i)}) + \exp(\log \gamma + \log \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y})
\end{align}

where we save $\b_t = \log \u_t$ instead of saving $\u_t$ directly.

For simplicity, let $b_t' = \log (1 - \gamma) + b_t^{(i)}$ and $w_t' = \log \gamma + \log \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}$, we have
\begin{align}
    \exp(b_{t+1}^{(i)}) = & \exp(b_t') + \exp(w_t') \\ 
                        = & \exp(b_t') + \exp(w_t' - b_t' + b_t') \\
                        = & \exp(b_t')(1 + \exp(w_t' - b_t')) \label{eq:merge_exp1}
\end{align}
For numerical stability, we use~\eqref{eq:merge_exp1} only if $w_t'$ is less or equal than $b_t'$, otherwise, we use another equivalent formula~\eqref{eq:merge_exp2}
\begin{align}
   \exp(b_{t+1}^{(i)}) = & \exp(w_t')(1 + \exp(b_t' - w_t')) \label{eq:merge_exp2}
\end{align}

In summary, we have
\begin{align}
    \exp(b_{t+1}^{(i)}) = \exp(\max\{b_t', w_t'\})(1 + \exp(-|b_t' - w_t'|)) \label{eq:merge_exp3}
\end{align}

Take the logarithm of both sides, we have
\begin{align}\label{eq:merge_exp}
    b_{t+1}^{(i)} = \max\{b_t', w_t'\} + \log (1 + \exp(-|b_t' - w_t'|)) =  \max\{b_t',w_t'\} - \log \sigma(|b_t' - w_t'|)
\end{align}

\begin{algorithm}[ht]
\caption{DFT}
\label{alg:LLM2}
\begin{algorithmic}[1] 
\STATE Initialize $\w, \u$
\FOR{$t=0,1,\dotsc,T-1$}
\STATE Sample a batch $\S_t\subset \{1,\dotsc,n\}$, $|\S_t| = S$ \label{lst:line:dual_start}
\FOR{each $\x_i\in\S_t$} 
\STATE Sample size-$B$ mini-batches $\B_t^{(i)}$ from $p^0(\y|\x_i)$
\STATE For each $\y \in \B_t^{(i)}$, compute $d_{\x_i,\y}=\frac{1}{\tau} \log p^t_g(\y|\x_i) - \log p^0(\y | \x_i)$,\\$w_{\x_i,\y} = \exp(d_{\x_i,\y} - \max_{\y} \{d_{\x_i,\y}\})$
\STATE Compute $w_t' = \log \gamma + \log(\frac{1}{B}\sum_{\y\in\B_t^{(i)}}w_{\x_i,\y}) + \max_{\y} \{d_{\x_i,\y}\}$, $b_t' = \log(1 - \gamma) + b_t^{(i)}$
\STATE Update $b_{t+1}^{(i)} = \max\{b_t', w_t'\} - \log \sigma(|b_t' - w_t'|)$
\ENDFOR
\STATE Compute $L_t =\frac{1}{S}\sum_{i\in\S_t}\left( -\log p_g^t(\y_i | \x_i)  + \frac{1}{B}\sum_{\y\in\B_t^{(i)}} \exp(d_{\x_i,\y}-b_{t+1}^{(i)}) \log p_g^t(\y | \x_i) \right)$  \label{lst:line:primal_start}
\STATE Compute $G_t = \nabla L_t$
\STATE Update $\w_{t+1}$ using Adam-W
\ENDFOR
\end{algorithmic}
\end{algorithm}



\subsection{Initialization of u}
We investigate how the initial value of $\mathbf{u}$ affects the weights to the log-likelihood of negative samples. As shown in table~\ref{tab:init_u}, a smaller initial value $\mathbf{u}_0$ can yield a larger average weight in the first epoch. When $\mathbf{u}_0 = 0$, the weight becomes a constant $\frac{1}{\gamma}$. In practice, we observe that a constant weight of $\frac{1}{\gamma}$ will push $\log p_g$ to $-\infty$ in a few steps, which causes over-fitting. We address this issue by tuning $\mathbf{u}_0$. 

We attribute this to the feature of the moving average. 
\begin{align}
     u_{t+1}^{(i)} = & (1-\gamma)u_t^{(i)} + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}
\end{align}
When $\mathbf{u}_0 = c$, the average weight $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{(1 - \gamma)c + \gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}$. If $c = 0$, we have the average weight $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{\gamma \frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}} = \frac{1}{\gamma}$. If $c > 0$, $w_{\x_i,\y}$ decreases very fast and soon become negligible comparing with $c$ in practice, then we have $\bar w = \frac{\frac{1}{B}\sum_{\y\in\B_t^{(i)}} w_{\x_i,\y}}{(1 - \gamma)c}$. It means a smaller weight will be assigned to those samples with smaller $w_{\x_i,\y}$, which mitigates the over-fitting.

% Show that u=e^{-50} differs from u=0
\begin{table}[ht]
\centering
\begin{tabular}{lcc}
\hline
Initial value of $\mathbf{u}$      & Average weight (1 epoch)  & Average weight (2 epochs) \\ \hline
$0$        & 1.053        & /     \\
$\exp(-5)$ & 0.398        & $6.23 \times 10^{-5}$    \\
$1$        & 0.240        & $5.51 \times 10^{-3}$     \\ 
$\exp(5)$  & 0.128        &  $1.31 \times 10^{-8} $    \\
\hline
\end{tabular}
\label{tab:init_u}
\end{table}

However, we observe that less value of $\u_0$ does not always mean larger weight. As shown in table~\ref{tab:init_u}, comparing to $\u_0 = 1$, $\u_0 = \exp(-5)$ has larger average weight on the epoch $1$, but smaller on the $2$ epochs. It is likely due to the smaller value of $w_{\x_i,\y}$ after epoch $1$ caused by large weight.

\bibliographystyle{unsrt}
\bibliography{refs}
\end{document}
