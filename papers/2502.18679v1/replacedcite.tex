\section{Related Work}
{\bf Supervised Finetuning.} The standard approach to SFT is mimicking pretraining, which maximizes the likelihood of tokens in the output response given input prompt ____. Although simple to implement, it often captures only superficial patterns rather than fostering a deeper understanding of task semantics ____. Recent works have studied inverse reinforcement learning to address this limitation, but such methods often involve interdependent updates between different models (e.g., the policy and a reward-related model), which complicates the training process ____. In contrast, our method neither requires online sampling from the current policy model nor involves a reward-related model, making it much more efficient.% for training.  %Our method directly models a discriminative likelihood of a text output given an input prompt. This encourages the model to learn from both good and bad outputs, integrating preference optimization on sampled data without incurring the complexity of the training procedure.

{\bf Preference Optimization.} Pioneering works proposed RL-based PO methods ____. These methods leverage a separate reward model, trained on human-labeled preference data, and optimize the SFT model against it using policy gradient methods such as PPO ____ and REINFORCE ____.
____ proposed direct preference optimization (DPO), which removes the step of training a reward model and directly optimizes a pairwise loss of the policy model on the preference data. Following DPO, many PO methods have been proposed with different loss functions, including R-DPO____, CPO____, IPO____, SimPO____, KTO____, to name just a few among others____.  
%More recently, several works have proposed reframing PO by using binary preferences as direct supervision for the SFT model ____. This approach simplifies the workflow and improves stability by removing the need for explicit reward modeling and RL.  

Several works____ have considered PO with a list of ranked preference data that may be explicitly labeled with a reward value.  ____ assumed a general preference function is given that can produce a probability telling one output is preferred over another output given an input. Different from these works, we do not assume any preference data or preference model other than annotated input-output pairs. %____ expanded on this by introducing a multi-category cross-entropy loss for datasets with multiple outputs per input, assigning reward scores instead of binary preferences. However, evaluating rewards for multiple outputs remain costly, and using implicit reward of DPO ____ as a score function is only reliable if the reference model can effectively distinguish output quality ____, making the SFT stage before PO essential. Unlike treating the problem as a multi-class classification task, our approach views it as a one-to-all discriminative classification task. We consider all possible outputs as negative samples and use an efficient and robust algorithm to compute the stochastic gradient estimator. Moreover, our method does not rely on a prior SFT stage, additional preference data, or a reward model.
%Different from the aforementioned methods, DFT does not rely on RL, reward modeling, or human-annotated data. By simplifying SFT+PO into a single stage, it achieves an alignment bonus rather than a tax.

{\bf Finetuning via Self-play.} Training a model on its own self-generated responses has been widely explored in the PO stage. For example, many variants of RLHF ____ use on-policy samples produced by the current policy under optimization. Some studies have exhibited benefits of using self-generated data for PO by reducing the distribution gap between the training data and the current model while fostering exploration of diverse response spaces ____. Moreover, leveraging synthetic data has proven essential for iterative (online) algorithmic improvement of these methods ____. A more closely related work is SPIN____, which uses a similar preference optimization objective as DPO but with data generated by the model to be finetuned as the losing responses. Although we also use self-generated data from the base model to be finetuned as our negative data, our formulation is derived from a discriminative learning framework, making our approach aided by advanced optimization better than the pairwise loss function used in SPIN and other pairwise preference optimization objectives (cf. Section~\ref{sec:po}). 

 %Beyond PO, recent works in SFT have generated a single negative sample per prompt and combined it with demonstration data in the SFT dataset to form a pairwise dataset ____. With a different motivation, our discriminative learning framework provides another perspective of  using multiple responses sampled from the base model, and  yields more effective algorithms for leveraging these potentially bad outputs. 
%Sun et al. studied self-alignment that defines a set of principles that the AI model must adhere to and provides in-context learning demonstrations to the base LLM to generate helpful, ethical, and reliable responses, which are then used for finetuning the base models. The finetuning approach is the same as traditional self-supervised finetuning (SFT).