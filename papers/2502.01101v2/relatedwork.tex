\section{Related work}
\subsection{Video Diffusion Models}
Video Diffusion Models  \cite{shi2024bivdiff,qing2024hierarchical,yuan2024instructvideo,chen2023control,guo2023animatediff} have emerged as a powerful method for video generation, gaining significant attention in recent years. Building on the success of diffusion models in Text-to-Image tasks \cite{li2024generative,liang2024rich,hollein2024viewdiff,ding2024freecustom,zhang2024pia}, VDMs extend the denoising process into the temporal domain to tackle video generation complexities. Recent studies \cite{xing2024simda,wu2023lamp,li2024vidtome} have introduced innovative attention mechanisms to ensure smooth transitions and spatiotemporal consistency, further advancing VDMs development. However, generating high-quality video animations directly from hand-drawn sketch sequences and simple text prompt remains unexplored, limiting ordinary users' artistic potential. To address this, our work integrates motion priors and leverages hand-drawn sketch sequences and simple prompts to guide video animations generation, expanding VDMs applications and laying groundwork for future advancements.



\begin{figure*}[!t]
\vskip 0.05in
\begin{center}
\includegraphics[width=0.95\textwidth]{figure/pipeline.jpg}
\vskip -0.05in
\caption{Pipeline of our \textbf{VidSketch}. During training, we use high-quality, small-scale video datasets categorized by type to train the TempSpatial Attention (TS-Attention) and Temporal Attention blocks, improving spatiotemporal consistency in video animations. During inference, users simply input a prompt and sketch sequences to generate tailored high-quality animations. Specifically, the first frame is generated using existing techniques, while the entire sketch sequence is processed by the Inflated T2I-Adapter \cite{mou2024t2i} to extract information, which is injected into VDM's upsampling layers to guide video generation.}
\label{fig:3}
\end{center}
\vskip -0.15in
\end{figure*}

\subsection{Sketch-Guided Generation Method}

Sketch-guided generation methods have gained significant attention in generative models. Early studies \cite{chen2023control,ye2023ip} mainly used sketches from concrete or original images to control the generation process but struggled with the varying abstraction levels of hand-drawn sketches. With technological advancements, some works have succeeded in generating static images from hand-drawn sketches. Studies \cite{voynov2023sketch,koley2024s} have shown that hand-drawn sketches effectively serve as semantic cues to create detailed, context-rich images. Recently, automated sketch animation generation has progressed notably. For example, \cite{gal2024breathing,bandyopadhyay2024flipsketch} use the first frame of a hand-drawn sketch to guide sketch animation generation, advancing sketch-guided methods. However, as of now, no work explores explore guiding high-quality video animation using only hand-drawn sketch sequences and simple prompt. To fill this gap, we propose \textbf{VidSketch}, a method to dynamically generate high-quality video animations using hand-drawn sketches with varying abstraction levels.