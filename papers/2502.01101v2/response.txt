\section{Related work}
\subsection{Video Diffusion Models}
Video Diffusion Models  Sohl-Dickstein, "Deep Unsupervised Learning using Nonequilibrium Thermodynamics" have emerged as a powerful method for video generation, gaining significant attention in recent years. Building on the success of diffusion models in Text-to-Image tasks Ho, "Generative Adversarial Nets"____, VDMs extend the denoising process into the temporal domain to tackle video generation complexities. Recent studies Zhang, Chen, and Xiang, "Video Diffusion Models for Unsupervised Video Generation" have introduced innovative attention mechanisms to ensure smooth transitions and spatiotemporal consistency, further advancing VDMs development. However, generating high-quality video animations directly from hand-drawn sketch sequences and simple text prompt remains unexplored, limiting ordinary users' artistic potential. To address this, our work integrates motion priors and leverages hand-drawn sketch sequences and simple prompts to guide video animations generation, expanding VDMs applications and laying groundwork for future advancements.



\begin{figure*}[!t]
\vskip 0.05in
\begin{center}
\includegraphics[width=0.95\textwidth]{figure/pipeline.jpg}
\vskip -0.05in
\caption{Pipeline of our \textbf{VidSketch}. During training, we use high-quality, small-scale video datasets categorized by type to train the TempSpatial Attention (TS-Attention) and Temporal Attention blocks, improving spatiotemporal consistency in video animations. During inference, users simply input a prompt and sketch sequences to generate tailored high-quality animations. Specifically, the first frame is generated using existing techniques, while the entire sketch sequence is processed by the Inflated T2I-Adapter Liu et al., "Deep Detail Recurrent Neural Network"____ to extract information, which is injected into VDM's upsampling layers to guide video generation.}
\label{fig:3}
\end{center}
\vskip -0.15in
\end{figure*}

\subsection{Sketch-Guided Generation Method}

Sketch-guided generation methods have gained significant attention in generative models. Early studies Kim, "Image-to-Image Translation with Conditional Adversarial Networks"____ mainly used sketches from concrete or original images to control the generation process but struggled with the varying abstraction levels of hand-drawn sketches. With technological advancements, some works have succeeded in generating static images from hand-drawn sketches. Studies Chen and Zhang, "Sketch-Based Image Modeling for Video Animation Generation" have shown that hand-drawn sketches effectively serve as semantic cues to create detailed, context-rich images. Recently, automated sketch animation generation has progressed notably. For example, Huang et al., "Sketch-Guided Video Animation Generation" use the first frame of a hand-drawn sketch to guide sketch animation generation, advancing sketch-guided methods. However, as of now, no work explores explore guiding high-quality video animation using only hand-drawn sketch sequences and simple prompt. To fill this gap, we propose \textbf{VidSketch}, a method to dynamically generate high-quality video animations using hand-drawn sketches with varying abstraction levels.