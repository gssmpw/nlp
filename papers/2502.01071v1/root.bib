@inproceedings{rt1,
    title={RT-1: Robotics Transformer for Real-World Control at Scale},
    author={Anthony	Brohan and Noah Brown and others},
    booktitle={arXiv preprint arXiv:2212.06817},
    year={2022}
}

@misc{rt2,
      title={RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control}, 
      author={Anthony Brohan and Noah Brown and others},
      year={2023},
      eprint={2307.15818},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2307.15818}, 
}

@misc{rt2x,
      title={Open X-Embodiment: Robotic Learning Datasets and RT-X Models}, 
      author={Embodiment Collaboration and Abby O'Neill and Abdul Rehman and others},
      year={2024},
      eprint={2310.08864},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2310.08864}, 
}

@misc{saycan,
      title={Do As I Can, Not As I Say: Grounding Language in Robotic Affordances}, 
      author={Michael Ahn and Anthony Brohan and others},
      year={2022},
      eprint={2204.01691},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2204.01691}, 
}

@misc{openvla,
      title={OpenVLA: An Open-Source Vision-Language-Action Model}, 
      author={Moo Jin Kim and Karl Pertsch and others},
      year={2024},
      eprint={2406.09246},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2406.09246}, 
}
@misc{fewshotvqa,
      title={Few-Shot Image Classification and Segmentation as Visual Question Answering Using Vision-Language Models}, 
      author={Tian Meng and Yang Tao and others},
      year={2024},
      eprint={2403.10287},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2403.10287}, 
}

@misc{yolo,
      title={You Only Look Once: Unified, Real-Time Object Detection}, 
      author={Joseph Redmon and Santosh Divvala and others},
      year={2016},
      eprint={1506.02640},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1506.02640}, 
}
@misc{alter3,
      title={From Text to Motion: Grounding GPT-4 in a Humanoid Robot "Alter3"}, 
      author={Takahide Yoshida and Atsushi Masumori and others},
      year={2023},
      eprint={2312.06571},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2312.06571}, 
}
@misc{langreward,
      title={Language to Rewards for Robotic Skill Synthesis}, 
      author={Wenhao Yu and Nimrod Gileadi and others},
      year={2023},
      eprint={2306.08647},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2306.08647}, 
}

@misc{mini-intern,
      title={InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks}, 
      author={Zhe Chen and Jiannan Wu and others},
      year={2024},
      eprint={2312.14238},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2312.14238}, 
}
@misc{CLIPSeg,
      title={Image Segmentation Using Text and Image Prompts}, 
      author={Timo LÃ¼ddecke and Alexander S. Ecker},
      year={2022},
      eprint={2112.10003},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2112.10003}, 
}

@misc{palix,
      title={PaLI-X: On Scaling up a Multilingual Vision and Language Model}, 
      author={Xi Chen and Josip Djolonga and others},
      year={2023},
      eprint={2305.18565},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2305.18565}, 
}

@misc{palmee,
      title={PaLM-E: An Embodied Multimodal Language Model}, 
      author={Danny Driess and Fei Xia and others},
      year={2023},
      eprint={2303.03378},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2303.03378}, 
}

@misc{mujoco,
      title={Predictive Sampling: Real-time Behaviour Synthesis with MuJoCo}, 
      author={Taylor Howell and Nimrod Gileadi and others},
      year={2022},
      eprint={2212.00541},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2212.00541}, 
}

@misc{gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and others},
      year={2024},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2303.08774}, 
}

@misc{llmrobotics,
      title={Large Language Models for Robotics: A Survey}, 
      author={Fanlong Zeng and Wensheng Gan and others},
      year={2023},
      eprint={2311.07226},
      archivePrefix={arXiv},
      primaryClass={cs.RO},
      url={https://arxiv.org/abs/2311.07226}, 
}

@misc{SegmentA,
      title={Segment Anything}, 
      author={Alexander Kirillov and Eric Mintun and others},
      year={2023},
      eprint={2304.02643},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2304.02643}, 
}

@ARTICLE{Phi3,
       author = {{Abdin}, Marah and {Aneja}, Jyoti and others},
        title = "{Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence},
         year = 2024,
        month = apr,
          eid = {arXiv:2404.14219},
        pages = {arXiv:2404.14219},
          doi = {10.48550/arXiv.2404.14219},
archivePrefix = {arXiv},
       eprint = {2404.14219},
 primaryClass = {cs.CL},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2024arXiv240414219A},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}

@misc{minilm,
  author       = {HuggingFace and Nils Reimers and Joao Gante and others},
  title        = {all-MiniLM-L6-v2: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2},
  year         = 2021,
  url          = {https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2}
}
