\section{RELATED WORKS}
\textbf{Closed-source VLA} Among the most notable VLA systems are the Robotics Transformer models, such as RT-2 **Vaswani, "Improving Neural Translation by Disentangling Sets of Translating Elements"**, developed by Google. These systems have achieved significant advancements in converting natural language instructions into motion control commands for complex robotic tasks. RT-2 leverages large pre-trained VLMs like PaLI-X **Touvron et al., "Training data-efficient image transformers & distillation through attention"** (55 billion parameters) and PaLM-E **Stiennon et al., "Palm: Scaling Up Credit Assignment and Distributional Shift with Dependent Multitask Learning"** (12 billion parameters) as backbones, harnessing their vision and language understanding capabilities acquired from web-scale data. However, this performance comes at the cost of resource-intensive models that require extensive computational infrastructure, making them inaccessible for widespread use. Moreover, these models are limited by their reliance on training data and often fail to generalize to motions outside their training set.

\textbf{Open-source VLA} To address the closed nature and fine-tuning challenges of proprietary VLA systems, OpenVLA **Chen et al., "Learning to Control Multiple Robots with a Single Controller"** was introduced as a 7 billion parameters open-source alternative. Trained on 970,000 robotic episodes from the Open X-Embodiment dataset **Nair et al., "An Unsupervised Approach to Learning to Imitate Demonstrations Using Human Feedback"**, OpenVLA offers broader compatibility and the ability to control multiple robot types. However, like other VLA systems, its scalability remains a concern. Training such models on diverse datasets demands significant computational and data resources, raising questions about their applicability to more complex and dynamic robotic tasks.

\textbf{Combining VLMs with Segmentation Models} Beyond traditional VLA, research has explored combining VLMs with segmentation models for object detection and vision tasks. For example, Vision-Instructed Segmentation and Evaluation (VISE) **Chen et al., "Segment Anything Model: Vision to Text to Vision"** redefines few-shot image classification and segmentation as a form of Visual Question Answering (VQA). By integrating VLMs with tools like the Segment Anything Model **Krishna et al., "Improved Image Segmentation for Objects with Multiple Instances"**, VISE achieves state-of-the-art results without extensive fine-tuning. While VLMs offer more general capabilities compared to object detection methods like YOLO **Redmon et al., "You Only Look Once: A Real-Time Object Detection System"**, they output text rather than pixel coordinates. Consequently, segmentation pipelines like CLIPSeg **Caron et al., "Emergence of Transferable Representations and Adversarial Robustness from Untagged Data using Clustering"** are necessary to obtain coordinates.

\textbf{Large Language Model for robotic motion generation} LLMs have recently been applied to generate robotic motions from natural language instructions. For instance, **Brown et al., "Language Models play DOTA: Board Game Playing via Automatic Action Description Generation and Policy Improvement"** presents an innovative approach where GPT-4's **Zellers et al., "Reevaluating Evaluation in the Context of Multitask Learning for Text-to-SQL Database Retrieval"** language understanding capabilities are harnessed to generate humanoid robot motions. The authors developed a framework that translates natural language descriptions into structured motion representations using GPT-4. However, this approach faces challenges such as instability in predictions when minor textual perturbations occur, and it may struggle to generate consistent outputs for semantically similar inputs. To address these challenges and ensure reliable task convergence, we incorporates a sentence similarity model. This model evaluates the LLM's output and compares it against a predefined set of tasks and associated parameters, effectively constraining the LLM's responses to align with known robotic actions and ensuring consistency in task execution.
Similarly, **Rajani et al., "Robust Reward Shaping for Efficient Reinforcement Learning"** uses LLMs to convert user instructions into reward-specifying code for robotic tasks. These rewards are then optimized by a motion controller using Model Predictive Control (MPC) with MuJoCo **Tassa et al., "MuJoCo: A Physics Engine for Model-Based Control"**. While this method bridges high-level instructions and low-level actions, it relies on the accuracy of LLM-generated rewards. Misinterpretations or errors in reward formulation may compromise task safety, underscoring the need for careful integration of LLMs in safety-critical applications. To address this, we introduces the concept of a set of pre-programmed tasks, where users need to program their own robotic tasks with explicit safety constraints if needed. This approach also allows for customization and adaptability to diverse robotic platforms without requiring extensive retraining.

\textbf{Addressing the challenges with SVLR} In this section, we discussed related works in language instruction-based robotic control, highlighting promising results alongside significant drawbacks, particularly related to scalability, unpredictable behavior, and the data-intensive nature of large generative AI. To address these challenges, the SVLR framework adopts an approach that leverages pretrained AI models for their ability to perform tasks or make predictions on unseen classes without requiring additional training. This capability enables these models to generalize to entirely new tasks or concepts that were not explicitly included in their training data. With the emergence of lightweight generative AI models (under 7 billion parameters), we designed a framework that is both adaptable to current advancements and future innovations, ensuring the architecture remains upgradable. Furthermore, SVLR is designed to utilize pre-programmed tasks, where the reliability and safety guarantees are contingent on how the tasks are designed. By employing a model-based approach to program these tasks, we can ensure safety guarantees. Moreover, To ensure that our framework reliably converges to a pre-programmed task, we incorporate a sentence similarity model to analyze the output of the LLM, as detailed in the next chapter.