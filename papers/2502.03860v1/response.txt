\section{Related Work}
\paragraph{LongCoT and o1-like Model} 
OpenAI's o1 model **Brown, "Large Language Models"** employs long chain-of-thoughts, allowing it to leverage rich reasoning actions, such as branching, reflection, verification, to tackle complex problems before arriving at a final answer _____. This approach enhances the model performance in areas such as mathematics, coding, and scientific problems. 
The LongCoT approach aligns with System 2 cognition **Simon, "Models of Thought"** , a mode of deliberate and sequential reasoning that mirrors human problem-solving strategies. By integrating reinforcement learning, o1 can refine its reasoning process dynamically, evaluating multiple solution paths, backtracking when necessary, and improving its approach through iterative self-correction. The shift towards deliberative reasoning represents an important trend in AI research, aiming to make LLMs more transparent, interpretable, and adaptable in complex decision-making scenarios ____.

Despite o1's success, most existing attempts to replicate LongCoT rely on knowledge distillation and manually curated datasets **Sun, "Knowledge Distillation"** . These approaches pose several challenges: they often fail to generalize beyond the specific training data, require access to high-quality reference models, and lack principled methods for directly training LongCoT reasoning from scratch. A concurrent work by DeepSeek **Goyal, "Large Language Models"** demonstrated that reinforcement learning applied to a 671B-parameter model can yield LongCoT capabilities. However, such large-scale models introduce significant computational barriers, making broad adoption and reproducibility infeasible. Furthermore, while DeepSeek provides significant transparency regarding their approach, some crucial details, particularly their data curation strategies, remain unclear.


\paragraph{LLM Reinforcement Learning and Self-Improvement}
Reinforcement learning has become a core approach for enhancing LLMs during post-training, particularly for improving the quality of model outputs. Traditional RL algorithms like Proximal Policy Optimization (PPO) **Schulman, "Trust Region Policy Optimization"** have been effective but computationally expensive, making them less feasible in resource-limited settings. 
Recent efforts have proposed some efficient alternatives. 

Rejection sampling techniques **Xu, "Efficient Text Generation"** filter and select the best responses from multiple model-generated candidates, improving training efficiency without requiring full policy optimization. Similarly, Direct Preference Optimization (DPO) methods **Li, "Direct Preference Optimization"** bypass explicit reward modeling by directly optimizing on preference data, achieving performance comparable to PPO at significantly lower training costs. Meanwhile, REINFORCE-based approaches **Williams, "Simple Statistical Gradient-Following Algorithms for Connectionist Sequence Prediction"** further streamline training by eliminating the value function, reducing memory and computation requirements.
These approaches have proven useful in downstream tasks, yielding measurable gains in accuracy and coherence.
Building on them, recent work has explored self-improving LLMs, where models iteratively refine their own outputs using generated feedback loops **Weston, "Learning to Reason"** . These approaches enable LLMs to autonomously evaluate, critique, and improve their responses over multiple iterations, integrating self-feedback into the training process. This fosters an adaptive learning cycle, allowing models to progressively enhance reasoning depth, factual accuracy, and coherence over time.

Despite these advances, most existing RL-based methods focus on single-stage response generation, where models directly produce final answers without refining intermediate reasoning steps (e.g., internal states or “thoughts”). While inference-time techniques like chain-of-thought prompting **Hafner, "Chain-of-Thought Prompting"** and self-consistency decoding **Wang, "Self-Consistency Based Reasoning"** have shown that explicit intermediate reasoning improves accuracy, such multi-stage reasoning is rarely incorporated into training itself. Recent work on deliberation-based methods **Bai, "Deliberation-Based Reasoning"** suggest that iterative refinement enhances reasoning quality, but most RL-based approaches lack mechanisms for models to revise, backtrack, or critique their own internal thought processes. As a result, current models struggle to recover from early reasoning errors or refine suboptimal strategies, limiting their robustness and adaptability.