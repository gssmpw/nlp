\section{Related Work}
\paragraph{LongCoT and o1-like Model} 
OpenAI's o1 model \citep{jaech2024openai} employs long chain-of-thoughts, allowing it to leverage rich reasoning actions, such as branching, reflection, verification, to tackle complex problems before arriving at a final answer \citep{dutta2024think}. This approach enhances the model performance in areas such as mathematics, coding, and scientific problems. 
The LongCoT approach aligns with System 2 cognition \citep{kahneman2011thinking}, a mode of deliberate and sequential reasoning that mirrors human problem-solving strategies. By integrating reinforcement learning, o1 can refine its reasoning process dynamically, evaluating multiple solution paths, backtracking when necessary, and improving its approach through iterative self-correction. The shift towards deliberative reasoning represents an important trend in AI research, aiming to make LLMs more transparent, interpretable, and adaptable in complex decision-making scenarios \citep{ackoff1994systems, kahneman2011thinking}.

Despite o1's success, most existing attempts to replicate LongCoT rely on knowledge distillation and manually curated datasets \citep{min2024imitate, huang2024o1}. These approaches pose several challenges: they often fail to generalize beyond the specific training data, require access to high-quality reference models, and lack principled methods for directly training LongCoT reasoning from scratch. A concurrent work by DeepSeek \citep{guo2025deepseek} demonstrated that reinforcement learning applied to a 671B-parameter model can yield LongCoT capabilities. However, such large-scale models introduce significant computational barriers, making broad adoption and reproducibility infeasible. Furthermore, while DeepSeek provides significant transparency regarding their approach, some crucial details, particularly their data curation strategies, remain unclear.


\paragraph{LLM Reinforcement Learning and Self-Improvement}
Reinforcement learning has become a core approach for enhancing LLMs during post-training, particularly for improving the quality of model outputs. Traditional RL algorithms like Proximal Policy Optimization (PPO) \citep{schulman2017ppo} have been effective but computationally expensive, making them less feasible in resource-limited settings. 
Recent efforts have proposed some efficient alternatives. 

Rejection sampling techniques \citep{zelikman2022star,dong2023raft,gulcehre2023reinforced} filter and select the best responses from multiple model-generated candidates, improving training efficiency without requiring full policy optimization. Similarly, Direct Preference Optimization (DPO) methods \citep{rafailov2023direct,munos2023nash,ethayarajh2024kto} bypass explicit reward modeling by directly optimizing on preference data, achieving performance comparable to PPO at significantly lower training costs. Meanwhile, REINFORCE-based approaches \citep{ahmadian2024back, li2023remax,ahmadian2024back,shao2024deepseekmath} further streamline training by eliminating the value function, reducing memory and computation requirements.
These approaches have proven useful in downstream tasks, yielding measurable gains in accuracy and coherence.
Building on them, recent work has explored self-improving LLMs, where models iteratively refine their own outputs using generated feedback loops \citep{xu2023some,snorkelai@pair,xiong2023iterative,yuan2024self,dong2024rlhf,guo2024direct}. These approaches enable LLMs to autonomously evaluate, critique, and improve their responses over multiple iterations, integrating self-feedback into the training process. This fosters an adaptive learning cycle, allowing models to progressively enhance reasoning depth, factual accuracy, and coherence over time.

Despite these advances, most existing RL-based methods focus on single-stage response generation, where models directly produce final answers without refining intermediate reasoning steps (e.g., internal states or “thoughts”). While inference-time techniques like chain-of-thought prompting \citep{wei2022chain} and self-consistency decoding \citep{wang2022self} have shown that explicit intermediate reasoning improves accuracy, such multi-stage reasoning is rarely incorporated into training itself. Recent work on deliberation-based methods \citep{madaan2023selfrefineiterativerefinementselffeedback} suggest that iterative refinement enhances reasoning quality, but most RL-based approaches lack mechanisms for models to revise, backtrack, or critique their own internal thought processes. As a result, current models struggle to recover from early reasoning errors or refine suboptimal strategies, limiting their robustness and adaptability.