\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{algorithm}
\usepackage{hyperref}
\hypersetup{hidelinks}
\usepackage{booktabs}
\usepackage{stfloats}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{textcomp}
\usepackage{array}
\usepackage{colortbl}%
\usepackage{tabularx}
  \newcommand{\myrowcolour}{\rowcolor[gray]{0.925}}
  
\newcommand{\tcp}[1]{\texttt{  // #1}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
    
\markboth{\hskip25PC}
{Khamesian \MakeLowercase{\textit{et al.}}: Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate}

\begin{document}

%\title{Effective Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate}
\title{Type 1 Diabetes Management using GLIMMER: Glucose Level Indicator Model with Modified Error Rate}
\author{Saman Khamesian$^{1, 2}$, Asiful Arefeen$^{1, 2}$, Adela Grando$^{1}$, Bithika Thompson$^{3}$, and Hassan Ghasemzadeh$^{1}$
\thanks{$^{1}$College of Health Solutions, Arizona State University, Phoenix, AZ 85004, USA}
\thanks{$^{2}$School of Computing and Augmented Intelligence, Arizona State University, Tempe, AZ 85281, USA}
\thanks{$^{3}$Department of Endocrinology, Mayo Clinic Arizona, Scottsdale, AZ 85259, USA}
\thanks{{email: \textcolor{blue}{skhamesi@asu.edu}}}}
\maketitle

\begin{abstract}
Managing Type 1 Diabetes (T1D) demands constant vigilance as individuals strive to regulate their blood glucose levels to avert the dangers of dysglycemia (hyperglycemia or hypoglycemia). Despite the advent of sophisticated technologies such as automated insulin delivery (AID) systems, achieving optimal glycemic control remains a formidable task. AID systems integrate continuous subcutaneous insulin infusion (CSII) and continuous glucose monitors (CGM) data, offering promise in reducing variability and increasing glucose time-in-range. However, these systems often fail to prevent dysglycemia, partly due to limitations in prediction algorithms that lack the precision to avert abnormal glucose events. This gap highlights the need for proactive behavioral adjustments. We address this need with \textit{GLIMMER}, \underline{G}lucose \underline{L}evel \underline{I}ndicator \underline{M}odel with \underline{M}odified \underline{E}rror \underline{R}ate, a machine learning approach for forecasting blood glucose levels. GLIMMER categorizes glucose values into normal and abnormal ranges and devises a novel custom loss function to prioritize accuracy in dysglycemic events where patient safety is critical. To evaluate the potential of GLIMMER for T1D management, we both use a publicly available dataset and collect new data involving 25 patients with T1D. In predicting next-hour glucose values, GLIMMER achieved a root mean square error (RMSE) of 23.97 (±3.77) and a mean absolute error (MAE) of 15.83 (±2.09) mg/dL. These results reflect a 23\% improvement in RMSE and a 31\% improvement in MAE compared to the best-reported error rates.
\end{abstract}

\begin{IEEEkeywords}
Glucose level forecasting, automated insulin delivery, type 1 diabetes, machine learning.
\end{IEEEkeywords}

\section{Introduction}
\label{sec:introduction}
\IEEEPARstart{T}{ype} 1 diabetes (T1D) is an autoimmune condition characterized by the destruction of insulin-producing beta cells in the pancreas, leading to a lifelong dependency on exogenous insulin. Managing T1D is particularly challenging due to the need for continuous monitoring and precise insulin dosing to maintain blood glucose levels within a target range. Poor glucose control can lead to severe complications, including cardiovascular diseases, neuropathy, retinopathy, and kidney failure \cite{atkinson2014type, diabetes2005intensive, karvonen2000incidence}. Effective management is crucial for enhancing quality of life and reducing the long-term complications associated with T1D. It is estimated that around 8.4 million people worldwide have T1D, representing approximately 5-10\% of all diabetes cases \cite{guo2022growing}. The prevalence of T1D varies significantly across different populations and regions, highlighting the importance of tailored management strategies to address the unique needs of patients globally. Over the years, several advanced technologies have emerged to aid in managing T1D \cite{guo2022growing}. These technologies aim to improve glycemic control, reduce the risk of complications, and alleviate the daily burden of diabetes management on patients. Each of the following systems has distinct features and functionalities that contribute to more effective diabetes management, addressing different aspects of glucose monitoring and insulin delivery \cite{atkinson2014type, diabetes2005intensive, karvonen2000incidence}.

\subsection{Type 1 Diabetes Management Technologies}
\subsubsection{Self-Monitoring of Blood Glucose}
Self-monitoring of Blood Glucose (SMBG) is essential in managing T1D, allowing patients to regularly measure their blood glucose levels. This enables informed adjustments to diet, insulin therapy, and exercise, helping maintain optimal glucose control and prevent dysglycemia \cite{pan2007self, american20165}. However, SMBG provides only intermittent data, potentially missing significant glucose fluctuations. Additionally, excessive testing can lead to insulin stacking, increasing the risk of iatrogenic hypoglycemia. Proper patient education and adherence to monitoring schedules are crucial to maximize SMBG benefits while minimizing risks \cite{knapp2016self, care20196}.

\subsubsection{Continuous Glucose Monitoring}
Continuous Glucose Monitoring (CGM) systems offer real-time, continuous data on glucose levels by measuring concentrations in the interstitial fluid, closely aligning with plasma glucose values \cite{desalvo2013continuous}. These systems include a sensor, transmitter, and receiver, providing frequent measurements that enhance glycemic control and enable timely interventions. Most FDA-approved CGM devices allow for non-adjunctive use, enabling therapeutic decisions without additional SMBG verification \cite{nielsen2005continuous}. Despite their advantages, CGMs have a physiological delay of about 5-6 minutes between blood and interstitial glucose, necessitating occasional SMBG confirmation during rapid glucose changes \cite{basu2014time}. In addition, patients must take this delay into account when making decisions about dosing insulin. 

\subsubsection{Hybrid Closed-loop Systems}
Hybrid Closed-loop (HCL) systems integrate CGM data with automated insulin delivery, using algorithms to adjust insulin dosing in real-time based on continuous glucose readings \cite{templer2022closed}. This integration reduces the manual burden on patients and helps maintain glucose levels within the target range more effectively. Clinical trials have shown that HCL systems improve glycemic outcomes, such as lowering HbA1c levels and increasing time in range, while enhancing patients' quality of life by minimizing the need for constant monitoring and insulin adjustments \cite{tauschmann2018closed}. As technology advances, HCL systems are expected to offer greater personalization and move closer to fully automated diabetes management solutions.

\subsubsection{Automated Insulin Delivery Systems}
Automated Insulin Delivery (AID) systems combine continuous subcutaneous insulin infusion (CSII) with CGM to dynamically regulate insulin delivery based on real-time glucose data \cite{sherr2022automated, renard2023safety, limbert2024automated}. Utilizing advanced algorithms, AID systems adjust basal insulin rates and administer corrective boluses to maintain glucose levels within desired ranges, reducing the risk of dysglycemia. Systems like the Tandem t:slim X2 with Control-IQ technology\footnote{https://www.tandemdiabetes.com/} automate insulin delivery based on sensor data, significantly increasing time in range and decreasing extreme glucose events \cite{chico2024hybrid, lum2021real}. Meta-analyses have demonstrated that AID systems improve glucose time in range (TIR) by substantial margins compared to conventional therapy, offering a more user-friendly and effective solution for daily diabetes management \cite{beck2023meta}.


\begin{figure*}[b]
\centerline{\includegraphics[scale=0.39]{figures/application.png}}
\caption{GLIMMER's application in a T1D management system integrates with sensor technology, where data is sent directly to a smartphone or transmitted from a CGM device via Bluetooth or other methods. The data is then processed and used as input to the GLIMMER model, which predicts the next hour of glucose levels and accurately reports potential dysglycemic events. This helps patients manage their condition more effectively and supports physicians in making informed decisions about the patient's treatment.}
\label{fig:application}
\end{figure*}

\subsection{Limitations of Prior Research}
Despite advancements in diabetes management technologies, several limitations persist. AID systems often struggle to predict and react quickly to post-meal glucose spikes, delivering corrective boluses that are too slow or insufficient \cite{oviedo2017review, zhu2018deep}. While CGM systems provide real-time glucose data, they lack predictive features to warn users of impending dysglycemia \cite{parkes2000new}. Both AID and HCL systems, although integrating CGM and insulin pumps, still require manual inputs for calibration and meals, limiting automation \cite{pratley2020effect, tauschmann2018closed}. SMBG, with intermittent testing, can miss critical glucose fluctuations, increasing the risk of undetected dysglycemia \cite{american20165}. Additionally, the limited availability of clinical data and the high cost of collecting historical data for training create a cold-start issue for researchers.

\subsection{Proposed Solution}
To address these limitations, we propose GLIMMER (Glucose Level Indicator Model with Modified Error Rate), an innovative machine learning model for continuous blood glucose forecasting. GLIMMER uses CGM data, insulin dosages, and meal inputs as key features and significantly improves prediction accuracy through a custom loss function that applies higher penalties in dysglycemic regions, effectively reducing errors in these critical areas and lowering overall prediction penalty to forecasting outcomes that are closely representing abnormal glucose events. By predicting glucose trends in advance, GLIMMER enhances AID systems, enabling proactive self-management behaviors and insulin adjustments. This allows AID systems to modify insulin delivery before glucose levels reach dangerous thresholds, mitigating physiological delays in glucose sensing and insulin action. Additionally, it allows patients to react to the prediction outcomes and actively engage in appropriate dietary, exercise, and insulin injection behaviors to prevent abnormal blood glucose events. As a result, GLIMMER creates opportunities to improve glycemic control and reduce the risk of adverse events, enhancing the safety and effectiveness of AID systems. In addition to developing an innovative glucose forecasting method, we collected approximately 26,707 hours of data from 25 patients with T1D using AID systems. This newly created dataset enables the evaluation of our model in real-world scenarios and addresses the prevalent challenge of limited data availability for experimentation. An example of GLIMMER's practical application and its potential implementation in clinical practice is illustrated in Fig. \ref{fig:application}.

\section{Related Work}
Advancements in forecasting future glucose levels have been crucial for managing patients with T1D, enabling them to proactively respond to glucose fluctuations and significantly improving glucose control. Marigliano et al. \cite{marigliano2024glucose} demonstrated that integrating predictive alarms with CGM technology reduced hypoglycemic events in adolescents by 40\% and severe hypoglycemia by 60\%, highlighting the tangible benefits of predictive alerts in real-world settings. Vettoretti et al. \cite{vettoretti2020advanced} further supported these findings by showcasing how artificial intelligence-based diabetes management systems can enhance patient outcomes through early glucose predictions, prompting timely interventions such as insulin dose adjustments or dietary changes to maintain stable glucose levels and mitigate risks like cardiovascular disease and nerve damage. Additionally, Shroff et al. \cite{shroff2023glucoseassist} emphasized the shift from reactive to proactive diabetes care with personalized prediction systems that learn individual response patterns, offering tailored alerts to meet each patient's unique physiological needs. These predictive technologies not only set a new standard in diabetes care by focusing on prevention over treatment but also significantly reduce the daily management burden. Arefeen et al. \cite{arefeen2022forewarning} suggest that machine learning algorithms can effectively predict hyperglycemia events using data from controlled feeding trials. In the following sections, we categorize these algorithms based on their architecture and discuss their approaches to predicting abnormal glucose levels, providing early warnings for timely interventions.

\subsection{Evidential Deep Learning and Meta-Learning}
Machine learning techniques, particularly deep learning algorithms, have achieved reliable glucose-level prediction performance with minimal feature engineering required. Zhu et al. \cite{zhu2022personalized} demonstrated the use of evidential deep learning combined with meta-learning to create a model that adapts to individual patient data. This approach significantly enhances prediction accuracy by considering the uncertainty in predictions and personalizing the model to each patient's unique glucose response patterns. This method's strength lies in its ability to provide precise predictions with fewer input features, simplifying the data collection process for patients.

\subsection{Convolutional Recurrent Neural Networks}
Another innovative approach is the use of Convolutional Recurrent Neural Networks (CRNN) \cite{keren2016convolutional} to estimate glucose levels for up to a 60-minute prediction horizon (PH) based on prior CGM data and information on meal and insulin intakes. Li et al. \cite{li2019convolutional} introduced this model, which combines the feature extraction capabilities of convolutional neural networks (CNN) \cite{li2021survey} with the temporal learning capabilities of recurrent neural networks (RNN) \cite{medsker2001recurrent}. The CRNN model demonstrated superior performance in both simulated and real patient data, providing accurate short-term glucose predictions that are essential for proactive diabetes management.

\subsection{Long Short-Term Memory Networks}
Long Short-Term Memory (LSTM) \cite{hochreiter1997long} units have also been employed by Aliberti et al. \cite{aliberti2019multi} to predict glucose levels. In this study, they developed a predictive model for blood glucose levels using a multi-patient dataset, focusing on leveraging the strengths of LSTM networks. The researchers compared the performance of LSTM networks with other models like Non-Linear Autoregressive (NAR) neural networks \cite{billings2013nonlinear} and found that the LSTM model significantly outperformed others in both short- and long-term predictions. The LSTM model demonstrated superior accuracy due to its ability to handle long-term dependencies and mitigate issues like the vanishing gradient problem that commonly affects traditional RNNs. This study's findings underscore the potential of LSTM networks in enhancing predictive accuracy and clinical outcomes for diabetes management.

\subsection{Bi-Directional LSTM Variants}
Further extending the capabilities of LSTM networks, researchers have explored bi-directional LSTM variants for glucose prediction \cite{sun2018predicting}. Butt et al. \cite{butt2023feature} investigated how feature transformation techniques could enhance the efficiency of blood glucose prediction models. By employing bi-directional LSTM units, the model can consider both past and future data points, providing a more comprehensive understanding of glucose trends and improving prediction accuracy.

\subsection{Linear Regression}
The Tandem t:slim X2 with Control-IQ technology employs simple linear regression to forecast blood glucose levels 30 minutes ahead based on previous CGM data, highlighting the importance of understanding these algorithms and their accuracy. To address the challenges of applying linear regression to time-series data and multi-step predictions, Zhang et al. \cite{zhang2021deep} developed a multiple linear regression (MLR) model that predicts each future time step separately. This MLR approach combines \( k \) individual linear regression models, denoted as \( L_i \), each trained to relate the training data \( X_{\text{train}} \) to the CGM values at future time points \( y_{\text{train}}(t + i) \) for \( i = 1 \) to \( k \). For instance, setting \( k = 6 \) or \( 12 \) corresponds to predicting 30 or 60 minutes into the future, respectively, as illustrated in Fig. \ref{fig:multiple_linear_regression}. During the prediction phase, the trained models utilize their respective coefficients and intercepts to forecast glucose levels for the next \( k \) time steps by applying the models to the testing data one time step prior to the first test point. This iterative process is repeated for each row of the testing matrix, enabling the generation of multi-horizon predictions. Despite the inherent difficulties of using linear regression for time-series forecasting, the structured MLR approach enhances the accuracy of glucose level predictions at various future points, thereby improving the reliability of automated insulin delivery systems. Additionally, this method allows for scalability and adaptability in different clinical settings, making it a valuable tool in the ongoing efforts to optimize diabetes management.

\begin{figure}[tb]
\centerline{\includegraphics[scale=0.25]{figures/multiple_linear_regression.png}}
\caption{Block diagram of the MLR model using multi PH \cite{zhang2021deep}.}
\label{fig:multiple_linear_regression}
\end{figure}

\section{Methodology}
\label{sec:metholodogy}
\subsection{Model Architecture}
While many machine learning algorithms are used for event forecasting, CNN-LSTM models excel in continuous multi-modal data by integrating spatially distributed data and capturing time-series patterns. These models are commonly applied in areas such as stock price forecasting, household load prediction, and wind power estimation \cite{lu2020cnn, alhussein2020hybrid, wu2021ultra}. Recent studies, including Jaloli and Cescon (2023) \cite{jaloli2023long}, have demonstrated that a CNN-LSTM model achieved a lower RMSE for glucose level predictions over a longer forecast horizon compared to other methods.

The decision to use a CNN-LSTM model for GLIMMER is rooted in its hybrid architecture, which combines the strengths of CNNs and LSTMs \cite{li2021survey, hochreiter1997long}. The CNN component excels at automatic feature extraction, capturing spatial relationships within the data, while the LSTM component is adept at learning temporal sequences and long-term dependencies. This combination allows the CNN-LSTM model to effectively extract hidden features and correlations among various physiological variables, making it well-suited for forecasting future blood glucose values.

In comparative analyses, the CNN-LSTM model demonstrated superior performance over LSTM, CRNN, and other models, both in terms of predictive accuracy and clinical acceptability \cite{jaloli2023long}. This improved performance is attributed to the model's sophisticated architecture, which includes stacks of convolutional and LSTM layers capable of learning complex, hidden features in multivariate datasets. The model's ability to capture rapid and abrupt changes in continuous glucose monitoring trends, due to its capacity to learn intricate dynamics and correlations between variables, further underscores its suitability for this task. However, it is important to note that the effectiveness of the CNN-LSTM model is contingent on the availability of a sufficiently large dataset, which also increases the computational cost compared to simpler reference models. To address the common issue of overfitting in LSTM networks, we incorporate dropout layers after each convolutional or LSTM layer, which has proven effective in enhancing model robustness \cite{srivastava2014dropout}. 

\subsection{Custom Loss Function}
Glucose values can be categorized into normal and critical regions, with the latter including hyperglycemia and hypoglycemia thresholds. For patients with T1D, incorrect predictions in these critical regions can be particularly dangerous, resulting in missed intervention opportunities and potentially severe health complications. Despite extensive prior research on predicting glucose levels, a recent review by Woldaregay et al. \cite{woldaregay2019data} highlights a significant gap in the field. There is a noticeable lack of comprehensive analysis and modeling concerning the penalties for prediction errors across various dysglycemic regions. Therefore, we propose to penalize the prediction model for errors that occur in critical regions more heavily than those in normal regions. Assuming two thresholds \(T_{hypo}\) and \(T_{hyper}\) representing hypoglycemic threshold and hyperglycemia thresholds, respectively, we classify the blood glucose data \(x\) into three regions: 1 (hypoglycemia) for values below \(T_{hypo}\), 2 (normal) for values between \(T_{hypo}\) and \(T_{hyper}\), and 3 (hyperglycemia) for values above \(T_{hyper}\). Commonly, insulin delivery devices and research articles set these thresholds to 70 mg/dL for hypoglycemia and 180 mg/dL for hyperglycemia \cite{chico2024hybrid}. Although these values may vary slightly from person to person, we use the general values commonly cited in similar studies \cite{marigliano2024glucose, vettoretti2020advanced}, leaving the in-depth analysis to determine optimal thresholds for each patient for future research. Fig. \ref{fig:sample_subject_reading} is an example of CGM readings data that highlights the regions and thresholds.

\begin{figure}[b]
\centerline{\includegraphics[scale=0.3]{figures/readings.png}}
\caption{CGM readings from a patient in the AZT1D dataset on December 19, 2023, showing blood glucose fluctuations over 24 hours. Regions are labeled as follows: (1) Hypoglycemia (below 70 mg/dL, blue), (2) Normal range, and (3) Hyperglycemia (above 180 mg/dL, red). Dashed lines indicate hypoglycemia (blue) and hyperglycemia (red) thresholds.}
\label{fig:sample_subject_reading}
\end{figure}

\begin{equation}
\text{Glucose Level Regions} = 
\begin{cases}
1, & \quad \phantom{T_{hypo} \leq{}} x < T_{hypo} \\
2, & \quad T_{hypo} \leq x \leq T_{hyper} \\
3, & \quad \phantom{T_{hypo} \leq{}} x > T_{hyper}
\label{eq:glucose_region_class}
\end{cases}
\end{equation}

We then break down the total error, which represents the cumulative prediction error across all data points, into individual errors within each critical region. By applying specific weights to these errors, our approach ensures that the model is more sensitive to inaccuracies where they matter most, enhancing its reliability and effectiveness in managing T1D. This total error, which we compute as the summation of individual losses, can be formalized as shown in (\ref{eq}):

\begin{equation} 
{Error}_{total} = \sum_{i=1}^{3} w_i \times {Error}_{i} \label{eq} 
\end{equation}

The total error serves as the model’s loss function aggregated over all instances, enabling it to capture and prioritize prediction accuracy within distinct regions. If we choose Mean Absolute Error (MAE) as the error term \({Error}_i\), and assign weights \(w_{hypo}\), \(w_{normal}\), and \(w_{hyper}\) to the hypoglycemia, normal, and hyperglycemia regions, respectively, the total error can be calculated as follows:

\begin{equation}
{MAE} = \frac{1}{N} \sum_{i=1}^{N} \left| y_i - \hat{y_i} \right|
\label{eq:mae_formula}
\end{equation}

\begin{equation}
\begin{aligned}
{Error}_{total} &=
\frac{w_{hypo}}{n_1} \phantom{i} \sum_{i=1}^{n_1} \left|y_i - \hat{y}_i \right| + 
\frac{w_{normal}}{n_2} \sum_{i=1}^{n_2} \left|y_i - \hat{y}_i \right| \\&+
\frac{w_{hyper}}{n_3} \sum_{i=1}^{n_3} \left|y_i - \hat{y}_i \right|
\label{eq:custom_loss_function}
\end{aligned}
\end{equation}

\noindent where \(n_1\), \(n_2\) and \(n_3\) represent the total number of blood glucose samples in each region, and \(y_i\) and \(\hat{y}_i\) represent the predicted value and true value of the glucose level, respectively. The main research question that remains to be answered is how to determine the parameters of this error equation, including weight and threshold values.

\subsection{Error Weights}
We classify the CGM values according to the threshold values discussed previously to identify the respective regions. Our next step is to finalize the custom loss function by determining the optimal weight parameters for each region, with a primary focus on dysglycemia areas. For this reason and to simplify the optimization problem, we set \(w_{normal}\) to 1, reducing our optimization task to finding the optimal values for \(w_{hypo}\) and \(w_{hyper}\).

To obtain optimal values of \(w_{hypo}\) and \(w_{hyper}\), we propose to use Genetic Algorithms (GA) \cite{mitchell1998introduction}. We chose this technique for several reasons. First, GAs are effective for optimization problems where the fitness function involves running another algorithm, particularly for complex or poorly defined problems. This is exactly the case in the problem at hand, finding the optimal weight values of \(w_{hypo}\) and \(w_{hyper}\) using the fitness function with the overall prediction error as output while also running a machine learning algorithm. Second, GAs do not need derivatives or extra information during the optimization process; they obtain the fitness score directly from the objective function \cite{mitchell1998introduction, jin2005comprehensive, khamesian2024hybrid}. Finally, it is straightforward to implement GAs, especially since we are also working on predicting CGM values with a CNN-LSTM network. Based on these considerations, we define our genetic algorithm's terminologies as follows:

\begin{itemize}
    \item \textbf{Genome or Gene}: A real number between [1, 10] representing a weight.
    \item \textbf{Chromosome or Individual Solution}: A pair of weights \((w_{hypo}, w_{hyper})\).
    \item \textbf{Population}: A pool of chromosomes of the size of N, each representing a candidate solution.
    \item \textbf{Crossover}: In each generation, crossover is performed by averaging the values of two randomly selected parents to create a new child chromosome.
    \item \textbf{Mutation}: The child chromosome undergoes mutation, where a small random perturbation is added to its values. This ensures diversity in the population.
    \item \textbf{Selection}: The best individuals from the current population are combined with the new offspring to form the next generation, preserving strong solutions while introducing variations.
    \item \textbf{Fitness Function}: The fitness function evaluates how well each chromosome performs by calculating the total error using the custom loss function. The fitness score is simply the value of this total error; lower fitness scores indicate better solutions. In each generation, the best scores are recorded, and the individuals with the lowest fitness scores are selected as the best individuals.
\end{itemize}

\begin{algorithm}[htbp]
\caption{Finding the Best Pair of Weights for Each Patient}
\label{alg:genetic_algorithm}

\begin{algorithmic}[1]
\STATE \textbf{Parameters:}
% \STATE $G \gets 25$ \tcp{Number of Generations}
% \STATE $N \gets 20$ \tcp{Population Size}

\FOR{each patient}
    \STATE Initialize population $P$ with random weights $w \in [1, 10]$
    % \STATE $P \gets \text{rand}(N, 2) \times 9 + 1$
    
    \FOR{$g \gets 1$ \TO $G$}
        \FOR{each individual $i \in P$}
            \STATE evaluate fitness $f_i$
        \ENDFOR
        
        \STATE sort population $P$ by fitness
        \STATE $P_{\text{best}} \gets \text{top } N/2 \text{ individuals from } P$
        
        \STATE $P_{\text{offspring}} \gets [\,]$
        
        \WHILE{$|P_{\text{offspring}}| < N/2$}
            \STATE select parents $p_1, p_2 \in P_{\text{best}}$
            \STATE $c \gets \frac{1}{2}(p_1 + p_2)$
            \STATE $m \gets \text{normal~mutation} \sim \mathcal{N}(0, 0.5)$
            \STATE $c \gets \text{clip}(c + m, 1, 10)$
            \STATE $P_{\text{offspring}} \gets P_{\text{offspring}} \cup c$
        \ENDWHILE
        
        \STATE $P \gets P_{\text{best}} \cup P_{\text{offspring}}$
    \ENDFOR
    
    \STATE $w^* \gets \arg\min(f_i)$
    \STATE save $w^*$
\ENDFOR

\end{algorithmic}
\end{algorithm}

By iterating through multiple generations, the genetic algorithm refines the weight parameters, aiming to find the pair \((w_{hypo}, w_{hyper})\) that minimizes the error in critical regions. This process balances the need for generalization across all patients to optimize performance in dysglycemic regions. Algorithm \ref{alg:genetic_algorithm} provides a high-level pseudo-code overview of the genetic algorithm process proposed to identify the best pair of weights. Fig. \ref{fig:methodology} illustrates the complete methodology for the design and evaluation of GLIMMER.

\begin{figure*}[t]
\centerline{\includegraphics[scale=0.27]{figures/methodology.png}}
\caption{The proposed methodology in GLIMMER for predicting blood glucose levels in patients with T1D. CGM data undergo preprocessing and feature extraction before being split for training and testing. A genetic algorithm optimizes parameters for a custom loss function, which is then used to train a CNN-LSTM model. Once trained, the custom loss function's parameters are finalized and applied to evaluate the model's performance using the test data.}
\label{fig:methodology}
\end{figure*}

\section{Evaluation Approach}
\subsection{Dataset}
\subsubsection{OhioT1DM Dataset}
We utilized the OhioT1DM dataset \cite{marling2020ohiot1dm}, which includes data from 12 individuals with T1D. The dataset contains raw glucose values recorded every 5 minutes, along with basal insulin, bolus insulin, and carbohydrate intake over an 8-week period.

\subsubsection{AZT1D Dataset}
In addition to using the OhioT1DM dataset, we also gathered data from 25 patients with T1D on AID systems who visited the endocrinology clinic at the Mayo Clinic in Scottsdale, AZ, between December 2023 and April 2024 for their regular appointments. Informed consents were taken from the recruited participants under the study named \textit{Machine Learning Design to Predict and Manage Postprandial Hyperglycemia in Patients with Type 1 Diabetes} (IRB \# 23-003065). For each patient, the data includes, on average, 26 days of recordings collected in real-world settings, featuring CGM signals captured using Dexcom G6 Pro, insulin logs, meal carbohydrate sizes, and device modes (regular/sleep/exercise) obtained using Tandem t:slim X2 insulin pump. This dataset offers comprehensive insights into diabetes management by documenting various bolus events. These include standard boluses for meal-related carbohydrate intake, correction boluses for glucose adjustments, and automatic corrections made by the pump's algorithm. Each event is meticulously recorded, detailing insulin doses, target blood glucose levels, and user adjustments. This group of participants consists of 13 females and 12 males, aged between 27 and 80 years, with an average age of 59 years. This dataset contained 320,488 CGM entries, covering 26,707 hours of monitoring data.

\subsection{Data Preparation}
The OhioT1DM dataset consists of 24 files containing data from 12 patients for training, validation, and testing. For each individual, there exist two files: one for training and one for testing \cite{marling2020ohiot1dm}. To prepare the data for use in our evaluation, we chronologically partition the training file, using the first 80\% for training and the remaining 20\% for validation. This setup ensures that the validation dataset effectively evaluates the model after hyperparameter tuning without any risk of data leakage. The entire testing file is reserved for testing purposes. We apply a similar approach to the AZT1D dataset, first splitting the data of each patient into 80\% for training and 20\% for testing and then dividing the training set into 80\% for training and 20\% for validation. This method of partitioning has been widely utilized in previous studies on blood glucose prediction \cite{zhu2022personalized, mirshekarian2019lstms}. In addition to the CGM values, basal insulin, bolus insulin, and carbohydrate amounts are included in the datasets.

In addition to existing data that are used as features, we also added other features that might improve the model performance. Using the 200-period moving average as a feature in forecasting CGM values can be highly beneficial. In the context of CGM data, where glucose levels can fluctuate due to various factors like meals, physical activity, and stress, the 200-period moving average helps smooth out these short-term fluctuations. This smoothing effect allows the model to better capture the underlying long-term trends in glucose levels, which are crucial for making accurate predictions \cite{zheng2018feature, brockwell2002introduction}. By focusing on the long-term trend, the model is less likely to overfit short-term spikes or drops, thereby improving its generalization to new data. Another feature we implemented involves assigning a class label to CGM values. As discussed in Section \ref{sec:metholodogy}, we can categorize CGM values based on the hypoglycemia and hyperglycemia thresholds. We assign integer values (1, 2, or 3) to represent different regions: hypoglycemia, normal, and hyperglycemia. These region identifiers serve as an input feature to the model. Overall, the data preprocessing involved gathering 6 input features and applying a sliding window to the multivariate sequences, as shown in Fig. \ref{fig:sample_data_shape}.

\begin{figure}[tb]
\centerline{\includegraphics[scale=0.2]{figures/data_shape.png}}
\caption{For time-series data, we treat it as sequential and use a sliding window to create samples for training a CNN-LSTM model. Since CGM values are recorded every five minutes, the window moves in five-minute steps, capturing both X (input) with 72 units and y (output) with 12 units. Each data unit includes 6 features. The figure above illustrates 3 data samples, each covering a 7-hour period.}
\label{fig:sample_data_shape}
\end{figure}

\begin{table*}[t]
\caption{RMSE for CNN-LSTM Models with Various Configurations of Patient 559 from the OhioT1DM Dataset}
\label{table:cnn_lstm_performance}
\centering
\setlength{\tabcolsep}{1.3em}
{\renewcommand{\arraystretch}{1}
\begin{tabular}{l|c|c|c|c|c|c|c|c}
\toprule
\multirow{2}{*}{\textbf{Convolutional Layers}} & \multicolumn{4}{c|}{\textbf{1 LSTM Layer}} & \multicolumn{4}{c}{\textbf{2 LSTM Layers}} \\ 
& \textbf{4 Units} & \textbf{8 Units} & \textbf{16 Units} & \textbf{32 Units} & \textbf{4 Units} & \textbf{8 Units} & \textbf{16 Units} & \textbf{32 Units} \\ \midrule
1 Layer (32 filters) & 38.62 & 31.48 & 34.22 & 33.94 & 33.04 & 31.90 & 43.78 & 43.02 \\
2 Layers (32 and 16 filters) & 29.28 & 30.12 & 35.12 & 33.72 & 35.17 & 32.28 & 34.63 & 39.84 \\
3 Layers (32, 16 and 8 filters) & 28.86 & \textbf{28.54} & 29.01 & 28.84 & 28.85 & 30.57 & 30.6 & 38.39 \\
\bottomrule
\end{tabular}}
\end{table*}

\subsection{Architecture Configuration}
We reviewed previous studies to determine common configurations for CNN-LSTM models, focusing on the number of layers and LSTM units. From the insights gained in prior studies \cite{el2020deep, dylag2023machine}, we identified a range of configurations, noting that these references utilized 1 to 4 convolutional layers and LSTM units in the range of 8 to 128. This understanding guided us in designing our experiment to systematically investigate the number of layers and units for our CNN-LSTM model. We utilized a subset of the dataset, dividing it into 80\% for training and 20\% for validation. We tested various configurations, including 1, 2, or 3 convolutional layers with 32, 16, and 8 filters and kernel sizes of 4 and 1 or 2 LSTM layers with 4, 8, 16, and 32 units. After each convolutional or LSTM layer, we applied a dropout layer with a rate of 0.1. The results of these experiments are shown in Table \ref{table:cnn_lstm_performance}. The optimal model configuration includes three 1D convolutional layers with kernel sizes of 4 and filter counts of 32, 16, and 8, respectively, to extract features from the input data. These features are then passed to an LSTM layer with 8 units for sequential processing. A flattening layer follows, preparing the features for a dense layer with a ReLU activation function that generates the final output. The proposed architecture is illustrated in Fig. \ref{fig:full_cnn_lstm_architecture}.

\begin{figure}[tb]
\centerline{\includegraphics[scale=0.25]{figures/full_cnn_lstm_architecture.png}}
\caption{Diagram of the proposed CNN-LSTM architecture. The input consists of 6 hours of time-series data with 6 features each, while the output provides a 1-hour prediction of glucose levels.}
\label{fig:full_cnn_lstm_architecture}
\end{figure}

\subsection{Custom Loss Function Parameters Configuration}
We designed an experiment and ran Algorithm \ref{alg:genetic_algorithm} to find the best weights for our custom loss function. For each patient, we created a random population of 20 candidates, each with weight pairs in the range \([1, 10]\), and evolved them over 25 generations following the methodology described in the Error Weights section. To simplify the process, we set \(w_{normal}\) to 1 and focused on optimizing the two parameters, \(w_{hypo}\) and \(w_{hyper}\). In each generation, we evaluated the performance of each candidate by setting the weights for the custom loss function and compiling and running our CNN-LSTM model. We then calculated the RMSE on the validation dataset, which served as the fitness score for each candidate. This process involved selecting the top-performing candidates to form the basis for the next generation, applying crossover and mutation operations to generate new candidates, and repeating the evaluation process. Ultimately, we identified the best weights for each patient. However, calculating the optimal weights for each individual patient is not feasible in a practical setting due to the time-consuming nature of running the genetic algorithm for each case. Instead, to provide a practical solution, we computed the average weights across all patients, resulting in \( (w_{hypo}, w_{hyper}) = (3.296, 2.382) \). This average provides a balanced approach that generalizes well across patients while avoiding the inefficiencies of individual optimization. Fig. \ref{fig:best_weights} shows the optimal weights for each patient alongside the average pair of weights. This method ensures a reasonable approximation of the custom loss function's performance without the impracticality of personalized optimization for each patient.

\begin{figure}[t]
\centerline{\includegraphics[scale=0.575]{figures/best_weights.png}}
\caption{Best pair of weights for all patients in OhioT1DM dataset and the average point.}
\label{fig:best_weights}
\end{figure}

\subsection{Experimental Setup}
After data preparation and finalizing the GLIMMER model by setting up the parameters of the architecture and custom loss function, we conducted extensive experiments using a batch size of 48, over 30 epochs, with a prediction horizon (PH) of 60 minutes. To minimize the impact of randomness, we conducted 10 iterations of the experiments for each patient using a unique seed number for each run and reported the average results of these trials. The code used for these experiments is available for other researchers to reproduce our findings. All experiments were performed on an Apple M3 Pro chip featuring a 12-core CPU, an 18-core GPU, a 16-core Neural Engine, and 18 GB of unified memory.

\subsection{Evaluation Metrics}
To assess the performance of the GLIMMER model, we employed standard metrics commonly used in related studies \cite{zhu2022personalized, li2019convolutional, aliberti2019multi, butt2023feature}, including:

\subsubsection{Root Mean Square Error (RMSE)}
This metric provides insight into the average deviation of predicted values from actual values, with an emphasis on larger errors:
\begin{equation}
\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
\end{equation}
Here, \( n \) represents the number of data points, \( y_i \) denotes the ground truth or actual CGM value, and \( \hat{y}_i \) is the predicted CGM value.

\subsubsection{Mean Absolute Error (MAE)}
This metric quantifies the average absolute difference between predicted and observed values, providing a straightforward interpretation of prediction accuracy. It is calculated as follows:

\begin{equation}
\text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
\end{equation}

\subsubsection{Precision}
This metric indicates the accuracy of the positive predictions made by the model, defined as the ratio of true positives (TP) to the sum of true positives and false positives (FP). It is calculated as:

\begin{equation}
\text{Precision} = \frac{TP}{TP + FP}
\end{equation}

\subsubsection{Recall}
This metric reflects the model's ability to identify all relevant instances, representing the true positive rate. It is the ratio of true positives to the sum of true positives and false negatives (FN):

\begin{equation}
\text{Recall} = \frac{TP}{TP + FN}
\end{equation}

\subsubsection{F1 Score}
The F1 score provides a balance between precision and recall, especially useful when dealing with imbalanced classes. It is calculated as:

\begin{equation}
\text{F1} = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}\\
\end{equation}

\subsubsection{Clarke Error Grid Analysis} The Clarke Error Grid (CEG) analysis \cite{clarke1987evaluating} is a widely accepted tool for evaluating the clinical accuracy of glucose predictions by comparing them to reference glucose values. It classifies predictions into five distinct regions, each representing varying levels of clinical significance:

\begin{itemize}
    \item \textbf{Region A}: Includes values within 20\% of the reference value, indicating clinically accurate predictions.
    \item \textbf{Region B}: Contains values outside of the 20\% range but unlikely to result in inappropriate treatment.
    \item \textbf{Region C}: Identifies predictions that may lead to unnecessary treatment.
    \item \textbf{Region D}: Represents predictions where critical hypoglycemia or hyperglycemia might be missed, posing a potential danger.
    \item \textbf{Region E}: Captures predictions that could lead to confusion between treating hypoglycemia and hyperglycemia, a highly dangerous scenario.
\end{itemize}

\section{Results}
Tables \ref{tab:model_performance_ohio_dataset} and \ref{tab:model_performance_mayo_dataset} summarize the performance comparison of GLIMMER with other methods for the OhioT1DM and AZT1D datasets, both using a PH of 60 minutes. We selected the most recent studies that employed a variety of classical and machine learning models, including Fast-Adaptive and Confident Neural Network (FCNN) \cite{zhu2022personalized}, CRNN \cite{li2019convolutional}, Bi-LSTM \cite{sun2018predicting}, transformer models \cite{yang2020html}, Random Forest Regression (RFR) \cite{georga2012multivariate, zhu2022personalized}, Support Vector Regression (SVR) \cite{georga2012predictive}, and Autoregressive Integrated Moving Average (ARIMA) \cite{plis2014machine}. Additionally, we included the GLIMMER model without modifications, represented as a basic CNN-LSTM, to highlight the effects of our enhancements, which incorporate two crafted features and a custom loss function.

\begin{table*}[b]
\caption{Prediction Performance Comparison on the OhioT1DM Dataset with PH = 60 minutes}
\centering
\setlength{\tabcolsep}{1.2em}
{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l c c c c c c c c}
\toprule
\multirow{2}{*}{\textbf{Model Name}} & \multirow{2}{*}{\textbf{RMSE (mg/dL)}} & \multirow{2}{*}{\textbf{MAE (mg/dL)}} & \multicolumn{5}{c}{\textbf{CEG-Regions (\%)}} \\
\cmidrule(lr){4-8} 
 & & & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{E} \\
\myrowcolour
\hline
FCNN \cite{zhu2022personalized} & 31.07 ± 3.62 & 22.86 ± 2.89 & 72.58 ± 7.87 & 24.39 ± 6.41 & 0.16 ± 0.14 & 2.85 ± 1.68 & 0.02 ± 0.04 \\
\hline
CRNN \cite{li2019convolutional} & 32.02 ± 3.76 & 23.82 ± 3.13 & 71.06 ± 8.69 & 25.57 ± 7.07 & 0.15 ± 0.17 & 3.20 ± 1.99 & \textbf{0.01 ± 0.04} \\
\myrowcolour
\hline
Bi-LSTM \cite{sun2018predicting} & 33.44 ± 3.76 & 24.59 ± 2.89 & 70.61 ± 8.21 & 25.98 ± 6.70 & 0.17 ± 0.13 & 3.19 ± 1.89 & 0.05 ± 0.07 \\
\hline
Transformer \cite{yang2020html} & 32.96 ± 3.70 & 24.19 ± 2.79 & 71.70 ± 7.77 & 25.20 ± 6.43 & 0.15 ± 0.15 & 2.92 ± 1.65 & 0.04 ± 0.05 \\
\myrowcolour
\hline
SVR \cite{georga2012predictive} & 33.83 ± 3.62 & 25.63 ± 2.98 & 66.43 ± 9.15 & 29.61 ± 7.30 & 0.20 ± 0.21 & 3.73 ± 2.62 & 0.03 ± 0.04 \\
\hline
RFR \cite{georga2012multivariate} & 35.31 ± 3.72 & 26.43 ± 3.02 & 67.03 ± 8.17 & 29.38 ± 6.29 & 0.23 ± 0.19 & 3.34 ± 2.14 & 0.02 ± 0.04 \\
\myrowcolour
\hline
ARIMA \cite{plis2014machine} & 35.42 ± 3.74 & 25.97 ± 2.70 & 68.77 ± 6.85 & 28.65 ± 5.83 & 0.46 ± 0.40 & 2.06 ± 1.00 & 0.05 ± 0.05 \\
\hline
Basic CNN-LSTM & 31.98 ± 4.15 & 23.00 ± 2.87 & 74.31 ± 7.03 & 23.12 ± 5.96 & 0.11 ± 0.14 & 2.43 ± 1.56 & 0.03 ± 0.08 \\
\myrowcolour
\hline
\textbf{GLIMMER} & \textbf{23.97 ± 3.77} & \textbf{15.83 ± 2.09} & \textbf{85.46 ± 4.87} & \textbf{13.26 ± 4.29} & \textbf{0.13 ± 0.17} & \textbf{1.12 ± 0.57} & 0.02 ± 0.07 \\
\bottomrule
\end{tabular}}
\label{tab:model_performance_ohio_dataset}
\end{table*}

\begin{table*}[b]
\caption{Prediction Performance Comparison on the AZT1D Dataset with PH = 60 minutes}
\centering
\setlength{\tabcolsep}{1.2em}
{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l c c c c c c c c}
\toprule
\multirow{2}{*}{\textbf{Model Name}} & \multirow{2}{*}{\textbf{RMSE (mg/dL)}} & \multirow{2}{*}{\textbf{MAE (mg/dL)}} & \multicolumn{5}{c}{\textbf{CEG-Regions (\%)}} \\
\cmidrule(lr){4-8} 
 & & & \textbf{A} & \textbf{B} & \textbf{C} & \textbf{D} & \textbf{E} \\
\myrowcolour%
\hline
Basic CNN-LSTM & 29.55 ± 6.49 & 21.61 ± 5.19 & 73.27 ± 8.57 & 24.47 ± 7.65 & 0.03 ± 0.07 & 2.21 ± 1.67 & 0.02 ± 0.05 \\
\hline
\textbf{GLIMMER} & \textbf{22.48 ± 3.57} & \textbf{15.58 ± 2.87} & \textbf{83.89 ± 5.01} & \textbf{14.94 ± 4.54} & \textbf{0.02 ± 0.03} & \textbf{1.12 ± 0.85} & \textbf{0.02 ± 0.04} \\
\bottomrule
\end{tabular}}
\label{tab:model_performance_mayo_dataset}
\end{table*}

The results include RMSE and MAE, presented as (Mean ± Standard Deviation), along with CEG reports, which serve as standard error metrics for predictions. For both RMSE and MAE, lower values indicate better performance. As shown in Table \ref{tab:model_performance_ohio_dataset}, GLIMMER demonstrates outstanding performance compared to other models, achieving a 23\% improvement in RMSE and a 31\% improvement in MAE relative to the best-reported errors. The basic CNN-LSTM results further indicate its potential as a viable candidate for analyses where other models may not be applicable. In the CEG analysis, higher values in Region A are preferable, while lower values are desirable in other regions. Notably, GLIMMER's predictions achieve 85\% within Region A, yielding a 15\% improvement compared to FCNN, and it also maintains one of the lowest values in other regions.

Table \ref{tab:model_performance_mayo_dataset} presents the results for GLIMMER, and the basic CNN-LSTM model applied to the AZT1D dataset. GLIMMER again outperforms the basic CNN-LSTM model, achieving a 24\% improvement in RMSE and a 28\% improvement in MAE. Since this dataset has been recently collected and is not publicly available, we could only generate results for these two models, leaving evaluations for other methods to future researchers. However, given the close results in Table \ref{tab:model_performance_ohio_dataset} for the basic CNN-LSTM and leading models like FCNN and CRNN, we anticipate similar outcomes.

To ensure a fair comparison, we did not include the MLR model in Table \ref{tab:model_performance_ohio_dataset}, as this model employs a multi-model approach that requires training a separate model for each PH. In our case, with a PH of 60 minutes, this necessitates the creation of 12 individual models to predict the next 5, 10, 15, ..., 55, and 60 minutes. In contrast, all the methods presented in Table \ref{tab:model_performance_ohio_dataset} utilize a single model for their predictions, and the MLR model did not report CEG analysis in their studies. While they achieved an RMSE of 24.58 mg/dL and an MAE of 17.42, which are better than those of some other models, GLIMMER still outperformed them.

We visualized the results of the CEG analysis for both the OhioT1DM and AZT1D datasets in Fig. \ref{fig:clarke_grid_error_figure}. The enhanced prediction accuracy of the GLIMMER model compared to the basic CNN-LSTM is evident, demonstrating its ability to accurately forecast blood glucose levels even in critical regions, which is crucial for clinical analysis and real-world applications. Additionally, Fig. \ref{fig:prediction_comparision_sample} illustrates the glucose level predictions of GLIMMER compared to the basic CNN-LSTM. This figure highlights GLIMMER’s ability to accurately predict blood glucose levels, particularly during peaks, due to the integration of the custom loss function. The consistent results across the OhioT1DM and AZT1D datasets further demonstrate that GLIMMER is robust and generalizable, making it suitable for use with various datasets.

\begin{figure*}[t]
    \centering
    \subfloat[\label{v1}]{{\includegraphics[scale=0.21]{figures/clarke_error_grid_ohiot1dm_basic_cnn_lstm.png}}}
    \subfloat[\label{v2}]{{\includegraphics[scale=0.21]{figures/clarke_error_grid_ohiot1dm_glimmer.png}}}
    \quad \quad
    \subfloat[\label{v3}]{{\includegraphics[scale=0.21]{figures/clarke_error_grid_mayo_basic_cnn_lstm.png}}}
    \subfloat[\label{v4}]{{\includegraphics[scale=0.21]{figures/clarke_error_grid_mayo_glimmer.png}}}
    \caption{Clarke Error Grid for the basic CNN-LSTM model and GLIMMER across all patients. Figures a and b pertain to the OhioT1DM dataset (with a for the basic CNN-LSTM and b for GLIMMER), while figures c and d relate to the AZT1D dataset (with c for the basic CNN-LSTM and d for GLIMMER). The detailed percentages for each region are represented in Tables \ref{tab:model_performance_ohio_dataset} and \ref{tab:model_performance_mayo_dataset}. In both datasets, the results are tightly clustered near the x=y line, indicating that GLIMMER's predictions are as close as possible to the reference values.}
    \label{fig:clarke_grid_error_figure}
\end{figure*}

\begin{figure}[t]
\centerline{\includegraphics[scale=0.29]{figures/prediction_ohio_patient_552.png}}
\caption{Forecasting comparison between GLIMMER and the basic CNN-LSTM model. The solid black line represents CGM values from patient 552 in the OhioT1DM test dataset, covering 700 data points at 5-minute intervals. The dashed gray and black lines indicate hypoglycemia and hyperglycemia thresholds at 70 mg/dL and 180 mg/dL, respectively. The red circles highlight the prediction accuracy of the two models, especially during peak values in critical regions.}
\label{fig:prediction_comparision_sample}
\end{figure}

In our analysis, we calculated error metrics separately for normal glucose levels and dysglycemic regions, providing a clearer understanding of the model's performance across different glucose conditions. This detailed breakdown highlights the model's strengths and identifies areas for improvement in managing varying glucose states. Precision and recall offer valuable insights: high precision indicates that the model accurately predicts dysglycemia, reducing false alarms, while high recall shows that the model effectively detects dysglycemic events, minimizing missed occurrences.

Tables \ref{tab:glycemic_errors_for_ohio} and \ref{tab:glycemic_errors_for_mayo} present these metrics for the OhioT1DM and AZT1D datasets, comparing the performance of the basic CNN-LSTM model and GLIMMER. These metrics, along with RMSE and MAE, provide a comprehensive assessment of the model’s reliability and effectiveness in supporting diabetes management and patient safety.
In the OhioT1DM dataset, GLIMMER significantly improves hypoglycemia detection, increasing recall from 16\% to 42\%, meaning it captures more true low blood sugar events, which is vital for patient safety. The F1 score for hypoglycemia rises from 35\% to 44\%, reflecting a better balance between detecting true events and minimizing false positives. For hyperglycemia, GLIMMER also improves recall by 10\%, from 78\% to 86\%, ensuring more high blood sugar episodes are detected. With a 5\% improvement in precision, it reduces false alarms, making the model more reliable and user-friendly in managing glucose levels.
In the AZT1D dataset, GLIMMER shows notable gains in both recall and precision for hypoglycemia detection. Recall improves from 2\% to 13\%, meaning it catches more low blood sugar events, while precision rises from 34\% to 47\%, reducing unnecessary alerts. For hyperglycemia, GLIMMER’s recall jumps from 48\% to 73\%, a 52\% improvement, allowing it to detect more high glucose events. The model also enhances precision by 15\%, ensuring better accuracy in its predictions, making it a more effective tool for managing critical dysglycemic events.

\begin{table*}[t]
\caption{Performance comparison between GLIMMER and the basic CNN-LSTM model in different glucose regions of the OhioT1DM dataset with PH = 60 minutes.}
\centering
\setlength{\tabcolsep}{0.525em}
{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l | c c c c | c c c c}
\toprule
\multirow{2}{*}{\textbf{Metrics}} & \multicolumn{4}{c|}{\textbf{GLIMMER}} & \multicolumn{4}{c}{\textbf{Basic CNN-LSTM}} \\
\cmidrule(lr){2-9} 
 & \textbf{Normal} & \textbf{Dysglycemia} & \textbf{Hyperglycemia} & \textbf{Hypoglycemia} & \textbf{Normal} & \textbf{Dysglycemia} & \textbf{Hyperglycemia} & \textbf{Hypoglycemia} \\
\myrowcolour
\hline
\textbf{RMSE (mg/dL)} & \textbf{21.46 ± 3.29} & \textbf{28.06 ± 6.06} & \textbf{28.77 ± 7.28} & \textbf{24.61 ± 6.98} & 26.43 ± 2.46 & 39.94 ± 7.92 & 40.58 ± 9.77 & 36.24 ± 9.04 \\
\hline
\textbf{MAE (mg/dL)} & \textbf{14.43 ± 2.14} & \textbf{18.54 ± 3.18} & \textbf{19.12 ± 4.05} & \textbf{18.45 ± 8.22} & 19.54 ± 1.90 & 29.33 ± 6.03 & 30.00 ± 7.61 & 30.41 ± 9.21 \\
\myrowcolour
\hline
\textbf{F1 Score (\%)} & \textbf{94.00 ± 1.00} & \textbf{91.00 ± 4.00} & \textbf{85.00 ± 4.00} & \textbf{44.00 ± 23.00} & 94.00 ± 2.00 & 85.00 ± 5.00 & 78.00 ± 6.00 & 35.00 ± 19.00 \\
\hline
\textbf{Recall (\%)} & \textbf{100.00 ± 0.00} & \textbf{100.00 ± 0.00} & \textbf{86.00 ± 5.00} & \textbf{42.00 ± 29.00} & \textbf{100.00 ± 0.00} & \textbf{100.00 ± 0.00} & 78.00 ± 7.00 & 16.00 ± 21.00 \\
\myrowcolour
\hline
\textbf{Precision (\%)} & \textbf{89.00 ± 2.00} & \textbf{84.00 ± 6.00} & \textbf{83.00 ± 5.00} & \textbf{46.00 ± 23.00} & 88.00 ± 3.00 & 74.00 ± 8.00 & 79.00 ± 9.00 & 34.00 ± 27.00 \\
\bottomrule
\end{tabular}}
\label{tab:glycemic_errors_for_ohio}
\end{table*}

\begin{table*}[t]
\caption{Performance Comparison between GLIMMER and the basic CNN-LSTM model in different glucose regions of the AZT1D dataset with PH = 60 minutes.}
\centering
\setlength{\tabcolsep}{0.525em}
{\renewcommand{\arraystretch}{1.4}
\begin{tabular}{l | c c c c | c c c c}
\toprule
\multirow{2}{*}{\textbf{Metrics}} & \multicolumn{4}{c|}{\textbf{GLIMMER}} & \multicolumn{4}{c}{\textbf{Basic CNN-LSTM}} \\
\cmidrule(lr){2-9} 
 & \textbf{Normal} & \textbf{Dysglycemia} & \textbf{Hyperglycemia} & \textbf{Hypoglycemia} & \textbf{Normal} & \textbf{Dysglycemia} & \textbf{Hyperglycemia} & \textbf{Hypoglycemia} \\
\myrowcolour
\hline
\textbf{RMSE (mg/dL)} & \textbf{19.85 ± 3.18} & \textbf{31.63 ± 6.15} & \textbf{31.75 ± 9.41} & \textbf{41.69 ± 21.73} & 23.63 ± 4.52 & 46.42 ± 10.55 & 46.82 ± 13.40 & 57.82 ± 21.75 \\
\hline
\textbf{MAE (mg/dL)} & \textbf{13.99 ± 2.51} & \textbf{22.86 ± 4.68} & \textbf{23.43 ± 8.95} & \textbf{36.15 ± 20.69} & 17.95 ± 4.04 & 37.28 ± 9.43 & 37.97 ± 13.09 & 54.89 ± 22.32 \\
\myrowcolour
\hline
\textbf{F1 Score (\%)} & \textbf{96.00 ± 1.00} & \textbf{82.00 ± 11.00} & \textbf{74.00 ± 9.00} & \textbf{22.00 ± 20.00} & 97.00 ± 3.00 & 58.00 ± 25.00 & 54.00 ± 21.00 & 11.00 ± 13.00 \\
\hline
\textbf{Recall (\%)} & \textbf{100.00 ± 0.00} & \textbf{100.00 ± 0.00} & \textbf{73.00 ± 5.00} & \textbf{13.00 ± 20.00} & \textbf{100.00 ± 0.00} & \textbf{100.00 ± 0.00} & 48.00 ± 25.00 & 2.00 ± 5.00 \\
\myrowcolour
\hline
\textbf{Precision (\%)} & \textbf{93.00 ± 3.00} & \textbf{71.00 ± 14.00} & \textbf{72.00 ± 9.00} & 47.00 ± 26.00 & 94.00 ± 5.00 & 45.00 ± 24.00 & 68.00 ± 18.00 & \textbf{49.00 ± 37.00} \\
\bottomrule
\end{tabular}}
\label{tab:glycemic_errors_for_mayo}
\end{table*}

\section{Discussion}
Our experiments demonstrated that GLIMMER achieves superior performance and accuracy as a predictive model, surpassing the state-of-the-art models. Additionally, CEG analysis validated its reliability in detecting and forecasting dysglycemic events. Recent work by Annuzzi et al. \cite{annuzzi2023exploring} explored how certain features affect blood glucose prediction using XAI methodologies. They reported an RMSE of 24.47 ± 4.27 on the AI4PG dataset, with a PH of 60 minutes. Their model depends on detailed data inputs, including preprandial blood glucose levels, insulin dosages, and meal-related factors such as energy intake, macro-nutrients, glycemic index, and glycemic load. However, we note that in real-world scenarios, obtaining these details can be challenging, as it often requires manual logging of meal and nutritional information. GLIMMER, on the other hand, achieves strong predictions using only features like CGM data, bolus and basal insulin levels, and carbohydrate amounts that are commonly obtained in automated insulin delivery systems by default. As a result, GLIMMER does not impose any additional data collection burden on the patients beyond what the standard of care requires. 

Despite these achievements, certain limitations remain. While GLIMMER shows improvement in F1 score, precision, and recall over the baseline CNN-LSTM model, its hypoglycemia prediction performance is still limited. One reason for this limitation is the sparsity of hypoglycemic events in both the OhioT1DM and AZT1D datasets used in this study. The scarcity of these events means the algorithm has fewer examples from which to learn hypoglycemia patterns effectively. Additionally, we were unable to compare GLIMMER to other models on the AZT1D dataset because the source codes for these methods are not publicly available. In future work, we plan to implement and evaluate additional models to achieve a more comprehensive assessment.

Although the core architecture of GLIMMER is CNN-LSTM, the methodologies presented in this article, including the proposed custom loss function and the optimization algorithm, are model-agnostic and can be adapted to other neural network models as well. Future work will focus on designing more advanced architectures, such as attention-based models, to explore GLIMMER’s potential within more complex time-series forecasting frameworks. We also aim to extend the prediction horizon and incorporate long-term features, which could improve accuracy over longer time periods. As shown in Fig. \ref{fig:application}, integrating GLIMMER with automated insulin delivery devices and developing a smartphone application to alert patients and physicians about potential dysglycemic events could have transformative effects. This application would provide projections for the next hour of blood glucose levels, acting as a preventive tool and allowing for a comparison of GLIMMER’s effectiveness against current methods.

\section{Conclusion}
In this paper, we introduce GLIMMER, a machine learning algorithm with a custom loss function designed for accurate prediction of blood glucose levels and to create more reliable opportunities for behavioral and medical treatments in type 1 diabetes management. Our contribution emphasizes predictions in dysglycemic regions, where patients face dangerous conditions and require precise forecasts to prevent adverse events. Utilizing carefully selected input features and a custom loss function fine-tuned through a genetic algorithm, GLIMMER has demonstrated improved performance over state-of-the-art models, reducing RMSE by 23\% and MAE by 31\% on the OhioT1DM dataset. Additionally, we collected a new dataset containing CGM records and insulin delivery events from 25 patients with T1D, allowing us to validate GLIMMER’s generalizability on a larger, real-world dataset while also creating a valuable resource for further research. GLIMMER can be integrated into automated insulin delivery systems and smartphone applications, supporting patients and physicians in more accurately managing T1D and preventing dysglycemia.

\bibliography{main}
\bibliographystyle{IEEEtran}

\end{document}
