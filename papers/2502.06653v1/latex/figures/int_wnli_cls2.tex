\begin{figure*}[t!]
    \centering
    \begin{minipage}[t]{\linewidth}
        \begin{subfigure}{0.31\linewidth}
            \centering
            \includegraphics[width=\textwidth]{latex/figures/ft/wnli_200_llama3_8b_class2_6bins.png}
            \caption{Finetuning: $y_1$ (Blue) short demonstrations, $y_2$ (Orange) long demonstrations.}\label{fig:ft_og_wnli}
        \end{subfigure}% 
        \hfill
        \begin{subfigure}{0.31\linewidth}
            \centering
            \includegraphics[width=\textwidth]{latex/figures/int/wnli_16_llama3_8b_class2_6bins.png}
            \caption{Intervention: $y_1$ (Blue) long demonstrations, $y_2$ (Orange) short demonstrations.}\label{fig:int_cls_wnli}
        \end{subfigure}
        \hfill
        \begin{subfigure}{0.31\linewidth}
            \centering
            \includegraphics[width=\textwidth]{latex/figures/int/wnli_16_llama3_8b_cls2_random_6bins.png}
            \caption{Intervention: $y_1$ (Blue) and $y_2$ (Orange) demonstrations randomly sampled.}\label{fig:int_ran_wnli}
        \end{subfigure}
    \end{minipage}% 
    \hfill
    \begin{minipage}[c]{\linewidth}
        \caption{\label{fig:int-wnli} WNLI validation set performance on a finetuned Llama 3 (8B) model exhibiting a length bias (see \autoref{fig:ft_og_wnli} for finetuning performance prior to intervention). \autoref{fig:int_cls_wnli} and \autoref{fig:int_ran_wnli} (respectively) show results on two debiasing conditions: ICL demonstrations ($k=16$) sampled from the opposite lengths from what the model saw during finetuning (i.e. $y_1$ long demonstrations, $y_2$ short demonstrations), and random sampling.}
    \end{minipage}
\end{figure*}