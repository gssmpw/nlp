% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[final]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
%\usepackage{graphicx}

\usepackage{microtype}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\definecolor{darkblue}{rgb}{0, 0, 0.5}
\hypersetup{colorlinks=true, citecolor=darkblue, linkcolor=darkblue, urlcolor=darkblue}
\usepackage{multirow}
\usepackage{multicol}
\usepackage{tablefootnote}

\usepackage{amsmath}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{wrapfig}
\usepackage{xcolor} 
\usepackage{graphicx}
\usepackage{float}

\usepackage{wrapfig}

\usepackage{algorithmic}
\usepackage[linesnumbered,ruled]{algorithm2e}

% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

%\title{Learning Shallow Feature Biases In-Context: \\ An Empirical Study and Comparison to Fine-tuning}
%\title{Learning (and Unlearning) Length Biases In-Context}

\title{In-Context Learning (and Unlearning) of Length Biases}

\author{Stephanie Schoch\quad Yangfeng Ji\\
  Department of Computer Science \\
  University of Virginia \\
  Charlottesville, VA 22903 \\
  \texttt{\{sns2gr,yangfeng\}@virginia.edu} 
  }

\begin{document}
\maketitle
\begin{abstract}
Large language models have demonstrated strong capabilities to learn in-context, where exemplar input-output pairings are appended to the prompt for demonstration. However, existing work has demonstrated the ability of models to learn lexical and label biases in-context, which negatively impacts both performance and robustness of models. The impact of other statistical data biases remains under-explored, which this work aims to address. We specifically investigate the impact of length biases on in-context learning. 
We demonstrate that models do learn length biases in the context window for their predictions, and further empirically analyze the factors that modulate the level of bias exhibited by the model. 
In addition, we show that learning length information in-context can be used to counter the length bias that has been encoded in models (e.g., via fine-tuning).
This reveals the power of in-context learning in debiasing model prediction behaviors without the need for costly parameter updates.
\end{abstract}

\section{Introduction}
In-context learning (ICL) has emerged as a new ability in large language models (LLMs), representative of a novel learning paradigm \citep{wei2022emergent}. With in-context learning, an LLM learns to perform an unseen task by seeing a number of demonstrations in the context window \citep{NEURIPS2020_1457c0d6}. Whereas previous methods such as fine-tuning update the model parameters to teach the model a desired task, ICL provides the model with input-output pairs as task exemplars directly at inference, with no parameter updates. While the goal of increased task accuracy is the same, the underlying mechanisms contributing to in-context learning are still being understood. 

\begin{figure}
    \centering
        \includegraphics[width=0.49 \textwidth]{latex/figures/sampling_illustration.pdf}
        \caption{An illustration of our experiment setup and hypothesis. When sampling from the tails of the distribution (left of image), we introduce a data length bias. If the model can learn this shortcut feature in-context, we expect class performance on the data of similar length to be higher than data of the opposite length than what was seen in the context window (right of image).}\label{fig:sampling illlustration}
\end{figure}

This has motivated a body of work aiming to understand how in-context learning works. Some works have demonstrated simlarities between fine-tuning and in-context learning. For example, instability due to the choice of examples occurs both in few-shot finetuning \citep{schick-schutze-2021-just, gao-etal-2021-making} and in-context learning \citep{rubin-etal-2022-learning, liu-etal-2022-makes, wu-etal-2023-self}. However, other work has shown counterintuitive results when comparing the apparent learning mechanisms of in-context learning and finetuning \citep{min-etal-2022-rethinking}. 

A key area that is underexplored is whether in-context learning exhibits similar biases to finetuning with regard to statistical data biases. Statistical data biases can be defined as correlations between features and class labels. Under traditional learning paradigms such as fine-tuning, language models can learn exploitable statistical biases in the data. Such biases, or shallow features, can be exploited by a model as discriminatory features when they exhibit biased distributions across classes or are correlated with a specific class. 
This can lead to overestimates of a model's performance on the underlying task \citep{poliak-etal-2018-hypothesis, gururangan-etal-2018-annotation}. 

Prior work has identified length as an exploitable statistical bias in natural inference datasets. For example, in the MultiNLI and SNLI datasets, length has been shown to be a discriminatory feature \citep{gururangan-etal-2018-annotation}, and on the ROC story cloze task choosing the longer ending performs above random baseline \citep{cai-etal-2017-pay}.
However, length biases have been largely ignored in prior work on ICL, and some existing studies on which factors impact ICL have treated length as a static variable, selecting examples with similar lengths to test inputs \citep{min-etal-2022-rethinking}.
It is therefore unclear whether models can exploit length biases in the data under an in-context learning setting (similar to finetuning) or whether length is another factor with counterintuitive tendencies. 
\begin{figure}
    \centering
        \includegraphics[width=0.49\textwidth]{latex/figures/icl_example.pdf}
        \vspace{-12mm}
        \caption{An overview of in-context learning using $K$ input-output demonstrations concatenated to the test input $\{x_{test}, y_{test}\}$.}\label{fig:icl_desc}
\end{figure}

In this work, we perform a series of empirical studies to investigate the ability of LLMs to learn statistical data biases in the context window during ICL (\autoref{fig:sampling illlustration}). 
This has been studied in the finetuning literature, yet is underexplored in the ICL literature. We demonstrate empirically the ability of LLMs to learn length biases in-context. In the sections to follow, we analyze which factors influence these results, and we show the efficacy of ICL in debiasing finetuned models. Our results show that ICL can introduce biases to LLMs that negatively influence task performance. Specifically, our contributions are as follows:
\begin{enumerate}
    \item We empirically demonstrate the ability of a range of LLM families to learn length biases in-context.
    \item We investigate the influence of number of examples, number of model parameters, and class-length difference on how models learn biases.
    \item We show that ICL can debias a model that contains existing length biases.
\end{enumerate}

\section{Background}
\paragraph{In-context learning}
In-context learning is an emergent ability of LLMs that enables pre-trained models to learn an unseen task using a set of exemplars concatenated in the context window (see \autoref{fig:icl_desc}). Formally, given a test example $x$, in-context learning concatenates $K$ demonstration examples to the task instruction $I$, where $S = \{x_{i}, y_{i}\}_{i=1}^K$ denotes the example set.
The performance of in-context learning, however, is highly dependent on both the selected examples \citep{rubin-etal-2022-learning, liu-etal-2022-makes, wu-etal-2023-self, pmlr-v202-ye23c} and their orderings \citep{lu-etal-2022-fantastically, chen-etal-2023-relation}, resulting in performance variation from nearly random to comparable with finetuned models.

\paragraph{In-Context Learning \& Bias}
While in-context learning has shown significant potential as a way to extract relevant information from an LLM and align the model with user expectations, it has also exhibited brittleness to an assortment of factors. These include selected examples \citep{rubin-etal-2022-learning, liu-etal-2022-makes, wu-etal-2023-self, pmlr-v202-ye23c} and their orderings \citep{lu-etal-2022-fantastically, chen-etal-2023-relation}, which have recently been categorized under the umbrella of demonstration biases \citep{li-etal-2024-debiasing}. 

Beyond demonstration bias, instability of ICL has been attributed to biases in the model toward predicting certain answers due to majority label bias, recency bias, and common token bias \citep{pmlr-v139-zhao21c}. Correspondingly, several works have looked at identifying and mitigating label bias \citep{pmlr-v139-zhao21c, fei-etal-2023-mitigating} \cite{fei-etal-2023-mitigating} with respect to lexical information, and \citet{ali2024mitigating} have looked at mitigating ``copy bias'', where LLMs copy lexical information from demonstrations rather than learning underlying task information. 

However, statistical data biases such as length information have been largely ignored in the in-context learning literature, yet received wide attention in the natural language inference literature with respect to traditional finetuned models \citep{mccoy-etal-2019-right, poliak-etal-2018-hypothesis, cai-etal-2017-pay, gururangan-etal-2018-annotation}. Our work bridges this gap by looking at in-context learning with relation to a specific statistical bias: length bias.

\section{Experiment Setup}
\input{latex/tables/datasets}
\input{latex/tables/models}

In this section, we describe the experiment setup used in our analyses. 

\paragraph{Datasets} We use $7$ binary classification datasets, representing natural language inference, sentiment analysis, and paraphrase detection tasks. As we sample from the tails of the length distributions, binary classification is ideal for our setting. For each dataset, we utilize the splits available from Huggingface. Dataset statistics are provided in \autoref{table:datasets}, with detailed descriptions in \autoref{app:datasets}. To count the length of each input, we use the \textsc{nltk} word-tokenize package \citep{bird-loper-2004-nltk} rather than the LLM-specific tokenizers, to maintain consistency across experiments.
Prompts are adapted from \cite{eval-harness} and provided in \autoref{app:prompts}.

\paragraph{Models} Experiments in \autoref{sec:q1} are run using five models from different LLM families, listed in \autoref{table:models}. The selected models vary in size from 2.7B parameters to 8B parameters. 
Notably, the upper bound of the parameter range is due to our resource constraint, as each experiment is run using a single NVIDIA A100 GPU. For experiments in \autoref{sec:factors} and \autoref{sec:int}, we use a subset of these models, Llama3 and GPT-Neo. 
For experiments in \autoref{sec:v1}, we use the OPT model family. 

\paragraph{Other Details} Following \citet{min-etal-2022-rethinking}, unless otherwise noted, all experiments use $k=16$ demonstrations. For finetuning experiments, we use $k=200$ finetuning examples. To minimize the impact of ordering effects, each result represents the mean of $4$ trials, with standard deviation shown using error bars. Results are all run on the full validation split of each dataset.

In \autoref{sec:q1}, we investigate whether LLMs can learn length biases in-context. To further analyze these results, in \autoref{sec:factors} we look at the impact of model parameter size, number of examples, and length distribution. Finally, \autoref{sec:int} demonstrates the utility of ICL to debias finetuned models that exhibit length biases.


\section{Length Biases in Finetuning and ICL}\label{sec:q1}
In this section, we investigate the question \textbf{\textit{do models learn length biases in-context?}} We demonstrate empirically the ability of LLMs to learn length biases in-context.

\subsection{Method}

\input{latex/figures/q1_icl_hans_cls2}
\input{latex/figures/q1_ft_hans_cls2}
\input{latex/figures/q1_icl_paws_cls2}
\input{latex/figures/q1_ft_paws_cls2}

Consider a dataset $D=\{(x_i ,y_i)\}_{i=1}^n$  that contains $n$ training instances. In this work, we consider binary classification datasets, where $Y=\{y^1, y^2\}$. We aim to introduce a distributional bias in the input lengths with respect to class.  To introduce a length bias in $k$ demonstrations drawn from $D$, we sample from the tails of each class's input length distribution. Specifically, we sample the top-$\frac{k}{2}$ examples belonging to $y^1$ and the bottom-$\frac{k}{2}$ from $y^2$ (and vice versa). This effectively produces a ``worst-case scenario'' in maximizing the distance between the classes under the given length distributions. 

To provide a baseline for comparison, we compare against finetuning. Specifically, we finetune each model (using LoRA adapters \citep{hu2022lora}) on $k=200$ training instances selected using the same procedure as above. As an additional baseline, for all experiments, we compare against randomly sampling the demonstrations and finetuning examples.

For results, we utilize a binning procedure. % adapted from \textcolor{red}{source}. 
Specifically, we bin the validation set based on length, with $b=6$ bins. In this setting, bin $1$ represents the shortest $16.67\%$ of validation instances and bin $6$ represent the longest $16.67\%$ validation instances across both classes. If a model has learned a length bias, for the validation class with the training set drawn from the shortest instances, we expect performance on bin $1$ to be higher than bin $6$, and vice versa for the validation class where the training set is drawn from the longest instances. 

As a performance measurement, we report the sum of the individual accuracy from each class. 
As there may be a slight imbalance across classes in each bin, reporting individual class accuracy rather than the percentage of the entire bin ensures we account for class imbalances across bins.

\subsection{Results}
We report results on HANS and PAWS-X$_{\textsc{EN}}$ under finetuning (\autoref{fig:q1-ft-hans} and \autoref{fig:q1-ft-en}) and ICL (\autoref{fig:q1-icl-hans} and \autoref{fig:q1-icl-en}), where $y_1$ demonstrations were sampled from short instances and $y_2$ demonstrations were sampled from long instances. $y_1$ and $y_2$ correspond to the Blue and Orange bars, respectively.
Our results show decreased performance on validation examples that do not have a similar length as the demonstrations belonging to each respective class. This indicates that models can pick up length biases in-context. Additional results can be found in the Appendix.

\section{Analysis of Influencing Factors}\label{sec:factors}
In this section, we investigate a further question of \textbf{\textit{what factors influence how LLMs learn length biases in-context?}} We find that increased numbers of examples can exacerbate learned biases, and models across a range of sizes can learn length biases. Further, we find that length bias can be learned from as little as a few tokens of difference in average length between classes. 

\subsection{Number of Model Parameters}\label{sec:v1}
Existing work has suggested that the number of model parameters influences the ability of models to learn in-context, with larger models performing better \citep{milios-etal-2023-context, lu-etal-2022-fantastically}. In this section, we investigate whether the number of parameters also influences the ability of models to learn length biases in-context. For example, if larger models are better at learning in-context, are they more susceptible or more resilient to learning statistical biases in the data? 

We use the OPT model family \citep{zhang2022opt} across $p=\{350\mathrm{M}, 1.3\mathrm{B}, 2.7\mathrm{B}, 6.7\mathrm{B}\}$ parameters with $k=16$ in-context examples. Note that the parameter count is upper-bounded based on computational resources. We use the procedure described in \autoref{sec:q1} to introduce a length bias in the in-context demonstrations.

\paragraph{Results}
\input{latex/figures/lineplots/params}

We report results on HANS and PAWS-X$_\textsc{EN}$ in \autoref{fig:num_params}. Notably, both datasets are designed to be challenging (see \autoref{app:datasets} for descriptions). Remaining datasets and conditions are reported in the Appendix. While we do observe length bias across varying model parameter sizes, there is not a consistent pattern of increased or decreased bias with increased model parameter sizes. Accordingly, we observe a dataset-model dependence with regard to the degree of length bias a model may learn.

\subsection{Number of Examples}\label{sec:v2}
The performance of ICL when using various numbers of examples has been studied in prior work \citep{wu-etal-2023-self, min-etal-2022-rethinking, lu-etal-2022-fantastically}. As such, we investigate the sensitivity of LLM's ability to learn length bias across different numbers of in-context examples. 

We use $k=\{2,4,8,16,24,32\}$ in-context examples on the datasets in \autoref{table:datasets} using Llama3 8B and GPT Neo 2.7B. Following the procedure from \autoref{sec:q1}, we select the longest $\frac{k}{2}$ examples from $y^1$ and shortest $\frac{k}{2}$ examples from the $y^2$ (and vice versa), thereby introducing a bias in the length distribution of inputs across classes. 

\input{latex/figures/lineplots/examples}

\paragraph{Results} We report on the PAWS-X$_{EN}$ dataset using Llama3 (8B) in \autoref{fig:num_ex} and provide the average length for each class in \autoref{app:details}. Our results show that models can generally begin learning biases around 8 in-context examples, with the effect typically strengthening with increased numbers of examples. 

Longer context models are gaining traction, with a recent line of work focusing on scaling in-context learning to larger numbers of demonstrations. Longer contexts can increase performance and decrease sensitivity to ordering effects \citep{cai2023scaling, hao2022structured}, and contexts (beginning around $k=8$) can decrease model calibration errors, where calibration is a measure of the faithfulness of a model's predictive uncertainty \citep{zhang-etal-2024-study}. Our results demonstrate that longer contexts exhibit a greater potential for statistical data biases being learned in-context, and underscore the need for balanced selection methods with regard to potential data biases.

\subsection{Difference in Average Demonstration Length Between Classes}\label{sec:v3}

Given the results from the previous section, we investigate whether the difference of the average demonstration length between classes influences the ability of LLMs to identify a length bias. We keep the number of examples consistent at $k=16$ and sample from $p=\{0.25\%, 0.5\%, 0.75\%\}$ of the longest and shortest inputs for each class, respectively. For example, if $y_1$ is the long class, we sample $\frac{k}{2}$ instances from the longest $p=\{0.25\%, 0.5\%, 0.75\%\}$ of the instances belonging to $y_1$, and sample $\frac{k}{2}$ instances from the shortest $p=\{0.25\%, 0.5\%, 0.75\%\}$ of the instances belonging to $y_2$.

\paragraph{Results}
\input{latex/figures/lineplots/length}

We report results using Llama 3 (8B) on the PAWS-X$_{\textsc{EN}}$ dataset in \autoref{fig:len},  where $0.03$ corresponds to an approximate sampling percentage from the previous experiment setup. We observe a length bias across different sampling percentages, despite the decrease in difference between average class lengths (see \autoref{table:avg_len_diff}). Intuitively, as the difference increases, so does the spread between performance across bins of different lengths. This indicates that while models can learn length biases from a few tokens difference (approximately $3$ tokens on HANS under $0.75$ sampling), the biases are amplified in the model as they are amplified in the demonstrations.

\input{latex/figures/int_hans_cls2}
\input{latex/figures/int_paws_cls2}
\input{latex/figures/int_wnli_cls2}

\section{ICL for Debiasing Finetuned Models}\label{sec:int}
In-context learning eliminates the need for expensive model parameter updates incurred when finetuning. However, it is often the case that a model may have encoded biases picked up from the pretraining and/or finetuning. As our previous experiments show that in-context learning can learn length information, a natural extension is to question whether ICL can be used to ``unlearn'' or mitigate previously learned length biases.
In this section, we answer the question \textbf{\textit{can ICL be used as an intervention to mitigate biases in finetuned models?}}

We use previously finetuned models from \autoref{sec:q1} and modify the length distribution to try to counteract the biases. Specifically we experiment with two conditions: 1) using in-context demonstrations drawn from the opposite tail of the length distribution from what was seen during finetuning, and 2) using randomly sampled in-context demonstrations. We again use Llama3 8B and GPT-Neo 2.7B for these experiments using the datasets in \autoref{table:datasets}.

\paragraph{Results}

Results using Llama3 (8B) on HANS, PAWS-X$_{\textsc{EN}}$, and WNLI are reported in \autoref{fig:int-hans}, \autoref{fig:int-paws} and \autoref{fig:int-wnli}, respectively. We find that random sampling was able to counteract the bias, essentially ``unlearning'' the finetuned bias. This suggests that balanced data sampling is critical to minimize the likelihood of learning biases in-context. Further, if a dataset is balanced, random sampling may be sufficient. However, if a dataset contains shortcut features, more sophisticated sampling methods to mitigate the bias may be necessary.

Moreover, our results suggest balanced sampling over showing the models demonstrations of opposite lengths with respect to the finetuned bias. Specifically, the models learned the bias in the length information in the context window, regardless of how it contradicted what was seen during finetuning. One possible explanation is that the task may be implicitly encoded during pretraining and ICL extracts the knowledge \citep{xie2022an, min-etal-2022-rethinking}, however, further study is warranted on whether knowledge-extraction from ICL overrides knowledge-gain during finetuning.

\section{Discussion}
In this work, we investigate the impact of demonstration length bias on model performance when learning in-context. Under this setting, length is a statistical data bias, where the shallow feature (length) is correlated with class labels. It is important to make the distinction, however, between length as a linguistic feature containing information relevant to the underlying task, and length as an artifact of the data collection protocol. For example, length is an informative syntactic feature in classifying truthfulness vs. deceptiveness \citep{yancheva-rudzicz-2013-automatic} and authorship detection \citep{yule1939sentence}, however, length biases can also arise from artifacts reflective of heuristics used by human data annotators \citep{gururangan-etal-2018-annotation}. Our work pertains to the latter settings where length is an artifact rather than a task-informative natural language feature.

\paragraph{Which variables have the greatest impact on models learning length biases?} In \autoref{app:details}, we observe that when varying the number of in-context examples, the distance between classes is greater with fewer in-context examples. However, the amount of bias increases with  increased numbers of examples. Further, while we observe that bias increases with increased length difference, we still observe learned bias when class length difference is reduced to as few as $3$ tokens on the HANS dataset. This suggests that a key factor in learning bias is the number of examples the model sees. Additionally, our results suggest that any model can learn bias, and model parameter size is not necessarily correlated with increased ability to learn biases in-context.


\section{Conclusion} In this work, we empirically investigated the ability of LLMs to learn length biases under an in-context learning paradigm. Our results show that LLMs can learn statistical biases in the data. We further show the impact of model parameter sizes, number of examples, and class length difference on length biases. Finally, we demonstrate the potential for ICL to be used as a tool to debias fine-tuned models with previously learned length biases.

\section{Limitations}
While we test models up to $8\mathrm{B}$ parameters, we acknowledge a limitation of this work is the parameter threshold due to available computational resources. We believe our results scale to larger models.


\section*{Acknowledgments}
We thank the reviewers for their insightful feedback, which has helped us improve this paper significantly. We also thank Jason Stock for his helpful feedback and suggestions. This research was supported in part by NSF SaTC \#2124538 and NSF III \#2007492.

\bibliography{custom}
\clearpage
\appendix

\section{Appendix}
\subsection{Datasets}\label{app:datasets}
\input{latex/tables/dataset_descriptions}

\subsection{Prompts}\label{app:prompts}
\input{latex/tables/prompts}

\subsection{Other Details}\label{app:details}
\input{latex/tables/avg_length}
\input{latex/tables/avg_len_diff}

\clearpage
\subsection{Additional Length Bias Results}\label{app:q1-results}
\input{latex/figures/q1_cls1}
\input{latex/figures/q1_cls2}
\input{latex/figures/q1_rand}

\clearpage
\subsection{Additional Model Parameter (OPT) Results}\label{app:opt-results}
Each of the following figures shows validation performance when varying the number of model parameters using the OPT model family. Bin 0 contains the shortest demonstrations and Bin 5 contains the longest demonstrations. Each subfigure shows the validation accuracy on a single class when in-context instances belonging to the respective class were sampled from long instances, short instances, and randomly sampled (left to right).
\input{latex/figures/lineplots/param_app}

\clearpage
\subsection{Additional Number of Examples Results}\label{app:num-ex-results}
Each of the following figures shows validation performance when varying the number of examples using Llama3 8B and GPT Neo 2.7B. Bin 0 contains the shortest demonstrations and Bin 5 contains the longest demonstrations. Each subfigure shows the validation accuracy on a single class when in-context instances belonging to the respective class were sampled from long instances, short instances, and randomly sampled (left to right).

\input{latex/figures/lineplots/num_ex_llama3}
\input{latex/figures/lineplots/num_ex_gpt}

\clearpage

\subsection{Additional Length Difference Results}\label{app:len-diff-results}
Each of the following figures shows validation performance when varying the sampling percentage from each class using Llama3 8B and GPT Neo 2.7B. Bin 0 contains the shortest demonstrations and Bin 5 contains the longest demonstrations. Each subfigure shows the validation accuracy on a single class when in-context instances belonging to the respective class were sampled from long instances (left) and short instances (right).
\input{latex/figures/lineplots/length_llama3}
\input{latex/figures/lineplots/length_gpt}

\clearpage
\subsection{Additional Intervention Results}\label{app:int-results}
Each of the following figures shows validation set performance on a finetuned Llama 3 8B or GPT Neo 2.7B model exhibiting a length bias. 
For each figure, (a) shows finetuning performance prior to intervention). (b) and (c) show results on two debiasing conditions: ICL demonstrations ($k=16$) sampled from the opposite lengths from what the model saw during finetuning (e.g $y_1$ long demonstrations, $y_2$ short demonstrations), and random sampling, respectively.

\input{latex/figures/int_llama3_cls2}
\input{latex/figures/int_llama3_cls1}
\input{latex/figures/int_gpt_cls2}
\input{latex/figures/int_gpt_cls1}

\end{document}
