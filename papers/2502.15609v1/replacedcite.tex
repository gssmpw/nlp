\section{Related works}
\textbf{In-context learning via transformers.}
%The powerful performance of large language models based on transformers may come from the in-context learning (ICL) ability of transformer____. In-context learning refers to the model's ability to flexibly adjust predictions based on additional data given in context contained in the input sequence itself, without updating parameters.
The powerful performance of transformers is generally believed to come from its in-context learning ability____. A line of recent works study the phenomenon of in-context learning from both theoretical ____ and empirical ____ perspectives on diverse settings. ____ first showed that GPT-3 can perform in-context learning. 
% Subsequently, numerous studies have relied on diverse settings to investigate the in-context learning ability of transformers. 
____ studied the role of different heads within transformers in performing in-context learning focusing on the sparse linear regression setting. ____ %first 
studied the ability of one-layer linear transformers to perform in-context learning for %random 
linear classification tasks. 
% These works build the foundation of the theoretical analysis for transformer models, which mainly focus on the standard training and test in-context learning behaviors, i.e., the distribution of the context examples are the same in both training and test phases. 


%\CC{which is related to our work. In this work, we maintain the linear classification setting which fits well with our modeling to study the robustness of transformers.}

\textbf{Mechanism interpretability of transformers.}
%To theoretically examine the expressive power of transformers
Among the various theoretical interpretations of transformers____, one of the most widely studied theories is the ability of transformers to implement optimization algorithms such as gradient descent____.
%Oswald et al.
____ theoretically and empirically proved that transformers can learn in-context by implementing a single step of gradient descent per layer. %Ahn et al. ____\CC{ 
____ theoretically analyzed that transformers can learn to implement preconditioned gradient descent for in-context learning.%Zhang et al 
____ considered ICL in the setting of linear regression with a non-zero mean Gaussian prior, a more general and common scenario where different tasks share a signal, which is highly relevant to our work.

\textbf{Robustness of transformers.}
The security issues of large language models have always attracted a great deal of attention____. However, most of the research focuses on jail-breaking black-box models____, such as context-based adversarial attacks____. There is very little white-box interpretation work of attacks on the transformer, the foundation model of LLMs____. 
%Qiang et al. 
____ first considered attacking large language models during in-context learning, but they did not study the role of transformers in robustness. 
% ____ studied the adversarial robustness of in-context learning in transformers based on linear regression problems, but they did not theoretically explain the internal mechanism of robustness. 
____ proposed the phenomenon of context hijacking, which became the key motivation of our work. They analyzed this problem from the perspective of associative memory models instead of the in-context learning ability of transformers.%In contrast, our paper formally models the context hijacking phenomenon from the perspective of in-context learning, which has received much attention in transformers and LLMs research, and analyzes the robustness of transformers theoretically and empirically.