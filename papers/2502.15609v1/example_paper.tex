%%%%%%%% ICML 2025 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

\documentclass{article}

% Recommended, but optional, packages for figures and better typesetting:
\usepackage{microtype}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{booktabs} % for professional tables

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2025} with \usepackage[nohyperref]{icml2025} above.
\usepackage{hyperref}
\usepackage{bbm}


% Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Use the following line for the initial blind version submitted for review:
\usepackage[accepted]{icml2025}

% If accepted, instead use the following line for the camera-ready submission:
%\usepackage[accepted]{icml2025}

% For theorems and such
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{enumitem}
\allowdisplaybreaks
% if you use cleveref..
\usepackage[capitalize,noabbrev]{cleveref}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% THEOREMS
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\theoremstyle{plain}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

% Todonotes is useful during development; simply uncomment the next line
%    and comment out the line below the next line to turn off comments
%\usepackage[disable,textsize=tiny]{todonotes}
\usepackage[textsize=tiny]{todonotes}
\def\CC{\textcolor{red}}


\usepackage{mylatexstyle}

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{On the Robustness of Transformers against Context Hijacking for Linear Classification}

\begin{document}

\twocolumn[
\icmltitle{On the Robustness of Transformers against Context Hijacking \\ for Linear Classification}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2025
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Tianle Li}{equal,1}
\icmlauthor{Chenyang Zhang}{equal,2}
\icmlauthor{Xingwu Chen}{3}
\icmlauthor{Yuan Cao}{2}
\icmlauthor{Difan Zou}{1,3}
%\icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
%\icmlauthor{Firstname7 Lastname7}{comp}
%\icmlauthor{}{sch}
%\icmlauthor{Firstname8 Lastname8}{sch}
%\icmlauthor{Firstname8 Lastname8}{yyy,comp}
%\icmlauthor{}{sch}
%\icmlauthor{}{sch}
\end{icmlauthorlist}

\icmlaffiliation{1}{Insititute of Data Science, The University of Hong Kong}

\icmlaffiliation{2}{Department of Statistics and Actuarial Science, The University of Hong Kong}

\icmlaffiliation{3}{Department of Computer Science, The University of Hong Kong}

%\icmlaffiliation{yyy}{Department of XXX, University of YYY, Location, Country}
%\icmlaffiliation{comp}{Company Name, Location, Country}
%\icmlaffiliation{sch}{School of ZZZ, Institute of WWW, Location, Country}

\icmlcorrespondingauthor{Difan Zou}{dzou@cs.hku.hk}
%\icmlcorrespondingauthor{Firstname2 Lastname2}{first2.last2@www.uk}

% You may provide any keywords that you
% find helpful for describing your paper; these are used to populate
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

% this must go after the closing bracket ] following \twocolumn[ ...

% This command actually creates the footnote in the first column
% listing the affiliations and the copyright notice.
% The command takes one argument, which is text to display at the start of the footnote.
% The \icmlEqualContribution command is standard text for equal contribution.
% Remove it (just {}) if you do not need this facility.

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
\printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
Transformer-based Large Language Models (LLMs) have demonstrated powerful in-context learning capabilities. However, their predictions can be disrupted by factually correct context, a phenomenon known as context hijacking, revealing a significant robustness issue. To understand this phenomenon theoretically, we explore an in-context linear classification problem based on recent advances in linear transformers. In our setup, context tokens are designed as factually correct query-answer pairs, where the queries are similar to the final query but have opposite labels.
Then, we develop a general theoretical analysis on the robustness of the linear transformers, which is formulated as a function of the model depth, training context lengths, and number of hijacking context tokens. A key finding is that a well-trained deeper transformer can achieve higher robustness, which aligns with empirical observations. We show that this improvement arises because deeper layers enable more fine-grained optimization steps, effectively mitigating interference from context hijacking. This is also well supported by our numerical experiments. Our findings provide theoretical insights into the benefits of deeper architectures and contribute to enhancing the understanding of transformer architectures.
%To theoretically understand this phenomenon, we consider a linear classification task involving ``context hijacking'' examples, where the in-context tokens exhibit a similar pattern to the query token but have opposite labels. We analyze the robustness of trained transformers with varying depths against ``context hijacking'' examples and theoretically demonstrate that transformers with deeper-layer architectures can achieve higher test accuracy. This improvement arises because the deeper-layer architecture of transformers is equivalent to performing more refined optimization steps, effectively mitigating the interference caused by 'context hijacking' examples. Our results provide a theoretical explanation for the benefits of deeper-layer architectures, particularly from the perspective of robustness to context hijacking.


% To theoretically understand this phenomenon, we consider a linear classification task involving ``context hijacking'' examples, where the in-context tokens exhibit a similar pattern to the query token but have opposite labels. We examine the robustness of trained transformers with varying depths against context hijacking and theoretically demonstrate that transformers with deeper layers can achieve higher test accuracy.  This is because deeper layers are equivalent to implement a more refined optimization process, which distribute the interference from ``context hijacking'' examples. 
% Our results contribute to the theoretical explanations of the benefits of deeper-layer architecture from the perspective of robustness to context hijacking.

% To theoretically analyze this phenomenon, we examine a linear classification task involving 'context hijacking' examples, where the in-context tokens share a similar structure but have labels opposite to that of the query token.


% and examine the robustness of trained transformers to XXX with varying layers. 
%, where the in-context data are \CC{XXX}. 
% However, the theoretical understanding of the transformers remains insufficient, especially the benefits of deeper-layer architecture. 
\end{abstract}

\section{Introduction}
Transformers~\cite{vaswani2017attention} have demonstrated remarkable capabilities in various fields of deep learning, such as natural language processing~\cite{radford2019language,achiam2023gpt,vig2019analyzing,touvron2023llama,ouyang2022training,devlin2018bert}. A common view of the superior performance of transformers lies in its remarkable in-context learning ability ~\cite{brown2020language,chen2022meta,liu2023pre}, that is, transformers can flexibly adjust predictions based on additional data given in context contained in the input sequence itself, without updating parameters. This impressive ability has triggered a series of theoretical studies attempting to understand the in-context learning mechanism of transformers ~\cite{olsson2022context,garg2022can,xieexplanation,guotransformers,wumany}. These studies suggest that transformers can behave as meta learners~\cite{chen2022meta}, implementing certain meta algorithms (such as gradient descent~\cite{von2023transformers,ahn2023transformers,zhang2024context} based on context examples, and then applying these algorithms to the queried input.

Despite the benefits of in-context learning abilities in transformers, this feature can also lead to certain negative impacts. Specifically, while well-designed in-context prompts can help generate desired responses, they can also mislead the transformer into producing incorrect or even harmful outputs, raising significant concerns about the robustness of transformers ~\cite{chowdhury2024breaking,liu2023trustworthy,zhao2024evaluating}. For instance, a significant body of work focuses on jailbreaking attacks ~\cite{chao2023jailbreaking,niu2024jailbreaking,shen2024anything,deng2023jailbreaker,yu2023gptfuzzer}, which aim to design specific context prompts that can bypass the defense mechanisms of large language models (LLMs) to produce answers to dangerous or harmful questions  (e.g., ``\texttt{how to build a bomb?}''). It has been demonstrated that, as long as the context prompt is sufficiently long and flexible to be adjusted, almost all LLMs can be successfully attacked ~\cite{anil2024many}. These studies can be categorized under adversarial robustness, where an attacker is allowed to perturb the contextual inputs arbitrarily to induce the transformer model to generate targeted erroneous outputs ~\cite{shi2023large,pandia2021sorting,creswellselection,yoranmaking}. 




% In-context learning capability brings benefits to transformers that effectively optimize results with the information from context, but this capability may also have a negative impact, leading to a decrease in the robustness of the model. The adversarial attack is a typical example of using context to affect model output\CC{cite}. Adversarial attacks refer to slightly interfering with the input data to induce the model to produce erroneous output, which is one of the main means of jailbreaking LLMs. The robustness of LLMs is easily affected by adversarial attacks, as these attacks often add malicious tokens to the input context to bypass the defenses of LLMs\CC{cite}.
%Among them, a line of research investigates the capacity of transformers to implement iterative optimization algorithms \CC{cite}. They empirically and theoretically demonstrate that one layer of a transformer is equivalent to performing a single step of gradient descent on a linear model using in-context data. Such an equivalence converts the original problem focusing on the expressive power of transformers, which might be tricky due to the complicated structure, to a well-understood optimization problem, and provides a start point for subsequent studies.
%In practice, in-context learning ability can also be largely reflected in transformer-based large language models (LLMs)\CC{cite}, which have been widely studied recently.
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{ch.png}
    \vskip -0.in
    \caption{\textbf{Context hijacking phenomenon in LLMs of different depths.} \textit{Left}: If there are no or only a few factually correct prepends, LLMs of different depths can correctly predict the next token. When the number of prepends increases, the outputs of models are disrupted, and deeper models are more robust. \textit{Right}: Four different types of tasks are introduced, each with a fixed template, and tested on LLMs of different depths. The horizontal axis is the model with depth from small to large, and the vertical axis is the average number of prepends required to successfully interfere with the model output. Experiments show that deeper models perform more robustly. (Experimental setup is given in Appendix \ref{appen:chinLLMs})}
    \label{fig:realworldexpr}
    \vskip -0.1in
\end{figure*}

%As the most advanced application of transformers in the field of natural language processing, LLMs have great commercial and research value. Since the emergence of LLMs, its security issues have always attracted a lot of attention, such as robustness\CC{cite}. 
However, in addition to the adversarial attack that may use harmful or incorrect context examples, it has been shown that the predictions of LLMs can also be disrupted by harmless and factually correct context. Such a phenomenon is referred to as \textit{context hijacking} \cite{jiangllms,jeong2023hijacking}, which is primarily discovered on fact retrieval tasks, i.e. the output of the LLMs can be simply manipulated by modifying the context with additional factual information. For example, as shown in Figure \ref{fig:realworldexpr}, the GPT2 model can correctly answer the question ``\texttt{\textcolor{blue}{Rafael Nadal's best sport is}}'' with ``\texttt{\textcolor{teal}{tennis}}'' when giving context examples. However, if factually correct context examples such as ``\texttt{\textcolor{red}{Rafael Nadal is not good at playing} \textcolor{orange}{basketball}}'' are provided before the question, the GPT-2 model may incorrectly respond with ``\texttt{\textcolor{orange}{basketball}}''.
Then, it is interesting to investigate whether such a phenomenon depends on different tasks and transformer architectures. To this end, we developed a class of context hijacking tasks and counted the number of context examples that led to incorrect outputs (see Figure \ref{fig:realworldexpr}). Our findings indicate that increasing the number of prepended context examples amplifies the effect on the transformer's prediction, making it more likely to generate incorrect outputs. Additionally, we observed that deeper transformer models exhibit higher robustness to context hijacking, requiring more prepended context examples to alter the model's output. Therefore, conducting a precise robustness analysis regarding context hijacking could provide valuable insights in understanding the architecture of the transformer model.

% However, when we increase the number of prepends to 2, 8, and 11 respectively, GPT2 of different depths will be interfered with by the factual context and output ``\texttt{basketball}". The phenomenon of context hijacking reveals the robustness issues of LLMs as the predictions can be manipulated by factually correct context. Facing context hijacking, an intuitive insight is that deeper models may have stronger robustness, which can be verified by real-world observations and experiments. \CC{figure} However, the mechanism behind the connection between the models and their robustness remains unknown, revealing a huge gap between theory and practice.

In this paper, we aim to develop a comprehensive theoretical analysis on the robustness of transformer against context hijacking. In particular, we follow the general design of many previous theoretical works \citep{olsson2022context,ahn2023transformers,frei2024trained} on the in-context learning of transformers, by considering the multi-layer linear transformer models for linear classification tasks, where the hijacking examples are designed as the data on the boundary but with an opposite label to the queried input. Starting from the view that the $L$-th transformer models can implement $L$-step gradient descent on the context examples, with an arbitrary initialization, we formulate the transformer training as finding the optimal multi-step gradient descent methods with respect to the learning rates and initialization. Then, we prove the optimal multi-step gradient strategy, and formulate the optimal learning rate and initialization as the function of the iteration number (i.e., model depth) and the context length. Furthermore, we deliver the theoretical analysis on the robustness based on the proved optimal gradient descent strategy, which shows that as the transformer become deeper, the corresponding more fine-grained optimization steps can be less affected by the hijacking examples, thus leading to higher robustness. This is well aligned with the empirical findings and validated by our numerical experiments. We summarize the main contributions of this paper as follows: 
% A line of research investigates the capacity of transformers to implement iterative optimization algorithms \CC{cite}. They empirically and theoretically demonstrate that one layer of a transformer is equivalent to performing a single step of gradient descent on a linear model using in-context data. Motivated by this view, we consider the robustness of transformers to the phenomenon of context hijacking. In this paper, we propose a comprehensive theoretical framework which can explain the context hijacking phenomenon from the perspective of in-context learning of transformers, complementing the white-box interpretation of the robustness of transformers and LLMs. Specifically, we first model the context hijacking phenomenon of language models as a linear classification problem on linear transformers in a way that is consistent with semantic understanding. Then we divide our theory framework into two stages: pre-training and testing. Starting from the view that transformers can implement gradient descent steps, we theoretically prove that deeper transformers can perform more refined optimization steps on context samples by showing trained transformers can learn the optimal multi-step gradient strategy. In addition, we experimentally verify our theoretical results with experiments under different settings, including linear transformers, standard transformers, and gradient descent. The main contributions of this paper are highlighted as follows:
\begin{itemize}[leftmargin=*,nosep]
    \item We develop the first theoretical framework to study the robustness of multi-layer transformer model against context hijacking, where the hijacked context example is designed as the data with the factually correct label but close to the prediction boundary. This is different from a very recent related work on the robustness of transformer \citep{anwar2024adversarial} that allows the context data to be arbitrarily perturbed, which could be factually incorrect.
    \item Based on the developed theoretical framework, we formulate the test robust accuracy of the transformer as a function with respect to the training context length, number of hijacked context examples, and the depth of the transformer model.  The key of our analysis is that we model the in-context learning mechanism of a well-trained multi-layer transformer as an optimized multi-step gradient descent, where the corresponding optimal initialization and learning rates can be precisely characterized. This could be independent interest to other problems that involve the gradient descent methods on linear problems.
    \item Based on the developed theoretical results, we demonstrate that deeper transformers are more robust because they are able to perform more fine-grained optimization steps on the context samples, which can potentially explain the practical observations of LLMs in the real world (see Figure \ref{fig:realworldexpr}). The theoretical results are well supported by synthetic numerical experiments in various settings.
\end{itemize}
\textbf{Notations.} Given two sequences $\{x_n\}$ and $\{y_n\}$, we denote $x_n = O(y_n)$ if there exist some absolute constant $C_1 > 0$ and $N > 0$ such that $|x_n|\le C_1 |y_n|$ for all $n \geq N$. Similarly, we denote $x_n = \Omega(y_n)$ if there exist $C_2 >0$ and $N > 0$ such that $|x_n|\ge C_2 |y_n|$ for all $n > N$. We say $x_n = \Theta(y_n)$ if $x_n = O(y_n)$ and $x_n = \Omega(y_n)$ both holds. Additionally, we denote $x_n = o(y_n)$ if, for any $\epsilon > 0$, there exists some $N(\epsilon) > 0$ such that $|x_n|\le \epsilon |y_n|$ for all $n \geq N(\epsilon)$, and we denote $x_n = \omega(y_n)$ if $y_n = o(x_n)$. We use $\tilde O(\cdot)$, $\tilde \Omega(\cdot)$, and $\tilde \Theta(\cdot)$ to hide logarithmic factors in these notations respectively. Finally, for any $n\in \NN_+$, we use $[n]$ to denote the set $\{1, 2, \cdots, n\}$.
\section{Related works}
\textbf{In-context learning via transformers.}
%The powerful performance of large language models based on transformers may come from the in-context learning (ICL) ability of transformer~\cite{brown2020language}. In-context learning refers to the model's ability to flexibly adjust predictions based on additional data given in context contained in the input sequence itself, without updating parameters.
The powerful performance of transformers is generally believed to come from its in-context learning ability~\cite{brown2020language,chen2022meta,min2022metaicl,liu2023pre,xieexplanation}. A line of recent works study the phenomenon of in-context learning from both theoretical ~\cite{bai2024transformers,guotransformers,lintransformers,chen2024transformers, frei2024trained,huangcontext,siyu2024training,lifine} and empirical ~\cite{garg2022can,akyurek2022learning,li2023transformersa,raventos2024pretraining,pathaktransformers,panwarcontext,bhattamishraunderstanding,fu2023does,lee2024supervised} perspectives on diverse settings. ~\citet{brown2020language} first showed that GPT-3 can perform in-context learning. 
% Subsequently, numerous studies have relied on diverse settings to investigate the in-context learning ability of transformers. 
~\citet{chen2024transformers} studied the role of different heads within transformers in performing in-context learning focusing on the sparse linear regression setting. ~\citet{frei2024trained} %first 
studied the ability of one-layer linear transformers to perform in-context learning for %random 
linear classification tasks. 
% These works build the foundation of the theoretical analysis for transformer models, which mainly focus on the standard training and test in-context learning behaviors, i.e., the distribution of the context examples are the same in both training and test phases. 


%\CC{which is related to our work. In this work, we maintain the linear classification setting which fits well with our modeling to study the robustness of transformers.}

\textbf{Mechanism interpretability of transformers.}
%To theoretically examine the expressive power of transformers
Among the various theoretical interpretations of transformers~\cite{friedman2024learning,yuntransformers,dehghaniuniversal,lindner2024tracr, pandit2021probing,perez2021attention, bills2023language,wei2022statistically,weiss2021thinking,zhoualgorithms, chen2024can}, one of the most widely studied theories is the ability of transformers to implement optimization algorithms such as gradient descent~\cite{von2023transformers,ahn2023transformers,zhang2024context,bai2024transformers,wumany,chengtransformers,akyureklearning,dai2023can,zhang2024trained}.
%Oswald et al.
~\citet{von2023transformers} theoretically and empirically proved that transformers can learn in-context by implementing a single step of gradient descent per layer. %Ahn et al. ~\cite{ahn2023transformers}\CC{ 
~\citet{ahn2023transformers} theoretically analyzed that transformers can learn to implement preconditioned gradient descent for in-context learning.%Zhang et al 
~\citet{zhang2024context} considered ICL in the setting of linear regression with a non-zero mean Gaussian prior, a more general and common scenario where different tasks share a signal, which is highly relevant to our work.

\textbf{Robustness of transformers.}
The security issues of large language models have always attracted a great deal of attention~\cite{yao2024survey,liu2023prompt,perezignore,zou2023universal,apruzzese2023real}. However, most of the research focuses on jail-breaking black-box models~\cite{chowdhury2024breaking}, such as context-based adversarial attacks~\cite{kumar2023certifying,wei2023jailbreak,xullm,wang2023adversarial,zhu2023promptbench,cheng2024leveraging,wang2023robustness}. There is very little white-box interpretation work of attacks on the transformer, the foundation model of LLMs~\cite{qiang2023hijacking,baileyimage,he2024data,anwar2024adversarial,jiangllms}. 
%Qiang et al. 
~\citet{qiang2023hijacking} first considered attacking large language models during in-context learning, but they did not study the role of transformers in robustness. 
% ~\citet{anwar2024adversarial} studied the adversarial robustness of in-context learning in transformers based on linear regression problems, but they did not theoretically explain the internal mechanism of robustness. 
~\citet{jiangllms} proposed the phenomenon of context hijacking, which became the key motivation of our work. They analyzed this problem from the perspective of associative memory models instead of the in-context learning ability of transformers.%In contrast, our paper formally models the context hijacking phenomenon from the perspective of in-context learning, which has received much attention in transformers and LLMs research, and analyzes the robustness of transformers theoretically and empirically.

\section{Preliminaries}\label{sec:Preliminaries}

In this section, we will provide a detailed introduction to our setup of the context hijacking problem, including the data model,  transformer architecture, and evaluation metric.


\subsection{Data model}
To understand the mechanism of context hijacking phenomenon, we model it as a binary classification task, where the query-answer pair is modeled as the input-response pair ($(\xb, y)\in \RR^d \times \{\pm 1\}$). In particular, we present the definition of the data model as follows:
% Moreover, we consider the setting that the label of each data $\xb$ can be perfectly classified via a linear model, i.e., $y=\mathrm{sign}(\langle \wb^*, \xb\rangle)$, where $\wb^*\in\RR^d$ is the underlying ground truth model parameter. 



% we consider a binary classification task where each input pair $(\xb, y)\in \RR^d \times \{\pm 1\}$. We then construct two data distributions to simulate the training dataset and testing dataset of the large language model respectively, which can imitate the context hijacking on real large language models. In the following, we first provide a formal definition of our training data distribution.

% label $y \in \{\pm 1\} $ and feature vector $ \xb \in \RR^d $. And the random instances of the classification problem are formalized as follows.
\begin{definition}[Data distribution]\label{def:train_data}
Let $\wb^*\in\RR^{d}$ be a vector drawn from a prior distribution on the $d$ dimensional unit sphere $\SSS^{d-1}$, denoted by $p_{\bbeta^*}(\cdot)$, where $\bbeta^*\in\SSS^{d-1}$ denotes the expected direction of $\wb^*$. 
    %Let $\wb^* \in \RR^d$ be a vector drawn from $\cN(\bbeta^*, \bLambda)$ with $\bbeta^\star \neq \mathbf{0}_d$, be a fixed vector. 
    Then given the generated $\wb^*$, the data pair $(\xb,y)$ is generated as follows:  the feature vector is $\xb\sim \cN(\mathbf{0}_d, \Ib_d)$ and the corresponding label is $y=\mathrm{sign}(\la\wb^*,\xb\ra)$.
    
    
    % Then the classification input pairs $\{(\xb_i, y_i)\}_{i=1}^{n+1}\subset \RR^d \times \{\pm 1\}$ are generated as follows: (i). Each feature vector $\xb_i$ is i.i.d. drawn from $\cN(\mathbf{0}_d, \Ib_d)$; (ii). Each label is determined as $y_i = \sign(\la\wb^*, \xb_i\ra)$. \CC{Moreover, the $n+1$-th feature vector $\xb_{\mathrm{query}}$ is considered as the query token, with its label $y_{n+1}$ unobserved.}
    % \begin{enumerate}
    %     \item Each feature vector $\xb^{(i)}$ follows $\cN(\mathbf{0}_d, \Ib_d)$
    %     \item 111
    % \end{enumerate}
    % feature vector $\xb$ follows $\cN(\mathbf{0}_d, \Ib_d)$. Given $\xb^{(i)}$'s and $\wb_\star$, the responses $\yb$ are defined as $\yb =\left[ y^{(1)}, \ldots, y^{(n+1)}\right] = \left[ \sign(\la \wb^*, \xb^{(1)}\ra), \ldots, \sign(\la \wb^*, \xb^{(n+1)}\ra)\right]$.
\end{definition}
% \begin{definition}[Training data distribution $\cD_{\mathrm{tr}}$]\label{def:train_data}
%     Let $\wb^* \in \RR^d$, which is drawn from $\cN(\bbeta^*, \bLambda)$ with $\bbeta^\star \neq \mathbf{0}_d$, be the ground truth vector. Then for each classification input pair $(\xb, y)\in \RR^d \times \{\pm 1\}$, feature vector $\xb$ follows $\cN(\mathbf{0}_d, \Ib_d)$. Given $\xb^{(i)}$'s and $\wb_\star$, the responses $\yb$ are defined as $\yb =\left[ y^{(1)}, \ldots, y^{(n+1)}\right] = \left[ \sign(\la \wb^*, \xb^{(1)}\ra), \ldots, \sign(\la \wb^*, \xb^{(n+1)}\ra)\right]$.
% \end{definition}
Based on the data distribution of each instance, we then introduce the detailed setup of the in-context learning task in our work. In particular, we consider the setting that the transformer is trained on the data with clean context examples and evaluated on the data with hijacked context. 

\textbf{Training phase.} During the training phase, we are given $n$ clean context examples 
$\{(\xb_1,y_1),\dots,(\xb_n,y_n)\}$ and a query $\xb_{\mathrm{query}}$ with its label $y_{\mathrm{query}}$. In particular, here we mean the clean examples as the $\{(\xb_1,y_1),\dots,(\xb_n,y_n)\}$ are drawn from the same data distribution $\cD_{\wb^*}$ as $(\xb_{\mathrm{query}}, y_{\mathrm{query}})$. Then, the input data matrix for in-context learning is designed as follows:
\begin{align}
\Zb &= \begin{bmatrix} \xb_1  & \dots & \xb_n & \xb_{\mathrm{query}} \\ y_1  & \dots & y_n & 0 \end{bmatrix} \in \RR^{(d+1) \times (n+1)}. \label{eq:Z_train}
\end{align}
% Based on the training data distribution $\cD_{\mathrm{tr}}$ defined in Definition~\ref{def:train_data}, we construct the input matrix for in-context learning task, denoted by $\Zb_0$, as follows:
% \begin{flalign}
% \Zb_0 &= \begin{bmatrix} \zb_1& \zb_2 & \ldots & \zb_n & \zb_{n+1} \end{bmatrix} \nonumber \\
% &= \begin{bmatrix} \xb_1 & \xb_2 & \ldots & \xb_n & \xb_{\mathrm{query}} \\ y_1 & y_2 & \ldots & y_n & 0 \end{bmatrix} \in \RR^{(d+1) \times (n+1)}. \label{eq:Z0}
% \end{flalign}
%Here, all pairs $(\xb^{(i)}, y^{(i)})$'s with $i\in [n]$ are i.i.d samples drawn from the distribution $\cD_{\mathrm{tr}}$ \CC{with same w*?}. 
%Here, \CC{the query token} $\xb^{(n+1)}$ follows the same distribution as the feature vector $\xb$ in $\cD_{\mathrm{tr}}$, with its label $y^{(n+1)}$ unknown. 
Here, to ensure that the dimension of $\xb_{\mathrm{query}}$ aligns with those of other input pair $(\xb_{i}, y_{i})$, we concatenate it with $0$ as a placeholder for the unknown label $y_{\mathrm{query}}$. Ideally, we anticipate that given the input $\Zb$, the output of the transformer model, denoted by $\hat y_{\mathrm{query}}$ can match the ground truth one. Moreover, we also emphasize that within each data matrix $\Zb$, the context examples and the queried data should be generated based on the same ground truth vector $\wb^*$, while for different input matrices, e.g., $\Zb$ and $\Zb'$, we allow their corresponding ground truth vectors could be different, which are i.i.d. drawn from the prior $p_{\bbeta^*}(\cdot)$.


% \textbf{Training Data distribution.} Let $\xb \in \RR^d$ be drawn i.i.d from a distribution $\mathcal{N}(\mathbf{0}, \Ib_d)$, and $\wb_{\star} \in \RR^d$ be drawn from $\mathcal{N}(\bbeta^\star, \bbeta_\epsilon)$, where $\bbeta^\star \neq 0$. Then $y$ is defined as $\sign(\langle \xb, \wb_{\star} \rangle)$. Define the input matrix $\Zb_0$ as
% \begin{flalign}
% \Zb_0 &= \begin{bmatrix} \zb^{(1)} & \zb^{(2)} & \ldots & \zb^{(n)} & \zb^{(n+1)} \end{bmatrix} \nonumber \\
% &= \begin{bmatrix} \xb^{(1)} & \xb^{(2)} & \ldots & \xb^{(n)} & \xb^{(n+1)} \\ y^{(1)} & y^{(2)} & \ldots & y^{(n)} & 0 \end{bmatrix} \in \RR^{(d+1) \times (n+1)}. \label{eq:Z0}
% \end{flalign}
% Here, the zero in the matrix is used to replace the ground truth of the feature $\xb^{(n+1)}$. That is, the output of the model is the value at the zero position and it will be compared with the ground truth $y^{(n+1)}$.\\

The training data distribution simulates the pre-training data of the large language model. 
% We reduce the prediction task to a binary classification problem, i.e., we take the si
% The model will learn the ground truth vector $\wb^*$ and make predictions about the context of the input based on it. By taking the symbol of the predicted value, the prediction is judged to be right or wrong. 
Unlike existing works \citep{ahn2023transformers,olsson2022context} where the prior of $\wb^*$ is assumed to have a zero mean, we consider a setting where $\wb^*$ has a non-zero mean (i.e., $\bbeta^*$). This approach is inspired by empirical observations (see Figure \ref{fig:realworldexpr}) that transformer models can perform accurate zero-shot predictions. Consequently, our model can encapsulate both memorization and in-context learning, where the former corresponds to recovering the mean of the prior distribution, i.e., $\bbeta^*$, and the latter aims to manipulate the $\{(\xb_1,y_1), \dots, (\xb_n,y_n)\}$ effectively. In contrast, existing works primarily focus on the latter, thereby failing to fully explain the interplay between memorization and in-context learning.



% In the reduced task, we hope that the model can have the ability of memory and in-context learning through pre-training. The former means that the model can learn $\bbeta^*$, the mean of weight $\wb$, corresponding to the general generalization ability of the large language model after pre-training. The latter means that the model can take advantage of context $(\xb_1,y_1),... ,(\xb_n,y_n)$ to fine-tune $\bbeta^*$ to $\wb^*$, which is aligned with the ability of large language models to use contextual information for predicting in practical applications.\\

\textbf{Test phase.} During the test phase, context examples are designed based on the query input $\xb_{\mathrm{query}}$ to effectively execute the attack. Inspired by empirical observations (Figure \ref{fig:realworldexpr}) and prior experience with jailbreaking attacks \citep{anil2024many}, we choose to use repeated hijacking context examples during the test phase. Specifically, since the hijacked context should be factually correct, we consider data similar to the queried input but with a correct and opposite label of low confidence. Mathematically, this involves projecting $\xb_{\mathrm{query}}$ onto the classification boundary. To this end, given the target query data $(\xb_{\mathrm{query}}, y_{\mathrm{query}})$, we formalize the design of the hijacked context example as follows.
\begin{definition}[Hijacked context data]\label{def:hijackedcontext} Let $(\xb, y)$ be a input pair and $\wb^*$ be the corresponding ground truth vector. Additionally, denote $\xb_\perp$ as the projection of  $\xb$ on the boundary of classifier, i.e. $\xb_\perp = (\Ib_d - \wb^*(\wb^*)^\top)\cdot\xb$. 
Then, the query pair $(\xb_{\mathrm{query}}, y_{\mathrm{query}})$ is generated as $\xb_{\mathrm{query}} = \xb_\perp + \sigma \wb^*$ and $y_{\mathrm{query}} =\sign\big(\la\wb^*, \xb_{\mathrm{query}}\ra\big)=\sign(\sigma)$ with $\sigma$ being a random variable, and 
the hijacked context example is designed as $\xb_{\mathrm{hc}}=\xb_\perp$ and $y_{\mathrm{hc}}=-y_{\mathrm{query}}$. 
\end{definition}
% \begin{definition}[Hijacked context data]\label{def:hijackedcontext} Let $(\xb_{\mathrm{query}}, y_{\mathrm{query}})$ be the query data and $\wb^*$ be the corresponding ground truth vector. Then, the hijacked context example is designed as $\xb_{\mathrm{hc}}=(\Ib_d - \wb^*(\wb^*)^\top)\cdot\xb_{\mathrm{query}}$ and $y_{\mathrm{hc}}=-y_{\mathrm{query}}$. 
% \end{definition}
Note that we pick $\la\xb_{\mathrm{hc}},\wb^*\ra=0$ to enforce hijacked context lies on the boundary of the classifier. A more rigorous design is to set $\xb_{\mathrm{hc}} = \xb_\perp - \eta\cdot y_{\mathrm{query}}\cdot \wb^*$ for some positive quantity $\eta$, where it can be clearly shown that $y_{\mathrm{hc}}=\text{sign}(\la\xb_{\mathrm{hc}},\wb^*\ra)=-y_{\mathrm{query}}$. Definition \ref{def:hijackedcontext} concerns the limiting regime by enforcing $\eta\rightarrow 0^+$.

Then, based on the above design, the input data matrix in the test phase is constructed as follows:
\begin{align}
\Zb^{\mathrm{hc}} = \begin{bmatrix} \xb_{\mathrm{hc}} &  \dots & \xb_{\mathrm{hc}} & \xb_{\mathrm{query}} \\ y_{\mathrm{hc}}  & \dots & y_{\mathrm{hc}}& 0 \end{bmatrix} \in \RR^{(d+1) \times (N+1)}.\label{eq:Z_test}
\end{align}
Here we use $N$ to denote the number of hijacked context examples. The example $(\xb_{\mathrm{hc}}, y_{\mathrm{hc}})$ can also be interpreted to the closest data to $\xb_{\mathrm{query}}$ but with a different label $-y_{\mathrm{query}}$, which principally has the ability to perturb the prediction of $\xb_{\mathrm{query}}$. Additionally, because the prediction is highly likely to be correct in the zero-shot regime (i.e., $N=0$ ), the prediction in the test phase can be viewed as a competition between model memorization and adversarial in-context learning. This dynamic is primarily influenced by the number of hijacked context examples.


% the transformer has the memorization ability, 




% \begin{definition}[Testing data distribution $\cD_{\mathrm{te}}$]\label{def:test_data}
%     Let $\wb^* \in \RR^d$ be the same ground-truth vector as generated in $\cD_{\mathrm{tr}}$ as defined in Definition~\ref{def:train_data}. Let $\xb \in \RR^d$ be a Gaussian vector generated from  $\mathcal{N}(\mathbf{0}_d, \Ib_d)$, and $\xb_{\perp}$ be the projection of $\xb$ onto the complement subspace of $\wb_\star$, i.e. $\xb_{\perp} = \big(\Ib_d-\wb^*(\wb^*)^\top\big)\cdot \xb$. Then the context example is generated as $\xb_{\mathrm{hc}} = \xb_{\perp} + \sigma_2 \wb^*$  with label $y_{\mathrm{hc}} = \sign(\la \wb^*, \xb_{\mathrm{hc}}\ra) = \sign(\sigma_2)$. The query example is generated as 
%     $\xb_{\mathrm{query}} = \xb_{\perp} + \sigma_1 \wb^*$ with label $y_t = \sign(\la \wb^*, \xb_{\mathrm{query}}\ra)=\sign(\sigma_1)$. Here $\sigma_1$ and $\sigma_2$ are two random variables with opposite signs.
% \end{definition}

%\textbf{Testing Data distribution.} Let $\xb \in \RR^d$ be drawn i.i.d from a distribution $\mathcal{N}(\mathbf{0}_d, \Ib_d)$, and $\wb_{\star} \in \RR^d$ be drawn from $\mathcal{N}(\bbeta^\star, \bbeta_\epsilon)$, where $\bbeta^\star \neq 0$. The projection of $\xb$ onto the orthogonal vector of $\wb_\star$ is $\xb_{\perp \wb}$. Define $ \xb_{\mathrm{query}} = \xb_{\perp \wb} - \bsigma_1 \frac{\wb}{\|\wb\|}$ and $ \xb_{\mathrm{hc}} = \xb_{\perp \wb} - \bsigma_2 \frac{\wb}{\|\wb\|}$ where $\bsigma_1$ and $\bsigma_2$ have different signs. Then $y_t = \sign(\la \wb^*, \xb_{\mathrm{query}}\ra)$ and $y_{\mathrm{hc}} = \sign(\la \wb^*, \xb_{\mathrm{hc}}\ra)$. So $\Zb_0$ is
% Based on $\cD_{\mathrm{te}}$, we construct the input matrix $\Zb_0$ as
% \begin{flalign}
% \Zb_0 &= \begin{bmatrix} \zb_{sim} & \zb_{sim} & \ldots & \zb_{sim} & \zb_t \end{bmatrix} \nonumber \\
% &= \begin{bmatrix} \xb_{\mathrm{hc}} & \xb_{\mathrm{hc}} & \ldots & \xb_{\mathrm{hc}} & \xb_{\mathrm{query}} \\ y_{\mathrm{hc}} & y_{\mathrm{hc}} & \ldots & y_{\mathrm{hc}} & 0 \end{bmatrix} \in \RR^{(d+1) \times (n+1)}.
% \end{flalign}
% The test data distribution simulates a context hijacking scenario. Projecting $\xb$ onto the boundary line, i.e. the orthogonal vector of $\wb_*$, and extending it slightly to both sides, we get $(\xb_{\mathrm{hc}},\yb_{sim})$ and $(\xb_{\mathrm{query}},\yb_t)$. They have the property that they are close to the boundary line of the classification but have opposite label. This is consistent with the data when context hijacking occurs, that is, for a prediction problem, first prompting a large number of similar, logically correct, and opposite predicted examples will affect the predicted next token of the target problem. We then train and test linear transformers based on these two distributions.

\subsection{Transformer model}  Following the extensive prior theoretical works for transformer \citep{zhang2024trained, zhang2024context, chen2024transformers, frei2024trained, ahn2023transformers},  we consider linear attention-only transformers, a prevalent simplified structure to investigate the behavior of transformer models. In particular, We define an $L$-layer linear transformer $\mathtt{TF}$ as a stack of $L$ single-head linear attention-only layers. For the input matrix $\Zb_{i-1}\in \RR^{(d+1) \times (n+1)}$, the $i$-th single-head linear attention-only layer $\mathtt{TF}_i$ updates the input as follows:
\begin{flalign}\label{eq:Zi}
\Zb_{i}= \mathtt{TF}_i(\Zb_{i-1})=\Zb_{i-1}+\Pb_i\Zb_i \Mb(\Zb_{i-1}^{\top}\Qb_i\Zb_{i-1}),
\end{flalign}
where $\Mb:= \begin{pmatrix} \Ib_{n} & 0 \\ 0 & 0 \end{pmatrix} \in \RR^{(n+1) \times (n+1)}$ is the mask matrix. We design this architecture to constrain the model's focus to the first $n$ in-context examples. Moreover, the matrix $\Pb: = \Wb_{v} \in \RR^{(d+1) \times (d+1)}$ serves as the value matrix in the standard self-attention layer, while the matrix $\Qb := \Wb_{k}^{\top} \Wb_{q} \in \RR^{(d+1) \times (d+1)}$ consolidates the key matrix and query matrix. This mild re-parameterization has been widely considered in numerous recent theoretical works \citep{huangcontext, wangtransformers, tian2023scan, jelassi2022vision}. To adapt the transformer for solving the linear classification problem, we introduce an additional linear embedding layer $ \Wb_E \in \mathbb{R}^{(d+1) \times (d+1)}$. Then the output of the transformer $\mathtt{TF}$ is defined as
\begin{flalign}\label{def:ybar}
\widehat{y}_{\mathrm{query}}&=\mathtt{TF}(\Zb_0;\Wb_E, \{\Pb_\ell, \Qb_\ell\}_{\ell=1}^L) \nonumber\\
&=-[\mathtt{TF}_{L}\circ \cdots \circ \mathtt{TF}_{1} \circ \Wb_E(\Zb_0)]_{(d+1),(n+1)} \nonumber\\
&=-[\Zb_L]_{(d+1),(n+1)},
\end{flalign}
i.e. the negative of the $(d+1,n+1)$-th entry of $\Zb_L$, and this position is replaced by $0$ in the input $\Zb_0$. The reason for taking the minus sign here is to align with previous work~\cite{von2023transformers,shen2024anything}, which will be explained in Proposition \ref{def: prop5.1}.

% Moreover, the matrices $\Pb$, $\Qb$ are re-parameterization of the value matrices $\Wb_v$, key matrices $\Wb_k$ and query matrices $\Wb_q$ in the 
% . In the standard self-attention layer setting, we have value matrices $\Wb_v$, key matrices $\Wb_k$ and query matrices $\Wb_q$. In the simplified structure, we reparameterize weights as $\Pb := \Wb_{v} \in \RR^{(d+1) \times (d+1)}$ and $\Qb := \Wb_{k}^{\top} \Wb_{q} \in \RR^{(d+1) \times (d+1)}$. Besides, we use the mask matrix $\Mb$ to constrain the model focus the first $n$ in-context examples.
%
% \begin{flalign}
% \Zb_{i}&= \Zb_{i-1}+\mathtt{TF}_i(\Zb_{i-1})=\Zb_{i-1}+\Pb_i\Zb_i \Mb(\Zb_{i-1}^{\top}\Qb_i\Zb_{i-1})\\
% \Mb &:= \begin{pmatrix} \Ib_{n} & 0 \\ 0 & 0 \end{pmatrix} \in \RR^{(n+1) \times (n+1)},
% \end{flalign}
% where $\Pb \in \RR^{(d+1) \times (d+1)} \quad \text{and} \quad \Qb \in \RR^{(d+1) \times (d+1)}$. In the standard self-attention layer setting, we have value matrices $\Wb_v$, key matrices $\Wb_k$ and query matrices $\Wb_q$. In the simplified structure, we reparameterize weights as $\Pb := \Wb_{v} \in \RR^{(d+1) \times (d+1)}$ and $\Qb := \Wb_{k}^{\top} \Wb_{q} \in \RR^{(d+1) \times (d+1)}$. Besides, we use the mask matrix $\Mb$ to constrain the model focus the first $n$ in-context examples.

\subsection{Evaluation metrics}
Based on the illustration regarding the transformer architecture, we first define the in-context learning risk of a $L$-layer model $\mathtt{TF}$ in the training phase. In particular, let $\cD_{\mathrm{tr}}$ be the distribution of the input data matrix $\Zb$ in \eqref{eq:Z_train} and the target $y_{\mathrm{query}}$, which covers the randomness of both $(\xb,y)$ and $\wb^*$, then the risk function in the training phase is defined as:
\begin{flalign}\label{eq:loss_tf}
\cR\left(\mathtt{TF}\right) := \mathbb{E}_{\Zb,y_{\mathrm{query}}\sim \cD_{\mathrm{tr}}} \big[\left( \mathtt{TF}(\Zb;\btheta) - y_{\mathrm{query}} \right)^2\big].
\end{flalign}
where $\btheta=\{\Wb_E, \{\Pb_\ell, \Qb_\ell\}_{\ell=1}^L\}$ denotes the collection of all trainable parameters of $\mathtt{TF}$. This risk function will be 
leveraged for training the transformer models (where we use the stochastic gradient in the experiments).

% \begin{flalign}\label{eq:loss_tf}
% \cR\left(\mathtt{TF}\right) := \mathbb{E}_{\{(\xb_i, y_i)\}_{i=1}^{n+1}, \wb^* \sim\cD_{\mathrm{tr}}} [\left( \widehat{y}_{n+1} - y_{\mathrm{query}} \right)^2].
% \end{flalign}
Additionally, in the test phase, let $\cD_{\mathrm{te}}$ be the distribution of the input data matrix $\Zb^{\mathrm{hc}}$ in \eqref{eq:Z_test} and the target $y_{\mathrm{query}}$, we consider the following population prediction error:
\begin{flalign}\label{eq:error_tf}
\cE\left(\mathtt{TF}\right) := \mathbb{P}_{\Zb^{\mathrm{hc}},y_{\mathrm{query}}\sim \cD_{\mathrm{te}}} \big[ \mathtt{TF}(\Zb^{\mathrm{hc}};\btheta)\cdot y_{\mathrm{query}} <0\big].
\end{flalign}

% \textbf{Training algorithm.}
% We consider utilize the gradient descent to minimize the empirical least-square loss. Specifically, the iterative rule for parameter vector $\wb_*^{(t)}$ is
% \begin{align}\label{eq:gd_update}
%     \wb^{(t+1)} = \wb^{(t)} - \alpha_t \nabla L(\wb^{(t)}), 
% \end{align}
% where $\alpha_{t}$ is the learning rate at $t$-th iteration.

%\section{\CC{Describe the task?}}
%construction, why linear classification instead of regression

\section{Main theory}\label{sec:theory}
In this section, we present how we establish our theoretical analysis framework regarding the robustness of transformers against context hijacking. In summary,  we can briefly sketch our framework into the following several steps: 
\begin{itemize}[leftmargin=*,nosep]
    \item Step 1. We establish the equivalence between the $L$-layer transformers and $L$ steps gradient descent, converting the original problem of identifying well-trained transformers to the problem of finding the optimal parameters of gradient descent (i.e., initialization and learning rates).
    \item Step 2. We derive the optimal learning rates and initialization of gradient descent, revealing its relationship with the number of layers $L$ and training context length $n$. 
    \item Step 3. By formulating the classification error of a linear model obtained by $L$ steps gradient descent with optimal parameters on hijacking distribution $\cD_{\mathrm{te}}$, we characterize how the number of layers $L$, the training context length $n$ and test context length $N$ affect the robustness.
\end{itemize}
% Next, we provide more details of the above three steps.
% Step 1. We establish the equivalence between the $L$-layer transformers and $L$ steps gradient descent, converting the original problem of identifying well-trained transformers to the problem of finding the optimal parameters of gradient descent.
% Step 2. We successfully derive the optimal learning rates and initialization of gradient as the number of layers $L$ and training context length $n$ vary. 
% Step 3. By formulating the classification error of a linear model obtained by $L$ steps gradient descent with optimal parameters on hijacking distribution $\cD_{\mathrm{te}}$, we derive our conclusions how the number of layers $L$, the training context length $n$ and number of hijacking examples $N$ affect the robustness of transformers.
% successfully formulate the 
%This suggests that we can obtain the prediction of a well-trained $L$-layer transformers by conducting the 
%  $L$ steps gradient descent with the derived optimal parameters. Step 3. By checking the classification error of a linear model obtained by $L$ steps gradient descent with optimal parameters on hijacking distribution $\cD_{\mathrm{te}}$, we successfully formulate the 

% More detailed results are demonstrated in the 


% with , This suggest that Step. 
% \CC{We first prove the connection between transformers and gradient descent steps, which is the starting point of our theory. Then we show our main theory that the trained transformers will learn the following optimal multi-step gradient descent strategy: for the same optimization objective, the optimal gradient descent with more steps will have a smaller learning rate per step than the optimal gradient descent with fewer steps, and this combination of more steps and small learning rates will perform better in the optimization process over context samples than the combination of fewer steps and large learning rates. When applying it to the context hijacking task, we found that our theoretical framework can well explain this phenomenon, that is, deeper transformers can perform better optimization over context samples by taking multiple steps with smaller learning rates. This provides a new perspective for the explanation of the robustness of TF and LLM.} 
\subsection{Optimizing over in-context examples}
%A line of recent work has studied the connection between transformers and gradient descent steps\CC{cite}, which is the starting point of our theory.
% As we attempt to investigate the robustness of transformers, our first task is to formulate the predictions from well-trained transformers for general inputs, including the hijacking inputs from $\cD_{\mathrm{te}}$. However, directly identifying the parameters of multiple-layer transformers by considering the entire optimization process from scratch is inherently complex and challenging due to the intricate
% structure of multi-layer transformers. Fortunately, we overcome this challenge by 
% showing that multi-layer transformers can implement one-step gradient descent on each layer, 
Inspired by a line of recent works \citep{zhang2024context, bai2024transformers,chen2024transformers, ahn2023transformers, olsson2022context} which connects the in-context learning of transformer with the gradient descent algorithm, we follow a similar approach by showing that, in the following proposition, multi-layer transformer can implement multi-step gradient descent, starting from any initialization, on the context examples.  
% constructing the equivalence between the output of $L$-layer transformers and that of a linear model trained by gradient descent on the context examples with $L$ steps, 
% inspired by several previous works 


% we have to .This implies that 
% the first c is to formulate a. However, considering the entire optimization process over the 



% Our theoretical basis is that in the case of shared signals, that is, the weights $\wb^*$ are sampled from a Gaussian distribution with a non-zero mean $\bbeta^* \neq 0$, multi-layer transformers will implement iterative optimization algorithms on the in-context tokens. More specifically, multi-layer transformers can implement multi-step gradient descent starting from any non-zero initialization. In our case, trained transformers will effectively learn and memorize the shared signal $\bbeta^*$, which is different from the weight $\wb^*$ in each specific instance, controlled by the variance $\bLambda$. Next, the model will iteratively optimize the shared signal $\bbeta^*$ by in-context learning in different specific instance. Each layer of transformer will implement a one-step gradient descent step, and the model will gradually fine-tune $\bbeta^*$ to $\wb^*$ through multiple iterations, thereby successfully classifying the query token.




% \textcolor{red}{I guess the following one only mention the general initial point rather than $\bbeta^*$?}
% \begin{proposition} [Transformers implement multi-step gradient descent with any initialization]\label{def: prop5.1}
\begin{proposition}\label{def: prop5.1}
%Consider a linear binary classification task where the sample consists of $(\xb^{(i)}, y^{(i)})$, where $y_i = \sign(\la\wb^*, \xb_i\ra)$.
For any $L$-layer single-head linear transformer, let 
$\widehat{y}_{\mathrm{query}}^{(l)}$ be the output of the $l$-th layer of the transformer, i.e. the $(d+1,n+1)$-th entry of $\Zb_l$. Then, there exists a single-head linear transformer with $L$ layers such that $\widehat{y}_{\mathrm{query}}^{(l)} = -\la \wb_{\mathrm{gd}}^{(l)}, {\xb}_{\mathrm{query}} \ra$. Here, $\wb_{\mathrm{gd}}^{(l)}$'s are the parameter vectors obtained by the following gradient descent iterative rule and the initialization $\wb_{\mathrm{gd}}^{(0)}$ can be arbitrary:
% as $\wb_{0}^{\text{gd}} = \bbeta^*$, a non-zero value, and as follows for $\ell = 0, \ldots, L-1$:
\begin{flalign}\label{eq:gd_update}
&\wb_{\mathrm{gd}}^{(l+1)} = \wb_{\mathrm{gd}}^{(l)} - \bGamma_{l} \nabla \widetilde{L}(\wb_{\mathrm{gd}}^{(l)}), \quad \nonumber\\
&\text{where} \quad \widetilde{L}(\wb) = \frac{1}{2} \sum_{i=1}^{n} (\la\wb, \xb_i\ra - y_i)^2.
\end{flalign}
Here $\bGamma_{l}$ can be any $d\times d$ matrix.
\end{proposition}
% Proposition \ref{def: prop5.1} shows that multi-layer transformers can implement iterative gradient descent starting from $\bbeta^\star$ with learning rates $\{\bGamma_{\ell}\}_{\ell=1}^L$ on linear classification 

As $\bGamma_{l}$ could be any $d\times d$ matrix, 
Proposition~\ref{def: prop5.1} demonstrates that the output of the $L$-layer transformers is equivalent to that of a linear model trained via $L$-steps of full-batch preconditioned gradient descent on the context examples, with $\{\bGamma_{l}\}_{l=0}^{L-1}$ being the learning rates. 
% Additionally, provided the same context examples $\{(\xb_i, y_i)\}_{i=1}^n$, the output of the linear model via $L$-steps gradient descent~\eqref{eq:gd_update} is entirely determined by the specific choices of initialization $\wb_{\mathrm{gd}}^{(0)}$ and learning rates  $\{\bGamma_{l}\}_{l=0}^{L-1}$. 
This suggests that each $L$-layer transformer defined in~\eqref{def:ybar}, with different parameters, can be viewed as an optimization process of a linear model characterized by a distinct set of initialization and learning rates $\{\wb_{\mathrm{gd}}^{(0)}, \bGamma_{0}, \ldots, \bGamma_{L-1}\}$. 
Therefore, it suffice to directly find the optimal parameters of the gradient descent process, without needing to infer the specific parameters of the well-trained transformers.
% Based on this observation, we can clearly determine the prediction of well-trained $L$-layer transformers by identifying the optimal output of the linear model trained by gradient descent with $L$ steps, without needing to infer the specific parameters of the well-trained $L$-layer transformers. Therefore, it suffices to find the optimal parameters of the gradient descent process, which will be expanded in the next section. 

% Therefore, we can obtain the oalthough we know nothing about the specific parameters
% This suggests that each $L$-layer transformer defined in~\eqref{def:ybar}, with different parameters, can be viewed as an optimization process of a linear model characterized by a distinct set of initialization and learning rates $\{\wb_{\mathrm{gd}}^{(0)}, \alpha_0, \ldots, \alpha_{L-1}\}$

% Such observations allow us to manually adjust the initialization and learning rates in the optimization process~\eqref{eq:gd_update} to identify pre-trained transformers with optimal parameters, i.e. the transformers can achieve the minimal population loss $\cR(\mathtt{TF})$ on training distribution $\cD_{\mathrm{tr}}$. Although we can not obtain the specific parameters of the pre-trained transformers, by finding the optimal learning rate and initialization of the gradient descent process~\eqref{eq:gd_update}, we can still definitely determine the optimal output of the transformers. 
% This approach is preferable to directly investigating the training process of transformers to find the optimal solution, which is inherently complex and challenging due to the intricate structure of multi-layer transformers. 
% The proof of Proposition \ref{def: prop5.1}
% can be found in Appendix \ref{appendix:prop5.1}. Proposition \ref{def: prop5.1} shows that multi-layer transformers can implement iterative gradient descent starting from $\bbeta^\star$ with learning rates $\{\bGamma_{\ell}\}_{\ell=1}^L$ on linear classification tasks, since $y^{(i)} = \sign(\la\wb^*, \xb^{(i)}\ra)$. Note that the output of each layer in this proposition is $-\la \wb_{\ell}^{\text{gd}}, {\xb}^{n+1} \ra$, so we get $\la \wb_{\ell}^{\text{gd}}, {\xb}^{n+1} \ra$ by adding a minus sign in \ref{def:ybar}. In the next subsection, we provide a further discussion regarding how we utilize Proposition ~\ref{def: prop5.1} to derive the pre-trained transformers, and present the mathematical results. 
Among all related works presenting similar conclusions that transformers can implement gradient descent, our result is general as we prove that transformers can implement multi-step gradient descent from any initialization. In comparison, for example, \citet{zhang2024context} shows that a single-layer transformer with MLP can implement one-step gradient descent from non-zero initialization. \citet{ahn2023transformers} demonstrate that linear transformers can implement gradient descent, but only from $\mathbf{0}$ initialization. 
% Our work is motivated by previous work\CC{cite}, indicating that transformers can implement gradient descent algorithm, but there are still differences. Ahn et al. \CC{cite} proved that linear transformers can implement iterative gradient descent from zero initialization. Zhang et al. \CC{cite} showed that transformer with multi-layer perceptron (MLP) can achieve gradient descent from non-zero initialization, but only considered the single-step case. We prove that transformers can implement multi-step gradient descent from any non-zero initialization.

\subsection{Optimal multi-step gradient descent}

Based on the discussion in the previous section, Proposition~\ref{def: prop5.1} successfully transforms the original problem of identifying the parameters of well-trained transformers into the task of finding the optimal learning rates and initialization for the gradient descent process~\eqref{eq:gd_update}. In this section, we present our conclusions regarding these optimal parameters. As we consider optimizing over the general training distribution $\cD_{\mathrm{tr}}$, where the tokens $\xb_i$'s follow the isotropic distribution, it follows that the updating step size should be equal in each direction from the perspective of expectation. Therefore we consider the case $\bGamma_l=\alpha_l\Ib_d$ to simplify the problem, with $\alpha_l$ being a scalar for all $l\in \{0, \ldots, L-1\}$. In the following, we focus on the optimal set of parameters $\{\wb_{\mathrm{gd}}^{(0)}, \alpha_0, \ldots, \alpha_{L-1}\}$. Specifically, we consider the population loss for $\wb_{\mathrm{gd}}^{(L)}$ as
\begin{align*}
\cR(\wb_{\mathrm{gd}}^{(L)}):= \mathbb{E}_{T, \wb^* \sim\cD_{\mathrm{tr}}} \big[\big( \la\wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra - y_{\mathrm{query}}\big)^2\big],
\end{align*}
where $T =\{(\xb_1, y_1),\ldots, (\xb_n, y_n), (\xb_{\mathrm{query}}, y_{\mathrm{query}})\}$ is the set of all classification pairs \footnote{Here we slightly abuse the notation of $\cD_{\mathrm{tr}}$ to denote both the distribution of $\Zb, \wb_{\mathrm{query}}$ and $T, \wb^*$.}. This definition resembles $\cR(\mathtt{TF})$ defined in~\eqref{eq:loss_tf}. We attempt to find the $\wb_{\mathrm{gd}}^{(L)}$ that minimizes this population loss, along with the corresponding learning rates $\{\alpha_l\}_{l=0}^L$ and initialization $\wb_{\mathrm{gd}}^{(0)}$, which can generate this $\wb_{}$ via gradient descent. 
We first present the following proposition demonstrating that there exists commutative invariance among the learning rates $\{\alpha_l\}_{l=0}^L$ for producing $\wb_{\mathrm{gd}}^{(L)}$.
% Proposition~\ref{def: prop5.1} demonstrates that the output of the $L$-layer transformers is equivalent to that of a linear model trained via $L$-steps of full-batch gradient descent on the context examples. Additionally, provided the same context examples $\{(\xb_i, y_i)\}_{i=1}^n$, the output of the linear model via $L$-steps gradient descent~\eqref{eq:gd_update} is entirely determined by the specific choices of initialization $\wb_{\mathrm{gd}}^{(0)}$ and learning rates $\{\alpha_{l}\}_{l=0}^{L-1}$. This implies that each $L$-layer transformer defined in~\eqref{def:ybar}, with different parameters, can be viewed as an optimization process of a linear model characterized by a distinct set of initialization and learning rates $\{\wb_{\mathrm{gd}}^{(0)}, \alpha_0, \ldots, \alpha_{L-1}\}$. Such observations allow us to manually adjust the initialization and learning rates in the optimization process~\eqref{eq:gd_update} to identify pre-trained transformers with optimal parameters, i.e. the transformers can achieve the minimal population loss $\cR(\mathtt{TF})$ on training distribution $\cD_{\mathrm{tr}}$. Although we can not obtain the specific parameters of the pre-trained transformers, by finding the optimal learning rate and initialization of the gradient descent process~\eqref{eq:gd_update}, we can still definitely determine the optimal output of the transformers. 
% This approach is preferable to directly investigating the training process of transformers to find the optimal solution, which is inherently complex and challenging due to the intricate structure of multi-layer transformers. 
% In the next, we present our theoretical results for identifying the optimal initialization $\wb_{\mathrm{gd}}^{(0)}$ and learning rates $\{\alpha_l\}_{l=0}^L$ which will produce the optimal $\wb_{\mathrm{gd}}^{(L)}$ achieving the minimal population loss, which is defined as 
% \begin{align*}
% \cR(\wb_{\mathrm{gd}}^{L}):= \mathbb{E}_{\{(\xb_i, y_i)\}_{i=1}^{n+1}, \wb^* \sim\cD_{\mathrm{tr}}} [( \la\wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra - y_{\mathrm{query}})^2].
% \end{align*}
% which is identical to $\cR(\mathtt{TF})$ defined in~\eqref{eq:loss_tf}.
% We first present the following proposition demonstrating that there exists commutative invariance among the learning rates $\{\alpha_l\}_{l=0}^L$ for producing $\wb_{\mathrm{gd}}^{(L)}$.

\begin{proposition}\label{prop:commutative invariance}
Let $\{\alpha_0, \alpha_1, \ldots, \alpha_{L-1}\}$ be a set of learning rates, and $\{\alpha_0', \alpha_1', \ldots, \alpha_{L-1}'\}$ be another set of learning rates that is a permutation of $\{\alpha_0, \alpha_1, \ldots, \alpha_{L-1}\}$, meaning both sets contain the same elements, with the only difference being the order of these elements. With $\wb_{\mathrm{gd}}^{(L)}\in \RR^{d}$ denoting as the parameters achieved by learning rates $\{\alpha_0, \alpha_1, \ldots, \alpha_{L-1}\}$ and $\wb_{\mathrm{gd'}}^{(L)}\in \RR^{d}$ as the parameters achieved by learning rates $\{\alpha_0', \alpha_1', \ldots, \alpha_{L-1}'\}$ from the same initialization $\wb_{\mathrm{gd}}^{(0)}$, it holds that $\wb_{\mathrm{gd}}^{(L)} = \wb_{\mathrm{gd'}}^{(L)}$.
% $\wb_{\mathrm{gd}}^{(0)}\in \RR^{d}$ being any initialization parameter vector, let 
\end{proposition}

% As discussed previously, to obtain the optimal pre-trained transformers for the training distribution $\cD_{\mathrm{tr}}$, it is sufficient to find the optimal learning rates $\{\alpha_l\}_{l=0}^L$ and initialization $\wb_{\mathrm{gd}}^{(0)}$ of the gradient descent process~\eqref{eq:gd_update}. While 
Proposition~\ref{prop:commutative invariance} implies that the learning rates at different steps contribute equally to the overall optimization process. Consequently, we will consider a consistent learning rate $\alpha$ through the entire gradient descent procedure, which significantly reduces the difficulty of analysis and does not incur any loss of generality. Now we are ready to present our main results regarding the derivation of the optimal parameters $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$.

% \textcolor{red}{the following theorem should be focusing on the optimal solution, rather than the bound on the loss function. Currently, I do not see any words in the following theorem mentioning that what's optimal beta and alpha.}
\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{gd_N10_50png.png}
    \vskip -0.in
    \caption{Gradient descent experiments using a single-layer neural network. We use grid search to obtain the optimal learning rate for different training context lengths $n$ and different steps of gradient descent $L$. Then we use the corresponding optimal learning rate to perform multi-step gradient descent optimization on the test dataset. The results show that longer training context lengths and more gradient descent steps lead to smaller optimal learning rate and better optimization.}
    \label{fig:gd_gridsearch}
    \vskip -0.1in
\end{figure*}

\begin{theorem}\label{thm:training_loss_bound}
    For training distribution $\cD_{\mathrm{tr}}$ in Definition~\ref{def:train_data}, suppose that the training context length $n$ is sufficiently large such that $n\geq \tilde\Omega(\max\{d^2, dL\})$. Additionally, suppose that the perturbation of $\wb^*$ around its expectation $\bbeta^*$ is smaller than $\frac{\pi}{2}$, i.e. $\la\wb^*, \bbeta^*\ra>0$. Based on these assumptions, the optimal learning rate $\alpha$ and initialization $\wb_{\mathrm{gd}}^{(0)}$, i.e. $\alpha, \wb_{\mathrm{gd}}^{(0)} =\arg\min_{\alpha, \wb_{\mathrm{gd}}^{(0)}}\cR(\wb_{\mathrm{gd}}^{(L)})$, take the value as follows:
    \begin{align*}
        \alpha &= \tilde\Theta\bigg(\frac{1}{nL}\bigg); \qquad \wb_{\mathrm{gd}}^{(0)} = c\bbeta^*,
    \end{align*}
    where $c$ is an absolute constant.
    % Additionally, for choices of $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$ defined above, $\cR(\wb_{\mathrm{gd}}^{(L)})$ achieves its minimal upper-bound as
    % \begin{flalign*}
    %     \cR(\wb_{\mathrm{gd}}^{(L)}) \leq &\ \tilde\Theta\Big(\frac{d}{n}\Big) + C.
    % \end{flalign*}
    % Here, both $c$ and $C$ are absolute constants.
\end{theorem}
% \begin{theorem}\label{thm:training_loss_bound}
%     For training distribution $\cD_{\mathrm{tr}}$ in Definition~\ref{def:train_data}, suppose that the training context length $n$ is sufficiently large such that $n\geq \tilde\Omega(\max\{d^2, dL\})$. Additionally, suppose that the perturbation of $\wb^*$ around its expectation $\bbeta^*$ is smaller than $\frac{\pi}{2}$, i.e. $\la\wb^*, \bbeta^*\ra>0$. then for any learning rate $\alpha$ and initialization $\wb_{\mathrm{gd}}^{(0)}$, it holds that 
%     \begin{flalign*}
%         \cR(\wb_{\mathrm{gd}}^{(L)}) \leq &\ \Theta\big((1-\alpha n)^L\|\wb_{\mathrm{gd}}^{(0)}-c_1\wb^*\|_2^2 \big) \\
%         &+  \tilde\Theta(\alpha d L) + C,
%     \end{flalign*}
%     where both $c_1, C$ are absolute constants. Additionally, by taking $\wb_{\mathrm{gd}}^{(0)} = c_1 \bbeta^*$ and $\alpha =\tilde\Theta(\frac{1}{nL})$, the upper bound above achieve its optimal rates as
%     \begin{flalign*}
%         \cR(\wb_{\mathrm{gd}}^{(L)}) \leq &\ \tilde\Theta\Big(\frac{d}{n}\Big) + C.
%     \end{flalign*}
% \end{theorem}
Theorem~\ref{thm:training_loss_bound} clearly identifies the optimal learning rate $\alpha$ and initialization $\wb_{\mathrm{gd}}^{(0)}$. Specifically, it shows that the optimal initialization $\wb_{\mathrm{gd}}^{(0)}$ aligns the direction of the expectation $\bbeta^*$, with its length independent of the number of steps $L$, and the context length $n$. Such a conclusion complies with our intuitions as the initialization $\wb_{\mathrm{gd}}^{(0)}$ represents the memory of large language models, which is not dependent on the task-specific context examples. In contrast, the optimal learning rate $\alpha$ is inversely related to both $n$ and $L$. This suggests that in both cases: (i) with more in-context examples; and (ii) with more layers, the output of pre-trained transformers will equal to that of a more fine-grained gradient descent process using a smaller learning rate. Generally, a small-step strategy ensures the convergence of the objective, highlighting the potential benefits of deeper architectures and training inputs with longer context. 
% Overall, the clear characterization of the optimal $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$ as $n, L$ vary enables us to provide specific analysis towards their robustness against the context-hijacking.
%, which will be further extended in the next section. 

% Besides, Theorem~\ref{thm:training_loss_bound} also provides the upper bound for the population loss achieved when choosing the optimal learning rate and initialization. Here, the constant $C$ represents an irreducible term arising from the variance of the model. Such an irreducible term always exists when considering least-squares loss, similar to the noise variance in classic linear regression problems. Therefore, when considering the problems with least-square loss function, it is common to define $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ as the excess risk and attempt to minimize this term. Consequently, Theorem~\ref{thm:training_loss_bound} reveals that when using the optimal parameters, the excess risk $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ will converge to $0$ as the context length $n$ goes to infinity.

% In comparison, the optimal learning rate $\alpha$ takes the order at the reciprocal of $n$ and $L$. This implies for both two cases: (i) more in-context examples;(ii) more layers; the output of pre-trained transformers will equal to that of a more refined gradient descent updating with smaller learning rates. In common, the small-step strategy usually guarantees the convergence of the objective, which implies the potential benefits of deeper-layer architecture. 

% Regarding the choice of optimal parameters, it demonstrates that the optimal initialization $\wb_{\mathrm{gd}}^{(0)}$ aligns the direction of the expectation $\bbeta^*$, with its length independent of the number of steps $L$. In comparison, the optimal learning rate $\alpha$ takes the order at the reciprocal of the number of steps $L$. These results clearly identify the fluctuations of optimal $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$ as $L$ varies, enabling us to investigate their performance at testing distribution $\cD_{\mathrm{te}}$.
% Besides, Theorem~\ref{thm:training_loss_bound} also provides the upper bound for the population loss achieved when choosing the optimal learning rate and initialization. Here, the constant $C$ represents an irreducible term arising from the variance of the model. Such an irreducible term always exists when considering least-squares loss, similar to the noise variance in classic linear regression problems. Therefore, when considering the problems with least-square loss function, it is common to define $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ as the excess risk and attempt to minimize this term. Consequently, Theorem~\ref{thm:training_loss_bound} reveals that when using the optimal parameters, the excess risk $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ will converge to $0$ as the context length $n$ goes to infinity. Regarding the choice of optimal parameters, it demonstrates that the optimal initialization $\wb_{\mathrm{gd}}^{(0)}$ aligns the direction of the expectation $\bbeta^*$, with its length independent of the number of steps $L$. In comparison, the optimal learning rate $\alpha$ takes the order at the reciprocal of the number of steps $L$. These results clearly identify the fluctuations of optimal $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$ as $L$ varies, enabling us to investigate their performance at testing distribution $\cD_{\mathrm{te}}$.

% In least square Therefore 
% term, which similar to the 
% Therefore, to investigate the capacity of transformers with different parameters, 
% is straightforward that the 
% This implies to study the output of transformers with different parameters, we can investigate the optimization process of 
 % mention optimal w0 and learning rate

\subsection{Robustness against context hijacking}
% In this section, we present our conclusions regarding the robustness of transformers against context hijacking.
The previous two subsections illustrate that for any input with context examples, we can obtain the corresponding prediction for that input from the well-trained transformers by applying gradient descent with the optimal parameters we derived in Theorem~\ref{thm:training_loss_bound}. As we model $\cD_{\mathrm{te}}$ the distribution of hijacking examples, to examine the robustness of $L$-layer transformers against hijacking, we only need to check whether the linear model achieved by $L$-step gradient on $(\xb_{\mathrm{hc}}, y_{\mathrm{hc}})$ can still conduct successful classification on $\xb_{\mathrm{query}}$. 
% By determining the optimal learning rates and initialization in Theorem~\ref{thm:training_loss_bound}, we successfully identified a pre-trained transformers that perform well on our general data distribution $\cD_{\mathrm{tr}}$. Thus, for any input with context examples, we can obtain the corresponding prediction for that input from the pre-trained transformers by applying gradient descent with the same optimal parameters. As we model $\cD_{\mathrm{te}}$ the distribution of hijacking examples, to examine the robustness of $L$-layer transformers against hijacking, we only need to check whether the linear model achieved by $L$-step gradient on $(\xb_{\mathrm{hc}}, y_{\mathrm{hc}})$ can still conduct successful classification on $\xb_{\mathrm{query}}$. 
Specifically, we consider the classification error of the parameter vector $\tilde\wb_{\mathrm{gd}}^{(L)}$ as, 
\begin{align*}
    \cE(\tilde\wb_{\mathrm{gd}}^{(L)}):= \mathbb{P}_{T, \wb^*\sim\cD_{\mathrm{te}}} \big(y_{\mathrm{query}}\cdot \la\tilde\wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra  <0\big),
\end{align*}
where $T=\{(\xb_{\mathrm{hc}}, y_{\mathrm{hc}}), (\xb_{\mathrm{query}}, y_{\mathrm{query}})\}$, and $\tilde\wb_{\mathrm{gd}}^{(L)}$ is obtained by implementing gradient descent on $(\xb_{\mathrm{hc}}, y_{\mathrm{hc}})$ with $L$ steps and the optimal $\alpha$ and $\wb_{\mathrm{gd}}^{(0)}$. Similar to the previous result, 
$\cE(\tilde\wb_{\mathrm{gd}}^{(L)})$ is identical to $\cE(\mathtt{TF})$ defined in~\eqref{eq:error_tf}. Based on these preliminaries, we are ready to present our results regarding the robustness against context hijacking. We first introduce the following lemma illustrating that when the context length of hijacking examples is small, we hardly observe the label flipping phenomenons of the prediction from well-trained transformers. 
\begin{lemma}\label{lemma:test_I}
    Assume that all assumptions in Theorem~\ref{thm:training_loss_bound} still hold.  Additionally, assume that the length of hijacking examples $N$ is small such that $N\leq \tilde O\big(\frac{n}{d^{3/2}}\big)$ and $\sigma$ follows any continuous distribution. Based on these assumptions, it holds that 
    \begin{align*}
        \cE(\tilde\wb_{\mathrm{gd}}^{(L)}) \leq \cE(\wb_{\mathrm{gd}}^{(0)}) +  o(1).
    \end{align*}
\end{lemma}
Lemma~\ref{lemma:test_I} demonstrates that when the context length of hijacking examples is small, the classification error of the linear model obtained through gradient descent on these hijacking examples is very close to that of the optimal initialization. The reasoning is straightforward: When $N$ is relatively smaller compared to training context length $n$, and since the optimal learning rate $\alpha$ is on the order of the reciprocal of $n$, the contributions from the hijacking examples become almost negligible in gradient descent iterations, allowing the model to remain close to its initialization. Consequently, we consider the case that $N$ is comparable with $n$ in the following theorem.
% , and present our result regarding the classification error in the following theorem.
%the context hijacking phenomenon occurs only when the linear model learns excessively in the direction of $y_{\mathrm{hc}}\cdot\xb_\perp$, and this component ultimately leads the model to fail in classifying $\xb_{\mathrm{query}}$. However, when $N$ is relatively smaller compared to training context length $n$, and since the optimal learning rate $\alpha$ is on the order of the reciprocal of $n$, the contributions from the hijacking examples become almost negligible in gradient descent iterations, allowing the model to remain close to its initialization. Consequently, we consider the case that $N$ is comparable with $n$, and present our result regarding the classification error in the following theorem.
\begin{theorem}\label{thm:test_II}
    Assume that all assumptions in Theorem~\ref{thm:training_loss_bound} still hold.  Additionally, assume that $N\geq \tilde\Omega\big(\frac{n}{d^{3/2}}\big)$, $n\geq \tilde\Omega\big(Nd\big) $,  and  $\sigma$ follows some uniform distribution. Based on these assumptions, it holds that 
    \begin{align*}
        \cE(\tilde\wb_{\mathrm{gd}}^{(L)}) \leq c_1 - c_2 \Bigg(1-\tilde\Theta\bigg(\frac{Nd}{nL}\bigg)\Bigg)^L,
    \end{align*}
    where $c_1$, $c_2$ are two positive scalar solely depending on the distribution of $\sigma$ and $\wb^*$.
\end{theorem}
Based on a general assumption that $\sigma$ follows the uniform distribution, Theorem~\ref{thm:test_II} formulates the upper bound of the classification error as a function of the training context length $n$, the number of hijacking examples $N$, and the number of layers $L$. Specifically, this upper bound contains a term proportional to $-\big(1-\tilde\Theta\left(\frac{Nd}{nL}\right)\big)^L$. As $\big(1-\tilde\Theta\left(\frac{Nd}{nL}\right)\big)^L$ is a monotonically increasing function for $N$ and a monotonically decreasing function for $n$ and $L$, Theorem~\ref{thm:test_II} successfully demonstrates two facts: (i) well-trained transformers with deeper architectures, or those pre-trained on longer context examples, will exhibit more robustness against context hijacking; and (ii) for a given well-trained transformer, the context hijacking phenomenon is easier to observe when provided with more hijacking examples. These conclusions align well with our experimental observations (Figure ~\ref{fig:lineartf_N}).
% after implements $$
% we regard as the metric to evaluate the robustness of transformers against hijacking.
% is equal


% can generate the output of the prediction of the pre-trained transformers



% first give the formula of the robustness, showing that with 
% sufficient number of hijacking context, the output of the data can be reversed.

% then show that deeper layers give more robustness.

\begin{figure*}
    \centering
    \includegraphics[width=1\linewidth]{linear_N10_40.png}
    \vskip -0.in
    \caption{Linear transformers experiments with different depths and different training context lengths. By testing the trained linear transformers on the test set, we can find that as the number of interference samples increases, the model prediction accuracy becomes worse. However, deeper models have higher accuracy, indicating stronger robustness. As the training context length increases, the model robustness will also increase because the accuracy converges significantly more slowly.}
    \label{fig:lineartf_N}
    \vskip -0.1in
\end{figure*}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{train_n3040.png}
    \vskip -0.in
    \caption{Linear transformers experiments on training dataset. By testing trained linear transformers on the training set, the initial accuracy of the model is high and can be improved with the increase of context length, indicating that the model can use in-context learning to fine-tune $\bbeta^\star$ to $\wb^\star$. And deeper models have stronger optimization capabilities.}
    \label{fig:lineartf_train}
    \vskip -0.1in
\end{figure}


%\section{Robustness of Transformers with Different Number of Layers}
\section{Experiments}

In this section, we conduct experiments based on the setting in Section \ref{sec:Preliminaries} to verify the theory we proposed in Section \ref{sec:theory}. We first verify the consistency of our theory with optimal multi-step gradient descent. Then we train a series of linear transformers to examine their robustness on test data. 
% Specifically, we analyze how the accuracy of the models changes with the number of model layers and context length.
\subsection{Optimal gradient descent with different steps}
In our theoretical framework, for the same optimization objective, the optimal gradient descent with more steps $L$ or longer training context length $n$ will have a smaller learning rate per step (Theorem ~\ref{thm:training_loss_bound}), and this combination of more steps with small learning rates will perform better on the optimization process over context samples (Theorem ~\ref{thm:test_II}). Our theory shows that a trained transformer will learn the optimal multi-step gradient descent, which will make it more robust during testing. Therefore, we directly verified the consistency between practice and theory in the multi-step gradient descent experiment.

We construct a single-layer neural network to conduct optimal multi-step gradient descent experiments. Each training sample $(\xb_i, y_i)$ is drawn i.i.d. from the distribution $\cD_{\mathrm{tr}}$ defined in Section \ref{def:train_data}. We consider the learning rate that minimizes the loss of the test sample which is also drawn from $\cD_{\mathrm{tr}}$ when the single-layer neural network is trained using 1 to 8 steps of gradient descent, that is, the optimal learning rate $\alpha_L$ corresponding to $L$-step gradient descent, which can be obtained by grid search. Figure \ref{fig:gd_gridsearch} shows that $\alpha_L$ decreases as $L$ and $n$ increases, which is aligned with our theoretical results (Theorem ~\ref{thm:training_loss_bound}).

Next, we discuss the second part of the theoretical framework, i.e., gradient descent with more steps and small step size performs a more fine-grained optimization (Theorem ~\ref{thm:test_II}), which can be verified by our experiment results. We apply the optimal learning rate searched in the training phase to the test phase, and perform gradient descent optimization on the test samples drawn from $\cD_{\mathrm{te}}$ with the optimal learning rate and its corresponding number of steps. We can find that with the increase in the number of gradient descent steps and the decrease in the corresponding learning rate, the performance of the model will be significantly improved.

\subsection{Robustness of linear transformers with different number of layers}


Applying our theoretical framework to the context hijacking task on transformers can explain it well, indicating that our theory has practical significance. We train linear transformers with different depths and context lengths on the training dataset based on distribution $\cD_{\mathrm{tr}}$. We mainly investigate the impact of training context length $n$, and model depth $L$ and the testing context length $N$ on model classification accuracy.

We first test the trained transformers on the training dataset to verify that the model can fine-tune the memorized $\bbeta^*$ to $\wb^\star$ . According to the Figure~\ref{fig:lineartf_train}, we can find that the model has a high classification accuracy when there are very few samples at the beginning. This means that the model successfully memorizes the shared signal $\bbeta^*$. As the context length increases, the accuracy of the model gradually increases and converges, meaning that the model can fine-tune the pre-trained $\bbeta^*$ by using the context samples. In addition, deeper models can converge to larger values faster, corresponding to the theoretical view that deeper models can perform more sophisticated optimization.

Then we conduct experiments on the test set. Observing the experiment results (Figure \ref{fig:lineartf_N}), we can see that as the context length increases, the accuracy of the model decreases significantly and converges to 50\%, showing that the model is randomly classifying the final query $\xb_\mathrm{query}$. This is consistent with the 
 context hijacking phenomenon that the model's robustness will deteriorate as the number of interference prompt words increases. When the number of layers increases, the models with different depths show the same trend as the context length increases, but the accuracy of the model will increase significantly, which is consistent with the phenomenon that deeper models show stronger robustness in practical applications. In addition, the model becomes significantly more robust as the training context length increases, which is reflected in the fact that the classification accuracy converges more slowly as the length increases.

\section{Conclusion and discussion}
In this paper, we explore the robustness of transformers from the perspective of in-context learning. We are inspired by the real-world problem of LLMs, namely context hijacking~\cite{jiangllms}, and we build a comprehensive theoretical framework by modeling context hijacking phenomenon as a linear classification problem. We first demonstrate the context hijacking phenomenon by conducting experiments on LLMs with different depths, i.e., the output of the LLMs can be simply manipulated by modifying the context with factually correct information. This reflects an intuition: deeper models may be more robust. Then we develop a comprehensive theoretical analysis of the robustness of transformer, showing that the well-trained transformers can achieve the optimal gradient descent strategy. More specifically, we show that as the number of model layers or the length of training context increase, the model will be able to perform more fine-grained optimization steps over context samples, which can be less affected by the hijacking examples, leading to stronger robustness. Specifically considering the context hijacking task, our theory can fully explain the various phenomena, which is supported by a series of numerical experiments.

Our work provides a new perspective for the robustness explanation of transformers and the understanding of in-context learning ability, which offer new insights to understand the benefit of deeper architecture. Besides, our analysis on the optimal multi-step gradient descent may also be leveraged to other problems that involve the numerical optimization for linear problems. 

% can well explain the real-world problems of LLMs, which fills the gap in the white-box robustness interpretation of transformer-based LLMs, and brings more realistic significance to our theory.

%\section*{Impact Statement}
 %This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here.



% \section{Electronic Submission}
% \label{submission}

% \section{Format of the Paper}

% \section*{Accessibility}

% \section*{Software and Data}

% \section*{Acknowledgements}

% \section*{Impact Statement}

%\nocite{langley00}

\bibliography{example_paper}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% APPENDIX
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\appendix
\onecolumn
%\section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one.  

% The $\mathtt{\backslash onecolumn}$ command above can be kept in place if you prefer a one-column appendix, or can be removed if you prefer a two-column appendix.  Apart from this possible change, the style (font size, spacing, margins, page numbering, etc.) should be kept the same as the main body.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{Utility notations}\label{sec:notations}
% In this section, we present preliminary illustrations of several utility notations. These notations will be used in subsequent technical derivations and proofs. First, we denote $\cS_{t, k}$ as the set of all $k$-dimensional tuples whose entries are drawn from $\{0, 1, \ldots, t-1\}$ without replacement, i.e.
% \begin{align*}
%     \cS_{t, k} = \big\{(j_1, j_2, \ldots, j_k)|j_1, j_2, \ldots, j_k\in \{0, 1, \ldots, t-1\};j_1\neq j_2\neq\ldots\neq j_k\big\}.
% \end{align*}
% Additionally, we define $\alpha_t' = 2 \alpha_t$ as the practical learning rate as the gradient of least-square loss introduces a factor of $2$. From this point forward, we will use $\alpha_t'$ instead of $\alpha_t$ in all subsequent sections. Then given the set of all historical learning rates before or at $t$-th iteration, i.e. $\{\alpha'_0, \alpha'_1, \ldots, \alpha'_{t-1}\}$, and $\cS_{t, k}$ defined above,  we define $A_{t, k}$ as
% \begin{align*}
%     A_{t, k}:=\sum_{(j_1, j_2, \ldots, j_k) \in \cS_{t, k}}\prod_{\kappa=1}^k\alpha_{j_\kappa}'.
% \end{align*}
% Moreover, 
% %we manually set $A_{t, 0} = 0$. Moreover, 
% let $x_1, x_2, \ldots, x_d$ be $d$ independent and identically distributed (i.i.d.) random variables following the distribution $\cN(0, 1)$. Then for any $j_1\neq j_2 \in [d]$, we define the following notations 
% \begin{align*}
%     &B_{d, k} := \EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^kx_{j_1}^2\Big];\quad C_{d, k} := \EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^kx_{j_1}^4\Big];\quad D_{d, k} := \EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^kx_{j_1}^2x_{j_2}^2\Big];\\
%     &E_{d, k} := \EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^k\sign(x_{j_1})x_{j_1}];\quad F_{d, k} := \EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^k\sign(x_{j_1})x_{j_1}^3];\quad G_{d, k} :=\EE\Big[\Big(\sum_{i=1}^d x_i^2\Big)^k\sign(x_{j_1})x_{j_1}x_{j_2}^2\Big].
% \end{align*}
% It is straightforward that the values of all $B_{d, k}$'s, $ C_{d, k}$'s, $D_{d, k}$'s, $E_{d, k}$'s, $F_{d, k}$'s, and $G_{d, k}$'s are independent of the choices of difference subscripts $j_1$ and $j_2$ due to the symmetry among $x_i$'s with $i\in [k]$. We provide detailed calculations for $B_{d, k}$'s, $ C_{d, k}$'s, $D_{d, k}$'s, $E_{d, k}$'s, $F_{d, k}$'s, and $G_{d, k}$'s in Section~\ref{sec:technical_lemmas}.
%\section{Notations}
%Given two sequences $\{x_n\}$ and $\{y_n\}$, we denote $x_n = O(y_n)$ if there exist some absolute constant $C_1 > 0$ and $N > 0$ such that $|x_n|\le C_1 |y_n|$ for all $n \geq N$. Similarly, we denote $x_n = \Omega(y_n)$ if there exist $C_2 >0$ and $N > 0$ such that $|x_n|\ge C_2 |y_n|$ for all $n > N$. We say $x_n = \Theta(y_n)$ if $x_n = O(y_n)$ and $x_n = \Omega(y_n)$ both holds. Additionally, we denote $x_n = o(y_n)$ if, for any $\epsilon > 0$, there exists some $N(\epsilon) > 0$ such that $|x_n|\le \epsilon |y_n|$ for all $n \geq N(\epsilon)$, and we denote $x_n = \omega(y_n)$ if $y_n = o(x_n)$. We use $\tilde O(\cdot)$, $\tilde \Omega(\cdot)$, and $\tilde \Theta(\cdot)$ to hide logarithmic factors in these notations respectively. Finally, for any $n\in \NN_+$, we use $[n]$ to denote the set $\{1, 2, \cdots, n\}$.
\section{Proof of Proposition \ref{def: prop5.1}}\label{appendix:prop5.1}
In this section we provide a proof for Proposition \ref{def: prop5.1}.
% \begin{proposition}[Restate of Proposition \ref{def: prop5.1}]\label{prop:construction}
% There exists a single-head linear transformer with $L$ layers, let ${y}_{\ell}^{n+1}$ be the output of the $l$-th layer, i.e. the $(d+1,n+1)$-th entry of $\Zb_l$, then it holds that ${y}_{\ell}^{n+1} = -\la \wb_{\ell}^{\text{gd}}, {\xb}^{n+1} \ra$. Here, $\{\wb_{\ell}^{\text{gd}}\}$ is defined as $\wb_{0}^{\text{gd}} = \bbeta^*$, a non-zero value, and as follows for $\ell = 0, \ldots, L-1$:\begin{flalign}
% &\wb_{\ell+1}^{\text{gd}} = \wb_{\ell}^{\text{gd}} - \bGamma_{\ell} \nabla \widetilde{L}(\wb_{\ell}^{\text{gd}}), \quad \nonumber\\
% &\text{where} \quad \widetilde{L}(\wb) = \frac{1}{2} \sum_{i=1}^{n} (\la\wb, \xb^{(i)}\ra - y^{(i)})^2.
% \end{flalign}
% \end{proposition}
\begin{proof}[Proof of Proposition~\ref{def: prop5.1}]
Our proof is inspired by Lemma 1 in \citet{ahn2023transformers}, while we consider a non-zero initialization. We first provide the parameters $\Wb_E, \Pb_\ell, \Qb_\ell\in \RR^{(d+1) \times (d+1)}$ of a $L$-layers transformer.
$$
\Wb_E = \begin{bmatrix}
\Ib_{n} & 0 \\
-\wb_{\mathrm{gd}}^{(0)} & 1
\end{bmatrix}, \quad
\Pb_\ell = \begin{bmatrix}
\mathbf{0}_{d \times d} & 0 \\
0 & 1
\end{bmatrix}, \quad
\Qb_\ell = \begin{bmatrix}
-\bGamma_\ell & 0 \\
0 & 0
\end{bmatrix} \quad \text{where } \bGamma_\ell \in \RR^{d \times d}.
$$
For the linear classification problem, the input sample $\Zb_0\in \RR^{(d+1) \times (n+1)}$ consists of $\{(\xb_i, y_i)\}_i=1^n$ and $(\xb_{\mathrm{query}}, y_{\mathrm{query}})$ in \eqref{eq:Z_train} , which will first be embedded by $\Wb_E$. Let $\Xb^{(0)}\in \RR^{d \times (n+1)}$ denote the first $d$ rows of $\Wb_E(\Zb_0)$ and let $\Yb^{(0)}\in \RR^{1 \times (n+1)}$ denote the $(d+1)$-th row of $\Wb_E(\Zb_0)$. In subsequent iterative updates in (\ref{eq:Zi}), the values at the same position will be denoted as $\Xb^{(l)}$ and $\Yb^{(l)}$, for $l = 1, \ldots, L$. Similarly, define $\bar{\Xb}^{(l)}\in \RR^{d \times n}$ and $\bar{\Yb}^{(l)}\in \RR^{1 \times n}$ as matrices that exclude the last query sample $(\xb_{\mathrm{query}}^{(l)}, y_{\mathrm{query}}^{(l)})$. That is, they only contain the first $n$ columns of the output of the $l$-th layer. Let $\xb_i^{(l)}$ and $y_i^{(l)}$ be the $i$-th pair of samples output by the $l$-th layer. Define a function $g(\xb, y, l) : \RR^d \times \RR \times \ZZ \rightarrow \RR$: let $\xb_{\mathrm{query}}^{(0)} = \xb$ and $y_{\mathrm{query}}^{(0)} = y - \la\wb_{\mathrm{gd}}^{(0)}, \xb\ra$, then $g(\xb, y, l) := y_{\mathrm{query}}^{(l)}$.
%So function $g(x, y, k)$ represents the output of the $k$-th layer when the initial query is $(x,y)$.
Next, based on the update formula (\ref{eq:Zi}) and the parameters constructed above, we have:
$$
\Xb^{(l+1)} = \Xb^{(l)} = \cdots = \Xb^{(0)},\quad
\Yb^{(l+1)} = \Yb^{(l)} - \Yb^{(l)} \Mb (\Xb^{(0)})^\top \bGamma_l \Xb^{(0)}.
$$
Then for all $i \in \{1, \ldots, n\}$,
$$
y_i^{(l+1)} = y_i^{(l)} - \sum_{j=1}^{n} {\xb_i}^\top \bGamma_l \xb_j y_j^{(l)}.
$$
So $y_i^{(l+1)}$ does not depend on $y_{\mathrm{query}}^{(l+1)}$. For query position,
$$
y_{\mathrm{query}}^{(l+1)} = y_{\mathrm{query}}^{(l)} - \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_l \xb_j y_j^{(l)}.
$$
Then we obtain $g(\xb, y, l)$ and $g(\xb, 0, l)$:
\begin{flalign*}
g(\xb, y, l)&= y_{\mathrm{query}}^{(l-1)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&=y_{\mathrm{query}}^{(l-2)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-2} \xb_j y_j^{(l-2)}-  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&\vdotswithin{=}\\
&=y_{\mathrm{query}}^{(0)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{0} \xb_j y_j^{(0)}-\cdots -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&=y-\la\wb_{\mathrm{gd}}^{(0)}, \xb_{\mathrm{query}}\ra- \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{0} \xb_j y_j^{(0)}-\cdots -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)};\\
g(\xb, 0, l)&= y_{\mathrm{query}}^{(l-1)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&=y_{\mathrm{query}}^{(l-2)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-2} \xb_j y_j^{(l-2)}-  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&\vdotswithin{=}\\
&=y_{\mathrm{query}}^{(0)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{0} \xb_j y_j^{(0)}-\cdots -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)}\\
&=-\la\wb_{\mathrm{gd}}^{(0)}, \xb_{\mathrm{query}}\ra- \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{0} \xb_j y_j^{(0)}-\cdots -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_{l-1} \xb_j y_j^{(l-1)};.
\end{flalign*}
So we have $g(\xb, y, l)=g(\xb, 0, l)+y$. Observing $g(\xb, 0, l)$, we can find that it is linear in $\xb$ for the reason that every term of $g(\xb, 0, l)$ is linear in $\xb_{\mathrm{query}}$, which means we can rewrite it. We verify that there exists a $\btheta_l \in \RR^d$ for each $l \in [L]$, such that for all $\xb,y$,
$$
g(\xb, y, l) = g(\xb, 0, l) + y = \la \btheta_l, \xb \ra + y.
$$
Let $l=0$, we have $\la \btheta_0, \xb \ra=g(\xb, y, 0)-y=y_{\mathrm{query}}^{(0)}-y=-\la\wb_{\mathrm{gd}}^{(0)}, \xb_{\mathrm{query}}\ra$, so $\btheta_0=-\wb_{\mathrm{gd}}^{(0)}$. Next, we will show that for all $(\xb_i, y_i )\in \{(\xb_1,y_1) \ldots, (\xb_n,y_n), (\xb_{\mathrm{query}}, y_{\mathrm{query}})\}$,
$$
g(\xb_i, y_i, l) = y_i^{(l)} = \la \btheta_l, \xb_i \ra + y_i.
$$
Observing the update formulas for $y_{i}^{(l+1)}$ and $y_{\mathrm{query}}^{(l+1)}$, if we let $\xb_{\mathrm{query}} := \xb_i$ for some $i$, we can get that $y_{i}^{(l+1)} = y_{\mathrm{query}}^{(l+1)}$ because $y_{i}^{(0)} = y_{\mathrm{query}}^{(0)}$ by definition. This indicates that
$$
\bar{\Yb}^{(l)} = \bar{\Yb}^{(0)} + \btheta_l^T \bar{\Xb}.
$$
Finally, we can rewrite the update formula for $y_{\mathrm{query}}^{(l)}$ 
\begin{flalign}
y_{\mathrm{query}}^{(l+1)} &= y_{\mathrm{query}}^{(l)} -  \sum_{j=1}^{n} \xb_{\mathrm{query}}^\top \bGamma_l \xb_j y_j^{(l)}.\nonumber\\
&= y_{\mathrm{query}}^{(l)} -  \la \bGamma_l \bar{\Xb} (\bar{\Yb}^{(l)})^\top, \xb_{\mathrm{query}} \ra\nonumber\\
\Rightarrow \quad \Big\la \btheta_{l+1}, \xb_{\mathrm{query}} \ra &= \la \btheta_l, \xb_{\mathrm{query}}\ra - \la \bGamma_l \bar{\Xb} \big( \bar{\Xb}^\top \btheta_l + (\bar{\Yb}^{(0)})^\top \big), \xb_{\mathrm{query}} \Big\ra\nonumber
\end{flalign}
Since $\xb_{\mathrm{query}}$ is an arbitrary variable, we get the more general update formula for $\btheta_l$:
$$
\btheta_{l+1} = \btheta_l -  \la \bGamma_l \bar{\Xb} \left( \bar{\Xb}^\top \btheta_l +  (\bar{\Yb}^{(0)})^\top\right)\ra.
$$
Notice that we use the mean squared error, we have
\begin{flalign}
\widetilde{L}(\wb) &= \frac{1}{2} \sum_{i=1}^{n} (\la\wb, \xb_i\ra - y_i)^2\nonumber\\
&=\frac{1}{2}\|\bar{\Xb}^\top \wb - (\bar{\Yb}^{(0)})^\top\|_2.\nonumber
\end{flalign}
Then we get its gradient $\nabla \widetilde{L}(\wb)= \bar{\Xb} \left( \bar{\Xb}^\top \wb - (\bar{\Yb}^{(0)})^\top\right)$. Let $ \wb_{\mathrm{gd}}^{(l)} := -\btheta_l $, we have
\begin{flalign}
\btheta_{l+1} &= \btheta_l -  \la \bGamma_l \bar{\Xb} \left( \bar{\Xb}^\top \btheta_l + \bar{\Yb}_0^\top \right)\ra\nonumber\\
\Rightarrow \quad \wb_{\mathrm{gd}}^{(l+1)} &= \wb_{\mathrm{gd}}^{(l)} -  \la \bGamma_k \bar{\Xb} \left( \bar{\Xb}^\top \wb_{\mathrm{gd}}^{(l)} - (\bar{\Yb}^{(0)})^\top \right)\ra\nonumber\\
&=\wb_{\mathrm{gd}}^{(l)}-\bGamma_l\nabla \widetilde{L}(\wb_{\mathrm{gd}}^{(l)}).\nonumber
\end{flalign}
And the output of the $l$-th layer ${y}_{\mathrm{query}}^{(l)}$ is
$$
g\left(\xb_{\mathrm{query}}, y_{\mathrm{query}}, l\right) = y_{\mathrm{query}} + \la \btheta_l, \xb_{\mathrm{query}} \ra = y_{\mathrm{query}} - \la \wb_{\mathrm{gd}}^{(l)}, \xb_{\mathrm{query}} \ra.
$$
In our settings, we have $y_{\mathrm{query}}^{(l)}=- \la \wb_{\mathrm{gd}}^{(l)}, \xb_{\mathrm{query}} \ra$ because the input query label is $0$.
\end{proof}

% $$
% y_{k+1}^{(n+1)} = y_k^{(n+1)} - \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_k \xb^{(j)} y_k^{(j)}.
% $$
% Then we obtain $g(\xb, y, k)$ and $g(\xb, 0, k)$:
% \begin{flalign}
% g(\xb, y, k)&= y_{k-1}^{(n+1)} -  \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &=y_{k-2}^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-2} \xb^{(j)} y_{k-2}^{(j)}-  \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &\vdotswithin{=}\\
% &=y_{0}^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{0} \xb^{(j)} y_{0}^{(j)}-\ldots- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &=y-{\bbeta^{\star}}^\top\xb^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{0} \xb^{(j)} y_{0}^{(j)}-\ldots- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)};\\
% g(\xb, 0, k)&= y_{k-1}^{(n+1)} - \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &=y_{k-2}^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-2} \xb^{(j)} y_{k-2}^{(j)}-  \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &\vdotswithin{=}\\
% &=y_{0}^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{0} \xb^{(j)} y_{0}^{(j)}-\ldots- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}\\
% &=-{\bbeta^{\star}}^\top\xb^{(n+1)}- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{0} \xb^{(j)} y_{0}^{(j)}-\ldots- \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_{k-1} \xb^{(j)} y_{k-1}^{(j)}.
% \end{flalign}
% So we have $g(\xb, y, k)=g(\xb, 0, k)+y$. Observing $g(\xb, 0, k)$, we can find that it is linear in $\xb$ for the reason that every term of $g(\xb, 0, k)$ is linear in $\xb^{(n+1)}$, which means we can rewrite it. We verify that there exists a $\btheta_k \in \RR^d$ for each $k \in \{1, \ldots, n\}$, such that for all $\xb,y$,
% $$
% g(\xb, y, k) = g(\xb, 0, k) + y = \la \btheta_k, x \ra + y.
% $$
% Let $k=0$, we have $\la \btheta_0, x \ra=g(\xb, y, 0)-y=y_0^{n+1}-y=-{\bbeta^{\star}}^\top\xb^{(n+1)}$, so $\btheta_0=-\bbeta^{\star}$. Next, we will show that for all $i \in \{1, \ldots, n+1\}$,
% $$
% g(\xb^{(i)}, y^{(i)}, k) = y_k^{(i)} = \la \btheta_k, \xb^{(i)} \ra + y^{(i)}.
% $$
% Observing the update formulas for $y_{k+1}^{(i)}$ and $y_{k+1}^{(n+1)}$, if we let $\xb^{(n+1)} := \xb^{(i)}$ for some $i$, we can get that $y_{k+1}^{(i)} = y_{k+1}^{(n+1)}$ because $y_{0}^{(i)} = y_{0}^{(n+1)}$ by definition. This indicates that
% $$
% \bar{\Yb}_k = \bar{\Yb}_0 + \btheta_k^T \bar{\Xb}.
% $$
% Finally, we can rewrite the update formula for $y_k^{(n+1)}$ 
% \begin{flalign}
% y_{k+1}^{(n+1)} &= y_k^{(n+1)} -  \sum_{j=1}^{n} {\xb^{(n+1)}}^\top \bGamma_k \xb^{(j)} y_k^{(j)}.\\
% &= y_k^{(n+1)} -  \la \bGamma_k \bar{\Xb} \bar{\Yb}_k^\top, \xb^{(n+1)} \ra\\
% \Rightarrow \quad \la \btheta_{k+1}, \xb^{(n+1)} \ra &= \la \btheta_k, \xb^{(n+1)} \ra - \la \bGamma_k \bar{\Xb} \left( \bar{\Xb}^\top \btheta_k + \bar{\Yb}_0^\top \right), \xb^{(n+1)} \ra
% \end{flalign}
% Since $\xb^{(n+1)}$ is an arbitrary variable, we get the more general update formula for $\btheta_k$:
% $$
% \btheta_{k+1} = \btheta_k -  \la \bGamma_k \bar{\Xb} \left( \bar{\Xb}^\top \btheta_k + \bar{\Yb}_0^\top \right)\ra.
% $$
% Notice that we use the mean squared error, we have
% \begin{flalign}
% \widetilde{L}(\wb) &= \frac{1}{2} \sum_{i=1}^{n} (\la\wb, \xb^{(i)}\ra - y^{(i)})^2\\
% &=\frac{1}{2}\|\bar{\Xb}^\top \wb - \bar{\Yb}_0^\top\|_2.
% \end{flalign}
% Then we get its gradient $\nabla \widetilde{L}(\wb)= \bar{\Xb} \left( \bar{\Xb}^\top \wb - \bar{\Yb}_0^\top \right)$. Let $ \wb_k^{\text{gd}} := -\btheta_k $, we have
% \begin{flalign}
% \btheta_{k+1} &= \btheta_k -  \la \bGamma_k \bar{\Xb} \left( \bar{\Xb}^\top \btheta_k + \bar{\Yb}_0^\top \right)\ra\\
% \Rightarrow \quad \wb_{k+1}^{\text{gd}} &= \wb_k^{\text{gd}} -  \la \bGamma_k \bar{\Xb} \left( \bar{\Xb}^\top \wb_k^{\text{gd}} - \bar{\Yb}_0^\top \right)\ra\\
% &=\wb_k^{\text{gd}}-\bGamma_k\nabla \widetilde{L}(\wb_k^{\text{gd}}).
% \end{flalign}
% And the output of the $k$-th layer ${y}_k^{n+1}$ is
% $$
% g\left(\xb^{(n+1)}, y^{(n+1)}, k\right) = y^{(n+1)} + \la \btheta_k, \xb^{(n+1)} \ra = y^{(n+1)} - \la \wb_k^{\text{gd}}, \xb^{(n+1)} \ra.
% $$
% In our settings, we have ${y}_k^{n+1}=- \la \wb_k^{\text{gd}}, \xb^{(n+1)} \ra$ because the input query label is $0$.

\section{Gradient descent updates of parameters}
In this section, we provide further details regarding the updating of parameters $\wb_{\mathrm{gd}}^{(l)}$, which will be utilized in subsequent proof. Besides, it can directly imply Proposition~\ref{prop:commutative invariance}. Before demonstrating the mathematical, we first introduce several utility notations, which will be used in subsequent technical derivations and proofs. We denote $\cS_{l, k}$ as the set of all $k$-dimensional tuples whose entries are drawn from $\{0, 1, \ldots, l-1\}$ without replacement, i.e.
\begin{align*}
    \cS_{t, k} = \big\{(j_1, j_2, \ldots, j_k)|j_1, j_2, \ldots, j_k\in \{0, 1, \ldots, l-1\};j_1\neq j_2\neq\ldots\neq j_k\big\}.
\end{align*}
%Additionally, we define $\alpha_t' = 2 \alpha_t$ as the practical learning rate as the gradient of least-square loss introduces a factor of $2$. From this point forward, we will use $\alpha_t'$ instead of $\alpha_t$ in all subsequent sections. 
Then given the set of all historical learning rates before or at $l$-th iteration, i.e. $\{\alpha_0, \alpha_1, \ldots, \alpha_{l-1}\}$, and $\cS_{l, k}$ defined above,  we define $A_{l, k}$ as
\begin{align*}
    A_{l, k}:=\sum_{(j_1, j_2, \ldots, j_k) \in \cS_{l, k}}\prod_{\kappa=1}^k\alpha_{j_\kappa}.
\end{align*}
Then we can observe that the permutation of elements of $\{\alpha_0, \alpha_1, \ldots, \alpha_{l-1}\}$ would not change the value of $ A_{l, k}$. Then based on these notations, we present mathematical derivation in the following.

By some basic gradient calculations, we can re-write the iterative rule of gradient descent~\eqref{eq:gd_update} as
\begin{align}\label{eq:gd_update_detail}
    \wb_{\mathrm{gd}}^{(l+1)} &= \wb_{\mathrm{gd}}^{(l)} - \alpha_l \nabla L(\wb_{\mathrm{gd}}^{(l)})\notag\\
    &= \wb_{\mathrm{gd}}^{(l)} -  \alpha_l\sum_{i=1}^n \big(\la\wb_{\mathrm{gd}}^{(l)},\xb_i\ra-\sign(\la \wb^*,\xb_i\ra)\big)\cdot\xb_i\notag\\
    &=\bigg(\Ib_d - \alpha_l\Big(\sum_{i=1}^n\xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(l)}+\alpha_l\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg).
\end{align}
% where $\alpha_t' = 2\alpha_t$ as defined in Section~\ref{sec:notations}. 
% Before we demonstrate our lemmas utilized in proof, we first introduce several notations for further presentation. 
% %For the set of all historical learning rates before or at $t$-th iteration, i.e. $\{\alpha'_0, \alpha'_1, \ldots, \alpha'_{t-1}\}$, 
% We denote $\cS_{t, k}$ as the set of all $k$-dimensional tuples whose entries are drawn from $\{0, 1, \ldots, t-1\}$ without replacement, i.e.
% \begin{align*}
%     \cS_{t, k} = \big\{(j_1, j_2, \ldots, j_k)|j_1, j_2, \ldots, j_k\in \{0, 1, \ldots, t-1\};j_1\neq j_2\neq\ldots\neq j_k\big\}.
% \end{align*}
% Then given the set of all historical learning rates before or at $t$-th iteration, i.e. $\{\alpha'_0, \alpha'_1, \ldots, \alpha'_{t-1}\}$, and $\cS_{t, k}$ defined above,  we define $A_{t, k}$ as
% \begin{align*}
%     A_{t, k}:=\sum_{(j_1, j_2, \ldots, j_k) \in \cS_{t, k}}\sum_{\kappa=1}^k\alpha_{j_\kappa}'.
% \end{align*}
Based on this detailed iterative formula, and the definition of $\cS_{l, k}$ and $A_{l, k}$ above, we present and prove the following lemma, which characterizes the closed-form expression for $\wb^{(l)}$. 

\begin{lemma}\label{lemma:iterative_induction}
    For the iterates of gradient descent, i.e. $\wb_{\mathrm{gd}}^{(l)}$'s with $l\in \{0, 1, \ldots, L-1\}$, it holds that 
    \begin{align}\label{eq:w_closed_form}
        \wb_{\mathrm{gd}}^{(l)}= \bigg(\Ib_d+\sum_{k=1}^l A_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^k\bigg)\cdot \wb_{\mathrm{gd}}^{(0)}+
        \bigg(\sum_{k=1}^l A_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1}\bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg).
        % \alpha_t'\bigg(\frac{1}{n}\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)
    \end{align}
    % \begin{align}\label{eq:w_closed_form}
    %     \wb^{(t)}= \prod_{\tau=0}^{t-1}\Bigg(\Ib_d - \alpha_\tau'\bigg(\frac{1}{n}\sum_{i=1}^n \xb_i\xb_i^\top\bigg)\Bigg)\cdot\wb^{(0)}+
    %     \Bigg[\sum_{k=1}^t \bigg(-\frac{1}{n}\sum_{i=1}^n \xb_i\xb_i^\top\bigg)^{k-1}\sum_{(j_1, \ldots, j_k) \in \binom{\{0, \ldots, t-1\}}{k}}\prod_{\kappa=1}^k\alpha_{j_\kappa}'\Bigg]\bigg(\frac{1}{n}\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg),
    %     % \alpha_t'\bigg(\frac{1}{n}\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)
    % \end{align}
    % where $\{j_1, j_2, \ldots,j_k\} \subset \{0, 1, 2, \ldots, t-1\}$. 
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:iterative_induction}]
Before we demonstrate our proof, we first present some conclusions regarding $\cS_{l, k}$ and $A_{l, k}$.
%a conclusion of the coefficient term of $\wb^{(0)}$ in~\eqref{eq:w_closed_form}. 
By directly applying the Binomial theorem and the definition of $A_{l, k}$, we can obtain that 
\begin{align}\label{eq:A_tk_propI}
     \prod_{k=0}^{l-1}\bigg(\Ib_d + \alpha_k\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg) = \Ib_d+\sum_{k=1}^l A_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^k.
\end{align}
Additionally, by utilizing the definition of $\cS_{l, k}$, we can easily derive that 
\begin{align*}
    \cS_{l+1, k} =& \big\{(j_1, j_2, \ldots, j_k)|j_1, j_2, \ldots, j_k\in \{0, 1, \ldots, l\};j_1\neq j_2\neq\ldots\neq j_k\big\}\\
    =& \big\{(j_1, j_2, \ldots, j_k)|j_1, j_2, \ldots, j_k\in \{0, 1, \ldots, l-1\};j_1\neq j_2\neq\ldots\neq j_k\big\}\\
    &\cup \big\{(j_1, j_2, \ldots, j_{k-1}, l)|j_1, j_2, \ldots, j_{k-1}\in \{0, 1, \ldots, l-1\};j_1\neq j_2\neq\ldots\neq j_{k-1}\big\}\\
    =&\cS_{l, k} \cup \big\{(j_1, j_2, \ldots, j_{k-1}, l)|(j_1, j_2, \ldots, j_{k-1})\in \cS_{l, k-1}\big\},
\end{align*}
holds when $k\leq l$. This result can further imply that 
\begin{align}\label{eq:A_tk_propII}
    A_{l+1, k}&=\sum_{(j_1, j_2, \ldots, j_k) \in \cS_{l+1, k}}\prod_{\kappa=1}^k\alpha_{j_\kappa}= \sum_{(j_1, j_2, \ldots, j_k) \in \cS_{l, k}}\prod_{\kappa=1}^k\alpha_{j_\kappa} + \sum_{(j_1, j_2, \ldots, j_{k-1}) \in \cS_{l, k-1}}\prod_{\kappa=1}^{k-1}\alpha_{j_\kappa}\alpha_l = A_{l, k} + \alpha_lA_{l, k-1}.
    %= \sum_{(j_1, j_2, \ldots, j_k) \in \cS_{t, k} \cup \{(j_1, j_2, \ldots, j_{k-1}, t)|(j_1, j_2, \ldots, j_{k-1})\in \cS_{t, k-1}\}}\sum_{\kappa=1}^k\alpha_{j_\kappa}'.
\end{align}
holds when $k\leq l$. Additionally, it is straightforward that 
\begin{align}\label{eq:A_tk_propIII}
    A_{l+1, l+1} = \alpha_lA_{l, l};\quad A_{l+1, 1} = A_{l, 1} + \alpha_l.
\end{align}
With these conclusions in hands, we will begin proving this lemma by induction. When $l=1$, by the iterative rule~\eqref{eq:gd_update_detail}, we can obtain that 
    \begin{align*}
        \wb_{\mathrm{gd}}^{(1)} 
    &=\bigg(\Ib_d - \alpha_0\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(0)}+\alpha_0\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg),
    \end{align*}
    which follows the conclusion of~\eqref{eq:w_closed_form} due to~\eqref{eq:A_tk_propI} and the definition of $A_{1, 1}$. By induction, we assume that~\eqref{eq:w_closed_form} holds at $l$-th iteration. Then at $(l+1)$-th iteration, we can obtain that,
    \begin{align*}
        \wb_{\mathrm{gd}}^{(l+1)} 
    =&\bigg(\Ib_d - \alpha_l\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(l)}+\alpha_l\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)\\
    =&\alpha_l\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) + \bigg(\Ib_d - \alpha_l\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\\
    &\cdot\Bigg\{\prod_{\tau=0}^{l-1}\bigg(\Ib_d - \alpha_\tau\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(0)}+
        \bigg(\sum_{k=1}^t A_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1}\bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)\Bigg\}\\
    =&\alpha_l\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) +\prod_{k=0}^{l}\bigg(\Ib_d - \alpha_k\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(0)}\\
    & +  \bigg(
    \sum_{k=1}^l A_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1} + \sum_{k=1}^l \alpha_lA_{l, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k}\bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) \\
    =&\prod_{k=0}^{l}\bigg(\Ib_d - \alpha_k\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(0)}+ (A_{l, 1} + \alpha_l)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) \\
    &+\Bigg(
    \sum_{k=2}^l (A_{l, k}+\alpha_l A_{l, k-1})\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1}\Bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) \\
    &+ \alpha_l A_{l, l}\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{l}\cdot\bigg(\sum_{i=1}^n\sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)\\
    =&\prod_{k=0}^{l}\bigg(\Ib_d - \alpha_k\Big(\sum_{i=1}^n \xb_i\xb_i^\top\Big)\bigg)\cdot\wb_{\mathrm{gd}}^{(0)}+ A_{l+1, 1}\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg) \\
    &+\bigg(
    \sum_{k=2}^t A_{l+1, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1}\bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)+ A_{l+1, l+1}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k}\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg)\\
    =&\bigg(\Ib_d+\sum_{k=1}^{l+1} A_{l+1, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^k\bigg)\cdot \wb_{\mathrm{gd}}^{(0)}+
        \bigg(\sum_{k=1}^{l+1} A_{l+1, k}\Big(-\sum_{i=1}^n \xb_i\xb_i^\top\Big)^{k-1}\bigg)\cdot\bigg(\sum_{i=1}^n \sign(\la\wb^*,\xb_i\ra)\cdot\xb_i\bigg).
    \end{align*}
The second equality holds by substituting $\wb_{\mathrm{gd}}^{(l)}$ with its expansion from~\eqref{eq:w_closed_form}, assuming it is valid at the $l$-th iteration by induction. The third and fourth equalities are established by rearranging the terms. The penultimate equality is derived by applying the conclusions regarding $A_{l, k}$ from~\eqref{eq:A_tk_propII} and~\eqref{eq:A_tk_propIII}. The final equality is obtained by applying~\eqref{eq:A_tk_propI}. This demonstrates that~\eqref{eq:w_closed_form} still holds at $l+1$-th iteration given it holds at $l$-th iteration, which finishes the proof of induction.
\end{proof}
Lemma~\ref{lemma:iterative_induction} demonstrate that the learning rates $\alpha_l$'s will only influence $\wb_{\mathrm{gd}}^{(L)}$ by determining the value of $A_{L, k}$'s. While as we have discussed above, the values of $A_{L, k}$'s depend solely on the elements in the $\{\alpha_0, \ldots, \alpha_{L-1}\}$, and remain unchanged when the order of these learning rates is rearranged. Consequently, the permutation of $\{\alpha_0, \ldots, \alpha_{L-1}\}$ will also not affect the value of $\wb_{\mathrm{gd}}^{(L)}$, thereby confirming that Proposition~\ref{prop:commutative invariance} holds.

\section{Proof of Theorem~\ref{thm:training_loss_bound}}
In this section, we provide a detailed proof for Theorem~\ref{thm:training_loss_bound}. We begin by introducing and proving a lemma that demonstrates how $\wb_{\mathrm{gd}}^{(0)}$ must align with the direction of $\bbeta^*$. This alignment constrains the choice of $\wb_{\mathrm{gd}}^{(0)}$ to a scalar multiple of $\bbeta^*$, specifically in the form of $c_0 \cdot \bbeta^*$. Additionally, in the subsequent sections, we will use the notation $\hat\bSigma = \sum_{i=1}^n \xb_i\xb_i^\top$.
\begin{lemma}\label{lemma:w_0direction}
    Under the same conditions with Theorem~\ref{thm:training_loss_bound}, to minimize the loss $\cR({\wb_{\mathrm{gd}}^{(L)}})$, $\wb_{\mathrm{gd}}^{(0)}$ is always in the form of $c_0 \cdot \bbeta^*$.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:w_0direction}]
    Utilizing the independence among the examples in $T =\{(\xb_1, y_1), \ldots, (\xb_n, y_n), (\xb_{\mathrm{query}}, y_{\mathrm{query}})\}$, and $\wb^*$, we can expand $\cR_{\wb_{\mathrm{gd}}^{(L)}}$ by law of total expectation as
    \begin{align*}
        \cR(\wb_{\mathrm{gd}}^{(L)})&= \EE_{T, \wb^*}\big[\big(\la \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra - \sign(\la \wb^*, \xb_{\mathrm{query}}\ra)\big)^2\big]\\
        &= \EE_{T, \wb^*}\big[\la \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra^2 - 2\sign(\la \wb^*, \xb_{\mathrm{query}}\ra)\la \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra\big] + 1\\
        &= \EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\Big[\EE_{(\xb_{\mathrm{query}}, y_{\mathrm{query}})}\big[\la \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra^2 - 2\sign(\la \wb^*, \xb_{\mathrm{query}}\ra)\la \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra\big|\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*\big]\Big] + 1\\
        &= \EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\Bigg[\bigg\|\wb_{\mathrm{gd}}^{(L)}-\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2^2\Bigg] + 1 - \frac{2}{\pi},
    \end{align*}
    where the last equality holds since $\EE_{\xb_{\mathrm{query}}}[\la\wb, \xb_{\mathrm{query}}\ra^2] = \wb^\top \EE_{\xb_{\mathrm{query}}}[ \xb_{\mathrm{query}}\xb_{\mathrm{query}}^\top]\wb = \|\wb\|_2^2$ when $\wb$ is independent with $\xb_{\mathrm{query}}$, and $\EE_{\xb_{\mathrm{query}}}[\la\wb_1, \xb_{\mathrm{query}}\ra\sign(\la\wb_2, \xb_{\mathrm{query}}\ra)] = \sqrt{\frac{2}{\pi}}\la\wb^*, \wb_{\mathrm{gd}}^{(L)}\ra$ implied by Lemma~\ref{lemma:expectation_I}. Therefore in the next we attempt to optimize the first term $\EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\Big[\Big\|\wb_{\mathrm{gd}}^{(L)}-\sqrt{\frac{2}{\pi}}\wb^*\Big\|_2^2\Big]$. By applying the closed form of $\wb_{\mathrm{gd}}^{(L)}$ in Lemma~\ref{lemma:iterative_induction} with all $\alpha_l = \alpha$, we have 
    \begin{align*}
        \wb_{\mathrm{gd}}^{(L)} = \big(\Ib_d-\alpha \hat\bSigma\big)^L\cdot \wb_{\mathrm{gd}}^{(0)} + \alpha \sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l}\cdot\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big).
    \end{align*}
   Based on this, we can further derive that 
    \begin{align}\label{eq:loss_boundI}
        \EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\Bigg[\bigg\|\wb_{\mathrm{gd}}^{(L)}-\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2^2\Bigg] =& (\wb_{\mathrm{gd}}^{(0)})^\top\EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\big[\big(\Ib_d-\alpha\hat\bSigma\big)^{2L}\big]\wb_{\mathrm{gd}}^{(0)} \notag\\
        &- 2\alpha (\wb_{\mathrm{gd}}^{(0)})^\top\EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\Big[\sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l+L}\cdot\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big)\Big] + C\notag\\
        =&c_1 \big\|\wb_{\mathrm{gd}}^{(0)}\big\|_2^2 - 2c_2 \la\wb_{\mathrm{gd}}^{(0)}, \bbeta^*\ra + C\notag\\
        =&c_1 \Big\|\wb_{\mathrm{gd}}^{(0)}-\frac{c_2}{c_1}\bbeta^*\Big\|_2^2 +  C- \frac{c_2^2}{c_1},
    \end{align}
    where $c_1, c_2, C$ are some scalar independent of $\wb_{\mathrm{gd}}^{(0)}$. The second inequality holds since $\EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\big[\big(\Ib_d-\alpha\hat\bSigma\big)^{2L}\big] =  c_1\Ib_d$ for some scalar $c_1$, guaranteed by Lemma~\ref{lemma:expectation_II}, and $\EE_{\{(\xb_i, y_i)\}_{i=1}^{n}, \wb^*}\big[\sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l+L}\cdot\big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\big)\big] = c_2\bbeta^*$ for some scalar $c_2$, guaranteed by Lemma~\ref{lemma:expectation_III}. As the result of~\eqref{eq:A_tk_propI} is a quartic function of $\wb_{\mathrm{gd}}^{(0)}$, we can easily conclude that it achieves the minimum value when $\wb_{\mathrm{gd}}^{(0)}=c_0\bbeta^*$ for some scalar $c_0$, which completes the proof.
\end{proof}

Based on Lemma~\ref{lemma:w_0direction}, in the following proof, we will directly replace $\wb_{\mathrm{gd}}^{(0)}$ with $c_0\bbeta^*$ and attempt to find the optimal $c_0$. Now we are ready to prove the following theorem, a representation of Theorem~\ref{thm:training_loss_bound}.
\begin{theorem}[Restate of Theorem~\ref{thm:training_loss_bound}]\label{thm:training_loss_boundII}
    For training distribution $\cD_{\mathrm{tr}}$ in Definition~\ref{def:train_data}, suppose that the training context length $n$ is sufficiently large such that $n\geq \tilde\Omega(\max\{d^2, dL\})$. Additionally, suppose that the perturbation of $\wb^*$ around its expectation $\bbeta^*$ is smaller than $\frac{\pi}{2}$, i.e. $\la\wb^*, \bbeta^*\ra>0$. then for any learning rate $\alpha$ and initialization $\wb_{\mathrm{gd}}^{(0)}$, it holds that 
    \begin{flalign*}
        \cR(\wb_{\mathrm{gd}}^{(L)}) \leq &\ \Theta\big((1-\alpha n)^L\|\wb_{\mathrm{gd}}^{(0)}-c_1\wb^*\|_2^2 \big) +  \tilde\Theta(\alpha d L) + C,
    \end{flalign*}
    where both $c_1, C$ are absolute constants. Additionally, by taking $\wb_{\mathrm{gd}}^{(0)} = c_1 \bbeta^*$ and $\alpha =\tilde\Theta(\frac{1}{nL})$, the upper bound above achieve its optimal rates as
    \begin{flalign*}
        \cR(\wb_{\mathrm{gd}}^{(L)}) \leq &\ \tilde\Theta\Big(\frac{d}{n}\Big) + C.
    \end{flalign*}
\end{theorem}

\begin{proof}[Proof of Theorem~\ref{thm:training_loss_boundII}]
Utilizing the fact that $\Ib_d-\big(\Ib_d-\alpha\hat\bSigma\big)^L= \alpha \sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l}\hat\bSigma$ and $\wb_{\mathrm{gd}}^{(0)} = c_0\bbeta^*$, we can re-write the close form of $\wb_{\mathrm{gd}}^{(L)}$ as 
\begin{align*}
    \wb_{\mathrm{gd}}^{(L)} = \Big(\Ib_d-\big(\Ib_d-\alpha\hat\bSigma\big)^L\Big)\cdot\sqrt{\frac{2}{\pi}}\wb^* + \big(\Ib_d-\alpha\hat\bSigma\big)^L\cdot c_0\bbeta^* - \alpha \sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l}\cdot\bigg(\sqrt{\frac{2}{\pi}}\hat\bSigma\wb^*-\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big)\bigg)
\end{align*}
Then by the similar calculation to Lemma~\ref{lemma:w_0direction}, we have 
\begin{align*}
    \cR(\wb_{\mathrm{gd}}^{(L)}) 
        &= \EE\Bigg[\bigg\|\wb_{\mathrm{gd}}^{(L)}-\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2^2\Bigg] + C\\
        &= \EE\Bigg[\bigg\|\big(\Ib_d-\alpha\hat\bSigma\big)^L\cdot\bigg(c_0\bbeta^*-\sqrt{\frac{2}{\pi}}\wb^*\bigg)-  \alpha \sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l}\cdot\bigg(\sqrt{\frac{2}{\pi}}\hat\bSigma\wb^*-\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big)\bigg)\bigg\|_2^2\Bigg] + C\\
        &\leq 2\underbrace{\EE\Bigg[\bigg\|\big(\Ib_d-\alpha\hat\bSigma\big)^L\cdot\bigg(c_0\bbeta^*-\sqrt{\frac{2}{\pi}}\wb^*\bigg)\bigg\|_2^2\Bigg]}_{I}+2 \underbrace{\EE\Bigg[\alpha^2 \bigg\|\sum_{l=0}^{L-1}\big(\Ib_d-\alpha\hat\bSigma\big)^{l}\cdot\bigg(\sqrt{\frac{2}{\pi}}\hat\bSigma\wb^*-\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big)\bigg)\bigg\|_2^2\Bigg]}_{II} + C
\end{align*}
where the last inequality hols by $(a+b)^2\leq 2a^2+2b^2$, and $C$ is an absolute constant. Therefore, in the following, we discuss the upper-bounds for $I$ and $II$ respectively. For $I$, we have 
\begin{align*}
    I\leq \EE\bigg[\Big\|\big(\Ib_d-\alpha\hat\bSigma\big)\Big\|_2^{2L}\bigg] \cdot\EE\Bigg[\bigg(c_0\bbeta^*-\sqrt{\frac{2}{\pi}}\wb^*\bigg)\bigg\|_2^2\Bigg] \leq O\big((1-\alpha n)^{2L}\big)\cdot\EE\Bigg[\bigg(c_0\bbeta^*-\sqrt{\frac{2}{\pi}}\wb^*\bigg)\bigg\|_2^2\Bigg],
\end{align*}
where the first inequality is derived by the independence among $\xb_i$ and $\wb^*$ and the submultiplicativity of $\ell_2$ norm, and the second inequality holds by the concentration results regarding $\|\hat\bSigma\|_2$ provided in Lemma~\ref{lemma:concentration_I}. For $II$, we can derive that
\begin{align*}
    II\leq \underbrace{\EE\Big[\alpha^2\sum_{l_1, l_2=0}^{L-1}\big\|\Ib_d-\alpha\hat\bSigma\big\|_2^{l_1+l_2}\Big]}_{II.1}\underbrace{\EE\bigg[\bigg\|\sqrt{\frac{2}{\pi}}\hat\bSigma\wb^*-\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i\Big)\bigg\|_2^2\bigg]}_{II.2}, 
\end{align*}
where the inequality is guaranteed by the submultiplicativity of $\ell_2$ norm. Then we discuss $II.1$ and $II.2$ respectively. For $II.1$, we have 
\begin{align*}
    II.1 \leq \frac{\alpha}{\|\hat\bSigma\|_2}\sum_{l_1, l_2=0}^{L-1} \frac{1}{l_1 + l_2 + 1} \leq \frac{\alpha L}{\|\hat\bSigma\|_2}\sum_{l_1=0}^{L-1} \frac{1}{l_1 + 1} \leq O\bigg(\frac{\alpha L\log L}{n}\bigg).
\end{align*}
The first inequality holds by the fact that $x(1-x)^k\leq \frac{1}{k+1}$ for $x\in [0, 1]$. The second inequality holds by replace $\frac{1}{l_1 + l_2 +1}$ with its upper bound $\frac{1}{l_1 + 1}$. The third inequality holds by $\sum_{l_1=0}^{L-1} \frac{1}{l_1 + 1}\leq \log L$ and $\|\hat\bSigma\|_2 =\Theta(n)$ demonstrated in Lemma~\ref{lemma:concentration_I}. For $II.2$, we have
\begin{align*}
    II.2 &= \EE\bigg[\bigg\|\sqrt{\frac{2}{\pi}}(\hat\bSigma -n\Ib_d)\wb^*-\Big(\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^*\Big)\bigg\|_2^2\bigg]\\
    &\leq \frac{4}{\pi}\EE\|\hat\bSigma -n \Ib_d\|_2^2 + 2\EE\bigg[\bigg\|\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2^2\bigg]\leq \tilde O(nd).
\end{align*}
The first equality adds and minuses the same term. The first inequality holds by the submultiplicativity of $\ell_2$ norm, 
and the fact $(a+b)^2 \leq 2a^2 +2b^2$. The second inequality holds as $\|\hat\bSigma -n \Ib_d\|_2\leq \tilde O(\sqrt{nd})$, proved in Lemma~\ref{lemma:concentration_I} and $\big\|\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^*\big\|_2\leq \tilde O(\sqrt{nd})$, proved in Lemma~\ref{lemma:concentration_II}. Combining all the preceding results, we can obtain that 
\begin{align*}
    \cR(\wb_{\mathrm{gd}}^{(L)}) \leq O\big((1-\alpha n)^{2L}\big)\cdot\EE\Bigg[\bigg(c_0\bbeta^*-\sqrt{\frac{2}{\pi}}\wb^*\bigg)\bigg\|_2^2\Bigg] + \tilde O(\alpha d L) + C.
\end{align*}
It is straightforward that when taking $c_0 =\sqrt{\frac{2}{\pi}}$, the expectation term will achieve its minimum, which is the variance of $\wb^*$ multiplying by a factor $\sqrt{\frac{2}{\pi}}$. This finishes the proof that the optimal initialization takes the value as $\wb_{\mathrm{gd}}^{(0)} = \sqrt{\frac{2}{\pi}}\bbeta^*$. We re-plug this result into the upper-bound above and utilize the fact that the variance is at the constant order. Then to find the optimal learning rate $\alpha$ is actually to optimize the summation of $(1-\alpha n)^{2L}$ and $\alpha d L$. We can note that the first term will decrease as $\alpha$ increases, while the second term will increase as $\alpha$ increases. Therefore, minimizing the summation of these two terms is essentially equivalent to finding an optimal $\alpha$ such that both terms are of the same order. Then we can notice that when consider $\alpha = \frac{\log(n/d)}{2nL}$, the first term can be bounded as 
\begin{align*}
    (1-\alpha n)^{2L} = \bigg(1-\frac{\log(n/d)}{2L}\bigg)^{2L} \leq \frac{d}{n}.
\end{align*}
Additionally, it is straightforward that $\alpha d L =\frac{d\log(n/d)}{n}$. When omitting the factors of $\log$, we conclude that these two terms are at the same order. Therefore, the optimal choice of learning rate is $\alpha =\tilde\Theta(\frac{1}{nL})$, which can optimize the excess risk as
\begin{align*}
    \cR(\wb_{\mathrm{gd}}^{(L)}) - C\leq \tilde O\bigg(\frac{d}{n}\bigg).
\end{align*}
This completes the proof.
\end{proof}

Here we provide further discussions regarding the upper bound for the population loss achieved when choosing the optimal learning rate and initialization. The constant $C$ represents an irreducible term arising from the variance of the model. Such an irreducible term always exists when considering least-squares loss, similar to the noise variance in classic linear regression problems. Therefore, when considering the problems with least-square loss function, it is common to define $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ as the excess risk and attempt to minimize this term. Consequently, Theorem~\ref{thm:training_loss_boundII} reveals that when using the optimal parameters, the excess risk $\cR(\wb_{\mathrm{gd}}^{(L)})-C$ will converge to $0$ as the context length $n$ goes to infinity.

\section{Proof of Lemma~\ref{lemma:test_I} and Theorem~\ref{thm:test_II}}
In this section, we provide the proof for both Lemma~\ref{lemma:test_I} and Theorem~\ref{thm:test_II}. W.L.O.G, we assume that $\sigma > 0$ in the subsequent proof. This implies that $y_{\mathrm{hc}}=-1$, $y_{\mathrm{query}} = 1$ and $\cE(\wb) = \PP(\la\wb, \xb_{\mathrm{query}}\ra<0)$ for any $\wb$. Then we first introduce a lemma providing a closed form for $\tilde\wb_{\mathrm{gd}}^{(l)}$, which is the parameter vector of the linear model trained by gradient descent with the optimal parameters derived in Theorem~\ref{thm:training_loss_bound} and data $(\xb_{\mathrm{hc}}, y_{\mathrm{hc}})$.

\begin{lemma}\label{lemma:test_closed_form}
    For the gradient descent iterates $\tilde\wb_{\mathrm{gd}}^{(l)}$, it holds that 
    \begin{align}\label{eq:test_closed_form}
        \tilde\wb_{\mathrm{gd}}^{(l)} = c\bbeta^*+a(l)\cdot \xb_\perp
    \end{align}
    for all $l\in \{0, 1, \ldots, L\}$. $c$ is the coefficient of $\bbeta^*$ of initialization $\wb_{\mathrm{gd}}^{(0)}$ and $a(l)$ follows that
    \begin{align*}
        a(l) = -\Big(1-\big(1-\alpha N\|\xb_\perp\|_2^2\big)^l\Big)\frac{1+c\la\xb_\perp, \bbeta^*\ra}{\|\xb_\perp\|_2^2}
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:test_closed_form}]
We prove this lemma by induction. It is straightforward that $\tilde\wb_{\mathrm{gd}}^{(0)} =c\bbeta^*$ and $\tilde\wb_{\mathrm{gd}}^{(1)} =c\bbeta^* -  \alpha N \xb_{\perp}$, complying with the formula~\eqref{eq:test_closed_form}. By induction, we assume~\eqref{eq:test_closed_form} still holds for $l$-th iteration. Then at the $l+1$-th iteration, we have
\begin{align*}
    \tilde\wb_{\mathrm{gd}}^{(l+1)} &= \big(\Ib_d-\alpha N \xb_\perp\xb_\perp^\top\big)\cdot \tilde\wb_{\mathrm{gd}}^{(l)} - \alpha N \xb_{\perp}\\
    &= \big(\Ib_d-\alpha N \xb_\perp\xb_\perp^\top\big)\cdot \big(c\bbeta^*+a(l)\xb_\perp\big) - \alpha N \xb_{\perp}\\
    &= c\bbeta^* + \big(a(l)(1-\alpha N\|\xb_\perp\|_2^2)-\alpha N(1+c\la\xb_\perp, \bbeta^*\ra)\big)=c\bbeta^* + a(l+1)\xb_\perp.
\end{align*}
Additionally, by the fact $a(l+1) = a(l)(1-\alpha N\|\xb_\perp\|_2^2)-\alpha N(1+c\la\xb_\perp, \bbeta^*\ra)$, we can derive that 
\begin{align*}
    \bigg(a(l+1) + \frac{1+c\la\xb_\perp, \bbeta^*\ra}{\|\xb_\perp\|_2^2}\bigg) &= \big(1-\alpha N\|\xb_\perp\|_2^2\big)\bigg(a(l) + \frac{1+c\la\xb_\perp, \bbeta^*\ra}{\|\xb_\perp\|_2^2}\bigg)\\
    &= \cdots \\
    &= -\big(1-\alpha N\|\xb_\perp\|_2^2\big)^l\frac{1+c\la\xb_\perp, \bbeta^*\ra}{\|\xb_\perp\|_2^2}.
\end{align*}
This implies that 
\begin{align*}
        a(l) = -\Big(1-\big(1-\alpha N\|\xb_\perp\|_2^2\big)^l\Big)\frac{1+c\la\xb_\perp, \bbeta^*\ra}{\|\xb_\perp\|_2^2},
    \end{align*}
which completes the proof.
\end{proof}

Based on the closed form of $\tilde \wb_{\mathrm{gd}}^{(L)}$ obtained by Lemma~\ref{lemma:test_closed_form}, we are ready to prove Lemma~\ref{lemma:test_I} and Theorem~\ref{thm:test_II}.


\begin{proof}[Proof of Lemma~\ref{lemma:test_I}]
By Lemma~\ref{lemma:test_closed_form}, the output of the linear model trained via gradient descent on $\xb_{\mathrm{query}}$ can be expanded as
\begin{align}\label{eq:test_expansion}
    \la\tilde \wb_{\mathrm{gd}}^{(L)}, \xb_{\mathrm{query}}\ra &= \la c\bbeta^*+a(L)\xb_\perp, \xb_\perp+\sigma \wb^*\ra\notag\\
    &= c\la\bbeta^*, \xb_\perp\ra + a(L)\|\xb_\perp\|_2^2 +c\sigma\la\wb^*, \bbeta^*\ra\notag\\
    &=c\la\bbeta^*, \xb_\perp\ra - \Big(1-\big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\Big)\big(1+c\la\xb_\perp, \bbeta^*\ra\big)+c\sigma\la\wb^*, \bbeta^*\ra\notag\\
    &=c\sigma\la\wb^*, \bbeta^*\ra - 1 +  \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big(1+c\la\xb_\perp, \bbeta^*\ra\big).
\end{align}
By utilizing the independence among $\wb^*$, $\sigma$, and $\xb_\perp$ and law of total expectation, we can derive that 
\begin{align}
    \cE(\tilde\wb_{\mathrm{gd}}^{(L)})&= \PP\Big(c\sigma\la\wb^*, \bbeta^*\ra - 1 +  \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big(1+c\la\xb_\perp, \bbeta^*\ra\big)\leq 0 \Big) \notag\\
    &=\EE\Big[\PP\Big(c\sigma\la\wb^*, \bbeta^*\ra - 1 +  \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big(1+c\la\xb_\perp, \bbeta^*\ra\big)\leq 0 \Big|\wb^*, \xb_\perp\Big)\Big]\notag\\
    &=\EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big(1+c\la\xb_\perp, \bbeta^*\ra\big)}{c\la\wb^*, \bbeta^*\ra}\bigg)\Bigg],
\end{align}
where $F_\sigma(\cdot)$ is the cumulative distribution function of $\sigma$. Similarly, we also have 
\begin{align*}
    \cE(\wb_{\mathrm{gd}}^{(0)}) = \EE\Bigg[F_{\sigma}\bigg(-\frac{\la\xb_\perp, \bbeta^*\ra}{\la\wb^*, \bbeta^*\ra}\bigg)\Bigg].
\end{align*}
Therefore, by Taylor's first order expansion, we have 
\begin{align*}
    \cE(\tilde\wb_{\mathrm{gd}}^{(L)})-\cE(\wb_{\mathrm{gd}}^{(0)}) &=  \EE\Bigg[F_{\sigma}'(\bxi)\frac{\big(1-\big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big)\big(1+c\la\xb_\perp, \bbeta^*\ra\big)}{1+c\la\wb^*, \bbeta^*\ra}\Bigg]\\
    &\leq \Bigg(1-\Bigg(1-\Theta\bigg(\frac{Nd}{nL}\bigg)\Bigg)^L\Bigg)\tilde O(\sqrt{d})\leq \tilde O\bigg(\frac{Nd^{3/2}}{n}\bigg)\leq \tilde o(1),
\end{align*}
where the first inequality utilizing the concentration results that $\|\xb_\perp\|_2^2\Theta(d)$ and $\la\xb_\perp, \bbeta^*\ra=\tilde O(\sqrt{d})$. The second inequality holds by the fact $\Big(1-\Theta\big(\frac{Nd}{nL}\big)\Big)^L = 1-\Theta\big(\frac{Nd}{n}\big)$ by our condition $n \leq o(d^{3/2}/n)$, which also implies the last inequality holds. Therefore, we finish the proof.
\end{proof}

In the next, we prove Theorem~\ref{thm:test_II}
\begin{proof}[Proof of Theorem~\ref{thm:test_II}]
Similar to the proof of Lemma~\ref{lemma:test_I}, we have that 
\begin{align*}
\cE(\tilde\wb_{\mathrm{gd}}^{(L)})&=\EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L\big(1+c\la\xb_\perp, \bbeta^*\ra\big)}{c\la\wb^*, \bbeta^*\ra}\bigg)\Bigg]\\
&=\EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)-F'_\sigma(\bxi)\frac{\big(1-\alpha N\|\xb_\perp\|_2^2\big)^L|\la\xb_{\perp}, \bbeta^*\ra|\sign(\la\xb_{\perp}, \bbeta^*\ra)}{\la\wb^*, \bbeta^*\ra}\Bigg]\\
&= \EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\alpha N\|\xb_\perp\|_2^2\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)\Bigg] = \EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)\Bigg],
\end{align*}
where the third inequality holds as $\sign(\la\xb_{\perp}, \bbeta^*\ra)$ is independent with $|\la\xb_{\perp}, \bbeta^*\ra|$ and $\|\xb_\perp\|_2^2$, and $F'(\bxi)$ is a constant. Additionally, let $\sigma$ follows the uniform distribution from $a$ to $b$, then we can expand the expectation above as
\begin{align*}
    \cE(\tilde\wb_{\mathrm{gd}}^{(L)})= & \EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)\mathbbm{1}\Bigg\{\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\leq a\Bigg\}\Bigg]\\
    &+ \EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)\mathbbm{1}\Bigg\{a<\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\leq b\Bigg\}\Bigg]\\
    &+\EE\Bigg[F_{\sigma}\bigg(\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\bigg)\mathbbm{1}\Bigg\{\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}> b\Bigg\}\Bigg]\\
    =& \Bigg(1-\bigg(1-\tilde\Theta\bigg(\frac{Nd}{nL}\bigg)\bigg)^L\Bigg)\EE\Bigg[\frac{1}{c\la\wb^*, \bbeta^*\ra}\mathbbm{1}\Bigg\{a<\frac{1- \big(1-\tilde\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}\leq b\Bigg\}\Bigg] +\EE\Bigg[\mathbbm{1}\Bigg\{\frac{1- \big(1-\Theta(\frac{Nd}{nL})\big)^L}{c\la\wb^*, \bbeta^*\ra}> b\Bigg\}\Bigg]\\
    =&c_1 -c_2 \bigg(1-\tilde\Theta\bigg(\frac{Nd}{nL}\bigg)\bigg)^L
\end{align*}
where $c_1$, $c_2$ are two positive scalars solely depending on $a, b$ and the distribution of $\wb^*$. This completes the proof.
\end{proof}

\section{Technical lemmas}\label{sec:technical_lemmas}
% We demonstrate several lemmas providing the calculation results regarding the value of $B_{d, k}$, $ C_{d, k}$, and $D_{d, k}$.
In this section, we introduce and prove some technical lemmas utilized in the previous proof.
\begin{lemma}\label{lemma:expectation_I}
    Let $\xb\sim\cN(\mathbf{0}_d, \Ib_d)$, and $\wb_1, \wb\in \RR^{d}$ be two vectors independent of $\xb$, with $\|\wb_1\|_{2}=1$, then it holds that 
    \begin{align*}
        \EE_{\xb}[\la\wb, \xb\ra\sign(\la\wb_1, \xb\ra)]=\sqrt{\frac{2}{\pi}}\la\wb, \wb_1\ra.
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:expectation_I}]
Since $\|\wb_1\|_2=1$, let $\bGamma=[\wb_1, \wb_2, \ldots, \wb_d]\in \RR^d$ be the orthogonal matrix with $\wb_1$ being its first column. Then we have 
\begin{align*}
    \EE_{\xb}[\la\wb, \xb\ra\sign(\la\wb_1, \xb\ra)] &= \EE_{\xb}[\wb^\top\bGamma\bGamma^\top\xb\sign(\la\wb_1, \xb\ra)] \\
    &= \sum_{k=1}^d \la\wb, \wb_k\ra\EE_{\xb}[\la\wb_k, \xb\ra\sign(\la\wb_1, \xb\ra)] = \sqrt{\frac{2}{\pi}}\la\wb, \wb_1\ra,
\end{align*}
where the last equality holds since $\la\wb_k, \xb\ra\sim\cN(0, 1)$ for all $k\in[d]$,  $\la\wb_{k_1}, \xb\ra$ and $\la\wb_{k_2}, \xb\ra$ are independent when $k_1\neq k_2$, and $\EE[\la\wb_1, \xb\ra\sign(\la\wb_1, \xb\ra)] = \EE[|\la\wb_1, \xb\ra|]=\sqrt{\frac{2}{\pi}}$. This completes the proof.
\end{proof}

For the next lemmas, we follow the notation we used in previous section that $\hat\bSigma =\sum_{i=1}^n \xb_i\xb_i^\top$.
\begin{lemma}\label{lemma:expectation_II}
    For any $k\in \NN$, it holds that $\EE[\hat\bSigma^k] = c \Ib_d$, where $c$ is a scalar.
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:expectation_II}]
Let $\bGamma$ be any orthogonal matrix, then we have $\bGamma \xb_i\sim\cN(\mathbf{0}_d, \Ib_d)$. This implies that $\sum_{i=1}^n(\bGamma \xb_i) (\bGamma \xb_i)^\top$ has the same distribution with $\hat\bSigma$. Therefore, we can derive that 
\begin{align*}
    \bGamma\EE[\hat\bSigma^k]\bGamma = \EE\Big[\Big(\sum_{i=1}^n(\bGamma \xb_i) (\bGamma \xb_i)^\top\Big)^k\Big]= \EE[\hat\bSigma^k]
\end{align*}
holds for any orthogonal matrix $\bGamma$, which implies that $\EE[\hat\bSigma^k]$ must be at the form $c\Ib_d$. This completes the proof.
\end{proof}
Lemma~\ref{lemma:expectation_II} implies that $\EE[(\Ib_d-\hat\bSigma)^k] = c\Ib_d$ for some scalar $c$ as by binomial formula it can be expanded as a summation of polynomials of $\hat\bSigma$, which all have the expectations with the form $c\Ib_d$.

\begin{lemma}\label{lemma:expectation_III}
    For any $k\in \NN$, it holds that 
    \begin{align*}
        \EE\Big[\hat\bSigma^k\Big(\sum_{i=1}^n\xb_i y_i\Big)\Big] = c \bbeta^*,
    \end{align*}
    where $c$ is some scalar. 
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:expectation_III}]
By binomial theorem, we have 
\begin{align*}
    \EE\Big[\hat\bSigma^k\Big(\sum_{i=1}^n\xb_i y_i\Big)\Big] = \sum_{i=1}^n\sum_{k_1=0}^k {k \choose k_1}\EE\Big[\Big(\sum_{i'\neq i}\xb_{i'}\xb_{i'}^\top\Big)^{k-k_1}\Big] \EE[(\xb_i\xb_i^\top)^{k_1}\xb_iy_i].
\end{align*}
By Lemma~\ref{lemma:expectation_II}, we already obtain that $\EE\Big[\Big(\sum_{i'\neq i}\xb_{i'}\xb_{i'}^\top\Big)^{k-k_1}\Big] = c\Ib_d$ for some scalar $c$. In the next, it suffices to show that $\EE[(\xb_i\xb_i^\top)^{k_1}\xb_iy_i] = c\bbeta^*$ for some scalar $c$. Since $\|\wb^*\|_2=1$, let $\bGamma=[\wb^*, \wb_2, \ldots, \wb_d]\in \RR^d$ be the orthogonal matrix with $\wb^*$ being its first column, and let $\xb_i' = \bGamma^\top\xb_i\sim\cN(0,\Ib_d)$. This implies that $y_i =\sign(\la\wb^*, \xb_i\ra) = \sign(\xb_{i, 1}')$, which is the first coordinate of $\xb_{i}'$. Based on this, for any fixed $\wb^*$, we can further derive that
\begin{align*}
\EE[(\xb_i\xb_i^\top)^{k_1}\xb_iy_i|\wb^*] = \bGamma\EE[(\xb_i'\xb_i'^\top)^{k_1}\xb_i'\sign(\xb_{i, 1}')] = \bGamma\EE[\|\xb_i'\|_2^{2k_1}\xb_i'\sign(\xb_{i, 1}')] = c\wb^*.
\end{align*}
The last equality holds as $\|\xb_i'\|_2^{2k_1}$ is a even function for each coordinate of $\xb_i'$, which implies that $\EE[\|\xb_i'\|_2^{2k_1}\xb_{i, j}'\sign(\xb_{i, 1}')] = 0$ for any $j\in[d]$ and $j\neq 1$. Therefore, we can finally obtain that 
\begin{align*}
    \EE[(\xb_i\xb_i^\top)^{k_1}\xb_iy_i] = \EE\big[\EE[(\xb_i\xb_i^\top)^{k_1}\xb_iy_i|\wb^*]\big] = c\EE[\wb^*] = c\bbeta^*,
\end{align*}
which completes the proof.
\end{proof}

\begin{lemma}[Theorem 9 in \citet{bartlett2020benign}]\label{lemma:concentration_I}
    For any $\delta>0$, with probability at least $1-\delta$, it holds that,
    \begin{align*}
        \bigg\|\frac{1}{n}\hat\bSigma-\Ib_d \bigg\|_2\leq O\Bigg(\max\bigg\{\frac{d}{n}, \sqrt{\frac{d}{n}}, \frac{\log(1/\delta)}{n}, \sqrt{\frac{\log(1/\delta)}{n}}\bigg\}\Bigg).
    \end{align*}
\end{lemma}
%\citet{bartlett2020benign}

\begin{lemma}\label{lemma:concentration_II}
    For any $\delta>0$, with probability at least $1-\delta$, it holds that,
    \begin{align*}
       \bigg\|\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2\leq O\big(\sqrt{nd\log(d/\delta)}\big).
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:concentration_II}]
Similar to the previous proof technique, let $\bGamma=[\wb^*, \wb_2, \ldots, \wb_d]\in \RR^d$ be the orthogonal matrix with $\wb^*$ being its first column, and let $\xb_i' = \bGamma^\top\xb_i\sim\cN(0,\Ib_d)$. Then we can derive that 
\begin{align*}
    \sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^* = \bigg[\sum_{i=1}^n \bigg(|\xb_{i, 1}'| - \sqrt{\frac{2}{\pi}}\bigg)\bigg]\cdot \wb^* +\sum_{j=2}^d\bigg[\sum_{i=1}^n \sign(\xb_{i, 1}')\xb_{i, j}'\bigg]\cdot\wb_j.
\end{align*}
Since $|\xb_{i, 1}'|$ is a subgaussian random variable with expectation $\sqrt{\frac{2}{\pi}}$, by Hoeffding's inequality we can derive that with probability at least $1-\delta/d$,
\begin{align*}
    \sum_{i=1}^n \bigg(|\xb_{i, 1}'| - \sqrt{\frac{2}{\pi}}\bigg)\leq O\big(\sqrt{n\log(d/\delta)}\big).
\end{align*}
Additionally, when $j\neq 1$, $\sign(\xb_{i, 1}')\xb_{i, j}'$ still follows a standard normal distribution (A standard normal random variable times an independent Rademacher random variable is still a standard normal random). Therefore, we can also derive that
\begin{align*}
    \sum_{i=1}^n \sign(\xb_{i, 1}')\xb_{i, j}'\leq O\big(\sqrt{n\log(d/\delta)}\big)
\end{align*}
holds with probability at least $1-\frac{\delta}{d}$. Then by taking an union bound, we can finally obtain that 
\begin{align*}
    \bigg\|\sum_{i=1}^n \sign(\la\wb^*, \xb_i\ra)\xb_i- n\sqrt{\frac{2}{\pi}}\wb^*\bigg\|_2^2 = \bigg[\sum_{i=1}^n \bigg(|\xb_{i, 1}'| - \sqrt{\frac{2}{\pi}}\bigg)\bigg]^2+\sum_{j=2}^d\bigg[\sum_{i=1}^n \sign(\xb_{i, 1}')\xb_{i, j}'\bigg]^2\leq O\big(nd\log(d/\delta)\big).
\end{align*}
The first equality holds by the orthogonality among $\wb^*, \wb_2, \ldots, \wb^*$. This completes the proof.
\end{proof}
%\citet{bartlett2020benign}

\begin{lemma}\label{lemma:concentration_III}
    For any $\delta>0$, with probability at least $1-\delta$, it holds that,
    \begin{align*}
       &\big|\|\xb_{\perp}\|_2^2-(d-1)\big|\leq O\big(\sqrt{d\log(1/\delta)}\big); \\ &\big|\la\xb_{\perp}, \bbeta^*\ra\big| \leq O\big(\sqrt{d\log(1/\delta)}\big).
    \end{align*}
\end{lemma}
\begin{proof}[Proof of Lemma~\ref{lemma:concentration_III}]
By the fact that $\|\xb_{\perp}\|_2^2\sim\chi_{d-1}^2$, we have $\EE[\|\xb_{\perp}\|_2^2]=d-1$. Then by the Bernstein's inequality, we can obtain that 
\begin{align*}
    \big|\|\xb_{\perp}\|_2^2-(d-1)\big|\leq O\big(\sqrt{d\log(1/\delta)}\big)
\end{align*}
holds with probability at least $1-\delta/2$. Besides, since $\la\xb_\perp, \bbeta^*\ra\sim \cN(0, 1-\la\wb^*, \bbeta^*\ra)$, by applying the tail bounds of Gaussian distribution, we can obtain that
\begin{align*}
    \big|\la\xb_{\perp}, \bbeta^*\ra\big| \leq O\big(\sqrt{d\log(1/\delta)}\big)
\end{align*}
holds with probability at least $1-\delta/2$. By applying a union bound, we obtain the final result.
\end{proof}

\section{Experimental setup}
\subsection{Context hijacking in LLMs}\label{appen:chinLLMs}
This section will describe our experimental setup for context hijacking on LLMs of different depths. We first construct four datasets for different tasks, including language, country, sports, and city. The samples in each dataset consist of four parts: prepend, error result, query, and correct result. Each task has a fixed template for the sample. For the language, the template is ``\texttt{\{People\} did not speak \{error result\}. The native language of \{People\} is \{correct result\}}''. For the country, the template is ``\texttt{\{People\} does not live in \{error result\}. \{People\} has a citizenship from \{correct result\}}''. For the sports, the template is ``\texttt{\{People\} is not good at playing \{error result\}. \{People\}'s best sport is \{correct result\}}''. For the city, the template is ``\texttt{\{Landmarks\} is not in \{error result\}. \{Landmarks\} is in the city of \{correct result\}}''. We allow samples to have certain deviations from the templates, but they must generally conform to the semantics of the templates. Instance always match the reality, and the main source of instances is the
\textit{CounterFact} dataset \cite{meng2022locating}. In our dataset, each task contains three hundred to seven hundred specific instances. We conduct experiments on GPT2 ~\cite{radford2019language} of different sizes. Specifically, we consider GPT2, GPT2-MEDIUM, GPT2-LARGE, and GPT2-XL. They have 12 layers, 24 layers, 36 layers, and 48 layers, respectively. We construct a pipeline that test each model on each task, recording the number of prepends for which the context just succeeded in perturbing the output. For those samples that fail to perturb within a certain number of prepends (which is determined by the maximum length of the pre-trained model), we exclude them from the statistics. Finally, we verify the relationship between model depth and robustness by averaging the number of prepends required to successfully perturb the output.
\subsection{Numerical experiments}
We use extensive numerical experiments to verify our theoretical results, including gradient descent and linear transformers.

\textbf{Gradient descent:} We use a single-layer neural network as the gradient descent model, which contains only one linear hidden layer. Its input dimension is the dimension $d$ of feature $\xb$, and we mainly experiment on $d=\{15,20,25\}$. Its output dimension is $1$, because we only need to judge the classification result by its sign. We use the mean square error as the loss function and \textit{SGD} as the optimizer. All data comes from the defined training distribution $\cD_{\mathrm{tr}}$. The hyperparameters we set include training context length $N=50$, mean of the Gaussian distribution $\bbeta^\star=1$, variance of the Gaussian distribution $\bSigma=0.1$ (then normalized). We initialize the neural network to $c\bbeta$, and then perform gradient descents with steps $Steps=\{1,2,...,8\}$ and learning rate $lr$. We use grid search to search for the optimal $c$ and $lr$ for the loss function. This is equivalent to the trained transformers of layers 1 to 8 learning to obtain the shared signal $c\bbeta$ and the optimal learning rate $lr$ for the corresponding number of layers. Then they can use in-context data to fine-tune $c\bbeta$ to a specific $\wb^\star$.

After obtaining the optimal initialization and learning rate, we test it on the dataset from $\cD_{\mathrm{te}}$. Again, we set exactly the same hyperparameters as above. In addition, we set the $\sigma$ in the test distribution to 0.1.

\textbf{Linear Transformer:} We train on multi-layer single-head linear transformers and use \textit{Adam} as the optimizer. The training settings for models with different numbers of layers are exactly the same. We use the initial learning rate $lr\in \{0.0001,0.0002\}$, and the training steps are 600,000. We use a learning rate decay mechanism, where the learning rate is decayed by half every 50,000 training steps. For training and testing data, we set the data dimension $d=20$ and the training context length $N=\{10,20,30,40\}$. We use a batchsize of 5,000 and apply gradient clipping with a threshold of 1.

\section{Additional experiments}
\subsection{Robustness of standard transformers with different number of layers}
\begin{figure}
    \centering
    \includegraphics[width=0.75\linewidth]{StandardTF.png}
    \vskip -0.in
    \caption{Standard transformers experiments with different depths. Testing the trained standard transformers (GPT-2 architecture ~\cite{radford2019language}) on the test set, as the number of interference samples increases, the model classification accuracy decreases and gradually converges. The results also show that deeper models are more robust.}
    \label{fig:standardtf}
    \vskip -0.2in
\end{figure}
To generalize the results to more realistic settings, we transfer the experiments from linear transformers to larger and standard transformers, such as GPT-2~\cite{radford2019language}. We train and test GPT-2 with different numbers of layers based on exactly the same settings as the linear transformers experiments. The results once again verify our theory (Figure \ref{fig:standardtf}). As the context length increases, the models accuracy decreases, but increasing the number of layers of the model significantly improves the robustness, indicating that our theory has more practical significance. Then we describe the setup of standard transformers experiments briefly.

\textbf{Setup:} We use the standard transformers of the GPT-2 architecture for the experiments, and the main settings are similar to ~\cite{garg2022can}. We set the embedding size to 256, the number of heads to 8, and the batch size to 64. We use a learning rate decay mechanism similar to linear transformers experiments, with an initial learning rate of 0.0002, and then reduced by half every 200,000 steps, for a total of 600,000 steps. We use \textit{Adam} as the optimizer.

\subsection{Linear transformers facing different interference intensity}
In this section, we mainly discuss how the robustness of the model changes with the interference intensity. In our modeling, the interference intensity is determined only by the distance between the query sample and the similar interference samples defined in the test set, that is, by the variable $\sigma$ in $\cD_{\mathrm{te}}$. In real-world observations, according to the idea of the induction head~\cite{olsson2022context}, the more similar the context prepend used for interference is to the query, the more likely the model is to use in-context learning to output incorrect results. Therefore, we examine different $\sigma$ to determine whether the model conforms to the actual real-world interference situation, that is, to verify the rationality of our modeling.

\begin{figure*}
    \centering
    \includegraphics[width=0.8\linewidth]{sigma_08_01.png}
    \vskip -0.in
    \caption{Linear transformers experiments with different depths and different $\sigma$. In real-world semantics, smaller $\sigma$ means stronger interference. Comparing the test performance of the model under different $\sigma$, we can find that as $\sigma$ decreases, the robustness of the model decreases significantly, which verifies the rationality of our modeling.}
    \label{fig:lineartf_sigma}
    \vskip -0.1in
\end{figure*}

Observing the experiment results in Figure~\ref{fig:lineartf_sigma}, when $\sigma$ gradually decreases from 0.8 to 0.1, that is, the interference intensity of the data gradually increases, the classification accuracy of the model decreases significantly. When $\sigma$ is larger and the interference context is less, the model can always classify accurately, indicating that weak interference does not affect the performance of the model, which is consistent with real observations. Various experimental phenomena show that our modeling of the context hijacking task by the distance between the interference sample and the query sample is consistent with the real semantics.

%In addition, we also investigate another more intense interference case, that is, the context is the same but the label is flipped. In this setting, the hijacking context sample $\xb_{\mathrm{hc}}$ is exactly the same as the query $\xb_{\mathrm{query}}$, but has the opposite label $y_{\mathrm{hc}}=-y_{\mathrm{query}}$, which simulates the situation of logically incorrect hijacking prepends. Unlike context hijacking, this is a typical malicious adversarial attack method. We study the change in accuracy under different $\sigma$ settings again. Figure~\ref{fig:lineartf_sigma_test2} shows that under the same $\sigma$ and $N$, using logically incorrect samples will have lower accuracy than correct samples, which is aligned with real observations.

%\begin{figure*}
    %\centering
    %\includegraphics[width=0.8\linewidth]{test2_sigma_08_01.png}
    %\vskip -0.in
    %\caption{Linear transformers experiments with different depths and different $\sigma$, under the setting of logically incorrect prepends. The accuracy changes with the changes of $N$ and $\sigma$, and the pattern is the same as that of context hijacking. However, compared with context hijacking, using logically incorrect samples will significantly reduce the accuracy.}
    %\label{fig:lineartf_sigma_test2}
    %\vskip -0.1in
%\end{figure*}

\end{document}

\end{document}




% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 and 2024 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.
