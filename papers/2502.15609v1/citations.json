[
  {
    "index": 0,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 1,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      },
      {
        "key": "chen2022meta",
        "author": "Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He",
        "title": "Meta-learning via Language Model In-context Tuning"
      },
      {
        "key": "min2022metaicl",
        "author": "Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh",
        "title": "MetaICL: Learning to Learn In Context"
      },
      {
        "key": "liu2023pre",
        "author": "Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham",
        "title": "Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing"
      },
      {
        "key": "xieexplanation",
        "author": "Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu",
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference"
      }
    ]
  },
  {
    "index": 2,
    "papers": [
      {
        "key": "bai2024transformers",
        "author": "Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song",
        "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection"
      },
      {
        "key": "guotransformers",
        "author": "Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu",
        "title": "How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations"
      },
      {
        "key": "lintransformers",
        "author": "Lin, Licong and Bai, Yu and Mei, Song",
        "title": "Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining"
      },
      {
        "key": "chen2024transformers",
        "author": "Chen, Xingwu and Zhao, Lei and Zou, Difan",
        "title": "How transformers utilize multi-head attention in in-context learning? a case study on sparse linear regression"
      },
      {
        "key": "frei2024trained",
        "author": "Frei, Spencer and Vardi, Gal",
        "title": "Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context"
      },
      {
        "key": "huangcontext",
        "author": "Huang, Yu and Cheng, Yuan and Liang, Yingbin",
        "title": "In-context Convergence of Transformers"
      },
      {
        "key": "siyu2024training",
        "author": "Siyu, Chen and Heejune, Sheen and Tianhao, Wang and Zhuoran, Yang",
        "title": "Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality"
      },
      {
        "key": "lifine",
        "author": "Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet",
        "title": "Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond"
      }
    ]
  },
  {
    "index": 3,
    "papers": [
      {
        "key": "garg2022can",
        "author": "Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory",
        "title": "What can transformers learn in-context? a case study of simple function classes"
      },
      {
        "key": "akyurek2022learning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? Investigations with linear models"
      },
      {
        "key": "li2023transformersa",
        "author": "Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet",
        "title": "Transformers as algorithms: Generalization and stability in in-context learning"
      },
      {
        "key": "raventos2024pretraining",
        "author": "Ravent{\\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya",
        "title": "Pretraining task diversity and the emergence of non-bayesian in-context learning for regression"
      },
      {
        "key": "pathaktransformers",
        "author": "Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu",
        "title": "Transformers can optimally learn regression mixture models"
      },
      {
        "key": "panwarcontext",
        "author": "Panwar, Madhur and Ahuja, Kabir and Goyal, Navin",
        "title": "In-Context Learning through the Bayesian Prism"
      },
      {
        "key": "bhattamishraunderstanding",
        "author": "Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun",
        "title": "Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions"
      },
      {
        "key": "fu2023does",
        "author": "Fu, Jingwen and Yang, Tao and Wang, Yuwang and Lu, Yan and Zheng, Nanning",
        "title": "How does representation impact in-context learning: A exploration on a synthetic task"
      },
      {
        "key": "lee2024supervised",
        "author": "Lee, Jonathan and Xie, Annie and Pacchiano, Aldo and Chandak, Yash and Finn, Chelsea and Nachum, Ofir and Brunskill, Emma",
        "title": "Supervised pretraining can learn in-context reinforcement learning"
      }
    ]
  },
  {
    "index": 4,
    "papers": [
      {
        "key": "brown2020language",
        "author": "Brown, Tom B",
        "title": "Language models are few-shot learners"
      }
    ]
  },
  {
    "index": 5,
    "papers": [
      {
        "key": "chen2024transformers",
        "author": "Chen, Xingwu and Zhao, Lei and Zou, Difan",
        "title": "How transformers utilize multi-head attention in in-context learning? a case study on sparse linear regression"
      }
    ]
  },
  {
    "index": 6,
    "papers": [
      {
        "key": "frei2024trained",
        "author": "Frei, Spencer and Vardi, Gal",
        "title": "Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context"
      }
    ]
  },
  {
    "index": 7,
    "papers": [
      {
        "key": "friedman2024learning",
        "author": "Friedman, Dan and Wettig, Alexander and Chen, Danqi",
        "title": "Learning transformer programs"
      },
      {
        "key": "yuntransformers",
        "author": "Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv",
        "title": "Are Transformers universal approximators of sequence-to-sequence functions?"
      },
      {
        "key": "dehghaniuniversal",
        "author": "Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz",
        "title": "Universal Transformers"
      },
      {
        "key": "lindner2024tracr",
        "author": "Lindner, David and Kram{\\'a}r, J{\\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir",
        "title": "Tracr: Compiled transformers as a laboratory for interpretability"
      },
      {
        "key": "pandit2021probing",
        "author": "Pandit, Onkar and Hou, Yufang",
        "title": "Probing for Bridging Inference in Transformer Language Models"
      },
      {
        "key": "perez2021attention",
        "author": "P{\\'e}rez, Jorge and Barcel{\\'o}, Pablo and Marinkovic, Javier",
        "title": "Attention is turing-complete"
      },
      {
        "key": "bills2023language",
        "author": "Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William",
        "title": "Language models can explain neurons in language models"
      },
      {
        "key": "wei2022statistically",
        "author": "Wei, Colin and Chen, Yining and Ma, Tengyu",
        "title": "Statistically meaningful approximation: a case study on approximating turing machines with transformers"
      },
      {
        "key": "weiss2021thinking",
        "author": "Weiss, Gail and Goldberg, Yoav and Yahav, Eran",
        "title": "Thinking like transformers"
      },
      {
        "key": "zhoualgorithms",
        "author": "Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Joshua M and Bengio, Samy and Nakkiran, Preetum",
        "title": "What Algorithms can Transformers Learn? A Study in Length Generalization"
      },
      {
        "key": "chen2024can",
        "author": "Chen, Xingwu and Zou, Difan",
        "title": "What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks"
      }
    ]
  },
  {
    "index": 8,
    "papers": [
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      },
      {
        "key": "ahn2023transformers",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      },
      {
        "key": "zhang2024context",
        "author": "Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L",
        "title": "In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization"
      },
      {
        "key": "bai2024transformers",
        "author": "Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song",
        "title": "Transformers as statisticians: Provable in-context learning with in-context algorithm selection"
      },
      {
        "key": "wumany",
        "author": "Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter",
        "title": "How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?"
      },
      {
        "key": "chengtransformers",
        "author": "Cheng, Xiang and Chen, Yuxin and Sra, Suvrit",
        "title": "Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context"
      },
      {
        "key": "akyureklearning",
        "author": "Aky{\\\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny",
        "title": "What learning algorithm is in-context learning? Investigations with linear models"
      },
      {
        "key": "dai2023can",
        "author": "Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu",
        "title": "Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers"
      },
      {
        "key": "zhang2024trained",
        "author": "Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L",
        "title": "Trained transformers learn linear models in-context"
      }
    ]
  },
  {
    "index": 9,
    "papers": [
      {
        "key": "von2023transformers",
        "author": "Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max",
        "title": "Transformers learn in-context by gradient descent"
      }
    ]
  },
  {
    "index": 10,
    "papers": [
      {
        "key": "ahn2023transformers",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      }
    ]
  },
  {
    "index": 11,
    "papers": [
      {
        "key": "ahn2023transformers",
        "author": "Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit",
        "title": "Transformers learn to implement preconditioned gradient descent for in-context learning"
      }
    ]
  },
  {
    "index": 12,
    "papers": [
      {
        "key": "zhang2024context",
        "author": "Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L",
        "title": "In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization"
      }
    ]
  },
  {
    "index": 13,
    "papers": [
      {
        "key": "yao2024survey",
        "author": "Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue",
        "title": "A survey on large language model (llm) security and privacy: The good, the bad, and the ugly"
      },
      {
        "key": "liu2023prompt",
        "author": "Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others",
        "title": "Prompt Injection attack against LLM-integrated Applications"
      },
      {
        "key": "perezignore",
        "author": "Perez, F{\\'a}bio and Ribeiro, Ian",
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models"
      },
      {
        "key": "zou2023universal",
        "author": "Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt",
        "title": "Universal and transferable adversarial attacks on aligned language models"
      },
      {
        "key": "apruzzese2023real",
        "author": "Apruzzese, Giovanni and Anderson, Hyrum S and Dambra, Savino and Freeman, David and Pierazzi, Fabio and Roundy, Kevin",
        "title": "\u201creal attackers don't compute gradients\u201d: bridging the gap between adversarial ml research and practice"
      }
    ]
  },
  {
    "index": 14,
    "papers": [
      {
        "key": "chowdhury2024breaking",
        "author": "Chowdhury, Arijit Ghosh and Islam, Md Mofijul and Kumar, Vaibhav and Shezan, Faysal Hossain and Jain, Vinija and Chadha, Aman",
        "title": "Breaking down the defenses: A comparative survey of attacks on large language models"
      }
    ]
  },
  {
    "index": 15,
    "papers": [
      {
        "key": "kumar2023certifying",
        "author": "Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu",
        "title": "Certifying llm safety against adversarial prompting"
      },
      {
        "key": "wei2023jailbreak",
        "author": "Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen",
        "title": "Jailbreak and guard aligned language models with only few in-context demonstrations"
      },
      {
        "key": "xullm",
        "author": "Xu, Xilie and Kong, Keyi and Liu, Ning and Cui, Lizhen and Wang, Di and Zhang, Jingfeng and Kankanhalli, Mohan",
        "title": "An LLM can Fool Itself: A Prompt-Based Adversarial Attack"
      },
      {
        "key": "wang2023adversarial",
        "author": "Wang, Jiongxiao and Liu, Zichen and Park, Keun Hee and Jiang, Zhuojun and Zheng, Zhaoheng and Wu, Zhuofeng and Chen, Muhao and Xiao, Chaowei",
        "title": "Adversarial demonstration attacks on large language models"
      },
      {
        "key": "zhu2023promptbench",
        "author": "Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Zhenqiang Gong, Neil and others",
        "title": "Promptbench: Towards evaluating the robustness of large language models on adversarial prompts"
      },
      {
        "key": "cheng2024leveraging",
        "author": "Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G",
        "title": "Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks"
      },
      {
        "key": "wang2023robustness",
        "author": "Wang, Jindong and Xixu, HU and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Ye, Wei and Huang, Haojun and Geng, Xiubo and others",
        "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective"
      }
    ]
  },
  {
    "index": 16,
    "papers": [
      {
        "key": "qiang2023hijacking",
        "author": "Qiang, Yao and Zhou, Xiangyu and Zhu, Dongxiao",
        "title": "Hijacking large language models via adversarial in-context learning"
      },
      {
        "key": "baileyimage",
        "author": "Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott",
        "title": "Image Hijacks: Adversarial Images can Control Generative Models at Runtime"
      },
      {
        "key": "he2024data",
        "author": "He, Pengfei and Xu, Han and Xing, Yue and Liu, Hui and Yamada, Makoto and Tang, Jiliang",
        "title": "Data Poisoning for In-context Learning"
      },
      {
        "key": "anwar2024adversarial",
        "author": "Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer",
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression"
      },
      {
        "key": "jiangllms",
        "author": "Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep Kumar and Aragam, Bryon",
        "title": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers"
      }
    ]
  },
  {
    "index": 17,
    "papers": [
      {
        "key": "qiang2023hijacking",
        "author": "Qiang, Yao and Zhou, Xiangyu and Zhu, Dongxiao",
        "title": "Hijacking large language models via adversarial in-context learning"
      }
    ]
  },
  {
    "index": 18,
    "papers": [
      {
        "key": "anwar2024adversarial",
        "author": "Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer",
        "title": "Adversarial Robustness of In-Context Learning in Transformers for Linear Regression"
      }
    ]
  },
  {
    "index": 19,
    "papers": [
      {
        "key": "jiangllms",
        "author": "Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep Kumar and Aragam, Bryon",
        "title": "Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers"
      }
    ]
  }
]