\section{Related works}
\textbf{In-context learning via transformers.}
%The powerful performance of large language models based on transformers may come from the in-context learning (ICL) ability of transformer~\cite{brown2020language}. In-context learning refers to the model's ability to flexibly adjust predictions based on additional data given in context contained in the input sequence itself, without updating parameters.
The powerful performance of transformers is generally believed to come from its in-context learning ability~\cite{brown2020language,chen2022meta,min2022metaicl,liu2023pre,xieexplanation}. A line of recent works study the phenomenon of in-context learning from both theoretical ~\cite{bai2024transformers,guotransformers,lintransformers,chen2024transformers, frei2024trained,huangcontext,siyu2024training,lifine} and empirical ~\cite{garg2022can,akyurek2022learning,li2023transformersa,raventos2024pretraining,pathaktransformers,panwarcontext,bhattamishraunderstanding,fu2023does,lee2024supervised} perspectives on diverse settings. ~\citet{brown2020language} first showed that GPT-3 can perform in-context learning. 
% Subsequently, numerous studies have relied on diverse settings to investigate the in-context learning ability of transformers. 
~\citet{chen2024transformers} studied the role of different heads within transformers in performing in-context learning focusing on the sparse linear regression setting. ~\citet{frei2024trained} %first 
studied the ability of one-layer linear transformers to perform in-context learning for %random 
linear classification tasks. 
% These works build the foundation of the theoretical analysis for transformer models, which mainly focus on the standard training and test in-context learning behaviors, i.e., the distribution of the context examples are the same in both training and test phases. 


%\CC{which is related to our work. In this work, we maintain the linear classification setting which fits well with our modeling to study the robustness of transformers.}

\textbf{Mechanism interpretability of transformers.}
%To theoretically examine the expressive power of transformers
Among the various theoretical interpretations of transformers~\cite{friedman2024learning,yuntransformers,dehghaniuniversal,lindner2024tracr, pandit2021probing,perez2021attention, bills2023language,wei2022statistically,weiss2021thinking,zhoualgorithms, chen2024can}, one of the most widely studied theories is the ability of transformers to implement optimization algorithms such as gradient descent~\cite{von2023transformers,ahn2023transformers,zhang2024context,bai2024transformers,wumany,chengtransformers,akyureklearning,dai2023can,zhang2024trained}.
%Oswald et al.
~\citet{von2023transformers} theoretically and empirically proved that transformers can learn in-context by implementing a single step of gradient descent per layer. %Ahn et al. ~\cite{ahn2023transformers}\CC{ 
~\citet{ahn2023transformers} theoretically analyzed that transformers can learn to implement preconditioned gradient descent for in-context learning.%Zhang et al 
~\citet{zhang2024context} considered ICL in the setting of linear regression with a non-zero mean Gaussian prior, a more general and common scenario where different tasks share a signal, which is highly relevant to our work.

\textbf{Robustness of transformers.}
The security issues of large language models have always attracted a great deal of attention~\cite{yao2024survey,liu2023prompt,perezignore,zou2023universal,apruzzese2023real}. However, most of the research focuses on jail-breaking black-box models~\cite{chowdhury2024breaking}, such as context-based adversarial attacks~\cite{kumar2023certifying,wei2023jailbreak,xullm,wang2023adversarial,zhu2023promptbench,cheng2024leveraging,wang2023robustness}. There is very little white-box interpretation work of attacks on the transformer, the foundation model of LLMs~\cite{qiang2023hijacking,baileyimage,he2024data,anwar2024adversarial,jiangllms}. 
%Qiang et al. 
~\citet{qiang2023hijacking} first considered attacking large language models during in-context learning, but they did not study the role of transformers in robustness. 
% ~\citet{anwar2024adversarial} studied the adversarial robustness of in-context learning in transformers based on linear regression problems, but they did not theoretically explain the internal mechanism of robustness. 
~\citet{jiangllms} proposed the phenomenon of context hijacking, which became the key motivation of our work. They analyzed this problem from the perspective of associative memory models instead of the in-context learning ability of transformers.%In contrast, our paper formally models the context hijacking phenomenon from the perspective of in-context learning, which has received much attention in transformers and LLMs research, and analyzes the robustness of transformers theoretically and empirically.