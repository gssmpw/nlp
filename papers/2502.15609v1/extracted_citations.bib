@article{ahn2023transformers,
  title={Transformers learn to implement preconditioned gradient descent for in-context learning},
  author={Ahn, Kwangjun and Cheng, Xiang and Daneshmand, Hadi and Sra, Suvrit},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={45614--45650},
  year={2023}
}

@inproceedings{akyurek2022learning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
  year={2022}
}

@inproceedings{akyureklearning,
  title={What learning algorithm is in-context learning? Investigations with linear models},
  author={Aky{\"u}rek, Ekin and Schuurmans, Dale and Andreas, Jacob and Ma, Tengyu and Zhou, Denny},
  booktitle={The Eleventh International Conference on Learning Representations},
year={2022}
}

@article{anwar2024adversarial,
  title={Adversarial Robustness of In-Context Learning in Transformers for Linear Regression},
  author={Anwar, Usman and Von Oswald, Johannes and Kirsch, Louis and Krueger, David and Frei, Spencer},
  journal={arXiv preprint arXiv:2411.05189},
  year={2024}
}

@inproceedings{apruzzese2023real,
  title={“real attackers don't compute gradients”: bridging the gap between adversarial ml research and practice},
  author={Apruzzese, Giovanni and Anderson, Hyrum S and Dambra, Savino and Freeman, David and Pierazzi, Fabio and Roundy, Kevin},
  booktitle={2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML)},
  pages={339--364},
  year={2023},
  organization={IEEE}
}

@article{bai2024transformers,
  title={Transformers as statisticians: Provable in-context learning with in-context algorithm selection},
  author={Bai, Yu and Chen, Fan and Wang, Huan and Xiong, Caiming and Mei, Song},
  journal={Advances in neural information processing systems},
  volume={36},
  year={2024}
}

@inproceedings{baileyimage,
  title={Image Hijacks: Adversarial Images can Control Generative Models at Runtime},
  author={Bailey, Luke and Ong, Euan and Russell, Stuart and Emmons, Scott},
  booktitle={Forty-first International Conference on Machine Learning},
year={2023}
}

@inproceedings{bhattamishraunderstanding,
  title={Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions},
  author={Bhattamishra, Satwik and Patel, Arkil and Blunsom, Phil and Kanade, Varun},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@article{bills2023language,
  title={Language models can explain neurons in language models},
  author={Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
  journal={URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023)},
  volume={2},
  year={2023}
}

@article{brown2020language,
  title={Language models are few-shot learners},
  author={Brown, Tom B},
  journal={arXiv preprint arXiv:2005.14165},
  year={2020}
}

@inproceedings{chen2022meta,
  title={Meta-learning via Language Model In-context Tuning},
  author={Chen, Yanda and Zhong, Ruiqi and Zha, Sheng and Karypis, George and He, He},
  booktitle={60th Annual Meeting of the Association for Computational Linguistics, ACL 2022},
  pages={719--730},
  year={2022},
  organization={Association for Computational Linguistics (ACL)}
}

@article{chen2024can,
  title={What Can Transformer Learn with Varying Depth? Case Studies on Sequence Learning Tasks},
  author={Chen, Xingwu and Zou, Difan},
  journal={arXiv preprint arXiv:2404.01601},
  year={2024}
}

@article{chen2024transformers,
  title={How transformers utilize multi-head attention in in-context learning? a case study on sparse linear regression},
  author={Chen, Xingwu and Zhao, Lei and Zou, Difan},
  journal={arXiv preprint arXiv:2408.04532},
  year={2024}
}

@article{cheng2024leveraging,
  title={Leveraging the Context through Multi-Round Interactions for Jailbreaking Attacks},
  author={Cheng, Yixin and Georgopoulos, Markos and Cevher, Volkan and Chrysos, Grigorios G},
  journal={arXiv preprint arXiv:2402.09177},
  year={2024}
}

@inproceedings{chengtransformers,
  title={Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context},
  author={Cheng, Xiang and Chen, Yuxin and Sra, Suvrit},
  booktitle={Forty-first International Conference on Machine Learning},
year={2023}
}

@article{chowdhury2024breaking,
  title={Breaking down the defenses: A comparative survey of attacks on large language models},
  author={Chowdhury, Arijit Ghosh and Islam, Md Mofijul and Kumar, Vaibhav and Shezan, Faysal Hossain and Jain, Vinija and Chadha, Aman},
  journal={arXiv preprint arXiv:2403.04786},
  year={2024}
}

@inproceedings{dai2023can,
  title={Why Can GPT Learn In-Context? Language Models Secretly Perform Gradient Descent as Meta-Optimizers},
  author={Dai, Damai and Sun, Yutao and Dong, Li and Hao, Yaru and Ma, Shuming and Sui, Zhifang and Wei, Furu},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={4005--4019},
  year={2023}
}

@inproceedings{dehghaniuniversal,
  title={Universal Transformers},
  author={Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  booktitle={International Conference on Learning Representations},
  year={2019}
}

@article{frei2024trained,
  title={Trained Transformer Classifiers Generalize and Exhibit Benign Overfitting In-Context},
  author={Frei, Spencer and Vardi, Gal},
  journal={arXiv preprint arXiv:2410.01774},
  year={2024}
}

@article{friedman2024learning,
  title={Learning transformer programs},
  author={Friedman, Dan and Wettig, Alexander and Chen, Danqi},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@article{fu2023does,
  title={How does representation impact in-context learning: A exploration on a synthetic task},
  author={Fu, Jingwen and Yang, Tao and Wang, Yuwang and Lu, Yan and Zheng, Nanning},
  journal={arXiv preprint arXiv:2309.06054},
  year={2023}
}

@article{garg2022can,
  title={What can transformers learn in-context? a case study of simple function classes},
  author={Garg, Shivam and Tsipras, Dimitris and Liang, Percy S and Valiant, Gregory},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={30583--30598},
  year={2022}
}

@inproceedings{guotransformers,
  title={How Do Transformers Learn In-Context Beyond Simple Functions? A Case Study on Learning with Representations},
  author={Guo, Tianyu and Hu, Wei and Mei, Song and Wang, Huan and Xiong, Caiming and Savarese, Silvio and Bai, Yu},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@article{he2024data,
  title={Data Poisoning for In-context Learning},
  author={He, Pengfei and Xu, Han and Xing, Yue and Liu, Hui and Yamada, Makoto and Tang, Jiliang},
  journal={arXiv preprint arXiv:2402.02160},
  year={2024}
}

@inproceedings{huangcontext,
  title={In-context Convergence of Transformers},
  author={Huang, Yu and Cheng, Yuan and Liang, Yingbin},
  booktitle={Forty-first International Conference on Machine Learning},
year={2023}
}

@inproceedings{jiangllms,
  title={Do LLMs dream of elephants (when told not to)? Latent concept association and associative memory in transformers},
  author={Jiang, Yibo and Rajendran, Goutham and Ravikumar, Pradeep Kumar and Aragam, Bryon},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@article{kumar2023certifying,
  title={Certifying llm safety against adversarial prompting},
  author={Kumar, Aounon and Agarwal, Chirag and Srinivas, Suraj and Li, Aaron Jiaxun and Feizi, Soheil and Lakkaraju, Himabindu},
  journal={arXiv preprint arXiv:2309.02705},
  year={2023}
}

@article{lee2024supervised,
  title={Supervised pretraining can learn in-context reinforcement learning},
  author={Lee, Jonathan and Xie, Annie and Pacchiano, Aldo and Chandak, Yash and Finn, Chelsea and Nachum, Ofir and Brunskill, Emma},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{li2023transformersa,
  title={Transformers as algorithms: Generalization and stability in in-context learning},
  author={Li, Yingcong and Ildiz, Muhammed Emrullah and Papailiopoulos, Dimitris and Oymak, Samet},
  booktitle={International Conference on Machine Learning},
  pages={19565--19594},
  year={2023a},
  organization={PMLR}
}

@inproceedings{lifine,
  title={Fine-grained Analysis of In-context Linear Estimation: Data, Architecture, and Beyond},
  author={Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024}
}

@article{lindner2024tracr,
  title={Tracr: Compiled transformers as a laboratory for interpretability},
  author={Lindner, David and Kram{\'a}r, J{\'a}nos and Farquhar, Sebastian and Rahtz, Matthew and McGrath, Tom and Mikulik, Vladimir},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{lintransformers,
  title={Transformers as Decision Makers: Provable In-Context Reinforcement Learning via Supervised Pretraining},
  author={Lin, Licong and Bai, Yu and Mei, Song},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@article{liu2023pre,
  title={Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing},
  author={Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
  journal={ACM Computing Surveys},
  volume={55},
  number={9},
  pages={1--35},
  year={2023},
  publisher={ACM New York, NY}
}

@article{liu2023prompt,
  title={Prompt Injection attack against LLM-integrated Applications},
  author={Liu, Yi and Deng, Gelei and Li, Yuekang and Wang, Kailong and Wang, Zihao and Wang, Xiaofeng and Zhang, Tianwei and Liu, Yepang and Wang, Haoyu and Zheng, Yan and others},
  journal={arXiv preprint arXiv:2306.05499},
  year={2023}
}

@inproceedings{min2022metaicl,
  title={MetaICL: Learning to Learn In Context},
  author={Min, Sewon and Lewis, Mike and Zettlemoyer, Luke and Hajishirzi, Hannaneh},
  booktitle={Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  pages={2791--2809},
  year={2022}
}

@inproceedings{pandit2021probing,
  title={Probing for Bridging Inference in Transformer Language Models},
  author={Pandit, Onkar and Hou, Yufang},
  booktitle={NAACL 2021-Annual Conference of the North American Chapter of the Association for Computational Linguistics},
  year={2021}
}

@inproceedings{panwarcontext,
  title={In-Context Learning through the Bayesian Prism},
  author={Panwar, Madhur and Ahuja, Kabir and Goyal, Navin},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@inproceedings{pathaktransformers,
  title={Transformers can optimally learn regression mixture models},
  author={Pathak, Reese and Sen, Rajat and Kong, Weihao and Das, Abhimanyu},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}

}

@article{perez2021attention,
  title={Attention is turing-complete},
  author={P{\'e}rez, Jorge and Barcel{\'o}, Pablo and Marinkovic, Javier},
  journal={Journal of Machine Learning Research},
  volume={22},
  number={75},
  pages={1--35},
  year={2021}
}

@inproceedings{perezignore,
  title={Ignore Previous Prompt: Attack Techniques For Language Models},
  author={Perez, F{\'a}bio and Ribeiro, Ian},
  booktitle={NeurIPS ML Safety Workshop},
year={2022}
}

@article{qiang2023hijacking,
  title={Hijacking large language models via adversarial in-context learning},
  author={Qiang, Yao and Zhou, Xiangyu and Zhu, Dongxiao},
  journal={arXiv preprint arXiv:2311.09948},
  year={2023}
}

@article{raventos2024pretraining,
  title={Pretraining task diversity and the emergence of non-bayesian in-context learning for regression},
  author={Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}

@inproceedings{siyu2024training,
  title={Training Dynamics of Multi-Head Softmax Attention for In-Context Learning: Emergence, Convergence, and Optimality},
  author={Siyu, Chen and Heejune, Sheen and Tianhao, Wang and Zhuoran, Yang},
  booktitle={The Thirty Seventh Annual Conference on Learning Theory},
  pages={4573--4573},
  year={2024},
  organization={PMLR}
}

@inproceedings{von2023transformers,
  title={Transformers learn in-context by gradient descent},
  author={Von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, Jo{\~a}o and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
  booktitle={International Conference on Machine Learning},
  pages={35151--35174},
  year={2023},
  organization={PMLR}
}

@article{wang2023adversarial,
  title={Adversarial demonstration attacks on large language models},
  author={Wang, Jiongxiao and Liu, Zichen and Park, Keun Hee and Jiang, Zhuojun and Zheng, Zhaoheng and Wu, Zhuofeng and Chen, Muhao and Xiao, Chaowei},
  journal={arXiv preprint arXiv:2305.14950},
  year={2023}
}

@inproceedings{wang2023robustness,
  title={On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective},
  author={Wang, Jindong and Xixu, HU and Hou, Wenxin and Chen, Hao and Zheng, Runkai and Wang, Yidong and Yang, Linyi and Ye, Wei and Huang, Haojun and Geng, Xiubo and others},
  booktitle={ICLR 2023 Workshop on Trustworthy and Reliable Large-Scale Machine Learning Models},
year={2023}
}

@article{wei2022statistically,
  title={Statistically meaningful approximation: a case study on approximating turing machines with transformers},
  author={Wei, Colin and Chen, Yining and Ma, Tengyu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={12071--12083},
  year={2022}
}

@article{wei2023jailbreak,
  title={Jailbreak and guard aligned language models with only few in-context demonstrations},
  author={Wei, Zeming and Wang, Yifei and Li, Ang and Mo, Yichuan and Wang, Yisen},
  journal={arXiv preprint arXiv:2310.06387},
  year={2023}
}

@inproceedings{weiss2021thinking,
  title={Thinking like transformers},
  author={Weiss, Gail and Goldberg, Yoav and Yahav, Eran},
  booktitle={International Conference on Machine Learning},
  pages={11080--11090},
  year={2021},
  organization={PMLR}
}

@inproceedings{wumany,
  title={How Many Pretraining Tasks Are Needed for In-Context Learning of Linear Regression?},
  author={Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Bartlett, Peter},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@inproceedings{xieexplanation,
  title={An Explanation of In-context Learning as Implicit Bayesian Inference},
  author={Xie, Sang Michael and Raghunathan, Aditi and Liang, Percy and Ma, Tengyu},
  booktitle={International Conference on Learning Representations},
year={2021}
}

@inproceedings{xullm,
  title={An LLM can Fool Itself: A Prompt-Based Adversarial Attack},
  author={Xu, Xilie and Kong, Keyi and Liu, Ning and Cui, Lizhen and Wang, Di and Zhang, Jingfeng and Kankanhalli, Mohan},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@article{yao2024survey,
  title={A survey on large language model (llm) security and privacy: The good, the bad, and the ugly},
  author={Yao, Yifan and Duan, Jinhao and Xu, Kaidi and Cai, Yuanfang and Sun, Zhibo and Zhang, Yue},
  journal={High-Confidence Computing},
  pages={100211},
  year={2024},
  publisher={Elsevier}
}

@inproceedings{yuntransformers,
  title={Are Transformers universal approximators of sequence-to-sequence functions?},
  author={Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  booktitle={International Conference on Learning Representations},
  year ={2020}
}

@article{zhang2024context,
  title={In-context learning of a linear Transformer block: benefits of the MLP component and one-step GD initialization},
  author={Zhang, Ruiqi and Wu, Jingfeng and Bartlett, Peter L},
  journal={arXiv preprint arXiv:2402.14951},
  year={2024}
}

@article{zhang2024trained,
  title={Trained transformers learn linear models in-context},
  author={Zhang, Ruiqi and Frei, Spencer and Bartlett, Peter L},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={49},
  pages={1--55},
  year={2024}
}

@inproceedings{zhoualgorithms,
  title={What Algorithms can Transformers Learn? A Study in Length Generalization},
  author={Zhou, Hattie and Bradley, Arwen and Littwin, Etai and Razin, Noam and Saremi, Omid and Susskind, Joshua M and Bengio, Samy and Nakkiran, Preetum},
  booktitle={The Twelfth International Conference on Learning Representations},
year={2023}
}

@article{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and Wang, Yidong and Yang, Linyi and Ye, Wei and Zhang, Yue and Zhenqiang Gong, Neil and others},
  journal={arXiv e-prints},
  pages={arXiv--2306},
  year={2023}
}

@article{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

