\section{Appendix}

\subsection{Hyperparameters for Training}
\label{appendix_hyper}

Here, we report the details of our training. 
The learning rate and batch size are listed in Table~\ref{hyperparameters}. 
Our experiments are running on 8 H100 and 8 A100.

\begin{table}[!ht]
\centering
\resizebox{0.48\textwidth}{!}{
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Hyperparameters}} & \multicolumn{2}{c}{\textbf{Llama-Base}} & \multicolumn{2}{c}{\textbf{Mistral-Base}} & \textbf{Llama-Inst} & \textbf{Mistral-Inst} \\
\cmidrule(lr){2-3} \cmidrule(lr){4-5} \cmidrule(lr){6-6} \cmidrule(lr){7-7}
& \textbf{SFT} & \textbf{DPO} & \textbf{SFT} & \textbf{DPO} & \textbf{DPO} & \textbf{DPO}\\
\midrule
Batch size       & 128  & 128   & 128  & 128  & 128  & 128\\
Epochs           & 1    & 1     & 1    & 1    & 1    & 1  \\
Learning rate    & 2e-5 & 5e-7  & 2e-5 & 3e-7 & 3e-7 & 3e-7\\
Beta             & -    & 0.01  & -    & 0.01 & 0.01 & 0.1\\
Warm-up ratio    & 0.1  & 0.01  & 0.1  & 0.01 & 0.01  & 0.01\\
\bottomrule
\end{tabular}
}
\caption{Hyperparameters for SFT and DPO training.}
\label{hyperparameters}
\end{table}




\subsection{Baseline Results}
\label{baseline}

For comparison, we report the results of SFT models and models trained with the conventional preference data construction strategy in Table~\ref{results_main_armorm}.
In base setting, the SFT models is trained on UltraChat, while it is the original instruct model in the instruct setting.
For the conventional preference data construction strategy, we sample five responses for each prompt.








\subsection{Win Rate Results}
\label{wr}

We report the win rate results of our policy models, which correspond to Section~\ref{data_cons} in Figure~\ref{main_fig_wr}. We find that our findings acquired in Section~\ref{data_cons} also hold in most cases.



\subsection{Extend Reward Points}
\label{appendix_extend}

We have tried data construction based on values $\left\{min, \mu \pm 2\sigma, \mu\pm\sigma, \mu, max\right\}$.
Here, we extend this set to further include values $\left\{\mu \pm 4\sigma, \mu \pm 3\sigma\right\}$. 
We experiment on Meta-Llama-3-8B-Instruct. 
We find that sample points on $\left\{\mu + 4\sigma, \mu + 3\sigma\right\}$ have no significant differences from $\left\{max\right\}$ when used to train models through DPO.
In addition, sample points on $\left\{\mu - 4\sigma, \mu - 3\sigma\right\}$ have no significant differences from $\left\{min\right\}$.
We report some of our results in Table~\ref{wide_scale}.




\subsection{400 Samples per Prompt}
\label{appendix_400}
In this part, we experiment with 400 samples per prompt to explore whether our results of 200 samples can still hold.
As we have emphasized, we are given sufficient sample budgets.
The SFT model is Meta-Llama-3-8B-Instruct.
We report the results in Table~\ref{results_main_armorm_400}.
To save some computation and evaluation costs, we only train and evaluate 11 models.
Our conclusions of constructed datasets are consistent between both scenarios, 200 samples and 400 samples per prompt, respectively.


\begin{table}[t]
\centering
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{l l c c }
\toprule
\multirow{2}{*}{\textbf{Chosen}} & \multirow{2}{*}{\textbf{Rejected}} & \multicolumn{2}{c}{\textbf{Llama-Inst}} \\
\cmidrule(lr){3-4} & & \textbf{LC} & \textbf{WR} \\
\midrule         
$max$          & $min$                              & 43.11      & 43.97         \\
\midrule
$max$          & $\mu-4\sigma$                      & 43.61      & 44.30         \\
$max$          & $\mu-3\sigma$                      & 44.10      & 43.78         \\
\midrule
$max$          & $\mu-2\sigma$                      & 48.12      & 49.66                \\
\midrule
$\mu+4\sigma$  & $\mu-2\sigma$                      & 48.11      & 49.46         \\
$\mu+3\sigma$  & $\mu-2\sigma$                      & 48.26      & 49.20         \\
\bottomrule
\end{tabular}
}
\caption{Evaluation results of extended data points.}
\label{wide_scale}
\vspace{-1em}
\end{table}


\begin{table}[!t]
\centering
\resizebox{0.35\textwidth}{!}{
\begin{tabular}{l l c c}
\toprule
\multirow{2}{*}{\textbf{Chosen}} & \multirow{2}{*}{\textbf{Rejected}} & \multicolumn{2}{c}{\textbf{Llama-Inst}} \\
\cmidrule(lr){3-4} & & \textbf{LC} & \textbf{WR} \\
\midrule         
${none}^{*}$    & ${none}^{*}$                               & 16.58      & 11.29         \\
$max\_of\_5$    & $min\_of\_5$                               & 45.23      & 47.86         \\
\midrule
$max$           & $min$                                      & 42.01      & 42.04         \\
\rowcolor[gray]{0.9}
$max$           & $\mu-2\sigma$                              & 49.09      & 50.23         \\
$max$           & $\mu-\sigma$                               & 48.27      & 49.76         \\
$max$           & $\mu$                                      & 48.21      & 48.76         \\
$max$           & $\mu+\sigma$                               & 42.32      & 38.35         \\
$max$           & $\mu+2\sigma$                              & 28.42      & 27.81         \\
$\mu+2\sigma$   & $min$                                      & 39.35      & 40.08         \\
\rowcolor[gray]{0.9}
$\mu+2\sigma$   & $\mu-2\sigma$                              & 50.71      & 51.37         \\
$\mu+2\sigma$   & $\mu-\sigma$                               & 48.41      & 49.56         \\ 
$\mu+2\sigma$   & $\mu$                                      & 46.79      & 45.39          \\
$\mu+2\sigma$   & $\mu+\sigma$                               & 35.15      & 32.57         \\

\bottomrule
\end{tabular}
}
\caption{400 Samples per prompt data construction results on Alpaca evaluation.}
\label{results_main_armorm_400}
\vspace{-1em}
\end{table}

\input{tabletex/base}
\input{figuretexs/ana_loss}
\input{figuretexs/reward}

% \input{figuretexs/bar_freq}





\begin{figure*}[!ht]
\centering

    \begin{minipage}[c]{\linewidth}
        \centering
        \includegraphics[width=0.48\linewidth]{figs/llama_base_wr.pdf}
        \hfill
        \includegraphics[width=0.48\linewidth]{figs/llama_instruct_wr.pdf}
        \\
        \includegraphics[width=0.48\linewidth]{figs/mistral_base_wr.pdf}
        \hfill
        \includegraphics[width=0.48\linewidth]{figs/llama_instruct_wr.pdf}
    \end{minipage}

\caption{Win rate results of Alpaca evaluation.}
\label{main_fig_wr}
\end{figure*}



\subsection{About Overfitting}
\label{overfitting}

We find that the data construction strategy, which selects the generation of maximal reward as the chosen response and selects the one of minimal reward as the rejected response among $n$ responses, may lead to overfitting of policy models when $n$ reaches a point.
\paragraph{Empirical Results.} As shown in Figure~\ref{ana_loss}, although the training loss can reach a lower bound when we increase $n$ from 5 to 400, the length-controlled win rate does not improve accordingly, 45.23, 46.88, 42.98, 42.01 for 5, 20, 60, 400 samples, respectively.
 
\paragraph{Theoretical Support.}
% The reward model assigns scores following an approximately normal distribution 
% \[
% r(y) \sim \mathcal{N}(\mu, \sigma^2).
% \]
% Given a large enough $n$, extreme values (i.e., $\max_{j=1}^{n} \{r(y_j)\}$  and $\min_{j=1}^{n} \{r(y_j)\}$) are likely to be statistical outliers rather than representative samples.
% Specifically, by extreme value theory, the expected maximum and minimum rewards for a large $n$ scale as:
% \[
% \mathbb{E}\left[ \max_{j=1}^{n} \{r(y_j)\} \right] \approx \mu + \sigma \sqrt{2 \log n}
% \]
% \[
% \mathbb{E} \left[ \min_{j=1}^{n} \{r(y_j)\} \right] \approx \mu - \sigma \sqrt{2 \log n}.
% \]
% Since the log-likelihood loss in DPO is sensitive to large differences in reward scores, extreme outliers distort the optimization process:
% \[
% \log \sigma \left(r(y_w) - r(y_l)\right) \approx \log \sigma \left(2\sigma \sqrt{2 \log n}\right).
% \]
% As $n$ increases, this term saturates, leading to diminished learning signals and potentially overfitting to statistical artifacts rather than generalizable improvements.

The reward model produces scores that are approximately normally distributed:
\[
r(y) \sim \mathcal{N}(\mu, \sigma^2).
\]
When evaluating a large number \( n \), the extreme values—namely, the maximum \( \max_{j=1}^{n} r(y_j) \) and the minimum \( \min_{j=1}^{n} r(y_j) \)—tend to be statistical outliers rather than typical examples from the distribution. According to extreme value theory, for large \( n \) the expected maximum and minimum rewards are approximately given by
\[
\mathbb{E}\left[ \max_{j=1}^{n} r(y_j) \right] \approx \mu + \sigma \sqrt{2 \log n},
\]
\[
\mathbb{E}\left[ \min_{j=1}^{n} r(y_j) \right] \approx \mu - \sigma \sqrt{2 \log n}.
\]

In the DPO framework, the log-likelihood loss is sensitive to the differences in reward scores. Specifically, consider the term
\[
\log \sigma \bigl(r(y_w) - r(y_l)\bigr),
\]
where \( r(y_w) \) and \( r(y_l) \) represent the rewards for the winning and losing samples, respectively. Using the approximations above, the difference between an extreme high and an extreme low reward scales roughly as
\[
r(y_w) - r(y_l) \approx 2\sigma \sqrt{2 \log n},
\]
so that
\[
\log \sigma \bigl(r(y_w) - r(y_l)\bigr) \approx \log \sigma \bigl(2\sigma \sqrt{2 \log n}\bigr).
\]

As \( n \) increases, this term becomes saturated, which diminishes the effective learning signal. In other words, the presence of extreme outliers can lead the optimization process to overfit to these statistical artifacts rather than capturing improvements that generalize well.


\subsection{Reward with More Samples}
\label{appendix_reward}


To support our view that the quality of top-ranking responses is getting better, we record the average reward score of three highest-ranking responses for 1,000 prompts in Ultrafeedback. 
As shown in Figure~\ref{reward}, the reward scores of both Armorm and Skywork reward models confirm our hypothesis that the quality of top-ranking responses improves as the number of samples increases.

\subsection{Evaluation on Academic Benchmarks}
\label{aca_bm}

\input{tabletex/academic_bench}

In Table~\ref{task_performance}, we evaluate our trained model based on Llama-3-8B-Instruct on a set of widely used academic tasks, including ARC~\cite{clark2018thinksolvedquestionanswering}, HellaSwag~\cite{zellers-etal-2019-hellaswag}, TruthfulQA~\cite{lin-etal-2022-truthfulqa} and GSM8K~\cite{cobbe2021trainingverifierssolvemath}.




% \begin{table}[t]
% \centering
% \resizebox{0.48\textwidth}{!}{
% \begin{tabular}{lcccccc}
% \toprule
% Position & $<\mu-2\sigma$ & $<\mu-\sigma \text{ and } >\mu-2\sigma$ & $<\mu \text{ and } >\mu-\sigma$ & $<\mu+\sigma \text{ and } >\mu$ & $< \mu+2\sigma \text{ and } >\mu+\sigma$ & $>\mu+2\sigma$ \\
% \midrule
% 1/4 & 0 & 0 & 280 (0.5\%) & 60237 (98.5\%) & 552 (0.9\%) & 66 (0.1\%) \\
% 1/2 & 0 & 0 & 10986 (17.9\%) & 50047 (81.9\%) & 0 & 102 (0.2\%) \\
% 3/4 & 0 & 551 (0.9\%) & 59458 (97.3\%) & 1060 (1.7\%) & 0 & 66 (0.1\%) \\
 
% \bottomrule
% \end{tabular}
% }
% \caption{We report the frequency of prompts whose $\frac{1}{4}$, $\frac{1}{2}$ and $\frac{3}{4}$ response reward point located in different intervals of the normal distribution.}
% \label{}
% \vspace{-1em}
% \end{table}








% \subsection{Evaluation on Academic Benchmarks}
% \label{appendix_aca}
% We evaluate our trained models of Meta-Llama-3-8B-Instruct on academic benchmarks. ARC~\cite{clark2018thinksolvedquestionanswering}, HellaSwag~\cite{zellers-etal-2019-hellaswag}, TruthfulQA~\cite{lin-etal-2022-truthfulqa} and GSM8K~\cite{cobbe2021trainingverifierssolvemath}. 
% We use Language Model Evaluation Harness~\cite{eval-harness} for evaluation.
% The results are recorded in Table~\ref{task_performance}.
% As we can see, our method (the last five rows) does not have negative effects on academic benchmarks.

% \begin{table}[!ht]
% \resizebox{0.48\textwidth}{!}{%
% \begin{tabular}{lcccccc}
% \toprule
% \textbf{Tasks} & \textbf{ARC\_challenge(5)} & \textbf{ARC\_easy(5)} & \textbf{HellaSwag(10)} & \textbf{TruthfulQA(0)} & \textbf{GSM(5)} \\
% \midrule
% Meta-Llama-3-8B-Instruct   & 57.25 & 85.14 & 58.71 & 35.99 & 75.06 \\
% \midrule
% 5sample-DPO       & 61.43 & 84.81 & 59.19 & 40.64 & 76.88 \\
% 20sample-DPO      & 61.52 & 84.64 & 58.90 & 39.78 & 75.15 \\
% 60sample-DPO      & 61.52 & 84.60 & 58.79 & 39.66 & 76.19 \\
% 100sample-DPO     & 61.26 & 84.64 & 59.02 & 39.41 & 76.19 \\
% 200sample-DPO     & 61.43 & 84.51 & 58.84 & 39.66 & 77.26 \\
% \bottomrule
% \end{tabular}%
% }
% \caption{Performance of trained models on academic benchmarks.}
% \label{task_performance}
% \end{table}

