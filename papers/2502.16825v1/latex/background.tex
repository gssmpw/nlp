\section{Background}

% \subsection{Reward Models}
% The reward model is a fundamental component of the conventional reinforcement learning from human feedback pipeline.
% Two responses of a prompt $x$, $y_w$ and $y_l$, represent the preferred and dispreferred completions, respectively. 
% It is assumed that these preferences are governed by a latent reward model $r^*(x, y)$, which is not directly accessible. 
% Among the many methods for modeling preferences, the Bradley-Terry (BT) framework is a popular choice. 
% This framework, along with more general approaches like the Plackett-Luce ranking model, can be extended to handle multiple answer ranking issues if needed. 
% The BT model defines the human preference probability $p^*$ as:
% \begin{align}
% p^*(y_w \succ y_l | x) &= \notag \\
% &\frac{\exp(r^*(x, y_w))}
% {\exp(r^*(x, y_w)) + \exp(r^*(x, y_l))} \nonumber
% \end{align}

% Given a dataset composed of pairs $D = \{(x^{(i)}, y_w^{(i)}, y_l^{(i)})\}_{i=1}^N$, a reward model $r_\phi(x, y)$ can be parameterized and trained using maximum likelihood. 
% Formulating this task as binary classification, the negative log-likelihood loss is given by:
% \begin{align}
% & \mathcal{L}_R(r_\phi, D) = \notag \\
% & -\mathbb{E}_{(x, y_w, y_l) \sim D} \left[ \log \sigma(r_\phi(x, y_w) - r_\phi(x, y_l)) \right] \nonumber
% \end{align}

% where $\sigma$ is the sigmoid function.

% In this paper, we focus on the application of reward models. We employ it to score completions of instructions and select appropriate responses according to the reward.

\input{figuretexs/dy_winrate}

\subsection{Direct Preference Optimization}
Different from conventional RLHF which first compresses human preferences into reward models, direct preference optimization is a RL-free algorithm for training language models to align with human preferences.
DPO is recognized as one of the most widely used methods for preference optimization. 
It reformulates the reward function $r$ into a closed-form expression aligned with the optimal policy:
\begin{equation}
r(x, y) = \beta \log \frac{\pi_\theta(y | x)}{\pi_{\text{ref}}(y | x)} + \beta \log Z(x) \nonumber
\end{equation}

where $\pi_\theta$ denotes the policy model, $\pi_{\text{ref}}$ represents the reference model (usually the supervised fine-tuned model) and $Z(x)$ is the partition function. 
By embedding this reward formulation into the Bradley-Terry (BT) ranking framework, the probability of preference $p(y_w > y_l | x)$ is computed as $\sigma(r(x, y_w) - r(x, y_l))$, where $\sigma$ is the sigmoid function. Therefore, DPO replaces the reliance on a reward model with the policy model, resulting in the following objective:
\begin{align}
& \mathcal{L}_{\text{DPO}}(\pi_\theta; \pi_{\text{ref}}) = \notag \\
& -\mathbb{E}_{(x, y_w, y_l) \sim \mathcal{D}} \Big[ \log \sigma(r(x, y_w) - r(x, y_l)) \Big] \notag
\end{align}

where $r(x, y) = \beta \log \frac{\pi_\theta(y \mid x)}{\pi_{\text{ref}}(y \mid x)}$.


\subsection{Preference Data Construction}
\label{conven_pipe}
Recently, a method for constructing preference pairs without relying on human annotations has been gaining popularity~\cite{dong2023raft, meng2024simpo}.
As shown in Figure~\ref{pipe}, given a language model policy $\pi_{\theta}$,  a reward function $r$ and $k$ prompts $\left\{x_i\right\}_{i=1}^k$, we sample $n$ generations $\left\{y_{ij}\right\}_{j=1}^n$ for the $i$-th prompt from $\pi_{\theta}$. 
The given reward model will be used to score the sampled generations.
The reward of $n$ candidate samples of $i$-th prompt is $\left\{r_{ij}\right\}_{j=1}^n$.
Afterwards, the completion of the highest reward score $\max_{j=1}^{n} \{r_{ij}\}$
is selected as the chosen response, while the completion of the lowest reward score $\min_{j=1}^{n} \{r_{ij}\}$ is selected as the rejected response to construct a preference pair $(y_w^{(i)}, y_l^{(i)})$ for $x_i$. 
In practice, $n=5$ can achieve significant performance gains~\cite{meng2024simpo}.
In this work, we explore the effects of increasing the number of samples, $n$.