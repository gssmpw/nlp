\section*{Limitations}

Our method assumes the existence of a strong reward model, which may not always be readily available or easily trainable for all tasks. 
The quality of the reward model directly impacts the effectiveness of our approach, and inaccuracies in the reward model can lead to suboptimal performance or biased outcomes.
Another limitation is the computational cost associated with generating a large number of responses for each prompt to construct the preference dataset. 
For tasks with extensive input space or high complexity, this can become resource-intensive and time-consuming.
We mainly focus on DPO in this paper and will explore other preference optimization methods in the future.
