




\section{Scaling Samples to Improve Alignment }
% \section{Scaling Reward Distribution-Based Preference Data for DPO}
\label{main_exp}

\input{figuretexs/fix_reject_wr}

\input{figuretexs/llama_skywork}

We established that selecting the rejected response at reward position \(\mu - 2\sigma\) is a key factor and model performance increases as the quality of the chosen response improves. 
Building on these insights, we propose a simple and effective preference pair construction strategy for DPO.
We further validate the effectiveness of this strategy across multiple reward models to ensure its robustness.  

\subsection{Data Construction Strategy}
Given a language model policy, a reward function, and \( k \) prompts \( \{x_i\}_{i=1}^k \), we sample \( n \) responses per prompt, denoted as \( \{y_{ij}\}_{j=1}^n \), from the policy model \( \pi_{\theta} \). 
Each response is scored using the reward function.
For the rejected response, we select the response with the lowest reward from 5 random samples. 
We find this approach to be an effective proxy for \(\mu - 2\sigma\) if the sample size is insufficient to approximate the true normal distribution of rewards. 
For the chosen response, we select the response with the highest reward among all \( n \) samples.  
This ensures that as \( n \) increases, the quality of the chosen responses improves naturally, leading to better preference optimization. 
An illustration of the data construction process is provided in Figure~\ref{pipe}. 
We further analyze how the quality of the chosen responses evolves with increasing sample size in Appendix~\ref{appendix_reward}. 

\subsection{Experiment Setup}
We evaluate our proposed preference data construction method by comparing it with the conventional approach, where the chosen response is selected as the one with the highest reward and the rejected response is the one with the lowest reward among five samples. 

For our method, we begin by sampling 5 responses per prompt. 
The response with the lowest reward is designated as the rejected response. 
As we progressively increase the number of sampled responses, we continue to select the chosen response as the one with the highest reward among all available candidates. 
This approach ensures that as the sample size grows, the quality of the chosen response improves, allowing us to examine the impact of a larger sample pool on model alignment. 
All experiments are conducted by following the implementation details outlined in Section~\ref{imp_detail}, unless specified otherwise.  

\subsection{Experimental Results and Analysis}

\paragraph{Scaling the Number of Samples.}  
The results of our proposed preference data construction strategy are presented in Figure~\ref{fix_wr}. 
For reference, the first point in each line represents the performance of the conventional approach. 
Since our method is identical to the baseline when using five samples per prompt, performance differences emerge as \( n \) increases. 
We observe a steady improvement in performance across all models as we increase the number of samples from 5 to 200, even though the rate of improvement may diminish in some cases. 
The only exception occurs in Llama-3-8B-Instruct, where performance experiences a slight drop when increasing the number of samples from 100 to 200.




%We hypothesize that this may be related to reward hacking, where the reward model assigns disproportionately high scores to certain responses that do not necessarily reflect true quality. Further investigation of this phenomenon is left for future work.  

\paragraph{Comparison with Prior Work.}  
To further validate the effectiveness of our method, we compare it with the results of \citet{meng2024simpo} (first 2 rows) in Table~\ref{compare_literature}, which employs the conventional data construction strategy.
Our method can outperform baseline results of DPO in both benchmarks, AlpacaEval 2 and Arena-hard.
Furthermore, it can also surpass the baseline results of SimPO in terms of Alpaca win rate and Arena-hard win rate.  

\input{tabletex/dpo_from_simpo}
% \input{tabletex/academic_bench}

\paragraph{Evaluation on Skywork Reward Model.}  
While our previous experiments used Armorm as the reward model, we also evaluate our preference data construction strategy using the Skywork reward model to ensure its general applicability. 
We adopt Llama-3-8B-Instruct as the SFT model and record the results of AlpacaEval 2 in Figure~\ref{llama_skywork}. 
We can see that model performance improves as the number of samples increases before reaching a platform, which confirms that our strategy is robust across different reward models.  

\paragraph{Evaluation on Academic Benchmarks.}
To assess whether our preference data construction method negatively affects performance on established NLP benchmarks, we evaluate our trained model based on Llama-3-8B-Instruct on a set of widely used academic tasks, including ARC~\cite{clark2018thinksolvedquestionanswering}, HellaSwag~\cite{zellers-etal-2019-hellaswag}, TruthfulQA~\cite{lin-etal-2022-truthfulqa} and GSM8K~\cite{cobbe2021trainingverifierssolvemath}. 
We use the Language Model Evaluation Harness~\cite{eval-harness} for evaluation. 
More details of our results are presented in the Appendix~\ref{aca_bm}. 
We observe that our policy models do not show performance drops on academic benchmarks.

% The results indicate that our method does not negatively impact model performance on these academic benchmarks. This suggests that improving model alignment via our preference data construction strategy does not come at the cost of general capability degradation, reinforcing the practicality of our approach.  



%\input{figuretexs/fix_reject_wr}

%\input{figuretexs/llama_skywork}

%Based on the findings that if the chosen response is properly selected, the performance of trained models will improve as the quality of chosen response improves and the rejected response should be selected from $\left\{\mu-2\sigma\right\}$, we present a simple and effective preference pair construction strategy with on-policy data for DPO in this section.
%We also validate the effectiveness of this strategy on multiple reward models.
 

% \subsection{A Simple Dataset Construction Strategy}
  
% Given a language model policy,  a reward function, and $k$ prompts $\left\{x_i\right\}_{i=1}^k$, we sample $n$ ($n\geq5$) generations $\left\{y_{ij}\right\}_{j=1}^n$ for each prompt from $\pi_{\theta}$. 
% We then ask the reward function to score the sampled generations.
% \emph{For the rejected response of a prompt, we select it as the one which has the minimal reward in 5 random samples. We find it an effective proxy for $\left\{\mu-2\sigma\right\}$ if the sample size is not large enough to accurately approximate the normal distribution.} 
% For the chosen response of a prompt, we select it as the one which has the maximal reward in all $n$ candidate generations. 
% The corresponding illustration can be found in Figure~\ref{pipe}.
% As we increase the number of samples, also $n$,  the quality of the chosen responses naturally improves to a certain extent, which we show in Appendix~\ref{appendix_reward}.


%\noindent\textbf{Baselines.} 
%We compare our strategy with the conventional preference data construction strategy, which identifies the response with the highest reward as the preferred response and the one with the lowest reward as the dispreferred response among 5 samples. 



%\paragraph{Implementation Details.} 
%In practice, we sample 5 responses for each prompt and select the one with the lowest reward as the rejected response for our method. 
%Subsequently, we increase the number of samples and select the chosen response as the one of maximal reward. 
%Unless otherwise specified, our experiments mainly follow the implementation details outlined in Section~\ref{imp_detail}.


%\paragraph{Results.}
%We record the results of our strategy in Figure~\ref{fix_wr}.
%The performance of the conventional approach is represented by the first point in each line of Figure~\ref{fix_wr}. 
%\emph{Our method and baseline are identical when the number of samples is 5.}
%We can find that the performance of trained models is steadily improving if we increase the number of samples from 5 to 200, although with diminishing returns in some cases.
%The only exception is that performance experiences a slight decline when we increase the number of samples from 100 to 200 on Meta-Llama-3-8B-Instruct.
%We hypothesize that the reason could be related to reward hacking and leave the exploration for future work.

%To further validate the effectiveness of our method, we compare it with the results of \citet{meng2024simpo} in Table~\ref{compare_literature}, which adopts the conventional data construction strategy.
%Our method can outperform baseline results with DPO in two benchmarks, Alpaca evaluation and Arena-hard.
%Furthermore, it can also surpass the baseline results with SimPO in terms of Alpaca win rate and Arena-hard win rate.



%\input{tabletex/dpo_from_simpo}
%\input{tabletex/academic_bench}


%\subsection{Effectiveness on Skywork}
%We have validated the effectiveness of our preference data construction method on Armorm.
%In this section, we demonstrate that our preference data construction strategy is also effective when applied to the Skywork reward model.
%We adopt Meta-Llama-3-8B-Instruct as the SFT model.
%We record the results in Figure~\ref{llama_skywork}, which are consistent with results of Armorm.
%Performance improves until a slight decline as we increase the number of samples per prompt.
%We can acquire improvement of performance within an extent while increasing the number of samples per prompt.



%\subsection{Evaluation on Academic Benchmarks}
%We further evaluate our trained models of Meta-Llama-3-8B-Instruct on academic benchmarks. ARC~\cite{clark2018thinksolvedquestionanswering}, HellaSwag~\cite{zellers-etal-2019-hellaswag}, TruthfulQA~\cite{lin-etal-2022-truthfulqa} and GSM8K~\cite{cobbe2021trainingverifierssolvemath}. 
%We use Language Model Evaluation Harness~\cite{eval-harness} for evaluation.
%The results are recorded in Table~\ref{task_performance}.
%As we can see, our method (the last five rows) does not have negative effects on academic benchmarks.


