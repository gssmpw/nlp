
\section{Preference Data Construction via Reward Distribution}
%\section{Exploration of Preference Data Construction} \label{method}
\label{main_method}
In this section, we explore alternative ways to categorize sampled responses based on their reward scores, focusing on a distribution-based approach. 
We first discuss the limitations of ranking-based categorization and introduce a reward distribution-based strategy.
We then describe the preference pair construction process, followed by experimental validation and key insights derived.

% In this section, we introduce two perspectives on the categorization of samples based on rewards.
% Furthermore, we categorize the sampled responses per prompt into 7 distinct groups based on the reward distribution and pair them to generate 21 sets of preference data. 
% Finally, we train the corresponding policy models through DPO for each constructed preference dataset, then evaluate their performance on AlpacaEval 2 and report our findings.

\subsection{Reward Distribution}
% Rather than assuming a uniform reward distribution, a more principled approach to preference pair construction is to categorize responses based on their underlying statistical properties. 
In reality, reward scores often exhibit a skewed or clustered distribution, making it challenging to establish meaningful distinctions using fixed ranking intervals.
Instead of dividing samples into equal-sized bins, we define preference categories based on the mean (\(\mu\)) and standard deviation (\(\sigma\)) of the underlying normal distribution, as illustrated in Figure~\ref{normal_dist}. 
This method ensures that preference pairs are drawn from statistically meaningful intervals.

By sampling responses at key points in the distribution, such as \(\mu \pm 2\sigma\), \(\mu \pm \sigma\), and \(\mu\), we can capture variations in reward scores that reflect quality distinctions of responses. 
% This approach prevents preference pairs from being constructed using responses with minimal reward differences, which can negatively impact learning dynamics in DPO. 
Another advantage of this approach is that it allows for precise control over the reward margin between chosen and rejected responses. 
By leveraging distribution-aware categorization, we aim to construct preference pairs systematically, enabling a more comprehensive understanding of trained models. 

%A straightforward approach to constructing preference samples for each prompt is to sort the sampled responses by their reward scores and divide them into equal-sized bins. Given a total of \( n \) sampled responses, this method partitions them into \( k \) bins, each containing approximately \( \frac{n}{k} \) responses. When constructing a preference pair \((s_{b_1}, s_{b_2})\), the chosen response \( s_{b_1} \) is sampled from bin \( b_1 \), while the rejected response \( s_{b_2} \) is sampled from bin \( b_2 \), ensuring that \( b_1 > b_2 \). While this ranking-based approach is intuitive, it inherently assumes that reward values are uniformly distributed across samples. In practice, however, reward distributions are often skewed or clustered, leading to bin boundaries that do not necessarily reflect meaningful differences in response quality. As a result, preference pairs generated through this method may not capture well-defined distinctions between responses, which can negatively impact the training dynamics. 

%An alternative approach accounts for the statistical properties of the reward distribution, as illustrated in Figure~\ref{normal_dist}. Rather than dividing samples into fixed ranking intervals, this method defines categories based on the mean (\(\mu\)) and standard deviation (\(\sigma\)) of the reward scores. This allows the categorization of responses in a way that reflects their natural distribution, ensuring that preference pairs are constructed from statistically meaningful intervals. Unlike the ranking-based approach, which imposes artificial bin boundaries, this method provides a more principled way to select responses by considering the overall distribution of rewards rather than assuming uniformity. By sampling from different regions of the reward distribution, this approach ensures that preference pairs are drawn from statistically distinct regions, which improves the stability of DPO training. The ability to systematically control the reward margin between chosen and rejected responses further enhances the learning process by mitigating potential biases introduced by arbitrarily defined ranking-based bins.

%By leveraging distribution-aware categorization, this approach constructs more informative preference pairs that better guide the DPO training process. In this work, we adopt the reward distribution-based approach due to its ability to capture more meaningful distinctions in response quality. 

%\subsection{Categorize Sampled Responses}

%\paragraph{Categorization by Reward Ranking.}  
%A straightforward way to categorize samples for each prompt is to sort responses by their reward scores and divide them into equal-sized groups. This approach assumes that reward values are uniformly distributed, but in practice, reward distributions are often skewed or clustered. As a result, this method may lead to arbitrary category boundaries that do not reflect meaningful distinctions in model quality.  

%\paragraph{Categorization by Reward Distribution.}  
%An alternative approach accounts for the statistical properties of the reward distribution, as illustrated in Figure~\ref{normal_dist}. Rather than using fixed ranking intervals, this method defines categories based on the mean (\(\mu\)) and standard deviation (\(\sigma\)) of the reward scores. This ensures that categories align with natural variations in the data, avoiding misrepresentation due to uneven reward spacing. By leveraging distribution-aware categorization, we aim to construct more informative preference pairs that better guide the DPO training process.  

%In this work, we adopt the reward distribution-based approach due to its ability to capture more meaningful distinctions in response quality. The following sections explore how this categorization framework enables a more robust and scalable preference optimization strategy.  


% \subsection{Sample Category}

% \paragraph{Categorize by Reward Ranking.}
% An intuitive approach to categorizing samples for each prompt is to first sort the samples by their reward scores. 
% The samples can then be divided into several categories with an equal number of samples in each, based on their reward values. 
% However, this method of categorization overlooks the underlying distribution of the rewards, which may lead to categories that do not accurately reflect meaningful distinctions in the data.


% \paragraph{Categorize by Reward Distribution.}  
% Another approach to categorizing samples involves considering the underlying Gaussian distribution of the reward scores, as shown in Figure~\ref{normal_dist}. 
% Instead of simply dividing the samples into equal-sized categories, this method uses statistical properties such as the mean (\(\mu\)) and standard deviation (\(\sigma\)) of the rewards to define meaningful boundaries. 
% This approach offers a more nuanced representation of the rewards by capturing key distinctions within the distribution. 
% It can prevent misrepresentation caused by uneven reward spacing and ensure that each category reflects a meaningful range of samples. 

% In this work, we adopt the second method to categorize samples per prompt by reward distribution, considering the advantages that it has.








% \paragraph{Implementation Details.} 
% To start our experiments, we first extract a random subset (1,000) prompts from UltraFeedback as our analysis cases.
% For each prompt, $n$ responses are sampled from Meta-Llama-3-8B-Instruct with temperature 0.8.
% In this experiment, $n$ is 400.
% We then employ reward models to compute the reward for each response paired with the corresponding prompt.
% On the reward model, we employ Absolute-Rating Multi-Objective reward model (Armorm)~\cite{wang2024arithmetic} and Skywork reward models (Skywork)~\cite{liu2024skyworkrewardbagtricksreward} to compute rewards.

% \paragraph{Observations.}
% We manually review the reward distribution of completions for each prompt. 
% As shown in Figure~\ref{normal_dist}, we find that the reward scores per prompt closely follow a Gaussian distribution.
% And this distribution holds for both Armorm and Skywork, though with different reward scales due to distinct training objective between Armorm and Skywork.
% In addition, we theoretically demonstrate that response rewards of approximately $20\%$ prompts can perfectly pass the Kolmogorov-Smirnov test for a Gaussian distribution.


% \input{tabletex/base}
\input{figuretexs/main_fig}



%\subsection{A Thorough Exploration of Preference Data Construction }
\subsection{Preference Data Construction}
\label{data_cons}
We propose a structured approach to constructing preference pairs and training policy models through DPO. 
Our method leverages the statistical properties of reward distributions to systematically select responses for preference pair construction.

For each prompt, we first generate \( n \) responses from an SFT model and compute their reward scores using a given reward model. 
Given the reward scores of responses for the \( i \)-th prompt, we approximate the distribution as \( N(\mu_i, \sigma_i^2) \), where \( \mu_i \) and \( \sigma_i \) denote the mean and standard deviation of the rewards, respectively. 
To ensure a representative selection of responses, we extract samples at key points in the reward distribution. 
Specifically, we select responses closest to the values \( \mu_i - 2\sigma_i, \mu_i - \sigma_i, \mu_i, \mu_i + \sigma_i, \mu_i + 2\sigma_i \), along with responses with minimum and maximum reward scores.
This process results in a set of seven different sample points: \( \{ min, \mu \pm 2\sigma, \mu \pm \sigma, \mu, max \} \).
The \(\mu\) and \(\sigma\) are prompt specific, we drop \(i\) for brevity in the rest parts of this paper.

The preference pairs are then constructed by considering all possible pairwise combinations of these seven points, following the principle that the chosen response should have a higher reward than the rejected response. 
This results in \( C_7^2 = 21 \) distinct preference pairs per prompt, also 21 preference datasets as a whole. 
We subsequently train 21 different policy models through DPO, each optimized on a unique preference dataset. 
Figure~\ref{pipe} illustrates the overall preference construction process.

\subsection{Experiment Setup}
%\paragraph{Implementation Details.}
We largely follow the same experimental and implementation setup described in Section~\ref{imp_detail}. 
We generate 200 samples per prompt and apply the proposed preference data construction strategy. 
For comparison, we also evaluate models trained with conventional preference pair selection, as detailed in Section~\ref{conven_pipe}. 
The results of these baseline models are reported in Appendix~\ref{baseline}.


%We propose a reward distribution-aware categorization strategy, where sampled responses per prompt are grouped into seven statistical bins based on their reward scores. We then systematically construct 21 preference datasets using pairwise combinations from these bins and train corresponding policy models through DPO. The following sections describe the technical details of this approach.

%Based on the Gaussian distribution of response rewards per prompt, we propose an intuitive approach to construct all preference pairs and train the corresponding policy models. 



%\paragraph{Preference Construction.}


%We first sample $n$ responses from SFT models for each prompt and compute the reward for them with a given reward model.
%Given reward scores of responses for $i$-th prompt, we can approximate its $\mu_i$ and $\sigma_i$ for $N\left(\mu_i, \sigma_i^2\right)$.
%Afterwards,  we first extract samples on reward points $\left\{\mu_i-2\sigma_i, \mu_i-\sigma_i, \mu_i, \mu_i+\sigma_i, \mu_i+2\sigma_i\right\}$\footnote{In practice, we select the sample point which has the closest reward score to the value in the set.} as well as completions that have maximal and minimal reward scores, respectively. 
%Therefore, we end up with 7 different sample points $\left\{min, \mu \pm 2\sigma, \mu\pm\sigma, \mu, max\right\}$.
%We continue to construct $C_7^2$ (21) preference pairs per prompt following the principle that the reward of the chosen response should be higher than that of the rejected response.
%Finally, we can train 21 different policy models through DPO in total for every SFT model.
%The corresponding illustration can be found in Figure~\ref{pipe}.

%\paragraph{Implementation Details.}
%We mainly follow Section~\ref{imp_detail} for our experiments if not specified.
%Specifically, we sample 200 for each given prompt.
%For comparison, we also report the results of the SFT models and models trained with the conventional preference data construction strategy (described in Section~\ref{conven_pipe}) in Appendix~\ref{baseline}.

% For \textbf{baselines}, we report the results of the conventional strategy, which selects the samples of maximal reward as the chosen response and the samples of minimal reward as the rejected response in 5 samples to construct the preference dataset.
% We also report the performance of SFT models.


% We follow \citet{meng2024simpo} to conduct experiments on two setups, \textbf{Base} and \textbf{Instruct}.
% For the base setting, we first train Meta-Llama-3-8B and Mistral-7B-v0.2 with 
% UltraChat-200k dataset~\cite{ding2023enhancing} to obtain the SFT model. 
% For the instruct setting, we directly employ Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 as SFT models.
% With the SFT model, we then perform sampling using prompts (instructions) from Ultrafeedback and construct an on-policy preference dataset.
% Specifically, we use temperature 0.8 and number of samples per prompt is 200. 
% We adopt DPO~\cite{rafailov2023direct} to train our policy models with constructed datasets.
% More hyperparameters about training can be found in Appdendix.

%\paragraph{Results.}
\subsection{Experiment Results}
We evaluate the performance of 84 policy models trained with the constructed preference datasets, with results presented in Figure~\ref{main_fig}. 
To mitigate biases introduced by response length, we primarily focus on the LC win rate as our evaluation metric~\cite{dubois2024lengthcontrolled}. 
In the following, we summarize our key findings and their implications for preference pair construction in DPO.  

\input{figuretexs/llama_loss}

\paragraph{Impact of Preference Pair Construction on Performance.}
Our results indicate that the chosen response should be selected from \(\{max, \mu+2\sigma\}\).
In addition, the rejected response should be selected at reward position \(\mu-2\sigma\) instead of the minimum reward to produce the optimal performance.  
Among all preference pairs, the pair \((\mu+2\sigma, \mu-2\sigma)\) consistently outperforms others in most cases. 
For example, Llama-3-8B-Instruct trained with this preference pair achieves a length-controlled win rate of 48.18\%, surpassing the conventional preference data construction strategy by about 3 percentage points. 
These findings suggest that preference pairs constructed from well-separated reward intervals improve preference optimization of policy models more effectively than naive max-min strategy.  

\paragraph{Effect of Reward Margins on Performance.}
A key observation from our experiments is that the performance of trained models improves as the reward of the chosen response increases, provided that the rejected response is appropriately selected. 
When the rejected response is at reward position \(\mu-2\sigma\), the length-controlled win rate increases as the chosen response moves toward higher reward values. 
This trend is witnessed across multiple models and settings, as illustrated in Figure~\ref{main_fig}. 
These results reinforce the importance of ensuring a sufficiently large reward margin between chosen and rejected responses, which contributes to more effective preference optimization.  

\paragraph{Limitation of Small Reward Margins.}
We further observe that preference pairs with small reward margins perform poorly.  
When the reward of the chosen response is only slightly higher than that of the rejected response, the model struggles to learn meaningful distinctions. 
For example, training Llama-3-8B-Instruct with the pair \((\mu+2\sigma, \mu+\sigma)\) results in a length-controlled win rate of 34.63\%, significantly lower than pairs with larger reward differences. 

\paragraph{Robustness of DPO Training.}
Notably, none of the preference pairs degrades the performance of the SFT checkpoint.
This confirms that DPO training remains robust to different preference pairs. 
Even for suboptimal preference pairs, model performance does not regress below the baseline established by the SFT checkpoint, highlighting the stability of the DPO.  



\subsection{Analysis}

\paragraph{Extending Reward Positions.}
To further explore the impact of preference data construction, we extend our data construction to include additional reward points at \(\mu \pm 4\sigma\) and \(\mu \pm 3\sigma\). 
Experiments conducted on Llama-3-8B-Instruct reveal that sample points at \(\mu + 4\sigma\) and \(\mu + 3\sigma\) show no significant difference from selecting max-reward responses. 
Similarly, \(\mu - 4\sigma\) and \(\mu - 3\sigma\) exhibit no substantial difference from selecting min-reward responses. 
These findings suggest that expanding the reward range beyond \(\mu \pm 2\sigma\) does not provide additional benefits for preference optimization, reinforcing the sufficiency of our selected reward points.
The experimental results can be found in Appendix~\ref{appendix_extend}.

\paragraph{Scaling to 400 Samples Per Prompt.}
While our main experiments use 200 samples per prompt due to computational constraints, we also evaluate the scalability of our findings by conducting experiments with 400 samples per prompt. 
Based on the experiment with Llama-3-8B-Instruct as the SFT model, we observe that our conclusions remain consistent across both sample sizes.
More details on these results are provided in Appendix~\ref{appendix_400}.  

\paragraph{Training Dynamics and Loss Analysis.}
To better understand how different preference pairs influence DPO training, we record the training loss of six datasets, corresponding to the pairs \((max, min)\), \((max, \mu - 2\sigma)\), \((max, \mu - \sigma)\), \((max, \mu)\), \((max, \mu + \sigma)\), and \((max, \mu + 2\sigma)\). 
The loss curves, presented in Figure~\ref{loss}, reveal several important trends. 
First, larger reward margins facilitate training by enabling the model to converge more effectively. 
Models trained with larger reward gaps achieve lower loss values, which correlate with improved performance. 
By contrast, training loss for the pair \((max, \mu + 2\sigma)\) stagnates, indicating underfitting. 
We assume the reason is that it is difficult for the model to distinguish the chosen and rejected in this pair, leading to ineffective optimization.  
Interestingly, the preference dataset \((max, min)\) exhibits the lowest training loss. 
While this may suggest faster convergence, it also raises concerns about overfitting, as models trained on this dataset fail to perform as well as those trained with intermediate reward pairs. These findings highlight the trade-off between reward margins, optimization efficiency, and generalization performance. 
A more detailed empirical and theoretical analysis is provided in Appendix~\ref{overfitting}.  


%In this part, we report the results of 84 policy models trained with the constructed preference dataset in Figure~\ref{main_fig}.
%We focus on the length-controlled win rate because it takes length bias into consideration~\cite{dubois2024lengthcontrolled}. Our findings are listed below.

%\input{figuretexs/llama_loss}


%\begin{itemize}[leftmargin=*]
%    \item To achieve superior performance, we find that the chosen response should be selected from $\left\{max, \mu+2\sigma\right\}$, while the rejected response should be selected from $\left\{\mu-2\sigma\right\}$. As we can see, pairs\footnote{We follow the format (chosen, rejected).} $(\mu+2\sigma, \mu-2\sigma)$ can surpass other constructed preference pairs in most cases. For example,  Meta-Llama-3-8B-Instruct trained with pair $(\mu+2\sigma, \mu-2\sigma)$ obtains a length-controlled win rate $48.18\%$, which outperforms its counterpart trained with conventional data construction strategy by 3 points.

%    \item  When rejected responses are appropriately selected, the performance of trained models can improve as the reward of the chosen responses increases. If the rejected response is selected as $\left\{\mu-2\sigma\right\}$, length-controlled win rate will improve as the reward of the chosen responses increases, as clearly indicated by rows of each subplot in Figure~\ref{main_fig}.
    
%    \item Preference pairs of small margins usually perform poorly. We find that if the reward of the chosen response is slightly higher than that of the rejected response, the models trained with them cannot achieve satisfactory performance. Specifically, the model trained with pair $(\mu+2\sigma, \mu+\sigma)$ only obtains a length-controlled win rate $34.63\%$ on Meta-Llama-3-8B-Instruct.

%    \item We also find that none of the preference pairs will degrade the performance of the SFT checkpoint, which confirms the robustness of the DPO training.
%\end{itemize}

%More experimental results can be found in Appendix~\ref{wr}.




%\paragraph{Extend Reward Positions.}
%In main experiment, we have tried dataset construction based on points $\left\{min, \mu \pm 2\sigma, \mu\pm\sigma, \mu, max\right\}$.
%Here, we extend this set to further include values $\left\{\mu \pm 4\sigma, \mu \pm 3\sigma\right\}$. 
%We experiment on Meta-Llama-3-8B-Instruct. 
%We find that sample points on $\left\{\mu + 4\sigma, \mu + 3\sigma\right\}$ have no significant differences from $\left\{max\right\}$ when used to train models through DPO.
%In addition, sample points on $\left\{\mu - 4\sigma, \mu - 3\sigma\right\}$ have no significant differences from $\left\{min\right\}$.
%We report some of our results in Appendix~\ref{appendix_extend}.




%\paragraph{400 Samples Per Prompt.}
%We mainly focus on 200 samples per prompt due to computation and evaluation costs in main experiment.
%In this part, we experiment with 400 samples per prompt to explore whether our results of 200 samples can still hold.
%As we have emphasized, we are given sufficient sample budgets.
%Here, we adopt Meta-Llama-3-8B-Instruct as SFT model.
%We find that conclusions of constructed datasets are consistent between both scenarios, 200 samples and 400 samples per prompt, respectively.
%We report the result in Appendix~\ref{appendix_400}.






%\paragraph{Training Dynamics.}

%To further understand the training dynamics of DPO with each dataset, we record the training loss every five steps for six datasets ($max$, $min$), ($max$, $\mu - 2\sigma$), ($max$, $\mu - \sigma$), ($max$, $\mu$), ($max$, $\mu + \sigma$) and ($max$, $\mu + 2\sigma$) for Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2, as shown in Figure~\ref{loss}.

%It can be seen that increasing the reward margin between the chosen and rejected responses may facilitate model training.
%Training loss can reach a lower bound when the reward margin increases.
%Furthermore, there is a strong correlation between the converged state of the loss and the performance of models. 
%Specifically, models that achieve lower loss values tend to demonstrate superior performance, indicating that effectively minimizing loss could enhance the model capabilities.
%In addition, the loss of ($max$, $\mu + 2\sigma$) does not show any notable decrease during training, which causes underfitting. 
%This stagnation is likely due to the difficulty in distinguishing between the chosen and rejected responses of this dataset.
%We also find that the preference dataset ($max$, $min$) tends to exhibit the lowest training loss.
%It may increase the risk of overfitting, which may help explain why it can only achieve inferior performance compared to ($max$, $\mu + 2\sigma$).
%We provide more empirical and theoretical analysis in Appendix~\ref{overfitting}.
% When comparing ($max$, $min$) with ($max$, $\mu - 2\sigma$), we observe that the latter dataset results in a slightly greater decline in the likelihood of the chosen response. 
% This may account for its superior performance, as \citet{chen2024improvedpreferenceoptimizationpipeline} stated that preference optimization tends to achieve better convergence when likelihood of prefered samples get slightly reduced .


