\section{Introduction}
\label{sec:intro}

Large language models (LLMs) have significantly advanced natural language processing, demonstrating remarkable capabilities across various tasks~\cite{NEURIPS2020_1457c0d6, wei2022finetuned, bubeck2023sparksartificialgeneralintelligence}.  
However, these models still generate unintended outputs due to their unsupervised nature~\cite{bai2022traininghelpfulharmlessassistant, 10.1145/3531146.3533088}.  
To mitigate these issues, recent efforts have focused on improving LLM alignment with human preferences~\cite{NEURIPS2022_b1efde53, rafailov2023direct, pmlr-v238-gheshlaghi-azar24a, pmlr-v235-ethayarajh24a, meng2024simpo}. 

Reinforcement learning from human feedback (RLHF) has become a widely adopted framework to align LLMs with human preferences.  
RLHF involves first training a reward model, which then provides feedback signals to optimize a policy model through reinforcement learning, typically using Proximal Policy Optimization (PPO)~\cite{ahmadian-etal-2024-back}.  
However, this approach is complex and often unstable as it requires substantial memory and computational resources to accommodate three separate models simultaneously. 
To address these challenges, \citet{rafailov2023direct} introduced Direct Preference Optimization (DPO), which bypasses the need for reward models.
Subsequent research has focused on improving preference optimization efficiency~\cite{liu2024statistical, pmlr-v238-gheshlaghi-azar24a, pmlr-v235-ethayarajh24a, han2024fpogeneralizingpreferenceoptimization}.

Currently, with the increasing availability of powerful reward models~\cite{pmlr-v202-gao23h, wang-etal-2024-interpretable, liu2024skyworkrewardbagtricksreward}, an effective pipeline (Figure~\ref{pipe}) to further enhance the alignment capabilities of LLMs without human annotations has gained popularity.  
To start, $n$ on-policy responses~\cite{tajwar2024preferencefinetuningllmsleverage, guo2024directlanguagemodelalignment} are first sampled from LLMs and subsequently scored by a reward model.
The response with the highest reward is selected as the chosen response, while the one with the lowest reward is selected as the rejected response to construct a preference dataset.
The constructed preference dataset can be used to train the policy model through DPO in return. 
In practice, five samples per prompt can achieve significant performance gains~\cite{meng2024simpo}.



%\paragraph{Motivation.} Large language models (LLMs) have revolutionized various fields by exhibiting impressive capabilities in natural language processing tasks~\cite{NEURIPS2020_1457c0d6, wei2022finetuned, bubeck2023sparksartificialgeneralintelligence}.  However, these models still occasionally generate unwanted outputs, which are harmful or toxic, or simply do not follow instructions due to their unsupervised nature~\cite{bai2022traininghelpfulharmlessassistant, 10.1145/3531146.3533088}.  Recent efforts have focused on addressing these issues to improve the alignment of LLMs with human preferences~\cite{NEURIPS2022_b1efde53, rafailov2023direct, pmlr-v238-gheshlaghi-azar24a, pmlr-v235-ethayarajh24a, meng2024simpo}. Currently, reinforcement learning from human feedback (RLHF) has been a standard method to align LLMs with human preferences.

% \input{figuretexs/pipe}
\input{figuretexs/full_exp}

%One common practice in RLHF is to first train a reward model, which is then used to provide feedback signals when training the policy model with reinforcement learning (i.e., PPO)~\cite{ahmadian-etal-2024-back}. However, this approach is complex and often unstable as it requires substantial memory and computational resources to accommodate three separate models simultaneously. To address these issues, \citet{rafailov2023direct} proposed to optimize policy models with direct preference optimization (DPO), which bypasses the need for reward models. Specifically, DPO maximizes the margin of implicit reward between the chosen and rejected responses. The reward is computed by the logarithmic ratio of the likelihood between the current policy model and the supervised fine-tuning (SFT) model.Subsequently, a series of studies have focused on enhancing the optimization efficiency of existing RLHF methods~\cite{liu2024statistical, pmlr-v238-gheshlaghi-azar24a, pmlr-v235-ethayarajh24a, meng2024simpo, han2024fpogeneralizingpreferenceoptimization}.

% \input{tabletex/stats}

% However, these methods require a large amount of human-annotated preference data to effectively optimize either reward models or policy models~\cite{stienon2020learning, bai2022traininghelpfulharmlessassistant, pmlr-v162-ethayarajh22a, NEURIPS2023_949f0f8f, wang2024helpsteer2preferencecomplementingratingspreferences, dai2024safe, pmlr-v235-cui24f}, while collecting annotations of preference data is both time consuming and costly.
% To reduce the dependence on human annotations, another line of research has explored the use of preference annotations generated by large language models~\cite{bai2022constitutionalaiharmlessnessai, yang2024rlcd, pmlr-v235-lee24t}.
% Although this approach leverages the capabilities of LLMs to produce high-quality annotations, potentially accelerating the annotation process, it incurs significant computations and financial costs.
% For example, the UltraFeedback dataset~\cite{pmlr-v235-cui24f} is created with the help of most powerful closed-source LLMs (i.e., GPT-4) to obtain responses and preference annotations.





%Currently, with the growing availability of powerful reward models~\cite{pmlr-v202-gao23h, wang-etal-2024-interpretable, liu2024skyworkrewardbagtricksreward} on Huggingface, an effective pipeline to further enhance the alignment capabilities of LLMs without human annotation has been gaining significant popularity. To start, $n$ on-policy responses~\cite{tajwar2024preferencefinetuningllmsleverage, guo2024directlanguagemodelalignment} are first generated by LLMs and subsequently scored using a reward model. To construct a preference dataset, the response with the highest reward is selected as the chosen response, while the one with the lowest reward is selected as the rejected response. The constructed preference dataset can be used to train the policy model through DPO in return.  In practice, generating 5 ($n$=5) responses for each prompt can achieve significant performance gains with DPO~\cite{meng2024simpo}.

% This paper investigates the scalability of preference data construction by systematically increasing the number of on-policy samples per prompt. While a larger $n$ should theoretically allow for better preference selection, we observe that conventional min-max selection fails to improve model alignment and may even degrade performance (Figure~\ref{dy_wr}). We hypothesize that this is due to informational inefficiency, where extreme values fail to capture meaningful distinctions in response quality.  

% To address this, we introduce a statistical approach to preference data selection. Instead of selecting only the highest- and lowest-reward responses, we categorize samples into seven groups based on the underlying reward distribution, and explore 21 preference pair combinations. Extensive evaluations on four models using AlpacaEval-2 reveal that selecting the rejected response at $\mu - 2\sigma$ instead of the lowest reward significantly improves model performance. Based on these findings, we propose a scalable and efficient preference pair construction strategy that consistently enhances model performance as sample size increases.

In this paper, we focus on the construction of preference pairs given sufficient sample budgets by scaling up the number of samples per prompt.
We first construct preference pairs following the pipeline described above and train policy models.
However, increasing the number of samples does not lead to any significant performance improvement and can even result in a decline, as shown in Figure~\ref{dy_wr}.
As preference pairs play a pivotal role in DPO training, we then investigate the construction of preference data based on the underlying normal distribution of rewards.
Specifically, we classify the samples per prompt into seven categories and construct 21 preference datasets. 
We systematically explore the performance of trained models through DPO with each preference dataset and observe consistent empirical results across different settings.
Finally, we propose a simple preference data construction strategy that can steadily improve the performance of trained models through DPO if we increase the number of samples.

%\paragraph{Research Objectives.} In this paper, we focus on the construction of preference pairs given sufficient sample budgets by scaling up the number of samples per prompt.
%We first construct preference pairs following the pipeline described above and train policy models.
%However, increasing $n$ does not lead to any significant performance improvement and can even result in a decline, as shown in Figure~\ref{dy_wr}.
%To address this, we first investigate the construction of preference data based on the underlying normal distribution of rewards, as preference pairs play a pivotal role in DPO training.
%Specifically, we classify the samples per prompt into 7 categories and construct 21 preference datasets. 
%We then thoroughly explore the performance of trained models through DPO with each preference dataset and observe consistent empirical results across different settings.
%Finally, we propose a simple preference data construction strategy that can steadily improve the performance of trained models through DPO if we increase the number of samples.


\paragraph{Contributions.} 
This work makes the following key contributions:
\begin{itemize}
    \item We point out that the conventional preference pair construction strategy fails to improve the performance of models when increasing the number of samples per prompt.
    
    \item We for the first time construct preference pairs based on the distribution of rewards and explore their effects on policy models. We find that selecting the rejected response at reward position $\mu - 2\sigma$ is a key factor for optimal results.

    \item We propose a scalable preference pair construction method to improve the performance of models with an increasing number of samples per prompt. Its effectiveness can be further demonstrated by comparing with previous work.
\end{itemize}

%\paragraph{Contributions.} To summarize: 
%\begin{itemize}
%    \item We point out that the conventional preference pair construction strategy fails to improve the performance of models when increasing the number of samples per prompt.
    
%    \item We for the first time construct preference pairs based on the distribution of rewards and explore their effects on policy models. Selecting the rejected response at reward position $\mu - 2\sigma$ is a key factor for promising results.

%    \item We propose a simple preference pair construction approach to improve the performance of models with an increasing number of samples per prompt.

%\end{itemize}




















