\begin{abstract}

% Reinforcement Learning from Human Feedback (RLHF) has become a standard approach for aligning Large Language Models (LLMs). 
% % Direct preference optimization~(DPO) has been widely adopted for aligning large language models~(LLMs). One commonly used training pipeline is to use a reward model to build preference data from on-policy samples. In this work, we aim to scale up the number of on-policy samples and 
% To improve the instruction-following capability of LLMs, a reward model is often used to construct preference datasets with on-policy data for RLHF method such as DPO. 
% This procedure typically involves pairing the sample with the highest reward as the chosen response and the one with the lowest reward as the rejected response. 
% However, construction strategies of preference pairs have so far been relatively underexplored. 
% In this paper, we observe that this conventional preference data construction strategy will fail to improve the performance of trained models when we increase the number of samples from which the chosen and rejected responses will be selected. 
% Motivated by this finding,  we further classify the samples into 7 categories based on the distribution of rewards per prompt and construct 21 preference datasets as a whole. 
% We thoroughly explore the performance of trained models through DPO with each preference dataset and obtain consistent empirical results across different settings.
% Finally, we propose a simple preference data construction strategy which can steadily improve the performance of trained models through DPO if given an increasing number of samples. 



% Iterative data generation and model retraining has been widely used to align large language models~(LLMs). It uses the policy model to generate on-policy responses and a learned reward model to select the data for model training. Direct Preference Optimization~(DPO) further enhances such a training pipeline by constructing a pair of chosen and rejected samples. In this work, we are interested in how scaling the number of on-policy samples can affect the final outcome. The most common and straightforward approach is to select the largest and smallest rewards as chosen and rejected pairs, respectively. However, through experiments, we first discovered that when scaling the number of samples, this method leads to a decline in training performance. 
% We then explored the issue of preference data construction as we scaled the number of samples. We considered the problem of pair construction based on the reward distribution of the samples. The reward distribution of the samples follows a normal distribution. On this distribution, we identified 7 positions that sufficiently cover the entire range. We then explored all possible combinations (\(C_7^2\)). 
% We tested four different models and evaluated their performance using AlpacaEval. Our findings revealed that constructing pairs from two positions with greater differences in the distribution results in better performance. Based on these results, we proposed a simple construction method that effectively improves the model's training performance as the sample size increases. 


Iterative data generation and model retraining are widely used to align large language models (LLMs).
It typically involves a policy model to generate on-policy responses and a reward model to guide training data selection. 
Direct Preference Optimization (DPO) further enhances this process by constructing preference pairs of chosen and rejected responses. 
In this work, we aim to \emph{scale up} the number of on-policy samples via repeated random sampling to improve alignment performance. 
Conventional practice selects the sample with the highest reward as chosen and the lowest as rejected for DPO. 
However, our experiments reveal that this strategy leads to a \emph{decline} in performance as the sample size increases. 
To address this, we investigate preference data construction through the lens of underlying normal distribution of sample rewards. 
We categorize the reward space into seven representative points and systematically explore all 21 ($C_7^2$) pairwise combinations. 
Through evaluations on four models using AlpacaEval 2, we find that selecting the rejected response at reward position $\mu - 2\sigma$ rather than the minimum reward, is crucial for optimal performance. 
We finally introduce a scalable preference data construction strategy that consistently enhances model performance as the sample scale increases.


%Iterative data generation and model retraining have been widely used to align large language models (LLMs). It leverages a policy model to generate on-policy responses and a learned reward model to select data for training. Direct Preference Optimization (DPO) further enhances this pipeline by constructing pairs of chosen and rejected samples. In this work, we aim to \emph{scale} the number of on-policy samples via repeated random sampling to improve the final outcomes. For DPO training, the common approach pairs samples with the highest rewards as chosen and the lowest as rejected. However, experiments reveal this method leads to a \emph{decline} in performance as the sample size grows. To address this, we examine the construction of preference data from a large scale of samples. We propose to look into the data from the view of its underlying normal distribution. To comprehensively cover the distribution, we divide the interval into 7 distinct points, representing the entire range effectively.  We then explore all possible $21$~(\(C_7^2\)) pairwise combinations of these positions.  Evaluating four models with AlpacaEval 2, we find that selecting the rejected sample at reward position $\mu - 2\sigma$ rather than the one of minimal reward is a key factor to yield the most promising results.  We finally propose a simple data construction method that can effectively improve the model's performance as the sample scale increases.





\end{abstract}




