\section{Conclusion}
\label{conclusion}

% In this paper, we first point out the failure case of a conventional preference data construction strategy for DPO.
% We then 
% By categorizing responses into seven statistical points, we construct 21 preference datasets and systematically evaluate their impact on model performance. 
% Our findings demonstrate that selecting the rejected response at reward position $\mu - 2\sigma$ is critical to optimal performance. 
% We finally introduce a scalable preference data construction strategy that consistently improves model performance as the sample size increases.  

We first point out the failure case of a conventional preference data construction strategy for DPO. 
To address this,  we then classify the samples into seven categories based on the underlying normal distribution of rewards per prompt and construct 21 preference datasets as a whole to systematically evaluate their impact on model performance.  
Our findings demonstrate that selecting the rejected response at reward position $\mu - 2\sigma$ is critical for effective optimization of DPO.
We finally propose a simple preference data construction strategy that can steadily improve the performance of trained models through DPO as the sample scale increases.


