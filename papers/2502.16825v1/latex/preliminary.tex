



\section{Case Study on Conventional Preference Data Construction}


% \paragraph{A Conventional Preference Data Construction Practice.} 
% Given a language model policy $\pi_{\theta}$,  a reward function $r$ and $k$ prompts $\left\{x_i\right\}_{i=1}^k$, we sample $n$ generations $\left\{y_{ij}\right\}_{j=1}^n$ for $i$-th prompt from $\pi_{\theta}$. 
%  We then ask the reward function to score the sampled generations.
%  The reward of $n$ candidate samples of $i$-th prompt is $\left\{r_{ij}\right\}_{j=1}^n$.
%  The completion of the highest reward score $\max_{j=1}^{n} \{r_{ij}\}$
%  is selected as the chosen response, while the completion of the lowest reward score $\min_{j=1}^{n} \{r_{ij}\}$ is selected as the rejected response to construct a preference pair $(y_w^{(i)}, y_l^{(i)})$ for $x_i$. 
%  In practice, $n=5$ can achieve significant performance gains.
%  We explore the effects of increasing the number of samples, $n$, in the part.
In this part, we follow the preference data strategy described in Section~\ref{conven_pipe} to construct preference data and train policy models.

\subsection{Experiment Setup}
\label{imp_detail}

We follow the setup in \citet{meng2024simpo} to conduct experiments using two setups: \textbf{Base} and \textbf{Instruct}. For Base setting, we first fine-tune Llama-3-8B and Mistral-7B-v0.1 on the UltraChat-200k dataset~\cite{ding2023enhancing} to obtain the SFT model. For Instruct setting, we use Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 as the SFT models.  
With these SFT models, we sample responses for prompts from UltraFeedback~\cite{pmlr-v235-cui24f}, constructing preference datasets as described in Section~\ref{conven_pipe}. 
We then use the preference dataset scored with the Absolute-Rating Multi-Objective Reward Model (Armorm)~\cite{wang2024arithmetic} to train the SFT model via DPO~\cite{rafailov2023direct}.  
For sampling, we use a temperature of 0.8 and scale the number of samples from 5 to 200. 
We employ vLLM~\cite{kwon2023efficient} for efficient inference.
Following \citet{meng2024simpo}, we set the evaluation temperature to 0.8 for Llama-3-8B and Llama-3-8B-Instruct, while using a temperature of 0.7 for Mistral-7B-v0.1 and 0.5 for Mistral-7B-Instruct-v0.2. 
Additional training hyperparameters are provided in Appendix~\ref{appendix_hyper}.


We mainly evaluate models on \textbf{AlpacaEval 2}~\cite{alpaca_eval, dubois2024lengthcontrolled}, which is the most widely used benchmark for instruction following.
AlpacaEval 2 consists of 805 questions from multiple domains and tasks, which enables a comprehensive assessment of LLMs.
We report both \textit{win rate} and \textit{length-controlled win rate} results (\textit{LC win rate}) for it.  
  


%We follow \citet{meng2024simpo} perform experiments on two setups, \textbf{Base} and \textbf{Instruct}.
%For the base setting, we first train Meta-Llama-3-8B and Mistral-7B-v0.1 on UltraChat-200k dataset~\cite{ding2023enhancing} to obtain the SFT model. 
%For the Instruct setting, we use Meta-Llama-3-8B-Instruct and Mistral-7B-Instruct-v0.2 as the SFT model.
%With the SFT model at hand, we then perform sampling for prompts in Ultrafeedback and construct a preference dataset as described in Section~\ref{conven_pipe}.
%Afterwards, we use the dataset constructed with Absolute-Rating Multi-Objective reward model (Armorm)~\cite{wang2024arithmetic} to train the SFT model through DPO~\cite{rafailov2023direct}.
%\emph{For sampling, we use temperature 0.8 and scale the number of samples from 5 to 200.}
%We use vllm~\cite{kwon2023efficient} for its high efficiency.
%We evaluate the policy model on AlpacaEval 2~\cite{alpaca_eval, dubois2024lengthcontrolled}.
%And both win rate and length-controlled win rate are reported.
%We also follow \citet{meng2024simpo} to use temperature 0.8 for the evaluation of Meta-Llama-3-8B and Meta-Llama-3-8B-Instruct.
%The temperature is 0.7 on Mistral-7B-v0.1 and 0.5 on Mistral-7B-Instruct-v0.2 for evaluation.
%More hyperparameters about training can be found in Appendix~\ref{appendix_hyper}.




\subsection{Experiment Results} 
The experimental results are presented in Figure~\ref{dy_wr}. 
As the number of samples increases, the performance of trained models, measured by both win rate and LC win rate, exhibits fluctuations rather than consistent improvements in the base setting of Llama. 
In the instruct setting of Llama, the degradation is even more pronounced, with performance declining as the sample size increases. 
A similar trend is observed for Mistral-7B-v0.1. 
Mistral-7B-Instruct-v0.2 shows a slight improvement before eventually declining, reinforcing the instability of this conventional preference pair construction method.  

These results indicate that max-min construction strategy can not improve alignment of LLMs as the sample size increases.  
This finding highlights a limitation in existing preference construction strategies and motivates the need for alternate approaches to constructing preference pairs, particularly when ample samples are available.  


%\paragraph{Results.} 
%We record experimental results in Figure~\ref{dy_wr}. 
%As we increase the number of samples, the performance of trained models, both win rate and length-controlled win rate, fluctuates in the base setting of Llama.
%In the instruct setting of Llama, the situation is even worse, with performance dramatically declining as we increase the sample budget.
%Furthermore, similar results are observed for Mistral-7B-v0.1.
%The only exception is Mistral-7B-Instruct-v0.2, which shows some improvement before eventually declining.
%In conclusion, none of the results show consistent improvement.
%The above results indicate that simply selecting the response of the highest reward and the response of the lowest reward in $n$ samples to construct preference pairs for DPO fails to improve the alignment capability of LLMs.
%Meanwhile, it also motivates us to explore how to construct preference pairs if we have sufficient sample budgets.

\input{figuretexs/reward_normal}