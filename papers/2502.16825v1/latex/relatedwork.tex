\section{Related Work}

\paragraph{Reinforcement Learning from Human Feedback.} 
RLHF is a dominant approach to align large language models with human preferences in the generation of natural language~\cite{NEURIPS2022_b1efde53, touvron2023llama2openfoundation}. 
The RLHF process generally involves three stages: initial supervised fine-tuning, reward modeling~\cite{lambert2024rewardbenchevaluatingrewardmodels}, and policy optimization. 
This approach has been extended to address various challenges such as reducing toxicity, improving safety and reasoning capabilities~\cite{qi2024safetyalignmentjusttokens, wu2023finegrained, dai2024safe, yu2024metamath}. 
However, RLHF has issues such as training instability and complexities due to the nature of reinforcement learning and multi-stage pipeline, potentially leading to biases and verbose model outputs.

DPO~\cite{rafailov2023direct} and its variants~\cite{meng2024simpo, pmlr-v235-ethayarajh24a, han2024fpogeneralizingpreferenceoptimization} were developed to overcome these limitations by directly integrating preferences into the policy model using pairwise preference data. 
This approach simplifies the policy optimization process by eliminating the need for a surrogate reward phase.
As an increasing number of powerful reward models become publicly available~\cite{jiang-etal-2023-llm, wang-etal-2024-interpretable, wang2024arithmetic, liu2024skyworkrewardbagtricksreward}, a popular practice~\cite{dong2023raft, liu2024statistical, meng2024simpo, ye-ng-2024-preference} has gained focus for models to enhance capability training through DPO, which employ reward models to select self-generated samples.


\paragraph{Synthetic Data for LLMs.} 
Human-curated data has consistently been a highly effective resource to enhance model performance in natural language processing~\cite{bai2022traininghelpfulharmlessassistant, kpf2023openassistant}.
Although human-curated data are typically of high quality, obtaining sufficient amounts can be prohibitively expensive. 
To address this challenge, the use of synthetic data has gained attention as a cost-effective alternative to human data~\cite{west-etal-2022-symbolic, hsieh-etal-2023-distilling, wang-etal-2023-self-instruct, dong2024selfboostinglargelanguagemodels, li2024selfalignment}.
This approach often involves the use of advanced LLMs to generate high-quality synthetic datasets~\cite{tajwar2024preferencefinetuningllmsleverage, dong2023raft, agarwal2024onpolicy}.
In particular, on-policy data has emerged as a highly effective and efficient approach, drawing considerable interest in recent work.
Previous work mainly aims to generate more high-quality data to improve the capabilities of LLMs.
In this paper, we focus on how to use self-generated data to construct optimal preference pairs for DPO.



\paragraph{Scaling Inference.}
Recently, many studies have explored how scaling inference (sample budgets) impacts the performance of large language models~\cite{wu2024scaling, brown2024largelanguagemonkeysscaling, snell2024scalingllmtesttimecompute, zhang2024scalingllminferenceoptimized}. 
They have shown that increasing the number of samples improves the solve rate with a simple Best-of-N strategy~\cite{amini2024variationalbestofnalignment} with a powerful reward. 
Specifically, improvements in the performance of these models in mathematical problems have been achieved by repeatedly sampling potential solutions by manipulating temperatures. 
Research has also been conducted to investigate the scaling effects related to various inference algorithms, reward functions, and model sizes~\cite{chi2024thoughtsculptreasoningintermediaterevision, wu2024scaling}.
% Unlike those studies, we investigate effective strategies for constructing preference data to optimize large language models for better alignment when sufficient sample budgets are available.