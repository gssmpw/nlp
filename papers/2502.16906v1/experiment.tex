\definecolor{deepgreen}{RGB}{0, 70, 0}
\definecolor{backgreen}{RGB}{226, 240, 217}
\newcommand{\highg}{\cellcolor{backgreen}}

% \section{Experiment}

\begin{table*}[!htbp]
    \centering
    \small
    % \resizebox{\linewidth}{!}{
    \begin{tabular}{cccccc}
    \toprule
\multirow{2}{*}{\textbf{}} & \multicolumn{2}{c}{\textbf{Source Corpus}} & \textbf{Stage 1} & \textbf{Stage 2} & \textbf{Stage3} \\ \cmidrule{2-3} 
                           & \textbf{\# Backgrounds}    & \textbf{\# Samples}   & \textbf{\# Sample}  & \textbf{\# Samples}  & \textbf{\# Samples} \\ \midrule
EN                         & 40                     & 230               & 210              & 206              & 1575            \\
ZH                         & 90                     & 123               & 147              & 139              & 883            \\ \bottomrule
\end{tabular}
% }
    \caption{Statistics of our testing data. The AutoLogi benchmark corresponds to the sample number of stage 2, and the AutoLogi (Augumented) corresponds to stage 3.
 }
    \label{tab:statistics}
    \vskip -0.1in
\end{table*}

\section{Dataset Construction}
%\cqy{Feels like overlap with Section 3 Methods, maybe put it to Experiment-Implementation Details.}
To construct the AutoLogi dataset, we leveraged two established logical reasoning datasets: AR-LSAT~\cite{zhong2021ar} and LogiQA~\cite{liu2020logiqachallengedatasetmachine}. Both datasets comprise multiple-choice questions designed to assess logical reasoning capabilities. The complete set of prompts utilized in our study is detailed in Section~\ref{sec:appendix-prompt}.

\paragraph{Testing Set Construction}
We applied our proposed method to construct the testing set through three stages, with detailed statistics presented in Table~\ref{tab:statistics}.

First, we employed GPT-4 to filter suitable samples from the source corpus, resulting in 230 English questions from AR-LSAT (40 distinct backgrounds) and 123 Chinese questions from LogiQA (90 backgrounds). In stage 1, we applied Puzzle Formulation to transform these multiple-choice questions into open-ended puzzles. To maximize coverage, we created separate cases by pairing each question stem with individual options. After deduplication, this yielded 210 English and 147 Chinese open-ended logic puzzles.

In stage 2, we utilized both GPT-4 and GPT-4o to generate format requirements and verifiers, as their complementary capabilities helped address model-specific limitations. %This process consumed approximately 18 million tokens for code generation, verification, and necessary regeneration, producing 206 English and 139 Chinese puzzles. 
Finally, in stage 3, we applied data augmentation techniques to expand our dataset to 1,575 English and 883 Chinese puzzles.






\paragraph{Training Set Construction}

We utilize the cleaned training sets from LogiQA and AR-LSAT as source corpora, applying our proposed synthesis method to generate 1,675 Chinese samples and 5,064 English samples as $D_{training}$. The rejection sampling process involves two models: Qwen2.5-7b-instruct and Qwen2.5-72b-instruct, each performing rejection sampling with 8 rounds per input to obtain corresponding candidate responses.

By employing our verifier to evaluate these candidate responses, we naturally derive two types of training datasets:
\begin{itemize}
    \item $D_{sft}$: A collection of responses verified as correct, serving as high-quality demonstrations for supervised fine-tuning.
    \item $D_{dpo}$: Pairs of correct and incorrect responses, forming natural positive-negative sample pairs (x, y\textsubscript{w}, y\textsubscript{l}) for DPO.
\end{itemize}

\begin{table}[!t]
    \centering
    \small
    \resizebox{\linewidth}{!}{
    \begin{tabular}{ccccc}
    \toprule
\textbf{}   & $D_{\text{training}}$ & $D_{\text{dpo}}$(7b) & $D_{\text{dpo}}$(72b) & $D_{\text{sft}}$(72b)  \\
\midrule
\textbf{EN} & 5064                  & 2877               & 2349                & 3724                 \\
\textbf{CN} & 1675                  & 901                & 621                 & 1170                  \\ \bottomrule    
\end{tabular}
}
    \caption{Statistics of our training data. 
 }
    \label{tab:statistics_training}
    \vskip -0.1in
\end{table}
Through eight rounds of rejection sampling, as presented in Table~\ref{tab:statistics_training}, we constructed two datasets: $D_{\text{sft}}$ and $D_{\text{pref}}$. $D_{\text{sft}}$ was formed by excluding examples without correct solutions, while $D_{\text{pref}}$ was further refined by removing instances lacking contrastive pairs. Given their trivial nature, single-constraint puzzles were excluded from both datasets.

\section{Experiment}
\subsection{Experimental Setup}

\paragraph{Models}
We evaluate 8 models in our experiments: Qwen2.5-7B/72B-Instruct~\cite{qwen2025qwen25technicalreport}, LLama3.1-8B/70B/405B-Instruct~\cite{dubey2024llama3herdmodels}, GPT-3.5-Turbo~\cite{ChatGPT}, GPT-4o-2024-08-06~\cite{openai2024gpt4technicalreport}, and Claude-3.5-Sonnet-20240620~\cite{cluade}.

\paragraph{Evaluation Datasets}
We conduct evaluations on two versions of our AutoLogi benchmark: the base version (AutoLogi) and its augmented version (AutoLogi Augmented). We also evaluate on the original multiple-choice datasets that served as our source corpus - AR-LSAT~\cite{zhong2021ar} and LogiQA~\cite{liu2020logiqachallengedatasetmachine}.
To analyze the reliability of our benchmark, we compare the performance patterns with two established reasoning benchmarks: MUSR~\cite{sprague2024musrtestinglimitschainofthought} and LiveBench (Reasoning, 2024-08-31)~\cite{white2024livebench}. 
%Additionally,  %- to assess the effectiveness of our training approach.
For all evaluations, we report the mean and standard deviation across 5 independent runs to ensure reliable results.

% \begin{table*}[!ht]
% \centering
% \small
% \resizebox{\textwidth}{!}{
% \setlength{\tabcolsep}{4pt}
% \begin{tabular}{cccccccccc}
% \toprule

% \multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{MUSR}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{LiveBench}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{AR-LSAT}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{LogiQA}}} & \multicolumn{2}{c}{\textbf{AutoLogi}}                             & \multicolumn{2}{c}{\textbf{AutoLogi(Augmented)}}                  \\ \cmidrule(lr){6-7} \cmidrule(lr){8-9}
% \multicolumn{1}{c}{}                                & \multicolumn{1}{c}{}                               & \multicolumn{1}{c}{}                                    & \multicolumn{1}{c}{}                                  & \multicolumn{1}{c}{}                                 & \multicolumn{1}{c}{\textbf{EN}} & \multicolumn{1}{c}{\textbf{CN}} & \multicolumn{1}{c}{\textbf{EN}} & \multicolumn{1}{c}{\textbf{CN}} \\ \midrule
% Qwen2.5-7b-instruct & $\text{47.14}_{\textcolor{deepgreen}{\pm\text{0.73}}}$ & $\text{30.67}^{\dagger}$ & $\text{22.70}_{\textcolor{deepgreen}{\pm\text{0.84}}}$ & $\text{34.42}_{\textcolor{deepgreen}{\pm\text{1.38}}}$ & $\text{27.28}_{\textcolor{deepgreen}{\pm\text{2.41}}}$ & $\text{30.94}_{\textcolor{deepgreen}{\pm\text{1.76}}}$ & $\text{43.64}_{\textcolor{deepgreen}{\pm\text{1.25}}}$ & $\text{42.08}_{\textcolor{deepgreen}{\pm\text{1.50}}}$ \\
% Qwen2.5-72b-instruct & $\text{54.21}_{\textcolor{deepgreen}{\pm\text{0.41}}}$ & $\text{46.00}^{\dagger}$ & $\text{31.65}_{\textcolor{deepgreen}{\pm\text{1.89}}}$ & \textbf{47.92}$_{\textcolor{deepgreen}{\pm\text{1.37}}}$ & $\text{53.50}_{\textcolor{deepgreen}{\pm\text{0.93}}}$ & $\text{54.10}_{\textcolor{deepgreen}{\pm\text{0.28}}}$ & $\text{68.18}_{\textcolor{deepgreen}{\pm\text{0.77}}}$ & $\text{63.92}_{\textcolor{deepgreen}{\pm\text{0.56}}}$ \\
% LLama3.1-8b-instruct & $\text{49.63}_{\textcolor{deepgreen}{\pm\text{0.58}}}$ & $\text{15.33}^{\dagger}$ & $\text{22.61}_{\textcolor{deepgreen}{\pm\text{2.47}}}$ & $\text{25.04}_{\textcolor{deepgreen}{\pm\text{1.40}}}$ & $\text{25.53}_{\textcolor{deepgreen}{\pm\text{3.64}}}$ & $\text{17.41}_{\textcolor{deepgreen}{\pm\text{3.61}}}$ & $\text{37.96}_{\textcolor{deepgreen}{\pm\text{1.41}}}$ & $\text{23.69}_{\textcolor{deepgreen}{\pm\text{1.33}}}$ \\
% LLama3.1-70b-instruct & $\text{57.28}_{\textcolor{deepgreen}{\pm\text{0.76}}}$ & $\text{40.67}^{\dagger}$ & $\text{30.26}_{\textcolor{deepgreen}{\pm\text{1.36}}}$ & $\text{36.25}_{\textcolor{deepgreen}{\pm\text{2.55}}}$ & $\text{53.79}_{\textcolor{deepgreen}{\pm\text{2.92}}}$ & $\text{42.59}_{\textcolor{deepgreen}{\pm\text{2.41}}}$ & $\text{62.47}_{\textcolor{deepgreen}{\pm\text{0.96}}}$ & $\text{53.77}_{\textcolor{deepgreen}{\pm\text{0.95}}}$ \\
% LLama3.1-405b-instruct & $\text{57.68}_{\textcolor{deepgreen}{\pm\text{0.67}}}$ & $\text{53.33}^{\dagger}$ & $\text{32.61}_{\textcolor{deepgreen}{\pm\text{1.94}}}$ & $\text{39.35}_{\textcolor{deepgreen}{\pm\text{1.10}}}$ & $\text{59.42}_{\textcolor{deepgreen}{\pm\text{3.04}}}$ & $\text{56.98}_{\textcolor{deepgreen}{\pm\text{1.79}}}$ & $\text{70.43}_{\textcolor{deepgreen}{\pm\text{1.39}}}$ & $\text{65.39}_{\textcolor{deepgreen}{\pm\text{1.07}}}$ \\
% GPT-3.5-Turbo & $\text{48.33}_{\textcolor{deepgreen}{\pm\text{1.07}}}$ & $\text{26.67}^{\dagger}$ & $\text{21.04}_{\textcolor{deepgreen}{\pm\text{1.71}}}$ & $\text{26.34}_{\textcolor{deepgreen}{\pm\text{4.13}}}$ & $\text{18.93}_{\textcolor{deepgreen}{\pm\text{3.83}}}$ & $\text{22.30}_{\textcolor{deepgreen}{\pm\text{2.39}}}$ & $\text{35.25}_{\textcolor{deepgreen}{\pm\text{0.81}}}$ & $\text{34.47}_{\textcolor{deepgreen}{\pm\text{1.39}}}$ \\
% GPT-4o-2024-08-06 & $\text{59.95}_{\textcolor{deepgreen}{\pm\text{1.13}}}$ & $\text{54.67}^{\dagger}$ & \textbf{37.39}$_{\textcolor{deepgreen}{\pm\text{0.91}}}$ & $\text{43.74}_{\textcolor{deepgreen}{\pm\text{1.30}}}$ & \textbf{62.04}$_{\textcolor{deepgreen}{\pm\text{1.59}}}$ & $\text{55.54}_{\textcolor{deepgreen}{\pm\text{3.78}}}$ & \textbf{72.61}$_{\textcolor{deepgreen}{\pm\text{0.76}}}$ & $\text{66.70}_{\textcolor{deepgreen}{\pm\text{1.25}}}$ \\
% Claude-3.5-sonnet & \textbf{60.53}$_{\textcolor{deepgreen}{\pm\text{0.63}}}$ & \textbf{58.68}$^{\dagger}$ & $\text{34.35}_{\textcolor{deepgreen}{\pm\text{1.76}}}$ & 46.83$_{\textcolor{deepgreen}{\pm\text{3.24}}}$ & $\text{60.97}_{\textcolor{deepgreen}{\pm\text{0.88}}}$ & \textbf{60.29}$_{\textcolor{deepgreen}{\pm\text{3.19}}}$ & $\text{72.53}_{\textcolor{deepgreen}{\pm\text{0.82}}}$ & \textbf{68.24}$_{\textcolor{deepgreen}{\pm\text{0.98}}}$ \\ \bottomrule
% \end{tabular}
% }
% \caption{Results on the original multiple-choice datasets (AR-LSAT, LogiQA), and our benchmarks (AutoLogi and Augmented AutoLogi). The accuracy was reported with a standard deviation (std) of 5 trials. Results marked with $^{\dagger}$ are sourced from the LiveBench leaderboard(2024-08-31)..
% }
% \label{table_main}
% \end{table*}

\begin{table*}[!ht]
\centering
\small
\resizebox{\textwidth}{!}{
\setlength{\tabcolsep}{4pt}
\begin{tabular}{cccccccccc}
\toprule

\multicolumn{1}{c}{\multirow{2}{*}{\textbf{Model}}} & \multicolumn{2}{c}{\textbf{AutoLogi}} & \multicolumn{2}{c}{\textbf{AutoLogi(Augmented)}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{AR-LSAT}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{LogiQA}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{MUSR}}} & \multicolumn{1}{c}{\multirow{2}{*}{\textbf{LiveBench}}} \\ \cmidrule(lr){2-3} \cmidrule(lr){4-5}
\multicolumn{1}{c}{} & \multicolumn{1}{c}{\textbf{EN}} & \multicolumn{1}{c}{\textbf{CN}} & \multicolumn{1}{c}{\textbf{EN}} & \multicolumn{1}{c}{\textbf{CN}} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} & \multicolumn{1}{c}{} \\ \midrule
Qwen2.5-7b-instruct & $\text{27.28}_{\textcolor{deepgreen}{\pm\text{2.41}}}$ & $\text{30.94}_{\textcolor{deepgreen}{\pm\text{1.76}}}$ & $\text{43.64}_{\textcolor{deepgreen}{\pm\text{1.25}}}$ & $\text{42.08}_{\textcolor{deepgreen}{\pm\text{1.50}}}$ & $\text{22.70}_{\textcolor{deepgreen}{\pm\text{0.84}}}$ & $\text{34.42}_{\textcolor{deepgreen}{\pm\text{1.38}}}$ & $\text{47.14}_{\textcolor{deepgreen}{\pm\text{0.73}}}$ & $\text{30.67}^{\dagger}$ \\
Qwen2.5-72b-instruct & $\text{53.50}_{\textcolor{deepgreen}{\pm\text{0.93}}}$ & $\text{54.10}_{\textcolor{deepgreen}{\pm\text{0.28}}}$ & $\text{68.18}_{\textcolor{deepgreen}{\pm\text{0.77}}}$ & $\text{63.92}_{\textcolor{deepgreen}{\pm\text{0.56}}}$ & $\text{31.65}_{\textcolor{deepgreen}{\pm\text{1.89}}}$ & \textbf{47.92}$_{\textcolor{deepgreen}{\pm\text{1.37}}}$ & $\text{54.21}_{\textcolor{deepgreen}{\pm\text{0.41}}}$ & $\text{46.00}^{\dagger}$ \\
LLama3.1-8b-instruct & $\text{25.53}_{\textcolor{deepgreen}{\pm\text{3.64}}}$ & $\text{17.41}_{\textcolor{deepgreen}{\pm\text{3.61}}}$ & $\text{37.96}_{\textcolor{deepgreen}{\pm\text{1.41}}}$ & $\text{23.69}_{\textcolor{deepgreen}{\pm\text{1.33}}}$ & $\text{22.61}_{\textcolor{deepgreen}{\pm\text{2.47}}}$ & $\text{25.04}_{\textcolor{deepgreen}{\pm\text{1.40}}}$ & $\text{49.63}_{\textcolor{deepgreen}{\pm\text{0.58}}}$ & $\text{15.33}^{\dagger}$ \\
LLama3.1-70b-instruct & $\text{53.79}_{\textcolor{deepgreen}{\pm\text{2.92}}}$ & $\text{42.59}_{\textcolor{deepgreen}{\pm\text{2.41}}}$ & $\text{62.47}_{\textcolor{deepgreen}{\pm\text{0.96}}}$ & $\text{53.77}_{\textcolor{deepgreen}{\pm\text{0.95}}}$ & $\text{30.26}_{\textcolor{deepgreen}{\pm\text{1.36}}}$ & $\text{36.25}_{\textcolor{deepgreen}{\pm\text{2.55}}}$ & $\text{57.28}_{\textcolor{deepgreen}{\pm\text{0.76}}}$ & $\text{40.67}^{\dagger}$ \\
LLama3.1-405b-instruct & $\text{59.42}_{\textcolor{deepgreen}{\pm\text{3.04}}}$ & $\text{56.98}_{\textcolor{deepgreen}{\pm\text{1.79}}}$ & $\text{70.43}_{\textcolor{deepgreen}{\pm\text{1.39}}}$ & $\text{65.39}_{\textcolor{deepgreen}{\pm\text{1.07}}}$ & $\text{32.61}_{\textcolor{deepgreen}{\pm\text{1.94}}}$ & $\text{39.35}_{\textcolor{deepgreen}{\pm\text{1.10}}}$ & $\text{57.68}_{\textcolor{deepgreen}{\pm\text{0.67}}}$ & $\text{53.33}^{\dagger}$ \\
GPT-3.5-Turbo & $\text{18.93}_{\textcolor{deepgreen}{\pm\text{3.83}}}$ & $\text{22.30}_{\textcolor{deepgreen}{\pm\text{2.39}}}$ & $\text{35.25}_{\textcolor{deepgreen}{\pm\text{0.81}}}$ & $\text{34.47}_{\textcolor{deepgreen}{\pm\text{1.39}}}$ & $\text{21.04}_{\textcolor{deepgreen}{\pm\text{1.71}}}$ & $\text{26.34}_{\textcolor{deepgreen}{\pm\text{4.13}}}$ & $\text{48.33}_{\textcolor{deepgreen}{\pm\text{1.07}}}$ & $\text{26.67}^{\dagger}$ \\
GPT-4o-2024-08-06 & \textbf{62.04}$_{\textcolor{deepgreen}{\pm\text{1.59}}}$ & $\text{55.54}_{\textcolor{deepgreen}{\pm\text{3.78}}}$ & \textbf{72.61}$_{\textcolor{deepgreen}{\pm\text{0.76}}}$ & $\text{66.70}_{\textcolor{deepgreen}{\pm\text{1.25}}}$ & \textbf{37.39}$_{\textcolor{deepgreen}{\pm\text{0.91}}}$ & $\text{43.74}_{\textcolor{deepgreen}{\pm\text{1.30}}}$ & $\text{59.95}_{\textcolor{deepgreen}{\pm\text{1.13}}}$ & $\text{54.67}^{\dagger}$ \\
Claude-3.5-sonnet & $\text{60.97}_{\textcolor{deepgreen}{\pm\text{0.88}}}$ & \textbf{60.29}$_{\textcolor{deepgreen}{\pm\text{3.19}}}$ & $\text{72.53}_{\textcolor{deepgreen}{\pm\text{0.82}}}$ & \textbf{68.24}$_{\textcolor{deepgreen}{\pm\text{0.98}}}$ & $\text{34.35}_{\textcolor{deepgreen}{\pm\text{1.76}}}$ & 46.83$_{\textcolor{deepgreen}{\pm\text{3.24}}}$ & \textbf{60.53}$_{\textcolor{deepgreen}{\pm\text{0.63}}}$ & \textbf{58.68}$^{\dagger}$ \\ \bottomrule
\end{tabular}
}
\caption{Results on the original multiple-choice datasets (AR-LSAT, LogiQA), and our benchmarks (AutoLogi and Augmented AutoLogi). The accuracy was reported with a standard deviation (std) of 5 trials. Results marked with $^{\dagger}$ are sourced from the LiveBench leaderboard(2024-08-31).
}
\label{table_main}
\end{table*}




\begin{table*}[!ht]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{\textbf{Model}} & \multicolumn{2}{c}{\textbf{AutoLogi}} & \multirow{2}{*}{\textbf{AR-LSAT}} & \multirow{2}{*}{\textbf{LogiQA}} & \multirow{2}{*}{\textbf{MUSR}} & \multirow{2}{*}{\textbf{LiveBench}} \\ \cmidrule(lr){2-3}
& \textbf{EN} & \textbf{CN} & & & & \\
\midrule
% Qwen2.5-7b-instruct & & & & & & \\
\textit{Baseline} Qwen2.5-7b-instruct & $\text{43.64}$ & $\text{42.08}$ & $\text{22.70}$ & $\text{34.42}$ & $\text{47.14}$ & $\text{30.67}^{\dagger}$ \\
% \midrule
% \textit{Self-Alignment} & & & & & & \\
+\textit{Self-Alignment}  $DPO$ & \highg$\text{48.80}_{\textcolor{deepgreen}{\text{+5.16}}}$ & $\text{45.39}_{\textcolor{deepgreen}{\text{+3.31}}}$ & $\text{26.09}_{\textcolor{deepgreen}{\text{+3.39}}}$ & \highg$\text{38.05}_{\textcolor{deepgreen}{\text{+3.63}}}$ & $\text{47.57}_{\textcolor{deepgreen}{\text{+0.43}}}$ & \highg$\text{35.73}_{\textcolor{deepgreen}{\text{+5.06}}}$ \\
% \midrule
% \textit{Strong-to-Weak} & & & & & & \\
+\textit{Strong-to-Weak} $RFT$ & $\text{48.33}_{\textcolor{deepgreen}{\text{+4.69}}}$ & \highg$\text{47.54}_{\textcolor{deepgreen}{\text{+5.46}}}$ & \highg$\text{27.30}_{\textcolor{deepgreen}{\text{+4.60}}}$ & $\text{35.93}_{\textcolor{deepgreen}{\text{+1.51}}}$ & \highg$\text{49.15}_{\textcolor{deepgreen}{\text{+2.01}}}$ & $\text{30.07}_{\textcolor{deepgreen}{-\text{0.60}}}$ \\
\midrule
% \textit{Baseline} & & & & & & \\
\textit{Baseline} Qwen2.5-72b-instruct & $\text{68.18}$ & $\text{63.92}$ & $\text{31.65}$ & $\text{47.92}$ & $\text{54.21}$ & $\text{46.00}^{\dagger}$ \\
% \midrule
% \textit{Self-Alignment} & & & & & & \\
+\textit{Self-Alignment} $DPO$ & \highg$\text{74.79}_{\textcolor{deepgreen}{\text{+6.61}}}$ & \highg$\text{69.54}_{\textcolor{deepgreen}{\text{+5.62}}}$ & \highg$\text{38.70}_{\textcolor{deepgreen}{\text{+7.05}}}$ & \highg$\text{48.14}_{\textcolor{deepgreen}{\text{+0.22}}}$ & \highg$\text{56.48}_{\textcolor{deepgreen}{\text{+2.27}}}$ & \highg$\text{52.13}_{\textcolor{deepgreen}{\text{+6.13}}}$ \\
\bottomrule
\end{tabular}
\caption{Performance comparison across different training settings. Results report the average of five trials, with standard deviations shown in Section~\ref{sec:appendix-training-details}. Subscripted values indicate performance gains over the baseline model, and the best accuracy for each setup is highlighted in \colorbox{backgreen}{green}. Results marked with $^{\dagger}$ are sourced from the LiveBench leaderboard(2024-08-31).}
\label{tab:training-set-results}
\end{table*}


% \subsection{Main Results}
% We demonstrate the effectiveness of our synthesis method from two perspectives: its capability to construct high-quality logical reasoning benchmarks and its utility in generating training data for enhancing models' logical reasoning abilities.
\subsection{Benchmark Results}
We evaluate the effectiveness of our synthesis method in constructing high-quality logical reasoning benchmarks through extensive experiments across eight modern LLMs.
\paragraph{Superior Assessment Capability}
Experimental results in Table~\ref{table_main} demonstrate AutoLogi's distinct advantage in model capability assessment compared to traditional benchmarks. On conventional multiple-choice datasets (AR-LSAT, LogiQA), models achieve similar scores within a narrow range, making it difficult to distinguish their true capabilities. In contrast, both AutoLogi and Augmented AutoLogi reveal more pronounced performance gaps between models, enabling finer-grained capability assessment. For instance, while Qwen2.5-72b achieves unexpectedly high scores on LogiQA, this performance anomaly diverges from its relative standings across other benchmarks, suggesting potential evaluation bias in traditional tests. Notably, AutoLogi yields clear performance stratification among models, and this hierarchical distribution strongly correlates with established benchmarks (MUSR, LiveBench), validating its reliability as an evaluation framework.

\paragraph{Bilingual Evaluation Capability}
A key feature of AutoLogi is its provision of parallel English and Chinese evaluation benchmarks. Our analysis reveals that most models maintain comparable performance across both languages, indicating robust bilingual capabilities. However, we observe notable variations in cross-lingual performance: LLaMA-series models demonstrate significantly lower performance on Chinese tests compared to English, indicating limitations in Chinese language processing. In contrast, Claude, Qwen, and GPT3.5 exhibit more balanced performance across both languages, suggesting superior cross-lingual generalization ability. This bilingual evaluation dimension provides crucial insights into models' language-agnostic reasoning capabilities.

\subsection{Training Results}
We investigate the utility of our synthesized data in enhancing models' logical reasoning abilities through training experiments.

\paragraph{Training Settings}
We investigate two training settings. The first is \textbf{Self-Alignment}, where we use the training model itself to perform rejection sampling to generate preference data $D_{pref}$ (containing pairs of correct and incorrect answers), followed by reinforcement learning training using Direct Preference Optimization (DPO, \citealp{rafailov2024directpreferenceoptimizationlanguage}).
The second setting is \textbf{Strong-to-weak Distillation} based on RFT (Rejection sampling Fine-Tuning,  \citealp{yuan2023scalingrelationshiplearningmathematical}). In this approach, we leverage a more powerful model (Qwen2.5-72b-instruct) to perform rejection sampling to generate SFT data $D_{sft}$, which is then used for fine-tuning the target model.

\paragraph{Significant In-domain Improvements}
As shown in Table~\ref{tab:training-set-results}, our optimization methods achieve substantial improvements on the AutoLogi benchmark. Notably, the DPO method brings significant performance gains of 6.61\% and 5.62\% on English and Chinese tests respectively for Qwen2.5-72b, demonstrating the effectiveness of our optimization approach on the target tasks.

\paragraph{Broad Cross-domain Generalization}
Models trained on our synthesized data demonstrate strong generalization capability on tasks with similar problem context but entirely different problem formats. For instance, Qwen2.5-72b achieves a 7.05\% improvement on the AR-LSAT test set. The enhanced performance extends to tasks with substantially different source distributions, as evidenced by a 6.13\% performance gain on the LiveBench test set, thoroughly validating the immense potential of our proposed synthesis approach in generating high-quality training data to enhance models' general logical reasoning capabilities.

\paragraph{Relationship between Model Scale and Optimization Effects}
Larger models demonstrate more substantial improvements in performance. Across various benchmarks, calculating the mean improvements under the DPO setting reveals that 72B models achieve higher average performance gains (4.65\%) compared to their 7B counterparts (3.50\%). This suggests that models with stronger foundational capabilities can further unlock their logical reasoning potential when leveraging high-quality synthetic training data.

\subsection{Analysis}
\label{sec:analysis}

\paragraph{Augmentation Effectiveness Analysis}

As illustrated in Figure~\ref{fig:varying-difficulty}, experimental results demonstrate that all models experience accuracy degradation with increasing number of constraints (the number of logical rules per puzzle) and decreasing Solution Space Ratio (the proportion of valid solutions within the total possibility space, representing theoretical random-guess probability). The consistent decline in model accuracy indicates increasing problem difficulty, and the significant correlation between these metrics validates that our augmentation strategy successfully generates problems across \textbf{diverse difficulty levels}.
The baseline Autologi dataset exhibited inherent limitations in its difficulty distribution, as problems were exclusively derived from the source corpus. This dependency resulted in an unbalanced complexity spectrum, with a maximum of 6 constraints per problem and a sparse representation of higher-difficulty instances. Our augmentation methodology addresses these limitations by introducing a more \textbf{uniform distribution} of problem complexities, thereby creating a more balanced and representative dataset for logical reasoning evaluation.


\begin{figure}[!t]
% \vspace{-10pt}
  \centering
  \includegraphics[width=1.0\linewidth]{pic/across-varying-difficulties_final_cn-cropped.pdf}
  \caption{The question quantities on the Chinese subset of AutoLogi before and after data augmentation, and the accuracy of eight models across different constraints and solution space proportions. The figure of English subset can be found in Appendix~\ref{appendix:en}.}
  % The top subfigures show changes in question quantities distribution after augmentation, across different constraints and solution space proportions. The bottom subfigures analyze the accuracy of eight models, indicating that as constraints increase and solution space widens, problem difficulty rises and model performance declines. 
  \label{fig:varying-difficulty}
% \vspace{-10pt}
\end{figure}

% \paragraph{Validation of Solution Existence}
\paragraph{Cross-Validated Solution Existence Analysis}
\label{sec:solution-exist}
To ensure dataset quality, we incorporate traversal functions in Stage 2 to verify the existence of valid solution paths. This verification mechanism proves essential in detecting incorrect generated verifiers and invalid problems before they enter our dataset. For instance, in our experiments with the AutoLogi Chinese subset, among the 139 problems initially generated from 147 information units, 23\% were identified as unsolvable(11\% in English subset). Furthermore, during Stage 3's augmentation process, where additional logical constraints are introduced, the traversal function becomes indispensable as each constraint addition risks creating unsolvable problems, with approximately 30\% of LLM-generated additional constraints leading to unsolvable puzzles. Through this cross-validation method, we effectively maintain dataset integrity by eliminating invalid problems throughout our pipeline.


\paragraph{Human Alignment Analysis}
\label{sec:human-alignment}
To evaluate the effectiveness of program-based Verifier as an evaluation tool, we conducted a comparative experiment focusing on human alignment. We randomly selected 90 AutoLogi reasoning problems from diverse backgrounds and utilized Claude-3.5-Sonnet to generate responses as the model outputs to be evaluated. To ensure evaluation reliability, we collected voting results from three human evaluators as the ground truth. In our evaluation methodology, we employed both the program-based Verifier for direct answer validation and GPT-4-Turbo as an LLM judge. 
%As shown in Figure~\ref{fig:LLM-Judger}, we designed specific prompts for GPT-4-Turbo to generate chain-of-thought reasoning processes before reaching final judgments.
The experimental results, illustrated through the confusion matrix in Figure~\ref{fig:CodeVsModel}, demonstrate a significant performance disparity between the two evaluation methods: the program-based Verifier showed only 3 mismatches out of 90 cases (F1 score: 0.96), while the LLM judge exhibited 17 mismatches (F1 score: 0.76). Figure~\ref{fig:LLM-Judger} also presents a representative case where the LLM judge made an incorrect assessment while the program-based Verifier provided accurate evaluation.

Beyond superior accuracy, the program-based Verifier demonstrates substantial advantages in computational efficiency and cost-effectiveness, particularly crucial for large-scale evaluations. These experimental findings strongly validate the reliability and practicality of utilizing program-based Verifier as an evaluation tool.

\begin{figure}[!t]
% \vspace{-10pt}
  \centering
  \includegraphics[width=1.0\linewidth]{pic/Human-Alignment-Analysis-cropped.pdf}
  \caption{The precision and recall of evaluations using our verification function (Program-based Verifier) and GPT-4 as the evaluator (LLM Judger). True/False indicates the ground truth correctness of the answer. Positive/Negative is the output label predicted by Program-based Verifier or LLM Judger.}
  % , with manual review of 90 questions serving as the ground truth.
  \label{fig:CodeVsModel}
% \vspace{-10pt}
\end{figure}

\begin{figure}[!htb]
% \vspace{-10pt}
  \centering
  \includegraphics[width=1.0\linewidth]{pic/LLM-Judger-v3-cropped.pdf}
  \caption{An example of the LLM Judger making mistakes in verifying model responses.}
  % , with manual review of 90 questions serving as the ground truth.
  \label{fig:LLM-Judger}
% \vspace{-10pt}
\end{figure}

\paragraph{Error Analysis}
Table~\ref{table_main} reveals that even advanced LLMs exhibit error rates exceeding 30\% in logical reasoning tasks. To gain a comprehensive understanding of these limitations, we conducted an in-depth error analysis using Claude-3.5-Sonnet as our representative model. Based on the 90 problems from our human alignment experiment in Section~\ref{sec:human-alignment}, we enlisted human annotators to provide detailed explanations for all failure cases. Through systematic analysis of these explanations, we categorized the error sources into four primary types: Incorrect Logical Inference (incorrect deductive steps, 81\%), Contradictory Conclusion (inconsistencies between inference and conclusions, 13\%), Unanalyzed Condition (failure to consider certain given logical constraint, 3\%), and Inconsistent Format (structural or presentational issues in responses, 3\%). Detailed statistical analysis of these error patterns and specific case studies for each category are thoroughly discussed in Appendix~\ref{sec:appendix-error-analysis} and Appendix~\ref{sec:appendix-case}, respectively.


% \paragraph{Cost Analysis}