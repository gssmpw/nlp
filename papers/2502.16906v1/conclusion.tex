\section{Conclusion}
% In conclusion, our approach reduces random guessing and uses code-based verification to ensure accurate evaluation. We also proposed data augmentation to enhance problem variety and difficulty. 
% Experiments with eight popular models showed that AutoLogi improves evaluation precision and highlights gaps in even the strongest models, such as GPT-4. The benchmark will be open-sourced to support further research in advancing LLMs' reasoning abilities.
In conclusion, we present a method for automatically synthesizing open-ended logic puzzles, which we use to create the bilingual benchmark AutoLogi and high-quality training data. Our approach provides more accurate assessment of LLMs' reasoning capabilities by mitigating random guessing and enabling controllable difficulty levels, while the synthesized training data proves effective in enhancing models' reasoning abilities. %We believe this work provides a systematic foundation for developing and evaluating logical reasoning in language models.