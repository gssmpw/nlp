\section{Introduction}
% Large Language Models (LLMs) have demonstrated remarkable success across various applications~\cite{openai2024gpt4technicalreport, cluade, qwen2025qwen25technicalreport}, with logical reasoning emerging as a particularly crucial capability~\cite{luo2023towards,wei2022chain,yao2024tree,zhu2024dynamic} that underpins both their current functionality and potential advancement toward Artificial General Intelligence (AGI).

% Evaluating the reasoning capabilities of large language models (LLMs) is a critical yet challenging task. Existing reasoning benchmarks predominantly adopt formats such as multiple-choice questions \cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar} or true/false statements \cite{han2024folionaturallanguagereasoning,ismayilzada2023crowbenchmarkingcommonsensereasoning}. These benchmarks typically assess models by comparing their predictions to predefined options or answers. 
Large Language Models (LLMs) have demonstrated remarkable capabilities across diverse applications~\cite{openai2024gpt4technicalreport, cluade, qwen2025qwen25technicalreport}. Among these capabilities, logical reasoning has emerged as a critical skill~\cite{luo2023towards,wei2022chain,yao2024tree,zhu2024dynamic}.
The increasing emphasis on reasoning capabilities has highlighted the pressing need for reliable evaluation methodologies.
% To assess these reasoning capabilities, existing benchmarks primarily rely on multiple-choice questions~\cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar} and true/false statements~\cite{han2024folionaturallanguagereasoning,ismayilzada2023crowbenchmarkingcommonsensereasoning}, where models are assessed based on their ability to select correct answers from predefined options.

\begin{figure}[t]
% \vspace{-10pt}
  \centering
  \includegraphics[width=1.0\linewidth]{pic/fig-intro-cropped.pdf}

  \caption{Comparison of evaluation processes between multiple-choice questions and our method. While multiple-choice questions may allow underperforming models to guess the correct answer, our method generates open generative questions, and utilize a verification function to validate the generated solution, providing a more accurate reflection of model performance.}
  \label{fig:intro}
\vspace{-5pt}
\end{figure}

However, the field of reasoning evaluation faces three fundamental challenges: the vulnerability to random guessing, insufficient difficulty variation to differentiate model capabilities, and high human annotation costs in dataset construction. A critical issue lies in the prevalent reliance on closed-ended questions in existing benchmarks, where models are simply required to select answers from predefined options~\cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar,han2024folionaturallanguagereasoning,ismayilzada2023crowbenchmarkingcommonsensereasoning}. This format allows models with minimal reasoning capabilities to achieve substantially overestimated performance, leading to deeply misleading evaluation results that mask true reasoning deficiencies. The rapid advancements in LLMs are leading to the saturation of scores on existing benchmarks, reducing their effectiveness in differentiating model capabilities. Consequently, there is a growing and continuous need for the development of more challenging and distinctive benchmarks to better evaluate model performance. Nevertheless, constructing such high-quality reasoning datasets requires extensive human annotation effort, which substantially limits the scale of available evaluation resources.

%However, this methodology has a notable limitation: models with limited reasoning abilities can achieve artificially inflated scores by randomly selecting among the provided options, resulting in evaluations that fail to accurately reflect the modelsâ€™ true reasoning performance. This issue is particularly pronounced in complex reasoning tasks, further undermining the reliability of evaluation outcomes.
%Additionally, the lack of well-defined difficulty gradients in these benchmarks often leads to indistinguishable performance across different models, making it challenging to discern their relative strengths. Moreover, the manual annotation process for creating new reasoning benchmarks is inherently resource-intensive and presents substantial scalability challenges.

%To address these limitations, we introduce a novel automated method for synthesizing open-ended logic puzzles, which we leverage to construct AutoLogi, a  bilingual benchmark with controlled difficulty levels. As shown in Figure~\ref{fig:intro}, AutoLogi offers two key advantages: (1) its open-ended format requires models to construct complete solutions from scratch rather than selecting from predefined options, significantly reducing the possibility of correct guessing; (2) its program-based verifiers enable reliable and efficient validation through code execution, ensuring rigorous assessment of model reasoning capabilities.

To address these challenges, we introduce a novel method for automatically synthesizing open-ended logic puzzles to construct a reasoning benchmark named AutoLogi. Our approach offers three key advantages: (1) as is illustrated in Figure   ~\ref{fig:intro}, the open-ended format requires models to construct complete solutions from scratch, significantly mitigating the performance inflation caused by random guessing; (2) automated augmentation generates puzzles with varying logical constraints, enabling balanced difficulty distribution; (3) fully automated generation with minimal human verification substantially reduces benchmark construction costs. These innovations directly address the aforementioned challenges, offering a more reliable, balanced, and scalable evaluation methodology.

% This evaluation method has two advantages:
% (1) The open-ended generative problem reduces the possibility of models guessing the correct answers by chance, ensuring a more accurate evaluation of the models being tested. Specifically, for multiple-choice or true/false questions, models have prior knowledge of the candidate's answers, which are typically simple and limited in number. In contrast, our proposed logical puzzle requires models to extract potential correct arrangements from the question itself and then provide a complete solution with all necessary details. This increases the likelihood that correctly answering the question genuinely reflects the model's understanding and reasoning ability, enhancing the credibility of the evaluation.
% (2) The verification function provides a more reliable and efficient validation method for assessing model performance than previous measures. Unlike rule-based matching, which struggles with full coverage due to varying solution sizes, or model-based scoring (i.e., LLM-as-a-judge), which introduces instability and bias in complex tasks, our method based on code execution excels in handling open-ended questions with multiple valid solutions by objectively evaluating the correctness of results.


% Moreover, the process of building our benchmark is almost automated, requiring only modest human labor, while being capable of generating a large number of test samples with varying levels of difficulty.

% Specifically, we employ GPT-4\cite{openai2024gpt4technicalreport} to formulate logic puzzles from existing dataset, and utilize its advanced coding ability to synthesize verification functions. 
% To improve the reliability of generated verification functions, we will ensure that they can be executed and correctly validate valid solutions. If a generated verification function fails this check, it will be regenerated, thereby significantly reducing potential errors.

% To further construct more test samples, we propose a data augmentation method that enhances open-ended generative problems by adding or removing logical constraints. This approach not only expands the dataset size but also allows for precise control over difficulty levels, enabling the creation of questions with varying levels of complexity.
% As a result of data augmentation, the number of puzzles increased by approximately 7-fold, ultimately yielding a total of 1,575 English puzzles and 883 Chinese puzzles in our AutoLogi benchmark.

% AutoLogi is not only applicable to constructing test benchmarks but also capable of generating training data for model fine-tuning. Through rejection-sampling with the verifier, we naturally obtain two types of training data: verified correct answers for supervised fine-tuning (SFT) and correct-incorrect answer pairs for reinforcement learning from human feedback (RLHF). A key advantage of our method lies in its ability to generate open-ended answers. Compared to multiple-choice formats, open-ended answers require models to provide complete reasoning processes, and the rejection-sampling offers higher reliability---as models might obtain correct answers through random guessing in multiple-choice questions while having flawed reasoning processes, thereby introducing noisy data. 

% Through extensive experiments across eight prominent language models, including closed-source models like GPT-4 and Claude, and open-source models such as Qwen and LLaMA, we demonstrate the effectiveness of AutoLogi in both evaluation and training. As a benchmark, AutoLogi exhibits superior discriminative power with scores ranging from 20.61 to 72.61, in contrast to traditional multiple-choice benchmarks where scores cluster narrowly between 21.04 and 37.39. Notably, even top-performing models like GPT-4 achieve less than 40\% accuracy on complex puzzles with nine or more constraints, indicating substantial room for improvement in logical reasoning capabilities.
% Our experiments further validate AutoLogi's effectiveness in synthesizing high-quality training data. Models trained on our synthesized dataset, whether through Self-Alignment or Strong-to-weak Distillation approaches, demonstrate consistent improvements in logical reasoning capabilities. These improvements generalize well to both in-domain and out-of-domain datasets, with particularly impressive results given our compact training dataset size (3,000-5,000 samples). The effectiveness spans different training paradigms, including both Rejection Fine-tuning (RFT) and Direct Preference Optimization (DPO), validating our synthesis methodology's efficiency and versatility in enhancing models' logical reasoning abilities.

% Our core contributions are:
% \begin{itemize}
%     \item We propose a method for synthesizing open-ended generative logic puzzles from existing benchmarks, along with a data augmentation strategy that enables the creation of more puzzles with varying levels of complexity.
%     \item We propose to synthesize the verification function and take code execution as an evaluation method for complex logic puzzles, which address issues related to multiple valid solutions while also being reliable and efficient.
%     \item Based on the above methods, we construct AutoLogi, a comprehensive framework that serves dual purposes: (1) a logical reasoning benchmark consisting of 1,575 English puzzles and 883 Chinese puzzles, and (2) an efficient training data synthesizer that demonstrably enhances models' logical reasoning capabilities across various datasets and training paradigms\footnote{The benchmark, training data, and associated code will be open-sourced upon paper publication.}.
%     \item Through extensive evaluation of eight open-source and closed-source modern LLMs, we demonstrate the effectiveness of AutoLogi in both evaluation and training. Our analysis of failure cases and training outcomes across different settings (e.g., Self-Alignment, Strong-to-weak Distillation) provides valuable insights for improving logical reasoning capabilities in language models.
% \end{itemize}

% Our AutoLogi benchmark offers two key advantages:
% (1) The open-ended generative format significantly reduces the possibility of correct answers by chance, as models must extract and arrange complete solutions from the questions themselves, better reflecting their true reasoning capabilities.
% (2) The verification function enables reliable and efficient validation through code execution. This overcomes the limitations of traditional evaluation approaches: rule-based matching is impractical as our benchmark only requires one feasible arrangement while allowing numerous valid solutions, especially when logical constraints are minimal, leading to an exponential growth in possible correct answers. Meanwhile, model-based scoring (LLM-as-a-judge) introduces instability in complex tasks.

%The benchmark construction process is largely automated with minimal human intervention. We leverage advanced LLMs to formulate logic puzzles from existing datasets and synthesize verification functions. 
The proposed method follows a three-stage pipeline: information extraction from corpora for open-ended question synthesis, program-based verifier generation using advanced LLMs, and dataset augmentation for difficulty balance. Since most generation steps are based on LLMs thus potentially introducing errors, we employ a cross-validation framework where verification functions and traversal functions verify each other. The traversal functions validate puzzle correctness through exhaustive search, while verification functions check the traversal results. Although this provides only necessary conditions, our experiments show it corrects 23\% of erroneous data. Building upon this foundation, We further develop a data augmentation method that creates puzzle variants by modifying logical constraints to control difficulty levels, enabling us to expand our dataset to 1,575 English and 883 Chinese puzzles.

%\cqy{Maybe need some motifs to introduce the training part. Like: Based on our insights, Code execution can serve as a verifiable reward function / good verifier for reasoning tasks, therefore, we explore its utility to synthesize logical reasoning data xxx.} 
Based on our insights that code execution provides verifiable reward signals for reasoning tasks, we explore its utility in synthesizing high-quality training data. Through rejection-sampling with verifiers, we generate both verified correct answers for supervised fine-tuning (SFT) and correct-incorrect answer pairs for Direct Preference Optimization (DPO, \citealp{rafailov2024directpreferenceoptimizationlanguage}). This verification mechanism provides stronger guarantees of solution correctness, thereby enabling the collection of higher-quality supervised training data.
%Notably, unlike multiple-choice questions where models may guess correctly despite flawed reasoning, our program-based verifiers rigorously validate the entire solution process, effectively filtering out responses with inconsistent reasoning steps.

% Through extensive experiments with eight prominent language models (including GPT-4o, Claude, Qwen, and LLaMA), we demonstrate the effectiveness of our method. On one hand, AutoLogi as a benchmark effectively discriminates models' logical reasoning capabilities, showing comparable results with existing datasets. On the other hand, models fine-tuned on our synthesized dataset demonstrate remarkable improvements across various reasoning benchmarks, regardless of training settings.
Through comprehensive experiments with eight state-of-the-art language models (including GPT-4, Claude, Qwen, and LLaMA), we evaluate our method from two aspects. First, as a benchmark, AutoLogi exhibits superior discriminative power with a wider score distribution (35.25 to 72.61) compared to traditional multiple-choice formats (21.04 to 37.39), better reflecting models' true reasoning capabilities. Second, when used for training, our synthesized dataset leads to substantial improvements on independent reasoning benchmarks, notably improving Qwen's performance on LiveBench from 30\% to 35\% at 7B scale and from 46\% to 52\% at 72B scale.

% Our core contributions are:
% \begin{itemize}
% \item We propose a method for transforming existing benchmarks %\cqy{Former intro talked less about convert existing MC benchmarks to open-ended ones. Maybe emphasize this point in the 3rd paragraph.} 
% into open-ended generative logic puzzles, along with a data augmentation strategy that enables the creation of puzzles with controllable complexity levels. Specifically, the open-ended format eliminates random guessing, while code-based verification ensures reliable evaluation of model responses.
% \item We create AutoLogi, a bilingual logical reasoning benchmark containing 1,575 English and 883 Chinese puzzles. Through extensive evaluation of eight modern LLMs, we demonstrate its superior discriminative power compared to traditional multiple-choice formats(with performance ranging from 21.04 to 37.39), with a wider score distribution (35.25 to 72.61) that better reflects models' true reasoning capabilities.
% \item Beyond evaluation, we show that our method serves as an efficient training data synthesizer. Through rejection sampling with verifiers, we generate high-quality reasoning examples that improve models' performance across various datasets and training paradigms.
% \end{itemize}
% Our core contributions are:
% \begin{itemize}
% \item We propose a method to transform existing benchmarks into open-ended logic puzzles with controllable complexity. Our approach eliminates random guessing through open-ended formats and ensures reliable evaluation through code-based verification.
% \item We introduce AutoLogi, a bilingual logical reasoning benchmark containing English and Chinese puzzles. Through comprehensive evaluation with eight state-of-the-art LLMs, we demonstrate its superior discriminative power in reflecting models' true reasoning capabilities compared to traditional multiple-choice formats.
% \item Beyond evaluation, we leverage our method as an efficient training data synthesizer. Through rejection sampling with verifiers, we generate high-quality reasoning examples that consistently improve models' performance across various benchmarks.
% \end{itemize}
Our core contributions are:
\begin{itemize}
\item We develop a method to generate open-ended logic puzzles with controllable complexity, mitigating the performance inflation caused by random guessing and ensuring reliable evaluation through code-based verification.
\item We introduce AutoLogi, a bilingual logical reasoning benchmark that better reflects models' reasoning capabilities compared to multiple-choice formats.
\item We leverage our method to generate training data that improves model performance across multiple independent benchmarks~\footnote{We have released our training and testing datasets, along with the implementation of data synthesis methods and evaluation metrics at \url{https://github.com/8188zq/AutoLogi}.}.
\end{itemize}