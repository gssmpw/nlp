\section{Related Work}
\subsection{Logical Reasoning Benchmarks}
%Logical reasoning is highly benchmarked and analyzed skill of LLMs ____.
%considered as a fundamental capability in the aspect of intelligence and an indispensable requirement for real-world applications for LLMs ____. 

%\paragraph{Logical Reasoning Tasks and Datasets} Various datasets and tasks have been developed to evaluate different aspects of logical reasoning capabilities. Examples include LogiBench, which assesses first-order logic (FOL) ____, FOLIO ____, and ProntoQA____. Datasets like RuleBert apply soft logical rules ____, and tasks such as TaxiNLI ____ and LogiNLI ____ assess natural language inference based on FOL. Various reasoning types like mathematical ____, spatial ____, sequential ____, and counterfactual reasoning ____ also form key areas of study.

\paragraph{Question Types}
Existing logical reasoning datasets primarily use multiple-choice questions ____, true/false questions ____, or classification tasks ____. These formats, while enabling simple evaluation through keyword matching, are vulnerable to random guessing. Open-ended questions provide more rigorous assessment by requiring complete solution generation. While datasets like Zebra Puzzle ____ have explored this format, our AutoLogi dataset leverages programmatic verification for more reliable evaluation of logical reasoning abilities.
% Existing logical reasoning datasets largely fall into three categories: multiple-choice, true/false, and classification questions. Most benchmarks use multiple-choice questions for easy answer extraction ____, allowing for broad assessment ____.
% FOLIO____ and CroW____ uses true/false questions.
% Classification problems include CLUTRR’s 16-class classification ____ and LogiNLI’s 3-class NLI task ____. These types typically have explicit answers, utilizing rule-based evaluation methods like keyword matching. %like Exact Match (EM).

% % A smaller subset of datasets, like Zebra Puzzle ____, uses open-ended questions, but they offer limited variety. Our AutoLogi dataset also features open-ended questions but is more complex, requiring models to generate detailed solutions, thus reducing guessing and enhancing evaluation credibility.
% These question formats, while convenient for evaluation, are vulnerable to random guessing and may not fully reflect models' reasoning capabilities. Open-ended questions provide a more rigorous assessment by requiring models to generate complete solutions rather than selecting from pre-defined options. While datasets like Zebra Puzzle ____ have explored this format, our AutoLogi dataset leverages open-ended questions with programmatic verification to enable more reliable evaluation of logical reasoning abilities.

\paragraph{Dataset Construction Approach}
The current mainstream approaches to constructing logical reasoning datasets are threefold: % human annotation, extraction from academic exams, and synthesis.
human annotation____, extraction from academic exams____, and synthesize____.
%
Among the three methods, synthesize is most cost-effective but usually limited in diversity since they heavily rely on rule-based templates.

%(1) Human annotation can produces high-quality questions with substantial difficulty, but it is also costly____.

%(2) Extraction from academic exams tends to offer a narrower range of question types, mostly multiple-choice questions____.

%(3) Synthesis question : There are examples such as PrOntoQA____, where each example is generated from a synthetic world model represented in first-order logic, and CLUTRR____, which uses a rule-based template that is relatively simplistic. Livebench enhances the difficulty by building upon BigBench’s The Web of Lies v2 task and the Zebra Puzzle, but it still only revolves around these two backgrounds, resulting in limited diversity____.

Our AutoLogi dataset is synthesized, but different from previous methods, we employ LLMs to reformulate questions from established academic reasoning assessments (such as LogiQA____ and AR-LSAT____, which are derived from real human logical reasoning tests), thus ensuring rich linguistic quality and logical, real-world connected contexts. We design our synthesis process to explicitly control difficulty levels (discussed in~\ref{sec:Data_Aug}), allowing for better discrimination of model reasoning abilities. %It also boasts high versatility and is suitable for any problem type as long as the questions involve multiple logical constraints, making them potential source corpora.

\subsection{Program-aided Methods}
LLMs, despite their capabilities, can generate factually incorrect responses. Integrating reliable feedback mechanisms, such as code interpreters, offers a potential solution. This approach, termed Program-aided Language model or Program of Thought, has been shown by ____ and ____ to outperform Chain of Thought methods across various domains, particularly in tasks requiring definitive answers ____. Inspired by ____'s use of program-based verifiers in training, we combine this approach with rejection sampling to generate our training data.