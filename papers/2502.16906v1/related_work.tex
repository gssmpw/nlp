\section{Related Work}

\subsection{Logical Reasoning Benchmarks}
%Logical reasoning is highly benchmarked and analyzed skill of LLMs \cite{}.
%considered as a fundamental capability in the aspect of intelligence and an indispensable requirement for real-world applications for LLMs \cite{khashabi2019reasoningdrivenquestionansweringnaturallanguage, beygi2022logicalreasoningtaskoriented}. 

%\paragraph{Logical Reasoning Tasks and Datasets} Various datasets and tasks have been developed to evaluate different aspects of logical reasoning capabilities. Examples include LogiBench, which assesses first-order logic (FOL) \cite{parmar2024logicbench}, FOLIO \cite{han2024folionaturallanguagereasoning}, and ProntoQA\cite{saparov2022language}. Datasets like RuleBert apply soft logical rules \cite{saeed2021rulebert}, and tasks such as TaxiNLI \cite{joshi2020taxinli} and LogiNLI \cite{tian2021diagnosing} assess natural language inference based on FOL. Various reasoning types like mathematical \cite{mishra2022numglue}, spatial \cite{mirzaee2021spartqa,tikhonov2024plugh,li2024reframing}, sequential \cite{yang2024aqa}, and counterfactual reasoning \cite{frohberg2022crass,tandon2019wiqa} also form key areas of study.

\paragraph{Question Types}
Existing logical reasoning datasets primarily use multiple-choice questions \cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar,srivastava2023beyond,suzgun2022challenging}, true/false questions \cite{han2024folionaturallanguagereasoning,ismayilzada2023crowbenchmarkingcommonsensereasoning}, or classification tasks \cite{sinha-etal-2019-clutrr,tian2021diagnosing}. These formats, while enabling simple evaluation through keyword matching, are vulnerable to random guessing. Open-ended questions provide more rigorous assessment by requiring complete solution generation. While datasets like Zebra Puzzle \cite{prosser1993hybrid} have explored this format, our AutoLogi dataset leverages programmatic verification for more reliable evaluation of logical reasoning abilities.
% Existing logical reasoning datasets largely fall into three categories: multiple-choice, true/false, and classification questions. Most benchmarks use multiple-choice questions for easy answer extraction \cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar}, allowing for broad assessment \cite{srivastava2023beyond,suzgun2022challenging}.
% FOLIO~\cite{han2024folionaturallanguagereasoning} and CroW~\cite{ismayilzada2023crowbenchmarkingcommonsensereasoning} uses true/false questions.
% Classification problems include CLUTRR’s 16-class classification \cite{sinha-etal-2019-clutrr} and LogiNLI’s 3-class NLI task \cite{tian2021diagnosing}. These types typically have explicit answers, utilizing rule-based evaluation methods like keyword matching. %like Exact Match (EM).

% % A smaller subset of datasets, like Zebra Puzzle \cite{prosser1993hybrid}, uses open-ended questions, but they offer limited variety. Our AutoLogi dataset also features open-ended questions but is more complex, requiring models to generate detailed solutions, thus reducing guessing and enhancing evaluation credibility.
% These question formats, while convenient for evaluation, are vulnerable to random guessing and may not fully reflect models' reasoning capabilities. Open-ended questions provide a more rigorous assessment by requiring models to generate complete solutions rather than selecting from pre-defined options. While datasets like Zebra Puzzle \cite{prosser1993hybrid} have explored this format, our AutoLogi dataset leverages open-ended questions with programmatic verification to enable more reliable evaluation of logical reasoning abilities.

\paragraph{Dataset Construction Approach}
The current mainstream approaches to constructing logical reasoning datasets are threefold: % human annotation, extraction from academic exams, and synthesis.
human annotation~\cite{clark2020transformerssoftreasonerslanguage,ismayilzada2023crowbenchmarkingcommonsensereasoning,han2024folionaturallanguagereasoning}, extraction from academic exams~\cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar}, and synthesize~\cite{saparov2022language, sinha-etal-2019-clutrr, white2024livebench}.
%
Among the three methods, synthesize is most cost-effective but usually limited in diversity since they heavily rely on rule-based templates.

%(1) Human annotation can produces high-quality questions with substantial difficulty, but it is also costly~\cite{clark2020transformerssoftreasonerslanguage,ismayilzada2023crowbenchmarkingcommonsensereasoning,han2024folionaturallanguagereasoning}.

%(2) Extraction from academic exams tends to offer a narrower range of question types, mostly multiple-choice questions~\cite{liu2020logiqachallengedatasetmachine,bansal2023fewshotunifiedquestionanswering,zhong2021ar}.

%(3) Synthesis question : There are examples such as PrOntoQA\cite{saparov2022language}, where each example is generated from a synthetic world model represented in first-order logic, and CLUTRR\cite{sinha-etal-2019-clutrr}, which uses a rule-based template that is relatively simplistic. Livebench enhances the difficulty by building upon BigBench’s The Web of Lies v2 task and the Zebra Puzzle, but it still only revolves around these two backgrounds, resulting in limited diversity\cite{white2024livebench}.

Our AutoLogi dataset is synthesized, but different from previous methods, we employ LLMs to reformulate questions from established academic reasoning assessments (such as LogiQA~\cite{liu2020logiqachallengedatasetmachine} and AR-LSAT~\cite{zhong2021ar}, which are derived from real human logical reasoning tests), thus ensuring rich linguistic quality and logical, real-world connected contexts. We design our synthesis process to explicitly control difficulty levels (discussed in~\ref{sec:Data_Aug}), allowing for better discrimination of model reasoning abilities. %It also boasts high versatility and is suitable for any problem type as long as the questions involve multiple logical constraints, making them potential source corpora.

\subsection{Program-aided Methods}
LLMs, despite their capabilities, can generate factually incorrect responses. Integrating reliable feedback mechanisms, such as code interpreters, offers a potential solution. This approach, termed Program-aided Language model or Program of Thought, has been shown by \citet{gao2023palprogramaidedlanguagemodels} and \citet{chen2023programthoughtspromptingdisentangling} to outperform Chain of Thought methods across various domains, particularly in tasks requiring definitive answers \cite{wang2024mintevaluatingllmsmultiturn, wang2024letilearninggeneratetextual, lu2023chameleonplugandplaycompositionalreasoning}. Inspired by \cite{dong2024selfplayexecutionfeedbackimproving}'s use of program-based verifiers in training, we combine this approach with rejection sampling to generate our training data.