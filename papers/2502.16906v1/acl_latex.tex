% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Change "review" to "final" to generate the final (sometimes called camera-ready) version.
% Change to "preprint" to generate a non-anonymous version with page numbers.
\usepackage[preprint]{acl}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out,
% but it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}

%Including images in your LaTeX document requires adding
%additional package(s)
\usepackage{graphicx}
% self add
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{colortbl}
\usepackage{xcolor}
\usepackage{arydshln}
\usepackage{makecell}
\usepackage{amssymb} % for \checkmark
\usepackage{CJKutf8} % for Chinese
% \usepackage{xeCJK}
\usepackage{longtable}  % 跨页表格
\usepackage{listings}
\lstset{
    language={},
    moredelim=[s][\color{gray}]{octave:}{>},
    basicstyle=\ttfamily,
    columns=fullflexible,
    frame=single,
    breaklines=true,
    postbreak=\mbox{\textcolor{red}{$\hookrightarrow$}\space},
}
\definecolor{codegray}{rgb}{0.95,0.95,0.95}
\lstnewenvironment{longprompt}[1][]
{
\onecolumn\lstset{basicstyle={\small\it},prebreak={},postbreak=\mbox{\hspace{-2.25 em}},backgroundcolor=\color{codegray},frame=single,framerule=1pt,escapeinside={(*@}{@*)},#1}}
{}
% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\newcommand{\ensuretext}[1]{#1}
\newcommand{\marker}[2]{\ensuremath{^{\textsc{#1}}_{\textsc{#2}}}}
\newcommand{\authcomment}[3]{\ensuretext{\textcolor{#3}{[#1 #2]}}}
\newcommand{\cqy}[1]{\authcomment{\marker{Qin}{Yuan}}{#1}{red}}

\title{AutoLogi: Automated Generation of Logic Puzzles for Evaluating Reasoning Abilities of Large Language Models}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a separate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{
 \textbf{Qin Zhu\textsuperscript{1,2,3}}\thanks{\ \ \ Work done during internship at Qwen Team, Alibaba Group.},
 \textbf{Fei Huang\textsuperscript{1}}\thanks{\ \ \ Corresponding authors.},
 \textbf{Runyu Peng\textsuperscript{2,3}},
 \textbf{Keming Lu\textsuperscript{1}},
 \textbf{Bowen Yu\textsuperscript{1}},
 \textbf{Qinyuan Cheng\textsuperscript{2,3}},
 \\
 \textbf{Xipeng Qiu\textsuperscript{2,3}},
 \textbf{Xuanjing Huang \textsuperscript{2,3}},
 \textbf{Junyang Lin \textsuperscript{1}}{$^\dagger$}
% \\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
% \\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
% \\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
\\
\\
 \textsuperscript{1}Qwen Team, Alibaba Group, \quad
 \textsuperscript{2}School of Computer Science, Fudan University,\\
 \textsuperscript{3}Shanghai Key Laboratory of Intelligent Information Processing, Fudan University.
 % \textsuperscript{4}Affiliation 4,
 % \textsuperscript{5}Affiliation 5
\\
 \small{
  \href{mailto:email@domain}
   {\{zhuq22,rypeng22\}@m.fudan.edu.cn} } \quad
   \href{mailto:email@domain}{\{chengqy21,xpqiu,xjhuang\}@fudan.edu.cn} \\
\small{
    \href{mailto:email@domain}{\{feihu.hf,lukeming.lkm,yubowen.ybw,junyang.ljy\}@alibaba-inc.com}  
 } 
}


% \author{First Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\\And
%   Second Author \\
%   Affiliation / Address line 1 \\
%   Affiliation / Address line 2 \\
%   Affiliation / Address line 3 \\
%   \texttt{email@domain} \\}

%\author{
%  \textbf{First Author\textsuperscript{1}},
%  \textbf{Second Author\textsuperscript{1,2}},
%  \textbf{Third T. Author\textsuperscript{1}},
%  \textbf{Fourth Author\textsuperscript{1}},
%\\
%  \textbf{Fifth Author\textsuperscript{1,2}},
%  \textbf{Sixth Author\textsuperscript{1}},
%  \textbf{Seventh Author\textsuperscript{1}},
%  \textbf{Eighth Author \textsuperscript{1,2,3,4}},
%\\
%  \textbf{Ninth Author\textsuperscript{1}},
%  \textbf{Tenth Author\textsuperscript{1}},
%  \textbf{Eleventh E. Author\textsuperscript{1,2,3,4,5}},
%  \textbf{Twelfth Author\textsuperscript{1}},
%\\
%  \textbf{Thirteenth Author\textsuperscript{3}},
%  \textbf{Fourteenth F. Author\textsuperscript{2,4}},
%  \textbf{Fifteenth Author\textsuperscript{1}},
%  \textbf{Sixteenth Author\textsuperscript{1}},
%\\
%  \textbf{Seventeenth S. Author\textsuperscript{4,5}},
%  \textbf{Eighteenth Author\textsuperscript{3,4}},
%  \textbf{Nineteenth N. Author\textsuperscript{2,5}},
%  \textbf{Twentieth Author\textsuperscript{1}}
%\\
%\\
%  \textsuperscript{1}Affiliation 1,
%  \textsuperscript{2}Affiliation 2,
%  \textsuperscript{3}Affiliation 3,
%  \textsuperscript{4}Affiliation 4,
%  \textsuperscript{5}Affiliation 5
%\\
%  \small{
%    \textbf{Correspondence:} \href{mailto:email@domain}{email@domain}
%  }
%}


\begin{document}
\maketitle
% \begin{abstract}
% Large Language Models (LLMs) have shown strong performance across domains, with logical reasoning being essential for progress towards Artificial General Intelligence (AGI). However, evaluating these abilities remains challenging, often skewed by random guessing in multiple-choice formats. To address this, we propose AutoLogi, which transforms existing multiple-choice questions into open-ended benchmarks and uses code execution for evaluation, ensuring a more accurate assessment of reasoning, especially for complex tasks. Implemented on both English and Chinese datasets, AutoLogi enhances benchmarking through tailored difficulty levels and augmented datasets, which better discern differences in model performance. Our evaluation of prominent LLMs revealed significant scope for improvement and identified frequent error patterns, informing potential advancements in model training. Our contributions include (1) transforming multiple-choice reasoning problems into open-ended tasks with controllable difficulty, (2) using code-based evaluation to handle solution variability, and (3) providing detailed analysis of model performance to support future improvements.
% \end{abstract}
\begin{abstract}
%Despite the impressive capabilities of Large Language Models (LLMs), evaluating their logical reasoning abilities remains challenging, as existing benchmarks primarily rely on multiple-choice formats vulnerable to random guessing. 

%However, evaluating these abilities remains challenging, often skewed by random guessing in multiple-choice formats. 
%We propose AutoLogi, a comprehensive framework that transforms existing multiple-choice questions into open-ended benchmarks and leverages code execution for evaluation, ensuring more accurate assessment of reasoning capabilities. 
%We propose a method that automatically transforms existing benchmarks into open-ended generative puzzles and augments them with controllable difficulty levels, utilizing program-based verification for reliable evaluation.

%This paper propose a general-purpose method that can automatically transforms diverse reasoning tasks into open-ended generative puzzles, requiring only problem descriptions without pre-defined answers. Our approach enables controllable difficulty levels and leverages code-based verification for reliable evaluation. Using this approach, we develop AutoLogi, a bilingual benchmark containing 1,575 English and 883 Chinese puzzles that provides a more fine-grained assessment of model capabilities compared to multiple-choice formats.

%The benchmark reveals significant performance gaps among current LLMs, with even top models achieving under 40\% accuracy on complex puzzles with multiple interrelated logical constraints.

%Beyond evaluation, our method can serve as a powerful training data synthesizer - through rejection sampling with verifiers, we generate high-quality reasoning examples that substantially boost model performance across diverse benchmarks. After training on our synthesized data,Qwen-2.5 demonstrates significant improvements on independent reasoning benchmarks, improving from 30\% to 35\% at 7B scale and from 46\% to 52\% at 72B scale on LiveBench, highlighting the generalizability and effectiveness of our approach.

%Beyond evaluation, our method serves as an efficient training data synthesizer - through rejection sampling with verifiers, we generate high-quality reasoning examples that consistently improve models' performance across various datasets and training paradigms.% using only 3k-5k samples.
%demonstrating significant improvements in models' logical reasoning abilities across various datasets and training paradigms. 
%Implemented on both English and Chinese datasets, AutoLogi enhances benchmarking through tailored difficulty levels and augmented datasets, which better discern differences in model performance. Our evaluation of state-of-the-art LLMs not only revealed substantial room for improvement but also validated the effectiveness of our synthetic training data in enhancing models' reasoning capabilities. 
%Our key contributions include a comprehensive method for generating controllable-difficulty open-ended puzzles, which enables efficient synthesis of high-quality training data through rejection sampling, and a novel bilingual benchmark constructed using this method, demonstrating superior discriminative power.
 % Our automated approach significantly reduces the cost and effort of creating reliable reasoning benchmarks. This work opens up new possibilities for systematic improvement of LLMs' reasoning capabilities through large-scale, verifiable training data generation.

While logical reasoning evaluation of Large Language Models (LLMs) has attracted significant attention, existing benchmarks predominantly rely on multiple-choice formats that are vulnerable to random guessing, leading to overestimated performance and substantial performance fluctuations. To obtain more accurate assessments of models' reasoning capabilities, we propose an automated method for synthesizing open-ended logic puzzles, and use it to develop a bilingual benchmark, AutoLogi. Our approach features program-based verification and controllable difficulty levels, enabling more reliable evaluation that better distinguishes models' reasoning abilities. Extensive evaluation of eight modern LLMs shows that AutoLogi can better reflect true model capabilities, with performance scores spanning from 35\% to 73\% compared to the narrower range of 21\% to 37\% on the source multiple-choice dataset. Beyond benchmark creation, this synthesis method can generate high-quality training data by incorporating program verifiers into the rejection sampling process, enabling systematic enhancement of LLMs' reasoning capabilities across diverse datasets. %This automated approach enables systematic enhancement of LLMs' reasoning capabilities while significantly reducing the cost of creating reliable reasoning benchmarks.
\end{abstract}

\input{intro}
\input{related_work}
\input{method}
\input{experiment}
\input{conclusion}
\input{limitation}

% Additional elements were taken from the formatting instructions of the \emph{International Joint Conference on Artificial Intelligence} and the \emph{Conference on Computer Vision and Pattern Recognition}.

% Bibliography entries for the entire Anthology, followed by custom entries
%\bibliography{anthology,custom}
% Custom bibliography entries only
\bibliography{custom}

\appendix

\include{appendix}


\end{document}
