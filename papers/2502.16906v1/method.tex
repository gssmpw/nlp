\section{Method}
\begin{figure*}[t!]
    \centering
    % \includegraphics[width=\linewidth]{pic/pipeline_v2.6.pdf}
    \includegraphics[width=\linewidth]{pic/method.pdf}
    \caption{An overview of our method. The process consists of three stages: \textbf{Stage 1} formulates logic puzzles by extracting background information and constraints from a source corpus. \textbf{Stage 2} uses large language models (LLMs) to generate verifiers, which are programs that check puzzle solutions and ensure correct formatting. \textbf{Stage 3} augments the puzzles by adding or removing constraints to create varying difficulty levels. All three stages leverage powerful LLMs, such as GPT-4, for generation. 
    %Additionally, multiple validation checks, implemented through a traversal function in Stage 2, ensure the reliability of the benchmark throughout the process.
   }
    \label{fig:pipeline_overview}
\end{figure*}

%We propose a method to automatically generate open-ended logic puzzles with controlled difficulty levels and equip them with program-based evaluation. 
As shown in Figure~\ref{fig:pipeline_overview}, our method consists of three stages:  \emph{Puzzle Formulation},  \emph{Format \& Verifiers Generation}, and  \emph{Data Augmentation}.
%The overview of our method, which is termed as AutoLogi, is presented in  
%The method comprises three stages that allow us to generate open-ended logical questions with varying difficulty levels. % We begin with a source corpus that contains several logical constraints, without requiring an answer.
%In \textbf{Stage 1}, we formulate logic puzzles by extracting background information and logical constraints from a source corpus. % that contains multiple constraints but does not provide answers. % The objective is to discover a feasible arrangement that satisfies all the given constraints, with no answer required. 

% In \textbf{Stage 2}, we employ advanced LLMs to generate programs, called verifiers, which are then used to validate the model answer, along with the format requirement for the newly formulated puzzles.

%This allows us to conduct evaluations based on the feedback from code execution.

% In \textbf{Stage 3}, we systematically augment the puzzles by adding or removing constraints, creating a comprehensive dataset with controlled difficulty levels.

% Throughout the process, we implement multiple validation mechanisms to ensure the reliability of the generated puzzles.

%the outputs generated by the model.

\label{sec:question_trans}
\subsection{Puzzle Formulation}

The Puzzle Formulation stage takes a source corpus containing puzzle-related content as input and leverages advanced LLMs with direct prompting (detailed prompts in Appendix~\ref{sec:appendix-prompt}) to extract and restructure the text. The source corpus should provide background information suitable for constructing logical puzzles, for example, questions from existing multiple-choice reasoning benchmarks.
%The Puzzle Formulation stage takes a source corpus containing logical reasoning information as input, and utilizes advanced LLMs with direct prompting (detailed prompts in Appendix~\ref{sec:appendix-prompt}) to extract and restructure the text. To ensure input quality, we leverage existing high-quality logical reasoning datasets as source materials, without restrictions on specific problem types. 
The output consists of two key components: \emph{Background} and \emph{Logical Constraints}. The \textbf{Background} provides the context and basic elements of the puzzle, defining the objects to be arranged and their value ranges. The \textbf{Logical Constraints} specify the conditions limiting valid arrangements. These components, combined with a request for test LLMs to generate any valid arrangement satisfying these constraints, as illustrated in Figure~\ref{fig:intro}, form the puzzle stem.

A notable strength of this transformation approach lies in its broad applicability - as long as the source text contains background information related to logical reasoning, it can be converted into our open-ended format, regardless of question types or the presence of answers.
%This enables a more accurate assessment of models' reasoning capabilities.
%Having established the puzzle stem, we now proceed to incorporate answer verification mechanisms to create a complete, verifiable puzzle.

\subsection{Format \& Verifiers Generation}

Stage 2 focuses on establishing a reliable evaluation mechanism. We design a program-based verification scheme consisting of three tightly coupled components: \textbf{Format Requirement} specifies the expected JSON output structure (including \textbf{Arrangement Format} and \textbf{Arrangement Example}); \textbf{Verifiers} contain program-based format verifier and constraint verifier for checking format compliance and logical constraints; \textbf{Traversal Function} searches for all possible valid solutions. This code execution-based verification approach effectively overcomes limitations of traditional evaluation methods:
%since our benchmark requires only one feasible solution while allowing
since our logical puzzles may have multiple valid answers (which can grow exponentially when logical constraints are sparse), rule-based matching becomes impractical; meanwhile, model-based scoring (LLM-as-a-judge) introduces instability in complex tasks (discussed in Section \ref{sec:human-alignment}).

Given the strong correlation between Format Requirement and Verifiers, we generate these two components simultaneously using advanced LLM, with Background and Logical Constraints from Stage 1 as input. Subsequently, we generate the Traversal Function based on the produced Format Requirement and Verifiers.

To ensure the reliability of the evaluation mechanism, we propose a cross-validation method to check the correctness of LLM-generated Verifiers and Traversal Function.
We run the Traversal Function to examines all possible combinations within the value ranges defined in Background (\textbf{Domain space}), and validates them through Verifiers to determine the existence of solutions satisfying all constraints (\textbf{Solution space}). %Since the Traversal Function is also generated by LLM, this verification approach essentially establishes cross-validation with Verifiers.
When no solution is found (empty solution space) or syntax errors occur, a regeneration is triggered. Through the verification, we ensure that each problem has at least one valid solution, which is a necessary condition for problem validity. The effectiveness of Traversal Function are discussed in Section \ref{sec:solution-exist}. %The corresponding prompts are provided in Appendix \ref{sec:appendix-prompt}.


% \subsection{Format \& Verifiers Generation}
% In Stage 2, our task is to establish a solution for evaluating the model's output. This involves defining a format requirement, creating program-based verifiers, and developing a traversal function. 
% The details of the three components are shown below.

% \begin{enumerate}
%     \item \textbf{Format Requirement}: To standardize the evaluation, we require the model to generate a JSON object. The format requirement provides the model with a description of the output format (referred to as the \textbf{Arrangement Format}) and an example (referred to as the \textbf{Arrangement Example}). Note that the format requirement is dictated by the puzzle background and therefore may differ from sample to sample.
%     \item \textbf{Verifiers}: The verifiers are used to determine whether the model output is correct. We introduce two verifiers: a format verifier and a constraint verifier. The format verifier checks compliance with the Arrangement Format, while the constraint verifier checks adherence to the Logical Constraints.
%     \item \textbf{Traversal Function}: Traversal Function can search the arrangements, finding all solutions satisfying the puzzle requirement. By running the traversal function, with the help of verifiers, we can obtain a domain space and a solution space.
%     The domain space corresponds to the set of all possible arrangements that conform to the Arrangement Format. The solution space is the subset of the domain space, where each solution must satisfy all the logical constraints. 
% \end{enumerate}
% In AutoLogi, we apply GPT-4 \cqy{Note the specific version in footnotes} across all the aforementioned steps. The format requirement and verifiers are generated simultaneously since they are closely interrelated. The Traversal Function are then generated since it depends on both the Format and Verifier. The relevant prompts are provided in the Appendix~\ref{sec:appendix-prompt}.

% The reason we opt for programs for evaluation is that, in our setting, most problems can have multiple solutions and standard answers are not always available. Therefore, commonly used rule-based matching methods like Exact Match are insufficient. Additionally, relying on LLM-based judges for evaluation poses limitations in terms of cost, time, and accuracy. \cqy{Cite some papers here to support the claim is better.} In contrast, model-generated code offers a faster, more accurate, and cost-effective solution without relying on the presence of a standard reference answer.

% \paragraph{Check and Re-generation} Since errors may occur at any step in this stage, we need to perform a check to ensure reliability. If the check fails, a re-generation will be triggered. We find that most of the errors fall into two categories:

% \begin{enumerate}
%     \item \textbf{Syntax Errors}: The generated verifiers or traversal functions may contain issues such as undefined variables, parameter mismatches, and so on. The issue can be simply checked by executing the verifiers using the Arrangement Example as the input.
%     \item \textbf{Logical Errors}: The generated verifiers may be syntactically correct but do not conform to the Logical Constraints described in the puzzle. To reduce the possibility, we execute the Traversal Function and obtain the solution space. If the solution space has no valid arrangement, the check is failed.
% \end{enumerate}

% Although the above checks cannot fully guarantee the correctness of the generated verifiers, we have found in practice that this method results in about 3\% error (discussed in Section \ref{sec:analysis}).




% After transforming the original questions into open generative problems where feasible solutions must be found, evaluating the model's output becomes a challenging task. A common approach is to use rule-based matching. However, the size of the solution set is often uncertain, making it difficult for rules alone to provide comprehensive coverage. Another method involves using more powerful models to verify the correctness of the answers. But model-based scoring often suffers from instability and inherent biases, particularly in tasks that involve complex multi-step logical reasoning, where these issues become even more pronounced.

% To address these challenges, we propose a novel method of scoring that leverages the code generated by the model. As shown in Figure~\ref{fig:pipeline_overview}, our proposed approach uses model-generated code for evaluation. We found that these constraints can be effectively represented by code. Therefore, the model can use the logical constraints extracted in Section~\ref{sec:question_trans} to generate corresponding Constraint Verifiers. Based on the background description, we generate an arrangement format and arrangement example to define the domain, and then create a Format Verifier to check if the inputs fall within the specified range. By combining these elements, we construct a verification function to assess the correctness of both the solution's format and logic, as detailed in the appendix~\ref{}. Moreover, the inputs for these functions are essentially shared—namely, the feasible solution that needs to be validated. As we generate these functions, we also produce a description of the inputs, which acts as an explanation of the domain boundaries (\texttt{Inputs\_description}), and an example input (\texttt{Inputs\_example}).


% Together, the \texttt{Inputs\_check\_function} and the \textit{Logial Constraints} form a \texttt{Verify\_function}, a function designed to assess both the format and logical correctness of an individual solution. Complete details about how to construct such functions, along with examples, can be found in the Appendix~\ref{sec:}.


% Since the code is generated by the model, there may be potential issues such as syntax errors or logical flaws. Hence, an additional verification step is necessary to ensure the quality of the data. This verification is divided into two parts:


% \begin{enumerate}

%     \item \textbf{Syntax Errors}: Issues such as undefined variables, parameter mismatches, and so on, are checked using the Python interpreter. Specifically, we execute the \texttt{Verify\_function} using the \texttt{Inputs\_example} as input—in what we call a \textit{"Self-Check"} step—to ensure both syntactic correctness and compatibility between the \texttt{Verify\_function} and the provided input example.

    

%     \item \textbf{Logical Errors}: Issues like incorrect implementation of the \textit{Logical Constraints} are addressed by traversing the problem's domain. The \texttt{Verify\_function} is used to evaluate the solution space, and we then check the logical correctness of the \texttt{Verify\_function} based on whether a non-empty solution set is generated.

% \end{enumerate}
\label{sec:Data_Aug}
\subsection{Data Augmentation}
Stage 3 performs data augmentation through two complementary techniques, Reduction and Expansion, to construct a dataset with balanced difficulty distribution, enabling better discrimination of models' logical reasoning capabilities.

In the \textbf{Reduction} process, we randomly select and remove one logical constraint along with its corresponding components in the Verifier. By reducing the number of constraints, the problem is simplified, yielding more problems of lower difficulty. For each problem, we iteratively remove logical constraints to generate new problems until only one constraint remains.

In the \textbf{Expansion} process, we utilize advanced LLMs to generate additional constraints and their corresponding Verifiers, taking as input the Background and Logical Constraints from Stage 1, along with the solution space and Verifier from Stage 2. To ensure data quality, we leverage the Traversal Function developed in Stage 2 to verify the solvability of newly generated problems. The expansion process terminates when either the maximum number of attempts is reached or the solution space size reduces to one.

% \subsection{Data Augmentation}
% \cqy{Is this section Stage 3? If so, it needs to be pointed to match the figure.}
% We propose two complementary techniques - Reduction and Expansion - to augment existing puzzles into a series of logic puzzles with varying difficulty levels.


% During \textbf{Reduction}, we randomly select a logical constraint from the list and remove it, along with its corresponding verifier. By decreasing the number of constraints, the puzzles are simplified, resulting in larger solution space. For each puzzle, we iteratively remove a logical constraint to form new puzzles until there is only one logical constraint left.


% During \textbf{Expansion}, we prompt GPT-4 to generate new constraints based on background, current logical constraints, solution space, and constraint verifier.

% Note that the solution space (i.e., several valid arrangements) is provided during this step, making it more effective in generating constraints that can eliminate a portion of the valid arrangements.

% By incrementally adding constraints, we obtain a series of progressively more complex questions. We can either control the max number of constraints or continue adding constraints until the problem has a unique arrangement.
% The specific prompts used to guide this process are detailed in the Appendix~\ref{sec:appendix-prompt}. 

% \paragraph{Simplification}

% From Here to write!

% Differentiation is crucial for the quality of a test dataset. The differentiation is determined by the difficulty gradient of the problems, which underscores the importance of having data with varying difficulty levels in the evaluation set. In our method, we achieve this by augmenting (adding more logical constraints) and simplifying (randomly reducing the logical constraints from the original problem) the \texttt{constraints\_list} to create data with varying degrees of difficulty.


% \textbf{Simplification} is relatively straightforward. We randomly reduce the \texttt{bool function} within the \texttt{constraints\_list} introduced in Section~\ref{sec:constraints_list}, producing versions with different lengths, each constituting a new \texttt{Verify\_function}. Simultaneously, we simplify the corresponding textual descriptions of the constraints from Section~\ref{sec:logi_constraints} to form new \textit{Logical Constraints}. In this way, we can generate new problems featuring \texttt{constraints\_list} of varying lengths, leading to differences in difficulty levels.


% \textbf{Augmentation} requires leveraging a Language Model (LLM). Using the problem background, existing constraints, and (part of) the solution set as references, the LLM generates new constraints along with corresponding \texttt{bool functions}. These are then combined with the original constraints to produce new \textit{Logical Constraints} and a new \texttt{Verify\_function}. Starting from the problem defined in Section~\ref{sec:problem}, we iteratively add one constraint at a time to the existing set of constraints until the size of the solution set reduces to one. The added constraints are generated by the LLM, which references the previously existing constraints and examines the characteristics of the current solution set.


% In doing so, we obtain augmented problems with different levels of difficulty. In fact, similar to the verification steps outlined in Section~\ref{sec:verify_steps}, we conduct syntax and logical validations during the calculation of the solution set.

\subsection{Synthesizing Training Data}
Beyond benchmark construction, our data synthesis method can also be used to generate model training data through rejection-sampling with a verifier, naturally obtaining two categories: verified correct responses for Supervised Fine-Tuning (SFT) and pairs of correct-incorrect responses for DPO.

%A key advantage of our approach lies in its open-ended response format. 
A significant advantage of this approach is that rejection sampling with program-based verifiers produces more accurate training data. Unlike multiple-choice questions that risk accepting responses where models guess correctly despite flawed reasoning, potentially introducing noise into the training data, our method ensures accuracy by using program-based verifier to examine model responses.

%To leverage the synthesized training data, we can either directly employ Direct Preference Optimization (DPO) or adopt the ReFT approach, which incorporates an SFT warm-up phase before DPO to achieve enhanced training effectiveness.