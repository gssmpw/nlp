\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_data_2024

% ready for submission
\usepackage[final,nonatbib]{neurips_2024}

% to compile a preprint version, add the [preprint] option, e.g.:
%     \usepackage[preprint]{neurips_data_2024}
% This will indicate that the work is currently under review.

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_data_2024}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_data_2024}

% Submissions to the datasets and benchmarks are typically non anonymous,
% but anonymous submissions are allowed. If you feel that you must submit 
% anonymously, you can compile an anonymous version by adding the [anonymous] 
% option, e.g.:
%     \usepackage[anonymous]{neurips_data_}
% This will hide all author names.

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage[dvipsnames]{xcolor}         % colors
\usepackage{enumitem}
\usepackage[english]{babel}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tcolorbox}
\usepackage{wrapfig}
\title{Adversarial Prompt Evaluation: \\Systematic Benchmarking of Guardrails \\Against Prompt Input Attacks on LLMs}


\author{%
  {Giulio Zizzo\quad Giandomenico Cornacchia\quad Kieran Fraser\quad 
Muhammad Zaid Hameed}\\
{\textbf{Ambrish Rawat}}\quad \textbf{Beat Buesser}\quad\textbf{Mark Purcell}\quad \textbf{Pin-Yu Chen}\\\textbf{Prasanna Sattigeri}\quad \textbf{Kush Varshney}\\
  IBM Research\\
  \texttt{\{giulio.zizzo2,giandomenico.cornacchia1,kieran.fraser}\\
  \texttt{zaid.hameed,beat.buesser,pin-yu.chen\}@ibm.com}\\
  \texttt{\{ambrish.rawat,markpurcell\}@ie.ibm.com} \\
  \texttt{\{psattig,krvarshn\}@us.ibm.com} \\
}

\begin{document}

\maketitle


\begin{abstract}
As large language models (LLMs) become integrated into everyday applications, ensuring their robustness and security is increasingly critical.
In particular, LLMs can be manipulated into unsafe behaviour by prompts known as jailbreaks. The variety of jailbreak styles is growing, necessitating the use of external defences known as guardrails. While many jailbreak defences have been proposed, not all defences are able to handle new out-of-distribution attacks due to the narrow segment of jailbreaks used to align them.
Moreover, the lack of systematisation around defences has created significant gaps in their practical application.
In this work, we perform systematic benchmarking across 15 different defences, considering a broad swathe of malicious and benign datasets.
We find that there is significant performance variation depending on the style of jailbreak a defence is subject to.
Additionally, we show that based on current datasets available for evaluation, simple baselines can display competitive out-of-distribution performance compared to many state-of-the-art defences.
Code is available at
~\url{https://github.com/IBM/Adversarial-Prompt-Evaluation}. 
\end{abstract}

\section{Introduction}
\input{src/Sections/1_Introduction}

\section{Related Works}
\input{src/Sections/2_Nuance}

\section{Datasets}
\label{sec:datasets}
\input{src/Sections/3_Dataset}

\section{Model Defences}
\input{src/Sections/4_Models}

\section{Experimental Setting}
\input{src/Sections/5_Methods}

\section{Results}
\input{src/Sections/6_Results}

\section{Conclusion and Limitations}
\input{src/Sections/7_Conclusion}

\bibliographystyle{unsrt}
\bibliography{main}

\newpage
\input{src/Sections/Appendix}

\end{document}
