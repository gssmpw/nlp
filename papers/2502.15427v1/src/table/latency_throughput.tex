\begin{table}[!htp]\centering
\scriptsize
\begin{tabular}{lrrrrrr}\toprule
\textbf{Guardrail defence} &\textbf{Memory(GB)} &\textbf{Device} &\textbf{Hardware} &\textbf{Latency [s/sample]} &\textbf{Throughput [samples/s]} \\\midrule
\rowcolor{gray!7}Random Forest &0.418 &CPU &10 cores\_x86&0.0142+/-0.0001 &70.99+/-0.4559 \\
BERT &0.436 &GPU &V100 32GB & 0.0120+/-0.0107 & 142.92+/-2.652 \\
\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}DeBERTa &0.371 &GPU &V100 32GB &0.0189+/-0.0034 &60.81+/-1.46 \\
GPT2 &0.548 &GPU &V100 32GB &0.0132+/-0.0139 &118.53+/-5.162 \\
\rowcolor{gray!7}Protect AI (v2) &0.738 &GPU &V100 32GB &0.0184+/-0.0004 &54.75+/-1.2123 \\
Llama-Guard 2 &16.07 &GPU &V100 32GB &0.1413+/-0.0006 &7.62+/-0.0201 \\
\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}\rowcolor{gray!7}LangKit Injection & 0.091 & GPU & V100 32GB & 0.0117+\-0.0173 & 170.36+/-2.223 & \\
LangKit Injection & 0.091 & CPU & 10 cores\_x86 & 0.1101+\-0.0068 & 12.093+/-1.868 & \\
\rowcolor{gray!7}LangKit Proactive Defence & 13.48 & GPU & V100 32GB & 4.911+/-0.0246 & 0.2453+/-0.0007 \\
SmoothLLM\textsuperscript{\textdagger} & 13.48 & GPU & V100 32GB & 17.979+/-0.6397 & 0.1237+/-0.0020\\
\rowcolor{gray!7}Perplexity & 0.523 & GPU & V100 32GB & 0.0777+/-0.0023 & 16.7519+/-0.3320 \\
NeMo ({\tt Vicuna-13b-v1.5}) &26.03 &GPU &V100 32GB & 0.4914+/-0.0446 & 2.5405+/-0.0072\\
\rowcolor{gray!7}{\tt Vicuna-13b-v1.5}\textsuperscript{\textdagger} &26.03 &GPU &V100 32GB &  5.5665+/-0.0169 & 0.2831+/-0.0009\\
GraniteGuardian 3.0 & 16.34 & GPU & V100 32GB & 0.9495+/-0.0016  & 1.4349+/-0.0053 \\
\bottomrule
\end{tabular}
\caption{The memory usage, latency, throughput, and hardware requirements of various guardrails methods have been evaluated. Specifically, in the table's results we show the average and standard deviation on 100 random prompts repeated 10 times using a batch size of 1. The experiments were conducted on a scheduled job utilizing 10 cores of an Intel(R) Xeon(R) Gold 6258R CPU @ 2.70GHz, 50GB of RAM, and a NVIDIA's V100 GPU with 32GB of memory. Items marked with \textsuperscript{\textdagger} represent inferences which will return a full response rather than a binary classification, and thus incur higher latency times. Note that due to the order of inversing and averaging, latency and throughput are not exact inverses e.g. mean([1, 0.5, 2, 3]) $\neq$ mean ([1/1, 1/0.5, 1/2, 1/3]) }\label{tab:time_performance}
\end{table}