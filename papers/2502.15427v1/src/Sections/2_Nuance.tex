\textbf{Attacks:} Prompt injection describe attacks where crafted inputs aim to generate an inappropriate response. This can be achieved by circumventing existing safeguards via jailbreaks \cite{zou2023universal,zhu2023autodan,chao2023jailbreaking,wei2024jailbroken} or via indirect injection attacks~\cite{abdelnabi2023not,liu2023prompt}.
Here, adversarial prompts are crafted to pursue different goals like mislead models into producing unwanted output, leak confidential information, or even perform malicious actions \cite{zhang2023prompts, kim2024propile, perez2022ignore}. Furthermore, attacks can be categorised based on their methods of generation, e.g optimization-based attacks, manually crafted attacks, and parameter-based attacks that exploit the model's sampling and decoding strategies for output generation \cite{zou2023universal, deng2023multilingual}.

\textbf{Defences:} Strategies to defend against prompt injection attacks include safety training~\cite{piet2023jatmo,openai2023gpt4}, guardrails~\cite{rebedea2023nemo,DBLP:journals/corr/abs-2312-06674}, or prompt engineering and instruction management~\cite{wallace2024instruction,xie2023defending,zhang2024parden}.
These techniques have different resource requirements, and currently, there is neither a silver bullet to defend against prompt injection attacks, nor a way to prescribe a specific defense.
Our work on benchmarking guardrails creates a system of recommendations for defences against prompt injection attacks.

\textbf{Benchmarks:} Our first line of benchmarking work includes representative datasets of inputs generating unwanted outputs. 
One such repository of sources is available at \href{https://safetyprompts.com/}{\texttt{www.safetyprompts.com}}~\cite{r√∂ttger2024safetyprompts} and contains multiple databases characterised along dimensions of safe vs.\ unsafe. Our second line of work attempts to consolidate prompt injection attacks for comparison, which includes works like HarmBench~\cite{mazeika2024harmbench}, Jailbreakbench~\cite{chao2024jailbreakbench}, and EasyJailbreak~\cite{zhou2024easyjailbreak}.
However, defences have not received the same attention and there is currently no benchmarking suite specifically for guardrails.