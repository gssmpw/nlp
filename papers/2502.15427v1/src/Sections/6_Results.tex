\input{src/table/aggregate_detection_results}

Our main results are presented in Tables \ref{tab:sub_sample_aggregate_results}--\ref{tab:ood_aggregate_results} and Figures~\ref{fig:TPrate}--\ref{fig:FPrate_ood}. We now discuss their implications in the context of the three research questions presented in Section \ref{research_questions}.

\textbf{Are Current Benchmarks Sufficient? (RQ1)}: We test a range of different models, from a simple Random Forest classifier to more sophisticated LLM based guardrails on the datasets described in Section \ref{sec:datasets}; results are presented in Table \ref{tab:sub_sample_aggregate_results}, \ref{tab:ood_aggregate_results} and \ref{tab:combined_aggregate_results}. We can draw the following observations:

\begin{itemize}[leftmargin=*,noitemsep]
    \item Using NeMo style guardrails boosts the detection and refusal performance of Vicuna-13b, however, an increased false positive rate is also observed compared to the baseline model. 
    \item The classifier models based on BERT, DeBERTa and GPT2 achieved high AUC and accuracy values on the test dataset as shown in Table \ref{tab:sub_sample_aggregate_results}, and also generalised competitively to new datasets as shown in Table \ref{tab:ood_aggregate_results}, surpassing more computationally expensive open and closed source defences.
    \item For defences for which we do not control the training, and thus the \emph{in-distribution} and \emph{out-of-distribution} splits do not apply, we show the results of the combined set of data in Table \ref{tab:combined_aggregate_results}.
    \item  A simple random forest trained on unigram features can give competitive performance on their \emph{in-distribution} test data (Table \ref{tab:sub_sample_aggregate_results}). Although they do not generalise well to the OOD data (Table \ref{tab:ood_aggregate_results}), their minimal computational cost and training time indicates that they can act as a viable defence that can be continuously updated with new datasets.
    \item Guardrails based on LLMs generally boost the detection rate at the cost of an increase in FP rate. This FP rate increase is not necessarily uniform among datasets, e.g., SmoothLLM incurred significant penalties on BoolQ and XSTest datasets. This highlights that defences must be evaluated using a broad a set of datasets, as SmoothLLM defence's evaluation in the original paper did not show low performance on benign datasets.
\end{itemize}

Overall, based on current benchmark results on Tables \ref{tab:sub_sample_aggregate_results} and \ref{tab:ood_aggregate_results}, we can remark that: (i) \emph{either} the breadth and range of openly available data is sufficient to adequately represent jailbreak attack diversity, in which case simpler classification-based defences can provide competitive performance at a fraction of the compute cost. (ii) \emph{Or}, if we are to hypothesise that LLM-based defences \emph{can} generalise better than their classifier-based counterparts, then we do not currently have a rich-enough source of data to demonstrate this in the academic literature, particularly when some papers evaluate only on a small quantity of data \cite{robey2023smoothllm}. This highlights both the limitations of available datasets in covering the attack space and, consequently, the rapid growth of new unexplored attacks, which makes it challenging to evaluate a defence's generalisation capability. 

\begin{figure}
    \centering
    \centerline{\includegraphics[width=1\textwidth]{src/figure/TPrate_subsample.pdf}}
    \caption{Heatmap illustration of the true positive rates of different guardrails defenses on each jailbreak dataset. NB: the GCG attack was computed against Vicuna 13b.}
    \label{fig:TPrate}
\end{figure}
\begin{figure}
    \centering
    \centerline{\includegraphics[width=1\textwidth]{src/figure/FPrate_subsample.pdf}}
    \caption{FP rate heatmap results of different guardrails defence on each benign dataset.}
    \label{fig:FPrate}
\end{figure}


\textbf{How do the Guardrails compare beyond performance metrics? (RQ2)}:
\label{sec:results_rq2}
We record model size and inference conditions for comparing guardrails in practical use.
The latter determines how input prompts of different lengths are handled, the inference time for each request, and the throughput of the guardrail.
For LLM-based guardrails this includes the number of inferences required by the defence. Exact time and throughput results can be seen in the Appendix. 

Firstly, memory footprint of guardrails varies from as little as 91\,MB from LangKit Injection Detection scheme to host the embedding model, to as high as 26.03\,GB to handle the memory footprint of Vicuna 13b. Detection-based approach rely on an underlying classification pipeline and generally are amongst those with the highest memory vs performance ratios: with the transformers of BERT, DeBERTa, and GPT2 varying in memory footprint between 371MB - 548MB.

\input{src/table/ood_aggregate}

Secondly, inference scheme for the different guardrails is tightly coupled with their latency and throughput.
For any guardrail that implicitly relies on a transformer, the maximal token length of the model determines the length of input prompts that the system can handle.
Chunking and windowing can be used to extend this to strings of arbitrary length, but this will increase the inference time and reduce the throughput.

Lastly, LLM-based guardrails including NeMo and LangKit's Proactive defence can be used as standalone guardrails, or as modules that protect larger/unaligned LLMs.
Comparing NeMo with the baseline provides an insight into the added benefits of using the NeMo pre-filtering step. 
However, in this modality using LLM based schemes incur additional non-negligible inference calls. SmoothLLM can add up to 10 extra inferences, while NeMo adds 1 extra inference per prompt.

In conclusion, when comparing guardrails beyond performance for deployment scenarios, model size, inference performance, and response metrics are crucial. LLM-based approaches can require additional inference calls which may impact memory footprint, latency and throughput.

\begin{figure}
    \centering
    \centerline{\includegraphics[width=1\textwidth]{src/figure/TPrate_ood.pdf}}
    \caption{Heatmap illustration of the true positive rates of different guardrails defenses on each OOD jailbreak dataset.}
    \label{fig:TPrate_ood}
\end{figure}


\textbf{How to recommend guardrails for practical use? (RQ3)}:

Recommending a guardrail for practical use requires knowledge of the defender's capabilities.
With access to compute resources, guardrails can be deployed as a standalone service which filters every inference request before feeding it to an LLM.
Most of the guardrails we have discussed within the context of this work do not use additional information about the LLM they are seeking to protect.
However, one can envision scenarios where white-box access to the underlying LLM is used to determine and filter a prompt attack vector. We can draw the following suggestions:

\begin{itemize}[leftmargin=*,noitemsep]
\item As discussed in Section \ref{sec:results_rq2} guardrails have significantly different resource requirements. Currently, there is no one-size-fits-all solution. 
LLMs receive safety training and often continue to be updated for patching observed security vulnerabilities like jailbreaks and prompt injections.
Therefore, the choice of guardrail for an LLM depends on a model's inherent defense mechanism against these threats as there will be an overlap between their defense capabilities.
Moreover, the spectrum of threat vectors can vary from direct instructions to adversarial manipulation via persuasive language \cite{zeng2024johnny}, or even algorithmically computed strings \cite{zou2023universal}.
Off-loading the detection of all such vectors to one guardrail is a significant challenge requiring a large range of representative datasets to have an effective true positive rate vs.\ false positive rate trade-off.
\begin{figure}
    \centering
    \centerline{\includegraphics[width=1\textwidth]{src/figure/FPrate_ood.pdf}}
    \caption{FP rate heatmap results of different guardrails defence on each OOD benign dataset.}
    \label{fig:FPrate_ood}
\end{figure}

\item Another dimension to consider is the extensibility of these models to new attack vectors.
Score-based guardrails like the perplexity threshold filter is only parameterised by a threshold. Therefore, it does not need model training, and can be easily adapted to new scenarios.
Similarly, the LangKit detector may be extended to new scenarios by adopting the database of vectors used for similarity comparisons.
Classifier-based approaches require model re-training to extend to new attack vectors.
NeMo guardrail is only parameterised by its prompt, and so is highly extensible and can be used in combination with an aligned LLM.
Llama-Guard on the other hand is parameterised by a system prompt but in addition also relies on the underlying model that has been tuned for the task of safe vs unsafe classification.
Adopting Llama-Guard to new scenarios will therefore require both training and potentially changes to the prompt template.
Finally, while the guardrails seek a binary decision of safe vs unsafe, it is useful to assess their performance by considering a third dimension of unsure. LLM as a judge based defences may be more easily extended to include this third option via prompting. However, adopting the binary classifiers to such scenarios may require retraining or techniques like conformal calibration on output probabilities \cite{angelopoulos2021gentle}. 
\end{itemize}


\begin{table}[t]\centering
\scriptsize
\begin{tabular}{lccccc}\toprule
Guardrail defence &ACC &F1 &Recall &Precision \\ \midrule
Protect AI (v2) & 0.917 & 0.400 & 0.274 & 0.741\\
\rowcolor{gray!7} Llama-Guard 2 & 0.955 & 0.758 & 0.693 & 0.836\\
Langkit Injection Detection & 0.929 & 0.560 & 0.448 & 0.746\\
\rowcolor{gray!7} SmoothLLM & 0.817 & 0.439 & 0.713 & 0.317\\
Perplexity & 0.797 & 0.070 & 0.076 & 0.065\\
\rowcolor{gray!7} OpenAI Moderation & 0.086 & 0.157 & 0.845 & 0.086\\
Azure AI Content Safety & 0.924 & 0.412 & 0.264 & 0.941\\
\rowcolor{gray!7} Nemo Inspired Input Rail & 0.530 & 0.253 & 0.791 & 0.150\\
Langkit Proactive Defence & 0.868 & 0.501 & 0.656 & 0.405\\
\rowcolor{gray!7} Vicuna-13b-v1.5 Refusal Rate & 0.899 & 0.546 & 0.607 & 0.496\\
Granite Guardian 3.0 & 0.960 & 0.821 & 0.921 & 0.741\\
\bottomrule\end{tabular}\caption{Guardrails defence results on combined OOD and in-distribution datasets. Presented are defences on which the split does not apply (e.g. ones which we do not control the training of in this paper).}
\label{tab:combined_aggregate_results}
\end{table}

In conclusion, recommending a guardrail for practical use requires understanding the defender's capabilities, as guardrails vary significantly in resource requirements and extensibility to new attacks. The choice of a guardrail depends on the model's inherent defenses and the spectrum of threat vectors it faces, highlighting the need for a tailored approach rather than a one-size-fits-all solution.


