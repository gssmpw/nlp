\input{src/table/dataset_categorisation}

Our benchmarking is founded on a compilation of diverse datasets containing both benign and malicious prompts.
These datasets are categorized based on their target type, either ``\textit{jailbreak}'' or ``\textit{benign}'', and their details in terms of their splits, the number of samples, and the types of prompts they include is described in Table~\ref{tab:dataCat}.
The prompt types span several categories, including instruction-based, question-based, artificial attacks (e.g., those generated iteratively with the use of language models), role-playing, harmful behavior, toxic content, and chat-based interactions. A detailed dataset description is found in the Appendix.

This characterisation of jailbreak datasets is useful for contextualising guardrails.
Comparing guardrails across these datasets highlights their strengths and shortcomings in terms of handling different jailbreak styles.
Additionally, we include several benign datasets to assess the false positive rate, and thus the feasibility of deploying guardrail defenses in production.
Generalisation capability of guardrail beyond the anecdotally observed jailbreaks is critical to their deployment.
Our analysis of out-of-distribution evaluation set is specifically tailored for this analysis.




