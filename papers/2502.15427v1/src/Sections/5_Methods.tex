\label{sec:data_splits}
\textbf{In- and Out-of-Distribution Sample Sets:} We divide our datasets into two categories: {\em in-distribution} datasets for training the classifier-based detection mechanisms, and {\em out-of-distribution} (OOD) datasets that have not been used for training or validation of any of the models we train ourselves. In-distribution-sample datasets are divided into 80\% training and 20\% testing samples. The training dataset is divided into an additional 20\% validation split. For each dataset, we remove both within-dataset and cross-dataset duplicate samples.
 
Our \emph{in-distribution} datasets comprise AART, Alpaca, AttaQ, AutoDAN, Awesome ChatGPT Prompts, BoolQ, Do Not Answer, Gandalf Ignore Instructions, GCG, Harmful Behaviours, Jailbreak Prompts, Prompt Extraction, No Robots, Puffin, SAP, Super Natural Instructions, UltraChat, and XSTest. 

Our \emph{out-of-distribution} datasets include Dolly, Human Preference, instruction-dataset, Orca DPO Pairs, PIQA, ChatGPT DAN, Jailbreakchat, ToxicChat, and MaliciousInstruct. Note, that this split only applies to models we trained ourselves (e.g. the Random Forest and Transformer based models). For all other detectors the distinction between the two splits does not apply.

\textbf{Evaluation Set:}
To establish a standardised testing environment we sample up to 2000 random instances from each of the test splits of the \emph{in-distribution} datasets and filter for duplicates. This yields a total of 11387 samples, with 9543 benign and 1844 malicious samples.

For the \emph{out-of-distribution} datasets this gives 10327 benign and 376 malicious OOD samples.