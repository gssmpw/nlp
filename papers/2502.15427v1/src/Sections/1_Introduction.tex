Large language models (LLMs) have gained attention due to their advanced capabilities, and are increasingly becoming part of more complex systems \cite{yuan2022wordcraft, nakano2021webgpt, zhu2023multilingual}, which necessitates the requirement that these models be robust against adversarial manipulations.

LLMs not only inherit traditional security pitfalls like evasion and poisoning attacks~\cite{carlini2017towards, munoz2017towards}, but are also prone to safety vulnerabilities like jailbreaks and prompt injection attacks.
To make LLMs robust, they are usually trained/fine-tuned to produce safe output in a process called  `safety training' or `alignment' \cite{carlini2024aligned}.

To evaluate the safety aspects of aligned LLMs, prompt injection and jailbreak attacks are of particular importance, as they are employed to target aligned LLM models to produce adversarially-controlled outputs \cite{shayegani2023survey, zou2023universal, zhu2023autodan, wei2024jailbroken}.
As jailbreaks have been shown to break alignment of safety-trained models, additional layers of protection called guardrails have been proposed. These guardrails can be used in addition to the alignment process, and make the overall LLM-based system more secure.
Some of these guardrails can be composed of perplexity filters, tools for input prompt paraphrasing \cite{jain2023baseline}, keyword or semantic similarity based detectors \cite{LangKit}, or  output filters that monitor the response generated by LLMs to detect any harmful information \cite{helbling2023llm}. Despite showing improvement in defending against jailbreak attacks, these approaches have limitations and their applicability against more sophisticated attackers remains an open research problem \cite{shayegani2023survey, anwar2024foundational}. 

Currently, there are few benchmark frameworks for evaluating different guardrails as the approaches proposed in the literature vary widely in terms of evaluation approaches, representative datasets used for comparison, and metrics, e.g. string matching evaluation or BERT-based models for classifying the prompt as jailbreak or benign \cite{anwar2024foundational}. 
In this context, our work addresses the following research questions (RQs):

\begin{itemize}[noitemsep]
\label{research_questions}
    \item[\textbf{RQ1:}]  Are the currently available benchmarking datasets sufficient to adequately assess the quality of proposed guardrails, and how well do existing guardrails and defences perform on a wide cross-section of different attacks and datasets?
    \item[\textbf{RQ2:}] How do guardrails compare when considering additional constraints such as memory size, inference time, and extensibility?
    \item[\textbf{RQ3:}] How to approach and recommend guardrails to practitioners for deployment and use? 
\end{itemize}

Guided by the above RQs, and existing limitations in jailbreak evaluation benchmarks, we present an extensive benchmark evaluation with the following contributions:

\begin{enumerate}[leftmargin=*,label=\Roman*,noitemsep]
    \item We highlight the limitations of previous benchmark evaluations, and how they might result in inaccurate attack and defence evaluation. 
    \item We evaluate attack success rates on known adversarial datasets in a systematic manner, using an evaluation framework combining different evaluation metrics.  
    \item We evaluate different defences proposed in the literature, including different guardrails using the evaluation benchmark presented in this paper.
    \item We provide insights on whether model complexity in the defence provides better out-of-distribution (OOD) generalization.   
\end{enumerate}