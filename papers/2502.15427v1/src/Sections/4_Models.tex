
Broadly, defences can be categorised into two groups. First, are detection-based approaches that construct guardrails externally to the LLM to detect attacks. Second, are methods that use LLMs to judge and filter out malicious prompts based on their alignment coupled with a defence algorithm.

\subsection{Detection-Based Approaches}

\noindent\textbf{Perplexity Threshold:} This detector uses perplexity as a mechanism for detecting perturbations within prompts. We implement the perplexity filter from Jain et al. \cite{jain2023baseline}, which was proposed for identifying sequences of text that contain adversarial perturbation, like those added by GCG \cite{zou2023universal}. We use GPT-2 for computing perplexity, and fix a threshold at the maximum perplexity calculated over all prompts in the AdvBench dataset, as per the author implementation.

\noindent\textbf{Random Forest:} The classifier consists of a simple {\em random forest} trained on unigram features extracted from the training dataset (see Section~\ref{sec:data_splits}). The text corpus is initially transformed to lower-case and then tokenized, using each \textit{word} and \textit{punctuation} as single token (i.e., feature).

\noindent\textbf{Transformer Based Classifiers:} We implement a series of simple baseline classifiers consisting of the BERT \cite{bert}, DeBERTa \cite{deberta}, and GPT2 \cite{gpt2_link} architectures. The classifiers are fine-tuned to detect malicious vs non-malicious prompts over the training datasets described in Section \ref{sec:data_splits}.

\noindent\textbf{LangKit Injection Detection:} In this approach\footnote{\url{https://github.com/whylabs/langkit/tree/main}}, a prompt is transformed to its embedding and compared to embeddings of known jailbreaks. Cosine similarity is used as the closeness metric. The exact prompts used for constructing the malicious embedding are not specified by WhyLab's LangKit.

\noindent\textbf{ProtectAI:}
ProtectAI Guard is a security tool designed to detect prompt injection attacks. The model is a fine-tuned version of the {\tt microsoft/deberta-v3-base}\label{deberta} model, which is based on Microsoft's BERT Language Model and features 86 million backbone parameters~\cite{he2021deberta}. ProtectAI Guard is trained on a diverse dataset comprising prompt injections, jailbreaks, and benign prompts.  In this work, we utilize the newer v2\footnote{\url{https://huggingface.co/protectai/deberta-v3-base-prompt-injection-v2}} version available on Hugging Face.

\noindent\textbf{Azure AI Content Safety:} The Azure AI Content Safety API is a service provided by Microsoft Azure for moderating content safety~\cite{azureaicontentsafety}. It utilizes a combination of classification models designed to prevent the generation of harmful content. 
For our experiment, we use the jailbreak endpoint API\footnote{The version used for the current experiment is {\tt 2023-10-01-preview}}. 


\noindent\textbf{OpenAI Moderation}:
OpenAI Moderator \cite{protectai_link} is an AI-powered content moderation API designed to monitor and filter potentially harmful user-generated content~\cite{openaimoderation}. 
In our experiments, we use the {\tt text-moderation-007} model, which classifies content into 11 categories, each associated with a probability score. We treat content moderation as a binary classification task, where the highest probability among the harmful categories indicates the likelihood of a jailbreak.

\subsection{LLM as a Judge}

\noindent\textbf{Vicuna:} As a baseline we use the Vicuna-13b LLM model and assess if it refused to answer a particular prompt. We follow a similar strategy to \cite{zou2023universal,robey2023smoothllm} and check for the presence of refusal keywords to automate the output analysis.

\noindent\textbf{SmoothLLM:} SmoothLLM \cite{robey2023smoothllm} aims to tackle GCG-style attacks. The core of the defence is to perturb the prompt such that the functionality of the adversarial payload breaks, and the LLM then refuses to answer the question. The principal drawback is the high computational cost: each prompt needs to be perturbed multiple times which can incur an order of magnitude higher compute costs, and the defence is relatively specialised tackling only a particular style of jailbreak.

\noindent\textbf{LangKit Proactive Defence:} This defence \cite{LangKit,liu2023prompt} relies on the idea of supplying a specific secret string for the LLM to repeat when concatenated with user prompts. As many attacks will contain elaborate instructions to override system prompt directives, when under attack, the model will not repeat the secret string but rather respond to the adversarial prompt. 

\noindent\textbf{NeMo Guardrails:} NeMo guardrails \cite{rebedea2023nemo} provides a toolkit for programmable guardrails that can be categorized into topical guardrails and execution guardrails. The input moderation guardrail is part of the execution guardrails where input is vetted by a well-aligned LLM, and then passed to the main system after vetting it. 
The input moderation guardrail implementation in this work is inspired by the NeMo input moderation guardrail\footnote{\url{ https://github.com/NVIDIA/NeMo-Guardrails/tree/a7874d15939543d7fbe512165287506f0820a57b/docs/getting_started/4_input_rails}}. It is modified by including additional instructions and splitting the template between system prompt and post-user prompt, which guides the initial response of the LLM. Changes are specified in the Appendix. 

\noindent\textbf{Llama-Guard:} Llama-Guard is an LLM-based safeguard model specifically designed for Human-AI conversation scenarios~\cite{DBLP:journals/corr/abs-2312-06674}. We consider the more recent Llama-Guard-2 \cite{llamaguard2} which belongs to the Llama3 family of models.
Llama-Guard models function as binary classifiers, categorizing prompts as either ``\textit{safe}'' or ``\textit{unsafe}''.

\noindent\textbf{Granite Guardian:} Granite Guardian is a fine tuned version of the Granite-3.0-8B-Instruct model. It is tuned to detect jailbreaks and harmful content in prompts along several different axes (harm, jailbreaks, violence, profanity, etc), and outputs a ``\emph{yes}''/\,``\emph{no}'' result for detection.  