\section{Conclusion}

The ability of generative models to produce high-quality content relies on effectively modeling the real world. A common type of generative model, the latent diffusion model, first encodes real-world samples into a latent space using a variational autoencoder (VAE), then learns the distribution of samples within that latent space. This generative paradigm implies that the modeling capability of the VAE directly influences the final generation results. Traditional VAEs compress images through reconstruction tasks, which only consider pixel-level local information and fail to capture the semantic priors of images effectively.

This paper enhances the semantic information in the latent space by aligning the VAE's latent space with semantic representation models. Experimental analysis shows that the latent space aligned with semantic representations exhibits better structural properties, characterized by increased diversity among different samples and enhanced correlations within the same sample. Generation experiments demonstrate that a semantically rich latent space is crucial for improving the generation quality of diffusion models. Furthermore, due to its rich semantics, diffusion models trained in this latent space inherently possess capabilities for various training-free perceptual downstream tasks.