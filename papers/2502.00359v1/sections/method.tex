\section{Method}

\paragraph{Overview.} As a leading paradigm in generative modeling, the latent diffusion model (LDM)~\cite{rombach2022ldm} operates in latent space. During training, a visual encoder first reduces the image from the original pixel space to the latent space. Diffusion model is then trained in this latent space through processes of adding noise and denoising. In the generation phase, LDM iteratively denoises the sampled latent noise into a clean latent representation, which is then converted into an image using a corresponding decoder. Traditional latent spaces primarily serve as spatial compressors and often lack the semantic information which is crucial for generation tasks. This work enhances the latent space by aligning semantic representations within a VAE, resulting in a more robust semantic structure that not only improves the quality of diffusion-generated images but also facilitates downstream tasks such as segmentation and depth detection.

\begin{figure*}
    \centering
    \includegraphics[width=0.99\linewidth]{figs/pipeline.jpg}
    \caption{\textbf{The training and inference pipeline of ReaLS.} During VAE training, the latents of the VAE are aligned with the features of DINOv2 using an alignment network implemented via MLP. After the VAE training concludes, latent diffusion model training is performed in this latent space. In the inference phase, the latents generated by the diffusion model are converted into corresponding generated images through the VAE decoder. At the same time, the alignment network extracts semantic features, which are provided to the corresponding downstream task heads, enabling training-free tasks such as segmentation and depth estimation.}
    \label{fig:framework}
    \vspace{-0.5cm}
\end{figure*}

\subsection{Preliminary}
\label{Sec:preliminary}
\paragraph{Variational Auto-Encoder.} 

Variational Autoencoders (VAE)~\cite{doersch2016tutorial} are a type of generative model that encodes images from pixel space to latent space by learning image reconstruction.
Let  $\vx \in \sR^{3 \times H \times W}$ represent an RGB image, where \(H\) and \(W\) denote its height and width, respectively. A VAE typically consists of two main components: an encoder and a decoder. The role of the encoder is to map the input data \(\vx\) to a latent space \(\vz \in \sR^{\frac{H}{p} \times \frac{W}{p} \times D}\) that follows a Gaussian distribution, where \(p\) represents patch size. This mapping is mathematically represented as:
% \begin{equation}
$q_\phi(\vz|\vx) = \mathcal{N}(\vz; \mu_\phi(\vx), \sigma^2_\phi(\vx) \mI) , $
% \end{equation}
where \(\mu_\phi(x)\) and \(\sigma^2_\phi(x)\) are computed by a neural network parameterized by \(\phi\). 
% During this process, it is essential for \(\vz\) to approximate a standard normal distribution, as this ensures the generative model's ability to generalize, meaning that noise sampled from the normal distribution can be decoded into high-quality images.
During this process, \(\vz\) approximately satisfies a standard normal distribution, so a noise sampled from the normal distribution can be decoded into a high-quality image.
Therefore, a KL divergence loss constraint is added, as shown in the following formula:
% \begin{equation}
$\mathcal{L}_{KL} = D_{KL}(q_\phi(\vz|\vx) || p(\vz)).$
% \end{equation}


After the encoding process, the decoder reconstructs the original data from the latent representation. It models the data distribution based on the latents to generate new samples:%, as follows:
% \begin{equation}
$p_\theta(\vx|\vz) = \mathcal{N}(\vx; \mu_\theta(\vz), \sigma^2_\theta(\vz) \mI) , $
% \end{equation}
where \(\mu_\theta(\vz)\) and \(\sigma_\theta(\vz)\) is computed by a neural network parameterized by \(\theta\). This collaborative structure between the encoder and decoder makes VAE a powerful tool in generative modeling.

\paragraph{Latent Diffusion Model.} 

Latent Diffusion Models (LDM) are a type of diffusion model trained in the latent space.
During training, LDM learns to predict the noise in the input latents that have been perturbed by various levels of Gaussian noise. During inference, starting from pure Gaussian noise, the LDM progressively removes the predicted noise and ultimately obtains a clean latent.
Since LDMs generate data in latent space, a well-structured latent space that incorporates both low-level pixel information and high-level semantic information is crucial for high-quality image generation.
In this paper, we demonstrate that by aligning with semantics, we can construct a more structured latent space, effectively enhancing the quality of the generated outputs.

% first adds noise to the latent representation \( \vz\) and then learns the denoising process through a network.
% Taking noise prediction as an example, LDM first adds noise to the latent representation \( \vz\) and then learns the denoising process through a network.
% This can be expressed with the following formulas:
% \begin{equation}
% z_{t-1} = z_t - \epsilon_\theta (z_t, t)
% \end{equation}

\subsection{Representation Alignment}

Traditional VAEs compress images into latent space through reconstruction tasks, resulting in a latent space that serves merely as a compressed representation of pixel data and lacks crucial semantic information. We enhance VAE training by incorporating semantic representation alignment, enriching the latent space with semantic content, which facilitates diffusion generation within this space.

Specifically, we use DINOv2~\cite{oquab2023dinov2} as the image semantic representation extractor. For an input image \( \vx \in \sR^{3 \times H \times W}\), DINOv2 outputs two types of features: a)~the image patch feature, denoted as \(\mathcal{F}_\text{p} \in \sR^{\frac{H}{p'} \times \frac{W}{p'} \times D'}\) where \(p'\) is the patch size of DINOv2; b)~the global image feature, denoted as \( \mathcal{F}_{\text{cls}} \in \mathbb{R}^{D'}\).
To align with \(\mathcal{F}_\text{p}\), we ensure that the patches obtained from DINOv2 have a one-to-one correspondence with VAE latents. Formally, we resize the image to \((H', W')\) before feeding it into DINOv2, where \((\frac{H'}{p'}, \frac{W'}{p'}) = (\frac{H}{p}, \frac{W}{p})\).
To align with \(\mathcal F_{\text{cls}}\) that reflects the global semantics of the image, such as object categories, we average the VAE latents across the spatial dimensions to gather the global information of the image.

Subsequently, through two align networks implemented with Multilayer Perceptron (MLP), we map the latents from dimension $D$ to the DINOv2 feature dimension $Dâ€™$:

\vspace{-0.5cm}
\begin{equation}
\left\{\begin{matrix}
\mathcal F_{\text{vae},ij}=\text{MLP}_{\text{patch}}(\vz_{ij})    \\
\mathcal F_{\text{vae},\text{cls}}=\text{MLP}_{\text{cls}}(\text{AP}(\vz))
\end{matrix}\right. ,
\vz = \mu_\phi(\vx)+\sigma_\phi(\vx)\epsilon,
\label{equ:alignnet}
\end{equation}
where \(\mu_\phi(\vx)\) and \(\sigma_\phi(\vx)\) are the mean and variance estimated by the VAE encoder, \(\vz\) is obtained by the reparameterization, \(\epsilon\) is a random noise, $\text{AP}(\cdot)$ denotes average pooling.


For the alignment loss, we use a combination of cosine similarity loss and smooth mean squared error (MSE) loss:
\begin{equation}
\gL_{\text{align}} = \lambda_1 \gL_{\text{cos}}(\mathcal F_{\text{vae}}, \mathcal F_{\text{dino}}) + \lambda_2 \gL_{\text{smMSE}}(\mathcal F_{\text{vae}}, \mathcal F_{\text{dino}}) .
\end{equation}
In actual experiments, we set $\lambda_1=0.9$ and $\lambda_2=0.1$.

\subsection{Optimization Objectives}
\label{sec:loss}
The training loss of the VAE can be divided into two parts. The first part is on the pixel space, which ensures that the reconstructed image is consistent with the original image. To improve the quality of the reconstructed image and prevent blurriness, the reconstruction loss also incorporates adversarial loss~\cite{creswell2018generative} and perceptual loss (LPIPS)~\cite{rad2019srobb}, as shown below:
% \vspace{-0.2cm}
\begin{equation}
\gL_{\text{pixel}} = \gL_{\text{MSE}} + \lambda_g \gL_{\text{GAN}} + \lambda_p \gL_{\text{perceptual}} .
\end{equation}
The second part of the loss is on the latent space. In traditional VAEs, a KL divergence loss is typically applied to the latents to ensure that \(\vz\) approximates a standard normal distribution \(\mathcal{N}(\mathbf{0},\mI)\). The KL loss enhances the cohesion of the latent space, allowing \(\vz\) obtained from different images to share a same space, which facilitates the diffusion model to sample and denoise from a normal distribution. Additionally, we introduce semantic constraints on the latent space through our alignment network, imparting semantic priors to $\vz$. As shown in Figure~\ref{fig:cluster}, our VAE exhibits a clear clustering in the latent space, despite not using image class labels during training. In summary, the loss on the latent space can be expressed in the following form:
% \vspace{-0.2cm}
\begin{equation}
\gL_{\text{latent}} = \lambda_{k} \gL_{\text{KL}} + \lambda_{a} \gL_{\text{align}} .
\label{equ:latent loss}
\end{equation}
\(\gL_{\text{latent}}\) guides the construction of an improved latent space from two dimensions. The KL loss constrains the overall integrity of the latent space, independent of individual samples. In contrast, \(\gL_{\text{align}}\) applies to each sample, enabling different semantic samples to exhibit diversity while making similar semantic samples have similar representations. Further analysis of these two losses on the latent space and the final generated quality will be discussed in the Section~\ref{sec:kl analysis}.
Finally, the total training loss is as follows:
% \vspace{-0.3cm}
\begin{equation}
\gL = \gL_{\text{pixel}} + \gL_{\text{latent}} .
\end{equation}

\subsection{Generation with Downstream Tasks.}

After completing the VAE model training, we proceed to train the diffusion model in the latent space. To highlight the improvement in generation quality from the semantically aligned latent space, we do not modify the architecture or training process of the diffusion models.

The semantically aligned latent space provides enhanced semantic priors for generation, improving the quality of the model outputs. Additionally, since the diffusion model is trained in this semantically rich latent space, the generated latents are equipped for various perceptual tasks, such as semantic segmentation and depth estimation.
Specifically, the latents produced by the diffusion model can be mapped to features in the DINOv2 dimension via the alignment network used during VAE training. With the corresponding segmentation and depth estimation heads, we can directly obtain the segmentation masks and depth information for the generated images, as shown in Figure~\ref{fig:visual_2}. This not only demonstrates that the latent space captures richer semantic features through the alignment loss, but also expands the applicability of the generative model to downstream tasks.