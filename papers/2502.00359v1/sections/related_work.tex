\section{Related Work}
\subsection{Variational Autoencoders in LDM Paradigm} 
% \paragraph{Variational Autoencoders for Visual Generation.} 
Stable Diffusion~\cite{rombach2022ldm} introduced the latent diffusion model paradigm, employing a variational autoencoder (VAE)~\cite{kingma2013vae} (SD-VAE), the most widely used VAE, to encode visual signals from image space into latent space and decode these latent tokens back into images. This approach has facilitated the training and scaling of diffusion models, establishing itself as the dominant choice for visual generation. The quality of the VAE sets the upper limit for generative models, prompting significant efforts to enhance VAEs. SDXL~\cite{podell2023sdxl} retains the SD-VAE architecture while adopting advanced training strategies to improve local and high-frequency details. LiteVAE~\cite{sadat2024litevae} utilizes the 2D discrete wavelet transform to boost scalability and computational efficiency without compromising output quality. SD3~\cite{esser2024sd3} and Emu~\cite{dai2023emu} expand the latent channels of VAEs to achieve better reconstruction and minimize information loss. DC-AE~\cite{chen2024dcae} and LTX-Video~\cite{hacohen2024ltxvideo} increase the compression ratio while maintaining satisfactory reconstruction quality. 

These VAEs often focus on improving image compression and reconstruction. However, we found that better reconstruction does not necessarily lead to better generation (discussed in detail in Section~\ref{discussion}). This paper explores enhancing the generation quality of LDMs by injecting semantic representation priors into the latent space, providing new insights for improving VAE training.


% \vspace{-0.2cm}

\subsection{Diffusion Generation and Perception} 

Beyond image generation, diffusion models have been increasingly applied to a variety of downstream perceptual tasks. VPD~\cite{zhao2023vpd} leverages the semantic information embedded in pre-trained text-to-image diffusion models, utilizing additional specific adapters for enhanced visual perception tasks. Marigold~\cite{ke2024marigold} repurposes a pre-trained Stable Diffusion model into a monocular depth estimator through an efficient tuning strategy. JointNet~\cite{zhang2023jointnet} and UniCon~\cite{li2024unicon} employ a symmetric architecture to facilitate the generation of both images and depth, incorporating advanced conditioning methods to enable versatile capabilities across diverse scenarios. SDP~\cite{ravishankar2024sdp} utilizes a pre-trained DiT-MoE model on ImageNet, exploring the advantages of fine-tuning and test-time computation for perceptual tasks. 

The models mentioned above do not seek to unify generation and perception within the latent space. In this work, LDM trained on ReaLS is inherently rich in semantics and enables training-free execution of downstream perceptual tasks, including segmentation and depth estimation.