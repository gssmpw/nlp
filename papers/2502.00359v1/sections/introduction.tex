\section{Introduction}

% generation and diffusion models
The objective of generative models is to accurately capture and model the distribution of the real world, enabling the creation of outputs that are not only visually compelling but also semantically coherent. Existing diffusion-based generative models~\cite{dit, chang2022maskgit, rombach2022ldm} typically sample from a random distribution, e.g., a Gaussian distribution, and then iteratively refine the samples to approximate the distribution of the real world. These models achieve remarkably successful results in fields such as image, audio, and video generation~\cite{bar2023multidiffusion, huang2023make, ho2022video}.


\begin{figure}[t]
    \centering
    \includegraphics[width=0.90\linewidth]{figs/cluster.jpg}
    \vspace{-0.6cm}
    \caption{\textbf{Representation-Aligned Latent Space (ReaLS) preserves more image semantics.} a) t-SNE visualization of our latent space reveals a clear clustering, with samples from the same category closer to each other. b) Attention map of our latents shows a significant improvement in the semantic relevance among patches.}
    \label{fig:cluster}
    \vspace{-0.7cm}
\end{figure}


Latent diffusion models (LDMs)~\cite{rombach2022ldm}, as a typical type of generative model, commonly utilize Variational Autoencoders (VAEs)~\cite{doersch2016tutorial} to enhance training and inference efficiency. VAEs first encode real samples into a latent space with spatial compression, where diffusion is performed to fit the latent distribution. However, the capability of the VAE's modeling of the real world limits the quality of the final samples generated by the LDM. Therefore, \textbf{\textit{developing a more effective latent space for diffusion models is essential, yet remains underexplored}}.

Traditional VAEs are optimized to compress images into more compact latent representations, prioritizing local textures at the expense of global image context. This local encoding property results in the VAE latent lacking rich semantic information about the images, which is crucial for perceiving the real world~\cite{yu2024representation}. 

To specifically illustrate the limitations of traditional latent space, we present t-SNE and attention map visualizations of SD-VAE~\cite{rombach2022high}, a widely used VAE in LDMs. Figure~\ref{fig:cluster} reveals two key observations: a) t-SNE visualization indicates that it struggles to represent the characteristics of different categories within the latent space; b) the attention maps of the latents show that it fails to capture the relationships between different parts of the same instance. These observations highlight the lack of semantic representation in SD-VAE, which hinders LDM learning. Consequently, although the generated outputs may appear visually plausible, they often fall short of achieving semantic congruence with the intended descriptions or tasks. 

In this work, we construct a semantically rich latent space through a new VAE training strategy, which not only compresses the original image but also preserves the inherent relationships within the data. Unlike traditional VAEs that apply KL constraints~\cite{doersch2016tutorial} solely in the latent space, we align the VAE's latents with features from DINOv2~\cite{oquab2023dinov2}, explicitly injecting semantic representations of images into the latent space. During training, we found that the quality of images generated by LDMs is closely related to the balance between the KL divergence constraint and alignment constraint in the latent space. This is because the KL constraint and the alignment constraint provide guidance for unity and differentiation, respectively. The former focuses on the overall consistency of the latents, driving them toward a standard normal distribution, while the latter considers the semantic differences between samples. When these two constraints are balanced, the latent space approximates a standard normal distribution while retaining the semantic features, as illustrated in Figure~\ref{fig:cluster}.

Extensive experiments demonstrate that existing generative models, such as DiT~\cite{dit} and SiT~\cite{sit}, benefit from Representation-Aligned Latent Space (\textbf{ReaLS}) without requiring modifications. It achieves a notable 15\% improvement in FID performance for image generation tasks. Additionally, the richer semantic representations in the latents enable more downstream perceptual tasks, such as image segmentation~\cite{minaee2021image} and depth estimation~\cite{ming2021deep}.

We summarize the contributions of this paper as follows:
\vspace{-0.15cm}
\begin{compactitem}
  \item We propose a novel representation-alignment VAE, which provides a better latent space for latent generative models with semantic priors.
  \item Representation-Aligned Latent Space (\textbf{ReaLS}) can significantly improve generation performance of existing LDMs without requiring any changes to them.
  \item The semantically rich latent space enables downstream perceptual tasks like segmentation and depth detection.
\end{compactitem}
