\newpage
\appendix
\onecolumn
\section*{Appendix}

\subsection*{Loss Hyperparameters}

The complete form of the loss function is shown in Equation~\ref{loss all}. During actual training, we set $\lambda_g=0.1$, $\lambda_p=1.0$, $\lambda_{k}=2e-5$, $\lambda_{a}=1.0$.

\begin{equation}
\gL = \gL_{\text{MSE}} + \lambda_g \gL_{\text{GAN}} + \lambda_p \gL_{\text{perceptual}} + \lambda_{k} \gL_{\text{KL}} + \lambda_{a} \gL_{\text{align}}
\label{loss all}
\end{equation}


% \subsection*{Reconstruction}
% Table~\ref{rec} shows the reconstruction metrics of our VAE on ImageNet $256\times256$. Although the reconstruction metrics of our VAE show a slight decline compared to SD-VAE, it provides a semantically rich latent space for the diffusion model, enhancing generation performance. This indicates that higher reconstruction quality does not necessarily lead to better generation results.

% \begin{table}[ht]
% \vskip 0.15in
% \caption{\textbf{Reconstruction Metrics of VAEs.}}
% \begin{center}
% \begin{small}
% % \begin{sc}
% \resizebox{7.5cm}{!}{
% \begin{tabular}{l|ccccc}
% \toprule
% Model   & rFID$\downarrow$ & PSNR$\uparrow$ & SSIM$\uparrow$ \\ \midrule
% SD-VAE~\cite{rombach2022high}  & 0.74             & 25.68          & 0.820          \\
% VQGAN~\cite{esser2021taming}   & 1.19             & 23.38          & 0.762          \\
% Ours    & 0.85             & 23.45          & 0.768          \\
% \bottomrule
% \end{tabular}
% }
% % \end{sc}
% \end{small}
% \end{center}
% \label{rec}
% \vskip -0.1in
% \end{table}

\subsection*{Latent Normalization}

As illustrated in Figure~\ref{fig:kl}, the KL weight has a significant impact on the final generation results. Additionally, the KL weight plays a crucial role in the distribution of the latent variables, as different KL weights can significantly affect the mean and variance of the latent space, as shown in Table~\ref{mean std}. Therefore, during the training of the diffusion model, to maintain consistency with the training of SD-VAE, we normalized the latents to the same numerical range as that of SD-VAE.

\begin{table}[ht]
\vskip 0.15in
\caption{\textbf{The distribution of latent space changes with kl weight.} Calculate the value by sampling 10,000 samples from the ImageNet $256\times256$, encoded by VAE aligned with DINOv2-base.}
\begin{center}
\begin{small}
% \begin{sc}
\resizebox{7.5cm}{!}{
\begin{tabular}{c|ccccc}
\toprule
kl weight   & mean & std & min & max \\ \midrule
SD-VAE   & 0.29287 & 4.58407 & -65.730 & 68.3175 \\
0        & 1.32886 & 5.42394 & -48.074 & 38.0941 \\
1.00E-06 & -0.0463 & 1.3918  & -8.8677 & 7.2072  \\
5.00E-06 & 0.00251 & 1.0678  & -7.9659 & 9.8775  \\
7.50E-06 & -0.0059 & 1.03842 & -8.2779 & 11.661  \\
1.00E-05 & -0.0092 & 1.02894 & -7.0984 & 9.31086 \\
1.50E-05 & -0.0091 & 1.02723 & -9.1763 & 11.1787 \\
2.00E-05 & 0.00394 & 1.0266  & -15.916 & 17.6631 \\
3.00E-05 & -0.0148 & 1.0256  & -15.192 & 16.1069 \\
5.00E-05 & -0.0002 & 1.00952 & -12.118 & 13.2441 \\
\bottomrule
\end{tabular}
}
% \end{sc}
\end{small}
\end{center}
\label{mean std}
\vskip -0.1in
\end{table}


In terms of normalization methods, we experimented with $\text{std}$ normalization and $\text{max}-\text{min}$ normalization, as presented in Table~\ref{norm}. The experiments indicate that using $\text{max}-\text{min}$ normalization yields better generation performance.

\begin{table}[ht]
\vskip 0.15in
\caption{\textbf{The impact of different normalization methods of latents on the quality of generation.} Use kl weight=5e-6, SiT-B/2 model at 400k optimization steps.}
\begin{center}
\begin{small}
% \begin{sc}
\resizebox{4cm}{!}{
\begin{tabular}{c|ccccc}
\toprule
normalization method & FID \\ \midrule
$\text{std}$ & 40 \\
$\text{max}-\text{min}$  & 32 \\
\bottomrule
\end{tabular}
}
% \end{sc}
\end{small}
\end{center}
\label{norm}
\vskip -0.1in
\end{table}

Specifically, during encoding, the latents are scaled by Equation~\ref{encode}, and during decoding, the latents generated by diffusion are scaled by Equation~\ref{decode}.

\begin{equation}
\left\{
\begin{matrix}
z=(z-\text{mean}_\text{ours})/(\text{max}_\text{ours}- 
\text{min}_\text{ours}) \\
z=z\times(\text{max}_\text{SD-VAE}-\text{min}_\text{SD-VAE})+\text{mean}_\text{SD-VAE} 
\end{matrix}
\right.
\label{encode}
\end{equation}

\begin{equation}
\left\{
\begin{matrix}
z=(z-\text{mean}_\text{SD-VAE})/(\text{max}_\text{SD-VAE}- 
\text{min}_\text{SD-VAE}) \\
z=z\times(\text{max}_\text{ours}-\text{min}_\text{ours})+\text{mean}_\text{ours} 
\end{matrix}
\right.
\label{decode}
\end{equation}

where $\text{mean}_\text{ours}=-0.016722$, $\text{max}_\text{ours}=10.762420$, $\text{min}_\text{ours}=-6.862830$, $\text{mean}_\text{SD-VAE}= 0.292873$, $\text{max}_\text{SD-VAE}=68.317589$, $\text{min}_\text{SD-VAE}=-65.730583$ for VAE aligned with DINOv2-large-reg (kl weight=2e-5).

