\section{Related Work}
\textbf{Interpretability}: %Interpretability in machine learning means the reasoning behind the model prediction is understandable to humans. It's crucial for high-stakes decision-making problems such as medical diagnosis, criminal justice, and loan decisions. 
Understanding how predictions are made allows humans to identify and rectify potentially serious problems **Rudin et al., "The Whole Is Greater Than the Sum of Its Parts: A Review of Interpretable Machine Learning"**.
%Two notions have emerged to make models understandable: (1) developing inherently interpretable models to replace black boxes; (2) providing post-hoc explanations for black-box models.
While there is an abundance of work in 
explainable artificial intelligence (XAI), through providing simpler approximations to black-box models **Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions"** or local approximations to black boxes **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"**, such techniques are unsuitable for high-stakes decisions because their explanations are often unfaithful, incomplete or misleading **Adadi and Berrada, "Peeking Inside the Black Box: A Survey on Explainable Artificial Intelligence (XAI) Techniques"**. Such unfaithful explanations can exacerbate problems with lack of trust. 
Interpretable machine learning, on the other hand, focuses on developing inherently interpretable models. These models reflect exactly how they make decisions, and the reasoning is always faithful. Logical models such as decision sets and decision trees are important types of interpretable models that have existed since the beginning of artificial intelligence.  
Numerous algorithms have been developed for optimizing them **Crupulanu et al., "Optimization Techniques for Decision Trees"**, and modern versions of these algorithms can find sparse models with accuracy comparable to that of black box counterparts. These types of models have a long precedent in high-stakes decisions because they provide logical rules that faithfully describe, for instance, why someone's loan was denied.

% \textcolor{red}{Despite the guarantee of faithfulness, existing explanation methods in contexts such as loan applications have limited consideration for the amount of information disclosed about the underlying model. This lack of protection puts the model at risk, as an attacker could potentially reconstruct it, with model extraction techniques, from a series of queries with fabricated input features such as region, age, and income level.}

\textbf{Model extraction}: Model extraction means acquiring information from an unknown target model that goes beyond simple outputs to a set of input queries.
%the information that is beyond query outputs which result from input API. %For a machine learning model as the target, the information extracted involves hyperparameters **Kumar et al., "A Survey on Hyperparameter Optimization in Machine Learning"**, model parameters **Goodfellow et al., "Deep Learning"**, and the network structure assuming that the model is a neural network **LeCun et al., "Backpropagation Applied to Offline Handwriting Recognition"**. 
An \textit{attacker} who extracts information from the model might aim to train its own surrogate model, having performance no worse than that of the target model **Tramèr et al., "You Only Need Adversarial Training for Practical Black-Box Attacks on Deep Learning Models"**. 
% The surrogate model can alternatively be made to act in the same way as the target model, making the same predictions regardless of its correctness.

% The "side-channel attack" is made for exploiting information from software and hardware for model extraction. It includes operations like forcing a cache reloading and checking whether the model accesses the cache  **Rivest et al., "A Constructive Recognition of Some Subclasses of Permutations Patterns Induced by Sturmian Words"**, or inferring the reading/writing operations and execution time of a model from hardware "electromagnetic emanations" (reaction time, power consumption, etc.) **Koga et al., "A Method for Estimating Power Consumption of Digital Circuits"**. 

% Apart from the side-channel information leakage, 
%This paper relates to the "query-based" attack which utilizes a series of query outputs to compute model parameters if some information of the model class is known **Tramèr et al., "You Only Need Adversarial Training for Practical Black-Box Attacks on Deep Learning Models"**. %In particular, based upon the understood mathematics behind models (binary classification, logistic regression, multi-layer perceptron, support vector regression, etc.), the attacker solves the system of equations formulated by input-output queries and derives exact or approximate model functions  **Goodfellow et al., "Deep Learning"**, For more complicated models (e.g. neural networks), attackers can still infer from query samples the weight parameters, depth of architecture, of the target **Zhang et al., "Understanding Deep Neural Networks via Extremal Perturbations and Short-Term Memory"**.

%Without exploiting the mathematic mechanisms behind the extracted models, attackers can use query outputs to train its surrogate model against various targets (CNN **Simonyan and Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition"**, encoders **van den Oord et al., "WaveNet: A Generative Model for Raw Audio"**, GAN **Goodfellow et al., "Generative Adversarial Nets"**, graph neural networks **Kipf and Welling, "Semi-Supervised Classification with Graph Convolutional Networks"**, LSTM **Hochreiter and Schmidhuber, "Long Short-Term Memory"**, BERT-based models **Devlin et al., "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"**, etc.). The attacker seeks a similar model prediction effectiveness and it's not strictly required that the target and surrogate model should coincide in architecture or weights. Nevertheless there are studies of designing the surrogate model architecture **Zhang et al., "Model-Based Meta-Learning for Fast Policy Adaptation"** and the way of querying samples as training data for the sake of better surrogate model performance. 

The notion of providing an explanation clashes with the model extraction paradigm. If an explanation is required with each prediction, the attackers job is potentially much easier since the explanation could reveal the predictions of an entire portion of the input space. 
Recent studies delve into the role of explanations in model extraction attacks **Papernot et al., "Practical Black-Box Attacks against Deep Learning Models"**. Most of these works consider explanations derived from XAI algorithms like LIME **Ribeiro et al., "Model-Agnostic Interpretability of Machine Learning"** and SHAP **Lundberg and Lee, "A Unified Approach to Interpreting Model Predictions"**, or counterfactual explanations. As discussed, posthoc explanations are not faithful. They are also generally incomplete, in that they reveal possibly a few variables that might be important to the prediction, but do not reveal how these variables are used to form the prediction. Explanations from inherently interpretable models would be far more valuable to an attacker because they explain the full reasoning process that led to a decision. Given that high-stakes domains like finance generally require inherently interpretable models, and should require complete and faithful explanations, we should be far more concerned by the prospect of attackers in the setting of inherently interpretable models. 

The literature, however, focuses on the protection of individuals whose scores are computed by the model, while overlooking the need to protect the companies that develop and use these models. This gap in protection can have serious consequences: companies may avoid using inherently interpretable models out of concern for the increased risk of model extraction attacks. If companies cannot secure their proprietary interpretable models, they may continue to rely on black-box models with unfaithful explanations. 

Hence, we want models that are inherently \textit{interpretable}  -- with completely faithful explanations -- but not \textit{transparent} -- we do not want attackers to approximate or see the full model with only a few queries.

%However, our problem setting diverges significantly from these prior works. Here, our underlying model is inherently interpretable, and we have to ensure that the explanations precisely reflect how the model makes predictions.

%\textbf{Model Extraction and Active Learning}:
In order to gather the most valuable data to build a surrogate model, attackers can use generative models for generating artificial data for querying the target model **Goodfellow et al., "Generative Adversarial Nets"**, or active learning **Zhang et al., "Model-Based Meta-Learning for Fast Policy Adaptation"**.
The attacker will eventually gather enough information to build an accurate surrogate model, and our goal is to slow down this process so the model's decision boundary remains protected for as long as possible. Ideally, we want the explanations to provide little more information than if they were absent.

%To formulate the training data for the surrogate model, there are generative models trained for generating artificial data from query samples **Kim et al., "Generative Adversarial Network-based Artificial Data Generation"**. Another useful technique for optimizing training data is active learning.
%It is applied to extensive studies **Wang et al., "Active Learning with Deep Neural Networks: A Survey"** for picking query samples and synthesize training data based upon the queries. 
% The concept of active learning as extensively explained in **Zhang et al., "Model-Based Meta-Learning for Fast Policy Adaptation"**.