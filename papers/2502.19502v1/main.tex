\documentclass[twoside]{article}

% \usepackage{aistats2025}
\usepackage[accepted]{aistats2025}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
% \usepackage{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}

\usepackage[round]{natbib}
\renewcommand{\bibname}{References}
\renewcommand{\bibsection}{\subsubsection*{\bibname}}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{url}
\usepackage{xurl}
\usepackage{hyperref}
\usepackage{subcaption}
\usepackage{multirow}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{booktabs}
\usepackage{makecell}
\usepackage{graphicx}

\usepackage{color,soul}
\usepackage{xr}
\externaldocument{aistats_2025_supp}


\newtheorem{theorem}{Theorem}
\newtheorem{corollary}{Corollary}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}


\def\X{\mathbf{X}}
\def\tX{\mathbf{\tilde{X}}}
\def\x{\mathbf{x}}
\def\tx{\mathbf{\tilde{x}}}
\def\y{\mathbf{y}}
\def\supp{\textrm{supp}}
\def\capture{\textrm{cap}}
\def\eadd{e^{\textrm{add}}}
\def\ebase{e^{\textrm{base}}}



% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\begin{document}

\twocolumn[

\aistatstitle{Models That Are Interpretable But Not Transparent}


\aistatsauthor{ Chudi Zhong  \And Panyu Chen \And  Cynthia Rudin }

\aistatsaddress{Duke University \\UNC-Chapel Hill \And  Duke University \And Duke University } ]

\begin{abstract}
Faithful explanations are essential for machine learning models in high-stakes applications. Inherently interpretable models are well-suited for these applications because they naturally provide faithful explanations by revealing their decision logic. However, model designers often need to keep these models proprietary to maintain their value. This creates a tension: we need models that are \textit{interpretable}â€”allowing human decision-makers to understand and justify predictions, 
%In this case, we ideally want models that are \textit{interpretable}, where the logic of every model prediction is made understandable to humans, 
but not \textit{transparent}, so that the model's decision boundary is not easily replicated by attackers. Shielding the model's decision boundary is particularly challenging alongside the requirement of completely faithful explanations, since such explanations reveal the true logic of the model for an entire subspace around each query point. This work provides an approach, \textit{FaithfulDefense}, that creates model explanations for logical models that are completely faithful, yet reveal as little as possible about the decision boundary. FaithfulDefense is based on a maximum set cover formulation, and we provide multiple formulations for it, taking advantage of submodularity.
\end{abstract}


\section{Introduction}
Many organizations rely on machine learning (ML) models and their employees' livelihoods rely on those models being proprietary; the competitive value of these models depends on their secrecy. At the same time, these companies would like to provide explanations for each prediction. Explanations are important for many reasons, including accountability, troubleshooting of the inputs, sanity checking, and allowing the individual subject to the scores to understand the decision and refute them \citep{rudin2019stop}. Explanations are also required by law for high stakes decisions in the European Union \citep{EuropeanParliament2016a, goodman2017european} and in the United States, via the Equal Credit Opportunity Act (ECOA). According to Rohit Chopra, the Director of the Consumer Financial Protection Bureau in the United States, ``Companies are not absolved of their legal responsibilities when they let a black-box model make lending decisions''...``The law gives every applicant the right to a specific explanation if their application for credit was denied, and that right is not diminished simply because a company uses a complex algorithm that it doesn't understand.'' \citep{cfpb_blackbox_2022}.
That is, to satisfy legal requirements, explanations must be completely \textit{faithful}, meaning that the explanations must reveal the reasoning process that is actually used in the predictive model to make the prediction. \textit{Inherently interpretable} models provide faithful explanations, but such explanations could release substantial information about the model; lenders would not use a model if it were easily reverse-engineered.

How might an organization balance between the two goals of keeping models secret and revealing information within explanations? 
This tension between keeping models interpretable and proprietary is embodied by the \textit{model extraction attack} \citep{tramer2016stealing}. The attacker queries an ML model to obtain predictions. The attacker may not know the exact model type or the true data distribution used to train the model. After enough queries, the attacker can use the collected query-label pairs to train a surrogate model that achieves accuracy very close to that of the confidential model. It is hard enough to prevent information leakage from these attacks by revealing only predictions, but the need to reveal explanations could make these attacks much more powerful. 

In this paper, we provide a model form 
that addresses this tension. We propose a novel explanation generation method for logical machine learning models, \textit{FaithfulDefense}, that ensures the released explanations are always faithful and yet does not disclose the true decision boundary used by the underlying model. Our approaches also allow organizations to customize the level of detail revealed in explanations. FaithfulDefense is derived as a maximum set cover formulation. If its solution does not use the full ``length'' budget allocated to it by the user, extra terms can be appended to preserve the disclosure of information about the model while still maintaining faithful explanations. Since maximum set cover is submodular, we also produce a greedy solution that also works well in practice. An empirical evaluation with 3 different attacker strategies and 6 different explanation methods demonstrates that our proposed method is effective in safeguarding the underlying model. An implementation of the algorithm is available at: \url{https://github.com/chudizhong/FaithfulDefense}.

% Machine learning has increasingly been deployed across various businesses to enhance efficiency in data analysis and accuracy in prediction. To make machine learning tools and models accessible to more users, many cloud service providers offer Machine Learning as a Service (MLaaS), providing customers with cost-effective access to machine learning tools. They also allow model owners to charge others for queries to their commercially valuable models. 

% With the development of machine learning models, the demand for model interpretability has also been increasing. Interpretability, which offers human understandable reasons for each prediction, is crucial for troubleshooting models and promoting responsible decision-making. While many explainable artificial intelligence tools such as LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2020local} have been developed and deployed by MLaaS, these tools can not provide \textit{faithful} explanations of the underlying model. Inherently interpretable models, on the other hand, can always provide faithful explanations, though more efforts are required to find these models \citep{rudin2019stop}. 

% The monetization of MLaaS and the increasing query access needs exemplify the tension between accessing queries and preserving the confidentiality of the model. This tension is known as the model extraction attack \citep{tramer2016stealing}. The attacker can query an ML model through an API to obtain predictions. The attacker may not know the exact model type or the true data distribution used to train the model. After enough queries, the attacker can use the collected query-label pairs to train a surrogate model that achieves accuracy very close to the confidential model.

% In this paper, we consider the model extraction attack on interpretable models. Although the model is proprietary and only supports query access, it is inherently interpretable. A faithful explanation is provided when the query receives a prediction of an inferior label class. For instance, in a loan application scenario, an explanation is required if a denial decision is made. We aim to develop a method to mitigate the extraction attack while providing faithful explanations.


\section{Related Work}

\textbf{Interpretability}: %Interpretability in machine learning means the reasoning behind the model prediction is understandable to humans. It's crucial for high-stakes decision-making problems such as medical diagnosis, criminal justice, and loan decisions. 
Understanding how predictions are made allows humans to identify and rectify potentially serious problems \citep{ashoori2019ai, brundage2020trustworthy, LoPiano20, RudinWa14, Thiebes20}.
%Two notions have emerged to make models understandable: (1) developing inherently interpretable models to replace black boxes; (2) providing post-hoc explanations for black-box models.
While there is an abundance of work in 
explainable artificial intelligence (XAI), through providing simpler approximations to black-box models \citep{bastani2017interpretability, lakkaraju2019faithful} or local approximations to black boxes \citep{lundberg2020local, lundberg2017unified, ribeiro2016should, simonyan2014deep, sundararajan2017axiomatic}, such techniques are unsuitable for high-stakes decisions because their explanations are often unfaithful, incomplete or misleading \citep{lakkaraju2020fool, LaugelEtAl19, rudin2019stop, adebayo2018sanity}. Such unfaithful explanations can exacerbate problems with lack of trust. 
Interpretable machine learning, on the other hand, focuses on developing inherently interpretable models. These models reflect exactly how they make decisions, and the reasoning is always faithful. Logical models such as decision sets and decision trees are important types of interpretable models that have existed since the beginning of artificial intelligence.  
Numerous algorithms have been developed for optimizing them \citep{wang2017bayesian, aglin2020learning, angelino2018learning, DashEtAl18, demirovic2022murtree, lin2020generalized, rudin2023globally}, and modern versions of these algorithms can find sparse models with accuracy comparable to that of black box counterparts. These types of models have a long precedent in high-stakes decisions because they provide logical rules that faithfully describe, for instance, why someone's loan was denied.

% \textcolor{red}{Despite the guarantee of faithfulness, existing explanation methods in contexts such as loan applications have limited consideration for the amount of information disclosed about the underlying model. This lack of protection puts the model at risk, as an attacker could potentially reconstruct it, with model extraction techniques, from a series of queries with fabricated input features such as region, age, and income level.}

\textbf{Model extraction}: Model extraction means acquiring information from an unknown target model that goes beyond simple outputs to a set of input queries.
%the information that is beyond query outputs which result from input API. %For a machine learning model as the target, the information extracted involves hyperparameters \citep{wang2018stealing,truong2021data}, model parameters \citep{lowd2005adversarial,tramer2016stealing}, and the network structure assuming that the model is a neural network \citep{oh2019towards,hua2018reverse,hu2020deepsniffer,yan2020cache,hong2018security,xiang2020open,zhu2021hermes}. 
An \textit{attacker} who extracts information from the model might aim to train its own surrogate model, having performance no worse than that of the target model \citep{tramer2016stealing,orekondy2019knockoff,shi2017steal,correia2018copycat,chandrasekaran2020exploring,shi2018active,teitelman2020stealing}. 
% The surrogate model can alternatively be made to act in the same way as the target model, making the same predictions regardless of its correctness.

% The "side-channel attack" is made for exploiting information from software and hardware for model extraction. It includes operations like forcing a cache reloading and checking whether the model accesses the cache  \citep{tromer2010efficient,yarom2014flush+}, or inferring the reading/writing operations and execution time of a model from hardware "electromagnetic emanations" (reaction time, power consumption, etc.) \citep{hu2020deepsniffer,yoshida2019model,breier2021sniff,regazzoni2020machine,batina2019csi,jap2020practical,yu2020deepem}. 

% Apart from the side-channel information leakage, 
%This paper relates to the "query-based" attack which utilizes a series of query outputs to compute model parameters if some information of the model class is known \citep{tramer2016stealing, reith2019efficiently}. %In particular, based upon the understood mathematics behind models (binary classification, logistic regression, multi-layer perceptron, support vector regression, etc.), the attacker solves the system of equations formulated by input-output queries and derives exact or approximate model functions  \citep{tramer2016stealing,reith2019efficiently}. For more complicated models (e.g. neural networks), attackers can still infer from query samples the weight parameters, depth of architecture, of the target \citep{jagielski2020high,milli2019model,rolnick2020reverse,carlini2020cryptanalytic}.

%Without exploiting the mathematic mechanisms behind the extracted models, attackers can use query outputs to train its surrogate model against various targets (CNN \citep{truong2021data,sanyal2022towards}, encoders \citep{liu2022stolenencoder,sha2023can,dziedzic2022difficulty}, GAN \citep{szyller2021good,hu2021stealing}, graph neural networks \citep{wu2022model,he2021stealing,shen2022model,defazio2019adversarial}, LSTM \citep{takemura2020model}, BERT-based models \citep{he2021model}, etc.). The attacker seeks a similar model prediction effectiveness and it's not strictly required that the target and surrogate model should coincide in architecture or weights. Nevertheless there are studies of designing the surrogate model architecture \citep{orekondy2019knockoff,shi2017steal,krishna2019thieves,juuti2019prada,takemura2020model} and the way of querying samples as training data for the sake of better surrogate model performance. 

The notion of providing an explanation clashes with the model extraction paradigm. If an explanation is required with each prediction, the attackers job is potentially much easier since the explanation could reveal the predictions of an entire portion of the input space. 
Recent studies delve into the role of explanations in model extraction attacks \citep{yan2022towards, yan2023explanation, wang2022dualcf, ezzeddine2024knowledge, miura2024megex, oksuz2023autolycus, nguyen2023xrand}. Most of these works consider explanations derived from XAI algorithms like LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2020local} or counterfactual explanations. As discussed, posthoc explanations are not faithful. They are also generally incomplete, in that they reveal possibly a few variables that might be important to the prediction, but do not reveal how these variables are used to form the prediction. Explanations from inherently interpretable models would be far more valuable to an attacker because they explain the full reasoning process that led to a decision. Given that high-stakes domains like finance generally require inherently interpretable models, and should require complete and faithful explanations, we should be far more concerned by the prospect of attackers in the setting of inherently interpretable models. 

The literature, however, focuses on the protection of individuals whose scores are computed by the model, while overlooking the need to protect the companies that develop and use these models. This gap in protection can have serious consequences: companies may avoid using inherently interpretable models out of concern for the increased risk of model extraction attacks. If companies cannot secure their proprietary interpretable models, they may continue to rely on black-box models with unfaithful explanations. 

Hence, we want models that are inherently \textit{interpretable}  -- with completely faithful explanations -- but not \textit{transparent} -- we do not want attackers to approximate or see the full model with only a few queries.

%However, our problem setting diverges significantly from these prior works. Here, our underlying model is inherently interpretable, and we have to ensure that the explanations precisely reflect how the model makes predictions.

%\textbf{Model Extraction and Active Learning}:
In order to gather the most valuable data to build a surrogate model, attackers can use generative models for generating artificial data for querying the target model \citep{mosafi2019stealing,kariyappa2021maze,shi2018generative,yuan2022attack,truong2021data,sanyal2022towards}, or active learning \citep{tramer2016stealing,chandrasekaran2020exploring,pal2019framework,pal2020activethief,pengcheng2018query,shi2018active,reith2019efficiently,wang2022enhance,xie2022game}.
The attacker will eventually gather enough information to build an accurate surrogate model, and our goal is to slow down this process so the model's decision boundary remains protected for as long as possible. Ideally, we want the explanations to provide little more information than if they were absent.

%To formulate the training data for the surrogate model, there are generative models trained for generating artificial data from query samples \citep{mosafi2019stealing,kariyappa2021maze,shi2018generative,yuan2022attack,truong2021data,sanyal2022towards}. Another useful technique for optimizing training data is active learning.
%It is applied to extensive studies \citep{tramer2016stealing,chandrasekaran2020exploring,pal2019framework,pal2020activethief,pengcheng2018query,shi2018active,reith2019efficiently,wang2022enhance,xie2022game} for picking query samples and synthesize training data based upon the queries. 
% The concept of active learning as extensively explained in \citep{settles2009active}.




\section{Problem Setting}
Let $\mathcal{D} = \{(\x_i,y_i)\}_{i=1}^n$ be the dataset, where $\x_i \in \mathbb{R}^p$ are features and $y_i \in \{0, 1\}$ denotes binary labels. 
% In machine learning, we train a model $f$ that minimizes the empirical risk of $\mathcal{D}$. The trained model can be deployed to assist in real-world decision-making, although the model is not necessarily publicly available. For high-stakes decision-making problems such as loan approvals and insurance claims, achieving high accuracy alone is not sufficient. The model should also be understandable to humans, for critical purposes of error-checking and troubleshooting, as well as for justifying the decision. 
This reflects high-stakes decision-making in, for instance, loan and insurance approvals. A model $f$ can be trained to learn relations between features $\{\x_i\}$ and outcomes $\{y_i\}$. We consider logical models that use ``if-else'' conditions, specifically decision sets. Decision sets encompass other logical models, including decision trees and decision lists \citep{rudin2022interpretable}. %, random forests, and boosted decision trees. 
Sparse decision sets are inherently interpretable and are particularly easy to understand and troubleshoot, but are also particularly difficult to protect from attackers when the true model logic is employed in explanations. Sparse generalized additive models with piecewise constant shape functions -- which are currently the most common models for financial risk scoring -- 
can be converted efficiently into decision sets, as described in Appendix \ref{ref:app:gams}. 

%This ensures easier troubleshooting and modified \citep{rudin2019stop, rudin2022interpretable}, and it also allows for providing explanations to customers when needed. 
%For example, in loan decisions, an explanation should be provided if a customer is denied a loan. 
% However, training an inherently interpre model for real datasets is often NP-hard \citep{rudin2022interpretable} and demands substantial effort.
% Consider a scenario where a customer is applying for a loan. The loan officer at the bank uses model $f$ to predict whether the applicant would default on a loan. If the prediction is negative, the applicant can get the loan. Otherwise, if the prediction is positive, the applicant cannot obtain the loan, but an explanation must be provided as to why they were denied. 
%This can assist individuals in repairing their credit, and it is mandated by law \citep{EuropeanParliament2016a, goodman2017european}. Since $f$ is inherently interpretable, the true logic used by $f$ can be directly employed within explanations.


\begin{algorithm}
\caption{ModelExtraction($max\_q$)}\label{alg:steal_model}
    \begin{algorithmic}[1]
        \Require a maximum number of queries $max\_q$
        \State $Q \leftarrow \{\}, \textit{label} \leftarrow \{\}, E \leftarrow \{\}$ 
        % \Comment{set of queries $Q$, predicted labels $\textit{label}$, explanations $E$}
        \For{$t \in \{1,...,max\_q\}$}
        \State $q \leftarrow$ AttackerGenerateQuery($Q, label, E$) 
        % \Comment{Generate query, send labels set and explanations set}
        \State $qhat, e\leftarrow$ DefenderStrategy($q$) 
        % \Comment{Defender provides prediction and explanation}
        \State $Q$.add($q$), $label$.add($qhat$), $E$.add($e$)
        
        % \Comment{include new query, label and explanation in attacker's dataset}
        \EndFor
        \State Train a surrogate model $f'$ using $Q$, $label$ and $E$. 
        % \State \textbf{Return} $f'$
    \end{algorithmic}
\end{algorithm}

An ML model extraction attack occurs when the attacker has query access to the target model $f$ and attempts to learn a surrogate model $f'$ that closely approximates or matches the performance of $f$ by sequentially asking queries. 
%The attacker asks queries sequentially to obtain labels and then utilizes the collected query-label set to train a surrogate model $f'$. 
Conversely, the defender must provide the label of the query and give an explanation if the query has a positive label (e.g., loan denied, insurance claim denied, job application denied). 
%Providing additional information like explanations could potentially benefit the attacker \citep{oksuz2023autolycus, yan2022towards}. 
The process is shown in Algorithm \ref{alg:steal_model}.
In this work, we stand on the position of the defender to address the challenge of how to provide meaningful and faithful explanations while preventing attackers from training an accurate surrogate model with a small number of queries. 




\section{Methodology}
In this section, we first introduce our defense method, detailing how to generate an explanation for a query predicted to be positive by the model $f$. We then illustrate how attackers leverage these explanations to launch more potent attacks.

\subsection{How a Defender Can Generate Explanations}\label{sec:defender}

We assume $f$ is trained on proprietary dataset $\mathcal{D}$ and belongs to an inherently interpretable model class, such as decision sets. A decision set, also known as a ``disjunction of conjunctions,'' ``disjunctive normal form'' (DNF), or an ``OR of ANDs'' is comprised of an unordered collection of rules, where each rule is a conjunction of conditions. A positive prediction is made if at least one of the rules is satisfied. Decision sets encompass decision trees and decision lists: any decision tree or decision list can be written as a decision set. A decision set is inherently interpretable and the satisfied rule can be directly used as a faithful explanation. However, generating explanations in this way discloses too much information. We now introduce a more frugal method to provide faithful explanations. 

Given a dataset $\mathcal{D} = \{\x_i, y_i\}_{i=1}^n$, we can turn it into a binary dataset by considering all categories for categorical features and binning continuous features using thresholds. Let $C=\{c_1, c_2, ..., c_m\}$ denote the full set of possible binary features. It can also be viewed as a collection of conditions. For example, a $c_j$ could be ``income $<$5K'', ``age $\leq 20$'', ``saving $<$ 5K'', etc. The binarized dataset is denoted as $\tX=\{\tx_i\}_{i=1}^n$ where $\tx_i \in \{0, 1\}^m$ is 
a binary vector of length $m$. %and $\tilde{y}_i = f(\x_i)$ is the prediction of $\x_i$ using model $f$.
Each rule in $f$ is a subset of $C$. 
Let $\beta$ be a set of conditions. We define $\capture(\beta, \tx_i) = 1$ if $\tx_i$ satisfies all conditions in $\beta$, and 0 otherwise. The collection of samples from the dataset captured by $\beta$ can be denoted as $S_{\beta}(\tX) = \{\tx_i: \capture(\beta, \tx_i) = 1\}$. 

% Let $\textrm{cap}(c_j)$ denotes the set of training samples covered by condition $c_j$, i.e., $$\textrm{cap}(c_j) = \{i:  x_{ij}=1 \}.$$

Let us define a faithful explanation for query $q$.
\begin{definition}
(Faithful explanation $e$ for query $q$): Let $q$ be a query such that its predicted label $f(q)$ is 1, where an explanation is required. Denote a rule in $f$ that $q$ satisfies as $f_r$, where $f_r\subseteq C$. Then, $e \subseteq C$ is a faithful explanation for $q$ if $f_r \subseteq e$.
\end{definition}

Intuitively, an explanation is faithful if it reveals part of the model. The explanation does not need to reveal \textit{all} rules in the model, or even an entire rule, to be faithful.
For instance, if a rule in the model is ``income $<$5K,'' then an explanation that ``income $<$5K and age $\leq 20$'' is faithful, and yet does not reveal that the entire ``income $<$5K'' subgroup has label 1. Providing an explanation that reveals only part of the model's original rule, rather than the full rule (``income $<$5K and age $\leq 20$'' instead of ``income $<$5K'') is identical to the process of (1) creating an equivalent underlying model with the rule split in two (i.e., replacing ``income $<$5K'' in the model with the pair of rules  ``income $<$5K and age $\leq 20$,'' ``income $<$5K and age $> 20$''), and then (2) providing a rule in this new model as the explanation (``income $<$5K and age $\leq 20$'').
% For example, if the underlying model has only one rule, i.e., ``region$=1$'', then we could have written it as (``region$=1$'' AND ``Age $<20$'') OR (``region$=1$'' AND ``Age $\geq 20$''). This is an equivalent model defined by 2 rules.  
% (``region$=1$'' OR ``credit$>10$''), then we could have written it as (``region$=1$'' AND ``age $<20$'') OR (``region$=1$'' AND ``age $\geq 20$'') OR (``credit$>10$''). This is an equivalent model defined by 3 rules. 
Thus, our definition of a faithful explanation takes into account this option for the modeler of creating an equivalent decision set before providing the explanation. Importantly, a faithful explanation does not need to be ``complete,'' meaning it need only reveal one reason for the decision, not all possible reasons. E.g., both ``income $<$5K'' and ``employed = no'' are true, but we need only give an explanation involving one of them. (The reason would come with a notification that there may be other reasons why the loan was denied, to prevent a misinterpretation that the explanation is complete.)   


%Let $q_t$ be the query asked at the $t^{th}$ iteration, and $f(q_t)$ be the predicted label of query $t$. If $f(q_t)=1$, an explanation is required. Let $e_t \subseteq C$ be the explanation for $q_t$.  
We define $|e|$ as the size of the explanation, i.e., the number of conditions in the explanation. 
Let $\supp(e, \tX)$ denote the number of samples captured by $e$, i.e., 
$$\supp(e, \tX) = \sum_{i=1}^n \capture(e, \tx_i) = |S_{e}(\tX)|.$$
$\supp(e, \tX)$ can be used to estimate the released information. There is a tradeoff between $\supp(e, \tX)$ and $|e|$. Typically, $\supp(e, \tX)$ monotonically decreases as $|e|$ increases, e.g., the size 2 rule ``income $<$5K and age$\leq$ 20'' has less support than the size 1 rule ``income $<$5K.'' 

Our goal is to find a simple explanation $e$, meaning the explanation can be described succinctly, i.e., $|e|$ is no larger than a max length $l$, and yet captures few positively predicted samples in the training set. Therefore, the optimization problem can be written in the following form: 
\begin{equation}\label{eq:prob}
\begin{aligned}
    \min_{\textrm{faithful explanation  } e}   \supp(e, \tX) \ 
    \textrm{s.t.} \ \  |e| \leq l.
\end{aligned}
\end{equation}
Since $f$ is a decision set, we can slightly modify Problem \eqref{eq:prob}. 
Let $e$ consist of two parts, i.e., $e = \{\ebase, \eadd\}$, where $\ebase = f_r$ is a set of conditions used by one of the rules in $f$ that $q$ satisfies, and $\eadd$ is the set of additional conditions satisfied by $q$ other than the conditions used by the satisfied rule, denoted as $C_{q} \subseteq C\backslash \ebase$. E.g., if $q$ obeys ``income $<10k$,'' and income is not one of the conditions in $\ebase$, we may choose to add it to $\eadd$ to narrow the explanation. Since $\ebase$ is fixed, we only need to optimize $\eadd$. Then, the modified optimization problem is 
\begin{equation}\label{eq:prob2}
    \min_{e^{\textrm{add}} \subseteq C_{q}} \ \ \supp(\{e^{\textrm{base}}, e^{\textrm{add}}\}, \tX) \quad \textrm{s.t.} \ \ |e^{\textrm{add}}| \leq l.
\end{equation}

\begin{theorem}
    %Let $\x$ be all samples, $C = \{c_1, ..., c_m\}$ be the set of conditions, $S_j = \{\x_i: \capture(c_j, \x_i)\}$ be the subset of samples that captured by condition $j$. 
    Let $q$ be the query with $f(q)=1$, $e^{\textrm{base}}$ be the set of conditions used by the rule in $f$ that $q$ satisfies, and $C_{q} \subseteq C \backslash e^{\textrm{base}}$ be the additional conditions satisfied by $q$. Problem \ref{eq:prob2} of selecting a subset  $e^{\textrm{add}} \subseteq C_{q}$ such that the intersection of the selected set of samples covers the minimum number of elements in $\tX$ is equivalent to the maximum coverage problem: selecting a subset $e^{\textrm{add}} \subseteq C_{q}$ such that the union of selected samples cover the maximum of the complement set $\tX^c$.
\end{theorem}


This theorem indicates that Problem \ref{eq:prob2} is the same as the maximum coverage problem, and we need only optimize $e^{\textrm{add}}$, the set of additional conditions beyond the base model to include within the explanation, though this is also intuitive.

% We can also add weight to each sample. If a sample is already covered by a previous explanation, then there is no extra cost to cover this sample again by another explanation. Therefore, the weight of this sample could be 0. This can be easily added to our formulation as a weighted maximum coverage problem. 


% In the maximum coverage problem (minimum set cover problem), given a number $k$, and a collection of sets $A=\{A_1, A_2, ..., A_n\}$, the goal is to find a subset $A' \subseteq A$ of sets, such that $|A'| \leq k$, and the number of covered elements $|\bigcup_{A_i \in A'}A_i|$ is maximized. 

%Let $C=\{C_1, C_2, ..., C_m\}$ be the collection of conditions in our case. Conditions could be ``age $\leq 20$'', ``saving $\leq$ 5k'', etc. $S_i$ for query $q_i$ is a subset of conditions, i.e., $S_i \subseteq C$. 

The maximum coverage problem is NP-hard \citep{karp2010reducibility} but a submodular problem \citep{nemhauser1978analysis}, and we use three methods % can use either a greedy algorithm or a mixed-integer-programming (MIP) formulation 
to find $e$ for each query $q$. 

\textbf{Method I: FaithfulDefense Greedy}. We choose the condition $c_j$ in each step such that $S_{\neg c_j}(\tX_{\textrm{base}})$ is maximized. The method approximates the optimal solution in a factor of $1-\frac{1}{e}$ \citep{nemhauser1978analysis}. 
If the greedy solution does not use all the length budget, i.e., $|\eadd| < l$, we randomly append extra conditions to use the full budget. 

\textbf{Method II: FaithfulDefense IP}. We can find an optimal solution for Problem \eqref{eq:prob2} by solving an integer programming (IP) problem. The IP formulation is shown below. Let $\mathbf{u} \in \{0,1\}^{n}, \mathbf{v} \in \{0,1\}^m$ be the binary vectors. Despite IP being NP-hard, the problem is usually solved in seconds by a solver.
\begin{eqnarray}
    \max_{\mathbf{u}, \mathbf{v}} && \sum_{\{i: \tx_i \textrm{not covered}\}}^n u_i \label{eq:mip_obj}\\
    s.t. && \sum_{j=1}^m v_j \leq l, \quad  \sum_{j:x_{ij} = 1} v_j \geq u_i \\
    && u_i \in \{0,1\}\  \forall i\in\{1,...,n\} \label{eq:mip_u}\\
    && v_j \in \{0,1\} \ \forall j\in \{1,...,m\}\label{eq:mip_v}
\end{eqnarray}

Equation \ref{eq:mip_obj} aims to maximize the sum of the covered samples. Equation \ref{eq:mip_u}-\ref{eq:mip_v} indicate that if $u_i=1$, $\tx_i$ is covered by at least one of the $\neg c_j$'s and if $v_j=1$, $\neg c_j$ is selected), respectively. 

\textbf{Method III: FaithfulDefense IP-RA}. If the IP solution does not use the full length budget, we randomly append extra conditions to use the entire budget. 

% Both greedy and MIP solutions may append less than $l$ conditions. We can also randomly append more conditions to hit the upper bound $l$.

Algorithm \ref{alg:safeguard} shows how the defender safeguards the model while providing a faithful explanation. 

\begin{algorithm}[h]
\caption{FaithfulDefense($q$)}\label{alg:safeguard}
    \begin{algorithmic}[1]
        \Require Base model $f$, dataset $\tX$, conditions $C$, query $q$, history of explanations $E$
        \If {$f(q) = 0$} 
        \State return $f(q), \emptyset$ \Comment{No need for explanation.}
        \Else
        \If {$q$ can be explained by an  $e\in E$}
        \State $e \leftarrow$ FindFromHistory($E, q$) \Comment{Return only the explanation shown previously.}
        \Else 
        \State $e \leftarrow \textrm{GenerateExplanation}(q, f, \tX, C)$. \Comment{Solve Problem \ref{eq:prob2} by IP, IP-RA or greedy method.}
        \EndIf
        \State return $f(q), e$ \Comment{Returns low support and small size explanations}
        \EndIf
    \end{algorithmic}
\end{algorithm}

% Proposition below shows it's NP-hard for the attacker to reconstruct rules in the underlying model with the proposed defense method. 

% \begin{proposition}
%     Let $f$ be the underlying decision set. Let $Q$ be the collection of queries asked by the attacker, $label$ be the collected predicted labels for the queries, and $E$ be the collected explanations for the positively predicted queries. Reconstructing the exact set of rules in $f$ from $Q, label, E$ is NP-hard.  
% \end{proposition}

\subsection{How an Attacker Can Use Explanations}\label{sec:perturbation}
Given explanations in addition to query labels, attackers will use these explanations to generate more informative queries. A straightforward attacker strategy is generating an unasked query that lies beyond the boundaries marked by past explanations. This strategy does not fully take advantage of these explanations. An alternative strategy is to explore a high-density area close to the decision boundary. Let us explain it in detail. 

Given a query $q$ and its explanation $e$, the attacker can generate candidate queries by adjusting one of the feature values close to and outside the boundary marked by $e$. For example, if $e$ involves $k$ features, the attacker can generate at least $k$ candidate queries. However, this could lead to too many candidate queries if $|e|$ is large. Given the limited number of queries the attacker can ask, it would be reasonable to only consider candidate queries that explore regions determined by more important features. So the next question is how to determine which features are more important. The attacker can track the number of times each feature appears in past explanations $E$. Features with higher counts can be viewed as more important than others. Then the attacker can select the top $k$ features in $e$ and generate new queries that differ from $q$ by adjusting the value of one of the selected features. If a feature $j$ used in $e$ appears most often in past explanations, the new value assigned to feature $j$ is positioned close to, yet beyond, the boundary provided by $e$. For categorical features, $q_{j}^{new}$ is set to $q_{j} \pm 1$ to transition to another category. For continuous features, $q_{j}^{new}$ is adjusted by $\text{boundary}(e, j) \pm \delta$. For instance, if the explanation for feature $j$ is ``$j \leq \$ 5000$,'' then the new value for $j$ could be set to $5001$. In cases where feature $j$ is bounded by $e$ on both sides, the attacker can generate two candidate queries. All other features retain the same values as $q$. Thus, a new candidate query is formulated as $q^{new} = [q_{1}, ..., q_{j-1}, q_{j}^{new}, q_{j+1}, ...q_{p}]$. The attacker then adds all new candidate queries into a query pool and rearranges them based on the importance of the perturbed feature.

This perturbation-based query generation method is inspired by \citep{oksuz2023autolycus}, which adopts the perspective of attackers upon receiving LIME explanations \citep{ribeiro2016should}. In their setup, a LIME explanation is provided for each query, reporting weights for each feature. They then slightly adjust the value of one of the features with a high weight as a candidate query. However, our explanations differ from LIME in three ways: (1) our explanations are always faithful, following the rules used to make predictions, while LIME explanations are local approximations of the underlying model; (2) our explanations do not assign weights to features since features are not weighted in the ground truth model, so the explanation itself cannot be used to rank features; and (3) our explanations are provided only for queries predicted to be positive, while in \citep{oksuz2023autolycus}, a LIME explanation is provided for each query. 
%Therefore, we introduce a perturbation-based method to generate queries using our explanations. 


Meanwhile, since our explanations are faithful, they can also be used as part of the attacker's surrogate model for prediction. For instance, upon the arrival of a new sample, the attacker initially verifies whether the sample aligns with any of the explanations. If such a match is found, a positive prediction is made directly. Otherwise, a prediction is generated based on the surrogate model $f'$, i.e., 
\begin{equation}\label{eq:pred}
    \hat{y}_i = \capture(E, \x_i) \vee f'(\x_i).
\end{equation}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=.7\textwidth]{figures/supp_3_10_0_test.jpg}
    \caption{Number of queries vs$.$ the proportion of positive samples covered by explanations. Lower curves are better. FaithfulDefense (red, orange, pink curves) captures fewer positive samples in test sets for all three datasets using three different querying strategies. LIME was only used for perturbation queries since attackers perturb the LIME explanations to generate the next query; LIME explanations are only applicable to the attacker's perturbation-based querying strategy. LIME explanations are incompatible with other attacker querying strategies. (max length $l=3$)}
    \label{fig:supp}
\end{figure*}

\section{Experiments}
In this section, we conduct experiments to show that (1) FaithfulDefense is efficient in that it reveals less information about the dataset than baselines (Section \ref{sec:info_leak}), (2) FaithfulDefense can provide explanations within seconds (Section \ref{sec:time}), (3) FaithfulDefense requires more queries by the attacker to achieve performance similar to $f$ on the test set (Section \ref{sec:test_perform}), and (4) FaithfulDefense always provides completely faithful explanations (Section \ref{sec:faithfulness}). 

Since loan decisions are a key situation where faithful explanations are required and where model owners keep models secret,  we use three credit datasets, the Fair Isaac (FICO) credit risk dataset \citep{competition} used for the Explainable ML Challenge and German credit dataset from UCI \citep{Dua:2019}. We use the merged training/testing sets of a loan approval prediction problem dataset from Kaggle \citep{kaggle}.  Each dataset is divided into train and test sets with an 80:20 split. Details are in Appendix \ref{app:setup}. We use fast sparse rule sets (FastSRS) \citep{LiFastSRS} on the training set to train the underlying interpretable model that we aim to protect. 
% \begin{table}[]
%     \centering
%     \begin{tabular}{|l|c|c|l|}\hline
%        Dataset & samples & features & description \\ \hline
%        FICO  &  10459 & 23 & whether someone would default on a loan\\\hline
%        German credit & 1000 & 20 & predict the approval of credit applications \\\hline
%        Kaggle loan prediction problem & 480 & 11 & predict the approval of loan applications\\\hline
       
%     \end{tabular}
%     \caption{Dataset summary}
%     \label{tab:dataset}
% \end{table}

Since, as discussed in the related work section, we know of no previous method that provides fully faithful explanations that is resilient to model extraction attacks, we compare FaithfulDefense with three natural baselines:  (1) returning the rule matched by the query in model $f$ (2) randomly appending $l$ conditions, (3) providing no explanation. We also use LIME explanations \citep{ribeiro2016should} as another baseline. 
We were able to coerce LIME into comparison because its explanations share a format (e.g., if x1>5 and x2<3, predict yes) similar to ours. Nevertheless, it's important to notice that LIME does not faithfully provide the reasoning process of the underlying model. 

We cannot include Shapley value-based explanation methods as a baseline in our comparison because they 
return only feature importances: Shapley values provide \textit{no} explanation of the underlying model's calculations, only what variables are estimated to be important in those calculations; they provide \textit{no} explanations about any observation other than the one being considered (e.g., ``the loan was denied because some of your features are more important than others''). To be faithful, the user would need to know \textit{how} the variables are combined, not just how important they might be.
%Some works make Shapley values differentially private by adding noise to the explanations, which makes the explanations even less faithful \cite{??}.
%you get something like ``x1's importance is 8 and x2's importance is -6.'' There's no way to compare with that since there's not a complete explanation to assess faithfulness. 
%and such explanations cannot be used to make predictions or assess faithfulness.


We assume the attacker knows the marginal distribution of each feature but not the joint distribution. 
There are three query generation strategies that are reasonable for the attacker: (1)\label{random_query} randomly generate an unasked query outside the boundaries marked by past explanations, (2) use an agnostic active learning algorithm called importance weighted active learning (iwal) \citep{beygelzimer2010agnostic, chandrasekaran2020exploring} to find an informative query that is outside the explanation boundaries, and (3) generate queries by adjusting the value of important features as described in Section \ref{sec:perturbation}. For each dataset, we allow the attacker to ask 2000 queries. Note that only perturbation-based queries are generated if LIME is used to provide explanations. We use the absolute value of coefficients in LIME explanations as the measure of variable importance. 




\subsection{How much information do the explanations leak?}\label{sec:info_leak}
% (solving the maximum coverage problem using either greedy or MIP method in Section \ref{sec:defender})

Figure \ref{fig:supp} demonstrates that \textit{FaithfulDefense} (red, orange, and pink curves) \textbf{reveals less information about the test set than the baseline defense strategies} (i.e., captures fewer positive samples) for all three datasets using three different querying strategies, as the three curves consistently fall below the green and blue curves. This figure also indicates that the perturbation-based querying method is usually more aggressive than the other two attacker methods. Training results are in Appendix \ref{app:more_results}. %The middle column in Figure \ref{fig:supp}a shows that the yellow curve surpasses the red curve, suggesting that the MIP solution discloses more samples than the greedy solution, even in the training set. This discrepancy arises because the MIP solver sometimes fails to find the optimal solution within the 180-second time limit. In such cases, it is preferable to use the greedy solution instead.



% \begin{wrapfigure}{r}{0.42\textwidth}
%   \begin{center}
%     \includegraphics[width=0.42\textwidth]{figures/fico_3_10_exp_time_0_perturb.png}
%   \end{center}
%   \caption{Time in seconds to generate explanations on the FICO dataset. 180-second time limit is set for the IP solver.\vspace*{-20pt}}
%   \label{fig:exp_time}
% \end{wrapfigure}

% Figure \ref{fig:exp_time} compares the time consumption for generating explanations on the FICO dataset. The time taken by our FaithfulDefense Greedy is very fast, usually less than 0.1 second when a perturbation-based querying strategy is used. In most cases, FaithfulDefense IP and FaithfulDefense IP-RA take longer than the greedy method but faster than LIME. Sometimes they take longer when a solver is used to find the optimal solution. In practice, if the IP method cannot easily find the optimal solution, it would be better to use FaithfulDefense Greedy instead. Returning base rules as explanations and using random explanations are instantaneous.  More results are shown in Appendix \ref{app:more_results}. 



\subsection{How much time does each method take to generate explanations?}\label{sec:time}
Figure \ref{fig:exp_time} shows the difference in time consumption between our FaithfulDefense methods and LIME for generating explanations.
\textbf{The time taken by our FaithfulDefense Greedy is generally very fast, usually less than 0.1 second.} In most cases, FaithfulDefense IP and FaithfulDefense IP-RA take longer than the greedy method but are faster than LIME.  Sometimes they can take longer because a solver is used to find the optimal solution. In practice, if the IP method cannot easily find the optimal solution, it would be better to use FaithfulDefense Greedy instead. Returning base rules as explanations and using random explanations are instantaneous. More time consumption results are in Appendix \ref{app:more_results}.


\begin{figure}[ht]
\centering
\includegraphics[width=0.49\textwidth]{figures/exp_time_3_10_0_perturb_new.jpg}
\caption{Time consumption of generating explanations when the attacker uses the perturbation strategy.(max length $l=3$).}
\label{fig:exp_time}
\end{figure}


% \begin{table}[]
% \centering
% \begin{tabular}
% {|m{2cm}|m{1cm}|m{1.1cm}|m{1.2cm}|m{1.2cm}|m{1.2cm}|m{1.2cm}|}
% \hline
%  & \multicolumn{6}{c|}{FICO}\\ \hline
% Attacker's strategy  &  Base rule & Random exp & Faithful Defense Greedy & Faithful Defense IP & Faithful Defense IP-RA & LIME \\ \hline
% Random query & $0.003 \pm 0.001$  & $0.005 \pm 0.0$  & $0.016 \pm 0.007$ & $2.859 \pm 16.914$  & $3.261 \pm 22.22$ & NA  \\ \hline
% Active learning query & $0.003 \pm 0.001$ & $0.004 \pm 0.0$               & $0.015 \pm 0.007$    & $2.883 \pm 17.961$  & $3.251 \pm 19.862$ & NA\\ \hline
% Perturbation query    & $0.004 \pm 0.001$ & $0.005 \pm 0.0$               & $0.012 \pm 0.008$      & $23.949 \pm 71.952$ & $27.921 \pm 76.736$ & $6.714 \pm 0.220$\\ \hline
% \end{tabular}
% \caption{Time in seconds to generate different explanations on the FICO dataset. 180 seconds time limit is set for the IP solver. }
% \label{tab:exp_time}
% \end{table}



\subsection{How well does the attacker's surrogate model perform?}\label{sec:test_perform}

The attacker can train a surrogate model using the collected query-label pairs. In real situations, the attacker may not know the exact model class and architecture of the underlying model. However, they may infer that the underlying model is a logical model based on the explanations provided. Therefore, we consider three different model classes (CART, \citealt{breiman1984classification}, Random Forest, \citealt{breiman2001random}, and Gradient Boosted Tree, \citealt{friedman2001greedy}) that the attacker might use to train a surrogate model $f'$. Since all explanations except LIME are faithful, they can also be used for prediction in addition to generating more informative queries. Given a test set, Eq \eqref{eq:pred} is used to make predictions. We compare the performance based on the similarity between predictions made by the surrogate model and the underlying model on the test set, i.e., $\frac{1}{n_{\textrm{test}}}\sum_{i=1}^{n_{\textrm{test}}} 1[f(\x^{\textrm{test}}_i) == \capture(E, \x^{\textrm{test}}_i) \vee f'(\x^{\textrm{test}}_i)]$. A new surrogate model is trained on the cumulative set once 50 more query-label pairs are obtained. This allows us to track how many queries are needed to achieve a similar performance as the underlying model.


Figure \ref{fig:cart_surrogate} compares the test performance among different explanation methods. CART with a depth of 5 is used to train the surrogate model. In this figure, a higher attacker's error indicates better defense. In other words, more queries are needed to achieve similar performance to the underlying model $f$. As observed, our method (red, orange, and pink curves) is usually above the green and blue curves on the FICO and German credit datasets even when 2000 query-label pairs are used, indicating that \textbf{our explanations are more effective in protecting the underlying model}. The phenomenon is less obvious in the loan prediction dataset. The results imply that our defense methods outperform the baselines more significantly in larger datasets. %which have underlying models harder to extract. 
Sometimes, we even observe overlap between the red, orange, and pink curves and the dark gray curve, suggesting that \textbf{providing explanations using FaithfulDefense is nearly equivalent to providing no explanations}. 

The bottom row in Figure \ref{fig:cart_surrogate} shows that sometimes when the attacker uses the perturbation-based querying strategy, FaithfulDefense is better than or similar to LIME in the 1000 queries and then becomes slightly worse. This is because explanations given by our FaithfulDefense are always faithful (see Table \ref{tab:fpr}) and can be used as part of the surrogate model for prediction but LIME explanations are permitted to be unfaithful. %FaithfulDefense sometimes performs even better than providing no explanations. Our hypothesis is that this strategy might constrain the attacker within a limited region of the data distribution, consequently preventing the development of a good surrogate model. 
More results are in Appendix \ref{app:more_results}.

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.8\linewidth]{figures/surrogate_test_cart_acc_3_10_compare_0.jpg}
    \caption{Comparison of test performance between base and surrogate models. CART is used by the attacker to train the surrogate model. (max length $l=3$).}
    \label{fig:cart_surrogate}
\end{figure*}

%Figure \ref{fig:rf_gbdt_surrogate} in Appendix \ref{app:more_results} shows the test performance when Random Forest and GBDT are used to train the surrogate model. We use the default setting for these two model configurations. Performance using random forest is similar to that of CART, where providing our explanations usually requires more queries comparing with the other two baselines and sometimes close to providing no explanations. GBDT is more powerful than CART and random forest. But note that Random Forest and GBDT are black box models. They cannot be used to replace the interpretable models to return faithful explanations. 



\subsection{How faithful the explanations are?}\label{sec:faithfulness}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/exp_fp_3_10_compare_0.jpg}
%     \caption{Number of explanations vs$.$ false positive rate on the test set when only explanations are used to make predictions. All explanation methods except LIME have completely faithful explanations, so their curves simply overlap the horizontal axis at false positive rate = 0.
%     } 
%     \label{fig:fpr}
% \end{figure}


We use the false positive rate (FPR) as a measure of the faithfulness of our explanations because an explanation is considered faithful when it aligns with the (partial) true model. In our framework, explanations are provided specifically when the true model predicts a positive outcome. Consequently, for samples that match the explanation, we expect them to consistently receive a positive prediction. A non-zero FPR would indicate a misalignment between the explanation and the true model. 
Table \ref{tab:fpr} shows the false positive rate on the test set when explanations themselves are used to make predictions. \textbf{All explanation methods except LIME have completely faithful explanations} as their FPR are always 0, while LIME explanations are not faithful given FPR greater than 0. %As we can see, the false positive rate of LIME explanations is not always 0, indicating that LIME explanations are not faithful. All other methods are faithful, as their false positive rates are always 0. %True positive rates increase as more explanations are used. We can also see that providing base rules and random explanations can reach a true positive rate of 1 more quickly than our FaithfulDefense, since we aim to minimize the support coverage in our formulation.

\begin{table}[h]
\caption{False positive rate (FPR) on the test set when only explanations are used to make predictions. All explanation methods except LIME have completely faithful explanations, so their FPRs are zero.}
\centering
% \resizebox{14cm}{2.25cm}{
\begin{tabular}{|m{3.6cm}|m{0.95cm}|m{1.1cm}|m{0.85cm}|}
  \hline
  \textbf{FPR} 
   & FICO & German Credit & Loan \\\hline
  Base rule & 0 & 0 & 0 \\\hline
  Random exp & 0 & 0 & 0 \\\hline
  FaithfulDefense IP & 0 & 0 & 0 \\\hline
  FaithfulDefense IP\_RA & 0 & 0 & 0 \\\hline
  FaithfulDefense Greedy & 0 & 0 & 0 \\\hline
  LIME & 19.78\% & 15.71\% & 3.33\%\\\hline
\end{tabular}
\label{tab:fpr}
\end{table}

% \textbf{Underlying model and example faithful explanations}. Here is an example of the underlying model fitted by FastSRS on the FICO dataset: 

% \begin{figure*}
%     \centering
%     \includegraphics[width=0.8\textwidth]{figures/fico_base_rules.png}
%     %\caption{Underlying decision set on the FICO dataset}
%     %\label{fig:fico_base_rules}
% \end{figure*}

% The query in Figure \ref{fig:example_exp} (left) satisfies rule \#1, \#2, and \#6 in $f$. FaithfulDefense returns an explanation (right) that satisfies rule \#6. 
% % Figure \ref{fig:example_exp} shows an example query-explanation procedure. The attacker forwards a query with feature values pertain to information of a loan applicant. An explanation is provided to explain why the application is approved. 
% % Notice that the attacker always gives a query beyond the coverage of explanations in previous query-explanation procedures. The explanation never violates the rules in the underlying model, for the preservation of complete faithfulness. It also correctly explains 
% \begin{figure*}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/example_exp.png}
%     \caption{Example query and the explanation in response. The explanation aligns with a rule in the underlying model to preserve complete faithfulness}
%     \label{fig:example_exp}
% \end{figure*}



\section{Discussion}
Our goal is to provide models that are \textit{interpretable}, meaning that they have completely faithful explanations and could be used in high-stakes situations, but not \textit{transparent}, meaning that they provide some level of protection for the model's formula, which enables organizations to protect their efforts and intellectual property. There is no perfect protection from a large attack, in the sense that even without the requirement of explanations, an attacker could create a surrogate model based on queries alone, given enough of them. Explanations make this problem worse in that, unless they are extremely specific to the query (``people with exactly your credit history all had their loan applications denied'') they tend to reveal whole subspaces. In this work, we aimed to strike a balance between providing faithful explanations and protecting models.  

Our approach can be directly used in practice for an immense number of applications, and would be valuable to those creating models for credit scoring, car insurance, health insurance, advertising or news agencies that may reject ads or articles based on content restrictions, social media companies that can reject posts, employment, rental housing applications, and utilities (water/natural gas/electricity). 

Our faithful explanations can benefit users in several ways: they can verify their data to ensure the decision is accurate, appeal the decision if the model's logic appears to be faulty, consider legal action against the decision-maker, make improvements to their situation, or reach out to decision-makers for further information.

We believe this work may open an interesting exchange within the research community about how much information should be shared with end-users regarding the explanation behind their decisions from the lens of protecting intellectual property.

% Though we focus on logic models in this paper, FaithfulDefense can be extended to other model classes such as sparse generalized additive models (GAMs). A sparse GAM consists of shape functions where each shape function is based on a single feature. The model is inherently interpretable since humans can understand how each feature contributes to the prediction by visualizing each shape function in a 2D plot. In recent literature, the shape function in sparse GAMs is a step function \cite{}. Similar to FaithfulDefense on logical models, we can provide limited but faithful information about the bins that the query falls into in sparse GAMs without disclosing the entire model. 


%Limitations: this work could be extended in many ways, including to classes of non-logical models. Negative societal impacts: our attacker strategies could be used directly by attackers.


% \newpage
\bibliographystyle{plainnat} 
\bibliography{biblio}




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Checklist}


% %%% BEGIN INSTRUCTIONS %%%
% The checklist follows the references. For each question, choose your answer from the three possible options: Yes, No, Not Applicable.  You are encouraged to include a justification to your answer, either by referencing the appropriate section of your paper or providing a brief inline description (1-2 sentences). 
% Please do not modify the questions.  Note that the Checklist section does not count towards the page limit. Not including the checklist in the first submission won't result in desk rejection, although in such case we will ask you to upload it during the author response period and include it in camera ready (if accepted).

% \textbf{In your paper, please delete this instructions block and only keep the Checklist section heading above along with the questions/answers below.}
% %%% END INSTRUCTIONS %%%


 \begin{enumerate}


 \item For all models and algorithms presented, check if you include:
 \begin{enumerate}
   \item A clear description of the mathematical setting, assumptions, algorithm, and/or model. [Yes] Please see Sections 4 and 5.
   \item An analysis of the properties and complexity (time, space, sample size) of any algorithm. [Yes] Please see Section 4.
   \item (Optional) Anonymized source code, with specification of all dependencies, including external libraries. [Yes] We will provide the code in the supplement. 
 \end{enumerate}


 \item For any theoretical claim, check if you include:
 \begin{enumerate}
   \item Statements of the full set of assumptions of all theoretical results. [Yes] Please see Section 4. 
   \item Complete proofs of all theoretical results. [Yes] Please see Appendix. 
   \item Clear explanations of any assumptions. [Yes]     
 \end{enumerate}


 \item For all figures and tables that present empirical results, check if you include:
 \begin{enumerate}
   \item The code, data, and instructions needed to reproduce the main experimental results (either in the supplemental material or as a URL). [Yes] A summary of datasets and experimental setup are discussed in the Appendix. We will provide the code in the supplement.
   \item All the training details (e.g., data splits, hyperparameters, how they were chosen). [Yes] Please see Appendix. 
         \item A clear definition of the specific measure or statistics and error bars (e.g., with respect to the random seed after running experiments multiple times). [Yes] Please see Section 5 and Appendix. 
         \item A description of the computing infrastructure used. (e.g., type of GPUs, internal cluster, or cloud provider). [Yes] Please see Appendix. 
 \end{enumerate}

 \item If you are using existing assets (e.g., code, data, models) or curating/releasing new assets, check if you include:
 \begin{enumerate}
   \item Citations of the creator If your work uses existing assets. [Yes]
   \item The license information of the assets, if applicable. [Not Applicable]
   \item New assets either in the supplemental material or as a URL, if applicable. [Not Applicable]
   \item Information about consent from data providers/curators. [Not Applicable]
   \item Discussion of sensible content if applicable, e.g., personally identifiable information or offensive content. [Not Applicable]
   All datasets and baselines we use are publicly available. 
 \end{enumerate}

 \item If you used crowdsourcing or conducted research with human subjects, check if you include:
 \begin{enumerate}
   \item The full text of instructions given to participants and screenshots. [Not Applicable]
   \item Descriptions of potential participant risks, with links to Institutional Review Board (IRB) approvals if applicable. [Not Applicable]
   \item The estimated hourly wage paid to participants and the total amount spent on participant compensation. [Not Applicable]
 \end{enumerate}

 \end{enumerate}







 
% If your paper is accepted and the title of your paper is very long,
% the style will print as headings an error message. Use the following
% command to supply a shorter title of your paper so that it can be
% used as headings.
%
%\runningtitle{I use this title instead because the last one was very long}

% If your paper is accepted and the number of authors is large, the
% style will print as headings an error message. Use the following
% command to supply a shorter version of the authors names so that
% they can be used as headings (for example, use only the surnames)
%
%\runningauthor{Surname 1, Surname 2, Surname 3, ...., Surname n}

% Supplementary material: To improve readability, you must use a single-column format for the supplementary material.
\onecolumn
\appendix
\aistatstitle{Models That Are Interpretable But Not Transparent\\
Supplementary Materials}

\section{Theorems and proofs}

\textbf{Theorem 1.}
\textit{Let $q$ be the query with $f(q)=1$, $e^{\textrm{base}}$ be the set of conditions used by the rule in $f$ that $q$ satisfies, and $C_{q} \subseteq C \backslash e^{\textrm{base}}$ be the additional conditions satisfied by $q$. Problem \ref{eq:prob2} of selecting a subset  $e^{\textrm{add}} \subseteq C_{q}$ such that the intersection of the selected set of samples covers the minimum number of elements in $\tX$ is equivalent to the maximum coverage problem: selecting a subset $e^{\textrm{add}} \subseteq C_{q}$ such that the union of selected samples cover the maximum of the complement set $\tX^c$.}


\begin{proof}


Let $\tX_{\textrm{base}}$ be the samples captured by $e^{\textrm{base}}$. The objective can be written as 
\begin{equation*}
    \min_{e^{\textrm{add}} \subseteq C_{q}} \supp(\{e^{\textrm{base}}, e^{\textrm{add}}\}, \tX) 
    = \min_{e^{\textrm{add}} \subseteq C_{q}} \left| S_{e^{\textrm{base}}}(\tX) \cap S_{e^{\textrm{add}}}(\tX)\right|
    = \min_{e^{\textrm{add}} \subseteq C_{q}} \left| S_{e^{\textrm{add}}}(\tX_{\textrm{base}})\right|.
\end{equation*}

Minimization of the intersection is equivalent to maximizing the complement, i.e, $$\max_{e^{\textrm{add}} \subseteq C_{q}} \left| \left( S_{e^{\textrm{add}}}(\tX_{\textrm{base}})\right)^c \right| = \max_{e^{\textrm{add}} \subseteq C_{q}} \left| \left( \cap_{c_j \in e^{\textrm{add}}}S_{c_j}(\tX_{\textrm{base}})\right)^c \right| $$
%The complement of the minimized intersection is equivalent to the maximized complement of the intersection, i.e, $$\left| \tX_{\textrm{base}}\right| - \min_{e^{\textrm{add}} \subseteq C_{q}}  \left| \left( S_{e^{\textrm{add}}}(\tX_{\textrm{base}})\right) \right| = \max_{e^{\textrm{add}} \subseteq C_{q}} \left| \left( \cap_{c_j \in e^{\textrm{add}}}S_{c_j}(\tX_{\textrm{base}})\right)^c \right| $$
which, by De Morgan's law, $$=
\max_{e^{\textrm{add}} \subseteq C_{q}} \left| \left( \cup_{c_j \in e^{\textrm{add}}} S_{c_j}(\tX_\textrm{base})^c\right) \right|. $$

Since $c_j$ are binary conditions, the complement of samples captured by $c_j$ is the collection of samples captured by $\neg c_j$, i.e., 
$$\max_{e^{\textrm{add}} \subseteq C_{q}} \left| \left( \cup_{c_j \in e^{\textrm{add}}} S_{\neg c_j}(\tX_{\textrm{base}}) \right) \right|.$$
% \begin{eqnarray}
%    &&  \min_{e^{\textrm{add}}_t \subseteq C_{q_t}} \supp(\{e^{\textrm{base}}_t, e^{\textrm{add}}_t\}, \x)\\
%    && \min_{e^{\textrm{add}}_t \subseteq C_{q_t}} \bigm| \bigcap_{c_j \in \{e^{\textrm{base}}_t, e^{\textrm{add}}_t\}} S_{c_j}(\x)\bigm| \\
%    &=& \min_{e^{\textrm{add}}_t \subseteq C_{q_t}} \bigm| \bigcap_{c_j \in e^{\textrm{add}}_t} S_{c_j}(\x_{\textrm{base}})\bigm|  \quad \textrm{(only consider samples captured by $base_t$)}\\
%    &=& \max_{e^{\textrm{add}}_t \subseteq C_{q_t}} \bigm| \Bigl(\bigcap_{c_j \in e^{\textrm{add}}_t} S_{c_j}(\x_{base})\Bigr)^c \bigm| \quad \textrm{(maximize the complement)}\\
%    &=& \max_{e^{\textrm{add}}_t \subseteq C_{q_t}} \bigm| \Bigl( \bigcup_{c_j \in e^{\textrm{add}}_t} S_{c_j}(\x_{base})^c\Bigr) \bigm| \quad \textrm{(by De Morgan's law)}\\
%    &=& \max_{e^{\textrm{add}}_t \subseteq C_{q_t}} \bigm| \Bigl( \bigcup_{c_j \in e^{\textrm{add}}_t} S_{\neg c_j}(\x_{base}) \Bigr) \bigm|
% \end{eqnarray}
Therefore, Problem \eqref{eq:prob2} is the same as the maximum coverage problem. 
\end{proof}

% \textbf{Proposition 1.} \textit{Let $f$ be the underlying decision set. Let $Q$ be the collection of queries asked by the attacker, $label$ be the collected predicted labels for the queries, and $E$ be the collected explanations for the positively predicted queries. Reconstructing the exact set of rules in $f$ from $Q, label, E$ is NP-hard. }

% \begin{proof}
%     $f$ is a DNF. Each positive predicted query and each explanation can be considered as an example where the DNF formula evaluates to True. Each negative predicted query can be considered as an example where the DNF formula evaluates to False. Given a set of positive and negative queries, finding a DNF that is consistent with these queries is NP-hard \citep{pitt1988computational}. 
% \end{proof}
\vfill

\section{Datasets and experimental setup}\label{app:setup}
We use three credit datasets FICO credit risk dataset \citep{competition}, German credit dataset from UCI \citep{Dua:2019}, and a loan approval prediction problem dataset from Kaggle \citep{kaggle}. Details about these datasets are in Table \ref{tab:dataset}. 

\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|l|}\hline
       Dataset &  Samples & Features & Description \\ \hline
       FICO  &  10459 & 23 & whether someone would default on a loan\\\hline
       German credit & 1000 & 20 & predict the approval of credit applications \\\hline
       Kaggle loan prediction problem & 480 & 11 & predict the approval of loan applications\\\hline
       
    \end{tabular}
    \vspace{1mm}
    \caption{Dataset summary}
    \label{tab:dataset}
\end{table}

All experiments are run on a 2.7Ghz (768GB RAM 48 cores) Intel Xeon Gold 6226 processor. Cplex Studio 22.1 is used for FaithfulDefense IP and FaithfulDefense IP-RA. The time limit is set to 180 seconds. To train the surrogate model, we use CART, Random Forest, and GBDT from scikit-learn 
\citep{scikit-learn}. We set CART's max\_depth to 5 and use the default setting for other hyperparameters. 

\section{More experimental results}\label{app:more_results}

% \textcolor{red}{If we use other datasets that base model can achieve similar performance to black boxes, we can add a table here. }
In this section, we show more experimental results. 

% Notice that there's no ``golden standard'' model that explains the the labeling in the datasets with 100 percent accuracy, so 
% We have to guarantee the underlying model trained from the training dataset is accurate enough to be a ``silver standard'' model. 

\textbf{Interpretable models are as accurate as complex models. } We compare models produced by FastSRS, Random Forest, and GBDT on the original datasets in Table \ref{tab:base_model_vs_black_box}. FastSRS can achieve performance comparable to complex models, indicating that interpretable models can replace complex models in real applications. This motivates us to develop models that are interpretable but not transparent for practical use. 
%we take the test accuracy of the surrogate model (CART, Random Forest, and GBDT) trained with 2000 random queries and no explanation given (the right endpoint of the dark gray line in the bottom left subplot of Figure \ref{fig:cart_surrogate} and \ref{fig:rf_gbdt_surrogate}). We compare it with the prediction accuracy of the underlying model on the testing set. Table \ref{tab:base_model_vs_black_box} displays the results for all three dataset. The similarity in test performance between the underlying model and the particular surrogate models ensures that our underlying models are reliable enough, so that building surrogate models cannot give performance significantly better than extracting the exact underlying models from these datasets.
\begin{table}[h]
    \centering
    \begin{tabular}{|l|c|c|c|l|}\hline
       % Dataset & underlying model & CART & Random Forest & GBDT \\ \hline
       % FICO  & 0.713 & 0.708 & 0.725 & \textbf{0.73} \\\hline
       % German credit & 0.755 & 0.69 & \textbf{0.765} & 0.755 \\\hline
       % Kaggle loan prediction problem & \textbf{0.844} & 0.823 & 0.802 & 0.833\\\hline
       Dataset & FastSRS & Random Forest & GBDT \\ \hline
       FICO  & 0.713  & 0.725 & \textbf{0.73} \\\hline
       German credit & 0.755  & \textbf{0.765} & 0.755 \\\hline
       Kaggle loan prediction problem & \textbf{0.844}  & 0.802 & 0.833\\\hline
       
    \end{tabular}
    \vspace{1mm}
    \caption{Test accuracy of interpretable models versus complex models on the original datasets. FastSRS achieves test accuracy comparable to Random Forest and GBDT. }
    \label{tab:base_model_vs_black_box}
\end{table}

\textbf{How Many Positive Training Points are Covered by the Explanation.} Figure \ref{fig:supp_train} demonstrates that our FaithfulDefense (red, orange, and pink curves) reveals less information (i.e., captures fewer positive samples) in training sets for all three datasets using three different querying strategies, as the red, orange, and pink curves consistently fall below the green and blue curves. 
% The mid-left subfigure shows that the orange curve surpasses the red curve, suggesting that the IP solution discloses more samples than the greedy solution, even in the training set. This discrepancy arises because the solver sometimes fails to find the optimal solution within the 180-second time limit. In such cases, it is preferable to use the greedy solution instead.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{figures/supp_3_10_0_train.jpg}
    \caption{Number of queries vs. the proportion of positive samples covered by explanations on the training set. (max length $l=3$).}
    \label{fig:supp_train}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.9\textwidth]{figures/exp_time_3_10_0.jpg}
\caption{Time consumption of generating explanations. (max length $l=3$)}
\label{fig:exp_time_app}
\end{figure}

\textbf{Explanation Generation Timing.} 
Figure \ref{fig:exp_time_app} shows the explanation generation time on three datasets when three different querying strategies are used. 
We observe that the time taken by our FaithfulDefense Greedy is generally fast, usually less than 0.1 second for different querying strategies, while FaithfulDefense IP and FaithfulDefense IP-RA have higher time costs.



\begin{figure}
\centering
\includegraphics[width=0.8\linewidth]{figures/surrogate_test_rf_acc_3_10_compare_0.jpg}
\includegraphics[width=0.8\linewidth]{figures/surrogate_test_gbdt_acc_3_10_compare_0.jpg}
\caption{Comparison of test performance between base and surrogate models. Random Forest (top) and GBDT (bottom) are used by the attacker to train the surrogate model. (max length $l=3$).}
\label{fig:rf_gbdt_surrogate}
\end{figure}

\textbf{Test Accuracy of Surrogate Models.} 
Due to the space limitation, we only showed the test performance of surrogate models trained using CART in the main paper. Figure \ref{fig:rf_gbdt_surrogate} shows the test performance when Random Forest and GBDT are used to train the surrogate model. We use the default setting for these two model configurations. Test performance using Random Forest is similar to that of CART, where providing our explanations usually requires more queries compared with the other two baselines and sometimes providing our explanations yields test performance close to providing no explanations. GBDT is more powerful than CART and Random Forest; however, Random Forest and GBDT are black box models. They cannot be used to replace the interpretable models to return faithful explanations. 


\textbf{Impact of Changing Max Length.} We also study how the value of max length $l$ influences information leakage and the attacker's performance. Figure \ref{fig:supp_5_7} shows the number of queries versus support coverage when $l$ is set to 5 and 7. For all explanation methods, support coverage decreases as $l$ increases on the test set since more conditions are used in the explanation. Our FaithfulDefense outperforms the baselines, as the red, orange, and pink curves are always below the green and blue curves. Additionally, our FaithfulDefense outperforms the baselines on surrogate model performance. As shown in Figure \ref{fig:accu_5_7_cart}-\ref{fig:accu_5_7_gbdt}, the red, orange, and pink curves for the FICO and German credit datasets are generally above the green and blue curves, regardless of the attacker's querying strategy or the model class used to train the surrogate model. 

\begin{figure}
    \centering
    \includegraphics[width=.7\textwidth]{figures/supp_5_10_0_test.jpg}
    \includegraphics[width=.7\textwidth]{figures/supp_7_10_0_test.jpg}
    \caption{Number of queries vs$.$ the proportion of positive samples covered by explanations on the test set (max length $l =5$ and $l = 7$).}
    \label{fig:supp_5_7}
\end{figure}

% \begin{figure}
%     \centering
%     \includegraphics[width=1\textwidth]{figures/supp_7_10_0_test.jpg}
%     \caption{Number of queries vs. the proportion of positive samples covered by explanations on the test set ($l = 7$).}
%     \label{fig:supp_7}
% \end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_cart_acc_5_10_compare_0.jpg}
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_cart_acc_7_10_compare_0.jpg}    
    \caption{Comparison of test performance when max length $l=5$ and $l=7$ with respect to the CART model.}
    \label{fig:accu_5_7_cart}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_rf_acc_5_10_compare_0.jpg}
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_rf_acc_7_10_compare_0.jpg}    
    \caption{Comparison of test performance when max length $l=5$ and $l=7$ with respect to the Random Forest model.}
    \label{fig:accu_5_7_rf}
\end{figure}

\begin{figure}
    \centering
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_gbdt_acc_5_10_compare_0.jpg}
        \includegraphics[width=.7\textwidth]{figures/surrogate_test_gbdt_acc_7_10_compare_0.jpg}    
    \caption{Comparison of test performance when max length $l=5$ and $l=7$ with respect to the GBDT model.}
    \label{fig:accu_5_7_gbdt}
\end{figure}

\newpage
\section{FaithfulDefense for other model classes}\label{ref:app:gams}

Generalized additive models \citep{hastie1990generalized} linearly combine flexible component functions for each feature:
\begin{equation}
    g(E[y]) = \omega_0 + f_1(x_1) + f_2(x_2) + \cdots + f_p(x_p),
\end{equation}
where $x_j$ indicates the $j$th feature, the $f_j$'s are learned univariate component functions that are possibly nonlinear, and $g(\cdot)$ is a link function, e.g., the identity function for regression or the inverse logistic function for classification. Each shape function $f_j$ operates on only one feature $x_j$, thus the shape functions can directly be plotted. This makes GAMs interpretable since the entire model can be easily visualized.

In practice, each continuous feature is usually divided into bins \citep{lou2012intelligible, liu2022fast}, thereby its shape function can be viewed as a set of step functions, i.e.,
\begin{equation}
    f_j(x_j) = \sum_{k=0}^{B_j-1}\omega_{j,k}\cdot \mathbf{1}[b_{j,k}<x_j \leq b_{j,k+1}],
\end{equation}
where $\{b_{j,k}\}_{k=0}^{B_j}$ are the bin edges of feature $j$, leading to  $B_j$ total bins. 

Sparsity regularization, i.e., the $\ell_0$ penalty on the number of steps, is often used to encourage generalization and avoid constructing overly complicated models. A sparse GAM model with piecewise constant shape functions can be efficiently converted into logic models. For example, given $p$ shape functions and each shape function has $B_j$ bins, the GAM model can be converted into a multi-split decision tree with at most depth $p$. In reality, not all leaves have to reach the depth $p$. If concatenating bins from other shape functions will not change the leaf prediction, then we can stop early. Any decision tree can then be converted into decision sets by extracting leaf paths with positive predictions. 

% We can transform a sparse GAM into a logical model by creating a multi-split decision tree where the depth of tree is equivalent to the number of valid shape functions in the GAM. 



\section{Relationship to recourse}
Our proposed FaithfulDefense does not aim to provide recourse. There are three issues in providing recourse discussed below: (1) Recourse has technical challenges in being too prescriptive; (2) Recourse reveals a tremendous amount about the model, making it difficult to keep it non-transparent; (3) Laws require explanations, but not recourse. This could easily be due to the two issues listed above. Let us discuss this in more depth below.

(1) Recourse technical challenges and being too prescriptive. Recourse requires being able to change the features, and knowing costs for each possible change to the features. 

- Many features may not be able to be changed. For loan decisions, the features are typically based on credit history and job status, and there is generally no way for loan applicants to change those. Thus, recourse may simply not be possible.

- In other situations where features could be changed, there is a cost for changing each feature in the recourse framework, but those costs might be unknown or too high. How much would it cost the loan applicant to change jobs to make more per month? It is not clear they would be able to do it at all let alone having the bank know the cost for the applicant to do that. Thus, any possible recourse given to the user may not estimate these costs correctly and may not be actionable in reality (i.e., may be too prescriptive).

(2) Recourse reveals a lot about the model. If we told the user that changing feature 1 would change the decision, they would then know that feature 1 is actually used in the model, whereas our approach hides that information in order to keep the model non-transparent. So, any defensive algorithm would fail to keep the model's variables as a secret. Since the bank would not use a model that is easily revealed, it most likely would resort to a black box with unfaithful explanations again. Another option is to provide an unfaithful recourse, but this would defeat the point of providing the recourse in the first place.

(3) The obligation of loan lenders to give an explanation for each denial is sourced from ``Right to explanation'' and other similar laws (e.g., in the Code of Federal Regulations, the \href{https://www.ecfr.gov/current/title-12/part-1002/section-1002.9#p-1002.9(b)(2)}{``Form of Equal Credit Opportunity Act (ECOA) notice and statement of specific reasons''}). Those laws only govern the right to explanation, not the right to recourse, possibly due to the issues mentioned above in recourse not actually being possible, and, if possible, being impractical to compute effectively.

% \clearpage
% \bibliographystyle{plainnat} 
% \bibliography{biblio}



\end{document}