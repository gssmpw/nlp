\section{Related Work}
\textbf{Interpretability}: %Interpretability in machine learning means the reasoning behind the model prediction is understandable to humans. It's crucial for high-stakes decision-making problems such as medical diagnosis, criminal justice, and loan decisions. 
Understanding how predictions are made allows humans to identify and rectify potentially serious problems \citep{ashoori2019ai, brundage2020trustworthy, LoPiano20, RudinWa14, Thiebes20}.
%Two notions have emerged to make models understandable: (1) developing inherently interpretable models to replace black boxes; (2) providing post-hoc explanations for black-box models.
While there is an abundance of work in 
explainable artificial intelligence (XAI), through providing simpler approximations to black-box models \citep{bastani2017interpretability, lakkaraju2019faithful} or local approximations to black boxes \citep{lundberg2020local, lundberg2017unified, ribeiro2016should, simonyan2014deep, sundararajan2017axiomatic}, such techniques are unsuitable for high-stakes decisions because their explanations are often unfaithful, incomplete or misleading \citep{lakkaraju2020fool, LaugelEtAl19, rudin2019stop, adebayo2018sanity}. Such unfaithful explanations can exacerbate problems with lack of trust. 
Interpretable machine learning, on the other hand, focuses on developing inherently interpretable models. These models reflect exactly how they make decisions, and the reasoning is always faithful. Logical models such as decision sets and decision trees are important types of interpretable models that have existed since the beginning of artificial intelligence.  
Numerous algorithms have been developed for optimizing them \citep{wang2017bayesian, aglin2020learning, angelino2018learning, DashEtAl18, demirovic2022murtree, lin2020generalized, rudin2023globally}, and modern versions of these algorithms can find sparse models with accuracy comparable to that of black box counterparts. These types of models have a long precedent in high-stakes decisions because they provide logical rules that faithfully describe, for instance, why someone's loan was denied.

% \textcolor{red}{Despite the guarantee of faithfulness, existing explanation methods in contexts such as loan applications have limited consideration for the amount of information disclosed about the underlying model. This lack of protection puts the model at risk, as an attacker could potentially reconstruct it, with model extraction techniques, from a series of queries with fabricated input features such as region, age, and income level.}

\textbf{Model extraction}: Model extraction means acquiring information from an unknown target model that goes beyond simple outputs to a set of input queries.
%the information that is beyond query outputs which result from input API. %For a machine learning model as the target, the information extracted involves hyperparameters \citep{wang2018stealing,truong2021data}, model parameters \citep{lowd2005adversarial,tramer2016stealing}, and the network structure assuming that the model is a neural network \citep{oh2019towards,hua2018reverse,hu2020deepsniffer,yan2020cache,hong2018security,xiang2020open,zhu2021hermes}. 
An \textit{attacker} who extracts information from the model might aim to train its own surrogate model, having performance no worse than that of the target model \citep{tramer2016stealing,orekondy2019knockoff,shi2017steal,correia2018copycat,chandrasekaran2020exploring,shi2018active,teitelman2020stealing}. 
% The surrogate model can alternatively be made to act in the same way as the target model, making the same predictions regardless of its correctness.

% The "side-channel attack" is made for exploiting information from software and hardware for model extraction. It includes operations like forcing a cache reloading and checking whether the model accesses the cache  \citep{tromer2010efficient,yarom2014flush+}, or inferring the reading/writing operations and execution time of a model from hardware "electromagnetic emanations" (reaction time, power consumption, etc.) \citep{hu2020deepsniffer,yoshida2019model,breier2021sniff,regazzoni2020machine,batina2019csi,jap2020practical,yu2020deepem}. 

% Apart from the side-channel information leakage, 
%This paper relates to the "query-based" attack which utilizes a series of query outputs to compute model parameters if some information of the model class is known \citep{tramer2016stealing, reith2019efficiently}. %In particular, based upon the understood mathematics behind models (binary classification, logistic regression, multi-layer perceptron, support vector regression, etc.), the attacker solves the system of equations formulated by input-output queries and derives exact or approximate model functions  \citep{tramer2016stealing,reith2019efficiently}. For more complicated models (e.g. neural networks), attackers can still infer from query samples the weight parameters, depth of architecture, of the target \citep{jagielski2020high,milli2019model,rolnick2020reverse,carlini2020cryptanalytic}.

%Without exploiting the mathematic mechanisms behind the extracted models, attackers can use query outputs to train its surrogate model against various targets (CNN \citep{truong2021data,sanyal2022towards}, encoders \citep{liu2022stolenencoder,sha2023can,dziedzic2022difficulty}, GAN \citep{szyller2021good,hu2021stealing}, graph neural networks \citep{wu2022model,he2021stealing,shen2022model,defazio2019adversarial}, LSTM \citep{takemura2020model}, BERT-based models \citep{he2021model}, etc.). The attacker seeks a similar model prediction effectiveness and it's not strictly required that the target and surrogate model should coincide in architecture or weights. Nevertheless there are studies of designing the surrogate model architecture \citep{orekondy2019knockoff,shi2017steal,krishna2019thieves,juuti2019prada,takemura2020model} and the way of querying samples as training data for the sake of better surrogate model performance. 

The notion of providing an explanation clashes with the model extraction paradigm. If an explanation is required with each prediction, the attackers job is potentially much easier since the explanation could reveal the predictions of an entire portion of the input space. 
Recent studies delve into the role of explanations in model extraction attacks \citep{yan2022towards, yan2023explanation, wang2022dualcf, ezzeddine2024knowledge, miura2024megex, oksuz2023autolycus, nguyen2023xrand}. Most of these works consider explanations derived from XAI algorithms like LIME \citep{ribeiro2016should} and SHAP \citep{lundberg2020local} or counterfactual explanations. As discussed, posthoc explanations are not faithful. They are also generally incomplete, in that they reveal possibly a few variables that might be important to the prediction, but do not reveal how these variables are used to form the prediction. Explanations from inherently interpretable models would be far more valuable to an attacker because they explain the full reasoning process that led to a decision. Given that high-stakes domains like finance generally require inherently interpretable models, and should require complete and faithful explanations, we should be far more concerned by the prospect of attackers in the setting of inherently interpretable models. 

The literature, however, focuses on the protection of individuals whose scores are computed by the model, while overlooking the need to protect the companies that develop and use these models. This gap in protection can have serious consequences: companies may avoid using inherently interpretable models out of concern for the increased risk of model extraction attacks. If companies cannot secure their proprietary interpretable models, they may continue to rely on black-box models with unfaithful explanations. 

Hence, we want models that are inherently \textit{interpretable}  -- with completely faithful explanations -- but not \textit{transparent} -- we do not want attackers to approximate or see the full model with only a few queries.

%However, our problem setting diverges significantly from these prior works. Here, our underlying model is inherently interpretable, and we have to ensure that the explanations precisely reflect how the model makes predictions.

%\textbf{Model Extraction and Active Learning}:
In order to gather the most valuable data to build a surrogate model, attackers can use generative models for generating artificial data for querying the target model \citep{mosafi2019stealing,kariyappa2021maze,shi2018generative,yuan2022attack,truong2021data,sanyal2022towards}, or active learning \citep{tramer2016stealing,chandrasekaran2020exploring,pal2019framework,pal2020activethief,pengcheng2018query,shi2018active,reith2019efficiently,wang2022enhance,xie2022game}.
The attacker will eventually gather enough information to build an accurate surrogate model, and our goal is to slow down this process so the model's decision boundary remains protected for as long as possible. Ideally, we want the explanations to provide little more information than if they were absent.

%To formulate the training data for the surrogate model, there are generative models trained for generating artificial data from query samples \citep{mosafi2019stealing,kariyappa2021maze,shi2018generative,yuan2022attack,truong2021data,sanyal2022towards}. Another useful technique for optimizing training data is active learning.
%It is applied to extensive studies \citep{tramer2016stealing,chandrasekaran2020exploring,pal2019framework,pal2020activethief,pengcheng2018query,shi2018active,reith2019efficiently,wang2022enhance,xie2022game} for picking query samples and synthesize training data based upon the queries. 
% The concept of active learning as extensively explained in \citep{settles2009active}.