\section{Related Work}
\subsection{Attribute-based Enhancement Methods.}
The purpose of Re-ID task is to match objects from a gallery, but there are differences for the matching of different objects (e.g., vehicles and people respectively). Due to the rich attribute information in pedestrian images, some attribute-based feature enhancement was initially popularized in person Re-ID ~\cite{tay2019aanet,yin2023efficient, 10555193,lin2019improving, 9733175}.
Recent works~\cite{quispe2021attributenet,li2022attribute,zhang2022graph,yu2022multi,yu2023semantic} have demonstrated the effectiveness of attribute-based enhancement in vehicle Re-ID. Quispe et al.~\cite{quispe2021attributenet} proposed AttributeNet (ANet) to refine valuable attribute features in Re-ID and integrating them with general ReID features to increase the discrimination capabilities. Li et al.~\cite{li2022attribute} designed the Attribute and State Guided Structural Embedding Network (ASSEN) to enhance the discriminative feature by mitigating the negative effects of illumination and view angle while capitalizing on the positive attributes of color and type.  
Yu et al.~\cite{yu2022multi} use transformer~\cite{vaswani2017attention} for attribute extraction and leverage a multi-attribute adaptive aggregation network to highlight the significance of key attributes. 
However, these attribute-based enhancement works, without exception, require additional annotation information to complete the training in a supervised manner.
In fact, the majority of Re-ID datasets lack attribute labels, and manually annotating attribute information is an exceptionally costly and time-consuming task.


\subsection{Fine-grained Enhancement Methods.}
The Re-ID task can be regarded as a form of fine-grained identification, primarily focusing on discerning intra-class variances while also distinguishing between different target classes. The attention mechanism, such as self-attention, plays a pivotal role in fine-grained recognition. Attention-based fine-grained enhancement has proven effective in numerous Re-ID studies~\cite{rao2021counterfactual,hong2021fine,yin2020fine,zhu2022dual}. Rao et al.~\cite{rao2021counterfactual} proposed a counterfactual attention learning method that leverages causal inference to enhance the effective learning of fine-grained features. 
Hong et al.~\cite{hong2021fine} designed the Shape-Appearance Mutual learning framework (FSAM), in which shape flow and appearance flow complement each other to extract fine body shape features guided by identity. 
Yin et al.~\cite{yin2020fine} used the attention module to focus on pedestrian pose features, which are more unique and help distinguish similar appearances between people.
In contrast to prior efforts, our approach leverages fully connected layer grouping to learn the weights of different semantic attributes, achieving an effect similar to the attention mechanism, but avoid the computational complexity associated with attention module.

\subsection{Large-scale Visual-language Learning.}
In recent years, large-scale visual-language models~\cite{jia2021scaling, li2022blip, li2021align, radford2021learning} have gained widespread popularity, with CLIP~\cite{radford2021learning} being a prominent example and serving as the foundation for numerous derivative works. CLIP is a pre-trained model based on contrastive text-image pairs, capable of understanding the content in images and associating it with textual descriptions. It exhibits strong generality and achieves impressive zero-shot performance in downstream tasks. Nonetheless, the reliance of CLIP on larger model capacities presents computational efficiency challenges, serving as a prominent impediment to its practical deployment. Notably, training smaller models directly often leads to suboptimal performance, necessitating compression techniques to yield more compact and faster models without compromising their effectiveness.
To address this problem, Wu et al.~\cite{wu2023tinyclip} introduced TinyCLIP, a novel method using knowledge distillation to compress the CLIP model. TinyCLIP effectively compresses the parameters of the CLIP model through affinity mimicry and weight inheritance, maintaining a lightweight size while demonstrating remarkable zero-shot accuracy on ImageNet with minimal parameters and exhibiting strong transfer capability to downstream tasks.
Considering the comprehensive aspects of our proposed model, we opted for image encoder from TinyCLIP for our experiments to alleviate the burden of model parameters.

\subsection{CLIP-based Re-ID Methods.}
With the popularity of CLIP framework, the Re-ID field has seen numerous investigations~\cite{li2023clip, lin2023exploring, yan2023clip} into applying the CLIP framework to Re-ID tasks. The pioneering CLIP-ReID first applied the CLIP paradigm to Re-ID tasks in a two-stage training approach, achieving highly competitive results. Given the lack of specific textual descriptions for target IDs in Re-ID datasets, traditional text-image contrastive learning proves difficult to implement. CLIP-ReID~\cite{li2023clip}, via the CoOp~\cite{zhou2022learning} method, generates textual prompts for each target in the first stage, fine-tuning the image encoder in the second stage to adapt to Re-ID tasks. The successful application of the CLIP paradigm has significantly advanced the Re-ID field. Yan et al~\cite{yan2023clip} explored the extraction of fine-grained information in pedestrian re-identification with the CLIP model, aiming to leverage robust capabilities of CLIP for cross-modal fine-grained alignment to enhance the performance of Re-ID models. 
From the methods mentioned above, it's clear that applying CLIP paradigm to Re-ID tasks involves training with additional text prompts. However, these trained text prompts can be unstable, significantly increasing the complexity of model training.

\begin{figure*}[!t]
\includegraphics[width=\linewidth]{image/model.pdf}
\caption{The pipeline of the CLIP-SENet framework.
For input images, both the CNN Backbone and the SEM process them in parallel. The CNN Backbone initially processes and extracts appearance features from the images. Concurrently, to prepare the images for SEM, they are processed to fit the input format of the ViT~\cite{dosovitskiy2020image}. Then, the SEM extracts raw semantic embeddings from vehicle images. These semantic embeddings are then fused with the vehicle appearance features in a high-dimensional space to maximize the preservation of semantic information. Meanwhile, the AFEM applies adaptive weighting to these raw semantic features,  reducing the weight of noisy attributes while favoring those conducive to identity identification, resulting in refined semantic attributes. Finally, the refined features are added element-wise with the fused features to enhance the final feature representation.}
\label{fig:model}
\end{figure*}