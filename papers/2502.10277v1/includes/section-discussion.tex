\section{Discussion}

Assessing DPRs involves identifying findings and accurately localizing them relative to anatomical landmarks, a dual challenge not addressed by most previous studies. 
To our knowledge, this study is the first to establish a comprehensive benchmark for AI systems on DPR assessment, demonstrating performance comparable to human readers and excelling in specific findings.

% Given the scarcity of prior studies~\citep{yuksel2021dental} that address both components in dental panoramic radiographs, establishing a robust benchmark for AI systems is crucial. 
% Our comprehensive study incorporated realistic comparisons of AI systems against a group of dentists with varying expertise levels and uses a diverse array of data sets collected from different global regions to demonstrate the robustness of our AI system.

%%% we are better than human and most prior works (under screening scenario), but we are approaching a point where it makes no sense to push the "overall performance", and rather the "stability across data sets" makes more sense

Most previous works employed \emph{detection-only} metrics for dental finding evaluation, which overlook correspondence to FDI-labeled tooth numbering. 
For instance, in detecting missing teeth, \citet{tuzoff2019tooth} reported sensitivity/specificity of 99.41\%/99.45\%, \citet{muramatsu2021tooth} achieved a sensitivity of 96.4\%, and \citet{leite2021artificial} attained sensitivity/precision of 98.9\%/99.6\%.
For works tackling the dual challenge, we reported sensitivity/specificity of 90.2\%/95.4\%, surpassing the 75.5\% sensitivity reported by \citet{kim2020automatic} and comparable to the 96.5\% reported by \citet{vinayahalingam2021automated}.
Furthermore, our AIâ€™s specificity was 95.9\% (95\% CI: 95.1\%--96.5\%), exceeding the 80.4\% from \citet{kim2020automatic} and comparable to the 97.2\% reported by \citet{chen2022missing}.

A limited number of studies have concurrently analyzed multiple findings in DPRs~\citep{vinayahalingam2021automated, bacsaran2022diagnostic, van2024combining}.
Our AI system demonstrated exceptional performance across several metrics.
For root canal fillings, our $\textrm{F}_1$ score reached 93.7\% (95\% CI: 90.2\%--97.1\%), surpassing prior works (81.96\%--88.6\%).
For implants, our $\textrm{F}_1$ score of 86.8\% (95\% CI: 78.7\%--94.8\%) was comparable to reported values of 80.9\%--94.33\%. Similarly, for fillings, our $\textrm{F}_1$ score of 83.3\% (95\% CI: 76.2\%--90.4\%) aligned with previous results (83.0\%--86.84\%).

However, directly comparing performance across studies is inherently complex, as such comparisons disregard variations in data distributions, collection processes, curation methodologies, and AI operating point selection.
As \citet{van2024combining} proposed, combining public DPR datasets into a unified benchmarking set can help address this challenge.
Nevertheless, practical research often prioritizes novel directions, such as exploring new clinical findings, imaging modalities, or diversified test data, making standardized data consolidation impractical.

To address these challenges, we propose evaluating AI performance relative to inter-human-reader agreement (\emph{human agreement}) levels among human experts, measured using overall mean of Cohen's Kappa between any pair of human readers.
These agreements represent an approximate upper bound for AI performance, as even diagnostic consensus among clinical experts rarely exceeds such values. 
Our study revealed minimal to moderate human agreement levels for residual roots (56.2\%, 95\% CI: 35.6\%--76.8\%), fillings (74.9\%, 95\% CI: 69.8\%--80.0\%), caries (46.5\%, 95\% CI: 34.8\%--58.1\%), and periapical radiolucencies (37.6\%, 95\% CI: 14.7\%--60.6\%). 
These results underscore the inherent clinical uncertainty that persists despite predefined diagnostic standards. 
Notably, our AI system achieved Kappa values comparable to human agreement across most findings, with the exception of caries detection ($p = .024$, two-sample $t$-test), indicating room for further improvement.

%%% ...and that is why "collecting more test data" in this work is important!

Another critical consideration often overlooked in prior studies is the need for diverse, adequately sized external test sets.
Most studies utilized single-site datasets; for example, \citet{van2024combining} used two datasets (458 cases and 4 findings), while \citet{bacsaran2022diagnostic} analyzed 10 findings but with only 118 cases.
Our study is, to the best of our knowledge, the most comprehensive to date, incorporating three test sets---two of which were \emph{external}---and including a total of $\num{2625}$ cases spanning 8 types of findings.

Despite being trained on data from the Netherlands, our AI system demonstrated strong generalizability on external test sets from Brazil and Taiwan, each with distinct imaging characteristics and treatment preferences.
For example, the Netherlands dataset included textual labels, the Brazil dataset exhibited strong contrast and spinal cord inclusion, and the Taiwan dataset displayed higher overall brightness.
Regionally, treatment preferences also varied, such as Taiwan's higher third molar extraction rate (8.6\%)~\citep{liang2022tooth}, resulting in elevated prevalence of missing teeth and residual roots.

% The AI system, although not specifically designed for throughput, achieved reading speeds 79 times faster (95\% CI: 75--82) than human readers.
% Beyond performance improvements, its three-staged approach holds promise for extension to other X-ray modalities and localized findings.
% As the AI system outputs probabilities, it can be adjusted to suit specific clinical needs, such as screening versus diagnostic tasks.

% \stmharry{The other upside of AI in general is that it can be tuned to different operating points: e.g., screening and diagnostic, depending on the specific needs.}
% \stmharry{Maybe give some insights here on how much time it is saving, from sifting out unnecessary cases in exam, and the speed it gives.}
% \stmharry{TBH [acknowledge weakness] under diagnostic scenario there are improvements to be made (when beta=1 or smaller). }

This study is not without limitations.
The training data annotations did not fully adhere to the diagnostic standards established herein, particularly for findings with low prevalence (\eg, caries, residual roots, and periapical radiolucencies).
While this did not affect the validity of our claims, it highlights room for improvement, as high-quality training data often drive the last bit of gains when already using state-of-the-art models.
Additionally, our reader group, though diverse, lacked representation from some dental specialties and international training backgrounds, which could have enriched comparisons with AI performance~\citep{endres2020development}.

% The reference standard used in this study could be improved.
% The most accurate reference standard would be derived from a prospective study where findings and prior treatments are documented during live clinical practice.
% However, such an approach can be highly resource-intensive, requiring significant financial and time investments.
% Given the retrospective nature of this study, relying solely on treatment records as the golden standard would risk false negatives.
% For example, some findings may not have been treated due to minimal symptoms or resource constraints, such as a lack of budget from the patient.
% We opted to use consensus-based annotations from multiple experts, ensuring a robust and reliable reference standard.

The optimal integration of AI into clinical workflows remains an open question, as this study did not assess active deployment scenarios.
However, the high sensitivity and adjustable operating points of our AI system suggest its potential to augment clinical practice, particularly for findings that may be overlooked due to time constraints or human error.

In conclusion, we implemented an AI system for detecting findings in dental panoramic radiographs (DPRs) with performance comparable to human readers for 7 of 8 included findings.
The system was validated across three continents, highlighting real-world challenges and opportunities for AI integration into dental practice.
Its time efficiency and operational benefits hold significant potential for enhancing clinical workflows.

% Interestingly, for the AI to predict a positive finding for a specific tooth, the raw score (ranging between 0 and 1) as output by the system only had to exceed 0.007 for periapical radiolucency, 0.010 for caries, and 0.027 for residual roots.

% We chose the operating points for our models to maximize the $\textrm{F}_\beta$ score with $\beta=2$, emphasizing utility in screening settings where we prioritize sensitivity over specificity--a contrast to the diagnostic emphasis in typical clinical priorities where specificity may take precedence to avoid over-diagnosing.

\iffalse

In comparison to \citet{bacsaran2022diagnostic}, who focused on only identifying findings without localizing them, our AI system demonstrated superior sensitivity and precision for:
\begin{itemize}
    \item \textbf{Crown/bridge:} Sensitivity of 98.14\% (95\% CI: 96.36\%--99.05\%) and precision of 93.76\% (95\% CI: 91.13\%--95.65\%), exceeding their sensitivity of 96.74\% and precision of 86.30\%.
    \item \textbf{Root canal fillings:} Sensitivity of 97.24\% (95\% CI: 94.84\%--98.54\%) and precision of 94.63\% (95\% CI: 91.67\%--96.57\%), compared to their sensitivity of 86.70\% and precision of 77.72\%.
    \item \textbf{Caries:} Sensitivity of 56.11\% (95\% CI: 48.81\%--63.16\%) and precision of 44.10\% (95\% CI: 37.82\%--50.58\%), exceeding their sensitivity of 30.26\% and precision of 50.96\%.
\end{itemize}

For the other findings, our results were comparable to their findings:
\begin{itemize}
    \item \textbf{Implants:} Sensitivity of 100.0\% (95\% CI: 95.4\%--100.0\%) and precision of 86.0\% (95\% CI: 77.5\%--91.6\%), compared to their sensitivity of 96.15\% and precision of 92.59\%, noting their sample was limited to 25 true positives.
    \item \textbf{Residual roots:} Sensitivity of 71.05\% (95\% CI: 55.24\%--82.97\%) and precision of 52.94\% (95\% CI: 39.52\%--65.95\%), compared to their sensitivity of 82.14\% and precision of 67.64\%, though their sample was limited to 23 true positives.
    \item \textbf{Fillings:} Sensitivity of 84.78\% (95\% CI: 81.71\%--87.41\%) and precision of 80.69\% (95\% CI: 77.45\%--83.55\%), similar to their sensitivity of 86.08\% and precision of 87.61\%.
\end{itemize}

Compared to \citet{endres2020development}, our AI system demonstrated superior performance in detecting periapical radiolucencies, achieving a sensitivity of 93.5\% (95\% CI: 88.1\%--96.5\%) and precision of 61.1\% (95\% CI: 54.4\%--67.5\%), compared to their sensitivity of 51\% (95\% CI: 41\%--61\%) and precision of 67\% (95\% CI: 57\%--77\%).
These comparisons collectively underscore the superior performance and reliability of our AI system in summarizing dental findings, demonstrating significant advancements in aiding clinical diagnosis.
Further research is needed to refine its precision in detecting challenging findings like residual roots and caries, ensuring optimal performance in real-world scenarios.
\fi