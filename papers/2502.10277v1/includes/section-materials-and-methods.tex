\section{Materials and Methods}

\subsection{Data Sets}

\input{./includes/fig-data-flow}
\input{./includes/tbl-data-statistics}

This study analyzed $\num{6669}$ DPRs sourced from distinct clinical sites across three regions: the Netherlands, Brazil, and Taiwan (Figure~\ref{fig:data-flow}).
The sample size was determined based on the need to estimate the sensitivity and specificity metrics (>70\%) with a margin of error of Â±3\% at a 95\% confidence level, leading to a minimum sample size of $\num{897}$.
We retrospectively collected data sets from the clinical sites, and further augmented radiographs from Brazil with a publicly available repository~\citep{silva2018automatic}.
Note for the aforementioned public data, we only utilized the radiograph images as their annotations did not align with the purpose of this study.

In the Netherlands, we included cases from various clinics in the Netherlands, acquired using several machines\footnote{\emph{Cranex Novus e} (Soredex, PaloDEx Group Oy, Finland), \emph{Orthopantomograph OP300} (Instrumentarium Dental, Finland), \emph{CS 8100 3D} (Carestream Dental LLC, United States), and \emph{PaX-i3D} (Vatech, Korea)}.
% the \emph{Department of Oral and Maxillofacial Surgery at \hospitalN}, acquired using a \tba{David}{\emph{Cranex Novus e Panoramic X-ray} machine set at 90~kV and 20~mA?}.
The data set from Brazil comprises \num{300} images sourced from \emph{\hospitalB}, and another \num{1500} images sourced from the open-access database.
As for Taiwan, we included cases from \emph{the Department of Dentistry at \hospitalT}\footnote{Obtained with a \emph{Veraviewepocs 2D} (J. Morita Mfg. Corporation., Japan) machine set at 60--80~kV, 1--10~mA, and 6.0--7.4~s, depending on the physical characteristics of the patients}.
All images were acquired using standard clinical protocols routinely employed at the respective hospitals.
We excluded DPRs from all sites that displayed mixed dentition, presence of metal accessories, or structural anomaly of bones (\eg, mandibular fractures or surgery thereof), as these conditions could potentially interfere with the imaging analysis.  
Additionally, images of inferior quality, which would typically necessitate a retake in a clinical setting, were also excluded.

The refined data sets comprised $\num{5245}$ images from the Netherlands, chronologically divided into $\num{4044}$ for AI training and validation, and $\num{1201}$ for AI testing and evaluation as the \emph{internal test set}. 
Also in the refined datasets were $\num{1173}$ images from Brazil and $\num{251}$ images from Taiwan, designated exclusively for AI testing and evaluation as the \emph{external test sets}.
Data collection procedures were reviewed and approved by the \emph{Institutional Review Board} of \hospitalN\footnote{Reference number: 2019-5232.} and \hospitalT\footnote{Reference number: \#202102018RINB.} respectively. 
All radiographs were anonymized and de-identified prior to their use in this study.

\subsection{Contour Labels for AI Training \& Reference for Evaluation}
\label{sec:label-for-training}

\input{./includes/fig-model-flow}

Two general dental practitioners both with 15 years of experience participated in the annotation of contour labels for AI training, as illustrated in Figure~\ref{fig:model-flow}. 
The training set comprised $\num{4044}$ DPRs from the Netherlands, analyzed for the 8 specific finding objects with no additional clinical information included:
\begin{enumerate*}[label=(\alph*)]
    \item presence of teeth,
    \item implants,
    \item residual roots, 
    \item crown/bridges, 
    \item root canal fillings, 
    \item fillings, 
    \item caries, and
    \item periapical radiolucencies.
\end{enumerate*}
Each object of interest was individually contoured by the first GDP, with concurrent annotation of tooth indices.
The annotated images, along with their associated contours and indices, were reviewed by the second GDP for accuracy, with adjustments made only where clearly necessary.
The annotated data set was then randomly divided into two subsets for AI system development: 70\% for training and 30\% for validation.

As for the evaluation of both AI and human reader assessment outcome, it was crucial to establish consistent reference finding assessments (\ie, the \emph{golden standard} for this study, as illustrated in Figure~\ref{fig:model-flow}) for each of the cases in the test sets.
The two GDPs who annotated the Netherlands training set also annotated the contour labels for the Netherlands test set and the Brazil test set similarly, and these contours were converted to the reference assessments.
For the Taiwan data, four GDPs (\gdpYW, \gdpCC, \gdpHC, and \gdpHW) were involved to produce the reference.
These four GDPs convened to standardize the diagnostic criteria for each finding prior to annotating their own assessments, and then they resolved discrepancies via a discussion to reach unanimous agreement for the final reference assessments.
% Refer to Appendix~\ref{sec:reference-data-flow} for reference annotation workflows.

% All findings for each tooth required unanimous agreement among the four GDPs; any initial discrepancies were resolved through discussion to ensure consistent and reliable reference data.
% For evaluating both AI and human reader performance in \emph{finding summarization}, it was crucial to establish a consistent reference finding summary (the \emph{ground truth} for this study) for all test sets.
% For the Netherlands and Brazil test data, the two GDPs first annotated the findings and tooth contours similarly to the training data set procedure.
% Subsequently, these annotations were matched by a conversion script to generate a preliminary reference.
%, with \num{256} binary labels per image (\num{32} teeth with 8 findings each).
% Following this step, a third GDP (\gdp3), with four years of clinical experience, reviewed each radiograph and its associated labels, making corrections only when necessary to ensure accuracy.

\subsection{Human Reader Benchmarks for AI Comparison}

To establish a benchmark for our AI system, we randomly selected $\num{118}$ DPRs from the Taiwan data set (denoted as Taiwan*).
Four experienced dental practitioners were recruited as readers -- two general dental practitioners each with 2 and 3 years of experience (denoted as G1 and G2 in subsequent analyses), and two specialized dentists each with 11 years (prosthodontics/orthodontics) and 15 years (endodontics) of experience (denoted as S1 and S2).
These readers were distinct from the reference annotators mentioned in Section~\ref{sec:label-for-training}.
During the evaluation, all readers followed the same protocols established for the reference annotators for the Taiwan set.
However, unlike the reference annotations where a consensus was required, each benchmark reader independently submitted their own finding assessments.
These submissions were then individually evaluated against the reference.

\subsection{AI System}
\label{sec:ai-system}

The AI system employed in this study consists of a three-stage pipeline as depicted in Figure~\ref{fig:model-flow}.
Initially, an object detection model~\citep{redmon2016you} identifies and locates teeth presence and 7 other types of dental findings (as described in Section~\ref{sec:label-for-training}) within the DPRs.
The second stage involves classification of tooth indices at the pixel level using a semantic segmentation model~\citep{chen2017deeplab}.
Both stages utilize deep convolutional neural networks (CNNs), taking only the DPRs as input and no other information.
The results from these two stages are integrated during a post-processing stage, employing a probabilistic algorithm to correlate detected objects and classified indices, producing a finding assessment table for each case.
This table assigns a confidence score, ranging from zero to one, for each finding associated with each tooth, resulting in $\num{256}$ scores per DPR for evaluation.

AI system training and testing (case reading) was done on a workstation running on Ubuntu 22.04 with an Intel Core i9-10900 CPU, a nVIDIA GeForce RTX 3080 GPU, and 64GB of system memory.
The software\footnote{\url{https://github.com/stmharry/dental-pano-ai.git}.} was implemented using python 3.11 and various deep learning libraries.
% Refer to Appendix~\ref{sec:ai-system-pipeline} for a more detailed description on AI pipeline and training.
Refer to Supplementary Material for a more detailed description on AI pipeline and training.

\subsection{Evaluation Metrics}

% The primary metrics for evaluating AI performance included per-finding analysis with receiver operating characteristic (ROC) curves and the area under the ROC curves (AUC-ROC).
% The AUC-ROC was estimated using the Wilcoxon \emph{U}-statistic, and confidence intervals were calculated following DeLong's method~\citep{delong1988comparing}.
% Once an operating point for the AI was established, metrics such as sensitivity, specificity, precision, and F-scores were evaluated, alongside their confidence intervals (CIs) using Wald's method for binomial proportions.

AI performance was evaluated using receiver operating characteristic (ROC) curves and the area under the ROC curve (AUC-ROC), estimated via the Wilcoxon \emph{U}-statistic with confidence intervals calculated by DeLong's method~\citep{delong1988comparing}. 
Key metrics, including sensitivity, specificity, precision, and F-scores, were computed with confidence intervals using Wald's method for binomial proportions.

% Superiority and non-inferiority tests adhered to the Obuchowski-Rockette-Hillis (ORH) multi-reader multi-case (MRMC) paradigm, employing the jackknife resampling method to estimate covariances of the data.
% The $p$-values were derived from a two-sided distribution of the $t$-statistics for superiority tests, and from a one-sided distribution in cases of non-inferiority tests.
% Cohen's Kappa was utilized to compare human readers' annotations, with confidence intervals calculated according to McHugh's method~\citep{mchugh2012interrater}.
% Statistical tests to differentiate distributions of Kappa values followed the ORH analysis, where readers were treated as randomly sampled parameters.

Superiority and non-inferiority tests followed the Obuchowski-Rockette-Hillis (ORH) multi-reader multi-case (MRMC) framework, with jackknife resampling to estimate data covariances. 
Cohen's Kappa assessed agreement between human readers with CI calculated using McHugh's method~\citep{mchugh2012interrater}.
Statistical differences in Kappa distributions were analyzed via ORH methods.

% Secondary evaluation measures included the reading time for both human readers and the AI system.
% Notably, the \emph{implant} finding was evaluated solely on a subset of teeth classified as \emph{missing} in the reference finding assessments.
% Similarly, evaluations for 6 other findings--residual root, crown/bridge, root canal filling, filling, caries, and periapical radiolucency--were conducted exclusively on present teeth in the reference.

Reading time comparisons included human readers and the AI system.
Evaluations for specific findings, such as \emph{implants}, were restricted to relevant tooth subsets (i.e., \emph{missing teeth} for implant evaluations and \emph{present teeth} for 6 other findings).
All metrics were calculated with 95\% confidence intervals (CI), and statistical significance was denoted for $p$-values below 0.05.

% All metrics were calculated with 95\% confidence intervals (95\% CI) to ensure statistical reliability.
% Statistical tests with $p$-values less than 0.05 were considered significant and labeled as ``*'', while non-significant tests were noted as ``n.s.''.
% Statistical analyses were conducted using Python version 3.11, leveraging packages specifically designed for MRMC analysis~\citep{mckinney2022comparing}.

Analyses were conducted in Python 3.11 using MRMC-specific libraries~\citep{mckinney2022comparing}.