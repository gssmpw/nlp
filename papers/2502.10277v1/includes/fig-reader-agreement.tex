\begin{figure}[!t]
    \centering
    \includegraphics[width=1.0\textwidth, height=0.80\textheight, keepaspectratio]{./assets/fig-reader-agreement.pdf}
    \caption{
        \textbf{Inter-Reader Agreement Levels for Dental Finding Summaries in Taiwan.}
        This figure presents the agreement levels between pairs of readers as measured by Cohen's Kappa for various dental findings.
        Each blue error bar illustrates the Kappa agreement between each of the six possible reader pairings (G1/G2, G1/S1, G1/S2, G2/S1, G2/S2, and S1/S2) from four participating readers, grouped into general dentists (G1 and G2) and specialists (S1 and S2).
        The agreement levels are averaged for pairs within the same expertise group (generalists or specialists) and across different expertise, shown as red lines.
        The error bars represent the 95\% confidence intervals for the Kappa values.
        Significance testing using $t$-statistics revealed that differences in mean agreement levels for residual roots were statistically significant ($p = .039$), while other findings showed no significant differences.
    }
    \label{fig:reader-agreement}
\end{figure}