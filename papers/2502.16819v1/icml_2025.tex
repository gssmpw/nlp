% Specific for icml submission

\documentclass{article}
\usepackage{icml2025}
% \usepackage[accepted]{icml2025}

\usepackage{microtype}
% \usepackage{subfigure}
\usepackage{booktabs} % for professional tables

\newcommand{\theHalgorithm}{\arabic{algorithm}}


% Previous packages

\usepackage{graphicx}   
\usepackage{amsmath}
\usepackage{amssymb} 
\usepackage{amsfonts}
\usepackage{amscd}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{verbatim}
% \usepackage[small,bf]{caption}
% \usepackage{subcaption}
% \setlength{\captionmargin}{30pt}
\usepackage{bbm}
\usepackage{microtype}
\usepackage{kantlipsum}
\usepackage{mathtools, cuted}
\usepackage{units}
\usepackage{comment}
\usepackage{color}  
\usepackage{url}
\usepackage{hyperref}
% \usepackage{subfig}
\usepackage{listings}
\usepackage{tikz} 
\usepackage{tikzlings}
\usepackage{tikzducks}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{fancyhdr}
\usetikzlibrary{patterns}
% \usepackage{algorithmicx}
% \usepackage{algorithm}
% \usepackage{algpseudocode}
\usepackage{framed}
% \usepackage{fullpage}
\usepackage{tcolorbox}
\usepackage{fge}
\usepackage{mathdots} 
\usepackage{enumitem}
\usepackage{amsthm}



\renewcommand{\mathbf}{\boldsymbol} 
\newcommand{\sff}{\mr{I\!I}}

\input{traversal_macros.def}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors
\usepackage{standalone}

\usepackage{caption}
\usepackage{subcaption}

\usepackage{mathtools}
% \mathtoolsset{showonlyrefs}

%%% ICML 2025 %%%

% \usepackage[capitalize,noabbrev]{cleveref}




%%% %%%



\icmltitlerunning{Fast, Accurate Manifold Denoising by Tunneling Riemannian Optimization}

\begin{document}

\twocolumn[
\icmltitle{Fast, Accurate Manifold Denoising by\texorpdfstring{\\}{ }Tunneling Riemannian Optimization}

\icmlsetsymbol{equal}{*}

\begin{icmlauthorlist}
\icmlauthor{Shiyu Wang}{ee,dsi}
\icmlauthor{Mariam Avagyan}{ee,dsi}
\icmlauthor{Yihan Shen}{cs,dsi}
\icmlauthor{Arnaud Lamy}{ee,dsi}
\icmlauthor{Tingran Wang}{ee,dsi}
\icmlauthor{Szabolcs M\'arka}{physics,dsi}
\icmlauthor{Zsuzsa M\'arka}{astrophysics Lab,dsi}
\icmlauthor{John Wright}{ee,apam,dsi}
\end{icmlauthorlist}


\icmlaffiliation{ee}{Department of Electrical Engineering, Columbia University, New York, United States}
\icmlaffiliation{cs}{Department of Computer Science, Columbia University, New York, United States}
\icmlaffiliation{physics}{Department of Physics, Columbia University, New York, United States}
\icmlaffiliation{astrophysics Lab}{Columbia Astrophysics Laboratory, Columbia University, New York, United States}
\icmlaffiliation{apam}{Department of Applied Physics and Applied Mathematics, Columbia University, New York, United States}
\icmlaffiliation{dsi}{Data Science Institute, Columbia University, New York, United States}


\icmlcorrespondingauthor{Shiyu Wang}{sw3601@columbia.edu}




\icmlkeywords{Machine Learning, ICML}

\vskip 0.3in
]

\printAffiliationsAndNotice{}


\begin{abstract}
Learned denoisers play a fundamental role in various signal generation (e.g., diffusion models) and reconstruction (e.g., compressed sensing) architectures, whose success derives from their ability to leverage low-dimensional structure in data. Existing denoising methods, however, either rely on local approximations that require a linear scan of the entire dataset or treat denoising as generic function approximation problems, often sacrificing efficiency and interpretability. We consider the problem of efficiently denoising a new noisy data point sampled from an unknown $d$-dimensional manifold $\mc M \in \mathbb{R}^D$, using only noisy samples. This work proposes a framework for test-time efficient manifold denoising, by framing the concept of ``learning-to-denoise'' as {\em ``learning-to-optimize''}. We have two technical innovations: (i) {\em online learning} methods which learn to optimize over the manifold of clean signals using only noisy data, effectively ``growing'' an optimizer one sample at a time. (ii) {\em mixed-order} methods which guarantee that the learned optimizers achieve global optimality, ensuring both efficiency and near-optimal denoising performance. We corroborate these claims with theoretical analyses of both the complexity and denoising performance of mixed-order traversal. Our experiments on scientific manifolds demonstrate significantly improved complexity-performance tradeoffs compared to nearest neighbor search, which underpins existing provable denoising approaches based on exhaustive search.
\end{abstract}


\section{Introduction} 

{\em Denoising} is a core task in signal and image processing. Denoisers also play a fundamental role in state-of-the-art approaches to signal {\em generation} and {\em reconstruction}. Diffusion models generate intricate images from pure noise, via a sequence of denoising steps; learned compressed sensing methods reconstruct accurate medical and scientific images from incomplete, indirect measurements, again via a sequence of denoising steps. While these iterative procedures produce high-quality results, they are computationally costly: a sophisticated learned denoiser needs to be applied repeatedly to produce a single output. The test-time cost of denoising is a major bottleneck for both high-resolution image generation and real-time image reconstruction. 

{\em Accurate} denoising is critical, because the denoiser encodes prior knowledge about the set of images of interest (natural, medical, scientific, etc.). These images reside near low-dimensional subsets of the image space, which are often conceptualized as low-dimensional manifolds; learning to denoise is tantamount to learning these manifolds. 

In this paper, we study a model {\em manifold denoising} problem, in which the goal is to learn to denoise data lying near a $d$-dimenisonal submanifold $\mc M$ of a high-dimensional data space $\bb R^D$. As we will review below, there are extensive literatures on learning to denoise, and learning manifold models from data. However, there is relatively little work on provably learning {\em test-time efficient} denoisers. Existing methods with provable near-optimal manifold denoising performance involve linearly scanning large datasets. As a baseline, nearest neighbor search across a minimal covering set has worst case complexity at least $O(D e^{cd})$, where $e^{cd}$ is the size of the dataset required to cover the d-dimensional manifold, and $D$ is the cost of computing distance in the ambient dimension. On the other hand, more practical neural network models currently lack guarantees of performance and efficiency. 




We bridge this gap, developing and analyzing a family of manifold denoisers that are (i) test-time efficient, (ii) accurate, and (iii) trainable. Our main idea is to {\em cast the problem of denoising a new, noisy test sample $\mb x$ as an optimization problem over the a-priori unknown Riemannian manifold $\mc M$.} This enables us to draw on tools from Riemannian optimization, to develop methods which converge to a small neighborhood of the ground truth -- a significant efficiency gain vis-a-vis exhaustive scan. 

One challenge stems from the unknown nature of the manifold and observation of only noisy samples. To apply ideas from Riemmanian Optimization, we need an accurate approximation of the manifold. Our framework addresses this by {\em learning} the optimizer directly from noisy data via an online algorithm. The method uses the manifold's low dimensional structure and learns local linear models to facilitate movement in the tangent direction.

Another challenge is that, in general, Riemannian optimization methods converge to critical points, which could include suboptimal local minimizers. To address this, we build tunnels (\cref{fig:tunnels}) to allow escape from each local minimizer. When stuck at a suboptimal local minimizer, we select the tunnel which would bring us closest to the target point, an idea reminiscent of graph-based nearest neighbor search \cite{malkov2018efficient}.

Our method, with high probability, achieves near optimal denoising error $\| \wh{\mb x} - \mb x_\natural \| \lesssim \kappa \sigma \sqrt{d}$ with test-time computational cost \footnote{Number of multiplications during inference.} \label{eq:introductino theory result}{\small $      O\left(C(\mc M,\epsilon_1,\delta) (Dd+e^{cd} d) +D \times \$^{\eta}_{c' \tau_{\mc M}} (\mc M)\right)$,} %\sw{in case we change the expression in appendix}, 
where $\$^{\eta}_{r} (\mc M)$ is a novel complexity measure which we describe in \Cref{sec:theory}, it quantifies the cost of escaping local minimizers, $C(\mc M,\epsilon_1,\delta)$ depends on the diameter and curvature of $\mc M$, the step size $\delta$ and stopping tolerance $\epsilon_1$, and $c,c'$ are numerical constants.

\section{Relationship to the Literature} 
\label{sec:prior} 




Denoising has a long history in signal processing and machine learning, evolving from early statistical techniques to modern deep learning methods. Traditional denoising techniques are often designed based on structural assumptions about the clean signal $\mathbf{x}_\natural$. For smooth signals, Fourier-based methods effectively suppress high-frequency noise \cite{wiener1949extrapolation}. Sparsity assumptions have given rise to wavelet shrinkage and dictionary learning techniques \cite{donoho1995wavelet, elad2006image}. In images with self-similarity, methods such as nonlocal means and BM3D leverage repetitive patterns to enhance denoising performance \cite{buades2005non, dabov2007image}. Low-dimensional subspace methods, e.g.,principal component analysis and the Karhunen–Loève transform, approximate signals using fewer basis components to filter out noise \cite{does2019evaluation, aysin1998denoising}. 

Recent developments in signal reconstruction and generation have placed denoisers in a more central role: an accurate denoiser serves as an implicit model for clean signals. The plug-and-play framework leverages denoisers as implicit priors within iterative optimization, enabling flexible and efficient reconstruction across tasks like deblurring, super-resolution, and inpainting \cite{venkatakrishnan2013plug, zhang2021plug}. A high-quality denoiser not only removes noise but also encodes structural information about the data, making it a powerful tool for complex inverse problems. More recently, diffusion models have demonstrated that iterative denoising can effectively model complex data distributions, emerging as one of the most powerful generative modeling techniques \cite{ho2020denoising, song2019generative}. Denoising has become a fundamental building block in modern computational frameworks.

With the advance of deep learning, generic ML architectures such as FCNNs, CNNs \cite{ilesanmi2021methods}, transformers \cite{yao2022dense}, or the U-Net \cite{fan2022sunet} have demonstrated strong denoising capabilities by learning to approximate denoising functions, a concept known as learning-to-denoise. Their effectiveness largely stems from their ability to capture underlying low-dimensional structures in data—an idea explicitly leveraged in autoencoder-based methods \cite{vincent2008extracting}. However, these models do not directly incorporate the low-dimensional structure of the data, leading to considerable inefficiency in computation and scalability. Furthermore, as neural networks function are black-box models, they often lack theoretical guarantees or clear interpretability, limiting their reliability in critical applications.

Given that high-dimensional data  often reside on or near a lower-dimensional submanifold \cite{tenenbaum2000global, fefferman2016testing}, incorporating \emph{manifold structure} into denoising tasks --- so-called manifold denoising \cite{hein2006manifold} --- has emerged as a rapidly growing area of interest. In the theoretical literature on manifold estimation and denoising \cite{genovese2012minimax}, the predominant methods are based on local approximation \cite{fefferman2020reconstruction, yao2023manifold}, and while they offer near-optimal theoretical estimation and denoising guarantees, their test-time efficiency suffers — each new test sample requires a linear scan of the entire dataset. 

In this work, we develop test-time efficient denoisers using ideas from Riemannian optimization \cite{absil2008optimization, sato2021riemannian}. Standard approaches to Riemannian optmization require $\mc M$ to be known a-priori. Several recent works \cite{sober2020approximating,sober2020manifold,shustin2022manifold} develop manifold optimizers for a-priori unknown $\mc M$, using local affine approximation (via moving least squares). While this approach is inspiring, it encounters the same test-time efficiency issues as the above manifold denoisers, since these approximations are formed on-the-fly by linearly scanning a large dataset. 

We develop test-time efficient denoisers by learning Riemannian optimizers over a particular geometric graph, which approximates $\mc M$. Our approach draws inspiration from graph-based approximate nearest neighbor search \mbox{\cite{malkov2018efficient}}, while leveraging the low dimensionality of $\mc M$ to avoid costly ambient-space distance calculations. 
\begin{figure}[t]
\centerline{ 
\resizebox{\columnwidth}{!}{ % Adjust the width here (e.g., 0.8\columnwidth)
    \input{figs/problem_formulation/problem_formulation}
}
}
\caption{{\bf Problem Setup.} $\mb x_1 = \mb x_{1,\natural} + \mb \nu_1, \dots, \mb x_N = \mb x_{N,\natural} + \mb \nu_N$ are noisy traning samples from an a-priori unknown Riemannian manifold $\mc M \subset \bb R^D$. $\mb x = \mb x_\natural + \mb \nu$ is the noisy test sample.}
\label{fig:setup} 
\end{figure}




\section{Problem Formulation} \label{sec:prob} 

Our goal is to {\em learn to denoise} data sampled from a $d$-dimensional submanifold $\mc M$ of $\bb R^D$. We observe iid training samples $\mb x_1, \dots, \mb x_N \in \bb R^D$  generated as 
\begin{equation} \label{eqn:training_sample}
    \underset{\text{\color{red!50!blue} \bf training sample}}{\mb x_i \strut} = \underset{\text{\color{blue} \bf clean signal}}{\mb x_{i,\natural} \strut} + \; \underset{\text{\color{red} \bf noise}}{ \mb \nu_i, \strut }
\end{equation}
with signal $\mb x_{i,\natural} \sim \mu_\natural$, a distribution supported on $\mc M$, and noise $\mb \nu_i \sim_{\mr{iid}} \mc N(0,\sigma^2)$,  independent of the signal. Figure \ref{fig:setup} illustrates this setup. 
Our goal is to produce $f : \bb R^D \to \bb R^D$ such that for new samples $\mb x = \mb x_\natural + \mb \nu$ from the same distribution, $f(\mb x_\natural + \mb \nu) \approx \mb x_\natural$,
i.e., $f$ {\em denoises} $\mb x$. We seek $f$ satisfying the following properties: 

{\bf [D1] Provably Accurate Denoising:} near-optimal denoising performance, i.e., $\bb E  \| f(\mb x) - \mb x_\natural \|_2^2 \lesssim d \sigma^2$, where $d$ is the intrinsic dimension of $\mc M$ and $\sigma$ the noise standard deviation.

{\bf [D2] Fast Evaluation at Test Time:} $f$ uses the low dimensional structure and can be applied to new samples with computational cost $C(\mc M) \cdot (D+e^{cd})d $, where $Dd$ is the cost of a linear projection from $D$ dimensions to $d$ dimensions, and $e^{cd} d$ is the cost of searching on a manifold of dimension $d$. 

{\bf [D3] Data-Driven Learning:} $f$ can be learned using only noisy training samples.


In next sections, we will introduce a trainable denoising method based on {\em manifold optimization}, which achieves {\bf [D1]}, {\bf [D2]} and {\bf[D3]}. 


\section{Denoising and Manifold Optimization} \label{sec:main-ideas} 

Since the observed signal $\mb x$ is a noisy version of some signal $\mb x_\natural$ on the manifold $\mc M$, one natural approach to denoising is to {\em project} $\mb x$ onto $\mc M$, by solving 
\begin{equation} \label{eqn:projection-problem}
    \min_{\mb q \in \mc M} \varphi_{\mb x} (\mb q) \equiv \tfrac{1}{2} \| \mb q - \mb x \|_2^2.
\end{equation}
the solution $\wh{\mb x}$ to this problem can be interpreted probabilistically as a {\em maximum likelihood} estimate of $\mb x_\natural$ when the underlying manifold has a uniform distribution; it also accurately approximates the minimum mean squared error (MMSE) denoiser when $\sigma$ is small.\footnote{ Indeed, letting $f_{\mr{MMSE}}(\mb x ) = \arg \min_{f} \bb E_{\mb x_\natural, \mb z} \| f(\mb x_\natural + \mb z) - \mb x_\natural \|_2^2$ denote the MMSE denoiser, we have that  $\| f_{\mr{MMSE}}(\mb x) - \mc P_{\mc M}[\mb x] \|_2 = O(\sigma)$, on a set of $\mb x$ of measure $1 -O(\sigma)$. Other statistical criteria, such as {\em maximum a-posteriori (MAP)} also lead to manifold optimization problems: in the Bayesian setting in which we have a prior density $\rho_{\natural} : \mc M \to \bb R$ on clean signals, the MAP estimate minimizes $\tfrac{1}{2} \| \mb q -\mb x\|_2^2 - \lambda \log \rho_\natural(\mb q)$ over $\mc M$.} The projection problem \eqref{eqn:projection-problem} can be interpreted as a {\em manifold optimization} problem -- we seek to minimize the function $\varphi_{\mb x}$ over a smooth Riemannian submanifold $\mc M$ of $\bb R^D$. 

\usetikzlibrary {arrows.meta} 

\newcommand{\dotcolor}{red}



\begin{figure}[h]
\centering
\begin{tikzpicture}
    % The right part: TikZ diagram
    \draw [fill=blue!5!white] (-1,-1) to [out=85,in=-65] (-1.2,1) to [out=15,in=160] (3,1) to [out=-45,in=85] (3.75,-1) to [out=160,in=20] cycle;
    \draw [\dotcolor, ultra thick, loosely dotted] (-.5,-.5) to [out=85,in=-70] (-.69,.92);
    \draw [\dotcolor, ultra thick, loosely dotted] (-.248,-.42) to [out=88.5,in=-69] (-.448,.95);
    \draw [\dotcolor, ultra thick, loosely dotted] (-.058,-.38) to [out=90,in=-69] (-.25,.98); 
    \draw [\dotcolor, ultra thick, loosely dotted] (.142,-.32) to [out=90,in=-70] (-.055,.94); 
    \draw [\dotcolor, ultra thick, loosely dotted] (.345,-.26) to [out=91,in=-71] (.1245 ,.965); 
    \draw [\dotcolor, ultra thick, loosely dotted] (.535,-.23) to [out=91,in=-71] (.3165 ,.9985); 
    \draw [\dotcolor, ultra thick, loosely dotted] (.73,-.19) to [out=92,in=-72]  (.5075 ,1.029); 
    \draw [\dotcolor, ultra thick, loosely dotted] (.95,-.14) to [out=92,in=-72]  (.70 ,1.06); 
    \draw [\dotcolor, ultra thick, loosely dotted] (1.165,-.12) to [out=93,in=-72]  (.9 ,1.08);
    \draw [\dotcolor, ultra thick, loosely dotted] (1.370,-.12) to [out=93.5,in=-72] (1.09 ,1.085);
    \draw [\dotcolor, ultra thick, loosely dotted] (1.575,-.12) to [out=94,in=-72.2] (1.285 ,1.085);
    \draw [\dotcolor, ultra thick, loosely dotted] (1.78,-.12) to [out=94,in=-72.2] (1.485 ,1.085);
    \draw [\dotcolor, ultra thick, loosely dotted] (1.96,-.13) to [out=94,in=-74.4] (1.685 ,1.082);
    \draw [\dotcolor, ultra thick, loosely dotted] (2.16,-.15) to [out=94,in=-74.4] (1.885 ,1.05);
    \draw [\dotcolor, ultra thick, loosely dotted] (2.4,-.185) to [out=94,in=-73] (2.07 ,1.02);
    \draw [\dotcolor, ultra thick, loosely dotted] (2.62,-.26) to [out=94,in=-73] (2.28,.99);
    \draw [\dotcolor, ultra thick, loosely dotted] (2.87,-.32) to [out=95,in=-60] (2.47 ,.93);
    \draw [\dotcolor, ultra thick, loosely dotted] (3.10,-.36) to [out=95,in=-60] (2.67 ,.93);
    \draw[color = red!50!blue, dashed] (2.3,.8) -- (2.45,1.5);
    \filldraw[color = black] (2.75,.75) circle (2pt);
    \filldraw[color = black] (2.45,1.5) circle (2pt);
    \node at (2.75,1.6) {\color{black} $\mb x$}; 
    \filldraw[color = red!50!blue] (2.3,.8) circle (2pt);  
    \draw [black,dashed] (2.75,.75) -- (2.45,1.5);
    \node at (3.1,.65) {\color{black} $\mb x_\natural$}; 
    \filldraw[color = blue] (-.5,-.5) circle (2pt); 
    \filldraw[color = blue] (.85,.25) circle (2pt); 
    \filldraw[color = blue] (1.9,.45) circle (2pt); 
    \draw [blue, thick, -{Stealth[length=3mm]}] (-.5,-.5) -> (.85,.25);
    \draw [blue, thick, -{Stealth[length=3mm]}] (.85,.25) -> (1.9,.45);
    \draw [blue, thick, -{Stealth[length=3mm]}] (1.9,.45) -> (2.3,.8); 
    
    % The left part: Explanation (moved below)
    \node [anchor=west] at (-2,-2.3) {\color{red} \bf Exhaustive Covering};
    \node [anchor=west] at (-2,-2.65) {\color{red} $( C / \eps)^d$ points for an $\eps$-accurate solution};
    \node [anchor=west] at (-2,-1.5) {\color{blue} \bf Optimization Trajectory};
    \node [anchor=west] at (-2,-1.85) {\color{blue} $C \log (1/\eps)$ steps for an $\eps$-accurate solution};
    \draw [blue, thick, -{Stealth[length=3mm]}] (-2.65,-1.5) -> (-2.1,-1.5); 
    \draw [\dotcolor, ultra thick, loosely dotted] (-2.65,-2.0) -> (-2.1,-2.0); 
\end{tikzpicture}
\caption{{\bf Dimension scaling advantage} of optimization for searching Riemannian manifolds. Brute force search (which forms the core of SOTA provable methods) requires test-time computation {\em exponential} in intrinsic dimension $d = \mr{dim}(\mc M)$.} 
\label{fig:opt-cover} 
\end{figure}



 
\paragraph{Dimension Scaling Advantage of Iterative Optimization.} The  optimization problem \eqref{eqn:projection-problem} could, in principle, be solved in a variety of ways. One simple approach is to compute $\varphi_{\mb x}$ on a dense grid of samples $\mb q_1, \dots, \mb q_M \in \mc M$, and select the sample $\mb q_i$ with the smallest objective value. As illustrated in Figure \ref{fig:opt-cover}, such exhaustive search becomes increasingly inefficient as the manifold dimension $d$ increases. While exhaustive search is not a method of choice for solving smooth optimization problems, it plays a critical role in state-of-the-art theoretical manifold denoisers \mbox{\cite{yao2023manifold}}. At their core is a local approximation of $\mc M$, formed by selecting near neighbors of $\mb x$ by linearly scanning a dataset $\mb x_1, \dots, \mb x_N$ which is large enough to densely cover $\mc M$ -- a form of exhaustive search. 

A more scalable alternative is to produce $\wh{\mb x}$ by {\em iterative optimization} -- e.g., by gradient descent. The objective function $\varphi_{\mb x}$ is differentiable, with gradient 
$\nabla_{\mb q} \varphi_{\mb x} = \mb q - \mb x$. The {\em Riemannian gradient} of $\varphi_{\mb x}$ at point $\mb q \in \mc M$ is the projection of $\nabla \varphi_{\mb x}$ onto the tangent space $T_{\mb q} \mc M$ of $\mc M$ at $\mb q$: 
\begin{equation}
    \mr{grad}[\varphi_{\mb x}](\mb q) = \mc P_{T_{\mb q} \mc M} \nabla \varphi_{\mb x}(\mb q). 
\end{equation}
This is the component of the gradient along the manifold $\mc M$, and a direction of steepest descent on $\mc M$. A {\em Riemannian gradient method} \cite{absil2008optimization} steps along $\mc M$ in the direction of $-\mr{grad}[\varphi_{\mb x}]$, setting 
\begin{equation} \label{eqn:riemannian-grad-iter} 
    \mb q^+ = \exp_{\mb q} \bigl( - t \,  \mr{grad}[\varphi_{\mb x}](\mb q) \bigr) 
\end{equation}
where $t > 0$ is a step size, and $\mr{exp}_{\mb q} : T_{\mb q} \mc M \to \mc M$ is the exponential map, which takes a direction (tangent vector) $\mb v \in T_{\mb q} \mc M$ to a new point $\exp_{\mb q}(\mb v)$ in $\mc M$. 

As illustrated in Figure \ref{fig:opt-cover}, with appropriate choice of $t$, this method converges linearly to the global minimizer $\wh{\mb x}$, provided it is initialized close enough to $\wh{\mb x}$ -- this means that the method requires $C \log ( 1 / \eps )$ steps to reach an $\eps$-approximation of $\wh{\mb x}$. Inspired by this observation, we set out to build test-time efficient denoisers that emulate the gradient iteration \eqref{eqn:riemannian-grad-iter}. There are two main challenges in realizing this idea: first, we do not know the manifold $\mc M$ -- we only have noisy samples $\mb x_1, \dots, \mb x_N$. Second, the optimization problem \eqref{eqn:projection-problem} can exhibit suboptimal local minimizes; to guarantee the performance of our denoiser, we need to ensure convergence to the global optimizer. In the next two paragraphs, we give a high-level sketch of our approach to these challenges, deferring a full construction to Section \ref{sec:online learning}. 


\definecolor{amber}{rgb}{1.0, 0.75, 0.0}

\begin{figure*}[ht]

\centerline{
\input{figs/optimizer-from-data/optimizer-from-data}
}
\caption{{\bf Learning a Manifold Optimizer from Samples.} Given raw data samples $\mb x_1, \dots, \mb x_N$ (left), we construct an approximation (center) to $\mc M$ which consists of a collection of landmarks $\mb q_1, \dots, \mb q_M$, approximate tangent spaces at the landmarks, and a geometric graph $G$ whose vertices are the landmarks. Right. We approximately optimize over $\mc M$ by optimizing over the graph $G$. } \label{fig:optimizer-from-data} 
\end{figure*}

\paragraph{Challenge I: $\mathcal M$ is a-priori unknown.} Our approach is to {\em learn} an approximate Riemannian optimizer from data. We will approximate the manifold with a collection of landmarks $\mb q_1, \dots, \mb q_M$, which are linked by a geometric graph $G$. As illustrated in Figure \ref{fig:optimizer-from-data}, we will equip this graph with all of the necessary structure to enable optimization -- in particular, an approximation to the tangent space to $\mc M$ at each landmark, which enables us to approximate the Riemannian gradient, and edge embeddings which enable us to traverse the graph in the negative gradient direction. 

\paragraph{Challenge II: Suboptimal Minimizers.} The distance function $\varphi_{\mb x}(\mb q)$ may exhibit suboptimal local minimizers. Take, for example, Figure \ref{fig:tunnels} (left): the point $\mb q$ is a local minimizer of the function $\varphi_{\mb x}(\mb q) = \tfrac{1}{2} \| \mb q - \mb x \|_2^2$. 




\begin{figure}[h]
\centering
\begin{tikzpicture}[shift={(-1.5,0)}]
    % Left part
    \draw [gray] (0,-.2) to [out = 75,in=-75] (0,.2) to [out = 105,in = -90] (-.25,1) to [out = 90,in=135] (.5,1) to [out = -45, in = 45] (.5,-1) to [out = -135,in = -90] (-.25,-1) to [out = 90, in = -105] cycle; 
    \draw [orange, ultra thick, dotted,  -{Stealth[length=3mm]}] (-.25,1) to [out = -90,in = 105] (0,.2); 
    \filldraw[color = red] (.93,0) circle (2pt);
    \filldraw[color = blue] (.02,0) circle (2pt);
    \node at (1.05,1) {\color{gray} $\mc M$};
    \node at (1.20,0) {\color{red} $\mb x$}; 
    \node at (1,0.40) {\color{red} Test Point}; 
    \node at (-0.3,0) {\color{blue}$\mb q$};
    \node at (0.5,-0.4) {\color{blue} Local Minimizer};
    \node at (0.3,1.5) {\color{orange} Local Descent};

    % Center part
    \draw [gray] (3,-.2) to [out = 75,in=-75] (3,.2) to [out = 105,in = -90] (2.75,1) to [out = 90,in=135] (3.5,1) to [out = -45, in = 45] (3.5,-1) to [out = -135,in = -90] (2.75,-1) to [out = 90, in = -105] cycle; 
    \draw [thick, color = red!50!blue] (3.02,0) -- (3.93,0);
    \node at (4.2,-.35) {\color{red!50!blue} Added Tunnel};
    \node at (4.05,1) {\color{gray}  $\mc M'$};

    % Right part
    \draw [gray] (6,-.2) to [out = 75,in=-75] (6,.2) to [out = 105,in = -90] (5.75,1) to [out = 90,in=135] (6.5,1) to [out = -45, in = 45] (6.5,-1) to [out = -135,in = -90] (5.75,-1) to [out = 90, in = -105] cycle; 
    \draw [orange, ultra thick, dotted, -{Stealth[length=3mm]}] (6.02,0) -- (6.93,0); 
    \draw [gray, -{Stealth[length=3mm]}] (6.02,0) -- (6.93,0); 
    \filldraw[color = red] (6.93,0) circle (2pt);
    \node at (6.95,1) {\color{gray}  $\mc M'$};
    % \node at (7.0,-.3) {\color{red} Global}; 
    % \node at (7.0,-0.7) {\color{red} Minimizer}; 
    \node at (7.0,-.6) {\parbox{2cm}{\centering \color{red} Global \\ Minimizer}};
    \node at (5.5,1.5) {\color{orange} Local Descent};

    \draw [orange, ultra thick, dotted] (6,.2) to [out = 105,in = -90] (5.75,1); 
    \draw [orange, ultra thick, dotted] (6,.2) to [out = -75,in = 90] (6.045,0); 

    % Arrows between parts
    \node at (2.0,.2) {$\Rightarrow$};
    \node at (5.0,.2) {$\Rightarrow$};

    \node at (0.25,-1.75) {\parbox{2cm}{\centering \bf Suboptimal \\ Minimizers}};
    \node at (3.25,-1.75) {\parbox{2cm}{\centering \bf Domain \\ Augmentation}};
    \node at (6.25,-1.75) {\parbox{2cm}{\centering \bf Local is \\ Global}};
\end{tikzpicture}
\caption{{\bf Eliminating Suboptimal Minimizers by Adding Tunnels.} Consider a test point $\mb x$, with corresponding objective function $\varphi_{\mb x}(\mb q) = \| \mb q - \mb x \|_2^2$. Left: point $\mb q$ is a local minimizer of $\mb q$ over $\mc M$. Center: We modify the domain $\mc M$ to connect $\mb q$ and $\mb x$ -- informally, adding a ``tunnel'' connecting these points. Right: local descent over the augmented domain $\mc M'$ converges to the {\em global minimizer} $\mb x$.}\vspace{-.125in}
\label{fig:tunnels} 
\end{figure}




In Section \ref{sec:online learning} below, we will show how to {\em eliminate suboptimal minimizers} by appropriately modifying the graph $G$ -- informally, by adding ``tunnels'' that allow local descent to escape local minimizers and obtain the global optimum. 



\section{Mixed-Order Riemannian Optimization over \texorpdfstring{$\mc M$}{M}} \label{sec:mo}
As described in the previous section, we build an approximate Riemannian optimizer for $\mc M$. Our optimizer operates over a collection of landmarks $\mb q_1, \dots, \mb q_M$. To traverse this set of landmarks, we need to be able to (i) approximate the Riemannian gradient of our objective function $\varphi_{\mb x}$ at a given landmark $\mb q$, and (ii) to choose which landmark $\mb q^+$ to move to next, based on the gradient. The following definition contains the required infrastructure: 

\begin{definition}\label{definition of tangent bundle graph}[Tangent Bundle Graph] A tangent bundle graph $G$ on vertices $V = (1,\dots, M)$ consists of set of undirected first-order edges $E^1 \subseteq V \times V$, where each element is denoted as $u \overset{1}{\leftrightarrow} v$ and \\
{\bf Landmarks $Q$}: $\mb q_i \in \bb R^D$ for each vertex $i = 1, \dots, M$, \\
 {\bf Tangent spaces $T$}: $T_i = \mr{span}(\mb U_i)$ \footnote{We use $T_i$, $T_{\mb q_i}$ interchangeably to denote the tangent space at landmark $\mb q_i$ or vertex $i$. We also use $T_{\mb q_i} \mc M$ when $\mb q_i \in \mc M$.}, with orthonormal basis $\mb U_i \in \bb R^{D \times d}$, at each vertex $i = 1, \dots, M$, \\
 {\bf Edge embeddings $\Xi$}: $\mb \xi_{u\rightarrow v} = \mc P_{T_u} (\mb q_v - \mb q_u) \in T_u$ \footnote{In the language of Riemannian geometry, the $\mb \xi_{u \to v}$ are intended to represent the {\em logarithmic map} $\log_{\mb q_u}(\mb q_v)$.}, for each first-order edge $u\overset{1}{\rightarrow} v \in E^1$, where $\mc P_{T_u}$ denotes the orthogonal projection onto the tangent space $T_u$.
\end{definition}

Based on these objects, we can approximate the Riemannian gradient of $\varphi_{\mb x}$ as 
    $\wh{\mr{grad}}[\varphi_{\mb x}](\mb q_i) = \mc P_{T_i} ( \mb q_i  - \mb x )$.

\paragraph{First-order (Gradient) Steps over the Tangent Bundle Graph.} 


The edge embedding $\mb \xi_{u \to v}$ represents a direction in the tangent space $T_u$ which points from $u$ to $v$, and negative Riemannian gradient $-\wh{\mr{grad}}[\varphi_{\mb x}](\mb q_u)$ at  $\mb q_u$ is our desired direction for movement. A very intuitive update rule is simply to move from $u$ to the vertex $u^+$ which satisfies
\begin{equation}\label{eq:first-order step rule}
    u^+ = \arg\max_{v: u \overset{1}{\rightarrow} v} \innerprod{-\wh{\mr{grad}}[\varphi_{\mb x}](\mb q_u)}{ \mb \xi_{u \to v} }.
\end{equation}



The test-time cost of computing such a gradient step is $O( Dd + d \cdot \mr{deg}^1(u))$. Here, $\mr{deg}^1(u)$ is the degree of the vertex $u$, i.e., its number of first-order neighbors. The $O(Dd)$ term is the cost of computing the Riemannian gradient, while the latter $d \cdot \mr{deg}^1(u)$ is the cost of searching for a neighbor of $u$ which maximizes the correlation in \Cref{eq:first-order step rule}.


\paragraph{Zero-order Edges and Steps.} The gradient method described above efficiently converges to the near-critical point. However, it may get trapped at local minimizers. To ensure global optimality, we extend our first-order method to a {\em mixed-order method} which takes both first-order steps, based on gradient information, and zero-order steps, based on function values. 



We add an additional set of edges $E^0$, which we term {\em zero-order edges} to the graph $G$.  We use the notation\footnote{Throughout the paper, we use $u \overset{0}{\rightarrow}v$ and $\mb q_u \overset{0}{\rightarrow} \mb q_v$ interchangeably to denote the zero-order edge from landmark $\mb q_u$ associated with the vertex $u$ to landmark $\mb q_v$ associated with vertex $v$.} $u \overset{0}{\rightarrow} v$ if $u$ and $v$ are connected by a zero-order edge. As outlined in \cref{algo:MT}, at each step, our mixed order method first attempts a gradient step, by selecting the first-order neighbor whose edge embedding is best aligned with the negative Riemannian gradient. If this step does not lead to a decrease in the objective value, the algorithm then performs a zero-order step, by choosing the zero-order neighbor with smallest objective value: 
\begin{equation}
    u^+ = \arg \min_{v : u \overset{0}{\rightarrow} v} \varphi_{\mb x}(v). 
\end{equation}
This operation requires us to compute the objective function $\varphi_{\mb x}$ at each of the zero-order neighbors $v$ of $u$. Thus, the computational cost is $O( D \, \mr{deg}^0(u) )$. When $D$ and $\mr{deg}^0(u)$ are large, the cost of a zero-order step is significantly larger than that of a first-order step -- this is why our method prioritizes first-order steps. However, zero-order steps are essential to guarantee global optimality. In the next section, we will show how to construct  $G$ to ensure that the mixed-order method converges to a global optimum. 


\section{Learning to Optimize over \texorpdfstring{$\mc M$}{M}}\label{sec:online learning}

The proposed mixed-order method enables efficient navigation of the manifold $\mc M$. However, $\mc M$ is a-priori unknown and only noisy samples are available. In this section, we propose an {\em online learning} method, detailed in \cref{alg:mtn-growth}, that learns a mixed-order Riemannian optimizer directly from noisy data.

Our online learning algorithm produces a set of landmarks $Q=\{\mb q_i\}$, tangent space $T_i$ and edge embeddings $\Xi_i$ at each landmark $\mb q_i$, first-order edges $E^1$ and zero-order edges $E^0$, which have been previously described in Section \ref{definition of tangent bundle graph}.

The algorithm processes incoming data sequentially. For each new noisy data point $\mb x$, we perform mixed-order manifold traversal (\cref{algo:MT}) using the existing traversal network $(Q,T,\Xi,E^0,E^1)$. Manifold traversal outputs a vertex $i$, which corresponds to a landmark $\mb q_i$ which locally minimizes the squared distance $\varphi_{\mb x}(\mb q) = \tfrac{1}{2} \| \mb q - \mb x \|_2^2$. The resulting vertex $i$ is taken as an input to \cref{alg:mtn-growth}.


Depending on $\varphi_{\mb x}(\mb q_i)$, 
we encounter one of three scenarios:



\begin{itemize}
    \item {\em Inlier}: Landmark $\mb q_i$ is sufficiently close to $\mb x$ (i.e., $\| \mb q_i - \mb x \| \le R(i)$). The noisy point $\mb x$ is denoised using the local model at $\mb q_i$ by setting
    \begin{equation}\wh{\mb x} = \mb q_i + P_{T_{\mb q_i}}(\mb x-\mb q_i).
    \end{equation}
    The noisy point is also used to update the local model at $\mb q_i$, by updating both the landmark $\mb q_i$ (as a running average) and tangent space $T_{\mb q_i}$ (using incremental PCA -- see Appendix \ref{sec:ipca_description}. We update the edge embeddings at this landmark by setting $\xi_{ij} = P_{T_{\mb q_i}}(\mb q_j - \mb q_i) \quad \forall i \overset{1}{\rightarrow} j \in E^1$.

    \item If $\| \mb q_i - \mb x \| > R(i)$, we perform exhaustive search, linearly scanning all landmarks to find $\mb q_{i_\star}$, the {\em global} minimizer. Based on $\| \mb q_{i_\star} - \mb x \|$, we distinguish between two cases: 

    \begin{itemize} 
    
    \item {\em $\mb q_i$ is a suboptimal local minimizer}:  
    If $\| \mb q_{i_\star} - \mb x \| \le R(i_\star)$, i.e., $\mb q_{i_\star}$ is close enough to $\mb x$, we build a tunnel $i \overset{0}{\rightarrow} i_{\star}$ from $\mb q_i$ to $\mb q_{i_\star}$, use $\mb q_{i_\star}$ to denoise $\mb x$, and use $\mb x$ to update the local model at $\mb q_{i_\star}$.
    \item {\em $\mb x$ is an outlier}: No existing landmark is sufficiently close to $\mb x$. We make $\mb x$ a new landmark $\mb q_{M}$, and build first-order edges $M \overset{1}{\leftrightarrow} j$ when 
    $\|\mb q_M - \mb q_j \| \leq R_{\mr{nbrs}}$, and initialize a local model at $\mb q_M$.
    \end{itemize} 
\end{itemize}
As more samples are grouped into this landmark, the cumulative effect of noise diminishes, gradually reducing both the landmark’s deviation from the true manifold and the error in its tangent space estimation. The threshold $R(i)$ for accepting inlying data points $\mb x$ is allowed to vary with the number of data points assigned to a given landmark $\mb q_i$ (see Section \ref{sec:experiment} and Appendix \ref{sec:R_i_description}).

By processing one sample at a time, the online learning approach distributes the computational cost of training over time and ensures memory efficiency, enabling it to adapt to large and high-dimensional datasets.


After seeing enough samples, \Cref{alg:mtn-growth} creates a set of landmarks $\mb Q$, which forms a discrete approximation of the manifold $\mc M$, along with a geometric graph that captures both the local geometry of the manifold and its global connectivity (\cref{fig:gw_graph_construction}). This structure enables efficient and accurate navigation for a new noisy sample at test time. 




\begin{algorithm}[tb]
\caption{$\mathtt{ManifoldTraversal}$}\label{algo:MT}
    \begin{algorithmic}
        \STATE \textbf{Input:} Network $G$, $\mb x \in \bb R^D$. 
        \STATE $i \leftarrow 1$ \COMMENT{Initialization}
        \WHILE{not converged}        
        \STATE $\mb g \leftarrow \mb U_i^* (\mb x - \mb q_i)$
        \STATE $i^\sharp \leftarrow \arg \max_{j : i \overset{1}{\rightarrow} j } \innerprod{ \mb g }{ \mb \xi_{ij} }$. 
        \IF{ $\| \mb q_{i^\sharp} - \mb x \| < \| \mb q_i - \mb x \|$ } 
        \STATE $i \leftarrow i^\sharp$
        \ELSE 
        \STATE $i \leftarrow \arg \min_{j : i \overset{0}{\rightarrow} j} \| \mb q_j - \mb x \|$ 
        \ENDIF
    \ENDWHILE
    \STATE \textbf{Output:} $i$
    \end{algorithmic}
\end{algorithm}





\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=0.9\columnwidth]{figs/manifold_plots.pdf}}
\caption{\textbf{Growing Traversal Networks from Data from Synthetic Manifolds.} Growing manifold traversal networks on the Swiss roll and Möbius strip at three different points during early training. First-order edges (blue) connect landmarks and provide first-order approximation to manifolds, and zero-order edges (red) provide tunnels {for global optimality}. Intrinsic dimension $d=2$ and ambient dimension $D=3$.}
\label{fig:growing_graph}
\end{center}
\vskip -0.2in
\end{figure}






\begin{figure}[ht]
\vskip 0.2in
\begin{center}
\begin{tabular}{p{0.28\columnwidth}p{0.28\columnwidth}p{0.28\columnwidth}}
\includegraphics[width=0.28\columnwidth]{figs/gw_graph_construction/1kpts.png} &
\includegraphics[width=0.28\columnwidth]{figs/gw_graph_construction/10kpts.png} &
\includegraphics[width=0.28\columnwidth]{figs/gw_graph_construction/100kpts.png}
\end{tabular}
\caption{{\bf Growing Traversal Networks from Scientific Data.} A 3D visualization of $2048$-dimensional gravitational waves. We show clean samples (orange), landmarks (blue dots), first-order edges (blue), and zero-order edges (green). As the online algorithm sees more points, it learns an increasingly better approximation to the manifold. Number of training points: Left: 1,000. Middle: 10,000. Right: 100,000.}
\label{fig:gw_graph_construction}
\end{center}
\vskip -0.2in
\end{figure}


\begin{algorithm}[tb]
\caption{{$\mathtt{Online Learning For Manifold Traversal}$}} \label{alg:mtn-growth} 
    \begin{algorithmic}
        \STATE {\bfseries Input:} Current network $G$, $\mb x \in \bb R^D$.
        \STATE ${i} \leftarrow {\mathtt{Manifold Traversal}(G,\mb x)}$ 
        \IF{$\| \mb q_{{i}} - \mb x \|_2 \le R(i)$}
        \STATE Denoise via $\wh{\mb x} \leftarrow \mc P_{\mb q_{{i}} + T_{{i}}} \mb x$
        \STATE Update local parameters $q_{{i}},T_{{i}},\Xi_{{i}}$ 
        \ELSE
        \STATE $i_\star \leftarrow \arg \min_{i} \| \mb q_i - \mb x \|_2$ 
        \IF{$\| \mb q_{i_\star} - \mb x \|_2 \le R(i_\star)$} 
        \STATE $\mb q_{{i}}$ is a local min, add a zero-order edge $i \overset{0}{\rightarrow} i_{\star}$
        \STATE Denoise $\mb x$, update $\mb q_{i_\star},T_{i_\star},\Xi_{i_\star}$
        \ELSE 
        \STATE Create a new landmark $\mb q_M \leftarrow \mb x$
        \STATE Connect it to neighboring landmarks within $ R_\text{nbrs}$
        \STATE Initialize local parameters $T_{M},\Xi_{M}$
        \ENDIF
        \ENDIF
        \STATE \textbf{Output:} {$G=(V,E)$}
    \end{algorithmic}
\end{algorithm}












\section{Theoretical Analysis}\label{sec:theory}

Our main theoretical result shows that the proposed mixed-order traversal method rapidly converges to a near-optimal denoised signal. We study the behavior of this method on a noisy input $\mb x = \mb x_\natural + \mb z$, with $\mb x_\natural$ an arbitrary element of $\mc M$, and $\mb z \sim_{\mr{iid}}\mc N(0,\sigma^2)$. Here, the goal is to produce an output $\wh{\mb x} \approx \mb x_\natural$ -- in particular, we would like to achieve $\| \wh{\mb x} - \mb x_\natural \| \lesssim \sigma \sqrt{d}$, which is optimal for small $\sigma$. 

Our analysis assumes access to an accurate collection of landmarks $\mb Q = \{ \mb q_1, \dots, \mb q_M \} \subset \mc M$ and their tangent spaces $T_{\mb q_i}$, as well as appropriately structured first-order and zero-order edge sets $E^1$ and $E^0$ -- in a nutshell, we prove that given an appropriately structured traversal network, mixed-order traversal is both {\em accurate} and highly {\em efficient}, corroborating the conceptual picture in Figure \ref{fig:opt-cover}.

We analyze a particular version of the mixed-order method, which consists of three phases: a first-order Phase I, which, starting from an arbitrary initialization, produces an approximate critical point $\mb q_{i_{\mr{I}}}$, a zero-order Phase II, which jumps to a point $\mb q_{i_{\mr{II}}}$ in a $c\tau_{\mc M}$ neighborhood of the ground truth $\mb x_\natural$, followed by a first-order Phase III, produces a point $\mb q_{i_{\mr{III}}}$ within distance $C \kappa \sigma \sqrt{d}$ of $\mb x_\natural$. This ``$\mathtt{101}$ method'' is stated more detail as Algorithm \ref{algo:1-0-1}.

\begin{figure}[ht]
\centerline{
\begin{tikzpicture}
    % The drawn figure
    \draw [very thick, color=blue] (3.5,1) to [out = -90, in = 135] (3.75,.5) to [out = -40, in = 0] (2.5,-.9) to [out = 180, in = -75] (1.5,.55); 
    \draw [very thick, color=blue] (0.5,1.3) to [out=240,in=25] (-1.2,-.5);    
    \draw [draw=black, fill = red!50!blue, fill opacity=0.1]
       (3.5,0) -- (5,2) -- (0,2) -- (-1.5,0) -- cycle;
    \draw [very thick, color=blue] (4.2,2.5) to [out = -105, in = 90] (3.5,1); 
    \draw [very thick, color=blue] (1.5,.55) to [out = 105, in = -20] (1.05,2.3) to [out = 160, in = 60] (0.5,1.3);
    \filldraw[color=blue] (3.5,1)  circle (3pt); 
    \filldraw[color=red] (1.5,.55) circle (3pt); 
    \filldraw[color=red] (0.5,1.3) circle (3pt);  
    
    % Labels for the figure
    \node at (3.9,1.1) {\color{blue} $\mb q$}; 
    \node at (1.9,.65) {\color{red} $\mb x_1$};
    \node at (.9,1.4) {\color{red} $\mb x_2$};

    % Mathematical content moved below
    \node at (2.5,-1.45) [align=center] {
        \begin{tcolorbox}[colframe=white, colback=white, width=2.5in]
        \centerline{\color{red!50!blue} \bf Normal Space $\mb q + N_{\mb q} \mc M$}
        \end{tcolorbox}
    };
    \node at (2.5,-2.35) {\begin{tcolorbox}[colframe = white, colback = white, width = 2.75in] \color{blue} $\mb q$ is a {\em critical point} of {\color{red} $\varphi_{\mb x_1} = \tfrac{1}{2} \| \mb q - \mb x_1 \|^2$} and {\color{red} $\varphi_{\mb x_2} = \tfrac{1}{2} \| \mb q - \mb x_2 \|^2$} over $\mc M$ \end{tcolorbox}};
\end{tikzpicture}
} 
\vspace{-.25in}
\caption{{\bf Critical Points of the Distance Function.} The point $\mb q$ is a critical point of the distance $\varphi_{\mb x}(\mb q) = \frac{1}{2}\| \mb q - \mb x \|_2^2$ for any point $\mb x$ satisfying $\mb x -\mb q \in N_{\mb q}\mc M$. The {\em vista number} $\$(\mc M)$ bounds the number of $\mb x$ for which this is true -- i.e., the number of $\mb x$ for which $\mb q$ is a local minimizer. This in turn bounds the number of tunnels which must be added to ensure that local descent converges to a global optimizer. In the example illustrated here, $\$(\mc M) = 3$.} \vspace{-.25in}
\label{fig:lochness} 
\end{figure}

\paragraph{Complexity of Escaping Suboptimal Minimizers.} A key element of Algorithm \ref{algo:1-0-1} (and more generally Algorithm \ref{algo:MT}) is the use of {\em zero-order edges} (or tunnels) to escape suboptimal critical points. The complexity of this step of the algorithm is dictated by the number of zero-order edges emanating from the point $\mb q_{i_{I}}$. There is a clear geometric interpretation to this number, which is illustrated in Figure \ref{fig:lochness}: a point $\mb q \in \mc M$ is a critical point of the distance function $\varphi_{\mb x}(\mb q) = \tfrac{1}{2} \| \mb x - \mb q \|_2^2$ if and only if $\mb x -\mb q \in N_{\mb q} \mc M$. Hence, the number of (clean) target points $\mb x_{\natural} \in \mc M$ for which $\mb q$ is a critical point is given by the number of intersections of $\mc M$ with the normal space $\mb q + N_{\mb q}\mc M$. Inspired by the geometry of this picture, we denote this quantity $\$(\mc M)$: 
\begin{equation}
    \$(\mc M) = \max_{\mb q\in \mc M} \# \left[  \mc M \cap (\mb q + N_{\mb q} \mc M) \right].
\end{equation}
Because Phase I of our algorithm produces {\em approximate} critical points, we work with a stable counterpart to this quantity: let $N^\eta_{\mb q} \mc M = N_{\mb q} \mc M + B(0,\eta)$ denote an $\eta$ dilation of the normal space at $\mb q$. We set
\begin{equation}
    \$^\eta_r(\mc M) = \max_{\mb q \in \mc M} \mf N\Bigl( (\mb q + N^{\eta}_{\mb q} \mc M ) \cap \mc M, d_{\mc M}(\cdot, \cdot), r \Bigr) 
\end{equation}
where $\mf N( S, \rho, r )$ denotes the covering number of set $S$ in metric $\rho$ with covering radius $r$. Intuitively, this counts the ``number of times'' the manifold intersects the dilated normal space $N^\eta$. As we will establish in Theorem \ref{thm:main}, this quantity upper bounds the number of zero-order edges at each landmark (i.e. $\mr{deg}^0(\mb q))$ required to guarantee global optimality. 




\vspace{-.1in}

\paragraph{Main Result.} Our main result is as follows:

\begin{theorem} \label{thm:main} 
Let $\mc M \subset \mathbb{R}^D$ be a complete and connected $d$-dimensional manifold whose extrinsic geodesic curvature is bounded by $\kappa$. Assume $\kappa \diam(\mc M) \ge 1$ and $\sigma \sqrt{D} \leq c_1\tau_{\mc M}$.  \\ 
{\bf \em Assumptions on $\mb Q$:} 
    The landmarks $\mb Q=\left\{\mb q_1, \dots, \mb q_{ M}\right\} \subset \mc M$ are $\delta$-separated, and form a $\delta$-net for $\mc M$, under the metric $d_{\mc M}(\cdot, \cdot)$. Assume $\delta \leq \diam{\mc M}$.\\
{\bf \em Assumptions on $E^1$:} First-order graph $E^1$ is defined such that $u \overset{1}{\rightarrow} v \in E^1$ when $\left\|\mb q_u - \mb q_v\right\|_2 \leq R_{\text{nbrs}}$. Assume ${a\delta} = R_{\text{nbrs}} \leq c_2 \sigma \sqrt{d}$ 
for some $a\ge40$. \\
{\bf \em Assumptions on $E^0$:} $E^0$ is a minimal collection of edges satisfying the following covering property: for distinct $\mb q , \mb q'\in Q$, if $\mb q' \in \mb q + N^{\eta}_{\mb q}\mc M$ with $$
    \eta \geq  \epsilon_1 + c_4\sigma\sqrt{d}\sqrt{\kappa\diam(\mc M)+\log(\delta^{-1}\diam(\mc M))},$$ 
    there exists a zero-order edge $\mb q \overset{0}{\rightarrow} \mb q''$ with $\mb q'' \in B_{\mc M}( \mb q', c_5\tau_{\mc M} )$.



With high probability in the noise $\mb z$, \cref{algo:1-0-1} with parameters 
\begin{align} 
R_a &= R_{\mr{nbrs}} - \delta,\\
\eps_1 &> R_{\mr{nbrs}}\Bigl(c_6\kappa \diam(\mc M) + \\
&\quad c_7\kappa\sigma d^{1/2}\sqrt{\kappa \diam(\sM) + \log(\delta^{-1} a\diam(\sM))}\Bigr),\\
\epsilon_2 &> c_8\max\{\kappa,1\} \sigma \sqrt{d},
\end{align}
produces an output $\mb q_\star$ satisfying %\begin{equation}
    $d_{\mc M}(\mb q_\star, \mb x_\natural) \leq 2\epsilon_2$
%\end{equation}
with an overall number of arithmetic operations bounded by 
    {\small \begin{equation}
      O\left((D+e^{c'\log\left(a\right)d})\Bigl(\frac{\diam^2(\mc M)}{\epsilon_1\delta}+\frac{1}{\kappa\delta}\Bigr)d +D \times \$^{\eta}_{c_5 \tau_{\mc M}} (\mc M)\right).
    \end{equation}}
\end{theorem} 

Here, $\tau_{\mc M}$ is the {\em reach} of $\mc M$, i.e., the radius of the largest tubular neighborhood of $\mc M$ on which the projection $\mc P_{\mc M} \mb x$ is unique \cite{federer1959curvature}. The assumption $\sigma \sqrt{D} \le c_1 \tau_{\mc M}$ ensures that with high probability $\| \mb z \| \le \tau_{\mc M}$ ensuring that the projection $P_{\mc M}(\mb x)$ is close to $\mb x_\natural$ in the intrinsic (Riemannian) distance $d_{\mc M}$. 

The extrinsic geodesic curvature $\kappa$ is the supremum of $\| \ddot \gamma \|$ over all unit speed geodesics $\gamma(t)$ on $\mc M$. This quantity measures how ``curvy'' geodesics in $\mc M$ are, in the ambient space $\bb R^D$. Finally, $\mr{diam}(\mc M)$ is the diameter of $\mc M$ in the intrinsic distance $d_{\mc M}$. $\$^{\eta}_{c_5 \tau_{\mc M}}(\mc M)$ upper bounds the number of zero-order edges per landmark, because it's defined as the worst case over $\mc M$, while our assumption on $E^0$ is a minimal collection that only considers the worst case over $Q$.

\paragraph{Interpretation.} This result shows that given an accurate set of landmarks, tangent spaces, and first-order and zero-order graphs, the algorithm converges to a $\max\{\kappa,1\} \sigma \sqrt{d}$ neighborhood of $x_\natural$, which is best achievable up to constant when $\kappa$ is bounded. The algorithm admits an upper bound on the required number of arithmetic operations. $(Dd+e^{c'\log\left(a\right)d} d)$ is the computational cost of taking one first-order step, as $Dd$ comes from projection of the $D$-dimensional gradient into the manifold's $d$-dimensional tangent space, and $d e^{\log(a)d}$ represents the cost of comparing the $d$-dimensional dot product between the embedded gradient with all the first-order neighbors, with number of neighbors bounded by $e^{c'\log\left(a\right)d}$.\\
$(\frac{\diam^2\left(\mc M\right)}{\epsilon_1\delta}+\frac{1}{\kappa\delta})$ 
represent the total number of first order steps taken. $\frac{\diam^2\left(\mc M\right)}{\epsilon_1\delta}$ represents the number of steps on the path from $\mb q_{i^0}$ to $\mb q_{i_{\mr I}}$. Since the initialization could be arbitrarily bad, in this phase we can only guarantee decrease in the value of $\varphi_{\mb x}(\mb q)$, so naturally $\diam^2(\mc M)$ captures the worst case initialization. The ${\epsilon_1\delta}$ represents the minimal decrease in each step: since the landmarks $Q$ forms a $\delta$-net of the manifold, each gradient direction is approximately covered, and each first order step have gradient norm at least $\epsilon_1,$ by definition of the stopping criterion. On the other hand, $\frac{1}{\kappa\delta}$ represents the number of operations from $\mb q_{i_{\mr{II}}}$ to $\mb q_{i_{\mr{III}}}$. By construction $\mb q_{i_{\mr{II}}} \in B_{\mc M}( \mb x_\natural, c_5\tau_{\mc M} )$, so when we consider $d_{\mc M}(\mb x_\natural, \cdot)$ as the new objective function, the worst case initialization is $\frac{1}{\kappa}$ and on this scale each gradient step is guaranteed to walk along the manifold, giving us a $\delta$-decrease in the intrinsic distance to $\mb x_\natural$.

Finally, the $D \times \$^{\eta}_{c_5 \tau_{\mc M}} (\mc M)$ represents the cost of the zero order step from $\mb q_{i_{\mr I}}$, the $\epsilon_1$-approximate critical point for $\varphi_{\mb x}$, to $\mb q_{i_{\mr{II}}}$, the point that lies intrinsically close to $\mb x_\natural.$ $\$^{\eta}_{\tau_{\mc M}} (\mc M)$ is a new geometric quantity that we've defined, and it captures how much $\mc M$ intersects its own dilated normal space. Intuitively larger $\$^{\eta}_{\tau_{\mc M}} (\mc M)$ means one would expect more local minimizes while performing first order descent. Notably  $\$^{\eta}_{\tau_{\mc M}} (\mc M)$ can be exponential in $d$ in the worst case manifold, in which case our algorithm behaves similarly to nearest neighbor search. Intuitively, the parameter $\eps_1$ (and corresponding requirement $\eta$ on the zero order edges) cuts out a tradeoff between the complexity of the first order phases and the complexity of the zero order phase. 






%1-0-1 algorithm box
\begin{algorithm}[tb]
\caption{$\mathtt{101Traversal}$}
    \begin{algorithmic} \label{algo:1-0-1}
    \STATE \textbf{Input:} Network $G$, $\mb x \in \bb \R^D, R_a, \epsilon_1, \epsilon_2$
        \STATE Initialization $i$
        \STATE \COMMENT{Phase $\mr{I}$}
        \WHILE{$\left\|\mc P_{T_i}(\mb x-\mb q_i)\right\|_2 > \epsilon_1$ } 
        \STATE {\small $i \leftarrow \argmin\limits_{j : i \overset{1}{\rightarrow} j} \left\| \mc P_{B(0,R_a)} \mc P_{T_i} (\mb x -\mb q_i) - \mc P_{T_i} (\mb q_j - \mb q_i) \right\|_2$ }
        \ENDWHILE
        \STATE $i_{\mr{I}} \leftarrow i$
        \STATE \COMMENT{Phase $\mr{II}$}
        \STATE $i_{\mr{II}} \leftarrow \arg \min_{j : i_{\mr{I}} \overset{0}{\rightarrow} j} \| \mb q_j - \mb x \|_2$ 
        \STATE $i \leftarrow i_{\mr{II}}$
        \STATE \COMMENT{Phase $\mr{III}$}
        \WHILE{$\left\|P_{T_i}(\mb x-\mb q_i)\right\|_2 > \epsilon_2$}
        \STATE {\small $i \leftarrow \argmin\limits_{j : i \overset{1}{\rightarrow} j} \left\| \mc P_{B(0,R_a)} \mc P_{T_i} (\mb x -\mb q_i) - \mc P_{T_i} (\mb q_j - \mb q_i) \right\|_2$ }
        \ENDWHILE
        \STATE $i_{\mr{III}} \leftarrow i$
        \STATE \textbf{Output:} $\mb q_{i_{\mr{III}}}$
    \end{algorithmic}
\end{algorithm}





\section{Simulations and Experiments}\label{sec:experiment}

In this section, we visualize the traversal networks constructed using \cref{alg:mtn-growth} across synthetic manifolds and high-dimensional scientific data. Our experiments show that denoising performance of \cref{alg:mtn-growth} improves with increased number of training data. Finally, we demonstrate that \cref{alg:mtn-growth} achieves better test-time complexity and accuracy tradeoff compared to Nearest Neighbor over the same set of landmarks.


\paragraph{Visualization of Traversal Network Construction for Various Manifolds.}  The online \cref{alg:mtn-growth} grows the manifold traversal network, processing one sample at a time and \emph{learning tangent spaces and landmarks} in the process. Figure \ref{fig:growing_graph} shows the graph construction process at various snapshots during training for the Swiss roll and Mobius strip which aligns with our intuition that first order edges (blue) captures local tangent information about the manifold and zero order edges (red) are tunnels used to help escape local minimizers. We learn a denoiser on a dataset of 100,000 noisy gravitational waves \cite{abramovici1992ligo, aasi2015advanced} using the \emph{online method} as described in Algorithm \ref{alg:mtn-growth}. Data are $D = 2048$-dimensional, with intrinsic dimension $d=2$, depicted in Figure: \ref{fig:gw_graph_construction}. We refer the reader to the Appendix \ref{sec:data_generation} for data generation details. 




\paragraph{Improvement of Denoising Performance with Streaming Data.}

We measure the performance of our learned denoiser on a dataset of 100,000 noisy gravitational waves. Figure \ref{fig:training_curve} shows  the training error of the learned denoiser. The training error across the first $n$ data points is given by 
\begin{equation} \label{eqn:MSE_train}
    \mr{MSE} = \frac{1}{n} \sum_{i=1}^n \left \| \wh{\mb x}_i - \mb x_{\natural i} \right\|_2^2
\end{equation}
 where $\wh {\mb x}_i$ is the denoised point, and $\mb x_{\natural i}$ is the ground truth. We plot the theoretical lower bound as $\sigma^2 d$ and see that the denoiser error decreases, showing potential to converge to the optimal theoretical lower bound.



\paragraph{Tradeoff Between Test-time efficiency and Denoising Performance.} 


Better denoising usually requires more computation -- models with higher accuracy often come with the cost of increased complexity. We investigate the tradeoffs between performance and complexity, showing that our method significantly improves tradeoffs.

\begin{figure}[ht]
\vskip -0.1in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/training_curve.pdf}}
\caption{\textbf{Training Error:} Training error decreases with the number of training points. We train a denoiser with 100,000 noisy samples with parameters $R(i), R_\text{nbrs}$. The error curve shows the potential to converge to the theoretical optimal $\sigma^2 d$.}
\label{fig:training_curve}
\end{center}
\vskip -0.3in
\end{figure}


After obtaining a traversal network $(Q, T, \Xi, E^0, E^1)$, we set out to compare efficiency-accuracy tradeoffs of nearest neighbor to our mixed-order method over the same set of landmarks $Q$ by the following experimental setup. We measure accuracy in mean squared error (same metric as in \eqref{eqn:MSE_train}) and complexity in number of multiplications. An important parameter in \cref{alg:mtn-growth} is the denoising radius $R(i)$ which controls the complexity by determining the number of landmarks created.

Conceptually, $R(i)$ measures distance between a noisy point $\mb x$ and the the landmark that best describes it. As landmarks are learned throughout \cref{alg:mtn-growth}, their error decreases, requiring $R(i)$ to be reduced accordingly. Hence, we define a general formula for $R(i)$ as follows:
\begin{equation}
    R(i) = \sqrt{\sigma^2D + \frac{\sigma^2 D}{N_i}+\sigma^2d}
\end{equation}
\noindent where the first error term $\sigma^2 D$ comes from noisy points $\mb x_i$, and the second term $\sigma^2 D / N_i$ comes from the fact that there is distance between landmarks and the true manifold. Initially, a landmark $\mb q_i$ is created using one noisy point $\mb x$. As more and more points are used to update landmark $\mb q_i$ and other local parameters at vertex $i$, local approximation gets more and more accurate, and the distance between the landmark and true manifold should decrease. This is why we divide the error $\sigma^2 D$ by  $N_i$, the number of points used to update landmark $\mb q_i$ and other local parameters, making $R(i)$ smaller. Lastly, $\sigma^2 d$ term comes from the error $\|P_{\mc M} \mb x - \mb x_\natural\|_2^2$ across the manifold $\mc{M}$. We provide more detail in Appendix \ref{sec:R_i_description}.


Figure \ref{fig:complexity_accuracy} summarizes test-time accuracy versus complexity of the proposed mixed-order method, comparing it to nearest neighbor search on the same set of landmarks. We do this comparison based on a test set of $5,000$ noisy points. By varying $R(i)$ and $R_{\mr{nbrs}}$ (specific values detailed in Tables \ref{complexity_accuracy_table}
and \ref{parameter_choices_table} of Appendix), we obtain twelve different networks $\{(Q_i, T_i, \Xi_i, E^0_i, E^1_i)\}_{i=1}^{12}$. We compare our method of network $i$ with Nearest Neighbor search over the same set of landmarks $Q_i$. The results are shown in Figure \ref{fig:complexity_accuracy}, where blue points represent manifold traversal and red points represent nearest neighbor. It is clear that manifold traversal achieves significantly better tradeoffs compared to nearest neighbor search over the same set of landmarks. Moreover, decreasing $R(i)$ with the number of datapoints $N_i$ assigned to it results in better accuracy as opposed to keeping it constant, as seen in the figure. With $R(i)$ decreasing, the tradeoff advantage of our method is even more evident, which manifests itself in a large gap between red and blue cross symbols in Figure \ref{fig:complexity_accuracy}.







\begin{figure}[ht]
\vskip 0in
\begin{center}
\centerline{\includegraphics[width=0.7\columnwidth]{figs/complexity_accuracy/complexity_accuracy_tradeoff.pdf}}
\caption{\textbf{Test-Time Complexity-Accuracy Tradeoff of Mixed-order Method versus Nearest Neighbor.} Over a test set of 5,000 noisy points, our proposed mixed-order method achieves better tradeoffs compared to nearest neighbor search over the same set of landmarks.}
\label{fig:complexity_accuracy}
\end{center}
\vskip -0.3in
\end{figure}





\section{Conclusions}\label{sec:conclusion}
Our work introduces a novel framework for test-time efficient and accurate manifold denoising in scenarios where the manifold is unknown and only noisy samples are given. The framework incorporates an {\em online learning} method to construct an augmented graph, facilitating the optimization on the approximated manifold, and a {\em mixed-order} method that ensures both efficient traversal and global optimality. Our experiments on scientific manifolds demonstrate that the proposed methods achieve a superior complexity-accuracy tradeoff compared to nearest neighbor search, which is the core of many existing provable denoising approaches. Furthermore, our analyses show that the mixed-order method attains near-optimal denoising performance, assuming the online learning method produces an ideal graph, and we provide complexity analyses for the mixed-order method under this assumption. 

A promising future direction is to establish theoretical guarantees for the accuracy of the landmarks generated by the online learning method, as they play a crucial role in denoising performance. The current learning method dynamically builds edges in the graph as needed. Another potential avenue for future research is to develop a sparser network using pruning techniques while maintaining global optimality, which could further improve test-time efficiency. More broadly, we aim to leverage this designed method to study the traversal properties of natural datasets across a wide and diverse range of datasets. Additionally, integrating this method as a denoiser block within signal generation and reconstruction architectures could be a valuable direction, potentially accelerating the entire process.






\newpage 

\newpage
\section*{Impact Statement}

This paper presents work whose goal is to advance the field of 
Machine Learning. There are many potential societal consequences 
of our work, none which we feel must be specifically highlighted here.




\bibliography{refs}
\bibliographystyle{icml2025}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\appendix
\onecolumn



\input{101_Analysis}


\end{document}