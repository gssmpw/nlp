

\paragraph{Preliminiaries on the logarithmic map.}

{
The following sequence of lemmas provides an upper bound on the number of landmarks $|Q|$, under the assumption that the landmarks are $\delta$-separated. Our argument will assume that the manifold $\mc M$ is {\em connected} and {\em geodesically complete}. Under these assumptions, the exponential map 
\begin{equation}
    \exp_{\mb x_\natural}(\cdot) : T_{\mb x_\natural} \mc M \to \mc M
\end{equation}
is surjective, i.e., for every $\mb q \in \mc M$, there exists $\mb v \in T_{\mb x_\natural}\mc M$ such that 
\begin{equation} \label{eqn:v-q}
    \exp_{\mb x_\natural}(\mb v) = \mb q.
\end{equation} 
Moreover, by the Hopf-Rinow theorem, there exists a length-minimizing geodesic joining $\mb x_\natural$ and $\mb q$, and hence there exists $\mb v \in T_{\mb x_\natural} \mc M$ of norm $\| \mb v \| = d_{\mc M}(\mb x_\natural, \mb q )$ satisfying \eqref{eqn:v-q}. In particular, for every $\mb q \in \mc M$, there exists $\mb v \in T_{\mb x_\natural} \mc M$ of norm at most $\| \mb v \| \le \mr{diam}(\mc M)$ satisfying \eqref{eqn:v-q}. 

The {\em logarithmic map}
\begin{equation}
    \wt{\log}_{\mb x_\natural}  : \mc M \to T_{\mb x_\natural} \mc M
\end{equation}
is defined, in the broadest generality, as the inverse of the exponential map. This mapping can be multi-valued, since for a given $\mb q$ there may be multiple tangent vectors $\mb v$ satisfying \eqref{eqn:v-q}. Notice that because $\exp$ is surjective, its inverse, $\wt{\log}$ is well defined for all $\mb q \in \mc M$. When $d_{\mc M}(\mb x_\natural,\mb q) \le r_{\mr{inj}}$ is smaller than the injectivity radius of the exponential map at $\mb x_\natural$ \footnote{$\text{inj}(\mb x_\natural) = \sup \{r >0 :\exp_{\mb x_\natural} \text{is a diffeomorphism on} B(0,r) \subset T_{\mb x_\natural}\mc M\}$}, there is a unique minimum norm element $\mb v_\star$ of the set $\wt{\log}_{\mb x_\natural}(\mb q)$. This is typically denoted 
\begin{equation}
    \log_{\mb x_\natural}(\mb q)
\end{equation}
and satisfies $\| \mb v_\star \| = d_{\mc M}(\mb x_\natural, \mb q )$.\footnote{This is often taken as the {\em definition} of the logarithmic map.} We can extend this notation from $\mb q \in B_{\mc M}(\mb x_\natural, r_{\mr{inj}})$ to all of $\mc M$, by letting 
\begin{equation}
    \log_{\mb x_\natural}(\mb q)
\end{equation}
denote a minimum norm element of the set $\wt{\log}_{\mb x_\natural}(\mb q)$, chosen arbitrarily in the case that there are multiple minimizers.\footnote{This selection is possible thanks to the axiom of choice.} With this choice, $\log_{\mb x_\natural}(\mb q)$ is well-defined, single-valued over all of $\mc M$, and defines a mapping 
\begin{equation}
    \log_{\mb x_\natural} : \mc M \to B_{T_{\mb x_\natural} \mc M} \Bigl( \mb 0, \mr{diam}(\mc M) \Bigr ) 
\end{equation}
Our analysis will assume that the landmarks $Q$ are $\delta$-separated on $\mc M$, i.e., $d_{\mc M}(\mb q_i, \mb q_j) \ge \delta$ for all $i \ne j$. We will show under this assumption that $\log_{\mb x_\natural}(\mb q_i)$ and $\log_{\mb x_\natural}(\mb q_j)$ are  $\delta'$-separated, albeit with a radius of separation $\delta'$ which could be significantly smaller than $\delta$. 

This argument makes heavy use of properties of geodesic triangles -- in particular, Toponogov's theorem, which compares side lengths of geodesic triangles in spaces of {\em bounded} sectional curvature to side lengths of triangles in spaces of {\em constant} sectional curvature. Our argument uses the following properties of the mapping $\log$ defined above: 
\begin{itemize}
    \item {\em Inverse Property}: for $\mb v = \log_{\mb x_\natural}(\mb q)$, $\exp_{\mb x_\natural}(\mb v) = \mb q$
    \item {\em Minimum Norm Property}: $\| \log_{\mb x_\natural}(\mb q) \| = d_{\mc M}(\mb x_\natural, \mb q) \le \mr{diam}(\mc M)$.
\end{itemize}
Our analysis {\em does not} require analytical properties of the logarithmic map, such as continuity, which do not obtain beyond the injectivity radius of the exponential map. 


}



\begin{lemma}\label{lemma:tangent space seperation}
    For any $R > 0$, and any $\delta$-separated pair of points $\mb q, \mb q' \in \mb B_{\mc M}(\mb x_\natural, R)$ (i.e., pair of points satisfying $d_{\mc M}(\mb q,\mb q') \geq \delta$), we have 
    \begin{align}
        \norm{\log_{\target}\vq - \log_{\target}\vq'} \ge \frac{\sqrt{2}}{4}\exp(-\kappa R)\delta. 
    \end{align}
\end{lemma}


\begin{proof}
    We will prove this claim by applying Toponogov's theorem, a fundamental result in Riemannian geometry. Toponogov's theorem is a comparison theorem for triangles, which allows us to compare side lengths of geodesic triangles in an arbitrary manifold of bounded sectional curvature to the side lengths of geodesic triangles in a model space of {\em constant} sectional curvature. 
    From Lemma 10 in \cite{yan2023tpopt}, the sectional curvatures of $\mc M$ are bounded from below by the extrinsic curvature $\kappa$, i.e.,
    \begin{equation}
        \kappa_s \geq -\kappa^2.
    \end{equation}
    Our plan is as follows: form a geodesic triangle $\triangle(\mb x, \mb q_{-\kappa^2}, \mb q'_{-\kappa^2} )$ in the model $M_{-\kappa^2}$ with constant section curvature $-\kappa^2$, whose side lengths satisfy
    \begin{equation}
        d_{\mc M_{-\kappa^2}}(\mb x, \mb q_{-\kappa^2}) = d_{\mc M}(\mb x_\natural, \mb q), \qquad d_{\mc M_{-\kappa^2}}(\mb x, \mb q'_{-\kappa^2}) = d_{\mc M}(\mb x_\natural, \mb q'),
    \end{equation}
    and whose angle satisfies 
    \begin{equation}
    \angle(\mb q_{-\kappa^2},\mb x, \mb q'_{-\kappa^2}) = \angle(\mb q,\mb x_\natural, \mb q')
    \end{equation} 
    Then by Toponogov's theorem, the third sides of these pair of triangles satisfy the inequality 
    \begin{equation}
    d_{\mc M}(\mb q,\mb q') \le d_{\mc M_{-\kappa^2}}( \mb q_{-\kappa^2}, \mb q'_{-\kappa^2} ), \label{eqn:topo}
    \end{equation} 
    i.e., the third side in the constant curvature model space is larger than that in $\mc M$. 

    We construct the triangle $\triangle( \mb x, \mb q_{-\kappa^2}, \mb q'_{-\kappa^2} )$ more explicitly as follows: fix a arbitrary base point $\mb x \in  M_{-\kappa^2}$. Let $\mb v, \mb v'$ be two distinct tangent vectors in the tangent space $T_{\mb x}\mc M_{-\kappa^2}$ satisfying 
\begin{equation}
    \|\mb v\|_2 = \|\log_{\mb x_\natural} \mb q\|_2, \;\;
    \|\mb v'\|_2 = \|\log_{\mb x_\natural} \mb q'\|_2,
\end{equation}
and $\theta = \angle(\log_{\mb x_\natural} \mb q, \log_{\mb x_\natural} \mb q')=\angle(\mb v, \mb v')$. Set $\mb q_{-\kappa^2} = \exp_{\mb x}(\mb v) \in \mc M_{-\kappa^2}, \mb q_{-\kappa^2}' = \exp_{\mb x}(\mb v') \in \mc M_{-\kappa^2}$. 

Notice that $\| \mb v - \mb v' \| = \| \log_{\mb x_\natural} \mb q - \log_{\mb x_\natural} \mb q' \|$. We would like to {\em lower bound} this quantity. From \eqref{eqn:topo} and the fact that $d_{\mc M}(\mb q,\mb q') \ge \delta$, we have $d_{\mc M_{-\kappa^2}}( \mb q_{-\kappa^2}, \mb q'_{-\kappa^2} ) \ge \delta$, and the task becomes one of lower bounding $\| \mb v - \mb v' \|$ in terms of this quantity. To facilitate this bound, we move $\mc M_{-\kappa^2}$ to hyperbolic 
space $\mc M_{-1}$, where we can apply standard results from hyperbolic trigonometry, by scaling all side lengths by $\kappa$. Namely, form a third geodesic triangle in $\mc M_{-1}$, by taking an arbitrary $\mb x_{-1} \in \mc M_{-1}$, choosing $\mb v_{-1}, \mb v'_{-1} \in T_{\mb x_{-1}} \mc M_{-1} $ with $\angle( \mb v_{-1}, \mb v'_{-1} ) = \theta$ and $\| \mb v_{-1} \|_2 = \kappa \| \mb v \|$, $\| \mb v'_{1} \|_2 = \kappa \| \mb v' \|$. As above, set $\mb q_{-1} = \exp_{\mb x_{-1}}(\mb v_{-1})$, and $\mb q'_{-1} = \exp_{\mb x_{-1}}(\mb v'_{-1})$. Then 
\begin{equation}
    d_{\mc M_{-1}}(\mb q_{-1},\mb q'_{-1}) = \kappa d_{\mc M_{-\kappa^2}}( \mb q_{-\kappa^2}, \mb q'_{-\kappa^2} ). 
\end{equation}
Moreover, 
\begin{equation}
    \| \mb v_{-1} - \mb v'_{-1} \| = \kappa \| \mb v - \mb v' \|. 
\end{equation}


\noindent For compactness of notation, let $L'$ denote the third sidelength of $\triangle_{-1}$, 
\begin{equation}
    L' =  d_{\mc M_{-1}}(\mb q_{-1},\mb q'_{-1})  = \kappa *  d_{\mc M_{-\kappa^2}}(\mb q_{-\kappa^2}, \mb q_{-\kappa^2}').
\end{equation}
The lengths of the other two side are $a =\kappa* \|\mb v\|_2 \leq \kappa R, b=\kappa* \|\mb v'\|_2 \leq \kappa R$, and angle between these two sides is equal to $\theta$. In the corresponding Euclidean triangle on the tangent space, we also have the two sides are of length $a$ and $b$, and the third side has length
\begin{equation}
   L = \kappa * \|\mb v -\mb v'\|_2. 
\end{equation}


As $\sM_{-1}$ is hyperbolic, from hyperbolic law of cosines, we have
\begin{equation}
    \cosh L' = \cosh a \cosh b - \sinh a \sinh b \cos \theta.
\end{equation}
From the fact that $\cosh(a - b) = \cosh a \cosh b - \sinh a \sinh b$, we could further get
\begin{equation}
    \begin{aligned}
        \cosh L' = \cosh(a-b) + \sinh a \sinh b (1 - \cos \theta).
    \end{aligned}
\end{equation}
Since $\sinh t $ is convex over $t \in [0,\infty)$, we have $\sinh t \leq \frac{t}{\kappa R} \sinh (\kappa R)$ for $t \le \kappa R$, hence $\sinh a \sinh b \leq \frac{ab}{(\kappa R)^2} \sinh^2(\kappa R)$. And $\cosh (a-b) = \cosh|a-b| \leq \cosh L$. 
Then we have 
\begin{equation}
    \cosh L' \leq \cosh L + \frac{ab}{(\kappa R)^2} \sinh^2(\kappa R)(1 - \cos \theta)
\end{equation}

From the law of cosines applying on Euclidean triangle with length of two sides $a,b$ and the length of the third side $L$, we know
\begin{equation}\label{eq:relationship between L and L'}
    \begin{aligned}
        L^2 
        &= a^2 + b^2 - 2ab \cos \theta\\
        &\geq 2ab(1-\cos\theta)\\
        &\geq 2ab \frac{(\cosh L' - \cosh L)(\kappa R)^2}{ab\sinh^2(\kappa R)}\\
        &= 2(\kappa R)^2 \frac{(\cosh L' - \cosh L)}{\sinh^2(\kappa R)}
    \end{aligned}
\end{equation}
Since $d_{\mc M}(\mb q,\mb q')\geq \delta$, then $L' = \kappa *  d_{\mc M_{-\kappa^2}}(\mb q_{-\kappa^2}, \mb q_{-\kappa^2}') \geq \kappa * d_{\mc M}(\mb q,\mb q')\geq \kappa\delta$. Then equation \eqref{eq:relationship between L and L'} implies 
\begin{align}\label{eq:app:tangent_space_sep_L_bound_1}
    L^2 + \frac{2(\kappa R)^2}{\sinh^2(\kappa R)}\cosh L \geq \frac{2(\kappa R)^2}{\sinh^2(\kappa R)} \cosh \kappa\delta.
\end{align}
\vspace{.1in}\\
By triangle inequality, we know $L \leq a + b \leq 2\kappa R$. From the mean value form of the Taylor series, we have $\sinh(\kappa R) = \kappa R \cosh(r_1)$ for some $0 \le r_1 \le \kappa R$ and 
\begin{align}
    \cosh L &\le 1 + \frac{L^2}{2} \cosh(r_2)
\end{align}
for some $0 \le r_2 \le L$. Multiply equation \eqref{eq:app:tangent_space_sep_L_bound_1} both side by $\frac{\sinh^2(\kappa R)}{2(\kappa R)^2}$ and plug in the value above, we get
\begin{align}
\cosh \kappa\delta &\le \frac{\sinh^2(\kappa R)}{2(\kappa R)^2}L^2 + \cosh L \\
&\le \frac{\cosh^2(r_1)}{2}L^2 + \paren{1 + \frac{L^2}{2}\cosh(r_2)}.
\end{align}
Rearrange the terms, we get
\begin{align}
    L^2 &\ge 2\paren{\cosh^2(r_1) + \cosh(r_2)}^{-1} \paren{\cosh(\kappa \delta) - 1} \\
    &\ge 2\paren{\cosh^2(\kappa R) + \cosh(2\kappa R)}^{-1} \frac{\kappa^2 \delta^2}{2} \\
    &\ge \frac{1}{2} \exp(-2\kappa R) \kappa^2 \delta^2.\\
\end{align}
where from the first to second line we used that $\cosh(t) \ge 1 + \frac{t^2}{2}$. 
As a result, we have $\norm{\log_{\target}\vq - \log_{\target}\vq'} = \norm{\vv - \vv'} = L/\kappa \ge \frac{\sqrt{2}}{2}\exp(-\kappa R)\delta \ge \frac{\sqrt{2}}{4}\exp(-\kappa R)\delta$.
\end{proof}

\begin{lemma}\label{lemma:bound of number of landmarks within R ball}
    Consider $\mb x_\natural \in \mc M$ and let $\mc Q_{R} = \{\mb q: \mb q\in  B_{\mc M}(\mb x_\natural, R) \cap Q\}$. Then the number of landmarks within $R$ ball centering at $\mb x_\natural$, i.e. $|\mc Q_{R}|$, satisfies 
    \begin{equation}
    |\mc Q_{R}| 
    \leq \left( 1 + 4\sqrt{2} R \e^{\kappa R}/ \delta\right)^d, \forall R > 0.
    \end{equation}
    In particular, we have
    \begin{align}\label{eq:app:Q_size_bound}
        \abs{\sQ} \le \paren{1 + 4\sqrt{2}\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)} }^d 
    \end{align}
\end{lemma}
\begin{proof}
    For every $\mb q \in \mc Q_{R}$, there is a {\em unique} tangent vector $\log_{\mb x_\natural} \mb q \in T_{\mb x_\natural} \mc M$. Now we define the set $\mc V_{R} = \{\log_{\mb x_\natural} \mb q \in T_{\mb x_\natural} \mc M \quad \forall \mb q \in \mc Q_{R}\}$. Then the number of landmarks within the intrinsic ball $\mb B_{\mc M}(\mb x_\natural, R)$ is $|\mc Q_{R}| = |\mc V_{R}|$.
    \vspace{.1in}\\
    Let $\delta_R = \frac{\sqrt{2}}{4}\exp(-\kappa R)\delta$. From Lemma \ref{lemma:tangent space seperation}, we know that $\mc V_{R}$ forms a $\delta_R$-separated subset of $B(0,R)$ on $T_{\mb x_\natural} \mc M$. And for any $\log_{\mb x_\natural}\mb q\in \mc V_{R}$, we have $\left\| \log_{\mb x_\natural}\mb q\right\|_2 = d_{\mc M}(\mb x_\natural, \mb q) \leq R$. Then it's natural to notice that $|\mc V_{R}| \leq P(B(0,R), \delta_R)$, where $P(B(0,R), \delta_R)$ is the largest cardinality of a $\delta_R$-separated subset of $B(0,R) \in T_{\mb x_\natural} \mc M$.
    \vspace{.1in}\\
    Since $P(B(0,R),\delta_R)$ is the largest number of closed disjoint balls with centers in $B(0,R)$ and radii $\delta_R$/2, then by volume comparison, we have
    \begin{equation}
         P(B(0,R),\delta_R) * \text{vol}(B_{\delta_R/2}) \leq \text{vol}(B_{\delta_R/2+R})
    \end{equation}
Then we will have $|\mc V_{R}| \leq P(B(0,R), \delta_R) \leq \left( 1+\frac{2R}{\delta_R}\right)^d$ which gives the bound we need. 

To bound $\sQ$, we can simply take $R = \diam(\sM)$ and notice $\abs{\sQ} =  \abs{\sQ_{\diam(\sM)}}$. 
\end{proof}



\begin{lemma}\label{lem:app:foe_bound}

    Let $\mc M \in \mathbb{R}^D$ be a complete $d$-dimensional manifold. Suppose the set of landmarks $Q = \{\mb q\} \subset \mc M$ forms a $\delta$-net for $\mc M$, and first -order edges $E^1$ satisfies that $u \overset{1}{\rightarrow} v \in E^1$ if $\|\mb q_u - \mb q_v\|_2 \leq R_{\text{nbrs}}$, where $R_{\text{nbrs}} \leq \tau_{\mc M}$, and $\tau_{\mc M}$ is the reach of the manifold. Assume $\delta \leq \diam{\mc M}$, and  $\kappa \diam(\sM) \ge 1$. Then the number of first-order edges $\abs{E^1}$ satisfies
    \begin{align}
        \log \abs{E^1} &\le \paren{\kappa \diam(\sM) + \log(a) - \log(\delta) + \log(\diam(\sM)) + \log(100)}d. 
    \end{align}
\end{lemma}
\begin{proof}
From construction, we have 
\begin{align}
    \abs{E^1} \le \abs{Q} \max_{\vq_u \in \sQ}\abs{E_{u}^1},
\end{align} 
where $E^1_u$ denotes the first-order edges at landmark $\mb q_u$.
As we have $\firstER \le \reach \le 1/\kappa$, following \cref{lemma:bound of number of landmarks within R ball} and \cref{eq:app:Q_size_bound}, we get
\begin{align}
    \abs{E^1} 
    &\le \paren{1 + 4\sqrt{2}\delta^{-1} \firstER e^{\kappa \firstER} }^d \paren{1 + 4\sqrt{2}\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)} }^d \\
    &= \paren{1 + 4\sqrt{2}a e^{\kappa \firstER} }^d \paren{1 + 4\sqrt{2}\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)} }^d 
\end{align}



We recall that $a = \firstER / \delta \ge 40$. Since $e^{\kappa \firstER} \ge 1$, we have $\frac{1}{40} a e^{\kappa \firstER} \ge 1$, which means  $1 + 4\sqrt{2}a e^{\kappa \firstER} \le (\frac{1}{40} + 4\sqrt{2})a e^{\kappa \firstER} \le (\frac{1}{40} + 4\sqrt{2})a\cdot e$, since $\kappa \firstER \le 1$. Similarly, since $\delta \leq \diam{\mc M}$, and $\kappa \diam(\sM) \ge 1$, $1 + 4\sqrt{2}\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)} \le (\frac{1}{e} + 4\sqrt{2})\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)}$, which gives 
\begin{align}
    \abs{E^1} 
    &\le\paren{(\frac{e}{40} + 4\sqrt{2}e)a}^d \paren{(\frac{1}{e} + 4\sqrt{2})\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)}}^d \\ 
    &\le\paren{(\frac{e}{40} + 4\sqrt{2}e)(\frac{1}{e} + 4\sqrt{2})}^d \paren{a\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)}}^d \\
    &\le\paren{100}^d \paren{a\delta^{-1} \diam(\sM) e^{\kappa \diam(\sM)}}^d
\end{align}

Taking the log,  we get
\begin{align}
    \log \abs{E^1} &  \le \paren{\kappa \diam(\sM) + \log(a) - \log(\delta) + \log(\diam(\sM)) + \log(100)}d.    
\end{align}  


\end{proof}





{
\begin{lemma}\label{lem:upper bound for Tmax} There exists a numerical constant $C$ such that with probabilty at least $1 - e^{-9d/2}$, 
    \begin{equation}
    T_{\max} = \sup \, \Bigl\{ \, \innerprod{ \mb v }{\mb z } \, \mid  \begin{array}{l} d_{\mc M}(\mb y, \mb x_\natural) \le 1/\kappa, \\ \mb v \in T_{\mb y} \mc M, \, \| \mb v \|_2 = 1 \end{array} \Bigr\}. 
\end{equation}
satisfies 
\begin{equation}
    T_{\max} 
    \leq C \max\{ \kappa, 1 \} \times \sigma \sqrt{d}.
\end{equation}
\end{lemma}

\begin{proof}
Let $\bar{\kappa} = \max \{ \kappa, 1 \}$. From Theorem 10 of \cite{yan2023tpopt}, we have that  with probability at least $1-e^{-\frac{x^{2}}{2\sigma^{2}}}$,  
\begin{equation}
    T_{\max} \le 12\sigma\left(\bar{\kappa} \sqrt{2\pi(d+1)}+\sqrt{\log12\bar{\kappa}}\right)+x. 
\end{equation}
We note that there exists numerical constants $C_1, C_2$, such that $\sqrt{\log 12\bar{\kappa}} \leq C_1 \bar{\kappa}$, and $\sqrt{d+1}\leq C_2 \sqrt{d}$. Setting $x = 3\sigma\sqrt{d}$, then we have
\begin{equation}
    \begin{aligned}                         
       12\sigma(\bar{\kappa}\sqrt{2\pi(d+1)}+\sqrt{\log12\bar{\kappa}})+x
        &\leq 12\sigma\bar{\kappa}\sqrt{2\pi} C_2 \sqrt{d} + \sigma C_1 \bar{\kappa} + 3\sigma\sqrt{d}\\
        &=\left( 12\bar{\kappa}\sqrt{2\pi} C_2 + C_1 \bar{\kappa} +3\right) \sigma \sqrt{d}
    \end{aligned},
\end{equation}
yielding the claimed bound. 

\end{proof}
}




{
\begin{lemma}\label{lem:lower bound for Tmax}
    Let
    \begin{equation}
    T_{\max} = \sup \, \Bigl\{ \, \innerprod{ \mb v }{\mb z } \, \mid  \begin{array}{l} d_{\mc M}(\mb y, \mb x_\natural) \le 1/\kappa, \\ \mb v \in T_{\mb y} \mc M, \, \| \mb v \|_2 = 1 \end{array} \Bigr\}. 
\end{equation}
Then with probability at least $1 - e^{-\frac{t^2}{2\sigma^2}}$, we have 
\begin{align}
    T_{\max} \ge \sigma \sqrt{d/2} - t. 
\end{align}
\end{lemma}
}

\begin{proof}
Since $T_{\max}$ is the supremum of a $1$-Lipschitz function and is therefore $1$-Lipschitz in $\mb{z}$, it follows that
\begin{equation}\label{eq:tmax_lb_lip}
    \prob{T_{\max} \leq \bb E[T_{\max}] - t} \le e^{-t^2/ 2 \sigma^2}.
\end{equation}
By setting $\vy = \target$ and $\vv = \sP_{\tangent{\target}{\sM}}\vz$ we obtain \(
    T_{\max} \geq \left\| \sP_{T_{\mb x_\natural} \mc M} \mb z\right\|_2 \) and thus \(\bb E[T_{\max}] \geq \bb E\left[\left\| \sP_{T_{\mb x_\natural} \mc M} \mb z\right\|_2\right]\). 

Since $\vz$ is $\iid$ Gaussian with variance $\sigma^2$, the rotational invariance of Gaussian distributions implies that $\E{\norm{\sP_{T_{\mb x_\natural} \mc M} \mb z}} = \E{\norm{\sigma \vg_d}}$, where $\vg_d \sim \sN(0, \vI_d)$ is a $d$ dimensional standard Gaussian vector. 
As $\norm{\vg_d}$ is $1$-Lipschitz, from \cref{lem:app:lipschitz_bounded_variance} we have
\begin{align}
    1 \ge \Var{\norm{\vg_d}} = \E{\norm{\vg_d}^2} - \E{\norm{\vg_d}}^2 = d - \E{\norm{\vg_d}}^2
\end{align}
and thus $\E{\norm{\vg_d}} \ge \sqrt{d - 1}$. 
Therefore, 
\begin{align}
    \E{\norm{\sP_{T_{\mb x_\natural} \mc M} \mb z}} = \E{\sigma \norm{\vg_d}} \ge \sigma \sqrt{d - 1} \ge \sigma \sqrt{d/2} , \quad \text{for } d \geq 2. 
\end{align}

For the case where $d = 1$, we compute directly:
\begin{align}
    \E{\norm{\sP_{T_{\target} \sM}}} &= \sqrt{2} \sigma \frac{\Gamma((d+1)/2)}{\Gamma(d/2)} = \sqrt{2/\pi} \sigma \ge \sqrt{d/2} \sigma. 
\end{align}
Combining the cases for $d \geq 2$ and $d = 1$, and substituting into \cref{eq:tmax_lb_lip}, we conclude that
\begin{equation}
        T_{\max} \geq \sigma \sqrt{d / 2} - t \quad \text{with probability at least } 1 - e^{-t^2 / 2\sigma^2}.
    \end{equation}
\end{proof}

\begin{lemma}[Bounded Variance of $1$-Lipschitz Function]\label{lem:app:lipschitz_bounded_variance}
    Let $\vg \sim \sN(\mb 0, \vI)$ be a standard Gaussian, $f$ is a $1$-Lipschitz function, then we have
    \begin{align}
        \Var{f(\vg)} \le 1.  
    \end{align}
\end{lemma}
\begin{proof}
    In the prove, we utilize the Gaussian Poincar\'e inequality \cite{boucheron2003concentration}[Theorem 3.20] which says that
\begin{align}
    \Var{h(\vg)} \le \E{\norm{\nabla h(\vg)}^2}
\end{align}
for any $C^1$ function $h$. Let $\rho_{\eps}$ be the standard Gaussian mollifier \(\rho_{\eps}(\vz) = \frac{1}{(2\pi\eps)^d}e^{-\frac{\norm{\vz}^2}{2\eps}}\)
and let \[f_{\eps} = f \ast \rho_{\eps} = \int_{\vz} f(\vx - \vx)\rho_{\eps}(\vz) d\vz,\]then $f_{\eps}$ is smooth. As
\begin{align}
    \abs{f_{\eps}(\vx) - f_{\eps}(\vy)} &= \abs{\int_{\vz} \paren{f_{\eps}(\vx - \vz) - f_{\eps}(\vy - \vz)}\rho_{\eps}(\vz)d\vz} \\
    &\le \int_{\vz} \abs{\paren{f_{\eps}(\vx - \vz) - f_{\eps}(\vy - \vz)}}\rho_{\eps}(\vz)d\vz\\
    &\le \norm{\vx - \vy} \int_{\vz}\rho_{\eps}(\vz)d\vz = \norm{\vx - \vy},
\end{align}
$f_{\eps}$ is also $1$-Lipschitz. 
Following the  Gaussian Poincar\'e inequality we have
\begin{align}
    \Var{f_{\eps}(\vg)} &\le \E{\norm{\nabla f_{\eps}(\vg)}^2} \le 1.
\end{align}
To conclude the result, we need to show the interchangeability of the interation and the limit. As $f_{\eps}$ is $1$-Lipschitz, we have 
\begin{align}
    \abs{f_{\eps}(\vx)}^2 &\le \norm{f_{\eps}(0)} + \norm{\vx} \\
    &\le \abs{\int_{\vz} f(-\vz) \rho_{\eps}(\vz)d\vz} + \norm{\vx}\\
    &\le \int_{\vz} \norm{\vz}\rho_{\eps}(\vz)d\vz + \norm{\vx} \\
    &= \sqrt{\eps} \E{\norm{\vg}} + \norm{\vx}
\end{align}
As the moments of a standard Gaussian are upper bounded, $\abs{f_{\eps}(\vg) - \E{f_{\eps}(\vg)}}^2$ can be uniformly upper bounded by some integrable function for all $\eps \le 1$. And thus
\begin{align}
    \Var{f[\vg]} &= \int_{\eps \to 0} \Var{f_{\eps}(\vg)} \le 1. 
\end{align}
\end{proof}

\newpage 












\section{Experimental Details}

\subsection{Gravitational Waves Data Generation} \label{sec:data_generation}

We generate synthetic gravitational waveforms with the PyCBC package \cite{nitz2023gwastro} with masses drawn from a Gaussian distribution with mean 35 and variance 15. We use rejection sampling to limit masses to the range [20, 50]. Each waveform is sampled at 2048Hz, padded or truncated to 1 second, and normalized to have unit $\ell^2$ norm. We simulate noise as i.i.d.\ Gaussian with standard deviation $\sigma = 0.01$ \cite{yan2023tpopt}. The training set consists of 100,000 noisy waveforms, the test set contains 5,000 noisy waveforms.





\section{Traversal Networks on Synthetic Manifolds} \label{sec:manifold_vis}

In this section, we present traversal networks created by \cref{alg:mtn-growth} on the following synthetic manifolds: sphere (Figure \ref{fig:sphere_visual}), torus (Figure \ref{fig:torus_visual}).

\begin{figure}[htbp]
    \centering
    \scalebox{0.8}{  % Scale to 80% of original size
    % First row
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_clean.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_LM.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_FOE.png}
    \end{minipage}
    
    \vspace{1em}
    % Second row
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_ZOE.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_network.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/torus/torus_all.png}
    \end{minipage}
    }
    
    \caption{Traversal network on the torus with noise level $\sigma=0.01$. Left-to-Right, Top-to-Bottom: Clean manifold, landmarks (blue dots), first-order edges (blue lines), zero-order edges (green lines), final traversal network, and final traversal network overlayed with clean manifold.}
    \label{fig:torus_visual}
\end{figure}



\begin{figure}[htbp]
    \centering
    \scalebox{0.8}{
    % First row
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_clean.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_LM.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_FOE.png}
    \end{minipage}
    \vspace{1em}
    % Second row
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_ZOE.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_network.png}
    \end{minipage}%
    \hfill%
    \begin{minipage}[t]{0.2\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figs/synthetic_mfs/sphere/sphere_all.png}
    \end{minipage}
    }
    
    \caption{Traversal network on the sphere created based on 100,000 noisy points with noise level $\sigma=0.01$. Left-to-Right, Top-to-Bottom: Clean manifold, landmarks (blue dots), first-order edges (blue lines), zero-order edges (green lines), final traversal network, and final traversal network overlayed with clean manifold.}
    \label{fig:sphere_visual}
\end{figure}















\section{Incremental PCA for Efficient Tangent Space Approximation} \label{sec:ipca_description}

\begin{algorithm}[tb]
      \caption{$\mathtt{IncrPCAonMatrix}(\mb X, d)$} \label{alg:TISVD_X}
      \begin{algorithmic}[1]
        \STATE \textbf{Input:} $\mb X = [\mb x_1, \dots, \mb x_n] \in \R^{D \times n}$ as a collection of points, $d$ as intrinsic dimension.
        \STATE $\mb U_1 \leftarrow [\mb x_1 / \| \mb x_1 \|]$
        \STATE $\mb S_1 \leftarrow [\|\mb x_1\|^2_2]$
        \FOR{i = 1, \dots, n-1}
        \STATE $\mb U_{i+1}, \mb S_{i+1} \leftarrow \mathtt{IncrPCA}(\mb x_{i+1}, \mb U_{i}, \mb S_{i}, i+1, d)$
    \ENDFOR
    \STATE \textbf{Output:} $\mb U_n, \mb S_n$
    \end{algorithmic}
\end{algorithm}

In this section, we detail the tangent space approximation implementation mentioned in \cref{alg:mtn-growth} and detailed in \cref{alg:TISVD_X} and \cref{alg:TISVD_one_point)}. We use incremental Principal Component Analysis (PCA) to efficiently process streaming high-dimensional data. Below we present the mathematics and algorithmic details of our implementation.

\paragraph{Initializing Local Model Parameters at New Landmarks:} If a newly created landmark $\mb q_M$ in \cref{alg:mtn-growth} has no other landmarks within $R_\mr{nbrs}$ distance, then its tangent space $T_{\mb q_M}$ is initialized randomly. Otherwise, we establish first-order connections to all existing landmarks within radius $R_\mr{nbrs}$, and the local parameters $T_{\mb q_M}$ and $\Xi_{\mb q_M}$ are then initialized in the following ways.



Let $ \left \{\mb q_i \right \}_{i=1}^k$ denote the set of first-order neighbors of landmark $\mb q_M$. We compute the normalized difference vectors:
\begin{align}
    \mb h_i = \frac{\mb q_i - \mb q_M}{\|\mb q_i - \mb q_M\|}
\end{align}
\noindent and assemble them into a matrix $\mb H = \left[ \begin{array}{cccc} \mb h_1 & \mb h_2 & \dots & \mb h_k \end{array} \right]$. The tangent space $T_{\mb q_M}$ is spanned by the orthonormal matrix $\mb U_{\mb q_M}$ obtained through truncated singular value decomposition of $\mb H$, which ensures $\mb U_{\mb q_M} \in \R^{D \times d}$, where $D$ and $d$ represent ambient and intrinsic dimensions, respectively. Edge embeddings $\Xi_{\mb q_M}$ are then created via projecting difference vectors $\mb q_i - \mb q_M$ onto $T_{\mb q_M}$.

\paragraph{Updating Tangent Space Approximations Efficiently:} Now that a new landmark $\mb q_M$ has been created along with $T_{\mb q_M}$ and $\Xi_{\mb q_M}$, we must update all three of them as more points arrive within radius $R(i)$ of $\mb q_M$. As \cref{alg:mtn-growth} proceeds, each new point $\mb x_{n+1}$ that appears within radius $R(i)$ of $\mb q_M$ is used to update the local parameters at vertex $M$. To approximate the local parameters at $\mb q_M$, we could consider the $n$ noisy points $\{\mb x_i\}_{i=1}^n$ which already lie within radius $R(i)$ of landmark $\mb q_M$, and local parameters $T_{\mb q_M}$ and $\Xi_{\mb q_M}$ can be established using these points, with $T_{\mb q_M} = \mr{span}(\mb U_n)$ where $\mb U_n \in \R^{D \times d}$. Landmark $\mb q_M$ is updated to be the average of all $n+1$ points. A straightforward way to approximate $T_{\mb q_M}$ would be to form $\mb X_{n+1} = \left[ \begin{array}{cccc} \mb x_1 & \mb x_2 & \dots & \mb x_{n+1} \end{array} \right]$ and simply let $\mb U_{n+1}, \mb S_{n+1}, \mb V_{n+1}^\ast \leftarrow \texttt{svd} \left(\mb X_{n+1} \right)$, and $ T_{\mb q_M} = \mr{span}(\mb U_{n+1})$. However, this presents a computational challenge, given dimensions of matrix $\mb X_{n+1}$ and computational complexity of SVD. Moreover, performing SVD on the entire set of points within $R(i)$ of $\mb q_M$ every time a new point is seen would be computationally redundant. This is why we implement tangent space estimation updates using the incremental PCA \cite{brand2006fast, arora2012stochastic}, detailed below.

Let vertex $M$ have local parameters $\mb q_M, T_{\mb q_M}, \Xi_{\mb q_M}$ with $T_{\mb q_M} = \mr{span}(\mb U_n)$. Let $\mb X_n = \left[ \begin{array}{cccc} \mb x_1 & \mb x_2 & \dots & \mb x_n \end{array} \right] \in \R^{D \times n}$, which is expanded by a new sample $\mb x_{n+1}$ to form the matrix $\mb X_{n+1} = \left[ \begin{array}{cc} \mb X_n & \mb x_{n+1}
\end{array} \right]$. Assume that we have the truncated singular value decomposition $\mb X_n \approx \mb U_n \mb S_n \mb V_n^T$, with orthonormal $\mb U_n \in \R^{D \times d}, \mb V_n \in \R^{n \times d}$, and diagonal $\mb S_n \in \R^{d \times d}$, with $\mb U_n$ spanning the tangent space at $\mb q_M$ prior the arrival of $\mb x_{n+1}$. Our goal is to compute the truncated SVD of $\mb X_{n+1} \approx \mb U_{n+1} \mb S_{n+1} \mb V_{n+1}^T$, and to do so \emph{efficiently}. We represent matrices $\mb U_{n+1}, \mb S_{n+1}, \mb V_{n+1}$ in terms of $\mb U_n, \mb S_n, \mb V_n$. 
\begin{eqnarray}
    \mb X_{n+1} &=& 
    \left[ \begin{array}{cc} \mb X_n & \mb x_{n+1} \end{array} \right] \\
    &=& {\left[ \begin{array}{cc} \mb X_n & \mb 0 \end{array} \right]} + {\mb x_{n+1}} \left[ \begin{array}{cc} \mb 0^T & 1 \end{array} \right] \\
    &=& {\left[ \begin{array}{cc} \mb U_n \mb S_n \mb V^T_n & \mb 0 \end{array} \right]} + {\mb x_{n+1}} {\left[ \begin{array}{cc} \mb 0^T & 1 \end{array} \right]} \label{eqn:sum1} \\
    &=& \mb U_{n+1} \mb S_{n+1} \mb V^T_{n+1}
\end{eqnarray}

for matrices $\mb U_{n+1} \in \R^{D \times d}, ~~ \mb S_{n+1} \in \R^{d \times d}$ and $ \mb V_{n+1} \in \R^{d \times (n+1)}$. Thus, finding the SVD of $\mb X_{n+1}$ is equivalent to finding the SVD of the sum in \eqref{eqn:sum1}.




\begin{algorithm}[tb]
      \caption{$\mathtt{IncrPCA}(\mb x_{i+1}, \mb U_{i}, \mb S_{i}, i+1, d)$}\label{alg:TISVD_one_point)}
      \begin{algorithmic}[1]
      \STATE \textbf{Inputs:} $\mb x_{i+1}$, $\mb U_{i}, \mb S_{i}$, $i+1$, $d$
      \STATE $\mb S_{\text{exp}} \leftarrow \left[  \begin{array}{cc}
       \mb S_i & \mb 0  \\
        \mb 0^T   &  0
      \end{array}\right]$
      \STATE $\mb x_{i+1}^\perp = \mb x_{i+1} - \mb U_i \mb U_i^T \mb x_{i+1}$
      \STATE $\mb K_{i+1} = \mb S_{exp} + \left[ \begin{array}{c} \mb U^T_i \mb x_{i+1} \\ \left\| \mb x_{i+1}^\perp \right\| \end{array} \right] \left[ \begin{array}{c} \mb U^T_i \mb x_{i+1} \\ \left\| \mb x_{i+1}^\perp \right\| \end{array} \right] ^T$
    \STATE $\mb U_{K_{i+1}}, \mb S_{K_{i+1}} \leftarrow \mathrm{svd}(\mb K_{i+1})$
    \STATE $\mb U_{i+1} \leftarrow \left[\begin{array}{cc} \mb U_i  & \frac{\mb x_{i+1}^\perp}{\left \| \mb x_{i+1}^\perp \right\|} \end{array} \right] \mb U_{\mb K_{i+1}}$
    \STATE $\mb S_{i+1} \leftarrow \mb S_{\mb K_{i+1}}$
    \IF{$i \geq d$}
        \STATE $\mb U_{i+1} \leftarrow  \mb U_{i+1}[:, :d]$
        \STATE $\mb S_{i+1} \leftarrow  \mb S_{i+1}[:d, :d]$
    \ENDIF

    \STATE \textbf{Output:} $\mb U_{i+1}, \mb S_{i+1}$
    \end{algorithmic}
\end{algorithm}


We then define the vector  $\mb b \in \R^{(n+1) \times 1}$ and the expand the matrix $\mb V_n$ to be

\begin{eqnarray}
    \mb b = \left[ \begin{array}{c} \mb 0 \\ 1 \end{array} \right] \in \R^{(n+1) \times 1}, \quad \mb V_{\text{exp}} = \left[ \begin{array}{c} \mb V_n \\ \mb 0^T \end{array}
\right] \in \R^{(n + 1) \times d}
\end{eqnarray}

and rewrite $\eqref{eqn:sum1}$ to be


\begin{eqnarray}
    \mb X_{n+1} &=& \mb U_n {\mb S_n} {\mb V_{\text{exp}}^T} + {\mb x_{n+1}} {\mb b^T} \\
    &=& \left[ \begin{array}{cc} {\mb U_n} & {\mb x_{n+1}} \end{array} \right] {\left[ \begin{array}{cc}
    \mb S_n & \mb 0  \\
    \mb 0^T     &  1
    \end{array} \right]} {\left[ \begin{array}{cc} \mb V_{\text{exp}} & \mb b \end{array} \right]^T}. \label{eqn:matrix_mult}
\end{eqnarray}

We now consider the first and the last matrices in the product above. Note that for a given point $\mb x_n$, we have  $\mb x_{n+1}^\perp = (\mb I - \mb U_n \mb U_n^T) \mb x_{n+1}$.

\begin{eqnarray}
    \left[ \begin{array}{cc} \mb U_n & \mb x_{n+1} \end{array} \right] &=& \left[ \begin{array}{cc} \mb U_n & \mb U_n \mb U^T_n \mb x_{n+1} + \underbrace{\mb x_{n+1} - \mb U_n \mb U^T_n \mb x_{n+1}}_\text{$\mb x^\perp_{n+1}$} \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb U_n & \mb U_n \mb U^T_n \mb x_{n+1} + \mb x^\perp_{n+1} \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \left[ \begin{array}{cc} \mb I & \mb U_n^T \mb x_{n+1} \\
    \mb 0^T & \left\| \mb x^\perp_{n+1} \right|\|  \end{array} \right]
\end{eqnarray}

Similarly,

\begin{eqnarray}
    \left[ \begin{array}{cc} \mb V_{\text{exp}} & \mb b \end{array} \right] &=& \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right] \left[ \begin{array}{cc} \mb I & \mb V_{\text{exp}}^T \mb b \\
    \mb 0^T & \left\| \mb b^\perp \right \|  \end{array} \right]
\end{eqnarray}
Putting these together, \eqref{eqn:matrix_mult} becomes
\begin{eqnarray}
    \mb X_{n+1} &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \left[ \begin{array}{cc} \mb I & \mb U_n^T \mb x_{n+1} \\
    \mb 0^T & \left\| \mb x^\perp_{n+1} \right|\|  \end{array} \right] \left[ \begin{array}{cc}
    \mb S_n & \mb 0  \\
    \mb 0^T     &  1
    \end{array} \right] \Bigg( \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right] \left[ \begin{array}{cc} \mb I & \mb V_{\text{exp}}^T \mb b \\
    \mb 0^T & \left\| \mb b^\perp \right \|  \end{array} \right] \Bigg)^T \\
    &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \underbrace{ \left[ \begin{array}{cc} \mb I & \mb U_n^T \mb x_{n+1} \\
    \mb 0^T & \left\| \mb x^\perp_{n+1} \right|\|  \end{array} \right] \left[ \begin{array}{cc}
    \mb S_n & \mb 0  \\
    \mb 0^T     &  1
    \end{array} \right]  \left[ \begin{array}{cc} \mb I & \mb 0  \\
    \mb b^T \mb V_{\text{exp}} & \left\| \mb b^\perp \right \|  \end{array} \right] }_\text{$\mb K$} \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right]^T \\
    &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \mb K \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right]^T \label{eqn:X_with_K}
\end{eqnarray}
where 
\begin{eqnarray}
    \mb K &=& \left[ \begin{array}{cc} \mb I & {\mb U_n^T \mb x_{n+1}} \\
    \mb 0^T & \left\| \mb x^\perp_{n+1} \right|\|  \end{array} \right] {\left[ \begin{array}{cc}
    {\mb S_n} & {\mb 0}  \\
    {\mb 0^T} &  {1}
    \end{array} \right]} \left[ \begin{array}{cc} \mb I & \mb 0  \\
    {\mb b^T \mb V_{\text{exp}}} & \left\| \mb b^\perp \right \|  \end{array} \right] \label{eqn:genera_K}
\end{eqnarray}
This is a general form for matrix $
\mb K$. We can further simplify it via
\begin{eqnarray}
    \mb K &=& \left[ \begin{array}{cc} \mb S_n & \mb U_n^T \mb x_{n+1} \\
    \mb 0^T & \left\| \mb x^\perp_{n+1} \right|\|  \end{array} \right] \left[ \begin{array}{cc} \mb I & \mb 0  \\
    \mb b^T \mb V_{\text{exp}} & \left\| \mb b^\perp \right \|  \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb S_n + \mb U_n^T \mb x_{n+1} \mb b^T \mb V_{\text{exp}} & \mb U_n^T \mb x_{n+1} \left\| \mb b^\perp \right \| \\
    \left\| \mb x^\perp_{n+1} \right\| \mb b^T \mb V_{\text{exp}} & \left\| \mb x^\perp_{n+1} \right\| \left\| \mb b^\perp \right \|  \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb S_n & \mb 0 \\
    \mb 0^T & 0  \end{array} \right] + \left[ \begin{array}{cc} \mb U_n^T \mb x_{n+1} \mb b^T \mb V_{\text{exp}} & \mb U_n^T \mb x_{n+1} \left\| \mb b^\perp \right \| \\
    \left\| \mb x^\perp_{n+1} \right\| \mb b^T \mb V_{\text{exp}} & \left\| \mb x^\perp_{n+1} \right\| \left\| \mb b^\perp \right \|  \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb S_n & \mb 0 \\
    \mb 0^T & 0  \end{array} \right] + \left[ \begin{array}{c} \mb U_n^T \mb x_{n+1} \\ \left\| \mb x^\perp_{n+1} \right\|  \end{array} \right] \left[ \begin{array}{cc} \mb b^T  \mb V_{\text{exp}} & \left\| \mb b^\perp \right\|  \end{array} \right] \\
    &=& \left[ \begin{array}{cc} \mb S_n & \mb 0 \\
    \mb 0^T & 0  \end{array} \right] + \left[ \begin{array}{c} \mb U_n^T \mb x_{n+1} \\ \left\| \mb x^\perp_{n+1} \right\|  \end{array} \right] \left[ \begin{array}{c}  \mb V_{\text{exp}}^T \mb b  \\ \left\| \mb b^\perp \right\|  \end{array} \right]^T \label{eqn:sparse_K}
\end{eqnarray}
Equation \eqref{eqn:sparse_K} is another general form for the matrix $\mb K$. Note that $\mb K$ is highly structured and sparse\cite{brand2006fast}. Since it is of size $(d +1) \times (d + 1)$, the $\mb U_K, \mb S_K, \mb V_K \leftarrow \texttt{svd}(\mb K)$ will merely cost $\mathcal{O} (d^3)$.
Finally, we rewrite \eqref{eqn:X_with_K} as
\begin{eqnarray}
    \mb X_{n+1} &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \mb U_K \mb S_K \mb V_K^T \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right]^T \\
    &=& \mb U_{n+1} \mb S_{n+1} \mb V_{n+1}^T
\end{eqnarray}



\begin{table}[t]
\caption{These are the simulation results which yield the performance/complexity tradeoff curve in Figure \ref{fig:complexity_accuracy}. The experiment setup is described in that section of the paper. We describe the hyperparameters defining each denoiser in the following table.}
\label{complexity_accuracy_table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{rccccc}
\toprule
Denoiser \# & \# Landmarks & MT Complexity & NN Complexity & MT Performance & NN Performance \\
\midrule
1 & 1221 & 7.99E04 & 2.50E06 & 1.13E-02 & 1.02E-02 \\
2 & 212 & 6.40E04 & 4.34E05 & 2.22E-02 & 2.06E-02 \\
3 & 1356 & 7.22E04 & 2.78E06 & 1.23E-02 & 9.67E-03 \\
4 & 54 & 5.43E04 & 1.11E05 & 4.4E-02 & 3.97E-02 \\
5 & 392 & 6.84E04 & 8.03E05 & 1.49E-02 & 1.30E-02 \\
6 & 3844 & 7.03E04 & 7.87E06 & 1.45E-02 & 1.04E-02 \\
7 & 84 & 5.00E04 & 1.72E05 & 2.91E-02 & 2.70E-02 \\
8 & 237 & 6.39E04 & 4.85E05 & 1.69E-02 & 1.28E-02 \\
9 & 358 & 6.44E04 & 7.33E05 & 2.35E-02 & 2.11E-02 \\
10 & 125 & 6.25E04 & 2.56E05 & 2.58E-02 & 2.22E-02 \\
11 & 44 & 5.01E04 & 9.01E04 & 5.32E-02 & 5.11E-02 \\
12 & 634 & 6.19E04 & 1.30E06 & 2.41E-02 & 2.19E-02 \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}


Thus, final update equations implemented in \cref{alg:TISVD_one_point)} are
\begin{eqnarray}
    \mb U_{n+1} &=& \left[ \begin{array}{cc} \mb U_n & \frac{\mb x_{n+1}^\perp}{\left \|\mb x_{n+1}^\perp \right \|} \end{array} \right] \mb U_K \label{eqn:update_U} \\
    \mb S_{n+1} &=& \mb S_K \label{eqn:update_S} \\
    \mb V_{n+1} &=&  \left[ \begin{array}{cc} \mb V_{\text{exp}} & \frac{\mb b^\perp}{\left \|\mb b^\perp \right \|} \end{array} \right] \mb V_K \label{eqn:update_V}
\end{eqnarray}

Equations \eqref{eqn:update_U} and  \eqref{eqn:update_S} define the final update equations. Note that in this specific case, $\mb b^\perp = \pmb b$, and $\left\| \mb b^\perp \right\| = 1 $. Here, the computational cost is $\mc{O}(nDd^2)$ with storage of $\mc{O}(Dd)$. 








\section{Choosing Denoising Radius} \label{sec:R_i_description}

The parameter called denoising radius $R(i)$ in \cref{alg:mtn-growth} controls complexity by determining the number of landmarks created. Conceptually, as the online algorithm learns, the error in landmarks decreases, which means that $R(i)$ needs to be decreased as the landmark gets learned. This is why we define a general formula for $R(i)$ as follows:

\begin{equation}
    R(i)^2 = c_1 \left(\sigma^2 D + \frac{\sigma^2 D}{{N_i}^k} + c_2\sigma^2 d \right)
\end{equation}

\noindent where $N_i$ denotes the number of points assigned to landmark $\mb q_i$. The power parameter $k$ helps us control the speed of decay of $R(i)$, making it adaptable to different datasets. Table \ref{complexity_accuracy_table} and \ref{parameter_choices_table} show the specific constants used to create the $R(i)$ parameter to produce Figure \ref{fig:complexity_accuracy}.


\begin{table}[t]
\caption{The choice of hyperparameters yielding each denoiser. $N_{i}$ corresponds to the number of points assigned to a landmark $q_i$. For all experiments, $\sigma=0.01$, $d=2$,and $D=2048$.}
\label{parameter_choices_table}
\vskip 0.15in
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{rccccc}
\toprule
Denoiser \# & $R_{\text{denoising}}^2$ & $R_{\text{nbrs}}$ \\
\midrule
1 & $1.2(\sigma^2D + \frac{\sigma^2 D}{N_i^{1/2}}+20\sigma^2d)$ & $2.39\sigma^2D$ \\
2 & $2.06\sigma^2D$ & $2.39\sigma^2D$ \\
3 & $1.2(\sigma^2D + \frac{\sigma^2 D}{N_i^{1/2}}+8\sigma^2d)$ & $2.39\sigma^2D$ \\
4 & $2.75 \sigma^2 D$ & $3.13\sigma^2D$ \\
5 & $1.3(\sigma^2D + \frac{\sigma^2 D}{N_i^{1/3}}+20\sigma^2d)$& $2.39\sigma^2D$ \\
6 & $1.15(\sigma^2D + \frac{\sigma^2 D}{N_i^{1/2}}+4\sigma^2d)$  & $2.39\sigma^2D$ \\
7 & $2.39\sigma^2D$ & $2.75\sigma^2D$ \\
8 &  $1.5 ( \sigma^2 D + \frac{\sigma^2 D}{Ni^1/2} +30 \sigma^2 d)$ & $2.39\sigma^2D$ \\
9 & $2\sigma^2D$ & $2.39\sigma^2D$ \\
10 & $2.19\sigma^2D$ & $2.39\sigma^2D$ \\
11 & $3.13\sigma^2D$ & $3.53\sigma^2D$ \\
12 & $1.94\sigma^2D$ & $2.39\sigma^2D$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vskip -0.1in
\end{table}









